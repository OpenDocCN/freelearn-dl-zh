# 前言

本书讲述的是强化学习（RL），它是机器学习（ML）的一个子领域；本书专注于学习在复杂环境中学习最优行为这一普遍而具有挑战性的问题。学习过程仅由从环境中获得的奖励值和观察结果驱动。这一模型非常通用，可以应用于许多实际情况，从玩游戏到优化复杂的制造过程。在本书中，我们主要集中在深度强化学习上，深度强化学习是利用深度学习（DL）方法的强化学习。

由于其灵活性和通用性，强化学习领域发展迅速，吸引了大量关注，既来自那些试图改进现有方法或创造新方法的研究人员，也来自那些希望以最有效的方式解决实际问题的实践者。

# 为什么我写这本书

强化学习（RL）领域在全球范围内有着大量的持续研究活动。几乎每天都有新的研究论文发表，许多深度学习（DL）会议，如神经信息处理系统会议（NeurIPS）或国际学习表示会议（ICLR），都专注于强化学习方法。此外，还有一些大型研究团队专注于将强化学习方法应用于机器人技术、医学、多智能体系统等领域。

然而，尽管关于最新研究的信息已广泛可得，但它们过于专业化和抽象，难以轻松理解。更糟糕的是，强化学习的实际应用方面，往往并不明显如何将一篇研究论文中以数学为主的抽象方法转化为能够解决实际问题的有效实现。

这使得有兴趣的人很难清楚地理解论文和会议报告中方法和思想的背后。虽然有一些关于强化学习各个方面的很好的博客文章，并附带了实际的示例，但博客文章的有限格式使得作者只能描述一两种方法，无法建立一个完整的结构化图像，也不能系统地展示不同方法之间的关系。本书写作的目的就是填补这一强化学习方法和途径的实践性和结构性信息的明显空白。

# 方法

这本书的一个关键方面是其实践导向。每种方法都适用于各种环境，从非常简单到相当复杂。我尝试使示例简洁易懂，这得益于 PyTorch 的表达力和强大功能。另一方面，示例的复杂性和要求是面向没有访问非常大计算资源（如图形处理单元（GPU）集群或非常强大的工作站）的强化学习（RL）爱好者的。我相信，这将使充满乐趣和激动人心的 RL 领域可以为比研究小组或大型人工智能公司更广泛的受众所接触。另一方面，这仍然是深度强化学习，因此强烈建议使用 GPU，因为加速计算将使实验变得更加方便（等待数周才能完成一次优化并不有趣）。本书中大约一半的示例将在 GPU 上运行时受益。

除了传统的中型 RL 环境示例，如 Atari 游戏或连续控制问题外，本书还包含了若干章节（第 10、13、14、19、20 和 21 章），这些章节包含了更大的项目，展示了如何将 RL 方法应用于更复杂的环境和任务。这些示例仍然不是完整的、现实生活中的项目（这些将占据一本独立的书），但只是一些更大的问题，说明了 RL 范式如何应用于超越公认基准的领域。

另一个需要注意的事情是，本书第一、二、三部分的示例我尽力使其自包含，源代码全部展示。有时这导致代码片段的重复（例如，大多数方法中的训练循环非常相似），但我认为给予你直接跳入你想学习的方法的自由，比避免一些重复更加重要。本书中的所有示例都可以在 GitHub 上找到，网址是 [`github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-3E/`](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-3E/)，欢迎你进行分支、实验和贡献。

除了源代码外，几个章节（第 15、16、19 和 22 章）还附带了训练模型的视频录制。这些录制可以在以下 YouTube 播放列表中找到：[`youtube.com/playlist?list=PLMVwuZENsfJmjPlBuFy5u7c3uStMTJYz7`](https://youtube.com/playlist?list=PLMVwuZENsfJmjPlBuFy5u7c3uStMTJYz7)。

# 本书的目标读者

本书非常适合机器学习工程师、软件工程师和数据科学家，他们希望学习并实际应用深度强化学习。书中假设读者已经熟悉 Python、微积分和机器学习概念。通过实际示例和高级概述，本书也适合有经验的专业人士，帮助他们加深对高级深度强化学习方法的理解，并在各行业中应用，如游戏和金融。

# 本书内容概览

第一章，《什么是强化学习？》介绍了强化学习（RL）的基本概念和主要的正式模型。

第二章，《OpenAI Gym API 与 Gymnasium》介绍了 RL 的实践方面，使用开源库 Gym 及其后代 Gymnasium。

第三章，《使用 PyTorch 进行深度学习》为你提供了 PyTorch 库的快速概述。

第四章，《交叉熵方法》介绍了 RL 中最简单的方法之一，让你对 RL 方法和问题有个基本了解。

第五章，《表格学习与贝尔曼方程》本章开启了本书的第二部分，专注于基于价值的方法。

第六章，《深度 Q 网络》描述了深度 Q 网络（DQN），这是一种扩展基本价值方法的技术，使我们能够解决复杂的环境问题。

第七章，《更高级的 RL 库》描述了 PTAN 库，我们将在本书中使用该库来简化 RL 方法的实现。

第八章，《DQN 扩展》详细概述了 DQN 方法的现代扩展，以改善其在复杂环境中的稳定性和收敛性。

第九章，《加速 RL 方法的方式》概述了加速 RL 代码执行的几种方法。

第十章，《使用 RL 进行股票交易》是第一个实际项目，重点应用 DQN 方法进行股票交易。

第十一章，《策略梯度》开启了本书的第三部分，并介绍了另一类基于直接优化策略的 RL 方法。

第十二章，《演员-评论员方法：A2C 和 A3C》描述了强化学习中最广泛使用的基于策略的方法之一。

第十三章，《TextWorld 环境》介绍了将 RL 方法应用于互动小说游戏。

第十四章，《网页导航》是另一个长篇项目，应用强化学习（RL）于网页导航，使用 MiniWoB++环境。

第十五章，《连续动作空间》开启了本书的高级 RL 部分，描述了使用连续动作空间的环境的特点和各种方法（广泛应用于机器人技术）。

第十六章，《信任域》是另一章关于连续动作空间的内容，描述了信任域集方法：PPO、TRPO、ACKTR 和 SAC。

第十七章，《RL 中的黑箱优化》展示了另一类不显式使用梯度的优化方法。

第十八章，《高级探索》介绍了更好探索环境的不同方法——这是 RL 中的一个非常重要的方面。

第十九章，《带有人工反馈的强化学习（RLHF）》，介绍并实现了通过给予人类反馈来指导学习过程的最新方法。这种方法在训练大型语言模型（LLMs）中被广泛应用。在这一章中，我们将从零开始实现 RLHF 流程，并检查其效率。

第二十章，《AlphaGo Zero 与 MuZero》，描述了 AlphaGo Zero 方法及其演变为 MuZero，并将这两种方法应用于游戏《四子连珠》。

第二十一章，《离散优化中的强化学习（RL）》，描述了将强化学习方法应用于离散优化领域，使用魔方作为环境。

第二十二章，《多智能体强化学习（Multi-Agent RL）》，介绍了一种相对较新的强化学习方法方向，适用于多个智能体的情境。

# 为了最大化本书的价值。

本书适合你，如果你使用的是至少 32 GB RAM 的机器。虽然并不严格要求 GPU，但强烈推荐使用 Nvidia GPU。代码已经在 Linux 和 macOS 上进行了测试。有关硬件和软件要求的更多详细信息，请参阅第二章。

本书中所有描述强化学习方法的章节都有相同的结构：一开始，我们讨论该方法的动机、理论基础以及背后的思想。然后，我们通过多个示例，展示该方法应用于不同环境的过程，并附上完整的源代码。

你可以以不同的方式使用本书：

+   为了快速熟悉某一特定方法，你可以仅阅读相关章节的引言部分。

+   为了更深入地理解方法的实现方式，你可以阅读代码及其附带的解释。

+   为了更深入地熟悉该方法（我认为这是最好的学习方式），你可以尝试重新实现该方法并使其正常工作，使用提供的源代码作为参考。

无论你选择哪种方式，我希望本书对你有帮助！

# 第三版的变化

相较于本书的第二版（2020 年出版），在新版本中对内容做了几个重大更改：

+   所有代码示例的依赖项已更新为最新版本或替换为更好的替代品。例如，OpenAI Gym 不再被支持，但我们有 Farama Foundation 的 Gymnasium 分支。另一个例子是 MiniWoB++库，它替代了 MiniWoB 和 Universe 环境。

+   新增了一章关于 RLHF（人类反馈强化学习），并且将 MuZero 方法加入了 AlphaGo Zero 章节。

+   有很多小的修复和改进——大多数图示已经重新绘制，以使其更清晰、更易理解。

为了更好地满足书籍篇幅的限制，几个章节进行了重新安排，我希望这样能使本书更加一致且易于阅读。

# 下载示例代码文件

本书的代码包托管在 GitHub 上，网址为[`github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition`](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Third-Edition)。我们还提供来自丰富书籍和视频目录的其他代码包，可在[`github.com/PacktPublishing/`](https://github.com/PacktPublishing/)查看！

# 下载彩色图像

我们还提供了一个 PDF 文件，其中包含本书中使用的屏幕截图/图表的彩色图像。您可以在此处下载：[`packt.link/gbp/9781835882702`](https://packt.link/gbp/9781835882702)。

# 使用的约定

本书中使用了许多文本约定。CodeInText：指示文本中的代码词、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟 URL、用户输入和 Twitter 用户名。例如：“对于奖励表，它表示为一个元组，其中[State，Action，State]，而对于转换表，则写为[State，Action]。”

代码块设置如下：

```py
import typing as tt 
import gymnasium as gym 
from collections import defaultdict, Counter 
from torch.utils.tensorboard.writer import SummaryWriter 

ENV_NAME = "FrozenLake-v1" 
GAMMA = 0.9 
TEST_EPISODES = 20
```

任何命令行输入或输出将写成以下形式：

```py
>>> e.action_space 
Discrete(2) 
>>> e.observation_space 
Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)
```

**粗体：**表示新术语、重要词汇或屏幕上显示的词语。例如，菜单或对话框中的词汇会以此方式出现在文本中。例如：“第二个术语称为交叉熵，这是深度学习中非常常见的优化目标。”引用使用紧凑的作者-年份格式放在方括号内，类似于[Sut88]或[Kro+11]。您可以在书末的参考书目部分找到相应论文的详细信息。

警告或重要提示将以此方式显示。

提示和技巧将以此方式显示。

# 联系我们

我们非常欢迎读者的反馈。

一般反馈：电子邮件 feedback@packtpub.com，并在邮件主题中提及书名。如果您对本书的任何方面有疑问，请通过 questions@packtpub.com 与我们联系。

勘误：尽管我们已经尽一切努力确保内容的准确性，但错误难免发生。如果您在本书中发现错误，请向我们报告。请访问[`www.packtpub.com/submit-errata`](http://www.packtpub.com/submit-errata)，点击“提交勘误”，填写表格。

盗版：如果您在互联网上发现我们作品的任何非法副本，请您提供位置地址或网站名称。请通过链接将信息发送至 copyright@packtpub.com。

如果您有兴趣成为作者：如果您在某个专题上有专业知识，并且有意撰写或为一本书作贡献，请访问[`authors.packtpub.com`](http://authors.packtpub.com)。

# 留下评论！

感谢你购买 Packt 出版的这本书——希望你喜欢！你的反馈非常宝贵，能够帮助我们改进和成长。阅读完后，请花点时间留下一个[亚马逊评论](https://packt.link/r/1835882714)；这只需一分钟，但对像你这样的读者来说，意义重大。

扫描下方二维码，选择一本免费的电子书。

![图片](img/file3.png)

*https://packt.link/NzOWQ*

# 下载此书的免费 PDF 副本

感谢你购买本书！

你喜欢随时阅读，但又不能把纸质书籍带到处走吗？你购买的电子书是否与你选择的设备不兼容？

别担心；每本 Packt 书籍，你现在都能免费获得该书的无 DRM PDF 版本。

在任何设备上随时阅读。直接从你最喜欢的技术书籍中搜索、复制并粘贴代码到你的应用程序中。

特权不止于此！你还可以获得独家折扣、新闻通讯以及每天发送到你邮箱的精彩免费内容。

按照以下简单步骤获得福利：

1.  扫描二维码或访问以下链接：

    ![图片](img/file4.png)

    *https://packt.link/free-ebook/9781835882702*

1.  提交你的购买凭证。

1.  就是这样！我们会直接将你的免费 PDF 和其他福利发送到你的电子邮箱。

# 第一部分

# 强化学习简介
