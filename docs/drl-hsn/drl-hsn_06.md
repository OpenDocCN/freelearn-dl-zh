

# ç¬¬å…­ç« ï¼šæ·±åº¦ Q ç½‘ç»œ

åœ¨ç¬¬äº”ç« ä¸­ï¼Œä½ å·²ç»ç†Ÿæ‚‰äº†è´å°”æ›¼æ–¹ç¨‹ä»¥åŠå…¶åº”ç”¨çš„å®é™…æ–¹æ³•â€”â€”ä»·å€¼è¿­ä»£ã€‚è¿™ä¸ªæ–¹æ³•è®©æˆ‘ä»¬åœ¨ FrozenLake ç¯å¢ƒä¸­æ˜¾è‘—æé«˜äº†é€Ÿåº¦å’Œæ”¶æ•›æ€§ï¼Œè¿™å¾ˆæœ‰å‰æ™¯ï¼Œä½†æˆ‘ä»¬èƒ½è¿›ä¸€æ­¥æå‡å—ï¼Ÿåœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠç›¸åŒçš„æ–¹æ³•åº”ç”¨åˆ°å¤æ‚åº¦æ›´é«˜çš„é—®é¢˜ä¸Šï¼šæ¥è‡ª Atari 2600 å¹³å°çš„è¡—æœºæ¸¸æˆï¼Œè¿™äº›æ¸¸æˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ç ”ç©¶ç¤¾åŒºçš„äº‹å®æ ‡å‡†ã€‚

ä¸ºäº†åº”å¯¹è¿™ä¸€æ–°çš„ã€æ›´å…·æŒ‘æˆ˜æ€§çš„ç›®æ ‡ï¼Œæœ¬ç« å°†ï¼š

+   è®¨è®ºä»·å€¼è¿­ä»£æ–¹æ³•çš„é—®é¢˜ï¼Œå¹¶è€ƒè™‘å®ƒçš„å˜ä½“â€”â€”Q å­¦ä¹ ã€‚

+   å°† Q å­¦ä¹ åº”ç”¨äºæ‰€è°“çš„ç½‘æ ¼ä¸–ç•Œç¯å¢ƒï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä¸ºè¡¨æ ¼ Q å­¦ä¹ ã€‚

+   è®¨è®º Q å­¦ä¹ ä¸ç¥ç»ç½‘ç»œï¼ˆNNsï¼‰çš„ç»“åˆã€‚è¿™ç§ç»“åˆè¢«ç§°ä¸ºæ·±åº¦ Q ç½‘ç»œï¼ˆDQNï¼‰ã€‚

åœ¨æœ¬ç« çš„ç»“å°¾ï¼Œæˆ‘ä»¬å°†é‡æ–°å®ç°è‘—åè®ºæ–‡ã€ŠPlaying Atari with Deep Reinforcement Learningã€‹[Mni13]ä¸­çš„ DQN ç®—æ³•ï¼Œè¯¥è®ºæ–‡äº 2013 å¹´å‘å¸ƒï¼Œå¹¶å¼€å¯äº†å¼ºåŒ–å­¦ä¹ å‘å±•çš„æ–°çºªå…ƒã€‚è™½ç„¶è®¨è®ºè¿™äº›åŸºæœ¬æ–¹æ³•çš„å®é™…åº”ç”¨è¿˜ä¸ºæ—¶è¿‡æ—©ï¼Œä½†éšç€ä½ æ·±å…¥é˜…è¯»æœ¬ä¹¦ï¼Œä½ ä¼šæ›´æ¸…æ¥šåœ°çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚

# ç°å®ç”Ÿæ´»ä¸­çš„ä»·å€¼è¿­ä»£

é€šè¿‡å°†äº¤å‰ç†µæ–¹æ³•è½¬æ¢ä¸ºä»·å€¼è¿­ä»£æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨ FrozenLake ç¯å¢ƒä¸­å–å¾—çš„æ”¹è¿›éå¸¸ä»¤äººé¼“èˆï¼Œå› æ­¤å°†ä»·å€¼è¿­ä»£æ–¹æ³•åº”ç”¨äºæ›´å…·æŒ‘æˆ˜æ€§çš„é—®é¢˜éå¸¸æœ‰å¸å¼•åŠ›ã€‚ç„¶è€Œï¼Œé‡è¦çš„æ˜¯è¦æŸ¥çœ‹æˆ‘ä»¬ä»·å€¼è¿­ä»£æ–¹æ³•çš„å‡è®¾å’Œå±€é™æ€§ã€‚ä½†è®©æˆ‘ä»¬å…ˆå¿«é€Ÿå›é¡¾ä¸€ä¸‹è¿™ä¸ªæ–¹æ³•ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œä»·å€¼è¿­ä»£æ–¹æ³•ä¼šéå†æ‰€æœ‰çŠ¶æ€ï¼Œå¹¶å¯¹æ¯ä¸ªçŠ¶æ€ä½¿ç”¨è´å°”æ›¼è¿‘ä¼¼è¿›è¡Œä»·å€¼æ›´æ–°ã€‚å¯¹ Q å€¼ï¼ˆåŠ¨ä½œçš„ä»·å€¼ï¼‰è¿›è¡Œçš„ç›¸åŒæ–¹æ³•å˜ä½“å‡ ä¹ç›¸åŒï¼Œä½†æˆ‘ä»¬è¦ä¸ºæ¯ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œè¿‘ä¼¼å¹¶å­˜å‚¨ä»·å€¼ã€‚é‚£ä¹ˆï¼Œè¿™ä¸ªè¿‡ç¨‹åˆ°åº•æœ‰ä»€ä¹ˆé—®é¢˜å‘¢ï¼Ÿ

ç¬¬ä¸€ä¸ªæ˜æ˜¾çš„é—®é¢˜æ˜¯ç¯å¢ƒçŠ¶æ€çš„æ•°é‡ä»¥åŠæˆ‘ä»¬éå†å®ƒä»¬çš„èƒ½åŠ›ã€‚åœ¨ä»·å€¼è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬äº‹å…ˆçŸ¥é“ç¯å¢ƒä¸­çš„æ‰€æœ‰çŠ¶æ€ï¼Œèƒ½å¤Ÿéå†å®ƒä»¬ï¼Œå¹¶ä¸”å¯ä»¥å­˜å‚¨å®ƒä»¬çš„ä»·å€¼è¿‘ä¼¼å€¼ã€‚å¯¹äº FrozenLake çš„ç®€å•ç½‘æ ¼ä¸–ç•Œç¯å¢ƒï¼Œè¿™å¾ˆå®¹æ˜“åšåˆ°ï¼Œä½†å¯¹äºå…¶ä»–ä»»åŠ¡å‘¢ï¼Ÿ

ä¸ºäº†ç†è§£è¿™ä¸€ç‚¹ï¼Œé¦–å…ˆè®©æˆ‘ä»¬çœ‹çœ‹ä»·å€¼è¿­ä»£æ–¹æ³•çš„å¯æ‰©å±•æ€§ï¼Œæ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬åœ¨æ¯æ¬¡å¾ªç¯ä¸­èƒ½å¤Ÿè½»æ¾éå†å¤šå°‘ä¸ªçŠ¶æ€ã€‚å³ä½¿æ˜¯ä¸­ç­‰å¤§å°çš„è®¡ç®—æœºï¼Œä¹Ÿå¯ä»¥åœ¨å†…å­˜ä¸­å­˜å‚¨æ•°åäº¿ä¸ªæµ®ç‚¹å€¼ï¼ˆåœ¨ 32GB çš„å†…å­˜ä¸­æ˜¯ 85 äº¿ï¼‰ï¼Œå› æ­¤ï¼Œä»·å€¼è¡¨æ‰€éœ€çš„å†…å­˜ä¼¼ä¹ä¸æ˜¯ä¸€ä¸ªå·¨å¤§çš„é™åˆ¶ã€‚éå†æ•°åäº¿ä¸ªçŠ¶æ€å’ŒåŠ¨ä½œä¼šæ›´åŠ æ¶ˆè€—ä¸­å¤®å¤„ç†å•å…ƒï¼ˆCPUï¼‰ï¼Œä½†è¿™å¹¶ä¸æ˜¯ä¸€ä¸ªæ— æ³•å…‹æœçš„é—®é¢˜ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰äº†å¤šæ ¸ç³»ç»Ÿï¼Œè¿™äº›ç³»ç»Ÿå¤§å¤šæ˜¯é—²ç½®çš„ï¼Œæ‰€ä»¥é€šè¿‡ä½¿ç”¨å¹¶è¡Œå¤„ç†ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åˆç†çš„æ—¶é—´å†…éå†æ•°åäº¿ä¸ªå€¼ã€‚çœŸæ­£çš„é—®é¢˜åœ¨äºï¼Œè·å–çŠ¶æ€è½¬æ¢åŠ¨æ€çš„è‰¯å¥½è¿‘ä¼¼æ‰€éœ€çš„æ ·æœ¬æ•°é‡ã€‚å‡è®¾ä½ æœ‰ä¸€ä¸ªç¯å¢ƒï¼Œå‡è®¾æœ‰åäº¿ä¸ªçŠ¶æ€ï¼ˆè¿™å¤§çº¦å¯¹åº”ä¸€ä¸ª 31600 Ã— 31600 å¤§å°çš„ FrozenLakeï¼‰ã€‚è¦è®¡ç®—è¿™ä¸ªç¯å¢ƒä¸­æ¯ä¸ªçŠ¶æ€çš„ç²—ç•¥è¿‘ä¼¼ï¼Œæˆ‘ä»¬éœ€è¦æ•°ç™¾äº¿ä¸ªçŠ¶æ€ä¹‹é—´å‡åŒ€åˆ†å¸ƒçš„è½¬æ¢ï¼Œè¿™åœ¨å®è·µä¸­æ˜¯ä¸å¯è¡Œçš„ã€‚

ä¸ºäº†ç»™ä½ ä¸€ä¸ªæ›´å¤§æ½œåœ¨çŠ¶æ€æ•°é‡çš„ç¯å¢ƒç¤ºä¾‹ï¼Œæˆ‘ä»¬å†æ¥çœ‹çœ‹ Atari 2600 æ¸¸æˆä¸»æœºã€‚è¿™æ¬¾ä¸»æœºåœ¨ 1980 å¹´ä»£éå¸¸æµè¡Œï¼Œå¹¶ä¸”æœ‰è®¸å¤šè¡—æœºé£æ ¼çš„æ¸¸æˆå¯ä¾›é€‰æ‹©ã€‚è™½ç„¶æŒ‰ç…§ä»Šå¤©çš„æ¸¸æˆæ ‡å‡†ï¼ŒAtari ä¸»æœºæ˜¾å¾—è¿‡æ—¶ï¼Œä½†å®ƒçš„æ¸¸æˆæä¾›äº†ä¸€ç»„å¾ˆå¥½çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é—®é¢˜ï¼Œäººç±»å¯ä»¥ç›¸å¯¹å¿«é€Ÿåœ°æŒæ¡è¿™äº›é—®é¢˜ï¼Œä½†å¯¹è®¡ç®—æœºæ¥è¯´ä»ç„¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ä¸è¶³ä¸ºå¥‡çš„æ˜¯ï¼Œæ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„ï¼Œè¿™ä¸ªå¹³å°ï¼ˆå½“ç„¶æ˜¯ä½¿ç”¨æ¨¡æ‹Ÿå™¨ï¼‰åœ¨å¼ºåŒ–å­¦ä¹ ç ”ç©¶ä¸­æ˜¯ä¸€ä¸ªéå¸¸å—æ¬¢è¿çš„åŸºå‡†ã€‚

è®©æˆ‘ä»¬æ¥è®¡ç®—ä¸€ä¸‹ Atari å¹³å°çš„çŠ¶æ€ç©ºé—´ã€‚å±å¹•çš„åˆ†è¾¨ç‡æ˜¯ 210 Ã— 160 åƒç´ ï¼Œæ¯ä¸ªåƒç´ æœ‰ 128 ç§é¢œè‰²ã€‚å› æ­¤ï¼Œæ¯ä¸€å¸§å±å¹•æœ‰ 210 â‹… 160 = 33600 ä¸ªåƒç´ ï¼Œè€Œæ‰€æœ‰å¯èƒ½çš„ä¸åŒå±å¹•æ€»æ•°æ˜¯ 128Â³Â³â¶â°â°ï¼Œçº¦ç­‰äº 10â·â°â¸â°Â²ã€‚å¦‚æœæˆ‘ä»¬å†³å®šåªåˆ—ä¸¾ä¸€æ¬¡ Atari æ‰€æœ‰å¯èƒ½çš„çŠ¶æ€ï¼Œå³ä½¿æ˜¯æœ€å¿«çš„è¶…çº§è®¡ç®—æœºä¹Ÿéœ€è¦æ•°åäº¿å¹´ã€‚è€Œä¸”ï¼Œè¿™é¡¹å·¥ä½œä¸­ 99(.9)%çš„æ—¶é—´éƒ½å°†æ˜¯æµªè´¹ï¼Œå› ä¸ºå¤§å¤šæ•°ç»„åˆåœ¨é•¿æ—¶é—´çš„æ¸¸æˆè¿‡ç¨‹ä¸­ä»æœªå‡ºç°è¿‡ï¼Œæˆ‘ä»¬ä¹Ÿä¸ä¼šæœ‰è¿™äº›çŠ¶æ€çš„æ ·æœ¬ã€‚ç„¶è€Œï¼Œå€¼è¿­ä»£æ–¹æ³•ä»ç„¶æƒ³éå†è¿™äº›çŠ¶æ€ï¼Œä»¥é˜²ä¸‡ä¸€ã€‚

å€¼è¿­ä»£æ–¹æ³•çš„ç¬¬äºŒä¸ªä¸»è¦é—®é¢˜æ˜¯ï¼Œå®ƒå°†æˆ‘ä»¬é™åˆ¶åœ¨ç¦»æ•£åŠ¨ä½œç©ºé—´ä¸­ã€‚å®é™…ä¸Šï¼ŒQ(s,a) å’Œ V(s) çš„è¿‘ä¼¼éƒ½å‡è®¾æˆ‘ä»¬çš„åŠ¨ä½œæ˜¯ä¸€ä¸ªç›¸äº’æ’æ–¥çš„ç¦»æ•£é›†åˆï¼Œä½†å¯¹äºè¿ç»­æ§åˆ¶é—®é¢˜æ¥è¯´ï¼ŒåŠ¨ä½œå¯ä»¥è¡¨ç¤ºè¿ç»­çš„å˜é‡ï¼Œä¾‹å¦‚æ–¹å‘ç›˜çš„è§’åº¦ã€æ‰§è¡Œå™¨ä¸Šçš„åŠ›ï¼Œæˆ–è€…åŠ çƒ­å™¨çš„æ¸©åº¦ï¼Œè¿™åœ¨æ­¤ç±»é—®é¢˜ä¸­å¹¶ä¸æˆç«‹ã€‚è¿™ä¸ªé—®é¢˜æ¯”ç¬¬ä¸€ä¸ªé—®é¢˜æ›´å…·æŒ‘æˆ˜æ€§ï¼Œæˆ‘ä»¬å°†åœ¨ä¹¦çš„æœ€åéƒ¨åˆ†ï¼Œä¸“é—¨è®¨è®ºè¿ç»­åŠ¨ä½œç©ºé—´é—®é¢˜çš„ç« èŠ‚ä¸­è¿›è¡Œè®²è§£ã€‚ç°åœ¨ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç¦»æ•£çš„åŠ¨ä½œè®¡æ•°ï¼Œå¹¶ä¸”è¿™ä¸ªè®¡æ•°ä¸æ˜¯å¾ˆå¤§ï¼ˆå³æ•°é‡çº§ä¸º 10 çš„æ•°é‡ï¼‰ã€‚æˆ‘ä»¬åº”è¯¥å¦‚ä½•å¤„ç†çŠ¶æ€ç©ºé—´å¤§å°çš„é—®é¢˜å‘¢ï¼Ÿ

# è¡¨æ ¼ Q å­¦ä¹ 

å¤„ç†çŠ¶æ€ç©ºé—´é—®é¢˜æ—¶éœ€è¦å…³æ³¨çš„å…³é”®é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬æ˜¯å¦çœŸçš„éœ€è¦éå†çŠ¶æ€ç©ºé—´ä¸­çš„æ¯ä¸ªçŠ¶æ€ï¼Ÿæˆ‘ä»¬æœ‰ä¸€ä¸ªç¯å¢ƒï¼Œå¯ä»¥ç”¨ä½œç°å®ç”Ÿæ´»ä¸­çŠ¶æ€æ ·æœ¬çš„æ¥æºã€‚å¦‚æœæŸä¸ªçŠ¶æ€æ²¡æœ‰è¢«ç¯å¢ƒå±•ç¤ºå‡ºæ¥ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬è¿˜éœ€è¦å…³å¿ƒå®ƒçš„ä»·å€¼å‘¢ï¼Ÿæˆ‘ä»¬åªèƒ½ä½¿ç”¨ä»ç¯å¢ƒä¸­è·å¾—çš„çŠ¶æ€æ¥æ›´æ–°çŠ¶æ€å€¼ï¼Œè¿™æ ·å¯ä»¥èŠ‚çœå¤§é‡çš„å·¥ä½œã€‚

å¦‚å‰æ‰€è¿°ï¼Œè¿™ç§å€¼è¿­ä»£æ–¹æ³•çš„ä¿®æ”¹è¢«ç§°ä¸º Q å­¦ä¹ ï¼Œå¯¹äºå…·æœ‰æ˜ç¡®çŠ¶æ€åˆ°å€¼æ˜ å°„çš„æƒ…å†µï¼Œå®ƒåŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š

1.  ä»ä¸€ä¸ªç©ºè¡¨å¼€å§‹ï¼Œå°†çŠ¶æ€æ˜ å°„åˆ°åŠ¨ä½œçš„å€¼ã€‚

1.  é€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œè·å–å…ƒç»„ s, a, r, sâ€²ï¼ˆçŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±å’Œæ–°çŠ¶æ€ï¼‰ã€‚åœ¨æ­¤æ­¥éª¤ä¸­ï¼Œä½ éœ€è¦å†³å®šé‡‡å–å“ªä¸ªåŠ¨ä½œï¼Œè€Œä¸”æ²¡æœ‰å•ä¸€çš„æ­£ç¡®æ–¹æ³•æ¥åšå‡ºè¿™ä¸ªå†³ç­–ã€‚æˆ‘ä»¬åœ¨ç¬¬ä¸€ç« ä¸­è®¨è®ºè¿‡è¿™ä¸ªé—®é¢˜ï¼Œæ¢ç´¢ä¸åˆ©ç”¨ï¼Œå¹¶å°†åœ¨æœ¬ç« ä¸­æ·±å…¥è®¨è®ºã€‚

1.  ä½¿ç”¨ Bellman è¿‘ä¼¼æ›´æ–° Q(s,a) çš„å€¼ï¼š

    ![Ï€ (a |s) = P[At = a|St = s] ](img/eq18.png)

1.  ä»ç¬¬ 2 æ­¥å¼€å§‹é‡å¤ã€‚

ä¸å€¼è¿­ä»£ä¸€æ ·ï¼Œç»“æŸæ¡ä»¶å¯ä»¥æ˜¯æŸä¸ªæ›´æ–°çš„é˜ˆå€¼ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥æ‰§è¡Œæµ‹è¯•å›åˆæ¥ä¼°ç®—ç­–ç•¥çš„æœŸæœ›å¥–åŠ±ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„å¦ä¸€ç‚¹æ˜¯å¦‚ä½•æ›´æ–° Q å€¼ã€‚å½“æˆ‘ä»¬ä»ç¯å¢ƒä¸­é‡‡æ ·æ—¶ï¼Œç›´æ¥åœ¨ç°æœ‰å€¼ä¸Šåˆ†é…æ–°å€¼é€šå¸¸æ˜¯ä¸€ä¸ªåä¸»æ„ï¼Œå› ä¸ºè®­ç»ƒå¯èƒ½ä¼šå˜å¾—ä¸ç¨³å®šã€‚

å®è·µä¸­é€šå¸¸é‡‡ç”¨çš„æ–¹æ³•æ˜¯ä½¿ç”¨ä¸€ç§â€œæ··åˆâ€æŠ€æœ¯æ›´æ–° Q(s,a)ï¼Œå³é€šè¿‡å­¦ä¹ ç‡ Î± å¯¹ Q çš„æ—§å€¼å’Œæ–°å€¼è¿›è¡Œå¹³å‡ï¼ŒÎ± çš„å€¼åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼š

![Ï€ (a |s) = P[At = a|St = s] ](img/eq19.png)

è¿™ä½¿å¾— Q å€¼èƒ½å¤Ÿå¹³ç¨³æ”¶æ•›ï¼Œå³ä½¿æˆ‘ä»¬çš„ç¯å¢ƒæ˜¯å˜ˆæ‚çš„ã€‚æœ€ç»ˆç‰ˆæœ¬çš„ç®—æ³•å¦‚ä¸‹ï¼š

1.  ä»ä¸€ä¸ªç©ºè¡¨å¼€å§‹ï¼Œè¡¨ç¤º Q(s,a)ã€‚

1.  ä»ç¯å¢ƒä¸­è·å– (s, a, r, sâ€²)ã€‚

1.  è¿›è¡Œ Bellman æ›´æ–°ï¼š

    ![Ï€ (a |s) = P[At = a|St = s] ](img/eq19.png)

1.  æ£€æŸ¥æ”¶æ•›æ¡ä»¶ã€‚å¦‚æœæ¡ä»¶æœªæ»¡è¶³ï¼Œä»ç¬¬ 2 æ­¥å¼€å§‹é‡å¤ã€‚

å¦‚å‰æ‰€è¿°ï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä¸ºè¡¨æ ¼ Q å­¦ä¹ ï¼Œå› ä¸ºæˆ‘ä»¬ç»´æŠ¤ä¸€ä¸ªåŒ…å«çŠ¶æ€åŠå…¶ Q å€¼çš„è¡¨æ ¼ã€‚è®©æˆ‘ä»¬åœ¨æˆ‘ä»¬çš„ FrozenLake ç¯å¢ƒä¸­å°è¯•ä¸€ä¸‹ã€‚å®Œæ•´çš„ç¤ºä¾‹ä»£ç åœ¨ Chapter06/01_frozenlake_q_learning.py ä¸­ï¼š

é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥åŒ…å¹¶å®šä¹‰å¸¸é‡å’Œä½¿ç”¨çš„ç±»å‹ï¼š

```py
import typing as tt 
import gymnasium as gym 
from collections import defaultdict 
from torch.utils.tensorboard.writer import SummaryWriter 

ENV_NAME = "FrozenLake-v1" 
GAMMA = 0.9 
ALPHA = 0.2 
TEST_EPISODES = 20 

State = int 
Action = int 
ValuesKey = tt.Tuple[State, Action] 

class Agent: 
    def __init__(self): 
        self.env = gym.make(ENV_NAME) 
        self.state, _ = self.env.reset() 
        self.values: tt.Dict[ValuesKey] = defaultdict(float)
```

è¿™é‡Œçš„æ–°å†…å®¹æ˜¯ Î± çš„å€¼ï¼Œå®ƒå°†ç”¨ä½œå€¼æ›´æ–°ä¸­çš„å­¦ä¹ ç‡ã€‚æˆ‘ä»¬ç°åœ¨çš„ Agent ç±»åˆå§‹åŒ–æ›´åŠ ç®€åŒ–ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†éœ€è¦è·Ÿè¸ªå¥–åŠ±å’Œè¿‡æ¸¡è®¡æ•°çš„å†å²è®°å½•ï¼Œåªéœ€è¦æˆ‘ä»¬çš„å€¼è¡¨ã€‚è¿™å°†ä½¿æˆ‘ä»¬çš„å†…å­˜å ç”¨æ›´å°ï¼Œè™½ç„¶å¯¹äº FrozenLake æ¥è¯´è¿™ä¸æ˜¯ä¸€ä¸ªå¤§é—®é¢˜ï¼Œä½†å¯¹äºæ›´å¤§çš„ç¯å¢ƒå¯èƒ½è‡³å…³é‡è¦ã€‚

æ–¹æ³• sample_env ç”¨äºä»ç¯å¢ƒä¸­è·å–ä¸‹ä¸€ä¸ªè¿‡æ¸¡ï¼š

```py
 def sample_env(self) -> tt.Tuple[State, Action, float, State]: 
        action = self.env.action_space.sample() 
        old_state = self.state 
        new_state, reward, is_done, is_tr, _ = self.env.step(action) 
        if is_done or is_tr: 
            self.state, _ = self.env.reset() 
        else: 
            self.state = new_state 
        return old_state, action, float(reward), new_state
```

æˆ‘ä»¬ä»åŠ¨ä½œç©ºé—´ä¸­éšæœºé‡‡æ ·ä¸€ä¸ªåŠ¨ä½œï¼Œå¹¶è¿”å›åŒ…å«æ—§çŠ¶æ€ã€æ‰€é‡‡å–çš„åŠ¨ä½œã€è·å¾—çš„å¥–åŠ±å’Œæ–°çŠ¶æ€çš„å…ƒç»„ã€‚è¯¥å…ƒç»„å°†åœ¨åç»­çš„è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨ã€‚

ä¸‹ä¸€ä¸ªæ–¹æ³•æ¥æ”¶ç¯å¢ƒçš„çŠ¶æ€ï¼š

```py
 def best_value_and_action(self, state: State) -> tt.Tuple[float, Action]: 
        best_value, best_action = None, None 
        for action in range(self.env.action_space.n): 
            action_value = self.values[(state, action)] 
            if best_value is None or best_value < action_value: 
                best_value = action_value 
                best_action = action 
        return best_value, best_action
```

æ­¤æ–¹æ³•é€šè¿‡é€‰æ‹©åœ¨è¡¨æ ¼ä¸­å…·æœ‰æœ€å¤§å€¼çš„åŠ¨ä½œæ¥ä»ç»™å®šçš„ç¯å¢ƒçŠ¶æ€ä¸­æ‰¾åˆ°æœ€ä½³åŠ¨ä½œã€‚å¦‚æœæˆ‘ä»¬æ²¡æœ‰ä¸çŠ¶æ€å’ŒåŠ¨ä½œå¯¹ç›¸å…³çš„å€¼ï¼Œåˆ™å°†å…¶è§†ä¸ºé›¶ã€‚è¯¥æ–¹æ³•å°†è¢«ä½¿ç”¨ä¸¤æ¬¡ï¼šç¬¬ä¸€æ¬¡ï¼Œåœ¨æµ‹è¯•æ–¹æ³•ä¸­ï¼Œå®ƒä½¿ç”¨æˆ‘ä»¬å½“å‰çš„å€¼è¡¨è¿›è¡Œä¸€æ¬¡å›åˆï¼ˆä»¥è¯„ä¼°æˆ‘ä»¬çš„ç­–ç•¥è´¨é‡ï¼‰ï¼›ç¬¬äºŒæ¬¡ï¼Œåœ¨æ‰§è¡Œå€¼æ›´æ–°çš„æ–¹æ³•ä¸­ï¼Œç”¨äºè·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å€¼ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¯å¢ƒä¸­çš„ä¸€æ­¥æ“ä½œæ¥æ›´æ–°æˆ‘ä»¬çš„å€¼è¡¨ï¼š

```py
 def value_update(self, state: State, action: Action, reward: float, next_state: State): 
        best_val, _ = self.best_value_and_action(next_state) 
        new_val = reward + GAMMA * best_val 
        old_val = self.values[(state, action)] 
        key = (state, action) 
        self.values[key] = old_val * (1-ALPHA) + new_val * ALPHA
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡å°†å³æ—¶å¥–åŠ±ä¸ä¸‹ä¸€ä¸ªçŠ¶æ€çš„æŠ˜æ‰£å€¼ç›¸åŠ æ¥è®¡ç®—æˆ‘ä»¬å½“å‰çŠ¶æ€ s å’ŒåŠ¨ä½œ a çš„è´å°”æ›¼è¿‘ä¼¼ã€‚ç„¶åï¼Œæˆ‘ä»¬è·å¾—çŠ¶æ€å’ŒåŠ¨ä½œå¯¹çš„å…ˆå‰å€¼ï¼Œå¹¶ä½¿ç”¨å­¦ä¹ ç‡å°†è¿™äº›å€¼æ··åˆåœ¨ä¸€èµ·ã€‚ç»“æœæ˜¯çŠ¶æ€ s å’ŒåŠ¨ä½œ a çš„æ–°è¿‘ä¼¼å€¼ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨æˆ‘ä»¬çš„è¡¨æ ¼ä¸­ã€‚

æˆ‘ä»¬çš„ Agent ç±»ä¸­çš„æœ€åä¸€ä¸ªæ–¹æ³•ä½¿ç”¨æä¾›çš„æµ‹è¯•ç¯å¢ƒè¿›è¡Œä¸€æ¬¡å®Œæ•´çš„å›åˆï¼š

```py
 def play_episode(self, env: gym.Env) -> float: 
        total_reward = 0.0 
        state, _ = env.reset() 
        while True: 
            _, action = self.best_value_and_action(state) 
            new_state, reward, is_done, is_tr, _ = env.step(action) 
            total_reward += reward 
            if is_done or is_tr: 
                break 
            state = new_state 
        return total_reward
```

æ¯ä¸€æ­¥çš„åŠ¨ä½œéƒ½æ˜¯ä½¿ç”¨æˆ‘ä»¬å½“å‰çš„ Q å€¼è¡¨æ¥é€‰æ‹©çš„ã€‚è¯¥æ–¹æ³•ç”¨äºè¯„ä¼°æˆ‘ä»¬å½“å‰çš„ç­–ç•¥ï¼Œä»¥æ£€æŸ¥å­¦ä¹ çš„è¿›å±•ã€‚è¯·æ³¨æ„ï¼Œæ­¤æ–¹æ³•ä¸ä¼šæ›´æ”¹æˆ‘ä»¬çš„å€¼è¡¨ï¼Œå®ƒä»…ä»…ä½¿ç”¨å€¼è¡¨æ¥æ‰¾åˆ°æœ€ä½³åŠ¨ä½œã€‚

ç¤ºä¾‹çš„å…¶ä½™éƒ¨åˆ†æ˜¯è®­ç»ƒå¾ªç¯ï¼Œç±»ä¼¼äºç¬¬äº”ç« ä¸­çš„ç¤ºä¾‹ï¼šæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæµ‹è¯•ç¯å¢ƒã€ä»£ç†å’Œæ‘˜è¦å†™å…¥å™¨ï¼Œç„¶ååœ¨å¾ªç¯ä¸­ï¼Œæˆ‘ä»¬åœ¨ç¯å¢ƒä¸­è¿›è¡Œä¸€æ­¥æ“ä½œï¼Œå¹¶ä½¿ç”¨è·å¾—çš„æ•°æ®æ‰§è¡Œå€¼æ›´æ–°ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡è¿›è¡Œå¤šä¸ªæµ‹è¯•å›åˆæ¥æµ‹è¯•æˆ‘ä»¬å½“å‰çš„ç­–ç•¥ã€‚å¦‚æœè·å¾—äº†è‰¯å¥½çš„å¥–åŠ±ï¼Œåˆ™åœæ­¢è®­ç»ƒï¼š

```py
if __name__ == "__main__": 
    test_env = gym.make(ENV_NAME) 
    agent = Agent() 
    writer = SummaryWriter(comment="-q-learning") 

    iter_no = 0 
    best_reward = 0.0 
    while True: 
        iter_no += 1 
        state, action, reward, next_state = agent.sample_env() 
        agent.value_update(state, action, reward, next_state) 

        test_reward = 0.0 
        for _ in range(TEST_EPISODES): 
            test_reward += agent.play_episode(test_env) 
        test_reward /= TEST_EPISODES 
        writer.add_scalar("reward", test_reward, iter_no) 
        if test_reward > best_reward: 
            print("%d: Best test reward updated %.3f -> %.3f" % (iter_no, best_reward, test_reward)) 
            best_reward = test_reward 
        if test_reward > 0.80: 
            print("Solved in %d iterations!" % iter_no) 
            break 
    writer.close()
```

ç¤ºä¾‹çš„ç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

```py
Chapter06$ ./01_frozenlake_q_learning.py 
1149: Best test reward updated 0.000 -> 0.500 
1150: Best test reward updated 0.500 -> 0.550 
1164: Best test reward updated 0.550 -> 0.600 
1242: Best test reward updated 0.600 -> 0.650 
2685: Best test reward updated 0.650 -> 0.700 
2988: Best test reward updated 0.700 -> 0.750 
3025: Best test reward updated 0.750 -> 0.850 
Solved in 3025 iterations!
```

ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œä¸ä¸Šä¸€ç« çš„å€¼è¿­ä»£æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ä¸ªç‰ˆæœ¬ä½¿ç”¨äº†æ›´å¤šçš„è¿­ä»£ï¼ˆä½†ä½ çš„å®éªŒå¯èƒ½æœ‰ä¸åŒçš„æ­¥éª¤æ•°ï¼‰æ¥è§£å†³é—®é¢˜ã€‚åŸå› åœ¨äºæˆ‘ä»¬ä¸å†ä½¿ç”¨åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­è·å¾—çš„ç»éªŒã€‚åœ¨ç¤ºä¾‹ Chapter05/02_frozenlake_q_iteration.py ä¸­ï¼Œå‘¨æœŸæ€§æµ‹è¯•å¯¼è‡´ Q è¡¨ç»Ÿè®¡çš„æ›´æ–°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ä¸è§¦åŠ Q å€¼ï¼Œè¿™å¯¼è‡´åœ¨ç¯å¢ƒè§£å†³ä¹‹å‰éœ€è¦æ›´å¤šçš„è¿­ä»£ã€‚

æ€»ä½“è€Œè¨€ï¼Œä»ç¯å¢ƒä¸­æ‰€éœ€çš„æ ·æœ¬æ€»æ•°å‡ ä¹ç›¸åŒã€‚TensorBoard ä¸­çš„å¥–åŠ±å›¾ä¹Ÿæ˜¾ç¤ºäº†è‰¯å¥½çš„è®­ç»ƒåŠ¨æ€ï¼Œè¿™ä¸å€¼è¿­ä»£æ–¹æ³•éå¸¸ç›¸ä¼¼ï¼ˆå€¼è¿­ä»£çš„å¥–åŠ±å›¾å¦‚å›¾ 5.9 æ‰€ç¤ºï¼‰ï¼š

![PIC](img/B22150_06_01.png)

å›¾ 6.1ï¼šFrozenLake çš„å¥–åŠ±åŠ¨æ€

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ‰©å±• Q-learning æ–¹æ³•ï¼Œç»“åˆç¥ç»ç½‘ç»œï¼ˆNNsï¼‰å¯¹ç¯å¢ƒçŠ¶æ€çš„é¢„å¤„ç†ã€‚è¿™å°†æå¤§åœ°æ‰©å±•æˆ‘ä»¬è®¨è®ºè¿‡çš„æ–¹æ³•çš„çµæ´»æ€§å’Œé€‚ç”¨æ€§ã€‚

# æ·±åº¦ Q å­¦ä¹ 

æˆ‘ä»¬åˆšåˆšä»‹ç»çš„ Q-learning æ–¹æ³•è§£å†³äº†éå†æ‰€æœ‰çŠ¶æ€é›†çš„é—®é¢˜ï¼Œä½†å½“å¯è§‚å¯ŸçŠ¶æ€é›†çš„æ•°é‡éå¸¸å¤§æ—¶ï¼Œå®ƒä»ç„¶å¯èƒ½é‡åˆ°å›°éš¾ã€‚ä¾‹å¦‚ï¼ŒAtari æ¸¸æˆå¯èƒ½æœ‰è®¸å¤šä¸åŒçš„å±å¹•ï¼Œå¦‚æœæˆ‘ä»¬å†³å®šå°†åŸå§‹åƒç´ ä½œä¸ºå•ç‹¬çš„çŠ¶æ€ï¼Œæˆ‘ä»¬å¾ˆå¿«å°±ä¼šæ„è¯†åˆ°æˆ‘ä»¬æœ‰å¤ªå¤šçš„çŠ¶æ€éœ€è¦è¿½è¸ªå’Œä¼°ç®—å€¼ã€‚

åœ¨æŸäº›ç¯å¢ƒä¸­ï¼Œä¸åŒçš„å¯è§‚å¯ŸçŠ¶æ€çš„æ•°é‡å‡ ä¹æ˜¯æ— é™çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨ CartPole ä¸­ï¼Œç¯å¢ƒç»™æˆ‘ä»¬æä¾›çš„çŠ¶æ€æ˜¯å››ä¸ªæµ®åŠ¨ç‚¹æ•°ã€‚æ•°å€¼ç»„åˆçš„æ•°é‡æ˜¯æœ‰é™çš„ï¼ˆå®ƒä»¬ä»¥æ¯”ç‰¹è¡¨ç¤ºï¼‰ï¼Œä½†è¿™ä¸ªæ•°å­—æå…¶åºå¤§ã€‚ä»…ç”¨æ¯”ç‰¹å€¼è¡¨ç¤ºæ—¶ï¼Œçº¦ä¸º 2^(4â‹…32) â‰ˆ 3.4 â‹… 10Â³â¸ã€‚å®é™…ä¸Šï¼Œè¿™ä¸ªå€¼ä¼šå°ä¸€äº›ï¼Œå› ä¸ºç¯å¢ƒçŠ¶æ€çš„å€¼æ˜¯æœ‰é™åˆ¶çš„ï¼Œå¹¶éæ‰€æœ‰çš„ 4 ä¸ª float32 å€¼çš„æ¯”ç‰¹ç»„åˆéƒ½æ˜¯å¯èƒ½çš„ï¼Œä½†ç»“æœçš„çŠ¶æ€ç©ºé—´ä¾ç„¶å¤ªå¤§ã€‚æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€äº›ç®±å­æ¥ç¦»æ•£åŒ–è¿™äº›å€¼ï¼Œä½†è¿™é€šå¸¸ä¼šå¸¦æ¥æ¯”è§£å†³æ›´å¤šé—®é¢˜ï¼›æˆ‘ä»¬éœ€è¦å†³å®šå“ªäº›å‚æ•°èŒƒå›´é‡è¦ï¼Œéœ€è¦åŒºåˆ†æˆä¸åŒçš„çŠ¶æ€ï¼Œè€Œå“ªäº›èŒƒå›´å¯ä»¥å½’ç±»åœ¨ä¸€èµ·ã€‚ç”±äºæˆ‘ä»¬å°è¯•ä»¥ä¸€èˆ¬çš„æ–¹å¼å®ç° RL æ–¹æ³•ï¼ˆè€Œä¸æ·±å…¥äº†è§£ç¯å¢ƒçš„å†…éƒ¨ç»“æ„ï¼‰ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå¾ˆæœ‰å‰æ™¯çš„æ–¹å‘ã€‚

åœ¨ Atari æ¸¸æˆçš„æƒ…å†µä¸‹ï¼Œå•ä¸€åƒç´ çš„å˜åŒ–å¹¶ä¸ä¼šé€ æˆå¤ªå¤§å·®å¼‚ï¼Œå› æ­¤æˆ‘ä»¬å¯èƒ½å¸Œæœ›å°†ç›¸ä¼¼çš„å›¾åƒè§†ä¸ºä¸€ä¸ªçŠ¶æ€ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦åŒºåˆ†ä¸€äº›çŠ¶æ€ã€‚

ä¸‹å›¾å±•ç¤ºäº† Pong æ¸¸æˆä¸­çš„ä¸¤ç§ä¸åŒæƒ…å†µã€‚æˆ‘ä»¬æ­£åœ¨ä¸äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰å¯¹æ‰‹å¯¹æˆ˜ï¼Œé€šè¿‡æ§åˆ¶ä¸€ä¸ªæŒ¡æ¿ï¼ˆæˆ‘ä»¬çš„æŒ¡æ¿åœ¨å³ä¾§ï¼Œå¯¹æ‰‹çš„æŒ¡æ¿åœ¨å·¦ä¾§ï¼‰ã€‚æ¸¸æˆçš„ç›®æ ‡æ˜¯å°†å¼¹è·³çƒé€è¿‡å¯¹æ‰‹çš„æŒ¡æ¿ï¼ŒåŒæ—¶é˜²æ­¢çƒä»æˆ‘ä»¬çš„æŒ¡æ¿æ—è¾¹é£è¿‡ã€‚æˆ‘ä»¬å¯ä»¥è®¤ä¸ºè¿™ä¸¤ç§æƒ…å†µæ˜¯å®Œå…¨ä¸åŒçš„ã€‚åœ¨å³ä¾§å±•ç¤ºçš„æƒ…å†µä¸­ï¼Œçƒé è¿‘å¯¹æ‰‹ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ”¾æ¾å¹¶è§‚å¯Ÿã€‚ç„¶è€Œï¼Œå·¦ä¾§çš„æƒ…å†µè¦æ±‚æ›´é«˜ï¼›å‡è®¾çƒä»å·¦å‘å³ç§»åŠ¨ï¼Œçƒæ­£æœæˆ‘ä»¬çš„æŒ¡æ¿ç§»åŠ¨ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è¿…é€Ÿç§»åŠ¨æˆ‘ä»¬çš„æŒ¡æ¿ï¼Œä»¥é¿å…å¤±åˆ†ã€‚å›¾ 6.2 ä¸­çš„ä¸¤ç§æƒ…å†µåªæ˜¯ 10â·â°â¸â°Â²ç§å¯èƒ½æƒ…å†µä¸­çš„ä¸¤ç§ï¼Œä½†æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ™ºèƒ½ä½“èƒ½å¯¹è¿™äº›æƒ…å†µåšå‡ºä¸åŒçš„ååº”ã€‚

![å›¾ç‰‡](img/file32.png)

å›¾ 6.2ï¼šPong ä¸­è§‚å¯Ÿçš„æ¨¡ç³Šæ€§ã€‚åœ¨å·¦ä¾§çš„å›¾åƒä¸­ï¼Œçƒæ­£å‘å³ç§»åŠ¨ï¼Œæœç€æˆ‘ä»¬çš„æŒ¡æ¿ï¼Œè€Œåœ¨å³ä¾§ï¼Œå®ƒçš„æ–¹å‘ç›¸åã€‚

ä½œä¸ºè¯¥é—®é¢˜çš„è§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ä¸ªéçº¿æ€§è¡¨ç¤ºï¼Œå°†çŠ¶æ€å’ŒåŠ¨ä½œæ˜ å°„åˆ°ä¸€ä¸ªå€¼ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œè¿™ç§°ä¸ºâ€œå›å½’é—®é¢˜â€ã€‚è¡¨ç¤ºå’Œè®­ç»ƒè¿™ç§è¡¨ç¤ºçš„å…·ä½“æ–¹æ³•å¯ä»¥æœ‰æ‰€ä¸åŒï¼Œä½†æ­£å¦‚ä½ ä»æœ¬èŠ‚æ ‡é¢˜ä¸­å·²ç»çŒœåˆ°çš„é‚£æ ·ï¼Œä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆNNï¼‰æ˜¯æœ€æµè¡Œçš„é€‰æ‹©ä¹‹ä¸€ï¼Œå°¤å…¶æ˜¯å½“å¤„ç†ä»¥å±å¹•å›¾åƒè¡¨ç¤ºçš„è§‚å¯Ÿæ—¶ã€‚è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯¹ Q-learning ç®—æ³•è¿›è¡Œä¿®æ”¹ï¼š

1.  ä½¿ç”¨ä¸€äº›åˆå§‹è¿‘ä¼¼å€¼åˆå§‹åŒ– Q(s,a)ã€‚

1.  é€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œè·å¾—å…ƒç»„(s, a, r, sâ€²)ã€‚

1.  è®¡ç®—æŸå¤±ï¼š

    ![Ï€ (a |s) = P[At = a|St = s] ](img/eq20.png)![Ï€ (a |s) = P[At = a|St = s] ](img/eq21.png)

1.  é€šè¿‡ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ç®—æ³•æ›´æ–° Q(s,a)ï¼Œé€šè¿‡æœ€å°åŒ–å…³äºæ¨¡å‹å‚æ•°çš„æŸå¤±æ¥è¿›è¡Œæ›´æ–°ã€‚

1.  ä»æ­¥éª¤ 2 å¼€å§‹é‡å¤ï¼Œç›´åˆ°æ”¶æ•›ã€‚

è¿™ä¸ªç®—æ³•çœ‹èµ·æ¥å¾ˆç®€å•ï¼Œä½†ä¸å¹¸çš„æ˜¯ï¼Œå®ƒçš„æ•ˆæœå¹¶ä¸å¥½ã€‚è®©æˆ‘ä»¬è®¨è®ºä¸€äº›å¯èƒ½å‡ºé”™çš„æ–¹é¢ï¼Œä»¥åŠæˆ‘ä»¬å¯ä»¥å¦‚ä½•å¤„ç†è¿™äº›æƒ…å†µã€‚

## ä¸ç¯å¢ƒäº¤äº’

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä»¥æŸç§æ–¹å¼ä¸ç¯å¢ƒäº¤äº’ï¼Œä»¥æ¥æ”¶æ•°æ®è¿›è¡Œè®­ç»ƒã€‚åœ¨ç®€å•çš„ç¯å¢ƒä¸­ï¼Œæ¯”å¦‚ FrozenLakeï¼Œæˆ‘ä»¬å¯ä»¥éšæœºè¡ŒåŠ¨ï¼Œä½†è¿™çœŸçš„æ˜¯æœ€å¥½çš„ç­–ç•¥å—ï¼Ÿæƒ³è±¡ä¸€ä¸‹ Pong æ¸¸æˆã€‚é€šè¿‡éšæœºç§»åŠ¨æŒ¡æ¿ï¼Œè·å¾—ä¸€ä¸ªå•ç‹¬å¾—åˆ†çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿè¿™ä¸ªæ¦‚ç‡ä¸æ˜¯é›¶ï¼Œä½†å®ƒæå…¶å°ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬éœ€è¦ç­‰å¾…å¾ˆé•¿æ—¶é—´ï¼Œæ‰èƒ½é‡åˆ°è¿™ç§ç½•è§çš„æƒ…å†µã€‚ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Q å‡½æ•°çš„è¿‘ä¼¼å€¼ä½œä¸ºè¡Œä¸ºçš„æ¥æºï¼ˆå°±åƒæˆ‘ä»¬åœ¨ä»·å€¼è¿­ä»£æ–¹æ³•ä¸­åšçš„é‚£æ ·ï¼Œå½“æ—¶æˆ‘ä»¬åœ¨æµ‹è¯•æœŸé—´è®°ä½äº†è‡ªå·±çš„ç»éªŒï¼‰ã€‚

å¦‚æœæˆ‘ä»¬çš„ Q è¡¨ç¤ºæ˜¯å¥½çš„ï¼Œé‚£ä¹ˆä»ç¯å¢ƒä¸­è·å¾—çš„ç»éªŒå°†å‘ä»£ç†æä¾›ç›¸å…³çš„æ•°æ®ç”¨äºè®­ç»ƒã€‚ç„¶è€Œï¼Œå½“æˆ‘ä»¬çš„è¿‘ä¼¼ä¸å®Œç¾æ—¶ï¼ˆä¾‹å¦‚åœ¨è®­ç»ƒçš„åˆæœŸï¼‰ï¼Œæˆ‘ä»¬å°±ä¼šé‡åˆ°é—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ä»£ç†å¯èƒ½ä¼šåœ¨æŸäº›çŠ¶æ€ä¸‹ä¸€ç›´é‡‡å–é”™è¯¯çš„è¡Œä¸ºï¼Œè€Œä»æœªå°è¯•è¿‡ä¸åŒçš„è¡Œä¸ºã€‚è¿™å°±æ˜¯åœ¨ç¬¬ä¸€ç« ä¸­ç®€è¦æåˆ°çš„æ¢ç´¢ä¸åˆ©ç”¨å›°å¢ƒï¼Œæˆ‘ä»¬å°†åœ¨æ­¤è¯¦ç»†è®¨è®ºã€‚ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬çš„ä»£ç†éœ€è¦æ¢ç´¢ç¯å¢ƒï¼Œä»¥å»ºç«‹å®Œæ•´çš„è½¬ç§»å’ŒåŠ¨ä½œç»“æœçš„å›¾æ™¯ã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬åº”å½“é«˜æ•ˆåœ°åˆ©ç”¨ä¸ç¯å¢ƒçš„äº¤äº’ï¼›æˆ‘ä»¬ä¸åº”æµªè´¹æ—¶é—´éšæœºå°è¯•æˆ‘ä»¬å·²ç»å°è¯•è¿‡å¹¶ä¸”å·²çŸ¥ç»“æœçš„åŠ¨ä½œã€‚

å¦‚ä½ æ‰€è§ï¼Œåœ¨è®­ç»ƒåˆæœŸï¼Œå½“æˆ‘ä»¬çš„ Q è¿‘ä¼¼å€¼è¾ƒå·®æ—¶ï¼Œéšæœºè¡Œä¸ºåè€Œæ›´å¥½ï¼Œå› ä¸ºå®ƒèƒ½æä¾›æ›´å¤šå‡åŒ€åˆ†å¸ƒçš„ç¯å¢ƒçŠ¶æ€ä¿¡æ¯ã€‚éšç€è®­ç»ƒçš„è¿›å±•ï¼Œéšæœºè¡Œä¸ºå˜å¾—ä½æ•ˆï¼Œæˆ‘ä»¬å¸Œæœ›å›å½’åˆ° Q è¿‘ä¼¼å€¼ä¸Šï¼Œä»¥å†³å®šå¦‚ä½•è¡ŒåŠ¨ã€‚

æ‰§è¡Œè¿™ç§ä¸¤ç§æç«¯è¡Œä¸ºæ··åˆçš„æ–¹æ³•è¢«ç§°ä¸º epsilon-è´ªå©ªæ–¹æ³•ï¼Œæ„æ€æ˜¯ä½¿ç”¨æ¦‚ç‡è¶…å‚æ•°ğœ–åœ¨éšæœºè¡Œä¸ºå’Œ Q ç­–ç•¥ä¹‹é—´åˆ‡æ¢ã€‚é€šè¿‡æ”¹å˜ğœ–çš„å€¼ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©éšæœºè¡Œä¸ºçš„æ¯”ä¾‹ã€‚é€šå¸¸çš„åšæ³•æ˜¯ä»ğœ– = 1.0ï¼ˆ100%çš„éšæœºè¡Œä¸ºï¼‰å¼€å§‹ï¼Œå¹¶é€æ¸å°†å…¶å‡å°‘åˆ°ä¸€ä¸ªè¾ƒå°çš„å€¼ï¼Œå¦‚ 5%æˆ– 2%çš„éšæœºè¡Œä¸ºã€‚ä½¿ç”¨ epsilon-è´ªå©ªæ–¹æ³•å¯ä»¥å¸®åŠ©æˆ‘ä»¬åœ¨è®­ç»ƒåˆæœŸæ¢ç´¢ç¯å¢ƒï¼Œå¹¶åœ¨è®­ç»ƒç»“æŸæ—¶åšæŒå¥½çš„ç­–ç•¥ã€‚è¿˜æœ‰å…¶ä»–è§£å†³æ¢ç´¢ä¸åˆ©ç”¨é—®é¢˜çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬ä¹¦çš„ç¬¬ä¸‰éƒ¨åˆ†è®¨è®ºå…¶ä¸­çš„ä¸€äº›ã€‚è¿™ä¸ªé—®é¢˜æ˜¯å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­çš„ä¸€ä¸ªåŸºç¡€æ€§æœªè§£é—®é¢˜ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªä»åœ¨ç§¯æç ”ç©¶çš„é¢†åŸŸï¼Œç¦»å®Œå…¨è§£å†³è¿˜è¿œã€‚

## SGD ä¼˜åŒ–

æˆ‘ä»¬çš„ Q å­¦ä¹ è¿‡ç¨‹çš„æ ¸å¿ƒå€Ÿé‰´äº†ç›‘ç£å­¦ä¹ ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬æ­£è¯•å›¾ç”¨ç¥ç»ç½‘ç»œï¼ˆNNï¼‰æ¥é€¼è¿‘ä¸€ä¸ªå¤æ‚çš„éçº¿æ€§å‡½æ•° Q(s,a)ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨è´å°”æ›¼æ–¹ç¨‹è®¡ç®—è¯¥å‡½æ•°çš„ç›®æ ‡å€¼ï¼Œç„¶åå‡è£…æˆ‘ä»¬é¢ä¸´çš„æ˜¯ä¸€ä¸ªç›‘ç£å­¦ä¹ é—®é¢˜ã€‚è¿™æ˜¯å¯ä»¥çš„ï¼Œä½† SGD ä¼˜åŒ–çš„ä¸€ä¸ªåŸºæœ¬è¦æ±‚æ˜¯è®­ç»ƒæ•°æ®æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼ˆé€šå¸¸ç¼©å†™ä¸º iidï¼‰ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®æ˜¯ä»æˆ‘ä»¬è¯•å›¾å­¦ä¹ çš„åº•å±‚æ•°æ®é›†ä¸­éšæœºæŠ½æ ·çš„ã€‚

åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ç”¨äº SGD æ›´æ–°çš„æ•°æ®ä¸ç¬¦åˆè¿™äº›æ ‡å‡†ï¼š

1.  æˆ‘ä»¬çš„æ ·æœ¬ä¸æ˜¯ç‹¬ç«‹çš„ã€‚å³ä½¿æˆ‘ä»¬ç´¯ç§¯äº†å¤§é‡çš„æ•°æ®æ ·æœ¬ï¼Œå®ƒä»¬ä¹‹é—´ä¹Ÿä¼šéå¸¸æ¥è¿‘ï¼Œå› ä¸ºå®ƒä»¬éƒ½å±äºåŒä¸€ä¸ªå›åˆã€‚

1.  æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®çš„åˆ†å¸ƒå°†ä¸å®Œå…¨ç­‰åŒäºæˆ‘ä»¬æƒ³è¦å­¦ä¹ çš„æœ€ä¼˜ç­–ç•¥æä¾›çš„æ ·æœ¬ã€‚æˆ‘ä»¬æ‹¥æœ‰çš„æ•°æ®å°†æ˜¯å…¶ä»–ç­–ç•¥ï¼ˆæˆ‘ä»¬çš„å½“å‰ç­–ç•¥ã€éšæœºç­–ç•¥ï¼Œæˆ–åœ¨ epsilon-è´ªå©ªæƒ…å†µä¸‹çš„ä¸¤è€…ç»“åˆï¼‰çš„ç»“æœï¼Œä½†æˆ‘ä»¬å¹¶ä¸æƒ³å­¦ä¹ å¦‚ä½•éšæœºåœ°è¡ŒåŠ¨ï¼šæˆ‘ä»¬å¸Œæœ›è·å¾—ä¸€ä¸ªå…·æœ‰æœ€ä½³å¥–åŠ±çš„æœ€ä¼˜ç­–ç•¥ã€‚

ä¸ºäº†åº”å¯¹è¿™ä¸€éš¾é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸éœ€è¦ä½¿ç”¨ä¸€ä¸ªåŒ…å«æˆ‘ä»¬è¿‡å»ç»éªŒçš„å¤§ç¼“å†²åŒºï¼Œå¹¶ä»ä¸­æŠ½å–è®­ç»ƒæ•°æ®ï¼Œè€Œä¸æ˜¯ä»…ä½¿ç”¨æˆ‘ä»¬æœ€æ–°çš„ç»éªŒã€‚è¿™ç§æŠ€æœ¯å«åšå›æ”¾ç¼“å†²åŒºã€‚æœ€ç®€å•çš„å®ç°æ–¹å¼æ˜¯ä¸€ä¸ªå›ºå®šå¤§å°çš„ç¼“å†²åŒºï¼Œæ–°æ•°æ®è¢«æ·»åŠ åˆ°ç¼“å†²åŒºçš„æœ«å°¾ï¼Œä»è€Œå°†æœ€æ—§çš„ç»éªŒæ¨å‡ºç¼“å†²åŒºä¹‹å¤–ã€‚

å›æ”¾ç¼“å†²åŒºå…è®¸æˆ‘ä»¬åœ¨æˆ–å¤šæˆ–å°‘ç‹¬ç«‹çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä½†è¿™äº›æ•°æ®ä»ç„¶è¶³å¤Ÿæ–°é²œï¼Œå¯ä»¥ç”¨äºè®­ç»ƒæˆ‘ä»¬æœ€è¿‘çš„ç­–ç•¥ç”Ÿæˆçš„æ ·æœ¬ã€‚åœ¨ç¬¬å…«ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ£€æŸ¥å¦ä¸€ç§å›æ”¾ç¼“å†²åŒºï¼Œä¼˜å…ˆçº§å›æ”¾ï¼Œå®ƒæä¾›äº†ä¸€ç§æ›´å¤æ‚çš„é‡‡æ ·æ–¹æ³•ã€‚

## æ­¥éª¤ä¹‹é—´çš„ç›¸å…³æ€§

é»˜è®¤è®­ç»ƒè¿‡ç¨‹çš„å¦ä¸€ä¸ªå®é™…é—®é¢˜ä¹Ÿä¸ç¼ºä¹ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆiidï¼‰æ•°æ®æœ‰å…³ï¼Œä½†æ–¹å¼ç¨æœ‰ä¸åŒã€‚è´å°”æ›¼æ–¹ç¨‹é€šè¿‡ Q(sâ€²,aâ€²)ä¸ºæˆ‘ä»¬æä¾› Q(s,a)çš„å€¼ï¼ˆè¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºè‡ªä¸¾ï¼Œå½“æˆ‘ä»¬é€’å½’ä½¿ç”¨è¯¥å…¬å¼æ—¶ï¼‰ã€‚ç„¶è€Œï¼ŒçŠ¶æ€ s å’Œ sâ€²ä¹‹é—´åªæœ‰ä¸€æ­¥ä¹‹é¥ã€‚è¿™ä½¿å¾—å®ƒä»¬éå¸¸ç›¸ä¼¼ï¼Œç¥ç»ç½‘ç»œå¾ˆéš¾åŒºåˆ†å®ƒä»¬ã€‚å½“æˆ‘ä»¬æ›´æ–°ç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œä½¿ Q(s,a)æ›´æ¥è¿‘é¢„æœŸç»“æœæ—¶ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé—´æ¥åœ°æ”¹å˜ Q(sâ€²,aâ€²)å’Œé™„è¿‘å…¶ä»–çŠ¶æ€çš„å€¼ã€‚è¿™å¯èƒ½å¯¼è‡´æˆ‘ä»¬çš„è®­ç»ƒéå¸¸ä¸ç¨³å®šï¼Œå°±åƒåœ¨è¿½é€è‡ªå·±çš„å°¾å·´ï¼›å½“æˆ‘ä»¬æ›´æ–°çŠ¶æ€ s çš„ Q å€¼æ—¶ï¼Œåœ¨éšåçš„çŠ¶æ€ä¸­ï¼Œæˆ‘ä»¬ä¼šå‘ç° Q(sâ€²,aâ€²)å˜å¾—æ›´ç³Ÿï¼Œä½†å°è¯•æ›´æ–°å®ƒå¯èƒ½ä¼šè¿›ä¸€æ­¥ç ´å Q(s,a)çš„è¿‘ä¼¼ï¼Œä¾æ­¤ç±»æ¨ã€‚

ä¸ºäº†ä½¿è®­ç»ƒæ›´åŠ ç¨³å®šï¼Œæœ‰ä¸€ä¸ªæŠ€å·§å«åšç›®æ ‡ç½‘ç»œï¼Œæˆ‘ä»¬ä¿ç•™ä¸€ä»½ç½‘ç»œçš„å‰¯æœ¬ï¼Œå¹¶ç”¨å®ƒæ¥è®¡ç®—è´å°”æ›¼æ–¹ç¨‹ä¸­çš„ Q(sâ€²,aâ€²)å€¼ã€‚è¿™ä¸ªç½‘ç»œä¸æˆ‘ä»¬çš„ä¸»ç½‘ç»œåªä¼šå®šæœŸåŒæ­¥ï¼Œä¾‹å¦‚ï¼Œæ¯éš” N æ­¥åŒæ­¥ä¸€æ¬¡ï¼ˆå…¶ä¸­ N é€šå¸¸æ˜¯ä¸€ä¸ªè¾ƒå¤§çš„è¶…å‚æ•°ï¼Œå¦‚ 1k æˆ– 10k è®­ç»ƒè¿­ä»£ï¼‰ã€‚

## é©¬å°”å¯å¤«æ€§è´¨

æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä»¥é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰å½¢å¼ä¸»ä¹‰ä¸ºåŸºç¡€ï¼Œå‡è®¾ç¯å¢ƒéµå¾ªé©¬å°”å¯å¤«æ€§è´¨ï¼šæ¥è‡ªç¯å¢ƒçš„è§‚å¯Ÿæ˜¯æˆ‘ä»¬é‡‡å–æœ€ä¼˜è¡ŒåŠ¨æ‰€éœ€çš„å…¨éƒ¨ä¿¡æ¯ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬çš„è§‚å¯Ÿå…è®¸æˆ‘ä»¬åŒºåˆ†ä¸åŒçš„çŠ¶æ€ã€‚

å¦‚ä½ ä»å‰é¢çš„ Pong æ¸¸æˆæˆªå›¾ï¼ˆå›¾ 6.2ï¼‰æ‰€è§ï¼Œä¸€å¼ æ¥è‡ª Atari æ¸¸æˆçš„å•ä¸€å›¾åƒä¸è¶³ä»¥æ•è·æ‰€æœ‰é‡è¦ä¿¡æ¯ï¼ˆä»…ä½¿ç”¨ä¸€å¼ å›¾åƒï¼Œæˆ‘ä»¬æ— æ³•çŸ¥é“ç‰©ä½“çš„é€Ÿåº¦å’Œæ–¹å‘ï¼Œæ¯”å¦‚çƒå’Œæˆ‘ä»¬å¯¹æ‰‹çš„æŒ¡æ¿ï¼‰ã€‚è¿™æ˜¾ç„¶è¿åäº†é©¬å°”å¯å¤«æ€§è´¨ï¼Œå¹¶å°†æˆ‘ä»¬çš„å•å¸§ Pong ç¯å¢ƒç§»å…¥éƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPsï¼‰çš„èŒƒç•´ã€‚POMDP åŸºæœ¬ä¸Šæ˜¯æ²¡æœ‰é©¬å°”å¯å¤«æ€§è´¨çš„ MDPï¼Œå®ƒåœ¨å®é™…åº”ç”¨ä¸­éå¸¸é‡è¦ã€‚ä¾‹å¦‚ï¼Œåœ¨å¤§å¤šæ•°æ‰‘å…‹ç‰Œæ¸¸æˆä¸­ï¼Œä½ æ— æ³•çœ‹åˆ°å¯¹æ‰‹çš„ç‰Œï¼Œè¿™äº›æ¸¸æˆè§‚å¯Ÿå°±æ˜¯ POMDPï¼Œå› ä¸ºå½“å‰çš„è§‚å¯Ÿï¼ˆå³ä½ æ‰‹ä¸­çš„ç‰Œå’Œæ¡Œé¢ä¸Šçš„ç‰Œï¼‰å¯èƒ½å¯¹åº”å¯¹æ‰‹æ‰‹ä¸­çš„ä¸åŒç‰Œã€‚

æœ¬ä¹¦ä¸­æˆ‘ä»¬ä¸ä¼šè¯¦ç»†è®¨è®ºéƒ¨åˆ†å¯è§‚å¯Ÿé©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPsï¼‰ï¼Œä½†æˆ‘ä»¬ä¼šä½¿ç”¨ä¸€ä¸ªå°æŠ€å·§å°†æˆ‘ä»¬çš„ç¯å¢ƒæ¨å›åˆ° MDP é¢†åŸŸã€‚è§£å†³æ–¹æ¡ˆæ˜¯ç»´æŠ¤è¿‡å»çš„å¤šä¸ªè§‚å¯Ÿï¼Œå¹¶å°†å®ƒä»¬ç”¨ä½œçŠ¶æ€ã€‚åœ¨ Atari æ¸¸æˆçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é€šå¸¸å°† k ä¸ªè¿ç»­çš„å¸§å †å åœ¨ä¸€èµ·ï¼Œå¹¶å°†å®ƒä»¬ä½œä¸ºæ¯ä¸ªçŠ¶æ€çš„è§‚å¯Ÿã€‚è¿™è®©æˆ‘ä»¬çš„æ™ºèƒ½ä½“èƒ½å¤Ÿæ¨æ–­å‡ºå½“å‰çŠ¶æ€çš„åŠ¨æ€ï¼Œä¾‹å¦‚ï¼Œè·å–çƒçš„é€Ÿåº¦å’Œæ–¹å‘ã€‚å¯¹äº Atari æ¸¸æˆï¼Œé€šå¸¸çš„â€œç»å…¸â€k å€¼æ˜¯å››ã€‚å½“ç„¶ï¼Œè¿™åªæ˜¯ä¸€ä¸ªæŠ€å·§ï¼Œå› ä¸ºç¯å¢ƒä¸­å¯èƒ½å­˜åœ¨æ›´é•¿çš„ä¾èµ–å…³ç³»ï¼Œä½†å¯¹äºå¤§å¤šæ•°æ¸¸æˆæ¥è¯´ï¼Œå®ƒè¡¨ç°å¾—å¾ˆå¥½ã€‚

## DQN è®­ç»ƒçš„æœ€ç»ˆå½¢å¼

ç ”ç©¶äººå‘˜å·²ç»å‘ç°äº†è®¸å¤šæŠ€å·§ï¼Œä½¿å¾— DQN è®­ç»ƒæ›´åŠ ç¨³å®šå’Œé«˜æ•ˆï¼Œæˆ‘ä»¬å°†åœ¨ç¬¬å…«ç« ä»‹ç»å…¶ä¸­æœ€å¥½çš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œepsilon-greedy ç­–ç•¥ã€å›æ”¾ç¼“å†²åŒºå’Œç›®æ ‡ç½‘ç»œæ„æˆäº†åŸºç¡€ï¼Œä½¿å¾— DeepMind å…¬å¸èƒ½å¤ŸæˆåŠŸåœ°åœ¨ 49 æ¬¾ Atari æ¸¸æˆä¸Šè®­ç»ƒ DQNï¼Œå±•ç¤ºäº†è¿™ç§æ–¹æ³•åœ¨å¤æ‚ç¯å¢ƒä¸­çš„æ•ˆç‡ã€‚

åŸå§‹è®ºæ–‡ã€ŠPlaying Atari with deep reinforcement learningã€‹[Mni13]ï¼ˆæ²¡æœ‰ç›®æ ‡ç½‘ç»œï¼‰å‘å¸ƒäº 2013 å¹´åº•ï¼Œæµ‹è¯•ä½¿ç”¨äº†ä¸ƒæ¬¾æ¸¸æˆã€‚åæ¥ï¼Œåœ¨ 2015 å¹´åˆï¼Œæ–‡ç« ç»è¿‡ä¿®è®¢ï¼Œæ ‡é¢˜ä¸ºã€ŠHuman-level control through deep reinforcement learningã€‹[Mni+15]ï¼Œæ­¤æ—¶å·²ä½¿ç”¨äº† 49 æ¬¾ä¸åŒçš„æ¸¸æˆï¼Œå¹¶å‘è¡¨äºã€Šè‡ªç„¶ã€‹æ‚å¿—ã€‚

æ¥è‡ªå‰è¿°è®ºæ–‡çš„ DQN ç®—æ³•æ­¥éª¤å¦‚ä¸‹ï¼š

1.  ä½¿ç”¨éšæœºæƒé‡åˆå§‹åŒ– Q(s,a) å’Œ QÌ‚(s,a) çš„å‚æ•°ï¼Œğœ– â† 1.0ï¼Œå¹¶æ¸…ç©ºå›æ”¾ç¼“å†²åŒºã€‚

1.  ä»¥æ¦‚ç‡ ğœ– é€‰æ‹©ä¸€ä¸ªéšæœºåŠ¨ä½œ aï¼›å¦åˆ™ï¼Œa = arg max[a]Q(s,a)ã€‚

1.  åœ¨æ¨¡æ‹Ÿå™¨ä¸­æ‰§è¡ŒåŠ¨ä½œ aï¼Œå¹¶è§‚å¯Ÿå¥–åŠ± r å’Œä¸‹ä¸€ä¸ªçŠ¶æ€ sâ€²ã€‚

1.  å°†è¿‡æ¸¡ (s, a, r, sâ€²) å­˜å‚¨åˆ°å›æ”¾ç¼“å†²åŒºä¸­ã€‚

1.  ä»å›æ”¾ç¼“å†²åŒºä¸­éšæœºæŠ½å–ä¸€ä¸ªå°æ‰¹é‡çš„è¿‡æ¸¡ã€‚

1.  å¯¹äºç¼“å†²åŒºä¸­çš„æ¯ä¸ªè¿‡æ¸¡ï¼Œè®¡ç®—ç›®æ ‡ï¼š

    ![Ï€ (a |s) = P[At = a|St = s] ](img/eq22.png) ![Ï€ (a |s) = P[At = a|St = s] ](img/eq23.png)

1.  è®¡ç®—æŸå¤±ï¼šâ„’ = (Q(s,a) âˆ’y)Â²ã€‚

1.  é€šè¿‡æœ€å°åŒ–æŸå¤±ç›¸å¯¹äºæ¨¡å‹å‚æ•°ï¼Œä½¿ç”¨ SGD ç®—æ³•æ›´æ–° Q(s,a)ã€‚

1.  æ¯éš” N æ­¥ï¼Œä» Q å¤åˆ¶æƒé‡åˆ° QÌ‚ã€‚

1.  ä»ç¬¬ 2 æ­¥å¼€å§‹é‡å¤ï¼Œç›´åˆ°æ”¶æ•›ã€‚

ç°åœ¨è®©æˆ‘ä»¬å®ç°è¿™ä¸ªç®—æ³•ï¼Œå¹¶å°è¯•å‡»è´¥ä¸€äº› Atari æ¸¸æˆï¼

# DQN åœ¨ Pong æ¸¸æˆä¸­çš„åº”ç”¨

åœ¨æˆ‘ä»¬å¼€å§‹ä»£ç ä¹‹å‰ï¼Œéœ€è¦è¿›è¡Œä¸€äº›ä»‹ç»ã€‚æˆ‘ä»¬çš„ç¤ºä¾‹å˜å¾—è¶Šæ¥è¶Šå…·æœ‰æŒ‘æˆ˜æ€§å’Œå¤æ‚æ€§ï¼Œè¿™å¹¶ä¸å¥‡æ€ªï¼Œå› ä¸ºæˆ‘ä»¬è¦è§£å†³çš„é—®é¢˜çš„å¤æ‚æ€§ä¹Ÿåœ¨å¢åŠ ã€‚å°½ç®¡ä¾‹å­å°½å¯èƒ½ç®€å•ç®€æ´ï¼Œä½†æœ‰äº›ä»£ç åˆçœ‹å¯èƒ½éš¾ä»¥ç†è§£ã€‚

å¦ä¸€ä¸ªéœ€è¦æ³¨æ„çš„äº‹é¡¹æ˜¯æ€§èƒ½ã€‚æˆ‘ä»¬ä¹‹å‰çš„ä¾‹å­ï¼ˆä¾‹å¦‚ FrozenLake æˆ– CartPoleï¼‰ä»èµ„æºè§’åº¦æ¥çœ‹å¹¶ä¸è‹›åˆ»ï¼Œå› ä¸ºè§‚å¯Ÿå€¼è¾ƒå°ï¼Œç¥ç»ç½‘ç»œå‚æ•°ä¹Ÿå¾ˆå°ï¼Œè®­ç»ƒå¾ªç¯ä¸­çš„é¢å¤–æ¯«ç§’å¹¶ä¸é‡è¦ã€‚ç„¶è€Œï¼Œä»ç°åœ¨å¼€å§‹ï¼Œæƒ…å†µå°±ä¸åŒäº†ã€‚æ¥è‡ª Atari ç¯å¢ƒçš„æ¯ä¸ªè§‚å¯Ÿå€¼æœ‰ 10 ä¸‡ä¸ªæ•°æ®ç‚¹ï¼Œè¿™äº›æ•°æ®éœ€è¦é¢„å¤„ç†ã€é‡æ–°ç¼©æ”¾å¹¶å­˜å‚¨åœ¨å›æ”¾ç¼“å†²åŒºä¸­ã€‚å¤šä¸€ä»½æ•°æ®å‰¯æœ¬å¯èƒ½ä¼šå½±å“è®­ç»ƒé€Ÿåº¦ï¼Œè¿™ä¸å†æ˜¯ç§’å’Œåˆ†é’Ÿçš„é—®é¢˜ï¼Œè€Œæ˜¯å³ä½¿æ˜¯æœ€å¿«çš„å›¾å½¢å¤„ç†å•å…ƒï¼ˆGPUï¼‰ä¹Ÿå¯èƒ½éœ€è¦æ•°å°æ—¶ã€‚

ç¥ç»ç½‘ç»œï¼ˆNNï¼‰è®­ç»ƒå¾ªç¯ä¹Ÿå¯èƒ½æˆä¸ºç“¶é¢ˆã€‚å½“ç„¶ï¼Œå¼ºåŒ–å­¦ä¹ æ¨¡å‹å¹¶ä¸åƒæœ€å…ˆè¿›çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰é‚£æ ·åºå¤§ï¼Œä½†å³ä¾¿æ˜¯ 2015 å¹´çš„ DQN æ¨¡å‹ä¹Ÿæœ‰è¶…è¿‡ 150 ä¸‡ä¸ªå‚æ•°ï¼Œéœ€è¦è°ƒæ•´æ•°ç™¾ä¸‡æ¬¡ã€‚å› æ­¤ï¼Œç®€è€Œè¨€ä¹‹ï¼Œæ€§èƒ½éå¸¸é‡è¦ï¼Œå°¤å…¶æ˜¯åœ¨ä½ è¿›è¡Œè¶…å‚æ•°å®éªŒæ—¶ï¼Œä¸ä»…éœ€è¦ç­‰å¾…ä¸€ä¸ªæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€Œæ˜¯å‡ åä¸ªæ¨¡å‹ã€‚

PyTorch ç›¸å½“å…·æœ‰è¡¨ç°åŠ›ï¼Œå› æ­¤æ•ˆç‡è¾ƒé«˜çš„å¤„ç†ä»£ç çœ‹èµ·æ¥é€šå¸¸ä¸å¦‚ä¼˜åŒ–è¿‡çš„ TensorFlow å›¾é‚£ä¹ˆæ™¦æ¶©ï¼Œä½†ä»ç„¶å­˜åœ¨å¾ˆå¤§æœºä¼šåšå¾—å¾ˆæ…¢å¹¶çŠ¯é”™è¯¯ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªç®€å•ç‰ˆçš„ DQN æŸå¤±è®¡ç®—ï¼Œå®ƒå¯¹æ¯ä¸ªæ‰¹æ¬¡æ ·æœ¬è¿›è¡Œå¾ªç¯å¤„ç†ï¼Œæ¯”å¹¶è¡Œç‰ˆæœ¬æ…¢å¤§çº¦ä¸¤å€ã€‚ç„¶è€Œï¼Œä»…ä»…æ˜¯å¯¹æ•°æ®æ‰¹æ¬¡åšä¸€ä¸ªé¢å¤–çš„å‰¯æœ¬ï¼Œå°±ä¼šä½¿å¾—ç›¸åŒä»£ç çš„é€Ÿåº¦å˜æ…¢ 13 å€ï¼Œè¿™éå¸¸æ˜¾è‘—ã€‚

ç”±äºå…¶é•¿åº¦ã€é€»è¾‘ç»“æ„å’Œå¯é‡ç”¨æ€§ï¼Œè¯¥ç¤ºä¾‹è¢«æ‹†åˆ†ä¸ºä¸‰ä¸ªæ¨¡å—ã€‚æ¨¡å—å¦‚ä¸‹ï¼š

+   Chapter06/lib/wrappers.pyï¼šè¿™äº›æ˜¯ Atari ç¯å¢ƒçš„åŒ…è£…å™¨ï¼Œä¸»è¦æ¥è‡ª Stable Baselines3ï¼ˆSB3ï¼‰é¡¹ç›®ï¼š[`github.com/DLR-RM/stable-baselines3`](https://github.com/DLR-RM/stable-baselines3)ã€‚

+   Chapter06/lib/dqn_model.pyï¼šè¿™æ˜¯ DQN ç¥ç»ç½‘ç»œå±‚ï¼Œå…¶æ¶æ„ä¸ DeepMind åœ¨ã€ŠNatureã€‹è®ºæ–‡ä¸­çš„ DQN ç›¸åŒã€‚

+   Chapter06/02_dqn_pong.pyï¼šè¿™æ˜¯ä¸»è¦æ¨¡å—ï¼ŒåŒ…å«è®­ç»ƒå¾ªç¯ã€æŸå¤±å‡½æ•°è®¡ç®—å’Œç»éªŒå›æ”¾ç¼“å†²åŒºã€‚

## åŒ…è£…å™¨

ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰è§£å†³ Atari æ¸¸æˆåœ¨èµ„æºæ–¹é¢æ˜¯ç›¸å½“æœ‰æŒ‘æˆ˜çš„ã€‚ä¸ºäº†åŠ å¿«é€Ÿåº¦ï¼Œé’ˆå¯¹ Atari å¹³å°çš„äº¤äº’åº”ç”¨äº†å‡ ç§è½¬æ¢ï¼Œè¿™äº›è½¬æ¢åœ¨ DeepMind çš„è®ºæ–‡ä¸­æœ‰è¯¦ç»†æè¿°ã€‚éƒ¨åˆ†è½¬æ¢ä»…å½±å“æ€§èƒ½ï¼Œè€Œæœ‰äº›åˆ™æ˜¯è§£å†³ Atari å¹³å°çš„ç‰¹æ€§ï¼Œè¿™äº›ç‰¹æ€§ä½¿å¾—å­¦ä¹ è¿‡ç¨‹æ—¢æ¼«é•¿åˆä¸ç¨³å®šã€‚è½¬æ¢é€šè¿‡ä¸åŒç§ç±»çš„ Gym åŒ…è£…å™¨æ¥å®ç°ã€‚å®Œæ•´çš„åˆ—è¡¨ç›¸å½“é•¿ï¼Œå¹¶ä¸”åŒä¸€ä¸ªåŒ…è£…å™¨æœ‰å¤šä¸ªå®ç°ç‰ˆæœ¬æ¥è‡ªä¸åŒæ¥æºã€‚æˆ‘çš„ä¸ªäººåå¥½æ˜¯ SB3 ä»“åº“ï¼Œå®ƒæ˜¯ OpenAI Baselines ä»£ç çš„æ¼”å˜ç‰ˆæœ¬ã€‚

SB3 åŒ…å«å¤§é‡ä½¿ç”¨ PyTorch å®ç°çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œæ—¨åœ¨ä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„åŸºå‡†ï¼Œæ¯”è¾ƒå„ç§æ–¹æ³•ã€‚ç›®å‰ï¼Œæˆ‘ä»¬å¯¹è¿™äº›æ–¹æ³•çš„å®ç°ä¸æ„Ÿå…´è¶£ï¼ˆæˆ‘ä»¬æ‰“ç®—è‡ªå·±é‡æ–°å®ç°å¤§å¤šæ•°æ–¹æ³•ï¼‰ï¼Œä½†ä¸€äº›åŒ…è£…å™¨éå¸¸æœ‰ç”¨ã€‚è¯¥ä»“åº“å¯ä»¥åœ¨[`github.com/DLR-RM/stable-baselines3`](https://github.com/DLR-RM/stable-baselines3)æ‰¾åˆ°ï¼ŒåŒ…è£…å™¨çš„æ–‡æ¡£å¯ä»¥åœ¨[`stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.xhtml`](https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.xhtml)æŸ¥çœ‹ã€‚å¼ºåŒ–å­¦ä¹ ç ”ç©¶äººå‘˜å¸¸ç”¨çš„ Atari è½¬æ¢åˆ—è¡¨åŒ…æ‹¬ï¼š

+   å°†æ¸¸æˆä¸­çš„æ¯ä¸ªç”Ÿå‘½è½¬åŒ–ä¸ºå•ç‹¬çš„å›åˆï¼šä¸€èˆ¬æ¥è¯´ï¼Œä¸€ä¸ªå›åˆåŒ…å«ä»æ¸¸æˆå¼€å§‹åˆ°â€œæ¸¸æˆç»“æŸâ€ç”»é¢æ‰€æœ‰æ­¥éª¤ï¼Œè¿™å¯èƒ½ä¼šæŒç»­æ•°åƒä¸ªæ¸¸æˆæ­¥éª¤ï¼ˆè§‚å¯Ÿå’ŒåŠ¨ä½œï¼‰ã€‚é€šå¸¸ï¼Œåœ¨è¡—æœºæ¸¸æˆä¸­ï¼Œç©å®¶ä¼šè·å¾—å‡ æ¡å‘½ï¼Œè¿™æä¾›äº†å‡ æ¬¡æ¸¸æˆå°è¯•ã€‚è¿™ç§è½¬æ¢å°†ä¸€ä¸ªå®Œæ•´å›åˆæ‹†åˆ†ä¸ºæ¯æ¡å‘½å¯¹åº”çš„å•ç‹¬å°å›åˆã€‚åœ¨å†…éƒ¨ï¼Œè¿™æ˜¯é€šè¿‡æ£€æŸ¥æ¨¡æ‹Ÿå™¨å…³äºå‰©ä½™ç”Ÿå‘½çš„ä¿¡æ¯æ¥å®ç°çš„ã€‚å¹¶éæ‰€æœ‰æ¸¸æˆéƒ½æ”¯æŒæ­¤åŠŸèƒ½ï¼ˆå°½ç®¡ä¹’ä¹“çƒæ¸¸æˆæ”¯æŒï¼‰ï¼Œä½†å¯¹äºæ”¯æŒçš„ç¯å¢ƒï¼Œè¿™é€šå¸¸æœ‰åŠ©äºåŠ é€Ÿæ”¶æ•›ï¼Œå› ä¸ºæˆ‘ä»¬çš„å›åˆå˜å¾—æ›´çŸ­ã€‚æ­¤é€»è¾‘åœ¨ SB3 ä»£ç ä¸­çš„ EpisodicLifeEnv åŒ…è£…ç±»ä¸­å¾—åˆ°äº†å®ç°ã€‚

+   åœ¨æ¸¸æˆå¼€å§‹æ—¶æ‰§è¡Œä¸€ä¸ªéšæœºæ•°é‡ï¼ˆæœ€å¤š 30 æ¬¡ï¼‰çš„ç©ºæ“ä½œï¼ˆä¹Ÿç§°ä¸ºâ€œæ— æ“ä½œâ€ï¼‰ï¼šè¿™è·³è¿‡äº†ä¸€äº›é›…è¾¾åˆ©æ¸¸æˆä¸­çš„ä»‹ç»ç”»é¢ï¼Œè¿™äº›ç”»é¢ä¸æ¸¸æˆç©æ³•æ— å…³ã€‚å®ƒåœ¨ NoopResetEnv åŒ…è£…ç±»ä¸­å¾—åˆ°äº†å®ç°ã€‚

+   æ¯ K æ­¥åšä¸€æ¬¡åŠ¨ä½œå†³ç­–ï¼Œå…¶ä¸­ K é€šå¸¸æ˜¯ 3 æˆ– 4ï¼šåœ¨ä¸­é—´å¸§ä¸Šï¼Œæ‰€é€‰çš„åŠ¨ä½œä¼šè¢«ç®€å•åœ°é‡å¤ã€‚è¿™ä½¿å¾—è®­ç»ƒèƒ½å¤Ÿæ˜¾è‘—åŠ é€Ÿï¼Œå› ä¸ºä½¿ç”¨ç¥ç»ç½‘ç»œå¤„ç†æ¯ä¸€å¸§æ˜¯ä¸€ä¸ªéå¸¸è´¹æ—¶çš„æ“ä½œï¼Œä½†è¿ç»­å¸§ä¹‹é—´çš„å·®å¼‚é€šå¸¸è¾ƒå°ã€‚è¿™åœ¨ MaxAndSkipEnv åŒ…è£…ç±»ä¸­å¾—åˆ°äº†å®ç°ï¼Œè¯¥ç±»ä¹ŸåŒ…å«åˆ—è¡¨ä¸­çš„ä¸‹ä¸€ä¸ªè½¬æ¢ï¼ˆä¸¤å¸§ä¹‹é—´çš„æœ€å¤§å€¼ï¼‰ã€‚

+   å–æ¯ä¸ªåƒç´ åœ¨æœ€åä¸¤å¸§ä¸­çš„æœ€å¤§å€¼å¹¶ä½œä¸ºè§‚å¯Ÿå€¼ï¼šä¸€äº›é›…è¾¾åˆ©æ¸¸æˆå­˜åœ¨é—ªçƒæ•ˆæœï¼Œè¿™æ˜¯ç”±äºå¹³å°çš„é™åˆ¶ã€‚ï¼ˆé›…è¾¾åˆ©æ¯å¸§ä¸Šå¯ä»¥æ˜¾ç¤ºçš„ç²¾çµæ•°é‡æ˜¯æœ‰é™çš„ã€‚ï¼‰å¯¹äºäººçœ¼æ¥è¯´ï¼Œè¿™ç§å¿«é€Ÿå˜åŒ–æ˜¯ä¸å¯è§çš„ï¼Œä½†å®ƒä»¬å¯èƒ½ä¼šå¹²æ‰°ç¥ç»ç½‘ç»œï¼ˆNNï¼‰ã€‚

+   åœ¨æ¸¸æˆå¼€å§‹æ—¶æŒ‰ä¸‹ FIRE é”®ï¼šæŸäº›æ¸¸æˆï¼ˆåŒ…æ‹¬ä¹’ä¹“çƒå’Œæ‰“ç –å—ï¼‰éœ€è¦ç”¨æˆ·æŒ‰ä¸‹ FIRE æŒ‰é’®æ‰èƒ½å¼€å§‹æ¸¸æˆã€‚å¦‚æœæ²¡æœ‰æŒ‰ä¸‹è¯¥æŒ‰é’®ï¼Œç¯å¢ƒå°†å˜ä¸ºéƒ¨åˆ†å¯è§‚æµ‹çš„é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆPOMDPï¼‰ï¼Œå› ä¸ºä»è§‚å¯Ÿä¸­ï¼Œä»£ç†æ— æ³•åˆ¤æ–­æ˜¯å¦å·²ç»æŒ‰ä¸‹äº† FIRE é”®ã€‚è¿™åœ¨ FireResetEnv åŒ…è£…ç±»ä¸­å¾—åˆ°äº†å®ç°ã€‚

+   å°†æ¯å¸§ä» 210 Ã— 160 çš„ä¸‰è‰²å›¾åƒç¼©æ”¾ä¸ºå•è‰²çš„ 84 Ã— 84 å›¾åƒï¼šæœ‰ä¸åŒçš„æ–¹æ³•å¯ä»¥å®ç°ã€‚ä¾‹å¦‚ï¼ŒDeepMind çš„è®ºæ–‡å°†æ­¤è½¬æ¢æè¿°ä¸ºä» YCbCr è‰²å½©ç©ºé—´ä¸­æå– Y è‰²é€šé“ï¼Œç„¶åå°†æ•´ä¸ªå›¾åƒé‡æ–°ç¼©æ”¾ä¸º 84 Ã— 84 çš„åˆ†è¾¨ç‡ã€‚å…¶ä»–ä¸€äº›ç ”ç©¶äººå‘˜è¿›è¡Œç°åº¦è½¬æ¢ï¼Œè£å‰ªæ‰å›¾åƒä¸­ä¸ç›¸å…³çš„éƒ¨åˆ†ç„¶åè¿›è¡Œç¼©æ”¾ã€‚åœ¨ SB3 çš„ä»£ç åº“ä¸­ï¼Œä½¿ç”¨äº†åä¸€ç§æ–¹æ³•ã€‚è¿™åœ¨ WarpFrame åŒ…è£…ç±»ä¸­å¾—åˆ°äº†å®ç°ã€‚

+   å°†å¤šä¸ªï¼ˆé€šå¸¸æ˜¯å››ä¸ªï¼‰è¿ç»­çš„å¸§å †å åœ¨ä¸€èµ·ï¼Œä»¥å‘ç½‘ç»œæä¾›æ¸¸æˆä¸­ç‰©ä½“åŠ¨æ€çš„ä¿¡æ¯ï¼šè¿™ç§æ–¹æ³•å·²ç»ä½œä¸ºè§£å†³å•ä¸€æ¸¸æˆå¸§ç¼ºä¹æ¸¸æˆåŠ¨æ€çš„å¿«é€Ÿæ–¹æ¡ˆè¿›è¡Œäº†è®¨è®ºã€‚åœ¨ SB3 é¡¹ç›®ä¸­æ²¡æœ‰ç°æˆçš„åŒ…è£…ç±»ï¼Œæˆ‘åœ¨ wrappers.BufferWrapper ä¸­å®ç°äº†æˆ‘çš„ç‰ˆæœ¬ã€‚

+   å°†å¥–åŠ±è£å‰ªåˆ° -1ã€0 å’Œ 1 çš„å€¼ï¼šè·å¾—çš„åˆ†æ•°åœ¨ä¸åŒæ¸¸æˆä¹‹é—´å¯èƒ½å·®å¼‚å¾ˆå¤§ã€‚ä¾‹å¦‚ï¼Œåœ¨ Pong æ¸¸æˆä¸­ï¼Œæ¯å½“ä½ å°†çƒæ‰“è¿‡å¯¹æ–¹çš„æŒ¡æ¿æ—¶ï¼Œä½ ä¼šè·å¾— 1 åˆ†ã€‚ç„¶è€Œï¼Œåœ¨æŸäº›æ¸¸æˆä¸­ï¼Œå¦‚ KungFuMasterï¼Œæ¯æ€æ­»ä¸€ä¸ªæ•Œäººä½ ä¼šè·å¾— 100 åˆ†ã€‚å¥–åŠ±å€¼çš„è¿™ç§å·®å¼‚ä½¿å¾—æˆ‘ä»¬åœ¨ä¸åŒæ¸¸æˆä¹‹é—´çš„æŸå¤±å‡½æ•°å°ºåº¦å®Œå…¨ä¸åŒï¼Œè¿™ä½¿å¾—æ‰¾åˆ°é€‚ç”¨äºä¸€ç»„æ¸¸æˆçš„é€šç”¨è¶…å‚æ•°å˜å¾—æ›´åŠ å›°éš¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¥–åŠ±è¢«è£å‰ªåˆ° âˆ’1 åˆ° 1 çš„èŒƒå›´å†…ã€‚è¿™åœ¨ ClipRewardEnv å°è£…å™¨ä¸­å®ç°ã€‚

+   é‡æ’è§‚å¯Ÿç»´åº¦ä»¥æ»¡è¶³ PyTorch å·ç§¯å±‚çš„è¦æ±‚ï¼šç”±äºæˆ‘ä»¬å°†ä½¿ç”¨å·ç§¯ï¼Œå¼ é‡éœ€è¦æŒ‰ PyTorch æœŸæœ›çš„æ–¹å¼è¿›è¡Œé‡æ’ã€‚Atari ç¯å¢ƒä»¥ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼Œé¢œè‰²ï¼‰çš„æ ¼å¼è¿”å›è§‚å¯Ÿæ•°æ®ï¼Œä½† PyTorch å·ç§¯å±‚è¦æ±‚é€šé“ç»´åº¦æ’åœ¨æœ€å‰é¢ã€‚è¿™åœ¨ wrappers.ImageToPyTorch ä¸­å¾—ä»¥å®ç°ã€‚

å¤§å¤šæ•°è¿™äº›å°è£…å™¨éƒ½åœ¨ stable-baseline3 åº“ä¸­å®ç°ï¼Œåº“ä¸­æä¾›äº† AtariWrapper ç±»ï¼Œå®ƒæ ¹æ®æ„é€ å‡½æ•°çš„å‚æ•°æŒ‰éœ€è¦çš„é¡ºåºåº”ç”¨å°è£…å™¨ã€‚å®ƒè¿˜ä¼šæ£€æµ‹åº•å±‚ç¯å¢ƒçš„å±æ€§ï¼Œå¹¶åœ¨éœ€è¦æ—¶å¯ç”¨ FireResetEnvã€‚å¹¶éæ‰€æœ‰å°è£…å™¨éƒ½éœ€è¦åœ¨ Pong æ¸¸æˆä¸­ä½¿ç”¨ï¼Œä½†ä½ åº”è¯¥äº†è§£ç°æœ‰çš„å°è£…å™¨ï¼Œä»¥é˜²ä½ å†³å®šå°è¯•å…¶ä»–æ¸¸æˆã€‚æœ‰æ—¶ï¼Œå½“ DQN ä¸æ”¶æ•›æ—¶ï¼Œé—®é¢˜å¹¶ä¸åœ¨ä»£ç ä¸­ï¼Œè€Œæ˜¯ç¯å¢ƒå°è£…é”™è¯¯ã€‚æˆ‘æ›¾ç»èŠ±äº†å‡ å¤©æ—¶é—´è°ƒè¯•æ”¶æ•›é—®é¢˜ï¼Œç»“æœæ˜¯å› ä¸ºåœ¨æ¸¸æˆå¼€å§‹æ—¶æ²¡æœ‰æŒ‰ä¸‹ FIRE æŒ‰é’®ï¼

è®©æˆ‘ä»¬æ¥çœ‹çœ‹å„ä¸ªå°è£…å™¨çš„å®ç°ã€‚æˆ‘ä»¬å°†ä» stable-baseline3 æä¾›çš„ç±»å¼€å§‹ï¼š

```py
class FireResetEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]): 
    def __init__(self, env: gym.Env) -> None: 
        super().__init__(env) 
        assert env.unwrapped.get_action_meanings()[1] == "FIRE" 
        assert len(env.unwrapped.get_action_meanings()) >= 3 

    def reset(self, **kwargs) -> AtariResetReturn: 
        self.env.reset(**kwargs) 
        obs, _, terminated, truncated, _ = self.env.step(1) 
        if terminated or truncated: 
            self.env.reset(**kwargs) 
        obs, _, terminated, truncated, _ = self.env.step(2) 
        if terminated or truncated: 
            self.env.reset(**kwargs) 
        return obs, {}
```

ä¸Šè¿°å°è£…å™¨åœ¨éœ€è¦æŒ‰ä¸‹ FIRE æŒ‰é’®æ‰èƒ½å¼€å§‹æ¸¸æˆçš„ç¯å¢ƒä¸­æŒ‰ä¸‹è¯¥æŒ‰é’®ã€‚é™¤äº†æŒ‰ä¸‹ FIRE æŒ‰é’®å¤–ï¼Œè¿™ä¸ªå°è£…å™¨è¿˜ä¼šæ£€æŸ¥ä¸€äº›åœ¨æŸäº›æ¸¸æˆä¸­å­˜åœ¨çš„è¾¹ç¼˜æƒ…å†µã€‚

è¿™ä¸ªå°è£…å™¨ç»“åˆäº†åœ¨ K å¸§å†…é‡å¤æ‰§è¡Œçš„åŠ¨ä½œå’Œæ¥è‡ªä¸¤å¸§ä¹‹é—´çš„åƒç´ ä¿¡æ¯ï¼š

```py
class MaxAndSkipEnv(gym.Wrapper[np.ndarray, int, np.ndarray, int]): 
    def __init__(self, env: gym.Env, skip: int = 4) -> None: 
        super().__init__(env) 
        self._obs_buffer = np.zeros((2, *env.observation_space.shape), 
            dtype=env.observation_space.dtype) 
        self._skip = skip 

    def step(self, action: int) -> AtariStepReturn: 
        total_reward = 0.0 
        terminated = truncated = False 
        for i in range(self._skip): 
            obs, reward, terminated, truncated, info = self.env.step(action) 
            done = terminated or truncated 
            if i == self._skip - 2: 
                self._obs_buffer[0] = obs 
            if i == self._skip - 1: 
                self._obs_buffer[1] = obs 
            total_reward += float(reward) 
            if done: 
                break 
        # Note that the observation on the done=True frame 
        # doesnâ€™t matter 
        max_frame = self._obs_buffer.max(axis=0) 

        return max_frame, total_reward, terminated, truncated, info
```

ä»¥ä¸‹å°è£…å™¨çš„ç›®æ ‡æ˜¯å°†æ¥è‡ªæ¨¡æ‹Ÿå™¨çš„è¾“å…¥è§‚å¯Ÿè½¬æ¢ä¸ºä¸€ä¸ªåˆ†è¾¨ç‡ä¸º 210 Ã— 160 åƒç´ å¹¶å…·æœ‰ RGB é¢œè‰²é€šé“çš„å›¾åƒï¼Œè½¬æ¢ä¸ºä¸€ä¸ªç°åº¦ 84 Ã— 84 çš„å›¾åƒã€‚å®ƒé€šè¿‡ä½¿ç”¨ CV2 åº“ä¸­çš„ cvtColor å‡½æ•°æ¥å®ç°è¯¥æ“ä½œï¼ŒcvTColor å‡½æ•°æ‰§è¡Œçš„æ˜¯è‰²åº¦ç°åº¦è½¬æ¢ï¼ˆè¿™ç§è½¬æ¢æ¯”ç®€å•çš„é¢œè‰²é€šé“å¹³å‡æ›´æ¥è¿‘äººç±»çš„é¢œè‰²æ„ŸçŸ¥ï¼‰ï¼Œç„¶åå›¾åƒä¼šè¢«ç¼©æ”¾ï¼š

```py
class WarpFrame(gym.ObservationWrapper[np.ndarray, int, np.ndarray]): 
    def __init__(self, env: gym.Env, width: int = 84, height: int = 84) -> None: 
        super().__init__(env) 
        self.width = width 
        self.height = height 
        self.observation_space = spaces.Box( 
            low=0, high=255, shape=(self.height, self.width, 1), 
            dtype=env.observation_space.dtype, 
        ) 

    def observation(self, frame: np.ndarray) -> np.ndarray: 
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) 
        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA) 
        return frame[:, :, None]
```

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»ä½¿ç”¨äº† stable-baseline3 ä¸­çš„å°è£…å™¨ï¼ˆæˆ‘è·³è¿‡äº† EpisodicLifeEnv å°è£…å™¨ï¼Œå› ä¸ºå®ƒæœ‰ç‚¹å¤æ‚ä¸”ä¸æ­¤ä¸å¤ªç›¸å…³ï¼‰ï¼›ä½ å¯ä»¥åœ¨ä»“åº“ stable_baselines3/common/atari_wrappers.py ä¸­æ‰¾åˆ°å…¶ä»–å¯ç”¨å°è£…å™¨çš„ä»£ç ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹æ¥è‡ª lib/wrappers.py ä¸­çš„ä¸¤ä¸ªå°è£…å™¨ï¼š

```py
class BufferWrapper(gym.ObservationWrapper): 
    def __init__(self, env, n_steps): 
        super(BufferWrapper, self).__init__(env) 
        obs = env.observation_space 
        assert isinstance(obs, spaces.Box) 
        new_obs = gym.spaces.Box( 
            obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0), 
            dtype=obs.dtype) 
        self.observation_space = new_obs 
        self.buffer = collections.deque(maxlen=n_steps) 

    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None): 
        for _ in range(self.buffer.maxlen-1): 
            self.buffer.append(self.env.observation_space.low) 
        obs, extra = self.env.reset() 
        return self.observation(obs), extra 

    def observation(self, observation: np.ndarray) -> np.ndarray: 
        self.buffer.append(observation) 
        return np.concatenate(self.buffer)
```

BufferWrapper ç±»åˆ›å»ºäº†ä¸€ä¸ªå †æ ˆï¼ˆä½¿ç”¨ deque ç±»å®ç°ï¼‰ï¼Œæ²¿ç€ç¬¬ä¸€ç»´åº¦å †å éšåçš„å¸§ï¼Œå¹¶å°†å®ƒä»¬ä½œä¸ºè§‚å¯Ÿå€¼è¿”å›ã€‚ç›®çš„æ˜¯è®©ç½‘ç»œäº†è§£ç‰©ä½“çš„åŠ¨æ€ä¿¡æ¯ï¼Œæ¯”å¦‚ä¹’ä¹“çƒçš„é€Ÿåº¦å’Œæ–¹å‘ï¼Œæˆ–è€…æ•Œäººæ˜¯å¦‚ä½•ç§»åŠ¨çš„ã€‚è¿™äº›ä¿¡æ¯éå¸¸é‡è¦ï¼Œæ˜¯ä»å•ä¸€å›¾åƒä¸­æ— æ³•è·å¾—çš„ã€‚

å…³äºè¿™ä¸ªåŒ…è£…å™¨æœ‰ä¸€ä¸ªéå¸¸é‡è¦ä½†ä¸å¤ªæ˜¾çœ¼çš„ç»†èŠ‚ï¼Œé‚£å°±æ˜¯è§‚å¯Ÿæ–¹æ³•è¿”å›çš„æ˜¯æˆ‘ä»¬ç¼“å†²åŒºä¸­è§‚å¯Ÿå€¼çš„å‰¯æœ¬ã€‚è¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬è¦å°†è§‚å¯Ÿå€¼ä¿å­˜åœ¨é‡æ”¾ç¼“å†²åŒºä¸­ï¼Œå› æ­¤éœ€è¦å‰¯æœ¬ä»¥é¿å…æœªæ¥ç¯å¢ƒæ­¥éª¤ä¸­å¯¹ç¼“å†²åŒºçš„ä¿®æ”¹ã€‚ä»åŸåˆ™ä¸Šè®²ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åœ¨å…¶ä¸­ä¿å­˜å›åˆçš„è§‚å¯Ÿå€¼å’Œå®ƒä»¬çš„ç´¢å¼•æ¥é¿å…åˆ¶ä½œå‰¯æœ¬ï¼ˆå¹¶å°†å†…å­˜å ç”¨å‡å°‘å››å€ï¼‰ï¼Œä½†è¿™éœ€è¦æ›´åŠ å¤æ‚çš„æ•°æ®ç»“æ„ç®¡ç†ã€‚ç›®å‰éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸ªåŒ…è£…å™¨å¿…é¡»æ˜¯åº”ç”¨äºç¯å¢ƒçš„åŒ…è£…å™¨é“¾ä¸­çš„æœ€åä¸€ä¸ªã€‚

æœ€åçš„åŒ…è£…å™¨æ˜¯ ImageToPyTorchï¼Œå®ƒå°†è§‚å¯Ÿå€¼çš„å½¢çŠ¶ä»é«˜åº¦ã€å®½åº¦ã€é€šé“ï¼ˆHWCï¼‰æ ¼å¼è½¬æ¢ä¸º PyTorch æ‰€éœ€çš„é€šé“ã€é«˜åº¦ã€å®½åº¦ï¼ˆCHWï¼‰æ ¼å¼ï¼š

```py
class ImageToPyTorch(gym.ObservationWrapper): 
    def __init__(self, env): 
        super(ImageToPyTorch, self).__init__(env) 
        obs = self.observation_space 
        assert isinstance(obs, gym.spaces.Box) 
        assert len(obs.shape) == 3 
        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1]) 
        self.observation_space = gym.spaces.Box( 
            low=obs.low.min(), high=obs.high.max(), 
            shape=new_shape, dtype=obs.dtype) 

    def observation(self, observation): 
        return np.moveaxis(observation, 2, 0)
```

å¼ é‡çš„è¾“å…¥å½¢çŠ¶çš„æœ€åä¸€ç»´æ˜¯é¢œè‰²é€šé“ï¼Œä½† PyTorch çš„å·ç§¯å±‚å‡è®¾é¢œè‰²é€šé“æ˜¯ç¬¬ä¸€ç»´ã€‚

æ–‡ä»¶çš„æœ€åæ˜¯ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œå®ƒåˆ›å»ºä¸€ä¸ªå¸¦æœ‰åç§°çš„ç¯å¢ƒï¼Œå¹¶å°†æ‰€æœ‰éœ€è¦çš„åŒ…è£…å™¨åº”ç”¨äºå®ƒï¼š

```py
def make_env(env_name: str, **kwargs): 
    env = gym.make(env_name, **kwargs) 
    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0) 
    env = ImageToPyTorch(env) 
    env = BufferWrapper(env, n_steps=4) 
    return env
```

å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨æ¥è‡ª stable-baseline3 çš„ AtariWrapper ç±»ï¼Œå¹¶ç¦ç”¨äº†ä¸€äº›ä¸å¿…è¦çš„åŒ…è£…å™¨ã€‚

è¿™å°±æ˜¯åŒ…è£…å™¨çš„å†…å®¹ï¼›æ¥ä¸‹æ¥æˆ‘ä»¬æ¥çœ‹çœ‹æˆ‘ä»¬çš„æ¨¡å‹ã€‚

## DQN æ¨¡å‹

å‘è¡¨åœ¨ã€Šè‡ªç„¶ã€‹æ‚å¿—ä¸Šçš„æ¨¡å‹æœ‰ä¸‰ä¸ªå·ç§¯å±‚ï¼Œåé¢è·Ÿç€ä¸¤ä¸ªå…¨è¿æ¥å±‚ã€‚æ‰€æœ‰çš„å±‚éƒ½ç”±ä¿®æ­£çº¿æ€§å•å…ƒï¼ˆReLUï¼‰éçº¿æ€§å‡½æ•°åˆ†éš”ã€‚è¯¥æ¨¡å‹çš„è¾“å‡ºæ˜¯ç¯å¢ƒä¸­æ¯ä¸ªå¯ç”¨åŠ¨ä½œçš„ Q å€¼ï¼Œä¸”æ²¡æœ‰åº”ç”¨éçº¿æ€§ï¼ˆå› ä¸º Q å€¼å¯ä»¥æ˜¯ä»»æ„å€¼ï¼‰ã€‚é€šè¿‡è®©æ‰€æœ‰ Q å€¼é€šè¿‡ç½‘ç»œä¸€æ¬¡è®¡ç®—å‡ºæ¥ï¼Œè¿™ç§æ–¹æ³•ç›¸æ¯”å°† Q(s,a)ç›´æ¥å¤„ç†å¹¶å°†è§‚å¯Ÿå’ŒåŠ¨ä½œè¾“å…¥ç½‘ç»œä»¥è·å–åŠ¨ä½œå€¼çš„æ–¹æ³•ï¼Œæ˜¾è‘—æé«˜äº†é€Ÿåº¦ã€‚

æ¨¡å‹çš„ä»£ç åœ¨ Chapter06/lib/dqn_model.py ä¸­ï¼š

```py
import torch 
import torch.nn as nn 

class DQN(nn.Module): 
    def __init__(self, input_shape, n_actions): 
        super(DQN, self).__init__() 

        self.conv = nn.Sequential( 
            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4), 
            nn.ReLU(), 
            nn.Conv2d(32, 64, kernel_size=4, stride=2), 
            nn.ReLU(), 
            nn.Conv2d(64, 64, kernel_size=3, stride=1), 
            nn.ReLU(), 
            nn.Flatten(), 
        ) 
        size = self.conv(torch.zeros(1, *input_shape)).size()[-1] 
        self.fc = nn.Sequential( 
            nn.Linear(size, 512), 
            nn.ReLU(), 
            nn.Linear(512, n_actions) 
        )
```

ä¸ºäº†èƒ½å¤Ÿä»¥é€šç”¨çš„æ–¹å¼ç¼–å†™æˆ‘ä»¬çš„ç½‘ç»œï¼Œå®ƒè¢«å®ç°ä¸ºä¸¤éƒ¨åˆ†ï¼šå·ç§¯éƒ¨åˆ†å’Œçº¿æ€§éƒ¨åˆ†ã€‚å·ç§¯éƒ¨åˆ†å¤„ç†è¾“å…¥å›¾åƒï¼Œå®ƒæ˜¯ä¸€ä¸ª 4 Ã— 84 Ã— 84 çš„å¼ é‡ã€‚æœ€åä¸€ä¸ªå·ç§¯æ»¤æ³¢å™¨çš„è¾“å‡ºè¢«å±•å¹³ä¸ºä¸€ä¸ªä¸€ç»´å‘é‡ï¼Œå¹¶è¾“å…¥åˆ°ä¸¤ä¸ªçº¿æ€§å±‚ä¸­ã€‚

å¦ä¸€ä¸ªå°é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬ä¸çŸ¥é“ç”±ç»™å®šå½¢çŠ¶è¾“å…¥äº§ç”Ÿçš„å·ç§¯å±‚è¾“å‡ºä¸­ç¡®åˆ‡çš„å€¼çš„æ•°é‡ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™ä¸ªæ•°å­—ä¼ é€’ç»™ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚çš„æ„é€ å‡½æ•°ã€‚ä¸€ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ¡ˆæ˜¯ç¡¬ç¼–ç è¿™ä¸ªæ•°å­—ï¼Œå®ƒæ˜¯è¾“å…¥å½¢çŠ¶å’Œæœ€åä¸€ä¸ªå·ç§¯å±‚é…ç½®çš„å‡½æ•°ï¼ˆå¯¹äº 84 Ã— 84 çš„è¾“å…¥ï¼Œå·ç§¯å±‚çš„è¾“å‡ºå°†æœ‰ 3,136 ä¸ªå€¼ï¼‰ï¼›ç„¶è€Œï¼Œè¿™ä¸æ˜¯æœ€å¥½çš„æ–¹å¼ï¼Œå› ä¸ºæˆ‘ä»¬çš„ä»£ç ä¼šå˜å¾—ä¸å¤ªå¥å£®ï¼Œæ— æ³•åº”å¯¹è¾“å…¥å½¢çŠ¶çš„å˜åŒ–ã€‚æ›´å¥½çš„è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡åº”ç”¨å·ç§¯éƒ¨åˆ†åˆ°ä¸€ä¸ªå‡è¾“å…¥å¼ é‡ï¼Œåœ¨è¿è¡Œæ—¶è·å–æ‰€éœ€çš„ç»´åº¦ã€‚ç»“æœçš„ç»´åº¦å°†ç­‰äºè¯¥åº”ç”¨è¿”å›çš„å‚æ•°æ•°é‡ã€‚è¿™æ ·åšéå¸¸å¿«é€Ÿï¼Œå› ä¸ºè¿™ä¸ªè°ƒç”¨åªä¼šåœ¨æ¨¡å‹åˆ›å»ºæ—¶æ‰§è¡Œä¸€æ¬¡ï¼Œè€Œä¸”å®ƒè¿˜å…è®¸æˆ‘ä»¬æ‹¥æœ‰é€šç”¨çš„ä»£ç ã€‚

æ¨¡å‹çš„æœ€åä¸€éƒ¨åˆ†æ˜¯ `forward()` å‡½æ•°ï¼Œå®ƒæ¥å— 4D è¾“å…¥å¼ é‡ã€‚ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æ‰¹é‡å¤§å°ï¼Œç¬¬äºŒä¸ªç»´åº¦æ˜¯é¢œè‰²é€šé“ï¼Œå®ƒæ˜¯æˆ‘ä»¬åç»­å¸§çš„å †å ï¼›ç¬¬ä¸‰å’Œç¬¬å››ä¸ªç»´åº¦æ˜¯å›¾åƒå°ºå¯¸ï¼š

```py
 def forward(self, x: torch.ByteTensor): 
        # scale on GPU 
        xx = x / 255.0 
        return self.fc(self.conv(xx))
```

åœ¨è¿™é‡Œï¼Œåœ¨åº”ç”¨æˆ‘ä»¬çš„ç½‘ç»œä¹‹å‰ï¼Œæˆ‘ä»¬å¯¹è¾“å…¥æ•°æ®è¿›è¡Œäº†ç¼©æ”¾å’Œç±»å‹è½¬æ¢ã€‚è¿™éœ€è¦ä¸€äº›è§£é‡Šã€‚

Atari å›¾åƒä¸­çš„æ¯ä¸ªåƒç´ è¡¨ç¤ºä¸ºä¸€ä¸ªæ— ç¬¦å·å­—èŠ‚ï¼Œå€¼çš„èŒƒå›´ä» 0 åˆ° 255ã€‚è¿™æ ·åšæœ‰ä¸¤ä¸ªå¥½å¤„ï¼šå†…å­˜æ•ˆç‡å’Œ GPU å¸¦å®½ã€‚ä»å†…å­˜çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬åº”è¯¥å°½é‡ä¿æŒç¯å¢ƒè§‚å¯Ÿæ•°æ®çš„å¤§å°ï¼Œå› ä¸ºæˆ‘ä»¬çš„å›æ”¾ç¼“å†²åŒºä¼šä¿å­˜æˆåƒä¸Šä¸‡çš„è§‚å¯Ÿç»“æœï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒä¿æŒå°½å¯èƒ½å°ã€‚å¦ä¸€æ–¹é¢ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™äº›è§‚å¯Ÿæ•°æ®è½¬ç§»åˆ° GPU å†…å­˜ä¸­ï¼Œä»¥è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°ç½‘ç»œå‚æ•°ã€‚ä¸»å†…å­˜å’Œ GPU ä¹‹é—´çš„å¸¦å®½æ˜¯æœ‰é™èµ„æºï¼Œå› æ­¤ä¿æŒè§‚å¯Ÿæ•°æ®å°½å¯èƒ½å°ä¹Ÿæ˜¯æœ‰é“ç†çš„ã€‚

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†è§‚å¯Ÿç»“æœä¿æŒä¸º `dtype=uint8` çš„ numpy æ•°ç»„ï¼Œå¹¶ä¸”ç½‘ç»œçš„è¾“å…¥å¼ é‡æ˜¯ ByteTensorã€‚ä½†æ˜¯ Conv2D å±‚æœŸæœ›è¾“å…¥çš„æ˜¯æµ®åŠ¨ç±»å‹å¼ é‡ï¼Œå› æ­¤é€šè¿‡å°†è¾“å…¥å¼ é‡é™¤ä»¥ 255.0ï¼Œæˆ‘ä»¬å°†å…¶ç¼©æ”¾åˆ° 0â€¦1 èŒƒå›´ï¼Œå¹¶è¿›è¡Œç±»å‹è½¬æ¢ã€‚è¿™æ˜¯å¿«é€Ÿçš„ï¼Œå› ä¸ºè¾“å…¥å­—èŠ‚å¼ é‡å·²ç»åœ¨ GPU å†…å­˜ä¸­ã€‚ä¹‹åï¼Œæˆ‘ä»¬å°†ç½‘ç»œçš„ä¸¤ä¸ªéƒ¨åˆ†åº”ç”¨äºç»“æœçš„ç¼©æ”¾å¼ é‡ã€‚

## è®­ç»ƒ

ç¬¬ä¸‰ä¸ªæ¨¡å—åŒ…å«ç»éªŒå›æ”¾ç¼“å†²åŒºã€æ™ºèƒ½ä½“ã€æŸå¤±å‡½æ•°è®¡ç®—å’Œè®­ç»ƒå¾ªç¯æœ¬èº«ã€‚åœ¨è¿›å…¥ä»£ç ä¹‹å‰ï¼Œéœ€è¦å…ˆè°ˆä¸€ä¸‹è®­ç»ƒçš„è¶…å‚æ•°ã€‚

DeepMind çš„ Nature è®ºæ–‡ä¸­åŒ…å«äº†ä¸€ä¸ªè¡¨æ ¼ï¼Œåˆ—å‡ºäº†ç”¨äºè®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°æ‰€æœ‰ 49 ä¸ª Atari æ¸¸æˆçš„è¶…å‚æ•°çš„è¯¦ç»†ä¿¡æ¯ã€‚DeepMind å¯¹æ‰€æœ‰æ¸¸æˆä¿æŒäº†ç›¸åŒçš„å‚æ•°è®¾ç½®ï¼ˆä½†æ¯ä¸ªæ¸¸æˆè®­ç»ƒäº†å•ç‹¬çš„æ¨¡å‹ï¼‰ï¼Œä»–ä»¬çš„ç›®çš„æ˜¯å±•ç¤ºè¯¥æ–¹æ³•è¶³å¤Ÿç¨³å¥ï¼Œå¯ä»¥ä½¿ç”¨ä¸€ä¸ªç»Ÿä¸€çš„æ¨¡å‹æ¶æ„å’Œè¶…å‚æ•°è§£å†³å¤šç§å¤æ‚åº¦ã€åŠ¨ä½œç©ºé—´ã€å¥–åŠ±ç»“æ„åŠå…¶ä»–ç»†èŠ‚å„å¼‚çš„æ¸¸æˆã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ç›®æ ‡è¦è°¦é€Šå¾—å¤šï¼šæˆ‘ä»¬åªå¸Œæœ›è§£å†³ Pong æ¸¸æˆã€‚

Pong ç›¸æ¯”äº Atari æµ‹è¯•é›†ä¸­å…¶ä»–æ¸¸æˆæ¥è¯´ç›¸å½“ç®€å•ç›´æ¥ï¼Œå› æ­¤æ–‡ä¸­æåˆ°çš„è¶…å‚æ•°å¯¹äºæˆ‘ä»¬çš„ä»»åŠ¡æ¥è¯´æ˜¯è¿‡å¤šçš„ã€‚ä¾‹å¦‚ï¼Œä¸ºäº†åœ¨æ‰€æœ‰ 49 ä¸ªæ¸¸æˆä¸­è·å¾—æœ€ä½³ç»“æœï¼ŒDeepMind ä½¿ç”¨äº†ç™¾ä¸‡æ¬¡è§‚æµ‹çš„é‡æ”¾ç¼“å†²åŒºï¼Œè¿™éœ€è¦å¤§çº¦ 20 GB çš„å†…å­˜æ¥å­˜å‚¨ï¼Œå¹¶ä¸”éœ€è¦å¤§é‡çš„ç¯å¢ƒæ ·æœ¬æ¥å¡«å……å®ƒã€‚

æ‰€ä½¿ç”¨çš„ epsilon è¡°å‡è®¡åˆ’å¯¹äºå•ä¸€çš„ Pong æ¸¸æˆä¹Ÿä¸æ˜¯æœ€ä¼˜çš„ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒDeepMind å°† epsilon ä» 1.0 çº¿æ€§è¡°å‡åˆ° 0.1ï¼Œè¡°å‡è¿‡ç¨‹æŒç»­äº†ä»ç¯å¢ƒä¸­è·å¾—çš„å‰ç™¾ä¸‡å¸§ã€‚ç„¶è€Œï¼Œæˆ‘è‡ªå·±çš„å®éªŒè¡¨æ˜ï¼Œå¯¹äº Pong æ¸¸æˆï¼Œè¡°å‡ epsilon åªéœ€è¦åœ¨å‰ 15 ä¸‡å¸§å†…å®Œæˆï¼Œç„¶åä¿æŒç¨³å®šå³å¯ã€‚é‡æ”¾ç¼“å†²åŒºä¹Ÿå¯ä»¥æ›´å°ï¼š10k æ¬¡è½¬æ¢å°±è¶³å¤Ÿäº†ã€‚

åœ¨ä»¥ä¸‹ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä½¿ç”¨äº†æˆ‘çš„å‚æ•°ã€‚è™½ç„¶è¿™äº›å‚æ•°ä¸è®ºæ–‡ä¸­çš„å‚æ•°ä¸åŒï¼Œä½†å®ƒä»¬èƒ½è®©æˆ‘ä»¬å¤§çº¦ä»¥ 10 å€çš„é€Ÿåº¦è§£å†³ Pong æ¸¸æˆã€‚åœ¨ GeForce GTX 1080 Ti ä¸Šï¼Œä»¥ä¸‹ç‰ˆæœ¬å¤§çº¦ 50 åˆ†é’Ÿå°±èƒ½æ”¶æ•›åˆ° 19.0 çš„å¹³å‡åˆ†æ•°ï¼Œä½†ä½¿ç”¨ DeepMind çš„è¶…å‚æ•°è‡³å°‘éœ€è¦ä¸€å¤©æ—¶é—´ã€‚

è¿™ç§åŠ é€Ÿå½“ç„¶æ˜¯é’ˆå¯¹ç‰¹å®šç¯å¢ƒçš„å¾®è°ƒï¼Œå¹¶å¯èƒ½å¯¼è‡´åœ¨å…¶ä»–æ¸¸æˆä¸­æ— æ³•æ”¶æ•›ã€‚ä½ å¯ä»¥è‡ªç”±åœ°è°ƒæ•´é€‰é¡¹å’Œå°è¯• Atari é›†åˆä¸­çš„å…¶ä»–æ¸¸æˆã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥æ‰€éœ€çš„æ¨¡å—ï¼š

```py
import gymnasium as gym 
from lib import dqn_model 
from lib import wrappers 

from dataclasses import dataclass 
import argparse 
import time 
import numpy as np 
import collections 
import typing as tt 

import torch 
import torch.nn as nn 
import torch.optim as optim 

from torch.utils.tensorboard.writer import SummaryWriter
```

ç„¶åæˆ‘ä»¬å®šä¹‰è¶…å‚æ•°ï¼š

```py
DEFAULT_ENV_NAME = "PongNoFrameskip-v4" 
MEAN_REWARD_BOUND = 19
```

è¿™ä¸¤ä¸ªå€¼è®¾ç½®äº†é»˜è®¤çš„è®­ç»ƒç¯å¢ƒä»¥åŠåœæ­¢è®­ç»ƒçš„å¥–åŠ±è¾¹ç•Œï¼ˆæœ€å 100 å›åˆï¼‰ã€‚å¦‚æœéœ€è¦ï¼Œä½ å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œ --env å‚æ•°é‡æ–°å®šä¹‰ç¯å¢ƒåç§°ï¼š

```py
GAMMA = 0.99 
BATCH_SIZE = 32 
REPLAY_SIZE = 10000 
LEARNING_RATE = 1e-4 
SYNC_TARGET_FRAMES = 1000 
REPLAY_START_SIZE = 10000
```

ä¸Šè¿°å‚æ•°å®šä¹‰äº†ä»¥ä¸‹å†…å®¹ï¼š

+   æˆ‘ä»¬ç”¨äºè´å°”æ›¼è¿‘ä¼¼çš„ Î³ å€¼ï¼ˆGAMMAï¼‰

+   ä»é‡æ”¾ç¼“å†²åŒºä¸­é‡‡æ ·çš„æ‰¹æ¬¡å¤§å°ï¼ˆBATCH_SIZEï¼‰

+   ç¼“å†²åŒºçš„æœ€å¤§å®¹é‡ï¼ˆREPLAY_SIZEï¼‰

+   æˆ‘ä»¬åœ¨å¼€å§‹è®­ç»ƒå‰ç­‰å¾…çš„å¸§æ•°ï¼Œç”¨äºå¡«å……é‡æ”¾ç¼“å†²åŒºï¼ˆREPLAY_START_SIZEï¼‰

+   åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ä½¿ç”¨çš„ Adam ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ï¼ˆLEARNING_RATEï¼‰

+   æˆ‘ä»¬å°†è®­ç»ƒæ¨¡å‹çš„æƒé‡åŒæ­¥åˆ°ç›®æ ‡æ¨¡å‹çš„é¢‘ç‡ï¼Œç›®æ ‡æ¨¡å‹ç”¨äºåœ¨è´å°”æ›¼è¿‘ä¼¼ä¸­è·å–ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å€¼ï¼ˆSYNC_TARGET_FRAMESï¼‰

```py
EPSILON_DECAY_LAST_FRAME = 150000 
EPSILON_START = 1.0 
EPSILON_FINAL = 0.01
```

æœ€åä¸€æ‰¹è¶…å‚æ•°ä¸ epsilon è¡°å‡è°ƒåº¦æœ‰å…³ã€‚ä¸ºäº†å®ç°é€‚å½“çš„æ¢ç´¢ï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒçš„æ—©æœŸé˜¶æ®µä» ğœ– = 1.0 å¼€å§‹ï¼Œè¿™ä¼šå¯¼è‡´æ‰€æœ‰åŠ¨ä½œéƒ½è¢«éšæœºé€‰æ‹©ã€‚ç„¶åï¼Œåœ¨å‰ 150,000 å¸§ä¸­ï¼Œğœ– ä¼šçº¿æ€§è¡°å‡åˆ° 0.01ï¼Œè¿™å¯¹åº”äº 1% çš„æ­¥éª¤ä¸­é‡‡å–éšæœºåŠ¨ä½œã€‚åŸå§‹ DeepMind è®ºæ–‡ä¸­ä¹Ÿä½¿ç”¨äº†ç±»ä¼¼çš„æ–¹æ¡ˆï¼Œä½†è¡°å‡çš„æŒç»­æ—¶é—´å‡ ä¹æ˜¯åŸæ¥çš„ 10 å€ï¼ˆå› æ­¤ ğœ– = 0.01 æ˜¯åœ¨ä¸€ç™¾ä¸‡å¸§åè¾¾åˆ°çš„ï¼‰ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å®šä¹‰äº†ç±»å‹åˆ«åå’Œæ•°æ®ç±» Experienceï¼Œç”¨äºä¿å­˜ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­çš„æ¡ç›®ã€‚å®ƒåŒ…å«å½“å‰çŠ¶æ€ã€é‡‡å–çš„åŠ¨ä½œã€è·å¾—çš„å¥–åŠ±ã€ç»ˆæ­¢æˆ–æˆªæ–­æ ‡å¿—ä»¥åŠæ–°çš„çŠ¶æ€ï¼š

```py
State = np.ndarray 
Action = int 
BatchTensors = tt.Tuple[ 
    torch.ByteTensor,           # current state 
    torch.LongTensor,           # actions 
    torch.Tensor,               # rewards 
    torch.BoolTensor,           # done || trunc 
    torch.ByteTensor            # next state 
] 

@dataclass 
class Experience: 
    state: State 
    action: Action 
    reward: float 
    done_trunc: bool 
    new_state: State
```

ä¸‹ä¸€æ®µä»£ç å®šä¹‰äº†æˆ‘ä»¬çš„ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œç›®çš„æ˜¯ä¿å­˜ä»ç¯å¢ƒä¸­è·å¾—çš„è½¬ç§»ï¼š

```py
class ExperienceBuffer: 
    def __init__(self, capacity: int): 
        self.buffer = collections.deque(maxlen=capacity) 

    def __len__(self): 
        return len(self.buffer) 

    def append(self, experience: Experience): 
        self.buffer.append(experience) 

    def sample(self, batch_size: int) -> tt.List[Experience]: 
        indices = np.random.choice(len(self), batch_size, replace=False) 
        return [self.buffer[idx] for idx in indices]
```

æ¯æ¬¡åœ¨ç¯å¢ƒä¸­æ‰§è¡Œä¸€æ­¥æ—¶ï¼Œæˆ‘ä»¬å°†è½¬ç§»æ¨å…¥ç¼“å†²åŒºï¼Œåªä¿ç•™å›ºå®šæ•°é‡çš„æ­¥æ•°ï¼ˆåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹æ˜¯ 10k æ¬¡è½¬ç§»ï¼‰ã€‚åœ¨è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ä»å›æ”¾ç¼“å†²åŒºéšæœºæŠ½å–ä¸€æ‰¹è½¬ç§»ï¼Œè¿™æ ·å¯ä»¥æ‰“ç ´ç¯å¢ƒä¸­åç»­æ­¥éª¤ä¹‹é—´çš„ç›¸å…³æ€§ã€‚

å¤§éƒ¨åˆ†ç»éªŒå›æ”¾ç¼“å†²åŒºçš„ä»£ç éƒ½éå¸¸ç›´æ¥ï¼šå®ƒåŸºæœ¬ä¸Šåˆ©ç”¨äº† deque ç±»æ¥ä¿æŒç¼“å†²åŒºä¸­çš„æŒ‡å®šæ•°é‡çš„æ¡ç›®ã€‚åœ¨ sample() æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªéšæœºç´¢å¼•çš„åˆ—è¡¨ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«ç»éªŒæ¡ç›®çš„åˆ—è¡¨ï¼Œä»¥ä¾¿é‡æ–°åŒ…è£…å¹¶è½¬æ¢ä¸ºå¼ é‡ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦çš„ç±»æ˜¯ Agentï¼Œå®ƒä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ï¼Œå¹¶å°†äº¤äº’çš„ç»“æœä¿å­˜åˆ°ä½ åˆšæ‰çœ‹åˆ°çš„ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­ï¼š

```py
class Agent: 
    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer): 
        self.env = env 
        self.exp_buffer = exp_buffer 
        self.state: tt.Optional[np.ndarray] = None 
        self._reset() 

    def _reset(self): 
        self.state, _ = env.reset() 
        self.total_reward = 0.0
```

åœ¨æ™ºèƒ½ä½“åˆå§‹åŒ–æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å­˜å‚¨å¯¹ç¯å¢ƒå’Œç»éªŒå›æ”¾ç¼“å†²åŒºçš„å¼•ç”¨ï¼Œè·Ÿè¸ªå½“å‰çš„è§‚å¯Ÿå€¼å’Œè¿„ä»Šä¸ºæ­¢ç´¯è®¡çš„æ€»å¥–åŠ±ã€‚

æ™ºèƒ½ä½“çš„ä¸»è¦æ–¹æ³•æ˜¯åœ¨ç¯å¢ƒä¸­æ‰§è¡Œä¸€æ­¥å¹¶å°†å…¶ç»“æœå­˜å‚¨åœ¨ç¼“å†²åŒºä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å…ˆé€‰æ‹©åŠ¨ä½œï¼š

```py
 @torch.no_grad() 
    def play_step(self, net: dqn_model.DQN, device: torch.device, 
                  epsilon: float = 0.0) -> tt.Optional[float]: 
        done_reward = None 

        if np.random.random() < epsilon: 
            action = env.action_space.sample() 
        else: 
            state_v = torch.as_tensor(self.state).to(device) 
            state_v.unsqueeze_(0) 
            q_vals_v = net(state_v) 
            _, act_v = torch.max(q_vals_v, dim=1) 
            action = int(act_v.item())
```

ä»¥æ¦‚ç‡ epsilonï¼ˆä½œä¸ºå‚æ•°ä¼ é€’ï¼‰ï¼Œæˆ‘ä»¬é‡‡å–éšæœºåŠ¨ä½œï¼›å¦åˆ™ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¨¡å‹æ¥è·å¾—æ‰€æœ‰å¯èƒ½åŠ¨ä½œçš„ Q å€¼ï¼Œå¹¶é€‰æ‹©æœ€ä¼˜çš„åŠ¨ä½œã€‚åœ¨æ­¤æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ PyTorch çš„ no_grad() è£…é¥°å™¨åœ¨æ•´ä¸ªæ–¹æ³•ä¸­ç¦ç”¨æ¢¯åº¦è¿½è¸ªï¼Œå› ä¸ºæˆ‘ä»¬æ ¹æœ¬ä¸éœ€è¦å®ƒä»¬ã€‚

å½“åŠ¨ä½œè¢«é€‰ä¸­åï¼Œæˆ‘ä»¬å°†å…¶ä¼ é€’ç»™ç¯å¢ƒä»¥è·å–ä¸‹ä¸€ä¸ªè§‚å¯Ÿå€¼å’Œå¥–åŠ±ï¼Œå°†æ•°æ®å­˜å‚¨åœ¨ç»éªŒç¼“å†²åŒºä¸­ï¼Œç„¶åå¤„ç†å›åˆç»“æŸçš„æƒ…å†µï¼š

```py
 new_state, reward, is_done, is_tr, _ = self.env.step(action) 
        self.total_reward += reward 

        exp = Experience( 
            state=self.state, action=action, reward=float(reward), 
            done_trunc=is_done or is_tr, new_state=new_state 
        ) 
        self.exp_buffer.append(exp) 
        self.state = new_state 
        if is_done or is_tr: 
            done_reward = self.total_reward 
            self._reset() 
        return done_reward
```

å‡½æ•°çš„ç»“æœæ˜¯æ€»çš„ç´¯è®¡å¥–åŠ±ï¼Œå¦‚æœæˆ‘ä»¬é€šè¿‡è¿™ä¸€æ­¥å·²ç»åˆ°è¾¾äº†å›åˆçš„ç»“æŸï¼Œåˆ™è¿”å›å¥–åŠ±ï¼Œå¦åˆ™è¿”å› Noneã€‚

å‡½æ•° batch_to_tensors æ¥å—ä¸€æ‰¹ Experience å¯¹è±¡ï¼Œå¹¶è¿”å›ä¸€ä¸ªåŒ…å«çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€å®Œæˆæ ‡å¿—å’Œæ–°çŠ¶æ€çš„å…ƒç»„ï¼Œè¿™äº›æ•°æ®ä¼šè¢«é‡æ–°æ‰“åŒ…ä¸ºå¯¹åº”ç±»å‹çš„ PyTorch å¼ é‡ï¼š

```py
def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors: 
    states, actions, rewards, dones, new_state = [], [], [], [], [] 
    for e in batch: 
        states.append(e.state) 
        actions.append(e.action) 
        rewards.append(e.reward) 
        dones.append(e.done_trunc) 
        new_state.append(e.new_state) 
    states_t = torch.as_tensor(np.asarray(states)) 
    actions_t = torch.LongTensor(actions) 
    rewards_t = torch.FloatTensor(rewards) 
    dones_t = torch.BoolTensor(dones) 
    new_states_t = torch.as_tensor(np.asarray(new_state)) 
    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \ 
           dones_t.to(device),  new_states_t.to(device)
```

å½“æˆ‘ä»¬å¤„ç†çŠ¶æ€æ—¶ï¼Œæˆ‘ä»¬å°½é‡é¿å…å†…å­˜å¤åˆ¶ï¼ˆé€šè¿‡ä½¿ç”¨ np.asarray()å‡½æ•°ï¼‰ï¼Œè¿™æ˜¯å¾ˆé‡è¦çš„ï¼Œå› ä¸º Atari çš„è§‚æµ‹æ•°æ®é‡å¤§ï¼ˆæ¯å¸§æœ‰ 84 Ã— 84 å­—èŠ‚ï¼Œå…±å››å¸§ï¼‰ï¼Œå¹¶ä¸”æˆ‘ä»¬æœ‰ 32 ä¸ªè¿™æ ·çš„å¯¹è±¡ã€‚å¦‚æœæ²¡æœ‰è¿™ä¸ªä¼˜åŒ–ï¼Œæ€§èƒ½ä¼šä¸‹é™å¤§çº¦ 20 å€ã€‚

ç°åœ¨æ˜¯è®­ç»ƒæ¨¡å—ä¸­æœ€åä¸€ä¸ªå‡½æ•°çš„æ—¶é—´äº†ï¼Œè¿™ä¸ªå‡½æ•°è®¡ç®—é‡‡æ ·æ‰¹æ¬¡çš„æŸå¤±ã€‚è¿™ä¸ªå‡½æ•°çš„å†™æ³•æ—¨åœ¨æœ€å¤§åŒ–åˆ©ç”¨ GPU çš„å¹¶è¡Œè®¡ç®—ï¼Œé€šè¿‡å‘é‡åŒ–æ“ä½œå¤„ç†æ‰€æœ‰æ‰¹æ¬¡æ ·æœ¬ï¼Œè¿™ä½¿å¾—å®ƒæ¯”ä¸€ä¸ªç®€å•çš„æ‰¹æ¬¡å¾ªç¯æ›´éš¾ç†è§£ã€‚ç„¶è€Œï¼Œè¿™ä¸ªä¼˜åŒ–æ˜¯å€¼å¾—çš„ï¼šå¹¶è¡Œç‰ˆæœ¬æ¯”æ˜¾å¼çš„å¾ªç¯å¿«äº†ä¸¤å€å¤šã€‚

æé†’ä¸€ä¸‹ï¼Œè¿™æ˜¯æˆ‘ä»¬éœ€è¦è®¡ç®—çš„æŸå¤±è¡¨è¾¾å¼ï¼š

![Ï€ (a |s) = P[At = a|St = s] ](img/eq24.png)

æˆ‘ä»¬ä½¿ç”¨å‰é¢çš„æ–¹ç¨‹å¤„ç†éå›åˆç»“æŸçš„æ­¥éª¤ï¼Œä½¿ç”¨ä»¥ä¸‹æ–¹ç¨‹å¤„ç†æœ€åçš„æ­¥éª¤ï¼š

![Ï€ (a |s) = P[At = a|St = s] ](img/eq25.png)

```py
def calc_loss(batch: tt.List[Experience], net: dqn_model.DQN, tgt_net: dqn_model.DQN, 
              device: torch.device) -> torch.Tensor: 
    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)
```

åœ¨è¿™äº›å‚æ•°ä¸­ï¼Œæˆ‘ä»¬ä¼ å…¥äº†æˆ‘ä»¬çš„æ‰¹æ¬¡ã€æ­£åœ¨è®­ç»ƒçš„ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œï¼Œç›®æ ‡ç½‘ç»œä¼šå®šæœŸä¸è®­ç»ƒå¥½çš„ç½‘ç»œåŒæ­¥ã€‚

ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆä½œä¸º net å‚æ•°ä¼ å…¥ï¼‰ç”¨äºè®¡ç®—æ¢¯åº¦ï¼›ç¬¬äºŒä¸ªæ¨¡å‹ï¼ˆåœ¨ tgt_net å‚æ•°ä¸­ï¼‰ç”¨äºè®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å€¼ï¼Œè¿™ä¸€è®¡ç®—ä¸åº”å½±å“æ¢¯åº¦ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨ PyTorch å¼ é‡çš„ detach()å‡½æ•°æ¥é˜²æ­¢æ¢¯åº¦æµå…¥ç›®æ ‡ç½‘ç»œçš„å›¾ä¸­ã€‚è¿™ä¸ªå‡½æ•°åœ¨ç¬¬ä¸‰ç« ä¸­æœ‰æè¿°ã€‚

åœ¨å‡½æ•°å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬è°ƒç”¨ batch_to_tensors å‡½æ•°å°†æ‰¹æ¬¡é‡æ–°æ‰“åŒ…æˆå•ç‹¬çš„å¼ é‡å˜é‡ã€‚

ä¸‹ä¸€è¡Œæœ‰ç‚¹å¤æ‚ï¼š

```py
 state_action_values = net(states_t).gather( 
        1, actions_t.unsqueeze(-1) 
    ).squeeze(-1)
```

è®©æˆ‘ä»¬è¯¦ç»†è®¨è®ºä¸€ä¸‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è§‚æµ‹æ•°æ®ä¼ å…¥ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ gather()å¼ é‡æ“ä½œæå–å·²æ‰§è¡ŒåŠ¨ä½œçš„ç‰¹å®š Q å€¼ã€‚gather()è°ƒç”¨çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯æˆ‘ä»¬å¸Œæœ›è¿›è¡Œèšåˆçš„ç»´åº¦ç´¢å¼•ï¼ˆåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå®ƒç­‰äº 1ï¼Œè¡¨ç¤ºåŠ¨ä½œç»´åº¦ï¼‰ã€‚

ç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ä¸ªå…ƒç´ ç´¢å¼•çš„å¼ é‡ï¼Œç”¨æ¥é€‰æ‹©éœ€è¦çš„å…ƒç´ ã€‚ä¸ºäº†è®¡ç®— gather()å‡½æ•°çš„ç´¢å¼•å‚æ•°å¹¶å»é™¤æˆ‘ä»¬åˆ›å»ºçš„å¤šä½™ç»´åº¦ï¼Œåˆ†åˆ«éœ€è¦é¢å¤–çš„ unsqueeze()å’Œ squeeze()è°ƒç”¨ã€‚ï¼ˆç´¢å¼•åº”ä¸æˆ‘ä»¬å¤„ç†çš„æ•°æ®å…·æœ‰ç›¸åŒçš„ç»´åº¦æ•°ã€‚ï¼‰åœ¨å›¾ 6.3 ä¸­ï¼Œæ‚¨å¯ä»¥çœ‹åˆ° gather()åœ¨ç¤ºä¾‹ä¸­çš„ä½œç”¨ï¼Œç¤ºä¾‹ä¸­æœ‰å…­ä¸ªæ¡ç›®çš„æ‰¹æ¬¡å’Œå››ä¸ªåŠ¨ä½œã€‚

![PIC](img/file33.png)

å›¾ 6.3ï¼šDQN æŸå¤±è®¡ç®—ä¸­çš„å¼ é‡è½¬æ¢

è¯·è®°ä½ï¼Œgather()åº”ç”¨äºå¼ é‡çš„ç»“æœæ˜¯ä¸€ä¸ªå¯å¾®åˆ†æ“ä½œï¼Œå®ƒä¼šä¿æŒä¸æœ€ç»ˆæŸå¤±å€¼ç›¸å…³çš„æ‰€æœ‰æ¢¯åº¦ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆè¿™ä¼šå¸¦æ¥ä¸€äº›é€Ÿåº¦æå‡ï¼‰ï¼Œå°†ç›®æ ‡ç½‘ç»œåº”ç”¨åˆ°ä¸‹ä¸€ä¸ªçŠ¶æ€çš„è§‚æµ‹ä¸­ï¼Œå¹¶æ²¿ç€ç›¸åŒè¡ŒåŠ¨ç»´åº¦ï¼ˆ1ï¼‰è®¡ç®—æœ€å¤§ Q å€¼ï¼š

```py
 with torch.no_grad(): 
        next_state_values = tgt_net(new_states_t).max(1)[0]
```

å‡½æ•° max() è¿”å›æœ€å¤§å€¼åŠå…¶ç´¢å¼•ï¼ˆå› æ­¤å®ƒåŒæ—¶è®¡ç®— max å’Œ argmaxï¼‰ï¼Œè¿™éå¸¸æ–¹ä¾¿ã€‚ç„¶è€Œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªå¯¹å€¼æ„Ÿå…´è¶£ï¼Œå› æ­¤æˆ‘ä»¬å–ç»“æœä¸­çš„ç¬¬ä¸€ä¸ªæ¡ç›®ï¼ˆæœ€å¤§å€¼ï¼‰ã€‚

ä»¥ä¸‹æ˜¯ä¸‹ä¸€è¡Œï¼š

```py
 next_state_values[dones_t] = 0.0
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¿›è¡Œä¸€ä¸ªç®€å•ä½†éå¸¸é‡è¦çš„è½¬æ¢ï¼šå¦‚æœæ‰¹æ¬¡ä¸­çš„è¿‡æ¸¡æ¥è‡ªå›åˆçš„æœ€åä¸€æ­¥ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„åŠ¨ä½œå€¼å°±æ²¡æœ‰ä¸‹ä¸€ä¸ªçŠ¶æ€çš„æŠ˜æ‰£å¥–åŠ±ï¼Œå› ä¸ºæ²¡æœ‰ä¸‹ä¸€ä¸ªçŠ¶æ€å¯ä»¥è·å–å¥–åŠ±ã€‚è¿™çœ‹èµ·æ¥å¯èƒ½æ˜¯å°äº‹ï¼Œä½†åœ¨å®é™…ä¸­éå¸¸é‡è¦ï¼›æ²¡æœ‰è¿™ä¸€ç‚¹ï¼Œè®­ç»ƒå°†æ— æ³•æ”¶æ•›ï¼ˆæˆ‘ä¸ªäººèŠ±äº†å‡ ä¸ªå°æ—¶è°ƒè¯•è¿™ä¸ªé—®é¢˜ï¼‰ã€‚

åœ¨ä¸‹ä¸€è¡Œä¸­ï¼Œæˆ‘ä»¬å°†å€¼ä»å…¶è®¡ç®—å›¾ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œä»¥é˜²æ­¢æ¢¯åº¦æµå…¥ç”¨äºè®¡ç®—ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ Q è¿‘ä¼¼å€¼çš„ç¥ç»ç½‘ç»œï¼š

```py
 next_state_values = next_state_values.detach()
```

è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºå¦‚æœæ²¡æœ‰è¿™ä¸ªï¼Œæˆ‘ä»¬çš„æŸå¤±åå‘ä¼ æ’­å°†å¼€å§‹å½±å“å½“å‰çŠ¶æ€å’Œä¸‹ä¸€ä¸ªçŠ¶æ€çš„é¢„æµ‹ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¸æƒ³è§¦åŠä¸‹ä¸€ä¸ªçŠ¶æ€çš„é¢„æµ‹ï¼Œå› ä¸ºå®ƒä»¬åœ¨è´å°”æ›¼æ–¹ç¨‹ä¸­ç”¨äºè®¡ç®—å‚è€ƒ Q å€¼ã€‚ä¸ºäº†é˜»æ­¢æ¢¯åº¦æµå…¥å›¾çš„è¿™ä¸ªåˆ†æ”¯ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ é‡çš„ detach() æ–¹æ³•ï¼Œè¯¥æ–¹æ³•è¿”å›ä¸€ä¸ªæ²¡æœ‰è¿æ¥åˆ°è®¡ç®—å†å²çš„å¼ é‡ã€‚

æœ€åï¼Œæˆ‘ä»¬è®¡ç®—è´å°”æ›¼è¿‘ä¼¼å€¼å’Œå‡æ–¹è¯¯å·®æŸå¤±ï¼š

```py
 expected_state_action_values = next_state_values * GAMMA + rewards_t 
    return nn.MSELoss()(state_action_values, expected_state_action_values)
```

ä¸ºäº†å…¨é¢äº†è§£æŸå¤±å‡½æ•°è®¡ç®—ä»£ç ï¼Œè®©æˆ‘ä»¬å®Œæ•´æŸ¥çœ‹è¿™ä¸ªå‡½æ•°ï¼š

```py
def calc_loss(batch: tt.List[Experience], net: dqn_model.DQN, tgt_net: dqn_model.DQN, 
              device: torch.device) -> torch.Tensor: 
    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device) 

    state_action_values = net(states_t).gather( 
        1, actions_t.unsqueeze(-1) 
    ).squeeze(-1) 
    with torch.no_grad(): 
        next_state_values = tgt_net(new_states_t).max(1)[0] 
        next_state_values[dones_t] = 0.0 
        next_state_values = next_state_values.detach() 

    expected_state_action_values = next_state_values * GAMMA + rewards_t 
    return nn.MSELoss()(state_action_values, expected_state_action_values)
```

è¿™ç»“æŸäº†æˆ‘ä»¬çš„æŸå¤±å‡½æ•°è®¡ç®—ã€‚

å‰©ä¸‹çš„ä»£ç æ˜¯æˆ‘ä»¬çš„è®­ç»ƒå¾ªç¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‘½ä»¤è¡Œå‚æ•°è§£æå™¨ï¼š

```py
if __name__ == "__main__": 
    parser = argparse.ArgumentParser() 
    parser.add_argument("--dev", default="cpu", help="Device name, default=cpu") 
    parser.add_argument("--env", default=DEFAULT_ENV_NAME, 
                        help="Name of the environment, default=" + DEFAULT_ENV_NAME) 
    args = parser.parse_args() 
    device = torch.device(args.dev)
```

æˆ‘ä»¬çš„è„šæœ¬å…è®¸æˆ‘ä»¬æŒ‡å®šä¸€ä¸ªç”¨äºè®¡ç®—çš„è®¾å¤‡ï¼Œå¹¶åœ¨ä¸é»˜è®¤ç¯å¢ƒä¸åŒçš„ç¯å¢ƒä¸­è¿›è¡Œè®­ç»ƒã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åˆ›å»ºæˆ‘ä»¬çš„ç¯å¢ƒï¼š

```py
 env = wrappers.make_env(args.env) 
    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device) 
    tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)
```

æˆ‘ä»¬çš„ç¯å¢ƒå·²ç»åº”ç”¨äº†æ‰€æœ‰å¿…éœ€çš„åŒ…è£…å™¨ï¼Œæˆ‘ä»¬å°†è®­ç»ƒçš„ç¥ç»ç½‘ç»œå’Œå…·æœ‰ç›¸åŒæ¶æ„çš„ç›®æ ‡ç½‘ç»œã€‚æœ€åˆï¼Œå®ƒä»¬ä¼šç”¨ä¸åŒçš„éšæœºæƒé‡åˆå§‹åŒ–ï¼Œä½†è¿™å¹¶ä¸é‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šæ¯ 1k å¸§åŒæ­¥ä¸€æ¬¡å®ƒä»¬ï¼Œè¿™å¤§è‡´å¯¹åº”ä¸€ä¸ª Pong å›åˆã€‚

ç„¶åï¼Œæˆ‘ä»¬åˆ›å»ºæ‰€éœ€å¤§å°çš„ç»éªŒå›æ”¾ç¼“å†²åŒºï¼Œå¹¶å°†å…¶ä¼ é€’ç»™æ™ºèƒ½ä½“ï¼š

```py
 writer = SummaryWriter(comment="-" + args.env) 
    print(net) 
    buffer = ExperienceBuffer(REPLAY_SIZE) 
    agent = Agent(env, buffer) 
    epsilon = EPSILON_START
```

Epsilon åˆå§‹å€¼ä¸º 1.0ï¼Œä½†ä¼šåœ¨æ¯æ¬¡è¿­ä»£æ—¶å‡å°ã€‚ä»¥ä¸‹æ˜¯è®­ç»ƒå¾ªç¯å¼€å§‹å‰æˆ‘ä»¬æ‰€åšçš„æœ€åå‡ ä»¶äº‹ï¼š

```py
 optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE) 
    total_rewards = [] 
    frame_idx = 0 
    ts_frame = 0 
    ts = time.time() 
    best_m_reward = None
```

æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªä¼˜åŒ–å™¨ã€ä¸€ä¸ªç”¨äºå­˜å‚¨å®Œæ•´å›åˆå¥–åŠ±çš„ç¼“å†²åŒºã€ä¸€ä¸ªå¸§è®¡æ•°å™¨å’Œå‡ ä¸ªå˜é‡æ¥è·Ÿè¸ªæˆ‘ä»¬çš„é€Ÿåº¦ï¼Œä»¥åŠè¾¾åˆ°çš„æœ€ä½³å¹³å‡å¥–åŠ±ã€‚æ¯å½“æˆ‘ä»¬çš„å¹³å‡å¥–åŠ±çªç ´è®°å½•æ—¶ï¼Œæˆ‘ä»¬ä¼šå°†æ¨¡å‹ä¿å­˜åˆ°æ–‡ä»¶ä¸­ã€‚

åœ¨è®­ç»ƒå¾ªç¯å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬ä¼šè®¡ç®—å®Œæˆçš„è¿­ä»£æ¬¡æ•°ï¼Œå¹¶æ ¹æ®æˆ‘ä»¬çš„è®¡åˆ’é™ä½ epsilon å€¼ï¼š

```py
 while True: 
        frame_idx += 1 
        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)
```

Epsilon ä¼šåœ¨ç»™å®šçš„å¸§æ•°ï¼ˆEPSILON_DECAY_LAST_FRAME=150kï¼‰å†…çº¿æ€§ä¸‹é™ï¼Œç„¶åä¿æŒåœ¨ç›¸åŒçš„æ°´å¹³ï¼Œå³ EPSILON_FINAL=0.01ã€‚

åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬è®©æˆ‘ä»¬çš„æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­æ‰§è¡Œä¸€æ­¥æ“ä½œï¼ˆä½¿ç”¨å½“å‰çš„ç½‘ç»œå’Œ epsilon çš„å€¼ï¼‰ï¼š

```py
 reward = agent.play_step(net, device, epsilon) 
        if reward is not None: 
            total_rewards.append(reward) 
            speed = (frame_idx - ts_frame) / (time.time() - ts) 
            ts_frame = frame_idx 
            ts = time.time() 
            m_reward = np.mean(total_rewards[-100:]) 
            print(f"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, " 
                  f"eps {epsilon:.2f}, speed {speed:.2f} f/s") 
            writer.add_scalar("epsilon", epsilon, frame_idx) 
            writer.add_scalar("speed", speed, frame_idx) 
            writer.add_scalar("reward_100", m_reward, frame_idx) 
            writer.add_scalar("reward", reward, frame_idx)
```

è¯¥å‡½æ•°ä»…åœ¨æ­¤æ­¥éª¤ä¸ºå›åˆçš„æœ€åä¸€æ­¥æ—¶è¿”å›æµ®åŠ¨å€¼ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æŠ¥å‘Šæˆ‘ä»¬çš„è¿›åº¦ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è®¡ç®—å¹¶æ˜¾ç¤ºä»¥ä¸‹å€¼ï¼Œåœ¨æ§åˆ¶å°å’Œ TensorBoard ä¸­å±•ç¤ºï¼š

+   é€Ÿåº¦ï¼ˆæ¯ç§’å¤„ç†çš„å¸§æ•°ï¼‰

+   å·²è¿›è¡Œçš„å›åˆæ•°

+   è¿‡å» 100 ä¸ªå›åˆçš„å¹³å‡å¥–åŠ±

+   å½“å‰ epsilon çš„å€¼

æ¯å½“è¿‡å» 100 ä¸ªå›åˆçš„å¹³å‡å¥–åŠ±è¾¾åˆ°æœ€å¤§å€¼æ—¶ï¼Œæˆ‘ä»¬ä¼šæŠ¥å‘Šè¿™ä¸€æƒ…å†µå¹¶ä¿å­˜æ¨¡å‹å‚æ•°ï¼š

```py
 if best_m_reward is None or best_m_reward < m_reward: 
                torch.save(net.state_dict(), args.env + "-best_%.0f.dat" % m_reward) 
                if best_m_reward is not None: 
                    print(f"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}") 
                best_m_reward = m_reward 
            if m_reward > MEAN_REWARD_BOUND: 
                print("Solved in %d frames!" % frame_idx) 
                break
```

å¦‚æœæˆ‘ä»¬çš„å¹³å‡å¥–åŠ±è¶…è¿‡äº†æŒ‡å®šçš„è¾¹ç•Œï¼Œåˆ™åœæ­¢è®­ç»ƒã€‚å¯¹äº Pongï¼Œè¾¹ç•Œæ˜¯ 19.0ï¼Œæ„å‘³ç€ä» 21 åœºæ¯”èµ›ä¸­èµ¢å¾—è¶…è¿‡ 19 åœºã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ£€æŸ¥æˆ‘ä»¬çš„ç¼“å†²åŒºæ˜¯å¦è¶³å¤Ÿå¤§ï¼Œèƒ½å¤Ÿè¿›è¡Œè®­ç»ƒï¼š

```py
 if len(buffer) < REPLAY_START_SIZE: 
            continue 
        if frame_idx % SYNC_TARGET_FRAMES == 0: 
            tgt_net.load_state_dict(net.state_dict())
```

é¦–å…ˆï¼Œæˆ‘ä»¬åº”è¯¥ç­‰å¾…è¶³å¤Ÿçš„æ•°æ®ç§¯ç´¯ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­æ˜¯ 10k æ¬¡è¿‡æ¸¡ã€‚ä¸‹ä¸€ä¸ªæ¡ä»¶æ˜¯åœ¨æ¯ä¸ª SYNC_TARGET_FRAMESï¼ˆé»˜è®¤æ˜¯ 1kï¼‰å‘¨æœŸåï¼Œä»ä¸»ç½‘ç»œåŒæ­¥å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œã€‚

è®­ç»ƒå¾ªç¯çš„æœ€åä¸€éƒ¨åˆ†éå¸¸ç®€å•ï¼Œä½†éœ€è¦èŠ±è´¹æœ€å¤šçš„æ—¶é—´æ¥æ‰§è¡Œï¼š

```py
 optimizer.zero_grad() 
        batch = buffer.sample(BATCH_SIZE) 
        loss_t = calc_loss(batch, net, tgt_net, device) 
        loss_t.backward() 
        optimizer.step()
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†æ¢¯åº¦å½’é›¶ï¼Œä»ç»éªŒé‡æ”¾ç¼“å†²åŒºä¸­é‡‡æ ·æ•°æ®æ‰¹æ¬¡ï¼Œè®¡ç®—æŸå¤±ï¼Œå¹¶æ‰§è¡Œä¼˜åŒ–æ­¥éª¤ä»¥æœ€å°åŒ–æŸå¤±ã€‚

## è¿è¡Œä¸æ€§èƒ½

è¿™ä¸ªä¾‹å­å¯¹èµ„æºè¦æ±‚è¾ƒé«˜ã€‚åœ¨ Pong ä¸Šï¼Œå®ƒéœ€è¦å¤§çº¦ 400k å¸§æ‰èƒ½è¾¾åˆ°å¹³å‡å¥–åŠ± 17ï¼ˆè¿™æ„å‘³ç€èµ¢å¾—è¶…è¿‡ 80% çš„æ¯”èµ›ï¼‰ã€‚ä¸ºäº†ä» 17 æå‡åˆ° 19ï¼Œç±»ä¼¼æ•°é‡çš„å¸§ä¹Ÿä¼šè¢«æ¶ˆè€—ï¼Œå› ä¸ºæˆ‘ä»¬çš„å­¦ä¹ è¿›åº¦å°†é¥±å’Œï¼Œæ¨¡å‹å¾ˆéš¾â€œæ¶¦è‰²ç­–ç•¥â€å¹¶è¿›ä¸€æ­¥æé«˜åˆ†æ•°ã€‚å› æ­¤ï¼Œå¹³å‡æ¥è¯´ï¼Œéœ€è¦ä¸€ç™¾ä¸‡ä¸ªæ¸¸æˆå¸§æ¥å®Œå…¨è®­ç»ƒå®ƒã€‚åœ¨ GTX 1080Ti ä¸Šï¼Œæˆ‘çš„é€Ÿåº¦å¤§çº¦æ˜¯ 250 å¸§æ¯ç§’ï¼Œå¤§çº¦éœ€è¦ä¸€ä¸ªå°æ—¶çš„è®­ç»ƒã€‚åœ¨ CPUï¼ˆi5-7600kï¼‰ä¸Šï¼Œé€Ÿåº¦è¦æ…¢å¾—å¤šï¼Œå¤§çº¦æ˜¯ 40 å¸§æ¯ç§’ï¼Œè®­ç»ƒå°†éœ€è¦å¤§çº¦ä¸ƒå°æ—¶ã€‚è®°ä½ï¼Œè¿™ä¸ªæ˜¯åœ¨ Pong ä¸Šï¼ŒPong ç›¸å¯¹å®¹æ˜“è§£å†³ã€‚å…¶ä»–æ¸¸æˆå¯èƒ½éœ€è¦æ•°äº¿å¸§å’Œä¸€ä¸ªå¤§ 100 å€çš„ç»éªŒé‡æ”¾ç¼“å†²åŒºã€‚

åœ¨ç¬¬å…«ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨è‡ª 2015 å¹´ä»¥æ¥ç ”ç©¶äººå‘˜å‘ç°çš„å„ç§æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥å¸®åŠ©æé«˜è®­ç»ƒé€Ÿåº¦å’Œæ•°æ®æ•ˆç‡ã€‚ç¬¬ä¹ç« å°†ä¸“æ³¨äºåŠ é€Ÿ RL æ–¹æ³•æ€§èƒ½çš„å·¥ç¨‹æŠ€å·§ã€‚å°½ç®¡å¦‚æ­¤ï¼Œé’ˆå¯¹ Atariï¼Œä½ ä»ç„¶éœ€è¦èµ„æºå’Œè€å¿ƒã€‚ä»¥ä¸‹å›¾è¡¨å±•ç¤ºäº†è®­ç»ƒè¿‡ç¨‹ä¸­å¥–åŠ±åŠ¨æ€çš„å˜åŒ–ï¼š

![å›¾ç‰‡](img/B22150_06_04.png)

å›¾ 6.4ï¼šè®¡ç®—è¿‡å» 100 ä¸ªå›åˆçš„å¹³å‡å¥–åŠ±åŠ¨æ€

ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ§åˆ¶å°è¾“å‡ºï¼ˆè¿™é‡Œåªå±•ç¤ºè¾“å‡ºçš„å¼€å§‹éƒ¨åˆ†ï¼‰ï¼š

```py
Chapter06$ ./02_dqn_pong.py --dev cuda 
A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7) 
[Powered by Stella] 
DQN( 
Â Â (conv): Sequential( 
Â Â Â (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4)) 
Â Â Â (1): ReLU() 
Â Â Â (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2)) 
Â Â Â (3): ReLU() 
Â Â Â (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1)) 
Â Â Â (5): ReLU() 
Â Â Â (6): Flatten(start_dim=1, end_dim=-1) 
Â Â ) 
Â Â (fc): Sequential( 
Â Â Â (0): Linear(in_features=3136, out_features=512, bias=True) 
Â Â Â (1): ReLU() 
Â Â Â (2): Linear(in_features=512, out_features=6, bias=True) 
Â Â ) 
) 
940: done 1 games, reward -21.000, eps 0.99, speed 1214.95 f/s 
1946: done 2 games, reward -20.000, eps 0.99, speed 1420.09 f/s 
Best reward updated -21.000 -> -20.000 
2833: done 3 games, reward -20.000, eps 0.98, speed 1416.26 f/s 
3701: done 4 games, reward -20.000, eps 0.98, speed 1421.84 f/s 
4647: done 5 games, reward -20.200, eps 0.97, speed 1421.63 f/s 
5409: done 6 games, reward -20.333, eps 0.96, speed 1395.67 f/s 
6171: done 7 games, reward -20.429, eps 0.96, speed 1411.90 f/s 
7063: done 8 games, reward -20.375, eps 0.95, speed 1404.49 f/s 
7882: done 9 games, reward -20.444, eps 0.95, speed 1388.26 f/s 
8783: done 10 games, reward -20.400, eps 0.94, speed 1283.64 f/s 
9545: done 11 games, reward -20.455, eps 0.94, speed 1376.47 f/s 
10307: done 12 games, reward -20.500, eps 0.93, speed 431.94 f/s 
11362: done 13 games, reward -20.385, eps 0.92, speed 276.14 f/s 
12420: done 14 games, reward -20.214, eps 0.92, speed 276.44 f/s
```

åœ¨å‰ 10k æ­¥ä¸­ï¼Œæˆ‘ä»¬çš„é€Ÿåº¦éå¸¸å¿«ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰è¿›è¡Œä»»ä½•è®­ç»ƒï¼Œè€Œè®­ç»ƒæ˜¯æˆ‘ä»¬ä»£ç ä¸­æœ€æ˜‚è´µçš„æ“ä½œã€‚10k æ­¥åï¼Œæˆ‘ä»¬å¼€å§‹é‡‡æ ·è®­ç»ƒæ‰¹æ¬¡ï¼Œæ€§èƒ½ä¸‹é™åˆ°æ›´å…·ä»£è¡¨æ€§çš„æ•°å­—ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ€§èƒ½ä¹Ÿä¼šç•¥æœ‰ä¸‹é™ï¼Œè¿™åªæ˜¯å› ä¸ºğœ–çš„å‡å°ã€‚å½“ğœ–è¾ƒé«˜æ—¶ï¼ŒåŠ¨ä½œæ˜¯éšæœºé€‰æ‹©çš„ã€‚å½“ğœ–æ¥è¿‘é›¶æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ‰§è¡Œæ¨ç†æ¥è·å¾— Q å€¼ä»¥è¿›è¡ŒåŠ¨ä½œé€‰æ‹©ï¼Œè¿™ä¹Ÿä¼šæ¶ˆè€—æ—¶é—´ã€‚

å‡ ååœºæ¸¸æˆä¹‹åï¼Œæˆ‘ä»¬çš„ DQN åº”è¯¥å¼€å§‹å¼„æ˜ç™½å¦‚ä½•åœ¨ 21 åœºæ¯”èµ›ä¸­èµ¢å¾— 1 åˆ° 2 åœºï¼Œå¹¶ä¸”å¹³å‡å¥–åŠ±å¼€å§‹å¢é•¿ï¼ˆé€šå¸¸åœ¨ğœ– = 0.5 æ—¶å‘ç”Ÿï¼‰ï¼š

```py
66024: done 68 games, reward -20.162, eps 0.56, speed 260.89 f/s 
67338: done 69 games, reward -20.130, eps 0.55, speed 257.63 f/s 
68440: done 70 games, reward -20.100, eps 0.54, speed 260.17 f/s 
69467: done 71 games, reward -20.113, eps 0.54, speed 260.02 f/s 
70792: done 72 games, reward -20.125, eps 0.53, speed 258.88 f/s 
72031: done 73 games, reward -20.123, eps 0.52, speed 259.54 f/s 
73314: done 74 games, reward -20.095, eps 0.51, speed 258.16 f/s 
74815: done 75 games, reward -20.053, eps 0.50, speed 257.56 f/s 
76339: done 76 games, reward -20.026, eps 0.49, speed 256.79 f/s 
77576: done 77 games, reward -20.013, eps 0.48, speed 257.86 f/s 
78978: done 78 games, reward -19.974, eps 0.47, speed 255.90 f/s 
80093: done 79 games, reward -19.962, eps 0.47, speed 256.84 f/s 
81565: done 80 games, reward -19.938, eps 0.46, speed 256.34 f/s 
83365: done 81 games, reward -19.901, eps 0.44, speed 254.22 f/s 
84841: done 82 games, reward -19.878, eps 0.43, speed 254.80 f/s
```

æœ€åï¼Œç»è¿‡æ›´å¤šçš„æ¯”èµ›ï¼Œæˆ‘ä»¬çš„ DQN ç»ˆäºèƒ½å¤Ÿä¸»å®°å¹¶å‡»è´¥ï¼ˆä¸å¤ªå¤æ‚çš„ï¼‰å†…ç½® Pong AI å¯¹æ‰‹ï¼š

```py
737860: done 371 games, reward 18.540, eps 0.01, speed 225.22 f/s 
739935: done 372 games, reward 18.650, eps 0.01, speed 232.70 f/s 
Best reward updated 18.610 -> 18.650 
741910: done 373 games, reward 18.650, eps 0.01, speed 231.66 f/s 
743964: done 374 games, reward 18.760, eps 0.01, speed 231.59 f/s 
Best reward updated 18.650 -> 18.760 
745939: done 375 games, reward 18.770, eps 0.01, speed 223.45 f/s 
Best reward updated 18.760 -> 18.770 
747950: done 376 games, reward 18.810, eps 0.01, speed 229.84 f/s 
Best reward updated 18.770 -> 18.810 
749925: done 377 games, reward 18.810, eps 0.01, speed 228.05 f/s 
752008: done 378 games, reward 18.910, eps 0.01, speed 225.41 f/s 
Best reward updated 18.810 -> 18.910 
753983: done 379 games, reward 18.920, eps 0.01, speed 229.75 f/s 
Best reward updated 18.910 -> 18.920 
755958: done 380 games, reward 19.030, eps 0.01, speed 228.71 f/s 
Best reward updated 18.920 -> 19.030 
Solved in 755958 frames!
```

ç”±äºè®­ç»ƒè¿‡ç¨‹ä¸­å­˜åœ¨éšæœºæ€§ï¼Œä½ çš„å®é™…åŠ¨æ€å¯èƒ½ä¸è¿™é‡Œå±•ç¤ºçš„ä¸åŒã€‚åœ¨ä¸€äº›ç½•è§çš„æƒ…å†µä¸‹ï¼ˆæ ¹æ®æˆ‘çš„å®éªŒï¼Œå¤§çº¦ 10 æ¬¡è¿è¡Œä¸­æœ‰ 1 æ¬¡ï¼‰ï¼Œè®­ç»ƒæ ¹æœ¬æ— æ³•æ”¶æ•›ï¼Œè¡¨ç°ä¸ºé•¿æ—¶é—´çš„å¥–åŠ±å§‹ç»ˆä¸ºâˆ’21ã€‚è¿™åœ¨æ·±åº¦å­¦ä¹ ä¸­å¹¶ä¸ç½•è§ï¼ˆç”±äºè®­ç»ƒçš„éšæœºæ€§ï¼‰ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¯èƒ½ä¼šæ›´å¸¸è§ï¼ˆç”±äºç¯å¢ƒäº¤äº’çš„é¢å¤–éšæœºæ€§ï¼‰ã€‚å¦‚æœä½ çš„è®­ç»ƒåœ¨å‰ 100k åˆ° 200k æ¬¡è¿­ä»£ä¸­æ²¡æœ‰ä»»ä½•æ­£å‘åŠ¨æ€ï¼Œä½ åº”è¯¥é‡æ–°å¼€å§‹è®­ç»ƒã€‚

## ä½ çš„æ¨¡å‹åœ¨å®è·µä¸­çš„è¡¨ç°

è®­ç»ƒè¿‡ç¨‹åªæ˜¯æ•´ä¸ªè¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ã€‚æˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡ä¸ä»…ä»…æ˜¯è®­ç»ƒæ¨¡å‹ï¼›æˆ‘ä»¬è¿˜å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹èƒ½ä»¥è‰¯å¥½çš„ç»“æœæ¥ç©æ¸¸æˆã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡æ›´æ–°è¿‡å» 100 åœºæ¸¸æˆçš„å¹³å‡å¥–åŠ±æœ€å¤§å€¼æ—¶ï¼Œæˆ‘ä»¬éƒ½ä¼šå°†æ¨¡å‹ä¿å­˜åˆ°æ–‡ä»¶ PongNoFrameskip-v4-best_<score>.dat ä¸­ã€‚åœ¨ Chapter06/03_dqn_play.py æ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªç¨‹åºå¯ä»¥åŠ è½½è¿™ä¸ªæ¨¡å‹æ–‡ä»¶å¹¶è¿›è¡Œä¸€æ¬¡æ¸¸æˆï¼Œå±•ç¤ºæ¨¡å‹çš„åŠ¨æ€ã€‚

ä»£ç éå¸¸ç®€å•ï¼Œä½†çœ‹åˆ°å‡ ä¸ªçŸ©é˜µï¼ˆä»…æœ‰ç™¾ä¸‡ä¸ªå‚æ•°ï¼‰é€šè¿‡è§‚å¯Ÿåƒç´ ï¼Œèƒ½å¤Ÿä»¥è¶…äººç²¾åº¦ç© Pong æ¸¸æˆï¼Œç®€ç›´åƒé­”æ³•ä¸€æ ·ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥ç†Ÿæ‚‰çš„ PyTorch å’Œ Gym æ¨¡å—ï¼š

```py
import gymnasium as gym 
import argparse 
import numpy as np 
import typing as tt 

import torch 

from lib import wrappers 
from lib import dqn_model 

import collections 

DEFAULT_ENV_NAME = "PongNoFrameskip-v4"
```

è„šæœ¬æ¥å—å·²ä¿å­˜æ¨¡å‹çš„æ–‡ä»¶åï¼Œå¹¶å…è®¸æŒ‡å®š Gym ç¯å¢ƒï¼ˆå½“ç„¶ï¼Œæ¨¡å‹å’Œç¯å¢ƒå¿…é¡»åŒ¹é…ï¼‰ï¼š

```py
if __name__ == "__main__": 
    parser = argparse.ArgumentParser() 
    parser.add_argument("-m", "--model", required=True, help="Model file to load") 
    parser.add_argument("-e", "--env", default=DEFAULT_ENV_NAME, 
                        help="Environment name to use, default=" + DEFAULT_ENV_NAME) 
    parser.add_argument("-r", "--record", required=True, help="Directory for video") 
    args = parser.parse_args()
```

æ­¤å¤–ï¼Œä½ è¿˜éœ€è¦ä¼ é€’é€‰é¡¹ -rï¼Œå¹¶æŒ‡å®šä¸€ä¸ªä¸å­˜åœ¨çš„ç›®å½•åï¼Œç³»ç»Ÿå°†æŠŠä½ æ¸¸æˆçš„å½•åƒä¿å­˜åœ¨è¯¥ç›®å½•ä¸‹ã€‚

ä»¥ä¸‹ä»£ç ä¹Ÿä¸å¤ªå¤æ‚ï¼š

```py
 env = wrappers.make_env(args.env, render_mode="rgb_array") 
    env = gym.wrappers.RecordVideo(env, video_folder=args.record) 
    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n) 
    state = torch.load(args.model, map_location=lambda stg, _: stg) 
    net.load_state_dict(state) 

    state, _ = env.reset() 
    total_reward = 0.0 
    c: tt.Dict[int, int] = collections.Counter()
```

æˆ‘ä»¬åˆ›å»ºç¯å¢ƒï¼Œå°†å…¶åŒ…è£…åœ¨ RecordVideo å°è£…å™¨ä¸­ï¼Œåˆ›å»ºæ¨¡å‹ï¼Œç„¶åä»ä¼ å…¥çš„æ–‡ä»¶ä¸­åŠ è½½æƒé‡ã€‚ä¼ é€’ç»™ torch.load() å‡½æ•°çš„ map_location å‚æ•°ç”¨äºå°†åŠ è½½çš„å¼ é‡ä½ç½®ä» GPU æ˜ å°„åˆ° CPUã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œtorch å°è¯•åŠ è½½ä¸ä¿å­˜æ—¶ç›¸åŒè®¾å¤‡ä¸Šçš„å¼ é‡ï¼Œä½†å¦‚æœä½ å°†æ¨¡å‹ä»ç”¨äºè®­ç»ƒçš„æœºå™¨ï¼ˆæœ‰ GPUï¼‰å¤åˆ¶åˆ°æ²¡æœ‰ GPU çš„ç¬”è®°æœ¬ç”µè„‘ä¸Šï¼Œå°±éœ€è¦é‡æ–°æ˜ å°„ä½ç½®ã€‚æˆ‘ä»¬çš„ç¤ºä¾‹å®Œå…¨ä¸ä½¿ç”¨ GPUï¼Œå› ä¸ºæ¨ç†é€Ÿåº¦è¶³å¤Ÿå¿«ï¼Œä¸éœ€è¦åŠ é€Ÿã€‚

è¿™æ˜¯å‡ ä¹ä¸è®­ç»ƒä»£ç ä¸­çš„ Agent ç±»çš„ play_step() æ–¹æ³•å®Œå…¨ç›¸åŒçš„ä»£ç ï¼Œä¸åŒ…å« epsilon-greedy åŠ¨ä½œé€‰æ‹©ï¼š

```py
 while True: 
        state_v = torch.tensor([state]) 
        q_vals = net(state_v).data.numpy()[0] 
        action = int(np.argmax(q_vals)) 
        c[action] += 1
```

æˆ‘ä»¬åªéœ€å°†è§‚å¯Ÿç»“æœä¼ é€’ç»™æ™ºèƒ½ä½“ï¼Œå¹¶é€‰æ‹©å…·æœ‰æœ€å¤§å€¼çš„åŠ¨ä½œã€‚

å‰©ä½™çš„ä»£ç ä¹Ÿå¾ˆç®€å•ï¼š

```py
 state, reward, is_done, is_trunc, _ = env.step(action) 
        total_reward += reward 
        if is_done or is_trunc: 
            break 
    print("Total reward: %.2f" % total_reward) 
    print("Action counts:", c) 
    env.close()
```

æˆ‘ä»¬å°†åŠ¨ä½œä¼ é€’ç»™ç¯å¢ƒï¼Œè®¡ç®—æ€»å¥–åŠ±ï¼Œå¹¶åœ¨å›åˆç»“æŸæ—¶åœæ­¢å¾ªç¯ã€‚å›åˆç»“æŸåï¼Œæˆ‘ä»¬æ˜¾ç¤ºæ€»å¥–åŠ±å’Œæ™ºèƒ½ä½“æ‰§è¡ŒåŠ¨ä½œçš„æ¬¡æ•°ã€‚

åœ¨è¿™ä¸ª YouTube æ’­æ”¾åˆ—è¡¨ä¸­ï¼Œä½ å¯ä»¥æ‰¾åˆ°åœ¨è®­ç»ƒä¸åŒé˜¶æ®µçš„æ¸¸æˆå½•åƒï¼š[`www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu`](https://www.youtube.com/playlist?list=PLMVwuZENsfJklt4vCltrWq0KV9aEZ3ylu)ã€‚

# å°è¯•çš„äº‹é¡¹

å¦‚æœä½ æ„Ÿåˆ°å¥½å¥‡å¹¶æƒ³è‡ªå·±å°è¯•æœ¬ç« çš„å†…å®¹ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å€¼å¾—æ¢ç´¢çš„æ–¹å‘ã€‚éœ€è¦è­¦å‘Šçš„æ˜¯ï¼šè¿™äº›å®éªŒå¯èƒ½ä¼šè€—è´¹å¤§é‡æ—¶é—´ï¼Œå¹¶ä¸”åœ¨å®éªŒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°æŒ«æŠ˜ã€‚ç„¶è€Œï¼Œä»å®è·µè§’åº¦çœ‹ï¼Œè¿™äº›å®éªŒæ˜¯æŒæ¡ææ–™çš„éå¸¸æœ‰æ•ˆçš„æ–¹å¼ï¼š

+   å°è¯•ä» Atari å¥—ä»¶ä¸­é€‰æ‹©å…¶ä»–æ¸¸æˆï¼Œä¾‹å¦‚ã€ŠBreakoutã€‹ã€ã€ŠAtlantisã€‹æˆ–ã€ŠRiver Raidã€‹ï¼ˆæˆ‘ç«¥å¹´çš„æœ€çˆ±ï¼‰ã€‚è¿™å¯èƒ½éœ€è¦è°ƒæ•´è¶…å‚æ•°ã€‚

+   ä½œä¸º FrozenLake çš„æ›¿ä»£æ–¹æ¡ˆï¼Œè¿˜æœ‰å¦ä¸€ä¸ªè¡¨æ ¼ç¯å¢ƒï¼ŒTaxiï¼Œå®ƒæ¨¡æ‹Ÿä¸€ä¸ªå‡ºç§Ÿè½¦å¸æœºéœ€è¦æ¥é€ä¹˜å®¢å¹¶å°†å…¶é€åˆ°ç›®çš„åœ°çš„åœºæ™¯ã€‚

+   å°è¯•è°ƒæ•´ Pong çš„è¶…å‚æ•°ã€‚æ˜¯å¦æœ‰å¯èƒ½è®­ç»ƒå¾—æ›´å¿«ï¼ŸOpenAI å£°ç§°ä½¿ç”¨å¼‚æ­¥ä¼˜åŠ¿æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ï¼ˆæœ¬ä¹¦ç¬¬ä¸‰éƒ¨åˆ†çš„ä¸»é¢˜ï¼‰å¯ä»¥åœ¨ 30 åˆ†é’Ÿå†…è§£å†³ Pong é—®é¢˜ã€‚ä¹Ÿè®¸ç”¨ DQN ä¹Ÿèƒ½åšåˆ°ã€‚

+   ä½ èƒ½è®© DQN è®­ç»ƒä»£ç æ›´å¿«å—ï¼ŸOpenAI Baselines é¡¹ç›®å·²ç»åœ¨ GTX 1080 Ti ä¸Šä½¿ç”¨ TensorFlow å®ç°äº† 350 FPSã€‚å› æ­¤ï¼Œä¼˜åŒ– PyTorch ä»£ç æ˜¯å¯èƒ½çš„ã€‚æˆ‘ä»¬å°†åœ¨ç¬¬ä¹ç« è®¨è®ºè¿™ä¸ªè¯é¢˜ï¼Œä½†åŒæ—¶ä½ ä¹Ÿå¯ä»¥è¿›è¡Œè‡ªå·±çš„å®éªŒã€‚

+   åœ¨è§†é¢‘å½•åˆ¶ä¸­ï¼Œä½ å¯èƒ½ä¼šæ³¨æ„åˆ°ï¼Œå¹³å‡åˆ†æ¥è¿‘é›¶çš„æ¨¡å‹è¡¨ç°å¾—ç›¸å½“ä¸é”™ã€‚å®é™…ä¸Šï¼Œæˆ‘æœ‰ä¸€ç§å°è±¡ï¼Œå¹³å‡åˆ†åœ¨ 10-19 ä¹‹é—´çš„æ¨¡å‹è¡¨ç°å¾—ä¸å¦‚è¿™äº›æ¨¡å‹ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºæ¨¡å‹å¯¹ç‰¹å®šçš„æ¸¸æˆæƒ…å†µè¿‡æ‹Ÿåˆã€‚ä½ èƒ½å°è¯•ä¿®å¤è¿™ä¸ªé—®é¢˜å—ï¼Ÿä¹Ÿè®¸å¯ä»¥ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œé£æ ¼çš„æ–¹æ³•ï¼Œè®©ä¸€ä¸ªæ¨¡å‹ä¸å¦ä¸€ä¸ªæ¨¡å‹å¯¹æˆ˜ï¼Ÿ

+   ä½ èƒ½é€šè¿‡ä¸€ä¸ªå¹³å‡å¾—åˆ†ä¸º 21 çš„æœ€ç»ˆ Pong ä¸»å®°è€…æ¨¡å‹å—ï¼Ÿè¿™åº”è¯¥ä¸éš¾â€”â€”å­¦ä¹ ç‡è¡°å‡æ˜¯ä¸€ä¸ªæ˜¾è€Œæ˜“è§çš„å°è¯•æ–¹æ³•ã€‚

# æ€»ç»“

åœ¨è¿™ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬æ¶‰åŠäº†è®¸å¤šæ–°çš„å¤æ‚å†…å®¹ã€‚ä½ å·²ç»äº†è§£äº†åœ¨å…·æœ‰å¤§è§‚æµ‹ç©ºé—´çš„å¤æ‚ç¯å¢ƒä¸­ï¼Œå€¼è¿­ä»£çš„å±€é™æ€§ï¼Œå¹¶è®¨è®ºäº†å¦‚ä½•é€šè¿‡ Q å­¦ä¹ æ¥å…‹æœè¿™äº›å±€é™æ€§ã€‚æˆ‘ä»¬åœ¨ FrozenLake ç¯å¢ƒä¸­éªŒè¯äº† Q å­¦ä¹ ç®—æ³•ï¼Œå¹¶è®¨è®ºäº†ä½¿ç”¨ç¥ç»ç½‘ç»œï¼ˆNNsï¼‰å¯¹ Q å€¼è¿›è¡Œé€¼è¿‘ï¼Œä»¥åŠç”±æ­¤é€¼è¿‘å¸¦æ¥çš„é¢å¤–å¤æ‚æ€§ã€‚

æˆ‘ä»¬ä»‹ç»äº†å¤šä¸ªé’ˆå¯¹æ·±åº¦ Q ç½‘ç»œï¼ˆDQNsï¼‰çš„æŠ€å·§ï¼Œä»¥æé«˜å®ƒä»¬çš„è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§ï¼Œä¾‹å¦‚ç»éªŒå›æ”¾ç¼“å†²åŒºã€ç›®æ ‡ç½‘ç»œå’Œå¸§å †å ã€‚æœ€åï¼Œæˆ‘ä»¬å°†è¿™äº›æ‰©å±•æ•´åˆåˆ°ä¸€ä¸ªå•ä¸€çš„ DQN å®ç°ä¸­ï¼ŒæˆåŠŸè§£å†³äº† Atari æ¸¸æˆå¥—ä»¶ä¸­çš„ Pong ç¯å¢ƒã€‚

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†ç®€è¦äº†è§£ä¸€äº›æ›´é«˜çº§çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åº“ï¼Œä¹‹åï¼Œæˆ‘ä»¬å°†å›é¡¾ä¸€ç»„è‡ª 2015 å¹´ä»¥æ¥ç ”ç©¶äººå‘˜å‘ç°çš„æŠ€å·§ï¼Œç”¨ä»¥æ”¹å–„ DQN çš„æ”¶æ•›æ€§å’Œè´¨é‡ï¼Œè¿™äº›æŠ€å·§ï¼ˆç»¼åˆèµ·æ¥ï¼‰å¯ä»¥åœ¨å¤§å¤šæ•° 54 æ¬¾ï¼ˆæ–°æ·»åŠ çš„ï¼‰Atari æ¸¸æˆä¸Šå®ç°æœ€å…ˆè¿›çš„æˆæœã€‚è¿™äº›æŠ€å·§åœ¨ 2017 å¹´å‘å¸ƒï¼Œæˆ‘ä»¬å°†åˆ†æå¹¶é‡æ–°å®ç°æ‰€æœ‰è¿™äº›æŠ€å·§ã€‚
