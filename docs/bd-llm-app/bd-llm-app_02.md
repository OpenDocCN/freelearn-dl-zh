

# 第二章：LLMs 用于 AI 驱动应用程序

在*第一章*，*大型语言模型简介*中，我们将**大型语言模型**（**LLM**s）介绍为具有生成能力和强大常识推理能力的强大基础模型。现在，下一个问题是：我应该用这些模型做什么？

在本章中，我们将看到 LLMs 如何彻底改变软件开发的世界，引领 AI 驱动应用程序的新时代。到本章结束时，你将更清楚地了解如何利用新的 AI 编排器框架将 LLMs 嵌入到不同的应用程序场景中，这些框架正在充斥着 AI 开发市场。

在本章中，我们将涵盖以下主题：

+   LLMs 如何改变软件开发

+   代码伴侣系统

+   介绍 AI 编排器以将 LLMs 嵌入到应用程序中

# LLMs 如何改变软件开发

LLMs 已被证明具有非凡的能力：从自然语言理解任务（摘要、命名实体识别和分类）到文本生成，从常识推理到头脑风暴技能。然而，它们并不仅仅本身令人难以置信。正如*第一章*中讨论的，LLMs 以及一般而言的**大型基础模型**（**LFMs**），通过作为构建强大应用程序的平台，正在彻底改变软件开发。

事实上，我们不必从头开始，今天的开发者可以通过调用 LLM 的托管版本来调用 API，并可以选择根据上一章中看到的方式进行定制。这种转变使得团队能够更轻松、更有效地将 AI 的力量整合到他们的应用程序中，类似于从专用计算到过去的时间共享的转变。

但在应用程序中具体如何整合 LLMs 呢？在整合 LLMs 时，有两个主要方面需要考虑：

+   **技术方面**，涵盖了*如何*。将 LLMs 集成到应用程序中涉及通过 REST API 调用嵌入它们，并通过 AI 编排器管理它们。这意味着设置允许通过 API 调用无缝与 LLMs 通信的架构组件。此外，使用 AI 编排器有助于在应用程序中高效地管理和协调 LLMs 的功能，正如我们将在本章后面讨论的那样。

+   **概念方面**，涵盖了*什么*。LLMs 带来了大量可以在应用程序中利用的新能力。这些能力将在本书的后续章节中详细探讨。一种看待 LLMs 影响的方式是将它们视为一种新的软件类别，通常被称为*代码伴侣*。这种分类突出了 LLMs 在增强应用程序功能方面提供的重大协助和协作。

我们将在本章后面深入探讨技术方面，而下一节将涵盖一个全新的软件类别——代码伴侣系统。

# 副驾驶系统

副驾驶系统是一种新的软件类别，它作为用户完成复杂任务的专家助手。这一概念由微软提出，并已应用于其应用程序中，例如 M365 副驾驶和新的 Bing，现在由 GPT-4 提供支持。使用这些产品相同的框架，开发者现在可以构建自己的副驾驶并将其嵌入到他们的应用程序中。

但副驾驶究竟是什么？

如其名所示，副驾驶旨在成为与用户并肩工作的 AI 助手，支持他们在各种活动中，从信息检索到博客写作和发布，从头脑风暴到代码审查和生成。

以下是副驾驶的一些独特特性：

+   **副驾驶由 LLM 驱动**，或者更普遍地说，由 LFM 驱动，这意味着这些是使副驾驶“智能”的推理引擎。这个推理引擎是其组件之一，但不是唯一的。副驾驶还依赖于其他技术，如应用程序、数据源和用户界面，以向用户提供有用且引人入胜的体验。以下插图显示了这是如何工作的：

![一个人物的卡通  描述自动生成](img/B21714_02_01.png)

图 2.1：副驾驶由 LLM 驱动

+   **副驾驶被设计成具有会话式用户界面**，允许用户使用自然语言与之交互。这减少了甚至消除了需要特定领域分类法（例如，查询表格数据需要了解编程语言如 T-SQL）的复杂系统与用户之间的知识差距。让我们看看这样一个对话的例子：

![](img/B21714_02_02.png)

图 2.2：一个用于减少用户与数据库之间差距的对话式 UI 示例

+   **副驾驶有一个作用域**。这意味着它被扎根于特定领域的数据，因此它只能在该应用程序或领域的范围内回答。

**定义**

扎根是通过使用 LLM 与特定用例的信息，这些信息是使用案例特定的、相关的，并且不是 LLM 训练知识的一部分。这对于确保输出质量、准确性和相关性至关重要。例如，假设你想要一个由 LLM 驱动的应用程序，在研究最新的论文（不包括在你的 LLM 训练数据集中）时帮助你。你还希望你的应用程序只有在答案包含在这些论文中时才做出回应。为此，你需要将你的 LLM 扎根于这些论文的集合，这样你的应用程序就只能在这个范围内做出回应。

通过一个名为检索增强生成（RAG）的架构框架实现扎根，这是一种通过在生成响应之前结合外部权威知识库的信息来增强 LLM 输出的技术。这个过程有助于确保生成的内容是相关、准确且最新的。

副驾驶和 RAG 之间的区别是什么？RAG 可以被看作是一种具有副驾驶特征的架构模式。每当我们要将副驾驶引导到特定领域的数据时，我们就使用 RAG 框架。请注意，RAG 不是唯一可以具有副驾驶特征的架构模式：还有其他框架，如函数调用或多智能体，我们将在本书中探讨。

例如，假设我们在公司内部开发了一个副驾驶，允许员工与企业知识库进行聊天。虽然这很有趣，但我们不能提供用户可以用来规划夏季旅行的副驾驶（那就像是在我们自己的托管成本下提供类似 ChatGPT 的工具！）；相反，我们希望副驾驶仅限于我们的企业知识库，以便它只能对特定领域上下文相关的答案做出回应。

下图展示了引导副驾驶系统的示例：

![](img/B21714_02_03.png)

图 2.3：引导副驾驶的示例

+   **副驾驶的能力可以通过技能进行扩展**，这些技能可以是代码或对其他模型的调用。实际上，LLM（我们的推理引擎）可能存在两种类型的限制：

    +   **参数知识有限**。这是由于知识库截止日期，这是 LLM 的一个生理特征。实际上，它们的训练数据集总是会“过时”，不符合当前趋势。这可以通过添加非参数知识并通过引导来克服，如之前所见。

    +   **缺乏执行权力**。这意味着 LLM 本身没有执行动作的权限。以众所周知的 ChatGPT 为例：如果我们要求它生成一篇关于生产力技巧的领英帖子，我们随后需要将其复制粘贴到我们的领英个人资料上，因为 ChatGPT 本身无法完成这一操作。这就是我们需要插件的原因。插件是 LLM 连接外部世界的桥梁，不仅作为输入源来扩展 LLM 的非参数知识（例如，允许网络搜索），而且还作为输出源，以便副驾驶能够实际执行动作。例如，通过领英插件，我们由 LLM 驱动的副驾驶不仅能够生成帖子，还能够将其发布到网上。![一个人指向插件的卡通  自动生成的描述](img/B21714_02_04.png)

    图 2.4：维基百科和领英插件的示例

注意，用户在自然语言中的提示并非模型处理的唯一输入。实际上，它是我们基于 LLM 的应用程序的后端逻辑以及提供给模型的指令集的一个关键组成部分。这种*元提示*或系统消息是新学科**提示工程**的研究对象。

**定义**

提示工程是为各种应用程序和研究主题设计并优化 LLM 提示的过程。提示是用于引导 LLM 输出的简短文本。提示工程技能有助于更好地理解 LLM 的能力和局限性。

提示工程涉及选择合适的单词、短语、符号和格式，以从 LLM 中获得期望的响应。提示工程还涉及使用其他控制手段，如参数、示例或数据源，来影响 LLM 的行为。例如，如果我们想让我们的 LLM 驱动应用程序为 5 岁的孩子生成响应，我们可以在类似“扮演一个向 5 岁孩子解释复杂概念的教师”的系统消息中指定这一点。

实际上，安德烈·卡帕西（Andrej Karpathy），特斯拉前 AI 总监，于 2023 年 2 月重返 OpenAI，他在推特上表示：“英语是最新最热门的编程语言。”

我们将在第四章“提示工程”中更深入地探讨提示工程的概念。在下一节中，我们将关注新兴的 AI 编排器。

# 将 AI 编排器引入以将 LLM 嵌入到应用程序中

在本章的早期部分，我们看到了在将 LLM 整合到应用程序中时需要考虑的两个主要方面：技术方面和概念方面。虽然我们可以用名为 Copilot 的全新软件类别来解释概念方面，但在本节中，我们将进一步探讨如何在我们的应用程序中技术性地嵌入和编排 LLM。

## AI 编排器的主要组件

从一方面来看，基础模型范式转变意味着在人工智能应用领域的大大简化：在生成模型之后，现在的趋势是消费模型。另一方面，在开发这种新型人工智能时可能会遇到许多障碍，因为其中有一些与大型语言模型（LLM）相关的组件是全新的，并且在应用生命周期中之前从未被管理过。例如，可能会有恶意行为者试图更改 LLM 指令（前面提到的系统消息），使得应用程序不遵循正确的指令。这是一个新的安全威胁集合的例子，这些威胁是 LLM 驱动应用程序的典型特征，需要用强大的反击或预防技术来解决。

下面的插图展示了此类应用程序的主要组件：

![计算机程序图  自动生成描述](img/B21714_02_05.png)

图 2.5：LLM 驱动应用程序的高级架构

让我们详细检查这些组件：

+   **模型**：模型就是我们决定嵌入到我们的应用程序中的 LLM 类型。模型主要有两大类：

    +   **专有 LLMs**：由特定公司或组织拥有的模型。例如，由 OpenAI 开发的 GPT-3 和 GPT-4，或由 Google 开发的 Bard。由于它们的源代码和架构不可用，这些模型不能从头开始在自定义数据上重新训练，但在需要时可以进行微调。

    +   **开源**：代码和架构可以自由获取和分发的模型，因此它们也可以在自定义数据上从头开始训练。例如，由阿布扎比的**技术创新研究所**（**TII**）开发的 Falcon LLM，或由 Meta 开发的 LLaMA。

我们将在**第三章**，**为您的应用程序选择一个 LLM**中深入了解今天可用的主要 LLM 集。

+   **记忆**：LLM 应用通常使用对话界面，这需要能够在对话中回溯到早期信息。这是通过一个“记忆”系统实现的，该系统允许应用程序存储和检索过去的交互。请注意，过去的交互也可能构成要添加到模型中的额外非参数知识。为了实现这一点，重要的是将所有过去的对话——适当嵌入——存储到 VectorDB 中，这是应用程序数据的核心。

**定义**

VectorDB 是一种基于向量嵌入存储和检索信息的数据库类型，这些嵌入是捕获文本意义和上下文的数值表示。通过使用 VectorDB，你可以根据意义的相似性而不是关键词进行语义搜索和检索。VectorDB 还可以通过提供上下文理解和丰富生成结果来帮助 LLMs 生成更相关和连贯的文本。VectorDB 的例子包括 Chroma、Elasticsearch、Milvus、Pinecone、Qdrant、Weaviate 以及**Facebook AI Similarity Search**（**FAISS**）。

FAISS 是由 Facebook（现在 Meta）于 2017 年开发的，是早期向量数据库之一。它旨在进行密集向量的高效相似性搜索和聚类，特别适用于多媒体文档和密集嵌入。最初，它是在 Facebook 的一个内部研究项目中。其主要目标是更好地利用 GPU 来识别与用户偏好相关的相似性。随着时间的推移，它发展成为可用的最快相似性搜索库，可以处理数十亿规模的数据集。FAISS 为推荐引擎和基于 AI 的助手系统开辟了可能性。

+   **插件**：它们可以被视为可以集成到 LLM 中的附加模块或组件，以扩展其功能或使其适应特定任务和应用。这些插件作为附加组件，增强了 LLM 在核心语言生成或理解能力之外的特性。

插件背后的理念是使 LLMs 更加灵活和适应性强，允许开发者和用户根据他们的特定需求自定义语言模型的行为。可以创建执行各种任务的插件，并且它们可以无缝地集成到 LLM 的架构中。

+   **提示**：这可能是 LLM 驱动的应用程序中最有趣和关键的部分。我们在上一节中已经引用了 Andrej Karpathy 的断言，“英语是最新最热门的编程语言”，你将在接下来的章节中理解原因。提示可以在两个不同的级别上定义：

    +   **“前端”，或用户所见**：一个“提示”指的是模型输入。这是用户与应用程序交互的方式，以自然语言提问。

    +   **“后端”，或用户所见不到的**：自然语言不仅是用户与前端交互的方式；它也是我们“编程”后端的方式。实际上，在用户的提示之上，有许多自然语言指令，或元提示，我们提供给模型，以便它能够正确地回答用户的查询。元提示的目的是指导模型按照预期的方式行动。例如，如果我们想限制我们的应用程序只回答与我们在 VectorDB 中提供的文档相关的问题，我们将在我们的元提示中指定以下内容：“*只有当问题与提供的文档相关时才回答。*”

最后，我们来到了*图 2.5*所示的高级架构的核心，即**AI 编排器**。有了 AI 编排器，我们指的是那些使嵌入和编排 LLMs（大型语言模型）在应用程序中变得更加容易的轻量级库。

到了 2022 年底，随着 LLMs 的流行，市场上开始涌现出许多库。在接下来的几节中，我们将重点关注其中的三个：LangChain、Semantic Kernel 和 Haystack。

## LangChain

LangChain 是由 Harrison Chase 在 2022 年 10 月作为开源项目推出的。它可以在 Python 和 JS/TS 中使用。它是一个用于开发由语言模型驱动的应用程序的框架，使它们具有数据感知性（具有扎根）和代理性——这意味着它们能够与外部环境交互。

让我们来看看 LangChain 的关键组件：

![图片](img/B21714_02_06.png)

图 2.6：LangChain 的组件

总体而言，LangChain 具有以下核心模块：

+   **模型**：这些是将成为应用程序引擎的 LLMs 或 LFMs。LangChain 支持来自 OpenAI 和 Azure OpenAI 的专有模型，以及可以从**Hugging Face Hub**获取的开源模型。

**定义**

Hugging Face 是一家公司和一个社区，致力于构建和分享自然语言处理和其他机器学习领域的最先进模型和工具。它开发了 Hugging Face Hub，这是一个平台，人们可以在这里创建、发现和协作机器学习模型、LLMs、数据集和演示。Hugging Face Hub 在各个领域和任务中托管了超过 120k 个模型、20k 个数据集和 50k 个演示，例如音频、视觉和语言。

除了模型之外，LangChain 还提供了许多与提示相关的组件，这使得管理提示流程变得更加容易。

+   **数据连接器**：这些是指构建所需的基本模块，用于检索我们希望提供给模型的额外外部知识（例如，在基于 RAG 的场景中）。数据连接器的例子包括文档加载器或文本嵌入模型。

+   **记忆**：这允许应用程序在短期和长期内保持对用户交互的引用。它通常基于存储在 VectorDB 中的矢量化嵌入。

+   **链**：这些是预定的一系列动作和对 LLM 的调用，这使得构建需要将 LLMs 链接在一起或与其他组件链接的复杂应用程序变得更加容易。链的一个例子可能是：获取用户查询，将其分成更小的部分，嵌入这些部分，在 VectorDB 中搜索相似的嵌入，使用 VectorDB 中最相似的三个部分作为上下文来提供答案，并生成答案。

+   **代理**：代理是驱动 LLM 驱动的应用程序中决策的实体。它们可以访问一系列工具，并根据用户输入和上下文决定调用哪个工具。代理是动态和自适应的，这意味着它们可以根据情况或目标改变或调整其行为。

LangChain 提供以下好处：

+   LangChain 为我们之前提到的、与语言模型一起工作的必要组件提供了模块化抽象，例如提示、记忆和插件。

+   除了这些组件之外，LangChain 还提供了预构建的 **链**，这是组件的结构化连接。这些链可以是针对特定用例预构建的，也可以是定制的。

在本书的 *第二部分* 中，我们将通过一系列基于 LangChain 的动手应用，从 *第五章* 开始，专注于将 LLMs 内置于您的应用程序中，我们将更深入地关注 LangChain 组件和整体框架。

## Haystack

Haystack 是由 Deepset 开发的基于 Python 的框架，Deepset 是一家成立于 2018 年的柏林初创公司，由 Milos Rusic、Malte Pietsch 和 Timo Möller 创立。Deepset 为开发者提供构建基于自然语言处理（**NLP**）的应用程序的工具，随着 Haystack 的引入，他们将这些工具提升到了新的水平。

下面的插图显示了 Haystack 的核心组件：

![](img/B21714_02_07.png)

图 2.7：Haystack 的组件

让我们详细看看这些组件：

+   **节点**：这些是执行特定任务或功能的组件，例如检索器、阅读器、生成器、摘要器等。节点可以是 LLM 或其他与 LLM 或其他资源交互的实用工具。在 LLM 中，Haystack 支持专有模型，例如 OpenAI 和 Azure OpenAI 中可用的模型，以及来自 Hugging Face Hub 的开源模型。

+   **管道**：这些是调用执行自然语言任务或与其他资源交互的节点的序列。管道可以是查询管道或索引管道，这取决于它们是否在文档集上执行搜索或为搜索准备文档。管道是预先确定和硬编码的，这意味着它们不会根据用户输入或上下文进行更改或适应。

+   **代理**：这是一个使用 LLM 生成对复杂查询的准确响应的实体。代理可以访问一组工具，这些工具可以是管道或节点，并且可以根据用户输入和上下文决定调用哪个工具。代理是动态和自适应的，这意味着它可以根据情况或目标改变或调整其行为。

+   **工具**：代理可以调用的函数来执行自然语言任务或与其他资源交互。工具可以是代理可用的管道或节点，它们可以被分组成工具包，这些工具包是一系列可以完成特定目标的工具。

+   **DocumentStores**：这些是存储和检索用于搜索的文档的后端。DocumentStores 可以基于不同的技术，包括 VectorDB（例如 FAISS、Milvus 或 Elasticsearch）。

Haystack 提供的一些好处包括：

+   **易用性**：Haystack 易于使用且直观。它通常用于轻量级任务和快速原型。

+   **文档质量**：Haystack 的文档被认为是高质量的，有助于开发者构建搜索系统、问答、摘要和对话式 AI。

+   **端到端框架**：Haystack 覆盖了整个 LLM 项目生命周期，从数据预处理到部署。它非常适合大规模搜索系统和信息检索。

+   关于 Haystack 的另一个优点是，你可以将其作为 REST API 部署，并且可以直接消费它。

## 语义内核

语义内核是我们将在本章中探索的第三个开源 SDK。它最初由微软开发，最初是用 C# 编写的，现在也支持 Python。

这个框架的名字来源于“内核”的概念，一般而言，内核指的是系统的核心或本质。在这个框架的上下文中，内核是指通过将一系列组件链接和连接成管道来处理用户输入的引擎，鼓励**函数组合**。

**定义**

在数学中，函数组合是将两个函数组合起来创建一个新函数的方法。想法是使用一个函数的输出作为另一个函数的输入，形成一个函数链。两个函数 f 和 g 的组合表示为(*f ![img/circle.png](img/circle.png) g*)，其中函数 g 首先应用，然后是函数 f ![img/arrow.png](img/arrow.png)(*f ![img/circle.png](img/circle.png) g*)(*x*) = *f*(*g*(*x*))。

计算机科学中的函数组合是一个强大的概念，它通过将较小的函数组合成较大的函数来创建更复杂和可重用的代码。它增强了模块化和代码组织，使程序更容易阅读和维护。

以下是对语义内核解剖结构的说明：

![](img/B21714_02_08.png)

图 2.8：语义内核的解剖结构

语义内核有以下主要组件：

+   **模型**：这些是将成为应用程序引擎的 LLM 或 LFM。语义内核支持专有模型，例如 OpenAI 和 Azure OpenAI 中可用的模型，以及可以从 Hugging Face Hub 获取的开源模型。

+   **内存**：它允许应用程序在短期和长期内保持对用户交互的引用。在语义内核的框架内，可以通过三种方式访问记忆：

    +   **键值对**：这包括保存存储简单信息的环境变量，例如名称或日期。

    +   **本地存储**：这包括将信息保存到可以通过其文件名检索的文件中，例如 CSV 或 JSON 文件。

    +   **语义记忆搜索**：这与 LangChain 和 Haystack 的记忆类似，因为它使用嵌入来表示和基于其意义搜索文本信息。

+   **函数**：函数可以被视为混合 LLM 提示和代码的技能，目的是使用户的请求可解释和可操作。有两种类型的函数：

    +   **语义函数**：这是一种模板化提示，是一种自然语言查询，它指定了 LLM 的输入和输出格式，同时也结合了提示配置，该配置设置了 LLM 的参数。

    +   **本地函数**：这些指的是可以路由语义函数捕获的意图并执行相关任务的本地计算机代码。

为了举例说明，一个语义函数可以要求 LLM 写一段关于 AI 的短段落，而一个本地函数实际上可以在 LinkedIn 等社交媒体上发布。

+   **插件**：这些是连接到外部来源或系统的连接器，旨在提供额外信息或执行自主操作的能力。语义内核提供了一些现成的插件，例如 Microsoft Graph 连接器套件，但你也可以通过利用函数（本地和语义，或两者的组合）来构建自定义插件。

+   **规划器**: 由于 LLM 可以被视为推理引擎，它们也可以被用来自动创建链或管道以满足新用户的需求。这个目标是通过规划器实现的，它是一个接受用户任务作为输入并生成实现目标所需的一组动作、插件和函数的功能。

语义内核的一些好处包括：

+   **轻量级和 C#支持**: 语义内核更轻量级，并包含 C#支持。对于 C#开发者或使用.NET 框架的开发者来说，它是一个极佳的选择。

+   **广泛的应用场景**: 语义内核功能多样，支持各种与 LLM 相关的任务。

+   **行业引领**: 语义内核是由微软开发的，它是公司构建自身协同飞行员框架所使用的框架。因此，它主要受行业需求和询问的驱动，使其成为企业级应用的稳固工具。

## 如何选择框架

总体而言，这三个框架提供，或多或少，类似的核心组件，有时被称为不同的分类法，但涵盖了协同飞行员系统概念中所示的所有模块。因此，一个自然的问题可能是：“我应该使用哪一个来构建我的 LLM 驱动应用程序？” 好吧，没有正确或错误答案！三者都非常有效。然而，有些特性可能对特定用例或开发者的偏好更为相关。以下是一些您可能想要考虑的标准：

+   **您熟悉或偏好的编程语言**: 不同的框架可能支持不同的编程语言，或者与它们的兼容性或集成程度不同。例如，语义内核支持 C#、Python 和 Java，而 LangChain 和 Haystack 主要基于 Python（尽管 LangChain 也引入了对 JS/TS 的支持）。您可能希望选择一个与您现有技能或偏好相匹配的框架，或者允许您使用最适合您的应用程序领域或环境的语言。

+   **您想要执行或支持的自然语言任务类型和复杂性**: 不同的框架可能具有不同的能力或特性来处理各种自然语言任务，例如摘要、生成、翻译、推理等。例如，LangChain 和 Haystack 提供了编排和执行自然语言任务的实用工具和组件，而语义内核允许您使用自然语言语义函数调用 LLM 和服务。您可能希望选择一个提供您为应用程序目标或场景所需的功能和灵活性的框架。

+   **您需要或希望对 LLM 及其参数或选项进行定制和控制的程度**：不同的框架可能有不同的方式来访问、配置和微调 LLM 及其参数或选项，例如模型选择、提示设计、推理速度、输出格式等。例如，Semantic Kernel 提供了连接器，使得向您的 AI 应用中添加记忆和模型变得容易，而 LangChain 和 Haystack 允许您为文档存储、检索器、阅读器、生成器、摘要器和评估器插入不同的组件。您可能希望选择一个框架，它能够提供您对 LLM 及其参数或选项所需或希望达到的定制和控制的程度。

+   **框架的文档、教程、示例和社区支持的可获得性和质量**：不同的框架可能有不同水平的文档、教程、示例和社区支持，这些可以帮助您学习、使用和调试框架。例如，Semantic Kernel 有一个包含文档、教程、示例和 Discord 社区的网站；LangChain 有一个包含文档、示例和问题的 GitHub 仓库；Haystack 有一个包含文档、教程、演示、博客文章和 Slack 社区的网站。您可能希望选择一个具有文档、教程、示例和社区支持的可获得性和质量，这可以帮助您开始使用框架并解决与框架相关的问题。

让我们简要总结一下这些编排器之间的差异：

| **功能** | **LangChain** | **Haystack** | **Semantic Kernel** |
| --- | --- | --- | --- |
| **LLM 支持** | 专有和开源 | 专有和开源 | 专有和开源 |
| **支持的语言** | Python 和 JS/TS | Python | C#、Java 和 Python |
| **流程编排** | 链 | 节点管道 | 函数管道 |
| **部署** | 无 REST API | REST API | 无 REST API |
| **功能** | **LangChain** | **Haystack** | **Semantic Kernel** |

表 2.1：三种 AI 编排器的比较

总体而言，这三个框架都提供了一系列的工具和集成，以构建您的 LLM 驱动应用程序，并且一个明智的方法可能是使用与您当前技能或公司整体方法最一致的框架。

# 摘要

在本章中，我们深入探讨了 LLM（大型语言模型）正在铺就的新应用开发方式，正如我们介绍了协同驾驶员的概念并讨论了新 AI 编排器的出现。在这些中，我们重点关注了三个项目——LangChain、Haystack 和 Semantic Kernel——并检查了它们的功能、主要组件以及一些选择哪一个的标准。

一旦我们确定了人工智能编排器，另一个关键步骤就是决定我们想要嵌入到我们的应用程序中的 LLM（大型语言模型）。在*第三章*，*为您的应用程序选择一个 LLM*，我们将了解市场上最突出的 LLM——既有专有也有开源的——并了解一些决策标准，以便根据应用程序用例选择合适的模型。

# 参考文献

+   LangChain 仓库：[`github.com/langchain-ai/langchain`](https://github.com/langchain-ai/langchain)

+   语义内核文档：[`learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages`](https://learn.microsoft.com/en-us/semantic-kernel/get-started/supported-languages)

+   Copilot 堆栈：[`build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b`](https://build.microsoft.com/en-US/sessions/bb8f9d99-0c47-404f-8212-a85fffd3a59d?source=/speakers/ef864919-5fd1-4215-b611-61035a19db6b)

+   Copilot 系统：[`www.youtube.com/watch?v=E5g20qmeKpg`](https://www.youtube.com/watch?v=E5g20qmeKpg)

# 加入我们的 Discord 社区

加入我们的社区 Discord 空间，与作者和其他读者进行讨论：

[`packt.link/llm`](https://packt.link/llm)

![二维码](img/QR_Code214329708533108046.png)
