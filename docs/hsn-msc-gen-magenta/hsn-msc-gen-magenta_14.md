# 第十二章：评估

# 第一章：Magenta 与生成艺术简介

1.  随机性。

1.  马尔可夫链。

1.  Algorave。

1.  **长短期记忆**（**LSTM**）。

1.  自主系统生成音乐，无需操作员输入；辅助音乐系统将在创作时补充艺术家的工作。

1.  符号化：乐谱、MIDI、MusicXML、AbcNotation。子符号化：原始音频（波形）、频谱图。

1.  "音符开启"与"音符关闭"的时序、音高范围为 1 到 127 kHz、速度和通道。

1.  在 96 kHz 的采样率下，奈奎斯特频率为 96 kHz/2 = 48 kHz，频率范围为 0 到 48 kHz。这对于听音频来说效果较差，因为 28 kHz 的音频在耳朵上是听不见的（记住，超过 20 kHz 的声音是无法听到的），而且这种采样率并不被很多音频设备正确支持。不过，在录音和音频编辑中它还是有用的。

1.  单个音乐音符 A4 被响亮地演奏 1 秒钟。

1.  鼓、声部（旋律）、和声（复音）、插值和操作。

# 第二章：使用鼓 RNN 生成鼓序列

1.  给定一个当前序列，预测下一个音符的乐谱，然后对每一个你希望生成的步骤进行预测。

1.  (1) RNN（递归神经网络）对向量序列进行操作，用于输入和输出，这对于像乐谱这样的序列数据非常适用；(2) 保持一个由前一输出步骤组成的内部状态，这对于基于过去输入做出预测非常有用，而不仅仅是基于当前输入。

1.  (1) 首先，隐藏层将得到 *h(t + 1)*，即前一个隐藏层的输出；(2) 它还将接收 *x(t + 2)*，即当前步骤的输入。

1.  生成的音符小节数将是 2 小节，或者 32 个步骤，因为每小节有 16 个步骤。在 80 QPM 下，每个步骤的时长为 0.1875 秒，因为你将一分钟的秒数除以 QPM，再除以每小节的步骤数：60 / 80 / 4 = 0.1875。最后，你有 32 个步骤，每个步骤时长 0.1875 秒，所以总时间为 32 * 0.1875 = 6 秒。

1.  增加分支因子会减少随机性，因为你有更多的分支可以选择最佳分支，但增加温度会增加随机性。两者同时进行会互相抵消，只是我们不知道这种抵消的比例。

1.  在每个步骤中，算法将生成四个分支并保留两个。在最后一次迭代中，束搜索将通过检查每一层剩余的两个节点与生成图的步骤数（即树的高度）相乘来搜索图中最佳的分支，这个数是三。因此，我们会遍历 2 * 3 个节点 = 6 个节点。

1.  `NoteSequence`。

1.  MIDI 音符映射到以下类别：36 映射到 0（踢鼓），40 映射到 1（军鼓），42 映射到 2（闭合高帽）。计算得到的索引为 2⁰ + 2¹ + 2² = 7，因此得到的向量为 *v = [0, 0, 0, 0, 0, 0, 1, 0, ... ]*。

1.  索引 131 的位表示是`10000011`（在 Python 中，你可以使用`"{0:b}".format(131)`来获取）。这是 2⁰ + 2¹ + 2⁷，得到以下类别：0（鼓组）、1（小军鼓）和 7（撞击钹）。然后，我们任意选择每个类别的第一个元素： *{36, 38, 49}*。

# 第三章：生成复调旋律

1.  消失梯度（在每个 RNN 步骤中，值被小值相乘）和梯度爆炸是常见的 RNN 问题，通常发生在反向传播步骤的训练过程中。LSTM 提供了一个专用的细胞状态，通过遗忘门、输入门和输出门进行修改，以缓解这些问题。

1.  **门控** **递归** **单元**（**GRUs**）是更简单但表达力较弱的记忆单元，其中遗忘门和输入门合并为一个单一的**更新门**。

1.  对于 3/4 拍的节奏标记，你需要每四分之一音符 3 步，每四分之一音符 4 步，总共每小节 12 步。对于一个二进制步数计数器，要计数到 12，你需要 5 个比特位（像 4/4 拍那样），它们只能计数到 12。对于 3 个回顾，你需要查看过去 3 小节，每小节 12 步，所以你得到 *[36, 24, 12]*。

1.  结果向量是前一步向量的和，每个向量都应用了注意力掩码，因此我们对 *[1, 0, 0, 0]* 应用了 0.1，对 *[0, 1, 0, x]* 应用了 0.5，得到了 *[0.10, 0.50, 0.00, 0.25]*。x 的值是 0.5，因为 0.5 乘以 0.5 等于 0.25。

1.  一个 C 大调和弦，时值为一个四分音符。

1.  在 Polyphony RNN 中，没有音符结束事件。如果在一个步骤中没有使用`CONTINUED_NOTE`来表示某个音高，则该音符会停止。在 Performance RNN 中，则会使用`NOTE_END 56`事件。

1.  (1) 使用`TIME_SHIFT`事件表达的时序性，这些事件在所有的 Performance RNN 模型中都有，例如在`performance`配置中，以及(2) 使用`VELOCITY`事件的动态播放，这些事件出现在`performance_with_dynamics`配置中。

1.  它将在 RNN 步骤调用期间改变迭代次数。每秒钟的音符数量越大，在生成过程中所需的 RNN 步骤就越多。

# 第四章：使用 MusicVAE 进行潜在空间插值

1.  主要用途是降维，迫使网络学习重要特征，从而使得重构原始输入成为可能。AE 的缺点是隐藏层表示的潜在空间不是连续的，难以进行采样，因为解码器无法理解某些点。

1.  重建损失在网络生成与输入不同的输出时进行惩罚。

1.  在 VAE 中，潜在空间是连续和平滑的，使得可以对空间中的任何点进行采样，并在两点之间进行插值。它是通过让潜在变量遵循 P(z) 的概率分布来实现的，通常是高斯分布。

1.  KL 散度衡量两个概率分布之间的差异。当与重构损失结合时，它会将聚类集中在 0 附近，并使它们更接近或更远离彼此。

1.  我们使用`np.random.randn(4, 512)`来采样正态分布。

1.  计算潜在空间中两点之间的方向。

# 第五章：使用 NSynth 和 GANSynth 进行音频生成

1.  你需要处理每秒 16,000 个样本（至少），并在更大的时间尺度上跟踪整体结构。

1.  NSynth 是一个 WaveNet 风格的自编码器，能够学习自己的时间嵌入，使得捕捉长期结构成为可能，并提供访问有用的隐藏空间。

1.  彩虹图中的颜色代表了时间嵌入的 16 个维度。

1.  请查看章节代码中`audio_utils.py`文件中的`timestretch`方法。

1.  GANSynth 使用上采样卷积，使得整个音频样本的训练和生成处理可以并行进行。

1.  你需要使用`np.random.normal(size=[10, 256])`来采样随机正态分布，其中 10 是采样的乐器数量，256 是潜在向量的大小（由`latent_vector_size`配置给出）。

# 第六章：训练数据准备

1.  MIDI 不是文本格式，因此更难使用和修改，但它非常常见。MusicXML 相对较少见且笨重，但它的优势在于采用文本格式。ABCNotation 也相对罕见，但其优势在于是文本格式，并且更接近乐谱。

1.  使用`chapter_06_example_08.py`中的代码，并在提取过程中更改`program=43`。

1.  LMD 中有 1,116 首摇滚歌曲，3,138 首爵士、蓝调和乡村歌曲。请参考`chapter_06_example_02.py`和`chapter_06_example_03.py`，了解如何根据音乐风格信息进行统计。

1.  在`melody_rnn_pipeline_example.py`中使用`RepeatSequence`类。

1.  使用`chapter_06_example_09.py`中的代码。是的，我们可以用它训练一个量化模型，因为数据准备管道会量化输入。

1.  对于小型数据集，数据增强在创造更多数据方面起着至关重要的作用，因为有时你根本没有更多数据。对于较大的数据集，数据增强也起到作用，通过创建更多相关数据和现有数据的变种，帮助网络的训练阶段。

# 第七章：训练 Magenta 模型

1.  请参见`chapter_07_example_03.py`。

1.  欠拟合的网络是指尚未达到最佳状态的网络，这意味着它在评估数据上无法做出良好预测，因为它对训练数据的拟合不佳（目前为止）。可以通过让其训练足够长时间、增加网络容量和更多数据来修正。

1.  过拟合的网络是指已学会预测输入数据，但无法推广到训练集之外的值的网络。可以通过增加更多数据、减少网络容量，或使用正则化技术（如 dropout）来修正。

1.  提前停止。

1.  阅读《关于深度学习大批量训练的问题：泛化差距与锐化极小值》，其中解释了更大的批量大小会导致锐化极小值，从而导致泛化能力较差。因此在效率方面更差，但在训练时间方面可能更好，因为可以同时处理更多数据。

1.  更大的网络会在预测中更加精确，因此最大化这一点很重要。网络的规模也应随数据的大小（和质量）增长。例如，对于数据而言过大的网络可能会导致过拟合。

1.  它有助于解决梯度爆炸问题，因为权重将乘以较小的值，限制大梯度的可能性。另一种方法是降低学习率。

1.  可用于在更强大的机器上启动训练，也可以同时启动多个训练会话。不幸的是，使用云提供商会产生成本，这意味着我们使用的训练时间和功率越多，成本就会越高。

# 第八章：在浏览器中使用 Magenta.js

1.  我们可以使用 TensorFlow.js 训练模型，但无法使用 Magenta.js 训练模型。我们需要使用 Python 在 Magenta 中训练模型，然后在 Magenta.js 中导入生成的模型。

1.  Web Audio API 允许在浏览器中使用音频节点进行音频合成、转换和路由。最简单的使用方法是使用像 Tone.js 这样的音频框架。

1.  方法是 `randomSample`，参数是生成音符的音高。例如，使用 60 将生成 MIDI 音高为 60 的单音符，或者在字母符号中是 C4。这也是使用 Tone.js 调高或调低音符音高的参考。

1.  方法是 `sample`，乐器数量取决于正在使用的模型。在我们的示例中，我们使用了 `trio` 模型，它生成三种乐器。使用 `melody` 模型将只生成一个主导乐器。

1.  因为 JavaScript 是单线程的，如果在 UI 线程中启动长时间的同步计算，将会阻塞其执行。使用 Web Worker 可以在另一个线程中执行代码。

1.  在浏览器中使用 Web MIDI API，目前支持不够完善，或者在服务器端使用 Magenta.js 进程，这样可以更容易地向其他进程发送 MIDI。

# 第九章：使 Magenta 与音乐应用程序互动

1.  DAW 将具有更多面向音乐制作的功能，如录制、音频、MIDI 编辑、效果和母带处理以及歌曲作曲。像 FluidSynth 这样的软件合成器功能较少，但优点是轻量且易于使用。

1.  大多数音乐软件不会自动打开 MIDI 端口，因此要在它们之间发送序列，我们必须手动打开端口。

1.  参见本章的代码 `chapter_09_example_05.py`。

1.  因为将两个失去同步的软件重新同步需要重启它们。MIDI 时钟可以在每个节拍时启用同步。

1.  因为 Magenta Studio 与现有的音乐制作工具（如 DAW）集成，并且不需要任何技术知识，它使得 AI 生成的音乐能够面向更广泛的受众，这也是 Magenta 的最终目标。

1.  Magenta Studio 插件和 Magenta Studio 独立版都基于 Magenta.js，并通过 Electron 打包。Magenta Studio 插件使用 Ableton Live 中的 Max MSP 集成来执行。
