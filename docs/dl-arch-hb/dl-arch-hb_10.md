

# 第十章：探索模型评估方法

一个没有经过任何验证的训练深度学习模型不能部署到生产环境中。在机器学习软件领域，生产指的是将机器学习模型部署到真实环境中并实际使用其预测结果。更广泛地说，模型评估在任何深度学习项目中都起着至关重要的作用。通常，一个深度学习项目会构建多个模型，最终选择一个模型在生产环境中使用。任何项目的良好模型评估过程都能带来以下结果：

+   通过模型比较和评估指标得到更好的最终模型性能

+   通过了解常见的模型陷阱，减少生产预测中的错误

+   通过模型洞察使得实践者与最终模型的行为更加一致

+   通过成功指标评估提高项目成功的概率

+   一个偏差更小、更公平并且能够产生更可信预测的最终模型

通常，模型评估过程能在整个机器学习生命周期中帮助做出更明智的决策。在本书*第二部分*的第一章中，我们将讨论所有有助于实现这些收益的模型评估类别。此外，在接下来的四章中，我们将深入探讨这些类别，这些章节都属于本书的*第二部分*。具体来说，我们将在本章讨论以下主题：

+   探索不同的模型评估方法

+   构建基础模型评估指标

+   探索自定义指标及其应用

+   探索用于比较模型指标的统计测试

+   将评估指标与成功关联起来

+   直接优化指标

# 技术要求

本章将通过使用 Python 编程语言进行实际操作。要完成这一部分，你只需在 Python 中安装`matplotlib`库。

代码文件可在 GitHub 上找到，链接为[`github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_10`](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_10)。

# 探索不同的模型评估方法

大多数实践者都熟悉与准确性相关的指标。这是最基本的评估方法。通常，对于监督学习问题，实践者会将准确性相关的指标视为黄金标准。在模型评估的背景下，“准确性指标”这一术语通常用于统称各种性能指标，如准确率、F1 分数、召回率、精度和均方误差。结合适当的交叉验证划分策略，使用指标作为独立评估策略，在大多数项目中可以取得很好的效果。在深度学习中，准确性相关的指标通常用于监控每个周期中模型的进展。随后，监控过程可以扩展到执行早停，当模型不再改善时停止训练，并确定何时减少学习率。此外，在训练过程结束时，可以加载最佳的模型权重，这些权重是在验证数据集上取得最佳指标分数时的权重。

仅仅依靠与准确性相关的指标并不能完整地反映一个机器学习模型的能力和行为。特别是在无监督项目中，准确性指标是表面性的，只对特定分布相关。对模型进行更全面的理解，可以让你在机器的生命周期中做出更明智的决策。以下是一些获得更深入理解模型的方式：

+   模型洞察可以作为评估所用数据准确性的代理。如果数据被认为是不准确的或存在一些轻微缺陷，可以重新进入机器学习生命周期中的数据准备阶段。

+   如果检测到偏差，可能需要重新训练模型以去除偏差。

+   评估模型是否能够像领域专家一样识别模式同样很重要。如果不能，可能需要采用不同的模型或数据准备方法。

+   需要考虑模型是否能够在预测中体现常识。如果不能，可能需要应用特殊的后处理技术来强制执行常识。

+   暴露其他性能指标，如推理速度和模型大小，在选择模型时可能至关重要。你不希望选择一个过慢或过大的模型，导致无法适应目标生产环境。

除了帮助项目朝着成功方向发展外，从模型中收集洞见还可以帮助早期识别潜在问题。在**第一章**中介绍的机器学习生命周期，*深度学习生命周期*，中，重要的是要记住，项目可能在规划阶段或在提供模型洞见时失败。失败是机器学习过程中的自然组成部分，事实上，许多项目注定不会成功。这可能是由于各种因素，如数据工程不适合机器学习或任务过于复杂。然而，重要的是要尽快失败并放弃该项目，从而将资源重新分配到更有成功机会的其他用例中。在项目至关重要且无法放弃的情况下，快速识别失败的根本原因可以通过将资源引导到修复问题上，并循环地在机器学习生命周期的各个阶段之间过渡，从而提高项目的执行效率。为了让项目能够快速失败，必须有一种负责任且有信心的方式来判断模型是否无法正常工作。总之，这种快速失败的能力是非常有益的，因为它节省了本可能浪费的时间和资源。

以下列表展示了一系列足够广泛的方法，可用于评估深度学习模型，其中一些方法也是通用的，适用于非深度学习模型：

+   **评估指标工程**：虽然评估指标在许多项目中常常被使用，但评估指标工程的实践通常被忽视。在本章中，我们将仔细探讨指标工程，并探索选择合适评估指标的过程。我们将首先讨论适用于各种问题类型的基础基线评估指标。然后，我们将进一步探讨如何将基线评估指标升级为适合项目领域和用例的特定指标。简而言之，本章将帮助你理解指标工程的重要性，并指导你选择适合项目的评估指标。

+   **学习曲线**：学习曲线用于确定深度学习模型的拟合程度。

+   **提升图**：提升图提供了预测模型性能的可视化表示。它显示了模型在预测正向结果方面比随机机会要好多少。

+   **接收者操作特征**（**ROC**）**曲线**：二分类模型性能的图形表示。它们在不同的分类阈值下绘制**真正例率**（**TPR**）与**假正例率**（**FPR**）。

+   **混淆矩阵**：混淆矩阵是一种性能评估工具，用于测量机器学习模型的分类准确度。它比较模型预测和实际结果，并以矩阵格式呈现。

+   **特征重要性**：这是确定数据集中哪些特征对机器学习模型输出影响最大的过程。此外，它有助于识别给定问题中最重要的因素，并可以帮助提高模型的整体性能。

+   **A/B 测试**：机器学习的 A/B 测试涉及比较两种不同模型或算法在特定任务上的表现，以确定哪个模型在实践中表现更好。这可以帮助从业者更明智地决定使用哪些模型或如何改进现有模型。

+   **队列分析**：队列分析是一种评估模型在不同用户子群或队列上表现的技术。它可以帮助确定模型是否对不同群体表现出不同的性能，并且可以用于了解如何针对特定分段改进模型。

+   **残差分析**：残差分析是一种通过检查观测值与预测值（**残差**）之间的差异来检查回归模型拟合优度的技术。它有助于识别残差中可能指示模型改进方向的模式或异常值。

+   **置信区间**：置信区间是估计中不确定性的一种度量。它可以用于确定在一定置信水平下，模型的真实性能可能落在的值范围内。置信区间对于比较不同模型的性能或确定模型性能是否具有统计显著性很有用。

+   **从预测中获取见解**：这将在*第十一章*，*解释神经网络预测*中进行讨论。

+   **解释神经网络**：这将在*第十二章*，*解释神经网络*中进行讨论。

+   **偏见和公平性分析**：这将在*第十三章*，*探索偏见和公平性*中进行讨论。

+   **对抗分析**：这将在*第十四章*，*分析对抗性能*中进行讨论。

在本书中，我们只会涵盖某种方式与神经网络相关的方法。在下一节中，我们将从工程基线评估方法开始，这是模型评估的度量标准。

# 工程基础模型评估指标

为你的使用场景设计一个度量标准是一项常常被忽视的技能。这很可能是因为大多数项目使用的是公开可用的数据集，这些数据集几乎总是已经有了一个度量标准。包括 Kaggle 上的项目和许多人用来进行基准测试的公共数据集。然而，现实生活中并不是这样，度量标准并不会直接提供给你。让我们在这里进一步探讨这个话题，掌握这一技能。

模型评估指标是监督学习项目中至关重要的首要评估方法，排除无监督学习项目。根据问题和目标类型，有一些基准指标被广泛使用，成为*事实标准*。此外，也有一些定制化版本的基准指标，专门针对特殊目标。例如，生成式任务可以通过一种名为平均意见分数（mean opinion score）的特殊人工评分来进行评估。这里推荐的策略是：总是从基准指标开始，然后逐步调整到能够反映错误在不同条件下如何合理分布的指标，这类似于推荐如何构建模型。

以下是不同条件下的基准指标：

+   **二分类问题**：

    +   **准确率**：这是所有样本中被正确分类的示例（真阳性和真阴性）的百分比。它是各个领域中最广泛知晓的评估指标。然而，实际上，这是一个偏颇的指标，可能会因为大多数数据集中自然存在的负样本过多，掩盖模型实际的正向预测表现。如果数据集中有 99 个负样本和 1 个正样本，仅仅通过始终预测为负样本，就能获得 99% 的准确率！准确率仍然是模型评估的主要方法，但它并不总是能实际使用。当有人说模型是*准确的*，他们可能并不是在使用准确率这一指标。

    +   **精确率**：这是所有预测为正样本的示例中，真阳性占的比例。它是一个健壮的替代指标，重点关注假阳性。这里对于二分类项目需要设置一个预测阈值。

    +   **召回率**：这是所有实际为正样本的示例中，真阳性占的比例。它是一个健壮的替代指标，重点关注假阴性。这里对于二分类项目需要设置一个预测阈值。

    +   **F1 分数**：F1 分数是精确率和召回率的调和均值。它提供了一个平衡的模型性能衡量标准。调和均值的公式是

n _  1 _ x1+  1 _ x2…  1 _ xn

其中，表示样本总数，表示单个样本的值。还有 F2 分数，它对召回率的权重高于精度。如果你对假阳性和假阴性同样重视，可以使用 F1 分数；如果更关注假阴性，可以使用 F2 分数。例如，在入侵检测的应用场景中，如果你想捕捉到所有的入侵者，不能容忍假阴性，但假阳性是可以接受的，因此使用 F2 分数会更合适。注意，使用的是调和平均而非算术平均，以确保对极端值进行惩罚。例如，召回率为 1.0 而精度为 0.01 时，结果将是 0.012，而不是接近 0.5。这里需要一个预测阈值。

+   **接收者操作特征曲线下面积**（**AUC ROC**）：这是 ROC 曲线下的面积，提供了衡量模型区分正例与负例能力的标准。这里不需要阈值。推荐在正负类别平衡时使用，因为不需要像 F1 和 F2 那样调整预测阈值。

+   **平均精度**（**mAP**）：这是精度度量的扩展，在这里，除了使用单一阈值，还使用多个阈值来计算精度，并对结果取平均，以获得在不同阈值下更稳健的精度值。对于多类别问题，平均精度会独立计算每个类别，并取平均得到 mAP。

+   **多分类问题**：

    +   **宏平均**：分别计算每个类别的二分类指标，然后对它们取平均。

    +   **微平均**：通过考虑预测类别中最高的类别，确定整体的真阳性率和假阳性率，然后使用这些比率来计算整体的精度。

+   **回归问题**：

    +   **均方误差**（**MSE**）：这是预测值与实际值之间的平方差的平均值。

    +   **均方根误差**（**RMSE**）：这是均方误差（MSE）的平方根。它提供与目标数据相同尺度的值，通常推荐使用 RMSE 而非 MSE。

    +   **平均绝对误差**（**MAE**）：这是预测值与实际值之间绝对差的平均值。当你关心预测值与实际标签之间的差异，而不关心差异的正负时，使用 MAE 优于 RMSE。

    +   **R 平方**：衡量模型拟合数据的程度，其值范围从 0（拟合差）到 1（拟合完美）。

+   **多标签问题**：

    +   **标签排序平均精度** (**LRAP**)：这是对每个真实标签分配给特定样本的平均精度。它考虑了预测标签与真实标签的排名，并根据真实标签在排名中的远近适当地分配分数。LRAP 适用于电影推荐系统等用例，在这些用例中，预测多个标签及其排名非常重要。它评估模型预测正确标签及其相关性顺序的能力，是需要精确且有意义排名的任务（如类别推荐）的理想度量。

+   **图像和视频标签**：当直接将原始图像和视频帧用作标签时，需使用一套自定义度量标准，能够提供比标准回归度量更有意义的评估。这些标准如下：

    +   **峰值信噪比** (**PSNR**)：这是衡量重建图像或视频质量的度量标准，基于原始图像与重建图像之间的差异。

    +   **结构相似性指数** (**SSIM**)：这是衡量两张图像或视频帧之间结构相似性的度量标准。

+   **文本标签**：尽管标准的分类损失和度量可用于文本预测任务，但一些度量可以衡量更高级的概念，这些概念更符合人类直觉。具体包括：

    +   **蓝色分数**：这是衡量机器生成文本和人类生成文本之间相似性的度量，基于 n-gram 重叠。

    +   **词错误率** (**WER**)：这是自动语音识别系统中的错误率衡量标准，基于词序列中的错误数量。

+   **基于人类质量的度量**：这些是无法通过编程计算的度量，必须由人工手动评估：

    +   **平均意见分数** (**MOS**)：这是由人类观察者给予的主观质量评分，可以用于验证和校准客观质量度量。

    +   **用户参与度**：可以使用网站或应用上的停留时间、点击率或跳出率等度量来评估用户参与度和满意度。

    +   **任务完成率**：这是成功完成特定任务或目标的用户比例，可用于评估产品或服务的可用性和有效性。

基准度量是根据你的问题类型和条件，应该首先选择的一组常见度量。话虽如此，选择使用哪个基准度量仍然取决于具体问题，以及模型性能不同方面之间的重要权衡。以下是逐步建议，帮助你如何选择和使用合适的模型评估度量：

1.  理解问题。考虑问题的性质、数据和期望的结果。这是一个关键步骤，它将帮助你识别出对任务最重要的关键标准。

    1.  留意数据的质量。在*第一章*中介绍的数据质量支柱（代表性、一致性、全面性、唯一性、公平性和有效性）会影响度量标准所代表的实际意义。如果你在一个不良数据集上评估模型的性能，那么所选择的度量标准可能无法反映模型在真实世界数据上的真实表现。

    1.  考虑与模型或模型输出进行交互的用户的视角。他们的期望和需求是什么？需要考虑哪些相关的质量因素？

    1.  明确定义预测需要实现的目标以及它们需要避免的目标。

1.  选择与定义目标一致的度量标准。你选择的度量标准应该与你的整体目标一致。例如，在二分类医学用例中用于检测癌症，错误地做出癌症假阳性诊断可能会毁掉患者多年的生活，因此应选择诸如精准度等度量标准，以量化减少假阳性。

    1.  考虑在类似问题中常用的基础度量标准，以及它们如何需要适应或修改以适应当前的问题。这个问题是否有任何独特的方面需要不同类型的度量标准或对现有度量标准的修改？

    1.  单一度量标准可能无法全面捕捉模型的表现。考虑使用多个度量标准来评估模型表现的不同方面。一个明显的例子是创建一个将精准度和召回率两个度量标准结合的 F1 分数。

1.  考虑度量标准之间的权衡。例如，在二分类项目中，通过修改模型的预测阈值提高召回率的表现可能会不利于精准度。评估度量标准之间的权衡（如果有的话），并选择最能与目标对齐的度量标准。

1.  交叉验证度量标准。确保度量标准是在验证集和持出集上计算的，而不仅仅是在训练集上，这对于估计模型在真实世界中的表现至关重要。在每个 epoch 中计算训练数据和验证数据上的度量标准，也能让你通过所选度量标准直接可视化学习曲线，而不是使用已采用的损失函数。

1.  考虑直接针对度量优化你的模型。有些度量可以在深度学习库中直接近似重现，并作为损失函数使用。直接优化有时可以帮助你获得一个更适合你选择的度量的模型。然而，也可能会出现一些陷阱，我们将在后面的*直接优化度量*部分详细讨论这一点。

1.  构建、评估并比较许多模型，针对评估度量通过迭代改进或使用多种技术进行调整。构建机器学习模型的过程可能只是整个 ML 生命周期中最短的过程。但请记住，你在构建和实验过程中投入的努力可以决定项目的成败。构建多种模型并迭代改进可能是一个艰难且耗时的任务。拥有可以适应大多数应用场景的模板代码，可以让这个过程变得更加高效和无缝，这样你就能将精力集中在项目中更为紧迫的问题上。或者，你也可以考虑使用 AutoML 工具来持续为每个应用场景训练不同的模型。期待本书最后一章，我们将感受 AutoML 工具如何简化模型构建过程！

1.  确保你定义与所选度量相关的成功标准。将所选评估度量转化为实际的成功度量时，重要的是要清楚地了解对于给定问题，什么才算成功。这通常涉及定义一个模型需要达到的性能阈值或目标水平，才能被认为是成功的。有时，成功标准可以根据特定类型的错误或特定数据组有更细致的定义。我们将在本章后面深入探讨这一点。

通过仔细考虑这些建议，可以选择一个独特的度量，准确衡量问题的相关方面。最终，这有助于开发更准确、更有效的机器学习解决方案，从而带来更好的性能和更成功的结果。

基础度量是一组常用的度量，许多从业人员使用它们来评估模型的性能。然而，在某些情况下，特定问题可能有额外的标准和独特的行为，需要在评估模型性能时加以考虑。在这种情况下，可能需要采用替代的或定制的度量，这些度量更适合该问题的具体需求。基础度量可以根据你希望用来评判模型的额外标准进一步调整。下一部分将探讨自定义度量及其应用，包括在何种情况下将其作为最合适的度量来用于特定的应用场景。

# 探索自定义度量及其应用

基本指标通常足以满足大多数使用场景的需求。然而，自定义指标是在基本指标的基础上构建的，融入了特定场景下的附加目标。可以将基本指标视为本科学位，而将自定义指标视为硕士或博士学位。如果基本指标满足你的需求，并且没有其他额外要求，那么只使用基本指标也是完全可以的。

自定义指标通常在项目初期自然出现，并且高度依赖于具体的应用场景。大多数实际应用场景不会公开其选择的指标，即使模型的预测最终是公开使用的，比如**Open AI**的**ChatGPT**。然而，在机器学习竞赛中，拥有实际应用场景和数据的公司会公开其选择的指标，以便找到可以建立的最佳模型。在这样的竞赛项目中，举办比赛的公司有动力进行高质量的指标工程工作，以反映其应用场景的理想。一个指标可以影响最终的最佳模型，如果公司没有工程出与其理想匹配的好指标，最终会花费大量资金。一些比赛提供高达 100,000 美元的奖金！

在本节中，我们将介绍一些常见且公开分享的自定义指标，以及来自机器学习竞赛的相关度量标准和使用案例，这些对于你考虑特定应用场景可能会有所帮助：

| **理想** | **使用案例** | **自定义指标** |
| --- | --- | --- |
| 对于基于时间序列回归的点预测，目标是季节性的，并且可能会根据数据所在的季节大幅波动。我们需要一个度量标准，确保错误不会对任何一个季节的权重过重。 | **M5 预测—准确性（Kaggle）**：这涉及到预测沃尔玛零售商品的销量。该比赛提供了沃尔玛的销售时间序列数据，数据遵循层级结构，从商品层级开始，逐步向部门、产品类别和门店层级扩展。数据慷慨地提供，覆盖了美国的三个地区：加利福尼亚、德克萨斯和威斯康星州。 | **加权均方根标准化误差**（**WRMSSE**）：这里的主要部分是 RMSSE，它是 RMSE 的一个修改版。在应用均方误差的平方根之前，RMSSE 将标准的 MSE 除以使用最近观测作为真实值的 MSE。这确保了任何季节的 RMSE 都会被缩放到相同的值域。 |
| 有些标签/类别在实际中并没有那么重要，可能是因为它们的出现频率较低，或者它们对预测后的决策影响不大。不要过度评价不重要标签的模型表现；应将更多关注放在更重要标签/类别的错误上。 | `any`标签用于考虑所有其他未使用特定标签表示的出血类型。`any`标签的数据量是其他任何单一标签的 2-3 倍。 | `any`标签的权重比其他任何标签都要高，尽管它的数据量更多。这表明，标签对指标的重要性并不单纯依赖于与该标签相关的数据稀缺性；它实际上取决于特定问题的上下文。 |
| **M5 预测——准确度（Kaggle）**：这涉及到预测沃尔玛零售商品的销售数量。竞赛提供了沃尔玛的时间序列销售数据，这些数据遵循分层结构，从商品级别开始，逐步扩展到部门、产品类别和门店级别。数据慷慨提供，涵盖了美国的三个地区：加利福尼亚、德克萨斯和威斯康星州。 | **WRMSSE**：竞赛的组织者更重视那些带来更多销售额的产品的单位销售预测。每个产品的权重是通过使用训练样本最后 28 个观察值中的产品销售量（销售单位的总和乘以其各自的价格）来获得的。 |
|  | **沃尔玛招聘——门店销售预测（Kaggle）**：这涉及到沃尔玛商品的销售预测。 | **加权绝对误差**（**WMAE**）：沃尔玛将假期周的预测误差加权为非假期周的五倍，因为假期周的销售额远高于非假期周。 |
| 我们其实并不太在意小的错误，因为它们可以被容忍，但我们关心的是大的错误，因为它们可能会导致触发不想要的操作，消耗了预测结果。 | **谷歌分析客户收入预测（Kaggle）**：这是一个回归问题，旨在预测在线商店中一个客户的总收入。收入值高度偏斜且包含许多零值，这使得使用传统的度量标准（如 MSE 或 MAE）来评估参与模型的表现变得具有挑战性。 | **均方根对数误差（RMSLE）**：RMSLE 在计算 RMSE 之前，对预测值和实际值应用对数变换，这有助于比小错误更严重地惩罚大错误。实现这一点的自然方式是简单地训练模型去预测目标的对数值，并应用 RMSE 来实现 RMSLE。 |
| **糖尿病视网膜病变检测（Kaggle）**：这是一个多类问题，旨在预测高分辨率视网膜图像是否存在糖尿病视网膜病变。该问题有五个类别：一个表示无病，其他四个表示不同程度的病变。 | **二次加权卡帕系数（QWK）**：卡帕系数是一种统计度量，用于量化多类分类任务中预测标签与实际标签之间的一致程度。二次加权机制使卡帕系数对小错误更为稳健，对大错误更为敏感。二次加权方案可以解决当高分类一致性导致卡帕系数膨胀的问题，同时仍能准确表示更难分类的情况的一致性水平。 |
|  | **Two Sigma Connect** **租赁列表查询（Kaggle）**：这是一个基于房源内容（如文字描述、照片、卧室数量和价格）预测公寓租赁列表受欢迎程度的问题。 | **对数损失（Log loss）**：对数损失是一种评估指标，它通过惩罚那些对错误预测过于自信的模型，来强调错误预测的重要性。 |
| 在一个多类问题中，我们对多类模型的容忍度较高，因为在我们的使用案例中，我们可以使用多个预测类别，而不是仅使用最可能的单一类别，以最大化模型的真正正例命中率表现。 | **Airbnb 新用户预订（Kaggle）**：这是一个多类问题，旨在预测新用户将在哪个国家/地区首次进行 Airbnb 预订（包括无预订类别）。预测的类别将帮助 Airbnb 为其社区提供更个性化的内容，缩短首次预订的平均时间，并更好地预测需求。Airbnb 对模型产生的多类预测有较为宽松的要求，可以根据前五个国家而非单一国家执行个性化内容，以提高真正正例命中率。 | **标准化折扣累计增益（NDCG）前 k 类**：NDCG 是衡量前 k 类排序效果的指标，通常用于推荐系统。该指标与 Airbnb 对多类预测的容忍度相匹配。它适用于任何能够容忍使用多个预测类别，而不是仅使用最预测类别的多类使用案例。 |
| 对于基于视频的多目标跟踪应用场景，我们希望使用一种衡量标准，能够惩罚不希望出现的跟踪行为，例如错误的物体识别和未能持续稳定地跟踪物体。此外，我们需要一种可以通过模型随着时间生成的多个轨迹与固定的真实轨迹集进行比较来计算的度量标准。 | `Track 1`，我们创建了一个大规模的合成数据集，包含动画人物。我们数据集中的所有摄像机视频源都具有高清分辨率（1080p），帧率为每秒 30 帧。 | **身份 F1 得分（IDF1）**：IDF1 通过评估跟踪算法的整体表现来处理此问题，具体是根据它将预测轨迹与真实轨迹匹配的效果，考虑到每个物体的身份随时间变化。该算法会惩罚假阳性、假阴性以及身份切换（即算法错误地将两个检测结果当作同一物体，或错误地将两个不同的物体身份归为同一个物体）。最重要的是，模型可以基于**交并比**（**IOU**）算法的一个版本，动态地确定匹配轨迹，该算法比较预测轨迹与真实轨迹之间的交集和并集。 |
| 我们更关心模型性能中的偏差，而非整体模型的表现。 | **Jigsaw 意外偏差在毒性分类中的应用（Kaggle）** [`www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation`](https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/overview/evalu)：这项任务涉及预测文本数据中毒性的不同强度。数据集包含竞争作者希望在偏差优化方面进行处理的身份。涉及的身份包括男性、女性、跨性别、其他性别、异性恋、同性恋（男同性恋或女同性恋）、双性恋、其他性取向、基督教徒、犹太教徒、穆斯林、印度教徒、佛教徒、无神论者、其他宗教、黑人、白人、亚洲人、拉丁裔、其他种族或族裔、身体残疾、智力或学习障碍、精神或心理疾病以及其他残疾。 | **偏差焦点 AUC**：该竞赛关注多个 AUC 度量的加权组合，这些 AUC 度量是基于数据集中不同身份的特定子集计算的，这些身份可以在文本数据行中提到。总体而言，AUC 与接下来的三个偏差焦点度量等权结合。**基于身份的子群 AUC**：这涉及分析仅包含提到特定身份子群的评论的数据集。在该度量中得分较低表示模型难以区分与该身份相关的有毒评论和无毒评论。 |
|  |  | **背景正例，子群负例（BPSN）AUC**：这涉及评估模型在提到身份的非毒性示例和不提到身份的毒性示例上的表现。此指标得分较低表示模型可能错误地将与身份相关的非毒性示例标记为毒性示例。 |
| **背景负例，子群正例（BNSP）AUC**：这涉及评估模型在提到身份的毒性示例和不提到身份的非毒性示例上的表现。此指标得分较低表示模型可能错误地将与身份相关的毒性示例标记为非毒性示例。 |

表 10.1 – 自定义理想的表格，包含示例用例和使用的指标

在第二个理想中，通用的指标思想是根据您认为更重要的内容对任何选择的指标应用权重。此外，权重还可以更加灵活地应用于任何不作为模型输入或目标的数据。最后，关于最后一个理想，期待*第十三章*，*探索偏见和公平性*，以发现优化偏见的方法。

尽管我们讨论的自定义理想和指标示例是有用的指南，但重要的是要记住，可能有许多不同的指标和理想与您的特定用例相关。不要害怕深入挖掘您的问题领域，找出对您的特定情况独特的指标。

我们提供的示例可以作为开发量身定制的指标的有用备忘单。通过理解在特定领域使用这些特殊指标背后的原因，您可以深入了解在评估模型性能时应考虑的因素。最终，关键是选择与您的目标一致、并能够捕捉到问题领域中最重要方面的指标。

接下来，我们将讨论一种强健的策略，用于比较不同模型在多个指标值下的性能，这些指标值是从不同的交叉验证折叠或数据集划分中计算得出的。

# 探索用于比较模型指标的统计检验

在机器学习中，基于指标的模型评估通常涉及使用来自不同折叠或划分（如保留集和验证集）的聚合指标的平均值，以比较各种模型的性能。然而，仅依赖这些平均性能指标可能无法提供对模型性能和泛化能力的全面评估。一个更强健的模型评估方法是引入统计假设检验，评估观察到的性能差异是否具有统计显著性，或是否只是由于随机机会造成的。

统计假设检验是用来确定观察到的数据是否提供了足够的证据以拒绝零假设，支持替代假设的程序，它帮助量化观察到的差异是否由随机机会或真实效应造成。在统计检验中，零假设（H0）是默认假设，表示变量之间没有效应或关系，作为与替代假设进行比较的基础，目的是确定观察到的数据是否提供了足够的证据以拒绝这一默认假设。在比较多个分区和数据集中的模型指标性能时，零假设通常是模型的性能之间没有差异，而替代假设则是存在差异。

总体而言，统计检验提供了一个正式的框架，以客观地判断性能差异是否显著或仅为偶然。此外，统计检验通过考虑指标中的变异性和不确定性，提供了对模型性能的全面理解。下表展示了常见的统计检验方法，包括执行所需的 Python 代码、如何解释结果以及何时使用它们：

| **统计检验** | **Python 代码** | **结果解释** | **推荐使用** |
| --- | --- | --- | --- |
| 配对 t 检验 | `from scipy.stats` `import ttest_rel``t_stat, p_value =` `ttest_rel(model1_scores, model2_scores)` | 如果 `p_value` < 0.05，模型之间存在显著差异。 | 用于比较两个依赖样本，且差异呈正态分布时。 |
| Mann-Whitney U 检验 | `from scipy.stats` `import mannwhitneyu``u_stat, p_value =` `mannwhitneyu(model1_scores, model2_scores)` | 如果 `p_value` < 0.05，模型之间存在显著差异。 | 用于比较两个独立样本，且数据为非正态分布或序数数据时。 |
| **方差分析** (**ANOVA**) | `from scipy.stats import f_oneway f_stat, p_value = f_oneway(model1_scores,` `model2_scores, model3_scores)` | 如果 `p_value` < 0.05，模型之间存在显著差异。 | 用于比较三个或更多独立样本，且数据呈正态分布且方差相等时。 |
| Kruskal-Wallis H 检验 | `from scipy.stats` `import kruskal``h_stat, p_value = kruskal(model1_scores,` `model2_scores, model3_scores)` | 如果 `p_value` < 0.05，模型之间存在显著差异。 | 用于比较三个或更多独立样本，且数据为非正态分布或序数数据时。 |

表 10.2 – 常见统计检验方法及其 Python 实现、结果解释和推荐使用场景

这些建议可以帮助你根据数据的条件和假设选择适当的统计检验方法，例如样本数量、数据类型（依赖性或独立性）以及数据的分布（正态分布或非正态分布）。

接下来，我们将讨论如何将度量工程的结果转化为成功标准。

# 将评估指标与成功联系起来

在机器学习项目中，定义成功是至关重要的，应当在项目初期就进行定义，正如在*第一章*的*定义成功*部分中介绍的那样，*深度学习生命周期*。成功可以定义为实现更高层次的目标，例如提高流程的效率或相比人工劳动提高流程的准确性。在一些罕见的情况下，机器学习可以使得以前由于人类限制无法实现的流程成为可能。实现这些目标的最终成功是为组织节省成本或创造更多的收入。

一个模型的度量性能得分为 0.80 的 F1 分数或 0.00123 的 RMSE，实际上并没有直接意义，需要在使用案例中转化为可衡量的内容。例如，应该回答诸如什么样的估计模型得分可以使项目实现预定的成本节约或收入增长的问题。从模型性能中量化可以获得的成功是至关重要的，特别是在机器学习项目可能很昂贵的情况下。如果模型无法达到一定的性能水平，投资回报率可能会很低。选择评估指标后，重要的是要为成功设定一个现实、可达的指标阈值，并且该阈值应当基于业务目标。

作为本主题的总结，我们通过一个示例工作流来将评估指标与基于假设使用案例的成功联系起来。

假设我们要考虑一个使用图像数据来识别制造过程中的缺陷产品的使用案例。假设生产一个产品的成本为 50 美元，而其零售价格为 200 美元。如果该产品是有缺陷的并且被送到客户手中，那么将导致 1000 美元的额外退款费用，并且需要退还客户支付的 200 美元，以补偿缺陷产品可能造成的损害。另一方面，如果一个合格的产品被误判为缺陷并被报废，那么就会产生 250 美元的成本；其中 50 美元用于生产该产品，200 美元则是作为报废损失的机会成本。

假设我们有一个包含 10,000 张产品图像的数据集，这是每月生产的产品数量。如果我们不使用模型，生产并运输所有 10,000 个产品，我们将有 95 个有缺陷的产品，这将导致 $500,000 的生产成本（10,000 x $50）和 $95,000 的退货费用，总成本为 $595,000。销售所有非缺陷产品后，公司将获得 $1,981,000 的销售收入（9,905 x $200）。总收入将是 $1,386,000。

假设我们使用一个训练好的深度学习模型来将这些图像分类为合格（负类）或有缺陷（正类）。为了确保使用深度学习模型来优化此过程是值得的，假设公司需要每年至少赚取 $120,000 才能使此过程值得投资。我们还要考虑到，维护一个机器学习模型的成本是每月 $20,000。任何度量的阈值都需要与每月至少赚取 $30,000 的收益相关。

假设我们想使用基于 F 分数的度量。由于假阴性对总金额的影响比假阳性更大，我们可能想使用 F2 分数而不是 F1 分数。但是，使用相同度量分数的模型是否会在这两个度量中表现出不同的货币回报？这是理解的关键，以便可以设置适当的成功阈值。让我们尝试使用 Python 代码来分析分数行为：

1.  首先，我们定义实际正例和实际负例的数量以及总数据量：

    ```py
    actual_positive = 95
    total_data = 10000
    actual_negative = total_data – actual_positive
    ```

1.  接下来，让我们定义方法来计算精准率、召回率、F1 分数和 F2 分数：

    ```py
    def precision(tp, fp):
         denominator = tp + fp
         if denominator == 0:
               return 0
         return tp/ denominator
    def recall(tp, fn):
         denominator = tp + fn
         if denominator == 0:
               return 0
         return tp/denominator
    def f1score(tp, fp, fn):
         prec = precision(tp, fp)
         rec = recall(tp, fn)
         denominator = prec+ rec
         if denominator == 0:
               return 0
         return 2 * (prec * rec) / denominator
    def fbeta(tp, fp, fn, beta=0.5):
         prec = precision(tp, fp)
         rec = recall(tp, fn)
         denominator = beta**2 * prec + rec
         if denominator == 0:
               return 0
         return (1+beta**2) * (prec * rec) / denominator
    ```

1.  接下来，让我们定义计算最终现金的方法：

    ```py
    def compute_total_cash(tp, fp, fn, tn):
         return tp * -50 + fp * -50 + fn * -1050 + tn * 150
    ```

1.  让我们使用 `compute_total_cash` 方法来计算当前设置下的基准现金，这时没有使用任何模型。这样我们可以找出一个模型被认为有足够价值用于使用的现金成功阈值：

    ```py
    baseline_cash = compute_total_cash(0, 0, actual_positive, actual_negative)
    threshold_cash_line = baseline_cash + 30000
    ```

1.  接下来，我们将模拟所有可能的真阳性（`tp`）、假阳性（`fp`）、真阴性（`tn`）和假阴性（`fn`）组合，并计算 F1 和 F2 分数：

    ```py
    f1_scores = []
    f2_scores = []
    total_cash = []
    for tp in range(0, 96):
         fn = 95 - tp
         for fp in range(0, actual_negative):
               tn = total_data - tp - fp - fn
               f1_scores.append(f1score(tp, fp, fn))
               f2_scores.append(fbeta(tp, fp, fn, beta=2.0))
               total_cash.append(compute_total_cash(tp, fp, fn, tn)
    ```

1.  现在，让我们分别绘制这两个分数与总现金回报的关系，同时使用 `threshold_cash_line` 和 `baseline_cash` 绘制水平线：

    ```py
    import matplotlib.pyplot as plt
    fig, axs = plt.subplots(2, figsize=(18, 15))
    axs[0].scatter(f1_scores, total_cash, alpha=0.01)
    axs[1].scatter(f2_scores, total_cash, alpha=0.01)
    for i in range(2):
         axs[i].axhline(y=baseline_cash, color='r', linestyle='dotted')
         axs[i].axhline(y=threshold_cash_line, color='y', linestyle='dashdot')
    ```

    这将产生以下图形：

![](img/B18187_10.01_(A).jpg)![图 10.1 – 现金与 F1 分数的关系，以及现金与 F2 分数的关系](img/B18187_10.01_(B).jpg)

图 10.1 – 现金与 F1 分数的关系，以及现金与 F2 分数的关系

1.  图表表明，尽管 F2 分数对召回率进行了较高的加权，但在较低分数范围内波动很大。这里的目标是确保度量标准能与成功正确关联，因此，在相同分数下出现更大的现金波动并不是一个理想特征。F1 分数在这种情况下可能是更明智的选择。以最上方的水平线（它指的是我们需要达到的最低 30,000 现金阈值）为参考，我们想找到一个点，在 F1 分数图中，无法获得比阈值更低的分数。大致上，0.65 的 F1 分数应能保证模型产生一个有效分数。

本例展示了正确选择度量标准并找到可以直接与成功关联的阈值所需的分析层次，同时考虑到货币的盈利与亏损。然而，重要的是要注意，并非所有机器学习项目都能以美元成本来衡量。一些项目可能与现金无关，这完全是可以接受的。然而，要想取得成功，关键是以利益相关者能理解的方式量化模型的价值。如果没有人理解模型所带来的价值，那么项目不太可能成功。

接下来，让我们深入探讨在深度学习模型中直接优化度量标准的概念。

# 直接优化度量标准

用于训练深度学习模型的损失函数和度量标准是两个独立的组成部分。你可以用来提高模型准确性的技巧之一是直接针对度量标准进行优化，而不仅仅是监控性能，以选择表现最佳的模型权重并使用提前停止。换句话说，就是直接将度量标准作为损失函数使用！

通过直接优化感兴趣的度量标准，模型有机会以与最终目标相关的方式改进，而不是仅仅为了优化一个可能与模型的最终表现无直接关系的代理损失函数。这意味着，当将度量标准作为损失函数直接使用时，模型可能会取得更好的表现。

然而，并非所有度量标准都可以用作损失函数，因为并非所有度量标准都是可微的。记住，反向传播要求所使用的所有函数都是可微的，以便计算梯度来更新神经网络的权重。请注意，离散的函数并不都可微。以下是一些常见的不可微离散函数，以及用于快速识别的 NumPy 方法：

+   `np.min`，`np.max`，`np.argmin`，和`np.argmax`

+   `np.clip`

+   NumPy 中需要注意的其他函数包括`np.sign`，`np.piecewise`，`np.digitize`，`np.searchsorted`，`np.histogram`，`np.fft`，`np.count_nonzero`，`np.round`，`np.cumsum`，和`np.percentile`

此外，使用度量作为损失函数有时可能导致次优性能，因为度量并不总是能够捕捉到模型需要学习的所有方面，这些方面对模型的良好表现至关重要。问题的一些重要方面可能很难直接度量或包含在度量中。这些方面可能构成模型学习的基础，模型必须先学习这些内容，才能逐渐在选定的度量上取得更好的表现。例如，在图像识别中，模型需要先学习识别更抽象的特征，如纹理、光照或视角，才能尝试在准确性上取得进展。如果这些特征没有在度量中得到体现，模型可能不会学会识别它们，导致次优的性能。一个好的解决方案是，初期可以先尝试更常见的损失函数，然后使用度量作为损失函数，或者结合原始损失函数和度量来微调模型。

虽然在某些情况下使用度量作为损失函数可能是有益的，但这并不是提升性能的万无一失的方法。这种方法的有效性在很大程度上依赖于具体的应用场景以及所处理问题的复杂性。通过这种方法获得的性能提升可能微乎其微，对于某些项目来说，甚至难以察觉。然而，当成功应用时，它可以带来性能的实质性提升。

# 总结

在本章中，我们简要概述了不同的模型评估方法，以及如何利用这些方法来衡量深度学习模型的性能。我们从所有介绍的方法中，首先探讨了度量工程的话题。我们介绍了常见的基本模型评估度量标准。基于此，我们讨论了使用基本模型评估度量标准的局限性，并引入了为特定问题量身定制模型评估度量标准的概念。我们还探讨了通过将度量作为损失函数直接进行优化的想法。虽然这种方法可以带来好处，但也需要考虑潜在的陷阱和局限性，以及这种方法适用于的特定使用场景。

深度学习模型的评估需要仔细考虑适当的评估方法、度量标准和统计检验。希望通过阅读本章内容，我能帮助你顺利进入度量工程的领域，鼓励你根据提供的指南迈出深入度量工程的第一步，并强调度量工程作为提升模型评估的重要组成部分的意义。

然而，无论最终选择的度量标准多么优秀和恰当，某些信息仍然可能隐藏在单一的度量值背后。在下一章，我们将介绍一种关键方法，帮助揭示神经网络模型在做出预测时隐藏的、无论是期望的还是不期望的行为。
