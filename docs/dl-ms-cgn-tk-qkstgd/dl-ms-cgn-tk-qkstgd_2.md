# 第二章：使用 CNTK 构建神经网络

在上一章中，我们讨论了深度学习是什么，以及神经网络如何在概念层面上运作。最后，我们讨论了 CNTK 以及如何在你的机器上安装它。在本章中，我们将使用 CNTK 构建并训练我们的第一个神经网络。

我们将使用 CNTK 库中的不同函数和类来构建一个神经网络。我们将通过一个基本的分类问题来实现。

一旦我们为分类问题构建了一个神经网络，我们将使用从开放数据集中获取的样本数据对其进行训练。训练完神经网络后，我们将学习如何使用它来进行预测。

在本章的最后，我们将花一些时间讨论在训练完模型后，如何提高模型性能。

本章将覆盖以下主题：

+   CNTK 中的基本神经网络概念

+   构建你的第一个神经网络

+   训练神经网络

+   使用神经网络进行预测

+   改进模型

# 技术要求

在本章中，我们将处理一个使用 Python 在 Jupyter 笔记本中构建的示例模型。Jupyter 是一种开源技术，它允许你创建包含 Python 代码、Markdown 和 HTML 的交互式网页。这使得文档化代码以及记录在构建深度学习模型时所做的假设变得更加容易。

如果你按照第一章中定义的步骤安装了 Anaconda，*开始使用 CNTK*，那么你已经在你的机器上安装了 Jupyter。如果你还没有安装 Anaconda，你可以从以下网址下载：[`anacondacloud.com/download`](https://www.anaconda.com/download/)。

你可以从以下网址获取本章的示例代码：[`github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch2`](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch2)。要运行示例代码，请在下载代码的目录中通过终端运行以下命令：

```py
cd ch2
jupyter notebook
```

找到`Train your first model.ipynb`笔记本，点击它以打开示例代码。你可以选择 Cell | Run All 一次性执行所有代码，这将执行笔记本中的所有步骤。

查看以下视频，了解代码的实际运行：

[`bit.ly/2YoyNKY`](http://bit.ly/2YoyNKY)

# CNTK 中的基本神经网络概念

在上一章中，我们学习了神经网络的基本概念。现在我们将这些概念映射到 CNTK 库中的组件，并探索如何利用这些概念来构建你自己的模型。

# 使用层函数构建神经网络

神经网络是由多个神经元层组成的。在 CNTK 中，我们可以使用在 layers 模块中定义的层函数来建模神经网络的各个层。CNTK 中的`layer`函数看起来就像一个普通的函数。例如，你可以通过一行代码创建最基本的层类型`Dense`：

```py
from cntk.layers import Dense
from cntk import input_variable

features = input_variable(100)
layer = Dense(50)(features)
```

按照给定的步骤创建最基本的层类型：

1.  首先，从 layers 包中导入`Dense`层函数

1.  接下来，从`cntk`根包中导入`input_variable`函数

1.  使用`input_variable`函数创建一个名为 features 的新输入变量，并将其大小设置为`100`

1.  使用`Dense`函数创建一个新层，提供所需的神经元数量

1.  调用配置好的`Dense`层函数，提供特征变量，以将`Dense`层与输入连接起来

在 CNTK 中处理层具有鲜明的函数式编程风格。当我们回顾前一章时，我们可以理解为什么 CNTK 选择了这种方式。归根结底，神经网络中的每一层都是一个数学函数。CNTK 中的所有层函数都会生成一个具有一组预定义参数的数学函数。再次调用该函数时，你将绑定最后一个缺失的参数——输入，来连接层。

当你想要创建具有复杂架构的神经网络时，通常会使用这种风格的编程。但是，对于大多数初学者来说，函数式风格可能会感到陌生。CNTK 提供了一个更简单的 API，当你想通过`Sequential`层函数构建基础神经网络时，可以使用它。

你可以使用`Sequential`层函数将多个层链接在一起，而无需使用函数式编程风格，方法如下：

```py
from cntk.layers import Sequential, Dense
from cntk import input_variable

features = input_variable(7)

network = Sequential([
  Dense(64),
  Dense(32),
  Dense(3)
])(features)
```

按照给定的步骤操作：

1.  首先，从`layers`包中导入你想使用的层函数

1.  导入`input_variable`函数来创建一个输入变量，用于将数据输入到神经网络

1.  创建一个新的输入变量，将数据输入到神经网络

1.  通过调用`Sequential`函数创建一个新的顺序层块

1.  将你想要链接在一起的层列表提供给`Sequential`函数

1.  调用配置好的`Sequential`函数对象，提供特征输入变量以完成网络结构

通过将`Sequential`函数与其他层函数结合，你可以创建任何神经网络结构。在下一部分中，我们将看看如何通过设置自定义层，以配置诸如`activation`函数等内容。

# 自定义层设置

CNTK 为构建神经网络提供了一套非常好的默认设置。但你会发现自己经常需要对这些设置进行实验。神经网络的行为和性能会根据你选择的`activation`函数和其他设置有所不同。因此，了解你可以配置哪些选项是很有帮助的。

每个层都有自己独特的配置选项，其中一些你会经常使用，另一些则用得较少。当我们查看 `Dense` 层时，有几个重要的设置是你需要定义的：

+   `shape`：层的输出形状

+   `activation`：层的 `activation` 函数

+   `init`：层的 `initialization` 函数

层的输出形状决定了该层中神经元的数量。每个神经元需要定义一个 `activation` 函数，以便它能转换输入数据。最后，我们需要一个函数来初始化该层的参数，以便我们开始训练神经网络。输出形状是每个 `layer` 函数中的第一个参数。`activation` 和 `init` 参数作为关键字参数提供。这些参数有默认值，因此如果你不需要自定义设置，可以省略它们。以下示例演示了如何使用自定义 `initializer` 和 `activation` 函数配置 `Dense` 层：

```py
from cntk.layers import Dense
from cntk.ops import sigmoid
from cntk.initializer import glorot_uniform

layer = Dense(128, activation=sigmoid, init=glorot_uniform)
```

配置 Dense 层的步骤如下：

1.  首先，从 `layers` 包中导入 `Dense` 层

1.  接下来，从 `ops` 包中导入 `sigmoid` 运算符，以便我们可以将其配置为 `activation` 函数

1.  然后从 `initializer` 包中导入 `glorot_uniform` 初始化函数

1.  最后，使用 `Dense` 层创建一个新层，将神经元数量作为第一个参数，并提供 `sigmoid` 运算符作为 `activation` 函数，`glorot_uniform` 函数作为层的 `init` 函数

有几种 `activation` 函数可供选择；例如，你可以使用 **修正线性单元** (**ReLU**) 或 `sigmoid` 作为 `activation` 函数。所有的 `activation` 函数都可以在 `cntk.ops` 包中找到。

每个 `activation` 函数对神经网络的性能会产生不同的影响。当我们在本章稍后构建神经网络时，我们将更详细地讨论 `activation` 函数。

初始化函数决定了当我们开始训练神经网络时，层中参数的初始化方式。你可以从 CNTK 中选择多种初始化函数。`Normal`、`uniform` 和 `glorot_uniform` 是 `cntk.initializer` 包中一些常用的初始化函数。当我们开始解决第一个深度学习问题时，我们会更详细地讨论选择哪个初始化函数。

无论你使用的是 CNTK 中的哪种初始化函数，都需要意识到它们使用随机数生成器来生成层中参数的初始值。这是一个重要的技术，因为它允许神经网络有效地学习正确的参数。CNTK 中的所有初始化函数都支持额外的种子设置。当你将此参数设置为固定值时，每次训练神经网络时都会得到相同的初始值。这在你尝试重现问题或实验不同设置时非常有用。

当你构建神经网络时，通常需要为网络中的多个层指定相同的设置。当你在实验你的模型时，这可能会变得很麻烦。为了解决这个问题，CNTK 提供了一个名为`default_options`的`utility`函数：

```py
from cntk import default_options
from cntk.layers import Dense, Sequential
from cntk.ops import sigmoid

with default_options(activation=sigmoid):
  network = Sequential([
    Dense(1024),
    Dense(512),
    Dense(256)
  ])
```

通过使用`default_options`函数，我们只需一行代码，就为所有三层配置了`sigmoid`激活函数。`default_options`函数接受一组标准设置，这些设置会应用到此函数作用范围内的所有层。使用`default_options`函数使得为一组层配置相同选项变得更加便捷。通过这种方式，你可以配置很多设置，例如，使用以下函数：

+   `activation`：要使用的`activation`函数

+   `init`：层的`initialization`函数

+   `bias`：是否应在层中包含`bias`项

+   `init_bias`：`bias`项的初始化函数

# 使用`learners`和`trainers`优化神经网络中的参数

在前面的章节中，我们已经学习了如何创建神经网络的结构以及如何配置各种设置。现在，让我们看看如何使用`learners`和`trainers`来优化神经网络的参数。在 CNTK 中，神经网络的训练是通过两部分组成的组合来完成的。第一部分是`trainer`组件，它实现了反向传播过程。第二部分是`learner`，它负责执行我们在第一章《**CNTK 入门**》中看到的梯度下降算法。

`trainer`将数据传递通过神经网络以获得预测。然后使用`learner`来获取神经网络中参数的新值。接着应用这些新值，并重复这个过程。这个过程一直进行，直到满足退出条件。当达到配置的迭代次数时，训练过程会停止。可以通过自定义回调来增强这一过程。

我们在第一章《**CNTK 入门**》中讨论了一个非常基础的梯度下降方法。但实际上，这个基本算法有许多变种。基本的梯度下降在复杂的情况下效果并不好。它经常会陷入局部最优解（可以理解为山坡上的一个小凸起），因此无法达到神经网络参数的全局最优值。其他算法，例如带动量的**随机梯度下降**（**SGD**），考虑了局部最优，并使用动量等概念来跨越损失曲线的坡度中的“凸起”。

这里列出了一些在 CNTK 库中包含的有趣`learners`：

+   **SGD**：基本的随机梯度下降，没有任何额外功能

+   **MomentumSGD**：应用动量来克服局部最优解

+   **RMSProp**：使用衰减学习率来控制下降速率

+   **Adam**：使用衰减的动量来减少随着时间推移的下降速率

+   **Adagrad**：为频繁和不频繁出现的特征使用不同的学习率

重要的是要知道，你可以根据你想要解决的问题选择不同的 `learners`。当我们开始使用神经网络解决第一个机器学习问题时，我们将进一步了解如何选择合适的优化器。

# 损失函数

为了使 `trainer` 和 `learner` 能够优化神经网络的参数，我们需要定义一个衡量神经网络损失的函数。`loss` 函数计算的是神经网络预测输出与我们预先知道的期望输出之间的差距。

CNTK 包含了多个 `loss` 函数，位于 `cntk.losses` 模块中。每个 `loss` 函数都有其特定的用途和特点。例如，当你想衡量一个预测连续值的模型的损失时，你需要使用 `squared_error` 损失函数。它衡量的是模型生成的预测值与在训练模型时提供的真实值之间的距离。

对于分类模型，你需要一组不同的 `loss` 函数。`binary_cross_entropy` 损失函数可以用来衡量用于二分类任务（如欺诈检测模型）模型的损失。`cross_entropy_with_softmax` 损失函数更适用于预测多个类别的分类模型。

# 模型度量

将 `learner` 和 `loss` 函数与 `trainer` 结合起来可以优化神经网络中的参数。这应该会生成一个好的模型，但为了确保这一点，我们需要使用度量来衡量模型的性能。度量是一个单一的数值，它告诉我们，例如，有多少百分比的样本被正确预测。

因为 `loss` 函数衡量的是实际值与预测值之间的差异，你可能会认为它是衡量我们模型表现的一个好指标。根据模型的不同，它可能提供一些有用的信息，但通常你需要使用单独的 `metric` 函数来以有意义的方式衡量模型的表现。

CNTK 提供了多个不同的 `metric` 函数，位于 `cntk.metrics` 包中。例如，如果你想衡量分类模型的性能，可以使用 `classification_error` 函数。它用于衡量正确预测的样本百分比。

`classification_error` 函数只是一个度量的例子。另一个重要的 `metric` 函数是 `ndcg_at_1` 度量。如果你正在使用排序模型，那么你会关心模型是如何根据预定义的排序来排列样本的。`ndcg_at_1` 度量就是用来评估这一点的。

# 构建你的第一个神经网络

现在我们已经了解了 CNTK 提供的概念来构建神经网络，我们可以开始将这些概念应用到一个实际的机器学习问题中。在这一部分，我们将探讨如何使用神经网络对鸢尾花的品种进行分类。

这不是一个典型的任务，通常你会选择使用神经网络。但正如你很快会发现的，数据集足够简单，可以帮助你很好地理解构建深度学习模型的过程，同时也包含足够的数据来确保模型的合理性能。

鸢尾花数据集描述了不同品种鸢尾花的物理属性：

+   萼片长度（单位：厘米）

+   萼片宽度（单位：厘米）

+   花瓣长度（单位：厘米）

+   花瓣宽度（单位：厘米）

+   类别（鸢尾花 Setosa，鸢尾花 Versicolor，鸢尾花 Virginica）

本章的代码包括鸢尾花数据集，您需要在此数据集上训练深度学习模型。如果您感兴趣，可以在线查找原始文件：[`archive.ics.uci.edu/ml/datasets/Iris`](http://archive.ics.uci.edu/ml/datasets/Iris)。这些文件也包括在本章的示例代码中。

我们将构建一个深度学习模型，该模型将基于萼片宽度和长度、花瓣宽度和长度来分类花卉。我们可以预测三种不同的类别作为模型的输出。

我们总共有 150 个不同的样本来进行训练，这应该足以在我们尝试使用模型来分类花卉时获得合理的性能。

# 构建网络结构

首先，我们需要确定要为神经网络使用什么架构。我们将构建一个常规的神经网络，通常被称为前馈神经网络。

我们首先需要定义输入层和输出层的神经元数量。接下来，我们需要定义神经网络中隐藏层的形状。由于我们解决的问题相对简单，所以我们不需要超过一层隐藏层。

当我们查看数据集时，我们可以看到它有四个特征和一个标签。由于我们有四个特征，我们需要确保神经网络的输入层有四个神经元。

接下来，我们需要为神经网络定义输出层。为此，我们需要查看需要模型能够预测的类别数量。在我们的例子中，我们有三种不同的花卉品种可以选择，因此我们需要在输出层中设置三个神经元。

首先，我们从 CNTK 库中导入必要的组件，这些组件包括层类型、`activation` 函数以及允许我们为网络定义输入变量的函数：

```py
from cntk import default_options, input_variable
from cntk.layers import Dense, Sequential
from cntk.ops import log_softmax, relu
```

然后，我们使用 `Sequential` 函数创建模型，并将我们需要的层传递给它。我们在网络中创建了两个不同的层——首先是一个有四个神经元的层，然后是一个有三个神经元的层：

```py
model = Sequential([
    Dense(4, activation=relu),
    Dense(3, activation=log_softmax)
])
```

最后，我们将网络与输入变量绑定，这将编译神经网络，使其具有一个包含四个神经元的输入层和一个包含三个神经元的输出层，如下所示：

```py
features = input_variable(4)
z = model(features)
```

现在，让我们回到我们的层结构。注意到我们在调用`Sequential`层函数时没有建模输入层。这是因为我们在代码中创建的`input_variable`就是神经网络的输入层。

在`sequential`调用的第一个层是网络中的隐藏层。通常的经验法则是，你希望隐藏层的大小不超过前一层神经元数量的两倍。

你需要通过实验来优化这个设置，以获得最佳结果。选择神经网络的层数和神经元数目需要一些经验和实验。没有硬性规定说明你应该包含多少个隐藏层。

# 选择激活函数

在前面的章节中，我们为神经网络选择了`sigmoid`激活函数。选择正确的激活函数对深度学习模型的性能有很大影响。

你会发现关于选择`激活`函数有很多不同的观点。这是因为有很多选择，而对于专家所做的选择，并没有足够的实质性证据。那么，如何为你的神经网络选择一个合适的激活函数呢？

# 选择输出层的激活函数

首先，我们需要定义我们正在解决的问题类型。这决定了你网络输出层的`激活`函数。对于回归问题，你需要在输出层使用`线性`激活函数。对于分类问题，二分类问题使用`sigmoid`函数，多分类问题使用`softmax`函数。

在我们构建的模型中，我们需要预测三类中的一种，这意味着我们需要在输出层使用`softmax`激活函数。

# 选择隐藏层的激活函数

现在，让我们来看一下隐藏层。为我们模型的隐藏层选择一个`激活`函数要难得多。我们需要进行一些实验并监控性能，以查看哪种`激活`函数效果最佳。

对于分类问题，像我们花卉分类模型那样，我们需要一些可以给出概率值的东西。我们需要这样做，因为我们需要预测一个样本属于某个特定类别的概率。`sigmoid`函数帮助我们实现这个目标。它的输出是一个概率，值介于 0 和 1 之间。

使用`sigmoid`激活函数时，我们需要解决一些问题。当你创建更大的网络时，你可能会遇到一个叫做**梯度消失**的问题。

给`sigmoid`函数输入非常大的值时，输出会趋近于零或一，具体取决于输入值是负数还是正数。这意味着，当我们使用模型的大输入值时，`sigmoid`函数的输出变化不大。输入值的变化将只导致输出的非常小的变化。优化器在训练时计算的梯度也非常小。有时，它会小到计算机将其四舍五入为零，这意味着优化器无法检测到参数值的调整方向。当优化器由于 CPU 的四舍五入问题无法计算梯度时，我们就会遇到梯度消失问题。

为了解决这个问题，科学家们提出了一种新的激活函数，`ReLU`。这个激活函数将所有负值转化为零，并且对正值起到通过过滤器的作用。它有助于解决梯度消失问题，因为它不会限制输出值。

然而，`ReLU`函数有两个问题。首先，它将负输入值转换为零。在某些情况下，这可能导致优化器将某些参数的权重也设置为零。这会导致网络中出现“死亡神经元”，当然，这会限制网络的功能。

第二个问题是，`ReLU`函数存在梯度爆炸的问题。因为该函数输出的上限没有限制，它可能会放大信号，导致优化器计算出接近无穷大的梯度。当你将这个梯度应用到网络中的参数时，网络开始输出 NaN 值。

选择隐藏层的正确激活函数需要一定的实验。再次强调，并没有硬性规定说要使用哪种激活函数。在本章的示例代码中，我们在对模型进行一些实验后，选择了`sigmoid`函数。

# 选择损失函数

当我们有了模型的结构后，就该考虑如何优化它了。为此，我们需要一个需要最小化的`loss`函数。可以选择的`loss`函数有很多。

合适的`loss`函数取决于你要解决的问题。例如，在像我们这样的分类模型中，我们需要一个能够衡量预测类别与实际类别之间差异的`loss`函数。它需要针对三个类别进行计算。`categorical cross entropy`函数是一个不错的选择。在 CNTK 中，这个`loss`函数实现为`cross_entropy_with_softmax`：

```py
label = input_variable(3)
loss = cross_entropy_with_softmax(z, label)
```

我们需要先从`cntk.losses`包中导入`cross_entropy_with_softmax`函数。导入`loss`函数后，我们创建一个新的输入变量，以便将期望标签传递给`loss`函数。然后，我们创建一个新的`loss`变量，保存对`loss`函数的引用。CNTK 中的任何`loss`函数都需要模型的输出和期望标签的输入变量。

# 记录指标

在结构搭建好并拥有`loss`函数后，我们就拥有了优化深度学习模型所需的所有元素。但在我们开始训练模型之前，让我们先看一下指标。

为了让我们看到网络的表现，我们需要记录一些指标。由于我们正在构建一个分类模型，我们将使用`classification_error`指标。该指标会生成一个介于 0 和 1 之间的数值，表示正确预测的样本百分比：

```py
error_rate = classification_error(z, label)
```

让我们从`cntk.metrics`包中导入`classification_error`。接着，我们创建一个新的`error_rate`变量，并将`classification_error`函数绑定到它。该函数需要网络的输出和期望标签作为输入。我们已经在定义模型和`loss`函数时准备好了这些。

# 训练神经网络

现在我们已经定义了深度学习的所有组件，接下来就可以开始训练了。你可以使用`learner`和`trainer`的组合在 CNTK 中训练模型。我们需要定义这些组件，然后通过`trainer`喂入数据来训练模型。让我们看看如何操作。

# 选择学习器并设置训练

有多种`learners`可供选择。对于我们的第一个模型，我们将使用`stochastic gradient descent`学习器。让我们配置`learner`和`trainer`来训练神经网络：

```py
from cntk.learners import sgd
from cntk.train.trainer import Trainer

learner = sgd(z.parameters, 0.01)

trainer = Trainer(z, (loss, error_rate), [learner])
```

要配置`learner`和`trainer`以训练神经网络，请按照以下步骤进行：

1.  首先，从`learners`包中导入`sgd`函数。

1.  然后，从`trainer`包中导入`Trainer`，该包是`train`包的一部分。

1.  现在通过调用`sgd`函数并提供模型的参数和学习率值来创建`learner`。

1.  最后，初始化`trainer`并提供网络、`loss`与`metric`的组合以及`learner`

我们提供给`sgd`函数的学习率控制优化速度，应该是一个小数字，通常在 0.1 到 0.001 之间。

请注意，每个`learner`都有自己的参数，因此请务必查看文档，了解在使用`cntk.learners`包中的特定`learner`时需要配置哪些参数。

# 向训练器输入数据以优化神经网络

我们花了很多时间来定义模型、配置`loss`、`metrics`，最后是`learner`。现在是时候在我们的数据集上进行训练了。然而，在我们训练模型之前，我们需要先加载数据集。

示例代码中的数据集存储为 CSV 文件。为了加载该数据集，我们需要使用类似`pandas`的数据处理包。这个包在你的 Anaconda 安装中默认包含。以下示例展示了如何使用`pandas`将数据集加载到内存中：

```py
import pandas as pd

df_source = pd.read_csv('iris.csv', 
    names=['sepal_length', 'sepal_width','petal_length','petal_width', 'species'], 
    index_col=False)
```

使用`pandas`将数据集加载到内存中，请按照以下步骤进行操作：

1.  首先，导入`pandas`包并为其起别名`pd`

1.  然后，调用`read_csv`函数从磁盘加载`iris.csv`文件

由于 CSV 文件没有列头，我们需要自己定义它们。这样以后引用特定列时会更加方便。

通常，`pandas`会使用输入文件中的第一列作为数据集的索引。索引将作为识别记录的键。由于我们的数据集中没有索引，因此我们通过`index_col`关键字参数禁用它的使用。

加载完数据集后，让我们将其拆分为特征集和标签集：

```py
X = df_source.iloc[:, :4].values
y = df_source['species'].values
```

要将数据集拆分为特征集和标签集，请按照以下步骤进行操作：

1.  首先，使用`iloc`函数从数据集中选择所有行和前四列

1.  接下来，从数据集中选择物种列，并使用 values 属性访问底层的`numpy`数组

我们的模型需要数值型输入值。但是，物种列是一个字符串值，表示花的类型。我们可以通过将物种列编码为数值向量表示来解决这个问题。我们正在创建的向量表示与神经网络的输出神经元数量匹配。向量中的每个元素代表一种花的物种，具体如下：

```py
label_mapping = {
    'Iris-setosa': 0,
    'Iris-versicolor': 1,
    'Iris-virginica': 2
}
```

为了创建物种的 one-hot 向量表示，我们将使用一个小的`utility`函数：

```py
def one_hot(index, length):
    result = np.zeros(length)
    result[index] = 1

    return result
```

`one_hot`函数执行以下步骤：

1.  首先，初始化一个填充为零的新数组，长度为所需的`length`

1.  接下来，选择指定的`index`处的元素并将其设置为`1`

现在我们已经有了一个将物种映射到索引的字典，并且有了创建 one-hot 向量的方法，我们可以通过添加一行代码将字符串值转换为其向量表示：

```py
y = np.array([one_hot(label_mapping[v], 3) for v in y])
```

按照以下步骤进行操作：

1.  首先，创建一个列表表达式遍历数组中的所有元素

1.  对数组中的每个值，在`label_mapping`字典中查找对应的值

1.  接下来，将这个转换后的数值应用`one_hot`函数转换为 one-hot 编码向量

1.  最后，将转换后的列表转换为`numpy`数组

当你训练深度学习模型或任何机器学习模型时，你需要记住，计算机将尝试记住你用于训练模型的所有样本。与此同时，它将尝试学习一般规则。当模型记住样本但不能从训练样本中推断规则时，它就会在数据集上过拟合。

为了检测过拟合，你需要将数据集的一小部分与训练集分开。训练集用于训练模型，而测试集则用于衡量模型的性能。

我们可以使用`scikit-learn`包中的`utility`函数，将数据集分为训练集和测试集：

```py
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y)
```

按照以下步骤操作：

1.  首先，从`sklearn`包中的`model_selection`模块导入`train_test_split`函数

1.  然后，调用`train_test_split`函数，传入特征`X`和标签`y`

1.  指定`test_size`为`0.2`，将 20%的数据留作测试集

1.  使用`stratify`关键字参数，传入标签数组`y`中的值，这样我们就能确保在训练集和测试集中，每个物种的样本数量是均等的。

如果你不使用`stratify`参数，最终可能会得到一个数据集，其中某个类别的样本完全没有，而另一个类别的样本却过多。这样，模型就无法学习如何分类缺失的类别，而会在另一个样本过多的类别上出现过拟合。

现在我们有了训练集和验证集，让我们看看如何将它们输入到模型中进行训练：

```py
trainer.train_minibatch({ features: X_train, label: y_train })
```

要训练模型，调用`trainer`上的`train_minibatch`方法，并传入一个字典，将输入数据映射到你用来定义神经网络及其相关`loss`函数的输入变量。

我们使用`train_minibatch`方法作为将数据输入到`trainer`中的便捷方式。在下一章中，我们将讨论其他输入数据的方式，并详细介绍`train_minibatch`方法的作用。

请注意，你需要调用`train_minibatch`方法多次，才能让网络得到充分训练。所以我们需要写一个简短的循环来调用这个方法：

```py
for _epoch in range(10):
    trainer.train_minibatch({ features: X_train, label: y_train })

    print('Loss: {}, Acc: {}'.format(
        trainer.previous_minibatch_loss_average,
        trainer.previous_minibatch_evaluation_average))
```

按照以下步骤操作：

1.  首先，使用`for`语句创建一个新的循环，并设置范围为`10`

1.  在循环中调用`train_minibatch`方法，并将输入变量与相关数据进行映射

1.  最后，打印出`previous_minibatch_loss_average`和`previous_minibatch_evaluation_average`，以监控训练进展。

当你调用`train_minibatch`方法时，`trainer`会更新`loss`函数的输出和我们提供给`trainer`的`metric`函数的值，并将其存储在`previous_minibatch_evaluation_average`中。

每次循环完成，并且我们将整个数据集通过`trainer`运行一次后，就完成了一个训练周期。正如我们在前一章中看到的那样，通常需要运行多个周期，直到模型表现足够好。作为额外的奖励，我们还会在每个周期后打印出`trainer`的进度。

# 检查神经网络的性能

每次我们将数据传递给训练器以优化模型时，它会通过我们为训练器配置的度量标准来衡量模型的表现。训练期间衡量的模型性能是基于训练集的。衡量训练集上的准确率很有用，因为它能告诉你模型是否从数据中学到了什么。

要全面分析模型的性能，你需要使用测试集来衡量模型的表现。可以通过以下方式调用`trainer`上的`test_minibatch`方法：

```py
trainer.test_minibatch( {features: X_test, label: y_test })
```

该方法接受一个字典，字典中包含输入变量与变量数据之间的映射关系。该方法的输出是你之前配置的`metric`函数的输出。在我们的例子中，它是基于我们输入的数据计算的模型准确率。

当测试集的准确率高于训练集的准确率时，我们会遇到欠拟合的情况。当测试集的准确率低于训练集的准确率时，我们会遇到过拟合的情况。

无论是欠拟合还是过拟合，过度都不好。当测试集和训练集的准确率几乎相同时，模型性能最佳。我们将在第四章中详细讨论模型性能，*验证模型性能*。

# 使用神经网络进行预测

训练深度学习模型后，最令人满意的事情之一就是将它实际应用到应用程序中。目前，我们将仅限于使用从测试集中随机挑选的样本来使用模型。但稍后，在第七章，*将模型部署到生产环境*中，我们将探讨如何将模型保存到磁盘，并在 C#或.NET 中使用它来构建应用程序。

让我们编写代码，通过我们训练过的神经网络进行预测：

```py
sample_index = np.random.choice(X_test.shape[0])
sample = X_test[sample_index]

inverted_mapping = {
    1: 'Iris-setosa',
    2: 'Iris-versicolor',
    3: 'Iris-virginica'
}

prediction = z(sample)
predicted_label = inverted_mapping[np.argmax(prediction)]

print(predicted_label)
```

按照以下步骤操作：

1.  首先，使用`np.random.choice`函数从测试集中随机挑选一个项目

1.  然后使用生成的`sample_index`从测试集中选择样本数据

1.  接下来，创建一个反向映射，以便你可以将神经网络的数值输出转换为实际标签

1.  现在，使用选定的`sample`数据，通过将神经网络`z`作为函数来进行预测

1.  从预测输出中，使用`numpy`包中的`np.argmax`函数获取具有最大值的神经元索引作为预测值

1.  使用`inverted_mapping`将索引值转换为真实标签

当你执行代码示例时，你会得到类似这样的输出：

```py
Iris-versicolor
```

# 改进模型

你会很快意识到，构建和训练神经网络需要不止一次尝试。通常，模型的第一版效果并不会如你所愿。需要相当多的实验才能设计出一个优秀的模型。

一个好的神经网络始于一个优秀的数据集。在几乎所有情况下，使用合适的数据集会带来更好的性能。许多数据科学家会告诉你，他们大约花费 80%的时间在处理一个好的数据集上。就像所有计算机软件一样，如果你输入垃圾，输出的也会是垃圾。

即使有了良好的数据集，你仍然需要花费相当多的时间来构建和训练不同的模型，才能获得你想要的性能。那么，让我们看看在第一次构建模型后，你可以做些什么来改进模型。

在第一次训练模型后，你有几个选择来改进你的模型。

查看你训练集和验证集的准确率。训练集上的准确率较低吗？尝试训练模型更多的轮次。通常，这会有助于提升模型的表现。

即使你训练模型更长时间，训练准确率仍然没有提高？那么你的模型可能无法学习数据集中的复杂关系。尝试改变模型结构，然后再次训练模型，看看是否能提高准确率。

例如，尝试更改激活函数或隐藏层中神经元的数量。这通常有助于模型学习数据集中更复杂的关系。

另外，你还可以检查模型中层的数量。添加一层可能对模型从输入数据中学习规则的能力产生很大的影响。

最后，当这些方法无效时，看看你模型中层的初始化。在某些情况下，选择不同的初始化函数有助于模型在初始学习阶段。

实验过程的关键是一次只改变一个参数，并跟踪你的实验结果。使用像 Git 这样的版本控制工具可以帮助你跟踪不同版本的训练代码。

# 总结

在本章中，我们构建了第一个神经网络，并训练它来识别鸢尾花。虽然这个示例非常基础，但它展示了如何使用 CNTK 构建和训练神经网络。

我们已经看到如何利用 CNTK 中的层库来快速定义神经网络的结构。在本章中，我们讨论了一些基本的构建块，如`Dense`层和`Sequential`层，用于将多个其他层连接在一起。在接下来的章节中，我们将学习其他层函数，以构建其他类型的神经网络，如卷积神经网络。

在本章中，我们还讨论了如何使用`learner`和`trainer`构建一个基本算法来训练我们的神经网络。我们使用了`train_minibatch`方法，并配合基本的循环来构建自己的训练过程。这是一种非常简单而强大的训练模型的方式。在下一章中，我们将更详细地讨论其他训练方法以及`train_minibatch`方法。

在训练完模型后，我们利用了 CNTK 的函数特性，使用训练好的模型进行预测。模型作为函数的这一特性非常强大，使得在应用程序中使用训练好的模型变得直观且简便。

最后，我们已经了解了如何使用`test_minibatch`方法来衡量模型性能，以及如何使用性能指标检查模型是否发生了过拟合。之后我们讨论了如何使用指标来判断如何改进模型。

在下一章，我们将探讨不同的方式来访问和输入数据到 CNTK 模型中。我们还将深入探讨 CNTK 中每种数据访问方法，并了解在不同情况下哪些方法最为合适。
