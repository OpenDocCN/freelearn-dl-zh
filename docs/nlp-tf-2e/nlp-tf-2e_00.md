# 前言

TensorFlow 是开发**机器学习**（**ML**）解决方案的核心。它是一个生态系统，可以支持 ML 项目生命周期中的各个阶段，从早期的原型设计到模型的生产化。TensorFlow 提供了各种可重用的构建模块，允许您构建不仅仅是最简单的，甚至是最复杂的深度神经网络。

# 本书适用对象

本书面向 TensorFlow 初学者到中级用户。读者可能来自学术界，从事机器学习的前沿研究，或者来自工业界，将机器学习应用于工作中。如果您已经对 TensorFlow（或类似的框架如 Pytorch）有一些基础了解，您将从本书中获得最大的收益。这将帮助您更快地掌握本书讨论的概念和使用案例。

# 本书内容概述

*第一章*，*自然语言处理简介*，解释了自然语言处理是什么，以及它可能涉及的任务类型。然后我们讨论了如何使用传统方法解决 NLP 任务。这为讨论如何在 NLP 中使用深度学习及其优势铺平了道路。最后，我们讨论了本书中使用的技术工具的安装和使用。

*第二章*，*理解 TensorFlow 2*，为您提供了编写程序和在 TensorFlow 2 中运行程序的完整指南。本章将首先深入解释 TensorFlow 如何执行程序。这将帮助您理解 TensorFlow 的执行流程，并熟悉 TensorFlow 的术语。接下来，我们将讨论 TensorFlow 中的各种构建模块和可用的有用操作。最后，我们将讨论如何利用所有这些 TensorFlow 知识来实现一个简单的神经网络，用于分类手写数字图像。

*第三章*，*Word2vec - 学习词嵌入*，介绍了 Word2vec——一种学习反映单词语义的数值表示方法。但在直接进入 Word2vec 技术之前，我们首先讨论了一些用于表示单词的经典方法，如独热编码表示法，以及**词频-逆文档频率**（**TF-IDF**）频率方法。接下来，我们将介绍一种现代的学习词向量的工具，即 Word2vec，它利用神经网络学习单词表示。我们将讨论两种流行的 Word2vec 变体：skip-gram 和**连续袋词模型**（**CBOW**）。最后，我们将使用降维技术可视化所学习到的单词表示，将向量映射到更易于解释的二维平面。

*第四章*，*高级词向量算法*，介绍了一种较新的词嵌入学习技术——GloVe，它结合了文本数据中的全局和局部统计信息来寻找词向量。接下来，我们将学习一种现代的、更复杂的技术，即基于词语上下文生成动态词表示的技术，称为 ELMo。

*第五章*，*卷积神经网络的句子分类*，介绍了**卷积神经网络**（**CNNs**）。CNNs 是一类强大的深度学习模型，它能够利用输入数据的空间结构进行学习。换句话说，CNN 可以处理二维形式的图像，而多层感知机则需要将图像展开成一维向量。我们将首先详细讨论 CNN 中涉及的各种操作，例如卷积操作和池化操作。接下来，我们将通过一个例子，学习如何使用 CNN 对衣物图像进行分类。然后，我们将进入 CNN 在自然语言处理（NLP）中的应用。更准确地说，我们将研究如何将 CNN 应用于句子分类任务，其中任务是将句子分类为与人、地点、物体等相关。

*第六章*，*递归神经网络*，重点介绍了**递归神经网络**（**RNNs**）及其在语言生成中的应用。RNN 与前馈神经网络（例如 CNNs）不同，因为 RNN 具有记忆。该记忆以持续更新的系统状态形式存储。我们将从前馈神经网络的表示开始，并修改该表示，使其能够从数据序列中学习，而非单个数据点。这个过程将把前馈网络转化为 RNN。接着，我们将详细描述 RNN 内部用于计算的精确方程。然后，我们将讨论用于更新 RNN 权重的 RNN 优化过程。随后，我们将遍历不同类型的 RNN，例如一对一 RNN 和一对多 RNN。接下来，我们将讨论 RNN 的一个流行应用，即识别文本中的命名实体（例如人名、组织名等）。在这里，我们将使用一个基础的 RNN 模型进行学习。然后，我们将通过在不同尺度（例如标记嵌入和字符嵌入）中引入嵌入来进一步增强我们的模型。标记嵌入通过嵌入层生成，而字符嵌入则通过 CNN 生成。最后，我们将分析新模型在命名实体识别任务中的表现。

*第七章*，*理解长短期记忆网络*，讨论了**长短期记忆网络**（**LSTM**），首先通过直观的解释让你理解这些模型是如何工作的，然后逐步深入技术细节，帮助你自己实现它们。标准的 RNN 模型存在一个重要的局限性——无法保持长期记忆。然而，已经提出了先进的 RNN 模型（例如 LSTM 和**门控循环单元**（**GRU**）），它们能够记住多个时间步的序列。我们还将探讨 LSTM 是如何缓解长期记忆保持问题的（这被称为梯度消失问题）。接着，我们将讨论几种可以进一步改进 LSTM 模型的修改方法，例如一次性预测多个时间步并且同时读取前后序列。最后，我们将讨论 LSTM 模型的几种变体，例如 GRU 和带窥视连接的 LSTM。

*第八章*，*LSTM 的应用——生成文本*，解释了如何实现*第七章*中讨论的 LSTM、GRU 和带窥视连接的 LSTM，*理解长短期记忆网络*。此外，我们还将从定性和定量两个方面比较这些扩展的性能。我们还将讨论如何实现*第七章*中考察的一些扩展，例如预测多个时间步（即束搜索），以及使用词向量作为输入，而不是使用独热编码表示。

*第九章*，*序列到序列学习—神经机器翻译*，讨论了机器翻译，这一领域由于自动化翻译的必要性以及任务本身的固有难度而引起了大量关注。我们从简短的历史回顾开始，解释了机器翻译在早期是如何实现的。这一讨论以对**神经机器翻译**（**NMT**）系统的介绍结束。我们将看到当前的 NMT 系统与老旧系统（如统计机器翻译系统）相比表现如何，这将激励我们进一步学习 NMT 系统。接下来，我们将讨论支撑 NMT 系统设计的基本概念，并继续讲解技术细节。然后，我们将讨论用于评估系统的评估指标。接下来，我们将研究如何从零开始实现一个英德翻译器。然后，我们将学习如何改进 NMT 系统。我们将详细介绍其中的一种扩展，即注意力机制。注意力机制已成为序列到序列学习问题中的关键因素。最后，我们将比较应用注意力机制后性能的提升，并分析性能提升的原因。本章的最后部分将讲解如何将 NMT 系统的相同概念扩展应用于聊天机器人。聊天机器人是能够与人类进行交流的系统，广泛用于满足各种客户需求。

*第十章*，*Transformer*，讨论了 Transformer，这一在自然语言处理领域的最新突破，已超越了许多先前的先进模型。在本章中，我们将使用 Hugging Face 的 Transformers 库，轻松地利用预训练模型进行下游任务。在本章中，我们将深入了解 Transformer 架构。接下来，我们将介绍一种流行的 Transformer 模型，称为 BERT，使用它来解决问题解答任务。我们将讨论 BERT 中一些特定的组件，以便有效地将其应用于实践。然后，我们将在一个流行的问答数据集 SQUAD 上训练模型。最后，我们将对模型进行评估，并使用训练好的模型为未见过的问题生成答案。

*第十一章*，*使用 Transformers 进行图像标题生成*，探讨了另一种激动人心的应用，使用 Transformers 生成图像的标题（即描述）。这个应用有趣之处在于，它展示了如何结合两种不同类型的模型，以及如何使用多模态数据（例如图像和文本）进行学习。在这里，我们将使用一个预训练的 Vision Transformer 模型，该模型为给定的图像生成丰富的隐藏表示。这个表示与标题令牌一起输入到基于文本的 Transformer 模型中。基于文本的 Transformer 根据之前的标题令牌预测下一个标题令牌。一旦模型训练完成，我们将定性和定量地评估模型生成的标题。我们还将讨论一些用于衡量序列质量（如图像标题）的常用指标。

*附录 A：* *数学基础与高级 TensorFlow*，介绍了各种数学数据结构（例如矩阵）和运算（例如矩阵求逆）。我们还将讨论概率中的一些重要概念。最后，我们将引导你学习如何使用 TensorBoard 可视化词嵌入。TensorBoard 是一个随 TensorFlow 提供的实用可视化工具，可以用来可视化和监控 TensorFlow 客户端中的各种变量。

# 如何最大化本书的学习效果

为了最大化本书的学习效果，你需要对 TensorFlow 或类似框架（如 PyTorch）有基本了解。通过网上免费提供的基础 TensorFlow 教程获得的熟悉程度应足以开始阅读本书。

本书中，基本的数学知识，包括对 n 维张量、矩阵乘法等的理解，将在学习过程中极为宝贵。最后，你需要对学习前沿的机器学习技术充满热情，这些技术正为现代自然语言处理解决方案奠定基础。

下载示例代码文件

本书的代码包托管在 GitHub 上，网址是 https://github.com/thushv89/packt_nlp_tensorflow_2。我们还提供了其他来自我们丰富图书和视频目录的代码包，地址是 https://github.com/PacktPublishing/。赶快去看看吧！

## 下载彩色图像

我们还提供了一个包含本书中使用的截图/图表的彩色图像的 PDF 文件。你可以在此下载：https://static.packt-cdn.com/downloads/9781838641351_ColorImages.pdf。

## 使用的约定

本书中使用了多种文本约定。

`CodeInText`：表示文本中的代码词、数据库表名、文件夹名称、文件名、文件扩展名、路径名、虚拟 URL、用户输入和 Twitter 用户名。例如：“运行 `pip install` 命令后，你应该能在 Conda 环境中使用 Jupyter Notebook。”

一段代码按以下方式设置：

```py
def layer(x, W, b):
    # Building the graph
    h = tf.nn.sigmoid(tf.matmul(x,W) + b) # Operation to perform
    return h 
```

任何命令行输入或输出均按以下方式书写：

```py
<tf.Variable 'ref:0' shape=(3, 2) dtype=float32, numpy=
array([[-1., -9.],
       [ 3., 10.],
       [ 5., 11.]], dtype=float32)> 
```

**粗体**：表示一个新术语或重要的词汇。你在屏幕上看到的词汇（例如在菜单或对话框中）也会像这样出现在文本中，例如：“在 TensorFlow 中自动构建计算图的功能被称为**AutoGraph**。”

警告或重要说明将以这种形式出现。

小贴士和技巧将以这种形式出现。

# 联系我们

我们欢迎读者的反馈。

**一般反馈**：发送电子邮件至`feedback@packtpub.com`，并在邮件主题中提及书名。如果你对本书的任何方面有疑问，请通过`questions@packtpub.com`与我们联系。

**勘误**：虽然我们已经尽力确保内容的准确性，但错误仍然可能发生。如果你在本书中发现错误，请向我们报告。请访问 http://www.packtpub.com/submit-errata，选择你的书籍，点击“勘误提交表格”链接并填写详细信息。

**盗版**：如果你在互联网上发现任何我们作品的非法复制品，请提供该位置地址或网站名称。请通过`copyright@packtpub.com`与我们联系，并附上该材料的链接。

**如果你有兴趣成为作者**：如果你在某个领域具有专长，并且有兴趣编写或参与撰写一本书，请访问 http://authors.packtpub.com。

# 分享你的想法

一旦你读完了*《使用 TensorFlow 进行自然语言处理（第二版）》*，我们非常希望听到你的想法！请[点击这里直接进入该书的亚马逊评论页面](https://packt.link/r/1838641351)并分享你的反馈。

你的评论对我们以及技术社区都非常重要，能帮助我们确保提供优质的内容。
