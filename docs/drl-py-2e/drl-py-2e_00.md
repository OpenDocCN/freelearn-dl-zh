# 前言

近年来，强化学习算法的质量和数量显著提升，本书第二版《Python 强化学习实战》已经被重塑为一本充满实例的指南，帮助读者学习最先进的**强化学习**（**RL**）和深度强化学习算法，使用 TensorFlow 2 和 OpenAI Gym 工具包。

除了探讨强化学习（RL）的基础知识和基础概念，如贝尔曼方程、马尔可夫决策过程和动态规划外，本第二版深入探讨了基于价值、基于策略和演员-评论员（actor-critic）强化学习方法的全貌。书中详细介绍了如 DQN、TRPO、PPO、ACKTR、DDPG、TD3 和 SAC 等最先进的算法，揭示了背后的数学原理，并通过简单的代码示例演示了实现过程。

本书新增了几章，专门介绍了新的强化学习技术，包括分布式强化学习、模仿学习、逆向强化学习和元强化学习。你将学习如何利用 Stable Baselines（一种 OpenAI 基准库的改进版）来轻松实现流行的强化学习算法。本书最后将概述一些有前景的研究方法，如元学习和想象增强代理（imagination augmented agents）。

# 本书适合谁阅读

如果你是一个对神经网络经验较少或没有经验的机器学习开发者，并且有兴趣了解人工智能、从零开始学习强化学习，那么本书适合你。需要具备一些线性代数、微积分和 Python 的基本知识，若有 TensorFlow 的经验会更有帮助。

# 本书内容

*第一章*，*强化学习基础*，帮助你建立强化学习概念的坚实基础。我们将学习强化学习的关键要素、马尔可夫决策过程，以及几个重要的基础概念，如动作空间、策略、回合、价值函数和 Q 函数。章节末我们还将了解一些强化学习的有趣应用，并探讨在强化学习中常用的关键术语和术语。

*第二章*，*Gym 工具包指南*，提供了 OpenAI Gym 工具包的完整指南。通过实际实现，我们将详细了解 Gym 提供的几种有趣的环境。我们将从这一章开始动手实践强化学习，通过 Gym 实现一些基本的强化学习概念。

*第三章*，*贝尔曼方程与动态规划*，将帮助我们通过大量数学推导详细理解贝尔曼方程。接下来，我们将学习两种有趣的经典强化学习算法——价值迭代和策略迭代方法，利用这些方法可以找到最优策略。我们还将看到如何通过实现价值迭代和策略迭代方法来解决 Frozen Lake 问题。

*第四章*，*蒙特卡罗方法*，解释了无模型方法——蒙特卡罗方法。我们将了解什么是预测和控制任务，然后详细研究蒙特卡罗预测方法和蒙特卡罗控制方法。接下来，我们将使用 Gym 工具包实现蒙特卡罗方法来解决二十一点游戏。

*第五章*，*理解时间差分学习*，讲解了一种非常流行且广泛使用的无模型方法——**时间差分**（**TD**）学习。首先，我们将详细了解 TD 预测方法如何工作，然后我们将深入研究基于策略的 TD 控制方法——SARSA，以及基于行为的 TD 控制方法——Q 学习。我们还将实现 TD 控制方法，使用 Gym 来解决冰湖问题。

*第六章*，*案例研究——多臂老虎机问题*，讲解了强化学习中的经典问题之一——**多臂老虎机**（**MAB**）问题。我们将从理解 MAB 问题开始，然后学习几种探索策略，如 ε-贪婪、软最大探索、上置信界和汤普森采样方法，以详细解决 MAB 问题。

*第七章*，*深度学习基础*，帮助我们建立深度学习的坚实基础。我们将从理解人工神经网络的工作原理开始。接着，我们将学习几种有趣的深度学习算法，如递归神经网络、LSTM 网络、卷积神经网络和生成对抗网络。

*第八章*，*TensorFlow 入门*，讲解了一个非常流行的深度学习库——TensorFlow。我们将通过实现一个神经网络来识别手写数字，了解如何使用 TensorFlow。接下来，我们将学习如何使用 TensorFlow 执行几种数学操作。之后，我们将学习 TensorFlow 2.0，并了解它与以前版本的区别。

*第九章*，*深度 Q 网络及其变种*，帮助我们启动深度强化学习之旅。我们将了解一种非常流行的深度强化学习算法——**深度 Q 网络**（**DQN**）。我们将逐步理解 DQN 的工作原理，并进行详细的数学推导。我们还将实现一个 DQN 来玩 Atari 游戏。接下来，我们将探讨 DQN 的几个有趣变种，包括双重 DQN、对抗 DQN、带优先经验回放的 DQN 和 DRQN。

*第十章*，*策略梯度方法*，介绍了策略梯度方法。我们将理解策略梯度方法如何工作，并进行详细推导。接下来，我们将学习几种方差减少方法，如带有回报目标的策略梯度和带有基准的策略梯度。我们还将理解如何使用策略梯度训练代理来完成平衡杆任务。

*第十一章*，*演员-评论家方法——A2C 和 A3C*，介绍了几种有趣的演员-评论家方法，如优势演员-评论家和异步优势演员-评论家。我们将详细学习这些演员-评论家方法的工作原理，然后我们将实现它们来进行山地车爬坡任务，使用 OpenAI Gym。

*第十二章*，*学习 DDPG、TD3 和 SAC*，涵盖了最先进的深度强化学习算法，如深度确定性策略梯度、双延迟 DDPG 和软演员方法，并进行了逐步推导。我们还将学习如何实现 DDPG 算法来执行倒立摆摆动任务，使用 Gym。

*第十三章*，*TRPO、PPO 和 ACKTR 方法*，介绍了几种流行的策略梯度方法，如 TRPO 和 PPO。我们将一步步深入探讨 TRPO 和 PPO 背后的数学原理，并理解 TRPO 和 PPO 如何帮助代理找到最优策略。接下来，我们将学习如何实现 PPO 来执行倒立摆摆动任务。最后，我们将详细了解一种名为使用 Kronecker 分解信任区域的演员-评论家方法。

*第十四章*，*分布式强化学习*，介绍了分布式强化学习算法。我们将从理解什么是分布式强化学习开始。然后，我们将探索一些有趣的分布式强化学习算法，如分类 DQN、分位回归 DQN 和分布式分布式 DDPG。

*第十五章*，*模仿学习与反向强化学习*，解释了模仿学习和反向强化学习算法。首先，我们将详细了解如何进行监督模仿学习、DAgger 和从示范中学习深度 Q 学习。接下来，我们将学习最大熵反向强化学习。最后，在本章的结尾，我们将学习生成对抗模仿学习。

*第十六章*，*使用 Stable Baselines 的深度强化学习*，帮助我们了解如何使用一个名为 Stable Baselines 的库来实现深度强化学习算法。我们将通过实现几种有趣的深度强化学习算法，如 DQN、A2C、DDPG、TRPO 和 PPO，详细学习什么是 Stable Baselines 以及如何使用它。

*第十七章*，*强化学习前沿*，详细介绍了强化学习中的几个有趣领域，如元强化学习、层次强化学习和想象增强代理。

# 要充分利用本书

您需要以下软件来学习本书：

+   Anaconda

+   Python

+   任何网络浏览器

## 下载示例代码文件

您可以从您的账户下载本书的示例代码文件，访问[`www.packtpub.com`](http://www.packtpub.com)。如果您是在其他地方购买本书，可以访问[`www.packtpub.com/support`](http://www.packtpub.com/support)，注册后将文件直接通过电子邮件发送给您。

您可以按照以下步骤下载代码文件：

1.  登录或注册[`www.packtpub.com`](http://www.packtpub.com)。

1.  选择**支持**标签。

1.  点击**代码下载与勘误**。

1.  在**搜索**框中输入书名并按照屏幕上的指示操作。

文件下载完成后，请确保使用以下最新版本解压或提取文件夹：

+   WinRAR / 7-Zip for Windows

+   Zipeg / iZip / UnRarX for Mac

+   7-Zip / PeaZip for Linux

本书的代码包也托管在 GitHub 上，地址为：[`github.com/PacktPublishing/Deep-Reinforcement-Learning-with-Python`](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-with-Python)。我们还提供了来自我们丰富书籍和视频目录中的其他代码包，地址为：[`github.com/PacktPublishing/`](https://github.com/PacktPublishing/)。快去看看吧！

## 下载彩色图像

我们还提供了一份包含本书中使用的截图/图表的彩色图像的 PDF 文件。你可以在此下载：[`static.packt-cdn.com/downloads/9781839210686_ColorImages.pdf`](https://static.packt-cdn.com/downloads/9781839210686_ColorImages.pdf)。

## 使用的约定

本书中使用了多种文本约定。

`CodeInText`：表示文本中的代码单词、数据库表名、文件夹名称、文件名、文件扩展名、路径名、虚拟网址、用户输入和 Twitter 用户名。例如：“`epsilon_greedy`计算最佳策略。”

一段代码如下设置：

```py
def epsilon_greedy(epsilon):
    if np.random.uniform(0,1) < epsilon:
        return env.action_space.sample()
    else:
        return np.argmax(Q) 
```

当我们希望引起你对代码块中特定部分的注意时，相关的行或项目会被高亮显示：

```py
def epsilon_greedy(epsilon):
**if** **np.random.uniform(****0****,****1****) < epsilon:**
        return env.action_space.sample()
    else:
        return np.argmax(Q) 
```

任何命令行输入或输出如下所示：

```py
source activate universe 
```

**粗体**：表示新术语、重要单词，或者你在屏幕上看到的词汇，例如在菜单或对话框中，也会像这样出现在文本中。例如：“**马尔可夫奖励过程**（**MRP**）是带有奖励函数的马尔可夫链的扩展。”

警告或重要提示如下所示。

提示和技巧如下所示。

# 联系我们

我们始终欢迎读者的反馈。

**一般反馈**：请发送电子邮件至`feedback@packtpub.com`，并在邮件主题中注明书名。如果你对本书的任何方面有疑问，请通过`questions@packtpub.com`与我们联系。

**勘误**：尽管我们已尽力确保内容的准确性，但错误仍然可能发生。如果你在本书中发现错误，我们将非常感激你能报告给我们。请访问[`www.packtpub.com/submit-errata`](http://www.packtpub.com/submit-errata)，选择你的书籍，点击勘误提交表单链接，并输入相关信息。

**盗版**：如果你在互联网上遇到我们作品的任何非法复制形式，我们将非常感激你能提供位置地址或网站名称。请通过`copyright@packtpub.com`与我们联系，并附上材料链接。

**如果你有兴趣成为作者**：如果你在某个领域有专长并且有意撰写或贡献书籍内容，请访问[`authors.packtpub.com`](http://authors.packtpub.com)。

## 评论

请留下评论。当您阅读并使用本书后，为什么不在您购买书籍的网站上留下评论呢？潜在的读者可以查看并参考您的公正意见来做出购买决定，我们 Packt 也可以了解您对我们产品的看法，而我们的作者可以看到您对他们书籍的反馈。谢谢！

如需了解更多关于 Packt 的信息，请访问 [packtpub.com](http://packtpub.com)。
