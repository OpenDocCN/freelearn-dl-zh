

# 第三章：机器学习和深度学习深入分析

在 AI 应用的时代，我们所处的 AI 现阶段，我们必须理解机器学习（**ML**）和深度学习（**DL**）的优缺点，以便最好地决定何时使用这两种技术。你可能还遇到过一些与 AI/ML 工具相关的其他术语，如**应用 AI**或**深度技术**。正如我们在本书中多次提到的，未来大多数 AI 产品背后的核心技术将是 ML 或 DL。这是因为基于专家或规则的系统正在慢慢被 ML 所取代，或者根本没有发展。所以，让我们更深入地探讨这些技术，并了解它们之间的区别。

在本章中，我们将探讨机器学习（ML）和深度学习（DL）之间的关系，以及它们如何为构建者和用户带来各自的期望、解释和阐述。无论你是使用自 50 年代以来就存在的 ML 模型的产品，还是使用最近才投入使用的尖端模型，你都会想理解它们的影响。将 ML 或 DL 纳入你的产品将会产生不同的影响。大多数时候，当你看到一个产品上标有 AI 标签时，它是通过 ML 或 DL 构建的，因此我们希望确保你在本章结束时，能够清楚了解这些领域的区别，以及这些区别对你未来产品的实际意义。

在*第一章*中，我们讨论了自 50 年代以来我们如何与使用机器的理念进行斗争，但我们希望扩展一下机器学习（ML）和深度学习（DL）**人工神经网络**（**ANNs**）的历史，让你了解这些模型已经存在多久。在本章中，我们将涵盖以下主题，帮助你更熟悉与 ML 和 DL 相关的细微差别：

+   旧的——探索机器学习（ML）

+   新的——探索深度学习（DL）

+   新兴技术——辅助和相关技术

+   可解释性——优化伦理、警告和责任

+   准确性——优化成功

# 旧的——探索机器学习（ML）

机器学习模型试图创建某种现实的表示，以帮助我们做出某种数据驱动的决策。实质上，我们使用数学来表示现实世界中发生的一些现象。机器学习本质上运用数学和统计学来预测或分类某种未来状态。路径的分歧有两种方式。第一组涉及通过统计模型不断发展的模型，第二组则是试图模仿我们自身自然神经智能的模型。通俗地说，这两类模型分别被称为传统机器学习（ML）和深度学习（DL）模型。

你可以把我们在*第二章*“模型类型——从线性回归到神经网络”一节中讨论的所有模型都看作是机器学习（ML）模型，但我们并没有深入探讨人工神经网络（ANNs）。我们将在本章后面的“神经网络类型”一节中进一步讨论这些模型。在本节中，我们将着眼于传统的统计学机器学习模型，以便理解这些模型的历史意义和普遍性。回顾一下机器学习的流程，本质上是一个检索数据、通过数据处理、数据整理和特征工程准备数据、将数据输入模型并评估模型性能，然后根据需要对其进行调优的过程。

一些最可靠且普遍使用的机器学习模型已经存在了很长时间。**线性回归**模型自 19 世纪末期就开始使用，并通过两位英国数学家卡尔·皮尔逊和弗朗西斯·高尔顿的工作而得到普及。他们的贡献为今天最流行的机器学习算法之一奠定了基础，尽管不幸的是，两位数学家都是著名的优生学家。卡尔·皮尔逊还被认为是在 1901 年发明了**主成分分析**（**PCA**），这是一种无监督学习方法，用于减少数据集的维度。

一种流行的机器学习方法——**朴素贝叶斯分类器**，出现在 20 世纪 60 年代，但它基于 18 世纪英国统计学家托马斯·贝叶斯及其条件概率定理的研究。逻辑函数由比利时数学家皮埃尔·弗朗索瓦·维尔霍斯特在 19 世纪中叶引入，**逻辑回归**模型则是由英国统计学家大卫·考克斯在 1958 年推广的。

**支持向量机**（**SVM**）由苏联数学家弗拉基米尔·瓦普尼克和阿列克谢·切尔沃涅尼斯于 1963 年在俄罗斯科学院控制科学研究所提出。第一个决策树分析算法也在 1963 年由美国统计学家詹姆斯·N·摩根和约翰·A·桑奎斯特发明，他们来自密歇根大学，并应用于他们的**自动交互检测**（**AID**）程序，但即便如此，这一方法也源于*波尔菲里树*，这是一种由古希腊哲学家波尔菲里（Porphyry）在公元前三世纪创制的基于分类树的图表。随机森林是由多个决策树组成的集成方法，由加利福尼亚大学的美国统计学家利奥·布雷曼于 2001 年发明。

**KNN 算法**是最简单的监督学习模型之一，用于分类和回归。它源于 1951 年由统计学家 Evelyn Fix 和 Joseph Lawson Hodges Jr.代表美国军方与伯克利大学合作完成的技术分析报告。K-means 聚类是无监督机器学习聚类的一种方法，最早由 UCLA 的数学家 James MacQueen 于 1967 年提出。

正如你所见，今天在机器学习（ML）模型中最常用的许多算法，其根源可以追溯到我们现代历史的较早时期。它们的简洁性和优雅性使得它们今天仍然具有重要的相关性。本节中我们所讨论的大多数模型，除了深度学习的 ANNs 之外，都是在*第二章*中提到过的。在接下来的章节中，我们将重点讨论深度学习。

# 新的——探索深度学习

我们在本书中将机器学习（ML）和深度学习（DL）概念上区分开来的部分目的是帮助读者在脑海中形成联想。对于大多数领域的技术人员来说，当你看到“ML”与“DL”作为产品的描述时，会联想到一些特定的模型和算法。在这里简单提醒一下，深度学习（DL）是机器学习（ML）的一个子集。如果你对这两个术语感到困惑，只需记住，深度学习是机器学习的一种形式，它已经发展并演变成了自己的生态系统。我们的目标是尽可能地揭开这个生态系统的神秘面纱，让你作为产品经理能够自信地理解深度学习产品背后的动态。

深度学习（DL）的基础思想围绕着我们自己的生物神经网络展开，DL 使用的是通常被称为人工神经网络（ANNs）的概念来解决复杂问题。正如我们将在下一节中看到的，DL 所形成的生态系统在很大程度上受到了我们大脑的启发，大脑就是“原始”神经网络的来源。这一灵感不仅来源于人脑的功能，尤其是通过示例学习的思想，还来源于其结构。

由于本书并非专门为深度学习工程师编写的技术性书籍，我们将避免深入探讨与深度学习相关的术语和数学内容。然而，基本了解人工神经网络（ANN）会有所帮助。在这一节中，请牢记神经网络是由人工神经元或节点组成的，这些节点按层堆叠在一起。通常，有三种类型的层：

+   输入层

+   隐藏层

+   输出层

虽然我们将讨论各种类型的人工神经网络（ANNs），但有一些基本概念是这些深度学习算法工作的基础。可以把它们看作是层和节点的组合。基本的思路是，数据通过每一层的每一个节点传递，而这些节点和层之间会传递权重和偏差。人工神经网络通过其训练数据来识别模式，从而帮助它们解决当前的问题。一个至少有三层的人工神经网络（即输入层、输出层和至少一个隐藏层）就被称为“深度”，因此可以归类为深度学习算法。至此，层的部分已经解决。

那么，节点呢？如果你记得，我们在前几章中讲过的最简单的模型之一就是线性回归模型。你可以把每个节点看作是它自己的迷你线性回归模型，因为这正是每个人工神经网络（ANN）节点内发生的计算。每个节点都有自己的数据、一个数据的权重和一个它用来得出输出的偏差或参数。所有这些节点在大规模进行这些计算时的总和，让你能够理解人工神经网络是如何工作的。如果你能想象成千上万层、每层内有许多节点的庞大规模，你就能开始理解为什么人工神经网络得出某些结论时，可能很难理解其原因。

深度学习通常被称为“黑箱”技术，而这也正好触及了为什么它被这样称呼的核心。根据我们的数学技能，我们人类可以解释为什么在一个简单的线性回归模型中会存在某个错误率或损失函数。我们能够概念化模型如何在拟合曲线时出错。当面对真实世界的数据时，这些数据并没有呈现出完美的曲线，这时我们也能理解其挑战。但如果我们扩大规模，尝试想象可能有数十亿个节点，每个节点代表一个线性回归模型，那么我们的头脑就开始感到痛苦了。

虽然深度学习（DL）常被讨论为前沿技术突破，但正如我们在前一节中看到的，这个旅程早在很久以前就已开始。

## 看不见的影响

理解影响机器学习（ML）和深度学习（DL）的潜在关系以及与之相关的历史非常重要。这是故事叙述的基础部分，但它也有助于更好地理解这项技术如何与我们周围的世界相联系。对于许多人来说，理解人工智能/机器学习概念可能会让人困惑，除非你来自技术或计算机科学背景，否则这些话题本身可能会显得令人生畏。即使是最好的情况，许多人也只能获得这项技术的基本理解，以及它是如何发展的。

我们希望通过让更深入的理解变得更加易于接触，来赋能那些有兴趣探索这项将在未来塑造许多产品和内部系统的基础技术的人。目前，已有偏见存在。大多数深入理解机器学习和深度学习的人，都是来自计算机科学背景的，无论是通过正规教育，还是通过训练营和其他技术培训项目。这意味着，在很大程度上，那些在这一领域进行研究和创业的人，传统上主要是白人且以男性为主。

除了人口统计因素外，从学术角度来看，对这些技术的投资水平也在上升。我们来看一些数据。斯坦福大学的人工智能指数显示，世界顶尖大学的研究生阶段人工智能投资增长了 41.7%。这个数字在本科阶段跳升至 102.9%。过去十年中，额外的 48%的人工智能博士获得者已离开学术界，转向私营部门追逐丰厚的薪水。10 年前，只有 14.2%的计算机科学博士与人工智能相关。现在，这一数字已超过 23%。特别是美国，正在留住其培养和吸引的人才。来美国攻读人工智能博士学位的外国学生，留下的比例为 81.8%。

这幅画面描绘的是一个世界，它迫切需要人工智能/机器学习领域的人才和技能。对人工智能/机器学习技能集的高需求，特别是一个人口多样化的人工智能技能集，正在使得那些在这一领域拥有硬技能的人很难留在学术界，而私营部门对拥有这些技能的人给予丰厚的回报。在初创公司中，许多风险投资公司和投资者能够自信地巩固他们的投资，因为他们知道某公司有一位拥有人工智能博士学位的员工，不论该公司的产品是否需要这种高深的专业知识。对具备这些抢手技能的人力资源的溢价，可能在短期内不会消失。

我们梦想着一个来自不同背景和能力的人们进入人工智能领域的世界，因为多样性迫切需要，而且我们面前的机会太大，以至于目前的门槛阻碍无法继续存在下去。人工智能的建设者了解其基础技术以及应用这些技术的强大力量非常重要。对于那些利用这一技术能力的商业利益相关者来说，了解眼前的选项和能力同样重要。归根结底，没有什么是如此复杂，以至于无法轻松解释。

## 深度学习的简史

1943 年，沃伦·S·麦卡洛克和沃尔特·皮茨发表了一篇论文《神经活动中固有思想的逻辑演算》，通过创建一个基于我们大脑内神经网络的计算机模型，利用一系列算法结合“阈值”来模拟我们如何在生物神经网络中传递信息，建立了数学和神经学之间的联系。然后，1958 年，弗兰克·罗森布拉特发表了一篇论文，广泛被认为是神经网络的祖先，名为《感知器：一个感知和识别的自动机》。就所有意图和目的而言，这是第一个、最简单、最古老的人工神经网络。

在 1960 年代，向反向传播的进展，或者说模型在训练数据集的过程中从过去的错误中学习的想法，朝着最终构成神经网络的方向迈出了重要步伐。当时发展的最重要部分是将启发式数学模型的思想与大脑基于神经元网络和反向传播的工作方式相结合，因为这为人工神经网络（ANN）奠定了基础，后者通过过去的迭代进行学习。

在这里需要注意的是，许多人工神经网络是以“前馈”的方式工作的，即它们依次经过输入层、隐藏层和输出层，且只能单向传播，从输入到输出。反向传播的思想基本上使得人工神经网络能够双向学习，从而能够在每个节点最小化误差，最终提高性能。

直到 1986 年，David Rumelhart、Geoffrey Hinton 和 Ronald Williams 发表了著名论文《*通过反向传播误差进行学习表示*》，人们才完全开始理解反向传播在深度学习成功中的作用。你可以通过反向传播跨越时间，这让神经网络能够分配适当的权重，并且训练具有隐藏层的神经网络，这在当时是革命性的。

每次发展后，机器学习和神经网络的潜力都引起了很多兴奋，但在 60 年代中期到 80 年代之间，有一个重大问题：缺乏数据和资金。如果你听过“人工智能寒冬”这个术语，这正是它所指的。尽管在建模方面取得了进展，但没有足够的数据来为正在开发的模型提供支持，也缺乏研究团队愿意收集这些数据的动力，这导致了模型的应用面临巨大挑战。

然后，在 1997 年，Sepp Hochreiter 和 Jürgen Schmidhuber 发表了他们开创性的工作《*长短时记忆网络*》，该论文有效地使得深度学习能够“解决以往递归网络算法无法解决的复杂人工长时间滞后任务”。这一发展的重要性在于它让序列的概念在深度学习问题中依然具有相关性。由于神经网络涉及隐藏层，时间的概念难以保持相关，这使得许多问题难以解决。例如，传统的递归神经网络可能无法像**长短时记忆网络（LSTM）**那样完成句子的自动补全，因为它无法理解完成句子所涉及的时间序列。

今天，大多数深度学习（DL）模型需要大量的监督数据集，这意味着驱动 DL 的神经网络需要大量的示例来理解某物是否是，比如说，一只狗或一匹马。然而，如果你稍微思考一下，这与我们大脑的工作方式并没有那么紧密的关系。一个刚刚开始学习世界的小孩可能需要一两次提醒关于狗和马的区别，但你大概不会反复提醒他们这一区别成千上万次。

从这个角度来看，深度学习正在朝着需要越来越少的示例来学习的方向发展。如果你记得之前的章节，我们介绍了监督学习和无监督学习技术，这在深度学习中变得尤为重要。没错，现如今我们能够收集大量数据供深度学习模型学习，但这些模型本身正在进化，在不需要大量数据的情况下提升，最终目标是无监督深度学习，它能够通过少量数据进行训练。

到目前为止，我们已经涵盖了一些塑造机器学习（ML）和深度学习（DL）领域的历史和影响。虽然我们并没有过多深入一些技术概念，但这为我们理解机器学习和深度学习是如何随着时间的发展以及为什么它们崛起为主流提供了良好的基础。在接下来的部分，我们将更为实际，深入探讨在深度学习中最常用的具体算法和神经网络。

## 神经网络的类型

现在，我们想将你的注意力转向今天在深度学习中最受欢迎的一些神经网络。基于上一部分的内容，其中一些可能听起来很熟悉，但熟悉这些概念将对你特别有帮助，尤其是如果你计划作为深度学习产品的产品经理工作。即便你现在不在这一岗位，也值得了解一下这些内容，以防你的职业未来转向深度学习产品。

以下是一些在深度学习中最常用的人工神经网络（ANN）列表：

+   **多层** **感知器**（**MLPs**）

+   **径向基函数** **网络**（**RBFNs**）

+   **卷积神经** **网络**（**CNNs**）

+   **递归神经** **网络**（**RNNs**）

+   **长短期记忆** **网络**（**LSTMs**）

+   **生成对抗** **网络**（**GANs**）

+   **自组织** **映射**（**SOMs**）

+   **深度置信** **网络**（**DBNs**）

在接下来的部分，我们将讨论这些不同的神经网络，帮助你了解它们最适合什么样的任务。就像在上一章中讨论机器学习算法一样，我们将描述每种人工神经网络最常见的一些应用场景，以便我们至少大致了解每种 ANN 的核心竞争力，这样在未来你如果想创建自己的深度学习产品时，可以将这些概念纳入考虑。如果你的目标是专门支持或构建自己的深度学习产品，这将是每种人工神经网络的一个很好的概览总结。

### MLPs

在 1986 年，大卫·鲁梅哈特、杰弗里·辛顿和罗纳德·威廉姆斯的论文《通过反向传播错误来学习表示》发布后，MLPs 得到了广泛推广，因为在那篇论文中，他们使用反向传播来训练 MLP。与 RNNs 不同，MLPs 是另一种前馈神经网络，使用反向传播来优化权重。因此，你可以将 MLPs 视为一些最基础的人工神经网络形式，因为它们是最早出现的，并且今天仍然常用于处理一些更新的人工神经网络所需的高计算能力。它们的易用性和可靠性至今仍然很有价值，这也是我们希望从 MLPs 开始列出这些算法的原因，它为我们构建其他深度学习算法提供了一个良好的基础。

它们的学习方式是，算法将数据从输入层通过中间层传递到输出层。然后，根据输出层的结果，它会计算误差，评估预测值的偏差程度。这时，反向传播起作用了，它会评估预测有多么错误，从而反向传播误差率。接着，它会通过调整网络中的权重来优化自己，从而有效地更新自身。

这个想法是，你会将这些步骤多次通过模型，直到对其性能满意为止。还记得在*第一章*中提到的监督学习与无监督学习的区别吗？因为 MLPs 通过反向传播来最小化错误率，调整权重，所以 MLPs 是一种监督式深度学习算法，因为它们知道根据我们的标签数据，自己与正确答案的差距到底有多远。这些算法也常常与其他人工神经网络一起使用，作为最终的优化阶段。

### RBFNs

RBFNs（径向基函数网络）出现在 1988 年，源自 D.S. Broomhead 和 David Lowe 的论文《*多变量函数插值与自适应网络*》。RBFNs 与本章将讨论的其他大多数 ANN（人工神经网络）不同，它们只有三层。大多数 ANN，包括我们在前一节讨论的 MLP（多层感知器），通常具有输入层和输出层，中间有几个隐藏层，而 RBFNs 只有一层隐藏层。另一个关键区别是，RBFNs 的输入层并不是计算层，而仅仅是将数据传递到隐藏层，因此这种 ANN 非常快速。这些深度学习算法是前馈模型，因此它们实际上只经过两层计算：隐藏层和输出层。

将这些网络类比为我们在上一章讨论的 KNN（K 近邻）算法可能会有帮助，KNN 算法旨在根据数据点周围的其他数据点来预测它们的值。之所以这样类比，是因为 RBFNs 通过距离、半径或欧几里得距离来逼近值，并且它们会将数据聚集或分组为圆形或球形，以更好地理解复杂的多变量数据集，这类似于*第一章*中的 K-means 聚类算法的工作方式。这是一个非常灵活的算法，可以用于分类和回归问题，并且可以在监督和无监督的情况下使用。

### SOMs

SOMs（自组织映射）由 Tuevo Hohonen 在 1980 年代提出，是另一种无监督竞争学习的 ANN 示例，其中算法将多变量数据集压缩成一个二维“地图”。每个节点将与其他节点竞争，以决定是否应该激活它，因此这本质上是一场大规模的竞争，通过这种方式它实现了自组织。不过，从结构上来看，SOMs 与大多数 ANN 非常不同。除了输入层之外，实际上只有一层或一个节点，称为 Kohonen 层。节点本身也不像传统的 ANN 那样连接。

SOM 的训练过程类似于我们大脑自我组织和映射输入的能力。当我们感知到某些输入时，大脑会将这些输入组织到特定区域，这些区域适合我们所看到、听到、感觉到、闻到或尝到的东西。SOM 也会将数据点聚类到特定的分组中。其过程通过学习/训练进行，算法将数据通过输入层和权重发送，随机选择输入数据点进行测试，直到根据节点与数据点之间的距离选择一个节点，然后更新该节点的权重。这个过程会反复进行，直到训练集完成，最优节点被选中。

SOMs 也属于同类聚类算法，例如 K-means 或我们在前面部分提到的 RBFN，因为它们在寻找数据集中未标记或未发现的关系和分组时非常有用。

### CNNs

CNNs，有时被称为 ConvNets，具有多个层，主要用于监督学习应用场景，在这些场景中，它们检测物体、处理图像，并检测医学和卫星图像中的异常。这个人工神经网络（ANN）的工作方式是通过前馈神经网络，因此它从输入层开始，经过隐藏层，最终到达输出层来对图像进行分类。这种类型的 ANN 被称为分类型 ANN，因此它的最终目标是将图像归类到不同的类别中。然后，一旦图像被分类，它会根据共享的相似性对图像进行分组，从而最终执行物体识别，这种技术用于检测人脸、动物、植物或街头的标志。CNNs 可以用于面部识别、物体识别、自动驾驶汽车或通常所说的 AI 计算机视觉应用。

CNNs 中的四个重要层如下：

+   卷积层

+   **修正线性单元**（**ReLU**）层

+   池化层

+   全连接层

卷积层将图像转换为一个由 0 和 1 组成的像素值矩阵，然后将该矩阵进一步简化为一个从第一个矩阵衍生出的更小矩阵。ReLU 层有效地减少了你传递给 CNN 的图像的维度。即使是彩色图像，在最初分配为 0 和 1 时也会被转换为灰度图像。因此，在 ReLU 阶段，CNN 实际上会去除图像中的黑色像素，以便进一步缩小图像，使其对模型处理来说计算更简便。还有一个层通过另一种方式减少图像的维度：池化层。

当 ReLU 层减少图像本身的梯度时，池化层减少图像的特征，因此，如果我们将一张猫的图像传递给 CNN，池化层将识别出耳朵、眼睛、鼻子和胡须等特征。你可以将卷积、ReLU 和池化层看作是对输入到模型的每个图像的部分进行操作，并同时将这些步骤的输出作为输入传递给全连接层，后者实际上将图像传递通过神经网络进行分类。实质上，卷积、ReLU 和池化层的作用是为图像通过神经网络准备，从而得出结论。

### RNNs

有一些操作是前馈神经网络无法很好完成的，包括处理基于时间的序列数据、需要对多个输入进行上下文处理（不仅仅是当前输入）的操作，以及需要从之前的输入中进行记忆的操作。正因为如此，RNN 的主要优势在于其内部记忆，使其能够执行并记住对会话型 AI（如苹果的 Siri）要求的强大操作。RNN 在处理序列数据方面表现优秀，并且非常注重上下文，以便在处理时间序列数据、DNA 和基因组数据、语音识别以及语音转文本等任务时发挥出色。与之前处理前馈功能的 CNN 示例相比，RNN 在循环中工作。

与其说运动从输入层经过隐藏层，最终到达输出层，不如说 RNN 在循环中来回传递，这就是它保持短期记忆的方式。这意味着数据首先通过输入层，然后在隐藏层中循环，最后传递到输出层。需要注意的是，RNN 只有短期记忆，这也是为什么需要 LSTM 网络的原因。更多内容将在下一节中介绍。

从本质上讲，RNN 实际上有两个输入。第一个是初始数据，它通过神经网络传递；第二个实际上是它在此过程中获取的信息和上下文。这就是它通过该框架有效地调整当前和先前输入的权重，从而在经过循环时进行修正的方式。这一过程即反向传播，它通过回顾上一节中的内容（*深度学习简史*），你会记得这是深度学习取得成功的一个重要进展。

可以将 RNN 想象成一个由神经网络组成的集合，这些神经网络通过反向传播不断被重新训练和优化，以提高准确性，这也是它被认为是一种监督学习算法的原因。由于它是如此强大和有效的深度学习算法，我们可以看到 RNN 被广泛应用于从图像字幕生成和理解，到时间序列预测，再到自然语言处理和机器翻译等各个领域。

### LSTM

LSTM 本质上是具有更多记忆能力的 RNN。通常，它们的表现方式是通过 LSTM 网络，因为它们实际上将 RNN 层连接起来，这使得它们能够在较长时间的延迟或更长时间段内保持输入。就像计算机一样，LSTM 可以读写或删除它所拥有的记忆数据。因此，它能够学习哪些数据需要长时间保留。正如 RNN 不断调整其权重并优化性能一样，LSTM 也通过为要存储或删除的数据分配重要性来进行相同的操作，权重值就是它们的依据。

LSTM 模型模拟了我们通过 LSTM 单元随着时间丢弃不相关或琐碎信息的能力，这些单元能够让信息作为输入进入，完全被遗忘或排除，或者通过传递影响输出。这些分类被称为门控，它们使得 LSTM 能够通过反向传播进行学习。

### GANs

GANs 是我们最喜欢的人工神经网络类型，因为它们本质上由两个神经网络组成，它们彼此对抗，因此得名，并朝着生成能通过现实世界数据的目标进行竞争。由于这种生成能力，GANs 被广泛用于图像、视频和语音生成。最初，它们因其生成和自我调节能力被用于无监督学习，但也可以用于有监督学习和强化学习。其工作原理是，其中一个神经网络被称为生成器，另一个被称为判别器，这两个网络在这个生成过程中进行竞争。

GANs 最早在 2014 年由 Ian J. Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David Warde-Farley、Sherjil Ozair、Aaron Courville 和 Yoshua Bengio 发表的一篇突破性论文中提出，文中指出 GANs *“同时训练两个模型：一个生成模型 G，它捕捉数据分布；一个判别模型 D，它估计一个样本来自训练数据而非 G 的概率。G 的训练过程是最大化 D 犯错的概率。”*

引用

Goodfellow, I. J., Mirza, M., Xu, B., Ozair, S., Courville, A., & Bengio, Y. (2014). *生成对抗网络*。*arXiv*。[`doi.org/10.48550/arXiv.1406.2661`](https://doi.org/10.48550/arXiv.1406.2661)

我们可以把判别模型和生成模型看作同一枚硬币的两面。判别模型关注图像可能具有的特征，例如，寻找它当前正在学习的所有狗图像之间的关联。生成模型则从类别本身出发，扩展到该类别图像可能具有的潜在特征。如果我们以“太空小猫”这个类别为例，生成模型可能会查看它所接受的示例数据，并推断出，如果它创建一张图像，它应该涉及到太空和小猫。然后，判别模型会接收生成模型创建的图像，并根据其自身的学习确认，任何属于太空小猫类别的图像必须包含小猫和太空这两个特征。

另一种解释方式是，生成模型将标签映射到潜在特征，而判别模型则将特征映射到标签。我们对 GAN 最感兴趣的是，它们实际上能够通过或未能通过自己版本的图灵测试。如何判断是否通过了测试？如果 GAN 正确地将生成的图像识别为伪造图像，那么它通过了（或者说失败了？）自己的测试。这完全取决于你怎么看待通过与失败。如果它错误地将伪造/生成的图像标记为“真实”图像，那就意味着生成模型非常强大，因为它的判别器没有正确地进行区分。然而，因为这是一个双面硬币，这也意味着判别器需要加强，以便更加敏锐。这一点非常具有自我反思性。

GAN 运行过程的步骤首先从生成器神经网络开始，该网络接收数据并返回一张图像，然后将该图像与来自真实世界数据集的其他图像一起输入给判别器。接着，判别器会输出一个介于 0 到 1 之间的数值，该数值表示它对每个被判别图像的概率，0 代表伪造图像，1 代表真实的世界图像。GAN 还使用反向传播，因此每当判别器做出错误判断时，GAN 就会从之前的错误中学习，调整权重并优化自身的准确性。

### DBN

DBN 也有多个层次，包括多个隐藏层，但同一层的节点之间并没有相互连接，尽管它们与其他层的节点是连接的。层与层之间是有关系的，但层内的节点之间没有关系。DBN 是所谓**限制玻尔兹曼机**（**RBM**）的无监督学习层，RBM 本身是另一种形式的人工神经网络。这些 RBM 层被串联在一起形成一个 DBN。正因为有这个链条，当数据通过每个 RBM 的输入层时，DBN 会从前一层学习并获得特征。你添加的 RBM 层越多，整个 DBN 的改善和训练效果也越好。此外，每个 RBM 都是单独训练的，直到所有 DBN 都训练完毕，整个 DBN 的训练才算完成。

DBN 被称为生成式人工神经网络（ANN），因为每个 RBM 都基于概率学习并获得数据点的潜在值。正因为它们具备这种生成能力，它们可以用于图像识别、捕捉运动数据或识别语音等任务。它们在计算上也非常节能，因为每个 RBM 集群都是独立操作的。与前馈型 ANN 中数据必须穿越所有层不同，数据仅在每个集群内部局部流动。

作为产品经理，你不需要深入了解每一个神经网络，因为如果你在构建一个包含深度学习（DL）组件的产品，你会有内部专家帮助你确定使用哪些神经网络。但了解一些常见的神经网络类型对你是有帮助的，这样你就不会对这些决策感到茫然。在接下来的部分中，我们将看到深度学习神经网络如何与其他新兴技术交叉，以便更好地理解深度学习的能力和影响。

# 新兴技术——辅助技术和相关技术

机器学习（ML）和深度学习（DL）在与自然语言处理（**自然语言生成**（**NLG**）以及**自然语言理解**（**NLU**））、语音识别、聊天机器人、虚拟代理和助手、决策管理、过程自动化、文本分析、生物识别、网络安全、内容创作、图像和情感识别以及营销自动化相关的应用中得到了广泛应用。特别是从产品经理的角度来看，记住 AI 将越来越多地融入到我们生活和工作方式中是非常重要的。如果你作为产品经理从事创新工作，参与未来产品的新用例和最小可行产品（MVP）的构思与创建，这一点尤为真实。

随着时间的推移，我们将看到人工智能继续通过内部自动化的过程以及通过采用基于 AI 的无代码/低代码外部产品和应用程序来增强我们的劳动力，这些产品和应用程序将提升各个岗位的工作职能、技能和能力。增强现实（AR）、虚拟现实（VR）和元宇宙也为我们提供了新的新兴领域，在这些领域中，机器学习将学习更多关于我们世界的信息，帮助我们了解自己，也帮助我们建立全新的世界。我们还将继续看到机器学习通过 AI 驱动的设备得到应用，例如自动驾驶的飞机、火车和汽车，以及生物识别技术、纳米技术和物联网设备，这些设备能够共享关于我们身体和家电的数据流，我们可以利用这些数据来优化我们的安全、健康和能源使用。

当然，除了机器学习（ML）和深度学习（DL）之外，还有其他形式的人工智能（AI），以及一些常常与我们在本章中讨论的技术一起使用的新兴辅助技术。例如，尽管波士顿动力公司推出的狗狗 Spot 伴随着创新和声誉，但我们最近惊讶地发现，它们实际上并没有使用机器学习来训练这些小家伙。但即便如此，Spot 也将很快看到操作系统的 AI 更新，帮助它进行物体识别以及对这些物体的语义上下文化处理。

一般来说，人工智能（AI），尤其是深度学习（DL），可能很快就会通过量子计算得到更新，因为 IBM 通过公开宣布其量子计算机发展的“路线图”，使其雄心壮志更为具体，其中包括到 2023 年建造一台包含 1000 个量子比特（qubits）的量子计算机的宏伟目标。目前，IBM 最大的一台量子计算机包含 65 个量子比特。

量子计算可以在以成本效益的方式处理数据存储和检索方面，特别是大数据方面，极大地帮助我们。因为许多深度学习项目可能需要数周的时间来训练，并且需要访问大量数据，量子计算的辅助发展有可能在深度学习领域带来突破，甚至使得算法在训练时需要的数据更少，同时也能更快速地处理更多的数据和计算能力。这还可能为我们提供更多机会，帮助我们理解模型如何得出某些结论，并帮助解决深度学习中可能最大的难题——可解释性。

# 可解释性 – 优化伦理、警告和责任

伦理和责任在处理客户数据和行为时起着基础性作用，而且因为你们中的大多数人将会构建帮助人类做决策的产品，最终总会有人问你，你的产品是如何得出结论的。批判性思维是人类推理的基石之一，如果你的产品是基于深度学习（DL）的，那么你的回答将无法真正让任何人的怀疑得到满足。我们由衷的建议是：不要创建一个可能会伤害他人、让你被起诉或对你的商业造成风险的产品。

如果你正在以某种方式使用机器学习（ML）或深度学习（DL），并且这种使用方式有可能对他人造成伤害，或者如果存在明显的偏见，影响到少数群体（无论是种族、性别还是文化方面），那么就应该回到构思阶段。这无论是直接伤害还是下游伤害，都应该引起重视。这是机器学习带给我们集体的一个普遍风险：即我们将社会偏见编码进人工智能中，而没有采取必要的预防措施，确保我们输入给算法的数据是真正无偏的。

构建这些人工神经网络（ANN）的工程师们无法真正理解其内部机制，也无法真正解释 ANN 是如何做出决策的。正如我们之前看到的，尽管是外行的简化解释，ANN 结构是建立在现有的机器学习（ML）算法基础上的，并进行了扩展，因此几乎不可能有任何人能够真正解释这些网络是如何得出某些结论的。

再次强调，这就是为什么深度学习算法常被称为黑箱的原因，因为它们抵制对其工作原理的深入分析。由于人工神经网络的性质和复杂性，深度学习本身具有天然的不透明性。请记住，人工神经网络实际上只是对影响每个神经元的权重做出微小的调整。它们基本上是利用数学和统计学进行优化的复杂模式发现器。它们在每个数据点上进行数百次优化，经过多次训练迭代。我们根本没有足够的思维能力或语言来解释这一过程。

你也不需要成为深度学习工程师才能真正理解你的产品如何影响他人。如果你作为产品经理，无法充分阐述深度学习是如何在你的产品中应用的，至少不能证明你的深度学习产品输出不会对他人造成伤害，那么这可能不是你想全力投入的产品。

深度学习仍然处于研究阶段，许多产品经理由于我们在本章前面讨论过的可解释性问题而对其应用感到犹豫。因此，我们建议产品经理在尝试深度学习时要小心谨慎。我们有很多机器学习（ML）造成伤害的例子，即使它们仅仅涉及基本的线性回归模型。没有对同行和客户的责任心，贸然推进如此复杂且充满潜力的深度学习技术，只会给世界带来更多混乱和伤害。

我们是否总是需要如此谨慎？不一定。如果深度学习应用在通过检测癌症拯救生命方面做得非常好，或者在机器人技术中应用效果更佳，我们有什么理由阻碍进步呢？在使用深度学习时，要批判性地考虑何时需要关注。如果你的系统运行有效，且因为深度学习而变得更好，并且没有因人工神经网络的不透明性产生任何问题或担忧，那么世界就是美好的。

# 准确性 – 优化成功的关键

对于深度学习来说，我们只能真正衡量它的性能。即使从性能的角度来看，许多深度学习项目也未能提供工程师们期望的结果，因此管理预期变得尤为重要。如果你是产品经理或企业家，考虑将深度学习纳入你的产品，要以科学和好奇心的精神进行。保持对预期的开放态度。

但请确保你为团队的成功奠定了基础。你 ANN 性能的一个重要部分也体现在你开始训练模型之前的数据准备上。将数据通过 ANN 处理是你管道中的最后一步。如果你没有好的验证，或者数据质量差，那么你是无法看到积极的结果的。然后，一旦你确信你拥有足够的且清洁的数据，可以通过 ANN 处理，就开始尝试实验吧。如果你寻求最佳性能，你将需要尝试几种不同的模型，或者将几种模型组合在一起，并比较它们的表现。

微调深度学习模型所需的时间也相当漫长。如果你习惯了其他形式的机器学习（ML），那么经历几天甚至几周的模型训练可能会让你大吃一惊。这主要是因为训练人工神经网络所需的数据量庞大；大多数情况下，你至少需要 10,000 个数据点，所有这些数据都需要通过多个节点层并由 ANN 处理。你的 ANN 大多数情况下也会是我们之前提到的几种类型 ANN 的集合体。这个链条因此变得相当长。

理解人工神经网络（ANN）的本质本身就很神秘，因为人工神经元层次的复杂性。我们看不见确定性的特性。即使你做了一切“正确”的事，得到一个良好的表现，你也不真正知道为什么。你只知道它有效。相同的情况也会出现在某些事情出错或者你看到表现不佳时。你再次不知道为什么。也许是 ANN 本身出了问题，或者是你使用的方法有问题，亦或是环境发生了变化。回到更好表现的过程也是一个迭代的过程。然后，一切又回到原点。

记住，这些是新兴的技术算法。我们大家可能需要一些时间去适应新技术，并真正理解它们的力量。或者也不必！深度学习的部分失望其实在于预期的调整。有些深度学习算法能够让惊人的事情发生，并展现出极具前景的表现，但其他一些算法却可能轻易地失败。这不是一剂神奇的灵药。它只是一个强大的工具，需要由拥有知识、智慧和经验的人以正确的方式使用。考虑到我们一起讨论的大部分 ANN 都来自 80 年代、90 年代和 2000 年代初，那其实并没有多少时间。

所以，如果你在构建、管理或构思深度学习（DL）产品时，一定要小心谨慎。如果有疑虑，还有其他更易于解释的模型可供选择，我们已经在*第一章*中讨论过。宁愿安全一些，也不要事后后悔。如果你有充足的时间、耐心、兴奋和好奇心，并且有一个安全、娱乐性质的深度学习应用想法，那么你可能已经处于一个很好的位置，去探索这个激情，并创造出一些世界能够善用的东西。

# 总结

在这一章中，我们有机会深入了解深度学习（DL），并理解一些影响这一机器学习子领域的主要社会和历史因素。我们还得以了解一些在深度学习驱动的产品中最常用的人工神经网络（ANN），以便更加熟悉我们在构建深度学习时可能会遇到的实际模型。最后，我们通过探讨一些与深度学习协作的新兴技术，并深入讨论深度学习中最重要的概念：可解释性和准确性，结束了本章内容。

深度学习的人工神经网络（ANN）非常强大，表现出色，但如果你需要解释它们，你将遇到比使用更传统的机器学习模型时更多的问题。我们已经用书中的前三章时间来熟悉人工智能产品管理的更技术性方面。现在我们有了这个基础，我们可以花时间将所有这些技术进行情境化。

在下一章中，我们将探索市场上我们看到的一些主要 AI 产品领域，并且讨论一些对商业化贡献最大的伦理问题和成功因素。

# 参考文献

+   一些多元观察分类与分析方法：[`books.google.com/`](https://books.google.com/)

+   K-最近邻算法：分类与回归明星 [`www.historyofdatascience.com/k-nearest-neighbors-algorithm-classification-and-regression-star/`](https://www.historyofdatascience.com/k-nearest-neighbors-algorithm-classification-and-regression-star/)

+   随机森林 [`www.stat.berkeley.edu/~breiman/randomforest2001.pdf`](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)

+   决策树 [`www.cse.unr.edu/~bebis/CS479/PaperPresentations/DecisionTrees.pdf`](https://www.cse.unr.edu/~bebis/CS479/PaperPresentations/DecisionTrees.pdf)

+   支持向量机：最流行的机器学习算法 [`cml.rhul.ac.uk/svm.html`](https://cml.rhul.ac.uk/svm.html)

+   逻辑回归 [`uc-r.github.io/logistic_regression#:~:text=Logistic%20regression%20(aka%20logit%20regression,more%20predictor%20variables%20(X)`](https://uc-r.github.io/logistic_regression#:~:text=Logistic%20regression%20(aka%20logit%20regression,more%20predictor%20variables%20(X))

+   逻辑回归历史 [`holypython.com/log-reg/logistic-regression-history/`](https://holypython.com/log-reg/logistic-regression-history/)

+   贝叶斯分类器 [`www.sciencedirect.com/topics/computer-science/bayes-classifier#:~:text=Na%C3%AFve%20Bayes%20classifier%20(also%20known,use%20Na%C3%AFve%20Bayes%20since%201960s`](https://www.sciencedirect.com/topics/computer-science/bayes-classifier#:~:text=Na%C3%AFve%20Bayes%20classifier%20(also%20known,use%20Na%C3%AFve%20Bayes%20since%201960s)

+   主成分分析 [`www.sciencedirect.com/topics/agricultural-and-biological-sciences/principal-component-analysis#:~:text=PCA%20was%20invented%20in%201901,the%20modeling%20of%20response%20data`](https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/principal-component-analysis#:~:text=PCA%20was%20invented%20in%201901,the%20modeling%20of%20response%20data)

+   加尔顿、皮尔逊与豌豆：统计学教师的线性回归简史 [`www.tandfonline.com/doi/full/10.1080/10691898.2001.11910537`](https://www.tandfonline.com/doi/full/10.1080/10691898.2001.11910537)

+   IBM 承诺到 2023 年推出 1000 量子比特量子计算机——这一里程碑 [`www.science.org/content/article/ibm-promises-1000-qubit-quantum-computer-milestone-2023`](https://www.science.org/content/article/ibm-promises-1000-qubit-quantum-computer-milestone-2023)

+   波士顿动力公司表示，Spot 机器狗的人工智能进展即将到来 [`venturebeat.com/ai/boston-dynamics-says-ai-advances-for-spot-the-robo-dog-are-coming/`](https://venturebeat.com/ai/boston-dynamics-says-ai-advances-for-spot-the-robo-dog-are-coming/)

+   卷积神经网络教程 [`www.simplilearn.com/tutorials/deep-learning-tutorial/convolutional-neural-network`](https://www.simplilearn.com/tutorials/deep-learning-tutorial/convolutional-neural-network)

+   生成对抗网络 [`arxiv.org/abs/1406.2661`](https://arxiv.org/abs/1406.2661)

+   自组织映射 [`sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf`](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1990-Kohonen-PIEEE.pdf)

+   多变量函数插值与自适应网络 [`sci2s.ugr.es/keel/pdf/algorithm/articulo/1988-Broomhead-CS.pdf`](https://sci2s.ugr.es/keel/pdf/algorithm/articulo/1988-Broomhead-CS.pdf)

+   长短期记忆 [`www.bioinf.jku.at/publications/older/2604.pdf`](http://www.bioinf.jku.at/publications/older/2604.pdf)

+   通过反向传播误差学习表示 [`www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf`](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)

+   变分问题的数值解法 [`www.sciencedirect.com/science/article/pii/0022247X62900045`](https://www.sciencedirect.com/science/article/pii/0022247X62900045)

+   感知机：一种感知与识别的自动机 [`blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf`](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)

+   神经活动中固有思想的逻辑演算 [`www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf`](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)

+   人工智能教育 [`aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-4`](https://aiindex.stanford.edu/wp-content/uploads/2021/03/2021-AI-Index-Report-_Chapter-4.pdf)
