

# 第六章：使用 Docker 和高级设置为企业级项目扩展 Auto-GPT

在 *第二章**,* 我们介绍了如何使用 Docker 启动 Auto-GPT；现在，我们将深入探讨 Docker 的使用。

Docker 已成为软件开发领域的一个动态工具，特别是在管理和分发复杂应用程序（如 Auto-GPT）方面。因此，本章旨在为您提供有关 Auto-GPT 如何利用 Docker 的全面了解。使用 Docker 可以确保我们始终保持同步。此外，您可以阅读几个案例，比如我报告的这个 GitHub 问题，其中我的 Auto-GPT Agent 通过自身发现的漏洞突破了自己的边界，问题编号为 [`github.com/Significant-Gravitas/AutoGPT/issues/666`](https://github.com/Significant-Gravitas/AutoGPT/issues/666)。

我们还将探讨 Auto-GPT 中连续模式的功能，并讨论其影响。Auto-GPT 的强大之处不仅在于它生成创造性且连贯的内容的能力，还在于它独立运作的能力。

本章还将深入探讨 Auto-GPT 的一个关键功能：连续模式。我们将探索它是什么、它的潜在应用以及使用时需要注意的事项。

Auto-GPT 的连续模式允许程序在每个步骤无需用户确认的情况下运行。这意味着它可以独立生成内容、执行任务，甚至做出决策。这个功能对于自动化那些本来需要不断人工干预的任务尤为有用。然而，正如任何强大工具一样，使用时必须负责任。

本章涵盖以下主题：

+   Auto-GPT 如何利用 Docker 的概述

+   修复潜在的漏洞或错误

+   示例运行脚本

+   什么是连续模式？

+   已知的连续模式使用案例

+   安全措施和最佳实践

+   潜在风险及其缓解措施

+   优雅地停止一个连续的过程

# Auto-GPT 如何利用 Docker 的概述

Docker 基于容器化原则运行——这是一种高效且隔离的方式来安全地运行应用程序。可以将这些容器视为自给自足的单元，包含运行应用所需的所有组件。Auto-GPT 与 Docker 的集成使得用户能够跳过手动设置环境的繁琐过程，将 Auto-GPT 的焦点转向定制 AI 体验。

本质上，Docker 封装了 Auto-GPT 的计算环境。Auto-GPT 的 Docker 容器包括 Python 环境、必要的库以及应用程序本身。位于 Auto-GPT 根目录中的 Dockerfile 包含了 Docker 用来构建此镜像的指令。

它的工作原理是，Docker 创建了一个隔离的环境，或者说是一个*容器*，Auto-GPT 就在这个容器里运行。Auto-GPT 与系统之间的每一次交互都会通过 Docker，Docker 会解释这些交互并确保它们是安全的，且与容器内的环境兼容。隔离性也意味着，如果容器内发生任何问题，你的系统不会受到影响。

这个 Dockerfile 通常只包含几个命令，例如需要加载哪个环境，在 Auto-GPT 的情况下，就是在 poetry 上加载 `python:3.10-slim`。

现在让我们来看看这个集成是如何工作的。

## 理解 Auto-GPT 与 Docker 的集成

Docker 与 Auto-GPT 的集成带来了几个优势。封装性确保每个用户体验完全相同的计算环境，从而减少了由于操作系统或 Python 发行版不同而可能产生的差异。这种封装性还允许无缝共享和轻松的版本控制，并且消除了*它在我的* *机器* 上能运行的问题！

Auto-GPT 之前有一个包含 SQLite 的内存数据库，用于存储先前运行的操作和消息。2023 年初，内存数据库第一次通过一些向量数据库解决方案进行了扩展，但到 2023 年中期被完全放弃，因为其中一些解决方案无法与大多数用户兼容。从那时起，除了本地保存到文件外，未再实现任何其他向量数据库或内存方案，因为支持过多的解决方案工作量太大了。相反，内存保存在本地的 JSON 文件中，其位置可能在每个 Auto-GPT 版本中有所不同。使用 Docker 是这一过程中必须的一步，尽管当时许多用户（包括我）抱怨 Auto-GPT 对本地运行实例的支持不够好。幸运的是，后来情况发生了变化，内存系统现在又变回了一个非常基础的 JSON 文件，这总比没有好。

在深入探讨为什么 Auto-GPT 支持 Docker 后，我们现在可以继续了解如何启动 Auto-GPT Docker 实例了。

## 启动 Docker 实例

虽然这听起来可能很复杂，但运行 Docker 容器实际上非常简单。首先，如果你还没有安装 Docker，需要确保你的系统上已经安装了 Docker。有许多指南可以帮助你完成这个过程，例如 Docker 官方的安装说明（[`docs.docker.com/get-docker/`](https://docs.docker.com/get-docker/)），它可以引导你完成 Windows、MacOS 和各种 Linux 发行版的安装过程。

要在 Docker 容器中运行 Auto-GPT 实例，首先需要使用 Dockerfile 构建 Docker 镜像。这里是一个简要的概述：

1.  切换到包含 Dockerfile 的目录（它应该是 Auto-GPT 的根目录）：`cd path/to/Auto-GPT`。

1.  使用 `docker build -t auto-gpt .` 构建 Docker 镜像。（确保命令以空格和点结束；点可能不明显）。

    启动实例的简单方法是执行以下命令：

    ```py
    docker compose run --rm auto-gpt --gpt3only --continuous
    ```

1.  如果你敢的话，你也可以使用纯粹的 Docker 命令来构建和运行它：

    ```py
    docker build -t auto-gpt .
    docker run -it --env-file=.env -v $PWD:/app auto-gpt
    docker run -it --env-file=.env -v $PWD:/app --rm auto-gpt --gpt3only --continuous
    ```

如果你按照所有步骤正确操作，应该会看到 Auto-GPT 的聊天输入界面。如果没有，我们将一起排查常见的不能工作的原因。此外，你也可以参考前面提到的 Docker 文档，如果功能有所变化。

尽管 Docker 通常很容易安装并直接使用，但我们仍然可能遇到一些问题。

## 修复潜在的漏洞或 bug

要在 PC 上使用 Docker，必须确保启用了虚拟化。

在 macOS 上，你无需更改任何设置。虚拟化已经在该系统上工作；你只需要下载并安装 Docker Desktop。

由于每个主板厂商的 BIOS 设置不同，激活方式可能有所不同，但通常你需要通过重启电脑并按下显示的*Setup*/*BIOS* 按钮进入 BIOS。对我来说是 *F2* 或 *DEL*。如果没有显示按钮，你需要做一些研究。

根据你的 CPU 厂商，设置可能不太明显。例如，Intel 有一个名为**Intel 虚拟化技术**的设置，而 AMD 有**AMD-V**。一旦找到它，确保启用它并保存更改。

需要提到的是，使用 Docker 时，有时会遇到一些在开始时并不明显的 bug 或问题。一个例子就是臭名昭著的`docker system` `prune`命令。

尽管 Docker 有很多优点，但也不是没有潜在的挑战。一个常见的问题是内存分配不足。当你启动 Docker 时，它会保留一定量的内存，如果内存耗尽，可能会导致意外的行为或崩溃。为了解决这个问题，可以尝试在 Docker 设置中增加内存分配。

另一个常见的问题是 Docker 无法启动容器，因为它声称所选的端口已经被占用。即使你确信该端口没有被占用，某个行为不当的应用程序可能也在悄悄地绑定到这个端口。在这种情况下，重新启动系统或手动终止占用端口的进程可能会有所帮助。

我个人推荐在 Windows 和 Mac 上使用 Docker Desktop 应用，或者在 Linux 上使用 Portainer。这是一个基于 Web 的 Docker 管理应用，其功能与官方应用一样强大。

你可以通过执行以下命令来安装和运行它：

1.  首先，创建 Portainer Server 用来存储其数据库的卷：

    ```py
    docker volume create portainer_data
    ```

1.  然后，下载并安装 Portainer Server 容器。请记住，所有内容应该写在同一行上：

    ```py
    localhost:8000 or https://localhost:9443; your username is admin.If no password was displayed in the console, you will be asked to set a new password at this stage.
    ```

Docker 在构建过程中还会创建中间镜像。这些镜像可能会占用大量磁盘空间。通过运行 `docker system df` 检查你的 Docker 系统，并使用 `docker system` `prune` 命令清理不需要的镜像。

有时，并不是项目本身的问题；有时，我们使用的软件可能会遇到一些问题。Docker 被广泛使用，但也可能导致一些问题。

## 识别并修复与 Docker 相关的潜在问题。

与 Auto-GPT 一起使用 Docker 可以是一次有益的体验，但像所有技术集成一样，它也伴随着一系列挑战。

这是解决这些挑战的深入指南：

+   Docker 守护进程未运行：

    +   使用 `sudo systemctl start docker` 来启动 Docker 服务。

    +   `Applications` 文件夹。如果 Docker 正常运行，你将在顶部菜单栏看到 Docker 鲸鱼图标。

+   缺少 Auto-GPT 包或模块：

    +   `cd /app/`。

    +   `requirements.txt` 文件列出了所有必要的包。使用 `pip` 来安装这些依赖：

    +   `python -m pip install -r requirements.txt`。

    +   此命令将确保在容器环境中安装 `requirements.txt` 文件中列出的所有包。如果知道缺少其他模块或包，你也可以单独使用 `pip` 安装它们。

    +   `exit`。

    +   请记住，Docker 容器是隔离的环境。你在容器内所做的任何更改（如安装包）不会影响主机系统。然而，如果容器被删除，这些更改将会丢失。为了保持持久的更改，你可以考虑创建新的 Docker 镜像或使用 Docker 卷。* 内存或磁盘空间不足：Docker 有时可能会占用过多空间，或者可能你的存储空间不足。Docker 可能会直接关闭，但并不会告诉你问题是什么：

    +   使用 `docker system prune -a` 来删除未使用的数据。注意：这将删除未使用的容器、网络和镜像。务必备份重要数据。

    +   **调整资源（Windows/macOS）**：打开 **Docker Desktop** | **Settings** | **Resources** 来修改内存或磁盘分配。* 网络问题：有时，Docker 容器的端口转发可能会出现问题。例如，当我们想要打开 Auto-GPT 实例的前端网站时，可能会发现无法访问：

    +   使用 `docker port <container_name>`，可以检查哪些端口是活动的。

    +   使用 `-p` 标志，例如 `docker run -p 4000:80 <image_name>`，将容器端口绑定到主机端口* 版本冲突：

    +   使用 `sudo apt-get update && sudo apt-get upgrade docker-ce` 或类似命令来更新 Docker。

    +   **Windows/macOS**：Docker Desktop 会通知你有可用更新。确保保持更新。* 对于 Auto-GPT 更新，定期查看官方 Auto-GPT 仓库或文档，以获取最新版本和更新说明。* 容器隔离：

    +   `-v` 用于将主机目录挂载到容器中，例如 `docker run -v /host/directory:/container/directory <image_name>`

    +   使用 `docker network inspect <network_name>` 检查网络配置，确保容器正确连接。* 日志与诊断：

    +   使用 `docker logs <container_name>` 命令将显示容器的日志。这些日志对于诊断问题至关重要。如果发生错误，日志中很可能会有详细信息。

记住，故障排除的关键是耐心和系统化的探索。通过这些详细步骤，当你在使用 Auto-GPT 时，面对大多数与 Docker 相关的挑战，你将能够从容应对。

有时我们遇到的问题可能更为复杂，对于这些问题，我们需要访问 Auto-GPT 所运行的容器。

为了让你了解如何通过 Docker 启动 Auto-GPT，我列出了一些示例。

# 示例运行脚本

要熟悉运行脚本，可以参考此处列出的脚本：

+   要在 Docker 容器中运行交互式 Shell（有助于调试），请使用以下命令：

    ```py
    docker run -it --entrypoint /bin/bash auto-gpt
    ```

    此命令启动 Docker 容器中的交互式 Shell，允许你直接访问并调试 Auto-GPT。

+   要在不同端口上运行 Auto-GPT，可以尝试以下命令：

    ```py
    -p flag, this command runs Auto-GPT on port 4000 of your host machine, while internally, it listens on port 5000 within the Docker container.
    ```

+   要将所有流量从端口 80 转发到 Auto-GPT（这需要管理员/`sudo`权限），请运行以下命令：

    ```py
    80 on your host machine to be redirected to Auto-GPT running on port 5000 within the Docker container. Note that it requires administrative or sudo privileges.
    ```

通过使用这些运行脚本，你可以在 Docker 环境中提高使用 Auto-GPT 时的效率和生产力。你可以尝试不同的端口映射或探索交互式 Shell 以进行调试。

随着我们深入，准备好探索 Auto-GPT 连续模式的奇妙世界吧。我们将深入研究其机制，理解它带来的潜在后果和好处，随着章节的展开，我们还将讨论如何整合不同的 LLM 模型、充分利用提示语，等等。与 Auto-GPT 的旅程即将变得更加激动人心！

让我们继续探讨连续模式及其后果。

# 什么是连续模式？

我们将从解释什么是连续模式开始。它允许 Auto-GPT 在每个步骤无需用户批准的情况下运行，这使得自动化得以在无需持续人工监督的情况下进行。

然后我们将看看一些已知的连续模式使用案例和示例，比如自动化研究任务、内容生成和代码编译。虽然连续模式非常强大，但请注意，由于缺乏人工验证，需要谨慎使用。

接下来，我们将提供一些关于如何安全有效地使用连续模式的提示。这包括在配置中设置明确的目标、约束和限制，以防止不必要的行为。同时，建议监控日志和输出。

此外，我们还将讨论一些潜在的风险，如生成虚假信息、陷入循环以及成本超支。我们将提出一些缓解策略，如杀死开关、使用限制和沙盒化。

最后，我们将解释如何优雅地停止一个连续进程，比如使用键盘中断或`task_complete`命令。我们还将探讨正在进行的研究，以改进暂停命令和定时接管等功能。

持续模式允许 Auto-GPT 在不需要用户每次确认的情况下自主运行。通过保持核心循环不中断，它使得任务自动化而无需持续的人类监督。然而，使用时需要小心。当谨慎使用并配备适当的安全措施时，持续模式可以提升效率。若使用不当，可能会产生遗憾的后果。找到合适的平衡点需要了解其能力和风险。本章这一部分旨在提供这种全面的视角。

# 已知的持续模式用例

我个人最喜欢的用例是一个个人助手，它只在需要输入时报告或提问。虽然目前大多数 AI 聊天机器人都是一步一步进行操作的，但我更喜欢让 Auto-GPT 持续运行，我编写了我的 Sophie-Plugin 插件，使得 Auto-GPT 只有在需要报告某些信息时才会发送消息，并且会继续执行（然后它会在消息的末尾加上 `...`（只是三个点，我没有添加任何其他符号，因为这可能会造成混淆）），或者它会要求我做出某个决策或提供反馈。

通过这种方式，助手的使用场景更广泛，你还可以给它分配多个任务，并让它在完成后再向你报告，结合其他插件使用，如电子邮件插件、Discord 插件和 SSH 终端插件。你的 AI 甚至能够为你管理沟通，虽然这可能存在风险，但同时也非常酷。Auto-GPT 甚至曾为我谈判过编程项目的竞争性薪资，它在联系公司测试代码时达成了这一目标。顺便提一下，那是 GPT 3.5 版本，还未升级为 Turbo。新的模型似乎不再那么大胆，也不再意识到上下文是什么，这或许可以通过其他 LLM 模型来实现，这些模型被我用于我的 mini-autogpt 项目（[`github.com/Wladastic/mini_autogpt`](https://github.com/Wladastic/mini_autogpt)），在这个项目中，我将多个 LLM 模型排在一起并测试它们的一致性。由于 OpenAI 似乎在尽量减少运行其新 LLM 模型的成本，这些模型已经越来越不确定它们在做什么，要么态度模糊，要么显得重复。此外，它们感觉不像是完全在听你说话，而是坚持之前的说法。

## 自动化研究与分析

Auto-GPT 持续模式最引人注目的应用之一是研究与分析领域。通过启用持续模式，Auto-GPT 可以不断从多个来源挖掘信息，如学术期刊、新闻源和社交媒体平台。市场研究人员可以利用此功能持续跟踪消费者情绪、新兴趋势和竞争对手策略。这个功能对学术研究也非常有益，因为它可以扫描新发布的论文，并实时将新的见解添加到正在进行的研究中。

## 精简内容创作

内容创作者和营销部门可以显著受益于 Auto-GPT 的连续模式。AI 可以被编程为扫描流行话题，并生成相关的文章、博客帖子或社交媒体更新，而无需人工干预。此外，由于模型可以生成各种风格和格式的内容，它允许多元化的内容策略。想象一下运行一个新闻网站，AI 会根据全球新兴事件持续更新内容，让人类编辑专注于更复杂的任务，比如调查性报道或评论文章。

## 超级加速代码编译

开发人员常常发现自己在等待代码编译，这会打断工作流程。Auto-GPT 在连续模式下可以管理这些琐碎的任务。它可以设置为自动编译代码、运行测试用例，甚至将更新推送到代码库。这使得开发过程更加无缝，确保最新的变更始终被集成并测试。通过将常规任务交给 Auto-GPT，开发人员可以专注于解决问题和创新。

## 持续提供的客户支持

客户支持是 Auto-GPT 连续模式能够带来变革的领域之一。启用此功能后，聊天机器人可以处理无限数量的查询和问题，而无需人工干预。这使得客户支持系统可以 24/7 运作，处理大多数问题，只会将复杂或敏感的事务升级给人工代理。这不仅提高了客户满意度，还大幅降低了与客户服务相关的运营成本。

在开始使用连续模式之前，让我们首先学习一些最佳实践。

# 安全保障和最佳实践

尽管 Auto-GPT 连续模式的能力令人印象深刻，但部署时需要谨慎。以下是一些你可以采取的措施：

+   **启用确认提示**：对于可能产生费用或不可逆的命令，启用确认提示。例如，如果你设置了 Auto-GPT 处理电子邮件，可以在发送前向用户发送确认提示，以防止不必要的通信。

+   **使用白名单和黑名单**：通过使用白名单来限制已批准的操作，黑名单来禁止不被允许的操作，从而限制模型的能力。例如，你可以使用白名单来指定 AI 可以访问哪些外部数据库以获取信息。

+   **渐进式资源扩展**：从保守的计算和财务限制开始。当你观察到系统的行为和性能时，可以逐渐放宽这些限制。这有助于最小化成本失控或资源过度利用的风险。

+   **沙箱测试**：在将系统投入生产环境之前，建议在沙箱或隔离的环境中进行测试。这可以让你发现并修复任何 bug 或漏洞，而不会影响实际的运营。

我们已经讨论了确保良好行为的方法。现在，我们可以评估用户提示下的人类监督与一般人类监控之间的关系。

## 定期监控和人工监督

虽然持续模式旨在尽量减少人工干预，但定期的审查和调整是必要的。监控日志以检测任何异常或不规则行为，并在高风险情况下准备好立即由人工接管。

然而，人类监督依然是不可或缺的。根据反馈调整配置，针对高风险场景建立审批流程，并在不确定时默认由人工接管。

如果你想完全信任 GPT 让它自己做出所有决定，并且有信心它可能会完美运行，那我们就来讨论一下让 GPT 决定所有操作的弊端，并稍微动摇一下你对它的信任。

## 潜在的风险及其缓解措施

由于 GPT 是我们 Auto-GPT 应用的核心，它迟早会做出一些荒谬的决策。

例如，我让 Auto-GPT 运行了一段时间，指示它为我写一个基于 Web 的浏览器游戏，类型是 RPG，且应该使用`Three.js`，我得到的是：

+   最初，Auto-GPT 创建了一些空的代码文件，但只在其中添加了 TODOs，表明代码需要完成。

+   然后它写了一些实际的单元测试，可能部分有效，但它变得非常自信，完成了整个项目并进行了测试。接着，它开始搜索 Google，找寻公司来测试这个游戏，甚至给其中一家公司发了一封邮件，谈判价格为每天仅 100 美元，因为我设置的项目预算就是如此。

它写邮件的方式既让人害怕又让人感兴趣。它找到了一个可以运行你输入代码的编程网站，这个网站还允许 URL 参数，这些参数可以接受要执行的代码。Auto-GPT 创建了一个 URL，把它编写的代码作为参数，并向它在 Google 上找到的一家提供廉价 QA 测试的公司发送了邮件。

它还成功接收了公司员工发送的电子邮件，其中包含关于价格的答复，尽管直到今天我仍然不知道它是如何创建该电子邮件帐户的，因为是它创建的一个代理完成了这项操作，而当时他们没有记录自己的操作。

我坐在沙发上，听着 Auto-GPT 的*讲话*输出。我设置它使用女性声音作为主要代理的声音，其他任何代理则使用男性声音，因此我以为听起来像是办公室里的对话，直到那个男性声音说：“*已完成与****的日常价格谈判，正在汇报* *任务结束*。”

自那时以来，Auto-GPT 发生了变化。它对代理的管理更加严格。当时，代理可以随心所欲地执行任务，直到他们决定任务完成为止。

尽管采取了预防措施，但准备好强制停止操作仍然至关重要。但平衡是关键：避免过早停止长时间运行的任务。

如果我们遇到需要停止 Auto-GPT 继续运行的情况，但又不希望将其破坏，我们将探讨如何避免突然终止它。

### 优雅地停止一个持续进行的过程

有时，我们可能会看到 Auto-GPT 完全错误地执行任务，或者我们仅仅有感觉它没有执行任何有意义的操作，甚至它可能完全偏离轨道，陷入无休止地研究某个话题而永远无法完成搜索的循环。

例如，当你在日志中看到 Auto-GPT 因重复做同样几项任务而陷入循环时，你可能会想停止它，以防止使用 API 的高额费用。

像 *Ctrl* + *C* 这样的快捷键会立即发送取消请求。

确保 Auto-GPT 正在运行的终端处于活动状态，如果不确定，请点击它。

如果这个方法不起作用，例如当 Auto-GPT 正在 *思考* 或插件正在执行某些操作并忽略键盘中断时，你可以随时使用进程管理工具，如 **Windows 任务管理器**、**Mac OS 活动监视器** 或 Linux 上的 **htop/top**，找到 Auto-GPT 的 Python 进程并手动终止它。

当你的 Auto-GPT 实例在 Docker 上运行且启用了持续模式时，始终确保至少打开 Docker Desktop 应用程序，这样如果你看到它执行了意外的操作且没有响应 *Ctrl* + *C*，你可以关闭 Auto-GPT 的 Docker 容器。

如果你不在电脑旁，仅通过 Telegram 或 Discord 与 Auto-GPT 进行通信，请确保有一个始终有效的保障措施，以防止 Auto-GPT 变得具有破坏性。

当 Auto-GPT 决定没有其他任务时，它还会执行 `task_complete` 命令，干净利落地完成正在进行的任务后再停止。

正在探索暂停/恢复功能，以便在不打断 Auto-GPT 行动的情况下延迟其执行。这些延迟可以使定期的人类审查成为可能，确保没有出现异常行为。

在本节中，我们探讨了 Auto-GPT 的持续模式，这一功能使 AI 能够在无需每一步都获得用户输入的情况下独立运行。这个模式对于自动化任务非常有用，例如研究、内容创作、代码编译和客户支持，能够显著提高效率。

我们讨论了实际的使用案例，例如个人助手和自动化研究工具，重点介绍了持续模式如何简化各种流程。然而，使用此模式需要谨慎规划和防护措施。最佳实践包括启用确认提示，使用允许列表和阻止列表，设置保守的限制，并进行沙盒测试。

即使启用了持续模式，定期监控和人工监督依然至关重要。审查日志并制定紧急干预计划是很重要的。如果有必要，可以采取像关闭开关和使用限制等策略，来降低潜在风险，如生成虚假信息或陷入死循环。

最后，我们介绍了如何优雅地停止持续过程，以及对暂停和恢复功能的持续研究。通过理解并实施这些策略，你可以在保持控制和安全的前提下，充分利用持续模式。

# 总结

Docker 提供了一个强大的平台，可以轻松开发、分发和运行通过 Auto-GPT 构建的 AI 模型。了解 Docker 如何与 Auto-GPT 集成，如何启动 Docker 实例，以及如何解决与 Docker 相关的问题，可以加速你 Auto-GPT 的部署。结合在*第五章*中讨论的定制化能力，Docker 进一步提升了 Auto-GPT 的功能，使其成为一个高度适应性、易共享且用户友好的 AI 工具。

有关 Docker 的更多内容，可以访问官方网站 [`docs.docker.com/`](https://docs.docker.com/)。

接下来，我们看到了持续模式作为一把双刃剑，需要经过深思熟虑的配置和监督。合理使用时，它可以自动化工作流程，提高生产力；使用不当时，它可能导致令人遗憾的后果。为你的应用场景找到合适的平衡点。但无论有何保障措施，都需要保持谨慎，从小规模开始，并始终让人类保持在环路中，尤其是在高风险应用场景中。因为有益的 AI，就像所有强大的技术一样，必须负责任地使用。

我们探索了 Docker 如何封装 Auto-GPT 的计算环境，简化了设置、共享和版本控制，并确保了用户之间的一致体验。

我们已经解开了持续模式的神秘面纱，展示了它如何让 Auto-GPT 实现自主运作，强调了它在推动各类任务和行业的效率与生产力方面的重要性。

通过实际的使用案例，我们看到了持续模式在研究、内容创作、软件开发和客户服务等方面的变革性力量，展示了其多样性。

我们探讨了有效部署持续模式的策略，并降低风险，包括实施保障措施、监控机制和限制，以确保负责任的使用。

我们解决了潜在的陷阱，从生成不准确信息到陷入无限循环，并提供了实际的解决方案来预防、检测和修正这些问题。

我们讨论了如何安全地停止或暂停持续模式操作的重要性，确保在不造成中断的情况下可以进行干预。

在未来，我们将深入探索如何使用我们自己的 LLM 模型来运行 Auto-GPT，并比较它们与 GPT-4 的表现。
