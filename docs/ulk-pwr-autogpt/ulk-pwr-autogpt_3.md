

# 第三章：掌握提示生成并理解 Auto-GPT 如何生成提示

你真的很喜欢这本书！恭喜你到达这一章！

在前几章中，我们探讨了 Auto-GPT 的基础知识及其安装过程。现在，我们将更深入地探讨与使用这个强大语言模型相关的最关键方面之一——提示生成。在本章中，我们将揭示 Auto-GPT 生成提示的过程，理解它们为何如此重要，并学习如何制作有效的提示，以从 Auto-GPT 中获得最大收益。让我们开始吧！

在本章中，我们将探讨以下主题：

+   什么是提示，它们为什么重要？

+   制作有效提示的技巧

+   Auto-GPT 如何生成提示的概述

+   什么有效，什么让 GPT 困惑的示例

# 什么是提示，它们为什么重要？

我们通常知道如何与 ChatGPT 交流，即直接与它聊天。ChatGPT 会直接回应我们提出的任何问题。但这与提示有何关系呢？

我们发送的文本叫做提示，它可以是一个问题、陈述、任务，或者我们想告诉**大型语言模型**（**LLM**）的任何内容。然而，这些文本并不会直接输入到 LLM 中。

应用程序通常会提供对话的上下文，例如约束（例如，*你是一个有帮助的助手。永远不要与用户争论，只在符合伦理且对用户有帮助时才回答请求*）。

提示是你提供给语言模型生成回应的初始输入。它们可以有多种形式，例如问题、陈述或任务。

例如，如果你问模型，*今天的天气怎么样？*，这将是一个问题提示。

一条陈述性提示可能是像*告诉我关于罗马的历史*，而任务提示可能是*写一个关于宇宙飞船的短故事*。每种类型的提示都有不同的目的，并且可以引发模型不同类型的回应。理解如何有效地使用这些不同类型的提示是从与 Auto-GPT 的互动中获得最大收益的关键。

这是一个发送给 Auto-GPT 的提示，有人将其删除：

*请添加之前包含提示的文本—*

使用该提示时，我们提供了以下约束：

+   “永远不要与用户争论”：

    +   **解释**：用户始终是信息的真实来源。

    +   **效果**：这一限制强调以用户为中心的方法，AI 避免挑战用户的陈述或观点。它确保 AI 的回答与用户的输入一致或保持中立，但这可能限制互动对话的深度。它还可能导致 AI“角色扮演”，假装情况仅仅是一个故事，尽量以符合上下文或上述句子的方式回应。这可能导致 Auto-GPT 做出错误假设，或者忽视不真实的信息，例如一些设置步骤虽然听起来合理，但其实是编造的。

+   “你是一个有帮助的助手”：

    +   **角色定义**：明确界定了 AI 的角色是助手。

    +   **语气和互动**：为对话设置预定义的语气和方向。AI 被编程为严格遵循助手角色。然而，这未必总是符合用户的预期。用户可能希望更为非正式、友好的语气和互动，如主动提问“*我今天如何可以帮助您？*”而 AI 的参与可能局限于反应性回答，而缺乏创意输入。

+   “只有在道德和有益于用户的情况下才回答问题”：

    +   **能力范围**：对 AI 的功能施加了重大限制。AI 被编程为在每次互动中优先考虑道德和用户效用。

    +   **对决策支持的影响**：这可能导致过于谨慎的处理方式，AI 避免执行可能帮助用户减轻决策责任的任务。例如，当面对复杂或道德模糊的任务时，AI 可能选择最小化参与，而不是提供全面的帮助。

    +   **实践中的一个例子**：当要求协助创作任务时，例如写一章关于玫瑰的不同颜色，AI 可能会将回应限制为概述潜在的做法（例如，提出章节的要点），而不是主动参与到创作过程本身。

提示定义了任务以及 LLM 应该回答的内容的背景。

正如我们在*第一章*中讨论的，Auto-GPT 会发送一个相当庞大的提示，其中定义了上下文、命令和限制，以及一条“`user`”消息，内容如下：

```py
role": "user",
"content": "Determine which next command to use, and respond using the format specified above:" :" :"
```

通过这种方式，GPT 实际上在一个假设的故事中扮演角色，包含当前信息的背景以及只有 Auto-GPT 才能执行的可能命令，依据其认为应使用的命令回应用户的查询提示。

如果考虑到例如让 ChatGPT 做某件事情，它通常会回应那句让人恼火的“*作为一个 AI* *语言模型...*”时，这会变得更加有趣。

## 词语表述

当我们查看 Auto-GPT 使用的提示时，我们很容易想到重新措辞，缩短提示以节省 token，或添加更多的上下文。

然而，每个操作都有其缺点。

大型语言模型（LLMs）并不是人类；它们并不真正理解它所接收的输入和输出的内容。它们只是被训练来生成在给定某个输入时最可能的文本。

这意味着即使两个句子具有完全相同的含义，它们也可能被完全不同地解释。

由于机器学习的性质，用于训练 LLM 的文本几乎也定义了它如何存储其知识。

如果句子“*我刚刚卖掉了我的车，因为我只是想买一个冰淇淋*”在训练数据中从未出现过，LLM 会更难理解这个句子，因为这些 token 没有关系。“买”会与“车”和“冰淇淋”相关，但“车”和“冰淇淋”之间的关联则非常薄弱。

由于 GPT-4 不是开源的，我们必须依赖 Llama 来理解模型是如何工作的。

Llama 有一些参数，比如长度惩罚和唯一性（即相同单词出现的频率）。这些限制了每个嵌入的向量长度，例如。

## 嵌入（Embeddings）

嵌入（Embeddings）是 Auto-GPT 生成提示的关键部分。它们本质上是一种将单词和短语表示为模型能够理解的数字形式的方法。每个单词或短语都被表示为多维空间中的一个点，其中点与点之间的距离和方向可以表示单词或短语之间的关系。

例如，在这个多维空间中，“king”和“queen”可能会靠得很近，表示它们之间有很强的关系，而“king”和“ice cream”则会相距较远，表示它们之间的关系较弱。这就是模型如何理解上下文并生成相关响应的方式。

创建这些嵌入的过程涉及将输入文本分解成 token，token 可以是单词、单词的一部分，甚至是单个字符，这取决于语言。然后，这些 token 会被映射到多维空间中的向量。

模型然后使用这些向量生成响应。它通过计算基于当前上下文每个可能下一个 token 的概率来实现这一点。具有最高概率的 token 被选中，然后重复这一过程，直到生成完整的响应。

这是该过程的简化解释，实际实现要复杂得多，包括使用注意力机制来确定输入中最相关的部分，以及使用 Transformer 模型来处理长序列的 token。

然而，关键的要点是，模型通过理解输入的上下文并计算出最可能的下一个词来生成提示。这就是为什么提示措辞如此重要，因为它可以极大地影响模型对上下文的理解，从而影响生成的回应。

在下一部分，我们将查看一些制作有效提示的技巧，并提供一些什么有效、什么让 Auto-GPT 困惑的例子。

# 制作有效提示的技巧

制作有效提示的艺术是一项需要对语言模型的能力和局限性有细致理解的技能。以下指南将帮助你制作更可能获得 Auto-GPT 理想回应的提示：

+   **精确至关重要**：提示的具体性会显著影响回应的相关性。例如，像“*告诉我关于狗的事*”这样的模糊提示可能会得到关于狗的通用回答。然而，更具体的提示，如“*狗的不同品种及其特点是什么？*”则更可能生成更详细和有益的信息。

+   **清晰与简洁**：重要的是要记住，虽然 Auto-GPT 是一个复杂的语言模型，但它不是人类。因此，最好避免使用术语或复杂语言，这可能会使模型感到困惑。相反，选择清晰、简洁的语言，模型能够轻松理解。

+   **上下文线索**：制作有效提示的一个关键技巧是提供足够的上下文线索。这意味着给模型足够的信息，以理解对话或任务的更广泛背景。例如，如果你要求模型编写一个发生在中世纪的故事，你可能会在给出实际提示之前，提供一些关于背景、角色和情节的上下文。这可以通过更详细的提示，或利用 Auto-GPT 的对话历史功能来实现。通过提供足够的上下文，你可以帮助模型生成更相关和连贯的回应。

+   **实验**：不要犹豫尝试不同的措辞和方法。提示措辞的细微变化有时会导致截然不同的回应。

接下来，我们将查看一些有效和无效提示的示例。

## 有效与无效提示的示例

为了更好地理解这些原则，让我们检查一些这样的提示示例：

+   **示例 1**：“告诉我一个笑话。”这个提示简单明了，Auto-GPT 很可能会回应一个笑话，展示其生成创意内容的能力。

+   **示例 2**：“生命的意义是什么？”这个提示哲学性强且宽泛，可能导致模糊或通用的回应，因为模型可能难以提供简洁而有意义的答案。

+   **示例 3**：“作为一个语言模型，解释机器学习的概念。”这个提示明确、具体，并提供了上下文，可能会导致对该概念的详细解释。

+   **示例 4**：“将以下文本翻译成法语：‘Hello, how are you?’”这个提示明确、具体，且以任务为导向，应该能得到正确的翻译。

总结来说，理解 Auto-GPT 如何生成提示的复杂性，并掌握有效提示的编写技巧，可以显著提升与模型的互动。记住，关键是要具体、使用简洁明了的语言、提供充分的上下文并进行实验。遵循这些指导方针，你将能够成为 Auto-GPT 的熟练用户。

# Auto-GPT 生成提示的概述

在这里，我们将了解 Auto-GPT 中的提示生成过程。

Auto-GPT 的提示生成过程是一个复杂的机制，涉及对输入上下文的深刻理解以及计算最可能的下一个标记。这个过程不仅仅是生成响应，还包括为对话设定舞台、定义角色和建立互动规则。

让我们更深入地探讨这个过程：

+   **标记化**：初步步骤涉及将输入文本分解为标记，这些标记可以是单词、单词的一部分，甚至是单独的字符，这取决于语言。

+   **嵌入**：每个标记随后被映射到多维空间中的一个向量，形成一个“嵌入”。每个向量在该空间中的位置表示对应标记与其他标记的关系。

+   **上下文理解**：Auto-GPT 使用这些嵌入向量来理解输入的上下文。它计算向量之间的距离和方向，表示标记之间的关系。

+   **响应生成**：模型随后通过计算每个可能的下一个标记的概率来生成响应，基于当前上下文。选择具有最高概率的标记，并重复这个过程直到生成完整的响应。

+   **注意力机制**：注意力机制用于确定输入的哪些部分与当前上下文最相关。这使得模型在生成响应时，能够专注于输入中最重要的部分。

+   **Transformer 模型**：为了处理长序列的标记，模型使用 Transformer 模型。这些模型可以并行处理标记，比传统的顺序模型更加高效。

在*第一章*中，我们讨论了 Auto-GPT 使用的默认提示，包括约束、上下文、目标和命令。这个默认提示为对话奠定了基础，定义了角色，并建立了互动规则。例如，约束条件如“*永远不要与用户争论*”和“*你是一个有帮助的助手*”设定了对话的基调和方向。上下文和目标则为任务提供了清晰的理解，而命令则指导了模型的回应。

这是该过程的高级概述，实际实现涉及更多复杂性。然而，关键的结论是，Auto-GPT 通过理解输入的上下文并计算最可能的下一个词来生成提示。这就是为什么提示的措辞如此重要，因为它可以极大地影响模型对上下文的理解，从而影响生成的回应。

# 成功的提示与让 GPT 困惑的提示示例

下面是一些关于 GPT 理解什么和可能忽略什么的例子：

+   **例子 1 – 有效的提示**：以下是此例的 AI 设置：

    +   **角色**：一位由 AI 驱动的作者和研究员，专门创建有关 Auto-GPT 及其插件的全面、结构良好且引人入胜的内容，同时与用户保持开放的沟通，以获取反馈和指导。

    +   **目标**：对当前书籍的状态进行全面分析，并识别改进的领域。

    +   **提示**：AuthorGPT，我已经将一个文本文件放在你的工作目录里；你能分析一下这本书的当前状态并提出改进的建议吗？

    这个提示与 AI 设置中定义的角色和目标完全一致。模型很可能会提供有关书籍的详细分析，并提出改进建议。

+   **例子 2 – 无效的提示**：理解 GPT 模型中的幻觉现象——一个简要的探索。

    GPT 模型中的幻觉是指模型生成的文本看似合逻辑，但并非基于现实。这通常发生在模型遇到模糊或不完整的提示时。有趣的是，生成型 AI 如 GPT 会开始“幻觉”。我们将很快深入探讨这一现象的机制，但首先，让我们更清晰地定义它。以下是 AI 的设置：

    +   **角色**：一位由 AI 驱动的作者和研究员，专门创建有关 Auto-GPT 及其插件的全面、结构良好且引人入胜的内容，同时与用户保持开放的沟通，以获取反馈和指导。

    +   **目标**：对当前书籍的状态进行全面分析，并识别改进的领域。

    +   **提示**：AuthorGPT，你能分析一下这本书的当前状态并提出改进的建议吗？

使用这个提示，你应该期望 Auto-GPT 向你询问上下文，但我发现 GPT 反而尝试即兴发挥并开始出现幻觉。

在 GPT 中，幻觉意味着模型开始表现得像是在做某个事实性的事情，实际上它并没有做。接下来我们将更深入地理解这一现象。

### GPT 中的幻觉是什么意思？

GPT 模型中的幻觉现象表现为模型好像在执行任务。其表现形式从为项目创建仅包含占位符的代码文件，到编造听起来符合语境的事实。例如，在某些情况下，模型可能会讨论一个与上下文关键词密切相关的话题。

GPT 等语言模型中的幻觉现象发生在模型生成的文本看起来合理，但与现实脱节。通常，这种现象是由于模型处理了模糊或不足的信息。

比如说，如果你请求模型编写一个关于虚构角色的故事，它可能会“幻觉”出关于该角色的生活、外貌或特征的细节。虽然这可能会产生创意和出人意料的结果，但也可能导致文本不合逻辑或与原始提示不相关。

举个例子，当我要求 Auto-GPT 开发一个基于`three.js`的 RPG 浏览器游戏时，它开始研究如何从一个不存在的 API 收集天气数据。这是因为我在切换到 GPT-4 之前使用过 GPT-3.5-turbo，导致处理过多的上下文信息。

Auto-GPT 的记忆有时会包含错误的事实或记忆。为了节省标记，它的记忆通过与 GPT 的*聊天完成*提示进行压缩。这可能会导致误解，特别是在总结已经总结过的文本并合并这些总结时。

困惑也可能来源于与当前上下文密切相关的标记。例如，如果编写一个天气 API 工具和一个基于 Web 的游戏都与“JavaScript”相关，模型可能会将天气 API 视为一个相关话题。由于 GPT-4 的先进参数和增强的精确度，这种困惑在 GPT-4 中较少出现。

### 混淆提示词及其对 AI 性能的影响

以下是一些 AI 设置，提示词可能会造成混淆，进而影响 AI 的表现：

+   **角色**：一位 AI 驱动的作者和研究员，专注于创建关于 Auto-GPT 及其插件的全面、结构良好且引人入胜的内容，同时保持与用户的开放沟通，以便提供反馈和指导。

+   **目标**：对当前书籍的状态进行深入分析，并识别需要改进的领域。

+   **提示词**：AuthorGPT，你能告诉我一个笑话吗？

    这个看似简单的提示词可能会让模型感到困惑，主要是因为它偏离了模型既定的角色和目标。要求一个笑话似乎与分析一本书并建议改进措施的上下文不符。

起初，模型可能会遵从并讲个笑话。然而，这种偏离可能会产生长期的影响。当模型将这种互动整合到其记忆总结中时，它可能会变得越来越困惑。这是因为 Auto-GPT 指示 GPT 尽可能保留信息，以防止话题转换、幻觉或之前的步骤被遗忘。

当 Auto-GPT 专注于某个特定任务，但遇到一个无关的提示时，会出现一个具体问题。例如，如果它收到一个与当前上下文无关的指令，模型可能会失去对之前动作的跟踪。它可能会陷入一个寻找与新输入相关的信息的循环，尝试将其与早期任务调和。因此，Auto-GPT 可能会开始将无关的笑话与其 Google 搜索结果混在一起，导致话题混乱。

这一场景突出了提示与 AI 定义的角色和目标对齐的重要性，以保持效果并避免混淆。

### 有效提示及其对 AI 性能的影响

以下是有效提示的 AI 设置及其对 AI 性能的优势：

+   **角色**：一名 AI 驱动的作者和研究员，专注于创建关于 Auto-GPT 及其插件的全面、结构良好且引人入胜的内容，同时与用户保持开放的沟通渠道，以便获取反馈和指导。

+   **目标**：制定一个全面的计划，创建任务列表，帮助你构建研究框架、每章的详细大纲以及各个部分的内容。

+   **提示**：AuthorGPT，你能帮我制定一个全面的计划，创建任务列表以结构化我的研究并为每一章拟定大纲吗？

这个提示明确、具体，并与 AI 设置中定义的角色和目标完美对齐。模型很可能会提供一个详细的任务列表创建计划和研究结构化方案。

### 混淆提示及其对 AI 性能的影响

下面是一个令人困惑的提示及其对 AI 性能的影响：

+   **角色**：一名 AI 驱动的作者和研究员，专注于创建关于 Auto-GPT 及其插件的全面、结构良好且引人入胜的内容，同时与用户保持开放的沟通渠道，以便获取反馈和指导。

+   **目标**：制定一个全面的计划，创建任务列表，帮助你构建研究框架、每章的详细大纲以及各个部分的内容。

+   **提示**：AuthorGPT，今天的天气怎么样？

### 理解差异

这个提示与 AI 的指定角色和目标形成鲜明对比。模型被配置为专注于任务列表创建和研究结构化，但面对与这些任务无关的提示时，陷入了困境。

这很可能会让模型感到困惑，因为它与定义的角色和目标不一致。模型可能会因为该提示不涉及创建计划或结构化研究而难以提供相关回答。

以下是一些潜在的 AI 行为场景：

+   尽管 Auto-GPT 可能通过查询天气来正确回应，或者通过解释天气不是主要关注点来轻描淡写地回避该问题，但它也可能感到困惑并放弃之前的任务，部分或完全丢失任务。这可能导致非常不准确的未来行为，甚至可能导致问题升级，导致 GPT 无法正确响应与其通信的 Auto-GPT 模块，从而引发致命错误。

+   该模型可能还会尝试将每个决策与当前的天气检查相关联，或者如果 Auto-GPT 没有正确压缩其记忆，它会在后续步骤中继续检查天气（例如，当它试图总结记忆以减少数据量时，可能会更加关注当前的天气数据）。

总结来说，为 Auto-GPT 设计有效的提示涉及将提示与 AI 设置中的定义角色和目标对齐。清晰、具体的提示更容易得到相关的回应，而不对齐的提示则可能会使模型产生困惑。

# 总结

在本章中，我们深入探讨了提示生成的复杂性以及 Auto-GPT 是如何生成提示的。我们首先定义了提示及其在塑造语言模型回应中的重要性。我们了解到，提示可以是问题、陈述、任务或任何我们希望传达给语言模型的文本。

我们还讨论了约束在为对话提供背景并指导模型回应中的作用。我们检查了具体的约束如何影响对话的语气、方向和伦理边界。

我们接着探讨了提示生成的技术细节，包括分词、嵌入、上下文理解、回应生成、注意力机制和变换器模型。我们了解到，模型通过理解输入的上下文并计算最可能的下一个标记来生成提示。

然后，我们提供了制定有效提示的技巧，强调了特定性、清晰度、上下文和实验的重要性。我们还查看了有效和令人困惑的提示示例，展示了在 AI 设置中与定义角色和目标对齐如何影响模型的回应。

最后，我们举了基于特定 AI 设置的提示示例，展示了如何通过有效的提示与定义的角色和目标对齐，而令人困惑的提示则没有。

总结来说，掌握提示生成并理解 Auto-GPT 是如何生成提示的，可以显著提高你与模型的互动效果。关键是要设计清晰、具体的提示，确保与模型的角色和目标对齐，提供充分的上下文，并勇于尝试。

在下一章中，我们将使用我们通过插件学习到的技能，学习如何自定义提示。
