

# ç¬¬äº”ç« ï¼šæ•°æ®æ­£åˆ™åŒ–

å°½ç®¡æœ‰è®¸å¤šæ­£åˆ™åŒ–æ–¹æ³•å¯ä¾›æ¨¡å‹ä½¿ç”¨ï¼ˆæ¯ä¸ªæ¨¡å‹éƒ½æœ‰ä¸€å¥—ç‹¬ç‰¹çš„è¶…å‚æ•°ï¼‰ï¼Œä½†æœ‰æ—¶æœ€æœ‰æ•ˆçš„æ­£åˆ™åŒ–æ–¹æ³•æ¥è‡ªäºæ•°æ®æœ¬èº«ã€‚äº‹å®ä¸Šï¼Œæœ‰æ—¶å³ä½¿æ˜¯æœ€å¼ºå¤§çš„æ¨¡å‹ï¼Œå¦‚æœæ•°æ®æ²¡æœ‰äº‹å…ˆæ­£ç¡®è½¬æ¢ï¼Œä¹Ÿæ— æ³•è¾¾åˆ°è‰¯å¥½çš„æ€§èƒ½ã€‚

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸€äº›æœ‰åŠ©äºé€šè¿‡æ•°æ®æ­£åˆ™åŒ–æ¨¡å‹çš„æ–¹æ³•ï¼š

+   å“ˆå¸Œé«˜åŸºæ•°ç‰¹å¾

+   èšåˆç‰¹å¾

+   å¯¹ä¸å¹³è¡¡æ•°æ®é›†è¿›è¡Œæ¬ é‡‡æ ·

+   å¯¹ä¸å¹³è¡¡æ•°æ®é›†è¿›è¡Œè¿‡é‡‡æ ·

+   ä½¿ç”¨ SMOTE å¯¹ä¸å¹³è¡¡æ•°æ®è¿›è¡Œé‡é‡‡æ ·

# æŠ€æœ¯è¦æ±‚

åœ¨æœ¬ç« ä¸­ï¼Œæ‚¨å°†å¯¹æ•°æ®åº”ç”¨å‡ ä¸ªæŠ€å·§ï¼Œå¹¶é€šè¿‡å‘½ä»¤è¡Œé‡é‡‡æ ·æ•°æ®é›†æˆ–ä¸‹è½½æ–°æ•°æ®ã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦ä»¥ä¸‹åº“ï¼š

+   NumPy

+   pandas

+   scikit-learn

+   imbalanced-learn

+   category_encoders

+   Kaggle API

# å“ˆå¸Œé«˜åŸºæ•°ç‰¹å¾

é«˜åŸºæ•°ç‰¹å¾æ˜¯å…·æœ‰è®¸å¤šå¯èƒ½å€¼çš„å®šæ€§ç‰¹å¾ã€‚é«˜åŸºæ•°ç‰¹å¾åœ¨è®¸å¤šåº”ç”¨ä¸­éƒ½ä¼šå‡ºç°ï¼Œä¾‹å¦‚å®¢æˆ·æ•°æ®åº“ä¸­çš„å›½å®¶ã€å¹¿å‘Šä¸­çš„æ‰‹æœºå‹å·ï¼Œæˆ– NLP åº”ç”¨ä¸­çš„è¯æ±‡ã€‚é«˜åŸºæ•°é—®é¢˜å¯èƒ½æ˜¯å¤šæ–¹é¢çš„ï¼šä¸ä»…å¯èƒ½å¯¼è‡´éå¸¸é«˜ç»´çš„æ•°æ®é›†ï¼Œè€Œä¸”éšç€è¶Šæ¥è¶Šå¤šçš„å€¼çš„å‡ºç°ï¼Œå®ƒä»¬è¿˜å¯èƒ½ä¸æ–­å‘å±•ã€‚äº‹å®ä¸Šï¼Œå³ä½¿å›½å®¶æ•°é‡æˆ–è¯æ±‡çš„æ•°æ®ç›¸å¯¹ç¨³å®šï¼Œæ¯å‘¨ï¼ˆå¦‚æœä¸æ˜¯æ¯å¤©ï¼‰ä¹Ÿä¼šæœ‰æ–°çš„æ‰‹æœºå‹å·å‡ºç°ã€‚

å“ˆå¸Œæ˜¯ä¸€ç§éå¸¸æµè¡Œä¸”æœ‰ç”¨çš„å¤„ç†è¿™ç±»é—®é¢˜çš„æ–¹æ³•ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£å®ƒæ˜¯ä»€ä¹ˆï¼Œä»¥åŠå¦‚ä½•åœ¨å®è·µä¸­ä½¿ç”¨å®ƒæ¥é¢„æµ‹å‘˜å·¥æ˜¯å¦ä¼šç¦»å¼€å…¬å¸ã€‚

## å…¥é—¨

å“ˆå¸Œæ˜¯è®¡ç®—æœºç§‘å­¦ä¸­éå¸¸æœ‰ç”¨çš„æŠ€å·§ï¼Œå¹¿æ³›åº”ç”¨äºå¯†ç å­¦æˆ–åŒºå—é“¾ç­‰é¢†åŸŸã€‚ä¾‹å¦‚ï¼Œå®ƒåœ¨å¤„ç†é«˜åŸºæ•°ç‰¹å¾æ—¶ä¹Ÿåœ¨æœºå™¨å­¦ä¹ ä¸­éå¸¸æœ‰ç”¨ã€‚å®ƒæœ¬èº«ä¸ä¸€å®šæœ‰åŠ©äºæ­£åˆ™åŒ–ï¼Œä½†æœ‰æ—¶å®ƒå¯èƒ½æ˜¯ä¸€ä¸ªå‰¯ä½œç”¨ã€‚

### ä»€ä¹ˆæ˜¯å“ˆå¸Œï¼Ÿ

å“ˆå¸Œé€šå¸¸åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç”¨äºå¤„ç†é«˜åŸºæ•°ç‰¹å¾ã€‚é«˜åŸºæ•°ç‰¹å¾å¾€å¾€å…·æœ‰è¶Šæ¥è¶Šå¤šçš„å¯èƒ½ç»“æœã€‚è¿™äº›ç»“æœå¯èƒ½åŒ…æ‹¬è¯¸å¦‚æ‰‹æœºå‹å·ã€è½¯ä»¶ç‰ˆæœ¬ã€å•†å“ ID ç­‰ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨ç‹¬çƒ­ç¼–ç ï¼ˆone-hot encodingï¼‰å¤„ç†é«˜åŸºæ•°ç‰¹å¾å¯èƒ½ä¼šå¯¼è‡´ä¸€äº›é—®é¢˜ï¼š

+   æ‰€éœ€ç©ºé—´ä¸æ˜¯å›ºå®šçš„ï¼Œæ— æ³•æ§åˆ¶

+   æˆ‘ä»¬éœ€è¦å¼„æ¸…æ¥šå¦‚ä½•ç¼–ç ä¸€ä¸ªæ–°å€¼

ä½¿ç”¨å“ˆå¸Œä»£æ›¿ç‹¬çƒ­ç¼–ç å¯ä»¥è§£å†³è¿™äº›é™åˆ¶ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨ä¸€ç§å“ˆå¸Œå‡½æ•°ï¼Œå®ƒå°†è¾“å…¥è½¬æ¢ä¸ºä¸€ä¸ªå¯æ§çš„è¾“å‡ºã€‚ä¸€ç§è‘—åçš„å“ˆå¸Œå‡½æ•°æ˜¯`md5`ã€‚å¦‚æœæˆ‘ä»¬å¯¹ä¸€äº›å­—ç¬¦ä¸²åº”ç”¨`md5`ï¼Œæˆ‘ä»¬å°†å¾—åˆ°å¦‚ä¸‹ç»“æœï¼š

```py
from hashlib import md5
print('hashing of "regularization" ->',
Â Â Â Â md5(b'regularization').hexdigest())
print('hashing of "regularized" ->',
Â Â Â Â md5(b'regularized').hexdigest())
print('hashing of "machine learning" ->',
Â Â Â Â md5(b'machine learning').hexdigest())
```

è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
hashing of "regularization" -> 04ef847b5e35b165c190ced9d91f65da
hashing of "regularized" -> bb02c45d3c38892065ff71198e8d2f89
hashing of "machine learning" -> e04d1bcee667afb8622501b9a4b4654d
```

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œå“ˆå¸Œå…·æœ‰å‡ ä¸ªæœ‰è¶£çš„ç‰¹æ€§ï¼š

+   æ— è®ºè¾“å…¥å¤§å°å¦‚ä½•ï¼Œè¾“å‡ºå¤§å°æ˜¯å›ºå®šçš„

+   ä¸¤ä¸ªç›¸ä¼¼çš„è¾“å…¥å¯èƒ½å¯¼è‡´éå¸¸ä¸åŒçš„è¾“å‡º

è¿™äº›å±æ€§ä½¿å¾—å“ˆå¸Œå‡½æ•°åœ¨ä¸é«˜åŸºæ•°ç‰¹å¾ä¸€èµ·ä½¿ç”¨æ—¶éå¸¸æœ‰æ•ˆã€‚æˆ‘ä»¬éœ€è¦åšçš„å°±æ˜¯è¿™æ ·ï¼š

1.  é€‰æ‹©ä¸€ä¸ªå“ˆå¸Œå‡½æ•°ã€‚

1.  å®šä¹‰è¾“å‡ºçš„é¢„æœŸç©ºé—´ç»´åº¦ã€‚

1.  ä½¿ç”¨è¯¥å‡½æ•°å¯¹æˆ‘ä»¬çš„ç‰¹å¾è¿›è¡Œç¼–ç ã€‚

å½“ç„¶ï¼Œå“ˆå¸Œä¹Ÿæœ‰ä¸€äº›ç¼ºç‚¹ï¼š

+   å¯èƒ½ä¼šå‘ç”Ÿå†²çªâ€”â€”ä¸¤ä¸ªä¸åŒçš„è¾“å…¥å¯èƒ½ä¼šæœ‰ç›¸åŒçš„è¾“å‡ºï¼ˆå³ä½¿è¿™ä¸ä¸€å®šä¼šå½±å“æ€§èƒ½ï¼Œé™¤éå†²çªéå¸¸ä¸¥é‡ï¼‰

+   æˆ‘ä»¬å¯èƒ½å¸Œæœ›ç›¸ä¼¼çš„è¾“å…¥æœ‰ç›¸ä¼¼çš„è¾“å‡ºï¼ˆä¸€ä¸ªç²¾å¿ƒé€‰æ‹©çš„å“ˆå¸Œå‡½æ•°å¯ä»¥å…·å¤‡è¿™æ ·çš„ç‰¹æ€§ï¼‰

### æ‰€éœ€çš„å®‰è£…

æˆ‘ä»¬éœ€è¦ä¸ºè¿™ä¸ªç¤ºä¾‹åšä¸€äº›å‡†å¤‡ã€‚ç”±äºæˆ‘ä»¬å°†ä¸‹è½½ä¸€ä¸ª Kaggle æ•°æ®é›†ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£… Kaggle APIï¼š

1.  ä½¿ç”¨`pip`å®‰è£…è¯¥åº“ï¼š

    ```py
    pip install kaggle
    ```

1.  å¦‚æœä½ è¿˜æ²¡æœ‰è¿™æ ·åšï¼Œåˆ›å»ºä¸€ä¸ª Kaggle è´¦æˆ·ï¼Œè®¿é—®[www.kaggle.com](https://www.kaggle.com)ã€‚

1.  è½¬åˆ°ä½ çš„ä¸ªäººèµ„æ–™é¡µé¢å¹¶é€šè¿‡ç‚¹å‡»`kaggle.json`æ–‡ä»¶å°†å…¶ä¸‹è½½åˆ°è®¡ç®—æœºï¼š

![å›¾ 5.1 â€“ Kaggle ç½‘ç«™æˆªå›¾](img/B19629_05_01.jpg)

å›¾ 5.1 â€“ Kaggle ç½‘ç«™æˆªå›¾

1.  ä½ éœ€è¦é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¡Œå°†æ–°ä¸‹è½½çš„`kaggle.json`æ–‡ä»¶ç§»åŠ¨åˆ°`~/.kaggle`ï¼š

    ```py
    mkdir ~/.kaggle && mv kaggle.json ~/.kaggle/.
    ```

1.  ç°åœ¨ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¡Œä¸‹è½½æ•°æ®é›†ï¼š

    ```py
    kaggle datasets download -d reddynitin/aug-train
    ```

1.  æˆ‘ä»¬ç°åœ¨åº”è¯¥æœ‰ä¸€ä¸ªåä¸º`aug-train.zip`çš„æ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«æˆ‘ä»¬å°†åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ä½¿ç”¨çš„æ•°æ®ã€‚æˆ‘ä»¬è¿˜éœ€è¦é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¡Œå®‰è£…`category_encoders`ã€`pandas`å’Œ`sklearn`åº“ï¼š

    ```py
    pip install category_encoders pandas scikit-learn.
    ```

## å¦‚ä½•åš...

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†åŠ è½½å¹¶å¿«é€Ÿå‡†å¤‡æ•°æ®é›†ï¼ˆå¿«é€Ÿæ„å‘³ç€æ›´å¤šçš„æ•°æ®å‡†å¤‡å¯èƒ½ä¼šå¸¦æ¥æ›´å¥½çš„ç»“æœï¼‰ï¼Œç„¶ååº”ç”¨é€»è¾‘å›å½’æ¨¡å‹æ¥å¤„ç†è¿™ä¸ªåˆ†ç±»ä»»åŠ¡ã€‚åœ¨æ‰€é€‰æ•°æ®é›†ä¸Šï¼Œ`city`ç‰¹å¾æœ‰ 123 ä¸ªå¯èƒ½çš„ç»“æœï¼Œå› æ­¤å¯ä»¥è®¤ä¸ºå®ƒæ˜¯ä¸€ä¸ªé«˜åŸºæ•°ç‰¹å¾ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯ä»¥åˆç†åœ°å‡è®¾ç”Ÿäº§æ•°æ®å¯èƒ½åŒ…å«æ›´å¤šçš„åŸå¸‚ï¼Œå› æ­¤å“ˆå¸ŒæŠ€å·§åœ¨è¿™é‡Œæ˜¯æœ‰æ„ä¹‰çš„ï¼š

1.  å¯¼å…¥æ‰€éœ€çš„æ¨¡å—ã€å‡½æ•°å’Œç±»ï¼š`pandas`ç”¨äºåŠ è½½æ•°æ®ï¼Œ`train_test_split`ç”¨äºåˆ’åˆ†æ•°æ®ï¼Œ`StandardScaler`ç”¨äºé‡æ–°ç¼©æ”¾å®šé‡ç‰¹å¾ï¼Œ`HashingEncoder`ç”¨äºç¼–ç å®šæ€§ç‰¹å¾ï¼Œ`LogisticRegression`ä½œä¸ºæ¨¡å‹ï¼š

    ```py
    Import numpy as np
    
    import pandas as pd
    
    from sklearn.model_selection import train_test_split
    
    from sklearn.preprocessing import StandardScaler, OneHotEncoder
    
    from category_encoders.hashing import HashingEncoder
    
    from sklearn.linear_model import LogisticRegression
    ```

1.  ä½¿ç”¨`pd.read_csv()`åŠ è½½æ•°æ®é›†ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸éœ€è¦å…ˆè§£å‹æ•°æ®é›†ï¼Œå› ä¸ºå‹ç¼©åŒ…åªåŒ…å«ä¸€ä¸ª CSV æ–‡ä»¶â€”â€”`pandas`ä¼šä¸ºæˆ‘ä»¬å¤„ç†è¿™ä¸€åˆ‡ï¼š

    ```py
    df = pd.read_csv('aug-train.zip')
    
    print('number of unique values for the feature city',
    
    Â Â Â Â df['city'].nunique())
    ```

æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ•°æ®é›†ä¸­çš„`city`ç‰¹å¾æœ‰`123`ä¸ªå¯èƒ½çš„å€¼ï¼š

```py
number of unique values for the feature city 123
```

1.  ç§»é™¤ä»»ä½•ç¼ºå¤±çš„æ•°æ®ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œé‡‡å–éå¸¸ç²—æš´çš„ç­–ç•¥ï¼šç§»é™¤æ‰€æœ‰ç¼ºå¤±æ•°æ®è¾ƒå¤šçš„ç‰¹å¾ï¼Œç„¶åç§»é™¤æ‰€æœ‰åŒ…å«å‰©ä½™ç¼ºå¤±æ•°æ®çš„è¡Œã€‚é€šå¸¸è¿™ä¸æ˜¯æ¨èçš„æ–¹æ³•ï¼Œå› ä¸ºæˆ‘ä»¬ä¸¢å¤±äº†å¤§é‡æ½œåœ¨æœ‰ç”¨çš„ä¿¡æ¯ã€‚ç”±äºå¤„ç†ç¼ºå¤±æ•°æ®ä¸åœ¨æ­¤è®¨è®ºèŒƒå›´å†…ï¼Œæˆ‘ä»¬å°†é‡‡å–è¿™ç§ç®€åŒ–çš„æ–¹æ³•ï¼š

    ```py
    df = df.drop(columns=['gender', 'major_discipline',
    
    Â Â Â Â 'company_size', 'company_type'])
    
    df = df.dropna()
    ```

1.  ä½¿ç”¨`train_test_split`å‡½æ•°å°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

    ```py
    X_train, X_test, y_train, y_test = train_test_split(
    
    Â Â Â Â df.drop(columns=['target']), df['target'],
    
    Â Â Â Â stratify=df['target'], test_size=0.2,
    
    Â Â Â Â random_state=0
    
    )
    ```

1.  é€‰æ‹©å¹¶é‡æ–°ç¼©æ”¾ä»»ä½•å®šé‡ç‰¹å¾ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æ ‡å‡†åŒ–å™¨æ¥é‡æ–°ç¼©æ”¾æ‰€é€‰çš„å®šé‡ç‰¹å¾ï¼Œä½†å…¶ä»–ä»»ä½•ç¼©æ”¾å™¨ä¹Ÿå¯èƒ½é€‚ç”¨ï¼š

    ```py
    quanti_feats = ['city_development_index', 'training_hours']
    
    # Instantiate the scaler
    
    scaler = StandardScaler()
    
    # Select quantitative features
    
    X_train_quanti = X_train[quanti_feats]
    
    X_test_quanti = X_test[quanti_feats]
    
    # Rescale quantitative features
    
    X_train_quanti = scaler.fit_transform(X_train_quanti)
    
    X_test_quanti = scaler.transform(X_test_quanti)
    ```

1.  é€‰æ‹©å¹¶å‡†å¤‡â€œå¸¸è§„â€å®šæ€§ç‰¹å¾ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`scikit-learn`ä¸­çš„ä¸€çƒ­ç¼–ç å™¨ï¼Œå°½ç®¡æˆ‘ä»¬ä¹Ÿå¯ä»¥å¯¹è¿™äº›ç‰¹å¾åº”ç”¨å“ˆå¸ŒæŠ€å·§ï¼š

    ```py
    quali_feats = ['relevent_experience',
    
    Â Â Â Â 'enrolled_university', 'education_level',
    
    Â Â Â Â 'experience', 'last_new_job']
    
    quali_feats = ['last_new_job']
    
    # Instantiate the one hot encoder
    
    encoder = OneHotEncoder()
    
    # Select qualitative features to one hot encode
    
    X_train_quali = X_train[quali_feats]
    
    X_test_quali = X_test[quali_feats]
    
    # Encode those features
    
    X_train_quali = encoder.fit_transform(
    
    Â Â Â Â X_train_quali).toarray()
    
    X_test_quali = encoder.transform(
    
    Â Â Â Â X_test_quali).toarray()
    ```

1.  ä½¿ç”¨å“ˆå¸Œå¯¹é«˜åŸºæ•°çš„`'city'`ç‰¹å¾è¿›è¡Œç¼–ç ã€‚ç”±äºå½“å‰è¯¥ç‰¹å¾æœ‰`123`ä¸ªå¯èƒ½çš„å€¼ï¼Œæˆ‘ä»¬å¯ä»¥åªä½¿ç”¨ 7 ä½æ¥ç¼–ç æ•´ä¸ªå¯èƒ½çš„å€¼ç©ºé—´ã€‚è¿™å°±æ˜¯`n_components=7`å‚æ•°æ‰€è¡¨ç¤ºçš„å†…å®¹ã€‚ä¸ºäº†å®‰å…¨èµ·è§ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶è®¾ç½®ä¸º 8 ä½æˆ–æ›´å¤šï¼Œä»¥è€ƒè™‘æ•°æ®ä¸­å¯èƒ½å‡ºç°çš„æ›´å¤šåŸå¸‚ï¼š

    ```py
    high_cardinality_feature = ['city']
    
    # Instantiate the hashing encoder
    
    hasher = HashingEncoder(n_components=7)
    
    # Encode the city feature with hashing
    
    X_train_hash = hasher.fit_transform(
    
    Â Â Â Â X_train[high_cardinality_feature])
    
    X_test_hash = hasher.fit_transform(
    
    Â Â Â Â X_test[high_cardinality_feature])
    
    # Display the result on the training set
    
    X_train_hash.head()
    ```

è¾“å‡ºç»“æœç±»ä¼¼äºä»¥ä¸‹å†…å®¹ï¼š

```py
Â Â col_0Â Â Â Â col_1Â Â Â Â col_2Â Â Â Â col_3Â Â Â Â col_4Â Â Â Â col_5Â Â Â Â col_6
18031Â Â Â Â Â 1Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â Â Â Â Â Â 0
16295Â Â Â Â Â 0Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â 1Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â Â Â Â Â Â 0
7679Â Â Â Â Â Â 0Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â 1Â Â Â Â Â Â Â Â Â Â Â Â Â Â 0
18154Â Â Â Â Â 0Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â 1Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â Â Â Â Â Â 0
10843Â Â Â Â Â 0Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â 0Â Â Â Â Â Â Â Â 1Â Â Â Â Â Â Â Â Â Â Â Â Â Â 0
```

æ³¨æ„

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ‰€æœ‰å€¼éƒ½è¢«ç¼–ç æˆä¸ƒåˆ—ï¼Œæ¶µç›–äº† 2â· = 128 ä¸ªå¯èƒ½çš„å€¼ã€‚

1.  å°†æ‰€æœ‰å‡†å¤‡å¥½çš„æ•°æ®è¿æ¥èµ·æ¥ï¼š

    ```py
    X_train = np.concatenate([X_train_quali,
    
    Â Â Â Â X_train_quanti, X_train_hash], 1)
    
    X_test = np.concatenate([X_test_quali,
    
    Â Â Â Â X_test_quanti, X_test_hash], 1)
    ```

1.  å®ä¾‹åŒ–å¹¶è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`scikit-learn`ä¸ºé€»è¾‘å›å½’æä¾›çš„é»˜è®¤è¶…å‚æ•°ï¼š

    ```py
    lr = LogisticRegression()
    
    lr.fit(X_train, y_train)
    ```

1.  ä½¿ç”¨`.``score()`æ–¹æ³•æ‰“å°è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å‡†ç¡®ç‡ï¼š

    ```py
    print('Accuracy train set:', lr.score(X_train,
    
    Â Â Â Â y_train))
    
    print('Accuracy test set:', lr.score(X_test, y_test))
    ```

è¾“å‡ºç»“æœç±»ä¼¼äºä»¥ä¸‹å†…å®¹ï¼š

```py
Accuracy train set: 0.7812087988342239
Accuracy test set: 0.7826810990840966
```

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡å¤§çº¦ä¸º 78%ï¼Œä¸”æ²¡æœ‰æ˜æ˜¾çš„è¿‡æ‹Ÿåˆã€‚

æ³¨æ„

å¯èƒ½æ·»åŠ ä¸€äº›ç‰¹å¾ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡ç‰¹å¾å·¥ç¨‹ï¼‰æœ‰åŠ©äºæ”¹å–„æ¨¡å‹ï¼Œå› ä¸ºç›®å‰æ¨¡å‹ä¼¼ä¹æ²¡æœ‰è¿‡æ‹Ÿåˆï¼Œä¸”åœ¨è‡ªèº«ä¸Šä¼¼ä¹æ²¡æœ‰å¤ªå¤šæå‡ç©ºé—´ã€‚

## å¦è§

+   ç±»åˆ«ç¼–ç å™¨åº“çš„å®˜æ–¹æ–‡æ¡£ï¼š[`contrib.scikit-learn.org/category_encoders/`](https://contrib.scikit-learn.org/category_encoders/)

+   å…³äºå“ˆå¸Œçš„ç±»åˆ«ç¼–ç å™¨é¡µé¢ï¼š[`contrib.scikit-learn.org/category_encoders/hashing.xhtml`](https://contrib.scikit-learn.org/category_encoders/hashing.xhtml)

# èšåˆç‰¹å¾

å½“ä½ å¤„ç†é«˜åŸºæ•°ç‰¹å¾æ—¶ï¼Œä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆæ˜¯å‡å°‘è¯¥ç‰¹å¾çš„å®é™…åŸºæ•°ã€‚è¿™é‡Œï¼Œèšåˆæ˜¯ä¸€ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½éå¸¸æœ‰æ•ˆã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è§£é‡Šèšåˆæ˜¯ä»€ä¹ˆï¼Œå¹¶è®¨è®ºä½•æ—¶åº”è¯¥ä½¿ç”¨å®ƒã€‚å®Œæˆè¿™äº›åï¼Œæˆ‘ä»¬å°†åº”ç”¨å®ƒã€‚

## å‡†å¤‡å·¥ä½œ

åœ¨å¤„ç†é«˜åŸºæ•°ç‰¹å¾æ—¶ï¼Œä¸€çƒ­ç¼–ç ä¼šå¯¼è‡´é«˜ç»´æ•°æ®é›†ã€‚ç”±äºæ‰€è°“çš„ç»´åº¦ç¾éš¾ï¼Œå³ä½¿æœ‰éå¸¸å¤§çš„è®­ç»ƒæ•°æ®é›†ï¼Œä¸€çƒ­ç¼–ç çš„é«˜åŸºæ•°ç‰¹å¾ä¹Ÿå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚å› æ­¤ï¼Œèšåˆæ˜¯ä¸€ç§é™ä½ä¸€çƒ­ç¼–ç ç»´åº¦çš„æ–¹æ³•ï¼Œä»è€Œé™ä½è¿‡æ‹Ÿåˆçš„é£é™©ã€‚

æœ‰å‡ ç§æ–¹æ³•å¯ä»¥è¿›è¡Œèšåˆã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«â€œæ‰‹æœºå‹å·â€ç‰¹å¾çš„å®¢æˆ·æ•°æ®åº“ï¼Œå…¶ä¸­åŒ…å«è®¸å¤šå¯èƒ½çš„æ‰‹æœºå‹å·ï¼ˆå³æ•°ç™¾ç§ï¼‰ã€‚è‡³å°‘æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥å¯¹è¿™ç§ç‰¹å¾è¿›è¡Œèšåˆï¼š

+   **æŒ‰å‡ºç°æ¦‚ç‡**ï¼šåœ¨æ•°æ®ä¸­å‡ºç°å°‘äº X%çš„ä»»ä½•æ¨¡å‹è¢«è§†ä¸ºâ€œå…¶ä»–â€

+   **æŒ‰ç»™å®šç›¸ä¼¼åº¦**ï¼šæˆ‘ä»¬å¯ä»¥æŒ‰ç”Ÿæˆã€å“ç‰Œç”šè‡³ä»·æ ¼æ¥èšåˆæ¨¡å‹

è¿™äº›æ–¹æ³•æœ‰å…¶ä¼˜ç¼ºç‚¹ï¼š

+   **ä¼˜ç‚¹**ï¼šæŒ‰å‡ºç°æ¬¡æ•°èšåˆç®€å•ã€å§‹ç»ˆæœ‰æ•ˆï¼Œå¹¶ä¸”ä¸éœ€è¦ä»»ä½•é¢†åŸŸçŸ¥è¯†

+   **ç¼ºç‚¹**ï¼šæŒ‰ç»™å®šç›¸ä¼¼åº¦èšåˆå¯èƒ½æ›´ç›¸å…³ï¼Œä½†éœ€è¦å¯¹ç‰¹å¾æœ‰ä¸€å®šäº†è§£ï¼Œè€Œè¿™äº›çŸ¥è¯†å¯èƒ½ä¸å¯å¾—ï¼Œæˆ–è€…å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœæœ‰æ•°ç™¾ä¸‡ä¸ªå€¼çš„è¯ï¼‰

æ³¨æ„

å½“ç‰¹å¾å­˜åœ¨é•¿å°¾åˆ†å¸ƒæ—¶ï¼Œèšåˆä¹Ÿæœ‰æ—¶å¾ˆæœ‰ç”¨ï¼Œè¿™æ„å‘³ç€ä¸€äº›å€¼å‡ºç°å¾—å¾ˆé¢‘ç¹ï¼Œè€Œè®¸å¤šå…¶ä»–å€¼åˆ™ä»…å¶å°”å‡ºç°ã€‚

åœ¨æœ¬é…æ–¹ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹ä¸€ä¸ªåŒ…å«è®¸å¤šåŸå¸‚ä½œä¸ºç‰¹å¾ä½†æ²¡æœ‰åŸå¸‚åç§°ä¿¡æ¯çš„æ•°æ®é›†è¿›è¡Œèšåˆã€‚è¿™å°†ä½¿æˆ‘ä»¬åªå‰©ä¸‹æŒ‰å‡ºç°æ¬¡æ•°èšåˆçš„é€‰æ‹©ã€‚æˆ‘ä»¬å°†é‡å¤ä½¿ç”¨å‰ä¸€ä¸ªé…æ–¹ä¸­çš„ç›¸åŒæ•°æ®é›†ï¼Œå› æ­¤æˆ‘ä»¬å°†éœ€è¦ Kaggle APIã€‚ä¸ºæ­¤ï¼Œè¯·å‚è€ƒå‰ä¸€ä¸ªé…æ–¹ã€‚é€šè¿‡ Kaggle APIï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ•°æ®é›†ï¼š

```py
kaggle datasets download -d reddynitin/aug-train
```

æˆ‘ä»¬è¿˜éœ€è¦`pandas`å’Œ`scikit-learn`åº“ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š

```py
pip install pandas scikit-learn.
```

## å¦‚ä½•æ“ä½œ...

æˆ‘ä»¬å°†ä½¿ç”¨ä¸å‰ä¸€ä¸ªé…æ–¹ç›¸åŒçš„æ•°æ®é›†ã€‚ä¸ºäº†å‡†å¤‡è¿™ä¸ªé…æ–¹ï¼Œæˆ‘ä»¬å°†åŸºäºæ•°æ®é›†ä¸­åŸå¸‚çš„å‡ºç°æ¬¡æ•°èšåˆåŸå¸‚ç‰¹å¾ï¼Œç„¶ååœ¨æ­¤æ•°æ®ä¸Šè®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹ï¼š

1.  å¯¼å…¥æ‰€éœ€çš„æ¨¡å—ã€å‡½æ•°å’Œç±»ï¼š`pandas`ç”¨äºåŠ è½½æ•°æ®ï¼Œ`train_test_split`ç”¨äºåˆ’åˆ†æ•°æ®ï¼Œ`StandardScaler`ç”¨äºé‡æ–°ç¼©æ”¾å®šé‡ç‰¹å¾ï¼Œ`OneHotEncoder`ç”¨äºç¼–ç å®šæ€§ç‰¹å¾ï¼Œ`LogisticRegression`ä½œä¸ºæ¨¡å‹ï¼š

    ```py
    Import numpy as np
    
    import pandas as pd
    
    from sklearn.model_selection import train_test_split
    
    from sklearn.preprocessing import OneHotEncoder, StandardScaler
    
    from sklearn.linear_model import LogisticRegression
    ```

1.  ä½¿ç”¨`pandas`åŠ è½½æ•°æ®é›†ã€‚æ— éœ€å…ˆè§£å‹æ–‡ä»¶â€”â€”è¿™ä¸€åˆ‡éƒ½ç”±`pandas`å¤„ç†ï¼š

    ```py
    df = pd.read_csv('aug-train.zip')
    ```

1.  åˆ é™¤ä»»ä½•ç¼ºå¤±æ•°æ®ã€‚å°±åƒåœ¨å‰ä¸€ä¸ªé…æ–¹ä¸­ä¸€æ ·ï¼Œæˆ‘ä»¬å°†é‡‡ç”¨ä¸€ä¸ªç®€å•çš„ç­–ç•¥ï¼Œåˆ é™¤æ‰€æœ‰æœ‰å¤§é‡ç¼ºå¤±æ•°æ®çš„ç‰¹å¾ï¼Œç„¶ååˆ é™¤åŒ…å«ç¼ºå¤±æ•°æ®çš„è¡Œï¼š

    ```py
    df = df.drop(columns=['gender', 'major_discipline',
    
    Â Â Â Â 'company_size', 'company_type'])
    
    df = df.dropna()
    ```

1.  ä½¿ç”¨`train_test_split`å‡½æ•°å°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

    ```py
    X_train, X_test, y_train, y_test = train_test_split(
    
    Â Â Â Â df.drop(columns=['target']), df['target'],
    
    Â Â Â Â stratify=df['target'], test_size=0.2,
    
    Â Â Â Â random_state=0
    
    )
    ```

1.  ä½¿ç”¨`scikit-learn`æä¾›çš„æ ‡å‡†ç¼©æ”¾å™¨å¯¹ä»»ä½•å®šé‡ç‰¹å¾è¿›è¡Œé‡æ–°ç¼©æ”¾ï¼š

    ```py
    quanti_feats = ['city_development_index',
    
    Â Â Â Â 'training_hours']
    
    scaler = StandardScaler()
    
    X_train_quanti = X_train[quanti_feats]
    
    X_test_quanti = X_test[quanti_feats]
    
    X_train_quanti = scaler.fit_transform(X_train_quanti)
    
    X_test_quanti = scaler.transform(X_test_quanti)
    ```

1.  ç°åœ¨ï¼Œæˆ‘ä»¬å¿…é¡»å¯¹`city`ç‰¹å¾è¿›è¡Œèšåˆï¼š

    ```py
    # Get only cities above threshold
    
    threshold = 0.1
    
    kept_cities = X_train['city'].value_counts(
    
    Â Â Â Â normalize=True)[X_train['city'].value_counts(
    
    Â Â Â Â normalize=True) > threshold].index
    
    # Update all cities below threshold as 'other'
    
    X_train.loc[~X_train['city'].isin(kept_cities),
    
    Â Â Â Â 'city'] = 'other'
    
    X_test.loc[~X_test['city'].isin(kept_cities),
    
    Â Â Â Â 'city'] = 'other'
    ```

1.  ä½¿ç”¨ç‹¬çƒ­ç¼–ç å‡†å¤‡å®šæ€§ç‰¹å¾ï¼ŒåŒ…æ‹¬æ–°èšåˆçš„`city`ç‰¹å¾ï¼š

    ```py
    # Get qualitative features
    
    quali_feats = ['city', 'relevent_experience',
    
    Â Â Â Â 'enrolled_university', 'education_level',
    
    Â Â Â Â 'experience', 'last_new_job']
    
    X_train_quali = X_train[quali_feats]
    
    X_test_quali = X_test[quali_feats]
    
    # Instantiate the one hot encoder
    
    encoder = OneHotEncoder()
    
    # Apply one hot encoding
    
    X_train_quali = encoder.fit_transform(
    
    Â Â Â Â X_train_quali).toarray()
    
    X_test_quali = encoder.transform(
    
    Â Â Â Â X_test_quali).toarray()
    ```

1.  å°†å®šé‡ç‰¹å¾å’Œå®šæ€§ç‰¹å¾é‡æ–°è¿æ¥åœ¨ä¸€èµ·ï¼š

    ```py
    X_train = np.concatenate([X_train_quali,
    
    Â Â Â Â X_train_quanti], 1)
    
    X_test = np.concatenate([X_test_quali, X_test_quanti], 1)
    ```

1.  å®ä¾‹åŒ–å¹¶è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä¿æŒæ¨¡å‹çš„é»˜è®¤è¶…å‚æ•°ï¼š

    ```py
    lr = LogisticRegression()
    
    lr.fit(X_train, y_train)
    ```

1.  è®¡ç®—å¹¶æ‰“å°æ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡ï¼š

    ```py
    print('Accuracy train set:', lr.score(X_train, y_train))
    
    print('Accuracy test set:', lr.score(X_test, y_test))
    ```

è¾“å‡ºç»“æœå°†å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
Accuracy train set: 0.7805842759003538
Accuracy test set: 0.774909797391063
```

æ³¨æ„

å¯¹äºè¿™ä¸ªç‰¹å®šæƒ…å†µï¼Œèšåˆä¼¼ä¹å¹¶æ²¡æœ‰æ˜¾è‘—å¸®åŠ©æˆ‘ä»¬è·å¾—æ›´å¼ºçš„ç»“æœï¼Œä½†è‡³å°‘å®ƒå¸®åŠ©æ¨¡å‹å˜å¾—æ›´ä¸å®¹æ˜“é¢„æµ‹ï¼Œä¸”å¯¹äºæ–°åŸå¸‚æ›´å…·é²æ£’æ€§ã€‚

## è¿˜æœ‰æ›´å¤š...

ç”±äºèšåˆä»£ç å¯èƒ½çœ‹èµ·æ¥æœ‰äº›å¤æ‚ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹æˆ‘ä»¬åšäº†ä»€ä¹ˆã€‚

å› æ­¤ï¼Œæˆ‘ä»¬æœ‰`city`ç‰¹å¾ï¼Œå®ƒæœ‰è®¸å¤šå¯èƒ½çš„å€¼ï¼›æ¯ä¸ªå€¼åœ¨è®­ç»ƒé›†ä¸­çš„é¢‘ç‡éƒ½å¯ä»¥é€šè¿‡`.value_counts(normalize=True)`æ–¹æ³•è®¡ç®—ï¼š

```py
df['city'].value_counts(normalize=True)
```

è¿™å°†äº§ç”Ÿä»¥ä¸‹è¾“å‡ºï¼š

```py
city_103Â Â Â Â Â Â Â Â Â 0.232819
city_21Â Â Â Â Â Â Â Â Â Â Â 0.136227
city_16Â Â Â Â Â Â Â Â Â Â Â 0.081659
city_114Â Â Â Â Â Â Â Â Â 0.069613
city_160Â Â Â Â Â Â Â Â Â 0.045354
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ...
city_111Â Â Â Â Â Â Â Â Â 0.000167
city_129Â Â Â Â Â Â Â Â Â 0.000111
city_8Â Â Â Â Â Â Â Â Â Â Â Â Â Â 0.000111
city_140Â Â Â Â Â Â Â Â Â 0.000056
city_171Â Â Â Â Â Â Â Â Â 0.000056
Name: city, Length: 123, dtype: float64
```

çœ‹èµ·æ¥ï¼Œåœ¨æ•´ä¸ªæ•°æ®é›†ä¸­ï¼Œ`city_103`çš„å€¼å‡ºç°çš„é¢‘ç‡è¶…è¿‡ 23%ï¼Œè€Œå…¶ä»–å€¼ï¼Œå¦‚`city_111`ï¼Œå‡ºç°çš„é¢‘ç‡ä¸åˆ° 1%ã€‚æˆ‘ä»¬åªéœ€å¯¹è¿™äº›å€¼åº”ç”¨ä¸€ä¸ªé˜ˆå€¼ï¼Œä»¥ä¾¿è·å–å‡ºç°é¢‘ç‡è¶…è¿‡ç»™å®šé˜ˆå€¼çš„åŸå¸‚åˆ—è¡¨ï¼š

```py
df['city'].value_counts(normalize=True) > 0.05
```

è¿™å°†äº§ç”Ÿä»¥ä¸‹è¾“å‡ºï¼š

```py
city_103Â Â Â Â Â Â Â Â Â Â Â True
city_21Â Â Â Â Â Â Â Â Â Â Â Â Â Â True
city_16Â Â Â Â Â Â Â Â Â Â Â Â Â Â True
city_114Â Â Â Â Â Â Â Â Â Â Â True
city_160Â Â Â Â Â Â Â Â Â False
Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ...
city_111Â Â Â Â Â Â Â Â Â False
city_129Â Â Â Â Â Â Â Â Â False
city_8Â Â Â Â Â Â Â Â Â Â Â Â Â Â False
city_140Â Â Â Â Â Â Â Â Â False
city_171Â Â Â Â Â Â Â Â Â False
Name: city, Length: 123, dtype: bool
```

ç°åœ¨ï¼Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯è·å–æ‰€æœ‰çœŸå®å€¼çš„ç´¢å¼•ï¼ˆå³åŸå¸‚åç§°ï¼‰ã€‚è¿™æ­£æ˜¯æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹å®Œæ•´è¡Œæ¥å®Œæˆçš„ï¼š

```py
kept_cities = df['city'].value_counts(normalize=True)[
Â Â Â Â df['city'].value_counts(normalize=True) > 0.05].index
kept_cities
```

è¿™å°†æ˜¾ç¤ºä»¥ä¸‹è¾“å‡ºï¼š

```py
Index(['city_103', 'city_21', 'city_16', 'city_114'], dtype='object')
```

æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œè¿™å°†è¿”å›å‡ºç°é¢‘ç‡è¶…è¿‡é˜ˆå€¼çš„åŸå¸‚åˆ—è¡¨ã€‚

# å¯¹ä¸å¹³è¡¡æ•°æ®é›†è¿›è¡Œæ¬ é‡‡æ ·

æœºå™¨å­¦ä¹ ä¸­çš„å…¸å‹æƒ…å†µæ˜¯æˆ‘ä»¬æ‰€ç§°çš„â€œä¸å¹³è¡¡æ•°æ®é›†â€ã€‚ä¸å¹³è¡¡æ•°æ®é›†æ„å‘³ç€å¯¹äºæŸä¸€ç±»åˆ«ï¼ŒæŸäº›å®ä¾‹æ¯”å…¶ä»–å®ä¾‹æ›´å¯èƒ½å‡ºç°ï¼Œä»è€Œå¯¼è‡´æ•°æ®çš„ä¸å¹³è¡¡ã€‚ä¸å¹³è¡¡æ•°æ®é›†çš„æ¡ˆä¾‹æœ‰å¾ˆå¤šï¼šåŒ»å­¦ä¸­çš„ç½•è§ç–¾ç—…ã€å®¢æˆ·è¡Œä¸ºç­‰ã€‚

åœ¨æœ¬æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å°†æå‡ºä¸€ç§å¯èƒ½çš„å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†çš„æ–¹æ³•ï¼šæ¬ é‡‡æ ·ã€‚åœ¨è§£é‡Šè¿™ä¸ªè¿‡ç¨‹åï¼Œæˆ‘ä»¬å°†å…¶åº”ç”¨äºä¸€ä¸ªä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›†ã€‚

## å‡†å¤‡å°±ç»ª

ä¸å¹³è¡¡æ•°æ®çš„é—®é¢˜åœ¨äºï¼Œå®ƒå¯èƒ½ä¼šåå€šæœºå™¨å­¦ä¹ æ¨¡å‹çš„ç»“æœã€‚å‡è®¾æˆ‘ä»¬æ­£åœ¨è¿›è¡Œä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼Œæ£€æµ‹æ•°æ®é›†ä¸­ä»…å  1%çš„ç½•è§ç–¾ç—…ã€‚æ­¤ç±»æ•°æ®çš„ä¸€ä¸ªå¸¸è§é™·é˜±æ˜¯æ¨¡å‹æ€»æ˜¯é¢„æµ‹ä¸ºå¥åº·çŠ¶æ€ï¼Œå› ä¸ºè¿™æ ·å®ƒä»ç„¶å¯ä»¥è¾¾åˆ° 99%çš„å‡†ç¡®ç‡ã€‚å› æ­¤ï¼Œæœºå™¨å­¦ä¹ æ¨¡å‹å¾ˆå¯èƒ½ä¼šæœ€å°åŒ–å…¶æŸå¤±ã€‚

æ³¨æ„

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…¶ä»–æŒ‡æ ‡ï¼Œå¦‚ F1 åˆ†æ•°æˆ–**ROC æ›²çº¿ä¸‹é¢ç§¯**ï¼ˆ**ROC AUC**ï¼‰ï¼Œé€šå¸¸æ›´ä¸ºç›¸å…³ã€‚

é˜²æ­¢è¿™ç§æƒ…å†µå‘ç”Ÿçš„ä¸€ç§æ–¹æ³•æ˜¯å¯¹æ•°æ®é›†è¿›è¡Œæ¬ é‡‡æ ·ã€‚æ›´å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ é™¤éƒ¨åˆ†æ ·æœ¬æ¥å¯¹è¿‡åº¦ä»£è¡¨çš„ç±»åˆ«è¿›è¡Œæ¬ é‡‡æ ·ï¼š

+   æˆ‘ä»¬ä¿ç•™æ‰€æœ‰æ¬ ä»£è¡¨ç±»åˆ«çš„æ ·æœ¬

+   æˆ‘ä»¬ä»…ä¿ç•™è¿‡åº¦ä»£è¡¨ç±»åˆ«çš„å­æ ·æœ¬

é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å¯ä»¥äººä¸ºåœ°å¹³è¡¡æ•°æ®é›†ï¼Œé¿å…ä¸å¹³è¡¡æ•°æ®é›†çš„é™·é˜±ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªç”±ä»¥ä¸‹å±æ€§ç»„æˆçš„æ•°æ®é›†ï¼š

+   100 ä¸ªå¸¦æœ‰ç–¾ç—…çš„æ ·æœ¬

+   9,900 ä¸ªæ²¡æœ‰ç–¾ç—…çš„æ ·æœ¬

å®Œå…¨å¹³è¡¡çš„æ¬ é‡‡æ ·å°†ç»™æˆ‘ä»¬ä»¥ä¸‹æ•°æ®é›†ä¸­çš„ç»“æœï¼š

+   100 ä¸ªå¸¦æœ‰ç–¾ç—…çš„æ ·æœ¬

+   100 ä¸ªéšæœºé€‰æ‹©çš„æ²¡æœ‰ç–¾ç—…çš„æ ·æœ¬

å½“ç„¶ï¼Œç¼ºç‚¹æ˜¯æˆ‘ä»¬åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ä¼šä¸¢å¤±å¤§é‡æ•°æ®ã€‚

å¯¹äºè¿™ä¸ªæ–¹æ³•ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸‹è½½æ•°æ®é›†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Kaggle APIï¼ˆè¯·å‚è€ƒ*å“ˆå¸Œé«˜åŸºæ•°ç‰¹å¾*çš„æ–¹æ³•äº†è§£å¦‚ä½•å®‰è£…ï¼‰ã€‚å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¡Œä¸‹è½½æ•°æ®é›†ï¼š

```py
kaggle datasets download -d mlg-ulb/creditcardfraud
```

è¿˜éœ€è¦ä»¥ä¸‹åº“ï¼š`pandas` ç”¨äºåŠ è½½æ•°æ®ï¼Œ`scikit-learn` ç”¨äºå»ºæ¨¡ï¼Œ`matplotlib` ç”¨äºæ˜¾ç¤ºæ•°æ®ï¼Œ`imbalanced-learn` ç”¨äºæ¬ é‡‡æ ·ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¡Œå®‰è£…ï¼š

```py
pip install pandas scikit-learn matplotlib imbalanced-learn
```

## å¦‚ä½•æ“ä½œ...

åœ¨æœ¬ç¯‡æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹ä¿¡ç”¨å¡æ¬ºè¯ˆæ•°æ®é›†åº”ç”¨æ¬ é‡‡æ ·ã€‚è¿™æ˜¯ä¸€ä¸ªç›¸å½“æç«¯çš„ç±»åˆ«ä¸å¹³è¡¡æ•°æ®é›†çš„ä¾‹å­ï¼Œå› ä¸ºå…¶ä¸­ä»…çº¦ 0.18% çš„æ ·æœ¬æ˜¯æ­£ç±»ï¼š

1.  å¯¼å…¥æ‰€éœ€çš„æ¨¡å—ã€ç±»å’Œå‡½æ•°ï¼š

    +   `pandas` ç”¨äºæ•°æ®åŠ è½½å’Œå¤„ç†

    +   `train_test_split` ç”¨äºæ•°æ®æ‹†åˆ†

    +   `StandardScaler` ç”¨äºæ•°æ®é‡æ–°ç¼©æ”¾ï¼ˆæ•°æ®é›†ä»…åŒ…å«å®šé‡ç‰¹å¾ï¼‰

    +   `RandomUnderSampler` ç”¨äºæ¬ é‡‡æ ·

    +   `LogisticRegression` ç”¨äºå»ºæ¨¡

    +   `roc_auc_score` ç”¨äºæ˜¾ç¤º ROC å’Œ ROC AUC è®¡ç®—ï¼š

        ```py
        import pandas as pd
        ```

        ```py
        import matplotlib.pyplot as plt
        ```

        ```py
        from sklearn.model_selection import train_test_split
        ```

        ```py
        from sklearn.preprocessing import StandardScaler
        ```

        ```py
        from imblearn.under_sampling import RandomUnderSampler
        ```

        ```py
        from sklearn.linear_model import LogisticRegression
        ```

        ```py
        from sklearn.metrics import roc_auc_score
        ```

1.  ä½¿ç”¨ `pandas` åŠ è½½æ•°æ®ã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥åŠ è½½ ZIP æ–‡ä»¶ã€‚æˆ‘ä»¬è¿˜å°†æ˜¾ç¤ºæ¯ä¸ªæ ‡ç­¾çš„ç›¸å¯¹æ•°é‡ï¼šæˆ‘ä»¬æœ‰å¤§çº¦ 99.8% çš„æ­£å¸¸äº¤æ˜“ï¼Œè€Œæ¬ºè¯ˆäº¤æ˜“çš„æ¯”ä¾‹ä¸åˆ° 0.18%ï¼š

    ```py
    df = pd.read_csv('creditcardfraud.zip')
    
    df['Class'].value_counts(normalize=True)
    ```

è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
0Â Â Â Â Â Â Â Â Â 0.998273
1Â Â Â Â Â Â Â Â Â 0.001727
Name: Class, dtype: float64
```

1.  å°†æ•°æ®æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¡®ä¿æ ‡ç­¾çš„åˆ†å±‚éå¸¸å…³é”®ï¼š

    ```py
    X_train, X_test, y_train, y_test = train_test_split(
    
    Â Â Â Â df.drop(columns=['Class']), df['Class'],
    
    Â Â Â Â test_size=0.2, random_state=0,
    
    Â Â Â Â stratify=df['Class'])
    ```

1.  åº”ç”¨éšæœºæ¬ é‡‡æ ·ï¼Œä½¿ç”¨æœ€å¤š 10% çš„é‡‡æ ·ç­–ç•¥ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¿…é¡»å¯¹è¿‡åº¦è¡¨ç¤ºçš„ç±»åˆ«è¿›è¡Œæ¬ é‡‡æ ·ï¼Œç›´åˆ°ç±»åˆ«å¹³è¡¡è¾¾åˆ° 10 æ¯” 1 çš„æ¯”ä¾‹ã€‚æˆ‘ä»¬å¯ä»¥åšåˆ° 1 æ¯” 1 çš„æ¯”ä¾‹ï¼Œä½†è¿™ä¼šå¯¼è‡´æ›´å¤šçš„æ•°æ®ä¸¢å¤±ã€‚æ­¤æ¯”ä¾‹ç”± `sampling_strategy=0.1` å‚æ•°å®šä¹‰ã€‚æˆ‘ä»¬è¿˜å¿…é¡»è®¾ç½®éšæœºçŠ¶æ€ä»¥ç¡®ä¿å¯é‡å¤æ€§ï¼š

    ```py
    # Instantiate the object with a 10% strategy
    
    rus = RandomUnderSampler(sampling_strategy=0.1,
    
    Â Â Â Â random_state=0)
    
    # Undersample the train dataset
    
    X_train, y_train = rus.fit_resample(X_train, y_train)
    
    # Check the balance
    
    y_train.value_counts()
    ```

è¿™å°†ç»™æˆ‘ä»¬ä»¥ä¸‹è¾“å‡ºï¼š

```py
0Â Â Â Â Â Â Â Â Â 3940
1Â Â Â Â Â Â Â Â Â Â Â 394
Name: Class, dtype: int64
```

åœ¨æ¬ é‡‡æ ·åï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°äº† `3940` ä¸ªæ­£å¸¸äº¤æ˜“æ ·æœ¬ï¼Œä¸ `394` ä¸ªæ¬ºè¯ˆäº¤æ˜“æ ·æœ¬ç›¸æ¯”ã€‚

1.  ä½¿ç”¨æ ‡å‡†ç¼©æ”¾å™¨é‡æ–°ç¼©æ”¾æ•°æ®ï¼š

    ```py
    # Scale the data
    
    scaler = StandardScaler()
    
    X_train = scaler.fit_transform(X_train)
    
    X_test = scaler.transform(X_test)
    ```

æ³¨æ„

å¯ä»¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é‡é‡‡æ ·ä¹‹å‰åº”ç”¨é‡æ–°ç¼©æ”¾ã€‚è¿™å°†ä½¿è¿‡åº¦è¡¨ç¤ºçš„ç±»åˆ«åœ¨é‡æ–°ç¼©æ”¾æ—¶æ›´å…·æƒé‡ï¼Œä½†ä¸ä¼šè¢«è§†ä¸ºæ•°æ®æ³„éœ²ã€‚

1.  åœ¨è®­ç»ƒé›†ä¸Šå®ä¾‹åŒ–å¹¶è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ï¼š

    ```py
    lr = LogisticRegression()
    
    lr.fit(X_train, y_train)
    ```

1.  è®¡ç®—è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ ROC AUCã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦è·å–æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡ï¼Œè¿™å¯ä»¥é€šè¿‡ `predict_proba()` æ–¹æ³•è·å¾—ï¼Œå¹¶ä¸”éœ€è¦ä½¿ç”¨å¯¼å…¥çš„ `roc_auc_score()` å‡½æ•°ï¼š

    ```py
    # Get the probas
    
    y_train_proba = lr.predict_proba(X_train)[:, 1]
    
    y_test_proba = lr.predict_proba(X_test)[:, 1]
    
    # Display the ROC AUC
    
    print('ROC AUC training set:', roc_auc_score(y_train,
    
    Â Â Â Â y_train_proba))
    
    print('ROC AUC test set:', roc_auc_score(y_test,
    
    Â Â Â Â y_test_proba))
    ```

è¿™å°†è¿”å›ä»¥ä¸‹ç»“æœï¼š

```py
ROC AUC training set: 0.9875041871730784
ROC AUC test set: 0.9731067071595099
```

æˆ‘ä»¬åœ¨æµ‹è¯•é›†ä¸Šè·å¾—äº†å¤§çº¦ 97% çš„ ROC AUCï¼Œåœ¨è®­ç»ƒé›†ä¸Šæ¥è¿‘ 99%ã€‚

## è¿˜æœ‰æ›´å¤šå†…å®¹...

å¯é€‰åœ°ï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ ROC æ›²çº¿ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `scikit-learn` ä¸­çš„ `roc_curve()` å‡½æ•°ï¼š

```py
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
# Display the ROC curve
fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)
fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)
plt.plot(fpr_test, tpr_test, label='test')
plt.plot(fpr_train, tpr_train, label='train')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.legend()
plt.show()
```

![å›¾ 5.2 â€“ è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ ROC æ›²çº¿ã€‚å›¾å½¢ç”±ä»£ç ç”Ÿæˆ](img/B19629_05_02.jpg)

å›¾ 5.2 â€“ è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ ROC æ›²çº¿ã€‚å›¾å½¢ç”±ä»£ç ç”Ÿæˆ

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œå°½ç®¡è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ ROC AUC éå¸¸ç›¸ä¼¼ï¼Œä½†æµ‹è¯•é›†çš„æ›²çº¿ç•¥ä½ã€‚è¿™æ„å‘³ç€ï¼Œæ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œæ¨¡å‹ç¨å¾®å‡ºç°äº†è¿‡æ‹Ÿåˆã€‚

è¯·æ³¨æ„ï¼Œå¾®è°ƒ `sampling_strategy` å¯èƒ½æœ‰åŠ©äºè·å¾—æ›´å¥½çš„ç»“æœã€‚

æ³¨æ„

ä¸ºäº†åŒæ—¶ä¼˜åŒ–é‡‡æ ·ç­–ç•¥å’Œæ¨¡å‹è¶…å‚æ•°ï¼Œå¯ä»¥ä½¿ç”¨ scikit-learn çš„`Pipeline`ç±»ã€‚

## å¦è§

+   `RandomUnderSampler`çš„æ–‡æ¡£ï¼š[`imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml`](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml)

+   `Pipeline`çš„æ–‡æ¡£ï¼š[`scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.xhtml`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.xhtml)

+   è¿™æ˜¯ä¸€ä¸ªå…³äºåŒæ­¥éª¤ç®¡é“è¶…å‚æ•°ä¼˜åŒ–çš„ä¼˜ç§€ä»£ç ç¤ºä¾‹ï¼š[`scikit-learn.org/stable/tutorial/statistical_inference/putting_together.xhtml`](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.xhtml%0D)

# å¯¹ä¸å¹³è¡¡æ•°æ®é›†è¿›è¡Œè¿‡é‡‡æ ·

å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†çš„å¦ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯éšæœºè¿‡é‡‡æ ·ã€‚è¿™æ˜¯éšæœºæ¬ é‡‡æ ·çš„å¯¹ç«‹é¢ã€‚åœ¨æœ¬é…æ–¹ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•åœ¨ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›†ä¸Šä½¿ç”¨å®ƒã€‚

## å‡†å¤‡å°±ç»ª

éšæœºè¿‡é‡‡æ ·å¯ä»¥çœ‹ä½œæ˜¯éšæœºæ¬ é‡‡æ ·çš„å¯¹ç«‹é¢ï¼šå…¶æ€æƒ³æ˜¯å¤åˆ¶æ¬ ä»£è¡¨ç±»åˆ«çš„æ•°æ®æ ·æœ¬ï¼Œä»¥é‡æ–°å¹³è¡¡æ•°æ®é›†ã€‚

å°±åƒå‰é¢çš„é…æ–¹ä¸€æ ·ï¼Œå‡è®¾ä¸€ä¸ª 1%-99%ä¸å¹³è¡¡çš„æ•°æ®é›†ï¼Œå…¶ä¸­åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

+   100 ä¸ªæœ‰ç–¾ç—…æ ·æœ¬

+   9,900 ä¸ªæ— ç–¾ç—…æ ·æœ¬

ä¸ºäº†ä½¿ç”¨ 1/1 ç­–ç•¥ï¼ˆå³å®Œå…¨å¹³è¡¡çš„æ•°æ®é›†ï¼‰å¯¹è¿™ä¸ªæ•°æ®é›†è¿›è¡Œè¿‡é‡‡æ ·ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ¯ä¸ªç–¾ç—…ç±»åˆ«çš„æ ·æœ¬è¿›è¡Œ 99 æ¬¡å¤åˆ¶ã€‚å› æ­¤ï¼Œè¿‡é‡‡æ ·åçš„æ•°æ®é›†å°†éœ€è¦åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

+   9,900 ä¸ªæœ‰ç–¾ç—…æ ·æœ¬ï¼ˆ100 ä¸ªåŸå§‹æ ·æœ¬å¹³å‡å¤åˆ¶ 99 æ¬¡ï¼‰

+   9,900 ä¸ªæ— ç–¾ç—…æ ·æœ¬

æˆ‘ä»¬å¯ä»¥è½»æ¾çŒœæµ‹è¿™ç§æ–¹æ³•çš„ä¼˜ç¼ºç‚¹ï¼š

+   **ä¼˜ç‚¹**ï¼šä¸æ¬ é‡‡æ ·ä¸åŒï¼Œæˆ‘ä»¬ä¸ä¼šä¸¢å¤±è¿‡ä»£è¡¨ç±»åˆ«çš„ä»»ä½•æ•°æ®ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥åœ¨æˆ‘ä»¬æ‹¥æœ‰çš„å®Œæ•´æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒ

+   **ç¼ºç‚¹**ï¼šæˆ‘ä»¬å¯èƒ½ä¼šæœ‰å¾ˆå¤šæ¬ ä»£è¡¨ç±»åˆ«çš„é‡å¤æ ·æœ¬ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´å¯¹è¿™äº›æ•°æ®çš„è¿‡æ‹Ÿåˆ

å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä½äº 1/1 çš„é‡å¹³è¡¡ç­–ç•¥ï¼Œä»è€Œé™åˆ¶æ•°æ®é‡å¤ã€‚

å¯¹äºè¿™ä¸ªé…æ–¹ï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½æ•°æ®é›†ã€‚å¦‚æœä½ å·²ç»å®Œæˆäº†*Undersampling an imbalanced dataset*é…æ–¹ï¼Œé‚£ä¹ˆä½ æ— éœ€åšå…¶ä»–æ“ä½œã€‚

å¦åˆ™ï¼Œé€šè¿‡ä½¿ç”¨ Kaggle APIï¼ˆè¯·å‚è€ƒ*Hashing high cardinality features*é…æ–¹äº†è§£å¦‚ä½•å®‰è£…ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¡Œä¸‹è½½æ•°æ®é›†ï¼š

```py
kaggle datasets download -d mlg-ulb/creditcardfraud
```

è¿˜éœ€è¦ä»¥ä¸‹åº“ï¼š`pandas`ç”¨äºåŠ è½½æ•°æ®ï¼Œ`scikit-learn`ç”¨äºå»ºæ¨¡ï¼Œ`matplotlib`ç”¨äºæ˜¾ç¤ºæ•°æ®ï¼Œ`imbalanced-learn`ç”¨äºè¿‡é‡‡æ ·éƒ¨åˆ†ã€‚å®ƒä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¡Œå®‰è£…ï¼š

```py
pip install pandas scikit-learn matplotlib imbalanced-learn.
```

## å¦‚ä½•æ“ä½œ...

åœ¨æœ¬é…æ–¹ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹ä¿¡ç”¨å¡æ¬ºè¯ˆæ•°æ®é›†åº”ç”¨è¿‡é‡‡æ ·ï¼š

1.  å¯¼å…¥æ‰€éœ€çš„æ¨¡å—ã€ç±»å’Œå‡½æ•°ï¼š

    +   `pandas`ç”¨äºæ•°æ®åŠ è½½å’Œå¤„ç†

    +   `train_test_split`ç”¨äºæ•°æ®åˆ’åˆ†

    +   `StandardScaler`ç”¨äºæ•°æ®ç¼©æ”¾ï¼ˆæ•°æ®é›†ä»…åŒ…å«å®šé‡ç‰¹å¾ï¼‰

    +   `RandomOverSampler`ç”¨äºè¿‡é‡‡æ ·

    +   `LogisticRegression`ç”¨äºå»ºæ¨¡

    +   `roc_auc_score`ç”¨äºæ˜¾ç¤º ROC å’Œ ROC AUC è®¡ç®—ï¼š

        ```py
        import pandas as pd
        ```

        ```py
        import matplotlib.pyplot as plt
        ```

        ```py
        from sklearn.model_selection import train_test_split
        ```

        ```py
        from sklearn.preprocessing import StandardScaler
        ```

        ```py
        from imblearn.over_sampling import RandomOverSampler
        ```

        ```py
        from sklearn.linear_model import LogisticRegression
        ```

        ```py
        from sklearn.metrics import roc_auc_score
        ```

1.  ä½¿ç”¨ pandas åŠ è½½æ•°æ®ã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥åŠ è½½ ZIP æ–‡ä»¶ã€‚ä¸å‰é¢çš„å°èŠ‚ä¸€æ ·ï¼Œæˆ‘ä»¬å°†æ˜¾ç¤ºæ¯ä¸ªæ ‡ç­¾çš„ç›¸å¯¹æ•°é‡ï¼Œä»¥æé†’æˆ‘ä»¬å¤§çº¦æœ‰ 99.8%çš„æ­£å¸¸äº¤æ˜“å’Œä¸åˆ° 0.18%çš„æ¬ºè¯ˆäº¤æ˜“ï¼š

    ```py
    df = pd.read_csv('creditcardfraud.zip')
    
    df['Class'].value_counts(normalize=True)
    ```

è¿™å°†è¾“å‡ºä»¥ä¸‹å†…å®¹ï¼š

```py
0Â Â Â Â Â Â Â Â Â 0.998273
1Â Â Â Â Â Â Â Â Â 0.001727
Name: Class, dtype: float64
```

1.  å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æˆ‘ä»¬å¿…é¡»æŒ‡å®šæ ‡ç­¾çš„åˆ†å±‚æŠ½æ ·ï¼Œä»¥ç¡®ä¿å¹³è¡¡ä¿æŒä¸å˜ï¼š

    ```py
    X_train, X_test, y_train, y_test = train_test_split(
    
    Â Â Â Â df.drop(columns=['Class']), df['Class'],
    
    Â Â Â Â test_size=0.2, random_state=0,
    
    Â Â Â Â stratify=df['Class'])
    ```

1.  ä½¿ç”¨ 10%çš„é‡‡æ ·ç­–ç•¥è¿›è¡Œéšæœºè¿‡é‡‡æ ·ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å°†è¿‡é‡‡æ ·ä¸è¶³è¡¨ç¤ºçš„ç±»åˆ«ï¼Œç›´åˆ°ç±»åˆ«å¹³è¡¡è¾¾åˆ° 10 æ¯” 1 çš„æ¯”ä¾‹ã€‚è¿™ä¸ªæ¯”ä¾‹ç”±`sampling_strategy=0.1`å‚æ•°å®šä¹‰ã€‚æˆ‘ä»¬è¿˜å¿…é¡»è®¾ç½®éšæœºçŠ¶æ€ä»¥ä¿è¯å¯é‡å¤æ€§ï¼š

    ```py
    # Instantiate the oversampler with a 10% strategy
    
    ros = RandomOverSampler(sampling_strategy=0.1,
    
    Â Â Â Â random_state=0)
    
    # Overersample the train dataset
    
    X_train, y_train = ros.fit_resample(X_train, y_train)
    
    # Check the balance
    
    y_train.value_counts()
    ```

è¿™å°†è¾“å‡ºä»¥ä¸‹å†…å®¹ï¼š

```py
0Â Â Â Â Â Â Â Â Â 227451
1Â Â Â Â Â Â Â Â Â Â Â 22745
Name: Class, dtype: int64
```

åœ¨è¿‡é‡‡æ ·åï¼Œæˆ‘ä»¬çš„è®­ç»ƒé›†ä¸­ç°åœ¨æœ‰`227451`ä¸ªæ­£å¸¸äº¤æ˜“ï¼ˆä¿æŒä¸å˜ï¼‰ï¼Œä¸`22745`ä¸ªæ¬ºè¯ˆäº¤æ˜“ã€‚

æ³¨æ„

å¯ä»¥æ›´æ”¹é‡‡æ ·ç­–ç•¥ã€‚åƒå¾€å¸¸ä¸€æ ·ï¼Œè¿™éœ€è¦åšå‡ºå–èˆï¼šæ›´å¤§çš„é‡‡æ ·ç­–ç•¥æ„å‘³ç€æ›´å¤šçš„é‡å¤æ ·æœ¬æ¥å¢åŠ å¹³è¡¡ï¼Œè€Œè¾ƒå°çš„é‡‡æ ·ç­–ç•¥åˆ™æ„å‘³ç€æ›´å°‘çš„é‡å¤æ ·æœ¬ï¼Œä½†å¹³è¡¡è¾ƒå·®ã€‚

1.  ä½¿ç”¨æ ‡å‡†ç¼©æ”¾å™¨å¯¹æ•°æ®è¿›è¡Œç¼©æ”¾ï¼š

    ```py
    # Scale the data
    
    scaler = StandardScaler()
    
    X_train = scaler.fit_transform(X_train)
    
    X_test = scaler.transform(X_test)
    ```

1.  åœ¨è®­ç»ƒé›†ä¸Šå®ä¾‹åŒ–å¹¶è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ï¼š

    ```py
    lr = LogisticRegression()
    
    lr.fit(X_train, y_train)
    ```

1.  è®¡ç®—è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„ ROC AUCã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨`predict_proba()`æ–¹æ³•ä»¥åŠå¯¼å…¥çš„`roc_auc_score()`å‡½æ•°æ¥è·å¾—ï¼š

    ```py
    # Get the probas
    
    y_train_proba = lr.predict_proba(X_train)[:, 1]
    
    y_test_proba = lr.predict_proba(X_test)[:, 1]
    
    # Display the ROC AUC
    
    print('ROC AUC training set:', roc_auc_score(y_train,
    
    Â Â Â Â Â y_train_proba))
    
    print('ROC AUC test set:', roc_auc_score(y_test,
    
    Â Â Â Â y_test_proba))
    ```

è¿™å°†è¿”å›ä»¥ä¸‹å†…å®¹ï¼š

```py
ROC AUC training set: 0.9884952360756659
ROC AUC test set: 0.9721115830969416
```

ç»“æœä¸æˆ‘ä»¬é€šè¿‡æ¬ é‡‡æ ·è·å¾—çš„ç»“æœç›¸å½“ç›¸ä¼¼ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ„å‘³ç€è¿™ä¸¤ç§æŠ€æœ¯æ€»æ˜¯ç›¸ç­‰çš„ã€‚

## è¿˜æœ‰æ›´å¤šâ€¦â€¦

å¯é€‰åœ°ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨*æ¬ é‡‡æ ·ä¸å¹³è¡¡æ•°æ®é›†*ä¸€èŠ‚ä¸­æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨`scikit-learn`ä¸­çš„`roc_curve()`å‡½æ•°ç»˜åˆ¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ ROC æ›²çº¿ï¼š

```py
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
# Display the ROC curve
fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)
fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)
plt.plot(fpr_test, tpr_test, label='test')
plt.plot(fpr_train, tpr_train, label='train')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.legend()
plt.show()
```

![å›¾ 5.3 â€“ è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ ROC æ›²çº¿ã€‚ä»£ç ç”Ÿæˆçš„å›¾](img/B19629_05_03.jpg)

å›¾ 5.3 â€“ è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ ROC æ›²çº¿ã€‚ä»£ç ç”Ÿæˆçš„å›¾

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæµ‹è¯•é›†çš„ ROC AUC æ›²çº¿æ˜æ˜¾ä½äºè®­ç»ƒé›†çš„æ›²çº¿ï¼Œè¿™æ„å‘³ç€æ¨¡å‹ç¨å¾®å‡ºç°äº†è¿‡æ‹Ÿåˆã€‚

## å¦è§

`RandomUnderSampler`çš„æ–‡æ¡£å¯ä»¥åœ¨[`imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml`](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml)æ‰¾åˆ°ã€‚

# ä½¿ç”¨ SMOTE é‡æ–°é‡‡æ ·ä¸å¹³è¡¡æ•°æ®

æœ€åï¼Œå¤„ç†ä¸å¹³è¡¡æ•°æ®é›†çš„ä¸€ä¸ªæ›´å¤æ‚çš„è§£å†³æ–¹æ¡ˆæ˜¯åä¸º SMOTE çš„æ–¹æ³•ã€‚åœ¨è§£é‡Š SMOTE ç®—æ³•åï¼Œæˆ‘ä»¬å°†åº”ç”¨è¿™ç§æ–¹æ³•æ¥å¤„ç†ä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹æ•°æ®é›†ã€‚

## å‡†å¤‡å·¥ä½œ

**SMOTE**ä»£è¡¨**Synthetic Minority Oversampling TEchnique**ã€‚é¡¾åæ€ä¹‰ï¼Œå®ƒä¸ºä¸å¹³è¡¡ç±»åˆ›å»ºåˆæˆæ ·æœ¬ã€‚ä½†å®ƒç©¶ç«Ÿå¦‚ä½•åˆ›å»ºåˆæˆæ•°æ®ï¼Ÿ

è¯¥æ–¹æ³•åœ¨ä¸å¹³è¡¡ç±»ä¸Šä½¿ç”¨ k-NN ç®—æ³•ã€‚SMOTE ç®—æ³•å¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤æ€»ç»“ï¼š

1.  éšæœºé€‰æ‹©å°‘æ•°ç±»ä¸­çš„æ ·æœ¬![](img/F_05_001.png)ã€‚

1.  ä½¿ç”¨ k-NNï¼Œåœ¨å°‘æ•°ç±»ä¸­éšæœºé€‰æ‹©![](img/F_05_002.png)çš„ k ä¸ªæœ€è¿‘é‚»ä¹‹ä¸€ã€‚æˆ‘ä»¬ç§°æ­¤æ ·æœ¬ä¸º![](img/F_05_003.png)ã€‚

1.  è®¡ç®—æ–°çš„åˆæˆæ ·æœ¬ï¼Œ![](img/F_05_004.png)ï¼Œå…¶ä¸­ğœ†åœ¨[0, 1]èŒƒå›´å†…éšæœºæŠ½å–ï¼š

![å›¾ 5.4 â€“ SMOTE çš„è§†è§‰è¡¨ç¤º](img/B19629_05_04.jpg)

å›¾ 5.4 â€“ SMOTE çš„è§†è§‰è¡¨ç¤º

ä¸éšæœºè¿‡é‡‡æ ·ç›¸æ¯”ï¼Œæ­¤æ–¹æ³•æ›´å¤æ‚ï¼Œå› ä¸ºå®ƒæœ‰ä¸€ä¸ªè¶…å‚æ•°ï¼šè€ƒè™‘çš„æœ€è¿‘é‚»æ•°`k`ã€‚æ­¤æ–¹æ³•ä¹Ÿæœ‰å…¶åˆ©å¼Šï¼š

+   **ä¼˜ç‚¹**ï¼šä¸éšæœºè¿‡é‡‡æ ·ä¸åŒï¼Œå®ƒé™åˆ¶äº†åœ¨ä¸å¹³è¡¡ç±»ä¸Šè¿‡æ‹Ÿåˆçš„é£é™©ï¼Œå› ä¸ºæ ·æœ¬ä¸ä¼šé‡å¤

+   **ç¼ºç‚¹**ï¼šåˆ›å»ºåˆæˆæ•°æ®æ˜¯ä¸€ç§å†’é™©çš„èµŒæ³¨ï¼›æ²¡æœ‰ä»»ä½•ä¿è¯å®ƒå…·æœ‰æ„ä¹‰ï¼Œå¹¶ä¸”å¯èƒ½ä¼šåœ¨çœŸå®æ•°æ®ä¸Šæˆä¸ºå¯èƒ½

è¦å®Œæˆæ­¤é…æ–¹ï¼Œå¦‚æœå°šæœªè¿™æ ·åšï¼Œè¯·ä¸‹è½½ä¿¡ç”¨å¡æ¬ºè¯ˆæ•°æ®é›†ï¼ˆæŸ¥çœ‹*Undersampling an imbalanced dataset*æˆ–*Oversampling an imbalanced dataset*é…æ–¹ä»¥äº†è§£å¦‚ä½•æ‰§è¡Œæ­¤æ“ä½œï¼‰ã€‚

ä½¿ç”¨ Kaggle APIï¼ˆå‚è€ƒ*Hashing high cardinality features*é…æ–¹ä»¥äº†è§£å¦‚ä½•å®‰è£…å®ƒï¼‰ï¼Œæˆ‘ä»¬å¿…é¡»é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¡Œä¸‹è½½æ•°æ®é›†ï¼š

```py
kaggle datasets download -d mlg-ulb/creditcardfraud
```

éœ€è¦ä»¥ä¸‹åº“ï¼š`pandas` ç”¨äºåŠ è½½æ•°æ®ï¼Œ`scikit-learn` ç”¨äºå»ºæ¨¡ï¼Œ`matplotlib` ç”¨äºæ˜¾ç¤ºæ•°æ®ï¼Œ`imbalanced-learn` ç”¨äºæ¬ é‡‡æ ·ã€‚å®ƒä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è¡Œå®‰è£…ï¼š

```py
pip install pandas scikit-learn matplotlib imbalanced-learn.
```

## å¦‚ä½•åšâ€¦

åœ¨æœ¬é…æ–¹ä¸­ï¼Œæˆ‘ä»¬å°†å¯¹ä¿¡ç”¨å¡æ¬ºè¯ˆæ•°æ®é›†åº”ç”¨ SMOTEï¼š

1.  å¯¼å…¥æ‰€éœ€çš„æ¨¡å—ã€ç±»å’Œå‡½æ•°ï¼š

    +   `pandas` ç”¨äºæ•°æ®åŠ è½½å’Œæ“ä½œ

    +   `train_test_split` ç”¨äºæ•°æ®æ‹†åˆ†

    +   `StandardScaler` ç”¨äºæ•°æ®é‡æ–°ç¼©æ”¾ï¼ˆæ•°æ®é›†ä»…åŒ…å«å®šé‡ç‰¹å¾ï¼‰

    +   `SMOTE` ç”¨äº SMOTE è¿‡é‡‡æ ·

    +   `LogisticRegression` ç”¨äºå»ºæ¨¡

    +   `roc_auc_score` ç”¨äºæ˜¾ç¤º ROC å’Œ ROC AUC è®¡ç®—ï¼š

        ```py
        import pandas as pd
        ```

        ```py
        import matplotlib.pyplot as plt
        ```

        ```py
        from sklearn.model_selection import train_test_split
        ```

        ```py
        from sklearn.preprocessing import StandardScaler
        ```

        ```py
        from imblearn.over_sampling import SMOTE
        ```

        ```py
        from sklearn.linear_model import LogisticRegression
        ```

        ```py
        from sklearn.metrics import roc_auc_score
        ```

1.  ä½¿ç”¨`pandas`åŠ è½½æ•°æ®ã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥åŠ è½½ ZIP æ–‡ä»¶ã€‚ä¸å‰ä¸¤ä¸ªé…æ–¹ä¸€æ ·ï¼Œæˆ‘ä»¬å°†æ˜¾ç¤ºæ¯ä¸ªæ ‡ç­¾çš„ç›¸å¯¹æ•°é‡ã€‚å†æ¬¡ï¼Œæˆ‘ä»¬å°†æœ‰å¤§çº¦ 99.8%çš„å¸¸è§„äº¤æ˜“å’Œå°‘äº 0.18%çš„æ¬ºè¯ˆäº¤æ˜“ï¼š

    ```py
    df = pd.read_csv('creditcardfraud.zip')
    
    df['Class'].value_counts(normalize=True)
    ```

è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
0Â Â Â Â Â Â Â Â Â 0.998273
1Â Â Â Â Â Â Â Â Â 0.001727
Name: Class, dtype: float64
```

1.  å°†æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚æˆ‘ä»¬å¿…é¡»åœ¨æ ‡ç­¾ä¸ŠæŒ‡å®šåˆ†å±‚ï¼Œä»¥ç¡®ä¿å¹³è¡¡ä¿æŒä¸å˜ï¼š

    ```py
    X_train, X_test, y_train, y_test = train_test_split(
    
    Â Â Â Â df.drop(columns=['Class']), df['Class'],
    
    Â Â Â Â test_size=0.2, random_state=0,
    
    Â Â Â Â stratify=df['Class'])
    ```

1.  ä½¿ç”¨`sampling_strategy=0.1`å‚æ•°åº”ç”¨ SMOTEï¼Œä»¥ 10%çš„é‡‡æ ·ç­–ç•¥ç”Ÿæˆä¸å¹³è¡¡ç±»çš„åˆæˆæ•°æ®ã€‚é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬å°†åœ¨ç±»å¹³è¡¡ä¸­å®ç° 10 æ¯” 1 çš„æ¯”ä¾‹ã€‚æˆ‘ä»¬è¿˜å¿…é¡»è®¾ç½®éšæœºçŠ¶æ€ä»¥å®ç°å¯é‡ç°æ€§ï¼š

    ```py
    # Instantiate the SLOT with a 10% strategy
    
    smote = SMOTE(sampling_strategy=0.1, random_state=0)
    
    # Overersample the train dataset
    
    X_train, y_train = smote.fit_resample(X_train,
    
    Â Â Â Â y_train)
    
    # Check the balance
    
    y_train.value_counts()
    ```

é€šè¿‡æ­¤æ–¹æ³•ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä»¥ä¸‹è¾“å‡ºï¼š

```py
0Â Â Â Â Â Â Â Â Â 227451
1Â Â Â Â Â Â Â Â Â Â Â 22745
Name: Class, dtype: int64
```

åœ¨è¿‡é‡‡æ ·ä¹‹åï¼Œæˆ‘ä»¬åœ¨è®­ç»ƒé›†ä¸­ç°åœ¨æœ‰ `227451` ä¸ªå¸¸è§„äº¤æ˜“ï¼ˆä¿æŒä¸å˜ï¼‰ï¼Œä¸ `22745` ä¸ªæ¬ºè¯ˆäº¤æ˜“ï¼Œå…¶ä¸­åŒ…æ‹¬è®¸å¤šåˆæˆç”Ÿæˆçš„æ ·æœ¬ã€‚

1.  ä½¿ç”¨æ ‡å‡†åŒ–ç¼©æ”¾å™¨å¯¹æ•°æ®è¿›è¡Œé‡ç¼©æ”¾ï¼š

    ```py
    # Scale the data
    
    scaler = StandardScaler()
    
    X_train = scaler.fit_transform(X_train)
    
    X_test = scaler.transform(X_test)
    ```

1.  åœ¨è®­ç»ƒé›†ä¸Šå®ä¾‹åŒ–å¹¶è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ï¼š

    ```py
    lr = LogisticRegression()
    
    lr.fit(X_train, y_train)
    ```

1.  è®¡ç®—è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„ ROC AUCã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡ï¼Œè¿™å¯ä»¥é€šè¿‡ä½¿ç”¨ `predict_proba()` æ–¹æ³•ä»¥åŠå¯¼å…¥çš„ `roc_auc_score()` å‡½æ•°æ¥è·å¾—ï¼š

    ```py
    # Get the probas
    
    y_train_proba = lr.predict_proba(X_train)[:, 1]
    
    y_test_proba = lr.predict_proba(X_test)[:, 1]
    
    # Display the ROC AUC
    
    print('ROC AUC training set:', roc_auc_score(y_train,
    
    Â Â Â Â y_train_proba))
    
    print('ROC AUC test set:', roc_auc_score(y_test,
    
    Â Â Â Â y_test_proba))
    ```

ç°åœ¨ï¼Œè¾“å‡ºåº”å¦‚ä¸‹æ‰€ç¤ºï¼š

```py
ROC AUC training set: 0.9968657635906649
ROC AUC test set: 0.9711737923925902
```

ç»“æœä¸æˆ‘ä»¬åœ¨éšæœºæ¬ é‡‡æ ·å’Œè¿‡é‡‡æ ·ä¸­å¾—åˆ°çš„ç»“æœç•¥æœ‰ä¸åŒã€‚å°½ç®¡åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½éå¸¸ç›¸ä¼¼ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ä¼¼ä¹æœ‰æ›´å¤šçš„è¿‡æ‹Ÿåˆã€‚å¯¹æ­¤ç±»ç»“æœæœ‰å‡ ç§å¯èƒ½çš„è§£é‡Šï¼Œå…¶ä¸­ä¹‹ä¸€æ˜¯åˆæˆæ ·æœ¬å¯¹æ¨¡å‹çš„å¸®åŠ©ä¸å¤§ã€‚

æ³¨æ„

æˆ‘ä»¬åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šå¾—åˆ°çš„é‡é‡‡æ ·ç­–ç•¥ç»“æœä¸ä¸€å®šèƒ½ä»£è¡¨æˆ‘ä»¬åœ¨å…¶ä»–æ•°æ®é›†ä¸Šå¾—åˆ°çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¿…é¡»å¾®è°ƒé‡‡æ ·ç­–ç•¥å’Œæ¨¡å‹ï¼Œæ‰èƒ½è·å¾—åˆé€‚çš„æ€§èƒ½å¯¹æ¯”ã€‚

## è¿˜æœ‰æ›´å¤š...

å¯é€‰åœ°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `scikit-learn` ä¸­çš„ `roc_curve()` å‡½æ•°ç»˜åˆ¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ ROC æ›²çº¿ï¼š

```py
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
# Display the ROC curve
fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)
fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)
plt.plot(fpr_test, tpr_test, label='test')
plt.plot(fpr_train, tpr_train, label='train')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.legend()
plt.show()
```

è¿™æ˜¯å®ƒçš„å›¾ç¤ºï¼š

![å›¾ 5.5 â€“ ä½¿ç”¨ SMOTE åçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›† ROC æ›²çº¿](img/B19629_05_05.jpg)

å›¾ 5.5 â€“ ä½¿ç”¨ SMOTE åçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›† ROC æ›²çº¿

ä¸éšæœºæ¬ é‡‡æ ·å’Œè¿‡é‡‡æ ·ç›¸æ¯”ï¼Œè¿™é‡Œè¿‡æ‹Ÿåˆä¼¼ä¹æ›´åŠ æ˜æ˜¾ã€‚

## å¦è§

SMOTE çš„å®˜æ–¹æ–‡æ¡£å¯ä»¥åœ¨ [`imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.xhtml`](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.xhtml) æ‰¾åˆ°ã€‚

ä¸å»ºè®®å°†æ­¤å®ç°åº”ç”¨äºåˆ†ç±»ç‰¹å¾ï¼Œå› ä¸ºå®ƒå‡è®¾æ ·æœ¬çš„ç‰¹å¾å€¼å¯ä»¥æ˜¯å…¶ä»–æ ·æœ¬ç‰¹å¾å€¼çš„ä»»ä½•çº¿æ€§ç»„åˆã€‚è¿™å¯¹äºåˆ†ç±»ç‰¹å¾æ¥è¯´å¹¶ä¸æˆç«‹ã€‚

ä¹Ÿæœ‰é’ˆå¯¹åˆ†ç±»ç‰¹å¾çš„å·¥ä½œå®ç°ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›ä¾‹å­ï¼š

+   **SMOTENC**ï¼šç”¨äºå¤„ç†åŒ…å«åˆ†ç±»ç‰¹å¾å’Œéåˆ†ç±»ç‰¹å¾çš„æ•°æ®é›†ï¼š[`imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.xhtml`](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.xhtml)

+   **SMOTEN**ï¼šç”¨äºå¤„ç†ä»…åŒ…å«åˆ†ç±»ç‰¹å¾çš„æ•°æ®é›†ï¼š[`imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.xhtml`](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.xhtml)
