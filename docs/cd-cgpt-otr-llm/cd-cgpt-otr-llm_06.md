

# 第六章：在 LLM 生成代码的法律框架中导航

本章探讨并讨论 LLM 代码的版权法和法规，谁拥有哪些**知识产权**（**IP**），谁负责，以及法规应当覆盖的范围。像本书中的所有章节一样，本章提供了如何行动的指导。本章将帮助你保持在法律框架内，避免任何法律问题。

在这里，我们将了解以下内容：

+   解读版权和知识产权问题

+   解决 LLM 生成的代码的责任和义务

+   审视在编码中使用 LLM 的法律框架

+   AI 生成代码的未来可能的监管

学习完本章后，你应该意识到使用 LLM 生成的代码可能引发的相关问题，了解如何处理与代码相关的法律问题，为未来做好规划，并创建具备未来可持续性和创新能力的代码。

# 技术要求

本章可能需要以下内容：

+   访问如 GPT-4 或 Gemini 等 LLM/聊天机器人；每个都需要登录。对于 GPT-4，你需要一个 OpenAI 账户，而对于 Gemini，你需要一个 Google 账户。

现在，我们将进入本章的第一小节，讨论版权和知识产权法。

# 解读版权和知识产权问题

使用 LLM，如 Devin、Gemini 和 GPT-4o，快速获取一些代码来解决问题很容易，但避免引发法律问题可能更为困难，需要仔细审查。

这可能很复杂，尤其是因为全球各地存在不同的司法管辖区，它在一定程度上取决于一件作品与另一件作品的相似度。如果你创作的作品与某个现有作品足够相似，现有作品的创作者可以声称你侵犯了版权法。接下来的问题是：“ *这第二个作品是否足够相似于第一个作品，构成复制、衍生作品、注释、再现、翻译、删节、压缩、戏剧化、虚构化或其他改编、转化或改动原有作品的行为？* ” 目前，没有普遍适用的法律框架来处理这种困惑，因为解决这种问题的方式在不同的司法管辖区、陪审团和法官之间有所不同。

“ *音符的数量是有限的* ”是一个可以用于音乐的论点，因为音乐音符确实是有限的，但将这一点延伸到人类语言中的单词就完全不成立了。

然而，这可能对代码适用，因为代码必须符合某些格式，例如函数、类、输入、输出，以及像**print**、**if**、**else**、**switch**、**while**等语句[LawStackExchange]。

这并不十分清晰或有帮助，可能会让人想蜷缩成一团，永远不再写任何东西！更明确的是，版权法通常保护人类创作的作品，而 LLMs 既不是人类，也不是法律实体，不能持有版权。

该代码由大语言模型（LLMs）生成，并且来自训练数据中的代码，这些代码大多数可能最初是由人类编写的，因此原始代码可能会受到版权保护。

包括 LLMS 拥有者在内的公司确实拥有由员工创作的知识产权，并且可以拥有这些作品作为“员工创作”的版权。那么，这是否延伸到 LLM 生成的作品呢？LLM 生成的代码是否需要或已经获得版权保护仍有争议。关于使用受版权保护的代码的法律立场并不明确，可能在不同的法域下有不同的看法（“合理使用”）或需要特定的许可 [*Gemini*]。

接下来我们来看看不同法域的情况。

## 欧盟——需要人类的触感

在欧盟，事情并不十分明确，但法律规定，只有当作品体现了“*作者自己的* *智力创造*”时，版权才能得到保护。

这通常被解释为作品中有显著的人类参与。**欧盟法院**（**CJEU**）将其解释为作品必须是原创的，并且反映出作者的个性和表达的自主性。这个原则通过 CJEU 的多次裁决得出 [*GPT-4o, Gemini*]。

欧盟成员国仍需决定何时以及在哪些情况下，人工智能生成的作品符合这一要求。在法国，法律要求证明存在“*个人触感或智力努力*”以符合原创性标准，并且“*自动化和约束性逻辑的实施*”没有“*真正的个人努力*”不足以使作品被视为原创。法国版权法（《知识产权法典》）通过法院判决来确定。法国法院始终解释原创性需要作者的“*个人触感*”或“*智力努力*”。L111-1 条及以下。[ *Cooley,* *GPT-4o, Gemini* ]。

在德国版权法（**Urheberrechtsgesetz**（**UrhG**）或版权法）中，要求机器或计算机程序不能成为作者；只有自然人才能成为作者。

德国法院始终将“*作者*”的概念解释为需要由自然人进行智力创造。机器或计算机程序本身未被认可为作者。

因此，欧盟似乎要求所有作品中有较大比例的内容必须来自人类。

## 英国——创意和安排是创作所必需的

Cooley 表示，英国法律表面上与欧盟法律相似，必须在作品中体现出人类的触感和创造力，才能受到版权保护。这意味着，如果创意完全来自软件，则该作品不受英国法律保护。

然而，1988 年《版权、设计与专利法》中的英国法律规定，作品的所有者是做出“ *创作作品所需安排* ”的人（第 9(3)条），即使它是一个计算机程序。尚不清楚这是否指的是创造软件的个人、团体或公司，还是像提示工程师这样的用户。1988 年 CDPA 第 9(3)条在 2022 年得到了英国政府和英国知识产权局的支持和认可，特别是在生成式 AI 相关问题上。

因此，AI 生成的作品可以在英国受到版权法保护，但这一问题仍未完全解决。

## 美国——没有 AI 生成作品的所有权

根据[Cooley]的说法，在美国，法律规定 AI 生成的作品不能由任何人拥有——既不是 AI，也不是 AI 的创造者，亦不是提示词编写者/用户。这使得这些 AI 生成的作品处于公共领域，不受版权保护。这也适用于猴子，就像“纳鲁托诉斯雷特”案件一样——猴子不能拥有它们拍摄的照片。美国地区法院法官裁定，版权不能授予（非人类）动物[ *Monkey_Business* ]。最有可能的版权拥有者是相机的拥有者 David Slater，但这一问题尚未完全解决。

## 中华人民共和国——做出最大贡献者

在中国，AI 生成作品也可能拥有所有权，并因此受到版权保护。如果一家公司开发了一个 AI 工具，包括设置，并且该工具产生了输出，那么公司可以被认定为作品和版权的所有者，就像深圳腾讯诉上海英迅案[ *Cooley* ]，其中腾讯被判定为所有者。

然而，在用户输入设置并且发起作品创意的情况下，版权可能归用户所有，比如在 Stable Diffusion 的案例中，尽管 Stable Diffusion 开发了 AI 平台。

看起来，所有权通常归属于做出最大贡献的一方，无论是 AI 工具或平台的开发者，还是用户。像其他国家一样，这一法律领域始终在发展中，尚未完全解决。

中国法律根据具体案件来判定[ *GPT-4o* ]。

## 台湾——人类创作性表达

台湾知识产权局在 2023 年 6 月 16 日的《受知 11252800520 函》中指出，使用受版权保护的作品来训练 AI 模型可能侵犯版权持有人的复制权；缺乏人类创作性表达的内容不受《著作权法》保护。所以，纯粹由 AI 生成的作品，仅依赖人类指令，是不受《著作权法》保护的。

这与欧洲法律相似，但正朝着美国法律的方向发展[ *Lexology* *和 GPT-4o* ]。

## 印度和加拿大——人类作者的技能和判断力

在印度，AI 生成的作品如果满足原创性要求，可以纳入版权法的保护，但新颖性不相关，因为它对于版权而言是一个过高的标准【*Ind_Law&Tech*】。作品必须是作者的技能和判断的产物，并且不能过于琐碎，以至于它仅仅是一个机械化的操作，才能获得版权保护。作品不能仅由 AI 工具或软件单独生成。这与加拿大法律非常相似，其中包含了人类的技能和判断。对于印度法律而言，作品所需的工作量是指没有人类干预或行动，作品将会是根本不同或根本不存在的。

## 澳大利亚——归于为创作作品做出必要安排的人

《澳大利亚 1968 年版权法》与英国法类似，作者身份归于为创作作品做出必要安排的人。这里仍存在不明确之处，尚未明确界定【*GPT4-o*】。

## 日本——著作权要求人类创作

目前，由 AI 生成的作品不受版权保护，且需要人工输入。法律正在为生成型人工智能时代进行调整，但目前并未涉及生成型 AI 作品【*GPT4-o* *和 Gemini*】。

## 韩国

韩国著作权法表示，纯粹由 AI 生成的作品不受版权保护，但如果有显著的人工输入，则可能会有所不同。韩国将“作品”定义为表达人的思想和情感，而将“作者”定义为创作作品的人【*GPT4-o* *和 Gemini*】。

## 巴西——要求人类创作

这与韩国、美国和日本相似。巴西的著作权法为文学、艺术或科学性质的智力创作提供保护。关于 AI 生成的代码是否符合资格，因缺乏人类作者而存在一定争议【*GPT4-o* *和 Gemini*】。

## 印度尼西亚——需要人类创作

印度尼西亚要求人类参与创作作品，以便作品能够享有版权保护；作品必须是原创的，并且包含大量的创造性和智力投资。纯粹由 AI 生成的作品可能在印度尼西亚不受保护。知识产权办公室尚未就 AI 生成作品和版权发布任何具体裁定【*GPT-4o* *和 Gemini*】。

## 不断变化的法律环境

在撰写本文时（2024 年），大多数国家正在适应和应对这一新技术，更新其法律以应对 AI 生成的代码、艺术、写作等问题。

## 判例

这件事我们以前也讨论过，关于摄影。1884 年，在美国，最高法院裁定，摄影师拥有他们拍摄的照片的所有权（使用相机）（Burrow-Giles Lithographic Co. v. Sarony, 111 U.S. 53 (1884)）。【*Liu_Medium*】

这些是知识产权的考虑因素，但对于来自大型语言模型（LLMs）的代码，谁应该负责，甚至谁应该承担责任呢？这将是我们接下来小节要讨论的内容。

# 解决 LLM 生成代码的责任和义务问题

我们需要了解在向公众展示代码时，我们应承担什么责任——无论是发布还是分享。我们可能会遇到什么麻烦，如何防范这些问题？

根据 Zerotolive.app 的说法，“ *代码是一种责任，因为你写的每一行都有持续的维护成本，永无止境。几乎任何开发者都会证明，代码永远不可能是‘* *完成的*。’” [ *Zerotolive* ]

所以，我们还有很多工作要做。

让我们从识别代码和 LLM 生成代码的主要责任和义务开始。

以下是与人工智能生成代码相关的各种法律问题：

+   许可

+   归属和信用

+   质量和可靠性

+   伦理考虑

+   产品责任

+   使用案例限制

+   安全问题

+   第三方依赖

在接下来的子章节中，我们将详细探讨这些问题。

然后，我们将讨论当事情出错时该如何处理。

## 许可

来自大型语言模型（LLMs）的代码可能需要与各种开源许可证兼容。许多软件许可证有特定要求，AI 生成的代码可能无法自动满足这些要求。

你需要了解并遵守所使用大型语言模型（LLM）的许可协议，因为这些协议通常规定了生成内容的使用方式。

## 归属和信用

如果生成的代码基于现有作品或受其影响，必须向原作者正确归属，以避免剽窃指控。

在涉及盗窃的案件中，如果人工智能被用来寻找、复制并重新分发未经所有者许可的代码，那么情况就不那么明确。如果复制的内容很多，那么在防范盗窃指控时可能会更困难，而如果声明被盗的代码中有大量创意添加进去，情况可能就比较容易辩护。

如果人工智能的*意图*是窃取版权代码（或其他材料），那么对抄袭者的案件可能会更清楚。

## 质量和可靠性

生成的代码可能包含错误或安全漏洞。获取代码的提示工程师或开发者负责在部署之前审查和测试代码，确保其符合质量和安全标准。

对未经充分审核的代码的潜在滥用可能会导致法律责任，尤其是当它导致数据泄露、财务损失或其他损害时。

## 伦理考虑

我们必须始终确保生成的代码不会延续偏见或导致不公平的行为。我们必须遵循伦理编程实践，以防止造成伤害。

用户和开发者应透明地说明代码是由人工智能模型生成的，这有助于减轻潜在的误解或误传。

重要提示

本书中的大部分或所有代码均由如 Gemini/Bard 和 GPT-4 等大型语言模型生成。请测试本书或其 GitHub 仓库中的所有代码和提示。我们不对使用这些提示和代码或滥用它们承担责任。本书和 GitHub 仓库中的代码和提示示例仅作为示例，供你学习如何使用 LLM 进行编程和理解代码，不适用于生产环境代码。

## 产品责任

如果 AI 生成的代码发生故障，导致物理损坏、数据丢失或财务损失，可能会出现关于责任的法律争议。了解法律保护或风险暴露的程度至关重要。

澄清任何关于 AI 生成代码功能性和安全性的明示或暗示保证。虚假陈述可能会导致法律责任。

## 使用场景限制

一些 LLM 开发者明确禁止生成内容的某些用途，例如军事应用、非法活动和其他敏感环境。作为代码的用户和发布者/分享者，你必须遵守这些限制。

某些行业（例如医疗保健和金融）有特定的法规和标准。确保你的 AI 生成代码符合这些行业特定的要求。

这些是责任，但当有关于你的 AI 生成代码的投诉和法律步骤时，应该如何处理？让我们讨论一下。

## 安全问题

AI 生成的代码可能会不小心引入安全漏洞，可能会被恶意人士利用。用户有责任进行彻底的安全审计并实施必要的保护措施。

实施安全编码的最佳实践，考虑使用自动化安全分析工具来检测 AI 生成代码中的潜在问题。例如，[`sonarsource.com`](https://sonarsource.com) 和 [`www.synopsys.com`](https://www.synopsys.com)。你可以在这里了解更多信息：[`zenitech.co.uk/insights/articles/security-analysis-of-ai-generated-code/`](https://zenitech.co.uk/insights/articles/security-analysis-of-ai-generated-code/)。

## 透明性和可解释性

AI 生成的代码有时可能不透明、缺乏明确文档，甚至是“黑盒”，使其难以理解和维护。请确保你的代码经过全面审查，并做好文档。

你应该能够理解并向他人解释你生成的代码如何工作，特别是在关键应用中，问责制至关重要。

这就是文档记录的重要性；你可以在需要之前就做好记录。记录预期的使用场景和用户。记录预期的代码生态系统。

我们在*第五章*中介绍了如何创建透明且可解释的代码。

## 第三方依赖

在将 AI 生成的代码与第三方软件或服务集成时，确保所有依赖项都是安全的，并符合法律和许可要求。

保持 AI 生成代码所使用的所有第三方组件的最新清单，并监控安全更新或法律变动。

## 使用良好的沟通来避免法律诉讼

以下是一些你可以采取的措施，确保你保持安全，同时不为你的用户带来麻烦：

+   **用户协议和服务条款**：清晰列出使用 AI 生成代码的服务条款和用户协议。

+   **责任限制**：明确规定开发者或平台提供者的责任限制。

+   **用户义务**：列出用户在使用生成代码时的责任和义务。

+   **赔偿条款**：概述与赔偿相关的条款，用户同意在某些情况下使开发者或提供者免责。

+   **反馈和改进**：建立渠道，供用户提供对生成代码的反馈、报告问题并建议改进。

## 使用 AI 时的伦理准则

下面是一些你可以考虑的做法，确保在法律上得到尽可能的保障。

你可以实施并遵循一套伦理准则，指导 AI 生成代码的开发、部署和使用。这有助于确保始终考虑到伦理因素。

你还可以鼓励 AI 模型工作方式的透明性，包括它们的训练数据和算法，以建立用户和利益相关者的信任。

### 社会责任

定期评估 AI 生成代码的社会影响。这涉及评估其对用户和更广泛社会的正面和负面效应。

与利益相关者互动，包括用户、监管机构和倡导团体，了解并解决与 AI 生成代码相关的伦理问题。

### 公共意识与教育

教育用户和公众如何负责任地使用 AI 生成代码。这可以包括最佳实践、潜在风险，以及审查 AI 输出的重要性。

清晰地向用户和利益相关者传达 AI 生成代码的能力、限制和潜在影响。这种透明度有助于建立信任，并促进信息充分的决策。

## 责任追究和补偿机制

补偿是指纠正、修复或赔偿错误、伤害或不公。这涉及采取措施弥补或解决不满，通常通过法律或正式机制进行。在法律和伦理责任的背景下，补偿机制确保个人或实体在受到某个行为、产品或服务的伤害时，有途径寻求正义或赔偿。

如果你正在开发能够生成代码的大型语言模型（LLM），你需要知道作为开发者或你的组织，在代码生成后可能要为其后果承担多少责任。这其中的一部分是确定如果代码失败或造成伤害，谁应当负责。

对于你的组织，建立明确的流程来处理生成代码中的错误或问题。当出现问题时，制定预定的流程来明确责任并解决问题至关重要。

对于因使用人工智能生成代码而遭遇负面结果的开发者，存在不同类型的补救机制。包括以下几种：

+   **赔偿**：实施政策，为受有缺陷或有害代码影响的用户提供赔偿。

+   **法律救济**：当人工智能生成的代码造成损害时，用户可以通过法律途径寻求补救或正义。（补救是指纠正某些问题。）

+   **支持系统**：开发者或平台提供商可以提供支持渠道，帮助用户解决生成代码的问题。

+   **争议解决**：实施机制以解决因使用人工智能生成代码而产生的争议。

+   **调解与仲裁**：建立中立的第三方调解或仲裁程序，以解决用户与开发者或平台提供商之间的冲突 [*GPT-4o*]。

### 合规监管

当然，你应该遵守所有相关的法律、法规和行业标准。这包括数据保护法、网络安全标准以及行业特定的法规。

主动与监管机构合作，帮助塑造和遵守与人工智能生成代码相关的不断变化的法律和标准。

请参阅*第五章*了解更多关于偏见减缓和公平性的信息。

既然我们已经了解了责任和义务，接下来我们需要了解在下一小节中大规模语言模型编码是如何受到法律框架的管控的。

# 检视关于大规模语言模型（LLM）在编码中的使用的法律框架

我们需要审查关于人工智能生成代码的相关法规。这些法规因国际联盟、国家、省份或司法管辖区的不同而有所差异。

## 联合国关于人工智能的决议

2024 年 3 月 21 日，联合国大会通过了首个全球人工智能决议。这一非约束性决议建议各国应保护隐私和人权，并警惕人工智能带来的风险。

这一决议的通过是必要的，因为人们担心人工智能可能被“*用来扰乱民主进程、加剧欺诈或导致大规模失业等* *危害*。” [*Reuters*]

这项联合国决议的焦点不是版权法，但当各国开始执行这些建议时，可能会产生一些影响 [*Gemini*]。

一些国家已经有了人工智能法律；让我们来探讨一下。

## 欧盟 – 欧洲议会通过了《人工智能法案》

2024 年 3 月 13 日，欧洲议会通过了《人工智能法案》，标志着全球首个全面的横向人工智能法律框架的诞生。

它在欧盟范围内建立了关于数据质量、透明度、人类监督和问责制的规则。

### 影响与惩罚

《人工智能法案》具有显著的域外效力，并提出了具有挑战性的要求。

在欧盟开展业务的公司可能面临最高 3500 万欧元的罚款或全球年收入的 7%（以较高者为准）。

AI 法案自在官方公报上发布后 20 天生效【2024 年 6 月发布】。

法案生效两年后条款开始适用，禁止的 AI 系统（六个月后）和生成性 AI（12 个月后）除外。

### AI 的定义

最终定义侧重于具有不同自主性并生成输出（如预测和决策，影响环境）的 AI 系统。

AI 系统不同于简单的软件。推理能力是其关键特征，指的是生成输出，如预测、内容、推荐或决策，这些输出可以影响物理或虚拟环境。

AI 系统包括从数据中学习的机器学习方法，以及从编码知识中推理的基于逻辑的方法。

### 范围和适用性

AI 法案适用于 AI 系统的提供者，包括开发者、进口商和在欧盟分销 AI 系统的商家。

它还适用于“部署者”——在其职业活动中使用 AI 系统的个人或实体。

本法具有域外效力，适用于在欧盟使用的 AI 系统，无论提供者位于何处。

### 豁免

专门用于科学研究和开发的 AI 系统可以豁免。本法不适用于市场投放前的 AI 研究、测试和开发活动，但实际环境中的测试除外。

自由和开源的 AI 系统可以豁免，除非被分类为高风险、禁止或生成性 AI。

### 基于风险的方法

AI 法案使用四个风险级别：不可接受风险、高风险、有限风险和最低风险。

#### 不可接受的风险（禁止）

对基本权利构成明显威胁的 AI 实践是被禁止的。包括以下内容：

+   操控人类行为或利用脆弱性（如年龄和残疾），目的是扭曲行为的 AI 系统

+   生物识别系统，如职场中的情感识别或对个体进行实时分类

#### 高风险

被识别为高风险的 AI 系统必须遵守严格要求，例如：

+   风险缓解措施

+   高质量数据集

+   活动记录

+   详细的文档和清晰的用户信息

+   人类监督

+   高水平的鲁棒性、准确性和网络安全性。

+   示例包括关键基础设施（能源和交通）、医疗设备，以及决定教育或就业机会的系统。

#### 有限风险

对于直接与自然人互动的 AI 系统，如聊天机器人，适用以下要求：

+   提供者必须确保告知用户他们正在与 AI 系统互动——全面披露要求。

+   生成或操控深度伪造的 AI 系统的部署者必须披露该内容是人工生成或操控的。

#### 最低风险

对于低风险 AI 系统，如支持的电子游戏或垃圾邮件过滤器，不会施加具体限制。然而，企业可以自愿承诺遵守行为准则，以确保 AI 的伦理使用。

### 通用 AI 模型/生成性 AI

《人工智能法案》新增了关于通用 AI 模型的章节。

它区分了通用 AI 模型、具有系统性风险的通用 AI 模型以及具有高影响力能力的模型。

这些模型需遵守特定的监管要求，特别是在透明度、数据质量和人工监督方面。

### 与《通用数据保护条例》（GDPR）的关系

《人工智能法案》不会影响 GDPR 或电子隐私指令（2002/58/EC）。

与个人数据处理相关的《人工智能法案》条款必须与现有的数据保护法规保持一致。

### 发展与合规

大多数条款（即企业需要遵守的具体规则和要求）将在法案正式成为法律后的两年生效。然而，与禁用 AI 系统相关的条款将在法案生效后六个月内适用，关于生成性 AI 的条款将在 12 个月后生效。

受《人工智能法案》影响的企业应尽早开始为合规做好准备，以满足这些时间表。这些准备可能涉及对产品和服务的重大重新设计、风险评估以及对新要求的调整。

《人工智能法案》是一个具有里程碑意义的法规，提出了全球首个全面的 AI 框架。

它对确保 AI 系统在欧盟范围内安全和伦理部署提出了严格要求。

法案涵盖了参与 AI 开发、分发和部署的各类实体，包括那些即使不在欧盟境内但其系统在欧盟境内使用的企业。

条款包括基于风险的分类要求，对于高风险系统有严格的要求，并对低风险系统设有透明度义务。

企业需要通过进行风险评估、必要时重新设计产品和服务，并将其运营与新要求对接，以为合规做好准备。

《人工智能法案》还与现有的数据保护法律（如 GDPR）互补，确保 AI 系统以负责任和透明的方式处理个人数据。

尽管有分阶段实施的时间表，但应立即开始为合规做好准备。[ *Zenitech,* *Gemini, GPT-4o* ]

想了解更多关于欧盟《人工智能法案》的信息，请访问[`artificialintelligenceact.eu/ #:~:text=What%20is%20the%20EU%20AI,AI%20to%20three%20risk%20categories`](https://artificialintelligenceact.eu/#:~:text=What%20is%20the%20EU%20AI,AI%20to%20three%20risk%20categories) 或 [`www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence`](https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence)。

## 加利福尼亚州提出了 AI“杀死开关”法案

加利福尼亚的科技公司表示担忧，因为该州参议院通过了一项法案，旨在为 AI 发展制定新的立法。一些大科技公司认为该法案设定了过多的限制，可能会限制创新。该法案名为《前沿人工智能系统安全与创新法》。

根据 Meta 生成式 AI 产品经理 Arun Rao 的说法，这可能会“*终结开源*”AI，例如 Meta 的 Llamas 模型。

Andrew Ng 说：“*如果有人想要制定压制创新的法规，几乎没有比这做得更好了*。”

科技公司开始思考是否需要离开加利福尼亚州。

然而，提出这项立法的民主党州参议员 Scott Weiner 认为这是一个“*温和法案*”，只是要求科技开发者采取基本的安全措施。法案的修正案表示，它仅适用于训练成本至少为 1 亿美元的大型模型[Financial_Times]。

## 其他国家的 AI 法案

巴西正在起草 AI 相关立法，而美国则有一些州级的规定[Gemini]。

新加坡在 AI 法规方面走在前面，并于 2019 年创建了《生成式 AI 模型治理框架》[DataGuidance]。

韩国（大韩民国）在国会中提出了一项 AI 法案，即《人工智能产业促进法及建立可信赖 AI 框架法》，旨在整合不同的分散性法案[*LawAsia*]。

印度政府与**Nasscom**（**国家软件与服务公司协会**）以及领先的行业伙伴合作，制定了 AI 发展的指南——**负责任的 AI**（**RAI**）资源包。虽然这些指南不具有法律约束力，但它为企业提供了一套工具和指导，帮助他们以负责任和安全的方式使用 AI，助力企业成长。[*IndiaAI_RAI, IndiaAI_RAI_Kit*]

其他正在制定 AI 法规的国家包括英国、中国和日本。

其他国家和地区可能会制定自己的 AI 法规，因此在这些司法管辖区部署或销售之前，确保获得良好的法律建议。

该网站概述了各国 AI 法规：[`iapp.org/resources/article/global-ai-governance-jurisdiction-overviews/`](https://iapp.org/resources/article/global-ai-governance-jurisdiction-overviews/)。

## 其他法规

除了直接规范 AI 和机器学习（ML）的法律外，还有与它们相关的国家法律。这些法律涉及训练数据和计算生态系统的技术——数据隐私、数据出口、硬件、软件以及可能为军事提供优势的思想和技术。

### 数据隐私法

在 194 个国家中，有 137 个以上的国家已制定数据隐私和安全法规。确保 AI 生成代码的使用符合数据隐私法规，如欧盟的 GDPR、美国的**加利福尼亚消费者隐私法案**（**CCPA**）、**儿童在线隐私保护法案**（**COPPA**）、巴西的**通用数据保护法**（**LGPD**）、中国的**个人信息保护法**（**PIPL**）、泰国的**个人数据保护法**（**PDPA**）、以色列的**隐私保护法**（**PPL**）以及日本的**个人信息保护法**（**APPI**），尤其是在代码处理个人数据时。

在部署你的代码之前，确保了解并遵守你所在地区的这些法规。

这里有一些进一步学习如何合规的资源：

+   CCPA: [`www.varonis.com/blog/ccpa-compliance`](https://www.varonis.com/blog/ccpa-compliance)

+   COPPA: [`www.accessibilitychecker.org/blog/coppa-compliance-guidelines/`](https://www.accessibilitychecker.org/blog/coppa-compliance-guidelines/)

+   LGPD: [`blog.didomi.io/what-is-the-lgpd-brazil-and-how-can-companies-be-compliant`](https://blog.didomi.io/what-is-the-lgpd-brazil-and-how-can-companies-be-compliant)

+   PIPL: [`www.cookieyes.com/blog/china-personal-information-protection-law-pipl/`](https://www.cookieyes.com/blog/china-personal-information-protection-law-pipl/)

+   PDPA: [`pdpa.guide/`](https://pdpa.guide/)

+   PPL: [`clym.io/regulations/israel-ppl`](https://clym.io/regulations/israel-ppl)

+   APPI 及其他: [`www.consentmo.com/compliance/appi#:~:text=To%20ensure%20employee%20training%20on,security%20measures%2C%20and%20reporting%20procedures`](https://www.consentmo.com/compliance/appi#:~:text=To%20ensure%20employee%20training%20on,security%20measures%2C%20and%20reporting%20procedures)。

### 导出控制法则

注意任何可能适用的出口控制规定，尤其是当代码或底层技术受到此类法律约束时。出口控制是指一个国家限制或禁止你将某些技术带出该国，或在国境外讨论这些技术。它们需要首先进入公共领域，因此，如果这些内容先在本国的会议上讨论或先在期刊上发表，那么你就可以在其他国家谈论它们。

政府关注的技术通常是那些可能具有军事或恐怖主义应用的技术，如 AI 和武器（核武器、化学武器或生物武器）。这包括软件、数据或知识[ *OxfordUni* ]。

现在我们已经了解了与 LLM 代码相关的法律框架，接下来我们将讨论未来这可能发生的变化。

未来针对 AI 生成代码可能实施哪些有效的监管措施？这是本章最后一个小节的主题。

# AI 生成代码的未来监管可能性

以下是一些关于未来可能涉及 AI 生成代码的法规变化的想法。这只是推测，但它可以帮助我们思考如何为未来做准备，并且还能让我们思考应该推动哪些变化。

## 未来的关键点

我们必须考虑法院、国际合作和标准化，以及各国之间的差异、国家竞争与主权、审计，以及未来 AI 法规的形成方式：

+   **法院与版权**：随着 AI 的进步，法院可能会对现有的著作权和版权法进行解读，适用于 AI 生成的内容。

+   **立法改革**：版权法可能需要更新，以应对 AI 生成作品的复杂性。各国可能会推出更清晰的指导方针。

+   **国际标准**：各国对 AI 作品的不同待遇可能会促使呼吁制定国际标准或关于 AI 和知识产权的条约。一些国家可能会效仿欧盟和美国等先行者。

+   **标准化挑战**：然而，文化差异、经济优先事项以及 AI 发展的快速步伐可能会阻碍全球标准化的进程。法律可能难以跟上这一变化。

+   **AI 开发者竞争**：各国之间争夺 AI 开发者的竞争可能会影响相关法规。例如，有人认为中国的法规较为宽松，有助于推动 AI 创新。

+   **公众关注**：相反，公众对隐私、偏见和 AI 带来的失业问题的担忧可能会导致更严格的法规。可能会关注 AI 的透明度和问责制等伦理问题。AI 系统可能需要解释其决策过程和数据来源。

+   **数据主权**：国家政策制定者可能会考虑数据主权规则，将数据保留在国境内，用于训练 AI 并保护国家利益（这超出了当前的出口管制）。

+   **行业认证**：航空、制药、建筑、工程和制造等行业需要通过认证以符合预定标准。AI 系统未来也可能需要类似的认证。

+   **问责与责任**：法律可能会为 AI 开发者、用户和发布者在造成伤害或滥用时建立更明确的问责和责任制度。

+   **AI 审计**：AI 系统可能需要进行独立审计，以确保遵守安全法律和伦理标准，类似于航空和制造等行业的认证要求。

+   **塑造未来的格局**：持续的研究、法律学术和行业的投入将对塑造未来的 AI 法规至关重要。技术专家、法律专家和政策制定者之间的合作对于制定既支持创新又解决伦理和社会问题的平衡法规是必不可少的。这种跨学科的方法能够推动制定有根据、务实且具有前瞻性的政策。

这些都是关于法律如何变化的理论；它们不是确定的计划，只是猜测[ GPT-4o, Gemini]。

## 仍需回答的问题

目前我们仍然需要问并解答关于人工智能的问题，我们需要了解在未来随着人工智能的日益普及，人类越来越依赖人工智能的情况下，我们该如何管理这个转型时期：

+   欧盟关于人工智能的法规建议允许深度伪造技术，但开发者必须公开这些伪造内容。深度伪造技术是否应该被允许？

+   立法者应该如何应对未成年人沉迷人工智能的风险？

+   当这些人工智能进行工作时，应该由谁或什么来征税？

    +   如果某人住在新加坡，使用加利福尼亚州的人工智能，由一家加州公司开发，而该人又为日本的公司工作，应该由这个人还是雇主来纳税？应该由人工智能的所有者、创作者或提供者来纳税吗？

    +   在人工智能时代，大多数任务和工作都由人工智能完成时，我们如何对人民进行税收并为政府服务提供资金？

+   当人工智能做所有工作的时，我们将从事哪些工作或兴趣爱好？

+   我们如何确保生物（除了疾病和害虫外）不受人工智能的威胁？

+   我们是否真的在乎人工智能是否具备意识？

    （专家表示这对我们的结果没有影响。）

+   我们应该绝对禁止人工智能从事哪些工作？

大多数这些问题需要得到回答，才能确保正确制定监管措施。

## 保持最新动态

这些关于监管的未来想法尚不确定。明确的是，人工智能立法像人工智能技术本身一样发展迅速，实践者需要跟进最新进展，确保不被甩在后头！

以下是一些有助于你保持最新了解人工智能合法性演变的有用链接：

+   **国际隐私专业人员协会（IAPP）全球人工智能法律与政策追踪**（[`iapp.org/`](https://iapp.org/)）：该网站提供了一个全面且定期更新的全球人工智能立法和政策发展的概览。

+   **生命未来研究所（FLI）**（[`futureoflife.org/`](https://futureoflife.org/)）：这个非营利组织专注于来自先进技术（包括人工智能）的生存风险。他们追踪和分析与人工智能相关的法律发展，特别是关于安全性和伦理的问题。

+   **Law.com – 人工智能话题**（[`www.law.com/topics/artificial-intelligence/`](https://www.law.com/topics/artificial-intelligence/)）：这个法律新闻网站提供关于人工智能技术相关法律问题的文章、分析和专家见解。

+   **Artificial Lawyer**（[`www.artificiallawyer.com/`](https://www.artificiallawyer.com/)）：这个网站专注于法律科技和人工智能新闻，包括人工智能法律法规的发展。

+   这里是保持与 AI 法规同步的指南；其中还包含了相关的链接帮助您实现这一目标：[`www.linkedin.com/pulse/beginners-guide-keeping-up-date-ai-regulations-bsfitaly-guqsf/`](https://www.linkedin.com/pulse/beginners-guide-keeping-up-date-ai-regulations-bsfitaly-guqsf/)

# 摘要

在本章中，我们看到使用 LLM 获取代码时，我们需要关注法律责任，遵守相关法规，甚至跟进变化中的法规。不同国家对于 AI 生成的材料（如代码）的版权有不同的规定。各国的管辖范围不同，它们在如何管理 AI 和 AI 代码方面有不同的解释，尽管在一些方面存在相似之处，比如它们对 AI 风险的看法以及对更多透明度和清晰度的要求。欧洲联盟等地的法规已开始出台。我们始终需要关注新闻动态。

一直思考 AI 应该被允许做什么，并与他人讨论，特别是与立法者交流。

在下一章，我们将揭示使用 LLM 时涉及的安全问题，如何准备应对威胁，以及如何防范这些威胁。

# 参考文献

+   *Cooley* : “AI 输出在全球范围内存在差异，”Cooley：[`www.cooley.com/news/insight/2024/2024-01-29-copyright-ownership-of-generative-ai-outputs-varies-around-the-world`](https://www.cooley.com/news/insight/2024/2024-01-29-copyright-ownership-of-generative-ai-outputs-varies-around-the-world)

+   *DataGuidance* : “新加坡：IMDA 发布生成式 AI 治理框架模型，”DataGuidance：[`www.dataguidance.com/news/singapore-imda-publishes-model-ai-governance-framework`](https://www.dataguidance.com/news/singapore-imda-publishes-model-ai-governance-framework)

+   *Financial_Times* : “硅谷因加州 AI 安全法案掀起轩然大波，”Hannah Murphy 和 Tabby Kinder：[`www.ft.com/content/eee08381-962f-4bdf-b000-eeff42234ee0`](https://www.ft.com/content/eee08381-962f-4bdf-b000-eeff42234ee0)

+   *Gemini* : Gemini，Alphabet：[`gemini.google.com/`](https://gemini.google.com/)

+   *GPT-4o* : GPT-4o，OpenAI：[`platform.openai.com/playground/chat?models=gpt-4o`](https://platform.openai.com/playground/chat?models=gpt-4o)

+   *IndiaAI_RAI* : “2023 年印度负责任 AI 状态，”Anjali Raja：[`indiaai.gov.in/research-reports/the-state-of-responsible-ai-in-india-2023`](https://indiaai.gov.in/research-reports/the-state-of-responsible-ai-in-india-2023)

+   *IndiaAI_RAI_Kit* : “NASSCOM 负责任 AI 资源包，”IndiaAI：[`indiaai.gov.in/responsible-ai/homepage`](https://indiaai.gov.in/responsible-ai/homepage)

+   *Ind_Law&Tech* : “平衡印度版权法与 AI 生成内容：‘重要人类输入’方法”，Harshal Chhabra, Kanishk Gaurav Pandey：[`www.ijlt.in/post/balancing-indian-copyright-law-with-ai-generated-content-the-significant-human-input-approach#:~:text=Novelty%20is%20considered%20too%20high,pre%2Dexisting%20bodies%20of%20knowledge`](https://www.ijlt.in/post/balancing-indian-copyright-law-with-ai-generated-content-the-significant-human-input-approach#:~:text=Novelty%20is%20considered%20too%20high,pre%2Dexisting%20bodies%20of%20knowledge) . ]

+   *LawAsia* : “分析韩国 AI 管理框架”，Hwan Kyoung Ko：[`law.asia/ai-regulatory-frameworks-south-korea/`](https://law.asia/ai-regulatory-frameworks-south-korea/)

+   *LawStackExchange* : “由生成 AI 贡献的代码的版权风险”，Dr Xorile, User6726, Dale M：[`law.stackexchange.com/questions/97621/copyright-risks-for-code-contributed-by-generative-ai`](https://law.stackexchange.com/questions/97621/copyright-risks-for-code-contributed-by-generative-ai)

+   *Lexology* : “台湾知识产权局发布解释以明确有关生成 AI 的版权争议”，Lee Tsai & Partners：[`www.lexology.com/library/detail.aspx?g=831aa3a8-5db4-4c6e-8988-a6e297614ba7`](https://www.lexology.com/library/detail.aspx?g=831aa3a8-5db4-4c6e-8988-a6e297614ba7)

+   *Liu_Medium* : “生成 AI 和版权法：谁拥有什么及您的知识产权权利”，Ginger Liu M.F.A.：[`medium.com/technology-hits/generative-ai-and-copyright-law-47aceb4ebb17`](https://medium.com/technology-hits/generative-ai-and-copyright-law-47aceb4ebb17)

+   *Monkey_Business* : “猴子的问题最终解决了：‘猴子自拍’争议”，Paulina Julia Perkal (IViR)：[`copyrightblog.kluweriplaw.com/2018/02/05/monkey-business-finally-settled-monkey-selfie-disputes/#:~:text=During%20the%20hearing%20in%20January,cannot%20be%20granted%20to%20animals`](https://copyrightblog.kluweriplaw.com/2018/02/05/monkey-business-finally-settled-monkey-selfie-disputes/#:~:text=During%20the%20hearing%20in%20January,cannot%20be%20granted%20to%20animals)

+   *OxfordUni* : “出口管制和研究合作”，Research Support：[`researchsupport.admin.ox.ac.uk/policy/export#:~:text=What%20items%20are%20controlled%3F,or%20their%20means%20of%20delivery`](https://researchsupport.admin.ox.ac.uk/policy/export#:~:text=What%20items%20are%20controlled%3F,or%20their%20means%20of%20delivery) .

+   *Reuters* : “联合国通过首个全球人工智能决议”，Alexandra Alper：[`www.reuters.com/technology/cybersecurity/un-adopts-first-global-artificial-intelligence-resolution-2024-03-21/`](https://www.reuters.com/technology/cybersecurity/un-adopts-first-global-artificial-intelligence-resolution-2024-03-21/)

+   *Wilmerhale* : “欧洲议会通过人工智能法案，”Krik J. Nahra, Arianna Evers, Ali A. Jessani, Martin Braun, Anne Vallery, Isiq Benizri: [`www.wilmerhale.com/en/insights/blogs/wilmerhale-privacy-and-cybersecurity-law/20240314-the-european-parliament-adopts-the-ai-act`](https://www.wilmerhale.com/en/insights/blogs/wilmerhale-privacy-and-cybersecurity-law/20240314-the-european-parliament-adopts-the-ai-act)

+   *Zenitech* : “人工智能生成代码的安全性分析，”Tautvydas Bakšys: [`zenitech.co.uk/insights/articles/security-analysis-of-ai-generated-code/`](https://zenitech.co.uk/insights/articles/security-analysis-of-ai-generated-code/)

+   *Zerotolive* : “代码是一种责任，”zerotolive: [`zerotolive.app/code-is-a-liability`](https://zerotolive.app/code-is-a-liability)
