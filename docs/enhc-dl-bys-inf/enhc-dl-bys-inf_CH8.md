# ç¬¬å…«ç« 

åº”ç”¨è´å¶æ–¯æ·±åº¦å­¦ä¹ 

æœ¬ç« å°†å¼•å¯¼ä½ äº†è§£è´å¶æ–¯æ·±åº¦å­¦ä¹ ï¼ˆBDLï¼‰çš„å¤šç§åº”ç”¨ã€‚è¿™äº›åº”ç”¨åŒ…æ‹¬ BDL åœ¨æ ‡å‡†åˆ†ç±»ä»»åŠ¡ä¸­çš„ä½¿ç”¨ï¼Œä»¥åŠå±•ç¤ºå¦‚ä½•åœ¨å¼‚å¸¸æ•°æ®æ£€æµ‹ã€æ•°æ®é€‰æ‹©å’Œå¼ºåŒ–å­¦ä¹ ç­‰æ›´å¤æ‚çš„ä»»åŠ¡ä¸­ä½¿ç”¨ BDLã€‚

æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­è®¨è®ºè¿™äº›ä¸»é¢˜ï¼š

+   æ£€æµ‹å¼‚å¸¸æ•°æ®

+   æé«˜å¯¹æ•°æ®é›†æ¼‚ç§»çš„é²æ£’æ€§

+   ä½¿ç”¨åŸºäºä¸ç¡®å®šæ€§çš„æ•°æ®æ˜¾ç¤ºé€‰æ‹©ï¼Œä¿æŒæ¨¡å‹çš„æ–°é²œåº¦

+   ä½¿ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡è¿›è¡Œæ›´æ™ºèƒ½çš„å¼ºåŒ–å­¦ä¹ 

+   å¯¹æŠ—æ€§è¾“å…¥çš„æ˜“æ„Ÿæ€§

## 8.1 æŠ€æœ¯è¦æ±‚

æœ¬ä¹¦çš„æ‰€æœ‰ä»£ç éƒ½å¯ä»¥åœ¨æœ¬ä¹¦çš„ GitHub ä»“åº“ä¸­æ‰¾åˆ°ï¼š[`github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference`](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference)

## 8.2 æ£€æµ‹å¼‚å¸¸æ•°æ®

å…¸å‹çš„ç¥ç»ç½‘ç»œåœ¨å¤„ç†å¼‚å¸¸æ•°æ®æ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬åœ¨*ç¬¬ä¸‰ç« *ã€*æ·±åº¦å­¦ä¹ åŸºç¡€*ä¸­çœ‹åˆ°ï¼ŒçŒ«ç‹—åˆ†ç±»å™¨å°†ä¸€å¼ é™è½ä¼çš„å›¾åƒé”™è¯¯åœ°åˆ†ç±»ä¸ºç‹—ï¼Œå¹¶ä¸”ç½®ä¿¡åº¦è¶…è¿‡ 99%ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•è§£å†³ç¥ç»ç½‘ç»œè¿™ä¸€å¼±ç‚¹ã€‚æˆ‘ä»¬å°†è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š

+   é€šè¿‡æ‰°åŠ¨`MNIST`æ•°æ®é›†ä¸­çš„ä¸€ä¸ªæ•°å­—ï¼Œç›´è§‚åœ°æ¢ç´¢è¿™ä¸ªé—®é¢˜

+   è§£é‡Šæ–‡çŒ®ä¸­é€šå¸¸å¦‚ä½•æŠ¥å‘Šå¼‚å¸¸æ•°æ®æ£€æµ‹çš„æ€§èƒ½

+   å›é¡¾æˆ‘ä»¬åœ¨æœ¬ç« ä¸­è®¨è®ºçš„å‡ ç§æ ‡å‡†å®ç”¨è´å¶æ–¯æ·±åº¦å­¦ä¹ ï¼ˆBDLï¼‰æ–¹æ³•åœ¨å¼‚å¸¸æ•°æ®æ£€æµ‹ä¸­çš„è¡¨ç°

+   æ¢ç´¢æ›´å¤šä¸“é—¨ç”¨äºå¼‚å¸¸æ•°æ®æ£€æµ‹çš„å®ç”¨æ–¹æ³•

### 8.2.1 æ¢ç´¢å¼‚å¸¸æ•°æ®æ£€æµ‹çš„é—®é¢˜

ä¸ºäº†æ›´å¥½åœ°å¸®åŠ©ä½ ç†è§£å¼‚å¸¸æ•°æ®æ£€æµ‹çš„æ•ˆæœï¼Œæˆ‘ä»¬å°†ä»ä¸€ä¸ªè§†è§‰ç¤ºä¾‹å¼€å§‹ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å°†è¦åšçš„äº‹æƒ…ï¼š

+   æˆ‘ä»¬å°†åœ¨`MNIST`æ•°å­—æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªæ ‡å‡†ç½‘ç»œ

+   ç„¶åï¼Œæˆ‘ä»¬å°†æ‰°åŠ¨ä¸€ä¸ªæ•°å­—ï¼Œå¹¶é€æ¸ä½¿å…¶å˜å¾—æ›´åŠ å¼‚å¸¸

+   æˆ‘ä»¬å°†æŠ¥å‘Šæ ‡å‡†æ¨¡å‹å’Œ MC dropout çš„ç½®ä¿¡åº¦å¾—åˆ†

é€šè¿‡è¿™ä¸ªè§†è§‰ç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç®€å•çš„è´å¶æ–¯æ–¹æ³•å¦‚ä½•åœ¨å¼‚å¸¸æ•°æ®æ£€æµ‹ä¸Šä¼˜äºæ ‡å‡†çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨`MNIST`æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªç®€å•çš„æ¨¡å‹ã€‚

![å›¾ç‰‡](img/file160.png)

å›¾ 8.1ï¼šMNIST æ•°æ®é›†çš„ç±»åˆ«ï¼šé›¶åˆ°ä¹çš„ 28x28 åƒç´ æ•°å­—å›¾åƒ

æˆ‘ä»¬ä½¿ç”¨`TensorFlow`æ¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨`numpy`è®©æˆ‘ä»¬çš„å›¾åƒæ›´å…·å¼‚å¸¸æ€§ï¼Œä½¿ç”¨`Matplotlib`æ¥å¯è§†åŒ–æ•°æ®ã€‚

```py

importÂ tensorflowÂ asÂ tf 
fromÂ tensorflow.kerasÂ importÂ datasets,Â layers,Â models 
importÂ numpyÂ asÂ np 
importÂ matplotlib.pyplotÂ asÂ plt
```

`MNIST`æ•°æ®é›†å¯ä»¥åœ¨ TensorFlow ä¸­æ‰¾åˆ°ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æ¥åŠ è½½å®ƒï¼š

```py

(train_images,Â train_labels),Â ( 
test_images, 
test_labels, 
)Â =Â datasets.mnist.load_data() 
train_images,Â test_imagesÂ =Â train_imagesÂ /Â 255.0,Â test_imagesÂ /Â 255.0
```

`MNIST`æ˜¯ä¸€ä¸ªç®€å•çš„æ•°æ®é›†ï¼Œå› æ­¤ä½¿ç”¨ç®€å•çš„æ¨¡å‹å¯ä»¥è®©æˆ‘ä»¬åœ¨æµ‹è¯•ä¸­è¾¾åˆ°è¶…è¿‡ 99%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ ‡å‡†çš„ CNNï¼ŒåŒ…å«ä¸‰å±‚å·ç§¯å±‚ï¼š

```py

defÂ get_model(): 
modelÂ =Â models.Sequential() 
model.add( 
layers.Conv2D(32,Â (3,Â 3),Â activation="relu",Â input_shape=(28,Â 28,Â 1)) 
) 
model.add(layers.MaxPooling2D((2,Â 2))) 
model.add(layers.Conv2D(64,Â (3,Â 3),Â activation="relu")) 
model.add(layers.MaxPooling2D((2,Â 2))) 
model.add(layers.Conv2D(64,Â (3,Â 3),Â activation="relu")) 
model.add(layers.Flatten()) 
model.add(layers.Dense(64,Â activation="relu")) 
model.add(layers.Dense(10)) 
returnÂ model 

modelÂ =Â get_model()
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç¼–è¯‘å¹¶è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚ç»è¿‡ 5 ä¸ª epochs åï¼Œæˆ‘ä»¬çš„éªŒè¯å‡†ç¡®ç‡è¶…è¿‡ 99%ã€‚

```py

defÂ fit_model(model): 
model.compile( 
optimizer="adam", 
loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), 
metrics=["accuracy"], 
) 

model.fit( 
train_images, 
train_labels, 
epochs=5, 
validation_data=(test_images,Â test_labels), 
) 
returnÂ model 

modelÂ =Â fit_model(model)
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªæ¨¡å‹å¦‚ä½•å¤„ç†åˆ†å¸ƒå¤–æ•°æ®ã€‚å‡è®¾æˆ‘ä»¬éƒ¨ç½²è¿™ä¸ªæ¨¡å‹æ¥è¯†åˆ«æ•°å­—ï¼Œä½†ç”¨æˆ·æœ‰æ—¶æ— æ³•å†™ä¸‹å®Œæ•´çš„æ•°å­—ã€‚å½“ç”¨æˆ·æ²¡æœ‰å†™ä¸‹å®Œæ•´çš„æ•°å­—æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿæˆ‘ä»¬å¯ä»¥é€šè¿‡é€æ¸ç§»é™¤æ•°å­—ä¸­çš„ä¿¡æ¯ï¼Œè§‚å¯Ÿæ¨¡å‹å¦‚ä½•å¤„ç†è¿™äº›æ‰°åŠ¨è¾“å…¥ï¼Œæ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥è¿™æ ·å®šä¹‰ç§»é™¤`signal`çš„å‡½æ•°ï¼š

```py

defÂ remove_signal(img:Â np.ndarray,Â num_lines:Â int)Â -*>*Â np.ndarray: 
imgÂ =Â img.copy() 
img[:num_lines]Â =Â 0 
Â Â Â returnÂ img
```

ç„¶åæˆ‘ä»¬å¯¹å›¾åƒè¿›è¡Œæ‰°åŠ¨ï¼š

```py

imgsÂ =Â [] 
forÂ iÂ inÂ range(28): 
img_perturbedÂ =Â remove_signal(img,Â i) 
ifÂ np.array_equal(img,Â img_perturbed): 
continue 
imgs.append(img_perturbed) 
ifÂ img_perturbed.sum()Â ==Â 0: 
Â Â Â Â Â break
```

æˆ‘ä»¬åªæœ‰åœ¨å°†æŸä¸€è¡Œè®¾ä¸º 0 ç¡®å®æ”¹å˜äº†åŸå§‹å›¾åƒæ—¶ï¼Œæ‰å°†æ‰°åŠ¨åçš„å›¾åƒæ·»åŠ åˆ°æˆ‘ä»¬çš„å›¾åƒåˆ—è¡¨ä¸­ï¼ˆ`if np.array_equal(img, img_perturbed))`ï¼‰ï¼Œå¹¶ä¸”ä¸€æ—¦å›¾åƒå®Œå…¨å˜é»‘ï¼Œå³ä»…åŒ…å«å€¼ä¸º 0 çš„åƒç´ ï¼Œæˆ‘ä»¬å°±åœæ­¢ã€‚æˆ‘ä»¬å¯¹è¿™äº›å›¾åƒè¿›è¡Œæ¨ç†ï¼š

```py

softmax_predictionsÂ =Â tf.nn.softmax(model(np.expand_dims(imgs,Â -1)),Â axis=1)
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶æ‰€æœ‰å›¾åƒåŠå…¶é¢„æµ‹æ ‡ç­¾å’Œç½®ä¿¡åº¦åˆ†æ•°ï¼š

```py

plt.figure(figsize=(10,Â 10)) 
bbox_dictÂ =Â dict( 
fill=True,Â facecolor="white",Â alpha=0.5,Â edgecolor="white",Â linewidth=0 
) 
forÂ iÂ inÂ range(len(imgs)): 
plt.subplot(5,Â 5,Â iÂ +Â 1) 
plt.xticks([]) 
plt.yticks([]) 
plt.grid(False) 
plt.imshow(imgs[i],Â cmap="gray") 
predictionÂ =Â softmax_predictions[i].numpy().max() 
labelÂ =Â np.argmax(softmax_predictions[i]) 
plt.xlabel(f"{label}Â -Â {prediction:.2%}") 
plt.text(0,Â 3,Â f"Â {i+1}",Â bbox=bbox_dict) 
plt.show()
```

è¿™ç”Ÿæˆäº†å¦‚ä¸‹çš„å›¾ï¼š

![PIC](img/file161.png)

å›¾ 8.2ï¼šæ ‡å‡†ç¥ç»ç½‘ç»œå¯¹äºé€æ¸åç¦»åˆ†å¸ƒçš„å›¾åƒæ‰€é¢„æµ‹çš„æ ‡ç­¾åŠç›¸åº”çš„ softmax åˆ†æ•°

æˆ‘ä»¬å¯ä»¥åœ¨*å›¾* *8.2*ä¸­çœ‹åˆ°ï¼Œæœ€åˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹éå¸¸è‡ªä¿¡åœ°å°†å›¾åƒåˆ†ç±»ä¸º`2`ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿åœ¨è¿™ç§åˆ†ç±»æ˜¾å¾—ä¸åˆç†æ—¶ï¼Œè¿™ç§è‡ªä¿¡ä¾ç„¶å­˜åœ¨ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹ä»ç„¶ä»¥ 97.83%çš„ç½®ä¿¡åº¦å°†å›¾åƒ 14 åˆ†ç±»ä¸º`2`ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜é¢„æµ‹å®Œå…¨æ°´å¹³çš„çº¿æ¡æ˜¯`1`ï¼Œç½®ä¿¡åº¦ä¸º 92.32%ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å›¾åƒ 17 ä¸­æ‰€è§ã€‚è¿™çœ‹èµ·æ¥æˆ‘ä»¬çš„æ¨¡å‹åœ¨é¢„æµ‹æ—¶è¿‡äºè‡ªä¿¡ã€‚

è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªç¨æœ‰ä¸åŒçš„æ¨¡å‹ä¼šå¦‚ä½•å¯¹è¿™äº›å›¾åƒåšå‡ºé¢„æµ‹ã€‚æˆ‘ä»¬ç°åœ¨å°†ä½¿ç”¨ MC Dropout ä½œä¸ºæˆ‘ä»¬çš„æ¨¡å‹ã€‚é€šè¿‡é‡‡æ ·ï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿæé«˜æ¨¡å‹çš„ä¸ç¡®å®šæ€§ï¼Œç›¸è¾ƒäºæ ‡å‡†çš„ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å…ˆå®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ï¼š

```py

defÂ get_dropout_model(): 
modelÂ =Â models.Sequential() 
model.add( 
layers.Conv2D(32,Â (3,Â 3),Â activation="relu",Â input_shape=(28,Â 28,Â 1)) 
) 
model.add(layers.Dropout(0.2)) 
model.add(layers.MaxPooling2D((2,Â 2))) 
model.add(layers.Conv2D(64,Â (3,Â 3),Â activation="relu")) 
model.add(layers.MaxPooling2D((2,Â 2))) 
model.add(layers.Dropout(0.5)) 
model.add(layers.Conv2D(64,Â (3,Â 3),Â activation="relu")) 
model.add(layers.Dropout(0.5)) 
model.add(layers.Flatten()) 
model.add(layers.Dense(64,Â activation="relu")) 
model.add(layers.Dropout(0.5)) 
model.add(layers.Dense(10)) 
Â Â Â Â returnÂ model
```

é‚£ä¹ˆæˆ‘ä»¬æ¥å®ä¾‹åŒ–å®ƒï¼š

```py

dropout_modelÂ =Â get_dropout_model() 
dropout_modelÂ =Â fit_model(dropout_model)
```

ä½¿ç”¨ dropout çš„æ¨¡å‹å°†å®ç°ä¸åŸå§‹æ¨¡å‹ç±»ä¼¼çš„å‡†ç¡®æ€§ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬ä½¿ç”¨ dropout è¿›è¡Œæ¨ç†ï¼Œå¹¶ç»˜åˆ¶ MC Dropout çš„å¹³å‡ç½®ä¿¡åº¦åˆ†æ•°ï¼š

```py

PredictionsÂ =Â np.array( 
[ 
tf.nn.softmax(dropout_model(imgs_np,Â training=True),Â axis=1) 
forÂ _Â inÂ range(100) 
] 
) 
Predictions_meanÂ =Â np.mean(predictions,Â axis=0) 
plot_predictions(predictions_mean)
```

è¿™å†æ¬¡ç”Ÿæˆäº†ä¸€ä¸ªå›¾ï¼Œæ˜¾ç¤ºäº†é¢„æµ‹æ ‡ç­¾åŠå…¶ç›¸å…³çš„ç½®ä¿¡åº¦åˆ†æ•°ï¼š

![PIC](img/file162.png)

å›¾ 8.3ï¼šMC Dropout ç½‘ç»œå¯¹äºé€æ¸åç¦»åˆ†å¸ƒçš„å›¾åƒæ‰€é¢„æµ‹çš„æ ‡ç­¾åŠç›¸åº”çš„ softmax åˆ†æ•°

æˆ‘ä»¬å¯ä»¥åœ¨*å›¾* *8.3*ä¸­çœ‹åˆ°ï¼Œæ¨¡å‹çš„è‡ªä¿¡åº¦å¹³å‡æ¥è¯´è¾ƒä½ã€‚å½“æˆ‘ä»¬ä»å›¾åƒä¸­ç§»é™¤è¡Œæ—¶ï¼Œæ¨¡å‹çš„ç½®ä¿¡åº¦å¤§å¹…ä¸‹é™ã€‚è¿™æ˜¯æœŸæœ›çš„è¡Œä¸ºï¼šå½“æ¨¡å‹ä¸çŸ¥é“è¾“å…¥æ—¶ï¼Œå®ƒåº”è¯¥è¡¨ç°å‡ºä¸ç¡®å®šæ€§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿèƒ½çœ‹åˆ°æ¨¡å‹å¹¶ä¸å®Œç¾ï¼š

+   å¯¹äºé‚£äº›çœ‹èµ·æ¥å¹¶ä¸åƒ`2`çš„å›¾åƒï¼Œæ¨¡å‹ä»ç„¶ä¿æŒè¾ƒé«˜çš„ç½®ä¿¡åº¦ã€‚

+   å½“æˆ‘ä»¬ä»å›¾åƒä¸­åˆ é™¤ä¸€è¡Œæ—¶ï¼Œæ¨¡å‹çš„ç½®ä¿¡åº¦å˜åŒ–å¾ˆå¤§ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹çš„ç½®ä¿¡åº¦åœ¨å›¾åƒ 14 å’Œå›¾åƒ 15 ä¹‹é—´ä» 61.72%è·ƒå‡è‡³ 37.20%ã€‚

+   æ¨¡å‹ä¼¼ä¹æ›´æœ‰ä¿¡å¿ƒå°†æ²¡æœ‰ä»»ä½•ç™½è‰²åƒç´ çš„å›¾åƒ 20 åˆ†ç±»ä¸º`1`ã€‚

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒMC Dropout æ˜¯ä¸€ä¸ªæœç€æ­£ç¡®æ–¹å‘è¿ˆå‡ºçš„æ­¥éª¤ï¼Œä½†å®ƒå¹¶æ²¡æœ‰å®Œç¾åœ°å¤„ç†åˆ†å¸ƒå¤–æ•°æ®ã€‚

### 8.2.2 ç³»ç»Ÿåœ°è¯„ä¼° OOD æ£€æµ‹æ€§èƒ½

ä¸Šè¿°ç¤ºä¾‹è¡¨æ˜ï¼ŒMC dropout é€šå¸¸ä¼šç»™å‡ºåˆ†å¸ƒå¤–å›¾åƒè¾ƒä½çš„ç½®ä¿¡åº¦åˆ†æ•°ã€‚ä½†æˆ‘ä»¬ä»…è¯„ä¼°äº† 20 å¼ å›¾åƒï¼Œä¸”å˜åŒ–æœ‰é™â€”â€”æˆ‘ä»¬åªæ˜¯åˆ é™¤äº†ä¸€è¡Œã€‚è¿™ä¸€å˜åŒ–ä½¿å¾—å›¾åƒæ›´åŠ åˆ†å¸ƒå¤–ï¼Œä½†å‰ä¸€éƒ¨åˆ†å±•ç¤ºçš„æ‰€æœ‰å›¾åƒä¸`MNIST`çš„è®­ç»ƒåˆ†å¸ƒç›¸æ¯”ï¼Œè¿˜æ˜¯ç›¸å¯¹ç›¸ä¼¼çš„ï¼Œå¦‚æœæ‹¿å®ƒå’Œè‡ªç„¶ç‰©ä½“å›¾åƒæ¯”è¾ƒã€‚ä¾‹å¦‚ï¼Œé£æœºã€æ±½è½¦æˆ–é¸Ÿç±»çš„å›¾åƒè‚¯å®šæ¯”å¸¦æœ‰å‡ è¡Œé»‘è‰²çš„`MNIST`å›¾åƒæ›´å…·åˆ†å¸ƒå¤–ç‰¹å¾ã€‚å› æ­¤ï¼Œä¼¼ä¹åˆç†çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¯„ä¼°æ¨¡å‹çš„ OOD æ£€æµ‹æ€§èƒ½ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨æ›´åŠ åˆ†å¸ƒå¤–çš„å›¾åƒä¸Šè¿›è¡Œæµ‹è¯•ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¥è‡ªå®Œå…¨ä¸åŒæ•°æ®é›†çš„å›¾åƒã€‚è¿™æ­£æ˜¯æ–‡çŒ®ä¸­é€šå¸¸ç”¨äºè¯„ä¼°åˆ†å¸ƒå¤–æ£€æµ‹æ€§èƒ½çš„æ–¹æ³•ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

1.  æˆ‘ä»¬åœ¨å†…éƒ¨åˆ†å¸ƒï¼ˆIDï¼‰å›¾åƒä¸Šè®­ç»ƒæ¨¡å‹ã€‚

1.  æˆ‘ä»¬é€‰å–ä¸€ä¸ªæˆ–å¤šä¸ªå®Œå…¨ä¸åŒçš„ OOD æ•°æ®é›†ï¼Œå¹¶å°†è¿™äº›æ•°æ®å–‚ç»™æˆ‘ä»¬çš„æ¨¡å‹ã€‚

1.  æˆ‘ä»¬ç°åœ¨å°†æ¨¡å‹åœ¨ ID å’Œ OOD æµ‹è¯•æ•°æ®é›†ä¸Šçš„é¢„æµ‹è§†ä¸ºä¸€ä¸ªäºŒè¿›åˆ¶é—®é¢˜ï¼Œå¹¶ä¸ºæ¯ä¸ªå›¾åƒè®¡ç®—ä¸€ä¸ªå•ä¸€çš„å¾—åˆ†ã€‚

    +   åœ¨è¯„ä¼° softmax åˆ†æ•°çš„æƒ…å†µä¸‹ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬ä¸ºæ¯ä¸ª ID å’Œ OOD å›¾åƒå–æ¨¡å‹çš„æœ€å¤§ softmax åˆ†æ•°ã€‚

1.  ä½¿ç”¨è¿™äº›å¾—åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—äºŒè¿›åˆ¶æŒ‡æ ‡ï¼Œå¦‚æ¥æ”¶è€…æ“ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰ã€‚

æ¨¡å‹åœ¨è¿™äº›äºŒè¿›åˆ¶æŒ‡æ ‡ä¸Šçš„è¡¨ç°è¶Šå¥½ï¼Œæ¨¡å‹çš„ OOD æ£€æµ‹æ€§èƒ½å°±è¶Šå¥½ã€‚

### 8.2.3 æ— éœ€é‡æ–°è®­ç»ƒçš„ç®€å•åˆ†å¸ƒå¤–æ£€æµ‹

å°½ç®¡ MC dropout å¯ä»¥æœ‰æ•ˆæ£€æµ‹å‡ºåˆ†å¸ƒå¤–æ•°æ®ï¼Œä½†å®ƒåœ¨æ¨ç†æ—¶å­˜åœ¨ä¸€ä¸ªä¸»è¦ç¼ºç‚¹ï¼šæˆ‘ä»¬éœ€è¦è¿›è¡Œäº”æ¬¡ï¼Œç”šè‡³ä¸€ç™¾æ¬¡æ¨ç†ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¸€æ¬¡ã€‚å¯¹äºæŸäº›å…¶ä»–è´å¶æ–¯æ·±åº¦å­¦ä¹ æ–¹æ³•ä¹Ÿå¯ä»¥è¯´ç±»ä¼¼ï¼šè™½ç„¶å®ƒä»¬æœ‰ç†è®ºä¾æ®ï¼Œä½†å¹¶ä¸æ€»æ˜¯è·å¾—è‰¯å¥½ OOD æ£€æµ‹æ€§èƒ½çš„æœ€å®é™…æ–¹æ³•ã€‚ä¸»è¦çš„ç¼ºç‚¹æ˜¯ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦é‡æ–°è®­ç»ƒç½‘ç»œï¼Œå¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œè¿™å¯èƒ½ä¼šéå¸¸æ˜‚è´µã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæœ‰ä¸€æ•´å¥—ä¸æ˜¾å¼ä¾èµ–è´å¶æ–¯ç†è®ºçš„ OOD æ£€æµ‹æ–¹æ³•ï¼Œä½†èƒ½æä¾›è‰¯å¥½ã€ç®€å•ï¼Œç”šè‡³æ˜¯ä¼˜ç§€çš„åŸºçº¿ã€‚è¿™äº›æ–¹æ³•é€šå¸¸ä¸éœ€è¦ä»»ä½•é‡æ–°è®­ç»ƒï¼Œå¯ä»¥ç›´æ¥åœ¨æ ‡å‡†ç¥ç»ç½‘ç»œä¸Šåº”ç”¨ã€‚æ–‡çŒ®ä¸­ç»å¸¸ä½¿ç”¨çš„ä¸¤ç§æ–¹æ³•å€¼å¾—ä¸€æï¼š

+   **ODIN**ï¼šä½¿ç”¨é¢„å¤„ç†å’Œç¼©æ”¾è¿›è¡Œ OOD æ£€æµ‹

+   **é©¬å“ˆæ‹‰è¯ºæ¯”æ–¯**ï¼šä½¿ç”¨ä¸­é—´ç‰¹å¾è¿›è¡Œ OOD æ£€æµ‹

#### ODINï¼šä½¿ç”¨é¢„å¤„ç†å’Œç¼©æ”¾è¿›è¡Œ OOD æ£€æµ‹

`O`ut-of-*DI*stribution æ£€æµ‹å™¨ï¼ˆODINï¼‰æ˜¯å®é™…åº”ç”¨ä¸­å¸¸ç”¨çš„æ ‡å‡†åˆ†å¸ƒå¤–æ£€æµ‹æ–¹æ³•ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒç®€å•æœ‰æ•ˆã€‚å°½ç®¡è¯¥æ–¹æ³•åœ¨ 2017 å¹´è¢«æå‡ºï¼Œä½†å®ƒä»ç„¶ç»å¸¸ä½œä¸ºæå‡ºåˆ†å¸ƒå¤–æ£€æµ‹æ–¹æ³•çš„è®ºæ–‡ä¸­çš„å¯¹æ¯”æ–¹æ³•ã€‚

ODIN åŒ…å«ä¸¤ä¸ªå…³é”®æ€æƒ³ï¼š

+   å¯¹ logit åˆ†æ•°è¿›è¡Œ**æ¸©åº¦ç¼©æ”¾**ï¼Œç„¶åå†åº”ç”¨ softmax æ“ä½œï¼Œä»¥æé«˜ softmax åˆ†æ•°åŒºåˆ†åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–å›¾åƒçš„èƒ½åŠ›ã€‚

+   **è¾“å…¥é¢„å¤„ç†** ä½¿åˆ†å¸ƒå†…å›¾åƒæ›´ç¬¦åˆåˆ†å¸ƒå†…

è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°çœ‹çœ‹è¿™ä¸¤ä¸ªæ€æƒ³ã€‚

æ¸©åº¦ç¼©æ”¾ ODIN é€‚ç”¨äºåˆ†ç±»æ¨¡å‹ã€‚ç»™å®šæˆ‘ä»¬è®¡ç®—çš„ softmax åˆ†æ•°å¦‚ä¸‹ï¼š

![pi(x) = âˆ‘--exp(fi(x))--- Nj=1 exp(fj(x)) ](img/file163.jpg)

åœ¨è¿™é‡Œï¼Œ`f``i` æ˜¯å•ä¸ª logit è¾“å‡ºï¼Œ`f``j` æ˜¯å•ä¸ªç¤ºä¾‹ä¸­æ‰€æœ‰ç±»åˆ«çš„ logitsï¼Œæ¸©åº¦ç¼©æ”¾æ„å‘³ç€æˆ‘ä»¬å°†è¿™äº› logit è¾“å‡ºé™¤ä»¥å¸¸æ•° `T`ï¼š

![ exp(f (x)âˆ•T) pi(x; T) = âˆ‘N------i---------- j=1 exp (fj(x)âˆ•T) ](img/file164.jpg)

å¯¹äºè¾ƒå¤§çš„ `T` å€¼ï¼Œæ¸©åº¦ç¼©æ”¾ä½¿å¾— softmax åˆ†æ•°æ›´æ¥è¿‘å‡åŒ€åˆ†å¸ƒï¼Œä»è€Œæœ‰åŠ©äºå‡å°‘è¿‡äºè‡ªä¿¡çš„é¢„æµ‹ã€‚

æˆ‘ä»¬å¯ä»¥åœ¨ Python ä¸­åº”ç”¨æ¸©åº¦ç¼©æ”¾ï¼Œå‡è®¾æœ‰ä¸€ä¸ªç®€å•çš„æ¨¡å‹è¾“å‡º logitsï¼š

```py

logitsÂ =Â model.predict(images) 
logits_scaledÂ =Â logitsÂ /Â temperature 
softmaxÂ =Â tf.nn.softmax(logits,Â axis=1)
```

è¾“å…¥é¢„å¤„ç† æˆ‘ä»¬åœ¨ *ç¬¬ä¸‰ç« *ï¼Œ*æ·±åº¦å­¦ä¹ åŸºç¡€* ä¸­çœ‹åˆ°ï¼Œ**å¿«é€Ÿæ¢¯åº¦** **ç¬¦å·æ–¹æ³•**ï¼ˆ**FGSM**ï¼‰ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ¬ºéª—ç¥ç»ç½‘ç»œã€‚é€šè¿‡ç¨å¾®æ”¹å˜ä¸€å¼ çŒ«çš„å›¾åƒï¼Œæˆ‘ä»¬å¯ä»¥è®©æ¨¡å‹ä»¥ 99.41% çš„ç½®ä¿¡åº¦é¢„æµ‹ä¸ºâ€œç‹—â€ã€‚è¿™é‡Œçš„æƒ³æ³•æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥è·å–æŸå¤±ç›¸å¯¹äºè¾“å…¥çš„æ¢¯åº¦ç¬¦å·ï¼Œå°†å…¶ä¹˜ä»¥ä¸€ä¸ªå°å€¼ï¼Œå¹¶å°†è¯¥å™ªå£°æ·»åŠ åˆ°å›¾åƒä¸­â€”â€”è¿™å°†æŠŠæˆ‘ä»¬çš„å›¾åƒä»åˆ†å¸ƒå†…ç±»åˆ«ä¸­ç§»åŠ¨ã€‚é€šè¿‡åšç›¸åçš„äº‹æƒ…ï¼Œå³ä»å›¾åƒä¸­å‡å»å™ªå£°ï¼Œæˆ‘ä»¬ä½¿å¾—å›¾åƒæ›´æ¥è¿‘åˆ†å¸ƒå†…ã€‚ODIN è®ºæ–‡çš„ä½œè€…è¡¨æ˜ï¼Œè¿™å¯¼è‡´åˆ†å¸ƒå†…å›¾åƒçš„ softmax åˆ†æ•°æ¯”åˆ†å¸ƒå¤–å›¾åƒæ›´é«˜ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¢åŠ äº† OOD å’Œ ID softmax åˆ†æ•°ä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œæé«˜äº† OOD æ£€æµ‹æ€§èƒ½ã€‚

![Ëœx = x âˆ’ ğœ€sign(âˆ’ âˆ‡x log SË†y(x;T)) ](img/file165.jpg)

å…¶ä¸­ `x` æ˜¯è¾“å…¥å›¾åƒï¼Œæˆ‘ä»¬ä»ä¸­å‡å»æ‰°åŠ¨å¹…åº¦ *ğœ–* ä¹˜ä»¥äº¤å‰ç†µæŸå¤±ç›¸å¯¹äºè¾“å…¥çš„æ¢¯åº¦ç¬¦å·ã€‚æœ‰å…³è¯¥æŠ€æœ¯çš„ TensorFlow å®ç°ï¼Œè¯·å‚è§ *ç¬¬ä¸‰ç« *ï¼Œ*æ·±åº¦å­¦ä¹ åŸºç¡€*ã€‚

å°½ç®¡è¾“å…¥é¢„å¤„ç†å’Œæ¸©åº¦ç¼©æ”¾æ˜“äºå®ç°ï¼ŒODIN ç°åœ¨è¿˜éœ€è¦è°ƒèŠ‚ä¸¤ä¸ªè¶…å‚æ•°ï¼šç”¨äºç¼©æ”¾ logits çš„æ¸©åº¦å’Œ *ğœ–*ï¼ˆå¿«é€Ÿæ¢¯åº¦ç¬¦å·æ³•çš„é€†ï¼‰ã€‚ODIN ä½¿ç”¨ä¸€ä¸ªå•ç‹¬çš„åˆ†å¸ƒå¤–æ•°æ®é›†æ¥è°ƒèŠ‚è¿™äº›è¶…å‚æ•°ï¼ˆiSUN æ•°æ®é›†çš„éªŒè¯é›†ï¼š8925 å¼ å›¾åƒï¼‰ã€‚

#### é©¬æ°è·ç¦»ï¼šä½¿ç”¨ä¸­é—´ç‰¹å¾è¿›è¡Œ OOD æ£€æµ‹

åœ¨ã€Š*ä¸€ç§ç®€å•ç»Ÿä¸€çš„æ¡†æ¶ç”¨äºæ£€æµ‹åˆ†å¸ƒå¤–æ ·æœ¬å’Œ* *å¯¹æŠ—æ”»å‡»*ã€‹ä¸€æ–‡ä¸­ï¼ŒKimin Lee ç­‰äººæå‡ºäº†ä¸€ç§æ£€æµ‹ OOD è¾“å…¥çš„ä¸åŒæ–¹æ³•ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ¯ä¸ªç±»åˆ«çš„åˆ†ç±»å™¨åœ¨ç½‘ç»œçš„ç‰¹å¾ç©ºé—´ä¸­éµå¾ªå¤šå…ƒé«˜æ–¯åˆ†å¸ƒã€‚åŸºäºè¿™ä¸€æ€æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰`C`ä¸ªç±»åˆ«æ¡ä»¶é«˜æ–¯åˆ†å¸ƒï¼Œå¹¶ä¸”å…·æœ‰å…±äº«çš„åæ–¹å·® *Ïƒ*ï¼š

![P(f(x) | y = c) = ğ’© (f(x) | Î¼c,Ïƒ ) ](img/file166.jpg)

å…¶ä¸­ *Î¼*[`c`] æ˜¯æ¯ä¸ªç±»åˆ« `c` çš„å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿè®¡ç®—ç»™å®šä¸­é—´å±‚è¾“å‡ºçš„æ¯ä¸ªç±»åˆ«çš„ç»éªŒå‡å€¼å’Œåæ–¹å·®ã€‚åŸºäºå‡å€¼å’Œåæ–¹å·®ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å•ä¸ªæµ‹è¯•å›¾åƒä¸åˆ†å¸ƒå†…æ•°æ®çš„é©¬æ°è·ç¦»ã€‚æˆ‘ä»¬å¯¹ä¸è¾“å…¥å›¾åƒæœ€æ¥è¿‘çš„ç±»åˆ«è®¡ç®—è¯¥è·ç¦»ï¼š

![M (x) = max âˆ’ (f(x)âˆ’ ^Î¼c)âŠ¤ ^Ïƒâˆ’ 1(f(x)âˆ’ ^Î¼c) c ](img/file167.jpg)

å¯¹äºåˆ†å¸ƒå†…çš„å›¾åƒï¼Œè¿™ä¸ªè·ç¦»åº”è¯¥è¾ƒå°ï¼Œè€Œå¯¹äºåˆ†å¸ƒå¤–çš„å›¾åƒï¼Œè¿™ä¸ªè·ç¦»åº”è¯¥è¾ƒå¤§ã€‚

`numpy` æä¾›äº†æ–¹ä¾¿çš„å‡½æ•°æ¥è®¡ç®—æ•°ç»„çš„å‡å€¼å’Œåæ–¹å·®ï¼š

```py

meanÂ =Â np.mean(features_of_class,Â axis=0) 
covarianceÂ =Â np.cov(features_of_class.T)
```

åŸºäºè¿™äº›ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼è®¡ç®—é©¬æ°è·ç¦»ï¼š

```py

covariance_inverseÂ =Â np.linalg.pinv(covariance) 
x_minus_muÂ =Â features_of_classÂ -Â mean 
mahalanobisÂ =Â np.dot(x_minus_mu,Â covariance_inverse).dot(x_minus_mu.T) 
mahalanobisÂ =Â np.sqrt(mahalanobis).diagonal()
```

é©¬æ°è·ç¦»è®¡ç®—ä¸éœ€è¦ä»»ä½•é‡æ–°è®­ç»ƒï¼Œä¸€æ—¦ä½ å­˜å‚¨äº†ç½‘ç»œæŸä¸€å±‚ç‰¹å¾çš„å‡å€¼å’Œï¼ˆåæ–¹å·®çš„é€†çŸ©é˜µï¼‰ï¼Œè¿™æ˜¯ä¸€é¡¹ç›¸å¯¹å»‰ä»·çš„æ“ä½œã€‚

ä¸ºäº†æé«˜æ–¹æ³•çš„æ€§èƒ½ï¼Œä½œè€…è¡¨æ˜æˆ‘ä»¬è¿˜å¯ä»¥åº”ç”¨ ODIN è®ºæ–‡ä¸­æåˆ°çš„è¾“å…¥é¢„å¤„ç†ï¼Œæˆ–è€…è®¡ç®—å¹¶å¹³å‡ä»ç½‘ç»œå¤šä¸ªå±‚æå–çš„é©¬æ°è·ç¦»ã€‚

## 8.3 æŠµæŠ—æ•°æ®é›†åç§»

æˆ‘ä»¬åœ¨*ç¬¬ä¸‰ç« *ã€Š*æ·±åº¦å­¦ä¹ åŸºç¡€*ã€‹ä¸­å·²ç»é‡åˆ°è¿‡æ•°æ®é›†åç§»ã€‚æé†’ä¸€ä¸‹ï¼Œæ•°æ®é›†åç§»æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼Œå‘ç”Ÿåœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µå’Œæ¨¡å‹æ¨ç†é˜¶æ®µï¼ˆä¾‹å¦‚ï¼Œåœ¨æµ‹è¯•æ¨¡å‹æˆ–åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡Œæ—¶ï¼‰è¾“å…¥ `X` å’Œè¾“å‡º `Y` çš„è”åˆåˆ†å¸ƒ `P`(*X,Y*) ä¸åŒçš„æƒ…å†µä¸‹ã€‚åå˜é‡åç§»æ˜¯æ•°æ®é›†åç§»çš„ä¸€ä¸ªç‰¹å®šæ¡ˆä¾‹ï¼Œå…¶ä¸­åªæœ‰è¾“å…¥çš„åˆ†å¸ƒå‘ç”Ÿå˜åŒ–ï¼Œè€Œæ¡ä»¶åˆ†å¸ƒ `P`(`Y` |`X`) ä¿æŒä¸å˜ã€‚

æ•°æ®é›†åç§»åœ¨å¤§å¤šæ•°ç”Ÿäº§ç¯å¢ƒä¸­æ™®éå­˜åœ¨ï¼Œå› ä¸ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾ˆéš¾åŒ…å«æ‰€æœ‰å¯èƒ½çš„æ¨ç†æ¡ä»¶ï¼Œè€Œä¸”å¤§å¤šæ•°æ•°æ®ä¸æ˜¯é™æ€çš„ï¼Œè€Œæ˜¯éšç€æ—¶é—´å‘ç”Ÿå˜åŒ–ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¾“å…¥æ•°æ®å¯èƒ½æ²¿ç€è®¸å¤šä¸åŒçš„ç»´åº¦å‘ç”Ÿåç§»ã€‚åœ°ç†å’Œæ—¶é—´æ•°æ®é›†åç§»æ˜¯ä¸¤ç§å¸¸è§çš„åç§»å½¢å¼ã€‚ä¾‹å¦‚ï¼Œå‡è®¾ä½ å·²åœ¨ä¸€ä¸ªåœ°ç†åŒºåŸŸï¼ˆä¾‹å¦‚æ¬§æ´²ï¼‰è·å¾—çš„æ•°æ®ä¸Šè®­ç»ƒäº†æ¨¡å‹ï¼Œç„¶åå°†æ¨¡å‹åº”ç”¨äºå¦ä¸€ä¸ªåœ°ç†åŒºåŸŸï¼ˆä¾‹å¦‚æ‹‰ä¸ç¾æ´²ï¼‰ã€‚ç±»ä¼¼åœ°ï¼Œæ¨¡å‹å¯èƒ½æ˜¯åœ¨ 2010 åˆ° 2020 å¹´é—´çš„æ•°æ®ä¸Šè®­ç»ƒçš„ï¼Œç„¶ååº”ç”¨äºä»Šå¤©çš„ç”Ÿäº§æ•°æ®ã€‚

æˆ‘ä»¬å°†çœ‹åˆ°ï¼Œåœ¨è¿™æ ·çš„æ•°æ®åç§»åœºæ™¯ä¸­ï¼Œæ¨¡å‹åœ¨æ–°çš„åç§»æ•°æ®ä¸Šçš„è¡¨ç°é€šå¸¸æ¯”åœ¨åŸå§‹è®­ç»ƒåˆ†å¸ƒä¸Šçš„è¡¨ç°å·®ã€‚æˆ‘ä»¬è¿˜å°†çœ‹åˆ°ï¼Œæ™®é€šç¥ç»ç½‘ç»œé€šå¸¸æ— æ³•æŒ‡ç¤ºè¾“å…¥æ•°æ®ä½•æ—¶åç¦»è®­ç»ƒåˆ†å¸ƒã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ¢è®¨æœ¬ä¹¦ä¸­ä»‹ç»çš„å„ç§æ–¹æ³•å¦‚ä½•é€šè¿‡ä¸ç¡®å®šæ€§ä¼°è®¡æ¥æŒ‡ç¤ºæ•°æ®é›†åç§»ï¼Œä»¥åŠè¿™äº›æ–¹æ³•å¦‚ä½•å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚ä»¥ä¸‹ä»£ç ç¤ºä¾‹å°†é›†ä¸­åœ¨å›¾åƒåˆ†ç±»é—®é¢˜ä¸Šã€‚ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›è§è§£é€šå¸¸å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–é¢†åŸŸï¼ˆå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼‰å’Œä»»åŠ¡ï¼ˆå¦‚å›å½’ï¼‰ã€‚

### 8.3.1 æµ‹é‡æ¨¡å‹å¯¹æ•°æ®é›†åç§»çš„å“åº”

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†å’Œä¸€ä¸ªå•ç‹¬çš„æµ‹è¯•é›†ï¼Œæˆ‘ä»¬å¦‚ä½•è¡¡é‡æ¨¡å‹åœ¨æ•°æ®å‘ç”Ÿåç§»æ—¶æ˜¯å¦èƒ½åŠæ—¶ååº”ï¼Ÿä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé¢å¤–çš„æµ‹è¯•é›†ï¼Œå…¶ä¸­æ•°æ®å·²ç»å‘ç”Ÿåç§»ï¼Œä»¥æ£€æŸ¥æ¨¡å‹å¦‚ä½•å“åº”æ•°æ®é›†åç§»ã€‚ä¸€ä¸ªå¸¸ç”¨çš„åˆ›å»ºæ•°æ®åç§»æµ‹è¯•é›†çš„æ–¹æ³•æœ€åˆç”± Dan Hendrycksã€Thomas Dietterich åŠå…¶ä»–äººäº 2019 å¹´æå‡ºã€‚è¿™ä¸ªæ–¹æ³•å¾ˆç®€å•ï¼šä»ä½ çš„åˆå§‹æµ‹è¯•é›†ä¸­å–å‡ºå›¾åƒï¼Œç„¶åå¯¹å…¶åº”ç”¨ä¸åŒç¨‹åº¦çš„å›¾åƒè´¨é‡æŸåã€‚Hendrycks å’Œ Dietterich æå‡ºäº†ä¸€å¥—åŒ…å« 15 ç§ä¸åŒç±»å‹å›¾åƒè´¨é‡æŸåçš„æ–¹æ³•ï¼Œæ¶µç›–äº†å›¾åƒå™ªå£°ã€æ¨¡ç³Šã€å¤©æ°”æŸåï¼ˆå¦‚é›¾éœ¾å’Œé›ªï¼‰ä»¥åŠæ•°å­—æŸåç­‰ç±»å‹ã€‚æ¯ç§æŸåç±»å‹éƒ½æœ‰äº”ä¸ªä¸¥é‡ç¨‹åº¦çº§åˆ«ï¼Œä» 1ï¼ˆè½»åº¦æŸåï¼‰åˆ° 5ï¼ˆä¸¥é‡æŸåï¼‰ã€‚*å›¾* *8.4*å±•ç¤ºäº†ä¸€åªå°çŒ«çš„å›¾åƒæœ€åˆæ ·å­ï¼ˆå·¦ä¾§ï¼‰ä»¥åŠåœ¨å›¾åƒä¸Šæ–½åŠ å™ªå£°æŸååçš„æ•ˆæœï¼Œåˆ†åˆ«æ˜¯ä¸¥é‡ç¨‹åº¦ä¸º 1ï¼ˆä¸­é—´ï¼‰å’Œ 5ï¼ˆå³ä¾§ï¼‰çš„æƒ…å†µã€‚

![PIC](img/file168.png)

å›¾ 8.4ï¼šé€šè¿‡åœ¨ä¸åŒæŸåä¸¥é‡ç¨‹åº¦ä¸‹åº”ç”¨å›¾åƒè´¨é‡æŸåæ¥ç”Ÿæˆäººå·¥æ•°æ®é›†åç§»

æ‰€æœ‰è¿™äº›å›¾åƒè´¨é‡æŸåå¯ä»¥æ–¹ä¾¿åœ°ä½¿ç”¨`imgaug` Python åŒ…ç”Ÿæˆã€‚ä»¥ä¸‹ä»£ç å‡è®¾æˆ‘ä»¬ç£ç›˜ä¸Šæœ‰ä¸€ä¸ªåä¸º"kitty.png"çš„å›¾åƒã€‚æˆ‘ä»¬ä½¿ç”¨ PIL åŒ…åŠ è½½å›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡æŸåå‡½æ•°çš„åç§°æŒ‡å®šæŸåç±»å‹ï¼ˆä¾‹å¦‚ï¼Œ`ShotNoise`ï¼‰ï¼Œå¹¶ä½¿ç”¨é€šè¿‡ä¼ é€’ç›¸åº”æ•´æ•°ç»™å…³é”®å­—å‚æ•°`severity`æ¥åº”ç”¨æŸåå‡½æ•°ï¼Œé€‰æ‹©ä¸¥é‡æ€§ç­‰çº§ 1 æˆ– 5ã€‚

```py

fromÂ PILÂ importÂ Image 
importÂ numpyÂ asÂ np 
importÂ imgaug.augmenters.imgcorruptlikeÂ asÂ icl 

imageÂ =Â np.asarray(Image.open("./kitty.png").convert("RGB")) 
corruption_functionÂ =Â icl.ShotNoise 
image_noise_level_01Â =Â corruption_function(severity=1,Â seed=0)(image=image) 
image_noise_level_05Â =Â corruption_function(severity=5,Â seed=0)(image=image)
```

é€šè¿‡è¿™ç§æ–¹å¼ç”Ÿæˆæ•°æ®åç§»çš„ä¼˜åŠ¿åœ¨äºï¼Œå®ƒå¯ä»¥åº”ç”¨äºå¹¿æ³›çš„è®¡ç®—æœºè§†è§‰é—®é¢˜å’Œæ•°æ®é›†ã€‚åº”ç”¨è¿™ç§æ–¹æ³•çš„å°‘æ•°å‰ææ¡ä»¶æ˜¯æ•°æ®ç”±å›¾åƒç»„æˆï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰ä½¿ç”¨è¿‡è¿™äº›å›¾åƒè´¨é‡æŸåï¼ˆä¾‹å¦‚ï¼Œç”¨äºæ•°æ®å¢å¼ºï¼‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡è®¾ç½®å›¾åƒè´¨é‡æŸåçš„ä¸¥é‡æ€§ï¼Œæˆ‘ä»¬å¯ä»¥æ§åˆ¶æ•°æ®é›†åç§»çš„ç¨‹åº¦ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¡¡é‡æ¨¡å‹å¯¹ä¸åŒç¨‹åº¦çš„æ•°æ®é›†åç§»çš„ååº”ã€‚æˆ‘ä»¬å¯ä»¥è¡¡é‡æ€§èƒ½å¦‚ä½•éšç€æ•°æ®é›†åç§»è€Œå˜åŒ–ï¼Œä»¥åŠæ ¡å‡†ï¼ˆåœ¨*ç¬¬äºŒç« *ï¼Œ*è´å¶æ–¯æ¨æ–­åŸºç¡€*ä¸­å¼•å…¥ï¼‰å¦‚ä½•å˜åŒ–ã€‚æˆ‘ä»¬é¢„è®¡ä½¿ç”¨è´å¶æ–¯æ–¹æ³•æˆ–æ‰©å±•æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹ä¼šæœ‰æ›´å¥½çš„æ ¡å‡†ï¼Œè¿™æ„å‘³ç€å®ƒä»¬èƒ½å¤Ÿå‘Šè¯‰æˆ‘ä»¬æ•°æ®ç›¸è¾ƒäºè®­ç»ƒæ—¶å·²ç»å‘ç”Ÿäº†åç§»ï¼Œå› æ­¤å®ƒä»¬å¯¹è¾“å‡ºçš„ä¿¡å¿ƒè¾ƒä½ã€‚

### 8.3.2 ä½¿ç”¨è´å¶æ–¯æ–¹æ³•æ­ç¤ºæ•°æ®é›†åç§»

åœ¨ä»¥ä¸‹çš„ä»£ç ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹ä¹¦ä¸­åˆ°ç›®å‰ä¸ºæ­¢é‡åˆ°çš„ä¸¤ç§ BDL æ–¹æ³•ï¼ˆåŸºäºåå‘ä¼ æ’­çš„è´å¶æ–¯æ–¹æ³•å’Œæ·±åº¦é›†æˆï¼‰ï¼Œå¹¶è§‚å¯Ÿå®ƒä»¬åœ¨å‰é¢æè¿°çš„äººå·¥æ•°æ®é›†åç§»ä¸‹çš„è¡¨ç°ã€‚æˆ‘ä»¬å°†å®ƒä»¬çš„è¡¨ç°ä¸æ™®é€šçš„ç¥ç»ç½‘ç»œè¿›è¡Œæ¯”è¾ƒã€‚

#### æ­¥éª¤ 1ï¼šå‡†å¤‡ç¯å¢ƒ

æˆ‘ä»¬é€šè¿‡å¯¼å…¥ä¸€ç³»åˆ—åŒ…æ¥å¼€å§‹è¿™ä¸ªç¤ºä¾‹ã€‚è¿™äº›åŒ…åŒ…æ‹¬ç”¨äºæ„å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œçš„ TensorFlow å’Œ TensorFlow Probabilityï¼›ç”¨äºå¤„ç†æ•°å€¼æ•°ç»„ï¼ˆå¦‚è®¡ç®—å‡å€¼ï¼‰çš„`numpy`ï¼›ç”¨äºç»˜å›¾çš„`Seaborn`ã€`Matplotlib`å’Œ`pandas`ï¼›ç”¨äºåŠ è½½å’Œå¤„ç†å›¾åƒçš„`cv2`å’Œ`imgaug`ï¼›ä»¥åŠç”¨äºè®¡ç®—æ¨¡å‹å‡†ç¡®åº¦çš„`scikit-learn`ã€‚

```py

importÂ cv2 
importÂ imgaug.augmentersÂ asÂ iaa 
importÂ imgaug.augmenters.imgcorruptlikeÂ asÂ icl 
importÂ matplotlib.pyplotÂ asÂ plt 
importÂ numpyÂ asÂ np 
importÂ pandasÂ asÂ pd 
importÂ seabornÂ asÂ sns 
importÂ tensorflowÂ asÂ tf 
importÂ tensorflow_probabilityÂ asÂ tfp 
fromÂ sklearn.metricsÂ importÂ accuracy_score
```

åœ¨è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬å°†åŠ è½½`CIFAR10`æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ï¼Œå¹¶æŒ‡å®šä¸åŒç±»åˆ«çš„åç§°ã€‚è¯¥æ•°æ®é›†åŒ…å« 10 ä¸ªä¸åŒçš„ç±»åˆ«ï¼Œæˆ‘ä»¬å°†åœ¨ä»¥ä¸‹ä»£ç ä¸­æŒ‡å®šè¿™äº›ç±»åˆ«çš„åç§°ï¼Œå¹¶æä¾› 50,000 ä¸ªè®­ç»ƒå›¾åƒå’Œ 10,000 ä¸ªæµ‹è¯•å›¾åƒã€‚æˆ‘ä»¬è¿˜å°†ä¿å­˜è®­ç»ƒå›¾åƒçš„æ•°é‡ï¼Œè¿™å°†åœ¨ç¨åä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§è®­ç»ƒæ¨¡å‹æ—¶ç”¨åˆ°ã€‚

```py

cifarÂ =Â tf.keras.datasets.cifar10 
(train_images,Â train_labels),Â (test_images,Â test_labels)Â =Â cifar.load_data() 

CLASS_NAMESÂ =Â [ 
"airplane","automobile",Â "bird",Â "cat",Â "deer", 
"dog",Â "frog",Â "horse",Â "ship",Â "truck" 
] 

NUM_TRAIN_EXAMPLESÂ =Â train_images.shape[0]
```

#### æ­¥éª¤ 2ï¼šå®šä¹‰å’Œè®­ç»ƒæ¨¡å‹

åœ¨è¿™é¡¹å‡†å¤‡å·¥ä½œå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰å¹¶è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸¤ä¸ªå‡½æ•°æ¥å®šä¹‰å’Œæ„å»º CNNã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸¤ä¸ªå‡½æ•°æ¥æ„å»ºæ™®é€šç¥ç»ç½‘ç»œå’Œæ·±åº¦é›†æˆç½‘ç»œã€‚ç¬¬ä¸€ä¸ªå‡½æ•°ç®€å•åœ°å°†å·ç§¯å±‚ä¸æœ€å¤§æ± åŒ–å±‚ç»“åˆèµ·æ¥â€”â€”è¿™æ˜¯ä¸€ç§å¸¸è§çš„åšæ³•ï¼Œæˆ‘ä»¬åœ¨*ç¬¬ä¸‰ç« *ã€Šæ·±åº¦å­¦ä¹ åŸºç¡€ã€‹ä¸­ä»‹ç»è¿‡ã€‚

```py

defÂ cnn_building_block(num_filters): 
returnÂ tf.keras.Sequential( 
[ 
tf.keras.layers.Conv2D( 
filters=num_filters,Â kernel_size=(3,Â 3),Â activation="relu" 
), 
tf.keras.layers.MaxPool2D(strides=2), 
] 
Â Â Â Â )
```

ç¬¬äºŒä¸ªå‡½æ•°åˆ™ä¾æ¬¡ä½¿ç”¨å¤šä¸ªå·ç§¯/æœ€å¤§æ± åŒ–å—ï¼Œå¹¶åœ¨æ­¤åºåˆ—åé¢è·Ÿç€ä¸€ä¸ªæœ€ç»ˆçš„å¯†é›†å±‚ï¼š

```py

defÂ build_and_compile_model(): 
modelÂ =Â tf.keras.Sequential( 
[ 
tf.keras.layers.Rescaling(1.0Â /Â 255,Â input_shape=(32,Â 32,Â 3)), 
cnn_building_block(16), 
cnn_building_block(32), 
cnn_building_block(64), 
tf.keras.layers.MaxPool2D(strides=2), 
tf.keras.layers.Flatten(), 
tf.keras.layers.Dense(64,Â activation="relu"), 
tf.keras.layers.Dense(10,Â activation="softmax"), 
] 
) 
model.compile( 
optimizer="adam", 
loss="sparse_categorical_crossentropy", 
metrics=["accuracy"], 
) 
Â Â Â Â returnÂ model
```

æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸¤ä¸ªç±»ä¼¼çš„å‡½æ•°ï¼Œç”¨äºåŸºäºé‡æ–°å‚æ•°åŒ–æŠ€å·§å®šä¹‰å’Œæ„å»ºä½¿ç”¨ Bayes By Backpropï¼ˆBBBï¼‰çš„ç½‘ç»œã€‚ç­–ç•¥ä¸æ™®é€šç¥ç»ç½‘ç»œç›¸åŒï¼Œåªä¸è¿‡æˆ‘ä»¬ç°åœ¨å°†ä½¿ç”¨æ¥è‡ª TensorFlow Probability åŒ…çš„å·ç§¯å±‚å’Œå¯†é›†å±‚ï¼Œè€Œä¸æ˜¯ TensorFlow åŒ…ã€‚å·ç§¯/æœ€å¤§æ± åŒ–å—å®šä¹‰å¦‚ä¸‹ï¼š

```py

defÂ cnn_building_block_bbb(num_filters,Â kl_divergence_function): 
returnÂ tf.keras.Sequential( 
[ 
tfp.layers.Convolution2DReparameterization( 
num_filters, 
kernel_size=(3,Â 3), 
kernel_divergence_fn=kl_divergence_function, 
activation=tf.nn.relu, 
), 
tf.keras.layers.MaxPool2D(strides=2), 
] 
Â Â Â Â )
```

æœ€ç»ˆçš„ç½‘ç»œå®šä¹‰å¦‚ä¸‹ï¼š

```py

defÂ build_and_compile_model_bbb(): 

kl_divergence_functionÂ =Â lambdaÂ q,Â p,Â _:Â tfp.distributions.kl_divergence( 
q,Â p 
)Â /Â tf.cast(NUM_TRAIN_EXAMPLES,Â dtype=tf.float32) 

modelÂ =Â tf.keras.models.Sequential( 
[ 
tf.keras.layers.Rescaling(1.0Â /Â 255,Â input_shape=(32,Â 32,Â 3)), 
cnn_building_block_bbb(16,Â kl_divergence_function), 
cnn_building_block_bbb(32,Â kl_divergence_function), 
cnn_building_block_bbb(64,Â kl_divergence_function), 
tf.keras.layers.Flatten(), 
tfp.layers.DenseReparameterization( 
64, 
kernel_divergence_fn=kl_divergence_function, 
activation=tf.nn.relu, 
), 
tfp.layers.DenseReparameterization( 
10, 
kernel_divergence_fn=kl_divergence_function, 
activation=tf.nn.softmax, 
), 
] 
) 

model.compile( 
optimizer="adam", 
loss="sparse_categorical_crossentropy", 
metrics=["accuracy"], 
experimental_run_tf_function=False, 
) 

model.build(input_shape=[None,Â 32,Â 32,Â 3]) 
Â Â Â Â returnÂ model
```

ç„¶åæˆ‘ä»¬å¯ä»¥è®­ç»ƒæ™®é€šç¥ç»ç½‘ç»œï¼š

```py

vanilla_modelÂ =Â build_and_compile_model() 
vanilla_model.fit(train_images,Â train_labels,Â epochs=10)
```

æˆ‘ä»¬è¿˜å¯ä»¥è®­ç»ƒä¸€ä¸ªäº”æˆå‘˜çš„é›†æˆæ¨¡å‹ï¼š

```py

NUM_ENSEMBLE_MEMBERSÂ =Â 5 
ensemble_modelÂ =Â [] 
forÂ indÂ inÂ range(NUM_ENSEMBLE_MEMBERS): 
memberÂ =Â build_and_compile_model() 
print(f"TrainÂ modelÂ {ind:02}") 
member.fit(train_images,Â train_labels,Â epochs=10) 
Â Â Â Â ensemble_model.append(member)
```

æœ€åï¼Œæˆ‘ä»¬è®­ç»ƒ BBB æ¨¡å‹ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬å°†è®­ç»ƒ BBB æ¨¡å‹ 15 ä¸ª epochï¼Œè€Œä¸æ˜¯ 10 ä¸ª epochï¼Œå› ä¸ºå®ƒæ”¶æ•›çš„æ—¶é—´ç¨é•¿ã€‚

```py

bbb_modelÂ =Â build_and_compile_model_bbb() 
bbb_model.fit(train_images,Â train_labels,Â epochs=15)
```

#### æ­¥éª¤ 3ï¼šè·å–é¢„æµ‹ç»“æœ

ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†ä¸‰ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨å®ƒä»¬å¯¹ä¿ç•™çš„æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ã€‚ä¸ºäº†ä¿æŒè®¡ç®—çš„å¯æ§æ€§ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºæµ‹è¯•é›†ä¸­çš„å‰ 1000 å¼ å›¾åƒï¼š

```py

NUM_SUBSETÂ =Â 1000 
test_images_subsetÂ =Â test_images[:NUM_SUBSET] 
test_labels_subsetÂ =Â test_labels[:NUM_SUBSET]
```

å¦‚æœæˆ‘ä»¬æƒ³è¦è¡¡é‡æ•°æ®é›†åç§»çš„å“åº”ï¼Œé¦–å…ˆéœ€è¦å¯¹æ•°æ®é›†åº”ç”¨äººå·¥å›¾åƒæŸåã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆæŒ‡å®šä¸€ç»„æ¥è‡ª`imgaug`åŒ…çš„å‡½æ•°ã€‚ä»è¿™äº›å‡½æ•°çš„åç§°ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ¨æ–­å‡ºæ¯ä¸ªå‡½æ•°å®ç°çš„æŸåç±»å‹ï¼šä¾‹å¦‚ï¼Œå‡½æ•°`icl.GaussianNoise`é€šè¿‡å‘å›¾åƒåº”ç”¨é«˜æ–¯å™ªå£°æ¥æŸåå›¾åƒã€‚æˆ‘ä»¬è¿˜é€šè¿‡å‡½æ•°çš„æ•°é‡æ¨æ–­å‡ºæŸåç±»å‹çš„æ•°é‡ï¼Œå¹¶å°†å…¶ä¿å­˜åœ¨`NUM_TYPES`å˜é‡ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æŸåçº§åˆ«è®¾ç½®ä¸º 5ã€‚

```py

corruption_functionsÂ =Â [ 
icl.GaussianNoise, 
icl.ShotNoise, 
icl.ImpulseNoise, 
icl.DefocusBlur, 
icl.GlassBlur, 
icl.MotionBlur, 
icl.ZoomBlur, 
icl.Snow, 
icl.Frost, 
icl.Fog, 
icl.Brightness, 
icl.Contrast, 
icl.ElasticTransform, 
icl.Pixelate, 
icl.JpegComdivssion, 
] 
NUM_TYPESÂ =Â len(corruption_functions) 
NUM_LEVELSÂ =Â 5
```

é…å¤‡äº†è¿™äº›å‡½æ•°åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å¼€å§‹æŸåå›¾åƒäº†ã€‚åœ¨ä¸‹ä¸€ä¸ªä»£ç å—ä¸­ï¼Œæˆ‘ä»¬éå†ä¸åŒçš„æŸåçº§åˆ«å’Œç±»å‹ï¼Œå¹¶å°†æ‰€æœ‰æŸåçš„å›¾åƒæ”¶é›†åˆ°åä¸º`corrupted_images`çš„å˜é‡ä¸­ã€‚

```py

corrupted_imagesÂ =Â [] 
#Â loopÂ overÂ differentÂ corruptionÂ severities 
forÂ corruption_severityÂ inÂ range(1,Â NUM_LEVELS+1): 
corruption_type_batchÂ =Â [] 
#Â loopÂ overÂ differentÂ corruptionÂ types 
forÂ corruption_typeÂ inÂ corruption_functions: 
corrupted_image_batchÂ =Â corruption_type( 
severity=corruption_severity,Â seed=0 
)(images=test_images_subset) 
corruption_type_batch.append(corrupted_image_batch) 
corruption_type_batchÂ =Â np.stack(corruption_type_batch,Â axis=0) 
corrupted_images.append(corruption_type_batch) 
corrupted_imagesÂ =Â np.stack(corrupted_images,Â axis=0)
```

åœ¨è®­ç»ƒå®Œä¸‰ä¸ªæ¨¡å‹å¹¶è·å¾—æŸåå›¾åƒåï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥çœ‹åˆ°æ¨¡å‹å¯¹ä¸åŒçº§åˆ«æ•°æ®é›†åç§»çš„ååº”ã€‚æˆ‘ä»¬å°†é¦–å…ˆè·å–ä¸‰ä¸ªæ¨¡å‹å¯¹æŸåå›¾åƒçš„é¢„æµ‹ç»“æœã€‚ä¸ºäº†è¿›è¡Œæ¨ç†ï¼Œæˆ‘ä»¬éœ€è¦å°†æŸåçš„å›¾åƒè°ƒæ•´ä¸ºæ¨¡å‹æ¥å—çš„è¾“å…¥å½¢çŠ¶ã€‚ç›®å‰ï¼Œè¿™äº›å›¾åƒä»ç„¶å­˜å‚¨åœ¨é’ˆå¯¹æŸåç±»å‹å’Œçº§åˆ«çš„ä¸åŒè½´ä¸Šã€‚æˆ‘ä»¬é€šè¿‡é‡æ–°è°ƒæ•´`corrupted_images`æ•°ç»„æ¥æ”¹å˜è¿™ä¸€ç‚¹ï¼š

```py

corrupted_imagesÂ =Â corrupted_images.reshape((-1,Â 32,Â 32,Â 3))
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ™®é€š CNN æ¨¡å‹å¯¹åŸå§‹å›¾åƒå’Œè…èš€å›¾åƒè¿›è¡Œæ¨ç†ã€‚åœ¨æ¨ç†æ¨¡å‹é¢„æµ‹åï¼Œæˆ‘ä»¬å°†é¢„æµ‹ç»“æœé‡å¡‘ï¼Œä»¥ä¾¿åˆ†ç¦»è…èš€ç±»å‹å’Œçº§åˆ«çš„é¢„æµ‹ï¼š

```py

#Â GetÂ predictionsÂ onÂ originalÂ images 
vanilla_predictionsÂ =Â vanilla_model.predict(test_images_subset) 
#Â GetÂ predictionsÂ onÂ corruptedÂ images 
vanilla_predictions_on_corruptedÂ =Â vanilla_model.predict(corrupted_images) 
vanilla_predictions_on_corruptedÂ =Â vanilla_predictions_on_corrupted.reshape( 
(NUM_LEVELS,Â NUM_TYPES,Â NUM_SUBSET,Â -1) 
)
```

ä¸ºäº†ä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªé¢„æµ‹å‡½æ•°ä»¥é¿å…ä»£ç é‡å¤ã€‚æ­¤å‡½æ•°å¤„ç†å¯¹é›†æˆä¸­ä¸åŒæˆå‘˜æ¨¡å‹çš„å¾ªç¯ï¼Œå¹¶æœ€ç»ˆé€šè¿‡å¹³å‡å°†ä¸åŒçš„é¢„æµ‹ç»“æœç»“åˆèµ·æ¥ï¼š

```py

defÂ get_ensemble_predictions(images,Â num_inferences): 
ensemble_predictionsÂ =Â tf.stack( 
[ 
ensemble_model[ensemble_ind].predict(images) 
forÂ ensemble_indÂ inÂ range(num_inferences) 
], 
axis=0, 
) 
Â Â Â Â returnÂ np.mean(ensemble_predictions,Â axis=0)
```

é…å¤‡äº†è¿™ä¸ªå‡½æ•°åï¼Œæˆ‘ä»¬å¯ä»¥å¯¹åŸå§‹å›¾åƒå’Œè…èš€å›¾åƒä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œæ¨ç†ï¼š

```py

#Â GetÂ predictionsÂ onÂ originalÂ images 
ensemble_predictionsÂ =Â get_ensemble_predictions( 
test_images_subset,Â NUM_ENSEMBLE_MEMBERS 
) 
#Â GetÂ predictionsÂ onÂ corruptedÂ images 
ensemble_predictions_on_corruptedÂ =Â get_ensemble_predictions( 
corrupted_images,Â NUM_ENSEMBLE_MEMBERS 
) 
ensemble_predictions_on_corruptedÂ =Â ensemble_predictions_on_corrupted.reshape( 
(NUM_LEVELS,Â NUM_TYPES,Â NUM_SUBSET,Â -1) 
)
```

å°±åƒå¯¹äºé›†æˆæ¨¡å‹ä¸€æ ·ï¼Œæˆ‘ä»¬ä¸º BBB æ¨¡å‹ç¼–å†™äº†ä¸€ä¸ªæ¨ç†å‡½æ•°ï¼Œè¯¥å‡½æ•°å¤„ç†ä¸åŒé‡‡æ ·å¾ªç¯çš„è¿­ä»£ï¼Œå¹¶æ”¶é›†å¹¶ç»“åˆç»“æœï¼š

```py

defÂ get_bbb_predictions(images,Â num_inferences): 
bbb_predictionsÂ =Â tf.stack( 
[bbb_model.predict(images)Â forÂ _Â inÂ range(num_inferences)], 
axis=0, 
) 
Â Â Â Â returnÂ np.mean(bbb_predictions,Â axis=0)
```

ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™ä¸ªå‡½æ•°è·å– BBB æ¨¡å‹åœ¨åŸå§‹å›¾åƒå’Œè…èš€å›¾åƒä¸Šçš„é¢„æµ‹ã€‚æˆ‘ä»¬ä» BBB æ¨¡å‹ä¸­é‡‡æ · 20 æ¬¡ï¼š

```py

NUM_INFERENCES_BBBÂ =Â 20 
#Â GetÂ predictionsÂ onÂ originalÂ images 
bbb_predictionsÂ =Â get_bbb_predictions( 
test_images_subset,Â NUM_INFERENCES_BBB 
) 
#Â GetÂ predictionsÂ onÂ corruptedÂ images 
bbb_predictions_on_corruptedÂ =Â get_bbb_predictions( 
corrupted_images,Â NUM_INFERENCES_BBB 
) 
bbb_predictions_on_corruptedÂ =Â bbb_predictions_on_corrupted.reshape( 
(NUM_LEVELS,Â NUM_TYPES,Â NUM_SUBSET,Â -1) 
)
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿”å›å…·æœ‰æœ€å¤§ softmax å¾—åˆ†çš„ç±»åˆ«ç´¢å¼•å’Œæœ€å¤§ softmax å¾—åˆ†ï¼Œåˆ†åˆ«å°†ä¸‰ä¸ªæ¨¡å‹çš„é¢„æµ‹è½¬æ¢ä¸ºé¢„æµ‹ç±»åˆ«åŠå…¶ç›¸å…³çš„ç½®ä¿¡åº¦å¾—åˆ†ï¼š

```py

defÂ get_classes_and_scores(model_predictions): 
model_predicted_classesÂ =Â np.argmax(model_predictions,Â axis=-1) 
model_scoresÂ =Â np.max(model_predictions,Â axis=-1) 
Â Â Â Â returnÂ model_predicted_classes,Â model_scores
```

ç„¶åå¯ä»¥åº”ç”¨æ­¤å‡½æ•°æ¥è·å–æˆ‘ä»¬ä¸‰ä¸ªæ¨¡å‹çš„é¢„æµ‹ç±»åˆ«å’Œç½®ä¿¡åº¦å¾—åˆ†ï¼š

```py

#Â VanillaÂ model 
vanilla_predicted_classes,Â vanilla_scoresÂ =Â get_classes_and_scores( 
vanilla_predictions 
) 
( 
vanilla_predicted_classes_on_corrupted, 
vanilla_scores_on_corrupted, 
)Â =Â get_classes_and_scores(vanilla_predictions_on_corrupted) 

#Â EnsembleÂ model 
( 
ensemble_predicted_classes, 
ensemble_scores, 
)Â =Â get_classes_and_scores(ensemble_predictions) 
( 
ensemble_predicted_classes_on_corrupted, 
ensemble_scores_on_corrupted, 
)Â =Â get_classes_and_scores(ensemble_predictions_on_corrupted) 

#Â BBBÂ model 
( 
bbb_predicted_classes, 
bbb_scores, 
)Â =Â get_classes_and_scores(bbb_predictions) 
( 
bbb_predicted_classes_on_corrupted, 
bbb_scores_on_corrupted, 
)Â =Â get_classes_and_scores(bbb_predictions_on_corrupted)
```

è®©æˆ‘ä»¬å¯è§†åŒ–è¿™ä¸‰ä¸ªæ¨¡å‹åœ¨ä¸€å¼ å±•ç¤ºæ±½è½¦çš„é€‰å®šå›¾åƒä¸Šé¢„æµ‹çš„ç±»åˆ«å’Œç½®ä¿¡åº¦å¾—åˆ†ã€‚ä¸ºäº†ç»˜å›¾ï¼Œæˆ‘ä»¬é¦–å…ˆå°†åŒ…å«è…èš€å›¾åƒçš„æ•°ç»„é‡å¡‘ä¸ºæ›´æ–¹ä¾¿çš„æ ¼å¼ï¼š

```py

plot_imagesÂ =Â corrupted_images.reshape( 
(NUM_LEVELS,Â NUM_TYPES,Â NUM_SUBSET,Â 32,Â 32,Â 3) 
)
```

ç„¶åï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†åˆ—è¡¨ä¸­å‰ä¸‰ç§è…èš€ç±»å‹çš„é€‰å®šæ±½è½¦å›¾åƒï¼Œæ¶µç›–æ‰€æœ‰äº”ä¸ªè…èš€çº§åˆ«ã€‚å¯¹äºæ¯ç§ç»„åˆï¼Œæˆ‘ä»¬åœ¨å›¾åƒæ ‡é¢˜ä¸­æ˜¾ç¤ºæ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹å¾—åˆ†ï¼Œå¹¶åœ¨æ–¹æ‹¬å·ä¸­æ˜¾ç¤ºé¢„æµ‹ç±»åˆ«ã€‚è¯¥å›¾å¦‚*å›¾**8.5*æ‰€ç¤ºã€‚

![PIC](img/file169.png)

å›¾ 8.5ï¼šä¸€å¼ æ±½è½¦å›¾åƒå·²ç»è¢«ä¸åŒçš„è…èš€ç±»å‹ï¼ˆè¡Œï¼‰å’Œçº§åˆ«ï¼ˆåˆ—ï¼Œä¸¥é‡ç¨‹åº¦ä»å·¦åˆ°å³å¢åŠ ï¼‰è…èš€

ä»£ç ç»§ç»­ï¼š

```py

#Â IndexÂ ofÂ theÂ selectedÂ images 
ind_imageÂ =Â 9 
#Â DefineÂ figure 
fig,Â axesÂ =Â plt.subplots(nrows=3,Â ncols=5,Â figsize=(16,Â 10)) 
#Â LoopÂ overÂ corruptionÂ levels 
forÂ ind_levelÂ inÂ range(NUM_LEVELS): 
#Â LoopÂ overÂ corruptionÂ types 
forÂ ind_typeÂ inÂ range(3): 
#Â PlotÂ slightlyÂ upscaledÂ imageÂ forÂ easierÂ inspection 
imageÂ =Â plot_images[ind_level,Â ind_type,Â ind_image,Â ...] 
image_upscaledÂ =Â cv2.resize( 
image,Â dsize=(150,Â 150),Â interpolation=cv2.INTER_CUBIC 
) 
axes[ind_type,Â ind_level].imshow(image_upscaled) 
#Â GetÂ scoreÂ andÂ classÂ predictedÂ byÂ vanillaÂ model 
vanilla_scoreÂ =Â vanilla_scores_on_corrupted[ 
ind_level,Â ind_type,Â ind_image,Â ... 
] 
vanilla_predictionÂ =Â vanilla_predicted_classes_on_corrupted[ 
ind_level,Â ind_type,Â ind_image,Â ... 
] 
#Â GetÂ scoreÂ andÂ classÂ predictedÂ byÂ ensembleÂ model 
ensemble_scoreÂ =Â ensemble_scores_on_corrupted[ 
ind_level,Â ind_type,Â ind_image,Â ... 
] 
ensemble_predictionÂ =Â ensemble_predicted_classes_on_corrupted[ 
ind_level,Â ind_type,Â ind_image,Â ... 
] 
#Â GetÂ scoreÂ andÂ classÂ predictedÂ byÂ BBBÂ model 
bbb_scoreÂ =Â bbb_scores_on_corrupted[ind_level,Â ind_type,Â ind_image,Â ...] 
bbb_predictionÂ =Â bbb_predicted_classes_on_corrupted[ 
ind_level,Â ind_type,Â ind_image,Â ... 
] 
#Â PlotÂ predictionÂ infoÂ inÂ title 
title_textÂ =Â ( 
f"Vanilla:Â {vanilla_score:.3f}Â " 
+Â f"[{CLASS_NAMES[vanilla_prediction]}]Â \n" 
+Â f"Ensemble:Â {ensemble_score:.3f}Â " 
+Â f"[{CLASS_NAMES[ensemble_prediction]}]Â \n" 
+Â f"BBB:Â {bbb_score:.3f}Â " 
+Â f"[{CLASS_NAMES[bbb_prediction]}]" 
) 
axes[ind_type,Â ind_level].set_title(title_text,Â fontsize=14) 
#Â RemoveÂ axesÂ ticksÂ andÂ labels 
axes[ind_type,Â ind_level].axis("off") 
fig.tight_layout() 
plt.show()
```

*å›¾**8.5*åªæ˜¾ç¤ºäº†å•å¼ å›¾åƒçš„ç»“æœï¼Œå› æ­¤æˆ‘ä»¬ä¸åº”è¿‡åº¦è§£è¯»è¿™äº›ç»“æœã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å·²ç»å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œä¸¤ä¸ªè´å¶æ–¯æ–¹æ³•ï¼ˆå°¤å…¶æ˜¯é›†æˆæ–¹æ³•ï¼‰çš„é¢„æµ‹å¾—åˆ†é€šå¸¸æ¯”æ™®é€šç¥ç»ç½‘ç»œæ›´ä¸æç«¯ï¼Œåè€…çš„é¢„æµ‹å¾—åˆ†é«˜è¾¾ 0.95ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çœ‹åˆ°ï¼Œå¯¹äºæ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹ï¼Œé¢„æµ‹å¾—åˆ†é€šå¸¸éšç€è…èš€çº§åˆ«çš„å¢åŠ è€Œé™ä½ã€‚è¿™æ˜¯é¢„æœŸçš„ï¼šç”±äºå›¾åƒä¸­çš„æ±½è½¦åœ¨è…èš€è¶Šä¸¥é‡æ—¶å˜å¾—è¶Šéš¾ä»¥è¾¨è®¤ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹çš„ç½®ä¿¡åº¦ä¹Ÿä¼šéšä¹‹é™ä½ã€‚ç‰¹åˆ«æ˜¯ï¼Œé›†æˆæ–¹æ³•åœ¨å¢åŠ è…èš€çº§åˆ«æ—¶æ˜¾ç¤ºå‡ºäº†é¢„æµ‹å¾—åˆ†çš„æ˜æ˜¾ä¸”ä¸€è‡´çš„ä¸‹é™ã€‚

#### ç¬¬ 4 æ­¥ï¼šè¡¡é‡å‡†ç¡®æ€§

æœ‰äº›æ¨¡å‹æ¯”å…¶ä»–æ¨¡å‹æ›´èƒ½é€‚åº”æ•°æ®é›†çš„åç§»å—ï¼Ÿæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹ä¸‰ç§æ¨¡å‹åœ¨ä¸åŒæŸåæ°´å¹³ä¸‹çš„å‡†ç¡®æ€§æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚é¢„è®¡æ‰€æœ‰æ¨¡å‹åœ¨è¾“å…¥å›¾åƒé€æ¸æŸåæ—¶å‡†ç¡®æ€§ä¼šé™ä½ã€‚ç„¶è€Œï¼Œæ›´é²æ£’çš„æ¨¡å‹åœ¨æŸåå˜å¾—æ›´ä¸¥é‡æ—¶ï¼Œå‡†ç¡®æ€§ä¸‹é™åº”è¯¥è¾ƒå°‘ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ä¸‰ç§æ¨¡å‹åœ¨åŸå§‹æµ‹è¯•å›¾åƒä¸Šçš„å‡†ç¡®æ€§ï¼š

```py

vanilla_accÂ =Â accuracy_score( 
test_labels_subset.flatten(),Â vanilla_predicted_classes 
) 
ensemble_accÂ =Â accuracy_score( 
test_labels_subset.flatten(),Â ensemble_predicted_classes 
) 
bbb_accÂ =Â accuracy_score( 
test_labels_subset.flatten(),Â bbb_predicted_classes 
)
```

æˆ‘ä»¬å¯ä»¥å°†è¿™äº›å‡†ç¡®æ€§å­˜å‚¨åœ¨å­—å…¸åˆ—è¡¨ä¸­ï¼Œè¿™å°†ä½¿æˆ‘ä»¬æ›´å®¹æ˜“ç³»ç»Ÿåœ°ç»˜åˆ¶å®ƒä»¬ã€‚æˆ‘ä»¬ä¼ é€’ç›¸åº”çš„æ¨¡å‹åç§°ã€‚å¯¹äºæŸåçš„`ç±»å‹`å’Œ`çº§åˆ«`ï¼Œæˆ‘ä»¬ä¼ é€’`0`ï¼Œå› ä¸ºè¿™äº›æ˜¯åŸå§‹å›¾åƒä¸Šçš„å‡†ç¡®æ€§ã€‚

```py

accuraciesÂ =Â [ 
{"model_name":Â "vanilla",Â "type":Â 0,Â "level":Â 0,Â "accuracy":Â vanilla_acc}, 
{"model_name":Â "ensemble",Â "type":Â 0,Â "level":Â 0,Â "accuracy":Â ensemble_acc}, 
{"model_name":Â "bbb",Â "type":Â 0,Â "level":Â 0,Â "accuracy":Â bbb_acc}, 
]
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¡ç®—ä¸‰ç§æ¨¡å‹åœ¨ä¸åŒæŸåç±»å‹å’ŒæŸåçº§åˆ«ç»„åˆä¸‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜å°†ç»“æœé™„åŠ åˆ°ä¹‹å‰å¼€å§‹çš„å‡†ç¡®æ€§åˆ—è¡¨ä¸­ï¼š

```py

forÂ ind_typeÂ inÂ range(NUM_TYPES): 
forÂ ind_levelÂ inÂ range(NUM_LEVELS): 
#Â CalculateÂ accuracyÂ forÂ vanillaÂ model 
vanilla_acc_on_corruptedÂ =Â accuracy_score( 
test_labels_subset.flatten(), 
vanilla_predicted_classes_on_corrupted[ind_level,Â ind_type,Â :], 
) 
accuracies.append( 
{ 
"model_name":Â "vanilla", 
"type":Â ind_typeÂ +Â 1, 
"level":Â ind_levelÂ +Â 1, 
"accuracy":Â vanilla_acc_on_corrupted, 
} 
) 

#Â CalculateÂ accuracyÂ forÂ ensembleÂ model 
ensemble_acc_on_corruptedÂ =Â accuracy_score( 
test_labels_subset.flatten(), 
ensemble_predicted_classes_on_corrupted[ind_level,Â ind_type,Â :], 
) 
accuracies.append( 
{ 
"model_name":Â "ensemble", 
"type":Â ind_typeÂ +Â 1, 
"level":Â ind_levelÂ +Â 1, 
"accuracy":Â ensemble_acc_on_corrupted, 
} 
) 

#Â CalculateÂ accuracyÂ forÂ BBBÂ model 
bbb_acc_on_corruptedÂ =Â accuracy_score( 
test_labels_subset.flatten(), 
bbb_predicted_classes_on_corrupted[ind_level,Â ind_type,Â :], 
) 
accuracies.append( 
{ 
"model_name":Â "bbb", 
"type":Â ind_typeÂ +Â 1, 
"level":Â ind_levelÂ +Â 1, 
"accuracy":Â bbb_acc_on_corrupted, 
} 
Â Â Â Â Â Â Â Â )
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶åŸå§‹å›¾åƒå’Œé€æ¸æŸåå›¾åƒçš„å‡†ç¡®æ€§åˆ†å¸ƒã€‚æˆ‘ä»¬é¦–å…ˆå°†å­—å…¸åˆ—è¡¨è½¬æ¢ä¸º pandas dataframeã€‚è¿™æœ‰ä¸€ä¸ªä¼˜åŠ¿ï¼Œå³ dataframe å¯ä»¥ç›´æ¥ä¼ é€’ç»™ç»˜å›¾åº“`seaborn`ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥æŒ‡å®šä¸åŒæ¨¡å‹çš„ç»“æœä»¥ä¸åŒè‰²è°ƒè¿›è¡Œç»˜åˆ¶ã€‚

```py

dfÂ =Â pd.DataFrame(accuracies) 
plt.figure(dpi=100) 
sns.boxplot(data=df,Â x="level",Â y="accuracy",Â hue="model_name") 
plt.legend(loc="centerÂ left",Â bbox_to_anchor=(1,Â 0.5)) 
plt.tight_layout 
plt.show()
```

è¿™ä¼šç”Ÿæˆä»¥ä¸‹è¾“å‡ºï¼š

![å›¾ç‰‡](img/file170.png)

å›¾ 8.6ï¼šä¸‰ç§ä¸åŒæ¨¡å‹ï¼ˆä¸åŒè‰²è°ƒï¼‰åœ¨åŸå§‹æµ‹è¯•å›¾åƒï¼ˆçº§åˆ« 0ï¼‰ä»¥åŠä¸åŒç¨‹åº¦çš„æŸåï¼ˆçº§åˆ« 1-5ï¼‰ä¸Šçš„å‡†ç¡®æ€§

ç»“æœå›¾å¦‚*å›¾**8.6*æ‰€ç¤ºã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨åŸå§‹æµ‹è¯•å›¾åƒä¸Šï¼Œæ™®é€šæ¨¡å‹å’Œ BBB æ¨¡å‹çš„å‡†ç¡®æ€§ç›¸å½“ï¼Œè€Œé›†æˆæ¨¡å‹çš„å‡†ç¡®æ€§ç¨é«˜ã€‚éšç€æŸåçš„å¼•å…¥ï¼Œæˆ‘ä»¬çœ‹åˆ°æ™®é€šç¥ç»ç½‘ç»œçš„è¡¨ç°æ¯”é›†æˆæ¨¡å‹æˆ– BBB æ¨¡å‹æ›´å·®ï¼ˆé€šå¸¸æ˜¯æ˜¾è‘—å·®ï¼‰ã€‚BDL æ¨¡å‹æ€§èƒ½çš„ç›¸å¯¹æå‡å±•ç¤ºäº†è´å¶æ–¯æ–¹æ³•çš„æ­£åˆ™åŒ–æ•ˆåº”ï¼šè¿™äº›æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰æ•°æ®çš„åˆ†å¸ƒï¼Œä½¿å…¶å¯¹æ‰°åŠ¨æ›´åŠ é²æ£’ã€‚BBB æ¨¡å‹ç‰¹åˆ«èƒ½æŠµå¾¡æ•°æ®æŸåçš„å¢åŠ ï¼Œå±•ç¤ºäº†å˜åˆ†å­¦ä¹ çš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿ã€‚

#### æ­¥éª¤ 5ï¼šè¡¡é‡æ ¡å‡†

æŸ¥çœ‹å‡†ç¡®åº¦æ˜¯ç¡®å®šæ¨¡å‹åœ¨æ•°æ®é›†å˜åŒ–ä¸‹çš„é²æ£’æ€§çš„ä¸€ç§å¥½æ–¹æ³•ã€‚ä½†å®ƒå¹¶æ²¡æœ‰çœŸæ­£å‘Šè¯‰æˆ‘ä»¬æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿé€šè¿‡è¾ƒä½çš„ç½®ä¿¡åº¦åˆ†æ•°ï¼ˆå½“æ•°æ®é›†å‘ç”Ÿå˜åŒ–æ—¶ï¼‰æé†’æˆ‘ä»¬ï¼Œå¹¶ä¸”æ¨¡å‹åœ¨è¾“å‡ºæ—¶å˜å¾—ä¸é‚£ä¹ˆè‡ªä¿¡ã€‚è¿™ä¸ªé—®é¢˜å¯ä»¥é€šè¿‡è§‚å¯Ÿæ¨¡å‹åœ¨æ•°æ®é›†å˜åŒ–ä¸‹çš„æ ¡å‡†è¡¨ç°æ¥å›ç­”ã€‚æˆ‘ä»¬åœ¨*ç¬¬ä¸‰ç« *çš„ã€Šæ·±åº¦å­¦ä¹ åŸºç¡€ã€‹ä¸­å·²ç»ä»‹ç»äº†æ ¡å‡†å’ŒæœŸæœ›æ ¡å‡†è¯¯å·®çš„æ¦‚å¿µã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†æŠŠè¿™äº›æ¦‚å¿µä»˜è¯¸å®è·µï¼Œä»¥ç†è§£å½“å›¾åƒå˜å¾—è¶Šæ¥è¶Šå—æŸä¸”éš¾ä»¥é¢„æµ‹æ—¶ï¼Œæ¨¡å‹æ˜¯å¦é€‚å½“åœ°è°ƒæ•´äº†å®ƒä»¬çš„ç½®ä¿¡åº¦ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å°†å®ç°*ç¬¬ä¸‰ç« *ã€Šæ·±åº¦å­¦ä¹ åŸºç¡€ã€‹ä¸­ä»‹ç»çš„æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰ï¼Œä½œä¸ºæ ¡å‡†çš„æ ‡é‡è¡¡é‡æ ‡å‡†ï¼š

```py

defÂ expected_calibration_error( 
divd_correct, 
divd_score, 
n_bins=5, 
): 
"""ComputeÂ expectedÂ calibrationÂ error. 
---------- 
divd_correctÂ :Â np.ndarrayÂ (n_samples,) 
WhetherÂ theÂ predictionÂ isÂ correctÂ orÂ not 
divd_scoreÂ :Â np.ndarrayÂ (n_samples,) 
ConfidenceÂ inÂ theÂ prediction 
n_binsÂ :Â int,Â default=5 
NumberÂ ofÂ binsÂ toÂ discretizeÂ theÂ [0,Â 1]Â interval. 
""" 
#Â ConvertÂ fromÂ boolÂ toÂ integerÂ (makesÂ countingÂ easier) 
divd_correctÂ =Â divd_correct.astype(np.int32) 

#Â CreateÂ binsÂ andÂ assignÂ predictionÂ scoresÂ toÂ bins 
binsÂ =Â np.linspace(0.0,Â 1.0,Â n_binsÂ +Â 1) 
binidsÂ =Â np.searchsorted(bins[1:-1],Â divd_score) 

#Â CountÂ numberÂ ofÂ samplesÂ andÂ correctÂ predictionsÂ perÂ bin 
bin_true_countsÂ =Â np.bincount( 
binids,Â weights=divd_correct,Â minlength=len(bins) 
) 
bin_countsÂ =Â np.bincount(binids,Â minlength=len(bins)) 

#Â CalculateÂ sumÂ ofÂ confidenceÂ scoresÂ perÂ bin 
bin_probsÂ =Â np.bincount(binids,Â weights=divd_score,Â minlength=len(bins)) 

#Â IdentifyÂ binsÂ thatÂ containÂ samples 
nonzeroÂ =Â bin_countsÂ !=Â 0 
#Â CalculateÂ accuracyÂ forÂ everyÂ bin 
bin_accÂ =Â bin_true_counts[nonzero]Â /Â bin_counts[nonzero] 
#Â CalculateÂ averageÂ confidenceÂ scoresÂ perÂ bin 
bin_confÂ =Â bin_probs[nonzero]Â /Â bin_counts[nonzero] 

Â Â Â Â returnÂ np.average(np.abs(bin_accÂ -Â bin_conf),Â weights=bin_counts[nonzero])
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ä¸‰ä¸ªæ¨¡å‹åœ¨åŸå§‹æµ‹è¯•å›¾åƒä¸Šçš„æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰ã€‚æˆ‘ä»¬å°†ç®±å­çš„æ•°é‡è®¾ç½®ä¸º`10`ï¼Œè¿™æ˜¯è®¡ç®— ECE æ—¶å¸¸ç”¨çš„é€‰æ‹©ï¼š

```py

NUM_BINSÂ =Â 10 

vanilla_calÂ =Â expected_calibration_error( 
test_labels_subset.flatten()Â ==Â vanilla_predicted_classes, 
vanilla_scores, 
n_bins=NUM_BINS, 
) 

ensemble_calÂ =Â expected_calibration_error( 
test_labels_subset.flatten()Â ==Â ensemble_predicted_classes, 
ensemble_scores, 
n_bins=NUM_BINS, 
) 

bbb_calÂ =Â expected_calibration_error( 
test_labels_subset.flatten()Â ==Â bbb_predicted_classes, 
bbb_scores, 
n_bins=NUM_BINS, 
)
```

å°±åƒæˆ‘ä»¬ä¹‹å‰å¤„ç†å‡†ç¡®åº¦ä¸€æ ·ï¼Œæˆ‘ä»¬å°†æŠŠæ ¡å‡†ç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªå­—å…¸åˆ—è¡¨ä¸­ï¼Œè¿™æ ·å°±æ›´å®¹æ˜“ç»˜åˆ¶å®ƒä»¬ï¼š

```py

calibrationÂ =Â [ 
{ 
"model_name":Â "vanilla", 
"type":Â 0, 
"level":Â 0, 
"calibration_error":Â vanilla_cal, 
}, 
{ 
"model_name":Â "ensemble", 
"type":Â 0, 
"level":Â 0, 
"calibration_error":Â ensemble_cal, 
}, 
{ 
"model_name":Â "bbb", 
"type":Â 0, 
"level":Â 0, 
"calibration_error":Â bbb_cal, 
}, 
]
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ ¹æ®ä¸åŒçš„è…èš€ç±»å‹å’Œè…èš€çº§åˆ«ç»„åˆï¼Œè®¡ç®—ä¸‰ä¸ªæ¨¡å‹çš„æœŸæœ›æ ¡å‡†è¯¯å·®ã€‚æˆ‘ä»¬è¿˜å°†ç»“æœé™„åŠ åˆ°ä¹‹å‰å¼€å§‹çš„æ ¡å‡†ç»“æœåˆ—è¡¨ä¸­ï¼š

```py

forÂ ind_typeÂ inÂ range(NUM_TYPES): 
forÂ ind_levelÂ inÂ range(NUM_LEVELS): 
#Â CalculateÂ calibrationÂ errorÂ forÂ vanillaÂ model 
vanilla_cal_on_corruptedÂ =Â expected_calibration_error( 
test_labels_subset.flatten() 
==Â vanilla_predicted_classes_on_corrupted[ind_level,Â ind_type,Â :], 
vanilla_scores_on_corrupted[ind_level,Â ind_type,Â :], 
) 
calibration.append( 
{ 
"model_name":Â "vanilla", 
"type":Â ind_typeÂ +Â 1, 
"level":Â ind_levelÂ +Â 1, 
"calibration_error":Â vanilla_cal_on_corrupted, 
} 
) 

#Â CalculateÂ calibrationÂ errorÂ forÂ ensembleÂ model 
ensemble_cal_on_corruptedÂ =Â expected_calibration_error( 
test_labels_subset.flatten() 
==Â ensemble_predicted_classes_on_corrupted[ind_level,Â ind_type,Â :], 
ensemble_scores_on_corrupted[ind_level,Â ind_type,Â :], 
) 
calibration.append( 
{ 
"model_name":Â "ensemble", 
"type":Â ind_typeÂ +Â 1, 
"level":Â ind_levelÂ +Â 1, 
"calibration_error":Â ensemble_cal_on_corrupted, 
} 
) 

#Â CalculateÂ calibrationÂ errorÂ forÂ BBBÂ model 
bbb_cal_on_corruptedÂ =Â expected_calibration_error( 
test_labels_subset.flatten() 
==Â bbb_predicted_classes_on_corrupted[ind_level,Â ind_type,Â :], 
bbb_scores_on_corrupted[ind_level,Â ind_type,Â :], 
) 
calibration.append( 
{ 
"model_name":Â "bbb", 
"type":Â ind_typeÂ +Â 1, 
"level":Â ind_levelÂ +Â 1, 
"calibration_error":Â bbb_cal_on_corrupted, 
} 
Â Â Â Â Â Â Â Â )
```

æœ€åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`pandas`å’Œ`seaborn`å†æ¬¡ç»˜åˆ¶æ ¡å‡†ç»“æœçš„ç®±å½¢å›¾ï¼š

```py

dfÂ =Â pd.DataFrame(calibration) 
plt.figure(dpi=100) 
sns.boxplot(data=df,Â x="level",Â y="calibration_error",Â hue="model_name") 
plt.legend(loc="centerÂ left",Â bbox_to_anchor=(1,Â 0.5)) 
plt.tight_layout 
plt.show()
```

æ ¡å‡†ç»“æœæ˜¾ç¤ºåœ¨*å›¾**8.7*ä¸­ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨åŸå§‹æµ‹è¯•å›¾åƒä¸Šï¼Œæ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹çš„æ ¡å‡†è¯¯å·®éƒ½æ¯”è¾ƒä½ï¼Œé›†æˆæ¨¡å‹çš„è¡¨ç°ç•¥é€Šè‰²äºå¦å¤–ä¸¤ä¸ªæ¨¡å‹ã€‚éšç€æ•°æ®é›†å˜åŒ–ç¨‹åº¦çš„å¢åŠ ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¼ ç»Ÿæ¨¡å‹çš„æ ¡å‡†è¯¯å·®å¤§å¹…å¢åŠ ã€‚å¯¹äºä¸¤ç§è´å¶æ–¯æ–¹æ³•ï¼Œæ ¡å‡†è¯¯å·®ä¹Ÿå¢åŠ äº†ï¼Œä½†æ¯”ä¼ ç»Ÿæ¨¡å‹è¦å°‘å¾—å¤šã€‚è¿™æ„å‘³ç€è´å¶æ–¯æ–¹æ³•åœ¨æ•°æ®é›†å‘ç”Ÿå˜åŒ–æ—¶èƒ½å¤Ÿæ›´å¥½åœ°é€šè¿‡è¾ƒä½çš„ç½®ä¿¡åº¦åˆ†æ•°æ¥æŒ‡ç¤ºï¼ˆå³æ¨¡å‹åœ¨è¾“å‡ºæ—¶å˜å¾—ç›¸å¯¹ä¸é‚£ä¹ˆè‡ªä¿¡ï¼Œéšç€è…èš€ç¨‹åº¦çš„å¢åŠ ï¼Œè¡¨ç°å‡ºè¿™ç§ç‰¹å¾ï¼‰ã€‚

![PIC](img/file171.png)

å›¾ 8.7ï¼šä¸‰ç§ä¸åŒæ¨¡å‹åœ¨åŸå§‹æµ‹è¯•å›¾åƒï¼ˆçº§åˆ« 0ï¼‰å’Œä¸åŒè…èš€çº§åˆ«ï¼ˆçº§åˆ« 1-5ï¼‰ä¸Šçš„æœŸæœ›æ ¡å‡†è¯¯å·®

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºæ•°æ®é€‰æ‹©ã€‚

## 8.4 ä½¿ç”¨é€šè¿‡ä¸ç¡®å®šæ€§è¿›è¡Œçš„æ•°æ®é€‰æ‹©æ¥ä¿æŒæ¨¡å‹çš„æ›´æ–°

æˆ‘ä»¬åœ¨æœ¬ç« å¼€å¤´çœ‹åˆ°ï¼Œèƒ½å¤Ÿä½¿ç”¨ä¸ç¡®å®šæ€§æ¥åˆ¤æ–­æ•°æ®æ˜¯å¦æ˜¯è®­ç»ƒæ•°æ®çš„ä¸€éƒ¨åˆ†ã€‚åœ¨ä¸»åŠ¨å­¦ä¹ è¿™ä¸€æœºå™¨å­¦ä¹ é¢†åŸŸçš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•è¿™ä¸ªæƒ³æ³•ã€‚ä¸»åŠ¨å­¦ä¹ çš„æ‰¿è¯ºæ˜¯ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿæ§åˆ¶æ¨¡å‹è®­ç»ƒçš„æ•°æ®ç±»å‹ï¼Œæ¨¡å‹å¯ä»¥åœ¨æ›´å°‘çš„æ•°æ®ä¸Šæ›´æœ‰æ•ˆåœ°å­¦ä¹ ã€‚ä»æ¦‚å¿µä¸Šè®²ï¼Œè¿™æ˜¯æœ‰é“ç†çš„ï¼šå¦‚æœæˆ‘ä»¬åœ¨è´¨é‡ä¸è¶³çš„æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå®ƒçš„è¡¨ç°ä¹Ÿä¸ä¼šå¾ˆå¥½ã€‚ä¸»åŠ¨å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡æä¾›å¯ä»¥ä»ä¸å±äºè®­ç»ƒæ•°æ®çš„æ•°æ®æ± ä¸­è·å–æ•°æ®çš„å‡½æ•°ï¼Œæ¥å¼•å¯¼æ¨¡å‹å­¦ä¹ è¿‡ç¨‹å’Œè®­ç»ƒæ•°æ®çš„æ–¹æ³•ã€‚é€šè¿‡åå¤ä»æ•°æ®æ± ä¸­é€‰æ‹©æ­£ç¡®çš„æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒå‡ºæ¯”éšæœºé€‰æ‹©æ•°æ®æ—¶è¡¨ç°æ›´å¥½çš„æ¨¡å‹ã€‚

ä¸»åŠ¨å­¦ä¹ å¯ä»¥åº”ç”¨äºè®¸å¤šç°ä»£ç³»ç»Ÿï¼Œåœ¨è¿™äº›ç³»ç»Ÿä¸­æœ‰å¤§é‡æœªæ ‡è®°çš„æ•°æ®å¯ä¾›ä½¿ç”¨ï¼Œæˆ‘ä»¬éœ€è¦ä»”ç»†é€‰æ‹©æƒ³è¦æ ‡è®°çš„æ•°æ®é‡ã€‚ä¸€ä¸ªä¾‹å­æ˜¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼šè½¦ä¸Šçš„æ‘„åƒå¤´è®°å½•äº†å¤§é‡æ•°æ®ï¼Œä½†é€šå¸¸æ²¡æœ‰é¢„ç®—æ ‡è®°æ‰€æœ‰æ•°æ®ã€‚é€šè¿‡ä»”ç»†é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ•°æ®ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥æ¯”éšæœºé€‰æ‹©æ•°æ®æ ‡è®°æ—¶æ›´ä½çš„æˆæœ¬æé«˜æ¨¡å‹æ€§èƒ½ã€‚åœ¨ä¸»åŠ¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œä¼°è®¡ä¸ç¡®å®šæ€§å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æ¨¡å‹é€šå¸¸ä¼šä»æ•°æ®åˆ†å¸ƒä¸­é‚£äº›ä½ç½®ä¿¡åº¦é¢„æµ‹çš„åŒºåŸŸå­¦åˆ°æ›´å¤šã€‚è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶æ¥çœ‹çœ‹å¦‚ä½•åœ¨ä¸»åŠ¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹ä½¿ç”¨ä¸ç¡®å®šæ€§ã€‚

åœ¨è¿™ä¸ªæ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç°ä¸€ç¯‡åŸºç¡€æ€§ä¸»åŠ¨å­¦ä¹ è®ºæ–‡çš„ç»“æœï¼š*åŸºäºå›¾åƒæ•°æ®çš„æ·±åº¦è´å¶æ–¯ä¸»åŠ¨å­¦ä¹ *ï¼ˆ2017ï¼‰ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`MNIST`æ•°æ®é›†ï¼Œå¹¶åœ¨è¶Šæ¥è¶Šå¤šçš„æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡ä¸ç¡®å®šæ€§æ–¹æ³•é€‰æ‹©è¦æ·»åŠ åˆ°è®­ç»ƒé›†ä¸­çš„æ•°æ®ç‚¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è®¤çŸ¥ä¸ç¡®å®šæ€§æ¥é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ•°æ®ç‚¹ã€‚å…·æœ‰é«˜è®¤çŸ¥ä¸ç¡®å®šæ€§çš„å›¾åƒåº”è¯¥æ˜¯æ¨¡å‹ä¹‹å‰æ²¡æœ‰è§è¿‡çš„å›¾åƒï¼›é€šè¿‡å¢åŠ æ›´å¤šè¿™æ ·çš„å›¾åƒï¼Œå¯ä»¥å‡å°‘ä¸ç¡®å®šæ€§ã€‚ä½œä¸ºå¯¹æ¯”ï¼Œæˆ‘ä»¬è¿˜å°†éšæœºé€‰æ‹©æ•°æ®ç‚¹ã€‚

#### ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡æ•°æ®é›†

æˆ‘ä»¬å°†é¦–å…ˆåˆ›å»ºåŠ è½½æ•°æ®é›†çš„å‡½æ•°ã€‚æ•°æ®é›†å‡½æ•°éœ€è¦ä»¥ä¸‹åº“å¯¼å…¥ï¼š

```py

importÂ dataclasses 
fromÂ pathlibÂ importÂ Path 
importÂ uuid 
fromÂ typingÂ importÂ Optional,Â Tuple 

importÂ numpyÂ asÂ np 
importÂ tensorflowÂ asÂ tf 
fromÂ sklearn.utilsÂ importÂ shuffle
```

ç”±äºæˆ‘ä»¬çš„æ€»æ•°æ®é›†å°†åŒ…å«ç›¸å½“å¤šçš„ç»„ä»¶ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå°çš„`dataclass`ï¼Œä»¥ä¾¿è½»æ¾è®¿é—®æ•°æ®é›†çš„ä¸åŒéƒ¨åˆ†ã€‚æˆ‘ä»¬è¿˜å°†ä¿®æ”¹`__repr__`å‡½æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿä»¥æ›´æ˜“è¯»çš„æ ¼å¼æ‰“å°æ•°æ®é›†å†…å®¹ã€‚

```py

@dataclasses.dataclass 
classÂ Data: 
x_train:Â np.ndarray 
y_train:Â np.ndarray 
x_test:Â np.ndarray 
y_test:Â np.ndarray 
x_train_al:Â Optional[np.ndarray]Â =Â None 
y_train_al:Â Optional[np.ndarray]Â =Â None 

defÂ __repr__(self)Â -*>*Â str: 
repr_strÂ =Â "" 
forÂ fieldÂ inÂ dataclasses.fields(self): 
repr_strÂ +=Â f"{field.name}:Â {getattr(self,Â field.name).shape}Â \n" 
Â Â Â Â Â Â Â Â returnÂ repr_str
```

ç„¶åæˆ‘ä»¬å¯ä»¥å®šä¹‰å‡½æ•°æ¥åŠ è½½æ ‡å‡†æ•°æ®é›†ã€‚

```py

defÂ get_data()Â -*>*Â Data: 
num_classesÂ =Â 10 
(x_train,Â y_train),Â (x_test,Â y_test)Â =Â tf.keras.datasets.mnist.load_data() 
#Â ScaleÂ imagesÂ toÂ theÂ [0,Â 1]Â range 
x_trainÂ =Â x_train.astype("float32")Â /Â 255 
x_testÂ =Â x_test.astype("float32")Â /Â 255 
#Â MakeÂ sureÂ imagesÂ haveÂ shapeÂ (28,Â 28,Â 1) 
x_trainÂ =Â np.expand_dims(x_train,Â -1) 
x_testÂ =Â np.expand_dims(x_test,Â -1) 
y_trainÂ =Â tf.keras.utils.to_categorical(y_train,Â num_classes) 
y_testÂ =Â tf.keras.utils.to_categorical(y_test,Â num_classes) 
Â Â Â Â returnÂ Data(x_train,Â y_train,Â x_test,Â y_test)
```

æœ€åˆï¼Œæˆ‘ä»¬å°†ä»*MNIST*æ•°æ®é›†ä¸­ä»…ä½¿ç”¨ 20 ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚ç„¶åæˆ‘ä»¬æ¯æ¬¡è·å– 10 ä¸ªæ•°æ®ç‚¹ï¼Œå¹¶é‡æ–°è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸ºäº†åœ¨å¼€å§‹æ—¶å¸®åŠ©æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†ç¡®ä¿è¿™ 20 ä¸ªæ•°æ®ç‚¹åœ¨æ•°æ®é›†çš„ä¸åŒç±»åˆ«ä¹‹é—´æ˜¯å¹³è¡¡çš„ã€‚ä»¥ä¸‹å‡½æ•°ç»™å‡ºäº†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨çš„ç´¢å¼•ï¼Œç”¨äºåˆ›å»ºåˆå§‹çš„ 20 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªç±»åˆ« 2 ä¸ªæ ·æœ¬ï¼š

```py

defÂ get_random_balanced_indices( 
data:Â Data,Â initial_n_samples:Â int 
)Â -*>*Â np.ndarray: 
labelsÂ =Â np.argmax(data.y_train,Â axis=1) 
indicesÂ =Â [] 
label_listÂ =Â np.unique(labels) 
forÂ labelÂ inÂ label_list: 
indices_labelÂ =Â np.random.choice( 
np.argwhere(labelsÂ ==Â label).flatten(), 
size=initial_n_samplesÂ //Â len(label_list), 
replace=False 
) 
indices.extend(indices_label) 
indicesÂ =Â np.array(indices) 
np.random.shuffle(indices) 
Â Â Â Â returnÂ indices
```

ç„¶åæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªå°å‡½æ•°ï¼Œå®é™…è·å–æˆ‘ä»¬çš„åˆå§‹æ•°æ®é›†ï¼š

```py

defÂ get_initial_ds(data:Â Data,Â initial_n_samples:Â int)Â -*>*Â Data: 
indicesÂ =Â get_random_balanced_indices(data,Â initial_n_samples) 
x_train_al,Â y_train_alÂ =Â data.x_train[indices],Â data.y_train[indices] 
x_trainÂ =Â np.delete(data.x_train,Â indices,Â axis=0) 
y_trainÂ =Â np.delete(data.y_train,Â indices,Â axis=0) 
returnÂ Data( 
x_train,Â y_train,Â data.x_test,Â data.y_test,Â x_train_al,Â y_train_al 
Â Â Â Â )
```

#### æ­¥éª¤ 2ï¼šè®¾ç½®é…ç½®

åœ¨æˆ‘ä»¬å¼€å§‹æ„å»ºæ¨¡å‹å¹¶åˆ›å»ºä¸»åŠ¨å­¦ä¹ å¾ªç¯ä¹‹å‰ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå°çš„é…ç½®`dataclass`æ¥å­˜å‚¨ä¸€äº›åœ¨è¿è¡Œä¸»åŠ¨å­¦ä¹ è„šæœ¬æ—¶å¯èƒ½æƒ³è¦è°ƒæ•´çš„ä¸»è¦å˜é‡ã€‚åˆ›å»ºè¿™æ ·çš„é…ç½®ç±»ä½¿ä½ å¯ä»¥çµæ´»è°ƒæ•´ä¸åŒçš„å‚æ•°ã€‚

```py

@dataclasses.dataclass 
classÂ Config: 
initial_n_samples:Â int 
n_total_samples:Â int 
n_epochs:Â int 
n_samples_per_iter:Â int 
#Â stringÂ representationÂ ofÂ theÂ acquisitionÂ function 
acquisition_type:Â str 
#Â numberÂ ofÂ mc_dropoutÂ iterations 
Â Â Â Â n_iter:Â int
```

#### æ­¥éª¤ 3ï¼šå®šä¹‰æ¨¡å‹

æˆ‘ä»¬ç°åœ¨å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å°å‹ CNN å¹¶åŠ å…¥ Dropoutã€‚

```py

defÂ build_model(): 
modelÂ =Â tf.keras.models.Sequential([ 
Input(shape=(28,Â 28,Â 1)), 
layers.Conv2D(32,Â kernel_size=(4,Â 4),Â activation="relu"), 
layers.Conv2D(32,Â kernel_size=(4,Â 4),Â activation="relu"), 
layers.MaxPooling2D(pool_size=(2,Â 2)), 
layers.Dropout(0.25), 
layers.Flatten(), 
layers.Dense(128,Â activation="relu"), 
layers.Dropout(0.5), 
layers.Dense(10,Â activation="softmax"), 
]) 
model.compile( 
tf.keras.optimizers.Adam(), 
loss="categorical_crossentropy", 
metrics=["accuracy"], 
experimental_run_tf_function=False, 
) 
Â Â Â Â returnÂ model
```

#### æ­¥éª¤ 4ï¼šå®šä¹‰ä¸ç¡®å®šæ€§å‡½æ•°

å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼ˆä¹Ÿç§°ä¸ºçŸ¥è¯†ä¸ç¡®å®šæ€§ï¼‰ä½œä¸ºæˆ‘ä»¬ä¸»è¦çš„ä¸ç¡®å®šæ€§å‡½æ•°æ¥è·å–æ–°æ ·æœ¬ã€‚è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æˆ‘ä»¬é¢„æµ‹çš„è®¤çŸ¥ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬å‡è®¾è¾“å…¥çš„é¢„æµ‹ï¼ˆ`divds`ï¼‰çš„å½¢çŠ¶ä¸º`n_images`ï¼Œ`n_predictions`ï¼Œ`n_classes`ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ€»ä¸ç¡®å®šæ€§ã€‚ç»™å®šä¸€ä¸ªé›†æˆæ¨¡å‹çš„é¢„æµ‹ï¼Œå®ƒå¯ä»¥å®šä¹‰ä¸ºé›†æˆå¹³å‡é¢„æµ‹çš„ç†µã€‚

```py

defÂ total_uncertainty( 
divds:Â np.ndarray,Â epsilon:Â floatÂ =Â 1e-10 
)Â -*>*Â np.ndarray: 
mean_divdsÂ =Â np.mean(divds,Â axis=1) 
log_divdsÂ =Â -np.log(mean_divdsÂ +Â epsilon) 
Â Â Â Â returnÂ np.sum(mean_divdsÂ *Â log_divds,Â axis=1)
```

ç„¶åæˆ‘ä»¬å®šä¹‰æ•°æ®ä¸ç¡®å®šæ€§ï¼ˆæˆ–ç§°ä¸ºéšæœºä¸ç¡®å®šæ€§ï¼‰ï¼Œå¯¹äºä¸€ä¸ªé›†æˆæ¨¡å‹æ¥è¯´ï¼Œå®ƒæ˜¯æ¯ä¸ªé›†æˆæˆå‘˜çš„ç†µçš„å¹³å‡å€¼ã€‚

```py

defÂ data_uncertainty(divds:Â np.ndarray,Â epsilon:Â floatÂ =Â 1e-10)Â -*>*Â np.ndarray: 
log_divdsÂ =Â -np.log(divdsÂ +Â epsilon) 
Â Â Â Â returnÂ np.mean(np.sum(divdsÂ *Â log_divds,Â axis=2),Â axis=1)
```

æœ€ç»ˆï¼Œæˆ‘ä»¬å¾—åˆ°äº†æˆ‘ä»¬çš„çŸ¥è¯†ï¼ˆæˆ–è®¤çŸ¥ï¼‰ä¸ç¡®å®šæ€§ï¼Œè¿™å°±æ˜¯é€šè¿‡ä»é¢„æµ‹çš„æ€»ä¸ç¡®å®šæ€§ä¸­å‡å»æ•°æ®ä¸ç¡®å®šæ€§æ¥å¾—åˆ°çš„ã€‚

```py

defÂ knowledge_uncertainty( 
divds:Â np.ndarray,Â epsilon:Â floatÂ =Â 1e-10 
)Â -*>*Â np.ndarray: 
Â Â Â Â returnÂ total_uncertainty(divds,Â epsilon)Â -Â data_uncertainty(divds,Â epsilon)
```

å®šä¹‰äº†è¿™äº›ä¸ç¡®å®šæ€§å‡½æ•°åï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰å®é™…çš„è·å–å‡½æ•°ï¼Œå®ƒä»¬çš„ä¸»è¦è¾“å…¥æ˜¯æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®å’Œæ¨¡å‹ã€‚ä¸ºäº†é€šè¿‡çŸ¥è¯†ä¸ç¡®å®šæ€§æ¥è·å–æ ·æœ¬ï¼Œæˆ‘ä»¬è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š

1.  é€šè¿‡ MC Dropout è·å–æˆ‘ä»¬çš„é›†æˆé¢„æµ‹ã€‚

1.  è®¡ç®—è¿™ä¸ªé›†æˆæ¨¡å‹çš„çŸ¥è¯†ä¸ç¡®å®šæ€§å€¼ã€‚

1.  å¯¹ä¸ç¡®å®šæ€§å€¼è¿›è¡Œæ’åºï¼Œè·å–å®ƒä»¬çš„ç´¢å¼•ï¼Œå¹¶è¿”å›æˆ‘ä»¬è®­ç»ƒæ•°æ®ä¸­å…·æœ‰æœ€é«˜è®¤çŸ¥ä¸ç¡®å®šæ€§çš„ç´¢å¼•ã€‚

ç„¶åï¼Œç¨åæˆ‘ä»¬å¯ä»¥é‡å¤ä½¿ç”¨è¿™äº›ç´¢å¼•æ¥ç´¢å¼•æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ï¼Œå®é™…ä¸Šè·å–æˆ‘ä»¬æƒ³è¦æ·»åŠ çš„è®­ç»ƒæ ·æœ¬ã€‚

```py

fromÂ typingÂ importÂ Callable 
fromÂ kerasÂ importÂ Model 
fromÂ tqdmÂ importÂ tqdm 

importÂ numpyÂ asÂ np 

defÂ acquire_knowledge_uncertainty( 
x_train:Â np.ndarray, 
n_samples:Â int, 
model:Â Model, 
n_iter:Â int, 
*args, 
**kwargs 
): 
divdsÂ =Â get_mc_predictions(model,Â n_iter,Â x_train) 
kuÂ =Â knowledge_uncertainty(divds) 
Â Â Â Â returnÂ np.argsort(ku,Â axis=-1)[-n_samples:]
```

æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å¾— MC Dropout é¢„æµ‹ï¼š

```py

defÂ get_mc_predictions( 
model:Â Model,Â n_iter:Â int,Â x_train:Â np.ndarray 
)Â -*>*Â np.ndarray: 
divdsÂ =Â [] 
forÂ _Â inÂ tqdm(range(n_iter)): 
divds_iterÂ =Â [ 
model(batch,Â training=True) 
forÂ batchÂ inÂ np.array_split(x_train,Â 6) 
] 
divds.append(np.concatenate(divds_iter)) 
#Â formatÂ dataÂ suchÂ thatÂ weÂ haveÂ n_images,Â n_predictions,Â n_classes 
divdsÂ =Â np.moveaxis(np.stack(divds),Â 0,Â 1) 
Â Â Â Â returnÂ divds
```

ä¸ºäº†é¿å…å†…å­˜æº¢å‡ºï¼Œæˆ‘ä»¬å°†è®­ç»ƒæ•°æ®åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹ 6 ä¸ªæ ·æœ¬ï¼Œå¯¹äºæ¯ä¸€æ‰¹ï¼Œæˆ‘ä»¬å°†è®¡ç®—`n_iter`æ¬¡é¢„æµ‹ã€‚ä¸ºäº†ç¡®ä¿æˆ‘ä»¬çš„é¢„æµ‹å…·æœ‰å¤šæ ·æ€§ï¼Œæˆ‘ä»¬å°†æ¨¡å‹çš„`training`å‚æ•°è®¾ç½®ä¸º`True`ã€‚

å¯¹äºæˆ‘ä»¬çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ªè·å–å‡½æ•°ï¼Œè¯¥å‡½æ•°è¿”å›ä¸€ä¸ªéšæœºçš„ç´¢å¼•åˆ—è¡¨ï¼š

```py

defÂ acquire_random(x_train:Â np.ndarray,Â n_samples:Â int,Â *args,Â **kwargs): 
Â Â Â Â returnÂ np.random.randint(low=0,Â high=len(x_train),Â size=n_samples)
```

æœ€åï¼Œæˆ‘ä»¬æ ¹æ®*å·¥å‚æ–¹æ³•æ¨¡å¼*å®šä¹‰ä¸€ä¸ªå°å‡½æ•°ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬å¯ä»¥åœ¨å¾ªç¯ä¸­ä½¿ç”¨ç›¸åŒçš„å‡½æ•°ï¼Œä½¿ç”¨éšæœºé‡‡é›†å‡½æ•°æˆ–çŸ¥è¯†ä¸ç¡®å®šæ€§ã€‚åƒè¿™æ ·çš„å·¥å‚å°å‡½æ•°æœ‰åŠ©äºåœ¨ä½ æƒ³ç”¨ä¸åŒé…ç½®è¿è¡Œç›¸åŒä»£ç æ—¶ä¿æŒä»£ç æ¨¡å—åŒ–ã€‚

```py

defÂ acquisition_factory(acquisition_type:Â str)Â -*>*Â Callable: 
ifÂ acquisition_typeÂ ==Â "knowledge_uncertainty": 
returnÂ acquire_knowledge_uncertainty 
ifÂ acquisition_typeÂ ==Â "random": 
Â Â Â Â Â Â Â Â returnÂ acquire_random
```

ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†é‡‡é›†å‡½æ•°ï¼Œæˆ‘ä»¬å·²ç»å‡†å¤‡å¥½å®é™…å®šä¹‰è¿è¡Œæˆ‘ä»¬ä¸»åŠ¨å­¦ä¹ è¿­ä»£çš„å¾ªç¯ã€‚

#### ç¬¬ 5 æ­¥ï¼šå®šä¹‰å¾ªç¯

é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„é…ç½®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨çŸ¥è¯†ä¸ç¡®å®šæ€§ä½œä¸ºæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§å‡½æ•°ã€‚åœ¨å¦ä¸€ä¸ªå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªéšæœºé‡‡é›†å‡½æ•°æ¥æ¯”è¾ƒæˆ‘ä»¬å³å°†å®šä¹‰çš„å¾ªç¯ç»“æœã€‚æˆ‘ä»¬å°†ä» 20 ä¸ªæ ·æœ¬å¼€å§‹æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œç›´åˆ°æˆ‘ä»¬è¾¾åˆ° 1,000 ä¸ªæ ·æœ¬ã€‚æ¯ä¸ªæ¨¡å‹å°†è®­ç»ƒ 50 ä¸ª epochï¼Œæ¯æ¬¡è¿­ä»£æˆ‘ä»¬è·å– 10 ä¸ªæ ·æœ¬ã€‚ä¸ºäº†è·å¾—æˆ‘ä»¬çš„ MC dropout é¢„æµ‹ï¼Œæˆ‘ä»¬å°†åœ¨æ•´ä¸ªè®­ç»ƒé›†ï¼ˆå‡å»å·²è·å–çš„æ ·æœ¬ï¼‰ä¸Šè¿è¡Œ 100 æ¬¡ã€‚

```py

cfgÂ =Â Config( 
initial_n_samples=20, 
n_total_samples=1000, 
n_epochs=50, 
n_samples_per_iteration=10, 
acquisition_type="knowledge_uncertainty", 
n_iter=100, 
)
```

ç„¶åæˆ‘ä»¬å¯ä»¥è·å–æ•°æ®ï¼Œå¹¶å®šä¹‰ä¸€ä¸ªç©ºå­—å…¸æ¥è·Ÿè¸ªæ¯æ¬¡è¿­ä»£çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè·Ÿè¸ªæˆ‘ä»¬æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­çš„æ‰€æœ‰ç´¢å¼•ã€‚

```py

data:Â DataÂ =Â get_initial_ds(get_data(),Â cfg.initial_n_samples) 
accuraciesÂ =Â {} 
added_indicesÂ =Â []
```

æˆ‘ä»¬è¿˜ä¸ºæˆ‘ä»¬çš„è¿è¡Œåˆ†é…äº†ä¸€ä¸ª**å…¨çƒå”¯ä¸€æ ‡è¯†ç¬¦**ï¼ˆ**UUID**ï¼‰ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬å¯ä»¥è½»æ¾æ‰¾åˆ°å®ƒï¼Œå¹¶ä¸”ä¸ä¼šè¦†ç›–æˆ‘ä»¬ä½œä¸ºå¾ªç¯ä¸€éƒ¨åˆ†ä¿å­˜çš„ç»“æœã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç›®å½•æ¥ä¿å­˜æˆ‘ä»¬çš„æ•°æ®ï¼Œå¹¶å°†é…ç½®ä¿å­˜åˆ°è¯¥ç›®å½•ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬å§‹ç»ˆçŸ¥é“`model_dir`ä¸­çš„æ•°æ®æ˜¯ä½¿ç”¨ä½•ç§é…ç½®åˆ›å»ºçš„ã€‚

```py

run_uuidÂ =Â str(uuid.uuid4()) 
model_dirÂ =Â Path("./models")Â /Â cfg.acquisition_typeÂ /Â run_uuid 
model_dir.mkdir(parents=True,Â exist_ok=True)
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å®é™…è¿è¡Œæˆ‘ä»¬çš„ä¸»åŠ¨å­¦ä¹ å¾ªç¯ã€‚æˆ‘ä»¬å°†æŠŠè¿™ä¸ªå¾ªç¯åˆ†æˆä¸‰ä¸ªéƒ¨åˆ†ï¼š

1.  æˆ‘ä»¬å®šä¹‰å¾ªç¯ï¼Œå¹¶åœ¨å·²è·å–çš„æ ·æœ¬ä¸Šæ‹Ÿåˆæ¨¡å‹ï¼š

    ```py

    forÂ iÂ inÂ range(cfg.n_total_samplesÂ //Â cfg.n_samples_per_iter): 
    iter_dirÂ =Â model_dirÂ /Â str(i) 
    modelÂ =Â build_model() 
    model.fit( 
    x=data.x_train_al, 
    y=data.y_train_al, 
    validation_data=(data.x_test,Â data.y_test), 
    epochs=cfg.n_epochs, 
    callbacks=[get_callback(iter_dir)], 
    verbose=2, 
    Â Â Â Â )
    ```

1.  ç„¶åï¼Œæˆ‘ä»¬åŠ è½½å…·æœ‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡çš„æ¨¡å‹ï¼Œå¹¶æ ¹æ®é‡‡é›†å‡½æ•°æ›´æ–°æˆ‘ä»¬çš„æ•°æ®é›†ï¼š

    ```py

    modelÂ =Â tf.keras.models.load_model(iter_dir) 
    indices_to_addÂ =Â acquisition_factory(cfg.acquisition_type)( 
    data.x_train, 
    cfg.n_samples_per_iter, 
    n_iter=cfg.n_iter, 
    model=model, 
    ) 
    added_indices.append(indices_to_add) 
    Â Â Â Â data,Â (iter_x,Â iter_y)Â =Â update_ds(data,Â indices_to_add)
    ```

1.  æœ€åï¼Œæˆ‘ä»¬ä¿å­˜å·²æ·»åŠ çš„å›¾ç‰‡ï¼Œè®¡ç®—æµ‹è¯•å‡†ç¡®ç‡ï¼Œå¹¶ä¿å­˜ç»“æœï¼š

    ```py

    save_images_and_labels_added(iter_dir,Â iter_x,Â iter_y) 
    divdsÂ =Â model(data.x_test) 
    accuracyÂ =Â get_accuracy(data.y_test,Â divds) 
    accuracies[i]Â =Â accuracy 
    Â Â Â Â save_results(accuracies,Â added_indices,Â model_dir)
    ```

åœ¨è¿™ä¸ªå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€äº›å°çš„è¾…åŠ©å‡½æ•°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„æ¨¡å‹å®šä¹‰äº†ä¸€ä¸ªå›è°ƒï¼Œä»¥å°†å…·æœ‰æœ€é«˜éªŒè¯å‡†ç¡®ç‡çš„æ¨¡å‹ä¿å­˜åˆ°æˆ‘ä»¬çš„æ¨¡å‹ç›®å½•ï¼š

```py

defÂ get_callback(model_dir:Â Path): 
model_checkpoint_callbackÂ =Â tf.keras.callbacks.ModelCheckpoint( 
str(model_dir), 
monitor="val_accuracy", 
verbose=0, 
save_best_only=True, 
) 
Â Â Â Â returnÂ model_checkpoint_callback
```

æˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æµ‹è¯•é›†çš„å‡†ç¡®ç‡ï¼š

```py

defÂ get_accuracy(y_test:Â np.ndarray,Â divds:Â np.ndarray)Â -*>*Â float: 
accÂ =Â tf.keras.metrics.CategoricalAccuracy() 
acc.update_state(divds,Â y_test) 
Â Â Â Â returnÂ acc.result().numpy()Â *Â 100
```

æˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸¤ä¸ªå°å‡½æ•°ï¼Œç”¨äºæ¯æ¬¡è¿­ä»£ä¿å­˜ç»“æœï¼š

```py

defÂ save_images_and_labels_added( 
output_path:Â Path,Â iter_x:Â np.ndarray,Â iter_y:Â np.ndarray 
): 
dfÂ =Â pd.DataFrame() 
df["label"]Â =Â np.argmax(iter_y,Â axis=1) 
iter_x_normalisedÂ =Â (np.squeeze(iter_x,Â axis=-1)Â *Â 255).astype(np.uint8) 
df["image"]Â =Â iter_x_normalised.reshape(10,Â 28*28).tolist() 
df.to_parquet(output_pathÂ /Â "added.parquet",Â index=False) 

defÂ save_results( 
accuracies:Â Dict[int,Â float],Â added_indices:Â List[int],Â model_dir:Â Path 
): 
dfÂ =Â pd.DataFrame(accuracies.items(),Â columns=["i",Â "accuracy"]) 
df["added"]Â =Â added_indices 
Â Â Â Â df.to_parquet(f"{model_dir}/results.parquet",Â index=False)
```

è¯·æ³¨æ„ï¼Œè¿è¡Œä¸»åŠ¨å­¦ä¹ å¾ªç¯éœ€è¦ç›¸å½“é•¿çš„æ—¶é—´ï¼šæ¯æ¬¡è¿­ä»£ï¼Œæˆ‘ä»¬è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹ 50 ä¸ª epochï¼Œç„¶ååœ¨æˆ‘ä»¬çš„æ± é›†ï¼ˆå®Œæ•´çš„è®­ç»ƒæ•°æ®é›†å‡å»å·²è·å–çš„æ ·æœ¬ï¼‰ä¸Šè¿è¡Œ 100 æ¬¡ã€‚ä½¿ç”¨éšæœºé‡‡é›†å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬è·³è¿‡æœ€åä¸€æ­¥ï¼Œä½†ä»ç„¶æ¯æ¬¡è¿­ä»£å°†éªŒè¯æ•°æ®è¿è¡Œ 50 æ¬¡ï¼Œä»¥ç¡®ä¿ä½¿ç”¨å…·æœ‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡çš„æ¨¡å‹ã€‚è¿™éœ€è¦æ—¶é—´ï¼Œä½†ä»…ä»…é€‰æ‹©å…·æœ‰æœ€ä½³*è®­ç»ƒ*å‡†ç¡®ç‡çš„æ¨¡å‹æ˜¯æœ‰é£é™©çš„ï¼šæˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤šæ¬¡çœ‹åˆ°ç›¸åŒçš„å‡ å¼ å›¾ç‰‡ï¼Œå› æ­¤å¾ˆå¯èƒ½ä¼šè¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚

#### ç¬¬ 6 æ­¥ï¼šæ£€æŸ¥ç»“æœ

ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰äº†å¾ªç¯ï¼Œå¯ä»¥æ£€æŸ¥è¿™ä¸ªè¿‡ç¨‹çš„ç»“æœã€‚æˆ‘ä»¬å°†ä½¿ç”¨`seaborn`å’Œ`matplotlib`æ¥å¯è§†åŒ–æˆ‘ä»¬çš„ç»“æœï¼š

```py

importÂ seabornÂ asÂ sns 
importÂ matplotlib.pyplotÂ asÂ plt 
importÂ pandasÂ asÂ pd 
importÂ numpyÂ asÂ np 
sns.set_style("darkgrid") 
sns.set_context("paper")
```

æˆ‘ä»¬æœ€æ„Ÿå…´è¶£çš„ä¸»è¦ç»“æœæ˜¯ä¸¤ç§æ¨¡å‹çš„æµ‹è¯•å‡†ç¡®ç‡éšæ—¶é—´çš„å˜åŒ–ï¼Œè¿™äº›æ¨¡å‹åˆ†åˆ«æ˜¯åŸºäºéšæœºè·å–å‡½æ•°è®­ç»ƒçš„æ¨¡å‹å’Œé€šè¿‡çŸ¥è¯†ä¸ç¡®å®šæ€§è·å–æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚ä¸ºäº†å¯è§†åŒ–è¿™ä¸ªç»“æœï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼ŒåŠ è½½ç»“æœå¹¶è¿”å›ä¸€ä¸ªå›¾è¡¨ï¼Œæ˜¾ç¤ºæ¯ä¸ªä¸»åŠ¨å­¦ä¹ è¿­ä»£å‘¨æœŸçš„å‡†ç¡®ç‡ï¼š

```py

defÂ plot(uuid:Â str,Â acquisition:Â str,Â ax=None): 
acq_nameÂ =Â acquisition.replace("_",Â "Â ") 
dfÂ =Â pd.read_parquet(f"./models/{acquisition}/{uuid}/results.parquet")[:-1] 
dfÂ =Â df.rename(columns={"accuracy":Â acq_name}) 
df["n_samples"]Â =Â df["i"].apply(lambdaÂ x:Â x*10Â +Â 20) 
returnÂ df.plot.line( 
x="n_samples",Â y=acq_name,Â style='.-',Â figsize=(8,5),Â ax=ax 
Â Â Â Â )
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªå‡½æ•°ç»˜åˆ¶ä¸¤ä¸ªè·å–å‡½æ•°çš„ç»“æœï¼š

```py

axÂ =Â plot("bc1adec5-bc34-44a6-a0eb-fa7cb67854e4",Â "random") 
axÂ =Â plot( 
"5c8d6001-a5fb-45d3-a7cb-2a8a46b93d18",Â "knowledge_uncertainty",Â ax=ax 
) 
plt.xticks(np.arange(0,Â 1050,Â 50)) 
plt.yticks(np.arange(54,Â 102,Â 2)) 
plt.ylabel("Accuracy") 
plt.xlabel("NumberÂ ofÂ acquiredÂ samples") 
plt.show()
```

è¿™å°†äº§ç”Ÿä»¥ä¸‹è¾“å‡ºï¼š

![å›¾ç‰‡](img/file172.png)

å›¾ 8.8ï¼šä¸»åŠ¨å­¦ä¹ ç»“æœ

*å›¾* *8.8* æ˜¾ç¤ºï¼Œé€šè¿‡çŸ¥è¯†ä¸ç¡®å®šæ€§è·å–æ ·æœ¬å¼€å§‹æ˜¾è‘—æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤§çº¦è·å–äº† 300 ä¸ªæ ·æœ¬ä¹‹åã€‚è¯¥æ¨¡å‹çš„æœ€ç»ˆå‡†ç¡®ç‡æ¯”éšæœºæ ·æœ¬è®­ç»ƒçš„æ¨¡å‹é«˜å‡ºå¤§çº¦ä¸¤ä¸ªç™¾åˆ†ç‚¹ã€‚è™½ç„¶è¿™çœ‹èµ·æ¥ä¸å¤šï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥ä»å¦ä¸€ä¸ªè§’åº¦æ¥åˆ†ææ•°æ®ï¼šä¸ºäº†å®ç°ç‰¹å®šçš„å‡†ç¡®ç‡ï¼Œéœ€è¦å¤šå°‘æ ·æœ¬ï¼Ÿå¦‚æœæˆ‘ä»¬æ£€æŸ¥å›¾è¡¨ï¼Œå¯ä»¥çœ‹åˆ°ï¼ŒçŸ¥è¯†ä¸ç¡®å®šæ€§çº¿åœ¨ 400 ä¸ªè®­ç»ƒæ ·æœ¬ä¸‹è¾¾åˆ°äº† 96%çš„å‡†ç¡®ç‡ã€‚è€Œéšæœºæ ·æœ¬è®­ç»ƒçš„æ¨¡å‹åˆ™è‡³å°‘éœ€è¦ 750 ä¸ªæ ·æœ¬æ‰èƒ½è¾¾åˆ°ç›¸åŒçš„å‡†ç¡®ç‡ã€‚è¿™æ„å‘³ç€ï¼Œåœ¨ç›¸åŒå‡†ç¡®ç‡ä¸‹ï¼ŒçŸ¥è¯†ä¸ç¡®å®šæ€§æ–¹æ³•åªéœ€è¦å‡ ä¹ä¸€åŠçš„æ•°æ®é‡ã€‚è¿™è¡¨æ˜ï¼Œé‡‡ç”¨æ­£ç¡®çš„è·å–å‡½æ•°è¿›è¡Œä¸»åŠ¨å­¦ä¹ éå¸¸æœ‰ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—èµ„æºå……è¶³ä½†æ ‡æ³¨æˆæœ¬æ˜‚è´µçš„æƒ…å†µä¸‹ï¼šé€šè¿‡æ­£ç¡®é€‰æ‹©æ ·æœ¬ï¼Œæˆ‘ä»¬å¯èƒ½èƒ½å¤Ÿå°†æ ‡æ³¨æˆæœ¬é™ä½ä¸€å€ï¼Œä»è€Œå®ç°ç›¸åŒçš„å‡†ç¡®ç‡ã€‚

å› ä¸ºæˆ‘ä»¬ä¿å­˜äº†æ¯æ¬¡è¿­ä»£è·å–çš„æ ·æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿå¯ä»¥æ£€æŸ¥ä¸¤ç§æ¨¡å‹é€‰æ‹©çš„å›¾åƒç±»å‹ã€‚ä¸ºäº†ä½¿æˆ‘ä»¬çš„å¯è§†åŒ–æ›´æ˜“äºè§£é‡Šï¼Œæˆ‘ä»¬å°†å¯è§†åŒ–æ¯ç§æ–¹æ³•å¯¹äºæ¯ä¸ªæ ‡ç­¾æ‰€é€‰çš„æœ€åäº”ä¸ªå›¾åƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¿”å›æ¯ä¸ªæ ‡ç­¾çš„å›¾åƒé›†ï¼Œå¯¹äºä¸€ç»„æ¨¡å‹ç›®å½•ï¼š

```py

defÂ get_imgs_per_label(model_dirs)Â -*>*Â Dict[int,Â np.ndarray]: 
imgs_per_labelÂ =Â {i:Â []Â forÂ iÂ inÂ range(10)} 
forÂ model_dirÂ inÂ model_dirs: 
dfÂ =Â pd.read_parquet(model_dirÂ /Â "images_added.parquet") 
df.imageÂ =Â df.image.apply( 
lambdaÂ x:Â x.reshape(28,Â 28).astype(np.uint8) 
) 
forÂ labelÂ inÂ df.label.unique(): 
dffÂ =Â df[df.labelÂ ==Â label] 
ifÂ len(dff)Â ==Â 0: 
continue 
imgs_per_label[label].append(np.hstack(dff.image)) 
Â Â Â Â returnÂ imgs_per_label
```

ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œåˆ›å»ºä¸€ä¸ª`PIL å›¾åƒ`ï¼Œå…¶ä¸­æŒ‰æ ‡ç­¾å°†å›¾åƒè¿›è¡Œæ‹¼æ¥ï¼Œä»¥ä¾¿ç”¨äºç‰¹å®šçš„è·å–å‡½æ•°ï¼š

```py

fromÂ PILÂ importÂ Image 
fromÂ pathlibÂ importÂ Path 

defÂ get_added_images( 
acquisition:Â str,Â uuid:Â str,Â n_iter:Â intÂ =Â 5 
)Â -*>*Â Image: 
base_dirÂ =Â Path("./models")Â /Â acquisitionÂ /Â uuid 
model_dirsÂ =Â filter(lambdaÂ x:Â x.is_dir(),Â base_dir.iterdir()) 
model_dirsÂ =Â sorted(model_dirs,Â key=lambdaÂ x:Â int(x.stem)) 
imgs_per_labelÂ =Â get_imgs_per_label(model_dirs) 
imgsÂ =Â [] 
forÂ iÂ inÂ range(10): 
label_imgÂ =Â np.hstack(imgs_per_label[i])[:,Â -(28Â *Â n_iter):] 
imgs.append(label_img) 
Â Â Â Â returnÂ Image.fromarray(np.vstack(imgs))
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨è¿™äº›å‡½æ•°ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ä½¿ç”¨ä»¥ä¸‹è®¾ç½®å’Œ*UUID*ï¼š

```py

uuidÂ =Â "bc1adec5-bc34-44a6-a0eb-fa7cb67854e4" 
img_randomÂ =Â get_added_images("random",Â uuid) 
uuidÂ =Â "5c8d6001-a5fb-45d3-a7cb-2a8a46b93d18" 
img_kuÂ =Â get_added_images("knowledge_uncertainty",Â uuid)
```

è®©æˆ‘ä»¬æ¯”è¾ƒä¸€ä¸‹è¾“å‡ºã€‚

![å›¾ç‰‡](img/file173.png)![å›¾ç‰‡](img/file174.png)

å›¾ 8.9ï¼šéšæœºé€‰æ‹©çš„å›¾åƒï¼ˆå·¦ï¼‰ä¸é€šè¿‡çŸ¥è¯†ä¸ç¡®å®šæ€§å’Œ MC ä¸¢å¼ƒæ³•é€‰æ‹©çš„å›¾åƒï¼ˆå³ï¼‰ã€‚æ¯ä¸€è¡Œæ˜¾ç¤ºæ¯ä¸ªæ ‡ç­¾æ‰€é€‰çš„æœ€åäº”ä¸ªå›¾åƒ

æˆ‘ä»¬å¯ä»¥åœ¨*å›¾* *8.9*ä¸­çœ‹åˆ°ï¼Œé€šè¿‡çŸ¥è¯†ä¸ç¡®å®šæ€§è·å–å‡½æ•°é€‰å–çš„å›¾åƒç›¸æ¯”éšæœºé€‰æ‹©çš„å›¾åƒå¯èƒ½æ›´éš¾ä»¥åˆ†ç±»ã€‚è¿™ä¸ªä¸ç¡®å®šæ€§è·å–å‡½æ•°é€‰æ‹©äº†æ•°æ®é›†ä¸­ä¸€äº›ä¸å¯»å¸¸çš„æ•°å­—è¡¨ç¤ºã€‚ç”±äºæˆ‘ä»¬çš„è·å–å‡½æ•°èƒ½å¤Ÿé€‰å–è¿™äº›å›¾åƒï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ•°æ®é›†çš„æ•´ä½“åˆ†å¸ƒï¼Œä»è€Œéšç€æ—¶é—´çš„æ¨ç§»æé«˜äº†å‡†ç¡®ç‡ã€‚

## 8.5 ä½¿ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡å®ç°æ›´æ™ºèƒ½çš„å¼ºåŒ–å­¦ä¹ 

**å¼ºåŒ–å­¦ä¹ **æ—¨åœ¨å¼€å‘èƒ½å¤Ÿä»ç¯å¢ƒä¸­å­¦ä¹ çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ã€‚å¼ºåŒ–å­¦ä¹ èƒŒåçš„åŸºæœ¬åŸåˆ™åœ¨å…¶åç§°ä¸­æœ‰ä¸€ä¸çº¿ç´¢ï¼šç›®æ ‡æ˜¯åŠ å¼ºæˆåŠŸçš„è¡Œä¸ºã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ç¯å¢ƒä¸­æ‰§è¡Œä¸€ç³»åˆ—çš„åŠ¨ä½œã€‚åœ¨è¿™äº›åŠ¨ä½œä¹‹åï¼Œæ™ºèƒ½ä½“ä»ç¯å¢ƒä¸­è·å¾—åé¦ˆï¼Œè€Œè¿™äº›åé¦ˆè¢«ç”¨æ¥å¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£å“ªäº›åŠ¨ä½œæ›´å¯èƒ½å¯¼è‡´åœ¨å½“å‰ç¯å¢ƒçŠ¶æ€ä¸‹è·å¾—ç§¯æçš„ç»“æœã€‚

ä»å½¢å¼ä¸Šè®²ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ç»„çŠ¶æ€ `S`ã€ä¸€ç»„åŠ¨ä½œ `A` æ¥æè¿°å®ƒä»¬å¦‚ä½•ä»å½“å‰çŠ¶æ€ `s` è½¬æ¢åˆ°æ–°çš„çŠ¶æ€ `s`^â€²ï¼Œä»¥åŠå¥–åŠ±å‡½æ•° `R`(*s,s*^â€²)ï¼Œæè¿°å½“å‰çŠ¶æ€ `s` å’Œæ–°çŠ¶æ€ `s`^â€² ä¹‹é—´çš„è¿‡æ¸¡å¥–åŠ±ã€‚çŠ¶æ€é›†ç”±ç¯å¢ƒçŠ¶æ€é›† `S`[`e`] å’Œæ™ºèƒ½ä½“çŠ¶æ€é›† `S`[`a`] ç»„æˆï¼Œä¸¤è€…å…±åŒæè¿°æ•´ä¸ªç³»ç»Ÿçš„çŠ¶æ€ã€‚

æˆ‘ä»¬å¯ä»¥å°†æ­¤ç±»æ¯”ä¸ºä¸€åœºé©¬å¯Â·æ³¢ç½—çš„æ¸¸æˆï¼Œå…¶ä¸­ä¸€ä¸ªç©å®¶é€šè¿‡â€œå–Šå«â€ä¸â€œå›åº”â€çš„æ–¹å¼æ¥æ‰¾åˆ°å¦ä¸€ä¸ªç©å®¶ã€‚å½“å¯»æ‰¾çš„ç©å®¶å–Šâ€œMarcoâ€æ—¶ï¼Œå¦ä¸€ä¸ªç©å®¶å›åº”â€œPoloâ€ï¼Œæ ¹æ®å£°éŸ³çš„æ–¹å‘å’Œå¹…åº¦ç»™å‡ºå¯»æ‰¾è€…å…¶ä½ç½®çš„ä¼°è®¡ã€‚å¦‚æœæˆ‘ä»¬å°†æ­¤ç®€åŒ–ä¸ºè€ƒè™‘è·ç¦»ï¼Œé‚£ä¹ˆè¾ƒè¿‘çš„çŠ¶æ€æ˜¯è·ç¦»å‡å°‘çš„çŠ¶æ€ï¼Œä¾‹å¦‚*Î´* = `d` âˆ’ `d`^â€² *>* 0ï¼Œå…¶ä¸­ `d` æ˜¯çŠ¶æ€ `s` çš„è·ç¦»ï¼Œ`d`^â€² æ˜¯çŠ¶æ€ `s`^â€² çš„è·ç¦»ã€‚ç›¸åï¼Œè¾ƒè¿œçš„çŠ¶æ€æ˜¯*Î´* = `d` âˆ’ `d`^â€² *<* 0ã€‚å› æ­¤ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„*Î´*å€¼ä½œä¸ºæ¨¡å‹çš„åé¦ˆï¼Œä½¿å¾—æˆ‘ä»¬çš„å¥–åŠ±å‡½æ•°ä¸º*Î´* = `R`(*s,s*^â€²) = `d` âˆ’ `d`^â€²ã€‚

![PIC](img/file175.jpg)

å›¾ 8.10ï¼šé©¬å¯Â·æ³¢ç½—å¼ºåŒ–å­¦ä¹ åœºæ™¯çš„æ’å›¾

è®©æˆ‘ä»¬æŠŠæ™ºèƒ½ä½“è§†ä¸ºå¯»æ‰¾ç©å®¶ï¼ŒæŠŠç›®æ ‡è§†ä¸ºéšè—ç©å®¶ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œæ™ºèƒ½ä½“ä¼šæ”¶é›†æ›´å¤šå…³äºç¯å¢ƒçš„ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°å»ºæ¨¡å…¶è¡ŒåŠ¨ `A`(`s`) å’Œå¥–åŠ±å‡½æ•° `R`(*s,s*^â€²) ä¹‹é—´çš„å…³ç³»ï¼ˆæ¢å¥è¯è¯´ï¼Œå®ƒåœ¨å­¦ä¹ éœ€è¦æœå“ªä¸ªæ–¹å‘ç§»åŠ¨ï¼Œä»¥ä¾¿æ›´æ¥è¿‘ç›®æ ‡ï¼‰ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦é¢„æµ‹å¥–åŠ±å‡½æ•°ï¼Œç»™å®šå½“å‰çŠ¶æ€ä¸‹çš„å¯èƒ½è¡ŒåŠ¨é›† `A`[`s`]ï¼Œä»¥ä¾¿é€‰æ‹©æœ€æœ‰å¯èƒ½æœ€å¤§åŒ–è¯¥å¥–åŠ±å‡½æ•°çš„è¡ŒåŠ¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¡ŒåŠ¨é›†å¯ä»¥æ˜¯æˆ‘ä»¬å¯ä»¥ç§»åŠ¨çš„æ–¹å‘é›†ï¼Œä¾‹å¦‚ï¼šå‰è¿›ã€åé€€ã€å·¦è½¬å’Œå³è½¬ã€‚

ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ä½¿ç”¨ä¸€ç§å«åš**Q å­¦ä¹ **çš„æ–¹æ³•æ¥å­¦ä¹ çŠ¶æ€ã€è¡ŒåŠ¨å’Œå¥–åŠ±ä¹‹é—´çš„å…³ç³»ã€‚Q å­¦ä¹ ä¸æ¶‰åŠç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œè€Œæ˜¯å°†çŠ¶æ€ã€è¡ŒåŠ¨å’Œå¥–åŠ±ä¿¡æ¯å­˜å‚¨åœ¨ä¸€ä¸ªè¡¨æ ¼ä¸­â€”â€”Q è¡¨æ ¼â€”â€”ç„¶åç”¨æ¥ç¡®å®šåœ¨å½“å‰çŠ¶æ€ä¸‹æœ€æœ‰å¯èƒ½äº§ç”Ÿæœ€é«˜å¥–åŠ±çš„è¡ŒåŠ¨ã€‚è™½ç„¶ Q å­¦ä¹ éå¸¸å¼ºå¤§ï¼Œä½†å¯¹äºå¤§é‡çš„çŠ¶æ€å’Œè¡ŒåŠ¨ï¼Œå®ƒçš„è®¡ç®—æˆæœ¬å˜å¾—ä¸å¯æ‰¿å—ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜å¼•å…¥äº†**æ·±åº¦ Q å­¦ä¹ **çš„æ¦‚å¿µï¼Œå…¶ä¸­ Q è¡¨æ ¼è¢«ç¥ç»ç½‘ç»œæ‰€æ›¿ä»£ã€‚åœ¨é€šå¸¸ç»è¿‡å¤§é‡è¿­ä»£åï¼Œç¥ç»ç½‘ç»œä¼šå­¦ä¹ åœ¨ç»™å®šå½“å‰çŠ¶æ€çš„æƒ…å†µä¸‹ï¼Œå“ªäº›è¡ŒåŠ¨æ›´æœ‰å¯èƒ½äº§ç”Ÿæ›´é«˜çš„å¥–åŠ±ã€‚

ä¸ºäº†é¢„æµ‹å“ªç§è¡ŒåŠ¨å¯èƒ½äº§ç”Ÿæœ€é«˜çš„å¥–åŠ±å€¼ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç»è¿‡è®­ç»ƒçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºæ‰€æœ‰å†å²è¡ŒåŠ¨ `A`[`h`]ã€çŠ¶æ€ `S`[`h`] å’Œå¥–åŠ± `R`[`h`]ã€‚æˆ‘ä»¬çš„è®­ç»ƒè¾“å…¥ `X` åŒ…å«è¡ŒåŠ¨ `A`[`h`] å’ŒçŠ¶æ€ `S`[`h`]ï¼Œè€Œç›®æ ‡è¾“å‡º `y` åŒ…å«å¥–åŠ±å€¼ `R`[`h`]ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†è¯¥æ¨¡å‹ä½œä¸º**æ¨¡å‹é¢„æµ‹æ§åˆ¶å™¨**ï¼ˆ**MPC**ï¼‰çš„ä¸€éƒ¨åˆ†ï¼Œé€‰æ‹©è¡ŒåŠ¨ï¼Œä¾æ®æ˜¯å“ªä¸ªè¡ŒåŠ¨ä¸æœ€é«˜é¢„æµ‹å¥–åŠ±ç›¸å…³ï¼š

![anext = argmax yiâˆ€ai âˆˆ As ](img/file176.jpg)

è¿™é‡Œï¼Œ`y`[`i`] æ˜¯æˆ‘ä»¬çš„æ¨¡å‹äº§ç”Ÿçš„å¥–åŠ±é¢„æµ‹ï¼Œ`f`(`a`[`i`]*,s*)ï¼Œå®ƒå°†å½“å‰çŠ¶æ€ `s` å’Œå¯èƒ½çš„åŠ¨ä½œ `a`[`i`] âˆˆ `A`[`s`] æ˜ å°„åˆ°å¥–åŠ±å€¼ã€‚ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬çš„æ¨¡å‹æœ‰ä»»ä½•ç”¨å¤„ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ”¶é›†æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å°†åœ¨å¤šä¸ªå›åˆä¸­ç§¯ç´¯æ•°æ®ï¼Œæ¯ä¸ªå›åˆåŒ…æ‹¬ä»£ç†é‡‡å–çš„ä¸€ç³»åˆ—åŠ¨ä½œï¼Œç›´åˆ°æ»¡è¶³æŸäº›ç»ˆæ­¢æ ‡å‡†ã€‚ç†æƒ³çš„ç»ˆæ­¢æ ‡å‡†æ˜¯ä»£ç†æ‰¾åˆ°ç›®æ ‡ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥è®¾ç½®å…¶ä»–æ ‡å‡†ï¼Œä¾‹å¦‚ä»£ç†é‡åˆ°éšœç¢ç‰©æˆ–ä»£ç†ç”¨å°½æœ€å¤§åŠ¨ä½œæ•°ã€‚ç”±äºæ¨¡å‹å¼€å§‹æ—¶æ²¡æœ‰ä»»ä½•ä¿¡æ¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç§åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¸¸è§çš„è´ªå©ªç­–ç•¥ï¼Œå«åš *ğœ–greedy* ç­–ç•¥ï¼Œå…è®¸ä»£ç†é€šè¿‡ä»ç¯å¢ƒä¸­éšæœºé‡‡æ ·å¼€å§‹ã€‚è¿™é‡Œçš„æƒ³æ³•æ˜¯ï¼Œæˆ‘ä»¬çš„ä»£ç†ä»¥ *ğœ–* çš„æ¦‚ç‡æ‰§è¡ŒéšæœºåŠ¨ä½œï¼Œå¦åˆ™ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ¥é€‰æ‹©åŠ¨ä½œã€‚åœ¨æ¯ä¸ªå›åˆä¹‹åï¼Œæˆ‘ä»¬ä¼šå‡å°‘ *ğœ–*ï¼Œä½¿å¾—ä»£ç†æœ€ç»ˆä»…æ ¹æ®æ¨¡å‹æ¥é€‰æ‹©åŠ¨ä½œã€‚è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªç®€å•çš„å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹ï¼Œçœ‹çœ‹è¿™ä¸€åˆ‡æ˜¯å¦‚ä½•è¿ä½œçš„ã€‚

#### ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–æˆ‘ä»¬çš„ç¯å¢ƒ

æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹å°†å›´ç»•æˆ‘ä»¬çš„ç¯å¢ƒå±•å¼€ï¼šè¿™å®šä¹‰äº†æ‰€æœ‰äº‹ä»¶å‘ç”Ÿçš„ç©ºé—´ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`Environment`ç±»æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾ç½®ç¯å¢ƒå‚æ•°ï¼š

```py

importÂ numpyÂ asÂ np 
importÂ tensorflowÂ asÂ tf 
fromÂ scipy.spatial.distanceÂ importÂ euclidean 
fromÂ tensorflow.kerasÂ importÂ ( 
Model, 
Sequential, 
layers, 
optimizers, 
metrics, 
losses, 
) 
importÂ pandasÂ asÂ pd 
fromÂ sklearn.preprocessingÂ importÂ StandardScaler 
importÂ copy 

classÂ Environment: 
defÂ __init__(self,Â env_size=8,Â max_steps=2000): 
self.env_sizeÂ =Â env_size 
self.max_stepsÂ =Â max_steps 
self.agent_locationÂ =Â np.zeros(2) 
self.target_locationÂ =Â np.random.randint(0,Â self.env_size,Â 2) 
self.action_spaceÂ =Â { 
0:Â np.array([0,Â 1]), 
1:Â np.array([0,Â -1]), 
2:Â np.array([1,Â 0]), 
3:Â np.array([-1,Â 0]), 
} 
self.deltaÂ =Â self.compute_distance() 
self.is_doneÂ =Â False 
self.total_stepsÂ =Â 0 
self.ideal_stepsÂ =Â self.calculate_ideal_steps() 
Â Â Â Â ...
```

åœ¨è¿™é‡Œï¼Œæ³¨æ„æˆ‘ä»¬çš„ç¯å¢ƒå¤§å°ï¼Œç”¨`env_size`è¡¨ç¤ºï¼Œå®ƒå®šä¹‰äº†ç¯å¢ƒä¸­çš„è¡Œæ•°å’Œåˆ—æ•°â€”â€”åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ 8 Ã— 8 çš„ç¯å¢ƒï¼Œç»“æœæ˜¯ 64 ä¸ªä½ç½®ï¼ˆä¸ºäº†ç®€ä¾¿ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªæ–¹å½¢ç¯å¢ƒï¼‰ã€‚æˆ‘ä»¬è¿˜å°†è®¾ç½®ä¸€ä¸ª`max_steps`é™åˆ¶ï¼Œä»¥ç¡®ä¿åœ¨ä»£ç†éšæœºé€‰æ‹©åŠ¨ä½œæ—¶ï¼Œå›åˆä¸ä¼šè¿›è¡Œå¾—å¤ªé•¿ã€‚

æˆ‘ä»¬è¿˜è®¾ç½®äº†`agent_location`å’Œ`target_location`å˜é‡â€”â€”ä»£ç†æ€»æ˜¯ä» [0, 0] ç‚¹å¼€å§‹ï¼Œè€Œç›®æ ‡ä½ç½®åˆ™æ˜¯éšæœºåˆ†é…çš„ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œå°†æ•´æ•°å€¼æ˜ å°„åˆ°ä¸€ä¸ªåŠ¨ä½œã€‚ä» 0 åˆ° 3ï¼Œè¿™äº›åŠ¨ä½œåˆ†åˆ«æ˜¯ï¼šå‘å‰ã€å‘åã€å‘å³ã€å‘å·¦ã€‚æˆ‘ä»¬è¿˜è®¾ç½®äº†`delta`å˜é‡â€”â€”è¿™æ˜¯ä»£ç†ä¸ç›®æ ‡ä¹‹é—´çš„åˆå§‹è·ç¦»ï¼ˆç¨åæˆ‘ä»¬å°†çœ‹åˆ°`compute_distance()`æ˜¯å¦‚ä½•å®ç°çš„ï¼‰ã€‚

æœ€åï¼Œæˆ‘ä»¬åˆå§‹åŒ–ä¸€äº›å˜é‡ï¼Œç”¨äºè·Ÿè¸ªç»ˆæ­¢æ ‡å‡†æ˜¯å¦å·²æ»¡è¶³ï¼ˆ`is_done`ï¼‰ã€æ€»æ­¥éª¤æ•°ï¼ˆ`total_steps`ï¼‰å’Œç†æƒ³æ­¥éª¤æ•°ï¼ˆ`ideal_steps`ï¼‰ã€‚åè€…æ˜¯ä»£ç†ä»èµ·å§‹ä½ç½®åˆ°è¾¾ç›®æ ‡æ‰€éœ€çš„æœ€å°æ­¥éª¤æ•°ã€‚æˆ‘ä»¬å°†ç”¨å®ƒæ¥è®¡ç®—é—æ†¾ï¼Œè¿™æ˜¯å¼ºåŒ–å­¦ä¹ å’Œä¼˜åŒ–ç®—æ³•ä¸­ä¸€ä¸ªæœ‰ç”¨çš„æ€§èƒ½æŒ‡æ ‡ã€‚ä¸ºäº†è®¡ç®—é—æ†¾ï¼Œæˆ‘ä»¬å°†å‘æˆ‘ä»¬çš„ç±»ä¸­æ·»åŠ ä»¥ä¸‹ä¸¤ä¸ªå‡½æ•°ï¼š

```py

... 

defÂ calculate_ideal_action(self,Â agent_location,Â target_location): 
min_deltaÂ =Â 1e1000 
ideal_actionÂ =Â -1 
forÂ kÂ inÂ self.action_space.keys(): 
deltaÂ =Â euclidean( 
agent_locationÂ +Â self.action_space[k],Â target_location 
) 
ifÂ deltaÂ *<*=Â min_delta: 
min_deltaÂ =Â delta 
ideal_actionÂ =Â k 
returnÂ ideal_action,Â min_delta 

defÂ calculate_ideal_steps(self): 
agent_locationÂ =Â copy.deepcopy(self.agent_location) 
target_locationÂ =Â copy.deepcopy(self.target_location) 
deltaÂ =Â 1e1000 
iÂ =Â 0 
whileÂ deltaÂ *>*Â 0: 
ideal_action,Â deltaÂ =Â self.calculate_ideal_action( 
agent_location,Â target_location 
) 
agent_locationÂ +=Â self.action_space[ideal_action] 
iÂ +=Â 1 
returnÂ i 
Â Â Â Â ...
```

åœ¨è¿™é‡Œï¼Œ`calculate_ideal_steps()`å°†ä¸€ç›´è¿è¡Œï¼Œç›´åˆ°ä»£ç†ä¸ç›®æ ‡ä¹‹é—´çš„è·ç¦»ï¼ˆ`delta`ï¼‰ä¸ºé›¶ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒä½¿ç”¨`calculate_ideal_action()`æ¥é€‰æ‹©èƒ½ä½¿ä»£ç†å°½å¯èƒ½æ¥è¿‘ç›®æ ‡çš„åŠ¨ä½œã€‚

#### ç¬¬äºŒæ­¥ï¼šæ›´æ–°æˆ‘ä»¬ç¯å¢ƒçš„çŠ¶æ€

ç°åœ¨æˆ‘ä»¬å·²ç»åˆå§‹åŒ–äº†æˆ‘ä»¬çš„ç¯å¢ƒï¼Œæˆ‘ä»¬éœ€è¦æ·»åŠ æˆ‘ä»¬ç±»ä¸­æœ€å…³é”®çš„ä¸€ä¸ªéƒ¨åˆ†ï¼š`update`æ–¹æ³•ã€‚è¿™æ§åˆ¶äº†å½“ä»£ç†é‡‡å–æ–°åŠ¨ä½œæ—¶ç¯å¢ƒçš„å˜åŒ–ï¼š

```py

... 
defÂ update(self,Â action_int): 
self.agent_locationÂ =Â ( 
self.agent_locationÂ +Â self.action_space[action_int] 
) 
#Â preventÂ theÂ agentÂ fromÂ movingÂ outsideÂ theÂ boundsÂ ofÂ theÂ environment 
self.agent_location[self.agent_locationÂ *>*Â (self.env_sizeÂ -Â 1)]Â =Â ( 
self.env_sizeÂ -Â 1 
) 
self.compute_reward() 
self.total_stepsÂ +=Â 1 
self.is_doneÂ =Â (self.deltaÂ ==Â 0)Â orÂ (self.total_stepsÂ *>*=Â self.max_steps) 
returnÂ self.reward 
Â Â Â Â ...
```

è¯¥æ–¹æ³•æ¥æ”¶ä¸€ä¸ªåŠ¨ä½œæ•´æ•°ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥è®¿é—®æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„`action_space`å­—å…¸ä¸­å¯¹åº”çš„åŠ¨ä½œã€‚ç„¶åæ›´æ–°ä»£ç†ä½ç½®ã€‚å› ä¸ºä»£ç†ä½ç½®å’ŒåŠ¨ä½œéƒ½æ˜¯å‘é‡ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç®€å•åœ°ä½¿ç”¨å‘é‡åŠ æ³•æ¥å®Œæˆè¿™ä¸€ç‚¹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ£€æŸ¥ä»£ç†æ˜¯å¦ç§»å‡ºäº†ç¯å¢ƒçš„è¾¹ç•Œ â€“ å¦‚æœæ˜¯ï¼Œåˆ™è°ƒæ•´å…¶ä½ç½®ä½¿å…¶ä»ç„¶ä¿æŒåœ¨æˆ‘ä»¬çš„ç¯å¢ƒè¾¹ç•Œå†…ã€‚

ä¸‹ä¸€è¡Œæ˜¯å¦ä¸€ä¸ªå…³é”®çš„ä»£ç ç‰‡æ®µï¼šä½¿ç”¨`compute_reward()`è®¡ç®—å¥–åŠ± â€“ æˆ‘ä»¬é©¬ä¸Šå°±ä¼šçœ‹åˆ°è¿™ä¸ªã€‚ä¸€æ—¦è®¡ç®—å‡ºå¥–åŠ±ï¼Œæˆ‘ä»¬å¢åŠ `total_steps`è®¡æ•°å™¨ï¼Œæ£€æŸ¥ç»ˆæ­¢æ¡ä»¶ï¼Œå¹¶è¿”å›åŠ¨ä½œçš„å¥–åŠ±å€¼ã€‚

æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å‡½æ•°æ¥ç¡®å®šå¥–åŠ±ã€‚å¦‚æœä»£ç†ä¸ç›®æ ‡ä¹‹é—´çš„è·ç¦»å¢åŠ ï¼Œåˆ™è¿”å›ä½å¥–åŠ±ï¼ˆ`1`ï¼‰ï¼Œå¦‚æœå‡å°‘ï¼Œåˆ™è¿”å›é«˜å¥–åŠ±ï¼ˆ`10`ï¼‰ï¼š

```py

... 
defÂ compute_reward(self): 
d1Â =Â self.delta 
self.deltaÂ =Â self.compute_distance() 
ifÂ self.deltaÂ *<*Â d1: 
self.rewardÂ =Â 10 
else: 
self.rewardÂ =Â 1 
Â Â Â Â ...
```

è¿™é‡Œä½¿ç”¨äº†`compute_distance()`å‡½æ•°ï¼Œè®¡ç®—ä»£ç†ä¸ç›®æ ‡ä¹‹é—´çš„æ¬§æ°è·ç¦»ï¼š

```py

... 
defÂ compute_distance(self): 
returnÂ euclidean(self.agent_location,Â self.target_location) 
Â Â Â Â ...
```

æœ€åï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°æ¥å…è®¸æˆ‘ä»¬è·å–ç¯å¢ƒçš„çŠ¶æ€ï¼Œä»¥ä¾¿å°†å…¶ä¸å¥–åŠ±å€¼å…³è”èµ·æ¥ã€‚æˆ‘ä»¬å°†å…¶å®šä¹‰å¦‚ä¸‹ï¼š

```py

... 
defÂ get_state(self): 
returnÂ np.concatenate([self.agent_location,Â self.target_location]) 
Â Â Â Â ...
```

#### ç¬¬ä¸‰æ­¥ï¼šå®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹

ç°åœ¨æˆ‘ä»¬å·²ç»è®¾ç½®å¥½äº†ç¯å¢ƒï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ¨¡å‹ç±»ã€‚è¿™ä¸ªç±»å°†å¤„ç†æ¨¡å‹è®­ç»ƒå’Œæ¨æ–­ï¼Œä»¥åŠæ ¹æ®æ¨¡å‹é¢„æµ‹é€‰æ‹©æœ€ä½³åŠ¨ä½œã€‚å’Œå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬ä»`__init__()`æ–¹æ³•å¼€å§‹ï¼š

```py

classÂ RLModel: 
defÂ __init__(self,Â state_size,Â n_actions,Â num_epochs=500): 
self.state_sizeÂ =Â state_size 
self.n_actionsÂ =Â n_actions 
self.num_epochsÂ =Â 200 
self.modelÂ =Â Sequential() 
self.model.add( 
layers.Dense( 
20,Â input_dim=self.state_size,Â activation="relu",Â name="layer_1" 
) 
) 
self.model.add(layers.Dense(8,Â activation="relu",Â name="layer_2")) 
self.model.add(layers.Dense(1,Â activation="relu",Â name="layer_3")) 
self.model.compile( 
optimizer=optimizers.Adam(), 
loss=losses.Huber(), 
metrics=[metrics.RootMeanSquaredError()], 
) 
Â Â Â Â ...
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¼ é€’äº†ä¸€äº›ä¸æˆ‘ä»¬çš„ç¯å¢ƒç›¸å…³çš„å˜é‡ï¼Œå¦‚çŠ¶æ€å¤§å°å’ŒåŠ¨ä½œæ•°é‡ã€‚ä¸æ¨¡å‹å®šä¹‰ç›¸å…³çš„ä»£ç åº”è¯¥å¾ˆç†Ÿæ‚‰ â€“ æˆ‘ä»¬åªæ˜¯ä½¿ç”¨ Keras å®ä¾‹åŒ–äº†ä¸€ä¸ªç¥ç»ç½‘ç»œã€‚éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨ Huber æŸå¤±ï¼Œè€Œä¸æ˜¯æ›´å¸¸è§çš„å‡æ–¹è¯¯å·®ã€‚è¿™æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ å’Œå¥å£®å›å½’ä»»åŠ¡ä¸­å¸¸è§çš„é€‰æ‹©ã€‚Huber æŸå¤±åŠ¨æ€åœ°åœ¨å‡æ–¹è¯¯å·®å’Œå¹³å‡ç»å¯¹è¯¯å·®ä¹‹é—´åˆ‡æ¢ã€‚å‰è€…éå¸¸æ“…é•¿æƒ©ç½šå°è¯¯å·®ï¼Œè€Œåè€…å¯¹å¼‚å¸¸å€¼æ›´ä¸ºå¥å£®ã€‚é€šè¿‡ Huber æŸå¤±ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªæ—¢å¯¹å¼‚å¸¸å€¼å¥å£®åˆæƒ©ç½šå°è¯¯å·®çš„æŸå¤±å‡½æ•°ã€‚

è¿™åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ç‰¹åˆ«é‡è¦ï¼Œå› ä¸ºç®—æ³•å…·æœ‰æ¢ç´¢æ€§ç‰¹å¾ï¼šæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°ä¸€äº›æå…·æ¢ç´¢æ€§çš„æ ·æœ¬ï¼Œå®ƒä»¬ä¸å…¶ä»–æ•°æ®ç›¸æ¯”åç¦»è¾ƒå¤§ï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¼è‡´è¾ƒå¤§çš„è¯¯å·®ã€‚

åœ¨å®Œæˆç±»çš„åˆå§‹åŒ–åï¼Œæˆ‘ä»¬ç»§ç»­å¤„ç† `fit()` å’Œ `predict()` å‡½æ•°ï¼š

```py

... 
defÂ fit(self,Â X_train,Â y_train,Â batch_size=16): 
self.scalerÂ =Â StandardScaler() 
X_trainÂ =Â self.scaler.fit_transform(X_train) 
self.model.fit( 
X_train, 
y_train, 
epochs=self.num_epochs, 
verbose=0, 
batch_size=batch_size, 
) 

defÂ predict(self,Â state): 
rewardsÂ =Â [] 
XÂ =Â np.zeros((self.n_actions,Â self.state_size)) 
forÂ iÂ inÂ range(self.n_actions): 
X[i]Â =Â np.concatenate([state,Â [i]]) 
XÂ =Â self.scaler.transform(X) 
rewardsÂ =Â self.model.predict(X) 
Â Â Â Â Â Â Â Â returnÂ np.argmax(rewards)
```

`fit()` å‡½æ•°åº”è¯¥éå¸¸ç†Ÿæ‚‰â€”â€”æˆ‘ä»¬åªæ˜¯å¯¹è¾“å…¥è¿›è¡Œç¼©æ”¾ï¼Œç„¶åå†æ‹Ÿåˆæˆ‘ä»¬çš„ Keras æ¨¡å‹ã€‚`predict()` å‡½æ•°åˆ™ç¨å¾®å¤æ‚ä¸€ç‚¹ã€‚å› ä¸ºæˆ‘ä»¬éœ€è¦å¯¹æ¯ä¸ªå¯èƒ½çš„åŠ¨ä½œï¼ˆå‰è¿›ã€åé€€ã€å³è½¬ã€å·¦è½¬ï¼‰è¿›è¡Œé¢„æµ‹ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸ºè¿™äº›åŠ¨ä½œç”Ÿæˆè¾“å…¥ã€‚æˆ‘ä»¬é€šè¿‡å°†ä¸åŠ¨ä½œç›¸å…³çš„æ•´æ•°å€¼ä¸çŠ¶æ€è¿›è¡Œæ‹¼æ¥ï¼Œæ¥ç”Ÿæˆå®Œæ•´çš„çŠ¶æ€-åŠ¨ä½œå‘é‡ï¼Œæ­£å¦‚ç¬¬ 11 è¡Œæ‰€ç¤ºã€‚å¯¹æ‰€æœ‰åŠ¨ä½œæ‰§è¡Œæ­¤æ“ä½œåï¼Œæˆ‘ä»¬å¾—åˆ°è¾“å…¥çŸ©é˜µ `X`ï¼Œå…¶ä¸­æ¯ä¸€è¡Œéƒ½å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„åŠ¨ä½œã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹ `X` è¿›è¡Œç¼©æ”¾ï¼Œå¹¶åœ¨å…¶ä¸Šè¿è¡Œæ¨ç†ï¼Œä»¥è·å¾—é¢„æµ‹çš„å¥–åŠ±å€¼ã€‚ä¸ºäº†é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œæˆ‘ä»¬ç®€å•åœ°ä½¿ç”¨ `np.argmax()` æ¥è·å–ä¸æœ€é«˜é¢„æµ‹å¥–åŠ±ç›¸å…³çš„ç´¢å¼•ã€‚

#### ç¬¬ 4 æ­¥ï¼šè¿è¡Œæˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ 

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»å®šä¹‰äº† `Environment` å’Œ `RLModel` ç±»ï¼Œå‡†å¤‡å¼€å§‹å¼ºåŒ–å­¦ä¹ äº†ï¼é¦–å…ˆï¼Œæˆ‘ä»¬è®¾ç½®ä¸€äº›é‡è¦çš„å˜é‡å¹¶å®ä¾‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼š

```py

env_sizeÂ =Â 8 
state_sizeÂ =Â 5 
n_actionsÂ =Â 4 
epsilonÂ =Â 1.0 
historyÂ =Â {"state":Â [],Â "reward":Â []} 
n_samplesÂ =Â 1000 
max_stepsÂ =Â 500 
regretsÂ =Â [] 

modelÂ =Â RLModel(state_size,Â n_actions)
```

è¿™äº›å†…å®¹ç°åœ¨åº”è¯¥å·²ç»å¾ˆç†Ÿæ‚‰äº†ï¼Œä½†æˆ‘ä»¬è¿˜æ˜¯ä¼šå†å›é¡¾ä¸€äº›å°šæœªè¦†ç›–çš„éƒ¨åˆ†ã€‚`history` å­—å…¸æ˜¯æˆ‘ä»¬å­˜å‚¨çŠ¶æ€å’Œå¥–åŠ±ä¿¡æ¯çš„åœ°æ–¹ï¼Œåœ¨æ¯ä¸€è½®çš„æ¯ä¸ªæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬ä¼šæ›´æ–°è¿™äº›ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¼šåˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚å¦ä¸€ä¸ªä¸å¤ªç†Ÿæ‚‰çš„å˜é‡æ˜¯ `n_samples`â€”â€”æˆ‘ä»¬è®¾ç½®è¿™ä¸ªå˜é‡æ˜¯å› ä¸ºæ¯æ¬¡è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¹¶ä¸æ˜¯ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„æ•°æ®ï¼Œè€Œæ˜¯ä»æ•°æ®ä¸­éšæœºæŠ½å– 1,000 ä¸ªæ•°æ®ç‚¹ã€‚è¿™æ ·å¯ä»¥é¿å…éšç€æ•°æ®é‡çš„ä¸æ–­å¢åŠ ï¼Œæˆ‘ä»¬çš„è®­ç»ƒæ—¶é—´ä¹Ÿä¸æ–­æš´å¢ã€‚è¿™é‡Œçš„æœ€åä¸€ä¸ªæ–°å˜é‡æ˜¯ `regrets`ã€‚è¿™ä¸ªåˆ—è¡¨å°†å­˜å‚¨æ¯ä¸€è½®çš„é—æ†¾å€¼ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œé—æ†¾è¢«ç®€å•åœ°å®šä¹‰ä¸ºæ¨¡å‹æ‰€é‡‡å–çš„æ­¥éª¤æ•°ä¸æ™ºèƒ½ä½“åˆ°è¾¾ç›®æ ‡æ‰€éœ€çš„æœ€å°æ­¥éª¤æ•°ä¹‹é—´çš„å·®å€¼ï¼š

![regret = steps âˆ’ steps model ideal ](img/file177.jpg)

å› æ­¤ï¼Œé—æ†¾ä¸ºé›¶ *â‡”* *steps*[*model*] == *steps*[*ideal*]ã€‚é—æ†¾å€¼å¯¹äºè¡¡é‡æ¨¡å‹å­¦ä¹ è¿‡ç¨‹ä¸­çš„è¡¨ç°éå¸¸æœ‰ç”¨ï¼Œæ­£å¦‚æˆ‘ä»¬ç¨åå°†çœ‹åˆ°çš„é‚£æ ·ã€‚æ¥ä¸‹æ¥å°±æ˜¯å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹çš„ä¸»å¾ªç¯ï¼š

```py

forÂ iÂ inÂ range(100): 
envÂ =Â Environment(env_size,Â max_steps=max_steps) 
whileÂ notÂ env.is_done: 
stateÂ =Â env.get_state() 
ifÂ np.random.rand()Â *<*Â epsilon: 
actionÂ =Â np.random.randint(n_actions) 
else: 
actionÂ =Â model.predict(state) 
rewardÂ =Â env.update(action) 
history["state"].append(np.concatenate([state,Â [action]])) 
history["reward"].append(reward) 
print( 
f"CompletedÂ episodeÂ {i}Â inÂ {env.total_steps}Â steps." 
f"IdealÂ steps:Â {env.ideal_steps}." 
f"Epsilon:Â {epsilon}" 
) 
regrets.append(np.abs(env.total_steps-env.ideal_steps)) 
idxsÂ =Â np.random.choice(len(history["state"]),Â n_samples) 
model.fit( 
np.array(history["state"])[idxs], 
np.array(history["reward"])[idxs] 
) 
Â Â Â Â epsilon-=epsilon/10
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¼šè¿è¡Œ 100 è½®ï¼Œæ¯æ¬¡éƒ½é‡æ–°åˆå§‹åŒ–ç¯å¢ƒã€‚é€šè¿‡å†…éƒ¨çš„ `while` å¾ªç¯å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬ä¼šä¸æ–­åœ°è¿­ä»£â€”â€”æ›´æ–°æ™ºèƒ½ä½“å¹¶è¡¡é‡å¥–åŠ±â€”â€”ç›´åˆ°æ»¡è¶³å…¶ä¸­ä¸€ä¸ªç»ˆæ­¢æ¡ä»¶ï¼ˆæ— è®ºæ˜¯æ™ºèƒ½ä½“è¾¾åˆ°ç›®æ ‡ï¼Œè¿˜æ˜¯æˆ‘ä»¬è¾¾åˆ°æœ€å¤§å…è®¸çš„è¿­ä»£æ¬¡æ•°ï¼‰ã€‚

æ¯ä¸€è½®ç»“æŸåï¼Œ`print`è¯­å¥ä¼šå‘Šè¯‰æˆ‘ä»¬è¯¥è½®æ˜¯å¦æ²¡æœ‰é”™è¯¯å®Œæˆï¼Œå¹¶å‘Šè¯‰æˆ‘ä»¬æˆ‘ä»¬çš„æ™ºèƒ½ä½“ä¸ç†æƒ³æ­¥æ•°çš„å¯¹æ¯”ç»“æœã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®¡ç®—é—æ†¾å€¼ï¼Œå¹¶å°†å…¶é™„åŠ åˆ°`regrets`åˆ—è¡¨ä¸­ï¼Œä»`history`ä¸­çš„æ•°æ®è¿›è¡Œé‡‡æ ·ï¼Œå¹¶åœ¨è¿™äº›æ ·æœ¬æ•°æ®ä¸Šæ‹Ÿåˆæˆ‘ä»¬çš„æ¨¡å‹ã€‚æœ€åï¼Œæ¯æ¬¡å¤–å¾ªç¯è¿­ä»£ç»“æŸæ—¶ï¼Œæˆ‘ä»¬ä¼šå‡å°‘ epsilon å€¼ã€‚

è¿è¡Œå®Œä¹‹åï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç»˜åˆ¶é—æ†¾å€¼å›¾ï¼Œä»¥æŸ¥çœ‹æˆ‘ä»¬çš„è¡¨ç°ï¼š

```py

importÂ matplotlib.pyplotÂ asÂ plt 
importÂ seabornÂ asÂ sns 

df_plotÂ =Â pd.DataFrame({"regret":Â regrets,Â "episode":Â np.arange(len(regrets))}) 
sns.lineplot(x="episode",Â y="regret",Â data=df_plot) 
figÂ =Â plt.gcf() 
fig.set_size_inches(5,Â 10) 
plt.show()
```

è¿™å°†ç”Ÿæˆä»¥ä¸‹å›¾è¡¨ï¼Œå±•ç¤ºæˆ‘ä»¬æ¨¡å‹åœ¨ 100 è½®è®­ç»ƒä¸­çš„è¡¨ç°ï¼š

![PIC](img/file178.png)

å›¾ 8.11ï¼šå¼ºåŒ–å­¦ä¹  100 è½®åçš„é—æ†¾å€¼å›¾

æ­£å¦‚æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„ï¼Œå®ƒä¸€å¼€å§‹è¡¨ç°å¾—å¾ˆå·®ï¼Œä½†æ¨¡å‹å¾ˆå¿«å­¦ä¼šäº†é¢„æµ‹å¥–åŠ±å€¼ï¼Œä»è€Œèƒ½å¤Ÿé¢„æµ‹æœ€ä¼˜åŠ¨ä½œï¼Œå°†é—æ†¾å‡å°‘åˆ° 0ã€‚

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œäº‹æƒ…è¿˜ç®—ç®€å•ã€‚äº‹å®ä¸Šï¼Œä½ å¯èƒ½ä¼šæƒ³ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æ¨¡å‹å‘¢â€”â€”ä¸ºä»€ä¹ˆä¸ç›´æ¥è®¡ç®—ç›®æ ‡å’Œæ‹Ÿè®®ä½ç½®ä¹‹é—´çš„è·ç¦»ï¼Œç„¶åé€‰æ‹©ç›¸åº”çš„åŠ¨ä½œå‘¢ï¼Ÿé¦–å…ˆï¼Œå¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯è®©æ™ºèƒ½ä½“åœ¨æ²¡æœ‰ä»»ä½•å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹å‘ç°å¦‚ä½•åœ¨ç»™å®šç¯å¢ƒä¸­è¿›è¡Œäº¤äº’â€”â€”æ‰€ä»¥ï¼Œå°½ç®¡æˆ‘ä»¬çš„æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡ŒåŠ¨ä½œï¼Œä½†å®ƒæ²¡æœ‰è·ç¦»çš„æ¦‚å¿µã€‚è¿™æ˜¯é€šè¿‡ä¸ç¯å¢ƒçš„äº’åŠ¨æ¥å­¦ä¹ çš„ã€‚å…¶æ¬¡ï¼Œæƒ…å†µå¯èƒ½æ²¡æœ‰é‚£ä¹ˆç®€å•ï¼šå¦‚æœç¯å¢ƒä¸­æœ‰éšœç¢ç‰©å‘¢ï¼Ÿåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“éœ€è¦æ¯”ç®€å•åœ°æœå£°éŸ³æºç§»åŠ¨æ›´èªæ˜ã€‚

å°½ç®¡è¿™åªæ˜¯ä¸€ä¸ªç¤ºèŒƒæ€§çš„ä¾‹å­ï¼Œä½†å¼ºåŒ–å­¦ä¹ åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ¶‰åŠä¸€äº›æˆ‘ä»¬çŸ¥è¯†éå¸¸æœ‰é™çš„æƒ…å¢ƒï¼Œå› æ­¤ï¼Œè®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿæ¢ç´¢ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•æœ€ä¼˜äº’åŠ¨çš„æ™ºèƒ½ä½“ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿä¸ºé‚£äº›æ— æ³•ä½¿ç”¨ç›‘ç£å­¦ä¹ æ–¹æ³•çš„åº”ç”¨å¼€å‘æ¨¡å‹ã€‚

å¦ä¸€ä¸ªåœ¨ç°å®ä¸–ç•Œæƒ…å¢ƒä¸­éœ€è¦è€ƒè™‘çš„å› ç´ æ˜¯é£é™©ï¼šæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ™ºèƒ½ä½“åšå‡º*æ˜æ™º*çš„å†³ç­–ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€å¤§åŒ–å¥–åŠ±çš„å†³ç­–ï¼šæˆ‘ä»¬éœ€è¦å®ƒèƒ½å¤Ÿç†è§£é£é™©/å›æŠ¥çš„æƒè¡¡ã€‚è¿™å°±æ˜¯ä¸ç¡®å®šæ€§ä¼°è®¡çš„ä½œç”¨æ‰€åœ¨ã€‚

### 8.5.1 å¸¦æœ‰ä¸ç¡®å®šæ€§çš„éšœç¢ç‰©å¯¼èˆª

é€šè¿‡ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å¥–åŠ±å’Œæ¨¡å‹å¯¹å…¶é¢„æµ‹çš„ä¿¡å¿ƒä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚å¦‚æœæ¨¡å‹çš„ä¿¡å¿ƒè¾ƒä½ï¼ˆæ„å‘³ç€ä¸ç¡®å®šæ€§è¾ƒé«˜ï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯èƒ½å¸Œæœ›å¯¹å¦‚ä½•æ•´åˆæ¨¡å‹çš„é¢„æµ‹ä¿æŒè°¨æ…ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬åˆšåˆšæ¢è®¨çš„å¼ºåŒ–å­¦ä¹ åœºæ™¯ã€‚åœ¨æ¯ä¸€è½®ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹å“ªä¸ªåŠ¨ä½œå°†è·å¾—æœ€é«˜çš„å¥–åŠ±ï¼Œç„¶åæˆ‘ä»¬çš„æ™ºèƒ½ä½“é€‰æ‹©è¯¥åŠ¨ä½œã€‚åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œäº‹æƒ…å¹¶ä¸æ˜¯é‚£ä¹ˆå¯é¢„æµ‹â€”â€”æˆ‘ä»¬çš„ç¯å¢ƒå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼Œå¯¼è‡´æ„å¤–çš„åæœã€‚å¦‚æœæˆ‘ä»¬çš„ç¯å¢ƒä¸­å‡ºç°äº†éšœç¢ç‰©ï¼Œå¹¶ä¸”ä¸éšœç¢ç‰©å‘ç”Ÿç¢°æ’ä¼šé˜»æ­¢æˆ‘ä»¬çš„æ™ºèƒ½ä½“å®Œæˆä»»åŠ¡ï¼Œé‚£ä¹ˆæ˜¾ç„¶ï¼Œå¦‚æœæˆ‘ä»¬çš„æ™ºèƒ½ä½“è¿˜æ²¡æœ‰é‡åˆ°è¿‡è¿™ä¸ªéšœç¢ç‰©ï¼Œå®ƒæ³¨å®šä¼šå¤±è´¥ã€‚å¹¸è¿çš„æ˜¯ï¼Œåœ¨è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„æƒ…å†µä¸‹ï¼Œæƒ…å†µå¹¶éå¦‚æ­¤ã€‚åªè¦æˆ‘ä»¬æœ‰æŸç§æ–¹å¼æ¥æ„ŸçŸ¥éšœç¢ç‰©ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“å°±èƒ½å¤Ÿæ£€æµ‹åˆ°éšœç¢ç‰©å¹¶é€‰æ‹©ä¸åŒçš„è·¯å¾„â€”â€”å³ä½¿è¯¥éšœç¢ç‰©åœ¨ä¹‹å‰çš„å›åˆä¸­æ²¡æœ‰å‡ºç°ã€‚

![PIC](img/file179.jpg)

å›¾ 8.12ï¼šä¸ç¡®å®šæ€§å¦‚ä½•å½±å“å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“è¡ŒåŠ¨çš„ç¤ºæ„å›¾

è¿™ä¸€åˆ‡ä¹‹æ‰€ä»¥å¯èƒ½ï¼Œå¾—ç›Šäºæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚å½“æ¨¡å‹é‡åˆ°ä¸å¯»å¸¸çš„æƒ…å†µæ—¶ï¼Œå®ƒå¯¹è¯¥é¢„æµ‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡å°†ä¼šè¾ƒé«˜ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬å°†å…¶èå…¥åˆ°æˆ‘ä»¬çš„ MPC æ–¹ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°±èƒ½åœ¨å¥–åŠ±å’Œä¸ç¡®å®šæ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Œç¡®ä¿æˆ‘ä»¬ä¼˜å…ˆè€ƒè™‘è¾ƒä½çš„é£é™©ï¼Œè€Œéè¾ƒé«˜çš„å¥–åŠ±ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä¿®æ”¹äº†æˆ‘ä»¬çš„ MPC æ–¹ç¨‹ï¼Œå…·ä½“å¦‚ä¸‹ï¼š

![anext = argmax (yi âˆ’ Î»Ïƒi)âˆ€ai âˆˆ As ](img/file180.jpg)

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬æ­£åœ¨ä»æˆ‘ä»¬çš„å¥–åŠ±é¢„æµ‹ `y`[`i`] ä¸­å‡å»ä¸€ä¸ªå€¼ï¼Œ*Î»Ïƒ*[`i`]ã€‚è¿™æ˜¯å› ä¸º *Ïƒ*[`i`] æ˜¯ä¸ç¬¬ `i` æ¬¡é¢„æµ‹ç›¸å…³çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨ *Î»* æ¥ç¼©æ”¾ä¸ç¡®å®šæ€§ï¼Œä»¥ä¾¿é€‚å½“æƒ©ç½šä¸ç¡®å®šçš„åŠ¨ä½œï¼›è¿™æ˜¯ä¸€ä¸ªå¯ä»¥æ ¹æ®åº”ç”¨è¿›è¡Œè°ƒæ•´çš„å‚æ•°ã€‚é€šè¿‡ä¸€ä¸ªç»è¿‡è‰¯å¥½æ ¡å‡†çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°åœ¨æ¨¡å‹å¯¹é¢„æµ‹ä¸ç¡®å®šæ—¶ï¼Œ*Ïƒ*[`i`] çš„å€¼ä¼šè¾ƒå¤§ã€‚è®©æˆ‘ä»¬åœ¨ä¹‹å‰çš„ä»£ç ç¤ºä¾‹çš„åŸºç¡€ä¸Šï¼Œçœ‹çœ‹è¿™ä¸€è¿‡ç¨‹å¦‚ä½•å®ç°ã€‚

#### ç¬¬ä¸€æ­¥ï¼šå¼•å…¥éšœç¢ç‰©

ä¸ºäº†ç»™æˆ‘ä»¬çš„æ™ºèƒ½ä½“åˆ¶é€ æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å°†å‘ç¯å¢ƒä¸­å¼•å…¥éšœç¢ç‰©ã€‚ä¸ºäº†æµ‹è¯•æ™ºèƒ½ä½“å¦‚ä½•åº”å¯¹ä¸ç†Ÿæ‚‰çš„è¾“å…¥ï¼Œæˆ‘ä»¬å°†æ”¹å˜éšœç¢ç‰©çš„ç­–ç•¥â€”â€”å®ƒå°†æ ¹æ®æˆ‘ä»¬çš„ç¯å¢ƒè®¾ç½®ï¼Œé€‰æ‹©éµå¾ªé™æ€ç­–ç•¥æˆ–åŠ¨æ€ç­–ç•¥ã€‚æˆ‘ä»¬å°†ä¿®æ”¹ `Environment` ç±»çš„ `__init__()` å‡½æ•°ï¼Œä»¥ä¾¿æ•´åˆè¿™äº›æ›´æ”¹ï¼š

```py

defÂ __init__(self,Â env_size=8,Â max_steps=2000,Â dynamic_obstacle=False,Â lambda_val=2): 
self.env_sizeÂ =Â env_size 
self.max_stepsÂ =Â max_steps 
self.agent_locationÂ =Â np.zeros(2) 
self.dynamic_obstacleÂ =Â dynamic_obstacle 
self.lambda_valÂ =Â lambda_val 
self.target_locationÂ =Â np.random.randint(0,Â self.env_size,Â 2) 
whileÂ euclidean(self.agent_location,Â self.target_location)Â *<*Â 4: 
self.target_locationÂ =Â np.random.randint(0,Â self.env_size,Â 2) 
self.action_spaceÂ =Â { 
0:Â np.array([0,Â 1]), 
1:Â np.array([0,Â -1]), 
2:Â np.array([1,Â 0]), 
3:Â np.array([-1,Â 0]), 
} 
self.deltaÂ =Â self.compute_distance() 
self.is_doneÂ =Â False 
self.total_stepsÂ =Â 0 
self.obstacle_locationÂ =Â np.array( 
[self.env_sizeÂ /Â 2,Â self.env_sizeÂ /Â 2],Â dtype=int 
) 
self.ideal_stepsÂ =Â self.calculate_ideal_steps() 
self.collisionÂ =Â False 

```

è¿™é‡Œæ¶‰åŠçš„å†…å®¹æ¯”è¾ƒå¤æ‚ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†é€ä¸€è®²è§£æ¯ä¸ªæ›´æ”¹ã€‚é¦–å…ˆï¼Œä¸ºäº†ç¡®å®šéšœç¢ç‰©æ˜¯é™æ€çš„è¿˜æ˜¯åŠ¨æ€çš„ï¼Œæˆ‘ä»¬è®¾ç½®äº† `dynamic_obstacle` å˜é‡ã€‚å¦‚æœè¯¥å€¼ä¸º `True`ï¼Œæˆ‘ä»¬å°†éšæœºè®¾ç½®éšœç¢ç‰©çš„ä½ç½®ã€‚å¦‚æœè¯¥å€¼ä¸º `False`ï¼Œåˆ™æˆ‘ä»¬çš„ç‰©ä½“å°†åœç•™åœ¨ç¯å¢ƒçš„ä¸­å¤®ã€‚æˆ‘ä»¬è¿˜åœ¨æ­¤è®¾ç½®äº†æˆ‘ä»¬çš„ `lambda` (*Î»*) å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 2ã€‚

æˆ‘ä»¬è¿˜åœ¨è®¾ç½® `target_location` æ—¶å¼•å…¥äº†ä¸€ä¸ª `while` å¾ªç¯ï¼šæˆ‘ä»¬è¿™ä¹ˆåšæ˜¯ä¸ºäº†ç¡®ä¿æ™ºèƒ½ä½“å’Œç›®æ ‡ä¹‹é—´æœ‰ä¸€å®šçš„è·ç¦»ã€‚æˆ‘ä»¬éœ€è¦è¿™ä¹ˆåšæ˜¯ä¸ºäº†ç¡®ä¿åœ¨æ™ºèƒ½ä½“å’Œç›®æ ‡ä¹‹é—´ç•™æœ‰è¶³å¤Ÿçš„ç©ºé—´ï¼Œä»¥ä¾¿æ”¾ç½®åŠ¨æ€éšœç¢ç‰©â€”â€”å¦åˆ™ï¼Œæ™ºèƒ½ä½“å¯èƒ½æ°¸è¿œæ— æ³•é‡åˆ°è¿™ä¸ªéšœç¢ç‰©ï¼ˆè¿™å°†ç¨å¾®è¿èƒŒæœ¬ç¤ºä¾‹çš„æ„ä¹‰ï¼‰ã€‚

æœ€åï¼Œæˆ‘ä»¬åœ¨ç¬¬ 17 è¡Œè®¡ç®—éšœç¢ç‰©çš„ä½ç½®ï¼šä½ ä¼šæ³¨æ„åˆ°è¿™åªæ˜¯å°†å®ƒè®¾ç½®åœ¨ç¯å¢ƒçš„ä¸­å¤®ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬ç¨åä¼šä½¿ç”¨ `dynamic_obstacle` æ ‡å¿—å°†éšœç¢ç‰©æ”¾ç½®åœ¨æ™ºèƒ½ä½“å’Œç›®æ ‡ä¹‹é—´â€”â€”æˆ‘ä»¬åœ¨ `calculate_ideal_steps()` å‡½æ•°ä¸­è¿™ä¹ˆåšï¼Œå› ä¸ºè¿™æ ·æˆ‘ä»¬å°±çŸ¥é“éšœç¢ç‰©å°†ä½äºæ™ºèƒ½ä½“çš„ç†æƒ³è·¯å¾„ä¸Šï¼ˆå› æ­¤æ›´æœ‰å¯èƒ½è¢«é‡åˆ°ï¼‰ã€‚

#### æ­¥éª¤ 2ï¼šæ”¾ç½®åŠ¨æ€éšœç¢ç‰©

å½“ `dynamic_obstacle` ä¸º `True` æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨æ¯ä¸ªå›åˆå°†éšœç¢ç‰©æ”¾ç½®åœ¨ä¸åŒçš„ä½ç½®ï¼Œä»è€Œä¸ºæˆ‘ä»¬çš„æ™ºèƒ½ä½“å¸¦æ¥æ›´å¤šæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨ä¹‹å‰æåˆ°çš„ `calculate_ideal_steps()` å‡½æ•°ä¸­è¿›è¡Œäº†ä¸€äº›ä¿®æ”¹ï¼š

```py

defÂ calculate_ideal_steps(self): 
agent_locationÂ =Â copy.deepcopy(self.agent_location) 
target_locationÂ =Â copy.deepcopy(self.target_location) 
deltaÂ =Â 1e1000 
iÂ =Â 0 
whileÂ deltaÂ *>*Â 0: 
ideal_action,Â deltaÂ =Â self.calculate_ideal_action( 
agent_location,Â target_location 
) 
agent_locationÂ +=Â self.action_space[ideal_action] 
ifÂ np.random.randint(0,Â 2)Â andÂ self.dynamic_obstacle: 
self.obstacle_locationÂ =Â copy.deepcopy(agent_location) 
iÂ +=Â 1 
Â Â Â Â Â Â Â Â returnÂ i
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬åœ¨æ¯æ¬¡æ‰§è¡Œ `while` å¾ªç¯æ—¶éƒ½è°ƒç”¨äº† `np.random.randint(0, 2)`ã€‚è¿™æ˜¯ä¸ºäº†éšæœºåŒ–éšœç¢ç‰©æ²¿ç†æƒ³è·¯å¾„çš„æ”¾ç½®ä½ç½®ã€‚

#### æ­¥éª¤ 3ï¼šæ·»åŠ æ„ŸçŸ¥åŠŸèƒ½

å¦‚æœæˆ‘ä»¬çš„æ™ºèƒ½ä½“æ— æ³•æ„ŸçŸ¥ç¯å¢ƒä¸­å¼•å…¥çš„ç‰©ä½“ï¼Œé‚£ä¹ˆå®ƒå°†æ²¡æœ‰ä»»ä½•å¸Œæœ›é¿å…è¿™ä¸ªç‰©ä½“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ªå‡½æ•°æ¥æ¨¡æ‹Ÿä¼ æ„Ÿå™¨ï¼š`get_obstacle_proximity()`ã€‚è¯¥ä¼ æ„Ÿå™¨å°†ä¸ºæˆ‘ä»¬çš„æ™ºèƒ½ä½“æä¾›å…³äºå¦‚æœå®ƒæ‰§è¡ŒæŸä¸ªç‰¹å®šåŠ¨ä½œæ—¶ï¼Œå®ƒä¼šæ¥è¿‘ç‰©ä½“çš„è·ç¦»ä¿¡æ¯ã€‚æ ¹æ®ç»™å®šåŠ¨ä½œå°†æˆ‘ä»¬çš„æ™ºèƒ½ä½“é è¿‘ç‰©ä½“çš„è·ç¦»ï¼Œæˆ‘ä»¬å°†è¿”å›é€æ¸å¢å¤§çš„æ•°å€¼ã€‚å¦‚æœåŠ¨ä½œå°†æ™ºèƒ½ä½“ç½®äºè¶³å¤Ÿè¿œçš„ä½ç½®ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè‡³å°‘ 4.5 ä¸ªç©ºé—´ï¼‰ï¼Œåˆ™æˆ‘ä»¬çš„ä¼ æ„Ÿå™¨å°†è¿”å›é›¶ã€‚è¿™ä¸ªæ„ŸçŸ¥åŠŸèƒ½ä½¿å¾—æˆ‘ä»¬çš„æ™ºèƒ½ä½“èƒ½å¤Ÿæœ‰æ•ˆåœ°çœ‹åˆ°ä¸€æ­¥ä¹‹é¥ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†è¯¥ä¼ æ„Ÿå™¨è§†ä¸ºå…·æœ‰ä¸€æ­¥çš„æ„ŸçŸ¥èŒƒå›´ã€‚

```py

defÂ get_obstacle_proximity(self): 
obstacle_action_distsÂ =Â np.array( 
[ 
euclidean( 
self.agent_locationÂ +Â self.action_space[k], 
self.obstacle_location, 
) 
forÂ kÂ inÂ self.action_space.keys() 
] 
) 
returnÂ self.lambda_valÂ *Â ( 
np.array(obstacle_action_distsÂ *<*Â 2.5,Â dtype=float) 
+Â np.array(obstacle_action_distsÂ *<*Â 3.5,Â dtype=float) 
+Â np.array(obstacle_action_distsÂ *<*Â 4.5,Â dtype=float) 
Â Â Â Â Â Â Â Â )
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é¦–å…ˆè®¡ç®—æ¯ä¸ªåŠ¨ä½œåæ™ºèƒ½ä½“çš„æœªæ¥æ¥è¿‘åº¦ï¼Œç„¶åè®¡ç®—æ•´æ•°â€œæ¥è¿‘åº¦â€å€¼ã€‚è¿™äº›å€¼æ˜¯é€šè¿‡é¦–å…ˆæ„é€ æ¯ä¸ªæ¥è¿‘åº¦æ¡ä»¶çš„å¸ƒå°”æ•°ç»„æ¥è®¡ç®—çš„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹åˆ†åˆ«ä¸º *Î´*[`o`] *<* 2*.*5, *Î´*[`o`] *<* 3*.*5 å’Œ *Î´*[`o`] *<* 4*.*5ï¼Œå…¶ä¸­ *Î´*[`o`] æ˜¯ä¸éšœç¢ç‰©çš„è·ç¦»ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›æ¡ä»¶æ±‚å’Œï¼Œä½¿å¾—æ¥è¿‘åº¦å¾—åˆ†å…·æœ‰ 3ã€2 æˆ– 1 çš„æ•´æ•°å€¼ï¼Œå…·ä½“å–å†³äºæ»¡è¶³å¤šå°‘ä¸ªæ¡ä»¶ã€‚è¿™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä¼ æ„Ÿå™¨ï¼Œå®ƒä¼šæ ¹æ®æ¯ä¸ªæè®®çš„åŠ¨ä½œè¿”å›æœ‰å…³éšœç¢ç‰©æœªæ¥æ¥è¿‘åº¦çš„åŸºæœ¬ä¿¡æ¯ã€‚

#### æ­¥éª¤ 4ï¼šä¿®æ”¹å¥–åŠ±å‡½æ•°

å‡†å¤‡ç¯å¢ƒçš„æœ€åä¸€ä»¶äº‹æ˜¯æ›´æ–°æˆ‘ä»¬çš„å¥–åŠ±å‡½æ•°ï¼š

```py

defÂ compute_reward(self): 
d1Â =Â self.delta 
self.deltaÂ =Â self.compute_distance() 
ifÂ euclidean(self.agent_location,Â self.obstacle_location)Â ==Â 0: 
self.rewardÂ =Â 0 
self.collisionÂ =Â True 
self.is_doneÂ =Â True 
elifÂ self.deltaÂ *<*Â d1: 
self.rewardÂ =Â 10 
else: 
Â Â Â Â Â Â Â Â Â Â Â Â self.rewardÂ =Â 1
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€æ¡è¯­å¥æ¥æ£€æŸ¥ä»£ç†ä¸éšœç¢ç‰©æ˜¯å¦å‘ç”Ÿç¢°æ’ï¼ˆæ£€æŸ¥ä¸¤è€…ä¹‹é—´çš„è·ç¦»æ˜¯å¦ä¸ºé›¶ï¼‰ã€‚å¦‚æœå‘ç”Ÿç¢°æ’ï¼Œæˆ‘ä»¬å°†è¿”å›å¥–åŠ±å€¼ 0ï¼Œå¹¶å°†`collision`å’Œ`is_done`å˜é‡è®¾ç½®ä¸º`True`ã€‚è¿™å¼•å…¥äº†æ–°çš„ç»ˆæ­¢æ ‡å‡†â€”â€”**ç¢°æ’**ï¼Œå¹¶å°†å…è®¸æˆ‘ä»¬çš„ä»£ç†å­¦ä¹ åˆ°ç¢°æ’æ˜¯æœ‰å®³çš„ï¼Œå› ä¸ºè¿™äº›æƒ…å†µä¼šå¾—åˆ°æœ€ä½çš„å¥–åŠ±ã€‚

#### ç¬¬ 5 æ­¥ï¼šåˆå§‹åŒ–æˆ‘ä»¬çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¨¡å‹

ç°åœ¨æˆ‘ä»¬çš„ç¯å¢ƒå·²ç»å‡†å¤‡å¥½ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ–°çš„æ¨¡å‹â€”â€”ä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆä¸ç¡®å®šæ€§ä¼°è®¡çš„æ¨¡å‹ã€‚å¯¹äºè¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå¸¦æœ‰å•ä¸ªéšè—å±‚çš„ MC dropout ç½‘ç»œï¼š

```py

classÂ RLModelDropout: 
defÂ __init__(self,Â state_size,Â n_actions,Â num_epochs=200,Â nb_inference=10): 
self.state_sizeÂ =Â state_size 
self.n_actionsÂ =Â n_actions 
self.num_epochsÂ =Â num_epochs 
self.nb_inferenceÂ =Â nb_inference 
self.modelÂ =Â Sequential() 
self.model.add( 
layers.Dense( 
10,Â input_dim=self.state_size,Â activation="relu",Â name="layer_1" 
) 
) 
#Â self.model.add(layers.Dropout(0.15)) 
#Â self.model.add(layers.Dense(8,Â activation='relu',Â name='layer_2')) 
self.model.add(layers.Dropout(0.15)) 
self.model.add(layers.Dense(1,Â activation="relu",Â name="layer_2")) 
self.model.compile( 
optimizer=optimizers.Adam(), 
loss=losses.Huber(), 
metrics=[metrics.RootMeanSquaredError()], 
) 

self.proximity_dictÂ =Â {"proximityÂ sensorÂ value":Â [],Â "uncertainty":Â []} 
Â Â Â Â ...
```

è¿™çœ‹èµ·æ¥åº”è¯¥å¾ˆç†Ÿæ‚‰ï¼Œä½†ä½ ä¼šæ³¨æ„åˆ°å‡ ä¸ªå…³é”®çš„ä¸åŒä¹‹å¤„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å†æ¬¡ä½¿ç”¨ Huber æŸå¤±å‡½æ•°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå­—å…¸`proximity_dict`ï¼Œå®ƒå°†è®°å½•ä»ä¼ æ„Ÿå™¨æ¥æ”¶åˆ°çš„é‚»è¿‘å€¼å’Œç›¸å…³çš„æ¨¡å‹ä¸ç¡®å®šæ€§ã€‚è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿç¨åè¯„ä¼°æ¨¡å‹å¯¹å¼‚å¸¸é‚»è¿‘å€¼çš„æ•æ„Ÿæ€§ã€‚

#### ç¬¬ 6 æ­¥ï¼šæ‹Ÿåˆæˆ‘ä»¬çš„ MC Dropout ç½‘ç»œ

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä»¥ä¸‹å‡ è¡Œä»£ç ï¼š

```py

... 
defÂ fit(self,Â X_train,Â y_train,Â batch_size=16): 
self.scalerÂ =Â StandardScaler() 
X_trainÂ =Â self.scaler.fit_transform(X_train) 
self.model.fit( 
X_train, 
y_train, 
epochs=self.num_epochs, 
verbose=0, 
batch_size=batch_size, 
) 
Â Â Â Â ...
```

è¿™åº”è¯¥å†æ¬¡çœ‹èµ·æ¥å¾ˆç†Ÿæ‚‰â€”â€”æˆ‘ä»¬åªæ˜¯é€šè¿‡é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œç¼©æ”¾æ¥å‡†å¤‡æ•°æ®ï¼Œç„¶åæ‹Ÿåˆæˆ‘ä»¬çš„æ¨¡å‹ã€‚

#### ç¬¬ 7 æ­¥ï¼šè¿›è¡Œé¢„æµ‹

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬ç¨å¾®ä¿®æ”¹äº†`predict()`å‡½æ•°ï¼š

```py

... 
defÂ predict(self,Â state,Â obstacle_proximity,Â dynamic_obstacle=False): 
rewardsÂ =Â [] 
XÂ =Â np.zeros((self.n_actions,Â self.state_size)) 
forÂ iÂ inÂ range(self.n_actions): 
X[i]Â =Â np.concatenate([state,Â [i],Â [obstacle_proximity[i]]]) 
XÂ =Â self.scaler.transform(X) 
rewards,Â y_stdÂ =Â self.predict_ll_dropout(X) 
#Â weÂ subtractÂ ourÂ standardÂ deviationsÂ fromÂ ourÂ predictedÂ rewardÂ values, 
#Â thisÂ wayÂ uncertainÂ predictionsÂ areÂ penalised 
rewardsÂ =Â rewardsÂ -Â (y_stdÂ *Â 2) 
best_actionÂ =Â np.argmax(rewards) 
ifÂ dynamic_obstacle: 
self.proximity_dict["proximityÂ sensorÂ value"].append( 
obstacle_proximity[best_action] 
) 
self.proximity_dict["uncertainty"].append(y_std[best_action][0]) 
returnÂ best_action 
Â Â Â Â ...
```

æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬æ·»åŠ äº†`obstacle_proximity`å’Œ`dynamic_obstacle`å˜é‡ã€‚å‰è€…å…è®¸æˆ‘ä»¬æ¥æ”¶ä¼ æ„Ÿå™¨ä¿¡æ¯ï¼Œå¹¶å°†å…¶çº³å…¥ä¼ é€’ç»™æ¨¡å‹çš„è¾“å…¥ä¸­ã€‚åè€…æ˜¯ä¸€ä¸ªæ ‡å¿—ï¼Œå‘Šè¯‰æˆ‘ä»¬æ˜¯å¦è¿›å…¥äº†åŠ¨æ€éšœç¢ç‰©é˜¶æ®µâ€”â€”å¦‚æœæ˜¯ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨`proximity_dict`å­—å…¸ä¸­è®°å½•ä¼ æ„Ÿå™¨å€¼å’Œä¸ç¡®å®šæ€§çš„ç›¸å…³ä¿¡æ¯ã€‚

ä¸‹ä¸€æ®µé¢„æµ‹ä»£ç åº”è¯¥å†æ¬¡çœ‹èµ·æ¥å¾ˆç†Ÿæ‚‰ï¼š

```py

... 
defÂ predict_ll_dropout(self,Â X): 
ll_divdÂ =Â [ 
self.model(X,Â training=True)Â forÂ _Â inÂ range(self.nb_inference) 
] 
ll_divdÂ =Â np.stack(ll_divd) 
Â Â Â Â Â Â Â Â returnÂ ll_divd.mean(axis=0),Â ll_divd.std(axis=0)
```

è¯¥å‡½æ•°ç®€å•åœ°å®ç°äº† MC dropout æ¨æ–­ï¼Œé€šè¿‡`nb_inference`æ¬¡å‰å‘ä¼ é€’è·å¾—é¢„æµ‹ï¼Œå¹¶è¿”å›ä¸æˆ‘ä»¬çš„é¢„æµ‹åˆ†å¸ƒç›¸å…³çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚

#### ç¬¬ 8 æ­¥ï¼šè°ƒæ•´æˆ‘ä»¬çš„æ ‡å‡†æ¨¡å‹

ä¸ºäº†ç†è§£æˆ‘ä»¬çš„è´å¶æ–¯æ¨¡å‹å¸¦æ¥çš„å·®å¼‚ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶ä¸éè´å¶æ–¯æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ›´æ–°ä¹‹å‰çš„`RLModel`ç±»ï¼Œæ·»åŠ ä»é‚»è¿‘ä¼ æ„Ÿå™¨è·å–é‚»è¿‘ä¿¡æ¯çš„åŠŸèƒ½ï¼š

```py

classÂ RLModel: 
defÂ __init__(self,Â state_size,Â n_actions,Â num_epochs=500): 
self.state_sizeÂ =Â state_size 
self.n_actionsÂ =Â n_actions 
self.num_epochsÂ =Â 200 
self.modelÂ =Â Sequential() 
self.model.add( 
layers.Dense( 
20,Â input_dim=self.state_size,Â activation="relu",Â name="layer_1" 
) 
) 
self.model.add(layers.Dense(8,Â activation="relu",Â name="layer_2")) 
self.model.add(layers.Dense(1,Â activation="relu",Â name="layer_3")) 
self.model.compile( 
optimizer=optimizers.Adam(), 
loss=losses.Huber(), 
metrics=[metrics.RootMeanSquaredError()], 
) 

defÂ fit(self,Â X_train,Â y_train,Â batch_size=16): 
self.scalerÂ =Â StandardScaler() 
X_trainÂ =Â self.scaler.fit_transform(X_train) 
self.model.fit( 
X_train, 
y_train, 
epochs=self.num_epochs, 
verbose=0, 
batch_size=batch_size, 
) 

defÂ predict(self,Â state,Â obstacle_proximity,Â obstacle=False): 
rewardsÂ =Â [] 
XÂ =Â np.zeros((self.n_actions,Â self.state_size)) 
forÂ iÂ inÂ range(self.n_actions): 
X[i]Â =Â np.concatenate([state,Â [i],Â [obstacle_proximity[i]]]) 
XÂ =Â self.scaler.transform(X) 
rewardsÂ =Â self.model.predict(X) 
returnÂ np.argmax(rewards) 

```

è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°æˆ‘ä»¬çš„å†³ç­–å‡½æ•°å¹¶æ²¡æœ‰å˜åŒ–ï¼šå› ä¸ºæˆ‘ä»¬æ²¡æœ‰æ¨¡å‹ä¸ç¡®å®šæ€§ï¼Œæˆ‘ä»¬çš„æ¨¡å‹çš„`predict()`å‡½æ•°ä»…åŸºäºé¢„æµ‹çš„å¥–åŠ±æ¥é€‰æ‹©åŠ¨ä½œã€‚

#### ç¬¬ 9 æ­¥ï¼šå‡†å¤‡è¿è¡Œæˆ‘ä»¬çš„æ–°å¼ºåŒ–å­¦ä¹ å®éªŒ

ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½è®¾ç½®æˆ‘ä»¬çš„æ–°å®éªŒäº†ã€‚æˆ‘ä»¬å°†åˆå§‹åŒ–ä¹‹å‰ä½¿ç”¨çš„å˜é‡ï¼Œå¹¶å¼•å…¥å‡ ä¸ªæ–°çš„å˜é‡ï¼š

```py

env_sizeÂ =Â 8 
state_sizeÂ =Â 6 
n_actionsÂ =Â 4 
epsilonÂ =Â 1.0 
historyÂ =Â {"state":Â [],Â "reward":Â []} 
modelÂ =Â RLModelDropout(state_size,Â n_actions,Â num_epochs=400) 
n_samplesÂ =Â 1000 
max_stepsÂ =Â 500 
regretsÂ =Â [] 
collisionsÂ =Â 0 
failedÂ =Â 0
```

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ª`collisions`å˜é‡å’Œä¸€ä¸ª`failed`å˜é‡ã€‚è¿™äº›å˜é‡å°†è¿½è¸ªç¢°æ’æ¬¡æ•°å’Œå¤±è´¥çš„å›åˆæ¬¡æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å°†è´å¶æ–¯æ¨¡å‹çš„è¡¨ç°ä¸éè´å¶æ–¯æ¨¡å‹çš„è¡¨ç°è¿›è¡Œæ¯”è¾ƒã€‚ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½è¿è¡Œå®éªŒäº†ï¼

#### ç¬¬ 10 æ­¥ï¼šè¿è¡Œæˆ‘ä»¬çš„ BDL å¼ºåŒ–å­¦ä¹ å®éªŒ

å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†å¯¹å®éªŒè¿›è¡Œ 100 å›åˆçš„è¿è¡Œã€‚ç„¶è€Œï¼Œè¿™æ¬¡ï¼Œæˆ‘ä»¬åªä¼šåœ¨å‰ 50 å›åˆè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚ä¹‹åï¼Œæˆ‘ä»¬å°†åœæ­¢è®­ç»ƒï¼Œè¯„ä¼°æ¨¡å‹åœ¨æ‰¾åˆ°å®‰å…¨è·¯å¾„åˆ°è¾¾ç›®æ ‡æ–¹é¢çš„è¡¨ç°ã€‚åœ¨è¿™æœ€å 50 å›åˆä¸­ï¼Œæˆ‘ä»¬å°†`dynamic_obstacle`è®¾ç½®ä¸º`True`ï¼Œæ„å‘³ç€æˆ‘ä»¬çš„ç¯å¢ƒå°†ä¸ºæ¯ä¸€å›åˆéšæœºé€‰æ‹©ä¸€ä¸ªæ–°çš„éšœç¢ç‰©ä½ç½®ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™äº›éšæœºä½ç½®å°†ä¼šä½äºä»£ç†ä¸ç›®æ ‡ä¹‹é—´çš„*ç†æƒ³è·¯å¾„*ä¸Šã€‚

è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä»£ç ï¼š

```py

forÂ iÂ inÂ range(100): 
ifÂ iÂ *<*Â 50: 
envÂ =Â Environment(env_size,Â max_steps=max_steps) 
dynamic_obstacleÂ =Â False 
else: 
dynamic_obstacleÂ =Â True 
epsilonÂ =Â 0 
envÂ =Â Environment( 
env_size,Â max_steps=max_steps,Â dynamic_obstacle=True 
) 
Â Â Â Â ...
```

é¦–å…ˆï¼Œæˆ‘ä»¬æ£€æŸ¥å›åˆæ˜¯å¦åœ¨å‰ 50 å›åˆä¹‹å†…ã€‚å¦‚æœæ˜¯ï¼Œæˆ‘ä»¬é€šè¿‡è®¾ç½®`dynamic_obstacle=False`å®ä¾‹åŒ–ç¯å¢ƒï¼Œå¹¶å°†å…¨å±€å˜é‡`dynamic_obstacle`è®¾ç½®ä¸º`False`ã€‚

å¦‚æœå›åˆæ˜¯æœ€å 50 å›åˆä¹‹ä¸€ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¸¦æœ‰éšæœºéšœç¢ç‰©çš„ç¯å¢ƒï¼Œå¹¶å°†`epsilon`è®¾ç½®ä¸º 0ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬åœ¨é€‰æ‹©åŠ¨ä½œæ—¶æ€»æ˜¯ä½¿ç”¨æ¨¡å‹é¢„æµ‹ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¿›å…¥`while`å¾ªç¯ï¼Œä½¿æˆ‘ä»¬çš„ä»£ç†å¼€å§‹ç§»åŠ¨ã€‚è¿™ä¸æˆ‘ä»¬åœ¨ä¸Šä¸€ä¸ªç¤ºä¾‹ä¸­çœ‹åˆ°çš„å¾ªç¯éå¸¸ç›¸ä¼¼ï¼Œåªä¸è¿‡è¿™æ¬¡æˆ‘ä»¬è°ƒç”¨äº†`env.get_obstacle_proximity()`ï¼Œå¹¶å°†è¿”å›çš„éšœç¢ç‰©æ¥è¿‘ä¿¡æ¯ç”¨äºæˆ‘ä»¬çš„é¢„æµ‹ï¼ŒåŒæ—¶ä¹Ÿå°†æ­¤ä¿¡æ¯å­˜å‚¨åœ¨å›åˆå†å²ä¸­ï¼š

```py

... 
whileÂ notÂ env.is_done: 
stateÂ =Â env.get_state() 
obstacle_proximityÂ =Â env.get_obstacle_proximity() 
ifÂ np.random.rand()Â *<*Â epsilon: 
actionÂ =Â np.random.randint(n_actions) 
else: 
actionÂ =Â model.predict(state,Â obstacle_proximity,Â dynamic_obstacle) 
rewardÂ =Â env.update(action) 
history["state"].append( 
np.concatenate([state,Â [action], 
[obstacle_proximity[action]]]) 
) 
history["reward"].append(reward) 
Â Â Â Â ...
```

æœ€åï¼Œæˆ‘ä»¬å°†è®°å½•ä¸€äº›å·²å®Œæˆå›åˆçš„ä¿¡æ¯ï¼Œå¹¶å°†æœ€æ–°å›åˆçš„ç»“æœæ‰“å°åˆ°ç»ˆç«¯ã€‚æˆ‘ä»¬æ›´æ–°`failed`å’Œ`collisions`å˜é‡ï¼Œå¹¶æ‰“å°å›åˆæ˜¯å¦æˆåŠŸå®Œæˆï¼Œä»£ç†æ˜¯å¦æœªèƒ½æ‰¾åˆ°ç›®æ ‡ï¼Œæˆ–ä»£ç†æ˜¯å¦ä¸éšœç¢ç‰©å‘ç”Ÿç¢°æ’ï¼š

```py

ifÂ env.total_stepsÂ ==Â max_steps: 
print(f"FailedÂ toÂ findÂ targetÂ forÂ episodeÂ {i}.Â Epsilon:Â {epsilon}") 
failedÂ +=Â 1 
elifÂ env.total_stepsÂ *<*Â env.ideal_steps: 
print(f"CollidedÂ withÂ obstacleÂ duringÂ episodeÂ {i}.Â Epsilon:Â {epsilon}") 
collisionsÂ +=Â 1 
else: 
print( 
f"CompletedÂ episodeÂ {i}Â inÂ {env.total_steps}Â steps." 
f"IdealÂ steps:Â {env.ideal_steps}." 
f"Epsilon:Â {epsilon}" 
) 
regrets.append(np.abs(env.total_steps-env.ideal_steps)) 
ifÂ notÂ dynamic_obstacle: 
idxsÂ =Â np.random.choice(len(history["state"]),Â n_samples) 
model.fit( 
np.array(history["state"])[idxs], 
np.array(history["reward"])[idxs] 
) 
Â Â Â Â Â Â Â Â epsilon-=epsilon/10
```

è¿™é‡Œçš„æœ€åä¸€æ¡è¯­å¥è¿˜æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦å¤„äºåŠ¨æ€éšœç¢ç‰©é˜¶æ®µï¼Œå¦‚æœä¸æ˜¯ï¼Œåˆ™è¿›è¡Œä¸€æ¬¡è®­ç»ƒï¼Œå¹¶å‡å°‘æˆ‘ä»¬çš„ epsilon å€¼ï¼ˆå¦‚åŒä¸Šä¸€ä¸ªç¤ºä¾‹ï¼‰ã€‚

é‚£ä¹ˆï¼Œæˆ‘ä»¬çš„è¡¨ç°å¦‚ä½•ï¼Ÿé‡å¤è¿›è¡Œä¸Šè¿° 100 å›åˆçš„å®éªŒï¼Œå¯¹äº`RLModel`å’Œ`RLModelDropout`æ¨¡å‹ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä»¥ä¸‹ç»“æœï¼š

|

* * *

|

* * *

|

* * *

|

* * *

|

| **æ¨¡å‹** | **å¤±è´¥çš„å›åˆæ•°** | **ç¢°æ’æ¬¡æ•°** | **æˆåŠŸçš„å›åˆæ•°** |
| --- | --- | --- | --- |

|

* * *

|

* * *

|

* * *

|

* * *

|

| **RLModelDropout** | 19 | 3 | 31 |
| --- | --- | --- | --- |

|

* * *

|

* * *

|

* * *

|

* * *

|

| **RLModel** | 16 | 10 | 34 |
| --- | --- | --- | --- |

|

* * *

|

* * *

|

* * *

|

* * *

|

|  |  |  |  |
| --- | --- | --- | --- |

å›¾ 8.13ï¼šä¸€å¼ æ˜¾ç¤ºç¢°æ’é¢„æµ‹çš„è¡¨æ ¼

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œåœ¨é€‰æ‹©ä½¿ç”¨æ ‡å‡†ç¥ç»ç½‘ç»œè¿˜æ˜¯è´å¶æ–¯ç¥ç»ç½‘ç»œæ—¶ï¼Œéƒ½æœ‰å…¶ä¼˜ç¼ºç‚¹â€”â€”æ ‡å‡†ç¥ç»ç½‘ç»œå®Œæˆäº†æ›´å¤šçš„æˆåŠŸå›åˆã€‚ç„¶è€Œï¼Œå…³é”®æ˜¯ï¼Œä½¿ç”¨è´å¶æ–¯ç¥ç»ç½‘ç»œçš„ä»£ç†ä»…ä¸éšœç¢ç‰©å‘ç”Ÿäº† 3 æ¬¡ç¢°æ’ï¼Œè€Œæ ‡å‡†æ–¹æ³•å‘ç”Ÿäº† 10 æ¬¡ç¢°æ’â€”â€”è¿™æ„å‘³ç€ç¢°æ’å‡å°‘äº† 70%ï¼

è¯·æ³¨æ„ï¼Œç”±äºå®éªŒæ˜¯éšæœºçš„ï¼Œæ‚¨çš„ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œä½†åœ¨ GitHub ä»“åº“ä¸­ï¼Œæˆ‘ä»¬å·²åŒ…æ‹¬å®Œæ•´çš„å®éªŒä»¥åŠç”¨äºç”Ÿæˆè¿™äº›ç»“æœçš„ç§å­ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹åœ¨`RLModelDropout`çš„`proximity_dict`å­—å…¸ä¸­è®°å½•çš„æ•°æ®ï¼Œæ›´å¥½åœ°ç†è§£ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼š

```py

importÂ matplotlib.pyplotÂ asÂ plt 
importÂ seabornÂ asÂ sns 

df_plotÂ =Â pd.DataFrame(model.proximity_dict) 
sns.boxplot(x="proximityÂ sensorÂ value",Â y="uncertainty",Â data=df_plot)
```

è¿™å°†äº§ç”Ÿä»¥ä¸‹å›¾è¡¨ï¼š

![PIC](img/file181.png)

å›¾ 8.14ï¼šä¸å¢åŠ çš„æ¥è¿‘ä¼ æ„Ÿå™¨å€¼ç›¸å…³çš„ä¸ç¡®å®šæ€§ä¼°è®¡åˆ†å¸ƒ

å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ¨¡å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡éšç€ä¼ æ„Ÿå™¨å€¼çš„å¢åŠ è€Œå¢åŠ ã€‚è¿™æ˜¯å› ä¸ºï¼Œåœ¨å‰ 50 ä¸ªå›åˆä¸­ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“å­¦ä¼šäº†é¿å¼€ç¯å¢ƒçš„ä¸­å¿ƒï¼ˆå› ä¸ºéšœç¢ç‰©å°±åœ¨è¿™é‡Œï¼‰â€”â€”å› æ­¤ï¼Œå®ƒä¹ æƒ¯äº†è¾ƒä½ï¼ˆæˆ–ä¸ºé›¶ï¼‰çš„æ¥è¿‘ä¼ æ„Ÿå™¨å€¼ã€‚è¿™æ„å‘³ç€è¾ƒé«˜çš„ä¼ æ„Ÿå™¨å€¼æ˜¯å¼‚å¸¸çš„ï¼Œå› æ­¤èƒ½å¤Ÿè¢«æ¨¡å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ‰€æ•æ‰åˆ°ã€‚ç„¶åï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“é€šè¿‡ä½¿ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥ MPC æ–¹ç¨‹ï¼ŒæˆåŠŸåœ°è§£å†³äº†è¿™ç§ä¸ç¡®å®šæ€§ã€‚

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•å°† BDL åº”ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œä»¥ä¿ƒè¿›å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æ›´è°¨æ…çš„è¡Œä¸ºã€‚å°½ç®¡è¿™é‡Œçš„ç¤ºä¾‹ç›¸å¯¹åŸºç¡€ï¼Œä½†å…¶å«ä¹‰å´ç›¸å½“æ·±è¿œï¼šæƒ³è±¡ä¸€ä¸‹å°†å…¶åº”ç”¨äºå®‰å…¨å…³é”®çš„åº”ç”¨åœºæ™¯ã€‚åœ¨è¿™äº›ç¯å¢ƒä¸­ï¼Œå¦‚æœæ»¡è¶³æ›´å¥½çš„å®‰å…¨è¦æ±‚ï¼Œæˆ‘ä»¬å¾€å¾€æ„¿æ„æ¥å—æ¨¡å‹æ€§èƒ½è¾ƒå·®ã€‚å› æ­¤ï¼ŒBDL åœ¨å®‰å…¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸä¸­å æœ‰é‡è¦åœ°ä½ï¼Œèƒ½å¤Ÿå¼€å‘å‡ºé€‚ç”¨äºå®‰å…¨å…³é”®åœºæ™¯çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚

åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ BDL åˆ›å»ºå¯¹æŠ—æ€§è¾“å…¥å…·æœ‰é²æ£’æ€§çš„æ¨¡å‹ï¼Œè¿™æ˜¯ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¦ä¸€ä¸ªå…³é”®è€ƒè™‘å› ç´ ã€‚

## 8.6 å¯¹æŠ—æ€§è¾“å…¥çš„æ˜“æ„Ÿæ€§

åœ¨*ç¬¬ä¸‰ç« *ã€*æ·±åº¦å­¦ä¹ åŸºç¡€*ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°é€šè¿‡ç¨å¾®æ‰°åŠ¨å›¾åƒçš„è¾“å…¥åƒç´ ï¼Œå¯ä»¥æ¬ºéª— CNNã€‚åŸæœ¬æ¸…æ™°çœ‹èµ·æ¥åƒçŒ«çš„å›¾ç‰‡ï¼Œè¢«é«˜ç½®ä¿¡åº¦åœ°é¢„æµ‹ä¸ºç‹—ã€‚æˆ‘ä»¬åˆ›å»ºçš„å¯¹æŠ—æ€§æ”»å‡»ï¼ˆ*FSGM*ï¼‰æ˜¯è®¸å¤šå¯¹æŠ—æ€§æ”»å‡»ä¹‹ä¸€ï¼ŒBDL å¯èƒ½æä¾›ä¸€å®šçš„é˜²æŠ¤ä½œç”¨ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™åœ¨å®è·µä¸­æ˜¯å¦‚ä½•è¿ä½œçš„ã€‚

#### ç¬¬ä¸€æ­¥ï¼šæ¨¡å‹è®­ç»ƒ

æˆ‘ä»¬ä¸æ˜¯ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚åœ¨*ç¬¬ä¸‰ç« *ã€*æ·±åº¦å­¦ä¹ åŸºç¡€*ä¸­æ‰€åšçš„é‚£æ ·ï¼Œè€Œæ˜¯ä»é›¶å¼€å§‹è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸*ç¬¬ä¸‰ç« *ã€*æ·±åº¦å­¦ä¹ åŸºç¡€*ä¸­ç›¸åŒçš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®â€”â€”æœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†ï¼Œè¯·å‚è§è¯¥ç« èŠ‚ã€‚æé†’ä¸€ä¸‹ï¼Œæ•°æ®é›†æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„çŒ«ç‹—æ•°æ®é›†ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ç±»ä¼¼ VGG çš„æ¶æ„ï¼Œä½†åœ¨æ¯ä¸ª`MaxPooling2D`å±‚ä¹‹ååŠ å…¥äº† dropoutï¼š

```py

defÂ conv_block(filters): 
returnÂ [ 
tf.keras.layers.Conv2D( 
filters, 
(3,Â 3), 
activation="relu", 
kernel_initializer="he_uniform", 
), 
tf.keras.layers.MaxPooling2D((2,Â 2)), 
tf.keras.layers.Dropout(0.5), 
] 

modelÂ =Â tf.keras.models.Sequential( 
[ 
tf.keras.layers.Conv2D( 
32, 
(3,Â 3), 
activation="relu", 
input_shape=(160,Â 160,Â 3), 
kernel_initializer="he_uniform", 
), 
tf.keras.layers.MaxPooling2D((2,Â 2)), 
tf.keras.layers.Dropout(0.2), 
*conv_block(64), 
*conv_block(128), 
*conv_block(256), 
*conv_block(128), 
tf.keras.layers.Conv2D( 
64, 
(3,Â 3), 
activation="relu", 
kernel_initializer="he_uniform", 
), 
tf.keras.layers.Flatten(), 
tf.keras.layers.Dense(64,Â activation="relu"), 
tf.keras.layers.Dropout(0.5), 
tf.keras.layers.Dense(2), 
] 
) 

```

ç„¶åï¼Œæˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹ï¼š

```py

train_dataset_divprocessedÂ =Â train_dataset.map(lambdaÂ x,Â y:Â (xÂ /Â 255.,Â y)) 
val_dataset_divprocessedÂ =Â validation_dataset.map(lambdaÂ x,Â y:Â (xÂ /Â 255.,Â y)) 

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), 
loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), 
metrics=['accuracy']) 
model.fit( 
train_dataset_divprocessed, 
epochs=200, 
validation_data=val_dataset_divprocessed, 
)
```

è¿™å°†ä½¿æˆ‘ä»¬çš„æ¨¡å‹å‡†ç¡®ç‡è¾¾åˆ°å¤§çº¦ 85%ã€‚

#### ç¬¬äºŒæ­¥ï¼šè¿è¡Œæ¨ç†å¹¶è¯„ä¼°æˆ‘ä»¬çš„æ ‡å‡†æ¨¡å‹

ç°åœ¨æˆ‘ä»¬å·²ç»è®­ç»ƒå¥½äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®ƒå¯¹æŠ—å¯¹æŠ—æ”»å‡»çš„ä¿æŠ¤æ•ˆæœæœ‰å¤šå¥½ã€‚åœ¨*ç¬¬ä¸‰ç« **æ·±åº¦å­¦ä¹ åŸºç¡€*ä¸­ï¼Œæˆ‘ä»¬ä»å¤´å¼€å§‹åˆ›å»ºäº†ä¸€ä¸ªå¯¹æŠ—æ”»å‡»ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`cleverhans`åº“æ¥ä¸ºå¤šä¸ªå›¾åƒä¸€æ¬¡æ€§åˆ›å»ºç›¸åŒçš„æ”»å‡»ï¼š

```py

fromÂ cleverhans.tf2.attacks.fast_gradient_methodÂ importÂ ( 
fast_gradient_methodÂ asÂ fgsm, 
)
```

é¦–å…ˆï¼Œè®©æˆ‘ä»¬è¡¡é‡æˆ‘ä»¬ç¡®å®šæ€§æ¨¡å‹åœ¨åŸå§‹å›¾åƒå’Œå¯¹æŠ—å›¾åƒä¸Šçš„å‡†ç¡®ç‡ï¼š

```py

Predictions_standard,Â predictions_fgsm,Â labelsÂ =Â [],Â [],Â [] 
forÂ imgs,Â labels_batchÂ inÂ test_dataset: 
imgsÂ /=Â 255\. 
predictions_standard.extend(model.predict(imgs)) 
imgs_advÂ =Â fgsm(model,Â imgs,Â 0.01,Â np.inf) 
predictions_fgsm.extend(model.predict(imgs_adv)) 
Â Â labels.extend(labels_batch)
```

ç°åœ¨æˆ‘ä»¬æœ‰äº†æˆ‘ä»¬çš„é¢„æµ‹ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å°å‡ºå‡†ç¡®ç‡ï¼š

```py

accuracy_standardÂ =Â CategoricalAccuracy()( 
labels,Â predictions_standard 
).numpy() 
accuracy_fgsmÂ =Â CategoricalAccuracy()( 
labels,Â predictions_fgsm 
).numpy() 
print(f"{accuracy_standard=.2%},Â {accuracy_fsgm=:.2%}") 
#Â accuracy_standard=83.67%,Â accuracy_fsgm=30.70%
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æ ‡å‡†æ¨¡å‹å¯¹è¿™ç§å¯¹æŠ—æ”»å‡»å‡ ä¹æ²¡æœ‰æä¾›ä»»ä½•ä¿æŠ¤ã€‚å°½ç®¡å®ƒåœ¨æ ‡å‡†å›¾åƒä¸Šçš„è¡¨ç°ç›¸å½“ä¸é”™ï¼Œä½†å®ƒåœ¨å¯¹æŠ—å›¾åƒä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º 30.70%ï¼è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªè´å¶æ–¯æ¨¡å‹èƒ½å¦åšå¾—æ›´å¥½ã€‚å› ä¸ºæˆ‘ä»¬è®­ç»ƒäº†å¸¦ dropout çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°å°†å…¶è½¬å˜ä¸º MC dropout æ¨¡å‹ã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ¨ç†å‡½æ•°ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¿æŒ dropoutï¼Œå¦‚`training=True`å‚æ•°æ‰€ç¤ºï¼š

```py

importÂ numpyÂ asÂ np 

defÂ mc_dropout(model,Â images,Â n_inference:Â intÂ =Â 50): 
returnÂ np.swapaxes(np.stack([ 
model(images,Â training=True)Â forÂ _Â inÂ range(n_inference) 
Â Â ]),Â 0,Â 1)
```

æœ‰äº†è¿™ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ MC dropout æ¨ç†æ›¿ä»£æ ‡å‡†çš„å¾ªç¯ã€‚æˆ‘ä»¬å†æ¬¡è·Ÿè¸ªæ‰€æœ‰çš„é¢„æµ‹ï¼Œå¹¶å¯¹æ ‡å‡†å›¾åƒå’Œå¯¹æŠ—å›¾åƒè¿›è¡Œæ¨ç†ï¼š

```py

Predictions_standard_mc,Â predictions_fgsm_mc,Â labelsÂ =Â [],Â [],Â [] 
forÂ imgs,Â labels_batchÂ inÂ test_dataset: 
imgsÂ /=Â 255\. 
predictions_standard_mc.extend( 
mc_dropout(model,Â imgs,Â 50) 
) 
imgs_advÂ =Â fgsm(model,Â imgs,Â 0.01,Â np.inf) 
predictions_fgsm_mc.extend( 
mc_dropout(model,Â imgs_adv,Â 50) 
) 
Â Â labels.extend(labels_batch)
```

æˆ‘ä»¬å¯ä»¥å†æ¬¡æ‰“å°å‡ºæˆ‘ä»¬çš„å‡†ç¡®ç‡ï¼š

```py

accuracy_standard_mcÂ =Â CategoricalAccuracy()( 
labels,Â np.stack(predictions_standard_mc).mean(axis=1) 
).numpy() 
accuracy_fgsm_mcÂ =Â CategoricalAccuracy()( 
labels,Â np.stack(predictions_fgsm_mc).mean(axis=1) 
).numpy() 
print(f"{accuracy_standard_mc=.2%},Â {accuracy_fgsm_mc=:.2%}") 
#Â accuracy_standard_mc=86.60%,Â accuracy_fgsm_mc=80.75%
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç®€å•çš„ä¿®æ”¹ä½¿å¾—æ¨¡å‹è®¾ç½®åœ¨é¢å¯¹å¯¹æŠ—æ ·æœ¬æ—¶æ›´åŠ ç¨³å¥ã€‚å‡†ç¡®ç‡ä»çº¦ 30%æé«˜åˆ°äº† 80%ä»¥ä¸Šï¼Œæ¥è¿‘äºç¡®å®šæ€§æ¨¡å‹åœ¨æœªæ‰°åŠ¨å›¾åƒä¸Šçš„ 83%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°ï¼ŒMC dropout ä¹Ÿä½¿å¾—æˆ‘ä»¬çš„æ ‡å‡†å›¾åƒå‡†ç¡®ç‡æé«˜äº†å‡ ä¸ªç™¾åˆ†ç‚¹ï¼Œä» 83%æå‡åˆ°äº† 86%ã€‚å‡ ä¹æ²¡æœ‰ä»»ä½•æ–¹æ³•èƒ½å¤Ÿå®Œç¾åœ°å¯¹æŠ—å¯¹æŠ—æ ·æœ¬ï¼Œå› æ­¤èƒ½å¤Ÿæ¥è¿‘æˆ‘ä»¬æ¨¡å‹åœ¨æ ‡å‡†å›¾åƒä¸Šçš„å‡†ç¡®ç‡æ˜¯ä¸€ä¸ªä¼Ÿå¤§çš„æˆå°±ã€‚

å› ä¸ºæˆ‘ä»¬çš„æ¨¡å‹ä¹‹å‰æ²¡æœ‰è§è¿‡å¯¹æŠ—å›¾åƒï¼Œæ‰€ä»¥ä¸€ä¸ªå…·æœ‰è‰¯å¥½ä¸ç¡®å®šæ€§å€¼çš„æ¨¡å‹åº”è¯¥åœ¨å¯¹æŠ—å›¾åƒä¸Šç›¸å¯¹äºæ ‡å‡†æ¨¡å‹è¡¨ç°å‡ºæ›´ä½çš„å¹³å‡ä¿¡å¿ƒã€‚è®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦æ˜¯è¿™æ ·ã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æˆ‘ä»¬ç¡®å®šæ€§æ¨¡å‹é¢„æµ‹çš„å¹³å‡ softmax å€¼ï¼Œå¹¶ä¸º MC dropout é¢„æµ‹åˆ›å»ºä¸€ä¸ªç±»ä¼¼çš„å‡½æ•°ï¼š

```py

defÂ get_mean_softmax_value(predictions)Â -*>*Â float: 
mean_softmaxÂ =Â tf.nn.softmax(predictions,Â axis=1) 
max_softmaxÂ =Â np.max(mean_softmax,Â axis=1) 
mean_max_softmaxÂ =Â max_softmax.mean() 
returnÂ mean_max_softmax 

defÂ get_mean_softmax_value_mc(predictions)Â -*>*Â float: 
predictions_npÂ =Â np.stack(predictions) 
predictions_np_meanÂ =Â predictions_np.mean(axis=1) 
Â Â returnÂ get_mean_softmax_value(predictions_np_mean)
```

ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å°å‡ºä¸¤ä¸ªæ¨¡å‹çš„å¹³å‡ softmax åˆ†æ•°ï¼š

```py

mean_standardÂ =Â get_mean_softmax_value(predictions_standard) 
mean_fgsmÂ =Â get_mean_softmax_value(predictions_fgsm) 
mean_standard_mcÂ =Â get_mean_softmax_value_mc(predictions_standard_mc) 
mean_fgsm_mcÂ =Â get_mean_softmax_value_mc(predictions_fgsm_mc) 
print(f"{mean_standard=:.2%},Â {mean_fgsm=:.2%}") 
print(f"{mean_standard_mc=:.2%},Â {mean_fgsm_mc=:.2%}") 
#Â mean_standard=89.58%,Â mean_fgsm=89.91% 
#Â mean_standard_mc=89.48%,Â mean_fgsm_mc=85.25%
```

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¸æ ‡å‡†å›¾åƒç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ ‡å‡†æ¨¡å‹åœ¨å¯¹æŠ—å›¾åƒä¸Šçš„ä¿¡å¿ƒå®é™…ä¸Šç¨å¾®æ›´é«˜ï¼Œå°½ç®¡å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ MC dropout æ¨¡å‹åœ¨å¯¹æŠ—å›¾åƒä¸Šçš„ä¿¡å¿ƒä½äºæ ‡å‡†å›¾åƒã€‚è™½ç„¶ä¿¡å¿ƒçš„ä¸‹é™å¹…åº¦ä¸å¤§ï¼Œä½†æˆ‘ä»¬å¾ˆé«˜å…´çœ‹åˆ°ï¼Œå°½ç®¡å‡†ç¡®ç‡ä¿æŒåˆç†ï¼Œæ¨¡å‹åœ¨å¯¹æŠ—å›¾åƒä¸Šçš„å¹³å‡ä¿¡å¿ƒä¸‹é™äº†ã€‚

## 8.7 æ€»ç»“

åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡äº”ä¸ªä¸åŒçš„æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†ç°ä»£ BDL çš„å„ç§åº”ç”¨ã€‚æ¯ä¸ªæ¡ˆä¾‹ç ”ç©¶éƒ½ä½¿ç”¨äº†ä»£ç ç¤ºä¾‹ï¼Œçªå‡ºäº† BDL åœ¨åº”å¯¹åº”ç”¨æœºå™¨å­¦ä¹ å®è·µä¸­çš„å„ç§å¸¸è§é—®é¢˜æ—¶çš„ç‰¹å®šä¼˜åŠ¿ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ BDL åœ¨åˆ†ç±»ä»»åŠ¡ä¸­æ£€æµ‹åˆ†å¸ƒå¤–å›¾åƒã€‚æ¥ç€ï¼Œæˆ‘ä»¬æ¢è®¨äº† BDL æ–¹æ³•å¦‚ä½•ç”¨äºä½¿æ¨¡å‹æ›´åŠ é²æ£’ï¼Œä»¥åº”å¯¹æ•°æ®é›†åç§»ï¼Œè¿™æ˜¯ç”Ÿäº§ç¯å¢ƒä¸­ä¸€ä¸ªéå¸¸å¸¸è§çš„é—®é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬å­¦ä¹ äº† BDL å¦‚ä½•å¸®åŠ©æˆ‘ä»¬é€‰æ‹©æœ€æœ‰ä¿¡æ¯é‡çš„æ•°æ®ç‚¹ï¼Œä»¥è®­ç»ƒå’Œæ›´æ–°æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è½¬å‘å¼ºåŒ–å­¦ä¹ ï¼Œçœ‹åˆ° BDL å¦‚ä½•å¸®åŠ©å¼ºåŒ–å­¦ä¹ ä»£ç†å®ç°æ›´åŠ è°¨æ…çš„è¡Œä¸ºã€‚æœ€åï¼Œæˆ‘ä»¬çœ‹åˆ°äº† BDL åœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡»æ—¶çš„åº”ç”¨ã€‚

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡å›é¡¾å½“å‰è¶‹åŠ¿å’Œæœ€æ–°æ–¹æ³•æ¥å±•æœ› BDL çš„æœªæ¥ã€‚

## 8.8 è¿›ä¸€æ­¥é˜…è¯»

ä»¥ä¸‹é˜…è¯»æ¸…å•å°†å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£æˆ‘ä»¬åœ¨æœ¬ç« ä¸­æ¶‰åŠçš„ä¸€äº›ä¸»é¢˜ï¼š

+   *åŸºå‡†æµ‹è¯•ç¥ç»ç½‘ç»œå¯¹å¸¸è§æŸåå’Œ* *æ‰°åŠ¨*çš„é²æ£’æ€§ï¼ŒDan Hendrycks å’Œ Thomas Dietterichï¼Œ2019 å¹´ï¼šè¿™ç¯‡è®ºæ–‡ä»‹ç»äº†å›¾åƒè´¨é‡æ‰°åŠ¨ï¼Œä»¥åŸºå‡†æµ‹è¯•æ¨¡å‹çš„é²æ£’æ€§ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨é²æ£’æ€§æ¡ˆä¾‹ç ”ç©¶ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚

+   *ä½ èƒ½ä¿¡ä»»æ¨¡å‹çš„ä¸ç¡®å®šæ€§å—ï¼Ÿè¯„ä¼°æ•°æ®é›†åç§»ä¸‹çš„* *é¢„æµ‹ä¸ç¡®å®šæ€§*ï¼ŒYaniv Ovadiaã€Emily Fertig ç­‰ï¼Œ2019 å¹´ï¼šè¿™ç¯‡æ¯”è¾ƒè®ºæ–‡ä½¿ç”¨å›¾åƒè´¨é‡æ‰°åŠ¨ï¼Œåœ¨ä¸åŒçš„ä¸¥é‡ç¨‹åº¦ä¸‹å¼•å…¥äººå·¥æ•°æ®é›†åç§»ï¼Œå¹¶è¡¡é‡ä¸åŒçš„æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å‡†ç¡®æ€§å’Œæ ¡å‡†æ–¹é¢å¦‚ä½•å“åº”æ•°æ®é›†åç§»ã€‚

+   *ç”¨äºæ£€æµ‹ç¥ç»ç½‘ç»œä¸­è¯¯åˆ†ç±»å’Œåˆ†å¸ƒå¤–* *æ ·æœ¬çš„åŸºå‡†*ï¼ŒDan Hendrycks å’Œ Kevin Gimpelï¼Œ2016 å¹´ï¼šè¿™ç¯‡åŸºç¡€æ€§çš„åˆ†å¸ƒå¤–æ£€æµ‹è®ºæ–‡ä»‹ç»äº†è¯¥æ¦‚å¿µï¼Œå¹¶è¡¨æ˜å½“æ¶‰åŠåˆ°åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ£€æµ‹æ—¶ï¼Œsoftmax å€¼å¹¶ä¸å®Œç¾ã€‚

+   *æé«˜ç¥ç»ç½‘ç»œä¸­åˆ†å¸ƒå¤–å›¾åƒæ£€æµ‹çš„å¯é æ€§*ï¼ŒShiyu Liangã€Yixuan Li å’Œ R. Srikantï¼Œ2017 å¹´ï¼šè¡¨æ˜è¾“å…¥æ‰°åŠ¨å’Œæ¸©åº¦ç¼©æ”¾å¯ä»¥æ”¹å–„ç”¨äºåˆ†å¸ƒå¤–æ£€æµ‹çš„ softmax åŸºå‡†ã€‚

+   *ç”¨äºæ£€æµ‹åˆ†å¸ƒå¤–* *æ ·æœ¬å’Œå¯¹æŠ—æ€§æ”»å‡»çš„ç®€å•ç»Ÿä¸€æ¡†æ¶*ï¼ŒKimin Leeã€Kibok Leeã€Honglak Lee å’Œ Jinwoo Shinï¼Œ2018 å¹´ï¼šè¡¨æ˜ä½¿ç”¨é©¬å“ˆæ‹‰è¯ºæ¯”æ–¯è·ç¦»åœ¨åˆ†å¸ƒå¤–æ£€æµ‹ä¸­å¯èƒ½æ˜¯æœ‰æ•ˆçš„ã€‚
