# 第十二章：参考文献

# 第三章

+   词表示: [`dl.acm.org/citation.cfm?id=1858721`](https://dl.acm.org/citation.cfm?id=1858721)

+   独热编码: [`machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/`](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)

+   表示学习: [`github.com/anujgupta82/Representation-Learning-for-NLP`](https://github.com/anujgupta82/Representation-Learning-for-NLP)

+   N-gram 模型: [`citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.53.9367`](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.53.9367)

+   TF-IDF: [`nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html`](https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html)

+   Mikolov *等* 2013: [`arxiv.org/abs/1310.4546`](https://arxiv.org/abs/1310.4546)

+   Maas 和 cgpotts 论文: [`web.stanford.edu/~cgpotts/papers/wvSent_acl2011.pdf`](https://web.stanford.edu/~cgpotts/papers/wvSent_acl2011.pdf)

+   scikit-learn 中的词袋模型: [`scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation`](http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation)

+   Kaggle 上的 word2vec [`www.kaggle.com/c/word2vec-nlp-tutorial`](https://www.kaggle.com/c/word2vec-nlp-tutorial)

+   Heap 法则: [`en.wikipedia.org/wiki/Heaps%27_law`](https://en.wikipedia.org/wiki/Heaps%27_law)

+   句子和文档的分布式表示，Mikolov *等*: [`cs.stanford.edu/~quocle/paragraph_vector.pdf`](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)

+   CBOW: [`towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa`](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)

+   Skip-gram: McCormick, C. (2016, April 19), *Word2Vec 教程 - Skip-Gram 模型* [`mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/`](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)

+   TensorFlow 中的 word2vec 实现: [`github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/word2vec/word2vec_basic.py`](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/word2vec/word2vec_basic.py)

+   Word2vec 详解: [`arxiv.org/abs/1411.2738`](https://arxiv.org/abs/1411.2738)

+   推导负采样: [`arxiv.org/abs/1402.3722`](https://arxiv.org/abs/1402.3722)

+   组合分布式语义学: [`youtu.be/hTmKoHJw3Mg`](https://youtu.be/hTmKoHJw3Mg)

+   fastText 和 skipgram: [`debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/09/28/fast-text-and-skip-gram/`](http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/09/28/fast-text-and-skip-gram/)

+   skip-gram 和 CBOW：[`iksinc.online/tag/continuous-bag-of-words-cbow/`](https://iksinc.online/tag/continuous-bag-of-words-cbow/)

+   斯坦福大学关于 CBOW 和 skip-gram 的讲座：[`cs224d.stanford.edu/lecture_notes/notes1.pdf`](https://cs224d.stanford.edu/lecture_notes/notes1.pdf)

+   [`mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf`](http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf)

+   fasttext PyTorch：[`github.com/PetrochukM/PyTorch-NLP`](https://github.com/PetrochukM/PyTorch-NLP)

+   Levy，Omer 和 Goldberg Yoav（2014），*基于依赖关系的词嵌入*，第 52 届计算语言学协会年会，ACL 2014——会议论文集，[2. 302-308. 10.3115/v1/P14-2050](http://www.aclweb.org/anthology/P14-2050)

+   *噪声对比估计与负采样的笔记*：[`arxiv.org/abs/1410.8251`](https://arxiv.org/abs/1410.8251)

+   *Sebastian Ruder，关于词嵌入 - 第二部分：逼近 Softmax*，[`ruder.io/word-embeddings-softmax`](http://ruder.io/word-embeddings-softmax)，2016。

+   可扩展的层次分布式语言模型。[`papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf`](http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf)

+   Softmax 函数及其导数。[`eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/`](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)

+   *什么是 Softmax 回归，它与逻辑回归有什么关系？*，Sebastian Raschka。[`www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html`](https://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html)

+   [`web.stanford.edu/class/cs224n/reports/2758157.pdf`](https://web.stanford.edu/class/cs224n/reports/2758157.pdf)

+   Softmax 回归，[`ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/`](http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/)

+   Google Allo：[`research.googleblog.com/2016/05/chat-smarter-with-allo.html`](https://research.googleblog.com/2016/05/chat-smarter-with-allo.html)

+   *层次化概率神经网络语言模型*，Morin 和 Bengio，2005，[`www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf`](https://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf)

+   *可扩展的层次分布式语言模型。Mnih，Andriy 和 Hinton*，Geoffrey E. 2009，[`papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model`](https://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model)

+   *自组织层次 Softmax*，[arXiv:1707.08588v1 [cs.CL] 2017 年 7 月 26 日](https://arxiv.org/pdf/1707.08588.pdf)

+   *基于哈夫曼编码算法的有效文本聚类方法*, Nikhil Pawar, 2012 年， [`www.ijsr.net/archive/v3i12/U1VCMTQ1NjE=.pdf`](https://www.ijsr.net/archive/v3i12/U1VCMTQ1NjE=.pdf)

+   Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado 和 Jeffrey Dean, *词和短语的分布式表示及其组合性,* *载于《神经信息处理系统进展》第 26 卷*：*第 27 届神经信息处理系统年会，2013 年*，*会议论文集，2013 年 12 月 5-8 日，美国内华达州湖塔霍，页 3111-3119*。

+   [`debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/09/28/fast-text-and-skip-gram/`](http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/09/28/fast-text-and-skip-gram/)

+   [`github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb`](https://github.com/nzw0301/keras-examples/blob/master/Skip-gram-with-NS.ipynb)

# 第四章

+   Vladimir Zolotov 和 David Kung 2017 年，*fastText 线性文本分类器的分析与优化*, [`arxiv.org/abs/1702.05531`](http://arxiv.org/abs/1702.05531)

+   *线性模型的文本分类*, [`www.cs.umd.edu/class/fall2017/cmsc723/slides/slides_03.pdf`](http://www.cs.umd.edu/class/fall2017/cmsc723/slides/slides_03.pdf)

+   *什么是文本分类*，斯坦福，[`nlp.stanford.edu/IR-book/html/htmledition/the-text-classification-problem-1.html#sec:classificationproblem`](https://nlp.stanford.edu/IR-book/html/htmledition/the-text-classification-problem-1.html#sec:classificationproblem)

+   [`nlp.stanford.edu/IR-book/html/htmledition/text-classification-and-naive-bayes-1.html`](https://nlp.stanford.edu/IR-book/html/htmledition/text-classification-and-naive-bayes-1.html)

+   [`research.fb.com/fasttext/`](https://research.fb.com/fasttext/)

+   *高效分类的技巧集,* [arXiv:1607.01759v3 [cs.CL] 2016 年 8 月 9 日](https://arxiv.org/pdf/1607.01759.pdf)

+   [`github.com/poliglot/fasttext`](https://github.com/poliglot/fasttext)

+   Joseph Turian, Lev Ratinov, 和 Yoshua Bengio, 2010 年，*词表示：一种简单而通用的半监督学习方法,* *载于* *第 48 届计算语言学协会年会论文集* *(ACL '10)*, *计算语言学协会，斯特劳兹堡，美国，384-394*。

+   [arXiv:1607.00570v1 [cs.IR] 2016 年 7 月 2 日](https://arxiv.org/abs/1607.00570)

+   *[Weinberger 等, 2009] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola 和 Josh Attenberg, 2009 年*, *特征哈希用于大规模多任务学习. 载于 ICML 会议*

+   [`developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html`](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html)

+   PyTorch 中的 Softmax 分类器：[`www.youtube.com/watch?v=lvNdl7yg4Pg`](https://www.youtube.com/watch?v=lvNdl7yg4Pg)

+   *分类的层次损失*：[arXiv:1709.01062v1 [cs.LG]](https://arxiv.org/abs/1709.01062)，2017 年 9 月 1 日

+   Svenstrup，Dan & Meinertz Hansen，Jonas & Winther，Ole. (2017)，哈希嵌入用于高效的词表示

+   解释核技巧，[`www.quora.com/How-does-Kernel-compute-inner-product-in-higher-dimensional-space-without-visiting-that-space/answer/Jeremy-McMinis`](https://www.quora.com/How-does-Kernel-compute-inner-product-in-higher-dimensional-space-without-visiting-that-space/answer/Jeremy-McMinis)

+   [`medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f`](https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f)

+   *极快的文本特征提取用于分类和索引*，乔治·福尔曼和埃文·基尔申鲍姆著

+   阿曼德·朱林， [等人. FastText.zip](https://arxiv.org/abs/1612.03651)： *压缩文本分类模型*，2016， [`arxiv.org/abs/1612.03651`](https://arxiv.org/abs/1612.03651)

+   向量量化， [`www.slideshare.net/rajanisharmaa/vector-quantization`](https://www.slideshare.net/rajanisharmaa/vector-quantization)

+   [`shodhganga.inflibnet.ac.in/bitstream/10603/132782/14/12_chapter%204.pdf`](http://shodhganga.inflibnet.ac.in/bitstream/10603/132782/14/12_chapter%204.pdf)

+   *基于 Voronoi 投影的快速最近邻搜索算法：盒子搜索和映射表搜索技术，V. 拉马苏布拉马尼安，K.K. 皮利瓦尔*。1997 年，[`www.sciencedirect.com/science/article/pii/S1051200497903006`](https://www.sciencedirect.com/science/article/pii/S1051200497903006)

+   *如何用 Python 从零实现学习向量量化。杰森·布朗尼*，2016，[`machinelearningmastery.com/implement-learning-vector-quantization-scratch-python/`](https://machinelearningmastery.com/implement-learning-vector-quantization-scratch-python/)

+   赫尔维·热戈、马蒂斯·杜兹和科尔德莉亚·施密德，*用于最近邻搜索的产品量化*，IEEE Trans. PAMI，2011 年 1 月。

+   韩松、毛会子和威廉·J·达利。深度压缩：*通过修剪、训练量化和霍夫曼编码压缩深度神经网络*，*在 ICLR 中*，2016 年

+   孙捷、何凯明、柯启发和孙剑，*优化的产品量化用于近似最近邻搜索*。 *在 CVPR 中*，2013 年 6 月

+   *期望最大化*，乔伊迪普·巴塔查尔吉。[`medium.com/technology-nineleaps/expectation-maximization-4bb203841757`](https://medium.com/technology-nineleaps/expectation-maximization-4bb203841757)

+   陈文林、詹姆斯·T·威尔逊、史蒂芬·泰瑞、基利安·Q·温伯格和陈逸欣，*利用哈希技巧压缩神经网络*， [arXiv:1504.04788, 2015](https://arxiv.org/abs/1504.04788)

+   Kai Zeng, Kun She, 和 Xinzheng Niu, *基于邻域熵的合作博弈理论特征选择*, *计算智能与神经科学*, vol. 2014, 文章 ID 479289, 10 页, 2014, [`doi.org/10.1155/2014/479289`](https://doi.org/10.1155/2014/479289).

# 第五章

+   *大规模语料主题建模软件框架*, Radim, 2010

+   Gensim fastText 教程: [`github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb`](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb)

+   P. Bojanowski, E. Grave, A. Joulin, T. Mikolov, *用子词信息丰富词向量*, [`arxiv.org/abs/1607.04606`](https://arxiv.org/abs/1607.04606)

+   [`proceedings.mlr.press/v37/kusnerb15.pdf`](http://proceedings.mlr.press/v37/kusnerb15.pdf)

+   Tomas Mikolov, Quoc V Le, Ilya Sutskever, 2013, (*利用语言间相似性进行机器翻译*) ([`arxiv.org/pdf/1309.4168.pdf`](https://arxiv.org/pdf/1309.4168.pdf))

+   Georgiana Dinu, Angelikie Lazaridou, 和 Marco Baroni. 2014, *通过缓解中心性问题改善零-shot 学习* ([`arxiv.org/pdf/1412.6568.pdf`](https://arxiv.org/pdf/1412.6568.pdf))

+   fastText 标准化, [`www.kaggle.com/mschumacher/using-fasttext-models-for-robust-embeddings/notebook`](https://www.kaggle.com/mschumacher/using-fasttext-models-for-robust-embeddings/notebook)

+   Luong, Minh-Thang 和 Socher, Richard 和 Manning, Christopher D. 2013, (*通过递归神经网络改善形态学词表示*) ([`nlp.stanford.edu/~lmthang/morphoNLM/`](https://nlp.stanford.edu/~lmthang/morphoNLM/))

# 第六章

+   Yoav Goldberg (2015), *神经网络模型入门：自然语言处理*

    *语言处理*, ([`arxiv.org/abs/1510.00726`](https://arxiv.org/abs/1510.00726))

+   [`www.wildml.com/2015/11/understanding-convolutional-neural-networksfor-nlp/`](http://www.wildml.com/2015/11/understanding-convolutional-neural-networksfor-nlp/)

+   [`mathworld.wolfram.com/Convolution.html`](http://mathworld.wolfram.com/Convolution.html)

+   [`www.joshuakim.io/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-embeddings/`](http://www.joshuakim.io/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-embeddings/)

+   [`machinelearningmastery.com/best-practices-document-classification-deep-learning/`](https://machinelearningmastery.com/best-practices-document-classification-deep-learning/)

+   [`keras.io/layers/embeddings/`](https://keras.io/layers/embeddings/)

+   [`pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html`](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)

+   [`pytorch.org/docs/master/nn.html`](https://pytorch.org/docs/stable/nn.html)

+   [`stackoverflow.com/a/35688187`](https://stackoverflow.com/a/35688187)

+   [`www.brightideasinanalytics.com/rnn-pretrained-word-vectors/`](http://www.brightideasinanalytics.com/rnn-pretrained-word-vectors/)

# 第七章

+   [`developer.android.com/training/basics/firstapp/`](https://developer.android.com/training/basics/firstapp/)

+   [`github.com/sszuev/fastText_java`](https://github.com/sszuev/fastText_java)

+   [`stackoverflow.com/a/35369267`](https://stackoverflow.com/a/35369267)

+   [`developer.android.com/studio/publish/app-signing`](https://developer.android.com/studio/publish/app-signing)

+   [`github.com/vinhkhuc/JFastText/blob/master/examples/api/src/main/java/ApiExample.java`](https://github.com/vinhkhuc/JFastText/blob/master/examples/api/src/main/java/ApiExample.java)

+   fastText 问题： [`github.com/vinhkhuc/JFastText/issues/28`](https://github.com/vinhkhuc/JFastText/issues/28)

+   [`github.com/linkfluence/fastText4j/tree/master/src/main/java/fasttext`](https://github.com/linkfluence/fastText4j/tree/master/src/main/java/fasttext)
