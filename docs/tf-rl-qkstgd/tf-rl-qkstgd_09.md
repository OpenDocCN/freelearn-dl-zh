# 第九章：评估

# 第一章

1.  离策略强化学习算法需要一个重放缓冲区。我们从重放缓冲区中抽取一个小批量的经验，用它来训练 DQN 中的*Q(s,a)* 状态值函数以及 DDPG 中的演员策略。

1.  我们对奖励进行折扣，因为关于智能体的长期表现存在更多不确定性。因此，立即奖励具有更高的权重，下一时间步获得的奖励的权重相对较低，再下一时间步的奖励权重更低，依此类推。

1.  如果*γ > 1*，智能体的训练将不稳定，智能体将无法学习到最优策略。

1.  基于模型的强化学习智能体有潜力表现得很好，但不能保证它一定会比基于无模型的强化学习智能体表现得更好，因为我们构建的环境模型未必总是一个好的模型。而且，构建一个足够准确的环境模型也非常困难。

1.  在深度强化学习中，深度神经网络用于表示*Q(s,a)* 和演员的策略（在 Actor-Critic 设置中是这样的）。在传统的强化学习算法中，使用的是表格型的*Q(s,a)*，但当状态的数量非常大时，这种方式无法应用，而这通常是大多数问题的情况。

# 第三章

1.  在 DQN 中，重放缓冲区用于存储过去的经验，从中抽取一个小批量的数据，并用来训练智能体。

1.  目标网络有助于提高训练的稳定性。这是通过保持一个额外的神经网络来实现的，该网络的权重通过使用主神经网络权重的指数移动平均来更新。或者，另一种广泛使用的方法是每隔几千步就将主神经网络的权重复制到目标网络中。

1.  在 Atari Breakout 问题中，单一帧作为状态是无法提供帮助的。这是因为单一帧无法提取时间信息。例如，单一帧中无法得出球的运动方向。但如果我们将多帧叠加起来，就可以确定球的速度和加速度。

1.  L2 损失已知会对异常值产生过拟合。因此，更倾向使用 Huber 损失，因为它结合了 L2 和 L1 损失。详见维基百科：[`en.wikipedia.org/wiki/Huber_loss`](https://en.wikipedia.org/wiki/Huber_loss)。

1.  也可以使用 RGB 图像。不过，我们需要为神经网络的第一隐藏层增加额外的权重，因为现在在状态堆栈中的四帧每一帧都有三个通道。这种对于状态空间的更精细的细节在 Atari 中并不是必须的。然而，RGB 图像可以在其他应用中提供帮助，例如在自动驾驶和/或机器人技术中。

# 第四章

1.  DQN 被认为会高估状态-动作值函数*Q(s,a)*。为了解决这个问题，引入了 DDQN。DDQN 在高估*Q(s,a)*方面比 DQN 少遇到问题。

1.  对抗网络架构为优势函数和状态价值函数提供了独立的流。然后，这些流被组合起来得到*Q(s,a)*。这种分支后再合并的方式被观察到有助于 RL 代理的训练更加稳定。

1.  **优先经验回放**（**PER**）赋予代理表现较差的经验样本更高的重要性，因此这些样本比代理表现良好的其他样本被采样的频率更高。通过频繁使用表现较差的样本，代理能够更频繁地解决自身的弱点，从而 PER 加速了训练。

1.  在一些计算机游戏中，例如 Atari Breakout，模拟器的帧率过高。如果在每个时间步长中都从策略中采样一个独立的动作，代理的状态可能在一个时间步长内变化不够，因为这个时间步长太小。因此，使用了粘性动作，其中相同的动作会在有限但固定的时间步数内重复，例如`n`，并且在这些 n 个时间步内累计的总奖励将作为执行该动作的奖励。在这些 n 个时间步内，代理的状态已经发生了足够的变化，可以评估所采取的动作的有效性。n 值过小会阻止代理学习到良好的策略；同样，n 值过大也会成为一个问题。必须选择合适的时间步长数，在这些时间步长内执行相同的动作，这取决于所使用的模拟器。

# 第五章

1.  DDPG 是一种基于策略外的算法，因为它使用了回放缓冲区。

1.  一般来说，演员和评论家的隐藏层数量以及每个隐藏层的神经元数量是相同的，但这不是强制要求。请注意，演员和评论家的输出层是不同的，演员的输出数量等于动作的数量；评论家只有一个输出。

1.  DDPG 用于连续控制，即当动作是连续且为实数值时。Atari Breakout 有离散动作，因此 DDPG 不适用于 Atari Breakout。

1.  我们使用`relu`激活函数，因此偏置初始化为小的正值，以便它们在训练开始时就能够激活并允许梯度回传。

1.  这是一个练习。请参见[`gym.openai.com/envs/InvertedDoublePendulum-v2/`](https://gym.openai.com/envs/InvertedDoublePendulum-v2/)。

1.  这也是一个练习。注意当第一层的神经元数量逐渐减少时，学习会发生什么变化。一般来说，信息瓶颈不仅在强化学习（RL）环境中观察到，任何深度学习（DL）问题中也会出现。

# 第六章

1.  **异步优势演员-评论家代理**（**A3C**）是一种基于策略的算法，因为我们并没有使用回放缓冲区来采样数据。然而，使用了一个临时缓冲区来收集即时样本，这些样本会用来训练一次，然后缓冲区会被清空。

1.  Shannon 熵项被用作正则化器——熵越高，策略越好。

1.  当使用过多工作线程时，训练可能会变慢并崩溃，因为内存有限。然而，如果你可以访问大量的处理器集群，那么使用大量的工作线程/进程将会有所帮助。

1.  Softmax 被用于策略网络中，以获取不同动作的概率。

1.  优势函数被广泛使用，因为它降低了策略梯度的方差。《A3C 论文》第 *3 节*（[`arxiv.org/pdf/1602.01783.pdf`](https://arxiv.org/pdf/1602.01783.pdf)）对此有更多说明。

1.  这是一个练习。

# 第七章

1.  **信任区域策略优化**（**TRPO**）具有目标函数和约束条件。因此，它需要二阶优化方法，如共轭梯度法。SGD 和 Adam 不适用于 TRPO。

1.  熵项有助于正则化，它允许智能体进行更多探索。

1.  我们剪切策略比率，以限制每次更新步骤对策略的变化量。如果这个剪切参数 epsilon 设置得较大，则每次更新中策略可能发生剧烈变化，这可能导致一个次优策略，因为智能体的策略变得更加嘈杂并且波动过大。

1.  动作的取值范围在负值和正值之间，因此对 `mu` 使用了 `tanh` 激活函数。对于 sigma，使用 `softplus` 作为 sigma，它始终为正。不能对 sigma 使用 `tanh` 函数，因为 `tanh` 可能会导致 sigma 为负值，这是没有意义的！

1.  奖励塑形通常有助于训练。但如果操作不当，它将无法帮助训练。你必须确保奖励塑形能够保持 `reward` 函数的密度并处于适当的范围内。

1.  不，奖励塑形仅在训练过程中使用。

# 第八章

1.  TORCS 是一个连续控制问题。DQN 仅适用于离散动作，因此不能用于 TORCS。

1.  初始化是另一种初始化策略；你也可以使用指定范围内的 `min` 和 `max` 值进行随机均匀初始化；另一种方法是从一个均值为零、sigma 值已指定的高斯分布中采样。感兴趣的读者应尝试这些不同的初始化方法，并比较智能体的表现。

1.  `abs()` 函数在 `reward` 函数中使用，因为我们对偏离中心的横向漂移进行平等惩罚（无论是左侧还是右侧）。第一个项是纵向速度，因此不需要 `abs()` 函数。

1.  为了探索而加入的高斯噪声可以随着回合数的增加逐渐减少，这可以导致更平稳的驾驶体验。当然，你还可以尝试许多其他技巧！

1.  DDPG 是一个脱离策略的算法，但**近端策略优化**（**PPO**）是一个基于策略的强化学习算法。因此，DDPG 需要一个回放缓冲区来存储过去的经验样本，而 PPO 则不需要回放缓冲区。
