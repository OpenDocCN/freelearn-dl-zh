# 第六章：高级特征工程与 NLP 算法

在本章中，我们将探讨一个既简单又令人惊叹的概念，叫做**词到向量**（**word2vec**）。这个概念是由 Google 的 Tomas Mikolov 领导的研究团队开发的。众所周知，Google 为我们提供了许多伟大的产品和概念，而 word2vec 就是其中之一。在自然语言处理（NLP）中，开发能够处理词语、短语、句子等语义的工具或技术是一项重要任务，而 word2vec 模型在理解词语、短语、句子、段落和文档的语义方面做得非常出色。我们将深入这个向量化的世界，并在其中待上一段时间。你不觉得这非常神奇吗？我们将从概念开始，最终通过一些有趣且实用的例子来结束。所以，让我们开始吧。

# 回顾词嵌入

我们已经在第五章中讨论了词嵌入，*特征工程与 NLP 算法*。我们已经探讨了 NLP 中的语言模型和特征工程技术，在这些模型中，词语或短语被映射到实数向量。将词语转换为实数的技术被称为**词嵌入**。我们一直在使用向量化技术，以及基于**词频-逆文档频率**（**tf-idf**）的向量化技术。那么，让我们直接跳入 word2vec 的世界吧。

# 理解 word2vec 的基础

在这里，我们将尝试通过使用 word2vec 来处理词级别的语义。接着，我们将扩展我们的概念到段落级和文档级。通过查看*图 6.1*，你将看到我们在本书中将要涵盖的不同种类的语义：

![](img/87ce8e19-40b7-4644-ba45-0293e84d7887.png)

图 6.1：不同种类的语义

语义学是处理 NLP 领域中意义的一门分支学科。我们已经在第三章中讨论了词汇语义学，*理解句子结构*。在这里，我们将更深入地讨论分布语义学。语义学中还有其他技术或类型，比如形式语义学和组合语义学；但是在本书中，我们现在不会涵盖这些类型或技术。

# 分布语义学

分布语义学是一个研究领域，专注于开发量化和分类基于大型文本数据样本中分布特性的语言项目之间语义相似性的技术或理论。

我想给你一个例子，让你了解我所说的分布式语义是什么意思。假设你有关于旅行博客的文本数据。现在，作为一个人，你知道意大利面、面条、汉堡等是可食用的食品，而果汁、茶、咖啡等是饮料。作为人类，我们可以轻松地将可饮用和可食用的食物分类，因为我们与每个词都有一定的语境联系，但机器并不能真正理解这些语义。所有描述食物的项目很可能会在数据集中与某些特定词汇一起出现。因此，这里我们关注的是语料库中词汇的分布，假设具有相似分布的语言项目或词汇具有相似的含义。这被称为**分布假设**。

我再给你一个例子。假设你有一个研究论文的数据集。数据集中的一些研究论文属于工程类，另一些属于法律类。包含“工程”、“方程式”、“方法”等词汇的文档与工程相关，因此它们应归为一组；而包含“法律”、“律师”、“法学院”等词汇的文档与法律领域的研究论文相关，因此它们应归为另一组。通过使用分布式语义技术，如 word2vec，我们可以通过使用它们的向量值来区分不同领域的词汇。所有具有相似含义的词汇会被归为一组，因为它们在语料库中的分布相似。你可以参考*图 6.2*，它展示了我们给定的分布式语义示例的向量空间图示，类似语境的词汇聚集在一起：

![](img/4f4f3862-fed5-4b60-b815-ca1d46516406.png)

图 6.2：我们分布式语义示例的向量空间图示

*图 6.3*，让你了解 word2vec 模型是从哪个分支派生出来的：

![](img/2dc79cdc-1778-4734-8495-8d70f8f7487b.png)

图 6.3：从分布式语义中衍生出的主要模型

本章的重点是分布式语义技术，称为**word2vec**。

# 定义 word2vec

Word2vec 是通过使用双层神经网络开发的。它以大量文本数据或文本语料库为输入，并从给定的文本中生成一组向量。

换句话说，我们可以说它生成了一个高维度的向量空间。这个向量空间有数百个维度。

请不要害怕高维度。在本章中，我会简化 word2vec 的整个概念。

当我说 word2vec 模型从文本中生成一组向量或向量空间时，你真的会明白我的意思。这里，我们使用的是一个两层神经网络，当前它是一个黑箱，执行某种逻辑并为我们生成向量。在这个向量空间中，语料库中每个唯一的单词都会被分配一个对应的向量。所以，向量空间只是所有词汇在大规模文本语料库中出现的向量表示。

所以，我想你明白了，对吧？基于我们所学的基础，你可以说 word2vec 是生成词嵌入的模型之一。请回忆一下第五章中关于向量化的部分，*特征工程与 NLP 算法*。我还想在这里强调一点，word2vec 是一种强大的、无监督的词嵌入技术。

# 无监督分布式语义模型的必要性——word2vec

本节内容让我们对 word2vec 为我们解决的众多挑战有了一个初步了解。这些挑战的解决方案引导我们发现了 word2vec 的真正需求。所以，我们首先来看一些挑战，然后再看看 word2vec 模型是如何解决这些挑战的。

# 挑战

这里列出了一些我们正在尝试解决的挑战：

+   当我们开发 NLP 应用时，有一个基本问题——我们知道机器无法理解我们的文本，因此需要将文本数据转换为数值格式。

+   有几种方法可以将文本数据转换为数值格式，但我们采用了一些简单的技术，其中之一就是 one-hot 编码，但这种技术存在以下问题：

    +   假设你有一句话：“I like apple juice”。现在，假设你对句子中的每个单词应用 one-hot 编码。如果你的语料库中有成千上万的句子，那么向量的维度就等于语料库的整个词汇表，如果这些高维列被用来开发 NLP 应用，那我们就需要强大的计算能力，并且需要对这些高维列进行矩阵运算，因为它们会消耗大量时间。

    +   对于语音识别词汇表，词汇量的平均大小是 20,000 个单词。如果我们正在开发一个机器翻译系统，那么可能会使用更多的词汇，比如 50 万个单词。处理这些庞大的向量是一个巨大的挑战。

    +   另一个问题是，当你对句子中的某个单词应用 one-hot 编码时，整个条目都会是零，除了实际表示该单词的那个值，而该值是**1**。为了简便起见，我们暂时考虑语句：“*I like apple juice*”。假设我们的语料库中只有一句话。那么，如果我试图对单词**apple**应用 one-hot 编码，那么**apple**的 one-hot 表示如下。请参考*图 6.4*：![](img/a8438127-5658-46e1-880e-eb3b7094488d.png)

图 6.4：苹果和果汁的单热编码表示

+   单热编码并不揭示词之间上下文相似性的事实。为了理解这一点，我想举个例子，如果你的语料库中有单词 cat 和 cats，则单热编码并不揭示单词 cat 和 cats 是非常相似的词。

+   如果我对苹果和果汁的单热编码向量进行 AND 运算，那么它将不会表达任何上下文相似性。举个例子，如果我对苹果和果汁的单热编码向量进行 AND 运算，答案是 0。事实上，这些词可以一起出现，并且具有很强的上下文关系，但单热编码本身并不能表达有关词相似性的任何重要信息。

+   如果你想找到准确的词相似性，那么 WordNet 将不能为你提供足够的帮助。WordNet 是由专家制作的，而 WordNet 包含的内容更主观，因为是人类用户创建的。

+   使用 WordNet 需要大量的时间和精力。

+   一些新词，如 Coding Ninja、Wizard 等，是 WordNet 的新词，可能不在网站上。由于缺少这些类型的词，我们无法从 WordNet 推导出其他语义关系。

在解决这些挑战之前的每一个挑战都在这些技术的发展中起了重要作用。在过去的二十年中，已经有很多努力致力于开发一种高效、简洁和相关的词的数值表示。最终，在 2013 年，Google 的托马斯·米科洛夫和他的研究团队提出了 word2vec 模型，这种模型以高效的方式解决了许多之前的挑战。

Word2vec 在发现词的相似性以及保留语义关系方面非常出色，这些是以前的技术（如单热编码或使用 WordNet）无法处理的。

我已经在 word2vec 上提供了很多背景，现在让我们开始理解 word2vec 模型的表示、组件和其他参数。

让我们开始我们神奇的旅程吧！

# 将 word2vec 模型从黑箱转换为白箱

从这一节开始，我们将了解 word2vec 模型的每个组成部分，以及模型的工作过程。简而言之，我们正在将 word2vec 的黑箱部分转化为白箱。

我们将重点关注以下程序，以了解 word2vec 模型：

+   基于分布相似性的表示

+   理解 word2vec 模型的组成部分

+   理解 word2vec 模型的逻辑

+   理解 word2vec 模型背后的算法和数学

+   关于 word2vec 模型的一些事实

+   应用 word2vec 模型

让我们开始吧！

# 基于分布相似性的表示

这是自然语言处理领域中一个非常古老且强大的观点。分布相似性的概念是，通过考虑某个特定单词出现的上下文，你可以为该单词的意义提供很多价值，而且它与上下文密切相关。著名语言学家约翰·费尔斯曾有一句非常著名的名言：

“你可以通过它所处的环境来了解一个词。”

让我们举个例子：如果我想要了解“银行业”一词的含义，我将收集成千上万的包含“银行业”一词的句子，然后我会开始查看与“银行业”一词一起出现的其他词汇，并尝试理解“银行业”一词所处的上下文。看一下这些例子，并理解分布相似性：

+   句子 1：银行业由政府进行监管。

+   句子 2：银行机构需要一些技术来改变其传统的运营模式。

在前面的句子中，"银行业"这个词与政府、部门、运营等词语频繁出现。这些词语对理解“银行业”一词的背景和意义非常有帮助。

这些其他词语确实有助于表征“银行业”一词的含义。你还可以利用“银行业”一词来预测该词出现时，最常见和频繁出现的词或短语。

为了更好地表示一个特定单词的含义，并对该单词上下文中出现的其他单词进行预测，我们需要理解该单词的分布式表示。

单词的分布式表示是一种向量形式，其中该单词可以被表示出来。单词以稠密向量的形式表达，而这个稠密向量必须被选择得足够好，以便它能够有效预测该单词上下文中出现的其他词语。现在，我们要对每个需要预测的其他词语也进行类似的操作，这些词语也会附带其他词语。因此，我们使用相似性度量方法，比如向量点积。这是一种递归的方法，每个单词会预测出能够出现在相同上下文中的其他单词，而其他被预测的单词也会通过预测其他单词来执行同样的操作。所以，我们需要一个巧妙的算法来执行这种递归操作。

在这里，请不要混淆分布相似性和分布表示术语。基于分布相似性的表示术语实际上是语义理论的一部分，它帮助我们理解单词在日常使用中的含义；而单词的分布表示是单词以向量形式的表示。要生成单词的向量形式，我们可以使用独热编码或其他任何技术，但这里的主要点是生成一个单词向量，它还带有相似性测量的意义，以便你可以理解单词的上下文含义。当我们谈论分布相似性时，word2vec 就会出现在画面中。

# 理解 word2vec 模型的组成部分

在这一节中，我们将了解 word2vec 模型的主要三个组成部分，如下所示：

+   word2vec 的输入

+   word2vec 的输出

+   word2vec 模型的构造组件

# word2vec 的输入

首先，我们应该意识到我们为开发 word2vec 模型而输入的内容，因为这是一个基础事物，你可以从中开始构建 word2vec。

所以，我想声明我们将使用原始文本语料库作为开发 word2vec 模型的输入。

在实际应用中，我们使用大型语料库作为输入。为简单起见，在本章中我们将使用一个相当小的语料库来理解概念。在本章的后面部分，我们将使用一个大语料库来开发一些很酷的东西，通过使用 word2vec 模型的概念。

# word2vec 的输出

这一节对于你的理解非常重要，因为在此之后，你所理解的一切都将是为了实现你在这里设定的输出。到目前为止，我们知道我们想要开发一个单词的向量表示，它承载单词的含义，并表达分布相似性测量。

现在，我将跳转到定义我们的目标和输出。我们想要定义一个模型，旨在预测一个中心词及其上下文中出现的词。因此，我们可以说我们想要预测给定词的上下文的概率。在这里，我们正在设置简单的预测目标。您可以通过参考以下图来理解这个目标：

![](img/6a5a1a02-fa68-4fe9-8eee-24a11697799f.png)

图 6.5：帮助理解我们的目标

如你所见，前面的图中给出了一些简单的例句。如果我们从第一句中取出单词**apple**，并根据我们的目标，将单词**apple**转换为向量形式，使得通过使用**apple**的向量形式，我们能够预测单词**eat**在单词**apple**上下文中出现的概率。相同的逻辑也适用于其他句子。例如，在第三个句子中，我们尝试找出单词**scooter**的向量，帮助我们预测像**driving**和**work**这样的单词在给定单词**scooter**的上下文中出现的概率。

因此，一般来说，我们的直接目标是将每个单词转换为向量形式，使得它们能够预测出在上下文中出现的单词，并且通过提供上下文，我们可以预测出最适合该上下文的单词的概率。

# word2vec 模型的构建组件

到目前为止，我们知道了输入和输出，因此你现在可能在想：我们如何通过我们的输入来实现目标呢？正如我之前提到的，我们需要一个聪明的算法来帮助我们实现目标。研究人员已经为我们做了这项工作，并得出结论：我们可以使用神经网络技术。我想给你一个简短的介绍，解释为什么我们要使用神经网络，但如果你想深入了解它，我建议你阅读一些这些论文。

[`wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html.`](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)

[`s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf`](http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf)[.](http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf)

[`blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf.`](http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)

我们之所以使用神经网络技术，是因为神经网络在从大量数据中学习时表现得非常优秀。如果你想构建一个简单、可扩展且易于训练的模型，那么神经网络是最佳的方法之一。如果你阅读我接下来列出的现代研究论文，它们会告诉你同样的真相。下面是这些论文的链接：

[`web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf`](http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf)

[`arxiv.org/abs/1301.3781`](https://arxiv.org/abs/1301.3781)

这种神经网络在生成分布式相似性方面创造了奇迹。Google 通过使用大量 Wikipedia 语料库来生成 word2vec 模型。请参考*图 6.6*，它给你展示了输入概况，以及 Google word2vec 模型的一些著名输出。对于我们来说，word2vec 模型仍然是一个强大的“黑盒”，能够生成一些很棒的结果。请看以下图中的 word2vec 黑盒表示：

![](img/d942cb9e-cdf8-469d-b4c4-2080447ba467.png)

图 6.6：Google word2vec 模型以 Wikipedia 文本作为输入并生成输出

上面的图像显示我们已经将文本数据作为输入提供给 word2vec 模型。word2vec 模型将我们的文本数据转换为向量形式，这样我们就可以对这些词汇的向量表示进行数学运算。word2vec 最著名的例子是：如果你有 king、man 和 woman 这三个词的向量。那么，如果你对 king 向量减去 man 向量，再加上 woman 向量，你将得到一个表示 queen 向量的结果。以下是这个例子的数学表示：*king - man + woman = queen*。

现在我们需要关注 word2vec 的架构组件概述。

# 架构组件

让我们来看看构建 word2vec 模型所涉及的架构组件。

word2vec 模型的主要架构组件是其神经网络。word2vec 模型的神经网络有两层，从这个意义上来说，它并不是一个深度神经网络。事实上，word2vec 并没有使用深度神经网络来生成词汇的向量表示。

这是 word2vec 模型中的一个关键组成部分，我们需要解码它的功能，才能清楚地了解 word2vec 是如何工作的。现在是解码 word2vec 模型神奇逻辑的时刻。

# 理解 word2vec 模型的逻辑

我们将开始分解 word2vec 模型，并尝试理解其逻辑。word2vec 是一款软件，使用了一堆算法。请参考*图 6.7*：

![](img/d49e251d-2ab4-4663-b447-9434f97416b2.png)

图 6.7：Word2vec 构建模块（图片来源：Xin Rong）

如*图 6.7*所示，主要有三个构建模块。我们将详细查看每一个模块：

+   词汇构建器

+   上下文构建器

+   两层神经网络

# 词汇构建器

词汇构建器是 word2vec 模型的第一个构建块。它接受原始文本数据，通常以句子的形式。词汇构建器用于从给定的文本语料库中构建词汇表。它会收集语料库中的所有唯一单词并构建词汇表。

在 Python 中，有一个名为`gensim`的库。我们将使用`gensim`为我们的语料库生成 word2vec。在`gensim`中有一些参数可以根据你的应用需求，用来从语料库中构建词汇表。以下是参数列表：

+   `min_count`：该参数用作阈值。它会忽略所有总频率低于指定值的单词。例如，如果你设置`min_count=5`，则词汇构建器的输出不会包含出现次数少于五次的单词。词汇构建器的输出只包含在语料库中出现五次或更多的单词。

+   `build_vocab(sentences, keep_raw_vocab=False, trim_rule=None, progress_per=10000, update=False)`：此语法用于从一系列句子（可以是一次性生成的流）中构建词汇表。每个句子必须是一个 Unicode 字符串的列表。

你可以点击这个链接，阅读更多关于其他参数的信息：[`radimrehurek.com/gensim/models/Word2vec.html.`](https://radimrehurek.com/gensim/models/Word2vec.html)

词汇表中的每个单词都与词汇对象相关联，该对象包含一个索引和一个计数。这是词汇构建器的输出。因此，你可以参考*图 6.8*，它帮助你理解词汇构建器的输入和输出：

![](img/6d986b99-ea28-413e-9356-a7533859046f.png)

图 6.8：词汇构建器的输入输出流程

# 上下文构建器

上下文构建器使用词汇构建器的输出，以及作为上下文窗口一部分的单词，作为输入并生成输出。

首先，让我们理解上下文窗口的概念。这个上下文窗口类似于一个滑动窗口。你可以根据将要使用 word2vec 的自然语言处理应用程序定义窗口的大小。通常，NLP 应用程序使用大小为五到十个单词的上下文窗口。如果你选择五个单词作为窗口大小，那么我们需要考虑中心单词左边的五个单词和右边的五个单词。通过这种方式，我们捕获了中心单词的所有周围单词的信息。

这里我想举一个例子，假设上下文窗口的大小为 1，因为我们有一个单句语料库。我有一句话：**I like deep learning,** 其中**deep**是中心词。那么接下来，你应该根据窗口大小考虑周围的词。因此，在这里，我需要考虑**like**和**learning**这两个词。在下一次迭代中，我们的中心词将是**learning**，它的周围词是**deep**，句子的结尾是一个**句点**（**.**）。

我希望你已经清晰理解了上下文窗口的概念。现在，我们需要将这个概念与上下文构建器如何使用这个概念以及词汇构建器的输出联系起来。词汇构建器对象包含单词的索引以及该单词在语料库中的频率。通过使用单词的索引，上下文构建器就能知道我们在查看哪个单词，并且根据上下文窗口的大小，它会考虑周围的其他单词。这些中心词和其他周围词将作为输入传递给上下文构建器。

现在你对上下文构建器的输入有了清晰的了解。让我们试着理解上下文构建器的输出。这个上下文构建器生成词对。参考*图 6.9*，了解词对的概念：

![](img/e49e6498-3f88-492d-b13d-cae42fbe2197.png)

图 6.9：理解词对

这些词对将被提供给神经网络。网络将根据每个词对出现的次数来学习基本统计信息。例如，神经网络可能会看到更多（deep, learning）这个词对的训练示例，而不是（deep, communication）。当训练完成后，如果你将**deep**作为输入，它将为 learning 或 network 输出更高的概率，而不是为 communication 输出。

所以这个词对是上下文构建器的输出，它将传递给下一个组件，即一个两层的神经网络。参考*图 6.10*，查看上下文构建器流程的概述：

![](img/2209d360-175f-4706-a845-f0399eb57340.png)

图 6.10：上下文构建器的输入和输出流

到目前为止，我们已经看到了 word2vec 构建块的两个主要组件。现在，我们的下一个关注点将是最终组件——神经网络。

# 两层神经网络

在本节中，我们将研究神经网络的输入和输出。除此之外，我们还将关注神经网络的结构部分，这将帮助我们了解单个神经元的样子，神经元的数量，激活函数是什么等等。那么，现在我们开始吧！

# word2vec 神经网络的结构细节

Word2vec 使用神经网络进行训练。所以对我们来说，理解神经网络的基本结构非常重要。神经网络的结构细节如下所示：

+   有一个输入层

+   第二层是隐藏层

+   第三层也是最后一层是输出层

# Word2vec 神经网络层的详细信息

正如我们所知，神经网络中有两层用于生成词向量。我们将开始详细查看每一层及其输入和输出。在这一部分中，我们不涉及 word2vec 模型背后的数学。稍后在本章中，我们将讨论 word2vec 背后的数学，并在那时让你知道如何映射你的点以便更好地理解。

让我们简要了解每一层的任务：

+   **输入层**：输入层的神经元数量与词汇表中的单词数量相同

+   **隐藏层**：隐藏层的大小以神经元数表示，是最终词向量的维度

+   **输出层**：输出层的神经元数量与输入层相同

第一个输入层的输入是采用独热编码的单词。假设我们的词汇大小为**V**，这意味着语料库中有**V**个不同的单词。在这种情况下，表示单词本身的位置信息编码为**1**，所有其他位置编码为**0**。请再次参考*图 6.4*以回顾独热编码的概念。假设这些单词的维度为**N**。因此，输入到隐藏层的连接可以由我们的输入矩阵**WI**（输入矩阵符号）表示，大小为*V * N*，其中矩阵**WI**的每一行代表一个词汇单词。同样，从隐藏层到输出层的连接意味着来自隐藏层的输出可以由隐藏层输出矩阵**WO**（隐藏层矩阵符号）描述。**WO**矩阵的大小为*N * V*。在这种情况下，**WO**矩阵的每一列代表给定词汇中的一个单词。请参考*图 6.11*以清晰地理解输入和输出。同时，我们还将通过一个简短的示例来理解这个概念：

![](img/3eb2989e-0e2b-4df7-b67c-c38e7e193d26.png)

图 6.11：双层神经网络输入和输出结构表示

现在让我们通过例子来讨论。我将以一个非常小的语料库为例。请看下面给出的我们小语料库中的句子：

+   the dog saw a cat

+   the dog chased a cat

+   the cat climbed a tree

上述三个句子中有八个（8）独特的单词。我们需要将它们按字母顺序排列，如果我们要访问它们，那么我们将参考每个单词的索引。请参考下面的表格：

| **Words** | **Index** |
| --- | --- |
| a | 1 |
| cat | 2 |
| chased | 3 |
| climbed | 4 |
| dog | 5 |
| saw | 6 |
| the | 7 |
| tree | 8 |

所以，在这里我们**V**的值等于**8**。在我们的神经网络中，我们需要八个输入神经元和八个输出神经元。现在假设我们在隐藏层中有三个（3）神经元。那么，在这种情况下，我们的**WI**和**WO**的值定义如下：

+   *WI = [V * N] = [8 * 3]*

+   *WO = [N * V] = [3 * 8]*

在训练开始之前，这些矩阵，**WI** 和 **WO**，通过使用小的随机值进行初始化，这在神经网络训练中非常常见。仅为说明目的，我们假设 **WI** 和 **WO** 初始化为以下值：

*WI =*

![](img/b40ddeaa-3a18-440c-9909-27d279c40262.png)

*WO =*

![](img/4de28604-a756-4429-b4da-c65dfdb7d747.png)

图片来源: https://iksinc.wordpress.com

我们的目标是让神经网络学习单词 **cat** 和 **climbed** 之间的关系。换句话说，我们可以解释为当单词 **cat** 输入到神经网络时，神经网络应为单词 **climbed** 提供较高的概率。因此，在词嵌入中，单词 **cat** 被称为上下文单词，而单词 **climbed** 被称为目标单词。

输入向量 *X* 表示单词 **cat**，它将是 *[0 1 0 0 0 0 0 0]t*。注意，向量的第二个分量是 *1*。之所以这样，是因为输入单词 **cat** 在语料库的排序列表中排在第二位。以同样的方式，目标单词是 **climbed**，目标向量为 **climbed** 的表示将是 *[0 0 0 1 0 0 0 0 ]t*。

第一层的输入是 *[0 1 0 0 0 0 0 0]t*。

隐藏层输出 *Ht* 通过以下公式计算：

*Ht = XtWI = [-0.490796 -0.229903 0.065460]*

从前面的计算中，我们可以看出，由于采用了独热编码表示，这里隐藏神经元的输出类似于 **WI** 矩阵第二行的权重。现在我们需要对隐藏层和输出层进行类似的计算。隐藏层和输出层的计算定义如下：

*HtWO = [0.100934 -0.309331 -0.122361 -0.151399 0.143463 -0.051262 -0.079686 0.112928]*

在这里，我们的最终目标是获取输出层中各个单词的概率。通过输出层，我们生成反映下一个单词与输入上下文单词关系的概率。因此，数学表示如下：

*概率 (wordk|wordcontext) 对于 k = 1...V*

在这里，我们讨论的是概率，但我们的输出是一个向量集合，因此我们需要将输出转换为概率。我们需要确保最终输出层神经元的输出之和等于 1。在 word2vec 中，我们通过使用 softmax 函数将输出层神经元的激活值转换为概率。

# Softmax 函数

在本节中，我们将讨论 softmax 函数。softmax 函数用于将输出向量转换为概率形式。我们使用 softmax 函数是因为我们希望将最后一层的输出转换为概率值，而 softmax 函数可以轻松地将向量值转换为概率值。这里，第*k*层神经元的输出将通过以下公式计算，其中 activation(*n*) 表示第*n*层输出神经元的激活值：

![](img/1f6ffc37-d899-4db6-8063-b8116a611312.png)

通过使用这个公式，我们可以计算语料库中八个单词的概率，概率值如下：

*[ 0.143073 0.094925 0.114441 0.111166 0.149289 0.122874 0.119431 0.144800 ]*

你一定在想，我是怎么得到这些概率值的。我使用了前面的 softmax 概率公式并生成了最终的概率向量。你可以在下面的代码片段中找到计算 softmax 函数的 Python 代码：

```py
import numpy as np 
def stablesoftmax(x): 
"""Compute the softmax of vector x in a numerically stable way.""" 
shiftx = x - np.max(x) 
exps = np.exp(shiftx) 
return exps / np.sum(exps) 
print stablesoftmax([0.100934,-0.309331,-0.122361,-0.151399, 0.143463,-0.051262,-0.079686, 0.112928]) 

```

给定的代码将生成以下输出向量：

```py
[ 0.143073   0.094925   0.114441   0.111166   0.149289   0.122874   0.119431   0.144800 ] 

```

如你所见，概率*0.111166*是为选定的目标词**climbed**计算的概率。我们知道，目标向量是 *[0 0 0 1 0 0 0 0]t*，因此我们可以通过预测来计算误差。为了生成预测误差或误差向量，我们需要从目标向量中减去概率向量，一旦我们知道了误差向量或误差值，就可以根据它调整权重。在这里，我们需要调整矩阵*WI*和*WO*的权重值。在网络中传播误差并重新调整*WI*和*WO*的权重值的技术被称为**反向传播**。

因此，训练可以通过从语料库中提取不同的上下文-目标词对来继续进行。这就是 word2vec 学习单词之间关系的方式，以便为语料库中的单词开发向量表示。

# 主要处理算法

Word2vec 有两个不同的版本。这些版本是 word2vec 的主要算法。请参见*图 6.12*：

![](img/010476eb-893d-4ca5-b3cd-251cb29f3f2f.png)

图 6.12：word2vec 的版本

在本节中，我们将介绍主要的两种处理算法。这些算法的名称如下：

+   连续词袋模型

+   Skip-gram

# 连续词袋模型

在**连续词袋模型**（**CBOW**）算法中，给定目标词时，上下文是由多个词来表示的。回想一下我们在前面部分提到的例子，其中上下文词是**cat**，目标词是**climbed**。例如，我们可以使用**cat**和**tree**作为上下文词来预测目标词**climbed**。在这种情况下，我们需要改变神经网络的结构，特别是输入层。现在，我们的输入层可能不再是单一词的 one-hot 编码向量，而需要增加另一个输入层来表示词**tree**。

如果你增加上下文词的数量，那么你需要添加一个额外的输入层来表示每个词，并且所有这些输入层都与隐藏层相连接。参考 *图 6.13*：

![](img/618b4974-58e4-4569-9ddd-bf8fd1588ad0.png)

图 6.13：CBOW 神经网络架构（图片来源： https://www.semanticscholar.org）

在这里，好的一点是计算公式保持不变；我们只需要计算其他上下文词的 *Ht*。

# 跳字模型

**skip-gram**（**SG**）模型颠倒了目标词和上下文词的使用。在这里，目标词作为输入以 one-hot 编码向量的形式输入到输入层。隐藏层保持不变。神经网络的输出层重复多次以生成所选数量的上下文词。

让我们以单词 **cat** 和 **tree** 作为上下文词，以单词 **climbed** 作为目标词为例。在 SG 模型中，输入向量将是单词 **climbed** 的 one-hot 编码词向量 *[0 0 0 1 0 0 0 0 ]t*，而这次，我们的输出向量应该是单词 **cat** 和单词 **tree** 的词向量。因此，输出向量应该是 **cat** 的 *[ 0 1 0 0 0 0 0 0]* 和 **tree** 的 *[0 0 0 0 0 0 0 1]*。参考 *图 6.14* 中的跳字模型结构：

![](img/61189b79-4a72-4e08-a4da-39260ed5896e.png)

图 6.14：跳字模型神经网络架构（图片来源： https://www.semanticscholar.org）

这次输出将不是一个单一的概率向量，而是两个不同的概率向量，因为我们有两个上下文词。在这里，误差向量将按照我们之前定义的方式计算。skip-gram 中的小变化是，所有输出层的误差向量会被加总，通过反向传播调整权重。这意味着我们需要确保每个输出层的矩阵 *WO* 的权重在训练过程中保持一致。

一些关于 word2vec 和其他技术背后的算法技术和数学将在下一节中解释。所以，准备好处理一些很酷的数学内容吧！

# 理解 word2vec 模型背后的算法技术和数学原理

本节非常重要，因为我们将在这里讨论在 word2vec 中使用的核心算法。到本节结束时，为了让你理解 word2vec 的概念，所有的秘密都将揭开。因此，本节将把 word2vec 的黑箱变成白箱。在这里，我还将包括数学部分，以便读者能够更好地理解核心概念。即使你不懂数学也不用担心，因为我将提供一些可能对你非常有用的资源。

# 理解 word2vec 算法的基本数学

首先，我们需要掌握一些基本的数学概念，以便更好地理解算法。我们所需的数学主题如下：

+   **向量**：向量具有大小和方向。因此，当我们在向量空间中绘制一个向量时，它不仅包含大小，还包含方向。你可以对向量执行基本的数学运算。

+   **矩阵**：矩阵是由数字或单词频率统计组成的网格。它具有行和列。我们可以通过计数矩阵所包含的行数和列数来定义矩阵的维度。你可以参考这个链接以获取更多关于矩阵的信息。

+   **偏导数**：如果一个函数包含多个变量，并且我们对该函数关于其中一个变量进行求导，并将其他变量保持不变，这就是偏导数的计算方法。偏导数在向量微积分中应用广泛。

+   **偏导数链式法则**：链式法则被定义为用于计算两个或更多函数复合的导数的公式。

现在让我们开始理解这些技巧。我已经将所有概念根据每个技巧使用的阶段，粗略分为三大类。

我列出了一些参考链接，你可以从中获得关于每个概念的深入理解。你可以访问这个链接：

[`www.khanacademy.org/math.`](https://www.khanacademy.org/math)

你可以参考以下链接来了解向量：

[`emweb.unl.edu/math/mathweb/vectors/vectors.html.`](http://emweb.unl.edu/math/mathweb/vectors/vectors.html)

[`www.mathsisfun.com/algebra/vectors.html.`](https://www.mathsisfun.com/algebra/vectors.html)

你可以参考这个链接以获取更多关于矩阵的信息：

[`medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c.`](https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c)

通过使用这个链接，你可以看到一些基础示例：

[`mathinsight.org/partial_derivative_examples.`](http://mathinsight.org/partial_derivative_examples)

[`www.analyzemath.com/calculus/multivariable/partial_derivatives.html.`](http://www.analyzemath.com/calculus/multivariable/partial_derivatives.html)

你可以参考以下链接来了解偏导数链式法则：

[`math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/chain/chain.html.`](https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/chain/chain.html)

[`www.youtube.com/watch?v=HOYA0-pOHsg.`](https://www.youtube.com/watch?v=HOYA0-pOHsg)

[`www.youtube.com/watch?v=aZcw1kN6B8Y.`](https://www.youtube.com/watch?v=aZcw1kN6B8Y)

上述列表已经足够帮助理解本章中算法背后的数学原理。

# 词汇构建阶段使用的技巧

在从数据集中生成词汇时，你可以使用一种优化技术，其中有损计数法是最常用于 word2vec 模型的技术。

# 有损计数

有损计数算法用于识别数据集中那些频率计数超过用户设定阈值的元素。该算法以数据流作为输入，而不是数据集的有限集合。

使用有损计数法，你会定期从频率表中移除计数非常低的元素。最常访问的词几乎不会有低频计数，哪怕它们有，通常也不会长时间停留在那里。

在这里，频率阈值通常由用户定义。当我们设置参数`min_count = 4`时，我们会移除在数据集中出现次数少于四次的词，并且不再考虑它们。

# 在词汇构建阶段使用它

有损计数法非常有用，特别是当你有一个大型语料库并且不想考虑那些出现非常罕见的词时。

在这种情况下，有损计数法非常有用，因为用户可以设置一个最小词频作为阈值，这样频率低于该阈值的词将不会被包含在我们的词汇中。

所以，如果你有一个大型语料库，并且想优化训练速度，那么我们可以使用这个算法。

换句话说，你可以说通过使用这个算法，你缩小了词汇量，从而可以加速训练过程。

# 应用

除了 word2vec 之外，有损计数算法还用于网络流量测量和 Web 服务器日志分析。

# 在构建上下文阶段使用的技术

在生成词对上下文时，上下文构建器使用以下技术：

+   动态窗口缩放或动态上下文窗口

+   子抽样

+   剪枝

# 动态窗口缩放

如你所见，动态窗口缩放是上下文构建器的一部分。我们将看到它如何有用，以及在使用它时会产生什么样的影响。动态窗口缩放也被称为**动态上下文窗口**。

# 理解动态上下文窗口技术

在 word2vec 实现中，动态上下文窗口是一种可选的技术，可以用来生成更精确的输出。你也可以将这些技术视为超参数。

动态上下文窗口技术使用权重方案来衡量上下文词与目标词之间的关系。

所以这里的直觉是，离目标词近的词比那些离目标词远的词更为重要。

让我们看看在构建词对时它如何发挥作用。动态上下文窗口考虑到附近的上下文单词对预测目标单词更为重要。在这里，我们通过对实际窗口大小（1 到 L 之间）使用均匀抽样的加权方案。举个例子，假设上下文窗口大小为 5，现在上下文单词的权重是均匀分布的，因此，最接近的单词权重为 5/5，紧接着的上下文单词权重为 4/5，依此类推。所以，最终的上下文单词权重将为 5/5、4/5、3/5、2/5、1/5。因此，通过加权，你可以微调最终结果。

# 子抽样

子抽样也是我们在构建词对时使用的技术之一，正如我们所知，这些词对是样本训练数据。

子抽样是去除最频繁单词的一种方法。这项技术对去除停用词非常有用。

这些技术还会随机删除单词，而这些随机选择的单词在语料库中出现得更频繁。因此，被删除的单词频率超过某个阈值 `t`，它们的出现概率为 `p`，其中 `f` 标记了单词的语料频率，在我们的实验中使用 *t = 10−5*。请参考 *图 6.15* 中给出的方程：

![](img/b79f0040-3022-4c81-bf10-b2979cdd8afc.png)

图 6.15：子抽样概率方程

这也作为一种有用的超参数发挥作用，它非常有用，因为我们在去除语料库和上下文窗口中最频繁且不必要的单词，从而提升了训练样本集的质量。

# 剪枝

剪枝在我们使用上下文构建器为训练目的构建词对时也会使用。当你需要处理大量词汇时，如果你包含了不太频繁的单词，就需要将它们去除。你还可以通过使用 Python `gensim` 库中的 `max_vocab_size` 参数来限制词汇表的大小。

让我们看看剪枝在生成 word2vec 时对我们有多大用处。剪枝用于修剪训练样本的大小，并提升其质量。如果你不从数据集中剪除那些很少出现的单词，那么模型的准确性可能会降低。这是一种提升准确性的技巧。

# 神经网络使用的算法

在这里，我们将看看单个神经元的结构。我们还将深入了解这两个算法的细节，从而理解 word2vec 如何从单词生成向量。

# 神经元的结构

我们已经看过整体的神经网络结构，但我们还没有看到每个神经元的构成以及神经元的结构。因此，在这一部分中，我们将查看每个单独输入神经元的结构。

我们将查看以下结构：

+   基本神经元结构

+   训练单个神经元

+   单个神经元的应用

+   多层神经网络

+   反向传播

+   word2vec 模型背后的数学

在这一部分，我们将大量包含数学公式。如果你没有数学背景，不用担心。我会用简单的语言给你解释，让你明白每一部分在做什么。

# 基本神经元结构

请参阅 *图 6.16*；它展示了基本的神经元结构：

![](img/f70b4079-ac1a-46ac-ab0e-d119336d8906.png)

图 6.16：基本神经元结构（图片来源：Xin Rong）

*图 6.16* 显示了一个基本的神经元结构。这个结构并不新颖。该结构接受输入，并且也有权重作为输入，它们计算加权和。在这里，*x1* 到 *xk* 是输入值，*w1* 到 *wk* 是对应的权重。因此，加权和通过 *图 6.17* 中给出的以下方程来表示：

![](img/cf4e0175-6266-422d-8ad7-a0c4a91b4e9c.png)

图 6.17：加权和方程

让我们通过一个例子来理解方程的使用。假设你有输入 *x=[1 5 6]* 和 *w=[0.5 0.2 0.1]*，则加权和 *u* 等于 *[1 * 0.5 + 5 * 0.2 + 6 * 0.1]*，所以我们的最终答案是 *u = [0.5 + 1 + 0.6] = 2.1*。这只是一个简单的例子，目的是给你一些关于给定方程实际运作的直观感受。以上是关于我们的输入。现在我们将讨论输出。

为了生成输出，我们可以从基本的神经元结构出发，得出我们的输出是加权和 *u* 的函数。在这里，*y* 是输出，*f(u)* 是加权和的函数。你可以看到在 *图 6.18* 中给出的方程：

![](img/382e56be-de82-4c74-80ea-51cc035412db.png)

图 6.18：输出 y 是加权和 u 的函数

在神经网络中，我们可以使用不同的可用函数，这些函数被称为 **激活函数**。这些函数如下所示：

+   阶跃函数

+   Sigmoid 函数

一些伟大的科学家表示，给定的函数是作为激活函数的不错选择。对于本章内容，我们不会深入探讨激活函数的细节，但我们将在 第九章，*自然语言处理与生成任务中的深度学习* 中详细讨论激活函数的所有细节。因此，我们将把这两个函数作为 word2vec 的激活函数。我们将使用阶跃函数或 sigmoid 函数中的一个，而不是同时使用这两个函数来开发 word2vec。请参阅 *图 6.19* 中的两个函数的方程：

![](img/a8c40266-ca20-4c0a-9f9e-778caeac164f.png)

图 6.19：激活函数，第一个是阶跃函数，第二个是 sigmoid 函数

在这里，*f(u)* 是阶跃函数，而 *f(u)* 也是 sigmoid 函数。

当我们画一个圆来表示神经元，类似于 *图 6.11* 中画的那样，这个圆包含了加权和和激活函数，这也是我们在 *图 6.16* 中表示虚线圆圈的原因。

在下一节中，我们将看到这些激活函数如何被使用，以及我们如何通过使用误差函数计算预测输出中的误差。我们将详细了解激活函数和误差函数的用法。所以让我们开始吧！

# 训练一个简单的神经元

现在是时候看看我们如何通过使用激活函数来训练单个神经元，并且让我们了解损失函数来计算预测输出中的误差。

主要思想被定义为误差函数，它实际上告诉我们预测中的错误程度；我们实际上试图使我们的误差值尽可能低。因此，换句话说，我们实际上正在尝试改进我们的预测。在训练过程中，我们使用输入并通过使用误差函数计算误差，并更新神经元的权重并重复我们的训练过程。我们将继续这个过程，直到我们得到最小误差率或最大、最好和准确的输出。

我们将要查看的两个最重要的概念如下所示：

+   定义误差函数（损失函数）

+   理解 word2vec 中的梯度下降

# 定义误差函数

在这里，我们的输入是向量`X`，词汇表`x1`到*xk*，我们的输出`y`是输出向量。因此，要计算误差`E`，我们需要定义误差函数或损失函数，我们使用的是 L2 损失函数。让我们从理解 L2 损失函数的基本概念开始，然后我们将看到它在 word2vec 中的用途。

在**机器学习**（**ML**）和**深度学习**（**DL**）中，有两种类型的损失函数被广泛使用。顺便说一下，我们将在即将到来的章节中查看 ML 和 DL，这些章节是*第八章*，*自然语言处理（NLP）问题的机器学习* 和 第九章，*自然语言处理（NLP）和自然语言生成（NLG）问题的深度学习*。有两种标准类型的损失函数，但在这里我们只关注**最小二乘误差**（**L2**），而在即将到来的章节中，这些章节是 第八章，*自然语言处理（NLP）问题的机器学习* 和 第九章，*自然语言处理（NLP）和自然语言生成（NLG）问题的深度学习*，我们将详细查看和比较这些误差函数。两种标准类型的损失函数分别是：

+   最小绝对偏差（L1）

+   最小二乘误差（L2）

最小二乘误差也被称为**L2 损失**函数。一般来说，损失函数希望在学习数据集时达到它们的最小化值，L2 损失函数也希望在误差值最小的情况下达到它们的值。因此，确切地说，L2 函数希望最小化估计值与现有目标值之间的平方差异。

在训练时单个神经元的结构如*图 6.20*所示：

![](img/f62cfc9f-068d-449c-9bb0-8307a712d33a.png)

图 6.20：训练时的单个神经元

所以，当我们尝试计算单个神经元的 L2 损失函数时，我们将使用 *图 6.21* 中给出的以下方程：

![](img/a6fa6d6a-7fa3-4359-8b3f-def68caff86e.png)

图 6.21：L2 损失函数方程（最小二乘误差函数）

这里，*t* 是目标向量值，*y* 是估计的向量值或预测的向量值，*E* 是误差函数。我们已经定义了我们的 L2 误差函数。我知道你一定很想知道我们将如何使用这个 L2 误差函数来得到最小的误差值，接下来我们需要理解梯度下降的概念。

# 理解 word2vec 中的梯度下降

现在，让我们理解一下我们将如何使用 L2 函数，以及它如何在实现准确输出中发挥作用。

正如我们之前所说，我们希望最小化这个函数值，以便准确预测目标值。为此，我们将对 *图 6.21* 中给出的 L2 函数方程对 *y* 进行偏导数。推导偏导数的过程，并通过使用这些推导来最小化误差值的过程，被称为 **梯度下降**。

在这种情况下，结果如 *图 6.22* 所示：

![](img/01aba3d0-bbfd-46d9-a936-b7b2c64a2c36.png)

图 6.22：关于 *y* 的 L2 损失函数偏导数结果

我们知道输出 *y* 依赖于 *f(u)*，而 *f(u)* 又依赖于输入权重值 *wi*。因此，我们需要应用链式法则来生成误差函数值。如果我们使用偏导数链式法则，我们将得到以下方程，这将在 word2vec 中非常有用。*图 6.23* 展示了偏导数链式法则的结果：

![](img/fe9277c1-6873-4076-8ba3-dd667802265c.png)

图 6.23：L2 误差函数的偏导数链式法则结果

在根据误差值计算 L2 损失函数之后，神经网络的输入权重将被更新，这种迭代过程将持续进行，直到我们达到最小的误差值或误差率。

到目前为止，我们一直在推导单个神经元的方程，因此了解这个单个神经元能为我们做什么将非常重要。这是我们接下来要讨论的重点。

# 单个神经元应用

我们已经学到了很多关于单个神经元的技术和数学内容，所以我真的想带你一起了解在 word2vec 模型中单个神经元的应用。让我们开始吧！

如果你想构建一个模型来识别哪些词是可食用的，哪些是不可食用的，那么我们可以使用单个神经元来构建该模型。这种应用程序将词汇分为可食用类别或不可食用类别，称为**二元分类****任务**。对于这种任务，神经元会接受 one-hot 编码的向量作为输入，单个神经元将学习哪些词与可食用物品相关，哪些与可食用物品无关。因此，它将生成一个查找表，如你在*图 6.24*中看到的那样：

![](img/8781ddad-2f2e-4aea-9849-e68108aeb524.png)

图 6.24：单个神经元可以将词汇分类为可食用和不可食用类别

之所以能够如此轻松地构建这种应用，是因为当你不断提供 one-hot 词向量并使用 sigmoid 或阶跃函数作为激活函数时，它就变成了一个标准的分类问题，而这种问题可以通过数学方法轻松解决。我们在前面的部分中已经定义了这一点，因为对于可食用物品，输出向量具有相同的值，而对于不可食用物品，生成的向量代表相同类型的输出向量。最终，我们将能够构建查找表。这种标准的分类问题让我们想起了逻辑回归，我们在应用相同的逻辑回归概念，但这里我们使用的是单个神经元结构。

我们已经了解了足够的单个神经元结构，现在是时候探索多层神经网络了。接下来的部分将为你提供有关多层神经网络的信息。

# 多层神经网络

多层神经网络是我们在使用 word2vec 时采用的结构。该函数以 one-hot 编码的词向量作为输入，这个向量以及加权和会传递到隐藏层。通过使用激活函数（在这种情况下是 sigmoid 函数），从隐藏层生成输出，并将该输出传递到下一层，即输出层。我们在本章的*具有两层的神经网络*部分已经看过一个示例。在该部分，我们没有深入探讨数学方面的内容，因此在这里我将带你了解神经网络的数学原理。

所以，让我们通过数学方程式来表示我在前一段中告诉你的内容。参见*图 6.25*中的多层神经网络结构：

![](img/0c11fdd7-ce94-4e85-9743-d7a96834cd2a.png)

图 6.25：多层神经网络

现在，让我们看看给定神经网络的数学方程式。在这里，我将为你提供高层次的直觉，这将帮助你理解流程，并让你了解输入和输出函数的概念。请参阅*图 6.26*：

![](img/9c362c99-f5b6-4fa5-b1e6-875ac3b76707.png)

图 6.26：多层神经网络的数学方程式

输入和输出的流动如下所示：

+   *图 6.24*中的第一个方程是输入层的加权和，输入层的结果将传递到隐藏层。*ui*是输入层的加权和。隐藏层的激活函数由第二个方程给出。激活函数*hi*使用 Sigmoid 函数并生成中间输出。

+   隐藏层的加权和将传递到输出层，第三个方程显示了隐藏层加权和的计算。*u'j*是来自隐藏层的加权和，它将传递到输出层。*yj*使用来自隐藏层的加权和，即*u'j*，因为这里的激活函数也是 Sigmoid。

我们已经通过基本的数学表示法看到了输入和输出的流动。

现在，一个主要的问题是如何将这个结构用于训练 word2vec 模型，答案是，我们使用反向传播来训练模型，这将在下一节中讨论。

# 反向传播

我们已经看到如何使用 L2 损失函数计算误差，L2 损失函数旨在最小化估计值与真实目标值之间的平方差。我们将相同的概念应用于多层神经网络。因此，我们需要定义损失函数，并对其进行求导，以更新神经网络的权重，从而生成最小误差值。在这里，我们的输入和输出是向量。

请参考*图 6.27*查看神经网络结构，*图 6.28*展示了我们需要应用的方程来计算多层神经网络中的误差函数：

![](img/95ea54bf-11b2-4027-b3de-19a570a01aa1.png)

图 6.27：用于计算误差函数的多层神经网络

现在让我们来看一下神经网络进行推导和数学计算的过程。你可以看到方程如下所示，请参考*图 6.28*：

![](img/13b778d6-f544-46b4-8401-df6697c5e815.png)

图 6.28：计算多层神经网络误差函数的方程

当你计算多层神经网络的误差函数时，你需要非常小心索引以及你正在计算哪个层的误差函数值。如*图 6.28*所示，我们将从输出层索引开始，反向传播误差到隐藏层，并更新权重。在第五个方程中，你可以看到我们需要计算输出层的误差函数，以便反向传播误差并更新输入层的权重。处理索引在多层神经网络中是一项具有挑战性的任务。但编码实现起来相当简单，因为你只需要写一个`for`循环来计算每一层的梯度。

现在，我们将把 word2vec 的所有数学方程和概念结合在一起，理解 word2vec 神经网络的最终数学部分。

# word2vec 模型背后的数学原理

在本节中，我们将通过将所有前面的方程和概念结合起来，来看最后一部分数学内容，并以概率的形式推导最终方程。我们已经在前一节中看到了概念、基本直觉、计算和示例，*Word2vec 神经网络层的详细信息*。

word2vec 神经网络使用一个独热编码的词向量作为输入，然后将该向量值传递给下一层，即隐藏层，这实际上是加权和值，它作为输入传入隐藏层。最后的输出层生成词向量值，但为了理解输出，我们会将该向量转换为概率格式，借助 softmax 技术，我们还将输出词向量转换为概率格式。我们将在接下来的章节中看到用于从输出向量生成概率的不同技术，在那之前，只需将 softmax 视为一个神奇函数。

请参考 *图 6.29* 以理解 word2vec 神经网络背后的数学原理：

![](img/9eb90f9d-6eb6-4ec1-b6c2-07b13b3ad9e0.png)

图 6.29：word2vec 模型背后的数学

在第一个方程中，我们可以看到输入词向量与权重的加权和，并在第二个方程中得到 *h*。我们将 *h* 与隐藏层词向量的加权和 *v'wj* 相乘。这里，权重和索引已经发生了变化。这次乘法得到了 *uj*。这里，*uj* 是激活函数。然后，我们将使用 *uj* 的值生成概率。因此，最终的方程就是 softmax 函数。让我们通过将第三个方程中的 *uj* 替换为输入和输出词向量格式来简化方程，最终得到最终方程。请参考 *图 6.30*：

![](img/f807cd33-cb1a-4344-a7c6-b0f8f21a41cf.png)

图 6.30：word2vec 模型的最终概率方程

这次，我们的输出是一个 softmax 函数，因此为了使用反向传播更新权重，我们需要定义损失函数。因此，我们将在这里定义一个以 softmax 函数形式的损失函数，我们将使用 softmax 函数的负对数概率，然后执行梯度下降。请参见 *图 6.31*：

![](img/c97abf5f-5208-4dcf-988b-3722bd1de0ac.png)

图 6.31：误差函数梯度下降形式的 softmax 函数负对数概率

在这里，我想大致说明输出向量值是如何被更新的。这是输出层的更新规则。你可以找到以下给出的方程。请参考 *图 6.32*：

![](img/a838de75-1d19-4140-ad57-d2ee51996bed.png)

图 6.32：更新输出向量的规则

在这个公式中，我们取原始输出向量，并减去输出节点的预测误差*ej*，*h* 是隐藏层的值。所以，这个公式的意思是，如果我们将单词**climbed**作为输入，并希望预测单词**cat**作为输出，那么我们如何更新单词**climbed**的向量，使其与单词**cat**的向量更加相似？简单来说，我们可以说，我们将单词**climbed**的部分向量加到单词**cat**的向量上，除此之外，我们还需要更新其他单词的输出向量，因为所有不作为目标词的单词也应更新其输出向量，使它们与目标词的相似度更低。

更新输入向量的规则也非常有用；更新输入向量的公式如下所示。请参阅*图 6.33*：

![](img/06d1f10f-cf58-46c8-8604-e7f34ef5a9fd.png)

图 6.33：更新输入向量的规则

这个公式有点复杂。所以，这里，直觉是输入向量，它将从预测误差的加权和中减去。意思是，这次我们要更新输入向量**cat**。我们将以一种方式更新单词**cat**的向量，使其接近单词**climbed**的向量。在这里，词语的共现起着重要作用。

我们几乎完成了 word2vec 模型的数学部分。我们已经看到很多在 word2vec 模型中使用的数学公式。现在我们将讨论用于生成最终向量和预测概率的技术。

# 用于生成最终向量和概率预测阶段的技术

在这一部分，我们将看到如何生成最终的向量。我们还将使用一些启发式方法来高效地生成输出。所以，我们也会讨论这些启发式方法。

如我们之前所见，为了生成词向量，我们需要更新输入向量以及输出向量。假设我们词汇表中有一百万个词，那么更新输入和输出向量的过程将会非常耗时且低效。我们必须解决这个挑战。因此，我们采用一些优化方法来执行相同的操作，这些技术如下所示：

+   层次化 softmax

+   负采样

那么，让我们开始了解这些技术。

# 层次化 softmax

在层次化 softmax 中，我们不将每个输出向量映射到其对应的词，而是将输出向量视为二叉树的形式。请参阅*图 6.34*中的层次化 softmax 结构：

![](img/98a86731-7bb0-4125-90d2-d12252c8907d.png)

图 6.34：层次化 Softmax 结构

所以，在这里，输出向量并不是预测某个词的概率，而是在预测你在二叉树中要走的路径。也就是说，你要么访问这一分支，要么访问另一个分支。参见*图 6.35*：

![](img/2fccd091-58bd-422a-bfda-654878270ec9.png)

图 6.35：使用层次化 softmax 外壳表示的预测路径

在这种情况下，考虑红色激活点向上（此处为浅灰色）和蓝色激活点向下（此处为深灰色），因此你可以看到，在这里，我们可以以较高的概率预测词**juice**。

在这里，优点是，如果你想反向传播一个错误，那么你只需要更新一个输出向量，并且错误将仅传播到在预测时被激活的三个节点。我们使用霍夫曼二叉树构建来生成二叉树。

# 负采样

负采样也是一种优化方法。在这种方法中，我们将更新输出词的向量，但不会更新其他所有词的向量。我们只从输出向量以外的词中取样。因此，我们从负样本集中的词中选择一个样本，这就是该技术被称为负采样的原因。

# 一些与 word2vec 相关的事实

这里有一些使用 word2vec 时你应该记住的事实：

+   到目前为止，你应该已经意识到，word2vec 使用神经网络，而这个神经网络并不是深度神经网络。它只有两层，但它非常有效地找出词语的相似性。

+   word2vec 神经网络使用简单的逻辑激活函数，不使用非线性函数。

+   隐藏层的激活函数是简单的线性函数，因为它直接将输入的加权和传递给下一层。

现在，我们已经看到了 word2vec 的几乎所有主要方面，接下来的部分我们将研究 word2vec 的应用。

# word2vec 的应用

让我介绍一些实际应用，其中使用了 word2vec。它们包括：

+   依赖解析器在解析时使用 word2vec 生成更好、更准确的词语之间的依赖关系。

+   命名实体识别也可以使用 word2vec，因为 word2vec 在**命名实体识别**（**NER**）中非常擅长发现相似性。所有相似的实体可以聚集在一起，从而获得更好的结果。

+   情感分析利用它来保持语义相似性，从而生成更好的情感结果。语义相似性帮助我们了解人们用来表达意见的词组或单词，运用 word2vec 概念进行情感分析时，你可以生成更好的洞察力和准确性。

+   我们还可以构建一个通过使用写作风格预测一个人名字的应用。

+   如果你想用简单的统计方法高精度地进行文档分类，那么 word2vec 就是你需要的工具。你可以使用这个概念来对文档进行分类，而不需要任何人工标签。

+   单词聚类是 word2vec 的基本产品。所有具有相似含义的单词都会被聚集在一起。

+   Google 使用 word2vec 和深度学习来提升他们的机器翻译产品。

有许多应用场景可以使用 word2vec 的概念。在这里，我们将实现一些有趣的例子。我们将构建有趣的应用程序，并对其进行可视化，这样你就能以更好的方式理解这个概念。

# 简单例子的实现

在这一部分，我们将实现著名的 word2vec 示例，即将 `woman` 和 `king` 相加，`man` 相减，最终得到的向量表示 `queen`。

我们不会在我们的数据上训练 word2vec 模型，然后构建自己的 word2vec 模型，因为 Google 已经在大量数据上训练了他们的 word2vec 模型，并为我们提供了预训练模型。如果你想在这么多数据上复制训练过程，那么我们需要大量的计算资源，所以我们将使用 Google 提供的预训练 word2vec 模型。你可以从这个链接下载预训练模型：[`code.google.com/archive/p/Word2vec/`](https://code.google.com/archive/p/Word2vec/)。

点击此链接后，你需要进入标题为“预训练词和短语向量”的部分，下载名为 `GoogleNews-vectors-negative300.bin.gz` 的模型，并将其解压。

我们将使用 `genism` 库来构建我们著名的示例。

# 著名示例（king - man + woman）

我们将通过使用 `gensim` 库加载二进制模型并复制这个例子。如果你在自己的计算机上运行它，可能需要几分钟时间，所以不用担心，保持脚本运行即可。参考*图 6.36*中的代码片段：

![](img/03d8ba9f-98d8-416c-ae56-1706cd536ff0.png)

图 6.36：示例代码片段 King - man + woman = queen

你可以通过点击这个 GitHub 链接查看代码：[`github.com/jalajthanaki/NLPython/blob/master/ch6/kingqueenexample.py`](https://github.com/jalajthanaki/NLPython/blob/master/ch6/kingqueenexample.py)

你可以参考*图 6.37*查看我们正在生成的输出：

![](img/454a681c-4042-41be-9c43-6d9afd811c74.png)

图 6.37：示例输出 King - man + woman = queen

现在，如果你想使用 Google 提供的数据从头开始训练模型，可以通过以下链接下载训练数据集：[`code.google.com/archive/p/Word2vec/`](https://code.google.com/archive/p/Word2vec/)。进入名为 *Where to obtain the training data* 的部分并下载所有训练数据集，然后参考给出的 GitHub 链接 [`github.com/LasseRegin/gensim-Word2vec-model`](https://github.com/LasseRegin/gensim-Word2vec-model)，你可以复制整个训练过程，但这将需要很长时间，因为这种训练需要大量的计算能力。

# word2vec 的优点

正如我们所看到的，word2vec 是一个非常好的生成分布式相似度的技术。它还有其他优点，我在这里列出了：

+   word2vec 的概念非常容易理解。它们并不复杂，以至于你根本不知道背后发生了什么。

+   使用 word2vec 很简单，它有非常强大的架构。与其他技术相比，它的训练速度较快。

+   训练过程中的人力投入非常少，因为这里不需要人工标注的数据。

+   该技术适用于小规模数据集和大规模数据集。因此，它是一个易于扩展的模型。

+   一旦你理解了概念和算法，你也可以将整个概念和算法应用到你的数据集上。

+   它在捕捉语义相似性方面表现得非常出色。

+   由于这是一种无监督方法，人力投入非常少，因此它是一种节省时间的方法。

# word2vec 的挑战

尽管 word2vec 概念非常高效，但也有一些你可能会觉得复杂或具有挑战性的地方。在这里，我将提出一些最常见的挑战。这些挑战列举如下：

+   word2vec 模型容易开发，但调试起来比较困难，因此调试能力是你在为数据集开发 word2vec 模型时面临的主要挑战之一。

+   它无法处理歧义。因此，如果一个词有多个含义，在现实世界中我们可以找到很多这种词汇，那么在这种情况下，嵌入将会在向量空间中反映这些含义的平均值。

# word2vec 如何在现实应用中使用？

本节将让你了解哪些类型的 NLP 应用程序使用 word2vec，以及 NLP 应用程序如何使用这一概念。除此之外，我还将讨论一些社区中最常见的问题，帮助你在实际使用 word2vec 时有更清晰的认识。

自然语言处理（NLP）应用程序，如文档分类、情感分析等，都可以使用 word2vec 技术。特别是在文档分类中，word2vec 实现能为你提供更好的结果，因为它保持了语义相似性。

对于情感分析，我们可以应用 word2vec，它能让你了解单词如何分布在数据集中，然后你可以使用自定义参数，如上下文窗口大小、子抽样等。你应该首先生成 **词袋（BOW）**，然后开始在该词袋上训练 word2vec，生成词向量。这些向量可以作为输入特征输入到机器学习算法中，从而生成情感分析的输出。

现在，讨论一下人们在尝试理解和使用 word2vec 技术处理自己数据集时常常提出的一些问题。

现在，让我们开始回答问题吧！

+   **我们需要什么样的语料库？**：word2vec 技术可以应用于文本数据集。因此，并没有什么特定的文本数据类型不能使用。因此，按照我的看法，你可以在任何数据集上应用 word2vec。

+   **我应该始终去除停用词吗？**：在谷歌的原始 word2vec 模型中，去除了一些停用词，例如 **a** 被从 word2vec 中移除，但单词 **the** 没有被移除。因此，并不是强制要求你去除这些单词：

    +   这完全取决于你的自然语言处理（NLP）应用。如果你正在开发情感分析应用，那么你可以去除所有停用词，但如果你在开发机器翻译应用，那么你应该去除一些停用词，而不是全部。如果你使用 word2vec 来开发词语聚类以理解语言的语法，那么你不应去除任何词语。

+   **我应该去除所有停用词吗？**：这个问题与前一个问题相关。对此问题的直接回答是否定的。并不是每个 NLP 应用都强制要求盲目去除所有停用词。每个 NLP 应用都是不同的，因此你应该根据你正在构建的 NLP 应用来做决定：

    +   如果你查看谷歌的原始 word2vec 模型，你会看到在该模型中，单词 **a** 不在其中，这意味着表示单词 **a** 的向量不存在，但表示单词 **the** 的向量存在。

+   我们将加载原始的 Google word2vec 模型，并通过简单的代码行，查看一些关于停用词的事实。

    请参考 *图 6.38* 中的代码片段：

![](img/ca5ab527-617d-4bd8-bd77-86f99f20ae63.png)

图 6.38：展示停用词事实的代码片段

对于 `the` 的向量值输出，请参考 *图 6.39*：

![](img/dc1bd72a-bcd4-4373-9e52-65f0c300ced5.png)

图 6.39：单词 “the”的词向量示例值

查看输出，你可以看到 word2vec 的词汇表中不包含 `a`，如 *图 6.40* 所示：

![](img/ae68391b-9340-46cf-bdd9-0a5fd97f9a03.png)

图 6.40：word2vec 不包含单词 a

+   **你不觉得我们在这里为每个单词生成了两个向量吗？**：我想告诉你，我们确实为每个单词生成了两个向量。这样做的原因是，句子中的单词既是目标单词也是上下文单词，所以当一个单词作为目标单词出现时，我们会生成向量；当一个单词作为上下文单词出现时，我们也会生成向量。我们在最终输出中考虑目标单词的向量，但当然，你也可以使用这两个向量。如何使用这两个向量并从中产生有意义的结果，这真的是个百万美元的问题！

# 你应该在什么情况下使用 word2vec？

Word2vec 捕捉语义相似性；这是我们在处理前一个问题的答案时需要牢记的最重要的一点。

如果你有一个 NLP 应用，想要使用分布式语义，那么 word2vec 就是为你准备的！一些 NLP 应用将使用这个概念从 word2vec 模型生成特征和输出向量，或者类似地，向量将作为 ML 算法的输入特征使用。

你应该知道哪些 NLP 应用可以使用 word2vec。如果你了解应用列表，那么你就可以轻松决定是否应该使用它。例如，你可以使用 k-均值聚类进行文档分类；如果你希望文档分类能够携带一些语义属性，那么你也可以使用 word2vec。如果你想构建一个问答系统，那么你需要能够在语义层面区分问题的技术。由于我们需要一些语义层次的信息，我们可以使用 word2vec。

现在，我们已经了解了足够的概念和理论，所以我们将开始我们最喜欢的部分——编程，而这次真的很有趣。

# 开发一些有趣的东西

在这里，我们将训练我们的 word2vec 模型。我将使用的数据集是 *权力的游戏* 的文本数据。因此，我们的正式目标是开发 word2vec 来探索 *冰与火之歌（来自《权力的游戏》）* 中实体之间的语义相似性。好的部分是，我们还会在此基础上进行可视化，以更好地从实际操作中理解这个概念。原始代码的作者是 Yuriy Guts。我只是创建了一个代码封装器以便更好地理解。

我使用了 IPython notebook。基本的依赖包有 `gensim`、`scikit-learn` 和 `nltk`，用于在《权力的游戏》的文本数据上训练 word2vec 模型。你可以在这个 GitHub 链接上找到代码：

[`github.com/jalajthanaki/NLPython/blob/master/ch6/gameofthrones2vec/gameofthrones2vec.ipynb.`](https://github.com/jalajthanaki/NLPython/blob/master/ch6/gameofthrones2vec/gameofthrones2vec.ipynb)

代码包含内联注释，你可以看到输出的代码片段。我们使用了 t-SNE 技术来减少词向量的维度，从而可以使用二维向量进行可视化。如果你想在内存为 2 到 4 GB 的普通计算机上运行 t-SNE，可能会花费较长时间。因此，为了成功运行 t-SNE 代码，你需要更多的内存，并且如果你的计算机内存有限，你可以跳过可视化部分。你可以查看可视化图像。一旦你将模型保存到磁盘上，就可以轻松使用它并生成输出。我已经在*图 6.41*到*图 6.45*中给出了示例输出。

你可以观察到这个词`Stark`的输出结果：

![](img/05b7c3fb-7cec-436a-a67d-d7dda024dec2.png)

图 6.41：词汇相似度输出（对于单词 stark）

这是最近的词汇输出：

![](img/96971b2d-b210-4ef6-81c4-b1bc30f7c0f8.png)

图 6.42：最近词汇的输出

现在我们将查看以下可视化输出图像：

![](img/18c6afd3-33d5-4c41-a2eb-0c1430939c6b.png)

图 6.43：使用 t-SNE 后，你可以在二维空间中可视化向量

现在我们将放大并尝试查看哪些词汇最终聚集在一起。

请查看以下图像，展示与 Kingsguard 相关的人物最终聚集在一起：

![](img/df02963d-e682-4191-9622-d0140e0dce9e.png)

图 6.44：人名分组在一起

请查看以下图像，展示了食品产品被很好地分组在一起：

![](img/9d64d433-19ce-4ed0-8cb7-62fbc3b423d9.png)

图 6.45：食品名称分组在一起

# 练习

我是哈利·波特的超级粉丝，因此在这个练习中，你需要从《哈利·波特》书中的文本数据生成 Word2vec。别担心数据集；我已经为你提供了，且它存储在这个 GitHub 链接：[`github.com/jalajthanaki/NLPython/tree/master/ch6/Harrypotter2vec/HPdataset`](https://github.com/jalajthanaki/NLPython/tree/master/ch6/Harrypotter2vec/HPdataset)

祝你好运，生成 HarryPotter2Vec！编码愉快！

# word2vec 概念的扩展

word2vec 概念可以扩展到不同层次的文本。这一概念可以应用于段落级别或文档级别，除此之外，你还可以生成全球向量，这就是**GloVe**。我们将尝试理解这些概念。在这里，我们将对每个概念进行概述。

以下是使用 word2vec 概念构建的扩展概念：

+   Para2vec

+   Doc2vec

+   GloVe

# Para2Vec

Para2vec 代表段落向量。段落向量是一种无监督算法，使用固定长度的特征表示。它通过从可变长度的文本片段（如句子、段落和文档）中派生出这种特征表示。

Para2vec 可以通过使用神经网络推导出来。大部分方面与 Word2vec 相同。通常，会考虑三个上下文词并将其输入到神经网络中。然后，神经网络会尝试预测第四个上下文词。在这里，我们尝试最大化对数概率，预测任务通常通过多类分类器来执行。我们使用的函数是 softmax。

请注意，在这里，上下文是固定长度的，通过在段落上使用滑动窗口生成上下文词。段落向量与从同一段落生成的所有上下文共享，但不会跨段落共享。

Para2vec 的优势在于能够从未标记的数据中学习预测单词，这样你就可以在没有足够标记数据集的情况下使用这些技术。

# Doc2Vec

**Doc2vec**（**文档向量**）是 word2vec 的扩展。它学习关联文档标签和单词，而不是单词之间的关系。在这里，你需要文档标签。你能够使用固定长度的向量表示整个句子。这也是使用了 word2vec 的概念。如果你将带标签的句子输入神经网络，它就会在给定的数据集上执行分类任务。所以，简而言之，你给文本打标签，然后使用这个标记的数据集作为输入，并在这个数据集上应用 Doc2vec 技术。该算法将为给定文本生成标签向量。你可以在这个 GitHub 链接中找到代码：[`github.com/jalajthanaki/NLPython/blob/master/ch6/doc2vecexample.py`](https://github.com/jalajthanaki/NLPython/blob/master/ch6/doc2vecexample.py)

我使用了非常小的数据集，仅仅是为了让你对如何开发 Doc2vec 有一个直观的了解，因此我忽略了开发模型的准确性因素。你可以参考这个参考链接中给出的代码：[`github.com/jhlau/doc2vec`](https://github.com/jhlau/doc2vec)

让我们看看 *图 6.46* 中的直观代码，并查看 *图 6.47* 中的输出片段：

![](img/0277849e-707b-47fd-9eec-ccc13dba224b.png)

图 6.47：doc2vec 的代码片段

你可能会看到以下输出：

![](img/864bb3b8-8e0f-40ff-a903-1c4ce4e0f9c3.png)

图 6.48：示例输出

# Doc2vec 的应用

让我们看看可以使用 Doc2vec 的应用场景：

+   文档聚类可以通过使用 Doc2vec 简单实现

+   我们可以对更大块的文本数据进行情感分析，我想你也可以考虑对一个非常大的文本块进行处理，并为这个大块生成情感输出

+   它还用于产品推荐

# GloVe

GloVe 代表全局向量。GloVe 是一种无监督学习算法。这个算法生成单词的向量表示。在这里，训练是通过使用聚合的全局单词-单词共现矩阵和语料库中的其他统计信息来执行的，结果表示给你提供了单词向量空间中有趣的线性子结构。所以，共现矩阵是 GloVe 的输入。

GloVe 使用余弦相似度或欧氏距离来估计相似的单词。GloVe 给你带来新颖的视角，并证明了如果你取最近邻，你可以看到这些单词，它们在频繁使用上是非常罕见的。即使如此，GloVe 仍然能够将这些罕见的单词捕捉到相似的聚类中。让我们看一个著名的例子：

例如，当我们使用目标单词“青蛙”时，以下是最接近的单词：

+   青蛙

+   青蛙

+   蟾蜍

+   Litoria

+   Leptodactylidae

+   Rana

+   蜥蜴

+   Eleutherodactylus

另一个例子是与比较级和最高级形式相关的单词聚集在一起，如果你使用可视化工具，可以看到以下输出。参见*图 6.48*：

![](img/34816d1b-7ecb-4a29-af52-0e6d17b67f4c.png)

图 6.48：GloVe 著名示例的结果

我正在使用`GloVe` Python 库为你提供一个直观的 GloVe 实际例子。请参见以下 GitHub 链接中的代码：[`github.com/jalajthanaki/NLPython/blob/master/ch6/gloveexample.py`](https://github.com/jalajthanaki/NLPython/blob/master/ch6/gloveexample.py)

在我们开始之前，需要下载数据集，请执行以下命令：

```py
    wget http://mattmahoney.net/dc/text8.zip -P /tmp
    unzip text8.zip
    cd /tmp
    sudo chmod 777 ./text8
    sudo chown yoursystemusername:yoursystemusername ./text8

```

你可以在*图 6.49*中看到代码片段，并在*图 6.50*中看到输出：

![](img/b5789625-456b-47e2-82bc-ae3d9990f95b.png)

图 6.49：GloVe 代码片段

以下是前面代码片段的输出：

![](img/41328e38-e1ee-47b4-98af-9d1b828a1494.png)

图 6.50：GloVe 的示例输出

# 练习

这个练习更像是一个阅读练习。你应该阅读关于 Para2vec、Doc2vec 和 GloVe 的研究论文。除此之外，你还可以检查是否有任何方法可以为连续字符串（例如 DNA 模式）找到向量表示。这个练习的主要目的是让你了解研究工作是如何完成的。你也可以思考一些向量表示的其他方面，并尝试解决其中的挑战。

# 向量化在深度学习中的重要性

这更像是我与你的一次讨论。众所周知，计算机无法直接理解自然语言（NL），因此我们需要将自然语言输出转换为数值格式。我们有多种词嵌入技术，以及一些基本的统计技术，如索引、tf-idf、独热编码等。通过使用所有这些技术或其中的一些技术，你可以将文本输入转换为数值格式。你选择哪些技术完全取决于自然语言处理（NLP）应用。因此，转换自然语言输入为数值格式的背后有两个主要原因。基本上，这是因为计算机只能理解数值数据，所以我们必须将文本数据转换为数值数据，而计算机擅长对给定的数值数据进行计算。这是我在转换文本数据时想到的两个主要原因。

让我们了解一下什么是深度学习。在这里，我想给你一个简要的概念。不用担心；我们将在第九章《用于 NLU 和 NLG 问题的深度学习》中详细讨论。当一个神经网络有很多层时，我们称之为**深度神经网络**。当我们使用多层深度神经网络，并利用它们通过大量数据和计算能力开发 NLP 应用时，这就是**深度学习**。

现在让我们来谈谈向量化。向量化是一个坚实的数学概念，易于理解和处理。如今，Python 有很多优秀的库，当我们想处理高维度向量形式的数据时，它们让我们的生活变得更加轻松。深度学习范式在很大程度上依赖于向量和矩阵的概念，所以为了深入掌握深度学习，你应该了解向量和矩阵。处理视频或音频等输入数据的深度学习应用程序也使用向量。视频和图像被转换为稠密的向量格式，而当我们谈论文本输入时，word2vec 是生成单词向量的基本构建块。Google TensorFlow 将 word2vec 作为其基本构建块，并利用这些概念改进了 Google 机器翻译、Google 语音识别和 Google 视觉应用程序的效果。因此，向量和矩阵在处理和理解这些数据时，给了我们极大的自由度。

除此之外，我还想给你一些思考。我希望你关注我们如何改进处理文本的方式。毫无疑问，word2vec 是将单词转换为向量形式最简单且高效的方法之一，但我绝对鼓励那些对研究工作感兴趣的读者，扩展这一概念到他们的母语，或发挥创意，贡献出创新的技术，帮助 NLP 社区克服诸如词义歧义等挑战。嗯，这些就是我给你的所有思考！

# 概述

在本章中，我们已经了解了如何使用 word2vec 来发现语义。简单的向量化技术对我们帮助很大。我们已经看到了它的一些应用。我们也触及了 word2vec 模型的技术细节。为了让你更好地理解这个模型，我向你介绍了许多新的数学和统计学术语。我们将 word2vec 的黑盒转换成了 word2vec 的白盒。我还实现了基础和扩展的示例，以便更好地理解。我们使用了大量的库和 API 来开发 word2vec 模型。我们还看到了在深度学习中使用向量化的优势。然后，我们扩展了 word2vec 的理解，发展出了 para2vec、doc2vec 和 GloVe 的概念。

下一章将基本介绍规则驱动技术如何用于开发自然语言处理（NLP）应用，以及各种 NLP 应用如何使用一种简单却非常有效的技术，称为规则或逻辑，来开发基本且有效的原型。Google 在其机器翻译项目中使用了规则驱动技术，Apple 也采用了这种技术，最后但同样重要的是，Google 还使用规则驱动系统来制作他们自动驾驶汽车的早期原型。我们将讨论规则驱动系统及其架构。我们还将了解规则驱动的 NLP 应用架构是什么。我会为你提供一种思维方式，通过使用这种思维方式，你也可以为你的 NLP 应用制定规则。我们将实现基本的语法规则和基于模式的规则。我们还将从零开始开发一个基于模板的聊天机器人。
