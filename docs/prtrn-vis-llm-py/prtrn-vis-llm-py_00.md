# 前言

所以，你想使用基础模型工作吗？那是一个非常好的起点！我们中许多机器学习领域的从业者多年来一直关注这些有趣的“生物”，从最早期的 Transformer 模型诞生，到它们在计算机视觉领域的扩展，再到如今在文本生成和互动对话中几乎无处不在的存在。

那么，基础模型是从哪里来的呢？它们是如何工作的？是什么驱动它们的运作，什么时候应该进行预训练和微调？如何在你的数据集和应用程序中挖掘出性能提升？你需要多少加速器？一个端到端的应用是什么样的，如何使用基础模型来掌握这个生成式 AI 的新兴趣浪潮？

这些页面希望能为这些非常重要的问题提供答案。正如你无疑已经意识到的，这个领域的创新速度真是令人瞩目，每天都有更多的基础模型从开源和专有模型供应商处上线。为了应对这一现实，我尽力在本书中专注于最重要的概念基础。这意味着，你在这里的细心学习至少会对未来几年产生回报。

在实践应用和指导方面，我主要聚焦于通过 AWS 提供的云计算选项，尤其是 Amazon SageMaker。我在 AWS 已经愉快工作了超过五年，很高兴与大家分享我所有的知识和经验！请注意，本书中所有的观点和意见仅代表我个人的看法，并不代表亚马逊的立场。

接下来的章节将重点讨论概念，而不是代码。这是因为软件变化迅速，而基本原理变化缓慢。你会在与本书相关的代码库中找到我在这十五章中提到的所有关键主题的资源链接，你可以立即使用它们来亲自实践你在这里学到的内容。从 2023 年 7 月 1 日起，你还将在代码库中找到一套新的预训练和微调示例，由我亲自提供，以完成所有相关主题。

你可能很难相信，但在我二十多岁的时候，我其实并没有在编程：我在探索佛教僧侣的生活。我曾在亚利桑那州的一个冥想静修中心——Garchen Institute——生活了五年。在这段时间里，我学会了冥想、集中精力、观察情绪并培养良好的习惯。多年后在芝加哥大学获得硕士学位，以及现在在亚马逊工作，我发现这些品质在今天的世界中也是非常有用的！

我提到这一点是为了让你振作起来。机器学习、人工智能、云计算、经济学、应用开发，这些话题都不简单。但只要你付出努力，真正拓宽思维，去考虑这些话题的核心基础，不断迎接挑战，实际上没有什么是你做不到的。这就是人类的美丽所在！如果一个从静谧的禅修小屋中走出的冥想瑜伽士，最终也能学会预训练和微调基础模型，那么你也一定可以做到！

有了这些概念，接下来让我们更多地了解这本书本身！

注意

这里提到的大部分概念，将会伴随脚本示例在 2023 年 7 月 1 日开始发布在仓库中。然而，为了帮助你提前入门，你今天就可以在仓库中找到一些资源列表，其中包含指向其他地方的实用示例链接，供展示使用。

# 这本书适合谁阅读？

如果你是希望启动基础模型项目的机器学习研究人员或爱好者，那么这本书非常适合你。应用科学家、数据科学家、机器学习工程师、解决方案架构师、产品经理以及学生都将从本书中受益。要求具备中级 Python 知识，以及云计算的基础概念。需要对深度学习基础有扎实的理解，同时本书也会解释高级话题。内容涵盖了先进的机器学习和云计算技术，以一种可操作、易于理解的方式进行讲解。

# 本书内容概述

*第一章**，基础模型预训练概述* 本章将介绍基础模型，这是当今许多人工智能和机器学习系统的支柱。我们将深入了解它们的创建过程，也就是预训练，了解在哪些方面提升模型准确性具有竞争力。我们将讨论支撑当前先进模型（如 Stable Diffusion、BERT、Vision Transformers、CLIP、Flan-T5 等）的核心变换器架构。你将学习解决各种应用场景的编码器和解码器框架。

*第二章**, 数据集准备：第一部分* 在本章中，我们开始讨论在数据集中需要什么内容来启动一个有意义的预训练项目。这是关于数据集准备的两部分中的第一部分。开篇介绍了关于如何为基础模型找到合适用例的一些业务指导，其中数据变得至关重要。然后，聚焦于数据集的内容，我们使用定性和定量指标将其与用于预训练其他顶级模型的数据集进行比较。你将学习如何使用规模定律来判断你的数据集是否“足够大”和“足够好”，以提高预训练过程中的准确性。我们还讨论了偏差识别与缓解，以及多语言和多模态的解决方案。

*第三章**, 模型准备* 在本章中，你将学习如何选择哪个模型最适合作为你预训练方案的基础。你将学习如何从参数的角度思考模型的大小，以及关键的损失函数如何决定模型在生产中的表现。你将结合规模定律和预期的数据集大小，选择模型的上限和下限，这些将帮助你指导实验的设计。

*第四章**, 云上的容器和加速器* 在本章中，你将学习如何将脚本容器化并优化它们以适应云上的加速器。我们将了解适用于基础模型的一系列加速器，包括在整个机器学习生命周期中的成本与性能的权衡。你将学习 Amazon SageMaker 和 AWS 的关键方面，以便在加速器上训练模型、优化性能并排除常见问题。如果你已经熟悉在 AWS 上使用加速器，可以跳过本章。

*第五章**, 分布式基础* 在本章中，你将学习用于大规模预训练和微调的分布式技术的概念基础。首先，你将掌握机器学习的主要分布式概念，特别是模型并行和数据并行。接下来，你将了解 Amazon SageMaker 如何与分布式软件集成，将任务运行在你需要的任意数量的 GPU 上。你将学习如何优化模型并行和数据并行，以便进行大规模训练，尤其是使用像分片数据并行这样的技术。然后，你将学习如何通过高级技术来减少内存消耗，例如优化器状态分片、激活检查点、编译等。最后，我们将通过一些语言、视觉等方面的示例，将所有这些概念结合起来。

*第六章**，数据集准备：第二部分，数据加载器* 本章将教您如何准备数据集，以便立即与您选择的模型一起使用。您将掌握数据加载器的概念，了解为什么它是大规模训练中常见的错误源。您将学习如何创建嵌入、使用分词器以及其他方法，将原始数据转换为适用于您的神经网络的特征。按照这些步骤，您将能够准备好整个数据集，适用于视觉和语言模型。最后，您将学习在 AWS 和 Amazon SageMaker 上的数据优化技术，以高效地将大小不一的数据集发送到训练集群。在本章中，我们将从训练循环开始，逐步完成您所需的所有步骤，使您能够在大规模训练中成功实现深度神经网络。您还将跟随一个案例研究，了解我是如何在 SageMaker 上训练 10TB 的 Stable Diffusion 模型的！

*第七章**，找到合适的超参数* 本章将深入探讨影响顶级视觉和语言模型性能的关键超参数，如批量大小、学习率等。首先，我们将为那些新手或需要轻度复习的读者快速回顾超参数调优，包括视觉和语言领域的关键示例。接着，我们将探讨基础模型中的超参数调优，了解今天可以实现的技术和未来可能出现的趋势。最后，我们将学习如何在 Amazon SageMaker 上进行这些操作，通过逐步增加集群大小并调整每个超参数。

*第八章**，SageMaker 上的大规模训练* 本章将介绍使用 Amazon SageMaker 进行高度优化的分布式训练的关键功能和特性。您将学习如何优化脚本以适应 SageMaker 训练，同时掌握关键的可用性特性。您还将了解 SageMaker 分布式训练的后端优化，如 GPU 健康检查、抗干扰训练、检查点保存、脚本模式等。

*第九章**，高级训练概念* 本章将讨论大规模训练中的高级概念，如评估吞吐量、计算每个设备的模型 TFLOPS、编译以及利用扩展法则确定合适的训练时间长度。在上一章中，您学习了如何在 SageMaker 上进行大规模训练的基本方法。本章您将学习一些特别复杂且高效的技术，帮助您降低工作的总体成本。这种降低的成本直接转化为更高的模型性能，因为它意味着您可以在相同的预算下训练更长时间。

*第十章**，微调与评估* 本章将教你如何在特定用例数据集上微调模型，并将其性能与现成的公共模型进行比较。你应能看到来自预训练过程的定量和定性提升。我们将深入探讨语言、文本及其中的各种示例。你还将学会如何设计一个人类在环的评估系统，包括使得 ChatGPT 运作的相同 RLHF！本章重点介绍*更新模型的可训练权重*。对于模拟学习但不更新权重的技术，如提示调优和标准的检索增强生成，请参见*第十三章*的提示工程或*第十五章*的未来趋势。

*第十一章**，偏见的检测、缓解和监控* 本章将分析针对大型视觉、语言和多模态模型的偏见识别和缓解策略。你将了解偏见的概念，包括其统计意义以及它如何以关键的方式影响人类。你将掌握量化和解决视觉及语言模型中偏见的关键方法，最终学习能够减少所有形式危害的监控策略，以便在应用基础模型时避免伤害。

*第十二章**，如何部署你的模型* 本章将介绍多种部署模型的技术，包括实时端点、无服务器计算、批处理选项等。这些概念适用于许多计算环境，但我们将重点讨论在 Amazon SageMaker 上的 AWS 可用功能。我们将讨论为什么在部署前应尝试缩小模型的大小，以及如何在视觉和语言领域应用相关技术。我们还将讨论分布式托管技术，适用于无法或不需要缩小模型的场景。最后，我们将探索模型服务技术和概念，帮助你优化模型的端到端性能。

*第十三章**，提示工程* 本章将深入探讨一组特殊的技术——提示工程。你将了解这一技术的高级概念，包括它与本书其他学习方法的相似性与差异。我们将探讨视觉和语言领域的示例，并深入了解关键术语和成功标准。特别是，本章涵盖了无需更新模型权重即可提高性能的所有技巧和窍门。这意味着我们将在不改变任何模型参数的情况下模拟学习过程。这包括一些高级技术，如提示和前缀调优。

*第十四章**, 视觉与语言的 MLOps* 在本章中，我们将介绍机器学习的操作和编排的核心概念，也称为 MLOps。包括构建管道、持续集成与部署、在不同环境中进行推广等内容。我们将探讨监控选项以及对模型预测进行人工审核的方式。我们还将识别在 MLOps 管道中支持大型视觉与语言模型的独特方法。

*第十五章**, 预训练基础模型的未来趋势* 在本章中，我们将通过展望本书中所有相关主题的未来趋势来结束本书。我们将探讨基础模型应用开发的趋势，如使用 LangChain 构建交互式对话应用程序，以及使用检索增强生成（RAG）等技术减少大语言模型的幻觉问题。我们还将探讨如何利用生成模型解决分类任务、人本设计，以及其他生成模式，如代码、音乐、产品文档、PPT 等！我们将讲解 AWS 提供的服务，如 SageMaker JumpStart 基础模型、Amazon Bedrock、Amazon Titan 和 Amazon Code Whisperer，并讨论基础模型和预训练本身的未来趋势。

# 要从本书中获得最大收益

如前所述，为了充分利用本书的内容，你需要在 Python 开发中非常得心应手。书中的页面并不会花太多时间聚焦在软件上，但 GitHub 仓库中的所有内容都是 Python。如果你已经在使用一些关键的 AWS 服务，如 Amazon SageMaker、S3 存储桶、ECR 镜像和 FSx for Lustre，那么你在应用本书中学到的知识时将会大大提速。如果你对这些服务不熟悉也没关系，我们将提供这些服务的介绍。

| **AWS 服务或开源** **软件框架** | **我们使用它的目的** |
| --- | --- |
| Amazon SageMaker | Studio、笔记本实例、训练任务、端点、管道 |
| S3 buckets | 存储对象并检索元数据 |
| Elastic Container Registry | 存储 Docker 镜像 |
| FSx for Lustre | 用于模型训练循环的大规模数据存储 |
| Python | 通用脚本：包括管理和与服务交互、导入其他包、清洗数据、定义模型训练与评估循环等 |
| PyTorch 和 TensorFlow | 定义神经网络的深度学习框架 |
| Hugging Face | 拥有超过 100,000 个开源预训练模型的 hub，以及无数在自然语言处理（NLP）和计算机视觉（CV）中非常有用且可靠的方法 |
| Pandas | 数据分析的首选库 |
| Docker | 构建和管理容器的开源框架 |

**如果您使用的是本书的数字版本，我们建议您通过本书 GitHub 仓库访问代码（下一个章节中会提供链接），逐步完成示例，并自己键入代码。这样做有助于您避免因复制和粘贴代码而可能出现的错误。**

# 下载示例代码文件

您可以从 GitHub 上下载本书的示例代码文件，网址为[`github.com/PacktPublishing/Pretrain-Vision-and-Large-Language-Models-in-Python`](https://github.com/PacktPublishing/Pretrain-Vision-and-Large-Language-Models-in-Python)。如果代码有更新，GitHub 仓库中会同步更新。

我们还提供了其他代码包，来自我们丰富的书籍和视频目录，您可以在[`github.com/PacktPublishing/`](https://github.com/PacktPublishing/)查看。

# 使用的约定

本书中使用了许多文本约定。

`文中的代码`：表示文本中的代码单词、数据库表名、文件夹名称、文件名、文件扩展名、路径名、虚拟 URL、用户输入和 Twitter 用户名。这里有一个例子：“将下载的`WebStorm-10*.dmg`磁盘镜像文件挂载为系统中的另一个磁盘。”

代码块的设置如下：

```py
html, body, #map {
 height: 100%;
 margin: 0;
 padding: 0
}
```

**粗体**：表示新术语、重要单词或您在屏幕上看到的单词。例如，菜单或对话框中的单词会以**粗体**显示。这里有一个例子：“从**管理**面板中选择**系统信息**。”

提示或重要说明

显示如下。

# 联系我们

我们始终欢迎读者的反馈。

**一般反馈**：如果您对本书的任何内容有疑问，请通过[customercare@packtpub.com](http://customercare@packtpub.com)给我们发邮件，并在邮件主题中注明书名。

**勘误**：虽然我们已经尽力确保内容的准确性，但难免会出现错误。如果您发现本书中的错误，我们将非常感激您向我们报告。请访问[www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)，并填写表单。

**盗版**：如果您在互联网上遇到我们作品的任何非法复制形式，我们将非常感激您能提供相应的地址或网站名称。请通过[copyright@packt.com](http://copyright@packt.com)与我们联系，并附上相关内容的链接。

**如果您有兴趣成为作者**：如果您在某个主题方面具有专业知识，并且有兴趣撰写或参与书籍的编写，请访问[authors.packtpub.com](http://authors.packtpub.com)。

# 分享您的想法

一旦您阅读了《Pretrain Vision and Large Language Models in Python》，我们非常希望听到您的反馈！[请点击这里直接访问该书的亚马逊评论页面](https://packt.link/r/1-804-61825-X)，并分享您的意见。

您的反馈对我们和技术社区都非常重要，将帮助我们确保提供优质的内容。

# 下载本书的免费 PDF 副本

感谢你购买这本书！

你喜欢随时随地阅读，但又无法携带纸质书籍吗？

你的电子书购买是否无法兼容你选择的设备？

别担心，现在购买每一本 Packt 书籍，你都能免费获得该书的无 DRM PDF 版本。

随时随地，在任何设备上阅读。直接从你最喜欢的技术书籍中搜索、复制并粘贴代码到你的应用程序中。

优惠不止于此，你还可以独享折扣、新闻通讯和每日免费内容，直接发送到你的邮箱。

按照这些简单步骤即可享受优惠：

1.  扫描二维码或访问下面的链接

![](img/B18942_QR_Free_PDF.jpg)

[`packt.link/free-ebook/9781804618257`](https://packt.link/free-ebook/9781804618257)

1.  提交你的购买证明

1.  就是这样！我们将直接把你的免费 PDF 和其他福利发送到你的邮箱。

# 第一部分：预训练之前

在第一部分，你将学习如何准备预训练一个大型视觉和/或语言模型，包括数据集和模型准备。

本节包含以下章节：

+   *第一章*，*预训练基础模型介绍*

+   *第二章*，*数据集准备：第一部分*

+   *第三章*，*模型准备*
