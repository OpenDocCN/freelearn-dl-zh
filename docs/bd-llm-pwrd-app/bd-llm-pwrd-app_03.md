

# 第三章：为您的应用选择一个 LLM

在上一章中，我们看到了在应用中正确编排**大型语言模型**（**LLM**）及其组件是多么关键。事实上，我们看到了并非所有 LLM 都是相同的。下一个关键决策是实际使用哪些 LLM。不同的 LLM 可能具有不同的架构、大小、训练数据、能力和局限性。为您的应用选择合适的 LLM 不是一个简单的决定，因为它可能会对您解决方案的性能、质量和成本产生重大影响。

在本章中，我们将引导您了解如何为您的应用选择合适的 LLM。我们将涵盖以下主题：

+   市场上最有潜力的 LLM 概述

+   比较 LLM 时使用的主要标准和工具

+   大小与性能之间的权衡

到本章结束时，您应该清楚地了解如何为您的应用选择合适的 LLM，以及如何有效地和负责任地使用它。

# 市场上最有潜力的 LLM

过去一年见证了 LLM 研究和开发的空前激增。不同组织发布了或宣布了几个新模型，每个模型都有其独特的特性和能力。其中一些模型是迄今为止创建的最大和最先进的模型，在数量级上超越了之前的**最先进技术**（**SOTA**）。其他模型更轻量，但在特定任务上更加专业化。

在本章中，我们将回顾截至 2024 年市场上一些最有潜力的 LLM。我们将介绍它们的背景、关键发现和主要技术。我们还将比较它们在各种基准和任务上的性能、优势和局限性。我们还将讨论它们的潜在应用、挑战以及对 AI 和社会未来的影响。

## 专有模型

专有 LLM 是由私营公司开发和拥有的，它们不公开代码。它们通常也需付费使用。

专有模型提供一系列优势，包括更好的支持和维护，以及安全性和一致性。由于它们的复杂性和训练数据集，它们在泛化方面通常优于开源模型。另一方面，它们充当一个“黑盒”，这意味着所有者不会向开发者公开源代码。

在接下来的章节中，我们将介绍截至 2023 年 8 月市场上最受欢迎的三种专有 LLM。

### GPT-4

2023 年 3 月发布的 GPT-4，连同其新发布的“表亲”GPT-4 Turbo，是**OpenAI**开发的最新模型之一，在撰写本书时（根据其首席执行官山姆·奥特曼的确认，OpenAI 已经在开发 GPT-5），是市场上表现最顶尖的模型之一。

它属于**生成预训练 Transformer**（**GPT**）模型类别，这是 OpenAI 引入的仅解码器 Transformer 架构。以下图显示了基本架构：

![图片](img/B21714_03_01.png)

图 3.1：仅解码器 Transformer 的高级架构

如前图所示，仅解码器架构仍然包括我们在*第一章*中介绍的 Transformer 架构中的主要元素，包括*位置嵌入*、*多头注意力*和*前馈*层。然而，在这个架构中，模型仅由一个解码器组成，该解码器被训练根据前面的标记来预测序列中的下一个标记。与编码器-解码器架构不同，仅解码器设计缺少一个用于总结输入信息的显式编码器。相反，信息隐含地编码在解码器的隐藏状态中，该状态在生成过程的每一步都会更新。

现在，我们将看看 GPT-4 相对于先前版本的一些改进。

GPT-4，就像 GPT 系列中的先前模型一样，在公开可用的和 OpenAI 许可的数据集（OpenAI 没有披露训练集的确切组成）上进行了训练。

此外，为了使模型与用户的意图更加一致，训练过程中还涉及**从人类反馈中进行强化学习**（**RLHF**）的训练。

**定义**

RLHF 是一种旨在使用人类反馈作为评估 LLM 生成输出的指标，然后使用该反馈进一步优化模型的技术。实现该目标主要有两个步骤：

1.  基于人类偏好训练奖励模型。

1.  优化 LLM 以适应奖励模型。这一步骤是通过强化学习完成的，它是一种机器学习范式，其中代理通过与环境的交互来学习做出决策。代理根据其行为获得奖励或惩罚的反馈，其目标是通过对行为进行试错来不断调整其行为，以最大化随时间累积的奖励。

通过 RLHF（强化学习与人类反馈），得益于奖励模型，LLM 能够从人类偏好中学习，并与用户的意图更加一致。

以 ChatGPT 为例。该模型集成了各种训练方法，包括无监督预训练、监督微调、指令调整和 RLHF。RLHF 组件涉及通过人类训练师的反馈来训练模型预测人类偏好。这些训练师审查模型的响应并提供评分或更正，指导模型生成更多有帮助、准确和一致的响应。

例如，如果一个语言模型最初产生的输出并不十分有帮助或准确，人类训练师可以提供反馈，指出首选的输出。然后，该模型使用这些反馈来调整其参数并改进未来的响应。这个过程迭代地进行，模型从一系列人类判断中学习，以更好地与人类标准认为的有用或适当的内容对齐。

GPT-4 在常识推理和分析技能方面表现出卓越的能力。它已经与 SOTA 系统进行了基准测试，包括我们在*第一章*中提到的**大规模多任务语言理解**（**MMLU**）。在 MMLU 上，GPT-4 不仅在英语上，而且在其他语言上也优于之前的模型。

以下是一个展示 GPT-4 在 MMLU 上表现的插图：

![一个带有绿色和蓝色条的图表，描述自动生成](img/B21714_03_02.png)

图 3.2：GPT-4 在多种语言上的 3 次射击准确率（来源：[`openai.com/research/gpt-4`](https://openai.com/research/gpt-4)）

除了 MMLU，GPT-4 还在各种 SOTA 系统和学术考试中进行了基准测试，您可以从以下图表中看出：

![一个性能图表，描述自动生成](img/B21714_03_03.png)

图 3.3：GPT 在学术和专业考试中的表现（来源：[`arxiv.org/pdf/2303.08774.pdf`](https://arxiv.org/pdf/2303.08774.pdf)）

**注意**：在前面的图表中，您可以看到 GPT-4 的两个版本，有视觉和无视觉（以及 GPT-3.5 用于基准测试）。这是因为 GPT-4 是一个多模态模型，意味着它可以接受图像作为输入，除了文本。然而，在本章中，我们将仅对其文本能力进行基准测试。

与其前辈（GPT-3.5 和 GPT-3）相比，GPT-4 的另一个重大改进是幻觉风险的显著降低。

**定义**

幻觉是一个描述 LLMs 生成不正确、无意义或不真实但看似合理或连贯的文本的现象。例如，一个 LLM 可能会虚构一个与来源或常识相矛盾的事实，一个不存在的名字，或者一个没有意义的句子。

幻觉可能发生，因为 LLMs 不是存储或检索事实信息的数据库或搜索引擎。相反，它们是从大量文本数据中学习的统计模型，根据它们学习的模式和概率产生输出。然而，这些模式和概率可能不反映真相或现实，因为数据可能是不完整、嘈杂或存在偏见的。此外，LLMs 的上下文理解和记忆有限，因为它们一次只能处理一定数量的标记并将它们抽象为潜在表示。因此，LLMs 可能会生成不支持任何数据或逻辑的文本，但根据提示最有可能或相关。

事实上，尽管它仍然不是 100%可靠的，但 GPT-4 在 TruthfulQA 基准测试中取得了巨大进步，这些测试旨在检验模型区分事实与错误陈述的能力（我们在*第一章*的*模型评估*部分中介绍了 TruthfulQA 基准测试）。

在这里，你可以看到一幅插图，比较了 GPT-4 在 TruthfulQA 基准测试中的结果与 GPT-3.5（OpenAI 的 ChatGPT 背后的模型）和 Anthropic-LM（我们将在下一节中介绍这个模型）的结果。

![不同颜色方块的图表，描述由系统自动生成](img/B21714_03_04.png)

图 3.4：在 TruthfulQA 基准测试中的模型比较（来源：[`openai.com/research/gpt-4`](https://openai.com/research/gpt-4)）

最后，随着 GPT-4 的推出，OpenAI 做出了额外的努力使其更加安全和一致，从一开始就组建了一个超过 50 人的专家团队，涵盖 AI 对齐风险、隐私和网络安全等领域，旨在了解这样一个强大模型的潜在风险程度以及如何预防这些风险。

**定义**

对齐是一个描述 LLM（大型语言模型）在多大程度上以对人类用户有用且无害的方式行为的术语。例如，一个 LLM 如果生成的文本是准确、相关、连贯且尊重的，那么它就是对齐的。如果一个 LLM 生成的文本是虚假的、误导性的、有害的或冒犯性的，那么它就是未对齐的。

多亏了这次分析，在训练 GPT-4 时收集并使用了更多数据，以减轻其潜在风险，与前辈 GPT-3.5 相比，风险有所降低。

### Gemini 1.5

Gemini 1.5 是由 Google 开发并于 2023 年 12 月发布的 SOTA（最先进的技术）生成 AI 模型。与 GPT-4 一样，Gemini 被设计为多模态，这意味着它可以处理和生成跨各种模态的内容，包括文本、图像、音频、视频和代码。它基于**专家混合**（**MoE**）Transformer。

**定义**

在 Transformer 架构的背景下，MoE 指的是在其层中集成了多个专门子模型（称为“专家”）的模型。每个专家都是一个神经网络，旨在更有效地处理不同类型的数据或任务。MoE 模型使用门控机制或路由器来确定哪个专家应该处理给定的输入，这使得模型能够动态分配资源并专门处理某些类型的信息。这种方法可以提高训练和推理的效率，因为它使得模型在规模和复杂性上可以扩展，而不会导致计算成本成比例增加。

Gemini 有多种尺寸，包括 Ultra、Pro 和 Nano，以满足从数据中心到移动设备的不同计算需求。要使用 Gemini，开发者可以通过为不同模型变体提供的 API 访问它，从而将它的功能集成到应用程序中。

与其前一代版本 Gemini 1.0 相比，当前模型在文本、视觉和音频任务上表现更优，如下面的截图所示：

![](img/B21714_03_05.png)

图 3.5：Gemini 1.5 Pro 和 Ultra 与其前一代版本 1.0 的比较（来源：[`storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf`](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf) )

类似地，它在数学、科学、推理以及编码和多元语言领域也展现出了卓越的能力：

![](img/B21714_03_06.png)

图 3.6：Gemini 1.5 Pro 与 Gemini 1.0 Pro 和 Ultra 在不同基准上的比较（来源：[`storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf`](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf))

注意，Gemini 1.5 Pro 在多个领域的许多基准测试中优于 Gemini 1.0 Ultra（体积显著更大）。截至今天，用户可以通过 gemini.[google.com](https://google.com)上的 web 应用免费尝试 Gemini Pro，而 Gemini Ultra 则可通过付费的增值订阅获得。另一方面，专为移动设备定制的 Gemini Nano 可以通过 Google AI Edge SDK for Android 在具备能力的 Android 设备上运行。请注意，截至 2024 年 4 月，此 SDK 仍处于早期访问预览阶段，您可以在[`docs.google.com/forms/d/e/1FAIpQLSdDvg0eEzcUY_-CmtiMZLd68KD3F0usCnRzKKzWb4sAYwhFJg/viewform`](https://docs.google.com/forms/d/e/1FAIpQLSdDvg0eEzcUY_-CmtiMZLd68KD3F0usCnRzKKzWb4sAYwhFJg/viewform)申请早期访问计划。最后，开发人员也可以通过 Google AI Studio 的 REST API 使用 Gemini Pro 和 Ultra。

### Claude 2

Claude 2，代表通过用户数据和专业知识实现宪政大规模对齐，是由 Anthropic 开发的一个 LLM，Anthropic 是一家由前 OpenAI 研究人员创立的研究公司，专注于 AI 安全和对齐。它于 2023 年 7 月宣布。

Claude 2 是一个基于 transformer 的 LLM，它通过无监督学习、RLHF 和**宪政 AI**（**CAI**）在互联网上公开可用的信息和专有数据的混合上进行了训练。

CAI 是 Claude 的一个真正独特之处。事实上，Anthropic 对 Claude 2 与安全原则的一致性给予了超乎寻常的关注。更具体地说，Anthropic 开发了一种独特的称为 CAI 的技术，该技术在 2022 年 12 月的论文《宪政 AI：从 AI 反馈中消除危害》中首次公开披露。

CAI 旨在通过防止产生有害或歧视性输出，不帮助人类从事非法或不道德的活动，以及广泛地创建一个有益、诚实且无害的 AI 系统，来使模型更安全并更符合人类价值观和意图。为了实现这一目标，它使用一系列原则来指导模型的行为和输出，而不是仅仅依赖人类反馈或数据。这些原则来源于各种来源，例如联合国人权宣言、信任和安全最佳实践、其他 AI 研究实验室提出的原则、非西方视角和实证研究。

CAI 在训练过程的两个阶段使用这些原则：

+   首先，模型被训练使用原则和几个示例来批评和修改自己的响应。

+   其次，模型通过强化学习进行训练，但不是使用人类反馈，而是使用基于原则的 AI 生成的反馈来选择更无害的输出。

以下插图显示了根据 CAI 技术进行的训练过程：

![流程图示意图  自动生成描述](img/B21714_03_07.png)

图 3.7：Claude 根据 CAI 技术进行的训练过程（来源：[`arxiv.org/abs/2212.08073`](https://arxiv.org/abs/2212.08073))

Claude 2 的另一个特点是上下文长度限制为 100,000 个标记。这意味着用户可以输入更长的提示，例如页面的技术文档甚至一本书，无需嵌入。此外，与其他 LLM 相比，该模型还可以生成更长的输出。

最后，Claude 2 在与代码一起工作时也展示了相关能力，在 HumanEval 基准测试中得分 71.2%。

**定义**

HumanEval 是评估 LLM 代码生成能力的基准。它由 164 个由人类编写的 Python 编程问题组成，每个问题都有一个提示、一个解决方案和一个测试套件。这些问题涵盖了各种主题，如数据结构、算法、逻辑、数学和字符串操作。该基准可以用来衡量 LLM 输出的功能性正确性、语法有效性和语义一致性。

总体而言，Claude 2 是一个非常有趣的模型，也是 GPT-4 的竞争对手，值得关注。它可以通过 REST API 或直接通过 Anthropic 的 beta 聊天体验（截至 2023 年 8 月仅限美国和英国用户）进行消费。

以下比较表显示了三个模型之间的主要差异：

|  | **GPT-4** | **Gemini** | **Claude 2** |
| --- | --- | --- | --- |
| **公司或机构** | OpenAI | Google | Anthropic |
| **首次发布** | 2023 年 3 月 | 2023 年 12 月 | 2023 年 7 月 |
| **架构** | 基于 Transformer，仅解码器 | 基于 Transformer | 基于 Transformer |
| **大小和变体** | 参数未正式指定两种上下文长度变体：GPT-4 8K 标记 GPT-4 32K 标记 | 从最小到最大分为三种大小：Nano、Pro 和 Ultra | 未正式指定 |
| **如何使用** | OpenAI 开发者平台上的 REST API | 使用 OpenAI Playground 在 [`platform.openai.com/playground`](https://platform.openai.com/playground) | Google AI Studio 上的 REST API | 使用 Gemini 在 [`gemini.google.com/`](https://gemini.google.com/) | 在 [`www.anthropic.com/claude`](https://www.anthropic.com/claude) 编译表单后的 REST API |

表 3.1：GPT-4、PaLM 2 和 Claude 2 的比较表

除了专有模型之外，目前市场上还有大量的开源 LLM。让我们在下一节讨论其中的一些。

## 开源模型

开源模型的优势在于，根据定义，开发者可以完全看到并访问源代码。在 LLM 的背景下，这意味着以下内容：

+   你对架构有主要控制权，这意味着你还可以修改你将在项目中使用的本地版本。这也意味着它们不太可能受到模型所有者对源代码进行的潜在更新的影响。

+   除了传统的微调之外，还有可能从头开始训练你的模型，这对于专有模型也是可用的。

+   免费使用，这意味着在使用这些 LLM 时，你不会产生任何费用，与那些按使用付费的专有模型形成对比。

为了比较开源模型，在本书中，我们将参考独立的 Hugging Face Open LLM 排行榜（你可以在 [`huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard`](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) 找到它），这是一个旨在评估和比较 LLM 在各种自然语言理解（**NLU**）任务上性能的项目。该项目托管在 Hugging Face Spaces 上，这是一个用于创建和分享机器学习应用的平台。

Open LLM 排行榜使用四个主要的评估基准，我们在 *第一章* 的 *模型评估* 部分进行了介绍：

+   **AI2 Reasoning Challenge**（**ARC**）：小学科学问题和复杂的自然语言理解（**NLU**）任务。

+   **HellaSwag**: 常识推理。

+   **MMLU**: 涵盖数学、计算机科学和法律等各个领域的任务。

+   **TruthfulQA**：评估模型在生成答案时的真实性。

尽管这些只是众多 LLM 基准测试中的一部分，但我们将坚持使用这个排行榜作为参考评估框架，因为它被广泛采用。

### LLaMA-2

**大型语言模型 Meta AI 2**（**LLaMA-2**）是 Meta 开发的新一代模型，于 2023 年 7 月 18 日向公众公布，开源且免费（其第一个版本最初仅限于研究人员）。

它是一个**自回归**模型，具有优化且仅包含解码器的 Transformer 架构。

**定义**

在 Transformer 的上下文中，自回归的概念指的是模型根据所有前面的标记来预测序列中的下一个标记。这是通过在输入中屏蔽未来的标记来实现的，这样模型就只能关注过去的标记。例如，如果输入序列是“The sky is blue”，模型会首先预测“The”，然后是“sky”，接着是“is”，最后是“blue”，使用屏蔽来隐藏每个预测之后的标记。

LLaMA-2 模型有三种大小：70 亿、130 亿和 700 亿参数。所有版本都训练在 2000 亿个标记上，上下文长度为 4092 个标记。

此外，所有模型大小都附带一个“聊天”版本，称为 LLaMA-2-chat，与基础模型 LLama-2 相比，它在通用对话场景中更加灵活。

**注意**

在 LLM 的上下文中，**基础模型**与“聊天”或**辅助模型**之间的区别主要在于它们的培训和预期用途：

+   基础模型：这些模型是在大量文本数据上训练的，通常来源于互联网，其主要功能是在给定上下文中预测下一个单词，这使得它们在理解和生成语言方面非常出色。然而，它们可能并不总是精确或专注于特定指令。

+   辅助模型：这些模型最初是基础 LLM，但通过包含指令和模型尝试遵循这些指令的输入-输出对进行进一步微调。它们通常使用 RLHF 来细化模型，使其更擅长提供帮助、诚实和无害。因此，它们不太可能生成有问题的文本，更适合聊天机器人内容和内容生成等实际应用。例如，辅助模型 GPT-3.5 Turbo（ChatGPT 背后的模型）是完成模型 GPT-3 的微调版本。

从本质上讲，尽管基础模型提供了对语言的广泛理解，但辅助模型是针对遵循指令和提供更准确、更相关上下文响应进行优化的。

LLaMA-2-chat 是通过一个包含两个主要步骤的微调过程开发的：

1.  **监督微调**：这一步骤涉及在公开可用的指令数据集和超过 100 万人的标注上进行模型微调，使其在对话用例中更有帮助和安全。微调过程使用所选提示列表来引导模型输出，并使用一个鼓励多样性和相关性的损失函数（这就是为什么它被称为“监督”的原因）。

1.  **RLHF**：正如我们在介绍 GPT-4 时所看到的，RLHF 是一种旨在使用人类反馈作为评估 LLM 生成输出的指标的技术，然后使用该反馈进一步优化模型。

以下是如何进行 LLaMA 训练过程的说明：

![图片](img/B21714_03_08.png)

图 3.8：两步微调以获得 LLaMa-2 聊天（来源：[`ai.meta.com/resources/models-and-libraries/llama/`](https://ai.meta.com/resources/models-and-libraries/llama/))

要访问该模型，您需要在 Meta 的网站上提交一个请求（表格可在 [`ai.meta.com/resources/models-and-libraries/llama-downloads/`](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) 找到）。一旦提交请求，您将收到一封电子邮件，其中包含您将能够下载以下资产的 GitHub 仓库。

+   模型代码

+   模型权重

+   README（用户指南）

+   责任使用指南

+   许可证

+   可接受使用政策

+   模型卡片

### Falcon LLM

Falcon LLM 是一种新的 LLM 趋势的体现，它侧重于构建更轻的模型（参数更少）并关注训练数据集的质量。事实上，像 GPT-4 这样具有万亿参数的复杂模型在训练阶段和推理阶段都非常沉重。这暗示了需要高成本的计算能力（GPU 和 TPU 驱动）以及漫长的训练时间。

Falcon LLM 是阿布扎比的 **技术创新研究所**（**TII**）于 2023 年 5 月推出的开源模型。它是一个自回归、仅解码器的变压器，训练了 1 万亿个标记，并拥有 400 亿个参数（尽管它也以 70 亿参数的较轻版本发布）。与 LlaMA 的情况类似，Falcon LLM 还附带了一个微调变体，称为“Instruct”，专门用于遵循用户的指令。

**定义**

指令模型专门用于遵循短格式指令。指令遵循是一个任务，其中模型必须执行一个自然语言命令或查询，例如“写一首关于猫的俳句”或“告诉我巴黎的天气。”Instruct 微调模型是在大量指令及其对应输出的数据集上训练的，例如斯坦福 Alpaca 数据集。

根据开放 LLM 排行榜，自其推出以来，Falcon LLM 一直位居全球前列，仅次于一些 LlaMA 版本。

因此，问题可能就是：一个“仅有”400 亿参数的模型是如何表现得如此出色的？事实上，答案在于数据集的质量。Falcon 是使用专用工具开发的，并采用了一个独特的数据管道，能够从网络数据中提取有价值的内容。该管道通过采用广泛的过滤和去重技术来设计提取高质量内容。由此产生的数据集，称为 *RefinedWeb*，由 TII 在 Apache-2.0 许可下发布，可在 [`huggingface.co/datasets/tiiuae/falcon-refinedweb`](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) 找到。

通过结合卓越的数据质量以及这些优化，Falcon 在分别利用 GPT-3 和 PaLM-62B 大约 75%和 80%的训练计算预算的情况下，实现了显著的性能。

### Mistral

我们将要介绍的第三个和最后一个开源模型系列是 Mistral，由 Mistral AI 开发，该公司成立于 2023 年 4 月，由一支曾在 Meta Platforms 和 Google DeepMind 工作的 AI 科学家团队创立。总部位于法国，该公司通过筹集大量资金和发布开源 LLM 迅速在业界崭露头角，强调 AI 开发中的透明度和可访问性。

Mistral 模型，特别是 Mistral-7B-v0.1，是一个仅具有解码器的 transformer，拥有 73 亿参数，专为生成文本任务设计。它以其创新的架构选择而闻名，如**分组查询注意力**（**GQA**）和**滑动窗口注意力**（**SWA**），这些选择使它在基准测试中优于其他模型。

**定义**

GQA 和 SWA 是旨在提高 LLM 效率和性能的机制。

GQA 是一种技术，与标准全注意力机制相比，可以实现更快的推理时间。它是通过将注意力机制的查询头部分成组，每组共享一个单一的关键头和价值头来实现的。

SWA 用于高效处理较长的文本序列。它扩展了模型注意力的范围，使其超出固定窗口大小，允许每个层引用前一层的一定范围内的位置。这意味着某一层的隐藏状态可以关注前一层的特定范围内的隐藏状态，从而使得模型能够访问更远距离的标记，并以降低推理成本的方式管理不同长度的序列。

该模型还提供了一个针对通用能力进行了微调的变体。这个变体被称为 Mistral-7B-instruct，它在 MT-Bench（一个使用 LLM 作为评判标准的评估框架）上优于市场上所有其他 70 亿参数的 LLM（截至 2024 年 4 月）。

与许多其他开源模型一样，Mistral 可以通过 Hugging Face Hub 进行消费和下载。

**注意**

2024 年 2 月，Mistral AI 和微软签署了一项多年合作协议，以加速人工智能创新。这次合作将利用微软的 Azure AI 超级计算基础设施来支持 Mistral AI 的 LLM 的开发和部署。Mistral AI 的模型，包括其高级模型 Mistral Large，将通过 Azure AI Studio 和 Azure 机器学习模型目录向客户开放。该合作的目标是扩大 Mistral AI 在全球市场的影响力，并促进持续的研究合作。

下面的比较表提供了三个模型之间的主要差异：

|  | **LlaMA** | **Falcon LLM** | **Mistral** |
| --- | --- | --- | --- |
| **公司或机构** | Meta | **技术创新研究所** (**TII**) | Mistral AI |
| **首次发布** | 2023 年 7 月 | 2023 年 5 月 | 2023 年 9 月 |
| **架构** | 自回归转换器，仅解码器 | 自回归转换器，仅解码器 | 转换器，仅解码器 |
| **大小和变体** | 三个大小：7B、13B 和 70B，以及微调版本（聊天） | 两个大小：7B 和 40B，以及微调版本（指令） | 7B 大小以及微调版本（指令） |
| **许可证** | 可在 [`ai.meta.com/resources/models-and-libraries/llama-downloads/`](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) 获取定制商业许可证 | 商业 Apache 2.0 许可证 | 商业 Apache 2.0 许可证 |
| **如何使用** | 在 [`ai.meta.com/resources/models-and-libraries/llama-downloads/`](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) 提交请求表并下载 GitHub 仓库 | 可在 Hugging Face Hub 上下载或使用推理 API/端点 | 可在 Hugging Face Hub 上下载或使用推理 API/端点或 Azure AI Studio |

表 3.2：LLM 的比较表

# 超越语言模型

到目前为止，我们只介绍了语言特定的基础模型，因为这是本书的重点。尽管如此，在人工智能应用背景下，值得提及的是，还有其他可以处理不同于文本的数据的基础模型，这些数据可以被嵌入和编排。

在这里，您可以找到一些市场上今天的一些 **大型基础模型**（**LFM**）的示例：

+   **Whisper**：这是一个由 OpenAI 开发的一般用途语音识别模型，可以在多种语言中转录和翻译语音。它在大量多样化的音频数据集上训练，也是一个多任务模型，可以进行多语言语音识别、语音翻译、口语语言识别和语音活动检测。

+   **Midjourney**：由同名独立研究实验室开发，Midjourney 基于一个序列到序列的转换器模型，它接受文本提示并输出一组与提示匹配的四张图像。Midjourney 被设计成艺术家和创意专业人士的工具，他们可以使用它进行艺术概念的快速原型设计、灵感或实验。

+   **DALL-E**：与之前的类似，DALL-E 由 OpenAI 开发，可以从自然语言描述中生成图像，使用的是在文本-图像对数据集上训练的 1200 亿参数版本的 GPT-3。

理念是我们可以在我们的应用程序中组合和编排多个 LFM 以实现非凡的结果。例如，假设我们想要写一篇关于年轻厨师采访的评论并发布在 Instagram 上。涉及的模型可能如下：

+   **Whisper** 将将采访音频转换为文本。

+   拥有网络插件的**LLM**，例如 Falcon-7B-instruct，将推断年轻厨师的名字并在互联网上搜索以获取其传记。

+   另一个**LLM**，例如 LlaMA，将处理转录并生成具有 Instagram 风格的评论。我们也可以要求同一个模型生成一个提示，让下一个模型根据帖子内容生成图片。

+   **Dall-E**将根据 LLM 生成的提示生成图像。

我们将为我们的 LFM 流程添加 Instagram 插件，以便应用能够在我们的个人资料上发布整个评论，包括插图。

最后，有一些新兴的 LFM 旨在实现多模态，这意味着它们可以使用单一架构处理多种数据格式。GPT-4 本身就是一个例子。

以下截图展示了 OpenAI 对 GPT-4 视觉的早期实验示例，展示了它对图像中幽默方面的理解：

![手机插上充电线](img/B21714_03_09.png)

图 3.9：GPT-4 视觉的早期实验（来源：[`openai.com/research/gpt-4`](https://openai.com/research/gpt-4)）

以下截图展示了 GPT-4 早期版本的一个例子，展示了它如何详细理解和解释图表：

![图表截图 自动生成的描述](img/B21714_03_10.png)

图 3.10：GPT-4 视觉的早期实验（来源：[`openai.com/research/gpt-4`](https://openai.com/research/gpt-4)）

以下示例展示了 GPT-4 早期版本如何理解和解决复杂的数学问题，同时为其响应提供相应的解释：

![计算机截图 自动生成的描述](img/B21714_03_11.png)

图 3.11：GPT-4 视觉的早期实验（来源：[`openai.com/research/gpt-4`](https://openai.com/research/gpt-4)）

GPT-4 只是**大型多模态模型**（**LMM**）的一个例子，它代表了未来几年我们可能会见证的趋势。

# 选择正确 LLM 的决策框架

在前面的段落中，我们介绍了一些市场上目前最有前景的 LLM。现在的问题是：在我的应用中我应该使用哪一个？事实是，这个问题没有直接的答案。

## 考虑事项

选择适用于您应用的 LLM 时需要考虑许多因素。这些因素还需要在两种场景下进行说明：专有和开源 LLM。以下是一些在选择 LLM 时您可能想要考虑的因素和权衡：

+   **大小和性能**：我们注意到，更复杂的模型（这意味着参数数量较多）往往具有更好的性能，尤其是在参数知识和泛化能力方面。然而，模型越大，处理输入和生成输出所需的计算和内存就越多，这可能导致更高的延迟，正如我们将看到的，还有更高的成本。

+   **成本和托管策略**：在我们将 LLMs 整合到我们的应用程序中时，我们必须考虑两种类型的成本：

    +   **模型消费成本**：这指的是我们为消费模型所支付的费用。像 GPT-4 或 Claude 2 这样的专有模型需要支付费用，这通常与处理的令牌数量成比例。另一方面，像 LlaMA 或 Falcon LLM 这样的开源模型是免费使用的。

    +   **模型托管成本**：这指的是您的托管策略。通常，专有模型托管在私有或公共超大规模计算平台上，以便通过 REST API 进行消费，您无需担心底层基础设施（例如，GPT-4 托管在微软 Azure 云中构建的超计算机）。对于开源模型，我们通常需要提供自己的基础设施，因为那些模型可以本地下载。当然，模型越大，所需的计算能力就越强。

        **注意**

        在开源模型的背景下，另一种消费这些模型的方式是使用 Hugging Face 推理 API。免费版本允许您以有限的速率在 Hugging Face 上托管的共享基础设施上测试和评估所有可用的 LLMs。对于生产用例，Hugging Face 还提供推理端点，这样您就可以轻松地将您的 LLMs 部署在专用且完全管理的基础设施上，并有可能配置区域、计算能力和安全级别等参数，以满足您在延迟、吞吐量和合规性方面的限制。

        推理端点的定价信息可在[`huggingface.co/docs/inference-endpoints/pricing`](https://huggingface.co/docs/inference-endpoints/pricing)公开获取。

+   **定制化**：这可能是您在决定采用哪种模型之前想要评估的需求。实际上，并非所有模型在定制化方面都同样灵活。当我们谈论定制化时，我们指的是两种活动：

    +   **微调**：这是对 LLMs 参数进行轻微调整以更好地适应特定领域的过程。所有开源模型都可以进行微调。当涉及到专有模型时，并非所有 LLMs 都可以进行微调：例如，OpenAI 的 GPT-3.5 可以进行微调，而 GPT-4-0613 的微调过程仍然是实验性的，并且需要根据 OpenAI 的要求进行（截至 2023 年 12 月）。

    从此以后，了解您是否需要在您的应用程序中进行微调，并据此做出决定是很重要的。

    +   **从头开始训练**：如果你真的想要一个对领域知识非常具体的 LLM，你可能需要从头开始重新训练模型。要从头开始训练一个 LLM，而不必重新发明架构，你可以下载开源 LLM，并在自定义数据集上简单地重新训练它们。当然，这暗示我们有权访问源代码，当我们与专有 LLM 合作时并非如此。

+   **特定领域的功能**：我们注意到评估大型语言模型（LLM）性能最流行的方式是跨领域平均不同的基准测试。然而，也有一些基准测试是针对特定能力的：如果 MMLU 衡量 LLM 的泛化文化和常识推理，那么 TruthfulQA 更关注 LLM 的对齐性，而 HumanEval 则是针对 LLM 的编码能力。

从此以后，如果你有一个特定的用例在心中，你可能想要使用在某个特定基准测试中表现最好的模型，而不是在所有基准测试中平均表现最好的模型。具体来说，如果你在寻找卓越的编码能力，可能会选择 Claude 2；如果你在寻找分析推理能力，可能会选择 PaLM 2。另一方面，如果你需要一个包含所有这些能力的模型，GPT-4 可能是你的正确选择。

选择特定领域的模型也是节省模型复杂度的一种方式。问题是，如果你需要为特定用例使用它，可能只需要一个相对较小的模型（例如，LlaMA-7B-instruct），这将带来成本和性能方面的所有好处。

**注意**

如果你正在寻找**极其**具体的 LLM，有许多模型是在特定领域的技术文档上训练的。例如，2023 年初，斯坦福大学**基础模型研究中心**（**CRFM**）和 MosaicML 宣布发布了 BioMedLM，这是一个仅解码的基于 Transformer 的 LLM，拥有 27 亿参数，在生物医学摘要和论文上进行了训练。

另一个例子是 BloombergGPT，这是一个针对金融领域开发的 50 亿参数 LLM，基于 Bloomberg 广泛的数据源构建的 3630 亿 token 数据集进行训练，可能是迄今为止最大的特定领域数据集，并增加了 3450 亿 token 的通用数据集。

为了使这个决策框架更加实用，让我们考虑以下关于 TechGen 公司的假设案例研究。

## 案例研究

TechGen Solutions，一家领先的 AI 驱动分析服务提供商，在为下一代客户交互系统选择两个高级语言模型（GPT-4 和 LLaMa-2）之间面临抉择。他们需要一个能够处理各种客户查询、提供准确技术信息并能与他们的专有软件集成的强大语言模型。以下是他们可选的方案：

+   GPT-4：由 OpenAI 开发，GPT-4 以其庞大的参数数量和能够处理文本和图像输入的能力而闻名

+   LLama 2：由 Meta AI 创建，LLama 2 是一个开源模型，因其在小数据集上的可访问性和性能而受到赞誉。

以下是他们做出决策时考虑的因素：

+   性能：TechGen 评估了这些模型的表现，特别是在生成技术内容和代码方面，GPT-4 显示出更高的准确性。

+   集成：与 TechGen 系统集成的简便性至关重要，GPT-4 由于其广泛采用，可能提供更无缝的兼容性。

+   成本：虽然 LLama 2 在某些条件下对商业用途是免费的，但 GPT-4 需要付费，TechGen 必须将其纳入他们的决策考量。

+   未来保障：TechGen 考虑每个模型的长期可行性，包括更新和改进的潜力。

基于这些考虑，TechGen 选择 GPT-4，受其生成复杂、技术性回应的优越性能和其多语言能力所吸引，这些能力与他们的国际扩张计划相一致。这一决策还受到 GPT-4 图像处理功能的启发，TechGen 预计随着他们将其更多多媒体内容融入客户服务，这一功能将变得越来越相关。

TechGen 选择 GPT-4 而非 LLama 2，是由其对一个高性能、多功能的语言模型的需求所驱动，该模型能够随着其全球影响力的增长和多样化的客户需求进行扩展。虽然 LLama 2 的开源特性和成本效益很有吸引力，但 GPT-4 的高级功能和未来保障特性为 TechGen 的雄心勃勃的目标提供了更有说服力的理由。

注意，这些决策因素并非旨在成为决定在应用程序中嵌入哪些模型的详尽指南。尽管如此，这些因素在设置您的应用程序流程时仍是有用的反思元素，这样您可以确定您的需求，然后筛选出那些更适合您目标的 LLMs。

# 摘要

本章介绍了市场上一些最有前景的 LLMs。它首先区分了专有和开源模型，并讨论了所有相关的优缺点。然后深入探讨了 GPT-4、PaLM-2、Claude 2、LLaMa-2、Falcon LLM 和 MPT 的架构和技术特性，并增加了一个关于一些 LMMs 的章节。最后，提供了一个轻量级框架，帮助开发者决定在构建 AI 驱动的应用程序时选择哪些 LLMs。鉴于您的行业特定场景，这对于从您的应用程序中获得最大影响至关重要。

从下一章开始，我们将开始在应用程序中动手操作 LLMs。

# 参考文献

+   GPT-4 技术报告。[`cdn.openai.com/papers/gpt-4.pdf`](https://cdn.openai.com/papers/gpt-4.pdf)

+   短时训练，长时测试：线性偏差注意力实现输入长度外推. [`arxiv.org/pdf/2108.12409.pdf`](https://arxiv.org/pdf/2108.12409.pdf)

+   宪法人工智能：从 AI 反馈中确保无害. [`arxiv.org/abs/2212.08073`](https://arxiv.org/abs/2212.08073)

+   Hugging Face 推理端点. [`huggingface.co/docs/inference-endpoints/index`](https://huggingface.co/docs/inference-endpoints/index)

+   Hugging Face 推理端点定价. [`huggingface.co/docs/inference-endpoints/pricing`](https://huggingface.co/docs/inference-endpoints/pricing)

+   BioMedLM 2.7B 模型卡片. [`huggingface.co/stanford-crfm/BioMedLM`](https://huggingface.co/stanford-crfm/BioMedLM)

+   PaLM 2 技术报告. [`ai.google/static/documents/palm2techreport.pdf`](https://ai.google/static/documents/palm2techreport.pdf)

+   使用语言模型解决定量推理问题. [`arxiv.org/abs/2206.14858`](https://arxiv.org/abs/2206.14858)

+   使用 MT-Bench 和聊天机器人竞技场评估 LLM 作为裁判. [`arxiv.org/abs/2306.05685`](https://arxiv.org/abs/2306.05685)

# 加入我们的 Discord 社区

加入我们的社区 Discord 空间，与作者和其他读者进行讨论：

[`packt.link/llm`](https://packt.link/llm)

![](img/QR_Code214329708533108046.png)
