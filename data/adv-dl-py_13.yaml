- en: Emerging Neural Network Designs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新兴的神经网络设计
- en: In this chapter, we'll look at some emerging **Neural Network** (**NN**) designs.
    They haven't reached maturity yet, but hold potential for the future because they
    try to address fundamental limitations in existing DL algorithms. If one day any
    of these technologies prove successful and useful for practical applications,
    we might get one step closer to artificial general intelligence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍一些新兴的**神经网络**（**NN**）设计。这些设计尚未成熟，但由于它们尝试解决现有深度学习算法中的基本局限性，因此具有未来潜力。如果这些技术在某一天能够证明在实际应用中是成功且有用的，我们可能会更接近人工通用智能。
- en: One thing that we need to bear in mind is the nature of structured data. So
    far in this book, we've focused on processing either images or text—in other words,
    unstructured data. This is not a coincidence, because NNs excel in the seemingly
    complex task of finding structure in combinations of pixels or text sequences.
    On the other hand, ML algorithms, such as gradient boosted trees or random forests,
    seem to perform on a par with, or better than, NNs when it comes to structured
    data, such as social-network graphs or brain connections. In this chapter, we'll
    introduce graph NNs to deal with arbitrary structured graphs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要牢记的一点是结构化数据的性质。到目前为止，在本书中，我们专注于处理图像或文本——换句话说，就是非结构化数据。这并非巧合，因为神经网络擅长于从像素或文本序列的组合中找到结构，而这一任务看似复杂。另一方面，机器学习算法，如梯度提升树或随机森林，在处理结构化数据（如社交网络图或大脑连接）时，表现似乎与神经网络相当，甚至更好。在本章中，我们将介绍图神经网络，用以处理任意结构化的图。
- en: Another NN limitation manifests itself with **Recurrent Networks** (**RNNs**).
    In theory, these are one of the most powerful NN models because they are Turing-complete,
    which means that an RNN can theoretically solve any computational problem. This
    is often not the case in practice. RNNs (even **Long Short-Term Memory **(**LSTM**))
    can struggle to carry information over extended periods of time. One possible
    solution is to extend the RNN with an external addressable memory. We'll look
    at how to do this in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个神经网络的局限性在于**递归神经网络**（**RNN**）。理论上，它们是最强大的神经网络模型之一，因为它们是图灵完备的，这意味着 RNN 理论上可以解决任何计算问题。然而，实际中往往并非如此。RNN（即使是**长短期记忆网络**（**LSTM**））在长时间跨度上保持信息的能力往往较弱。一个可能的解决方案是通过外部可寻址内存扩展
    RNN。本章将介绍如何做到这一点。
- en: The topics in this chapter are not detached from the rest of the topics in this
    book. In fact, we'll see that the new network architectures that we'll look at
    are based on many of the algorithms that we've already covered. These include
    convolutions, RNNs, and attention models, as well as others.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的主题与本书其他部分的内容是紧密相关的。事实上，我们将看到，接下来要探讨的新的网络架构是基于我们之前已经讨论过的许多算法。这些算法包括卷积、RNN
    和注意力模型等。
- en: 'This chapter will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing graph NNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入图神经网络
- en: Introducing memory-augmented NNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入内存增强神经网络
- en: Introducing Graph NNs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入图神经网络
- en: Before learning about **graph NNs** (**GNNs**), let's look at why we need graph
    networks in the first place. We'll start by defining a graph, which is a set of
    objects (also known as **nodes** or **vertices**) where some pairs of objects
    have connections (or **edges**) between them.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习**图神经网络**（**GNN**）之前，让我们先了解一下我们为什么需要图网络。我们从定义图开始，图是由一组对象（也称为**节点**或**顶点**）组成的，其中一些对象之间存在连接（或**边**）。
- en: In this section, we'll use several survey papers as resources, most notably
    *A* *Comprehensive Survey on Graph Neural Networks* ([https://arxiv.org/abs/1901.00596](https://arxiv.org/abs/1901.00596)),
    which contains some quotes and images.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用几篇调查论文作为参考资料，最著名的是《图神经网络的综合调查》（[https://arxiv.org/abs/1901.00596](https://arxiv.org/abs/1901.00596)），其中包含了一些引用和图片。
- en: 'A graph has the following properties:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个图具有以下属性：
- en: We'll represent the graph as ![](img/40da6b62-68d7-4d59-9247-d8e142246f89.png),
    where *V* is the set of nodes and *E* is the set of edges.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将图表示为 ![](img/40da6b62-68d7-4d59-9247-d8e142246f89.png)，其中 *V* 是节点集合，*E* 是边集合。
- en: The expression ![](img/88ec490a-336a-4f32-b501-138e471e5801.png) describes an
    edge between two nodes, ![](img/46829fd0-f93c-4710-909c-612208907d50.png).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表达式 ![](img/88ec490a-336a-4f32-b501-138e471e5801.png) 描述了两个节点之间的边， ![](img/46829fd0-f93c-4710-909c-612208907d50.png)。
- en: An adjacency matrix, ![](img/2bd7df0f-0b56-4b25-aef4-9c8855ffcc90.png), where *n*
    is the number of graph nodes. This is written as ![](img/f3dd7e83-f3cd-4074-9873-d75fac7463d8.png) if
    an edge ![](img/96f0fb9d-25e2-4d01-bdcc-8bb2f14baa86.png) exists and ![](img/d6be8b1c-c944-47bc-9051-aab6d6c44840.png) if
    it doesn't.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邻接矩阵，![](img/2bd7df0f-0b56-4b25-aef4-9c8855ffcc90.png)，其中 *n* 是图的节点数。如果存在边 ![](img/96f0fb9d-25e2-4d01-bdcc-8bb2f14baa86.png)，则写作
    ![](img/f3dd7e83-f3cd-4074-9873-d75fac7463d8.png)；如果没有，则写作 ![](img/d6be8b1c-c944-47bc-9051-aab6d6c44840.png)。
- en: Graphs can be **directed** when the edges have a direction and **undirected** when
    they don't. The adjacency matrix of an undirected graph is symmetric—that is ![](img/8a2dc410-555b-4395-a48d-6034f07430d0.png).
    The adjacency matrix of a directed graph is asymmetric—that is ![](img/0e4b2a68-8374-496e-b3ef-90c570166d30.png).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图可以是**循环的**或**非循环的**。顾名思义，循环图包含至少一个环路，即一个非空的节点路径，其中只有第一个节点和最后一个节点是相同的。非循环图不包含环路。
- en: Graphs can be **cyclic** or **acyclic**. As the name suggests, a cyclic graph
    contains at least one cycle, which is a non-empty path of nodes where only the
    first and the last node are the same. Acyclic graphs don't contain cycles.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图可以是**有向的**或**无向的**。顾名思义，有向图的边是有方向的，而无向图的边没有方向。无向图的邻接矩阵是对称的——即 ![](img/8a2dc410-555b-4395-a48d-6034f07430d0.png)。有向图的邻接矩阵是非对称的——即
    ![](img/0e4b2a68-8374-496e-b3ef-90c570166d30.png)。
- en: Both graph edges and nodes can have associated attributes, known as feature
    vectors. We'll denote the *d*-dimensional feature vector of node *v* with ![](img/0f8228cc-225b-44df-bd68-1449db642346.png).
    If a graph has *n* nodes, we can represent them as a matrix ![](img/1c5fd1ea-48cd-43c1-854d-e7b081d2ea18.png).
    Analogously, each edge attribute is a *c*-dimensional feature vector, expressed
    as ![](img/500c1356-80bc-4683-8bc8-afb9b4cd5109.png), where *v* and *u* are nodes.
    We can represent the set of edge attributes of a graph as a matrix ![](img/f22c4803-7135-40dc-ae0e-d6ab74e68ee5.png).
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图的边和节点都可以有相关联的属性，称为特征向量。我们用 ![](img/0f8228cc-225b-44df-bd68-1449db642346.png)
    来表示节点 *v* 的 *d* 维特征向量。如果图有 *n* 个节点，我们可以将它们表示为一个矩阵 ![](img/1c5fd1ea-48cd-43c1-854d-e7b081d2ea18.png)。类似地，每条边的属性是一个
    *c* 维的特征向量，表示为 ![](img/500c1356-80bc-4683-8bc8-afb9b4cd5109.png)，其中 *v* 和 *u*
    是节点。我们可以将图的所有边的属性表示为一个矩阵 ![](img/f22c4803-7135-40dc-ae0e-d6ab74e68ee5.png)。
- en: 'The following diagram shows a directed graph with five nodes and its corresponding
    adjacency matrix:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示显示了一个有五个节点的有向图及其对应的邻接矩阵：
- en: '![](img/4859621a-c3d2-4895-a357-11a006dfefe6.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4859621a-c3d2-4895-a357-11a006dfefe6.png)'
- en: Directed graph with five nodes and its corresponding ![](img/e6a20be5-c2c0-46cf-82ef-5b766a62dc65.png) adjacency
    matrix
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有五个节点的有向图及其对应的邻接矩阵 ![](img/e6a20be5-c2c0-46cf-82ef-5b766a62dc65.png)
- en: 'A graph is a versatile data structure that lends itself well to the way data
    is organized in many real-world scenarios. The following is a nonexhaustive list
    of examples:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图是一种多功能的数据结构，非常适合现实世界中数据的组织方式。以下是一些非详尽的示例：
- en: We can use graphs to represent users in a social network (nodes) and their groups
    of friends (edges). In fact, this is what Facebook does with their social graph
    (*The Anatomy of the Facebook Social Graph, *[https://arxiv.org/abs/1111.4503](https://arxiv.org/abs/1111.4503)).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用图来表示社交网络中的用户（节点）及其朋友关系（边）。事实上，这正是 Facebook 用其社交图谱（*The Anatomy of the
    Facebook Social Graph*，[https://arxiv.org/abs/1111.4503](https://arxiv.org/abs/1111.4503)）所做的。
- en: We can represent a molecule as a graph, where the nodes are atoms and the edges
    are the chemical bonds between them.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将分子表示为图，其中节点是原子，边是它们之间的化学键。
- en: We can represent a street network (a classic example) as a graph, where the
    streets are edges and their intersections are nodes.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将街道网络（一个经典例子）表示为图，其中街道是边，交叉口是节点。
- en: In online commerce, we can represent both users and items as nodes and the relationships
    between them as edges.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在在线商业中，我们可以将用户和商品表示为节点，它们之间的关系则表示为边。
- en: 'Next, let''s discuss the types of task we can solve with graphs. They fall
    broadly into three categories:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论一下可以用图形解决的任务类型。它们大致分为三类：
- en: '**Node-focused**: Classification and regression of individual nodes. For example,
    in the famous Zachary''s karate club problem ([https://en.wikipedia.org/wiki/Zachary%27s_karate_club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club))
    we have a number of karate club members (nodes) and the friendships between them
    (edges). Initially, the club has a single instructor and all the members train
    as a group under that instructor. Later, the club splits into two groups with
    two separate instructors. Assuming that all but one club member opts to join one
    of the two groups, the goal is to determine which group will choose the last undecided
    member (classification), given its set of friendships with other members.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点聚焦**：对单个节点进行分类和回归。例如，在著名的扎卡里武术俱乐部问题中（[https://en.wikipedia.org/wiki/Zachary%27s_karate_club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club)），我们有多个武术俱乐部成员（节点）以及他们之间的友谊（边）。最初，俱乐部只有一名教练，所有成员都在该教练的带领下作为一个小组进行训练。后来，俱乐部分裂成两个小组，每个小组有一名教练。假设除了最后一名成员外，所有人都选择加入这两个小组中的一个，目标是根据该成员与其他成员的友谊来确定他会选择加入哪个小组（分类任务）。'
- en: '**Edge-focused**: Classification and regression of individual edges of the
    graph. For example, we can predict how likely it is that two people in a social
    network know each other. In other words, the task is to determine whether an edge
    exists between two graph nodes.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边聚焦**：对图的单个边进行分类和回归。例如，我们可以预测社交网络中两个人彼此认识的可能性。换句话说，任务是确定两个图节点之间是否存在边。'
- en: '**Graph-focused**: Classification and regression of full graphs. For example,
    given a molecule represented as a graph, we can predict whether the molecule is
    toxic.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图聚焦**：对整个图进行分类和回归。例如，给定一个表示分子结构的图，我们可以预测这个分子是否有毒。'
- en: 'Next, let''s outline the main training frameworks of GNNs:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们概述GNN的主要训练框架：
- en: '**Supervised**: All training data is labeled. We can apply supervised learning
    at node, edge, and graph level.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：所有训练数据都有标签。我们可以在节点、边和图级别应用监督学习。'
- en: '**Unsupervised**: The goal here is to learn some form of graph embedding—for
    example, using autoencoders (we''ll discuss this scenario later in the chapter).
    We can apply unsupervised learning at node, edge, and graph level.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：目标是学习某种形式的图嵌入——例如，使用自编码器（我们将在本章稍后讨论这种情况）。我们可以在节点、边和图级别应用无监督学习。'
- en: '**Semi-supervised**: This is usually applied at node level, where some graph
    nodes are labeled and some aren''t. Semi-supervised learning is especially suited
    for graphs because we can make the simple (but often true) assumption that neighboring
    nodes are likely to have the same labels. For example, say that we have two neighboring
    connected nodes. One of them contains an image of a car and the other contains
    an image of a truck. Let''s assume that the truck node is labeled as a vehicle
    while the car node is unlabeled. We can safely assume that the car node is also
    a vehicle because of its proximity to another vehicle node (the truck). There
    are multiple ways we can utilize this graph property in GNNs. We''ll outline two 
    of them (they are not mutually exclusive):'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半监督学习**：这通常应用于节点级别，其中一些图节点是已标注的，而另一些则不是。半监督学习特别适用于图，因为我们可以做一个简单（但往往正确）的假设：相邻的节点很可能具有相同的标签。例如，假设我们有两个相邻的连接节点，其中一个包含一辆汽车的图片，另一个包含一辆卡车的图片。假设卡车节点被标记为“车辆”，而汽车节点没有标注。我们可以安全地假设汽车节点也是“车辆”，因为它靠近另一个车辆节点（卡车）。我们可以在图神经网络（GNNs）中利用这一图特性有多种方法。我们将概述其中的两种方法（它们并非相互排斥）：'
- en: Use this property implicitly by feeding the adjacency matrix of the graph as
    input to the network. The network will do its magic and hopefully infer that neighboring
    nodes are likely to have the same labels, thereby increasing the accuracy of the
    predictions thanks to the additional information. Most GNNs we'll discuss in this
    chapter use this mechanism.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐式地利用这一特性，通过将图的邻接矩阵作为输入提供给网络。网络将发挥其魔力，并希望推断出相邻节点很可能具有相同的标签，从而利用附加信息提高预测的准确性。我们将在本章讨论的大多数GNN都使用这一机制。
- en: '**Label propagation**, where we can use labeled nodes as a seed for assigning
    labels to unlabeled ones based on their proximity to the labeled. We can do this
    in an iterative way as far as convergence by going through the following steps:'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签传播**，我们可以使用已标注的节点作为种子，基于它们与已标注节点的接近关系为未标注节点分配标签。我们可以通过以下步骤以迭代方式执行，直到收敛：'
- en: Start with the seed labels.
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从种子标签开始。
- en: For all graph nodes (except the seed), assign a label based on the labels of
    their neighboring nodes. This step creates a new label configuration for the whole
    graph, where some of the nodes might need a new label, based on the modified neighbors'
    labels.
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对所有图节点（除了种子节点），根据其邻居节点的标签分配标签。此步骤会为整个图创建一个新的标签配置，其中某些节点可能需要根据修改后的邻居标签重新分配标签。
- en: Stop label propagation if a convergence criterion is met; otherwise, repeat step
    2.
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果满足收敛准则，则停止标签传播；否则，重复步骤 2。
- en: We'll use this short introduction to graphs as a base for the next few sections,
    where we'll discuss various types of graph-focused NN model. The GNN arena is
    relatively new, and there is no outright perfect model resembling **convolutional
    networks** (**CNNs**) in computer vision. Instead, we have different models with
    various properties. Most of them fall into a few general categories, and there
    are attempts to create a framework that is generic enough to combine them all. This
    book doesn't aim to invent new models or model taxonomies but; instead, we'll
    introduce you to some existing ones.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用这段简短的图介绍作为基础，进入接下来的几节内容，我们将讨论各种类型的图集中神经网络模型。GNN领域相对较新，目前尚未出现与计算机视觉中的**卷积网络**（**CNNs**）类似的完美模型。相反，我们有不同的模型，它们具有各种不同的属性。大多数模型可归为几类，并且有人尝试创建一个足够通用的框架来将它们结合在一起。本书的目的是介绍一些现有的模型，而非发明新的模型或模型分类法。
- en: Recurrent GNNs
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环图神经网络（Recurrent GNNs）
- en: We'll start this section by looking at **graph neural networks** (**GraphNNs**; see *The
    Graph Neural Network Model*, **[https://ieeexplore.ieee.org/document/4700287](https://ieeexplore.ieee.org/document/4700287)**).
    Although the authors of the paper abbreviated the model to GNN, we'll refer to
    it with the GraphNN acronym to avoid conflict with the GNN abbreviation, which
    is reserved for the general class of graph networks. This is one of the first
    GNN models to be proposed. It extends existing NNs to process graph-structured
    data. In the same way that we used the context of a word (that is, its surrounding
    words) to create embedding vectors ([Chapter 6](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml)*,
    Language Modeling*), we can use the neighboring graph nodes of a node to do the
    same. GraphNNs aim to create an *s*-dimensional vector state ![](img/e1c524a1-4bb1-4438-966b-00993d86d5af.png) of
    a node *v* based on the neighborhood of that node. In a similar way to language
    modeling, the vector state can serve as the input for other tasks, such as node
    classification.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过介绍**图神经网络**（**GraphNNs**；参见*图神经网络模型*，**[https://ieeexplore.ieee.org/document/4700287](https://ieeexplore.ieee.org/document/4700287)**）来开始本节内容。尽管论文的作者将该模型缩写为GNN，但为了避免与GNN这一通用图网络类别的缩写发生冲突，我们将使用GraphNN这一缩写。该模型是最早提出的GNN模型之一，它将现有的神经网络扩展到图结构数据的处理。就像我们利用单词的上下文（即它的周围词）来创建词嵌入向量（[第6章](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml)*，语言建模*）一样，我们也可以利用一个节点的邻居图节点来做到这一点。GraphNNs旨在基于节点的邻居创建一个*s*维的节点状态向量
    ![](img/e1c524a1-4bb1-4438-966b-00993d86d5af.png)。类似于语言建模，向量状态可以作为其他任务的输入，例如节点分类。
- en: 'The state of a node is updated by exchanging neighborhood information recurrently
    until a stable equilibrium is reached. Let''s denote the set of neighborhood nodes
    *v* with ![](img/3bd465a0-a7b2-44d0-9ca6-f316a4c15867.png) and a single node of
    that neighborhood with *u*. The hidden state of a node is recurrently updated
    using the following formula:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的状态通过递归交换邻居信息来更新，直到达到稳定的平衡状态。我们用 ![](img/3bd465a0-a7b2-44d0-9ca6-f316a4c15867.png)
    表示邻居节点的集合，用*u*表示该邻居集合中的一个单独节点。节点的隐状态通过以下公式递归更新：
- en: '![](img/bc73d220-66ee-4229-9e08-46567127e7c9.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc73d220-66ee-4229-9e08-46567127e7c9.png)'
- en: Here, *f* is a parametric function (for example, a **feed-forward NN** (**FFNN**))
    and each state ![](img/e1889615-9da0-42d8-96b9-833d85ff49c1.png) is initialized
    randomly. The parametric function *f *takes as inputs the feature vector *![](img/8e00e0c7-e952-4459-9bda-debeb4a0f2bd.png)*
    of *v* , the feature vector ![](img/dda100a7-6d39-42b9-b482-ebfc6882f7e5.png) of
    its neighbor *u*, the feature vector ![](img/2123d1d9-f3ac-4968-b4fd-c8a1b4633c9e.png) of
    the edge connecting *u* and *v*, and the state vector ![](img/b40c18b0-cbc8-4505-9cf3-b9db3e3e3a66.png) of *u*
    at step *t-1*. In other words, *f* uses all known information about the neighborhood
    of *v*. The expression ![](img/161bd9cc-1ccb-4477-972e-908460c7624b.png) is a
    sum of the *f* applied over all neighboring nodes, which allows GraphNN to be
    independent of the number of neighbors and their ordering. The function *f* is
    the same (that is, has the same weights) for all steps of the process.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*f* 是一个参数化函数（例如，**前馈神经网络**（**FFNN**）），每个状态 ![](img/e1889615-9da0-42d8-96b9-833d85ff49c1.png)
    是随机初始化的。参数化函数 *f* 以 *v* 的特征向量 ![](img/8e00e0c7-e952-4459-9bda-debeb4a0f2bd.png)
    为输入，*u* 的邻居特征向量 ![](img/dda100a7-6d39-42b9-b482-ebfc6882f7e5.png)，连接 *u* 和 *v*
    的边特征向量 ![](img/2123d1d9-f3ac-4968-b4fd-c8a1b4633c9e.png)，以及 *u* 在第 *t-1* 步的状态向量
    ![](img/b40c18b0-cbc8-4505-9cf3-b9db3e3e3a66.png)。换句话说，*f* 使用所有关于 *v* 邻域的已知信息。表达式
    ![](img/161bd9cc-1ccb-4477-972e-908460c7624b.png) 是对所有邻近节点应用 *f* 的总和，这使得 GraphNN
    不依赖于邻居的数量和顺序。*f* 函数在整个过程的每个步骤中都是相同的（即具有相同的权重）。
- en: 'Note that we have an iterative (or recurrent) process, where the states at
    step *t* are based on the number of steps up to *t-1*, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们有一个迭代（或递归）过程，其中第 *t* 步的状态基于直到 *t-1* 步的步骤数，如下所示：
- en: '![](img/9e7c34c0-c84f-4979-b13c-02c9e6d344f3.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9e7c34c0-c84f-4979-b13c-02c9e6d344f3.png)'
- en: The recurrent process of updating the feature vector states; the **Grec** recurrent
    layer is the same (that is, has the same weights) for all steps; Source: https://arxiv.org/abs/1901.00596
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量状态更新的递归过程；**Grec** 递归层在所有步骤中都是相同的（即具有相同的权重）；来源：https://arxiv.org/abs/1901.00596
- en: 'The process continues until a stable equilibrium is reached. For this to work,
    the function *f* must be a contraction mapping. Let''s clarify this: when applied
    to any two points (or values) A and B, a contraction mapping function *f* satisfies
    the condition ![](img/6a9b9e1e-bd66-4509-9081-1d2371d51fb5.png), where γ is a
    scalar value and ![](img/20f023c9-77a0-42d0-b88d-6a2f2359c604.png). In other words, the
    contraction mapping shrinks the distance between two points after mapping. This
    ensures that the system will converge (exponentially quickly) to the equilibrium
    state vector ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) for any initial
    value ![](img/3601a48a-7d0c-4dd3-98f6-112e93dd8b86.png). We can modify an NN to
    be a contracting function, but this goes beyond the scope of this book.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程将继续，直到达到稳定平衡。为了使其有效，函数 *f* 必须是一个收缩映射。让我们澄清一下：当应用于任意两个点（或值）A 和 B 时，收缩映射函数
    *f* 满足条件 ![](img/6a9b9e1e-bd66-4509-9081-1d2371d51fb5.png)，其中 γ 是一个标量值，且 ![](img/20f023c9-77a0-42d0-b88d-6a2f2359c604.png)。换句话说，收缩映射在映射后缩小两个点之间的距离。这确保了系统会（以指数速度）收敛到平衡状态向量
    ![](img/b0bb88a6-0596-4699-a87a-446972438586.png)，对于任何初始值 ![](img/3601a48a-7d0c-4dd3-98f6-112e93dd8b86.png)。我们可以修改一个神经网络成为一个收缩函数，但这超出了本书的范围。
- en: 'Now that we have the hidden state, we can use it for tasks such as node classification.
    We can express this with the following formula:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了隐藏状态，可以将其用于节点分类等任务。我们可以用以下公式表示：
- en: '![](img/db4574c8-41af-44da-9fb4-7e90b7721985.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db4574c8-41af-44da-9fb4-7e90b7721985.png)'
- en: In this equation, ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) is the state
    once an equilibrium is reached and *g* is a parametric function—for example, a
    fully connected layer with softmax activation for classification tasks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，![](img/b0bb88a6-0596-4699-a87a-446972438586.png) 是达到平衡时的状态，而 *g* 是一个参数化函数——例如，一个带有
    softmax 激活的全连接层，用于分类任务。
- en: 'Next, let''s look at how to train the GraphNN, given a set of training labels
    *t[i]* for some or all graph nodes and a mini-batch of size *m*. To train the
    GraphNN, go through the following steps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下如何训练 GraphNN，给定一些或所有图节点的训练标签 *t[i]* 以及大小为 *m* 的小批量。训练 GraphNN 的步骤如下：
- en: Compute ![](img/b0bb88a6-0596-4699-a87a-446972438586.png)and *o[v]* for all *m*
    nodes, following the recurrent process we just described.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) 和 *o[v]* 对所有 *m* 个节点，遵循我们刚刚描述的递归过程。
- en: 'Compute the cost function (*t[i]* is the label of node *i*):'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算代价函数（*t[i]* 是节点 *i* 的标签）：
- en: '![](img/bd326435-7a13-4c36-b3ac-25821841be2e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd326435-7a13-4c36-b3ac-25821841be2e.png)'
- en: Propagate the cost backward. Note that alternating the node state update of
    step 1 with the gradient propagation of the current step allows GraphNN to process
    cyclic graphs.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播代价。请注意，通过将第1步的节点状态更新与当前步骤的梯度传播交替，可以让GraphNN处理循环图。
- en: Update the weights of the combined network *g(f)*.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新组合网络的权重*g(f)*。
- en: GraphNN has several limitations, one of which is that computing the equilibrium
    state vector ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) is not efficient.
    Furthermore, as we mentioned previously in this section, GraphNN uses the same
    parameters (weights) to update ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) over
    all steps *t*. In contrast, other NN models can use multiple stacked layers with
    different sets of weights, which makes it possible for us to capture the hierarchical
    structure of the data. It also allows us to compute ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) in
    a single forward pass. Finally, it's worth mentioning that, although computing ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) is
    a recurrent process, GraphNN isn't a recurrent network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: GraphNN有几个局限性，其中之一是计算平衡状态向量![](img/b0bb88a6-0596-4699-a87a-446972438586.png)效率不高。此外，正如我们在本节之前提到的，GraphNN在所有步骤*t*中使用相同的参数（权重）来更新![](img/b0bb88a6-0596-4699-a87a-446972438586.png)。相比之下，其他NN模型可以使用多个堆叠的层，每一层都有不同的权重集合，这使我们能够捕捉数据的层次结构。它还允许我们在单次前向传播中计算![](img/b0bb88a6-0596-4699-a87a-446972438586.png)。最后，值得一提的是，尽管计算![](img/b0bb88a6-0596-4699-a87a-446972438586.png)是一个递归过程，但GraphNN并不是一个递归网络。
- en: 'The **Gated Graph NN** model (**GGNN**, [https://arxiv.org/abs/1511.05493](https://arxiv.org/abs/1511.05493))
    tries to overcome these limitations with the help of **Gated Recurrent Unit**
    cells (or **GRU**; for more information, see [Chapter 7](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml),
    *Understanding Recurrent Networks*) as a recurrent function. We can define GGNN
    as the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**门控图神经网络**（**GGNN**，[https://arxiv.org/abs/1511.05493](https://arxiv.org/abs/1511.05493)）试图通过**门控递归单元**（**GRU**；更多信息请参见[第7章](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml)，*理解递归网络*）单元来克服这些局限性，作为递归函数。我们可以将GGNN定义为以下形式：'
- en: '![](img/c6865cb0-0fab-4627-b4b6-02ff5cb78f3d.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6865cb0-0fab-4627-b4b6-02ff5cb78f3d.png)'
- en: In the preceding formula, ![](img/e46f22e8-6d32-4c56-8d73-4470554a49de.png).
    To clarify, GGNN updates the state based on its neighboring states ![](img/d3c1e172-c9e8-4106-a5b6-54d77730e785.png) of
    the same step *t* and its previous hidden state ![](img/75ceb0fc-8945-42a1-b824-4096c1bb42a0.png).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，![](img/e46f22e8-6d32-4c56-8d73-4470554a49de.png)。为澄清，GGNN基于同一步骤*t*的邻接状态![](img/d3c1e172-c9e8-4106-a5b6-54d77730e785.png)和前一个隐藏状态![](img/75ceb0fc-8945-42a1-b824-4096c1bb42a0.png)更新状态。
- en: From a historical perspective, GraphNNs were one of the first GNN models. But
    as we mentioned, they have some limitations. In the next section, we'll discuss
    Convolutional Graph Networks, which are a more recent development.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史的角度来看，GraphNN是最早的GNN模型之一。但正如我们之前提到的，它们存在一些局限性。在下一节中，我们将讨论卷积图神经网络，它是更近期的发展。
- en: Convolutional Graph Networks
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积图神经网络
- en: '**Convolutional Graph Networks** (**ConvGNN**) use a stack of special graph
    convolutional layers (Gconv*) to perform a convolution over the nodes of a graph
    when updating the state vectors. In a similar way to GraphNNs, the graph convolution
    takes the neighbors of a node and produces its vector representation ![](img/6f08cd10-d31e-4f38-ac59-8cabc213aad4.png).
    But whereas GraphNN uses the same layer (that is, the same set of weights) over
    all steps *t* of the computation of ![](img/bf2fd2ce-b2d5-48fa-8702-dbbeeea83486.png),
    ConvGNN uses different layers at every step. The difference between the two approaches
    is illustrated in the following diagram:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积图神经网络**（**ConvGNN**）使用一组特殊的图卷积层（Gconv*）在更新状态向量时对图的节点进行卷积。与GraphNN类似，图卷积通过节点的邻居生成其向量表示![](img/6f08cd10-d31e-4f38-ac59-8cabc213aad4.png)。但与GraphNN在计算![](img/bf2fd2ce-b2d5-48fa-8702-dbbeeea83486.png)的所有步骤*t*中使用相同的层（即相同的权重集合）不同，ConvGNN在每个步骤中使用不同的层。两者方法的区别在下面的图中进行了说明：'
- en: '![](img/75832ae4-bfe8-484e-a504-9eefdee3a8bb.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75832ae4-bfe8-484e-a504-9eefdee3a8bb.png)'
- en: 'Top: GraphNN uses the same Grec recurrent layer over all steps t; Bottom: GCN
    uses a different Gconv[*] layer for each step; Source: https://arxiv.org/abs/1901.00596'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上图：GraphNN在所有步骤t上使用相同的Grec递归层；下图：GCN为每个步骤使用不同的Gconv[*]层；来源：[https://arxiv.org/abs/1901.00596](https://arxiv.org/abs/1901.00596)
- en: 'With ConvGNN, the number of steps *t* is defined by the depth of the network.
    Although we will discuss this from a somewhat different perspective, ConvGNN behaves
    as a regular FFNN, but with graph convolutions. By stacking multiple layers, the
    final hidden representation of each node receives messages from a further neighborhood,
    as we can see in the following diagram:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ConvGNN，步骤数 *t* 由网络的深度定义。尽管我们将从略有不同的角度讨论这个问题，ConvGNN 行为上与常规的 FFNN 类似，但进行了图卷积。通过堆叠多个层，每个节点的最终隐藏表示从更远的邻域接收信息，就像下图所示：
- en: '![](img/90f9c2fd-c6b9-4b89-a3af-73dd9243052f.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90f9c2fd-c6b9-4b89-a3af-73dd9243052f.png)'
- en: 'Top: Node-level classification GraphCN; Bottom: Graph-level classification
    GraphCN. Source: https://arxiv.org/abs/1901.00596'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上图：节点级分类 GraphCN；下图：图级分类 GraphCN。来源：[https://arxiv.org/abs/1901.00596](https://arxiv.org/abs/1901.00596)
- en: 'The diagram shows two scenarios:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图示展示了两种场景：
- en: Node-level (top), where the output of each convolutional layer (including the
    last) is a vector for each node of the graph. We can perform node-level operations
    over these vectors.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点级（上图），每个卷积层（包括最后一个）的输出是图中每个节点的一个向量。我们可以对这些向量执行节点级操作。
- en: Graph-level (bottom), which alternates graph convolutions and pooling operations
    and ends with a readout layer, followed by several fully connected layers that
    summarize the whole graph to produce a single output.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图级（下图），交替进行图卷积和池化操作，并以读取层结束，之后是几个全连接层，这些层汇总整个图形并产生单一输出。
- en: Now that we have a high-level overview of ConvGNN, in the following section,
    we'll discuss graph convolutions (and after that, we'll talk about the readout
    and pooling layers).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对 ConvGNN 有了高层次的了解，在接下来的部分中，我们将讨论图卷积（之后会讨论读取和池化层）。
- en: Spectral-based convolutions
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于谱的卷积
- en: There are various types of graph convolutions (check out *A Comprehensive Survey
    on Graph Neural Networks*), but in this section, we'll discuss the algorithm from *Semi-Supervised
    Classification with Graph Convolutional Networks* ([https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907)).
    We'll denote this convolution with GCN to avoid confusion with the general ConvGNN
    notation, which refers to graph convolutional networks in general. GCN is a representative
    of the so-called **spectral-based** category of ConvGNNs. These algorithms define
    graph convolutions by introducing filters from the perspective of graph-signal
    processing, where the graph convolutional operation is interpreted as removing
    noises from graph signals.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图卷积有多种类型（可以查看 *图神经网络的综合调查*），但在本节中，我们将讨论来自 *半监督分类与图卷积网络* 的算法（[https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907)）。为了避免与一般的
    ConvGNN 表示法混淆，我们将此卷积称为 GCN。GCN 是所谓的 **基于谱的** ConvGNN 类别的代表。这些算法通过从图信号处理的角度引入滤波器来定义图卷积，其中图卷积操作被解释为从图信号中去除噪声。
- en: 'In the *Graph neural network* section*,* we defined the hidden node state ![](img/b1e6e909-4469-4c99-8193-9c922b7114c7.png) and
    noted that ![](img/f907d77e-60b9-436e-a154-6b69b18be076.png) in the case of GGNN.
    Let''s extend this notation by stacking the hidden vector states of all nodes
    in the graph in a matrix ![](img/b2332c0f-094d-4ecb-94e0-a1d354281c7d.png), where
    *n* is the total number of nodes in the graph and *d* is the size of the feature
    vectors. Each row of the matrix represents the hidden state of a single node.
    Then, we can define the generic formula for a single GCN layer at step *l+1* as
    the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图神经网络* 部分，我们定义了隐藏节点状态 ![](img/b1e6e909-4469-4c99-8193-9c922b7114c7.png)，并注意到在
    GGNN 的情况下 ![](img/f907d77e-60b9-436e-a154-6b69b18be076.png)。我们通过将图中所有节点的隐藏向量状态堆叠成一个矩阵
    ![](img/b2332c0f-094d-4ecb-94e0-a1d354281c7d.png)，其中 *n* 是图中节点的总数，*d* 是特征向量的大小。矩阵的每一行表示单个节点的隐藏状态。然后，我们可以为单个
    GCN 层在步骤 *l+1* 定义通用公式如下：
- en: '![](img/145d7088-3fdd-4fae-ae22-91ca8ce6b26c.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/145d7088-3fdd-4fae-ae22-91ca8ce6b26c.png)'
- en: Here, **A** is the adjacency matrix, *f* is a nonlinear activation, such as
    ReLU, and ![](img/49f58e0e-95c2-4168-b77d-3e0cfa0058e4.png) (the feature vector
    matrix). Since ![](img/7562814c-e3e6-4725-83d2-ce4ee25ae57c.png) and ![](img/8630b436-b3bc-4ec0-89fe-8bc4fb72235b.png) have
    the same size, ![](img/95d3840b-a76f-4f57-b8bb-69b70ff60f2f.png) has the same
    dimensions as the node feature matrix **X** (see the *Graph neural networks *section). However, ![](img/05b65ee5-1076-4b17-a762-c2bd6b74bf16.png),
    where *z* is the size of the hidden state vector ![](img/150965c7-c90c-4a99-99be-972af915b99d.png) and
    is not necessarily the same as the initial *d*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**A** 是邻接矩阵，*f* 是一个非线性激活函数，例如 ReLU，![](img/49f58e0e-95c2-4168-b77d-3e0cfa0058e4.png)（特征向量矩阵）。由于
    ![](img/7562814c-e3e6-4725-83d2-ce4ee25ae57c.png) 和 ![](img/8630b436-b3bc-4ec0-89fe-8bc4fb72235b.png)
    的大小相同，![](img/95d3840b-a76f-4f57-b8bb-69b70ff60f2f.png) 具有与节点特征矩阵 **X** 相同的维度（见
    *图神经网络* 部分）。然而，![](img/05b65ee5-1076-4b17-a762-c2bd6b74bf16.png)，其中 *z* 是隐状态向量
    ![](img/150965c7-c90c-4a99-99be-972af915b99d.png) 的大小，并不一定与初始的 *d* 相同。
- en: 'Let''s continue with a simplified but concrete version of the GCN:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续用一个简化但具体的 GCN 版本。
- en: '![](img/181962b9-f896-49b5-9978-75461955b3af.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/181962b9-f896-49b5-9978-75461955b3af.png)'
- en: 'Here, ![](img/b3c301c4-c317-4e82-8ab4-3e9b17d1d5b5.png) is a weight matrix
    and σ is the sigmoid function. Since the adjacency matrix **A** represents the
    graph in matrix form, we can compute the output of the layer in a single operation.
    The ![](img/fa44df2d-508c-42aa-a875-20de6c4de0f9.png) operation allows each node
    to receive input from its neighboring nodes (it also allows GCN to work with both
    directed and undirected graphs). Let''s see how this works with an example. We''ll
    use the five-node graph that we introduced in the *Graph neural networks *section*.*
    For the sake of readability, we''ll assign a one-dimensional vector hidden state ![](img/712b0558-85ca-42f7-9e12-f4c66e78f352.png)
    for each node with a value equal to the node number![](img/ac37c382-7f41-405e-bd39-1cf4acf16e4d.png).
    Then we can compute the example with the following formula:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/b3c301c4-c317-4e82-8ab4-3e9b17d1d5b5.png) 是权重矩阵，σ 是 sigmoid 函数。由于邻接矩阵
    **A** 以矩阵形式表示图形，我们可以通过一次运算计算该层的输出。![](img/fa44df2d-508c-42aa-a875-20de6c4de0f9.png)
    运算允许每个节点接收来自其邻居节点的输入（这也使得 GCN 可以处理有向图和无向图）。让我们通过一个例子来看看它是如何工作的。我们将使用在 *图神经网络*
    部分介绍的五节点图。为了易于阅读，我们为每个节点分配一个一维向量隐状态 ![](img/712b0558-85ca-42f7-9e12-f4c66e78f352.png)，其值等于节点编号
    ![](img/ac37c382-7f41-405e-bd39-1cf4acf16e4d.png)。然后，我们可以用以下公式计算该例子：
- en: '![](img/f1d830f5-de0c-4fcd-94f8-5876ec5da8cc.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1d830f5-de0c-4fcd-94f8-5876ec5da8cc.png)'
- en: 'We can see how ![](img/2a88e6d6-53ea-4b02-977e-90196ff09f68.png) because it
    receives input from nodes 2, 3, and 5\. If ![](img/712b0558-85ca-42f7-9e12-f4c66e78f352.png) had
    more dimensions, then each cell of the output vector would be a sum of the corresponding
    cells of the state vectors of the input nodes:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 ![](img/2a88e6d6-53ea-4b02-977e-90196ff09f68.png)，因为它接收来自节点 2、3 和 5 的输入。如果
    ![](img/712b0558-85ca-42f7-9e12-f4c66e78f352.png) 有更多维度，那么输出向量的每个单元格将是输入节点的状态向量对应单元格的总和：
- en: '![](img/72198d96-67d3-4f33-bf65-0a078dfcf0fd.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72198d96-67d3-4f33-bf65-0a078dfcf0fd.png)'
- en: Here, ![](img/a64b3256-e78f-4152-b6ae-e3ef4f263258.png) are the cells of the
    adjacency matrix.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/a64b3256-e78f-4152-b6ae-e3ef4f263258.png) 是邻接矩阵的单元格。
- en: 'Although this solution is elegant, it has two limitations:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个解决方案很优雅，但它有两个限制：
- en: Not all nodes receive input from their own previous state. In the preceding
    example, only node 2 takes input from itself because it has a loop edge (this
    is the edge that connects the node to itself). The solution to this problem is
    to artificially create loop edges for all nodes by setting all values along the
    main diagonal of the adjacency matrix to ones: ![](img/7ba61638-6168-435c-8cff-ba793c77c713.png).
    In this equation, **I** is the identity matrix, which has ones along the main
    diagonal and zeros in all other cells.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有节点都接收来自自身前一状态的输入。在前面的例子中，只有节点 2 从自身接收输入，因为它有一个环路边（这是将节点与自身连接的边）。解决这个问题的方法是通过将邻接矩阵主对角线上的所有值设置为
    1，来人为地为所有节点创建环路边：![](img/7ba61638-6168-435c-8cff-ba793c77c713.png)。在这个方程中，**I**
    是单位矩阵，它的主对角线为 1，其它单元格为 0。
- en: 'Since **A** is not normalized, the state vectors of nodes with a large number
    of neighbors will change their scale in a different way compared to nodes with
    a smaller number of neighbors. We can see this in the preceding example, where ![](img/82a870e8-c31a-4587-a0b3-572fa5aa1099.png) is
    larger compared to the other nodes because node 4 has 3 nodes in its neighborhood.
    The solution to this problem is to normalize the adjacency matrix in such a way
    that the sum of all elements in one row is equal to 1: ![](img/d2340045-f977-47fe-98c8-f1f2c167e1f6.png).
    We can achieve this by multiplying **A** by the inverse degree matrix **D**^(-1).
    The degree matrix **D** is a diagonal matrix (that is, all other elements except
    the main diagonal are zeros) that contains information about the degree of each
    node. We refer to the number of neighbors of a node as a degree of that node.
    For example, the degree matrix of our example graph is the following:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于**A**没有归一化，邻居数量较多的节点的状态向量会与邻居数量较少的节点发生不同的尺度变化。在之前的例子中，我们可以看到，![](img/82a870e8-c31a-4587-a0b3-572fa5aa1099.png)相对于其他节点来说较大，因为节点4的邻域有3个节点。解决这个问题的方法是对邻接矩阵进行归一化，使得一行中所有元素的和为1：![](img/d2340045-f977-47fe-98c8-f1f2c167e1f6.png)。我们可以通过将**A**与逆度矩阵**D**^(-1)相乘来实现这一点。度矩阵**D**是一个对角矩阵（即，除了主对角线外，所有其他元素均为零），其中包含每个节点的度数信息。我们将节点的邻居数量称为该节点的度数。例如，我们示例图的度矩阵如下所示：
- en: '![](img/d6e2c432-11b2-4ea4-ade3-a9b3e6a13ff0.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6e2c432-11b2-4ea4-ade3-a9b3e6a13ff0.png)'
- en: 'Therefore, **D**^(-1)**A** becomes the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**D**^(-1)**A**变为以下形式：
- en: '![](img/441e83ce-02c3-40d5-bfac-0b6e443512a6.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/441e83ce-02c3-40d5-bfac-0b6e443512a6.png)'
- en: This mechanism assigns the same weight to each of the neighboring nodes. In
    practice, the authors of the paper discovered that using the symmetric normalization ![](img/9f5251dd-2d80-411a-b3ed-76f750f333c8.png)works
    better.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个机制给每个邻居节点分配相同的权重。实际上，论文的作者发现，使用对称归一化的![](img/9f5251dd-2d80-411a-b3ed-76f750f333c8.png)效果更好。
- en: 'After we incorporate these two improvements, the final form of the GCN formula
    can be written as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结合了这两个改进后，GCN公式的最终形式可以写成如下：
- en: '![](img/3baa1355-11c2-4958-9f68-0ce56e796f8c.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3baa1355-11c2-4958-9f68-0ce56e796f8c.png)'
- en: Note that the GCN we just described includes only the immediate neighborhood
    of the node as the context. Each stacked layer effectively increases the receptive
    field of the node beyond its immediate neighbors by 1\. The receptive field of
    the second layer of a ConvGNN includes the immediate neighbors, the receptive
    field of the second layer includes the nodes that are two hops away from the current
    node, and so on.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们刚刚描述的GCN仅包括节点的直接邻域作为上下文。每一层堆叠都有效地将节点的感受野扩展到其直接邻居以外的区域1层。ConvGNN第二层的感受野包括直接邻居，第二层的感受野包括当前节点两跳远的节点，依此类推。
- en: In the next section, we'll look at the second major category of graph convolution
    operations, called spatial-based convolutions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论图卷积操作的第二大类，即基于空间的卷积。
- en: Spatial-based convolutions with attention
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带注意力的基于空间的卷积
- en: 'The second ConvGNN category is spatial-based methods, which take inspiration
    from the computer vision convolution ([Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*). We can think of an image as a graph,
    where each pixel is a node, directly connected to its neighboring pixels (the
    left-hand image in the following diagram). For example, if we use 3 × 3 as a filter,
    the neighborhood of each pixel consists of eight pixels. In the image convolution,
    this 3 × 3 weighted filter is applied over the 3 × 3 patch and the result is a
    weighted sum of the intensities of all nine pixels. Similarly, the spatial-based
    graph convolution convolves the representation of the central node with the representations
    of its neighbors to derive an updated representation for the central node, as
    illustrated in the right-hand image in the following diagram:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类ConvGNN是基于空间的方法，它们的灵感来源于计算机视觉中的卷积（[第2章](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)，*理解卷积神经网络*）。我们可以把图像看作一个图，其中每个像素都是一个节点，直接连接到其相邻的像素（下图中的左侧图像）。例如，如果我们使用3
    × 3作为滤波器，每个像素的邻域由八个像素组成。在图像卷积中，这个3 × 3的加权滤波器被应用到3 × 3的区域，结果是所有九个像素强度的加权和。同样，基于空间的图卷积将中心节点的表示与其邻居的表示进行卷积，从而得到中心节点的更新表示，如下图右侧所示：
- en: '![](img/ff69efd8-2240-4c09-91b1-0dc56b8601e9.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff69efd8-2240-4c09-91b1-0dc56b8601e9.png)'
- en: 'Left: 2D convolution over a pixel grid; Right: Spatial graph convolution. Source: https://arxiv.org/abs/1901.00596'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 左图：在像素网格上的二维卷积；右图：空间图卷积。来源：[https://arxiv.org/abs/1901.00596](https://arxiv.org/abs/1901.00596)
- en: The generic spatial-based convolution is somewhat similar to the GCN in the
    sense that both operations rely on graph neighbors. The GCN uses the inverse degree
    matrix to assign weights to each neighbor. Spatial convolutions use the convolution
    filter for the same purpose. The main difference between the two is that in the
    case of GCNs, the weights are fixed and normalized, whereas the filter weights
    of the spatial convolution are learnable and not normalized. In some sense, we
    can think of the GCN as a spatial-based approach as well.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 通用的基于空间的卷积在某种程度上类似于GCN，因为这两种操作都依赖于图的邻居。GCN使用逆度矩阵为每个邻居分配权重，而空间卷积则使用卷积滤波器来完成同样的任务。两者的主要区别在于，GCN的权重是固定的并且已经归一化，而空间卷积的滤波器权重是可学习的，并且没有归一化。从某种意义上说，我们也可以将GCN视为一种基于空间的方法。
- en: We'll continue this section with a specific type of spatial-based model called
    the **Graph Attention Network** (**GAT**) (for more information, go to [https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)),
    which implements graph convolutions with a special graph self-attention layer.
    Instead of learning a convolutional filter or using the averaged adjacency matrix
    as a GCN, GAT uses the attention scores of the self-attention mechanism to assign
    weights to each of the neighboring nodes. The GAT layer is the main building block
    of graph attention networks, which consist of multiple stacked GAT layers. As
    with GCN, each additional layer increases the receptive field of the target node.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节继续讨论一种特定类型的基于空间的模型，称为**图注意力网络**（**GAT**）（更多信息，请访问 [https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)），它通过一个特殊的图自注意力层实现图卷积。与学习卷积滤波器或使用平均邻接矩阵作为GCN不同，GAT使用自注意力机制的注意力得分为每个邻近节点分配权重。GAT层是图注意力网络的主要构建模块，图注意力网络由多个堆叠的GAT层组成。与GCN一样，每增加一层，目标节点的感受野也会增加。
- en: 'Similar to GCN, the GAT layer takes as input a set of node feature vectors
    ![](img/c8d92091-850b-429b-9887-f9951f2b976f.png) and outputs a different set
    of feature vectors ![](img/94b3890d-c654-4568-a6e0-7b28defceaa8.png), not necessarily
    of the same cardinality. Following the procedure we outlined in [Chapter 8](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml), *Sequence-to-Sequence
    Models and Attention*, the operation starts by computing the alignment scores
    between the feature vectors ![](img/22e50d7b-c169-4fe9-9244-60ca93fa9882.png) and
    ![](img/25e3efe7-32af-43ac-8b81-e7d78773b0fd.png) of each two nodes of the neighborhood:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于GCN，GAT层输入一组节点特征向量 ![](img/c8d92091-850b-429b-9887-f9951f2b976f.png)，并输出一组不同的特征向量
    ![](img/94b3890d-c654-4568-a6e0-7b28defceaa8.png)，这些输出特征向量的个数不一定与输入的相同。按照我们在[第8章](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml)《序列到序列模型与注意力》一节中概述的过程，操作开始时会计算特征向量之间的对齐得分
    ![](img/22e50d7b-c169-4fe9-9244-60ca93fa9882.png) 和 ![](img/25e3efe7-32af-43ac-8b81-e7d78773b0fd.png)，这些向量对应着每两个节点的邻域关系：
- en: '![](img/e6e85a94-9570-4d47-9407-50a8683c25e8.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6e85a94-9570-4d47-9407-50a8683c25e8.png)'
- en: 'Here, ![](img/dda45c47-d17b-4ed9-a7ed-3d28f1e198fb.png) is a weight matrix
    that transforms the input vectors to the cardinality of the output vectors and
    provides the necessary learnable parameters. The *f[a]* expression is a simple
    FFN with a single layer and LeakyReLU activation, which is parameterized by a
    weight vector ![](img/722ace34-34b7-4991-a8e3-5f3ab6c05625.png) and implements
    the additive attention mechanism:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/dda45c47-d17b-4ed9-a7ed-3d28f1e198fb.png) 是一个权重矩阵，它将输入向量转换为输出向量的个数，并提供必要的可学习参数。表达式
    *f[a]* 是一个简单的前馈神经网络（FFN），具有单层和LeakyReLU激活函数，它由权重向量 ![](img/722ace34-34b7-4991-a8e3-5f3ab6c05625.png)
    参数化，并实现加性注意力机制：
- en: '![](img/c7f67757-7c1b-422f-9386-391ea231573b.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7f67757-7c1b-422f-9386-391ea231573b.png)'
- en: Here, ![](img/cfa05b0f-6474-45b1-9e53-9721753eb371.png) represents concatenation. If
    we don't impose any restrictions, each node will be able to attend to all other
    nodes of the graph, regardless of their proximity to the target node; however,
    we're only interested in the neighboring nodes. The authors of GAT propose to
    solve this by using masked attention, where the mask covers all nodes that are
    not immediate neighbors of the target node. We'll denote the immediate neighbors
    of node *i* with ![](img/aff2d979-9d86-46ce-a910-60474eb51ada.png).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/cfa05b0f-6474-45b1-9e53-9721753eb371.png)表示拼接。如果我们不施加任何限制，每个节点将能够关注图中所有其他节点，无论它们与目标节点的距离如何；然而，我们只对邻居节点感兴趣。GAT的作者建议通过使用掩蔽注意力来解决这个问题，其中掩蔽覆盖了所有不是目标节点直接邻居的节点。我们将节点*i*的直接邻居表示为![](img/aff2d979-9d86-46ce-a910-60474eb51ada.png)。
- en: 'Next, we compute attention scores by using softmax. The following are the generic formula
    and the formula with *f[a]* (applied only over the immediate neighbors):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过使用softmax计算注意力得分。以下是通用公式和带有*f[a]*的公式（仅应用于直接邻居）：
- en: '![](img/d88cf72d-7ec5-4a14-a934-b8d252571566.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d88cf72d-7ec5-4a14-a934-b8d252571566.png)'
- en: 'Once we have the attention scores, we can use them to compute the final output
    feature vector of each node (we referred to this as the context vector in [Chapter
    8](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml), *Sequence-to-Sequence Models and
    Attention*), which is a weighted combination of the input feature vectors of all
    neighbors:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到注意力得分，就可以使用它们来计算每个节点的最终输出特征向量（我们在[第8章](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml)，*序列到序列模型与注意力*中称之为上下文向量），它是所有邻居输入特征向量的加权组合：
- en: '![](img/78546e42-dca7-43b1-9697-59eab1457c33.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78546e42-dca7-43b1-9697-59eab1457c33.png)'
- en: 'Here, σ is the sigmoid function. The authors of the paper also found multihead
    attention to be beneficial to the performance of the model:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，σ是sigmoid函数。论文的作者还发现多头注意力有利于模型的性能：
- en: '![](img/88571404-92dd-4633-b741-4b6ee1978dcc.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88571404-92dd-4633-b741-4b6ee1978dcc.png)'
- en: 'Here, *k* is the index of each head (for a total of *K* heads), ![](img/8969baab-d817-478e-b2fe-06e80d6affc7.png) are
    the attention scores for each attention head, and ![](img/152f315b-ca65-41dc-a5b5-2d1e1a86a10b.png)is
    the weight matrix of each attention head. Since ![](img/2ce5f52e-452e-46cb-8bf7-2bdd2ed5d3a8.png) is
    a result of concatenation, its cardinality will be *k × z[l+1]*. Because of this,
    concatenation is not possible in the final attention layer of the network. To
    solve this, the authors of the paper suggest that you should average the outputs
    of the attention heads in the final layer (denoted with index *L*):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*k*是每个头的索引（总共有*K*个头），![](img/8969baab-d817-478e-b2fe-06e80d6affc7.png)是每个注意力头的注意力得分，![](img/152f315b-ca65-41dc-a5b5-2d1e1a86a10b.png)是每个注意力头的权重矩阵。由于![](img/2ce5f52e-452e-46cb-8bf7-2bdd2ed5d3a8.png)是拼接的结果，它的基数将是*k
    × z[l+1]*。因此，在网络的最终注意力层中无法进行拼接。为了解决这个问题，论文的作者建议在最后一层中平均注意力头的输出（用索引*L*表示）：
- en: '![](img/07cb2578-7a7a-48e3-ad4c-2f725c517613.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07cb2578-7a7a-48e3-ad4c-2f725c517613.png)'
- en: 'The following diagram shows a comparison between regular and multihead attention
    in the GAT context. In the left image, we can see the regular attention mechanism,
    applied between two nodes and *i* and *j*. In the right image, we can see the
    multihead attention with *k = 3* heads of node *1* with its neighborhood. The
    aggregated features are either concatenated (for all hidden GAT layers) or averaged
    (for the final GAT layer):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了在GAT上下文中常规注意力和多头注意力的比较。在左侧的图像中，我们可以看到应用于两个节点*i*和*j*之间的常规注意力机制。在右侧的图像中，我们可以看到节点*1*与其邻域之间的多头注意力，*k
    = 3*表示有3个头。聚合后的特征可以是拼接（对于所有隐藏的GAT层）或者平均（对于最终的GAT层）：
- en: '![](img/7ec5f308-a34d-4a98-9ad7-2afb2b22536f.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ec5f308-a34d-4a98-9ad7-2afb2b22536f.png)'
- en: 'Left: Regular attention over two nodes; Right: Multihead attention of node
    1 with its neighborhood. Source: https://arxiv.org/abs/1710.10903'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 左：两个节点之间的常规注意力；右：节点1与其邻域之间的多头注意力。来源：https://arxiv.org/abs/1710.10903
- en: Once we have the output of the final GAT layer, we can use it as input to the
    next task-specific layers. For example, this could be a fully connected layer
    with softmax activation for node classification.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到最终GAT层的输出，就可以将其作为输入传递到下一个特定任务的层。例如，这可能是一个带有softmax激活的全连接层，用于节点分类。
- en: 'Before we conclude this section devoted to ConvGNNs, let''s discuss two final
    components that we haven''t addressed yet. The first is the readout layer that
    we introduced in the graph-level classification example at the beginning of the *Convolutional*
    *Graph* *Networks* section. It takes as input all the node states of the last
    graph convolutional layer **H**^((*L*)) and outputs a single vector that summarizes
    the whole graph. We can define it formally as the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们总结这一节关于ConvGNN的内容之前，让我们讨论一下两个我们尚未涉及的最后组成部分。第一个是我们在*卷积* *图* *网络*部分开头的图级分类示例中介绍的读出层。它的输入是最后一层图卷积层的所有节点状态**H**^((*L*))，并输出一个总结整个图的单一向量。我们可以将其正式定义如下：
- en: '![](img/02793365-ff3a-479e-bef6-59c93a6c6798.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02793365-ff3a-479e-bef6-59c93a6c6798.png)'
- en: Here, *G* represents the set of graph nodes and *R* is the readout function.
    There are various ways to implement it, but the simplest is to take the element-wise
    sum or mean of all node states.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*G*表示图的节点集合，*R*是读出函数。实现它的方法有很多种，但最简单的方法是对所有节点状态进行逐元素求和或求平均。
- en: 'The next (and final) ConvGNN component we''ll look at is the pooling operation.
    Once again, there are various ways to use this, but one of the simplest is to
    use the same max/average pooling operations as we did in the computer vision convolutions:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要看待的下一个（也是最后一个）ConvGNN组件是池化操作。再次强调，有多种方法可以使用池化操作，但最简单的一种是像我们在计算机视觉卷积中使用的最大池化/平均池化操作：
- en: '![](img/20c1fb51-9f9a-46b6-a295-1e57bf879451.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20c1fb51-9f9a-46b6-a295-1e57bf879451.png)'
- en: Here, *p* indicates the size of the pooling window. If the pooling window contains
    the whole graph, the pooling becomes similar to the readout.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*p*表示池化窗口的大小。如果池化窗口包含整个图，则池化操作变得类似于读出操作。
- en: This concludes our discussion about ConvGNNs. In the next section, we'll discuss
    graph autoencoders, which provide a way to generate new graphs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们关于ConvGNN的讨论。在下一节中，我们将讨论图自编码器，它提供了一种生成新图的方法。
- en: Graph autoencoders
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图自编码器
- en: Let's have a quick recap of autoencoders, which we first introduced in [Chapter
    5](319c18b2-c733-402e-937c-ace912ff87ca.xhtml),*Generative Models*. An **autoencoder**
    is an FFN that tries to reproduce its input (more accurately, it tries to learn
    an identity function, ![](img/9b1a90cb-7d76-4d0a-b2ac-ab69e9a8f07e.png)). We can
    think of the autoencoder as a virtual composition of two components—the **encoder**,
    which maps the input data to the network's internal latent feature space (represented
    as vector *z*)*,* and the **decoder**, which tries to reconstruct the input from
    the network's internal data representation. We can train the autoencoder in an
    unsupervised way by minimizing a loss function (known as a **reconstruction error)**,
    which measures the distance between the original input and its reconstruction.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速回顾一下自编码器，我们在[第五章](319c18b2-c733-402e-937c-ace912ff87ca.xhtml)的*生成模型*中首次介绍过。**自编码器**是一个前馈神经网络（FFN），它试图重建其输入（更准确地说，它试图学习一个恒等函数，![](img/9b1a90cb-7d76-4d0a-b2ac-ab69e9a8f07e.png)）。我们可以将自编码器看作是两个组件的虚拟组合——**编码器**，它将输入数据映射到网络的内部潜在特征空间（表示为向量*z*），以及**解码器**，它试图从网络的内部数据表示中重建输入。我们可以通过最小化一个损失函数（称为**重建误差**）以无监督的方式训练自编码器，该损失函数衡量原始输入与其重建之间的距离。
- en: '**Graph autoencoders** (**GAE**) are similar to autoencoders, with the distinction
    that the encoder maps the graph nodes into the autoencoder latent feature space
    and then the decoder tries to reconstruct specific graph features from it. In
    this section, we''ll discuss a GAE variant, introduced in *Variational Graph Auto-Encoders*
    ([https://arxiv.org/abs/1611.07308](https://arxiv.org/abs/1611.07308)), which
    also outlines the variational version of GAE (**VGAE**). The following diagram shows
    an example GAE:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**图自编码器**（**GAE**）类似于自编码器，区别在于编码器将图节点映射到自编码器的潜在特征空间，然后解码器尝试从中重建特定的图特征。在这一节中，我们将讨论一种GAE的变体，该变体在《变分图自编码器》([https://arxiv.org/abs/1611.07308](https://arxiv.org/abs/1611.07308))中介绍，同时也概述了GAE的变分版本（**VGAE**）。下图展示了一个GAE的示例：'
- en: '![](img/ad20131f-4b1e-4cd7-b137-7c2e4b24725b.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad20131f-4b1e-4cd7-b137-7c2e4b24725b.png)'
- en: An example of graph autoencoder. Source: https://arxiv.org/abs/1901.00596
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图自编码器的示例。来源：[https://arxiv.org/abs/1901.00596](https://arxiv.org/abs/1901.00596)
- en: 'The encoder is a GCN model that we defined in the *Spectral-based convolutions* section to
    compute a network embedding![](img/e799be66-a517-4c9e-b202-62de22c1ae57.png) for
    graph nodes, where the embedding for each of the *n* total nodes is a *d*-dimensional
    vector **z**. It takes as input the adjacency matrix **A** and the set of node
    feature vectors **X** (like the other GNN models we discussed in this chapter).
    The encoder is represented by the following formula:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器是我们在 *基于谱的卷积* 部分定义的 GCN 模型，用来计算图节点的网络嵌入！[](img/e799be66-a517-4c9e-b202-62de22c1ae57.png)，其中每个
    *n* 个节点的嵌入是一个 *d* 维向量 **z**。它以邻接矩阵 **A** 和节点特征向量集合 **X** 作为输入（像本章讨论的其他 GNN 模型一样）。编码器由以下公式表示：
- en: '![](img/d0753995-166a-4769-8d36-7d50faced583.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0753995-166a-4769-8d36-7d50faced583.png)'
- en: Here, **W[1]** and **W[2]** are the learnable parameters (weights) of the two
    GCN graph convolutions, and *f* is a nonlinear activation function, like ReLU. The
    authors of the paper use two graph convolutional layers, although the proposed
    algorithm can work for any number of layers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**W[1]** 和 **W[2]** 是两个 GCN 图卷积层的可学习参数（权重），*f* 是一个非线性激活函数，如 ReLU。论文的作者使用了两个图卷积层，尽管所提出的算法可以适用于任何数量的层。
- en: 'The decoder tries to reconstruct the graph adjacency matrix ![](img/3f98fb9c-d5e9-454b-9459-6b3c28bfe9c7.png):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器试图重建图的邻接矩阵 ![](img/3f98fb9c-d5e9-454b-9459-6b3c28bfe9c7.png)：
- en: '![](img/2ccf67b6-cb38-47a2-b535-a03c5c14aa87.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ccf67b6-cb38-47a2-b535-a03c5c14aa87.png)'
- en: 'Here, σ is the sigmoid function. It first computes the dot (or inner) product
    between **Z** and its transpose: ![](img/c60a2b16-86cd-4a4a-b5ec-5a9ced360624.png). To
    clarify, this operation computes a dot product of the vector embedding *z[i]*
    of each node *i* and the vector embedding *z[j]* of every other node *j* of the
    graph, as shown in the following example:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，σ 是 sigmoid 函数。它首先计算 **Z** 和其转置之间的点积（或内积）：![](img/c60a2b16-86cd-4a4a-b5ec-5a9ced360624.png)。为了明确，这个操作计算了图中每个节点
    *i* 的向量嵌入 *z[i]* 与所有其他节点 *j* 的向量嵌入 *z[j]* 之间的点积，如下例所示：
- en: '![](img/8f72a2b2-988f-4043-a076-8ad4bc89f926.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f72a2b2-988f-4043-a076-8ad4bc89f926.png)'
- en: As we mentioned in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts of Neural Networks*, we can think of the dot product as a
    similarity measure between vectors. Therefore, ![](img/2921f759-7c77-4965-95a0-a5832062c067.png) measures
    the distance between every possible pair of nodes. These distances serve as a
    base for the reconstruction effort. After this, the decoder applies a nonlinear
    activation function and proceeds to reconstruct the graph adjacency matrix. We
    can train the GAE by minimizing the discrepancy between the real reconstructed
    adjacency matrices.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [第 1 章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml) *《神经网络的基础》* 中提到的，我们可以将点积看作是向量之间的相似度度量。因此，![](img/2921f759-7c77-4965-95a0-a5832062c067.png)
    测量了每对节点之间的距离。这些距离为重建过程提供了基础。之后，解码器应用一个非线性激活函数，继续重建图的邻接矩阵。我们可以通过最小化实际重建邻接矩阵之间的差异来训练
    GAE。
- en: 'Next, let''s focus on **Variational Graph Autoencoders** (**VGAE**). Much like
    the **Variational Autoencoders** (**VAE**) we discussed in [Chapter 5](319c18b2-c733-402e-937c-ace912ff87ca.xhtml),
    *Generative Models*, the VGAE is a generative model that can generate new graphs
    (more specifically, new adjacency matrices). To understand this, let''s start
    with a short recap of VAEs. Unlike regular autoencoders, the VAE bottleneck layer
    won''t directly output latent state vectors. Instead, it will output two vectors,
    which describe the **mean** μ and the **variance** σ of the distribution of the
    latent vector **z**. We''ll use them to sample a random vector ε with the same
    dimensions as **z** from a Gaussian distribution. More specifically, we''ll shift
    ε by the latent distribution''s mean μ and scale it by the latent distribution''s
    variance σ:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们关注 **变分图自编码器**（**VGAE**）。与我们在 [第 5 章](319c18b2-c733-402e-937c-ace912ff87ca.xhtml)
    *《生成模型》* 中讨论的 **变分自编码器**（**VAE**）类似，VGAE 是一种生成模型，可以生成新的图（更具体地说，是新的邻接矩阵）。为了理解这一点，我们先回顾一下
    VAE。与常规自编码器不同，VAE 的瓶颈层不会直接输出潜在状态向量。而是会输出两个向量，描述潜在向量 **z** 的分布的 **均值** μ 和 **方差**
    σ。我们将使用它们从高斯分布中采样一个与 **z** 相同维度的随机向量 ε。更具体地说，我们将通过潜在分布的均值 μ 来偏移 ε，并通过潜在分布的方差 σ
    来缩放它：
- en: '![](img/b8e54537-e06d-4f5f-aa11-7fccb06056bc.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8e54537-e06d-4f5f-aa11-7fccb06056bc.png)'
- en: This technique is known as the **reparameterization** trick, and it allows the
    random vector to have the same mean and variance as the original dataset.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这项技术被称为**重参数化**技巧，它使得随机向量具有与原始数据集相同的均值和方差。
- en: 'We can think of VGAE as a combination of GAE and VAE in the sense that it works
    with graph inputs (such as GAE) and follows the same principles to generate new
    data (like VAE). First, let''s focus on the encoder, which is split into two paths:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将VGAE看作GAE和VAE的结合体，因为它处理图输入（类似于GAE），并遵循相同的原理生成新数据（像VAE一样）。首先，让我们关注编码器，它被分成两个路径：
- en: '![](img/79b471f1-7bc7-43e7-ae07-ea9e44021ce1.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79b471f1-7bc7-43e7-ae07-ea9e44021ce1.png)'
- en: 'Here, the weights **W[0]** are shared between the paths, *![](img/37a71eea-c7d9-44c7-9af8-f0b2cc2217cc.png)* is
    the symmetrically normalized adjacency matrix, ***μ*** is the matrix of mean vectors ***μ**[i]*,
    and ***σ*** is the matrix of variances ***σ**[i]* of each graph node. Then, the
    encoder inference step for the full graph is defined as the inner product of the
    latent representations of all graph nodes *i*:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，权重**W[0]**在各路径之间共享，*![](img/37a71eea-c7d9-44c7-9af8-f0b2cc2217cc.png)*是对称归一化的邻接矩阵，***μ***是均值向量***μ**[i]*的矩阵，***σ***是每个图节点的方差矩阵***σ**[i]*。然后，完整图的编码器推理步骤被定义为所有图节点*i*的潜在表示的内积：
- en: '![](img/4b4675d4-526e-406a-9f32-a24fb0b4444b.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b4675d4-526e-406a-9f32-a24fb0b4444b.png)'
- en: 'In this formula, *n* is the number of nodes in the graph and ![](img/bf7e4027-8056-49cc-ba8d-d2bdc0e9618e.png) represents the
    encoder approximation of the real probability distribution ![](img/9451f080-cacc-4328-a19d-760505c64b27.png),
    where *φ* is the network parameters (here, we have preserved the notation of [Chapter
    5](319c18b2-c733-402e-937c-ace912ff87ca.xhtml), *Generative Models*). The approximation
    is a Gaussian distribution with node-specific mean *μ[i]* and diagonal covariance
    values ![](img/2f51c5e2-c99d-47ca-bb62-40c4b90271e7.png):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，*n*是图中的节点数，![](img/bf7e4027-8056-49cc-ba8d-d2bdc0e9618e.png)表示解码器对真实概率分布![](img/9451f080-cacc-4328-a19d-760505c64b27.png)的近似，其中*φ*是网络参数（在这里，我们保留了[第5章](319c18b2-c733-402e-937c-ace912ff87ca.xhtml)《生成模型》中的符号表示）。该近似是一个高斯分布，节点特定的均值为*μ[i]*，对角协方差值为![](img/2f51c5e2-c99d-47ca-bb62-40c4b90271e7.png)：
- en: '![](img/0778279e-d32e-46ba-8829-8147a91ed5ae.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0778279e-d32e-46ba-8829-8147a91ed5ae.png)'
- en: 'Next, we define the generative step, which creates the new adjacency matrix.
    It is an inner product of the random latent vectors:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义生成步骤，生成新的邻接矩阵。它是随机潜在向量的内积：
- en: '![](img/033b088c-576d-467d-b720-253c5476b649.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/033b088c-576d-467d-b720-253c5476b649.png)'
- en: 'Here, ![](img/9ca6cbb0-7521-41d1-a4d5-7c34c4c78dd7.png) indicates whether an
    edge exists between two nodes *i* and *j*, and ![](img/154fcf49-1337-4f63-982d-b95af38be050.png) represents
    the decoder approximation of the real probability ![](img/7daf605e-a4ff-4848-85e4-4628a726bec9.png). We
    can train the VGAE using the already familiar VAE cost:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/9ca6cbb0-7521-41d1-a4d5-7c34c4c78dd7.png)表示节点*i*和节点*j*之间是否存在边，![](img/154fcf49-1337-4f63-982d-b95af38be050.png)表示解码器对真实概率![](img/7daf605e-a4ff-4848-85e4-4628a726bec9.png)的近似。我们可以使用已经熟悉的VAE代价来训练VGAE：
- en: '![](img/98b1075f-740f-4e14-bae3-2ef89538d3d8.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98b1075f-740f-4e14-bae3-2ef89538d3d8.png)'
- en: Here, the first term is the Kullback–Leibler divergence and the second is the
    reconstruction cost.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，第一项是Kullback–Leibler散度，第二项是重构代价。
- en: This concludes our description of GAE and VGAE. In the next section, we'll discuss
    yet another graph-learning paradigm, which makes it possible to mix structured
    and unstructured data as network inputs.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对GAE和VGAE的描述。在接下来的章节中，我们将讨论另一种图学习范式，它使得可以将结构化和非结构化数据作为网络输入进行混合。
- en: Neural graph learning
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经图学习
- en: 'In this section, we''ll describe the **Neural Graph Learning** paradigm (**NGL**)
    (for more information, see *Neural Graph Learning: Training Neural Networks Using
    Graphs* at [https://storage.googleapis.com/pub-tools-public-publication-data/pdf/bbd774a3c6f13f05bf754e09aa45e7aa6faa08a8.pdf](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/bbd774a3c6f13f05bf754e09aa45e7aa6faa08a8.pdf)),
    which makes it possible to augment training based on unstructured data with structured
    signals. More specifically, we''ll discuss the **neural structured learning**
    framework (**NSL**) (for more information, go to [https://www.tensorflow.org/neural_structured_learning/](https://www.tensorflow.org/neural_structured_learning/)),
    which is based on TensorFlow 2.0 and implements these principles.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将描述**神经图学习**范式（**NGL**）（更多信息请参见*Neural Graph Learning: Training Neural
    Networks Using Graphs*，链接：[https://storage.googleapis.com/pub-tools-public-publication-data/pdf/bbd774a3c6f13f05bf754e09aa45e7aa6faa08a8.pdf](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/bbd774a3c6f13f05bf754e09aa45e7aa6faa08a8.pdf)），它使得基于非结构化数据的训练能够与结构化信号相结合。更具体地说，我们将讨论基于TensorFlow
    2.0的**神经结构化学习**框架（**NSL**）（更多信息，请访问[https://www.tensorflow.org/neural_structured_learning/](https://www.tensorflow.org/neural_structured_learning/)），该框架实现了这些原理。'
- en: To understand how NGL works, we'll use the CORA dataset ([https://relational.fit.cvut.cz/dataset/CORA](https://relational.fit.cvut.cz/dataset/CORA)),
    which consists of 2,708 scientific publications classified into 1 of 7 classes
    (this is the unstructured part of the dataset). The number of unique words in
    all publications (that is, the vocabulary) in the dataset is 1,433\. Each publication
    is described as a single **multihot** encoded vector. This is a vector of size
    1,433 (the same as the vocabulary), where the cell values are either 0 or 1\.
    If a publication contains the *i-*th word of the vocabulary, then the *i*th cell
    of the one-hot encoded vector of that publication is set to 1\. If the word is
    not present in the publication, the cell is set to 0\. This mechanism preserves
    information about the words present in an article, but not information about their
    order. The dataset also contains a directed graph of 5,429 citations, where the
    nodes are publications and the edges between them indicate whether publication
    *v* cites publication *u* (this is the structured part of the dataset).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解NGL是如何工作的，我们将使用CORA数据集（[https://relational.fit.cvut.cz/dataset/CORA](https://relational.fit.cvut.cz/dataset/CORA)），该数据集由2,708篇科学出版物组成，分为7个类别之一（这是数据集的非结构化部分）。数据集中所有出版物中的唯一词汇量（即词汇表）为1,433个。每篇出版物都被描述为一个**multihot**编码的向量。这个向量的大小为1,433（与词汇表大小相同），其中单元格的值为0或1。如果某篇出版物包含词汇表中的第*i*个单词，那么该出版物的one-hot编码向量的第*i*个单元格被设为1。如果该词汇不出现在该出版物中，则单元格为0。此机制保留了文章中存在的单词信息，但不保留单词的顺序信息。数据集还包含一个有向图，表示5,429个引用关系，其中节点是出版物，节点之间的边表示出版物*v*是否引用出版物*u*（这是数据集的结构化部分）。
- en: 'Next, let''s focus on NGL itself, starting with the following diagram:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们聚焦于NGL本身，从以下图表开始：
- en: '![](img/3465ccd4-42ab-4e41-9ebb-38a0638e7460.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3465ccd4-42ab-4e41-9ebb-38a0638e7460.png)'
- en: 'The NGL framework: green solid lines show the unstructured input data flow;
    yellow dashed lines show the structured signals data flow; Inspired by: https://www.tensorflow.org/neural_structured_learning/framework'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: NGL框架：绿色实线表示非结构化输入数据流；黄色虚线表示结构化信号数据流；灵感来源：[https://www.tensorflow.org/neural_structured_learning/framework](https://www.tensorflow.org/neural_structured_learning/framework)
- en: It acts as a kind of wrapper over the regular NN training framework, and it
    can be applied over any type of network, including FFN and RNN. For example, we
    can have a regular FFN, which takes as input the multihot encoded publication
    vector and tries to classify it to one of the 7 classes, using softmax output,
    as illustrated in the preceding diagram with green uninterrupted lines. NGL allows
    us to extend this network with structured data, offered by the citations, as illustrated
    by the yellow dashed lines.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 它作为常规神经网络训练框架的一种封装，可以应用于任何类型的网络，包括FFN和RNN。例如，我们可以使用常规的FFN，它将**multihot**编码的出版物向量作为输入，尝试将其分类为7个类别之一，使用softmax输出，正如前面图表中绿色实线所示。NGL使我们能够通过引用提供的结构化数据来扩展这个网络，如黄色虚线所示。
- en: 'Let''s look at how this works. We start with the assumption that neighboring
    nodes in the graph are somewhat similar. We can transfer this assumption to the
    NN domain by saying that the embedding vector produced by the NN (the embedding
    is the output of the last hidden layer) of sample *i* should be somewhat similar
    to the embedding of sample *j*, provided that the two samples are neighbors in
    the associated graph. In our example, we can assume that the embedding vector
    of publication *i* should be similar to the embedding of publication *j*, provided
    that one of them cites the other (that is, they are neighbors in the graph of
    citations). In practice, we can implement this with the following steps:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个是如何工作的。我们从假设图中的相邻节点是有些相似的开始。我们可以将这个假设转移到神经网络领域，假设样本 *i* 通过神经网络生成的嵌入向量（嵌入是最后一层隐藏层的输出）应该与样本
    *j* 的嵌入向量相似，前提是这两个样本在关联图中是邻居。在我们的例子中，我们可以假设，如果出版物 *i* 引用了出版物 *j*（即它们在引用图中是邻居），那么出版物
    *i* 的嵌入向量应该与出版物 *j* 的嵌入向量相似。实际上，我们可以通过以下步骤来实现这一点：
- en: Start with a dataset that contains both unstructured data (multihot-encoded
    publications) and structured data (the graph of citations).
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个包含非结构化数据（多热编码出版物）和结构化数据（引用图）的数据集开始。
- en: Build special types of composite training samples (organized in batches), where
    each composite sample consists of a single regular input sample (one multihot-encoded
    publication) and *K* of its neighboring samples (the multihot-encoded publications
    that cite or are cited by the initial sample).
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建特殊类型的复合训练样本（按批组织），其中每个复合样本由单一常规输入样本（一个多热编码出版物）和 *K* 个邻居样本（引用或被引用的多热编码出版物）组成。
- en: Feed the composite sample to the NN and produce embeddings for both the initial
    sample and its neighbors. Although the preceding diagram shows the two paths running
    in parallel, this is not the case. The diagram aims to illustrate that the network
    processes both the central sample and its neighbors, but the actual NN is not
    privy to this arrangement—it just takes all of the multihot-encoded inputs as
    part of a single batch and processes them. Instead, the NSL portion on top of
    the regular NN differentiates the two components.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将复合样本输入到神经网络中，生成初始样本及其邻居的嵌入向量。尽管前面的图示显示这两条路径是并行运行的，但实际情况并非如此。该图旨在说明网络处理了中央样本及其邻居，但实际的神经网络并不清楚这种安排——它只是将所有多热编码的输入作为单个批次的一部分进行处理。相反，位于常规神经网络上方的NSL部分区分了这两个组件。
- en: 'Compute a special type of composite loss function composed of two parts: regular
    supervised loss and regularization neighbor loss, which uses a metric to measure
    the distance between the initial sample embedding and the embedding of its neighbors.
    The neighbor loss is the mechanism that allows us to augment unstructured training
    data with structured signals. The composite loss is defined as follows:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算一种特殊类型的复合损失函数，由两部分组成：常规监督损失和正则化邻居损失，使用度量来衡量初始样本嵌入与其邻居嵌入之间的距离。邻居损失是我们用结构化信号增强非结构化训练数据的机制。复合损失定义如下：
- en: '![](img/e4f50ba5-e319-41ab-8440-81d100e27716.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4f50ba5-e319-41ab-8440-81d100e27716.png)'
- en: 'This formula has the following features:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式具有以下特点：
- en: '*n* is the number of composite samples in the mini-batch.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*n* 是小批量中的复合样本数量。'
- en: '![](img/7ea30bdb-fb32-4c9b-9c37-4ebfca7b74f5.png) is the supervised loss function.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/7ea30bdb-fb32-4c9b-9c37-4ebfca7b74f5.png) 是监督损失函数。'
- en: '*f[θ]* is the NN function with weights *θ.*'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f[θ]* 是具有权重 *θ* 的神经网络函数。'
- en: '*α* is a scalar parameter that determines the relative weight between the two
    loss components.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*α* 是一个标量参数，确定两种损失组件之间的相对权重。'
- en: '![](img/e380c730-c4df-4fa8-b86d-afe275fc6ed0.png) is the set of graph neighbors
    of sample *x[i]*. Note that the neighbor loss iterates over all neighbors of all
    nodes of the graph (two sums).'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/e380c730-c4df-4fa8-b86d-afe275fc6ed0.png) 是样本 *x[i]* 的图邻居集合。请注意，邻居损失遍历图中所有节点的所有邻居（两个求和）。'
- en: '![](img/8dae52a2-49ba-4628-8a68-369127f9643c.png) is the weight of the graph
    edge between samples *i* and *j*. If the task doesn''t have a notion of weights,
    we can assume that all weights are 1.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/8dae52a2-49ba-4628-8a68-369127f9643c.png) 是样本 *i* 和 *j* 之间图边的权重。如果任务没有权重的概念，我们可以假设所有权重都为1。'
- en: '![](img/9820c380-205c-4127-8628-991725805d52.png) is the distance metric between
    the embedding vectors of samples *i* and *j*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/9820c380-205c-4127-8628-991725805d52.png) 是样本 *i* 和 *j* 之间嵌入向量的距离度量。'
- en: Because of the regularization nature of the neighbor loss, NGL is also referred
    to as **graph regularization**.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 由于邻居损失的正则化特性，NGL 也被称为 **图正则化**。
- en: Propagate the error backward and update the network weights *θ*.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播误差并更新网络权重 *θ*。
- en: Now that we have an overview of graph regularization, let's implement it.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对图正则化有了一个概览，让我们开始实现它。
- en: Implementing graph regularization
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现图正则化
- en: In this section, we'll implement graph regularization over the Cora dataset
    with the help of the NSL framework. This example is based on the tutorial available
    at [https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_mlp_cora](https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_mlp_cora).
    Before we proceed with the implementation, we have to satisfy some prerequisites.
    First, we need TensorFlow 2.0 and the `neural-structured-learning` 1.1.0 package
    (available via `pip`).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在 NSL 框架的帮助下实现对 Cora 数据集的图正则化。这个示例基于[https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_mlp_cora](https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_mlp_cora)中的教程。在开始实现之前，我们需要满足一些前提条件。首先，我们需要
    TensorFlow 2.0 和 `neural-structured-learning` 1.1.0 包（可以通过 `pip` 安装）。
- en: 'Once we satisfy these requirements, we can proceed with the implementation:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦满足这些要求，我们就可以继续实现：
- en: 'We''ll start with the package imports:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先进行包的导入：
- en: '[PRE0]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We''ll continue with some constant parameters of the program (hopefully the
    constant names and the comments speak for themselves):'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将继续定义程序中的一些常量参数（希望常量名称和注释能够自解释）：
- en: '[PRE1]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The files under `TRAIN_DATA_PATH` and `TEST_DATA_PATH` contain the Cora dataset
    and labels, preprocessed in a TensorFlow-friendly format.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 位于 `TRAIN_DATA_PATH` 和 `TEST_DATA_PATH` 下的文件包含经过 TensorFlow 友好格式处理的 Cora 数据集和标签。
- en: 'Next, let''s load the dataset. This process is implemented by using two functions: `make_dataset`,
    which builds the whole dataset, and `parse_example`, which parses a single composite
    sample (`make_dataset` uses `parse_example` internally). We''ll start with `make_dataset`:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们加载数据集。这个过程通过两个函数实现：`make_dataset`，它构建整个数据集，以及 `parse_example`，它解析单个复合样本（`make_dataset`
    内部使用 `parse_example`）。我们将从 `make_dataset` 开始：
- en: '[PRE2]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that `dataset.map(parse_example)` internally applies `parse_example` over
    all samples of the dataset. Let''s continue with the definition of `parse_example` ,
    starting from the declaration:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`dataset.map(parse_example)` 内部会对数据集中的所有样本应用 `parse_example`。我们将继续定义 `parse_example`，从声明开始：
- en: '[PRE3]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The function creates the `feature_spec` dictionary that represents a kind of
    template for a single composite sample, which is later filled with actual data
    from the dataset. First, we fill `feature_spec` with the placeholder instances
    of `tf.io.FixedLenFeature` for `''words''`, which represents a multihot-encoded
    publication, and `''label''`, which represents the class of the publication (please
    bear in mind the indentation as this code is still part of `parse_example`):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数创建了 `feature_spec` 字典，表示单个复合样本的模板，稍后将用来自数据集的实际数据填充。首先，我们用 `tf.io.FixedLenFeature`
    的占位符实例填充 `feature_spec`，`'words'` 代表一个多热编码的出版物，`'label'` 代表出版物的类别（请注意缩进，因为这段代码仍然是
    `parse_example` 的一部分）：
- en: '[PRE4]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we iterate over the first `NUM_NEIGHBORS` neighbors and add their multihot
    vectors and edge weights to `feature_spec` under the `nbr_feature_key` and `nbr_weight_key`
    keys respectively:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们迭代前 `NUM_NEIGHBORS` 个邻居，并将它们的多热向量和边权重分别添加到 `feature_spec` 中的 `nbr_feature_key`
    和 `nbr_weight_key` 键下：
- en: '[PRE5]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Note that we populate the template with a real sample from the dataset with
    the following code snippet:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用以下代码片段将模板填充为来自数据集的真实样本：
- en: '[PRE6]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, we can instantiate the training and testing datasets:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以实例化训练和测试数据集：
- en: '[PRE7]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, let''s implement the model, which is a simple FFN with two hidden layers
    and softmax as output. The model takes the multihot-encoded publication vector
    as input and outputs the publication class. It is independent of NSL and can be
    trained, in a simple supervised way, as a classification:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实现模型，它是一个简单的 FFN，具有两个隐藏层和 softmax 输出。该模型将多热编码的出版物向量作为输入，并输出出版物的类别。它独立于
    NSL，可以以简单的监督方式作为分类进行训练：
- en: '[PRE8]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, let''s instantiate the model:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们实例化模型：
- en: '[PRE9]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have all the ingredients that we need to use graph regularization. We''ll
    start by wrapping the `model` with the NSL wrapper:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经准备好了进行图正则化所需的所有要素。我们将首先使用 NSL 包装器包装 `model`：
- en: '[PRE10]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We instantiate the `graph_reg_config` object (an instance of `nsl.configs.GraphRegConfig`)
    with the graph regularization parameters: `max_neighbors=NUM_NEIGHBORS` is the
    number of neighbors to use, `multiplier=0.1` is equivalent to the parameter α
    of the composite loss we introduced in the *Neural structured learning *section,
    and `distance_type=nsl.configs.DistanceType.L2` is the distance metric between
    the neighboring node embeddings.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实例化`graph_reg_config`对象（`nsl.configs.GraphRegConfig`的实例），并配置图正则化参数：`max_neighbors=NUM_NEIGHBORS`是使用的邻居数，`multiplier=0.1`相当于我们在*神经结构学习*章节中介绍的复合损失的参数α，`distance_type=nsl.configs.DistanceType.L2`是邻居节点嵌入之间的距离度量。
- en: 'Next, we can build a training framework and initiate the training for 100 epochs:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以构建一个训练框架，并开始进行100个epoch的训练：
- en: '[PRE11]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Once the training is done, we can run the trained model over the test dataset:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们可以在测试数据集上运行已训练的模型：
- en: '[PRE12]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If everything goes alright, the output of the program should be:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，程序的输出应为：
- en: '[PRE13]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This concludes our discussion about GNN. As we mentioned, there are various
    types of GNNs, and we only included a small set here. If you are interested in
    learning more, I suggest that you refer to the survey paper we introduced at the
    beginning of the section or check out the following curated list of GNN-related
    papers at [https://github.com/thunlp/GNNPapers](https://github.com/thunlp/GNNPapers).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对GNN的讨论。如我们所提到的，GNN有多种类型，而我们这里只包含了一个小的集合。如果你有兴趣了解更多，我建议你参考我们在本节开头提到的综述论文，或者查看以下整理好的GNN相关论文列表：[https://github.com/thunlp/GNNPapers](https://github.com/thunlp/GNNPapers)。
- en: In the next section, we'll discuss a new type of NN that uses external memory
    to store information.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论一种新型的神经网络（NN），它使用外部存储来保存信息。
- en: Introducing memory-augmented NNs
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍记忆增强神经网络
- en: We've already seen the concept of memory (albeit in a strange form) in NNs—for
    example, the LSTM cell can add or delete information on its hidden cell state
    with the help of the input and the forget gates. Another example is the attention
    mechanism, where the set of vectors that represent the encoded source sequence
    can be viewed as external memory that is written to by the encoder and read from
    by the decoder. But this ability comes with some limitations. For one, the encoder
    can only write to a single memory location, which is the current element of the
    sequence. It also cannot update previously written vectors. On the other hand,
    the decoder can only read from the database, but cannot write to it.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在神经网络中看到过记忆的概念（尽管以一种奇特的形式存在）——例如，LSTM单元可以借助输入门和遗忘门在其隐藏单元状态中添加或删除信息。另一个例子是注意力机制，其中代表编码源序列的向量集合可以视为外部记忆，编码器写入其中，解码器从中读取。但是这种能力也有一些局限性。例如，编码器只能写入一个单一的记忆位置，即当前序列元素，也无法更新先前写入的向量。另一方面，解码器只能从数据库中读取，无法写入。
- en: In this section, we'll take the concept of memory one step further and look
    at **Memory-Augmented NNs** (**MANNs**), which resolve these limitations. This
    is a new class of algorithm and is still in its early stages, unlike the more
    mainstream types of NN, such as convolutional and RNNs, which have been around
    for decades. The first MANN network we'll discuss is the neural Turing machine.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进一步探讨记忆的概念，介绍**记忆增强神经网络**（**MANNs**），它解决了这些局限性。这是一类新的算法，仍处于早期阶段，不像更为主流的神经网络类型（如卷积神经网络和RNN），这些类型已经存在了几十年。我们将讨论的第一个MANN网络是神经图灵机。
- en: Neural Turing machines
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经图灵机
- en: 'The concept of MANNs was first introduced with the concept of the **neural
    Turing machine** (**NTM**) (for more information, go to  [https://arxiv.org/abs/1410.5401](https://arxiv.org/abs/1410.5401)).
    The NTM has two components:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: MANN的概念首次通过**神经图灵机**（**NTM**）引入（更多信息，请访问[https://arxiv.org/abs/1410.5401](https://arxiv.org/abs/1410.5401)）。NTM有两个组件：
- en: A NN controller.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个神经网络控制器。
- en: An external memory, represented as a matrix ![](img/3a804a65-9cd9-4d40-ba70-790164f5d22b.png).
    The matrix contains *n* rows of *d*-dimensional vectors.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个外部记忆，表示为一个矩阵！[](img/3a804a65-9cd9-4d40-ba70-790164f5d22b.png)。该矩阵包含*n*行*d*维向量。
- en: 'The following diagram provides an overview of the NTM architecture:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 下图提供了NTM架构的概述：
- en: '![](img/63d5e3d7-f148-42f6-b002-0023aba080ea.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63d5e3d7-f148-42f6-b002-0023aba080ea.png)'
- en: NTM Source: https://arxiv.org/abs/1410.5401
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: NTM来源：[https://arxiv.org/abs/1410.5401](https://arxiv.org/abs/1410.5401)
- en: An NTM works in a sequential fashion (like an RNN), where the controller takes
    input vectors and produces output vectors in response. It also reads and writes
    to memory with the help of multiple parallel read/write heads.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: NTM 以顺序方式工作（像 RNN 一样），控制器接受输入向量并产生响应的输出向量。它还通过多个并行的读/写头帮助读取和写入记忆。
- en: 'Let''s focus on the reading operation, which is very similar to the attention
    mechanism we looked at in [Chapter 8](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml),
    *Sequence-to-Sequence Models and Attention*. A read head always reads the full
    memory matrix, but it does so by attending to different memory vectors with different
    intensities. To do this, the read head emits an *n*-dimensional vector ![](img/6daa56ae-2570-4259-8817-ab17c4c9ce79.png) (at
    step *t*) with the following constraints:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们集中讨论读取操作，它与我们在[第8章](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml)中讨论的注意力机制非常相似，*序列到序列模型与注意力*。读取头总是读取完整的记忆矩阵，但它通过以不同的强度关注不同的记忆向量来实现这一点。为此，读取头发出一个
    *n* 维向量 ![](img/6daa56ae-2570-4259-8817-ab17c4c9ce79.png)（在步骤 *t* 处），并满足以下约束：
- en: '![](img/1814d2d9-a6b2-452d-9a6d-b3bcc602ba3e.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1814d2d9-a6b2-452d-9a6d-b3bcc602ba3e.png)'
- en: 'The ![](img/6d2eb2e7-c39d-4d13-b8bb-caa46bb4c330.png) implements an attention
    mechanism, where each cell *i* of the vector indicates the weight of the *i*th
    memory vector (that is, the *i*th row of the matrix **M**) in forming the output.
    The output of a read operation at step *t* is a *d*-dimensional vector **r**[*t*],
    defined as the weighted sum of all memory vectors:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 该![](img/6d2eb2e7-c39d-4d13-b8bb-caa46bb4c330.png)实现了一个注意力机制，其中向量的每个单元 *i* 表示形成输出时第
    *i* 个记忆向量（即矩阵 **M** 的第 *i* 行）的权重。在步骤 *t* 处的读取操作的输出是一个 *d* 维向量 **r**[*t*]，定义为所有记忆向量的加权和：
- en: '![](img/b280f69c-ef47-48e3-b01b-3aab5fc3bbf2.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b280f69c-ef47-48e3-b01b-3aab5fc3bbf2.png)'
- en: This operation is similar to the soft attention mechanism we discussed in [Chapter
    8](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml),*Sequence-to-Sequence Models and
    Attention.* Soft attention (unlike hard attention) is differentiable, and this
    also true of this operation. In this way, the whole NTM (controller and memory)
    is a single differentiable system, which makes it possible to train it with gradient
    descent and backpropagation.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这一操作类似于我们在[第8章](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml)中讨论的软注意力机制，*序列到序列模型与注意力*。与硬注意力不同，软注意力是可微的，这个操作也具有相同的特性。通过这种方式，整个
    NTM（控制器和记忆）是一个可微分的系统，这使得我们可以通过梯度下降和反向传播对其进行训练。
- en: 'Next, let''s focus on the writing operation, which is composed of two steps:
    **erase** followed by an **add**. The write head emits the same type of attention
    vector ![](img/3f322f02-bf5f-4755-bf19-2a8c6a629a17.png) as the reading heads.
    It also emits another **erase** vector ![](img/16e529ef-8102-4f10-bd67-49b4debee77d.png),
    whose values are all within the (0, 1) range. We can define the erase operation at
    step *t* over a single row *i* of the memory as a function of these two vectors
    and the memory state at step *t-1*, *![](img/1615bdc5-cb7e-4add-9413-819093aa8950.png)*:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们集中讨论写入操作，它由两个步骤组成：**擦除**操作，接着是**加法**操作。写头发出与读取头相同类型的注意力向量 ![](img/3f322f02-bf5f-4755-bf19-2a8c6a629a17.png)。它还发出另一个**擦除**向量
    ![](img/16e529ef-8102-4f10-bd67-49b4debee77d.png)，其值都在 (0, 1) 范围内。我们可以将步骤 *t*
    上的擦除操作定义为这两个向量和步骤 *t-1* 时的记忆状态 *！[](img/1615bdc5-cb7e-4add-9413-819093aa8950.png)*
    的函数：
- en: '![](img/9ac76018-5dec-4d02-a939-af1502e59eab.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ac76018-5dec-4d02-a939-af1502e59eab.png)'
- en: Here, **1** is a *d*-dimensional vector of ones and the multiplication between ![](img/0a454eb0-2060-4036-8808-ec22c05940af.png) and
    the erase component is element-based. According to this formula, a memory location
    can be erased only if both the weight ![](img/b683581b-9d4f-4dbb-a5e9-c31f603969ac.png) and
    **e**[*t*] are nonzero. This mechanism can work with multiple attention heads
    writing in an arbitrary order, because multiplication is commutative.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**1** 是一个 *d* 维的全为 1 的向量，![](img/0a454eb0-2060-4036-8808-ec22c05940af.png)
    和擦除组件的乘法是逐元素相乘。根据这个公式，只有当权重 ![](img/b683581b-9d4f-4dbb-a5e9-c31f603969ac.png)
    和 **e**[*t*] 都不为零时，才能擦除一个记忆位置。这个机制可以与多个注意力头一起工作，按任意顺序进行写入，因为乘法是可交换的。
- en: 'The erase operation is followed by the add operation. The write head produces
    an **add** vector ![](img/0e7b6af0-ab80-48ed-ad62-7615fc08410f.png), which is
    added to the memory after the erase to produce the final memory state at step
    *t*:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 擦除操作之后是加法操作。写头生成一个**加法**向量 ![](img/0e7b6af0-ab80-48ed-ad62-7615fc08410f.png)，它在擦除之后被加到记忆中，生成步骤
    *t* 的最终记忆状态：
- en: '![](img/f5634fb9-b32d-4620-9f0c-f6d4fb9ca6dd.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5634fb9-b32d-4620-9f0c-f6d4fb9ca6dd.png)'
- en: 'We are now familiar with read and write operations, but we still don''t know
    how to produce attention vectors ![](img/b68175cd-78ce-4ada-844c-77285e9aa0e9.png)(we''ll
    omit the superscript index, because the following descriptions apply for both
    read and write heads). NTM uses two complementary addressing mechanisms to do
    this: content-based and location-based.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经熟悉了读写操作，但我们仍然不知道如何生成注意力向量 ![](img/b68175cd-78ce-4ada-844c-77285e9aa0e9.png)（我们将省略上标索引，因为以下描述适用于读头和写头）。NTM使用两种互补的寻址机制来完成这一点：基于内容的和基于位置的。
- en: 'We''ll start with content-based addressing, where each head (both reading and
    writing) emits a key vector ![](img/199a165e-f567-4c4c-8c71-d4ffd10f4489.png).
    This vector is compared to each memory vector ![](img/1a7e81ac-d914-400e-98d8-a4e4705d0dc9.png) using
    the similarity measure ![](img/ccf35344-a461-4be5-8eb0-4adf5effc5ba.png). The
    NTM authors propose using cosine similarity, defined as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从内容基础的寻址开始，其中每个头（包括读取和写入）都会发出一个键向量 ![](img/199a165e-f567-4c4c-8c71-d4ffd10f4489.png)。这个向量与每个记忆向量 ![](img/1a7e81ac-d914-400e-98d8-a4e4705d0dc9.png) 使用相似度度量 ![](img/ccf35344-a461-4be5-8eb0-4adf5effc5ba.png)进行比较。NTM的作者建议使用余弦相似度，定义如下：
- en: '![](img/ff8af2ff-641e-4ad7-a7d7-1e4a2dee9576.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff8af2ff-641e-4ad7-a7d7-1e4a2dee9576.png)'
- en: 'Then, we define a single cell of the content-based addressing vector as a softmax
    over the similarity results of all memory vectors:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将内容基础的寻址向量的单个单元定义为所有记忆向量相似度结果的softmax：
- en: '![](img/45358260-1d54-47a0-8550-50668fbc10a8.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45358260-1d54-47a0-8550-50668fbc10a8.png)'
- en: Here, ![](img/25c1dc3b-42fa-4009-8b71-1ce5a3b112d2.png) is a scalar value key
    strength, which widens or narrows the scope of the focus. For small values of ![](img/68d864a1-6d82-4cb8-b3e8-8e99f90b6eab.png),
    the attention will diffuse over all memory vectors, and for large ![](img/fd2b11cb-dc95-4bf1-85f3-26254d747cc3.png),
    the attention will focus only on the most similar memory vectors.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， ![](img/25c1dc3b-42fa-4009-8b71-1ce5a3b112d2.png) 是一个标量值键强度，它决定了焦点的范围。对于小值的 ![](img/68d864a1-6d82-4cb8-b3e8-8e99f90b6eab.png)，注意力会扩展到所有记忆向量，而对于大值的 ![](img/fd2b11cb-dc95-4bf1-85f3-26254d747cc3.png)，注意力则会聚焦于最相似的记忆向量。
- en: The authors of NTM argue that in some problems, content-based attention is not
    enough, because the content of a variable can be arbitrary but its address has
    to be recognizable. They cite arithmetic problems as one such problem: two variables, *x*
    and *y,* can take on any two values, but the procedure *f(x, y) = x × y* should
    still be defined. A controller for this task could take the values of the variables
    *x* and *y*, store them in different addresses, then retrieve them and perform
    a multiplication algorithm. In this case, the variables are addressed by location,
    not by content, which brings us to the location-based addressing mechanism. It
    works with both random-access memory jumps and simple iterations across locations.
    It does this by shifting the attention weights one step forward or backward.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: NTM的作者认为，在一些问题中，内容基础的注意力是不够的，因为一个变量的内容可以是任意的，但其地址必须是可识别的。他们以算术问题为例：两个变量，*x*和*y*，可以取任何两个值，但过程
    *f(x, y) = x × y* 仍然应该被定义。这个任务的控制器可以获取变量 *x* 和 *y* 的值，将它们存储在不同的地址中，然后检索它们并执行乘法算法。在这种情况下，变量是按位置寻址的，而不是按内容寻址的，这就引出了基于位置的寻址机制。它既可以与随机访问内存跳跃一起工作，也可以在位置之间进行简单的迭代。它通过将注意力权重向前或向后移动一步来实现这一点。
- en: For example, if the current weighting focuses entirely on a single location,
    a rotation of 1 would shift the focus to the next location. A negative shift would
    move the weighting in the opposite direction.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果当前的加权完全集中在一个位置，那么旋转1将把焦点移到下一个位置。负向旋转会将加权移动到相反方向。
- en: 'Content and location addressing work in combination, as shown in the following
    diagram:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 内容和位置寻址是结合工作的，如下图所示：
- en: '![](img/85a0d3f5-3abe-477a-ad36-8d35cf7cf6d4.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85a0d3f5-3abe-477a-ad36-8d35cf7cf6d4.png)'
- en: Flow diagram of the addressing mechanism. Source: https://arxiv.org/abs/1410.5401
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 寻址机制的流程图。来源：[https://arxiv.org/abs/1410.5401](https://arxiv.org/abs/1410.5401)
- en: 'Let''s see how it works step by step:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步看看它是如何工作的：
- en: Content addressing produces the content addressing vector ![](img/24698595-cba6-4e49-a5cc-eaa5b7e39db7.png), based
    on the memory ![](img/0ca2bba2-b305-4317-be6d-f6fb3f8f4706.png), the key vector ![](img/9f276f5b-544b-4fcc-901c-381034e1ce43.png),
    and the key strength ![](img/05075069-9a60-4db0-a319-edb925e1bee5.png).
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 内容寻址生成内容寻址向量 ![](img/24698595-cba6-4e49-a5cc-eaa5b7e39db7.png)，基于记忆 ![](img/0ca2bba2-b305-4317-be6d-f6fb3f8f4706.png)，键向量 ![](img/9f276f5b-544b-4fcc-901c-381034e1ce43.png)，以及键强度 ![](img/05075069-9a60-4db0-a319-edb925e1bee5.png)。
- en: '**Interpolation** is the first of three steps in the location addressing mechanism,
    and it comes before the actual weight shifting. Each head (read or write) emits
    a scalar **interpolation gate** *g[t]* in the (0, 1) range. The *g[t]* dictates
    whether to preserve the weight ![](img/913e6859-677b-4e4a-a2a9-f201b25b0b42.png) produced
    by the head at step *t-1* or replace it with the content-based weight ![](img/24698595-cba6-4e49-a5cc-eaa5b7e39db7.png) of
    the current step *t.* The interpolation is defined as follows:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**插值**是定位寻址机制中的三个步骤之一，它发生在实际的权重平移之前。每个头（读取或写入）会发出一个标量**插值门** *g[t]*，其值范围在（0，1）之间。*g[t]*
    决定是保留在步骤 *t-1* 中由头部生成的权重 ![](img/913e6859-677b-4e4a-a2a9-f201b25b0b42.png)，还是用当前步骤
    *t* 的基于内容的权重 ![](img/24698595-cba6-4e49-a5cc-eaa5b7e39db7.png) 来替换它。插值定义如下：'
- en: '![](img/1b1a66be-56db-4c99-8fbe-eba8c9f4d51a.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b1a66be-56db-4c99-8fbe-eba8c9f4d51a.png)'
- en: If *g[t] = 0*, then we'll preserve the previous addressing vector completely.
    Alternatively, if *g[t] = 1*, we'll only use the content-based addressing vector.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *g[t] = 0*，那么我们将完全保留先前的寻址向量。或者，如果 *g[t] = 1*，我们将只使用基于内容的寻址向量。
- en: 'The next step is the **convolutional shift**, which takes interpolation attention
    ![](img/2695b5b4-f6e0-4ea2-ad94-4206e85b7c04.png)and determines how to shift it.
    Let''s assume that the head attention can shift forward (+1), backward (-1), or
    stay the same (0). Each head emits a shift weighting *s[t]* that defines a normalized
    distribution over the allowed shifts. In this case, *s[t]* will have three elements,
    which indicate the degree to which shifts of -1, 0, and 1 are performed. If we
    assume that the memory vector indices are 0-based (from 0 to *n-1*), then we can
    define the rotation of ![](img/f90f0bf5-bcad-43ac-a5d5-907197941f39.png) by *s[t]*
    as a circular convolution:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是**卷积平移**，它将插值注意力 ![](img/2695b5b4-f6e0-4ea2-ad94-4206e85b7c04.png) 用于确定如何进行平移。假设头部注意力可以前移（+1）、后移（-1）或保持不变（0）。每个头部会发出一个平移加权
    *s[t]*，它定义了允许平移的归一化分布。在这种情况下，*s[t]* 将有三个元素，表示执行 -1、0 和 1 移动的程度。如果假设内存向量索引是从 0
    开始的（从 0 到 *n-1*），那么我们可以通过 *s[t]* 定义 ![](img/f90f0bf5-bcad-43ac-a5d5-907197941f39.png)
    的旋转为循环卷积：
- en: '![](img/575ed1d9-39a1-487c-83c0-ec259ba0c411.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/575ed1d9-39a1-487c-83c0-ec259ba0c411.png)'
- en: Note that, although we iterate over all memory indices, *s[t]* will have nonzero
    values only at the allowed positions.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管我们遍历了所有内存索引，*s[t]* 只有在允许的位置才会有非零值。
- en: 'The final addressing step is the **sharpening** step. One side, effect of the
    ability to simultaneously shift with different degrees over multiple directions
    is that the attention might blur. For example, let''s say that we shift forward
    (+1) with a probability of 0.6, shift backward (-1) with a probability of 0.2,
    and don''t shift (0) with a probability of 0.2\. When we apply the shifting, the
    original focused attention will blur between the three locations. To solve this,
    the authors of NTM suggest that you modify each head to emit another scalar ![](img/d3f6fbce-c18b-4f0e-9827-0f14088edf05.png),
    which will sharpen the final results using the following formula:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的寻址步骤是**锐化**步骤。一个副作用是，能够同时在多个方向上以不同的程度进行平移可能导致注意力模糊。例如，假设我们以 0.6 的概率向前平移（+1），以
    0.2 的概率向后平移（-1），并以 0.2 的概率不平移（0）。当我们应用平移时，原始聚焦的注意力将在这三个位置之间模糊。为了解决这个问题，NTM 的作者建议修改每个头部，使其发出另一个标量
    ![](img/d3f6fbce-c18b-4f0e-9827-0f14088edf05.png)，通过以下公式来锐化最终结果：
- en: '![](img/65574cfa-18eb-4cbe-94c2-3d48206dded0.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65574cfa-18eb-4cbe-94c2-3d48206dded0.png)'
- en: Now that we know how addressing works, let's focus on the controller, where
    we can use either RNN (for example, LSTM) or FFN. The authors of NTM argue that
    an LSTM controller has internal memory, which is complementary to the external
    memory and also allows the controller to mix information from multiple time steps.
    However, in the context of NTM, an FFN controller can mimic an RNN one by reading
    and writing at the same memory location at every step. Additionally, the FFN is
    more transparent because its read/write pattern is easier to interpret than the
    internal RNN state.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了寻址的工作原理，接下来我们来关注控制器，在控制器中我们可以使用 RNN（例如，LSTM）或 FFN。NTM 的作者认为 LSTM 控制器具有内存，这与外部内存是互补的，并且还允许控制器混合来自多个时间步骤的信息。然而，在
    NTM 的背景下，FFN 控制器可以通过在每个步骤读取和写入相同的内存位置来模拟 RNN 控制器。此外，FFN 更具透明性，因为它的读写模式比内部的 RNN
    状态更容易解释。
- en: 'The authors of the paper illustrate how NTM works with several tasks, one of
    which is a copy operation where the NTM has to replicate the input sequence as
    output. The task illustrates the model''s ability to store and access information
    over long periods of time. The input sequence has a random length between 1 and
    20\. Each element of the sequence is a vector with eight binary elements (representing
    a single byte). First, the model takes the input sequence step by step until a
    special delimiter is reached. Then, it starts to generate the output sequence.
    No additional inputs are presented during the generation phase to ensure that
    the model can generate the entire sequence without intermediate assistance. The
    authors compare the performance of NTM- and LSTM-based models and note that NTM
    converges faster during training and can replicate longer sequences compared to
    LSTM. Based on these results, and after examining the interactions of the controller
    and the memory, they conclude that NTM doesn''t simply memorize the input sequence;
    instead, it learns a type of copy algorithm. We can describe the sequence of operations
    for the algorithm with the following pseudocode:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者通过多个任务展示了NTM的工作原理，其中一个任务是复制操作，NTM必须将输入序列复制为输出。该任务展示了模型在长时间内存储和访问信息的能力。输入序列的长度在1到20之间随机变化。序列中的每个元素是一个包含八个二进制元素的向量（表示一个字节）。首先，模型逐步处理输入序列，直到遇到特殊的分隔符。然后，它开始生成输出序列。在生成阶段，不会提供额外的输入，以确保模型能够在没有中间辅助的情况下生成整个序列。作者将NTM和LSTM模型的性能进行了比较，指出NTM在训练期间收敛得更快，并且能够复制比LSTM更长的序列。基于这些结果，并在检查控制器与记忆之间的交互后，作者得出结论，NTM不仅仅是记忆输入序列；相反，它学习了一种复制算法。我们可以用以下伪代码描述该算法的操作顺序：
- en: '![](img/1caca29c-2202-4f57-b9d7-fe759aca2b8d.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1caca29c-2202-4f57-b9d7-fe759aca2b8d.png)'
- en: The NTM model learns a form of copy algorithm: source: https://arxiv.org/abs/1410.5401
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: NTM模型学习了一种复制算法：来源：[https://arxiv.org/abs/1410.5401](https://arxiv.org/abs/1410.5401)
- en: 'Next, let''s focus on the copy algorithm from the perspective of the interaction
    between the controller and the memory, as illustrated in the following image:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从控制器与记忆之间的交互角度关注复制算法，正如下图所示：
- en: '![](img/6ca3e380-8db5-43da-9134-41804d670ece.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ca3e380-8db5-43da-9134-41804d670ece.png)'
- en: The controller/memory interaction during the copy algorithm; Source: https://arxiv.org/abs/1410.5401
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 复制算法中的控制器/记忆交互；来源：[https://arxiv.org/abs/1410.5401](https://arxiv.org/abs/1410.5401)
- en: The left column shows the input phase. The top-left image represents the input
    sequence of 8-bit binary vectors, the middle-left image represents the vectors
    added to the memory, and the bottom-left image represents the memory write attention
    weights at each step. The right column shows the output phase. The top-right image
    represents the generated output sequence of 8-bit binary vectors, the middle-right
    image represents the vectors read from the memory, and the bottom-right image
    represents the memory-read attention weights at each step. The bottom images illustrate
    incremental shifts of the head locations during write and read operations. Note
    that attention weights are clearly focused on a single memory location. At the
    same time, the input and output sequences read from the same location at each
    time step and the read vectors are equivalent to the write vectors. This indicates
    that each element of the input sequence is stored in a single memory location.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 左列显示了输入阶段。左上方的图像表示8位二进制向量的输入序列，左中方的图像表示添加到记忆中的向量，左下方的图像表示每一步的记忆写入注意力权重。右列显示了输出阶段。右上方的图像表示生成的8位二进制向量输出序列，右中方的图像表示从记忆中读取的向量，右下方的图像表示每一步的记忆读取注意力权重。底部的图像展示了写入和读取操作期间头位置的逐步变化。请注意，注意力权重显著集中在单一的记忆位置。同时，输入和输出序列在每个时间步读取相同的位置，读取的向量与写入的向量相等。这表明输入序列的每个元素都存储在单一的记忆位置中。
- en: 'Before we conclude this section, let''s mention that the authors of NTM have
    released an improved memory network architecture called a **Differential Neural
    Computer** (**DNC**) (for more information, see *Hybrid computing using a neural
    network with dynamic external memory*, at [https://www.nature.com/articles/nature20101](https://www.nature.com/articles/nature20101)).
    The DNC introduces several improvements over NTM:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束这一部分之前，值得提到的是，NTM 的作者发布了一种改进的记忆网络架构，称为 **差分神经计算机** (**DNC**)（有关更多信息，请参阅
    *Hybrid computing using a neural network with dynamic external memory*，在 [https://www.nature.com/articles/nature20101](https://www.nature.com/articles/nature20101)）。DNC
    相较于 NTM 引入了若干改进：
- en: The model only uses content-based addressing (as opposed to content and location
    in NTM).
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型只使用基于内容的寻址（与 NTM 中的内容和位置寻址不同）。
- en: The model uses dynamic memory allocation by maintaining a list of available
    memory locations by adding locations to, and removing them from, a linked list
    (this is still differentiable). This mechanism allows the model to write new data
    only at locations that are marked as free.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型通过维持可用记忆位置的列表，动态分配记忆空间，向链表中添加或移除位置（这仍然是可微的）。这一机制使得模型只在标记为空闲的位置写入新数据。
- en: The model uses temporal memory linkage by maintaining information about the
    order of the memory locations that the controller writes to, which allows it to
    store sequential data at different memory locations.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型通过维持控制器写入的记忆位置的顺序信息，使用时间记忆链接，这使得它可以在不同的记忆位置存储顺序数据。
- en: This concludes our description of the NTM architecture. In the next section,
    we'll discuss an improvement to NTM introduced in the *One-shot Learning with
    Memory-Augmented Neural Networks* paper ([https://arxiv.org/abs/1605.06065](https://arxiv.org/abs/1605.06065)).
    We'll denote the improved architecture with MANN* to avoid confusion with the
    MANN acronym, which references the general class of memory networks.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对 NTM 架构的描述。在下一部分，我们将讨论在 *One-shot Learning with Memory-Augmented Neural
    Networks* 论文中介绍的 NTM 改进（[https://arxiv.org/abs/1605.06065](https://arxiv.org/abs/1605.06065)）。为了避免与代表一般记忆网络类的
    MANN 缩写混淆，我们将改进后的架构称为 MANN*。
- en: MANN*
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MANN*
- en: 'The MANN* read operation is very similar to the NTM read operation, with the
    exception that it doesn''t include the key strength parameter ![](img/8c0a80bc-2768-4808-8e72-f461746e5822.png).
    On the other hand, MANN* introduces a new content-based write addressing mechanism
    called **Least Recently Used Access** (**LRUA**) as a replacement for the combined
    content/location NTM addressing mechanism. The LRUA write operation writes to
    either the least-used memory location or the most recently used one. There are
    two reasons for implementing this: to preserve recently stored information by
    writing new memories to the most rarely-used locations, and by writing new data
    to the last used location, the new information serves as a kind of update to the
    previously written state. But how does the model know which of the two options
    to use? The MANN* addressing mechanism interpolates between the two options by
    introducing a vector of usage weights ![](img/942f9145-7af4-4021-91b2-b8273dc3dbc9.png).
    These weights are updated at each time step by adding the usage weights ![](img/c3d2a1a3-ef5d-4dbe-a8b7-730b11c08666.png) at
    step *t-1* with the current read and write attention weights:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: MANN* 的读取操作与 NTM 的读取操作非常相似，唯一的不同是它不包括键强度参数 ![](img/8c0a80bc-2768-4808-8e72-f461746e5822.png)。另一方面，MANN*
    引入了一种基于内容的写入寻址机制，称为 **最近最少使用访问** (**LRUA**)，以替代 NT的内容/位置结合寻址机制。LRUA 写入操作要么写入最少使用的记忆位置，要么写入最近使用的记忆位置。实现这一机制有两个原因：一是通过将新记忆写入最少使用的记忆位置来保存最近存储的信息；二是通过将新数据写入最后使用的位置，使新信息作为对之前写入状态的更新。那么模型如何知道使用哪种方式呢？MANN*
    的寻址机制通过引入一个使用权重向量 ![](img/942f9145-7af4-4021-91b2-b8273dc3dbc9.png) 在这两种选项之间进行插值。每个时间步，使用权重
    ![](img/c3d2a1a3-ef5d-4dbe-a8b7-730b11c08666.png) 会与当前的读写注意力权重结合，从而更新这些权重。
- en: '![](img/2cbd95cb-891b-4356-8f4d-3d5e2c75332b.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cbd95cb-891b-4356-8f4d-3d5e2c75332b.png)'
- en: 'Here, the scalar γ is a decay parameter, which determines the balance between
    the two components of the equation. MANN* also introduces the least recently used
    weights vector ![](img/d164f3e8-dc60-4160-a78e-948c3fa5e6d3.png), where each element
    of the vector is defined as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，标量 γ 是一个衰减参数，用于决定方程中两个分量之间的平衡。MANN* 还引入了最近最少使用的权重向量 ![](img/d164f3e8-dc60-4160-a78e-948c3fa5e6d3.png)，其中向量的每个元素定义如下：
- en: '![](img/56e10e46-4b66-48f6-b857-36a016d72d29.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56e10e46-4b66-48f6-b857-36a016d72d29.png)'
- en: 'Here, ![](img/fd4e3e1b-12e6-4f16-b340-f7cce5feb910.png) is the *n*th smallest
    element of the vector ![](img/069a6770-86b9-4e68-b663-c171e9c6bf2a.png) and *n*
    is equal to the number of memory reads. At this point, we can compute the write
    weights, which are an interpolation between the read weights and the least-recently
    used weights at step *t-1*:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/fd4e3e1b-12e6-4f16-b340-f7cce5feb910.png) 是向量 ![](img/069a6770-86b9-4e68-b663-c171e9c6bf2a.png)
    的 *n* 第小元素，*n* 等于读取的内存数量。此时，我们可以计算写入权重，它是读取权重与步骤 *t-1* 时最近未使用权重之间的插值：
- en: '![](img/0e93623e-c9b1-448f-a4ef-dc45cd8afea3.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e93623e-c9b1-448f-a4ef-dc45cd8afea3.png)'
- en: 'Here, σ is the sigmoid function and α is a learnable scalar parameter, which
    indicates how to balance between the 2 input weights. Now, we can write new data
    to the memory, which is done in 2 steps: the first is for computing the least
    recently used location using the weights ![](img/c3d2a1a3-ef5d-4dbe-a8b7-730b11c08666.png).
    The second step is the actual writing:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，σ 是 sigmoid 函数，α 是一个可学习的标量参数，表示如何在两个输入权重之间进行平衡。现在，我们可以将新数据写入内存，这可以分为两步：第一步是计算最近最少使用的位置，使用权重 ![](img/c3d2a1a3-ef5d-4dbe-a8b7-730b11c08666.png)。第二步是实际写入：
- en: '![](img/96e22cde-65e1-45cc-92c2-bde6ce8ebecc.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96e22cde-65e1-45cc-92c2-bde6ce8ebecc.png)'
- en: Here, ![](img/a9d608c2-aecf-41f2-9de9-0bdffed8b1aa.png) is the key vector we
    defined when we discussed NTM.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/a9d608c2-aecf-41f2-9de9-0bdffed8b1aa.png) 是我们在讨论 NTM 时定义的关键向量。
- en: 'The MANN* paper goes into a bit more detail (compared to the original NTM paper)
    about the way the controller interacts with the input data and the read/write
    heads. The authors of the paper noted that their best performing models use LSTM (see
    [Chapter 7](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml), *Understanding Recurrent*
    *Networks*) controllers. So the following is how the LSTM controller plugs into
    the MANN* system:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: MANN* 论文比原始的 NTM 论文更详细地讨论了控制器如何与输入数据和读写头交互。论文的作者指出，他们表现最好的模型使用了 LSTM 控制器（参见
    [第七章](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml)，*理解递归神经网络*）。因此，以下是 LSTM 控制器如何接入
    MANN* 系统：
- en: The controller inputs at step *t* are the concatenated vectors ![](img/70297af4-5d9d-42f7-978d-154a789d8906.png),
    where ![](img/29facb26-b5e5-4791-b7d2-f46ea48f2edc.png) is the input data and ![](img/00d06425-4270-48fc-a1f8-38ad093018f3.png) is
    the system's output at step *t-1*. In classification tasks, the outputs ![](img/45e28d30-ccfd-4c93-9b00-134ae9ad11e1.png) are
    one-hot encoded class representations.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器在步骤 *t* 的输入是连接起来的向量 ![](img/70297af4-5d9d-42f7-978d-154a789d8906.png)，其中 ![](img/29facb26-b5e5-4791-b7d2-f46ea48f2edc.png)
    是输入数据，![](img/00d06425-4270-48fc-a1f8-38ad093018f3.png) 是步骤 *t-1* 时系统的输出。在分类任务中，输出 ![](img/45e28d30-ccfd-4c93-9b00-134ae9ad11e1.png)
    是经过 one-hot 编码的类别表示。
- en: The controller outputs at step *t* are the concatenated ![](img/0d1c22f1-5e92-42ed-a861-30e93b59c749.png),
    where ![](img/da198e0a-707e-4d3a-9631-8823a3723322.png) is the LSTM cell hidden
    state and ![](img/109f037d-5344-433d-a0b5-8e60125ed09c.png) is a result of the
    read operation. For classification tasks, we can use ![](img/2672393f-2e20-4718-8c23-0fb701375f61.png) as
    an input for a fully connected layer with softmax output, resulting in the expression ![](img/bc440539-668c-4312-b516-773202d903cc.png),
    where ![](img/0518e846-b423-4882-9564-61c3c2dca8c8.png) is the fully connected
    layer weights.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制器在步骤 *t* 的输出是连接起来的 ![](img/0d1c22f1-5e92-42ed-a861-30e93b59c749.png)，其中 ![](img/da198e0a-707e-4d3a-9631-8823a3723322.png)
    是 LSTM 单元的隐藏状态，![](img/109f037d-5344-433d-a0b5-8e60125ed09c.png) 是读取操作的结果。对于分类任务，我们可以使用 ![](img/2672393f-2e20-4718-8c23-0fb701375f61.png)
    作为全连接层的输入，带有 softmax 输出，从而得到表达式 ![](img/bc440539-668c-4312-b516-773202d903cc.png)，其中 ![](img/0518e846-b423-4882-9564-61c3c2dca8c8.png)
    是全连接层的权重。
- en: The key vector ![](img/36e80a51-1d5c-4419-b724-02754ea8fd03.png), which serves as
    a base for the attention weights of the read/write operations, is the LSTM cell
    state ![](img/71582670-adc7-4831-a821-11d6edc829bd.png).
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键向量 ![](img/36e80a51-1d5c-4419-b724-02754ea8fd03.png)，作为读取/写入操作的注意力权重的基础，是
    LSTM 单元状态 ![](img/71582670-adc7-4831-a821-11d6edc829bd.png)。
- en: This concludes our discussion of MANNs and, indeed, the chapter.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对 MANN 的讨论，实际上也结束了这一章。
- en: Summary
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered two categories of emerging NN models—GNNs and MANNs.
    We started with a short introduction to graphs and then we looked at several different
    types of GNN, including GraphNN, graph convolutional networks, graph attention
    networks, and graph autoencoders. We concluded the graph section by looking at
    the NGL and we implemented an NGL example using the TensorFlow-based NSL framework.
    Then we focused on memory-augmented networks, where we looked at the NTM and MANN*
    architectures.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了两类新兴的神经网络模型——图神经网络（GNNs）和记忆增强神经网络（MANNs）。我们首先简要介绍了图结构，然后探讨了几种不同类型的图神经网络，包括GraphNN、图卷积网络、图注意力网络和图自编码器。我们通过研究NGL并使用基于TensorFlow的NSL框架实现了一个NGL示例，结束了图部分的内容。接着，我们专注于记忆增强网络，探讨了NTM和MANN*架构。
- en: In the next chapter, we'll look at the emerging field of meta learning, which
    involves making ML algorithms learn to learn.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨新兴的元学习领域，它涉及到使机器学习算法学会如何学习。
