- en: Markov Decision Process
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: In this chapter, we will talk about another application of HMMs known as **Markov
    Decision Process** (**MDP**). In the case of MDPs, we introduce a reward to our
    model, and any sequence of states taken by the process results in a specific reward.
    We will also introduce the concept of discounts, which will allow us to control
    how short-sighted or far-sighted we want our agent to be. The goal of the agent
    would be to maximize the total reward that it can get.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论隐马尔可夫模型（HMMs）的另一种应用，即**马尔可夫决策过程**（**MDP**）。在MDP的情况下，我们为模型引入了奖励，任何由过程经历的状态序列都会产生特定的奖励。我们还将引入折扣的概念，这将使我们能够控制代理的短视或远视程度。代理的目标是最大化它能够获得的总奖励。
- en: 'In this chapter, we will be covering the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Reinforcement learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: The Markov reward process
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔可夫奖励过程
- en: Markov decision processes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: Code example
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码示例
- en: Reinforcement learning
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement learning is a different paradigm in machine learning where an
    agent tries to learn to behave optimally in a defined environment by making decisions/actions
    and observing the outcome of that decision. So, in the case of reinforcement learning,
    the agent is not really from some given dataset, but rather, by interacting with
    the environment, the agent tries to learn by observing the effects of its actions.
    The environment is defined in such a way that the agent gets rewards if its action
    gets it closer to the goal.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习中的一种不同范式，其中代理通过做出决策/行动并观察结果来学习在定义的环境中如何最优地行为。因此，在强化学习中，代理并不是从某个给定的数据集中学习，而是通过与环境互动，代理尝试通过观察其行为的效果来学习。环境的定义方式使得代理在其行动使其接近目标时会获得奖励。
- en: Humans are known to learn in this way. For example, consider a child in front
    of a fireplace where the child is the agent and the space around the child is
    the environment. Now, if the child moves its hand towards the fire, it feels the
    warmth, which feels good and, in a way, the child (or the agent) is rewarded for
    the action of moving its hand close to the fire. But if the child moves its hand
    too close to the fire, its hand will burn, hence receiving a negative reward.
    Using these rewards, the child is able to figure out the optimal distance to keep
    its hand from the fire. Reinforcement learning tries to imitate exactly this kind
    of system in order to train the agent to learn to optimize its goal in the given
    environment.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人类就是以这种方式学习的。例如，考虑一个站在壁炉前的孩子，孩子是代理，周围的空间是环境。现在，如果孩子把手靠近火炉，他会感受到温暖，这种感觉是愉快的，某种程度上，孩子（或代理）因将手靠近火炉而获得奖励。但如果孩子将手移得太靠近火炉，手就会被烧伤，因此会收到负奖励。通过这些奖励，孩子能够找出保持手与火炉之间的最优距离。强化学习正是模仿这种系统，训练代理学习如何在给定的环境中优化其目标。
- en: 'Making this more formal, to train an agent we will need to have an environment
    which represents the world in which the agent should be able to take actions.
    For each of these actions, the environment should return observations which contain
    information about the reward, telling it how good or bad the action was. The observation
    should also have information regarding the next state of the agent in the environment.
    And, based on these observations, the agent tries to figure out the optimal way
    to reach its goal. *Figure 1* shows the interaction between an agent and an environment:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，要训练一个代理，我们需要有一个环境，这个环境代表了代理应该能够采取行动的世界。对于这些行动，环境应该返回包含奖励信息的观察结果，告诉代理这次行动的好坏。观察结果还应包含关于代理在环境中下一状态的信息。基于这些观察结果，代理尝试找出到达目标的最优方式。*图1*
    显示了代理与环境之间的互动：
- en: '![](img/6ea82be0-74db-4985-b571-e29668558f2a.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ea82be0-74db-4985-b571-e29668558f2a.png)'
- en: The thing that makes reinforcement learning fundamentally different from other
    algorithms is that there is no supervisor. Instead, there is only a reward signal
    giving feedback to the agent about the action it took. Another important thing
    to mention here is that an environment can be constructed in such a way that the
    reward is delayed, which can make the agent wander around before reaching its
    goal. It is also possible that the agent might have to go through a lot of negative
    feedback before reaching its goal.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习与其他算法根本不同的地方在于没有监督者。相反，只有一个奖励信号，反馈代理它所采取的行动。这里还需要提到一个重要的事情，那就是环境可以被构建成奖励是延迟的，这可能会导致代理在达到目标之前四处游荡。也有可能代理在达到目标之前要经历大量的负反馈。
- en: In the case of supervised learning, we are given the dataset, which basically
    tells our learning algorithm the right answers in different situations. Our learning
    algorithm then looks over all these different situations, and the solutions in
    those cases, and tries to generalize based upon it. Hence, we also expect that
    the dataset given to use is **independent and identically distributed** (**IID**).
    But in the case of reinforcement learning the data is not IID, the data generated
    depends on the path the agent took, and, hence, it depends on the actions taken
    by the agent. Hence, reinforcement learning is an active learning process in which
    the actions taken by the agent influence the environment which in turn influences
    the data generated by the environment.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们给定的数据集基本上会告诉我们的学习算法在不同情况下的正确答案。然后，我们的学习算法会查看所有这些不同的情况以及这些情况下的解决方案，并试图基于这些情况进行归纳。因此，我们也期望给定的数据集是**独立同分布**（**IID**）的。但在强化学习中，数据不是IID，生成的数据取决于代理所走的路径，因此取决于代理采取的行动。因此，强化学习是一个主动学习过程，代理所采取的行动影响环境，进而影响环境生成的数据。
- en: 'We can take a very simple example to better understand how a reinforcement
    learning agent and environment behave. Consider an agent trying to learn to play
    Super Mario Bros:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一个非常简单的例子来更好地理解强化学习代理和环境的行为。考虑一个代理试图学习玩超级马里奥兄弟：
- en: The agent will receive the initial state from the environment. In the case of
    Super Mario, this would be the current frame of the game.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理将从环境中接收初始状态。以超级马里奥为例，这将是当前的游戏画面。
- en: Having received the state information, the agent will try to take an action.
    Let's say the action the agent took is to move to the right.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接收到状态信息后，代理将尝试采取行动。假设代理采取的行动是向右移动。
- en: When the environment receives this action it will return the next state based
    on it. The next state would also be a frame, but the frame could be of a dying
    Mario if there was an enemy next to Mario in the previous state. Otherwise, the
    frame would just have shown Mario to have moved one step to the right. The environment
    will also return the rewards based on the action. If there was an enemy on the
    right of Mario the rewards could be, let's say -5 (since the action killed Mario)
    or could be +1 if the Mario moved towards finishing the level.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当环境接收到这个行动时，它将基于此返回下一个状态。下一个状态也会是一个画面，但如果前一个状态中马里奥旁边有敌人，那么画面可能会显示马里奥死亡。否则，画面只会显示马里奥向右移动了一步。环境还会根据行动返回奖励。如果马里奥右边有敌人，奖励可能是-5（因为该行动导致马里奥死亡），或者如果马里奥朝着完成关卡的方向移动，奖励可能是+1。
- en: Reward hypothesis
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励假设
- en: The whole concept of reinforcement learning is based on something called **reward
    hypothesis**. According to reward hypothesis, all goals can be defined by the
    maximization of the expected cumulative reward.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的整个概念是基于一种叫做**奖励假设**的东西。根据奖励假设，所有目标都可以通过最大化预期的累积奖励来定义。
- en: 'At any given time *t*, the total cumulative reward can be given by:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在任意给定的时间点 *t*，总的累积奖励可以表示为：
- en: '![](img/2458c36c-959a-4168-adfe-73035ba18602.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2458c36c-959a-4168-adfe-73035ba18602.png)'
- en: 'But, in reality, the rewards which are closer to the current state are more
    probable that the ones which are further away. To deal with this, we would introduce
    another term called the **discount rate,** ![](img/0f5e1c1c-8fea-4597-ad98-a1ffa23cf86e.png).
    The value of the discount rate is always between 0 and 1\. A large value of ![](img/7e86090e-61af-42bb-993e-e105248db956.png) means
    a smaller discount, which make the agent care more about the long-term rewards,
    whereas, for smaller values of ![](img/058b9431-3af7-4648-b9ea-184921bdc7e9.png),
    the agent cares more about the short-term rewards. With the discount rate term,
    we can now define our cumulative reward as:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 但实际上，离当前状态较近的奖励比远离的奖励更可能发生。为了解决这个问题，我们引入了另一个术语，称为**折扣率**，![](img/0f5e1c1c-8fea-4597-ad98-a1ffa23cf86e.png)。折扣率的值始终在0和1之间。较大的折扣率![](img/7e86090e-61af-42bb-993e-e105248db956.png)意味着较小的折扣，这使得智能体更关心长期奖励；而较小的折扣率![](img/058b9431-3af7-4648-b9ea-184921bdc7e9.png)则使得智能体更关心短期奖励。通过引入折扣率项，我们现在可以定义我们的累积奖励为：
- en: '![](img/1e708f38-37af-4060-8246-3ba7c185487e.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e708f38-37af-4060-8246-3ba7c185487e.png)'
- en: State of the environment and the agent
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境和智能体的状态
- en: 'As we saw earlier, the agent interacts with the environment at intervals of
    time *t = 0, 1, 2, 3, ..,* and, at each time instance, the agent gets the state
    of the environment *S[t]* based on which it takes an action *A[t]* and gets a
    reward *R[t+1]*. This sequence of state, action and rewards over time is known
    as the **history of the agen****t** and is given as:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所看到的，智能体与环境在时间间隔*t = 0, 1, 2, 3, ..,* 进行交互，在每个时间点，智能体根据环境的状态*S[t]*采取动作*A[t]*并获得奖励*R[t+1]*。这种随时间变化的状态、动作和奖励的序列被称为**智能体的历史**，表示为：
- en: '![](img/9ca92514-748f-4bf9-8d99-bad282d7763d.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ca92514-748f-4bf9-8d99-bad282d7763d.png)'
- en: 'Ideally, we would like the action taken by the agent to be based on its total
    history, but it is generally unfeasible because the history of the agent can be
    huge. Therefore, we define the state in a way such that it is a summary of all
    the history of the agent:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望智能体所采取的动作能够基于其所有历史，但通常不可行，因为智能体的历史可能非常庞大。因此，我们定义状态的方式是将其作为智能体历史的总结：
- en: '![](img/6698b496-3ddc-40bd-ac20-19db2f9c5c4b.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6698b496-3ddc-40bd-ac20-19db2f9c5c4b.png)'
- en: Basically, we are defining the state to be a function of the history of the
    agent. It's important to notice that the environment state is the state that the
    environment uses to determine its next state, based on the action, and give out
    rewards. Also, it is private to the environment.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们将状态定义为智能体历史的一个函数。需要注意的是，环境状态是环境用来根据动作决定其下一个状态的状态，并且给出奖励。此外，环境状态是环境的私有信息。
- en: On the other hand, the agent state is the state that the agent uses to decide
    on the next action. The agent state is its internal representation, and can be
    any function of the history as mentioned before.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，智能体状态是智能体用来决定下一步动作的状态。智能体的状态是其内部表示，可以是前述历史的任何函数。
- en: We use a Markov state to represent an agent's state, which basically means that
    the current state of the agent is able to summarize all the history, and the next
    action of the agent will depend only on the current state of the agent. Hence,
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用马尔可夫状态来表示智能体的状态，这基本上意味着智能体的当前状态能够总结所有历史，智能体的下一步动作只会依赖于当前的状态。因此，
- en: '![](img/9eed4f10-981c-4cee-b55f-64575586d4b8.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9eed4f10-981c-4cee-b55f-64575586d4b8.png)'
- en: In this chapter, we will only be considering the case when the agent is directly
    able to observe the environment's state. This results in the observation from
    the environment to be both the current state of the agent as well as the environment.
    This special case is also commonly known as **MDP**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们仅考虑智能体能够直接观察环境状态的情况。这导致环境的观察既是智能体的当前状态，也是环境的状态。这种特殊情况通常被称为**MDP**。
- en: Components of an agent
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能体的组件
- en: In this section, we will formally define the different kinds of components that
    an agent can have.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将正式定义智能体可能具有的不同类型的组件。
- en: '**Policy**: It is a conditional over the action given the state. Based on this
    conditional distribution, the agent chooses its action at any given state. It
    is possible for the policy to be deterministic: *a = π(s)* or stochastic: ![](img/26fa3ff6-449d-4502-ba5a-58f7b5ffbb1a.png).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**：它是给定状态下对动作的条件概率分布。根据这个条件分布，智能体在任何给定状态下选择其动作。策略可以是确定性的：*a = π(s)*，也可以是随机的：![](img/26fa3ff6-449d-4502-ba5a-58f7b5ffbb1a.png)。'
- en: '**Value function**: The value function tries to predict the reward to expect
    on taking a given action in a given state. It is given as:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值函数**：价值函数试图预测在给定状态下采取某个动作所期望的奖励。它的表达式为：'
- en: '![](img/9cd0fb06-8243-45fc-8474-230d7d871484.png).'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![](img/9cd0fb06-8243-45fc-8474-230d7d871484.png)。'
- en: where *E* is the expectation and ![](img/06b33013-eeab-4b47-92e5-64ae385ff31e.png) is
    the discount factor.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 *E* 是期望值，![](img/06b33013-eeab-4b47-92e5-64ae385ff31e.png) 是折扣因子。
- en: '**Model**: The model is the agent''s representation of the environment. It
    is defined using a transition function *P* which predicts the next state of the
    environment:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型**：模型是智能体对环境的表征。它使用一个转移函数 *P* 来定义，预测环境的下一个状态：'
- en: '![](img/55f70d9a-aaef-4be0-beee-f6f8cdc0a70a.png).'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![](img/55f70d9a-aaef-4be0-beee-f6f8cdc0a70a.png)。'
- en: and a reward function which predicts the reward associated with any given action
    at a given state: ![](img/87664401-62c1-42c9-a764-6582d6bfa877.png).
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以及一个奖励函数，用于预测在给定状态下采取某个动作所关联的奖励： ![](img/87664401-62c1-42c9-a764-6582d6bfa877.png)。
- en: 'Based on these components, agents can be classified into the following five
    categories:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些组件，智能体可以被划分为以下五类：
- en: '**Value-based agents**: Have an implicit policy and the agent takes decisions
    for actions based on the value function'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于价值的智能体**：拥有隐式的策略，智能体根据价值函数做出决策'
- en: '**Policy-based agents**: Have an explicit policy and the agent searches for
    the most optimal action-value function'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于策略的智能体**：拥有显式的策略，智能体搜索最优的动作-价值函数'
- en: '**Actor-critic agents**: Combination of both value-based and policy-based agents'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**演员-评论员智能体**：结合了基于价值和基于策略的智能体'
- en: '**Model-based agents**: Try to build a model based on the environment'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的智能体**：尝试基于环境构建一个模型'
- en: '**Model-free agents**: Don''t try to learn the environment, rather they try
    to learn the policy and value function'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无模型智能体**：不尝试学习环境，而是尝试学习策略和价值函数'
- en: The Markov reward process
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫奖励过程
- en: In the previous section, we gave an introduction to MDP. In this section, we
    will define the problem statement formally and see the algorithms for solving
    it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了MDP。在这一节中，我们将正式定义问题陈述，并看到解决该问题的算法。
- en: An MDP is used to define the environment in reinforcement learning and almost
    all reinforcement learning problems can be defined using an MDP.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: MDP（马尔可夫决策过程）用于定义强化学习中的环境，几乎所有的强化学习问题都可以使用MDP来定义。
- en: 'For understanding MDPs we need to use the concept of the **Markov reward process**
    (**MRP**). An MRP is a stochastic process which extends a Markov chain by adding
    a reward rate to each state. We can also define an additional variable to keep
    a track of the accumulated reward over time. Formally, an MRP is defined by ![](img/7488784d-0670-480e-a673-8320989866c4.png) where
    *S* is a finite state space, *P* is the state transition probability function,
    *R* is a reward function, and ![](img/b829afec-9f1c-4b23-8a78-f77547dbc951.png) is
    the discount rate:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解MDP，我们需要使用**马尔可夫奖励过程**（**MRP**）的概念。MRP是一个随机过程，通过给每个状态添加奖励率，扩展了马尔可夫链。我们还可以定义一个附加变量来跟踪随时间积累的奖励。形式上，MRP定义为
    ![](img/7488784d-0670-480e-a673-8320989866c4.png)，其中 *S* 是有限状态空间，*P* 是状态转移概率函数，*R*
    是奖励函数，且 ![](img/b829afec-9f1c-4b23-8a78-f77547dbc951.png) 是折扣率：
- en: '![](img/a0b927f4-6354-4751-ac3a-13c348781d1a.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0b927f4-6354-4751-ac3a-13c348781d1a.png)'
- en: where ![](img/5ff856d1-0e22-4ad3-ac30-6867098e1371.png) denotes the expectation.
    And the term *R[s]* here denotes the expected reward at the state *s*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/5ff856d1-0e22-4ad3-ac30-6867098e1371.png) 表示期望值。这里的 *R[s]* 表示状态 *s*
    的期望奖励。
- en: 'In the case of MRPs, we can define the expected return when starting from a
    state *s* as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在MRP的情况下，我们可以定义从状态 *s* 开始时的期望回报为：
- en: '![](img/2f5e4c4d-cff7-45a4-86bb-8ca971bd66c3.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f5e4c4d-cff7-45a4-86bb-8ca971bd66c3.png)'
- en: 'where *G[t]* is the cumulative gain as we had defined in the previous section:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *G[t]* 是我们在前一节中定义的累计增益：
- en: '![](img/53d2fd3d-43c8-45ec-8f65-bda4e3592f97.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53d2fd3d-43c8-45ec-8f65-bda4e3592f97.png)'
- en: Now, for maximizing the cumulative reward, the agent will try to get the most
    expected sum of rewards from every state it goes into. To do that we need to find
    the optimal value function. We will see the algorithm for doing that in the next
    section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了最大化累计奖励，智能体将尽力从它进入的每个状态中获取期望的奖励总和。为了做到这一点，我们需要找到最优的价值函数。我们将在下一节看到解决这个问题的算法。
- en: Bellman equation
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: 'Using the Bellman equation, we decompose our value function into two separate parts,
    one representing the immediate reward and the other term representing the future
    rewards. From our previous definitions, the immediate reward is represented as
    *R[t+1]* and the future rewards by ![](img/d9847e5a-6360-4875-8b40-3f4296a066c1.png) where:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝尔曼方程，我们将值函数分解为两个独立的部分，一个表示即时奖励，另一个表示未来奖励。根据我们之前的定义，即时奖励表示为*R[t+1]*，未来奖励表示为 ![](img/d9847e5a-6360-4875-8b40-3f4296a066c1.png)，其中：
- en: '![](img/fbee6c17-72b3-4084-bb61-b9d622a90e7b.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbee6c17-72b3-4084-bb61-b9d622a90e7b.png)'
- en: 'Let''s now unroll *G[t]* and substitute *G[t+1]* in it:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们展开*G[t]*并将*G[t+1]*代入其中：
- en: '![](img/a8d00aeb-1527-47c2-88e6-8960ec6a5d58.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8d00aeb-1527-47c2-88e6-8960ec6a5d58.png)'
- en: 'Now, since we know that:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于我们知道：
- en: '![](img/c34fe84e-b0f3-4d8c-b9c9-cd2386ff72b1.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c34fe84e-b0f3-4d8c-b9c9-cd2386ff72b1.png)'
- en: 'Using this identity we have:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个恒等式，我们得到：
- en: '![](img/e81848b6-cc68-4947-96d1-58d008685f5a.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e81848b6-cc68-4947-96d1-58d008685f5a.png)'
- en: 'And this gives us the Bellman equation for MRPs:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了MRP的贝尔曼方程：
- en: '![](img/5ddd6fb3-0ebf-44d8-b385-b49dd34cbe0b.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ddd6fb3-0ebf-44d8-b385-b49dd34cbe0b.png)'
- en: MDP
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MDP
- en: 'Now that we have a basic understanding of MRPs, we can move on to MDPs. An
    MDP is an MRP which also involved decisions. All the states in the environment
    are also Markov, hence the next state is only dependent on the current state.
    Formally, an MDP can be represented using ![](img/57989e16-1d7b-45a9-b644-35221cfb5099.png) where
    *S* is the state space, *A* is the action set, *P* is the state transition probability
    function, *R* is the reward function, and ![](img/ada7f4d2-3aba-46a2-9172-6b188b3457dd.png) is
    the discount rate. The state transition probability function *P* and the reward
    function *R* are formally defined as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对MRP有了基本的理解，可以继续讨论MDP。MDP是包含决策的MRP。环境中的所有状态也是马尔可夫的，因此下一个状态只依赖于当前状态。形式上，MDP可以使用 ![](img/57989e16-1d7b-45a9-b644-35221cfb5099.png) 来表示，其中*S*是状态空间，*A*是动作集，*P*是状态转移概率函数，*R*是奖励函数，*γ*是折扣率。状态转移概率函数*P*和奖励函数*R*被正式定义为：
- en: '![](img/f4c103f7-991b-427d-bcc1-e686de7e028d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4c103f7-991b-427d-bcc1-e686de7e028d.png)'
- en: 'We can also formally define a policy *π* as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以正式定义一个策略*π*为：
- en: '![](img/dfa1a5a9-553b-4755-8c4a-6add32b46623.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfa1a5a9-553b-4755-8c4a-6add32b46623.png)'
- en: Since the states in the MDP are considered to be Markov, the MDP policies depend
    only on the current state, which means that the policies are stationary that is, ![](img/04f513a8-31a0-4304-8c53-5d2ad9d73d74.png). This
    means that whenever the agent falls into the same state, it will take the decision
    based on the same policy it had decided before. The decision function can be made
    stochastic so that the agent doesn't keep on taking the same decisions and hence
    is able to explore the environment.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MDP中的状态被认为是马尔可夫的，MDP策略只依赖于当前状态，这意味着策略是静态的，也就是说， ![](img/04f513a8-31a0-4304-8c53-5d2ad9d73d74.png)。这意味着每当智能体进入相同的状态时，它将根据之前决定的相同策略做出决策。决策函数可以是随机的，这样智能体就不会一直做出相同的决策，从而能够探索环境。
- en: 'Now, since we want to use the Bellman Equation in the case of MDP, we will
    recover an MRP from the MDPs. Given an MDP, ![](img/b35de36f-a812-4f4d-a6a8-56112f1f568f.png) and
    a policy ![](img/f40666b4-eb4f-4fff-a06c-76f3b1f83173.png) the state sequence
    *S[1], S[2], ...* is a Markov Process *(S,P)* on the policy *π*. The state and
    reward sequence *S1, R1, S2, R2, ... *is also an MRP given by ![](img/d82e40a8-d340-4e3c-b9d7-41eb27928ae2.png) where:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于我们希望在MDP的情况下使用贝尔曼方程，我们将从MDP恢复一个MRP。给定一个MDP，![](img/b35de36f-a812-4f4d-a6a8-56112f1f568f.png)和一个策略 ![](img/f40666b4-eb4f-4fff-a06c-76f3b1f83173.png)，状态序列*S[1],
    S[2], ...*是策略*π*上的马尔可夫过程*(S,P)*。状态和奖励序列*S1, R1, S2, R2, ...*也是由 ![](img/d82e40a8-d340-4e3c-b9d7-41eb27928ae2.png) 给出的MRP，其中：
- en: '![](img/f901d745-4aeb-4c7c-a33d-26d0e9786154.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f901d745-4aeb-4c7c-a33d-26d0e9786154.png)'
- en: 'We can similarly formulate our reward function as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以类似地将奖励函数表示为：
- en: '![](img/a46a80a3-643f-4fb5-8c7f-91ea46ee8a84.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a46a80a3-643f-4fb5-8c7f-91ea46ee8a84.png)'
- en: 'And, since we know that the state-value function *Vπ(s)* of an MDP is the expected
    return starting from state *S* and then following the policy *π*, the value function
    is given as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，由于我们知道MDP的状态值函数*Vπ(s)*是从状态*S*开始，然后遵循策略*π*的期望回报，值函数表示为：
- en: '![](img/15bcda57-42d2-41de-8726-9027a70ffe97.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15bcda57-42d2-41de-8726-9027a70ffe97.png)'
- en: 'Also, the action-value function can be given as:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，动作值函数可以表示为：
- en: '![](img/7c0a8568-b3be-4d46-9928-0d7ed49b47f6.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c0a8568-b3be-4d46-9928-0d7ed49b47f6.png)'
- en: 'Having these values, we can again derive the Bellman expectation equation in
    the case of MDPs. We again start by decomposing the state-value function into
    immediate reward and future rewards:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些值后，我们可以再次推导出MDP情况下的贝尔曼期望方程。我们再次通过将状态值函数分解为即时奖励和未来奖励来开始：
- en: '![](img/92216e16-9342-4643-9bc0-4b636cedfcc7.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92216e16-9342-4643-9bc0-4b636cedfcc7.png)'
- en: 'And similar to the case of MRPs, the action-value function can also be decomposed
    as:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 与MRP的情况类似，动作值函数也可以分解为：
- en: '![](img/7c5528ee-2aa8-4ef0-a8ff-8f8a7e10f534.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c5528ee-2aa8-4ef0-a8ff-8f8a7e10f534.png)'
- en: 'And as we have multiple actions from each state *S* and the policy defines
    the probability distribution over the actions, we will need to average over it
    to get the Bellman expectation equation:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们从每个状态*S*有多个动作，并且策略定义了动作的概率分布，因此我们需要对其进行平均，以得到贝尔曼期望方程：
- en: '![](img/c407fadd-243a-482f-87bf-3e00d4e38a55.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c407fadd-243a-482f-87bf-3e00d4e38a55.png)'
- en: 'We can also average over all the possible action-values to know how good being
    in a given state *S* is:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对所有可能的动作值进行平均，以了解在给定状态*S*下有多好：
- en: '![](img/a4aa3925-892e-4d08-a5f8-09c0fcc04420.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4aa3925-892e-4d08-a5f8-09c0fcc04420.png)'
- en: Code example
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码示例
- en: 'In the following code example we implement a simple MDP:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下的代码示例中，我们实现了一个简单的MDP：
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using this MDP, we can now code up a simple betting game:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个MDP，我们现在可以编写一个简单的投注游戏代码：
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Summary
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started with a short introduction to Reinforcement Learning.
    We talked about agents, rewards and our learning goals in reinforcement learning.
    In the next section, we introduced MRP which is one of the main concepts underlying
    MDP. Having an understanding of MRP we next introduce the concepts of MDP along
    with a code example.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先简要介绍了强化学习。我们讨论了智能体、奖励和强化学习中的学习目标。在下一节中，我们介绍了MRP，它是MDP的一个主要概念。在理解了MRP之后，我们接下来介绍了MDP的概念，并附上了一个代码示例。
