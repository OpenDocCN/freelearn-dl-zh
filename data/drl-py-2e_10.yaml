- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Policy Gradient Method
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度方法
- en: In the previous chapters, we learned how to use value-based reinforcement learning
    algorithms to compute the optimal policy. That is, we learned that with value-based
    methods, we compute the optimal Q function iteratively and from the optimal Q
    function, we extract the optimal policy. In this chapter, we will learn about policy-based
    methods, where we can compute the optimal policy without having to compute the
    optimal Q function.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了如何使用基于值的强化学习算法来计算最优策略。也就是说，我们学到的是，使用基于值的方法时，我们迭代地计算最优Q函数，并从最优Q函数中提取最优策略。在本章中，我们将学习基于策略的方法，在这种方法中，我们可以在不计算最优Q函数的情况下直接计算最优策略。
- en: We will start the chapter by looking at the disadvantages of computing a policy
    from the Q function, and then we will learn how policy-based methods learn the
    optimal policy directly without computing the Q function. Next, we will examine
    one of the most popular policy-based methods, called the policy gradient. We will
    first take a broad overview of the policy gradient algorithm, and then we will
    learn more about it in detail.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过分析从Q函数计算策略的缺点来开始本章的学习，然后我们将学习基于策略的方法如何直接学习最优策略，而无需计算Q函数。接下来，我们将研究一种最流行的基于策略的方法——策略梯度。我们将首先概览策略梯度算法，然后详细学习它。
- en: Going forward, we will also learn how to derive the policy gradient step by
    step and examine the algorithm of the policy gradient method in more detail. At
    the end of the chapter, we will learn about the variance reduction techniques
    in the policy gradient method.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们还将学习如何一步步推导策略梯度，并更加详细地研究策略梯度方法的算法。章节最后，我们将学习策略梯度方法中的方差减少技术。
- en: 'In this chapter, we will learn the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将学习以下内容：
- en: Why policy-based methods?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择基于策略的方法？
- en: Policy gradient intuition
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度直觉
- en: Deriving the policy gradient
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推导策略梯度
- en: Algorithm of policy gradient
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略梯度算法
- en: Policy gradient with reward-to-go
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用奖励到达的策略梯度
- en: Policy gradient with baseline
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带基准的策略梯度
- en: Algorithm of policy gradient with baseline
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带基准的策略梯度算法
- en: Why policy-based methods?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择基于策略的方法？
- en: The objective of reinforcement learning is to find the optimal policy, which
    is the policy that provides the maximum return. So far, we have learned several
    different algorithms for computing the optimal policy, and all these algorithms
    have been value-based methods. Wait, what are value-based methods? Let's recap
    what value-based methods are, and the problems associated with them, and then
    we will learn about policy-based methods. Recapping is always good, isn't it?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是找到最优策略，也就是提供最大回报的策略。到目前为止，我们已经学习了几种计算最优策略的不同算法，这些算法都是基于值的方法。等等，什么是基于值的方法？让我们回顾一下基于值的方法以及它们的问题，然后我们将学习基于策略的方法。回顾一下总是有益的，不是吗？
- en: 'With value-based methods, we extract the optimal policy from the optimal Q
    function (Q values), meaning we compute the Q values of all state-action pairs
    to find the policy. We extract the policy by selecting an action in each state
    that has the maximum Q value. For instance, let''s say we have two states *s*[0]
    and *s*[1] and our action space has two actions; let the actions be 0 and 1\.
    First, we compute the Q value of all the state-action pairs, as shown in the following
    table. Now, we extract policy from the Q function (Q values) by selecting action
    0 in state *s*[0] and action 1 in state *s*[1] as they have the maximum Q value:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于值的方法时，我们从最优Q函数（Q值）中提取最优策略，这意味着我们计算所有状态-动作对的Q值，以找到策略。我们通过在每个状态中选择具有最大Q值的动作来提取策略。例如，假设我们有两个状态*s*[0]和*s*[1]，我们的动作空间有两个动作，分别为0和1。首先，我们计算所有状态-动作对的Q值，如下表所示。现在，我们通过在状态*s*[0]中选择动作0，在状态*s*[1]中选择动作1来从Q函数（Q值）中提取策略，因为它们具有最大Q值：
- en: '![](img/B15558_10_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_01.png)'
- en: 'Table 10.1: Q table'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.1：Q表
- en: 'Later, we learned that it is difficult to compute the Q function when our environment
    has a large number of states and actions as it would be expensive to compute the
    Q values of all possible state-action pairs. So, we resorted to the **Deep Q Network**
    (**DQN**). In DQN, we used a neural network to approximate the Q function (Q value).
    Given a state, the network will return the Q values of all possible actions in
    that state. For instance, consider the grid world environment. Given a state,
    our DQN will return the Q values of all possible actions in that state. Then we
    select the action that has the highest Q value. As we can see in *Figure 10.1*,
    given state **E**, DQN returns the Q value of all possible actions (*up, down,
    left, right*). Then we select the *right* action in state **E** since it has the
    maximum Q value:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 后来我们了解到，当环境具有大量的状态和动作时，计算 Q 函数会很困难，因为计算所有可能状态-动作对的 Q 值非常昂贵。因此，我们转向了**深度 Q 网络**（**DQN**）。在
    DQN 中，我们使用神经网络来逼近 Q 函数（Q 值）。给定一个状态，网络将返回该状态下所有可能动作的 Q 值。例如，考虑网格世界环境。给定一个状态，我们的
    DQN 会返回该状态下所有可能动作的 Q 值。然后我们选择具有最高 Q 值的动作。如在*图 10.1*中所示，给定状态**E**，DQN 返回所有可能动作（*上、下、左、右*）的
    Q 值。然后我们在状态**E**中选择*右*动作，因为它具有最大 Q 值：
- en: '![](img/B15558_10_02.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_02.png)'
- en: 'Figure 10.1: DQN'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.1: DQN'
- en: Thus, in value-based methods, we improve the Q function iteratively, and once
    we have the optimal Q function, then we extract optimal policy by selecting the
    action in each state that has the maximum Q value.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在基于值的方法中，我们会迭代地改善 Q 函数，一旦我们得到最优的 Q 函数，就可以通过选择每个状态下具有最大 Q 值的动作来提取最优策略。
- en: One of the disadvantages of the value-based method is that it is suitable only
    for discrete environments (environments with a discrete action space), and we
    cannot apply value-based methods in continuous environments (environments with
    a continuous action space).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于值的方法的一个缺点是，它仅适用于离散环境（具有离散动作空间的环境），我们不能在连续环境（具有连续动作空间的环境）中应用基于值的方法。
- en: We have learned that a discrete action space has a discrete set of actions;
    for example, the grid world environment has discrete actions (up, down, left,
    and right) and the continuous action space consists of actions that are continuous
    values, for example, controlling the speed of a car.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，离散的动作空间有一组离散的动作；例如，网格世界环境中有离散的动作（上、下、左、右），而连续的动作空间包含的是连续值的动作，例如控制汽车的速度。
- en: So far, we have only dealt with a discrete environment where we had a discrete
    action space, so we easily computed the Q value of all possible state-action pairs.
    But how can we compute the Q value of all possible state-action pairs when our
    action space is continuous? Say we are training an agent to drive a car and say
    we have one continuous action in our action space. Let the action be the speed
    of the car and the value of the speed of the car ranges from 0 to 150 kmph. In
    this case, how can we compute the Q value of all possible state-action pairs with
    the action being a continuous value?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只处理了一个离散的环境，其中有离散的动作空间，因此我们可以轻松计算所有可能状态-动作对的 Q 值。但当我们的动作空间是连续的，我们该如何计算所有可能状态-动作对的
    Q 值呢？假设我们正在训练一个智能体驾驶汽车，并且我们在动作空间中有一个连续的动作。假设这个动作是汽车的速度，而汽车速度的范围是从 0 到 150 公里每小时。在这种情况下，我们该如何计算所有可能状态-动作对的
    Q 值，其中动作是一个连续的值呢？
- en: In this case, we can discretize the continuous actions into speed (0 to 10)
    as action 1, speed (10 to 20) as action 2, and so on. After discretization, we
    can compute the Q value of all possible state-action pairs. However, discretization
    is not always desirable. We might lose several important features and we might
    end up in an action space with a huge set of actions.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以将连续的动作离散化为速度（0 到 10）作为动作 1，速度（10 到 20）作为动作 2，依此类推。离散化之后，我们可以计算所有可能的状态-动作对的
    Q 值。然而，离散化并不总是可取的。我们可能会丧失一些重要特征，并且可能最终会得到一个包含大量动作的动作空间。
- en: Most real-world problems have continuous action space, say, a self-driving car,
    or a robot learning to walk and more. Apart from having a continuous action space
    they also have a high dimension. Thus, the DQN and other value-based methods cannot
    deal with the continuous action space effectively.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现实世界的问题都有连续的动作空间，比如自动驾驶汽车，或者机器人学习走路等。除了拥有连续的动作空间，它们还具有高维度。因此，DQN 及其他基于值的方法无法有效地处理连续的动作空间。
- en: So, we use the policy-based methods. With policy-based methods, we don't need
    to compute the Q function (Q values) to find the optimal policy; instead, we can
    compute them directly. That is, we don't need the Q function to extract the policy.
    Policy-based methods have several advantages over value-based methods, and they
    can handle both discrete and continuous action spaces.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用基于策略的方法。在基于策略的方法中，我们不需要计算 Q 函数（Q 值）来找到最优策略；相反，我们可以直接计算它们。也就是说，我们不需要 Q
    函数来提取策略。与基于值的方法相比，基于策略的方法有几个优点，并且它们能够处理离散和连续的动作空间。
- en: We learned that DQN takes care of the exploration-exploitation dilemma by using
    the epsilon-greedy policy. With the epsilon-greedy policy, we either select the
    best action with the probability 1-epsilon or a random action with the probability
    epsilon. Most policy-based methods use a stochastic policy. We know that with
    a stochastic policy, we select actions based on the probability distribution over
    the action space, which allows the agent to explore different actions instead
    of performing the same action every time. Thus, policy-based methods take care
    of the exploration-exploitation trade-off implicitly by using a stochastic policy.
    However, there are several policy-based methods that use a deterministic policy
    as well. We will learn more about them in the upcoming chapters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，DQN 通过使用 epsilon-greedy 策略来解决探索与利用的困境。使用 epsilon-greedy 策略时，我们以 1-epsilon
    的概率选择最佳动作，或者以 epsilon 的概率选择随机动作。大多数基于策略的方法使用随机策略。我们知道，使用随机策略时，我们根据动作空间的概率分布选择动作，这使得智能体能够探索不同的动作，而不是每次都执行相同的动作。因此，基于策略的方法通过使用随机策略隐式地解决了探索与利用的权衡。然而，也有一些基于策略的方法使用确定性策略。我们将在接下来的章节中了解更多关于这些方法的内容。
- en: Okay, how do policy-based methods work, exactly? How do they find an optimal
    policy without computing the Q function? We will learn about this in the next
    section. Now that we have a basic understanding of what a policy gradient method
    is, and also the disadvantages of value-based methods, in the next section we
    will learn about a fundamental and interesting policy-based method called policy gradient.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，基于策略的方法究竟是如何工作的呢？它们是如何在不计算 Q 函数的情况下找到最优策略的？我们将在下一节中学习这一点。现在，我们已经对什么是策略梯度方法以及基于值的方法的缺点有了基本的理解，在接下来的章节中，我们将学习一种基本且有趣的基于策略的方法，称为策略梯度。
- en: Policy gradient intuition
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略梯度直觉
- en: Policy gradient is one of the most popular algorithms in deep reinforcement
    learning. As we have learned, policy gradient is a policy-based method by which
    we can find the optimal policy without computing the Q function. It finds the
    optimal policy by directly parameterizing the policy using some parameter ![](img/B15558_09_044.png).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度是深度强化学习中最流行的算法之一。正如我们所了解的，策略梯度是一种基于策略的方法，我们可以通过它在不计算 Q 函数的情况下找到最优策略。它通过使用某些参数
    ![](img/B15558_09_044.png) 来直接参数化策略，从而找到最优策略。
- en: The policy gradient method uses a stochastic policy. We have learned that with
    a stochastic policy, we select an action based on the probability distribution
    over the action space. Say we have a stochastic policy ![](img/B15558_04_099.png),
    then it gives the probability of taking an action *a* given the state *s*. It
    can be denoted by ![](img/B15558_10_003.png). In the policy gradient method, we
    use a parameterized policy, so we can denote our policy as ![](img/B15558_10_004.png),
    where ![](img/B15558_09_002.png) indicates that our policy is parameterized.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法使用的是随机策略。我们已经了解到，使用随机策略时，我们根据动作空间的概率分布来选择一个动作。假设我们有一个随机策略 ![](img/B15558_04_099.png)，那么它给出了在状态
    *s* 下采取动作 *a* 的概率。可以表示为 ![](img/B15558_10_003.png)。在策略梯度方法中，我们使用参数化的策略，因此我们可以将我们的策略表示为
    ![](img/B15558_10_004.png)，其中 ![](img/B15558_09_002.png) 表示我们的策略是参数化的。
- en: Wait! What do we mean when we say a parameterized policy? What is it exactly?
    Remember with DQN, we learned that we parameterize our Q function to compute the
    Q value? We can do the same here, except instead of parameterizing the Q function,
    we will directly parameterize the policy to compute the optimal policy. That is,
    we can use any function approximator to learn the optimal policy, and ![](img/B15558_09_054.png)
    is the parameter of our function approximator. We generally use a neural network
    as our function approximator. Thus, we have a policy ![](img/B15558_03_084.png)
    parameterized by ![](img/B15558_10_008.png) where ![](img/B15558_10_009.png) is
    the parameter of the neural network.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！当我们说一个参数化的策略时，这是什么意思呢？它到底是什么？记住，在DQN中，我们学到我们将Q函数参数化来计算Q值？我们在这里也可以做类似的事，只不过不是参数化Q函数，而是直接参数化策略来计算最优策略。也就是说，我们可以使用任何函数逼近器来学习最优策略，且
    ![](img/B15558_09_054.png) 是我们函数逼近器的参数。我们通常使用神经网络作为函数逼近器。因此，我们有一个由 ![](img/B15558_10_008.png)
    参数化的策略 ![](img/B15558_03_084.png)，其中 ![](img/B15558_10_009.png) 是神经网络的参数。
- en: Say we have a neural network with a parameter ![](img/B15558_09_029.png). First,
    we feed the state of the environment as an input to the network and it will output
    the probability of all the actions that can be performed in the state. That is,
    it outputs a probability distribution over an action space. We have learned that
    with policy gradient, we use a stochastic policy. So, the stochastic policy selects
    an action based on the probability distribution given by the neural network. In
    this way, we can directly compute the policy without using the Q function.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个带有参数的神经网络 ![](img/B15558_09_029.png)。首先，我们将环境的状态作为输入传递给网络，网络会输出在该状态下可以执行的所有动作的概率。也就是说，它输出的是动作空间上的概率分布。我们已经学过，使用策略梯度时，我们采用的是一个随机策略。所以，这个随机策略是基于神经网络给出的概率分布来选择一个动作的。通过这种方式，我们可以直接计算策略，而无需使用Q函数。
- en: 'Let''s understand how the policy gradient method works with an example. Let''s
    take our favorite grid world environment for better understanding. We know that
    in the grid world environment our action space has four possible actions: *up*,
    *down*, *left*, and *right*.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解策略梯度方法是如何工作的。为了更好地理解，我们以我们最喜欢的网格世界环境为例。我们知道，在网格世界环境中，我们的动作空间有四个可能的动作：*上*、*下*、*左*和*右*。
- en: 'Given any state as an input, the neural network will output the probability
    distribution over the action space. That is, as shown in *Figure 10.2*, when we
    feed the state **E** as an input to the network, it will return the probability
    distribution over all actions in our action space. Now, our stochastic policy
    will select an action based on the probability distribution given by the neural
    network. So, it will select action *up* 10% of the time, *down* 10% of the time,
    *left* 10% of the time, and *right* 70% of the time:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 给定任何状态作为输入，神经网络将输出一个动作空间上的概率分布。也就是说，如*图 10.2*所示，当我们将状态**E**作为输入传递给网络时，网络会返回所有动作的概率分布。现在，我们的随机策略将基于神经网络给出的概率分布选择一个动作。因此，它会在10%的时间里选择*上*，10%的时间里选择*下*，10%的时间里选择*左*，70%的时间里选择*右*：
- en: '![](img/B15558_10_03.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_03.png)'
- en: 'Figure 10.2: A policy network'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：一个策略网络
- en: We should not get confused with the DQN and the policy gradient method. With
    DQN, we feed the state as an input to the network, and it returns the Q values
    of all possible actions in that state, then we select an action that has a maximum
    Q value. But in the policy gradient method, we feed the state as input to the
    network, and it returns the probability distribution over an action space, and
    our stochastic policy uses the probability distribution returned by the neural
    network to select an action.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不应该把DQN和策略梯度方法弄混淆。使用DQN时，我们将状态作为输入传递给网络，网络会返回该状态下所有可能动作的Q值，然后我们选择具有最大Q值的动作。但在策略梯度方法中，我们将状态作为输入传递给网络，网络会返回一个动作空间上的概率分布，然后我们的随机策略使用神经网络返回的概率分布来选择一个动作。
- en: Okay, in the policy gradient method, the network returns the probability distribution
    (action probabilities) over the action space, but how accurate are the probabilities?
    How does the network learn?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，在策略梯度方法中，网络返回的是动作空间上各个动作的概率分布（动作概率），那么这些概率有多准确呢？网络是如何学习的呢？
- en: Unlike supervised learning, here we will not have any labeled data to train
    our network. So, our network does not know the correct action to perform in the
    given state; that is, the network does not know which action gives the maximum
    reward. So, the action probabilities given by our neural network will not be accurate
    in the initial iterations, and thus we might get a bad reward.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习不同，在这里我们没有任何标签数据来训练我们的网络。因此，我们的网络不知道在给定状态下应该执行哪个正确的动作；也就是说，网络不知道哪个动作能获得最大的奖励。因此，在初始迭代中，我们的神经网络给出的动作概率将不准确，从而我们可能会得到不好的回报。
- en: But that is fine. We simply select the action based on the probability distribution
    given by the network, store the reward, and move to the next state until the end
    of the episode. That is, we play an episode and store the states, actions, and
    rewards. Now, this becomes our training data. If we win the episode, that is,
    if we get a positive return or high return (the sum of the rewards of the episode),
    then we increase the probability of all the actions that we took in each state
    until the end of the episode. If we get a negative return or low return, then
    we decrease the probability of all the actions that we took in each state until
    the end of the episode.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这没关系。我们只是根据网络给出的概率分布选择动作，存储回报，并在本回合结束前进入下一个状态。也就是说，我们玩一个回合，并存储状态、动作和回报。现在，这些就成了我们的训练数据。如果我们赢得了本回合，也就是如果我们得到了正回报或高回报（本回合所有奖励的总和），那么我们会增加本回合中所有动作的概率。如果我们得到了负回报或低回报，那么我们会降低本回合中所有动作的概率，直到本回合结束。
- en: 'Let''s understand this with an example. Say we have states *s*[1] to *s*[8]
    and our goal is to reach state *s*[8]. Say our action space consists of only two
    actions: *left and right.* So, when we feed any state to the network, then it
    will return the probability distribution over the two actions.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解这一点。假设我们有从 *s*[1] 到 *s*[8] 的状态，我们的目标是到达 *s*[8] 状态。假设我们的动作空间仅包含两个动作：*left*
    和 *right*。因此，当我们将任何状态输入到网络时，它会返回关于这两个动作的概率分布。
- en: 'Consider the following trajectory (episode) ![](img/B15558_10_011.png), where
    we select an action in each state based on the probability distribution returned
    by the network using a stochastic policy:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下轨迹（回合） ![](img/B15558_10_011.png)，其中我们基于网络返回的概率分布，通过随机策略在每个状态中选择一个动作：
- en: '![](img/B15558_10_04.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_04.png)'
- en: 'Figure 10.3: Trajectory ![](img/B15558_10_012.png)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：轨迹 ![](img/B15558_10_012.png)
- en: The return of this trajectory is ![](img/B15558_10_013.png). Since we got a
    positive return, we increase the probabilities of all the actions that we took
    in each state until the end of the episode. That is, we increase the probabilities
    of action *left* in *s*[1], action *right* in *s*[2], and so on until the end
    of the episode.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 该轨迹的回报是 ![](img/B15558_10_013.png)。由于我们得到了正回报，我们会增加每个状态中所有动作的概率，直到本回合结束。也就是说，我们会增加在
    *s*[1] 中执行 *left* 动作的概率，在 *s*[2] 中执行 *right* 动作的概率，依此类推，直到本回合结束。
- en: 'Let''s suppose we generate another trajectory ![](img/B15558_10_014.png), where
    we select an action in each state based on the probability distribution returned
    by the network using a stochastic policy, as shown in *Figure 10.4*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们生成了另一条轨迹 ![](img/B15558_10_014.png)，在其中，我们根据网络返回的概率分布，通过随机策略在每个状态中选择一个动作，如
    *图 10.4* 所示：
- en: '![](img/B15558_10_05.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_05.png)'
- en: 'Figure 10.4: Trajectory ![](img/B15558_10_015.png)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4：轨迹 ![](img/B15558_10_015.png)
- en: The return of this trajectory is ![](img/B15558_10_016.png). Since we got a
    negative return, we decrease the probabilities of all the actions that we took
    in each state until the end of the episode. That is, we will decrease the probabilities
    of action *right* in *s*[1], action *right* in *s*[3], and so on until the end
    of the episode.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该轨迹的回报是 ![](img/B15558_10_016.png)。由于我们得到了负回报，我们会降低每个状态中所有动作的概率，直到本回合结束。也就是说，我们将降低在
    *s*[1] 中执行 *right* 动作的概率，在 *s*[3] 中执行 *right* 动作的概率，依此类推，直到本回合结束。
- en: Okay, but how exactly do we increase and decrease these probabilities? We learned
    that if the return of the trajectory is positive, then we increase the probabilities
    of all actions in the episode, else we decrease it. How can we do this exactly?
    This is where backpropagation helps us. We know that we train the neural network
    by backpropagation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但我们究竟如何增加或减少这些概率呢？我们已经知道，如果轨迹的回报是正的，那么我们会增加本回合中所有动作的概率；否则我们会减少它。那么我们究竟该如何操作呢？这正是反向传播的帮助所在。我们知道，我们通过反向传播来训练神经网络。
- en: So, during backpropagation, the network calculates gradients and updates the
    parameters of the network ![](img/B15558_09_098.png). Gradients updates are in
    such a way that actions yielding high return will get high probabilities and actions
    yielding low return will get low probabilities.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在反向传播过程中，网络计算梯度并更新网络参数 ![](img/B15558_09_098.png)。梯度更新的方式是，高回报的动作会获得高概率，而低回报的动作会获得低概率。
- en: In a nutshell, in the policy gradient method, we use a neural network to find
    the optimal policy. We initialize the network parameter ![](img/B15558_09_054.png)
    with random values. We feed the state as an input to the network and it will return
    the action probabilities. In the initial iteration, since the network is not trained
    with any data, it will give random action probabilities. But we select actions
    based on the action probability distribution given by the network and store the
    state, action, and reward until the end of the episode. Now, this becomes our
    training data. If we win the episode, that is, if we get a high return, then we
    assign high probabilities to all the actions of the episode, else we assign low
    probabilities to all the actions of the episode.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在策略梯度方法中，我们使用神经网络来找到最优策略。我们将网络参数 ![](img/B15558_09_054.png) 初始化为随机值。我们将状态作为输入传入网络，网络会返回动作概率。在初始迭代中，由于网络没有经过任何数据的训练，它会给出随机的动作概率。但我们根据网络给定的动作概率分布选择动作，并将状态、动作和奖励存储到剧集结束为止。现在，这些数据成为了我们的训练数据。如果我们赢得了这一轮，即得到了高回报，那么我们会给这一轮的所有动作分配高概率，否则，我们会给这一轮的所有动作分配低概率。
- en: Since we are using a neural network to find the optimal policy, we can call
    this neural network a policy network. Now that we have a basic understanding of
    the policy gradient method, in the next section, we will learn how exactly the
    neural network finds the optimal policy; that is, we will learn how exactly the
    gradient computation happens and how we train the network.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用神经网络来找到最优策略，我们可以将这个神经网络称为策略网络。现在我们对策略梯度方法有了基本的理解，在下一节中，我们将学习神经网络如何精确找到最优策略；也就是说，我们将学习梯度计算是如何进行的，以及如何训练网络。
- en: Understanding the policy gradient
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解策略梯度
- en: In the last section, we learned that, in the policy gradient method, we update
    the gradients in such a way that actions yielding a high return will get a high
    probability, and actions yielding a low return will get a low probability. In
    this section, we will learn how exactly we do that.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学到了，在策略梯度方法中，我们以一种方式更新梯度，使得高回报的动作会得到高概率，低回报的动作会得到低概率。在本节中，我们将学习如何精确做到这一点。
- en: 'The goal of the policy gradient method is to find the optimal parameter ![](img/B15558_09_118.png)
    of the neural network so that the network returns the correct probability distribution
    over the action space. Thus, the objective of our network is to assign high probabilities
    to actions that maximize the expected return of the trajectory. So, we can write
    our objective function *J* as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法的目标是找到神经网络的最优参数 ![](img/B15558_09_118.png)，使得网络能够返回正确的动作空间概率分布。因此，我们网络的目标是将高概率分配给那些最大化轨迹期望回报的动作。所以，我们可以将目标函数
    *J* 写作：
- en: '![](img/B15558_10_020.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_020.png)'
- en: 'In the preceding equation, the following applies:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，以下内容适用：
- en: '![](img/B15558_03_037.png) is the trajectory.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_03_037.png) 是轨迹。'
- en: '![](img/B15558_10_022.png) denotes that we are sampling the trajectory based
    on the policy ![](img/B15558_04_032.png) given by network parameterized by ![](img/B15558_09_056.png).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_10_022.png) 表示我们根据由 ![](img/B15558_09_056.png) 参数化的网络给定的策略 ![](img/B15558_04_032.png)
    来采样轨迹。'
- en: '![](img/B15558_10_025.png) is the return of the trajectory ![](img/B15558_10_026.png).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_10_025.png) 是轨迹 ![](img/B15558_10_026.png) 的回报。'
- en: 'Thus, maximizing our objective function maximizes the return of the trajectory.
    How can we maximize the preceding objective function? We generally deal with minimization
    problems, where we minimize the loss function (objective function) by calculating
    the gradients of our loss function and updating the parameter using gradient descent.
    But here, our goal is to maximize the objective function, so we calculate the
    gradients of our objective function and perform gradient ascent. That is:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最大化我们的目标函数即最大化轨迹的回报。那么，如何最大化前面的目标函数呢？我们通常处理的是最小化问题，通过计算损失函数的梯度并使用梯度下降法更新参数，来最小化损失函数（即目标函数）。但在这里，我们的目标是最大化目标函数，因此我们计算目标函数的梯度并进行梯度上升。即：
- en: '![](img/B15558_10_027.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_027.png)'
- en: Where ![](img/B15558_10_028.png) implies the gradients of our objective function.
    Thus, we can find the optimal parameter ![](img/B15558_09_118.png) of our network
    using gradient ascent.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_10_028.png) 表示我们目标函数的梯度。因此，我们可以使用梯度上升法找到我们网络的最优参数 ![](img/B15558_09_118.png)。
- en: The gradient ![](img/B15558_10_030.png) is derived as ![](img/B15558_10_031.png).
    We will learn how exactly we derived this gradient in the next section. In this
    section, let's focus only on getting a good fundamental understanding of the policy
    gradient.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度 ![](img/B15558_10_030.png) 是通过 ![](img/B15558_10_031.png) 推导出来的。我们将在下一节学习如何推导这个梯度。在本节中，我们暂时只关注如何获得对策略梯度的良好基础理解。
- en: 'We learned that we update our network parameter with:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，我们通过以下方式更新我们的网络参数：
- en: '![](img/B15558_10_027.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_027.png)'
- en: 'Substituting the value of gradient, our parameter update equation becomes:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将梯度的值代入后，我们的参数更新方程变为：
- en: '![](img/B15558_10_033.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_033.png)'
- en: 'In the preceding equation, the following applies:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，以下内容适用：
- en: '![](img/B15558_10_034.png) represents the log probability of taking an action
    *a* given the state *s* at a time *t*.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_10_034.png) 代表在时间 *t* 给定状态 *s* 时采取动作 *a* 的对数概率。'
- en: '![](img/B15558_10_035.png) represents the return of the trajectory.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_10_035.png) 代表轨迹的回报。'
- en: We learned that we update the gradients in such a way that actions yielding
    a high return will get a high probability, and actions yielding a low return will
    get a low probability. Let's now see how exactly we are doing that.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，我们以这样的方式更新梯度：使得产生高回报的动作获得较高的概率，而产生低回报的动作获得较低的概率。现在让我们来看看到底是怎么做到的。
- en: '**Case 1**:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 1**：'
- en: Suppose we generate an episode (trajectory) using the policy ![](img/B15558_10_036.png),
    where ![](img/B15558_10_037.png) is the parameter of the network. After generating
    the episode, we compute the return of the episode. If the return of the episode
    is negative, say -1, that is, ![](img/B15558_10_038.png), then we decrease the
    probability of all the actions that we took in each state until the end of the
    episode.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用策略 ![](img/B15558_10_036.png) 生成一个回合（轨迹），其中 ![](img/B15558_10_037.png)
    是网络的参数。在生成回合后，我们计算该回合的回报。如果回合的回报是负的，比如 -1，即 ![](img/B15558_10_038.png)，那么我们会减少我们在每个状态下采取的所有动作的概率，直到回合结束。
- en: 'We learned that our parameter update equation is given as:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，我们的参数更新方程可以表示为：
- en: '![](img/B15558_10_033.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_033.png)'
- en: 'In the preceding equation, multiplying ![](img/B15558_10_040.png) by the negative
    return ![](img/B15558_10_038.png) implies that we are decreasing the log probability
    of action *a*[t] in state *s*[t]. Thus, we perform a negative update. That is:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，将 ![](img/B15558_10_040.png) 乘以负回报 ![](img/B15558_10_038.png) 表示我们正在减少在状态
    *s*[t] 下，采取动作 *a*[t] 的对数概率。因此，我们执行一个负向更新。即：
- en: 'For each step in the episode, *t* = 0, . . ., *T*-1, we update the parameter
    ![](img/B15558_09_087.png) as:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回合中的每一步，*t* = 0, . . ., *T*-1，我们按照以下方式更新参数 ![](img/B15558_09_087.png)：
- en: '![](img/B15558_10_043.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_043.png)'
- en: It implies that we are decreasing the probability of all the actions that we
    took in each state until the end of the episode.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们在每个状态下，直到回合结束，都会减少我们采取的所有动作的概率。
- en: '**Case 2**:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 2**：'
- en: Suppose we generate an episode (trajectory) using the policy ![](img/B15558_10_044.png),
    where ![](img/B15558_09_054.png) is the parameter of the network. After generating
    the episode, we compute the return of the episode. If the return of the episode
    is positive, say +1, that is, ![](img/B15558_10_046.png), then we increase the
    probability of all the actions that we took in each state until the end of the
    episode.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用策略 ![](img/B15558_10_044.png) 生成一个剧集（轨迹），其中 ![](img/B15558_09_054.png)
    是网络的参数。生成剧集后，我们计算该剧集的回报。如果剧集的回报为正，比如 +1，也就是 ![](img/B15558_10_046.png)，那么我们会增加每个状态下采取的所有行动的概率，直到剧集结束。
- en: 'We learned that our parameter update equation is given as:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到我们的参数更新方程如下：
- en: '![](img/B15558_10_033.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_033.png)'
- en: 'In the preceding equation, multiplying ![](img/B15558_10_048.png) by the positive
    return, ![](img/B15558_10_046.png), means that we are increasing the log probability
    of action *a*[t] in the state *s*[t]. Thus, we perform a positive update. That
    is:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，将 ![](img/B15558_10_048.png) 乘以正回报 ![](img/B15558_10_046.png) 意味着我们正在增加在状态
    *s*[t] 中执行动作 *a*[t] 的对数概率。因此，我们执行一个正向更新。也就是：
- en: 'For each step in the episode, *t* = 0, . . ., *T*-1, we update the parameter
    ![](img/B15558_09_098.png) as:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于剧集中的每一步，*t* = 0, . . ., *T*-1，我们更新参数 ![](img/B15558_09_098.png) 如下：
- en: '![](img/B15558_10_051.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_051.png)'
- en: Thus, if we get a positive return then we increase the probability of all the
    actions performed in that episode, else we will decrease the probability.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们得到一个正回报，我们会增加该剧集中所有执行动作的概率，否则我们会降低其概率。
- en: 'We learned that, for each step in the episode, *t* = 0, . . ., *T*-1, we update
    the parameter ![](img/B15558_10_037.png) as:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，对于剧集中的每一步，*t* = 0, . . ., *T*-1，我们更新参数 ![](img/B15558_10_037.png) 如下：
- en: '![](img/B15558_10_033.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_033.png)'
- en: 'We can simply denote the preceding equation as:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简化前述方程为：
- en: '![](img/B15558_10_054.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_054.png)'
- en: 'Thus, if the episode (trajectory) gives a high return, we will increase the
    probabilities of all the actions of the episode, else we decrease the probabilities.
    We learned that ![](img/B15558_10_031.png). What about that expectation? We have
    not included that in our update equation yet. When we looked at the Monte Carlo
    method, we learned that we can approximate the expectation using the average.
    Thus, using the Monte Carlo approximation method, we change the expectation term to
    the sum over *N* trajectories. So, our update equation becomes:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果剧集（轨迹）给出了较高的回报，我们将增加剧集中所有动作的概率，否则我们会降低它们的概率。我们了解到 ![](img/B15558_10_031.png)。那期望值呢？我们在更新方程中还没有考虑到这一点。当我们查看蒙特卡洛方法时，我们了解到可以通过平均值来近似期望值。因此，使用蒙特卡洛近似方法，我们将期望项改为对
    *N* 条轨迹的求和。因此，我们的更新方程变为：
- en: '![](img/B15558_10_056.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_056.png)'
- en: 'It shows that instead of updating the parameter based on a single trajectory,
    we collect a set of *N* trajectories following the policy ![](img/B15558_10_057.png)
    and update the parameter based on the average value, that is:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 它表明，与其基于单一轨迹更新参数，我们收集一组 *N* 条轨迹来跟随策略 ![](img/B15558_10_057.png)，并根据平均值更新参数，即：
- en: '![](img/B15558_10_08.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_08.png)'
- en: 'Thus, first, we collect *N* number of trajectories ![](img/B15558_10_058.png)
    following the policy ![](img/B15558_10_057.png) and compute the gradient as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先，我们收集 *N* 条轨迹 ![](img/B15558_10_058.png) 来跟随策略 ![](img/B15558_10_057.png)，并计算梯度如下：
- en: '![](img/B15558_10_060.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_060.png)'
- en: 'And then we update our parameter as:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们更新参数如下：
- en: '![](img/B15558_10_027.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_027.png)'
- en: But we can't find the optimal parameter ![](img/B15558_09_098.png) by updating
    the parameter for just one iteration. So, we repeat the previous step for many
    iterations to find the optimal parameter.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们无法仅通过更新参数一次迭代来找到最优参数 ![](img/B15558_09_098.png)。因此，我们需要重复之前的步骤进行多次迭代，以找到最优参数。
- en: Now that we have a fundamental understanding of how policy gradient method work,
    in the next section, we will learn how to derive the policy gradient ![](img/B15558_10_063.png).
    After that, we will learn about the policy gradient algorithm in detail step by
    step.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对策略梯度方法的工作原理有了基本的了解，在下一节中，我们将学习如何推导策略梯度 ![](img/B15558_10_063.png)。之后，我们将一步一步地详细学习策略梯度算法。
- en: Deriving the policy gradient
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推导策略梯度
- en: In this section, we will get into more details and learn how to compute the
    gradient ![](img/B15558_10_064.png) and how it is equal to ![](img/B15558_10_065.png).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将深入了解并学习如何计算梯度 ![](img/B15558_10_064.png)，以及它如何等于 ![](img/B15558_10_065.png)。
- en: Let's deep dive into the interesting math and see how to calculate the derivative
    of our objective function *J* with respect to the model parameter ![](img/B15558_10_066.png)
    in simple steps. Don't get intimidated by the upcoming equations, it's actually
    a pretty simple derivation. Before going ahead, let's revise some math prerequisites
    in order to understand our derivation better.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一下有趣的数学，并看看如何简单地计算目标函数 *J* 关于模型参数 ![](img/B15558_10_066.png) 的导数。不要被接下来的方程吓到，其实这只是一个相当简单的推导。在继续之前，让我们复习一些数学基础知识，以便更好地理解我们的推导。
- en: 'Definition of the expectation:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 期望的定义：
- en: 'Let *X* be a discrete random variable whose **probability mass function** (**pmf**)
    is given as *p*(*x*). Let *f* be a function of a discrete random variable *X*.
    Then the expectation of a function *f*(*X*) can be defined as:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 设 *X* 是一个离散随机变量，其 **概率质量函数** (**pmf**) 为 *p*(*x*)。设 *f* 是离散随机变量 *X* 的一个函数。那么，函数
    *f*(*X*) 的期望可以定义为：
- en: '![](img/B15558_10_067.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_067.png)'
- en: 'Let *X* be a continuous random variable whose **probability density function**
    (**pdf**) is given as *p*(*x*). Let *f* be a function of a continuous random variable
    *X*. Then the expectation of a function *f*(*X*) can be defined as:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 设 *X* 是一个连续随机变量，其 **概率密度函数** (**pdf**) 为 *p*(*x*)。设 *f* 是连续随机变量 *X* 的一个函数。那么，函数
    *f*(*X*) 的期望可以定义为：
- en: '![](img/B15558_10_068.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_068.png)'
- en: 'A log derivative trick is given as:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一个对数导数技巧如下：
- en: '![](img/B15558_10_069.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_069.png)'
- en: 'We learned that the objective of our network is to maximize the expected return
    of the trajectory. Thus, we can write our objective function *J* as:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，网络的目标是最大化轨迹的期望回报。因此，我们可以将目标函数 *J* 写成：
- en: '![](img/B15558_10_070.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_070.png)'
- en: 'In the preceding equation, the following applies:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，以下条件适用：
- en: '![](img/B15558_10_071.png) is the trajectory.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_10_071.png) 是轨迹。'
- en: '![](img/B15558_10_072.png) shows that we are sampling the trajectory based
    on the policy ![](img/B15558_03_139.png) given by network parameterized by ![](img/B15558_09_002.png).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_10_072.png) 表示我们正在基于由 ![](img/B15558_09_002.png) 参数化的网络给定的策略
    ![](img/B15558_03_139.png) 对轨迹进行采样。'
- en: '![](img/B15558_10_075.png) is the return of the trajectory.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B15558_10_075.png) 是轨迹的回报。'
- en: 'As we can see, our objective function, equation (4), is in the expectation
    form. From the definition of the expectation given in equation (2), we can expand
    the expectation and rewrite equation (4) as:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们的目标函数，方程 (4)，处于期望形式。根据方程 (2) 中给出的期望定义，我们可以展开期望并将方程 (4) 重写为：
- en: '![](img/B15558_10_076.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_076.png)'
- en: 'Now, we calculate the derivative of our objective function *J* with respect
    to ![](img/B15558_10_037.png):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算目标函数 *J* 关于 ![](img/B15558_10_037.png) 的导数：
- en: '![](img/B15558_10_078.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_078.png)'
- en: 'Multiplying and dividing by ![](img/B15558_10_079.png), we can write:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过乘以并除以 ![](img/B15558_10_079.png)，我们可以写成：
- en: '![](img/B15558_10_080.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_080.png)'
- en: 'Rearranging the preceding equation, we can write:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新排列上述方程，我们可以写成：
- en: '![](img/B15558_10_081.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_081.png)'
- en: 'From equation (3), substituting ![](img/B15558_10_082.png) in the preceding
    equation, we can write:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从方程 (3) 中，将 ![](img/B15558_10_082.png) 代入上面的方程，我们可以写成：
- en: '![](img/B15558_10_083.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_083.png)'
- en: 'From the definition of expectation given in equation (2), we can rewrite the
    preceding equation in expectation form as:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 从方程 (2) 给出的期望定义，我们可以将上面的方程重新写成期望形式：
- en: '![](img/B15558_10_084.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_084.png)'
- en: The preceding equation gives us the gradient of the objective function. But
    we still haven't solved the equation yet. As we can see, in the preceding equation
    we have the term ![](img/B15558_10_085.png). Now we will see how we can compute
    that.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的方程给出了目标函数的梯度。但我们仍然没有解决方程。正如我们所看到的，在上述方程中，我们有项 ![](img/B15558_10_085.png)。现在我们来看一下如何计算它。
- en: 'The probability distribution of trajectory can be given as:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹的概率分布可以表示为：
- en: '![](img/B15558_10_086.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_086.png)'
- en: 'Where *p*(*s*[0]) is the initial state distribution. Taking the log on both
    sides, we can write:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p*(*s*[0]) 是初始状态分布。对两边取对数，我们可以写成：
- en: '![](img/B15558_10_087.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_087.png)'
- en: 'We know that the log of a product is equal to the sum of the logs, that is,
    ![](img/B15558_10_088.png). Applying this log rule to the preceding equation,
    we can write:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，乘积的对数等于对数的和，也就是说，![](img/B15558_10_088.png)。将这个对数规则应用到上面的方程中，我们可以写成：
- en: '![](img/B15558_10_089.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_089.png)'
- en: 'Again, we apply the same rule, log of product = sum of logs, and change the
    log ![](img/B15558_10_090.png) to ![](img/B15558_10_091.png) logs, as shown here:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 再次应用相同的规则，乘积的对数 = 对数之和，并将对数![](img/B15558_10_090.png)更改为![](img/B15558_10_091.png)的对数，如下所示：
- en: '![](img/B15558_10_094.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_094.png)'
- en: 'Now, we compute the derivate with respect to ![](img/B15558_10_095.png):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们计算相对于![](img/B15558_10_095.png)的导数：
- en: '![](img/B15558_10_096.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_096.png)'
- en: 'Note that we are calculating derivative with respect to ![](img/B15558_09_087.png)
    and, as we can see in the preceding equation, the first and last term on the **right-hand
    side** (**RHS**) does not depend on the ![](img/B15558_09_106.png), and so they
    will become zero while calculating derivative. Thus our equation becomes:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们正在计算相对于![](img/B15558_09_087.png)的导数，正如我们在前面的方程中看到的，**右侧**（**RHS**）的第一项和最后一项不依赖于![](img/B15558_09_106.png)，因此在计算导数时它们会变为零。因此，我们的方程变为：
- en: '![](img/B15558_10_099.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_099.png)'
- en: 'Now that we have found the value for ![](img/B15558_10_100.png), substituting
    this in equation (5) we can write:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经找到了![](img/B15558_10_100.png)的值，将其代入方程（5）中可以写出：
- en: '![](img/B15558_10_101.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_101.png)'
- en: 'That''s it. But can we also get rid of that expectation? Yes! We can use a
    Monte Carlo approximation method and change the expectation to the sum over *N*
    trajectories. So, our final gradient becomes:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。但我们能否也去掉那个期望？是的！我们可以使用蒙特卡罗近似方法，将期望改为*N*个轨迹的总和。因此，我们的最终梯度变为：
- en: '![](img/B15558_10_102.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_102.png)'
- en: Equation (6) shows that instead of updating a parameter based on a single trajectory,
    we collect *N* number of trajectories and update the parameter based on its average
    value over *N* trajectories.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（6）表明，代替基于单一轨迹更新参数，我们收集*N*个轨迹，并基于这*N*个轨迹的平均值来更新参数。
- en: 'Thus, after computing the gradient, we can update our parameter as:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在计算梯度之后，我们可以更新我们的参数为：
- en: '![](img/B15558_10_027.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_027.png)'
- en: Thus, in this section, we have learned how to derive a policy gradient. In the
    next section, we will get into more details and learn about the policy gradient
    algorithm step by step.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这一部分中，我们学会了如何推导策略梯度。在下一部分中，我们将深入了解更多细节，逐步学习策略梯度算法。
- en: Algorithm – policy gradient
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法 – 策略梯度
- en: 'The policy gradient algorithm we discussed so far is often called REINFORCE
    or **Monte Carlo policy gradient**. The algorithm of the REINFORCE method is given
    in the following steps:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前讨论的策略梯度算法通常称为REINFORCE或**蒙特卡罗策略梯度**。REINFORCE方法的算法如下所示：
- en: Initialize the network parameter ![](img/B15558_10_095.png) with random values
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化网络参数![](img/B15558_10_095.png)
- en: Generate *N* trajectories ![](img/B15558_10_105.png) following the policy ![](img/B15558_10_057.png)
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成*N*个轨迹![](img/B15558_10_105.png)，遵循策略![](img/B15558_10_057.png)
- en: Compute the return of the trajectory ![](img/B15558_10_107.png)
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算轨迹的回报![](img/B15558_10_107.png)
- en: Compute the gradients![](img/B15558_10_108.png)
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度！![](img/B15558_10_108.png)
- en: Update the network parameter as ![](img/B15558_10_109.png)
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新网络参数为![](img/B15558_10_109.png)
- en: Repeat *steps 2* to *5* for several iterations
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤2*至*5*进行多次迭代
- en: As we can see from this algorithm, the parameter ![](img/B15558_09_054.png)
    is getting updated in every iteration. Since we are using the parameterized policy
    ![](img/B15558_10_111.png), our policy is getting updated in every iteration.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从这个算法中看到的，参数![](img/B15558_09_054.png)在每次迭代中都在更新。由于我们使用了参数化策略![](img/B15558_10_111.png)，我们的策略在每次迭代中都在更新。
- en: The policy gradient algorithm we just learned is an on-policy method, as we
    are using only a single policy. That is, we are using a policy to generate trajectories
    and we are also improving the same policy by updating the network parameter ![](img/B15558_09_054.png)
    in every iteration.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习的策略梯度算法是一种在线方法，因为我们只使用单一策略。也就是说，我们使用一个策略生成轨迹，并且通过在每次迭代中更新网络参数![](img/B15558_09_054.png)来改进同一个策略。
- en: We learned that with the policy gradient method (the REINFORCE method), we use
    a policy network that returns the probability distribution over the action space
    and then we select an action based on the probability distribution returned by
    our network using a stochastic policy. But this applies only to a discrete action
    space, and we use categorical policy as our stochastic policy.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，使用策略梯度方法（REINFORCE方法）时，我们使用一个策略网络，该网络返回动作空间的概率分布，然后我们根据网络返回的概率分布使用随机策略选择一个动作。但这仅适用于离散的动作空间，并且我们使用类别策略作为我们的随机策略。
- en: What if our action space is continuous? That is, when the action space is continuous,
    how can we select actions? Here, our policy network cannot return the probability
    distribution over the action space as the action space is continuous. So, in this
    case, our policy network will return the mean and variance of the action as output,
    and then we generate a Gaussian distribution using this mean and variance and
    select an action by sampling from this Gaussian distribution using the Gaussian
    policy. We will learn more about this in the upcoming chapters. Thus, we can apply
    the policy gradient method to both discrete and continuous action spaces. Next,
    we will look at two methods to reduce the variance of policy gradient updates.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的动作空间是连续的呢？也就是说，当动作空间是连续的时，我们如何选择动作？在这种情况下，我们的策略网络不能返回动作空间上的概率分布，因为动作空间是连续的。所以，在这种情况下，我们的策略网络将返回动作的均值和方差作为输出，然后我们使用这个均值和方差生成一个高斯分布，并通过从该高斯分布中采样来选择动作，采用高斯策略。我们将在接下来的章节中详细学习这个方法。因此，我们可以将策略梯度方法应用于离散和连续的动作空间。接下来，我们将学习两种减少策略梯度更新方差的方法。
- en: Variance reduction methods
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方差减少方法
- en: In the previous section, we learned one of the simplest policy gradient methods,
    called the REINFORCE method. One major issue we face with the policy gradient
    method we learned in the previous section is that the gradient, ![](img/B15558_10_113.png),
    will have high variance in each update. The high variance is basically due to
    the major difference in the episodic returns. That is, we learned that policy
    gradient is the on-policy method, which means that we improve the same policy
    with which we are generating episodes in every iteration. Since the policy is
    getting improved on every iteration, our return varies greatly in each episode
    and it introduces a high variance in the gradient updates. When the gradients
    have high variance, then it will take a lot of time to attain convergence.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学到了最简单的策略梯度方法之一，叫做 REINFORCE 方法。我们在上一节学到的策略梯度方法面临的一个主要问题是，梯度 ![](img/B15558_10_113.png)
    在每次更新中会有很高的方差。高方差主要是由于每次回合的回报差异较大。也就是说，我们学到策略梯度是一种在策略上的方法，这意味着我们在每次迭代中都使用相同的策略来生成回合。由于策略在每次迭代中都在改进，我们的回报在每个回合中差异很大，这在梯度更新中引入了很高的方差。当梯度具有高方差时，将需要很长时间才能达到收敛。
- en: 'Thus, now we will learn the following two important methods to reduce the variance:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们将学习以下两种减少方差的重要方法：
- en: Policy gradients with reward-to-go (causality)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有奖励到达（因果关系）的策略梯度
- en: Policy gradients with baseline
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有基线的策略梯度
- en: Policy gradient with reward-to-go
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有奖励到达的策略梯度
- en: 'We learned that the policy gradient is computed as:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到，策略梯度的计算方式是：
- en: '![](img/B15558_10_108.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_108.png)'
- en: 'Now, we make a small change in the preceding equation. We know that the return
    of the trajectory is the sum of the rewards of that trajectory, that is:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对前面的公式做一个小的修改。我们知道，轨迹的回报是该轨迹奖励的总和，即：
- en: '![](img/B15558_10_115.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_115.png)'
- en: Instead of using the return of trajectory ![](img/B15558_10_116.png), we use
    something called reward-to-go *R*[t]. Reward-to-go is basically the return of
    the trajectory starting from state *s*[t]. That is, instead of multiplying the
    log probabilities by the return of the full trajectory ![](img/B15558_10_117.png)
    in every step of the episode, we multiply them by the reward-to-go *R*[t]. The
    reward-to-go implies the return of the trajectory starting from state *s*[t].
    But why do we have to do this? Let's understand this in more detail with an example.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是使用轨迹的回报 ![](img/B15558_10_116.png)，而是使用一种叫做奖励到达 *R*[t] 的东西。奖励到达基本上是从状态 *s*[t]
    开始的轨迹的回报。也就是说，我们不是在每个回合的每一步都将对数概率乘以整个轨迹的回报 ![](img/B15558_10_117.png)，而是将其乘以奖励到达
    *R*[t]。奖励到达意味着从状态 *s*[t] 开始的轨迹的回报。但为什么我们要这么做呢？让我们通过一个例子更详细地理解这一点。
- en: 'We learned that we generate *N* number of trajectories and compute the gradient
    as:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到，我们生成 *N* 个轨迹并计算梯度如下：
- en: '![](img/B15558_10_108.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_108.png)'
- en: 'For better understanding, let''s take only one trajectory by setting *N*=1,
    so we can write:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，我们只取一个轨迹，设置 *N*=1，所以我们可以写成：
- en: '![](img/B15558_10_119.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_119.png)'
- en: 'Say, we generated the following trajectory with the policy ![](img/B15558_10_120.png):'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，我们用策略 ![](img/B15558_10_120.png) 生成了以下轨迹：
- en: '![](img/B15558_10_06.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_06.png)'
- en: 'Figure 10.5: Trajectory'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：轨迹
- en: The return of the preceding trajectory is ![](img/B15558_10_121.png).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 前述轨迹的回报是 ![](img/B15558_10_121.png)。
- en: 'Now, we can compute gradient as:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算梯度为：
- en: '![](img/B15558_10_122.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_122.png)'
- en: As we can observe from the preceding equation, in every step of the episode,
    we are multiplying the log probability of the action by the return of the full
    trajectory ![](img/B15558_10_107.png), which is 2 in the preceding example.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从之前的方程中观察到的那样，在每一步中，我们都在将动作的对数概率与整个轨迹的回报相乘 ![](img/B15558_10_107.png)，在前面的例子中它是2。
- en: Let's suppose we want to know how good the action *right* is in the state *s*[2].
    If we understand that the action *right* is a good action in the state *s*[2],
    then we can increase the probability of moving *right* in the state *s*[2], else
    we decrease it. Okay, how can we tell whether the action *right* is good in the
    state *s*[2]? As we learned in the previous section (when discussing the REINFORCE
    method), if the return of the trajectory ![](img/B15558_10_075.png) is high, then
    we increase the probability of the action *right* in the state *s*[2], else we decrease
    it.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想知道在状态 *s*[2] 下 *right* 动作的效果。如果我们理解到 *right* 是状态 *s*[2] 下一个好的动作，那么我们可以增加在状态
    *s*[2] 下向右移动的概率，否则我们降低它。好，那么我们如何判断 *right* 动作在状态 *s*[2] 下是否好呢？正如我们在前一部分中所学到的（讨论
    REINFORCE 方法时），如果轨迹的回报 ![](img/B15558_10_075.png) 很高，那么我们增加状态 *s*[2] 下 *right*
    动作的概率，否则我们降低它。
- en: 'But we don''t have to do that now. Instead, we can compute the return (the
    sum of the rewards of the trajectory) only starting from the state *s*[2] because
    there is no use in including all the rewards that we obtain from the trajectory
    before taking the action *right* in the state *s*[2]. As *Figure 10.6* shows,
    including all the rewards that we obtain before taking the action *right* in the
    state *s*[2] will not help us understand how good the action *right* is in the
    state *s*[2]:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在我们不必这样做。相反，我们可以只从状态 *s*[2] 开始计算回报（轨迹的奖励总和），因为在采取 *right* 动作时，包含从轨迹开始到达状态
    *s*[2] 之前获得的所有奖励并没有用处。正如 *图10.6* 所示，包含所有这些奖励并不会帮助我们理解在状态 *s*[2] 下 *right* 动作的效果：
- en: '![](img/B15558_10_07.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_07.png)'
- en: 'Figure 10.6: Trajectory'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6：轨迹
- en: Thus, instead of taking the complete return of the trajectory in all the steps
    of the episode, we use reward-to-go *R*[t], which is the return of the trajectory
    starting from the state *s*[t].
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不再在所有步骤中使用整个轨迹的回报，而是使用奖励到达 *R*[t]，它表示从状态 *s*[t] 开始的轨迹的回报。
- en: 'Thus, now we can write:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们可以写出：
- en: '![](img/B15558_10_125.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_125.png)'
- en: Where *R*[0] indicates the return of the trajectory starting from the state
    *s*[0], *R*[1] indicates the return of the trajectory starting from the state
    *s*[1], and so on. If *R*[0] is high value, then we increase the probability of
    the action *up* in the state *s*[0], else we decrease it. If *R*[1] is high value,
    then we increase the probability of the action *down* in the state *s*[1], else
    we decrease it.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *R*[0] 表示从状态 *s*[0] 开始的轨迹的回报，*R*[1] 表示从状态 *s*[1] 开始的轨迹的回报，依此类推。如果 *R*[0]
    值较高，则我们增加状态 *s*[0] 下 *up* 动作的概率，否则我们降低它。如果 *R*[1] 值较高，则我们增加状态 *s*[1] 下 *down*
    动作的概率，否则我们降低它。
- en: 'Thus, now, we can define the reward-to-go as:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们可以将奖励到达定义为：
- en: '![](img/B15558_10_126.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_126.png)'
- en: 'The preceding equation states that the reward-to-go *R*[t] is the sum of rewards
    of the trajectory starting from the state *s*[t]. Thus, now we can rewrite our
    gradient with reward-to-go instead of the return of the trajectory as:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方程表示奖励到达 *R*[t] 是从状态 *s*[t] 开始的轨迹的奖励总和。因此，现在我们可以将梯度用奖励到达替代轨迹的回报重新写为：
- en: '![](img/B15558_10_127.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_127.png)'
- en: 'We can simply express the preceding equation as:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以简单地将前述方程表示为：
- en: '![](img/B15558_10_128.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_128.png)'
- en: Where the reward-to-go is ![](img/B15558_10_126.png).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励到达（reward-to-go）是 ![](img/B15558_10_126.png)。
- en: 'After computing the gradient, we update the parameter as:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 计算梯度后，我们可以按以下方式更新参数：
- en: '![](img/B15558_10_027.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_027.png)'
- en: Now that we have understood what policy gradient with reward-to-go is, in the
    next section, we will look into the algorithm for more clarity.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了带有奖励到达的策略梯度，接下来的部分将探讨该算法以更清楚地说明。
- en: Algorithm – Reward-to-go policy gradient
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法 – 奖励到达策略梯度
- en: 'The algorithm of policy gradient with reward-to-go is similar to the REINFORCE
    method, except now we compute the reward-to-go (return of the trajectory starting
    from a state *s*[t]) instead of using the full return of the trajectory, as shown
    here:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 带有回报后续（reward-to-go）的策略梯度算法与 REINFORCE 方法类似，不同的是我们现在计算回报后续（从状态 *s*[t] 开始的轨迹的回报），而不是使用轨迹的完整回报，如下所示：
- en: Initialize the network parameter ![](img/B15558_09_056.png) with random values
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用随机值初始化网络参数 ![](img/B15558_09_056.png)
- en: Generate *N* number of trajectories ![](img/B15558_10_058.png) following the
    policy ![](img/B15558_10_057.png)
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据策略 ![](img/B15558_10_057.png) 生成 *N* 条轨迹 ![](img/B15558_10_058.png)：
- en: Compute the return (reward-to-go) *R*[t]
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报（回报后续） *R*[t]
- en: Compute the gradients:![](img/B15558_10_128.png)
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度：![](img/B15558_10_128.png)
- en: Update the network parameter as ![](img/B15558_10_135.png)
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新网络参数为 ![](img/B15558_10_135.png)
- en: Repeat *steps 2* to *5* for several iterations
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2* 到 *步骤 5* 多次迭代
- en: From the preceding algorithm, we can observe that we are using reward-to-go
    instead of the return of the trajectory. To get a clear understanding of how the
    reward-to-go policy gradient works, let's implement it in the next section.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的算法中，我们可以观察到我们使用了回报后续（reward-to-go），而不是轨迹的回报。为了清楚理解回报后续策略梯度的工作原理，让我们在下一节中实现它。
- en: Cart pole balancing with policy gradient
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用策略梯度进行倒立摆平衡
- en: Now, let's learn how to implement the policy gradient algorithm with reward-to-go
    for the cart pole balancing task.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何实现具有回报后续（reward-to-go）的策略梯度算法来进行倒立摆平衡任务。
- en: For a clear understanding of how the policy gradient method works, we use TensorFlow
    in the non-eager mode by disabling TensorFlow 2 behavior.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清楚地理解策略梯度方法如何工作，我们通过禁用 TensorFlow 2 的行为，使用非急切模式来实现 TensorFlow。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入所需的库：
- en: '[PRE0]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Create the cart pole environment using gym:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 gym 创建倒立摆环境：
- en: '[PRE1]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Get the state shape:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 获取状态的形状：
- en: '[PRE2]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Get the number of actions:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 获取动作的数量：
- en: '[PRE3]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Computing discounted and normalized reward
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算折扣和归一化奖励
- en: Instead of using the rewards directly, we can use the discounted and normalized
    rewards.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用折扣和归一化奖励，而不是直接使用奖励。
- en: 'Set the discount factor, ![](img/B15558_03_190.png):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 设置折扣因子，![](img/B15558_03_190.png)：
- en: '[PRE4]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s define a function called `discount_and_normalize_rewards` for computing
    the discounted and normalized rewards:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个名为 `discount_and_normalize_rewards` 的函数，用于计算折扣和归一化的奖励：
- en: '[PRE5]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Initialize an array for storing the discounted rewards:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个数组来存储折扣奖励：
- en: '[PRE6]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Compute the discounted reward:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 计算折扣奖励：
- en: '[PRE7]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Normalize and return the reward:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化并返回奖励：
- en: '[PRE8]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Building the policy network
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建策略网络
- en: 'First, let''s define the placeholder for the state:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义状态的占位符：
- en: '[PRE9]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the placeholder for the action:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 定义动作的占位符：
- en: '[PRE10]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define the placeholder for the discounted reward:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 定义折扣奖励的占位符：
- en: '[PRE11]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define layer 1:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第 1 层：
- en: '[PRE12]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define layer 2\. Note that the number of units in layer 2 is set to the number
    of actions:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 定义第 2 层。请注意，第 2 层的单元数量设置为动作的数量：
- en: '[PRE13]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Obtain the probability distribution over the action space as an output of the
    network by applying the softmax function to the result of layer 2:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对第 2 层的结果应用 softmax 函数，获得动作空间上的概率分布作为网络的输出：
- en: '[PRE14]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We learned that we compute gradient as:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到梯度的计算方式为：
- en: '![](img/B15558_10_128.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_128.png)'
- en: 'After computing the gradient, we update the parameter of the network using
    gradient ascent:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算出梯度后，我们使用梯度上升法来更新网络的参数：
- en: '![](img/B15558_10_027.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_027.png)'
- en: 'However, it is a standard convention to perform minimization rather than maximization.
    So, we can convert the preceding maximization objective into the minimization
    objective by just adding a negative sign. We can implement this using `tf.nn.softmax_cross_entropy_with_logits_v2`.
    Thus, we can define the negative log policy as:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常的约定是进行最小化而不是最大化。因此，我们可以通过仅添加一个负号将前面的最大化目标转化为最小化目标。我们可以使用 `tf.nn.softmax_cross_entropy_with_logits_v2`
    来实现这一点。因此，我们可以定义负对数策略为：
- en: '[PRE15]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let''s define the loss:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义损失函数：
- en: '[PRE16]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Define the train operation for minimizing the loss using the Adam optimizer:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 定义用于最小化损失的训练操作，并使用 Adam 优化器：
- en: '[PRE17]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Training the network
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练网络
- en: Now, let's train the network for several iterations. For simplicity, let's just
    generate one episode in every iteration.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练网络进行多次迭代。为了简化起见，我们在每次迭代中只生成一个回合。
- en: 'Set the number of iterations:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 设置迭代次数：
- en: '[PRE18]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Start the TensorFlow session:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 TensorFlow 会话：
- en: '[PRE19]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Initialize all the TensorFlow variables:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化所有TensorFlow变量：
- en: '[PRE20]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For every iteration:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 对每次迭代：
- en: '[PRE21]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Initialize an empty list for storing the states, actions, and rewards obtained
    in the episode:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个空列表，用于存储回合中获得的状态、动作和奖励：
- en: '[PRE22]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Set `done` to `False`:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 将`done`设置为`False`：
- en: '[PRE23]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Initialize the `Return`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`Return`：
- en: '[PRE24]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重置环境初始化状态：
- en: '[PRE25]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'While the episode is not over:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 当回合尚未结束时：
- en: '[PRE26]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Reshape the state:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 改变状态形状：
- en: '[PRE27]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Feed the state to the policy network and the network returns the probability
    distribution over the action space as output, which becomes our stochastic policy
    ![](img/B15558_10_139.png):'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态输入到策略网络中，网络返回动作空间的概率分布作为输出，这就是我们的随机策略![](img/B15558_10_139.png)：
- en: '[PRE28]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we select an action using this stochastic policy:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用这个随机策略选择一个动作：
- en: '[PRE29]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Perform the selected action:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 执行所选动作：
- en: '[PRE30]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Render the environment:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染环境：
- en: '[PRE31]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Update the return:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 更新回报：
- en: '[PRE32]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'One-hot encode the action:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对动作进行独热编码：
- en: '[PRE33]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Store the state, action, and reward in their respective lists:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态、动作和奖励存储到各自的列表中：
- en: '[PRE34]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Update the state to the next state:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 将状态更新为下一个状态：
- en: '[PRE35]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Compute the discounted and normalized reward:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 计算折扣化和归一化后的奖励：
- en: '[PRE36]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Define the feed dictionary:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 定义输入字典：
- en: '[PRE37]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Train the network:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 训练网络：
- en: '[PRE38]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Print the return for every 10 iterations:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 每10次迭代打印一次回报：
- en: '[PRE39]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now that we have learned how to implement the policy gradient algorithm with
    reward-to-go, in the next section, we will learn another interesting variance
    reduction technique called policy gradient with baseline.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何实现带有奖励的策略梯度算法，在下一节中，我们将学习另一种有趣的方差减少技术——带基准的策略梯度。
- en: Policy gradient with baseline
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带基准的策略梯度
- en: 'We have learned that we find the optimal policy by using a neural network and
    we update the parameter of our network using gradient ascent:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何通过使用神经网络找到最优策略，并且通过梯度上升更新网络的参数：
- en: '![](img/B15558_10_027.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_027.png)'
- en: 'Where the value of the gradient is:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，梯度的值为：
- en: '![](img/B15558_10_141.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_141.png)'
- en: 'Now, to reduce variance, we introduce a new function called a baseline function.
    Subtracting the baseline *b* from the return (reward-to-go) *R*[t] reduces the
    variance, so we can rewrite the gradient as:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了减少方差，我们引入一个新的函数，称为基准函数。将基准*b*从回报（未来奖励）*R*[t]中减去可以减少方差，因此我们可以将梯度重写为：
- en: '![](img/B15558_10_142.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_142.png)'
- en: Wait. What is the baseline function? And how does subtracting it from *R*[t]
    reduce the variance? The purpose of the baseline is to reduce the variance in
    the return. Thus, if the baseline *b* is a value that can give us the expected
    return from the state the agent is in, then subtracting *b* in every step will
    reduce the variance in the return.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，什么是基准函数？减去基准函数后，*R*[t]如何减少方差？基准的目的是减少回报的方差。因此，如果基准*b*是一个可以为我们提供从智能体所在状态开始的预期回报的值，那么在每一步减去*b*就会减少回报的方差。
- en: 'There are several choices for the baseline functions. We can choose any function
    as a baseline function but the baseline function should not depend on our network
    parameter. A simple baseline could be the average return of the sampled trajectories:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 基准函数有几种选择。我们可以选择任何函数作为基准函数，但基准函数不应依赖于我们的网络参数。一个简单的基准可能是采样轨迹的平均回报：
- en: '![](img/B15558_10_143.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_143.png)'
- en: Thus, subtracting current return *R*[t] and the average return helps us to reduce
    variance. As we can see, our baseline function doesn't depend on the network parameter
    ![](img/B15558_09_106.png). So, we can use any function as a baseline function
    and it should not affect our network parameter ![](img/B15558_09_106.png).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，减去当前回报*R*[t]和平均回报有助于减少方差。如我们所见，我们的基准函数不依赖于网络参数![](img/B15558_09_106.png)。所以，我们可以使用任何函数作为基准函数，它不应影响我们的网络参数![](img/B15558_09_106.png)。
- en: 'One of the most popular functions of the baseline is the value function. We
    learned that the value function or the value of a state is the expected return
    an agent would obtain starting from that state following the policy ![](img/B15558_10_146.png).
    Thus, subtracting the value of a state (the expected return) and the current return
    *R*[t] can reduce the variance. So, we can rewrite our gradient as:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 基准的最常见功能之一是值函数。我们了解到，值函数或状态的值是从该状态开始，按照策略![](img/B15558_10_146.png)执行的智能体预期获得的回报。因此，减去状态的值（即预期回报）和当前回报*R*[t]可以减少方差。所以，我们可以将梯度重写为：
- en: '![](img/B15558_10_147.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_147.png)'
- en: Other than the value function, we can also use different baseline functions
    such as the Q function, the advantage function, and more. We will learn more about
    them in the next chapter.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 除了值函数，我们还可以使用不同的基准函数，例如Q函数、优势函数等。我们将在下一章中详细了解它们。
- en: But now the question is how can we learn the baseline function? Say we are using
    the value function as the baseline function. How can we learn the optimal value
    function? Just like we are approximating the policy, we can also approximate the
    value function using another neural network parameterized by ![](img/B15558_10_148.png).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在的问题是，我们如何学习基准函数？假设我们使用值函数作为基准函数。我们如何学习最优的值函数？就像我们在近似策略一样，我们也可以使用另一个由！[](img/B15558_10_148.png)参数化的神经网络来近似值函数。
- en: That is, we use another network for approximating the value function (the value
    of a state) and we can call this network a value network. Okay, how can we train
    this value network?
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们使用另一个网络来近似值函数（一个状态的值），我们可以称这个网络为值网络。那么，我们如何训练这个值网络呢？
- en: 'Since the value of the state is a continuous value, we can train the network
    by minimizing the **mean squared error** (**MSE**). The MSE can be defined as
    the mean squared difference between the actual return *R*[t] and the predicted
    return ![](img/B15558_10_170.png). Thus, the objective function of the value network
    can be defined as:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 由于状态的值是一个连续值，我们可以通过最小化**均方误差**（**MSE**）来训练网络。MSE可以定义为实际回报*R*[t]与预测回报！[](img/B15558_10_170.png)之间的均方差。因此，值网络的目标函数可以定义为：
- en: '![](img/B15558_10_149.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_149.png)'
- en: 'We can minimize the error using the gradient descent and update the network
    parameter as:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用梯度下降最小化误差，并更新网络参数如下：
- en: '![](img/B15558_10_150.png)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_150.png)'
- en: Thus, in the policy gradient with the baseline method, we minimize the variance
    in the gradient updates by using the baseline function. A baseline function can
    be any function and it should not depend on the network parameter ![](img/B15558_09_118.png).
    We use the value function as a baseline function, then to approximate the value
    function we use a different neural network parameterized by ![](img/B15558_10_152.png),
    and we find the optimal value function by minimizing the MSE.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在带基准的策略梯度方法中，我们通过使用基准函数来最小化梯度更新中的方差。基准函数可以是任何函数，并且不应依赖于网络参数！[](img/B15558_09_118.png)。我们使用值函数作为基准函数，然后为了近似值函数，我们使用另一个由！[](img/B15558_10_152.png)参数化的神经网络，并通过最小化均方误差（MSE）来找到最优的值函数。
- en: 'In a nutshell, in the policy gradient with the baseline function, we use two
    neural networks:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在带基准函数的策略梯度中，我们使用两个神经网络：
- en: '**Policy network parameterized by ![](img/B15558_10_153.png)**: This finds
    the optimal policy by performing gradient ascent:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '**由！[](img/B15558_10_153.png)参数化的策略网络**：通过执行梯度上升来寻找最优策略：'
- en: '![](img/B15558_10_154.png)![](img/B15558_10_027.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_154.png)![](img/B15558_10_027.png)'
- en: '**Value network parameterized by ![](img/B15558_10_156.png)**: This is used
    to correct the variance in the gradient update by acting as a baseline, and it
    finds the optimal value of a state by performing gradient descent:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '**由！[](img/B15558_10_156.png)参数化的值网络**：通过作为基准来修正梯度更新中的方差，并通过执行梯度下降来找到状态的最优值：'
- en: '![](img/B15558_10_149.png)![](img/B15558_10_150.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_10_149.png)![](img/B15558_10_150.png)'
- en: Note that the policy gradient with the baseline function is often referred to
    as the **REINFORCE** **with baseline** method.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，带基准函数的策略梯度通常被称为**带基准的REINFORCE**方法。
- en: Now that we have seen how the policy gradient method with baseline works by
    using a policy and a value network, in the next section we will look into the
    algorithm to get more clarity.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了带基准的策略梯度方法是如何通过使用策略和值网络工作的，在接下来的部分，我们将研究该算法以获得更清晰的理解。
- en: Algorithm – REINFORCE with baseline
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法 – 带基准的REINFORCE
- en: 'The algorithm of the policy gradient method with the baseline function (REINFORCE
    with baseline) is shown here:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 带基准函数的策略梯度方法（REINFORCE带基准）的算法如下所示：
- en: Initialize the policy network parameter ![](img/B15558_10_159.png) and value
    network parameter ![](img/B15558_10_148.png)
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化策略网络参数！[](img/B15558_10_159.png)和值网络参数！[](img/B15558_10_148.png)
- en: Generate *N* number of trajectories ![](img/B15558_10_058.png) following the
    policy ![](img/B15558_10_044.png)
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成*N*个轨迹！[](img/B15558_10_058.png)，按照策略！[](img/B15558_10_044.png)执行。
- en: Compute the return (reward-to-go) *R*[t]
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算回报（未来奖励）*R*[t]
- en: Compute the policy gradient:![](img/B15558_10_163.png)
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算策略梯度：！[](img/B15558_10_163.png)
- en: Update the policy network parameter ![](img/B15558_09_123.png) using gradient
    ascent as ![](img/B15558_10_027.png)
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度上升法更新策略网络参数 ![](img/B15558_09_123.png) ，如 ![](img/B15558_10_027.png)
- en: Compute the MSE of the value network:![](img/B15558_10_166.png)
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算值网络的均方误差（MSE）：![](img/B15558_10_166.png)
- en: Compute gradients ![](img/B15558_10_093.png) and update the value network parameter
    ![](img/B15558_10_148.png) using gradient descent as ![](img/B15558_10_150.png)
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度 ![](img/B15558_10_093.png) 并使用梯度下降更新值网络参数 ![](img/B15558_10_148.png) ，如
    ![](img/B15558_10_150.png)
- en: Repeat *steps 2* to *7* for several iterations
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 2* 到 *步骤 7* 若干次
- en: Summary
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started off the chapter by learning that with value-based methods, we extract
    the optimal policy from the optimal Q function (Q values). Then we learned that
    it is difficult to compute the Q function when our action space is continuous.
    We can discretize the action space; however, discretization is not always desirable,
    and it leads to the loss of several important features and an action space with
    a huge set of actions.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过学习基于值的方法开始本章，在基于值的方法中，我们从最优 Q 函数（Q 值）中提取最优策略。然后我们了解到，当我们的动作空间是连续时，计算 Q 函数是困难的。我们可以对动作空间进行离散化；然而，离散化并不总是可取的，它会导致丧失若干重要特征，并生成一个包含大量动作的巨大动作空间。
- en: So, we resorted to the policy-based method. In the policy-based method, we compute
    the optimal policy without the Q function. We learned about one of the most popular
    policy-based methods called the policy gradient, in which we find the optimal
    policy directly by parameterizing the policy using some parameter ![](img/B15558_09_008.png).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们采用了基于策略的方法。在基于策略的方法中，我们在没有 Q 函数的情况下计算最优策略。我们了解了一种最受欢迎的基于策略的方法，称为策略梯度，其中我们通过使用某些参数来对策略进行参数化，从而直接找到最优策略！[](img/B15558_09_008.png)。
- en: We also learned that in the policy gradient method, we select actions based
    on the action probability distribution given by the network, and if we win the
    episode, that is, if we get a high return, then we assign high probabilities to
    all the actions in the episode, else we assign low probabilities to all the actions
    in the episode. Later, we learned how to derive the policy gradient step by step,
    and then we looked into the algorithm of policy gradient method in more detail.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还了解到，在策略梯度方法中，我们根据网络给出的动作概率分布来选择动作，如果我们赢得了这一回合，也就是获得了高回报，那么我们会为回合中的所有动作分配高概率，否则我们会为回合中的所有动作分配低概率。之后，我们学习了如何一步步推导策略梯度，并进一步详细研究了策略梯度方法的算法。
- en: Moving forward, we learned about the variance reduction methods such as reward-to-go
    and the policy gradient method with the baseline function. In the policy gradient
    method with the baseline function, we use two networks called the policy and value
    network. The role of the policy network is to find the optimal policy, and the
    role of the value network is to correct the gradient updates in the policy network
    by estimating the value function.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们了解了方差减少方法，如奖励到达（reward-to-go）和带基准函数的策略梯度方法。在带基准函数的策略梯度方法中，我们使用两个网络，分别是策略网络和价值网络。策略网络的作用是找到最优策略，价值网络的作用是通过估计值函数来修正策略网络中的梯度更新。
- en: In the next chapter, we will learn about another interesting set of algorithms
    called the actor-critic methods.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习一组有趣的算法，称为演员-评论家方法（actor-critic methods）。
- en: Questions
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s evaluate our understanding of the policy gradient method by answering
    the following questions:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回答以下问题来评估我们对策略梯度方法的理解：
- en: What is a value-based method?
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是基于值的方法？
- en: Why do we need a policy-based method?
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们需要基于策略的方法？
- en: How does the policy gradient method work?
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 策略梯度方法是如何工作的？
- en: How do we compute the gradient in the policy gradient method?
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何计算策略梯度方法中的梯度？
- en: What is a reward-to-go?
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是奖励到达（reward-to-go）？
- en: What is the policy gradient with the baseline function?
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是带基准函数的策略梯度？
- en: Define the baseline function.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义基准函数。
- en: Further reading
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information about the policy gradient, we can refer to the following
    paper:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 关于策略梯度的更多信息，我们可以参考以下论文：
- en: '**Policy Gradient Methods for Reinforcement Learning with Function Approximation**
    by *Richard S. Sutton et al*., [https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-a)'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有函数逼近的强化学习策略梯度方法**，作者 *理查德·S·萨顿 等*，[https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-a)'
