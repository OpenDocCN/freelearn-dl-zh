- en: '*Chapter 10*: Implementing DL Explainability with MLflow'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 10 章*：使用 MLflow 实现深度学习可解释性'
- en: The importance of **deep learning** (**DL**) explainability is now well established,
    as we learned in the previous chapter. In order to implement DL explainability
    in a real-world project, it is desirable to log the explainer and the explanations
    as artifacts, just like other model artifacts in the MLflow server, so that we
    can easily track and reproduce the explanation. The integration of DL explainability
    tools such as SHAP ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))
    with MLflow can support different implementation mechanisms, and it is important
    to understand how these integrations can be used for our DL explainability scenarios.
    In this chapter, we will explore several ways to integrate the SHAP explanations
    into MLflow by using different MLflow capabilities. As tools for explainability
    and DL models are both rapidly evolving, we will also highlight the current limitations
    and workarounds when using MLflow for DL explainability implementation. By the
    end of this chapter, you will feel comfortable implementing SHAP explanations
    and explainers using MLflow APIs for scalable model explainability.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**（**DL**）可解释性的重要性在前一章中已有充分讨论。为了在实际项目中实现深度学习可解释性，最好像其他模型工件一样，将解释器和解释作为工件记录在
    MLflow 服务器中，这样我们就可以轻松跟踪和重现解释。将 SHAP（[https://github.com/slundberg/shap](https://github.com/slundberg/shap)）等深度学习可解释性工具与
    MLflow 集成，可以支持不同的实现机制，理解这些集成如何应用于我们的深度学习可解释性场景是非常重要的。本章将探讨通过使用不同的 MLflow 功能将 SHAP
    解释集成到 MLflow 中的几种方法。由于可解释性工具和深度学习模型都在快速发展，我们还将重点介绍使用 MLflow 实现深度学习可解释性时的当前限制和解决方法。到本章结束时，你将能够使用
    MLflow API 实现 SHAP 解释和解释器，从而实现可扩展的模型可解释性。'
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Understanding current MLflow explainability integration
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解当前的 MLflow 可解释性集成
- en: Implementing SHAP explanations using the MLflow artifact logging API
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MLflow 工件日志记录 API 实现 SHAP 解释
- en: Implementing SHAP explainers using the MLflow pyfunc API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MLflow pyfunc API 实现 SHAP 解释器
- en: Technical requirements
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following requirements are necessary to complete this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章所需的以下要求：
- en: 'MLflow full-fledged local server: This is the same one we have been using since
    [*Chapter 3*](B18120_03_ePub.xhtml#_idTextAnchor040), *Tracking Models, Parameters,
    and Metrics*.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完整的 MLflow 本地服务器：这与我们自[*第 3 章*](B18120_03_ePub.xhtml#_idTextAnchor040)以来一直使用的服务器相同，*跟踪模型、参数和指标*。
- en: 'The SHAP Python library: [https://github.com/slundberg/shap](https://github.com/slundberg/shap).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHAP Python 库：[https://github.com/slundberg/shap](https://github.com/slundberg/shap)。
- en: 'Spark 3.2.1 and PySpark 3.2.1: See the details in the `README.md` file of this
    chapter''s GitHub repository.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 3.2.1 和 PySpark 3.2.1：请参阅本章 GitHub 仓库中的 `README.md` 文件了解详细信息。
- en: 'Code from the GitHub repository for this chapter: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter10](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter10).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章 GitHub 仓库中的代码：[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter10](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter10)。
- en: Understanding current MLflow explainability integration
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解当前的 MLflow 可解释性集成
- en: 'MLflow has several ways to support explainability integration. When implementing
    explainability, we refer to two types of artifacts: explainers and explanations:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow 支持多种可解释性集成方式。在实现可解释性时，我们提到两种类型的工件：解释器和解释：
- en: An explainer is an explainability model, and a common one is a SHAP model that
    could be different kinds of SHAP explainers, such as **TreeExplainer**, **KernelExplainer**,
    and **PartitionExplainer** ([https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html](https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html)).
    For computational efficiency, we usually choose **PartitionExplainer** for DL
    models.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释器是一个可解释性模型，常见的模型是 SHAP 模型，其中可能有多种 SHAP 解释器，如 **TreeExplainer**、**KernelExplainer**
    和 **PartitionExplainer**（[https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html](https://shap.readthedocs.io/en/latest/generated/shap.explainers.Partition.html)）。为了提高计算效率，我们通常选择
    **PartitionExplainer** 来处理深度学习模型。
- en: An explanation is an artifact that shows some form of output from the explainer,
    which could be text, numerical values, or plots. Explanations can happen in offline
    training or testing, or can happen during online production. Thus, we should be
    able to provide an explainer for offline evaluation or an explainer endpoint for
    online queries if we want to know why the model provides certain predictions.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释是一个产物，它展示了某种形式的输出，可能是文本、数值或图表。解释可以发生在离线训练或测试中，或者发生在在线生产中。因此，如果我们想知道模型为何给出某些预测，我们应该能够提供一个离线评估的解释器或一个在线查询的解释器端点。
- en: 'Here, we give a brief overview of the current capability as of MLflow version
    1.25.1 ([https://pypi.org/project/mlflow/1.25.1/](https://pypi.org/project/mlflow/1.25.1/)).
    There are four different ways to use MLflow for explainability as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们简要概述了截至 MLflow 版本 1.25.1（[https://pypi.org/project/mlflow/1.25.1/](https://pypi.org/project/mlflow/1.25.1/)）的当前功能。使用
    MLflow 进行可解释性的方式有四种，如下所示：
- en: Use the `mlflow.log_artifact` API ([https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact))
    to log relevant explanation artifacts such as bar plots and Shapley values arrays.
    This gives maximum flexibility for logging explanations. This can be used either
    offline as batch processing or online when we automatically log a SHAP bar plot
    for a certain prediction. Note that logging an explanation for each prediction
    during online production scenarios is expensive, so we should provide a separate
    explanation API for on-demand queries.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `mlflow.log_artifact` API（[https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_artifact)）记录相关的解释产物，如条形图和
    Shapley 值数组。这为记录解释提供了最大的灵活性。无论是离线批处理处理，还是在线自动记录某一预测的 SHAP 条形图，都可以使用该功能。需要注意的是，在在线生产场景中为每个预测记录解释是非常昂贵的，因此我们应为按需查询提供一个单独的解释
    API。
- en: Use the `mlflow.pyfunc.PythonModel` API ([https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel))
    to create an explainer that can be logged and loaded with MLflow's `pyfunc` methods,
    `mlflow.pyfunc.log_model` for logging and `mlflow.pyfunc.load_model` or `mlflow.pyfunc.spark_udf`
    for loading an explainer. This gives us maximum flexibility to create customized
    explainers as MLflow generic `pyfunc` models and can be used for either offline
    batch explanation or online as an **Explanation as a Service** (**EaaS**).
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `mlflow.pyfunc.PythonModel` API（[https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel](https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel)）来创建一个解释器，该解释器可以通过
    MLflow 的 `pyfunc` 方法进行记录和加载，`mlflow.pyfunc.log_model` 用于记录，`mlflow.pyfunc.load_model`
    或 `mlflow.pyfunc.spark_udf` 用于加载解释器。这为我们提供了最大灵活性，可以将自定义解释器作为 MLflow 通用的 `pyfunc`
    模型创建，并且可以用于离线批处理解释或在线提供**解释即服务**（**EaaS**）。
- en: Use the `mlflow.shap` API ([https://www.mlflow.org/docs/latest/python_api/mlflow.shap.html](https://www.mlflow.org/docs/latest/python_api/mlflow.shap.html)).
    This has some limitations. For example, the `mlflow.shap.log_explainer` method
    only supports scikit-learn and PyTorch models. The `mlflow.shap.log_explanation`
    method only supports `shap.KernelExplainer` ([https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html)).
    This is very computationally intensive, as the computing time grows exponentially
    with respect to the number of features; thus, it is not feasible to compute explanations
    for even a moderate size dataset (see a posted GitHub issue [https://github.com/mlflow/mlflow/issues/4071](https://github.com/mlflow/mlflow/issues/4071)).
    The existing examples provided by MLflow are for classical ML models in scikit-learn
    packages such as linear regression or random forest, with no DL model explainability
    examples ([https://github.com/mlflow/mlflow/tree/master/examples/shap](https://github.com/mlflow/mlflow/tree/master/examples/shap)).
    We will show in later sections of this chapter that this API currently does not
    support the transformers-based SHAP explainers and explanations, thus we will
    not use this API in this chapter. We will highlight some of the issues as we walk
    through our examples in this chapter.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`mlflow.shap` API（[https://www.mlflow.org/docs/latest/python_api/mlflow.shap.html](https://www.mlflow.org/docs/latest/python_api/mlflow.shap.html)）。该API存在一些限制。例如，`mlflow.shap.log_explainer`方法仅支持scikit-learn和PyTorch模型。`mlflow.shap.log_explanation`方法仅支持`shap.KernelExplainer`（[https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html)）。这非常消耗计算资源，因为计算时间会随着特征数量的增加呈指数增长；因此，甚至对于中等规模的数据集，也无法计算解释（请参阅已发布的GitHub问题[https://github.com/mlflow/mlflow/issues/4071](https://github.com/mlflow/mlflow/issues/4071)）。MLflow提供的现有示例仅适用于scikit-learn包中的经典机器学习模型，如线性回归或随机森林，并没有提供深度学习模型的解释示例（[https://github.com/mlflow/mlflow/tree/master/examples/shap](https://github.com/mlflow/mlflow/tree/master/examples/shap)）。我们将在本章后面的章节中展示，当前该API不支持基于transformers的SHAP解释器和解释，因此在本章中我们将不使用该API。我们将在讲解本章示例时突出一些问题。
- en: Use the `mlflow.evaluate` API ([https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate)).
    This can be used for evaluation after the model is already trained and tested.
    This is an experimental feature and might change in the future. It supports MLflow
    `pyfunc` models. However, it has some limitations in that the evaluation dataset
    label values must be numeric or Boolean, all feature values must be numeric, and
    each feature column must only contain scalar values ([https://www.mlflow.org/docs/latest/models.html#model-evaluation](https://www.mlflow.org/docs/latest/models.html#model-evaluation)).
    Again, existing examples provided by MLflow are only for classical ML models in
    scikit-learn packages ([https://github.com/mlflow/mlflow/tree/master/examples/evaluation](https://github.com/mlflow/mlflow/tree/master/examples/evaluation)).
    We could use this API to just log the classifier metrics for an NLP sentiment
    model, but the explanation part will be skipped automatically by this API because
    it requires a feature column containing scalar values (an NLP model input is a
    text input). Thus, this is not applicable to the DL model explainability we need.
    So, we will not use this API in this chapter.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`mlflow.evaluate` API（[https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate](https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate)）。该API可以在模型训练和测试之后用于评估。这是一个实验性功能，未来可能会有所变化。它支持MLflow
    `pyfunc`模型。然而，它有一些限制：评估数据集的标签值必须是数字或布尔值，所有特征值必须是数字，每个特征列必须只包含标量值（[https://www.mlflow.org/docs/latest/models.html#model-evaluation](https://www.mlflow.org/docs/latest/models.html#model-evaluation)）。同样，MLflow提供的现有示例仅适用于scikit-learn包中的经典机器学习模型（[https://github.com/mlflow/mlflow/tree/master/examples/evaluation](https://github.com/mlflow/mlflow/tree/master/examples/evaluation)）。我们可以使用该API仅记录NLP情感模型的分类器指标，但该API会自动跳过解释部分，因为它要求特征列包含标量值（NLP模型的输入是文本输入）。因此，这不适用于我们需要的深度学习模型可解释性。所以，我们在本章中将不使用该API。
- en: Given that some of these APIs are still experimental and are still evolving,
    users should be aware of the limitations and workarounds to successfully implement
    explainability with MLflow. For DL model explainability, as we will learn in this
    chapter, it is quite challenging to implement using MLflow as the MLflow integration
    with SHAP is still a work-in-progress as of MLflow version 1.25.1\. In the following
    sections, we will learn when and how to use these different APIs to implement
    explanations and log and load explainers for DL models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于某些API仍处于实验阶段并且还在不断发展，用户应注意限制和解决方法，以便成功实现MLflow的可解释性。在本章中，我们将学习如何实现深度学习模型的可解释性，使用MLflow来实现这一点相当具有挑战性，因为MLflow与SHAP的集成仍在进行中（截至MLflow版本1.25.1）。在接下来的章节中，我们将学习何时以及如何使用这些不同的API来实现解释，并记录和加载深度学习模型的解释器。
- en: Implementing a SHAP explanation using the MLflow artifact logging API
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MLflow工件记录API实现SHAP解释
- en: 'MLflow has a generic tracking API that can log any artifact: `mlflow.log_artifact`.
    However, the examples given in the MLflow documentation usually use scikit-learn
    and tabular numerical data for training, testing, and explaining. Here, we want
    to show how to use `mlflow.log_artifact` for an NLP sentimental DL model to log
    relevant artifacts, such as Shapley value arrays and Shapley value bar plots.
    You can check out the Python VS Code notebook, `shap_mlflow_log_artifact.py`,
    in this chapter''s GitHub repository ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_log_artifact.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_log_artifact.py))
    to follow along with the steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow有一个通用的跟踪API，可以记录任何工件：`mlflow.log_artifact`。然而，MLflow文档中给出的示例通常使用scikit-learn和表格型数值数据进行训练、测试和解释。在这里，我们希望展示如何使用`mlflow.log_artifact`来记录与NLP情感分析深度学习模型相关的工件，如Shapley值数组和Shapley值条形图。您可以查看本章GitHub仓库中的Python
    VS Code笔记本`shap_mlflow_log_artifact.py`（[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_log_artifact.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_log_artifact.py)），以便按照步骤操作：
- en: Make sure you have the prerequisites, including a local full-fledged MLflow
    server and the conda virtual environment, ready. Follow the instructions in the
    `README.md` ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md))
    file in the [*Chapter 10*](B18120_10_ePub.xhtml#_idTextAnchor127) folder to get
    these ready.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请确保您已准备好相关的前提条件，包括本地完整的MLflow服务器和conda虚拟环境。请按照`README.md`文件中的说明（[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md)）中的步骤，确保这些内容已就绪，文件位于[
    *第10章* ](B18120_10_ePub.xhtml#_idTextAnchor127)文件夹中。
- en: 'Make sure you activate the `chapter10-dl-explain` virtual environment as follows
    before you start running any code in this chapter:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始运行本章任何代码之前，请确保按照如下方式激活`chapter10-dl-explain`虚拟环境：
- en: '[PRE0]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the relevant libraries at the beginning of the notebook as follows:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本开头导入相关库，如下所示：
- en: '[PRE1]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The next step is to set up some environment variables. The first three environment
    variables are for the local MLflow URIs, and the fourth is for disabling a Hugging
    Face warning that arises due to a known Hugging Face tokenization issue:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是设置一些环境变量。前三个环境变量用于本地MLflow URI，第四个用于禁用由于已知Hugging Face标记化问题而产生的Hugging Face警告：
- en: '[PRE2]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will also need to set up the MLflow experiment and show the MLflow experiment
    ID as an output on the screen:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要设置MLflow实验，并将MLflow实验ID作为输出显示在屏幕上：
- en: '[PRE3]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you have been running the notebook, you should see an output like the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经运行了该笔记本，应该能看到类似如下的输出：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This means the MLflow experiment ID for the experiment name `dl_explain_chapter10`
    is `14`. Note that, you could also set the MLflow tracking URI as an environment
    variable as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着实验名称为`dl_explain_chapter10`的MLflow实验ID是`14`。请注意，您也可以按照如下方式将MLflow跟踪URI设置为环境变量：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we use MLflow's `mlflow.set_tracking_uri` API to define the URI location
    instead. Either way is fine.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用MLflow的`mlflow.set_tracking_uri` API来定义URI位置。两种方式都可以。
- en: 'Now we can create a DL model to classify a sentence into either positive or
    negative sentiment using Hugging Face''s transformer pipeline API. Since this
    is already fine-tuned, we will focus on how to get the explainer and explanation
    for the model, rather than focusing on how to train or finetune a model:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以创建一个 DL 模型，使用 Hugging Face 的 transformer pipeline API 将一句话分类为正面或负面情感。由于这个模型已经进行过微调，我们将重点放在如何获取该模型的解释器和解释内容，而不是如何训练或微调模型：
- en: '[PRE6]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The code snippets create a sentiment analysis model, `dl_model`, and then create
    a SHAP `explainer` for this model. Then we provide a list of two sentences for
    this explainer to get the `shap_values` object. This will be used for logging
    in MLflow.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码片段创建了一个情感分析模型`dl_model`，然后为该模型创建一个 SHAP `explainer`。接着，我们为这个解释器提供了一个包含两句话的列表，以获取
    `shap_values` 对象。该对象将用于在 MLflow 中记录。
- en: 'Given the `shap_values` object, we can now start a new MLflow run and log both
    the Shapley values and the bar plot that we saw in the previous chapter ([*Chapter
    9*](B18120_09_ePub.xhtml#_idTextAnchor112)*, Fundamentals of Deep Learning Explainability*).
    The first line of code makes sure all active MLflow runs are ended. This is useful
    if we want to rerun this block of code multiple times interactively:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 `shap_values` 对象，我们现在可以开始一个新的 MLflow 运行，并记录 Shapley 值和我们在上一章中看到的条形图（[*第9章*](B18120_09_ePub.xhtml#_idTextAnchor112)*，深度学习可解释性基础*）。第一行代码确保所有活跃的
    MLflow 运行都已结束。如果我们想多次交互性地重新运行这段代码，这非常有用：
- en: '[PRE7]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then we define two constants. One, `artifact_root_path`, is for the root path
    in the MLflow artifact store, which will be used to store all the SHAP explanation
    objects. The other, `shap_bar_plot`, is for the artifact filename, which will
    be used for the bar plot figure:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义两个常量。一个是`artifact_root_path`，用于 MLflow 工件存储中的根路径，将用于存储所有 SHAP 解释对象。另一个是`shap_bar_plot`，用于工件文件名，将用于条形图图形：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then start a new MLflow run, under which we will generate and log three
    SHAP files into the MLflow artifact store under the path `model_explanations_shap`:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们开始一个新的 MLflow 运行，在该运行中，我们将生成并记录三个 SHAP 文件到 MLflow 工件存储中，路径为 `model_explanations_shap`：
- en: '[PRE9]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We also need to have a temporary local directory, as shown in the preceding
    code snippet to first save the SHAP files, and then log those files to the MLflow
    server. If you have run the notebook up to this point, you should see a temporary
    directory in the output like the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个临时的本地目录，如前面的代码片段所示，用来先保存 SHAP 文件，然后再将这些文件记录到 MLflow 服务器。如果你已经运行到此步骤，你应该在输出中看到一个类似以下的临时目录：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we are ready to generate the SHAP files and save them. The first one is
    the bar plot, which is a little bit tricky to save and log. Let''s walk through
    the following code to understand how we do this:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们准备生成并保存 SHAP 文件。第一个是条形图，这个稍微有点复杂，保存和记录起来有点难度。让我们一起看看以下代码，了解我们是如何做到的：
- en: '[PRE11]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note that we are using `matplotlib.pyplot`, which was imported as `plt` to first
    clear the figure using `plt.clf()` and then create a subplot with some adjustments.
    Here, we define `bottom=0.2`, which means the position of the bottom edge of the
    subplots is at 20% of the figure height. Similarly, we adjust the left edge of
    the subplot. Then we use the `shap.plots.bar` SHAP API to plot the bar plot for
    the first sentence's feature contribution to the prediction, but with the `show`
    parameter to be `False`. This means, we will not see the plot in the interactive
    run, but the figure is stored in the pyplot `plt` variable, which can then be
    saved using `plt.savefig` to a local temporary directory with the filename prefix
    `shap_bar_plot`. `pyplot` will automatically add the file extension `.png` to
    the file once it is saved. So, this will save a local image file called `shap_bar_plot.png`
    in the temporary folder. The last statement calls MLflow's `mlflow.log_artifact`
    to upload this PNG file to the MLflow tracking server's artifact store in the
    root folder, `model_explanations_shap`. We also need to make sure that we close
    the current figure by calling `plt.close(plt.gcf())`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了 `matplotlib.pyplot`，并将其导入为 `plt`，首先通过 `plt.clf()` 清除图形，然后创建一个具有一些调整的子图。在这里，我们定义了
    `bottom=0.2`，意味着子图底部边缘的位置位于图形高度的 20%。同样，我们调整了子图的左边缘。然后，我们使用 `shap.plots.bar` SHAP
    API 绘制第一句话的特征贡献的条形图，但将 `show` 参数设置为 `False`。这意味着我们在交互式运行中看不到该图，但图形会存储在 pyplot
    `plt` 变量中，然后可以使用 `plt.savefig` 将其保存到本地临时目录，文件名前缀为 `shap_bar_plot`。`pyplot` 会在文件保存后自动添加文件扩展名
    `.png`。因此，这会将名为 `shap_bar_plot.png` 的本地图像文件保存到临时文件夹中。最后的语句调用了 MLflow 的 `mlflow.log_artifact`，将此
    PNG 文件上传到 MLflow 跟踪服务器的工件存储的根文件夹 `model_explanations_shap`。我们还需要确保通过调用 `plt.close(plt.gcf())`
    来关闭当前图形。
- en: 'In addition to logging the `shap_bar_plot.png` to the MLflow server, we also
    want to log the Shapley `base_values` array and `shap_values` array as NumPy arrays
    into the MLflow track server. This can be done through the following statements:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了将 `shap_bar_plot.png` 日志记录到 MLflow 服务器外，我们还希望将 Shapley `base_values` 数组和 `shap_values`
    数组作为 NumPy 数组记录到 MLflow 跟踪服务器中。这可以通过以下语句实现：
- en: '[PRE12]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This will first save a local copy of `shap_values.npy` and `base_values.npy`
    in the local temporary folder and then upload it to the MLflow tracking server's
    artifact store.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这将首先在本地临时文件夹中保存 `shap_values.npy` 和 `base_values.npy` 的本地副本，然后将其上传到 MLflow 跟踪服务器的工件存储中。
- en: 'If you followed the notebook up until here, you should be able to verify in
    the local MLflow server whether these artifacts are successfully stored. Go to
    the MLflow UI at the localhost – `http://localhost/` and then find the experiment
    `dl_explain_chapter10`. You should then be able to find the experiment you just
    ran. It should look something like *Figure 10.1*, where you can find three files
    in the `model_explanations_shap` folder: `base_values.npy`, `shap_bar_plot.png`,
    and `shap_values.npy`. *Figure 10.1* shows the bar plot of feature contribution
    of different tokens or words for the prediction result of the sentence – `Not
    a good movie to spend time on`. The URL for this experiment page is something
    like the following:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你按照笔记本的步骤操作到这里，你应该能够在本地 MLflow 服务器中验证这些工件是否成功存储。前往本地主机上的 MLflow UI – `http://localhost/`，然后找到实验
    `dl_explain_chapter10`。你应该能够找到刚刚运行的实验。它应该类似于 *图 10.1*，在那里你可以在 `model_explanations_shap`
    文件夹中找到三个文件：`base_values.npy`、`shap_bar_plot.png` 和 `shap_values.npy`。*图 10.1*
    显示了句子 `Not a good movie to spend time on` 的预测结果的不同标记或词汇的特征贡献条形图。此实验页面的 URL 类似于以下内容：
- en: '[PRE13]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Figure 10.1 – MLflow log_artifact API saves the SHAP bar plot as an image'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.1 – MLflow log_artifact API 将 SHAP 条形图作为图像保存'
- en: in the MLflow tracking server
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MLflow 跟踪服务器中
- en: '](img/B18120_10_001.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_10_001.jpg)'
- en: Figure 10.1 – MLflow log_artifact API saves the SHAP bar plot as an image in
    the MLflow tracking server
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – MLflow log_artifact API 将 SHAP 条形图作为图像保存到 MLflow 跟踪服务器
- en: Alternatively, you can also use code to programmatically download these files
    stored in the MLflow tracking server and check them locally. We provide such code
    in the last cell of the notebook.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您也可以使用代码以编程方式下载存储在 MLflow 跟踪服务器中的这些文件并在本地查看。我们在笔记本的最后一个单元格中提供了这样的代码。
- en: 'If you run the last cell block of the notebook code, which is to download the
    three files from the MLflow server we just saved and print them out, you should
    be able to see the following output, as displayed in *Figure 10.2*. The mechanism
    to download artifacts from the MLflow tracking server is to use the `MlflowClient().download_artifacts`
    API, where you provide the MLflow run ID (in our example, it is `10f0655189f740aeb813a015f1f6e115`
    ) and the artifact root path `model_explanations_shap` as the parameters to the
    API:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你运行笔记本代码中的最后一个单元格，该单元格用于从我们刚才保存的MLflow服务器下载这三个文件并打印它们，你应该能够看到以下输出，如*图10.2*所示。从MLflow跟踪服务器下载工件的机制是使用`MlflowClient().download_artifacts`
    API，你需要提供MLflow运行ID（在我们的示例中是`10f0655189f740aeb813a015f1f6e115`）和工件根路径`model_explanations_shap`作为API的参数：
- en: '[PRE14]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will download all files in `model_explanations_shap` on the MLflow tracking
    server to a local path, which is the return variable `downloaded_local_path`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把`model_explanations_shap`文件夹中的所有文件从MLflow跟踪服务器下载到本地路径，返回的变量为`downloaded_local_path`：
- en: '![Figure 10.2 – Download the SHAP base_values and shap_values array from the
    MLflow tracking server to a local path and display them'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.2 – 从MLflow跟踪服务器下载SHAP的base_values和shap_values数组到本地路径并显示它们'
- en: '](img/B18120_10_002.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_10_002.jpg)'
- en: Figure 10.2 – Download the SHAP base_values and shap_values array from the MLflow
    tracking server to a local path and display them
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 从MLflow跟踪服务器下载SHAP的base_values和shap_values数组到本地路径并显示它们
- en: 'To display the two NumPy arrays, we need to call NumPy''s `load` API to load
    them and then print them:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要显示这两个NumPy数组，我们需要调用NumPy的`load` API来加载它们，然后打印它们：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that we need to set the `allow_pickle` parameter to `True` when calling
    the `np.load` API so that NumPy can correctly load these files back into memory.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当调用`np.load` API时，我们需要将`allow_pickle`参数设置为`True`，以便NumPy可以正确地将这些文件加载回内存。
- en: 'While you can run this notebook interactively in the VS Code environment, you
    can also run it in the command line as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以在VS Code环境中交互式运行这个笔记本，但你也可以按如下方式在命令行中运行它：
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This will produce all the output in the console and log all the artifacts into
    the MLflow server as we have seen in our interactive running of the notebook.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生所有控制台输出，并将所有工件记录到MLflow服务器中，正如我们在交互式运行笔记本时所看到的。
- en: If you have run the code so far, congratulations on the successful completion
    of implementing logging SHAP explanations to the MLflow tracking server using
    MLflow's `mlflow.log_artifact` API!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经运行了到目前为止的代码，恭喜你成功实现了使用MLflow的`mlflow.log_artifact` API将SHAP解释记录到MLflow跟踪服务器！
- en: Although the process of logging all the explanations seems a little bit long,
    this approach does have the advantage of having no dependency on what kind of
    explainer is used since the explainer is defined outside of the MLflow artifact
    logging API.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管记录所有解释的过程看起来有些冗长，但这种方法确实有一个优点，即不依赖于使用何种解释器，因为解释器是在MLflow工件日志API之外定义的。
- en: In the next section, we will see how to use the built-in `mlflow.pyfunc.PythonModel`
    API to log a SHAP explainer as an MLflow model and then deploy as an endpoint
    or use it in a batch mode as if it is a generic MLflow `pyfunc` model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用内置的`mlflow.pyfunc.PythonModel` API将SHAP解释器记录为MLflow模型，然后将其部署为端点或像通用的MLflow
    `pyfunc`模型一样在批处理模式中使用。
- en: Implementing a SHAP explainer using the MLflow pyfunc API
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MLflow pyfunc API实现SHAP解释器
- en: As we know from the previous section, a SHAP explainer can be used offline whenever
    needed by creating a new instance of an explainer using SHAP APIs. However, as
    the underlying DL models are often logged into the MLflow server, it is desirable
    to also log the corresponding explainer into the MLflow server, so that we not
    only keep track of the DL models, but also their explainers. In addition, we can
    use the generic MLflow pyfunc model logging and loading APIs for the explainer,
    thus unifying access to DL models and their explainers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中了解到的，SHAP解释器可以在需要时离线使用，只需通过SHAP API创建一个新的解释器实例。然而，由于底层的深度学习模型通常会被记录到MLflow服务器中，因此希望将相应的解释器也记录到MLflow服务器中，这样我们不仅可以跟踪深度学习模型，还能跟踪它们的解释器。此外，我们还可以使用通用的MLflow
    pyfunc模型日志和加载API来处理解释器，从而统一访问深度学习模型及其解释器。
- en: 'In this section, we will learn step-by-step how to implement a SHAP explainer
    as a generic MLflow pyfunc model and how to use it for offline and online explanation.
    We will break the process up into three subsections:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将一步一步学习如何将SHAP解释器实现为一个通用的MLflow pyfunc模型，并如何将其用于离线和在线解释。我们将把过程分为三个小节：
- en: Creating and logging an MLflow pyfunc explainer
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建并记录MLflow pyfunc解释器
- en: Deploying an MLflow pyfunc explainer for an EaaS
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署MLflow pyfunc解释器用于EaaS
- en: Using an MLflow pyfunc explainer for batching explanation
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MLflow pyfunc解释器进行批量解释
- en: Let's start with the first subsection on creating and logging a MLflow pyfunc
    explainer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建和记录MLflow pyfunc解释器的第一小节开始。
- en: Creating and logging an MLflow pyfunc explainer
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建并记录MLflow pyfunc解释器
- en: 'In order to follow this section, please check out `nlp_sentiment_classifier_explainer.py`
    in the GitHub repository ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/pipeline/nlp_sentiment_classifier_explainer.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/pipeline/nlp_sentiment_classifier_explainer.py)):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本节内容，请查看GitHub仓库中的`nlp_sentiment_classifier_explainer.py`文件（[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/pipeline/nlp_sentiment_classifier_explainer.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/pipeline/nlp_sentiment_classifier_explainer.py)）：
- en: 'First, by subclassing `mlflow.pyfunc.PythonModel`, we can create a customized
    MLflow model that encapsulates a SHAP explainer. So, let''s declare this class
    as follows:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，通过子类化`mlflow.pyfunc.PythonModel`，我们可以创建一个定制的MLflow模型，该模型封装了一个SHAP解释器。因此，按如下方式声明此类：
- en: '[PRE17]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we need to instantiate an explainer. Instead of creating an explainer
    in the `init` method of this class, we will use the `load_context` method to load
    a SHAP explainer for the Hugging Face NLP sentiment analysis classifier, as follows:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要实例化一个解释器。我们将不在这个类的`init`方法中创建解释器，而是使用`load_context`方法为Hugging Face NLP情感分析分类器加载一个SHAP解释器，如下所示：
- en: '[PRE18]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This will create a SHAP explainer whenever this `SentimentAnalysisExplainer`
    class is executed. Note that the sentiment classifier is a Hugging Face pipeline
    object, with the `return_all_scores` parameter set to `True`. This means that
    this will return the label and probability score for both positive and negative
    sentiment of each input text.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在每次执行`SentimentAnalysisExplainer`类时创建一个SHAP解释器。请注意，情感分类器是一个Hugging Face管道对象，`return_all_scores`参数设置为`True`。这意味着它将返回每个输入文本的正负情感标签和概率得分。
- en: Avoid Runtime Errors for SHAP explainers
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 避免SHAP解释器的运行时错误
- en: If we implement `self.explainer` in the `init` method in this class, we will
    encounter a runtime error related to the SHAP package's `_masked_model.py` file,
    which complains about `init` method will be serialized by MLflow, so it is clear
    that this runtime error comes from MLflow's serialization. However, implementing
    `self.explainer` in the `load_context` function avoids MLflow's serialization,
    and works correctly when invoking this explainer at runtime.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在此类的`init`方法中实现`self.explainer`，则会遇到与SHAP包的`_masked_model.py`文件相关的运行时错误，该错误抱怨`init`方法将被MLflow序列化，因此很明显，这个运行时错误来自MLflow的序列化。然而，在`load_context`函数中实现`self.explainer`可以避免MLflow的序列化，并且在运行时调用这个解释器时能正常工作。
- en: 'We will then implement the `sentiment_classifier_explanation` method, which
    takes an input of a pandas DataFrame row and produces a pickled `shap_values`
    output as an explanation for a single row of text input:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将实现`sentiment_classifier_explanation`方法，该方法接收一个pandas DataFrame行作为输入，并生成一个已保存的`shap_values`输出，作为单行文本输入的解释：
- en: '[PRE19]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that we need to use a pair of square brackets to enclose the `row['text']`
    value so that it becomes a list not just a single value. This is because this
    SHAP explainer expects a list of texts, not just a single string. If we don't
    enclose the value within the square brackets, then the explainer will split the
    entire string character by character, treating each character as if it is a word,
    which is not what we want. Once we get the Shapley values as the output from the
    explainer as `shap_values`, we then need to serialize them using `pickle.dumps`
    before returning to the caller. MLflow pyfunc model input and output signature
    do not support complex object without serialization, so this pickling step makes
    sure that the model output signature is MLflow compliant. We will see the definition
    of this MLflow pyfunc explainer's input and output signature in *step 5* shortly.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要使用一对方括号将 `row['text']` 的值括起来，使其成为一个列表，而不仅仅是单一的值。这是因为这个 SHAP 解释器期望的是一个文本列表，而不是单一的字符串。如果我们不将值放入方括号中，解释器会按字符拆分整个字符串，将每个字符当作一个单词，这不是我们想要的。一旦我们从解释器获得了作为
    `shap_values` 的 Shapley 值输出，我们就需要使用 `pickle.dumps` 对其进行序列化，然后再返回给调用方。MLflow pyfunc
    模型的输入输出签名不支持未序列化的复杂对象，因此这一步的 pickling 确保了模型输出签名符合 MLflow 的要求。稍后我们将在 *第5步* 中看到这个
    MLflow pyfunc 解释器的输入输出签名的定义。
- en: 'Next, we need to implement the required `predict` method for this class. This
    will apply the `sentiment_classifier_explanation` method to the entire input pandas
    DataFrame, as follows:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要为这个类实现所需的 `predict` 方法。这将应用 `sentiment_classifier_explanation` 方法到整个输入的
    pandas DataFrame，如下所示：
- en: '[PRE20]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will produce a new column named `shap_values` for each row of the input
    pandas DataFrame in the `text` column. We then drop the `text` column and return
    a single-column `shap_values` DataFrame as the final prediction result: in this
    case, the explanation results as a DataFrame.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为输入 pandas DataFrame 中的 `text` 列的每一行生成一个新的列，命名为 `shap_values`。然后我们删除 `text`
    列，返回一个单列的 `shap_values` DataFrame 作为最终的预测结果：在这种情况下，解释结果以 DataFrame 的形式呈现。
- en: 'Now that we have the `SentimentAnalysisExplainer` class implementation, we
    can use the standard MLflow pyfunc model logging API to log this model into the
    MLflow tracking server. Before doing the MLflow logging, let''s make sure we declare
    this explainer''s model signature, as follows:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经实现了 `SentimentAnalysisExplainer` 类，可以使用标准的 MLflow pyfunc 模型日志记录 API 将此模型记录到
    MLflow 跟踪服务器中。在进行 MLflow 日志记录之前，让我们确保声明此解释器的模型签名，如下所示：
- en: '[PRE21]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: These statements declare that the input is a DataFrame with a single `string`
    type `text` column and the output is a DataFrame with a single `string` type `shap_values`
    column. Recall that this `shap_values` column is a pickled serialized bytes string,
    which contains the Shapley values object.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这些语句声明了输入是一个包含单一 `string` 类型 `text` 列的 DataFrame，输出是一个包含单一 `string` 类型 `shap_values`
    列的 DataFrame。回想一下，这个 `shap_values` 列是一个经过 pickled 序列化的字节串，包含了 Shapley 值对象。
- en: 'Finally, we can implement the explainer logging step using the `mlflow.pyfunc.log_model`
    method in a task method, as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以在任务方法中使用 `mlflow.pyfunc.log_model` 方法实现解释器日志记录步骤，如下所示：
- en: '[PRE22]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: There are four parameters in the `log_model` method that we use. The `MODEL_ARTIFACT_PATH`
    is the name of the folder in the MLflow tracking server where the explainer will
    be stored. Here, the value is defined as `nlp_sentiment_classifier_explainer`
    in the Python file you checked out. `CONDA_ENV` is the `conda.yaml` file in this
    chapter's root folder. The `python_model` parameter is the `SentimentAnalysisExplainer`
    class we just implemented, and `signature` is the explainer input and output signature
    we defined.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `log_model` 方法中使用了四个参数。`MODEL_ARTIFACT_PATH` 是 MLflow 跟踪服务器中存储解释器的文件夹名称。这里，值在你查看的
    Python 文件中定义为 `nlp_sentiment_classifier_explainer`。`CONDA_ENV` 是本章根目录中的 `conda.yaml`
    文件。`python_model` 参数是我们刚刚实现的 `SentimentAnalysisExplainer` 类，`signature` 是我们定义的解释器输入输出签名。
- en: 'Now we are ready to run this whole file as follows in the command line:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好按如下方式在命令行运行整个文件：
- en: '[PRE23]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Assuming you have the local MLflow tracking server and environment variables
    set up correctly by following the `README.md` file for this chapter in the GitHub
    repository, this will produce the following two lines in the console output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经根据 GitHub 仓库中本章的 `README.md` 文件正确设置了本地 MLflow 跟踪服务器和环境变量，那么在控制台输出中将生成以下两行：
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This means we have successfully logged the explainer in our local MLflow tracking
    server.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们已经成功地将解释器记录到本地的 MLflow 跟踪服务器中。
- en: 'Go to the MLflow web UI at `http://localhost/` in the web browser and click
    the `dl_explain_chapter10` experiment folder. You should be able to find this
    run and the logged explainer in the `Artifacts` folder under `nlp_sentiment_classifier_explainer`,
    which should look as shown in *Figure 10.3*:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Web 浏览器中访问 `http://localhost/`，然后点击 `dl_explain_chapter10` 实验文件夹。你应该能在 `nlp_sentiment_classifier_explainer`
    下的 `Artifacts` 文件夹中找到此运行和记录的解释器，它应该如*图 10.3*所示：
- en: '![Figure 10.3 – A SHAP explainer is logged as an MLflow pyfunc model'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.3 – SHAP 解释器作为 MLflow pyfunc 模型被记录'
- en: '](img/B18120_10_003.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_10_003.jpg)'
- en: Figure 10.3 – A SHAP explainer is logged as an MLflow pyfunc model
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – SHAP 解释器作为 MLflow pyfunc 模型被记录
- en: Notice that the `MLmodel` metadata shown in *Figure 10.3* does not differ much
    from the normal DL inference pipeline that we logged before as an MLflow pyfunc
    model except for the `artifact_path` name and the `signature`. That's the advantage
    of using this approach because now we can use the generic MLflow pyfunc model
    methods to load this explainer or deploy it as a service.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*图 10.3* 中显示的 `MLmodel` 元数据与我们之前记录为 MLflow pyfunc 模型的普通 DL 推理流水线差别不大，除了
    `artifact_path` 名称和 `signature`。这就是使用这种方法的优势，因为现在我们可以使用通用的 MLflow pyfunc 模型方法来加载此解释器或将其作为服务部署。
- en: Problems with the mlflow.shap.log_explainer API
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: mlflow.shap.log_explainer API 的问题
- en: 'As we mentioned earlier, MLflow has a `mlflow.shap.log_explainer` API that
    provides a method to log an explainer. However, this API does not support our
    NLP sentiment classifier explainer because our NLP pipeline is not a known model
    flavor that MLflow currently supports. Thus even though `log_explainer` can write
    this explainer object into the tracking server, when loading the explainer back
    into memory using the `mlflow.shap.load_explainer` API, it will fail with the
    following error message: `mlflow.shap.log_explainer` API in this book.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，MLflow 提供了一个 `mlflow.shap.log_explainer` API，它提供了一种记录解释器的方法。然而，这个
    API 不支持我们的 NLP 情感分类器解释器，因为我们的 NLP 流水线不是 MLflow 当前支持的已知模型类型。因此，即使 `log_explainer`
    能将该解释器对象写入跟踪服务器，当通过 `mlflow.shap.load_explainer` API 将解释器加载回内存时，它会因以下错误消息而失败：`mlflow.shap.log_explainer`
    API 在本书中的问题。
- en: 'Now that we have a logged explainer, we can use it in two ways: deploy it into
    a web service so that we can create an endpoint to establish an EaaS, or load
    the explainer directly through MLflow pyfunc `load_model` or `spark_udf` method
    using the MLflow `run_id`. Let''s start with the web service deployment by setting
    up a local web service.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了已记录的解释器，可以通过两种方式使用它：将其部署到 Web 服务中，以便我们可以创建一个端点来建立 EaaS，或者直接通过 MLflow pyfunc
    的 `load_model` 或 `spark_udf` 方法使用 MLflow 的 `run_id` 加载解释器。让我们从设置本地 Web 服务来开始部署。
- en: Deploying an MLflow pyfunc explainer for an EaaS
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署 MLflow pyfunc 解释器以进行 EaaS
- en: 'We can set up a local EaaS in a standard MLflow way since now the SHAP explainer
    is just like a generic MLflow pyfunc model. Perform the following steps to see
    how this can be implemented locally:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现在 SHAP 解释器就像一个通用的 MLflow pyfunc 模型，我们可以按标准 MLflow 方式设置一个本地 EaaS。请按照以下步骤查看如何在本地实现：
- en: 'Run the following MLflow command to set up a local web service for the explainer
    we just logged. The `run_id` in this example is `ad1edb09e5ea4d8ca0332b8bc2f5f6c9`:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下 MLflow 命令，以为我们刚刚记录的解释器设置本地 Web 服务。此示例中的 `run_id` 是 `ad1edb09e5ea4d8ca0332b8bc2f5f6c9`：
- en: '[PRE25]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will produce the following console output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下控制台输出：
- en: '![ Figure 10.4 – SHAP EaaS console output'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.4 – SHAP EaaS 控制台输出'
- en: '](img/B18120_10_004.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_10_004.jpg)'
- en: Figure 10.4 – SHAP EaaS console output
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – SHAP EaaS 控制台输出
- en: 'Notice that in *Figure 10.4*, the default underlying pretrained language model
    is loaded after the `gunicore` HTTP server is up and running. This is because
    our implementation of the explainer was inside the `load_context` method, which
    is exactly what is to be expected: loading the explainer immediately after the
    web service is up and running.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在*图 10.4*中，默认的预训练语言模型在 `gunicore` HTTP 服务器启动并运行后加载。这是因为我们实现的解释器位于 `load_context`
    方法中，这正是预期的行为：在 Web 服务启动并运行后立即加载解释器。
- en: 'In a different terminal window, type the following command to invoke the explainer
    web service at port `5000` of localhost with two sample texts as input:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个终端窗口中，输入以下命令以调用本地主机端口 `5000` 上的解释器 Web 服务，并输入两个示例文本：
- en: '[PRE26]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This will produce the following output:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![Figure 10.5 – Response in a DataFrame after calling our SHAP EaaS'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.5 – 调用我们的 SHAP EaaS 后在数据框中的响应'
- en: '](img/B18120_10_005.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_10_005.jpg)'
- en: Figure 10.5 – Response in a DataFrame after calling our SHAP EaaS
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 - 调用我们的SHAP EaaS后DataFrame中的响应
- en: Note that in *Figure 10.5*, the column name is `shap_values`, while the values
    are pickled bytes hexadecimal data. These are not human readable, but can be converted
    back to the original `shap_values` using `pickle.loads` method at the caller side.
    So, if you see a response output like *Figure 10.5*, congratulations for setting
    up a local EaaS! You can deploy this explainer service just like other MLflow
    service deployments, as described in [*Chapter 8*](B18120_08_ePub.xhtml#_idTextAnchor095)*,
    Deploying a DL Inference Pipeline at Scale*, since this explainer now can be called
    just like a generic MLflow pyfunc model service.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在*图10.5*中，列名为 `shap_values`，而其值为pickle序列化的字节十六进制数据。这些数据并不易于阅读，但可以在调用方使用`pickle.loads`方法将其转换回原始的
    `shap_values`。因此，如果您看到类似*图10.5*的响应输出，恭喜您已经设置了本地EaaS！您可以像其他MLflow服务部署一样部署此解释器服务，正如[*第8章*](B18120_08_ePub.xhtml#_idTextAnchor095)*，在规模上部署DL推理管道*中描述的那样，因为此解释器现在可以像通用的MLflow
    pyfunc模型服务一样调用。
- en: Next, we will see how to use the MLflow pyfunc explainer for batch explanation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何使用MLflow pyfunc解释器进行批量解释。
- en: Using an MLflow pyfunc explainer for batch explanation
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用MLflow pyfunc解释器进行批量解释
- en: 'There are two ways to implement offline batch explanation using an MLflow pyfunc
    explainer:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以使用MLflow pyfunc解释器实现离线批量解释：
- en: Load the pyfunc explainer as an MLflow pyfunc model to explain a given pandas
    DataFrame input.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将pyfunc解释器加载为MLflow pyfunc模型，以解释给定的pandas DataFrame输入。
- en: Load the pyfunc explainer as a PySpark UDF to explain a given PySpark DataFrame
    input.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将pyfunc解释器加载为PySpark UDF以解释给定的PySpark DataFrame输入。
- en: Let's start with loading the explainer as an MLflow pyfunc model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从将解释器作为MLflow pyfunc模型加载开始。
- en: Loading the MLflow pyfunc explainer as an MLflow pyfunc model
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将MLflow pyfunc解释器加载为MLflow pyfunc模型
- en: 'As we have already mentioned, another way to consume an MLflow logged explainer
    is to load the explainer in a local Python code using MLflow''s pyfunc `load_model`
    method directly, instead of deploying it into a web service. This is very straightforward,
    and we will show you how it can be done. You can check out the code in the `shap_mlflow_pyfunc_explainer.py`
    file in the GitHub repository ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyfunc_explainer.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyfunc_explainer.py)):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，消费MLflow记录的解释器的另一种方法是直接在本地Python代码中使用MLflow的pyfunc `load_model`方法加载解释器，而不是将其部署为Web服务。这非常直接，我们将向您展示如何完成。您可以在GitHub存储库中的`shap_mlflow_pyfunc_explainer.py`文件中查看代码（[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyfunc_explainer.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyfunc_explainer.py))：
- en: 'The first step is to load the logged explainer back into memory. The following
    code does this using `mlflow.pyfunc.load_model` and the explainer `run_id` URI:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是将记录的解释器加载回内存。以下代码使用 `mlflow.pyfunc.load_model` 和解释器 `run_id` URI 来实现这一点：
- en: '[PRE27]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This should load the explainer as if it is just a generic MLflow pyfunc model.
    We can print out the metadata of the explainer by running the following code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该加载解释器，就像它只是一个通用的MLflow pyfunc模型一样。我们可以通过运行以下代码打印解释器的元数据：
- en: '[PRE28]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This will show the following output:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下输出：
- en: '[PRE29]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This means this is a `mlflow.pyfunc.model` flavor, which is great news, since
    we can use the same MLflow pyfunc API to use this explainer.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着这是一个`mlflow.pyfunc.model`风格，这是个好消息，因为我们可以使用相同的MLflow pyfunc API来使用这个解释器。
- en: 'Next, we will get some example data to test the newly loaded explainer:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将获取一些示例数据来测试新加载的解释器：
- en: '[PRE30]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This will load the IMDb test dataset, truncate each review text to 500 characters,
    and pick the first 20 rows to make a pandas DataFrame for explanation in the next
    step.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这将加载IMDb测试数据集，将每个评论文本截断为500个字符，并选择前20行，以便为下一步的解释创建一个pandas DataFrame。
- en: 'Now, we can run the explainer as follows:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以按如下方式运行解释器：
- en: '[PRE31]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This will run the SHAP partition explainer for the input DataFrame `df_test`.
    It will show the following output for each row of the DataFrame when it is running:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为输入DataFrame `df_test` 运行SHAP分区解释器。当运行时，它将显示DataFrame的每一行的以下输出：
- en: '[PRE32]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The result will be a pandas DataFrame with a single column, `shap_values`. This
    may take a few minutes as it needs to tokenize each row, execute the explainer,
    and serialize the output.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是一个包含单列`shap_values`的pandas DataFrame。这可能需要几分钟时间，因为它需要对每一行进行分词、执行解释器并序列化输出。
- en: 'Once the explainer execution is done, we can check the results by deserializing
    the row content. Here is the code to check the first output:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦解释器执行完毕，我们可以通过反序列化行内容来检查结果。以下是检查第一行输出的代码：
- en: '[PRE33]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This will print out the first row''s `shap_values`. *Figure 10.6* shows a partial
    screenshot of the output of `shap_values`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出第一行的`shap_values`。*图10.6*展示了`shap_values`输出的部分截图：
- en: '![Figure 10.6 – Partial output of the deserialized shap_values from the explanation'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.6 – 解释中反序列化`shap_values`的部分输出'
- en: '](img/B18120_10_006.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_10_006.jpg)'
- en: Figure 10.6 – Partial output of the deserialized shap_values from the explanation
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 – 解释中反序列化`shap_values`的部分输出
- en: As we can see in *Figure 10.6*, the output of `shap_values` is no different
    from what we learned in [*Chapter 9*](B18120_09_ePub.xhtml#_idTextAnchor112)*,
    Fundamentals of Deep Learning Explainability*, when we did not use MLflow to log
    and load the explainer. We can also generate Shapley text plots to highlight the
    contribution of the texts to the predicted sentiment.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在*图10.6*中所看到的，`shap_values`的输出与我们在[*第9章*](B18120_09_ePub.xhtml#_idTextAnchor112)*《深度学习可解释性基础》*中学到的没有什么不同，当时我们没有使用MLflow记录和加载解释器。我们还可以生成Shapley文本图，以突出文本对预测情感的贡献。
- en: 'Run the following statement in the notebook to see the Shapely text plot:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本中运行以下语句以查看Shapley文本图：
- en: '[PRE34]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This will generate a plot displayed in *Figure 10.7*:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成一个如*图10.7*所示的图：
- en: '![Figure 10.7 – Shapley text plot using deserialized shap_values from our MLflow
    logged explainer'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.7 – 使用我们通过MLflow记录的解释器反序列化`shap_values`生成的Shapley文本图'
- en: '](img/B18120_10_007.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_10_007.jpg)'
- en: Figure 10.7 – Shapley text plot using deserialized shap_values from our MLflow
    logged explainer
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 – 使用我们通过MLflow记录的解释器反序列化`shap_values`生成的Shapley文本图
- en: As can be seen in *Figure 10.7*, this review has a positive sentiment and the
    keywords or phrases that contribute to the predicted sentiment are `good`, `love`,
    and some other phrases highlighted in red. When you see this Shapley text plot,
    you should give yourself a round of applause, as you have finished learning how
    to use an MLflow logged explainer to generate batch explanation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.7*所示，这条评论具有正面情感，贡献于预测情感的关键词或短语包括`good`、`love`以及其他一些以红色标记的短语。当你看到这个Shapley文本图时，应该为自己鼓掌，因为你已经学会了如何使用MLflow记录的解释器来生成批量解释。
- en: As mentioned during the step-by-step implementation of this batch explanation,
    it is a little slow to do a large batch explanation using this pyfunc model approach.
    Luckily, we have another way to implement the batch explanation using the PySpark
    UDF function, which we will explain in the next subsection.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如在这一批量解释的逐步实现中提到的，使用这种pyfunc模型方法进行大规模批量解释会稍微慢一些。幸运的是，我们还有另一种方法可以使用PySpark UDF函数实现批量解释，我们将在下一个子章节中进行解释。
- en: Loading the pyfunc explainer as a PySpark UDF
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载pyfunc解释器作为PySpark UDF
- en: 'For scalable batch explanation, we can use Spark''s distributed computing capability,
    which is supported by loading the pyfunc explainer as a PySpark UDF. There is
    no extra work to use this capability, since this is provided by the MLflow pyfunc
    API already through the `mlflow.pyfunc.spark_udf` method. We will show you how
    to implement this at-scale explanation step by step:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可扩展的批量解释，我们可以利用Spark的分布式计算能力，这通过将pyfunc解释器作为PySpark UDF加载来支持。使用此功能不需要额外的工作，因为MLflow
    pyfunc API已经通过`mlflow.pyfunc.spark_udf`方法提供了这一功能。我们将一步步向你展示如何实现这种大规模解释：
- en: First, make sure you have worked through the `README.md` file ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md))
    to install Spark, create and activate the `chapter10-dl-pyspark-explain` virtual
    environment, and set up all the environment variables before you run the PySpark
    UDF code to do the explanation at scale.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，确保你已经完成了`README.md`文件的操作（[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/README.md)），以安装Spark，创建并激活`chapter10-dl-pyspark-explain`虚拟环境，并在运行PySpark
    UDF代码进行大规模解释之前设置所有环境变量。
- en: 'Then you can start running the VS Code notebook, `shap_mlflow_pyspark_explainer.py`,
    which you can check out in the GitHub repository: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyspark_explainer.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyspark_explainer.py).
    Run the following command at `chapter10/notebooks/`:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以开始运行VS Code笔记本`shap_mlflow_pyspark_explainer.py`，您可以在GitHub仓库中查看：[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyspark_explainer.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter10/notebooks/shap_mlflow_pyspark_explainer.py)。在`chapter10/notebooks/`目录下运行以下命令：
- en: '[PRE35]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You will get the final output displayed in *Figure 10.8*, among quite a few
    lines of output preceding these final few lines:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您将会得到显示在*图10.8*中的最终输出，前面有相当多行输出在这些最后几行之前：
- en: '![Figure 10.8 – PySpark UDF explainer''s output of the first two rows of text''s
    shap_values along with their input texts'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.8 – PySpark UDF解释器输出的前两行文本的shap_values以及它们的输入文本'
- en: '](img/B18120_10_008.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_10_008.jpg)'
- en: Figure 10.8 – PySpark UDF explainer's output of the first two rows of text's
    shap_values along with their input texts
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 – PySpark UDF解释器输出的前两行文本的shap_values以及它们的输入文本
- en: 'As can be seen in *Figure 10.8*, the PySpark UDF explainer''s output is a PySpark
    DataFrame that has two columns: `text` and `shap_values`. The `text` column is
    the original input text, while the `shap_values` column contains the pickled serialized
    Shapley values, just like we saw in the previous subsection when we used the pyfunc
    explainer for the pandas DataFrame.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在*图10.8*中所示，PySpark UDF解释器的输出是一个PySpark DataFrame，包含两列：`text`和`shap_values`。`text`列是原始输入文本，而`shap_values`列包含了像我们在前面子节中使用pandas
    DataFrame时看到的Shapley值的pickled序列化形式。
- en: Now let's see what's happening in the code. We will explain the key code blocks
    in the `shap_mlflow_pyspark_explainer.py` file. Since this is a VS Code notebook,
    you can run it either in the command line as we just did or interactively inside
    the VS Code IDE window.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看代码中发生了什么。我们将解释`shap_mlflow_pyspark_explainer.py`文件中的关键代码块。由于这是一个VS Code笔记本，您可以像我们刚刚在命令行中运行或在VS
    Code IDE窗口中交互式地运行它。
- en: 'The first key code block is to load the explainer using the `mflow.pyfunc.spark_udf`
    method, as follows:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个关键代码块是使用`mflow.pyfunc.spark_udf`方法加载解释器，如下所示：
- en: '[PRE36]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The first statement is to initialize a `SparkSession` variable and then use
    `run_id` to load the logged explainer into memory. Run the explainer to get the
    metadata as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条语句是初始化一个`SparkSession`变量，然后使用`run_id`将记录的解释器加载到内存中。运行解释器以获取元数据如下：
- en: '[PRE37]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We will get the following result:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下结果：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This means we now have a SHAP explainer wrapped as a Spark UDF function. This
    allows us to directly apply the SHAP explainer for an input PySpark DataFrame
    in the next step.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们现在将SHAP解释器包装为Spark UDF函数。这允许我们在下一步直接对输入的PySpark DataFrame应用SHAP解释器。
- en: 'We load the IMDb test dataset as before to get a list of `short_data`, and
    then create a PySpark DataFrame for the top 20 rows of the test dataset for explanation:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们像以前一样加载IMDb测试数据集，以获得`short_data`列表，然后为测试数据集的前20行创建一个PySpark DataFrame以进行解释：
- en: '[PRE39]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note the last statement, which uses PySpark's `withColumn` function to add a
    new `shap_values` column to the input DataFrame, `spark_df`, which originally
    contained only one column, `text`. This is a natural way to use Spark's parallel
    and distributed computing capability. If you have run both the previous non-Spark
    approach using the MLflow pyfunc `load_model` method and the current PySpark UDF
    one, you will notice that the Spark approach runs much faster, even on a local
    computer. This allows us to do SHAP explanation at scale for many instances of
    input texts.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 注意最后一条语句，它使用PySpark的`withColumn`函数将一个新的`shap_values`列添加到输入DataFrame `spark_df`中，该DataFrame最初只包含一个列`text`。这是使用Spark的并行和分布式计算能力的自然方式。如果您已经运行了使用MLflow
    pyfunc `load_model`方法和当前PySpark UDF方法的前面的非Spark方法，您会注意到Spark方法即使在本地计算机上也运行得更快。这使我们能够为许多输入文本实例规模化地进行SHAP解释。
- en: Finally, to verify the results, we show the `spark_df` DataFrame's top two rows,
    which was illustrated in *Figure 10.8*.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，为了验证结果，我们展示了`spark_df` DataFrame的前两行，这在*图10.8*中有所说明。
- en: By now, with MLflow's pyfunc Spark UDF wrapped SHAP explainer, we can confidently
    do large-scale batch explanation. Congratulations!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过MLflow的pyfunc Spark UDF包装的SHAP解释器，我们可以自信地进行大规模批量解释。恭喜！
- en: Let's now summarize what we have learned in this chapter in the next section.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的部分总结本章所学内容。
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we first reviewed the existing approaches in the MLflow APIs
    that could be used for implementing explainability. Two existing MLflow APIs,
    `mlflow.shap` and `mlflow.evaluate`, have limitations, thus cannot be used for
    the complex DL models and pipelines explainability scenarios we need. We then
    focused on two main approaches to implement SHAP explanations and explainers within
    the MLflow API framework: `mlflow.log_artifact` for logging explanations and `mlflow.pyfunc.PythonModel`
    for logging a SHAP explainer. Using the `log_artifact` API can allow us to log
    Shapley values and explanation plots into the MLflow tracking server. Using `mlflow.pyfunc.PythonModel`
    allows us to log a SHAP explainer as a MLflow pyfunc model, thus opening doors
    to deploy a SHAP explainer as a web service to create an EaaS endpoint. It also
    opens doors to use SHAP explainers through the MLflow pyfunc `load_model` or `spark_udf`
    API for large-scale offline batch explanation. This enables us to confidently
    implement explainability at scale for DL models.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先回顾了可以用于实现可解释性的 MLflow API 中的现有方法。两个现有的 MLflow API，`mlflow.shap` 和 `mlflow.evaluate`，存在一定的局限性，因此不能用于我们需要的复杂深度学习模型和管道的可解释性场景。随后，我们重点介绍了在
    MLflow API 框架内实现 SHAP 解释和解释器的两种主要方法：使用 `mlflow.log_artifact` 记录解释内容，以及使用 `mlflow.pyfunc.PythonModel`
    记录 SHAP 解释器。使用 `log_artifact` API 可以将 Shapley 值和解释图记录到 MLflow 跟踪服务器中。而使用 `mlflow.pyfunc.PythonModel`
    则可以将 SHAP 解释器作为 MLflow pyfunc 模型记录，从而为将 SHAP 解释器作为 Web 服务部署并创建 EaaS 端点提供了可能。同时，它也为通过
    MLflow pyfunc 的 `load_model` 或 `spark_udf` API 执行大规模离线批量解释提供了可能。这使我们能够在大规模应用中为深度学习模型实现可解释性，增加了实现的信心。
- en: As the field of explainability continues to evolve, MLflow's integration with
    SHAP and other explainability toolboxes will also continue to improve. Interested
    readers are encouraged to continue their learning journey through the links provided
    in the further reading section. Happy continuous learning and growing!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 随着可解释性领域的不断发展，MLflow 与 SHAP 及其他可解释性工具箱的集成也将持续改进。我们鼓励有兴趣的读者通过进一步阅读部分提供的链接，继续他们的学习之旅。祝愿大家持续学习和成长！
- en: Further reading
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Shapley Values at Scale: [https://neowaylabs.github.io/data-science/shapley-values-at-scale/](https://neowaylabs.github.io/data-science/shapley-values-at-scale/)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模 Shapley 值：[https://neowaylabs.github.io/data-science/shapley-values-at-scale/](https://neowaylabs.github.io/data-science/shapley-values-at-scale/)
- en: 'Scaling SHAP Calculations With PySpark and Pandas UDF: [https://databricks.com/blog/2022/02/02/scaling-shap-calculations-with-pyspark-and-pandas-udf.html](https://databricks.com/blog/2022/02/02/scaling-shap-calculations-with-pyspark-and-pandas-udf.html)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PySpark 和 Pandas UDF 扩展 SHAP 计算：[https://databricks.com/blog/2022/02/02/scaling-shap-calculations-with-pyspark-and-pandas-udf.html](https://databricks.com/blog/2022/02/02/scaling-shap-calculations-with-pyspark-and-pandas-udf.html)
- en: 'Speeding up Shapley value computation using Ray, a distributed computing system:
    [https://www.telesens.co/2020/10/05/speeding-up-shapley-value-computation-using-ray-a-distributed-computing-system/](https://www.telesens.co/2020/10/05/speeding-up-shapley-value-computation-using-ray-a-distributed-computing-system/)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray 分布式计算系统加速 Shapley 值计算：[https://www.telesens.co/2020/10/05/speeding-up-shapley-value-computation-using-ray-a-distributed-computing-system/](https://www.telesens.co/2020/10/05/speeding-up-shapley-value-computation-using-ray-a-distributed-computing-system/)
- en: 'Interpreting an NLP model with LIME and SHAP: [https://medium.com/@kalia_65609/interpreting-an-nlp-model-with-lime-and-shap-834ccfa124e4](mailto:https://medium.com/@kalia_65609/interpreting-an-nlp-model-with-lime-and-shap-834ccfa124e4)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 LIME 和 SHAP 解释 NLP 模型：[https://medium.com/@kalia_65609/interpreting-an-nlp-model-with-lime-and-shap-834ccfa124e4](mailto:https://medium.com/@kalia_65609/interpreting-an-nlp-model-with-lime-and-shap-834ccfa124e4)
- en: 'Model Evaluation in MLflow: [https://databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html](https://databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLflow 中的模型评估：[https://databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html](https://databricks.com/blog/2022/04/19/model-evaluation-in-mlflow.html)
