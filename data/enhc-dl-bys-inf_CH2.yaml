- en: Chapter 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章
- en: Fundamentals of Bayesian Inference
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯推理基础
- en: 'Before we get into Bayesian inference with **Deep Neural Networks** (**DNNs**),
    we should take some time to understand the fundamentals. In this chapter, we’ll
    do just that: exploring the core concepts of Bayesian modeling, and taking a look
    at some of the popular methods used for Bayesian inference. By the end of this
    chapter, you should have a good understanding of why we use probabilistic modeling,
    and what kinds of properties we look for in well principled – or well conditioned
    – methods.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始探讨使用**深度神经网络**（**DNNs**）进行贝叶斯推理之前，我们应该花一些时间理解基本原理。在本章中，我们将进行这样的探讨：探索贝叶斯建模的核心概念，并了解一些常用的贝叶斯推理方法。在本章结束时，你应该能很好地理解我们为何使用概率建模，以及我们在良好原则化的或良好条件化的模型中寻求什么样的特性。
- en: 'This content will be covered in the following sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本内容将涵盖以下部分：
- en: Refreshing our knowledge of Bayesian modeling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刷新我们对贝叶斯建模的理解
- en: Bayesian inference via sampling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过采样进行贝叶斯推理
- en: Exploring the Gaussian processes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索高斯过程
- en: 2.1 Refreshing our knowledge of Bayesian modeling
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 刷新我们对贝叶斯建模的理解
- en: 'Bayesian modeling is concerned with understanding the probability of an event
    occurring given some prior assumptions and some observations. The prior assumptions
    describe our initial beliefs, or hypothesis, about the event. For example, let’s
    say we have two six-sided dice, and we want to predict the probability that the
    sum of the two dice is 5\. First, we need to understand how many possible outcomes
    there are. Because each die has 6 sides, the number of possible outcomes is 6
    × 6 = 36\. To work out the possibility of rolling a 5, we need to work out how
    many combinations of values will sum to 5:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯建模关注的是在一些先验假设和观察数据的基础上理解事件发生的概率。先验假设描述了我们对事件的初步信念或假设。例如，假设我们有两个六面骰子，并且我们想预测两个骰子和为5的概率。首先，我们需要了解有多少种可能的结果。因为每个骰子有6面，所有可能的结果数是6
    × 6 = 36。为了计算掷出和为5的可能性，我们需要算出哪些数值的组合加起来是5：
- en: '![PIC](img/file7.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file7.png)'
- en: 'Figure 2.1: Illustration of all values summing to five when rolling two six-sided
    dice'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：展示掷出两个六面骰子时所有数值和为5的情况
- en: As we can see here, there are 4 combinations that add up to 5, thus the probability
    of having two dice produce a sum of 5 is ![-4 36](img/file8.jpg), or ![1 9](img/file9.jpg).
    We call this initial belief the **prior**. Now, what happens if we incorporate
    information from an observation? Let’s say we know what the value for one of the
    dice will be – let’s say 3\. This shrinks our number of possible values down to
    6, as we only have the remaining die to roll, and for the result to be 5, we’d
    need this value to be 2.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，有4种组合的和为5，因此两个骰子和为5的概率是！[-4 36](img/file8.jpg)，即![1 9](img/file9.jpg)。我们称这个初步的信念为**先验**。现在，如果我们加入来自观察的信息会发生什么呢？假设我们知道其中一个骰子的值是3，这就将我们可能的值缩小到6，因为我们只剩下另一个骰子要掷，为了使结果为5，我们需要另一个骰子的值是2。
- en: '![PIC](img/file10.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file10.png)'
- en: 'Figure 2.2: Illustration of remaining value, which sums to five after rolling
    the first die'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：展示剩余的值，在掷出第一个骰子后和为5
- en: 'Because we assume our die is fair, the probability of the sum of the dice being
    5 is now ![1 6](img/file11.jpg). This probability, called the **posterior**, is
    obtained using information from our observation. At the core of Bayesian statistics
    is Bayes’ rule (hence ”Bayesian”), which we use to determine the posterior probability
    given some prior knowledge. Bayes’ rule is defined as:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们假设骰子是公平的，现在两个骰子和为5的概率是![1 6](img/file11.jpg)。这个概率，称为**后验**，是通过我们的观察得到的信息计算出来的。贝叶斯统计的核心是贝叶斯定理（因此称为“贝叶斯”），我们用它来在有一些先验知识的情况下确定后验概率。贝叶斯定理定义如下：
- en: '![P(A |B ) = P(B-|A)×-P-(A)- P(B ) ](img/file12.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![P(A |B ) = P(B-|A)×-P-(A)- P(B ) ](img/file12.jpg)'
- en: 'Where we can define *P*(*A*|*B*) as *P*(*d*[1] + *d*[2] = 5|**d**[1] = **3**),
    where *d*[1] and *d*[2] represent dice 1 and 2 respectively. We can see this in
    action using our previous example. Starting with the **likelihood**, that is,
    the term on the left of our numerator, we see that:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以定义*P*(*A*|*B*)为*P*(*d*[1] + *d*[2] = 5|**d**[1] = **3**)，其中*d*[1]和*d*[2]分别代表骰子1和骰子2。我们可以通过之前的例子看到这一点。从**似然**开始，即分子左侧的项，我们看到：
- en: '![1- P (B |A) = P (d1 = 3|d1 + d2 = 5) = 4 ](img/file13.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![1- P (B |A) = P (d1 = 3|d1 + d2 = 5) = 4 ](img/file13.jpg)'
- en: 'We can verify this by looking at our grid. Moving to the second part of the
    numerator – the prior – we see that:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看我们的网格来验证这一点。移动到分子部分的第二部分——先验——我们看到：
- en: '![ 4 1 P(A ) = P (d1 + d2 = 5) =--= -- 36 9 ](img/file14.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![ 4 1 P(A ) = P (d1 + d2 = 5) =--= -- 36 9 ](img/file14.jpg)'
- en: 'On the denominator, we have our **normalization constant** (also referred to
    as the **marginal likelihood**), which is simply:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在分母部分，我们有我们的**归一化常数**（也称为**边际似然**），它就是：
- en: '![P(B ) = P (d1 = 3) = 1 6 ](img/file15.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![P(B ) = P (d1 = 3) = 1 6 ](img/file15.jpg)'
- en: 'Putting this all together using Bayes’ theorem, we have:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 将这一切结合在一起使用贝叶斯定理，我们得到：
- en: '![ 14 × 19 1 P(d1 + d2 = 5|d1 = 3) = --1---= 6- 6 ](img/file16.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![ 14 × 19 1 P(d1 + d2 = 5|d1 = 3) = --1---= 6- 6 ](img/file16.jpg)'
- en: What we have here is the *probability* of the outcome being 5 if we know one
    die’s value. However, in this book, we’ll often be referring to **uncertainties**
    rather than probabilities – and learning methods to obtain uncertainty estimates
    with DNNs. These methods belong to a broader class of **uncertainty** **quantification**,
    and aim to quantify the uncertainty in the predictions from an ML model. That
    is, we want to predict *P*(*ŷ*|*𝜃*), where *ŷ* is a prediction from a model, and
    *𝜃* represents the parameters of the model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们得到的是，如果我们知道一个骰子的值，结果为 5 的*概率*。然而，在本书中，我们将经常提到**不确定性**，而不是概率——并且学习如何通过深度神经网络（DNN）获得不确定性估计。这些方法属于更广泛的**不确定性量化**类别，旨在量化来自机器学习模型预测中的不确定性。也就是说，我们希望预测
    *P*(*ŷ*|*𝜃*)，其中 *ŷ* 是模型的预测值，而 *𝜃* 代表模型的参数。
- en: As we know from fundamental probability theory, probabilities are bound between
    0 and 1\. The closer we are to 1, the more likely – or probable – the event is.
    We can view our uncertainty as subtracting our probability from 1\. In the context
    of the example here, the probability of the sum being 5 is *P*(*d*[1] + *d*[2]
    = 5|*d*[1] = 3) = ![1 6](img/file17.jpg) = 0*.*166\. So, our uncertainty is simply
    1 −![16](img/file18.jpg) = ![56](img/file19.jpg) = 0*.*833, meaning that there’s
    a *>* 80% chance that the outcome *will* *not* be 5\. As we proceed through the
    book, we’ll learn about different sources of uncertainty, and how uncertainties
    can help us to develop more robust deep learning systems.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从基础概率论中知道的那样，概率是介于 0 和 1 之间的。越接近 1，事件发生的可能性就越大——或称为概率越高。我们可以将我们的不确定性视为从
    1 中减去概率。在这里的示例中，和为 5 的概率是 *P*(*d*[1] + *d*[2] = 5|*d*[1] = 3) = ![1 6](img/file17.jpg)
    = 0*.*166。所以，我们的不确定性就是 1 −![16](img/file18.jpg) = ![56](img/file19.jpg) = 0*.*833，意味着结果*不会*是
    5 的概率*大于* 80%。随着我们深入本书，我们将学习不同的不确定性来源，以及如何利用不确定性来开发更强大的深度学习系统。
- en: 'Let’s continue using our dice example to build a better understanding of for
    model uncertainty estimates. Many common machine learning models work on the basis
    of **maximum likelihood estimation** or **MLE**. That is, they look to predict
    the value that is *most likely*: tuning their parameters during training to produce
    the most likely outcome *ŷ* given some input *x*. As a simple illustration, let’s
    say we want to predict the value of *d*[1] + *d*[2] given a value of *d*[1]. We
    can simply define this as the **expectation** of *d*[1] + *d*[2] conditioned on
    *d*[1]:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续使用我们的骰子示例，以更好地理解模型不确定性估计。许多常见的机器学习模型基于**最大似然估计**（**MLE**）。也就是说，它们寻求预测最*可能*的值：在训练过程中调节其参数，以根据一些输入
    *x* 产生最可能的输出 *ŷ*。作为简单的说明，假设我们要预测 *d*[1] + *d*[2] 的值，给定 *d*[1] 的值。我们可以简单地将其定义为**期望**，即在
    *d*[1] 条件下，*d*[1] + *d*[2] 的期望值：
- en: '![ˆy = 𝔼 [d + d |d ] 1 2 1 ](img/file20.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![ˆy = 𝔼 [d + d |d ] 1 2 1 ](img/file20.jpg)'
- en: That is, the *mean* of the possible values of *d*[1] + *d*[2].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 即，*d*[1] + *d*[2] 的可能值的*均值*。
- en: 'Setting *d*[1] = 3, our possible values for *d*[1] + *d*[2] are {4*,*5*,*6*,*7*,*8*,*9}
    (as illustrated in *Figure 2.2*), making our mean:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 *d*[1] = 3，我们可能得到的 *d*[1] + *d*[2] 的值为 {4*,*5*,*6*,*7*,*8*,*9}（如*图 2.2*所示），使得我们的均值：
- en: '![ 1 ∑6 4+ 5 + 6+ 7+ 8 + 9 μ = -- ai = --------------------= 6.5 6 i=1 6 ](img/file21.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 ∑6 4+ 5 + 6+ 7+ 8 + 9 μ = -- ai = --------------------= 6.5 6 i=1 6 ](img/file21.jpg)'
- en: 'This is the value we’d get from a simple linear model, such as a linear regression
    defined by:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们从简单线性模型中得到的值，比如由以下定义的线性回归模型：
- en: '![ˆy = βx + ξ ](img/file22.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![ˆy = βx + ξ ](img/file22.jpg)'
- en: 'In this case, the values of our intersection and bias are *β* = 1, *ξ* = 3*.*5\.
    If we change our value of *d*[1] to 1, we see that this mean changes to 4*.*5
    – the mean of the set of possible values of *d*[1] + *d*[2]|*d*[1] = 1, in other
    words {2*,*3*,*4*,*5*,*6*,*7}. This perspective on our model predictions is important:
    while this example is very straightforward, the same principle applies to far
    more sophisticated models and data. The value we typically see with ML models
    is the *expectation*, otherwise known as the mean. As you are likely aware, the
    mean is often referred to as the **first statistical moment** – with the **second
    statistical** **moment** being the **variance**, and the variance allows us to
    quantify uncertainty.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的交集和偏差值为*β* = 1，*ξ* = 3*.*5。如果我们将*d*[1]的值更改为1，我们会看到均值变为4*.*5——即*d*[1]
    + *d*[2]|*d*[1] = 1的可能值集合的均值，换句话说，{2*,*3*,*4*,*5*,*6*,*7}。这种对我们模型预测的视角很重要：虽然这个例子非常简单，但同样的原则适用于更复杂的模型和数据。我们在机器学习模型中通常看到的值是*期望值*，也就是均值。如你所知，均值通常被称为**第一统计矩**——而**第二统计矩**就是**方差**，方差使我们能够量化不确定性。
- en: 'The variance for our simple example is defined as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简单示例的方差定义如下：
- en: '![ ∑6 2 σ2 = --i=1(ai −-μ) n − 1 ](img/file23.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑6 2 σ2 = --i=1(ai −-μ) n − 1 ](img/file23.jpg)'
- en: 'These statistical moments should be familiar to you, as should the fact that
    the variance here is represented as the square of the **standard deviation**,
    *σ*. For our example here, for which we assume *d*[2] is a fair die, the variance
    will always be constant: *σ*² = 2*.*917\. That is to say, given any value of *d*[1],
    we know that values of *d*[2] are all equally likely, so the uncertainty does
    not change. But what if we have an unfair die *d*[2], which has a 50% chance of
    landing on a 6, and a 10% chance of landing on each other number? This changes
    both our mean and our variance. We can see this by looking at how we would represent
    this as a set of possible values (in other words, a perfect sample of the die)
    – the set of possible values for *d*[1] + *d*[2]|*d*[1] = 1 now becomes {2*,*3*,*4*,*5*,*6*,*7*,*7*,*7*,*7*,*7}.
    Our new model will now have a bias of *ξ* = 4*.*5, making our prediction:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这些统计量对你应该是熟悉的，方差在这里表示为**标准差**的平方，*σ*。对于我们的示例，这里假设*d*[2]是一个公平的骰子，方差始终是常数：*σ*²
    = 2*.*917。也就是说，给定任何*d*[1]的值，我们知道*d*[2]的所有值都是同等可能的，因此不确定性不会变化。但如果我们有一个不公平的骰子*d*[2]，它有50%的概率掷出6，且每个其他数字的概率是10%呢？这将改变我们的均值和方差。我们可以通过看如何表示这一点作为一组可能的值（换句话说，骰子的一个完美样本）来看到这一点——*d*[1]
    + *d*[2]|*d*[1] = 1的可能值集合现在变为{2*,*3*,*4*,*5*,*6*,*7*,*7*,*7*,*7*,*7}。我们的新模型现在将有一个偏差*ξ*
    = 4*.*5，这样我们的预测就变为：
- en: '![ˆy = 1 × 1 + 4.5 = 5.5 ](img/file24.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![ˆy = 1 × 1 + 4.5 = 5.5 ](img/file24.jpg)'
- en: 'We see that the expectation has increased due to the change in the underlying
    probability of the values of die *d*[1]. However, the important difference here
    is in the change in the variance value:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，期望值由于骰子*d*[1]的基础概率的变化而增加。然而，这里重要的区别在于方差值的变化：
- en: '![ ∑10 (a − μ)2 σ2 = --i=1--i----- = 3.25 n − 1 ](img/file25.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑10 (a − μ)2 σ2 = --i=1--i----- = 3.25 n − 1 ](img/file25.jpg)'
- en: 'Our variance has *increased*. As variance essentially gives us the average
    of the distance of each possible value from the mean, this shouldn’t be surprising:
    given the weighted die, it’s more likely that the outcome will be distant from
    the mean than with an unweighted die, and thus our variance increases. To summarize,
    in terms of uncertainty: the greater the likelihood that the outcome will be further
    from the mean, the greater the uncertainty.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方差已经*增加*。由于方差本质上给出了每个可能值与均值的距离的平均值，这并不令人惊讶：考虑到加权骰子，结果更有可能远离均值，而不像不加权骰子那样，因此我们的方差增加。总结来说，在不确定性方面：结果距离均值越远的可能性越大，不确定性也就越大。
- en: This has important implications for how we interpret predictions from machine
    learning models (and statistical models more generally). If our predictions are
    an approximation of the mean, and our uncertainty quantifies how likely it is
    for an outcome to be distant from the mean, then our uncertainty tells us **how
    likely it** **is that our model prediction is incorrect**. Thus, model uncertainties
    allow us to decide when to trust the predictions, and when we should be more cautious.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们如何解读机器学习模型（以及更广泛的统计模型）预测结果具有重要意义。如果我们的预测是均值的近似值，而我们的不确定性量化了某个结果偏离均值的可能性，那么我们的不确定性告诉我们**我们的模型预测错误的可能性有多大**。因此，模型的不确定性让我们能够决定何时信任预测，何时需要更加谨慎。
- en: 'The examples given here are very basic, but should help to give you an idea
    of what we’re looking to achieve with model uncertainty quantification. We will
    continue to explore these concepts as we learn about some of the benchmark methods
    for Bayesian inference, learning how these concepts apply to more complex, real-world
    problems. We’ll start with perhaps the most fundamental method of Bayesian inference:
    sampling.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里给出的例子非常基础，但应该能帮助你理解我们希望通过模型不确定性量化实现的目标。随着我们学习一些贝叶斯推断的基准方法，我们将继续探索这些概念，了解它们如何应用于更复杂的现实问题。我们将从可能是贝叶斯推断最基本的方法——采样开始。
- en: 2.2 Bayesian inference via sampling
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 通过采样进行贝叶斯推断
- en: In practical applications, it’s not possible to know exactly what a given outcome
    would be, and, similarly, it’s not possible to observe all possible outcomes.
    In these cases, we need to make a best estimate based on the evidence we have.
    The evidence is formed of **samples** – observations of possible outcomes. The
    aim of ML, broadly speaking, is to learn models that generalize well from a subset
    of data. The aim of Bayesian ML is to do so while also providing an estimate of
    the uncertainty associated with the model’s predictions. In this section, we’ll
    learn about how we can use sampling to do this, and will also learn why sampling
    may not be the most sensible approach.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，无法精确知道某个结果会是什么，类似地，也无法观察到所有可能的结果。在这种情况下，我们需要根据已有的证据做出最佳估计。证据由**样本**构成——即可能结果的观察。广义而言，机器学习的目标是学习能够从数据子集良好泛化的模型。贝叶斯机器学习的目标是在做到这一点的同时，还要提供一个模型预测的不确定性估计。在这一部分中，我们将学习如何利用采样来实现这一点，同时也会了解为什么采样可能不是最合理的方法。
- en: 2.2.1 Approximating distributions
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 近似分布
- en: At the most fundamental level, sampling is about approximating distributions.
    Say we want to know the distribution of the height of people in New York. We could
    go out and measure everyone, but that would involve measuring the height of 8.4
    million people! While this would give us our most accurate answer, it’s also a
    deeply impractical approach.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从最基础的层面来看，采样就是在进行分布的近似。假设我们想知道纽约人身高的分布。我们可以去测量每个人的身高，但那将涉及到测量840万人！虽然这会给我们最准确的答案，但这种方法在实践中是极为不切实际的。
- en: Instead, we can sample from the population. This gives us a basic example of
    **Monte Carlo sampling**, where we use random sampling to provide data from which
    we can approximate a distribution. For example, given a database of New York residents,
    we could select – at random – a sub-population of residents, and use this to approximate
    the height distribution of all residents. With random sampling – and any sampling,
    for that matter – the accuracy of the approximation is dependent on the size of
    the sub-population. What we’re looking to achieve is a **statistically significant**
    sub-sample, such that we can be confident in our approximation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以从总体中进行采样。这给我们提供了一个基本的**蒙特卡洛采样**的例子，在这个过程中，我们通过随机采样提供数据，从而近似一个分布。例如，假设我们有一个纽约居民的数据库，我们可以随机选择一部分居民，并用这些数据来近似所有居民的身高分布。通过随机采样——以及任何采样——这种近似的准确性取决于子总体的大小。我们希望实现的是一个**统计显著**的子样本，从而使我们对近似结果有信心。
- en: 'To get a better imdivssion of this, we’ll simulate the problem by generating
    100,000 data points from a truncated normal distribution, to approximate the kind
    of height distribution we may see for a population of 100,000 people. Say we draw
    10 samples, at random, from our population. Here’s what our distribution would
    look like (on the right) compared with the true distribution (on the left):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，我们将通过从截尾正态分布生成10万个数据点来模拟这个问题，以近似看到10万人口的身高分布。假设我们随机抽取10个样本。这里是我们的分布图（右侧）与真实分布图（左侧）的对比：
- en: '![PIC](img/file26.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file26.png)'
- en: 'Figure 2.3: Plot of true distribution (left) versus sample distribution (right)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：真实分布图（左）与样本分布图（右）的绘制。
- en: 'As we can see, this isn’t a great representation of the true distribution:
    what we see here is closer to a triangular distribution than a truncated normal.
    If we were to infer something about the population’s height based on this distribution
    alone, we’d arrive at a number of inaccurate conclusions, such as missing the
    truncation above 200 cm, and the tail on the left of the distribution.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们可以看出，这并不是真实分布的一个很好的表示：这里看到的更接近一个三角形分布而不是一个截尾正态分布。如果我们仅仅基于这个分布推断人口的身高，我们会得出许多不准确的结论，比如忽略了200厘米以上的截尾，以及分布左侧的尾部。
- en: 'We can get a better imdivssion by increasing our sample size – let’s try drawing
    100 samples:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加样本量，我们可以得到更好的估计 —— 让我们尝试抽取100个样本：
- en: '![PIC](img/file27.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file27.png)'
- en: 'Figure 2.4: Plot of true distribution (left) versus sample distribution (right).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：真实分布图（左）与样本分布图（右）的绘制。
- en: 'Things are starting to look better: we’re starting to see some of the tail
    on the left as well as the truncation toward 200 cm. However, this sample has
    sampled more from some regions than others, leading to misrepresentation: our
    mean has been pulled down, and we’re seeing two distinct peaks, rather than the
    single peak we see in the true distribution. Let’s increase our sample size by
    a further order of magnitude, scaling up to 1,000 samples:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 情况开始好转：我们开始看到左侧的一些尾部以及朝200厘米处的截尾。然而，这个样本从某些区域采样更多，导致了误代表：我们的平均数被拉低了，我们看到了两个明显的高峰，而不是真实分布中的单一高峰。让我们将样本量增加一个数量级，扩展到1,000个样本：
- en: '![PIC](img/file28.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file28.png)'
- en: 'Figure 2.5: Plot of true distribution (left) versus sample distribution (right)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：真实分布图（左）与样本分布图（右）的绘制。
- en: 'This is looking much better – with a sample set of only one hundredth the size
    of our true population, we now see a distribution that closely matches our true
    distribution. This example demonstrates how, through random sampling, we can approximate
    the true distribution using a significantly smaller pool of observations. But
    that pool still has to have enough information to allow us to arrive at a good
    approximation of the true distribution: too few samples and our subset will be
    statistically *insufficient*, leading to poor approximation of the underlying
    distribution.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看起来好多了 —— 通过一个比真实人口规模小一百倍的样本集，我们现在看到的分布与真实分布非常接近。这个例子展示了通过随机抽样，我们可以使用显著较小的观察池来近似真实分布。但是，这个池子仍然必须包含足够的信息，以使我们能够得出对真实分布良好近似的结论：样本太少，我们的子集将统计上不足，导致对潜在分布的近似不良。
- en: 'But simple random sampling isn’t the most practical method for approximating
    distributions. To achieve this, we turn to **probabilistic inference**. Given
    a model, probabilistic inference provides a way to find the model parameters that
    best describe our data. To do so, we need to first define the type of model –
    this is our prior. For our example, we’ll use a truncated Gaussian: the idea here
    being, using our intuition, it’s reasonable to assume people’s height follows
    a normal distribution, but that very few people are above, say, 6’5.” So, we’ll
    specify a truncated Gaussian distribution with an upper limit of 205 cm, or just
    over 6’5.” As it’s a Gaussian distribution, in other words, 𝒩(*μ,σ*), our model
    parameters are *𝜃* = {*μ,σ*} – with the additional constraint that our distribution
    has an upper limit of *b* = 205.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，简单随机采样并不是逼近分布的最实际方法。为了实现这一点，我们转向**概率推断**。给定一个模型，概率推断提供了一种方法来找到最能描述我们数据的模型参数。为此，我们需要首先定义模型的类型——这就是我们的先验。在我们的例子中，我们使用截断的高斯分布：基于我们的直觉，假设人类的身高遵循正态分布是合理的，但很少有人会超过6’5”
    （约196厘米）。因此，我们将指定一个截断的高斯分布，设定上限为205厘米，或者说稍超过6’5”。由于它是一个高斯分布，换句话说，𝒩(*μ,σ*)，我们的模型参数是*𝜃*
    = {*μ,σ*}——并且有额外的约束条件，即我们的分布的上限为*b* = 205。
- en: 'This brings us to a fundamental class of algorithms: **Markov Chain Monte**
    **Carlo**, or **MCMC** methods. Like simple random sampling, these allow us to
    build a picture of the true underlying distribution, but they do so sequentially,
    whereby each sample is dependent on the sample before it. This sequential dependence
    is known as the **Markov property**, thus the *Markov chain* component of the
    name. This sequential approach accounts for the probabilistic dependence between
    samples and allows us to better approximate the probability density.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们进入了一类基础的算法：**马尔科夫链蒙特卡罗**（**Markov Chain Monte Carlo**，或简称**MCMC**）方法。与简单随机采样一样，这些方法也允许我们构建真实底层分布的图像，但它们是顺序进行的，每个样本都依赖于之前的样本。这种顺序依赖性被称为**马尔科夫性质**，因此名字中的*马尔科夫链*部分。这个顺序方法考虑了样本之间的概率依赖性，使我们能够更好地逼近概率密度。
- en: 'MCMC achieves this through sequential random sampling. Just as with the random
    sampling we’re familiar with, MCMC randomly samples from our distribution. But,
    unlike simple random sampling, MCMC considers pairs of samples: some previous
    sample *x*[*t*−1] and some current sample *x*[*t*]. For each pair of samples,
    we have some criteria that specifies whether or not we keep the sample (this varies
    depending on the particular flavor of MCMC). If the new value meets this criteria,
    say if *x*[*t*] is ”preferential to” our previous value *x*[*t*−1], then the sample
    is added to the chain and becomes *x*[*t*] for the next round. If the sample doesn’t
    meet the criteria, we stick with the current *x*[*t*] for the next round. We repeat
    this over a (usually large) number of iterations, and in the end we should arrive
    at a good approximation of our distribution.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: MCMC通过顺序随机采样实现这一点。就像我们熟悉的随机采样一样，MCMC从我们的分布中随机采样。但与简单随机采样不同，MCMC考虑的是成对的样本：一些先前的样本
    *x*[*t*−1] 和一些当前的样本 *x*[*t*]。对于每一对样本，我们有一些标准来指定是否保留该样本（这个标准取决于具体的MCMC变体）。如果新值符合这个标准，比如说
    *x*[*t*] 比我们的先前值 *x*[*t*−1] “更优”，那么该样本会被加入到链中，并成为下一轮的 *x*[*t*]。如果样本不符合标准，我们则会保留当前的
    *x*[*t*] 作为下一轮的样本。我们会在（通常是很大的）迭代次数中重复这个过程，最终应该能够得到我们分布的良好近似。
- en: 'The result is an efficient sampling method that is able to closely approximate
    the true parameters of our distribution. Let’s see how this applies to our height
    distribution example. Using MCMC with just 10 samples, we arrive at the following
    approximation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个高效的采样方法，能够密切地逼近我们分布的真实参数。让我们看看它是如何应用于我们的身高分布示例的。使用仅有10个样本的MCMC，我们得到了以下的近似结果：
- en: '![PIC](img/file29.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file29.png)'
- en: 'Figure 2.6: Plot of true distribution (left) versus approximate distribution
    via MCMC (right)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：真实分布（左）与通过MCMC得到的近似分布（右）的对比图
- en: 'Not bad for ten samples – certainly far better than the triangular distribution
    we arrived at with simple random sampling. Let’s see how we do with 100:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 十个样本的表现不错——显然比我们通过简单随机采样得到的三角分布要好得多。让我们看看使用100个样本的效果：
- en: '![PIC](img/file30.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file30.png)'
- en: 'Figure 2.7: Plot of true distribution (left) versus approximate distribution
    via MCMC (right)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：真实分布（左）与通过MCMC得到的近似分布（右）的对比图
- en: 'This is looking pretty excellent – in fact, we’re able to obtain a better approximation
    of our distribution with 100 MCMC samples than we are with 1,000 simple random
    samples. If we continue to larger numbers of samples, we’ll arrive at closer and
    closer approximations of our true distribution. But our simple example doesn’t
    fully capture the power of MCMC: MCMC’s true advantage comes from being able to
    approximate high-dimensional distributions, and has made it an invaluable technique
    for approximating intractable high-dimensional integrals in a variety of domains.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果看起来相当不错——实际上，通过使用100个MCMC样本，我们能够获得比使用1,000个简单随机样本更好的分布近似。如果我们继续增加样本数量，我们将得到越来越接近真实分布的近似值。但我们的简单例子并没有完全展现MCMC的强大：MCMC的真正优势在于能够近似高维分布，这使得它成为在各种领域中近似难以求解的高维积分的宝贵技术。
- en: In this book, we’re interested in how we can estimate the probability distribution
    of the parameters of machine learning models – this allows us to estimate the
    uncertainty associated with our predictions. In the next section, we’ll take a
    look at how we do this practically by applying sampling to Bayesian linear regression.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的重点是如何估计机器学习模型参数的概率分布——这使我们能够估计与预测相关的不确定性。在下一节中，我们将实际演示如何通过将采样应用于贝叶斯线性回归来完成这一任务。
- en: 2.2.2 Implementing probabilistic inference with Bayesian linear regression
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 使用贝叶斯线性回归实现概率推断
- en: 'In typical linear regression, we want to predict some output *ŷ* from some
    input *x* using a linear function *f*(*x*), such that *ŷ* = *βx* + *ξ*. With Bayesian
    linear regression, we do this probabilistically, introducing another parameter,
    *σ*², such that our regression equation becomes:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的线性回归中，我们希望使用线性函数*f*(*x*)从某个输入*x*预测输出*ŷ*，使得*ŷ* = *βx* + *ξ*。在贝叶斯线性回归中，我们以概率的方式进行预测，引入了另一个参数*σ*²，使得我们的回归方程变为：
- en: '![ˆy = 𝒩 (x β + ξ,σ2 ) ](img/file31.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![ŷ = 𝒩 (x β + ξ,σ² ) ](img/file31.jpg)'
- en: That is, *ŷ* follows a Gaussian distribution.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，*ŷ*服从高斯分布。
- en: 'Here, we see our familiar bias term *ξ* and intercept *β*, and introduce a
    variance parameter *σ*². To fit our model, we need to define a prior over these
    parameters – just as we did for our MCMC example in the last section. We’ll define
    these priors as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到了我们熟悉的偏置项*ξ*和截距*β*，并引入了方差参数*σ*²。为了拟合我们的模型，我们需要为这些参数定义先验——就像我们在上一节中的MCMC示例一样。我们将这些先验定义为：
- en: '![ξ ≈ 𝒩 (0,1 ) ](img/file32.jpg)![β ≈ 𝒩 (0,1) ](img/file33.jpg)![σ2 ≈ |𝒩 (0,1)|
    ](img/file34.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![ξ ≈ 𝒩 (0,1 ) ](img/file32.jpg)![β ≈ 𝒩 (0,1) ](img/file33.jpg)![σ² ≈ |𝒩 (0,1)|
    ](img/file34.jpg)'
- en: 'Note that equation 2.15 denotes the half-normal of a Gaussian distribution
    (the positive half of a zero-mean Gaussian, as standard deviation cannot be negative).
    We’ll refer to our model parameters as *𝜃* = *β,ξ,σ*², and we’ll use sampling
    to find the parameters that maximise the likelihood of these given our data, in
    other words, the conditional probability of our parameters given our data *D*:
    *P*(*𝜃*|*D*).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，方程2.15表示的是高斯分布的半正态分布（零均值高斯的正半部分，因为标准差不能为负）。我们将模型参数表示为*𝜃* = *β,ξ,σ*²，并使用采样来找到最大化给定数据下这些参数似然的参数，换句话说，就是在给定数据*D*的条件下，我们参数的条件概率*P*(*𝜃*|*D*)。
- en: There are a variety of MCMC sampling approaches we could use to find our model
    parameters. A common approach is to use the **Metropolis-Hastings** algorithm.
    Metropolis-Hastings is particularly useful for sampling from intractable distributions.
    It does so through the use of a proposal distribution, *Q*(*𝜃*′|*𝜃*), which is
    proportional to, but not exactly equal to, our true distribution. This means that,
    for example, if some value *x*[1] is twice as likely as some other value *x*[2]
    in our true distribution, this will be true of our proposal distribution too.
    Because we’re interested in the probability of observations, we don’t need to
    know what the *exact* value would be in our true distribution – we just need to
    know that, proportionally, our proposal distribution is equivalent to our true
    distribution.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种MCMC采样方法可以用来寻找我们的模型参数。一种常见的方法是使用**Metropolis-Hastings**算法。Metropolis-Hastings特别适合从难以求解的分布中进行采样。它通过使用一个提议分布*Q*(*𝜃*′|*𝜃*)来实现这一点，该分布与我们的真实分布成比例，但并不完全相同。这意味着，例如，如果某个值*x*[1]在真实分布中是另一个值*x*[2]的两倍可能性，那么在我们的提议分布中也会是如此。由于我们关心的是观察结果的概率，我们不需要知道在真实分布中的*精确*值——我们只需要知道我们的提议分布在比例上与真实分布等价。
- en: Here are the key steps of Metropolis-Hastings for our Bayesian linear regression.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们贝叶斯线性回归中 Metropolis-Hastings 的关键步骤。
- en: 'First, we initialize with an arbitrary point *𝜃* sampled from our parameter
    space, according to the priors for each of our parameters. Using a Gaussian distribution
    centered on our first set of parameters *𝜃*, select a new point *𝜃*′. Then, for
    each iteration *t* ∈ *T*, do the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从根据每个参数的先验分布在参数空间中随机选择一个点 *𝜃* 作为初始化。然后，使用以我们的第一个参数集 *𝜃* 为中心的高斯分布，选择一个新的点
    *𝜃*′。接着，对于每次迭代 *t* ∈ *T*，执行以下操作：
- en: 'Calculate the acceptance criteria, defined as:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算接受准则，定义为：
- en: '![ P(𝜃′|D ) α = -------- P(𝜃|D ) ](img/file35.jpg)'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![ P(𝜃′|D ) α = -------- P(𝜃|D ) ](img/file35.jpg)'
- en: Generate a random number from a uniform distribution *𝜖* ∈ [0*,*1]. If *𝜖 <*=
    *α*, accept the new candidate parameters – adding these to the chain, assigning
    *𝜃* = *𝜃*′. If *𝜖 > α*, keep the current *𝜃* and draw a new value.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从均匀分布 *𝜖* ∈ [0*,*1] 中生成一个随机数。如果 *𝜖 <* = *α*，则接受新的候选参数——将这些加入到链中，设 *𝜃* = *𝜃*′。如果
    *𝜖 > α*，则保持当前的 *𝜃* 并重新抽取一个新值。
- en: This acceptance criteria means that, if our new set of parameters have a higher
    likelihood than our last set of parameters, we’ll see *α >* 1, in which case *α
    < 𝜖*. This means that, when we sample parameters that are *more likely* given
    our data, we’ll always accept these parameters. If, on the other hand, *α <* 1,
    there’s a chance we’ll reject the parameters, but we may also accept them – allowing
    us to explore regions of lower likelihood.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这个接受准则意味着，如果我们的新参数集比上一个参数集具有更高的似然性，我们会看到 *α >* 1，在这种情况下 *α < 𝜖*。这意味着，当我们根据数据采样得到的参数更“可能”时，我们总是会接受这些参数。另一方面，如果
    *α <* 1，虽然有可能会拒绝这些参数，但我们也可能会接受它们——允许我们探索低似然区域。
- en: These mechanics of Metropolis-Hastings result in samples that can be used to
    compute high-quality approximations of our posterior distribution. Practically,
    Metropolis-Hastings (and MCMC methods more generally) requires a burn-in phase
    – an initial phase of sampling used to escape regions of low density, which are
    typically encountered given the arbitrary initialization.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Metropolis-Hastings 的这些机制产生的样本可用于计算我们后验分布的高质量近似值。在实际操作中，Metropolis-Hastings（以及更一般的
    MCMC 方法）需要一个预热阶段——一个初步采样阶段，用于逃离低密度区域，这些区域通常在给定任意初始化时会遇到。
- en: 'Let’s apply this to a simple problem: we’ll generate some data for the function
    *y* = *x*² + 5 + *η*, where *η* is a noise parameter distributed according to
    *η* ≈𝒩(0*,*5). Using Metropolis-Hastings to fit our Bayesian linear regressor,
    we get the following fit using the points sampled from our function (represented
    by the crosses):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其应用于一个简单的问题：我们将为函数 *y* = *x*² + 5 + *η* 生成一些数据，其中 *η* 是一个根据 *η* ≈𝒩(0*,*5)
    分布的噪声参数。使用 Metropolis-Hastings 来拟合我们的贝叶斯线性回归器，得到以下拟合结果，该拟合使用从我们的函数中采样的点（通过交叉点表示）：
- en: '![PIC](img/file36.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file36.png)'
- en: 'Figure 2.8: Bayesian linear regression on generated data with low variance'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：在低方差生成数据上的贝叶斯线性回归
- en: 'We see that our model fits the data in the same way we would expect for standard
    linear regression. However, unlike standard linear regression, our model produces
    predictive uncertainty: this is represented by the shaded region. This predictive
    uncertainty gives an imdivssion of how much our underlying data varies; this makes
    this model much more useful than a standard linear regression, as now we can get
    an imdivssion of the sdivad of our data, as well as the general trend. We can
    see how this varies if we generate new data and fit again, this time increasing
    the sdivad of the data by modifying our noise distribution to *η* ≈𝒩(0*,*20):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到我们的模型以标准线性回归预期的方式拟合数据。然而，与标准线性回归不同，我们的模型产生了预测不确定性：这通过阴影区域表示。这种预测不确定性提供了关于我们基础数据变化程度的一个近似；这使得这个模型比标准线性回归更有用，因为我们现在不仅可以得到数据的趋势，还能获取数据的变化幅度。如果我们生成新数据并重新拟合，增加数据的标准差，可以通过修改噪声分布为
    *η* ≈𝒩(0*,*20) 来实现：
- en: '![PIC](img/file37.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file37.png)'
- en: 'Figure 2.9: Bayesian linear regression on generated data with high variance'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9：在高方差生成数据上的贝叶斯线性回归
- en: 'We see that our predictive uncertainty has increased proportionally to the
    sdivad of the data. This is an important property in uncertainty-aware methods:
    when we have small uncertainty, we know our prediction fits the data well, whereas
    when we have large uncertainty, we know to treat our prediction with caution,
    as it indicates the model isn’t fitting this region particularly well. We’ll see
    a better example of this in the next section, which will go on to demonstrate
    how regions of more or less data contribute to our model uncertainty estimates.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，预测的不确定性与数据的标准差成正比增加。这是不确定性感知方法中的一个重要特性：当不确定性较小时，我们知道预测与数据拟合得很好；而当不确定性较大时，我们知道需要谨慎对待预测，因为这表明模型在该区域的拟合效果较差。我们将在下一节中看到一个更好的例子，展示数据更多或更少的区域如何影响我们模型的不确定性估计。
- en: Here, we see that our predictions fit our data pretty well. In addition, we
    see that *σ*² varies according to the availability of data in different regions.
    What we’re seeing here is a great example of a very important concept, **well
    calibrated uncertainty** – also termed **high-quality** **uncertainty**. This
    refers to the fact that, in regions where our Predictions are inaccurate, our
    uncertainty is also high. Our uncertainty estimates are **poorly calibrated**
    if we’re very confident in regions with inaccurate predictions, or very uncertain
    in regions with accurate predictions. As it’s well-calibrated, sampling is often
    used as a benchmark for uncertainty quantification.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到我们的预测与数据拟合得相当好。此外，我们还看到*σ*²根据不同区域中数据的可用性而变化。我们在这里看到的是一个非常重要的概念的极好例子，**良好校准的不确定性**——也称为**高质量**
    **不确定性**。这意味着，在预测不准确的区域，我们的不确定性也很高。如果我们在预测不准确的区域过于自信，或者在预测准确的区域过于不确定，我们的不确定性估计就是**校准不良**的。由于其良好的校准性，采样常常被作为不确定性量化的基准。
- en: Unfortunately, while sampling is effective for many applications, the need to
    obtain many samples for each parameter means that it quickly becomes computationally
    prohibitive for high dimensions of parameters. For example, if we wanted to start
    sampling parameters for complex, non-linear relationships (such as sampling the
    weights of a neural network), sampling would no longer be practical. Despite this,
    it’s still useful in some cases, and later we’ll see how various BDL methods make
    use of sampling.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，尽管采样对于许多应用非常有效，但每个参数需要获得多个样本，这意味着当参数维度很高时，采样很快变得在计算上无法承受。例如，如果我们想为复杂的非线性关系（例如神经网络权重的采样）开始进行采样，那么采样就不再具有实用性。尽管如此，它在某些情况下仍然有用，稍后我们将看到各种BDL方法如何利用采样。
- en: In the next section, we’ll explore the Gaussian process – another fundamental
    method for Bayesian inference, and a method that does not suffer from the same
    computational overheads as sampling.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨高斯过程——一种贝叶斯推断的基本方法，并且是一种不像采样方法那样遭受计算开销的技术。
- en: 2.3 Exploring the Gaussian process
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 探索高斯过程
- en: As we’ve seen in the previous section, sampling quickly becomes prohibitively
    expensive. To address this, we can use ML models specifically designed to produce
    uncertainty estimates – the gold standard of which is the **Gaussian** **process**.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中所看到的，采样很快会变得非常昂贵。为了解决这个问题，我们可以使用专门设计的机器学习模型来产生不确定性估计——其中最为经典的就是**高斯**
    **过程**。
- en: The Gaussian process, or **GP**, has become a staple probabilistic ML model,
    seeing use in a broad variety of applications from pharmacology through to robotics.
    Its success is largely down to its ability to produce high-quality uncertainty
    estimates over its predictions in a well-principled fashion. So, what do we mean
    by a Gaussian process?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程，或称**GP**，已经成为一种基础的概率论机器学习模型，广泛应用于从药理学到机器人学等多个领域。它的成功主要归功于其能够以一种科学的方式生成对预测结果的高质量不确定性估计。那么，什么是高斯过程呢？
- en: 'In essence, a GP is a distribution over functions. To understand what we mean
    by this, let’s take a typical ML use case. We want to learn some function *f*(**x**),
    which maps a series of inputs **x** onto a series of outputs **y**, such that
    we can approximate our output via **y** = *f*(**x**). Before we see any data,
    we know nothing about our underlying function; there is an infinite number of
    possible functions this could be:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，GP 是一个函数的分布。为了理解我们所说的这一点，我们来看一个典型的机器学习用例。我们希望学习某个函数 *f*(**x**)，它将一系列输入 **x**
    映射到一系列输出 **y**，使得我们能够通过 **y** = *f*(**x**) 来近似我们的输出。在我们看到任何数据之前，我们对潜在的函数一无所知；它可能是无限多种可能的函数之一：
- en: '![PIC](img/file38.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file38.png)'
- en: 'Figure 2.10: Illustration of space of possible functions before seeing data'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10：在看到数据之前，可能函数空间的示意图
- en: 'Here, the black line is the true function we wish to learn, while the dotted
    lines are the possible functions given the data (in this case, no data). Once
    we observe some data, we see that the number of possible functions becomes more
    constrained, as we see here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，黑线表示我们希望学习的真实函数，而虚线表示在给定数据（在这种情况下，没有数据）下可能的函数。一旦我们观察到一些数据，就会发现可能函数的数量变得更加有限，如下所示：
- en: '![PIC](img/file39.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file39.png)'
- en: 'Figure 2.11: Illustration of space of possible functions after seeing some
    data'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11：在观察到一些数据后，可能函数空间的示意图
- en: 'Here, we see that our possible functions all pass through our observed data
    points, but outside of those data points, our functions take on a range of very
    different values. In a simple linear model, we don’t care about these deviations
    in possible values: we’re happy to interpolate from one data point to another,
    as we see in *Figure* [2.12](#x1-30007r12):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到所有可能的函数都经过我们观察到的数据点，但在这些数据点之外，我们的函数会取一系列非常不同的值。在一个简单的线性模型中，我们不关心这些可能值的偏差：我们乐于在一个数据点和另一个数据点之间进行插值，正如我们在*图*
    [2.12](#x1-30007r12)中看到的那样：
- en: '![PIC](img/file40.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file40.png)'
- en: 'Figure 2.12: Illustration of linearly interpolating through our observations'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12：通过我们的观测值进行线性插值的示意图
- en: But this interpolation can lead to wildly inaccurate predictions, and has no
    way of accounting for the degree of uncertainty associated with our model predictions.
    The deviations that we see here in the regions without data points are exactly
    what we want to capture with our GP. When there are a variety of possible values
    our function can take, then there is uncertainty – and through capturing the degree
    of uncertainty, we are able to estimate what the possible variation in these regions
    may be.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这种插值可能导致非常不准确的预测，并且无法考虑与模型预测相关的不确定性程度。我们在没有数据点的区域看到的偏差，正是我们希望通过高斯过程捕捉的内容。当我们的函数可以取不同的可能值时，就会有不确定性——通过捕捉这种不确定性的程度，我们能够估计这些区域内可能的变化。
- en: 'Formally, a GP can be defined as a function:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，高斯过程可以定义为一个函数：
- en: '![f(x) ≈ GP (m (x),k(x,x′)) ](img/file41.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![f(x) ≈ GP (m (x),k(x,x′)) ](img/file41.jpg)'
- en: 'Here, *m*(**x**) is simply the mean of our possible function values for a given
    point **x**:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*m*(**x**) 仅仅是给定点 **x** 上我们可能函数值的均值：
- en: '![m (x) = 𝔼[f (x)] ](img/file42.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![m (x) = 𝔼[f (x)] ](img/file42.jpg)'
- en: The next term, *k*(**x***,***x**′) is a covariance function, or kernel. This
    is a fundamental component of the GP as it defines the way we model the relationship
    between different points in our data. GPs use the mean and covariance functions
    to model the space of possible functions, and thus to produce predictions as well
    as their associated uncertainties. Now that we’ve introduced some of the high-level
    concepts, let’s dig a little deeper and understand exactly how it is they model
    the space of possible functions, and thus estimate uncertainty. To do this, we
    need to understand GP priors.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个项，*k*(**x***,***x**′) 是协方差函数，或称为核函数。这是高斯过程（GP）的一个基本组成部分，因为它定义了我们如何建模数据中不同点之间的关系。高斯过程使用均值和协方差函数来建模可能函数的空间，从而生成预测及其相关的不确定性。现在我们已经介绍了一些高层次的概念，接下来让我们更深入地理解它们如何建模可能函数的空间，并估计不确定性。为此，我们需要理解高斯过程的先验。
- en: 2.3.1 Defining our prior beliefs with kernels
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 使用核函数定义我们的先验信念
- en: GP kernels describe the prior beliefs we have about our data, and so you’ll
    often see them referred to as GP priors. In the same way that the prior in equation
    2.3 tells us something about the probability of the outcome of our two dice rolls,
    the GP prior tells us something important about the relationship we expect from
    our data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程的核函数描述了我们对数据的先验信念，因此你会经常看到它们被称为高斯过程先验。就像方程 2.3 中的先验告诉我们关于两个骰子投掷结果的概率一样，高斯过程先验告诉我们有关我们期望从数据中得到的关系的重要信息。
- en: While there are advanced methods for inferring a prior from our data, they are
    beyond the scope of this book. We will instead focus on more traditional uses
    of GPs, for which we select a prior using our knowledge of the data we’re working
    with.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有一些高级方法可以根据我们的数据推断先验，但它们超出了本书的范围。我们将专注于高斯过程的更传统用法，其中我们使用对所处理数据的了解来选择先验。
- en: In the literature and any implementations you encounter, you’ll see that the
    GP prior is often referred to as the **kernel** or **covariance function** (just
    as we have here). These three terms are all interchangeable, but for consistency
    with other work, we will henceforth refer to this as the kernel. Kernels simply
    provide a means of calculating a distance between two data points, and are exdivssed
    as *k*(*x,x*′), where *x* and *x*′ are data points, and *k*() represents the function
    of the kernel. While the kernel can take on many forms, there are a small number
    of fundamental kernels that are used in a large proportion of GP applications.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中以及你遇到的任何实现中，你会发现高斯过程先验常常被称为**核函数**或**协方差函数**（就像我们在这里一样）。这三个术语是可以互换的，但为了与其他工作的术语一致，今后我们将称之为核函数。核函数提供了一种计算两个数据点之间距离的方法，通常表示为
    *k*(*x*, *x*′)，其中 *x* 和 *x*′ 是数据点，而 *k*() 表示核函数。虽然核函数可以有多种形式，但有少数几种基本核函数在大量高斯过程应用中被广泛使用。
- en: 'Perhaps the most commonly encountered kernel is the **squared exponential**
    or **radial basis function** (**RBF**) kernel. This kernel takes the form:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 也许最常见的核函数是**平方指数**或**径向基函数**（**RBF**）核函数。这个核函数的形式是：
- en: '![ (x − x ′)2 k(x,x ′) = σ2exp − ----2---- 2l ](img/file43.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![ (x − x ′)² k(x,x ′) = σ²exp − ----2---- 2l ](img/file43.jpg)'
- en: 'This introduces us to a couple of common kernel parameters: *l* and *σ*². The
    output variance parameter *σ*² is simply a scaling factor, used to control the
    distance of the function from its mean. The length scale parameter *l* controls
    the smoothness of the function – in other words, how much your function is expected
    to vary across particular dimensions. This parameter can either be a scalar that
    is applied to all input dimensions, or a vector with a different scalar value
    for each input dimension. The latter is often achieved using **Automatic Relevance**
    **Determination**, or **ARD**, which identifies the relevant values in the input
    space.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这引入了几个常见的核参数：*l* 和 *σ*²。输出方差参数 *σ*² 只是一个缩放因子，用于控制函数与其均值之间的距离。长度尺度参数 *l* 控制函数的平滑度——换句话说，控制函数在特定维度上的变化幅度。这个参数可以是一个标量，应用于所有输入维度，或者是一个向量，每个输入维度有不同的标量值。后者通常是通过**自动相关性**
    **确定**（**ARD**）实现的，该方法识别输入空间中的相关值。
- en: GPs make predictions via a covariance matrix based on the kernel – essentially
    comparing a new data point to previously observed data points. However, just as
    with all ML models, GPs need to be trained, and this is where the length scale
    comes in. The length scale forms the parameters of our GP, and through the training
    process it learns the optimal value(s) for the length scale(s). This is typically
    done using a nonlinear optimizer, such as the **Broyden-Fletcher-Goldfarb-Shanno**
    (**BFGS**) optimizer. Many optimizers can be used, including optimizers you may
    be familiar with for deep learning, such as stochastic gradient descent and its
    variants.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程通过基于核函数的协方差矩阵进行预测——本质上是将新数据点与之前观察到的数据点进行比较。然而，就像所有机器学习模型一样，高斯过程也需要训练，这就是长度尺度发挥作用的地方。长度尺度构成了我们高斯过程的参数，通过训练过程，它学习长度尺度的最佳值。这通常通过使用非线性优化器来完成，例如**Broyden-Fletcher-Goldfarb-Shanno**（**BFGS**）优化器。可以使用多种优化器，包括你可能熟悉的深度学习优化器，例如随机梯度下降及其变种。
- en: 'Let’s take a look at how different kernels affect GP predictions. We’ll start
    with a straightforward example – a simple sine wave:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看不同的核函数如何影响高斯过程预测。我们从一个简单的示例开始——一个简单的正弦波：
- en: '![PIC](img/file44.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file44.png)'
- en: 'Figure 2.13: Plot of sine wave with four sampled points'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.13：带有四个采样点的正弦波图
- en: 'We can see the function illustrated here, as well as some points sampled from
    this function. Now, let’s fit a GP with a periodic kernel to the data. The periodic
    kernel is defined as:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这里展示的函数，以及从这个函数中采样的一些点。现在，让我们为数据拟合一个具有周期性核的高斯过程。周期性核函数定义为：
- en: '![ ′ 2 ( 2sin2(π |x − x′|∕p)) kper(x, x) = σ exp -------l2-------- ](img/file45.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![ ′ 2 ( 2sin²(π |x − x′|∕p)) kper(x, x) = σ exp -------l²-------- ](img/file45.jpg)'
- en: 'Here, we see a new parameter: *p*. This is simply the period of the periodic
    function. Setting *p* = 1 and applying a GP with a periodic kernel to the preceding
    example, we get the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到一个新的参数：*p*。这只是周期函数的周期。设定 *p* = 1，并将具有周期性核的高斯过程应用到前面的示例中，我们得到如下结果：
- en: '![PIC](img/file46.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file46.png)'
- en: 'Figure 2.14: Plot of posterior predictions from a periodic kernel with *p*
    = 1'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.14：周期性核函数预测结果图，*p* = 1
- en: 'This looks pretty noisy, but you should be able to see that there is clear
    periodicity in the functions produced by the posterior. It’s noisy for a couple
    of reasons: a lack of data, and a poor prior. If we’re limited on data, we can
    try to fix the problem by improving our prior. In this case, we can use our knowledge
    of the periodicity of the function to improve our prior by setting *p* = 6:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来相当嘈杂，但你应该能够看到后验生成的函数中存在明显的周期性。它之所以嘈杂，原因有几个：数据不足和先验较差。如果数据有限，我们可以尝试通过改善先验来解决这个问题。在这种情况下，我们可以利用我们对函数周期性的知识，通过设置*p*
    = 6来改进我们的先验：
- en: '![PIC](img/file47.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file47.png)'
- en: 'Figure 2.15: Plot of posterior predictions from a periodic kernel with *p*
    = 6'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15：周期性核函数的后验预测图，*p* = 6
- en: 'We see that this fits the data pretty well: we’re still uncertain in regions
    for which we have little data, but the periodicity of our posterior now looks
    sensible. This is possible because we’re using an informative prior; that is,
    a prior that incorporates information that describes the data well. This prior
    is composed of two key components:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到这与数据拟合得相当好：我们仍然在数据稀缺的区域存在不确定性，但后验的周期性现在看起来合理。这之所以能实现，是因为我们使用了一个信息丰富的先验；即，包含了描述数据的关键信息的先验。这个先验由两个关键部分组成：
- en: Our periodic kernel
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的周期性核函数
- en: Our knowledge about the periodicity of the function
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们关于该函数周期性的知识
- en: 'We can see how important this is if we modify our GP to use an RBF kernel:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将高斯过程修改为使用RBF核函数，就能清楚看到这个差异：
- en: '![PIC](img/file48.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file48.png)'
- en: 'Figure 2.16: Plot of posterior predictions from an RBF kernel'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16：RBF核函数的后验预测图
- en: 'With an RBF kernel, we see that things are looking pretty chaotic again: because
    we have limited data and a poor prior, we’re unable to appropriately constrain
    the space of possible functions to fit our true function. In the ideal case, we’d
    fix this by using a more appropriate prior, as we saw in *Figure* [*2.15*](#x1-31013r15)
    – but this isn’t always possible. Another solution is to sample more data. Sticking
    with our RBF kernel, we sample 10 data points from our function and re-train our
    GP:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RBF核函数时，我们看到情况又变得相当混乱：由于我们只有有限的数据和较差的先验，我们无法恰当地约束可能的函数空间以拟合我们的真实函数。在理想情况下，我们可以通过使用更合适的先验来解决这一问题，就像在*图*
    [*2.15*](#x1-31013r15)中看到的那样——但这并不总是可行的。另一种解决方案是增加数据量。继续使用我们的RBF核函数，我们从函数中采样了10个数据点并重新训练了我们的高斯过程（GP）：
- en: '![PIC](img/file49.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file49.png)'
- en: 'Figure 2.17: Plot of posterior predictions from an RBF kernel, trained on 10
    observations'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17：基于10个观测值训练的RBF核函数的后验预测图
- en: This is looking much better – but what if we have more data *and* an informative
    prior?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来好多了——但如果我们有更多数据*并且*一个信息丰富的先验呢？
- en: '![PIC](img/file50.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file50.png)'
- en: 'Figure 2.18: Plot of posterior predictions from a periodic kernel with *p*
    = 6, trained on 10 observations'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18：周期性核函数的后验预测图，*p* = 6，基于10个观测值训练
- en: The posterior now fits our true function very closely. Because we don’t have
    infinite data, there are still some areas of uncertainty, but the uncertainty
    is relatively small.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在后验预测非常接近真实函数。因为我们没有无限的数据，仍然存在一些不确定区域，但不确定性相对较小。
- en: 'Now that we’ve seen some of the core principles in action, let’s return to
    our example from *Figures* [*2.10*](#x1-30002r10)*-*[*2.12*](#x1-30007r12). Here’s
    a quick reminder of our target function, our posterior samples, and the linear
    interpolation we saw earlier:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到一些核心原理的实际应用，让我们回到*图* [*2.10*](#x1-30002r10)-[*2.12*](#x1-30007r12)中的例子。这里是我们目标函数、后验样本和之前看到的线性插值的快速回顾：
- en: '![PIC](img/file51.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file51.png)'
- en: 'Figure 2.19: Plot illustrating the difference between linear interpolation
    and the true function'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19：展示线性插值与真实函数差异的图
- en: 'Now that we’ve got some idea of how a GP will affect our predictive posterior,
    it’s easy to see that linear interpolation falls very short of what we achieve
    with a GP. To illustrate this more clearly, let’s take a look at what the GP prediction
    would be for this function given the three samples:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对高斯过程（GP）如何影响预测后验有了一些了解，很容易看出线性插值远远达不到我们通过高斯过程实现的效果。为了更清楚地说明这一点，让我们看看在给定三个样本的情况下，高斯过程预测这个函数的结果：
- en: '![PIC](img/file52.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file52.png)'
- en: 'Figure 2.20: Plot illustrating the difference between GP predictions and the
    true function'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.20：展示高斯过程预测与真实函数差异的图
- en: 'Here, the dotted lines are our mean (*μ*) predictions from the GP, and the
    shaded area is the uncertainty associated with those predictions – the standard
    deviation (*σ*) around the mean. Let’s contrast what we see in *Figure* [*2.20*](#x1-31026r20)
    with *Figure* [*2.19*](#x1-31024r19). The differences may seem subtle at first,
    but we can clearly see that this is no longer a straightforward linear interpolation:
    the predicted values from the GP are being ”pulled” toward our actual function
    values. As with our earlier sine wave examples, the behavior of the GP predictions
    are affected by two key factors: the prior (or kernel) and the data.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，虚线表示我们高斯过程（GP）的均值 (*μ*) 预测值，阴影区域表示与这些预测值相关的不确定性——即均值周围的标准差 (*σ*)。让我们将 *图*
    [*2.20*](#x1-31026r20) 中看到的内容与 *图* [*2.19*](#x1-31024r19) 进行对比。差异一开始可能看起来很微妙，但我们可以清楚地看到，这不再是一个简单的线性插值：高斯过程的预测值正在“拉”向我们的实际函数值。就像我们之前的正弦波示例一样，高斯过程预测的行为受两大关键因素的影响：先验（或核函数）和数据。
- en: 'But there’s another crucial detail illustrated in *Figure* [*2.20*](#x1-31026r20):
    the predictive uncertainties from our GP. We see that, unlike many typical ML
    models, a GP gives us uncertainties associated with its predictions. This means
    we can make better decisions about what we do with the model’s predictions – having
    this information will help us to ensure that our systems are more robust. For
    example, if the uncertainty is too great, we can fall back to a manual system.
    We can even keep track of data points with high predictive uncertainty so that
    we can continuously refine our models.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 但在 *图* [*2.20*](#x1-31026r20) 中还展示了另一个至关重要的细节：我们高斯过程（GP）的预测不确定性。我们可以看到，与许多典型的机器学习模型不同，高斯过程会给出与其预测相关的不确定性。这意味着我们可以更好地决定如何使用模型的预测——拥有这些信息将帮助我们确保我们的系统更为稳健。例如，如果不确定性太大，我们可以回退到手动系统。我们甚至可以追踪那些具有较高预测不确定性的数据点，以便不断改进我们的模型。
- en: 'We can see how this refinement affects our predictions by adding a few more
    observations – just as we did in the earlier examples:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过增加一些观测点来看到这种改进如何影响我们的预测——就像我们在之前的示例中做的那样：
- en: '![PIC](img/file53.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file53.png)'
- en: 'Figure 2.21: Plot illustrating the difference between GP predictions and the
    true function, trained on 5 observations'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.21：展示在 5 个观测点上训练的高斯过程（GP）预测与真实函数之间差异的图表
- en: '*Figure* [*2.21*](#x1-31029r21) illustrates how our uncertainty changes over
    regions with different numbers of observations. We see here that between *x* =
    3 and *x* = 4 our uncertainty is quite high. This makes a lot of sense, as we
    can also see that our GP’s mean predictions deviate significantly from the true
    function values. Conversely, if we look at the region between *x* = 0*.*5 and
    *x* = 2, we can see that our GP’s predictions follow the true function fairly
    closely, and our model is also more confident about these predictions, as we can
    see from the smaller interval of uncertainty in this region.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*图* [*2.21*](#x1-31029r21) 展示了我们的不确定性如何在不同数量观测的区域之间变化。我们可以看到，在 *x* = 3 和 *x*
    = 4 之间，我们的不确定性相当高。这很有道理，因为我们也可以看到，我们的高斯过程（GP）的均值预测值与真实函数值存在较大偏差。相反，如果我们查看 *x*
    = 0.5 和 *x* = 2 之间的区域，我们可以看到我们的 GP 预测值与真实函数非常接近，并且我们对这些预测也更有信心，因为我们可以从这个区域的不确定性间隔较小中看出。'
- en: 'What we’re seeing here is a great example of a very important concept: **well**
    **calibrated uncertainty** – also termed **high-quality uncertainty**. This refers
    to the fact that, in regions where our predictions are inaccurate, our uncertainty
    is also high. Our uncertainty estimates are **poorly calibrated** if we’re very
    confident in regions with inaccurate predictions, or very uncertain in regions
    with accurate predictions.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到的是一个非常重要概念的典范：**良好** **校准的不确定性**——也称为 **高质量的不确定性**。这指的是在我们预测不准确的区域，我们的不确定性也很高。如果我们在预测不准确的区域非常自信，或在预测准确的区域非常不确定，那么我们的不确定性估计就是
    **校准不良的**。
- en: 'GPs are what we can term a **well principled** method – this means that they
    have solid mathematical foundations, and thus come with strong theoretical guarantees.
    One of these guarantees is that they are well calibrated, and this is what makes
    GPs so popular: if we use GPs, we know we can rely on their uncertainty estimates.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯过程（GP）是一种我们可以称之为 **原理明确** 的方法——这意味着它们有坚实的数学基础，因此具有强大的理论保障。其中之一是它们具有良好的校准性，这也是高斯过程如此受欢迎的原因：如果我们使用高斯过程，我们知道可以依赖它们的不确定性估计。
- en: Unfortunately, however, GPs are not without their shortcomings – we’ll learn
    more about these in the following section.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，GP并非没有缺点——我们将在接下来的章节中详细了解这些问题。
- en: 2.3.2 Limitations of Gaussian processes
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 高斯过程的局限性
- en: 'Given the fact that GPs are well-principled and capable of producing high-quality
    uncertainty estimates, you’d be forgiven for thinking they’re the perfect uncertainty-aware
    ML model. GPs struggle in a few key situations:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到GP具有良好的原理基础，并且能够生成高质量的不确定性估计，您可能会认为它们是完美的、不确定性感知型机器学习模型。然而，GP在几个关键情境下表现不佳：
- en: High-dimensional data
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高维数据
- en: Large amounts of data
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大量数据
- en: Highly complex data
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度复杂的数据
- en: The first two points here are largely down to the inability of GPs to scale
    well. To understand this, we just need to look at the training and inference procedures
    for GPs. While it’s beyond the scope of this book to cover this in detail, the
    key point here is in the matrix operations required for GP training.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的前两点主要归因于GP在扩展性上的不足。为了理解这一点，我们只需要看看GP的训练和推理过程。尽管详细讨论这些内容超出了本书的范围，但关键点在于GP训练所需的矩阵运算。
- en: 'During training, it is necessary to invert a *D* × *D* matrix, where *D* is
    the dimensionality of our data. Because of this, GP training quickly becomes computationally
    prohibitive. This can be somewhat alleviated through the use of Cholesky deomposition,
    rather than direct matrix inversion. As well as being more computationally efficient,
    Cholesky decomposition is also more numerically stable. Unfortunately, Cholesky
    decomposition also has its weaknesses: computationally, its complexity is *O*(*n*³).
    This means that, as the size of our dataset increases, GP training becomes more
    and more expensive.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，需要对一个 *D* × *D* 的矩阵进行求逆，其中 *D* 是数据的维度。因此，GP的训练很快变得计算开销巨大。通过使用Cholesky分解来代替直接矩阵求逆，可以在一定程度上缓解这一问题。Cholesky分解不仅在计算上更高效，而且数值稳定性也更好。不幸的是，Cholesky分解也有其不足之处：在计算上，它的复杂度是
    *O*(*n*³)。这意味着，随着数据集大小的增加，GP训练变得越来越昂贵。
- en: 'But it’s not only training that’s affected: because we need to compute the
    covariance between a new data point and all observed data points at inference,
    GPs have a *O*(*n*²) computational complexity at inference.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 但受影响的不仅仅是训练过程：因为在推理时我们需要计算新数据点与所有已观察数据点之间的协方差，GP（高斯过程）在推理时的计算复杂度为 *O*(*n*²)。
- en: 'As well as the computational cost, GPs aren’t light in memory: because we need
    to store our covariance matrix **K**, GPs have a *O*(*n*²) memory complexity.
    Thus, in the case of large datasets, even if we have the compute resources necessary
    to train them, it may not be practical to use them in real-world applications
    due to their memory requirements.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算成本，GP在内存使用上也不轻便：因为我们需要存储协方差矩阵 **K**，GP的内存复杂度为 *O*(*n*²)。因此，在大数据集的情况下，即使我们拥有训练它们所需的计算资源，由于其内存需求，使用它们进行实际应用可能也不现实。
- en: The last point in our list concerns the complexity of data. As you are probably
    aware – and as we’ll touch on in *Chapter 3, Fundamentals of Deep Learning* –
    one of the major advantages of DNNs is their ability to process complex, high-dimensional
    data through layers of non-linear transformations. While GPs are powerful, they’re
    also relatively simple models, and they’re not able to learn the kinds of powerful
    feature representations that are possible with DNNs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们清单中的最后一点涉及数据的复杂性。正如你可能已经知道的——并且我们将在 *第3章 深度学习基础* 中提到——DNN（深度神经网络）的一个主要优点是它们能够通过层层非线性变换处理复杂的高维数据。虽然GP很强大，但它们也是相对简单的模型，无法学习DNN所能实现的强大特征表示。
- en: 'All of these factors mean that, while GPs are an excellent choice for relatively
    low-dimensional data and reasonably small datasets, they aren’t practical for
    many of the complex problems we face in ML. And so, we turn to BDL methods: methods
    that have the flexibility and scalability of deep learning, while also producing
    model uncertainty estimates.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些因素表明，虽然GP对于相对低维数据和较小的数据集是一个很好的选择，但对于我们在机器学习中面临的许多复杂问题来说，它们并不实用。因此，我们转向BDL方法：这些方法具备深度学习的灵活性和可扩展性，同时还能够提供模型的不确定性估计。
- en: 2.4 Summary
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 总结
- en: 'In this chapter, we’ve covered some of the fundamental concepts and methods
    related to Bayesian inference. First, we reviewed Bayes’ theorem and the fundamentals
    of probability theory – allowing us to understand the concept of uncertainty,
    as well as how we apply it to the predictions of ML models. Next, we introduced
    sampling, and an important class of algorithms: Markov Chain Monte Carlo, or MCMC,
    methods. Lastly, we covered Gaussian processes, and illustrated the crucial concept
    of well calibrated uncertainty. These key topics will provide you with the necessary
    foundation for the content that will follow, however, we encourage you to explore
    the recommended reading materials for a more comprehensive treatment of the topics
    introduced in this chapter.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一些与贝叶斯推断相关的基本概念和方法。首先，我们回顾了贝叶斯定理和概率论的基础知识——使我们能够理解不确定性的概念，以及如何将其应用于机器学习模型的预测中。接下来，我们介绍了抽样和一个重要的算法类别：马尔可夫链蒙特卡洛方法（MCMC）。最后，我们讲解了高斯过程，并阐明了良好校准不确定性的关键概念。这些核心主题将为后续内容提供必要的基础，然而，我们鼓励您阅读推荐的参考材料，以便更全面地学习本章中介绍的主题。
- en: In the next chapter, we will see how DNNs have changed the landscape of machine
    learning over the last decade, exploring the tremendous advantages offered by
    deep learning, and the motivation behind the development of BDL methods.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到深度神经网络（DNN）如何在过去十年中改变机器学习的格局，探索深度学习带来的巨大优势，以及BDL方法发展的动机。
- en: 2.5 Further reading
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 进一步阅读
- en: 'There are a variety of techniques being explored to improve the flexibility
    and scalability of GPs – such as Deep GPs or Sparse GPs. The following resources
    explore some of these topics, and also provide a more thorough treatment of the
    content covered in this chapter:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高高斯过程（GP）的灵活性和可扩展性，正在探索多种技术——例如深度高斯过程（Deep GPs）或稀疏高斯过程（Sparse GPs）。以下资源探讨了其中一些主题，并对本章内容进行了更深入的阐述：
- en: '*Bayesian Analysis with Python*, Martin: this book comprehensively covers core
    topics in statistical modeling and probabilistic programming, and includes practical
    walk-throughs of various sampling methods, as well as a good overview of Gaussian
    processes and a variety of other techniques core to Bayesian analysis.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《Python 贝叶斯分析》*，马丁：这本书全面覆盖了统计建模和概率编程的核心主题，包括对各种抽样方法的实用演示，以及对高斯过程和其他多种贝叶斯分析核心技术的良好概述。'
- en: '*Gaussian Processes for Machine Learning*, Rasmussen and Williams: this is
    often considered the definitive text on Gaussian processes, and provides highly
    detailed explanations of the theory underlying Gaussian processes. A key text
    for anyone serious about Bayesian inference.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《机器学习中的高斯过程》*，拉斯穆森和威廉姆斯：这本书通常被视为高斯过程的权威教材，提供了高斯过程背后理论的详细解释。对于任何认真学习贝叶斯推断的人来说，这是一本关键的教材。'
