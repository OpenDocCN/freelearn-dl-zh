- en: ChapterÂ 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬äºŒç« 
- en: Fundamentals of Bayesian Inference
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯æ¨ç†åŸºç¡€
- en: 'Before we get into Bayesian inference with **Deep Neural Networks** (**DNNs**),
    we should take some time to understand the fundamentals. In this chapter, weâ€™ll
    do just that: exploring the core concepts of Bayesian modeling, and taking a look
    at some of the popular methods used for Bayesian inference. By the end of this
    chapter, you should have a good understanding of why we use probabilistic modeling,
    and what kinds of properties we look for in well principled â€“ or well conditioned
    â€“ methods.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å¼€å§‹æ¢è®¨ä½¿ç”¨**æ·±åº¦ç¥ç»ç½‘ç»œ**ï¼ˆ**DNNs**ï¼‰è¿›è¡Œè´å¶æ–¯æ¨ç†ä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥èŠ±ä¸€äº›æ—¶é—´ç†è§£åŸºæœ¬åŸç†ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†è¿›è¡Œè¿™æ ·çš„æ¢è®¨ï¼šæ¢ç´¢è´å¶æ–¯å»ºæ¨¡çš„æ ¸å¿ƒæ¦‚å¿µï¼Œå¹¶äº†è§£ä¸€äº›å¸¸ç”¨çš„è´å¶æ–¯æ¨ç†æ–¹æ³•ã€‚åœ¨æœ¬ç« ç»“æŸæ—¶ï¼Œä½ åº”è¯¥èƒ½å¾ˆå¥½åœ°ç†è§£æˆ‘ä»¬ä¸ºä½•ä½¿ç”¨æ¦‚ç‡å»ºæ¨¡ï¼Œä»¥åŠæˆ‘ä»¬åœ¨è‰¯å¥½åŸåˆ™åŒ–çš„æˆ–è‰¯å¥½æ¡ä»¶åŒ–çš„æ¨¡å‹ä¸­å¯»æ±‚ä»€ä¹ˆæ ·çš„ç‰¹æ€§ã€‚
- en: 'This content will be covered in the following sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬å†…å®¹å°†æ¶µç›–ä»¥ä¸‹éƒ¨åˆ†ï¼š
- en: Refreshing our knowledge of Bayesian modeling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åˆ·æ–°æˆ‘ä»¬å¯¹è´å¶æ–¯å»ºæ¨¡çš„ç†è§£
- en: Bayesian inference via sampling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡é‡‡æ ·è¿›è¡Œè´å¶æ–¯æ¨ç†
- en: Exploring the Gaussian processes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢ç´¢é«˜æ–¯è¿‡ç¨‹
- en: 2.1 Refreshing our knowledge of Bayesian modeling
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 åˆ·æ–°æˆ‘ä»¬å¯¹è´å¶æ–¯å»ºæ¨¡çš„ç†è§£
- en: 'Bayesian modeling is concerned with understanding the probability of an event
    occurring given some prior assumptions and some observations. The prior assumptions
    describe our initial beliefs, or hypothesis, about the event. For example, letâ€™s
    say we have two six-sided dice, and we want to predict the probability that the
    sum of the two dice is 5\. First, we need to understand how many possible outcomes
    there are. Because each die has 6 sides, the number of possible outcomes is 6
    Ã— 6 = 36\. To work out the possibility of rolling a 5, we need to work out how
    many combinations of values will sum to 5:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: è´å¶æ–¯å»ºæ¨¡å…³æ³¨çš„æ˜¯åœ¨ä¸€äº›å…ˆéªŒå‡è®¾å’Œè§‚å¯Ÿæ•°æ®çš„åŸºç¡€ä¸Šç†è§£äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ã€‚å…ˆéªŒå‡è®¾æè¿°äº†æˆ‘ä»¬å¯¹äº‹ä»¶çš„åˆæ­¥ä¿¡å¿µæˆ–å‡è®¾ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªå…­é¢éª°å­ï¼Œå¹¶ä¸”æˆ‘ä»¬æƒ³é¢„æµ‹ä¸¤ä¸ªéª°å­å’Œä¸º5çš„æ¦‚ç‡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦äº†è§£æœ‰å¤šå°‘ç§å¯èƒ½çš„ç»“æœã€‚å› ä¸ºæ¯ä¸ªéª°å­æœ‰6é¢ï¼Œæ‰€æœ‰å¯èƒ½çš„ç»“æœæ•°æ˜¯6
    Ã— 6 = 36ã€‚ä¸ºäº†è®¡ç®—æ·å‡ºå’Œä¸º5çš„å¯èƒ½æ€§ï¼Œæˆ‘ä»¬éœ€è¦ç®—å‡ºå“ªäº›æ•°å€¼çš„ç»„åˆåŠ èµ·æ¥æ˜¯5ï¼š
- en: '![PIC](img/file7.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file7.png)'
- en: 'FigureÂ 2.1: Illustration of all values summing to five when rolling two six-sided
    dice'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.1ï¼šå±•ç¤ºæ·å‡ºä¸¤ä¸ªå…­é¢éª°å­æ—¶æ‰€æœ‰æ•°å€¼å’Œä¸º5çš„æƒ…å†µ
- en: As we can see here, there are 4 combinations that add up to 5, thus the probability
    of having two dice produce a sum of 5 is ![-4 36](img/file8.jpg), or ![1 9](img/file9.jpg).
    We call this initial belief the **prior**. Now, what happens if we incorporate
    information from an observation? Letâ€™s say we know what the value for one of the
    dice will be â€“ letâ€™s say 3\. This shrinks our number of possible values down to
    6, as we only have the remaining die to roll, and for the result to be 5, weâ€™d
    need this value to be 2.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæœ‰4ç§ç»„åˆçš„å’Œä¸º5ï¼Œå› æ­¤ä¸¤ä¸ªéª°å­å’Œä¸º5çš„æ¦‚ç‡æ˜¯ï¼[-4 36](img/file8.jpg)ï¼Œå³![1 9](img/file9.jpg)ã€‚æˆ‘ä»¬ç§°è¿™ä¸ªåˆæ­¥çš„ä¿¡å¿µä¸º**å…ˆéªŒ**ã€‚ç°åœ¨ï¼Œå¦‚æœæˆ‘ä»¬åŠ å…¥æ¥è‡ªè§‚å¯Ÿçš„ä¿¡æ¯ä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿå‡è®¾æˆ‘ä»¬çŸ¥é“å…¶ä¸­ä¸€ä¸ªéª°å­çš„å€¼æ˜¯3ï¼Œè¿™å°±å°†æˆ‘ä»¬å¯èƒ½çš„å€¼ç¼©å°åˆ°6ï¼Œå› ä¸ºæˆ‘ä»¬åªå‰©ä¸‹å¦ä¸€ä¸ªéª°å­è¦æ·ï¼Œä¸ºäº†ä½¿ç»“æœä¸º5ï¼Œæˆ‘ä»¬éœ€è¦å¦ä¸€ä¸ªéª°å­çš„å€¼æ˜¯2ã€‚
- en: '![PIC](img/file10.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file10.png)'
- en: 'FigureÂ 2.2: Illustration of remaining value, which sums to five after rolling
    the first die'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.2ï¼šå±•ç¤ºå‰©ä½™çš„å€¼ï¼Œåœ¨æ·å‡ºç¬¬ä¸€ä¸ªéª°å­åå’Œä¸º5
- en: 'Because we assume our die is fair, the probability of the sum of the dice being
    5 is now ![1 6](img/file11.jpg). This probability, called the **posterior**, is
    obtained using information from our observation. At the core of Bayesian statistics
    is Bayesâ€™ rule (hence â€Bayesianâ€), which we use to determine the posterior probability
    given some prior knowledge. Bayesâ€™ rule is defined as:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬å‡è®¾éª°å­æ˜¯å…¬å¹³çš„ï¼Œç°åœ¨ä¸¤ä¸ªéª°å­å’Œä¸º5çš„æ¦‚ç‡æ˜¯![1 6](img/file11.jpg)ã€‚è¿™ä¸ªæ¦‚ç‡ï¼Œç§°ä¸º**åéªŒ**ï¼Œæ˜¯é€šè¿‡æˆ‘ä»¬çš„è§‚å¯Ÿå¾—åˆ°çš„ä¿¡æ¯è®¡ç®—å‡ºæ¥çš„ã€‚è´å¶æ–¯ç»Ÿè®¡çš„æ ¸å¿ƒæ˜¯è´å¶æ–¯å®šç†ï¼ˆå› æ­¤ç§°ä¸ºâ€œè´å¶æ–¯â€ï¼‰ï¼Œæˆ‘ä»¬ç”¨å®ƒæ¥åœ¨æœ‰ä¸€äº›å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹ç¡®å®šåéªŒæ¦‚ç‡ã€‚è´å¶æ–¯å®šç†å®šä¹‰å¦‚ä¸‹ï¼š
- en: '![P(A |B ) = P(B-|A)Ã—-P-(A)- P(B ) ](img/file12.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![P(A |B ) = P(B-|A)Ã—-P-(A)- P(B ) ](img/file12.jpg)'
- en: 'Where we can define *P*(*A*|*B*) as *P*(*d*[1] + *d*[2] = 5|**d**[1] = **3**),
    where *d*[1] and *d*[2] represent dice 1 and 2 respectively. We can see this in
    action using our previous example. Starting with the **likelihood**, that is,
    the term on the left of our numerator, we see that:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰*P*(*A*|*B*)ä¸º*P*(*d*[1] + *d*[2] = 5|**d**[1] = **3**)ï¼Œå…¶ä¸­*d*[1]å’Œ*d*[2]åˆ†åˆ«ä»£è¡¨éª°å­1å’Œéª°å­2ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¹‹å‰çš„ä¾‹å­çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚ä»**ä¼¼ç„¶**å¼€å§‹ï¼Œå³åˆ†å­å·¦ä¾§çš„é¡¹ï¼Œæˆ‘ä»¬çœ‹åˆ°ï¼š
- en: '![1- P (B |A) = P (d1 = 3|d1 + d2 = 5) = 4 ](img/file13.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![1- P (B |A) = P (d1 = 3|d1 + d2 = 5) = 4 ](img/file13.jpg)'
- en: 'We can verify this by looking at our grid. Moving to the second part of the
    numerator â€“ the prior â€“ we see that:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹æˆ‘ä»¬çš„ç½‘æ ¼æ¥éªŒè¯è¿™ä¸€ç‚¹ã€‚ç§»åŠ¨åˆ°åˆ†å­éƒ¨åˆ†çš„ç¬¬äºŒéƒ¨åˆ†â€”â€”å…ˆéªŒâ€”â€”æˆ‘ä»¬çœ‹åˆ°ï¼š
- en: '![ 4 1 P(A ) = P (d1 + d2 = 5) =--= -- 36 9 ](img/file14.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![ 4 1 P(A ) = P (d1 + d2 = 5) =--= -- 36 9 ](img/file14.jpg)'
- en: 'On the denominator, we have our **normalization constant** (also referred to
    as the **marginal likelihood**), which is simply:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åˆ†æ¯éƒ¨åˆ†ï¼Œæˆ‘ä»¬æœ‰æˆ‘ä»¬çš„**å½’ä¸€åŒ–å¸¸æ•°**ï¼ˆä¹Ÿç§°ä¸º**è¾¹é™…ä¼¼ç„¶**ï¼‰ï¼Œå®ƒå°±æ˜¯ï¼š
- en: '![P(B ) = P (d1 = 3) = 1 6 ](img/file15.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![P(B ) = P (d1 = 3) = 1 6 ](img/file15.jpg)'
- en: 'Putting this all together using Bayesâ€™ theorem, we have:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¿™ä¸€åˆ‡ç»“åˆåœ¨ä¸€èµ·ä½¿ç”¨è´å¶æ–¯å®šç†ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: '![ 14 Ã— 19 1 P(d1 + d2 = 5|d1 = 3) = --1---= 6- 6 ](img/file16.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![ 14 Ã— 19 1 P(d1 + d2 = 5|d1 = 3) = --1---= 6- 6 ](img/file16.jpg)'
- en: What we have here is the *probability* of the outcome being 5 if we know one
    dieâ€™s value. However, in this book, weâ€™ll often be referring to **uncertainties**
    rather than probabilities â€“ and learning methods to obtain uncertainty estimates
    with DNNs. These methods belong to a broader class of **uncertainty** **quantification**,
    and aim to quantify the uncertainty in the predictions from an ML model. That
    is, we want to predict *P*(*Å·*|*ğœƒ*), where *Å·* is a prediction from a model, and
    *ğœƒ* represents the parameters of the model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæˆ‘ä»¬å¾—åˆ°çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“ä¸€ä¸ªéª°å­çš„å€¼ï¼Œç»“æœä¸º 5 çš„*æ¦‚ç‡*ã€‚ç„¶è€Œï¼Œåœ¨æœ¬ä¹¦ä¸­ï¼Œæˆ‘ä»¬å°†ç»å¸¸æåˆ°**ä¸ç¡®å®šæ€§**ï¼Œè€Œä¸æ˜¯æ¦‚ç‡â€”â€”å¹¶ä¸”å­¦ä¹ å¦‚ä½•é€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰è·å¾—ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚è¿™äº›æ–¹æ³•å±äºæ›´å¹¿æ³›çš„**ä¸ç¡®å®šæ€§é‡åŒ–**ç±»åˆ«ï¼Œæ—¨åœ¨é‡åŒ–æ¥è‡ªæœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹ä¸­çš„ä¸ç¡®å®šæ€§ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›é¢„æµ‹
    *P*(*Å·*|*ğœƒ*)ï¼Œå…¶ä¸­ *Å·* æ˜¯æ¨¡å‹çš„é¢„æµ‹å€¼ï¼Œè€Œ *ğœƒ* ä»£è¡¨æ¨¡å‹çš„å‚æ•°ã€‚
- en: As we know from fundamental probability theory, probabilities are bound between
    0 and 1\. The closer we are to 1, the more likely â€“ or probable â€“ the event is.
    We can view our uncertainty as subtracting our probability from 1\. In the context
    of the example here, the probability of the sum being 5 is *P*(*d*[1] + *d*[2]
    = 5|*d*[1] = 3) = ![1 6](img/file17.jpg) = 0*.*166\. So, our uncertainty is simply
    1 âˆ’![16](img/file18.jpg) = ![56](img/file19.jpg) = 0*.*833, meaning that thereâ€™s
    a *>* 80% chance that the outcome *will* *not* be 5\. As we proceed through the
    book, weâ€™ll learn about different sources of uncertainty, and how uncertainties
    can help us to develop more robust deep learning systems.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬ä»åŸºç¡€æ¦‚ç‡è®ºä¸­çŸ¥é“çš„é‚£æ ·ï¼Œæ¦‚ç‡æ˜¯ä»‹äº 0 å’Œ 1 ä¹‹é—´çš„ã€‚è¶Šæ¥è¿‘ 1ï¼Œäº‹ä»¶å‘ç”Ÿçš„å¯èƒ½æ€§å°±è¶Šå¤§â€”â€”æˆ–ç§°ä¸ºæ¦‚ç‡è¶Šé«˜ã€‚æˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬çš„ä¸ç¡®å®šæ€§è§†ä¸ºä»
    1 ä¸­å‡å»æ¦‚ç‡ã€‚åœ¨è¿™é‡Œçš„ç¤ºä¾‹ä¸­ï¼Œå’Œä¸º 5 çš„æ¦‚ç‡æ˜¯ *P*(*d*[1] + *d*[2] = 5|*d*[1] = 3) = ![1 6](img/file17.jpg)
    = 0*.*166ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§å°±æ˜¯ 1 âˆ’![16](img/file18.jpg) = ![56](img/file19.jpg) = 0*.*833ï¼Œæ„å‘³ç€ç»“æœ*ä¸ä¼š*æ˜¯
    5 çš„æ¦‚ç‡*å¤§äº* 80%ã€‚éšç€æˆ‘ä»¬æ·±å…¥æœ¬ä¹¦ï¼Œæˆ‘ä»¬å°†å­¦ä¹ ä¸åŒçš„ä¸ç¡®å®šæ€§æ¥æºï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨ä¸ç¡®å®šæ€§æ¥å¼€å‘æ›´å¼ºå¤§çš„æ·±åº¦å­¦ä¹ ç³»ç»Ÿã€‚
- en: 'Letâ€™s continue using our dice example to build a better understanding of for
    model uncertainty estimates. Many common machine learning models work on the basis
    of **maximum likelihood estimation** or **MLE**. That is, they look to predict
    the value that is *most likely*: tuning their parameters during training to produce
    the most likely outcome *Å·* given some input *x*. As a simple illustration, letâ€™s
    say we want to predict the value of *d*[1] + *d*[2] given a value of *d*[1]. We
    can simply define this as the **expectation** of *d*[1] + *d*[2] conditioned on
    *d*[1]:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»§ç»­ä½¿ç”¨æˆ‘ä»¬çš„éª°å­ç¤ºä¾‹ï¼Œä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚è®¸å¤šå¸¸è§çš„æœºå™¨å­¦ä¹ æ¨¡å‹åŸºäº**æœ€å¤§ä¼¼ç„¶ä¼°è®¡**ï¼ˆ**MLE**ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬å¯»æ±‚é¢„æµ‹æœ€*å¯èƒ½*çš„å€¼ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒèŠ‚å…¶å‚æ•°ï¼Œä»¥æ ¹æ®ä¸€äº›è¾“å…¥
    *x* äº§ç”Ÿæœ€å¯èƒ½çš„è¾“å‡º *Å·*ã€‚ä½œä¸ºç®€å•çš„è¯´æ˜ï¼Œå‡è®¾æˆ‘ä»¬è¦é¢„æµ‹ *d*[1] + *d*[2] çš„å€¼ï¼Œç»™å®š *d*[1] çš„å€¼ã€‚æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†å…¶å®šä¹‰ä¸º**æœŸæœ›**ï¼Œå³åœ¨
    *d*[1] æ¡ä»¶ä¸‹ï¼Œ*d*[1] + *d*[2] çš„æœŸæœ›å€¼ï¼š
- en: '![Ë†y = ğ”¼ [d + d |d ] 1 2 1 ](img/file20.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![Ë†y = ğ”¼ [d + d |d ] 1 2 1 ](img/file20.jpg)'
- en: That is, the *mean* of the possible values of *d*[1] + *d*[2].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å³ï¼Œ*d*[1] + *d*[2] çš„å¯èƒ½å€¼çš„*å‡å€¼*ã€‚
- en: 'Setting *d*[1] = 3, our possible values for *d*[1] + *d*[2] are {4*,*5*,*6*,*7*,*8*,*9}
    (as illustrated in *Figure 2.2*), making our mean:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½® *d*[1] = 3ï¼Œæˆ‘ä»¬å¯èƒ½å¾—åˆ°çš„ *d*[1] + *d*[2] çš„å€¼ä¸º {4*,*5*,*6*,*7*,*8*,*9}ï¼ˆå¦‚*å›¾ 2.2*æ‰€ç¤ºï¼‰ï¼Œä½¿å¾—æˆ‘ä»¬çš„å‡å€¼ï¼š
- en: '![ 1 âˆ‘6 4+ 5 + 6+ 7+ 8 + 9 Î¼ = -- ai = --------------------= 6.5 6 i=1 6 ](img/file21.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![ 1 âˆ‘6 4+ 5 + 6+ 7+ 8 + 9 Î¼ = -- ai = --------------------= 6.5 6 i=1 6 ](img/file21.jpg)'
- en: 'This is the value weâ€™d get from a simple linear model, such as a linear regression
    defined by:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬ä»ç®€å•çº¿æ€§æ¨¡å‹ä¸­å¾—åˆ°çš„å€¼ï¼Œæ¯”å¦‚ç”±ä»¥ä¸‹å®šä¹‰çš„çº¿æ€§å›å½’æ¨¡å‹ï¼š
- en: '![Ë†y = Î²x + Î¾ ](img/file22.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![Ë†y = Î²x + Î¾ ](img/file22.jpg)'
- en: 'In this case, the values of our intersection and bias are *Î²* = 1, *Î¾* = 3*.*5\.
    If we change our value of *d*[1] to 1, we see that this mean changes to 4*.*5
    â€“ the mean of the set of possible values of *d*[1] + *d*[2]|*d*[1] = 1, in other
    words {2*,*3*,*4*,*5*,*6*,*7}. This perspective on our model predictions is important:
    while this example is very straightforward, the same principle applies to far
    more sophisticated models and data. The value we typically see with ML models
    is the *expectation*, otherwise known as the mean. As you are likely aware, the
    mean is often referred to as the **first statistical moment** â€“ with the **second
    statistical** **moment** being the **variance**, and the variance allows us to
    quantify uncertainty.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„äº¤é›†å’Œåå·®å€¼ä¸º*Î²* = 1ï¼Œ*Î¾* = 3*.*5ã€‚å¦‚æœæˆ‘ä»¬å°†*d*[1]çš„å€¼æ›´æ”¹ä¸º1ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°å‡å€¼å˜ä¸º4*.*5â€”â€”å³*d*[1]
    + *d*[2]|*d*[1] = 1çš„å¯èƒ½å€¼é›†åˆçš„å‡å€¼ï¼Œæ¢å¥è¯è¯´ï¼Œ{2*,*3*,*4*,*5*,*6*,*7}ã€‚è¿™ç§å¯¹æˆ‘ä»¬æ¨¡å‹é¢„æµ‹çš„è§†è§’å¾ˆé‡è¦ï¼šè™½ç„¶è¿™ä¸ªä¾‹å­éå¸¸ç®€å•ï¼Œä½†åŒæ ·çš„åŸåˆ™é€‚ç”¨äºæ›´å¤æ‚çš„æ¨¡å‹å’Œæ•°æ®ã€‚æˆ‘ä»¬åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­é€šå¸¸çœ‹åˆ°çš„å€¼æ˜¯*æœŸæœ›å€¼*ï¼Œä¹Ÿå°±æ˜¯å‡å€¼ã€‚å¦‚ä½ æ‰€çŸ¥ï¼Œå‡å€¼é€šå¸¸è¢«ç§°ä¸º**ç¬¬ä¸€ç»Ÿè®¡çŸ©**â€”â€”è€Œ**ç¬¬äºŒç»Ÿè®¡çŸ©**å°±æ˜¯**æ–¹å·®**ï¼Œæ–¹å·®ä½¿æˆ‘ä»¬èƒ½å¤Ÿé‡åŒ–ä¸ç¡®å®šæ€§ã€‚
- en: 'The variance for our simple example is defined as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç®€å•ç¤ºä¾‹çš„æ–¹å·®å®šä¹‰å¦‚ä¸‹ï¼š
- en: '![ âˆ‘6 2 Ïƒ2 = --i=1(ai âˆ’-Î¼) n âˆ’ 1 ](img/file23.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![ âˆ‘6 2 Ïƒ2 = --i=1(ai âˆ’-Î¼) n âˆ’ 1 ](img/file23.jpg)'
- en: 'These statistical moments should be familiar to you, as should the fact that
    the variance here is represented as the square of the **standard deviation**,
    *Ïƒ*. For our example here, for which we assume *d*[2] is a fair die, the variance
    will always be constant: *Ïƒ*Â² = 2*.*917\. That is to say, given any value of *d*[1],
    we know that values of *d*[2] are all equally likely, so the uncertainty does
    not change. But what if we have an unfair die *d*[2], which has a 50% chance of
    landing on a 6, and a 10% chance of landing on each other number? This changes
    both our mean and our variance. We can see this by looking at how we would represent
    this as a set of possible values (in other words, a perfect sample of the die)
    â€“ the set of possible values for *d*[1] + *d*[2]|*d*[1] = 1 now becomes {2*,*3*,*4*,*5*,*6*,*7*,*7*,*7*,*7*,*7}.
    Our new model will now have a bias of *Î¾* = 4*.*5, making our prediction:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç»Ÿè®¡é‡å¯¹ä½ åº”è¯¥æ˜¯ç†Ÿæ‚‰çš„ï¼Œæ–¹å·®åœ¨è¿™é‡Œè¡¨ç¤ºä¸º**æ ‡å‡†å·®**çš„å¹³æ–¹ï¼Œ*Ïƒ*ã€‚å¯¹äºæˆ‘ä»¬çš„ç¤ºä¾‹ï¼Œè¿™é‡Œå‡è®¾*d*[2]æ˜¯ä¸€ä¸ªå…¬å¹³çš„éª°å­ï¼Œæ–¹å·®å§‹ç»ˆæ˜¯å¸¸æ•°ï¼š*Ïƒ*Â²
    = 2*.*917ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®šä»»ä½•*d*[1]çš„å€¼ï¼Œæˆ‘ä»¬çŸ¥é“*d*[2]çš„æ‰€æœ‰å€¼éƒ½æ˜¯åŒç­‰å¯èƒ½çš„ï¼Œå› æ­¤ä¸ç¡®å®šæ€§ä¸ä¼šå˜åŒ–ã€‚ä½†å¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªä¸å…¬å¹³çš„éª°å­*d*[2]ï¼Œå®ƒæœ‰50%çš„æ¦‚ç‡æ·å‡º6ï¼Œä¸”æ¯ä¸ªå…¶ä»–æ•°å­—çš„æ¦‚ç‡æ˜¯10%å‘¢ï¼Ÿè¿™å°†æ”¹å˜æˆ‘ä»¬çš„å‡å€¼å’Œæ–¹å·®ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡çœ‹å¦‚ä½•è¡¨ç¤ºè¿™ä¸€ç‚¹ä½œä¸ºä¸€ç»„å¯èƒ½çš„å€¼ï¼ˆæ¢å¥è¯è¯´ï¼Œéª°å­çš„ä¸€ä¸ªå®Œç¾æ ·æœ¬ï¼‰æ¥çœ‹åˆ°è¿™ä¸€ç‚¹â€”â€”*d*[1]
    + *d*[2]|*d*[1] = 1çš„å¯èƒ½å€¼é›†åˆç°åœ¨å˜ä¸º{2*,*3*,*4*,*5*,*6*,*7*,*7*,*7*,*7*,*7}ã€‚æˆ‘ä»¬çš„æ–°æ¨¡å‹ç°åœ¨å°†æœ‰ä¸€ä¸ªåå·®*Î¾*
    = 4*.*5ï¼Œè¿™æ ·æˆ‘ä»¬çš„é¢„æµ‹å°±å˜ä¸ºï¼š
- en: '![Ë†y = 1 Ã— 1 + 4.5 = 5.5 ](img/file24.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![Ë†y = 1 Ã— 1 + 4.5 = 5.5 ](img/file24.jpg)'
- en: 'We see that the expectation has increased due to the change in the underlying
    probability of the values of die *d*[1]. However, the important difference here
    is in the change in the variance value:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°ï¼ŒæœŸæœ›å€¼ç”±äºéª°å­*d*[1]çš„åŸºç¡€æ¦‚ç‡çš„å˜åŒ–è€Œå¢åŠ ã€‚ç„¶è€Œï¼Œè¿™é‡Œé‡è¦çš„åŒºåˆ«åœ¨äºæ–¹å·®å€¼çš„å˜åŒ–ï¼š
- en: '![ âˆ‘10 (a âˆ’ Î¼)2 Ïƒ2 = --i=1--i----- = 3.25 n âˆ’ 1 ](img/file25.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![ âˆ‘10 (a âˆ’ Î¼)2 Ïƒ2 = --i=1--i----- = 3.25 n âˆ’ 1 ](img/file25.jpg)'
- en: 'Our variance has *increased*. As variance essentially gives us the average
    of the distance of each possible value from the mean, this shouldnâ€™t be surprising:
    given the weighted die, itâ€™s more likely that the outcome will be distant from
    the mean than with an unweighted die, and thus our variance increases. To summarize,
    in terms of uncertainty: the greater the likelihood that the outcome will be further
    from the mean, the greater the uncertainty.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ–¹å·®å·²ç»*å¢åŠ *ã€‚ç”±äºæ–¹å·®æœ¬è´¨ä¸Šç»™å‡ºäº†æ¯ä¸ªå¯èƒ½å€¼ä¸å‡å€¼çš„è·ç¦»çš„å¹³å‡å€¼ï¼Œè¿™å¹¶ä¸ä»¤äººæƒŠè®¶ï¼šè€ƒè™‘åˆ°åŠ æƒéª°å­ï¼Œç»“æœæ›´æœ‰å¯èƒ½è¿œç¦»å‡å€¼ï¼Œè€Œä¸åƒä¸åŠ æƒéª°å­é‚£æ ·ï¼Œå› æ­¤æˆ‘ä»¬çš„æ–¹å·®å¢åŠ ã€‚æ€»ç»“æ¥è¯´ï¼Œåœ¨ä¸ç¡®å®šæ€§æ–¹é¢ï¼šç»“æœè·ç¦»å‡å€¼è¶Šè¿œçš„å¯èƒ½æ€§è¶Šå¤§ï¼Œä¸ç¡®å®šæ€§ä¹Ÿå°±è¶Šå¤§ã€‚
- en: This has important implications for how we interpret predictions from machine
    learning models (and statistical models more generally). If our predictions are
    an approximation of the mean, and our uncertainty quantifies how likely it is
    for an outcome to be distant from the mean, then our uncertainty tells us **how
    likely it** **is that our model prediction is incorrect**. Thus, model uncertainties
    allow us to decide when to trust the predictions, and when we should be more cautious.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¹æˆ‘ä»¬å¦‚ä½•è§£è¯»æœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆä»¥åŠæ›´å¹¿æ³›çš„ç»Ÿè®¡æ¨¡å‹ï¼‰é¢„æµ‹ç»“æœå…·æœ‰é‡è¦æ„ä¹‰ã€‚å¦‚æœæˆ‘ä»¬çš„é¢„æµ‹æ˜¯å‡å€¼çš„è¿‘ä¼¼å€¼ï¼Œè€Œæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§é‡åŒ–äº†æŸä¸ªç»“æœåç¦»å‡å€¼çš„å¯èƒ½æ€§ï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§å‘Šè¯‰æˆ‘ä»¬**æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹é”™è¯¯çš„å¯èƒ½æ€§æœ‰å¤šå¤§**ã€‚å› æ­¤ï¼Œæ¨¡å‹çš„ä¸ç¡®å®šæ€§è®©æˆ‘ä»¬èƒ½å¤Ÿå†³å®šä½•æ—¶ä¿¡ä»»é¢„æµ‹ï¼Œä½•æ—¶éœ€è¦æ›´åŠ è°¨æ…ã€‚
- en: 'The examples given here are very basic, but should help to give you an idea
    of what weâ€™re looking to achieve with model uncertainty quantification. We will
    continue to explore these concepts as we learn about some of the benchmark methods
    for Bayesian inference, learning how these concepts apply to more complex, real-world
    problems. Weâ€™ll start with perhaps the most fundamental method of Bayesian inference:
    sampling.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œç»™å‡ºçš„ä¾‹å­éå¸¸åŸºç¡€ï¼Œä½†åº”è¯¥èƒ½å¸®åŠ©ä½ ç†è§£æˆ‘ä»¬å¸Œæœ›é€šè¿‡æ¨¡å‹ä¸ç¡®å®šæ€§é‡åŒ–å®ç°çš„ç›®æ ‡ã€‚éšç€æˆ‘ä»¬å­¦ä¹ ä¸€äº›è´å¶æ–¯æ¨æ–­çš„åŸºå‡†æ–¹æ³•ï¼Œæˆ‘ä»¬å°†ç»§ç»­æ¢ç´¢è¿™äº›æ¦‚å¿µï¼Œäº†è§£å®ƒä»¬å¦‚ä½•åº”ç”¨äºæ›´å¤æ‚çš„ç°å®é—®é¢˜ã€‚æˆ‘ä»¬å°†ä»å¯èƒ½æ˜¯è´å¶æ–¯æ¨æ–­æœ€åŸºæœ¬çš„æ–¹æ³•â€”â€”é‡‡æ ·å¼€å§‹ã€‚
- en: 2.2 Bayesian inference via sampling
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 é€šè¿‡é‡‡æ ·è¿›è¡Œè´å¶æ–¯æ¨æ–­
- en: In practical applications, itâ€™s not possible to know exactly what a given outcome
    would be, and, similarly, itâ€™s not possible to observe all possible outcomes.
    In these cases, we need to make a best estimate based on the evidence we have.
    The evidence is formed of **samples** â€“ observations of possible outcomes. The
    aim of ML, broadly speaking, is to learn models that generalize well from a subset
    of data. The aim of Bayesian ML is to do so while also providing an estimate of
    the uncertainty associated with the modelâ€™s predictions. In this section, weâ€™ll
    learn about how we can use sampling to do this, and will also learn why sampling
    may not be the most sensible approach.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ— æ³•ç²¾ç¡®çŸ¥é“æŸä¸ªç»“æœä¼šæ˜¯ä»€ä¹ˆï¼Œç±»ä¼¼åœ°ï¼Œä¹Ÿæ— æ³•è§‚å¯Ÿåˆ°æ‰€æœ‰å¯èƒ½çš„ç»“æœã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®å·²æœ‰çš„è¯æ®åšå‡ºæœ€ä½³ä¼°è®¡ã€‚è¯æ®ç”±**æ ·æœ¬**æ„æˆâ€”â€”å³å¯èƒ½ç»“æœçš„è§‚å¯Ÿã€‚å¹¿ä¹‰è€Œè¨€ï¼Œæœºå™¨å­¦ä¹ çš„ç›®æ ‡æ˜¯å­¦ä¹ èƒ½å¤Ÿä»æ•°æ®å­é›†è‰¯å¥½æ³›åŒ–çš„æ¨¡å‹ã€‚è´å¶æ–¯æœºå™¨å­¦ä¹ çš„ç›®æ ‡æ˜¯åœ¨åšåˆ°è¿™ä¸€ç‚¹çš„åŒæ—¶ï¼Œè¿˜è¦æä¾›ä¸€ä¸ªæ¨¡å‹é¢„æµ‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•åˆ©ç”¨é‡‡æ ·æ¥å®ç°è¿™ä¸€ç‚¹ï¼ŒåŒæ—¶ä¹Ÿä¼šäº†è§£ä¸ºä»€ä¹ˆé‡‡æ ·å¯èƒ½ä¸æ˜¯æœ€åˆç†çš„æ–¹æ³•ã€‚
- en: 2.2.1 Approximating distributions
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 è¿‘ä¼¼åˆ†å¸ƒ
- en: At the most fundamental level, sampling is about approximating distributions.
    Say we want to know the distribution of the height of people in New York. We could
    go out and measure everyone, but that would involve measuring the height of 8.4
    million people! While this would give us our most accurate answer, itâ€™s also a
    deeply impractical approach.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æœ€åŸºç¡€çš„å±‚é¢æ¥çœ‹ï¼Œé‡‡æ ·å°±æ˜¯åœ¨è¿›è¡Œåˆ†å¸ƒçš„è¿‘ä¼¼ã€‚å‡è®¾æˆ‘ä»¬æƒ³çŸ¥é“çº½çº¦äººèº«é«˜çš„åˆ†å¸ƒã€‚æˆ‘ä»¬å¯ä»¥å»æµ‹é‡æ¯ä¸ªäººçš„èº«é«˜ï¼Œä½†é‚£å°†æ¶‰åŠåˆ°æµ‹é‡840ä¸‡äººï¼è™½ç„¶è¿™ä¼šç»™æˆ‘ä»¬æœ€å‡†ç¡®çš„ç­”æ¡ˆï¼Œä½†è¿™ç§æ–¹æ³•åœ¨å®è·µä¸­æ˜¯æä¸ºä¸åˆ‡å®é™…çš„ã€‚
- en: Instead, we can sample from the population. This gives us a basic example of
    **Monte Carlo sampling**, where we use random sampling to provide data from which
    we can approximate a distribution. For example, given a database of New York residents,
    we could select â€“ at random â€“ a sub-population of residents, and use this to approximate
    the height distribution of all residents. With random sampling â€“ and any sampling,
    for that matter â€“ the accuracy of the approximation is dependent on the size of
    the sub-population. What weâ€™re looking to achieve is a **statistically significant**
    sub-sample, such that we can be confident in our approximation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ä»æ€»ä½“ä¸­è¿›è¡Œé‡‡æ ·ã€‚è¿™ç»™æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºæœ¬çš„**è’™ç‰¹å¡æ´›é‡‡æ ·**çš„ä¾‹å­ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡éšæœºé‡‡æ ·æä¾›æ•°æ®ï¼Œä»è€Œè¿‘ä¼¼ä¸€ä¸ªåˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªçº½çº¦å±…æ°‘çš„æ•°æ®åº“ï¼Œæˆ‘ä»¬å¯ä»¥éšæœºé€‰æ‹©ä¸€éƒ¨åˆ†å±…æ°‘ï¼Œå¹¶ç”¨è¿™äº›æ•°æ®æ¥è¿‘ä¼¼æ‰€æœ‰å±…æ°‘çš„èº«é«˜åˆ†å¸ƒã€‚é€šè¿‡éšæœºé‡‡æ ·â€”â€”ä»¥åŠä»»ä½•é‡‡æ ·â€”â€”è¿™ç§è¿‘ä¼¼çš„å‡†ç¡®æ€§å–å†³äºå­æ€»ä½“çš„å¤§å°ã€‚æˆ‘ä»¬å¸Œæœ›å®ç°çš„æ˜¯ä¸€ä¸ª**ç»Ÿè®¡æ˜¾è‘—**çš„å­æ ·æœ¬ï¼Œä»è€Œä½¿æˆ‘ä»¬å¯¹è¿‘ä¼¼ç»“æœæœ‰ä¿¡å¿ƒã€‚
- en: 'To get a better imdivssion of this, weâ€™ll simulate the problem by generating
    100,000 data points from a truncated normal distribution, to approximate the kind
    of height distribution we may see for a population of 100,000 people. Say we draw
    10 samples, at random, from our population. Hereâ€™s what our distribution would
    look like (on the right) compared with the true distribution (on the left):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°†é€šè¿‡ä»æˆªå°¾æ­£æ€åˆ†å¸ƒç”Ÿæˆ10ä¸‡ä¸ªæ•°æ®ç‚¹æ¥æ¨¡æ‹Ÿè¿™ä¸ªé—®é¢˜ï¼Œä»¥è¿‘ä¼¼çœ‹åˆ°10ä¸‡äººå£çš„èº«é«˜åˆ†å¸ƒã€‚å‡è®¾æˆ‘ä»¬éšæœºæŠ½å–10ä¸ªæ ·æœ¬ã€‚è¿™é‡Œæ˜¯æˆ‘ä»¬çš„åˆ†å¸ƒå›¾ï¼ˆå³ä¾§ï¼‰ä¸çœŸå®åˆ†å¸ƒå›¾ï¼ˆå·¦ä¾§ï¼‰çš„å¯¹æ¯”ï¼š
- en: '![PIC](img/file26.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file26.png)'
- en: 'FigureÂ 2.3: Plot of true distribution (left) versus sample distribution (right)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 2.3ï¼šçœŸå®åˆ†å¸ƒå›¾ï¼ˆå·¦ï¼‰ä¸æ ·æœ¬åˆ†å¸ƒå›¾ï¼ˆå³ï¼‰çš„ç»˜åˆ¶ã€‚
- en: 'As we can see, this isnâ€™t a great representation of the true distribution:
    what we see here is closer to a triangular distribution than a truncated normal.
    If we were to infer something about the populationâ€™s height based on this distribution
    alone, weâ€™d arrive at a number of inaccurate conclusions, such as missing the
    truncation above 200 cm, and the tail on the left of the distribution.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œè¿™å¹¶ä¸æ˜¯çœŸå®åˆ†å¸ƒçš„ä¸€ä¸ªå¾ˆå¥½çš„è¡¨ç¤ºï¼šè¿™é‡Œçœ‹åˆ°çš„æ›´æ¥è¿‘ä¸€ä¸ªä¸‰è§’å½¢åˆ†å¸ƒè€Œä¸æ˜¯ä¸€ä¸ªæˆªå°¾æ­£æ€åˆ†å¸ƒã€‚å¦‚æœæˆ‘ä»¬ä»…ä»…åŸºäºè¿™ä¸ªåˆ†å¸ƒæ¨æ–­äººå£çš„èº«é«˜ï¼Œæˆ‘ä»¬ä¼šå¾—å‡ºè®¸å¤šä¸å‡†ç¡®çš„ç»“è®ºï¼Œæ¯”å¦‚å¿½ç•¥äº†200å˜ç±³ä»¥ä¸Šçš„æˆªå°¾ï¼Œä»¥åŠåˆ†å¸ƒå·¦ä¾§çš„å°¾éƒ¨ã€‚
- en: 'We can get a better imdivssion by increasing our sample size â€“ letâ€™s try drawing
    100 samples:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å¢åŠ æ ·æœ¬é‡ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ›´å¥½çš„ä¼°è®¡ â€”â€” è®©æˆ‘ä»¬å°è¯•æŠ½å–100ä¸ªæ ·æœ¬ï¼š
- en: '![PIC](img/file27.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file27.png)'
- en: 'FigureÂ 2.4: Plot of true distribution (left) versus sample distribution (right).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 2.4ï¼šçœŸå®åˆ†å¸ƒå›¾ï¼ˆå·¦ï¼‰ä¸æ ·æœ¬åˆ†å¸ƒå›¾ï¼ˆå³ï¼‰çš„ç»˜åˆ¶ã€‚
- en: 'Things are starting to look better: weâ€™re starting to see some of the tail
    on the left as well as the truncation toward 200 cm. However, this sample has
    sampled more from some regions than others, leading to misrepresentation: our
    mean has been pulled down, and weâ€™re seeing two distinct peaks, rather than the
    single peak we see in the true distribution. Letâ€™s increase our sample size by
    a further order of magnitude, scaling up to 1,000 samples:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ…å†µå¼€å§‹å¥½è½¬ï¼šæˆ‘ä»¬å¼€å§‹çœ‹åˆ°å·¦ä¾§çš„ä¸€äº›å°¾éƒ¨ä»¥åŠæœ200å˜ç±³å¤„çš„æˆªå°¾ã€‚ç„¶è€Œï¼Œè¿™ä¸ªæ ·æœ¬ä»æŸäº›åŒºåŸŸé‡‡æ ·æ›´å¤šï¼Œå¯¼è‡´äº†è¯¯ä»£è¡¨ï¼šæˆ‘ä»¬çš„å¹³å‡æ•°è¢«æ‹‰ä½äº†ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†ä¸¤ä¸ªæ˜æ˜¾çš„é«˜å³°ï¼Œè€Œä¸æ˜¯çœŸå®åˆ†å¸ƒä¸­çš„å•ä¸€é«˜å³°ã€‚è®©æˆ‘ä»¬å°†æ ·æœ¬é‡å¢åŠ ä¸€ä¸ªæ•°é‡çº§ï¼Œæ‰©å±•åˆ°1,000ä¸ªæ ·æœ¬ï¼š
- en: '![PIC](img/file28.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file28.png)'
- en: 'FigureÂ 2.5: Plot of true distribution (left) versus sample distribution (right)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾Â 2.5ï¼šçœŸå®åˆ†å¸ƒå›¾ï¼ˆå·¦ï¼‰ä¸æ ·æœ¬åˆ†å¸ƒå›¾ï¼ˆå³ï¼‰çš„ç»˜åˆ¶ã€‚
- en: 'This is looking much better â€“ with a sample set of only one hundredth the size
    of our true population, we now see a distribution that closely matches our true
    distribution. This example demonstrates how, through random sampling, we can approximate
    the true distribution using a significantly smaller pool of observations. But
    that pool still has to have enough information to allow us to arrive at a good
    approximation of the true distribution: too few samples and our subset will be
    statistically *insufficient*, leading to poor approximation of the underlying
    distribution.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨çœ‹èµ·æ¥å¥½å¤šäº† â€”â€” é€šè¿‡ä¸€ä¸ªæ¯”çœŸå®äººå£è§„æ¨¡å°ä¸€ç™¾å€çš„æ ·æœ¬é›†ï¼Œæˆ‘ä»¬ç°åœ¨çœ‹åˆ°çš„åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒéå¸¸æ¥è¿‘ã€‚è¿™ä¸ªä¾‹å­å±•ç¤ºäº†é€šè¿‡éšæœºæŠ½æ ·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ˜¾è‘—è¾ƒå°çš„è§‚å¯Ÿæ± æ¥è¿‘ä¼¼çœŸå®åˆ†å¸ƒã€‚ä½†æ˜¯ï¼Œè¿™ä¸ªæ± å­ä»ç„¶å¿…é¡»åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œä»¥ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¾—å‡ºå¯¹çœŸå®åˆ†å¸ƒè‰¯å¥½è¿‘ä¼¼çš„ç»“è®ºï¼šæ ·æœ¬å¤ªå°‘ï¼Œæˆ‘ä»¬çš„å­é›†å°†ç»Ÿè®¡ä¸Šä¸è¶³ï¼Œå¯¼è‡´å¯¹æ½œåœ¨åˆ†å¸ƒçš„è¿‘ä¼¼ä¸è‰¯ã€‚
- en: 'But simple random sampling isnâ€™t the most practical method for approximating
    distributions. To achieve this, we turn to **probabilistic inference**. Given
    a model, probabilistic inference provides a way to find the model parameters that
    best describe our data. To do so, we need to first define the type of model â€“
    this is our prior. For our example, weâ€™ll use a truncated Gaussian: the idea here
    being, using our intuition, itâ€™s reasonable to assume peopleâ€™s height follows
    a normal distribution, but that very few people are above, say, 6â€™5.â€ So, weâ€™ll
    specify a truncated Gaussian distribution with an upper limit of 205 cm, or just
    over 6â€™5.â€ As itâ€™s a Gaussian distribution, in other words, ğ’©(*Î¼,Ïƒ*), our model
    parameters are *ğœƒ* = {*Î¼,Ïƒ*} â€“ with the additional constraint that our distribution
    has an upper limit of *b* = 205.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œç®€å•éšæœºé‡‡æ ·å¹¶ä¸æ˜¯é€¼è¿‘åˆ†å¸ƒçš„æœ€å®é™…æ–¹æ³•ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è½¬å‘**æ¦‚ç‡æ¨æ–­**ã€‚ç»™å®šä¸€ä¸ªæ¨¡å‹ï¼Œæ¦‚ç‡æ¨æ–­æä¾›äº†ä¸€ç§æ–¹æ³•æ¥æ‰¾åˆ°æœ€èƒ½æè¿°æˆ‘ä»¬æ•°æ®çš„æ¨¡å‹å‚æ•°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦é¦–å…ˆå®šä¹‰æ¨¡å‹çš„ç±»å‹â€”â€”è¿™å°±æ˜¯æˆ‘ä»¬çš„å…ˆéªŒã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆªæ–­çš„é«˜æ–¯åˆ†å¸ƒï¼šåŸºäºæˆ‘ä»¬çš„ç›´è§‰ï¼Œå‡è®¾äººç±»çš„èº«é«˜éµå¾ªæ­£æ€åˆ†å¸ƒæ˜¯åˆç†çš„ï¼Œä½†å¾ˆå°‘æœ‰äººä¼šè¶…è¿‡6â€™5â€
    ï¼ˆçº¦196å˜ç±³ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æŒ‡å®šä¸€ä¸ªæˆªæ–­çš„é«˜æ–¯åˆ†å¸ƒï¼Œè®¾å®šä¸Šé™ä¸º205å˜ç±³ï¼Œæˆ–è€…è¯´ç¨è¶…è¿‡6â€™5â€ã€‚ç”±äºå®ƒæ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œæ¢å¥è¯è¯´ï¼Œğ’©(*Î¼,Ïƒ*)ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å‚æ•°æ˜¯*ğœƒ*
    = {*Î¼,Ïƒ*}â€”â€”å¹¶ä¸”æœ‰é¢å¤–çš„çº¦æŸæ¡ä»¶ï¼Œå³æˆ‘ä»¬çš„åˆ†å¸ƒçš„ä¸Šé™ä¸º*b* = 205ã€‚
- en: 'This brings us to a fundamental class of algorithms: **Markov Chain Monte**
    **Carlo**, or **MCMC** methods. Like simple random sampling, these allow us to
    build a picture of the true underlying distribution, but they do so sequentially,
    whereby each sample is dependent on the sample before it. This sequential dependence
    is known as the **Markov property**, thus the *Markov chain* component of the
    name. This sequential approach accounts for the probabilistic dependence between
    samples and allows us to better approximate the probability density.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è®©æˆ‘ä»¬è¿›å…¥äº†ä¸€ç±»åŸºç¡€çš„ç®—æ³•ï¼š**é©¬å°”ç§‘å¤«é“¾è’™ç‰¹å¡ç½—**ï¼ˆ**Markov Chain Monte Carlo**ï¼Œæˆ–ç®€ç§°**MCMC**ï¼‰æ–¹æ³•ã€‚ä¸ç®€å•éšæœºé‡‡æ ·ä¸€æ ·ï¼Œè¿™äº›æ–¹æ³•ä¹Ÿå…è®¸æˆ‘ä»¬æ„å»ºçœŸå®åº•å±‚åˆ†å¸ƒçš„å›¾åƒï¼Œä½†å®ƒä»¬æ˜¯é¡ºåºè¿›è¡Œçš„ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½ä¾èµ–äºä¹‹å‰çš„æ ·æœ¬ã€‚è¿™ç§é¡ºåºä¾èµ–æ€§è¢«ç§°ä¸º**é©¬å°”ç§‘å¤«æ€§è´¨**ï¼Œå› æ­¤åå­—ä¸­çš„*é©¬å°”ç§‘å¤«é“¾*éƒ¨åˆ†ã€‚è¿™ä¸ªé¡ºåºæ–¹æ³•è€ƒè™‘äº†æ ·æœ¬ä¹‹é—´çš„æ¦‚ç‡ä¾èµ–æ€§ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´å¥½åœ°é€¼è¿‘æ¦‚ç‡å¯†åº¦ã€‚
- en: 'MCMC achieves this through sequential random sampling. Just as with the random
    sampling weâ€™re familiar with, MCMC randomly samples from our distribution. But,
    unlike simple random sampling, MCMC considers pairs of samples: some previous
    sample *x*[*t*âˆ’1] and some current sample *x*[*t*]. For each pair of samples,
    we have some criteria that specifies whether or not we keep the sample (this varies
    depending on the particular flavor of MCMC). If the new value meets this criteria,
    say if *x*[*t*] is â€preferential toâ€ our previous value *x*[*t*âˆ’1], then the sample
    is added to the chain and becomes *x*[*t*] for the next round. If the sample doesnâ€™t
    meet the criteria, we stick with the current *x*[*t*] for the next round. We repeat
    this over a (usually large) number of iterations, and in the end we should arrive
    at a good approximation of our distribution.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: MCMCé€šè¿‡é¡ºåºéšæœºé‡‡æ ·å®ç°è¿™ä¸€ç‚¹ã€‚å°±åƒæˆ‘ä»¬ç†Ÿæ‚‰çš„éšæœºé‡‡æ ·ä¸€æ ·ï¼ŒMCMCä»æˆ‘ä»¬çš„åˆ†å¸ƒä¸­éšæœºé‡‡æ ·ã€‚ä½†ä¸ç®€å•éšæœºé‡‡æ ·ä¸åŒï¼ŒMCMCè€ƒè™‘çš„æ˜¯æˆå¯¹çš„æ ·æœ¬ï¼šä¸€äº›å…ˆå‰çš„æ ·æœ¬
    *x*[*t*âˆ’1] å’Œä¸€äº›å½“å‰çš„æ ·æœ¬ *x*[*t*]ã€‚å¯¹äºæ¯ä¸€å¯¹æ ·æœ¬ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›æ ‡å‡†æ¥æŒ‡å®šæ˜¯å¦ä¿ç•™è¯¥æ ·æœ¬ï¼ˆè¿™ä¸ªæ ‡å‡†å–å†³äºå…·ä½“çš„MCMCå˜ä½“ï¼‰ã€‚å¦‚æœæ–°å€¼ç¬¦åˆè¿™ä¸ªæ ‡å‡†ï¼Œæ¯”å¦‚è¯´
    *x*[*t*] æ¯”æˆ‘ä»¬çš„å…ˆå‰å€¼ *x*[*t*âˆ’1] â€œæ›´ä¼˜â€ï¼Œé‚£ä¹ˆè¯¥æ ·æœ¬ä¼šè¢«åŠ å…¥åˆ°é“¾ä¸­ï¼Œå¹¶æˆä¸ºä¸‹ä¸€è½®çš„ *x*[*t*]ã€‚å¦‚æœæ ·æœ¬ä¸ç¬¦åˆæ ‡å‡†ï¼Œæˆ‘ä»¬åˆ™ä¼šä¿ç•™å½“å‰çš„
    *x*[*t*] ä½œä¸ºä¸‹ä¸€è½®çš„æ ·æœ¬ã€‚æˆ‘ä»¬ä¼šåœ¨ï¼ˆé€šå¸¸æ˜¯å¾ˆå¤§çš„ï¼‰è¿­ä»£æ¬¡æ•°ä¸­é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œæœ€ç»ˆåº”è¯¥èƒ½å¤Ÿå¾—åˆ°æˆ‘ä»¬åˆ†å¸ƒçš„è‰¯å¥½è¿‘ä¼¼ã€‚
- en: 'The result is an efficient sampling method that is able to closely approximate
    the true parameters of our distribution. Letâ€™s see how this applies to our height
    distribution example. Using MCMC with just 10 samples, we arrive at the following
    approximation:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„é‡‡æ ·æ–¹æ³•ï¼Œèƒ½å¤Ÿå¯†åˆ‡åœ°é€¼è¿‘æˆ‘ä»¬åˆ†å¸ƒçš„çœŸå®å‚æ•°ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•åº”ç”¨äºæˆ‘ä»¬çš„èº«é«˜åˆ†å¸ƒç¤ºä¾‹çš„ã€‚ä½¿ç”¨ä»…æœ‰10ä¸ªæ ·æœ¬çš„MCMCï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä»¥ä¸‹çš„è¿‘ä¼¼ç»“æœï¼š
- en: '![PIC](img/file29.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file29.png)'
- en: 'FigureÂ 2.6: Plot of true distribution (left) versus approximate distribution
    via MCMC (right)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.6ï¼šçœŸå®åˆ†å¸ƒï¼ˆå·¦ï¼‰ä¸é€šè¿‡MCMCå¾—åˆ°çš„è¿‘ä¼¼åˆ†å¸ƒï¼ˆå³ï¼‰çš„å¯¹æ¯”å›¾
- en: 'Not bad for ten samples â€“ certainly far better than the triangular distribution
    we arrived at with simple random sampling. Letâ€™s see how we do with 100:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åä¸ªæ ·æœ¬çš„è¡¨ç°ä¸é”™â€”â€”æ˜¾ç„¶æ¯”æˆ‘ä»¬é€šè¿‡ç®€å•éšæœºé‡‡æ ·å¾—åˆ°çš„ä¸‰è§’åˆ†å¸ƒè¦å¥½å¾—å¤šã€‚è®©æˆ‘ä»¬çœ‹çœ‹ä½¿ç”¨100ä¸ªæ ·æœ¬çš„æ•ˆæœï¼š
- en: '![PIC](img/file30.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file30.png)'
- en: 'FigureÂ 2.7: Plot of true distribution (left) versus approximate distribution
    via MCMC (right)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.7ï¼šçœŸå®åˆ†å¸ƒï¼ˆå·¦ï¼‰ä¸é€šè¿‡MCMCå¾—åˆ°çš„è¿‘ä¼¼åˆ†å¸ƒï¼ˆå³ï¼‰çš„å¯¹æ¯”å›¾
- en: 'This is looking pretty excellent â€“ in fact, weâ€™re able to obtain a better approximation
    of our distribution with 100 MCMC samples than we are with 1,000 simple random
    samples. If we continue to larger numbers of samples, weâ€™ll arrive at closer and
    closer approximations of our true distribution. But our simple example doesnâ€™t
    fully capture the power of MCMC: MCMCâ€™s true advantage comes from being able to
    approximate high-dimensional distributions, and has made it an invaluable technique
    for approximating intractable high-dimensional integrals in a variety of domains.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç»“æœçœ‹èµ·æ¥ç›¸å½“ä¸é”™â€”â€”å®é™…ä¸Šï¼Œé€šè¿‡ä½¿ç”¨100ä¸ªMCMCæ ·æœ¬ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè·å¾—æ¯”ä½¿ç”¨1,000ä¸ªç®€å•éšæœºæ ·æœ¬æ›´å¥½çš„åˆ†å¸ƒè¿‘ä¼¼ã€‚å¦‚æœæˆ‘ä»¬ç»§ç»­å¢åŠ æ ·æœ¬æ•°é‡ï¼Œæˆ‘ä»¬å°†å¾—åˆ°è¶Šæ¥è¶Šæ¥è¿‘çœŸå®åˆ†å¸ƒçš„è¿‘ä¼¼å€¼ã€‚ä½†æˆ‘ä»¬çš„ç®€å•ä¾‹å­å¹¶æ²¡æœ‰å®Œå…¨å±•ç°MCMCçš„å¼ºå¤§ï¼šMCMCçš„çœŸæ­£ä¼˜åŠ¿åœ¨äºèƒ½å¤Ÿè¿‘ä¼¼é«˜ç»´åˆ†å¸ƒï¼Œè¿™ä½¿å¾—å®ƒæˆä¸ºåœ¨å„ç§é¢†åŸŸä¸­è¿‘ä¼¼éš¾ä»¥æ±‚è§£çš„é«˜ç»´ç§¯åˆ†çš„å®è´µæŠ€æœ¯ã€‚
- en: In this book, weâ€™re interested in how we can estimate the probability distribution
    of the parameters of machine learning models â€“ this allows us to estimate the
    uncertainty associated with our predictions. In the next section, weâ€™ll take a
    look at how we do this practically by applying sampling to Bayesian linear regression.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦çš„é‡ç‚¹æ˜¯å¦‚ä½•ä¼°è®¡æœºå™¨å­¦ä¹ æ¨¡å‹å‚æ•°çš„æ¦‚ç‡åˆ†å¸ƒâ€”â€”è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿä¼°è®¡ä¸é¢„æµ‹ç›¸å…³çš„ä¸ç¡®å®šæ€§ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å®é™…æ¼”ç¤ºå¦‚ä½•é€šè¿‡å°†é‡‡æ ·åº”ç”¨äºè´å¶æ–¯çº¿æ€§å›å½’æ¥å®Œæˆè¿™ä¸€ä»»åŠ¡ã€‚
- en: 2.2.2 Implementing probabilistic inference with Bayesian linear regression
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 ä½¿ç”¨è´å¶æ–¯çº¿æ€§å›å½’å®ç°æ¦‚ç‡æ¨æ–­
- en: 'In typical linear regression, we want to predict some output *Å·* from some
    input *x* using a linear function *f*(*x*), such that *Å·* = *Î²x* + *Î¾*. With Bayesian
    linear regression, we do this probabilistically, introducing another parameter,
    *Ïƒ*Â², such that our regression equation becomes:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å…¸å‹çš„çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨çº¿æ€§å‡½æ•°*f*(*x*)ä»æŸä¸ªè¾“å…¥*x*é¢„æµ‹è¾“å‡º*Å·*ï¼Œä½¿å¾—*Å·* = *Î²x* + *Î¾*ã€‚åœ¨è´å¶æ–¯çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬ä»¥æ¦‚ç‡çš„æ–¹å¼è¿›è¡Œé¢„æµ‹ï¼Œå¼•å…¥äº†å¦ä¸€ä¸ªå‚æ•°*Ïƒ*Â²ï¼Œä½¿å¾—æˆ‘ä»¬çš„å›å½’æ–¹ç¨‹å˜ä¸ºï¼š
- en: '![Ë†y = ğ’© (x Î² + Î¾,Ïƒ2 ) ](img/file31.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Å· = ğ’© (x Î² + Î¾,ÏƒÂ² ) ](img/file31.jpg)'
- en: That is, *Å·* follows a Gaussian distribution.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå°±æ˜¯è¯´ï¼Œ*Å·*æœä»é«˜æ–¯åˆ†å¸ƒã€‚
- en: 'Here, we see our familiar bias term *Î¾* and intercept *Î²*, and introduce a
    variance parameter *Ïƒ*Â². To fit our model, we need to define a prior over these
    parameters â€“ just as we did for our MCMC example in the last section. Weâ€™ll define
    these priors as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°äº†æˆ‘ä»¬ç†Ÿæ‚‰çš„åç½®é¡¹*Î¾*å’Œæˆªè·*Î²*ï¼Œå¹¶å¼•å…¥äº†æ–¹å·®å‚æ•°*Ïƒ*Â²ã€‚ä¸ºäº†æ‹Ÿåˆæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸ºè¿™äº›å‚æ•°å®šä¹‰å…ˆéªŒâ€”â€”å°±åƒæˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­çš„MCMCç¤ºä¾‹ä¸€æ ·ã€‚æˆ‘ä»¬å°†è¿™äº›å…ˆéªŒå®šä¹‰ä¸ºï¼š
- en: '![Î¾ â‰ˆ ğ’© (0,1 ) ](img/file32.jpg)![Î² â‰ˆ ğ’© (0,1) ](img/file33.jpg)![Ïƒ2 â‰ˆ |ğ’© (0,1)|
    ](img/file34.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Î¾ â‰ˆ ğ’© (0,1 ) ](img/file32.jpg)![Î² â‰ˆ ğ’© (0,1) ](img/file33.jpg)![ÏƒÂ² â‰ˆ |ğ’© (0,1)|
    ](img/file34.jpg)'
- en: 'Note that equation 2.15 denotes the half-normal of a Gaussian distribution
    (the positive half of a zero-mean Gaussian, as standard deviation cannot be negative).
    Weâ€™ll refer to our model parameters as *ğœƒ* = *Î²,Î¾,Ïƒ*Â², and weâ€™ll use sampling
    to find the parameters that maximise the likelihood of these given our data, in
    other words, the conditional probability of our parameters given our data *D*:
    *P*(*ğœƒ*|*D*).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæ–¹ç¨‹2.15è¡¨ç¤ºçš„æ˜¯é«˜æ–¯åˆ†å¸ƒçš„åŠæ­£æ€åˆ†å¸ƒï¼ˆé›¶å‡å€¼é«˜æ–¯çš„æ­£åŠéƒ¨åˆ†ï¼Œå› ä¸ºæ ‡å‡†å·®ä¸èƒ½ä¸ºè´Ÿï¼‰ã€‚æˆ‘ä»¬å°†æ¨¡å‹å‚æ•°è¡¨ç¤ºä¸º*ğœƒ* = *Î²,Î¾,Ïƒ*Â²ï¼Œå¹¶ä½¿ç”¨é‡‡æ ·æ¥æ‰¾åˆ°æœ€å¤§åŒ–ç»™å®šæ•°æ®ä¸‹è¿™äº›å‚æ•°ä¼¼ç„¶çš„å‚æ•°ï¼Œæ¢å¥è¯è¯´ï¼Œå°±æ˜¯åœ¨ç»™å®šæ•°æ®*D*çš„æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬å‚æ•°çš„æ¡ä»¶æ¦‚ç‡*P*(*ğœƒ*|*D*)ã€‚
- en: There are a variety of MCMC sampling approaches we could use to find our model
    parameters. A common approach is to use the **Metropolis-Hastings** algorithm.
    Metropolis-Hastings is particularly useful for sampling from intractable distributions.
    It does so through the use of a proposal distribution, *Q*(*ğœƒ*â€²|*ğœƒ*), which is
    proportional to, but not exactly equal to, our true distribution. This means that,
    for example, if some value *x*[1] is twice as likely as some other value *x*[2]
    in our true distribution, this will be true of our proposal distribution too.
    Because weâ€™re interested in the probability of observations, we donâ€™t need to
    know what the *exact* value would be in our true distribution â€“ we just need to
    know that, proportionally, our proposal distribution is equivalent to our true
    distribution.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¤šç§MCMCé‡‡æ ·æ–¹æ³•å¯ä»¥ç”¨æ¥å¯»æ‰¾æˆ‘ä»¬çš„æ¨¡å‹å‚æ•°ã€‚ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯ä½¿ç”¨**Metropolis-Hastings**ç®—æ³•ã€‚Metropolis-Hastingsç‰¹åˆ«é€‚åˆä»éš¾ä»¥æ±‚è§£çš„åˆ†å¸ƒä¸­è¿›è¡Œé‡‡æ ·ã€‚å®ƒé€šè¿‡ä½¿ç”¨ä¸€ä¸ªæè®®åˆ†å¸ƒ*Q*(*ğœƒ*â€²|*ğœƒ*)æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¯¥åˆ†å¸ƒä¸æˆ‘ä»¬çš„çœŸå®åˆ†å¸ƒæˆæ¯”ä¾‹ï¼Œä½†å¹¶ä¸å®Œå…¨ç›¸åŒã€‚è¿™æ„å‘³ç€ï¼Œä¾‹å¦‚ï¼Œå¦‚æœæŸä¸ªå€¼*x*[1]åœ¨çœŸå®åˆ†å¸ƒä¸­æ˜¯å¦ä¸€ä¸ªå€¼*x*[2]çš„ä¸¤å€å¯èƒ½æ€§ï¼Œé‚£ä¹ˆåœ¨æˆ‘ä»¬çš„æè®®åˆ†å¸ƒä¸­ä¹Ÿä¼šæ˜¯å¦‚æ­¤ã€‚ç”±äºæˆ‘ä»¬å…³å¿ƒçš„æ˜¯è§‚å¯Ÿç»“æœçš„æ¦‚ç‡ï¼Œæˆ‘ä»¬ä¸éœ€è¦çŸ¥é“åœ¨çœŸå®åˆ†å¸ƒä¸­çš„*ç²¾ç¡®*å€¼â€”â€”æˆ‘ä»¬åªéœ€è¦çŸ¥é“æˆ‘ä»¬çš„æè®®åˆ†å¸ƒåœ¨æ¯”ä¾‹ä¸Šä¸çœŸå®åˆ†å¸ƒç­‰ä»·ã€‚
- en: Here are the key steps of Metropolis-Hastings for our Bayesian linear regression.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯æˆ‘ä»¬è´å¶æ–¯çº¿æ€§å›å½’ä¸­ Metropolis-Hastings çš„å…³é”®æ­¥éª¤ã€‚
- en: 'First, we initialize with an arbitrary point *ğœƒ* sampled from our parameter
    space, according to the priors for each of our parameters. Using a Gaussian distribution
    centered on our first set of parameters *ğœƒ*, select a new point *ğœƒ*â€². Then, for
    each iteration *t* âˆˆ *T*, do the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ä»æ ¹æ®æ¯ä¸ªå‚æ•°çš„å…ˆéªŒåˆ†å¸ƒåœ¨å‚æ•°ç©ºé—´ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªç‚¹ *ğœƒ* ä½œä¸ºåˆå§‹åŒ–ã€‚ç„¶åï¼Œä½¿ç”¨ä»¥æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªå‚æ•°é›† *ğœƒ* ä¸ºä¸­å¿ƒçš„é«˜æ–¯åˆ†å¸ƒï¼Œé€‰æ‹©ä¸€ä¸ªæ–°çš„ç‚¹
    *ğœƒ*â€²ã€‚æ¥ç€ï¼Œå¯¹äºæ¯æ¬¡è¿­ä»£ *t* âˆˆ *T*ï¼Œæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: 'Calculate the acceptance criteria, defined as:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—æ¥å—å‡†åˆ™ï¼Œå®šä¹‰ä¸ºï¼š
- en: '![ P(ğœƒâ€²|D ) Î± = -------- P(ğœƒ|D ) ](img/file35.jpg)'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![ P(ğœƒâ€²|D ) Î± = -------- P(ğœƒ|D ) ](img/file35.jpg)'
- en: Generate a random number from a uniform distribution *ğœ–* âˆˆ [0*,*1]. If *ğœ– <*=
    *Î±*, accept the new candidate parameters â€“ adding these to the chain, assigning
    *ğœƒ* = *ğœƒ*â€². If *ğœ– > Î±*, keep the current *ğœƒ* and draw a new value.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»å‡åŒ€åˆ†å¸ƒ *ğœ–* âˆˆ [0*,*1] ä¸­ç”Ÿæˆä¸€ä¸ªéšæœºæ•°ã€‚å¦‚æœ *ğœ– <* = *Î±*ï¼Œåˆ™æ¥å—æ–°çš„å€™é€‰å‚æ•°â€”â€”å°†è¿™äº›åŠ å…¥åˆ°é“¾ä¸­ï¼Œè®¾ *ğœƒ* = *ğœƒ*â€²ã€‚å¦‚æœ
    *ğœ– > Î±*ï¼Œåˆ™ä¿æŒå½“å‰çš„ *ğœƒ* å¹¶é‡æ–°æŠ½å–ä¸€ä¸ªæ–°å€¼ã€‚
- en: This acceptance criteria means that, if our new set of parameters have a higher
    likelihood than our last set of parameters, weâ€™ll see *Î± >* 1, in which case *Î±
    < ğœ–*. This means that, when we sample parameters that are *more likely* given
    our data, weâ€™ll always accept these parameters. If, on the other hand, *Î± <* 1,
    thereâ€™s a chance weâ€™ll reject the parameters, but we may also accept them â€“ allowing
    us to explore regions of lower likelihood.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¥å—å‡†åˆ™æ„å‘³ç€ï¼Œå¦‚æœæˆ‘ä»¬çš„æ–°å‚æ•°é›†æ¯”ä¸Šä¸€ä¸ªå‚æ•°é›†å…·æœ‰æ›´é«˜çš„ä¼¼ç„¶æ€§ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ° *Î± >* 1ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ *Î± < ğœ–*ã€‚è¿™æ„å‘³ç€ï¼Œå½“æˆ‘ä»¬æ ¹æ®æ•°æ®é‡‡æ ·å¾—åˆ°çš„å‚æ•°æ›´â€œå¯èƒ½â€æ—¶ï¼Œæˆ‘ä»¬æ€»æ˜¯ä¼šæ¥å—è¿™äº›å‚æ•°ã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœ
    *Î± <* 1ï¼Œè™½ç„¶æœ‰å¯èƒ½ä¼šæ‹’ç»è¿™äº›å‚æ•°ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯èƒ½ä¼šæ¥å—å®ƒä»¬â€”â€”å…è®¸æˆ‘ä»¬æ¢ç´¢ä½ä¼¼ç„¶åŒºåŸŸã€‚
- en: These mechanics of Metropolis-Hastings result in samples that can be used to
    compute high-quality approximations of our posterior distribution. Practically,
    Metropolis-Hastings (and MCMC methods more generally) requires a burn-in phase
    â€“ an initial phase of sampling used to escape regions of low density, which are
    typically encountered given the arbitrary initialization.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Metropolis-Hastings çš„è¿™äº›æœºåˆ¶äº§ç”Ÿçš„æ ·æœ¬å¯ç”¨äºè®¡ç®—æˆ‘ä»¬åéªŒåˆ†å¸ƒçš„é«˜è´¨é‡è¿‘ä¼¼å€¼ã€‚åœ¨å®é™…æ“ä½œä¸­ï¼ŒMetropolis-Hastingsï¼ˆä»¥åŠæ›´ä¸€èˆ¬çš„
    MCMC æ–¹æ³•ï¼‰éœ€è¦ä¸€ä¸ªé¢„çƒ­é˜¶æ®µâ€”â€”ä¸€ä¸ªåˆæ­¥é‡‡æ ·é˜¶æ®µï¼Œç”¨äºé€ƒç¦»ä½å¯†åº¦åŒºåŸŸï¼Œè¿™äº›åŒºåŸŸé€šå¸¸åœ¨ç»™å®šä»»æ„åˆå§‹åŒ–æ—¶ä¼šé‡åˆ°ã€‚
- en: 'Letâ€™s apply this to a simple problem: weâ€™ll generate some data for the function
    *y* = *x*Â² + 5 + *Î·*, where *Î·* is a noise parameter distributed according to
    *Î·* â‰ˆğ’©(0*,*5). Using Metropolis-Hastings to fit our Bayesian linear regressor,
    we get the following fit using the points sampled from our function (represented
    by the crosses):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°†å…¶åº”ç”¨äºä¸€ä¸ªç®€å•çš„é—®é¢˜ï¼šæˆ‘ä»¬å°†ä¸ºå‡½æ•° *y* = *x*Â² + 5 + *Î·* ç”Ÿæˆä¸€äº›æ•°æ®ï¼Œå…¶ä¸­ *Î·* æ˜¯ä¸€ä¸ªæ ¹æ® *Î·* â‰ˆğ’©(0*,*5)
    åˆ†å¸ƒçš„å™ªå£°å‚æ•°ã€‚ä½¿ç”¨ Metropolis-Hastings æ¥æ‹Ÿåˆæˆ‘ä»¬çš„è´å¶æ–¯çº¿æ€§å›å½’å™¨ï¼Œå¾—åˆ°ä»¥ä¸‹æ‹Ÿåˆç»“æœï¼Œè¯¥æ‹Ÿåˆä½¿ç”¨ä»æˆ‘ä»¬çš„å‡½æ•°ä¸­é‡‡æ ·çš„ç‚¹ï¼ˆé€šè¿‡äº¤å‰ç‚¹è¡¨ç¤ºï¼‰ï¼š
- en: '![PIC](img/file36.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file36.png)'
- en: 'FigureÂ 2.8: Bayesian linear regression on generated data with low variance'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2.8ï¼šåœ¨ä½æ–¹å·®ç”Ÿæˆæ•°æ®ä¸Šçš„è´å¶æ–¯çº¿æ€§å›å½’
- en: 'We see that our model fits the data in the same way we would expect for standard
    linear regression. However, unlike standard linear regression, our model produces
    predictive uncertainty: this is represented by the shaded region. This predictive
    uncertainty gives an imdivssion of how much our underlying data varies; this makes
    this model much more useful than a standard linear regression, as now we can get
    an imdivssion of the sdivad of our data, as well as the general trend. We can
    see how this varies if we generate new data and fit again, this time increasing
    the sdivad of the data by modifying our noise distribution to *Î·* â‰ˆğ’©(0*,*20):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä»¥æ ‡å‡†çº¿æ€§å›å½’é¢„æœŸçš„æ–¹å¼æ‹Ÿåˆæ•°æ®ã€‚ç„¶è€Œï¼Œä¸æ ‡å‡†çº¿æ€§å›å½’ä¸åŒï¼Œæˆ‘ä»¬çš„æ¨¡å‹äº§ç”Ÿäº†é¢„æµ‹ä¸ç¡®å®šæ€§ï¼šè¿™é€šè¿‡é˜´å½±åŒºåŸŸè¡¨ç¤ºã€‚è¿™ç§é¢„æµ‹ä¸ç¡®å®šæ€§æä¾›äº†å…³äºæˆ‘ä»¬åŸºç¡€æ•°æ®å˜åŒ–ç¨‹åº¦çš„ä¸€ä¸ªè¿‘ä¼¼ï¼›è¿™ä½¿å¾—è¿™ä¸ªæ¨¡å‹æ¯”æ ‡å‡†çº¿æ€§å›å½’æ›´æœ‰ç”¨ï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨ä¸ä»…å¯ä»¥å¾—åˆ°æ•°æ®çš„è¶‹åŠ¿ï¼Œè¿˜èƒ½è·å–æ•°æ®çš„å˜åŒ–å¹…åº¦ã€‚å¦‚æœæˆ‘ä»¬ç”Ÿæˆæ–°æ•°æ®å¹¶é‡æ–°æ‹Ÿåˆï¼Œå¢åŠ æ•°æ®çš„æ ‡å‡†å·®ï¼Œå¯ä»¥é€šè¿‡ä¿®æ”¹å™ªå£°åˆ†å¸ƒä¸º
    *Î·* â‰ˆğ’©(0*,*20) æ¥å®ç°ï¼š
- en: '![PIC](img/file37.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file37.png)'
- en: 'FigureÂ 2.9: Bayesian linear regression on generated data with high variance'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2.9ï¼šåœ¨é«˜æ–¹å·®ç”Ÿæˆæ•°æ®ä¸Šçš„è´å¶æ–¯çº¿æ€§å›å½’
- en: 'We see that our predictive uncertainty has increased proportionally to the
    sdivad of the data. This is an important property in uncertainty-aware methods:
    when we have small uncertainty, we know our prediction fits the data well, whereas
    when we have large uncertainty, we know to treat our prediction with caution,
    as it indicates the model isnâ€™t fitting this region particularly well. Weâ€™ll see
    a better example of this in the next section, which will go on to demonstrate
    how regions of more or less data contribute to our model uncertainty estimates.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°ï¼Œé¢„æµ‹çš„ä¸ç¡®å®šæ€§ä¸æ•°æ®çš„æ ‡å‡†å·®æˆæ­£æ¯”å¢åŠ ã€‚è¿™æ˜¯ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ–¹æ³•ä¸­çš„ä¸€ä¸ªé‡è¦ç‰¹æ€§ï¼šå½“ä¸ç¡®å®šæ€§è¾ƒå°æ—¶ï¼Œæˆ‘ä»¬çŸ¥é“é¢„æµ‹ä¸æ•°æ®æ‹Ÿåˆå¾—å¾ˆå¥½ï¼›è€Œå½“ä¸ç¡®å®šæ€§è¾ƒå¤§æ—¶ï¼Œæˆ‘ä»¬çŸ¥é“éœ€è¦è°¨æ…å¯¹å¾…é¢„æµ‹ï¼Œå› ä¸ºè¿™è¡¨æ˜æ¨¡å‹åœ¨è¯¥åŒºåŸŸçš„æ‹Ÿåˆæ•ˆæœè¾ƒå·®ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­çœ‹åˆ°ä¸€ä¸ªæ›´å¥½çš„ä¾‹å­ï¼Œå±•ç¤ºæ•°æ®æ›´å¤šæˆ–æ›´å°‘çš„åŒºåŸŸå¦‚ä½•å½±å“æˆ‘ä»¬æ¨¡å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚
- en: Here, we see that our predictions fit our data pretty well. In addition, we
    see that *Ïƒ*Â² varies according to the availability of data in different regions.
    What weâ€™re seeing here is a great example of a very important concept, **well
    calibrated uncertainty** â€“ also termed **high-quality** **uncertainty**. This
    refers to the fact that, in regions where our Predictions are inaccurate, our
    uncertainty is also high. Our uncertainty estimates are **poorly calibrated**
    if weâ€™re very confident in regions with inaccurate predictions, or very uncertain
    in regions with accurate predictions. As itâ€™s well-calibrated, sampling is often
    used as a benchmark for uncertainty quantification.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„é¢„æµ‹ä¸æ•°æ®æ‹Ÿåˆå¾—ç›¸å½“å¥½ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜çœ‹åˆ°*Ïƒ*Â²æ ¹æ®ä¸åŒåŒºåŸŸä¸­æ•°æ®çš„å¯ç”¨æ€§è€Œå˜åŒ–ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æ¦‚å¿µçš„æå¥½ä¾‹å­ï¼Œ**è‰¯å¥½æ ¡å‡†çš„ä¸ç¡®å®šæ€§**â€”â€”ä¹Ÿç§°ä¸º**é«˜è´¨é‡**
    **ä¸ç¡®å®šæ€§**ã€‚è¿™æ„å‘³ç€ï¼Œåœ¨é¢„æµ‹ä¸å‡†ç¡®çš„åŒºåŸŸï¼Œæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§ä¹Ÿå¾ˆé«˜ã€‚å¦‚æœæˆ‘ä»¬åœ¨é¢„æµ‹ä¸å‡†ç¡®çš„åŒºåŸŸè¿‡äºè‡ªä¿¡ï¼Œæˆ–è€…åœ¨é¢„æµ‹å‡†ç¡®çš„åŒºåŸŸè¿‡äºä¸ç¡®å®šï¼Œæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§ä¼°è®¡å°±æ˜¯**æ ¡å‡†ä¸è‰¯**çš„ã€‚ç”±äºå…¶è‰¯å¥½çš„æ ¡å‡†æ€§ï¼Œé‡‡æ ·å¸¸å¸¸è¢«ä½œä¸ºä¸ç¡®å®šæ€§é‡åŒ–çš„åŸºå‡†ã€‚
- en: Unfortunately, while sampling is effective for many applications, the need to
    obtain many samples for each parameter means that it quickly becomes computationally
    prohibitive for high dimensions of parameters. For example, if we wanted to start
    sampling parameters for complex, non-linear relationships (such as sampling the
    weights of a neural network), sampling would no longer be practical. Despite this,
    itâ€™s still useful in some cases, and later weâ€™ll see how various BDL methods make
    use of sampling.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œå°½ç®¡é‡‡æ ·å¯¹äºè®¸å¤šåº”ç”¨éå¸¸æœ‰æ•ˆï¼Œä½†æ¯ä¸ªå‚æ•°éœ€è¦è·å¾—å¤šä¸ªæ ·æœ¬ï¼Œè¿™æ„å‘³ç€å½“å‚æ•°ç»´åº¦å¾ˆé«˜æ—¶ï¼Œé‡‡æ ·å¾ˆå¿«å˜å¾—åœ¨è®¡ç®—ä¸Šæ— æ³•æ‰¿å—ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³ä¸ºå¤æ‚çš„éçº¿æ€§å…³ç³»ï¼ˆä¾‹å¦‚ç¥ç»ç½‘ç»œæƒé‡çš„é‡‡æ ·ï¼‰å¼€å§‹è¿›è¡Œé‡‡æ ·ï¼Œé‚£ä¹ˆé‡‡æ ·å°±ä¸å†å…·æœ‰å®ç”¨æ€§ã€‚å°½ç®¡å¦‚æ­¤ï¼Œå®ƒåœ¨æŸäº›æƒ…å†µä¸‹ä»ç„¶æœ‰ç”¨ï¼Œç¨åæˆ‘ä»¬å°†çœ‹åˆ°å„ç§BDLæ–¹æ³•å¦‚ä½•åˆ©ç”¨é‡‡æ ·ã€‚
- en: In the next section, weâ€™ll explore the Gaussian process â€“ another fundamental
    method for Bayesian inference, and a method that does not suffer from the same
    computational overheads as sampling.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨é«˜æ–¯è¿‡ç¨‹â€”â€”ä¸€ç§è´å¶æ–¯æ¨æ–­çš„åŸºæœ¬æ–¹æ³•ï¼Œå¹¶ä¸”æ˜¯ä¸€ç§ä¸åƒé‡‡æ ·æ–¹æ³•é‚£æ ·é­å—è®¡ç®—å¼€é”€çš„æŠ€æœ¯ã€‚
- en: 2.3 Exploring the Gaussian process
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 æ¢ç´¢é«˜æ–¯è¿‡ç¨‹
- en: As weâ€™ve seen in the previous section, sampling quickly becomes prohibitively
    expensive. To address this, we can use ML models specifically designed to produce
    uncertainty estimates â€“ the gold standard of which is the **Gaussian** **process**.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨å‰ä¸€èŠ‚ä¸­æ‰€çœ‹åˆ°çš„ï¼Œé‡‡æ ·å¾ˆå¿«ä¼šå˜å¾—éå¸¸æ˜‚è´µã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸“é—¨è®¾è®¡çš„æœºå™¨å­¦ä¹ æ¨¡å‹æ¥äº§ç”Ÿä¸ç¡®å®šæ€§ä¼°è®¡â€”â€”å…¶ä¸­æœ€ä¸ºç»å…¸çš„å°±æ˜¯**é«˜æ–¯**
    **è¿‡ç¨‹**ã€‚
- en: The Gaussian process, or **GP**, has become a staple probabilistic ML model,
    seeing use in a broad variety of applications from pharmacology through to robotics.
    Its success is largely down to its ability to produce high-quality uncertainty
    estimates over its predictions in a well-principled fashion. So, what do we mean
    by a Gaussian process?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹ï¼Œæˆ–ç§°**GP**ï¼Œå·²ç»æˆä¸ºä¸€ç§åŸºç¡€çš„æ¦‚ç‡è®ºæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¿æ³›åº”ç”¨äºä»è¯ç†å­¦åˆ°æœºå™¨äººå­¦ç­‰å¤šä¸ªé¢†åŸŸã€‚å®ƒçš„æˆåŠŸä¸»è¦å½’åŠŸäºå…¶èƒ½å¤Ÿä»¥ä¸€ç§ç§‘å­¦çš„æ–¹å¼ç”Ÿæˆå¯¹é¢„æµ‹ç»“æœçš„é«˜è´¨é‡ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚é‚£ä¹ˆï¼Œä»€ä¹ˆæ˜¯é«˜æ–¯è¿‡ç¨‹å‘¢ï¼Ÿ
- en: 'In essence, a GP is a distribution over functions. To understand what we mean
    by this, letâ€™s take a typical ML use case. We want to learn some function *f*(**x**),
    which maps a series of inputs **x** onto a series of outputs **y**, such that
    we can approximate our output via **y** = *f*(**x**). Before we see any data,
    we know nothing about our underlying function; there is an infinite number of
    possible functions this could be:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬è´¨ä¸Šï¼ŒGP æ˜¯ä¸€ä¸ªå‡½æ•°çš„åˆ†å¸ƒã€‚ä¸ºäº†ç†è§£æˆ‘ä»¬æ‰€è¯´çš„è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªå…¸å‹çš„æœºå™¨å­¦ä¹ ç”¨ä¾‹ã€‚æˆ‘ä»¬å¸Œæœ›å­¦ä¹ æŸä¸ªå‡½æ•° *f*(**x**)ï¼Œå®ƒå°†ä¸€ç³»åˆ—è¾“å…¥ **x**
    æ˜ å°„åˆ°ä¸€ç³»åˆ—è¾“å‡º **y**ï¼Œä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡ **y** = *f*(**x**) æ¥è¿‘ä¼¼æˆ‘ä»¬çš„è¾“å‡ºã€‚åœ¨æˆ‘ä»¬çœ‹åˆ°ä»»ä½•æ•°æ®ä¹‹å‰ï¼Œæˆ‘ä»¬å¯¹æ½œåœ¨çš„å‡½æ•°ä¸€æ— æ‰€çŸ¥ï¼›å®ƒå¯èƒ½æ˜¯æ— é™å¤šç§å¯èƒ½çš„å‡½æ•°ä¹‹ä¸€ï¼š
- en: '![PIC](img/file38.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file38.png)'
- en: 'FigureÂ 2.10: Illustration of space of possible functions before seeing data'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2.10ï¼šåœ¨çœ‹åˆ°æ•°æ®ä¹‹å‰ï¼Œå¯èƒ½å‡½æ•°ç©ºé—´çš„ç¤ºæ„å›¾
- en: 'Here, the black line is the true function we wish to learn, while the dotted
    lines are the possible functions given the data (in this case, no data). Once
    we observe some data, we see that the number of possible functions becomes more
    constrained, as we see here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œé»‘çº¿è¡¨ç¤ºæˆ‘ä»¬å¸Œæœ›å­¦ä¹ çš„çœŸå®å‡½æ•°ï¼Œè€Œè™šçº¿è¡¨ç¤ºåœ¨ç»™å®šæ•°æ®ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ²¡æœ‰æ•°æ®ï¼‰ä¸‹å¯èƒ½çš„å‡½æ•°ã€‚ä¸€æ—¦æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸€äº›æ•°æ®ï¼Œå°±ä¼šå‘ç°å¯èƒ½å‡½æ•°çš„æ•°é‡å˜å¾—æ›´åŠ æœ‰é™ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![PIC](img/file39.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file39.png)'
- en: 'FigureÂ 2.11: Illustration of space of possible functions after seeing some
    data'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2.11ï¼šåœ¨è§‚å¯Ÿåˆ°ä¸€äº›æ•°æ®åï¼Œå¯èƒ½å‡½æ•°ç©ºé—´çš„ç¤ºæ„å›¾
- en: 'Here, we see that our possible functions all pass through our observed data
    points, but outside of those data points, our functions take on a range of very
    different values. In a simple linear model, we donâ€™t care about these deviations
    in possible values: weâ€™re happy to interpolate from one data point to another,
    as we see in *Figure* [2.12](#x1-30007r12):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æ‰€æœ‰å¯èƒ½çš„å‡½æ•°éƒ½ç»è¿‡æˆ‘ä»¬è§‚å¯Ÿåˆ°çš„æ•°æ®ç‚¹ï¼Œä½†åœ¨è¿™äº›æ•°æ®ç‚¹ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„å‡½æ•°ä¼šå–ä¸€ç³»åˆ—éå¸¸ä¸åŒçš„å€¼ã€‚åœ¨ä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ä¸å…³å¿ƒè¿™äº›å¯èƒ½å€¼çš„åå·®ï¼šæˆ‘ä»¬ä¹äºåœ¨ä¸€ä¸ªæ•°æ®ç‚¹å’Œå¦ä¸€ä¸ªæ•°æ®ç‚¹ä¹‹é—´è¿›è¡Œæ’å€¼ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨*å›¾*
    [2.12](#x1-30007r12)ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼š
- en: '![PIC](img/file40.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file40.png)'
- en: 'FigureÂ 2.12: Illustration of linearly interpolating through our observations'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2.12ï¼šé€šè¿‡æˆ‘ä»¬çš„è§‚æµ‹å€¼è¿›è¡Œçº¿æ€§æ’å€¼çš„ç¤ºæ„å›¾
- en: But this interpolation can lead to wildly inaccurate predictions, and has no
    way of accounting for the degree of uncertainty associated with our model predictions.
    The deviations that we see here in the regions without data points are exactly
    what we want to capture with our GP. When there are a variety of possible values
    our function can take, then there is uncertainty â€“ and through capturing the degree
    of uncertainty, we are able to estimate what the possible variation in these regions
    may be.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯è¿™ç§æ’å€¼å¯èƒ½å¯¼è‡´éå¸¸ä¸å‡†ç¡®çš„é¢„æµ‹ï¼Œå¹¶ä¸”æ— æ³•è€ƒè™‘ä¸æ¨¡å‹é¢„æµ‹ç›¸å…³çš„ä¸ç¡®å®šæ€§ç¨‹åº¦ã€‚æˆ‘ä»¬åœ¨æ²¡æœ‰æ•°æ®ç‚¹çš„åŒºåŸŸçœ‹åˆ°çš„åå·®ï¼Œæ­£æ˜¯æˆ‘ä»¬å¸Œæœ›é€šè¿‡é«˜æ–¯è¿‡ç¨‹æ•æ‰çš„å†…å®¹ã€‚å½“æˆ‘ä»¬çš„å‡½æ•°å¯ä»¥å–ä¸åŒçš„å¯èƒ½å€¼æ—¶ï¼Œå°±ä¼šæœ‰ä¸ç¡®å®šæ€§â€”â€”é€šè¿‡æ•æ‰è¿™ç§ä¸ç¡®å®šæ€§çš„ç¨‹åº¦ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä¼°è®¡è¿™äº›åŒºåŸŸå†…å¯èƒ½çš„å˜åŒ–ã€‚
- en: 'Formally, a GP can be defined as a function:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å½¢å¼ä¸Šè®²ï¼Œé«˜æ–¯è¿‡ç¨‹å¯ä»¥å®šä¹‰ä¸ºä¸€ä¸ªå‡½æ•°ï¼š
- en: '![f(x) â‰ˆ GP (m (x),k(x,xâ€²)) ](img/file41.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![f(x) â‰ˆ GP (m (x),k(x,xâ€²)) ](img/file41.jpg)'
- en: 'Here, *m*(**x**) is simply the mean of our possible function values for a given
    point **x**:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ*m*(**x**) ä»…ä»…æ˜¯ç»™å®šç‚¹ **x** ä¸Šæˆ‘ä»¬å¯èƒ½å‡½æ•°å€¼çš„å‡å€¼ï¼š
- en: '![m (x) = ğ”¼[f (x)] ](img/file42.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![m (x) = ğ”¼[f (x)] ](img/file42.jpg)'
- en: The next term, *k*(**x***,***x**â€²) is a covariance function, or kernel. This
    is a fundamental component of the GP as it defines the way we model the relationship
    between different points in our data. GPs use the mean and covariance functions
    to model the space of possible functions, and thus to produce predictions as well
    as their associated uncertainties. Now that weâ€™ve introduced some of the high-level
    concepts, letâ€™s dig a little deeper and understand exactly how it is they model
    the space of possible functions, and thus estimate uncertainty. To do this, we
    need to understand GP priors.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªé¡¹ï¼Œ*k*(**x***,***x**â€²) æ˜¯åæ–¹å·®å‡½æ•°ï¼Œæˆ–ç§°ä¸ºæ ¸å‡½æ•°ã€‚è¿™æ˜¯é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰çš„ä¸€ä¸ªåŸºæœ¬ç»„æˆéƒ¨åˆ†ï¼Œå› ä¸ºå®ƒå®šä¹‰äº†æˆ‘ä»¬å¦‚ä½•å»ºæ¨¡æ•°æ®ä¸­ä¸åŒç‚¹ä¹‹é—´çš„å…³ç³»ã€‚é«˜æ–¯è¿‡ç¨‹ä½¿ç”¨å‡å€¼å’Œåæ–¹å·®å‡½æ•°æ¥å»ºæ¨¡å¯èƒ½å‡½æ•°çš„ç©ºé—´ï¼Œä»è€Œç”Ÿæˆé¢„æµ‹åŠå…¶ç›¸å…³çš„ä¸ç¡®å®šæ€§ã€‚ç°åœ¨æˆ‘ä»¬å·²ç»ä»‹ç»äº†ä¸€äº›é«˜å±‚æ¬¡çš„æ¦‚å¿µï¼Œæ¥ä¸‹æ¥è®©æˆ‘ä»¬æ›´æ·±å…¥åœ°ç†è§£å®ƒä»¬å¦‚ä½•å»ºæ¨¡å¯èƒ½å‡½æ•°çš„ç©ºé—´ï¼Œå¹¶ä¼°è®¡ä¸ç¡®å®šæ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£é«˜æ–¯è¿‡ç¨‹çš„å…ˆéªŒã€‚
- en: 2.3.1 Defining our prior beliefs with kernels
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 ä½¿ç”¨æ ¸å‡½æ•°å®šä¹‰æˆ‘ä»¬çš„å…ˆéªŒä¿¡å¿µ
- en: GP kernels describe the prior beliefs we have about our data, and so youâ€™ll
    often see them referred to as GP priors. In the same way that the prior in equation
    2.3 tells us something about the probability of the outcome of our two dice rolls,
    the GP prior tells us something important about the relationship we expect from
    our data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹çš„æ ¸å‡½æ•°æè¿°äº†æˆ‘ä»¬å¯¹æ•°æ®çš„å…ˆéªŒä¿¡å¿µï¼Œå› æ­¤ä½ ä¼šç»å¸¸çœ‹åˆ°å®ƒä»¬è¢«ç§°ä¸ºé«˜æ–¯è¿‡ç¨‹å…ˆéªŒã€‚å°±åƒæ–¹ç¨‹ 2.3 ä¸­çš„å…ˆéªŒå‘Šè¯‰æˆ‘ä»¬å…³äºä¸¤ä¸ªéª°å­æŠ•æ·ç»“æœçš„æ¦‚ç‡ä¸€æ ·ï¼Œé«˜æ–¯è¿‡ç¨‹å…ˆéªŒå‘Šè¯‰æˆ‘ä»¬æœ‰å…³æˆ‘ä»¬æœŸæœ›ä»æ•°æ®ä¸­å¾—åˆ°çš„å…³ç³»çš„é‡è¦ä¿¡æ¯ã€‚
- en: While there are advanced methods for inferring a prior from our data, they are
    beyond the scope of this book. We will instead focus on more traditional uses
    of GPs, for which we select a prior using our knowledge of the data weâ€™re working
    with.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æœ‰ä¸€äº›é«˜çº§æ–¹æ³•å¯ä»¥æ ¹æ®æˆ‘ä»¬çš„æ•°æ®æ¨æ–­å…ˆéªŒï¼Œä½†å®ƒä»¬è¶…å‡ºäº†æœ¬ä¹¦çš„èŒƒå›´ã€‚æˆ‘ä»¬å°†ä¸“æ³¨äºé«˜æ–¯è¿‡ç¨‹çš„æ›´ä¼ ç»Ÿç”¨æ³•ï¼Œå…¶ä¸­æˆ‘ä»¬ä½¿ç”¨å¯¹æ‰€å¤„ç†æ•°æ®çš„äº†è§£æ¥é€‰æ‹©å…ˆéªŒã€‚
- en: In the literature and any implementations you encounter, youâ€™ll see that the
    GP prior is often referred to as the **kernel** or **covariance function** (just
    as we have here). These three terms are all interchangeable, but for consistency
    with other work, we will henceforth refer to this as the kernel. Kernels simply
    provide a means of calculating a distance between two data points, and are exdivssed
    as *k*(*x,x*â€²), where *x* and *x*â€² are data points, and *k*() represents the function
    of the kernel. While the kernel can take on many forms, there are a small number
    of fundamental kernels that are used in a large proportion of GP applications.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ–‡çŒ®ä¸­ä»¥åŠä½ é‡åˆ°çš„ä»»ä½•å®ç°ä¸­ï¼Œä½ ä¼šå‘ç°é«˜æ–¯è¿‡ç¨‹å…ˆéªŒå¸¸å¸¸è¢«ç§°ä¸º**æ ¸å‡½æ•°**æˆ–**åæ–¹å·®å‡½æ•°**ï¼ˆå°±åƒæˆ‘ä»¬åœ¨è¿™é‡Œä¸€æ ·ï¼‰ã€‚è¿™ä¸‰ä¸ªæœ¯è¯­æ˜¯å¯ä»¥äº’æ¢çš„ï¼Œä½†ä¸ºäº†ä¸å…¶ä»–å·¥ä½œçš„æœ¯è¯­ä¸€è‡´ï¼Œä»Šåæˆ‘ä»¬å°†ç§°ä¹‹ä¸ºæ ¸å‡½æ•°ã€‚æ ¸å‡½æ•°æä¾›äº†ä¸€ç§è®¡ç®—ä¸¤ä¸ªæ•°æ®ç‚¹ä¹‹é—´è·ç¦»çš„æ–¹æ³•ï¼Œé€šå¸¸è¡¨ç¤ºä¸º
    *k*(*x*, *x*â€²)ï¼Œå…¶ä¸­ *x* å’Œ *x*â€² æ˜¯æ•°æ®ç‚¹ï¼Œè€Œ *k*() è¡¨ç¤ºæ ¸å‡½æ•°ã€‚è™½ç„¶æ ¸å‡½æ•°å¯ä»¥æœ‰å¤šç§å½¢å¼ï¼Œä½†æœ‰å°‘æ•°å‡ ç§åŸºæœ¬æ ¸å‡½æ•°åœ¨å¤§é‡é«˜æ–¯è¿‡ç¨‹åº”ç”¨ä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚
- en: 'Perhaps the most commonly encountered kernel is the **squared exponential**
    or **radial basis function** (**RBF**) kernel. This kernel takes the form:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æœ€å¸¸è§çš„æ ¸å‡½æ•°æ˜¯**å¹³æ–¹æŒ‡æ•°**æˆ–**å¾„å‘åŸºå‡½æ•°**ï¼ˆ**RBF**ï¼‰æ ¸å‡½æ•°ã€‚è¿™ä¸ªæ ¸å‡½æ•°çš„å½¢å¼æ˜¯ï¼š
- en: '![ (x âˆ’ x â€²)2 k(x,x â€²) = Ïƒ2exp âˆ’ ----2---- 2l ](img/file43.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![ (x âˆ’ x â€²)Â² k(x,x â€²) = ÏƒÂ²exp âˆ’ ----2---- 2l ](img/file43.jpg)'
- en: 'This introduces us to a couple of common kernel parameters: *l* and *Ïƒ*Â². The
    output variance parameter *Ïƒ*Â² is simply a scaling factor, used to control the
    distance of the function from its mean. The length scale parameter *l* controls
    the smoothness of the function â€“ in other words, how much your function is expected
    to vary across particular dimensions. This parameter can either be a scalar that
    is applied to all input dimensions, or a vector with a different scalar value
    for each input dimension. The latter is often achieved using **Automatic Relevance**
    **Determination**, or **ARD**, which identifies the relevant values in the input
    space.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¼•å…¥äº†å‡ ä¸ªå¸¸è§çš„æ ¸å‚æ•°ï¼š*l* å’Œ *Ïƒ*Â²ã€‚è¾“å‡ºæ–¹å·®å‚æ•° *Ïƒ*Â² åªæ˜¯ä¸€ä¸ªç¼©æ”¾å› å­ï¼Œç”¨äºæ§åˆ¶å‡½æ•°ä¸å…¶å‡å€¼ä¹‹é—´çš„è·ç¦»ã€‚é•¿åº¦å°ºåº¦å‚æ•° *l* æ§åˆ¶å‡½æ•°çš„å¹³æ»‘åº¦â€”â€”æ¢å¥è¯è¯´ï¼Œæ§åˆ¶å‡½æ•°åœ¨ç‰¹å®šç»´åº¦ä¸Šçš„å˜åŒ–å¹…åº¦ã€‚è¿™ä¸ªå‚æ•°å¯ä»¥æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œåº”ç”¨äºæ‰€æœ‰è¾“å…¥ç»´åº¦ï¼Œæˆ–è€…æ˜¯ä¸€ä¸ªå‘é‡ï¼Œæ¯ä¸ªè¾“å…¥ç»´åº¦æœ‰ä¸åŒçš„æ ‡é‡å€¼ã€‚åè€…é€šå¸¸æ˜¯é€šè¿‡**è‡ªåŠ¨ç›¸å…³æ€§**
    **ç¡®å®š**ï¼ˆ**ARD**ï¼‰å®ç°çš„ï¼Œè¯¥æ–¹æ³•è¯†åˆ«è¾“å…¥ç©ºé—´ä¸­çš„ç›¸å…³å€¼ã€‚
- en: GPs make predictions via a covariance matrix based on the kernel â€“ essentially
    comparing a new data point to previously observed data points. However, just as
    with all ML models, GPs need to be trained, and this is where the length scale
    comes in. The length scale forms the parameters of our GP, and through the training
    process it learns the optimal value(s) for the length scale(s). This is typically
    done using a nonlinear optimizer, such as the **Broyden-Fletcher-Goldfarb-Shanno**
    (**BFGS**) optimizer. Many optimizers can be used, including optimizers you may
    be familiar with for deep learning, such as stochastic gradient descent and its
    variants.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹é€šè¿‡åŸºäºæ ¸å‡½æ•°çš„åæ–¹å·®çŸ©é˜µè¿›è¡Œé¢„æµ‹â€”â€”æœ¬è´¨ä¸Šæ˜¯å°†æ–°æ•°æ®ç‚¹ä¸ä¹‹å‰è§‚å¯Ÿåˆ°çš„æ•°æ®ç‚¹è¿›è¡Œæ¯”è¾ƒã€‚ç„¶è€Œï¼Œå°±åƒæ‰€æœ‰æœºå™¨å­¦ä¹ æ¨¡å‹ä¸€æ ·ï¼Œé«˜æ–¯è¿‡ç¨‹ä¹Ÿéœ€è¦è®­ç»ƒï¼Œè¿™å°±æ˜¯é•¿åº¦å°ºåº¦å‘æŒ¥ä½œç”¨çš„åœ°æ–¹ã€‚é•¿åº¦å°ºåº¦æ„æˆäº†æˆ‘ä»¬é«˜æ–¯è¿‡ç¨‹çš„å‚æ•°ï¼Œé€šè¿‡è®­ç»ƒè¿‡ç¨‹ï¼Œå®ƒå­¦ä¹ é•¿åº¦å°ºåº¦çš„æœ€ä½³å€¼ã€‚è¿™é€šå¸¸é€šè¿‡ä½¿ç”¨éçº¿æ€§ä¼˜åŒ–å™¨æ¥å®Œæˆï¼Œä¾‹å¦‚**Broyden-Fletcher-Goldfarb-Shanno**ï¼ˆ**BFGS**ï¼‰ä¼˜åŒ–å™¨ã€‚å¯ä»¥ä½¿ç”¨å¤šç§ä¼˜åŒ–å™¨ï¼ŒåŒ…æ‹¬ä½ å¯èƒ½ç†Ÿæ‚‰çš„æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨ï¼Œä¾‹å¦‚éšæœºæ¢¯åº¦ä¸‹é™åŠå…¶å˜ç§ã€‚
- en: 'Letâ€™s take a look at how different kernels affect GP predictions. Weâ€™ll start
    with a straightforward example â€“ a simple sine wave:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ä¸åŒçš„æ ¸å‡½æ•°å¦‚ä½•å½±å“é«˜æ–¯è¿‡ç¨‹é¢„æµ‹ã€‚æˆ‘ä»¬ä»ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹å¼€å§‹â€”â€”ä¸€ä¸ªç®€å•çš„æ­£å¼¦æ³¢ï¼š
- en: '![PIC](img/file44.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file44.png)'
- en: 'FigureÂ 2.13: Plot of sine wave with four sampled points'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2.13ï¼šå¸¦æœ‰å››ä¸ªé‡‡æ ·ç‚¹çš„æ­£å¼¦æ³¢å›¾
- en: 'We can see the function illustrated here, as well as some points sampled from
    this function. Now, letâ€™s fit a GP with a periodic kernel to the data. The periodic
    kernel is defined as:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œå±•ç¤ºçš„å‡½æ•°ï¼Œä»¥åŠä»è¿™ä¸ªå‡½æ•°ä¸­é‡‡æ ·çš„ä¸€äº›ç‚¹ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä¸ºæ•°æ®æ‹Ÿåˆä¸€ä¸ªå…·æœ‰å‘¨æœŸæ€§æ ¸çš„é«˜æ–¯è¿‡ç¨‹ã€‚å‘¨æœŸæ€§æ ¸å‡½æ•°å®šä¹‰ä¸ºï¼š
- en: '![ â€² 2 ( 2sin2(Ï€ |x âˆ’ xâ€²|âˆ•p)) kper(x, x) = Ïƒ exp -------l2-------- ](img/file45.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![ â€² 2 ( 2sinÂ²(Ï€ |x âˆ’ xâ€²|âˆ•p)) kper(x, x) = Ïƒ exp -------lÂ²-------- ](img/file45.jpg)'
- en: 'Here, we see a new parameter: *p*. This is simply the period of the periodic
    function. Setting *p* = 1 and applying a GP with a periodic kernel to the preceding
    example, we get the following:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªæ–°çš„å‚æ•°ï¼š*p*ã€‚è¿™åªæ˜¯å‘¨æœŸå‡½æ•°çš„å‘¨æœŸã€‚è®¾å®š *p* = 1ï¼Œå¹¶å°†å…·æœ‰å‘¨æœŸæ€§æ ¸çš„é«˜æ–¯è¿‡ç¨‹åº”ç”¨åˆ°å‰é¢çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°å¦‚ä¸‹ç»“æœï¼š
- en: '![PIC](img/file46.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file46.png)'
- en: 'FigureÂ 2.14: Plot of posterior predictions from a periodic kernel with *p*
    = 1'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2.14ï¼šå‘¨æœŸæ€§æ ¸å‡½æ•°é¢„æµ‹ç»“æœå›¾ï¼Œ*p* = 1
- en: 'This looks pretty noisy, but you should be able to see that there is clear
    periodicity in the functions produced by the posterior. Itâ€™s noisy for a couple
    of reasons: a lack of data, and a poor prior. If weâ€™re limited on data, we can
    try to fix the problem by improving our prior. In this case, we can use our knowledge
    of the periodicity of the function to improve our prior by setting *p* = 6:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœ‹èµ·æ¥ç›¸å½“å˜ˆæ‚ï¼Œä½†ä½ åº”è¯¥èƒ½å¤Ÿçœ‹åˆ°åéªŒç”Ÿæˆçš„å‡½æ•°ä¸­å­˜åœ¨æ˜æ˜¾çš„å‘¨æœŸæ€§ã€‚å®ƒä¹‹æ‰€ä»¥å˜ˆæ‚ï¼ŒåŸå› æœ‰å‡ ä¸ªï¼šæ•°æ®ä¸è¶³å’Œå…ˆéªŒè¾ƒå·®ã€‚å¦‚æœæ•°æ®æœ‰é™ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•é€šè¿‡æ”¹å–„å…ˆéªŒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨æˆ‘ä»¬å¯¹å‡½æ•°å‘¨æœŸæ€§çš„çŸ¥è¯†ï¼Œé€šè¿‡è®¾ç½®*p*
    = 6æ¥æ”¹è¿›æˆ‘ä»¬çš„å…ˆéªŒï¼š
- en: '![PIC](img/file47.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file47.png)'
- en: 'FigureÂ 2.15: Plot of posterior predictions from a periodic kernel with *p*
    = 6'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.15ï¼šå‘¨æœŸæ€§æ ¸å‡½æ•°çš„åéªŒé¢„æµ‹å›¾ï¼Œ*p* = 6
- en: 'We see that this fits the data pretty well: weâ€™re still uncertain in regions
    for which we have little data, but the periodicity of our posterior now looks
    sensible. This is possible because weâ€™re using an informative prior; that is,
    a prior that incorporates information that describes the data well. This prior
    is composed of two key components:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°è¿™ä¸æ•°æ®æ‹Ÿåˆå¾—ç›¸å½“å¥½ï¼šæˆ‘ä»¬ä»ç„¶åœ¨æ•°æ®ç¨€ç¼ºçš„åŒºåŸŸå­˜åœ¨ä¸ç¡®å®šæ€§ï¼Œä½†åéªŒçš„å‘¨æœŸæ€§ç°åœ¨çœ‹èµ·æ¥åˆç†ã€‚è¿™ä¹‹æ‰€ä»¥èƒ½å®ç°ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªä¿¡æ¯ä¸°å¯Œçš„å…ˆéªŒï¼›å³ï¼ŒåŒ…å«äº†æè¿°æ•°æ®çš„å…³é”®ä¿¡æ¯çš„å…ˆéªŒã€‚è¿™ä¸ªå…ˆéªŒç”±ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ç»„æˆï¼š
- en: Our periodic kernel
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å‘¨æœŸæ€§æ ¸å‡½æ•°
- en: Our knowledge about the periodicity of the function
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å…³äºè¯¥å‡½æ•°å‘¨æœŸæ€§çš„çŸ¥è¯†
- en: 'We can see how important this is if we modify our GP to use an RBF kernel:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†é«˜æ–¯è¿‡ç¨‹ä¿®æ”¹ä¸ºä½¿ç”¨RBFæ ¸å‡½æ•°ï¼Œå°±èƒ½æ¸…æ¥šçœ‹åˆ°è¿™ä¸ªå·®å¼‚ï¼š
- en: '![PIC](img/file48.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file48.png)'
- en: 'FigureÂ 2.16: Plot of posterior predictions from an RBF kernel'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.16ï¼šRBFæ ¸å‡½æ•°çš„åéªŒé¢„æµ‹å›¾
- en: 'With an RBF kernel, we see that things are looking pretty chaotic again: because
    we have limited data and a poor prior, weâ€™re unable to appropriately constrain
    the space of possible functions to fit our true function. In the ideal case, weâ€™d
    fix this by using a more appropriate prior, as we saw in *Figure* [*2.15*](#x1-31013r15)
    â€“ but this isnâ€™t always possible. Another solution is to sample more data. Sticking
    with our RBF kernel, we sample 10 data points from our function and re-train our
    GP:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨RBFæ ¸å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°æƒ…å†µåˆå˜å¾—ç›¸å½“æ··ä¹±ï¼šç”±äºæˆ‘ä»¬åªæœ‰æœ‰é™çš„æ•°æ®å’Œè¾ƒå·®çš„å…ˆéªŒï¼Œæˆ‘ä»¬æ— æ³•æ°å½“åœ°çº¦æŸå¯èƒ½çš„å‡½æ•°ç©ºé—´ä»¥æ‹Ÿåˆæˆ‘ä»¬çš„çœŸå®å‡½æ•°ã€‚åœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨æ›´åˆé€‚çš„å…ˆéªŒæ¥è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå°±åƒåœ¨*å›¾*
    [*2.15*](#x1-31013r15)ä¸­çœ‹åˆ°çš„é‚£æ ·â€”â€”ä½†è¿™å¹¶ä¸æ€»æ˜¯å¯è¡Œçš„ã€‚å¦ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯å¢åŠ æ•°æ®é‡ã€‚ç»§ç»­ä½¿ç”¨æˆ‘ä»¬çš„RBFæ ¸å‡½æ•°ï¼Œæˆ‘ä»¬ä»å‡½æ•°ä¸­é‡‡æ ·äº†10ä¸ªæ•°æ®ç‚¹å¹¶é‡æ–°è®­ç»ƒäº†æˆ‘ä»¬çš„é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰ï¼š
- en: '![PIC](img/file49.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file49.png)'
- en: 'FigureÂ 2.17: Plot of posterior predictions from an RBF kernel, trained on 10
    observations'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.17ï¼šåŸºäº10ä¸ªè§‚æµ‹å€¼è®­ç»ƒçš„RBFæ ¸å‡½æ•°çš„åéªŒé¢„æµ‹å›¾
- en: This is looking much better â€“ but what if we have more data *and* an informative
    prior?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœ‹èµ·æ¥å¥½å¤šäº†â€”â€”ä½†å¦‚æœæˆ‘ä»¬æœ‰æ›´å¤šæ•°æ®*å¹¶ä¸”*ä¸€ä¸ªä¿¡æ¯ä¸°å¯Œçš„å…ˆéªŒå‘¢ï¼Ÿ
- en: '![PIC](img/file50.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file50.png)'
- en: 'FigureÂ 2.18: Plot of posterior predictions from a periodic kernel with *p*
    = 6, trained on 10 observations'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.18ï¼šå‘¨æœŸæ€§æ ¸å‡½æ•°çš„åéªŒé¢„æµ‹å›¾ï¼Œ*p* = 6ï¼ŒåŸºäº10ä¸ªè§‚æµ‹å€¼è®­ç»ƒ
- en: The posterior now fits our true function very closely. Because we donâ€™t have
    infinite data, there are still some areas of uncertainty, but the uncertainty
    is relatively small.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨åéªŒé¢„æµ‹éå¸¸æ¥è¿‘çœŸå®å‡½æ•°ã€‚å› ä¸ºæˆ‘ä»¬æ²¡æœ‰æ— é™çš„æ•°æ®ï¼Œä»ç„¶å­˜åœ¨ä¸€äº›ä¸ç¡®å®šåŒºåŸŸï¼Œä½†ä¸ç¡®å®šæ€§ç›¸å¯¹è¾ƒå°ã€‚
- en: 'Now that weâ€™ve seen some of the core principles in action, letâ€™s return to
    our example from *Figures* [*2.10*](#x1-30002r10)*-*[*2.12*](#x1-30007r12). Hereâ€™s
    a quick reminder of our target function, our posterior samples, and the linear
    interpolation we saw earlier:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»çœ‹åˆ°ä¸€äº›æ ¸å¿ƒåŸç†çš„å®é™…åº”ç”¨ï¼Œè®©æˆ‘ä»¬å›åˆ°*å›¾* [*2.10*](#x1-30002r10)-[*2.12*](#x1-30007r12)ä¸­çš„ä¾‹å­ã€‚è¿™é‡Œæ˜¯æˆ‘ä»¬ç›®æ ‡å‡½æ•°ã€åéªŒæ ·æœ¬å’Œä¹‹å‰çœ‹åˆ°çš„çº¿æ€§æ’å€¼çš„å¿«é€Ÿå›é¡¾ï¼š
- en: '![PIC](img/file51.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file51.png)'
- en: 'FigureÂ 2.19: Plot illustrating the difference between linear interpolation
    and the true function'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.19ï¼šå±•ç¤ºçº¿æ€§æ’å€¼ä¸çœŸå®å‡½æ•°å·®å¼‚çš„å›¾
- en: 'Now that weâ€™ve got some idea of how a GP will affect our predictive posterior,
    itâ€™s easy to see that linear interpolation falls very short of what we achieve
    with a GP. To illustrate this more clearly, letâ€™s take a look at what the GP prediction
    would be for this function given the three samples:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯¹é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰å¦‚ä½•å½±å“é¢„æµ‹åéªŒæœ‰äº†ä¸€äº›äº†è§£ï¼Œå¾ˆå®¹æ˜“çœ‹å‡ºçº¿æ€§æ’å€¼è¿œè¿œè¾¾ä¸åˆ°æˆ‘ä»¬é€šè¿‡é«˜æ–¯è¿‡ç¨‹å®ç°çš„æ•ˆæœã€‚ä¸ºäº†æ›´æ¸…æ¥šåœ°è¯´æ˜è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹åœ¨ç»™å®šä¸‰ä¸ªæ ·æœ¬çš„æƒ…å†µä¸‹ï¼Œé«˜æ–¯è¿‡ç¨‹é¢„æµ‹è¿™ä¸ªå‡½æ•°çš„ç»“æœï¼š
- en: '![PIC](img/file52.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file52.png)'
- en: 'FigureÂ 2.20: Plot illustrating the difference between GP predictions and the
    true function'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.20ï¼šå±•ç¤ºé«˜æ–¯è¿‡ç¨‹é¢„æµ‹ä¸çœŸå®å‡½æ•°å·®å¼‚çš„å›¾
- en: 'Here, the dotted lines are our mean (*Î¼*) predictions from the GP, and the
    shaded area is the uncertainty associated with those predictions â€“ the standard
    deviation (*Ïƒ*) around the mean. Letâ€™s contrast what we see in *Figure* [*2.20*](#x1-31026r20)
    with *Figure* [*2.19*](#x1-31024r19). The differences may seem subtle at first,
    but we can clearly see that this is no longer a straightforward linear interpolation:
    the predicted values from the GP are being â€pulledâ€ toward our actual function
    values. As with our earlier sine wave examples, the behavior of the GP predictions
    are affected by two key factors: the prior (or kernel) and the data.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œè™šçº¿è¡¨ç¤ºæˆ‘ä»¬é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰çš„å‡å€¼ (*Î¼*) é¢„æµ‹å€¼ï¼Œé˜´å½±åŒºåŸŸè¡¨ç¤ºä¸è¿™äº›é¢„æµ‹å€¼ç›¸å…³çš„ä¸ç¡®å®šæ€§â€”â€”å³å‡å€¼å‘¨å›´çš„æ ‡å‡†å·® (*Ïƒ*)ã€‚è®©æˆ‘ä»¬å°† *å›¾*
    [*2.20*](#x1-31026r20) ä¸­çœ‹åˆ°çš„å†…å®¹ä¸ *å›¾* [*2.19*](#x1-31024r19) è¿›è¡Œå¯¹æ¯”ã€‚å·®å¼‚ä¸€å¼€å§‹å¯èƒ½çœ‹èµ·æ¥å¾ˆå¾®å¦™ï¼Œä½†æˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œè¿™ä¸å†æ˜¯ä¸€ä¸ªç®€å•çš„çº¿æ€§æ’å€¼ï¼šé«˜æ–¯è¿‡ç¨‹çš„é¢„æµ‹å€¼æ­£åœ¨â€œæ‹‰â€å‘æˆ‘ä»¬çš„å®é™…å‡½æ•°å€¼ã€‚å°±åƒæˆ‘ä»¬ä¹‹å‰çš„æ­£å¼¦æ³¢ç¤ºä¾‹ä¸€æ ·ï¼Œé«˜æ–¯è¿‡ç¨‹é¢„æµ‹çš„è¡Œä¸ºå—ä¸¤å¤§å…³é”®å› ç´ çš„å½±å“ï¼šå…ˆéªŒï¼ˆæˆ–æ ¸å‡½æ•°ï¼‰å’Œæ•°æ®ã€‚
- en: 'But thereâ€™s another crucial detail illustrated in *Figure* [*2.20*](#x1-31026r20):
    the predictive uncertainties from our GP. We see that, unlike many typical ML
    models, a GP gives us uncertainties associated with its predictions. This means
    we can make better decisions about what we do with the modelâ€™s predictions â€“ having
    this information will help us to ensure that our systems are more robust. For
    example, if the uncertainty is too great, we can fall back to a manual system.
    We can even keep track of data points with high predictive uncertainty so that
    we can continuously refine our models.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†åœ¨ *å›¾* [*2.20*](#x1-31026r20) ä¸­è¿˜å±•ç¤ºäº†å¦ä¸€ä¸ªè‡³å…³é‡è¦çš„ç»†èŠ‚ï¼šæˆ‘ä»¬é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰çš„é¢„æµ‹ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¸è®¸å¤šå…¸å‹çš„æœºå™¨å­¦ä¹ æ¨¡å‹ä¸åŒï¼Œé«˜æ–¯è¿‡ç¨‹ä¼šç»™å‡ºä¸å…¶é¢„æµ‹ç›¸å…³çš„ä¸ç¡®å®šæ€§ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°å†³å®šå¦‚ä½•ä½¿ç”¨æ¨¡å‹çš„é¢„æµ‹â€”â€”æ‹¥æœ‰è¿™äº›ä¿¡æ¯å°†å¸®åŠ©æˆ‘ä»¬ç¡®ä¿æˆ‘ä»¬çš„ç³»ç»Ÿæ›´ä¸ºç¨³å¥ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸ç¡®å®šæ€§å¤ªå¤§ï¼Œæˆ‘ä»¬å¯ä»¥å›é€€åˆ°æ‰‹åŠ¨ç³»ç»Ÿã€‚æˆ‘ä»¬ç”šè‡³å¯ä»¥è¿½è¸ªé‚£äº›å…·æœ‰è¾ƒé«˜é¢„æµ‹ä¸ç¡®å®šæ€§çš„æ•°æ®ç‚¹ï¼Œä»¥ä¾¿ä¸æ–­æ”¹è¿›æˆ‘ä»¬çš„æ¨¡å‹ã€‚
- en: 'We can see how this refinement affects our predictions by adding a few more
    observations â€“ just as we did in the earlier examples:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡å¢åŠ ä¸€äº›è§‚æµ‹ç‚¹æ¥çœ‹åˆ°è¿™ç§æ”¹è¿›å¦‚ä½•å½±å“æˆ‘ä»¬çš„é¢„æµ‹â€”â€”å°±åƒæˆ‘ä»¬åœ¨ä¹‹å‰çš„ç¤ºä¾‹ä¸­åšçš„é‚£æ ·ï¼š
- en: '![PIC](img/file53.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file53.png)'
- en: 'FigureÂ 2.21: Plot illustrating the difference between GP predictions and the
    true function, trained on 5 observations'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 2.21ï¼šå±•ç¤ºåœ¨ 5 ä¸ªè§‚æµ‹ç‚¹ä¸Šè®­ç»ƒçš„é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰é¢„æµ‹ä¸çœŸå®å‡½æ•°ä¹‹é—´å·®å¼‚çš„å›¾è¡¨
- en: '*Figure* [*2.21*](#x1-31029r21) illustrates how our uncertainty changes over
    regions with different numbers of observations. We see here that between *x* =
    3 and *x* = 4 our uncertainty is quite high. This makes a lot of sense, as we
    can also see that our GPâ€™s mean predictions deviate significantly from the true
    function values. Conversely, if we look at the region between *x* = 0*.*5 and
    *x* = 2, we can see that our GPâ€™s predictions follow the true function fairly
    closely, and our model is also more confident about these predictions, as we can
    see from the smaller interval of uncertainty in this region.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾* [*2.21*](#x1-31029r21) å±•ç¤ºäº†æˆ‘ä»¬çš„ä¸ç¡®å®šæ€§å¦‚ä½•åœ¨ä¸åŒæ•°é‡è§‚æµ‹çš„åŒºåŸŸä¹‹é—´å˜åŒ–ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ *x* = 3 å’Œ *x*
    = 4 ä¹‹é—´ï¼Œæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§ç›¸å½“é«˜ã€‚è¿™å¾ˆæœ‰é“ç†ï¼Œå› ä¸ºæˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰çš„å‡å€¼é¢„æµ‹å€¼ä¸çœŸå®å‡½æ•°å€¼å­˜åœ¨è¾ƒå¤§åå·®ã€‚ç›¸åï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹ *x*
    = 0.5 å’Œ *x* = 2 ä¹‹é—´çš„åŒºåŸŸï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„ GP é¢„æµ‹å€¼ä¸çœŸå®å‡½æ•°éå¸¸æ¥è¿‘ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯¹è¿™äº›é¢„æµ‹ä¹Ÿæ›´æœ‰ä¿¡å¿ƒï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ä»è¿™ä¸ªåŒºåŸŸçš„ä¸ç¡®å®šæ€§é—´éš”è¾ƒå°ä¸­çœ‹å‡ºã€‚'
- en: 'What weâ€™re seeing here is a great example of a very important concept: **well**
    **calibrated uncertainty** â€“ also termed **high-quality uncertainty**. This refers
    to the fact that, in regions where our predictions are inaccurate, our uncertainty
    is also high. Our uncertainty estimates are **poorly calibrated** if weâ€™re very
    confident in regions with inaccurate predictions, or very uncertain in regions
    with accurate predictions.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„æ˜¯ä¸€ä¸ªéå¸¸é‡è¦æ¦‚å¿µçš„å…¸èŒƒï¼š**è‰¯å¥½** **æ ¡å‡†çš„ä¸ç¡®å®šæ€§**â€”â€”ä¹Ÿç§°ä¸º **é«˜è´¨é‡çš„ä¸ç¡®å®šæ€§**ã€‚è¿™æŒ‡çš„æ˜¯åœ¨æˆ‘ä»¬é¢„æµ‹ä¸å‡†ç¡®çš„åŒºåŸŸï¼Œæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§ä¹Ÿå¾ˆé«˜ã€‚å¦‚æœæˆ‘ä»¬åœ¨é¢„æµ‹ä¸å‡†ç¡®çš„åŒºåŸŸéå¸¸è‡ªä¿¡ï¼Œæˆ–åœ¨é¢„æµ‹å‡†ç¡®çš„åŒºåŸŸéå¸¸ä¸ç¡®å®šï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§ä¼°è®¡å°±æ˜¯
    **æ ¡å‡†ä¸è‰¯çš„**ã€‚
- en: 'GPs are what we can term a **well principled** method â€“ this means that they
    have solid mathematical foundations, and thus come with strong theoretical guarantees.
    One of these guarantees is that they are well calibrated, and this is what makes
    GPs so popular: if we use GPs, we know we can rely on their uncertainty estimates.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰æ˜¯ä¸€ç§æˆ‘ä»¬å¯ä»¥ç§°ä¹‹ä¸º **åŸç†æ˜ç¡®** çš„æ–¹æ³•â€”â€”è¿™æ„å‘³ç€å®ƒä»¬æœ‰åšå®çš„æ•°å­¦åŸºç¡€ï¼Œå› æ­¤å…·æœ‰å¼ºå¤§çš„ç†è®ºä¿éšœã€‚å…¶ä¸­ä¹‹ä¸€æ˜¯å®ƒä»¬å…·æœ‰è‰¯å¥½çš„æ ¡å‡†æ€§ï¼Œè¿™ä¹Ÿæ˜¯é«˜æ–¯è¿‡ç¨‹å¦‚æ­¤å—æ¬¢è¿çš„åŸå› ï¼šå¦‚æœæˆ‘ä»¬ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ï¼Œæˆ‘ä»¬çŸ¥é“å¯ä»¥ä¾èµ–å®ƒä»¬çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚
- en: Unfortunately, however, GPs are not without their shortcomings â€“ weâ€™ll learn
    more about these in the following section.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼ŒGPå¹¶éæ²¡æœ‰ç¼ºç‚¹â€”â€”æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­è¯¦ç»†äº†è§£è¿™äº›é—®é¢˜ã€‚
- en: 2.3.2 Limitations of Gaussian processes
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 é«˜æ–¯è¿‡ç¨‹çš„å±€é™æ€§
- en: 'Given the fact that GPs are well-principled and capable of producing high-quality
    uncertainty estimates, youâ€™d be forgiven for thinking theyâ€™re the perfect uncertainty-aware
    ML model. GPs struggle in a few key situations:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘åˆ°GPå…·æœ‰è‰¯å¥½çš„åŸç†åŸºç¡€ï¼Œå¹¶ä¸”èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæ‚¨å¯èƒ½ä¼šè®¤ä¸ºå®ƒä»¬æ˜¯å®Œç¾çš„ã€ä¸ç¡®å®šæ€§æ„ŸçŸ¥å‹æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ç„¶è€Œï¼ŒGPåœ¨å‡ ä¸ªå…³é”®æƒ…å¢ƒä¸‹è¡¨ç°ä¸ä½³ï¼š
- en: High-dimensional data
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é«˜ç»´æ•°æ®
- en: Large amounts of data
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¤§é‡æ•°æ®
- en: Highly complex data
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é«˜åº¦å¤æ‚çš„æ•°æ®
- en: The first two points here are largely down to the inability of GPs to scale
    well. To understand this, we just need to look at the training and inference procedures
    for GPs. While itâ€™s beyond the scope of this book to cover this in detail, the
    key point here is in the matrix operations required for GP training.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„å‰ä¸¤ç‚¹ä¸»è¦å½’å› äºGPåœ¨æ‰©å±•æ€§ä¸Šçš„ä¸è¶³ã€‚ä¸ºäº†ç†è§£è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åªéœ€è¦çœ‹çœ‹GPçš„è®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚å°½ç®¡è¯¦ç»†è®¨è®ºè¿™äº›å†…å®¹è¶…å‡ºäº†æœ¬ä¹¦çš„èŒƒå›´ï¼Œä½†å…³é”®ç‚¹åœ¨äºGPè®­ç»ƒæ‰€éœ€çš„çŸ©é˜µè¿ç®—ã€‚
- en: 'During training, it is necessary to invert a *D* Ã— *D* matrix, where *D* is
    the dimensionality of our data. Because of this, GP training quickly becomes computationally
    prohibitive. This can be somewhat alleviated through the use of Cholesky deomposition,
    rather than direct matrix inversion. As well as being more computationally efficient,
    Cholesky decomposition is also more numerically stable. Unfortunately, Cholesky
    decomposition also has its weaknesses: computationally, its complexity is *O*(*n*Â³).
    This means that, as the size of our dataset increases, GP training becomes more
    and more expensive.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéœ€è¦å¯¹ä¸€ä¸ª *D* Ã— *D* çš„çŸ©é˜µè¿›è¡Œæ±‚é€†ï¼Œå…¶ä¸­ *D* æ˜¯æ•°æ®çš„ç»´åº¦ã€‚å› æ­¤ï¼ŒGPçš„è®­ç»ƒå¾ˆå¿«å˜å¾—è®¡ç®—å¼€é”€å·¨å¤§ã€‚é€šè¿‡ä½¿ç”¨Choleskyåˆ†è§£æ¥ä»£æ›¿ç›´æ¥çŸ©é˜µæ±‚é€†ï¼Œå¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£è¿™ä¸€é—®é¢˜ã€‚Choleskyåˆ†è§£ä¸ä»…åœ¨è®¡ç®—ä¸Šæ›´é«˜æ•ˆï¼Œè€Œä¸”æ•°å€¼ç¨³å®šæ€§ä¹Ÿæ›´å¥½ã€‚ä¸å¹¸çš„æ˜¯ï¼ŒCholeskyåˆ†è§£ä¹Ÿæœ‰å…¶ä¸è¶³ä¹‹å¤„ï¼šåœ¨è®¡ç®—ä¸Šï¼Œå®ƒçš„å¤æ‚åº¦æ˜¯
    *O*(*n*Â³)ã€‚è¿™æ„å‘³ç€ï¼Œéšç€æ•°æ®é›†å¤§å°çš„å¢åŠ ï¼ŒGPè®­ç»ƒå˜å¾—è¶Šæ¥è¶Šæ˜‚è´µã€‚
- en: 'But itâ€™s not only training thatâ€™s affected: because we need to compute the
    covariance between a new data point and all observed data points at inference,
    GPs have a *O*(*n*Â²) computational complexity at inference.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å—å½±å“çš„ä¸ä»…ä»…æ˜¯è®­ç»ƒè¿‡ç¨‹ï¼šå› ä¸ºåœ¨æ¨ç†æ—¶æˆ‘ä»¬éœ€è¦è®¡ç®—æ–°æ•°æ®ç‚¹ä¸æ‰€æœ‰å·²è§‚å¯Ÿæ•°æ®ç‚¹ä¹‹é—´çš„åæ–¹å·®ï¼ŒGPï¼ˆé«˜æ–¯è¿‡ç¨‹ï¼‰åœ¨æ¨ç†æ—¶çš„è®¡ç®—å¤æ‚åº¦ä¸º *O*(*n*Â²)ã€‚
- en: 'As well as the computational cost, GPs arenâ€™t light in memory: because we need
    to store our covariance matrix **K**, GPs have a *O*(*n*Â²) memory complexity.
    Thus, in the case of large datasets, even if we have the compute resources necessary
    to train them, it may not be practical to use them in real-world applications
    due to their memory requirements.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†è®¡ç®—æˆæœ¬ï¼ŒGPåœ¨å†…å­˜ä½¿ç”¨ä¸Šä¹Ÿä¸è½»ä¾¿ï¼šå› ä¸ºæˆ‘ä»¬éœ€è¦å­˜å‚¨åæ–¹å·®çŸ©é˜µ **K**ï¼ŒGPçš„å†…å­˜å¤æ‚åº¦ä¸º *O*(*n*Â²)ã€‚å› æ­¤ï¼Œåœ¨å¤§æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œå³ä½¿æˆ‘ä»¬æ‹¥æœ‰è®­ç»ƒå®ƒä»¬æ‰€éœ€çš„è®¡ç®—èµ„æºï¼Œç”±äºå…¶å†…å­˜éœ€æ±‚ï¼Œä½¿ç”¨å®ƒä»¬è¿›è¡Œå®é™…åº”ç”¨å¯èƒ½ä¹Ÿä¸ç°å®ã€‚
- en: The last point in our list concerns the complexity of data. As you are probably
    aware â€“ and as weâ€™ll touch on in *Chapter 3, Fundamentals of Deep Learning* â€“
    one of the major advantages of DNNs is their ability to process complex, high-dimensional
    data through layers of non-linear transformations. While GPs are powerful, theyâ€™re
    also relatively simple models, and theyâ€™re not able to learn the kinds of powerful
    feature representations that are possible with DNNs.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ¸…å•ä¸­çš„æœ€åä¸€ç‚¹æ¶‰åŠæ•°æ®çš„å¤æ‚æ€§ã€‚æ­£å¦‚ä½ å¯èƒ½å·²ç»çŸ¥é“çš„â€”â€”å¹¶ä¸”æˆ‘ä»¬å°†åœ¨ *ç¬¬3ç«  æ·±åº¦å­¦ä¹ åŸºç¡€* ä¸­æåˆ°â€”â€”DNNï¼ˆæ·±åº¦ç¥ç»ç½‘ç»œï¼‰çš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯å®ƒä»¬èƒ½å¤Ÿé€šè¿‡å±‚å±‚éçº¿æ€§å˜æ¢å¤„ç†å¤æ‚çš„é«˜ç»´æ•°æ®ã€‚è™½ç„¶GPå¾ˆå¼ºå¤§ï¼Œä½†å®ƒä»¬ä¹Ÿæ˜¯ç›¸å¯¹ç®€å•çš„æ¨¡å‹ï¼Œæ— æ³•å­¦ä¹ DNNæ‰€èƒ½å®ç°çš„å¼ºå¤§ç‰¹å¾è¡¨ç¤ºã€‚
- en: 'All of these factors mean that, while GPs are an excellent choice for relatively
    low-dimensional data and reasonably small datasets, they arenâ€™t practical for
    many of the complex problems we face in ML. And so, we turn to BDL methods: methods
    that have the flexibility and scalability of deep learning, while also producing
    model uncertainty estimates.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›å› ç´ è¡¨æ˜ï¼Œè™½ç„¶GPå¯¹äºç›¸å¯¹ä½ç»´æ•°æ®å’Œè¾ƒå°çš„æ•°æ®é›†æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©ï¼Œä½†å¯¹äºæˆ‘ä»¬åœ¨æœºå™¨å­¦ä¹ ä¸­é¢ä¸´çš„è®¸å¤šå¤æ‚é—®é¢˜æ¥è¯´ï¼Œå®ƒä»¬å¹¶ä¸å®ç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è½¬å‘BDLæ–¹æ³•ï¼šè¿™äº›æ–¹æ³•å…·å¤‡æ·±åº¦å­¦ä¹ çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼ŒåŒæ—¶è¿˜èƒ½å¤Ÿæä¾›æ¨¡å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚
- en: 2.4 Summary
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 æ€»ç»“
- en: 'In this chapter, weâ€™ve covered some of the fundamental concepts and methods
    related to Bayesian inference. First, we reviewed Bayesâ€™ theorem and the fundamentals
    of probability theory â€“ allowing us to understand the concept of uncertainty,
    as well as how we apply it to the predictions of ML models. Next, we introduced
    sampling, and an important class of algorithms: Markov Chain Monte Carlo, or MCMC,
    methods. Lastly, we covered Gaussian processes, and illustrated the crucial concept
    of well calibrated uncertainty. These key topics will provide you with the necessary
    foundation for the content that will follow, however, we encourage you to explore
    the recommended reading materials for a more comprehensive treatment of the topics
    introduced in this chapter.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€äº›ä¸è´å¶æ–¯æ¨æ–­ç›¸å…³çš„åŸºæœ¬æ¦‚å¿µå’Œæ–¹æ³•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å›é¡¾äº†è´å¶æ–¯å®šç†å’Œæ¦‚ç‡è®ºçš„åŸºç¡€çŸ¥è¯†â€”â€”ä½¿æˆ‘ä»¬èƒ½å¤Ÿç†è§£ä¸ç¡®å®šæ€§çš„æ¦‚å¿µï¼Œä»¥åŠå¦‚ä½•å°†å…¶åº”ç”¨äºæœºå™¨å­¦ä¹ æ¨¡å‹çš„é¢„æµ‹ä¸­ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»‹ç»äº†æŠ½æ ·å’Œä¸€ä¸ªé‡è¦çš„ç®—æ³•ç±»åˆ«ï¼šé©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›æ–¹æ³•ï¼ˆMCMCï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬è®²è§£äº†é«˜æ–¯è¿‡ç¨‹ï¼Œå¹¶é˜æ˜äº†è‰¯å¥½æ ¡å‡†ä¸ç¡®å®šæ€§çš„å…³é”®æ¦‚å¿µã€‚è¿™äº›æ ¸å¿ƒä¸»é¢˜å°†ä¸ºåç»­å†…å®¹æä¾›å¿…è¦çš„åŸºç¡€ï¼Œç„¶è€Œï¼Œæˆ‘ä»¬é¼“åŠ±æ‚¨é˜…è¯»æ¨èçš„å‚è€ƒææ–™ï¼Œä»¥ä¾¿æ›´å…¨é¢åœ°å­¦ä¹ æœ¬ç« ä¸­ä»‹ç»çš„ä¸»é¢˜ã€‚
- en: In the next chapter, we will see how DNNs have changed the landscape of machine
    learning over the last decade, exploring the tremendous advantages offered by
    deep learning, and the motivation behind the development of BDL methods.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°æ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNï¼‰å¦‚ä½•åœ¨è¿‡å»åå¹´ä¸­æ”¹å˜æœºå™¨å­¦ä¹ çš„æ ¼å±€ï¼Œæ¢ç´¢æ·±åº¦å­¦ä¹ å¸¦æ¥çš„å·¨å¤§ä¼˜åŠ¿ï¼Œä»¥åŠBDLæ–¹æ³•å‘å±•çš„åŠ¨æœºã€‚
- en: 2.5 Further reading
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 è¿›ä¸€æ­¥é˜…è¯»
- en: 'There are a variety of techniques being explored to improve the flexibility
    and scalability of GPs â€“ such as Deep GPs or Sparse GPs. The following resources
    explore some of these topics, and also provide a more thorough treatment of the
    content covered in this chapter:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æé«˜é«˜æ–¯è¿‡ç¨‹ï¼ˆGPï¼‰çš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§ï¼Œæ­£åœ¨æ¢ç´¢å¤šç§æŠ€æœ¯â€”â€”ä¾‹å¦‚æ·±åº¦é«˜æ–¯è¿‡ç¨‹ï¼ˆDeep GPsï¼‰æˆ–ç¨€ç–é«˜æ–¯è¿‡ç¨‹ï¼ˆSparse GPsï¼‰ã€‚ä»¥ä¸‹èµ„æºæ¢è®¨äº†å…¶ä¸­ä¸€äº›ä¸»é¢˜ï¼Œå¹¶å¯¹æœ¬ç« å†…å®¹è¿›è¡Œäº†æ›´æ·±å…¥çš„é˜è¿°ï¼š
- en: '*Bayesian Analysis with Python*, Martin: this book comprehensively covers core
    topics in statistical modeling and probabilistic programming, and includes practical
    walk-throughs of various sampling methods, as well as a good overview of Gaussian
    processes and a variety of other techniques core to Bayesian analysis.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ã€ŠPython è´å¶æ–¯åˆ†æã€‹*ï¼Œé©¬ä¸ï¼šè¿™æœ¬ä¹¦å…¨é¢è¦†ç›–äº†ç»Ÿè®¡å»ºæ¨¡å’Œæ¦‚ç‡ç¼–ç¨‹çš„æ ¸å¿ƒä¸»é¢˜ï¼ŒåŒ…æ‹¬å¯¹å„ç§æŠ½æ ·æ–¹æ³•çš„å®ç”¨æ¼”ç¤ºï¼Œä»¥åŠå¯¹é«˜æ–¯è¿‡ç¨‹å’Œå…¶ä»–å¤šç§è´å¶æ–¯åˆ†ææ ¸å¿ƒæŠ€æœ¯çš„è‰¯å¥½æ¦‚è¿°ã€‚'
- en: '*Gaussian Processes for Machine Learning*, Rasmussen and Williams: this is
    often considered the definitive text on Gaussian processes, and provides highly
    detailed explanations of the theory underlying Gaussian processes. A key text
    for anyone serious about Bayesian inference.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ã€Šæœºå™¨å­¦ä¹ ä¸­çš„é«˜æ–¯è¿‡ç¨‹ã€‹*ï¼Œæ‹‰æ–¯ç©†æ£®å’Œå¨å»‰å§†æ–¯ï¼šè¿™æœ¬ä¹¦é€šå¸¸è¢«è§†ä¸ºé«˜æ–¯è¿‡ç¨‹çš„æƒå¨æ•™æï¼Œæä¾›äº†é«˜æ–¯è¿‡ç¨‹èƒŒåç†è®ºçš„è¯¦ç»†è§£é‡Šã€‚å¯¹äºä»»ä½•è®¤çœŸå­¦ä¹ è´å¶æ–¯æ¨æ–­çš„äººæ¥è¯´ï¼Œè¿™æ˜¯ä¸€æœ¬å…³é”®çš„æ•™æã€‚'
