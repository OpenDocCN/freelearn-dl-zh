- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introduction to Large Language Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型简介
- en: In the world of technology, **Chat Generative Pre-trained Transformer** (**ChatGPT**)
    is a **large language model** (**LLM**)-based chatbot that was launched by OpenAI
    on November 30, 2022\. Just in ChatGPT’s first week, over a million people started
    using the technology. This is an important moment because it shows how regular
    people are now using generative **artificial intelligence** (**AI**) in their
    daily lives. By January 2023, ChatGPT had over 100 million users, making it the
    fastest-growing application in history and making OpenAI, the company behind it,
    worth $29 billion.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术的世界里，**聊天生成预训练变换器**（**ChatGPT**）是一个基于**大型语言模型**（**LLM**）的聊天机器人，由OpenAI于2022年11月30日发布。在ChatGPT的首周内，已有超过一百万人开始使用这项技术。这是一个重要的时刻，因为它展示了普通人现在如何在日常生活中使用生成性**人工智能**（**AI**）。到2023年1月，ChatGPT的用户数超过了1亿，成为历史上增长最快的应用，并使得背后的公司OpenAI的估值达到了290亿美元。
- en: In this introductory chapter, we’ll establish the basic concepts behind LLMs,
    look at some examples, understand the concept of foundation models, and provide
    various business use cases where LLMs can be applied to solve complex problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节中，我们将建立LLM的基本概念，查看一些示例，理解基础模型的概念，并提供多个业务应用场景，展示LLM如何解决复杂问题。
- en: 'This chapter covers the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What are LLMs?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是LLM？
- en: LLM examples
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM示例
- en: The concept of foundation models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础模型的概念
- en: LLM use cases
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的应用场景
- en: What are LLMs?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是LLM？
- en: LLMs are a modern breakthrough in deep learning that focuses on human languages.
    They’ve shown themselves to be useful in many ways, such as content creation,
    customer support, coding assistance, education and tutoring, medical diagnosis,
    sentiment analysis, legal assistance, and more. Simply put, an LLM is a kind of
    smart computer program that can understand and create text like humans can by
    using large transformer models under the hood. The Transformer architecture enables
    models to understand context and relationships within data more effectively, making
    it particularly powerful for tasks involving human language and sequential data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LLM是深度学习中的现代突破，专注于人类语言。它们在许多方面表现出了巨大的实用性，例如内容创作、客户支持、编码辅助、教育与辅导、医疗诊断、情感分析、法律援助等。简单来说，LLM是一种智能计算机程序，能够像人类一样理解和生成文本，并且其底层使用了大型变换器模型。变换器架构使得模型能更有效地理解数据中的上下文和关系，这使得它在涉及人类语言和序列数据的任务中尤为强大。
- en: For humans, text is a bunch of words put together. We read sentences, sentences
    make up paragraphs, and paragraphs make up chapters in a document. But for computers,
    text is just a series of letters and symbols. To make computers understand text,
    we can create a model using something called recurrent neural networks. This model
    goes through the text one word or character at a time and gives an answer when
    it finishes reading everything. This model is good, but sometimes, when it gets
    to the end of a block of text, it has trouble recalling the text from the beginning
    of that block. This is where the Transformer architecture shines. The key innovation
    of the Transformer architecture was its use of the self-attention mechanism, which
    allowed it to capture relationships between different parts of a sequence more
    effectively than previous models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对人类来说，文本是一堆拼凑在一起的单词。我们阅读句子，句子组成段落，段落组成文档中的章节。但是对计算机来说，文本只是一个由字母和符号组成的序列。为了让计算机理解文本，我们可以创建一个使用所谓的循环神经网络的模型。这个模型一次处理一个单词或字符，当它读完所有内容时，会给出一个答案。这个模型效果不错，但有时当它读取到一段文本的结尾时，它很难回忆起该段文本的开头部分。这就是变换器架构的亮点所在。变换器架构的关键创新是使用了自注意力机制，这使得它能够比以前的模型更有效地捕捉序列中不同部分之间的关系。
- en: Back in 2017, Ashish Vaswani and their team wrote a paper called *Attention
    is All You Need* ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf))
    to introduce a new model called the Transformer. This model uses something called
    attention. Unlike the old way in which recurrent neural networks process text,
    attention lets you look at a whole sentence or even a whole paragraph all at once,
    instead of just one word at a time. This helps the transformer to better “understand”
    words as a result of the added context. Nowadays, many of the best LLMs are built
    on transformers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 早在 2017 年，Ashish Vaswani 和他们的团队写了一篇名为 *Attention is All You Need* ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf))
    的论文，介绍了一种叫做 Transformer 的新模型。这个模型使用了一种叫做注意力（attention）的技术。与旧的递归神经网络处理文本的方式不同，注意力可以让你一次性查看整个句子，甚至是整段话，而不是一次只看一个单词。这有助于
    Transformer 更好地“理解”单词，因为它获得了更多的上下文信息。如今，许多最好的大型语言模型（LLM）都是基于 Transformer 构建的。
- en: When you want a Transformer model to understand a piece of text, you must break
    it down into separate words or parts called tokens. These tokens are then turned
    into numbers and mapped to special codes called embeddings, which are like special
    maps that store the semantic meaning of the tokens. Finally, the transformer’s
    encoder takes these embeddings and turns them into a representation. This “representation”
    is a vector that captures the contextual meaning of the input tokens, allowing
    the model to understand and process the input more effectively. In simple terms,
    you can think of it as putting all the pieces together to understand the whole
    story.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当你希望一个 Transformer 模型理解一段文本时，必须将其拆解为单独的单词或称为 token 的部分。这些 tokens 然后会被转化为数字，并映射到称为嵌入（embeddings）的特殊编码，这些嵌入就像是存储
    token 语义信息的特殊地图。最终，Transformer 的编码器将这些嵌入转化为一种表示。这个“表示”是一个向量，能够捕捉输入 tokens 的上下文含义，使得模型能够更有效地理解和处理输入。简单来说，你可以将其理解为将所有碎片拼凑在一起，以便理解完整的故事。
- en: Here’s an example of a text string, its tokenization, and its vector embedding.
    Note that tokenization can turn words into subwords. For example, the word “generative”
    can be tokenized into “gener” and “ative.”
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个文本字符串，它的 token 化及其向量嵌入。请注意，token 化可以将单词拆分成子单词。例如，单词“generative”可以被拆分为“gener”和“ative”。
- en: 'Let’s look at the input text:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看输入文本：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here’s the tokenized text:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 token 化后的文本：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let’s look at the embeddings:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看嵌入：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Think of the context vector as the heart of the input information. It enables
    the transformer’s decoder to determine what to say next. For example, by providing
    the decoder with a starting sentence as a hint, it can suggest the next word that
    makes sense. This process repeats, with each new suggestion becoming part of the
    hint, allowing the decoder to generate a naturally flowing paragraph from an initial
    sentence.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 把上下文向量看作是输入信息的核心。它使得 Transformer 的解码器能够确定接下来该说什么。例如，通过给解码器提供一个起始句子作为提示，它可以建议下一个合适的单词。这个过程会重复，每次新的建议都会成为提示的一部分，从而让解码器从一个初始句子生成一个自然流畅的段落。
- en: Decoder-based content generation is like a game, where each move is based on
    the previous one, and you end up with a complete story. This method of content
    generation is called “autoregressive generation.” Broadly speaking, this is how
    LLMs work. Autoregressive generation-based models can handle long input texts
    while also maintaining a context vector big enough to deal with complicated ideas.
    In addition to this, it has many layers in its decoder, making it highly sophisticated.
    It’s so big that it typically can’t run on just one computer and instead must
    run on a cluster of nodes working together that are accessed often. That’s why
    it’s offered as a service through an **application programming interface** (**API**).
    As you might have guessed, this enormous model is trained using a massive amount
    of text until it understands how language works, including all the patterns and
    structures of sentences. Now, let’s understand the main structure of LLMs.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于解码器的内容生成就像是一场游戏，每一步都基于前一步，最终你将得到一个完整的故事。这种内容生成方法叫做“自回归生成”（autoregressive generation）。广义来说，这就是大型语言模型（LLM）的工作原理。基于自回归生成的模型能够处理长篇输入文本，同时保持足够大的上下文向量来处理复杂的概念。此外，解码器中有很多层，使得它非常复杂。它大到通常无法仅在一台计算机上运行，而必须依赖于多个节点协同工作，这些节点通常需要频繁访问。这就是为什么它通过
    **应用程序编程接口** (**API**) 提供服务的原因。如你所猜测，这个庞大的模型是通过大量文本的训练来学习语言的运作方式，包括句子的所有模式和结构。现在，让我们理解大型语言模型（LLM）的主要结构。
- en: 'An LLM’s structure (see *Figure 1**.1*) is mainly made up of different layers
    of neural networks, such as recurrent layers, feedforward layers, embedding layers,
    and attention layers. These layers collaborate to handle input text and make predictions
    about the output. Let’s take a closer look:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的结构（参见*图 1.1*）主要由不同层次的神经网络组成，例如递归层、前馈层、嵌入层和注意力层。这些层次协作处理输入文本并对输出进行预测。我们来仔细看看：
- en: '**The embedding layer** changes each word in the input text into a special
    kind of detailed description, kind of like a unique fingerprint. These descriptions
    hold crucial details about the words and their meanings, which helps the model
    understand the bigger picture.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入层**将输入文本中的每个单词转换为一种特殊的详细描述，类似于独特的指纹。这些描述包含关于单词及其含义的重要细节，帮助模型理解更大的图景。'
- en: '**The feedforward layers** in LLMs consist of many connected layers that process
    the detailed descriptions created in the embedding layer. These layers perform
    complex transformations on these embeddings, which helps the model understand
    the more important ideas in the input text.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈层**由许多连接的层组成，这些层处理在嵌入层中创建的详细描述。这些层对这些嵌入进行复杂的变换，帮助模型理解输入文本中更重要的思想。'
- en: '**The recurrent layers** in LLMs are designed to read the input text one step
    at a time. These layers have hidden memory that gets updated as they read each
    part of the text. This helps the model remember how the words are related to each
    other in a sentence.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归层**设计用于一步步地读取输入文本。这些层具有隐藏的记忆，随着每一步读取，它们会更新记忆。这帮助模型记住句子中单词之间的关系。'
- en: '**The attention mechanism** is another important part of LLMs. It’s like a
    spotlight where the model shines on different parts of the input text. This helps
    the model focus on the most important parts of the text and make better predictions.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意力机制**是LLM的另一个重要部分。它就像一个聚光灯，模型将其聚焦于输入文本的不同部分。这帮助模型集中注意力于文本中最重要的部分，从而做出更好的预测。'
- en: For example, when you read, you don’t pay equal attention to every word; instead,
    you focus more on keywords and important phrases to grasp the main idea. For instance,
    in the sentence “The cat sat on the mat,” you might emphasize “cat” and “mat”
    to understand what’s happening. Additionally, you use context from previous sentences
    to make sense of the current one – if you read about a cat playing earlier, you
    understand why the cat is now sitting on the mat. As you continue reading, you
    adjust your focus based on what’s important for comprehension, revisiting or paying
    more attention to crucial sections that help you understand the overall plot.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，当你阅读时，并不会对每个词都给予同等的关注；而是更多地集中在关键词和重要短语上，以掌握主旨。例如，在句子“猫坐在垫子上”中，你可能会强调“猫”和“垫子”来理解发生了什么。此外，你还会利用前面句子的上下文来理解当前句子——如果你之前读到有只猫在玩耍，你就能理解为什么这只猫现在坐在垫子上。随着阅读的进行，你会根据理解的需要调整关注重点，重新审视或更加关注那些有助于理解整体情节的关键部分。
- en: 'In essence, just as humans read text by focusing on important words and using
    context to understand meaning, the attention mechanism in transformers focuses
    on key parts of the input and adjusts dynamically to capture the context and relationships
    between words:'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本质上，就像人类通过关注重要词汇并利用上下文理解含义来阅读文本一样，变换器中的注意力机制也会关注输入的关键部分，并动态调整，以捕捉单词之间的上下文和关系：
- en: '![Figure 1.1: Transformer architecture (source: https://arxiv.org/pdf/1706.03762.pdf)](img/B21019_01_1.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.1：变换器架构（来源： https://arxiv.org/pdf/1706.03762.pdf）](img/B21019_01_1.jpg)'
- en: 'Figure 1.1: Transformer architecture (source: [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf))'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1：变换器架构（来源：[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)）
- en: Now that we’ve learned the basic concepts behind LLMs, let’s focus on some of
    the top industry examples.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了LLM背后的基本概念，接下来让我们关注一些行业中的顶级实例。
- en: LLM examples
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 示例
- en: 'Cutting-edge LLMs have been developed by many companies, including OpenAI (GPT-4),
    Meta (Llama 3.1), Anthropic (Claude), and Google (Gemini), to name a few. OpenAI
    has consistently maintained a dominant role in the field of LLMs. Let’s look at
    the top models that are used at the time of writing:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司已经开发出了先进的LLM，包括OpenAI（GPT-4）、Meta（Llama 3.1）、Anthropic（Claude）和Google（Gemini），等等。OpenAI在LLM领域一直保持着主导地位。我们来看一下在撰写时使用的顶级模型：
- en: '**Generative Pre-trained Transformer (GPT)**: OpenAI has created various GPT
    models, including GPT1 (117 million parameters), GPT2 (1.5 billion parameters),
    GPT-3 (175 billion parameters), GPT 3.5, GPT4-Turbo, GPT4-o, and GPT4-o mini.
    GPT4-o is one of the most advanced LLMs globally. These models learn from a huge
    amount of text and can provide human-like answers to many subjects and questions.
    They also remember various parts of conversations.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Generative Pre-trained Transformer (GPT)**: OpenAI创建了多个GPT模型，包括GPT1（1.17亿参数）、GPT2（15亿参数）、GPT-3（1750亿参数）、GPT
    3.5、GPT4-Turbo、GPT4-o和GPT4-o mini等。GPT4-o是全球最先进的语言大模型之一。这些模型从大量文本中学习，能够提供类似人类的回答，涵盖多个主题和问题，并且能记住对话中的各个部分。'
- en: '**Anthropic**: Anthropic’s Claude models are a family of advanced LLMs that
    are designed to handle complex tasks with high efficiency. The latest iteration,
    Claude 3, includes models such as Opus, Sonnet, and Haiku, each tailored for different
    performance needs. Opus is the most powerful, excelling in complex analysis and
    higher-order tasks, while Sonnet balances speed and intelligence, and Haiku offers
    the fastest response times for lightweight actions. These models are built with
    a focus on security, reliability, and ethical AI practices.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Anthropic**: Anthropic的Claude模型是一系列先进的语言大模型（LLM），旨在以高效的方式处理复杂任务。最新版本Claude
    3包括了Opus、Sonnet和Haiku等不同型号，每种型号都根据不同的性能需求进行了优化。Opus是最强大的，擅长处理复杂的分析和高阶任务，而Sonnet则在速度和智能之间取得平衡，Haiku则为轻量级操作提供最快的响应时间。这些模型注重安全性、可靠性和道德的人工智能实践。'
- en: '**Llama 3.1**: Llama 3.1 is a cutting-edge LLM that represents a significant
    milestone in AI research. With its advanced architecture and massive scale, Llama
    3.1 is capable of understanding and generating human-like text with unprecedented
    accuracy and nuance. This powerful tool has far-reaching implications for various
    applications, including **natural language processing** (**NLP**), text generation,
    and conversational AI.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Llama 3.1**: Llama 3.1是一款前沿的语言大模型，代表着人工智能研究的一个重要里程碑。凭借其先进的架构和庞大的规模，Llama
    3.1能够以前所未有的准确性和细致度理解和生成类似人类的文本。这款强大的工具在自然语言处理（**NLP**）、文本生成和对话式人工智能等多个应用领域具有深远的影响。'
- en: '**Llama 2**: Llama 2 is a second-generation LLM developed by Meta. It’s open
    source and can be used to create chatbots such as ChatGPT or Google Bard. Llama
    2 was trained on 40% more data than Llama1 to make logical and natural-sounding
    responses. Llama 2 is available for anyone to use for research or business. Meta
    says that Llama 2 understands twice as much context as Llama 1\. This makes it
    a smarter language model that can give answers that sound just like what a human
    would provide.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Llama 2**: Llama 2是Meta开发的第二代语言大模型。它是开源的，可以用来创建类似ChatGPT或Google Bard的聊天机器人。Llama
    2的训练数据比Llama 1多40%，从而使其能提供更加逻辑清晰且自然的回应。Llama 2可以供任何人用于研究或商业用途。Meta表示，Llama 2能够理解比Llama
    1更多的上下文信息，这使得它成为一个更加智能的语言模型，能够提供听起来像人类回答的回复。'
- en: '**Gemini**: Google Gemini is a family of advanced multimodal LLMs developed
    by Google DeepMind. Announced on December 6, 2023, Gemini includes variants such
    as Gemini Ultra, Gemini Pro, Gemini Flash, and Gemini Nano. It’s designed to understand
    and operate across different types of information seamlessly, including text,
    images, audio, video, and code. Positioned as a competitor to OpenAI’s GPT-4,
    Gemini powers Google’s AI chatbot and aims to boost creativity and productivity.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gemini**: Google Gemini是由Google DeepMind开发的一系列先进的多模态语言大模型（LLM）。该系列于2023年12月6日发布，包括Gemini
    Ultra、Gemini Pro、Gemini Flash和Gemini Nano等版本。Gemini旨在无缝理解和处理不同类型的信息，包括文本、图像、音频、视频和代码。作为OpenAI
    GPT-4的竞争者，Gemini驱动着Google的AI聊天机器人，并旨在提升创造力和生产力。'
- en: '**PaLM 2**: Finally, PaLM 2 is Google’s updated LLM. It’s skilled at handling
    complex tasks such as working with code and math, categorizing and answering questions,
    translating languages, being proficient in multiple languages, and creating human-like
    sentences. It outperforms the previously mentioned models, including the original
    PaLM. Google is careful about how it creates and uses AI, and PaLM 2 is a part
    of this approach. It went through thorough evaluations so that it could be checked
    for potential problems and biases. PaLM 2 is not just used in isolation but is
    also used in other advanced models, such as Med-PaLM 2 and Sec-PaLM. It’s also
    responsible for powering AI features and tools at Google, such as Bard and the
    PaLM API.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PaLM 2**：最后，PaLM 2 是谷歌更新的 LLM（大型语言模型）。它擅长处理复杂任务，如处理代码和数学、分类和回答问题、翻译语言、精通多种语言，并能创造类人句子。它的表现超过了前述的模型，包括最初的
    PaLM。谷歌在创建和使用 AI 时非常谨慎，PaLM 2 就是这一方法的一部分。它经过了全面评估，以检查潜在的问题和偏见。PaLM 2 不仅单独使用，还被应用于其他先进模型，如
    Med-PaLM 2 和 Sec-PaLM。它还为谷歌的 AI 功能和工具提供动力，如 Bard 和 PaLM API。'
- en: 'The evolutionary tree (see *Figure 1**.2*) of modern LLMs illustrates how these
    models have evolved in recent years and highlights some of the most famous ones.
    Models that are closely related are shown on the same branches. Models that use
    the Transformer architecture are shown in different colors: those that only decode
    are on the blue branch, ones that only encode are on the pink branch, and models
    that do both encoding and decoding are on the green branch. The position of the
    models on the timeline shows when they were released. Open source models are represented
    by filled squares, while models that are not open source are represented by empty
    squares. The bar chart at the bottom right displays the number of models from
    different companies and organizations:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现代 LLM 的进化树（见*图 1.2*）展示了这些模型近年来的演变，并突出了其中一些最著名的模型。相关性较强的模型位于同一分支上。使用 Transformer
    架构的模型显示在不同的颜色上：仅解码的模型位于蓝色分支，只有编码的模型位于粉色分支，而既进行编码又进行解码的模型位于绿色分支。模型在时间线上的位置表示它们的发布时点。开源模型以实心方块表示，而非开源模型则以空心方块表示。右下角的柱状图显示了不同公司和组织发布的模型数量：
- en: '![Figure 1.2: The evolutionary tree of modern LLMs (source: https://arxiv.org/abs/2304.13712)](img/B21019_01_2.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2：现代 LLM 的进化树（来源：[https://arxiv.org/abs/2304.13712](https://arxiv.org/abs/2304.13712))](img/B21019_01_2.jpg)'
- en: 'Figure 1.2: The evolutionary tree of modern LLMs (source: [https://arxiv.org/abs/2304.13712](https://arxiv.org/abs/2304.13712))'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2：现代 LLM 的进化树（来源：[https://arxiv.org/abs/2304.13712](https://arxiv.org/abs/2304.13712)）
- en: Having explored some exemplary LLM instances, let’s discuss the concept of foundation
    models and their advantages and disadvantages.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了一些典型的 LLM 实例后，让我们讨论一下基础模型的概念及其优缺点。
- en: The concept of foundation models
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础模型的概念
- en: In recent years, there has been a huge buzz around LLMs such as ChatGPT sweeping
    across the world. LLMs are a subset of a broader category of models known as foundation
    models. Interestingly, the term “foundation models” was initially introduced by
    a team from Stanford. They observed a shift in the AI landscape, leading to the
    emergence of a new paradigm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，围绕 LLM（如 ChatGPT）的热议席卷了全球。LLM 是一个更广泛模型类别——基础模型（foundation models）的子集。有趣的是，“基础模型”这一术语最初由斯坦福大学的团队提出。他们观察到
    AI 领域发生了变化，推动了一个新范式的出现。
- en: 'In the past, AI applications were constructed by training individual AI models,
    each tailored to a specific task using specialized data. This approach often involved
    assembling a library of various AI models in a mostly supervised training manner.
    Its foundational capability, known as a foundation model (see *Figure 1**.3*),
    would become the driving force behind various applications and use cases. Essentially,
    this single model could cater to the very same applications that were once powered
    by distinct AI models in the traditional approach. This meant that a single model
    could fuel an array of diverse applications. The key here is that this model possesses
    the incredible ability to adapt to a multitude of tasks. What empowers this model
    to achieve such versatility is the fact that it has undergone extensive training
    on an immense volume of unstructured data through an unsupervised approach:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，人工智能应用程序是通过训练单个人工智能模型构建的，每个模型都针对特定任务使用专门的数据进行训练。这种方法通常涉及以大多数是监督学习的方式组装一个包含各种人工智能模型的库。其基础能力，即基础模型（见
    *图 1.3*），将成为各种应用和用例背后的驱动力。实际上，这个单一的模型能够满足那些曾经由传统方法中不同的人工智能模型支持的应用程序。这意味着一个模型可以驱动多种多样的应用程序。关键在于，这个模型拥有惊人的能力，能够适应多种任务。促使该模型实现这种多功能性的因素在于，它通过无监督的方式，在海量的非结构化数据上进行了广泛的训练：
- en: '![Figure 1.3: Foundational model (source: https://arxiv.org/pdf/2108.07258.pdf)](img/B21019_01_3.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3：基础模型（来源：https://arxiv.org/pdf/2108.07258.pdf)](img/B21019_01_3.jpg)'
- en: 'Figure 1.3: Foundational model (source: [https://arxiv.org/pdf/2108.07258.pdf](https://arxiv.org/pdf/2108.07258.pdf))'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3：基础模型（来源：[https://arxiv.org/pdf/2108.07258.pdf](https://arxiv.org/pdf/2108.07258.pdf)）
- en: Consider a scenario where I start a sentence with “Don’t count your chickens
    before they’re.” Now, my goal is to guide the model in predicting the last word,
    which could be “hatched,” “grown,” or even “gone.” This process involves training
    the model to anticipate the appropriate word by analyzing the context provided
    by the words that come before it in the sentence. The impressive ability to generate
    predictions for the next word, while drawing on the context of preceding words
    it has encountered, positions foundation models within the realm of generative
    AI. In essence, these models fall under the category of generative AI because
    they’re capable of crafting something novel – in this case, predicting the upcoming
    word in a sentence.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个场景，我开始一句话说“不要在鸡还没孵出来之前就开始数它们。”现在，我的目标是引导模型预测最后一个词，这个词可能是“孵化”（hatched）、“生长”（grown）甚至是“消失”（gone）。这个过程包括通过分析句子中前面给出的上下文，训练模型预测合适的词。能够根据前面遇到的词的上下文来生成对下一个词的预测，这种令人印象深刻的能力使得基础模型处于生成性人工智能的领域。本质上，这些模型属于生成性人工智能，因为它们能够创造一些新东西——在这种情况下，就是预测句子中的下一个词。
- en: Although these models are primarily designed to generate predictions, particularly
    anticipating the next word in a sentence, they offer immense capabilities. With
    the addition of a modest amount of labeled data, we can adjust these models to
    perform exceptionally well on more traditional NLP tasks. These tasks include
    activities such as classification or named-entity recognition, which are typically
    not associated with generative capabilities. This transformation is achieved through
    a process known as fine-tuning. When you fine-tune your foundation model with
    a modest dataset, you adjust its parameters so that it can excel at a specific
    natural language task. This way, the model evolves from being primarily generative
    to being a powerful tool for targeted NLP tasks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些模型主要设计用来生成预测，特别是预测句子中的下一个词，但它们具有巨大的能力。只需添加少量带标签的数据，我们就可以调整这些模型，使其在更传统的自然语言处理（NLP）任务上表现得非常出色。这些任务包括分类或命名实体识别等活动，这些通常与生成能力无关。通过一种叫做微调的过程，可以实现这一转变。当你使用适量的数据集来微调你的基础模型时，你会调整其参数，使其能够在特定的自然语言任务上表现出色。通过这种方式，模型从主要是生成型的演变为一个强大的、针对性的NLP任务工具。
- en: Even with limited data, foundation models can prove highly effective, especially
    in domains where data is scarce. Through a process known as prompting, or prompt
    engineering, techniques such as in-context learning, zero-shot, one-shot, and
    few-shot learning can be used to tackle complex downstream tasks. Let’s break
    down how you could prompt a model for a classification task. Imagine that you
    provide the model with a sentence and follow it up with the question, “Does this
    sentence carry a positive or negative sentiment?” The model would then work its
    magic, completing the sentence with generated words. The very next word it generates
    would serve as the answer to your classification question. Depending on where
    it perceives the sentiment of the sentence to lie, the model would respond with
    either “positive” or “negative.” This method leverages the model’s inherent ability
    to generate contextually relevant text to solve a specific classification challenge.
    We’ll cover different prompting techniques and advanced prompt engineering later
    in this book.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是在数据有限的情况下，基础模型也可以证明其高效性，特别是在数据稀缺的领域。通过称为提示或提示工程的过程，可以使用技术如上下文学习、零样本、一次样本和少样本学习来解决复杂的下游任务。让我们来分析一下如何为分类任务设置提示。想象一下，你向模型提供一个句子，然后跟上这样一个问题：“这个句子带有积极还是消极情绪？”模型会运用其魔法，生成完整的句子。它生成的下一个词将作为你分类问题的答案。根据它认为句子情感所在的位置，模型将回答“积极”或“消极”。这种方法利用了模型生成上下文相关文本的内在能力，以解决特定的分类挑战。我们将在本书后面讨论不同的提示技术和高级提示工程。
- en: 'Let’s talk about some of the key advantages of foundation models:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来谈谈基础模型的一些关键优势：
- en: '**Performance**: These models have been trained on an immense amount of content
    with data volumes regularly in the terabyte range. When employed for smaller tasks,
    these models exhibit remarkable performance that far surpasses models trained
    on only a handful of data points.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能**：这些模型经过大量数据的训练，数据量通常达到了几TB。当用于较小的任务时，这些模型展示出非凡的性能，远远超过仅训练在少量数据点上的模型。'
- en: '**Productivity gain**: LLMs can boost productivity in a big way. They’re like
    a super-efficient human for tasks that usually take a lot of time and effort.
    For instance, in customer service, LLMs can quickly answer common questions, freeing
    up human workers to handle more complex issues. In businesses, they can process
    and organize data way faster than people can. Using LLMs, companies can save time
    and money. This lets them focus on important tasks and thus acts like a turbocharger
    for productivity.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产力提升**：LLM可以极大地提升生产力。它们就像超高效的人类，可以完成通常需要大量时间和精力的任务。例如，在客户服务中，LLM可以快速回答常见问题，从而让人类工作人员有更多时间处理更复杂的问题。在企业中，它们可以比人类更快地处理和组织数据。使用LLM，公司可以节省时间和金钱。这让他们能够专注于重要任务，因此像生产力的涡轮增压器一样发挥作用。'
- en: 'However, these foundation models also have key challenges and limitations:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些基础模型也存在关键的挑战和限制：
- en: '**Cost**: These models tend to be quite costly to train because of the huge
    data volumes needed. This often poses challenges for smaller businesses attempting
    to train their foundation models. Additionally, as these models grow in size,
    reaching a scale of several billion parameters, they can become pricey to use
    for inference.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**：由于需要大量的数据量来训练，这些模型的训练成本往往相当高昂。这通常对试图训练自己的基础模型的小型企业构成挑战。此外，随着这些模型规模的扩大，达到数十亿参数的规模，它们在推断方面的使用成本可能会昂贵。'
- en: Cloud providers such as Microsoft offer a service called **Azure OpenAI**. This
    service lets businesses use these models on-demand and pay only for what they
    use. This is similar to renting a powerful computer for a short time instead of
    buying one outright. Leveraging this service-based capability allows companies
    to save money on both model training and model use, especially considering the
    powerful, GPU-based hardware required.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 诸如微软之类的云服务提供商提供了一项名为**Azure OpenAI**的服务。这项服务允许企业按需使用这些模型，并且仅支付他们使用的部分。这类似于租用一台强大的计算机一段时间，而不是直接购买一台。利用这种基于服务的能力使公司能够节省在模型训练和使用上的资金，特别是考虑到需要强大的基于GPU的硬件。
- en: To summarize, using services such as Azure OpenAI, businesses can take advantage
    of these advanced models without spending a ton on resources and infrastructure.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总结一下，通过使用Azure OpenAI等服务，企业可以利用这些先进的模型，而无需在资源和基础设施上花费大量资金。
- en: '**Trustworthiness**: Just as data serves as a massive advantage for these models,
    there’s a flip side to consider: LLMs are trained on vast amounts of internet-scraped
    language data that may contain biases, hate speech, or toxic content, compromising
    their reliability. This would be a monumental task. Furthermore, there’s the challenge
    of not even fully knowing what the data comprises. For many open source models,
    the exact datasets used for training many LLMs are unclear, making it difficult
    to assess their raising concerns about the models’ trustworthiness and potential
    biases. The sheer scale of LLM training data makes it nearly impossible for human
    annotators to thoroughly vet each data point, increasing the risk of unintended
    consequences such as perpetuating harmful biases or generating toxic content.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可信度**：就像数据对这些模型来说是巨大的优势一样，也有另一面需要考虑：LLM 是通过大量从互联网抓取的语言数据进行训练的，这些数据可能包含偏见、仇恨言论或有害内容，进而影响其可靠性。这将是一个艰巨的任务。此外，还有一个挑战是我们甚至不能完全了解这些数据的构成。对于许多开源模型来说，用于训练
    LLM 的确切数据集不明确，这使得评估它们是否存在可信度和潜在偏见问题变得困难。LLM 训练数据的庞大规模几乎使得人工注释人员无法彻底审查每一个数据点，从而增加了无意中引发负面后果的风险，比如加剧有害偏见或生成有毒内容。'
- en: Big organizations are fully aware of the immense possibilities that these technologies
    hold. To solve the foundational model trustworthiness issue, OpenAI, Microsoft,
    Google, and Anthropic are jointly unveiling the creation of the Frontier Model
    Forum ([https://blogs.microsoft.com/on-the-issues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-model-forum](https://blogs.microsoft.com/on-the-issues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-model-forum)),
    a fresh industry initiative aimed at ensuring the safe and responsible advancement
    of frontier AI models. This new collaborative entity will tap into the collective
    technical and operational prowess of its member companies to foster progress across
    the broader AI landscape. One of its core objectives involves driving technical
    evaluations and benchmarks forward. Additionally, the forum will strive to construct
    a publicly accessible repository of solutions, bolstering the adoption of industry
    best practices and standards throughout the AI domain.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大型组织充分意识到这些技术蕴藏的巨大潜力。为了解决基础模型的可信度问题，OpenAI、微软、谷歌和 Anthropic 正联合推出“前沿模型论坛”（[https://blogs.microsoft.com/on-the-issues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-model-forum](https://blogs.microsoft.com/on-the-issues/2023/07/26/anthropic-google-microsoft-openai-launch-frontier-model-forum)），这是一个旨在确保前沿
    AI 模型安全和负责任发展的全新行业倡议。这个新的合作实体将利用成员公司在技术和运营方面的集体优势，推动整个 AI 领域的进展。它的核心目标之一是推动技术评估和基准测试的前进。此外，论坛还将努力构建一个公开可访问的解决方案库，促进
    AI 领域行业最佳实践和标准的采用。
- en: '**Hallucination**: Sometimes, LLMs can come up with information or answers
    that might not be entirely accurate. This is like when you have a dream that seems
    real, but it’s not based on what’s happening. LLMs might generate text that sounds
    right but isn’t completely true or accurate. So, while LLMs are highly intelligent,
    they can sometimes make mistakes or come up with content that isn’t real.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幻觉**：有时，LLM 可能会生成一些信息或答案，这些内容可能并不完全准确。这就像是你做了一个看起来很真实的梦，但它并不基于现实中的情况。LLM
    可能生成听起来正确但实际上并不完全真实或准确的文本。因此，虽然 LLM 具有高度智能，但有时它们也会犯错或生成并不存在的内容。'
- en: The applications of LLMs often require human oversight to make sure the outputs
    are trustworthy. However, there is a promising technique called **grounding the
    model** that aims to improve this situation. Grounding means connecting the LLM’s
    understanding with real-world information and context. This is like making sure
    the model is firmly rooted in reality. Later in this book, we’ll talk more about
    how to use this technique to stop the model from making things up and only give
    answers based on the given context.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM 的应用通常需要人工监督，以确保输出结果值得信赖。然而，有一种前景广阔的技术叫做 **模型对接**，旨在改善这种情况。对接意味着将 LLM 的理解与现实世界的信息和上下文联系起来。这就像是确保模型牢牢扎根于现实中一样。本书后面会详细讨论如何使用这种技术来防止模型凭空捏造内容，并只给出基于给定上下文的答案。
- en: '**Limited context window**: LLMs have a limited context window or token size.
    A context window or token size can be seen as the amount of memory a model can
    process at a time. LLMs can only understand a certain number of pieces of information
    at once. For example, ChatGPT (GPT4-o) can handle 128K input tokens. This means
    that if you give it too much to read, it won’t be able to handle that and will
    throw errors. Therefore, it’s important to keep the input within this limit for
    the model to work well.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限的上下文窗口**：LLM 有一个有限的上下文窗口或令牌大小。上下文窗口或令牌大小可以看作是模型每次能处理的记忆量。LLM 一次只能理解一定数量的信息。例如，ChatGPT（GPT4-o）可以处理
    128K 输入令牌。这意味着如果给它的输入太多，它将无法处理并会报错。因此，保持输入在此限制内对于模型正常运行非常重要。'
- en: Following our understanding of the foundation model concept, let’s delve into
    some practical use cases of LLMs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们理解基础模型概念的基础上，接下来让我们深入探讨 LLM 的一些实际应用案例。
- en: Exploring LLM use cases
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 LLM 的应用案例
- en: 'LLMs have a wide range of use cases across various fields and industries due
    to their ability to understand and generate human-like text. Let’s take a look
    at some of them:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 在各个领域和行业中有着广泛的应用案例，因为它们能够理解并生成类人文本。让我们来看看其中的一些：
- en: '**Content generation**: LLMs can generate written content for blogs, articles,
    marketing materials, and social media posts. They can be used to automate content
    creation and come up with creative ideas.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容生成**：LLM 可以生成博客、文章、营销材料和社交媒体帖子的书面内容。它们可以用于自动化内容创作并提供创意灵感。'
- en: '**Customer support**: LLMs can provide automated responses to customer queries
    and support tickets, thus handling common questions and issues, freeing up human
    agents to handle more complex cases.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户支持**：LLM 可以为客户查询和支持工单提供自动化回应，从而处理常见问题和问题，将更复杂的案例交由人工代理处理。'
- en: '**Language translation**: LLMs can be employed to translate text between languages,
    making communication easier and more accessible on a global scale.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言翻译**：LLM 可以用于在不同语言之间翻译文本，使全球范围内的沟通更加便捷和可达。'
- en: '**Text summarization**: LLMs can quickly summarize lengthy texts, making it
    easier to grasp the main points of articles, reports, and other written materials.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本摘要**：LLM 可以快速总结长篇文本，使得更容易把握文章、报告及其他书面材料的要点。'
- en: '**Chatbots and virtual assistants**: LLMs can power chatbots and virtual assistants
    that engage in natural language conversations, helping users with tasks, inquiries,
    and information retrieval.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聊天机器人和虚拟助手**：LLM 可以驱动聊天机器人和虚拟助手，与用户进行自然语言对话，帮助完成任务、解答询问和获取信息。'
- en: '**Content personalization**: LLMs can analyze user preferences and behavior
    to personalize recommendations, advertisements, and content delivery on platforms
    such as social media and streaming services.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容个性化**：LLM 可以分析用户偏好和行为，个性化推荐、广告和在社交媒体和流媒体服务平台上的内容推送。'
- en: '**Data entry and extraction**: LLMs can extract relevant information from unstructured
    text, such as documents or emails, and enter it into structured databases or spreadsheets.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据录入与提取**：LLM 可以从非结构化文本中提取相关信息，例如文档或电子邮件，并将其录入结构化的数据库或电子表格中。'
- en: '**Creative writing**: LLMs can assist writers by generating story ideas, dialogs,
    character descriptions, and even entire narratives.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创意写作**：LLM 可以通过生成故事创意、对话、角色描述甚至整个叙事来协助作家。'
- en: '**Healthcare chatbots**: LLM-powered chatbots can answer health-related questions,
    provide first-aid advice, and offer information about common medical conditions.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗聊天机器人**：LLM 驱动的聊天机器人可以回答与健康相关的问题，提供急救建议，并提供有关常见医疗状况的信息。'
- en: '**Medical diagnostics**: LLMs can aid in diagnosing medical conditions by analyzing
    patient symptoms and medical records to provide potential diagnoses and treatment
    options.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医学诊断**：LLM 可以通过分析患者症状和病历来帮助诊断医疗状况，提供潜在的诊断和治疗方案。'
- en: '**Mental health support**: LLMs can provide empathetic responses and resources
    to individuals seeking support for mental health concerns.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**心理健康支持**：LLM 可以为寻求心理健康支持的个人提供同情的回应和资源。'
- en: '**Tourism and travel planning**: LLMs can assist travelers by suggesting itineraries,
    recommending attractions, and providing information about local customs and cuisines.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**旅游和旅行规划**：LLM 可以通过建议行程、推荐景点并提供有关当地习俗和美食的信息来协助旅行者。'
- en: '**Recipe creation**: LLMs can devise creative recipes based on ingredients
    and dietary preferences, offering new culinary experiences.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**食谱创作**：LLM可以根据食材和饮食偏好设计创意食谱，提供全新的烹饪体验。'
- en: '**Cybersecurity analysis**: LLMs can analyze cybersecurity threats and suggest
    strategies for protecting digital systems and data.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络安全分析**：LLM可以分析网络安全威胁，并建议保护数字系统和数据的策略。'
- en: '**Fashion recommendations**: LLMs can suggest clothing and accessory combinations
    based on personal style preferences and current fashion trends.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时尚推荐**：LLM可以根据个人风格偏好和当前流行趋势，推荐服装和配饰搭配。'
- en: '**Legal document review**: LLMs can review legal documents, contracts, and
    case histories to identify relevant information, anomalies, and potential issues.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**法律文件审查**：LLM可以审查法律文件、合同和案件历史，识别相关信息、异常情况和潜在问题。'
- en: '**Academic research**: LLMs can assist researchers by providing summaries of
    academic papers, helping with literature reviews, and generating ideas for further
    study.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学术研究**：LLM可以通过提供学术论文摘要、帮助进行文献综述，并生成进一步研究的想法，来协助研究人员。'
- en: '**Financial analysis**: LLMs can process and analyze financial data, generate
    reports, and provide insights into market trends and investment opportunities.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**财务分析**：LLM可以处理和分析财务数据，生成报告，并提供市场趋势和投资机会的洞察。'
- en: '**Language learning**: LLMs can help learners practice and improve their language
    skills by engaging in conversations, providing explanations, and offering exercises.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言学习**：LLM可以通过与学习者对话、提供解释和提供练习，帮助学习者练习和提升语言技能。'
- en: '**Accessibility tools**: LLMs can be used to create audio descriptions for
    visually impaired individuals, generate subtitles for videos, and convert text
    into speech for people with reading difficulties.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无障碍工具**：LLM可以用于为视障人士创建音频描述、为视频生成字幕，并将文本转化为语音，帮助有阅读困难的人群。'
- en: While these are just a few example use cases, the limitless flexibility of LLMs
    allows them to also be applied to even more situations as technology progresses.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些只是一些示例用例，但LLM的无限灵活性使其能够在技术进步的过程中应用于更多的场景。
- en: People who want to create generative AI applications without the burden of training
    an LLM themselves or spending money on costly hardware can use the Azure OpenAI
    API. This lets them use advanced LLMs made by OpenAI.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 想要创建生成型AI应用程序但不想自己训练LLM或花钱购买昂贵硬件的人，可以使用Azure OpenAI API。这使他们能够使用由OpenAI制作的先进LLM。
- en: So far, we’ve covered the basics of LLMs. Moving forward, the next chapter will
    focus on the Azure OpenAI service in more detail.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了LLM的基础知识。接下来的一章将更详细地讨论Azure OpenAI服务。
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started by introducing LLMs and how they’re influenced by
    Transformer networks. Then, we explored the various parts that make up LLMs. Next,
    we dove into some of the top LLM models created by OpenAI, Meta, and Google, discussing
    how these models have evolved. We also covered the concept of foundation models,
    including their advantages and limitations. Lastly, we looked at various business
    applications where LLMs have shown great potential.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们从介绍LLM及其如何受Transformer网络影响开始。然后，我们探讨了构成LLM的各个部分。接下来，我们深入研究了由OpenAI、Meta和Google创建的一些顶级LLM模型，讨论了这些模型如何演变。我们还介绍了基础模型的概念，包括其优势和局限性。最后，我们查看了LLM在各个商业应用中的巨大潜力。
- en: Moving forward to the next chapter, our focus will be on Azure OpenAI service.
    We’ll learn how to access this service, including models such as GPT 3.5, GPT-4,
    Embeddings, and DALL.E 2\. We’ll also explain how the pricing works, discussing
    options such as pay-as-you-go and reserved capacity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将专注于Azure OpenAI服务。我们将学习如何访问该服务，包括GPT 3.5、GPT-4、嵌入式模型和DALL.E 2等模型。我们还将解释定价机制，讨论按需付费和预留容量等选项。
- en: Further reading
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*AI Explainer*: *Foundation models and the next era of* *AI* ([https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/](https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/))'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AI解释器*：*基础模型与AI的下一个时代* ([https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/](https://www.microsoft.com/en-us/research/blog/ai-explainer-foundation-models-and-the-next-era-of-ai/))'
- en: '*Orca:* *Progressive Learning from Complex Explanation Traces of* *GPT-4* ([https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/](https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/))'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Orca:* *来自* *GPT-4* *的复杂解释轨迹的渐进学习* ([https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/](https://www.microsoft.com/en-us/research/publication/orca-progressive-learning-from-complex-explanation-traces-of-gpt-4/))'
- en: '*Florence:* *A New Foundation Model for Computer* *Vision* ([https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/](https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/))'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Florence:* *计算机视觉的新基础模型* ([https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/](https://www.microsoft.com/en-us/research/publication/florence-a-new-foundation-model-for-computer-vision/))'
- en: '*Accelerating Foundation Models* *Research* ([https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/](https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/))'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*加速基础模型研究* ([https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/](https://www.microsoft.com/en-us/research/collaboration/accelerating-foundation-models-research/))'
- en: '*Attention Is All You* *Need* ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*注意力即是你所需要的* ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))'
- en: '*On the Opportunities and Risks of Foundation* *Models* ([https://arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258))'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于基础模型的机会与风险* ([https://arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258))'
