- en: Develop an Autonomous Agent with Deep R Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发一个深度强化学习的自主代理
- en: Welcome to the chapter on reinforcement learning. In the previous chapters,
    we have worked on solving supervised learning problems. In this chapter, we will
    learn to build and train a deep reinforcement learning model capable of playing
    games.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到强化学习这一章节。在之前的章节中，我们已经解决了监督学习的问题。在本章中，我们将学习如何构建和训练一个能够玩游戏的深度强化学习模型。
- en: Reinforcement learning is often a new paradigm for deep learning engineers and
    this is why we're using the framework of a game for this training. The business
    use cases that we should be looking out for are typified by process optimization.
    Reinforcement learning is great for gaming, but also applicable in use cases ranging
    from drone control ([https://arxiv.org/pdf/1707.05110.pdf](https://arxiv.org/pdf/1707.05110.pdf))
    and navigation to optimizing file downloads over mobile networks ([http://anrg.usc.edu/www/papers/comsnets_2017.pdf](http://anrg.usc.edu/www/papers/comsnets_2017.pdf)).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习通常是深度学习工程师接触到的一种新范式，这也是我们选择用游戏框架进行本次训练的原因。我们需要关注的业务应用场景通常涉及过程优化。强化学习在游戏中表现出色，但也适用于从无人机控制（[https://arxiv.org/pdf/1707.05110.pdf](https://arxiv.org/pdf/1707.05110.pdf)）和导航到优化移动网络文件下载（[http://anrg.usc.edu/www/papers/comsnets_2017.pdf](http://anrg.usc.edu/www/papers/comsnets_2017.pdf)）等各种应用场景。
- en: 'We will do this with something called deep Q-learning and deep **State-Action-Reward-State-Action**
    (**SARSA**) learning. The idea is that we will build a deep learning model, also
    called an agent in reinforcement learning terms, that interacts with the game
    environment and learns how to play the game while maximizing rewards after several
    attempts at playing. Here is a  diagram illustrating reinforcement learning:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过深度Q学习和深度**状态-动作-奖励-状态-动作**（**SARSA**）学习来实现这一点。我们的想法是构建一个深度学习模型，在强化学习术语中也称为代理（Agent），它与游戏环境互动，并在多次游戏尝试后学习如何玩游戏，同时最大化奖励。这里是一个强化学习的示意图：
- en: '![](img/599cfdb5-da3b-4f20-aaa2-75dfec68c33c.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![](img/599cfdb5-da3b-4f20-aaa2-75dfec68c33c.png)'
- en: 'Figure 14.1: Reinforcement learning'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1：强化学习
- en: For the purpose of this chapter, we will be using the CartPole game from OpenAI
    Gym.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将使用OpenAI Gym的倒立摆游戏（CartPole）。
- en: 'What we''ll learn in this chapter is the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将学习以下内容：
- en: How to interact with the Gym toolkit
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何与Gym工具包进行交互
- en: What is Q-learning and SARSA learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是Q学习和SARSA学习
- en: Coding the RL model and defining hyperparameters
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写强化学习模型并定义超参数
- en: Building and understanding the training loop
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和理解训练循环
- en: Testing the model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试模型
- en: It would be better if you implement the code snippets as you go along in this
    chapter, either in a Jupyter Notebook or any source code editor. This will make
    it easier for you to follow along, as well as understand what each part of the
    code does.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最好在本章进行时就实现代码片段，无论是在Jupyter Notebook还是任何源代码编辑器中。这将使你更容易跟上进度，并理解代码的每一部分所做的事情。
- en: All of the Python and the Jupyter Notebook files for this chapter can be found
    [at https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter14](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter14).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有Python和Jupyter Notebook文件可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter14](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter14)找到。
- en: Let's get to the code!
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 让我们开始编码吧！
- en: In this exercise, we will be using the Gym toolkit from OpenAI for developing
    reinforcement learning models. It supports teaching agents such as CartPole and
    pinball games.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用来自OpenAI的Gym工具包来开发强化学习模型。它支持像倒立摆（CartPole）和弹球（Pinball）这样的游戏教学。
- en: To know more about the Gym toolkit from OpenAI and the games it supports, visit [http://gym.openai.com/](http://gym.openai.com/).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于OpenAI Gym工具包及其支持的游戏，请访问[http://gym.openai.com/](http://gym.openai.com/)。
- en: We will also be using the Keras deep learning library, which is a high-level
    neural network API capable of running on top of TensorFlow, Theano, or Cognitive
    Toolkit (CNTK).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用Keras深度学习库，它是一个高级神经网络API，能够运行在TensorFlow、Theano或认知工具包（CNTK）之上。
- en: To learn more about Keras and its functionalities visit [https://keras.io/](https://keras.io/).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于Keras及其功能的信息，请访问[https://keras.io/](https://keras.io/)。
- en: Deep Q-learning
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q学习
- en: In this segment, we will implement deep Q-learning with a deep learning model
    built using the Keras deep learning library as the function approximator.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将实现深度Q学习，并使用Keras深度学习库构建的深度学习模型作为函数近似器。
- en: We will start off this segment with a gentle introduction as to how to use the
    Gym module and then move on to understanding what Q-learning is, and finally,
    implement the deep Q-learning. We will be using the CartPole environment from
    OpenAI Gym.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从如何使用 Gym 模块的简单介绍开始，然后继续了解什么是 Q 学习，最后实现深度 Q 学习。我们将使用 OpenAI Gym 中的 CartPole
    环境。
- en: To follow along, refer to the Jupyter Notebook code file for the deep Q-learning
    section at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2015/DQN.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2015/DQN.ipynb).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟进，参考 Jupyter Notebook 代码文件中的深度 Q 学习部分：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2015/DQN.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2015/DQN.ipynb)。
- en: Importing all of the dependencies
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入所有依赖
- en: 'We will be using `numpy`, `gym`, `matplotlib`, `keras`, and `tensorflow` packages
    in this segment of the exercise. Here, TensorFlow will be used as the backend
    for Keras. You can install these packages using `pip`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节练习中，我们将使用 `numpy`、`gym`、`matplotlib`、`keras` 和 `tensorflow` 包。在这里，TensorFlow
    将作为 Keras 的后端。你可以使用 `pip` 安装这些包：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`deque` is a list-like container with fast appends and pops on either end.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`deque` 是一个类似列表的容器，可以在两端快速追加和弹出元素。'
- en: Exploring the CartPole game
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 CartPole 游戏
- en: In the CartPole game, you will find a pole attached by an unattached joint to
    the cart, which moves on a frictionless track. At the beginning of each game,
    the pole starts in the upright position and the goal is to hold it in the upright
    position as long as possible or for a given number of time steps. You can control
    the CartPole system by applying a force of +1 and -1 (to move the cart either
    to the right or to the left) and prevent the pole from falling over. The game/episode
    ends when the cart moves more than 2.4 units from the center or when the pole
    is more than 45 degrees from the vertical.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CartPole 游戏中，你会看到一根通过未固定的关节连接到小车的杆，杆在无摩擦的轨道上移动。在每一局游戏开始时，杆会处于竖直位置，目标是尽可能长时间地保持杆在竖直位置，或者保持给定的时间步数。你可以通过施加
    +1 和 -1 的力（分别使小车向右或向左移动）来控制 CartPole 系统，防止杆倒下。游戏/回合结束的条件是小车从中心位置移动超过 2.4 个单位，或者杆与竖直方向的夹角超过
    45 度。
- en: Interacting with the CartPole game
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 CartPole 游戏互动
- en: OpenAI Gym makes it super easy to interact with the game. In this section, we
    will cover how to load, reset, and play the CartPole game.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym 使得与游戏交互变得非常简单。在本节中，我们将介绍如何加载、重置并玩 CartPole 游戏。
- en: Loading the game
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载游戏
- en: 'Let''s load the `CartPole-v1` game from the `gym` module. It''s very simple.
    All you have to do is feed the `gym.make()` function the name of the game. In our
    case, the game is `CartPole-v1`. Gym then loads the game into your workspace:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 `gym` 模块加载 `CartPole-v1` 游戏。非常简单，你只需要将游戏名称传递给 `gym.make()` 函数。在我们的例子中，游戏是
    `CartPole-v1`。然后 Gym 会将游戏加载到你的工作空间中：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It is important that you set `seed` for reproducibility:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 `seed` 以保证结果可复现非常重要：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s explore how many variables we have in the CartPole game:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一下在 CartPole 游戏中有哪些变量：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following is the output:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/6ad4918d-90a9-4b51-bd73-f8e682edde99.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ad4918d-90a9-4b51-bd73-f8e682edde99.png)'
- en: We can see that the CartPole has `4` variables and these are namely the position
    (`x`), velocity (`x_dot`), angular position (`theta`), and the angular velocity
    (`theta_dot`).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 CartPole 游戏有 `4` 个变量，分别是位置（`x`）、速度（`x_dot`）、角度位置（`theta`）和角速度（`theta_dot`）。
- en: 'Let''s explore how many possible responses we have in this game using the following
    code:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一下在这个游戏中我们有多少种可能的响应，使用以下代码：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following is the output:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/32179474-c616-4311-bf21-8b2bcb7aad96.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32179474-c616-4311-bf21-8b2bcb7aad96.png)'
- en: We see that the CartPole environment has `2` possible responses/buttons, namely
    move left and move right.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 CartPole 环境有 `2` 种可能的响应/按钮，即向左移动和向右移动。
- en: Resetting the game
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重置游戏
- en: 'You can reset the game with the following code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下代码重置游戏：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The preceding snippet will reset the game and also return you the state (`x`,
    `x_dot`, `theta`, `theta_dot`) of the CartPole after the reset, which will be
    an array of the shape of  (`4`,).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码片段将重置游戏，并返回重置后 CartPole 的状态（`x`、`x_dot`、`theta`、`theta_dot`），该状态将是形状为 (`4`,)
    的数组。
- en: Playing the game
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩游戏
- en: 'Now, once you have reset the game, all there is to do is play. You can feed
    your actions/responses to the game with the use of the following code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦你重置了游戏，接下来就是玩游戏。你可以使用以下代码将你的动作/响应输入到游戏中：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `env.step` function accepts your response/action (move left or right) and
    generates the `new_state`/orientation (x, x_dot, theta, theta_dot) of the CartPole
    system. Along with the new state, the `env.step` function also returns the `reward`,
    which indicates the score you receive for the `action` you just took; `done`,
    which indicates if the game has finished; and `info`, which has system-related
    information.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`env.step` 函数接受你的响应/动作（向左或向右移动），并生成 CartPole 系统的 `new_state`/方向（x, x_dot, theta,
    theta_dot）。随着新状态的生成，`env.step` 函数还返回 `reward`，即你刚才采取的 `action` 得到的分数；`done`，指示游戏是否结束；以及
    `info`，包含系统相关信息。'
- en: When the game begins, `done` is set to `False`. Only when the CartPole orientation
    exceeds the game rules will `done` be set to `True`, indicating that either the
    cart moved 2.4 units from the center or the pole was more than 45 degrees from
    the vertical.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当游戏开始时，`done` 被设置为 `False`。只有当 CartPole 的方向超出游戏规则时，`done` 才会被设置为 `True`，这表示要么小车已从中心位置移动了
    2.4 单位，要么杆子与垂直方向的夹角超过了 45 度。
- en: As long as every step you take is within the game over limits, the reward for
    that step will be 1 unit, otherwise zero.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 只要你所采取的每一步都在游戏规则范围内，那么该步骤的奖励为 1 单位，否则为零。
- en: 'Let''s play the game by making random actions:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过进行随机动作来玩这个游戏：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following is the Terminal output:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是终端输出：
- en: '![](img/fa4390d9-7bf3-4023-9a24-07358b86bbe7.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa4390d9-7bf3-4023-9a24-07358b86bbe7.png)'
- en: 'Figure 14.2: Scores from random actions game'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2：随机动作游戏的得分
- en: 'The following is the CartPole game output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 CartPole 游戏的输出：
- en: '![](img/a3a94675-ba98-4953-8e54-f72402e48d24.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3a94675-ba98-4953-8e54-f72402e48d24.png)'
- en: 'Figure 14.3: Snapshot of the CartPole game that gets displayed on the screen
    when rendered'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3：渲染时显示的 CartPole 游戏快照
- en: '`random.choice` returns a randomly selected item from a non-empty sequence
    such as a list/array.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`random.choice` 从非空序列（如列表/数组）中返回随机选择的项。'
- en: Q-learning
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning
- en: Q-learning is a policy-based reinforcement learning technique where the goal
    of Q-learning is to learn an optimal policy that helps an agent decide what action
    to take under which circumstances of the environment.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 是一种基于策略的强化学习技术，Q-learning 的目标是学习一种最优策略，帮助智能体在环境的不同状态下决定采取什么行动。
- en: To implement Q-learning, you need to understand what a *Q* function is.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现 Q-learning，你需要了解什么是 *Q* 函数。
- en: A *Q* function accepts a state and a corresponding action as input and yields
    the total expected reward. It can be expressed as *Q(s, a)*. When at the *s* state,
    an optimal *Q* function indicates to the agent how good of a choice is picking
    an action, *a*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *Q* 函数接受一个状态和相应的动作作为输入，并返回总期望奖励。它可以表示为 *Q(s, a)*。当处于 *s* 状态时，最优的 *Q* 函数会告诉智能体选择某个动作
    *a* 的优劣。
- en: 'For a single state, *s,* and an action, *a*, *Q(s, a)* can be expressed in
    terms of the *Q* value of the next state, *s''*, given by using the following
    equation:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单一状态 *s* 和动作 *a*，*Q(s, a)* 可以通过以下公式表示为下一个状态 *s'* 的 *Q* 值：
- en: '![](img/f8058bff-e7d2-4eb6-b543-551e2a944f5d.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8058bff-e7d2-4eb6-b543-551e2a944f5d.png)'
- en: This is known as the Bellman equation. It tells us that the maximum reward is
    the sum of the reward the agent received for entering the current state, *s,*
    and the discounted maximum future reward for the next state, *s'*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是著名的贝尔曼方程。它告诉我们，最大奖励是智能体进入当前状态 *s* 所得到的奖励和下一个状态 *s'* 的折扣后最大未来奖励之和。
- en: 'The following is the pseudocode for the Q-learning algorithm from the book
    *Reinforcement Learning: An Introduction,* by Richard S. Sutton and Andrew G.
    Barto:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是《强化学习：导论》一书中 Q-learning 算法的伪代码，作者是 Richard S. Sutton 和 Andrew G. Barto：
- en: '![](img/a0808dbd-1be2-487f-83de-3e055b19ffb1.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0808dbd-1be2-487f-83de-3e055b19ffb1.png)'
- en: 'Figure 14.4: Pseudocode for Q-learning'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4：Q-learning 的伪代码
- en: '*Reinforcement Learning: An Introduction* by Richard S. Sutton and Andrew G.
    Barto *(*[http://incompleteideas.net/book/ebook/the-book.html](http://incompleteideas.net/book/ebook/the-book.html)).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 《强化学习：导论》一书，由 Richard S. Sutton 和 Andrew G. Barto 编著 *（* [http://incompleteideas.net/book/ebook/the-book.html](http://incompleteideas.net/book/ebook/the-book.html)）。
- en: Defining hyperparameters for Deep Q Learning (DQN)
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义深度 Q 学习（DQN）的超参数
- en: 'The following are some of the hyperparameters defined that we will be using
    throughout the code and are totally configurable:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在代码中使用的一些超参数，这些超参数完全可配置：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following are the parameters used:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是所使用的参数：
- en: '`gamma` : Discount parameter in the Bellman equation'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gamma`：贝尔曼方程中的折扣参数'
- en: '`epsilon_decay`: Multiplier by which you want to discount the value of `epsilon` after
    each episode/game'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon_decay`：你希望在每一局/游戏后按比例折扣`epsilon`的值。'
- en: '`epsilon_min`: Minimum value of `epsilon` beyond which you do not want to decay
    it'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epsilon_min`：`epsilon`的最小值，低于该值后不再衰减。'
- en: '`deque_len`: Size of the `deque` container used to store the training examples
    (state, reward, done, and action)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deque_len`：`deque`容器的大小，用于存储训练示例（包括状态、奖励、完成标志和动作）。'
- en: '`target_score`: The average score over 100 epochs that you want the agent to
    score after which you stop the learning process'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target_score`：你希望代理在100个训练周期内达到的平均得分，达到该分数后停止学习过程。'
- en: '`episodes`: Maximum number of games you want the agent to play'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`episodes`：你希望代理玩的最大游戏次数。'
- en: '`batch_size`: Size of the batch of training data (stored in the `deque` container)
    used to train the agent after each episode'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`：用于训练代理的批量数据大小（存储在`deque`容器中），每一局游戏后使用这些数据来训练代理。'
- en: '`optimizer`: Optimizer of choice for training the agent'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`optimizer`：用于训练代理的优化器'
- en: '`loss`: Loss of choice for training the agent'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`：用于训练代理的损失函数'
- en: Experiment with different learning rates, optimizers, batch sizes as well as
    `epsilon_decay` values to see how these factors affect the quality of your model
    and, if you get better results, show it to the deep learning community.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同的学习率、优化器、批次大小以及`epsilon_decay`值，看看这些因素如何影响模型的质量。如果得到更好的结果，分享给深度学习社区。
- en: Building the model components
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建模型组件
- en: 'In this section, we will define all of the functions that go into training
    the reinforcement learning agent. These functions are as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义所有用于训练强化学习代理的函数。这些函数如下：
- en: Agent
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理
- en: Agent action
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理动作
- en: Memory
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记忆
- en: Performance plot
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能图
- en: Replay
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回放
- en: Training and testing to train and test the agent
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和测试，用于训练和测试代理
- en: Defining the agent
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义代理
- en: Let's define an agent/function approximator.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个代理/函数近似器。
- en: The agent is nothing but a simple deep neural network that takes in the state
    (four variables) of the CartPole system and returns the maximum possible reward
    for each of the two actions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 代理其实就是一个简单的深度神经网络，输入的是CartPole系统的状态（四个变量），输出的是每个动作的最大可能奖励。
- en: The first, second, and third layers are simple `Dense` layers with 16 neurons
    and with activation as `relu`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第一、第二和第三层是简单的`Dense`层，具有16个神经元，激活函数为`relu`。
- en: 'The final layer is a `Dense` layer with two neurons equal to the number of
    possible `actions`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层是一个`Dense`层，具有两个神经元，等于可能的`actions`数量：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following is the output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/11f5fa24-2da6-4ebf-9cdf-0252287c1c2d.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11f5fa24-2da6-4ebf-9cdf-0252287c1c2d.png)'
- en: 'Figure 14.5: Summary of the agent'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5：代理摘要
- en: Play around with the parameters of the agent to suit the needs of the problem
    you are trying to solve. Try using leaky `relu` in the model if needed.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你要解决的问题调整代理的参数。必要时可以在模型中尝试使用泄漏`relu`。
- en: Defining the agent action
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义代理动作
- en: 'Let''s define a function that, when called, will return the action that needs
    to be taken for that specific state:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个函数，当被调用时，将返回该特定状态下需要采取的动作：
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: For any value from the uniform distribution (between 0 and 1), less than or
    equal to `epsilon`, the action returned will be `random`. For any value greater
    than `epsilon`, the action chosen will be that predicted by the agent we have
    defined in the preceding code.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于来自均匀分布（介于0和1之间）的任何值，如果小于或等于`epsilon`，返回的动作将是`random`。对于任何大于`epsilon`的值，选择的动作将是我们在前面的代码中定义的代理预测的动作。
- en: The `numpy.random.rand` function generates a random number from a uniform distribution
    over 0 and 1\. `numpy.argmax` returns the index of the maximum value in the sequence.
    `random.randrange` returns a randomly selected item from `range()`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`numpy.random.rand`函数从0到1的均匀分布中生成一个随机数。`numpy.argmax`返回序列中最大值的索引。`random.randrange`从`range()`中随机选择一个项目。'
- en: Defining the memory
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义记忆
- en: 'Let''s define a `deque` object to store the information (`state`, `action`,
    `reward`, and `done`) related to every relevant step we take when playing the
    game. We will then be using the data stored in this `deque` object for training:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个`deque`对象，用来存储与每个相关步骤（如游戏过程中的`state`、`action`、`reward`和`done`）相关的信息。我们将使用存储在这个`deque`对象中的数据进行训练：
- en: '[PRE11]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We have defined the `deque` object to be of a size of `20000`. Once this container
    is filled with 20,000 data points, every new append being made at one end will
    result in popping a data point at the other end. Then, we will end up retaining
    only the latest information over time.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`deque`对象的大小定义为`20000`。一旦该容器填满20,000个数据点，每当在一端添加新数据时，另一端的数据点会被弹出。然后，我们将只保留最新的信息。
- en: 'We will define a function called `memory`, which, when called during the game,
    will accept the information related to `action`, `state`, `reward`, and `done`
    as input at that time step, and then will store it in the training data `deque`
    container we have defined in the preceding code. You will see that we are storing
    these five variables as a tuple entry at each timestep:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个名为`memory`的函数，在游戏中调用时，它会在该时间步接受与`action`、`state`、`reward`和`done`相关的信息作为输入，然后将其存储在我们在前面代码中定义的训练数据`deque`容器中。你会看到，我们在每个时间步将这五个变量作为元组条目进行存储：
- en: '[PRE12]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Defining the performance plot
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义性能图
- en: 'The following `performance_plot` function plots the performance of the model
    over time. This function has been placed such that it is only plotted once our
    target of 200 points has been reached. You can also place this function to plot
    the progress after every 100 episodes during training:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`performance_plot`函数绘制模型的性能随时间变化的图像。这个函数被放置在只有当我们达成200分的目标时才会绘制。你也可以将这个函数放置在每训练100个回合后绘制进度：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'A sample plot output of the function (after the goal has been achieved) is
    shown in the following screenshot:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是函数（在目标达成后）示例图输出的截图：
- en: '![](img/16cad3a0-7a27-4dd1-b6e6-3cab837c971d.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16cad3a0-7a27-4dd1-b6e6-3cab837c971d.png)'
- en: 'Figure 14.6: Sample plot output of performance_plot function'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6：性能图函数的示例输出
- en: Defining replay
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义replay
- en: 'The following `replay` function is called inside the `train` function (defined
    in the next section) at the end of the game for training the agent. It is in this
    function that we define the targets for each state using the *Q* function Bellman
    equation:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下`replay`函数会在游戏结束时，在`train`函数（在下一节定义）内部被调用，用于训练代理。在这个函数中，我们使用*Q*函数贝尔曼方程来定义每个状态的目标：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It is inside this function that we train the agent compiled with mean squared
    error loss to learn to maximize the reward. We have done so because we are predicting
    the numerical value of the reward possible for the two actions. Remember that
    the agent accepts the state as input that is of a shape of 1*4\. The output of
    this agent is of shape 1*2, and it basically contains the expected reward for
    the two possible actions.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 正是在这个函数中，我们训练代理使用均方误差损失来学习最大化奖励。我们这样做是因为我们在预测两个动作的奖励的数值。记住，代理将状态作为输入，状态的形状是1*4。该代理的输出形状是1*2，它基本上包含了两个可能动作的期望奖励。
- en: So, when an episode ends, we use a batch of data stored in the `deque` container
    to train the agent.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当一个回合结束时，我们使用存储在`deque`容器中的一批数据来训练代理。
- en: 'In this batch of data, consider the 1^(st) tuple:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这批数据中，考虑第1个元组：
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For the `state`, we know the `action` that needs to be taken to enter the `new_state` and `reward` for
    doing so. We also have `done`, which indicates whether the `new_state` entered
    is within the game rules.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`state`，我们知道需要采取的`action`以进入`new_state`，以及为此所获得的`reward`。我们还有`done`，它表示进入的`new_state`是否符合游戏规则。
- en: 'As long as the new state, *s''* ,being entered is within the game rules, that
    is, `done` is `False`, the total `reward` according to the Bellman equation for
    entering the new state *s''* form state *s* by taking an `action` can be written
    in Python as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 只要进入的新状态，*s'*，符合游戏规则，即`done`为`False`，根据贝尔曼方程，进入新状态*s'*通过采取`action`从状态*s*过渡的总`reward`可以在Python中写为如下：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Output of `model.predict(new_state)[0]` be `[-0.55639267, 0.37972435]`. The `np.amax([-0.55639267,
    0.37972435])` will be `0.37972435`*.*
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.predict(new_state)[0]`的输出为`[-0.55639267, 0.37972435]`。`np.amax([-0.55639267,
    0.37972435])`的结果为`0.37972435`*。*'
- en: With the discount/`gamma` as 0.95 and the `reward` as `1`, this gives us the
    following value. The `reward + gamma * np.amax(model.predict(new_state)[0])` end
    us up as `1.36073813587427`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在折扣/`gamma`为0.95和`reward`为`1`时，得到如下值。`reward + gamma * np.amax(model.predict(new_state)[0])`的结果为`1.36073813587427`。
- en: This is the value of the target defined previously.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这是先前定义的目标值。
- en: Using the model, let's predict the reward for the two possible actions for the
    current state. `target_f = model.predict(state)` will be `[[-0.4597198 0.31523475]]`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型，我们预测当前状态下两种可能动作的奖励。`target_f = model.predict(state)` 将返回 `[[-0.4597198
    0.31523475]]`。
- en: Since we already know the `action` that needs to be taken for the `state`, which
    is `0`, to maximize the reward for the next state, we will set the `reward` at
    index zero of `target_f` equal to the `reward` computed using the Bellman equation,
    which is, `target_f[0][action] = 1.3607381358742714`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经知道需要采取的 `action`，即 `0`，以最大化下一个状态的奖励，我们将 `target_f` 中索引为零的 `reward` 设置为使用贝尔曼方程计算得到的
    `reward`，即 `target_f[0][action] = 1.3607381358742714`。
- en: Finally, `target_f` will be equal to `[[1.3607382 0.31523475]]`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，`target_f` 将等于 `[[1.3607382 0.31523475]]`。
- en: We will use the state as `input` and the `target_f` as the target reward and
    fit the agent/model on it.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用状态作为 `input`，`target_f` 作为目标奖励，并根据它来训练代理/模型。
- en: This process will be repeated for all of the data points in the batch of training
    data. Also, for each call of the replay function, the value of epsilon is reduced
    by the multiplier epsilon decay.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程将对训练数据批次中的所有数据点重复执行。此外，每次调用回放函数时，epsilon 的值会根据衰减因子减少。
- en: '`random.sample` samples *n* elements from a population set. `np.amax` returns
    the maximum value in an array.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`random.sample` 从一个集合中随机抽取 *n* 个元素。`np.amax` 返回数组中的最大值。'
- en: Training loop
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练循环
- en: 'Now, let''s put all of the pieces we have formed until now together to implement
    training of the agent using the `train()` function that we have defined here:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将到目前为止形成的所有部分结合起来，使用我们在此定义的 `train()` 函数实现代理的训练：
- en: Load the agent by calling the `agent()` function and compile it with the loss
    as `loss` and with the optimizer as `optimizer`, which we have defined in the
    *Defining hyperparameters for Deep Q Learning (DQN)* section.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `agent()` 函数加载代理，并将其与损失函数 `loss` 和优化器 `optimizer` 编译，这些内容我们已在 *定义深度 Q 学习（DQN）超参数*
    章节中定义。
- en: Reset the environment and reshape the initial state.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境并调整初始状态的形状。
- en: Call the `agent_action` function by passing the `model`, `epsilon`, and `state`
    information and obtain the next action that needs to be taken.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `agent_action` 函数，传入 `model`、`epsilon` 和 `state` 信息，获取需要采取的下一个动作。
- en: Take the action obtained in *Step 3* using the `env.step` function. Store the
    resulting information in the `training_data` deque container by calling the `memory`
    function and passing the required arguments.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `env.step` 函数获取在 *Step 3* 中获得的动作。通过调用 `memory` 函数并传递必要的参数，将结果信息存储在 `training_data`
    双端队列容器中。
- en: Assign the new state obtained in *Step 4* to the `state` variable and increment
    the time step by 1 unit.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将在 *Step 4* 中获得的新状态分配给 `state` 变量，并将时间步长增加 1 单位。
- en: Until done resulting in *Step 4* turns `True`, repeat *Step 3* through *Step
    5*.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直到 *Step 4* 返回 `True`，重复 *Step 3* 到 *Step 5*。
- en: Call the `replay` function to train the agent on a batch of the training data
    at the end of the episode/game.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次回合/游戏结束时，调用 `replay` 函数，在一批训练数据上训练代理。
- en: 'Repeat *Step 2* through *Step 7* until the target score has been achieved:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *Step 2* 到 *Step 7*，直到达到目标分数：
- en: 'Following code shows the implementation of the `train()` function:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了 `train()` 函数的实现：
- en: '[PRE17]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'For the remaining part of this code snippet, please refer to the `DQN.ipynb`
    file here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/DQN.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/DQN.ipynb)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此代码片段的其余部分，请参阅此处的 `DQN.ipynb` 文件：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/DQN.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/DQN.ipynb)
- en: To view the CartPole game on your screen when training, set the `render` argument
    to `True` inside the `train` function. Also, visualizing the game will slow down
    the training.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 若要在训练时在屏幕上查看 CartPole 游戏，请将 `train` 函数中的 `render` 参数设置为 `True`。另外，游戏可视化会减慢训练速度。
- en: 'The following two images are the outputs generated during training of DQN:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两张图片是 DQN 训练过程中生成的输出：
- en: '![](img/531bf89c-4cb8-4708-8727-89a4ec9b6f6f.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/531bf89c-4cb8-4708-8727-89a4ec9b6f6f.png)'
- en: 'Figure 14.7: Scores output when training the agent'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7：训练代理时的分数输出
- en: '![](img/79840a92-e93b-4271-a9fe-953773276f67.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79840a92-e93b-4271-a9fe-953773276f67.png)'
- en: 'Figure 14.8: Plot of scores v/s episodes when training the agent'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8：训练代理时分数与回合的关系图
- en: We can see that, when training the agent, our target score of `200` points averaging
    over `100` latest episodes was reached at the end of `300` games.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在训练智能体时，我们设定的200分目标在`300`场游戏结束时达到了，且这个分数是在最近`100`场游戏中平均计算得出的。
- en: We have been using the epsilon-greedy policy to train the agent. Feel free to
    use other policies listed at [https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py),
    once you have finished mastering the training of DQN.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在使用ε-greedy策略来训练智能体。一旦你掌握了DQN的训练过程，可以尝试使用[https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py)中列出的其他策略。
- en: It is not always necessary that, when you give a try at training the agent,
    it takes you just 300 games. In some cases, it might even take more than 300\.
    Refer to the notebook at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2015/DQN.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2015/DQN.ipynb)
    to see the five tries made at training the agent and the number of episodes it
    took to train it.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 训练智能体时，不一定每次都能在300场游戏内完成。有时可能需要超过300场游戏。你可以参考这个笔记本：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2015/DQN.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter%2015/DQN.ipynb)，查看训练智能体的五次尝试以及每次训练所用的游戏回合数。
- en: Testing the DQN model
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试DQN模型
- en: 'Now, let''s test how our trained DQN model performs on new games. The following `test`
    function uses the trained DQN model to play ten games and see whether our average
    target of 200 points will be achieved:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们测试我们训练的DQN模型在新游戏中的表现。以下`test`函数使用训练好的DQN模型进行十场游戏，看看我们设定的200分的目标能否达成：
- en: '[PRE18]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: To view the CartPole game on your screen when testing, set the `render` argument
    to `true` inside the `test` function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 要在测试时查看CartPole游戏画面，请在`test`函数内将`render`参数设置为`true`。
- en: 'The following is the output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/a9fa9444-0a9e-45dc-ac84-5f4184434e14.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9fa9444-0a9e-45dc-ac84-5f4184434e14.png)'
- en: 'Figure 14.9: Test scores with the trained Q agent'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.9：使用训练好的Q智能体测试得分
- en: When the agent is tested on the new 100 CartPole games, it is averaging a score
    of `277.88`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当智能体在新的100场CartPole游戏中进行测试时，它的平均得分为`277.88`。
- en: Remove the threshold of 200 points and aim at training the agent to consistently
    score an average of 450 points or more.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 移除200分的门槛，目标是训练智能体始终保持平均得分为450分或更多。
- en: Deep Q-learning scripts in modular form
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q学习脚本模块化形式
- en: The entire script can be split into four modules named `train_dqn.py`, `agent_reply_dqn.py`,
    `test_dqn.py`, and `hyperparameters_dqn.py`. Store these in a folder of your choice,
    for example `chapter_15`. Set `chapter_15` as the project folder in your favorite
    source code editor and just run the `train_dqn.py` file.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 整个脚本可以分为四个模块，分别是`train_dqn.py`、`agent_reply_dqn.py`、`test_dqn.py`和`hyperparameters_dqn.py`。将它们存储在你选择的文件夹中，例如`chapter_15`。将`chapter_15`设置为你喜欢的源代码编辑器中的项目文件夹，然后运行`train_dqn.py`文件。
- en: The `train_dqn.py` Python file will import functions from all of the other modules
    in places where they are needed for execution.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_dqn.py` Python文件将在需要执行的地方从其他模块中导入函数。'
- en: Now let's walk through the contents of each file.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们逐步讲解每个文件的内容。
- en: Module 1 – hyperparameters_dqn.py
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块 1 – hyperparameters_dqn.py
- en: 'This Python file contains the hyperparameters of the DQN model:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Python文件包含DQN模型的超参数：
- en: '[PRE19]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Module 2 – agent_replay_dqn.py
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块 2 – agent_replay_dqn.py
- en: 'This Python file contains the four functions, namely  `agent()`, `agent_action()`,
    `performance_plot()`, and `replay()`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Python文件包含四个函数，分别是`agent()`、`agent_action()`、`performance_plot()`和`replay()`：
- en: '[PRE20]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For the remaining part of this file, please visit here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/agent_replay_dqn.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/agent_replay_dqn.py)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此文件的其余部分，请访问这里：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/agent_replay_dqn.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/agent_replay_dqn.py)
- en: Module 3 – test_dqn.py
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块 3 – test_dqn.py
- en: 'This module contains the `test()` function, which will be called in the `train_dqn.py`
    script to test the performance of the DQN agent:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模块包含`test()`函数，它将在`train_dqn.py`脚本中调用，以测试DQN智能体的表现：
- en: '[PRE21]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Module 4 – train_dqn.py
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块 4 – train_dqn.py
- en: 'In this module, we include the `memory()` and  `train()` functions and also
    the calls to train and test the reinforcement learning model:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模块中，我们包括了`memory()`和`train()`函数，并且调用了用于训练和测试强化学习模型的函数：
- en: '[PRE22]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'For the remaining part of this code, please visit here: [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/train_dqn.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/train_dqn.py)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此代码的其余部分，请访问：[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/train_dqn.py](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/train_dqn.py)
- en: Deep SARSA learning
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度SARSA学习
- en: In this segment, we will implement deep SARSA learning with the `keras-rl` library.
    The `keras-rl` library is a simple neural network API that allows simple and easy
    implementation of reinforcement learning models (Q, SARSA, and others). To learn
    more about the `keras-rl` library, visit the documentation at [https://keras-rl.readthedocs.io/en/latest/](https://keras-rl.readthedocs.io/en/latest/).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，我们将使用`keras-rl`库实现深度SARSA学习。`keras-rl`库是一个简单的神经网络API，允许简单而易于实现强化学习模型（Q、SARSA等）。要了解更多有关`keras-rl`库的信息，请访问文档：[https://keras-rl.readthedocs.io/en/latest/](https://keras-rl.readthedocs.io/en/latest/)。
- en: We will be using the same CartPole environment we have been using so far from
    OpenAI Gym.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用到现在为止在OpenAI Gym中使用的相同CartPole环境。
- en: A Jupyter Notebook code example for deep SARSA learning can be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/Deep%20SARSA.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/Deep%20SARSA.ipynb).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关于深度SARSA学习的Jupyter Notebook代码示例可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/Deep%20SARSA.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter14/Deep%20SARSA.ipynb)找到。
- en: SARSA learning
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SARSA学习
- en: SARSA learning, like Q-learning, is also a policy-based reinforcement learning
    technique. Its goal is to learn an optimal policy, which helps an agent decide
    on the action that needs to be taken under various possible circumstances.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA学习方法和Q-learning一样，都是基于策略的强化学习技术。它的目标是学习一个最优策略，帮助智能体在不同的可能情况下决定需要采取的行动。
- en: SARSA and Q-learning are very similar to each other, except Q-learning is an
    off-policy algorithm and SARSA is an on-policy algorithm. The Q value learned
    by SARSA is not based on a greedy policy like in Q-learning but is based on the
    action performed under the current  policy.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA和Q-learning非常相似，除了Q-learning是一个脱离策略的算法，而SARSA是一个基于策略的算法。SARSA学习的Q值不是像Q-learning那样基于贪心策略，而是基于当前策略下执行的动作。
- en: 'For a single state, *s,* and an action, *a*, *Q(s, a)* can be expressed in
    terms of the Q value of the next state, *s''* ,and action, *a''*, given by the
    following formula:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个状态，*s*，和一个动作，*a*，*Q(s, a)* 可以通过以下公式表示为下一个状态，*s'*，和动作，*a'*，的Q值：
- en: '![](img/b69b5557-639d-477c-b48a-2bb34d6d41ce.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b69b5557-639d-477c-b48a-2bb34d6d41ce.png)'
- en: 'The following is the pseudocode for the SARSA learning algorithm from the book, *Reinforcement
    Learning: An Introduction,* by Richard S. Sutton and Andrew G. Barto:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是《强化学习：导论》一书中，*理查德·S·萨顿*和*安德鲁·G·巴托*编写的SARSA学习算法的伪代码：
- en: '![](img/872c9fb4-5fd1-41ae-a9cb-d92c1112e656.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/872c9fb4-5fd1-41ae-a9cb-d92c1112e656.png)'
- en: 'Figure 14.10: Pseudocode for SARSA learning'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10：SARSA学习的伪代码
- en: Importing all of the dependencies
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入所有依赖项
- en: 'We will be using `numpy`, `gym`, `matplotlib`, `keras`, `tensorflow`, and the `keras-rl` package
    in this segment of the exercise. Here, TensorFlow will be used as the backend
    for Keras. You can install these packages with `pip`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分练习中，我们将使用`numpy`、`gym`、`matplotlib`、`keras`、`tensorflow`和`keras-rl`包。这里，TensorFlow将作为Keras的后端。您可以使用`pip`安装这些包：
- en: '[PRE23]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Loading the game environment
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载游戏环境
- en: 'Just like we loaded the game in the DQN segment, we will load the game into
    the workspace and set `seed` for reproducibility:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在DQN部分加载游戏一样，我们将游戏加载到工作区并设置`seed`以确保结果可重复：
- en: '[PRE24]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Defining the agent
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义智能体
- en: 'For deep SARSA learning, we will be using the same agent we used in the Deep
    Q-learning segment:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度SARSA学习，我们将使用在深度Q-learning部分中使用的相同智能体：
- en: '[PRE25]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Training the agent
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练智能体
- en: 'Training an agent using the `keras-rl` library is very easy:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`keras-rl`库训练智能体非常简单：
- en: Define the policy you want the training to follow. We will be using the epsilon-greedy
    policy. The equivalent of this in the DQN section would be the agent `action`
    function. To know more about other policies, visit [https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py).
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义您希望训练遵循的策略。我们将使用epsilon-greedy策略。与DQN部分相对应的是智能体的`action`函数。要了解更多其他策略的信息，请访问[https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py](https://github.com/keras-rl/keras-rl/blob/master/rl/policy.py)。
- en: Load the agent you would like to use. In this case, the SARSA agent has a lot
    of parameters of which the important ones that need to be defined are `model`,
    `nb_actions`, and `policy`. `model` is the deep learning agent you have defined
    in the preceding code, `nb_actions` is the number of possible actions in the system, and
    `policy` is your preferred choice of policy to train the SARSA agent.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载您希望使用的智能体。在这种情况下，SARSA智能体有许多参数，重要的参数包括`model`、`nb_actions`和`policy`。`model`是您在前面代码中定义的深度学习智能体，`nb_actions`是系统中可能的操作数，而`policy`是您偏好的训练SARSA智能体的策略。
- en: We compile the SARSA agent with loss and optimizer of choice.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为SARSA智能体编译所选的损失函数和优化器。
- en: 'We fit the SARSA agent by feeding the `.fit` function the arguments environment
    and number of steps to train:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过将环境和训练步数作为参数传递给`.fit`函数来训练SARSA智能体：
- en: To get complete details on the usage of agents from the `keras-rl` library and
    their parameter definitions, visit this documentation by Keras at [http://keras-rl.readthedocs.io/en/latest/agents/sarsa/#sarsaagent](http://keras-rl.readthedocs.io/en/latest/agents/sarsa/#sarsaagent).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取`keras-rl`库中智能体的完整使用细节及其参数定义，请访问Keras的文档：[http://keras-rl.readthedocs.io/en/latest/agents/sarsa/#sarsaagent](http://keras-rl.readthedocs.io/en/latest/agents/sarsa/#sarsaagent)。
- en: '[PRE26]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: To view the CartPole game on your screen when training, set visualize argument
    to true inside the `.fit` function. But visualizing the game will slow down the
    training.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时，要在`.fit`函数内将`visualize`参数设置为`true`，以便在屏幕上查看CartPole游戏。但可视化游戏会减慢训练速度。
- en: 'Here is the scores output when training the SARSA agent:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练SARSA智能体时的得分输出：
- en: '![](img/e8f1da64-1c3b-407b-bb32-2394bed5a92a.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8f1da64-1c3b-407b-bb32-2394bed5a92a.png)'
- en: 'Figure 14.11: Scores output when training SARSA agent'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.11：训练SARSA智能体时的得分输出
- en: Testing the agent
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试智能体
- en: 'Once the agent has been trained, we evaluate its performance over 100 new episodes.
    This can be done by calling the `.test` function and feeding the arguments environment
    and number of episodes on which to test:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦智能体经过训练，我们将在100个新的回合上评估其表现。可以通过调用`.test`函数并提供测试环境和回合数作为参数来实现：
- en: '[PRE27]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: To view the CartPole game on your screen when testing, set the `visualize` argument
    to `True` inside the `.test` function.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试时，要在`.test`函数内将`visualize`参数设置为`True`，以便在屏幕上查看CartPole游戏。
- en: 'The following is the output after testing 100 episodes:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是测试100轮后的输出：
- en: '![](img/8476eba6-c485-4ca5-96c4-a16d9c20fbce.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8476eba6-c485-4ca5-96c4-a16d9c20fbce.png)'
- en: 'Following the the output at the end of the code execution:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码执行结束后的输出：
- en: '![](img/ab9fad31-1de5-4204-b1fe-23c09ea57abd.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab9fad31-1de5-4204-b1fe-23c09ea57abd.png)'
- en: 'Figure 14.12: Test scores with trained SARSA agent'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.12：训练后的SARSA智能体测试得分
- en: Deep SARSA learning script in modular form
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度SARSA学习脚本（模块化形式）
- en: 'For SARSA learning, we have only one script, which implements both the training
    and testing of the SARSA agent:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SARSA学习，我们只有一个脚本，它实现了SARSA智能体的训练和测试：
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The conclusion to the project
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目的结论
- en: This project was to build a deep reinforcement learning model to successfully
    play the game of CartPole-v1 from OpenAI Gym. The use case of this chapter is
    to build a reinforcement learning model on a simple game environment and then
    extend it to other complex games such as Atari.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的目标是构建一个深度强化学习模型，成功地玩OpenAI Gym中的CartPole-v1游戏。本章的用例是在一个简单的游戏环境中构建强化学习模型，然后将其扩展到其他复杂的游戏，如Atari。
- en: In the first half of this chapter, we built a deep Q-learning model to play
    the CartPole game. The DQN model during testing scored an average of 277.88 points
    over 100 games.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 本章前半部分，我们构建了一个深度Q学习（DQN）模型来玩CartPole游戏。在测试过程中，DQN模型在100场游戏中的平均得分为277.88分。
- en: In the second half of this chapter, we built a deep SARSA learning model (using
    the same epsilon-greedy policy as Q-learning) to play the CartPole game. The SARSA
    model during testing scored an average of 365.67 points over 100 games.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后半部分，我们建立了一个深度 SARSA 学习模型（使用与 Q-learning 相同的 epsilon-greedy 策略）来玩 CartPole
    游戏。SARSA 模型在测试期间，100 局游戏的平均得分为 365.67 分。
- en: Now, let's follow the same technique we have been following in the previous
    chapters for evaluating the performance of the models from the restaurant chain
    point of view.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们遵循之前章节中用于评估模型表现的相同方法，从餐饮连锁的角度来看待模型的表现。
- en: What are the implications of this score?
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这个得分意味着什么？
- en: An average score of 277.88 with Q-learning means that we have successfully solved
    the game of CartPole as defined on the OpenAI site. It also means that our model
    survives slightly more than half the length of the game with the total game length
    being 500 points.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning 平均得分为 277.88，这意味着我们已经成功解决了 OpenAI 网站定义的 CartPole 游戏。这也意味着我们的模型能够存活超过游戏时长的一半，且总游戏时长为
    500 分钟。
- en: As regards SARSA learning, on the other hand, an average score of 365.67 with
    Q-learning means that we have successfully solved the game of CartPole as defined
    on the OpenAI site and that our model survives more than 70% the length of the
    game, with the total game length being 500 points.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 至于 SARSA 学习，另一方面，Q-learning 的平均得分为 365.67，这意味着我们已经成功解决了 OpenAI 网站定义的 CartPole
    游戏，并且我们的模型能够存活超过游戏时长的 70%，总游戏时长为 500 分钟。
- en: It is still not a level of performance you should be happy with because the
    goal should not just be to solve the problem but to train a model that is really
    good at scoring a consistent 500 points at each game, so you can see why we'd
    need to continue fine-tuning the models to get the maximum performance possible.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然不是你应该感到满意的表现水平，因为目标不仅仅是解决问题，而是训练一个能够在每局游戏中稳定得分 500 分的优秀模型，因此你可以理解为什么我们需要继续对模型进行微调，以获得最大的性能。
- en: Summary
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have successfully built a deep reinforcement learning model,
    each with Q-learning and SARSA learning in Keras using the CartPole game from
    OpenAI Gym. We understood Q-learning, SARSA learning, how to interact with game
    environments from Gym, and the function of the agent (deep learning model). We
    defined some key hyperparameters, as well as, in some places, reasoned with why
    we used what we did. Finally, we tested the performance of our reinforcement learning
    on new games and determined that we succeeded in achieving our goals.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们成功构建了一个深度强化学习模型，分别使用 Q-learning 和 SARSA 学习，基于 OpenAI Gym 中的 CartPole
    游戏。我们理解了 Q-learning、SARSA 学习，如何与 Gym 中的游戏环境交互，以及代理（深度学习模型）的功能。我们定义了一些关键的超参数，并且在一些地方，我们也解释了为什么选择使用这些方法。最后，我们在新游戏上测试了我们强化学习模型的表现，并确定我们成功地实现了目标。
