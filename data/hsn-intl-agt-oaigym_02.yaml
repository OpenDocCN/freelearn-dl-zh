- en: Reinforcement Learning and Deep Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习与深度强化学习
- en: This chapter provides a concise explanation of the basic terminology and concepts
    in reinforcement learning. It will give you a good understanding of the basic
    reinforcement learning framework for developing artificial intelligent agents.
    This chapter will also introduce deep reinforcement learning and provide you with
    a flavor of the types of advanced problems the algorithms enable you to solve.
    You will find mathematical expressions and equations used in quite a few places
    in this chapter. Although there's enough theory behind reinforcement learning
    and deep reinforcement learning to fill a whole book, the key concepts that are
    useful for practical implementation are discussed in this chapter, so that when
    we actually implement the algorithms in Python to train our agents, you can clearly
    understand the logic behind them. It is perfectly alright if you are not able
    to grasp all of it in your first pass. You can always come back to this chapter
    and revise whenever you need a better understanding.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章简要解释了强化学习中的基本术语和概念，帮助你更好地理解用于开发人工智能智能体的基本强化学习框架。本章还将介绍深度强化学习，并为你展示算法能够解决的高级问题类型。你会在本章中看到不少数学表达式和公式。尽管强化学习和深度强化学习背后有足够的理论可以填满整本书，但本章讨论的是对实际应用有帮助的关键概念，因此当我们在
    Python 中实现算法以训练我们的智能体时，你可以清晰地理解其背后的逻辑。如果你第一次阅读时没有完全掌握，完全没有问题。你可以随时返回本章，进行复习，直到你更好地理解为止。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What is reinforcement learning?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是强化学习？
- en: The Markov Decision Process
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马尔科夫决策过程
- en: The reinforcement learning framework
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习框架
- en: What is deep reinforcement learning?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是深度强化学习？
- en: How do deep reinforcement learning agents work in practice?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度强化学习智能体在实践中是如何工作的？
- en: What is reinforcement learning?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是强化学习？
- en: If you are new to the field of **Artificial Intelligence** (**AI**) or machine
    learning, you might be wondering what reinforcement learning is all about. In
    simple terms, it is learning through reinforcement. *Reinforcement*, as you know
    from general English or psychology, is the act of increasing or strengthening
    the choice to take a particular action in response to something, because of the
    perceived benefit of receiving higher rewards for taking that action. We humans
    are good at learning through reinforcement from a very young age. Those who have
    kids may be utilizing this fact more often to teach good habits to them. Nevertheless,
    we will all be able to relate to this, because not so long ago we all went through
    that phase of life! Say parents reward their kid with chocolate if the kid completes
    their homework on time after school every day. The kid *learns* the fact that
    he/she will receive chocolate (*a **reward*) if he/she completes their homework
    every day. Therefore, this strengthens their decision to finish their homework
    every day to receive the chocolate. This process of learning to strengthen a particular
    choice of action, motivated by the reward they will receive for taking such an
    action, is called learning by reinforcement or reinforcement learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是 **人工智能** (**AI**) 或机器学习领域的新手，可能会想知道强化学习到底是什么。简单来说，它是通过强化来学习。*强化*，如你从普通英语或心理学中所知，是在对某个行为的反应中增加或加强某个选择的行为，因为通过采取该行为可以获得更高的回报。我们人类从小就擅长通过强化来学习。那些有孩子的父母，可能会更频繁地利用这一点来教导他们良好的习惯。不过，我们每个人都能与此产生共鸣，因为就在不久之前，我们每个人都经历过这个阶段！比如，父母每天奖励孩子巧克力，如果孩子按时完成作业。孩子*学会*了只要按时完成作业，就能得到巧克力（*奖励*）。因此，这增强了他们每天完成作业的决心，以获得巧克力。这个通过奖励强化某个特定行为选择的学习过程，就是通过强化学习或强化学习进行的学习。
- en: 'You might be thinking, "*Oh yeah. That human psychology sounds very familiar
    to me. But what has that got to do with machine learning or AI?"* Good thought.
    The concept of reinforcement learning was in fact inspired by behavioral psychology.
    It is at the nexus of several fields of research, the most important being computer
    science, mathematics, neuroscience, and psychology, as shown in the following
    diagram:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，"*哦，是的。人类心理学听起来对我很熟悉。那么，这与机器学习或人工智能有什么关系呢？*" 好问题。强化学习的概念实际上是受行为心理学的启发。它位于多个研究领域的交汇处，最重要的包括计算机科学、数学、神经科学和心理学，正如下图所示：
- en: '![](img/00011.jpeg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00011.jpeg)'
- en: As we will soon realize, reinforcement learning is one of the most promising
    methods in machine learning leading towards AI. If all these terms are new to
    you, do not worry! Starting from the next paragraph, we will go over these terms
    and understand their relationship with each other to make you feel comfortable.
    If you already know these terms, it will be a refreshing read with a different
    perspective.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们很快就会意识到的那样，强化学习是机器学习中最有前景的方法之一，指向人工智能的未来。如果这些术语对你来说很陌生，别担心！从下一段开始，我们将一一讲解这些术语，并理解它们之间的关系，让你轻松理解。如果你已经了解这些术语，那将是一次耳目一新的阅读，从不同的视角来看待这些概念。
- en: Understanding what AI means and what's in it in an intuitive way
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直观地理解人工智能的含义及其内涵
- en: 'The intelligence demonstrated by humans and animals is called *natural intelligence*,
    but the intelligence demonstrated by machines is called AI, for obvious reasons.
    We humans develop algorithms and technologies that provide intelligence to machines.
    Some of the greatest developments on this front are in the fields of machine learning,
    artificial neural networks, and deep learning. These fields collectively drive
    the development of AI. There are three main types of machine learning paradigms
    that have been developed to some reasonable level of maturity so far, and they
    are the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人类和动物所展示的智能被称为*自然智能*，而机器所展示的智能被称为人工智能（AI），原因显而易见。我们人类开发了为机器提供智能的算法和技术。迄今为止，在这一领域的伟大进展主要体现在机器学习、人工神经网络和深度学习等领域。这些领域共同推动了人工智能的发展。目前已经发展出了三种主要的机器学习范式，并且已经达到了一定的成熟度，它们分别是：
- en: Supervised learning
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有监督学习
- en: Unsupervised learning
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reinforcement learning
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: 'In the following diagram, you can get an intuitive picture of the field of
    AI. You can see that these learning paradigms are subsets of the field of machine
    learning, and machine learning itself is a subset/branch of AI:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，你可以直观地了解人工智能的领域。你会看到，这些学习范式是机器学习领域的子集，而机器学习本身是人工智能的一个子集/分支：
- en: '![](img/00012.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00012.jpeg)'
- en: Supervised learning
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有监督学习
- en: Supervised learning is similar to how we would teach a kid to recognize someone
    or some object by name. We provide an input and a name/class label (*label* for
    short) associated with that input, and expect the machine to learn that input-to-label
    mapping. This might sound simple if we just want the machine to learn the input-to-label
    mapping for a few objects (like in object recognition-type tasks) or persons (like
    in face/voice/person recognition tasks), but what if we want a machine to learn
    about several thousand classes where each class may have several different variations
    in the input? For example, if the task is to recognize a person's face from image
    inputs, with a thousand other input images with faces to distinguish it from,
    the task might be complicated even for an adult. There might be several variations
    in the input images for the same person's face. The person may be wearing glasses
    in one of the input images, or wearing a hat in another, or sporting a different
    facial expression altogether. It is a much harder task for a machine to be able
    to see the input image, identify the face, and recognize it. With recent advancements
    in the field of deep learning, supervised classification tasks like these are
    no longer hard for machines. Machines can recognize faces, among several other
    things, at an unprecedented level of accuracy. For example, the DeepFace system
    ([https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)),
    developed by the Facebook AI research lab, reached an accuracy of 97.45% in face
    recognition on the Labelled Faces in the Wild dataset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习类似于我们教孩子通过名字识别某人或某物的方式。我们提供一个输入和与该输入相关的名字/类别标签（简称*标签*），并期望机器学习这种输入到标签的映射。如果我们只希望机器学习几个对象（如在物体识别类任务中）或几个人（如在面部/语音/人物识别任务中）的输入到标签的映射，这听起来可能很简单。但如果我们希望机器学习几千个类别，而每个类别可能在输入中有多个不同的变化呢？例如，如果任务是通过图像输入识别一个人的面部，而且需要从其他一千个包含面部的输入图像中区分出来，这个任务即使对于成年人来说也可能非常复杂。相同一个人的面部输入图像可能存在多种变化。某个输入图像中的人可能戴着眼镜，另一个图像中可能戴着帽子，或者表现出完全不同的面部表情。对于机器来说，能够看懂输入图像、识别面部并将其辨认出来是一个更具挑战性的任务。随着深度学习领域的最新进展，像这样的监督分类任务对机器来说不再困难。机器可以以前所未有的准确度识别面部，以及许多其他事物。例如，由Facebook
    AI研究实验室开发的DeepFace系统（[https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf)）在Labelled
    Faces in the Wild数据集上的面部识别准确度达到了97.45%。
- en: Unsupervised learning
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'Unsupervised learning is a form of learning in which there is no label provided
    to the learning algorithm along with the inputs, unlike the supervised learning
    paradigm. This class of learning algorithm is typically used to figure out patterns
    in the input data and cluster similar data together. A recent advancement in the
    field of deep learning introduced a new form of learning called Generative Adversarial
    Networks, which have gained massive popularity during the time this book was being
    written. If you are interested, you can learn a lot more about Generative Adversarial
    Networks from this video: [https://www.packtpub.com/big-data-and-business-intelligence/learning-generative-adversarial-networks-video](https://www.packtpub.com/big-data-and-business-intelligence/learning-generative-adversarial-networks-video).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是一种学习形式，与监督学习范式不同，它在输入数据的同时不会为学习算法提供标签。这类学习算法通常用于发现输入数据中的模式，并将相似的数据聚类在一起。深度学习领域的最新进展引入了一种新型学习方法，称为生成对抗网络（Generative
    Adversarial Networks），在本书写作期间，这一方法获得了巨大的关注。如果你感兴趣，可以通过这个视频进一步了解生成对抗网络：[https://www.packtpub.com/big-data-and-business-intelligence/learning-generative-adversarial-networks-video](https://www.packtpub.com/big-data-and-business-intelligence/learning-generative-adversarial-networks-video)。
- en: Reinforcement learning
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement learning is kind of a hybrid way of learning compared to supervised
    and unsupervised learning. As we learned at the start of this section, reinforcement
    learning is driven by a reward signal. In the case of the *kid with their homework*
    problem, the reward signal was the chocolate from their parents. In the machine
    learning world, a chocolate may not be enticing for a computer (well, we could
    program a computer to want chocolates, but why would we? Aren't kids enough?!),
    but a mere scalar value (a number) will do the trick! The reward signals are still
    human-specified in some way, signifying the intended goal of the task. For example,
    to train an agent to play Atari games using reinforcement learning, the scores
    from the games can be the reward signal. This makes reinforcement learning much
    easier (for humans and not for the machine!) because we don't have to label the
    button to be pressed at each point in the game to teach the machine how to play
    the game. Instead, we just ask the machine to learn on its own to maximize their
    score. Doesn't it sound fascinating that we could make a machine figure out how
    to play a game, or how to control a car, or how to do its homework all by itself,
    and all we have to do is just say how it did with a score? That is why we are
    learning about it in this chapter. You will develop some of those cool machines
    yourself in the upcoming chapters.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种混合型的学习方式，相较于监督学习和无监督学习。正如我们在本节开始时所了解到的，强化学习是由奖励信号驱动的。在“*做作业的孩子*”问题中，奖励信号来自父母给的巧克力。在机器学习的世界中，巧克力可能并不吸引计算机（好吧，我们可以编程让计算机想要巧克力，但为什么要这么做呢？难道孩子们还不够吗？），但是一个简单的标量值（一个数字）就能解决问题！奖励信号仍然是以某种方式由人类指定的，表示任务的预期目标。例如，为了训练智能体使用强化学习来玩雅达利游戏，游戏得分可以作为奖励信号。这使得强化学习变得更简单（对于人类而非机器！），因为我们不需要在游戏的每一时刻标记按下哪个按钮来教机器如何玩游戏。相反，我们只是让机器自主学习如何最大化得分。难道这不令人着迷吗？我们能够让机器自己找出如何玩游戏、如何驾驶汽车，或如何做作业，只要我们给出分数来评估它的表现？这就是我们在本章学习这一内容的原因。在接下来的章节中，你将亲自开发一些这样的酷炫机器。
- en: Practical reinforcement learning
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的强化学习
- en: Now that you have an intuitive understanding of what AI really means and the
    various classes of algorithm that drive its development, we will now focus on
    the practical aspects of building a reinforcement learning machine.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对人工智能的真正含义以及推动其发展的各种算法类别有了直观的理解，我们将专注于构建强化学习机器的实际方面。
- en: 'Here are the core concepts that you need to be aware of to develop reinforcement
    learning systems:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你在开发强化学习系统时需要了解的核心概念：
- en: Agent
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体
- en: Rewards
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励
- en: Environment
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境
- en: State
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态
- en: Value function
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价值函数
- en: Policy
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略
- en: Agent
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能体
- en: In the reinforcement learning world, a machine is run or instructed by a (software)
    agent. The agent is the part of the machine that possesses intelligence and makes
    decisions on what to do next. You will come across the term "agent" several times
    as we dive deeper into reinforcement learning. Reinforcement learning is based
    on the reward hypothesis, which states that any goal can be described by the maximization
    of the expected cumulative reward. So, what is this reward exactly? That's what
    we'll discuss next.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习的世界里，机器是由（软件）智能体来运行或指导的。智能体是机器中具备智能并决定接下来做什么的部分。当我们深入了解强化学习时，你会多次遇到“智能体”这个术语。强化学习基于奖励假设，该假设指出任何目标都可以通过最大化期望的累计奖励来描述。那么，究竟什么是这个奖励呢？接下来我们将讨论这个问题。
- en: Rewards
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励
- en: 'A reward, denoted by  ![](img/00013.jpeg), is usually a scalar quantity that
    is provided as feedback to the agent to drive its learning. The goal of the agent
    is to maximize the sum of the reward, and this signal indicates how well the agent
    is doing at time step ![](img/00014.jpeg).  The following examples of reward signals
    for different tasks may help you get a more intuitive understanding:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励，通常表示为 ![](img/00013.jpeg)，通常是一个标量量，它作为反馈提供给智能体，以驱动其学习。智能体的目标是最大化奖励的总和，而该信号表示智能体在时间步 ![](img/00014.jpeg)
    时的表现。以下是不同任务的奖励信号示例，可能帮助你更直观地理解这一概念：
- en: For the Atari games we discussed before, or any computer games in general, the
    reward signal can be `+1` for every increase in score and `-1` for every decrease
    in score.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于我们之前讨论的雅达利游戏，或者一般的计算机游戏，每当分数增加时，奖励信号可以是`+1`，而每当分数减少时，奖励信号则是`-1`。
- en: For stock trading, the reward signal can be `+1` for each dollar gained and
    `-1` for each dollar lost by the agent.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于股票交易，奖励信号可以是每赚取一美元就奖励`+1`，每亏损一美元就惩罚`-1`。
- en: For driving a car in simulation, the reward signal can be `+1` for every mile
    driven and `-100` for every collision.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于模拟驾驶汽车，奖励信号可以是每行驶一英里奖励`+1`，每发生一次碰撞则惩罚`-100`。
- en: Sometimes, the reward signal can be sparse. For example, for a game of chess
    or Go, the reward signal could be `+1` if the agent wins the game and `-1` if
    the agent loses the game. The reward is sparse because the agent receives the
    reward signal only after it completes one full game, not knowing how good each
    move it made was.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，奖励信号可能是稀疏的。例如，在一场国际象棋或围棋比赛中，如果代理赢得比赛，奖励信号可能是`+1`，如果代理输了比赛，则奖励信号为`-1`。奖励是稀疏的，因为代理只有在完成一整局游戏后才能收到奖励信号，而在此过程中，它无法知道每一步棋的好坏。
- en: Environment
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境
- en: In the first chapter, we looked into the different environments provided by
    the OpenAI Gym toolkit. You might have been wondering why they were called environments
    instead of problems, or tasks, or something else. Now that you have progressed
    to this chapter, does it ring a bell in your head?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们探讨了 OpenAI Gym 工具包提供的不同环境。你可能会疑惑，为什么它们被称为“环境”而不是“问题”、 “任务”或其他什么东西。现在你已经进展到了这一章，是否在脑海中响起了某种提示？
- en: 'The environment is the platform that represents the problem or task that we
    are interested in, and with which the agent interacts.  The following diagram
    shows the general reinforcement learning paradigm at the highest level of abstraction:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 环境是代表我们感兴趣的任务或问题的平台，代理在其中与环境进行互动。下图展示了最高抽象层次下的强化学习范式：
- en: '![](img/00015.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00015.jpeg)'
- en: At each time step, denoted by ![](img/00016.jpeg), the agent receives an observation ![](img/00017.jpeg) from
    the environment and then executes an action ![](img/00018.jpeg), for which it
    receives a scalar reward ![](img/00019.jpeg) back from the environment, along
    with the next observation ![](img/00020.jpeg), and then this process repeats until
    a terminal state is reached. What is an observation and what is a state? Let's
    look into that next.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步（![](img/00016.jpeg)）中，代理从环境中接收到一个观察（![](img/00017.jpeg)），然后执行一个动作（![](img/00018.jpeg)），并从环境中获得一个标量奖励（![](img/00019.jpeg)），同时还会获得下一个观察（![](img/00020.jpeg)）。这个过程会不断重复，直到达到终止状态。什么是观察，什么是状态？接下来我们来深入探讨一下。
- en: State
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 状态
- en: 'As the agent interacts with an environment, the process results in a sequence
    of observations (![](img/00021.jpeg)), actions (![](img/00022.jpeg)), and rewards
    (![](img/00023.jpeg)), as described previously.  At some time step ![](img/00024.jpeg),
    what the agent knows so far is the sequence of ![](img/00025.jpeg), ![](img/00026.jpeg),
    and ![](img/00027.jpeg) that it observed until time step ![](img/00028.jpeg). It
    intuitively makes sense to call this the history:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 随着代理与环境的互动，这一过程会产生一系列的观察（![](img/00021.jpeg)）、动作（![](img/00022.jpeg)）和奖励（![](img/00023.jpeg)），正如之前所描述的那样。在某一时间步（![](img/00024.jpeg)），代理到目前为止知道的是它在时间步（![](img/00028.jpeg)）之前观察到的一系列（![](img/00025.jpeg)）、（![](img/00026.jpeg)）和（![](img/00027.jpeg)）。直观地看，这可以被称为历史：
- en: '![](img/00029.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00029.jpeg)'
- en: 'What happens next at time step  ![](img/00030.jpeg) depends on the history.
    Formally, the information used to determine what happens next is called the *state**. ***Because
    it depends on the history up until that time step, it can be denoted as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间步（![](img/00030.jpeg)）时，接下来发生的事情取决于历史。正式来说，用来决定接下来发生什么的信息被称为*状态*。***因为它依赖于直到该时间步为止的历史，所以可以表示如下：
- en: '![](img/00031.jpeg),'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/00031.jpeg),'
- en: Here, ![](img/00032.jpeg) denotes some function.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/00032.jpeg)表示某个函数。
- en: 'There is one subtle piece of information that is important for you to understand
    before we proceed. Let''s have another look at the general representation of a
    reinforcement learning system:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，有一个细节非常重要，你需要理解。我们再来看看强化学习系统的一般表示形式：
- en: '![](img/00033.jpeg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00033.jpeg)'
- en: Now, you will notice that the two main entities in the system, the agent and
    the environment, each has its own representation of the state. The *environment
    state*, sometimes denoted by ![](img/00034.jpeg), is the environment's own (private)
    representation, which the environment uses to pick the next observation and reward.
    This state is not usually visible/available to the agent. Likewise, the agent
    has its own internal representation of the state, sometimes denoted by ![](img/00035.jpeg),
    which is the information used by the agent to base its actions on. Because this
    representation is internal to the agent, it is up to the agent to use any function
    to represent it. Typically, it is some function based on the historythat the agent
    has observed so far. On a related note, a *Markov state*is a representation of
    the state using all the useful information from the history. By definition, using
    the Markov property, a state ![](img/00036.jpeg) is Markov or Markovian if, and
    only if, ![](img/00037.jpeg), which means that *the future is independent of the
    past given the present*. In other words, such a state is a sufficient statistic
    of the future. Once the state is known, the history can be thrown away. Usually,
    the environment state, ![](img/00038.jpeg), and the history, ![](img/00039.jpeg),
    satisfy the Markov property.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你会注意到系统中的两个主要实体——智能体和环境——各自都有自己的状态表示。*环境状态*，有时用 ![](img/00034.jpeg) 表示，是环境自身的（私有）表示，环境用它来选择下一个观察结果和奖励。这个状态通常对智能体不可见/不可用。同样，智能体也有自己对状态的内部表示，有时用
    ![](img/00035.jpeg) 表示，这是智能体用来基于其动作的内部信息。因为这个表示是智能体内部的，所以由智能体决定使用任何函数来表示它。通常，它是基于智能体迄今为止观察到的历史的某种函数。顺便提一下，*马尔可夫状态*是使用来自历史的所有有用信息来表示状态的方式。根据定义，使用马尔可夫性质，状态
    ![](img/00036.jpeg) 是马尔可夫状态或马尔可夫过程，当且仅当 ![](img/00037.jpeg)，这意味着*给定当前状态，未来与过去无关*。换句话说，这样的状态是未来的充分统计量。一旦状态已知，历史信息可以被丢弃。通常，环境状态
    ![](img/00038.jpeg) 和历史 ![](img/00039.jpeg) 满足马尔可夫性质。
- en: In some cases, the environment may make its internal environmental state directly
    visible to the agent. Such environments are called *fully observable environments. *In
    cases where the agent cannot directly observe the environment state, the agent
    must construct its own state representation from what it observes. Such environments
    are called *partially observable environments.* For example, an agent playing
    poker can only observe the public cards and not the cards the other players possess.
    Therefore, it is a partially observed environment. Similarly, an autonomous car
    with just a camera does not know its absolute location in its environment, which
    makes the environment only partially observable.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，环境可能会直接向智能体展示其内部的环境状态。这种环境称为*完全可观察环境*。在智能体无法直接观察环境状态的情况下，智能体必须根据自己所观察到的信息构建自己的状态表示。这种环境称为*部分可观察环境*。例如，一个玩扑克的智能体只能观察到公共牌，而无法知道其他玩家手中的牌。因此，它是一个部分可观察环境。同样，只有摄像头的自动驾驶汽车无法知道其在环境中的绝对位置，这使得该环境仅为部分可观察。
- en: In the next sections, we will learn about some of the key components of an agent.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习智能体的一些关键组成部分。
- en: Model
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: 'A model is an agent''s representation of the environment. It is similar to
    the mental models we have about people and things around us. An agent uses its
    model of the environment to predict what will happen next. There are two key pieces
    to it:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是智能体对环境的表示。它类似于我们对周围人和事物的心理模型。智能体利用其对环境的模型来预测接下来会发生什么。它有两个关键部分：
- en: '![](img/00040.jpeg): The state transition model/probability'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/00040.jpeg)：状态转移模型/概率'
- en: '![](img/00041.jpeg): The reward model'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/00041.jpeg)：奖励模型'
- en: 'The state transition model ![](img/00042.jpeg) is a probability distribution
    or a function that predicts the probability of ending up in a state ![](img/00043.jpeg) in
    the next time step ![](img/00044.jpeg) given the state ![](img/00045.jpeg) and
    the action ![](img/00046.jpeg) at time step ![](img/00047.jpeg). Mathematically,
    it is expressed as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 状态转移模型 ![](img/00042.jpeg) 是一个概率分布或函数，用来预测在下一个时间步骤 ![](img/00044.jpeg) 中，给定状态
    ![](img/00045.jpeg) 和动作 ![](img/00046.jpeg) 在时间步骤 ![](img/00047.jpeg) 后进入状态 ![](img/00043.jpeg)
    的概率。数学表达式如下：
- en: '![](img/00048.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00048.jpeg)'
- en: 'The agent uses the reward model ![](img/00049.jpeg) to predict the immediate
    next reward that it would get if it were to take action ![](img/00050.jpeg) while
    in state ![](img/00051.jpeg) at time step ![](img/00052.jpeg).  This expectation
    of the reward at the next time step ![](img/00053.jpeg) can be mathematically
    expressed as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体使用奖励模型 ![](img/00049.jpeg) 来预测如果它在时间步 ![](img/00052.jpeg) 处于状态 ![](img/00051.jpeg) 并采取行动 ![](img/00050.jpeg)，它将获得的即时奖励 ![](img/00053.jpeg)。这种对下一个时间步奖励的期望可以通过以下数学公式表示：
- en: '![](img/00054.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00054.jpeg)'
- en: Value function
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值函数
- en: 'A value function represents the agent''s prediction of future rewards. There
    are two types of value function: state-value function and action-value function.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 值函数表示智能体对未来奖励的预测。值函数有两种类型：状态值函数和动作值函数。
- en: State-value function
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 状态值函数
- en: 'A state-value function is a function that represents the agent''s estimate
    of how good it is to be in a state ![](img/00055.jpeg) at time step *t*. It is
    denoted by ![](img/00056.jpeg) and is usually just called the *value function*.
    It represents the agent''s prediction of the future reward it would get if it
    were to end up in state ![](img/00057.jpeg) at time step *t*. Mathematically,
    it can be represented as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 状态值函数是一个表示智能体在时间步 *t* 处于状态 ![](img/00055.jpeg) 时的估计值的函数。它用 ![](img/00056.jpeg) 表示，通常简称为
    *值函数*。它表示智能体对未来奖励的预测，即如果它最终在时间步 *t* 处于状态 ![](img/00057.jpeg)，将会获得的奖励。数学上，它可以表示为：
- en: '![](img/00058.jpeg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00058.jpeg)'
- en: What this expression means is that the value of state ![](img/00059.jpeg) under
    policy ![](img/00060.jpeg) is the expected sum of the discounted future rewards,
    where ![](img/00061.jpeg) is the discount factor and is a real number in the range
    [0,1]. Practically, the discount factor is typically set to be in the range of
    [0.95,0.99]. The other new term is ![](img/00062.jpeg), which is the policy of
    the agent.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表达式的意思是，策略 ![](img/00060.jpeg) 下状态 ![](img/00059.jpeg) 的值是未来奖励的折扣总和的期望，其中 ![](img/00061.jpeg) 是折扣因子，是一个位于
    [0,1] 范围内的实数。在实际中，折扣因子通常设置在 [0.95,0.99] 的范围内。另一个新术语是 ![](img/00062.jpeg)，它是智能体的策略。
- en: Action-value function
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动作值函数
- en: 'The action-value function is a function that represents the agent''s estimate
    of how good it is to take action ![](img/00063.jpeg) in state ![](img/00064.jpeg).
    It is denoted by ![](img/00065.jpeg). It is related to the state-value function
    by the following equation:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 动作值函数是一个表示智能体在状态 ![](img/00064.jpeg) 下采取动作 ![](img/00063.jpeg) 的估计值的函数。它用 ![](img/00065.jpeg) 表示。它与状态值函数通过以下方程式相关：
- en: '![](img/00066.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00066.jpeg)'
- en: Policy
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略
- en: 'The policy denoted by ![](img/00067.jpeg) prescribes what action is to be taken
    given the state. It can be seen as a function that maps states to actions. There
    are two major types of policy: deterministic policies and stochastic policies.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 策略用 ![](img/00067.jpeg) 表示，规定了给定状态下应该采取的动作。它可以看作是一个将状态映射到动作的函数。策略主要有两种类型：确定性策略和随机策略。
- en: A deterministic policy prescribes one action for a given state, that is, there
    is only one action, ![](img/00068.jpeg), given *s*. Mathematically, it means ![](img/00069.jpeg).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一个确定性策略为给定状态规定一个动作，也就是说，给定状态 *s* 只有一个动作 ![](img/00068.jpeg)。数学上，它表示为 ![](img/00069.jpeg)。
- en: A stochastic policy prescribes an action distribution given a state ![](img/00070.jpeg) at
    time step ![](img/00071.jpeg), that is, there are multiple actions with a probability
    value for each action. Mathematically, it means ![](img/00072.jpeg).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一个随机策略在给定状态 ![](img/00070.jpeg) 和时间步 ![](img/00071.jpeg) 时，规定了一种动作分布，即每个动作都有一个概率值。数学上，它表示为 ![](img/00072.jpeg)。
- en: Agents following different policies may exhibit different behaviors in the same
    environment.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循不同策略的智能体可能在相同环境下表现出不同的行为。
- en: Markov Decision Process
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程
- en: 'A **Markov Decision Process **(**MDP**) provides a formal framework for reinforcement
    learning. It is used to describe a fully observable environment where the outcomes
    are partly random and partly dependent on the actions taken by the agent or the
    decision maker. The following diagram is the progression of a Markov Process into
    a Markov Decision Process through the Markov Reward Process:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程**（**MDP**）为强化学习提供了一个正式框架。它用于描述一个完全可观察的环境，其中结果部分是随机的，部分依赖于智能体或决策者所采取的行动。以下图示展示了一个马尔可夫过程如何通过马尔可夫奖励过程演变为马尔可夫决策过程：'
- en: '![](img/00073.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00073.jpeg)'
- en: 'These stages can be described as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些阶段可以描述如下：
- en: A **Markov Process** (or a *markov chain) *is a sequence of random states s1,
    s2,...  that obeys the *Markov property.*In simple terms, it is a random process
    without any memory about its history.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**马尔可夫过程**（或 *马尔可夫链*）是一个随机状态序列 s1, s2,...，它遵循 *马尔可夫性质*。简单来说，它是一个没有关于历史的记忆的随机过程。'
- en: A **Markov Reward Process** (**MRP**) is a *Markov Process (*also called a M*arkov
    chain) *with values.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**马尔可夫奖励过程**（**MRP**）是一个 *马尔可夫过程*（也叫 *马尔可夫链*）带有值。'
- en: A **Markov Decision Process** is a *Markov Reward Process* with decisions.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程** 是一个 *马尔可夫奖励过程*，并且带有决策。'
- en: Planning with dynamic programming
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用动态规划进行规划
- en: Dynamic programming is a very general method to efficiently solve problems that
    can be decomposed into overlapping sub-problems. If you have used any type of
    recursive function in your code, you might have already got some preliminary flavor
    of dynamic programming. Dynamic programming, in simple terms, tries to cache or
    store the results of sub-problems so that they can be used later if required,
    instead of computing the results again.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划是一种非常通用的方法，用于高效地解决可以分解为重叠子问题的问题。如果你在代码中使用过任何类型的递归函数，你可能已经有了一些动态规划的初步体验。简单来说，动态规划尝试缓存或存储子问题的结果，以便在需要时可以重复使用，而不是重新计算结果。
- en: 'Okay, so how is that relevant here, you may ask. Well, they are pretty useful
    for solving a fully defined MDP, which means that an agent can find the most optimal
    way to act in an environment to achieve the highest reward using dynamic programming
    if it has full knowledge of the MDP! In the following table, you will find a concise
    summary of what the inputs and outputs are when we are interested in sequential
    prediction or control:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，你可能会问，这与这里有什么关系呢？嗯，它们对于解决一个完全定义的 MDP 非常有用，这意味着如果代理完全了解 MDP，它可以使用动态规划找到在环境中采取最优行动的方式，从而获得最高奖励！在下表中，你会看到一个简明的总结，列出了在我们关注顺序预测或控制时的输入和输出：
- en: '| Task/objective | Input | Output |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 任务/目标 | 输入 | 输出 |'
- en: '| Prediction | MDP or MRP and policy ![](img/00074.jpeg) | Value function ![](img/00075.jpeg)
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 预测 | MDP 或 MRP 和策略 ![](img/00074.jpeg) | 值函数 ![](img/00075.jpeg) |'
- en: '| Control | MDP | Optimal value function ![](img/00076.jpeg) and optimal policy ![](img/00077.jpeg)
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 控制 | MDP | 最优值函数 ![](img/00076.jpeg) 和最优策略 ![](img/00077.jpeg) |'
- en: Monte Carlo learning and temporal difference learning
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛学习和时序差分学习
- en: At this point, we understand that it is very useful for an agent to learn the
    state value function ![](img/00078.jpeg), which informs the agent about the long-term
    value of being in state ![](img/00079.jpeg) so that the agent can decide if it
    is a good state to be in or not. The **Monte Carlo** (**MC**) and **Temporal Difference**
    (**TD**) learning methods enable an agent to learn that!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们明白了学习状态值函数 ![](img/00078.jpeg) 对于代理来说非常有用，它可以告知代理处于状态 ![](img/00079.jpeg) 时的长期价值，从而帮助代理决定是否这个状态是值得处于的。**蒙特卡洛**（**MC**）和
    **时序差分**（**TD**）学习方法使得代理能够学到这一点！
- en: The goal of MC and TD learning is to learn the value functions from the agent's
    experience as the agent follows its policy ![](img/00080.jpeg).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: MC 和 TD 学习的目标是从代理的经验中学习值函数，代理遵循其策略 ![](img/00080.jpeg)。
- en: 'The following table summarizes the value estimate''s update equation for the
    MC and TD learning methods:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了 MC 和 TD 学习方法的值估计更新公式：
- en: '| **Learning method** | **State-value function** |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **学习方法** | **状态值函数** |'
- en: '| Monte Carlo | ![](img/00081.jpeg) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 蒙特卡洛 | ![](img/00081.jpeg) |'
- en: '| Temporal Difference | ![](img/00082.jpeg) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 时序差分 | ![](img/00082.jpeg) |'
- en: MC learning updates the value towards the **actual return** ![](img/00083.jpeg),
    which is the total discounted reward from time step *t*. This means that ![](img/00084.jpeg) until
    the end. It is important to note that we can calculate this value only after the
    end of the sequence, whereas TD learning (TD(0) to be precise), updates the value
    towards the *estimated return* given by ![](img/00085.jpeg), which can be calculated
    after every step.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: MC 学习方法将值更新到 **实际回报** ![](img/00083.jpeg)，这是从时间步 *t* 开始的总折现奖励。这意味着 ![](img/00084.jpeg) 直到结束。需要注意的是，我们只能在序列结束后计算该值，而时序差分学习（严格来说是
    TD(0)）会根据 ![](img/00085.jpeg) 给出的 *估计回报* 更新值，这个值可以在每一步后计算。
- en: SARSA and Q-learning
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SARSA 和 Q-learning
- en: 'It is also very useful for an agent to learn the action value function ![](img/00086.jpeg),
    which informs the agent about the long-term value of taking action ![](img/00087.jpeg) in
    state ![](img/00088.jpeg) so that the agent can take those actions that will maximize
    its expected, discounted future reward. The SARSA and Q-learning algorithms enable
    an agent to learn that! The following table summarizes the update equation for
    the SARSA algorithm and the Q-learning algorithm:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代理来说，学习行动价值函数也非常有用！[](img/00086.jpeg)，该函数告知代理在状态！[](img/00088.jpeg)下采取行动！[](img/00087.jpeg)的长期价值，从而帮助代理采取那些能够最大化其预期折扣未来奖励的行动。SARSA和Q学习算法使得代理能够学习到这一点！下表总结了SARSA算法和Q学习算法的更新公式：
- en: '| **Learning method** | **Action-value function** |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| **学习方法** | **行动价值函数** |'
- en: '| SARSA | ![](img/00089.jpeg) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| SARSA | ![](img/00089.jpeg) |'
- en: '| Q-learning | ![](img/00090.jpeg) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Q学习 | ![](img/00090.jpeg) |'
- en: 'SARSA is so named because of the sequence State->Action->Reward->State''->Action''
    that the algorithm''s update step depends on. The description of the sequence
    goes like this: the agent, in state *S*, takes an action A and gets a reward R,
    and ends up in the next state S'', after which the agent decides to take an action
    A'' in the new state. Based on this experience, the agent can update its estimate
    of Q(S,A).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA之所以得名，是因为算法的更新步骤依赖于序列State->Action->Reward->State'->Action'。该序列的描述如下：代理在状态*S*下采取行动A并获得奖励R，最终进入下一个状态S'，然后代理决定在新状态下采取行动A'。基于这一经验，代理可以更新其对Q(S,A)的估计。
- en: Q-learning is a popular off-policy learning algorithm, and it is similar to
    SARSA, except for one thing. Instead of using the Q value estimate for the new
    state and the action that the agent took in that new state, it uses the Q value
    estimate that corresponds to the action that leads to the *maximum* obtainable
    Q value from that new state, S'.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是一个流行的离策略学习算法，它与SARSA相似，除了一个区别：它并不是使用新状态下代理所采取行动的Q值估计，而是使用一个对应于从该新状态S'出发能够获得的*最大*Q值的行动的Q值估计。
- en: Deep reinforcement learning
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: With a basic understanding of reinforcement learning, you are now in a better
    state (hopefully you are not in a strictly Markov state where you have forgotten
    the history/things you have learned so far) to understand the basics of the cool
    new suite of algorithms that have been rocking the field of AI in recent times.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在你对强化学习有了基本理解后，你现在应该处于一个更好的状态（希望你不是处于一个严格的马尔可夫状态，忘记了之前学过的历史/知识），以便理解这套近年来在人工智能领域引起轰动的新算法的基础知识。
- en: Deep reinforcement learning emerged naturally when people made advancements
    in the deep learning field and applied them to reinforcement learning. We learned
    about the state-value function, action-value function, and policy. Let's briefly
    look at how they can be represented mathematically or realized through computer
    code. The state-value function ![](img/00091.jpeg) is a real-value function that
    takes the current state ![](img/00092.jpeg) as the input and outputs a real-value
    number (such as 4.57). This number is the agent's prediction of how good it is
    to be in state ![](img/00093.jpeg) and the agent keeps updating the value function
    based on the new experiences it gains. Likewise, the action-value function ![](img/00094.jpeg)
    is also a real-value function, which takes action ![](img/00095.jpeg) as an input
    in addition to state ![](img/00096.jpeg), and outputs a real number. One way to
    represent these functions is using neural networks because neural networks are
    universal function approximators, which are capable of representing complex, non-linear
    functions. For an agent trying to play a game of Atari by just looking at the
    images on the screen (like we do), state ![](img/00097.jpeg) could be the pixel
    values of the image on the screen. In such cases, we could use a deep neural network
    with convolutional layers to extract the visual features from the state/image,
    and then a few fully connected layers to finally output ![](img/00098.jpeg)  or ![](img/00099.jpeg) ,
    depending on which function we want to approximate.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习自然地出现在人们在深度学习领域取得进展并将其应用于强化学习时。我们学习了状态值函数、动作值函数和策略。让我们简要看看它们是如何在数学上表示或者通过计算机代码实现的。状态值函数 ![](img/00091.jpeg) 是一个实值函数，它将当前状态 ![](img/00092.jpeg) 作为输入，并输出一个实值数字（例如4.57）。这个数字是智能体预测在状态 ![](img/00093.jpeg) 中所处的情况有多好，智能体会根据它获得的新经验不断更新该值函数。同样，动作值函数 ![](img/00094.jpeg) 也是一个实值函数，它除了状态 ![](img/00096.jpeg) 之外，还将动作 ![](img/00095.jpeg) 作为输入，输出一个实数。表示这些函数的一种方式是使用神经网络，因为神经网络是通用的函数逼近器，能够表示复杂的非线性函数。对于一个试图通过仅仅查看屏幕上的图像（就像我们做的那样）来玩
    Atari 游戏的智能体，状态 ![](img/00097.jpeg) 可能是屏幕上图像的像素值。在这种情况下，我们可以使用带有卷积层的深度神经网络从状态/图像中提取视觉特征，然后通过几个全连接层来最终输出 ![](img/00098.jpeg) 或者 ![](img/00099.jpeg)，具体取决于我们想要逼近哪个函数。
- en: Recall from the earlier sections of this chapter that ![](img/00100.jpeg) is
    the state-value function and provides an estimate of the value of being in state ![](img/00101.jpeg),
    and ![](img/00099.jpeg) is the action-value function, which provides an estimate
    of the value of each action given the  state.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆本章早些时候提到的， ![](img/00100.jpeg) 是状态值函数，它提供了处于状态 ![](img/00101.jpeg) 时的价值估计，而 ![](img/00099.jpeg) 是动作值函数，它提供了在给定状态下每个动作的价值估计。
- en: If we do this, then we are doing deep reinforcement learning! Easy enough to
    understand? I hope so. Let's look at some other ways in which we can use deep
    learning in reinforcement learning.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们这样做，那么我们就在进行深度强化学习！够简单理解吧？希望如此。让我们看看还有哪些方式可以将深度学习应用于强化学习。
- en: Recall that a policy is represented as ![](img/00102.jpeg) in the case of deterministic
    policies, and as ![](img/00103.jpeg) in the case of stochastic policies, where
    action ![](img/00104.jpeg) could be discrete (such as "move left," "move right,"
    or "move straight ahead") or continuous values (such as "0.05" for acceleration,
    "0.67" for steering, and so on), and they can be single or multi-dimensional.
    Therefore, a policy can be a complicated function at times! It might have to take
    in a multi-dimensional state (such as an image) as input and output a multi-dimensional
    vector of probabilities as output (in the case of stochastic policies). So, this
    does look like it will be a monster function, doesn't it? Yes it does. That's
    where deep neural networks come to the rescue! We could approximate an agent's
    policy using a deep neural network and directly learn to update the policy (by
    updating the parameters of the deep neural network). This is called policy optimization-based
    deep reinforcement learning and it has been shown to be quite efficient in solving
    several challenging control problems, especially in robotics.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请回想，在确定性策略的情况下，策略表示为![](img/00102.jpeg)，在随机策略的情况下，策略表示为![](img/00103.jpeg)，其中动作![](img/00104.jpeg)可以是离散的（例如“向左移动”，“向右移动”或“直行”）或连续的值（例如“0.05”表示加速，“0.67”表示转向，等等），并且这些值可以是单维的或多维的。因此，策略有时可能是一个复杂的函数！它可能需要接受多维状态（例如图像）作为输入，并输出一个多维的概率向量作为输出（在随机策略的情况下）。那么，这看起来像是一个巨大的函数，不是吗？是的，确实如此。正是在这里，深度神经网络派上了用场！我们可以通过深度神经网络来逼近智能体的策略，并直接学习如何更新策略（通过更新深度神经网络的参数）。这被称为基于策略优化的深度强化学习，并且已被证明在解决一些具有挑战性的控制问题时非常高效，尤其是在机器人学领域。
- en: So in summary, deep reinforcement learning is the application of deep learning
    to reinforcement learning and so far, researchers have applied deep learning to
    reinforcement learning successfully in two ways. One way is using deep neural
    networks to approximate the value functions, and the other way is to use a deep
    neural network to represent the policy.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，深度强化学习是将深度学习应用于强化学习，迄今为止，研究人员已经成功地通过两种方式将深度学习应用于强化学习。一种方式是使用深度神经网络来逼近价值函数，另一种方式是使用深度神经网络来表示策略。
- en: These ideas have been known from the early days, when researchers were trying
    to use neural networks as value function approximators, even back in 2005\. But
    it rose to stardom only recently because although neural networks or other non-linear
    value function approximators can better represent the complex values of environment
    states and actions, they were prone to instability and often led to sub-optimal
    functions. Only recently have researchers such as Volodymyr Mnih and his colleagues
    at DeepMind (now part of Google) figured out the trick of stabilizing the learning
    and trained agents with deep, non-linear function approximators that converged
    to near-optimal value functions. In the later chapters of this book, we will in
    fact reproduce some of their then-groundbreaking results, which surpassed human
    Atari game playing capabilities!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些想法从早期就已被人们知晓，当时研究人员试图使用神经网络作为价值函数的逼近器，甚至早在2005年。但直到最近，这些想法才崭露头角，因为尽管神经网络或其他非线性价值函数逼近器能更好地表示环境状态和动作的复杂值，它们却容易导致不稳定性，并且常常会导致次优函数。直到最近，像Volodymyr
    Mnih和他在DeepMind（现为Google的一部分）的同事们才找到了稳定学习的窍门，并用深度非线性函数逼近器训练智能体，使得其收敛到近优的价值函数。在本书的后续章节中，我们实际上将重现他们当时开创性的成果，这些成果甚至超越了人类在Atari游戏中的表现！
- en: Practical applications of reinforcement and deep reinforcement learning algorithms
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习和深度强化学习算法的实际应用
- en: 'Until recently, practical applications of reinforcement learning and deep reinforcement
    learning were limited, due to sample complexity and instability. But, these algorithms
    proved to be quite powerful in solving some really hard practical problems. Some
    of them are listed here to give you an idea:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 直到最近，由于样本复杂性和不稳定性，强化学习和深度强化学习的实际应用一直受到限制。但这些算法在解决一些真正困难的实际问题上，已被证明非常强大。以下列出了一些应用实例，帮助你了解这些算法：
- en: '**Learning to play video games better than humans**: This news has probably
    reached you by now. Researchers at DeepMind and others developed a series of algorithms,
    starting with DeepMind''s Deep-Q-Network, or DQN for short, which reached human-level
    performance in playing Atari games. We will actually be implementing this algorithm
    in a later chapter of this book! In essence, it is a deep variant of the Q-learning
    algorithm we briefly saw in this chapter, with a few changes that increased the
    speed of learning and the stability. It was able to reach human-level performance
    in terms of game scores after several games. What is more impressive is that the
    same algorithm achieved this level of play without any game-specific fine-tuning
    or changes!'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习比人类更好地玩视频游戏：** 这一消息可能已经传到你耳中了。DeepMind和其他研究人员开发了一系列算法，从DeepMind的深度Q网络（简称DQN）开始，该算法在玩Atari游戏时达到了人类水平的表现。我们将在本书的后续章节中实现这一算法！本质上，它是Q-learning算法的一个深度变种，我们在本章中简要提到过，经过一些修改，使学习速度更快，稳定性更强。它能够在进行几局游戏后达到人类水平的游戏得分。更令人印象深刻的是，同一个算法在没有进行任何特定游戏的微调或修改的情况下，达到了这一水平的表现！'
- en: '**Mastering the game of Go**: Go is a Chinese game that has challenged AI for
    several decades. It is played on a full-size 19 x 19 board and is orders of magnitude
    more complex than chess because of the large number (![](img/00105.jpeg)) of possible
    board positions. Until recently, no AI algorithm or software was able to play
    anywhere close to the level of humans at this game. AlphaGo—the AI agent from
    DeepMind that uses deep reinforcement learning and Monte Carlo tree search—changed
    this all and beat the human world champions Lee Sedol (4-1) and Fan Hui (5-0).
    DeepMind released more advanced versions of their AI agent, named AlphaGO Zero
    (which uses zero human knowledge and learned to play all by itself!) and AlphaZero
    (which could play the games of Go, chess, and Shogi!), all of which used deep
    reinforcement learning as the core algorithm.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**掌握围棋游戏：** 围棋是一种中国游戏，几十年来一直是人工智能的挑战。围棋在19 x 19的完整棋盘上进行，比国际象棋更为复杂，因为可能的棋盘位置数量庞大（见图）。直到最近，尚没有任何人工智能算法或软件能够在这一游戏中接近人类的水平。AlphaGo—DeepMind开发的使用深度强化学习和蒙特卡洛树搜索的AI代理—改变了这一切，战胜了人类世界冠军李世石（4-1）和范晖（5-0）。DeepMind发布了更先进版本的AI代理，命名为AlphaGo
    Zero（该版本不依赖任何人类知识，完全自学会下围棋！）和AlphaZero（能够玩围棋、国际象棋和将棋！），它们都以深度强化学习为核心算法。'
- en: '**Helping AI win Jeopardy!**: IBM''s Watson—an AI system developed by IBM,
    which came to fame by beating humans at Jeopardy!—used an extension of TD learning
    to create its *daily-double wagering* strategies that helped it to win against
    human champions.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**帮助人工智能赢得《Jeopardy!》游戏：** IBM的Watson—由IBM开发的人工智能系统，以战胜人类选手而成名—利用TD学习的扩展，制定了*每日双倍下注*策略，帮助其战胜了人类冠军。'
- en: '**Robot locomotion and manipulation:** Both reinforcement learning and deep
    reinforcement learning have enabled the control of complex robots, both for locomotion
    and navigation. Several recent works from the researchers at UC Berkeley have
    shown how, using deep reinforcement, they train policies that offer vision and
    control for robotic manipulation tasks and generate join actuations for making
    a complex bipedal humanoid walk and run.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器人运动和操作：** 强化学习和深度强化学习都使得控制复杂的机器人成为可能，涵盖了运动和导航功能。来自UC伯克利大学的几项最新研究展示了如何通过深度强化学习训练策略，提供机器人操作任务的视觉与控制，并生成关节驱动，使复杂的双足人形机器人能够行走和奔跑。'
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how an agent interacts with an environment by
    taking an action based on the observation it receives from the environment, and
    the environment responds to the agent's action with an (optional) reward and the
    next observation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了智能体如何通过根据从环境中获得的观察信息采取行动与环境互动，环境则通过（可选的）奖励和下一个观察来响应智能体的行动。
- en: With a concise understanding of the foundations of reinforcement learning, we
    went deeper to understand what deep reinforcement learning is, and uncovered the
    fact that we could use deep neural networks to represent value functions and policies.
    Although this chapter was a little heavy on notation and definitions, hopefully
    it laid a strong foundation for us to develop some cool agents in the upcoming
    chapters. In the next chapter, we will consolidate our learning in the first two
    chapters and put it to use by laying out the groundwork to train an agent to solve
    some interesting problems.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在简要了解强化学习的基础知识后，我们深入探讨了深度强化学习的概念，并揭示了我们可以使用深度神经网络来表示价值函数和策略的事实。尽管这一章在符号和定义上稍显繁琐，但希望它为我们在接下来的章节中开发一些有趣的智能体奠定了坚实的基础。在下一章中，我们将巩固前两章的学习，并通过为训练一个智能体来解决一些有趣的问题打下基础。
