- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Case Study – The MAB Problem
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究——MAB问题
- en: So far in the previous chapters, we have learned the fundamental concepts of
    reinforcement learning and also several interesting reinforcement learning algorithms.
    We learned about a model-based method called dynamic programming and a model-free
    method called the Monte Carlo method, and then we learned about the temporal difference
    method, which combines the advantages of dynamic programming and the Monte Carlo
    method.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经学习了强化学习的基本概念，并且了解了几种有趣的强化学习算法。我们学习了基于模型的方法——动态规划，以及无模型的方法——蒙特卡洛方法，随后我们学习了时间差分方法，它结合了动态规划和蒙特卡洛方法的优点。
- en: In this chapter, we will learn about one of the classic problems in reinforcement
    learning called the **multi-armed bandit** (**MAB**) problem. We start the chapter
    by understanding the MABproblem, and then we will learn about several exploration
    strategies, called epsilon-greedy, softmax exploration, upper confidence bound,
    and Thompson sampling, for solving the MAB problem. Following this, we will learn
    how a MAB is useful in real-world use cases.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习强化学习中的经典问题之一——**多臂土匪**（**MAB**）问题。我们首先理解MAB问题，然后学习几种探索策略，包括epsilon-greedy、softmax探索、上置信界和汤普森采样，来解决MAB问题。接着，我们将学习MAB在实际用例中的应用。
- en: Moving forward, we will understand how to find the best advertisement banner
    that is clicked on most frequently by users by framing it as a MAB problem. At
    the end of the chapter, we will learn about contextual bandits and how they are
    used in different use cases.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解如何通过将其构造为MAB问题来找到用户最常点击的最佳广告横幅。在本章结束时，我们将学习上下文土匪及其在不同用例中的应用。
- en: 'In this chapter, we will learn about the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将学习以下内容：
- en: The MAB problem
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAB问题
- en: The epsilon-greedy method
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Epsilon-greedy方法
- en: Softmax exploration
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax探索
- en: The upper confidence bound algorithm
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上置信界算法
- en: The Thompson sampling algorithm
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汤普森采样算法
- en: Applications of MAB
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAB的应用
- en: Finding the best advertisement banner using MAB
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MAB找到最佳广告横幅
- en: Contextual bandits
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文土匪
- en: The MAB problem
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MAB问题
- en: The MAB problem is one of the classic problems in reinforcement learning. A
    MAB is a slot machine where we pull the arm (lever) and get a payout (reward)
    based on some probability distribution. A single slot machine is called a one-armed
    bandit and when there are multiple slot machines it is called a MAB or *k*-armed
    bandit, where *k* denotes the number of slot machines.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MAB问题是强化学习中的经典问题之一。MAB是一种老虎机，我们拉动臂（杠杆），根据某种概率分布获得支付（奖励）。单个老虎机叫做单臂土匪（one-armed
    bandit），当有多个老虎机时，称为MAB或*k*臂土匪，其中*k*表示老虎机的数量。
- en: '*Figure 6.1* shows a 3-armed bandit:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.1*展示了一个三臂土匪：'
- en: '![](img/B15558_06_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_01.png)'
- en: 'Figure 6.1: 3-armed bandit slot machines'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：三臂土匪老虎机
- en: Slot machines are one of the most popular games in the casino, where we pull
    the arm and get a reward. If we get 0 reward then we lose the game, and if we
    get +1 reward then we win the game. There can be several slot machines, and each
    slot machine is referred to as an arm. For instance, slot machine 1 is referred
    to as arm 1, slot machine 2 is referred to as arm 2, and so on. Thus, whenever
    we say arm *n*, it actually means that we are referring to slot machine *n*.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 老虎机是赌场中最受欢迎的游戏之一，我们拉动臂并获得奖励。如果获得0奖励，则输了游戏；如果获得+1奖励，则赢得游戏。可以有多个老虎机，每个老虎机称为一个臂。例如，老虎机1称为臂1，老虎机2称为臂2，以此类推。因此，每当我们说臂*n*时，实际上是指老虎机*n*。
- en: Each arm has its own probability distribution indicating the probability of
    winning and losing the game. For example, let's suppose we have two arms. Let
    the probability of winning if we pull arm 1 (slot machine 1) be 0.7 and the probability
    of winning if we pull arm 2 (slot machine 2) be 0.5\.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每个臂都有自己的概率分布，表示赢得游戏和输掉游戏的概率。例如，假设我们有两个臂。如果拉动臂1（老虎机1）时，赢的概率为0.7，拉动臂2（老虎机2）时，赢的概率为0.5。
- en: Then, if we pull arm 1, 70% of the time we win the game and get the +1 reward,
    and if we pull arm 2, then 50% of the time we win the game and get the +1 reward.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果我们拉动臂1，70%的时间我们会赢得游戏并获得+1奖励；如果拉动臂2，50%的时间我们会赢得游戏并获得+1奖励。
- en: Thus, we can say that pulling arm 1 is desirable as it makes us win the game
    70% of the time. However, this probability distribution of the arm (slot machine)
    will not be given to us. We need to find out which arm helps us to win the game
    most of the time and gives us a good reward.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说，拉臂 1 是值得的，因为它让我们在 70% 的情况下赢得了游戏。然而，这种臂的概率分布（老虎机）不会直接给我们。我们需要找出哪个臂最能帮助我们赢得游戏并提供良好的奖励。
- en: Okay, how can we find this?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们如何找到这个臂呢？
- en: Say we pulled arm 1 once and received a +1 reward, and we pulled arm 2 once
    and received a 0 reward. Since arm 1 gives a +1 reward, we cannot come to the
    conclusion that arm 1 is the best arm immediately after pulling it only once.
    We need to pull both of the arms many times and compute the average reward we
    obtain from each of the arms, and then we can select the arm that gives the maximum
    average reward as the best arm.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们拉臂 1 一次并获得 +1 奖励，拉臂 2 一次并获得 0 奖励。由于臂 1 给出了 +1 奖励，我们不能仅仅通过拉一次臂 1 就得出它是最佳臂的结论。我们需要多次拉这两个臂，并计算每个臂的平均奖励，然后选择给予最大平均奖励的臂作为最佳臂。
- en: 'Let''s denote the arm by *a* and define the average reward by pulling the arm
    *a* as:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 设臂为 *a*，并定义拉臂 *a* 得到的平均奖励为：
- en: '![](img/B15558_06_001.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_001.png)'
- en: Where *Q*(*a*) denotes the average reward of arm *a*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q*(*a*) 表示臂 *a* 的平均奖励。'
- en: 'The optimal arm *a** is the one that gives us the maximum average reward, that
    is:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最优臂 *a* 是给我们带来最大平均奖励的臂，即：
- en: '![](img/B15558_06_002.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_002.png)'
- en: Okay, we have learned that the arm that gives the maximum average reward is
    the optimal arm. But how can we find this?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经了解到，给出最大平均奖励的臂是最优臂。那么我们如何找到这个臂呢？
- en: We play the game for several rounds and we can pull only one arm in each round.
    Say in the first round we pull arm 1 and observe the reward, and in the second
    round we pull arm 2 and observe the reward. Similarly, in every round, we keep
    pulling arm 1 or arm 2 and observe the reward. After completing several rounds
    of the game, we compute the average reward of each of the arms, and then we select
    the arm that has the maximum average reward as the best arm.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行多轮游戏，每一轮只能拉一个臂。假设在第一轮我们拉臂 1 并观察奖励，在第二轮我们拉臂 2 并观察奖励。类似地，在每一轮中，我们继续拉臂 1 或臂
    2 并观察奖励。在完成几轮游戏后，我们计算每个臂的平均奖励，然后选择具有最大平均奖励的臂作为最佳臂。
- en: But this is not a good approach to find the best arm. Say we have 20 arms; if
    we keep pulling a different arm in each round, then in most of the rounds we will
    lose the game and get a 0 reward. Along with finding the best arm, our goal should
    be to minimize the cost of identifying the best arm, and this is usually referred
    to as regret.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但这种方法并不是找到最佳臂的好办法。假设我们有 20 个臂；如果我们每一轮都拉一个不同的臂，那么在大多数回合中我们会输掉游戏并获得 0 奖励。除了找到最佳臂之外，我们的目标还应该是最小化识别最佳臂的成本，这通常被称为遗憾。
- en: Thus, we need to find the best arm while minimizing regret. That is, we need
    to find the best arm, but we don't want to end up selecting the arms that make
    us lose the game in most of the rounds.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要在最小化遗憾的同时找到最佳臂。也就是说，我们需要找到最佳臂，但我们不希望最终选择那些让我们在大多数回合中输掉游戏的臂。
- en: So, should we explore a different arm in each round, or should we select only
    the arm that got us a good reward in the previous rounds? This leads to a situation
    called the exploration-exploitation dilemma, which we learned about in *Chapter
    4*, *Monte Carlo Methods*. So, to resolve this, we use the epsilon-greedy method
    and select the arm that got us a good reward in the previous rounds with probability
    1-epsilon and select the random arm with probability epsilon. After completing
    several rounds, we select the best arm as the one that has the maximum average
    reward.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们是应该每一轮探索一个不同的臂，还是只选择在之前回合中获得良好奖励的臂呢？这就引出了探索-利用困境（exploration-exploitation
    dilemma），我们在*第4章*《蒙特卡罗方法》中学习过。为了应对这个问题，我们使用 epsilon-greedy 方法，以概率 1-epsilon 选择之前获得良好奖励的臂，并以概率
    epsilon 随机选择一个臂。经过若干轮后，我们选择具有最大平均奖励的臂作为最佳臂。
- en: Similar to the epsilon-greedy method, there are several different exploration
    strategies that help us to overcome the exploration-exploitation dilemma. In the
    upcoming section, we will learn more about several different exploration strategies
    in detail and how they help us to find the optimal arm, but first let's look at
    creating a bandit.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 epsilon-贪婪算法，还有几种不同的探索策略帮助我们克服探索与利用的困境。在接下来的章节中，我们将详细学习几种不同的探索策略，并了解它们如何帮助我们找到最优臂，但首先，让我们来看看如何创建一个
    bandit。
- en: Creating a bandit in the Gym
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Gym 中创建一个 bandit
- en: Before going ahead, let's learn how to create a bandit environment with the
    Gym toolkit. The Gym does not come with a prepackaged bandit environment. So,
    we need to create a bandit environment and integrate it with the Gym. Instead
    of creating the bandit environment from scratch, we will use the open-source version
    of the bandit environment provided by Jesse Cooper.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们学习如何使用 Gym 工具包创建一个 bandit 环境。Gym 并没有预先包装好的 bandit 环境，因此我们需要创建一个 bandit
    环境并将其与 Gym 集成。我们将使用 Jesse Cooper 提供的开源版本 bandit 环境，而不是从零开始创建 bandit 环境。
- en: 'First, let''s clone the Gym bandits repository:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们克隆 Gym bandits 仓库：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we can install it using `pip`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用 `pip` 安装它：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After installation, we import `gym_bandits` and also the `gym` library:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，我们导入 `gym_bandits` 和 `gym` 库：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`gym_bandits` provides several versions of the bandit environment. We can examine
    the different bandit versions at [https://github.com/JKCooper2/gym-bandits](https://github.com/JKCooper2/gym-bandits).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`gym_bandits` 提供了多个版本的 bandit 环境。我们可以在 [https://github.com/JKCooper2/gym-bandits](https://github.com/JKCooper2/gym-bandits)
    上查看不同的 bandit 版本。'
- en: 'Let''s just create a simple 2-armed bandit whose environment ID is `BanditTwoArmedHighLowFixed-v0`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个简单的 2 臂 bandit，其环境 ID 为 `BanditTwoArmedHighLowFixed-v0`：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Since we created a 2-armed bandit, our action space will be `2` (as there are
    two arms), as shown here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们创建了一个双臂 bandit，所以我们的行动空间将是 `2`（因为有两个臂），如下所示：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code will print:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将输出：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can also check the probability distribution of the arm with:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过以下方式检查臂的概率分布：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding code will print:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将输出：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It indicates that, with arm 1, we win the game 80% of the time and with arm
    2, we win the game 20% of the time. Our goal is to find out whether pulling arm
    1 or arm 2 makes us win the game most of the time.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 它表示使用臂 1 时，我们有 80% 的概率赢得游戏，而使用臂 2 时，我们有 20% 的概率赢得游戏。我们的目标是找出拉臂 1 还是臂 2 更能让我们在大多数情况下赢得游戏。
- en: Now that we have learned how to create bandit environments in the Gym, in the
    next section, we will explore different exploration strategies to solve the MAB
    problem and we will implement them with the Gym.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何在 Gym 中创建 bandit 环境，接下来的章节中，我们将探索不同的探索策略来解决 MAB 问题，并将它们与 Gym 一起实现。
- en: Exploration strategies
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索策略
- en: 'At the beginning of the chapter, we learned about the exploration-exploitation
    dilemma in the MAB problem. To overcome this, we use different exploration strategies
    and find the best arm. The different exploration strategies are listed here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，我们学习了 MAB 问题中的探索与利用困境。为了克服这个困境，我们使用不同的探索策略来找到最优臂。这里列出了不同的探索策略：
- en: Epsilon-greedy
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Epsilon-贪婪算法
- en: Softmax exploration
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax 探索
- en: Upper confidence bound
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上置信界限
- en: Thomson sampling
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thomson 采样
- en: Now, we will explore all of these exploration strategies in detail and implement
    them to find the best arm.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将详细探讨所有这些探索策略，并实现它们来找到最优臂。
- en: Epsilon-greedy
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Epsilon-贪婪算法
- en: We learned about the epsilon-greedy algorithm in the previous chapters. With
    epsilon-greedy, we select the best arm with a probability 1-epsilon and we select
    a random arm with a probability epsilon. Let's take a simple example and learn
    how to find the best arm with the epsilon-greedy method in more detail.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中学习了 epsilon-贪婪算法。使用 epsilon-贪婪时，我们以概率 1-epsilon 选择最优臂，以概率 epsilon 随机选择一个臂。让我们通过一个简单的例子，详细学习如何通过
    epsilon-贪婪方法找到最优臂。
- en: Say we have two arms—arm 1 and arm 2\. Suppose with arm 1 we win the game 80% of
    the time and with arm 2 we win the game 20% of the time. So, we can say that arm
    1 is the best arm as it makes us win the game 80% of the time. Now, let's learn
    how to find this with the epsilon-greedy method.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个臂——臂 1 和臂 2。假设使用臂 1 时我们有 80% 的概率赢得游戏，而使用臂 2 时我们只有 20% 的概率赢得游戏。所以，我们可以说臂
    1 是最好的臂，因为它让我们赢得游戏的概率是 80%。现在，让我们学习如何通过 epsilon-贪婪方法来找出这个最优臂。
- en: 'First, we initialize the `count` (the number of times the arm is pulled), `sum_rewards`
    (the sum of rewards obtained from pulling the arm), and `Q` (the average reward
    obtained by pulling the arm), as *Table 6.1* shows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化 `count`（臂被拉动的次数）、`sum_rewards`（从拉动臂获得的奖励总和）和 `Q`（拉动臂获得的平均奖励），正如*表 6.1*所示：
- en: '![](img/B15558_06_02.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_02.png)'
- en: 'Table 6.1: Initialize the variables with zero'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1：将变量初始化为零
- en: '**Round 1**:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一轮**：'
- en: 'Say, in round 1 of the game, we select a random arm with a probability epsilon,
    and suppose we randomly pull arm 1 and observe the reward. Let the reward obtained
    by pulling arm 1 be 1\. So, we update our table with `count` of arm 1 set to 1,
    and `sum_rewards` of arm 1 set to 1, and thus the average reward `Q` of arm 1
    after round 1 is 1 as *Table 6.2* shows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，在游戏的第一轮中，我们以 epsilon 的概率选择一个随机的臂，假设我们随机拉动臂 1，并观察奖励。让拉动臂 1 所获得的奖励为 1\。因此，我们更新我们的表格，将臂
    1 的 `count` 设置为 1，臂 1 的 `sum_rewards` 设置为 1，因此，第一轮后，臂 1 的平均奖励 `Q` 为 1，正如*表 6.2*所示：
- en: '![](img/B15558_06_03.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_03.png)'
- en: 'Table 6.2: Results after round 1'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.2：第一轮后的结果
- en: '**Round 2**:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二轮**：'
- en: Say, in round 2, we select the best arm with a probability 1-epsilon. The best
    arm is the one that has the maximum average reward. So, we check our table to
    see which arm has the maximum average reward. Since arm 1 has the maximum average
    reward, we pull arm 1 and observe the reward and let the reward obtained from
    pulling arm 1 be 1\.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，在第二轮中，我们以 1-epsilon 的概率选择最好的臂。最好的臂是那个具有最大平均奖励的臂。所以，我们查看我们的表格，看看哪个臂具有最大的平均奖励。由于臂
    1 拥有最大平均奖励，我们拉动臂 1，观察奖励，并让拉动臂 1 所获得的奖励为 1\。
- en: 'So, we update our table with `count` of arm 1 to 2 and `sum_rewards` of arm
    1 to 2, and thus the average reward `Q` of arm 1 after round 2 is 1 as *Table
    6.3* shows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们更新我们的表格，将臂 1 的 `count` 设置为 2，臂 1 的 `sum_rewards` 设置为 2，因此，第二轮后，臂 1 的平均奖励
    `Q` 为 1，正如*表 6.3*所示：
- en: '![](img/B15558_06_04.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_04.png)'
- en: 'Table 6.3: Results after round 2'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.3：第二轮后的结果
- en: '**Round 3**:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**第三轮**：'
- en: 'Say, in round 3, we select a random arm with a probability epsilon. Suppose
    we randomly pull arm 2 and observe the reward. Let the reward obtained by pulling
    arm 2 be 0\. So, we update our table with `count` of arm 2 set to 1 and `sum_rewards`
    of arm 2 set to 0, and thus the average reward `Q` of arm 2 after round 3 is 0
    as *Table 6.4* shows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，在第三轮中，我们以 epsilon 的概率选择一个随机的臂。假设我们随机拉动臂 2，并观察奖励。让拉动臂 2 所获得的奖励为 0\。因此，我们更新我们的表格，将臂
    2 的 `count` 设置为 1，臂 2 的 `sum_rewards` 设置为 0，因此，第三轮后，臂 2 的平均奖励 `Q` 为 0，正如*表 6.4*所示：
- en: '![](img/B15558_06_05.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_05.png)'
- en: 'Table 6.4: Results after round 3'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.4：第三轮后的结果
- en: '**Round 4**:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**第四轮**：'
- en: 'Say, in round 4, we select the best arm with a probability 1-epsilon. So, we
    pull arm 1 since it has the maximum average reward. Let the reward obtained by
    pulling arm 1 be 0 this time. Now, we update our table with `count` of arm 1 to
    3 and `sum_rewards` of arm 2 to 2, and thus the average reward `Q` of arm 1 after
    round 4 will be 0.66 as *Table 6.5* shows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，在第四轮中，我们以 1-epsilon 的概率选择最好的臂。因此，我们拉动臂 1，因为它具有最大平均奖励。让拉动臂 1 所获得的奖励这次为 0。现在，我们更新我们的表格，将臂
    1 的 `count` 设置为 3，臂 2 的 `sum_rewards` 设置为 2，因此，第四轮后，臂 1 的平均奖励 `Q` 将是 0.66，正如*表
    6.5*所示：
- en: '![](img/B15558_06_06.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_06.png)'
- en: 'Table 6.5: Results after round 4'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.5：第四轮后的结果
- en: We repeat this process for several rounds; that is, for several rounds of the
    game, we pull the best arm with a probability 1-epsilon and we pull a random arm
    with probability epsilon.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为多个轮次重复这个过程；也就是说，在多个游戏轮次中，我们以 1-epsilon 的概率拉动最好的臂，并以 epsilon 的概率拉动一个随机的臂。
- en: '*Table 6.6* shows the updated table after 100 rounds of the game:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 6.6* 显示了游戏进行 100 轮后的更新表格：'
- en: '![](img/B15558_06_07.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_07.png)'
- en: 'Table 6.6: Results after 100 rounds'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.6：100 轮后的结果
- en: From *Table 6.6*, we can conclude that arm 1 is the best arm since it has the
    maximum average reward.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从*表 6.6*中，我们可以得出结论，臂 1 是最好的臂，因为它具有最大平均奖励。
- en: Implementing epsilon-greedy
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现 epsilon-greedy
- en: 'Now, let''s learn to implement the epsilon-greedy method to find the best arm.
    First, let''s import the necessary libraries:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习实现 epsilon-greedy 方法来找到最好的臂。首先，让我们导入必要的库：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For better understanding, let''s create the bandit with only two arms:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，我们只创建一个具有两个臂的赌博机：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s check the probability distribution of the arm:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下这个臂的概率分布：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The preceding code will print:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将输出：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can observe that with arm 1 we win the game with 80% probability and with
    arm 2 we win the game with 20% probability. Here, the best arm is arm 1, as with
    arm 1 we win the game with 80% probability. Now, let's see how to find this best
    arm using the epsilon-greedy method.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，使用臂 1 时，我们有 80% 的概率赢得游戏，使用臂 2 时，我们有 20% 的概率赢得游戏。这里，最佳臂是臂 1，因为使用臂 1 时，我们有
    80% 的概率获胜。现在，让我们看看如何使用 epsilon-greedy 方法找到这个最佳臂。
- en: First, let's initialize the variables.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们初始化变量。
- en: 'Initialize the `count` for storing the number of times an arm is pulled:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 `count` 以存储每个臂被拉取的次数：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Initialize `sum_rewards` for storing the sum of rewards of each arm:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`sum_rewards`以存储每个臂的奖励总和：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Initialize `Q` for storing the average reward of each arm:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`Q`以存储每个臂的平均奖励：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Set the number of rounds (iterations):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 设置回合数（迭代次数）：
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now, let's define the `epsilon_greedy` function.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义 `epsilon_greedy` 函数。
- en: 'First, we generate a random number from a uniform distribution. If the random
    number is less than epsilon, then we pull the random arm; else, we pull the best
    arm that has the maximum average reward, as shown here:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从均匀分布中生成一个随机数。如果随机数小于 epsilon，那么我们拉取一个随机臂；否则，我们拉取具有最大平均奖励的最佳臂，如下所示：
- en: '[PRE16]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, let's play the game and try to find the best arm using the epsilon-greedy
    method.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始游戏，并尝试使用 epsilon-greedy 方法找到最佳臂。
- en: 'For each round:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一回合：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Select the arm based on the epsilon-greedy method:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 epsilon-greedy 方法选择臂：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Pull the arm and store the reward and next state information:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 拉取臂并存储奖励和下一个状态信息：
- en: '[PRE19]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Increment the count of the arm by `1`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将臂的计数增加`1`：
- en: '[PRE20]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Update the sum of rewards of the arm:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 更新臂的奖励总和：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Update the average reward of the arm:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 更新臂的平均奖励：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After all the rounds, we look at the average reward obtained from each of the
    arms:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有回合结束后，我们查看每个臂获得的平均奖励：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The preceding code will print something like this:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印出如下内容：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we can select the optimal arm as the one that has the maximum average
    reward:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以选择平均奖励最大化的臂作为最佳臂：
- en: '![](img/B15558_06_002.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_002.png)'
- en: 'Since arm 1 has a higher average reward than arm 2, our optimal arm will be
    arm 1:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于臂 1 的平均奖励高于臂 2，我们的最佳臂将是臂 1：
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code will print:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印出：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Thus, we have found the optimal arm using the epsilon-greedy method.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经使用 epsilon-greedy 方法找到了最佳臂。
- en: Softmax exploration
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Softmax 探索
- en: Softmax exploration, also known as Boltzmann exploration, is another useful
    exploration strategy for finding the optimal arm.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 探索，也称为 Boltzmann 探索，是另一种有用的探索策略，用于找到最佳臂。
- en: In the epsilon-greedy policy, we learned that we select the best arm with probability
    1-epsilon and a random arm with probability epsilon. As you may have noticed,
    in the epsilon-greedy policy, all the non-best arms are explored equally. That
    is, all the non-best arms have a uniform probability of being selected. For example,
    say we have 4 arms and arm 1 is the best arm. Then we explore the non-best arms
    – [arm 2, arm 3, arm 4] – uniformly.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在 epsilon-greedy 策略中，我们学习到，以 1-epsilon 的概率选择最佳臂，以 epsilon 的概率选择随机臂。正如你可能注意到的，在
    epsilon-greedy 策略中，所有非最佳臂被平等地探索。也就是说，所有非最佳臂被选择的概率是均匀的。例如，假设我们有 4 个臂，其中臂 1 是最佳臂。那么我们就会均匀地探索非最佳臂——[臂
    2，臂 3，臂 4]。
- en: Say arm 3 is never a good arm and it always gives a reward of 0\. In this case,
    instead of exploring arm 3 again, we can spend more time exploring arm 2 and arm
    4\. But the problem with the epsilon-greedy method is that we explore all the
    non-best arms equally. So, all the non-best arms – [arm 2, arm 3, arm 4] – will
    be explored equally.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 假设臂 3 永远不是一个好臂，它总是给出 0 的奖励。在这种情况下，我们可以花更多时间探索臂 2 和臂 4，而不是再探索臂 3。但 epsilon-greedy
    方法的问题在于，它会平等地探索所有非最佳臂。因此，所有非最佳臂——[臂 2，臂 3，臂 4]——将被平等地探索。
- en: To avoid this, if we can give priority to arm 2 and arm 4 over arm 3, then we
    can explore arm 2 and arm 4 more than arm 3.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，如果我们可以优先选择臂 2 和臂 4，而不是臂 3，那么我们就可以更频繁地探索臂 2 和臂 4，而不是臂 3。
- en: Okay, but how can we give priority to the arms? We can give priority to the
    arms by assigning a probability to all the arms based on the average reward *Q*.
    The arm that has the maximum average reward will have high probability, and all
    the non-best arms have a probability proportional to their average reward.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但我们如何能优先选择某些臂呢？我们可以通过基于平均奖励 *Q* 为所有臂分配一个概率来实现臂的优先选择。拥有最大平均奖励的臂将有更高的概率，而所有非最佳臂的选择概率将与它们的平均奖励成正比。
- en: 'For instance, as *Table 6.7* shows, arm 1 is the best arm as it has a high
    average reward *Q*. So, we assign a high probability to arm 1\. Arms 2, 3, and
    4 are the non-best arms, and we need to explore them. As we can observe, arm 3
    has an average reward of 0\. So, instead of selecting all the non-best arms uniformly,
    we give more priority to arms 2 and 4 than arm 3\. So, the probability of arm
    2 and 4 will be high compared to arm 3:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，正如*表6.7*所示，臂1是最佳臂，因为它有较高的平均奖励*Q*。因此，我们给臂1分配较高的概率。臂2、3和4是非最佳臂，我们需要探索它们。正如我们所观察到的，臂3的平均奖励为0。因此，我们不会均匀地选择所有的非最佳臂，而是将更多的优先级给予臂2和臂4，而非臂3。所以，臂2和4的概率将高于臂3：
- en: '![](img/B15558_06_08.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_08.png)'
- en: 'Table 6.7: Average reward for a 4-armed bandit'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.7：四臂老虎机的平均奖励
- en: 'Thus, in softmax exploration, we select the arms based on a probability. The
    probability of each arm is directly proportional to its average reward:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在softmax探索中，我们是基于概率来选择臂的。每个臂的概率与其平均奖励成正比：
- en: '![](img/B15558_06_004.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_004.png)'
- en: 'But wait, the probabilities should sum to 1, right? The average reward (Q value)
    will not sum to 1\. So, we convert them into probabilities with the softmax function,
    as shown here:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，概率应该加起来等于1，对吧？平均奖励（Q值）加起来并不等于1。因此，我们使用softmax函数将它们转换为概率，如下所示：
- en: '![](img/B15558_06_005.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_005.png)'
- en: So, now the arm will be selected based on the probability. However, in the initial
    rounds we will not know the correct average reward of each arm, so selecting the
    arm based on the probability of average reward will be inaccurate in the initial
    rounds. To avoid this, we introduce a new parameter called *T*. *T* is called
    the temperature parameter.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在根据概率来选择臂。然而，在最初的几轮中，我们不知道每个臂的正确平均奖励，因此在初期基于平均奖励的概率选择臂会不准确。为了避免这种情况，我们引入了一个新的参数，叫做*T*。*T*被称为温度参数。
- en: 'We can rewrite the preceding equation with the temperature *T*, as shown here:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用温度*T*重新写出之前的方程，如下所示：
- en: '![](img/B15558_06_006.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_006.png)'
- en: Okay, how will this *T* help us? When *T* is high, all the arms have an equal
    probability of being selected and when *T* is low, the arm that has the maximum
    average reward will have a high probability. So, we set *T* to a high number in
    the initial rounds, and after a series of rounds we reduce the value of *T*. This
    means that in the initial round we explore all the arms equally and after a series
    of rounds, we select the best arm that has a high probability.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么这个*T*怎么帮助我们呢？当*T*较高时，所有的臂被选择的概率相等；而当*T*较低时，具有最大平均奖励的臂将具有较高的选择概率。因此，我们在最初的几轮将*T*设置为较大的数值，在经过一系列轮次后，我们逐渐降低*T*的值。这意味着在最初的轮次中，我们平等地探索所有的臂，而在经过一系列轮次后，我们选择概率较高的最佳臂。
- en: 'Let''s understand this with a simple example. Say we have four arms, arm 1
    to arm 4. Suppose we pull arm 1 and receive a reward of 1\. Then the average reward
    of arm 1 will be 1 and the average reward of all other arms will be 0, as *Table
    6.8* shows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来理解这一点。假设我们有四个臂，从臂1到臂4。假设我们拉动了臂1并获得了奖励1，那么臂1的平均奖励将是1，而所有其他臂的平均奖励将是0，正如*表6.8*所示：
- en: '![](img/B15558_06_09.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_09.png)'
- en: 'Table 6.8: Average reward for each arm'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.8：每个臂的平均奖励
- en: 'Now, if we convert the average reward to probabilities using the softmax function
    given in equation (1), then our probabilities look like the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们使用方程（1）中给出的softmax函数将平均奖励转换为概率，那么我们的概率如下所示：
- en: '![](img/B15558_06_10.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_10.png)'
- en: 'Table 6.9: Probability of each arm'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.9：每个臂的概率
- en: 'As we can observe, we have a 47% probability for arm 1 and a 17% probability
    for all other arms. But we cannot assign a high probability to arm 1 by just pulling
    arm 1 once. So, we set *T* to a high number, say *T* = 30, and calculate the probabilities
    based on equation (2). Now our probabilities become:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所观察到的，臂1的概率为47%，而其他所有臂的概率为17%。但是，我们不能仅仅通过拉动一次臂1就为臂1分配高概率。因此，我们将*T*设置为一个较大的数字，比如*T*
    = 30，并根据方程（2）计算概率。现在我们的概率变成了：
- en: '![](img/B15558_06_11.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_11.png)'
- en: 'Table 6.10: Probability of each arm with T=30'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.10：T=30时每个臂的概率
- en: 'As we can see, now all the arms have equal probabilities of being selected.
    Now we explore the arms based on this probability and over a series of rounds,
    the *T* value will be reduced, and we will have a high probability to the best
    arm. Let''s suppose after some 30 rounds, the average reward of all the arms is:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，现在所有臂被选中的概率相等。现在我们根据这个概率探索各个臂，经过若干回合后，*T*值将被降低，我们会将较高的概率分配给最优臂。假设经过大约30回合后，所有臂的平均奖励为：
- en: '![](img/B15558_06_12.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_12.png)'
- en: 'Table 6.11: Average reward for each arm after 30+ rounds'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.11：经过30多轮后的每个臂的平均奖励
- en: 'We learned that the value of *T* is reduced over several rounds. Suppose the
    value of *T* is reduced and it is now 0.3 (*T*=0.3); then the probabilities will
    become:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，*T*的值在多轮中会逐渐减少。假设*T*的值减少，现在为0.3（*T*=0.3）；此时概率将变为：
- en: '![](img/B15558_06_13.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_13.png)'
- en: 'Table 6.12: Probabilities for each arm with T now set to 0.3'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.12：当前T值设置为0.3时每个臂的概率
- en: As we can see, arm 1 has a high probability compared to other arms. So, we select
    arm 1 as the best arm and explore the non-best arms – [arm 2, arm 3, arm 4] –
    based on their probabilities in the next rounds.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，臂1相较于其他臂有较高的概率。因此，我们选择臂1作为最佳臂，并在接下来的回合中根据它们的概率探索非最佳臂——[臂2, 臂3, 臂4]。
- en: Thus, in the initial round, we don't know which arm is the best arm. So instead
    of assigning a high probability to the arm based on the average reward, we assign
    an equal probability to all the arms in the initial round with a high value of
    *T* and over a series of rounds, we reduce the value of *T* and assign a high
    probability to the arm that has a high average reward.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在初始回合中，我们并不知道哪个臂是最佳臂。因此，我们不是根据平均奖励为臂分配较高的概率，而是为所有臂分配相等的概率，并且在初始回合中设置较高的*T*值，随着若干回合的进行，我们逐步减少*T*的值，并为具有较高平均奖励的臂分配较高的概率。
- en: Implementing softmax exploration
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现软最大探索
- en: 'Now, let''s learn how to implement softmax exploration to find the best arm.
    First, let''s import the necessary libraries:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何实现软最大探索方法来找到最佳臂。首先，我们导入必要的库：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s take the same two-armed bandit we saw in the epsilon-greedy section:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以在ε-贪婪部分看到的同一个二臂老虎机为例：
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now, let's initialize the variables.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们初始化变量。
- en: 'Initialize `count` for storing the number of times an arm is pulled:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`count`用于存储每个臂被拉动的次数：
- en: '[PRE29]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Initialize `sum_rewards` for storing the sum of rewards of each arm:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`sum_rewards`用于存储每个臂的奖励总和：
- en: '[PRE30]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Initialize `Q` for storing the average reward of each arm:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`Q`用于存储每个臂的平均奖励：
- en: '[PRE31]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Set the number of rounds (iterations):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 设置回合数（迭代次数）：
- en: '[PRE32]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we define the softmax function with the temperature *T*:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义具有温度*T*的软最大函数：
- en: '![](img/B15558_06_007.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_007.png)'
- en: '[PRE33]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Compute the probability of each arm based on the preceding equation:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的公式计算每个臂的概率：
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Select the arm based on the computed probability distribution of arms:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 根据计算的臂的概率分布选择臂：
- en: '[PRE35]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now, let's play the game and try to find the best arm using the softmax exploration
    method.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始游戏，并尝试使用软最大探索方法找到最佳臂。
- en: 'Let''s begin by setting the temperature `T` to a high number, say, `50`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从将温度`T`设置为一个较大的数值开始，比如`50`：
- en: '[PRE36]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'For each round:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一回合：
- en: '[PRE37]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Select the arm based on the softmax exploration method:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 根据软最大探索方法选择臂：
- en: '[PRE38]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Pull the arm and store the reward and next state information:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 拉动臂并存储奖励和下一状态信息：
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Increment the count of the arm by 1:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 将该臂的计数增加1：
- en: '[PRE40]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Update the sum of rewards of the arm:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 更新该臂的奖励总和：
- en: '[PRE41]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Update the average reward of the arm:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 更新该臂的平均奖励：
- en: '[PRE42]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Reduce the temperature `T`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 降低温度`T`：
- en: '[PRE43]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'After all the rounds, we check the Q value, that is, the average reward of
    all the arms:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 所有回合结束后，我们检查Q值，也就是每个臂的平均奖励：
- en: '[PRE44]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The preceding code will print something like this:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码会输出类似于以下内容：
- en: '[PRE45]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'As we can see, arm 1 has a higher average reward than arm 2, so we select arm
    1 as the optimal arm:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，臂1的平均奖励高于臂2，因此我们选择臂1作为最优臂：
- en: '[PRE46]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The preceding code prints:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码会输出：
- en: '[PRE47]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Thus, we have found the optimal arm using the softmax exploration method.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经通过软最大探索方法找到了最优臂。
- en: Upper confidence bound
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上置信区间
- en: In this section, we will explore another interesting algorithm called **upper
    confidence bound** (**UCB**) for handling the exploration-exploitation dilemma.
    The UCB algorithm is based on a principle called optimism in the face of uncertainty.
    Let's take a simple example and understand how exactly the UCB algorithm works.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将探讨另一个有趣的算法，叫做**上置信界限**（**UCB**），它用于解决探索与开发的困境。UCB算法基于一个叫做“面对不确定性时的乐观主义”的原则。让我们通过一个简单的例子，了解UCB算法是如何工作的。
- en: Suppose we have two arms – arm 1 and arm 2\. Let's say we played the game for
    20 rounds by pulling arm 1 and arm 2 randomly and found that the mean reward of
    arm 1 is 0.6 and the mean reward of arm 2 is 0.5\. But how can we be sure that
    this mean reward is actually accurate? That is, how can we be sure that this mean
    reward represents the true mean (population mean)? This is where we use the confidence
    interval.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个臂——臂1和臂2。假设我们通过随机拉臂1和臂2玩了20轮游戏，发现臂1的平均奖励是0.6，臂2的平均奖励是0.5。但是我们怎么确定这个平均奖励是准确的呢？也就是说，我们怎么确定这个平均奖励代表了真实的平均值（总体均值）呢？这时，我们就需要使用置信区间。
- en: The confidence interval denotes the interval within which the true value lies.
    So, in our setting, the confidence interval denotes the interval within which
    the true mean reward of the arm lies.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 置信区间表示真实值所在的区间。因此，在我们的设置中，置信区间表示臂的真实平均奖励所在的区间。
- en: 'For instance, from *Figure 6.2*, we can see that the confidence interval of
    arm 1 is 0.2 to 0.9, which indicates that the mean reward of arm 1 lies in the
    range of 0.2 to 0.9\. 0.2 is the lower confidence bound and 0.9 is the upper confidence
    bound. Similarly, we can observe that the confidence interval of arm 2 is 0.5
    to 0.7, which indicates that the mean reward of arm 2 lies in the range of 0.5
    to 0.7\. where 0.5 is the lower confidence bound and 0.7 is the upper confidence
    bound:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，从*图6.2*中，我们可以看到臂1的置信区间是0.2到0.9，这表明臂1的平均奖励位于0.2到0.9之间。0.2是下置信界限，0.9是上置信界限。类似地，我们可以观察到臂2的置信区间是0.5到0.7，这表明臂2的平均奖励位于0.5到0.7之间，其中0.5是下置信界限，0.7是上置信界限。
- en: '![](img/B15558_06_14.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_14.png)'
- en: 'Figure 6.2: Confidence intervals for arms 1 and 2'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：臂1和臂2的置信区间
- en: Okay, from *Figure 6.2*, we can see the confidence intervals of arm 1 and arm
    2\. Now, how can we make a decision? That is, how can we decide whether to pull
    arm 1 or arm 2? If we look closely, we can see that the confidence interval of
    arm 1 is large and the confidence interval of arm 2 is small.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，从*图6.2*中我们可以看到臂1和臂2的置信区间。那么，我们该如何做出决策呢？也就是说，我们如何决定是拉臂1还是拉臂2？如果我们仔细观察，可以看到臂1的置信区间较大，而臂2的置信区间较小。
- en: When the confidence interval is large, we are uncertain about the mean value.
    Since the confidence interval of arm 1 is large (0.2 to 0.9), we are not sure
    what reward we would obtain by pulling arm 1 because the average reward varies
    from as low as 0.2 to as high as 0.9\. So, there is a lot of uncertainty in arm
    1 and we are not sure whether arm 1 gives a high reward or a low reward.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当置信区间较大时，我们对平均值的确定性较低。由于臂1的置信区间较大（0.2到0.9），我们无法确定拉动臂1时能获得什么奖励，因为平均奖励可能从0.2低至0.9高。所以，臂1存在很大的不确定性，我们不能确定臂1会给出高奖励还是低奖励。
- en: When the confidence interval is small, then we are certain about the mean value.
    Since the confidence interval of arm 2 is small (0.5 to 0.7), we can be sure that
    we will get a good reward by pulling arm 2 as our average reward is in the range
    of 0.5 to 0.7.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当置信区间较小时，我们对平均值较为确定。由于臂2的置信区间较小（0.5到0.7），我们可以确定拉臂2时会得到一个不错的奖励，因为我们的平均奖励在0.5到0.7的范围内。
- en: But what is the reason for the confidence interval of arm 2 being small and
    the confidence interval of arm 1 being large? At the beginning of the section,
    we learned that we played the game for 20 rounds by pulling arm 1 and arm 2 randomly
    and computed the mean reward of arm 1 and arm 2\. Say arm 2 has been pulled 15
    times and arm 1 has been pulled only 5 times. Since arm 2 has been pulled many
    times, the confidence interval of arm 2 is small and it denotes a certain mean
    reward. Since arm 1 has been pulled fewer times, the confidence interval of the
    arm is large and it denotes an uncertain mean reward. Thus, it indicates that
    arm 2 has been explored a lot more than arm 1.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，臂2的置信区间为什么小，臂1的置信区间为什么大呢？在本节的开头，我们了解到我们通过随机拉动臂1和臂2玩了20回合游戏，并计算了臂1和臂2的平均奖励。假设臂2已经被拉动了15次，而臂1只被拉动了5次。由于臂2被拉动了很多次，臂2的置信区间很小，表示有确定的平均奖励。由于臂1被拉动的次数较少，臂的置信区间较大，表示有不确定的平均奖励。因此，这表明臂2比臂1探索得更多。
- en: Okay, coming back to our question, should we pull arm 1 or arm 2? In UCB, we
    always select the arm that has a high upper confidence bound, so in our example,
    we select arm 1 since it has a high upper confidence bound of 0.9\. But why do
    we have to select the arm that has the highest upper confidence bound? Selecting
    the arm with the highest upper bound helps us to select the arm that gives the
    maximum reward.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，回到我们的问题，我们应该拉动臂1还是臂2？在UCB中，我们总是选择具有高上置信边界的臂，所以在我们的例子中，我们选择臂1，因为它的上置信边界高达0.9。但为什么我们必须选择具有最高上置信边界的臂呢？选择具有最高上界的臂帮助我们选择给出最大奖励的臂。
- en: But there is a small catch here. When the confidence interval is large, we will
    not be sure about the mean reward. For instance, in our example, we select arm
    1 since it has a high upper confidence bound of 0.9; however, since the confidence
    interval of arm 1 is large, our mean reward could be anywhere from 0.2 to 0.9,
    and so we can even get a low reward. But that's okay, we still select arm 1 as
    it promotes exploration. When the arm is explored well, then the confidence interval
    gets smaller.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 但这里有一个小问题。当置信区间较大时，我们无法确定平均奖励。例如，在我们的例子中，我们选择了臂1，因为它有一个很高的上置信边界为0.9；然而，由于臂1的置信区间较大，我们的平均奖励可能在0.2到0.9之间，因此我们甚至可能获得较低的奖励。但这没关系，我们仍然选择臂1，因为它促进了探索。当臂被充分探索时，置信区间变得较小。
- en: 'As we play the game for several rounds by selecting the arm that has a high
    UCB, our confidence interval of both arms will get narrower and denote a more
    accurate mean value. For instance, as we can see in *Figure 6.3*, after playing
    the game for several rounds, the confidence interval of both the arms becomes
    small and denotes a more accurate mean value:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过选择具有高UCB的臂来进行几轮游戏时，我们两臂的置信区间将变得更窄，并表示更准确的平均值。例如，正如我们在*图6.3*中看到的那样，在进行几轮游戏后，两臂的置信区间变小，并表示一个更准确的平均值：
- en: '![](img/B15558_06_15.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_15.png)'
- en: 'Figure 6.3: Confidence intervals for arms 1 and 2 after several rounds'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：几轮后臂1和臂2的置信区间
- en: From *Figure 6.3*, we can see that the confidence interval of both arms is small
    and we have a more accurate mean, and since in UCB we select arm that has the
    highest UCB, we select arm 2 as the best arm.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图6.3*中，我们可以看到两臂的置信区间较小，并且我们有一个更准确的平均值，由于在UCB中我们选择具有最高UCB的臂，因此我们选择臂2作为最佳臂。
- en: Thus, in UCB, we always select the arm that has the highest upper confidence
    bound. In the initial rounds, we may not select the best arm as the confidence
    interval of the arms will be large in the initial round. But over a series of
    rounds, the confidence interval gets smaller and we select the best arm.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在UCB中，我们总是选择具有最高上置信边界的臂。在初始回合中，我们可能不会选择最佳臂，因为臂的置信区间在初始回合中较大。但在一系列回合中，置信区间变得较小，我们选择了最佳臂。
- en: 'Let *N*(*a*) be the number of times arm *a* was pulled and *t* be the total
    number of rounds, then the upper confidence bound of arm *a* can be computed as:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让*N*(*a*)表示臂*a*被拉动的次数，*t*表示总回合数，则臂*a*的上置信边界可以计算为：
- en: '![](img/B15558_06_008.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_008.png)'
- en: 'We select the arm that has the highest upper confidence bound as the best arm:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择具有最高上置信边界的臂作为最佳臂：
- en: '![](img/B15558_06_009.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_009.png)'
- en: 'The algorithm of UCB is given as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: UCB算法如下所示：
- en: Select the arm whose upper confidence bound is high
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择上置信边界高的臂
- en: Pull the arm and receive a reward
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拉动臂并获得奖励
- en: Update the arm's mean reward and confidence interval
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新臂的平均奖励和置信区间
- en: Repeat *steps 1* to *3* for several rounds
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 *步骤 1* 到 *步骤 3* 进行若干轮重复
- en: Implementing UCB
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现 UCB
- en: Now, let's learn how to implement the UCB algorithm to find the best arm.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何实现 UCB 算法来找到最佳臂。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE48]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let''s create the same two-armed bandit we saw in the previous section:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建与上一节看到的相同的双臂赌博机：
- en: '[PRE49]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Now, let's initialize the variables.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们初始化这些变量。
- en: 'Initialize `count` for storing the number of times an arm is pulled:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 `count` 用于存储某个臂被拉动的次数：
- en: '[PRE50]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Initialize `sum_rewards` for storing the sum of rewards of each arm:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 `sum_rewards` 用于存储每个臂的奖励总和：
- en: '[PRE51]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Initialize `Q` for storing the average reward of each arm:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 `Q` 用于存储每个臂的平均奖励：
- en: '[PRE52]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Set the number of rounds (iterations):'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 设置轮次数（迭代次数）：
- en: '[PRE53]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, we define the **UCB function**, which returns the best arm as the one
    that has the highest UCB:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义 **UCB 函数**，它返回具有最高 UCB 的最佳臂：
- en: '[PRE54]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Initialize the `numpy` array for storing the UCB of all the arms:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 `numpy` 数组，用于存储所有臂的 UCB：
- en: '[PRE55]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Before computing the UCB, we explore all the arms at least once, so for the
    first 2 rounds, we directly select the arm corresponding to the round number:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算 UCB 之前，我们至少探索每个臂一次，因此在前两轮中，我们直接选择与轮次编号对应的臂：
- en: '[PRE56]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'If the round is greater than 2, then we compute the UCB of all the arms as
    specified in equation (3) and return the arm that has the highest UCB:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果轮次大于 2，则我们按照公式（3）计算所有臂的 UCB，并返回具有最高 UCB 的臂：
- en: '[PRE57]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Now, let's play the game and try to find the best arm using the UCB method.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们玩这个游戏，试着通过 UCB 方法找到最佳臂。
- en: 'For each round:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 每一轮：
- en: '[PRE58]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Select the arm based on the UCB method:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 UCB 方法选择臂：
- en: '[PRE59]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Pull the arm and store the reward and next state information:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 拉动臂并存储奖励和下一个状态信息：
- en: '[PRE60]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Increment the count of the arm by `1`:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 将该臂的计数增加 `1`：
- en: '[PRE61]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Update the sum of rewards of the arm:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 更新臂的奖励总和：
- en: '[PRE62]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Update the average reward of the arm:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 更新臂的平均奖励：
- en: '[PRE63]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'After all the rounds, we can select the optimal arm as the one that has the
    maximum average reward:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有回合结束后，我们可以选择具有最大平均奖励的最佳臂：
- en: '[PRE64]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The preceding code will print:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印：
- en: '[PRE65]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Thus, we found the optimal arm using the UCB method.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用 UCB 方法找到了最佳臂。
- en: Thompson sampling
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 汤普森采样
- en: '**Thompson sampling** (**TS**) is another interesting exploration strategy
    to overcome the exploration-exploitation dilemma and it is based on a beta distribution.
    So, before diving into Thompson sampling, let''s first understand the beta distribution.
    The beta distribution is a probability distribution function and it is expressed
    as:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**汤普森采样**（**TS**）是另一种有趣的探索策略，用于克服探索与利用之间的困境，并且它基于贝塔分布。因此，在深入研究汤普森采样之前，我们首先了解一下贝塔分布。贝塔分布是一种概率分布函数，表示为：'
- en: '![](img/B15558_06_010.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_010.png)'
- en: Where ![](img/B15558_06_011.png) and ![](img/B15558_06_012.png) is the gamma
    function.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B15558_06_011.png) 和 ![](img/B15558_06_012.png) 是伽马函数。
- en: The shape of the distribution is controlled by the two parameters ![](img/B15558_06_013.png)
    and ![](img/B15558_06_014.png). When the values of ![](img/B15558_05_055.png)
    and ![](img/B15558_06_016.png) are the same, then we will have a symmetric distribution.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 分布的形状由两个参数控制！[](img/B15558_06_013.png) 和 ![](img/B15558_06_014.png)。当 ![](img/B15558_05_055.png)
    和 ![](img/B15558_06_016.png) 的值相同时，我们将得到一个对称分布。
- en: 'For instance, as *Figure 6.4* shows, since the value of ![](img/B15558_05_055.png)
    and ![](img/B15558_06_018.png) is equal to two we have a symmetric distribution:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，正如 *图 6.4* 所示，由于 ![](img/B15558_05_055.png) 和 ![](img/B15558_06_018.png)
    的值相等，我们得到了一个对称分布：
- en: '![](img/B15558_06_16.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_16.png)'
- en: 'Figure 6.4: Symmetric beta distribution'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：对称贝塔分布
- en: 'When the value of ![](img/B15558_06_019.png) is higher than ![](img/B15558_06_020.png)
    then we will have a probability closer to 1 than 0\. For instance, as *Figure
    6.5* shows, since the value of ![](img/B15558_06_021.png) and ![](img/B15558_06_022.png),
    we have a high probability closer to 1 than 0:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ![](img/B15558_06_019.png) 的值大于 ![](img/B15558_06_020.png) 时，我们会得到一个接近 1 而非
    0 的概率。例如，正如 *图 6.5* 所示，由于 ![](img/B15558_06_021.png) 和 ![](img/B15558_06_022.png)
    的值相同，我们有一个接近 1 而非 0 的高概率：
- en: '![](img/B15558_06_17.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_17.png)'
- en: 'Figure 6.5: Beta distribution where ![](img/B15558_06_023.png)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：贝塔分布，其中 ![](img/B15558_06_023.png)
- en: 'When the value of ![](img/B15558_06_024.png) is higher than ![](img/B15558_05_055.png)
    then we will have a high probability closer to 0 than 1\. For instance, as shown
    in the following plot, since the value of ![](img/B15558_06_026.png) and ![](img/B15558_06_027.png),
    we have a high probability closer to 0 than 1:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ![](img/B15558_06_024.png) 的值大于 ![](img/B15558_05_055.png) 时，我们将有一个接近 0 而不是
    1 的高概率。例如，如下图所示，由于 ![](img/B15558_06_026.png) 和 ![](img/B15558_06_027.png) 的值，我们有一个接近
    0 而不是 1 的高概率：
- en: '![](img/B15558_06_18.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_18.png)'
- en: 'Figure 6.6: Gamma distribution where ![](img/B15558_06_028.png)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：伽玛分布，其中 ![](img/B15558_06_028.png)
- en: 'Now that we have a basic idea of the beta distribution, let''s explore how
    Thompson sampling works and how it uses the beta distribution. Understanding the
    true distribution of each arm is very important because once we know the true
    distribution of the arm, then we can easily understand whether the arm will give
    us a good reward; that is, we can understand whether pulling the arm will help
    us to win the game. For example, let''s say we have two arms – arm 1 and arm 2\.
    *Figure 6.7* shows the true distribution of the two arms:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对贝塔分布有了基本的了解，让我们探讨一下汤普森采样是如何工作的，以及它是如何利用贝塔分布的。理解每个臂部的真实分布非常重要，因为一旦我们知道臂部的真实分布，我们就能轻松了解该臂是否会给我们带来良好的奖励；也就是说，我们可以了解拉动该臂是否能帮助我们赢得比赛。例如，假设我们有两个臂——臂
    1 和臂 2。*图 6.7* 显示了这两个臂的真实分布：
- en: '![](img/B15558_06_19.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_19.png)'
- en: 'Figure 6.7: True distributions for arms 1 and 2'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：臂 1 和臂 2 的真实分布
- en: From *Figure 6.7*, we can see that it is better to pull arm 1 than arm 2 because
    arm 1 has a high probability close to 1, but arm 2 has a high probability close
    to 0\. So, if we pull arm 1, we get a reward of 1 and win the game, but if we
    pull arm 2 we get a reward of 0 and lose the game. Thus, once we know the true
    distribution of the arms then we can understand which arm is the best arm.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 6.7*中我们可以看到，拉臂 1 比拉臂 2 更好，因为臂 1 的高概率接近 1，而臂 2 的高概率接近 0。因此，如果我们拉臂 1，我们将获得奖励
    1 并赢得比赛，但如果我们拉臂 2，我们将获得奖励 0 并输掉比赛。因此，一旦我们知道臂部的真实分布，我们就能理解哪个臂部是最好的臂。
- en: But how can we learn the true distribution of arm 1 and arm 2? This is where
    we use the Thompson sampling method. Thompson sampling is a probabilistic method
    and it is based on a prior distribution.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我们怎么才能了解臂 1 和臂 2 的真实分布呢？这就是我们使用汤普森采样方法的地方。汤普森采样是一种基于概率的方法，它是基于先验分布的。
- en: 'First, we take *n* samples from arm 1 and arm 2 and compute their distribution.
    However, in the initial iterations, the computed distributions of arm 1 and arm
    2 will not be the same as the true distribution, and so we will call this the
    prior distribution. As *Figure 6.8* shows, we have the prior distribution of arm
    1 and arm 2, and it varies from the true distribution:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从臂 1 和臂 2 中各取 *n* 个样本并计算它们的分布。然而，在初始的几次迭代中，臂 1 和臂 2 的计算分布将不会与真实分布相同，因此我们称之为先验分布。正如*图
    6.8*所示，我们有臂 1 和臂 2 的先验分布，并且它与真实分布有所不同：
- en: '![](img/B15558_06_20.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_20.png)'
- en: 'Figure 6.8: Prior distributions for arms 1 and 2'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8：臂 1 和臂 2 的先验分布
- en: 'But over a series of iterations, we learn the true distribution of arm 1 and
    arm 2 and, as *Figure 6.9* shows, the prior distributions of the arms look the
    same as the true distribution after a series of iterations:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 但是通过一系列迭代，我们会学到臂 1 和臂 2 的真实分布，正如*图 6.9*所示，经过一系列迭代后，臂的先验分布看起来与真实分布相同：
- en: '![](img/B15558_06_21.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_21.png)'
- en: 'Figure 6.9: The prior distributions move closer to the true distributions'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9：先验分布趋向真实分布
- en: Once we have learned the true distributions of all the arms, then we can easily
    select the best arm. Okay, but how exactly do we learn the true distribution?
    Let's explore this in more detail.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们学到所有臂的真实分布，那么我们就可以轻松选择最优臂部。好吧，但我们究竟是如何学习到真实分布的呢？让我们更详细地探索一下。
- en: 'Here, we use the beta distribution as a prior distribution. Say we have two
    arms, so we will have two beta distributions (prior distributions), and we initialize
    both ![](img/B15558_05_055.png) and ![](img/B15558_06_030.png) to the same value,
    say 3, as *Figure 6.10* shows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用贝塔分布作为先验分布。假设我们有两个臂部，那么我们将有两个贝塔分布（先验分布），并且我们将初始化两者！[](img/B15558_05_055.png)
    和 ![](img/B15558_06_030.png) 为相同的值，比如 3，正如*图 6.10*所示：
- en: '![](img/B15558_06_22.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_22.png)'
- en: 'Figure 6.10: Initialized prior distributions for arms 1 and 2 look the same'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10：初始化的臂 1 和臂 2 的先验分布看起来相同
- en: As we can see, since we initialized alpha and beta to the same value, the beta
    distributions of arm 1 and arm 2 look the same.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，由于我们将alpha和beta初始化为相同的值，臂1和臂2的beta分布看起来是一样的。
- en: 'In the first round, we just randomly sample a value from these two distributions
    and select the arm that has the maximum sampled value. Let''s say the sampled
    value of arm 1 is high, so in this case, we pull arm 1\. Say we win the game by
    pulling arm 1, then we update the distribution of arm 1 by incrementing the alpha
    value of the distribution by 1; that is, we update the alpha value as ![](img/B15558_06_031.png).
    As *Figure 6.11* shows, the alpha value of the distribution of arm 1 is incremented,
    and as we can see, arm 1''s beta distribution has slightly high probability closer
    to 1 compared to arm 2:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一轮中，我们只是从这两个分布中随机抽取一个值，并选择具有最大抽样值的臂。假设臂1的抽样值较高，所以在这种情况下，我们拉动臂1。假设我们通过拉动臂1赢得了游戏，那么我们通过将臂1分布的alpha值增加1来更新其分布；也就是说，我们更新alpha值为
    ![](img/B15558_06_031.png)。正如*图6.11*所示，臂1分布的alpha值已增加，并且我们可以看到，臂1的beta分布与臂2相比，在接近1的地方具有略高的概率：
- en: '![](img/B15558_06_23.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_23.png)'
- en: 'Figure 6.11: Prior distributions for arms 1 and 2 after round 1'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：第一轮后臂1和臂2的先验分布
- en: 'In the next round, we again sample a value randomly from these two distributions
    and select the arm that has the maximum sampled value. Suppose, in this round
    as well, we got the maximum sampled value from arm 1\. Then we pull the arm 1
    again. Say we win the game by pulling arm 1, then we update the distribution of
    arm 1 by updating the alpha value to ![](img/B15558_06_031.png). As *Figure 6.12*
    shows, the alpha value of arm 1''s distribution is incremented, and arm 1''s beta
    distribution has a slightly high probability close to 1:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一轮中，我们再次从这两个分布中随机抽取一个值，并选择具有最大抽样值的臂。假设在这一轮中，我们仍然从臂1获得了最大抽样值。那么我们再次拉动臂1。假设我们通过拉动臂1赢得了游戏，那么我们通过将臂1的alpha值更新为
    ![](img/B15558_06_031.png) 来更新臂1的分布。正如*图6.12*所示，臂1分布的alpha值已增加，且臂1的beta分布在接近1的地方具有略高的概率：
- en: '![](img/B15558_06_24.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_24.png)'
- en: 'Figure 6.12: Prior distributions for arms 1 and 2 after round 2'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12：第二轮后臂1和臂2的先验分布
- en: 'Similarly, in the next round, we again randomly sample a value from these distributions
    and pull the arm that has the maximum value. Say this time we got the maximum
    value from arm 2, so we pull arm 2 and play the game. Suppose we lose the game
    by pulling arm 2\. Then we update the distribution of arm 2 by updating the beta
    value as ![](img/B15558_06_033.png). As *Figure 6.13* shows, the beta value of
    arm 2''s distribution is incremented and the beta distribution of arm 2 has a
    slightly high probability close to 0:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在下一轮中，我们再次从这些分布中随机抽取一个值，并拉动具有最大值的臂。假设这次我们从臂2获得了最大值，因此我们拉动臂2并进行游戏。假设我们通过拉动臂2输掉了游戏。那么我们通过将臂2的beta值更新为
    ![](img/B15558_06_033.png) 来更新臂2的分布。正如*图6.13*所示，臂2分布的beta值已增加，并且臂2的beta分布在接近0的地方具有略高的概率：
- en: '![](img/B15558_06_25.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_25.png)'
- en: 'Figure 6.13: Prior distributions for arms 1 and 2 after round 3'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13：第三轮后臂1和臂2的先验分布
- en: 'Again, in the next round, we randomly sample a value from the beta distribution
    of arm 1 and arm 2\. Say the sampled value of arm 2 is high, so we pull arm 2\.
    Say we lose the game again by pulling arm 2\. Then we update the distribution
    of arm 2 by updating the beta value as ![](img/B15558_06_034.png). As *Figure
    6.14* shows, the beta value of arm 2''s distribution is incremented by 1 and also
    arm 2''s beta distribution has a slightly high probability close to 0:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在下一轮中，我们从臂1和臂2的beta分布中随机抽取一个值。假设臂2的抽样值较高，因此我们拉动臂2。假设我们再次通过拉动臂2输掉了游戏。那么我们通过将臂2的beta值更新为
    ![](img/B15558_06_034.png) 来更新臂2的分布。正如*图6.14*所示，臂2分布的beta值增加了1，并且臂2的beta分布在接近0的地方具有略高的概率：
- en: '![](img/B15558_06_26.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_26.png)'
- en: 'Figure 6.14: Prior distributions for arms 1 and 2 after round 4'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14：第四轮后臂1和臂2的先验分布
- en: 'Okay, so, did you notice what we are doing here? We are essentially increasing
    the alpha value of the distribution of the arm if we win the game by pulling that
    arm, else we increase the beta value. If we do this repeatedly for several rounds,
    then we can learn the true distribution of the arm. Say after several rounds,
    our distribution will look like *Figure 6.15*. As we can see, the distributions
    of both arms resemble the true distributions:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，你注意到我们在做什么吗？如果通过拉取某个臂我们赢得了游戏，我们实际上是在增加该臂分布的alpha值，否则我们增加beta值。如果我们在多轮中重复这样做，就能学到该臂的真实分布。假设经过若干轮后，我们的分布将像*图
    6.15*一样。正如我们所见，两个臂的分布都接近真实分布：
- en: '![](img/B15558_06_27.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_27.png)'
- en: 'Figure 6.15: Prior distributions for arms 1 and 2 after several rounds'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15：经过若干轮后，臂 1 和臂 2 的先验分布
- en: Now if we sample a value from each of these distributions, then the sampled
    value will always be high from arm 1 and we always pull arm 1 and win the game.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们从这些分布中抽取一个值，那么从臂 1 中抽取的值总是高的，我们总是拉臂 1 并赢得游戏。
- en: 'The steps involved in the Thomson sampling method are given here:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 汤普森抽样方法的步骤如下：
- en: Initialize the beta distribution with alpha and beta set to equal values for
    all *k* arms
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用相等的alpha和beta值初始化所有*k*臂的beta分布：
- en: Sample a value from the beta distribution of all *k* arms
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从所有*k*臂的beta分布中抽取一个值：
- en: Pull the arm whose sampled value is high
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拉取抽取值高的臂
- en: If we win the game, then update the alpha value of the distribution to ![](img/B15558_06_035.png)
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们赢得游戏，则将分布的alpha值更新为 ![](img/B15558_06_035.png)
- en: If we lose the game, then update the beta value of the distribution to ![](img/B15558_06_034.png)
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们输掉游戏，则将分布的beta值更新为 ![](img/B15558_06_034.png)
- en: Repeat *steps 2* to *5* for many rounds
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 2*到*步骤 5*多轮
- en: Implementing Thompson sampling
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现汤普森抽样
- en: Now, let's learn how to implement the Thompson sampling method to find the best
    arm.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何实现汤普森抽样方法来找到最佳臂。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE66]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'For better understanding, let''s create the same two-armed bandit we saw in
    the previous section:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好理解，让我们创建前一节中看到的相同的双臂老虎机：
- en: '[PRE67]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Now, let's initialize the variables.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们初始化变量。
- en: 'Initialize `count` for storing the number of times an arm is pulled:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`count`用于存储每个臂被拉取的次数：
- en: '[PRE68]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Initialize `sum_rewards` for storing the sum of rewards of each arm:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`sum_rewards`用于存储每个臂的奖励总和：
- en: '[PRE69]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Initialize `Q` for storing the average reward of each arm:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化`Q`用于存储每个臂的平均奖励：
- en: '[PRE70]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Initialize the alpha value as `1` for both arms:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 为两个臂初始化alpha值为`1`：
- en: '[PRE71]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Initialize the beta value as `1` for both arms:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 为两个臂初始化beta值为`1`：
- en: '[PRE72]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Set the number of rounds (iterations):'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 设置轮次数量（迭代次数）：
- en: '[PRE73]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Now, let's define the `thompson_sampling` function.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义`thompson_sampling`函数。
- en: 'As the following code shows, we randomly sample values from the beta distributions
    of both arms and return the arm that has the maximum sampled value:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 如下代码所示，我们从两个臂的beta分布中随机抽取值，并返回具有最大抽取值的臂：
- en: '[PRE74]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Now, let's play the game and try to find the best arm using the Thompson sampling
    method.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始游戏，并尝试使用汤普森抽样方法找到最佳臂。
- en: 'For each round:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一轮：
- en: '[PRE75]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Select the arm based on the Thompson sampling method:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 根据汤普森抽样方法选择臂：
- en: '[PRE76]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Pull the arm and store the reward and next state information:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 拉取臂并存储奖励和下一状态信息：
- en: '[PRE77]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Increment the count of the arm by 1:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 将臂的计数增加 1：
- en: '[PRE78]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Update the sum of rewards of the arm:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 更新臂的奖励总和：
- en: '[PRE79]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Update the average reward of the arm:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 更新臂的平均奖励：
- en: '[PRE80]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'If we win the game, that is, if the reward is equal to 1, then we update the
    value of alpha to ![](img/B15558_06_037.png), else we update the value of beta
    to ![](img/B15558_06_033.png):'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们赢得游戏，即奖励为 1，则将alpha值更新为 ![](img/B15558_06_037.png)，否则将beta值更新为 ![](img/B15558_06_033.png)：
- en: '[PRE81]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'After all the rounds, we can select the optimal arm as the one that has the
    highest average reward:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 所有轮次结束后，我们可以选择拥有最高平均奖励的臂作为最优臂：
- en: '[PRE82]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The preceding code will print:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将输出：
- en: '[PRE83]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Thus, we found the optimal arm using the Thompson sampling method.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用汤普森抽样方法找到了最优臂。
- en: Applications of MAB
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MAB的应用
- en: So far, we have learned about the MAB problem and how can we solve it using
    various exploration strategies. But our goal is not to just use these algorithms
    for playing slot machines. We can apply the various exploration strategies to
    several different use cases.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了MAB问题以及如何使用各种探索策略来解决它。但我们的目标不仅仅是使用这些算法来玩老虎机。我们可以将这些不同的探索策略应用到几个不同的使用场景中。
- en: For instance, bandits can be used as an alternative to AB testing. AB testing
    is one of the most commonly used classic methods of testing. Say we have two versions
    of the landing page of our website. Suppose we want to know which version of the
    landing page is most liked by the users. In this case, we conduct AB testing to
    understand which version of the landing page is most liked by the users. So, we
    show version 1 of the landing page to a particular set of users and version 2
    of the landing page to other set of users. Then we measure several metrics, such
    as click-through rate, average time spent on the website, and so on, to understand
    which version of the landing page is most liked by the users. Once we understand
    which version of the landing page is most liked by the users, then we will start
    showing that version to all the users.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，强盗可以作为AB测试的替代方案。AB测试是最常用的经典测试方法之一。假设我们有我们网站的两个版本的登陆页面。假设我们想知道哪个版本的登陆页面最受用户喜欢。在这种情况下，我们进行AB测试，以了解哪个版本的登陆页面最受用户喜欢。于是，我们将版本1的登陆页面展示给一组特定的用户，将版本2的登陆页面展示给另一组用户。然后我们衡量几个指标，如点击率、网站平均停留时间等，以了解哪个版本的登陆页面最受用户喜欢。一旦我们了解了哪个版本的登陆页面最受用户喜爱，我们将开始将该版本展示给所有用户。
- en: Thus, in AB testing, we schedule a separate time for exploration and exploitation.
    That is, AB testing has two different dedicated periods for exploration and exploitation.
    But the problem with AB testing is that it will incur high regret. We can minimize
    the regret using the various exploration strategies that we have used to solve
    the MAB problem. So, instead of performing complete exploration and exploitation
    separately, we can perform exploration and exploitation simultaneously in an adaptive
    fashion with the various exploration strategies we learned in the previous sections.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在AB测试中，我们为探索和利用安排了独立的时间段。也就是说，AB测试有两个不同的专用时间段来进行探索和利用。但AB测试的问题在于，它会带来较高的后悔值。我们可以使用解决MAB问题时使用的各种探索策略来最小化后悔值。因此，我们可以通过在探索和利用中同时进行自适应操作，而不是完全分开地进行探索和利用，从而优化AB测试的效果。
- en: Bandits are widely used for website optimization, maximizing conversion rates,
    online advertisements, campaigning, and so on.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 强盗广泛应用于网站优化、最大化转化率、在线广告、活动策划等领域。
- en: Finding the best advertisement banner using bandits
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用强盗找到最佳广告横幅
- en: In this section, let's see how to find the best advertisement banner using bandits.
    Suppose we are running a website and we have five different banners for a single
    advertisement on our website, and say we want to figure out which advertisement
    banner is most liked by the users.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们看看如何使用强盗找到最好的广告横幅。假设我们正在运行一个网站，并且我们有五个不同的广告横幅，用于展示同一个广告，并且假设我们想弄清楚哪个广告横幅最受用户喜欢。
- en: We can frame this problem as a MAB problem. The five advertisement banners represent
    the five arms of the bandit, and we assign +1 reward if the user clicks the advertisement
    and 0 reward if the user does not click the advertisement. So, to find out which
    advertisement banner is most clicked by the users, that is, which advertisement
    banner can give us the maximum reward, we can use various exploration strategies.
    In this section, let's just use an epsilon-greedy method to find the best advertisement
    banner.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个问题框架化为一个MAB问题。五个广告横幅代表了强盗的五个臂，如果用户点击广告，则奖励为+1；如果用户没有点击广告，则奖励为0。因此，为了找出哪个广告横幅最受用户点击，即哪个广告横幅可以为我们带来最大奖励，我们可以使用各种探索策略。在本节中，我们只使用epsilon-greedy方法来找出最好的广告横幅。
- en: 'First, let''s import the necessary libraries:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们导入必要的库：
- en: '[PRE84]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Creating a dataset
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据集
- en: 'Now, let''s create a dataset. We generate a dataset with five columns denoting
    the five advertisement banners, and we generate 100,000 rows, where the values
    in the rows will be either 0 or 1, indicating whether the advertisement banner
    has been clicked (1) or not clicked (0) by the user:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个数据集。我们生成一个包含五列的列表示五个广告横幅的数据集，并生成 100,000 行，其中每行的值为 0 或 1，表示用户是否点击了广告横幅（1）或未点击（0）：
- en: '[PRE85]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Let''s look at the first few rows of our dataset:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下数据集的前几行：
- en: '[PRE86]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The preceding code will print the following. As we can see, we have the five
    advertisement banners (0 to 4) and the rows consisting of values of 0 or 1, indicating
    whether the banner has been clicked (1) or not clicked (0).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印以下内容。正如我们所见，我们有五个广告横幅（0 到 4），每行由 0 或 1 的值组成，表示广告横幅是否被点击（1）或未被点击（0）。
- en: '![](img/B15558_06_28.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_28.png)'
- en: 'Figure 6.14: Clicks per banner'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14：每个横幅的点击次数
- en: Initialize the variables
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化变量
- en: Now, let's initialize some of the important variables.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们初始化一些重要的变量。
- en: 'Set the number of iterations:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 设置迭代次数：
- en: '[PRE87]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Define the number of banners:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 定义横幅的数量：
- en: '[PRE88]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Initialize `count` for storing the number of times the banner was clicked:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 `count` 以存储横幅被点击的次数：
- en: '[PRE89]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Initialize `sum_rewards` for storing the sum of rewards obtained from each
    banner:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 `sum_rewards` 以存储每个横幅获得的奖励总和：
- en: '[PRE90]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Initialize `Q` for storing the mean reward of each banner:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 `Q` 以存储每个横幅的平均奖励：
- en: '[PRE91]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Define a list for storing the selected banners:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个列表来存储已选择的横幅：
- en: '[PRE92]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Define the epsilon-greedy method
  id: totrans-414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义 epsilon-greedy 方法
- en: 'Now, let''s define the epsilon-greedy method. We generate a random value from
    a uniform distribution. If the random value is less than epsilon, then we select
    the random banner; else, we select the best banner that has the maximum average
    reward:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义 epsilon-greedy 方法。我们从均匀分布中生成一个随机值。如果随机值小于 epsilon，则选择一个随机横幅；否则，选择具有最大平均奖励的最佳横幅：
- en: '[PRE93]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: Run the bandit test
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行赌博机测试
- en: Now, we run the epsilon-greedy policy to find out which advertisement banner
    is the best.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们运行 epsilon-greedy 策略来找出哪一个广告横幅是最好的。
- en: 'For each iteration:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次迭代：
- en: '[PRE94]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Select the banner using the epsilon-greedy policy:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 epsilon-greedy 策略选择横幅：
- en: '[PRE95]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Get the reward of the banner:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 获取横幅的奖励：
- en: '[PRE96]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Increment the counter:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 增加计数器：
- en: '[PRE97]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Store the sum of rewards:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 存储奖励总和：
- en: '[PRE98]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'Compute the average reward:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 计算平均奖励：
- en: '[PRE99]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Store the banner to the banner selected list:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 将横幅存储到已选择的横幅列表中：
- en: '[PRE100]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'After all the rounds, we can select the best banner as the one that has the
    maximum average reward:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有轮次结束后，我们可以选择平均奖励最大的一面作为最佳横幅：
- en: '[PRE101]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'The preceding code will print:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将输出：
- en: '[PRE102]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'We can also plot and see which banner is selected the most often:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以绘制并查看哪一面横幅被选择得最多：
- en: '[PRE103]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'The preceding code will plot the following. As we can see, banner 2 is selected
    most often:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将绘制以下内容。正如我们所见，横幅 2 被选择的次数最多：
- en: '![](img/B15558_06_29.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_06_29.png)'
- en: 'Figure 6.15: Banner 2 is the best advertisement banner'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15：横幅 2 是最佳广告横幅
- en: Thus, we have learned how to find the best advertisement banner by framing our
    problem as a MAB problem.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们学会了如何通过将问题框架化为 MAB 问题来找到最佳广告横幅。
- en: Contextual bandits
  id: totrans-443
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文赌博机
- en: We just learned how to use bandits to find the best advertisement banner for
    the users. But the banner preference varies from user to user. That is, user A
    likes banner 1, but user B might like banner 3, and so on. Each user has their
    own preferences. So, we have to personalize advertisement banners according to
    each user. How can we do that? This is where we use contextual bandits.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学会了如何使用赌博机来为用户找到最好的广告横幅。但每个用户的横幅偏好是不同的。也就是说，用户 A 喜欢横幅 1，但用户 B 可能更喜欢横幅 3，依此类推。每个用户都有自己的偏好。因此，我们需要根据每个用户的需求个性化广告横幅。我们该如何做到这一点呢？这时，我们就需要使用上下文赌博机。
- en: In the MAB problem, we just perform the action and receive a reward. But with
    contextual bandits, we take actions based on the state of the environment and
    the state holds the context.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MAB 问题中，我们只执行动作并获得奖励。但在上下文赌博机中，我们根据环境的状态采取行动，状态包含了上下文信息。
- en: For instance, in the advertisement banner example, the state specifies the user
    behavior and we will take action (show the banner) according to the state (user
    behavior) that will result in the maximum reward (ad clicks).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在广告横幅示例中，状态指定了用户行为，我们将根据状态（用户行为）采取行动（显示横幅），以便获得最大奖励（广告点击）。
- en: Contextual bandits are widely used for personalizing content according to the
    user's behavior. They are also used to solve the cold-start problems faced by
    recommendation systems. Netflix uses contextual bandits for personalizing the
    artwork for TV shows according to user behavior.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文赌博机广泛应用于根据用户行为个性化内容。它们还用于解决推荐系统面临的冷启动问题。Netflix 使用上下文赌博机根据用户行为个性化电视节目的艺术作品。
- en: Summary
  id: totrans-448
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started off the chapter by understanding what the MAB problem is and how
    it can be solved using several exploration strategies. We first learned about
    the epsilon-greedy method, where we select a random arm with a probability epsilon
    and select the best arm with a probability 1-epsilon. Next, we learned about the
    softmax exploration method, where we select the arm based on the probability distribution,
    and the probability of each arm is proportional to the average reward.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从理解 MAB 问题是什么以及如何通过几种探索策略来解决它开始。本章首先介绍了 epsilon-greedy 方法，在该方法中，我们以概率 epsilon
    随机选择一个臂，并以概率 1-epsilon 选择最好的臂。接下来，我们学习了 softmax 探索方法，在该方法中，我们根据概率分布选择臂，每个臂的概率与其平均奖励成正比。
- en: Following this, we learned about the UCB algorithm, where we select the arm
    that has the highest upper confidence bound. Then, we explored the Thomspon sampling
    method, where we learned the distributions of the arms based on the beta distribution.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 继而，我们学习了 UCB 算法，在该算法中，我们选择具有最高上置信界限的臂。然后，我们探索了 Thomspon 采样方法，其中我们基于贝塔分布学习臂的分布。
- en: Moving forward, we learned how MAB can be used as an alternative to AB testing
    and how can we find the best advertisement banner by framing the problem as a
    MAB problem. At the end of the chapter, we also had an overview of contextual
    bandits.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步学习后，我们了解了 MAB 如何作为 AB 测试的替代方法，以及如何通过将问题框定为 MAB 问题来找到最佳的广告横幅。在本章的结尾，我们还对上下文赌博机有了概述。
- en: In the next chapter, we will learn about several interesting deep learning algorithms
    that are essential for deep reinforcement learning.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习几种对深度强化学习至关重要的有趣深度学习算法。
- en: Questions
  id: totrans-453
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Let''s evaluate the knowledge we gained in this chapter by answering the following
    questions:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 通过回答以下问题，来评估我们在本章中获得的知识：
- en: What is a MAB problem?
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 MAB 问题？
- en: How does the epsilon-greedy policy select an arm?
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: epsilon-greedy 策略如何选择一个臂？
- en: What is the significance of *T* in softmax exploration?
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 softmax 探索中，*T* 的意义是什么？
- en: How do we compute the upper confidence bound?
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何计算上置信界限？
- en: What happens when the value of alpha is higher than the value of beta in the
    beta distribution?
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当贝塔分布中的 alpha 值大于 beta 值时会发生什么？
- en: What are the steps involved in Thompson sampling?
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Thompson 采样中涉及的步骤是什么？
- en: What are contextual bandits?
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是上下文赌博机？
- en: Further reading
  id: totrans-462
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For more information, check out these interesting resources:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请查阅以下有趣的资源：
- en: '**Introduction to Multi-Armed Bandits** by *Aleksandrs Slivkins*, [https://arxiv.org/pdf/1904.07272.pdf](https://arxiv.org/pdf/1904.07272.pdf)'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多臂赌博机介绍** *由 Aleksandrs Slivkins* 撰写，[https://arxiv.org/pdf/1904.07272.pdf](https://arxiv.org/pdf/1904.07272.pdf)'
- en: '**A Survey on Practical Applications of Multi-Armed and Contextual Bandits**
    by *Djallel Bouneffouf, Irina Rish*, [https://arxiv.org/pdf/1904.10040.pdf](https://arxiv.org/pdf/1904.10040.pdf)'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多臂老虎机和上下文赌博机的实际应用调查** *由 Djallel Bouneffouf, Irina Rish* 撰写，[https://arxiv.org/pdf/1904.10040.pdf](https://arxiv.org/pdf/1904.10040.pdf)'
- en: '**Collaborative Filtering Bandits** by *Shuai Li, Alexandros Karatzoglou, Claudio
    Gentile*, [https://arxiv.org/pdf/1502.03473.pdf](https://arxiv.org/pdf/1502.03473.pdf)'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协同过滤赌博机** *由 Shuai Li, Alexandros Karatzoglou, Claudio Gentile* 撰写，[https://arxiv.org/pdf/1502.03473.pdf](https://arxiv.org/pdf/1502.03473.pdf)'
