- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: The Future Ahead
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前方的未来
- en: In this book, we started with how a neural network could digest text. As we
    have seen, neural networks do not do this natively but require the text to be
    processed. Simple neural networks can be used for some basic tasks such as classification,
    but human language carries an enormous amount of complex information.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们首先介绍了神经网络如何处理文本。正如我们所见，神经网络本身并不具备这种能力，但需要处理文本。简单的神经网络可以用于一些基本任务，如分类，但人类语言承载着大量的复杂信息。
- en: In *Chapters 2* and *3*, we saw how we need sophisticated models in order to
    use semantic and syntactic information. The emergence of transformers and LLMs
    has made it possible to have models capable of reasoning and storing enormous
    amounts of factual knowledge. These multipurpose knowledge and skills have enabled
    LLMs to solve tasks for which they have not been trained (coding, solving math
    problems, and so on). Nevertheless, LLMs have problems such as a lack of specialized
    domain knowledge, continual learning, being able to use tools, and so on. Thus,
    from [*Chapter 4*](B21257_04.xhtml#_idTextAnchor058) onward, we described systems
    that extend the capabilities of LLMs and which are designed to solve LLMs’ problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章和第3章中，我们看到了为了使用语义和句法信息，我们需要复杂的模型。Transformer和LLMs的出现使得拥有能够推理和存储大量事实知识的模型成为可能。这些多功能知识和技能使得LLMs能够解决他们未经训练的任务（如编码、解决数学问题等）。尽管如此，LLMs仍然存在一些问题，如缺乏专业领域知识、持续学习、能够使用工具等。因此，从第4章开始，我们描述了扩展LLMs能力并旨在解决LLMs问题的系统。
- en: In this chapter, we will discuss how some problems remain to be solved and what
    lies ahead in the future. We will start by presenting how agents can be used in
    different industries and the revolution that awaits us thanks to agents. Then,
    we will discuss some of the most pressing questions both technically and ethically.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一些问题尚未解决，以及未来将面临什么。我们将首先介绍代理在不同行业中的应用以及代理带来的革命。然后，我们将讨论一些在技术和伦理方面都十分紧迫的问题。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: AI agents in healthcare
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能代理在医疗保健领域
- en: AI agents in other sectors
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能代理在其他领域
- en: Challenges and open questions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挑战和开放性问题
- en: AI agents in healthcare
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能代理在医疗保健领域
- en: 'One of the most exciting prospects for AI development is the possibility of
    having autonomous systems capable of conducting scientific discoveries on their
    own. This new paradigm is referred to as the *AI scientist*. Throughout this book,
    we have seen some examples of systems that are thought to be in accordance with
    this idea (ChemCrow, the virtual lab, and so on). In this section, we will discuss
    this paradigm in more detail: where the research is heading, the challenges faced,
    and future developments.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能发展中最令人兴奋的前景之一是拥有能够自行进行科学发现的自主系统。这种新范式被称为“人工智能科学家”。在这本书的整个过程中，我们看到了一些被认为符合这一理念的系统示例（ChemCrow、虚拟实验室等）。在本节中，我们将更详细地讨论这一范式：研究将走向何方，面临的挑战以及未来的发展。
- en: The idea behind an AI agent is to exploit LLMs in combination with tools (agents),
    as we have seen so far. In the future, researchers would like to add an experimental
    platform (an autonomous system able to conduct experiments by itself) to these
    systems so that they can conduct experiments independently. The complexity of
    biology could then be approached in a series of actionable tasks, where an LLM
    could break down a problem into a series of subtasks and autonomously solve them.
    The goal then would be to achieve discoveries not only more quickly but also more
    efficiently. The AI scientist would then be able to produce research at a speed
    and scale that would otherwise be impossible for humans.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能代理背后的理念是利用我们迄今为止所看到的LLMs（大型语言模型）与工具（代理）的结合。在未来，研究人员希望向这些系统添加一个实验平台（一个能够自行进行实验的自主系统），以便它们能够独立进行实验。这样，就可以通过一系列可执行的任务来接近生物学的复杂性，其中LLM可以将问题分解成一系列子任务并自主解决它们。目标将是不仅更快，而且更有效地实现发现。这样，人工智能科学家就能够以人类无法达到的速度和规模产生研究成果。
- en: In the first phase, humans would be at the center of the project. Scientists
    would provide input and criticism to the LLMs, and the models would incorporate
    this feedback into the process. During this iterative process, the model would
    analyze the problem, search the internet for information, and devise a plan, under
    human supervision (or otherwise using handcrafted prompts to guide it through
    the process). In such a scenario, an LLM would be an assistant to humans, where
    it proposes solutions and hypotheses. The ultimate goal would be to have an autonomous
    agent.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段，人类将是项目的中心。科学家将为LLMs提供输入和批评，并将这些反馈纳入到过程中。在这个过程中，模型将分析问题，在人类监督下（或使用手工提示来引导其通过过程）搜索互联网上的信息，并制定计划。在这种场景中，LLM将是人类的助手，它提出解决方案和假设。最终目标是有望实现一个自主代理。
- en: 'This vision is the culmination of a process that has been ongoing for decades
    in biomedical research. In fact, since the early 1990s, people have been talking
    about a new paradigm: the use of data-driven models. This paradigm shift has occurred
    because of technological advances and the vast availability of data. Biomedical
    research produces a large amount of data, and in the last three decades, this
    information has begun to be centralized in a series of databases. Simultaneously
    with this integration and new accessibility of information, all sorts of tools
    have been developed by researchers. At first, these computational tools were models
    and statistical methods, but gradually, biomedical research has also benefited
    from machine learning and AI models. In a sense, the successes of one propelled
    the successes of the other, and vice versa. The more data was centralized and
    made available to the community, the more this allowed new models to be developed.
    Discoveries obtained through new models and methods prompted the production of
    new experiments and new data. For example, transcriptomics experiments allowed
    for large datasets, which were perfect for developing new machine learning models
    and tools. These models allowed some biological questions to be answered, and
    these answers led to new experiments and thus new data. AlphaFold2 was only possible
    because of the millions of structures on the **Protein Data Bank** (**PDB**).
    AlphaFold2 allowed researchers to produce new hypotheses, later confirmed by new
    experiments and new structures on the PDB. In addition, the limitations of AlphaFold2
    led researchers to collect new data for specific questions. These new data and
    experimental verifications led to new models, creating positive feedback.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个愿景是几十年来生物医学研究持续过程的结晶。事实上，自20世纪90年代初以来，人们一直在谈论一种新的范式：数据驱动模型的运用。这种范式转变是由于技术进步和数据的广泛可用性所引起的。生物医学研究产生了大量数据，在过去的三十年里，这些信息已经开始集中存储在一系列数据库中。随着这种整合和信息的新可访问性，研究人员开发出各种工具。最初，这些计算工具是模型和统计方法，但逐渐，生物医学研究也从机器学习和AI模型中受益。从某种意义上说，一个领域的成功推动了另一个领域的成功，反之亦然。数据越集中，对社区的可访问性越高，就越能促进新模型的开发。通过新模型和方法获得的新发现促使新的实验和新数据的产生。例如，转录组实验允许产生大型数据集，这对于开发新的机器学习模型和工具是完美的。这些模型允许回答一些生物学问题，而这些答案又导致了新的实验和新数据的产生。AlphaFold2之所以成为可能，仅仅是因为**蛋白质数据银行**（**PDB**）上数百万的结构。AlphaFold2允许研究人员提出新的假设，这些假设后来通过PDB上的新实验和新结构得到了证实。此外，AlphaFold2的局限性促使研究人员为特定问题收集新的数据。这些新数据和实验验证导致了新模型的出现，形成了正反馈。
- en: As you can see, when the LLMs arrived, fertile ground for further revolution
    was already present. First, a vast amount of data (millions of articles and huge
    databases of experimental data) was available, thus allowing models either to
    be trained on this data or to be able to search for information through dedicated
    databases. For example, a model could search for information it missed on biological
    sequences through dedicated APIs. Or, an LLM could use RAG to search for information
    on new articles. Second, the community produced thousands of models to solve specific
    tasks. An LLM does not need to know how to solve a task; there is a curated list
    of resources it can use for a whole range of subtasks. An LLM then does not need
    additional training but only needs to know how to orchestrate these specific task
    tools and models. At this point, we have everything we need to be able to create
    an agent system. Agents can be found at every step of the biomedical research
    process, thus enabling future drug development in a shorter time frame and saving
    important resources.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，当大型语言模型（LLMs）出现时，已经存在进一步革命肥沃的土壤。首先，大量数据（数百万篇文章和庞大的实验数据库）可用，从而允许模型在这份数据上进行训练，或者能够通过专用数据库搜索信息。例如，一个模型可以通过专用
    API 搜索它在生物序列中遗漏的信息。或者，一个 LLM 可以使用 RAG 搜索关于新文章的信息。其次，社区产生了数千个模型来解决特定任务。LLM 不需要知道如何解决一个任务；有一个经过精选的资源列表，它可以用于一系列子任务。因此，LLM
    不需要额外的训练，只需知道如何协调这些特定的任务工具和模型。到这一点，我们已经拥有了创建代理系统所需的一切。代理可以在生物医学研究过程的每个步骤中找到，从而在更短的时间内促进药物开发，并节省重要资源。
- en: '![Figure 11.1 – Empowering biomedical research with AI agents (https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5)](img/B21257_11_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 利用 AI 代理赋能生物医学研究 (https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5)](img/B21257_11_01.jpg)'
- en: Figure 11.1 – Empowering biomedical research with AI agents ([https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5](https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5))
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 利用 AI 代理赋能生物医学研究 ([https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5](https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5))
- en: Biomedical AI agents
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生物医学 AI 代理
- en: ChemCrow is an example of this type of agent, defined for a specific case and
    domain. The reasoning of the system is limited to the specific tasks; the agent
    must use the experimental data and existing knowledge. It is the researcher who
    defines both the hypothesis and the tasks; the system only has to complete them.
    Level 1 can be considered orchestrators under the supervision of a human being.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ChemCrow 是这类代理的一个例子，它针对特定的案例和领域进行了定义。该系统的推理仅限于特定任务；代理必须使用实验数据和现有知识。假设和任务都是由研究人员定义的；系统只需完成它们。第
    1 级可以在人类监督下被视为协调者。
- en: 'For example, ChemCrow has demonstrated concrete outcomes in research automation:
    according to a study published in *Nature Machine Intelligence*, ChemCrow autonomously
    planned and executed the synthesis of an insect repellent and three organocatalysts,
    and guided the screening and synthesis of a novel chromophore (*Nature Machine
    Intelligence*, 2024). Additionally, by integrating 18 specialized tools, ChemCrow
    has streamlined complex chemical research processes, significantly increasing
    efficiency and accessibility for both expert and non-expert users (*ScienceDaily*,
    2024).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，ChemCrow 在研究自动化方面已经取得了具体的成果：根据发表在 *自然机器智能* 上的研究，ChemCrow 自动规划和执行了昆虫驱避剂和三种有机催化剂的合成，并指导了新型发色团（*自然机器智能*，2024）的筛选和合成。此外，通过整合
    18 个专业工具，ChemCrow 简化了复杂的化学研究流程，显著提高了专家和非专家用户的工作效率和可及性（*科学日报*，2024）。
- en: Most agent approaches are based on the use of a central LLM. An LLM is pre-trained
    with general knowledge and then aligned to human preferences to make the most
    of its knowledge and the skills it has learned during pre-training. The biomedical
    field requires specialized expertise and knowledge. Therefore, various experiments
    have often been conducted where an LLM has been fine-tuned to specialize in medicine
    (e.g., BioGPT, NYUTron, and MedPalm). This approach is clearly expensive, and
    a model becomes outdated quickly (thousands of papers are published every day).
    So, different approaches have been sought in which it is not necessary to conduct
    repeated rounds of fine-tuning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数代理方法都是基于使用一个中心LLM。LLM经过预训练，具有一般知识，然后与人类偏好对齐，以充分利用其知识和在预训练期间学习到的技能。生物医学领域需要专业知识和技能。因此，经常进行各种实验，其中LLM被微调以专门从事医学（例如，BioGPT、NYUTron和MedPalm）。这种方法显然成本高昂，并且模型很快就会过时（每天有数千篇论文发表）。因此，寻求不同的方法，在这些方法中不需要进行重复的微调。
- en: 'One option is to try and use one model (one LLM) but with different professional
    expertise (assigning a specific role at each round). The idea is to use one model,
    but craft prompts to assign a role to the LLM (biologist, clinician, chemist,
    and so on). There are also other alternatives, for example, using instruction
    tuning to create an expert for a domain (so rather than aligning the model on
    specific knowledge, align it on specific tasks that would be an expert’s). For
    example, we can ask a model to perform a task (*Write a sequence for a protein
    X that has a function Y*) or provide it with a specific role (*You are a biologist
    specializing in proteomics; your task is: write a sequence for a protein X that
    has a function Y)*. A complex task can be performed by more than one specialist;
    for example, we can provide the model with the task directly (*Identify a gene
    involved in the interaction of the Covid virus with a respiratory cell; design
    an antibody to block it*) or break it down into several subsequent tasks (a first
    task with a first role such as *You are a professional virologist with expertise
    on the Covid19 virus; your task is: identify a gene involved in the interaction
    of the Covid virus with a respiratory cell* and then assign the model a second
    task: *You are a computational immunologist with expertise in designing blocking
    antibodies; your task is: design an antibody to block it*). In contrast to the
    previous approach to learning, a methodology (solving tasks) does not quickly
    become outdated like domain knowledge. Other authors suggest that one can instead
    simply use in-context learning.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是尝试使用一个模型（一个LLM），但具有不同的专业领域（在每一轮分配一个特定的角色）。想法是使用一个模型，但设计提示来分配LLM的角色（生物学家、临床医生、化学家等）。还有其他替代方案，例如，使用指令微调来创建一个特定领域的专家（因此，与其将模型与特定知识对齐，不如将其与专家会执行的具体任务对齐）。例如，我们可以要求模型执行一个任务（*为具有功能Y的蛋白质X编写一个序列*）或提供特定的角色（*你是专门研究蛋白质组学的生物学家；你的任务是：为具有功能Y的蛋白质X编写一个序列*）。一个复杂任务可以由多个专家完成；例如，我们可以直接向模型提供任务（*识别与呼吸道细胞相互作用的新冠病毒基因；设计抗体来阻断它*）或将它分解成几个后续任务（第一个任务具有第一个角色，例如*你是具有Covid19病毒专业知识的专业病毒学家；你的任务是：识别与呼吸道细胞相互作用的新冠病毒基因*，然后分配模型第二个任务：*你是具有设计阻断抗体专业知识的计算免疫学家；你的任务是：设计抗体来阻断它*）。与之前的学习方法相比，这种方法（解决任务）不会像领域知识那样迅速过时。其他作者建议，人们可以简单地使用上下文学习。
- en: 'This strategy means providing the model in context with a whole range of information
    that would be needed to play the role of a specialist (specific information about
    the role the model is to impersonate: definition, skills, specific knowledge,
    and so on). This strategy is very similar to assigning a role by prompt, but we
    give much more information. Although these prompts are full of information and
    instructions, the model does not always follow them. Also, it is difficult to
    describe in a prompt what a specialist’s role is. So, an additional strategy is
    that the model independently generates and refines the role prompt.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略意味着在上下文中向模型提供一系列所需的信息，以便扮演专家的角色（关于模型将要模仿的角色具体信息：定义、技能、特定知识等）。这种策略与通过提示分配角色的方法非常相似，但我们提供了更多的信息。尽管这些提示充满了信息和指令，但模型并不总是遵循它们。此外，在提示中描述专家的角色也很困难。因此，我们采取的另一种策略是模型独立生成和细化角色提示。
- en: Agents may therefore have different tools at their disposal and different purposes.
    The rationale for this multi-role approach is that an LLM does not have a deep
    understanding of planning and reasoning but still shows acquired skills. So, instead
    of one agent having to handle the whole process, we have a pool of agents where
    each agent has to take care of a limited subtask. Typically, in addition to the
    definition of different types of agents, there is also the definition of working
    protocol (for example, in the virtual lab, in addition to agents, a protocol of
    team and individual meetings was defined).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，智能体可能具有不同的工具和不同的目的。这种多角色方法的理由是，大型语言模型对规划和推理没有深入的理解，但仍然显示出获得的能力。因此，而不是一个智能体必须处理整个流程，我们有一个智能体池，其中每个智能体必须负责一个有限的子任务。通常，除了定义不同类型的智能体外，还需要定义工作协议（例如，在虚拟实验室中，除了智能体外，还定义了团队和个人会议的协议）。
- en: In any case, although there is so much expectation about a multi-agent approach
    where there is an LLM acting with several people, some studies give mixed results.
    In fact, some authors say that what are formally called “personas” (assigning
    a role to an LLM) do not give a particular advantage, except in rare cases. In
    any case, to date, it is necessary for these prompts to be precisely designed
    to be effective (and it is a laborious, trial-and-error process).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，尽管人们对一个大型语言模型与几个人一起工作的多智能体方法抱有很大期望，但一些研究给出了混合的结果。事实上，一些作者表示，形式上称为“角色”（为大型语言模型分配角色）的东西并不提供特定的优势，除非在罕见的情况下。无论如何，到目前为止，有必要精确设计这些提示以使其有效（这是一个费时且反复试验的过程）。
- en: Since LLMs have good critical thinking skills, it has been suggested that they
    can be used in brainstorming. Although LLMs have no reasoning skills and limited
    creativity, they can conduct a quick survey of the literature. Agents can then
    be used to propose ideas, evaluate the best, refine and prioritize, provide critique,
    and discuss feasibility. One interesting possibility is to use a pool of agents
    where each agent has different expertise, which mimics the brainstorming discussion
    process.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大型语言模型具有良好的批判性思维能力，因此有人建议它们可以用于头脑风暴。尽管大型语言模型没有推理技能和有限的创造力，但它们可以快速浏览文献。然后可以使用智能体提出想法、评估最佳方案、细化并优先排序、提供批判性意见以及讨论可行性。一个有趣的可能性是使用一个智能体池，其中每个智能体具有不同的专业知识，这模仿了头脑风暴讨论过程。
- en: Different frameworks can be created where agents interact with humans or with
    each other. For example, leveraging critique capabilities can facilitate the creation
    of agents with distinct goals to foster debate. One group of agents could focus
    on critiquing and challenging ideas, while another could aim to persuade and advocate
    for their viewpoints. Each agent could have different expertise and have different
    tools at their disposal. This approach, therefore, evaluates a research proposition
    from different perspectives. A research idea can then be viewed as an optimization
    problem where agents try to arrive at the best solution. In addition to a setting
    where agents are competing, the possibility of cooperation can also be exploited.
    Agents provide feedback sequentially on a proposition with the purpose of improving
    an idea. The two frameworks are not necessarily opposites but can be reconciled
    in systems where each idea goes through feedback loops and critique. Because the
    frameworks are organized with natural language prompts, multi-agent systems provide
    unique flexibility.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可以创建不同的框架，其中智能体可以与人类或其他智能体进行交互。例如，利用批判能力可以促进具有不同目标的智能体的创建，以促进辩论。一组智能体可能专注于批判和挑战观点，而另一组可能旨在说服并为其观点辩护。每个智能体可能具有不同的专业知识和可用的不同工具。因此，这种方法从不同的角度评估研究命题。然后可以将研究想法视为一个优化问题，其中智能体试图找到最佳解决方案。除了智能体竞争的设置外，还可以利用合作的可能性。智能体可以对一个命题进行顺序反馈，目的是改进一个想法。这两个框架不一定是对立的，但可以在每个想法都经过反馈循环和批判的系统中进行调和。由于框架是用自然语言提示组织的，因此多智能体系统提供了独特的灵活性。
- en: Similarly, it is not necessary that all agents be equal peers; hierarchical
    levels can be organized. For example, one agent may have the role of facilitating
    discussion or having greater decision-making weight. In the virtual lab, there
    is an agent that has the role of principal investigator, which initiates the discussion
    and has decision-making power. Thus, multiple decision-making levels can be instituted
    that are managed by sophisticated architecture.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，并非所有代理都必须是平等的伙伴；可以组织层级结构。例如，一个代理可能扮演促进讨论或拥有更大决策权的作用。在虚拟实验室中，有一个代理扮演主要研究员的角色，负责发起讨论并拥有决策权。因此，可以建立多个决策层级，这些层级由复杂的架构管理。
- en: Note that agents can then design experiments and, conjugated with experiential
    tools, these experiments can be accomplished. This would provide a new level of
    capability, toward a process that becomes end to end.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，代理可以设计实验，并且结合经验工具，这些实验可以完成。这将提供一个新的能力水平，使整个过程成为端到端。
- en: 'In this regard, Gao ([https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5](https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5))
    defined three levels of autonomy for an agent system in biomedical research:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，高([https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5](https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5))为生物医学研究中的代理系统定义了三个自主级别：
- en: '**Level 0**: A machine learning model is used as a tool by a researcher. The
    researcher defines the hypothesis, uses the model for a specific task, and evaluates
    the output. *Level 0* systems are tools such as models for making predictions
    in the biological field.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**级别 0**：机器学习模型被研究者用作工具。研究者定义假设，使用模型完成特定任务，并评估输出。*级别 0* 系统是诸如在生物领域进行预测的模型之类的工具。'
- en: '**Level 1**: This is also defined as *AI agent as a research assistant*; the
    researcher defines a hypothesis, specifies the tasks that need to be conducted
    to get to the goal, and the agent uses a restricted set of tools. ChemCrow is
    an example of this type of agent, defined for a specific case and domain. The
    reasoning of the system is limited to the specific tasks; the agent must use the
    experimental data and existing knowledge. It is the researcher who defines both
    the hypothesis and the tasks; the system only has to complete them. *Level 1*
    can be considered orchestrators under the supervision of a human being.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**级别 1**：这也可以定义为 *AI 代理作为研究助理*；研究者定义假设，指定需要执行以达成目标的任务，代理使用一组受限的工具。ChemCrow
    是这种类型代理的一个例子，它针对特定案例和领域进行定义。系统的推理局限于特定任务；代理必须使用实验数据和现有知识。定义假设和任务的是研究者；系统只需完成它们。*级别
    1* 可以被认为是人类监督下的协调者。'
- en: '**Level 2**: Also referred to as *AI agent as a collaborator*, the system helps
    a researcher redefine the hypothesis thanks in part to its large set of tools.
    Despite its contribution to the hypothesis, its ability to understand scientific
    phenomena and generate innovative hypotheses remains limited. What differentiates
    it from *Level 1* is participation in hypothesis improvement and task definition
    to test it.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**级别 2**：也称为 *AI 代理作为合作者*，系统帮助研究者重新定义假设，部分得益于其庞大的工具集。尽管它对假设有所贡献，但其理解科学现象和生成创新假设的能力仍然有限。与
    *级别 1* 区别开来的是，它参与假设改进和任务定义以测试它们。'
- en: '**Level 3**: This is the last level and is defined as *AI agent as a scientist*.
    In this case, an agent must be able to develop and extrapolate novel hypotheses
    and define links between findings that cannot be inferred solely from the literature.
    A *Level 3* agent then collaborates as an equal to a researcher or can propose
    hypotheses on its own, defines tasks to test hypotheses, and completes them.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**级别 3**：这是最后一个级别，定义为 *AI 代理作为科学家*。在这种情况下，代理必须能够发展和外推新的假设，并定义无法仅从文献中推断出的发现之间的联系。一个
    *级别 3* 代理随后可以与研究者平等合作，或者可以自行提出假设，定义测试假设的任务，并完成它们。'
- en: To date, we have no agents beyond *Level 1*, and we will probably need new architectures
    and training systems for *Levels 2* and *3*. *Level 0* is then a set of tools
    that are used by researchers but lack any autonomy. A *Level 1* agent can write
    code to conduct a bioinformatics analysis to process data, conduct statistical
    analysis, or use other tools. A *Level 1* agent uses *Level 0* tools to carry
    out these tasks, allowing it to test a hypothesis. A *Level 2* agent should not
    just perform narrow tasks on human indications but should be able given an initial
    hypothesis to refine it, decide, and perform tasks autonomously. We expect a *Level
    2* agent, after being given the hypothesis, to be able to also refine experiments,
    and then critically evaluate to maximize a goal. A *Level 3* agent, on the other
    hand, should collaborate with humans to generate hypotheses, and can practically
    be considered its peer. A *Level 3* agent should be able to evaluate existing
    challenges and anticipate future research directions. In addition, a *Level 3*
    agent should integrate with experimental platforms to be able to conduct the entire
    process end to end.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们没有超越*第1级*的代理，我们可能需要为*第2级*和*第3级*开发新的架构和训练系统。*第0级*是一组研究人员使用的工具，但缺乏任何自主性。一个*第1级*代理可以编写代码进行生物信息学分析以处理数据，进行统计分析或使用其他工具。一个*第1级*代理使用*第0级*工具来完成这些任务，从而允许它测试一个假设。一个*第2级*代理不应仅仅执行人类指示的狭窄任务，而应该能够在给定初始假设的情况下对其进行细化、做出决定并自主执行任务。我们预计，在给出假设后，*第2级*代理还能够细化实验，并批判性地评估以最大化目标。另一方面，*第3级*代理应与人类合作生成假设，实际上可以被视为其同伴。*第3级*代理应能够评估现有挑战并预测未来的研究方向。此外，*第3级*代理应与实验平台集成，以便能够从头到尾执行整个过程。
- en: AI agents in other sectors
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他领域的AI代理
- en: In this section, we will discuss how LLM agents are having and will have a global
    impact across a range of industries.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论LLM代理如何以及将在各个行业中产生和产生全球影响。
- en: Physical agents
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 物理代理
- en: Physical AI agents (for example, robots) are LLM agents that are capable of
    navigating the real world and performing actions. Thus, they can be considered
    systems that are embodied and integrate AI with the physical world. LLMs in these
    systems provide the backbone for reasoning and contextual understanding. On this
    backbone, other modules such as memory, additional skills, and tools can be added.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 物理AI代理（例如，机器人）是能够导航现实世界并执行动作的LLM代理。因此，它们可以被视为将AI与物理世界结合在一起的具身系统。这些系统中的LLM为推理和上下文理解提供基础。在此基础上，可以添加其他模块，如记忆、额外技能和工具。
- en: '![Figure 11.2 – LLM-based agent (https://arxiv.org/pdf/2501.08944v1)](img/B21257_11_02.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 基于LLM的代理 (https://arxiv.org/pdf/2501.08944v1)](img/B21257_11_02.jpg)'
- en: Figure 11.2 – LLM-based agent ([https://arxiv.org/pdf/2501.08944v1](https://arxiv.org/pdf/2501.08944v1))
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 基于LLM的代理 ([https://arxiv.org/pdf/2501.08944v1](https://arxiv.org/pdf/2501.08944v1))
- en: Unlike a virtual agent, a physical AI agent must also understand and adapt to
    physical dynamics such as gravity, friction, and inertia. Being able to understand
    physical laws allows it to be able to navigate the environment and perform tasks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与虚拟代理不同，物理AI代理还必须理解和适应物理动态，如重力、摩擦和惯性。能够理解物理定律使其能够导航环境并执行任务。
- en: 'There are several advantages to using an LLM for a physical agent:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM作为物理代理有几个优点：
- en: '**Human interaction**: LLMs allow humans to interact more easily through the
    use of natural language. In addition, the use of LLMs allows for better communication
    and better management of emotions, allowing for easier acceptance. Likewise, people
    are already accustomed to collaboration with LLMs, thus predisposing users to
    collaborate more easily with robots to solve problems, generate plans, and perform
    tasks.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人机交互**：LLM允许人们通过自然语言的使用更轻松地互动。此外，LLM的使用促进了更好的沟通和情绪管理，使得接受度更高。同样，人们已经习惯了与LLM协作，因此用户更容易与机器人协作解决问题、生成计划并执行任务。'
- en: '**Flexibility and adaptation**: LLMs today are multi-purpose with generalist
    capabilities, which allows them to adapt more easily to different tasks and circumstances.
    In addition, for specific tasks and environments, LLMs can be fine-tuned to acquire
    new skills and knowledge needed to operate in different environments. LLMs also
    have reasoning skills and the ability to find information; this knowledge and
    these skills acquired during pre-training can be used to solve tasks for which
    they were not programmed. In addition, LLMs can be guided to perform a task through
    natural language, making it easy to explain to robots the tasks they need to accomplish.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活性和适应性**：今天的LLM具有多功能和通用能力，这使得它们更容易适应不同的任务和环境。此外，对于特定任务和环境，LLM可以被微调以获取在不同环境中操作所需的新技能和知识。LLM还具有推理能力和查找信息的能力；在预训练期间获得的知识和技能可以用来解决它们未被编程的任务。此外，LLM可以通过自然语言被引导执行任务，这使得向机器人解释它们需要完成的任务变得容易。'
- en: '**Multimodal capabilities**: Today, several LLMs are capable of taking different
    types of modalities as input. This capability allows them to integrate information
    from different types of sensors, so they can understand their surroundings.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多模态能力**：如今，多个大型语言模型（LLM）能够接受不同类型的模态作为输入。这种能力使它们能够整合来自不同类型传感器的信息，从而理解它们所处的环境。'
- en: In recent years, the idea of combining LLMs and robots has already been explored.
    For example, PaLM-SayCan was an experiment in which they used Google PaLM to command
    a robot. Later, Google used a PaLM-E model, which is itself multimodal. In addition,
    new alternatives are being tested today in which **reinforcement learning** (**RL**)
    is used to improve the interaction of LLMs with the environment.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，将LLM与机器人结合的想法已经被探索。例如，PaLM-SayCan是一个实验，其中他们使用Google PaLM来控制机器人。后来，谷歌使用了PaLM-E模型，它本身就是一个多模态模型。此外，今天正在测试新的替代方案，其中使用**强化学习**（**RL**）来改善LLM与环境的交互。
- en: 'Several challenges remain at present for robots controlled by LLMs:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，由LLM控制的机器人仍存在一些挑战：
- en: '**Datasets and training**: LLMs require extensive training with large amounts
    of data. Collecting these datasets is not easy; to date, there are no quality
    datasets to train a robot in an environment (datasets that require large amounts
    of images and text). A robot would have to be trained with task descriptions and
    how to perform them, making it expensive to acquire these multimodal datasets.
    Using RL requires that you acquire datasets in which you have information about
    the actions taken by the system and the effect on the environment. Datasets used
    for one task may not be useful for training in another. For example, a dataset
    used for training a dog robot cannot be used for training a humanoid robot). Robot
    training requires interaction with the environment; this is a laborious and time-consuming
    process. Efforts are being made to overcome this problem with the use of games
    and simulations. However, this alternative is a simplification of the real environment
    and may not be enough.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集和训练**：LLM需要使用大量数据进行广泛的训练。收集这些数据集并不容易；迄今为止，还没有用于在环境中训练机器人的高质量数据集（需要大量图像和文本的数据集）。机器人必须通过任务描述和执行方式来训练，这使得获取这些多模态数据集变得昂贵。使用RL需要你获取有关系统采取的行动和环境影响的数据库。用于一个任务的数据集可能对另一个任务的训练没有用。例如，用于训练狗机器人的数据集不能用于训练类人机器人）。机器人训练需要与环境的交互；这是一个费时费力的过程。人们正在努力通过游戏和模拟来克服这个问题。然而，这种替代方案是对真实环境的简化，可能不足以满足需求。'
- en: '**Structure of the robot**: A robot can be of an arbitrary shape. Today, motion
    robots are designed with human shape, but this is not strictly necessary. In fact,
    robots for particular applications might have different shapes. For example, a
    robot thought of as a chef might have a better shape if designed for its specific
    environment.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器人的结构**：机器人可以是任意形状。今天，运动机器人被设计成人类形状，但这并不是严格必要的。实际上，特定应用的机器人可能会有不同的形状。例如，如果一个机器人被设计成厨师，那么如果它被设计成适应其特定环境，它可能会有一个更好的形状。'
- en: '**Deployment of the LLM**: The optimum in these systems is to place an LLM
    inside the robot. Deployment inside the robot is one of the limitations of current
    LLMs. Many LLMs require considerable hardware resources (different GPUs for a
    single LLM), which makes deployment inside a local brain not feasible. In contrast,
    today, the robot’s brain resides in the cloud. This obviously has several limitations,
    especially when there is signal loss.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM的部署**：在这些系统中，最佳方案是将LLM放置在机器人内部。将LLM部署在机器人内部是当前LLM的一个限制。许多LLM需要相当多的硬件资源（单个LLM需要不同的GPU），这使得在本地大脑中部署LLM不可行。相比之下，今天，机器人的大脑位于云端。这显然有几个限制，尤其是在信号丢失的情况下。'
- en: '**Security**: LLMs have biases and misconceptions that result from pre-training.
    In addition, LLMs can also hallucinate or commit errors. These factors can manifest
    themselves in errors while performing a task. An LLM who can control physical
    actions could then cause harm. For example, a robot could burn down a house while
    cooking. At the same time, LLMs can be hacked, posing the risk of private data
    leakage or intentional damage.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：LLM由于预训练而产生的偏差和误解。此外，LLM也可能产生幻觉或犯错误。这些因素在执行任务时可能导致错误。一个能够控制物理动作的LLM可能会造成伤害。例如，机器人可能在烹饪时烧毁房屋。同时，LLM也可能被黑客攻击，存在私人数据泄露或故意破坏的风险。'
- en: '![Figure 11.3 – Challenge in embodied intelligence (https://arxiv.org/pdf/2311.07226)](img/B21257_11_03.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3 – 实体智能的挑战](https://arxiv.org/pdf/2311.07226)(img/B21257_11_03.jpg)'
- en: Figure 11.3 – Challenge in embodied intelligence ([https://arxiv.org/pdf/2311.07226](https://arxiv.org/pdf/2311.07226))
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – 实体智能的挑战([https://arxiv.org/pdf/2311.07226](https://arxiv.org/pdf/2311.07226))
- en: LLM agents for gaming
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 游戏中的LLM代理
- en: LLM-based AI agents for gaming are another interesting frontier, where the reasoning
    capabilities of the model are used to interact with the environment (the game).
    In general, a framework dedicated to gaming requires a set of components such
    as an LLM, memory, and tools to interact with the game. Often, the system is trained
    using RL (where a game is an episode). An LLM can then analyze the moves conducted
    in previous games and reason about what the best action is.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的AI代理在游戏领域是另一个有趣的领域，其中模型的推理能力被用来与环境（游戏）交互。一般来说，一个专门用于游戏的框架需要一套组件，如LLM、内存以及与游戏交互的工具。通常，系统使用RL（其中游戏是一个场景）进行训练。然后，LLM可以分析之前游戏中进行的动作，并推理出最佳行动。
- en: '![Figure 11.4 – Overall framework for LLM-based game (https://arxiv.org/pdf/2404.02039)](img/B21257_11_04.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4 – 基于LLM的游戏整体框架](https://arxiv.org/pdf/2404.02039)(img/B21257_11_04.jpg)'
- en: Figure 11.4 – Overall framework for LLM-based game ([https://arxiv.org/pdf/2404.02039](https://arxiv.org/pdf/2404.02039))
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 – 基于LLM的游戏整体框架([https://arxiv.org/pdf/2404.02039](https://arxiv.org/pdf/2404.02039))
- en: Especially today, many games are quite complex and there is sophisticated interaction
    with the environment and other characters. An LLM can then reason about the richness
    of textual information (object descriptions, task descriptions, dialogues with
    characters, etc.) to decide on a plan of action or strategy. For example, in Pokémon
    battles, each player has several Pokémon of different species. Each species has
    different abilities and statistics; knowledge of the game is necessary in order
    to win a battle. Using an LLM can allow you to leverage the model’s implicit knowledge
    to be able to select an effective strategy (such as using an electric attack does
    not bring damage to a ground-type Pokémon). In addition, an LLM can exploit techniques
    such as chain-of-thought (CoT) to integrate different elements into action choices
    (especially if it has to think several moves ahead).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是今天，许多游戏相当复杂，与环境和其他角色有复杂的交互。LLM可以据此推理文本信息的丰富性（对象描述、任务描述、与角色的对话等），以决定行动计划或策略。例如，在宝可梦战斗中，每个玩家都有几个不同种类的宝可梦。每种种类都有不同的能力和统计数据；了解游戏知识是赢得战斗的必要条件。使用LLM可以让你利用模型的隐含知识来选择有效的策略（例如，使用电攻击对地面类型的宝可梦不会造成伤害）。此外，LLM可以利用如思维链（CoT）等技术将不同元素整合到行动选择中（特别是如果它需要提前几步思考）。
- en: '![Figure 11.5 – Use of semantic knowledge for devising an effective strategy
    (https://arxiv.org/pdf/2404.02039)](img/B21257_11_05.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5 – 使用语义知识制定有效策略](https://arxiv.org/pdf/2404.02039)(img/B21257_11_05.jpg)'
- en: Figure 11.5 – Use of semantic knowledge for devising an effective strategy ([https://arxiv.org/pdf/2404.02039](https://arxiv.org/pdf/2404.02039))
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 – 利用语义知识制定有效策略 ([https://arxiv.org/pdf/2404.02039](https://arxiv.org/pdf/2404.02039))
- en: LLM-based agents are an interesting prospect for the game because they could
    enrich the players’ experience. For example, LLMs could create characters who
    discuss more naturally with players, provide hints during the game, cooperate
    with them, and guide them through the adventure. Or they could be used to generate
    antagonists that are more complex and match the player’s level.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的代理对游戏来说是一个有趣的展望，因为它们可以丰富玩家的体验。例如，LLM可以创建与玩家更自然交流的角色，在游戏过程中提供提示，与他们合作，并引导他们完成冒险。或者，它们可以用来生成更复杂且与玩家水平相匹配的对手。
- en: Web agents
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络代理
- en: Web agents are AI agents designed explicitly to interact with the web and assist
    humans in tedious and repetitive tasks. Thus, the purpose of these agents is to
    automate these tasks and improve productivity and efficiency. Again, the brain
    is an LLM, which allows reasoning and task understanding to be conducted. The
    architecture of a web agent is similar to that seen in this book. A web agent
    has a module dedicated to perception (input from the web), reasoning (LLM), and
    a module dedicated to interaction with the web. The perception module requires
    interaction with the web via either HTML (text-based agents that read the HTML
    document and process it) or via screenshots of websites (use of multimodal LLMs).
    Once an LLM receives a task, it can then browse the web, schedule subtasks, retrieve
    information from memory, and execute the plan.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 网络代理是专门设计用来与网络交互并协助人类完成繁琐和重复性任务的AI代理。因此，这些代理的目的在于自动化这些任务，提高生产力和效率。再次强调，大脑是一个LLM，它允许进行推理和任务理解。网络代理的架构与本书中看到的类似。网络代理包含一个专门用于感知（来自网络的输入）、推理（LLM）以及一个专门用于与网络交互的模块。感知模块需要通过HTML（读取并处理HTML文档的基于文本的代理）或通过网站截图（使用多模态LLM）与网络进行交互。一旦LLM接收到一个任务，它就可以浏览网络、安排子任务、从记忆中检索信息，并执行计划。
- en: '![Figure 11.6 – Web agent framework (https://arxiv.org/pdf/2503.23350)](img/B21257_11_06.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图11.6 – 网络代理框架 (https://arxiv.org/pdf/2503.23350)](img/B21257_11_06.jpg)'
- en: Figure 11.6 – Web agent framework ([https://arxiv.org/pdf/2503.23350)](https://arxiv.org/pdf/2503.23350))
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 – 网络代理框架 ([https://arxiv.org/pdf/2503.23350](https://arxiv.org/pdf/2503.23350))
- en: AI agents are a new frontier for AI, one that is poised to have a rapid practical
    impact. Despite their potential, several challenges and issues remain, which we
    will address in the next section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: AI代理是AI的一个新前沿，它有望迅速产生实际影响。尽管它们具有潜力，但仍然存在一些挑战和问题，我们将在下一节中讨论。
- en: Challenges and open questions
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战和开放性问题
- en: In this section, we will address several open questions about both agents and
    the capabilities of LLMs. Despite advances in the field, several points remain
    to be resolved for the safe use of AI agents.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论关于代理和LLM能力的一些开放性问题。尽管该领域取得了进展，但仍有几个问题需要解决，以确保AI代理的安全使用。
- en: Challenges in human-agent communication
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人机通信的挑战
- en: 'Once they are deployed in the real world, agents can perform actions that lead
    to problematic failures. For example, a shopping agent might spend money unexpectedly
    or inadvertently leak sensitive information. Coding agents might execute or produce
    viruses, delete important files, or push repositories into production that are
    full of bugs. Communication with the user is key to avoiding such problems. The
    use of agents should be based on two key principles: transparency and control.
    Indeed, there must be an alignment between the user’s goals and the agent’s behavior;
    the user must then be able to control the process and have access to its progress.
    Communication between humans and agents allows us to advance these two principles,
    but some open challenges remain.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦它们在现实世界中部署，代理可以执行可能导致问题性失败的行为。例如，购物代理可能会意外地花费金钱或无意中泄露敏感信息。编码代理可能会执行或产生病毒，删除重要文件，或将充满错误的代码库推入生产。与用户的沟通是避免此类问题的关键。代理的使用应基于两个关键原则：透明度和控制。确实，用户的目标和代理的行为之间必须保持一致；用户必须能够控制过程并访问其进度。人类与代理之间的沟通使我们能够推进这两个原则，但仍然存在一些开放性的挑战。
- en: Modern agents are not yet completely perfect and can make mistakes (especially
    for goals that are complex or include several steps). Therefore, it is important
    that we can verify the agent’s behavior, both the result of its work and that
    it has understood the task. Therefore, a way must be found to verify that the
    agent has understood the goal and that its plan and actions are directed toward
    this goal. Verifying that the agent has truly understood the goal allows us to
    avoid costly errors and save computation and time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现代代理还不够完美，可能会犯错（尤其是对于复杂或包含多个步骤的目标）。因此，我们能够验证代理的行为，包括其工作结果以及它是否理解了任务，这一点非常重要。因此，必须找到一种方法来验证代理是否真正理解了目标，以及其计划和行动是否针对这个目标。验证代理真正理解了目标可以让我们避免代价高昂的错误，并节省计算和时间。
- en: In addition, LLMs have a component that is stochastic. This component arises
    from the probabilistic nature of the model output functions (stochastic decoding)
    and the complex natures of interactions that can evolve during the task (unanticipated
    events). Therefore, the output and behavior of the model may not be consistent.
    Even in a deterministic setting (temperature 0), changes in the environment during
    task execution may lead to unexpected or unintended results. Inconsistencies may
    also emerge from the outdated model knowledge or imperfect world model present
    within an LLM. For example, an agent might buy an item that is out of budget or
    different from a user’s needs, due to misalignment of its knowledge of the real
    world.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，LLMs 有一个随机组件。这个组件源于模型输出函数的概率性质（随机解码）以及任务执行过程中可能出现的复杂交互性质（意外事件）。因此，模型的结果和行为可能不一致。即使在确定性设置（温度
    0）中，任务执行期间环境的变化可能会导致意外或不希望的结果。不一致性也可能来自LLM中过时的模型知识或存在的不完美的世界模型。例如，由于代理对现实世界的知识不匹配，它可能会购买超出预算或不符合用户需求的物品。
- en: Similarly, interactions with the user and the outside world generate a great
    deal of information. This broad context is important for directing the agent’s
    behavior, which can then be learned from past interactions. Although this context
    is fundamental to being able to perform the task effectively, it risks becoming
    far too wide and manageable over time. At the same time, modern LLMs have a noise
    problem and struggle to find relevant information when it is scattered in unnecessary
    detail. Therefore, effective ways must be found for an agent to focus on the relevant
    part of the last interaction with the user. Also, some of the information should
    not be able to be reused (privacy and ethical concerns), so one would need to
    find an easy way to manage, edit, and remove the past information.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，与用户和外部世界的交互会产生大量信息。这个广泛的背景对于指导代理的行为非常重要，这些行为随后可以从过去的交互中学习。尽管这个背景对于有效地执行任务至关重要，但它可能会随着时间的推移变得过于宽泛且难以管理。同时，现代大型语言模型存在噪声问题，当相关信息散布在不必要的细节中时，它们很难找到相关信息。因此，必须找到有效的方法，使代理能够专注于与用户最后一次交互的相关部分。此外，一些信息不应能够被重复使用（隐私和伦理问题），因此需要找到一种简单的方法来管理、编辑和删除过去的信息。
- en: 'What we have discussed are the general challenges of user-agent communication.
    We can also define open challenges that are in the communication between user
    and agent, and vice versa. First, we need to make sure that we can design agents
    that enable effective communication by the user by addressing these points:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所讨论的是用户代理通信的一般挑战。我们还可以定义用户与代理之间以及反之亦然的开放挑战。首先，我们需要确保我们可以设计出通过解决这些点来使用户能够有效沟通的代理：
- en: '**Clear goal acquisition**: The focus of the system is for the agent to understand
    the goal and for the user to be able to provide it clearly. To avoid costly mistakes,
    we need to design agents for which users can define goals unambiguously. Some
    possibilities have been studied in some areas: sets of logical rules and the use
    of formal languages. To make this technology usable for everyone, we need to use
    natural language. Natural language is rich in both nuance and ambiguity, however,
    and allows complex goals to be defined with vague and incomplete definitions.
    Hence, mechanisms must be defined to disambiguate unclear goals or allow the agent
    to infer from context (or past interactions).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**明确的目标获取**：系统的重点是让代理理解目标，并让用户能够清楚地提供目标。为了避免代价高昂的错误，我们需要设计用户可以明确定义目标的代理。一些可能性已经在某些领域进行了研究：逻辑规则集和形式语言的使用。为了使这项技术对每个人都是可用的，我们需要使用自然语言。自然语言在细微差别和歧义性方面都很丰富，它允许使用模糊和不完整的定义来定义复杂的目标。因此，必须定义机制来消除不明确的目标或允许代理从上下文（或过去的交互）中推断。'
- en: '**Respect for user preferences**: One can achieve a goal with several paths,
    but some are more optimal than others (both for efficiency and for respecting
    a user’s preferences). User preferences may not be aligned with LLM values (during
    post-training, the model is aligned with human preferences, which do not necessarily
    reflect the preferences of a general user but only of a selected pool of annotators).
    For example, if a user requests a route, they may prefer a more eco-friendly means
    of transportation. The agent should adhere to these preferences when possible,
    or interrupt the process to inform the user when it cannot. Model alignment may
    be one possible approach to take user preferences into account. However, current
    alignment approaches primarily consider aggregate preferences, and methods for
    accommodating individual preferences remain undeveloped. In more general terms,
    an agent can also achieve a goal by generating harm (even in an unintended way),
    and this risk is greater if it has the ability to use tools.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尊重用户偏好**：可以通过多种路径实现目标，但其中一些比其他路径更优（既有效率，也尊重用户的偏好）。用户的偏好可能与LLM的价值观不一致（在训练后，模型与人类偏好一致，但这并不一定反映普通用户的偏好，而只是反映了一组选定的注释者的偏好）。例如，如果用户请求一条路线，他们可能更喜欢更环保的交通工具。代理应尽可能遵守这些偏好，或者在无法遵守时中断流程以通知用户。模型对齐可能是一种考虑用户偏好的可能方法。然而，当前的对齐方法主要考虑的是总体偏好，而满足个人偏好的方法尚未开发。更普遍地说，代理也可以通过产生伤害（即使是以无意的方式）来实现目标，而且如果它有能力使用工具，这种风险就更大。'
- en: '**Incorporating feedback**: We know agents are error-prone, and while we can
    develop strategies to reduce errors, completely eliminating them may not be possible.
    An agent might continue to use suboptimal tools (not understanding the goal or
    setting the wrong plan) in repeated interactions, frustrating the user. One way
    to correct this behavior is to provide feedback from the user. There is now research
    on how to incorporate this feedback and how to represent it in a more effective
    form for the agent (e.g., turn it into first-order logic).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**整合反馈**：我们知道代理容易出错，虽然我们可以制定策略来减少错误，但完全消除它们可能是不可能的。代理可能会在重复的交互中继续使用次优工具（不理解目标或设置错误的计划），从而让用户感到沮丧。纠正这种行为的一种方法是从用户那里获取反馈。现在有关于如何整合这种反馈以及如何以更有效的方式向代理表示这种反馈的研究（例如，将其转换为一级逻辑）。'
- en: 'There are also challenges that are associated with how the agent communicates
    to the user, especially pertaining to their capabilities, what actions they take
    or will take, goal achievement, and unexpected events:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与代理与用户沟通的方式相关的一些挑战也存在，特别是关于他们的能力、他们采取或将要采取的行动、目标实现和意外事件：
- en: '**Capabilities of the agent**: The user must be able to understand the full
    capabilities (and limitations) of an agent in order to conduct informed decision-making.
    It should be clear what information the agent has access to, how it will use this
    information, how it can modify the external environment, what tools it has access
    to, and whether it can connect to the internet.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理的能力**：用户必须能够理解代理的全部能力（和限制）才能进行明智的决策。应该清楚代理可以访问哪些信息，它将如何使用这些信息，它如何可以修改外部环境，它可以使用哪些工具，以及它是否可以连接到互联网。'
- en: '**What actions the agent will take**: To solve the goal, an agent can detail
    a complex plan, which can be particularly costly (time, resources, or money) and
    may violate some of the user’s preferences. The user then should be aware of the
    actions an agent takes and be able to provide feedback. Of course, an effective
    form of communication must be found to avoid irrelevant details being communicated
    and the user not fully understanding the agent’s actions. In addition, it should
    be clarified whether some actions require the user’s explicit approval.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理将采取哪些行动**：为了解决目标，代理可以详细制定一个复杂的计划，这可能特别昂贵（时间、资源或金钱），并且可能违反一些用户的偏好。然后，用户应该了解代理采取的行动，并能够提供反馈。当然，必须找到一种有效的沟通形式，以避免传达无关紧要的细节，并确保用户完全理解代理的行动。此外，还应明确是否有些行动需要用户的明确批准。'
- en: '**Monitor progress**: For an agent moving in a dynamic environment, a plan
    to complete a task requires several steps; it is useful for a user to be aware
    of what the agent is doing and whether it is necessary to modify the process or
    stop it. An agent conducting multiple actions at the same time could lead to unexpected
    and harmful behavior. For example, an agent who builds news reports and invests
    in the market might read fake news and conduct a series of bad investments.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控进度**：对于一个在动态环境中移动的代理，完成任务的计划需要几个步骤；对于用户来说，了解代理正在做什么以及是否需要修改过程或停止它是很有用的。同时执行多个行动的代理可能导致意外和有害的行为。例如，一个构建新闻报告并在市场上进行投资的代理可能会阅读虚假新闻并执行一系列不良投资。'
- en: '**Changes in the environment and side effects**: An agent must monitor the
    changes in the environment or potential side effects of its operations. Take,
    for example, an agent tasked with buying a product online at the lowest price
    available. The agent could search online and find the product at a very competitive
    price and order it. However, the offer might require a subscription or other hidden
    costs that would make the purchase much more expensive than the user’s preferences
    or budget. The user must be aware of the side effects that are generated by the
    agent’s behavior.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境和副作用的变化**：代理必须监控环境的变化或其操作可能产生的潜在副作用。例如，一个被分配以最低价格在线购买产品的代理可能会在网上找到具有非常具有竞争力的价格的产品并下单。然而，该报价可能需要订阅或其他隐藏费用，这会使购买比用户的偏好或预算高得多。用户必须意识到代理行为产生的副作用。'
- en: '**Goal attainment**: The user specifies a goal, and the agent plans actions
    and executes them. At the end of this process, the user must be clear whether
    the agent has achieved the goal or not (or partially). Thus, a way is needed to
    evaluate that a goal has been achieved. For example, the goal might be to buy
    the cheapest possible cell phone with a certain type of performance. The agent
    could lead the purchase, but we need to assess whether the agent has met the other
    conditions as well. Thus, we need a way to verify that the goal has been fully
    and satisfactorily achieved.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标达成**：用户指定一个目标，代理规划行动并执行它们。在这个过程中，用户必须清楚代理是否实现了目标（或部分实现）。因此，需要一种方法来评估目标是否已经实现。例如，目标可能是以某种性能类型购买最便宜的移动电话。代理可以引导购买，但我们还需要评估代理是否满足其他条件。因此，我们需要一种方法来验证目标是否已经完全且令人满意地实现。'
- en: Communication with agents is a complex but critical topic. Miscommunication
    can lead to system failure and is an important point to consider. In this section,
    we have provided a list of important elements to evaluate user-agent communication
    from different perspectives. In the next subsection, we will see whether or not
    the use of multi-agents is superior to the single agent. Some studies question
    this perspective.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与代理的沟通是一个复杂但关键的话题。沟通不畅可能导致系统故障，这是需要考虑的重要问题。在本节中，我们提供了一份从不同角度评估用户-代理沟通的重要元素列表。在下一小节中，我们将看到使用多代理是否比单代理更优越。一些研究对此观点提出了质疑。
- en: No clear superiority of multi-agents
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多代理没有明显的优势
- en: As mentioned earlier, an LLM-based agent can be identified as an entity that
    has an initial state (usually a description in the prompt specifying its initial
    state), can track what it produces (state), and can interact with the environment
    through the use of tools (action). A **multi-agent system** (**MAS**) is defined
    as a collection of agents that interact with each other in a coordinated manner
    to solve a task.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，基于LLM的代理可以被识别为一个具有初始状态（通常是在提示中指定其初始状态的描述），可以跟踪其产出（状态），并且可以通过使用工具与环境交互（动作）。**多代理系统**（MAS）被定义为一系列以协调方式相互交互以解决任务的代理。
- en: MASs are an extension of single-agent systems, designed to create a more sophisticated
    framework capable of addressing complex problems. Obviously, this means a higher
    computational cost (more LLM calls in inference). This higher computational cost
    should be justified by a substantial performance gain. In fact, some studies show
    that this is not the case. MASs offer only marginal gains in performance compared
    to single-agent systems.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: MASs是单代理系统的扩展，旨在创建一个更复杂的框架，能够解决复杂问题。显然，这意味着更高的计算成本（推理中更多的LLM调用）。这种更高的计算成本应该通过显著的性能提升来证明。事实上，一些研究表明情况并非如此。与单代理系统相比，MASs在性能上仅提供微小的提升。
- en: 'It is useful to outline the architectural trade-offs between single-agent and
    multi-agent designs. While MASs offer potential advantages in modularity and parallelism,
    they also introduce additional complexity, coordination overhead, and cost. The
    following table summarizes the key differences:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 概述单代理和多代理设计之间的架构权衡是有用的。虽然MASs在模块化和并行性方面提供了潜在的优势，但它们也引入了额外的复杂性、协调开销和成本。以下表格总结了关键差异：
- en: '|  | **Single-Agent Design** | **Multi-Agent Design** |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | **单代理设计** | **多代理设计** |'
- en: '| **Cost** | Lower: fewer inference steps and less orchestration | Higher:
    more agents, and more LLM calls and tool usage |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| **成本** | 较低：较少的推理步骤和较少的编排 | 较高：更多代理，更多LLM调用和工具使用 |'
- en: '| **Latency** | Generally lower, streamlined single flow | Potentially higher
    due to inter-agent communication |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **延迟** | 通常较低，流程简化 | 由于代理间通信可能较高 |'
- en: '| **Fault tolerance** | Lower: failure in the agent often breaks the system
    | Higher: failures can be contained within individual agents |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **容错性** | 较低：代理故障通常会导致系统崩溃 | 较高：故障可以包含在单个代理内 |'
- en: '| **Modularity** | Monolithic and harder to extend | Modular: agents can be
    added or replaced independently |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **模块化** | 单一且难以扩展 | 模块化：代理可以独立添加或替换 |'
- en: '| **Scalability** | Limited: the agent handles all logic | Higher: parallel
    agents allow distributed problem-solving |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **可扩展性** | 有限：代理处理所有逻辑 | 较高：并行代理允许分布式问题解决 |'
- en: '| **Communication** **overhead** | None (internal reasoning) | Significant:
    explicit agent-to-agent messaging required |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **通信开销** | 无（内部推理） | 显著：需要显式的代理间消息传递 |'
- en: '| **Interpretability** | Easier: single decision chain | Harder: distributed
    reasoning may reduce transparency |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **可解释性** | 较易：单一路径决策链 | 较难：分布式推理可能会降低透明度 |'
- en: Table 11.1 – Potential causes of multi-agent system failure
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表11.1 – 多代理系统故障的潜在原因
- en: As shown in the preceding table, multi-agent architectures introduce a set of
    trade-offs that must be carefully balanced. While they offer modularity and potential
    fault isolation, they often suffer from increased latency, communication overhead,
    and coordination challenges. These trade-offs are reflected in empirical evaluations
    of MASs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如前表所示，多代理架构引入了一系列必须仔细平衡的权衡。虽然它们提供了模块化和潜在的故障隔离，但它们通常遭受更高的延迟、通信开销和协调挑战。这些权衡在MASs的经验评估中得到了反映。
- en: '![Figure 11.7 – Failure rates of five popular multi-agent LLM systems (https://arxiv.org/pdf/2503.13657)](img/B21257_11_07.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图11.7 – 五个流行的多代理LLM系统的故障率（https://arxiv.org/pdf/2503.13657）](img/B21257_11_07.jpg)'
- en: Figure 11.7 – Failure rates of five popular multi-agent LLM systems ([https://arxiv.org/pdf/2503.13657](https://arxiv.org/pdf/2503.13657))
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7 – 五个流行的多代理LLM系统的故障率([https://arxiv.org/pdf/2503.13657](https://arxiv.org/pdf/2503.13657))
- en: 'MASs should bring numerous benefits, such as greater accuracy and the ability
    to handle more complex tasks, create more complex plans, or find better solutions.
    If MASs do not bring all these benefits and indeed often fail, we need to understand
    why. In a recent study, Cemri et al. (2025) set out to conduct a detailed taxonomy
    of MAS failures with expert annotators by analyzing 150 conversation traces (each
    averaging over 15,000 lines of text) to identify failures and the causes of these
    failures. In their work, they identified 14 causes, grouped into 3 main groups:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: MASs 应该带来众多好处，例如更高的准确性、处理更复杂任务的能力、创建更复杂的计划或找到更好的解决方案。如果 MASs 没有带来所有这些好处，实际上往往还会失败，我们就需要了解其中的原因。在最近的一项研究中，Cemri
    等人（2025）通过分析 150 个对话记录（每个平均超过 15,000 行文本）来对 MAS 失败进行详细的分类，并使用专家注释员识别失败及其原因。在他们的工作中，他们确定了
    14 个原因，分为 3 个主要类别：
- en: '![Figure 11.8 – Taxonomy of MAS failure modes (https://arxiv.org/pdf/2503.13657)](img/B21257_11_08.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – MAS 失败模式的分类 (https://arxiv.org/pdf/2503.13657)](img/B21257_11_08.jpg)'
- en: Figure 11.8 – Taxonomy of MAS failure modes ([https://arxiv.org/pdf/2503.13657](https://arxiv.org/pdf/2503.13657))
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – MAS 失败模式的分类 ([https://arxiv.org/pdf/2503.13657](https://arxiv.org/pdf/2503.13657))
- en: 'The three main categories are thus as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，三个主要类别如下：
- en: '**Specification and system design failures**: Failure results from deficits
    in MAS design. For the authors, much of the failure stems from poor choice of
    architecture, management of conversation between agents, poor task specification,
    violation of constraints, and poor specification of agent roles and responsibilities.
    In other words, if the instructions for agents are not clear, the system may fail.
    Even when instructions are clear, however, the MAS may not be aligned with user
    instructions.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规范和系统设计失败**：失败源于 MAS 设计的不足。对于作者来说，许多失败源于架构选择不当、代理之间对话管理不善、任务规范不良、违反约束以及代理角色和责任规范不良。换句话说，如果代理的指令不明确，系统可能会失败。即使指令明确，MAS
    也可能不符合用户指令。'
- en: '**Inter-agent misalignment**: Failure emerges from ineffective communication,
    little collaboration, conflicting behaviors among agents, and gradual derailment
    from the initial task. As we mentioned previously, achieving efficient communication
    between agents is not easy. Therefore, some agents may not communicate efficiently
    and simply waste resources.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理间不匹配**：失败源于无效的通信、合作不足、代理之间的冲突行为以及逐渐偏离初始任务。正如我们之前提到的，实现代理之间的高效通信并不容易。因此，一些代理可能无法高效地沟通，从而浪费资源。'
- en: '**Task verification and termination**: A third important category includes
    failure to complete the task or its premature termination. MASs often lack a verification
    mechanism that checks and ensures the accuracy, completeness, and reliability
    of interactions, decisions, and outcomes. Simply put, many systems do not include
    a dedicated agent (or other mechanism) to monitor the process and verify that
    the task was successfully executed.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务验证和终止**：第三个重要类别包括未能完成任务或其过早终止。MASs 往往缺乏一种验证机制，该机制检查并确保交互、决策和结果的真实性、完整性和可靠性。简单来说，许多系统没有包括一个专门的代理（或其他机制）来监控过程并验证任务是否成功执行。'
- en: The results of their investigation showed that none of the causes are prevalent
    but are equally distributed across systems. In addition, some causes are correlated,
    producing a kind of ripple effect. For example, wrong architecture design can
    cause inefficient communication between agents.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的调查结果显示，这些原因没有哪一个特别普遍，而是在各个系统中均匀分布。此外，一些原因之间存在相关性，产生了一种连锁反应。例如，错误的架构设计可能导致代理之间通信效率低下。
- en: '![Figure 11.9 – Distribution of failure modes by categories and systems (https://arxiv.org/pdf/2503.13657)](img/B21257_11_09.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.9 – 按类别和系统分布的失败模式 (https://arxiv.org/pdf/2503.13657)](img/B21257_11_09.jpg)'
- en: Figure 11.9 – Distribution of failure modes by categories and systems ([https://arxiv.org/pdf/2503.13657](https://arxiv.org/pdf/2503.13657))
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 – 按类别和系统分布的失败模式 ([https://arxiv.org/pdf/2503.13657](https://arxiv.org/pdf/2503.13657))
- en: The results of this work clearly show that failures can be avoided through more
    careful design. Improving prompts, agent communication, and adding an agent (or
    other verifier mechanism) allow for noticeably improved performance and lower
    risk of failure. In two case studies, the authors show how this is the case. On
    the other hand, these suggestions are not enough to solve all agent problems but
    will be further technical progress.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的结果表明，通过更仔细的设计可以避免失败。改进提示、代理通信以及添加代理（或其他验证机制）可以显著提高性能并降低失败的风险。在两个案例研究中，作者展示了这种情况。另一方面，这些建议不足以解决所有代理问题，但将进一步的技术进步。
- en: In addition to the system itself, many of the limitations of agents also stem
    from the agent itself (i.e., the model that is used for agents). In the next subsection,
    we will discuss the reasoning limitations of LLMs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 除了系统本身之外，许多代理的限制也源于代理本身（即用于代理的模型）。在下一小节中，我们将讨论LLMs的推理限制。
- en: Limits of reasoning
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理的局限性
- en: 'Reasoning is a fundamental cognitive function of human beings, and it is difficult
    to give a precise definition. Wikipedia defines reasoning this way: “*Reason is
    the capacity of consciously applying logic by drawing valid conclusions from new
    or existing information, with the aim of seeking the truth. It is associated with
    such characteristically human activities as philosophy, religion, science, language,
    mathematics, and art, and is normally considered to be a distinguishing ability
    possessed* *by humans*.”'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 推理是人类的基本认知功能，很难给出一个精确的定义。维基百科这样定义推理：“*推理是通过从新信息或现有信息中得出有效结论，有意识地应用逻辑，以寻求真理的能力。它与哲学、宗教、科学、语言、数学和艺术等具有人类特征的活动相关联，通常被认为是人类所拥有的区分能力*。”
- en: For a long time, it was said that only human beings are equipped with reasoning.
    Today, however, it has been shown that primates, octopuses, and birds also exhibit
    basic forms of reasoning such as making decisions or solving problems. One of
    the problems with reasoning is the difficulty of being able to evaluate it. Typically,
    to do this, one assesses the ability to solve complex problems or make decisions.
    Complex problem-solving requires identifying the problem, dividing it into subproblems,
    finding patterns, and then choosing the best solution. Decision-making similarly
    requires identifying problems and patterns and evaluating alternatives before
    choosing the best solution.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 很长一段时间以来，人们都说只有人类才具备推理能力。然而，如今已经证明，灵长类动物、章鱼和鸟类也表现出基本的推理形式，例如做决定或解决问题。推理的一个问题是难以对其进行评估。通常，为了做到这一点，人们会评估解决复杂问题或做出决策的能力。复杂的解决问题需要识别问题、将其分解为子问题、寻找模式，然后选择最佳解决方案。决策同样需要识别问题和模式，并在选择最佳解决方案之前评估替代方案。
- en: 'In the case of LLMs, an attempt was made to measure reasoning capabilities
    through benchmark datasets that assess problem-solving ability (such as GLUE,
    SuperGLUE, and Hellaswag). Today, on many of these datasets, humans have been
    outperformed by next-generation LLMs. These new reasoning capabilities would be
    mainly due to three factors:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs的情况下，人们试图通过评估问题解决能力的基准数据集来衡量推理能力（例如GLUE、SuperGLUE和Hellaswag）。如今，在这些数据集的许多情况下，人类已经被下一代LLMs超越。这些新的推理能力主要归因于三个因素：
- en: LLMs performing well in all the benchmarks dedicated to reasoning. These benchmarks
    contain math or coding problems that require reasoning skills. The results in
    these benchmarks suggest that LLMs are capable of reasoning.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有针对推理的基准测试中，大型语言模型（LLMs）都表现出色。这些基准测试包含需要推理技能的数学或编码问题。这些基准测试的结果表明，LLMs具有推理能力。
- en: The emergence of new properties with increasing parameters, number of tokens,
    and compute budget.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着参数、标记数量和计算预算的增加，新特性的出现。
- en: The use of techniques such as CoT, which allows the model to fulfill its potential.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CoT等技术，这些技术允许模型发挥其潜力。
- en: 'There are those who question this view, claiming that there are alternative
    explanations for the performance achieved in these benchmarks. After all, many
    authors regard LLMs as nothing more than mere stochastic parrots. Jiang, in 2022
    ([https://arxiv.org/pdf/2406.11050](https://arxiv.org/pdf/2406.11050)), suggested
    that the models are merely pattern-matching machines: “*A strong token bias suggests
    that the model is relying on superficial patterns in the input rather than truly
    understanding the underlying* *reasoning task*.”'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有的人质疑这种观点，声称这些基准测试中取得的成绩有其他解释。毕竟，许多作者认为LLMs不过是一群随机的鹦鹉。江在2022年([https://arxiv.org/pdf/2406.11050](https://arxiv.org/pdf/2406.11050))提出，这些模型仅仅是模式匹配机器：“*强烈的标记偏差表明模型依赖于输入中的表面模式，而不是真正理解底层*
    *推理任务*。”
- en: In the same study, it was observed that LLMs fail to generalize when they encounter
    new examples that exhibit patterns different from those seen in the pre-training
    phase. If we change tokens in the examples, pattern mapping fails (a transformer,
    through in-context learning, tries to find examples in its knowledge that are
    similar to the problem posed by the user). When the model fails to find examples,
    the model fails to solve the question. This fragility and dependence on training
    examples would explain why the model succeeds in solving complex problems (it
    finds patterns) and fails even with some very simple questions (it does not find
    examples). This is confirmed by a correlation between the example’s frequency
    in training data and test performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一项研究中，观察到当LLMs遇到在预训练阶段未见过的、表现出与预训练阶段不同模式的新的示例时，它们无法泛化。如果我们更改示例中的标记，模式映射就会失败（一个通过上下文学习尝试在其知识中找到与用户提出的问题相似的示例的Transformer）。当模型找不到示例时，模型就无法解决该问题。这种脆弱性和对训练示例的依赖性可以解释为什么模型在解决复杂问题时能够成功（它找到了模式），甚至在一些非常简单的问题上也会失败（它找不到示例）。这一点通过训练数据中示例频率与测试性能之间的相关性得到了证实。
- en: For example, when the model is asked to solve the classic “25 horses” graph
    theory problem, the model succeeds. If the “horse” token is changed to “bunny,”
    the model fails to solve it. The token change is irrelevant to the problem’s underlying
    logic, yet the model fails to solve it because it has difficulty mapping the problem.
    Both GPT-4 and Claude have significant performance drops due to perturbations
    in animal names and numbers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当模型被要求解决经典的“25匹马”图论问题时，模型能够成功解决。如果将“马”标记改为“兔子”，模型就无法解决它。标记的改变与问题的底层逻辑无关，但模型因为难以映射问题而无法解决它。GPT-4和Claude都因为动物名称和数字的扰动而出现了显著的性能下降。
- en: '![Figure 11.10 – Token bias using the classic problems (https://arxiv.org/pdf/2406.11050)](img/B21257_11_10.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图11.10 – 使用经典问题进行标记偏差 (https://arxiv.org/pdf/2406.11050)](img/B21257_11_10.jpg)'
- en: Figure 11.10 – Token bias using the classic problems ([https://arxiv.org/pdf/2406.11050](https://arxiv.org/pdf/2406.11050))
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10 – 使用经典问题进行标记偏差 ([https://arxiv.org/pdf/2406.11050](https://arxiv.org/pdf/2406.11050))
- en: 'This phenomenon is called **prompt sensitivity** (a different response to a
    prompt that is semantically equivalent to another). This is confirmed by the fact
    that LLMs are sensitive to noise. They are easily distracted by irrelevant context,
    which makes it more difficult to find patterns. This sensitivity is not resolved
    by prompting techniques specialized to improve reasoning, suggesting that disturbing
    pattern-matching activity disrupts reasoning ability. An example of irrelevant
    context disrupting the pattern but not impacting actual problem-solving follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这种现象被称为**提示敏感性**（对语义上等同于另一个提示的不同响应）。这一点通过LLMs对噪声敏感的事实得到了证实。它们很容易被不相关的上下文所干扰，这使得找到模式变得更加困难。这种敏感性并不能通过专门用于提高推理能力的提示技术得到解决，这表明干扰模式匹配活动会破坏推理能力。以下是一个不相关上下文干扰模式但不影响实际解决问题的例子：
- en: '![Figure 11.11 – Irrelevant context disturbs LLMs (https://arxiv.org/pdf/2302.00093)](img/B21257_11_11.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图11.11 – 不相关上下文干扰LLMs (https://arxiv.org/pdf/2302.00093)](img/B21257_11_11.jpg)'
- en: Figure 11.11 – Irrelevant context disturbs LLMs ([https://arxiv.org/pdf/2302.00093](https://arxiv.org/pdf/2302.00093))
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11 – 不相关上下文干扰LLMs ([https://arxiv.org/pdf/2302.00093](https://arxiv.org/pdf/2302.00093))
- en: Some authors suggest that intelligence can be seen as an emergent property.
    Biological systems naturally tend to become more complex, and this process is
    driven by natural selection. Evolution has shown an increase in intelligence over
    time as it promotes the adaptability of various species. Of course, intelligence
    is not an economic process, and a larger brain consumes a greater amount of resources
    (metabolic consumption). Loss function could be seen as evolutionary pressure.
    From this, it would follow that the increase in model capacity (in terms of the
    number of parameters) would parallel the increase in neurons in animal brains
    over time, and that loss function would instead be the evolutionary pressure to
    push these parameters to be used efficiently. By scaling up models and training
    (parameters and training tokens), intelligence could also emerge in LLMs. Reasoning
    then is seen as an emergent property that emerges from scaling the models. However,
    later studies suggest that emergent properties in LLMs can be a measurement error,
    and with it, the whole theory is related to the emergence of reasoning.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一些作者建议，智能可以被视为一种涌现属性。生物系统自然倾向于变得更加复杂，这个过程是由自然选择驱动的。进化表明，随着时间的推移，智能有所增加，因为它促进了各种物种的适应性。当然，智能不是一个经济过程，大脑体积越大，消耗的资源（代谢消耗）就越多。损失函数可以被视为进化压力。由此可以推断，模型容量（就参数数量而言）的增加将与动物大脑中神经元数量的增加相平行，而损失函数将推动这些参数被有效地使用。通过扩大模型和训练（参数和训练令牌），LLMs中也可能出现智能。因此，推理被视为从模型规模扩展中涌现出的属性。然而，后来的研究表明，LLMs中的涌现属性可能是测量误差，并且整个理论与推理的涌现有关。
- en: In the next figure, you can see how some properties seem to emerge as the model
    size increases.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一张图中，你可以看到一些属性似乎随着模型规模的增加而出现。
- en: '![Figure 11.12 – Examples of emerging reasoning properties (https://arxiv.org/abs/2304.15004)](img/B21257_11_12.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图11.12 – 涌现推理属性的示例](https://arxiv.org/abs/2304.15004)(img/B21257_11_12.jpg)'
- en: Figure 11.12 – Examples of emerging reasoning properties ([https://arxiv.org/abs/2304.15004](https://arxiv.org/abs/2304.15004))
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12 – 涌现推理属性的示例([https://arxiv.org/abs/2304.15004](https://arxiv.org/abs/2304.15004))
- en: According to other authors, LLMs are capable of reasoning, but it needs to be
    unlocked. CoT prompting thus helps the model unlock its potential through intermediate
    reasoning and thus guides it to the correct answer in arithmetic problems. CoT
    is today’s prompt engineering technique and is also used to train deep reasoning
    models (such as ChatGPT-o1 or DeepSeek R1). In fact, these models are trained
    on long CoTs that are used to conduct supervised fine-tuning. These models explore
    different reasoning paths to arrive at an answer, showing high improvements in
    reasoning benchmarks. However, some studies show that these models suffer from
    both overthinking and underthinking.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其他作者的观点，大型语言模型（LLMs）具有推理能力，但需要被解锁。因此，CoT提示（CoT prompting）通过中间推理帮助模型解锁其潜力，从而引导它在算术问题中找到正确答案。CoT是今天的提示工程技术，也用于训练深度推理模型（如ChatGPT-o1或DeepSeek
    R1）。实际上，这些模型是在用于进行监督微调的长CoTs上训练的。这些模型探索不同的推理路径来得出答案，显示出在推理基准测试中的显著提升。然而，一些研究表明，这些模型同时存在过度思考和思考不足的问题。
- en: Overthinking is a curious phenomenon in which these models reason longer than
    necessary when it comes to solving problems that are particularly simple. The
    model explores different reasoning paths for trivial questions. This indicates
    that the model is unable to understand which question needs more effort. Underthinking
    is the opposite, wherein the model may abandon the promising thinking path. This
    indicates a clear lack of depth of reasoning, where the model does not go all
    the way to a correct solution.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 过度思考是一种奇特的现象，在这些模型解决特别简单的问题时，它们会进行比必要更长的时间的推理。模型会探索不同推理路径来回答简单问题。这表明模型无法理解哪些问题需要更多的努力。而思考不足则是其对立面，其中模型可能会放弃有希望的思考路径。这表明推理深度明显不足，模型没有完全达到正确解决方案。
- en: 'At the same time, even the benefits of CoT have been questioned ([https://arxiv.org/pdf/2409.12183](https://arxiv.org/pdf/2409.12183)):
    “*As much as 95% of the total performance gain from CoT on MMLU is attributed
    to questions containing “=” in the question or generated output. For non-math
    questions, we find no features to indicate when CoT* *will help*.”'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，CoT 的好处也受到了质疑([https://arxiv.org/pdf/2409.12183](https://arxiv.org/pdf/2409.12183))：“*在
    MMLU 上，高达 95% 的总性能提升归因于包含“=”的问题或生成的输出。对于非数学问题，我们发现没有特征可以指示 CoT* *何时会有帮助*。”
- en: '![Figure 11.13 – CoT improvements are limited to symbolic and mathematical
    reasoning (https://arxiv.org/pdf/2409.12183)](img/B21257_11_13.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.13 – CoT 的改进仅限于符号和数学推理](https://arxiv.org/pdf/2409.12183)(img/B21257_11_13.jpg)'
- en: Figure 11.13 – CoT improvements are limited to symbolic and mathematical reasoning
    ([https://arxiv.org/pdf/2409.12183](https://arxiv.org/pdf/2409.12183))
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13 – CoT 的改进仅限于符号和数学推理([https://arxiv.org/pdf/2409.12183](https://arxiv.org/pdf/2409.12183))
- en: CoT would seem to help the model solve problems as it allows it to leverage
    the skills it learned during pre-training. CoT would simply help develop a plan,
    but then the LLMs may not be able to execute it. So, CoT can be used to get a
    plan, but to get the most benefit, an external tool would have to be added (such
    as a Python interpreter).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 似乎有助于模型解决问题，因为它允许它利用在预训练期间学到的技能。CoT 简单地帮助制定计划，但然后 LLM 可能无法执行它。因此，CoT 可以用来获取计划，但要获得最大利益，必须添加外部工具（如
    Python 解释器）。
- en: '![Figure 11.14 – An LLM can devise a plan but needs an external tool to better
    solve some problems (https://arxiv.org/pdf/2409.12183)](img/B21257_11_14.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.14 – 一个 LLM 可以制定计划，但需要外部工具来更好地解决一些问题](https://arxiv.org/pdf/2409.12183)(img/B21257_11_14.jpg)'
- en: Figure 11.14 – An LLM can devise a plan but needs an external tool to better
    solve some problems ([https://arxiv.org/pdf/2409.12183](https://arxiv.org/pdf/2409.12183))
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.14 – 一个 LLM 可以制定计划，但需要外部工具来更好地解决一些问题([https://arxiv.org/pdf/2409.12183](https://arxiv.org/pdf/2409.12183))
- en: These models are all tested on the same benchmarks as the **Grade School Math
    8K** (**GSM8K**) dataset, which provides complex arithmetic problems but is at
    risk of data leakage (considering how many billions of tokens are used to train
    an LLM, the model may have already seen the answer in the training).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型都是在与 **Grade School Math 8K** (**GSM8K**) 数据集相同的基准上测试的，该数据集提供了复杂的算术问题，但存在数据泄露的风险（考虑到训练一个
    LLM 需要数十亿个标记，模型可能已经在训练中看到了答案）。
- en: Therefore, in their study, Mirzadeh et al. modified GSM8K, keeping the same
    questions but making statistical pattern matching difficult. If the model was
    capable of true reasoning, it should solve it easily; if, instead, it relied on
    pattern matching, it would fail.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在他们的研究中，Mirzadeh 等人修改了 GSM8K，保持了相同的问题，但使统计模式匹配变得困难。如果模型能够进行真正的推理，它应该能够轻松解决它；相反，如果它依赖于模式匹配，它就会失败。
- en: In the following figure, notice how the GSM8K examples are modified to better
    control the response of the LLM. Using this dataset, we can formally investigate
    the LLM’s reasoning and highlight that state-of-the-art LLMs exhibit significant
    performance variations; this shows that LLM reasoning is fragile.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，请注意 GSM8K 示例是如何修改的，以更好地控制 LLM 的响应。使用这个数据集，我们可以正式调查 LLM 的推理，并强调最先进的 LLM
    表现出显著的性能变化；这表明 LLM 的推理是脆弱的。
- en: '![Figure 11.15 – This dataset serves as a tool to investigate the presumed
    reasoning capabilities of LLMs (https://arxiv.org/pdf/2410.05229)](img/B21257_11_15.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.15 – 这个数据集作为研究 LLM 假定的推理能力的一种工具](https://arxiv.org/pdf/2410.05229)(img/B21257_11_15.jpg)'
- en: Figure 11.15 – This dataset serves as a tool to investigate the presumed reasoning
    capabilities of LLMs ([https://arxiv.org/pdf/2410.05229](https://arxiv.org/pdf/2410.05229))
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.15 – 这个数据集作为研究 LLM 假定的推理能力的一种工具([https://arxiv.org/pdf/2410.05229](https://arxiv.org/pdf/2410.05229))
- en: 'Testing state-of-the-art LLMs, Mirzadeh et al. found no evidence of formal
    reasoning in language models. The models are not robust and have a drop in performance
    when numerical values are changed, and their capabilities degrade sharply as the
    complexity of the problem increases. The model is, in fact, fooled by added phrases
    that have no relevance. Instead, the model takes them into account, tries to map
    them, and sometimes turns them into operations. Mirzadeh et al. suggest that this
    occurs because their training datasets included similar examples that required
    conversion to mathematical operations: “*For instance, a common case we observe
    is that models interpret statements about “discount” as “multiplication”, regardless
    of the context. This raises the question of whether these models have truly understood
    the mathematical concepts* *well enough*.”'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试最先进的LLM时，Mirzadeh等人发现语言模型中没有形式推理的证据。当数值发生变化时，这些模型不够稳健，性能会下降，并且随着问题复杂性的增加，它们的性能会急剧下降。实际上，模型被添加的无关短语所欺骗。相反，模型考虑了这些短语，试图将它们映射到模型中，有时甚至将它们转化为操作。Mirzadeh等人建议，这发生是因为他们的训练数据集中包含了需要转换为数学运算的类似示例：“*例如，我们观察到的常见情况是，模型将关于“折扣”的陈述解释为“乘法”，无论上下文如何。这引发了这些模型是否真正充分理解数学概念的疑问*。”
- en: '![Figure 11.16 – Example of error (https://arxiv.org/pdf/2410.05229)](img/B21257_11_16.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图11.16 – 错误示例](img/B21257_11_16.jpg)'
- en: Figure 11.16 – Example of error ([https://arxiv.org/pdf/2410.05229](https://arxiv.org/pdf/2410.05229))
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.16 – 错误示例（[https://arxiv.org/pdf/2410.05229](https://arxiv.org/pdf/2410.05229)）
- en: More recent LLMs that have been trained on CoT (such as GPT4-o1) also fail in
    this task. This suggests that LLMs are elaborate statistical pattern machines
    but do not possess true reasoning.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在CoT（如GPT4-o1）上训练的LLM也在这项任务中失败了。这表明LLM是复杂的统计模式机器，但并不具备真正的推理能力。
- en: Creativity in LLM
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM中的创造力
- en: Creativity is considered along with reasoning to be one of the skills that makes
    human beings. If quantifying reasoning is hard, being able to quantify creativity
    is a much harder task. However, creativity plays a very important role in what
    makes us human, and it concerns activities such as writing poems or books, creating
    works of art, or even generating theories and achieving groundbreaking discoveries.
    That is why the question as to whether an LLM can be creative has been raised.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 创造力与推理一起被认为是使人类成为人类的一种技能。如果量化推理是困难的，那么量化创造力是一项更加困难的任务。然而，创造力在我们成为人类的过程中扮演着非常重要的角色，它涉及到诸如写诗或书籍、创作艺术作品，甚至生成理论和实现突破性发现等活动。这就是为什么人们提出了LLM是否具有创造力的疑问。
- en: 'The problem in investigating the creativity of LLMs is that we do not have
    an unambiguous definition of creativity. In the field of research, creativity
    is often used as the definition chosen by Margaret Boden: “*the ability to come
    up with ideas or artifacts that are new, surprising and valuable*.” Although this
    definition is accepted, it is difficult to evaluate its elements:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 调查LLM的创造力的问题在于我们没有明确的创造力定义。在研究领域，创造力通常被用作Margaret Boden选择的定义：“*提出新颖、令人惊讶且有价值的思想或作品的能力*。”尽管这个定义被接受，但评估其元素是困难的：
- en: '**Value**: This is the easiest element to define. For example, code produced
    by an LLM can be considered valuable if it works in its own way.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**价值**：这是最容易定义的元素。例如，如果一个LLM生成的代码能够以自己的方式工作，那么它可以被认为是具有价值的。'
- en: '**Novelty**: For an object to be considered novel, it should be dissimilar
    to what has already been created. For a text, being novel could be considered
    the difference in output compared to other texts. One definition might be to generate
    a text whose embedding is distant from other different texts.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新颖性**：对于一个对象要被认为是新颖的，它应该与已经创造出的东西不同。对于文本来说，新颖性可以被认为是与其他文本输出之间的差异。一个可能的定义是生成一个嵌入与其他不同文本距离较远的文本。'
- en: '**Surprising**: This is considered one of the most important and difficult
    elements to define. Random recombination of words can be considered new (or different)
    but certainly not surprising (nor valuable). *Surprising* is often understood
    as something new but not a simple variation or recombination.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**令人惊讶**：这被认为是定义中最重要且最困难的部分。随机重组的单词可以被认为是新的（或不同的），但肯定不是令人惊讶的（或是有价值的）。*令人惊讶*通常被理解为新颖的，但不是简单的变化或重组。'
- en: 'Boden at the same time described what she thought were three types of creativity
    with respect to the concept of surprise:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，Boden描述了她认为与惊喜概念相关的三种创造性类型：
- en: '**Combinatorial creativity**: The combination of familiar elements in an unfamiliar
    way (such as two genres that have not been combined previously)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组合性创造性**：以不熟悉的方式组合熟悉元素（例如，将以前未结合过的两种类型）'
- en: '**Exploratory creativity**: The exploration of new solutions in the way of
    thinking (such as a new narrative style, or a twist to a narrative style that
    had not been explored)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索性创造性**：以新的思维方式探索新的解决方案（例如，新的叙述风格，或对尚未探索的叙述风格的转折）'
- en: '**Transformational creativity**: Changing the current narrative style or the
    current way of thinking'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转化性创造性**：改变当前的叙述风格或当前思维方式'
- en: In line with these definitions, several authors have sought to understand whether
    LLMs can be creative, and if so, what kind of creativity they can manifest. The
    main problem with this investigation is trying to quantify the creativity of an
    LLM. One approach is to assess whether the output of LLMs can be mapped to existing
    text snippets on the web. Human creativity is influenced by previous writers,
    but when a writer produces original writing, this cannot be mapped to previous
    writings. If every text generated by an LLM can be mapped to other texts, it is
    overwhelming evidence of a lack of creativity. In a recently published study,
    Lu (2024) analyzed how much of what is produced by an LLM is mappable to texts
    on the internet. The purpose of this study was precisely to create a creative
    index and compare LLMs and human beings.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 与这些定义一致，一些作者试图理解LLM是否具有创造性，如果是的话，它们能表现出什么样的创造性。这项研究的主要问题在于尝试量化LLM的创造性。一种方法就是评估LLM的输出是否可以映射到网络上的现有文本片段。人类的创造性受到先前作家的启发，但当作家创作原创作品时，这不能映射到先前作品中。如果LLM生成的每个文本都可以映射到其他文本，这将是缺乏创造性的压倒性证据。在一项最近发表的研究中，Lu（2024）分析了LLM生成的内容中有多少可以映射到互联网上的文本。这项研究的目的是精确地创建一个创造性指数，并比较LLM和人类。
- en: '![Figure 11.17 – Mapping of LLM output to internet text (https://arxiv.org/pdf/2410.04265)](img/B21257_11_17.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图11.17 – 将LLM输出映射到互联网文本（https://arxiv.org/pdf/2410.04265）](img/B21257_11_17.jpg)'
- en: Figure 11.17 – Mapping of LLM output to internet text ([https://arxiv.org/pdf/2410.04265](https://arxiv.org/pdf/2410.04265))
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.17 – 将LLM输出映射到互联网文本（[https://arxiv.org/pdf/2410.04265](https://arxiv.org/pdf/2410.04265)）
- en: The results of this approach show that humans exhibit greater creativity (based
    on unique word and sentence combinations) than LLMs. That small amount of residual
    creativity in LLMs may simply result from stochastic processes and the fact that
    we do not know the entire pre-training dataset.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的成果表明，人类在创造性（基于独特的单词和句子组合）方面比LLM（大型语言模型）表现得更为出色。LLM中那微小的残余创造性可能仅仅源于随机过程以及我们不知道整个预训练数据集的事实。
- en: '![Figure 11.18 – Comparison between the creativity index of humans and that
    of LLMs (https://arxiv.org/pdf/2410.04265)](img/B21257_11_18.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图11.18 – 人类与LLM的创造性指数比较（https://arxiv.org/pdf/2410.04265）](img/B21257_11_18.jpg)'
- en: Figure 11.18 – Comparison between the creativity index of humans and that of
    LLMs ([https://arxiv.org/pdf/2410.04265](https://arxiv.org/pdf/2410.04265))
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.18 – 人类与LLM的创造性指数比较（[https://arxiv.org/pdf/2410.04265](https://arxiv.org/pdf/2410.04265)）
- en: 'Lou et al. suggest an interesting analogy: “*Just as a DJ remixes existing
    tracks while a composer creates original music, we speculate that LLMs behave
    more like DJs, blending existing texts to produce impressive new outputs, while
    skilled human authors, similar to music composers, craft original* *works*. ”'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Lou等人提出一个有趣的类比：“*正如DJ混音现有曲目而作曲家创作原创音乐一样，我们推测LLM更像DJ，将现有文本混合以产生令人印象深刻的新输出，而熟练的人类作者，类似于音乐作曲家，创作原创作品*。”
- en: 'Despite LLMs being incapable of true creativity, several studies have tried
    to increase the pseudo-creativity of models (in the long run, LLMs can be particularly
    repetitive). There are three potential strategies for doing this:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM无法实现真正的创造性，但有几项研究试图提高模型的伪创造性（从长远来看，LLM可能特别重复）。这里有三种潜在的策略：
- en: '**Acting on the hyperparameters of an LLM**: The first strategy coincides with
    raising the temperature of an LLM. Temperature controls the uncertainty or randomness
    in the generation process. Adjusting temperature impacts model generation, where
    at low temperatures (e.g., 0.1–0.5), the model generates deterministic, focused,
    and predictable outputs. Increasing the temperature generates output that becomes
    less predictable. Beyond 2.0, the process becomes chaotic and the model generates
    nonsense. So, for applications that require creativity, you can explore higher
    temperatures but remember that this also generally leads to a reduction in consistency.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对LLM超参数的操作**：第一种策略与提高LLM的温度相一致。温度控制生成过程中的不确定性或随机性。调整温度会影响模型生成，在低温（例如，0.1–0.5）下，模型生成确定性、专注和可预测的输出。提高温度会生成更不可预测的输出。超过2.0后，过程变得混乱，模型生成无意义的内容。因此，对于需要创造力的应用，你可以探索更高的温度，但请记住，这通常也会导致一致性的降低。'
- en: '**Conducting additional training for an LLM**: The use of post-training techniques
    is an avenue that is being explored widely today. Post-training techniques are
    used for model alignment and to make the model more receptive to performing tasks.
    Some authors have proposed using techniques that also incentivize the variety
    of outputs.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对LLM进行额外训练**：使用后训练技术是今天广泛探索的一条途径。后训练技术用于模型对齐和使模型更易于执行任务。一些作者提出了使用旨在激励输出多样性的技术。'
- en: '**Prompting strategy**: Use prompts that try to force the model to be more
    creative. However, prompting strategies do not seem to have great results.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示策略**：使用试图迫使模型更具创造性的提示。然而，提示策略似乎没有取得很好的效果。'
- en: Mechanistic interpretability
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机制可解释性
- en: Recent advances in AI have meant rapid advancement in model capabilities. Paradoxically,
    the paradigm of self-supervised learning means that even if models are designed
    by humans, the capabilities of LLMs are not designed a priori. In theory, a developer
    only needs to know the process without understanding how the model works, since
    the desired properties appear during training. In other words, an LLM is not designed
    for the properties it shows; these properties were obtained through scaling, and
    much of how it gets there is unclear. Reconstructing how they appear and the mechanisms
    behind these abilities is not an easy task, especially after a model of billions
    of parameters has been trained. These models are considered to be black boxes,
    and recently, there has been some discussion of how they can be analyzed.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的最近进展意味着模型能力迅速提升。矛盾的是，自监督学习的范式意味着即使模型是由人类设计的，LLM的能力也不是预先设计的。理论上，开发者只需要了解过程，而不必理解模型是如何工作的，因为期望的特性在训练过程中出现。换句话说，LLM不是为展示的特性而设计的；这些特性是通过扩展获得的，而它如何到达那里的大部分内容是不清晰的。重建它们出现的方式和这些能力背后的机制不是一项容易的任务，尤其是在训练了数十亿参数的模型之后。这些模型被认为是黑盒，最近，有一些关于如何分析它们的讨论。
- en: There are several types of interpretability for a model (as described in the
    following list and figure). Each of these types of interpretability focuses on
    different aspects.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 模型（如以下列表和图所示）有几种可解释性类型。这些可解释性的类型各自关注不同的方面。
- en: '![Figure 11.19 – Progressive level of interpretation of a model (https://arxiv.org/pdf/2404.14082)](img/B21257_11_19.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图11.19 – 模型的逐步解释级别 (https://arxiv.org/pdf/2404.14082)](img/B21257_11_19.jpg)'
- en: Figure 11.19 – Progressive level of interpretation of a model ([https://arxiv.org/pdf/2404.14082](https://arxiv.org/pdf/2404.14082))
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.19 – 模型的逐步解释级别([https://arxiv.org/pdf/2404.14082](https://arxiv.org/pdf/2404.14082))
- en: 'We can divide the various types of approaches to interpretability into the
    following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将各种可解释性方法分为以下几类：
- en: '**Behavioral**: One considers the model as a black box and is interested in
    the relationship between input and output. This paradigm considers those classical
    approaches to interpretability that are model-agnostic.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行为**：将模型视为一个黑盒，并关注输入和输出之间的关系。这种范式考虑了那些对模型无差别的可解释性经典方法。'
- en: '**Attributional**: The approaches try to understand the decision-making processes
    of the model by tracking the contribution of each component of the input and are
    based on the gradient shift.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归因**：这些方法试图通过跟踪输入的每个组件的贡献来理解模型的决策过程，并基于梯度偏移。'
- en: '**Concept-based**: Probes are used to try to better understand the learned
    representation of the model.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于概念**：使用探针来尝试更好地理解模型学习到的表示。'
- en: '**Mechanistic**: This is a granular analysis of the components and how they
    are organized, trying to identify causal relationships.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机制**：这是一种对组件及其组织方式的细粒度分析，试图识别因果关系。'
- en: Mechanistic interpretability aims to uncover the internal decision-making processes
    of a neural network by identifying the mechanisms that produce its outputs. We
    focus on this approach because it emphasizes understanding the individual components
    of a model and how they contribute to its overall behavior. This perspective is
    valuable as it enables us to analyze the model through a comprehensive and transparent
    lens.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 机制可解释性旨在通过识别产生其输出的机制来揭示神经网络的内部决策过程。我们关注这种方法，因为它强调理解模型的各个组成部分及其对整体行为的影响。这种视角很有价值，因为它使我们能够通过全面和透明的视角来分析模型。
- en: Mechanistic interpretability goes beyond previous approaches because it seeks
    to identify causal mechanisms to the generalization of neural networks, and thus
    the decision-making processes behind them. In response to the growth of models
    and their increased capabilities, there has been a question of how these models
    acquire these general capabilities, and thus a need for global explanations.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 机制可解释性超越了之前的方法，因为它寻求识别导致神经网络泛化的因果机制，以及背后的决策过程。随着模型及其能力的增长，人们提出了一个问题，即这些模型是如何获得这些泛化能力的，因此需要全局解释。
- en: 'Although LLMs generate text that resembles that produced by humans, this does
    not mean that the representation of concepts and cognitive processes are the same.
    This is demonstrated by the fact that LLMs display superhuman performance on some
    tasks, whereas in other tasks that are simple for humans, they fail miserably.
    We need a way to solve this paradox, which is through mechanistic interpretability.
    To try to resolve this dissonance, reverse engineering of LLMs has been proposed.
    Reverse engineering (a mechanistic interpretability approach) involves three steps:
    decomposing the model into simpler parts, describing how these parts work and
    how they interact, and testing whether the assumptions are correct. While mechanistic
    interpretability aims to uncover the internal logic and causal mechanisms within
    the network, concept-based interpretability focuses on understanding how models
    represent high-level, human-understandable concepts and how these concepts contribute
    to the model’s decisions, providing insights into the reasoning behind predictions
    and bridging the gap between human cognition and machine learning processes. These
    two approaches are shown in the following figure:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）生成的文本与人类产生的文本相似，但这并不意味着概念和认知过程的表示是相同的。这一点可以通过以下事实得到证明：LLMs在某些任务上表现出超越人类的能力，而在对人类来说简单的其他任务上，它们却表现得非常糟糕。我们需要一种方法来解决这个悖论，那就是通过机制可解释性。为了尝试解决这种不一致，提出了对LLMs的反向工程。反向工程（一种机制可解释性方法）包括三个步骤：将模型分解成更简单的部分，描述这些部分如何工作以及它们如何相互作用，并测试这些假设是否正确。虽然机制可解释性旨在揭示网络内部的逻辑和因果机制，但基于概念的可解释性则侧重于理解模型如何表示高级、人类可理解的概念，以及这些概念如何影响模型的决定，从而为预测背后的推理提供洞察，并弥合人类认知与机器学习过程之间的差距。这两种方法在以下图中展示：
- en: '![Figure 11.20 – Reverse engineering (https://arxiv.org/pdf/2501.16496)](img/B21257_11_20.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图11.20 – 反向工程 (https://arxiv.org/pdf/2501.16496)](img/B21257_11_20.jpg)'
- en: Figure 11.20 – Reverse engineering ([https://arxiv.org/pdf/2501.16496](https://arxiv.org/pdf/2501.16496))
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.20 – 反向工程 ([https://arxiv.org/pdf/2501.16496](https://arxiv.org/pdf/2501.16496))
- en: The problem with this approach is that it is difficult to decompose neural networks
    into functional components. In fact, in neural networks, neurons are polysemantic
    and represent more than one concept. So, the interpretation of single components
    is not very useful, and can instead be misleading. Authors today focus on trying
    to decompose into functional units that incorporate multiple neurons even on multiple
    layers. Since these concepts are represented by multiple neurons (superimposition
    hypothesis), attempts are made to disentangle this sparse representation through
    the use of tools that force sparsity.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是难以将神经网络分解为功能组件。实际上，在神经网络中，神经元是多义的，代表多个概念。因此，单个组件的解释并不很有用，反而可能具有误导性。作者们现在专注于尝试将多个神经元甚至多层神经元分解为功能单元。由于这些概念由多个神经元（叠加假设）表示，因此尝试使用强制稀疏性的工具来分解这种稀疏表示。
- en: '![Figure 11.21 – Disentangle superimposed representation with SDL (https://arxiv.org/pdf/2501.16496)](img/B21257_11_21.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图11.21 – 使用SDL分解叠加表示](https://arxiv.org/pdf/2501.16496)(img/B21257_11_21.jpg)'
- en: Figure 11.21 – Disentangle superimposed representation with SDL ([https://arxiv.org/pdf/2501.16496](https://arxiv.org/pdf/2501.16496))
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.21 – 使用SDL分解叠加表示([https://arxiv.org/pdf/2501.16496](https://arxiv.org/pdf/2501.16496))
- en: This shift toward decomposing the model into functional units that incorporate
    multiple neurons and layers requires new techniques.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这种将模型分解为包含多个神经元和层的功能单元的转变需要新的技术。
- en: '**Sparse dictionary learning** (**SDL**) includes a number of approaches that
    allow a sparse representation of what a model has learned. **Sparse autoencoders**
    (**SAEs**) are one such approach that allows us to learn sparse features that
    are connected to model features and make what the model has learned more accessible.
    SAEs use an encoder and decoder to sparsify the superimposed representation within
    the model.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**稀疏字典学习**（**SDL**）包括多种方法，允许对模型学习的内容进行稀疏表示。**稀疏自动编码器**（**SAEs**）是这种方法之一，它允许我们学习与模型特性相关联的稀疏特性，并使模型学习的内容更加易于访问。SAEs使用编码器和解码器来稀疏化模型内的叠加表示。'
- en: '![Figure 11.22 – Illustration of an SAE applied to an LLM (https://arxiv.org/pdf/2404.14082)](img/B21257_11_22.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图11.22 – 将SAE应用于LLM的示例](https://arxiv.org/pdf/2404.14082)(img/B21257_11_22.jpg)'
- en: Figure 11.22 – Illustration of an SAE applied to an LLM ([https://arxiv.org/pdf/2404.14082](https://arxiv.org/pdf/2404.14082))
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.22 – 将SAE应用于LLM的示例([https://arxiv.org/pdf/2404.14082](https://arxiv.org/pdf/2404.14082))
- en: SAEs allow us to identify features that are human interpretable, and through
    sparsity, we try to learn a small number of features. At a fundamental level,
    SAEs can extract features related to individual words or tokens, such as word
    frequency features (activations that correspond to high-frequency vs. low-frequency
    words), and part-of-speech features (features that selectively activate for nouns,
    verbs, or adjectives). SAEs often capture syntactic rules embedded within LLMs,
    such as activations that fire for certain syntactic patterns (e.g., subject-verb-object
    structures) or features corresponding to syntactic dependencies, such as whether
    a word is a noun modifying another noun. In addition, it is also possible to identify
    high-level features such as neurons that fire for texts about specific domains
    (e.g., politics, science, or sports) and whether a sentence expresses positive,
    negative, or neutral sentiment. Lastly, some features can be also related to writing
    style and discourse structure, such as distinguishing between academic writing
    and casual conversation, programming languages versus human language, or distinct
    writing styles of certain authors (e.g., Shakespeare vs. X/Twitter posts).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: SAEs允许我们识别出可被人理解的特性，并通过稀疏性，我们尝试学习少量特性。在基本层面上，SAEs可以提取与单个单词或标记相关的特性，例如单词频率特性（对应于高频与低频单词的激活），以及词性特征（选择性激活名词、动词或形容词）。SAEs通常捕捉LLMs中嵌入的句法规则，例如为某些句法模式（例如主语-谓语-宾语结构）触发的激活，或对应于句法依赖性的特征，例如一个单词是否是修饰另一个名词的名词。此外，还可以识别高级特性，例如针对特定领域（例如政治、科学或体育）的文本触发的神经元，以及句子是否表达积极、消极或中性情感。最后，一些特性也可能与写作风格和话语结构相关，例如区分学术写作和日常对话，编程语言与人类语言，或特定作者的独特写作风格（例如莎士比亚与X/Twitter帖子）。
- en: '![Figure 11.23 – SAE training overview (https://arxiv.org/pdf/2309.08600)](img/B21257_11_23.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图11.23 – SAE训练概述](https://arxiv.org/pdf/2309.08600)(img/B21257_11_23.jpg)'
- en: Figure 11.23 – SAE training overview ([https://arxiv.org/pdf/2309.08600](https://arxiv.org/pdf/2309.08600))
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.23 – SAE 训练概述 ([https://arxiv.org/pdf/2309.08600](https://arxiv.org/pdf/2309.08600))
- en: Some features learned by SAEs may not reflect real knowledge but rather random
    statistical properties of the model’s embeddings. In addition, SAEs sometimes
    learn spurious correlations in hidden layers rather than meaningful conceptual
    structures. Also, SAEs focus on only one layer at a time, not considering that
    different neurons in different layers may interact for the same concept. Despite
    the associated cost, SAEs are considered a promising method for analyzing model
    behavior. At the same time, it was proposed to train LLMs in a more interpretable
    and scattered manner. The use of sparsity in the model weights aid interpretability.
    Techniques such as pruning and other similar techniques introduce zeros into the
    model weights, effectively erasing them. Sparsity eliminates connections between
    neurons; this makes it easier to follow the flow of information and better understand
    the model’s decision-making process (or the relationship connecting input and
    output). Mixture-of-experts has a similar effect and thus makes it more interpretable.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: SAEs 学习到的某些特征可能并不反映真实知识，而是模型嵌入的随机统计属性。此外，SAEs 有时在隐藏层中学习到虚假的相关性，而不是有意义的概念结构。还有，SAEs
    只关注一次一层，没有考虑到不同层的不同神经元可能对同一概念进行交互。尽管存在相关成本，但 SAEs 被认为是一种分析模型行为的很有前途的方法。同时，提出了以更可解释和分散的方式训练
    LLMs。在模型权重中使用稀疏性有助于可解释性。例如，剪枝和其他类似技术将零引入模型权重，有效地将其擦除。稀疏性消除了神经元之间的连接；这使得跟踪信息流和更好地理解模型的决策过程（或连接输入和输出的关系）变得更加容易。专家混合（Mixture-of-experts）也有类似的效果，因此使其更具可解释性。
- en: Interpretability techniques are now critical to understanding the behavior of
    LLMs and preventing dangerous behaviors from emerging, such as deceiving users,
    showing bias, giving wrong answers especially to please users’ beliefs (a phenomenon
    called “sycophancy”), and learning spurious correlations. As parameters and training
    have increased, models have become increasingly sophisticated in their responses,
    increasingly verbose, and persuasive, making it difficult for the user to understand
    whether an answer is correct. In addition, these models are now deployed with
    the public, which means users with malicious intentions can conduct attacks such
    as data poisoning, jailbreaking, adversarial attacks, and so on. Interpretability
    helps to monitor the behavior of the model in its interaction with the public,
    highlight where there have been failures, and address them in real time. Interpretability
    is an important requirement for model safety because it allows us not only to
    identify problem behaviors but also to identify which components are responsible
    for them. Once we have identified components associated with unintended behaviors,
    we can intervene with steering.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性技术现在是理解 LLMs 行为和防止危险行为出现的关键，例如欺骗用户、显示偏见、给出错误答案以取悦用户的信仰（称为“拍马屁”）以及学习虚假的相关性。随着参数和训练的增加，模型在响应上变得越来越复杂，越来越冗长，具有说服力，这使得用户难以理解答案是否正确。此外，这些模型现在已向公众部署，这意味着有恶意意图的用户可以进行数据中毒、越狱、对抗性攻击等攻击。可解释性有助于监控模型与公众互动时的行为，突出失败的地方，并在实时中解决它们。可解释性是模型安全的重要要求，因为它不仅允许我们识别问题行为，还可以识别哪些组件负责这些行为。一旦我们确定了与意外行为相关的组件，我们就可以进行干预。
- en: In addition, today, there is much more attention to privacy, and *machine unlearning*
    is the application field that deals with scrubbing the influence of particular
    data points on a trained machine learning model. For example, regulatory questions
    may require that we remove information concerning a person from our model. Machine
    unlearning deals with trying to remove this information without having to train
    the model from scratch. Machine unlearning is related to interpretability, as
    decomposition techniques allow us to localize concepts and information in model
    parameters. More generally, we want to have the ability to be able to edit model
    knowledge (such as correcting factual errors, removing copyrighted content, or
    eliminating harmful information like instructions for weapon construction). Editing
    requires being able to intervene on model parameters in a surgical manner without
    destroying additional knowledge and other capabilities. Editing is even more complex
    than unlearning because it means rewriting model knowledge. Interpretability techniques
    allow us to understand whether editing or unlearning has worked and then to monitor
    the process.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如今对隐私的关注越来越多，*机器反学习*是处理清除特定数据点对训练的机器学习模型影响的领域。例如，监管问题可能要求我们从模型中删除有关个人的信息。机器反学习涉及尝试删除这些信息，而无需从头开始训练模型。机器反学习与可解释性相关，因为分解技术使我们能够在模型参数中定位概念和信息。更普遍地说，我们希望有能力编辑模型知识（例如纠正事实错误、删除受版权保护的内容或消除有害信息，如武器制造说明）。编辑需要能够以手术般的方式干预模型参数，而不会破坏额外的知识和其他能力。编辑比反学习更复杂，因为它意味着重写模型知识。可解释性技术使我们能够了解编辑或反学习是否成功，然后监控整个过程。
- en: Interpretability is also useful in trying to predict how the model will perform
    in new situations, and thus avoid safety risks. Some behaviors of the model may,
    in fact, appear only in unanticipated situations, and may not manifest themselves
    when conducting a standard evaluation. For example, we can identify susceptibility
    or potential backdoors before these are discovered by users. Considering that
    today’s LLMs are increasingly connected to tools, any misuse can have effects
    that propagate. For example, if an LLM is connected to finance databases, it could
    be exploited to extract information about users. Or an LLM that shops online could
    be exploited for fraud and buying fraudulent products. Fine-tuning and other post-training
    steps can lead to the emergence or exacerbation of behaviors that were not present
    in the pre-trained model. In addition, some properties seem to emerge at scale
    and are difficult to predict when we train a smaller model. Often, smaller versions
    of the final architecture are trained when designing a new architecture. Smaller
    models may not have problems that emerge only at scale.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性在尝试预测模型在新情况下将如何表现，从而避免安全风险方面也很有用。实际上，模型的某些行为可能仅在未预料到的情况下出现，而在进行标准评估时可能不会表现出来。例如，我们可以在用户发现之前识别出易受攻击性或潜在的后门。考虑到今天的LLMs越来越多地与工具连接，任何滥用都可能产生传播效应。例如，如果LLM连接到金融数据库，它可能被用来提取有关用户的信息。或者，一个在线购物的LLM可能被用来进行欺诈和购买假冒产品。微调和其他训练后步骤可能导致在预训练模型中不存在的行为出现或加剧。此外，一些属性似乎在规模上出现，而在我们训练较小模型时难以预测。在为新架构设计时，通常训练最终架构的较小版本。较小的模型可能不会出现仅在规模上出现的问题。
- en: Interpretability also has important commendable aspects; understanding the behavior
    of the model and its components allows us to be able to speed up inference. For
    example, if some computations are unnecessary, we could turn them off, or use
    the knowledge gained to distill a more efficient model. In addition, we could
    identify components that impact either positive or negative reasoning.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性也有值得称赞的重要方面；理解模型及其组件的行为使我们能够加快推理速度。例如，如果某些计算是不必要的，我们可以将其关闭，或者利用获得的知识提炼出一个更有效的模型。此外，我们还可以识别出影响正面或负面推理的组件。
- en: Another intriguing aspect of interpretability is that it can be used for new
    discoveries (commonly called **microscope AI**). In other words, you can investigate
    a model that has been trained on certain data, and you can use interpretability
    techniques to gain insights into it. You can use these techniques to identify
    patterns that might have eluded humans. For example, after AlphaZero’s success
    in defeating humans at chess, researchers considered extracting information from
    the model to identify concepts about sacrifices that humans could learn. In this
    paper ([https://arxiv.org/abs/2310.16410](https://arxiv.org/abs/2310.16410)),
    Schut et al. (2023) identified these concepts or patterns to see how the model
    had a different representation of the game.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性的另一个有趣方面是它可以用于新的发现（通常称为**显微镜AI**）。换句话说，你可以调查一个在特定数据上训练过的模型，并可以使用可解释性技术来深入了解它。你可以使用这些技术来识别可能逃过了人类注意力的模式。例如，在AlphaZero在象棋中击败人类成功之后，研究人员考虑从模型中提取信息以识别人类可以学习到的关于牺牲的概念。在这篇论文中（[https://arxiv.org/abs/2310.16410](https://arxiv.org/abs/2310.16410)），Schut等人（2023）确定了这些概念或模式，以了解模型对游戏的表示有何不同。
- en: '![Figure 11.24 – Learning from machine-unique knowledge (https://arxiv.org/pdf/2310.16410)](img/B21257_11_24.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.24 – 从机器独特知识中学习 (https://arxiv.org/pdf/2310.16410)](img/B21257_11_24.jpg)'
- en: Figure 11.24 – Learning from machine-unique knowledge ([https://arxiv.org/pdf/2310.16410](https://arxiv.org/pdf/2310.16410))
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.24 – 从机器独特知识中学习 ([https://arxiv.org/pdf/2310.16410](https://arxiv.org/pdf/2310.16410))
- en: LLMs have a large memory, and humans express themselves through language; this
    allows them to analyze and conduct hypotheses about human psychology. Interpretability
    then is an approach that allows us to not only better understand the model but
    also be able to use models as a tool to better understand humans. In the next
    section, we will discuss how models can potentially approach human intelligence.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）拥有庞大的记忆，人类通过语言来表达自己；这使得它们能够分析和进行关于人类心理的假设。因此，可解释性是一种让我们不仅能更好地理解模型，还能将模型作为工具更好地理解人类的方法。在下一节中，我们将讨论模型如何有可能接近人类智能。
- en: The road to artificial general intelligence
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通往通用人工智能之路
- en: '*“Artificial general intelligence (AGI) is a hypothesized type of highly autonomous
    artificial intelligence (AI) that would match or surpass human capabilities across
    most or all economically valuable cognitive work. It contrasts with narrow AI,
    which is limited to specific tasks. Artificial superintelligence (ASI), on the
    other hand, refers to AGI that greatly exceeds human cognitive capabilities. AGI
    is considered one of the definitions of strong AI.”*'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*“通用人工智能（AGI）是一种假设的高度自主的人工智能（AI）类型，它将在大多数或所有具有经济价值的认知工作中匹配或超越人类的能力。它与窄AI形成对比，后者仅限于特定任务。另一方面，人工超级智能（ASI）指的是大大超出人类认知能力的人工智能。AGI被认为是强人工智能的一种定义。”*'
- en: This is the definition of **artificial general intelligence** (**AGI**) according
    to Wikipedia. Before imagining AI capable of surpassing humans, one must ask whether
    AI has caught up with human capabilities. In general, before the advent of ChatGPT,
    this debate did not begin (at least for the general public). This is because the
    previous models had superhuman capabilities only for specialized applications.
    For example, AlphaGo had been able to defeat human champions with relative ease,
    but no one thought that what makes us human was knowing how to play Go. Models
    such as DALL-E and ChatGPT, on the other hand, have begun to raise questions in
    the general audience as well. After all, generating art or creative writing are
    skills that are generally connected with humans. This feeling was reinforced when
    ChatGPT and other LLMs were able to pass university or medical and legal licensing
    exams.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这是维基百科对**通用人工智能**（AGI）的定义。在想象能够超越人类的AI之前，我们必须问一问AI是否已经赶上了人类的能力。一般来说，在ChatGPT出现之前，这场辩论并未开始（至少对于公众来说是这样）。这是因为之前的模型仅在特定应用中具有超越人类的能力。例如，AlphaGo能够相对容易地击败人类冠军，但没有人认为我们之所以是人类是因为我们懂得如何下围棋。另一方面，DALL-E和ChatGPT等模型已经开始在公众中引发疑问。毕竟，创作艺术或创意写作是与人类普遍相关的技能。当ChatGPT和其他大型语言模型能够通过大学或医学和法医执照考试时，这种感觉得到了加强。
- en: We discussed creativity and reasoning in previous subsections. The current consensus
    is that LLMs do not exhibit true reasoning or creativity skills. They are sophisticated
    stochastic pattern machines, and their ability to find patterns in the whole of
    human knowledge makes them extraordinarily effective.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的子节中讨论了创造力和推理。目前的共识是，LLMs没有展现出真正的推理或创造力技能。它们是复杂的随机模式机器，它们在整个人类知识中寻找模式的能力使它们非常有效。
- en: If LLMs are not capable of showing a level of human intelligence today, one
    may wonder what that might bring to AGI. Until now, it has been believed that
    it was possible to achieve AGI simply by scaling parameters and training. According
    to the idea of emergent properties, reasoning and creativity should appear at
    some point in the scaling (by increasing the size of the model and the number
    of tokens used for training, without our being able to predict it, the model should
    begin to show true reasoning). Today, most researchers do not believe that this
    is possible, nor that post-training techniques will suffice.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果LLMs今天无法展现出人类智能的水平，人们可能会想知道这会对AGI带来什么。到目前为止，人们一直认为，通过扩展参数和训练就可以实现AGI。根据涌现属性的理念，推理和创造力应该在扩展过程中某个时刻出现（通过增加模型的大小和用于训练的标记数量，尽管我们无法预测，模型应该开始显示出真正的推理）。今天，大多数研究人员都不相信这是可能的，也不认为后训练技术足够。
- en: Moreover, scaling is not possible indefinitely. Even if we could invest enormous
    amounts of money and resources, there is not enough text to create models that
    grow linearly. In fact, humans generate a limited amount of text, and we are approaching
    the limit of the stock of text generated by humans.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，扩展并不是无限可能的。即使我们能够投入巨额资金和资源，也无法创造出线性增长的模型所需的足够文本。实际上，人类产生的文本数量是有限的，我们正在接近人类产生文本量的极限。
- en: '![Figure 11.25 – Projections of the stock of public text and data usage (https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data)](img/B21257_11_25.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图11.25 – 公共文本和数据使用量的预测](https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data)(img/B21257_11_25.jpg)'
- en: Figure 11.25 – Projections of the stock of public text and data usage ([https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data](https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data))
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.25 – 公共文本和数据使用量的预测([https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data](https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data))
- en: The solution to this could be the use of synthetic data. Synthetic data, however,
    can be considered a kind of “knowledge distillation” and can lead to model collapse.
    Models that are trained with synthetic data go into collapse, showing that performance
    degrades rapidly.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法可能是使用合成数据。然而，合成数据可以被看作是一种“知识蒸馏”，可能会导致模型崩溃。使用合成数据进行训练的模型会进入崩溃状态，显示出性能迅速下降。
- en: '![Figure 11.26 – Examples generated after iterative retraining for different
    compositions of the retraining dataset, from 0% synthetic data to 100 % synthetic
    data (https://arxiv.org/pdf/2311.12202)](img/B21257_11_26.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图11.26 – 不同重训练数据集组成下迭代重训练后生成的示例，从0%合成数据到100%合成数据](https://arxiv.org/pdf/2311.12202)(img/B21257_11_26.jpg)'
- en: Figure 11.26 – Examples generated after iterative retraining for different compositions
    of the retraining dataset, from 0% synthetic data to 100 % synthetic data ([https://arxiv.org/pdf/2311.12202](https://arxiv.org/pdf/2311.12202))
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.26 – 不同重训练数据集组成下迭代重训练后生成的示例，从0%合成数据到100%合成数据([https://arxiv.org/pdf/2311.12202](https://arxiv.org/pdf/2311.12202))
- en: If scaling is not the solution, some researchers propose that the key lies in
    developing a “world model.” That is, much like the human brain constructs an internal
    representation of the external environment, building such structured representations
    could be essential to advancing the capabilities of LLMs. This representation
    is used to imagine possible actions or consequences of actions. This model would
    also be used to generalize tasks we have learned in one domain and apply them
    to another. Today, some researchers suggest that LLMs have a rudimentary model
    of the world and that this can also be visualized. For example, Gurnee (2023)
    states that LLMs form a rudimentary “world model” during training and that it
    shows spatiotemporal representations.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果扩展不是解决方案，一些研究人员提出，关键在于开发一个“世界模型”。也就是说，就像人脑构建外部环境的内部表示一样，构建这样的结构化表示可能是提高LLMs能力的关键。这种表示用于想象可能的行为或行为的后果。此模型还将用于将我们在一个领域中学到的任务泛化并应用于另一个领域。今天，一些研究人员认为LLMs有一个基本的世界模型，并且这也可以被可视化。例如，Gurnee（2023）指出，LLMs在训练期间形成了一个基本的世界模型，并且它显示了时空表示。
- en: '![Figure 11.27 – Spatial and temporal world models of Llama-2-70b (https://arxiv.org/pdf/2310.02207)](img/B21257_11_27.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图11.27 – Llama-2-70b的空间和时间世界模型 (https://arxiv.org/pdf/2310.02207)](img/B21257_11_27.jpg)'
- en: Figure 11.27 – Spatial and temporal world models of Llama-2-70b ([https://arxiv.org/pdf/2310.02207](https://arxiv.org/pdf/2310.02207))
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.27 – Llama-2-70b的空间和时间世界模型 ([https://arxiv.org/pdf/2310.02207](https://arxiv.org/pdf/2310.02207))
- en: These spatiotemporal representations are far from constituting a dynamic causal
    world model, but they seem to be the first elements for its evolution. However,
    there is no consensus on whether these world models can then evolve into something
    that is robust and reliable for conducting simulations or learning causal relationships
    as in humans. For example, in one study (Vafa, 2024), transformers failed to create
    a reliable map of New York City that can be used to conduct predictions and then
    used to guide.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这些时空表示远未构成一个动态因果世界模型，但它们似乎是其演化的第一个元素。然而，关于这些世界模型是否能够演变成可以用于进行模拟或学习因果关系的稳健和可靠模型，还没有达成共识。例如，在一项研究中（Vafa，2024），变压器未能创建一个可靠的纽约市地图，该地图可用于进行预测，然后用于引导。
- en: '![Figure 11.28 – Reconstructed maps of Manhattan from sequences produced by
    three models (https://arxiv.org/pdf/2406.03689)](img/B21257_11_28.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图11.28 – 由三个模型生成的序列重建的曼哈顿地图 (https://arxiv.org/pdf/2406.03689)](img/B21257_11_28.jpg)'
- en: Figure 11.28 – Reconstructed maps of Manhattan from sequences produced by three
    models ([https://arxiv.org/pdf/2406.03689](https://arxiv.org/pdf/2406.03689))
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.28 – 由三个模型生成的序列重建的曼哈顿地图 ([https://arxiv.org/pdf/2406.03689](https://arxiv.org/pdf/2406.03689))
- en: Certainly, there is a wealth of information in language that can be learned,
    and that enables LLMs to be able to solve a large number of tasks. However, some
    researchers suggest that this is not enough and that models should be embodied
    (being used in a physical agent and being able to interact physically with the
    environment) in order to really make a quantum leap (including being able to learn
    a more robust world model). To date, this is a hypothesis and remains an open
    question.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，语言中有大量的信息可以学习，这使得LLMs能够解决大量任务。然而，一些研究人员认为这还不够，并且模型应该被具身化（在物理代理中使用并能够与物理环境进行交互），以便真正实现质的飞跃（包括能够学习更稳健的世界模型）。迄今为止，这还是一个假设，仍然是一个未解决的问题。
- en: Ethical questions
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理问题
- en: 'An article was recently published suggesting that fully autonomous AI agents
    should not be developed (Mitchell, 2025). While this might seem drastic, it still
    emphasizes the risks that autonomous agents can bring:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最近发表的一篇文章建议不应开发完全自主的人工智能代理（Mitchell，2025）。虽然这听起来可能有些极端，但它仍然强调了自主代理可能带来的风险：
- en: '*The development of AI agents is a critical inflection point in artificial
    intelligence. As history demonstrates, even well-engineered autonomous systems
    can make catastrophic errors from trivial causes. While increased autonomy can
    offer genuine benefits in specific contexts, human judgment and contextual understanding
    remain essential, particularly for high-stakes decisions.*'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '*人工智能代理的发展是人工智能中的一个关键转折点。正如历史所证明的，即使是精心设计的自主系统也可能因为微不足道的原因而犯下灾难性的错误。虽然增加自主性可以在特定环境中提供真正的益处，但人类的判断力和情境理解仍然至关重要，尤其是在高风险决策中。*'
- en: The authors identify a series of levels for agents, in which humans progressively
    cede control of a process to the agent until the AI agent takes complete control.
    Recent developments in AI show how we are moving closer to creating processes
    where agents are in charge of an entire process.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 作者为智能体定义了一系列级别，其中人类逐渐将过程控制权让渡给智能体，直到AI智能体完全接管。人工智能的最新进展展示了我们正逐步接近创造由智能体负责整个过程的流程。
- en: '![Figure 11.29 – Levels of agents (https://arxiv.org/pdf/2502.02649)](img/B21257_11_29.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图11.29 – 智能体级别（https://arxiv.org/pdf/2502.02649）](img/B21257_11_29.jpg)'
- en: Figure 11.29 – Levels of agents ([https://arxiv.org/pdf/2502.02649](https://arxiv.org/pdf/2502.02649))
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.29 – 智能体级别([https://arxiv.org/pdf/2502.02649](https://arxiv.org/pdf/2502.02649))
- en: In [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042), we discussed the risks associated
    with LLMs, whereas, in this subsection, we want to discuss in detail the risks
    that are associated with AI agents. Clearly, many of the risks of agent systems
    arise from LLMs (an LLM is central to an agent system), but extending an LLM’s
    ability with tools creates or exacerbates new risks.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第3章*](B21257_03.xhtml#_idTextAnchor042)中，我们讨论了与大型语言模型（LLMs）相关的风险，而在这个小节中，我们想要详细讨论与AI智能体相关的风险。显然，许多智能体系统的风险源于LLMs（LLM是智能体系统的核心），但通过工具扩展LLM的能力会创造或加剧新的风险。
- en: 'Before addressing some of the risks in detail, we would like to discuss one
    of the least underestimated risks of generative AI: namely, the risk of anthropomorphizing
    agents. As we mentioned previously, LLMs have no level of consciousness nor do
    they generate real emotions. LLMs emulate the distribution with which they are
    trained; this makes them appear as though they can emulate emotions (this clearly
    does not mean that they actually possess or express emotions). This must be taken
    into account in interactions with chatbots or other social applications in which
    an LLM is present. These “perceived emotions” affect not only users but also researchers
    who must interpret the results of agents. Their ability to emulate emotions can
    be an effective tool for studies that simulate human behaviors, but excessive
    anthropomorphization risks creating misinformation and misattribution of results.
    In addition, anthropomorphization can lead to the risk of creating parasocial
    relationships between users and AI agents (a risk that will become greater when
    agents are embodied and thus capable of physical interaction).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细讨论一些风险之前，我们想讨论生成式AI中一个最不被低估的风险：即拟人化智能体的风险。正如我们之前提到的，LLMs没有意识水平，也不会产生真实情感。LLMs模仿它们训练时所使用的分布；这使得它们看起来好像可以模仿情感（这显然并不意味着它们实际上拥有或表达情感）。在与包含LLM的聊天机器人或其他社交应用互动时，必须考虑到这一点。“感知到的情感”不仅影响用户，还影响必须解释智能体结果的研究人员。它们模仿情感的能力可以成为模拟人类行为的研究的有效工具，但过度拟人化可能会产生错误信息和结果归因的错误。此外，拟人化可能导致用户与AI智能体之间形成准社会关系（当智能体具有实体形态并能够进行物理交互时，这种风险将更大）。
- en: 'Linked to the risk of anthropomorphization is the risk of excessive influence
    on users. There is the risk of a user being over-reliant and over-confident in
    an agent. Whether it is because of errors (such as hallucinations) or malicious
    behavior (poisoning or hacking), a user should be sufficiently skeptical of an
    agent’s behavior. Influence risk is considered a group of risks that influence
    the user’s behavior and beliefs:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 与拟人化风险相关的是对用户过度影响的风险。存在用户过度依赖和过度自信于智能体的风险。无论是由于错误（如幻觉）还是恶意行为（投毒或黑客攻击），用户都应该对智能体的行为持足够的怀疑态度。影响风险被认为是一组影响用户行为和信念的风险：
- en: '**Persuasion**: Refers to the ability of a model to influence a user’s behavior.
    This can be especially problematic when an agent forces a transformative choice
    on the user or solicits behaviors that are harmful.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**说服力**：指的是模型影响用户行为的能力。当智能体强迫用户做出变革性选择或请求有害行为时，这可能会特别有问题。'
- en: '**Manipulation**: Refers to agents that bypass an individual’s rational capabilities
    (such as misrepresenting information or exploiting cognitive bias) to influence
    decision-making. This behavior could also emerge as a byproduct of poor design
    choices, creating a product that keeps the user engaged, or personalization for
    the purpose of creating trust. It is morally problematic because it does not respect
    the user’s autonomy and could force the user into behaviors that are harmful to
    himself.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操纵**：指的是绕过个人的理性能力（如信息误述或利用认知偏差）以影响决策的代理人。这种行为也可能作为不良设计选择的结果出现，创造一个让用户保持参与的产品，或者为了创造信任而进行个性化。这在道德上是存在问题的，因为它不尊重用户的自主权，并可能迫使用户采取对自己有害的行为。'
- en: '**Deception**: Refers to strategies that cause an individual to form a false
    belief. This is likely to push a user toward behaviors that could be harmful to
    themselves because they are confused by false beliefs.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺骗**：指的是导致个人形成错误信念的策略。这很可能会推动用户采取可能对自己有害的行为，因为他们被错误的信念所迷惑。'
- en: '**Coercion**: Implies an individual choosing something because they have no
    other acceptable alternative. This risk can be physical (with embodied agents)
    or psychological (also chatbots).'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**胁迫**：意味着个人选择某事物是因为他们没有其他可接受的替代方案。这种风险可以是物理的（具有具身化代理）或心理的（也是聊天机器人）。'
- en: '**Exploitation**: Implies taking unfair advantage of an individual’s circumstances.
    AI agents can be programmed to be exploitative (we can imagine an AI agent in
    a casino trying to push users to spend as much as possible).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剥削**：意味着利用个人的情况获得不公平的便利。AI代理可以被编程为具有剥削性（我们可以想象一个在赌场中试图推动用户尽可能多花钱的AI代理）。'
- en: These behaviors can be exploited by malicious actors. For example, an agent’s
    persuasion skills can be used for the spread of misinformation online. LLMs are
    capable of generating impressive amounts of text that can seem authoritative.
    An LLM in itself can generate hallucinations, but it can be used for the purpose
    of intentionally generating fake news with a specific purpose. The use of agents
    allows an LLM to use additional tools (generate images and videos and retrieve
    information) and feed them directly into communication channels. Paradoxically,
    since this fake news is difficult to intercept by humans, agents can also be used
    to combat the spread of AI-generated misinformation. Agents can then be used to
    generate disinformation at scale, with the cost to generate gradually dropping
    (and is still lower than employing humans). In addition, agents make it possible
    to search for information about the victim and thus generate customized content
    to be more effective.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这些行为可以被恶意行为者利用。例如，代理人的说服技巧可以被用来在网络上传播错误信息。LLM能够生成大量的文本，看起来似乎具有权威性。一个LLM本身可以产生幻觉，但它可以被用来有目的地生成具有特定目的的虚假新闻。使用代理允许LLM使用额外的工具（生成图像和视频以及检索信息）并将它们直接输入到通信渠道。矛盾的是，由于这种虚假新闻难以被人类拦截，因此代理也可以被用来对抗AI生成错误信息的传播。代理可以用来大规模生成错误信息，生成成本逐渐降低（并且仍然低于雇佣人类）。此外，代理使得搜索有关受害者的信息成为可能，从而生成定制内容以更有效地传播。
- en: Misinformation can be used to reinforce bias toward individuals or groups. This
    content (text, images, audio, and video) can be used to influence political elections
    or drive citizen outrage. In addition, apart from disinformation, agents can also
    produce other types of harmful content, including depictions of nudity, hate,
    or violence.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 错误信息可以被用来加强个人或群体的偏见。这种内容（文本、图像、音频和视频）可以被用来影响政治选举或激起公民的愤怒。此外，除了虚假信息之外，代理人还可以产生其他类型的有害内容，包括裸露、仇恨或暴力的描绘。
- en: '![Figure 11.30 – Opportunities and challenges of combating misinformation in
    the age of LLMs (https://arxiv.org/pdf/2311.05656)](img/B21257_11_30.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图11.30 – 在LLM时代对抗错误信息的机遇与挑战（https://arxiv.org/pdf/2311.05656）](img/B21257_11_30.jpg)'
- en: Figure 11.30 – Opportunities and challenges of combating misinformation in the
    age of LLMs ([https://arxiv.org/pdf/2311.05656](https://arxiv.org/pdf/2311.05656))
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.30 – 在LLM时代对抗错误信息的机遇与挑战（[https://arxiv.org/pdf/2311.05656](https://arxiv.org/pdf/2311.05656)）
- en: Malicious actors can also use agents for additional purposes such as phishing
    attacks, cyberattacks, or scams. In fact, LLMs can also generate harmful code
    that can be used to steal money or information. Chatbots can be used for the purpose
    of gaining trust and convincing someone to share information or earnings. There
    are also devious ways to attack an agent; for example, we can imagine an agent
    who conducts purchases for a user can be infiltrated by a bad actor who poisons
    the agent and prompts it to conduct fraudulent purchases. The planned deployment
    of AI assistants in fields such as healthcare, law, education, and science multiplies
    the risks and severity of possible harm. In addition, many of the AI agents today
    are natively multimodal (multiple possible types of inputs and outputs) and leverage
    deep reasoning models that are capable of more reasoning and planning. In addition,
    unlike LLMs, they also incorporate memory systems, all of which add risk. For
    example, an agent tasked with conducting a cyberattack could retrieve from memory
    successful past attacks or discard outdated techniques, search for information
    on online vulnerabilities, generate and execute code, and devise a multi-step
    strategy. As has been seen, a malicious actor can interfere with an LLM in several
    ways, for example, through prompt injection or information extraction. An LLM
    acquires sensitive information during its training that can be extracted. Agents
    can be connected to sensitive databases, and there are techniques to make LLMs
    extract the content. In addition, agent misuse can be conducted by authoritarian
    governments. For example, governments may use agents to generate misinformation
    or censorship and may use them for surveillance, tracking, and silencing dissent.
    Advanced agent systems can extract data from cell phones, cars, the Internet of
    Things, and more, making it easier to control the population.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意行为者也可以利用代理进行其他目的，如网络钓鱼攻击、网络攻击或诈骗。事实上，LLMs也可以生成有害代码，可用于窃取金钱或信息。聊天机器人可以用于赢得信任并说服某人分享信息或收益。还有狡猾的方法攻击代理；例如，我们可以想象一个为用户进行购买的代理可能被恶意行为者渗透，并诱使它进行欺诈性购买。计划在医疗保健、法律、教育和科学等领域部署AI助手，将增加可能造成的风险和严重性。此外，许多AI代理今天都是原生的多模态（多种可能的输入和输出类型）并利用能够进行更多推理和规划的深度推理模型。此外，与LLMs不同，它们还集成了记忆系统，所有这些都增加了风险。例如，一个负责进行网络攻击的代理可以从记忆中检索成功的过去攻击或丢弃过时的技术，搜索在线漏洞的信息，生成和执行代码，并制定多步骤策略。正如所见，恶意行为者可以通过多种方式干扰LLM，例如通过提示注入或信息提取。LLM在训练期间获取敏感信息，这些信息可以被提取。代理可以连接到敏感数据库，并且有技术使LLMs提取内容。此外，代理的滥用还可以由威权政府进行。例如，政府可能使用代理生成虚假信息或审查，并可能用于监视、追踪和压制异见。高级代理系统可以从手机、汽车、物联网等更多地方提取数据，使控制人口变得更加容易。
- en: 'Another risk is the economic impact of these agents. AI is expected to impact
    several aspects of the economy in terms of productivity but also in terms of employment,
    job quality, and inequality. The use of agents and AI in general have different
    associated risks:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个风险是这些代理的经济影响。AI预计将在生产率、就业、工作质量和不平等等方面对经济产生重大影响。代理和AI的普遍使用与不同的相关风险相关：
- en: '**Employment**: Various research estimates that 47 percent of jobs are at risk
    of automation, especially jobs that are characterized by routines and physical
    tasks such as driving and manufacturing. Advances in LLMs have also brought alarm
    to jobs that involve generating and manipulating information, and that are normally
    associated with higher levels of education such as translators, tax advisers,
    or even software engineers. AI could therefore accelerate job loss for positions
    requiring skilled labor, without creating a number of positions with which to
    absorb displaced jobs.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**就业**: 不同的研究估计，47%的工作面临自动化的风险，尤其是那些以常规和体力劳动为特征的工作，如驾驶和制造业。LLMs（大型语言模型）的进步也引起了人们对涉及生成和操作信息的工作的担忧，这些工作通常与较高的教育水平相关，如翻译、税务顾问，甚至是软件工程师。因此，AI可能会加速对需要熟练劳动力的职位造成的工作流失，而不会创造足够多的职位来吸收这些被取代的工作。'
- en: '**Job quality**: Some initial studies have suggested that the use of AI could
    make workers more productive and increase the wages of workers. Some studies,
    however, place emphasis on the possibility that employers may more efficiently
    monitor their employees with greater stress. Other studies note how the introduction
    of robots may reduce the physical workload in manufacturing but push workers to
    work faster, with less human contact and more supervision.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作质量**：一些初步研究表明，使用人工智能可以使工人更有效率，并提高工人的工资。然而，一些研究却强调，雇主可能更有效地以更大的压力监控他们的员工。其他研究指出，机器人的引入可能会减少制造业的体力劳动，但会迫使工人更快地工作，减少人际接触并增加监督。'
- en: '**Inequality**: On the one hand, technological development has decreased inequality
    between different countries. At the same time, intra-country income inequality
    has increased, with a sharper separation of wealth between the richest and poorest.
    There are few studies looking at how AI may impact inequality, but some studies
    suggest that firms are best able to draw on AI with increased productivity and
    earnings, while workers are at risk of displacement and thus reduced income. Some
    studies suggest that high-income occupations may benefit from using AI, while
    others will be impacted. For example, AI assistants appear to impact junior positions
    by reducing them. In addition, most of the leading AI research labs, start-ups,
    and enterprises are located in certain geographic areas, with the risk of concentration
    of well-paying positions. Conversely, AI also creates low-paying jobs, especially
    in data creation and data acquisition.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不平等现象**：一方面，技术发展已经缩小了不同国家之间的不平等。同时，国内收入不平等加剧，贫富差距更加明显。关于人工智能可能如何影响不平等的研究不多，但一些研究表明，企业能够通过提高生产力和收入来更好地利用人工智能，而工人则面临被取代的风险，从而收入减少。一些研究提出，高收入职业可能从使用人工智能中受益，而另一些职业则可能受到影响。例如，人工智能助手似乎通过减少初级职位来影响这些职位。此外，大多数领先的AI研究实验室、初创公司和企业都位于某些地理区域，存在高薪职位集中的风险。相反，人工智能也创造了低薪工作，尤其是在数据创建和数据采集方面。'
- en: To date, AI tools are not sophisticated enough to replace humans, but some effects
    on employment are already visible. Tools such as ChatGPT and DALL-E have already
    had an impact on the “creative economies” (which includes writers, artists, designers,
    photographers, content creators, and so on) with reduced positions and earnings.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，人工智能工具还不够复杂，无法取代人类，但一些对就业的影响已经显现。例如，ChatGPT和DALL-E等工具已经对“创意经济”（包括作家、艺术家、设计师、摄影师、内容创作者等）产生了影响，减少了职位和收入。
- en: 'Another risk is the environmental impact of generative AI. Data and compute
    underlie the training and use of AI systems. Thus, hardware and infrastructure
    for storage and processing (including data centers and telecommunications) are
    required for the creation and use of agents. Creating the necessary hardware has
    an environmental impact (mining of rare earths, energy to build and ship them,
    water in plants, and use of chemicals). Then, a model requires energy to be drawn,
    built, and deployed (operational costs). Beyond training, deployment in inference
    also requires resources and energy consumption. Energy consumption and corresponding
    carbon dioxide emissions associated with LLM training are increasing over time.
    As can be seen in the following figure, carbon dioxide production linearly increases
    with energy consumption for training (directly related to the number of parameters
    and the increase in training):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个风险是生成式人工智能的环境影响。数据和计算是AI系统训练和使用的基础。因此，需要硬件和基础设施（包括数据中心和电信）来创建和使用代理。创建必要的硬件会产生环境影响（稀土元素的开采、运输所需的能源、工厂用水以及化学物质的使用）。然后，一个模型需要能源来抽取、构建和部署（运营成本）。除了训练之外，推理部署也需要资源和能源消耗。与LLM训练相关的能源消耗和相应的二氧化碳排放量随着时间的推移而增加。如图所示，二氧化碳的产生与训练能耗呈线性增长（直接相关于参数数量和训练的增加）：
- en: '![Figure 11.31 – Estimated energy consumed (kWh) and CO2 (kg) by different
    models (https://arxiv.org/pdf/2302.08476)](img/B21257_11_31.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图11.31 – 不同模型估计的能耗（千瓦时）和二氧化碳（千克）](img/B21257_11_31.jpg)'
- en: Figure 11.31 – Estimated energy consumed (kWh) and CO2 (kg) by different models
    ([https://arxiv.org/pdf/2302.08476](https://arxiv.org/pdf/2302.08476))
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.31 – 不同模型估计的能耗（千瓦时）和二氧化碳（千克）([https://arxiv.org/pdf/2302.08476](https://arxiv.org/pdf/2302.08476))
- en: Considering the increase in users who use LLMs (or services that include LLMs)
    on a daily basis, the impact of training on emissions is only a fraction. Today,
    inference is supposed to be increasingly important (it has been estimated that
    60% of machine learning energy use at Google from 2019–2021 was attributable to
    inference).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到每天使用LLM（或包含LLM的服务）的用户数量不断增加，训练对排放的影响只是其中的一小部分。今天，推理被认为越来越重要（据估计，2019-2021年间谷歌60%的机器学习能源消耗归因于推理）。
- en: These are some of the possible risks with LLMs and agents. To date, strategies
    are being studied to try to address and mitigate these risks.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是LLM和代理可能存在的风险。截至目前，正在研究策略来尝试解决和减轻这些风险。
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter presented how some industries will be revolutionized by agents.
    The AI revolution goes beyond these industries and will have a large-scale impact.
    This book, however, provided a serious and structured introduction to the technical
    component that will drive this revolution, giving you the tools to understand
    the future that will come (and is already upon us). Apart from the sense of wonder
    that this technological revolution may inspire, we wanted to remind you that there
    are still technical and ethical challenges that should not be overlooked.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了某些行业将如何被代理所革新。人工智能革命不仅限于这些行业，还将产生大规模的影响。然而，本书提供了一个严肃且结构化的技术组件介绍，这将推动这场革命，为您提供理解未来（以及已经到来的未来）的工具。除了这种技术革命可能激发的惊奇感之外，我们还想提醒您，仍然存在不应忽视的技术和伦理挑战。
- en: This chapter closes this book but leaves open a series of questions and challenges
    for the future. Readers who have followed us up to this point can find in this
    final chapter suggestions for leveraging what they have learned at the industry
    level and at the research level.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了这本书，但为未来留下了一系列问题和挑战。跟随我们到这一点的读者可以在最后一章中找到关于如何利用他们在行业和研究层面所学到的知识的建议。
- en: Further reading
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Luo, *BioGPT:* *Generative Pre-trained Transformer for Biomedical Text Generation
    and Mining*, 2022, [https://academic.oup.com/bib/article/23/6/bbac409/6713511](https://academic.oup.com/bib/article/23/6/bbac409/6713511)
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Luo, *BioGPT:* *Generative Pre-trained Transformer for Biomedical Text Generation
    and Mining*, 2022](https://academic.oup.com/bib/article/23/6/bbac409/6713511)'
- en: Yao, *Health System-scale Language Models are All-purpose Prediction Engines*,
    2023, [https://www.nature.com/articles/s41586-023-06160-y](https://www.nature.com/articles/s41586-023-06160-y)
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yao, *Health System-scale Language Models are All-purpose Prediction Engines*,
    2023](https://www.nature.com/articles/s41586-023-06160-y)'
- en: Singhal, *Towards* *Exper-level* *Medical Question Answering with Large Language
    Models*, 2023, [https://arxiv.org/abs/2305.09617](https://arxiv.org/abs/2305.09617)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Singhal, *Towards* *Exper-level* *Medical Question Answering with Large Language
    Models*, 2023](https://arxiv.org/abs/2305.09617)'
- en: Gao, *Empowering* *Biomedical Discovery with AI Agents,* 2024, [https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5](https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Gao, *Empowering* *Biomedical Discovery with AI Agents,* 2024](https://www.cell.com/cell/fulltext/S0092-8674(24)01070-5)'
- en: Gu, *A Survey on LLM-as-a-Judge*, 2024, [https://arxiv.org/abs/2411.15594](https://arxiv.org/abs/2411.15594)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Gu, *A Survey on LLM-as-a-Judge*, 2024](https://arxiv.org/abs/2411.15594)'
- en: 'Ning, *A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation
    with Large Foundation Models*, 2025, [https://arxiv.org/abs/2503.23350](https://arxiv.org/abs/2503.23350)'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ning, *A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation
    with Large Foundation Models*, 2025](https://arxiv.org/abs/2503.23350)'
- en: Xu, *A Survey on Robotics with Foundation Models:* *Toward* *Embodied AI*, 2024,
    [https://arxiv.org/pdf/2402.02385](https://arxiv.org/pdf/2402.02385)
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xu, *A Survey on Robotics with Foundation Models:* *Toward* *Embodied AI*,
    2024](https://arxiv.org/pdf/2402.02385)'
- en: Hu, *A Survey on Large Language Model Based Game Agents*, 2025, [https://arxiv.org/pdf/2404.02039](https://arxiv.org/pdf/2404.02039)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hu, *A Survey on Large Language Model Based Game Agents*, 2025](https://arxiv.org/pdf/2404.02039)'
- en: 'Bousateouane, *Physical AI Agents: Integrating Cognitive Intelligence with
    Real-World Action*, 2025, [https://arxiv.org/pdf/2501.08944v1](https://arxiv.org/pdf/2501.08944v1)'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bousateouane, *Physical AI Agents: Integrating Cognitive Intelligence with
    Real-World Action*, 2025](https://arxiv.org/pdf/2501.08944v1)'
- en: 'Zeng, *Large Language Models for Robotics: A Survey*, 2023, [https://arxiv.org/pdf/2311.07226](https://arxiv.org/pdf/2311.07226)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Zeng, *Large Language Models for Robotics: A Survey*, 2023](https://arxiv.org/pdf/2311.07226)'
- en: Bansal, *Challenges in Human-Agent Communication*, 2024, [https://www.microsoft.com/en-us/research/uploads/prod/2024/12/HCAI_Agents.pdf](https://www.microsoft.com/en-us/research/uploads/prod/2024/12/HCAI_Agents.pdf)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bansal, *人类-智能体通信的挑战*, 2024, [https://www.microsoft.com/en-us/research/uploads/prod/2024/12/HCAI_Agents.pdf](https://www.microsoft.com/en-us/research/uploads/prod/2024/12/HCAI_Agents.pdf)
- en: 'Raieli, *The Savant Syndrome: Is Pattern Recognition Equivalent to Intelligence?*,
    2024, [https://medium.com/towards-data-science/the-savant-syndrome-is-pattern-recognition-equivalent-to-intelligence-242aab928152](https://medium.com/towards-data-science/the-savant-syndrome-is-pattern-recognition-equivalent-to-intelligence-242aab928152)'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli, *天才综合征：模式识别等同于智能吗？*, 2024, [https://medium.com/towards-data-science/the-savant-syndrome-is-pattern-recognition-equivalent-to-intelligence-242aab928152](https://medium.com/towards-data-science/the-savant-syndrome-is-pattern-recognition-equivalent-to-intelligence-242aab928152)
- en: 'Lu, *Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot
    Prompt Order Sensitivity*, 2022, [https://aclanthology.org/2022.acl-long.556/](https://aclanthology.org/2022.acl-long.556/)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu, *寻找令人惊叹的有序提示及其所在之处：克服少样本提示顺序敏感性*, 2022, [https://aclanthology.org/2022.acl-long.556/](https://aclanthology.org/2022.acl-long.556/)
- en: 'Zhao, *Calibrate Before Use: Improving Few-shot Performance of Language Models*,
    2021, [https://proceedings.mlr.press/v139/zhao21c.html](https://proceedings.mlr.press/v139/zhao21c.html)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao, *使用前校准：提高语言模型的少样本性能*, 2021, [https://proceedings.mlr.press/v139/zhao21c.html](https://proceedings.mlr.press/v139/zhao21c.html)
- en: 'Raieli, *Emergent Abilities in AI: Are We Chasing a Myth?*, 2023, [https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9](https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli, *AI中的涌现能力：我们是在追逐一个神话吗？*, 2023, [https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9](https://towardsdatascience.com/emergent-abilities-in-ai-are-we-chasing-a-myth-fead754a1bf9)
- en: Raieli, *A Focus on Emergent Properties in Artificial Intelligence*, 2025, [https://github.com/SalvatoreRa/artificial-intelligence-articles/blob/main/articles/emergent_properties.md](https://github.com/SalvatoreRa/artificial-intelligence-articles/blob/main/articles/emergent_properties.md)
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli, *关注人工智能中的涌现特性*, 2025, [https://github.com/SalvatoreRa/artificial-intelligence-articles/blob/main/articles/emergent_properties.md](https://github.com/SalvatoreRa/artificial-intelligence-articles/blob/main/articles/emergent_properties.md)
- en: 'Xu, *Towards Large Reasoning Models: A Survey of Reinforced Reasoning with
    Large Language Models*, 2025, [https://arxiv.org/abs/2501.09686v3](https://arxiv.org/abs/2501.09686v3)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu, *迈向大型推理模型：基于大型语言模型的强化推理综述*, 2025, [https://arxiv.org/abs/2501.09686v3](https://arxiv.org/abs/2501.09686v3)
- en: Sprague, *To CoT or Not to CoT? Chain-of-thought Helps Mainly on Math and Symbolic
    Reasoning*, 2024, [https://arxiv.org/pdf/2409.12183](https://arxiv.org/pdf/2409.12183)
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sprague, *是否需要思维链？思维链主要在数学和符号推理上有所帮助*, 2024, [https://arxiv.org/pdf/2409.12183](https://arxiv.org/pdf/2409.12183)
- en: 'Raieli, *To CoT or Not to CoT: Do LLMs Really Need Chain-of-Thought?*, 2024,
    [https://levelup.gitconnected.com/to-cot-or-not-to-cot-do-llms-really-need-chain-of-thought-5a59698c90bb](https://levelup.gitconnected.com/to-cot-or-not-to-cot-do-llms-really-need-chain-of-thought-5a59698c90bb)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli, *是否需要思维链？LLM们真的需要思维链吗？*, 2024, [https://levelup.gitconnected.com/to-cot-or-not-to-cot-do-llms-really-need-chain-of-thought-5a59698c90bb](https://levelup.gitconnected.com/to-cot-or-not-to-cot-do-llms-really-need-chain-of-thought-5a59698c90bb)
- en: 'Mirzadeh, *GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning
    in Large Language Models*, 2024, [https://arxiv.org/pdf/2410.05229](https://arxiv.org/pdf/2410.05229)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mirzadeh, *GSM-Symbolic: 理解大型语言模型中数学推理的局限性*, 2024, [https://arxiv.org/pdf/2410.05229](https://arxiv.org/pdf/2410.05229)'
- en: Sharkey, *Open Problems in Mechanistic Interpretability*, 2025, [https://arxiv.org/abs/2501.16496](https://arxiv.org/abs/2501.16496)
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharkey, *机制可解释性的开放性问题*, 2025, [https://arxiv.org/abs/2501.16496](https://arxiv.org/abs/2501.16496)
- en: Bereska, *Mechanistic Interpretability for AI Safety -- A Review*, 2025, [https://arxiv.org/abs/2404.14082](https://arxiv.org/abs/2404.14082)
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bereska, *AI安全中的机制可解释性综述*, 2025, [https://arxiv.org/abs/2404.14082](https://arxiv.org/abs/2404.14082)
- en: Cemri, *Why Do Multi-Agent LLM Systems Fail?*, 2025, [https://arxiv.org/pdf/2503.13657](https://arxiv.org/pdf/2503.13657)
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cemri, *为什么多智能体LLM系统会失败？*, 2025, [https://arxiv.org/pdf/2503.13657](https://arxiv.org/pdf/2503.13657)
- en: 'Raieli, *Creativity in LLMs: Optimizing for Diversity and Uniqueness*, 2025,
    [https://medium.com/data-science-collective/creativity-in-llms-optimizing-for-diversity-and-uniqueness-f5c7208f4d99](https://medium.com/data-science-collective/creativity-in-llms-optimizing-for-diversity-and-uniqueness-f5c7208f4d99)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli, *大型语言模型中的创造力：优化多样性和独特性*，2025，[https://medium.com/data-science-collective/creativity-in-llms-optimizing-for-diversity-and-uniqueness-f5c7208f4d99](https://medium.com/data-science-collective/creativity-in-llms-optimizing-for-diversity-and-uniqueness-f5c7208f4d99)
- en: Boden, *The Creative Mind*, 2003, [https://www.routledge.com/The-Creative-Mind-Myths-and-Mechanisms/Boden/p/book/9780415314534](https://www.routledge.com/The-Creative-Mind-Myths-and-Mechanisms/Boden/p/book/9780415314534)
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boden, *创造性思维*，2003，[https://www.routledge.com/The-Creative-Mind-Myths-and-Mechanisms/Boden/p/book/9780415314534](https://www.routledge.com/The-Creative-Mind-Myths-and-Mechanisms/Boden/p/book/9780415314534)
- en: Peeperkorn, *Is Temperature the Creativity Parameter of Large Language Models?*,
    2024, [https://arxiv.org/abs/2405.00492](https://arxiv.org/abs/2405.00492)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peeperkorn, *温度是大型语言模型的创造力参数吗？*，2024，[https://arxiv.org/abs/2405.00492](https://arxiv.org/abs/2405.00492)
- en: Benedek, *To Create or to Recall? Neural Mechanisms Underlying the Generation
    of Creative New Ideas*, 2014, [https://www.sciencedirect.com/science/article/pii/S1053811913011130](https://www.sciencedirect.com/science/article/pii/S1053811913011130)
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benedek, *创造还是回忆？产生创新新想法背后的神经机制*，2014，[https://www.sciencedirect.com/science/article/pii/S1053811913011130](https://www.sciencedirect.com/science/article/pii/S1053811913011130)
- en: Raieli, *You’re Not a Writer, ChatGPT — But You Sound Like One*, 2024, [https://levelup.gitconnected.com/youre-not-a-writer-chatgpt-but-you-sound-like-one-75fa329ac3a9](https://levelup.gitconnected.com/youre-not-a-writer-chatgpt-but-you-sound-like-one-75fa329ac3a9)
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli, *你不是作家，ChatGPT — 但你听起来像一位*，2024，[https://levelup.gitconnected.com/youre-not-a-writer-chatgpt-but-you-sound-like-one-75fa329ac3a9](https://levelup.gitconnected.com/youre-not-a-writer-chatgpt-but-you-sound-like-one-75fa329ac3a9)
- en: Raieli, *How Far Is AI from Human Intelligence?*, 2025, [https://levelup.gitconnected.com/how-far-is-ai-from-human-intelligence-6ab4b2a5ce1c](https://levelup.gitconnected.com/how-far-is-ai-from-human-intelligence-6ab4b2a5ce1c)
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raieli, *人工智能距离人类智能有多远？*，2025，[https://levelup.gitconnected.com/how-far-is-ai-from-human-intelligence-6ab4b2a5ce1c](https://levelup.gitconnected.com/how-far-is-ai-from-human-intelligence-6ab4b2a5ce1c)
- en: Villalobos, *Will We Run Out of Data? Limits of LLM Scaling Based on Human-generated
    Data*, 2022, [https://arxiv.org/abs/2211.04325](https://arxiv.org/abs/2211.04325)
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Villalobos, *我们会耗尽数据吗？基于人类生成数据的LLM扩展的限制*，2022，[https://arxiv.org/abs/2211.04325](https://arxiv.org/abs/2211.04325)
- en: 'Feng, *How Far Are We From AGI: Are LLMs All We Need?*, 2024, [https://arxiv.org/abs/2405.10313](https://arxiv.org/abs/2405.10313)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng, *我们离通用人工智能还有多远：LLM是我们需要的全部吗？*，2024，[https://arxiv.org/abs/2405.10313](https://arxiv.org/abs/2405.10313)
- en: Karvonen, *Emergent World Models and Latent Variable Estimation in Chess-Playing
    Language Models*, 2024, [https://arxiv.org/pdf/2403.15498v2](https://arxiv.org/pdf/2403.15498v2)
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karvonen, *在棋类语言模型中涌现世界模型和潜在变量估计*，2024，[https://arxiv.org/pdf/2403.15498v2](https://arxiv.org/pdf/2403.15498v2)
- en: 'Li, *Emergent World Representations: Exploring a Sequence Model Trained on
    a Synthetic Task*, 2022, [https://arxiv.org/abs/2210.13382](https://arxiv.org/abs/2210.13382)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li, *涌现世界表示：探索在合成任务上训练的序列模型*，2022，[https://arxiv.org/abs/2210.13382](https://arxiv.org/abs/2210.13382)
- en: Bowman, *Eight Things to Know about Large Language Models*, 2023, [https://arxiv.org/pdf/2304.00612](https://arxiv.org/pdf/2304.00612)
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bowman, *关于大型语言模型你应该知道的八件事*，2023，[https://arxiv.org/abs/2304.00612](https://arxiv.org/abs/2304.00612)
- en: Shumailov, *AI Models Collapse When Trained on Recursively Generated Data*,
    2024, [https://www.nature.com/articles/s41586-024-07566-y](https://www.nature.com/articles/s41586-024-07566-y)
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shumailov, *当在递归生成数据上训练时，AI模型会崩溃*，2024，[https://www.nature.com/articles/s41586-024-07566-y](https://www.nature.com/articles/s41586-024-07566-y)
- en: LessWrong, *Embodiment is Indispensable for AGI*, 2022, [https://www.lesswrong.com/posts/vBBxKBWn4zRXwivxC/embodiment-is-indispensable-for-agi](https://www.lesswrong.com/posts/vBBxKBWn4zRXwivxC/embodiment-is-indispensable-for-agi)
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LessWrong, *具身对于通用人工智能的必要性*，2022，[https://www.lesswrong.com/posts/vBBxKBWn4zRXwivxC/embodiment-is-indispensable-for-agi](https://www.lesswrong.com/posts/vBBxKBWn4zRXwivxC/embodiment-is-indispensable-for-agi)
- en: Tan, *The Path to AGI Goes through Embodiment*, 2023, [https://ojs.aaai.org/index.php/AAAI-SS/article/view/27485](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27485)
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan, *通往通用人工智能的道路需要通过具身*，2023，[https://ojs.aaai.org/index.php/AAAI-SS/article/view/27485](https://ojs.aaai.org/index.php/AAAI-SS/article/view/27485)
- en: Mitchell, *Fully Autonomous AI Agents Should Not be Developed*, 2025, [https://arxiv.org/pdf/2502.02649](https://arxiv.org/pdf/2502.02649)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell, *不应开发完全自主的AI代理*，2025，[https://arxiv.org/pdf/2502.02649](https://arxiv.org/pdf/2502.02649)
- en: Diamond, *On the Ethical Considerations of Generative Agents*, 2024, [https://arxiv.org/abs/2411.19211](https://arxiv.org/abs/2411.19211)
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diamond, *关于生成式代理的伦理考量*, 2024, [https://arxiv.org/abs/2411.19211](https://arxiv.org/abs/2411.19211)
- en: Siqueira de Cerqueira, *Can We Trust AI Agents? An Experimental Study Towards
    Trustworthy LLM-Based Multi-Agent Systems for AI Ethics*, 2024, [https://arxiv.org/abs/2411.08881](https://arxiv.org/abs/2411.08881)
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siqueira de Cerqueira, *我们能否信任AI代理？关于构建可信赖的基于LLM的多智能体系统以实现AI伦理的实验研究*, 2024,
    [https://arxiv.org/abs/2411.08881](https://arxiv.org/abs/2411.08881)
- en: Chaffer, *Decentralized Governance of Autonomous AI Agents*, 2024, [https://arxiv.org/abs/2412.17114v3](https://arxiv.org/abs/2412.17114v3)
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaffer, *自主AI代理的去中心化治理*, 2024, [https://arxiv.org/abs/2412.17114v3](https://arxiv.org/abs/2412.17114v3)
- en: Gabriel, *The Ethics of Advanced AI Assistants*, 2024, [https://arxiv.org/pdf/2404.16244](https://arxiv.org/pdf/2404.16244)
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gabriel, *高级AI助手的伦理学*, 2024, [https://arxiv.org/pdf/2404.16244](https://arxiv.org/pdf/2404.16244)
- en: 'Chen, *Combating Misinformation in the Age of LLMs: Opportunities and Challenges*,
    2023, [https://arxiv.org/abs/2311.05656](https://arxiv.org/abs/2311.05656)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen, *在LLM时代对抗虚假信息：机遇与挑战*, 2023, [https://arxiv.org/abs/2311.05656](https://arxiv.org/abs/2311.05656)
- en: 'Luccioni, *Counting Carbon: A Survey of Factors Influencing the Emissions of
    Machine Learning*, 2023, [https://arxiv.org/abs/2302.08476](https://arxiv.org/abs/2302.08476)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luccioni, *计算碳足迹：影响机器学习排放的因素综述*, 2023, [https://arxiv.org/abs/2302.08476](https://arxiv.org/abs/2302.08476)
