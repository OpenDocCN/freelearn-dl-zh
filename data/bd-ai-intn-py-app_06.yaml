- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: AI/ML Application Design
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI/ML应用设计
- en: As the landscape of intelligent applications evolves, their architectural design
    becomes pivotal for efficiency, scalability, operability, and security. This chapter
    provides a guide on key topics to consider as you embark on creating robust and
    responsive AI/ML applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着智能应用领域的演变，它们的架构设计对于效率、可扩展性、可操作性和安全性变得至关重要。本章提供了一些建议，帮助你开始创建稳健和响应迅速的AI/ML应用。
- en: 'The chapter begins with **data modeling**, examining how to organize data in
    a way that maximizes effectiveness for three different consumers: humans, applications,
    and AI models. You will learn about **data storage**, considering the impact of
    different data types and determining the best storage technology. You will estimate
    storage needs and determine the best MongoDB Atlas cluster configuration for your
    example application.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从**数据建模**开始，探讨如何以最大化三种不同消费者（人类、应用程序和AI模型）的有效性的方式组织数据。你将了解**数据存储**，考虑不同数据类型的影响，并确定最佳存储技术。你将估算存储需求，并确定适用于示例应用的最佳MongoDB
    Atlas集群配置。
- en: As you learn about **data flow**, you will explore the detailed movement of
    data through ingestion, processing, and output to maintain integrity and velocity.
    This chapter also addresses **data lifecycle management**, including updates,
    aging, and retention, ensuring that data remains relevant and compliant.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当你学习**数据流**时，你将探索数据通过摄取、处理和输出的详细移动，以保持完整性和速度。本章还讨论了**数据生命周期管理**，包括更新、老化和管理，确保数据保持相关性和合规性。
- en: Security concerns are stretched further for AI/ML applications due to the risks
    of exposing data or logic to AI models. This chapter discusses security measures
    and **role-based access control** (**RBAC**) to protect sensitive data and logic
    integrity. You will also learn the best principles for data storage, flow, modeling,
    and security, providing practical advice to avoid common pitfalls.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 由于将数据或逻辑暴露给AI模型的风险，AI/ML应用的安全问题进一步扩大。本章讨论了安全措施和**基于角色的访问控制**（**RBAC**）来保护敏感数据和逻辑完整性。你还将了解数据存储、流动、建模和安全的最佳原则，提供避免常见陷阱的实用建议。
- en: Throughout the chapter, you will use a fictitious news application called **MongoDB
    Developer News** (**MDN**), which will be like Medium.com, equipping you to create
    intelligent applications by using a practical example.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将使用一个虚构的新闻应用，称为**MongoDB开发者新闻**（**MDN**），它将类似于Medium.com，通过实际示例让你能够创建智能应用。
- en: 'This chapter will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Data modeling
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据建模
- en: Data storage
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储
- en: Data flow
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据流
- en: Freshness and retention
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新鲜度和保持
- en: Security and RBAC
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全性和RBAC
- en: Best practices
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳实践
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following are the prerequisites to follow along with the code in this chapter:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在本章中跟随代码的先决条件：
- en: A MongoDB Atlas cluster `M0` tier (free) should be sufficient
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个MongoDB Atlas集群`M0`层（免费）应该足够
- en: An OpenAI account and API key with access to the `text-embedding-3-large` model
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有访问`text-embedding-3-large`模型权限的OpenAI账户和API密钥
- en: A Python 3 working environment
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个Python 3工作环境
- en: Installed Python libraries for MongoDB, LangChain, and OpenAI
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于MongoDB、LangChain和OpenAI的已安装Python库
- en: Atlas Search indexes and Vector Search indexes created on the MongoDB Atlas
    cluster
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在MongoDB Atlas集群上创建的Atlas Search索引和Vector Search索引
- en: Data modeling
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据建模
- en: 'This section delves into the diverse types of data required by AI/ML systems,
    including structured, unstructured, and semi-structured data, and how these are
    applied to MDN’s news articles. The following are short descriptions of each to
    set a basic understanding:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本节深入探讨了AI/ML系统所需的各种类型的数据，包括结构化、非结构化和半结构化数据，以及这些数据如何应用于MDN的新闻文章。以下是对每种数据的简要描述，以建立基本理解：
- en: '**Structured data** conforms to a predefined schema and is traditionally stored
    in relational databases for transactional information. It powers systems of engagement
    and intelligence.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结构化数据**符合预定义的模式，并且传统上存储在关系数据库中，用于事务信息。它为参与和智能系统提供动力。'
- en: '**Unstructured data** includes binary assets, such as PDFs, images, videos,
    and others. Object stores such as Amazon S3 allow storing these under a flexible
    directory structure at a lower cost.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非结构化数据**包括二进制资产，如PDF文件、图像、视频等。对象存储，如Amazon S3，允许以较低的成本在灵活的目录结构下存储这些数据。'
- en: '**Semi-structured data**, such as JSON documents, allow each document to define
    its schema, accommodating both common and unique data points, or even the absence
    of some data.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半结构化数据**，如JSON文档，允许每个文档定义其架构，以适应常见的和独特的数据点，甚至可以处理某些数据的缺失。'
- en: MDN will store news articles, subscriber profiles, billing information, and
    more. For simplicity, in this chapter, you will focus on the data about each news
    article and related binary content (which would be images). *Figure 6**.1* describes
    the data model of the `articles` collection.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: MDN将存储新闻文章、订阅者资料、账单信息等。为了简化，在本章中，你将专注于每篇新闻文章及其相关二进制内容（例如图片）的数据。*图6*.1描述了`articles`集合的数据模型。
- en: '![](img/B22495_06_01.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22495_06_01.jpg)'
- en: 'Figure 6.1: Schema for the articles collection'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：文章集合的架构
- en: The articles collection represents a news article with metadata, including creation
    details, tags, and contributors. All documents feature a title, summary, body
    content in HTML and plain text, and associated media elements such as images.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 文章集合表示新闻文章的元数据，包括创建细节、标签和贡献者。所有文档都包含标题、摘要、HTML和纯文本正文内容，以及相关的媒体元素，如图片。
- en: Enriching data with embeddings
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用嵌入丰富数据
- en: To complete the MDN data model, you need to consider data that will also be
    represented and stored via embeddings. **Text embeddings** for article titles
    and summaries will enable semantic search, while **image embeddings** will help
    find similar artwork used across articles. *Table 6.1* describes the data fields,
    embedding models to use, and their vector sizes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成MDN数据模型，你需要考虑将通过嵌入表示和存储的数据。**文本嵌入**用于文章标题和摘要，将实现语义搜索，而**图像嵌入**将有助于找到跨文章使用的相似艺术品。*表6*.1描述了数据字段、要使用的嵌入模型及其向量大小。
- en: '| **Type** | **Field(s)** | **Embedding model** | **Vector size** |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| **类型** | **字段** | **嵌入模型** | **向量大小** |'
- en: '| Text | `title` | OpenAI `text-embedding-3-large` | 1,024 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | `标题` | OpenAI `text-embedding-3-large` | 1,024 |'
- en: '| Text | `summary` |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | `摘要` |'
- en: '| Image | `contents` | OpenAI CLIP | 768 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 图片 | `内容` | OpenAI CLIP | 768 |'
- en: 'Table 6.1: Embeddings for the articles collection'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1：文章集合的嵌入
- en: Each article has a title and summary. Instead of embedding them separately,
    you will concatenate them and create one text embedding for simplicity. Ideally,
    for images, you would store the embedding with each content object in the `contents`
    array. However, support for fields inside arrays of objects for vector indexes
    is not available today in MongoDB Atlas and leads to the **anti-pattern of bloated
    documents**. The best practice is to store image embeddings in a separate collection
    and use the **extended reference schema design pattern**. You can learn more about
    indexing arrays with MongoDB, bloated documents, and the extended reference pattern
    from the links given in the *Further Reading* chapter of this book. *Figure 6**.2*
    shows the updated data model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 每篇文章都有一个标题和摘要。为了简化，你将它们连接起来，创建一个文本嵌入。理想情况下，对于图片，你将嵌入存储在每个内容对象数组中的`contents`数组中。然而，MongoDB
    Atlas目前不支持对象数组内部的字段用于向量索引，这导致了**膨胀文档的反模式**。最佳实践是将图像嵌入存储在单独的集合中，并使用**扩展引用架构设计模式**。你可以从本书*进一步阅读*章节中提供的链接了解更多关于使用MongoDB索引数组、膨胀文档和扩展引用模式的信息。*图6*.2显示了更新的数据模型。
- en: '![](img/B22495_06_02.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22495_06_02.jpg)'
- en: 'Figure 6.2: Schema for articles with embeddings'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：具有嵌入的文章架构
- en: '*Table 6.2* shows the corresponding vector indexes.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*表6*.2显示了相应的向量索引。'
- en: '| `articles` | `article_content_embeddings` |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| `articles` | `article_content_embeddings` |'
- en: '| `semantic_embedding_vix` | `content_embedding_vix` |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| `semantic_embedding_vix` | `content_embedding_vix` |'
- en: '|'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 6.2: Vector search index definitions'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2：向量搜索索引定义
- en: Considering search use cases
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考虑搜索用例
- en: 'Before finalizing the data model, let’s consider search use cases for articles,
    and adapt the model once more. Here are some broader search use cases:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在最终确定数据模型之前，让我们考虑文章的搜索用例，并对模型进行进一步的调整。以下是一些更广泛的搜索用例：
- en: '`title` and `summary` fields for text search, and the `brand` and `subscription_type`
    fields for filtering.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`标题`和`摘要`字段用于文本搜索，以及`品牌`和`订阅类型`字段用于过滤。'
- en: '`tags` field. You will also need a vector search index to cover the `title
    +` `summary` embedding.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tags`字段。你还需要一个向量搜索索引来覆盖`标题+``摘要`嵌入。'
- en: '`_id`, `brand`, and `subscription_type` fields from the `articles` collection
    into the `article_content_embeddings` collection. Since there is already an `_id`
    field in this collection, you can create a composite primary key that includes
    the `_id` of the article and the `_id` of the content. *Figure 6**.3* shows the
    updated data model.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`articles`集合中的`_id`、`brand`和`subscription_type`字段合并到`article_content_embeddings`集合中。由于该集合中已经存在一个`_id`字段，您可以创建一个包含文章`_id`和内容`_id`的复合主键。*图6**.3*显示了更新的数据模型。
- en: '![](img/B22495_06_03.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22495_06_03.jpg)'
- en: 'Figure 6.3: Updated schema for articles with embeddings'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：带有嵌入的更新后的文章模式
- en: '*Table 6.3* shows updated vector indexes.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*表6.3* 展示了更新的向量索引。'
- en: '| `articles` | `article_content_embeddings` |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `articles` | `article_content_embeddings` |'
- en: '| `semantic_embedding_vix` | `content_embedding_vix` |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `semantic_embedding_vix` | `content_embedding_vix` |'
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE20]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE38]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '|'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 6.3: Updated vector search index definitions'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3：更新的向量搜索索引定义
- en: '*Table 6.4* shows the new text search index.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*表6.4* 展示了新的文本搜索索引。'
- en: '| `articles` |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| `articles` |'
- en: '| `lexical_six` |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| `lexical_six` |'
- en: '|'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE60]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '|'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 6.4: Text search index definition'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.4：文本搜索索引定义
- en: You learned about writing vector search queries in [*Chapter 4*](B22495_04.xhtml#_idTextAnchor061)*,
    Embedding Models*. To learn more about hybrid search queries, you can refer to
    the tutorial at [https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/](https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您在[*第4章*](B22495_04.xhtml#_idTextAnchor061)*，嵌入模型*中学习了如何编写向量搜索查询。要了解更多关于混合搜索查询的信息，您可以参考[https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/](https://www.mongodb.com/docs/atlas/atlas-vector-search/tutorials/reciprocal-rank-fusion/)上的教程。
- en: Now that you understand your data model and the indexes required, you need to
    consider the number of articles MDN will bear (including the sizes of embeddings
    and indexes), peak daily times, and more to determine the overall storage and
    database cluster requirements.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了您的数据模型和所需的索引，您需要考虑MDN将承载的文章数量（包括嵌入和索引的大小）、峰值每日时间等因素，以确定整体存储和数据库集群需求。
- en: Data storage
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据存储
- en: In this section, you will perform sizing, which is an educated estimate, for
    storage requirements. You will consider not just volume size and speed, but also
    several other aspects of the database cluster that are needed for harnessing the
    data of your application while following expected data access patterns.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将进行容量估算，这是一种有根据的估计，用于存储需求。您需要考虑的不仅仅是体积大小和速度，还包括数据库集群的多个其他方面，这些方面对于在遵循预期的数据访问模式的同时利用您应用程序的数据是必需的。
- en: MDN plans to publish 100 articles daily. Keeping the articles from the last
    5 years, the number of articles would total 182,500\. With 48 million subscribers
    and 24 million daily active users, peak access occurs for 30 minutes daily across
    three major time zones, as shown in *Figure 6**.4*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: MDN计划每天发布100篇文章。保留过去5年的文章，文章总数将达到182,500篇。有4800万订阅者和2400万每日活跃用户，每天在三个主要时区中，高峰访问时间为30分钟，如*图6**.4*所示。
- en: '![](img/B22495_06_04.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22495_06_04.jpg)'
- en: 'Figure 6.4: MDN subscriber time zones and peak times'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：MDN订阅者时区和峰值时间
- en: First, you will estimate the total data size. Each article has one 1,024-dimension
    embedding for semantic search and five 768-dimension embeddings for image search,
    totaling 40 KB uncompressed (dimensions use the double type). With the `title`,
    `summary`, `body` (with and without markup), and other fields, the average article
    size will be about 300 KB uncompressed.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您将估算总数据量。每篇文章都有一个用于语义搜索的1024维嵌入和五个用于图像搜索的768维嵌入，总共40 KB未压缩（维度使用双精度类型）。加上`标题`、`摘要`、`正文`（带和不含标记）以及其他字段，平均文章大小约为300
    KB未压缩。
- en: Five years of articles will require about 100 GB uncompressed. With MongoDB’s
    **WiredTiger** compression (Snappy, zlib, and zstd are also available as compression
    options), this reduces to about 50 GB on disk. The defined vector indexes add
    about 3.6 GB. Images and binary assets will be stored in Amazon S3\. For simplicity,
    you will not estimate the size of search and traditional indexes. You can safely
    say that MDN will need 80 to 100 GB on disk in MongoDB Atlas, which is very manageable
    by today’s cloud computing standards.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 五年的文章将需要大约100 GB未压缩。使用MongoDB的**WiredTiger**压缩（Snappy、zlib和zstd也是可用的压缩选项），这将在磁盘上减少到大约50
    GB。定义的向量索引增加了大约3.6 GB。图像和二进制资产将存储在Amazon S3上。为了简化，您将不会估算搜索和传统索引的大小。您可以放心地说，MDN在MongoDB
    Atlas上需要80到100 GB的磁盘空间，这在今天的云计算标准下是非常可管理的。
- en: Now, you will determine the most suitable MongoDB Atlas cluster configuration.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将确定最合适的MongoDB Atlas集群配置。
- en: Determining the type of database cluster
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定数据库集群类型
- en: 'MongoDB Atlas provides two main cluster types:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB Atlas提供两种主要的集群类型：
- en: '**Replica sets** have a primary node for writes and secondary nodes for high
    availability, which can also be used for reads. These sets scale vertically and
    can also scale horizontally for reads by adding more nodes in the same or different
    cloud regions.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**副本集**有一个主节点用于写入，以及用于高可用性的辅助节点，这些节点也可以用于读取。这些集可以垂直扩展，并且可以通过在相同或不同的云区域添加更多节点来水平扩展读取。'
- en: '**Sharded clusters** consist of multiple shards, each being a part of the overall
    dataset, and each being a replica set. They scale vertically and horizontally
    for both reads and writes. Shards can be placed in different cloud regions to
    enhance data locality and compliance.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分片集群**由多个分片组成，每个分片都是整体数据集的一部分，并且每个分片都是一个副本集。它们在读写方面都可以垂直和水平扩展。分片可以放置在不同的云区域，以增强数据本地性和合规性。'
- en: So, how can you determine whether a replica set is sufficient or a sharded cluster
    is needed? Key factors include the size of the dataset or the throughput of applications
    that can challenge the capacity of a single server. For example, high query rates
    can exhaust the server’s CPU capacity and working set sizes larger than the system’s
    RAM can stress the I/O capacity of disk drives. MDN publishes 100 articles per
    day, so sharding is not necessary for this reason.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，您如何确定副本集是否足够，或者是否需要分片集群？关键因素包括数据集的大小或应用程序的吞吐量，这些可能会挑战单台服务器的容量。例如，高查询率可能会耗尽服务器的CPU容量，而大于系统RAM的工作集大小可能会对磁盘驱动器的I/O容量造成压力。MDN每天发布100篇文章，因此从这个角度来看，分片不是必需的。
- en: Other reasons for sharding include data governance and compliance and **recovery
    point objective** (**RPO**) and **recovery time objective** (**RTO**) policies,
    which are key metrics in disaster recovery and business continuity planning. None
    of these are applicable to MDN.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 分片的其他原因包括数据治理和合规性以及**恢复点目标**（**RPO**）和**恢复时间目标**（**RTO**）策略，这些是灾难恢复和业务连续性规划中的关键指标。这些都不适用于MDN。
- en: Considering the small number of writes per second and manageable data size,
    it makes sense to use a replica set. You will now need to determine the amount
    of RAM and IOPS needed; both are key components for fast response times.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到每秒写入次数较少和数据量可管理，使用副本集是合理的。现在您需要确定所需的RAM和IOPS数量；这两个都是快速响应时间的关键组件。
- en: Determining IOPS
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定IOPS
- en: MDN is a low-write, high-read use case. With only 100 articles added per day,
    there is minimal pressure on the storage system for writes. *Table 6.5* shows
    the storage and IOPS options provided by MongoDB Atlas.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: MDN是一个低写入、高读取的使用案例。每天仅添加100篇文章，对存储系统的写入压力很小。*表6.5*显示了MongoDB Atlas提供的存储和IOPS选项。
- en: '| **Storage types** | **Lowest IOPS/storage** | **Highest IOPS/storage** |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| **存储类型** | **最低IOPS/存储** | **最高IOPS/存储** |'
- en: '| Standard IOPS | 3,000 IOPS/10 GB | 12,288 IOPS/4 TB 16,000 IOPS/14 TB**Extended
    storage enabled |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 标准IOPS | 3,000 IOPS/10 GB | 12,288 IOPS/4 TB 16,000 IOPS/14 TB**扩展存储已启用**
    |'
- en: '| Provisioned IOPS | 100 IOPS/10 GB | 64,000 IOPS/4 TB |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 分配IOPS | 100 IOPS/10 GB | 64,000 IOPS/4 TB |'
- en: '| NVMe | 100,125 100% random read IOPS35,000 write IOPS 380 GB | 3,300,000
    100% random read IOPS1,400,000 write IOPS 4,000 GB |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| NVMe | 100,125 100%随机读IOPS 35,000写IOPS 380 GB | 3,300,000 100%随机读IOPS 1,400,000写IOPS
    4,000 GB |'
- en: 'Table 6.5: MongoDB Atlas storage types on AWS'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.5：MongoDB Atlas在AWS上的存储类型
- en: As shown in *Figure 6**.4*, there will be a 30-minutes peak period, during which
    24 million users are expected to be active daily. So, you need to provision 6,000
    IOPS, as shown in *Table 6.6*. This is based on subscriber distribution, memory
    versus disk reads, and each article requiring 3 IOPS for disk reads (150 KB compressed
    ÷ 64 KB I/O size of Amazon EBS).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图6**.4所示，将有一个30分钟的峰值时段，在此期间预计每天有2400万用户活跃。因此，您需要配置6,000 IOPS，如*表6.6*所示。这是基于订阅者分布、内存与磁盘读取以及每篇文章需要3
    IOPS进行磁盘读取（150 KB压缩 ÷ 64 KB亚马逊EBS I/O大小）。
- en: '| **Region** | **Allocation** | **DAU** | **20% reads** **from disk** | **Disk
    reads/sec during** **peak time** | **IOPS required** |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| **区域** | **分配** | **DAU** | **20% 读操作** **来自磁盘** | **高峰时段的磁盘读操作/秒** | **所需的IOPS**
    |'
- en: '| `AMER`^ | 40% | 9,600,000 | 1,920,000 | 1,067 | 3,200^ |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| `AMER`^ | 40% | 9,600,000 | 1,920,000 | 1,067 | 3,200^ |'
- en: '| `EMEA`^ | 20% | 4,800,000 | 960,000 | 533 | 1,600^ |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| `EMEA`^ | 20% | 4,800,000 | 960,000 | 533 | 1,600^ |'
- en: '| `APAC` | 25% | 6,000,000 | 1,200,000 | 667 | 2,000 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| `APAC` | 25% | 6,000,000 | 1,200,000 | 667 | 2,000 |'
- en: '| `LATAM`^ | 15% | 3,600,000 | 720,000 | 400 | 1,200^ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| `LATAM`^ | 15% | 3,600,000 | 720,000 | 400 | 1,200^ |'
- en: '| ^ Zones overlapping at peak time |  |  |  | Peak IOPS | 6,000 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| ^ 高峰时段重叠的区域 |  |  |  | 高峰IOPS | 6,000 |'
- en: 'Table 6.6: MDN global subscriber distribution'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.6：MDN全球订阅者分布
- en: The minimum standard IOPS on any Atlas cluster on AWS is 3,000\. To achieve
    6,000 IOPS you would need to use an Atlas `M50` tier with 2TB disk, which feels
    over-provisioned and would not provide low latency to all readers if deployed
    in a single cloud region. To address this, MDN will deploy the application stack
    in major geographies, enabling regional provisioning, workload distribution, and
    local reads for an optimal customer experience.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS上的任何Atlas集群上的最低标准IOPS为3,000。要达到6,000 IOPS，您需要使用带有2TB磁盘的Atlas `M50` 层，这感觉是过度配置的，并且如果在一个云区域中部署，不会为所有读者提供低延迟。为了解决这个问题，MDN将在主要地理区域部署应用程序堆栈，实现区域配置、工作负载分配和本地读取，以提供最佳客户体验。
- en: With MongoDB Atlas, you can place vector search nodes across regions. The `S40`
    tier offers 26,875 read IOPS, which is sufficient for this example, and a 2-node
    minimum per region, which ensures high availability.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MongoDB Atlas，您可以在不同地区放置向量搜索节点。`S40` 层提供26,875次读取IOPS，这对于本例来说已经足够了，并且每个区域至少需要2个节点，这确保了高可用性。
- en: While Vector Search nodes will handle lexical, semantic, and image search, the
    full JSON document must be fetched from a MongoDB data node after matching. To
    fully support local reads, we must provision read-only nodes in the same regions
    and meet IOPS requirements. We can do this with the Atlas `M40` tier. Having determined
    the IOPS needed, you now need to estimate RAM.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然向量搜索节点将处理词法、语义和图像搜索，但必须在匹配后从MongoDB数据节点获取完整的JSON文档。为了完全支持本地读取，我们必须在同一地区配置只读节点并满足IOPS要求。我们可以使用Atlas
    `M40` 层来实现这一点。在确定了所需的IOPS后，您现在需要估计RAM。
- en: Determining RAM
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定RAM
- en: For data nodes, the Atlas `M40` tier provides 16 GB of RAM. The MongoDB WiredTiger
    storage engine reserves 50% of (RAM - 1 GB) for its cache. With documents averaging
    300 KB in size, the cache can hold approximately 28,000 documents. Keep in mind
    that traditional index sizes might slightly reduce this number. Given the addition
    of 100 new articles daily, the cache on an `M40` tier can accommodate data for
    about 280 days, or roughly 9 months, which is more than sufficient for this example.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据节点，Atlas `M40` 层提供16 GB的RAM。MongoDB WiredTiger存储引擎为其缓存保留了(RAM - 1 GB)的50%。考虑到文档平均大小为300
    KB，缓存可以存储大约28,000个文档。请注意，传统的索引大小可能会略微减少这个数字。考虑到每天增加100篇文章，`M40` 层的缓存可以容纳大约280天的数据，或大约9个月，这对于本例来说已经足够了。
- en: The Search `S40` tier offers 16 GB of RAM, 2 vCPUs, and 100 GB of storage. The
    HNSW graph, or the vector index, must fit in the memory.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索`S40` 层提供16 GB的RAM、2个vCPU和100 GB的存储。HNSW图或向量索引必须适应内存。
- en: Note
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You learned about **HNSW** or **hierarchical navigable small worlds** in [*Chapter
    5*](B22495_05.xhtml#_idTextAnchor115), *Vector Databases*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 您在[*第5章*](B22495_05.xhtml#_idTextAnchor115) *向量数据库*中学习了**HNSW**或**分层可导航小世界**。
- en: One article uses 1 x 1,024 vector + 5 x 768 vectors = 19.5 KB. With 3.5 GB needed
    for 182,500 articles, 16 GB of RAM is more than sufficient for vector search and
    leaves room for the lexical search index. The `S30` tier, which offers 4 GB of
    RAM, 1 vCPU, and 50 GB storage, is less costly, but note that more CPUs allows
    more concurrent searches.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一篇文章使用1 x 1,024向量 + 5 x 768向量 = 19.5 KB。对于182,500篇文章，需要3.5 GB的空间，16 GB的RAM对于向量搜索来说已经足够，并且还留有空间用于词法搜索索引。提供4
    GB RAM、1 vCPU和50 GB存储的`S30` 层成本较低，但请注意，更多的CPU允许更多的并发搜索。
- en: Final cluster configuration
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最终集群配置
- en: You have now determined the cluster configuration for MDN. *Table 6.7* describes
    the MDN global cloud architecture, detailing the distribution of Atlas nodes across
    different regions. The `AMER` region, identified as the primary region, uses `M40`
    tier nodes and `S30` vector search nodes to serve writes and searches for the
    Americas, while the `EMEA`, `APAC`, and `LATAM` regions use `M40` read-only nodes
    and `S30` vector search nodes to serve local searches only for their respective
    region. Each region will need a deployment of the MDN application stack, as pictured
    in the global map in *Table 6.7*.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经确定了MDN的集群配置。*表6.7* 描述了MDN的全局云架构，详细说明了Atlas节点在不同地区的分布。`AMER` 区域，被标识为主要区域，使用`M40`
    层节点和`S30` 向量搜索节点为美洲提供服务读写和搜索，而`EMEA`、`APAC` 和`LATAM` 区域使用`M40` 只读节点和`S30` 向量搜索节点只为各自区域提供本地搜索。每个区域都需要部署MDN应用程序堆栈，如图*表6.7*中的全局地图所示。
- en: '| **Region** | **Atlas base** **tier nodes** | **Atlas** **read-only nodes**
    | **Atlas Vector** **Search nodes** |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| **区域** | **Atlas 基础** **层节点** | **Atlas** **只读节点** | **Atlas Vector** **搜索节点**
    |'
- en: '| `AMER`(primary region) | `M40`(three included) |  | `S30` x2 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| `AMER`(主要区域) | `M40`(包含三个) |  | `S30` x2 |'
- en: '| `EMEA` |  | `M40` x2 | `S30` x2 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| `EMEA` |  | `M40` x2 | `S30` x2 |'
- en: '| `APAC` |  | `M40` x2 | `S30` x2 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| `APAC` |  | `M40` x2 | `S30` x2 |'
- en: '| `LATAM` |  | `M40` x2 | `S30` x2 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| `LATAM` |  | `M40` x2 | `S30` x2 |'
- en: '| **MDN global** **cloud architecture**![](img/B22495_Table_6.7.jpg) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| **MDN 全球** **云架构**![](img/B22495_Table_6.7.jpg) |'
- en: 'Table 6.7: MongoDB Atlas cluster configuration for MDN'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.7：MDN 的 MongoDB Atlas 集群配置
- en: Performance and availability versus cost
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能、可用性与成本对比
- en: Notice that additional read-only nodes were not provisioned in the `AMER` region,
    using the two secondary nodes as read-only instead. This saves costs due to MDN’s
    low write profile, despite potential resource competition. Provisioning only one
    `M40` read-only node in other regions saves more costs but increases latency during
    maintenance windows, as reads will be rerouted.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 `AMER` 区域没有配置额外的只读节点，而是使用两个辅助节点作为只读节点。这节省了成本，因为 MDN 的写入模式较低，尽管可能存在资源竞争。在其他区域仅配置一个
    `M40` 只读节点可以节省更多成本，但会增加维护窗口期间的延迟，因为读取将被重新路由。
- en: To protect against a complete `AMER` outage while adhering to best practices,
    consider provisioning five nodes across three regions and deploying the application
    stack in the two regions with two electable nodes each.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在遵守最佳实践的同时防止 `AMER` 完全中断，考虑在三个区域中配置五个节点，并在每个区域部署具有两个可选举节点的应用程序堆栈。
- en: Data flow
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据流
- en: '**Data flow** involves the movement of data through a system, affecting the
    accuracy, relevance, and speed of the results delivered to consumers, which, in
    turn, influences their engagement. This section explores design considerations
    for handling data sources, processing data, prompting LLMs, and embedding models
    to enrich data using MDN as an example. *Figure 6**.5* illustrates this flow.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据流** 涉及数据通过系统的移动，影响提供给消费者的结果的准确性、相关性和速度，这反过来又影响他们的参与度。本节探讨了处理数据源、处理数据、提示
    LLM 和嵌入模型以使用 MDN 作为示例来丰富数据的设计考虑因素。*图 6**.5* 展示了此流程。'
- en: '![](img/B22495_06_06.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22495_06_06.jpg)'
- en: 'Figure 6.5: Typical data flow in an AI/ML application'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：AI/ML 应用程序中的典型数据流
- en: Let's us begin with the design for handling data sources. Data can be ingested
    into MongoDB Atlas either statically (at rest) from files as it is, or dynamically
    (in motion), allowing for continuous updates, data transformation, and logic execution.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从处理数据源的设计开始。数据可以以静态（静止）方式直接从文件中导入到 MongoDB Atlas，也可以以动态（移动）方式导入，允许进行持续更新、数据转换和逻辑执行。
- en: Handling static data sources
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理静态数据源
- en: The simplest way to import static data is to use `mongoimport`, which supports
    JSON, CSV, and TSV formats. It is ideal for initial loads or bulk updates as it
    can handle large datasets. Moreover, increasing the number of insertion workers
    to match the host’s vCPUs can boost import speed.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单导入静态数据的方式是使用 `mongoimport`，它支持 JSON、CSV 和 TSV 格式。它非常适合初始加载或批量更新，因为它可以处理大型数据集。此外，将插入工作进程的数量增加到与主机
    vCPU 相匹配可以提升导入速度。
- en: '`mongoimport` can also be used dynamically to update externally sourced data.
    You can build invocation commands at runtime and execute them as out-of-process
    tasks. Some video game companies use this method to update player profiles with
    purchase data from mobile app stores.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`mongoimport` 还可以动态地更新外部数据源。您可以在运行时构建调用命令，并将它们作为进程外任务执行。一些视频游戏公司使用这种方法来更新来自移动应用商店的购买数据以更新玩家资料。'
- en: Using MDN as an example, users can provide their GitHub ID when subscribing.
    With GitHub’s API, you can create a list of the programming languages used in
    the repositories that users own or have contributed to. A scheduled job can fetch
    this data periodically. The list of languages can then imported and merged into
    their profiles to recommend articles later. *Table 6.8* demonstrates how you can
    do this.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以 MDN 为例，用户在订阅时可以提供他们的 GitHub ID。使用 GitHub 的 API，您可以创建一个列表，列出用户拥有或贡献的存储库中使用的编程语言。一个计划的任务可以定期获取这些数据。然后可以将语言列表导入并合并到他们的个人资料中，以便稍后推荐文章。*表
    6.8* 展示了如何进行此操作。
- en: '| `github-20240719.json` |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| `github-20240719.json` |'
- en: '|'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE85]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '|'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `mdn.subscribers` |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| `mdn.subscribers` |'
- en: '|'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE87]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '|'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| The `mongoimport` invocation to merge data matching on the `github_id` field
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| `mongoimport` 命令用于合并匹配 `github_id` 字段的数据库数据 |'
- en: '|'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE89]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '|'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `mdn.subscribers` after merge |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 合并后的 `mdn.subscribers` |'
- en: '|'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE93]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '|'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 6.8: Example of using mongoimport to merge data'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.8：使用 mongoimport 合并数据的示例
- en: While `mongoimport` is a versatile tool for various data import needs, it does
    not support continuous synchronization, logic execution, or data transformations.
    You will now explore some methods that do support these functions.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `mongoimport` 是一个适用于各种数据导入需求的通用工具，但它不支持连续同步、逻辑执行或数据转换。现在您将探索一些支持这些功能的方法。
- en: Storing operational data enriched with vector embeddings
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储带有向量嵌入的操作数据
- en: 'When original representations are stored or updated, their corresponding vector
    embeddings must be refreshed to accurately reflect the content. This can be done
    in the following ways:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 当原始表示存储或更新时，其相应的向量嵌入必须刷新以准确反映内容。这可以通过以下方式完成：
- en: '**Synchronously**: Obtains the updated vector embedding before the database
    operation, writing both data and embedding together. This method is suitable for
    fast, simple embedding models or when the model is locally hosted. However, it
    may fail if the response times of the embedding model vary.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步**: 在数据库操作之前获取更新的向量嵌入，同时写入数据和嵌入。此方法适用于快速、简单的嵌入模型或当模型在本地托管时。然而，如果嵌入模型的响应时间不同，可能会失败。'
- en: '**Asynchronously**: Ensures immediate consistency of primary data and allows
    for prompting the embedding model afterward. While this offers scalability and
    handles unpredictable models, it introduces latency during which embeddings are
    temporarily outdated.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步**: 确保主数据的即时一致性，并允许在之后提示嵌入模型。虽然这提供了可伸缩性和处理不可预测的模型的能力，但它在嵌入数据暂时过时的过程中引入了延迟。'
- en: 'You can keep embeddings up to date asynchronously in MongoDB using the following
    four methods:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下四种方法在 MongoDB 中异步保持嵌入数据的最新状态：
- en: '**Kafka connector**: You can facilitate data flow from Apache Kafka into MongoDB
    collections through the Kafka connector. It is a Confluent-verified connector
    and allows data to flow from Apache Kafka topics into MongoDB as a **data sink**
    and publishes changes from MongoDB to Kafka topics as a **data source**. To keep
    embeddings up to date, you would use the sink connector and develop a post-processor
    in Java. You can learn more about sink post-processors here: [https://www.mongodb.com/docs/kafka-connector/v1.3/sink-connector/fundamentals/post-processors/#sink-connector-post-processors](https://www.mongodb.com/docs/kafka-connector/v1.3/sink-connector/fundamentals/post-processors/#sink-connector-post-processors).'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka 连接器**: 您可以通过 Kafka 连接器促进数据从 Apache Kafka 流入 MongoDB 集合。这是一个经过 Confluent
    验证的连接器，允许数据从 Apache Kafka 主题流入 MongoDB 作为 **数据汇**，并将 MongoDB 的更改发布到 Kafka 主题作为
    **数据源**。为了保持嵌入数据的最新状态，您将使用汇连接器并在 Java 中开发后处理器。您可以在[https://www.mongodb.com/docs/kafka-connector/v1.3/sink-connector/fundamentals/post-processors/#sink-connector-post-processors](https://www.mongodb.com/docs/kafka-connector/v1.3/sink-connector/fundamentals/post-processors/#sink-connector-post-processors)了解更多有关汇后处理器的信息。'
- en: '**Atlas Stream Processing**: This method handles complex data streams with
    the same query API as MongoDB Atlas databases. It enables continuous aggregation
    and includes schema validation for message integrity and timely issue detection.
    Processed data can be written to Atlas collections, and they are integrated into
    Atlas projects and independent of Atlas clusters. Atlas Stream Processing logic
    is programmed in JavaScript using MongoDB aggregation syntax. For an example of
    using Atlas Stream Processing to handle embedding data, see [https://www.mongodb.com/solutions/solutions-library/rag-applications](https://www.mongodb.com/solutions/solutions-library/rag-applications).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Atlas 流处理**: 此方法使用与 MongoDB Atlas 数据库相同的查询 API 处理复杂的数据流。它支持连续聚合，并包括用于消息完整性和及时问题检测的模式验证。处理后的数据可以写入
    Atlas 集合，并且它们集成到 Atlas 项目中，独立于 Atlas 集群。Atlas 流处理逻辑使用 JavaScript 和 MongoDB 聚合语法进行编程。有关使用
    Atlas 流处理处理嵌入数据的示例，请参阅 [https://www.mongodb.com/solutions/solutions-library/rag-applications](https://www.mongodb.com/solutions/solutions-library/rag-applications)。'
- en: '**Atlas Triggers**: Atlas Triggers execute application and database logic by
    responding to events or following predefined schedules. Each Trigger listens for
    specific event types and is linked to an Atlas Function. When a matching event
    occurs, the Trigger fires and passes the event object to the linked Function.
    Triggers can respond to various events, such as specific operations in a collection,
    authentication events such as user creation or deletion, and scheduled times.
    They are fully managed instances of change streams but limited to JavaScript.
    For an example of using Atlas Triggers to keep embeddings up to date, see [https://www.mongodb.com/developer/products/atlas/semantic-search-mongodb-atlas-vector-search/](https://www.mongodb.com/developer/products/atlas/semantic-search-mongodb-atlas-vector-search/).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Atlas 触发器**：Atlas 触发器通过响应事件或遵循预定义的日程来执行应用程序和数据库逻辑。每个触发器都监听特定的事件类型，并与 Atlas
    函数相关联。当发生匹配的事件时，触发器被触发，并将事件对象传递给相关联的函数。触发器可以响应各种事件，例如集合中的特定操作、用户创建或删除等认证事件，以及预定时间。它们是完全管理的变更流实例，但仅限于
    JavaScript。有关使用 Atlas 触发器保持嵌入数据最新的示例，请参阅[https://www.mongodb.com/developer/products/atlas/semantic-search-mongodb-atlas-vector-search/](https://www.mongodb.com/developer/products/atlas/semantic-search-mongodb-atlas-vector-search/)。'
- en: '**Change streams**: This method provides real-time access to data changes.
    Applications can subscribe to changes in a collection, database, or entire deployment
    and react immediately, with events processed in order and being resumable? Using
    the aggregation framework, change streams allow filtering and transforming notifications.
    They can be used with any programming language supported by an official MongoDB
    driver. However, they are not fully managed, requiring a running host to be maintained
    alongside the main application.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变更流**：此方法提供对数据变更的实时访问。应用程序可以订阅集合、数据库或整个部署中的变更，并立即做出反应，事件按顺序处理且可恢复。使用聚合框架，变更流允许过滤和转换通知。它们可以与任何官方
    MongoDB 驱动程序支持的编程语言一起使用。然而，它们不是完全管理的，需要维护一个运行的主机，以配合主应用程序。'
- en: Given that this book is written for Python developers, you will learn how to
    use a change stream written in Python. *Table 6.9* shows a Python 3 change stream
    using LangChain and OpenAI to embed the title and summary of an MDN article. It
    is triggered for new articles or changes to the title or summary following the
    data model from *Figure 6**.3* and the vector index from *Table 6.3*.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '由于本书是为 Python 开发者编写的，您将学习如何使用用 Python 编写的变更流。*表 6.9* 展示了使用 LangChain 和 OpenAI
    将 MDN 文章的标题和摘要嵌入的 Python 3 变更流。它根据 *图 6**.3* 中的数据模型和 *表 6.3* 中的向量索引触发新文章或标题或摘要的更改。 '
- en: '[PRE95]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Table 6.9: Change stream written in Python to set or update embeddings'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.9：用于设置或更新嵌入的 Python 编写的变更流
- en: Now that you have learned how to handle the data flow for setting or updating
    embeddings, you will learn about data freshness and retention, which are essential
    for delivering relevant and timely content.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经学会了如何处理设置或更新嵌入的数据流，您将学习关于数据新鲜度和保留度，这对于提供相关和及时的内容至关重要。
- en: Freshness and retention
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新鲜度和保留度
- en: Fresh data and effective retention strategies ensure that your content is relevant
    and delivered on time. **Freshness** keeps users engaged with the latest articles,
    comments, and recommendations. **Retention strategies** manage the data lifecycle,
    preserving valuable historical data for analytics while purging obsolete data.
    This section explores methods for ensuring up-to-date content and efficient data
    flow.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 新鲜数据和有效的保留策略确保您的内容相关且及时送达。**新鲜度**使用户保持对最新文章、评论和推荐的参与度。**保留策略**管理数据生命周期，保留有价值的历史数据以供分析，同时清除过时的数据。本节探讨了确保内容更新和高效数据流的方法。
- en: Real-time updates
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时更新
- en: The primary concern is to ingest and update new data in real time, making it
    available across all cloud regions. For the news site, this means new articles
    and their vector embeddings should be promptly persisted and replicated for global
    access.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 主要关注的是实时摄取和更新新数据，使其在所有云区域可用。对于新闻网站，这意味着新文章及其向量嵌入应迅速持久化和复制，以便全球访问。
- en: To achieve this with a distributed data model and application, use an ACID transaction
    to ensure that the article and its content embeddings are written together as
    a single unit. For an example of creating MongoDB transactions in Python, see
    [https://learn.mongodb.com/learn/course/mongodb-crud-operations-in-python/lesson-6-creating-mongodb-transactions-in-python-applications/learn?page=2](https://learn.mongodb.com/learn/course/mongodb-crud-operations-in-python/lesson-6-creating-mongodb-transactions-in-python-applications/learn?page=2).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用分布式数据模型和应用程序实现这一点，请使用ACID事务来确保文章及其内容嵌入作为一个单一单元一起写入。有关在Python中创建MongoDB事务的示例，请参阅[https://learn.mongodb.com/learn/course/mongodb-crud-operations-in-python/lesson-6-creating-mongodb-transactions-in-python-applications/learn?page=2](https://learn.mongodb.com/learn/course/mongodb-crud-operations-in-python/lesson-6-creating-mongodb-transactions-in-python-applications/learn?page=2)。
- en: 'Next, balance data reliability, consistency, and performance in a distributed
    setup using MongoDB’s tunable consistency with `writeConcern`, `readConcern`,
    and `readPreference`. These modifiers help to ensure data integrity and quick
    access. The following is an explanation of these modifiers, but for a deeper understanding,
    you can visit [https://www.mongodb.com/docs/manual/core/causal-consistency-read-write-concerns/](https://www.mongodb.com/docs/manual/core/causal-consistency-read-write-concerns/):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在分布式设置中使用MongoDB的`writeConcern`、`readConcern`和`readPreference`的可调一致性来平衡数据可靠性、一致性和性能。这些修饰符有助于确保数据完整性和快速访问。以下是对这些修饰符的解释，但为了更深入的理解，您可以访问[https://www.mongodb.com/docs/manual/core/causal-consistency-read-write-concerns/](https://www.mongodb.com/docs/manual/core/causal-consistency-read-write-concerns/)：
- en: '`writeConcern:majority` ensures data consistency and durability by acknowledging
    write operations only after data is written to the majority of replica set members,
    reducing the risk of data loss during failures. It is the default write concern.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`writeConcern:majority`通过仅在数据写入大多数副本集成员后确认写入操作来确保数据一致性和持久性，从而在故障期间降低数据丢失的风险。这是默认的写入关注点。'
- en: '`readConcern:majority` provides read consistency by ensuring that read operations
    return the most recent data acknowledged by the majority of the replica set members,
    providing a consistent view of the data across the application.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readConcern:majority`通过确保读取操作返回大多数副本集成员确认的最最新数据来提供读取一致性，为应用程序提供数据的一致视图。'
- en: '`readPreference:nearest` optimizes latency by directing read operations to
    the replica set member with the lowest network latency. For MDN, this minimizes
    response times by allowing each regional application deployment to read from the
    nearest MongoDB data and vector nodes, and balancing consistency and performance.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`readPreference:nearest`通过将读取操作定向到具有最低网络延迟的副本集成员来优化延迟。对于MDN，这通过允许每个区域应用程序部署从最近的MongoDB数据和向量节点读取，从而最小化响应时间，并平衡一致性和性能。'
- en: Now that you have learned how to ensure data availability and speed, the next
    focus is on data lifecycle management, a key aspect of data freshness and retention.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经学会了如何确保数据可用性和速度，接下来的重点是数据生命周期管理，这是数据新鲜度和保留的关键方面。
- en: Data lifecycle
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据生命周期
- en: '**Data lifecycle** refers to the various stages data goes through from creation
    to deletion, and how it may traverse and change systems or storage formats, including
    when data is archived or deleted. As latest content is added, older content may
    become less relevant.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据生命周期**指的是数据从创建到删除所经历的各个阶段，以及数据如何穿越和改变系统或存储格式，包括数据归档或删除时的情况。随着最新内容的添加，旧内容的相关性可能会降低。'
- en: 'For example, older articles can be moved to an archive database or cold storage,
    reducing storage costs and optimizing active database performance. However, moving
    data to cold storage may reduce search capabilities compared to the operational
    database. Here are three approaches for handling the data lifecycle, along with
    their trade-offs:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，旧文章可以移动到存档数据库或冷存储，从而降低存储成本并优化活动数据库的性能。然而，将数据移动到冷存储可能会比操作数据库降低搜索能力。以下是处理数据生命周期的三种方法及其权衡：
- en: '**All data in the operational cluster**: Keeping all data in the operational
    cluster is the most performant but costly approach, suitable for scenarios where
    most data is frequently accessed, such as global online games, authentication
    providers, or financial platforms. MongoDB Atlas supports this with sharded clusters
    and global clusters. Global clusters allocate *data zones* to cloud regions for
    capacity management and data locality.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作集群中的所有数据**：将所有数据保留在操作集群中是最高性能但成本最高的方法，适用于大多数数据频繁访问的场景，例如全球在线游戏、身份验证提供商或金融平台。MongoDB
    Atlas通过分片集群和全局集群支持这一点。全局集群为云区域分配**数据区域**以进行容量管理和数据本地性。'
- en: '**Active and historic operational data clusters**: This involves using high-performance
    hardware for recent data and less capable hardware for older data, balancing functionality,
    and cost savings. With MongoDB Atlas, data can be moved from active to historic
    cluster(s) using Cluster-to-Cluster Sync and TTL indexes. Other platforms such
    as Apache Kafka, Confluent, and Striim also support this method.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活跃和历史操作数据集群**：这涉及到使用高性能硬件处理最近的数据，使用能力较低的硬件处理旧数据，以平衡功能和成本节约。使用MongoDB Atlas，可以使用集群间同步和TTL索引将数据从活跃集群移动到历史集群。其他平台如Apache
    Kafka、Confluent和Striim也支持这种方法。'
- en: '**Active data cluster and historical storage**: Full historical data can be
    offloaded to cold storage while retaining key fields in the operational cluster,
    allowing for full or limited query and search capabilities. For MDN, this ensures
    that users can find historical articles through lexical semantic searches, with
    full articles stored in cold storage and accessed when needed. With MongoDB Atlas,
    this can be achieved using Online Archive and Data Federation. **Online Archive**
    automatically moves data from the cluster to lower-cost cloud storage based on
    the set expiration. **Data Federation** allows transparent querying of both clusters
    and the archive, regardless of the source.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活跃数据集群和历史存储**：可以将全部历史数据卸载到冷存储，同时在操作集群中保留关键字段，从而实现完全或有限的查询和搜索能力。对于MDN来说，这确保了用户可以通过词汇语义搜索找到历史文章，全文存储在冷存储中，并在需要时访问。使用MongoDB
    Atlas，可以通过在线归档和数据联邦来实现这一点。**在线归档**会根据设定的过期时间自动将数据从集群移动到成本更低的云存储。**数据联邦**允许透明地查询集群和归档，无论数据来源。'
- en: 'This section covered data lifecycle management, emphasizing how data is managed
    from creation to archival. You learned about three strategies: maintaining all
    data in the operational cluster for maximum performance, separating active and
    historical data to balance cost and performance, and offloading historical data
    to cold storage while retaining some search functionality. Now, you will learn
    about upgrading embedding models.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了数据生命周期管理，强调了数据从创建到归档的管理方式。你学习了三种策略：在操作集群中保留所有数据以实现最佳性能、分离活跃和和历史数据以平衡成本和性能，以及在保留一些搜索功能的同时将历史数据卸载到冷存储。现在，你将学习如何升级嵌入模型。
- en: Adopting new embedding models
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采用新的嵌入模型
- en: OpenAI superseded the `text-search-davinci-*-001` model with `text-embedding-ada-002`
    on December 15, 2022, and subsequently with `text-embedding-small/large` on January
    25, 2024\. It is likely that by the time you read this book, these models will
    be replaced too.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI于2022年12月15日用`text-embedding-ada-002`取代了`text-search-davinci-*-001`模型，随后于2024年1月25日用`text-embedding-small/large`取代。很可能在你阅读这本书的时候，这些模型也会被取代。
- en: As you learned in the [*Chapter 4*](B22495_04.xhtml#_idTextAnchor061), *Embedding
    Models*, embeddings from one model are not compatible with another. Re-embedding
    previously indexed data may be necessary as newer models are adopted. This is
    a resource-intensive activity that requires design considerations upfront.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[*第4章*](B22495_04.xhtml#_idTextAnchor061)“嵌入模型”中学习的，从一个模型中提取的嵌入与另一个模型不兼容。随着新模型的采用，可能需要重新嵌入先前索引的数据。这是一个资源密集型活动，需要提前进行设计考虑。
- en: 'You will need to choose an approach toward adopting new embedding models. You
    can either continue using the existing vector fields and perform lengthy all-or-nothing
    upgrades, double-embed for a period, or implement a gradual upgrade. Let''s explore
    these three approaches:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要选择一种方法来采用新的嵌入模型。你可以继续使用现有的向量字段并执行长时间的全有或全无升级，暂时进行双重嵌入，或者实施逐步升级。让我们探讨这三种方法：
- en: '**Use existing vector fields**: This approach keeps the application code intact
    but requires downtime to re-embed data and replace vector indexes. This approach
    is suitable if the re-embedding and re-indexing time fits within your allowable
    downtime windows.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用现有的向量字段**：这种方法保持应用程序代码完整，但需要停机时间来重新嵌入数据和替换向量索引。如果重新嵌入和重新索引时间适合您的允许停机窗口，则此方法适用。'
- en: '**Double-embed temporarily**: This approach double embeds fields for new or
    modified data using the old and new model. It uses a background job to add new
    embeddings for data that is not modified. When all data has double embeddings,
    the application will be updated and deployed to use the new embeddings. Once stable,
    the deprecated vectors and indexes can be removed with another background job.
    Ensure sufficient disk space and memory for when two sets of vectors coexist.
    This approach is suitable if the downtime windows are small and only accommodate
    application deployment times.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**临时双重嵌入**：这种方法使用旧模型和新模型双重嵌入新或修改后的字段。它使用后台作业为未修改的数据添加新的嵌入。当所有数据都有双重嵌入时，应用程序将更新并部署以使用新的嵌入。一旦稳定，可以另一个后台作业移除过时的向量和索引。确保有足够的磁盘空间和内存，因为当两组向量共存时。如果停机窗口很小且仅容纳应用程序部署时间，则此方法适用。'
- en: '`filter` type of MongoDB’s vector indexes (shown in *Table 6.3*), you can introduce
    a new field to distinguish between documents with old and new vectors and implement
    the union. Eventually, old vectors and indexes can be dropped, and you can remove
    unneeded logic. This approach is suitable if no downtime is allowed.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MongoDB的向量索引的`filter`类型（如*表6.3*所示），您可以引入一个新字段来区分具有旧向量和新向量的文档并实现联合。最终，可以删除旧向量和索引，并可以移除不必要的逻辑。如果允许没有停机时间，则此方法适用。
- en: By addressing these three main concerns—data ingestion and real-time updates,
    managing the data lifecycle and aging, and upgrading embedding models—your application
    can ensure that its data remains fresh and relevant, providing an optimal platform
    and striving for the best user experience. Now, you will learn about security
    and its considerations for AI-intensive applications.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解决这三个主要问题——数据摄取和实时更新、管理数据生命周期和老化、以及升级嵌入模型——您的应用程序可以确保其数据保持新鲜和相关性，提供一个最佳的平台并努力实现最佳的用户体验。现在，您将了解安全和其在人工智能密集型应用中的考虑因素。
- en: Security and RBAC
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安全性和RBAC
- en: '**Security measures** protect data from unauthorized access and breaches, while
    RBAC ensures appropriate access levels based on roles. Here are key security and
    RBAC strategies to protect data integrity and privacy:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**安全措施**保护数据免受未经授权的访问和泄露，而RBAC确保基于角色的适当访问级别。以下是保护数据完整性和隐私的关键安全和RBAC策略：'
- en: '**Data encryption and secure storage**: Encrypting data at rest and in transit
    is crucial for securing an application. Encryption at rest protects data from
    unauthorized access, while encryption in transit secures data as it moves between
    users and the application. MongoDB Atlas offers built-in integration with **AWS
    Key Management Service** (**AWS KMS**) for encryption at rest and TLS/SSL out
    of the box for data in transit.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据加密和安全的存储**：静态和传输中的数据加密对于确保应用程序的安全性至关重要。静态加密保护数据免受未经授权的访问，而传输加密确保数据在用户与应用程序之间移动时的安全性。MongoDB
    Atlas提供与**AWS密钥管理服务（AWS KMS**）的内置集成，用于静态加密，以及TLS/SSL作为数据传输的默认设置。'
- en: '**Access controls and user authentication**: RBAC manages permissions, ensuring
    that users access only necessary data and functionalities. In the case of MDN,
    separate roles, such as editors and readers, require various levels of access.
    Different database users on MongoDB can be set up with distinct levels of permissions
    following the principle of least privilege. For example, only the application
    identity used by the microservice that embeds data would have write permissions
    to the collections where embeddings are stored, while the application identity
    used by human actors would only have read permissions.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问控制和用户身份验证**：基于角色的访问控制（RBAC）管理权限，确保用户只能访问必要的数据和功能。在MDN的情况下，如编辑者和读者这样的不同角色需要不同级别的访问。根据最小权限原则，可以为MongoDB上的不同数据库用户设置不同级别的权限。例如，仅用于嵌入数据的微服务所使用的应用程序标识符将对存储嵌入的集合具有写入权限，而用于人类操作者的应用程序标识符则只有读取权限。'
- en: '**Monitoring and auditing**: Continuous monitoring and auditing detect and
    respond to security incidents in real time. Monitoring tools and audit logs track
    user activities and identify unusual access patterns. MongoDB Atlas offers advanced
    monitoring and alerting capabilities, allowing administrators to set up alerts
    for suspicious activities. Regularly reviewing audit logs ensures compliance with
    security policies and provides insights for improving security.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控和审计**：持续的监控和审计能够实时检测和响应安全事件。监控工具和审计日志跟踪用户活动并识别异常访问模式。MongoDB Atlas提供高级监控和警报功能，允许管理员为可疑活动设置警报。定期审查审计日志确保符合安全策略，并为改进安全提供见解。'
- en: '**Data backup and recovery**: Maintain data integrity and availability with
    regular backups to minimize downtime and loss during security breaches or incidents.
    MongoDB Atlas offers automated backup solutions with snapshots, ensuring quick
    recovery. If encryption at rest is enabled (for example, AWS KMS), embeddings
    and operational data are encrypted under the same key in both volumes and backups.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据备份和恢复**：通过定期备份维护数据完整性和可用性，以最小化安全漏洞或事件期间的中断和损失。MongoDB Atlas提供带有快照的自动化备份解决方案，确保快速恢复。如果启用了静态加密（例如，AWS
    KMS），则在卷和备份中，嵌入和操作数据都使用相同的密钥进行加密。'
- en: While there are many security-related concerns, the ones just covered should
    suffice to start building AI applications. Ensuring security is a continuous effort
    that organizations must adopt and enforce to maintain compliance, foster user
    trust, and safeguard application integrity.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有许多与安全相关的关注点，但刚刚提到的那些应该足以开始构建AI应用程序。确保安全是一个组织必须采用并执行的持续努力，以维持合规性、培养用户信任并保护应用程序完整性。
- en: Best practices for AI/ML application design
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI/ML应用程序设计的最佳实践
- en: This section covers best practices for the five concerns covered in this chapter—data
    modeling, data storage, data flow, data freshness and retention, and security
    and RBAC. These guidelines will help ensure that your application is efficient,
    scalable, and secure, providing a solid foundation for building reliable and high-performing
    AI apps. Here are the top two best practices for each aspect of your AI/ML application
    design.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了本章涵盖的五个关注点——数据建模、数据存储、数据流、数据新鲜度和保留以及安全和RBAC的最佳实践。这些指南将帮助确保您的应用程序高效、可扩展且安全，为构建可靠且高性能的AI应用程序提供坚实的基础。以下是AI/ML应用程序设计每个方面的前两个最佳实践。
- en: '**Data modeling**: The following techniques ensure efficiency and performance
    for handling embeddings:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据建模**：以下技术确保处理嵌入的高效性和性能：'
- en: '**Embeddings in separate collections**: Store embeddings in a separate collection
    to avoid bloated documents, especially when multiple embeddings and nested indexing
    limitations are involved. Duplicate fields to ensure efficient filtering and maintain
    performant searches.'
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在单独的集合中存储嵌入**：将嵌入存储在单独的集合中，以避免文档膨胀，尤其是在涉及多个嵌入和嵌套索引限制时。复制字段以确保高效的过滤并保持搜索性能。'
- en: '**Hybrid search**: Combine semantic and lexical searches using reciprocal rank
    fusion. This hybrid approach boosts search functionality by leveraging the strengths
    of both.'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合搜索**：使用互惠排名融合结合语义和词汇搜索。这种混合方法通过利用两者的优势来提升搜索功能。'
- en: '**Data storage**: To optimize database cluster sizing, implement the following
    best practices:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据存储**：为了优化数据库集群的大小，实施以下最佳实践：'
- en: '**Sufficient IOPS and RAM based on peak usage**: Calculate required IOPS based
    on peak access times and application read/write patterns. Ensure data and search
    nodes have enough RAM to handle the caching and indexing needs of the most requested
    data.'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于峰值使用情况的足够的IOPS和RAM**：根据峰值访问时间和应用程序的读写模式计算所需的IOPS。确保数据和搜索节点有足够的RAM来处理最请求数据的缓存和索引需求。'
- en: '**Local reads**: Deploying nodes across regions helps minimize read latency
    and enhances the user experience. Ensure that each region has all the nodes required
    to fully serve data locally.'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地读取**：在各个区域部署节点有助于最小化读取延迟并提升用户体验。确保每个区域都有所有必要的节点以完全本地提供服务。'
- en: '**Data flow**: Consider the following strategies for harnessing data flow effectively:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据流**：考虑以下策略以有效利用数据流：'
- en: '**Asynchronous embedding updates**: Ensure primary data consistency by updating
    vector embeddings asynchronously. This method accommodates scalability and unpredictable
    model response times, although it introduces temporary latency.'
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步嵌入更新**: 通过异步更新向量嵌入来确保主数据一致性。此方法适应可扩展性和不可预测的模型响应时间，尽管它引入了暂时的延迟。'
- en: '**Dynamic data handling**: Leverage technologies such as change streams, Atlas
    Triggers, Kafka, and Atlas Stream Processing to handle continuous updates, transformations,
    and logic execution.'
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态数据处理**: 利用变更流、Atlas触发器、Kafka和Atlas流处理等技术来处理连续更新、转换和逻辑执行。'
- en: '**Data freshness and retention**: The following best practices can ensure that
    your application is relevant and prompt:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据新鲜度和保留**: 以下最佳实践可以确保您的应用相关且及时：'
- en: '**Up-to-date embedding models**: Embeddings from one model are not compatible
    with another. Plan for model upgrades during downtime if possible, or consider
    gradual upgrades, which are architecturally complex but require no downtime. Leverage
    MongoDB’s flexible data model to transition between embeddings.'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最新的嵌入模型**: 一个模型的嵌入与另一个模型不兼容。如果可能，在停机期间计划模型升级，或者考虑逐步升级，这从架构上来说是复杂的，但不需要停机时间。利用MongoDB灵活的数据模型在嵌入之间进行转换。'
- en: '**Data tiering**: Implement a data aging strategy by moving older data to an
    archive cluster or cold storage while keeping recent data in high-performance
    clusters. Use broader MongoDB Atlas features such as Online Archive, Data Federation,
    and more for effective data tiering.'
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分层**: 通过将旧数据移动到归档集群或冷存储，同时保持最近的数据在高性能集群中，实现数据老化策略。使用MongoDB Atlas的更广泛功能，如在线归档、数据联邦等，以实现有效的数据分层。'
- en: '**Security and RBAC**: Following are the best practices for ensuring the security
    of your data:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**安全和RBAC（基于角色的访问控制）**: 以下是为确保您的数据安全的最佳实践：'
- en: '**RBAC**: Assign role-based permissions and follow the **principle of least
    privilege** (**PoLP**), ensuring that users and entities access only necessary
    data and actions. For instance, code embedding data should have write access only
    to embedding collections.'
  id: totrans-298
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RBAC（基于角色的访问控制）**: 分配基于角色的权限并遵循**最小权限原则**（**PoLP**），确保用户和实体只能访问必要的数据和操作。例如，代码嵌入数据应仅对嵌入集合具有写入访问权限。'
- en: '**Encryption and storage**: Turn on encryption at rest and integrate with KMS
    to ensure that all data volumes and backups are encrypted with your own key.'
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加密和存储**: 启用静态加密并集成KMS，以确保所有数据卷和备份都使用您自己的密钥进行加密。'
- en: Implementing these best practices boosts the efficiency, scalability, and security
    of your AI/ML applications. Though just a starting point, these guidelines lay
    a solid foundation for building reliable, high-performing systems. With these
    best practices, you can navigate the complexities of modern AI and prepare your
    applications for long-term success and adaptability in a rapidly evolving tech
    landscape.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 实施这些最佳实践可提高您AI/ML应用的效率、可扩展性和安全性。虽然这只是起点，但这些指南为构建可靠、高性能的系统奠定了坚实的基础。有了这些最佳实践，您可以导航现代AI的复杂性，并使您的应用为在快速发展的技术环境中长期成功和适应性做好准备。
- en: Summary
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter covered critical architectural considerations for developing intelligent
    applications. You learned about data modeling and how to evolve your model to
    fulfill use cases, address technical limitations, and consider patterns and anti-patterns.
    This approach ensures that data is not only useful but also accessible and optimally
    utilized across various components of your AI/ML system.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了开发智能应用的关键架构考虑因素。您学习了数据建模以及如何使模型适应用例、解决技术限制并考虑模式和反模式。这种方法确保数据不仅有用，而且可访问并在您的AI/ML系统的各个组件中得到优化利用。
- en: Data storage was another key aspect of this chapter, focusing on the selection
    of appropriate storage technologies based on different data types and the specific
    needs of the application. It highlighted the importance of accurately estimating
    storage requirements and other aspects of choosing the right MongoDB Atlas cluster
    configuration. The fictitious example of the MDN application served as a practical
    case study, illustrating how to apply these principles in a real-world scenario.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储是本章的另一个关键方面，侧重于根据不同的数据类型和应用程序的具体需求选择适当的存储技术。它强调了准确估计存储需求以及选择正确的MongoDB Atlas集群配置的其他方面的重要性。MDN应用程序的虚构示例作为一个实际案例研究，说明了如何在现实世界场景中应用这些原则。
- en: The chapter also explored the flow of data through ingestion, processing, and
    output to ensure data integrity and maintain the velocity of data operations.
    This chapter also addressed data lifecycle management, including the importance
    of data freshness and retention. You learned strategies for managing updates and
    changing embedding models used by your application.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还探讨了数据通过摄取、处理和输出的流程，以确保数据完整性并保持数据操作的速度。本章还讨论了数据生命周期管理，包括数据新鲜度和保留的重要性。您学习了管理更新和更改应用程序使用的嵌入模型策略。
- en: Security is a paramount concern in AI/ML applications, and you learned brief
    but important points about protecting the integrity of data and application logic.
    Concluding with a compilation of best practices, this chapter summarized key principles
    from data modeling, storage, flow, and security, offering practical advice to
    avoid common pitfalls and enhance the development of robust AI/ML applications.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能/机器学习应用中，安全性是一个至关重要的关注点，您学习了关于保护数据完整性和应用程序逻辑的简要但重要的要点。本章以最佳实践的汇编结束，总结了数据建模、存储、流动和安全性方面的关键原则，提供了避免常见陷阱并增强稳健人工智能/机器学习应用程序开发的实用建议。
- en: In the next chapter, you will explore different AI/ML frameworks, Python libraries,
    and publicly available APIs and other tools.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将探索不同的AI/ML框架、Python库、公开可用的API和其他工具。
- en: Part 2
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分
- en: 'Building Your Python Application: Frameworks, Libraries, APIs, and Vector Search'
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建您的Python应用程序：框架、库、API和向量搜索
- en: This following set of chapters will equip you with the necessary tools for AI
    development through detailed instructions and examples on enhancing developer
    and user experience with Python and retrieval-augmented generation.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将为您提供通过详细的说明和示例来增强开发者和使用者体验的必要工具，以用于人工智能开发。
- en: 'This part of the book includes the following chapters:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 本书本部分包括以下章节：
- en: '[*Chapter 7*](B22495_07.xhtml#_idTextAnchor162), *Useful Frameworks, Libraries,
    and APIs*'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B22495_07.xhtml#_idTextAnchor162)，*有用的框架、库和API*'
- en: '[*Chapter 8*](B22495_08.xhtml#_idTextAnchor180), *Implementing Vector Search
    in AI Applications*'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B22495_08.xhtml#_idTextAnchor180)，*在人工智能应用中实现向量搜索*'
