- en: Improving Your RNN Performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高你的 RNN 性能
- en: 'This chapter goes through some techniques for improving your recurrent neural
    network model. Often, the initial results from your model can be disappointing,
    so you need to find ways of improving them. This can be done with various methods
    and tools, but we will focus on two main areas:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一些提高递归神经网络模型效果的技术。通常，模型的初始结果可能令人失望，因此你需要找到改善它们的方法。可以通过多种方法和工具来实现这一点，但我们将重点关注两个主要领域：
- en: Improving the RNN model performance with data and tuning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过数据和调整来改善 RNN 模型性能
- en: Optimizing the TensorFlow library for better results
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化 TensorFlow 库以获得更好的结果
- en: First, we will see how more data, as well as tuning the hyperparameters, can
    yield significantly better results. Then our focus will shift to getting the most
    out of the built-in TensorFlow functionality. Both approaches are applicable to
    any task that involves the neural network model, so the next time you want to
    do image recognition with convolutional networks or fix a rescaled image with
    GAN, you can apply the same techniques for perfecting your model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看到如何通过增加数据量以及调整超参数，来获得显著更好的结果。然后，我们将重点放在如何充分利用 TensorFlow 的内建功能。这两种方法适用于任何涉及神经网络模型的任务，因此，下次你想使用卷积网络进行图像识别或用
    GAN 修复一个重新缩放的图像时，可以应用相同的技巧来完善你的模型。
- en: Improving your RNN model
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改善你的 RNN 模型
- en: 'When working on a problem using RNN (or any other network), your process looks
    like this:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 RNN（或任何其他网络）解决问题时，你的过程大致如下：
- en: '![](img/f0060b1e-ad15-412c-97bd-61882cb9331a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0060b1e-ad15-412c-97bd-61882cb9331a.png)'
- en: First, you come up with an **idea for the model**, its hyperparameters, the
    number of layers, how deep the network should be, and so on. Then the model is **implemented
    and trained** in order to produce some results. Finally, these results are **assessed**
    and the necessary modifications are made. It is rarely the case that you'll receive
    meaningful results from the first run. This cycle may occur multiple times until
    you are satisfied with the outcome.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你提出**模型的想法**，其超参数、层数、网络的深度等。然后，**实现并训练**模型，以产生一些结果。最后，**评估**这些结果并进行必要的修改。通常情况下，你不会在第一次运行时得到有意义的结果。这个周期可能会多次发生，直到你对结果感到满意为止。
- en: 'Considering this approach, one important question comes to mind: *How can we
    change the model so the next cycle produces better results?*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这种方法，一个重要的问题浮现出来：*我们如何改变模型，使得下一个周期产生更好的结果？*
- en: This question is tightly connected to your understanding of the network's results.
    Let's discuss that now.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题与理解网络结果密切相关。现在让我们来讨论这个问题。
- en: 'As you already know, in the beginning of each model training, you need to prepare
    lots of quality data. This step should happen before the **Idea** part of the
    aforementioned cycle. Then, during the Idea stage, you should come up with the
    actual neural network and its characteristics. After that comes the Code stage,
    where you use your data to supply the model and perform the actual training. There
    is something important to keep in mind—*once your data is collected, you need
    to split it into 3 parts: training (80%), validation (10%) and testing (10%)*.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，在每次模型训练的开始，你需要准备大量优质数据。这一步应发生在上述循环的**想法**部分之前。然后，在想法阶段，你应该构思出实际的神经网络及其特征。接下来是代码阶段，在该阶段，你用数据来支持模型并进行实际训练。有一点很重要——*一旦你的数据收集完成，你需要将其分为三部分：训练（80%）、验证（10%）和测试（10%）*。
- en: The Code stage only uses the training part of your data. Then, the Experiment stage
    uses the validation part to evaluate the model. Based on the results of these
    two operations, we will make the necessary changes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 代码阶段仅使用训练部分数据。然后，实验阶段使用验证部分数据来评估模型。根据这两项操作的结果，我们将进行必要的调整。
- en: You should use the testing data **only** after you have gone through all the
    necessary cycles and have identified that your model is performing well. The testing
    data will help you understand the rate of accuracy you are receiving on unseen
    data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在你完成所有必要的循环并确定模型表现良好后，你**只应**使用测试数据。测试数据将帮助你理解在未见过的数据上你得到的准确率。
- en: 'At the end of each cycle, you need to determine how good your model is. Based
    on the results, you will see that your model is always either **underfitting**
    (**high bias**) or **overfitting** (**high variance**) the data (by varying degrees).
    You should aim for both the bias and variance to be low, so there is almost no
    underfitting or overfitting. The next diagram may help you understand this concept
    better:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个周期结束时，你需要评估你的模型表现如何。根据结果，你会发现你的模型总是出现**欠拟合**（**高偏差**）或**过拟合**（**高方差**）（程度不同）。你应该努力使偏差和方差都很低，这样几乎不会发生欠拟合或过拟合。下图可能帮助你更好地理解这一概念：
- en: '![](img/cf47792e-c51b-4d37-b84f-97657b1f4f3d.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf47792e-c51b-4d37-b84f-97657b1f4f3d.png)'
- en: 'Examining the preceding diagram, we can state the following definitions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看前面的图表，我们可以给出以下定义：
- en: '**Underfitting (high bias)**: This occurs when the network is not influenced
    enough by the training data, and generalizes the prediction'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠拟合（高偏差）**：当网络没有足够地受到训练数据的影响，并且过于泛化预测时，就会发生欠拟合。'
- en: '**Just Right (low bias, low variance)**: This occurs when the network makes
    quality predictions, both during training and in the general case during testing'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**恰到好处（低偏差，低方差）**：当网络能够做出高质量的预测时，无论是在训练期间，还是在测试时的通用情况下。'
- en: '**Overfitting (high variance)**: This occurs when the network is influenced
    by the training data too much, and makes false decisions on new entries.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合（高方差）**：当网络受到训练数据的影响过大，并且对新输入做出错误决策时，就会发生过拟合。'
- en: The preceding diagram may be helpful to understand the concepts of high bias
    and high variance, but it is difficult to apply this to real examples. The problem
    is that we normally deal with data of more than two dimensions. That is why we
    will be using the loss (error) function values produced by the model to make the
    same evaluation for higher dimensional data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表有助于理解高偏差和高方差的概念，但将这些应用到实际案例中比较困难。问题在于我们通常处理的是多于两维的数据。因此，我们将使用模型生成的损失（错误）函数值来对更高维的数据进行相同的评估。
- en: 'Let''s say we are evaluating the Spanish-to-English translator neural network
    from [Chapter 4](982f956b-d1b6-446c-85b1-71f55faf114f.xhtml), *Creating a Spanish-to-English
    Translator*. We can assume that the lowest possible error on that task can be
    produced by a human, and it is 1.5%. Now we will evaluate the results based on
    all the error combinations that our network can give:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在评估[第4章](982f956b-d1b6-446c-85b1-71f55faf114f.xhtml)中的西班牙语到英语的翻译神经网络，*创建一个西班牙语到英语的翻译器*。我们可以假设，最小的错误率是由人类完成的，约为1.5%。现在我们将基于我们网络可能给出的所有错误组合来评估结果：
- en: 'Training data error: ~2%; Validation data error: ~14%: **high variance**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据误差：~2%；验证数据误差：~14%：**高方差**
- en: 'Training data error: ~14%; Validation data error: ~15%: **high bias**'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据误差：~14%；验证数据误差：~15%：**高偏差**
- en: 'Training data error: ~14%; Validation data error: ~30%: **high variance, high
    bias**'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据误差：~14%；验证数据误差：~30%：**高方差，高偏差**
- en: 'Training data error:~2%; Validation data error: ~2.4%: **low variance, low
    bias**'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据误差：~2%；验证数据误差：~2.4%：**低方差，低偏差**
- en: The desired output is having low variance and low bias. Of course, it takes
    a lot of time and effort to get this kind of improvement, but in the end, it is
    worth doing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所期望的结果是既有低方差又有低偏差。当然，得到这种改进需要大量的时间和精力，但最终是值得的。
- en: You have now got familiar with how to read your model results and evaluate the
    model's performance. Now, let's see what can be done to **lower both the variance
    and the bias of the model**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经熟悉了如何读取模型结果并评估模型的表现。接下来，我们来看一下如何**降低模型的方差和偏差**。
- en: '***How can we lower the variance? (fixing overfitting)***'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '***我们如何降低方差？（解决过拟合）***'
- en: A very useful approach is to collect and transform more data. This will generalize
    the model and make it perform well on both the training and validation sets.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一种非常有用的方法是收集和转换更多的数据。这将使模型具有更好的泛化能力，并在训练集和验证集上都表现良好。
- en: '***How can we lower the bias?* *(fixing underfitting)***'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '***我们如何降低偏差？*（解决欠拟合）***'
- en: This can be done by increasing the network depth—that is, changing the numbers
    of layers and of hidden units, and tuning the hyperparameters.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过增加网络的深度来实现——即改变层数和隐藏单元的数量，并调整超参数。
- en: Next, we will cover both of these approaches and see how to use them effectively
    to improve our neural network's performance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论这两种方法，并了解如何有效地使用它们来提高我们神经网络的性能。
- en: Improving performance with data
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过数据改善性能
- en: 'A large amount of quality data is critical for the success of any deep learning
    model. A good comparison can be made to other algorithms, where an increased volume
    of data does not necessarily improve performance:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的优质数据对于任何深度学习模型的成功至关重要。可以做一个很好的比较：对于其他算法，数据量的增加并不一定能提高性能：
- en: '![](img/b802c1b6-d305-48c4-a183-2a451e703b50.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b802c1b6-d305-48c4-a183-2a451e703b50.png)'
- en: But this doesn't mean that gathering more data is always the right approach.
    For example, if our model suffers from underfitting, more data won't increase
    the performance. On the other hand, solving the overfitting problem can be done
    using exactly that approach.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不意味着收集更多数据总是正确的方法。例如，如果我们的模型出现了欠拟合，更多数据并不会提高性能。另一方面，解决过拟合问题可以通过使用正是这种方法来完成。
- en: 'Improving the model performance with data comes in three steps: **selecting
    data**, **processing data**, and **transforming data**. It is important to note
    that all three steps should be done according to your specific problem. For some
    tasks, such as recognizing digits inside an image, a nicely formatted dataset
    can be found easily. For more concrete tasks (e.g. analyzing images from plants),
    you may need to experiment and come up with non-trivial decisions.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据提升模型性能分为三个步骤：**选择数据**、**处理数据**和**转换数据**。需要注意的是，所有这三步应该根据你具体的问题来完成。例如，对于某些任务，如识别图像中的数字，一个格式良好的数据集很容易找到。而对于更为具体的任务（例如分析植物图像），你可能需要进行实验并做出一些非平凡的决策。
- en: Selecting data
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择数据
- en: This is a pretty straightforward technique. You either collect more dataor invent
    more training examples.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种相当直接的技术。你要么收集更多数据，要么发明更多的训练示例。
- en: Finding more data can be done using an online collection of datasets ([https://skymind.ai/wiki/open-datasetshttps://skymind.ai/wiki/open-datasets](https://skymind.ai/wiki/open-datasetshttps://skymind.ai/wiki/open-datasets)). Other
    methods are to scrape web pages, or use the advanced options of Google Search ([https://www.google.com/advanced_search](https://www.google.com/advanced_search)).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 查找更多数据可以通过在线数据集集合来完成 ([https://skymind.ai/wiki/open-datasetshttps://skymind.ai/wiki/open-datasets](https://skymind.ai/wiki/open-datasetshttps://skymind.ai/wiki/open-datasets))。其他方法包括抓取网页，或者使用Google搜索的高级选项 ([https://www.google.com/advanced_search](https://www.google.com/advanced_search))。
- en: On the other hand, inventing or augmenting data is a challenging and complex
    problem, especially if we are trying to generate text or images. For example,
    a new approach ([https://www.quora.com/What-data-augmentation-techniques-are-available-for-deep-learning-on-text](https://www.quora.com/What-data-augmentation-techniques-are-available-for-deep-learning-on-text)) for
    augmenting text was created recently. It is done by translating an English sentence
    to another language and then back to English. This way we are getting two slightly
    different but meaningful sentences, which increases and diversifies our dataset substantially.
    Another interesting technique for augmenting data, specifically for RNN language
    models, can be found in the paper on *Data Noising as Smoothing in Neural Network
    Language Models* ([https://arxiv.org/abs/1703.02573](https://arxiv.org/abs/1703.02573)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，发明或增加数据是一个具有挑战性和复杂的问题，特别是当我们尝试生成文本或图像时。例如，最近创建了一种增强文本的新方法 ([https://www.quora.com/What-data-augmentation-techniques-are-available-for-deep-learning-on-text](https://www.quora.com/What-data-augmentation-techniques-are-available-for-deep-learning-on-text))。这个方法通过将一句英语句子翻译成另一种语言，然后再翻译回英语来完成。这样，我们得到了两句稍有不同但有意义的句子，从而大大增加和多样化了我们的数据集。另一种增强数据的有趣技术，专门针对RNN语言模型，可以在论文*《神经网络语言模型中的数据噪声作为平滑》*中找到 ([https://arxiv.org/abs/1703.02573](https://arxiv.org/abs/1703.02573))。
- en: Processing data
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数据
- en: 'After you have selected the required data, the time comes for processing. This
    can be done with these three steps:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择所需数据后，接下来是处理阶段。这可以通过以下三个步骤来完成：
- en: '**Formatting**: This involves converting the data into the most suitable format
    for your application. Imagine, for example, that your data is the text from thousands
    of PDF files. You should extract the text and covert the data into CSV format.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**格式化**：这涉及将数据转换为最适合你应用的格式。例如，假设你的数据是来自成千上万的PDF文件中的文本。你应该提取文本并将数据转换为CSV格式。'
- en: '**Cleaning**: Often, it is the case that your data may be incomplete. For example,
    if you have scraped book metadata from the internet, some entries may have missing
    data (such as ISBN, date of writing, and so on). Your job is to decide whether
    to fix or discard the metadata for the whole book.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清理**：通常情况下，数据可能是缺失的。例如，如果你从互联网上抓取了书籍元数据，某些条目可能会缺少数据（例如ISBN、写作日期等）。你的任务是决定是修复这些元数据还是舍弃整个书籍的元数据。'
- en: '**Sampling**: Using a small part of the dataset can reduce computational time
    and speed up your training cycles while you are determining the model accuracy.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样**：使用数据集的一小部分可以减少计算时间，并在确定模型准确性时加速训练过程。'
- en: The order of the preceding steps is not determined, and you may revisit them
    multiple times.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 前面步骤的顺序并没有固定，你可以多次回头再做。
- en: Transforming data
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换
- en: Finally, you need to transform the data using techniques such as scaling, decomposition,
    and feature selection. First, it is good to plot/visualize your data using Matplotlib
    (a Python library) or TensorFlow's TensorBoard ([https://www.tensorflow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要使用缩放、分解和特征选择等技术来转换数据。首先，最好使用Matplotlib（一个Python库）或TensorFlow的TensorBoard（[https://www.tensorflow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)）来绘制/可视化你的数据。
- en: '*Scaling* is a technique that converts every entry into a number within a specific
    range (0-1) without mitigating its effectiveness. Normally, scaling is done within the
    bounds of your activation functions. If you are using sigmoid activation functions,
    rescale your data to values between 0 and 1\. If you''re using the hyperbolic
    tangent (tanh), rescale to values between -1 and 1\. This applies to inputs (x)
    and outputs (y).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*缩放*是一种将每个条目转换为特定范围内（0-1）的数字，而不降低其有效性的技术。通常，缩放是在激活函数的范围内完成的。如果你使用的是sigmoid激活函数，请将数据缩放到0到1之间的值。如果使用的是双曲正切（tanh）激活函数，则将数据缩放到-1到1之间的值。这适用于输入（x）和输出（y）。'
- en: '*Decomposition* is a technique of splitting some features into their components
    and using them instead. For example, the feature time may have minutes and hours,
    but we care only about the minutes.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*分解*是一种将某些特征分解为其组件并使用它们的技术。例如，特征“时间”可能包含分钟和小时，但我们只关心分钟。'
- en: '*Feature selection* is one of the most important decisions you would make when
    building your model. A great tutorial to follow when deciding how to choose the
    most appropriate features is Jason Brownlee''s *An Introduction to Feature Selection*
    ([https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征选择*是构建模型时最重要的决策之一。选择最合适特征的一个很好的教程是Jason Brownlee的*特征选择简介*（[https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)）。'
- en: Processing and transforming data can be accomplished using the vast selection
    of Python libraries, such as NumPy, among others. They turn out to be pretty handy
    when it comes to data manipulation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的处理和转换可以使用众多Python库来实现，例如NumPy等。这些工具在数据处理时非常有用。
- en: After you have gone through all of the preceding steps (probably multiple times),
    you can move forward to building your neural network model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成所有前面的步骤（可能需要多次操作）之后，你可以继续构建你的神经网络模型。
- en: Improving performance with tuning
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过调优提高性能
- en: After selecting, processing, and transforming your data, it's time for a second
    optimization technique—hyperparameter tuning. This approach is one of the most
    important components in building your model and you need to spend the time necessary
    to execute it well.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择、处理和转换数据之后，是时候进行第二次优化技术——超参数调优。这种方法是构建模型中最重要的组成部分之一，你需要花时间将其执行得当。
- en: Every neural network model has parameters and hyperparameters. These are two
    distinct sets of values. Parameters are learned by the model during training,
    such as weights and biases. On the other hand, hyperparameters are predefined
    values that are selected after careful observation. In a standard recurrent neural
    network, the set of hyperparameters includes the number of hidden units, number
    of layers, RNN model type, sequence length, batch size, number of epochs (iterations),
    and the learning rate.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 每个神经网络模型都有参数和超参数。这是两个不同的值集合。参数是模型在训练过程中学习到的，例如权重和偏差。另一方面，超参数是预先定义的值，这些值在仔细观察后选择。在标准的递归神经网络中，超参数集合包括隐藏单元数、层数、RNN模型类型、序列长度、批次大小、迭代次数（epoch）和学习率。
- en: Your task is to identify the best of all possible combinations so that the network
    performs pretty well. This is a pretty challenging task and often takes a lot
    of time (hours, days, even months) and computational power.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你的任务是识别所有可能组合中的最佳选择，以使网络表现得相当好。这是一个相当具有挑战性的任务，通常需要大量时间（几个小时、几天甚至几个月）和计算能力。
- en: 'Following Andrew Ng''s tutorial on hyperparameter tuning ([https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc](https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc)),
    we can separate this process into two different techniques: *Pandas* vs *Caviar*.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Andrew Ng关于超参数调优的教程（[https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc](https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc)），我们可以将此过程分为两种不同的技术：*Pandas*与*Caviar*。
- en: The *Pandas* approach follows the way pandas (that is, the animal) raise their
    children. We initialize our model with a specific set of parameters, and then
    improve these values after every training operation until we achieve delightful
    results. This approach is ideal if you lack computational power and multiple GPUs
    to train neural networks simultaneously.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*Pandas*方法遵循熊猫（即动物）养育后代的方式。我们用一组特定的参数初始化模型，然后在每次训练操作后改进这些值，直到取得令人满意的结果。如果你缺乏计算能力或多GPU来同时训练神经网络，这种方法是理想的选择。'
- en: The *Caviar* approach follows the way fish reproduce. We introduce multiple
    models at once (using different sets of parameters) and train them at the same
    time, while tracking the results. This technique will likely require access to
    more computational power.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*Caviar*方法遵循鱼类繁殖的方式。我们同时引入多个模型（使用不同的参数集），并在训练的同时追踪结果。这种技术可能需要更多的计算能力。'
- en: 'Now the question becomes: *How can we decide what should be included in our
    set of hyperparameters?*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在问题变成了：*我们如何决定哪些内容应包含在超参数集内？*
- en: 'Summarizing a great article on hyperparameters optimization ([http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe)),
    we can define five ways for tuning:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一篇关于超参数优化的精彩文章（[http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe)），我们可以定义五种调优方法：
- en: '**Grid search**'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网格搜索**'
- en: '**Random search**'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机搜索**'
- en: '**Hand-tuning**'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**手动调优**'
- en: '**Bayesian optimization**'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝叶斯优化**'
- en: '**Tree-structured Parzen Estimators** (**TPE**)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树结构Parzen估计器**（**TPE**）'
- en: During the beginning phase of your deep learning journey, you will mostly be utilizing
    grid search, random search, and hand-tuning. The last two techniques are more
    complex in terms of understanding and implementation. We will cover both of them
    in the following section, but bear in mind that, for trivial tasks, you can go
    with normal hand-tuning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习之旅的初期阶段，你主要会使用网格搜索、随机搜索和手动调优。后两种技术在理解和实施上更为复杂。我们将在接下来的部分讨论这两种方法，但请记住，对于简单的任务，你可以选择正常的手动调优。
- en: Grid search
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网格搜索
- en: 'This is the most straightforward way of finding the right hyperparameters.
    It follows the approach in this graph:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这是找到合适超参数的最直接方法。它遵循以下图表中的方法：
- en: '![](img/3e6f5dba-6b9c-42ba-a2bc-6ac19146dfae.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3e6f5dba-6b9c-42ba-a2bc-6ac19146dfae.png)'
- en: Here, we generate all the possible combinations of values for the hyperparameters
    and perform separate training cycles. This works for small neural networks, but
    is impractical for more complex tasks. That is why we should use the better approach
    listed in the following section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们生成所有可能的超参数值组合，并进行单独的训练周期。对于小型神经网络来说，这种方法可行，但对于更复杂的任务来说则不切实际。这就是为什么我们应该使用以下部分列出的更好的方法。
- en: Random search
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机搜索
- en: 'This technique is similar to grid search. You can follow the graph here:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术类似于网格搜索。你可以参考这里的图表：
- en: '![](img/99c9f7e1-d9fe-48a9-8a2c-62eebf40cc6d.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99c9f7e1-d9fe-48a9-8a2c-62eebf40cc6d.png)'
- en: Instead of taking all the possible combinations, we sample a smaller set of
    random values and use these values to train the model. If we see that a particular
    group of closely positioned dots tends to perform better, we can examine this
    region more closely and focus on it.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是考虑所有可能的组合，而是随机抽取一小部分值，并用这些值来训练模型。如果我们发现某一组位置接近的点表现较好，我们可以更仔细地检查这个区域，并专注于它。
- en: Hand-tuning
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动调优
- en: Bigger networks normally require more time for training. This is why the aforementioned
    approaches are not ideal for such situations. In these cases, we often use the
    hand-tuning technique. The idea is to initially try one set of values, and then
    evaluate the performance. Then, our intuition, as well as our learning experience,
    may lead to ideas on a specific sequence of changes. We perform those tweaks and
    learn new things about the model. After several iterations, we have a good understanding
    of what needs to change for future improvement.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的网络通常需要更多的训练时间。这就是为什么前述方法不适用于这种情况。在这些情况下，我们通常使用手动调优技术。其理念是初步尝试一组值，然后评估性能。之后，我们的直觉和学习经验可能会引导我们提出一系列特定的修改步骤。我们进行这些调整，并学到新的东西。经过若干次迭代，我们对未来改进所需的变动有了充分的理解。
- en: Bayesian optimization
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: This approach is a way of learning the hyperparameters without the need to manually
    determine different values. It uses a Gaussian process that utilizes a set of
    previously evaluated parameters, and the resultant accuracy, to make an assumption
    about unobserved parameters. An acquisition function uses this information to
    suggest the next set of parameters. For more information, I suggest watching Professor
    Hinton's lecture on *Bayesian optimization of Hyper Parameters *([https://www.youtube.com/watch?v=cWQDeB9WqvU](https://www.youtube.com/watch?v=cWQDeB9WqvU)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是一种无需手动确定不同值的超参数学习方式。它使用高斯过程，利用一组先前评估过的参数及其结果精度，对未观察到的参数做出假设。一个采集函数利用这些信息来建议下一组参数。更多信息，我建议观看
    Hinton 教授的讲座，*超参数的贝叶斯优化*（[https://www.youtube.com/watch?v=cWQDeB9WqvU](https://www.youtube.com/watch?v=cWQDeB9WqvU)）。
- en: Tree-structured Parzen Estimators (TPE)
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树状帕尔岑估计器（TPE）
- en: The idea behind this approach is that, at each iteration, TPE collects new observations,
    and at the end of the iteration, the algorithm decides which set of parameters
    it should try next. For more information, I suggest taking a look at this amazing
    article on *Hyperparameters optimization for Neural Networks* ([http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法背后的理念是，在每次迭代中，TPE 会收集新的观察结果，迭代结束时，算法会决定应该尝试哪一组参数。更多信息，我建议阅读这篇精彩的文章，*神经网络超参数优化*（[http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe)）。
- en: Optimizing the TensorFlow library
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化 TensorFlow 库
- en: This section focuses mostly on practical advice that can be directly implemented
    in your code. The TensorFlow team has provided a large set of tools that can be
    utilized to improve your performance. These techniques are constantly being updated
    to achieve better results. I strongly recommend watching TensorFlow's video on
    training performance from the 2018 TensorFlow conference ([https://www.youtube.com/watch?v=SxOsJPaxHME](https://www.youtube.com/watch?v=SxOsJPaxHME)).
    This video is accompanied by nicely aggregated documentation, which is also a
    must-read ([https://www.tensorflow.org/performance/](https://www.tensorflow.org/performance/)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本节主要关注可以直接在代码中实现的实用建议。TensorFlow 团队提供了一套庞大的工具集，可以用来提高你的性能。这些技术正在不断更新，以取得更好的结果。我强烈推荐观看
    TensorFlow 2018 年大会上的训练性能视频（[https://www.youtube.com/watch?v=SxOsJPaxHME](https://www.youtube.com/watch?v=SxOsJPaxHME)）。此视频附带了精心整理的文档，也是必读之物（[https://www.tensorflow.org/performance/](https://www.tensorflow.org/performance/)）。
- en: Now, let's dive into more details around what you can do to achieve faster and
    more reliable training.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地探讨如何实现更快、更可靠的训练。
- en: 'Let''s first start with an illustration from TensorFlow that presents the general
    steps of training a neural network. You can divide this process into three phases:
    **data processing**, **performing training**, and** optimizing gradients**:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从 TensorFlow 提供的插图开始，展示了训练神经网络的总体步骤。你可以将这个过程分为三个阶段：**数据处理**、**执行训练**和**优化梯度**：
- en: '**Data processing** **(step 1)**: This phase includes fetching the data (locally
    or from a network) and transforming it to fit our needs. These transformations
    might include augmentation, batching, and so on. Normally, these operations are
    done on the **CPU**.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据处理** **(第 1 步)**：此阶段包括获取数据（本地或来自网络）并将其转化为符合我们需求的格式。这些转换可能包括数据增强、批处理等。通常，这些操作是在**CPU**上执行的。'
- en: '**Perform training (steps 2a, 2b and 2c)**: This phase includes computing the
    forward pass during training, which requires a specific neural network model—LSTM,
    GPU, or a basic RNN in our case. These operations utilize powerful **GPUs** and
    **TPUs**.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**执行训练（第 2a、2b 和 2c 步）**：此阶段包括在训练期间进行前向传播计算，这需要一个特定的神经网络模型——LSTM、GPU，或者在我们这个案例中是基础的
    RNN。这些操作使用强大的**GPU**和**TPU**。'
- en: '**Optimize gradients (step 3)**: This phase includes the process of minimizing
    the loss function with the aim of optimizing the weights. The operation is again
    performed on **GRUs** and **TPUs**.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**优化梯度（第 3 步）**：此阶段包括最小化损失函数以优化权重的过程。这个操作同样是在**GRU**和**TPU**上执行的。'
- en: 'This graph illustrates the above steps:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图展示了上述步骤：
- en: '![](img/48d71258-83c8-4cae-bb5f-4d6f279f9125.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48d71258-83c8-4cae-bb5f-4d6f279f9125.png)'
- en: Next, let's explain how to improve each of these steps.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们解释如何改进这些步骤。
- en: Data processing
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据处理
- en: You need to examine if loading and transforming the data is the bottleneck of
    your performance. You can do this with several approaches, some of which involve
    estimating the time it takes to perform these tasks, as well as tracking the CPU
    usage.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要检查数据加载和转换是否成为性能瓶颈。你可以通过几种方法来进行检查，其中一些方法包括估算执行这些任务所需的时间，并跟踪CPU使用情况。
- en: After you have determined that these operations are slowing down the performance
    of your model, it's time to apply some useful techniques to speed things up.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你确定这些操作会降低模型的性能，就该应用一些有效的技巧来加速这一过程。
- en: 'As we said, these operations (loading and transforming data) should be performed
    on the CPU, rather than the GPU, so that you free up the latter for training.
    To ensure this, wrap your code as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所说，这些操作（数据加载和转换）应该在 CPU 上执行，而不是 GPU 上，这样你就可以为训练腾出 GPU。为确保这一点，可以按照以下方式包装你的代码：
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, you need to focus on both the process of loading (fetching) and transforming
    the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你需要专注于数据的加载（获取）和转换过程。
- en: Improving data loading
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进数据加载
- en: The TensorFlow team has been working hard to make this as easy as possible by
    providing the `tf.data` API ([https://www.tensorflow.org/performance/performance_guide](https://www.tensorflow.org/performance/performance_guide)), which
    works incredibly well. To learn more about it and understand how to use it efficiently,
    I recommend watching TensorFlow's talk on `tf.data` ([https://www.youtube.com/watch?v=uIcqeP7MFH0](https://www.youtube.com/watch?v=uIcqeP7MFH0)).
    This API should always be used, instead of the standard `feed_dict` approach you
    have seen so far.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow团队一直在努力使这一过程尽可能简单，提供了`tf.data` API（[https://www.tensorflow.org/performance/performance_guide](https://www.tensorflow.org/performance/performance_guide)），其效果非常好。要了解更多内容并学习如何高效使用它，我推荐观看TensorFlow关于`tf.data`的演讲（[https://www.youtube.com/watch?v=uIcqeP7MFH0](https://www.youtube.com/watch?v=uIcqeP7MFH0)）。在此之前，你所看到的标准`feed_dict`方法应该始终被替代。
- en: Improving data transformation
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改善数据转换
- en: 'Transformations can come in different forms, for example, cropping images,
    splitting text, and rendering and batching files. TensorFlow offers solutions
    for these techniques. For example, if you are cropping images before training,
    it is good to use `tf.image.decode_and_crop_jpeg`, which decodes only the part
    of the image required. Another optimization can be made in the batching process.
    The TensorFlow library offers two methods:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换可以有不同的形式，例如裁剪图像、分割文本、渲染和批处理文件。TensorFlow为这些技术提供了解决方案。例如，如果你在训练前裁剪图像，最好使用`tf.image.decode_and_crop_jpeg`，该函数只解码图像中需要的部分。另一个可以优化的地方是批处理过程。TensorFlow库提供了两种方法：
- en: '[PRE1]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The second method is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法如下：
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s clarify these lines:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们澄清一下这些行：
- en: 'Batch normalization is performed to a neural network model to speed up the
    process of training. Refer to this amazing article, *Batch Normalization in Neural
    Networks*, for more details: [https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量归一化是对神经网络模型进行的一种操作，目的是加速训练过程。有关更多细节，请参阅这篇精彩的文章，*神经网络中的批量归一化*：[https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)。
- en: The `fused` parameter indicates whether or not the method should combine the
    multiple operations, required for batch normalization, into a single kernel.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fused`参数指示该方法是否应将执行批量归一化所需的多个操作合并为一个内核。'
- en: The `data_format` parameter refers to the structure of the Tensor passed to
    a given Operation (such as summation, division, training, and so on). A good explanation
    can be found under *Data formats* in the TensorFlow performance guide ([https://www.tensorflow.org/performance/](https://www.tensorflow.org/performance/)).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data_format`参数指的是传递给某个操作（如求和、除法、训练等）的张量的结构。可以在TensorFlow性能指南中的*数据格式*部分找到一个很好的解释（[https://www.tensorflow.org/performance/](https://www.tensorflow.org/performance/)）。'
- en: Performing the training
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行训练
- en: Now, let's move on to the phase of performing the training. Here, we are using
    one of TensorFlow's built-in functions for initializing recurrent neural network
    cells and calculating their weights using the preprocessed data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入训练阶段。在这里，我们使用TensorFlow内置的函数来初始化递归神经网络单元，并使用预处理过的数据计算它们的权重。
- en: 'Depending on your situation, different techniques for optimizing your training
    may be more appropriate:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的情况，优化训练的不同技术可能更合适：
- en: For small and experimental models, you can use `tf.nn.rnn_cell.BasicLSTMCell`.
    Unfortunately, this is highly inefficient and takes up more memory than the following
    optimized versions. That is why using it is **not** recommended, unless you are
    just experimenting.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于小型和实验性的模型，你可以使用`tf.nn.rnn_cell.BasicLSTMCell`。不幸的是，这种方法效率低下，并且比以下优化版本占用更多的内存。因此，除非你只是做实验，否则**不**推荐使用它。
- en: An optimized version of the previous code is `tf.contrib.rnn.LSTMBlockFusedCell`.
    It should be used when you don't have access to GPUs or TPUs and want run a more
    efficient cell.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上述代码的优化版本是`tf.contrib.rnn.LSTMBlockFusedCell`。当你无法使用GPU或TPU，并且想要运行更高效的单元时，应使用此方法。
- en: The best set of cells to use is under `tf.contrib.cudnn_rnn.*` (`CuddnnCompatibleGPUCell`
    for GPU cells and more). They are highly optimized to run on GPUs and perform
    significantly better than the preceding ones.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳的单元集可以在`tf.contrib.cudnn_rnn.*`下找到（例如，GPU单元的`CuddnnCompatibleGPUCell`等）。这些方法在GPU上进行了高度优化，比前面的那些方法性能更好。
- en: 'Finally, you should always perform the training using `tf.nn.dynamic_rnn` (see
    the TensorFlow documentation: [https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn))
    and pass the specific cell. This method optimizes the training of the recurrent
    neural networks by occasionally swapping memory between GPUs and CPUs to enable
    training of large sequences.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你应该始终使用 `tf.nn.dynamic_rnn` 进行训练（参见 TensorFlow 文档：[https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)）并传递特定的单元。此方法通过偶尔在
    GPU 和 CPU 之间交换内存，优化了递归神经网络的训练，从而使得训练大型序列成为可能。
- en: Optimizing gradients
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化梯度
- en: The last optimization technique will actually improve the performance of our
    backpropagation algorithm. Recall from the previous chapters that your goal during
    training is to minimize the loss function by adjusting the weights and biases
    of the model. Adjusting (optimizing) these weights and biases can be accomplished
    with different built-in TensorFlow optimizers, such as `tf.train.AdamOptimizer` and `tf.train.GradientDescentOptimizer`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种优化技巧实际上会提升我们反向传播算法的性能。回顾前几章，你在训练过程中要通过调整模型的权重和偏差来最小化损失函数。通过不同的内置 TensorFlow
    优化器（如 `tf.train.AdamOptimizer` 和 `tf.train.GradientDescentOptimizer`）可以实现这些权重和偏差的调整（优化）。
- en: 'TensorFlow offers the ability to distribute this process across multiple TPUs
    using this code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了通过此代码在多个 TPU 上分布处理的能力：
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, `existing_optimizer = tf.train.AdamOptimizer()`, and your training step
    will look like `train_step = optimizer.minimize(loss)`.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`existing_optimizer = tf.train.AdamOptimizer()`，你的训练步骤将如下所示：`train_step
    = optimizer.minimize(loss)`。
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered a lot of new and exciting approaches for optimizing
    your model's performance, both on a general level, and specifically, using the
    TensorFlow library.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了许多优化模型性能的新颖且令人兴奋的方法，既有通用的策略，也有特别使用 TensorFlow 库的优化方法。
- en: The first part covered techniques for improving your RNN performance by selecting,
    processing, and transforming your data, as well as tuning your hyperparameters.
    You also learned how to understand your model in more depth, and now know what
    should be done to make it work better.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分介绍了通过选择、处理和转换数据，以及调优超参数来提升 RNN 性能的技巧。你还学习了如何更深入地理解模型，现在你知道该怎么做才能让模型表现得更好。
- en: The second part was specifically focused on practical ways of improving your
    model's performance using the built-in TensorFlow functions. The team at TensorFlow
    seeks to make it as easy as possible for you to quickly achieve the results you
    want by providing distributed environments and optimization techniques with just
    a few lines of code.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分专门聚焦于使用内置的 TensorFlow 函数以提升模型性能的实际方法。TensorFlow 团队旨在通过提供分布式环境和优化技术，让你可以通过少量代码快速实现想要的结果。
- en: Combining both of the techniques covered in this chapter will enhance your knowledge
    in deep learning and let you experiment with more complicated models without worrying
    about performance issues. The knowledge you gained is applicable for any neural
    network model, so you can confidently apply exactly the same technique for broader
    sets of problems.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 将本章中介绍的两种技巧结合起来，将有助于你在深度学习领域的知识，并且让你能够在不担心性能问题的情况下，尝试更复杂的模型。你获得的知识适用于任何神经网络模型，因此你可以自信地将相同的技术应用于更广泛的问题集。
- en: External links
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部链接
- en: Open source data collection: [https://skymind.ai/wiki/open-datasets ](https://skymind.ai/wiki/open-datasets)or
    awesome-public-datasets GitHub repo: [https://github.com/awesomedata/awesome-public-datasets](https://github.com/awesomedata/awesome-public-datasets)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源数据集收集：[https://skymind.ai/wiki/open-datasets](https://skymind.ai/wiki/open-datasets)
    或 awesome-public-datasets GitHub 仓库：[https://github.com/awesomedata/awesome-public-datasets](https://github.com/awesomedata/awesome-public-datasets)
- en: Google Search Advanced: [https://www.google.com/advanced_search](https://www.google.com/advanced_search)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌高级搜索：[https://www.google.com/advanced_search](https://www.google.com/advanced_search)
- en: Augmenting text: [https://www.quora.com/What-data-augmentation-techniques-are-available-for-deep-learning-on-text](https://www.quora.com/What-data-augmentation-techniques-are-available-for-deep-learning-on-text)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本增强：[https://www.quora.com/What-data-augmentation-techniques-are-available-for-deep-learning-on-text](https://www.quora.com/What-data-augmentation-techniques-are-available-for-deep-learning-on-text)
- en: Data Noising as Smoothing in Neural Network Language Models: [https://arxiv.org/abs/1703.02573](https://arxiv.org/abs/1703.02573)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据噪声化作为神经网络语言模型中的平滑处理：[https://arxiv.org/abs/1703.02573](https://arxiv.org/abs/1703.02573)
- en: TensorBoard: [https://www.tensorflow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorBoard：[https://www.tensorflow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)
- en: An Introduction to Feature Selection: [https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择简介：[https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)
- en: Andrew Ng's course on hyperparameters tuning: [https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc](https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrew Ng的超参数调优课程：[https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc](https://www.coursera.org/lecture/deep-neural-network/hyperparameters-tuning-in-practice-pandas-vs-caviar-DHNcc)
- en: Hyperparameters optimization for Neural Networks: [http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe ](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe)
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的超参数优化：[http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html#tree-structured-parzen-estimators-tpe)
- en: Bayesian Optimization of Hyper Parameters: [https://www.youtube.com/watch?v=cWQDeB9WqvU](https://www.youtube.com/watch?v=cWQDeB9WqvU)
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数的贝叶斯优化：[https://www.youtube.com/watch?v=cWQDeB9WqvU](https://www.youtube.com/watch?v=cWQDeB9WqvU)
- en: Training performance (TensorFlow Summit 2018): [https://www.youtube.com/watch?v=SxOsJPaxHME](https://www.youtube.com/watch?v=SxOsJPaxHME)
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练性能（TensorFlow Summit 2018）：[https://www.youtube.com/watch?v=SxOsJPaxHME](https://www.youtube.com/watch?v=SxOsJPaxHME)
- en: TensorFlow Performance Guide: [https://www.tensorflow.org/performance/](https://www.tensorflow.org/performance/)
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow性能指南：[https://www.tensorflow.org/performance/](https://www.tensorflow.org/performance/)
- en: tf.data API: [https://www.tensorflow.org/performance/performance_guide](https://www.tensorflow.org/performance/performance_guide)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf.data API：[https://www.tensorflow.org/performance/performance_guide](https://www.tensorflow.org/performance/performance_guide)
- en: '`tf.data` (TensorFlow Summit 2018): [https://www.youtube.com/watch?v=uIcqeP7MFH0](https://www.youtube.com/watch?v=uIcqeP7MFH0)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.data`（TensorFlow Summit 2018）：[https://www.youtube.com/watch?v=uIcqeP7MFH0](https://www.youtube.com/watch?v=uIcqeP7MFH0)'
- en: Batch Normalization in Neural Networks: [https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络中的批量归一化：[https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c](https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c)
