- en: Chapter 3. Deep Belief Nets and Stacked Denoising Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 深度置信网络与堆叠去噪自编码器
- en: From this chapter through to the next chapter, you are going to learn the algorithms
    of deep learning. We'll follow the fundamental math theories step by step to fully
    understand each algorithm. Once you acquire the fundamental concepts and theories
    of deep learning, you can easily apply them to practical applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章到下一章，你将学习深度学习的算法。我们将一步步地跟随基础数学理论，全面理解每一个算法。一旦你掌握了深度学习的基本概念和理论，你就可以轻松地将它们应用到实际应用中。
- en: 'In this chapter, the topics you will learn about are:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你将学习到的主题包括：
- en: Reasons why deep learning could be a breakthrough
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习为何能成为突破性技术
- en: The differences between deep learning and past machine learning (neural networks)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习与过去的机器学习（神经网络）之间的区别
- en: Theories and implementations of the typical algorithms of deep learning, **deep
    belief nets** (**DBN**), and **Stacked Denoising Autoencoders** (**SDA**)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习的典型算法、**深度置信网络**（**DBN**）和**堆叠去噪自编码器**（**SDA**）的理论与实现
- en: Neural networks fall
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的失败
- en: In the previous chapter, you learned about the typical algorithm of neural networks
    and saw that nonlinear classification problems cannot be solved with perceptrons
    but can be solved by making multi-layer modeled neural networks. In other words,
    nonlinear problems can be learned and solved by inserting a hidden layer between
    the input and output layer. There is nothing else to it; but by increasing the
    number of neurons in a layer, the neural networks can express more patterns as
    a whole. If we ignore the time cost or an over-fitting problem, theoretically,
    neural networks can approximate any function.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，你了解了神经网络的典型算法，并且看到非线性分类问题无法通过感知器解决，但可以通过构建多层的神经网络模型来解决。换句话说，非线性问题可以通过在输入层和输出层之间插入一个隐藏层来学习和解决。仅此而已；但是通过增加一层中的神经元数量，神经网络作为一个整体能够表达更多的模式。如果我们忽略时间成本或过拟合问题，理论上，神经网络可以逼近任何函数。
- en: So, can we think this way? If we increase the number of hidden layers—accumulate
    hidden layers over and over—can neural networks solve any complicated problem?
    It's quite natural to come up with this idea. And, as a matter of course, this
    idea has already been examined. However, as it turns out, this trial didn't work
    well. Just accumulating layers didn't make neural networks solve the world's problems.
    On the contrary, some cases have less accuracy when predicting than others with
    fewer layers.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们可以这样想吗？如果我们增加隐藏层的数量——反复堆叠隐藏层——神经网络能否解决任何复杂问题？提出这个想法是很自然的。而且，顺理成章，这个想法已经被研究过了。然而，事实证明，这个尝试并没有取得好的效果。仅仅堆叠层数并没有让神经网络解决世界上的问题。相反，某些情况下，层数较少的网络在预测时准确度反而更高。
- en: Why do these cases happen? It's not wrong for neural networks with more layers
    to have more expression. So, where is the problem? Well, it is caused because
    of the feature that learning algorithms have in feed-forward networks. As we saw
    in the previous chapter, the backpropagation algorithm is used to propagate the
    learning error into the whole network efficiently with the multi-layer neural
    networks. In this algorithm, an error is reversed in each layer of the neural
    network and is conveyed to the input layer one by one in order. By backpropagating
    the error at the output layer to the input layer, the weight of the network is
    adjusted at each layer in order and the whole weight of a network is optimized.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会出现这些情况？神经网络层数更多的确可以有更多的表现能力。那么，问题出在哪里呢？嗯，这是由前馈网络中学习算法的特性引起的。正如我们在前一章看到的，反向传播算法用于在多层神经网络中高效地传播学习误差。在这个算法中，误差会在神经网络的每一层反向传播，并依次传递到输入层。通过将输出层的误差反向传播到输入层，网络的权重会在每一层按顺序进行调整，从而优化整个网络的权重。
- en: This is where the problem occurs. If the number of layers of a network is small,
    an error backpropagating from an output layer can contribute to adjusting the
    weights of each layer well. However, once the number of layers increases, an error
    gradually disappears every time it backpropagates layers, and doesn't adjust the
    weight of the network. At a layer near the input layer, an error is not fed back
    at all.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 问题出现在这里。如果网络的层数较少，从输出层反向传播的误差可以很好地帮助调整每一层的权重。然而，一旦层数增加，误差在每次反向传播时都会逐渐消失，无法有效调整网络的权重。在靠近输入层的层，误差根本不会被反馈。
- en: The neural networks where the link among layers is dense have an inability to
    adjust weights. Hence, the weight of the whole of the networks cannot be optimized
    and, as a matter of course, the learning cannot go well. This serious problem
    is known as the **vanishing gradient problem** and has troubled researchers as
    a huge problem that the neural network had for a long time until deep learning
    showed up. The neural network algorithm reached a limit at an early stage.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 连接层之间稠密的神经网络无法调整权重。因此，整个网络的权重无法优化，学习自然无法顺利进行。这个严重的问题被称为**消失梯度问题**，它一直困扰着研究人员，成为神经网络长期存在的巨大难题，直到深度学习的出现才得以突破。神经网络算法在早期阶段就达到了瓶颈。
- en: Neural networks' revenge
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的反击
- en: Because of the vanishing gradient problem, neural networks lost their popularity
    in the field of machine learning. We can say that the number of cases used for
    data mining in the real world by neural networks was remarkably small compared
    to other typical algorithms such as logistic regression and SVM.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于消失梯度问题，神经网络在机器学习领域失去了人气。我们可以说，神经网络在现实世界中的数据挖掘应用案例，相比于其他典型算法（如逻辑回归和支持向量机）显得格外少。
- en: But then deep learning showed up and broke all the existing conventions. As
    you know, deep learning is the neural network accumulating layers. In other words,
    it is deep neural networks, and it generates astounding predictability in certain
    fields. Now, speaking of AI research, it's no exaggeration to say that it's the
    research into deep neural networks. Surely it's the counterattack by neural networks.
    If so, why didn't the vanishing gradient problem matter in deep learning? What's
    the difference between this and the past algorithm?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，深度学习的出现打破了所有现有的常规。正如你所知，深度学习是由神经网络层层堆叠而成。换句话说，它就是深度神经网络，并且在某些领域产生了令人震惊的可预测性。现在，说到人工智能研究，毫不夸张地说，它就是对深度神经网络的研究。可以说，这正是神经网络的反击。那么，为什么在深度学习中消失梯度问题不再重要呢？这与过去的算法有什么不同？
- en: In this section, we'll look at why deep learning can generate such predictability
    and its mechanisms.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将探讨深度学习为何能够产生如此强大的可预测性以及它的机制。
- en: Deep learning's evolution – what was the breakthrough?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的演变——突破点是什么？
- en: We can say that there are two algorithms that triggered deep learning's popularity.
    The first one, as mentioned in [Chapter 1](ch01.html "Chapter 1. Deep Learning
    Overview"), *Deep Learning Overview*, is DBN pioneered by Professor Hinton ([https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf)).
    The second one is SDA, proposed by Vincent et al. ([http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf](http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf)).
    SDA was introduced a little after the introduction of DBN. It also recorded high
    predictability even with deep layers by taking a similar approach to DBN, although
    the details of the algorithm are different.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说，有两个算法引发了深度学习的流行。第一个，如[第1章](ch01.html "第1章：深度学习概述")《深度学习概述》中所提到的，是由Hinton教授开创的DBN([https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf](https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf))。第二个是由Vincent等人提出的SDA([http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf](http://www.iro.umontreal.ca/~vincentp/Publications/denoising_autoencoders_tr1316.pdf))。SDA是在DBN介绍后不久提出的。它采用类似DBN的方式，虽然算法的细节不同，但仍然记录了即使在深层情况下也具有高预测性的结果。
- en: 'So, what is the common approach that solved the vanishing gradient problem?
    Perhaps you are nervously preparing to solve difficult equations in order to understand
    DBN or SDA, but don''t worry. DBN is definitely an algorithm that is understandable.
    On the contrary, the mechanism itself is really simple. Deep learning was established
    by a very simple and elegant solution. The solution is: **layer-wise training**.
    That''s it. You might think it''s obvious if you see it, but this is the approach
    that made deep learning popular.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，解决消失梯度问题的常见方法是什么呢？也许你正在紧张地准备解决复杂的方程式，以理解DBN或SDA，但别担心。DBN绝对是一个可以理解的算法。相反，它的机制本身非常简单。深度学习是通过一个非常简单且优雅的解决方案建立的。这个解决方案就是：**逐层训练**。仅此而已。你可能会觉得看到它时觉得显而易见，但正是这个方法让深度学习变得流行。
- en: As mentioned earlier, in theory if there are more units or layers of neural
    networks, it should have more expressions and increase the number of problems
    it is able to solve. It doesn't work well because an error cannot be fed back
    to each layer correctly and parameters, as a whole network, cannot be adjusted
    properly. This is where the innovation was brought in for learning at a respective
    layer. Because each layer adjusts the weights of the networks independently, the
    whole network (that is, the parameters of the model) can be optimized properly
    even though the numbers of layers are piled up.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，理论上，如果神经网络的单元或层数更多，它应该具有更多的表达能力，并增加它能够解决的问题的数量。但它工作得不好，因为误差无法正确地反馈到每一层，并且整个网络的参数无法正确调整。这就是引入逐层学习创新的地方。由于每一层独立调整网络的权重，即使层数堆叠起来，整个网络（即模型的参数）也能得到正确的优化。
- en: Previous models didn't go well because they tried to backpropagate errors from
    an output layer to an input layer straight away and tried to optimize themselves
    by adjusting the weights of the network with backpropagated errors. So, the algorithm
    shifted to layer-wise training and then the model optimization went well. That's
    what the breakthrough was for deep learning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以前的模型效果不好，因为它们试图直接将输出层的误差反向传播到输入层，并试图通过调整网络的权重来优化自己。因此，算法转向了逐层训练，模型优化也就顺利进行了。这就是深度学习的突破。
- en: However, although we simply say **layer-wise training**, we need techniques
    for how to implement the learning. Also, as a matter of course, parameter adjustments
    for whole networks can't only be done with layer-wise training. We need the final
    adjustment. This phase of layer-wise training is called **pre-training** and the
    last adjustment phase is called **fine-tuning**. We can say that the bigger feature
    introduced in DBN and SDA is pre-training, but these two features are both part
    of the the necessary flow of deep learning. How do we do pre-training? What can
    be done in fine-tuning? Let's take a look at these questions one by one.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管我们简单地说是**逐层训练**，我们仍然需要技术来实现学习。而且，理所当然地说，整个网络的参数调整不能仅仅通过逐层训练来完成。我们还需要最终的调整。这个逐层训练阶段叫做**预训练**，而最后的调整阶段叫做**微调**。我们可以说，DBN和SDA中引入的一个更大特点就是预训练，但这两个特点都是深度学习必要流程的一部分。我们如何进行预训练？微调可以做什么？让我们逐一探讨这些问题。
- en: Deep learning with pre-training
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练的深度学习
- en: 'Deep learning is more like neural networks with accumulated hidden layers.
    The layer-wise training in pre-training undertakes learning at each layer. However,
    you might still have the following questions: if both layers are hidden (that
    is, neither of the layers are input nor output layers), then how is the training
    done? What can the input and output be?'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习更像是具有累积隐藏层的神经网络。预训练中的逐层训练在每一层进行学习。然而，你可能仍然有以下问题：如果两个层都是隐藏层（即既不是输入层也不是输出层），那么训练是如何进行的？输入和输出是什么？
- en: 'Before thinking of these questions, remind yourself of the following point
    again (reiterated persistently): deep learning is neural networks with piled up
    layers. This mean, model parameters are still the weights of the network (and
    bias) in deep learning. Since these weights (and bias) need to be adjusted among
    each layer, in the standard three layered neural network (that is, the input layer,
    the hidden layer, and the output layer), we need to optimize only the weights
    between the input layer and the hidden layer and between the hidden layer and
    the output layer. In deep learning, however, the weight between two hidden layers
    also needs to be adjusted.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在思考这些问题之前，请再次提醒自己以下几点（反复强调）：深度学习是堆叠层的神经网络。这意味着，模型参数仍然是网络中的权重（和偏置）。由于这些权重（和偏置）需要在每一层之间进行调整，在标准的三层神经网络中（即输入层、隐藏层和输出层），我们只需要优化输入层和隐藏层之间以及隐藏层和输出层之间的权重。然而，在深度学习中，两个隐藏层之间的权重也需要进行调整。
- en: First of all, let's think about the input of a layer. You can imagine this easily
    with a quick thought. The value propagated from the previous layer will become
    the input as it is. The value propagated from the previous layer is none other
    than the value forward propagated from the previous layers to the current layer
    by using the weight of the network, the same as in general feed-forward networks.
    It looks simple in writing, but you can see that it has an important meaning if
    you step into it further and try to understand what it means. The value from the
    previous layer becomes the input, which means that the features the previous layer(s)
    learned become the input of the current layer, and from there the current layer
    newly learns the feature of the given data. In other words, in deep learning,
    features are learned from the input data in stages (and semi-automatically). This
    implies a mechanism where the deeper a layer becomes, the higher the feature it
    learns. This is what normal multi-layer neural networks couldn't do and the reason
    why it is said "a machine can learn a concept."
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们来思考一下一个层的输入。你可以通过快速思考轻松地理解这一点。从前一层传播过来的值会作为输入直接进入当前层。前一层传播过来的值，实际上就是通过网络的权重将前一层的值向前传播到当前层，就像一般的前馈网络一样。写起来看似简单，但如果你进一步探究并尝试理解它的含义，你会发现它具有重要的意义。前一层的值作为输入，意味着前一层（或前几层）学习到的特征成为当前层的输入，从而当前层就能从给定的数据中学习到新的特征。换句话说，在深度学习中，特征是分阶段（且半自动）从输入数据中学习的。这意味着，层次越深，学习到的特征就越复杂。这是普通的多层神经网络做不到的，也是为什么人们说“机器可以学习概念”的原因。
- en: 'Now, let''s think about the output. Please bear in mind that thinking about
    the output means thinking about how it learns. DBN and SDA have completely different
    approaches to learning, but both fill the following condition: to learn in order
    to equate output values and input values. You might think "What are you talking
    about?" but this is the technique that makes deep learning possible.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来思考一下输出。请记住，思考输出就意味着思考它是如何学习的。DBN 和 SDA 在学习方法上完全不同，但两者都满足以下条件：为了使输出值与输入值相等而进行学习。你可能会想“你在说什么？”但这就是使深度学习成为可能的技术。
- en: 'The value comes and goes back to the input layer through the hidden layer,
    and the technique is to adjust the weight of the networks (that is, to equate
    the output value and the input value) to eliminate the error at that time. The
    graphical model can be illustrated as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 值从隐藏层进入，再回到输入层，技术是调整网络的权重（也就是使输出值与输入值相等）以消除当时的误差。该图示可以如下所示：
- en: '![Deep learning with pre-training](img/B04779_03_01.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![带预训练的深度学习](img/B04779_03_01.jpg)'
- en: 'It looks different from standard neural networks at a glance, but there''s
    nothing special. If we intentionally draw the diagram of the input layer and the
    output layer separately, the mechanism is the same shape as the normal neural
    network:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从表面上看，它与标准的神经网络有所不同，但其实没有什么特别的。如果我们故意将输入层和输出层分别画出，机制与普通神经网络是相同的：
- en: '![Deep learning with pre-training](img/B04779_03_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![带预训练的深度学习](img/B04779_03_02.jpg)'
- en: For a human, this action of *matching input and output* is not intuitive, but
    for a machine it is a valid action. If so, how it can learn features from input
    data by matching the output layer and input layer?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类来说，这种*匹配输入和输出*的行为并不直观，但对于机器来说，这是一个有效的动作。那么，它是如何通过匹配输出层和输入层来从输入数据中学习特征的呢？
- en: 'Need a little explanation? Let''s think about it this way: in the algorithm
    of machine learning, including neural networks, learning intends to minimize errors
    between the model''s prediction output and the dataset output. The mechanism is
    to remove an error by finding a pattern from the input data and making data with
    a common pattern the same output value (for example, 0 or 1). What would then
    happen if we turned the output value into the input value?'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一点解释吗？我们可以这样理解：在机器学习算法中，包括神经网络，学习的目的是最小化模型预测输出与数据集输出之间的误差。其机制是通过从输入数据中寻找模式来去除误差，并使具有相同模式的数据输出相同的值（例如，0
    或 1）。那么，如果我们将输出值转化为输入值，结果会怎样呢？
- en: When we look at problems that should be solved as a whole through deep learning,
    input data is, fundamentally, a dataset that can be divided into some patterns.
    This means that there are some common features in the input data. If so, in the
    process of learning where each output value becomes respective input data, the
    weight of networks should be adjusted to focus more on the part that reflects
    the common features. And, even within the data categorized in the same class,
    learning should be processed to reduce weight on the non-common feature part,
    that is, the noise part.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们观察需要通过深度学习整体解决的问题时，输入数据从根本上来说是一个可以划分为某些模式的数据集。这意味着输入数据中存在一些共同特征。如果是这样，在学习过程中，每个输出值成为相应的输入数据时，网络的权重应该被调整，以更多地关注反映这些共同特征的部分。而且，即使是在同一类别中分类的数据，学习也应该处理减少对非共同特征部分的权重，也就是噪声部分。
- en: 'Now you should understand what the input and output is in a certain layer and
    how learning progresses. Once the pre-training is done at a certain layer, the
    network moves on to learning in the next layer. However, as you can see in the
    following images, please also keep in mind that a hidden layer becomes an input
    layer when the network moves to learning in the next layer:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该理解了在某一层中输入和输出是什么，以及学习是如何进行的。一旦某一层的预训练完成，网络将转到下一层的学习。然而，正如你在以下图像中看到的，请记住，当网络转向下一层的学习时，隐藏层会变成输入层：
- en: '![Deep learning with pre-training](img/B04779_03_03.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![带有预训练的深度学习](img/B04779_03_03.jpg)'
- en: The point here is that the layer after the pre-training can be treated as normal
    feed-forward neural networks where the weight of the networks is adjusted. Hence,
    if we think about the input value, we can simply calculate the value forward propagated
    from the input layer to the current layer through the network.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键是，预训练后的层可以视为普通的前馈神经网络，其中网络的权重会被调整。因此，如果我们考虑输入值，我们可以简单地计算从输入层到当前层通过网络传播的值。
- en: 'Up to now, we''ve looked through the flow of pre-training (that is, layer-wise
    training). In the hidden layers of deep neural networks, features of input data
    are extracted in stages through learning where the input matches the output. Now,
    some of you might be wondering: I understand that features can be learned in stages
    from input data by pre-training, but that alone doesn''t solve the classification
    problem. So, how can it solve the classification problem?'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了预训练的流程（即逐层训练）。在深度神经网络的隐藏层中，通过学习输入与输出匹配的方式，输入数据的特征逐步被提取出来。现在，可能有些人会问：我明白通过预训练输入数据的特征可以逐步学习，但仅此并不能解决分类问题。那么，如何解决分类问题呢？
- en: 'Well, during pre-training, the information pertaining to which data belongs
    to which class is not provided. This means the pre-training is unsupervised training
    and it just analyzes the hidden pattern using only input data. This is meaningless
    if it can''t be used to solve the problem however it extracts features. Therefore,
    the model needs to complete one more step to solve classification problems properly.
    That is fine-tuning. The main roles of fine-tuning are the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练过程中，未提供关于哪些数据属于哪个类别的信息。这意味着预训练是无监督训练，它仅通过输入数据分析隐藏模式。如果无法用于解决问题，尽管它提取了特征，这就毫无意义。因此，模型需要完成一步以正确解决分类问题。那就是微调。微调的主要作用如下：
- en: To add an output layer to deep neural networks that completed pre-training and
    to perform supervised training.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为完成预训练的深度神经网络添加一个输出层，并进行监督学习。
- en: To do final adjustments for the whole deep neural network.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对整个深度神经网络进行最终调整。
- en: 'This can be illustrated as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下方式来说明：
- en: '![Deep learning with pre-training](img/B04779_03_04.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![带有预训练的深度学习](img/B04779_03_04.jpg)'
- en: The supervised training in an output layer uses a machine learning algorithm,
    such as logistic regression or SVM. Generally, logistic regression is used more
    often considering the balance of the amount of calculation and the precision gained.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层中的监督训练使用了机器学习算法，如逻辑回归或支持向量机（SVM）。通常，考虑到计算量和精度的平衡，逻辑回归使用得更为频繁。
- en: In fine-tuning, sometimes only the weights of an output layer will be adjusted,
    but normally the weights of whole neural networks, including the layer where the
    weights have been adjusted in pre-training, will also be adjusted. This means
    the standard learning algorithm, or in other words the backpropagation algorithm,
    is applied to the deep neural networks just as one multi-layer neural network.
    Thus, the model of neural networks with the problem of solving more complicated
    classification is completed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调过程中，有时只有输出层的权重会被调整，但通常包括预训练中已经调整过权重的层在内，整个神经网络的权重都会被调整。这意味着标准学习算法，换句话说，反向传播算法，像处理单一多层神经网络一样应用于深度神经网络。因此，解决更复杂分类问题的神经网络模型得以完成。
- en: 'Even so, you might have the following questions: why does learning go well
    with the standard backpropagation algorithm even in multi-layer neural networks
    where layers are piled up? Doesn''t the vanishing gradient problem occur? These
    questions can be solved by pre-training. Let''s think about the following: in
    the first place, the problem is that the weights of each network are not correctly
    adjusted due to improperly fed back errors in multi-layer neural networks without
    pre-training; in other words, the multi-layer neural networks where the vanishing
    gradient problem occurs. On the other hand, once the pre-training is done, the
    learning starts from the point where the weight of the network is almost already
    adjusted. Therefore, a proper error can be propagated to a layer close to an input
    layer. Hence the name fine-tuning. Thus, through pre-training and fine-tuning,
    eventually deep neural networks become neural networks with increased expression
    by having deep layers.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 即便如此，你可能会有以下问题：为什么即使在层次堆叠的多层神经网络中，学习仍然与标准反向传播算法配合良好？难道没有梯度消失问题吗？这些问题可以通过预训练来解决。让我们思考一下：问题的根本在于，在没有预训练的多层神经网络中，由于反馈错误不正确，导致每个网络的权重没有得到正确调整；换句话说，就是发生了梯度消失问题的多层神经网络。另一方面，一旦完成预训练，学习就从网络权重几乎已经调整好的地方开始。因此，适当的误差可以传播到接近输入层的层。因此，称之为微调。通过预训练和微调，最终深度神经网络通过拥有深层次的结构，成为具有更强表达能力的神经网络。
- en: 'From the next section onwards, we will finally look through the theory and
    implementation of DBN and SDA, the algorithms of deep learning. But before that,
    let''s look back at the flow of deep learning once again. Below is the summarized
    diagram of the flow:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从下一节开始，我们将最终回顾DBN和SDA这两种深度学习算法的理论与实现。但在此之前，让我们再次回顾一下深度学习的流程。下面是该流程的简化图：
- en: '![Deep learning with pre-training](img/B04779_03_05.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![带有预训练的深度学习](img/B04779_03_05.jpg)'
- en: The parameters of the model are optimized layer by layer during pre-training
    and then adjusted as single deep neural networks during fine-tuning. Deep learning,
    the breakthrough of AI, is a very simple algorithm.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的参数在预训练过程中会逐层优化，然后在微调过程中作为单一深度神经网络进行调整。深度学习，作为人工智能的突破，是一个非常简单的算法。
- en: Deep learning algorithms
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习算法
- en: Now, let's look through the theory and implementation of deep learning algorithms.
    In this chapter, we will see DBN and SDA (and the related methods). These algorithms
    were both researched explosively, mainly between 2012 and 2013 when deep learning
    started to spread out rapidly and set the trend of deep learning on fire. Even
    though there are two methods, the basic flow is the same and consistent with pre-training
    and fine-tuning, as explained in the previous section. The difference between
    these two is which pre-training (that is, unsupervised training) algorithm is
    applied to them.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下深度学习算法的理论与实现。在这一章中，我们将讨论DBN和SDA（以及相关方法）。这些算法在2012到2013年间得到了爆炸式的研究，正是那时深度学习开始迅速传播，并点燃了深度学习的热潮。尽管有两种方法，但基本流程是相同的，并与前面一节中提到的预训练和微调一致。它们之间的区别在于应用了哪种预训练（即无监督训练）算法。
- en: Therefore, if there could be difficult points in deep learning, it should be
    the theory and equation of the unsupervised training. However, you don't have
    to be afraid. All the theories and implementations will be explained one by one,
    so please read through the following sections carefully.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果深度学习中有困难的地方，应该是无监督训练的理论和方程式。不过，你不必担心。所有的理论和实现都会逐一讲解，请仔细阅读接下来的章节。
- en: Restricted Boltzmann machines
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: The method used in the layer-wise training of DBN, pre-training, is called **Restricted
    Boltzmann Machines** (**RBM**). To begin with, let's take a look at the RBM that
    forms the basis of DBN. As RBM stands for Restricted Boltzmann Machines, of course
    there's a method called **Boltzmann Machines** (**BMs**). Or rather, BMs are a
    more standard form and RBM is the special case of them. Both are one of the neural
    networks and both were proposed by Professor Hinton.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: DBN 中逐层训练的方法，预训练，被称为**受限玻尔兹曼机**（**RBM**）。首先，让我们看一下构成 DBN 基础的 RBM。由于 RBM 代表受限玻尔兹曼机，当然也有一种方法叫做**玻尔兹曼机**（**BMs**）。或者说，BMs
    是一种更标准的形式，而 RBM 是它的特殊情况。两者都是神经网络的一种，且都是由 Hinton 教授提出的。
- en: 'The implementation of RBM and DBNs can be done without understanding the detailed
    theory of BMs, but in order to understand these concepts, we''ll briefly look
    at the idea BMs are based on. First of all, let''s look at the following figure,
    which shows a graphical model of BMs:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: RBM 和 DBN 的实现可以在不理解 BMs 详细理论的情况下完成，但为了理解这些概念，我们将简要看一下 BMs 所基于的思想。首先，让我们看看以下图形，它展示了
    BMs 的图示模型：
- en: '![Restricted Boltzmann machines](img/B04779_03_06.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机](img/B04779_03_06.jpg)'
- en: 'BMs look intricate because they are fully connected, but they are actually
    just simple neural networks with two layers. By rearranging all the units in the
    networks to get a better understanding, BMs can be shown as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: BMs 看起来复杂是因为它们是全连接的，但实际上它们只是具有两层的简单神经网络。通过重新排列网络中的所有单元来更好地理解，BMs 可以表示如下：
- en: '![Restricted Boltzmann machines](img/B04779_03_07.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![受限玻尔兹曼机](img/B04779_03_07.jpg)'
- en: Please bear in mind that normally the input/output layer is called the **visible
    layer** in BMs and RBMs (a hidden layer is commonly used as it is), for it is
    the networks that presume the hidden condition (unobservable condition) from the
    observable condition. Also, the neurons of the visible layer are called **visible
    units** and the neurons of the hidden layer are called **hidden units**. Signs
    in the previous figure are described to match the given names.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，通常在 BMs 和 RBMs 中，输入/输出层被称为**可见层**（通常使用一个隐藏层，因为它是网络从可观察条件推测隐性条件的层）。另外，可见层的神经元被称为**可见单元**，隐藏层的神经元被称为**隐藏单元**。前面图中的符号描述与这些名称相匹配。
- en: As you can see in the diagram, the structure of BMs is not that different from
    standard neural networks. However, its way of thinking has a big feature. The
    feature is to adopt the concept of *energy* in neural networks. Each unit has
    a stochastic state respectively and the whole of the networks' energy is determined
    depending on what state each unit takes. (The first model that adopted the concept
    of energy in networks is called the **Hopfield network**, and BMs are the developed
    version of it. Since details of the Hopfield network are not totally relevant
    to deep learning, it is not explained in this book.) The condition that memorizes
    the correct data is the steady state of networks and the least amount of energy
    these networks have. On the other hand, if data with noise is provided to the
    network, each unit has a different state, but not a steady state, hence its condition
    makes the transition to stabilize the whole network, in other words, to transform
    it into a steady state.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，BMs 的结构与标准神经网络并没有太大不同。然而，它的思维方式有一个显著特点。这个特点是采用了神经网络中的*能量*概念。每个单元都有一个随机状态，整个网络的能量取决于每个单元的状态。（第一个采用能量概念的网络模型被称为**霍普菲尔德网络**，而
    BMs 是它的扩展版本。由于霍普菲尔德网络的细节与深度学习不完全相关，因此本书不做详细解释。）记住正确数据的条件是网络的稳态和这些网络拥有的最小能量。另一方面，如果将带噪声的数据提供给网络，每个单元都有不同的状态，但不是稳态，因此其条件会使网络过渡以稳定整个网络，换句话说，就是将其转化为稳态。
- en: This means that the weights of the model are adjusted and the state of each
    unit is transferred to minimize the energy function the networks have. These operations
    can remove the noise and extract the feature from inputs as a whole. Although
    the energy of networks sounds enormous, it's not too difficult to imagine because
    minimizing the energy function has the same effect as minimizing the error function.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着模型的权重被调整，每个单元的状态会被传递，以最小化网络的能量函数。这些操作可以去除噪声并从输入中整体提取特征。尽管网络的能量听起来很庞大，但并不难理解，因为最小化能量函数与最小化误差函数有相同的效果。
- en: 'The concept of BMs was wonderful, but various problems occurred when BMs were
    actually applied to practical problems. The biggest problem was that BMs are fully
    connected networks and take an enormous amount of calculation time. Therefore,
    RBM was devised. RBM is the algorithm that can solve various problems in a realistic
    time frame by making BMs restricted. Just as in BM, RBM is a model based on the
    energy of a network. Let''s look at RBM in the diagram below:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: BM的概念非常出色，但在实际应用中却出现了各种问题。最大的问题是，BM是完全连接的网络，需要消耗大量的计算时间。因此，提出了RBM（限制玻尔兹曼机）。RBM通过对BM进行限制，使其能够在现实的时间框架内解决各种问题。与BM一样，RBM是基于网络能量的模型。我们来看一下下面的RBM示意图：
- en: '![Restricted Boltzmann machines](img/B04779_03_08.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_08.jpg)'
- en: Here, ![Restricted Boltzmann machines](img/B04779_03_14.jpg) is the number of
    units in the visible layer and ![Restricted Boltzmann machines](img/B04779_03_15.jpg)
    the number of units in the hidden layer. ![Restricted Boltzmann machines](img/B04779_03_16.jpg)
    denotes the value of a visible unit, ![Restricted Boltzmann machines](img/B04779_03_17.jpg)
    the value of a hidden unit, and ![Restricted Boltzmann machines](img/B04779_03_18.jpg)
    the weight between these two units. As you can see, the difference between BM
    and RBM is that RBM doesn't have connections between the same layer. Because of
    this restriction, the amount of calculation decreases and it can be applied to
    realistic problems.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![限制玻尔兹曼机](img/B04779_03_14.jpg)是可见层单元的数量，![限制玻尔兹曼机](img/B04779_03_15.jpg)是隐藏层单元的数量。![限制玻尔兹曼机](img/B04779_03_16.jpg)表示可见单元的值，![限制玻尔兹曼机](img/B04779_03_17.jpg)表示隐藏单元的值，![限制玻尔兹曼机](img/B04779_03_18.jpg)是这两个单元之间的权重。如你所见，BM和RBM的区别在于，RBM没有同一层之间的连接。由于这一限制，计算量减少，能够应用于实际问题。
- en: Now, let's look through the theory.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看理论。
- en: Tip
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Be careful that, as a prerequisite, the value that each visible unit and hidden
    unit in RBM can take is generally {0, 1}, that is, binary (this is the same as
    BMs).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，作为前提，RBM中每个可见单元和隐藏单元所能取的值通常是{0, 1}，即二进制（这与BM相同）。
- en: If we expand the theory, it can also handle continuous values. However, this
    could make equations complex, where it's not the core of the theory and where
    it's implemented with binary in the original DBN proposed by Professor Hinton.
    Therefore, we'll also implement binary RBM in this book. RBM with binary inputs
    is sometimes called **Bernoulli RBM**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们扩展这个理论，它也可以处理连续值。然而，这可能会使方程变得复杂，而这并不是理论的核心，而且在教授Hinton提出的原始DBN中，使用的是二进制。因此，本书中我们也会实现二进制RBM。带有二进制输入的RBM有时被称为**伯努利RBM**。
- en: 'RBM is the energy-based model, and the status of a visible layer or hidden
    layer is treated as a stochastic variable. We''ll look at the equations in order.
    First of all, each visible unit is propagated to the hidden units throughout a
    network. At this time, each hidden unit takes a binary value based on the probability
    distribution generated in accordance with its propagated inputs:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: RBM是基于能量的模型，可见层或隐藏层的状态被视为随机变量。我们将按顺序查看这些方程式。首先，每个可见单元通过网络传播到隐藏单元。在此过程中，每个隐藏单元根据其传播输入生成的概率分布，取一个二进制值：
- en: '![Restricted Boltzmann machines](img/B04779_03_21.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_21.jpg)'
- en: Here, ![Restricted Boltzmann machines](img/B04779_03_19.jpg) is the bias in
    a hidden layer and ![Restricted Boltzmann machines](img/B04779_03_20.jpg) denotes
    the sigmoid function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![限制玻尔兹曼机](img/B04779_03_19.jpg)是隐藏层的偏置，![限制玻尔兹曼机](img/B04779_03_20.jpg)表示Sigmoid函数。
- en: This time, it was conversely propagated from a hidden layer to a visible layer
    through the same network. As in the previous case, each visible unit takes a binary
    value based on probability distribution generated in accordance with propagated
    values.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，传播是从隐藏层反向传播到可见层，通过相同的网络进行。与之前的情况一样，每个可见单元根据传播值生成的概率分布，取一个二进制值。
- en: '![Restricted Boltzmann machines](img/B04779_03_22.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_22.jpg)'
- en: Here, ![Restricted Boltzmann machines](img/B04779_03_23.jpg) is the bias of
    the visible layer. This value of visible units is expected to match the original
    input values. This means if ![Restricted Boltzmann machines](img/B04779_03_24.jpg),
    the weight of the network as a model parameter, and ![Restricted Boltzmann machines](img/B04779_03_25.jpg),
    ![Restricted Boltzmann machines](img/B04779_03_26.jpg), the bias of a visible
    layer and a hidden layer, are shown as a vector parameter, ![Restricted Boltzmann
    machines](img/B04779_03_27.jpg), it leans ![Restricted Boltzmann machines](img/B04779_03_27.jpg)
    in order for the probability ![Restricted Boltzmann machines](img/B04779_03_28.jpg)
    that can be obtained above to get close to the distribution of ![Restricted Boltzmann
    machines](img/B04779_03_29.jpg).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![限制玻尔兹曼机](img/B04779_03_23.jpg)是可见层的偏置。期望这些可见单元的值与原始输入值匹配。这意味着，如果![限制玻尔兹曼机](img/B04779_03_24.jpg)，网络的权重作为模型参数，且![限制玻尔兹曼机](img/B04779_03_25.jpg)、![限制玻尔兹曼机](img/B04779_03_26.jpg)，即可见层和隐藏层的偏置，作为向量参数表示，![限制玻尔兹曼机](img/B04779_03_27.jpg)，它倾向于![限制玻尔兹曼机](img/B04779_03_27.jpg)，以便使得能获得的概率![限制玻尔兹曼机](img/B04779_03_28.jpg)接近![限制玻尔兹曼机](img/B04779_03_29.jpg)的分布。
- en: 'For this learning, the energy function, that is, the evaluation function, needs
    to be defined. The energy function is shown as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这项学习，需要定义能量函数，即评价函数。能量函数表示如下：
- en: '![Restricted Boltzmann machines](img/B04779_03_30.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_30.jpg)'
- en: 'Also, the joint probability density function showing the demeanor of a network
    can be shown as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，显示网络行为的联合概率密度函数可以表示如下：
- en: '![Restricted Boltzmann machines](img/B04779_03_31.jpg)![Restricted Boltzmann
    machines](img/B04779_03_32.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_31.jpg)![限制玻尔兹曼机](img/B04779_03_32.jpg)'
- en: 'From the preceding formulas, the equations for the training of parameters will
    be determined. We can get the following equation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的公式出发，训练参数的方程式将被确定。我们可以得到如下方程：
- en: '![Restricted Boltzmann machines](img/B04779_03_33.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_33.jpg)'
- en: 'Hence, the **log likelihood** can be shown as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**对数似然**可以表示如下：
- en: '![Restricted Boltzmann machines](img/B04779_03_34.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_34.jpg)'
- en: 'Then, we''ll calculate each gradient against the model parameter. The derivative
    can be calculated as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将计算每个梯度相对于模型参数的导数。导数可以按如下方式计算：
- en: '![Restricted Boltzmann machines](img/B04779_03_35.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_35.jpg)'
- en: Some equations in the middle are complicated, but it turns out to be simple
    with the term of the probability distribution of the model and the original data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 中间的一些方程比较复杂，但通过模型的概率分布项和原始数据，实际上变得很简单。
- en: 'Therefore, the gradient of each parameter is shown as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个参数的梯度可以表示如下：
- en: '![Restricted Boltzmann machines](img/B04779_03_36.jpg)![Restricted Boltzmann
    machines](img/B04779_03_37.jpg)![Restricted Boltzmann machines](img/B04779_03_38.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_36.jpg)![限制玻尔兹曼机](img/B04779_03_37.jpg)![限制玻尔兹曼机](img/B04779_03_38.jpg)'
- en: Now then, we could find the equation of the gradient, but a problem occurs when
    we try to apply this equation as it is. Think about the term of ![Restricted Boltzmann
    machines](img/B04779_03_39.jpg). This term implies that we have to calculate the
    probability distribution for all the {0, 1} patterns, which can be assumed as
    input data that includes patterns that don't actually exist in the data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以找到梯度的方程，但当我们尝试直接应用这个方程时会遇到问题。考虑一下![限制玻尔兹曼机](img/B04779_03_39.jpg)这一项。这个项意味着我们需要计算所有{0,
    1}模式的概率分布，而这些模式可以视为包含数据中实际上不存在的模式的输入数据。
- en: We can easily imagine how this term can cause a combinatorial explosion, meaning
    we can't solve it within a realistic time frame. To solve this problem, the method
    for approximating data using Gibbs sampling, called **Contrastive Divergence**
    (**CD**), was introduced. Let's look at this method now.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很容易想象这一项如何引起组合爆炸，这意味着我们无法在现实的时间框架内解决它。为了解决这个问题，引入了使用吉布斯采样近似数据的方法，称为**对比散度**（**CD**）。现在让我们来看一下这个方法。
- en: Here, ![Restricted Boltzmann machines](img/B04779_03_40.jpg) is an input vector.
    Also, ![Restricted Boltzmann machines](img/B04779_03_41.jpg) is an input (output)
    vector that can be obtained by sampling for k-times using this input vector.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![限制玻尔兹曼机](img/B04779_03_40.jpg)是输入向量。同时，![限制玻尔兹曼机](img/B04779_03_41.jpg)是通过对这个输入向量进行k次采样得到的输入（输出）向量。
- en: 'Then, we get:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到：
- en: '![Restricted Boltzmann machines](img/B04779_03_42.jpg)![Restricted Boltzmann
    machines](img/B04779_03_43.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_42.jpg)![限制玻尔兹曼机](img/B04779_03_43.jpg)'
- en: 'Hence, when approximating ![Restricted Boltzmann machines](img/B04779_03_44.jpg)
    after reiterating Gibbs sampling, the derivative of the log likelihood function
    can be represented as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在重新迭代吉布斯采样后，近似![限制玻尔兹曼机](img/B04779_03_44.jpg)时，似然函数的导数可以表示如下：
- en: '![Restricted Boltzmann machines](img/B04779_03_45.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_45.jpg)'
- en: 'Therefore, the model parameter can be shown as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型参数可以如下表示：
- en: '![Restricted Boltzmann machines](img/B04779_03_46.jpg)![Restricted Boltzmann
    machines](img/B04779_03_47.jpg)![Restricted Boltzmann machines](img/B04779_03_48.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_46.jpg)![限制玻尔兹曼机](img/B04779_03_47.jpg)![限制玻尔兹曼机](img/B04779_03_48.jpg)'
- en: Here, ![Restricted Boltzmann machines](img/B04779_03_49.jpg) is the number of
    iterations and ![Restricted Boltzmann machines](img/B04779_03_50.jpg) is the learning
    rate. As shown in the preceding formulas, generally, CD that performs sampling
    k-times is shown as CD-k. It's known that CD-1 is sufficient when applying the
    algorithm to realistic problems.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![限制玻尔兹曼机](img/B04779_03_49.jpg)是迭代次数，![限制玻尔兹曼机](img/B04779_03_50.jpg)是学习率。如前面的公式所示，通常，执行k次采样的CD被表示为CD-k。已知在将该算法应用于实际问题时，CD-1就足够了。
- en: 'Now, let''s go through the implementation of RMB. The package structure is
    as shown in the following screenshot:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们了解RMB的实现。包结构如下图所示：
- en: '![Restricted Boltzmann machines](img/B04779_03_09.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![限制玻尔兹曼机](img/B04779_03_09.jpg)'
- en: Let's look through the `RestrictedBoltzmannMachines.java` file. Because the
    first part of the main method just defines the variables needed for a model and
    generates demo data, we won't look at it here.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下`RestrictedBoltzmannMachines.java`文件。由于主方法的第一部分只是定义模型所需的变量并生成演示数据，我们在这里不讨论它。
- en: 'So, in the part where we generate an instance of a model, you may notice there
    are many `null` values in arguments:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在我们生成模型实例的部分，你可能会注意到在参数中有许多`null`值：
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When you look at the constructor, you might know that these `null` values are
    the RBM''s weight matrix, bias of hidden units, and bias of visible units. We
    define arguments as `null` here because they are for DBN''s implementation. In
    the constructor, these are initialized as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看构造函数时，你可能会知道这些`null`值是RBM的权重矩阵、隐单元的偏置和可见单元的偏置。我们在这里将参数定义为`null`，因为它们是为DBN的实现而设定的。在构造函数中，它们被初始化如下：
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The next step is training. CD-1 is applied for each mini-batch:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是训练。每个小批量都应用CD-1：
- en: '[PRE2]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let''s look into the essential point of RBM, the `contrastiveDivergence`
    method. CD-1 can obtain a sufficient solution when we actually run this program
    (and so we have k = 1 in the demo), but this method is defined to deal with CD-k
    as well:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨RBM的关键点，即`contrastiveDivergence`方法。当我们实际运行这个程序时，CD-1可以得到足够的解（因此在演示中我们使用了k
    = 1），但是这个方法也被定义用来处理CD-k：
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It appears that two different types of method, `sampleHgivenV` and `gibbsHVH`,
    are used in CD-k, but when you look into `gibbsHVH`, you can see:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来在CD-k中使用了两种不同类型的方法，`sampleHgivenV`和`gibbsHVH`，但是当你查看`gibbsHVH`时，你会发现：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: So, CD-k consists of only two methods for sampling, `sampleVgivenH` and `sampleHgivenV`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，CD-k仅由两个采样方法组成，`sampleVgivenH`和`sampleHgivenV`。
- en: 'As the name of the method indicates, `sampleHgivenV` is the method that sets
    the probability distribution and sampling data generated in a hidden layer based
    on the given value of visible units and vice versa:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正如方法名称所示，`sampleHgivenV`是一个根据给定的可见单元值生成隐层的概率分布和采样数据的函数，反之亦然：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `propup` and `propdown` tags that set values to respective means are the
    method that activates values of each unit by the `sigmoid` function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 设置各自均值的`propup`和`propdown`标签是通过`sigmoid`函数激活每个单元值的方法：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `binomial` method that sets a value to a sample is defined in `RandomGenerator.java`.
    The method returns `0` or `1` based on the binomial distribution. With this method,
    a value of each unit becomes binary:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 设置样本值的`binomial`方法在`RandomGenerator.java`中定义。该方法根据二项分布返回`0`或`1`。使用此方法，每个单元的值变成二进制：
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once approximated values are obtained by sampling, what we need to do is just
    calculate the gradient of a `model` parameter and renew a parameter using a mini-batch.
    There''s nothing special here:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦通过采样获得了近似值，我们需要做的就是计算`model`参数的梯度，并通过小批量更新参数。这里没有什么特别的：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we''re done with the `model` training. Next comes the test and evaluation
    in general cases, but note that the model cannot be evaluated with barometers
    such as accuracy because RBM is a generative model. Instead, let''s briefly look
    at how noisy data is changed by RBM here. Since RBM after training can be seen
    as a neural network, the weights of which are adjusted, the model can obtain reconstructed
    data by simply propagating input data (that is, noisy data) through a network:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们完成了`model`的训练。接下来是一般情况下的测试和评估，但请注意，由于 RBM 是一个生成模型，无法通过准确度等标准来评估模型。而是让我们简单看一下
    RBM 如何改变噪声数据。由于训练后的 RBM 可以看作是一个神经网络，其权重会进行调整，因此模型可以通过简单地将输入数据（即噪声数据）传递通过网络来获得重建数据：
- en: '[PRE9]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Deep Belief Nets (DBNs)
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度置信网络（DBNs）
- en: 'DBNs are deep neural networks where logistic regression is added to RBMS as
    the output layer. Since the theory necessary for implementation has already been
    explained, we can go directly to the implementation. The package structure is
    as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: DBNs 是一种深度神经网络，其中将逻辑回归添加到 RBM 作为输出层。由于实现所需的理论已经解释过了，我们可以直接进入实现部分。包的结构如下：
- en: '![Deep Belief Nets (DBNs)](img/B04779_03_10.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![深度置信网络（DBNs）](img/B04779_03_10.jpg)'
- en: 'The flow of the program is very simple. The order is as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的流程非常简单，顺序如下：
- en: Setting up parameters for the model.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置模型的参数。
- en: Building the model.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型。
- en: Pre-training the model.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对模型进行预训练。
- en: Fine-tuning the model.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调模型。
- en: Testing and evaluating the model.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试和评估模型。
- en: Just as in RBM, the first step in setting up the main method is the declaration
    of variables and the code for creating demo data (the explanation is omitted here).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在 RBM 中一样，设置主方法的第一步是声明变量和创建演示数据的代码（此处省略说明）。
- en: 'Please check that in the demo data, the number of units for an input layer
    is 60, a hidden layer has 2 layers, their combined number of units is 20, and
    the number of units for an output layer is 3\. Now, let''s look through the code
    from the *Building the Model* section:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 请检查演示数据中，输入层的单元数为60，隐藏层有2层，它们的单元数总和为20，输出层的单元数为3。现在，让我们通过*构建模型*部分的代码：
- en: '[PRE10]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The variable of `hiddenLayerSizes` is an array and its length represents the
    number of hidden layers in deep neural networks. The deep learning algorithm takes
    a huge amount of calculation, hence the program gives us an output of the current
    status so that we can see which process is proceeding. The variable of `hiddenLayerSizes`
    is an array and its length represents the number of hidden layers in deep neural
    networks. Each layer is constructed in the constructor.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`hiddenLayerSizes` 变量是一个数组，它的长度表示深度神经网络中的隐藏层数量。深度学习算法需要大量的计算，因此程序会输出当前状态，以便我们查看哪个过程正在进行。`hiddenLayerSizes`
    变量是一个数组，它的长度表示深度神经网络中的隐藏层数量。每一层在构造函数中构建。'
- en: Tip
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Please bear in mind that `sigmoidLayers` and `rbmLayers` are, of course, different
    objects but their weights and bias are shared.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`sigmoidLayers` 和 `rbmLayers` 当然是不同的对象，但它们的权重和偏置是共享的。
- en: 'This is because, as explained in the theory section, pre-training performs
    layer-wise training, whereas the whole model can be regarded as one neural network:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为，如理论部分所述，预训练进行的是逐层训练，而整个模型可以看作是一个神经网络：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The first thing to do after building the model is pre-training:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型后，首先需要做的是预训练：
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Pre-training needs to be processed with each `minibatch` but, at the same time,
    with each layer. Therefore, all training data is given to the `pretrain` method
    first, and then the data of each mini-batch is processed in the method:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练需要使用每个`minibatch`来处理，但同时也是每一层进行处理。因此，所有训练数据首先传递给`pretrain`方法，然后在该方法中处理每个小批量的数据：
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Since the actual learning is done through CD-1 of RBM, the description of DBN
    within the code is very simple. In DBN (RBM), units of each layer have binary
    values, so the output method of `HiddenLayer` cannot be used because it returns
    double. Hence, the `outputBinomial` method is added to the class, which returns
    the `int` type (the code is omitted here). Once the pre-training is complete,
    the next step is fine-tuning.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于实际学习是通过RBM的CD-1进行的，代码中的DBN描述非常简单。在DBN（RBM）中，每一层的单元具有二值值，因此无法使用`HiddenLayer`的输出方法，因为它返回的是双精度数值。因此，类中添加了`outputBinomial`方法，它返回`int`类型（代码在此省略）。预训练完成后，下一步是微调。
- en: Tip
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Be careful not to use training data that was used in the pre-training.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 小心不要使用在预训练中已经使用过的训练数据。
- en: 'We can easily fall into overfitting if we use the whole data set for both pre-training
    and fine-tuning. Therefore, the validation data set is prepared separately from
    the training dataset and is used for fine-tuning:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将整个数据集同时用于预训练和微调，我们很容易陷入过拟合。因此，验证数据集需要与训练数据集分开准备，并用于微调：
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the `finetune` method, the backpropagation algorithm in multi-layer neural
    networks is applied where the logistic regression is used for the output layer.
    To backpropagate unit values among multiple hidden layers, we define variables
    to maintain each layer''s inputs:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在`finetune`方法中，应用了多层神经网络中的反向传播算法，其中输出层使用逻辑回归。为了在多个隐藏层之间反向传播单元值，我们定义了变量来维护每一层的输入：
- en: '[PRE15]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The training part of DBN is just how it is seen in the preceding code. The hard
    part is probably the theory and implementation of RBM, so you might think it's
    not too hard when you just look at the code of DBN.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: DBN的训练部分与前面的代码中的内容一致。困难的部分可能是RBM的理论和实现，所以你可能会觉得只看DBN的代码并不难。
- en: 'Since DBN after the training can be regarded as one (deep) neural network,
    you simply need to forward propagate data in each layer when you try to predict
    which class the unknown data belongs to:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练后的DBN可以看作一个（深度）神经网络，因此在尝试预测未知数据属于哪个类别时，你只需要在每一层进行前向传播：
- en: '[PRE16]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As for evaluation, no explanation should be needed because it's not much different
    from the previous classifier model.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 至于评估，由于与之前的分类器模型差异不大，因此不需要特别解释。
- en: Congratulations! You have now acquired knowledge of one of the deep learning
    algorithms. You might be able to understand it more easily than expected. However,
    the difficult part of deep learning is actually setting up the parameters, such
    as setting how many hidden layers there are, how many units there are in each
    hidden layer, the learning rate, the iteration numbers, and so on. There are way
    more parameters to set than in the method of machine learning. Please remember
    that you might find this point difficult when you apply this to a realistic problem.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你现在已经掌握了一个深度学习算法的知识。你可能比预期的更容易理解它。然而，深度学习的难点实际上在于设置参数，比如设置有多少个隐藏层，每个隐藏层有多少个单元，学习率，迭代次数等等。需要设置的参数比机器学习方法多得多。请记住，当你将其应用到实际问题中时，这一点可能会感到困难。
- en: Denoising Autoencoders
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 去噪自编码器
- en: 'The method used in pre-training for SDA is called **Denoising Autoencoders**
    (**DA**). It can be said that DA is the method that emphasizes the role of equating
    inputs and outputs. What does this mean? The processing content of DA is as follows:
    DA adds some noise to input data intentionally and partially damages the data,
    and then DA performs learning as it restores corrupted data to the original input
    data. This intentional noise can be easily substantiated if the input data value
    is [0, 1]; by turning the value of the relevant part into 0 compulsorily. If a
    data value is out of this range, it can be realized, for example, by adding Gaussian
    noise, but in this book, we''ll think about the former [0, 1] case to understand
    the core part of the algorithm.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在SDA的预训练中使用的方法被称为**去噪自编码器**（**DA**）。可以说，DA是一种强调输入与输出等价关系的方法。这是什么意思呢？DA的处理内容如下：DA故意向输入数据中添加一些噪声，并部分破坏数据，然后通过恢复损坏的数据到原始输入数据的过程进行学习。这种故意添加噪声的方法在输入数据值为[0,
    1]时可以轻松实现；通过强制将相关部分的值变为0。如果数据值超出了这个范围，可以通过添加高斯噪声等方式来实现，但在本书中，我们将基于[0, 1]的情况来理解算法的核心部分。
- en: 'In DA as well, an input/output layer is called a visible layer. DA''s graphical
    model can be shown to be the same shape of RBM, but to get a better understanding,
    let''s follow this diagram:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在DA中，输入/输出层被称为可见层。DA的图形模型与RBM的形状相同，但为了更好地理解，我们可以按照以下图示：
- en: '![Denoising Autoencoders](img/B04779_03_11.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_11.jpg)'
- en: 'Here, ![Denoising Autoencoders](img/B04779_03_51.jpg) is the corrupted data,
    the input data with noise. Then, forward propagation to the hidden layer and the
    output layer can be represented as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![去噪自编码器](img/B04779_03_51.jpg)是被破坏的数据，带有噪声的输入数据。然后，前向传播到隐藏层和输出层可以表示如下：
- en: '![Denoising Autoencoders](img/B04779_03_52.jpg)![Denoising Autoencoders](img/B04779_03_53.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_52.jpg)![去噪自编码器](img/B04779_03_53.jpg)'
- en: 'Here, ![Denoising Autoencoders](img/B04779_03_19.jpg) denotes the bias of the
    hidden layer and ![Denoising Autoencoders](img/B04779_03_23.jpg) the bias of the
    visible layer. Also, ![Denoising Autoencoders](img/B04779_03_20.jpg) denotes the
    sigmoid function. As seen in the preceding diagram, corrupting input data and
    mapping to a hidden layer is called **Encode** and mapping to restore the encoded
    data to the original input data is called **Decode**. Then, DA''s evaluation function
    can be denoted with a negative log likelihood function of the original input data
    and decoded data:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![去噪自编码器](img/B04779_03_19.jpg)表示隐藏层的偏置，![去噪自编码器](img/B04779_03_23.jpg)表示可见层的偏置。同时，![去噪自编码器](img/B04779_03_20.jpg)表示sigmoid函数。如前面的图示所示，破坏输入数据并映射到隐藏层的过程称为**编码**，而将编码后的数据映射回原始输入数据的过程称为**解码**。然后，DA的评估函数可以表示为原始输入数据和解码数据的负对数似然函数：
- en: '![Denoising Autoencoders](img/B04779_03_54.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_54.jpg)'
- en: 'Here, ![Denoising Autoencoders](img/B04779_03_27.jpg) is the model parameter,
    the weight and the bias of the visible layer and the hidden layer. What we need
    to do is just find the gradients of these parameters against the evaluation function.
    To deform equations easily, we define the functions here:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![去噪自编码器](img/B04779_03_27.jpg)是模型参数，即可见层和隐藏层的权重与偏置。我们需要做的就是找到这些参数对评估函数的梯度。为了方便变形方程，我们在这里定义了这些函数：
- en: '![Denoising Autoencoders](img/B04779_03_55.jpg)![Denoising Autoencoders](img/B04779_03_56.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_55.jpg)![去噪自编码器](img/B04779_03_56.jpg)'
- en: 'Then, we get:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们得到：
- en: '![Denoising Autoencoders](img/B04779_03_57.jpg)![Denoising Autoencoders](img/B04779_03_58.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_57.jpg)![去噪自编码器](img/B04779_03_58.jpg)'
- en: 'Using these functions, each gradient of a parameter can be shown as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些函数，每个参数的梯度可以表示如下：
- en: '![Denoising Autoencoders](img/B04779_03_59.jpg)![Denoising Autoencoders](img/B04779_03_60.jpg)![Denoising
    Autoencoders](img/B04779_03_61.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_59.jpg)![去噪自编码器](img/B04779_03_60.jpg)![去噪自编码器](img/B04779_03_61.jpg)'
- en: 'Therefore, only two terms are required. Let''s derive them one by one:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，只需要两个项。我们逐一推导它们：
- en: '![Denoising Autoencoders](img/B04779_03_62.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_62.jpg)'
- en: 'Here, we utilized the derivative of the `sigmoid` function:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们利用了`sigmoid`函数的导数：
- en: '![Denoising Autoencoders](img/B04779_03_63.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_63.jpg)'
- en: 'Also, we get:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们得到：
- en: '![Denoising Autoencoders](img/B04779_03_64.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_64.jpg)'
- en: 'Therefore, the following equation can be obtained:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以得到以下方程：
- en: '![Denoising Autoencoders](img/B04779_03_65.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_65.jpg)'
- en: 'On the other hand, we can also get the following:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们还可以得到以下结果：
- en: '![Denoising Autoencoders](img/B04779_03_66.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_66.jpg)'
- en: 'Hence, the renewed equation for each parameter will be as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个参数的更新方程如下：
- en: '![Denoising Autoencoders](img/B04779_03_67.jpg)![Denoising Autoencoders](img/B04779_03_68.jpg)![Denoising
    Autoencoders](img/B04779_03_69.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_67.jpg)![去噪自编码器](img/B04779_03_68.jpg)![去噪自编码器](img/B04779_03_69.jpg)'
- en: Here, ![Denoising Autoencoders](img/B04779_03_70.jpg) is the number of iterations
    and ![Denoising Autoencoders](img/B04779_03_50.jpg) is the learning rate. Although
    DA requires a bit of technique for deformation, you can see that the theory itself
    is very simple compared to RBM.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![去噪自编码器](img/B04779_03_70.jpg)是迭代次数，![去噪自编码器](img/B04779_03_50.jpg)是学习率。尽管DA在变形方面需要一些技巧，但与RBM相比，你会发现理论本身非常简单。
- en: Now, let's proceed with the implementation. The package structure is the same
    as the one for RBM.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行实现。包结构与RBM的结构相同。
- en: '![Denoising Autoencoders](img/B04779_03_12.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器](img/B04779_03_12.jpg)'
- en: 'As for model parameters, in addition to the number of units in a hidden layer,
    the amount of noise being added to the input data is also a parameter in DA. Here,
    the corruption level is set at `0.3`. Generally, this value is often set at `0.1
    ~ 0.3`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 至于模型参数，除了隐藏层中的单元数量外，添加到输入数据中的噪声量也是 DA 中的一个参数。这里，破坏水平设置为 `0.3`。通常，这个值常设置为 `0.1
    ~ 0.3`：
- en: '[PRE17]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The flow from the building model to training is the same as RBM. Although this
    method of training is called `contrastiveDivergence` in RBM, it''s simply set
    as `train` in DA:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从构建模型到训练的流程与 RBM 相同。尽管在 RBM 中，训练方法称为 `contrastiveDivergence`，但在 DA 中它被简单地设置为
    `train`：
- en: '[PRE18]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The content of `train` is as explained in the theory section. First of all,
    add noise to the input data, then encode and decode it:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`train` 的内容如理论部分所述。首先，对输入数据添加噪声，然后进行编码和解码：'
- en: '[PRE19]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The process of adding noise is, as previously explained, the compulsory turning
    of the value of the corresponding part of the data into `0`:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 添加噪声的过程，如前所述，是将数据对应部分的值强制设为 `0`：
- en: '[PRE20]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The other processes are just simple activation and propagation, so we won''t
    go through them here. The calculation of the gradients follows math equations:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 其他过程只是简单的激活和传播，所以我们这里不再详细讲解。梯度计算遵循数学方程：
- en: '[PRE21]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Compared to RBM, the implementation of DA is also quite simple. When you test
    (`reconstruct`) the model, you don''t need to corrupt the data. As in standard
    neural networks, you just need to forward propagate the given inputs based on
    the weights of the networks:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 与 RBM 相比，DA 的实现也相当简单。当你测试（`重构`）模型时，不需要破坏数据。与标准神经网络一样，你只需根据网络的权重前向传播给定的输入：
- en: '[PRE22]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Stacked Denoising Autoencoders (SDA)
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆叠去噪自编码器（SDA）
- en: 'SDA is deep neural networks with piled up DA layers. In the same way that DBN
    consists of RBMs and logistic regression, SDA consists of DAs and logistic regression:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: SDA 是具有堆叠 DA 层的深度神经网络。就像 DBN 由 RBM 和逻辑回归组成一样，SDA 由 DA 和逻辑回归组成：
- en: '![Stacked Denoising Autoencoders (SDA)](img/B04779_03_13.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![堆叠去噪自编码器 (SDA)](img/B04779_03_13.jpg)'
- en: The flow of implementation is not that different between DBN and SDA. Even though
    there is a difference between RBM and DA in pre-training, the content of fine-tuning
    is exactly the same. Therefore, not much explanation might be needed.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: DBN 和 SDA 的实现流程没有太大区别。尽管在预训练中 RBM 和 DA 存在差异，但微调的内容完全相同。因此，可能不需要过多解释。
- en: 'The method for pre-training is not that different, but please note that the
    point where the `int` type was used for DBN is changed to double type, as DA can
    handle `[0, 1]`, not binary:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的方法没有太大不同，但请注意，原本在 DBN 中使用的 `int` 类型已改为双精度类型，因为 DA 可以处理 `[0, 1]`，而非二进制：
- en: '[PRE23]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `predict` method after learning is also exactly the same as in DBN. Considering
    that both DBN and SDA can be treated as one multi-layer neural network after learning
    (that is, the pre-training and fine-tuning), it's natural that most of the processes
    are common.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 学习后的 `predict` 方法与 DBN 完全相同。考虑到 DBN 和 SDA 在学习后可以视为一个多层神经网络（即预训练和微调），因此大多数过程是相通的。
- en: Overall, SDA can be implemented more easily than DBN, but the precision to be
    obtained is almost the same. This is the merit of SDA.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，SDA 比 DBN 更容易实现，但获得的精度几乎相同。这就是 SDA 的优点。
- en: Summary
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at the problem of the previous neural networks algorithm
    and what the breakthrough was for deep learning. Also, you learned about the theory
    and implementation of DBN and SDA, the algorithm that fueled the boom of deep
    learning, and of RBM and DA used in each respective method.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了之前神经网络算法的问题以及深度学习的突破。你还了解了 DBN 和 SDA 的理论与实现，这些算法推动了深度学习的繁荣，以及分别在每种方法中使用的
    RBM 和 DA。
- en: In the next chapter, we'll look at more deep learning algorithms. They take
    different approaches to obtain high precision rates and are well developed.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍更多的深度学习算法。它们采取不同的方法来获得高精度，并且发展得相当成熟。
