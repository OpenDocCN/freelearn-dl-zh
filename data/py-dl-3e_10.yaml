- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: Operations (MLOps)
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 运营（MLOps）
- en: 'So far in this book, we have focused on the theory of **neural networks** (**NNs**),
    various NN architectures, and the tasks we can solve with them. This chapter is
    a little different because we’ll focus on some of the practical aspects of NN
    development. We’ll delve into this topic because the development and production
    deployment of ML models (and NNs in particular) have some unique challenges. We
    can split this process into three steps:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们专注于**神经网络（NNs）**的理论、各种NN架构以及我们可以解决的任务。这一章节有些不同，因为我们将专注于NN开发的一些实际方面。我们将深入探讨这个主题，因为ML模型（特别是NNs）的开发和生产部署存在一些独特的挑战。我们可以将这个过程分为三个步骤：
- en: '**Training dataset creation**: Data collection, cleanup, storage, transformations,
    and feature engineering.'
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练数据集创建**：数据收集、清理、存储、转换和特征工程。'
- en: '**Model development**: Experiment with different models and training algorithms
    and evaluate them.'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型开发**：尝试不同的模型和训练算法，并评估它们。'
- en: '**Deployment**: Deploy trained models in the production environment and monitor
    their performance in computational and accuracy terms.'
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署**：在生产环境中部署训练好的模型，并监控其在计算和准确性方面的性能。'
- en: 'This multi-step complex pipeline presupposes some of the challenges when solving
    ML tasks:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个多步骤的复杂流程预设了解决ML任务时的一些挑战：
- en: '**Diverse software toolkit**: Each step has multiple competing tools.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样化的软件工具包**：每个步骤都有多个竞争工具。'
- en: '**Model development is hard**: Each training instance has a large number of
    variables. These could be modifications in the NN architecture, variations in
    the training hyperparameters (such as learning rate or momentum), or different
    training data distributions. On top of that, NNs have sources of randomness, such
    as weight initialization or data augmentation. Therefore, if we cannot reproduce
    earlier results, it won’t be easy to pinpoint the reason. Even if we have a bug
    in the code, it might not result in an easily detectable runtime exception. Instead,
    it might just deteriorate the model accuracy slightly. So that we don’t lose track
    of all the experiments, we need a robust tracking and monitoring system.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型开发很难**：每个训练实例都有大量的变量。这些可能包括NN架构的修改、训练超参数的变化（如学习率或动量）或不同的训练数据分布。此外，NNs还有随机源，例如权重初始化或数据增强。因此，如果我们无法重现早期的结果，很难找出原因。即使代码中有错误，它可能不会导致易于检测的运行时异常。相反，它可能只是稍微降低模型的准确性。为了不丢失所有实验的记录，我们需要一个强大的跟踪和监控系统。'
- en: '**Complex deployment and monitoring**: NNs require GPUs and batch-organized
    data for optimal performance. These requirements might collide with the real-world
    requirements of processing data in streams or sample-wise. In addition, the nature
    of the user data might change with time, which could cause **model drift**.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂的部署和监控**：NNs 需要GPU和批处理组织的数据以达到最佳性能。这些要求可能与实时处理数据或逐样本处理的真实世界要求发生冲突。此外，用户数据的性质可能随时间变化，这可能导致**模型漂移**。'
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Understanding model development
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解模型开发
- en: Exploring model deployment
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索模型部署
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We’ll implement the examples in this chapter using Python, PyTorch, **TensorFlow**
    (**TF**), and **Hugging Face** (**HF**) Transformers, among others. If you don’t
    have an environment set up with these tools, fret not – the examples are available
    as Jupyter Notebooks on Google Colab. You can find the code examples in this book’s
    GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter10).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Python、PyTorch、**TensorFlow（TF）**和**Hugging Face（HF）** Transformers等工具实现本章中的示例。如果您还没有配置好这些工具的环境，不要担心——示例代码可以在Google
    Colab的Jupyter Notebook中找到。您可以在本书的GitHub存储库中找到这些代码示例：[https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter10)。
- en: Understanding model development
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解模型开发
- en: In this section, we’ll discuss various tools that will help us manage the model
    development phase of the ML solution life cycle. Let’s start with the most important
    question – which NN framework should we choose?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论各种工具，这些工具将帮助我们管理ML解决方案生命周期的模型开发阶段。让我们从最重要的问题开始——我们应该选择哪个NN框架？
- en: Choosing an NN framework
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择一个NN框架
- en: So far in this book, we’ve mostly used PyTorch and TensorFlow. We can refer
    to them as **foundational** frameworks as these are the most important components
    of the entire NN software stack. They serve as a base for other components in
    the ML NN ecosystem, such as Keras or HF Transformers, which can use either of
    them as a backend (multi-backend support will come with Keras 3.0). In addition
    to TF, Google has also released JAX ([https://github.com/google/jax](https://github.com/google/jax)),
    a foundational library that supports GPU-accelerated NumPy operations and Autograd.
    Other popular libraries such as NumPy, pandas, and scikit-learn ([https://scikit-learn.org](https://scikit-learn.org))
    go beyond the scope of this book as they are not strictly related to NNs. Because
    of the importance of foundational libraries, they are the first and most important
    choice in our toolkit. But which one should we choose if we start a project from
    scratch?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中我们主要使用了PyTorch和TensorFlow。我们可以将它们称为**基础**框架，因为它们是整个神经网络软件堆栈中最重要的组件。它们作为机器学习神经网络生态系统中其他组件的基础，比如Keras或HF
    Transformers，这些组件可以使用它们作为后端（Keras 3.0将支持多后端）。除了TensorFlow，Google还发布了JAX（[https://github.com/google/jax](https://github.com/google/jax)），这是一个支持GPU加速的NumPy操作和Autograd的基础库。其他流行的库，如NumPy、pandas和scikit-learn（[https://scikit-learn.org](https://scikit-learn.org)）超出了本书的范围，因为它们与神经网络没有直接关系。由于基础库的重要性，它们是我们工具包中的首选和最重要的选择。但如果我们从零开始启动一个项目，应该选择哪一个呢？
- en: PyTorch versus TensorFlow versus JAX
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PyTorch与TensorFlow与JAX
- en: Let’s check the level of community adoption for these libraries. Our first stop
    is **Papers with Code** ([https://paperswithcode.com/](https://paperswithcode.com/)),
    which indexes ML papers, code, datasets, and results. The site also maintains
    the trend of paper implementations grouped by framework ([https://paperswithcode.com/trends](https://paperswithcode.com/trends)).
    As of September 2023, 57% of the new papers are using PyTorch. TF and JAX are
    distant second and third with 3% and 2%, respectively. This trend isn’t new –
    PyTorch was released in 2016, but it has already surpassed TF in 2019\. This particular
    data point indicates that PyTorch dominates cutting-edge research, which is what
    the most recent papers are. Therefore, if you want to always use the latest and
    greatest in the field, it’s a good idea to stick to PyTorch. Next, let’s look
    at the ML models, hosted on the HF platform ([https://huggingface.co/models](https://huggingface.co/models)),
    where we can also filter by project framework. Out of ~335,000 total models hosted,
    ~131,000 use PyTorch, ~10,000 use TF, and ~9,000 use JAX. Again, this is a strong
    result in favor of PyTorch. However, this is not the full picture, as these results
    are for public and open source projects. They are not necessarily indicative of
    what companies use in production. More representative of this could be PyPI Stats
    ([https://pypistats.org/](https://pypistats.org/)), which provides aggregate download
    information on Python packages available from the **Python Package Index** (**PyPi**,
    [https://pypi.org/](https://pypi.org/)). The picture here is a bit more nuanced
    – PyTorch has 11,348,753 downloads for the last month (August-September 2023)
    versus 16,253,288 for TF and 3,041,747 for JAX. However, we should be cautious
    with PyPi Stats because many automated processes (such as continuous integration)
    can inflate the PyPI download count, without indicating real-world use. In addition,
    the PyTorch download page advises installing the library through Conda ([https://conda.io/](https://conda.io/)).
    The monthly statistics show 759,291 PyTorch downloads versus 154,504 for TF and
    6,260 for JAX. Therefore, PyTorch leads here as well. Overall, my conclusion is
    that PyTorch is more popular than TF, but both libraries are used in production
    environments.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来查看这些库在社区中的采用程度。我们的第一站是**Papers with Code** ([https://paperswithcode.com/](https://paperswithcode.com/))，它索引了机器学习论文、代码、数据集和结果。该网站还维护了按框架分组的论文实现趋势（[https://paperswithcode.com/trends](https://paperswithcode.com/trends)）。截至2023年9月，57%的新论文使用PyTorch。TF和JAX分别以3%和2%的比例位居第二和第三。这个趋势并不新鲜——PyTorch于2016年发布，但它在2019年已经超过了TF。这个特定的数据点表明，PyTorch主导着前沿研究，而这些研究正是最新的论文。因此，如果你希望始终使用该领域最新和最优秀的技术，选择PyTorch是个不错的主意。接下来，我们来看一下托管在HF平台上的机器学习模型（[https://huggingface.co/models](https://huggingface.co/models)），我们也可以按项目框架进行筛选。在大约335,000个托管模型中，约131,000个使用PyTorch，约10,000个使用TF，约9,000个使用JAX。再次，这一结果强烈支持PyTorch。然而，这并不是完整的图景，因为这些结果仅适用于公开和开源项目。它们不一定能反映公司在生产环境中的使用情况。更具代表性的可能是PyPI
    Stats（[https://pypistats.org/](https://pypistats.org/)），它提供了**Python软件包索引**（**PyPi**，[https://pypi.org/](https://pypi.org/)）上Python包的下载汇总信息。这里的情况稍微复杂一些——PyTorch在过去一个月（2023年8月-9月）有11,348,753次下载，而TF为16,253,288次，JAX为3,041,747次。然而，我们应该对PyPi
    Stats持谨慎态度，因为许多自动化流程（例如持续集成）可能会使PyPi的下载次数膨胀，而这些并不代表真实世界的使用情况。此外，PyTorch的下载页面建议通过Conda（[https://conda.io/](https://conda.io/)）安装该库。月度统计数据显示，PyTorch有759,291次下载，而TF为154,504次，JAX为6,260次。因此，PyTorch在这里也占据领先地位。总的来说，我的结论是，PyTorch比TF更受欢迎，但这两者都在生产环境中使用。
- en: My advice, which you can take with as many pinches of salt as you wish, would
    be to select PyTorch if you start a project now. This is why this book has put
    more emphasis on PyTorch compared to TF. One exception to this rule is if your
    project runs on mobile or edge devices ([https://en.wikipedia.org/wiki/Edge_device](https://en.wikipedia.org/wiki/Edge_device))
    with limited computational power. TF has better support for such devices through
    the TF Lite library ([https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议是，如果你现在开始一个项目，可以选择PyTorch，具体如何采纳这个建议可以根据你的实际情况来决定。正因如此，本书在讲解时相对于TF更多强调了PyTorch。这个规则的一个例外是，如果你的项目运行在移动设备或边缘设备（[https://en.wikipedia.org/wiki/Edge_device](https://en.wikipedia.org/wiki/Edge_device)）上，并且计算能力有限。TF通过TF
    Lite库（[https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)）对这类设备提供了更好的支持。
- en: But ultimately, you can work with your preferred software stack and then convert
    your models into other libraries for deployment. We’ll see how this is possible
    in the next section.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但最终，你可以使用自己偏好的软件栈进行工作，然后将模型转换为其他库以进行部署。我们将在下一节中看到这如何实现。
- en: Open Neural Network Exchange
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开放神经网络交换格式
- en: '**Open Neural Network Exchange** (**ONNX**, [https://onnx.ai/](https://onnx.ai/))
    provides an open source format for NN-based and traditional ML models (we’ll focus
    on NNs here). It defines an extensible computation graph model, built-in operators,
    and standard data types. In other words, ONNX provides a universal NN representation
    format, which allows us to convert models implemented with one library (for example,
    PyTorch) into others (such as TF), provided that both the source and target libraries
    support ONNX. In this way, you can train your model with one library and then
    convert it into another when deploying to production. This also makes sense because
    ONNX focuses on inference mode and not training (representing the training process
    using ONNX in experimental mode).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**开放神经网络交换格式**（**ONNX**，[https://onnx.ai/](https://onnx.ai/)）提供了一个用于基于神经网络（NN）和传统机器学习（ML）模型的开源格式（我们将在这里专注于神经网络）。它定义了一个可扩展的计算图模型、内置操作符和标准数据类型。换句话说，ONNX提供了一个通用的神经网络表示格式，允许我们将一个库（例如PyTorch）实现的模型转换为另一个库（如TF）的模型，前提是源库和目标库都支持ONNX。通过这种方式，你可以使用一个库来训练模型，然后在部署到生产环境时将其转换为另一个库。这也很有意义，因为ONNX关注推理模式，而不是训练（使用ONNX表示训练过程是实验模式）。'
- en: 'ONNX represents an NN as a computational `onnx` (`!pip install onnx`) Python
    package. Let’s start:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX通过计算`onnx`（`!pip install onnx`）Python包来表示一个神经网络。我们开始吧：
- en: 'Define the graph representation’s input (`X`, `A`, `B`) and output (`Y`) variables:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义图表示的输入（`X`、`A`、`B`）和输出（`Y`）变量：
- en: '[PRE0]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, `make_tensor_value_info` declares a named graph I/O variables (`X` and
    `Y`) with a type (`elem_type`) and `shape`. `shape=[None]` means any shape, and
    `shape=[None, None]` means a two-dimensional tensor without specific dimension
    sizes. On the other hand, `A` and `B` are the function parameters (weights), and
    we initialize them with pre-defined values from NumPy arrays.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`make_tensor_value_info`声明了命名的图输入输出变量（`X`和`Y`），并为其定义了类型（`elem_type`）和`shape`。`shape=[None]`意味着任意形状，而`shape=[None,
    None]`意味着没有具体维度大小的二维张量。另一方面，`A`和`B`是函数参数（权重），我们用NumPy数组中的预定义值对它们进行初始化。
- en: 'Define the graph operations:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义图的操作：
- en: '[PRE1]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`mat_mul` represents matrix multiplication (`MatMul`) between the `X` and `A`
    input matrices into the `XA` output variable. `addition` sums the output of `mat_mul`,
    `XA`, with the bias, `B`.'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`mat_mul`表示输入矩阵`X`和`A`之间的矩阵乘法（`MatMul`），并将结果存储到输出变量`XA`中。`addition`将`mat_mul`的输出`XA`与偏置`B`相加。'
- en: ONNX operators
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX操作符
- en: This example introduces the `MatMul` and `Add` ONNX operators. The full list
    of supported operators (available at [https://onnx.ai/onnx/operators/](https://onnx.ai/onnx/operators/))
    includes many other NN building blocks, such as activation functions, convolutions,
    pooling, and tensor operators (for example, `concat`, `pad`, `reshape`, and `flatten`).
    In addition, it supports the so-called `if` operator executes one subgraph or
    another, depending on a Boolean value. ONNX itself doesn’t implement the operators.
    Instead, the libraries that support it (such as PyTorch) have their own implementations.
    Conversely, the ONNX conversion will fail if your library model has operators
    that aren’t supported by ONNX.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例介绍了`MatMul`和`Add`的ONNX操作符。支持的操作符完整列表（请参见[https://onnx.ai/onnx/operators/](https://onnx.ai/onnx/operators/)）包括许多其他神经网络构建块，如激活函数、卷积、池化以及张量操作符（例如`concat`、`pad`、`reshape`和`flatten`）。此外，它还支持所谓的`if`操作符，根据布尔值执行一个子图或另一个子图。ONNX本身并不实现这些操作符。相反，支持它的库（如PyTorch）有自己的实现。反过来，如果你的库模型使用了ONNX不支持的操作符，ONNX转换将会失败。
- en: 'We now have the ingredients to define the computational `graph`:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经具备了定义计算`graph`的条件：
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can see our computational graph in the following figure:'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以在以下图中看到我们的计算图：
- en: '![Figure 10.1 – Linear regression ONNX computational graph](img/B19627_10_1.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – 线性回归ONNX计算图](img/B19627_10_1.jpg)'
- en: Figure 10.1 – Linear regression ONNX computational graph
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 线性回归ONNX计算图
- en: 'Use `graph` to create an `onnx_model` instance. The model allows you to add
    additional metadata to the graph, such as docstring, version, author, and license,
    among others:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`graph`来创建一个`onnx_model`实例。该模型允许你向图中添加额外的元数据，如文档字符串、版本、作者和许可证等：
- en: '[PRE3]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Check the model for consistency. This verifies that the input type or shapes
    match between the model components:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查模型的一致性。这可以验证模型组件之间输入类型或形状是否匹配：
- en: '[PRE4]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we can compute the output of the model for two random input samples
    with an instance of `ReferenceEvaluator`:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用 `ReferenceEvaluator` 实例计算两个随机输入样本的模型输出：
- en: '[PRE5]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The result of the computation is a NumPy array:'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算的结果是一个NumPy数组：
- en: '[PRE6]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'ONNX allows us to serialize and deserialize both the model structure and its
    weights with **Protocol Buffers** (**protobuf**, [https://protobuf.dev/](https://protobuf.dev/)).
    Here’s how to do this:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ONNX 允许我们使用 **协议缓冲区** (**Protocol Buffers**，**protobuf**，[https://protobuf.dev/](https://protobuf.dev/))
    来序列化和反序列化模型结构及其权重。以下是操作方法：
- en: '[PRE7]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have introduced ONNX, let’s see how we can use it in practice by
    exporting PyTorch and TF models to ONNX.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了ONNX，接下来看看如何通过将PyTorch和TF模型导出到ONNX来实际应用它。
- en: 'In addition to `torch` and `tensorflow`, we’ll also need the `torchvision`,
    `onnx`, and `tf2onnx` ([https://github.com/onnx/tensorflow-onnx](https://github.com/onnx/tensorflow-onnx),
    `!pip install tf2onnx`) packages. Let’s start with PyTorch:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `torch` 和 `tensorflow`，我们还需要 `torchvision`、`onnx` 和 `tf2onnx`（[https://github.com/onnx/tensorflow-onnx](https://github.com/onnx/tensorflow-onnx)，`!pip
    install tf2onnx`）包。我们先从PyTorch开始：
- en: 'Load a pre-trained model (`MobileNetV3`, refer to [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146)):'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个预训练模型（`MobileNetV3`，参考 [*第5章*](B19627_05.xhtml#_idTextAnchor146)）：
- en: '[PRE8]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, export the model:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，导出模型：
- en: '[PRE9]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Most parameters speak for themselves. `args=torch.randn(1, 3, 224, 224)` specifies
    a dummy tensor. This is necessary because the serializer might invoke the model
    once to infer the graph structure and tensor sizes. The dummy tensor will serve
    as input for this invocation. However, this exposes one of the limitations of
    the conversion process: if the model includes a dynamic computational graph, the
    converter will only convert the path of the current invocation. `export_params`
    tells the exporter to include the model weights, besides the model structure.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数参数不言而喻。`args=torch.randn(1, 3, 224, 224)` 指定了一个虚拟张量。这是必要的，因为序列化器可能会调用模型一次，以推断图结构和张量的大小。这个虚拟张量将作为调用的输入。然而，这也暴露了转换过程中的一个限制：如果模型包含动态计算图，转换器仅会转换当前调用路径。`export_params`
    告诉导出器在导出模型结构时也包括模型的权重。
- en: 'Use ONNX to load the exported model and check it for consistency (spoiler:
    it works):'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ONNX加载导出的模型并检查其一致性（剧透：它可以正常工作）：
- en: '[PRE10]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, let’s do the same but with TF. Unlike PyTorch, TF doesn’t have out-of-the-box
    ONNX serialization support. Instead, we’ll use the `tf2onnx` package:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们也可以使用TF执行相同的操作。与PyTorch不同，TF没有开箱即用的ONNX序列化支持。相反，我们将使用 `tf2onnx` 包：
- en: 'Load a pre-trained `MobileNetV3` model:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一个预训练的 `MobileNetV3` 模型：
- en: '[PRE11]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Serialize the model using `tf2onnx`. It follows the same principle as PyTorch,
    down to the dummy input tensor (`input_signature`), which is necessary for model
    invocation:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `tf2onnx` 序列化模型。它遵循与PyTorch相同的原理，包括虚拟输入张量（`input_signature`），这是调用模型时必需的：
- en: '[PRE12]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once again, we can load the model with ONNX to verify its consistency.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次可以使用ONNX加载模型，以验证其一致性。
- en: 'Next, we can use `torch_model.onnx` or `tf_model.onnx`). This is a graphical
    viewer for NNs and other ML models. It exists as a web **user interface** (**UI**)
    or a standalone app. It supports ONNX, TensorFlow Lite, and PyTorch (experimental),
    among other libraries. For example, the following figure shows the initial **MobileNetV3**
    layers in detail, as visualized by Netron (the full model visualization is too
    large to display within this chapter):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用 `torch_model.onnx` 或 `tf_model.onnx`。这是一种用于神经网络和其他机器学习模型的图形查看工具。它可以作为
    **用户界面** (**UI**) 的Web版，或者作为独立应用程序存在。它支持ONNX、TensorFlow Lite和PyTorch（实验性支持），以及其他一些库。例如，以下图显示了通过Netron可视化的初始**MobileNetV3**层（完整模型的可视化太大，无法在本章中显示）：
- en: '![Figure 10.2 – Netron visualization of the MobileNetV3 ONNX model file](img/B19627_10_2.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – MobileNetV3 ONNX模型文件的Netron可视化](img/B19627_10_2.jpg)'
- en: Figure 10.2 – Netron visualization of the MobileNetV3 ONNX model file
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – MobileNetV3 ONNX模型文件的Netron可视化
- en: Here, the input shape is 3×224×224, **W** is the shape of the convolutional
    filter, and **B** is the bias. We introduced the rest of the convolution attributes
    in [*Chapter 4*](B19627_04.xhtml#_idTextAnchor107).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，输入形状为3×224×224，**W** 是卷积滤波器的形状，**B** 是偏置。我们在 [*第4章*](B19627_04.xhtml#_idTextAnchor107)
    中介绍了其余的卷积属性。
- en: Unfortunately, neither PyTorch nor TF comes with the integrated ability to load
    ONNX models. However, there are open source packages that allow us to do this.
    Two of them are `onnx2torch` ([https://github.com/ENOT-AutoDL/onnx2torch](https://github.com/ENOT-AutoDL/onnx2torch))
    for PyTorch and `onnx2tf` ([https://github.com/PINTO0309/onnx2tf](https://github.com/PINTO0309/onnx2tf))
    for TF.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，PyTorch 和 TF 都没有集成加载 ONNX 模型的功能。但是，已经有开源包允许我们实现这一点。其中有两个分别为 PyTorch 提供的
    `onnx2torch` ([https://github.com/ENOT-AutoDL/onnx2torch](https://github.com/ENOT-AutoDL/onnx2torch))
    和为 TF 提供的 `onnx2tf` ([https://github.com/PINTO0309/onnx2tf](https://github.com/PINTO0309/onnx2tf))。
- en: Next, we’ll focus on a tool that will ease the training process.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将重点介绍一款能够简化训练过程的工具。
- en: Introducing TensorBoard
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 TensorBoard
- en: '**TensorBoard** (**TB**, [https://www.tensorflow.org/tensorboard/](https://www.tensorflow.org/tensorboard/),
    [https://github.com/tensorflow/tensorboard](https://github.com/tensorflow/tensorboard))
    is a TF-complement web-based tool that provides visualization and tooling for
    machine learning experiments. Some of its functions are as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorBoard**（**TB**，[https://www.tensorflow.org/tensorboard/](https://www.tensorflow.org/tensorboard/)，[https://github.com/tensorflow/tensorboard](https://github.com/tensorflow/tensorboard)）是一个
    TF 补充的基于网页的工具，提供了机器学习实验的可视化和工具支持。它的一些功能如下：'
- en: Metrics (such as loss and accuracy) tracking and visualization
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标（如损失和精度）跟踪和可视化
- en: Model graph visualization (similar to Netron)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型图可视化（类似 Netron）
- en: A time series histogram of the change of weights, biases, or other tensors over
    time
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示权重、偏差或其他张量随时间变化的时间序列直方图
- en: Low-dimensional embedding projections
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低维嵌入投影
- en: TB can work with both TF/Keras and PyTorch, but it has better integration with
    TF (after all, it is developed by the TF team). In both cases, TB doesn’t communicate
    directly with the models during training. Instead, the training process stores
    its state and current progress in a special log file. TB tracks the file for changes
    and automatically updates its graphical interface with the latest information.
    In this way, it can visualize the training as it progresses. In addition, the
    file stores the entire training history to be displayed even after it finishes.
    To better understand how it works, we’ll add TB to the transfer learning computer
    vision examples that we introduced in [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146).
    As a quick recap, we’ll start with ImageNet pre-trained MobileNetV3 models. Then,
    we’ll use two transfer learning techniques, **feature engineering** and **fine-tuning**,
    to train these models to classify the CIFAR-10 dataset. TB will visualize the
    training.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 可以与 TF/Keras 和 PyTorch 一起使用，但它与 TF 的集成更好（毕竟是由 TF 团队开发的）。在这两种情况下，TensorBoard
    在训练过程中并不会直接与模型通信。相反，训练过程会将其状态和当前进度存储在一个特殊的日志文件中。TensorBoard 跟踪该文件的变化，并自动更新其图形界面，展示最新信息。通过这种方式，它可以随着训练的进展实时可视化训练过程。此外，该文件还会存储整个训练历史，即使训练完成后，仍然可以展示这些数据。为了更好地理解其工作原理，我们将把
    TensorBoard 添加到我们在 [*第5章*](B19627_05.xhtml#_idTextAnchor146) 中介绍的迁移学习计算机视觉示例中。简要回顾一下，我们将从
    ImageNet 预训练的 MobileNetV3 模型开始。接着，我们将使用两种迁移学习技术，**特征工程**和**微调**，来训练这些模型以对 CIFAR-10
    数据集进行分类。TensorBoard 将可视化训练过程。
- en: 'Let’s start with the Keras example. We’ll only include the relevant part of
    the code, and not the full example, as we discussed it in [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146).
    More specifically, we’ll focus on the `train_model(model, epochs=5)` function,
    which takes the pre-trained `model` and the number of training `epochs` as parameters.
    The following is the function’s body (please note that the actual implementation
    has indentation):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 Keras 示例开始。我们只会包括相关部分的代码，而不是完整的示例，因为我们在 [*第5章*](B19627_05.xhtml#_idTextAnchor146)
    中已经讨论过了。更具体地说，我们将专注于 `train_model(model, epochs=5)` 函数，该函数将预训练的 `model` 和训练的 `epochs`
    数量作为参数。以下是该函数的主体（请注意，实际的实现有缩进）：
- en: Initializing TensorBoard
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化 TensorBoard
- en: This example assumes that TB is initialized and running (although the code works
    even if it is not available). We won’t include the initialization of TB because
    it differs depending on the environment. However, it is included in the Jupyter
    Notebook of this example.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例假设 TensorBoard 已经初始化并正在运行（尽管即使未安装，代码仍然可以正常工作）。我们不会包括 TensorBoard 的初始化代码，因为它取决于环境设置。但它在本示例的
    Jupyter Notebook 中是有包含的。
- en: 'Follow these steps:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: 'First, we’ll configure the training of the pre-trained Keras model with the
    Adam optimizer, binary cross-entropy loss, and accuracy tracking:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用 Adam 优化器、二元交叉熵损失函数以及精度跟踪来配置预训练 Keras 模型的训练：
- en: '[PRE13]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we’ll add the special `tensorboard_callback`, which implements the TB
    connection:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将添加特殊的`tensorboard_callback`，它实现了TB连接：
- en: '[PRE14]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The callback parameters are as follows:'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回调参数如下：
- en: '`log_dir`: This instructs `tensorboard_callback` to write the log file in a
    unique time-stamped folder, `''logs/tb/'' + datetime.datetime.now().strftime(''%Y%m%d-%H%M%S'')`,
    located in the main `''logs/tb/''` folder. TB will simultaneously pick all training
    folders under `''logs/tb/''` and display them in its UI as unique training instances.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`log_dir`：这指示`tensorboard_callback`将日志文件写入一个唯一的时间戳文件夹，即`''logs/tb/'' + datetime.datetime.now().strftime(''%Y%m%d-%H%M%S'')`，位于主文件夹`''logs/tb/''`下。TB将同时选择`''logs/tb/''`下所有训练文件夹，并在其UI中显示它们作为唯一的训练实例。'
- en: '`update_freq=1`: Updates the log file once per epoch.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`update_freq=1`：每个周期更新日志文件。'
- en: '`histogram_freq=1`: Computes weight histograms once per epoch.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`histogram_freq=1`：每个周期计算一次权重直方图。'
- en: '`write_graph=True`: Generates a graph visualization of the NN architecture.'
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write_graph=True`：生成NN架构的图形可视化。'
- en: '`write_images=True`: Visualizes the model weights as an image.'
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write_images=True`：将模型权重可视化为图像。'
- en: '`write_steps_per_second=True`: Logs the training steps per second.'
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write_steps_per_second=True`：记录每秒训练步骤。'
- en: '`profile_batch=1`: Profiles the first batch to sample its compute characteristics.'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`profile_batch=1`：对第一批次进行分析以采样其计算特性。'
- en: '`Embeddings_freq=0`: The frequency (in epochs) at which embedding layers will
    be visualized (we don’t have embedding layers, so it’s disabled by default).'
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Embeddings_freq=0`：嵌入层将被可视化的频率（以周期为单位）（我们没有嵌入层，因此默认情况下禁用）。'
- en: 'Finally, we’ll run the training with the `model.fit` method:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用`model.fit`方法运行训练：
- en: '[PRE15]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We add `tensorboard_callback` to the list of `model` `callbacks`. The training
    process notifies each callback for various training events: start of training,
    end of training, start of testing, end of testing, start of epoch, end of epoch,
    start of batch, and end of batch. In turn, `tensorboard_callback` updates the
    log file according to its configuration and the current event.'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将`tensorboard_callback`添加到`model`的回调列表中。训练过程会通知每个回调不同的训练事件：训练开始、训练结束、测试开始、测试结束、周期开始、周期结束、批次开始和批次结束。反过来，`tensorboard_callback`根据其配置和当前事件更新日志文件。
- en: 'The TB UI displays all the information in the log file. Although it’s too complex
    to include here, we can still show a snippet with accuracy:'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TB UI显示了日志文件中的所有信息。虽然它过于复杂无法在此处包含，但我们仍然可以显示一个关于准确度的片段：
- en: '![Figure 10.3 – Accuracy in the TB UI](img/B19627_10_3.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3 – TB UI中的准确度](img/B19627_10_3.jpg)'
- en: Figure 10.3 – Accuracy in the TB UI
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – TB UI中的准确度
- en: Here, TB displays the accuracy for four different experiments – train/test for
    feature engineering and train/test for fine-tuning.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，TB显示了四个不同实验的准确度 - 特征工程的训练/测试和微调的训练/测试。
- en: Next, let’s see how PyTorch integrates with TB. It provides a special `torch.utils.tensorboard.SummaryWriter`
    class, which writes entries directly to event log files in the `log_dir` folder
    to be consumed by TB. It follows the same principle as in Keras. The high-level
    API of `SummaryWriter` allows us to create an event file in `log_dir` and asynchronously
    add content to it. The main difference with Keras is that we’re responsible for
    adding the content, instead of an automated event listener doing it. Let’s see
    how that works in practice. As with Keras, we’ll use the computer vision transfer
    learning example from [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146). We’ll only
    focus on the relevant parts, but you can see the full example in the Jupyter Notebook
    in this book’s GitHub repository.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看PyTorch如何与TB集成。它提供了一个特殊的`torch.utils.tensorboard.SummaryWriter`类，它将条目直接写入事件日志文件，以供TB消费。它遵循与Keras相同的原则。`SummaryWriter`的高级API允许我们在`log_dir`中创建一个事件文件，并异步向其添加内容。与Keras不同的主要区别在于，我们负责添加内容，而不是由自动事件侦听器执行。让我们看看实际操作中是如何工作的。与Keras一样，我们将使用计算机视觉迁移学习示例来自[*第5章*](B19627_05.xhtml#_idTextAnchor146)。我们只关注相关部分，但您可以在本书的GitHub存储库的Jupyter笔记本中查看完整示例。
- en: 'First, we’ll initialize two `SummaryWriter` instances for the feature extractor
    fine-tuning modes. It doesn’t matter where we do it, so long as it happens before
    we start using them. As with Keras, each training instance has a unique time-stamped
    folder under `''logs/tb/''` (we’re only showing one initialization because they
    are identical):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将初始化两个`SummaryWriter`实例，用于特征提取器的微调模式。无论我们在哪里执行它，只要在开始使用它们之前执行即可。与Keras一样，每个训练实例都有一个唯一的时间戳文件夹，位于`'logs/tb/'`下（我们仅显示一个初始化，因为它们都是相同的）：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For the sake of clarity, we’ll include the initialization of the MobileNetV3
    pre-trained model:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，我们将包括初始化 MobileNetV3 预训练模型的代码：
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we’ll jump to the training (or testing) loop, where `train_loader`, an
    instance of `torch.utils.data.DataLoader`, yields pairs of `inputs` and `labels`
    mini-batches:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进入训练（或测试）循环，其中 `train_loader`，`torch.utils.data.DataLoader` 的一个实例，生成
    `inputs` 和 `labels` 的小批量数据：
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Within the loop, we can add the model graph to the log file. It takes the model
    and the input tensor as parameters to generate the visualization (hence the need
    to call `add_graph` in the training loop):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在循环中，我们可以将模型图添加到日志文件中。它以模型和输入张量作为参数生成可视化（因此需要在训练循环中调用 `add_graph`）：
- en: '[PRE19]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, at the end of the training loop, we’ll add the loss and the accuracy
    for the current `epoch` as scalar values:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在训练循环的末尾，我们将添加当前 `epoch` 的损失和准确率作为标量值：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Each scalar value has a unique `tag` (besides the two tags in the code, we also
    have `tag='validation/loss'`). Note that `global_step` (equal to the epoch) stores
    `scalar_value` as a sequence within the same `tag`. In addition to graphs and
    scalars, `SummaryWriter` can add images, tensors, histograms, and embeddings,
    among others.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标量值都有一个唯一的 `tag`（除了代码中的两个标签外，我们还有 `tag='validation/loss'`）。请注意，`global_step`（等于
    epoch）将 `scalar_value` 存储为同一 `tag` 下的一个序列。除了图形和标量外，`SummaryWriter` 还可以添加图像、张量、直方图和嵌入等内容。
- en: This concludes our introduction to TB. Next, we’ll learn how to develop NN models
    for edge devices.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本文结束了我们对 TB 的介绍。接下来，我们将学习如何为边缘设备开发神经网络模型。
- en: Developing NN models for edge devices with TF Lite
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 TF Lite 开发边缘设备的神经网络模型
- en: 'TF Lite is a TF-derived set of tools that allows us to run models on mobile,
    embedded, and edge devices. Its versatility is part of TF’s appeal for industrial
    applications (as opposed to research applications, where PyTorch dominates). The
    key paradigm of TF Lite is that the models run on-device, contrary to client-server
    architecture, where the model is deployed on remote, more powerful, hardware.
    This organization has the following implications (both good and bad):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: TF Lite 是一个源自 TF 的工具集，使我们能够在移动设备、嵌入式设备和边缘设备上运行模型。其多功能性是 TF 在工业应用中受欢迎的原因之一（与
    PyTorch 主导的研究应用领域相对）。TF Lite 的核心范式是模型在设备上运行，而不是传统的客户端-服务器架构，其中模型部署在远程、更强大的硬件上。这种组织方式有以下影响（包括正面和负面）：
- en: '**Low-latency execution**: The lack of server-round trip significantly reduces
    the model inference time and allows us to run real-time applications.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低延迟执行**：缺少服务器的往返连接显著减少了模型推理时间，使我们能够运行实时应用程序。'
- en: '**Privacy**: The user data never leaves the device.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐私**：用户数据从不离开设备。'
- en: '**Internet connectivity**: Internet connectivity is not required.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互联网连接**：不需要互联网连接。'
- en: '`.tflite` file extension. Besides its small size, it allows us to access data
    directly without parsing/unpacking it first.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.tflite` 文件扩展名。除了文件体积小，它还允许我们直接访问数据，而无需首先解析/解包它。'
- en: 'TF Lite models support a subset of the TF Core operations and allow us to define
    custom ones:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: TF Lite 模型支持 TF Core 操作的子集，并允许我们定义自定义操作：
- en: '**Low power consumption**: The devices often run on battery.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低功耗**：这些设备通常使用电池供电。'
- en: '**Divergent training and inference**: NN training is a lot more computationally
    intensive compared to inference. Because of this, the model training runs on a
    different, more powerful, piece of hardware than the actual devices, where the
    models will run inference.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练与推理的差异**：神经网络训练比推理需要更多的计算资源。因此，模型训练通常在比实际设备更强大的硬件上进行，而这些设备用于推理。'
- en: 'In addition, TF Lite has the following key features:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，TF Lite 具有以下关键特性：
- en: Multi-platform and multi-language support, including Android (Java), iOS (Objective-C
    and Swift) devices, web (JavaScript), and Python for all other environments. Google
    provides a TF Lite wrapper API called **MediaPipe Solutions** ([https://developers.google.com/mediapipe](https://developers.google.com/mediapipe),
    [https://github.com/google/mediapipe/](https://github.com/google/mediapipe/)),
    which supersedes the previous TF Lite API.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持多平台和多语言，包括 Android（Java）、iOS（Objective-C 和 Swift）设备、Web（JavaScript）以及其他环境的
    Python。谷歌提供了一个名为 **MediaPipe Solutions** 的 TF Lite 封装 API ([https://developers.google.com/mediapipe](https://developers.google.com/mediapipe),
    [https://github.com/google/mediapipe/](https://github.com/google/mediapipe/))，它取代了之前的
    TF Lite API。
- en: Optimized for performance.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能优化。
- en: It has end-to-end solution pipelines. TF Lite is oriented toward practical applications,
    rather than research. Because of this, it includes different pipelines for common
    ML tasks such as image classification, object detection, text classification,
    and question answering among others. The computer vision pipelines use modified
    versions of EfficientNet or MobileNet ([*Chapter 4*](B19627_04.xhtml#_idTextAnchor107)),
    and the natural language processing pipelines use BERT-based ([*Chapter* *7*](B19627_07.xhtml#_idTextAnchor202))
    models.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有端到端的解决方案管道。TF Lite 主要面向实际应用，而非研究。因此，它包含了用于常见机器学习任务的不同管道，如图像分类、物体检测、文本分类和问答等。计算机视觉管道使用了修改版的
    EfficientNet 或 MobileNet（[*第 4 章*](B19627_04.xhtml#_idTextAnchor107)），自然语言处理管道则使用基于
    BERT 的（[*第 7 章*](B19627_07.xhtml#_idTextAnchor202)）模型。
- en: 'So, how does TF Lite model development work? First, we’ll select a model in
    one of the following ways:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，TF Lite 模型开发是如何工作的呢？首先，我们将通过以下方式选择一个模型：
- en: An existing pre-trained `.tflite` model ([https://tfhub.dev/s?deployment-format=lite](https://tfhub.dev/s?deployment-format=lite)).
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个已存在的预训练 `.tflite` 模型（[https://tfhub.dev/s?deployment-format=lite](https://tfhub.dev/s?deployment-format=lite)）。
- en: Use `.tflite` model with a custom training dataset. Model Maker only works with
    Python.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `.tflite` 模型和自定义训练数据集。Model Maker 仅适用于 Python。
- en: Convert a full-fledged TF model into `.``tflite` format.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一个完整的 TF 模型转换为 `.tflite` 格式。
- en: TFLite model metadata
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: TFLite 模型元数据
- en: 'The `.tflite` models may include optional metadata with three components:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`.tflite` 模型可能包含三个组件的可选元数据：'
- en: '-- **Human-readable part**: Provides additional information for the model.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: -- **可读部分**：为模型提供额外的信息。
- en: '-- **Input information**: Describes the input data format and the necessary
    pre-processing steps'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: -- **输入信息**：描述输入数据格式以及必要的预处理步骤。
- en: '-- **Output information**: Describes the output data format and the necessary
    post-processing steps.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: -- **输出信息**：描述输出数据格式以及必要的后处理步骤。
- en: The last two parts can be leveraged by code generators (for example, Android
    code generator) to create ready-to-use model wrappers in the target platform.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两部分可以被代码生成器（例如，Android 代码生成器）利用，以在目标平台上创建现成的模型包装器。
- en: 'Next, let’s see how to use Model Maker to train a `.tflite` model and then
    use it to classify images. We’re only going to show relevant parts of the code,
    but the full example is available as a Jupyter Notebook in this book’s GitHub
    repository. Let’s start:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看如何使用 Model Maker 训练一个 `.tflite` 模型，然后用它来分类图像。我们只会展示相关的代码部分，但完整的示例可以在本书的
    GitHub 仓库中的 Jupyter Notebook 中找到。让我们开始吧：
- en: 'First, we’ll create training and validation datasets:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将创建训练和验证数据集：
- en: '[PRE21]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, `dataset_path` is a path to the Flowers dataset ([https://www.tensorflow.org/datasets/catalog/tf_flowers](https://www.tensorflow.org/datasets/catalog/tf_flowers)),
    which contains 3,670 RGB low-resolution images of flowers, distributed in five
    classes (one subfolder per class). `data.split(0.9)` splits the dataset (instances
    of `image_classifier.Dataset`) into `train_data` (90% of the images) and `validation_data`
    (10% of the images) parts.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`dataset_path` 是 Flowers 数据集的路径（[https://www.tensorflow.org/datasets/catalog/tf_flowers](https://www.tensorflow.org/datasets/catalog/tf_flowers)），该数据集包含了
    3,670 张低分辨率的 RGB 花卉图片，分为五个类别（每个类别一个子文件夹）。`data.split(0.9)` 将数据集（`image_classifier.Dataset`
    实例）拆分为 `train_data`（90%的图片）和 `validation_data`（10%的图片）两部分。
- en: 'Next, we’ll define the training hyperparameters – train for three epochs with
    a mini-batch size of 16 and export the trained model in the `export_dir` folder
    (other parameters are available as well):'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义训练超参数——训练三轮，使用 mini-batch 大小为 16，并将训练好的模型导出到 `export_dir` 文件夹（也可以使用其他参数）：
- en: '[PRE22]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we’ll define the model parameters (we’ll use `EfficientNet`):'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将定义模型参数（我们将使用 `EfficientNet`）：
- en: '[PRE23]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we’ll create a new model and we’ll run the training:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将创建一个新模型并开始训练：
- en: '[PRE24]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This model achieves around 92% accuracy in three epochs. The training process
    creates a TB-compatible log file, so we’ll be able to track the progress with
    TB (available in the Jupyter Notebook).
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个模型在三轮训练中达到了大约 92% 的准确率。训练过程会创建一个与 TB 兼容的日志文件，因此我们可以通过 TB 跟踪进度（在 Jupyter Notebook
    中可用）。
- en: 'Next, we’ll export the model in `.tflite` format for the next phase of our
    example:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将导出模型为 `.tflite` 格式，进入示例的下一个阶段：
- en: '[PRE25]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now that we have a trained model, we can use it to classify images. We’re going
    to use the `MediaPipe` Python API (which is different than Model Maker):'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有了一个训练好的模型，可以用它来分类图像。我们将使用 `MediaPipe` Python API（与 Model Maker 不同）：
- en: '[PRE26]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here, `classifier` is the pre-trained model, `generic_options` contains the
    file path to the `.tflite` model, and `cls_options` contains classification-specific
    options (we use the default values).
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`classifier`是预训练模型，`generic_options`包含`.tflite`模型的文件路径，而`cls_options`包含特定于分类的选项（我们使用默认值）。
- en: 'We’ll load five random flower images (one for each flower class, listed in
    `labels`) in a list called `image_paths` (not displayed here). We’ll classify
    each image, and we’ll compare its predicted label to the real one:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将加载五张随机的花卉图像（每个花卉类别一张，如`labels`中所列），并将它们存储在一个名为`image_paths`的列表中（这里不显示）。我们将对每张图像进行分类，并将其预测标签与真实标签进行比较：
- en: '[PRE27]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Predictably, the model classifies all images correctly.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以预见，模型能够正确分类所有图像。
- en: Next, we’ll learn how to optimize the training with mixed-precision computations.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何使用混合精度计算来优化训练过程。
- en: Mixed-precision training with PyTorch
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch进行混合精度训练
- en: 'We discussed mixed-precision training in the context of LLMs in [*Chapter 8*](B19627_08.xhtml#_idTextAnchor220).
    In this section, we’ll see how to use it in practice with PyTorch. Once again,
    we’ll use the transfer learning PyTorch example from [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146)
    as a base for our implementation. All the code modifications are concentrated
    in the `train_model` function. We’ll only include `train_model` here, but the
    full example is available as a Jupyter Notebook in this book’s GitHub repository.
    The following is a shortened version of the function definition:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第8章*](B19627_08.xhtml#_idTextAnchor220)中讨论了LLM的混合精度训练。在这一节中，我们将展示如何在实践中使用PyTorch来实现它。我们将再次使用[*第5章*](B19627_05.xhtml#_idTextAnchor146)中的转移学习PyTorch示例作为实现的基础。所有的代码修改都集中在`train_model`函数中。这里我们只包括`train_model`，但完整的示例可以在本书的GitHub仓库中的Jupyter
    Notebook中找到。以下是该函数定义的简化版本：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We use a combination of two separate and unrelated mechanisms for mixed-precision
    training:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两种独立且不相关的机制来进行混合精度训练：
- en: '`torch.autocast`: This acts as a context manager (or decorator) and allows
    a region of the code to run in mixed precision. `device_type` specifies the device
    that `autocast` applies to. `dtype` specifies the data type with which the CUDA
    operations work. The PyTorch documentation suggests only wrapping the forward
    and loss computation with `torch.autocast`. The backward operations automatically
    run with the same data type as the forward ones.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.autocast`：它充当上下文管理器（或装饰器），允许代码的某个区域在混合精度下运行。`device_type`指定`autocast`应用的设备，`dtype`指定CUDA操作使用的数据类型。PyTorch文档建议仅将前向传播和损失计算封装在`torch.autocast`中，反向传播操作会自动使用与前向传播相同的数据类型。'
- en: '`torch.cuda.amp.GradScaler`: When the forward pass uses `float16` precision
    operations, so does the backward pass, which computes the gradients. However,
    due to the lower precision, some gradient values will flush to zero. To prevent
    this, **gradient scaling** multiplies the NN’s loss by a scale factor and invokes
    a backward pass with the scaled value. Gradients flowing backward through the
    network are also scaled by the same factor. In this way, the entire backward pass
    uses a larger magnitude to prevent flushing to zero. Before the weight updates,
    the mechanism *unscales* the scaled gradient values, so the weight updates work
    with the actual values.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.cuda.amp.GradScaler`：当前向传播使用`float16`精度操作时，反向传播也使用相同的精度进行梯度计算。然而，由于较低的精度，一些梯度值可能会变为零。为了防止这种情况，**梯度缩放**将神经网络的损失乘以一个缩放因子，并用缩放后的值执行反向传播。反向传播时，梯度流也会按相同的因子进行缩放。通过这种方式，整个反向传播过程使用较大的数值，以防止梯度被清零。在权重更新之前，该机制会*反缩放*梯度值，以确保权重更新时使用的是实际的梯度值。'
- en: This concludes our introduction to the model development tools. Next, we’ll
    discuss some model deployment mechanisms.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对模型开发工具的介绍。接下来，我们将讨论一些模型部署机制。
- en: Exploring model deployment
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索模型部署
- en: In this section, we’ll discuss two basic model deployment examples. They’ll
    help you create simple, yet functional, proof-of-concept apps for your experiments.
    Let’s start.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将讨论两个基本的模型部署示例。这些示例将帮助你创建简单但功能完整的概念验证应用程序，用于你的实验。我们开始吧。
- en: Deploying NN models with Flask
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Flask部署神经网络模型
- en: In our first example, we’ll use Google Colab in combination with `prompt` parameter,
    generate an image with it, and return the image as a result.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个示例中，我们将结合使用Google Colab和`prompt`参数，生成图像，并将其作为结果返回。
- en: According to its home page, Flask is a lightweight `localhost`), we won’t be
    able to access it. To solve this, we’ll need `flask-ngrok` ([https://ngrok.com/docs/using-ngrok-with/flask/](https://ngrok.com/docs/using-ngrok-with/flask/)),
    which will expose the server to the outside world (you’ll need a free `ngrok`
    registration and authentication token to run this example).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其主页，Flask是一个轻量级的`localhost`，但我们无法访问它。为了解决这个问题，我们需要`flask-ngrok`（[https://ngrok.com/docs/using-ngrok-with/flask/](https://ngrok.com/docs/using-ngrok-with/flask/)），它将使服务器暴露给外界（你需要一个免费的`ngrok`注册和认证令牌来运行这个示例）。
- en: 'To satisfy all dependencies, we’ll need the `transformers`, `diffusers`, `accelerate`,
    and `flask-ngrok` packages. Let’s start:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足所有依赖项，我们需要安装`transformers`、`diffusers`、`accelerate`和`flask-ngrok`包。让我们开始吧：
- en: 'First, we’ll initialize the SD HF pipeline (`sd_pipe`) in the same way as we
    did in [*Chapter 9*](B19627_09.xhtml#_idTextAnchor236):'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将以与[*第9章*](B19627_09.xhtml#_idTextAnchor236)中相同的方式初始化SD HF管道（`sd_pipe`）：
- en: '[PRE29]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we’ll initialize our Flask `app`:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将初始化我们的Flask `app`：
- en: '[PRE30]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, `run_with_ngrok` indicates that the app will run with `ngrok`, but the
    actual `app` is not running yet (this will come at the end of this example). Since
    we don’t have access to Colab’s `localhost`, `ngrok` will make it possible to
    access it from our test client.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`run_with_ngrok`表示应用将使用`ngrok`运行，但实际的`app`尚未启动（这将在本示例的最后进行）。由于我们无法访问Colab的`localhost`，`ngrok`将使我们能够通过测试客户端访问它。
- en: 'Then, we’ll implement our `text-to-image` endpoint, which will process the
    prompts, which are coming in as web requests, and generate images based on them:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将实现我们的`text-to-image`端点，它将处理作为Web请求传入的提示，并基于它们生成图像：
- en: '[PRE31]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The endpoint’s name is `/text-to-image` and it will process both `POST` and
    `GET` requests (the processing pipeline is the same). The function will parse
    the textual `prompt` parameter and it will feed it to `sd_pipe` to generate an
    `image` parameter (in the same way as in the [*Chapter 9*](B19627_09.xhtml#_idTextAnchor236)
    example). Finally, the `send_file` function will return the result of `image`
    to the client.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该端点的名称是`/text-to-image`，它将处理`POST`和`GET`请求（处理流程是相同的）。该函数将解析文本的`prompt`参数，并将其传递给`sd_pipe`以生成`image`参数（与[*第9章*](B19627_09.xhtml#_idTextAnchor236)示例中的方式相同）。最后，`send_file`函数将把`image`的结果返回给客户端。
- en: We can now start the Flask app with the `app.run()` command. It will initialize
    the Flask development server so that our endpoint will be ready to process requests.
    In addition, `ngrok` will expose the app to the outside world with a URL of the
    [http://RANDOM-SEQUENCE.ngrok.io/](http://RANDOM-SEQUENCE.ngrok.io/) type.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以通过`app.run()`命令启动Flask应用。它将初始化Flask开发服务器，使我们的端点准备好处理请求。此外，`ngrok`将通过[http://RANDOM-SEQUENCE.ngrok.io/](http://RANDOM-SEQUENCE.ngrok.io/)类型的URL将应用暴露给外界。
- en: 'We can use this URL to initiate a test request to the `text-to-image` endpoint
    (this is outside the Colab notebook):'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用这个URL发起对`text-to-image`端点的测试请求（这在Colab笔记本外进行）：
- en: '[PRE32]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can display the image with the following code:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码来显示图像：
- en: '[PRE33]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This concludes our REST API example. Next, we’ll deploy a model in a web environment
    with a UI.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们的REST API示例。接下来，我们将在Web环境中部署一个带有UI的模型。
- en: Building ML web apps with Gradio
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Gradio构建机器学习Web应用
- en: Gradio ([https://www.gradio.app/](https://www.gradio.app/)) is an open source
    Python library that allows us to build interactive web-based demos for our ML
    models. HF Spaces ([https://huggingface.co/spaces](https://huggingface.co/spaces))
    supports hosting Gradio apps. So, we can build a Gradio app on top of the HF infrastructure,
    which includes not only hosting but also has access to all available HF models
    ([https://huggingface.co/models](https://huggingface.co/models)).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Gradio（[https://www.gradio.app/](https://www.gradio.app/)）是一个开源的Python库，允许我们为ML模型构建互动式Web演示。HF
    Spaces（[https://huggingface.co/spaces](https://huggingface.co/spaces)）支持托管Gradio应用。因此，我们可以在HF基础设施上构建一个Gradio应用，它不仅包括托管，还可以访问所有可用的HF模型（[https://huggingface.co/models](https://huggingface.co/models)）。
- en: We can create an HF space at [https://huggingface.co/new-space](https://huggingface.co/new-space).
    The space has a name (which will be its URL as well), a license, and an SDK. At
    the time of writing, HF Spaces supports Streamlit-based ([https://streamlit.io/](https://streamlit.io/)),
    Gradio-based, and Static instances. However, you can also deploy custom Docker
    containers for more flexibility.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 [https://huggingface.co/new-space](https://huggingface.co/new-space) 创建一个
    HF 空间。这个空间有一个名字（它也将成为其 URL）、一个许可证和一个 SDK。在写作时，HF Spaces 支持基于 Streamlit 的（[https://streamlit.io/](https://streamlit.io/)）、基于
    Gradio 的和静态实例。然而，你也可以部署自定义的 Docker 容器以获得更多灵活性。
- en: Each new HF space has an associated Git repository. For example, the space of
    this example is located at [https://huggingface.co/spaces/ivan-vasilev/gradio-demo](https://huggingface.co/spaces/ivan-vasilev/gradio-demo),
    which is also the URL of its corresponding Git repository. The Gradio-based space
    expects a Python module called `app.py` in its root (in our case, the whole example
    will reside in `app.py`) and a `requirements.txt` file. Every time you push changes
    to the repository, the app will automatically pick them up and restart itself.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 每个新的 HF 空间都有一个关联的 Git 仓库。例如，本文示例的空间位于 [https://huggingface.co/spaces/ivan-vasilev/gradio-demo](https://huggingface.co/spaces/ivan-vasilev/gradio-demo)，这也是其相应
    Git 仓库的 URL。基于 Gradio 的空间期望在其根目录中有一个名为 `app.py` 的 Python 模块（在我们的例子中，整个示例将驻留在 `app.py`
    中）和一个 `requirements.txt` 文件。每次你推送更改到仓库时，应用程序将自动接收这些更改并重新启动。
- en: Note
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To replicate this example, you’ll need an HF account. HF Spaces has different
    hardware tiers. The basic one is free, but this particular example requires the
    GPU-enabled tier, which has an hourly cost. Therefore, if you want to run this
    example, you can duplicate it in your own account and enable the GPU tier.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 若要复制此示例，你需要一个 HF 账户。HF Spaces 提供不同的硬件等级。基本版是免费的，但此特定示例需要启用 GPU 的等级，这会按小时收费。因此，如果你想运行此示例，可以将其复制到自己的账户中并启用
    GPU 等级。
- en: 'Gradio starts with a central high-level class called `gradio.Interface`. Its
    constructor takes three main parameters:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Gradio 从一个名为 `gradio.Interface` 的中央高级类开始。其构造函数接受三个主要参数：
- en: '`fn`: The main function, which will process the inputs and return outputs.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fn`：主函数，将处理输入并返回输出。'
- en: '`inputs`: One or more Gradio input components. These could be textual inputs,
    file uploads, or combo boxes, among others. You can specify the component as a
    class instance or via its string label. The number of inputs should match the
    number of `fn` parameters.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inputs`：一个或多个 Gradio 输入组件。这些可以是文本输入、文件上传或组合框等。你可以将组件指定为类实例或通过其字符串标签。输入的数量应与
    `fn` 参数的数量匹配。'
- en: '`outputs`: One or more Gradio components, which will represent the result of
    the execution of `fn`. The number of outputs should match the number of values
    returned by `fn`.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`outputs`：一个或多个 Gradio 组件，表示 `fn` 执行结果。输出的数量应与 `fn` 返回的值的数量匹配。'
- en: Gradio will automatically instantiate and arrange the UI components based on
    the `input` and `output` parameters.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: Gradio 将根据 `input` 和 `output` 参数自动实例化并排列 UI 组件。
- en: 'Next, we’ll implement our example. We’ll use the same text-to-image SD scenario
    that we used in the *Deploying NN models with Flask* section. To avoid duplication,
    we’ll assume that the `sd_pipe` pipeline has already been initialized. Let’s start:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现我们的示例。我们将使用与 *使用 Flask 部署神经网络模型* 部分相同的文本到图像的 SD 场景。为了避免重复，我们假设 `sd_pipe`
    流水线已被初始化。现在开始：
- en: 'First, we’ll implement the `generate_image` function, which uses `prompt` to
    synthesize an image in a total of `inf_steps` steps:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将实现 `generate_image` 函数，该函数使用 `prompt` 在 `inf_steps` 步骤内合成一张图像：
- en: '[PRE34]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we’ll initialize the `gradio.Interface` class:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将初始化 `gradio.Interface` 类：
- en: '[PRE35]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As we discussed, the `inputs` and `outputs` `gr.Interface` parameters match
    the input/output signature of the `generate_image` function.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如我们所讨论的，`inputs` 和 `outputs` `gr.Interface` 参数与 `generate_image` 函数的输入/输出签名相匹配。
- en: 'Finally, we can run the app with the `interface.launch()` command. Here is
    what the responsive UI of the app looks like:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用 `interface.launch()` 命令运行应用程序。以下是该应用程序响应式 UI 的样子：
- en: '![Figure 10.4 – The SD Gradio app’s responsive UI, hosted on HF Spaces. Top:
    input components; bottom: generated image](img/B19627_10_4.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4 – SD Gradio 应用程序的响应式 UI，托管在 HF Spaces 上。上方：输入组件；下方：生成的图像](img/B19627_10_4.jpg)'
- en: 'Figure 10.4 – The SD Gradio app’s responsive UI, hosted on HF Spaces. Top:
    input components; bottom: generated image'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – SD Gradio 应用程序的响应式 UI，托管在 HF Spaces 上。上方：输入组件；下方：生成的图像
- en: This concludes our introduction to Gradio and model deployment.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容总结了我们对Gradio和模型部署的介绍。
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we outlined three major components of the ML development life
    cycle – training dataset creation, model development, and model deployment. We
    focused on the latter two, starting with development. First, we discussed the
    popularity of the foundational NN frameworks. Then, we focused on several model
    development topics – the ONNX universal model representation format, the TB monitoring
    platform, the TF Lite mobile development library, and mixed precision PyTorch
    training. Next, we discussed two basic scenarios for model deployment – a REST
    service as a Flask app and an interactive web app with Gradio.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们概述了机器学习开发生命周期的三个主要组成部分——训练数据集的创建、模型开发和模型部署。我们主要关注了后两者，从开发开始。首先，我们讨论了基础神经网络框架的流行。接着，我们聚焦于几个模型开发主题——ONNX通用模型表示格式、TB监控平台、TF
    Lite移动开发库，以及混合精度的PyTorch训练。然后，我们讨论了两种基本的模型部署场景——作为Flask应用的REST服务和使用Gradio的交互式Web应用。
- en: This concludes this chapter and this book. I hope you’ve enjoyed the journey!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这章以及本书到此结束。希望你享受这段旅程！
