- en: Implementing an Intelligent Agent for Optimal Control using Deep Q-Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度 Q-learning 实现智能代理进行最优控制
- en: In the previous chapter, we implemented an intelligent agent that used Q-learning
    to solve the Mountain Car problem from scratch in about seven minutes on a dual-core
    laptop CPU. In this chapter, we will implement an advanced version of Q-learning
    called deep Q-learning, which can be used to solve several discrete control problems
    that are much more complex than the Mountain Car problem. Discrete control problems
    are (sequential) decision-making problems in which the action space is discretized
    into a finite number of values. In the previous chapter, the learning agent used
    a 2-dimensional state-space vector as the input, which contained the information
    about the position and velocity of the cart to take optimal control actions. In
    this chapter, we will see how we can implement a learning agent that takes (the
    on-screen) visual image as input and learns to take optimal control actions. This
    is close to how we would approach the problem, isn't it? We humans do not calculate
    the location and velocity of an object to decide what to do next. We simply observe
    what is going on and then learn to take actions that improve over time, eventually
    solving the problem completely.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们实现了一个使用 Q-learning 的智能代理，该代理在约七分钟内使用双核笔记本 CPU 从零开始解决了 Mountain Car 问题。在这一章中，我们将实现
    Q-learning 的一个高级版本，叫做深度 Q-learning，它可以用来解决比 Mountain Car 问题复杂得多的多个离散控制问题。离散控制问题是（顺序）决策问题，在这些问题中，动作空间被离散化为有限的值。在上一章中，学习代理使用了一个二维的状态空间向量作为输入，这个向量包含了小车的位置和速度信息，以便采取最佳控制行动。在这一章中，我们将看到如何实现一个学习代理，该代理以（屏幕上的）视觉图像作为输入，并学习采取最佳控制行动。这与我们解决问题的方法非常接近，不是吗？我们人类不会计算物体的位置和速度来决定接下来做什么。我们只是观察发生了什么，然后学习采取随时间改进的行动，最终完全解决问题。
- en: This chapter will guide you in how to progressively build a better agent by
    improving upon our Q-learning agent implementation step-by-step using recently
    published methods for stable Q-learning with deep neural network function approximation. By
    the end of this chapter, you will have learnt how to implement and train a deep
    Q-learning agent that observes the pixels on the screen and plays Atari games
    using the Atari Gym environment and gets pretty good scores! We will also discuss
    how you can visualize and compare the performance of the agent as the learning
    progresses. You will see how the same agent algorithm can be trained on several
    different Atari games and that the agent is still able to learn to play the games
    well. If you cannot wait to see something in action or if you like to see and
    get a glimpse of what you will be developing before diving in, you can check out
    the code for this chapter under the `ch6` folder from the book's code repository
    and try out the pre-trained agents on several Atari games! Instructions on how
    to run the pre-trained agents are available in the `ch6/README.md` file.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将指导你如何通过逐步改进我们的 Q-learning 代理实现，使用最近发布的稳定 Q-learning 深度神经网络函数近似方法，逐步构建一个更好的代理。在本章结束时，你将学会如何实现并训练一个深度
    Q-learning 代理，该代理通过观察屏幕上的像素并使用 Atari Gym 环境玩 Atari 游戏，获得相当不错的分数！我们还将讨论如何在学习过程中可视化和比较代理的表现。你将看到相同的代理算法如何在多个不同的
    Atari 游戏上进行训练，并且该代理仍然能够学会很好地玩这些游戏。如果你迫不及待地想看到实际效果，或者你喜欢在深入之前先大致了解你将开发的内容，可以查看本章代码仓库中
    `ch6` 文件夹下的代码，并在多个 Atari 游戏上尝试预训练的代理！有关如何运行预训练代理的说明，可以在 `ch6/README.md` 文件中找到。
- en: 'This chapter has a lot of technical details to equip you with enough background
    and knowledge for you to understand the step-by-step process of improving the
    basic Q-learning algorithm and building a much more capable and intelligent agent
    based on deep Q-learning, along with several modules and tools needed to train
    and test the agent in a systematic manner. The following is an outline of the
    higher-level topics that will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含大量的技术细节，旨在为你提供足够的背景和知识，使你能够理解逐步改进基本 Q-learning 算法的过程，并基于深度 Q-learning 构建一个更强大、更智能的代理，同时提供多个训练和测试代理所需的模块和工具。以下是本章将涵盖的高级主题的概述：
- en: 'Various methods to improve the Q-learning agent, including the following:'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进 Q-learning 代理的各种方法，包括以下内容：
- en: Neural network approximation of action-value functions
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络近似动作-价值函数
- en: Experience replay
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经验回放
- en: Exploration schedules
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索调度
- en: Implementing deep convolutional neural networks using PyTorch for action-value
    function approximation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PyTorch 实现深度卷积神经网络进行动作-价值函数逼近
- en: Stabilizing deep Q-networks using target networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用目标网络稳定深度 Q 网络
- en: Logging and monitoring learning performance of PyTorch agents using TensorBoard
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorBoard 记录和监控 PyTorch 代理的学习性能
- en: Managing parameters and configurations
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理参数和配置
- en: Atari Gym environment
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atari Gym 环境
- en: Training deep Q-learners to play Atari games
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练深度 Q 学习者玩 Atari 游戏
- en: Let's get started with the first topic and see how we can start from where we
    left off in the previous chapter and continue making progress toward a more capable
    and intelligent agent.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个主题开始，看看如何从上一章的内容接续开始，继续朝着更强大、更智能的代理迈进。
- en: Improving the Q-learning agent
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进 Q 学习代理
- en: In the last chapter, we revisited the Q-learning algorithm and implemented the `Q_Learner` class.
    For the Mountain car environment, we used a multi-dimensional array of shape 51x51x3
    to represent the action-value function,![](img/00130.jpeg). Note that we had discretized
    the state space to a fixed number of bins given by the `NUM_DISCRETE_BINS` configuration
    parameter (we used 50) . We essentially quantized or approximated the observation
    with a low-dimensional, discrete representation to reduce the number of possible
    elements in the n-dimensional array. With such a discretization of the observation/state
    space, we restricted the possible location of the car to a fixed set of 50 locations
    and the possible velocity of the car to a fixed set of 50 values. Any other location
    or velocity value would be approximated to one of those fixed set of values. Therefore,
    it is possible that the agent receives the same value for the position when the
    car was actually at different positions. For some environments, that can be an
    issue. The agent may not learn enough to distinguish between falling off a cliff
    versus standing just on the edge so as to leap forward. In the next section, we
    will look into how we can use a more powerful function approximator to represent
    the action-value function instead of a simple n-dimensional array/table that has
    its limitations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们回顾了 Q 学习算法，并实现了 `Q_Learner` 类。对于 Mountain car 环境，我们使用了形状为 51x51x3 的多维数组来表示动作-价值函数，![](img/00130.jpeg)。请注意，我们已经将状态空间离散化为固定数量的区间，这个数量由
    `NUM_DISCRETE_BINS` 配置参数给出（我们使用了 50）。我们本质上是对观察进行了量化或近似，用低维离散表示来减少 n 维数组中可能的元素数量。通过对观察/状态空间的这种离散化，我们将小车的可能位置限制为
    50 个固定位置，速度也限制为 50 个固定值。任何其他位置或速度值都会被近似为这些固定值之一。因此，代理可能会收到相同的位置值，即使小车实际上位于不同的位置。对于某些环境，这可能是一个问题。代理可能无法学会区分从悬崖掉下去和仅站在悬崖边缘以便向前跳跃。在下一节中，我们将研究如何使用更强大的函数逼近器来表示动作-价值函数，而不是使用简单的
    n 维数组/表格，因为它有一定的局限性。
- en: Using neural networks to approximate Q-functions
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络逼近 Q 函数
- en: Neural networks are shown to be effective as universal function approximators.
    In fact, there is a universal approximation theorem that states that a single
    hidden layer feed-forward neural network can approximate any continuous function
    that is closed and bounded in ![](img/00131.jpeg). It basically means that even
    simple (shallow) neural networks can approximate several functions. Doesn't it
    feel too good to be true that you can use a simple neural network with a fixed
    number of weights/parameters to approximate practically any function? It is actually
    true, except for one note that prevents it from being used practically anywhere
    and everywhere. Though a single hidden layer neural network can approximate any
    function with a finite set of parameters, we do not have a universally guaranteed
    way of *learning* those parameters that can best represent any function. You will
    see that researchers have been able to use neural networks to approximate several
    sophisticated and useful functions. Today, most of the intelligence that is built
    into the ubiquitous smartphones are powered by (heavily optimized) neural networks.
    Several best-performing systems that organize your photos into albums automatically
    based on people, places, and the context in the photos, systems that recognize
    your face and voice, or systems that automatically compose email replies for you
    are all powered by neural networks. Even the state-of-the-art techniques that
    generate human-like realistic voices that you hear from voice assistants such
    as Google Assistant, are powered by neural networks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络被证明是有效的通用函数逼近器。事实上，有一个通用逼近定理，表明一个单隐层前馈神经网络可以逼近任何闭合且有界的连续函数 ![](img/00131.jpeg)。这基本上意味着即使是简单（浅层）神经网络也可以逼近多个函数。难道不觉得用一个固定数量的权重/参数的简单神经网络来逼近几乎任何函数，听起来太好了不真实吗？但这确实是真的，唯一需要注意的地方是，它并不能在任何地方和所有地方都能实际应用。虽然单隐层神经网络可以用有限的参数集逼近任何函数，但我们并没有一种普遍保证的*学习*这些参数的方法，以便最好地表示任何函数。你会看到，研究人员已经能够使用神经网络来逼近多个复杂且有用的函数。如今，几乎所有智能手机内置的智能功能都由（经过大量优化的）神经网络驱动。许多表现最好的系统，比如自动根据人物、地点和照片中的上下文将照片分类的系统，识别你面部和声音的系统，或者为你自动撰写电子邮件回复的系统，都是由神经网络提供动力的。即使是生成类似人类真实语音的最先进技术，比如你在
    Google Assistant 等语音助手中听到的声音，也是由神经网络驱动的。
- en: Google Assistant currently uses WaveNet and WaveNet2 developed by Deepmind for
    **Text-To-Speech** (**TTS**) synthesis, which is shown to be much more realistic
    than any other TTS system that has been developed so far.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Google Assistant 当前使用由 Deepmind 开发的 WaveNet 和 WaveNet2 进行 **文本到语音** (**TTS**)
    合成，研究表明，这比目前开发的任何其他 TTS 系统都更加逼真。
- en: I hope that motivates you enough to use a neural network to approximate the
    Q-function! In this section, we will start by approximating the Q-function with
    a shallow (not deep) single-hidden layer neural network and use it to solve the
    famous Cart Pole problem. Though neural networks are powerful function approximators,
    we will see that it is not trivial to train even a single layer neural network
    to approximate Q-functions for reinforcement learning problems. We will look at
    some ways to improve Q-learning with neural network approximation and, in the
    later sections of this chapter, we will look at how we can use deep neural networks
    with much more representation power to approximate the Q-function.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这足以激励你使用神经网络来逼近 Q 函数！在本节中，我们将从用一个浅层（非深层）单隐层神经网络来逼近 Q 函数开始，并利用它来解决著名的倒立摆问题。尽管神经网络是强大的函数逼近器，我们将看到，即便是单隐层神经网络，训练它来逼近强化学习问题中的
    Q 函数也并非易事。我们将探讨一些利用神经网络逼近改进 Q 学习的方法，并且在本章的后续部分，我们将研究如何使用具有更强表示能力的深度神经网络来逼近 Q 函数。
- en: 'Let''s get started with the neural network approximation by first revisiting
    the `Q_Learner` class''s `__init__(...)` method that we implemented in the previous
    chapter:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过重新审视前一章中实现的 `Q_Learner` 类的 `__init__(...)` 方法来开始神经网络逼近：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, the line in bold font is where we initialize the Q-function
    as a multi-dimensional NumPy array. In the following sections, we will see how
    we can replace the NumPy array representation with a more powerful neural network
    representation.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，粗体字体的那一行是我们将 Q 函数初始化为一个多维的 NumPy 数组。在接下来的部分，我们将看到如何将 NumPy 数组表示替换为更强大的神经网络表示。
- en: Implementing a shallow Q-network using PyTorch
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch 实现一个浅层 Q 网络
- en: In this section, we will start implementing a simple neural network using PyTorch's
    neural network module and then look at how we can use that to replace the multi-dimensional
    array-based Q action-value table-like function.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将开始使用 PyTorch 的神经网络模块实现一个简单的神经网络，然后看看如何使用它来替代基于多维数组的 Q 行为值表函数。
- en: 'Let''s start with the neural network implementation. The following code illustrates
    how you can implement a **Single Layer Perceptron** (**SLP**) using PyTorch:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从神经网络的实现开始。以下代码演示了如何使用 PyTorch 实现一个 **单层感知机** (**SLP**)：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The SLP class implements a single layer neural network with 40 hidden units
    between the input and the output layer using the `torch.nn.Linear`class, and uses
    the **Rectified Linear Unit** (**ReLU** or **relu**) as the activation function.
    This code is available as `ch6/function_approximator/perceptron.py` in this book's
    code repository. The number 40 is nothing special, so feel free to vary the number
    of hidden units in the neural network.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: SLP 类实现了一个单层神经网络，输入层和输出层之间有 40 个隐藏单元，使用 `torch.nn.Linear` 类，并使用 **修正线性单元** (**ReLU**
    或 **relu**) 作为激活函数。本书的代码库中提供了这段代码，路径为 `ch6/function_approximator/perceptron.py`。数字
    40 并没有特别的含义，你可以根据需要调整神经网络中的隐藏单元数量。
- en: Implementing the Shallow_Q_Learner
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现 Shallow_Q_Learner
- en: 'We can then modify the `Q_Learner` class to use this SLP to represent the Q-function.
    Note that we will have to modify the `Q_Learner` class `learn(...)` method as
    well to calculate the gradients of loss with respect to the weights of the SLP
    and backpropagate them so as to update and optimize the neural network''s weights
    to improve its Q-value representation to be close to the actual values. We''ll
    also slightly modify the `get_action(...)` method to get the Q-values with a forward
    pass through the neural network. The following code for the `Shallow_Q_Learner` class with
    the changes from the `Q_Learner` class implementation are shown in bold to make
    it easy for you to see the differences at a glance:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以修改 `Q_Learner` 类，使用这个 SLP 来表示 Q 函数。请注意，我们还需要修改 `Q_Learner` 类中的 `learn(...)`
    方法，以计算损失函数关于 SLP 权重的梯度，并进行反向传播，以更新和优化神经网络的权重，从而改进其 Q 值表示，使其更接近实际值。同时，我们还会稍微修改
    `get_action(...)` 方法，通过神经网络的前向传播来获得 Q 值。以下是带有 `Q_Learner` 类实现中变化的 `Shallow_Q_Learner`
    类代码，修改部分用 **粗体** 显示，以便你一眼看出差异：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `Shallow_Q_Learner` class implementation is discussed here just to make
    it easy for you to understand how a neural network-based Q-function approximation
    can be implemented to replace the traditional tabular Q-learning implementations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论了 `Shallow_Q_Learner` 类的实现，目的是让你更容易理解如何实现基于神经网络的 Q 函数逼近，进而替代传统的表格型 Q 学习实现。
- en: Solving the Cart Pole problem using a Shallow Q-Network
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Shallow Q-Network 解决 Cart Pole 问题
- en: 'In this section, we will implement a full training script to solve the Cart
    Pole problem using the Shallow `Q_Learner` class that we developed in the previous
    section:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将实现一个完整的训练脚本，使用我们在前一部分中开发的 Shallow `Q_Learner` 类来解决 Cart Pole 问题：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create a script named `shallow_Q_Learner.py` with the preceding code in the
    `ch6` folder and run it like so:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `ch6` 文件夹中创建一个名为 `shallow_Q_Learner.py` 的脚本，并像下面这样运行：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You will see the agent learning to balance the Pole on the Cart in the Gym's
    `CartPole-v0` environment. You should see the episode number, the number of steps
    the agent took before the episode ended, the episode reward the agent received,
    the mean episode reward that the agent has received, and also the best episode
    reward that the agent has received so far printed on the console. You can uncomment
    the `env.render()` line if you want to visually see the Cart Pole environment
    and how the agent is trying to learn and balance it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到代理在 Gym 的 `CartPole-v0` 环境中学习如何平衡杠杆。你应该能够看到控制台打印出的以下信息：当前回合数、代理在回合结束前所采取的步数、代理所获得的回合奖励、代理所获得的平均回合奖励，以及代理至今所获得的最佳回合奖励。如果你想直观地看到
    Cart Pole 环境，并了解代理如何尝试学习并保持平衡，你可以取消注释 `env.render()` 这一行代码。
- en: The `Shallow_Q_Learner` class implementation and the full training script shows
    how you can use a simple neural network to approximate the Q-function. It is not
    a good implementation to solve complex games like Atari. In the following subsequent
    sections, we will systematically improve their performance using new techniques.
    We will also implement a Deep Convolutional Q-Network that can take the raw screen
    image as the input and predict the Q-values that the agent can use to play various
    Atari games.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`Shallow_Q_Learner`类的实现和完整的训练脚本展示了如何使用简单的神经网络来逼近Q函数。这并不是解决复杂游戏（如Atari）的一个好实现。在接下来的几个部分中，我们将使用新的技术系统地改进它们的性能。我们还将实现一个深度卷积Q网络，该网络可以将原始的屏幕图像作为输入，并预测智能体可以用来玩各种Atari游戏的Q值。'
- en: You may notice that it takes a very long time for the agent to improve and finally
    be able to solve the problem. In the next section, we will implement the concept
    of experience replay to improve the performance.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，智能体需要很长时间才能改善并最终能够解决问题。在下一部分中，我们将实现经验回放的概念来提升性能。
- en: Experience replay
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经验回放
- en: In most environments, the information received by the agent is not **independent
    and identically distributed** (**i.i.d**). What this means is that the observation
    that the agent receives is strongly correlated with the previous observation it
    had received and the next observation it will receive. This is understandable
    because typically, the problems that the agent solves in typical reinforcement
    learning environments are sequential. Neural networks are shown to converge better
    if the samples are i.i.d.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数环境中，智能体接收到的信息并不是**独立同分布**（**i.i.d**）。这意味着智能体接收到的观察值与它之前接收到的观察值以及它将来接收到的观察值之间存在较强的相关性。这个现象是可以理解的，因为通常，智能体在典型的强化学习环境中所解决的问题是顺序性的。研究表明，如果样本是i.i.d，神经网络的收敛效果会更好。
- en: Experience replay also enables the reuse of the past experience of the agent.
    Neural network updates, especially with lower learning rates, require several
    back-propagation and optimization steps to converge to good values. Reusing the
    past experience data, especially in mini-batches to update the neural network,
    greatly helps with the convergence of the Q-network which is close to the true
    action values.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 经验回放还使得智能体能够重用过去的经验。神经网络更新，尤其是在较低学习率的情况下，需要多次反向传播和优化步骤才能收敛到好的值。重用过去的经验数据，尤其是通过小批量来更新神经网络，极大地帮助了Q网络的收敛，Q网络的结果接近真实的动作值。
- en: Implementing the experience memory
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现经验记忆
- en: 'Let''s implement an experience memory class to store the experiences collected
    by the agent. Before that, let''s cement our understanding of what we mean by
    *experience*. In reinforcement learning where the problems are represented using
    **Markov Decision Processes** (**MDP**), which we discussed in [Chapter 2](part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12), *Reinforcement
    Learning and Deep Reinforcement Learning*, it is efficient to represent one experience
    as a data structure that consists of the observation at time step *t*, the action
    taken following that observation, the reward received for that action, and the
    next observation (or state) that the environment transitioned to due to the agent''s
    action. It is useful to also include the "done" Boolean value that signifies whether
    this particular next observation marked the end of the episode or not. Let''s
    use Python''s `namedtuple` from the collections library to represent such a data
    structure, as shown in the following code snippet:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个经验记忆类，用于存储智能体收集的经验。在此之前，让我们先明确一下我们所说的*经验*是什么意思。在强化学习中，问题通常通过**马尔可夫决策过程**（**MDP**）来表示，正如我们在[第2章](part0033.html#VF2I0-22c7fc7f93b64d07be225c00ead6ce12)《强化学习与深度强化学习》中所讨论的那样，表示一次经验为一个数据结构，其中包含了在时间步*t*的观察值、根据该观察值采取的行动、为该行动获得的奖励，以及由于智能体的行动而导致环境过渡到的下一个观察值（或状态）。包括“done”布尔值，表示该次观察是否标志着一轮实验的结束也是有用的。我们可以使用Python的`namedtuple`（来自collections库）来表示这种数据结构，如下方代码片段所示：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `namedtuple` data structure makes it convenient to access the elements using
    a name attribute (like 'obs', 'action', and so on) instead of a numerical index
    (like 0, 1 and so on).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`namedtuple`数据结构使得使用名称属性（如''obs''、''action''等）而不是数字索引（如0、1等）来访问元素变得更加方便。'
- en: We can now move on to implement the experience memory class using the experience
    data structure we just created. To figure out what methods we need to implement
    in the experience memory class, let's think about how we will be using it later.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续使用刚刚创建的经验数据结构来实现经验记忆类。为了弄清楚我们需要在经验记忆类中实现哪些方法，让我们考虑一下以后如何使用它。
- en: First, we want to be able to store new experiences in the experience memory
    that the agent collects. Then, we want to sample or retrieve experiences in batches
    from the experience memory when we want to replay to update the Q-function. So,
    essentially, we will need a method that can store new experiences and a method
    that can sample a single or a batch of experiences.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们希望能够存储智能体收集的新经验到经验记忆中。然后，当我们想要回放并更新Q函数时，我们希望能从经验记忆中按批次采样或提取经验。因此，基本上，我们将需要一个方法来存储新经验，以及一个可以采样单个或批量经验的方法。
- en: 'Let''s dive into the experience memory implementation, starting with the initialization
    method where we initialize the memory with the desired capacity, as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解经验记忆的实现，从初始化方法开始，我们用所需的容量初始化内存，如下所示：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `mem_idx` member variable will be used to point to the current writing head
    or the index location where we will be storing new experiences when they arrive.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`mem_idx`成员变量将用于指向当前的写入头或索引位置，我们将在该位置存储新到达的经验。'
- en: 'A "cyclic buffer" is also known by other names that you may have heard of:
    "circular buffer", "ring buffer", and "circular queue". They all represent the
    same underlying data structure that uses a ring-like fixed-size data representation.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: “循环缓冲区”也有其他名字，你可能听过：“环形缓冲区”、“环形队列”和“循环队列”。它们都代表相同的底层数据结构，使用类似环形的固定大小数据表示。
- en: 'Next, we''ll look at the `store` method''s implementation:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看`store`方法的实现：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Simple enough, right? We are storing the experience at `mem_idx`, like we discussed.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 足够简单，对吧？我们正在存储经验到`mem_idx`，正如我们所讨论的那样。
- en: 'The next code is our `sample` method implementation:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的代码是我们`sample`方法的实现：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the preceding code, we make use of Python''s random library to uniformly
    sample experiences from the experience memory at random. We will also implement
    a simple `get_size` helper method, which we will use to find out how many experiences
    are already stored in the experience memory:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们利用Python的random库从经验记忆中均匀地随机采样经验。我们还将实现一个简单的`get_size`辅助方法，用于查找经验记忆中已经存储了多少经验：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The full implementation of the experience memory class is available at `ch6/utils/experience_memory.py`,
    in this book's code repository.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 经验记忆类的完整实现可以在`ch6/utils/experience_memory.py`中找到，位于本书的代码仓库中。
- en: Next, we'll look at how we can replay experiences sampled from the experience
    memory to update the agent's Q-function.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何从经验记忆中回放采样的经验，以更新智能体的Q函数。
- en: Implementing the replay experience method for the Q-learner class
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Q-learner类的回放经验方法
- en: So, we have implemented a memory system for the agent to store its past experience
    using a neat cyclic buffer. In this section, we will look at how we can use the
    experience memory to replay experience in the Q-learner class.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经为智能体实现了一个内存系统，使用整洁的循环缓冲区来存储它的过去经验。在本节中，我们将探讨如何在Q-learner类中使用经验记忆来回放经验。
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the case of online learning methods like SARSA, the action value estimate
    was updated after every step of interaction between the agent and the environment.
    This way, the updates propagated information that the agent just experienced.
    If the agent does not experience something quite often, such updates may let the
    agent forget about those experiences and may result in bad performance when the
    agent encounters a similar situation in the future. This is undesirable, especially
    with neural networks which have many parameters (or weights) that needs to be
    adjusted to the right set of values. That is one of the main motivations behind
    using an experience memory and replaying the past experiences during updates to
    the Q action-value estimates. We will now implement the `learn_from_batch_experience` method
    that extends the `learn` method we implemented in the previous chapter to learn
    from a batch of experiences rather than from a single experience. The following
    is the method''s implementation:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像SARSA这样的在线学习方法，行动值估计在代理与环境交互的每一步之后都会更新。这种方式使得更新传播了代理刚刚经历的信息。如果代理不经常经历某些事物，这样的更新可能会导致代理忘记这些经历，当代理在未来遇到类似情况时可能表现不佳。这是不可取的，特别是对于具有许多参数（或权重）需要调整到正确值的神经网络来说。这是使用经验记忆并在更新Q行动值估计时重播过去经验的主要动机之一。我们现在将实现`learn_from_batch_experience`方法，扩展我们在上一章中实现的`learn`方法，以从一批经验中学习，而不是从单个经验中学习。以下是该方法的实现：
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The method receives a batch (or a mini-batch) of experience and first extracts
    the observation batches, action batches, reward batches, and the next observation
    batches separately in order to use them individually in the subsequent steps.
    The `done_batch` signifies for each experience whether or not the next observation
    is the end of an episode. We then calculate the **Temporal Difference** (**TD**)
    error with a max over action, which is the Q-learning target. Note that we multiply
    the second term in the `td_target` calculation with `~done_batch`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接收一批（或小批量）经验，并首先分别提取观察批次、动作批次、奖励批次和下一个观察批次，以便在随后的步骤中单独使用它们。`done_batch`表示每个经验的下一个观察是否是一集的结束。然后，我们计算最大化动作的**时间差分**（**TD**）误差，这是Q学习的目标。请注意，在`td_target`计算中，我们将第二项乘以`~done_batch`。
- en: This takes care of specifying a zero value for terminal states. So, if a particular
    `next_obs` in the `next_obs_batch` was terminal, the second term would become
    0, resulting in just `td_target = rewards_batch`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这负责为终止状态指定零值。因此，如果`next_obs_batch`中的特定`next_obs`是终止状态，则第二项将变为0，结果仅为`td_target
    = rewards_batch`。
- en: We then calculate a mean squared error between the `td_target` (target Q-value)
    and the Q-value predicted by the Q-network. We use this error as the guiding signal
    and back-propagate it to all the nodes in the neural network before making an
    optimization step to update the parameters/weights to minimize the error.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算`td_target`（目标Q值）与Q网络预测的Q值之间的均方误差。我们将此误差作为指导信号，并在进行优化步骤之前将其反向传播到神经网络中的所有节点，以更新参数/权重以最小化误差。
- en: Revisiting the epsilon-greedy action policy
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视ε-greedy行动策略
- en: In the previous chapter, we discussed the ![](img/00132.jpeg)-greedy action
    selection policy which takes the best action as per the agent's action-value estimate
    with a probability of 1-![](img/00133.jpeg) and takes a random action with a probability
    given by epsilon, ![](img/00134.jpeg). Epsilon is a hyperparameter that can be
    tuned based on the experiments to a good value. A higher value of ![](img/00135.jpeg) means
    that the agent's actions will be random and a lower value of ![](img/00136.jpeg) means
    that the agent's action will more likely exploit what it already knows about the
    environment and will not try to explore. Should I explore more by taking never/less
    tried actions? Or should I exploit what I already know and take the best action
    to my knowledge which may be limited? This is the exploration-exploitation dilemma
    that a reinforcement learning agent suffers from.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们讨论了贪心行动选择策略，它根据智能体的行动-价值估计，以1-的概率采取最佳行动， 以给定的epsilon概率采取随机行动。epsilon是一个可以根据实验调节的超参数。较高的epsilon值意味着智能体的行为将更加随机，而较低的epsilon值则意味着智能体更可能利用它已知的环境信息而不会尝试探索。我的目标是通过采取从未尝试或较少尝试的行动来进行更多探索，还是通过采取我已知的最佳行动来进行利用？这是强化学习智能体面临的探索-利用困境。
- en: Intuitively, it is helpful to have a very high value (the maximum is 1.0) for ![](img/00137.jpeg) during
    the initial stages of the agent's learning process so that the agent can explore
    the state space by taking mostly random actions. Once it has got enough experience
    and has gained a better understanding of the environment, lowering the ![](img/00138.jpeg) value
    will let the agent take actions based on what it believes to be the best actions
    more often. It will be useful to have a utility function that takes care of varying
    the ![](img/00139.jpeg) value, right? Let's implement such a function in the next
    section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，在智能体学习过程的初期，保持一个很高的值（最大为1.0）对于epsilon是有帮助的，这样智能体可以通过大多数随机行动来探索状态空间。一旦它积累了足够的经验并对环境有了更好的理解，降低epsilon值将使智能体更常基于它认为的最佳行动来采取行动。我们需要一个工具函数来处理epsilon值的变化，对吧？让我们在下一节实现这样的函数。
- en: Implementing an epsilon decay schedule
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现epsilon衰减调度
- en: 'We can decay (or decrease) the ![](img/00140.jpeg) value linearly (in the following
    left-hand side graph), exponentially (in the following right-hand side graph)
    or using some other decay schedule. Linear and exponential schedules are the most
    commonly used decay schedules for the exploration parameter ![](img/00141.jpeg):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以线性衰减（或减少）epsilon值（如下左侧图表），也可以采用指数衰减（如下右侧图表）或其他衰减方案。线性和指数衰减是探索参数epsilon最常用的衰减调度：
- en: '![](img/00142.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00142.jpeg)'
- en: In the preceding graphs, you can see how the epsilon (exploration) value varies
    with the different schedule schemes (linear on the left graph, exponential on
    the right graph). The decay schedule shown in the preceding graphs use an epsilon_max
    (start) value of 1, epsilon_min (final) value of 0.01 in the linear case, and
    exp(-10000/2000) in the exponential case, with both of them maintaining a constant
    value of epsilon_min after 10,000 episodes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，你可以看到epsilon（探索）值如何随着不同的调度方案变化（左图为线性，右图为指数）。前面图表中显示的衰减调度在线性情况下使用了epsilon_max（初始值）为1，epsilon_min（最终值）为0.01，在指数情况下使用了exp(-10000/2000)，在经过10,000个回合后，它们都保持一个常数值的epsilon_min。
- en: 'The following code implements the `LinearDecaySchedule`, which we will use
    for our `Deep_Q_Learning` agent implementation to play Atari games:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码实现了`LinearDecaySchedule`，我们将在`Deep_Q_Learning`智能体的实现中使用它来玩Atari游戏：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The preceding script is available at `ch6/utils/decay_schedule.py` in this book's
    code repository. If you run the script, you will see that the `main `function
    creates a linear decay schedule for epsilon and plots the value. You can experiment
    with different values of `MAX_NUM_EPISODES`, `MAX_STEPS_PER_EPISODE`, `epsilon_initial`,
    and `epsilon_final` to visually see how the epsilon values vary with the number
    of steps. In the next section, we will implement the `get_action(...)` method
    which implements the ![](img/00143.jpeg)-greedy action selection policy.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 上述脚本可以在本书的代码仓库中的 `ch6/utils/decay_schedule.py` 找到。如果运行该脚本，您将看到 `main` 函数为 epsilon
    创建一个线性衰减计划并绘制该值。您可以尝试不同的 `MAX_NUM_EPISODES`、`MAX_STEPS_PER_EPISODE`、`epsilon_initial`
    和 `epsilon_final` 值，以直观地查看 epsilon 值如何随步数变化。在下一节中，我们将实现 `get_action(...)` 方法，它实现了
    ![](img/00143.jpeg) - 贪婪动作选择策略。
- en: Implementing a deep Q-learning agent
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现深度 Q 学习智能体
- en: In this section, we will discuss how we can scale up our shallow Q-learner to
    a more sophisticated and powerful deep Q-learner-based agent that can learn to
    act based on raw visual image inputs, which we will use towards the end of this
    chapter to train agents that play Atari games well. Note that you can train this
    deep Q-learning agent in any learning environments with a discrete action space.
    The Atari game environments are one such interesting class of environments that
    we will use in this book.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何将我们的浅层 Q 学习器扩展为更复杂、更强大的深度 Q 学习器基础智能体，这样它可以基于原始视觉图像输入来学习行动，我们将在本章末尾用来训练能够玩
    Atari 游戏的智能体。请注意，您可以在任何具有离散动作空间的学习环境中训练这个深度 Q 学习智能体。Atari 游戏环境就是我们将在本书中使用的一个有趣的环境类别。
- en: We will start with a deep convolutional Q-network implementation and incorporate
    it into our Q-learner. Then, we will see how we can use the technique of target
    Q-networks to improve the stability of the deep Q-learner. We will then combine
    all the techniques we have discussed so far to put together the full implementation
    of our deep Q learning-based agent.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个深度卷积 Q 网络实现开始，并将其整合到我们的 Q 学习器中。接着，我们将看到如何使用目标 Q 网络技术来提高深度 Q 学习器的稳定性。然后，我们将结合目前为止讨论的所有技术，完整实现我们的深度
    Q 学习基础智能体。
- en: Implementing a deep convolutional Q-network in PyTorch
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 PyTorch 中实现深度卷积 Q 网络
- en: 'Let''s implement a 3-layer deep **Convolutional Neural Network** (**CNN**)
    that takes the Atari game screen pixels as the input and outputs the action-values
    for each of the possible actions for that particular game, which is defined in
    the OpenAI Gym environment. The following code is for the CNN class:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个 3 层深度 **卷积神经网络** (**CNN**)，该网络将 Atari 游戏的屏幕像素作为输入，输出每个可能动作的动作值，这些动作是在
    OpenAI Gym 环境中定义的。以下是 CNN 类的代码：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, it is easy to add more layers to the neural network. We could
    use a deeper network that has more than three layers, but it will come at the
    cost of requiring more compute power and time. In deep reinforcement learning,
    and especially in Q learning with function approximation, there are no proven
    convergence guarantees. We should therefore make sure that our agent's implementation
    is good enough to learn and make progress well before increasing the capacity
    of the Q /value-function representation by using a much deeper neural network.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，向神经网络中添加更多层非常简单。我们可以使用一个更深的网络，层数超过三层，但这样做的代价是需要更多的计算能力和时间。在深度强化学习，尤其是 Q
    学习与函数近似的情况下，并没有已证明的收敛保证。因此，在通过使用更深的神经网络来增加 Q /值函数表示的能力之前，我们应该确保智能体的实现已经足够好，能够在学习和进步上取得良好效果。
- en: Using the target Q-network to stabilize an agent's learning
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用目标 Q 网络来稳定智能体的学习
- en: A simple technique of freezing the Q-network for a fixed number of steps and
    then using that to generate the Q learning targets to update the parameters of
    the deep Q-network was shown to be considerably effective in reducing the oscillations
    and stabilize Q learning with neural network approximation. This technique is
    a relatively simpler one, but it turns out to be very helpful for stable learning.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 冻结 Q 网络固定步数，并利用该网络生成 Q 学习目标来更新深度 Q 网络的参数，这一简单技术已被证明在减少振荡和稳定 Q 学习与神经网络近似方面非常有效。这个技术相对简单，但实际上对于稳定学习非常有帮助。
- en: 'The implementation is going to be straightforward and simple. We have to make
    two changes or updates to our existing deep Q-learner class:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实现将非常直接和简单。我们需要对现有的深度 Q 学习器类进行两处修改或更新：
- en: Create a target Q-network and sync/update it with the original Q-network periodically
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建目标 Q 网络并定期与原始 Q 网络同步/更新
- en: Use the target Q-network to generate the Q-learning targets
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用目标 Q 网络生成 Q 学习目标。
- en: To compare how the agent performs with and without the target Q-network, you
    can use the parameter manager and the logging and visualization tools that we
    developed in the earlier sections of this chapter to visually verify the performance
    gain with the target Q-network enabled.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较代理在使用和不使用目标 Q 网络时的表现，你可以使用我们在本章前面部分开发的参数管理器、日志记录和可视化工具，来直观地验证启用目标 Q 网络后性能的提升。
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can then modify the `learn_from_batch_experience` method that we implemented
    earlier to use the target Q-network to create the Q-learning target. The following
    code snippet shows the changes in bold font from our first implementation:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以修改之前实现的 `learn_from_batch_experience` 方法，使用目标 Q 网络来创建 Q 学习目标。以下代码片段显示了我们第一次实现中的更改，已用粗体标出：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This completes our target Q-network implementation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们的目标 Q 网络实现。
- en: How do we know if the agent is benefiting from the target Q-network and other
    improvements we discussed in the previous sections? In the next section, we will
    look at ways to log and visualize the agent's performance so that we can monitor
    and figure out whether or not the improvements we discussed actually lead to better
    performances.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何知道代理是否从目标 Q 网络和前面讨论的其他改进中受益？在下一节中，我们将探讨如何记录和可视化代理的表现，从而监控并弄清楚这些改进是否真的导致了更好的表现。
- en: Logging and visualizing an agent's learning process
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记录和可视化代理的学习过程
- en: We now have a learning agent that uses a neural network to learn Q-values and
    update itself to perform better at the task. The agent takes a while to learn
    before it starts acting wisely. How do we know what is going on with the agent
    at a given time? How do we know if the agent is making progress or simply acting
    dumb? How do we see and measure the progress of the agent with time? Should we
    just sit and wait for the training to end? No. There should be some better way,
    don't you think?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个学习代理，它使用神经网络来学习 Q 值，并更新自身以便在任务中表现得更好。代理需要一段时间来学习，直到它开始做出明智的决策。我们如何知道代理在某一时刻的状态？我们如何知道代理是否在进步，还是只是表现得很笨？我们如何随着时间的推移看到和衡量代理的进展？我们应该只是坐在那里等待训练结束吗？不，应该有更好的方法，不是吗？
- en: Yes, and there is! It is actually important for us, the developers of the agents,
    to be able to observe how the agent is performing in order to figure out if there
    is an issue with the implementation or if some of the hyperparameters are too
    bad for the agent to learn anything. We have had the preliminary version of logging
    and seen how the agent's learning was progressing with the console outputs generated
    using the print statements. That gave us a first-hand look into the episode number,
    episode reward, the maximum reward, and so on, but it was more like a single snapshot
    at a given time. We want to be able to see the history of the progress to see
    if the agent's learning is converging with the learning error decreasing or not,
    and so on. This will enable us to think in the right direction to update our implementation
    or tweak the parameters to improve the learning performance of the agent.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，确实如此！对我们这些开发代理的人员来说，能够观察代理的表现是很重要的，这样才能发现实现中是否存在问题，或者某些超参数是否太差，导致代理无法学习任何东西。我们已经有了日志记录的初步版本，并通过使用打印语句生成的控制台输出，看到了代理学习的进展情况。这为我们提供了关于回合数、回合奖励、最大奖励等的第一手数据，但它更像是在某一时刻的快照。我们希望能够看到学习过程的历史，看看代理的学习是否在收敛，学习误差是否在减少，等等。这将帮助我们朝着正确的方向思考，更新实现或调整参数，以提高代理的学习表现。
- en: The TensorFlow deep learning library offers a tool called TensorBoard. It is
    a powerful tool to visualize the neural network graphs, plot quantitative metrics
    like learning errors, rewards, and so on as the training progresses. It can even
    be used to visualize images and a few other useful data types. It makes it easier
    to understand, identify, and debug our deep learning algorithm implementations.
    In the next section, we will see how we can use TensorBoard to log and visualize
    our agent's progress.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow深度学习库提供了一种名为TensorBoard的工具。它是一个强大的工具，可以用来可视化神经网络图、绘制量化指标（如学习误差、奖励等），随着训练的进行，它还可以用来可视化图像以及其他几种有用的数据类型。它使得我们更容易理解、识别和调试我们的深度学习算法实现。在下一节中，我们将看到如何使用TensorBoard来记录和可视化代理的进展。
- en: Using TensorBoard for logging and visualizing a PyTorch RL agent's progress
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorBoard记录和可视化PyTorch强化学习代理的进展
- en: 'Though TensorBoard is a tool that was released for the TensorFlow deep learning
    library, it is a flexible tool in itself, which can be used with other deep learning
    libraries like PyTorch. Basically, the TensorBoard tool reads the TensorFlow events
    summary data from log files and updates the visualizations and plots periodically.
    Fortunately, we have a library called `tensorboardX` that provides a convenient
    interface to create the events that TensorBoard can work with. This way, we can
    easily generate the appropriate events from our agent training code to log and
    visualize how our agent''s learning process is progressing. The use of this library
    is pretty straightforward and simple. We import `tensorboardX` and create a `SummaryWriter` object
    with the desired log file name. We can then add new scalars (and also other supported
    data) using the `SummaryWriter` object to add new data points to the plot which
    will be updated periodically. The following screenshot is an example of what the
    TensorBoard''s output will look like with the kind of information we will be logging
    in our agent training script to visualize its progress:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管TensorBoard是为TensorFlow深度学习库发布的工具，但它本身是一个灵活的工具，可以与其他深度学习库（如PyTorch）一起使用。基本上，TensorBoard工具读取来自日志文件的TensorFlow事件总结数据，并定期更新可视化和图表。幸运的是，我们有一个叫做`tensorboardX`的库，它提供了一个方便的接口来创建TensorBoard可以使用的事件。通过这种方式，我们可以轻松地从代理训练代码中生成适当的事件，以记录和可视化代理学习过程的进展。使用这个库非常简单和直接。我们只需要导入`tensorboardX`，然后创建一个带有所需日志文件名的`SummaryWriter`对象。然后，我们可以使用`SummaryWriter`对象添加新的标量（以及其他支持的数据），以便将新的数据点添加到图表中，并定期更新。以下截图是一个例子，展示了在我们的代理训练脚本中记录的那些信息，TensorBoard的输出将会是什么样的：
- en: '![](img/00144.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00144.jpeg)'
- en: In the preceding screenshot, the bottom-right most plot titled **main/mean_ep_reward** shows
    how the agent has been learning to progressively get higher and higher rewards
    over time steps. In all the plots in the preceding screenshot, the *x*-axis shows
    the number of training steps and the *y*-axis has the value of the data that was
    logged, as signified by the titles of each of the plots.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，右下角的图表标题为**main/mean_ep_reward**，展示了代理如何在时间步长上逐渐学会获得越来越高的奖励。在前面截图中的所有图表中，*x*-轴表示训练步骤的数量，*y*-轴则表示记录的数据的值，具体由每个图表的标题所指示。
- en: Now, we know how to log and visualize the performance of the agent as it is
    training. But still, a question remains as to how we can compare the agent with
    and without one or more of the improvements we discussed in the earlier sections
    in this chapter. We discussed several improvements, and each adds new hyperparameters.
    In order to manage the various hyperparameters and to easily turn on and off the
    improvements and configurations, in the next section, we will discuss a way to
    achieve this by building a simple parameter management class.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经知道如何在训练过程中记录和可视化代理的性能。但是，仍然有一个问题需要解决，那就是我们如何比较包含或不包含本章前面讨论的一个或多个改进的代理。我们讨论了几项改进，每一项都增加了新的超参数。为了管理这些不同的超参数，并方便地启用或禁用这些改进和配置，在下一节中，我们将讨论通过构建一个简单的参数管理类来实现这一目标。
- en: Managing hyperparameters and configuration parameters
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理超参数和配置参数
- en: As you may have noticed, our agent has several hyperparameters like the learning
    rate, gamma, epsilon start/minimum value, and so on. There are also several configuration
    parameters for both the agent and the environment that we would want to be able
    to modify easily and run instead of searching through the code to find where that
    parameter was defined. Having a simple and good way to manage these parameters
    also helps when we want to automate the training process or run parameter sweeps
    or other methods to tune and find the best set of parameters that work for the
    agent.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能注意到的，我们的智能体有多个超参数，比如学习率、gamma、epsilon 起始值/最小值等等。还有一些智能体和环境的配置参数，我们希望能够轻松修改并运行，而不是在代码中查找这些参数的定义。拥有一种简单而良好的方法来管理这些参数，也有助于我们在想要自动化训练过程或进行参数扫描等方法时，找到适合智能体的最佳参数集。
- en: In the following two subsections, we will look at how we can use a JSON file
    to specify the parameters and hyperparameters in an easy to use way and implement
    a parameter manager class to handle these externally configurable parameters to
    update the agent and the environment configuration.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两个小节中，我们将探讨如何使用 JSON 文件以一种易于使用的方式指定参数和超参数，并实现一个参数管理类来处理这些外部可配置的参数，从而更新智能体和环境的配置。
- en: Using a JSON file to easily configure parameters
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 JSON 文件轻松配置参数
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The parameters manager
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数管理器
- en: Did you like the parameter configuration file example that you just saw? I hope
    you did. In this section, we will implement a parameter manger that will help
    us load, get, and set these parameters as necessary.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你喜欢刚才看到的参数配置文件示例吗？我希望你喜欢。在本节中，我们将实现一个参数管理器，帮助我们根据需要加载、获取和设置这些参数。
- en: 'We will start by creating a Python class named `ParamsManger` that initializes
    the `params` member variable with the dictionary of parameters read from the `params_file` using
    the JSON Python library, as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从创建一个名为 `ParamsManger` 的 Python 类开始，该类通过使用 JSON Python 库从 `params_file` 读取的参数字典来初始化
    `params` 成员变量，如下所示：
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will then implement a few methods that will be convenient for us. We will
    start with the `get_params` method that returns the whole dictionary of parameters
    that we read from the JSON file:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将实现一些对我们有用的方法。我们将从`get_params`方法开始，该方法返回我们从 JSON 文件读取的整个参数字典：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Sometimes, we may just want to get the parameters that correspond to the agent
    or those that correspond to the environment which we can pass in while we initialize
    the agent or the environment. Since we had neatly separated the agent and the
    environment parameters in the `parameters.json` file that we saw in the previous
    section, the implementation is straightforward, as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们可能只想获取与智能体或环境对应的参数，这些参数可以在初始化智能体或环境时传入。由于我们在上一节中已经将智能体和环境的参数整齐地分开存储在`parameters.json`文件中，因此实现起来非常直接，如下所示：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will also implement another simple method to update the agent parameters
    so that we can also supply/read the agent parameters from the command line when
    we launch our training script:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将实现另一个简单的方法来更新智能体的参数，这样我们也可以在启动训练脚本时从命令行传入/读取智能体参数：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The preceding parameter manager implementation, along with a simple test procedure,
    is available at `ch6/utils/params_manager.py`, in this book's code repository. In
    the next section, we will consolidate all the techniques we have discussed and
    implemented so far to put together a complete deep Q learning-based agent.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上述参数管理器的实现以及一个简单的测试程序可以在本书的代码库中的 `ch6/utils/params_manager.py` 找到。在下一节中，我们将整合所有我们已经讨论和实现的技术，打造一个完整的基于深度
    Q 学习的智能体。
- en: A complete deep Q-learner to solve complex problems with raw pixel input
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个完整的深度 Q 学习者，用于解决复杂问题的原始像素输入
- en: From the beginning of this chapter, we have implemented several additional techniques
    and utility tools to improve the agent. In this section, we will consolidate all
    the improvements and the utility tools we have discussed so far into a unified
    `deep_Q_Learner.py` script. We will be using this unified agent script to train
    on the Atari Gym environment in the next section and watch the agent improving
    its performance and fetching more and more scores over time.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章开始，我们已经实现了几种额外的技术和实用工具来改进智能体。在本节中，我们将把迄今为止讨论的所有改进和实用工具整合成一个统一的 `deep_Q_Learner.py`
    脚本。我们将在下一节中使用这个统一的智能体脚本在 Atari Gym 环境中进行训练，并观察智能体如何逐步提升性能，随着时间的推移获取越来越多的分数。
- en: 'The following code is the unified version that utilizes the following features
    that we developed in the previous sections of this chapter:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是我们在本章前面各节中开发的功能的统一版本：
- en: Experience memory
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经验记忆
- en: Experience replay to learn from (mini) batches of experience
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用经验重放从（小批量）经验中学习
- en: Linear epsilon decay schedule
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性 epsilon 衰减计划
- en: Target network for stable learning
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定学习的目标网络
- en: Parameter management using JSON files
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 JSON 文件进行参数管理
- en: 'Performance visualization and logging using TensorBoard:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorBoard 进行性能可视化和日志记录：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding code, along with some additional changes required for using the
    Atari wrappers that we will be discussing in the next section, are available at `ch6/deep_Q_Learner.py`,
    in this book's code repository. After we complete the next section on *The Atari
    Gym environment*, we will use the agent implementation in `deep_Q_Learner.py` to
    train the agents on the Atari games and see their performance in the end.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码以及一些额外的更改（用于使用我们将在下一节讨论的 Atari 包装器）可以在本书代码库的 `ch6/deep_Q_Learner.py` 中找到。在我们完成下一节关于
    *Atari Gym 环境* 的内容后，我们将使用 `deep_Q_Learner.py` 中的智能体实现来训练 Atari 游戏中的智能体，并最终查看它们的表现。
- en: This book's code repository will have the latest and up-to-date code implementations
    with improvements and bug fixes that will be committed after this book is printed.
    So, it is a good idea to star and watch the book's code repository on GitHub to
    get automatic updates about these changes and improvements.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的代码库将包含最新的代码实现，包括改进和 bug 修复，这些将会在本书印刷后提交。因此，建议您在 GitHub 上为本书的代码库加星并关注，以便获得关于这些更改和改进的自动更新。
- en: The Atari Gym environment
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Atari Gym 环境
- en: 'In [Chapter 4](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12), *Exploring
    the Gym and its Features*, we looked at the various list of environments available
    in the Gym, including the Atari games category, and used a script to list all
    the Gym environments available on your computer. We also looked at the nomenclature
    of the environment names, especially for the Atari games. In this section, we
    will use the Atari environments and see how we can customize the environments
    with Gym environment wrappers. The following is a collage of 9 screenshots from
    9 different Atari environments:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 4 章](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12)，*探索 Gym 及其特性*
    中，我们查看了 Gym 中提供的各种环境列表，包括 Atari 游戏类别，并使用脚本列出了计算机上所有可用的 Gym 环境。我们还了解了环境名称的命名法，特别是
    Atari 游戏的命名。在本节中，我们将使用 Atari 环境，并查看如何使用 Gym 环境包装器定制这些环境。以下是来自 9 个不同 Atari 环境的
    9 张截图的拼贴：
- en: '![](img/00145.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00145.jpeg)'
- en: Customizing the Atari Gym environment
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定制 Atari Gym 环境
- en: 'Sometimes, we may want to change the way the observations are sent back by
    the environment or change the scale of the rewards so that our agents can learn
    better or filter out some information before the agent receives them or change
    the way the environment is rendered on the screen. So far, we have been developing
    and customizing our agent to make it act well in the environment. Wouldn''t it
    be nice to have some flexibility around how and what the environment sends back
    to the agent so that we can customize how the agent learns to act? Fortunately,
    the Gym library makes it easy to extend or customize the information sent by the
    environment with the help of Gym environment wrappers. The wrapper interface allows
    us to subclass and add routines as layers on top of the previous routines. We
    can add custom processing statements to one or more of the following methods of
    the Gym environment class:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们可能希望改变环境返回观察结果的方式，或者改变奖励的尺度，以便我们的智能体能更好地学习，或者在智能体接收信息之前过滤掉一些信息，或改变环境在屏幕上的渲染方式。到目前为止，我们一直在开发和定制我们的智能体，使其在环境中表现良好。若能对环境返回给智能体的内容和方式进行一定的灵活定制，让我们可以根据需要定制智能体的学习行为，这岂不是很好吗？幸运的是，Gym
    库通过 Gym 环境包装器使我们可以轻松扩展或定制环境返回的信息。包装器接口允许我们通过子类化并添加新的例程作为现有例程的附加层来进行定制。我们可以向 Gym
    环境类的一个或多个方法中添加自定义处理语句：
- en: '`__init__(self, env)__`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__init__(self, env)__`'
- en: '`_seed`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_seed`'
- en: '`_reset`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_reset`'
- en: '`_step`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_step`'
- en: '`_render`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_render`'
- en: '`_close`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_close`'
- en: Depending on the customization we would like to do to the environment, we can
    decide which methods we want to extend. For example, if we want to change the
    shape/size of the observation, we can extend the `_step` and `_reset` methods. In
    the next subsection, we will see how we can make use of the wrapper interface
    to customize the Atari Gym environments.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们希望对环境进行的自定义，可以决定我们要扩展哪些方法。例如，如果我们希望改变观察的形状/大小，我们可以扩展`_step`和`_reset`方法。在接下来的小节中，我们将展示如何利用包装器接口来定制Atari
    Gym环境。
- en: Implementing custom Gym environment wrappers
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现自定义的Gym环境包装器
- en: In this section, we will look at a few Gym environment wrappers that are especially
    very useful for the Gym Atari environments. Most of the wrappers we will implement
    in this section can be used with other environments as well to improve the learning
    performance of the agents.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍一些对于Gym Atari环境尤其有用的环境包装器。我们将在这一部分实现的大多数包装器也可以与其他环境一起使用，以提高智能体的学习表现。
- en: 'The following table mentions a list of the wrappers will be implementing in
    the following section with a brief description for each of the wrappers to give
    you an overview:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下表列出了我们将在接下来的部分实现的包装器，并为每个包装器提供简要描述，帮助你了解概况：
- en: '| **Wrapper** | **Brief description of the purpose** |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **包装器** | **目的简要描述** |'
- en: '| `ClipRewardEnv` |  To implement reward clipping |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `ClipRewardEnv` | 实现奖励裁剪 |'
- en: '| `AtariRescale` |  To rescale the screen pixels to a 84x84x1 gray scale image
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| `AtariRescale` | 将屏幕像素重新缩放为84x84x1的灰度图像 |'
- en: '| `NormalizedEnv` | To normalize the images based on the mean and variance
    observed in the environment |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| `NormalizedEnv` | 根据环境中观察到的均值和方差对图像进行标准化 |'
- en: '| `NoopResetEnv` | To perform a random number of `noop` (empty) actions on
    reset to sample different initial states |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| `NoopResetEnv` | 在重置时执行随机数量的`noop`（空操作），以采样不同的初始状态 |'
- en: '| `FireResetEnv` | To perform a fire action on reset |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `FireResetEnv` | 在重置时执行火焰动作 |'
- en: '| `EpisodicLifeEnv` | To mark end of life as end of episode and reset when
    game is over |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `EpisodicLifeEnv` | 将生命结束标记为回合结束，并在游戏结束时进行重置 |'
- en: '| `MaxAndSkipEnv` | Repeats the action for a fixed number (specified using
    the `skip` argument; the default is 4) of steps |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `MaxAndSkipEnv` | 对一个固定数量的步骤（使用`skip`参数指定，默认为4）重复执行动作 |'
- en: Reward clipping
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励裁剪
- en: 'Different problems or environments provide different ranges of reward values.
    For example, we saw in the previous chapter that in the `Mountain Car v0` environment,
    the agent receives a reward of -1 for every time step until episode termination,
    no matter which way the agent moves the car. In the `Cart Pole v0` environment,
    the agent receives a reward of +1 for every time step until episode termination. In
    Atari game environments like MS Pac-Man, if the agent eats a single ghost, it
    will receive a reward of up to +1,600\. We can start to see how the magnitudes
    of the reward as well as the occasion of the reward varies widely across different
    environments and learning problems. If our deep Q-learner agent algorithm has
    to solve this variety of problems without us trying to fine-tune the hyperparameters
    to work well for each of the environments independently, we have to do something
    about the varying scales of reward. This is exactly the intuition behind reward
    clipping, in which we clip the reward to be either -1, 0, or +1, depending on
    the sign of the actual reward received from the environment. This way, we limit
    the magnitude of the reward which can vary widely across the different environments.
    We can implement this simple reward clipping technique and apply it to our environments
    by inheriting from the `gym.RewardWrapper` class and modifying the `reward(...) `function,
    as shown in the following code snippet:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的问题或环境提供不同范围的奖励值。例如，我们在上一章中看到，在`Mountain Car v0`环境中，智能体在每个时间步都会收到-1的奖励，直到回合结束，无论智能体如何操作小车。在`Cart
    Pole v0`环境中，智能体在每个时间步都会收到+1的奖励，直到回合结束。在MS Pac-Man这样的Atari游戏环境中，如果智能体吃掉一个鬼魂，它将获得最高+1,600的奖励。我们可以开始看到，不同环境和学习问题中，奖励的幅度和奖励出现的时机差异非常大。如果我们的深度Q学习算法要解决这种不同的问题，而我们又不希望单独微调每个环境的超参数，那么我们必须解决奖励尺度的差异。这正是奖励裁剪背后的直觉：根据从环境中收到的实际奖励的符号，我们将奖励裁剪为-1、0或+1。通过这种方式，我们限制了奖励的幅度，避免了在不同环境之间存在过大的差异。我们可以通过继承`gym.RewardWrapper`类并修改`reward(...)`函数来实现这一简单的奖励裁剪技术，并将其应用于我们的环境，代码片段如下所示：
- en: '[PRE25]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The technique of clipping the reward to (-1, 0, 1) works well for Atari games.
    But, it is good to know that this may not be the the best technique to universally
    handle environments with varying reward magnitudes and frequency. Clipping the
    reward value modifies the learning objective of the agent and may sometimes lead
    to qualitatively different policies being learned by the agent than what is desired.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 截取奖励值到 (-1, 0, 1) 的技术对于 Atari 游戏效果很好。但是，值得注意的是，这可能不是处理奖励大小和频率变化的环境的最佳技术。截取奖励值会改变智能体的学习目标，有时可能导致智能体学习到与预期不同的策略。
- en: Preprocessing Atari screen image frames
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理 Atari 屏幕图像帧
- en: The Atari Gym environment produces observations which typically have a shape
    of 210x160x3, which represents a RGB (color) image of a width of 210 pixels and
    a height of 160 pixels. While the color image at the original resolution of 210x160x3
    has more pixels and therefore more information, it turns out that often, better
    performance is possible with reduced resolution. Lower resolution means less data
    to be processed by the agent at every step, which translates to faster training
    time, especially on consumer grade computing hardware that you and I own.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Atari Gym 环境产生的观察通常具有 210x160x3 的形状，代表一个 RGB（彩色）图像，宽度为 210 像素，高度为 160 像素。尽管原始分辨率为
    210x160x3 的彩色图像包含更多的像素，因此包含更多的信息，但事实证明，降低分辨率往往能获得更好的性能。较低的分辨率意味着每一步由智能体处理的数据较少，从而加快训练速度，尤其是在我们常见的消费级计算硬件上。
- en: 'Let''s create a preprocessing pipeline that would take the original observation
    image (of the Atari screen) and perform the following operations:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来创建一个预处理管道，该管道将接收原始的 Atari 屏幕图像，并执行以下操作：
- en: '![](img/00146.jpeg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00146.jpeg)'
- en: We can crop out the region on the screen that does not have any useful information
    regarding the environment for the agent.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以裁剪掉屏幕上没有任何与环境相关的有用信息的区域。
- en: 'Finally, we resize the image to a dimension of 84x84\. We can choose a different
    number, other than 84, as long as it contains a reasonable amount of pixels. However,
    it is efficient to have a square matrix (like 84x84 or 80x80) as the convolution
    operations (for example, with CUDA) are optimized for such square input:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将图像调整为 84x84 的尺寸。我们可以选择不同的数字，除了 84，只要它包含适量的像素。然而，选择一个方形矩阵（如 84x84 或 80x80）是高效的，因为卷积操作（例如使用
    CUDA）会对这种方形输入进行优化。
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that with a resolution of 84x84 pixels for one observation frame with a
    data type of `numpy.float32` which takes 4 bytes, we need about 4x84x84 = 28,224
    bytes. As you may recall from the *Experience memory* section, one experience
    object contains two frames (one for the observation and the other for the next
    observation), which means we'll need 2x 28,224 = 56,448 bytes (+ 2 bytes for *action*
    + 4 bytes for *reward). *The 56,448 bytes (or 0.056448 MB) may not seem much,
    but if you consider the fact that it is typical to be using an experience memory
    capacity in the order of 1e6 (million), you may realize that we need about 1e6
    x 0.056448 MB = 56,448 MB or 56.448 GB! This means that we will need 56.448 GB
    of RAM just for the experience memory with a capacity of 1 million experiences!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，假设每一帧观察图像的分辨率为 84x84 像素，数据类型为 `numpy.float32`，每个像素占用 4 字节，那么我们需要大约 4x84x84
    = 28,224 字节。如你在*经验记忆*部分所记得的，每个经验对象包含两帧（一个是当前观察，另一个是下一次观察），这意味着我们需要 2x 28,224 =
    56,448 字节（再加上 2 字节用于 *action* 和 4 字节用于 *reward*）。*56,448 字节（或 0.056448 MB）看起来不多，但如果考虑到通常情况下经验记忆的容量是
    1e6（百万），你会意识到我们需要大约 1e6 x 0.056448 MB = 56,448 MB 或 56.448 GB！这意味着我们仅仅为了存储 100
    万个经验对象，就需要 56.448 GB 的内存！*
- en: You can do a couple of memory optimizations to reduce the required RAM for training
    the agent. Using a smaller experience memory is a straightforward way to reduce
    the memory footprint in some games. In some environments, having a larger experience
    memory will help the agent to learn faster. One way to reduce the memory footprint
    it by not scaling the frames (by dividing by 255) while storing, which requires
    a floating point representation (`numpy.float32`) and rather storing the frames
    as numpy.uint8 so that we only need 1 byte instead of 4 bytes per pixels, which
    will help in reducing the memory requirement by a factor of 4\. Then, when we
    want to use the stored experiences in our forward pass to the network to the deep
    Q-network to get the Q-value predictions, we can scale the images to be in the
    range 0.0 to 1.0.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以进行一些内存优化，以减少训练代理所需的RAM。在某些游戏中，使用较小的经验内存是一种减少内存占用的直接方法。在某些环境中，较大的经验内存可以帮助代理更快地学习。减少内存占用的一种方法是，在存储时不对帧进行缩放（即不除以255），这要求使用浮点表示（`numpy.float32`），而是将帧存储为`numpy.uint8`，这样我们每个像素只需要1个字节，而不是4个字节，从而帮助减少内存需求，降低四倍。然后，当我们想在前向传递到深度Q网络时使用存储的经验以获取Q值预测时，我们可以将图像缩放到0.0到1.0的范围内。
- en: Normalizing observations
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标准化观测值
- en: 'In some cases, normalizing the observations can help with convergence speed.
    The most commonly used normalization process involves two steps:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，标准化观测值有助于提高收敛速度。最常用的标准化过程包含两个步骤：
- en: Zero-centering using mean subtraction
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用均值减法进行零中心化
- en: Scaling using the standard deviation
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准差进行缩放
- en: 'In essence, the following is the normalization process:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，以下是标准化过程：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the previous process, x is the observation. Note that other normalization
    processes are also used, depending on the range of the normalized value that is
    desired. For example, if we wanted the values after the normalization to lie between
    0 and 1, we could use the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的过程中，x是观测值。请注意，其他标准化过程也可以使用，具体取决于所需的标准化值范围。例如，如果我们希望标准化后的值位于0到1之间，可以使用以下方法：
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In the previous process, instead of subtracting the mean, we subtract the minimum
    value and divide by the difference between the maximum and the minimum value.
    This way, the minimum value in the observation/x gets normalized to 0 and the
    maximum value gets normalized to a value of 1.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的过程中，我们不是减去均值，而是减去最小值并除以最大值和最小值之间的差。这种方式下，观测值/x中的最小值将被标准化为0，最大值将被标准化为1。
- en: 'Alternatively, if we wanted the values after the normalization to lie between
    -1 and +1, then the following can be used:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们希望标准化后的值位于-1到+1之间，可以使用以下方法：
- en: '[PRE29]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In our environment normalization wrapper implementation, we will use the first
    method where we zero-center the observation data using mean subtraction and scale
    using the standard deviation of the data in the observation. In fact, we will
    go one step further and calculate the running mean and standard deviation of all
    the observations we have received so far to normalize the observations based on
    the distribution of the observation data the agent has observed so far. This is
    more appropriate as there can be high variance between different observations
    from the same environment. The following is the implementation code for the normalization
    wrapper that we discussed:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的环境标准化包装器实现中，我们将使用第一种方法，通过均值减法进行零中心化，并使用观测数据的标准差进行缩放。事实上，我们会更进一步，计算我们迄今为止接收到的所有观测值的运行均值和标准差，以根据代理目前所观察到的观测数据分布来标准化观测值。这种方法更为适合，因为同一环境下不同观测值之间可能存在较大的方差。以下是我们讨论的标准化包装器的实现代码：
- en: '[PRE30]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The image frames that we get as observation from the environment (even after
    our preprocessing wrapper) is already on the same scale (0-255 or 0.0 to 1.0).
    The scaling step in the normalization procedure may not be very helpful in this
    case. This wrapper in general could be useful for other environment types and
    was also not observed to be detrimental to the performance for already scaled
    image observations from Gym environments like Atari.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从环境中获取的图像帧（即使经过我们的预处理包装器）已经处于相同的尺度（0-255或0.0到1.0）。在这种情况下，标准化过程中的缩放步骤可能没有太大帮助。这个包装器通常对其他类型的环境可能会很有用，并且我们也没有观察到它会对来自Gym环境（如Atari）已经缩放的图像观测值的性能产生不利影响。
- en: Random no-ops on reset
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在重置时进行随机无操作
- en: 'When the environment is reset, the agent usually starts from the same initial
    state and therefore receives the same observation on reset. The agent may memorize
    or get used to the starting state in one game level so much that they might start
    performing poorly they start in a slightly different position or game level. Sometimes,
    it was found to be helpful to randomize the initial state, such as sampling different
    initial states from which the agent starts the episode. To make that happen, we
    can add a Gym wrapper that performs a random number of "no-ops" before sending
    out the first observation after the reset. The Arcade Learning Environment for
    the Atari 2600 that the Gym library uses for the Atari environment supports a
    "NOOP" or no-operation action, which in the Gym library is coded as an action
    with a value of 0\. So, we will step the environment with a random number of *action*=0before
    returning the observation to the agent, as shown in the following code snippet:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当环境重置时，智能体通常从相同的初始状态开始，因此在重置时会得到相同的观察结果。智能体可能会记住或习惯某个游戏关卡的初始状态，甚至可能在稍微改变起始位置或游戏关卡时表现不佳。有时候，发现随机化初始状态会有帮助，比如从不同的初始状态中随机选择一个作为智能体开始的地方。为了实现这一点，我们可以添加一个Gym包装器，在发送重置后的第一次观察之前，执行一个随机数量的“无操作”（no-op）动作。Gym库使用的Atari
    2600的街机学习环境（Arcade Learning Environment）支持一种“NOOP”或无操作动作，在Gym库中，这个动作的值为0。所以，我们将在环境中执行一个随机数量的*action*=0，然后再将观察结果返回给智能体，如下代码片段所示：
- en: '[PRE31]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Fire on reset
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重置时按下Fire按钮
- en: Some Atari games require the player to press the Fire button to start the game.
    Some games require the Fire button to be pressed after every life is lost. More
    often that not, this is the only use for the Fire button! Although it might look
    trivial for us to realize that, it may be difficult for the reinforcement learning
    agents to figure that out on their own sometimes. It is not the case that they
    are incapable of learning that. In fact, they are capable of figuring out lots
    of hidden glitches or modes in the game that no human has ever figured out! For
    example, in the game of Qbert, an agent trained using Evolutionary Strategies
    (which is a black-box type learning strategy inspired by genetic algorithms) figured
    out a peculiar way with which it can keep receiving scores and never let the game
    end! You know how much the agent was able to score? ~1,000,000! They could only
    get that much because the game was reset artificially due to a time limit. Can
    you try scoring that much in the game of Qbert? You can see that agent scoring
    in action here: [https://www.youtube.com/watch?v=meE5aaRJ0Zs](https://youtu.be/meE5aaRJ0Zs).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一些Atari游戏要求玩家按下Fire按钮以开始游戏。某些游戏要求在每次失去一条生命后按下Fire按钮。通常来说，这几乎是Fire按钮唯一的用途！虽然对我们来说，这看起来微不足道，但有时候对于强化学习的智能体来说，自己弄明白这一点可能并不容易。并不是说它们无法学会这一点，事实上，它们能够发现游戏中的许多隐藏漏洞或模式，这些人类从未发现过！例如，在Qbert游戏中，使用进化策略（这是一种受遗传算法启发的黑箱学习策略）训练的智能体发现了一种特殊的方法，可以不断获得分数并让游戏永远不会结束！你知道它的得分是多少吗？大约1,000,000！它们之所以能得这么多分，是因为游戏由于时间限制被人为重置过。你能在Qbert游戏中尝试获得这么多分吗？你可以在这里看到智能体得分的实际情况：[https://www.youtube.com/watch?v=meE5aaRJ0Zs](https://youtu.be/meE5aaRJ0Zs)。
- en: The point is not that the agents are so smart to figure all these things out.
    They definitely can, but most of the time, this harms the progress the agent can
    make in a reasonable amount of time. This is especially true when we want a single
    agent to tackle several different varieties of games (one at a time). We are better
    off starting with simpler assumptions and making them more complicated after we
    have been able to train the agents to play well using the simpler assumptions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 关键不在于智能体如此聪明能够搞明白所有这些事情。它们当然能做到，但大多数情况下，这反而会妨碍智能体在合理时间内取得进展。特别是当我们希望一个智能体同时应对几种不同类型的游戏（一次一个）时，这种情况尤为明显。我们最好从简单的假设开始，并在通过这些简单假设训练智能体成功后，再逐步让假设变得更复杂。
- en: 'Therefore, we will implement a `FireResetEnv` Gym wrapper that will press the
    Fire button on every reset and get the environment started for the agent. The
    code''s implementation is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将实现一个`FireResetEnv` Gym包装器，它会在每次重置时按下Fire按钮，并启动环境供智能体使用。代码实现如下：
- en: '[PRE32]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Episodic life
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 片段化的生活
- en: In many games, including Atari games, the player gets to play with more than
    one life.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多游戏中，包括Atari游戏，玩家有多个生命可以使用。
- en: It was observed, used, and reported by Deepmind that terminating an episode
    when a life is lost helps the agent learn better. It has to be noted that the
    intention is to signify to the agent that losing a life is a bad thing to do.
    In this case, when the episode terminates, we will not reset the environment and
    rather continue until the game is actually over, after which we reset the environment.
    If we reset the game after every loss of life, we would be limiting the agent's
    exposure to observations and experiences that can be collected with just one life,
    which is usually bad for the agent's learning performance.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Deepmind 观察到并报告称，当一个生命丧失时终止一个回合，有助于智能体更好地学习。需要注意的是，目的是向智能体表明失去一条生命是不好的行为。在这种情况下，当回合终止时，我们不会重置环境，而是继续进行，直到游戏结束后再重置环境。如果在每次失去生命后都重置游戏，我们会限制智能体对仅凭一条生命可以收集到的观察和经验的暴露，而这通常对智能体的学习表现不利。
- en: 'To implement what we just discussed, we will use the `EpisodicLifeEnv` class
    that marks the end of an episode when a life is lost, and reset the environment
    when the game is over, as shown in the following code snippet:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现我们刚才讨论的内容，我们将使用 `EpisodicLifeEnv` 类，该类在失去生命时标记回合结束，并在游戏结束时重置环境，代码片段如下所示：
- en: '[PRE33]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Max and skip-frame
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大值和跳帧
- en: 'The `Gym` library provides environments that have `NoFrameskip` in their ID,
    which we discussed in [Chapter 4](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12), *Exploring
    the Gym and its Features*, where we discussed the nomenclature of the Gym environments.
    As you may recall from our discussion in [Chapter 4](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12),
    *Exploring the Gym and its Features*, by default, if there is no presence of `Deterministic`
    or `NoFrameskip` in the environment name, the action sent to the environment is
    repeatedly performed for a duration of *n *frames, where *n* is uniformly sampled
    from (2, 3, 4). If we want to step through the environment at a specific rate,
    we can use the Gym Atari environments with `NoFrameskip` in their ID, which will
    step through the underlying environment without any alteration to the step duration.
    The step rate, in this case, is of![](img/00147.jpeg) a second, which is 60 frames
    per second. We can then customize the environment to skip at our choice to skip
    the rate (*k*) to step at a specific rate. The implementation for such a custom
    step/skip rate is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`Gym` 库提供了在其 ID 中带有 `NoFrameskip` 的环境，我们在 [第 4 章](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12)《探索
    Gym 及其特性》中讨论过，在那里我们讨论了 Gym 环境的命名法。如你从 [第 4 章](part0071.html#23MNU0-22c7fc7f93b64d07be225c00ead6ce12)《探索
    Gym 及其特性》中回忆的那样，默认情况下，如果环境名称中没有 `Deterministic` 或 `NoFrameskip`，发送到环境的动作会在 *n*
    帧内重复执行，其中 *n* 从 (2, 3, 4) 中均匀采样。如果我们希望以特定速率逐步进行环境的操作，可以使用带有 `NoFrameskip` 的 Gym
    Atari 环境，这样会在没有改变步骤持续时间的情况下逐步执行底层环境。在这种情况下，步骤速率为每秒 60 帧。然后我们可以自定义环境，选择跳过特定速率 (*k*)
    来以特定速率执行步骤。以下是自定义步骤/跳帧速率的实现：'
- en: '[PRE34]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Notice that we are also taking the maximum of the pixel values over the frames
    that were skipped and sending that as the observation instead of totally ignoring
    all the intermediate image frames that were skipped.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还会在跳过的帧上取像素值的最大值，并将其作为观察值，而不是完全忽略跳过的所有中间图像帧。
- en: Wrapping the Gym environment
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 包装 Gym 环境
- en: 'Finally, we will apply the preceding wrappers that we developed based on the
    environment configuration we specify using the `parameters.JSON` file:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将应用之前根据我们使用 `parameters.JSON` 文件指定的环境配置开发的包装器：
- en: '[PRE35]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: All of the environment wrappers that we discussed previously are implemented
    and available in the `ch6/environment/atari.py` in this book's code repository.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过的所有环境包装器都已经在本书代码库的 `ch6/environment/atari.py` 中实现并可用。
- en: Training the deep Q-learner to play Atari games
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练深度 Q 学习器玩 Atari 游戏
- en: We have gone through several new techniques in this chapter. You deserve a pat
    on your back for making it this far! Now starts the fun part where you can let
    your agents train by themselves to play several Atari games and see how they are
    progressing. What is great about our deep Q-learner is the fact that we can use
    the same agent to train and play any of the Atari games!
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们介绍了几种新的技术。你已经做得很棒，值得为自己鼓掌！接下来是有趣的部分，你可以让你的智能体自主训练，玩几个 Atari 游戏，并观察它们的进展。我们深度
    Q 学习器的一个亮点是，我们可以使用相同的智能体来训练和玩任何 Atari 游戏！
- en: 'By the end of this section, you should be able to use our deep Q learning agent
    to observe the pixels on the screen and take actions by sending the joystick commands
    to the Atari Gym environment, just like what is shown in the following screenshot:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 到本节结束时，你应该能够使用我们的深度Q学习代理观察屏幕上的像素，并通过向Atari Gym环境发送摇杆命令来执行动作，就像下面的截图所示：
- en: '![](img/00148.jpeg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00148.jpeg)'
- en: Putting together a comprehensive deep Q-learner
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将所有我们讨论的技巧整合成一个全面的深度Q学习器
- en: 'It is time to combine all the techniques we have discussed into a comprehensive
    implementation that makes use of all of those techniques to get maximum performance. We
    will use the `environment.atari` module that we created in the previous section
    with several useful Gym environment wrappers. Let''s look at the code outline
    to understand the code''s structure:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将我们讨论过的所有技术结合起来，形成一个全面的实现，利用这些技术以获得最佳性能。我们将使用前一节中创建的`environment.atari`模块，并添加几个有用的Gym环境包装器。让我们来看一下代码大纲，了解代码结构：
- en: You will notice that some sections of the code are removed for brevity and replaced
    with`...`, signifying that the code in that section has been folded/hidden. You can
    find the latest version of the complete code in this book's code repository at `ch6/deep_Q_Learner.py`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，代码的某些部分被省略了，为了简洁起见，这些部分用`...`表示，意味着这些部分的代码已经被折叠/隐藏。你可以在本书的代码库中的`ch6/deep_Q_Learner.py`找到完整的代码的最新版本。
- en: '[PRE36]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Hyperparameters
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数
- en: 'The following is a list of hyperparameters that our deep Q-learner uses, with
    a brief description of what they are and the types of values they take:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们的深度Q学习器使用的一些超参数列表，简要描述它们的作用及其接受的值类型：
- en: '| **Hyperparameter** | **Brief description** | **Value type** |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| **超参数** | **简要描述** | **值的类型** |'
- en: '| `max_num_episodes` | Maximum number of episodes to run the agent. | Integer
    (for example, 100,000) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| `max_num_episodes` | 运行代理的最大回合数。 | 整数（例如：100,000） |'
- en: '| `replay_memory_capacity` | Total capacity of the experience memory capacity.
    | Integer or exponential notation (for example, 1e6) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| `replay_memory_capacity` | 经验记忆的总容量。 | 整数或指数表示法（例如：1e6） |'
- en: '| `replay_batch_size` | Number of transitions used in a (mini) batch to update
    the Q-function in each update iteration during experience replay. | Integer (for
    example, 2,000) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| `replay_batch_size` | 每次更新过程中，用于更新Q函数的（迷你）批次中的过渡数量。 | 整数（例如：2,000） |'
- en: '| `use_target_network` | Whether a target Q-network is to be used or not. |
    Boolean (true/false) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| `use_target_network` | 是否使用目标Q网络。 | 布尔值（true/false） |'
- en: '| `target_network_update_freq` | The number of steps after which the target
    Q-network is updated using the main Q-network. | Integer (for example, 1,000)
    |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| `target_network_update_freq` | 更新目标Q网络的步数，使用主Q网络进行更新。 | 整数（例如：1,000） |'
- en: '| `lr` | The learning rate for the deep Q-network. | float (for example, 1e-4)
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| `lr` | 深度Q网络的学习率。 | 浮动数值（例如：1e-4） |'
- en: '| `gamma` | The discount factor for the MDP. | float (for example, 0.98) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| `gamma` | MDP的折扣因子。 | 浮动数值（例如：0.98） |'
- en: '| `epsilon_max` | The maximum value of the epsilon from which the decay starts.
    | float (for example, 1.0) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| `epsilon_max` | epsilon的最大值，从该值开始衰减。 | 浮动数值（例如：1.0） |'
- en: '| `epsilon_min` | The minimum value for epsilon to which the decay will finally
    settle to. | float (for example, :0.05) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| `epsilon_min` | epsilon的最小值，衰减最终将会稳定到该值。 | 浮动数值（例如：0.05） |'
- en: '| `seed` | The seed used to seed numpy and torch (and `torch.cuda`) to be able
    to reproduce (to some extent) the randomness introduced by those libraries. |
    Integer (for example, :555) |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| `seed` | 用于为numpy和torch（以及`torch.cuda`）设置随机种子的种子值，以便（在某种程度上）重现这些库引入的随机性。
    | 整数（例如：555） |'
- en: '| `use_cuda` | Whether or not to use CUDA based GPU if a GPU is available.
    | Boolean (for example, : true) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| `use_cuda` | 是否在GPU可用时使用基于CUDA的GPU。 | 布尔值（例如：true） |'
- en: '| `load_trained_model` | Whether or not to load a trained model if one exists
    for this environment/problem. If this parameter is set to true but no trained
    model is available, the model will be trained from scratch. | Boolean (for example,
    : true) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| `load_trained_model` | 是否加载已经训练好的模型（如果存在的话），适用于当前的环境/问题。如果设置为true，但没有可用的已训练模型，则模型将从头开始训练。
    | 布尔值（例如：true） |'
- en: '| `load_dir` | The path to the directory (including the forward slash) from
    where the trained model should be loaded from to resume training. | String (for
    example, : "trained_models/") |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| `load_dir` | 训练模型应该从哪个目录加载以恢复训练的路径（包括斜杠）。 | 字符串（例如："trained_models/"） |'
- en: '| `save_dir` | The path to a directory where the models should be saved. New
    models are saved every time the agent achieves a new best score/reward. | string
    (for example, : trained_models/") |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| `save_dir` | 保存模型的目录路径。每当智能体取得新的最佳分数/奖励时，新模型将被保存。 | string（例如：trained_models/）|'
- en: Please refer to the `ch6/parameters.JSON` file in this book's code repository
    for the updated list of parameters used by the agent.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考本书代码库中的 `ch6/parameters.JSON` 文件，以获取智能体使用的最新参数列表。
- en: Launching the training process
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动训练过程
- en: We have now put together all the pieces for the deep Q-learner and are ready
    to train the agent! Be sure to check out/pull/download the latest code from this
    book's code repository.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经将深度 Q 学习者的所有部分拼凑在一起，准备好训练智能体了！务必查看/拉取/下载本书代码库中的最新代码。
- en: 'You can pick any environment from the list of Atari environments and train
    the agent we developed using the following command:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 Atari 环境列表中选择任何一个环境，并使用以下命令训练我们开发的智能体：
- en: '[PRE37]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'In the previous command, `ENV_ID` is the name/ID of the Atari Gym environment.
    For example, if you want to train the agent on the `pong` environment with no
    frame skip, you would run the following command:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的命令中，`ENV_ID` 是 Atari Gym 环境的名称/ID。例如，如果你想在 `pong` 环境中训练智能体，并且不使用帧跳跃，你可以运行以下命令：
- en: '[PRE38]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'By default, the training logs will be saved to `./logs/DQL_{ENV}_{T}`, where `{ENV}`
    is the name of the environment and `{T}` is the time stamp obtained when you run
    the agent. If you start a TensorBoard instance using the following command:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，训练日志将保存在 `./logs/DQL_{ENV}_{T}`，其中 `{ENV}` 是环境的名称，`{T}` 是运行智能体时获得的时间戳。如果你使用以下命令启动
    TensorBoard 实例：
- en: '[PRE39]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: By default, our `deep_Q_learner.py` script will use the `parameters.JSON` file
    located in the same directory as the script for reading the configurable parameter
    values. You can override with a different parameter configuration file using the
    command-line `--params-file` argument.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，我们的 `deep_Q_learner.py` 脚本将使用与脚本位于同一目录中的 `parameters.JSON` 文件来读取可配置的参数值。你可以使用命令行
    `--params-file` 参数覆盖并使用不同的参数配置文件。
- en: If the `load_trained_model` parameter is set to `true` in the `parameters.JSON`
    file and if a saved model for the chosen environment is available, our script
    will try to initialize the agent with the model that it learned previously so
    that it can resume from where it left off rather than train from scratch.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在 `parameters.JSON` 文件中将 `load_trained_model` 参数设置为 `true`，并且为所选环境有一个已保存的模型，我们的脚本会尝试用之前训练的模型初始化智能体，这样它可以从中断的地方继续，而不是从头开始训练。
- en: Testing performance of your deep Q-learner in Atari games
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试你的深度 Q 学习者在 Atari 游戏中的表现
- en: 'It feels great, doesn''t it? You have now developed an agent that can learn
    to play any Atari game and get better at it by itself! Once you have your agent
    trained on any Atari game, you can use the test mode of the script to test the
    agent''s performance based on its learning so far. You can enable the test mode
    by using the `--test` argument in the `deep_q_learner.py` script. It is useful
    to enable rendering of the environment, too, so that you can visually see (apart
    from the rewards printed on the console) how the agent is performing. As an example,
    you can test the agent in the `Seaquest` Atari game using the following command:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 感觉不错吧？你现在已经开发出一个能够学习任何 Atari 游戏并且自动提升表现的智能体！一旦你训练好你的智能体在任何 Atari 游戏中，你可以使用脚本的测试模式来根据它到目前为止的学习情况测试智能体的表现。你可以通过在
    `deep_q_learner.py` 脚本中使用 `--test` 参数来启用测试模式。启用环境渲染也非常有用，这样你不仅可以看到控制台上打印的奖励，还能直观地看到智能体的表现。例如，你可以使用以下命令在
    `Seaquest` Atari 游戏中测试智能体：
- en: '[PRE40]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: You will see the Seaquest game window come up and the agent showing of its skills!
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到 Seaquest 游戏窗口弹出，智能体展示它的技能！
- en: 'A couple of points to note regarding the `test` mode are the following:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 `test` 模式，有几个需要注意的要点：
- en: The test mode will turn off the agent's learning routine. Therefore, the agent
    will not learn or update itself in the test mode. This mode is only used to test
    how a trained agent is performing. If you want to see how the agent is performing
    while it is learning, you can just use the `--render` option without the `--test`
    option.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试模式会关闭智能体的学习过程。因此，智能体在测试模式下不会学习或更新自己。这个模式仅用于测试已训练的智能体的表现。如果你想查看智能体在学习过程中如何表现，你可以只使用
    `--render` 选项，而不使用 `--test` 选项。
- en: The test mode assumes that a trained model for the environment you choose exists
    in the `trained_models` folder. Otherwise, a newborn agent, without any prior
    knowledge, will start playing the game from scratch. Also, since learning is disabled,
    you will not see the agent improving!
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试模式假定你选择的环境中已经存在一个训练好的模型，存放在`trained_models`文件夹中。否则，一个没有任何先验知识的全新代理将从头开始玩游戏。此外，由于学习被禁用，你将看不到代理的进步！
- en: Now, it's your turn to go out, experiment, review, and compare the performance
    of the agent we implemented in different Atari Gym environments and see how much
    the agent can score! If you train an agent to play well in a game, you can show
    and share it to other fellow readers by opening a pull request on this book's
    code repository from your fork. You will be featured on the page!
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在轮到你了，去外面进行实验，回顾并比较我们在不同雅达利Gym环境中实现的代理的表现，看看代理能得多少分！如果你训练了一个代理并使其在游戏中表现良好，你可以通过在本书代码库的分支上发起pull
    request，向其他读者展示并分享你的成果。你会被特写在页面上！
- en: Once you get comfortable using the code base we developed, you can do several
    experiments with it. For example, you can turn off the target Q-network or increase/decrease
    the experience memory/replay batch size by simply changing the `parameters.JSON`
    file and comparing the performance using the very convenient TensorBoard dashboard.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你熟悉了我们开发的代码库，你可以进行多种实验。例如，你可以通过简单地修改`parameters.JSON`文件，关闭目标Q网络或增加/减少经验记忆/重放批量大小，并使用非常方便的TensorBoard仪表板比较性能。
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started this chapter with the grand goal of developing intelligent learning
    agents that can achieve great scores in Atari games. We made incremental progress
    towards it by implementing several techniques to improve upon the Q-learner that
    we developed in the previous chapter. We first started with learning how we can
    use a neural network to approximate the Q action-value function and made our learning
    concrete by practically implementing a shallow neural network to solve the famous
    Cart Pole problem. We then implemented experience memory and experience replay
    that enables the agent to learn from (mini) randomly sampled batches of experiences
    that helped in improving the performance by breaking the correlations between
    the agent's interactions and increasing the sample efficiency with the batch replay
    of the agent's prior experience. We then revisited the epsilon-greedy action selection
    policy and implemented a decay schedule to decrease the exploration based on a
    schedule to let the agent rely more on its learning.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以开发能够在雅达利游戏中取得高分的智能学习代理为宏大目标展开。我们通过实现几种技术，逐步改进了上一章中开发的Q-学习器。我们首先学习了如何使用神经网络来逼近Q动作值函数，并通过实际实现一个浅层神经网络来解决著名的Cart
    Pole问题，使我们的学习更加具体。接着，我们实现了经验记忆和经验重放机制，使得代理能够从（小批量）随机采样的经验中学习，这有助于通过打破代理与环境互动之间的相关性，提升性能，并通过批量重放代理先前的经验来增加样本效率。然后，我们重新审视了epsilon-贪婪行动选择策略，并实现了一个衰减计划，根据预定计划减少探索，使代理更多依赖于自己的学习。
- en: We then looked at how to use TensorBoard's logging and visualization capabilities
    with our PyTorch-based learning agent so that we can watch the agent's training
    progress in a simple and intuitive way. We also implemented a neat little parameter
    manager class that enabled us to configure the hyperparameters of the agent and
    other configuration parameters using an external easy-to-read JSON file.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们看了如何利用TensorBoard的日志记录和可视化功能，与基于PyTorch的学习代理一起使用，从而以一种简单直观的方式观察代理的训练进展。我们还实现了一个小巧的参数管理类，使我们能够通过外部易于阅读的JSON文件配置代理的超参数和其他配置参数。
- en: After we got a good baseline and the helpful utility tools implemented, we started
    our implementation of the deep Q-learner. We started that section by implementing
    a deep convolutional neural network in PyTorch which we then used to represent
    our agent's Q (action-value) function. We then saw how easy it was to implement
    the idea of using a target Q-network which is known to stabilize the agent's Q
    learning process. We then put together our deep Q learning-based agent that can
    learn to act based on just the raw pixel observations from a Gym environment.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们得到了一个良好的基准并实现了有用的工具之后，我们开始实现深度Q学习器。我们首先通过在PyTorch中实现一个深度卷积神经网络来表示我们代理的Q（动作值）函数。接着，我们展示了如何轻松实现使用目标Q网络的想法，目标Q网络被证明能稳定代理的Q学习过程。然后，我们将这些结合起来，构建了一个基于深度Q学习的代理，能够仅凭Gym环境中的原始像素观测进行学习并执行动作。
- en: We then laid our eyes and hands on the Atari Gym environments and looked at
    several ways to customize the Gym environments using Gym environment wrappers.
    We also discussed several useful wrappers for the Atari environment and specifically
    implemented wrappers to clip the reward, preprocess the observation image frames,
    normalize the observations over all the entire sampled observation distribution,
    send random noop actions on reset to sample different start states, press the
    Fire button on resets, and to step at a custom rate by frame skipping. We finally
    saw how we can consolidate this all together into a comprehensive agent training
    code base and train the agent on any Atari game and see the progress summary on
    TensorBoard. We also looked at how we could save the state and resume the training
    of the agent from a previous saved state instead of rerunning the training from
    scratch. Towards the end, we saw the improving performance of the agent we implemented
    and trained.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们将目光和双手放在了Atari Gym环境上，研究了几种使用Gym环境包装器定制Gym环境的方法。我们还讨论了几个用于Atari环境的有用包装器，特别实现了包装器来裁剪奖励、预处理观察图像帧、对所有采样的观测分布进行归一化、在重置时发送随机noop动作以采样不同的初始状态、在重置时按下Fire按钮，并通过跳帧以自定义速率进行步进。最终，我们展示了如何将这些整合成一个全面的代理训练代码库，并在任何Atari游戏上训练代理，随后在TensorBoard中查看进展摘要。我们还展示了如何保存代理的状态，并从之前保存的状态恢复训练，而不是从头开始重新训练。最后，我们看到了我们实现并训练的代理性能的提升。
- en: We hope that you had a lot of fun throughout this whole chapter. We will be
    looking at and implementing a different algorithm in the next chapter, which can
    be used for taking much more complex actions rather than a discrete set of button
    presses and how we can use it to train an agent to autonomously control a car
    in simulation!
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望你在整个章节中都玩得很开心。在下一个章节中，我们将研究并实现一种不同的算法，这种算法可以用于执行比一组离散按钮按压更复杂的动作，并且我们将展示如何利用它训练一个代理，在模拟中自主地控制汽车！
