- en: Constructing an LSTM Neural Network for Sequence Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建用于序列分类的LSTM神经网络
- en: 'In the previous chapter, we discussed classifying time series data for multi-variate
    features. In this chapter, we will create a **long short-term memory** (**LSTM**) neural
    network to classify univariate time series data. Our neural network will learn
    how to classify a univariate time series. We will have **UCI** (short for **University
    of California Irvine**) synthetic control data on top of which the neural network
    will be trained. There will be 600 sequences of data, with every sequence separated
    by a new line to make our job easier. Every sequence will have values recorded
    at 60 time steps. Since it is a univariate time series, we will only have columns
    in CSV files for every example recorded. Every sequence is an example recorded.
    We will split these sequences of data into train/test sets to perform training
    and evaluation respectively. The possible categories of class/labels are as follows:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了如何为多变量特征对时间序列数据进行分类。本章将创建一个**长短期记忆**（**LSTM**）神经网络来对单变量时间序列数据进行分类。我们的神经网络将学习如何对单变量时间序列进行分类。我们将使用**UCI**（即**加利福尼亚大学欧文分校**）的合成控制数据，并以此为基础对神经网络进行训练。数据将有600个序列，每个序列之间用新行分隔，方便我们的操作。每个序列将在60个时间步骤上记录数据。由于这是单变量时间序列，我们的CSV文件中将仅包含每个记录的示例列。每个序列都是一个记录的示例。我们将把这些数据序列分成训练集和测试集，分别进行训练和评估。分类/标签的可能类别如下：
- en: Normal
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正常
- en: Cyclic
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环的
- en: Increasing trend
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增长趋势
- en: Decreasing trend
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下降趋势
- en: Upward shift
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向上移动
- en: Downward shift
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向下移动
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节我们将介绍以下食谱：
- en: Extracting time series data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取时间序列数据
- en: Loading training data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载训练数据
- en: Normalizing training data
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对训练数据进行归一化
- en: Constructing input layers for the network
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建网络的输入层
- en: Constructing output layers for the network
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建网络的输出层
- en: Evaluating the LSTM network for classified output
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估LSTM网络的分类输出
- en: Let's begin.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Technical requirements
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter's implementation code can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/07_Constructing_LSTM_Neural_network_for_sequence_classification/sourceCode/cookbookapp/src/main/java/UciSequenceClassificationExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/07_Constructing_LSTM_Neural_network_for_sequence_classification/sourceCode/cookbookapp/src/main/java/UciSequenceClassificationExample.java).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节的实现代码可以在[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/07_Constructing_LSTM_Neural_network_for_sequence_classification/sourceCode/cookbookapp/src/main/java/UciSequenceClassificationExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/07_Constructing_LSTM_Neural_network_for_sequence_classification/sourceCode/cookbookapp/src/main/java/UciSequenceClassificationExample.java)找到。
- en: After cloning our GitHub repository, navigate to the `Java-Deep-Learning-Cookbook/07_Constructing_LSTM_Neural_network_for_sequence_classification/sourceCode`
    directory. Then import the `cookbookapp` project as a Maven projectby importing `pom.xml`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆我们的GitHub仓库后，导航到`Java-Deep-Learning-Cookbook/07_Constructing_LSTM_Neural_network_for_sequence_classification/sourceCode`目录。然后将`cookbookapp`项目作为Maven项目导入，通过导入`pom.xml`。
- en: Download the data from this UCI website: [https://archive.ics.uci.edu/ml/machine-learning-databases/synthetic_control-mld/synthetic_control.data](https://archive.ics.uci.edu/ml/machine-learning-databases/synthetic_control-mld/synthetic_control.data).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个UCI网站下载数据：[https://archive.ics.uci.edu/ml/machine-learning-databases/synthetic_control-mld/synthetic_control.data](https://archive.ics.uci.edu/ml/machine-learning-databases/synthetic_control-mld/synthetic_control.data)。
- en: 'We need to create directories to store the train and test data. Refer to the
    following directory structure:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建目录来存储训练数据和测试数据。请参阅以下目录结构：
- en: '![](img/2ec9aeb2-e022-40dc-9b40-f463321fc911.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ec9aeb2-e022-40dc-9b40-f463321fc911.png)'
- en: 'We need to create two separate folders for the train and test datasets and
    then create subdirectories for `features` and `labels` respectively:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为训练集和测试集分别创建两个单独的文件夹，然后分别为`features`和`labels`创建子目录：
- en: '![](img/6d26b709-f15b-482c-a21c-82d26854bc27.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d26b709-f15b-482c-a21c-82d26854bc27.png)'
- en: This folder structure is a prerequisite for the aforementioned data extraction. We
    separate features and labels while performing the extraction.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件夹结构是前述数据提取的前提条件。我们在执行提取时会将特征和标签分开。
- en: 'Note that, throughout this cookbook, we are using the DL4J version 1.0.0-beta
    3, except in this chapter. You might come across the following error while executing
    the code that we discuss in this chapter:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在本食谱的所有章节中，我们使用的是DL4J版本1.0.0-beta 3，除了这一章。在执行我们在本章中讨论的代码时，你可能会遇到以下错误：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: At the time of writing, a new version of DL4J has been released that resolves
    the issue. Hence, we will use version 1.0.0-beta 4to run the examples in this
    chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在写作时，DL4J的一个新版本已发布，解决了该问题。因此，我们将使用版本1.0.0-beta 4来运行本章中的示例。
- en: Extracting time series data
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取时间序列数据
- en: We are using another time series use case, but this time we are targeting time
    series univariate sequence classification. ETL needs to be discussed before we
    configure the LSTM neural network. Data extraction is the first phase in the ETL
    process. This recipe covers data extraction for this use case.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用另一个时间序列的用例，但这次我们针对的是时间序列单变量序列分类。在配置LSTM神经网络之前，首先需要讨论ETL。数据提取是ETL过程中的第一阶段。本食谱将涵盖该用例的数据提取。
- en: How to do it...
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Categorize the sequence data programmatically:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用编程方式对序列数据进行分类：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Store the features/labels in their corresponding directories by following the
    numbered format:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照编号格式将特征/标签存储在各自的目录中：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Use `FileUtils` to write the data into files:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`FileUtils`将数据写入文件：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How it works...
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'When we open the synthetic control data after the download, it will look like
    the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下载后，当我们打开合成控制数据时，它将如下所示：
- en: '![](img/32b32679-b82b-4f8c-9f86-99cf9f128d91.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32b32679-b82b-4f8c-9f86-99cf9f128d91.png)'
- en: A single sequence is marked in the preceding screenshot. There are 600 sequences
    in total, and each sequence is separated by a new line. In our example, we can
    split the dataset in such a way that 450 sequences will be used for training and
    the remaining 150 sequences will be used for evaluation. We are trying to categorize
    a given sequence against six known classes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中标记了一个单独的序列。总共有600个序列，每个序列由新的一行分隔。在我们的示例中，我们可以将数据集划分为450个序列用于训练，剩下的150个序列用于评估。我们正试图将给定的序列分类到六个已知类别中。
- en: Note that this is a univariate time series. The data that is recorded in a single
    sequence is spread across different time steps. We create separate files for every
    single sequence. A single data unit (observation) is separated by a space within
    the file. We will replace spaces with new line characters so that measurements
    for every time step in a single sequence will appear on a new line. The first
    100 sequences represent category 1, and the next 100 sequences represent category
    2, and so on. Since we have univariate time series data, there is only one column
    in the CSV files. So, one single feature is recorded over multiple time steps.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是一个单变量时间序列。记录在单个序列中的数据跨越不同的时间步。我们为每个单独的序列创建单独的文件。单个数据单元（观察值）在文件中由空格分隔。我们将空格替换为换行符，以便单个序列中每个时间步的测量值出现在新的一行中。前100个序列代表类别1，接下来的100个序列代表类别2，依此类推。由于我们处理的是单变量时间序列数据，因此CSV文件中只有一列。所以，单个特征在多个时间步上被记录。
- en: In step 1, the `contentAndLabels` list will have sequence-to-label mappings.
    Each sequence represents a label. The sequence and label together form a pair.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1中，`contentAndLabels`列表将包含序列到标签的映射。每个序列代表一个标签。序列和标签一起构成一个对。
- en: 'Now we can have two different approaches to splitting data for training/testing
    purposes:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以采用两种不同的方法来划分数据用于训练/测试：
- en: Randomly shuffle the data and take 450 sequences for training and the remaining
    150 sequences for evaluation/testing purposes.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机打乱数据，选择450个序列用于训练，剩下的150个序列用于评估/测试。
- en: Split the train/test data in such a way that the categories are equally distributed
    across the dataset. For example, we can have 420 sequences of train data with
    70 samples for each of the six categories.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练/测试数据集划分为类别在数据集中的分布均匀。例如，我们可以将训练数据划分为420个序列，每个类别有70个样本，共六个类别。
- en: We use randomization as a measure to increase the generalization power of the
    neural network. Every sequence-to-label pair was written to a separate CSV file
    following the numbered file naming convention.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用随机化作为一种提高神经网络泛化能力的手段。每个序列到标签的对都写入一个单独的CSV文件，遵循编号的文件命名规则。
- en: In step 2, we mention that there are 450 samples for training, and the remaining
    150 are for evaluation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，我们提到训练用的样本为450个，剩余的150个用于评估。
- en: 'In step 3, we use `FileUtils` from the Apache Commons library to write the
    data to a file. The final code will look like the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 3 中，我们使用了来自 Apache Commons 库的`FileUtils`将数据写入文件。最终的代码如下所示：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We fetch the sequence data and add it to the `features` directory, and each
    sequence will be represented by a separate CSV file. Similarly, we add the respective
    labels to a separate CSV file.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取序列数据并将其添加到`features`目录中，每个序列将由一个单独的 CSV 文件表示。类似地，我们将相应的标签添加到单独的 CSV 文件中。
- en: '`1.csv` in the `label` directory will be the respective label for the `1.csv`
    feature in the `feature` directory.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`label` 目录中的`1.csv`将是`feature`目录中`1.csv`特征的对应标签。'
- en: Loading training data
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载训练数据
- en: Data transformation is, as usual, the second phase after data extraction. The
    time series data we're discussing doesn't have any non-numeric fields or noise
    (it had already been cleaned). So we can focus on constructing the iterators from
    the data and loading them directly into the neural network. In this recipe, we
    will load univariate time series data for neural network training. We have extracted
    the synthetic control data and stored it in a suitable format so the neural network
    can process it effortlessly. Every sequence is captured over 60 time steps. In
    this recipe, we will load the time series data into an appropriate dataset iterator,
    which can be fed to the neural network for further processing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换通常是数据提取后的第二个阶段。我们讨论的时间序列数据没有任何非数字字段或噪音（数据已经过清理）。因此，我们可以专注于从数据中构建迭代器，并将其直接加载到神经网络中。在本食谱中，我们将加载单变量时间序列数据用于神经网络训练。我们已经提取了合成控制数据并以合适的格式存储，以便神经网络能够轻松处理。每个序列覆盖了
    60 个时间步。在本食谱中，我们将把时间序列数据加载到适当的数据集迭代器中，供神经网络进行进一步处理。
- en: How to do it...
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何做的...
- en: 'Create a `SequenceRecordReader` instance to extract and load features from
    the time series data:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`SequenceRecordReader`实例，从时间序列数据中提取并加载特征：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create a `SequenceRecordReader` instance to extract and load labels from the
    time series data:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`SequenceRecordReader`实例，从时间序列数据中提取并加载标签：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create sequence readers for testing and evaluation:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为测试和评估创建序列读取器：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Use `SequenceRecordReaderDataSetIterator` to feed the data into our neural
    network:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`SequenceRecordReaderDataSetIterator`将数据输入到我们的神经网络中：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Rewrite the train/test iterator (with `AlignmentMode`) to support time series
    of varying lengths:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重写训练/测试迭代器（使用`AlignmentMode`）以支持不同长度的时间序列：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works...
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'We have used `NumberedFileInputSplit` in step 1\. It is necessary to use `NumberedFileInputSplit`
    to load data from multiple files that follow a numbered file naming convention.
    Refer to step 1 in this recipe:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在步骤 1 中使用了`NumberedFileInputSplit`。必须使用`NumberedFileInputSplit`从多个遵循编号文件命名规则的文件中加载数据。请参阅本食谱中的步骤
    1：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We stored files as a sequence of numbered files in the previous recipe. There
    are 450 files, and each one of them represents a sequence. Note that we have stored
    150 files for testing as demonstrated in step 3.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一个食谱中将文件存储为一系列编号文件。共有 450 个文件，每个文件代表一个序列。请注意，我们已经为测试存储了 150 个文件，如步骤 3 所示。
- en: 'In step 5, `numOfClasses` specifies the number of categories against which
    the neural network is trying to make a prediction. In our example, it is `6`.
    We mentioned `AlignmentMode.ALIGN_END` while creating the iterator. The alignment
    mode deals with input/labels of varying lengths. For example, our time series
    data has 60 time steps, and there''s only one label at the end of the 60^(th)
    time step. That''s the reason why we use `AlignmentMode.ALIGN_END` in the iterator
    definition, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 5 中，`numOfClasses`指定了神经网络试图进行预测的类别数量。在我们的示例中，它是`6`。我们在创建迭代器时提到了`AlignmentMode.ALIGN_END`。对齐模式处理不同长度的输入/标签。例如，我们的时间序列数据有
    60 个时间步，且只有一个标签出现在第 60 个时间步的末尾。这就是我们在迭代器定义中使用`AlignmentMode.ALIGN_END`的原因，如下所示：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can also have time series data that produces labels at every time step. These
    cases refer to many-to-many input/label connections.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以有时间序列数据，在每个时间步产生标签。这些情况指的是多对多的输入/标签连接。
- en: 'In step 4, we started with the regular way of creating iterators, as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 4 中，我们开始使用常规的创建迭代器方式，如下所示：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that this is not the only way to create sequence reader iterators. There
    are multiple implementations available in DataVec to support different configurations.
    We can also align the input/label at the last time step of the sample. For this
    purpose, we added `AlignmentMode.ALIGN_END` into the iterator definition. If there
    are varying time steps, shorter time series will be padded to the length of the
    longest time series. So, if there are samples that have fewer than 60 time steps
    recorded for a sequence, then zero values will be padded to the time series data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这不是创建序列读取器迭代器的唯一方法。DataVec中有多种实现可支持不同的配置。我们还可以在样本的最后时间步对输入/标签进行对齐。为此，我们在迭代器定义中添加了`AlignmentMode.ALIGN_END`。如果时间步长不一致，较短的时间序列将会填充至最长时间序列的长度。因此，如果有样本的时间步少于60步，则会将零值填充到时间序列数据中。
- en: Normalizing training data
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 归一化训练数据
- en: Data transformation alone may not improve the neural network's efficiency. The
    existence of large and small ranges of values within the same dataset can lead
    to overfitting (the model captures noise rather than signals). To avoid these
    situations, we normalize the dataset, and there are multiple DL4J implementations
    to do this. The normalization process converts and fits the raw time series data
    into a definite value range, for example, *(0, 1)*. This will help the neural
    network process the data with less computational effort. We also discussed normalization
    in previous chapters, showing that it will reduce favoritism toward any specific
    label in the dataset while training a neural network.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换本身可能不会提高神经网络的效率。同一数据集中大范围和小范围的值可能会导致过拟合（模型捕捉到噪声而非信号）。为了避免这种情况，我们对数据集进行归一化，DL4J提供了多种实现来完成这一操作。归一化过程将原始时间序列数据转换并拟合到一个确定的值范围内，例如*(0,
    1)*。这将帮助神经网络以更少的计算量处理数据。我们在前面的章节中也讨论了归一化，表明它会减少在训练神经网络时对数据集中特定标签的偏倚。
- en: How to do it...
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create a standard normalizer and fit the data:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建标准归一化器并拟合数据：
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Call the `setPreprocessor()` method to normalize the data on the fly:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`setPreprocessor()`方法以实时规范化数据：
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How it works...
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何工作...
- en: 'In step 1, we used `NormalizerStandardize` to normalize the dataset. `NormalizerStandardize`
    normalizes the data (features) so they have a mean of *0* and a standard deviation
    of *1*. In other words, all the values in the dataset will be normalized within
    the range of *(0, 1)*:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们使用`NormalizerStandardize`来归一化数据集。`NormalizerStandardize`会对数据（特征）进行归一化，使其具有*0*的均值和*1*的标准差。换句话说，数据集中的所有值都将归一化到*(0,
    1)*的范围内：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is a standard normalizer in DL4J, although there are other normalizer implementations
    available in DL4J. Also, note that we don't need to call `fit()` on test data
    because we use the scaling parameters learned during training to scale the test
    data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是DL4J中的标准归一化器，尽管DL4J中还有其他归一化器实现。还请注意，我们不需要对测试数据调用`fit()`，因为我们使用在训练过程中学习到的缩放参数来缩放测试数据。
- en: We need to call the `setPreprocessor()` method as we demonstrated in step 2
    for both train/test iterators. Once we have set the normalizer using `setPreprocessor()`,
    the data returned by the iterator will be auto-normalized using the specified
    normalizer. Hence it is important to call `setPreprocessor()` along with the `fit()`
    method.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要像第2步中展示的那样，为训练/测试迭代器调用`setPreprocessor()`方法。一旦使用`setPreprocessor()`设置了归一化器，迭代器返回的数据将会自动使用指定的归一化器进行归一化。因此，重要的是在调用`fit()`方法时同时调用`setPreprocessor()`。
- en: Constructing input layers for the network
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络的输入层
- en: Layer configuration is an important step in neural network configuration. We
    need to create input layers to receive the univariate time series data that was
    loaded from disk. In this recipe, we will construct an input layer for our use
    case. We will also add an LSTM layer as a hidden layer for the neural network.
    We can use either a computation graph or a regular multilayer network to build
    the network configuration. In most cases, a regular multilayer network is more
    than enough; however, we are using a computation graph for our use case. In this
    recipe, we will configure input layers for the network.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 层配置是神经网络配置中的一个重要步骤。我们需要创建输入层来接收从磁盘加载的单变量时间序列数据。在这个示例中，我们将为我们的用例构建一个输入层。我们还将添加一个LSTM层作为神经网络的隐藏层。我们可以使用计算图或常规的多层网络来构建网络配置。在大多数情况下，常规多层网络就足够了；然而，我们的用例使用的是计算图。在本示例中，我们将为网络配置输入层。
- en: How to do it...
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Configure the neural network with default configurations:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认配置配置神经网络：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Specify the input layer labels by calling `addInputs()`:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`addInputs()`来指定输入层标签：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Add an LSTM layer using the `addLayer()` method:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`addLayer()`方法添加LSTM层：
- en: '[PRE18]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In step 1, we specify the default `seed` values, the initial default weights
    (`weightInit`), the weight `updater`, and so on. We set the gradient normalization
    strategy to `ClipElementWiseAbsoluteValue`. We have also set the gradient threshold
    to `0.5` as an input to the `gradientNormalization` strategy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 1 中，我们指定了默认的`seed`值、初始的默认权重（`weightInit`）、权重`updater`等。我们将梯度规范化策略设置为`ClipElementWiseAbsoluteValue`。我们还将梯度阈值设置为`0.5`，作为`gradientNormalization`策略的输入。
- en: 'The neural network calculates the gradients across neurons at each layer. We
    normalized the input data earlier in the *Normalizing training data* recipe, using
    a normalizer. It makes sense to mention that we need to normalize the gradient
    values to achieve data preparation goals. As we can see in step 1, we have used
    `ClipElementWiseAbsoluteValue` gradient normalization. It works in such a way
    that the absolute value of the gradient cannot be greater than the threshold.
    For example, if the gradient threshold value is 3, then the value range would
    be [-3, 3]. Any gradient values that are less than -5 would be treated as -3 and
    any gradient values that are higher than 3 would be treated as 3\. Gradient values
    in the range [-3, 3] will be unmodified. We have mentioned the gradient normalization
    strategy as well as the threshold in the network configuration, as shown here:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络在每一层计算神经元的梯度。我们在*标准化训练数据*这一部分中已经使用标准化器对输入数据进行了标准化。需要提到的是，我们还需要对梯度值进行标准化，以实现数据准备的目标。如步骤
    1 所示，我们使用了`ClipElementWiseAbsoluteValue`梯度标准化。它的工作方式是使梯度的绝对值不能超过阈值。例如，如果梯度阈值为3，则值的范围为[-3,
    3]。任何小于-3的梯度值都将被视为-3，任何大于3的梯度值将被视为3。范围在[-3, 3]之间的梯度值将保持不变。我们已经在网络配置中提到了梯度标准化策略和阈值，如下所示：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In step 3, the `trainFeatures` label is referred to the input layer label. The
    inputs are basically the graph vertex objects returned by the `graphBuilder()` method.
    The specified LSTM layer name (`L1` in our example) in step 2 will be used while
    configuring the output layer. If there's a mismatch, our program will throw an
    error during execution saying that the layers are configured in such a way that
    they are disconnected. We will discuss this in more depth in the next recipe,
    when we design output layers for the neural network. Note that we have yet to
    add output layers in the configuration.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 3 中，`trainFeatures`标签引用了输入层标签。输入基本上是由`graphBuilder()`方法返回的图顶点对象。步骤 2 中指定的LSTM层名称（我们示例中的`L1`）将在配置输出层时使用。如果存在不匹配，我们的程序将在执行过程中抛出错误，表示层的配置方式导致它们断开连接。我们将在下一个教程中更深入地讨论这个问题，当时我们将设计神经网络的输出层。请注意，我们尚未在配置中添加输出层。
- en: Constructing output layers for the network
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为网络构建输出层
- en: The very next step after the input/hidden layer design is the output layer design.
    As we mentioned in earlier chapters, the output layer should reflect the output
    you want to receive from the neural network. You may need a classifier or a regression
    model depending on the use case. Accordingly, the output layer has to be configured.
    The activation function and error function need to be justified for their use
    in the output layer configuration. This recipe assumes that the neural network
    configuration has been completed up to the input layer definition. This is going
    to be the last step in network configuration.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输入/隐藏层设计之后的下一步是输出层设计。正如我们在前面章节中提到的，输出层应该反映你希望从神经网络中获得的输出。根据使用场景的不同，你可能需要一个分类器或回归模型。因此，输出层必须进行配置。激活函数和误差函数需要根据其在输出层配置中的使用进行合理化。本教程假设神经网络的配置已经完成到输入层定义为止。这将是网络配置中的最后一步。
- en: How to do it...
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Use `setOutputs()` to set the output labels:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`setOutputs()`设置输出标签：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Construct an output layer using the `addLayer()` method and `RnnOutputLayer`:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`addLayer()`方法和`RnnOutputLayer`构造输出层：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How it works...
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In step 1, we have added a `predictSequence` label for the output layer. Note
    that we mentioned the input layer reference when defining the output layer. In
    step 2, we specified it as `L1`, which is the LSTM input layer created in the
    previous recipe. We need to mention this to avoid any errors during execution
    due to disconnection between the LSTM layer and the output layer. Also, the output
    layer definition should have the same layer name we specified in the `setOutput()`
    method.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们为输出层添加了一个`predictSequence`标签。请注意，在定义输出层时，我们提到了输入层的引用。在第2步中，我们将其指定为`L1`，这是在前一个步骤中创建的LSTM输入层。我们需要提到这一点，以避免在执行过程中因LSTM层与输出层之间的断开连接而导致的错误。此外，输出层的定义应该与我们在`setOutput()`方法中指定的层名称相同。
- en: In step 2, we have used `RnnOutputLayer` to construct the output layer. This
    DL4J output layer implementation is used for use cases that involve recurrent
    neural networks. It is functionally the same as `OutputLayer` in multi-layer perceptrons,
    but output and label reshaping are automatically handled.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步中，我们使用`RnnOutputLayer`构建了输出层。这个DL4J输出层实现用于涉及递归神经网络的使用案例。它在功能上与多层感知器中的`OutputLayer`相同，但输出和标签的形状调整是自动处理的。
- en: Evaluating the LSTM network for classified output
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估LSTM网络的分类输出
- en: Now that we have configured the neural network, the next step is to start the
    training instance, followed by evaluation. The evaluation phase is very important
    for the training instance. The neural network will try to optimize the gradients
    for optimal results. An optimal neural network will have good and stable evaluation
    metrics. So it is important to evaluate the neural network to direct the training
    process toward the desired results. We will use the test dataset to evaluate the
    neural network.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经配置好神经网络，下一步是启动训练实例，然后进行评估。评估阶段对训练实例非常重要。神经网络将尝试优化梯度以获得最佳结果。一个最佳的神经网络将具有良好且稳定的评估指标。因此，评估神经网络以将训练过程引导至期望的结果是很重要的。我们将使用测试数据集来评估神经网络。
- en: In the previous chapter, we explored a use case for time series binary classification.
    Now we have six labels against which to predict. We have discussed various ways
    to enhance the network's efficiency. We follow the same approach in the next recipe
    to evaluate the neural network for optimal results.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了时间序列二分类的一个使用案例。现在我们有六个标签进行预测。我们讨论了多种方法来提高网络的效率。在下一步骤中，我们将采用相同的方法，评估神经网络的最佳结果。
- en: How to do it...
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Initialize the `ComputationGraph` model configuration using the `init()` method:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`init()`方法初始化`ComputationGraph`模型配置：
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Set a score listener to monitor the training process:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置分数监听器以监控训练过程：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Start the training instance by calling the `fit()` method:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`fit()`方法启动训练实例：
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Call `evaluate()` to calculate the evaluation metrics:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`evaluate()`计算评估指标：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: How it works...
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, we used a computation graph when configuring the neural network's
    structure. Computation graphs are the best choice for recurrent neural networks.
    We get an evaluation score of approximately 78% with a multi-layer network and
    a whopping 94% while using a computation graph. We get better results with `ComputationGraph` than
    the regular multi-layer perceptron. `ComputationGraph` is meant for complex network
    structures and can be customized to accommodate different types of layers in various
    orders. `InvocationType.EPOCH_END` is used (score iteration) in step 1 to call
    the score iterator at the end of a test iteration.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们在配置神经网络的结构时使用了计算图。计算图是递归神经网络的最佳选择。我们使用多层网络得到的评估得分大约为78%，而使用计算图时得分高达94%。使用`ComputationGraph`可以获得比常规多层感知器更好的结果。`ComputationGraph`适用于复杂的网络结构，并且可以根据不同层的顺序进行定制。第1步中使用了`InvocationType.EPOCH_END`（分数迭代）来在测试迭代结束时调用分数迭代器。
- en: 'Note that we''re calling the score iterator for every test iteration, and not
    for the training set iteration. Proper listeners need to be set by calling `setListeners()`
    before your training event starts to log the scores for every test iteration,
    as shown here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们为每次测试迭代调用了分数迭代器，而不是为训练集迭代调用。为了记录每次测试迭代的分数，需要通过调用`setListeners()`设置适当的监听器，在训练事件开始之前，如下所示：
- en: '[PRE26]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In step 4, the model was evaluated by calling `evaluate()`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步中，模型通过调用`evaluate()`进行了评估：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We passed the test dataset to the `evaluate()` method in the form of an iterator
    that was created earlier in the *Loading the training data* recipe.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将测试数据集以迭代器的形式传递给`evaluate()`方法，这个迭代器是在*加载训练数据*步骤中创建的。
- en: 'Also, we use the `stats()` method to display the results. For a computation
    graph with 100 epochs, we get the following evaluation metrics:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使用`stats()`方法来显示结果。对于一个有100个训练周期（epochs）的计算图，我们得到以下评估指标：
- en: '![](img/fc6f1069-4d8d-4155-9feb-fd4c1c779f11.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc6f1069-4d8d-4155-9feb-fd4c1c779f11.png)'
- en: Now, the following are the experiments you can perform to optimize the results
    even better.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，以下是您可以执行的实验，以便进一步优化结果。
- en: 'We used 100 epochs in our example. Reduce the epochs from 100 or increase this
    setting to a specific value. Note the direction that gives better results. Stop
    when the results are optimal. We can evaluate the results once in every epoch
    to understand the direction in which we can proceed. Check out the following training
    instance logs:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在示例中使用了100个训练周期。您可以将训练周期从100减少，或者将其设置为一个特定的值。注意哪个方向能带来更好的结果。当结果达到最佳时停止。我们可以在每个训练周期结束后评估一次结果，以了解我们应该朝哪个方向继续。请查看以下训练实例日志：
- en: '![](img/5ac34bbc-fc03-4ba3-97ff-27e9597656b9.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ac34bbc-fc03-4ba3-97ff-27e9597656b9.png)'
- en: The accuracy declines after the previous epoch in the preceding example. Accordingly,
    you can decide on the optimal number of epochs. The neural network will simply
    memorize the results if we go for large epochs, and this leads to overfitting.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，准确率在上一个训练周期后下降。因此，您可以决定最佳的训练周期数量。如果我们选择更大的训练周期，神经网络将仅仅记住结果，这会导致过拟合。
- en: Instead of randomizing the data at first, you can ensure that the six categories
    are uniformly distributed across the training set. For example, we can have 420
    samples for training and 180 samples for testing. Then, each category will be
    represented by 70 samples. We can now perform randomization followed by iterator
    creation. Note that we had 450 samples for training in our example. In this case,
    the distribution of labels/categories isn't unique and we are totally relying
    on the randomization of data in this case.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在最开始没有对数据进行随机化时，您可以确保六个类别在训练集中的分布是均匀的。例如，我们可以将420个样本用于训练，180个样本用于测试。这样，每个类别将有70个样本。然后，我们可以进行随机化，并创建迭代器。请注意，在我们的示例中，我们有450个用于训练的样本。在这种情况下，标签/类别的分布并不是唯一的，我们完全依赖于数据的随机化。
