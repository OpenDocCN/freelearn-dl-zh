- en: Chapter 5. Analyzing Sentiment with a Bidirectional LSTM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 5 章：使用双向 LSTM 分析情感
- en: This chapter is a bit more practical to get a better sense of the commonly used
    recurrent neural networks and word embeddings presented in the two previous chapters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章更具实用性，以便更好地了解在前两章中介绍的常用循环神经网络和词嵌入。
- en: It is also an opportunity to introduce the reader to a new application of deep
    learning, sentiment analysis, which is another field of **Natural Language Processing**
    (**NLP**). It is a many-to-one scheme, where a variable-length sequence of words
    has to be assigned to one class. An NLP problem where such a scheme can be used
    similarly is language detection (english, french, german, italian, and so on).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是一个将读者引入深度学习新应用——情感分析的机会，这也是**自然语言处理**（**NLP**）的另一个领域。这是一个多对一的方案，其中一系列可变长度的单词必须分配到一个类别。一个类似可以使用这种方案的NLP问题是语言检测（如英语、法语、德语、意大利语等）。
- en: While the previous chapter demonstrated how to build a recurrent neural network
    from scratch, this chapter shows how a high-level library built on top of Theano,
    Keras, can help implement and train the model with prebuilt modules. Thanks to
    this example, the reader should be able to decide when to use Keras in their projects.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上一章展示了如何从零开始构建循环神经网络，但本章将展示如何使用基于 Theano 构建的高级库 Keras，帮助实现和训练使用预构建模块的模型。通过这个示例，读者应该能够判断何时在项目中使用
    Keras。
- en: 'The following points are developed in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论以下几个要点：
- en: A recap of recurrent neural networks and word embeddings
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络和词嵌入的回顾
- en: Sentiment analysis
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析
- en: The Keras library
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 库
- en: Bidirectional recurrent networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双向循环神经网络
- en: Automated sentiment analysis is the problem of identifying opinions expressed
    in text. It normally involves the classification of text into categories such
    as *positive*, *negative*, and *neutral*. Opinions are central to almost all human
    activities and they are key influencers of our behaviors.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化情感分析是识别文本中表达的意见的问题。它通常涉及将文本分类为*积极*、*消极*和*中性*等类别。意见是几乎所有人类活动的核心，它们是我们行为的关键影响因素。
- en: Recently, neural networks and deep learning approaches have been used to build
    sentiment analysis systems. Such systems have the ability to automatically learn
    a set of features to overcome the drawbacks of handcrafted approaches.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，神经网络和深度学习方法被用于构建情感分析系统。这些系统能够自动学习一组特征，以克服手工方法的缺点。
- en: '**Recurrent Neural Networks** (**RNN**) have been proved in the literature
    to be a very useful technique to represent sequential inputs, such as text. A
    special extension of recurrent neural networks called **Bi-directional Recurrent
    Neural Networks** (**BRNN**) can capture both the preceding and the following
    contextual information in a text.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）在文献中已被证明是表示序列输入（如文本）的非常有用的技术。循环神经网络的一种特殊扩展——**双向循环神经网络**（**BRNN**）能够捕捉文本中的前后上下文信息。'
- en: In this chapter, we'll present an example to show how a bidirectional recurrent
    neural network using the **Long Short Term Memory** (**LSTM**) architecture can
    be used to deal with the problem of the sentiment analysis. We aim to implement
    a model in which, given an input of text (that is, a sequence of words), the model
    attempts to predict whether it is positive, negative, or neutral.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将展示一个示例，演示如何使用**长短时记忆**（**LSTM**）架构的双向循环神经网络来解决情感分析问题。我们的目标是实现一个模型，给定一段文本输入（即一系列单词），该模型试图预测其是积极的、消极的还是中性的。
- en: Installing and configuring Keras
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和配置 Keras
- en: 'Keras is a high-level neural network API, written in Python and capable of
    running on top of either TensorFlow or Theano. It was developed to make implementing
    deep learning models as fast and easy as possible for research and development.
    You can install Keras easily using conda, as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个高级神经网络 API，用 Python 编写，可以在 TensorFlow 或 Theano 上运行。它的开发目的是让实现深度学习模型变得尽可能快速和简单，以便于研究和开发。你可以通过
    conda 轻松安装 Keras，如下所示：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When writing your Python code, importing Keras will tell you which backend
    is used:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写 Python 代码时，导入 Keras 会告诉你使用的是哪个后端：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you have installed Tensorflow, it might not use Theano. To specify which
    backend to use, write a Keras configuration file, `~/.keras/keras.json:`
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经安装了 TensorFlow，它可能不会使用 Theano。要指定使用哪个后端，请编写一个 Keras 配置文件 `~/.keras/keras.json:`。
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It is also possible to specify the Theano backend directly with the environment
    variable:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以直接通过环境变量指定 Theano 后端：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that the device used is the device we specified for Theano in the `~/.theanorc`
    file. It is also possible to modify these variables with Theano environment variables:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所使用的设备是我们在 `~/.theanorc` 文件中为 Theano 指定的设备。也可以通过 Theano 环境变量来修改这些变量：
- en: '[PRE4]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Programming with Keras
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Keras 编程
- en: Keras provides a set of methods for data preprocessing and for building models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了一套数据预处理和构建模型的方法。
- en: 'Layers and models are callable functions on tensors and return tensors. In
    Keras, there is no difference between a layer/module and a model: a model can
    be part of a bigger model and composed of multiple layers. Such a sub-model behaves
    as a module, with inputs/outputs.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 层和模型是对张量的可调用函数，并返回张量。在 Keras 中，层/模块和模型没有区别：一个模型可以是更大模型的一部分，并由多个层组成。这样的子模型作为模块运行，具有输入/输出。
- en: 'Let''s create a network with two linear layers, a ReLU non-linearity in between,
    and a softmax output:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个包含两个线性层、中间加入 ReLU 非线性层并输出 softmax 的网络：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `model` module contains methods to get input and output shape for either
    one or multiple inputs/outputs, and list the submodules of our module:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`model` 模块包含用于获取输入和输出形状的方法，无论是单个输入/输出还是多个输入/输出，并列出我们模块的子模块：'
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In order to avoid specify inputs to every layer, Keras proposes a functional
    way of writing models with the `Sequential` module, to build a new module or model
    composed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免为每一层指定输入，Keras 提出了通过 `Sequential` 模块编写模型的函数式方法，以构建由多个模块或模型组成的新模块或模型。
- en: 'The following definition of the model builds exactly the same model as shown
    previously, with `input_dim` to specify the input dimension of the block that
    would be unknown otherwise and generate an error:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模型定义与之前展示的模型完全相同，使用 `input_dim` 来指定输入维度，否则将无法知道该维度并生成错误：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `model` is considered a module or layer that can be part of a bigger model:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`model` 被视为可以是更大模型的一部分的模块或层：'
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Each module/model/layer can be compiled then and trained with data :'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模块/模型/层都可以进行编译，然后使用数据进行训练：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let us see Keras in practice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实践一下 Keras。
- en: SemEval 2013 dataset
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SemEval 2013 数据集
- en: Let us start by preparing the data. In this chapter, we will use the standard
    dataset used in the supervised task of Twitter sentiment classification (message-level)
    presented in the SemEval 2013 competition. It contains 3662 tweets as a training
    set, 575 tweets as a development set, and 1572 tweets as a testing set. Each sample
    in this dataset consists of the tweet ID, the polarity (positive, negative, or
    neutral) and the tweet.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从准备数据开始。在本章中，我们将使用在 SemEval 2013 竞赛中用于监督任务的 Twitter 情感分类（消息级别）的标准数据集。该数据集包含
    3662 条推文作为训练集，575 条推文作为开发集，1572 条推文作为测试集。该数据集中的每个样本包含推文 ID、极性（正面、负面或中性）和推文内容。
- en: 'Let''s download the dataset:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载数据集：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**A** refers to subtask A, which is message-level sentiment classification
    *our aim of study in this chapter*, where **B** refers to subtask B term level
    sentiment analysis.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**A** 指的是子任务 A，即消息级情感分类 *我们本章研究的目标*，其中 **B** 指的是子任务 B 的术语级情感分析。'
- en: The `input` directories do not contain the labels, just the tweets. `full` contains
    one more level of classification, *subjective* or *objective*. Our interest is
    in the `gold` or `cleansed` directories.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`input` 目录不包含标签，仅包含推文。`full` 目录包含更多级别的分类，*主观* 或 *客观*。我们的关注点是 `gold` 或 `cleansed`
    目录。'
- en: 'Let''s use the script to convert them:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用脚本来转换它们：
- en: '[PRE11]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Preprocessing text data
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本数据预处理
- en: As we know, it is common to use URLs, user mentions, and hashtags frequently
    on Twitter. Thus, first we need to preprocess the tweets as follow.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，在 Twitter 上常常使用 URL、用户提及和话题标签。因此，我们首先需要按照以下步骤预处理推文。
- en: Ensure that all the tokens are separated using the space. Each tweet is lowercased.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 确保所有的标记（tokens）之间使用空格分隔。每条推文都会被转换为小写字母。
- en: 'The URLs, user mentions, and hashtags are replaced by the `<url>`, `<user>`,
    and `<hashtag>` tokens respectively. This step is done using the `process` function,
    it takes a tweet as input, tokenizes it using the NLTK `TweetTokenizer`, preprocesses
    it, and returns the set of words (token) in the tweet:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: URL、用户提及和话题标签分别被 `<url>`、`<user>` 和 `<hashtag>` 代替。此步骤通过 `process` 函数完成，它以推文为输入，使用
    NLTK 的 `TweetTokenizer` 进行分词，进行预处理，并返回推文中的词汇（token）集合：
- en: '[PRE12]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'For example, if we have the tweet `RT @mhj: just an example! :D http://example.com
    #NLP`, the function process is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，如果我们有推文 `RT @mhj: just an example! :D http://example.com #NLP`，该函数的处理过程如下：'
- en: '[PRE13]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: returns
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following function is used to read the datasets and return a list of tuples,
    where each tuple represents one sample of (tweet, class), with the class an integer
    in {0, 1, or 2} defining the polarity:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数用于读取数据集，并返回一个元组列表，每个元组表示一个样本（推文，类别），其中类别是一个整数，取值为 {0, 1 或 2}，定义了情感极性：
- en: '[PRE15]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we can build the vocabulary, which is a dictionary to map each word to
    a fixed index. The following function receives as input a set of data and returns
    the vocabulary and maximum length of the tweets:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以构建词汇表，它是一个字典，用于将每个单词映射到一个固定的索引。以下函数接收一个数据集作为输入，并返回词汇表和推文的最大长度：
- en: '[PRE16]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We also need a function to transfer each tweet or set of tweets into the indices
    based on the vocabulary if the words exist, or replacing **out-of-vocabulary**
    (**OOV**) words with the unknown token (index 0) as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个函数，将每条推文或一组推文转换为基于词汇表的索引，如果单词存在的话，否则用未知标记（索引 0）替换**词汇表外**（**OOV**）的单词，具体如下：
- en: '[PRE17]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can save some memory:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以节省一些内存：
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Keras provides a helper method to pad the sequences to ensure they all have
    the same length, so that a batch of sequences can be represented by a tensor,
    and use optimized operations on tensors, either on a CPU or on a GPU.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了一个辅助方法来填充序列，确保它们具有相同的长度，以便一批序列可以通过张量表示，并在 CPU 或 GPU 上对张量进行优化操作。
- en: 'By default, the method pads at the beginning, which helps get us better classification
    results:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，该方法会在序列开头进行填充，这有助于获得更好的分类结果：
- en: '![Preprocessing text data](img/00085.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![预处理文本数据](img/00085.jpeg)'
- en: '[PRE19]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Lastly, Keras provides a method to convert the classes into their one-hot encoding
    representation, by adding a dimension:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Keras 提供了一个方法，通过添加一个维度，将类别转换为它们的一热编码表示：
- en: '![Preprocessing text data](img/00086.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![预处理文本数据](img/00086.jpeg)'
- en: 'With Keras `to_categorical` method:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Keras 的 `to_categorical` 方法：
- en: '[PRE20]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Designing the architecture for the model
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计模型架构
- en: 'The main blocks of the model in this example will be the following:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例中的模型主要模块如下：
- en: First, the words of the input sentence are mapped to vectors of real numbers.
    This step is called vector representation of words or word embedding (for more
    details, see [Chapter 3](part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 3. Encoding Word into Vector"), *Encoding Word into Vector*).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，输入句子的单词会被映射为实数向量。这个步骤称为词的向量表示或词嵌入（更多细节，请参见[第3章](part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b
    "第3章：将单词编码为向量")，*将单词编码为向量*）。
- en: Afterwards, this sequence of vectors is represented by one fixed-length and
    real-valued vector using a bi-LSTM encoder. This vector summarizes the input sentence
    and contains semantic, syntactic, and/or sentimental information based on the
    word vectors.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，使用双向 LSTM 编码器将这组向量表示为一个固定长度的实值向量。这个向量总结了输入句子，并包含基于词向量的语义、句法和/或情感信息。
- en: Finally, this vector is passed through a softmax classifier to classify the
    sentence into positive, negative, or neutral.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，这个向量通过一个 softmax 分类器，将句子分类为正面、负面或中立。
- en: Vector representations of words
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词的向量表示
- en: Word embeddings are an approach to distributional semantics that represents
    words as vectors of real numbers. Such a representation has useful clustering
    properties, since the words that are semantically and syntactically related are
    represented by similar vectors (see [Chapter 3](part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b
    "Chapter 3. Encoding Word into Vector"), *Encoding Word into Vector*).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是分布式语义学的一种方法，它将单词表示为实数向量。这种表示具有有用的聚类特性，因为在语义和句法上相关的单词会被表示为相似的向量（参见[第3章](part0040_split_000.html#164MG1-ccdadb29edc54339afcb9bdf9350ba6b
    "第3章：将单词编码为向量")，*将单词编码为向量*）。
- en: The main aim of this step is to map each word into a continuous, low-dimensional,
    and real-valued vector, which can later be used as an input to any model. All
    the word vectors are stacked into a matrix ![Vector representations of words](img/00087.jpeg);
    here, *N* is the vocabulary size and d the vector dimension. This matrix is called
    the embedding layer or the lookup table layer. The embedding matrix can be initialized
    using a pre-trained model such as **Word2vec** or **Glove**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步的主要目的是将每个单词映射到一个连续的、低维的实值向量，这些向量可以作为任何模型的输入。所有单词向量被堆叠成一个矩阵 ![单词的向量表示](img/00087.jpeg)；其中，*N*
    是词汇表大小，d 是向量维度。这个矩阵被称为嵌入层或查找表层。嵌入矩阵可以使用预训练模型（如 **Word2vec** 或 **Glove**）进行初始化。
- en: 'In Keras, we can simply define the embedding layer as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，我们可以简单地定义嵌入层，如下所示：
- en: '[PRE21]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The first parameter represents the vocabulary size, `output_dim` is the vector
    dimension, and `input_length` is the length of the input sequences.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数表示词汇表大小，`output_dim` 是向量维度，`input_length` 是输入序列的长度。
- en: 'Let us add this layer as the input layer to the model and declare the model
    as a sequential model:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将此层作为输入层添加到模型中，并声明模型为顺序模型：
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Sentence representation using bi-LSTM
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用双向 LSTM 进行句子表示
- en: A recurrent neural network has the ability to represent sequences such as sentences.
    However, in practice, learning long-term dependencies with a vanilla RNN is difficult
    due to vanishing/exploding gradients. As presented in the previous chapter, **Long
    Short-Term Memory** (**LSTM**) networks were designed to have more persistent
    memory (that is, state), specialized in keeping and transmitting long-term information,
    making them very useful for capturing long-term dependencies between the elements
    of a sequence.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络具有表示序列（如句子）的能力。然而，在实际应用中，由于梯度消失/爆炸问题，使用普通的 RNN 学习长期依赖关系是困难的。如前一章所述，**长短期记忆**（**LSTM**）网络被设计为具有更持久的记忆（即状态），专门用于保持和传递长期信息，这使得它们在捕捉序列中元素之间的长期依赖关系方面非常有用。
- en: LSTM units are the basic components of the model used in this chapter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 单元是本章所用模型的基本组件。
- en: 'Keras proposes a method, `TimeDistributed`, to clone any model in multiple
    time steps and make it recurrent. But for commonly used recurrent units such as
    LSTM, there already exists a module in Keras:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了一种方法 `TimeDistributed`，用于在多个时间步上克隆任何模型并使其具有递归性。但对于常用的递归单元，如LSTM，Keras
    中已经存在一个模块：
- en: '[PRE23]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following is identical:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容相同：
- en: '[PRE24]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'And for the subsequent layers, we do not need to specify the input size (this
    is the case since the LSTM layer comes after the embedding layer), thus we can
    define the `lstm` unit simply as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于后续层，我们无需指定输入大小（这是因为 LSTM 层位于嵌入层之后），因此我们可以简单地定义 `lstm` 单元，如下所示：
- en: '[PRE25]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Last but not least, in this model, we''d like to use a bidirectional LSTM.
    It has proved to lead to better results, capturing the meaning of the current
    word given the previous words, as well as words appearing after:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，在这个模型中，我们希望使用双向 LSTM。它已经证明能够带来更好的结果，在给定前一个词的情况下捕捉当前词的含义，以及在之后出现的词：
- en: '![Sentence representation using bi-LSTM](img/00088.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![使用双向 LSTM 进行句子表示](img/00088.jpeg)'
- en: 'To make this unit process the input bidirectionally, we can simply use Bidirectional,
    a bidirectional wrapper for RNNs:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这个单元以双向方式处理输入，我们可以简单地使用Bidirectional，这是一个针对RNN的双向封装器：
- en: '[PRE26]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Outputting probabilities with the softmax classifier
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 softmax 分类器输出概率
- en: 'Finally, we can pass the vector obtained from `bi_lstm` to a softmax classifier
    as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将从 `bi_lstm` 获得的向量传递给 softmax 分类器，如下所示：
- en: '[PRE27]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, let us print the summary of the model:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打印出模型的摘要：
- en: '[PRE28]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Compiling and training the model
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译和训练模型
- en: Now that the model is defined, it is ready to be compiled. To compile the model
    in Keras, we need to determine the optimizer, the loss function, and optionally
    the evaluation metrics. As we mentioned previously, the problem is to predict
    if the tweet is positive, negative, or neutral. This problem is known as a multi-category
    classification problem. Thus, the loss (or the objective) function that will be
    used in this example is the `categorical_crossentropy`. We will use the `rmsprop`
    optimizer and the accuracy evaluation metric.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型已定义，准备好进行编译。要在 Keras 中编译模型，我们需要确定优化器、损失函数，并可选地指定评估指标。如前所述，问题是预测推文是正面、负面还是中立的。这是一个多类别分类问题。因此，在这个示例中使用的损失（或目标）函数是
    `categorical_crossentropy`。我们将使用 `rmsprop` 优化器和准确率评估指标。
- en: 'In Keras, you can find state-of-the-art optimizers, objectives, and evaluation
    metrics implemented. Compiling the model in Keras is very easy using the compile
    function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，您可以找到实现的最先进的优化器、目标函数和评估指标。使用编译函数在Keras中编译模型非常简单：
- en: '[PRE29]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We have defined the model and compiled it, and it is now ready to be trained.
    We can train or fit the model on the defined data by calling the fit function.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义并编译了模型，现在它已经准备好进行训练。我们可以通过调用fit函数在定义的数据上训练或拟合模型。
- en: 'The training process runs for a certain number of iterations through the dataset,
    called epochs, which can be specified using the `epochs` parameter. We can also
    set the number of instances that are fed to the model at each step using the `batch_size`
    argument. In this case, we will use a small number of `epochs` = `30` and use
    a small batch size of `10`. We can also evaluate the model during training by
    explicitly feeding the development set using the `validation_data` parameter,
    or choosing a sub set from the training set using the `validation_split` parameter.
    In this case, we will use the development set that we defined previously:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程会经过若干次数据集迭代，称为epochs，可以通过`epochs`参数来指定。我们还可以使用`batch_size`参数设置每次训练时输入给模型的实例数。在本例中，我们将使用较小的`epochs`
    = `30`，并使用较小的批次大小`10`。我们还可以通过显式地使用`validation_data`参数输入开发集来在训练过程中评估模型，或者通过`validation_split`参数选择训练集的一个子集。在本例中，我们将使用之前定义的开发集：
- en: '[PRE30]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Evaluating the model
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'We have trained the model on the train test and now we can evaluate the performance
    of the network on the test set. This can be done using the `evaluation()` function.
    This function returns the loss value and the metrics values for the model in test
    mode:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在训练集上训练了模型，现在可以评估网络在测试集上的性能。可以使用`evaluation()`函数来完成这一操作。该函数返回模型在测试模式下的损失值和指标值：
- en: '[PRE31]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Saving and loading the model
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存和加载模型
- en: 'To save the weights of the Keras model, simply call the `save` function, and
    the model is serialized into `.hdf5` format:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 要保存Keras模型的权重，只需调用`save`函数，模型将序列化为`.hdf5`格式：
- en: '[PRE32]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To load the model, use the `load_model` function provided by Keras as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载模型，请使用Keras提供的`load_model`函数，如下所示：
- en: '[PRE33]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'It is now ready for evaluation and does not need to be compiled. For example,
    on the same test set we must obtain the same results:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 它现在已经准备好进行评估，并且无需重新编译。例如，在相同的测试集上，我们必须获得相同的结果：
- en: '[PRE34]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Running the example
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行示例
- en: 'To run the model, we can execute the following command line:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行模型，我们可以执行以下命令行：
- en: '[PRE35]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Further reading
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Please refer to the following articles:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下文章：
- en: '*SemEval Sentiment Analysis in Twitter* [https://www.cs.york.ac.uk/semeval-2013/task2.html](https://www.cs.york.ac.uk/semeval-2013/task2.html)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SemEval Sentiment Analysis in Twitter* [https://www.cs.york.ac.uk/semeval-2013/task2.html](https://www.cs.york.ac.uk/semeval-2013/task2.html)'
- en: '*Personality insights with IBM Watson demo* [https://personality-insights-livedemo.mybluemix.net/](https://personality-insights-livedemo.mybluemix.net/)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Personality insights with IBM Watson demo* [https://personality-insights-livedemo.mybluemix.net/](https://personality-insights-livedemo.mybluemix.net/)'
- en: '*Tone analyzer* [https://tone-analyzer-demo.mybluemix.net/](https://tone-analyzer-demo.mybluemix.net/)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Tone analyzer* [https://tone-analyzer-demo.mybluemix.net/](https://tone-analyzer-demo.mybluemix.net/)'
- en: '*Keras* [https://keras.io/](https://keras.io/)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Keras* [https://keras.io/](https://keras.io/)'
- en: 'Deep Speech: Scaling up end-to-end speech recognition, Awni Hannun, Carl Case,
    Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev
    Satheesh, Shubho Sengupta, Adam Coates, Andrew Y. Ng, 2014'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deep Speech: 扩展端到端语音识别，Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro,
    Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam
    Coates, Andrew Y. Ng, 2014'
- en: Speech Recognition with Deep Recurrent Neural Networks, Alex Graves, Abdel-Rahman
    Mohamed, Geoffrey Hinton, 2013
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度递归神经网络语音识别，Alex Graves, Abdel-Rahman Mohamed, Geoffrey Hinton, 2013
- en: 'Deep Speech 2: End-to-End Speech Recognition in English and Mandarin, Dario
    Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro,
    Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse
    Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick
    LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan
    Raiman, Sanjeev Satheesh,David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang,
    Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu, 2015'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deep Speech 2：英语和普通话的端到端语音识别，作者：Dario Amodei, Rishita Anubhai, Eric Battenberg,
    Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam
    Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner,
    Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang,
    Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David
    Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama,
    Jun Zhan, Zhenyao Zhu，2015
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter acted as a review of the basic concepts introduced in the previous
    chapters, while introducing a new application, sentiment analysis, and a high-level
    library, Keras, to simplify the development of models with the Theano engine.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 本章回顾了前几章介绍的基本概念，同时介绍了一种新应用——情感分析，并介绍了一个高层次的库 Keras，旨在简化使用 Theano 引擎开发模型的过程。
- en: Among these basic concepts were recurrent networks, word embeddings, batch sequence
    padding, and class one-hot encoding. Bidirectional recurrency was presented to
    improve the results.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基本概念包括循环网络、词嵌入、批量序列填充和类别独热编码。为了提高结果，提出了双向递归。
- en: In the next chapter, we'll see how to apply recurrency to images, with another
    library, Lasagne, which is more lightweight than Keras, and will let you mix the
    library modules with your own code for Theano more smoothly.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何将递归应用于图像，使用一个比 Keras 更轻量的库 Lasagne，它能让你更顺利地将库模块与自己的 Theano 代码结合。
