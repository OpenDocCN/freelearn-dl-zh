- en: Caption Generation for Images
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像的描述生成
- en: Caption generation is one of the most important applications in the field of
    deep learning and has gained quite a lot of interest recently. Image captioning
    models involve a combination of both visual information along with natural language
    processing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述生成是深度学习领域最重要的应用之一，近年来获得了相当大的关注。图像描述模型结合了视觉信息和自然语言处理。
- en: 'In this chapter, we will learn about:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将学习：
- en: Recent advancements in the field of the caption generation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像描述生成领域的最新进展
- en: How caption generation works
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像描述生成的工作原理
- en: Implementation of caption generation models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像描述生成模型的实现
- en: What is caption generation?
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是图像描述生成？
- en: Caption generation is the task of describing an image with natural language.
    Previously, caption generation models worked on object detection models combined
    with templates that were used to generate text for detected objects. With all
    the advancements in deep learning, these models have been replaced with a combination
    of convolutional neural networks and recurrent neural networks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述生成是用自然语言描述图像的任务。以前，描述生成模型是基于物体检测模型与模板的组合，这些模板用于为检测到的物体生成文本。随着深度学习的进步，这些模型已经被卷积神经网络和循环神经网络的组合所取代。
- en: 'An example is shown as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例：
- en: '![](img/41534487-3f30-46d0-94e2-2c346a61bb88.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41534487-3f30-46d0-94e2-2c346a61bb88.png)'
- en: Source: https://arxiv.org/pdf/1609.06647.pdf
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/pdf/1609.06647.pdf](https://arxiv.org/pdf/1609.06647.pdf)
- en: There are several datasets that help us create image captioning models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个数据集帮助我们创建图像描述模型。
- en: Exploring image captioning datasets
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索图像描述数据集
- en: Several datasets are available for captioning image task. The datasets are usually
    prepared by showing an image to a few persons and asking them to write a sentence
    each about the image. Through this method, several captions are generated for
    the same image. Having multiple options of captions helps in better generalization.
    The difficulty lies in the ranking of model performance. For each generation,
    preferably, a human has to evaluate the caption. Automatic evaluation is difficult
    for this task. Let's explore the `Flickr8` dataset.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个数据集可用于图像描述任务。这些数据集通常通过向几个人展示一张图片，并要求他们分别写出一段关于该图片的描述来准备。通过这种方法，同一张图片会生成多个描述。拥有多个描述选项有助于更好的泛化能力。难点在于模型性能的排序。每次生成后，最好由人类来评估描述。对于这个任务，自动评估是困难的。让我们来探索一下`Flickr8`数据集。
- en: Downloading the dataset
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载数据集
- en: '`Flickr8` is gathered from Flickr and is not permitted for commercial usage.
    Download the `Flickr8` dataset from [https://forms.illinois.edu/sec/1713398](https://forms.illinois.edu/sec/1713398).
    The descriptions can be found at [http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html](http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html).
    Download the text and images separately. Access to it can be obtained by filling
    in a form shown on the page:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`Flickr8`数据集来自Flickr，禁止用于商业用途。你可以从[https://forms.illinois.edu/sec/1713398](https://forms.illinois.edu/sec/1713398)下载`Flickr8`数据集。描述可以在[http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html](http://nlp.cs.illinois.edu/HockenmaierGroup/8k-pictures.html)找到。请分别下载文本和图片。通过填写页面上的表格，你可以获取访问权限：'
- en: '![](img/4cffe58f-56d6-4303-ab4a-5d017ebb7257.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4cffe58f-56d6-4303-ab4a-5d017ebb7257.png)'
- en: 'An email will be sent with the download link. Once downloaded and extracted,
    the files should be like this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下载链接将通过电子邮件发送。下载并解压后，文件应该是这样的：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following are a couple of examples given in the dataset:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数据集中给出的几个示例：
- en: '![](img/d427f636-3f7d-4499-a65c-cb6e45b86f7e.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d427f636-3f7d-4499-a65c-cb6e45b86f7e.jpg)'
- en: 'The preceding figure shows the following components:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了以下组件：
- en: A man in street racer armor is examining the tire of another racer's motor bike
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一名穿着街头赛车盔甲的男子正在检查另一名赛车手的摩托车轮胎
- en: The two racers drove the white bike down the road
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两名赛车手骑着白色自行车沿着道路行驶
- en: Two motorists are riding along on their vehicle that is oddly designed and colored
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两名摩托车手正骑着一辆设计奇特且颜色鲜艳的车辆
- en: Two people are in a small race car driving by a green hill
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个人在小型赛车中驾驶，驶过绿色的山丘
- en: Two people in racing uniforms in a street car
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两名穿着赛车服的人员在街车中
- en: 'The following is example two:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是第二个示例：
- en: '![](img/a07da3d8-9578-4836-91b7-a236cc829a20.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a07da3d8-9578-4836-91b7-a236cc829a20.jpg)'
- en: 'The preceding figure shows the following components:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了以下组件：
- en: A man in a black hoodie and jeans skateboards down a railing
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一名穿着黑色连帽衫和牛仔裤的男子在扶手上滑板
- en: A man skateboards down a steep railing next to some steps
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一名男子正在滑板沿着一根陡峭的栏杆下滑，旁边有一些台阶
- en: A person is sliding down a brick rail on a snowboard
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个人正在用滑雪板滑下一个砖制栏杆
- en: A person walks down the brick railing near a set of steps
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个人正走下靠近台阶的砖制栏杆
- en: A snowboarder rides down a handrail without snow
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一名滑雪者正在没有雪的栏杆上滑行
- en: As you can see, there are different captions provided for one image. The captions
    show the difficulty of the image captioning task.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，对于一张图像提供了不同的标题。这些标题展示了图像标题生成任务的难度。
- en: Converting words into embeddings
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将单词转化为嵌入
- en: English words have to be converted into embeddings for caption generation. An
    embedding is nothing but a vector or numerical representation of words or images.
    It is useful if words are converted to a vector form such that arithmetic can
    be performed using the vectors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 英文单词必须转换为嵌入才能生成标题。嵌入其实就是单词或图像的向量或数值表示。如果将单词转换为向量形式，则可以使用向量进行运算。
- en: 'Such an embedding can be learned by two methods, as shown in the following
    figure:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种嵌入可以通过两种方法进行学习，如下图所示：
- en: '![](img/6d11a280-a213-4e63-80a6-2ec7971c36b3.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d11a280-a213-4e63-80a6-2ec7971c36b3.png)'
- en: 'The **CBOW** method learns the embedding by predicting a word given the surrounding
    words. The **Skip-gram** method predicts the surrounding words given a word, which
    is the reverse of **CBOW**. Based on the history, a target word can be trained,
    as shown in the following figure:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**CBOW**方法通过预测给定上下文单词来学习嵌入。**Skip-gram**方法则是给定一个单词，预测其上下文单词，是**CBOW**的逆过程。基于历史数据，可以训练目标单词，如下图所示：'
- en: '![](img/918d0423-dbd7-4279-87d3-9574d66af009.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/918d0423-dbd7-4279-87d3-9574d66af009.png)'
- en: 'Once trained, the embedding can be visualized as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，嵌入可以如下所示进行可视化：
- en: '![](img/63a42ddb-81c2-4564-9f58-f9fd2d53ce4b.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63a42ddb-81c2-4564-9f58-f9fd2d53ce4b.png)'
- en: Visualization of words
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 单词的可视化
- en: This type of embedding can be used to perform vector arithmetic of words. This
    concept of word embedding will be helpful throughout this chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的嵌入可以用来执行单词的向量运算。单词嵌入的概念在本章中将会非常有帮助。
- en: Image captioning approaches
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像标题生成方法
- en: There are several approaches to captioning images. Earlier methods used to construct
    a sentence based on the objects and attributes present in the image. Later, **recurrent
    neural networks** (**RNN**) were used to generate sentences. The most accurate
    method uses the attention mechanism. Let's explore these techniques and results
    in detail in this section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以进行图像标题生成。早期的方法是基于图像中的物体和属性来构建句子。后来，**循环神经网络**（**RNN**）被用来生成句子。最准确的方法是使用注意力机制。让我们在本节中详细探讨这些技术和结果。
- en: Conditional random field
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件随机场
- en: 'Initially a method was tried with the **conditional random field** (**CRF**)
    constructing the sentence with the objects and attributes detected in the image.
    The steps involved in this process are shown as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最初尝试了一种方法，使用**条件随机场**（**CRF**）通过图像中检测到的物体和属性构建句子。此过程的步骤如下所示：
- en: '![](img/7418c864-3eaf-4ee1-9448-87f293d689ce.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7418c864-3eaf-4ee1-9448-87f293d689ce.png)'
- en: 'System flow for an example images (Source: http://www.tamaraberg.com/papers/generation_cvpr11.pdf)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 示例图像的系统流程（来源：[http://www.tamaraberg.com/papers/generation_cvpr11.pdf](http://www.tamaraberg.com/papers/generation_cvpr11.pdf)）
- en: 'CRF has limited ability to come up with sentences in a coherent manner. The
    quality of generated sentences is not great, as shown in the following screenshot:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**CRF** 在生成连贯句子方面的能力有限。生成的句子的质量不高，如以下截图所示：'
- en: '![](img/c80448c0-73d4-402c-8c3c-5167ac91119a.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c80448c0-73d4-402c-8c3c-5167ac91119a.png)'
- en: The sentences shown here are too structured despite getting the objects and
    attributes correct.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经正确地获取了物体和属性，但这里展示的句子仍然过于结构化。
- en: Kulkarni et al., in the paper [http://www.tamaraberg.com/papers/generation_cvpr11.pdf](http://www.tamaraberg.com/papers/generation_cvpr11.pdf),
    proposed a method of finding the objects and attributes from an image and using it
    to generate text with a **conditional random field** (**CRF**).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Kulkarni 等人在论文 [http://www.tamaraberg.com/papers/generation_cvpr11.pdf](http://www.tamaraberg.com/papers/generation_cvpr11.pdf)
    中提出了一种方法，通过从图像中找到物体和属性，利用**条件随机场**（**CRF**）生成文本。
- en: Recurrent neural network on convolution neural network
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络上的循环神经网络
- en: 'A recurrent neural network can be combined with convolutional neural network
    features to produce new sentences. This enables end-to-end training of the models.
    The following is the architecture of such a model:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将循环神经网络与卷积神经网络特征结合起来生成新的句子。这使得模型能够进行端到端训练。以下是该模型的架构：
- en: '![](img/c66418d9-a2bd-4cf8-8c61-a50790b04846.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c66418d9-a2bd-4cf8-8c61-a50790b04846.png)'
- en: 'LSTM model (Source: https://arxiv.org/pdf/1411.4555.pdf)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM模型（来源：[https://arxiv.org/pdf/1411.4555.pdf](https://arxiv.org/pdf/1411.4555.pdf)）
- en: 'There are several layers of **LSTM** used to produce the desired results. A
    few of the results produced by this model are shown in the following screenshot:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了多个**LSTM**层来生成期望的结果。以下是该模型生成的一些结果截图：
- en: '![](img/477f57f6-4861-4c3d-912a-e77913dd64b6.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/477f57f6-4861-4c3d-912a-e77913dd64b6.png)'
- en: 'Source: https://arxiv.org/pdf/1411.4555.pdf'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/pdf/1411.4555.pdf](https://arxiv.org/pdf/1411.4555.pdf)
- en: These results are better than the results produced by CRF. This shows the power
    of LSTM in generating sentences.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果优于CRF生成的结果。这展示了LSTM在生成句子方面的强大能力。
- en: 'Reference: Vinyals et al., in the paper [https://arxiv.org/pdf/1411.4555.pdf](https://arxiv.org/pdf/1411.4555.pdf),
    proposed an end to end trainable deep learning for image captioning, which has
    CNN and RNN stacked back to back.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 参考：Vinyals等人在论文中提出了[https://arxiv.org/pdf/1411.4555.pdf](https://arxiv.org/pdf/1411.4555.pdf)，提出了一种端到端可训练的深度学习图像标注方法，其中CNN和RNN堆叠在一起。
- en: Caption ranking
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标题排序
- en: 'Caption ranking is an interesting way of selecting a caption from a set of
    captions. First, the images are ranked according to their features and corresponding
    captions are picked, as shown in this screenshot:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 标题排序是一种从一组标题中选择一个标题的有趣方法。首先，根据图像的特征对其进行排序，并选择相应的标题，如下图所示：
- en: '![](img/1d6c5f49-2c4f-4ad7-8806-66b9de6d2775.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d6c5f49-2c4f-4ad7-8806-66b9de6d2775.png)'
- en: 'Source: http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf](http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf)
- en: 'The top images can be re-ranked using a different set of attributes. By getting
    more images, the quality can improve a lot as shown in the following screenshot:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用不同的属性集合，顶部图像可以重新排序。通过获取更多的图像，质量可以大幅提高，正如以下截图所示：
- en: '![](img/8390766c-a13d-47c9-949c-39f9bd46af62.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8390766c-a13d-47c9-949c-39f9bd46af62.png)'
- en: 'Source: http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf](http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf)
- en: The results are better with an increase in the number of images in the dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据集中文件数量的增加，结果得到了改善。
- en: To learn more about caption ranking, refer: [http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf](http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于标题排序的信息，请参考：[http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf](http://papers.nips.cc/paper/4470-im2text-describing-images-using-1-million-captioned-photographs.pdf)
- en: Dense captioning
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 密集标注
- en: 'Dense captioning is the problem of multiple captions on a single image. The
    following is the architecture of the problem:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 密集标注是一个问题，即单张图像上有多个标题。以下是该问题的架构：
- en: '![](img/96c1aa2c-16be-47f6-8bb5-8d8039b2d61b.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96c1aa2c-16be-47f6-8bb5-8d8039b2d61b.png)'
- en: 'Source: https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf)
- en: This architecture produces good results.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构产生了良好的结果。
- en: For more understanding refer: Johnson et al., in the paper [https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf),
    proposed a method for dense captioning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多，请参考：Johnson等人在论文中提出了[https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf)，提出了一种密集标注方法。
- en: RNN captioning
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 标注
- en: The visual features can be used with sequence learning to form the output.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 可将视觉特征与序列学习结合，形成输出。
- en: '![](img/5bd7bd01-b1a9-43a4-a024-b30cb6a4984d.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bd7bd01-b1a9-43a4-a024-b30cb6a4984d.png)'
- en: This is an architecture for generating captions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个生成标题的架构。
- en: For details, refer: Donahue et al., in the paper [https://arxiv.org/pdf/1411.4389.pdf](https://arxiv.org/pdf/1411.4389.pdf),
    proposed **Long-term recurrent convolutional architectures** (**LRCN**) for the task of image captioning.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 详情请参考：Donahue等人在论文中，[https://arxiv.org/pdf/1411.4389.pdf](https://arxiv.org/pdf/1411.4389.pdf)
    提出了**长时记忆卷积神经网络**（**LRCN**）用于图像标题生成任务。
- en: Multimodal captioning
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态标题生成
- en: Both the image and text can be mapped to the same embedding space to generate
    a caption.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和文本都可以映射到同一个嵌入空间，以生成标题。
- en: '![](img/32ad2629-93d0-4a4f-bf63-69e56fc63f33.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32ad2629-93d0-4a4f-bf63-69e56fc63f33.png)'
- en: A decoder is required to generate the caption.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 需要一个解码器来生成标题。
- en: Attention-based captioning
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于注意力的标题生成
- en: For detailed learning, refer: Xu et al., in the paper, [https://arxiv.org/pdf/1502.03044.pdf](https://arxiv.org/pdf/1502.03044.pdf),
    proposed a method for image captioning using an **attention mechanism**.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于详细学习，参考：Xu等人在论文中，[https://arxiv.org/pdf/1502.03044.pdf](https://arxiv.org/pdf/1502.03044.pdf)
    提出了使用**注意力机制**的图像标题生成方法。
- en: 'Attention-based captioning has become popular recently as it provides better
    accuracy:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的标题生成最近变得流行，因为它提供了更好的准确度：
- en: '![](img/c6f17ce4-f2c2-4668-86e6-2424644885af.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6f17ce4-f2c2-4668-86e6-2424644885af.png)'
- en: 'This method trains an attention model in the sequence of the caption, thereby
    producing better results:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法按标题的顺序训练一个注意力模型，从而产生更好的结果：
- en: '![](img/79f3a9f7-3f04-4a27-9375-4f82267d442e.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79f3a9f7-3f04-4a27-9375-4f82267d442e.png)'
- en: 'Here is a diagram of **LSTM** with attention-generating captions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个带有注意力生成标题的**LSTM**图示：
- en: '![](img/5f0220af-d7d0-4281-b1ba-092ef70358f6.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f0220af-d7d0-4281-b1ba-092ef70358f6.png)'
- en: 'There are several examples shown here, with an excellent visualization of objects
    unfolding in a time series manner:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了几个示例，并且通过时间序列方式非常好地可视化了对象的展开：
- en: '![](img/ed973502-76a8-4952-8809-e25aef013f7a.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed973502-76a8-4952-8809-e25aef013f7a.png)'
- en: Unfolding objects in time series manner
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以时间序列方式展开对象
- en: The results are really excellent!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 结果非常出色！
- en: Implementing a caption generation model
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个标题生成模型
- en: 'First, let''s read the dataset and transform it the way we need. Import the `os`
    library and declare the directory in which the dataset is present, as shown in
    the following code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，读取数据集并按需要进行转换。导入`os`库并声明数据集所在的目录，如以下代码所示：
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, define a function to open a file and return the lines present in the
    file as a list:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义一个函数来打开文件并返回文件中的行作为列表：
- en: '[PRE2]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Read the image paths of the training and testing datasets followed by the captions
    file:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 读取训练和测试数据集的图片路径，并加载标题文件：
- en: '[PRE3]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This should print the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该打印出以下内容：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, the image-to-caption map has to be generated. This will help in training
    for easily looking up captions. Also, unique words present in the caption dataset
    will help to create the vocabulary:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，需要生成图像到标题的映射。这将帮助训练中更方便地查找标题。此外，标题数据集中独特的词汇有助于创建词汇表：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, two maps have to be formed. One is word to index and the other is index
    to word map:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，需要生成两个映射。一个是词到索引，另一个是索引到词的映射：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The maximum number of words present in a caption is 38, which will help in
    defining the architecture. Next, import the libraries:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 标题中出现的最大词数为38，这将有助于定义架构。接下来，导入所需的库：
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now create the `ImageModel` class for loading the VGG model with weights:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建`ImageModel`类来加载带有权重的VGG模型：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The weights are downloaded and stored. It may take some time at the first attempt.
    Next, a separate model is created so that a second fully connected layer is predicted.
    The following is a method for reading an image from a path and preprocessing:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 权重已下载并存储。第一次尝试时可能需要一些时间。接下来，创建一个单独的模型，以便预测第二个全连接层。以下是从路径读取图像并进行预处理的方法：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, define a method to load the image and do prediction. The predicted second
    fully connected layer can be reshaped to `4096`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义一个方法来加载图像并进行预测。预测的第二个全连接层可以重新调整为`4096`：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Go through a list of image paths and create a list of features:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历图片路径列表并创建特征列表：
- en: '[PRE11]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, store the extracted features as a pickle file:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将提取的特征存储为pickle文件：
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, initialize the class and extract both training and testing image features:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，初始化类并提取训练和测试图片特征：
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Import the layers required to construct the model:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 导入构建模型所需的层：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Get the vocabulary required:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 获取所需的词汇表：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For the final caption generation model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最终的标题生成模型：
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For the language, a model is created:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言，创建一个模型：
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The two different models are merged to form the final model:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 两种不同的模型被合并以形成最终模型：
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This model can be trained to generate captions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型可以被训练生成描述。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we learned image captioning techniques. First, we understood
    the embedding space of word vectors. Then, several approaches for image captioning
    were learned. Then came the implementation of the image captioning model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了图像描述技术。首先，我们理解了词向量的嵌入空间。然后，我们学习了几种图像描述的方法。接着，开始了图像描述模型的实现。
- en: In the next chapter, we will take a look at the concept of **Generative Adversarial Networks**
    (**GAN**). GANs are intriguing and useful for generating images for various purposes.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨**生成对抗网络**（**GAN**）的概念。GAN非常有趣，并且在生成各种用途的图像方面非常有用。
