- en: Building Neural Networks with CNTK
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNTK构建神经网络
- en: In the previous chapter, we talked about what deep learning is, and how neural
    networks work on a conceptual level. Finally, we talked about CNTK, and how to
    get it installed on your machine. In this chapter, we will build our first neural
    network with CNTK and train it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了深度学习是什么，以及神经网络如何在概念层面上运作。最后，我们讨论了CNTK以及如何在你的机器上安装它。在本章中，我们将使用CNTK构建并训练我们的第一个神经网络。
- en: We will look at building a neural network using the different functions and
    classes from the CNTK library. We will do this with a basic classification problem.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用CNTK库中的不同函数和类来构建一个神经网络。我们将通过一个基本的分类问题来实现。
- en: Once we have a neural network for our classification problem, we will train
    it with sample data obtained from an open dataset. After our neural network is
    trained, we will look at how to use it to make predictions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们为分类问题构建了一个神经网络，我们将使用从开放数据集中获取的样本数据对其进行训练。训练完神经网络后，我们将学习如何使用它来进行预测。
- en: At the end of this chapter, we will spend some time talking about ways to improve
    your model once you've trained it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们将花一些时间讨论在训练完模型后，如何提高模型性能。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下主题：
- en: Basic neural network concepts in CNTK
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNTK中的基本神经网络概念
- en: Building your first neural network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建你的第一个神经网络
- en: Training the neural network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: Making predictions with a neural network
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络进行预测
- en: Improving the model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进模型
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will work on a sample model, built using Python in a Jupyter
    notebook. Jupyter is an open source technology that allows you to create interactive
    web pages that contain sections of Python code, Markdown, and HTML. It makes it
    much easier to document your code and assumptions you made while building your
    deep learning model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将处理一个使用Python在Jupyter笔记本中构建的示例模型。Jupyter是一种开源技术，它允许你创建包含Python代码、Markdown和HTML的交互式网页。这使得文档化代码以及记录在构建深度学习模型时所做的假设变得更加容易。
- en: 'If you''ve installed Anaconda using the steps defined in [Chapter 1](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml),
    *Getting Started with CNTK*, you already have Jupyter installed on your machine.
    Should you not have Anaconda yet, you can download it from: [https://anacondacloud.com/download](https://www.anaconda.com/download/).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你按照[第1章](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml)中定义的步骤安装了Anaconda，*开始使用CNTK*，那么你已经在你的机器上安装了Jupyter。如果你还没有安装Anaconda，你可以从以下网址下载：[https://anacondacloud.com/download](https://www.anaconda.com/download/)。
- en: 'You can get the sample code for this chapter from: [https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch2](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch2).
    To run the sample code, run the following commands inside a Terminal in the directory
    where you downloaded the sample code:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从以下网址获取本章的示例代码：[https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch2](https://github.com/PacktPublishing/Deep-Learning-with-Microsoft-Cognitive-Toolkit-Quick-Start-Guide/tree/master/ch2)。要运行示例代码，请在下载代码的目录中通过终端运行以下命令：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Look for the `Train your first model.ipynb` notebook, and click it to open up
    the sample code. You can execute all the code in one step by choosing Cell | Run
    All. This will execute all the steps in the notebook.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 找到`Train your first model.ipynb`笔记本，点击它以打开示例代码。你可以选择Cell | Run All一次性执行所有代码，这将执行笔记本中的所有步骤。
- en: 'Check out the following video to see the code in action:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，了解代码的实际运行：
- en: '[http://bit.ly/2YoyNKY](http://bit.ly/2YoyNKY)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2YoyNKY](http://bit.ly/2YoyNKY)'
- en: Basic neural network concepts in CNTK
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNTK中的基本神经网络概念
- en: In the previous chapter, we looked at the basic concepts of a neural network.
    Let's map the concepts we've learned to components in the CNTK library, and discover
    how you can use these concepts to build your own model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了神经网络的基本概念。现在我们将这些概念映射到CNTK库中的组件，并探索如何利用这些概念来构建你自己的模型。
- en: Building neural networks using layer functions
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用层函数构建神经网络
- en: 'Neural networks are made using several layers of neurons. In CNTK, we can model
    the layers of a neural network using layer functions defined in the layers module.
    A `layer` function in CNTK looks like a regular function. For example, you can
    create the most basic layer type, `Dense`, with one line of code:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是由多个神经元层组成的。在CNTK中，我们可以使用在layers模块中定义的层函数来建模神经网络的各个层。CNTK中的`layer`函数看起来就像一个普通的函数。例如，你可以通过一行代码创建最基本的层类型`Dense`：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To Create the most basic layer type following the given steps:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤创建最基本的层类型：
- en: First, import the `Dense` layer function from the layers package
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从layers包中导入`Dense`层函数
- en: Next, import the `input_variable` function from the `cntk` root package
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从`cntk`根包中导入`input_variable`函数
- en: Create a new input variable with the name features using the `input_variable`
    function and give it a size of `100`
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`input_variable`函数创建一个名为features的新输入变量，并将其大小设置为`100`
- en: Create a new layer using the `Dense` function providing it with the number of
    neurons you want
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Dense`函数创建一个新层，提供所需的神经元数量
- en: Invoke the configured `Dense` layer function providing the features variable
    to connect the `Dense` layer to the input
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用配置好的`Dense`层函数，提供特征变量，以将`Dense`层与输入连接起来
- en: Working with layers in CNTK has a distinct functional programming feel to it.
    When we look at the previous chapter, we can understand why CNTK has gone down
    this route. Ultimately, every layer in a neural network is a mathematical function.
    All the layer functions in CNTK produce a mathematical function with a set of
    predefined parameters. Invoke the function again, and you bind the last missing
    parameter, the input, to the layer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNTK中处理层具有鲜明的函数式编程风格。当我们回顾前一章时，我们可以理解为什么CNTK选择了这种方式。归根结底，神经网络中的每一层都是一个数学函数。CNTK中的所有层函数都会生成一个具有一组预定义参数的数学函数。再次调用该函数时，你将绑定最后一个缺失的参数——输入，来连接层。
- en: You will typically build neural networks with this style of programming when
    you want to create a neural network with a complex architecture. But, for most
    starting developers, the functional style feels unfamiliar. CNTK provides an easier
    API for when you want to build a basic neural network through the `Sequential`
    layer function.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想要创建具有复杂架构的神经网络时，通常会使用这种风格的编程。但是，对于大多数初学者来说，函数式风格可能会感到陌生。CNTK提供了一个更简单的API，当你想通过`Sequential`层函数构建基础神经网络时，可以使用它。
- en: 'You can use the `Sequential` layer function to chain several layers together,
    without having to use the functional programming style, as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`Sequential`层函数将多个层链接在一起，而无需使用函数式编程风格，方法如下：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To do so, follow the given steps:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 按照给定的步骤操作：
- en: First, import the layer functions you want to use from the `layers` package
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`layers`包中导入你想使用的层函数
- en: Import the the `input_variable` function to create an input variable used to
    feed data into the neural network
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`input_variable`函数来创建一个输入变量，用于将数据输入到神经网络
- en: Create a new input variable to feed data into the neural network
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的输入变量，将数据输入到神经网络
- en: Create a new sequential layer block by invoking the `Sequential` function
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`Sequential`函数创建一个新的顺序层块
- en: Provide the list of layers that you want to chain together to the `Sequential`
    function
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你想要链接在一起的层列表提供给`Sequential`函数
- en: Invoke the configured `Sequential` function object providing the features input
    variable to complete the network structure
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用配置好的`Sequential`函数对象，提供特征输入变量以完成网络结构
- en: By combining the `Sequential` function and other layer functions you can create
    any neural network structure. In the next section, we'll take a look at how to
    customize layers with settings to configure things like the `activation` function.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将`Sequential`函数与其他层函数结合，你可以创建任何神经网络结构。在下一部分中，我们将看看如何通过设置自定义层，以配置诸如`activation`函数等内容。
- en: Customizing layer settings
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义层设置
- en: CNTK provides a pretty good set of defaults for building neural networks. But
    you'll find yourself experimenting with those settings a lot. The behavior and
    performance of the neural network will be different based on the `activation`
    function and other settings you choose. Because of this, it is good to understand
    what you can configure.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: CNTK为构建神经网络提供了一套非常好的默认设置。但你会发现自己经常需要对这些设置进行实验。神经网络的行为和性能会根据你选择的`activation`函数和其他设置有所不同。因此，了解你可以配置哪些选项是很有帮助的。
- en: 'Each layer has its own unique configuration options, some of which you will
    use a lot, and others you will use less. When we look at the `Dense` layer, there
    are a few important settings that you want to define:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 每个层都有自己独特的配置选项，其中一些你会经常使用，另一些则用得较少。当我们查看 `Dense` 层时，有几个重要的设置是你需要定义的：
- en: '`shape`:The output shape of the layer'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shape`：层的输出形状'
- en: '`activation`: The `activation` function for the layer'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation`：层的 `activation` 函数'
- en: '`init`: The `initialization` function of the layer'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init`：层的 `initialization` 函数'
- en: 'The output shape of a layer determines the number of neurons in that layer.
    Each neuron needs to have an `activation` function defined so it can transform
    the input data. Finally, we need a function that will initialize the parameters
    of the layer when we start training the neural network. The output shape is the
    first parameter in each `layer` function. The `activation` and `init` arguments
    are supplied as keyword arguments. These parameters have default values for them,
    so you can omit them should you not need a custom setting. The next sample demonstrates
    how to configure a `Dense` layer with a custom `initializer` and `activation`
    function:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 层的输出形状决定了该层中神经元的数量。每个神经元需要定义一个 `activation` 函数，以便它能转换输入数据。最后，我们需要一个函数来初始化该层的参数，以便我们开始训练神经网络。输出形状是每个
    `layer` 函数中的第一个参数。`activation` 和 `init` 参数作为关键字参数提供。这些参数有默认值，因此如果你不需要自定义设置，可以省略它们。以下示例演示了如何使用自定义
    `initializer` 和 `activation` 函数配置 `Dense` 层：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To configure a Dense layer follow the given steps:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 Dense 层的步骤如下：
- en: First, import the `Dense` layer from the `layers` package
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从 `layers` 包中导入 `Dense` 层
- en: Next, import the `sigmoid` operator from the `ops` package so we can use it
    to configure as an `activation` function
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从 `ops` 包中导入 `sigmoid` 运算符，以便我们可以将其配置为 `activation` 函数
- en: Then import the `glorot_uniform` initializer from the `initializer` package
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后从 `initializer` 包中导入 `glorot_uniform` 初始化函数
- en: Finally, create a new layer using the `Dense` layer providing the number of
    neurons as the first argument and provide the `sigmoid` operator as the `activation`
    function and the `glorot_uniform` function as the `init` function for the layer
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用 `Dense` 层创建一个新层，将神经元数量作为第一个参数，并提供 `sigmoid` 运算符作为 `activation` 函数，`glorot_uniform`
    函数作为层的 `init` 函数
- en: There are several `activation` functions to choose from; for example, you can
    use **Rectified Linear Unit** (**ReLU**),or `sigmoid`, as an `activation` function.
    All `activation` functions can be found in the `cntk.ops` package.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种 `activation` 函数可供选择；例如，你可以使用 **修正线性单元** (**ReLU**) 或 `sigmoid` 作为 `activation`
    函数。所有的 `activation` 函数都可以在 `cntk.ops` 包中找到。
- en: Each `activation` function will have a different effect on the performance of
    your neural network. We will go into more detail regarding `activation` functions
    when we build a neural network later in this chapter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 `activation` 函数对神经网络的性能会产生不同的影响。当我们在本章稍后构建神经网络时，我们将更详细地讨论 `activation` 函数。
- en: Initializers determine how the parameters in the layer are initialized when
    we start training our neural network. You can choose from various initializers
    in CNTK. `Normal`, `uniform`, and `glorot_uniform` are some of the more widely
    used initializers in the `cntk.initializer` package. We will get into more detail
    about which initializer to use when we start to solve our first deep learning
    problem.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化函数决定了当我们开始训练神经网络时，层中参数的初始化方式。你可以从 CNTK 中选择多种初始化函数。`Normal`、`uniform` 和 `glorot_uniform`
    是 `cntk.initializer` 包中一些常用的初始化函数。当我们开始解决第一个深度学习问题时，我们会更详细地讨论选择哪个初始化函数。
- en: Whatever initializer function you're using from CNTK, it's important to realize
    that they use random number generators to generate the initial values for the
    parameters in the layer. This is an important technique, because it allows the
    neural network to learn the right parameters effectively. All initializer functions
    in CNTK support an extra seed setting. When you set this parameter to a fixed
    value, you will get the same initial values every time you train your neural network.
    This can be useful when you're trying to reproduce a problem, or are experimenting
    with different settings.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你使用的是 CNTK 中的哪种初始化函数，都需要意识到它们使用随机数生成器来生成层中参数的初始值。这是一个重要的技术，因为它允许神经网络有效地学习正确的参数。CNTK
    中的所有初始化函数都支持额外的种子设置。当你将此参数设置为固定值时，每次训练神经网络时都会得到相同的初始值。这在你尝试重现问题或实验不同设置时非常有用。
- en: 'When you are building a neural network, you typically have to specify the same
    set of settings for several layers in your neural network. This can become problematic
    when you are experimenting with your model. To solve this, CNTK includes a `utility`
    function called `default_options`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当你构建神经网络时，通常需要为网络中的多个层指定相同的设置。当你在实验你的模型时，这可能会变得很麻烦。为了解决这个问题，CNTK提供了一个名为`default_options`的`utility`函数：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'By using the `default_options` function, we''ve configured the `sigmoid` activation
    function for all three layers, with just one line of code. The `default_options`
    function accepts a standard set of settings that get applied to all layers in
    the scope of this function. Using the `default_options` function makes configuring
    the same options for a set of layers much more comfortable. You can configure
    quite a lot of settings this way; for example, with the following functions:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`default_options`函数，我们只需一行代码，就为所有三层配置了`sigmoid`激活函数。`default_options`函数接受一组标准设置，这些设置会应用到此函数作用范围内的所有层。使用`default_options`函数使得为一组层配置相同选项变得更加便捷。通过这种方式，你可以配置很多设置，例如，使用以下函数：
- en: '`activation`:The `activation` function to use'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation`：要使用的`activation`函数'
- en: '`init`: The `initialization` function for the layers'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init`：层的`initialization`函数'
- en: '`bias`: Whether the layers should have a `bias` term included'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias`：是否应在层中包含`bias`项'
- en: '`init_bias`:The `initialization` function for the bias term'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_bias`：`bias`项的初始化函数'
- en: Using learners and trainers to optimize the parameters in a neural network
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`learners`和`trainers`优化神经网络中的参数
- en: In the previous sections we've seen how to create the structure for a neural
    network and how to configure various settings. Now let's look at how to use `learners`
    and `trainers` to optimize the parameters of a neural network. In CNTK, a neural
    network is trained using a combination of two components. The first component
    is the `trainer` component, which implements the backpropagation process. The
    second component is the `learner`. It is responsible for performing the gradient
    descent algorithm that we've seen in [Chapter 1](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml),
    *Getting Started with CNTK.*
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经学习了如何创建神经网络的结构以及如何配置各种设置。现在，让我们看看如何使用`learners`和`trainers`来优化神经网络的参数。在CNTK中，神经网络的训练是通过两部分组成的组合来完成的。第一部分是`trainer`组件，它实现了反向传播过程。第二部分是`learner`，它负责执行我们在[第1章](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml)《**CNTK入门**》中看到的梯度下降算法。
- en: The `trainer` passes the data through the neural network to obtain a prediction.
    It then uses the `learner` to obtain the new values for the parameters in the
    neural network. It then applies these new values, and repeats the process. This
    goes on until an exit criterion is met. The training process is stopped when a
    configured number of iterations is reached. This can be enhanced using custom
    callbacks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`trainer`将数据传递通过神经网络以获得预测。然后使用`learner`来获取神经网络中参数的新值。接着应用这些新值，并重复这个过程。这个过程一直进行，直到满足退出条件。当达到配置的迭代次数时，训练过程会停止。可以通过自定义回调来增强这一过程。'
- en: We've discussed a very basic form of gradient descent in [Chapter 1](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml),
    *Getting Started with CNTK*. But, in reality, there are many variations on this
    basic algorithm. The basic gradient descent doesn't work very well for complex
    cases. Often, it gets stuck in a local optimum (a bump in the hillside, if you
    will), so it doesn't reach a globally optimal value for the parameters in the
    neural network. Other algorithms, such as **Stochastic Gradient Descent** (**SGD**)
    with momentum, account for local optima, and use concepts such as momentum to
    get past bumps in the slope of the loss curve.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](9a2c8c46-f9a0-4e05-86ef-31300a28a7ba.xhtml)《**CNTK入门**》中讨论了一个非常基础的梯度下降方法。但实际上，这个基本算法有许多变种。基本的梯度下降在复杂的情况下效果并不好。它经常会陷入局部最优解（可以理解为山坡上的一个小凸起），因此无法达到神经网络参数的全局最优值。其他算法，例如带动量的**随机梯度下降**（**SGD**），考虑了局部最优，并使用动量等概念来跨越损失曲线的坡度中的“凸起”。
- en: 'Here are few interesting `learners` that are included in the CNTK library:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了一些在CNTK库中包含的有趣`learners`：
- en: '**SGD**: The basic stochastic gradient descent, without any extras'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SGD**：基本的随机梯度下降，没有任何额外功能'
- en: '**MomentumSGD**: Applies momentum to overcome local optima'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MomentumSGD**：应用动量来克服局部最优解'
- en: '**RMSProp**: Uses decaying learning rates to control the rate of descent'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSProp**：使用衰减学习率来控制下降速率'
- en: '**Adam**: Uses decaying momentum to decrease the rate of descent over time'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adam**：使用衰减的动量来减少随着时间推移的下降速率'
- en: '**Adagrad**: Uses different learning rates for frequently, and infrequently,
    occurring features'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adagrad**：为频繁和不频繁出现的特征使用不同的学习率'
- en: It's important to know that you can choose different `learners`, depending on
    the problem you want to solve. We will learn more about choosing the right optimizer
    when we start to solve our first machine learning problem with a neural network.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要知道，你可以根据你想要解决的问题选择不同的 `learners`。当我们开始使用神经网络解决第一个机器学习问题时，我们将进一步了解如何选择合适的优化器。
- en: Loss functions
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: In order for the `trainer` and `learner` to be able to optimize the parameters
    of the neural network, we need to define a function that measures the loss in
    the neural network. The `loss` function calculates how big the difference is between
    the predicted output of the neural network, and the expected output that we know
    beforehand.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 `trainer` 和 `learner` 能够优化神经网络的参数，我们需要定义一个衡量神经网络损失的函数。`loss` 函数计算的是神经网络预测输出与我们预先知道的期望输出之间的差距。
- en: CNTK contains a number of `loss` functions in the `cntk.losses` module. Each
    `loss` function has its own use and specific characteristics. For example, when
    you want to measure the loss in a model that predicts a continuous value, you're
    going to need the `squared_error` loss function. It measures the distance between
    the predicted value generated by the model, and the real value that you provided
    when training the model.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: CNTK 包含了多个 `loss` 函数，位于 `cntk.losses` 模块中。每个 `loss` 函数都有其特定的用途和特点。例如，当你想衡量一个预测连续值的模型的损失时，你需要使用
    `squared_error` 损失函数。它衡量的是模型生成的预测值与在训练模型时提供的真实值之间的距离。
- en: For classification models, you will need a different set of `loss` functions.
    The `binary_cross_entropy` loss function can be used to measure the loss in a
    model that is used for binary classification jobs, such as a fraud detection model.
    The `cross_entropy_with_softmax` loss function is more suitable for classification
    models that predict multiple classes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类模型，你需要一组不同的 `loss` 函数。`binary_cross_entropy` 损失函数可以用来衡量用于二分类任务（如欺诈检测模型）模型的损失。`cross_entropy_with_softmax`
    损失函数更适用于预测多个类别的分类模型。
- en: Model metrics
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型度量
- en: Combining a `learner` and `loss` function with a `trainer` allows us to optimize
    the parameters in the neural network. This should produce a good model, but in
    order to know that for sure we need metrics to measure model performance. A metric
    is a single value that tells us, for example, what percentage of samples was predicted
    correctly.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `learner` 和 `loss` 函数与 `trainer` 结合起来可以优化神经网络中的参数。这应该会生成一个好的模型，但为了确保这一点，我们需要使用度量来衡量模型的性能。度量是一个单一的数值，它告诉我们，例如，有多少百分比的样本被正确预测。
- en: Because the `loss` function measures the difference between the actual value
    and the predicted value, you might think that it's a good measure of how well
    our model is doing. Depending on the model, it may provide some value, but often
    you will need to use a separate `metric` function to measure your model's performance
    in a meaningful way.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 `loss` 函数衡量的是实际值与预测值之间的差异，你可能会认为它是衡量我们模型表现的一个好指标。根据模型的不同，它可能提供一些有用的信息，但通常你需要使用单独的
    `metric` 函数来以有意义的方式衡量模型的表现。
- en: CNTK offers a number of different `metric` functions in the `cntk.metrics` package.
    For example, if you want to measure the performance of a classification model,
    you can use the `classification_error` function. This is used to measure the percentage
    of samples that were predicted correctly.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: CNTK 提供了多个不同的 `metric` 函数，位于 `cntk.metrics` 包中。例如，如果你想衡量分类模型的性能，可以使用 `classification_error`
    函数。它用于衡量正确预测的样本百分比。
- en: The `classification_error` function is just one example of a metric. One other
    important `metric` function is the `ndcg_at_1` metric. If you're working with
    a ranking model, then you are interested in how closely your model ranked the
    samples according to a predefined ranking. This is what the `ndcg_at_1` metric
    gives you.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`classification_error` 函数只是一个度量的例子。另一个重要的 `metric` 函数是 `ndcg_at_1` 度量。如果你正在使用排序模型，那么你会关心模型是如何根据预定义的排序来排列样本的。`ndcg_at_1`
    度量就是用来评估这一点的。'
- en: Building your first neural network
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建你的第一个神经网络
- en: Now that we've learned what concepts CNTK offers to build a neural network,
    we can start to apply these concepts to a real machine learning problem. In this
    section, we'll explore how to use a neural network to classify species of iris
    flowers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 CNTK 提供的概念来构建神经网络，我们可以开始将这些概念应用到一个实际的机器学习问题中。在这一部分，我们将探讨如何使用神经网络对鸢尾花的品种进行分类。
- en: This is not a typical task where you want to use a neural network. But, as you
    will soon discover, the dataset is simple enough to get a good grasp of the process
    of building a deep learning model. Yet it contains enough data to ensure that
    the model works reasonably well.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个典型的任务，通常你会选择使用神经网络。但正如你很快会发现的，数据集足够简单，可以帮助你很好地理解构建深度学习模型的过程，同时也包含足够的数据来确保模型的合理性能。
- en: 'The iris dataset describes the physical properties of different varieties of
    iris flowers:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 鸢尾花数据集描述了不同品种鸢尾花的物理属性：
- en: Sepal length in cm
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片长度（单位：厘米）
- en: Sepal width in cm
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片宽度（单位：厘米）
- en: Petal length in cm
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣长度（单位：厘米）
- en: Petal width in cm
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度（单位：厘米）
- en: Class (iris setosa, iris versicolor, iris virginica)
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别（鸢尾花 Setosa，鸢尾花 Versicolor，鸢尾花 Virginica）
- en: 'The code for this chapter includes the iris dataset, on which you need to train
    the deep learning model. If you''re interested, you can find the original files
    online at: [http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris).
    It is also included with the sample code for this chapter.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码包括鸢尾花数据集，您需要在此数据集上训练深度学习模型。如果您感兴趣，可以在线查找原始文件：[http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)。这些文件也包括在本章的示例代码中。
- en: We are going to build a deep learning model that is going to classify a flower
    based on the physical properties of sepal width and length, and petal width and
    length. We can predict three different classes as output for the model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个深度学习模型，该模型将基于萼片宽度和长度、花瓣宽度和长度来分类花卉。我们可以预测三种不同的类别作为模型的输出。
- en: We have a total of 150 different samples to train on, which should be enough
    to get reasonable performance when we try to use the model to classify a flower.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总共有150个不同的样本来进行训练，这应该足以在我们尝试使用模型来分类花卉时获得合理的性能。
- en: Building the network structure
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建网络结构
- en: First, we need to determine what architecture to use for our neural network.
    We will be building a regular neural network, which is often called a feedforward
    neural network.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要确定要为神经网络使用什么架构。我们将构建一个常规的神经网络，通常被称为前馈神经网络。
- en: We need to define the number of neurons on the input and output layers first.
    Then, we need to define the shape of the hidden layer in our neural network. Because
    the task that we're solving is a simple one, we don't need more than one layer.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要定义输入层和输出层的神经元数量。接下来，我们需要定义神经网络中隐藏层的形状。由于我们解决的问题相对简单，所以我们不需要超过一层隐藏层。
- en: When we look at our dataset, we can see it has four features and one label.
    Because we have four features, we need to make sure our neural network has an
    input layer with four neurons.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看数据集时，我们可以看到它有四个特征和一个标签。由于我们有四个特征，我们需要确保神经网络的输入层有四个神经元。
- en: Next, we need to define the output layer for our neural network. For this, we
    look at the number of classes that we need to be able to predict with our model.
    In our case, we have three different species of flowers to choose from, so we
    need three neurons in the output layer.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要为神经网络定义输出层。为此，我们需要查看需要模型能够预测的类别数量。在我们的例子中，我们有三种不同的花卉品种可以选择，因此我们需要在输出层中设置三个神经元。
- en: 'First, we import the necessary components from the CNTK library, which are
    our layer types, `activation` functions, and a function that allows us to define
    an input variable for our network:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从 CNTK 库中导入必要的组件，这些组件包括层类型、`activation` 函数以及允许我们为网络定义输入变量的函数：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We then create our model using the `Sequential` function, and feed it the layers
    that we want. We create two distinct layers in our network—first, one with four
    neurons, and then, another one with three neurons:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 `Sequential` 函数创建模型，并将我们需要的层传递给它。我们在网络中创建了两个不同的层——首先是一个有四个神经元的层，然后是一个有三个神经元的层：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we bind the network to the input variable, which will compile the
    neural network so it has an input layer with four neurons, and an output layer
    with three neurons, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将网络与输入变量绑定，这将编译神经网络，使其具有一个包含四个神经元的输入层和一个包含三个神经元的输出层，如下所示：
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, let's go back to our layer structure. Notice that we didn't model an input
    layer when we invoked the `Sequential` layer function. This is because the `input_variable`
    we created in our code is the input layer for the neural network.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到我们的层结构。注意到我们在调用`Sequential`层函数时没有建模输入层。这是因为我们在代码中创建的`input_variable`就是神经网络的输入层。
- en: The first layer in the sequential call is the hidden layer in the network. As
    a general rule of thumb, you want hidden layers that are no bigger than two times
    the number of neurons in the previous layer.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sequential`调用的第一个层是网络中的隐藏层。通常的经验法则是，你希望隐藏层的大小不超过前一层神经元数量的两倍。
- en: You will want to experiment with this setup in order to get the best results.
    Picking the right numbers of layers and neurons in your neural network requires
    some experience and experimentation. There are no hard rules that determine how
    many hidden layers you should include.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要通过实验来优化这个设置，以获得最佳结果。选择神经网络的层数和神经元数目需要一些经验和实验。没有硬性规定说明你应该包含多少个隐藏层。
- en: Choosing an activation function
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择激活函数
- en: In the previous section, we chose the `sigmoid` activation function for our
    neural network. Choosing the right activation makes a big difference to how well
    your deep learning model will perform.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们为神经网络选择了`sigmoid`激活函数。选择正确的激活函数对深度学习模型的性能有很大影响。
- en: You will find a lot of opinions about choosing an `activation` function. That's
    because there's a lot to choose from, and not enough hard proof for any of the
    choices made by experts in the field. So, how do you pick one for your neural
    network?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现关于选择`激活`函数有很多不同的观点。这是因为有很多选择，而对于专家所做的选择，并没有足够的实质性证据。那么，如何为你的神经网络选择一个合适的激活函数呢？
- en: Choosing an activation function for the output layer
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择输出层的激活函数
- en: First, we need to define what kind of problem we're solving. This determines
    the `activation` function for the output layer of your network. For regression
    problems, you want to use a `linear` activation function on the output layer.
    For a classification problem, you will want to use `sigmoid` for binary classification,
    and the `softmax` function for multi-class classification problems.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义我们正在解决的问题类型。这决定了你网络输出层的`激活`函数。对于回归问题，你需要在输出层使用`线性`激活函数。对于分类问题，二分类问题使用`sigmoid`函数，多分类问题使用`softmax`函数。
- en: In the model that we're building, we need to predict one of three classes, which
    means we need to use the `softmax` activation function on the output layer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建的模型中，我们需要预测三类中的一种，这意味着我们需要在输出层使用`softmax`激活函数。
- en: Choosing an activation function for the hidden layers
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择隐藏层的激活函数
- en: Now, let's look at the hidden layers. Choosing an `activation` function for
    the hidden layers in our model is much harder. We will need to run some experiments
    and monitor the performance to see which `activation` function works best.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下隐藏层。为我们模型的隐藏层选择一个`激活`函数要难得多。我们需要进行一些实验并监控性能，以查看哪种`激活`函数效果最佳。
- en: For classification problems, like our flower classification model, we need something
    that gives us probabilistic values. We need this because we need to predict the
    probability a sample belongs to a specific class. The `sigmoid` function helps
    us reach this goal. Its output is a probability, measured as a value between 0
    and 1\.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类问题，像我们花卉分类模型那样，我们需要一些可以给出概率值的东西。我们需要这样做，因为我们需要预测一个样本属于某个特定类别的概率。`sigmoid`函数帮助我们实现这个目标。它的输出是一个概率，值介于0和1之间。
- en: There are some problems that we have to account for with a `sigmoid` activation
    function. When you create larger networks, you may run into a problem called the
    **vanishing gradient**.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sigmoid`激活函数时，我们需要解决一些问题。当你创建更大的网络时，你可能会遇到一个叫做**梯度消失**的问题。
- en: Very large input values given to a `sigmoid` function will converge to either
    zero or one, depending on whether they are negative or positive. This means that,
    when we work with large input values for our model, we won't see a lot of difference
    in the output of the `sigmoid` function. A change in an already large input value
    will result in only a very small change in the output. The gradient that is derived
    from this by the optimizer during training is also very small. Sometimes, it is
    so small that your computer will round it to zero, which means the optimizer can't
    detect which way to go with the values for the parameters. When the optimizer
    can't calculate gradients because of rounding problems in the CPU, we're dealing
    with a vanishing gradient problem.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 给`sigmoid`函数输入非常大的值时，输出会趋近于零或一，具体取决于输入值是负数还是正数。这意味着，当我们使用模型的大输入值时，`sigmoid`函数的输出变化不大。输入值的变化将只导致输出的非常小的变化。优化器在训练时计算的梯度也非常小。有时，它会小到计算机将其四舍五入为零，这意味着优化器无法检测到参数值的调整方向。当优化器由于CPU的四舍五入问题无法计算梯度时，我们就会遇到梯度消失问题。
- en: To solve this problem, scientists have come up with a new activation function,
    `ReLU`. This activation function converts all negative values to zero, and works
    as a pass-through filter for positive values. It helps solve the vanishing gradient
    problem, because it doesn't limit the output value.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，科学家们提出了一种新的激活函数，`ReLU`。这个激活函数将所有负值转化为零，并且对正值起到通过过滤器的作用。它有助于解决梯度消失问题，因为它不会限制输出值。
- en: There are, however, two problems with the `ReLU` function. First, it converts
    negative input to zero. In some cases, this can lead to a situation where the
    optimizer sets the weight of some parameters to zero as well. This causes your
    network to have dead neurons. That, of course, limits what your network can do.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`ReLU`函数有两个问题。首先，它将负输入值转换为零。在某些情况下，这可能导致优化器将某些参数的权重也设置为零。这会导致网络中出现“死亡神经元”，当然，这会限制网络的功能。
- en: The second problem is that the `ReLU` function suffers from exploding gradients.
    Because the upper bound of the output of this function isn't limited, it can amplify
    signals in such a way that the optimizer will calculate gradients that are close
    to infinity. When you apply this gradient to parameters in your network, your
    network will start to output NaN values.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是，`ReLU`函数存在梯度爆炸的问题。因为该函数输出的上限没有限制，它可能会放大信号，导致优化器计算出接近无穷大的梯度。当你将这个梯度应用到网络中的参数时，网络开始输出NaN值。
- en: Choosing the correct activation function for hidden layers requires some experimentation.
    Again, there is no hard rule that says which activation function to use. In the
    example code of this chapter, we choose the `sigmoid` function, after experimenting
    a bit with the model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 选择隐藏层的正确激活函数需要一定的实验。再次强调，并没有硬性规定说要使用哪种激活函数。在本章的示例代码中，我们在对模型进行一些实验后，选择了`sigmoid`函数。
- en: Picking a loss function
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择损失函数
- en: When we have the structure for the model, it is time to take a look at how to
    optimize it. For this, we need a `loss` function to minimize. There are quite
    a few `loss` functions to choose from.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有了模型的结构后，就该考虑如何优化它了。为此，我们需要一个需要最小化的`loss`函数。可以选择的`loss`函数有很多。
- en: 'The right `loss` function depends on what kind of problem you are solving.
    For example, in a classification model like ours, we need a `loss` function that
    can measure the difference between a predicted class and an actual class. It needs
    to do so for three classes. The `categorical cross entropy` function is a good
    candidate. In CNTK, this `loss` function is implemented as `cross_entropy_with_softmax`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 合适的`loss`函数取决于你要解决的问题。例如，在像我们这样的分类模型中，我们需要一个能够衡量预测类别与实际类别之间差异的`loss`函数。它需要针对三个类别进行计算。`categorical
    cross entropy`函数是一个不错的选择。在CNTK中，这个`loss`函数实现为`cross_entropy_with_softmax`：
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We need to import the `cross_entropy_with_softmax` function from the `cntk.losses`
    package first. After we've imported the `loss` function, we create a new input
    variable so we can feed the expected label into the `loss` function. Then we create
    a new `loss` variable that will hold a reference to the `loss` function. Any `loss`
    function in CNTK requires the output of the model and an input variable for the
    expected label.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要先从`cntk.losses`包中导入`cross_entropy_with_softmax`函数。导入`loss`函数后，我们创建一个新的输入变量，以便将期望标签传递给`loss`函数。然后，我们创建一个新的`loss`变量，保存对`loss`函数的引用。CNTK中的任何`loss`函数都需要模型的输出和期望标签的输入变量。
- en: Recording metrics
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记录指标
- en: With the structure in place and a `loss` function, we have all the ingredients
    we need to start optimizing our deep learning model. But before we start to look
    at how to train the model, let's take a look at metrics.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构搭建好并拥有`loss`函数后，我们就拥有了优化深度学习模型所需的所有元素。但在我们开始训练模型之前，让我们先看一下指标。
- en: 'In order for us to see how our network is doing, we need to record some metrics.
    Since we''re building a classification model, we''re going to use a `classification_error`
    metric. This metric produces a number between 0 and 1, which indicates the percentage
    of samples correctly predicted:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们看到网络的表现，我们需要记录一些指标。由于我们正在构建一个分类模型，我们将使用`classification_error`指标。该指标会生成一个介于0和1之间的数值，表示正确预测的样本百分比：
- en: '[PRE9]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let's import `classification_error` from the `cntk.metrics` package. We then
    create a new `error_rate` variable and bind the `classification_error` function
    to it. The function needs the output of the network and the expected label as
    input. We already have those available from defining our model and `loss` function.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`cntk.metrics`包中导入`classification_error`。接着，我们创建一个新的`error_rate`变量，并将`classification_error`函数绑定到它。该函数需要网络的输出和期望标签作为输入。我们已经在定义模型和`loss`函数时准备好了这些。
- en: Training the neural network
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: Now that we have all the components for the deep learning defined, let's train
    it. You can train a model in CNTK using a combination of a `learner` and `trainer`.
    We're going to need to define those and then feed data through the trainer to
    train the model. Let's see how that works.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了深度学习的所有组件，接下来就可以开始训练了。你可以使用`learner`和`trainer`的组合在CNTK中训练模型。我们需要定义这些组件，然后通过`trainer`喂入数据来训练模型。让我们看看如何操作。
- en: Choosing a learner and setting up training
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择学习器并设置训练
- en: 'There are several `learners` to choose from. For our first model, we are going
    to use the `stochastic gradient descent` learner. Let''s configure the `learner`
    and `trainer` to train the neural network:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种`learners`可供选择。对于我们的第一个模型，我们将使用`stochastic gradient descent`学习器。让我们配置`learner`和`trainer`来训练神经网络：
- en: '[PRE10]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To configure the `learner` and `trainer` to train the neural network, follow
    the given steps:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置`learner`和`trainer`以训练神经网络，请按照以下步骤进行：
- en: First, import the `sgd` function from the `learners` package
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`learners`包中导入`sgd`函数。
- en: Then, import the `Trainer` from the `trainer` package which is part of the `train`
    package
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从`trainer`包中导入`Trainer`，该包是`train`包的一部分。
- en: Now create a `learner` by invoking the `sgd` function providing the parameters
    of the model and a value for the learning rate
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在通过调用`sgd`函数并提供模型的参数和学习率值来创建`learner`。
- en: Finally, initialize the `trainer` and provide it the network, the combination
    of the `loss` and `metric` and the `learner`
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，初始化`trainer`并提供网络、`loss`与`metric`的组合以及`learner`
- en: The learning rate that we provide to the `sgd` function controls the speed of
    optimization and should be a small number somewhere in the area of 0.1 to 0.001\.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供给`sgd`函数的学习率控制优化速度，应该是一个小数字，通常在0.1到0.001之间。
- en: Note that every `learner` has its own parameters, so be sure to check the documentation
    to find out what parameters you need to configure when using a specific `learner`
    from the `cntk.learners` package.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个`learner`都有自己的参数，因此请务必查看文档，了解在使用`cntk.learners`包中的特定`learner`时需要配置哪些参数。
- en: Feeding data into the trainer to optimize the neural network
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向训练器输入数据以优化神经网络
- en: We spent quite a bit of time defining our model, configuring the `loss`, `metrics`,
    and, finally, the `learner`. Now it is time to train it on our dataset. Before
    we can train our model, however, we need to load the dataset.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们花了很多时间来定义模型、配置`loss`、`metrics`，最后是`learner`。现在是时候在我们的数据集上进行训练了。然而，在我们训练模型之前，我们需要先加载数据集。
- en: 'The dataset in the example code is stored as a CSV file. In order to load this
    dataset, we need to use a data wrangling package such as `pandas`. This package
    is included by default in your Anaconda installation. The following sample demonstrates
    how to use `pandas` to load the dataset into memory:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 示例代码中的数据集存储为 CSV 文件。为了加载该数据集，我们需要使用类似`pandas`的数据处理包。这个包在你的 Anaconda 安装中默认包含。以下示例展示了如何使用`pandas`将数据集加载到内存中：
- en: '[PRE11]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To load the dataset into memory using `pandas` follow the given steps:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pandas`将数据集加载到内存中，请按照以下步骤进行操作：
- en: First, import the `pandas` package under the alias `pd`
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入`pandas`包并为其起别名`pd`
- en: Then, invoke the `read_csv` function to load the `iris.csv` file from disk
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，调用`read_csv`函数从磁盘加载`iris.csv`文件
- en: Because the CSV file doesn't include column headers, we need to define them
    ourselves. It will make it easier to refer to specific columns later on.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CSV 文件没有列头，我们需要自己定义它们。这样以后引用特定列时会更加方便。
- en: Normally, `pandas` will use the first column in the input file as the index
    of the dataset. The index will serve as a key by which you can identify records.
    We don't have an index in our dataset, so we disable its use through the `index_col`
    keyword argument.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，`pandas`会使用输入文件中的第一列作为数据集的索引。索引将作为识别记录的键。由于我们的数据集中没有索引，因此我们通过`index_col`关键字参数禁用它的使用。
- en: 'After we have loaded the dataset, let''s split it into a set of features and
    a label:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 加载完数据集后，让我们将其拆分为特征集和标签集：
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To split the dataset into a set of features and label, follow the given steps:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据集拆分为特征集和标签集，请按照以下步骤进行操作：
- en: First, use the `iloc` function to select all rows and the first four columns
    from the dataset
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用`iloc`函数从数据集中选择所有行和前四列
- en: Next, select the species column from the dataset and use the values property
    to access the underlying `numpy` array
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从数据集中选择物种列，并使用 values 属性访问底层的`numpy`数组
- en: 'Our model requires numeric input values. But the species column is a string
    value, indicating the type of flower. We can fix this by encoding the species
    column to a numeric vector representation. The vector representation we''re creating
    matches the number of output neurons of the neural network. Each element in the
    vector represents a species of flowers as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型需要数值型输入值。但是，物种列是一个字符串值，表示花的类型。我们可以通过将物种列编码为数值向量表示来解决这个问题。我们正在创建的向量表示与神经网络的输出神经元数量匹配。向量中的每个元素代表一种花的物种，具体如下：
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To create one-hot vector representations for the species, we will use a small
    `utility` function:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建物种的 one-hot 向量表示，我们将使用一个小的`utility`函数：
- en: '[PRE14]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `one_hot` function performs the following steps:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`one_hot`函数执行以下步骤：'
- en: First, initialize a new array filled with zeros with the required `length`
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，初始化一个填充为零的新数组，长度为所需的`length`
- en: Next, select the element at the specified `index` and set it to `1`
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，选择指定的`index`处的元素并将其设置为`1`
- en: 'Now that we have a dictionary mapping the species to the index, and a way to
    create one-hot vectors, we can turn the string values into their vector representation
    using one additional line of code:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一个将物种映射到索引的字典，并且有了创建 one-hot 向量的方法，我们可以通过添加一行代码将字符串值转换为其向量表示：
- en: '[PRE15]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Follow the given steps:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤进行操作：
- en: First, create a list expression to iterate over all elements in the array
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个列表表达式遍历数组中的所有元素
- en: For each value in the array perform a look up in the `label_mapping` dictionary
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数组中的每个值，在`label_mapping`字典中查找对应的值
- en: Next, take this converted numeric value and apply the `one_hot` function to
    convert it to a one-hot encoded vector
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将这个转换后的数值应用`one_hot`函数转换为 one-hot 编码向量
- en: Finally, take the converted list and turn it into a `numpy` array
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将转换后的列表转换为`numpy`数组
- en: When you are training a deep learning model, or any machine learning model for
    that matter, you need to keep in mind that the computer will try to remember all
    the samples that you've used for training the model. At the same time, it will
    try to learn general rules. When the model remembers samples, but isn't able to
    deduce rules from the training samples, it is overfitting on your dataset.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 当你训练深度学习模型或任何机器学习模型时，你需要记住，计算机将尝试记住你用于训练模型的所有样本。与此同时，它将尝试学习一般规则。当模型记住样本但不能从训练样本中推断规则时，它就会在数据集上过拟合。
- en: To detect overfitting, you want to keep a small portion of your dataset separate
    from the training set. The training set is then used to train the model, while
    the test set is used to measure the performance of the model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测过拟合，你需要将数据集的一小部分与训练集分开。训练集用于训练模型，而测试集则用于衡量模型的性能。
- en: 'We can split our dataset into training and test sets using a `utility` function
    from the `scikit-learn` package:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`scikit-learn`包中的`utility`函数，将数据集分为训练集和测试集：
- en: '[PRE16]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Follow the given steps:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: First, import the `train_test_split` function from the `model_selection` module
    in the `sklearn` package
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`sklearn`包中的`model_selection`模块导入`train_test_split`函数
- en: Then, invoke the `train_test_split` function with the features `X` and the labels
    `y`
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，调用`train_test_split`函数，传入特征`X`和标签`y`
- en: Specify a `test_size` of `0.2` to set aside 20% of the data
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定`test_size`为`0.2`，将20%的数据留作测试集
- en: Use the `stratify` keyword argument with the values from the labels array `y`
    so that we get an equal amount of samples in the training and test set for each
    of the species in the dataset
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`stratify`关键字参数，传入标签数组`y`中的值，这样我们就能确保在训练集和测试集中，每个物种的样本数量是均等的。
- en: If you don't use the `stratify` argument, you end up with a dataset that might
    not contain any samples for one class, while it has too many of another class.
    The model then doesn't learn how to classify the class that is missing in the
    training set, while it overfits on the other class, which has too many samples
    available.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不使用`stratify`参数，最终可能会得到一个数据集，其中某个类别的样本完全没有，而另一个类别的样本却过多。这样，模型就无法学习如何分类缺失的类别，而会在另一个样本过多的类别上出现过拟合。
- en: 'Now that we have a training set and validation set, let''s see how to feed
    them to our model to train it:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练集和验证集，让我们看看如何将它们输入到模型中进行训练：
- en: '[PRE17]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To train the model, invoke the `train_minibatch` method on the `trainer` and
    give it a dictionary that maps the input data to the input variables that you
    used to define the neural network and its associated `loss` function.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型，调用`trainer`上的`train_minibatch`方法，并传入一个字典，将输入数据映射到你用来定义神经网络及其相关`loss`函数的输入变量。
- en: We're using the `train_minibatch` method as a convenient way to feed data into
    the trainer. In the next chapter, we'll discuss other ways to feed data. We'll
    also look at what the `train_minibatch` method does in greater detail.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`train_minibatch`方法作为将数据输入到`trainer`中的便捷方式。在下一章中，我们将讨论其他输入数据的方式，并详细介绍`train_minibatch`方法的作用。
- en: 'Note that you will have to call `train_minibatch` a number of times to get
    the network decently trained. So we''ll have to write a short loop around this
    method call:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，你需要调用`train_minibatch`方法多次，才能让网络得到充分训练。所以我们需要写一个简短的循环来调用这个方法：
- en: '[PRE18]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Follow the given steps:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: First, create a new loop using the `for` statement and give it a range of `10`
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用`for`语句创建一个新的循环，并设置范围为`10`
- en: Within the loop invoke the `train_minibatch` method with a mapping between the
    input variables and the associated data
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在循环中调用`train_minibatch`方法，并将输入变量与相关数据进行映射
- en: Finally, print the `previous_minibatch_loss_average` and `previous_minibatch_evaluation_average`
    to monitor the training progress.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，打印出`previous_minibatch_loss_average`和`previous_minibatch_evaluation_average`，以监控训练进展。
- en: When you invoke the `train_minibatch` method, the `trainer` will update the
    output of the `loss` function and the value for the `metric` function that we
    provided to the `trainer` and store it in the `previous_minibatch_evaluation_average`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用`train_minibatch`方法时，`trainer`会更新`loss`函数的输出和我们提供给`trainer`的`metric`函数的值，并将其存储在`previous_minibatch_evaluation_average`中。
- en: Each time the loop completes, and we've run the whole dataset through the `trainer`,
    we've completed one epoch of training. As we have seen in the previous chapter,
    it is normal to run several epochs before a model works well enough. As an added
    bonus, we're also printing the progress of our `trainer` after each epoch.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 每次循环完成，并且我们将整个数据集通过`trainer`运行一次后，就完成了一个训练周期。正如我们在前一章中看到的那样，通常需要运行多个周期，直到模型表现足够好。作为额外的奖励，我们还会在每个周期后打印出`trainer`的进度。
- en: Checking the performance of the neural network
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查神经网络的性能
- en: Every time we pass data through the trainer to optimize our model, it measures
    the performance of the model through the metric that we configured for the trainer.
    The model performance measured during training is on the training set. It is useful
    to measure the accuracy on the training set, because it will tell you whether
    the model is actually learning anything from the data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们将数据传递给训练器以优化模型时，它会通过我们为训练器配置的度量标准来衡量模型的表现。训练期间衡量的模型性能是基于训练集的。衡量训练集上的准确率很有用，因为它能告诉你模型是否从数据中学到了什么。
- en: 'For a full analysis of the model performance, you need to measure the performance
    of the model using the test set. This can be done by invoking the `test_minibatch`
    method on the `trainer` as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要全面分析模型的性能，你需要使用测试集来衡量模型的表现。可以通过以下方式调用`trainer`上的`test_minibatch`方法：
- en: '[PRE19]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This method accepts a dictionary with a mapping between the input variables
    and the data for the variables. The output of this method is the output of the
    `metric` function you've configured earlier. In our case, it's the accuracy of
    our model based on the data we've given as input.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接受一个字典，字典中包含输入变量与变量数据之间的映射关系。该方法的输出是你之前配置的`metric`函数的输出。在我们的例子中，它是基于我们输入的数据计算的模型准确率。
- en: When the accuracy on the test set is higher than the accuracy on the training
    set, we will have a model that is underfitting. We're dealing with overfitting
    when the accuracy on the test set is lower than the accuracy on the training set.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当测试集的准确率高于训练集的准确率时，我们会遇到欠拟合的情况。当测试集的准确率低于训练集的准确率时，我们会遇到过拟合的情况。
- en: Both underfitting and overfitting are bad if you take them too far. The best
    performance is achieved when the accuracy on both test set and training set are
    almost the same. We'll talk more about model performance in [Chapter 4](e39df191-73e4-414f-b44b-efca6f0ad4cd.xhtml),
    *Validating Model Performance*.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是欠拟合还是过拟合，过度都不好。当测试集和训练集的准确率几乎相同时，模型性能最佳。我们将在[第4章](e39df191-73e4-414f-b44b-efca6f0ad4cd.xhtml)中详细讨论模型性能，*验证模型性能*。
- en: Making predictions with a neural network
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络进行预测
- en: One of the most satisfying things after training a deep learning model is to
    actually use it in an application. For now, we'll limit ourselves to using the
    model with a sample that we randomly pick from our test set. But, later on, in
    [Chapter 7](8db9f932-5716-4a33-82a7-0c5ce5fe2ed4.xhtml), *Deploying Models to*
    *Production*, we'll look at how to save the model to disk and use it in C# or
    .NET to build applications with it.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度学习模型后，最令人满意的事情之一就是将它实际应用到应用程序中。目前，我们将仅限于使用从测试集中随机挑选的样本来使用模型。但稍后，在[第7章](8db9f932-5716-4a33-82a7-0c5ce5fe2ed4.xhtml)，*将模型部署到生产环境*中，我们将探讨如何将模型保存到磁盘，并在C#或.NET中使用它来构建应用程序。
- en: 'Let''s write the code to make a prediction with the neural network that we
    trained:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写代码，通过我们训练过的神经网络进行预测：
- en: '[PRE20]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Follow the given steps:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤操作：
- en: First, pick a random item from the test set using the `np.random.choice` function
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用`np.random.choice`函数从测试集中随机挑选一个项目
- en: Then select the sample data from the test set using the generated `sample_index`
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用生成的`sample_index`从测试集中选择样本数据
- en: Next, create an inverted mapping so you can convert the numeric output of the
    neural network to an actual label
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个反向映射，以便你可以将神经网络的数值输出转换为实际标签
- en: Now, use the selected `sample` data and make a prediction by invoking the neural
    network `z` as a function
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用选定的`sample`数据，通过将神经网络`z`作为函数来进行预测
- en: From the predicted output, take the index of the neuron that has the highest
    value as the predicted value using the `np.argmax` function from the `numpy` package
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从预测输出中，使用`numpy`包中的`np.argmax`函数获取具有最大值的神经元索引作为预测值
- en: Use the `inverted_mapping` to convert the index value into the real label
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`inverted_mapping`将索引值转换为真实标签
- en: 'When you execute the code sample, you will get output similar to this:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行代码示例时，你会得到类似这样的输出：
- en: '[PRE21]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Improving the model
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进模型
- en: You will quickly learn that building and training neural networks takes more
    than one attempt. Usually, the first version of your model will not work as well
    as you hope. It requires quite a bit of experimentation to come up with a great
    model.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你会很快意识到，构建和训练神经网络需要不止一次尝试。通常，模型的第一版效果并不会如你所愿。需要相当多的实验才能设计出一个优秀的模型。
- en: A good neural network starts with a great dataset. In nearly all cases, better
    performance is achieved by using a proper dataset. Many data scientists will tell
    you that they spend about 80% of their time working on a good dataset. As with
    all computer software, if you put garbage in, you will get garbage out.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的神经网络始于一个优秀的数据集。在几乎所有情况下，使用合适的数据集会带来更好的性能。许多数据科学家会告诉你，他们大约花费80%的时间在处理一个好的数据集上。就像所有计算机软件一样，如果你输入垃圾，输出的也会是垃圾。
- en: Even with a good dataset, you still need to spend quite some time to build and
    train different models before you get the performance you're after. So, let's
    see what you can do to improve your model after you've built it for the first
    time.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有了良好的数据集，你仍然需要花费相当多的时间来构建和训练不同的模型，才能获得你想要的性能。那么，让我们看看在第一次构建模型后，你可以做些什么来改进模型。
- en: After you've trained the model for the first time, you have a couple of options
    to choose from in order to improve your model.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次训练模型后，你有几个选择来改进你的模型。
- en: Take a look at the accuracy of your training and validation sets. Is the accuracy
    on the training set lower? Try to train the model for more epochs. Usually, this
    will help improve the model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 查看你训练集和验证集的准确率。训练集上的准确率较低吗？尝试训练模型更多的轮次。通常，这会有助于提升模型的表现。
- en: Does the training accuracy not improve even, if you train the model for longer?
    Then your model is probably unable to learn the complex relationships in your
    dataset. Try to change the model structure, and train the model again to see if
    that improves the accuracy.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你训练模型更长时间，训练准确率仍然没有提高？那么你的模型可能无法学习数据集中的复杂关系。尝试改变模型结构，然后再次训练模型，看看是否能提高准确率。
- en: For example, try to change the activation function or the number of neurons
    in your hidden layers. This will usually help the model to learn the more complex
    relationships in the dataset.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，尝试更改激活函数或隐藏层中神经元的数量。这通常有助于模型学习数据集中更复杂的关系。
- en: Alternatively, you can take a look at the number of layers in your model. Adding
    one more layer can have quite a large effect on the ability of your model to learn
    rules from the data you feed it.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你还可以检查模型中层的数量。添加一层可能对模型从输入数据中学习规则的能力产生很大的影响。
- en: Finally, when that doesn't help, take a look at the initialization of the layers
    in your model. In some cases, choosing a different initialization function helps
    the model during the initial learning steps.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当这些方法无效时，看看你模型中层的初始化。在某些情况下，选择不同的初始化函数有助于模型在初始学习阶段。
- en: The key to the process of experimentation is to change one thing at a time and
    keep track of your experiments. Using a source control solution such as Git can
    help you keep track of different versions of your training code.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 实验过程的关键是一次只改变一个参数，并跟踪你的实验结果。使用像Git这样的版本控制工具可以帮助你跟踪不同版本的训练代码。
- en: Summary
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've built our first neural network and trained it to recognize
    iris flowers. While this sample is really basic, it shows how to use CNTK to build
    and train neural networks.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们构建了第一个神经网络，并训练它来识别鸢尾花。虽然这个示例非常基础，但它展示了如何使用CNTK构建和训练神经网络。
- en: We've seen how to use the layer library in CNTK to our advantage to quickly
    define the structure for our neural network. In this chapter, we've talked about
    a few basic building blocks, such as the `Dense` layer and the `Sequential` layer,
    to chain several other layers together. In the coming chapters, we will learn
    other layer functions to build other types of neural networks such as convolutional
    networks.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何利用CNTK中的层库来快速定义神经网络的结构。在本章中，我们讨论了一些基本的构建块，如`Dense`层和`Sequential`层，用于将多个其他层连接在一起。在接下来的章节中，我们将学习其他层函数，以构建其他类型的神经网络，如卷积神经网络。
- en: In this chapter, we've also discussed how to use `learner` and `trainer` to
    build a basic algorithm to train our neural network. We've used the `train_minibatch`
    method, together with a basic loop, to construct our own training process. This
    is a pretty simple and powerful way to train our model. In the next chapter, we'll
    discuss other methods of training and the `train_minibatch` method in much more
    detail.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们还讨论了如何使用`learner`和`trainer`构建一个基本算法来训练我们的神经网络。我们使用了`train_minibatch`方法，并配合基本的循环来构建自己的训练过程。这是一种非常简单而强大的训练模型的方式。在下一章中，我们将更详细地讨论其他训练方法以及`train_minibatch`方法。
- en: After we trained the model, we made use of the functional properties of CNTK
    to make a prediction with our trained model. The fact that a model is a function
    is quite powerful, and makes it really intuitive to use trained models in your
    application.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完模型后，我们利用了CNTK的函数特性，使用训练好的模型进行预测。模型作为函数的这一特性非常强大，使得在应用程序中使用训练好的模型变得直观且简便。
- en: Finally, we've seen how to measure model performance using the `test_minibatch`
    method, and how to use performance metrics to check whether our model is overfitting.
    We later discussed how to use metrics to determine how to improve the model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们已经了解了如何使用`test_minibatch`方法来衡量模型性能，以及如何使用性能指标检查模型是否发生了过拟合。之后我们讨论了如何使用指标来判断如何改进模型。
- en: In the next chapter, we will look at different ways to access and feed data
    to CNTK models. We'll also explore each method of data access in CNTK, and which
    are the most appropriate to use in different circumstances.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨不同的方式来访问和输入数据到CNTK模型中。我们还将深入探讨CNTK中每种数据访问方法，并了解在不同情况下哪些方法最为合适。
