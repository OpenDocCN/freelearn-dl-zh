- en: Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Along with generative networks, reinforcement learning algorithms have provided
    the most visible advances in **Artificial Intelligence** (**AI**) today. For many
    years, computer scientists have worked toward creating algorithms and machines
    that can perceive and react to their environment like a human would. Reinforcement
    learning is a manifestation of that, giving us the wildly popular AlphaGo and
    self-driving cars. In this chapter, we'll cover the foundations of reinforcement
    learning that will allow us to create advanced artificial agents later in this
    book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 与生成对抗网络一起，强化学习算法是目前**人工智能**（**AI**）领域最为显著的进展。多年来，计算机科学家一直在努力创建能够像人类一样感知并对环境作出反应的算法和机器。强化学习就是这一努力的体现，它带来了广受欢迎的AlphaGo和自动驾驶汽车。在本章中，我们将介绍强化学习的基础，为本书后续章节中创建更先进的人工智能智能体打下基础。
- en: Reinforcement learning plays off the human notion of learning from experience. Like
    generative models, it learns based on **evaluative feedback**. Unlike instructive
    feedback ...
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习发挥了人类从经验中学习的思想。像生成模型一样，它基于**评估反馈**进行学习。与指导性反馈不同...
- en: Technical requirements
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will be utilizing TensorFlow in Python. We will also be
    using the OpenAI gym to test our algorithms.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Python中的TensorFlow。同时，我们还将使用OpenAI gym来测试我们的算法。
- en: OpenAI gym is an open source toolkit for developing and testing reinforcement
    learning algorithms. Written in Python, there are environments from Atari games
    to robot simulations. As we develop reinforcement learning algorithms in this
    chapter and later chapters, gym will give us access to test environments that
    would otherwise be very complicated to construct on our own.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI gym是一个用于开发和测试强化学习算法的开源工具包。它是用Python编写的，提供了从Atari游戏到机器人仿真等多种环境。当我们在本章及后续章节中开发强化学习算法时，gym将为我们提供可以测试的环境，而这些环境通常是我们自己构建起来非常复杂的。
- en: 'You will need either a macOS or Linux environment to run gym. You can install
    gym by running a simple `pip install` command:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你将需要macOS或Linux环境来运行gym。你可以通过运行一个简单的`pip install`命令来安装gym：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You should now have gym installed! If you've run into an error, you may have
    dependency issues. Check the official gym GitHub repo for the latest dependencies
    ([https://github.com/openai/gym](https://github.com/openai/gym)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该已经安装了gym！如果遇到错误，可能是依赖问题。请检查官方gym GitHub仓库以获取最新的依赖项（[https://github.com/openai/gym](https://github.com/openai/gym)）。
- en: Principles of reinforcement learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习原理
- en: 'Reinforcement learning is based on the concept of learning from interaction
    with a surrounding environment and consequently rewarding positive actions taken
    in that environment. In reinforcement learning, we refer to our algorithm as the **agent** because
    it takes action on the world around it:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习基于从与周围环境的互动中学习，并因此奖励在环境中采取的正面行动的概念。在强化学习中，我们将我们的算法称为**智能体**，因为它对周围的世界采取行动：
- en: '![](img/bb5e4352-6aa7-4258-adf7-dade399596c8.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb5e4352-6aa7-4258-adf7-dade399596c8.png)'
- en: When an agent takes an action, it receives a reward or penalty depending on
    whether it took the *correct *action or not. Our goal in reinforcement learning
    is to let the agent learn to take actions that maximize the rewards it receives
    from its environment. These concepts are not at all new; in fact, they've been
    around ...
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当智能体采取行动时，依据它是否采取了*正确*的行动，它会获得奖励或惩罚。我们在强化学习中的目标是让智能体学会采取能够最大化其从环境中获得的奖励的行动。这些概念并不新颖；事实上，它们已经存在...
- en: Markov processes
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 马尔可夫过程
- en: At the crux of reinforcement learning is the **Markov Decision process** (**MDP**).
    Markov processes are random strings of events where the future probabilities of
    events happening are determined by the probability of the most recent event. They
    extend the basic Markov Chain by adding rewards and decisions to the process.
    The fundamental problem of reinforcement learning can be modeled as an MDP. **Markov
    models** are a general class of models that are utilized to solve MDPs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的核心是**马尔可夫决策过程**（**MDP**）。马尔可夫过程是随机事件的串联，其中事件发生的未来概率由最近事件的概率决定。它们通过在过程中特别添加奖励和决策扩展了基本的马尔可夫链。强化学习的基本问题可以建模为一个MDP。**马尔可夫模型**是一类广泛应用于解决MDP问题的模型。
- en: '**Markov models** rely on a very important property, called the **Markov property**,
    where the current state in a Markov process completely characterizes and explains
    the state of the world at that time; everything we need to know about predicting
    future events is dependent on where we are in the process. For instance, the following
    Markov process models the state of the stock market at any given time. There are
    three states– a **Bull market**, a **Bear market**, or a **Stagnant market **–
    and the respective probabilities for staying in each **state** or transitioning
    to another state:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**马尔可夫模型**依赖于一个非常重要的属性，称为**马尔可夫性质**，即在马尔可夫过程中的当前状态完全表征并解释了那个时刻世界的状态；我们需要了解的关于预测未来事件的一切都依赖于我们当前所处的过程。例如，以下的马尔可夫过程模型描述了股市在任何给定时刻的状态。共有三种状态——**牛市**、**熊市**或**停滞市场**——以及在每个**状态**中停留或转换到另一个状态的相应概率：'
- en: '![](img/5df44dcc-f24f-45d3-94cd-e6f3b9cc3cd1.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5df44dcc-f24f-45d3-94cd-e6f3b9cc3cd1.png)'
- en: 'The entity that navigates an MDP is called an **agent**. In this case, the
    agent would be the stock market itself. We can remember the parameters of the
    Markov process by SAP:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 导航马尔可夫决策过程（MDP）的实体称为**代理**。在这个例子中，代理就是股市本身。我们可以通过SAP记住马尔可夫过程的参数：
- en: '**Set of possible states** (**S**):The possible states of being that an agent
    can be in at any given time. When we talk about states in reinforcement learning,
    this is what we are referring to.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可能的状态集**（**S**）：代理在任何给定时刻可能处于的状态。我们在强化学习中谈论的状态指的就是这个。'
- en: '**Set of possible actions** (**A**): All of the possible actions that an agent
    can take in its environment. These are the lines between the states; what actions
    can happen between two states?'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可能的行动集**（**A**）：代理在其环境中可以采取的所有可能的行动。这些是状态之间的连接线；在两个状态之间可以发生哪些行动？'
- en: '**Transition probability** (**P**): The probability of moving to any of the
    new given states.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转移概率**（**P**）：转移到任何给定新状态的概率。'
- en: The goal of any reinforcement learning agent is to solve a given MDP by maximizing
    the **reward** it receives from taking specific actions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 任何强化学习代理的目标是通过最大化它从采取特定行动中获得的**奖励**来解决给定的MDP。
- en: Rewards
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励
- en: 'As we mentioned previously, reinforcement learning algorithms seek to maximize
    their potential future reward. In deep learning languages, we call this the expected **reward**.
    At each time step, *t*, in the training process of a reinforcement learning algorithm,
    we want to maximize the return, *R*:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，强化学习算法的目标是最大化它们潜在的未来奖励。在深度学习术语中，我们称之为预期**奖励**。在强化学习算法的训练过程中，每个时间步骤*t*，我们希望最大化回报*R*：
- en: '![](img/dddc4ed9-78e8-410a-8c21-4484f8c0a281.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dddc4ed9-78e8-410a-8c21-4484f8c0a281.png)'
- en: 'Our final reward is the summation of all of the expected rewards at each time
    step – we call this the **cumulative reward**. Mathematically, we can write the
    preceding equation as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终奖励是每个时间步骤中所有预期奖励的总和——我们称之为**累积奖励**。在数学上，我们可以将上述方程式写作：
- en: '![](img/e8b23352-c032-4d95-a205-32d3e3c353a2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8b23352-c032-4d95-a205-32d3e3c353a2.png)'
- en: Theoretically, this process could go on forever; the termination ...
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，这个过程可以永远持续下去；终止...
- en: Policies
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略
- en: 'A policy, simply stated, is a way of acting; your place of employment or education
    has policies about how, what, and when you can do things. This term is no different
    when used in the context of reinforcement learning. We use policies to map states
    to potential actions that a reinforcement learning agent can take. Mathematically
    speaking, policies in reinforcement learning are represented by the Greek letter
    π, and they tell an agent what action to take at any given state in an MDP. Let''s
    look at a simple MDP to examine this; imagine that you are up late at night, you
    are sleepy, but maybe you are stuck into a good movie. Do you stay awake or go
    to bed? In this scenario, we would have three states:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 策略，简单来说，就是行动的方式；你的工作或教育机构有关于你如何、何时以及做什么事情的政策。在强化学习的语境下，这个术语没有区别。我们使用策略将状态映射到强化学习代理可以采取的潜在行动。数学上来说，强化学习中的策略用希腊字母π表示，它们告诉代理在给定的状态下应该采取什么行动。让我们通过一个简单的MDP来检验这一点；假设你熬夜到很晚，感到困倦，但也许你正沉浸在一部好电影中。你是继续清醒还是去睡觉？在这个场景中，我们会有三个状态：
- en: Your initial state of sleepiness
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你最初的困倦状态
- en: Being well rested
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 充分休息
- en: Being sleep deprived
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 睡眠不足
- en: Each of these states has a transition probability and reward associated with
    taking an action based on them.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每个状态都有与其相关的转移概率和奖励，这些基于采取的行动。
- en: '![](img/838e883c-5989-4ca8-b55f-ff3db83f0249.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/838e883c-5989-4ca8-b55f-ff3db83f0249.png)'
- en: For example, Let's say you decide to stay up. In this scenario, you'll find
    yourself in the **Don't Sleep** transition state. From here, there is only one
    place to go - a 100% chance of ending up in the **Sleep Deprived** State.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你决定熬夜。在这种情况下，你会发现自己处于**不睡觉**的转移状态。从这里，你只能去一个地方——100%的概率进入**睡眠不足**状态。
- en: If you slept, there is a 90% chance of you being well rested and a 10% chance
    of you still being tired. You could return to your tired state by not sleeping
    as well. In this scenario, we'd want to choose the actions (sleeping) that maximize
    our rewards. That voice in your head that's telling you to go to sleep is the
    policy.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你睡觉了，那么你有90%的概率会恢复精力，10%的概率还是会感到疲倦。你也有可能因为没有睡得好而回到疲劳的状态。在这种情况下，我们希望选择那些能最大化奖励的行动（睡觉）。你脑海中告诉你去睡觉的声音就是策略。
- en: Our objective is to learn a policy (![](img/f069ed33-7df1-4b2e-8f86-580c65a7db09.png))
    that maximizes the network's reward; we call this the **optimal policy**. In this
    case, the optimal policy is **deterministic**, meaning that there is a clear optimal
    action to take at each state. Policies can also be **stochastic**, meaning that
    there is a distribution of possible actions that can be drawn from.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是学习一个策略 (![](img/f069ed33-7df1-4b2e-8f86-580c65a7db09.png))，使得网络的奖励最大化；我们称之为**最优策略**。在这种情况下，最优策略是**确定性的**，意味着在每个状态下都有一个明确的最优行动可采取。策略也可以是**随机的**，即从中可以抽取一个行动的分布。
- en: Reinforcement learning agents can learn **on-policy** or **off-policy**; when
    an algorithm is on-policy, it learns the policy from all of the agent's actions,
    including exploratory actions that it may take. It improves the *existing policy*.
    Off-policy learning is *off from the previous policy*, or in other words, evaluating
    or learning a policy that was different from the original policy. Off-policy learning
    happens independent of an agent's previous. Later on in this chapter, we'll discuss
    two different approaches to reinforcement learning – one on-policy (policy gradients)
    and one off-policy (Q-learning).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习代理可以**在策略内**或**在策略外**学习；当算法在策略内时，它通过所有代理的行动来学习策略，包括它可能采取的探索性行动。它改进了*现有策略*。在策略外学习是指*偏离之前的策略*，换句话说，是评估或学习一个与原始策略不同的策略。在本章后面，我们将讨论两种不同的强化学习方法——一种是**在策略内**的（策略梯度），另一种是**在策略外**的（Q学习）。
- en: To help our algorithm learn an optimal policy, we utilize **value functions**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们的算法学习最优策略，我们使用**价值函数**。
- en: Value functions
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 价值函数
- en: A value function helps us measure the expected reward at certain states; it
    represents the expected cumulative reward from following a certain policy at any
    given state. There are two types of value functions used in the field of reinforcement
    learning; **state value functions ***V*(*s*) and **action value functions ![](img/c08de824-20ac-4874-a897-77581021565b.png)**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数帮助我们衡量在某些状态下的预期奖励；它表示在任何给定状态下，遵循某个策略的预期累计奖励。强化学习领域使用两种类型的价值函数；**状态价值函数**
    *V*(*s*) 和 **行动价值函数** ![](img/c08de824-20ac-4874-a897-77581021565b.png)。
- en: 'The state value function describes the value of a state when following a policy.
    It is the expected return that an agent will achieve when starting at state *s* under
    a policy π. This function will give us the expected reward for an agent given
    that it starts following a policy at state*s*:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 状态价值函数描述了在遵循某个策略时，一个状态的价值。它是代理在某一状态*s*下，遵循策略π时，预期获得的回报。这个函数会给我们代理在开始遵循某个策略时，在状态*s*下的预期奖励：
- en: Let's break down what this function ...
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下这个函数...
- en: The Bellman equation
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝尔曼方程
- en: 'As one of the most important equations in the entire field of reinforcement
    learning, the Bellman equation is the cornerstone of solving reinforcement learning
    problems. Developed by applied mathematician Richard Bellman, it''s less of a
    equation and more of a condition of optimization that models the reward of an
    agent''s decision at a point in time based on the expected choices and rewards
    that could come from said decision. The Bellman equation can be derived for either
    the state value function or the action value function:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 作为强化学习领域最重要的方程之一，贝尔曼方程是解决强化学习问题的基石。贝尔曼是应用数学家，他开发了这个方程，它不仅仅是一个方程，更像是一个优化条件，用来模拟代理在某一时刻决策的奖励，基于该决策可能带来的预期选择和奖励。贝尔曼方程可以为状态值函数或动作值函数推导：
- en: '![](img/6b28e04b-517e-4e36-a3b8-a3dac6d176e5.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b28e04b-517e-4e36-a3b8-a3dac6d176e5.png)'
- en: '![](img/43056f1b-558f-4419-9e41-5644a86bc786.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43056f1b-558f-4419-9e41-5644a86bc786.png)'
- en: 'As usual, let''s break down these equations. We''re going to focus on the state
    value function. First, we have the summation of all policies for every state/action
    pair:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，让我们分解这些方程式。我们将重点关注状态值函数。首先，我们有每个状态/行动对的所有策略的总和：
- en: '![](img/bd04bdaf-5aa1-4575-9faa-20dd81d56f38.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd04bdaf-5aa1-4575-9faa-20dd81d56f38.png)'
- en: 'Next, we have the transition probability; it''s the probability of being in
    state *s*, taking action *a*, and ending up in state ![](img/825e8573-ef10-4033-8474-2e48bda6425a.png):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们有转移概率；它是处于状态*s*，采取行动*a*，并最终到达状态的概率！[](img/825e8573-ef10-4033-8474-2e48bda6425a.png)：
- en: '![](img/f6d0db5d-ba0c-4f29-9cef-64e094924511.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6d0db5d-ba0c-4f29-9cef-64e094924511.png)'
- en: 'Next is the cumulative reward that we discussed earlier:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们之前讨论的累计奖励：
- en: '![](img/29d7d849-d342-487c-b4b3-32ff1fbec440.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29d7d849-d342-487c-b4b3-32ff1fbec440.png)'
- en: 'Lastly, we have the discounted value of the function:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有该函数的折扣值：
- en: '![](img/568aee06-6340-4d42-90c5-515f1b87864e.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/568aee06-6340-4d42-90c5-515f1b87864e.png)'
- en: 'Altogether, we''re describing the entire reinforcement learning process; we
    want to find a state value function or an action value function that satisfies
    the Bellman equation. We''re missing one key part here; how do we solve this in
    practice? One option is to use a paradigm called **dynamic programming**, which
    is an optimization method that was also developed by Bellman himself. One means
    of solving for the optimal policy with dynamic programming is to use the **value
    iteration method**. In this manner, we use the Bellman equation as an iterative
    update function. We want to converge from Q to Q* by enforcing the Bellman equation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们描述了整个强化学习过程；我们希望找到一个满足贝尔曼方程的状态值函数或动作值函数。这里缺少一个关键部分；我们如何在实践中解决这个问题？一种选择是使用一种叫做**动态规划**的方法，它也是贝尔曼本人开发的优化方法。使用动态规划求解最优策略的一种方法是使用**值迭代法**。通过这种方式，我们将贝尔曼方程作为一个迭代更新函数。我们希望通过强制执行贝尔曼方程从Q收敛到Q*：
- en: '![](img/9ae2e7b8-896c-4e3d-b9b1-1921a1ea5b86.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ae2e7b8-896c-4e3d-b9b1-1921a1ea5b86.png)'
- en: Let's see how this would work in Python by trying to solve the cartpole problem. With
    value iteration, we start with a random value function and then find an improved
    value function in an iterative process until we reach an optimal value function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这在Python中是如何工作的，通过尝试解决推车倒立摆问题。通过值迭代法，我们从一个随机值函数开始，然后通过迭代过程找到改进的值函数，直到我们得到一个最优值函数。
- en: 'We can attempt this on the cartpole problem. Let''s import `gym` and generate
    a random value function to begin with:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试在推车倒立摆问题上进行这个操作。让我们导入`gym`并先生成一个随机的值函数：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, let''s turn that policy into an action:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们把策略转化为行动：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Lastly, we can run the training process:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以运行训练过程：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: While value iterations work in this simplistic environment, we can quickly run
    into problems when utilizing it in larger, more complex environments. As we have
    to individually compute the value for every state/value pair, many unstructured
    inputs such as images become impossibly large. Imagine how expensive it would
    be to compute this function for every single pixel in an advanced video game,
    every time our reinforcement learning algorithm tried to make a move!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在这种简单的环境中值迭代方法有效，但当我们将其应用于更大、更复杂的环境时，很快就会遇到问题。因为我们必须单独计算每个状态/值对的值，许多未结构化的输入（如图像）变得庞大得难以处理。想象一下，如果每次我们的强化学习算法试图进行一次动作时，都需要为高级视频游戏中的每一个像素计算这个函数，那将是多么昂贵！
- en: For this reason, we utilize deep learning methods to do these computations for
    us. Deep Neural Networks can act as function approximators. There are two primary
    methods, called **Deep Q**-**learning** and **policy gradients**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正因为如此，我们使用深度学习方法来为我们进行这些计算。深度神经网络可以充当函数逼近器。主要有两种方法，分别是**深度Q学习**和**策略梯度**。
- en: Q–learning
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q–学习
- en: Q-learning is a reinforcement learning method that utilizes the action value
    function, or Q function, to solve tasks. In this section, we'll talk about both
    traditional Q-learning as well as Deep Q-learning.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习是一种利用动作值函数或Q函数来解决任务的强化学习方法。在本节中，我们将讨论传统的Q学习和深度Q学习。
- en: 'Standard Q-learning works off the core concept of the Q-table. You can think
    of the Q-table as a reference table; every row represents a state and every column
    represents an action. The values of the table are the expected future rewards
    that are received for a specific combination of actions and states. Procedurally,
    we do the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 标准Q学习基于Q表的核心概念。你可以将Q表看作一个参考表；每一行代表一个状态，每一列代表一个动作。表中的值是针对特定的动作和状态组合所获得的预期未来奖励。从程序步骤上，我们做如下操作：
- en: Initialize the Q-table
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化Q表
- en: Choose an action
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个动作
- en: Perform that action
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作
- en: Measure the reward that was received
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量获得的奖励
- en: Update the Q- value
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新Q值
- en: Let's walk through each of these steps to better understand the algorithm. ...
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步解析这些步骤，以更好地理解算法。…
- en: Policy optimization
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略优化
- en: '**Policy optimization methods** are an alternative to Q-learning and value
    function approximation. Instead of learning the Q-values for state/action pairs,
    these methods directly learn a policy π that maps state to an action by calculating
    a gradient. Fundamentally, for a search such as for an optimization problem, policy
    methods are a means of learning the correct policy from a stochastic distribution
    of potential policy actions. Therefore, our network architecture changes a bit
    to learn a policy directly:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略优化方法**是Q学习和价值函数逼近的替代方法。这些方法不是学习状态/动作对的Q值，而是通过计算梯度直接学习一个将状态映射到动作的策略π。从根本上讲，对于像优化问题这样的搜索，策略方法是一种从潜在的政策动作的随机分布中学习正确策略的手段。因此，我们的网络架构稍作调整，以直接学习策略：'
- en: '![](img/e760c7dd-ebb7-4f86-80a1-99b79ca3f002.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e760c7dd-ebb7-4f86-80a1-99b79ca3f002.png)'
- en: 'Because every state has a distribution of possible actions, the optimization
    problem becomes easier. We no longer have to compute exact rewards for specific
    actions. Recall that deep learning methods rely on the concept of an episode.
    In the case of deep reinforcement learning, each episode represents a game or
    task, while **trajectories** represent plays or directions within that game or
    task. We can define a trajectory as a path of state/action/reward combinations,
    represented by the Greek letter tau (*Τ*):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个状态都有一组可能的动作分布，所以优化问题变得更简单。我们不再需要为特定动作计算精确的奖励。回想一下，深度学习方法依赖于“回合”这一概念。在深度强化学习中，每个回合代表一场游戏或任务，而**轨迹**代表游戏或任务中的某些动作或方向。我们可以将轨迹定义为状态/动作/奖励组合的路径，用希腊字母tau
    (*Τ*)表示：
- en: '![](img/a2ccc04a-71d3-4348-9738-0e06229375e7.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2ccc04a-71d3-4348-9738-0e06229375e7.png)'
- en: Think about a robot learning to walk; if we used Q-learning or another dynamic
    programming method for this task, our algorithms would need to learn exactly how
    much reward to assign to every single joint movement for every possible trajectory.
    The algorithm would need to learn timings, exact angles to bend the robotic limbs,
    and so on. By learning a policy directly, the algorithm can simply focus on the
    overall task of moving the robot's feet when it walks.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个机器人学习走路；如果我们使用Q学习或其他动态规划方法来完成这个任务，我们的算法需要精确地学习如何为每个可能的轨迹中的每个关节动作分配奖励。算法需要学习时机、如何弯曲机器人的肢体角度，等等。通过直接学习策略，算法可以简单地专注于当机器人走路时，如何移动机器人的脚。
- en: 'When utilizing policy gradient methods, we can define an individual policy
    in the same simple manner as we did with Q-learning; our policy is the expected
    sum of future discounted rewards:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用策略梯度方法时，我们可以像使用Q学习那样简单地定义一个个体策略；我们的策略是未来折扣奖励的期望总和：
- en: '![](img/679aca0a-2d0d-4591-baf9-47de01f4025f.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/679aca0a-2d0d-4591-baf9-47de01f4025f.png)'
- en: 'Therefore, the goal of our network becomes to *maximize* some policy, *J*,
    in order to maximize our expected future reward:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们网络的目标是*最大化*某个策略，*J*，以最大化我们期望的未来奖励：
- en: '![](img/effaee75-f1d0-4f0e-a1db-7efaede13cdf.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/effaee75-f1d0-4f0e-a1db-7efaede13cdf.png)'
- en: 'One way to learn a policy is to use gradient methods, hence the name *policy*
    gradients. Since we want to find a policy that maximizes reward, we perform the
    opposite of gradient descent, *gradient ascent*, on a distribution of potential
    policies:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 学习策略的一种方法是使用梯度方法，因此得名*策略*梯度。由于我们希望找到最大化奖励的策略，我们在潜在策略的分布上执行与梯度下降相反的操作——*梯度上升*：
- en: '![](img/9f9110be-7efa-4c04-8d9d-ad8bbee171e1.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f9110be-7efa-4c04-8d9d-ad8bbee171e1.png)'
- en: 'The parameters of the new neural network become the parameters of our policy,
    so by performing gradient ascent and updating the network parameters, ![](img/1fe06433-f3bb-4efd-a882-95e32b284d69.png)becomes ![](img/449d7209-ba25-4190-8b0b-f499112d9a48.png). Fully
    deriving policy gradients is outside the scope of this book, however, let''s touch
    upon some high-level concepts of how policy gradient methods work in practice.
    Take a look at the following landscape; there are three potential paths with three
    potential end rewards:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 新神经网络的参数成为我们策略的参数，因此通过执行梯度上升并更新网络参数，![](img/1fe06433-f3bb-4efd-a882-95e32b284d69.png)变为![](img/449d7209-ba25-4190-8b0b-f499112d9a48.png)。完整推导策略梯度超出了本书的范围，但让我们简要介绍一些策略梯度方法在实际中的高级概念。请看以下的景观；有三条潜在路径和三种可能的最终奖励：
- en: '![](img/f98503e9-e293-4df2-a122-80db07f2d5ec.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f98503e9-e293-4df2-a122-80db07f2d5ec.png)'
- en: 'Policy gradient methods seek to increase the probability of taking the path
    that leads to the highest reward. Mathematically, the standard policy gradient
    method looks as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法旨在增加采取通向最高奖励路径的概率。从数学上讲，标准的策略梯度方法如下所示：
- en: '![](img/8bf327fa-4f88-42ab-89aa-1a183c69e620.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bf327fa-4f88-42ab-89aa-1a183c69e620.png)'
- en: 'As usual, let''s break this down a bit. We''ll start by saying that our gradient,
    *g*, is the expected value of the gradient *times* the log of our policy for a
    given action/state pair, times the **advantage**. Advantage is simply the action
    value function minus the state value function:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，让我们稍微分解一下。我们可以从以下几点开始：我们的梯度，*g*，是梯度期望值与某一给定动作/状态对的策略的对数相乘，再乘以**优势**。优势即为动作价值函数减去状态价值函数：
- en: '![](img/ef3592e7-994c-43bd-9d6b-cd714f9512e0.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef3592e7-994c-43bd-9d6b-cd714f9512e0.png)'
- en: In implementation, vanilla policy gradient methods still run into problems.
    One particularly notable issue is called the credit assignment problem, in which
    a reward signal received from a long trajectory of interaction happens at the
    end of the trajectory. Policy gradient methods have a hard time ascertaining which
    action caused this reward and finds difficulty *assigning credit* for that action.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现中，传统的策略梯度方法仍然会遇到问题。一个特别显著的问题被称为“信用分配问题”，即从一个长时间交互轨迹中获得的奖励信号发生在轨迹的末尾。策略梯度方法很难确定是哪一步动作导致了这个奖励，并且难以*分配信用*给该动作。
- en: Policy gradients are an outline for an entire class of algorithms that can be
    built around this idea. The key to optimizing these methods is in the details
    of individual optimization procedures. In the next section, we'll look at several
    means to improve on vanilla policy gradient methods.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度是一类可以围绕这一思想构建的算法的概述。优化这些方法的关键在于各个优化过程的细节。在接下来的部分，我们将探讨几种改进传统策略梯度方法的方式。
- en: Extensions on policy optimization
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略优化的扩展
- en: One common way to compute policy gradients is with the **Reinforce** **algorithm**. Reinforce
    is a Monte-Carlo policy gradient method that uses likelihood ratios to estimate
    the value of a policy at a given point. The algorithm can lead to high variance.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 计算策略梯度的一种常见方法是**Reinforce** **算法**。Reinforce是一种蒙特卡洛策略梯度方法，它使用似然比来估计给定点上策略的价值。该算法可能导致较高的方差。
- en: Vanilla policy gradient methods can be challenging as they are extremely sensitive
    to what you choose for your step size parameter. Choose a step size too big and
    the correct policy is overwhelmed by noise – too small and the training becomes
    incredibly slow. Our next class of reinforcement learning algorithms, **proximal
    policy optimization** (**PPO**), seeks to remedy this shortcoming of policy gradients. PPO is
    a new class of reinforcement learning algorithms that was ...
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的策略梯度方法可能会比较具有挑战性，因为它们对步长参数的选择极其敏感。步长过大会使得正确的策略被噪声压倒——步长过小则训练变得异常缓慢。我们的下一类强化学习算法，**近端策略优化**（**PPO**），旨在解决策略梯度的这一短板。PPO是一类新的强化学习算法，旨在...
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned the important foundations of reinforcement learning,
    one of the most visible practices in the AI field.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了强化学习的重要基础，它是人工智能领域中最为显著的实践之一。
- en: Reinforcement learning is based on the concepts of agents acting in an environment
    and taking action based on what it sees in its surrounding environment. An agent's
    actions are guided by either policy optimization methods or dynamic programming
    methods that help it learn how to interact with its environment. We use dynamic
    programming methods when we care more about exploration and off-policy learning.
    On the other hand, we use policy optimization methods when we have dense, continuous
    problem spaces and we only want to optimize for what we care about.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习基于智能体在环境中行动的概念，并根据它所观察到的周围环境做出决策。智能体的行为由策略优化方法或动态规划方法指导，这些方法帮助它学习如何与环境互动。当我们更注重探索和离策略学习时，我们使用动态规划方法。另一方面，当我们面临密集的、连续的问题空间且只想优化我们关心的内容时，我们使用策略优化方法。
- en: We'll look at several different real-world applications of reinforcement learning
    in the upcoming chapter.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨强化学习在多个现实世界中的应用。
