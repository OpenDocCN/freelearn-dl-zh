- en: Creating a Chatbot
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建聊天机器人
- en: Dialogue agents and chatbots have been on the rise in recent years. Many businesses
    have resorted to chatbots to answer customer inquiries, and this has been largely
    successful. Chatbots have been growing quickly, at 5.6x in the last year ([https://chatbotsmagazine.com/chatbot-report-2018-global-trends-and-analysis-4d8bbe4d924b](https://chatbotsmagazine.com/chatbot-report-2018-global-trends-and-analysis-4d8bbe4d924b)). Chatbots
    can help organizations to communicate and interact with customers without any
    human intervention, at a very minimal cost. Over 51% of customers have stated
    that they want businesses to be available 24/7, and they expect replies in less
    than one hour. For businesses to achieve this kind of success in an affordable
    manner, especially with a large customer base, they must resort to chatbots.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 对话代理和聊天机器人近年来不断崛起。许多企业已开始依靠聊天机器人来回答客户的咨询，这一做法取得了显著成功。聊天机器人在过去一年增长了5.6倍 ([https://chatbotsmagazine.com/chatbot-report-2018-global-trends-and-analysis-4d8bbe4d924b](https://chatbotsmagazine.com/chatbot-report-2018-global-trends-and-analysis-4d8bbe4d924b))。聊天机器人可以帮助组织与客户进行沟通和互动，且无需人工干预，成本非常低廉。超过51%的客户表示，他们希望企业能够24/7提供服务，并期望在一小时内得到回复。为了以一种负担得起的方式实现这一成功，尤其是在拥有大量客户的情况下，企业必须依赖聊天机器人。
- en: The background problem
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 背景问题
- en: Many chatbots are created with regular machine learning natural language processing
    algorithms, and these focus on immediate responses. A new concept is to create
    chatbots with the use of deep reinforcement learning. This would mean that the
    future implications of our immediate responses would be considered to maintain
    coherence.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 许多聊天机器人是使用常规的机器学习自然语言处理算法创建的，这些算法侧重于即时响应。一个新的概念是使用深度强化学习来创建聊天机器人。这意味着我们会考虑即时响应的未来影响，以保持对话的连贯性。
- en: In this chapter, you will learn how to apply deep reinforcement learning to
    natural language processing. Our reward function will be a future-looking function,
    and you will learn how to think probabilistically through the creation of this
    function.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，你将学习如何将深度强化学习应用于自然语言处理。我们的奖励函数将是一个面向未来的函数，您将通过创建该函数学会如何从概率的角度思考。
- en: Dataset
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: The dataset that we will use mainly consists of conversations from selected
    movies. This dataset will help to stimulate and understand conversational methods
    in the chatbot. Also, there are movie lines, which are essentially the same as
    the movie conversations, albeit shorter exchanges between people. Other data sets
    that will be used include some containing movie titles, movie characters, and
    raw scripts.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的这个数据集主要由选定电影中的对话组成。这个数据集有助于激发并理解聊天机器人的对话方法。此外，其中还包含电影台词，这些台词与电影中的对话本质相同，不过是人与人之间较简短的交流。其他将使用的数据集还包括一些包含电影标题、电影角色和原始剧本的数据集。
- en: Step-by-step guide
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分步指南
- en: 'Our solution will use modeling and will focus on the future direction of a
    dialogue agent, so as to generate coherent and interesting dialogue. The model
    will simulate the dialogue between two virtual agents, with the use of policy
    gradient methods. These methods are designed to reward the sequences of interaction
    that display three important properties of conversation: informativeness (non-repeating
    turns), high coherence, and simplicity in answering (this is related to the forward-looking
    function). In our solution, an action will be defined as the dialogue or communication
    utterance that the chatbot generates. Also, a state will be defined as the two
    previous interaction turns. In order to achieve all of this, we will use the scripts
    in the following sections.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解决方案将使用建模方法，重点关注对话代理的未来方向，从而生成连贯且有趣的对话。该模型将模拟两个虚拟代理之间的对话，使用策略梯度方法。这些方法旨在奖励显示出对话三个重要特性的交互序列：信息性（不重复的回合）、高度连贯性和简洁的回答（这与面向未来的函数相关）。在我们的解决方案中，动作将被定义为聊天机器人生成的对话或交流话语。此外，状态将被定义为之前的两轮互动。为了实现这一目标，我们将使用以下章节中的剧本。
- en: Data parser
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据解析器
- en: 'The data parser script is designed to help with the cleaning and preprocessing
    of our datasets. There are a number of dependencies in this script, such as `pickle`,
    `codecs`, `re`, `OS`, `time`, and `numpy`. This script contains three functions.
    The first function helps to filter words, by preprocessing word counts and creating
    vocabulary based on word count thresholds. The second function helps to parse
    all words into this script, and the third function helps to extract only the defined
    vocabulary from the data:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据解析脚本旨在帮助清理和预处理我们的数据集。此脚本有多个依赖项，如`pickle`、`codecs`、`re`、`OS`、`time`和`numpy`。该脚本包含三个功能。第一个功能帮助通过预处理词频并基于词频阈值创建词汇表来过滤词汇。第二个功能帮助解析所有词汇到此脚本中，第三个功能帮助从数据中提取仅定义的词汇：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following module cleans and preprocesses the text in the training dataset:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下模块清理并预处理训练数据集中的文本：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, iterate through the captions and create the vocabulary.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，遍历字幕并创建词汇表。
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, parse all the words from the movie lines.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，解析所有电影台词中的词汇。
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Extract only the vocabulary part of the data, as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 仅提取数据中的词汇部分，如下所示：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, the utterance dictionary is created and stored.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建并存储话语字典。
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The data is parsed and can be utilized in further steps.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据已解析，并可以在后续步骤中使用。
- en: Data reader
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据读取
- en: 'The data reader script helps to generate trainable batches from the preprocessed
    training text from the data parser script. Let''s start by importing the required
    methods:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据读取脚本帮助从数据解析脚本预处理后的训练文本中生成可训练的批次。我们首先通过导入所需的方法开始：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This helper module helps generate trainable batches from the preprocessed training
    text.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此辅助模块帮助从预处理后的训练文本中生成可训练的批次。
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following code gets the batch number from the data:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码从数据中获取批次号：
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following code shuffles the index from the data:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码打乱来自数据的索引：
- en: '[PRE9]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following code generates the batch indices, based on the batch number that
    was obtained earlier:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码基于之前获取的批次号生成批次索引：
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following code generates the training batch:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码生成训练批次：
- en: '[PRE11]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The following function generates training batch with the former.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数使用前者生成训练批次。
- en: '[PRE12]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following code generates the testing batch:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码生成测试批次：
- en: '[PRE13]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This concludes the data reading part.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分内容结束于数据读取。
- en: Helper methods
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 辅助方法
- en: 'This script consists of a `Seq2seq` dialogue generator model, which is used
    for the reverse model of the backward entropy loss. This will determine the semantic
    coherence reward for the policy gradients dialogue. Essentially, this script will
    help us to represent our future reward function. The script will achieve this
    via the following actions:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本由一个`Seq2seq`对话生成模型组成，用于反向模型的逆向熵损失。它将确定政策梯度对话的语义连贯性奖励。实质上，该脚本将帮助我们表示未来的奖励函数。该脚本将通过以下操作实现：
- en: Encoding
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码
- en: Decoding
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码
- en: Generating builds
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成构建
- en: All of the preceding actions are based on **long short-term memory** (**LSTM**)
    units.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所有先前的操作都基于**长短期记忆**（**LSTM**）单元。
- en: The feature extractor script helps with the extraction of features and characteristics
    from the data, in order to help us train it better. Let us start by importing
    the required modules.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取脚本帮助从数据中提取特征和特性，以便更好地训练它。我们首先通过导入所需的模块开始。
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, define the model inputs. If reinforcement learning is set to True, a scalar
    is computed based on semantic coherence and ease of answering loss caption.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义模型输入。如果强化学习被设置为True，则基于语义连贯性和回答损失字幕的易用性计算标量。
- en: '[PRE15]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next, define the encoding layers which perform encoding for the sequence to
    sequence network. The input sequence is passed into the encoder and returns the
    output of RNN output and the state.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义执行序列到序列网络编码的编码层。输入序列传递给编码器，并返回RNN输出和状态。
- en: '[PRE16]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, define the training process for decoder using LSTMS cells with the encoder
    state together with the decoder inputs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义使用LSTM单元的解码器训练过程，结合编码器状态和解码器输入。
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, define an inference decoder similar to the one used for the training.
    Makes use of a greedy helper which feeds the last output of the decoder as the
    next decoder input. The output returned contains the training logits and the sample
    id.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义一个类似于训练时使用的推理解码器。使用贪心策略辅助工具，将解码器的最后输出作为下一个解码器输入。返回的输出包含训练logits和样本ID。
- en: '[PRE18]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, create a decoding layer.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建解码层。
- en: '[PRE19]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Next, create the bos inclusion which appends the index corresponding to <bos>
    referring to the beginning of a sentence to the first index of the caption tensor
    for every batch.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建 bos 包含部分，将对应于 <bos> 的索引添加到每个批次的标题张量的第一个索引，<bos> 表示句子的开始。
- en: '[PRE20]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, define pad sequences which creates an array of size maxlen from every
    question by padding with zeros or truncating where necessary.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义 pad 序列，该方法通过用零填充或在必要时截断每个问题，创建大小为 maxlen 的数组。
- en: '[PRE21]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Ignore non-vocabulary parts if the data and take only all alphabets.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据中存在非词汇部分，请忽略它们，只保留所有字母。
- en: '[PRE22]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Next, create batches to be fed into the network from in word vector representation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建批次，将词向量表示送入网络。
- en: '[PRE23]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Next, generate sentences from the predicted indices. Replace <unk>, <pad> with
    the word with the next highest probability whenever predicted.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从预测的索引生成句子。每当预测时，将 <unk> 和 <pad> 替换为具有下一个最高概率的单词。
- en: '[PRE24]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This concludes all the helper functions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了所有辅助函数。
- en: Chatbot model
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聊天机器人模型
- en: The following script contains the policy gradient model, which will be used
    where it combines reinforcement learning rewards with the cross-entropy loss.
    The dependencies include `numpy` and `tensorflow`. Our policy gradient is based
    on an LSTM encoder-decoder. We will use a stochastic demonstration of our policy
    gradient, which will be a probability distribution of actions over specified states.
    The script represents all of these, and specifies the policy gradient loss to
    be minimized.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本包含策略梯度模型，它将用于结合强化学习奖励与交叉熵损失。依赖项包括 `numpy` 和 `tensorflow`。我们的策略梯度基于 LSTM
    编码器-解码器。我们将使用策略梯度的随机演示，这将是一个关于指定状态的动作概率分布。该脚本表示了这一切，并指定了需要最小化的策略梯度损失。
- en: Run the output of the first cell through the second cell; the input is concatenated
    with zeros. The final state for the responses mostly consists of two components—the
    latent representation of the input by the encoder, and the state of the decoder,
    based on the selected words. The return includes placeholder tensors and other
    tensors, such as losses and training optimization operation. Let's start by importing
    the required libraries.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过第二个单元运行第一个单元的输出；输入与零拼接。响应的最终状态通常由两个部分组成——编码器对输入的潜在表示，以及基于选定单词的解码器状态。返回的内容包括占位符张量和其他张量，例如损失和训练优化操作。让我们从导入所需的库开始。
- en: '[PRE25]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We will create a chatbot class to create the model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个聊天机器人类来构建模型。
- en: '[PRE26]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Next, create a method that builds the model. If policy gradients are requested,
    then get the input accordingly.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个构建模型的方法。如果请求策略梯度，则根据需要获取输入。
- en: '[PRE27]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Next, get the inference layer.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，获取推理层。
- en: '[PRE28]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Next, get the loss layers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，获取损失层。
- en: '[PRE29]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Depending on the state of the policy gradient, either minimize cross entropy
    loss or policy gradient loss.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 根据策略梯度的状态，选择最小化交叉熵损失或策略梯度损失。
- en: '[PRE30]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now we have all the methods that are required for training.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了训练所需的所有方法。
- en: Training the data
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据
- en: 'The scripts that were written previously were combined with training the dataset. Let''s
    start the training by importing all the modules that are developed in the previous
    sections as shown here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 之前编写的脚本与训练数据集结合起来。让我们通过导入在前面章节中开发的所有模块来开始训练，如下所示：
- en: '[PRE31]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, let''s create a set of generic responses observed in the original `seq2seq` model
    which the policy gradients are trained to avoid:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一组在原始 `seq2seq` 模型中观察到的通用响应，策略梯度会避免这些响应。
- en: '[PRE32]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Next, we will define all the constants that are required for the training. Tha
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义训练所需的所有常量。
- en: '[PRE33]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, define the training function. Based on the type, either the forward or
    reverse sequence to sequence model is loaded. The data is also read in reverse
    model based on the model as shown here:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义训练函数。根据类型，加载前向或反向序列到序列模型。数据也根据模型读取，反向模型如下所示：
- en: '[PRE34]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, create the vocabulary as shown here:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，按照以下方式创建词汇表：
- en: '[PRE35]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The above command print should print the following indicated the vocabulary
    size that is filtered.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令的输出应打印以下内容，表示已过滤的词汇表大小。
- en: '[PRE36]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `word_to_index` variable is filled with the map of filtered words to an
    integer as shown here:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`word_to_index` 变量被填充为过滤后单词到整数的映射，如下所示：'
- en: '[PRE37]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `index_to_word` variable is filled with the map of integer to the filtered
    works which will work as a reverse lookup.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`index_to_word` 变量被填充为从整数到过滤后的单词的映射，这将作为反向查找。'
- en: '[PRE38]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Next, load the word to vector model from `gensim` library.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从`gensim`库加载词到向量的模型。
- en: '[PRE39]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Next, instantiate and build the model the Chatbot model with all the constants
    that were defined. Restore a checkpoint, if present from the previous run or initialize
    the graph.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，实例化并构建聊天机器人模型，使用所有已定义的常量。如果有之前训练的检查点，则恢复它；否则，初始化图。
- en: '[PRE40]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Next, start the training by iterating through the epochs and start the batches.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过迭代纪元并开始批量处理来启动训练。
- en: '[PRE41]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The `batch_input` has the list of words from the training set.  The `batch_target` has
    the list of sentences for the input which will be the target. The list of words
    is converted to vector form using the helper functions. Make the feed dictionary
    for the graph using the transformed inputs, masks and targets.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`batch_input`包含来自训练集的单词列表。`batch_target`包含输入的句子列表，这些句子将作为目标。单词列表通过辅助函数转换为向量形式。使用转换后的输入、掩码和目标构建图的喂入字典。'
- en: '[PRE42]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Next, train the model by calling the optimizer by feeding the training data.
    Log the loss value at certain intervals to see the progress of the training. Save
    the model at the end.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过调用优化器并输入训练数据来训练模型。在某些间隔记录损失值，以查看训练的进展。训练结束后保存模型。
- en: '[PRE43]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The output should appear as shown here.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应如下所示。
- en: '[PRE44]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The model is trained for both forward and reverse and the corresponding models
    are stored. In the next function, the models are restored and trained again to
    create the chatbot.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 模型经过正向和反向训练，相应的模型被存储。在下一个函数中，模型被恢复并重新训练，以创建聊天机器人。
- en: '[PRE45]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Two graphs are created to load the trained models.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 创建两个图表以加载训练好的模型。
- en: '[PRE46]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Next, the data is loaded to train the data in batches.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，加载数据以批量训练数据。
- en: '[PRE47]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Also, learn when to say generic texts as shown here:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，学习何时说出通用文本，如下所示：
- en: '[PRE48]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Next, call the functions defined in sequence. First train a forward model, followed
    by reverse model and policy gradient at the end.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，按顺序调用已定义的函数。首先训练正向模型，然后训练反向模型，最后训练策略梯度。
- en: '[PRE49]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This concludes the training of the chatbot. The model is trained in forward
    and reverse manner to
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着聊天机器人的训练结束。模型通过正向和反向训练
- en: Testing and results
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试和结果
- en: 'After training the model, we tested it against our test dataset and obtained
    reasonably coherent dialogue. There is one very important issue: the context of
    the communication. Hence, depending on the dataset that is used, the result will
    be in its context. For our context, the results that were obtained were very reasonable,
    and they satisfied our three measures of performance—informativeness (non-repeating
    turns), high coherence, and simplicity in answering (this is related to the forward-looking
    function).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型后，我们用测试数据集进行了测试，得到了相当连贯的对话。有一个非常重要的问题：交流的上下文。因此，根据所使用的数据集，结果会有其上下文。就我们的上下文而言，获得的结果非常合理，并且满足了我们的三项性能指标——信息量（无重复回合）、高度连贯性和回答的简洁性（这与前瞻性功能有关）。
- en: '[PRE50]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Next, declare the paths to the various model that are already trained.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，声明已经训练好的各种模型的路径。
- en: '[PRE51]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Next, declare the path of the files consisting of questions and responses.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，声明包含问题和回应的文件路径。
- en: '[PRE52]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Next, declare the constants required for the model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，声明模型所需的常量。
- en: '[PRE53]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, load the data and the model as shown here:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，加载数据和模型，如下所示：
- en: '[PRE54]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, open the responses file and prepare the list of questions as shown here:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开回应文件，并准备如下所示的问题列表：
- en: '[PRE55]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: By passing the path to the model, we can test the chatbot for various responses.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递模型的路径，我们可以测试聊天机器人以获取各种回应。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Chatbots are taking the world by storm, and are predicted to become more prevalent
    in the coming years. The coherence of the results obtained from dialogues with
    these chatbots has to constantly improve if they are to gain widespread acceptance.
    One way to achieve this would be via the use of reinforcement learning.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人正在迅速席卷全球，预计在未来几年将变得更加普及。如果要获得广泛的接受，这些聊天机器人通过对话得到的结果的连贯性必须不断提高。实现这一目标的一种方式是通过使用强化学习。
- en: In this chapter, we implemented reinforcement learning in the creation of a chatbot.
    The learning was based on a policy gradient method that focused on the future
    direction of a dialogue agent, in order to generate coherent and interesting interactions.
    The datasets that we used were from movie conversations. We proceeded to clean
    and preprocess the datasets, obtaining the vocabulary from them. We then formulated
    our policy gradient method. Our reward functions were represented by a sequence
    to sequence model. We then trained and tested our data and obtained very reasonable
    results, proving the viability of using reinforcement learning for dialogue agents.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们实现了在创建聊天机器人过程中使用强化学习。该学习方法基于一种政策梯度方法，重点关注对话代理的未来方向，以生成连贯且有趣的互动。我们使用的数据集来自电影对话。我们对数据集进行了清理和预处理，从中获取了词汇表。然后，我们制定了我们的政策梯度方法。我们的奖励函数通过一个序列到序列模型表示。接着，我们训练并测试了我们的数据，获得了非常合理的结果，证明了使用强化学习进行对话代理的可行性。
