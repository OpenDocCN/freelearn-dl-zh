- en: Advanced Feature Engineering and NLP Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级特征工程与NLP算法
- en: In this chapter, we will look at an amazing and simple concept called **word
    to vector** (**word2vec**). This concept was developed by a team of researchers
    led by Tomas Mikolov at Google. As we all know, Google provides us with a lot
    of great products and concepts. Word2vec is one of them. In NLP, developing tools
    or techniques that can deal with the semantics of words, phrases, sentences, and
    so on are quite a big deal, and the word2vec model does a great job of figuring
    out the semantics of words, phrases, sentences, paragraphs, and documents. We
    are going to jump into this vectorization world and live our life in it for a
    while. Don't you think this is quite amazing? We will be starting from the concepts
    and we will end with some fun and practical examples. So, let's begin.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一个既简单又令人惊叹的概念，叫做**词到向量**（**word2vec**）。这个概念是由Google的Tomas Mikolov领导的研究团队开发的。众所周知，Google为我们提供了许多伟大的产品和概念，而word2vec就是其中之一。在自然语言处理（NLP）中，开发能够处理词语、短语、句子等语义的工具或技术是一项重要任务，而word2vec模型在理解词语、短语、句子、段落和文档的语义方面做得非常出色。我们将深入这个向量化的世界，并在其中待上一段时间。你不觉得这非常神奇吗？我们将从概念开始，最终通过一些有趣且实用的例子来结束。所以，让我们开始吧。
- en: Recall word embedding
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾词嵌入
- en: We have already covered word embedding in [Chapter 5](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml),
    *Feature Engineering and NLP Algorithms*. We have looked at language models and
    feature engineering techniques in NLP, where words or phrases from the vocabulary
    are mapped to vectors of real numbers. The techniques used to convert words into
    real numbers are called **word embedding**. We have been using vectorization,
    as well as **term frequency-inverse document frequency** (**tf-idf**) based vectorization.
    So, let's just jump into the world of word2vec.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第五章](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml)中讨论了词嵌入，*特征工程与NLP算法*。我们已经探讨了NLP中的语言模型和特征工程技术，在这些模型中，词语或短语被映射到实数向量。将词语转换为实数的技术被称为**词嵌入**。我们一直在使用向量化技术，以及基于**词频-逆文档频率**（**tf-idf**）的向量化技术。那么，让我们直接跳入word2vec的世界吧。
- en: Understanding the basics of word2vec
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解word2vec的基础
- en: 'Here, we will try to handle semantics at word level by using word2vec. Then,
    we will expand our concepts to paragraph level and document level. By looking
    at *Figure 6.1*, you will see the different kinds of semantics that we are going
    to cover in this book:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将尝试通过使用word2vec来处理词级别的语义。接着，我们将扩展我们的概念到段落级和文档级。通过查看*图6.1*，你将看到我们在本书中将要涵盖的不同种类的语义：
- en: '![](img/87ce8e19-40b7-4644-ba45-0293e84d7887.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](img/87ce8e19-40b7-4644-ba45-0293e84d7887.png)'
- en: 'Figure 6.1: Different kinds of semantics'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：不同种类的语义
- en: Semantics is a branch that deals with meaning in the area of NLP. We have already
    covered lexical semantic in [Chapter 3](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml),
    *Understanding Structure of Sentences*. So, here we will discuss more about distributional
    semantics. There are also other techniques or types in semantics, such as formal
    semantics compositional semantics; but right now, in this book, we are not going
    to cover these types or techniques.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 语义学是处理NLP领域中意义的一门分支学科。我们已经在[第三章](f65e61fc-1d20-434f-b606-f36cf401fc41.xhtml)中讨论了词汇语义学，*理解句子结构*。在这里，我们将更深入地讨论分布语义学。语义学中还有其他技术或类型，比如形式语义学和组合语义学；但是在本书中，我们现在不会涵盖这些类型或技术。
- en: Distributional semantics
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布语义学
- en: Distributional semantics is a research area that focuses on developing techniques
    or theories that quantify and categorize semantic similarities between linguistic
    items based on their distributional properties in large samples of text data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 分布语义学是一个研究领域，专注于开发量化和分类基于大型文本数据样本中分布特性的语言项目之间语义相似性的技术或理论。
- en: I want to give an example here that gives you an idea of what I mean by distributional
    semantics. Suppose you have text data of travelling blogs. Now, you as a person
    know that pasta, noodles, burgers, and so on are edible food items, whereas juice,
    tea, coffee, and so on are drinkable items. As a human, we can easily classify
    drinkable and edible food items because we have a certain context related with
    each of them, but machines cannot really know these kind of semantics. There is
    a higher chance that all described food items come along with certain words in
    the dataset. So, here we are focusing on the distribution of words in corpus and,
    let's say, that linguistic items or words with similar distributions have similar
    meanings. This is called the **distributional hypothesis**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我想给你一个例子，让你了解我所说的分布式语义是什么意思。假设你有关于旅行博客的文本数据。现在，作为一个人，你知道意大利面、面条、汉堡等是可食用的食品，而果汁、茶、咖啡等是饮料。作为人类，我们可以轻松地将可饮用和可食用的食物分类，因为我们与每个词都有一定的语境联系，但机器并不能真正理解这些语义。所有描述食物的项目很可能会在数据集中与某些特定词汇一起出现。因此，这里我们关注的是语料库中词汇的分布，假设具有相似分布的语言项目或词汇具有相似的含义。这被称为**分布假设**。
- en: 'I will give you another example. Suppose you have a dataset of research papers.
    Some of the research papers in the dataset belong to the engineering category,
    and others belong to the legal category. Documents with words such as engineering,
    equation, methods, and so on are related to engineering, so they should be part
    of one group, and words such as legal, lawyer, law institutes, and so on are related
    to research papers of the legal domain, so they should be grouped together. By
    using distributional semantics techniques such as word2vec, we can segregate the
    different domain words by using their vector values. All words with a similar
    meaning are grouped together because they have a similar distribution on the corpus.
    You can refer to *Figure 6.2*, which shows a pictorial representation of a vector
    space of our given distributional semantics example where similar contextual words
    come together:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我再给你一个例子。假设你有一个研究论文的数据集。数据集中的一些研究论文属于工程类，另一些属于法律类。包含“工程”、“方程式”、“方法”等词汇的文档与工程相关，因此它们应归为一组；而包含“法律”、“律师”、“法学院”等词汇的文档与法律领域的研究论文相关，因此它们应归为另一组。通过使用分布式语义技术，如word2vec，我们可以通过使用它们的向量值来区分不同领域的词汇。所有具有相似含义的词汇会被归为一组，因为它们在语料库中的分布相似。你可以参考*图6.2*，它展示了我们给定的分布式语义示例的向量空间图示，类似语境的词汇聚集在一起：
- en: '![](img/4f4f3862-fed5-4b60-b815-ca1d46516406.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f4f3862-fed5-4b60-b815-ca1d46516406.png)'
- en: 'Figure 6.2: Pictorial representation of vector space of our distributional
    semantics example'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：我们分布式语义示例的向量空间图示
- en: '*Figure 6.3*, gives you an idea about from which branch the word2vec model
    was derived:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.3*，让你了解word2vec模型是从哪个分支派生出来的：'
- en: '![](img/2dc79cdc-1778-4734-8495-8d70f8f7487b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2dc79cdc-1778-4734-8495-8d70f8f7487b.png)'
- en: 'Figure 6.3: Major models derived from distributional semantics'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：从分布式语义中衍生出的主要模型
- en: Our main concentration in this chapter is the distributional semantics technique
    called **word2vec**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是分布式语义技术，称为**word2vec**。
- en: Defining word2vec
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义word2vec
- en: Word2vec is developed by using two-layer neural networks. It takes a large amount
    of text data or text corpus as input and generates a set of vectors from the given
    text.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec是通过使用双层神经网络开发的。它以大量文本数据或文本语料库为输入，并从给定的文本中生成一组向量。
- en: In other words, we can say that it generates high-dimensional vector space.
    This vector space has several hundred dimensions.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以说它生成了一个高维度的向量空间。这个向量空间有数百个维度。
- en: Please don't be afraid of the high dimensionality. I make the whole concept
    of word2vec simple for you during this chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请不要害怕高维度。在本章中，我会简化word2vec的整个概念。
- en: You will really want to know what I mean here when I say that the word2vec model
    generates a set of vectors or vector space from text. Here, we are using a two-layer
    neural network, which right now a black box that performs some kind of logic and
    generates vectors in the vector space for us. In the vector space, each unique
    word in the corpus is assigned a corresponding vector. So, the vector space is
    just a vector representation of all words present in the large text corpus.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当我说word2vec模型从文本中生成一组向量或向量空间时，你真的会明白我的意思。这里，我们使用的是一个两层神经网络，当前它是一个黑箱，执行某种逻辑并为我们生成向量。在这个向量空间中，语料库中每个唯一的单词都会被分配一个对应的向量。所以，向量空间只是所有词汇在大规模文本语料库中出现的向量表示。
- en: So, I think you get it, right? On the basics of what we have learnt, you are
    able to say that word2vec is one of the models that generates word embedding.
    Please recall the vectorization section of [Chapter 5](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml),
    *Feature Engineering and NLP Algorithms*. I also want to make a point here, by
    saying that word2vec is a powerful, unsupervised word embedding technique.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我想你明白了，对吧？基于我们所学的基础，你可以说word2vec是生成词嵌入的模型之一。请回忆一下[第5章](07f71ca1-6c8a-492d-beb3-a47996e93f04.xhtml)中关于向量化的部分，*特征工程与NLP算法*。我还想在这里强调一点，word2vec是一种强大的、无监督的词嵌入技术。
- en: Necessity of unsupervised distribution semantic model - word2vec
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督分布式语义模型的必要性——word2vec
- en: This section gives us an idea of the numerous challenges that word2vec solves
    for us. The solution of those challenges leads us to the real need for word2vec.
    So, first we will look at some challenges, and then take a look at how the word2vec
    model solves those challenges.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容让我们对word2vec为我们解决的众多挑战有了一个初步了解。这些挑战的解决方案引导我们发现了word2vec的真正需求。所以，我们首先来看一些挑战，然后再看看word2vec模型是如何解决这些挑战的。
- en: Challenges
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: 'There are a couple of challenges that are listed here that we are trying to
    solve:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了一些我们正在尝试解决的挑战：
- en: When we are developing an NLP application, there is one fundamental problem--we
    know that the machine can't understand our text and we need to convert the text
    data into a numerical format.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们开发NLP应用时，有一个基本问题——我们知道机器无法理解我们的文本，因此需要将文本数据转换为数值格式。
- en: 'There are certain ways to convert text data into a numerical format, but we
    apply some naive techniques, and one of them is one-hot encoding, but the problems
    with this techniques are given as follows:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有几种方法可以将文本数据转换为数值格式，但我们采用了一些简单的技术，其中之一就是one-hot编码，但这种技术存在以下问题：
- en: 'Suppose you have a sentence: I like apple juice. Now, suppose that you apply
    one-hot encoding for each of the words in the sentence. If you have thousands
    of sentences in your corpus, then vector dimension is equal to the entire vocabulary
    of your corpus, and if these kind of high dimensional columns have been used to
    develop an NLP application, then we need high computation power and matrix operation
    on these high-dimension columns as they take too much time.'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设你有一句话：“I like apple juice”。现在，假设你对句子中的每个单词应用one-hot编码。如果你的语料库中有成千上万的句子，那么向量的维度就等于语料库的整个词汇表，如果这些高维列被用来开发NLP应用，那我们就需要强大的计算能力，并且需要对这些高维列进行矩阵运算，因为它们会消耗大量时间。
- en: For speech recognition vocabulary, size is on average 20,000 words. If we are
    developing a machine translation system then perhaps we will use more vocabulary,
    like 500,000 words. To deal with these kinds of gigantic vectors is a big challenge.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于语音识别词汇表，词汇量的平均大小是20,000个单词。如果我们正在开发一个机器翻译系统，那么可能会使用更多的词汇，比如50万个单词。处理这些庞大的向量是一个巨大的挑战。
- en: 'Another problem is that when you apply one-hot encoding on a particular word
    in a sentence, then the whole entry has zero values, except the one that actually
    represents the word, and that value is **1**. Suppose, for simplicity, we take
    the sentence: *I like apple juice*. For a while consider that there is only one
    sentence in our corpus. Now, if I try to apply one hot encoding on the word **apple**
    then one-hot representation of **apple**, is given as follows. Refer to *Figure
    6.4*:![](img/a8438127-5658-46e1-880e-eb3b7094488d.png)'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个问题是，当你对句子中的某个单词应用one-hot编码时，整个条目都会是零，除了实际表示该单词的那个值，而该值是**1**。为了简便起见，我们暂时考虑语句：“*I
    like apple juice*”。假设我们的语料库中只有一句话。那么，如果我试图对单词**apple**应用one-hot编码，那么**apple**的one-hot表示如下。请参考*图6.4*：![](img/a8438127-5658-46e1-880e-eb3b7094488d.png)
- en: Figure 6.4:One-hot encoding representation of the words apple and juice
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：苹果和果汁的单热编码表示
- en: One-hot encoding doesn't reveal the facts about context similarity between words.
    To understand this, I want to give an example, if your corpus has words cat and
    cats then one-hot encoding does not reveal the fact that word cat and cats are
    very similar words.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单热编码并不揭示词之间上下文相似性的事实。为了理解这一点，我想举个例子，如果你的语料库中有单词 cat 和 cats，则单热编码并不揭示单词 cat 和
    cats 是非常相似的词。
- en: If I apply an AND operation on the one-hot encoded vectors, then it will not
    express any contextual similarity. Take an example, if I apply an AND operation
    means a dot product on the one-hot vectors of **Apple** and **juice**,then the
    answer is **0**. In reality, these words can appear together and have a strong
    contextual relationship as well, but one-hot encoding alone does not express anything
    significant about word similarity.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我对苹果和果汁的单热编码向量进行 AND 运算，那么它将不会表达任何上下文相似性。举个例子，如果我对苹果和果汁的单热编码向量进行 AND 运算，答案是
    0。事实上，这些词可以一起出现，并且具有很强的上下文关系，但单热编码本身并不能表达有关词相似性的任何重要信息。
- en: If you want to find accurate word similarities, then WordNet will not help you
    enough. WordNet is made by experts and whatever WordNet contains is more subjective
    because human users created it.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想找到准确的词相似性，那么 WordNet 将不能为你提供足够的帮助。WordNet 是由专家制作的，而 WordNet 包含的内容更主观，因为是人类用户创建的。
- en: Using WordNet takes a lot of time and effort.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 WordNet 需要大量的时间和精力。
- en: Some new words, such as Coding Ninja, Wizard, and so on are new words for WordNet
    and may not be present on the website. Because of the absence of these kinds of
    words, we cannot derive the other semantic relationships from WordNet.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些新词，如 Coding Ninja、Wizard 等，是 WordNet 的新词，可能不在网站上。由于缺少这些类型的词，我们无法从 WordNet
    推导出其他语义关系。
- en: Each of the preceding challenges has played a major role in the development
    of the techniques to solve them. In the last two decades, there has been a lot
    of effort put into developing an efficient, concise, and relevant numerical representation
    of words. Finally, in 2013, Tomas Mikolov and his research team at Google came
    up with the word2vec model, which solves many of the previous challenges in an
    efficient way.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决这些挑战之前的每一个挑战都在这些技术的发展中起了重要作用。在过去的二十年中，已经有很多努力致力于开发一种高效、简洁和相关的词的数值表示。最终，在2013年，Google
    的托马斯·米科洛夫和他的研究团队提出了 word2vec 模型，这种模型以高效的方式解决了许多之前的挑战。
- en: Word2vec is very good at finding out word similarity, as well as preserving
    the semantic relationship between words that couldn't be handled by previous techniques,
    such as one-hot encoding or by using WordNet.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 在发现词的相似性以及保留语义关系方面非常出色，这些是以前的技术（如单热编码或使用 WordNet）无法处理的。
- en: I have given so much background on word2vec now, so let's start understanding
    the representation, components, and other parameters of the word2vec model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经在 word2vec 上提供了很多背景，现在让我们开始理解 word2vec 模型的表示、组件和其他参数。
- en: Let's begin our magical journey!
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始我们神奇的旅程吧！
- en: Converting the word2vec model from black box to white box
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 word2vec 模型从黑箱转换为白箱
- en: From this section onwards, we are going to get an understanding of each of the
    components of the word2vec model, as well as the model's working process. So,
    in short, we are converting the black box part of word2vec into a white box.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一节开始，我们将了解 word2vec 模型的每个组成部分，以及模型的工作过程。简而言之，我们正在将 word2vec 的黑箱部分转化为白箱。
- en: 'We will focus on the following procedures in order to understand the word2vec
    model:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点关注以下程序，以了解 word2vec 模型：
- en: Distributional similarity based representation
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于分布相似性的表示
- en: Understanding the components of the word2vec model
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 word2vec 模型的组成部分
- en: Understanding the logic of the word2vec model
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 word2vec 模型的逻辑
- en: Understanding the algorithms and math behind the word2vec model
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 word2vec 模型背后的算法和数学
- en: Some of the facts regarding the word2vec model
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于 word2vec 模型的一些事实
- en: Application of the word2vec model
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用 word2vec 模型
- en: Let's begin!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Distributional similarity based representation
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于分布相似性的表示
- en: 'This is quite an old and powerful idea in NLP. The notion of distributional
    similarity is that you can get a lot of value for representing the meaning of
    a word by considering the context in which that particular word appears, and it
    is highly related with that context. There is a very famous quote by a famous
    linguist John Firth:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是自然语言处理领域中一个非常古老且强大的观点。分布相似性的概念是，通过考虑某个特定单词出现的上下文，你可以为该单词的意义提供很多价值，而且它与上下文密切相关。著名语言学家约翰·费尔斯曾有一句非常著名的名言：
- en: '"You shall know the word by the company it keeps."'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: “你可以通过它所处的环境来了解一个词。”
- en: 'Let''s take an example: if I want to find the meaning of the word banking,
    I am going to collect thousands of sentences in which the word banking is included,
    and then I will start looking at the other words with the word banking and try
    to understand the context in which the word banking is used. So, look at these
    examples and understand the distributional similarity:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子：如果我想要了解“银行业”一词的含义，我将收集成千上万的包含“银行业”一词的句子，然后我会开始查看与“银行业”一词一起出现的其他词汇，并尝试理解“银行业”一词所处的上下文。看一下这些例子，并理解分布相似性：
- en: 'Sentence 1: The banking sector is regulated by the government'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子 1：银行业由政府进行监管。
- en: 'Sentence 2: Banking institutions need some technology to change their traditional
    operations.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句子 2：银行机构需要一些技术来改变其传统的运营模式。
- en: In the previous sentences, the word banking is included more frequently with
    words such as government, department, operations, and so on. All these words are
    very useful to understand the context and meaning of the word banking.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的句子中，"银行业"这个词与政府、部门、运营等词语频繁出现。这些词语对理解“银行业”一词的背景和意义非常有帮助。
- en: These other words are really helpful to represent the meaning of the word banking.
    You can also use the word banking to predict the most common frequently occurring
    words or phrases when the word banking is present in a sentence.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些其他词语确实有助于表征“银行业”一词的含义。你还可以利用“银行业”一词来预测该词出现时，最常见和频繁出现的词或短语。
- en: To understand how we can better represent the meaning of a particular word,
    as well as performing predictions about other words appearing in the context of
    this given word, we need to understand the distributional representation of the
    word in question.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地表示一个特定单词的含义，并对该单词上下文中出现的其他单词进行预测，我们需要理解该单词的分布式表示。
- en: The distributional representation of a word is a vector form in which the word
    can be represented. Words are expressed in the form of a dense vector, and the
    dense vector has to be chosen so that it will be good at predicting other words
    that appear in the context of this word. Now, each of those other words that we
    are making predictions about also have other words attached to them, so we use
    a similarity measure, such as the vector dot product. This is a kind of recursive
    approach, where each word will predict the other words that can appear in the
    same context, and other predicted words also perform the same operation by predicting
    some other words. So, we need a clever algorithm to perform this kind of recursive
    operation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 单词的分布式表示是一种向量形式，其中该单词可以被表示出来。单词以稠密向量的形式表达，而这个稠密向量必须被选择得足够好，以便它能够有效预测该单词上下文中出现的其他词语。现在，我们要对每个需要预测的其他词语也进行类似的操作，这些词语也会附带其他词语。因此，我们使用相似性度量方法，比如向量点积。这是一种递归的方法，每个单词会预测出能够出现在相同上下文中的其他单词，而其他被预测的单词也会通过预测其他单词来执行同样的操作。所以，我们需要一个巧妙的算法来执行这种递归操作。
- en: Here, please do not get confused between the terminology of distributional similarity
    and distribution representation. Distributional similarity based representation
    is actually used as part of the theory of semantics, which helps us to understand
    the meaning of the word in regular life usage; whereas the distributional representation
    of a word is the representation of a word in a vector form. To generate the vector
    form of a word, we can use one hot encoding or any other techniques, but the major
    point here is to generate the vector for a word that also carries the significance
    of similarity measure so that you can understand the contextual meaning of the
    word. Word2vec comes into the picture when we talk about distributional similarity.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，请不要混淆分布相似性和分布表示术语。基于分布相似性的表示术语实际上是语义理论的一部分，它帮助我们理解单词在日常使用中的含义；而单词的分布表示是单词以向量形式的表示。要生成单词的向量形式，我们可以使用独热编码或其他任何技术，但这里的主要点是生成一个单词向量，它还带有相似性测量的意义，以便你可以理解单词的上下文含义。当我们谈论分布相似性时，word2vec就会出现在画面中。
- en: Understanding the components of the word2vec model
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解word2vec模型的组成部分
- en: 'In this section, we will get an understanding of the main three components
    of the word2vec model, which are given as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将了解word2vec模型的主要三个组成部分，如下所示：
- en: Input of word2vec
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec的输入
- en: Output of word2vec
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec的输出
- en: Construction components of the word2vec model
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec模型的构造组件
- en: Input of the word2vec
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec的输入
- en: First of all, we should be aware of our input for developing the word2vec model,
    because that is a fundamental thing, from which you can start building word2vec.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该意识到我们为开发word2vec模型而输入的内容，因为这是一个基础事物，你可以从中开始构建word2vec。
- en: So, I want to state that we will use a raw text corpus as an input for developing
    the word2vec model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我想声明我们将使用原始文本语料库作为开发word2vec模型的输入。
- en: In real-life applications, we use large corpora as input. For simplicity, we
    will use a fairly small corpus to understand the concepts in this chapter. In
    later parts of this chapter, we will use a big corpus to develop some cool stuff
    by using word2vec model concepts.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，我们使用大型语料库作为输入。为简单起见，在本章中我们将使用一个相当小的语料库来理解概念。在本章的后面部分，我们将使用一个大语料库来开发一些很酷的东西，通过使用word2vec模型的概念。
- en: Output of word2vec
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec的输出
- en: This section is very important for your understanding because, after this point,
    whatever you understand will be just to achieve the output that you have set here.
    So, so far, we know that we want to develop the vector representation of a word
    that carries the meaning of the word, as well as to express the distribution similarity
    measure.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节对于你的理解非常重要，因为在此之后，你所理解的一切都将是为了实现你在这里设定的输出。到目前为止，我们知道我们想要开发一个单词的向量表示，它承载单词的含义，并表达分布相似性测量。
- en: 'Now, I will jump towards defining our goal and output. We want to define a
    model that aims to predict a central word and words that appear in its context.
    So, we can say that we want to predict the probability of the context given the
    word. Here, we are setting up the simple prediction objective. You can understand
    this goal by referring to the following figure:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我将跳转到定义我们的目标和输出。我们想要定义一个模型，旨在预测一个中心词及其上下文中出现的词。因此，我们可以说我们想要预测给定词的上下文的概率。在这里，我们正在设置简单的预测目标。您可以通过参考以下图来理解这个目标：
- en: '![](img/6a5a1a02-fa68-4fe9-8eee-24a11697799f.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a5a1a02-fa68-4fe9-8eee-24a11697799f.png)'
- en: 'Figure 6.5: Help to understand our goal'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：帮助理解我们的目标
- en: As you can see, there are some simple example sentences given in the preceding
    figure. If we take the word **apple** from the first sentence and, as per our
    goal, convert the word **apple** into a vector form such that, by using that vector
    form of **apple**, we can predict the probability of the word **eat** appearing
    in the context of the word **apple**. The same logic applies to the other sentences.
    For example, in the third sentence, where we try to find out the vector of the
    word **scooter**, which helps us to predict the probability of words such as **driving**
    and **work** in the context of the given word **scooter**.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，前面的图中给出了一些简单的例句。如果我们从第一句中取出单词**apple**，并根据我们的目标，将单词**apple**转换为向量形式，使得通过使用**apple**的向量形式，我们能够预测单词**eat**在单词**apple**上下文中出现的概率。相同的逻辑也适用于其他句子。例如，在第三个句子中，我们尝试找出单词**scooter**的向量，帮助我们预测像**driving**和**work**这样的单词在给定单词**scooter**的上下文中出现的概率。
- en: So, in general, our straightforward goal is that we need to convert every word
    into vector format, such that they are good at predicting the words that appear
    in their context, and by giving the context, we can predict the probability of
    the word that is best suited for the given context.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一般来说，我们的直接目标是将每个单词转换为向量形式，使得它们能够预测出在上下文中出现的单词，并且通过提供上下文，我们可以预测出最适合该上下文的单词的概率。
- en: Construction components of the word2vec model
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec模型的构建组件
- en: 'We know our input and output so far, so now you''re probably thinking: how
    can we achieve our goal by using our input. As I mentioned, we need a clever algorithm
    that will help us to achieve our goal. Researchers have done the work for us and
    concluded that we can use neural network techniques. I would like to give you
    just a brief idea of why we are going to use neural network, but if you want a
    deep insight of it, then I would encourage you to read some of these papers.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们知道了输入和输出，因此你现在可能在想：我们如何通过我们的输入来实现目标呢？正如我之前提到的，我们需要一个聪明的算法来帮助我们实现目标。研究人员已经为我们做了这项工作，并得出结论：我们可以使用神经网络技术。我想给你一个简短的介绍，解释为什么我们要使用神经网络，但如果你想深入了解它，我建议你阅读一些这些论文。
- en: '[http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html.](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html.](http://wwwold.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node12.html)'
- en: '[http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf](http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf)[.](http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf](http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf)[.](http://s3.amazonaws.com/academia.edu.documents/44666285/jhbs.pdf?AWSAccessKeyId=AKIAIWOWYYGZ2Y53UL3A&Expires=1497377031&Signature=CtXl5qOa4OpzF%2BAcergNU%2F6dUAU%3D&response-content-disposition=inline%3B%20filename%3DPhysio_logical_circuits_The_intellectua.pdf)'
- en: '[http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf.](http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf.](http://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)'
- en: 'The reason why we are using a neural network technique is because neural networks
    are good algorithms when we are trying to learn from a large amount of data. If
    you want to build a simple, scalable, and easy to train model then a neural network
    is one of the best approaches. If you read the modern research papers that I have
    listed as follows, they will tell you the same truth. Links to the papers mentioned
    are as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以使用神经网络技术，是因为神经网络在从大量数据中学习时表现得非常优秀。如果你想构建一个简单、可扩展且易于训练的模型，那么神经网络是最佳的方法之一。如果你阅读我接下来列出的现代研究论文，它们会告诉你同样的真相。下面是这些论文的链接：
- en: '[http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf.](http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf](http://web2.cs.columbia.edu/~blei/seminar/2016_discrete_data/readings/MikolovSutskeverChenCorradoDean2013.pdf)'
- en: '[https://arxiv.org/abs/1301.3781.](https://arxiv.org/abs/1301.3781)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1301.3781](https://arxiv.org/abs/1301.3781)'
- en: 'This kind of neural network creates magic in terms of generating distributional
    similarity. Google generated the word2vec model by using the large corpus of Wikipedia.
    Refer to *Figure 6.6*, which gives you an overview of the input, and some famous
    output from the Google word2vec model. For us, the word2vec model is still a powerful
    black box that generates some great results. See the black box representation
    of word2vec in the following figure:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这种神经网络在生成分布式相似性方面创造了奇迹。Google 通过使用大量 Wikipedia 语料库来生成 word2vec 模型。请参考*图 6.6*，它给你展示了输入概况，以及
    Google word2vec 模型的一些著名输出。对于我们来说，word2vec 模型仍然是一个强大的“黑盒”，能够生成一些很棒的结果。请看以下图中的 word2vec
    黑盒表示：
- en: '![](img/d942cb9e-cdf8-469d-b4c4-2080447ba467.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d942cb9e-cdf8-469d-b4c4-2080447ba467.png)'
- en: 'Figure 6.6: Google word2vec model takes Wikipedia text as input and generates
    output'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：Google word2vec 模型以 Wikipedia 文本作为输入并生成输出
- en: 'The preceding image shows that we have provided text data as input to the word2vec
    model. The word2vec model converts our text data to the vector form so that we
    can perform mathematical operations on this vector representation of words. The
    most famous example of word2vec is: if you have the vectors of king, man, and
    woman. Then, if you apply the mathematical operation subtracting the vector value
    of man from the king vector and add the vector value of the word woman to it,
    then we will get a resultant vector that represents the same vector value of the
    word queen. Here''s the mathematical representation of this example: *king - man
    + woman = queen*.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图像显示我们已经将文本数据作为输入提供给 word2vec 模型。word2vec 模型将我们的文本数据转换为向量形式，这样我们就可以对这些词汇的向量表示进行数学运算。word2vec
    最著名的例子是：如果你有 king、man 和 woman 这三个词的向量。那么，如果你对 king 向量减去 man 向量，再加上 woman 向量，你将得到一个表示
    queen 向量的结果。以下是这个例子的数学表示：*king - man + woman = queen*。
- en: Now we need to focus on the overview of the architectural component for word2vec.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要关注 word2vec 的架构组件概述。
- en: Architectural component
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构组件
- en: Let's look at the architectural components involved in building a word2vec model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看构建 word2vec 模型所涉及的架构组件。
- en: The major architectural component for the word2vec model is its neural network.
    The neural network for a word2vec model has two layers, and in that sense, it
    is not a deep neural network. The fact is that word2vec doesn't use deep neural
    networks to generate vector forms of words.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 模型的主要架构组件是其神经网络。word2vec 模型的神经网络有两层，从这个意义上来说，它并不是一个深度神经网络。事实上，word2vec
    并没有使用深度神经网络来生成词汇的向量表示。
- en: This is one of the critical and main components of the word2vec model and we
    need to decode its functionality to get a clear idea about how word2vec works.
    Now it's time to decode the magical logic of the word2vec model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 word2vec 模型中的一个关键组成部分，我们需要解码它的功能，才能清楚地了解 word2vec 是如何工作的。现在是解码 word2vec 模型神奇逻辑的时刻。
- en: Understanding the logic of the word2vec model
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 word2vec 模型的逻辑
- en: 'We will start decomposing the word2vec model and try to understand the logic
    of it. word2vec is a piece of software and it uses a bunch of algorithms. Refer
    to *Figure 6.7*:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始分解 word2vec 模型，并尝试理解其逻辑。word2vec 是一款软件，使用了一堆算法。请参考*图 6.7*：
- en: '![](img/d49e251d-2ab4-4663-b447-9434f97416b2.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d49e251d-2ab4-4663-b447-9434f97416b2.png)'
- en: 'Figure 6.7: Word2vec building block (Image credit: Xin Rong)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7：Word2vec 构建模块（图片来源：Xin Rong）
- en: 'As you can see in *Figure 6.7*, there are three main building blocks. We will
    examine each of them in detail:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 6.7*所示，主要有三个构建模块。我们将详细查看每一个模块：
- en: Vocabulary builder
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词汇构建器
- en: Context builder
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文构建器
- en: Neural network with two layers
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两层神经网络
- en: Vocabulary builder
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词汇构建器
- en: The vocabulary builder is the first building block of the word2vec model. It
    takes raw text data, mostly in the form of sentences. The vocabulary builder is
    used to build vocabulary from your given text corpus. It will collect all the
    unique words from your corpus and build the vocabulary.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇构建器是 word2vec 模型的第一个构建块。它接受原始文本数据，通常以句子的形式。词汇构建器用于从给定的文本语料库中构建词汇表。它会收集语料库中的所有唯一单词并构建词汇表。
- en: 'In Python, there is a library called `gensim`. We will use `gensim` to generate
    word2vec for our corpus. There are some parameters available in `gensim` that
    we can use to build vocabulary from our corpus as per your application needs.
    The parameter list is given as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，有一个名为`gensim`的库。我们将使用`gensim`为我们的语料库生成 word2vec。在`gensim`中有一些参数可以根据你的应用需求，用来从语料库中构建词汇表。以下是参数列表：
- en: '`min_count`: This parameter is used as a threshold value. This ignores all
    words with a total frequency of lower than the specified value. So, for example,
    if you set `min_count = 5`, then the output of the vocabulary builder doesn''t
    contain words that occur less than five times. The vocabulary builder output contains
    only words that appeared in the corpus more than or equal to five times.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_count`：该参数用作阈值。它会忽略所有总频率低于指定值的单词。例如，如果你设置`min_count=5`，则词汇构建器的输出不会包含出现次数少于五次的单词。词汇构建器的输出只包含在语料库中出现五次或更多的单词。'
- en: '`build_vocab(sentences`, `keep_raw_vocab=False`, `trim_rule=None`, `progress_per=10000`,
    `update=False)`: This syntax is used to build vocabulary from a sequence of sentences
    (can be a once-only generator stream). Each sentence must be a list of unicode
    strings.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`build_vocab(sentences, keep_raw_vocab=False, trim_rule=None, progress_per=10000,
    update=False)`：此语法用于从一系列句子（可以是一次性生成的流）中构建词汇表。每个句子必须是一个 Unicode 字符串的列表。'
- en: 'There are other parameters that you can read about by clicking on this link:
    [https://radimrehurek.com/gensim/models/Word2vec.html.](https://radimrehurek.com/gensim/models/Word2vec.html)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以点击这个链接，阅读更多关于其他参数的信息：[https://radimrehurek.com/gensim/models/Word2vec.html.](https://radimrehurek.com/gensim/models/Word2vec.html)
- en: 'Each word present in the vocabulary has an association with the vocabulary
    object, which contains an index and a count. That is the output of the vocabulary
    builder. So you can refer to *Figure 6.8*, which helps you to understand the input
    and output of the vocabulary builder:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表中的每个单词都与词汇对象相关联，该对象包含一个索引和一个计数。这是词汇构建器的输出。因此，你可以参考*图 6.8*，它帮助你理解词汇构建器的输入和输出：
- en: '![](img/6d986b99-ea28-413e-9356-a7533859046f.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d986b99-ea28-413e-9356-a7533859046f.png)'
- en: 'Figure 6.8: Input and output flow of vocabulary builder'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8：词汇构建器的输入输出流程
- en: Context builder
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上下文构建器
- en: The context builder uses output of the vocabulary builder, as well as words
    that are part of the context window, as input and generates the output.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文构建器使用词汇构建器的输出，以及作为上下文窗口一部分的单词，作为输入并生成输出。
- en: First of all, let's understand the concept of a context window. This context
    window is kind of a sliding window. You can define the window size as per the
    NLP application in which you will use word2vec. Generally, NLP applications use
    the context window size of five to ten words. If you decide to go with a window
    size of five, then we need to consider the five words on the left side from the
    center word and the five words on the right side of the center word. In this way,
    we capture the information about what all the surrounding words are for our center
    word.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们理解上下文窗口的概念。这个上下文窗口类似于一个滑动窗口。你可以根据将要使用 word2vec 的自然语言处理应用程序定义窗口的大小。通常，NLP
    应用程序使用大小为五到十个单词的上下文窗口。如果你选择五个单词作为窗口大小，那么我们需要考虑中心单词左边的五个单词和右边的五个单词。通过这种方式，我们捕获了中心单词的所有周围单词的信息。
- en: 'Here, I want to state an example, and for this, the context window''s size
    is equal to one, as we have a one-sentence corpus. I have the sentence: **I like
    deep learning,** and **deep** is the center word. So then, you should consider
    the surrounding words as per our window size. So here, I need to consider the
    words **like** and **learning**. In the next iteration our center word will be
    **learning,** its surrounding words are **deep**, and at the end of the sentence
    is a **period** (**.**).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我想举一个例子，假设上下文窗口的大小为1，因为我们有一个单句语料库。我有一句话：**I like deep learning,** 其中**deep**是中心词。那么接下来，你应该根据窗口大小考虑周围的词。因此，在这里，我需要考虑**like**和**learning**这两个词。在下一次迭代中，我们的中心词将是**learning**，它的周围词是**deep**，句子的结尾是一个**句点**（**.**）。
- en: I hope the context window concept is clear in your head. Now, we need to link
    this concept and see how the context builder uses this concept and the output
    of the vocabulary builder. The vocabulary builder object has word indexes and
    the frequency of the word in the corpus. By using the index of the word, the context
    builder has an idea of which word we are looking at and, according to the context
    window size, it considers the other surrounding words. These center words and
    the other surrounding words are input to the context builder.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你已经清晰理解了上下文窗口的概念。现在，我们需要将这个概念与上下文构建器如何使用这个概念以及词汇构建器的输出联系起来。词汇构建器对象包含单词的索引以及该单词在语料库中的频率。通过使用单词的索引，上下文构建器就能知道我们在查看哪个单词，并且根据上下文窗口的大小，它会考虑周围的其他单词。这些中心词和其他周围词将作为输入传递给上下文构建器。
- en: 'Now you have a clear idea about what the inputs to the context builder are.
    Let''s try to understand the output of the context builder. This context builder
    generates the word pairing. Refer to *Figure 6.9*, to get an idea about word paring:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对上下文构建器的输入有了清晰的了解。让我们试着理解上下文构建器的输出。这个上下文构建器生成词对。参考*图6.9*，了解词对的概念：
- en: '![](img/e49e6498-3f88-492d-b13d-cae42fbe2197.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e49e6498-3f88-492d-b13d-cae42fbe2197.png)'
- en: 'Figure 6.9: Understanding word paring'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9：理解词对
- en: These word pairings will be given to the neural network. The network is going
    to learn the basic statistics from the number of times each word pairing shows
    up. So, for example, the neural network is probably going to get many more training
    examples of (deep, learning) than it is of (deep, communication). When the training
    is finished, if you give it the word **deep** as input, then it will output a
    much higher probability for learning or network than it will for communication.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词对将被提供给神经网络。网络将根据每个词对出现的次数来学习基本统计信息。例如，神经网络可能会看到更多（deep, learning）这个词对的训练示例，而不是（deep,
    communication）。当训练完成后，如果你将**deep**作为输入，它将为learning或network输出更高的概率，而不是为communication输出。
- en: 'So this word pair is the output of the context builder and it will pass to
    the next component, which is a two layer neural network. Refer to *Figure 6.10*,
    to see the summary of the flow of context builder:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个词对是上下文构建器的输出，它将传递给下一个组件，即一个两层的神经网络。参考*图6.10*，查看上下文构建器流程的概述：
- en: '![](img/2209d360-175f-4706-a845-f0399eb57340.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2209d360-175f-4706-a845-f0399eb57340.png)'
- en: 'Figure 6.10: Input and output flow of context builder'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10：上下文构建器的输入和输出流
- en: So far, we have seen two major components of the word2vec building blocks. Now,
    our next focus will be on the final component, which is the neural network.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了word2vec构建块的两个主要组件。现在，我们的下一个关注点将是最终组件——神经网络。
- en: Neural network with two layers
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两层神经网络
- en: In this section, we will look at the input and output of the neural network.
    Apart from that, we will also focus on the structural part of the neural network,
    which will give us an idea of how a single neuron looks, how many neurons there
    should be, what an activation function is, and so on. So, now, let's get started!
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究神经网络的输入和输出。除此之外，我们还将关注神经网络的结构部分，这将帮助我们了解单个神经元的样子，神经元的数量，激活函数是什么等等。那么，现在我们开始吧！
- en: Structural details of a word2vec neural network
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec神经网络的结构细节
- en: 'Word2vec uses the neural network for training. So, for us, it is very important
    to understand the basic structure of the neural network. The structural details
    of a neural network are given as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 使用神经网络进行训练。所以对我们来说，理解神经网络的基本结构非常重要。神经网络的结构细节如下所示：
- en: There is one input layer
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个输入层
- en: The second layer is the hidden layer
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层是隐藏层
- en: The third and final layer is the output layer
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层也是最后一层是输出层
- en: Word2vec neural network layer's details
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2vec 神经网络层的详细信息
- en: As we know, there are two layers in the neural network for generating word vectors.
    We will start to look at each of the layers and their input and output in detail.
    Here, we are not including the math behind the word2vec model in this section.
    Later in this chapter, we will also look at the math behind word2vec, and I will
    let you know at that point of time to map your dots for better interpretation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，神经网络中有两层用于生成词向量。我们将开始详细查看每一层及其输入和输出。在这一部分中，我们不涉及word2vec模型背后的数学。稍后在本章中，我们将讨论word2vec背后的数学，并在那时让你知道如何映射你的点以便更好地理解。
- en: 'Let''s understand the task of each layer in brief:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要了解每一层的任务：
- en: '**Input layer**: An input layer has as many neurons as there are words in the
    vocabulary for training'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：输入层的神经元数量与词汇表中的单词数量相同'
- en: '**Hidden layer**: The hidden layer size in terms of neurons is the dimensionality
    of the resulting word vectors'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：隐藏层的大小以神经元数表示，是最终词向量的维度'
- en: '**Output layer**: The output layer has the same number of neurons as the input
    layer'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：输出层的神经元数量与输入层相同'
- en: 'The input for the first input layer is the word with one-hot encoding. Assume
    that our vocabulary size for learning word vectors is **V**, which means there
    are **V** numbers of different words in the corpus. In that case, the position
    of the word that represents itself is encoded as **1** and all others positions
    are encoded as **0**. Refer to *Figure 6.4* again to recall the concept of one-hot
    encoding. Suppose the dimension of these words is **N**. So, the input to the
    hidden layer connections can be represented by our input matrix **WI** (input
    matrix symbol) of size *V * N* ,with each row of the matrix **WI** representing
    a vocabulary word. Similarly, the connections from the hidden layer to the output
    layer means the output from the hidden layer can be described by the hidden layer
    output matrix **WO** (hidden layer matrix symbol). The **WO** matrix is of size
    *N * V*. In this case, each column of the **WO** matrix represents a word from
    the given vocabulary. Refer to *Figure 6.11* to get a crystal clear picture of
    the input and output. As well as this, we will also look at one short example
    to understand the concept:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个输入层的输入是采用独热编码的单词。假设我们的词汇大小为**V**，这意味着语料库中有**V**个不同的单词。在这种情况下，表示单词本身的位置信息编码为**1**，所有其他位置编码为**0**。请再次参考*图
    6.4*以回顾独热编码的概念。假设这些单词的维度为**N**。因此，输入到隐藏层的连接可以由我们的输入矩阵**WI**（输入矩阵符号）表示，大小为*V *
    N*，其中矩阵**WI**的每一行代表一个词汇单词。同样，从隐藏层到输出层的连接意味着来自隐藏层的输出可以由隐藏层输出矩阵**WO**（隐藏层矩阵符号）描述。**WO**矩阵的大小为*N
    * V*。在这种情况下，**WO**矩阵的每一列代表给定词汇中的一个单词。请参考*图 6.11*以清晰地理解输入和输出。同时，我们还将通过一个简短的示例来理解这个概念：
- en: '![](img/3eb2989e-0e2b-4df7-b67c-c38e7e193d26.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3eb2989e-0e2b-4df7-b67c-c38e7e193d26.png)'
- en: 'Figure 6.11: two layer neural network input and output structural representation'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11：双层神经网络输入和输出结构表示
- en: 'Now let''s talk in terms of examples. I will take a very small set of the corpus.
    See the sentences from our small corpus given as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过例子来讨论。我将以一个非常小的语料库为例。请看下面给出的我们小语料库中的句子：
- en: the dog saw a cat
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: the dog saw a cat
- en: the dog chased a cat
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: the dog chased a cat
- en: the cat climbed a tree
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: the cat climbed a tree
- en: 'The preceding three sentences have eight (8) unique words. We need to order
    them in alphabetical order, and if we want to access them, then we will refer
    to the index of each word. Refer to the following table:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 上述三个句子中有八个（8）独特的单词。我们需要将它们按字母顺序排列，如果我们要访问它们，那么我们将参考每个单词的索引。请参考下面的表格：
- en: '| **Words** | **Index** |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| **Words** | **Index** |'
- en: '| a | 1 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| a | 1 |'
- en: '| cat | 2 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| cat | 2 |'
- en: '| chased | 3 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| chased | 3 |'
- en: '| climbed | 4 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| climbed | 4 |'
- en: '| dog | 5 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| dog | 5 |'
- en: '| saw | 6 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| saw | 6 |'
- en: '| the | 7 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| the | 7 |'
- en: '| tree | 8 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| tree | 8 |'
- en: 'So, here our value for **V** is equal to **8**. In our neural network, we need
    eight input neurons and eight output neurons. Now let''s assume we will have three
    (3) neurons in the hidden layer. So in this case, our **WI** and **WO** values
    are defined as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在这里我们**V**的值等于**8**。在我们的神经网络中，我们需要八个输入神经元和八个输出神经元。现在假设我们在隐藏层中有三个（3）神经元。那么，在这种情况下，我们的**WI**和**WO**的值定义如下：
- en: '*WI = [V * N] = [8 * 3]*'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*WI = [V * N] = [8 * 3]*'
- en: '*WO = [N * V] = [3 * 8]*'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*WO = [N * V] = [3 * 8]*'
- en: 'Before training begins, these matrices, **WI** and **WO** , are initialized
    by using small random values, as is very common in neural network training. Just
    for illustration purposes, let us assume that **WI** and **WO** are to be initialized
    to the following values:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始之前，这些矩阵，**WI** 和 **WO**，通过使用小的随机值进行初始化，这在神经网络训练中非常常见。仅为说明目的，我们假设 **WI**
    和 **WO** 初始化为以下值：
- en: '*WI =*'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*WI =*'
- en: '![](img/b40ddeaa-3a18-440c-9909-27d279c40262.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b40ddeaa-3a18-440c-9909-27d279c40262.png)'
- en: '*WO =*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*WO =*'
- en: '![](img/4de28604-a756-4429-b4da-c65dfdb7d747.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4de28604-a756-4429-b4da-c65dfdb7d747.png)'
- en: 'Image source: https://iksinc.wordpress.com'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '图片来源: https://iksinc.wordpress.com'
- en: We are targeting it so that our neural network can learn the relationship between
    the words **cat** and **climbed**. So, in other words, we can explain that the
    neural network should give high probability for the word **climbed** when the
    word **cat** is fed into the neural network as an input. So, in word embedding,
    the word **cat** is referred to as a context word and the word **climbed** is
    referred to as a target word.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是让神经网络学习单词 **cat** 和 **climbed** 之间的关系。换句话说，我们可以解释为当单词 **cat** 输入到神经网络时，神经网络应为单词
    **climbed** 提供较高的概率。因此，在词嵌入中，单词 **cat** 被称为上下文单词，而单词 **climbed** 被称为目标单词。
- en: The input vector *X* stands for the word **cat** and it will be *[0 1 0 0 0
    0 0 0]t*. Notice that only the second component of the vector is *1*. The reason
    behind this is that the input word **cat**, holds the second position in a sorted
    list of the corpus. In the same way, the target word is **climbed** and the target
    vector for **climbed** will look like *[0 0 0 1 0 0 0 0 ]t*.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量 *X* 表示单词 **cat**，它将是 *[0 1 0 0 0 0 0 0]t*。注意，向量的第二个分量是 *1*。之所以这样，是因为输入单词
    **cat** 在语料库的排序列表中排在第二位。以同样的方式，目标单词是 **climbed**，目标向量为 **climbed** 的表示将是 *[0 0
    0 1 0 0 0 0 ]t*。
- en: The input for the first layer is *[0 1 0 0 0 0 0 0]t*.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层的输入是 *[0 1 0 0 0 0 0 0]t*。
- en: 'The hidden layer output *Ht* is calculated by using the following formula:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层输出 *Ht* 通过以下公式计算：
- en: '*Ht = XtWI = [-0.490796 -0.229903 0.065460]*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ht = XtWI = [-0.490796 -0.229903 0.065460]*'
- en: 'From the preceding calculation, we can figure out that, here, the output of
    the hidden neurons mimics the weights of the second row of the **WI** matrix because
    of one-hot encoding representation. Now we need to check a similar calculation
    for the hidden layer and output layer. The calculation for the hidden layer and
    output layer is defined as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的计算中，我们可以看出，由于采用了独热编码表示，这里隐藏神经元的输出类似于 **WI** 矩阵第二行的权重。现在我们需要对隐藏层和输出层进行类似的计算。隐藏层和输出层的计算定义如下：
- en: '*HtWO = [0.100934 -0.309331 -0.122361 -0.151399 0.143463 -0.051262 -0.079686
    0.112928]*'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*HtWO = [0.100934 -0.309331 -0.122361 -0.151399 0.143463 -0.051262 -0.079686
    0.112928]*'
- en: 'Here, our final goal is to obtain probabilities for words in the output layer.
    From the output layer, we are generating probability that reflects the next word
    relationship with the context word at input. So, the mathematical representation
    is given as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的最终目标是获取输出层中各个单词的概率。通过输出层，我们生成反映下一个单词与输入上下文单词关系的概率。因此，数学表示如下：
- en: '*Probability (wordk|wordcontext) for k = 1...V*'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*概率 (wordk|wordcontext) 对于 k = 1...V*'
- en: Here, we are talking in terms of probability, but our output is in the form
    of a set of vectors, so we need to convert our output into probability. We need
    to take care that the sum of the neuron output from the final output layer should
    be added to one. In word2vec, we are converting activation values of the output
    layer neurons to probabilities by using the softmax function.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论的是概率，但我们的输出是一个向量集合，因此我们需要将输出转换为概率。我们需要确保最终输出层神经元的输出之和等于 1。在 word2vec
    中，我们通过使用 softmax 函数将输出层神经元的激活值转换为概率。
- en: Softmax function
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax 函数
- en: 'In this section, we will look at the softmax function. The softmax function
    is used for converting output vectors into a probability form. We are using the
    softmax function because we want to convert out last layer output in terms of
    probability and softmax function can easily convert vector values into probability
    values. Here, the output of the *k*^(th) neuron will be computed by the following
    equation, where activation(*n*) represents the activation value of the *n*th output
    layer neuron:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 softmax 函数。softmax 函数用于将输出向量转换为概率形式。我们使用 softmax 函数是因为我们希望将最后一层的输出转换为概率值，而
    softmax 函数可以轻松地将向量值转换为概率值。这里，第*k*层神经元的输出将通过以下公式计算，其中 activation(*n*) 表示第*n*层输出神经元的激活值：
- en: '![](img/1f6ffc37-d899-4db6-8063-b8116a611312.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f6ffc37-d899-4db6-8063-b8116a611312.png)'
- en: 'By using this equation, we can calculate the probabilities for eight words
    in the corpus and the probability values are given as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这个公式，我们可以计算语料库中八个单词的概率，概率值如下：
- en: '*[ 0.143073 0.094925 0.114441 0.111166 0.149289 0.122874 0.119431 0.144800
    ]*'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*[ 0.143073 0.094925 0.114441 0.111166 0.149289 0.122874 0.119431 0.144800
    ]*'
- en: 'You must be wondering how I got these probability values. I used the previous
    softmax probability equation and generated the final probability vector. You can
    find the Python code for the softmax function in the following code snippet:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你一定在想，我是怎么得到这些概率值的。我使用了前面的 softmax 概率公式并生成了最终的概率向量。你可以在下面的代码片段中找到计算 softmax
    函数的 Python 代码：
- en: '[PRE0]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The given code will generate the following output vector:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的代码将生成以下输出向量：
- en: '[PRE1]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, the probability *0.111166* is for the chosen target word climbed.
    As we know, the target vector is *[0 0 0 1 0 0 0 0]t*, so we can compute the error
    by prediction. To generate a prediction error or error vector, we need to subtract
    the probability vector from the target vector, and once we know the error vector
    or error values, we can adjust the weight according to that. Here, we need to
    adjust the weight values of the matrices *WI* and *WO*. The technique of propagating
    errors in the network and readjusting the weight values of *WI* and *WO* is called
    **backpropagation**.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，概率*0.111166*是为选定的目标词**climbed**计算的概率。我们知道，目标向量是 *[0 0 0 1 0 0 0 0]t*，因此我们可以通过预测来计算误差。为了生成预测误差或误差向量，我们需要从目标向量中减去概率向量，一旦我们知道了误差向量或误差值，就可以根据它调整权重。在这里，我们需要调整矩阵*WI*和*WO*的权重值。在网络中传播误差并重新调整*WI*和*WO*的权重值的技术被称为**反向传播**。
- en: Thus, the training can continue by taking different context-target word pairs
    from the corpus. This is the way word2vec learns relationships between words in
    order to develop a vector representation of words in the corpus.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练可以通过从语料库中提取不同的上下文-目标词对来继续进行。这就是 word2vec 学习单词之间关系的方式，以便为语料库中的单词开发向量表示。
- en: Main processing algorithms
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要处理算法
- en: 'Word2vec has two different versions. These versions are the main algorithms
    for word2vec. Refer to *Figure 6.12*:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 有两个不同的版本。这些版本是 word2vec 的主要算法。请参见*图 6.12*：
- en: '![](img/010476eb-893d-4ca5-b3cd-251cb29f3f2f.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/010476eb-893d-4ca5-b3cd-251cb29f3f2f.png)'
- en: 'Figure 6.12: Versions of word2vec'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12：word2vec 的版本
- en: 'In this section, we will look at the main two processing algorithms. Those
    algorithms names are given as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍主要的两种处理算法。这些算法的名称如下：
- en: Continuous bag of words
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续词袋模型
- en: Skip-gram
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skip-gram
- en: Continuous bag of words
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续词袋模型
- en: In the **continuous bag of words** (**CBOW**) algorithm, context is represented
    by multiple words for given target words. Just recall our example that we stated
    in an earlier section, where our context word was **cat** and our target word
    was **climbed.** For example, we can use **cat** as well as **tree** as context
    words to predict the word **climbed** as the target word. In this case, we need
    to change the architecture of the neural network, especially the input layer.
    Now, our input layer may not represent the single-word one-hot encode vector,
    but we need to put another input layer that represents the word **tree**.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在**连续词袋模型**（**CBOW**）算法中，给定目标词时，上下文是由多个词来表示的。回想一下我们在前面部分提到的例子，其中上下文词是**cat**，目标词是**climbed**。例如，我们可以使用**cat**和**tree**作为上下文词来预测目标词**climbed**。在这种情况下，我们需要改变神经网络的结构，特别是输入层。现在，我们的输入层可能不再是单一词的
    one-hot 编码向量，而需要增加另一个输入层来表示词**tree**。
- en: 'If you increase the context words, then you need to put an additional input
    layer to represent each of the words, and all these input layers are connected
    to the hidden layer. Refer to *Figure 6.13*:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你增加上下文词的数量，那么你需要添加一个额外的输入层来表示每个词，并且所有这些输入层都与隐藏层相连接。参考 *图 6.13*：
- en: '![](img/618b4974-58e4-4569-9ddd-bf8fd1588ad0.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/618b4974-58e4-4569-9ddd-bf8fd1588ad0.png)'
- en: 'Figure 6.13: CBOW neural network architecture (Image credit: https://www.semanticscholar.org)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13：CBOW 神经网络架构（图片来源： https://www.semanticscholar.org）
- en: Here, the good part is that the computation formula remains the same; we just
    need to compute the *Ht* for other context words as well.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，好的一点是计算公式保持不变；我们只需要计算其他上下文词的 *Ht*。
- en: Skip-gram
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跳字模型
- en: The **skip-gram** (**SG**) model reverses the usage of target words and context
    words. Here, the target word is given as input to the input layer in the form
    of a one-hot encoded vector. The hidden layer remains the same. The output layer
    of the neural network is repeated multiple times to generate the chosen number
    of context words.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**skip-gram**（**SG**）模型颠倒了目标词和上下文词的使用。在这里，目标词作为输入以 one-hot 编码向量的形式输入到输入层。隐藏层保持不变。神经网络的输出层重复多次以生成所选数量的上下文词。'
- en: 'Let''s take an example of the words **cat** and **tree** as context words and
    the word **climbed** as a target word. The input vector in the SG model will be
    the one-hot encoded word vector of the word **climbed** *[0 0 0 1 0 0 0 0 ]t*
    and this time, our output vectors should be vectors for the word **cat** and the
    word **tree**. So, the output vector should be *[ 0 1 0 0 0 0 0 0]* for **cat**
    and *[0 0 0 0 0 0 0 1]* for **tree**. Refer to the structure of the skip-gram
    algorithm in *Figure 6.14*:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以单词 **cat** 和 **tree** 作为上下文词，以单词 **climbed** 作为目标词为例。在 SG 模型中，输入向量将是单词 **climbed**
    的 one-hot 编码词向量 *[0 0 0 1 0 0 0 0 ]t*，而这次，我们的输出向量应该是单词 **cat** 和单词 **tree** 的词向量。因此，输出向量应该是
    **cat** 的 *[ 0 1 0 0 0 0 0 0]* 和 **tree** 的 *[0 0 0 0 0 0 0 1]*。参考 *图 6.14* 中的跳字模型结构：
- en: '![](img/61189b79-4a72-4e08-a4da-39260ed5896e.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61189b79-4a72-4e08-a4da-39260ed5896e.png)'
- en: 'Figure 6.14: Skip gram neural network architecture (Image credit: https://www.semanticscholar.org)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14：跳字模型神经网络架构（图片来源： https://www.semanticscholar.org）
- en: This time the output will not be a single vector of probability, but two different
    vectors of probability, as we have two words as the context word. Here, error
    vector will be calculated in the same manner as we defined earlier. The small
    change in skip-gram is that the error vectors from all the output layers are summed
    up to adjust the weights via backpropagation. This means we need to ensure that
    the weight of matrix *WO* for each output layer should remain identical through
    the training.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这次输出将不是一个单一的概率向量，而是两个不同的概率向量，因为我们有两个上下文词。在这里，误差向量将按照我们之前定义的方式计算。skip-gram 中的小变化是，所有输出层的误差向量会被加总，通过反向传播调整权重。这意味着我们需要确保每个输出层的矩阵
    *WO* 的权重在训练过程中保持一致。
- en: Some of the algorithmic techniques and math behind word2vec and other techniques
    will be explained in the next section. So, let's get ready to deal with some cool
    mathematical stuff!
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于 word2vec 和其他技术背后的算法技术和数学将在下一节中解释。所以，准备好处理一些很酷的数学内容吧！
- en: Understanding algorithmic techniques and the mathematics behind the word2vec
    model
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 word2vec 模型背后的算法技术和数学原理
- en: This section is very important, as we are going to discuss here the core algorithms
    that have been used in word2vec. By the end of this section, there won't be any
    secrets left in order for you to understand the concept of word2vec. Thus, this
    section is converting word2vec black box into word2vec white box. Here, I'm going
    to include the math part as well, so readers can understand the core concepts
    in a better manner. Don't worry if you don't know the math, because I will provide
    you with some resources that you may find really useful.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 本节非常重要，因为我们将在这里讨论在 word2vec 中使用的核心算法。到本节结束时，为了让你理解 word2vec 的概念，所有的秘密都将揭开。因此，本节将把
    word2vec 的黑箱变成白箱。在这里，我还将包括数学部分，以便读者能够更好地理解核心概念。即使你不懂数学也不用担心，因为我将提供一些可能对你非常有用的资源。
- en: Understanding the basic mathematics for the word2vec algorithm
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 word2vec 算法的基本数学
- en: 'To begin with, we need some of the basic mathematics concepts in place for
    a better understanding of the algorithm. The topics from mathematics that we need
    are listed as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要掌握一些基本的数学概念，以便更好地理解算法。我们所需的数学主题如下：
- en: '**Vectors**: Vectors have magnitude and direction. So, when we draw a vector
    in vector space it carries some magnitude as well as direction. You can perform
    basic math operations on vectors.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量**：向量具有大小和方向。因此，当我们在向量空间中绘制一个向量时，它不仅包含大小，还包含方向。你可以对向量执行基本的数学运算。'
- en: '**Matrices**: A matrix is a grid of numbers or frequency count of words. It
    has rows and columns. We can define the dimensions of the matrix by counting the
    number of rows and columns it contains. You can refer to this link for more information
    on matrices.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**矩阵**：矩阵是由数字或单词频率统计组成的网格。它具有行和列。我们可以通过计数矩阵所包含的行数和列数来定义矩阵的维度。你可以参考这个链接以获取更多关于矩阵的信息。'
- en: '**Partial derivative**: If there is a function that contains more than one
    variable and we perform a derivative for this kind of function with respect to
    one of these variables and hold others constant, this is how we perform partial
    derivative. Partial derivatives are used in vector calculus.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏导数**：如果一个函数包含多个变量，并且我们对该函数关于其中一个变量进行求导，并将其他变量保持不变，这就是偏导数的计算方法。偏导数在向量微积分中应用广泛。'
- en: '**Partial derivative chain rule**: The chain rule is defined as a formula that
    is used for computing the derivative of the composition of two or more functions.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏导数链式法则**：链式法则被定义为用于计算两个或更多函数复合的导数的公式。'
- en: Now let's jump to the understanding of the techniques. I have segregated all
    concepts broadly into three sections according to the stages where each and every
    technique has been used.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始理解这些技巧。我已经将所有概念根据每个技巧使用的阶段，粗略分为三大类。
- en: 'I have listed some of the reference links from where you get an in-depth idea
    about each of the given concepts. You can follow this link:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我列出了一些参考链接，你可以从中获得关于每个概念的深入理解。你可以访问这个链接：
- en: '[https://www.khanacademy.org/math.](https://www.khanacademy.org/math)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.khanacademy.org/math.](https://www.khanacademy.org/math)'
- en: 'You can refer to the following links for vectors:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下链接来了解向量：
- en: '[http://emweb.unl.edu/math/mathweb/vectors/vectors.html.](http://emweb.unl.edu/math/mathweb/vectors/vectors.html)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://emweb.unl.edu/math/mathweb/vectors/vectors.html.](http://emweb.unl.edu/math/mathweb/vectors/vectors.html)'
- en: '[https://www.mathsisfun.com/algebra/vectors.html.](https://www.mathsisfun.com/algebra/vectors.html)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.mathsisfun.com/algebra/vectors.html.](https://www.mathsisfun.com/algebra/vectors.html)'
- en: 'You can refer to this link for more information on matrix:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考这个链接以获取更多关于矩阵的信息：
- en: '[https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c.](https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c.](https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c)'
- en: 'You can see some basic examples by using this link:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这个链接，你可以看到一些基础示例：
- en: '[http://mathinsight.org/partial_derivative_examples.](http://mathinsight.org/partial_derivative_examples)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://mathinsight.org/partial_derivative_examples.](http://mathinsight.org/partial_derivative_examples)'
- en: '[http://www.analyzemath.com/calculus/multivariable/partial_derivatives.html.](http://www.analyzemath.com/calculus/multivariable/partial_derivatives.html)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://www.analyzemath.com/calculus/multivariable/partial_derivatives.html.](http://www.analyzemath.com/calculus/multivariable/partial_derivatives.html)'
- en: 'You can refer to the following links for the partial derivative chain rule:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下链接来了解偏导数链式法则：
- en: '[https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/chain/chain.html.](https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/chain/chain.html)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/chain/chain.html.](https://math.oregonstate.edu/home/programs/undergrad/CalculusQuestStudyGuides/vcalc/chain/chain.html)'
- en: '[https://www.youtube.com/watch?v=HOYA0-pOHsg.](https://www.youtube.com/watch?v=HOYA0-pOHsg)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=HOYA0-pOHsg.](https://www.youtube.com/watch?v=HOYA0-pOHsg)'
- en: '[https://www.youtube.com/watch?v=aZcw1kN6B8Y.](https://www.youtube.com/watch?v=aZcw1kN6B8Y)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=aZcw1kN6B8Y.](https://www.youtube.com/watch?v=aZcw1kN6B8Y)'
- en: The preceding list is more than enough for this chapter to understand the mathematics
    behind the algorithms.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 上述列表已经足够帮助理解本章中算法背后的数学原理。
- en: Techniques used at the vocabulary building stage
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词汇构建阶段使用的技巧
- en: While generating vocabulary from the dataset, you may use an optimization technique,
    and lossy counting is the one that is used the most for the word2vec model.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在从数据集中生成词汇时，你可以使用一种优化技术，其中有损计数法是最常用于word2vec模型的技术。
- en: Lossy counting
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有损计数
- en: The lossy count algorithm is used to identify elements in a dataset whose frequency
    count exceeds a user-given threshold. This algorithm takes data streams as an
    input instead of the finite set of a dataset.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 有损计数算法用于识别数据集中那些频率计数超过用户设定阈值的元素。该算法以数据流作为输入，而不是数据集的有限集合。
- en: With lossy counting, you periodically remove very low-count elements from the
    frequency table. The most frequently accessed words would almost never have low
    counts anyway, and if they did, they wouldn't be likely to stay there for long.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用有损计数法，你会定期从频率表中移除计数非常低的元素。最常访问的词几乎不会有低频计数，哪怕它们有，通常也不会长时间停留在那里。
- en: Here, the frequency threshold is usually defined by the user. When we give a
    parameter of `min_count = 4`, we remove the words that appear in the dataset less
    than four times and we will not consider them.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，频率阈值通常由用户定义。当我们设置参数`min_count = 4`时，我们会移除在数据集中出现次数少于四次的词，并且不再考虑它们。
- en: Using it at the stage of vocabulary building
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在词汇构建阶段使用它
- en: Lossy counting is very useful; especially when you have a large corpus and you
    don't want to consider the words that appear very rarely.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 有损计数法非常有用，特别是当你有一个大型语料库并且不想考虑那些出现非常罕见的词时。
- en: At this time, lossy counting is very useful because the user can set the minimum
    word frequency count as a threshold, so words that occur less than the threshold
    frequency count won't be included in our vocabulary.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，有损计数法非常有用，因为用户可以设置一个最小词频作为阈值，这样频率低于该阈值的词将不会被包含在我们的词汇中。
- en: So, if you have a large corpus and you want to optimize the speed of training,
    then we can use this algorithm.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你有一个大型语料库，并且想优化训练速度，那么我们可以使用这个算法。
- en: In other words, you can say that by using this algorithm you narrow down your
    vocabulary size, thus, you can speed up the training process.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，你可以说通过使用这个算法，你缩小了词汇量，从而可以加速训练过程。
- en: Applications
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: Apart from word2vec, the lossy counting algorithm is used in network traffic
    measurements and analysis of web server logs.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 除了word2vec之外，有损计数算法还用于网络流量测量和Web服务器日志分析。
- en: Techniques used at the context building stage
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在构建上下文阶段使用的技术
- en: 'While generating word context pairs, the context builder uses the following
    techniques:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成词对上下文时，上下文构建器使用以下技术：
- en: Dynamic window scaling or dynamic context window
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态窗口缩放或动态上下文窗口
- en: Subsampling
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子抽样
- en: Pruning
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剪枝
- en: Dynamic window scaling
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态窗口缩放
- en: As you can see, dynamic window scaling is a part of the context builder. We
    will see how it can be useful and what kind of impact it generates when we use
    it. Dynamic window scaling is also known as **dynamic context window**.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，动态窗口缩放是上下文构建器的一部分。我们将看到它如何有用，以及在使用它时会产生什么样的影响。动态窗口缩放也被称为**动态上下文窗口**。
- en: Understanding dynamic context window techniques
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解动态上下文窗口技术
- en: In the word2vec implementation, dynamic context window is an optional technique
    that may be applied to generate more accurate output. You can also consider these
    techniques as hyperparameters.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在word2vec实现中，动态上下文窗口是一种可选的技术，可以用来生成更精确的输出。你也可以将这些技术视为超参数。
- en: Dynamic context window techniques use weight schema for context words with respect
    to target words.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 动态上下文窗口技术使用权重方案来衡量上下文词与目标词之间的关系。
- en: So the intuition here is that words that are near to the target word are more
    important than other words that are far away from the target word.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里的直觉是，离目标词近的词比那些离目标词远的词更为重要。
- en: Let us see how it will be useful when we are building word pairs. Dynamic context
    window considers that nearby context words hold more importance to predicting
    the target word. Here, we are applying the weighting scheme by using uniform sampling
    on the actual window size between 1 and L. For example, suppose the context window
    size is 5 and now the weight of context words are distributed in a uniform manner,
    so the weight of most nearby words is 5/5, the very next context word weight is
    4/5, and so on. So, the final weight for context words will be 5/5, 4/5, 3/5,
    2/5, 1/5\. Thus, by providing weight, you can fine-tune the final result.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在构建词对时它如何发挥作用。动态上下文窗口考虑到附近的上下文单词对预测目标单词更为重要。在这里，我们通过对实际窗口大小（1 到 L 之间）使用均匀抽样的加权方案。举个例子，假设上下文窗口大小为
    5，现在上下文单词的权重是均匀分布的，因此，最接近的单词权重为 5/5，紧接着的上下文单词权重为 4/5，依此类推。所以，最终的上下文单词权重将为 5/5、4/5、3/5、2/5、1/5。因此，通过加权，你可以微调最终结果。
- en: Subsampling
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子抽样
- en: Subsampling is also one of the techniques that we use when we are building word
    pairs, and as we know, these word pairs are sample training data.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 子抽样也是我们在构建词对时使用的技术之一，正如我们所知，这些词对是样本训练数据。
- en: Subsampling is the method that removes the most frequent words. This technique
    is very useful for removing stop words.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 子抽样是去除最频繁单词的一种方法。这项技术对去除停用词非常有用。
- en: 'These techniques also remove words randomly, and these randomly chosen words
    occur in the corpus more frequently. So, words that are removed are more frequent
    than some threshold `t` with a probability of `p`, where `f` marks the words corpus
    frequency and we use *t = 10−5* in our experiments. Refer to the following equation
    given in *Figure 6.15*:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术还会随机删除单词，而这些随机选择的单词在语料库中出现得更频繁。因此，被删除的单词频率超过某个阈值 `t`，它们的出现概率为 `p`，其中 `f`
    标记了单词的语料频率，在我们的实验中使用 *t = 10−5*。请参考 *图 6.15* 中给出的方程：
- en: '![](img/b79f0040-3022-4c81-bf10-b2979cdd8afc.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b79f0040-3022-4c81-bf10-b2979cdd8afc.png)'
- en: 'Figure 6.15: Subsampling probability equation'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15：子抽样概率方程
- en: This also acts as one of the useful hyperparameters, and it is very useful because
    we are removing the most frequent and unnecessary words from the corpus, as well
    as from the context window, and that way, we are improving the quality of our
    training sample set.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这也作为一种有用的超参数发挥作用，它非常有用，因为我们在去除语料库和上下文窗口中最频繁且不必要的单词，从而提升了训练样本集的质量。
- en: Pruning
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 剪枝
- en: Pruning is also used when we are building our word pairs for training purposes
    using context builder. When you have a large amount of vocabulary to deal with,
    if you have included less frequent words, then you need to remove them. You can
    also restrict the size of the total vocabulary by using the `max_vocab_size` parameter
    given in the Python `gensim` library.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝在我们使用上下文构建器为训练目的构建词对时也会使用。当你需要处理大量词汇时，如果你包含了不太频繁的单词，就需要将它们去除。你还可以通过使用 Python
    `gensim` 库中的 `max_vocab_size` 参数来限制词汇表的大小。
- en: Let's see how useful pruning is for us in order to generate word2vec. Pruning
    is used to prune the training sample size, as well as make the quality of it better.
    If you don't prune the rarely occurred words from the dataset, then the model
    accuracy may degrade. This is a kind of hack to improve the accuracy.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看剪枝在生成 word2vec 时对我们有多大用处。剪枝用于修剪训练样本的大小，并提升其质量。如果你不从数据集中剪除那些很少出现的单词，那么模型的准确性可能会降低。这是一种提升准确性的技巧。
- en: Algorithms used by neural networks
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络使用的算法
- en: Here, we will look at the structure of individual neurons. We will also look
    into the details about the two algorithms, thus, we will understand how word2vec
    generates vectors from words.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将看看单个神经元的结构。我们还将深入了解这两个算法的细节，从而理解 word2vec 如何从单词生成向量。
- en: Structure of the neurons
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经元的结构
- en: We have seen the overall neural network structure, but we haven't yet seen what
    each neuron is made of and what the structure of the neurons is. So, in this section,
    we will look at the structure of each single input neuron.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过整体的神经网络结构，但我们还没有看到每个神经元的构成以及神经元的结构。因此，在这一部分中，我们将查看每个单独输入神经元的结构。
- en: 'We will look at the following structures:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看以下结构：
- en: Basic neuron structure
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本神经元结构
- en: Training a single neuron
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练单个神经元
- en: Single neuron application
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个神经元的应用
- en: Multi-layer neural network
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层神经网络
- en: Backpropagation
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反向传播
- en: Mathematics behind the word2vec model
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec 模型背后的数学
- en: In this, we will heavily include the mathematical formulas. If you are not from
    a mathematical background, then don't worry. I will give you explanations in simple
    words so you know what is going on in each section.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将大量包含数学公式。如果你没有数学背景，不用担心。我会用简单的语言给你解释，让你明白每一部分在做什么。
- en: Basic neuron structure
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本神经元结构
- en: 'Refer to *Figure 6.16;* it shows the basic neuron structure:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅 *图 6.16*；它展示了基本的神经元结构：
- en: '![](img/f70b4079-ac1a-46ac-ab0e-d119336d8906.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f70b4079-ac1a-46ac-ab0e-d119336d8906.png)'
- en: 'Figure 6.16: Basic neuron structure (Image credits: Xin Rong)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16：基本神经元结构（图片来源：Xin Rong）
- en: '*Figure 6.16* shows a basic neuron structure. This structure is not new. This
    structure takes an input, and there are weights also as input, and they calculate
    the weighted sum. Here, *x1* to *xk* is the input value and *w1* to *wk* is the
    corresponding weights. So, the weighted sum is expressed by the following equation
    given in *Figure 6.17*:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.16* 显示了一个基本的神经元结构。这个结构并不新颖。该结构接受输入，并且也有权重作为输入，它们计算加权和。在这里，*x1* 到 *xk*
    是输入值，*w1* 到 *wk* 是对应的权重。因此，加权和通过 *图 6.17* 中给出的以下方程来表示：'
- en: '![](img/cf4e0175-6266-422d-8ad7-a0c4a91b4e9c.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf4e0175-6266-422d-8ad7-a0c4a91b4e9c.png)'
- en: 'Figure 6.17: Weighted sum equation'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17：加权和方程
- en: Let's take an example to understand the usage of the equation. So, if you have
    input *x=[1 5 6]* and *w=[0.5 0.2 0.1],* the weighted sum *u* is equal to *[1
    * 0.5 + 5 * 0.2 + 6 * 0.1],* so our final answer is *u = [0.5 + 1 + 0.6] = 2.1*.
    This is just a simple example to give you some concrete practical intuition about
    the real workings of the given equation. This is all about our input. Now we will
    talk about the output.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来理解方程的使用。假设你有输入 *x=[1 5 6]* 和 *w=[0.5 0.2 0.1]*，则加权和 *u* 等于 *[1 * 0.5
    + 5 * 0.2 + 6 * 0.1]*，所以我们的最终答案是 *u = [0.5 + 1 + 0.6] = 2.1*。这只是一个简单的例子，目的是给你一些关于给定方程实际运作的直观感受。以上是关于我们的输入。现在我们将讨论输出。
- en: 'In order to generate output, we can say from our basic neuron structure that
    our output is the function of weighted sum *u*. Here, *y* is the output and *f(u)*
    is the function of the weighted sum. You can see the equation given in *Figure
    6.18*:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成输出，我们可以从基本的神经元结构出发，得出我们的输出是加权和 *u* 的函数。在这里，*y* 是输出，*f(u)* 是加权和的函数。你可以看到在
    *图 6.18* 中给出的方程：
- en: '![](img/382e56be-de82-4c74-80ea-51cc035412db.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/382e56be-de82-4c74-80ea-51cc035412db.png)'
- en: 'Figure 6.18: Output y is function of weighted sum u'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18：输出 y 是加权和 u 的函数
- en: 'In neural networks, we can use different available functions and these functions
    are called **activation functions**. These functions are listed as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，我们可以使用不同的可用函数，这些函数被称为 **激活函数**。这些函数如下所示：
- en: Step function
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阶跃函数
- en: Sigmoid function
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: 'Some great scientists have stated that the given functions are a good choice
    as activation functions. For this chapter, we are not getting into the details
    of activation functions, but we will look at all the details regarding the activation
    functions in [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml), *Deep Learning
    for NLP and NLG Problems*. So, we will consider the given two functions as activation
    functions for word2vec. We will use either the step function or sigmoid function,
    not both at the same time, to develop word2vec. Refer to the equation of both
    of the functions in *Figure 6.19*:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 一些伟大的科学家表示，给定的函数是作为激活函数的不错选择。对于本章内容，我们不会深入探讨激活函数的细节，但我们将在 [第九章](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml)，*自然语言处理与生成任务中的深度学习*
    中详细讨论激活函数的所有细节。因此，我们将把这两个函数作为 word2vec 的激活函数。我们将使用阶跃函数或 sigmoid 函数中的一个，而不是同时使用这两个函数来开发
    word2vec。请参阅 *图 6.19* 中的两个函数的方程：
- en: '![](img/a8c40266-ca20-4c0a-9f9e-778caeac164f.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8c40266-ca20-4c0a-9f9e-778caeac164f.png)'
- en: 'Figure 6.19: Activation functions, the first is the step function and the second
    is the sigmoid function'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19：激活函数，第一个是阶跃函数，第二个是 sigmoid 函数
- en: Here, *f(u)* is the step function and *f(u)* is the sigmoid function.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*f(u)* 是阶跃函数，而 *f(u)* 也是 sigmoid 函数。
- en: When we draw a circle to represent the neuron similar to the one drawn in *Figure
    6.11*, this circle contains the weighted sum and activation function, which is
    the reason we have indicated the dotted circle in *Figure 6.16*.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们画一个圆来表示神经元，类似于 *图 6.11* 中画的那样，这个圆包含了加权和和激活函数，这也是我们在 *图 6.16* 中表示虚线圆圈的原因。
- en: In the next section, we will see how these activation functions can be used
    and how we can calculate the errors in predicted output by using the error function.
    We will see this in detail about the usage of the activation function and error
    function. So let's begin!
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到这些激活函数如何被使用，以及我们如何通过使用误差函数计算预测输出中的误差。我们将详细了解激活函数和误差函数的用法。所以让我们开始吧！
- en: Training a simple neuron
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个简单的神经元
- en: Now it is time to see how we can use a single neuron for training by using the
    activation function, and let's understand the loss function to calculate the error
    in predicted output.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看我们如何通过使用激活函数来训练单个神经元，并且让我们了解损失函数来计算预测输出中的误差。
- en: The main idea is defined as the error function, which actually tells us the
    degree of error in our prediction; we will actually try to make our error value
    as low as possible. So, in other words, we are actually trying to improve our
    prediction. During training, we use input and calculate the error by using the
    error function and update the weight of neurons and repeat our training process.
    We will continue this process until we get the minimum error rate of maximum,
    best, and accurate output.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 主要思想被定义为误差函数，它实际上告诉我们预测中的错误程度；我们实际上试图使我们的误差值尽可能低。因此，换句话说，我们实际上正在尝试改进我们的预测。在训练过程中，我们使用输入并通过使用误差函数计算误差，并更新神经元的权重并重复我们的训练过程。我们将继续这个过程，直到我们得到最小误差率或最大、最好和准确的输出。
- en: 'The two most important concepts that we are going to look at are listed as
    follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要查看的两个最重要的概念如下所示：
- en: Define error function (loss function)
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义误差函数（损失函数）
- en: Understanding of gradient descent in word2vec
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解word2vec中的梯度下降
- en: Define error function
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义误差函数
- en: Here, our input is vector `X` with vocabulary `x1` to *xk* and our output `y`
    is the output vector. So, to calculate the error `E` , we need to define the error
    function or loss function, and we are using the L2 loss function. Let's begin
    with understanding the basic concept of the L2 loss function, and then we will
    see how it will be useful in word2vec.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的输入是向量`X`，词汇表`x1`到*xk*，我们的输出`y`是输出向量。因此，要计算误差`E`，我们需要定义误差函数或损失函数，我们使用的是L2损失函数。让我们从理解L2损失函数的基本概念开始，然后我们将看到它在word2vec中的用途。
- en: 'There are two type of loss functions that are mostly used across **machine
    learning** (**ML**) and **deep learning** (**DL**). By the way, we will look at
    ML and DL in upcoming chapters, which are [*Chapter 8*](97808151-90d2-4034-8d53-b94123154265.xhtml),
    *Machine Learning ( ML) for NLP Problems* and [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml),
    *Deep Learning for NLP and NLG Problems*. There are two standard types of loss
    functions, but here we will only look at **least square error** (**L2**), and
    in upcoming chapters, which are [Chapter 8](97808151-90d2-4034-8d53-b94123154265.xhtml),
    *Machine Learning for NLP Problems* and [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml),
    *Deep Learning for NLP and NLG Problems* we will look at various error functions
    in detail and compare those error functions as well. The two standard types of
    loss functions are:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在**机器学习**（**ML**）和**深度学习**（**DL**）中，有两种类型的损失函数被广泛使用。顺便说一下，我们将在即将到来的章节中查看ML和DL，这些章节是[*第8章*](97808151-90d2-4034-8d53-b94123154265.xhtml)，*自然语言处理（NLP）问题的机器学习*
    和 [第9章](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml)，*自然语言处理（NLP）和自然语言生成（NLG）问题的深度学习*。有两种标准类型的损失函数，但在这里我们只关注**最小二乘误差**（**L2**），而在即将到来的章节中，这些章节是
    [第8章](97808151-90d2-4034-8d53-b94123154265.xhtml)，*自然语言处理（NLP）问题的机器学习* 和 [第9章](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml)，*自然语言处理（NLP）和自然语言生成（NLG）问题的深度学习*，我们将详细查看和比较这些误差函数。两种标准类型的损失函数分别是：
- en: Least absolute deviation (L1)
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小绝对偏差（L1）
- en: Least square error (L2)
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小二乘误差（L2）
- en: Least square error is also called **L2 loss** function. In general, loss functions
    want to achieve their minimized value while learning from a dataset and L2 loss
    functions also want to achieve their value where the error value will be minimal.
    So, precisely, the L2 function wants to minimize the squared differences between
    the estimated and existing target values.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 最小二乘误差也被称为**L2损失**函数。一般来说，损失函数希望在学习数据集时达到它们的最小化值，L2损失函数也希望在误差值最小的情况下达到它们的值。因此，确切地说，L2函数希望最小化估计值与现有目标值之间的平方差异。
- en: 'The structure of a single neuron at the time of training is given in *Figure
    6.20*:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时单个神经元的结构如*图6.20*所示：
- en: '![](img/f62cfc9f-068d-449c-9bb0-8307a712d33a.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f62cfc9f-068d-449c-9bb0-8307a712d33a.png)'
- en: 'Figure 6.20: Single neuron at the time of training'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.20：训练时的单个神经元
- en: 'So, when we are trying to calculate an L2 loss function of a single neuron,
    we will use the following equation given in *Figure 6.21*:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当我们尝试计算单个神经元的 L2 损失函数时，我们将使用 *图 6.21* 中给出的以下方程：
- en: '![](img/a6fa6d6a-7fa3-4359-8b3f-def68caff86e.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6fa6d6a-7fa3-4359-8b3f-def68caff86e.png)'
- en: 'Figure 6.21: L2 loss function equation (Least Squared error function)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.21：L2 损失函数方程（最小二乘误差函数）
- en: Here, *t* is the target vector value, *y* is the estimated vector value or predicted
    vector value, and *E* is the error function. We have defined our L2 error function.
    I know you must be very keen to know what we will be doing with this L2 error
    function to get a least error value, and that is where we need to understand the
    concept of gradient descent.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*t* 是目标向量值，*y* 是估计的向量值或预测的向量值，*E* 是误差函数。我们已经定义了我们的 L2 误差函数。我知道你一定很想知道我们将如何使用这个
    L2 误差函数来得到最小的误差值，接下来我们需要理解梯度下降的概念。
- en: Understanding gradient descent in word2vec
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 word2vec 中的梯度下降
- en: Now, let's understand what we are going to do with the L2 function and how it
    will be useful in achieving an accurate output.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解一下我们将如何使用 L2 函数，以及它如何在实现准确输出中发挥作用。
- en: As we said earlier, we want to minimize this function value so, we can accurately
    predict the target value, and to achieve this we will take partial derivative
    of the L2 function equation given in *Figure 6.21* with respect to *y*. The procedure
    of deriving derivation and then by using this derivation trying to minimize error
    values, is called **gradient descent**.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说，我们希望最小化这个函数值，以便准确预测目标值。为此，我们将对 *图 6.21* 中给出的 L2 函数方程对 *y* 进行偏导数。推导偏导数的过程，并通过使用这些推导来最小化误差值的过程，被称为
    **梯度下降**。
- en: 'In that case, the result is given in *Figure 6.22*:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，结果如 *图 6.22* 所示：
- en: '![](img/01aba3d0-bbfd-46d9-a936-b7b2c64a2c36.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01aba3d0-bbfd-46d9-a936-b7b2c64a2c36.png)'
- en: 'Figure 6.22: Result of Partial derivatives of L2 loss function with respect
    to y'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.22：关于 *y* 的 L2 损失函数偏导数结果
- en: 'We know that output *y* is dependent on *f(u)* and *f(u)* is dependent on input
    weight values *wi*. So we need to apply chain rules and generate the error function
    value. If we use partial derivative chain rules, then we will get the following
    equation, which will be useful in word2vec. *Figure 6.23* shows the result of
    the partial derivative chain rule:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道输出 *y* 依赖于 *f(u)*，而 *f(u)* 又依赖于输入权重值 *wi*。因此，我们需要应用链式法则来生成误差函数值。如果我们使用偏导数链式法则，我们将得到以下方程，这将在
    word2vec 中非常有用。*图 6.23* 展示了偏导数链式法则的结果：
- en: '![](img/fe9277c1-6873-4076-8ba3-dd667802265c.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe9277c1-6873-4076-8ba3-dd667802265c.png)'
- en: 'Figure 6.23: Partial derivative chain rule result for L2 error function'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.23：L2 误差函数的偏导数链式法则结果
- en: After calculating the L2 loss function as per the value of the error, the neural
    network input weight will be updated and this kind of iteration will continue
    until we achieve the minimum error value or error rate.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在根据误差值计算 L2 损失函数之后，神经网络的输入权重将被更新，这种迭代过程将持续进行，直到我们达到最小的误差值或误差率。
- en: So far, we have been deriving equations for a single neuron, so it will be important
    to know what this single neuron can do for us. That is our next point of discussion.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在推导单个神经元的方程，因此了解这个单个神经元能为我们做什么将非常重要。这是我们接下来要讨论的重点。
- en: Single neuron application
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单个神经元应用
- en: We have learnt a lot of technical and mathematical stuff about single neurons,
    so I really want to walk you through the application on single neurons with respect
    to the word2vec model. So let's begin!
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学到了很多关于单个神经元的技术和数学内容，所以我真的想带你一起了解在 word2vec 模型中单个神经元的应用。让我们开始吧！
- en: 'If you want to build a model that identifies the words that are edible and
    that are not, then we can use a single neuron to build the model. This kind of
    application, where we are segregating words either into edible classes or non-edible
    classes is called a **binary classification** **task**. For this kind of task,
    neurons are used to take one-hot encoded vectors as input and a single neuron
    will learn which words are related to edible items and which are not related to
    edible items. So, it will generate a look up table, which you can see in *Figure
    6.24*:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想构建一个模型来识别哪些词是可食用的，哪些是不可食用的，那么我们可以使用单个神经元来构建该模型。这种应用程序将词汇分为可食用类别或不可食用类别，称为**二元分类****任务**。对于这种任务，神经元会接受
    one-hot 编码的向量作为输入，单个神经元将学习哪些词与可食用物品相关，哪些与可食用物品无关。因此，它将生成一个查找表，如你在*图 6.24*中看到的那样：
- en: '![](img/8781ddad-2f2e-4aea-9849-e68108aeb524.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8781ddad-2f2e-4aea-9849-e68108aeb524.png)'
- en: 'Figure 6.24: Single neuron can classify words into edible and non edible categories'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.24：单个神经元可以将词汇分类为可食用和不可食用类别
- en: The reason why this kind of application can be built with such ease is that
    when you are continuously providing the one-hot word vector and use the sigmoid
    or step functions as the activation function, then it becomes a standard classification
    problem, and this kind of problem can be solved easily by using mathematics. We
    have defined this in previous sections because for edible items, the output vector
    have the same kind of values, and for non-edible items generated vectors, they
    represent the same kind of output vector. In the end, we will be able to build
    the lookup table. This kind of standard classification problem reminds us of logistic
    regression, and we are applying the same logistic regression concepts, but here
    we are using a single neuron structure.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以能够如此轻松地构建这种应用，是因为当你不断提供 one-hot 词向量并使用 sigmoid 或阶跃函数作为激活函数时，它就变成了一个标准的分类问题，而这种问题可以通过数学方法轻松解决。我们在前面的部分中已经定义了这一点，因为对于可食用物品，输出向量具有相同的值，而对于不可食用物品，生成的向量代表相同类型的输出向量。最终，我们将能够构建查找表。这种标准的分类问题让我们想起了逻辑回归，我们在应用相同的逻辑回归概念，但这里我们使用的是单个神经元结构。
- en: We have seen enough about the single neuron structure, now it is time to explore
    the multi-layer neural network. Our next section will provide you with information
    about multilayer neural networks.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了足够的单个神经元结构，现在是时候探索多层神经网络了。接下来的部分将为你提供有关多层神经网络的信息。
- en: Multi-layer neural networks
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层神经网络
- en: A multi-layer neural network is the structure that we are using for word2vec.
    This function takes input as a one-hot encoded word vector and this vector, as
    well as a weighted sum is passed to a hidden layer. By using the activation function,
    which is the sigmoid function in this case, output is generated from the hidden
    layer and this output is passed to the next layer, which is the output layer.
    We have already seen an example in this chapter, in the section entitled *Neural
    network with two layers*. In that section, we did not look at the mathematical
    aspect, so here I will walk you through the mathematical aspect of neural networks.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 多层神经网络是我们在使用 word2vec 时采用的结构。该函数以 one-hot 编码的词向量作为输入，这个向量以及加权和会传递到隐藏层。通过使用激活函数（在这种情况下是
    sigmoid 函数），从隐藏层生成输出，并将该输出传递到下一层，即输出层。我们在本章的*具有两层的神经网络*部分已经看过一个示例。在该部分，我们没有深入探讨数学方面的内容，因此在这里我将带你了解神经网络的数学原理。
- en: 'So, let''s represent what I told you in the preceding paragraph through a mathematical
    equation. See the structure of a multi-layer neural network in *Figure 6.25*:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们通过数学方程式来表示我在前一段中告诉你的内容。参见*图 6.25*中的多层神经网络结构：
- en: '![](img/0c11fdd7-ce94-4e85-9743-d7a96834cd2a.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0c11fdd7-ce94-4e85-9743-d7a96834cd2a.png)'
- en: 'Figure 6.25: Multi-layer neural network'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.25：多层神经网络
- en: 'Now, let''s see the mathematical equations for the given neural network. Here,
    I''m going to provide you with high-level intuition, which will help you to understand
    the flow and you will get an idea about the input and output functions. Refer
    to *Figure 6.26*:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看给定神经网络的数学方程式。在这里，我将为你提供高层次的直觉，这将帮助你理解流程，并让你了解输入和输出函数的概念。请参阅*图 6.26*：
- en: '![](img/9c362c99-f5b6-4fa5-b1e6-875ac3b76707.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c362c99-f5b6-4fa5-b1e6-875ac3b76707.png)'
- en: 'Figure 6.26: Mathematical equation for multilayer neural networks'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.26：多层神经网络的数学方程式
- en: 'The flow of the input and output is given as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 输入和输出的流动如下所示：
- en: The first equation of *Figure 6.24* is a weighted sum the of input layer and
    the result of the input layer will be passed on to the hidden layer. *ui* is the
    weighted sum of the input layer. The activation function of the hidden layer is
    given in the second equation. The activation function *hi* uses the sigmoid function
    and generates the intermediate output.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图6.24*中的第一个方程是输入层的加权和，输入层的结果将传递到隐藏层。*ui*是输入层的加权和。隐藏层的激活函数由第二个方程给出。激活函数*hi*使用Sigmoid函数并生成中间输出。'
- en: The weighted sum of the hidden layer will be passed to the output layer and
    the third equation shows the calculation of the hidden layer weighted sum. *u'j*
    is the weighted sum from the hidden layer and it will be passed to the output
    layer. *yj* uses the weighted sum from the hidden layer, which is *u'j* as well,
    as here also the activation function is sigmoid.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的加权和将传递到输出层，第三个方程显示了隐藏层加权和的计算。*u'j*是来自隐藏层的加权和，它将传递到输出层。*yj*使用来自隐藏层的加权和，即*u'j*，因为这里的激活函数也是Sigmoid。
- en: We have seen the flow of input and output by using a basic mathematical representation.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经通过基本的数学表示法看到了输入和输出的流动。
- en: Now, a major concern is how this structure is used to get trained for the word2vec
    model, and the answer to that is, we use backpropagation to train the model, which
    we will see in the next section.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一个主要的问题是如何将这个结构用于训练word2vec模型，答案是，我们使用反向传播来训练模型，这将在下一节中讨论。
- en: Backpropagation
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: We have already seen how errors have been calculated using the L2 loss function,
    and the L2 loss function wants to minimize the squared differences between the
    estimated and existing target values. We will apply the same concepts to the multi-layer
    neural network. So, we need to define the loss function as well as we to take
    the gradient of the function and update the weight of the neural network in order
    to generate a minimum error value. Here, our input and output is vectors.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何使用L2损失函数计算误差，L2损失函数旨在最小化估计值与真实目标值之间的平方差。我们将相同的概念应用于多层神经网络。因此，我们需要定义损失函数，并对其进行求导，以更新神经网络的权重，从而生成最小误差值。在这里，我们的输入和输出是向量。
- en: 'Refer to *Figure 6.27* to see the neural network structure, and *Figure 6.28*
    shows what equations we need to apply to calculate error functions in a multi-layer
    neural network:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*图6.27*查看神经网络结构，*图6.28*展示了我们需要应用的方程来计算多层神经网络中的误差函数：
- en: '![](img/95ea54bf-11b2-4027-b3de-19a570a01aa1.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95ea54bf-11b2-4027-b3de-19a570a01aa1.png)'
- en: 'Figure 6.27: Multi-layer neural network for calculating error functions'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.27：用于计算误差函数的多层神经网络
- en: 'Now let''s see the derivation and mathematical calculation performed by neural
    networks. You can see the equations are given as follows, refer to *Figure 6.28*:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下神经网络进行推导和数学计算的过程。你可以看到方程如下所示，请参考*图6.28*：
- en: '![](img/13b778d6-f544-46b4-8401-df6697c5e815.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13b778d6-f544-46b4-8401-df6697c5e815.png)'
- en: 'Figure 6.28: Equations for calculating error function for multi-layer neural
    networks'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.28：计算多层神经网络误差函数的方程
- en: When you are calculating error function for multi-layer neural networks, you
    need to be very careful about the indexes, as well as which layer you are calculating
    an error function value for. As you can see in *Figure 6.28,* we will start with
    the output index, backpropagate the error to the hidden layer, and update the
    weight. In the fifth equation, you can see that we need to calculate the error
    function for the output layer so that backpropagate the error and we can update
    the weight of the input layer. To deal with indexes is a kind of a challenging
    task in multi-layer neural networks. But coding it up is quite easy because you
    just need to write a `for` loop to calculate each of the layer gradients.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 当你计算多层神经网络的误差函数时，你需要非常小心索引以及你正在计算哪个层的误差函数值。如*图6.28*所示，我们将从输出层索引开始，反向传播误差到隐藏层，并更新权重。在第五个方程中，你可以看到我们需要计算输出层的误差函数，以便反向传播误差并更新输入层的权重。处理索引在多层神经网络中是一项具有挑战性的任务。但编码实现起来相当简单，因为你只需要写一个`for`循环来计算每一层的梯度。
- en: Now, we will put all the mathematical equations and concepts of word2vec together
    and understand the final mathematical piece of word2vec neural networks.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将把 word2vec 的所有数学方程和概念结合在一起，理解 word2vec 神经网络的最终数学部分。
- en: Mathematics behind the word2vec model
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec 模型背后的数学原理
- en: In this section, we will look at the final piece of mathematics by combining
    all the previous equations and concepts, and we will derive the final equation
    in the form of probability. We have already seen the concept and basic intuition,
    calculation, and example in the previous section, *Word2vec neural network layers
    details*.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过将所有前面的方程和概念结合起来，来看最后一部分数学内容，并以概率的形式推导最终方程。我们已经在前一节中看到了概念、基本直觉、计算和示例，*Word2vec
    神经网络层的详细信息*。
- en: The word2vec neural network is using a one-hot encoded word vector as input
    and then it passes this vector value to the next layer, which is the hidden layer,
    and this is nothing but the weighted sum values that feed into the hidden layer
    as input. The last output layer generates the vector value, but to make sense
    of the output, we will convert the vector into probability format, and with the
    help of softmax techniques, we will also convert the output word vector into probability
    format. We will see all the different techniques that are used to generate probability
    from the output vector in the upcoming section, until then, just use softmax as
    a magic function.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 神经网络使用一个独热编码的词向量作为输入，然后将该向量值传递给下一层，即隐藏层，这实际上是加权和值，它作为输入传入隐藏层。最后的输出层生成词向量值，但为了理解输出，我们会将该向量转换为概率格式，借助
    softmax 技术，我们还将输出词向量转换为概率格式。我们将在接下来的章节中看到用于从输出向量生成概率的不同技术，在那之前，只需将 softmax 视为一个神奇函数。
- en: 'Refer to *Figure 6.29* to understand the mathematics behind word2vec neural
    networks:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考 *图 6.29* 以理解 word2vec 神经网络背后的数学原理：
- en: '![](img/9eb90f9d-6eb6-4ec1-b6c2-07b13b3ad9e0.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9eb90f9d-6eb6-4ec1-b6c2-07b13b3ad9e0.png)'
- en: 'Figure 6.29: Mathematics behind word2vec model'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.29：word2vec 模型背后的数学
- en: 'In the first equation, we can see the weighted sum of the input word vector
    and weights and get *h* in the second equation. We multiply *h* and the weighted
    sum of word vectors of the hidden layer *v''wj*. Here, weight and index has been
    changed. This multiplication is *uj* . Here, *uj* is the activation function.
    We will then generate probability by using the value of *uj* . So, the final equation
    is the softmax function. Let''s simplify the equation by replacing *uj* in the
    third equation with the input and output word vector format, then you will get
    the final equation. Refer to *Figure 6.30*:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个方程中，我们可以看到输入词向量与权重的加权和，并在第二个方程中得到 *h*。我们将 *h* 与隐藏层词向量的加权和 *v'wj* 相乘。这里，权重和索引已经发生了变化。这次乘法得到了
    *uj*。这里，*uj* 是激活函数。然后，我们将使用 *uj* 的值生成概率。因此，最终的方程就是 softmax 函数。让我们通过将第三个方程中的 *uj*
    替换为输入和输出词向量格式来简化方程，最终得到最终方程。请参考 *图 6.30*：
- en: '![](img/f807cd33-cb1a-4344-a7c6-b0f8f21a41cf.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f807cd33-cb1a-4344-a7c6-b0f8f21a41cf.png)'
- en: 'Figure 6.30: Final probability equation of the word2vec model'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.30：word2vec 模型的最终概率方程
- en: 'This time, our output is a softmax function, so for updating weight using backpropagation,
    we need to define the loss function. So, here, we will define the loss function
    in the form of the softmax function, so we will use minus log probability of the
    softmax function and then we will perform gradient descent. See *Figure 6.31*:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们的输出是一个 softmax 函数，因此为了使用反向传播更新权重，我们需要定义损失函数。因此，我们将在这里定义一个以 softmax 函数形式的损失函数，我们将使用
    softmax 函数的负对数概率，然后执行梯度下降。请参见 *图 6.31*：
- en: '![](img/c97abf5f-5208-4dcf-988b-3722bd1de0ac.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c97abf5f-5208-4dcf-988b-3722bd1de0ac.png)'
- en: 'Figure 6.31: Error function gradient descent in form of minus log probability
    of the softmax function'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.31：误差函数梯度下降形式的 softmax 函数负对数概率
- en: 'Here, I want to give some idea of how the output vector value has been updated.
    So, it is an updating rule for an output layer. You can find the equation given
    as follows. Refer to *Figure 6.32*:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我想大致说明输出向量值是如何被更新的。这是输出层的更新规则。你可以找到以下给出的方程。请参考 *图 6.32*：
- en: '![](img/a838de75-1d19-4140-ad57-d2ee51996bed.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a838de75-1d19-4140-ad57-d2ee51996bed.png)'
- en: 'Figure 6.32: Rule for updating the output vector'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.32：更新输出向量的规则
- en: In this equation, we are taking the original output vector and subtracting the
    prediction error *ej* of the output node and *h* is the value of the hidden layer.
    So, the meaning of this equation is that, if we have the word **climbed** as input,
    and we want to predict the word **cat** as output, then how we can update the
    vector value of the word **climbed** so that it will be more similar to the vector
    of the word **cat**? So in simple language we can say we will add some part of
    the vector of **climbed** to the vector of the word **cat** and apart from this,
    we also need to update the output vector of other words because all other words
    that are not our target words should update their output vector so that they will
    be less similar to the target words.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，我们取原始输出向量，并减去输出节点的预测误差*ej*，*h* 是隐藏层的值。所以，这个公式的意思是，如果我们将单词**climbed**作为输入，并希望预测单词**cat**作为输出，那么我们如何更新单词**climbed**的向量，使其与单词**cat**的向量更加相似？简单来说，我们可以说，我们将单词**climbed**的部分向量加到单词**cat**的向量上，除此之外，我们还需要更新其他单词的输出向量，因为所有不作为目标词的单词也应更新其输出向量，使它们与目标词的相似度更低。
- en: 'The rule for updating the input vector is also useful; the equation of updating
    an input vector is given as follows. Refer to *Figure 6.33*:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 更新输入向量的规则也非常有用；更新输入向量的公式如下所示。请参阅*图 6.33*：
- en: '![](img/06d1f10f-cf58-46c8-8604-e7f34ef5a9fd.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06d1f10f-cf58-46c8-8604-e7f34ef5a9fd.png)'
- en: 'Figure 6.33: Rule for updating input vector'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.33：更新输入向量的规则
- en: This equation is a bit complex. So, here, intuition is the input vector, which
    will be subtracted from the weighted sum of the prediction errors. The meaning
    of this is that, this time, we are going to update the input vector **cat**. We
    will update the vector value of the word cat in such a manner that it will come
    close to the vector of the word **climbed**. Here, co-occurrence of the words
    play a major role.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式有点复杂。所以，这里，直觉是输入向量，它将从预测误差的加权和中减去。意思是，这次我们要更新输入向量**cat**。我们将以一种方式更新单词**cat**的向量，使其接近单词**climbed**的向量。在这里，词语的共现起着重要作用。
- en: We are almost done with the math portion of the word2vec model. We have seen
    a lot of mathematical equations that are used in the word2vec model. Now we will
    talk about the techniques that are used to generate the final vectors and probability
    of the prediction.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎完成了word2vec模型的数学部分。我们已经看到很多在word2vec模型中使用的数学公式。现在我们将讨论用于生成最终向量和预测概率的技术。
- en: Techniques used to generate final vectors and probability prediction stage
  id: totrans-380
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于生成最终向量和概率预测阶段的技术
- en: In this section, we will see how we will generate the final vector. We will
    also use some heuristics to generate output efficiently. So, we will talk about
    those heuristics as well.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将看到如何生成最终的向量。我们还将使用一些启发式方法来高效地生成输出。所以，我们也会讨论这些启发式方法。
- en: 'As we have already seen, to generate the word vector we need to update input
    as well as the output vector. Suppose we have a million words in our vocabulary,
    so the process of updating the input and output vectors will take a lot of time
    and it will be inefficient. We have to solve this challenge. So we use some of
    the optimized ways to perform the same operations and those techniques are given
    as follows:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，为了生成词向量，我们需要更新输入向量以及输出向量。假设我们词汇表中有一百万个词，那么更新输入和输出向量的过程将会非常耗时且低效。我们必须解决这个挑战。因此，我们采用一些优化方法来执行相同的操作，这些技术如下所示：
- en: Hierarchical softmax
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次化 softmax
- en: Negative sampling
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负采样
- en: So, let's start to understand these techniques.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们开始了解这些技术。
- en: Hierarchical softmax
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次化 softmax
- en: 'In hierarchical softmax, instead of mapping each output vector to its corresponding
    word, we consider the output vector as a form of binary tree. Refer to the structure
    of hierarchical softmax in *Figure 6.34*:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在层次化 softmax 中，我们不将每个输出向量映射到其对应的词，而是将输出向量视为二叉树的形式。请参阅*图 6.34*中的层次化 softmax 结构：
- en: '![](img/98a86731-7bb0-4125-90d2-d12252c8907d.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98a86731-7bb0-4125-90d2-d12252c8907d.png)'
- en: 'Figure 6.34: Hierarchical Softmax structure'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.34：层次化 Softmax 结构
- en: 'So, here, the output vector is not making a prediction about how probable the
    word is, but it is making a prediction about which way you want to go in the binary
    tree. So, either you want to visit this branch or you want to visit the other
    branch. Refer to *Figure 6.35*:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在这里，输出向量并不是预测某个词的概率，而是在预测你在二叉树中要走的路径。也就是说，你要么访问这一分支，要么访问另一个分支。参见*图6.35*：
- en: '![](img/2fccd091-58bd-422a-bfda-654878270ec9.png)'
  id: totrans-391
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fccd091-58bd-422a-bfda-654878270ec9.png)'
- en: 'Figure 6.35: Prediction path using hierarchical softmax mantle representation'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.35：使用层次化softmax外壳表示的预测路径
- en: In this case, consider the red activated dot going up (light grey here) and
    the blue activated dot going downwards (dark grey here), so you can see that,
    here, we can predict the word **juice** with high probability.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，考虑红色激活点向上（此处为浅灰色）和蓝色激活点向下（此处为深灰色），因此你可以看到，在这里，我们可以以较高的概率预测词**juice**。
- en: Here, the advantage is, if you want to backpropagate an error, then you just
    need to update one output vector and the error will propagate to only three nodes
    that are activated at the time of prediction. We use the Huffman binary tree construction
    to generate the binary tree.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，优点是，如果你想反向传播一个错误，那么你只需要更新一个输出向量，并且错误将仅传播到在预测时被激活的三个节点。我们使用霍夫曼二叉树构建来生成二叉树。
- en: Negative sampling
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负采样
- en: Negative sampling is also a kind of optimization method. In this method, we
    are going to update the vector of the output word, but we are not going to update
    all the vectors of other words. We just take the sample from the words other than
    the output vector. So, we are selecting a sample from the negative sample set
    of words, hence the name of this technique is negative sampling.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 负采样也是一种优化方法。在这种方法中，我们将更新输出词的向量，但不会更新其他所有词的向量。我们只从输出向量以外的词中取样。因此，我们从负样本集中的词中选择一个样本，这就是该技术被称为负采样的原因。
- en: Some of the facts related to word2vec
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些与word2vec相关的事实
- en: 'Here are some of the facts about the word2vec models that you should keep in
    mind when you are actually using it:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些使用word2vec时你应该记住的事实：
- en: So far, you will have realized that word2vec uses neural networks and this neural
    network is not a deep neural network. It only has two layers, but it works very
    well to find out the words similarity.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到目前为止，你应该已经意识到，word2vec使用神经网络，而这个神经网络并不是深度神经网络。它只有两层，但它非常有效地找出词语的相似性。
- en: Word2vec neural network uses a simple logistic activation function that does
    not use non-linear functions.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec神经网络使用简单的逻辑激活函数，不使用非线性函数。
- en: The activation function of the hidden layer is simply linear because it directly
    passes its weighted sum of inputs to the next layer.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层的激活函数是简单的线性函数，因为它直接将输入的加权和传递给下一层。
- en: Now, we have seen almost all the major aspects of word2vec, so in the next section,
    we will look at the application of word2vec.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经看到了word2vec的几乎所有主要方面，接下来的部分我们将研究word2vec的应用。
- en: Applications of word2vec
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec的应用
- en: 'Allow me to introduce some real-life applications in which word2vec has been
    used. They are:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 让我介绍一些实际应用，其中使用了word2vec。它们包括：
- en: Dependency parser uses word2vec to generate better and accurate dependency relationship
    between words at the time of parsing.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖解析器在解析时使用word2vec生成更好、更准确的词语之间的依赖关系。
- en: Name entity recognition can also use word2vec, as word2vec is very good at finding
    out similarity in **named entity recognition** (**NER**). All similar entities
    can come together and you will have better results.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别也可以使用word2vec，因为word2vec在**命名实体识别**（**NER**）中非常擅长发现相似性。所有相似的实体可以聚集在一起，从而获得更好的结果。
- en: Sentiment analysis uses it to preserve semantic similarity in order to generate
    better sentiment results. Semantic similarity helps us to know which kind of phrases
    or words people use to express their opinions, and you can generate good insights
    and accuracy by using word2vec concepts in sentiment analysis.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析利用它来保持语义相似性，从而生成更好的情感结果。语义相似性帮助我们了解人们用来表达意见的词组或单词，运用word2vec概念进行情感分析时，你可以生成更好的洞察力和准确性。
- en: We can also build an application that predicts a person's name by using their
    writing style.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以构建一个通过使用写作风格预测一个人名字的应用。
- en: If you want to do document classification with high accuracy and using simple
    statistics, then word2vec is for you. You can use the concept and categorize the
    documents without any human labels.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想用简单的统计方法高精度地进行文档分类，那么 word2vec 就是你需要的工具。你可以使用这个概念来对文档进行分类，而不需要任何人工标签。
- en: Word clustering is the fundamental product of word2vec. All words carrying a
    similar meaning are clustered together.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词聚类是 word2vec 的基本产品。所有具有相似含义的单词都会被聚集在一起。
- en: Google uses word2vec and deep learning to improve their machine translation
    product.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google 使用 word2vec 和深度学习来提升他们的机器翻译产品。
- en: There are so many use cases where you could use word2vec concepts. Here, we
    are going to implement some fun examples. We are going to build fun applications,
    as well as doing some visualization on them so you can understand the concept
    in a far better manner.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多应用场景可以使用 word2vec 的概念。在这里，我们将实现一些有趣的例子。我们将构建有趣的应用程序，并对其进行可视化，这样你就能以更好的方式理解这个概念。
- en: Implementation of simple examples
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单例子的实现
- en: In this section, we are going to implement the famous word2vec example, which
    is adding `woman` and `king` and subtracting `man`, and then the resultant vector
    shows the vector value of `queen`.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将实现著名的 word2vec 示例，即将 `woman` 和 `king` 相加，`man` 相减，最终得到的向量表示 `queen`。
- en: 'We are not going to train the word2vec model, on our data and then build our
    own word2vec model because there is a huge amount of data on which Google has
    already trained their word2vec model and provided us with pre-trained models.
    Now, if you want to replicate the training process on that much data, then we
    need a lot of computational resources, so we will use pre-trained word2vec models
    from Google. You can download the pre-trained model from this link: [https://code.google.com/archive/p/Word2vec/](https://code.google.com/archive/p/Word2vec/).'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在我们的数据上训练 word2vec 模型，然后构建自己的 word2vec 模型，因为 Google 已经在大量数据上训练了他们的 word2vec
    模型，并为我们提供了预训练模型。如果你想在这么多数据上复制训练过程，那么我们需要大量的计算资源，所以我们将使用 Google 提供的预训练 word2vec
    模型。你可以从这个链接下载预训练模型：[https://code.google.com/archive/p/Word2vec/](https://code.google.com/archive/p/Word2vec/)。
- en: After clicking on this link, you need to go to the section entitled pre-trained
    word and phrase vectors, download the model named `GoogleNews-vectors-negative300.bin.gz`,
    and extract it.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 点击此链接后，你需要进入标题为“预训练词和短语向量”的部分，下载名为 `GoogleNews-vectors-negative300.bin.gz` 的模型，并将其解压。
- en: We will use the `genism` library to build our famous example.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `genism` 库来构建我们著名的示例。
- en: Famous example (king - man + woman)
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 著名示例（king - man + woman）
- en: 'We are going to load the binary model by using the `gensim` library and replicate
    the example. If you are running it on your computer, then it will take a few minutes,
    so don''t worry and keep the script running. Refer to *Figure 6.36*, for the code
    snippet:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用 `gensim` 库加载二进制模型并复制这个例子。如果你在自己的计算机上运行它，可能需要几分钟时间，所以不用担心，保持脚本运行即可。参考*图6.36*中的代码片段：
- en: '![](img/03d8ba9f-98d8-416c-ae56-1706cd536ff0.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03d8ba9f-98d8-416c-ae56-1706cd536ff0.png)'
- en: 'Figure 6.36: Code snippet for example King - man + woman = queen'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.36：示例代码片段 King - man + woman = queen
- en: 'You can see the code by clicking on this GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch6/kingqueenexample.py](https://github.com/jalajthanaki/NLPython/blob/master/ch6/kingqueenexample.py)'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击这个 GitHub 链接查看代码：[https://github.com/jalajthanaki/NLPython/blob/master/ch6/kingqueenexample.py](https://github.com/jalajthanaki/NLPython/blob/master/ch6/kingqueenexample.py)
- en: 'You can refer to *Figure 6.37* for the output we are generating:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考*图6.37*查看我们正在生成的输出：
- en: '![](img/454a681c-4042-41be-9c43-6d9afd811c74.png)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![](img/454a681c-4042-41be-9c43-6d9afd811c74.png)'
- en: 'Figure 6.37: Output of the example King - man + woman = queen'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.37：示例输出 King - man + woman = queen
- en: 'Now, if you want to train the model from scratch by using data that is provided
    by Google, then download the training dataset by using the following link: [https://code.google.com/archive/p/Word2vec/](https://code.google.com/archive/p/Word2vec/).
    Go to the section entitled *Where to obtain the training data* and download all
    training datasets, then, by taking a reference from the given GitHub link [https://github.com/LasseRegin/gensim-Word2vec-model](https://github.com/LasseRegin/gensim-Word2vec-model),
    you can replicate the whole training process, but it will take a lot of time because
    this kind of training needs a lot of computational power.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你想使用 Google 提供的数据从头开始训练模型，可以通过以下链接下载训练数据集：[https://code.google.com/archive/p/Word2vec/](https://code.google.com/archive/p/Word2vec/)。进入名为
    *Where to obtain the training data* 的部分并下载所有训练数据集，然后参考给出的 GitHub 链接 [https://github.com/LasseRegin/gensim-Word2vec-model](https://github.com/LasseRegin/gensim-Word2vec-model)，你可以复制整个训练过程，但这将需要很长时间，因为这种训练需要大量的计算能力。
- en: Advantages of word2vec
  id: totrans-427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec 的优点
- en: 'As we have seen, word2vec is a very good technique for generating distributional
    similarity. There are other advantages of it as well, which I''ve listed here:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，word2vec 是一个非常好的生成分布式相似度的技术。它还有其他优点，我在这里列出了：
- en: Word2vec concepts are really easy to understand. They are not so complex that
    you really don't know what is happening behind the scenes.
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec 的概念非常容易理解。它们并不复杂，以至于你根本不知道背后发生了什么。
- en: Using word2vec is simple and it has very powerful architecture. It is fast to
    train compared to other techniques.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 word2vec 很简单，它有非常强大的架构。与其他技术相比，它的训练速度较快。
- en: Human effort for training is really minimal because, here, human tagged data
    is not needed.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练过程中的人力投入非常少，因为这里不需要人工标注的数据。
- en: This technique works for both a small amount of datasets and a large amount
    of datasets. So it is an easy-to-scale model.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该技术适用于小规模数据集和大规模数据集。因此，它是一个易于扩展的模型。
- en: Once you understand the concept and algorithms, then you can replicate the whole
    concept and algorithms on your dataset as well.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦你理解了概念和算法，你也可以将整个概念和算法应用到你的数据集上。
- en: It does exceptionally well on capturing semantic similarity.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在捕捉语义相似性方面表现得非常出色。
- en: As this is a kind of unsupervised approach, human effort is very minimal, so
    it is a time-saving method.
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于这是一种无监督方法，人力投入非常少，因此它是一种节省时间的方法。
- en: Challenges of word2vec
  id: totrans-436
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec 的挑战
- en: 'Although the word2vec concept is very efficient, there are some points that
    you may find complex or challenging. Here, I will propose the most common challenges.
    Those points are listed as follows:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 word2vec 概念非常高效，但也有一些你可能会觉得复杂或具有挑战性的地方。在这里，我将提出一些最常见的挑战。这些挑战列举如下：
- en: The word2vec model is easy to develop, but difficult to debug, so debug ability
    is one of the major challenges when you are developing a word2vec model for your
    dataset.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: word2vec 模型容易开发，但调试起来比较困难，因此调试能力是你在为数据集开发 word2vec 模型时面临的主要挑战之一。
- en: It does not handle ambiguities. So, if a word has multiple meanings, and in
    the real world we can find many of these kinds of words, then in that case, embedding
    will reflect the average of these senses in vector space.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它无法处理歧义。因此，如果一个词有多个含义，在现实世界中我们可以找到很多这种词汇，那么在这种情况下，嵌入将会在向量空间中反映这些含义的平均值。
- en: How is word2vec used in real-life applications?
  id: totrans-440
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec 如何在现实应用中使用？
- en: This section will give you an idea of which kinds of NLP applications use word2vec
    and how NLP applications use this concept. Apart from that, I will also discuss
    some of the most frequently-asked questions across the community in order for
    you to have a clear insight of word2vec when you try it out in real life.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将让你了解哪些类型的 NLP 应用程序使用 word2vec，以及 NLP 应用程序如何使用这一概念。除此之外，我还将讨论一些社区中最常见的问题，帮助你在实际使用
    word2vec 时有更清晰的认识。
- en: NLP applications such as document classification, sentiment analysis, and so
    on can use word2vec techniques. Especially in document classification, word2vec
    implementation gives you more good results, as it preserves semantic similarity.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）应用程序，如文档分类、情感分析等，都可以使用 word2vec 技术。特别是在文档分类中，word2vec 实现能为你提供更好的结果，因为它保持了语义相似性。
- en: For sentiment analysis, we can apply word2vec, which gives you an idea about
    how words are spread across the dataset, and then you can use customized parameters
    such as context window size, subsampling, and so on. You should first generate
    **bag of words** (**BOW**) and then start to train word2vec on that BOW and generate
    word vectors. These vectors can be fed as input features for the ML algorithm,
    then generate sentiment analysis output.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 对于情感分析，我们可以应用 word2vec，它能让你了解单词如何分布在数据集中，然后你可以使用自定义参数，如上下文窗口大小、子抽样等。你应该首先生成
    **词袋（BOW）**，然后开始在该词袋上训练 word2vec，生成词向量。这些向量可以作为输入特征输入到机器学习算法中，从而生成情感分析的输出。
- en: Now, it's time to discuss some of the questions that people usually ask when
    they are trying to understand and use word2vec techniques on their own dataset.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，讨论一下人们在尝试理解和使用 word2vec 技术处理自己数据集时常常提出的一些问题。
- en: Now, let's fire up the questions!
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始回答问题吧！
- en: '**What kind of corpus do we need?**: Word2vec techniques can be applied on
    text datasets. As such, there is not any specific kind of text data that you cannot
    use. So, as per my view, you can apply word2vec on any dataset.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我们需要什么样的语料库？**：word2vec 技术可以应用于文本数据集。因此，并没有什么特定的文本数据类型不能使用。因此，按照我的看法，你可以在任何数据集上应用
    word2vec。'
- en: '**Should I always remove stop words?**: In original models of word2vec that
    were from Google, remove some of the stop words, such as **a** has been removed
    in word2vec, but the word **the** has not been removed. So it is not mandatory
    that you remove the words:'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我应该始终去除停用词吗？**：在谷歌的原始 word2vec 模型中，去除了一些停用词，例如 **a** 被从 word2vec 中移除，但单词
    **the** 没有被移除。因此，并不是强制要求你去除这些单词：'
- en: It is totally dependent on your NLP application. If you are developing a sentiment
    analysis application, then you can remove all stop words, but if you are developing
    machine translation applications, then you should remove some of the stop words;
    not all. If you are using word2vec for developing word clusters to understand
    the grammar of the language, then you should not remove any of the words.
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这完全取决于你的自然语言处理（NLP）应用。如果你正在开发情感分析应用，那么你可以去除所有停用词，但如果你在开发机器翻译应用，那么你应该去除一些停用词，而不是全部。如果你使用
    word2vec 来开发词语聚类以理解语言的语法，那么你不应去除任何词语。
- en: '**Should I remove all stop words?**: This question is related to the previous
    question. The straightforward answer to this question is no. It is not compulsory
    that you should remove all stop words blindly for every NLP application. Each
    and every NLP application is different, so you should take a decision based on
    the NLP application that you are trying to build:'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**我应该去除所有停用词吗？**：这个问题与前一个问题相关。对此问题的直接回答是否定的。并不是每个 NLP 应用都强制要求盲目去除所有停用词。每个 NLP
    应用都是不同的，因此你应该根据你正在构建的 NLP 应用来做决定：'
- en: If you look at the Google original word2vec model, then you will see that in
    that model the word **a** is not there, which means a vector that represents the
    word **a** is not present, but a vector for the word **the** is there.
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你查看谷歌的原始 word2vec 模型，你会看到在该模型中，单词 **a** 不在其中，这意味着表示单词 **a** 的向量不存在，但表示单词 **the**
    的向量存在。
- en: We will load the original Google word2vec model and, using simple lines of code,
    we will look at some of the facts regarding stop words.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将加载原始的 Google word2vec 模型，并通过简单的代码行，查看一些关于停用词的事实。
- en: 'Refer to the code snippet in *Figure 6.38*:'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参考 *图 6.38* 中的代码片段：
- en: '![](img/ca5ab527-617d-4bd8-bd77-86f99f20ae63.png)'
  id: totrans-453
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca5ab527-617d-4bd8-bd77-86f99f20ae63.png)'
- en: 'Figure 6.38: Code snippet that shows the fact of stop words'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.38：展示停用词事实的代码片段
- en: 'For the output that is the vector value of `the`, refer to *Figure 6.39*:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `the` 的向量值输出，请参考 *图 6.39*：
- en: '![](img/dc1bd72a-bcd4-4373-9e52-65f0c300ced5.png)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc1bd72a-bcd4-4373-9e52-65f0c300ced5.png)'
- en: 'Figure 6.39: Sample values of word vector for the word the'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.39：单词 “the”的词向量示例值
- en: 'See the output, where you can see that word2vec doesn''t contain `a` in its
    vocabulary in *Figure 6.40*:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 查看输出，你可以看到 word2vec 的词汇表中不包含 `a`，如 *图 6.40* 所示：
- en: '![](img/ae68391b-9340-46cf-bdd9-0a5fd97f9a03.png)'
  id: totrans-459
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae68391b-9340-46cf-bdd9-0a5fd97f9a03.png)'
- en: 'Figure 6.40: Word2vec doesn''t contain the word a'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.40：word2vec 不包含单词 a
- en: '**Don''t you think that, here, we have generated two vectors for each word?**:
    I would like to let you know that we have generated two vectors for each word.
    The reason behind this is that word in a sentence is coming on a target word as
    well as a context word, so when a word appears as a target word, we will generate
    vectors, and when a word appears as a context word, then we also generate vectors.
    We consider the target word vector in our final output, but yes, you can use both
    vectors. How to use the two vectors and generate making sense out of that is kind
    of a million dollar question!'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**你不觉得我们在这里为每个单词生成了两个向量吗？**：我想告诉你，我们确实为每个单词生成了两个向量。这样做的原因是，句子中的单词既是目标单词也是上下文单词，所以当一个单词作为目标单词出现时，我们会生成向量；当一个单词作为上下文单词出现时，我们也会生成向量。我们在最终输出中考虑目标单词的向量，但当然，你也可以使用这两个向量。如何使用这两个向量并从中产生有意义的结果，这真的是个百万美元的问题！'
- en: When should you use word2vec?
  id: totrans-462
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你应该在什么情况下使用 word2vec？
- en: Word2vec captures semantic similarity; this is the most important point that
    we need to keep in mind when we are processing the answer to the preceding question.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 捕捉语义相似性；这是我们在处理前一个问题的答案时需要牢记的最重要的一点。
- en: If you have an NLP application in which you want to use the distributional semantic,
    then word2vec is for you! Some NLP applications will use this concept to generate
    the features and the output vector from the word2vec model, or similarly, vectors
    will be used as input features for the ML algorithm.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个 NLP 应用，想要使用分布式语义，那么 word2vec 就是为你准备的！一些 NLP 应用将使用这个概念从 word2vec 模型生成特征和输出向量，或者类似地，向量将作为
    ML 算法的输入特征使用。
- en: You should know which NLP applications can use word2vec. If you know the list
    of applications, it becomes easy for you to decide whether you should use it or
    not. Suppose you can use k-mean clustering for document classification; if you
    want document classification to carry some of the attributes of semantics, then
    you can use word2vec as well. If you want to build a question-answer system, then
    you will need techniques that differentiate questions on a semantic level. As
    we need some semantic level information, we can use word2vec.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该知道哪些 NLP 应用可以使用 word2vec。如果你了解应用列表，那么你就可以轻松决定是否应该使用它。例如，你可以使用 k-均值聚类进行文档分类；如果你希望文档分类能够携带一些语义属性，那么你也可以使用
    word2vec。如果你想构建一个问答系统，那么你需要能够在语义层面区分问题的技术。由于我们需要一些语义层次的信息，我们可以使用 word2vec。
- en: Now, we have seen enough about the concepts and theories, so we will begin our
    favorite part, which is coding, and this time it is really fun.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经了解了足够的概念和理论，所以我们将开始我们最喜欢的部分——编程，而这次真的很有趣。
- en: Developing something interesting
  id: totrans-467
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发一些有趣的东西
- en: Here, we are going to train our word2vec model. The dataset that I'm going to
    use is text data of *Game of Thrones*. So, our formal goal is to develop word2vec
    to explore semantic similarities between the entities of *A Song of Ice and Fire
    (from the show Game of Thrones)*. The good part is we are also doing visualization
    on top of that, to get a better understanding of the concept practically. The
    original code credit goes to Yuriy Guts. I have just created a code wrapper for
    better understanding.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将训练我们的 word2vec 模型。我将使用的数据集是 *权力的游戏* 的文本数据。因此，我们的正式目标是开发 word2vec 来探索
    *冰与火之歌（来自《权力的游戏》）* 中实体之间的语义相似性。好的部分是，我们还会在此基础上进行可视化，以更好地从实际操作中理解这个概念。原始代码的作者是
    Yuriy Guts。我只是创建了一个代码封装器以便更好地理解。
- en: 'I have used IPython notebook. Basic dependencies are `gensim`, `scikit-learn`,
    and `nltk` to train the word2vec model on the text data of Game of Thrones. You
    can find the code on this GitHub link:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了 IPython notebook。基本的依赖包有 `gensim`、`scikit-learn` 和 `nltk`，用于在《权力的游戏》的文本数据上训练
    word2vec 模型。你可以在这个 GitHub 链接上找到代码：
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch6/gameofthrones2vec/gameofthrones2vec.ipynb.](https://github.com/jalajthanaki/NLPython/blob/master/ch6/gameofthrones2vec/gameofthrones2vec.ipynb)'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/blob/master/ch6/gameofthrones2vec/gameofthrones2vec.ipynb.](https://github.com/jalajthanaki/NLPython/blob/master/ch6/gameofthrones2vec/gameofthrones2vec.ipynb)'
- en: The code contains inline comments and you can see the snippet of the output.
    We have used the t-SNE technique to reduce the dimensions of the word vector,
    so we can use the two-dimensional vector for visualization. The t-SNE technique
    takes a lot of time if you want to run on a normal computer with 2 to 4 GB RAM.
    So, you need more RAM to run t-SNE code successfully at your end and you can skip
    the visualization part if you have memory constraints. You can see the visualization
    images. Once you have saved the model on to disk, you can use it and generate
    output easily. I have given sample output in *Figures 6.41* to *Figure 6.45*.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 代码包含内联注释，你可以看到输出的代码片段。我们使用了 t-SNE 技术来减少词向量的维度，从而可以使用二维向量进行可视化。如果你想在内存为 2 到 4
    GB 的普通计算机上运行 t-SNE，可能会花费较长时间。因此，为了成功运行 t-SNE 代码，你需要更多的内存，并且如果你的计算机内存有限，你可以跳过可视化部分。你可以查看可视化图像。一旦你将模型保存到磁盘上，就可以轻松使用它并生成输出。我已经在*图
    6.41*到*图 6.45*中给出了示例输出。
- en: 'You may observe the output for the word `Stark`here:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以观察到这个词`Stark`的输出结果：
- en: '![](img/05b7c3fb-7cec-436a-a67d-d7dda024dec2.png)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05b7c3fb-7cec-436a-a67d-d7dda024dec2.png)'
- en: 'Figure 6.41: Word similarity output for the word stark'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.41：词汇相似度输出（对于单词 stark）
- en: 'Here is the output for the nearest words:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最近的词汇输出：
- en: '![](img/96971b2d-b210-4ef6-81c4-b1bc30f7c0f8.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96971b2d-b210-4ef6-81c4-b1bc30f7c0f8.png)'
- en: 'Figure 6.42: Output for the nearest words'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.42：最近词汇的输出
- en: 'Now we will see the following figures for output of visualization:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将查看以下可视化输出图像：
- en: '![](img/18c6afd3-33d5-4c41-a2eb-0c1430939c6b.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18c6afd3-33d5-4c41-a2eb-0c1430939c6b.png)'
- en: 'Figure 6.43: After using t-SNE you can visualize vectors in 2-D space'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.43：使用 t-SNE 后，你可以在二维空间中可视化向量
- en: Now we will zoom in and try to see which words have ended up together.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将放大并尝试查看哪些词汇最终聚集在一起。
- en: 'See the following figure, which shows people related to Kingsguard ending up
    together:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下图像，展示与 Kingsguard 相关的人物最终聚集在一起：
- en: '![](img/df02963d-e682-4191-9622-d0140e0dce9e.png)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df02963d-e682-4191-9622-d0140e0dce9e.png)'
- en: 'Figure 6.44: People names grouped together'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.44：人名分组在一起
- en: 'See the following figure, which shows food products grouped together nicely:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下图像，展示了食品产品被很好地分组在一起：
- en: '![](img/9d64d433-19ce-4ed0-8cb7-62fbc3b423d9.png)'
  id: totrans-486
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d64d433-19ce-4ed0-8cb7-62fbc3b423d9.png)'
- en: 'Figure 6.45: Name of food items grouped together'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.45：食品名称分组在一起
- en: Exercise
  id: totrans-488
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'I''m a big fan of Harry Potter, and so, in this exercise, you need to generate
    Word2vec from the text data from a Harry Potter book. Don''t worry about the dataset;
    I have already provided it for you and it resides on this GitHub link: [https://github.com/jalajthanaki/NLPython/tree/master/ch6/Harrypotter2vec/HPdataset](https://github.com/jalajthanaki/NLPython/tree/master/ch6/Harrypotter2vec/HPdataset)'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 我是哈利·波特的超级粉丝，因此在这个练习中，你需要从《哈利·波特》书中的文本数据生成 Word2vec。别担心数据集；我已经为你提供了，且它存储在这个
    GitHub 链接：[https://github.com/jalajthanaki/NLPython/tree/master/ch6/Harrypotter2vec/HPdataset](https://github.com/jalajthanaki/NLPython/tree/master/ch6/Harrypotter2vec/HPdataset)
- en: Good luck with generating HarryPotter2Vec! Happy Coding!
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 祝你好运，生成 HarryPotter2Vec！编码愉快！
- en: Extension of the word2vec concept
  id: totrans-491
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: word2vec 概念的扩展
- en: The word2vec concept can be extended to different levels of text. This concept
    can be applied on the paragraph level or on the document level, and apart from
    this, you can also generate the global vector, which is called **GloVe**. We will
    try to understand them. Here, we are going to get an overview of each of the concepts.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec 概念可以扩展到不同层次的文本。这一概念可以应用于段落级别或文档级别，除此之外，你还可以生成全球向量，这就是**GloVe**。我们将尝试理解这些概念。在这里，我们将对每个概念进行概述。
- en: 'Here are the following extended concepts built by using the word2vec concept:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用 word2vec 概念构建的扩展概念：
- en: Para2vec
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Para2vec
- en: Doc2vec
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doc2vec
- en: GloVe
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GloVe
- en: Para2Vec
  id: totrans-497
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Para2Vec
- en: Para2vec stands for paragraph vector. The paragraph vector is an unsupervised
    algorithm that uses fixed-length feature representation. It derives this feature
    representation from variable-length pieces of texts such as sentences, paragraphs,
    and documents.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: Para2vec 代表段落向量。段落向量是一种无监督算法，使用固定长度的特征表示。它通过从可变长度的文本片段（如句子、段落和文档）中派生出这种特征表示。
- en: Para2vec can be derived by using the neural network. Most of the aspects are
    the same as Word2vec. Usually, three context words are considered and fed into
    the neural network. The neural network then tries to predict the fourth context
    word. Here, we are trying to maximize the log probability and the prediction task
    is typically performed via a multi-class classifier. The function we use is softmax.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: Para2vec 可以通过使用神经网络推导出来。大部分方面与 Word2vec 相同。通常，会考虑三个上下文词并将其输入到神经网络中。然后，神经网络会尝试预测第四个上下文词。在这里，我们尝试最大化对数概率，预测任务通常通过多类分类器来执行。我们使用的函数是
    softmax。
- en: Please note that, here, the contexts are fixed-length and generate the context
    words by using a sliding window over the paragraph. The paragraph vector is shared
    with all contexts generated from the same paragraph, but not across paragraphs.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这里，上下文是固定长度的，通过在段落上使用滑动窗口生成上下文词。段落向量与从同一段落生成的所有上下文共享，但不会跨段落共享。
- en: The advantage of Para2vec is to learn to predict the words from unlabeled data
    so that you can use these techniques when you don't have enough labeled datasets.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: Para2vec 的优势在于能够从未标记的数据中学习预测单词，这样你就可以在没有足够标记数据集的情况下使用这些技术。
- en: Doc2Vec
  id: totrans-502
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Doc2Vec
- en: '**Doc2vec** (**Document vectors**) is an extension of word2vec. It learns to
    correlate document labels and words, rather than words with other words. Here,
    you need document tags. You are able to represent an entire sentence using a fixed-length
    vector. This is also using word2vec concepts. If you feed the sentences with labels
    into the neural network, then it performs classification on a given dataset. So,
    in short, you tag your text and then use this tagged dataset as input and apply
    the Doc2vec technique on that given dataset. This algorithm will generate tag
    vectors for the given text. You can find the code at this GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch6/doc2vecexample.py](https://github.com/jalajthanaki/NLPython/blob/master/ch6/doc2vecexample.py)'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '**Doc2vec**（**文档向量**）是 word2vec 的扩展。它学习关联文档标签和单词，而不是单词之间的关系。在这里，你需要文档标签。你能够使用固定长度的向量表示整个句子。这也是使用了
    word2vec 的概念。如果你将带标签的句子输入神经网络，它就会在给定的数据集上执行分类任务。所以，简而言之，你给文本打标签，然后使用这个标记的数据集作为输入，并在这个数据集上应用
    Doc2vec 技术。该算法将为给定文本生成标签向量。你可以在这个 GitHub 链接中找到代码：[https://github.com/jalajthanaki/NLPython/blob/master/ch6/doc2vecexample.py](https://github.com/jalajthanaki/NLPython/blob/master/ch6/doc2vecexample.py)'
- en: 'I have used very small datasets to just give you intuition on how you can develop
    Doc2vec, so I''m ignoring the accuracy factor of the developed model. You can
    refer to the code given at this reference link: [https://github.com/jhlau/doc2vec](https://github.com/jhlau/doc2vec)'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了非常小的数据集，仅仅是为了让你对如何开发 Doc2vec 有一个直观的了解，因此我忽略了开发模型的准确性因素。你可以参考这个参考链接中给出的代码：[https://github.com/jhlau/doc2vec](https://github.com/jhlau/doc2vec)
- en: 'Let''s see the intuitive code in *Figure 6.46* and see the output snippet in
    *Figure 6.47*:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 *图 6.46* 中的直观代码，并查看 *图 6.47* 中的输出片段：
- en: '![](img/0277849e-707b-47fd-9eec-ccc13dba224b.png)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0277849e-707b-47fd-9eec-ccc13dba224b.png)'
- en: 'Figure 6.47: Code snippet of doc2vec'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.47：doc2vec 的代码片段
- en: 'You may see the following output:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会看到以下输出：
- en: '![](img/864bb3b8-8e0f-40ff-a903-1c4ce4e0f9c3.png)'
  id: totrans-509
  prefs: []
  type: TYPE_IMG
  zh: '![](img/864bb3b8-8e0f-40ff-a903-1c4ce4e0f9c3.png)'
- en: 'Figure 6.48: Sample output'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.48：示例输出
- en: Applications of Doc2vec
  id: totrans-511
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Doc2vec 的应用
- en: 'Let''s see the applications that can use Doc2vec:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看可以使用 Doc2vec 的应用场景：
- en: Document clustering can be easily implemented by using Doc2vec
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档聚类可以通过使用 Doc2vec 简单实现
- en: We can perform sentiment analysis on larger chunks of text data, and I suppose
    you could consider a very big chunk of text and generate the sentiment output
    for that large chunk
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以对更大块的文本数据进行情感分析，我想你也可以考虑对一个非常大的文本块进行处理，并为这个大块生成情感输出
- en: It is also used in product recommendation
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它还用于产品推荐
- en: GloVe
  id: totrans-516
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GloVe
- en: GloVe stands for global vector. GloVe is an unsupervised learning algorithm.
    This algorithm generates vector representations for words. Here, training is performed
    by using an aggregated global word-word co-occurrence matrix and other statistics
    from a corpus, and the resulting representations give you interesting linear substructures
    of the word vector space. So the co-occurrence matrix is the input of GloVe.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe 代表全局向量。GloVe 是一种无监督学习算法。这个算法生成单词的向量表示。在这里，训练是通过使用聚合的全局单词-单词共现矩阵和语料库中的其他统计信息来执行的，结果表示给你提供了单词向量空间中有趣的线性子结构。所以，共现矩阵是
    GloVe 的输入。
- en: 'GloVe uses cosine similarity or the Euclidean distance to get an idea of similar
    words. Glove gives you fresh aspects and proves that if you take the nearest neighbor,
    then you can see such kinds of words that are very rare in terms of their frequent
    usage. GloVe can still capture those rare words in similar clusters. Let''s look
    at the a famous example:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: GloVe 使用余弦相似度或欧氏距离来估计相似的单词。GloVe 给你带来新颖的视角，并证明了如果你取最近邻，你可以看到这些单词，它们在频繁使用上是非常罕见的。即使如此，GloVe
    仍然能够将这些罕见的单词捕捉到相似的聚类中。让我们看一个著名的例子：
- en: 'For example, here are the closest words when we have the target word frog:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当我们使用目标单词“青蛙”时，以下是最接近的单词：
- en: Frog
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 青蛙
- en: Frogs
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 青蛙
- en: Toad
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蟾蜍
- en: Litoria
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Litoria
- en: Leptodactylidae
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leptodactylidae
- en: Rana
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rana
- en: Lizard
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蜥蜴
- en: Eleutherodactylus
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eleutherodactylus
- en: 'Another example is the words related to the comparative-superlative form clustered
    together, and you can see the following output if you use the visualization tool.
    See *Figure 6.48*:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是与比较级和最高级形式相关的单词聚集在一起，如果你使用可视化工具，可以看到以下输出。参见*图 6.48*：
- en: '![](img/34816d1b-7ecb-4a29-af52-0e6d17b67f4c.png)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34816d1b-7ecb-4a29-af52-0e6d17b67f4c.png)'
- en: 'Figure 6.48: Result of GloVe famous example'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.48：GloVe 著名示例的结果
- en: 'I''m using the `GloVe` Python library to give you an intuitive practical example
    of GloVe. See the code given at the following GitHub link: [https://github.com/jalajthanaki/NLPython/blob/master/ch6/gloveexample.py](https://github.com/jalajthanaki/NLPython/blob/master/ch6/gloveexample.py)'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用`GloVe` Python 库为你提供一个直观的 GloVe 实际例子。请参见以下 GitHub 链接中的代码：[https://github.com/jalajthanaki/NLPython/blob/master/ch6/gloveexample.py](https://github.com/jalajthanaki/NLPython/blob/master/ch6/gloveexample.py)
- en: 'Before we start, we need to download the dataset, so execute the following
    command:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，需要下载数据集，请执行以下命令：
- en: '[PRE2]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can see the snippet of code in *Figure 6.49* and see the output in *Figure
    6.50*:'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图 6.49*中看到代码片段，并在*图 6.50*中看到输出：
- en: '![](img/b5789625-456b-47e2-82bc-ae3d9990f95b.png)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5789625-456b-47e2-82bc-ae3d9990f95b.png)'
- en: 'Figure 6.49: GloVe code snippet'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.49：GloVe 代码片段
- en: 'Following is the output of the preceding code snippet:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码片段的输出：
- en: '![](img/41328e38-e1ee-47b4-98af-9d1b828a1494.png)'
  id: totrans-538
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41328e38-e1ee-47b4-98af-9d1b828a1494.png)'
- en: 'Figure 6.50: Sample output of GloVe'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.50：GloVe 的示例输出
- en: Exercise
  id: totrans-540
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: This exercise is more of a reading exercise for you. You should read the research
    papers on Para2vec, Doc2vec, and GloVe. Apart from this, you can also check whether
    there is any way that you can find vector representation for continuous strings,
    such as a DNA pattern. The main purpose of this exercise is to give you an idea
    of how research work has been done. You can also think of some other aspects of
    vector representation and try to solve the challenges.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习更像是一个阅读练习。你应该阅读关于 Para2vec、Doc2vec 和 GloVe 的研究论文。除此之外，你还可以检查是否有任何方法可以为连续字符串（例如
    DNA 模式）找到向量表示。这个练习的主要目的是让你了解研究工作是如何完成的。你也可以思考一些向量表示的其他方面，并尝试解决其中的挑战。
- en: Importance of vectorization in deep learning
  id: totrans-542
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化在深度学习中的重要性
- en: This is more of a discussion with you from my end. As we all know, computers
    can't understand NL directly, so we need to convert our NL output into numerical
    format. We have various word embedding techniques, as well as some basic statistical
    techniques such as indexing, tf-idf, one-hot encoding, and so on. By using all
    these techniques, or some of these techniques, you can convert your text input
    into numerical format. Which techniques you choose totally depends on the NLP
    applications. So, there are two major points behind why we convert NL input to
    numerical format. It is basically done because the computer can only understand
    numerical data, so we have to convert text data to numerical data and computers
    are very good at performing computation on given numerical data. These are two
    major points that come to my mind when we are converting text data.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 这更像是我与你的一次讨论。众所周知，计算机无法直接理解自然语言（NL），因此我们需要将自然语言输出转换为数值格式。我们有多种词嵌入技术，以及一些基本的统计技术，如索引、tf-idf、独热编码等。通过使用所有这些技术或其中的一些技术，你可以将文本输入转换为数值格式。你选择哪些技术完全取决于自然语言处理（NLP）应用。因此，转换自然语言输入为数值格式的背后有两个主要原因。基本上，这是因为计算机只能理解数值数据，所以我们必须将文本数据转换为数值数据，而计算机擅长对给定的数值数据进行计算。这是我在转换文本数据时想到的两个主要原因。
- en: Let's understand what deep learning is. Here, I want to give you just a brief
    idea about it. Don't worry; we will see more detail in [Chapter 9](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml),
    *Deep Learning for NLU and NLG Problems.* When a neural network is many layers
    deep, it is called **deep neural network**. When we use many-layered deep neural
    networks and use them to develop NLP applications using lots of data and lots
    of computation power, it is called **deep learning**.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解一下什么是深度学习。在这里，我想给你一个简要的概念。不用担心；我们将在[第 9 章](f414d38e-b88e-4239-88bd-2d90e5ce67ab.xhtml)《用于
    NLU 和 NLG 问题的深度学习》中详细讨论。当一个神经网络有很多层时，我们称之为**深度神经网络**。当我们使用多层深度神经网络，并利用它们通过大量数据和计算能力开发
    NLP 应用时，这就是**深度学习**。
- en: Now let's talk about vectorization. Vectorization is a solid mathematical concept
    and it is easy to understand and deal with. Nowadays, Python has a lot of good
    libraries that make our life easier when we want to deal with high dimensional
    vector forms of data. The deep learning paradigm heavily relies on the vectorization
    and matrices concepts, so in order to get a good grip on deep learning, you should
    have knowledge of vectors and matrices. Deep learning applications that deal with
    input data such as video or audio also use vectors. Videos and images are converted
    into the dense vector format, and when talk about text input, word2vec is its
    basic building block for generating vectors from words. Google TensorFlow uses
    word2vec as their basic building block and it uses these concepts and improvises
    the results of Google Machine translation, Google Speech recognition, and Google
    Vision applications. So, vectors and matrices give us a lot of freedom in terms
    of their processing and making sense out of it.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来谈谈向量化。向量化是一个坚实的数学概念，易于理解和处理。如今，Python 有很多优秀的库，当我们想处理高维度向量形式的数据时，它们让我们的生活变得更加轻松。深度学习范式在很大程度上依赖于向量和矩阵的概念，所以为了深入掌握深度学习，你应该了解向量和矩阵。处理视频或音频等输入数据的深度学习应用程序也使用向量。视频和图像被转换为稠密的向量格式，而当我们谈论文本输入时，word2vec
    是生成单词向量的基本构建块。Google TensorFlow 将 word2vec 作为其基本构建块，并利用这些概念改进了 Google 机器翻译、Google
    语音识别和 Google 视觉应用程序的效果。因此，向量和矩阵在处理和理解这些数据时，给了我们极大的自由度。
- en: Apart from this, I also need to give you some thoughts. I want you to focus
    on how we can improvise the way we deal with text. No doubt word2vec is one of
    the most simple and efficient approaches for converting words into vector form,
    but I would definitely encourage my readers who are interested in research work
    to extend this concept for their native languages or become creative and contribute
    to building very innovative techniques that will help the NLP community to overcome
    challenges such as word ambiguities. Well, these are all my thoughts for you!
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，我还想给你一些思考。我希望你关注我们如何改进处理文本的方式。毫无疑问，word2vec 是将单词转换为向量形式最简单且高效的方法之一，但我绝对鼓励那些对研究工作感兴趣的读者，扩展这一概念到他们的母语，或发挥创意，贡献出创新的技术，帮助
    NLP 社区克服诸如词义歧义等挑战。嗯，这些就是我给你的所有思考！
- en: Summary
  id: totrans-547
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we have seen how word2vec can be used to find semantics. The
    simple vectorization techniques help us a lot. We have seen some of the applications
    of it. We have touched upon the technicalities of the word2vec model. I have introduced
    lots of new mathematical, as well as statistical, terms to you in order to give
    you a better understanding of the model. We have converted the word2vec black
    box into the word2vec white box. I have also implemented basic as well as extended
    examples for better understanding. We have used a ton of libraries and APIs to
    develop word2vec models. We have also seen the advantages of having vectorization
    in deep learning. Then, we extended our word2vec understanding and developed the
    concepts of para2vec, doc2vec, and GloVe.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经了解了如何使用 word2vec 来发现语义。简单的向量化技术对我们帮助很大。我们已经看到了它的一些应用。我们也触及了 word2vec
    模型的技术细节。为了让你更好地理解这个模型，我向你介绍了许多新的数学和统计学术语。我们将 word2vec 的黑盒转换成了 word2vec 的白盒。我还实现了基础和扩展的示例，以便更好地理解。我们使用了大量的库和
    API 来开发 word2vec 模型。我们还看到了在深度学习中使用向量化的优势。然后，我们扩展了 word2vec 的理解，发展出了 para2vec、doc2vec
    和 GloVe 的概念。
- en: The next chapter will basically give you an in-depth idea of how rule-based
    techniques are used in order to develop NLP applications and how various NLP applications
    use a very simple, but yet very effective, technique called rules or logic for
    developing basic and effective prototypes for NLP applications. Google use the
    rule-based techniques for their machine translation projects, Apple also use this
    technique, and last but not least, Google used the rule-based system to make an
    early prototype of their self-driving car. We will discuss the rule-based system
    and its architecture. We will also see what the architecture of rule-based NLP
    applications is. I will provide you with a thought process, and by using that
    thought process, you can also make rules for your NLP application. We will implement
    the basic grammar rules and pattern-based rules. We will also develop a basic
    template-based chatbot from scratch.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将基本介绍规则驱动技术如何用于开发自然语言处理（NLP）应用，以及各种NLP应用如何使用一种简单却非常有效的技术，称为规则或逻辑，来开发基本且有效的原型。Google在其机器翻译项目中使用了规则驱动技术，Apple也采用了这种技术，最后但同样重要的是，Google还使用规则驱动系统来制作他们自动驾驶汽车的早期原型。我们将讨论规则驱动系统及其架构。我们还将了解规则驱动的NLP应用架构是什么。我会为你提供一种思维方式，通过使用这种思维方式，你也可以为你的NLP应用制定规则。我们将实现基本的语法规则和基于模式的规则。我们还将从零开始开发一个基于模板的聊天机器人。
