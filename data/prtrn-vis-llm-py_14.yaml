- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: MLOps for Vision and Language
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLOps在视觉和语言中的应用
- en: In this chapter, we’ll introduce the core concepts of operations and orchestration
    for machine learning, also known as MLOps. This includes building pipelines, continuous
    integration and deployment, promotion through environments, and more. We’ll explore
    options for monitoring and human-in-the-loop auditing of model predictions. We’ll
    also identify unique ways to support large vision and language models in your
    MLOps pipelines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍机器学习操作与协调的核心概念，也称为MLOps。这包括构建管道、持续集成与部署、环境间的推广等内容。我们将探讨如何监控模型预测并进行人工干预审计。我们还将寻找独特的方式，以支持在MLOps管道中使用大型视觉和语言模型。
- en: 'We’ll be covering the following topics in the chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What is MLOps?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是MLOps？
- en: Continuous integration and continuous deployment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续集成与持续部署
- en: Model monitoring and human-in-the-loop
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型监控与人工干预
- en: MLOps for foundation models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础模型的MLOps
- en: AWS offerings for MLOps
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS提供的MLOps服务
- en: What is MLOps?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是MLOps？
- en: We’ve covered such a huge amount of content in this book that it’s almost inconceivable.
    From the absolute foundations of pretraining, we’ve worked through use cases,
    datasets, models, GPU optimizations, distribution basis, optimizations, hyperparameters,
    working with SageMaker, fine-tuning, bias detection and mitigation, hosting your
    model, and prompt engineering. Now, we come to the art and science of *tying it*
    *all together*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们已经涵盖了如此庞大的内容，几乎令人难以想象。从预训练的绝对基础开始，我们一步步深入探讨了用例、数据集、模型、GPU优化、分布式基础、优化、超参数、与SageMaker的配合、微调、偏差检测与缓解、托管模型及提示工程等内容。现在，我们将进入将*所有内容*
    *结合在一起*的艺术与科学。
- en: '**MLOps** stands for **machine learning operations**. Broadly speaking, it
    includes a whole set of technologies, people, and processes that your organization
    can adopt to streamline your machine learning workflows. In the last few chapters,
    you learned about building RESTful APIs to host your model, along with tips to
    improve your prompt engineering. Here, we’ll focus on *building a deployment workflow*
    to integrate this model into your application.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLOps**代表**机器学习操作**。广义来说，它包括一整套技术、人员和流程，供您的组织采纳，以简化机器学习工作流。在前几章中，您学习了如何构建RESTful
    API来托管模型，并且获得了改进提示工程的技巧。在这里，我们将重点讨论*构建部署工作流*，以将该模型集成到您的应用程序中。'
- en: Personally, I find the pipeline aspect of MLOps the most poignant. A **pipeline**
    is a set of steps you can build to orchestrate your machine learning workflow.
    This can include everything from automatically retraining your model, hyperparameter
    tuning, auditing, and monitoring, application testing and integration, and promotion
    to more secure environments, drift and bias detection, and adversarial hardening.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 就个人而言，我认为MLOps中的管道（**pipeline**）部分最为关键。**管道**是一个由多个步骤组成的流程，用于协调机器学习工作流。这可以包括自动重新训练模型、超参数调整、审计与监控、应用测试与集成、推广到更安全的环境、漂移与偏差检测，以及对抗性强化等内容。
- en: '![Figure 14.1 – Pipelines for machine learning operations](img/B18942_Figure_14_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.1 – 机器学习操作的管道](img/B18942_Figure_14_01.jpg)'
- en: Figure 14.1 – Pipelines for machine learning operations
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1 – 机器学习操作的管道
- en: Pipelines are tools you can build using any number of software options. If you’re
    using SageMaker-native tooling and you don’t already have an orchestration stack,
    you might start by looking at **SageMaker Pipelines**. Alternatively, if you’re
    already using an orchestration stack, such as AirFlow, KubeFlow, Ray, MLFlow,
    or StepFunctions, you might continue using those and simply point to SageMaker
    APIs for your machine learning workflows.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 管道是您可以使用多种软件工具构建的工具。如果您使用的是SageMaker本地工具，并且还没有一个协调栈，您可以从查看**SageMaker管道**开始。或者，如果您已经在使用某个协调栈，例如AirFlow、KubeFlow、Ray、MLFlow或StepFunctions，您也可以继续使用它们，并简单地调用SageMaker
    API来处理您的机器学习工作流。
- en: The core component of a pipeline is a *step*. A step might be something such
    as **data preprocessing**, **model training**, **model evaluation**, a **manual
    review**, **model deployment**, and so on. A basic pipeline will flow through
    a number of steps that you define. Pipelines usually start with a **trigger**,
    some event that delivers a notification system to the pipeline. Your trigger could
    be an upload to S3, a commit to your repository, a time of day, an update on your
    dataset, or a customer event. Usually, you’ll see one trigger kicking off the
    entire pipeline, with each step initiated after the previous one completes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线的核心组件是*步骤*。一个步骤可能是**数据预处理**、**模型训练**、**模型评估**、**人工审核**、**模型部署**等。一个基本的流水线将通过你定义的多个步骤。流水线通常以**触发器**开始，触发器是某个事件，它将通知系统引入流水线。你的触发器可能是上传到
    S3、提交到代码库、某个时间点、数据集更新或客户事件。通常，你会看到一个触发器启动整个流水线，在前一步骤完成后启动每个后续步骤。
- en: Let’s move on to the common MLOps pipelines.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论常见的 MLOps 流水线。
- en: Common MLOps pipelines
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 常见的 MLOps 流水线
- en: 'Let’s examine a few of the most common pipelines in machine learning:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看几个在机器学习中常见的流水线：
- en: '**Model deployment pipeline**: Here, the core task is to point to your pretrained
    model artifacts, notably your inference script and the model itself, and put these
    into whichever deployment option you select. You might use **SageMaker RealTime
    Endpoints** for product recommendation, or **asynchronous endpoints** to host
    large language models. You might have a variety of images through the **multi-container
    endpoint**, or even with the **multi-model endpoint**. In any case, the most basic
    pipeline steps might look something like this:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型部署流水线**：在这里，核心任务是指向你的预训练模型工件，特别是推理脚本和模型本身，并将这些内容放入你选择的任何部署选项中。你可能会使用**SageMaker
    实时端点**进行产品推荐，或者使用**异步端点**来托管大型语言模型。你可能通过**多容器端点**使用不同的镜像，甚至通过**多模型端点**使用不同的模型。无论如何，最基本的流水线步骤可能如下所示：'
- en: Update a model artifact.
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新模型工件。
- en: Create a new endpoint
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新端点。
- en: Test the endpoint.
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试端点。
- en: If the test is successful, set production traffic to the endpoint. If the test
    fails, notify the development team.
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果测试成功，将生产流量指向该端点。如果测试失败，通知开发团队。
- en: '**Model retraining pipeline**: A model retraining pipeline is useful for use
    cases where you need to retrain a model regularly. This might be every time you
    have new data, which can be as often as every few hours, or as irregular as every
    month. For a simple case, such as rerunning a report or notebook, you might use
    SageMaker’s *notebook job* feature, launched in December 2022, to run a notebook
    on a schedule. A pipeline, however, would be useful if you wanted to trigger this
    retraining based on updated data. Alternatively, if the model or dataset were
    large and needed distributed training, a pipeline would be a natural fit. Your
    pipeline steps might look something like this:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型再训练流水线**：模型再训练流水线适用于那些需要定期再训练模型的使用场景。每次有新数据时都需要再训练，可能是每隔几小时，或者每个月一次。对于像重新运行报告或笔记本这样的简单场景，你可以使用
    SageMaker 的*notebook job*功能（自 2022 年 12 月推出）按计划运行笔记本。然而，如果你希望基于更新的数据触发此再训练，则流水线将非常有用。或者，如果模型或数据集很大并且需要分布式训练，流水线将是一个理想的选择。你的流水线步骤可能如下所示：'
- en: Upload new data.
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上传新数据。
- en: Run preprocessing.
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行预处理。
- en: Train the model.
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。
- en: Tune the model.
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整模型。
- en: Trigger the deployment pipeline.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 触发部署流水线。
- en: '**Environment promotion pipeline**: Some customers, particularly in security-sensitive
    settings such as highly regulated industries, require that applications are upgraded
    through increasingly more secure environments. Here, the word *environment* means
    an isolated compute boundary, usually either a full new AWS account or, more simply,
    a different region. The steps for this pipeline might look something like this:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境推广流水线**：某些客户，特别是那些在安全敏感环境中工作的客户（例如高度监管的行业），要求通过越来越安全的环境来升级应用程序。在这里，*环境*一词指的是一个隔离的计算边界，通常是一个全新的
    AWS 账户，或者更简单地说，是一个不同的区域。此流水线的步骤可能如下所示：'
- en: Trigger the pipeline from data scientists in the development account.
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从开发账户中的数据科学家触发流水线。
- en: Promote the resources to a test account.
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将资源提升到测试账户。
- en: Test the endpoint.
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试端点。
- en: If the endpoint passes, promote it to a production account. If the endpoint
    fails, notify the data scientists.
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果端点通过测试，将其提升到生产账户。如果端点未通过测试，通知数据科学家。
- en: In the production account, create the endpoint.
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生产账户中，创建端点。
- en: Set production traffic to the endpoint.
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将生产流量设置到端点。
- en: As you have no doubt noticed, each of these pipelines can interact with each
    other. They can trigger each other as unique steps, interact with other components,
    and continuously add value. Their basic components are also interchangeable –
    you can easily substitute some steps with others, defining whatever overall system
    you need.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你无疑注意到的那样，每个管道都可以相互交互。它们可以作为独立步骤触发彼此，与其他组件交互，并持续添加价值。它们的基本组件也是可互换的 – 你可以轻松替换一些步骤，定义你需要的整体系统。
- en: A concept underlying much of this is **microservices**. You can think of each
    of these pipelines as a microservice, starting with some input and delivering
    an output. To maximize value across teams, you might build and maintain a base
    set of templates for each step, or for entire pipelines, to make it easier for
    future teams to use them.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 支撑其中大部分概念的一个概念是**微服务**。你可以把每个管道都视为一个微服务，从某些输入开始并生成输出。为了最大化跨团队的价值，你可以构建和维护每个步骤或整个管道的基本模板，以便未来的团队更容易使用它们。
- en: As we learned earlier in [*Chapter 12*](B18942_12.xhtml#_idTextAnchor178), *How
    to Deploy Your Model*, there are quite a few techniques you can use to improve
    your model for deployment. This includes quantization and compression, bias detection,
    and adversarial hardening *(1)*. Personally, I tend to see many of the methods
    executed rarely on a model, such as when it first moves from the R&D team to the
    deployment team. For regular retraining, I’d avoid extensive compute resources,
    assuming that much of the basic updates incorporated into the model work in more
    recent versions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第12章*](B18942_12.xhtml#_idTextAnchor178)中学到的那样，*如何部署你的模型*，你可以使用很多技术来改进你的模型以进行部署。这包括量化和压缩、偏差检测和对抗性硬化
    *(1)*。个人而言，我倾向于看到许多方法在模型上执行得很少，比如当它首次从研发团队移交到部署团队时。对于定期的重新训练，我会避免过度的计算资源，假设模型的大部分基本更新已在更近期的版本中工作。
- en: Continuous integration and continuous deployment
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持续集成和持续部署
- en: In machine learning, we tend to look at two somewhat different stacks. On the
    one hand, you have the model creation and deployment process. This includes your
    model artifacts, datasets, metrics, and target deployment options. As we discussed
    previously, you might create a pipeline to automate this. On the other hand, you
    have the actual software application where you want to expose your model. This
    might be a visual search mobile app, a question/answering chat, an image generation
    service, a price forecasting dashboard, or really any other process to improve
    using data and automated decisions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们倾向于看两种略有不同的堆栈。一方面，你有模型创建和部署过程。这包括你的模型工件、数据集、指标和目标部署选项。正如我们之前讨论的那样，你可能创建一个管道来自动化这个过程。另一方面，你有真正的软件应用程序，你想要暴露你的模型。这可能是一个视觉搜索移动应用程序、问答聊天、图像生成服务、价格预测仪表板，或者真正使用数据和自动化决策来改进的任何其他过程。
- en: Many software stacks use their own **continuous integration and continuous deployment**
    (**CI**/**CD**) pipelines to seamlessly connect all the parts of an application.
    This can include integration tests, unit tests, security scans, and machine learning
    tests. **Integration** refers to putting the application together, while **deployment**
    refers to taking steps to move the application into production.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 许多软件堆栈使用他们自己的**持续集成和持续部署**（**CI**/**CD**）管道，无缝连接应用程序的所有部分。这可以包括集成测试、单元测试、安全扫描和机器学习测试。**集成**是指将应用程序组合在一起，而**部署**则是指采取步骤将应用程序投入生产环境。
- en: Many of the pipelines we looked at previously could be considered CD pipelines,
    especially when they refer to updating the service in production. A continuous
    integration pipeline might include steps that point to the application, testing
    a variety of responses, and ensuring that the model responds appropriately. Let’s
    take a closer look.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看过的许多管道可能被认为是CD管道，特别是当它们涉及更新生产中的服务时。持续集成管道可能包括指向应用程序的步骤，测试各种响应，并确保模型作出适当的响应。让我们仔细看看。
- en: '![Figure 14.2 – CI/CD options for machine learning](img/B18942_Figure_14_02.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图14.2 – 机器学习的CI/CD选项](img/B18942_Figure_14_02.jpg)'
- en: Figure 14.2 – CI/CD options for machine learning
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 – 机器学习的CI/CD选项
- en: What I’m trying to convey here is that *you have many options for how to set
    up your pipelines*. For a large-scale foundation model, such as your own pretrained
    LLM or text-to-vision model, you might possibly have handfuls of extremely robust
    repositories that each team develops for different pieces of the puzzle. Integrating
    these, using slices of them to support each other, and automating as much as you
    can with robust unit testing to ensure the highest performance across the board
    is in your best interest. Separately from the model development, you’ll likely
    have a deployment pipeline that checks all the boxes to prepare your model for
    real-time traffic and successful communication with your client application.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里想表达的是，*你有许多选项可以设置你的管道*。对于一个大规模的基础模型，比如你自己的预训练大语言模型或文本到视觉模型，你可能会拥有由不同团队为拼图的不同部分开发的极其强大的多个代码库。将这些代码库整合在一起，使用它们的片段相互支持，并通过强大的单元测试尽可能地自动化，以确保整体的最高性能是符合你最佳利益的。与模型开发分开，你可能还会有一个部署管道，能够检查所有必要条件，为模型准备好实时流量并成功地与客户端应用进行通信。
- en: Now that we’ve covered a few foundational topics in general operations, let’s
    take a closer look at two key aspects that relate especially to machine learning
    – model monitoring and human-in-the-loop.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了一些关于一般运营的基础话题，那么让我们仔细看看与机器学习特别相关的两个关键方面——模型监控和人工介入。
- en: Model monitoring and human-in-the-loop
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型监控和人工介入
- en: In *Chapter 11*, we explored topics around bias detection, mitigation, and monitoring
    for large vision and language models. This was mostly in the context of evaluating
    your model. Now that we’ve made it to the section on deploying your models, with
    an extra focus on operations, let’s take a closer look at model monitoring.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第11章*中，我们探讨了关于偏见检测、缓解和监控的大型视觉与语言模型的相关话题。这些内容主要集中在评估模型的背景下。现在，我们已经进入了关于模型部署的部分，重点是运营，我们来更仔细地看看模型监控。
- en: Once you have a model deployed into any application, it’s extremely useful to
    be able to view the performance of that model over time. This is the case for
    any of the use cases we discussed earlier – chat, general search, forecasting,
    image generation, recommendations, classification, question answering, and so
    on. All of these applications benefit from being able to see how your model is
    trending over time and provide relevant alerts.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你将模型部署到任何应用中，能够查看该模型随时间变化的表现就变得极其有用。这对于我们之前讨论的任何用例都是如此——聊天、通用搜索、预测、图像生成、推荐、分类、问答等等。所有这些应用都从能够看到你的模型随时间变化的趋势并提供相关警报中受益。
- en: Imagine, for example, that you have a price forecasting model that suggests
    a price for a given product based on economic conditions. You train your model
    on certain economic conditions, maybe those in January, and deploy the model in
    February. While deployed, the model continues to look at those same conditions
    and help price your project. However, you may not realize that in March, the entire
    market conditions changed. Things in our world change so rapidly that entire sectors
    may have inverted. Your model came into the world thinking that everything looks
    exactly the same as when it was trained. Unless you recalibrate your model, it
    won’t realize that things are different.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你有一个价格预测模型，它根据经济状况为某个特定产品建议价格。你在某些经济条件下训练模型，也许是1月的经济条件，并在2月部署了该模型。在部署期间，模型继续关注那些相同的条件并帮助定价。然而，你可能没有意识到，3月时整个市场状况发生了变化。我们的世界变化如此迅速，以至于整个行业可能已经发生了颠覆。你的模型进入世界时认为一切看起来和训练时完全一样。除非你重新校准模型，否则它不会意识到情况已经不同。
- en: But how are you supposed to know when to recalibrate your model? Through Model
    Monitor! Using Amazon SageMaker, including our fully managed Model Monitor capabilities,
    you can easily run tests that learn summary statistics of your training data.
    You can then schedule jobs to compare these summaries with the data hitting your
    endpoint. This means that as the new data interacts with your model, you can store
    all of these requests in S3\. After the requests are stored, you can use the model
    monitor service to schedule jobs that compare these inference requests with your
    training data. This is useful because you can use it to send yourself alerts about
    how your model is trending on inference, especially if you need to trigger a retraining
    job. The same basic concepts of Model Monitor should also apply to vision and
    language; the only question is how we generate summary statistics.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，您该如何知道何时重新校准模型呢？通过模型监控！使用Amazon SageMaker，包括我们完全托管的模型监控功能，您可以轻松运行测试，学习训练数据的汇总统计信息。然后，您可以安排作业将这些汇总与传入端点的数据进行比较。这意味着，随着新数据与您的模型互动，您可以将所有这些请求存储在S3中。请求存储之后，您可以使用模型监控服务安排作业，将这些推理请求与您的训练数据进行对比。这很有用，因为您可以利用它向自己发送警报，了解模型在推理上的趋势，尤其是在您需要触发重新训练作业时。模型监控的基本概念同样适用于视觉和语言；唯一的问题是如何生成汇总统计信息。
- en: Now, how does Model Monitor relate to human-in-the-loop? It’s because you can
    also use triggers from your hosted model to *trigger a manual review*. As shown
    in the following figure, you can bring in some software checks to confirm that
    your model outputs content that is mostly in line with your expectations. If not,
    you can trigger a manual review. This uses another option on SageMaker, **Augmented
    Artificial Intelligence** (**A2I**), which in turn relies on SageMaker Ground
    Truth. Put another way, if the model doesn’t act as you expect, you can send the
    prediction request and response to a team for manual review. This helps your teams
    build more trust in the overall solution, not to mention improving your dataset
    for the next iteration of the model! Let’s take a look at this visually.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，模型监控与人为参与有何关系呢？因为您还可以使用来自托管模型的触发器来*触发人工审核*。如下面的图所示，您可以引入一些软件检查，以确认模型输出的内容是否大致符合您的预期。如果不符合，您可以触发人工审核。这使用了SageMaker的另一个选项——**增强型人工智能**（**A2I**），而这个功能又依赖于SageMaker
    Ground Truth。换句话说，如果模型没有按预期工作，您可以将预测请求和响应发送给团队进行人工审核。这有助于您的团队对整体解决方案建立更多信任，更不用说提升您的数据集以便为模型的下一次迭代做准备了！让我们直观地看一下这个过程。
- en: '![Figure 14.3 – Model monitoring with human-in-the-loop](img/B18942_Figure_14_03.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图14.3 – 带有人类参与的模型监控](img/B18942_Figure_14_03.jpg)'
- en: Figure 14.3 – Model monitoring with human-in-the-loop
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3 – 带有人类参与的模型监控
- en: In the preceding figure, you can see a variety of components, or microservices,
    that you can combine to provide a complete pipeline with model monitoring and
    humans kept in the loop. First, your client application can interact with a Lambda
    function that, in turn, invokes a SageMaker model. You might store the model requests
    and responses in an S3 bucket by writing it in Lambda yourself, or you could set
    the SageMaker endpoint to do this for you. Once you have records stored in S3,
    you can run **model monitoring** jobs. This can use a feature of SageMaker, model
    monitor, to learn the statistical differences between your training and inferencing
    data, sending you alerts if these fall out of a large range. Alternatively, you
    could write your own comparison script and run these jobs yourself on SageMaker
    training or processing jobs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，您可以看到各种组件或微服务，您可以将它们组合起来，提供一个完整的管道，同时进行模型监控并保持人为参与。首先，您的客户端应用可以与一个Lambda函数进行交互，而该函数又会调用一个SageMaker模型。您可以通过自己在Lambda中编写代码，将模型请求和响应存储在S3存储桶中，或者您也可以设置SageMaker端点来为您完成此操作。一旦您将记录存储在S3中，就可以运行**模型监控**作业。这可以利用SageMaker的功能——模型监控，来学习训练数据与推理数据之间的统计差异，并在这些差异超出大范围时向您发送警报。或者，您也可以编写自己的对比脚本，并在SageMaker的训练或处理作业中运行这些任务。
- en: Once you have some visibility into how your model is responding on aggregate,
    your best move is to incorporate human feedback as much as you can. This is increasingly
    true in the generative space, where accuracy, style, and tone of the content are
    top criteria for most organizations. A great option for this is **SageMaker Ground
    Truth**! As we learned in [*Chapter 2*](B18942_02.xhtml#_idTextAnchor034) on preparing
    data, this is a fully managed service you can use to both increase your labeled
    datasets and augment your model responses in real time.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你对模型的整体响应情况有了一定的了解，接下来的最佳策略是尽可能地融入人工反馈。在生成领域尤其如此，在这里，内容的准确性、风格和语气是大多数组织的首要标准。一个很好的选择是**SageMaker
    Ground Truth**！正如我们在[*第2章*](B18942_02.xhtml#_idTextAnchor034)中学习的数据准备部分所述，这是一个完全托管的服务，你可以用它来增加标注数据集，并实时增强模型响应。
- en: A similar approach here is to use multiple models to confirm the prediction
    result. Imagine that you process a document quickly and want to extract content
    from it accurately. Your customer uploads a PDF to your website, you parse it
    using ML models, and you want to confirm or deny the contents of a given field.
    One way to increase your stakeholder’s confidence in the accuracy of your system
    is to just use more than one model. Maybe you use your own, a custom deep learning
    model hosted in SageMaker, while at the same time, you point to a fully managed
    AWS service such as Textract that can extract digital natural language from visual
    forms. Then, you might have a Lambda function to see whether both models agree
    on the response. If they do, then you could respond to the customer directly!
    If they don’t, then you could route the request for manual review.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的类似方法是使用多个模型来确认预测结果。试想你快速处理了一份文档，并希望准确提取其中的内容。你的客户将PDF文件上传到你的网站，你通过机器学习模型解析它，并希望确认或否定某个字段的内容。增加利益相关者对系统准确性的信心的一种方式就是使用不止一个模型。也许你使用的是自己的模型，一个托管在SageMaker中的定制深度学习模型，同时，你也可以指向一个完全托管的AWS服务，如Textract，它可以从视觉表单中提取数字化自然语言。然后，你可能还会有一个Lambda函数来查看两个模型是否对结果一致。如果一致，那么你可以直接回复客户！如果不一致，则可以将请求路由到人工审核。
- en: There are countless other ways to monitor your models, including ways to integrate
    these with people! For now, however, let’s move on to components of MLOps that
    are specifically scoped to vision and language.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 监控模型的方法有无数种，其中包括将这些方法与人类结合的方式！不过，现在让我们继续探讨与视觉和语言相关的MLOps组件。
- en: MLOps for foundation models
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础模型的MLOps
- en: Now that you have a good idea of MLOps, including some ideas about how to use
    human-in-the-loop and model monitoring, let’s examine specifically what aspects
    of vision and language models merit our attention from an MLOps perspective.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经对MLOps有了一个清晰的了解，并且掌握了一些关于如何使用“人工在环”和模型监控的思路，接下来让我们从MLOps的角度具体探讨视觉和语言模型中哪些方面值得我们关注。
- en: The answer to this question isn’t immediately obvious because, from a certain
    angle, vision and language are just slightly different aspects of machine learning
    and artificial intelligence. Once you have the right packages, images, datasets,
    access, governance, and security configured, the rest should just flow naturally.
    Getting to that point, however, is quite an uphill battle!
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案并不那么显而易见，因为从某个角度来看，视觉和语言只是机器学习和人工智能的略微不同的方面。一旦你配置好了合适的包、图像、数据集、访问权限、治理和安全性，其他部分应该会自然而然地流畅进行。然而，达到这一点的过程却是一场艰难的斗争！
- en: Building a pipeline for large language models is no small task. As I mentioned
    previously, I see at least two very different aspects of this. On one side of
    the equation, you’re looking at the entire model development life cycle. As we’ve
    learned throughout this book, that’s a massive scope of development. From dataset,
    model, and script preparation to the training and evaluation loops, and performance
    and hyperparameter optimizations, there are countless techniques to track in order
    to produce your foundation model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个大型语言模型的管道并非易事。正如我之前提到的，我认为这其中至少涉及两个非常不同的方面。从其中的一方面来看，你需要关注整个模型开发生命周期。正如我们在本书中所学到的，这涉及到大量的开发工作。从数据集、模型和脚本准备到训练和评估循环，再到性能和超参数优化，有无数种技术需要追踪，以便构建你的基础模型。
- en: Once you have the foundation model, preparing it for development is a different
    beast. As we discussed previously, *adversarial hardening* includes a variety
    of techniques you can use to improve the performance of your model for the target
    domain. Everything we learned about in fine-tuning and evaluation from [*Chapter
    10*](B18942_10.xhtml#_idTextAnchor152), bias detection and mitigation in [*Chapter
    11*](B18942_11.xhtml#_idTextAnchor167), and deployment techniques in [*Chapter
    12*](B18942_12.xhtml#_idTextAnchor178) come right to the forefront. To me, it
    seems natural to locate these in a different pipeline that is focused squarely
    on deployment. Let’s take a look at these in the following visual.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有了基础模型，为开发做准备就变成了一个不同的问题。正如我们之前讨论的，*对抗性强化*包括了一系列技术，你可以用它们来提升模型在目标领域中的表现。从[*第
    10 章*](B18942_10.xhtml#_idTextAnchor152)中学到的微调和评估，[*第 11 章*](B18942_11.xhtml#_idTextAnchor167)中的偏差检测与缓解，以及[*第
    12 章*](B18942_12.xhtml#_idTextAnchor178)中的部署技术，都可以直接应用到这里。对我来说，将这些内容放在一个专注于部署的不同流程中似乎很自然。让我们通过下图来看看这些内容。
- en: '![Figure 14.4 – LLM development and deployment pipelines](img/B18942_Figure_14_04.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.4 – LLM 开发与部署流程](img/B18942_Figure_14_04.jpg)'
- en: Figure 14.4 – LLM development and deployment pipelines
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4 – LLM 开发与部署流程
- en: What makes this much more complicated is that *many of these disparate steps
    use similar packages and functions*. This means that to implement each of these
    steps, you’re looking at pointing to at least one, and possibly a few `git` repositories
    and packages. When you decouple these, using different containers, resources,
    and steps to manage each piece, it helps each team work on them independently.
    We all know that the pace of foundation model development is only going to increase
    over the next few years, so assume that each step here will mean you need to pause
    periodically, capture the latest open source scripts or research techniques, develop
    and test them, and integrate them back into the larger pipeline.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使这件事变得更加复杂的是，*这些不同的步骤中有许多使用了相似的软件包和函数*。这意味着，要实现每一个步骤，你可能需要指向至少一个，甚至可能是几个 `git`
    仓库和软件包。当你将这些步骤解耦，使用不同的容器、资源和步骤来管理每个部分时，这有助于每个团队独立工作。我们都知道，基础模型的开发速度在未来几年只会加快，因此可以预见的是，每一个步骤都会需要你定期暂停，捕捉最新的开源脚本或研究技术，进行开发和测试，并将它们重新集成到更大的流程中。
- en: Now, let’s learn about MLOps for vision.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习一下视觉领域的 MLOps。
- en: MLOps for vision
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉领域的 MLOps
- en: How do vision foundation models compare with what we suggested previously for
    language models? To some degree, not much. You’re still working with images, scripts,
    packages, datasets, and model quality. You still want to keep your models up to
    date, and you still want to incorporate as much human feedback in the best way
    you can. As we’ve seen in the book so far, models and evaluation metrics will
    vary, datasets will be quite different, and tasks are not entirely the same. A
    lot of the basic logic, however, carries over. One quick word of caution though
    – fine-tuning in language is not at all the same as fine-tuning in vision.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉基础模型与我们之前为语言模型提出的建议相比如何？在某种程度上，差别不大。你依然是在处理图像、脚本、软件包、数据集和模型质量。你仍然希望保持模型的最新状态，并且尽可能多地融入人类反馈。正如我们在本书中迄今所看到的，模型和评估指标会有所不同，数据集也会大不相同，任务也并非完全相同。然而，许多基本的逻辑仍然是通用的。不过，值得提醒的是——在语言领域的微调与视觉领域的微调是完全不同的。
- en: A word of caution on overfitting in vision and a call for common sense
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于视觉领域的过拟合和呼吁常识的警告
- en: Please keep in mind that vision *is much more sensitive to overfitting than
    language*. To understand this, let’s consider the fundamental differences between
    the two modalities. Language is inherently discrete; we represent the entire world
    with only letters and words, items that are noncontinuous by default. You could
    say that the entire modality of language is equal to the sum of all dictionaries
    in all languages around the world, to the extent that dictionaries themselves
    are only approximations of words spoken, used, and developed by humans constantly.
    The arrangement of these words, and the interpretation and meaning of them across
    the wide breadth of lived human experiences, is infinite.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，视觉*比语言更容易受到过拟合的影响*。为了理解这一点，我们可以考虑这两种模式之间的根本区别。语言本质上是离散的；我们通过字母和单词来表达整个世界，而这些项本身就是非连续的。你可以说，语言的整个模式等同于世界上所有语言词典的总和，词典本身只是人类不断使用、发展和表达的词语的近似。词语的排列、它们的解释以及在广泛的人类生活经验中的意义是无穷无尽的。
- en: Vision is completely different. The modality itself is continuous; while pixels
    themselves of course start and stop, the delineation between objects in a picture
    is almost a matter of opinion. We use metrics to quantify the quality of and discrepancy
    between labeled objects, such as *intersection over union*. Objects rotate; they
    seem to change completely in different lighting and backgrounds. Their patterns
    might seem to be the same even across totally different types, such as animals
    and cups, street signs and clothing, furniture, and natural landscapes. While
    both vision and language decompose into embeddings on their way into a model,
    the ability of neural nets to capture the meaning of the content provided and
    extrapolate this into other settings seems to be very different in language than
    in vision. Language fine-tuning works well in many cases, while vision fine-tuning
    very commonly results in poor performance at first blush.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉完全不同。该模式本身是连续的；尽管像素本身当然是有起止的，但图像中物体之间的界定几乎是主观的。我们使用指标来量化标记物体之间的质量和差异，比如*交集与并集比*。物体会旋转；在不同的光照和背景下，它们似乎完全变化。即使是完全不同类型的物体，例如动物与杯子、路标与衣物、家具与自然景观，它们的模式可能看起来也非常相似。虽然视觉和语言都在进入模型时被分解成嵌入，但神经网络在捕捉内容的意义并将其推断到其他环境中的能力，似乎在语言和视觉中是非常不同的。语言微调在许多情况下效果很好，而视觉微调通常在初期表现较差。
- en: Personally, I find another machine learning technique very interesting, which
    appears to operate, in essence, at the core of these combined modalities – common
    sense reasoning. Machine common sense refers to ensuring logical consistency between
    concepts, objects, and the defining characteristics of those objects. Most humans
    excel at this, such as knowing that water is wet, that heavy objects fall when
    dropped into open space, that fire produces heat, and so on.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 就个人而言，我觉得另一种机器学习技术非常有趣，它在本质上似乎是这些结合模式的核心——常识推理。机器常识指的是确保概念、物体以及这些物体的定义特征之间的逻辑一致性。大多数人擅长这一点，比如知道水是湿的，重物被丢到空中会下落，火会产生热量，等等。
- en: Computers, however, are terrible at this. It’s almost as if the physical dimension
    doesn’t exist; certainly, the biological plain is a complete anomaly to them.
    Image generators don’t understand that food has to go into a mouth to constitute
    eating. Image classifiers routinely miscategorize zebras with furniture. Language
    models don’t appreciate the pace of human communication, occasionally overwhelming
    their operators. Movie generators regularly cause more disgust than delight because
    they fail to recognize the first basic discrimination mastered by infants – humans
    and their movement. To humans, it’s immediately obvious that a cup and an animal
    are both objects, and might even occasionally share some stylistic traits, but
    in the physical world, they come from completely different domains. To computers,
    it’s as if this physical dimension doesn’t exist. They are quite literally only
    learning what exists inside the two-dimensional frames you provide. This is why
    we use labels in the first place – to give your model some meaning by translating
    the physical world into pixels.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，计算机在这方面做得非常糟糕。它们似乎根本无法理解物理维度的存在；当然，生物学层面对它们来说完全是个异常。图像生成器不了解食物必须进入嘴巴才能算作进食。图像分类器常常把斑马与家具混淆。语言模型不理解人类沟通的节奏，偶尔让使用者感到不堪重负。电影生成器经常让人反感多于愉悦，因为它们无法识别婴儿首先掌握的基本辨别能力——人类及其动作。对于人类来说，杯子和动物都是物体，可能偶尔还会有一些相似的风格特征，但在物理世界里，它们分别来自完全不同的领域。对计算机而言，似乎这个物理维度根本不存在。它们实际上只是在学习你提供的二维图像中存在的内容。这就是我们一开始使用标签的原因——通过将物理世界转化为像素，赋予模型一定的意义。
- en: I had the distinct pleasure of meeting and briefly chatting with Yejin Choi
    *(2)* last year. She delivered a keynote to the general assembly of the Association
    of Computational Linguists, one of the best NLP conferences in the world, on a
    fascinating hypothetical forecast of the next 60 years of natural language research.
    I was completely blown away by her passion for the humanities, philosophy, and
    deep scientific discoveries. She started exploring machine common sense in an
    era when it was extremely unpopular to do so, and in fact, she jokes today that
    she was actively discouraged from doing so, since everyone thought it would be
    impossible to get published on this topic. Since then, she’s turned into probably
    the world’s leading expert in this area, largely operating with language and vision
    as her modalities. Since then, I’ve been curious about common sense and wanted
    to explore it in more detail.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我有幸在去年与叶津·崔*(2)*见面并进行了简短的交流。她在世界顶尖的自然语言处理会议之一——计算语言学协会年会上发表了主题演讲，讲述了一个引人入胜的假设性预测，关于未来60年自然语言研究的走向。我被她对人文学科、哲学以及深层科学发现的热情深深打动。她在一个极少有人关注机器常识的时代开始了这方面的探索，事实上，她今天开玩笑说，当时她还遭到积极反对，因为大家都认为在这个话题上发表文章几乎是不可能的。从那时起，她已经成为这个领域可能最顶尖的专家，主要通过语言和视觉作为她的研究方式。从那时起，我对常识产生了浓厚的兴趣，想要更详细地探讨它。
- en: I wonder if human knowledge itself is inherently relational and possibly multimodal.
    We build concepts in our minds based on experiences – lived, perceived, imagined,
    and understood. These mental concepts guide our words and actions, expressing
    themselves in some cases verbally, and other times purely physically. Perhaps
    we need deeper representations to guide the intermingling of modalities. Perhaps
    language might help our vision models adapt to new domains more quickly. Perhaps
    this is because it provides a bit of common sense.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我在想，人类的知识本身是否具有内在的关系性，甚至可能是多模态的。我们在大脑中构建概念，是基于经历——无论是亲身体验、感知、想象还是理解。这些心理概念引导着我们的言语和行动，有时通过语言表达，有时则完全通过身体语言表达。或许我们需要更深层的表示来引导模态的交融。或许语言能帮助我们的视觉模型更快适应新领域。也许这是因为它提供了一些常识。
- en: Practically, I’m bringing this up because, if you’re about to embark on a vision
    fine-tuning exercise, I want you to go in knowing that it won’t be easy, and what
    worked for you in language probably won’t translate as well as you thought. I’m
    also bringing this up because I want you future researchers out there to take
    courage, trust your intuition, and challenge your assumptions. Now that we’ve
    learned a bit about MLOps for foundation models, let’s take a look at some AWS
    offerings to help simplify and speed you up to nail this subject!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我之所以提到这一点，是因为如果你准备开始进行视觉微调工作，我希望你提前知道，这不会是件容易的事，而且在语言领域对你有效的东西，可能并不会像你想的那样顺利迁移到视觉领域。我还想提到这一点，是因为我希望你们这些未来的研究人员要有勇气，相信直觉，挑战自己的假设。现在我们已经了解了基础模型的
    MLOps，让我们来看一看 AWS 提供的一些服务，帮助简化流程并加速你掌握这一主题！
- en: AWS offerings for MLOps
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS 提供的 MLOps 服务
- en: Happily, AWS provides a variety of tools to help simplify this! One nice feature
    is called **lineage tracking**. SageMaker can automatically create the lineage
    *(3)* for key artifacts, including across accounts. This includes dataset artifacts,
    images, algorithm specifications, data configs, training job components, endpoints,
    and checkpoints. This is integrated with the **Experiments SDK**, letting you
    compare experiments and results programmatically and at scale. Let’s explore this
    visually. We’ll even generate a visualization for you to see how all of these
    are connected! Check it out in the following figure.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，AWS 提供了各种工具来简化这一过程！其中一个很棒的功能叫做**血缘追踪**。SageMaker 可以自动为关键工件创建血缘*(3)*，甚至跨账户创建血缘。这包括数据集工件、镜像、算法规格、数据配置、训练作业组件、端点和检查点。这与**实验
    SDK** 集成，允许你以编程方式在大规模上比较实验和结果。让我们通过可视化来看一下。我们甚至会为你生成一个可视化图，展示所有这些是如何关联的！请查看下图。
- en: '![Figure 14.5 – SageMaker automatically creates lineage tracking](img/B18942_Figure_14_05.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.5 – SageMaker 自动创建血缘追踪](img/B18942_Figure_14_05.jpg)'
- en: Figure 14.5 – SageMaker automatically creates lineage tracking
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5 – SageMaker 自动创建血缘追踪
- en: As you can see, the first step in tracking your lineage is running on key SageMaker
    resources such as training jobs, images, and processing jobs. You can use the
    entities that are automatically tracked, or you can define your own entities.
    To generate the lineage view as shown in the previous figure, you can interact
    with the **lineage query language**. If you want to jump straight into the notebook,
    which ships with a visualization solution, that’s available as point *(4)* in
    the *References* section. The lineage tracking is explained in more detail here
    – *(5)*, and the querying is defined here – *(6)*. Using SageMaker Lineage, you
    can easily trace how a model was trained and where it was deployed.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，追踪血缘的第一步是运行 SageMaker 中的关键资源，如训练作业、镜像和处理作业。你可以使用自动追踪的实体，也可以定义自己的实体。为了生成如前图所示的血缘视图，你可以使用**血缘查询语言**。如果你想直接进入带有可视化解决方案的笔记本，可以查看*参考资料*部分中的点（4）。血缘追踪的详细说明见此处
    – *(5)*，查询语言的定义见此处 – *(6)*。通过 SageMaker 血缘追踪，你可以轻松追踪一个模型是如何训练的以及它被部署到哪里。
- en: How does it work? You can use the *LineageFilter API* to look for different
    objects, such as endpoints, that are associated with a model artifact. You can
    also search for trial components associated with endpoints, find datasets associated
    with models, and traverse forward and backward through the graph of associated
    items. Having these relationships available programmatically makes it much easier
    to take all of the necessary resources and put them into pipelines and other governance
    structures.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 它是如何工作的？你可以使用*LineageFilter API*来查找与模型工件相关的不同对象，如端点。你还可以搜索与端点相关的试验组件，查找与模型相关的数据集，并在关联项的图谱中前后穿梭。拥有这些关系的程序化访问，使得将所有必要资源放入管道和其他治理结构中变得更加容易。
- en: Once you have the resources identified, how do you wrap them into a pipeline?
    As we mentioned earlier in the chapter, many of the basic AWS and SageMaker resources
    are available as discrete building blocks. This includes the model, relevant model
    artifacts, deployment configurations, associated training and processing jobs,
    hyperparameter tuning, and containers. This means you can use the AWS SDK for
    Python, **boto3**, and the **SageMaker Python SDK** to point to and execute all
    of your resources and tasks programmatically. Wrapping these in a pipeline then
    means using whatever tooling stack you prefer to use to operationalize these automatically.
    One option for doing so is **SageMaker Pipelines**!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你识别了资源，如何将它们封装成一个管道？正如我们在本章之前提到的，许多基础的AWS和SageMaker资源都可以作为独立的构建模块使用。这些资源包括模型、相关的模型工件、部署配置、关联的训练和处理作业、超参数调优和容器。这意味着你可以使用AWS
    SDK for Python，**boto3**和**SageMaker Python SDK**来编程地指向并执行所有资源和任务。将这些资源封装到一个管道中意味着使用你偏好的工具堆栈来自动化操作它们。实现这一点的一种方式是**SageMaker
    Pipelines**！
- en: A quick introduction to SageMaker Pipelines
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SageMaker Pipelines简介
- en: If you’re working with SageMaker-native resources, such as jobs, endpoints,
    model artifacts, and Docker images, then connecting them through the Pipelines
    SDK construct *(7)* should not be too much of an additional lift. SageMaker Pipelines
    is a managed feature you can use to create, run, and manage complete workflows
    for machine learning on AWS. Once you’ve defined your base Python SDK objects
    for SageMaker, such as a training job, evaluation metrics, hyperparameter tuning,
    and an endpoint, you can pass each of these objects to the Pipelines API and create
    it as a graph! Let’s explore this in more detail in the following figure.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用SageMaker原生资源，例如作业、端点、模型工件和Docker镜像，那么通过Pipelines SDK构造*(7)*将它们连接起来应该不会增加太多额外的负担。SageMaker
    Pipelines是一个托管功能，你可以使用它来创建、运行和管理AWS上的完整机器学习工作流。一旦你为SageMaker定义了基础的Python SDK对象，如训练作业、评估指标、超参数调优和端点，你就可以将这些对象传递给Pipelines
    API并将其创建为一个图！接下来让我们在下面的图中更详细地探讨这一点。
- en: '![Figure 14.6 – SageMaker Pipelines](img/B18942_Figure_14_06.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.6 – SageMaker Pipelines](img/B18942_Figure_14_06.jpg)'
- en: Figure 14.6 – SageMaker Pipelines
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6 – SageMaker Pipelines
- en: For a notebook walk-through of creating a graph very similar to the one in the
    preceding figure, there is a resource on GitHub *(8)*. The core idea is that you
    build each part of the pipeline separately, such as the data processing, training,
    and evaluation steps, and then pass each of these to the SageMaker Pipelines API
    to create the connected graph.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要查看一个类似于前面图中所示的图形的笔记本示例，可以访问GitHub上的资源*(8)*。核心思路是，你将管道的每个部分单独构建，比如数据处理、训练和评估步骤，然后将这些部分传递给SageMaker
    Pipelines API，创建一个连接的图。
- en: As you can see in the preceding figure, this is presented visually in SageMaker
    Studio! This makes it much easier for data scientists to develop, review, manage,
    and execute these pipelines. Studio also has a handful of other relevant features
    for MLOps, such as a feature store, model registry, endpoint management, model
    monitoring, and inference recommender. For a deeper dive into these topics, there’s
    a full white paper from the AWS Well-Architected Framework on machine learning
    *(9)*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在上面的图中看到的，这在SageMaker Studio中以可视化方式呈现！这使得数据科学家更容易开发、审查、管理和执行这些管道。Studio还提供了许多其他与MLOps相关的功能，例如特征存储、模型注册、端点管理、模型监控和推理推荐器。如果你想更深入了解这些话题，可以查阅AWS
    Well-Architected Framework关于机器学习的完整白皮书*(9)*。
- en: Now that we’ve learned about the AWS offerings for MLOps, let’s close out the
    chapter with a full recap.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了AWS的MLOps产品，让我们通过一个完整的回顾来结束本章内容。
- en: Summary
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the core concept of MLOps, especially in the
    context of vision and language. We discussed machine learning operations, including
    some of the technologies, people, and processes that make it work. We especially
    focused on the pipeline aspect, learning about technologies useful to build them,
    such as SageMaker Pipelines, Apache Airflow, and Step Functions. We looked at
    a handful of different types of pipelines relevant to machine learning, such as
    model deployment, model retraining, and environment promotion. We discussed core
    operations concepts, such as CI and CD. We learned about model monitoring and
    human-in-the-loop design patterns. We learned about some specific techniques for
    vision and language within MLOps, such as common development and deployment pipelines
    for large language models. We also looked at how the core methods that might work
    in language can be inherently less reliable in vision, due to the core differences
    in the modalities and how current learning systems operate. We took a quick tour
    down the philosophical route by discussing common sense reasoning, and then we
    closed out the chapter with key AWS offerings for MLOps, such as SageMaker Lineage,
    Experiments, and Pipelines.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了MLOps的核心概念，尤其是在视觉和语言的背景下。我们讨论了机器学习操作，包括使其运行的一些技术、人员和流程。我们特别关注了流水线方面，学习了构建它们的有用技术，如SageMaker
    Pipelines、Apache Airflow和Step Functions。我们研究了与机器学习相关的几种不同类型的流水线，如模型部署、模型再训练和环境推广。我们讨论了核心操作概念，如CI和CD。我们了解了模型监控和人机协同设计模式。我们学习了MLOps中一些适用于视觉和语言的特定技术，如大型语言模型的常见开发和部署流水线。我们还探讨了语言中可能有效的核心方法，如何由于模态之间的根本差异以及当前学习系统的运作方式，在视觉中固有地不太可靠。我们通过讨论常识推理，快速走了一遍哲学路线，然后通过介绍AWS的MLOps关键产品，如SageMaker
    Lineage、Experiments和Pipelines，结束了本章内容。
- en: Now, let’s conclude the book with one final chapter on future trends.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过最后一章讨论未来趋势来结束本书。
- en: References
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Please go through the following content for more information on a few topics
    covered in the chapter:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请阅读以下内容，以获取本章中涵盖的几个主题的更多信息：
- en: '*Hardening Deep Neural Networks via Adversarial Model* *Cascades*:[https://arxiv.org/pdf/1802.01448.pdf](https://arxiv.org/pdf/1802.01448.pdf)'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*通过对抗模型* *级联强化深度神经网络*：[https://arxiv.org/pdf/1802.01448.pdf](https://arxiv.org/pdf/1802.01448.pdf)'
- en: '*Yejin* *Choi*: [https://homes.cs.washington.edu/~yejin/](https://homes.cs.washington.edu/~yejin/)'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Yejin* *Choi*：[https://homes.cs.washington.edu/~yejin/](https://homes.cs.washington.edu/~yejin/)'
- en: '*Amazon SageMaker ML Lineage* *Tracking*: [https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking.html](https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking.html)'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Amazon SageMaker ML 谱系* *追踪*：[https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking.html](https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking.html)'
- en: '*aws/amazon-sagemaker-examples*: [https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-lineage/sagemaker-lineage.ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-lineage/sagemaker-lineage.ipynb)'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*aws/amazon-sagemaker-examples*：[https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-lineage/sagemaker-lineage.ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-lineage/sagemaker-lineage.ipynb)'
- en: '*Lineage Tracking* *Entities**:* [https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking-entities.html](https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking-entities.html)'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*谱系追踪* *实体*：[https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking-entities.html](https://docs.aws.amazon.com/sagemaker/latest/dg/lineage-tracking-entities.html)'
- en: '*Querying Lineage* *Entities*:[https://docs.aws.amazon.com/sagemaker/latest/dg/querying-lineage-entities.html](https://docs.aws.amazon.com/sagemaker/latest/dg/querying-lineage-entities.html)'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*查询谱系* *实体*：[https://docs.aws.amazon.com/sagemaker/latest/dg/querying-lineage-entities.html](https://docs.aws.amazon.com/sagemaker/latest/dg/querying-lineage-entities.html)'
- en: '*SageMaker* *Pipelines*:[https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/index.html](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/index.html)'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*SageMaker* *流水线*：[https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/index.html](https://sagemaker.readthedocs.io/en/stable/workflows/pipelines/index.html)'
- en: '*aws/amazon-sagemaker-examples*: [https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/train-register-deploy-pipeline-model/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/train-register-deploy-pipeline-model/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb)'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*aws/amazon-sagemaker-examples*：[https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/train-register-deploy-pipeline-model/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb](https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-pipelines/tabular/train-register-deploy-pipeline-model/train%20register%20and%20deploy%20a%20pipeline%20model.ipynb)'
- en: '*Machine Learning Lens AWS Well-Architected* *Framework*:[https://docs.aws.amazon.com/pdfs/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf](https://docs.aws.amazon.com/pdfs/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf)'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*机器学习镜头 AWS Well-Architected* *框架*：[https://docs.aws.amazon.com/pdfs/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf](https://docs.aws.amazon.com/pdfs/wellarchitected/latest/machine-learning-lens/wellarchitected-machine-learning-lens.pdf)'
