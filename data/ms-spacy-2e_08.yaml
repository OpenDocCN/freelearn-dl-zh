- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Training an NER Component with Your Own Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自己的数据训练NER组件
- en: In this chapter, you will learn how to use your own data to train spaCy’s pre-trained
    models. We will do that by training a **named entity recognition** ( **NER** )
    pipeline, but you can apply the same knowledge to preprocess and train spaCy pipelines
    for any NLP task. In this chapter, we will focus more on how to collect and label
    your own data, since we saw how to train models with spaCy’s **config.cfg** file
    in [*Chapter 6*](B22441_06.xhtml#_idTextAnchor087) .
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何使用自己的数据来训练spaCy的预训练模型。我们将通过训练一个**命名实体识别**（**NER**）管道来实现这一点，但你也可以将同样的知识应用到预处理和训练spaCy管道的任何NLP任务上。在本章中，我们将更多地关注如何收集和标注自己的数据，因为我们已经在[*第6章*](B22441_06.xhtml#_idTextAnchor087)中看到了如何使用spaCy的**config.cfg**文件来训练模型。
- en: The learning journey of this chapter includes how to make the best use of **Prodigy**
    , the annotation tool from Explosion, and the team behind spaCy. We will also
    see how to annotate NER data using Jupyter Notebook. After that, we will update
    the spaCy pipeline’s NER component with this labeled data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的学习之旅包括如何最大限度地利用来自Explosion的标注工具**Prodigy**以及spaCy背后的团队。我们还将看到如何使用Jupyter
    Notebook标注NER数据。之后，我们将使用这些标注数据更新spaCy管道的NER组件。
- en: This chapter takes you through a complete machine learning practice, including
    collecting data, annotating data, and training a model for information extraction.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将带你完成整个机器学习实践，包括收集数据、标注数据和训练信息提取模型。
- en: 'By the end of this chapter, you’ll be ready to train spaCy models on your own
    data. You’ll have the full skill set of collecting data, preprocessing data into
    the format that spaCy can recognize, and finally, training spaCy models with this
    data. We’re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将准备好在自己的数据上训练spaCy模型。你将具备收集数据、将数据预处理成spaCy可以识别的格式，以及最终使用这些数据训练spaCy模型的全套技能。我们将涵盖以下主要主题：
- en: Getting started with data preparation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始数据准备
- en: Annotating and preparing data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标注和准备数据
- en: Training a NER pipeline component
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练NER管道组件
- en: Combining multiple NER components in the same pipeline
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在同一管道中结合多个NER组件
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code of this Chapter can be found at [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    . We will use the **nertk** library to annotate the NER data using a Jupyter notebook.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)找到。我们将使用**nertk**库通过Jupyter笔记本来标注NER数据。
- en: Getting started with data preparation
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始数据准备
- en: spaCy out-of-the-box models are very successful for general NLP purposes but
    sometimes we have to work on very specific domains that require custom training.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy开箱即用的模型在通用NLP方面非常成功，但有时我们必须处理需要定制训练的非常具体的领域。
- en: 'Training models requires time and effort. Before even starting the training
    process, you should decide whether the training is necessary. To determine whether
    you really need custom training, a good starting point is to ask yourself the
    following questions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型需要时间和精力。在开始训练过程之前，你应该决定是否需要训练。为了确定你是否真的需要定制训练，一个很好的起点是问自己以下问题：
- en: Do spaCy models perform well enough on your data?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy模型在你的数据上表现足够好吗？
- en: Does your domain include many labels that are absent in spaCy models?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的领域是否包含许多在spaCy模型中缺失的标签？
- en: Is there a pre-trained model/application in Hugging Face Hub or elsewhere already?
    (We wouldn’t want to reinvent the wheel.)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face Hub或其他地方已经有了预训练的模型/应用程序吗？（我们不想重新发明轮子。）
- en: Let’s discuss these two first questions in detail in the following sections.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在以下章节中详细讨论这两个问题。
- en: Do spaCy models perform well enough on your data?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: spaCy模型在你的数据上表现足够好吗？
- en: 'In general, if the model performs well enough (generally, something above 0.75
    accuracy), then you can customize the model output by means of another spaCy component.
    For example, let’s say we work on the navigation domain and we have utterances
    such as the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，如果模型表现足够好（通常，准确率在0.75以上），那么你可以通过另一个spaCy组件来定制模型输出。例如，假设我们在导航领域工作，我们有以下这样的表述：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s see what entities spaCy’s NER model outputs for these sentences:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看spaCy的NER模型为这些句子输出了哪些实体：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, **home** isn’t recognized as an entity at all, but we want it to be recognized
    as a location entity. Also, spaCy’s NER model labels **Oxford Street** as **FAC**
    , meaning a building/highway/airport/bridge type entity, which is not what we
    want.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**家**根本不被识别为实体，但我们希望它被识别为地点实体。此外，spaCy的NER模型将**牛津街**标记为**FAC**，意味着建筑/公路/机场/桥梁类型的实体，这并不是我们想要的。
- en: We want this entity to be recognized as **GPE** , a location. Here, we can train
    NER further to recognize street names as **GPE** , as well as recognize some location
    words (such as work, home, and my mama’s house) as **GPE** .
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这个实体被识别为**GPE**，即一个地点。在这里，我们可以进一步训练NER以识别街道名称为**GPE**，以及识别一些地点词汇（如工作、家和我的妈妈家）为**GPE**。
- en: Another example is the newspaper domain. In this domain, **person** , **place**
    , **date** , **time** , and **organization** are all target entities that should
    be extracted, but you need one more entity type – **vehicle** (car, bus, airplane,
    and so on). Hence, instead of training from scratch, you can add a new entity
    type by using spaCy’s **SpanRuler** (explained in [*Chapter 4*](B22441_04.xhtml#_idTextAnchor056)
    ). Always examine your data first and calculate the spaCy models’ success rate.
    If the success rate is satisfying, then use other spaCy components to customize.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是报纸领域。在这个领域，**人物**、**地点**、**日期**、**时间**和**组织**都是需要提取的目标实体，但你还需要一个额外的实体类型——**车辆**（汽车、公共汽车、飞机等）。因此，你不必从头开始训练，而是可以使用spaCy的**SpanRuler**（在第[*第4章*](B22441_04.xhtml#_idTextAnchor056)中解释）添加一个新的实体类型。始终首先检查你的数据，并计算spaCy模型的成功率。如果成功率令人满意，那么可以使用其他spaCy组件进行定制。
- en: Does your domain include many labels that are absent in spaCy models?
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你的领域是否包含许多在spaCy模型中不存在的标签？
- en: For instance, in the preceding newspaper example, only one entity label, **vehicle**
    , is missing from the spaCy’s NER model’s labels but other entity types are recognized.
    Since we can recognize the **vehicle** entity with **SpanRuler** , in this case,
    you don’t need custom training.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在先前的报纸示例中，只有**车辆**这个实体标签在spaCy的NER模型标签中缺失，但其他实体类型都被识别了。由于我们可以使用**SpanRuler**识别**车辆**实体，在这种情况下，你不需要定制训练。
- en: Consider the medical domain again. The entities are diseases, symptoms, drugs,
    dosages, chemical compound names, and so on. This is a specialized and long list
    of entities. For the medical domain, you require custom model training.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑医疗领域。实体包括疾病、症状、药物、剂量、化合物名称等等。这是一份专业且长的实体列表。对于医疗领域，你需要进行定制模型训练。
- en: 'If we need custom model training, we usually follow these steps:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要定制模型训练，我们通常遵循以下步骤：
- en: Collect our data.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集我们的数据。
- en: Annotate our data.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标注我们的数据。
- en: Decide to update an existing model or train a model from scratch.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定更新现有模型或从头开始训练一个模型。
- en: 'In the data collection step, we decide how much data to collect: 1,000 sentences,
    5,000 sentences, or more. The amount of data depends on the complexity of our
    task and domain. Usually, we start with an acceptable amount of data, train the
    model for the first time, and see how it performs; then, we can add more data
    and retrain the model.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据收集步骤中，我们决定收集多少数据：1,000个句子、5,000个句子，或者更多。数据量取决于我们任务的复杂性和领域。通常，我们从可接受的数据量开始，第一次训练模型，看看它的表现；然后，我们可以添加更多数据并重新训练模型。
- en: After collecting your dataset, you need to annotate your data in such a way
    that the spaCy training code recognizes it. In the next section, we will see the
    training data format and how to annotate data with Explosion’s Prodigy tool.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集完数据集后，你需要以这种方式标注你的数据，使得spaCy的训练代码能够识别它。在下一节中，我们将看到训练数据格式以及如何使用Explosion的Prodigy工具标注数据。
- en: 'The third step is to decide on training a blank model from scratch or making
    updates to an existing model. Here, the rule of thumb is as follows: if your entities/labels
    are present in the existing model but you don’t see a very good performance, then
    update the model with your own data, such as in the preceding **vehicle** example.
    If your entities are not present in the current spaCy model at all, then most
    probably you need custom training.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步是决定从头开始训练一个空白模型，还是对现有模型进行更新。在这里，一个经验法则是：如果你的实体/标签存在于现有模型中，但你没有看到非常好的性能，那么就用你自己的数据更新模型，例如在先前的**车辆**示例中。如果你的实体在当前的spaCy模型中根本不存在，那么你很可能需要进行定制训练。
- en: 'We’ll start our journey of building a model with the first step: preparing
    our training data. Let’s move on to the next section and see how to prepare and
    annotate our training data.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始构建模型的旅程，第一步是准备我们的训练数据。让我们继续到下一节，看看如何准备和标注我们的训练数据。
- en: Annotating and preparing data
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标注和准备数据
- en: The first step of training a model is always preparing training data. You usually
    collect data from customer logs and then turn them into a dataset by dumping the
    data as a CSV file or a JSON file. spaCy model training code works with **DocBin**
    objects, so we will be annotating the data and converting it to **DocBin** objects
    in this chapter.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型的第一步始终是准备训练数据。你通常从客户日志中收集数据，然后通过将数据作为CSV文件或JSON文件导出，将它们转换为数据集。spaCy模型训练代码与**DocBin**对象一起工作，因此在本章中，我们将标注数据并将其转换为**DocBin**对象。
- en: After collecting our data, we annotate our data. Annotation means labeling the
    intent, entities, POS tags, and so on.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 收集我们的数据后，我们标注我们的数据。标注意味着标记意图、实体、POS标签等。
- en: 'This is an example of annotated data, taken from spaCy’s *Detecting Fashion
    Brands in Online Comments* tutorial (available at [https://github.com/explosion/projects/tree/v3/tutorials/ner_fashion_brands](https://github.com/explosion/projects/tree/v3/tutorials/ner_fashion_brands)
    ):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个标注数据的示例，取自spaCy的*在线评论中检测时尚品牌*教程（可在[https://github.com/explosion/projects/tree/v3/tutorials/ner_fashion_brands](https://github.com/explosion/projects/tree/v3/tutorials/ner_fashion_brands)
    获取）：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We annotated the data with the goal of pointing the statistical algorithm to
    what we want the model to learn. In this example, we want the model to learn about
    the entities; hence, we feed examples with entities annotated.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们标注数据的目的是指向统计算法我们想要模型学习的内容。在这个例子中，我们希望模型学习关于实体；因此，我们提供带有标注实体的示例。
- en: In this section, we’ll also see spaCy’s annotation tool, **Prodigy** , along
    with **nertk** , a simple open source Python library to annotate NER data using
    Jupyter Notebook. Prodigy is tailored for small teams working on fast-paced projects.
    Although it’s not free or open source, buying Prodigy helps fund open source initiatives,
    including the development of spaCy itself.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们还将看到spaCy的标注工具**Prodigy**，以及**nertk**，一个简单的开源Python库，用于使用Jupyter Notebook标注NER数据。Prodigy专为快速项目的小团队量身定制。尽管它不是免费或开源的，但购买Prodigy有助于资助开源项目，包括spaCy本身的发展。
- en: Annotating data with Prodigy
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Prodigy标注数据
- en: Prodigy is a modern tool powered by active learning. We will be using the *Prodigy
    live demo* ( [https://demo.prodi.gy/](https://demo.prodi.gy/) ) to exhibit how
    an annotation tool works.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Prodigy是一个由主动学习驱动的现代工具。我们将使用**Prodigy live demo**（[https://demo.prodi.gy/](https://demo.prodi.gy/)）来展示标注工具的工作方式。
- en: 'Let’s get started:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'We navigate to the Prodigy Live Demo and view an example text by Prodigy to
    be annotated, as shown in the following screenshot:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导航到Prodigy Live Demo，并查看Prodigy要标注的示例文本，如下截图所示：
- en: '![Figure 8.1 – Prodigy interface (screenshot taken from their Prodigy Demo
    page)](img/B22441_08_01.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – Prodigy界面（从他们的Prodigy演示页面截取的截图）](img/B22441_08_01.jpg)'
- en: Figure 8.1 – Prodigy interface (screenshot taken from their Prodigy Demo page)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – Prodigy界面（从他们的Prodigy演示页面截取的截图）
- en: This screenshot, Figure 8 .1, shows an example text that we want to annotate.
    The buttons at the bottom of the screenshot showcase the means to either accept
    this training example, reject it, or ignore it. If the text is relevant and the
    annotation is good, then we accept it and it joins our dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这张截图，图8.1，展示了我们想要标注的示例文本。截图底部的按钮展示了接受这个训练示例、拒绝它或忽略它的方法。如果文本相关且标注良好，我们就接受它，并将其加入我们的数据集。
- en: 'Next, we’ll label the entities. To do that, first, we select an entity type
    from the upper bar. This corpus includes two types of entities, **PERSON** and
    **ORG** . Then, we’ll just select the words we want to label as an entity with
    the cursor, as shown in the following screenshot:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将标注实体。为此，首先，我们从顶部栏中选择一个实体类型。这个语料库包括两种类型的实体，**PERSON**和**ORG**。然后，我们将用光标选择我们想要标注为实体的单词，如下截图所示：
- en: '![Figure 8.2 – Annotating PERSON entities on the Prodigy Demo page](img/B22441_08_02.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 在Prodigy演示页面上标注PERSON实体](img/B22441_08_02.jpg)'
- en: Figure 8.2 – Annotating PERSON entities on the Prodigy Demo page
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 在Prodigy演示页面上标注PERSON实体
- en: After we’re finished with annotating the text, we click the **Accept** button.
    Once the session is finished, you can dump the annotated data as a JSON file.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们完成文本标注后，我们点击**接受**按钮。一旦会话结束，你可以将标注的数据导出为JSON文件。
- en: Prodigy features do not only include NER but also text classification, computer
    vision, prompt engineering, large language models, and more.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Prodigy的功能不仅包括命名实体识别（NER），还包括文本分类、计算机视觉、提示工程、大型语言模型等。
- en: Annotating data with nertk
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用nertk标注数据
- en: Another annotation tool is **nertk** , which is an open source Python library
    for NER text annotation ( [https://github.com/johnsmithm/nertk](https://github.com/johnsmithm/nertk)
    ). The tool works inline on Jupyter Notebook allowing you to interact with your
    data and annotate it quickly and easily. The project is currently in alpha development,
    so it’s useful only for simple and quick NER annotation sessions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个标注工具是**nertk**，它是一个开源的Python库，用于NER文本标注（[https://github.com/johnsmithm/nertk](https://github.com/johnsmithm/nertk)）。该工具在Jupyter
    Notebook中在线工作，允许你快速轻松地与数据交互并进行标注。该项目目前处于alpha开发阶段，因此它仅适用于简单和快速的NER标注会话。
- en: 'To work with **nertk** , you need to install it using **pip install nertk**
    , run a Jupyter Notebook server, and open the URL for the server in a web browser.
    To process the data for **nertk** , you need to tokenize the text and pass the
    data you want to annotate. Let’s do that step by step:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用**nertk**，你需要使用**pip install nertk**安装它，运行Jupyter Notebook服务器，并在网页浏览器中打开服务器的URL。为了处理**nertk**的数据，你需要对文本进行分词并传递你想要标注的数据。让我们一步一步来做：
- en: 'First, we need to tokenize the data because **nertk** works with lists of words.
    Our dataset for this chapter is in Portuguese so we import the **en** model and
    select only the **tokenizer** component since is the only thing we need:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要对数据进行分词，因为**nertk**与单词列表一起工作。我们这一章的数据集是葡萄牙语，所以我们导入**en**模型并仅选择**tokenizer**组件，因为这是我们唯一需要的：
- en: '[PRE3]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will use the dataset of spacy’s *Detecting fashion brands in online comments*
    tutorial. The raw text was sourced from the **r/MaleFashionAdvice** and **r/FemaleFashionAdvice**
    subreddits. The data is in the **jsonl** format, so we’ll use the **srsly** library
    to read it. Let’s print the first item to see what the data looks like:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用spacy的*在线评论中检测时尚品牌*教程的数据集。原始文本来自**r/MaleFashionAdvice**和**r/FemaleFashionAdvice**子版块。数据是**jsonl**格式，因此我们将使用**srsly**库来读取它。让我们打印第一个条目来看看数据的样子：
- en: '[PRE4]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Since the data is already annotated with Prodigy, it has the **['text', 'meta',
    '_input_hash', '_task_hash', 'tokens', 'spans', '_session_id', '_view_id', 'answer']**
    keys. To learn how we would go if we didn’t have the entities, let’s use just
    the **text** key, which contains the text of Reddit’s comment.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于数据已经用Prodigy进行了标注，它具有**['text', 'meta', '_input_hash', '_task_hash', 'tokens',
    'spans', '_session_id', '_view_id', 'answer']**键。为了了解如果没有实体我们会怎么做，让我们只使用**text**键，它包含Reddit评论的文本。
- en: 'The next step is to tokenize the data. We need to create a list where each
    item is also a list, but a list of the words of the comment. To do that, we’ll
    loop through the data, tokenize it with the model we loaded previously, and add
    this to the main list:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是对数据进行分词。我们需要创建一个列表，其中每个项目也是一个列表，但它是评论单词的列表。为此，我们将遍历数据，使用我们之前加载的模型对其进行分词，并将其添加到主列表中：
- en: '[PRE5]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we are ready to start the **nertk** tool. We need to pass to the **Entator**
    class the list with the entity label names and the list with the words of each
    comment. Then, we can call the **run()** method to start annotating:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好开始使用**nertk**工具。我们需要将实体标签名称的列表和每个评论的单词列表传递给**Entator**类。然后，我们可以调用**run()**方法开始标注：
- en: '[PRE6]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We will annotate all the **FASHION_BRAND** entities. *Figure 8* *.3* shows
    the **nertk** interface:'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将标注所有的**FASHION_BRAND**实体。*图8.3* *.3*显示了**nertk**界面：
- en: '![Figure 8.3 – The nertk interface](img/B22441_08_03.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – nertk界面](img/B22441_08_03.jpg)'
- en: Figure 8.3 – The nertk interface
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – nertk界面
- en: To select the tokens that are part of **FASHION_BRAND** entities, you simply
    click on them. When you are ready with the example, you can click on the next
    button to annotate another one. The results of the annotated tokens are stored
    in the **annotator.targets** attribute.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要选择属于**FASHION_BRAND**实体的标记，你只需点击它们。当你准备好示例后，你可以点击下一个按钮来标注另一个。标注的标记结果存储在**annotator.targets**属性中。
- en: Converting the annotated data to DocBin objects
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将标注的数据转换为DocBin对象
- en: 'To use this annotated data to train the models, we need to convert it to the
    **DocBin** format. To do that, we will create a **Doc** object for each comment
    and add the entities we’ve annotated. Let’s create the code to do that:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此注释数据来训练模型，我们需要将其转换为**DocBin**格式。为此，我们将为每个注释创建一个**Doc**对象，并添加我们已注释的实体。让我们创建执行此操作的代码：
- en: 'The first thing to do is to create the **DocBin** object and loop through each
    comment getting the indexes of the annotated entities:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首件事是创建**DocBin**对象，并遍历每个注释以获取注释实体的索引：
- en: '[PRE7]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With **nertk** , we annotated each token, but an entity can encompass more
    than one token. To handle that, we use a helper function that gets the **indexes_entity_tokens**
    list and creates tuples with the indexes of the start and end token of each entity
    (this code is still inside the main **for** loop):'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用**nertk**，我们注释了每个标记，但实体可以包含多个标记。为了处理这种情况，我们使用一个辅助函数，该函数获取**indexes_entity_tokens**列表，并为每个实体的起始和结束标记的索引创建元组（此代码仍然位于主**for**循环内）：
- en: '[PRE8]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, it’s time to create the **Span** objects for each entity. We create an
    **ents** list to store these spans, set this list as the **doc.ents** parameter,
    and add this **Doc** to the **DocBin** object (this code is also still inside
    the main **for** loop):'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候为每个实体创建**Span**对象了。我们创建一个**ents**列表来存储这些跨度，将此列表设置为**doc.ents**参数，并将此**Doc**添加到**DocBin**对象中（此代码仍然位于主**for**循环内）：
- en: '[PRE9]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we save the **DocBin** file to disk:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将**DocBin**文件保存到磁盘：
- en: '[PRE10]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here is the full code to convert the **nertk** annotated entities to **DocBin**
    :'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是将**nertk**注释的实体转换为**DocBin**的完整代码：
- en: '[PRE11]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This is the process of creating the training data to fine-tune the **ner**
    spaCy component. To create training data for any other trainable component, the
    process is the same. You create the **Doc** object, setting the annotations with
    the appropriate format, and then save these Docs as a **DocBin** object. You can
    check all the data formats here: [https://spacy.io/api/data-formats#dict-input](https://spacy.io/api/data-formats#dict-input)
    .'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是创建用于微调spaCy组件的**ner**的训练数据的流程。要为任何其他可训练组件创建训练数据，流程是相同的。你创建**Doc**对象，使用适当的格式设置注释，然后将这些Docs保存为**DocBin**对象。你可以在这里检查所有数据格式：[https://spacy.io/api/data-formats#dict-input](https://spacy.io/api/data-formats#dict-input)。
- en: With the training data set, the next step is to train the **ner** component.
    Let’s do that in the next section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据集之后，下一步是训练**ner**组件。让我们在下一节中完成这项工作。
- en: Training an NER pipeline component
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练NER管道组件
- en: Our goal is to create a pipeline to identify the **FASHION_BRAND** entities.
    We will do that by training the **EntityRecognizer** pipeline component. It uses
    a transition-based algorithm that assumes the most decisive information about
    the entities that are close to the initial tokens.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是创建一个用于识别**FASHION_BRAND**实体的管道。我们将通过训练**EntityRecognizer**管道组件来实现这一点。它使用基于转换的算法，该算法假设关于接近初始标记的实体的最关键信息。
- en: 'The first thing we need to do to train the component is to create the **config**
    file. We’ll use the **spacy init config** CLI command to do that:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 训练组件的第一件事是创建**config**文件。我们将使用**spacy init config** CLI命令来完成此操作：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This creates the **cpu_config.cfg** file with all default configurations and
    sets the **ner** component for training optimized for **efficiency** (faster inference,
    smaller model, and lower memory consumption). We will use the **preprocess.py**
    script from the spaCy tutorial to convert the **jsonl** data into **DocBin** objects:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建包含所有默认配置的**cpu_config.cfg**文件，并将**ner**组件设置为针对**效率**（更快推理、更小的模型和更低的内存消耗）进行训练优化。我们将使用spaCy教程中的**preprocess.py**脚本来将**jsonl**数据转换为**DocBin**对象：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, we are ready to train the model using the **spacy train** CLI command:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好使用**spacy train** CLI命令来训练模型：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The **output** parameter specifies the directory to store the trained pipeline.
    In *Figure 8* *.4* , we can see the messages produced during training.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**output**参数指定了存储训练管道的目录。在*图8* *.4*中，我们可以看到训练过程中产生的消息。'
- en: '![Figure 8.4 – Messages produced during the training of the ner component](img/B22441_08_04.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – ner组件训练过程中产生的消息](img/B22441_08_04.jpg)'
- en: Figure 8.4 – Messages produced during the training of the ner component
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – ner组件训练过程中产生的消息
- en: We can see that, during training, there is a training loss for a **tok2vec**
    component and a **ner** component. That’s because the **ner** component by default
    uses the **spacy.TransitionBasedParser.v2** model architecture. As the spaCy documentation
    says ( [https://spacy.io/api/architectures#TransitionBasedParser](https://spacy.io/api/architectures#TransitionBasedParser)
    ), the model consists of either two or three subnetworks. The **tok2vec** component
    transforms each token into a vector, running once per batch. **Lower** generates
    a vector for each **(token, feature)** pair per batch, combining features and
    applying non-linearity to form the state representation. **Upper** is an optional
    feed-forward network that derives scores from the state representation. If omitted,
    the lower model’s output is used directly as action scores. In summary, the **tok2vec**
    component is responsible for mapping the tokens into vector representations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在训练过程中，有一个**tok2vec**组件和**ner**组件的训练损失。这是因为**ner**组件默认使用**spacy.TransitionBasedParser.v2**模型架构。正如spaCy文档所说（[https://spacy.io/api/architectures#TransitionBasedParser](https://spacy.io/api/architectures#TransitionBasedParser)），模型由两个或三个子网络组成。**tok2vec**组件将每个标记转换为一个向量，每次批量运行一次。**Lower**为每个批次的**（标记，特征）**对生成一个向量，结合特征并应用非线性以形成状态表示。**Upper**是一个可选的前馈网络，它从状态表示中推导出分数。如果省略，则直接使用底层模型的输出作为动作分数。总之，**tok2vec**组件负责将标记映射到向量表示。
- en: Evaluating the accuracy of the NER component
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估NER组件的精度
- en: 'We can use the **spacy evaluate** CLI command to evaluate the accuracy of the
    pipeline. The command expects a loadable spaCy pipeline and evaluation data in
    the **DocBin** format. We don’t have a test dataset so we will use the **fashion_brands_eval.spacy**
    dataset just to see how the command works. The results are then **overestimated**
    because the dataset was also used to decide when to stop the model training. This
    is called **data leakage** . However, here, we are training the model for learning
    purposes so we’re fine. Let’s run the command and check the results:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**spacy evaluate** CLI命令来评估管道的精度。该命令期望一个可加载的spaCy管道和**DocBin**格式的评估数据。我们没有测试数据集，所以我们将使用**fashion_brands_eval.spacy**数据集只是为了看看命令是如何工作的。由于数据集也被用来决定何时停止模型训练，所以结果会被高估。这被称为**数据泄露**。然而，在这里，我们是为了学习目的而训练模型，所以我们没问题。让我们运行命令并检查结果：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The command displays the output and saves the metrics to the **training_cpu/metrics.json**
    file. *Figure 8* *.5* shows the results:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 命令会显示输出并将指标保存到**training_cpu/metrics.json**文件中。*图8.5*显示了结果：
- en: '![Figure 8.5 – Accuracy of the ner model](img/B22441_08_05.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – ner模型的精度](img/B22441_08_05.jpg)'
- en: Figure 8.5 – Accuracy of the ner model
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – ner模型的精度
- en: The precision score is **76.88** and the recall is **55.88** . This is a model
    trained on the CPU; what if we train it with GPU and use transformers? spaCy makes
    it simple to try this; we just need to create a new **config.cfg** file and run
    the **spacy train** command again. Let’s do that in the next section.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 精度得分为**76.88**，召回率为**55.88**。这是一个在CPU上训练的模型；如果我们用GPU训练它并使用transformers会怎样呢？spaCy使尝试这一点变得简单；我们只需要创建一个新的**config.cfg**文件，然后再次运行**spacy
    train**命令。让我们在下一节中这样做。
- en: Training an NER component optimized for accuracy
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练一个针对精度优化的NER组件
- en: 'Training a component optimized for accuracy and that runs on a GPU requires
    that you install the **spacy-transformers** library. After that, we can create
    the new config file like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个针对精度优化且在GPU上运行的组件需要您安装**spacy-transformers**库。之后，我们可以创建新的配置文件，如下所示：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we can train the model again, now pointing to this **gpu_config.cfg**
    file:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以再次训练模型，这次指向这个**gpu_config.cfg**文件：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'My machine has a GPU so I’m setting it using the **–gpu-id 1** parameter. When
    we train for accuracy, usually, the model behaves better but we also end up with
    a potentially larger and slower model. *Figure 8* *.6* shows the messages produced
    during training:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我的机器有一个GPU，所以我使用**–gpu-id 1**参数来设置它。当我们为了精度而训练时，通常模型的表现会更好，但我们也可能得到一个更大、更慢的模型。*图8.6*显示了训练过程中产生的消息：
- en: '![Figure 8.6 – Messages produced during the training of the ner component for
    accuracy](img/B22441_08_06.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 训练NER组件以实现精度时产生的消息](img/B22441_08_06.jpg)'
- en: Figure 8.6 – Messages produced during the training of the ner component for
    accuracy
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 训练NER组件以实现精度时产生的消息
- en: 'This architecture uses a transformer Model component instead of the **tok2vec**
    component. We can see that the scores of this pipeline at each evaluation point
    are higher, always above **0.78** . Let’s run the evaluation again to compare
    the models:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构使用了一个transformer Model组件而不是**tok2vec**组件。我们可以看到，在每个评估点，这个管道的分数都更高，始终高于**0.78**。让我们再次运行评估来比较模型：
- en: '[PRE18]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The command displays the output and saves the metrics to the **training_gpu/metrics.json**
    file. *Figure 8* *.7* shows the results:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 命令显示输出并将指标保存到**training_gpu/metrics.json**文件。*图8.7*显示了结果：
- en: '![Figure 8.7 – Accuracy of the ner model trained for accuracy](img/B22441_08_07.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – 为准确性训练的ner模型的准确性](img/B22441_08_07.jpg)'
- en: Figure 8.7 – Accuracy of the ner model trained for accuracy
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 为准确性训练的ner模型的准确性
- en: 'The precision score is **88.79** and the recall is **79.83** . This is a significant
    increase if we compare it with the **76.88** and **55.88** results of the model
    trained for efficiency. We can load the models for usage by passing the directories
    created during training to the **nlp.load()** method. Let’s get the entities of
    the sentence **Givenchy is looking at buying U.K. startup for $** **1 billion**
    :'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率为**88.79**，召回率为**79.83**。如果我们将其与为效率训练的模型的**76.88**和**55.88**结果进行比较，这是一个显著的增加。我们可以通过将训练期间创建的目录传递给**nlp.load()**方法来加载模型以供使用。让我们获取句子**Givenchy
    is looking at buying U.K. startup for $** **1 billion** 的实体：
- en: 'First, we load the libraries and the pipeline:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们加载库和管道：
- en: '[PRE19]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we can apply the pipeline to the text:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以将管道应用于文本：
- en: '[PRE20]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The result is displayed in *Figure 8* *.8* ; the pipeline was able to find
    the **Givenchy** entity:'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示在*图8.8*；管道能够找到**Givenchy**实体：
- en: '![Figure 8.8 – The entities extracted using the ner pipeline trained for accuracy](img/B22441_08_08.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – 使用为准确性训练的ner管道提取的实体](img/B22441_08_08.jpg)'
- en: Figure 8.8 – The entities extracted using the ner pipeline trained for accuracy
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 使用为准确性训练的ner管道提取的实体
- en: 'The component can only identify **FASHION_BRAND** entities, which was the only
    entity we’ve trained it for. But the **en_core_web_sm** model made available by
    spaCy can identify more entities. Let’s check it out:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 该组件只能识别**FASHION_BRAND**实体，这是我们唯一为其训练的实体。但spaCy提供的**en_core_web_sm**模型可以识别更多实体。让我们来看看：
- en: 'Load the libraries and the model:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载库和模型：
- en: '[PRE21]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Apply the pipeline to the text:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将管道应用于文本：
- en: '[PRE22]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The result is displayed in *Figure 8* *.9* ; the pipeline was able to find
    the **GPE** and **MONEY** entities:'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示在*图8.9*；管道能够找到**GPE**和**MONEY**实体：
- en: '![Figure 8.9 – The entities extracted using the en_core_web_sm pipeline](img/B22441_08_09.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图8.9 – 使用en_core_web_sm管道提取的实体](img/B22441_08_09.jpg)'
- en: Figure 8.9 – The entities extracted using the en_core_web_sm pipeline
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – 使用en_core_web_sm管道提取的实体
- en: What if we want to extract both the entities we’ve trained for and the entities
    the **en_core_web_sm** NER component is trained for? We can do that by combining
    multiple NER components in the same pipeline.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要提取我们训练过的实体以及**en_core_web_sm** NER组件训练过的实体呢？我们可以通过在同一个管道中结合多个NER组件来实现这一点。
- en: Combining multiple NER components in the same pipeline
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在同一个管道中结合多个NER组件
- en: When spaCy loads a pipeline, it iterates over the pipeline names and looks up
    each component name in the **[components]** block. The components can be built
    either using **factory** or **source** . **Factories** are named functions that
    take settings and return a pipeline component function, and the source is used
    to reference the name of the path of a trained pipeline to copy components from.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 当spaCy加载一个管道时，它会遍历管道名称，并在**[components]**块中查找每个组件名称。组件可以通过**factory**或**source**来构建。**Factories**是命名函数，接受设置并返回一个管道组件函数，而**source**用于引用一个训练管道的路径名称，以便从中复制组件。
- en: To use both the NER component we’ve trained and the NER component of **en_core_web_sm**
    , we will create a new **config.cfg** file and source the NER components in the
    pipeline. With this **config.cfg** file set, we will then use the **spacy assemble**
    command to assemble the config without additional training. To be able to reference
    the trained NER component using **spacy assemble** , we will create a package
    for the pipeline and install it with **pip install** .
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用我们训练过的NER组件以及**en_core_web_sm**的NER组件，我们将创建一个新的**config.cfg**文件，并在管道中引用NER组件。使用这个**config.cfg**文件设置后，我们将使用**spacy
    assemble**命令来组装配置，而无需额外的训练。为了能够使用**spacy assemble**引用训练过的NER组件，我们将为管道创建一个包，并使用**pip
    install**安装它。
- en: Creating a package for the trained pipeline
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为训练的管道创建包
- en: The **spacy package** CLI command generates an installable Python package from
    an existing pipeline directory. spaCy creates a build artifact that you can install
    with **pip install** . This is useful if you want to deploy your pipeline in a
    production environment or share it with others.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**spacy包** CLI命令从现有的管道目录生成一个可安装的Python包。spaCy创建了一个可以与**pip install**一起安装的构建工件。如果您想在生产环境中部署您的管道或与他人共享，这很有用。'
- en: 'We will provide the path to the directory containing pipeline data, the path
    to the directory in which to create the package folder, and the package name.
    This is the full command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提供包含管道数据的目录路径，创建包文件夹的目录路径以及包名称。这是完整的命令：
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The command creates a folder named **en_ner_fashion_brands-0.0.0** . We can
    now install this package and load it in Python:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 命令创建了一个名为**en_ner_fashion_brands-0.0.0**的文件夹。我们现在可以安装这个包并在Python中加载它：
- en: 'Install the package:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装包：
- en: '[PRE24]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Import the package, load the pipeline, and apply it to the text:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入包，加载管道，并将其应用于文本：
- en: '[PRE25]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It works just like loading the pipeline from its directory. Now, we can point
    to the NER component of this pipeline in a new **config** file to create a pipeline
    that uses this NER component and the **en_core_web_sm** NER component.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作方式就像从其目录中加载管道一样。现在，我们可以在新的**config**文件中指向这个管道的NER组件，以创建一个使用此NER组件和**en_core_web_sm**
    NER组件的管道。
- en: Creating a pipeline with different NER components
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建具有不同NER组件的管道
- en: 'The **spacy assemble** CLI command assembles a pipeline from a **config** file
    without additional training. Simple as that. This **config** file is simpler since
    it’s used only to build the pipeline and not to train it. Let’s create the **combined_ner.cfg**
    file:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**spacy assemble** CLI命令从**config**文件组装管道，无需额外训练。就这么简单。这个**config**文件更简单，因为它仅用于构建管道，而不是用于训练。让我们创建**combined_ner.cfg**文件：'
- en: 'First, we define the **nlp** object. We will specify the pipeline language
    ISO code and the pipeline components’ names in order. We will specify how to load
    them in the **[components]** section, so we can name them as we want:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义**nlp**对象。我们将指定管道语言ISO代码和管道组件的名称顺序。我们将在**[components]**部分指定如何加载它们，这样我们就可以按我们的意愿命名它们：
- en: '[PRE26]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Since we want to copy the components from existing pipelines, we will set the
    **source** argument of each component. The argument values can be a loadable spaCy
    pipeline package or a path. After defining, we set the **component** argument,
    which is the name of the component in the **source** pipeline. Let’s first define
    how to load the **ner** component of the **en_core_web_sm** pipeline:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们想要从现有管道中复制组件，我们将设置每个组件的**source**参数。参数值可以是可加载的spaCy管道包或路径。定义后，我们设置**component**参数，这是**source**管道中组件的名称。让我们首先定义如何加载**en_core_web_sm**管道的**ner**组件：
- en: '[PRE27]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, to load the **ner** component we’ve trained to identify the fashion brands,
    we also need to include a copy of the **tok2vec** listener of the **source** pipeline
    using the **replace_listeners** argument, like this:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了加载我们训练来识别时尚品牌的**ner**组件，我们还需要使用**replace_listeners**参数包括**source**管道的**tok2vec**监听器的副本，如下所示：
- en: '[PRE28]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now, we have a **config** file to assemble our final pipeline. We’ll do that
    next.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个用于组装最终管道的**config**文件。我们将在下一步做这件事。
- en: Assembling the pipeline from a config file
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从配置文件组装管道
- en: 'To assemble the pipeline, we will provide the **combined_ner.cfg** file we
    have created and the output directory to store the final pipeline:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 要组装管道，我们将提供我们创建的**combined_ner.cfg**文件和存储最终管道的输出目录：
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If everything worked correctly, we could now load this pipeline and it will
    identify all the entities of both **ner** components. Let’s see whether that is
    the case:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常工作，我们现在可以加载这个管道，它将识别出两个**ner**组件的所有实体。让我们看看这是否是情况：
- en: 'Let’s first load the pipeline we’ve assembled:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先加载我们组装的管道：
- en: '[PRE30]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, let’s process the text and show the entities with **displacy** :'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们处理文本并使用**displacy**显示实体：
- en: '[PRE31]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can see the results in *Figure 8* *.10* :'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以在*图8.10*中看到结果：
- en: '![Figure 8.10 – Entities identified with the assembled pipeline #TODO: take
    new screenshot](img/B22441_08_10.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图8.10 – 使用组装的管道识别的实体 #TODO：获取新的截图](img/B22441_08_10.jpg)'
- en: 'Figure 8.10 – Entities identified with the assembled pipeline #TODO: take new
    screenshot'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '图8.10 – 使用组装的管道识别的实体 #TODO：获取新的截图'
- en: Neat, right? We could combine as many **ner** components as we need.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 真是 neat，对吧？我们可以组合我们需要的任意数量的**ner**组件。
- en: Summary
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored how to train spaCy NER components with our own
    domain and data. First, we learned the key points of deciding whether we really
    need custom model training. Then, we went through an essential part of model training
    – data collection and labeling.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何使用我们自己的领域和数据来训练 spaCy NER 组件。首先，我们学习了决定我们是否真的需要自定义模型训练的关键点。然后，我们了解了模型训练的一个关键部分——数据收集和标注。
- en: We learned about two annotation tools – Prodigy and **nertk** – and learned
    how to convert the data for training and how to train the component using spaCy’s
    CLI. Then, we used spaCy CLI commands to train the component and create a Python
    package for the pipeline.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解了两种标注工具——Prodigy 和 **nertk**——并学习了如何将数据转换为训练格式，以及如何使用 spaCy 的 CLI 来训练组件。接着，我们使用
    spaCy CLI 命令来训练组件并创建用于管道的 Python 包。
- en: Finally, we learned how to combine different NER components into a single pipeline.
    In the next chapter, we will learn how to manage and share end-to-end workflows
    for different use cases and domains using spaCy and Weasel.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了如何将不同的 NER 组件组合成一个单一的管道。在下一章中，我们将学习如何使用 spaCy 和 Weasel 管理和共享针对不同用例和领域的端到端工作流程。
