- en: Generating Matching Shoe Bags from Shoe Images Using DiscoGANs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DiscoGAN 从鞋图像生成匹配的鞋包
- en: Human beings are quite smart when it comes to understanding the relationship
    between different domains. For example, we can easily understand the relationship
    between a Spanish sentence and its translated version in English. We can even
    guess which color tie to wear to match a certain kind of suit. While it seems
    easy for humans, this is not a straightforward process for machines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在理解不同领域之间的关系方面相当聪明。例如，我们可以轻松理解西班牙语句子及其英文翻译之间的关系。我们甚至能猜出穿什么颜色的领带来搭配某种西装。虽然这对人类来说似乎很简单，但对机器来说却不是一个直接的过程。
- en: The task of style transfer across different domains for machines can be framed
    as a conditional image generation problem. Given an image from one domain, can
    we learn to map to an image from a different domain.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 跨领域的风格迁移任务可以被框定为一个条件图像生成问题。给定一个来自某个领域的图像，我们能否学会将其映射到来自另一个领域的图像？
- en: While there have been many approaches to achieve this using pairwise labeled
    data from two different domains, these approaches are fraught with problems. The
    major issue with these approaches is obtaining the pairwise labeled data, which
    is both an expensive and time-consuming process.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多方法尝试使用来自两个不同领域的成对标注数据来实现这一目标，但这些方法充满了问题。这些方法的主要问题在于获取成对标注数据，这既是一个昂贵又耗时的过程。
- en: In this chapter, we will learn about an approach for learning style transfer
    without explicitly providing pairwise labeled data to the algorithm. This approach,
    known as DiscoGANs, is highlighted in the recently released paper by Kim et. al 
    named *Learning to Discover Cross-Domain Relations with Generative Adversarial
    Networks* ([https://arxiv.org/pdf/1703.05192.pdf](https://arxiv.org/pdf/1703.05192.pdf)). Specifically,
    we will try to generate matching shoes from shoe bag images.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习一种无需明确提供成对标注数据给算法的风格迁移方法。这种方法被称为 DiscoGAN，最近由 Kim 等人在论文《*利用生成对抗网络学习发现跨域关系*》中提出
    ([https://arxiv.org/pdf/1703.05192.pdf](https://arxiv.org/pdf/1703.05192.pdf))。具体来说，我们将尝试从鞋包图像中生成匹配的鞋子。
- en: 'The remainder of this chapter is organized as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的剩余部分组织结构如下：
- en: Introduction to **Generative Adversarial Networks** (**GANs**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络（GANs）** 介绍'
- en: What are DiscoGANs?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 DiscoGAN？
- en: How to generate matching shoes from shoe bag images and vice versa
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从鞋包图像生成匹配的鞋子，反之亦然
- en: Understanding generative models
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解生成模型
- en: An unsupervised learning model that learns the underlying data distribution
    of the training set and generates new data that may or may not have variations
    is commonly known as a **generative model**. Knowing the true underlying distribution
    might not always be a possibility, hence the neural network trains on a function
    that tries to be as close a match as possible to the true distribution.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一种无监督学习模型，它学习训练集的基础数据分布并生成可能具有或不具有变化的新数据，通常被称为**生成模型**。知道真实的基础分布可能并不总是可行，因此神经网络会训练一个尽可能接近真实分布的函数。
- en: 'The most common methods used to train generative models are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 训练生成模型最常用的方法如下：
- en: '**Variational autoencoders: **A high dimensional input image is encoded by
    an auto-encoder to create a lower dimensional representation. During this process,
    it is of the utmost importance to preserve the underlying data distribution. This
    encoder can only be used to map to the input image using a decoder and cannot
    introduce any variability to generate similar images. The VAE introduces variability
    by generating constrained latent vectors that still follow the underlying distribution.
    Though VAEs help in creating probabilistic graphical models, the generated images
    tend to be slightly blurry.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变分自编码器（VAE）：** 一个高维输入图像通过自编码器编码，创建一个低维的表示。在这个过程中，最重要的是保持数据的基础分布。这个编码器只能通过解码器来映射输入图像，不能引入任何变异来生成相似的图像。VAE通过生成受约束的潜在向量来引入变异，这些向量仍然遵循基础分布。尽管VAE有助于创建概率图模型，但生成的图像往往会略显模糊。'
- en: '**PixelRNN/PixelCNN: **These auto-regressive models are used to train networks
    that model the conditional distribution of a successive individual pixel, given
    previous pixels starting from the top left. RNNs move horizontally and vertically
    over any image. The training for PixelRNNs is a stable and simple process with
    better log likelihoods than other models, but they are time consuming and relatively
    inefficient.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PixelRNN/PixelCNN：**这些自回归模型用于训练能够建模条件分布的网络，给定从左上角开始的前几个像素，预测下一个像素。RNNs 可以在图像中横向和纵向移动。PixelRNN
    的训练过程稳定且简单，相较于其他模型，它具有更好的对数似然值，但训练时间较长且效率较低。'
- en: '**Generative adversarial networks: **Generative adversarial networks were first
    published in the 2014 paper by *Goodfellow et al.* ([https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)).
    These can be thought of as a competition framework with two adversaries called
    the generator and the discriminator. These are nothing but two differentiable
    functions in the form of neural networks. The generator takes a randomly generated
    input known as a latent sample and produces an image. The overall objective of
    the generator is to generate an image that is as close as possible to the real
    input image (such as MNIST digits) and give it as an input to the discriminator.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络：**生成对抗网络最早由 *Goodfellow 等人* 在 2014 年的论文中提出 ([https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661))。它们可以被看作是一个竞争框架，包含两个对立的部分：生成器和判别器。这两个部分实际上是两个可微分的神经网络函数。生成器接受一个随机生成的输入，称为潜在样本，并生成一张图像。生成器的整体目标是生成一张尽可能接近真实输入图像（如
    MNIST 数字）的图像，并将其作为输入提供给判别器。'
- en: The discriminator is essentially a classifier that's trained to distinguish
    between real images (original MNIST digits) and fake images (output of the generator).
    Ideally, after being trained, the generator should adapt its parameters and capture
    the underlying training data distribution and fool the discriminator about its
    input being a real image.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器本质上是一个分类器，经过训练后用于区分真实图像（原始的 MNIST 数字）和伪造图像（生成器的输出）。理想情况下，经过训练后，生成器应当调整其参数，捕捉潜在的训练数据分布，并让判别器误认为输入是一个真实图像。
- en: Let's consider an analogy that is inspired by the real world. Imagine that GANs
    work like the relationship between a forger making counterfeit currency and the
    police identifying and discarding that forged currency. The aim of the forger
    is to try and pass off the fake currency as real currency in the market. This
    is analogous to what a generator tries to do. The police try and inspect every
    currency note it can, accepting the original notes and scrapping the fake ones.
    The police know of the details of the original currency and compare it to the
    properties of the currency in question in order to make a decision regarding its
    authenticity. If there is a match, the currency is retained; otherwise, it is
    scrapped. This is in line with the work of a discriminator.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个来自现实世界的类比。想象 GAN 的工作方式就像伪造者制造伪钞和警察识别并销毁这些伪钞之间的关系。伪造者的目的是试图将伪钞当作真钱在市场上流通。这就类似于生成器的工作。警察会检查每一张钞票，接受真实的钞票，销毁伪钞。警察了解真实钞票的特征，并将其与待检验钞票的属性进行比较，以判断其真伪。如果匹配，钞票会被保留，否则就会被销毁。这与判别器的工作原理类似。
- en: Training GANs
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 GAN
- en: 'The following diagram illustrates the basic architecture of GANs:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 GAN 的基本架构：
- en: '![](img/6296918a-8ac6-4574-b1ff-2368e6946b4c.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6296918a-8ac6-4574-b1ff-2368e6946b4c.png)'
- en: A random input is used to generate a sample of data. For example, a generator,
    *G(z)*, uses a prior distribution, *p(z)*, to achieve an input, *z*. Using *z*,
    it then generates some data. This output is fed as input to the discriminator
    neural network, *D(x)*. It takes an input x from ![](img/1ff29fae-3f62-4e27-8ebe-6c1a91597318.png), where ![](img/49289ba1-2cdd-4375-9077-36f8649f8e3a.png)
    is our real data distribution. *D(x)* then solves a binary classification problem
    using the `sigmoid` function, which gives us an output in the range of 0 to 1.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机输入来生成数据样本。例如，生成器 *G(z)* 使用先验分布 *p(z)* 来生成输入 *z*，然后使用 *z* 生成一些数据。这个输出作为输入被传递到判别器神经网络
    *D(x)*。它从 ![](img/1ff29fae-3f62-4e27-8ebe-6c1a91597318.png) 中获取输入 *x*，其中 ![](img/49289ba1-2cdd-4375-9077-36f8649f8e3a.png)
    是我们的真实数据分布。然后，*D(x)* 使用 `sigmoid` 函数解决一个二分类问题，输出结果在 0 到 1 之间。
- en: 'GANS are trained to be part of a competition between the generator and discriminator.
    The objective function can be represented mathematically as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: GANs的训练是生成器和判别器之间的竞争过程。目标函数可以通过以下数学形式表示：
- en: '![](img/61972a78-46bc-4575-aa1b-c94c1fe6e907.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61972a78-46bc-4575-aa1b-c94c1fe6e907.png)'
- en: 'In this, the following applies:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，以下适用：
- en: '![](img/f619e329-5665-425b-a42f-227ca87bc47e.png) denotes the parameters of
    the discriminator'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/f619e329-5665-425b-a42f-227ca87bc47e.png) 表示判别器的参数'
- en: '![](img/ea4c4428-39c2-4afb-9c6c-2a4a6ac531ee.png) denotes the parameters of
    the generator'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/ea4c4428-39c2-4afb-9c6c-2a4a6ac531ee.png) 表示生成器的参数'
- en: '![](img/173c7e8d-6049-4dcb-8467-b7fa266f45ff.png)denotes the underlying distribution
    of training data'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/173c7e8d-6049-4dcb-8467-b7fa266f45ff.png) 表示训练数据的基础分布'
- en: '![](img/dc44753e-ecd8-423b-9633-8be92c39bbb5.png) denotes the discriminator
    operation over input images *x*'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/dc44753e-ecd8-423b-9633-8be92c39bbb5.png) 表示判别器对输入图像 *x* 的操作'
- en: '![](img/22d6d1fc-22dd-4ca2-9fcb-b1683de79bbc.png) denotes the generator operation
    over latent sample *z*'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/22d6d1fc-22dd-4ca2-9fcb-b1683de79bbc.png) 表示生成器在潜在样本 *z* 上的操作'
- en: '![](img/7fdda8a8-8bdf-4bf7-9683-c546acea033a.png) denotes the discriminator
    output for generated fake data ![](img/0b5145dd-c1da-404d-9ca8-c20abecc4498.png)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/7fdda8a8-8bdf-4bf7-9683-c546acea033a.png) 表示生成的假数据的判别器输出 ![](img/0b5145dd-c1da-404d-9ca8-c20abecc4498.png)'
- en: In the objective function, the first term from the left represents the cross
    entropy of the discriminator's output from a real distribution (![](img/71f08933-8f06-4736-a01b-d04cfe1736ef.png)).
    The second term from the left is the cross entropy between the random distribution
    (![](img/bebcff21-5a62-4571-94ec-1e3dfcde3341.png)) and one minus the prediction
    of the discriminator on the output of the generator that was generated using random
    sample z from ![](img/817e906e-f25e-40d8-b1e0-d3010aa96031.png). The discriminator
    tries to maximize both terms to classify the images as real and fake, respectively.
    On the other hand, the generator tries to fool the discriminator by minimizing
    this objective.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标函数中，左边的第一个项表示判别器输出来自真实分布的交叉熵（![](img/71f08933-8f06-4736-a01b-d04cfe1736ef.png)）。左边的第二个项是随机分布（![](img/bebcff21-5a62-4571-94ec-1e3dfcde3341.png)）与判别器对生成器输出的预测（该输出是使用来自
    ![](img/817e906e-f25e-40d8-b1e0-d3010aa96031.png) 的随机样本 z 生成的）的交叉熵。判别器试图最大化这两个项，分别将图像分类为真实和假的。另一方面，生成器则试图通过最小化此目标来欺骗判别器。
- en: 'In order to train GANs, gradient-based optimization algorithms, such as stochastic
    gradient descent, are used. Algorithmically, it flows as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练GAN，使用基于梯度的优化算法，例如随机梯度下降。算法流程如下：
- en: First, sample *m* noise samples and *m* real data samples.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，采样 *m* 个噪声样本和 *m* 个真实数据样本。
- en: Freeze the generator, that is, set the training as false so that the generator
    network only does a forward pass without any back propagation. Train the discriminator
    on this data.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结生成器，即将训练设置为false，以便生成器网络仅进行前向传播而不进行反向传播。然后对这些数据训练判别器。
- en: Sample different *m* noise samples.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对不同的 *m* 噪声样本进行采样。
- en: Freeze the discriminator and train the generator on this data.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 冻结判别器，并在这些数据上训练生成器。
- en: Iterate through the preceding steps.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复执行前述步骤。
- en: Formally, the pseudocode is as follows.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，伪代码如下所示。
- en: 'In this example, we are performing mini-batch stochastic gradient descent training
    of generative adversarial nets. The number of steps to apply to the discriminator,
    *k*, is a hyper parameter. We used *k=1*, the least expensive option, in our experiment:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们正在对生成对抗网络进行小批量随机梯度下降训练。应用于判别器的步数 *k* 是一个超参数。我们在实验中使用了 *k=1*，这是最不昂贵的选项：
- en: '![](img/193ed986-2232-4d70-8c9b-0849d7207c54.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/193ed986-2232-4d70-8c9b-0849d7207c54.png)'
- en: Pseudocode for GAN training. With k=1, this equates to training D, then G, one
    after the other. Adapted from Goodfellow et al. 2014
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: GAN训练的伪代码。对于 k=1，这等价于依次训练 D 和 G。改编自 Goodfellow 等人，2014年
- en: The gradient-based updates can use any standard gradient-based learning rule.
    We used momentum in our experiment.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的更新可以使用任何标准的梯度学习规则。我们在实验中使用了动量。
- en: Applications
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用
- en: Some of the applications of GANs include converting monochrome or black and
    white images into colored images, filling additional details in an image, such
    as the insertion of objects into a partial image or into an image with only edges,
    and constructing images representing what somebody would look like when they are
    older given an image of their present self.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GANs的一些应用包括将单色或黑白图像转换为彩色图像、在图像中填充附加细节，如将物体插入到部分图像或仅包含边缘的图像中，以及构建一个人的老年形象，基于其当前的图像。
- en: Challenges
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: Though GANs generate one of the sharpest images from a given piece of input
    data, their optimization is difficult to achieve due to unstable training dynamics.
    They also suffer from other challenges, such as mode collapse and bad initialization.
    Mode collapse is a phenomena where, if the data is multimodal, the generator is
    never incentivized to cover both modes, which leads to lower variability among
    generated samples and, hence, lower utility of GANs. If all generated samples
    start to become identical, it leads to complete collapse. In cases where most
    of the samples show some commonality, there is partial collapse of the model.
    At the core of this, GANs work on an objective function that aims to achieve optimization
    of min-max, but if the initial parameters end up being inefficient, then it becomes
    an oscillating process with no true optimization. In addition to this, there are
    issues such as GANs failing to differentiate the count of particular objects that
    should occur at a location. For example, GANs have no idea that there can't be
    more than two eyes and can generate images of human faces with 3 eyes.  There
    are also issues with GANs being unable to adapt to a 3D perspective, such as front
    and posterior view. This gives a flat 2D image instead of depth for a 3D object.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GANs能够从给定的输入数据生成非常清晰的图像，但由于训练动态的不稳定，其优化是很难实现的。它们还面临其他挑战，如模式崩溃和初始化问题。模式崩溃是指当数据是多模态时，生成器从未被激励去覆盖所有模式，导致生成样本的变化性较低，从而降低了GANs的实用性。如果所有生成的样本开始变得相同，就会导致完全崩溃。在大多数样本表现出一些共性时，模型会出现部分崩溃。在这一过程中，GANs通过一个旨在实现最小最大优化的目标函数进行工作，但如果初始参数无效，就会变成一个无真正优化的振荡过程。此外，还有像GANs无法区分在某个位置应该出现的特定物体数量等问题。例如，GANs无法理解不能有超过两个眼睛，并且可能生成带有三个眼睛的人脸图像。还有GANs无法适应三维视角的问题，比如前视图和后视图。这会导致产生平面2D图像，而不是三维物体的深度效果。
- en: 'Different variants of GANs have evolved over time. Some of them are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的GAN变体随着时间的推移不断发展。以下是其中的一些：
- en: '**Deep convolutional GANs** (**DCGANs**) were one of the first major improvements
    on the GAN architecture. It is made up of convolutional layers that avoid the
    use of max pooling or fully connected layers. The convolutional stride and transposed
    convolution for downsampling and upsampling is majorly used by this. It also uses
    ReLU activation in the generator and LeakyReLU in the discriminator.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度卷积生成对抗网络** (**DCGANs**) 是GAN架构的一次重要改进。它由卷积层组成，避免了最大池化或全连接层的使用。该架构主要使用卷积步幅和反卷积来进行下采样和上采样。它还在生成器中使用ReLU激活函数，在判别器中使用LeakyReLU。'
- en: '**InfoGANs** are another variant of GANs that try and encode meaningful features
    of the image (for example, rotation) in parts of the noise vector, z.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**InfoGANs** 是另一种GAN变体，它尝试将图像的有意义特征（例如旋转）编码到噪声向量z的部分中。'
- en: '**Conditional GANs** (**cGANs**) use extra conditional information that describes
    some aspect of the data as input to the generator and discriminator. For example,
    if we are dealing with vehicles, the condition could describe attributes such
    as four-wheeled or two-wheeled. This helps generate better samples and additional
    features. In this chapter, we will mainly focus on DiscoGANs, which are described
    in the following section.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条件生成对抗网络** (**cGANs**) 使用额外的条件信息，描述数据的某些方面，作为输入提供给生成器和判别器。例如，如果我们处理的是车辆，条件信息可能描述四轮或两轮等属性。这有助于生成更好的样本和附加特征。在本章中，我们将主要关注DiscoGAN，它将在接下来的部分中进行描述。'
- en: Understanding DiscoGANs
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解DiscoGANs
- en: In this section, we are mainly going to take a closer look at Discovery GANS,
    which are popularly known as **DiscoGANs**.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点深入了解Discovery GANs，它们通常被称为**DiscoGANs**。
- en: Before going further, let's try to understand reconstruction loss in machine
    learning, since this is one of the concepts that this chapter is majorly dependent
    on. When learning about the representation of an unstructured data type such as
    an image/text, we want our model to encode the data in such a manner that when
    it's decoded, the underlying image/text can be generated back. To incorporate
    this condition in the model explicitly, we use a reconstruction loss (essentially
    the Euclidean distance between the reconstructed and original image) in training
    the model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究之前，让我们先尝试理解机器学习中的重构损失，因为这是本章主要依赖的概念之一。当我们学习无结构数据类型（如图像/文本）的表示时，我们希望模型以一种方式对数据进行编码，使得当它被解码时，可以恢复出底层的图像/文本。为了在模型中显式地加入这个条件，我们在训练模型时使用重构损失（本质上是重构图像与原始图像之间的欧几里得距离）。
- en: Style transfer has been one of the most prominent use cases of GANs. Style transfer
    basically refers to the problem where, if you are given an image/data in one domain,
    is it possible to successfully generate an image/data in another domain. This
    problem has become quite famous among several researchers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 风格迁移一直是GAN最显著的应用之一。风格迁移基本上是指在给定一个领域中的图片/数据的情况下，是否可以成功生成另一个领域中的图片/数据。这个问题在许多研究人员中变得非常著名。
- en: 'You can read more about style transfer problems from the paper  Neural Style
    Transfer: A Review  ([https://arxiv.org/abs/1705.04058](https://arxiv.org/abs/1705.04058))
    by Jing et. al. However, most of the work is done by using an explicitly paired
    dataset that''s generated by humans or other algorithms. This puts limitations
    on these approaches, since paired data is seldom available and is too costly to
    generate.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 Jing 等人的论文《神经风格迁移：综述》（[https://arxiv.org/abs/1705.04058](https://arxiv.org/abs/1705.04058)）中了解更多关于风格迁移问题的内容。然而，大多数工作都是通过使用一个由人工或其他算法生成的显式配对数据集来完成的。这给这些方法带来了限制，因为配对数据很少能够获得，而且生成配对数据的成本太高。
- en: DiscoGANs, on the other hand, propose a method of learning cross-domain relations
    without the explicit need for paired datasets. This method takes an image from
    one domain and generates the corresponding image from the other domain. Let's
    say we are trying to transfer an image from Domain A to Domain B. During the learning
    process, we force the generated image to be the image-based representation of
    the image from Domain A through a reconstruction loss and to be as close to the
    image in Domain B as possible through a GAN loss, as mentioned earlier. Essentially,
    this approach tends to generate a bijective (one-to-one) mapping between two domains,
    rather than a many-to-one or one-to-many mapping.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，DiscoGAN提出了一种学习跨领域关系的方法，无需显式的配对数据集。该方法从一个领域中获取一张图片，并生成另一个领域中的对应图片。假设我们正尝试将Domain
    A中的图片迁移到Domain B。在学习过程中，我们通过重构损失强制生成的图像成为Domain A图像的图像表示，并通过GAN损失使其尽可能接近Domain
    B中的图像，如前所述。本质上，这种方法倾向于在两个领域之间生成一个双射（一对一）映射，而不是多对一或一对多的映射。
- en: Fundamental units of a DiscoGAN
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DiscoGAN的基本单元
- en: 'As mentioned previously, normal GANs have a generator and a discriminator.
    Let''s try to understand the building blocks of DiscoGANs and then proceed to
    understand how to combine them so that we can learn about cross-domain relationships.
    These are as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，普通的GAN包含生成器和判别器。我们来尝试理解DiscoGAN的构建块，然后再继续理解如何将它们结合起来，从而学习跨领域关系。具体包括以下内容：
- en: '**Generator:** In the original GANs, the generator would take an input vector *z* randomly
    sampled from, say, Gaussian distribution, and generate fake images. In this case,
    however, since we are looking to transfer images from one domain to another, we
    replace the input vector *z* with an image. Here are the parameters of the generator
    function:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成器：** 在原始GAN中，生成器会从一个输入向量*z*（例如，随机从高斯分布中采样）生成伪造的图像。然而，在这种情况下，由于我们希望将图像从一个领域转移到另一个领域，我们将输入向量*z*替换为一张图片。以下是生成器函数的参数：'
- en: '| **Parameters** | **Value** |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **值** |'
- en: '| Input image size | 64x64x3 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 输入图片尺寸 | 64x64x3 |'
- en: '| Output image size | 64x64x3 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 输出图片尺寸 | 64x64x3 |'
- en: '| # Convolutional layers | 4 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| # 卷积层 | 4 |'
- en: '| # Conv transpose/Deconv layers | 4 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| # 卷积转置/反卷积层 | 4 |'
- en: '| Normalizer function | Batch Normalization |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 归一化函数 | 批量归一化 |'
- en: '| Activation function | LeakyReLU |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | LeakyReLU |'
- en: Before specifying the structure of each particular layer, let's try to understand
    a few of the terms that were mentioned in the parameters.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定每个特定层的结构之前，让我们先尝试理解一下在参数中提到的几个术语。
- en: '**Transposed convolution: **As we mentioned previously, generators are used
    to generate images from the input vector. In our case, the input image is first
    convolved by 4 convolutional layers, which produce an embedding. Generating an
    image from the embedding involves upsampling from a low resolution to a higher
    resolution.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转置卷积：**正如我们之前提到的，生成器用于从输入向量生成图像。在我们的案例中，输入图像首先通过4层卷积层进行卷积，生成一个嵌入。通过上采样将低分辨率图像转换为高分辨率图像，从嵌入中生成图像。'
- en: General ways of upsampling include manual feature engineering to interpolate
    lower dimensional images. A better approach could be to employ Transposed Convolution, also
    known as the **fractional stride convolution**/**deconvolution**. It doesn't employ
    any predefined interpolation method. Let's say that we have a 4x4 matrix that
    is convolved with a 3x3 filter (stride 1 and no padding); this will result in
    a matrix of size 2x2\. As you can see, we downsampled the original image from
    4x4 to 2x2\. The process of going from 2x2 back to 4x4 can be achieved through
    transposed convolution.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的上采样方法包括手动特征工程，通过插值来处理低维度图像。一种更好的方法是采用转置卷积，也称为**分数步幅卷积**/**反卷积**。它不使用任何预定义的插值方法。假设我们有一个4x4矩阵，它与一个3x3的滤波器进行卷积（步幅为1，且不做填充）；这将得到一个2x2的矩阵。如你所见，我们将原始图像从4x4降采样到2x2。从2x2恢复到4x4的过程可以通过转置卷积实现。
- en: From an implementation perspective, the built-in function in TensorFlow for
    defining convolutional layers can be used directly with the `num_outputs` value,
    which can be changed to perform upsampling.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从实现角度来看，TensorFlow中用于定义卷积层的内置函数可以直接与`num_outputs`值一起使用，`num_outputs`可以进行更改以执行上采样。
- en: '**Batch normalization**: This method is used to counter the internal covariance
    shift that happens in deep neural networks.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量归一化：** 这种方法用于对抗深度神经网络中发生的内部协方差偏移。'
- en: A covariance shift can be defined as when the model is able to predict when
    the distribution of inputs changes. Let's say we train a model to detect black
    and white images of dogs. During the inference phase, if we supply colored images
    of dogs to the model, it will not perform well. This is because the model learned
    the parameters based on black and white images, which are not suitable for predicting
    colored images.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差偏移可以定义为当模型能够预测输入分布变化时。例如，假设我们训练一个模型来检测黑白狗的图像。在推理阶段，如果我们向模型提供彩色狗的图像，模型的表现将不佳。这是因为模型根据黑白图像学习了参数，而这些参数不适用于预测彩色图像。
- en: Deep neural networks experience what is known as an internal covariance shift,
    since changes in the parameters of an internal layer change the distribution of
    input in the next layer. To fix this issue, we normalize the output of each batch
    by using its mean and variance, and pass on a weighted combination of mean and
    variance to the next layer. Due to the weighted combination, batch normalization
    adds two extra parameters in each layer of the neural network.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络会经历所谓的内部协方差偏移，因为内部层参数的变化会改变下一层输入的分布。为了解决这个问题，我们通过使用每个批次的均值和方差来规范化输出，并将均值和方差的加权组合传递给下一层。由于加权组合，批量归一化在每一层的神经网络中添加了两个额外的参数。
- en: Batch normalization helps speed up training and tends to reduce overfitting
    because of its regularization effects.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 批量归一化有助于加速训练，并因其正则化效果而倾向于减少过拟合。
- en: In this model, we use batch normalization in all of the convolutional and convolutional
    transpose layers, except the first and the last layers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，我们在所有卷积层和卷积转置层中使用批量归一化，除了第一层和最后一层。
- en: '**Leaky ReLU: ****ReLU**, or **rectified linear units**, are quite popular
    in the deep learning domain today as activation functions. ReLU units in deep
    neural networks can be fragile at times since they can cause neurons to die or
    never get activated again at any data point. The following diagram illustrates
    the ReLU function:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Leaky ReLU: ** **ReLU**（或称**修正线性单元**）在深度学习领域作为激活函数非常流行。深度神经网络中的ReLU单元有时会变得脆弱，因为它们可能导致神经元死亡或在任何数据点上都无法再次激活。下图展示了ReLU函数：'
- en: '![](img/96327a14-715d-473b-a725-fbc12a33026e.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96327a14-715d-473b-a725-fbc12a33026e.png)'
- en: 'Leaky ReLU is used to try and fix this problem. They have small negative values
    instead of zeros for negative input values. This avoids the dying issue with regard
    to neurons. The following diagram illustrates a sample Leaky ReLU function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Leaky ReLU来尝试解决这个问题。对于负输入值，它们具有小的负值，而不是零。这避免了神经元死掉的问题。以下图示展示了一个Leaky ReLU函数的示例：
- en: '![](img/89a49618-25a6-4d85-9714-8b94641120cd.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89a49618-25a6-4d85-9714-8b94641120cd.png)'
- en: '**Discriminator**: In the GANs that we described previously, the generator
    takes an input vector that''s been randomly sampled from, say, a Gaussian distribution,
    and generates fake images. In this case, however, since we are looking to transfer
    images from one domain to another, we replace the input vector with an image.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**判别器：** 在我们之前描述的GAN中，生成器从一个随机采样的输入向量（比如高斯分布）生成假图像。然而，在这里，由于我们希望将图像从一个领域转移到另一个领域，我们将输入向量替换为一张图像。'
- en: 'The parameters of the discriminator from an architectural standpoint are as
    follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构角度来看，判别器的参数如下：
- en: '**Layers:** The discriminator is made up of 5 convolutional layers, each stacked
    on top of the other, and then followed by two fully connected layers.'
  id: totrans-81
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层：** 判别器由5个卷积层组成，每个卷积层堆叠在一起，然后是两个全连接层。'
- en: '**Activation:** Leaky ReLU activation is used for all layers except the last
    fully connected layer. The last layer uses `sigmoid` to predict the probability
    of a sample.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数：** 所有层使用Leaky ReLU激活，除了最后一层全连接层。最后一层使用`sigmoid`来预测样本的概率。'
- en: '**Normalizer:** This performs batch normalization, except on the first and
    last layers of the network.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**归一化器：** 该操作执行批量归一化，但不包括网络的第一层和最后一层。'
- en: '**Stride:** A stride length of 2 is used for all of the convolutional layers.'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步幅：** 所有卷积层的步幅长度为2。'
- en: DiscoGAN modeling
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DiscoGAN建模
- en: 'For each mapping, that is, **handbags** (denoted by **b**) to **shoes** (denoted
    by **s**), or vice versa, we add two generators. Let''s say, for the mapping **b** to **s,** the
    first generator maps the input image from domain **b** to **s**, while the second
    generator reconstructs the image from domain **s** to domain **b**. Intuitively,
    we need a second generator to achieve the objective (one-to-one) mapping we talked
    about in the previous sections. Mathematically, this can be represented as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个映射，也就是**手袋**（用**b**表示）到**鞋子**（用**s**表示），或者反过来，我们加入两个生成器。假设，对于映射**b**到**s**，第一个生成器将输入图像从**b**领域映射到**s**领域，而第二个生成器将图像从**s**领域重建到**b**领域。直观上，我们需要第二个生成器来实现我们在前面章节中提到的一一映射目标。数学上，可以表示如下：
- en: '![](img/55e10729-cbc7-4f69-b8b5-b1ef79cef683.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55e10729-cbc7-4f69-b8b5-b1ef79cef683.png)'
- en: '![](img/78432287-0789-4a32-9c18-9ef46abe80b7.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78432287-0789-4a32-9c18-9ef46abe80b7.png)'
- en: 'While modeling, since this is a very hard constraint to satisfy, we add a reconstruction
    loss. Reconstruction loss is given as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模时，由于这是一个非常难以满足的约束，我们加入了重建损失。重建损失如下所示：
- en: '![](img/7967859e-4513-4b97-86c1-4a74e0255488.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7967859e-4513-4b97-86c1-4a74e0255488.png)'
- en: 'Now, the usual GAN loss that is required to generate fake images of another
    domain is given by the following equation:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，生成假图像的常见GAN损失如下所示：
- en: '![](img/f5c80320-09bc-4d6a-8d6b-b94c42ec5f99.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5c80320-09bc-4d6a-8d6b-b94c42ec5f99.png)'
- en: 'For each mapping, the generator receives two losses:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个映射，生成器接收两个损失：
- en: The reconstruction loss, which tries to see how well we can map the generated
    image to its original domain
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重建损失，旨在查看我们如何能够将生成的图像映射到其原始领域
- en: The usual GAN loss, which is for the task of fooling the discriminator
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常的GAN损失用于欺骗判别器的任务
- en: In this case, the discriminator is the usual discriminator, with the loss that
    we mentioned in the section of *Training GANs*. Let's denote it by using ![](img/ab0bc834-4b50-4fd0-a928-30a465dfbe1c.png).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，判别器是常见的判别器，使用我们在*训练GAN*部分提到的损失。我们用 ![](img/ab0bc834-4b50-4fd0-a928-30a465dfbe1c.png)表示它。
- en: 'The total generator and discriminator loss is given by the following equation:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器和判别器的总损失如下所示：
- en: '![](img/3a5e2a5b-5eba-4322-b245-4ed3e71702fb.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a5e2a5b-5eba-4322-b245-4ed3e71702fb.png)'
- en: '![](img/1571492e-824a-4eed-846e-caa3fe9f0a69.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1571492e-824a-4eed-846e-caa3fe9f0a69.png)'
- en: Building a DiscoGAN model
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个DiscoGAN模型
- en: The base datasets in this problem are obtained from the `edges2handbags` ([https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz)) and
    `edges2shoes` ([https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz))
    datasets. Each image that's present in these datasets contain two sub-images.
    One is the colored image of the object, while the other is the image of the edges
    of the corresponding color image.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本问题中的基本数据集来自`edges2handbags`([https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz))和`edges2shoes`([https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz](https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz))数据集。这些数据集中的每张图像都包含两个子图像。一个是物体的彩色图像，另一个是对应彩色图像的边缘图像。
- en: 'Follow the steps to build a DiscoGAN model:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤构建DiscoGAN模型：
- en: 'First, resize and crop the images in this dataset to obtain the handbag and
    shoe images:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，调整和裁剪此数据集中的图像，以获取手袋和鞋子图像：
- en: '[PRE0]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Save the images in the corresponding folders of `bags` and `shoes`. Some of
    the sample images are shown as follows:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像保存在`bags`和`shoes`的相应文件夹中。一些示例图像如下所示：
- en: '| ![](img/eaab41c1-360b-48f1-b667-fd83e0169c31.png) | ![](img/50d09e05-b3a9-4323-a25d-c6b20b021356.png)
    | ![](img/db30ccae-c0fe-43df-bd2c-404654975f3b.png) | ![](img/7f0fbff0-68c3-4714-a612-fa2bf1beefb6.png)
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/eaab41c1-360b-48f1-b667-fd83e0169c31.png) | ![](img/50d09e05-b3a9-4323-a25d-c6b20b021356.png)
    | ![](img/db30ccae-c0fe-43df-bd2c-404654975f3b.png) | ![](img/7f0fbff0-68c3-4714-a612-fa2bf1beefb6.png)
    |'
- en: '| Shoes |  | Bags |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 鞋子 |  | 包包 |  |'
- en: 'Implement the `generator` function with  4 convolutional layers followed by
    4 convolutional transpose (or deconv) layers.The kernel size used in this scenario
    is 4, while the `stride` is `2` and `1` for the convolutional and deconv layers,
    respectively. Leaky Relu is used as activation function in all the layers. Code
    for the function is as follows:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`generator`函数，包含4个卷积层，后跟4个卷积转置（或反卷积）层。该场景中使用的卷积核大小为4，而卷积层和反卷积层的`stride`分别为`2`和`1`。所有层的激活函数均使用Leaky
    Relu。该函数的代码如下：
- en: '[PRE1]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the discriminator using the parameters that we mentioned previously
    in section *Fundamental Units of a DiscoGAN*:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们在*DiscoGAN的基本单元*部分之前提到的参数定义判别器：
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Use the following `define_network` function, which defines the two generators
    and two discriminator for each domain. In the function, the definition of  `generator` and `discriminator` remains
    the same as what we defined by using functions in the previous step. However,
    for DiscoGANs, the function defines one `generator` that generates fake images
    in another domain, and one `generator` that does the reconstruction. Also, the `discriminators` are
    defined for both real and fake images in each domain. Code the function is as
    follows:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下`define_network`函数，该函数为每个领域定义了两个生成器和两个判别器。在该函数中，`generator`和`discriminator`的定义与我们在前一步使用函数时定义的一样。然而，对于DiscoGAN，函数定义了一个`generator`，它生成另一个领域的假图像，一个`generator`负责重建。另外，`discriminators`为每个领域中的真实图像和假图像定义。该函数的代码如下：
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s define the `loss` function that we defined previously in *DiscoGAN modeling*
    section. Following function `define_loss` defines the reconstruction loss based
    on the Euclidean distance between the reconstructed and original image. To generate
    the GAN and discriminator loss, the function uses the cross entropy function:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义我们在*DiscoGAN建模*部分之前定义的`loss`函数。以下`define_loss`函数定义了基于重建图像和原始图像之间欧几里得距离的重建损失。为了生成GAN和判别器损失，该函数使用交叉熵函数：
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Use the  `AdamOptimizer` that was defined in [Chapter 3](60549866-497e-4dfa-890c-6651f34cf8e4.xhtml), *Sentiment
    Analysis in Your Browser Using TensorFlow.js*, of the book and implement the following `define_optimizer`
    function as follows:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在[第3章](60549866-497e-4dfa-890c-6651f34cf8e4.xhtml)《使用TensorFlow.js在浏览器中进行情感分析》中定义的`AdamOptimizer`，并实现以下`define_optimizer`函数：
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For debugging, write the summary into a logging file. While you can add anything
    to the summary, the function `summary_` below adds all of the losses just to observe
    the curves on how various losses change over time. Code for the function is as
    follows:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了调试，将摘要写入日志文件。虽然你可以在摘要中添加任何内容，但下面的`summary_`函数仅添加所有的损失，以便观察各种损失随时间变化的曲线。该函数的代码如下：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the following parameters for training the model:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为训练模型定义以下参数：
- en: 'Batch Size: 256'
  id: totrans-121
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小：256
- en: 'Learning rate: 0.0002'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：0.0002
- en: Epochs = 100,000 (use more if you are not getting the desired result)
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Epochs = 100,000（如果没有得到期望的结果，可以使用更多的epoch）
- en: 'Use the following code to train the model. Here is the brief explanation of
    what it does:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码训练模型。这里是它的简要说明：
- en: For each Epoch, the code obtains the mini batch images of both shoes and bags.
    It passes the mini batch through the model to update the discriminator loss first.
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个Epoch，代码会获取鞋子和包的小批量图像。它将小批量图像传递给模型，首先更新判别器损失。
- en: Samples a mini batch again for both shoes and bags and updates the generator
    loss keeping discriminator parameters fixed.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次为鞋子和包进行小批量采样，并在保持判别器参数固定的情况下更新生成器损失。
- en: For every 10 epochs, it writes a summary to Tensorboard.
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每10个epoch，它会将总结写入Tensorboard。
- en: For every 1000 epochs, it randomly samples 1 image from both bags and shoes
    dataset and saves the reconstructed and fake images for visualization purposes
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每1000个epoch，它会随机从包和鞋子数据集中采样1张图像，并保存重建图像和伪造图像，以便可视化。
- en: Also, for every 1000 epochs, it saves the model which can be helpful if you
    want to restore training at some point.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，每1000个epoch，它会保存模型，这对于你想在某个时刻恢复训练非常有用。
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We carried out the training on one GTX 1080 graphics card, which took a significant
    amount of time. Highly recommended to use a GPU with better processing than GTX
    1080 if possible.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一块GTX 1080显卡上进行了训练，花费了大量时间。如果可能，强烈建议使用处理能力更强的GPU。
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we first looked at what GANs are. They are a new kind of generative
    model that helps us to generate new images.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先了解了GAN是什么。它们是一种新的生成模型，帮助我们生成新的图像。
- en: We also touched upon other kinds of generative models, such as Variational Auto-encoders
    and PixelRNN, to get an overview of different kinds of generative models. We also
    talked about different kinds of GANs to discuss the progress that had been made
    in this space since the first paper on GANs was published in 2014.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提到了其他类型的生成模型，如变分自编码器（Variational Auto-encoders）和PixelRNN，以概览不同种类的生成模型。我们还讨论了不同种类的GAN，回顾了自2014年GAN第一篇论文发布以来该领域的进展。
- en: Then, we learned about DiscoGANs, a new type of GAN that can help us to learn
    about cross- domain relationships. Specifically, in this chapter, our focus was
    on building a model to generate handbag images from shoes and vice versa.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们了解了DiscoGAN，这是一种新的GAN类型，可以帮助我们了解跨领域的关系。具体来说，在本章中，我们的重点是构建一个模型，从鞋子生成手袋图像，反之亦然。
- en: Finally, we learned about the architecture of DiscoGANs and how they differ
    from usual GANs.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们了解了DiscoGAN的架构以及它们与常规GAN的不同之处。
- en: In the next chapter, we will learn how to implement capsule networks on the
    Fashion MNIST dataset.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何在Fashion MNIST数据集上实现胶囊网络。
- en: Questions
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Can you change the training parameters like learning rate, batch size and observe
    the changes in quality of the reconstructed and fake images?
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能改变训练参数，如学习率、批量大小，并观察重建图像和伪造图像的质量变化吗？
- en: Can you visualize the stored summary in Tensorboard to understand the learning
    process?
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能在Tensorboard中可视化存储的总结，以便理解学习过程吗？
- en: Can you think of other datasets which can be used for Style transfer?
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能想到其他可以用于风格迁移的数据集吗？
