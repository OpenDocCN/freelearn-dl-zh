- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Inference Pipeline Deployment
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理管道部署
- en: Deploying the inference pipeline for the **large language model** (**LLM**)
    Twin application is a critical stage in the **machine learning** (**ML**) application
    life cycle. It’s where the most value is added to your business, making your models
    accessible to your end users. However, successfully deploying AI models can be
    challenging, as the models require expensive computing power and access to up-to-date
    features to run the inference. To overcome these constraints, it’s crucial to
    carefully design your deployment strategy. This ensures that it meets the application’s
    requirements, such as latency, throughput, and costs. As we work with LLMs, we
    must consider the inference optimization techniques presented in *Chapter 8*,
    such as model quantization. Also, to automate the deployment processes, we must
    leverage MLOps best practices, such as model registries that version and share
    our models across our infrastructure.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 部署**大型语言模型**（**LLM**）Twin应用程序的推理管道是**机器学习**（**ML**）应用程序生命周期中的一个关键阶段。这是为您的业务增加最大价值的地方，使您的模型能够被最终用户访问。然而，成功部署AI模型可能具有挑战性，因为模型需要昂贵的计算能力和访问最新特征来运行推理。为了克服这些限制，精心设计您的部署策略至关重要。这确保了它满足应用程序的要求，例如延迟、吞吐量和成本。当我们与LLM合作时，我们必须考虑第8章中提出的推理优化技术，例如模型量化。此外，为了自动化部署流程，我们必须利用MLOps最佳实践，例如模型注册，它可以在我们的基础设施中版本控制和共享我们的模型。
- en: 'To understand how to design the deployment architecture of the LLM Twin, we
    will first look at three deployment types we can choose from: online real-time
    inference, asynchronous inference, and offline batch transform. Also, to better
    understand which option to choose for our LLM Twin use case, we will quickly walk
    you through a set of critical criteria we must consider before making an architectural
    decision, such as latency, throughput, data, and infrastructure. Also, we’ll weigh
    the pros and cons of monolithic and microservices architecture in model serving,
    a decision that can significantly influence the scalability and maintainability
    of your service.Once we’ve grasped the various design choices available, we’ll
    focus on understanding the deployment strategy for the LLM Twin’s inference pipeline.
    Subsequently, we will walk you through an end-to-end tutorial on deploying the
    LLM Twin service, including deploying our custom fine-tuned LLM to AWS SageMaker
    endpoints and implementing a FastAPI server as the central entry point for our
    users. We will then wrap up this chapter with a short discussion on autoscaling
    strategies and how to use them on SageMaker.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解如何设计LLM Twin的部署架构，我们首先将查看我们可以选择的三个部署类型：在线实时推理、异步推理和离线批量转换。为了更好地理解为我们的LLM
    Twin用例选择哪个选项，我们将快速带您了解在做出架构决策之前必须考虑的一组关键标准，例如延迟、吞吐量、数据和基础设施。此外，我们将权衡单体架构和微服务架构在模型服务中的优缺点，这个决定可以显著影响您服务的可扩展性和可维护性。一旦我们掌握了各种设计选择，我们将专注于理解LLM
    Twin推理管道的部署策略。随后，我们将带您完成部署LLM Twin服务的端到端教程，包括将我们的自定义微调LLM部署到AWS SageMaker端点和实现FastAPI服务器作为我们用户的中心入口点。然后，我们将以关于自动扩展策略及其在SageMaker上的使用方法的简短讨论来结束本章。
- en: 'Hence, in this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将涵盖以下主题：
- en: Criteria for choosing deployment types
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择部署类型的标准
- en: Understanding inference deployment types
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解推理部署类型
- en: Monolithic versus microservices architecture in model serving
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务中的单体架构与微服务架构
- en: Exploring the LLM Twin’s inference pipeline deployment strategy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索LLM Twin的推理管道部署策略
- en: Deploying the LLM Twin service
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署LLM Twin服务
- en: Autoscaling capabilities to handle spikes in usage
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理使用高峰的自扩展能力
- en: Criteria for choosing deployment types
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择部署类型的标准
- en: 'When it comes to deploying ML models, the first step is to understand the four
    requirements present in every ML application: throughput, latency, data, and infrastructure.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到部署ML模型时，第一步是理解每个ML应用程序中存在的四个要求：吞吐量、延迟、数据和基础设施。
- en: Understanding them and their interaction is essential. When designing the deployment
    architecture for your models, there is always a trade-off between the four that
    will directly impact the user’s experience. For example, should your model deployment
    be optimized for low latency or high throughput?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 理解它们及其相互作用是至关重要的。在设计您模型的部署架构时，始终存在四个直接影响用户体验的权衡。例如，您的模型部署应该优化为低延迟还是高吞吐量？
- en: Throughput and latency
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 吞吐量和延迟
- en: '**Throughput** refers to the number of inference requests a system can process
    in a given period. It is typically measured in **requests per second** (**RPS**).
    Throughput is crucial when deploying ML models when you expect to process many
    requests. It ensures the system can handle many requests efficiently without becoming
    a bottleneck.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**吞吐量**是指系统在给定时间内可以处理的推理请求数量。通常以每秒请求数（**RPS**）来衡量。当部署ML模型并预期处理大量请求时，吞吐量至关重要。它确保系统可以高效地处理大量请求，而不会成为瓶颈。'
- en: High throughput often requires scalable and robust infrastructure, such as machines
    or clusters with multiple high-end GPUs.**Latency** is the time it takes for a
    system to process a single inference request from when it is received until the
    result is returned. Latency is critical in real-time applications where quick
    response times are essential, such as in live user interactions, fraud detection,
    or any system requiring immediate feedback. For example, the average latency of
    OpenAI’s API is the average response time from when a user sends a request, and
    the service provides a result that is accessible within your application.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 高吞吐量通常需要可扩展且健壮的基础设施，例如配备多个高端GPU的机器或集群。**延迟**是指系统从接收到单个推理请求到返回结果所需的时间。在需要快速响应时间的实时应用中，延迟至关重要，例如在实时用户交互、欺诈检测或任何需要即时反馈的系统。例如，OpenAI
    API的平均延迟是从用户发送请求到服务提供可在您的应用程序中访问的结果的平均响应时间。
- en: The latency is the sum of the network I/O, serialization and deserialization,
    and the LLM’s inference time. Meanwhile, the throughput is the average number
    of requests the API processes and serves a second.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟是网络I/O、序列化和反序列化以及LLM推理时间的总和。同时，吞吐量是API每秒处理的平均请求数量。
- en: Low-latency systems require optimized and often more costly infrastructure,
    such as faster processors, lower network latency, and possibly edge computing
    to reduce the distance data needs to travel.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 低延迟的系统需要优化且通常成本更高的基础设施，例如更快的处理器、更低的网络延迟，以及可能需要边缘计算来减少数据需要传输的距离。
- en: A lower latency translates to higher throughput when the service processes multiple
    queries in parallel successfully. For example, if the service takes 100 ms to
    process requests, this translates to a throughput of 10 requests per second. If
    the latency reaches 10 ms per request, the throughput rises to 100 requests per
    second.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务成功并行处理多个查询时，较低的延迟会转化为更高的吞吐量。例如，如果服务处理请求需要100毫秒，这相当于每秒10个请求的吞吐量。如果延迟达到每个请求10毫秒，吞吐量将上升到每秒100个请求。
- en: However, to complicate things, most ML applications adopt a batching strategy
    to simultaneously pass multiple data samples to the model. In this case, a lower
    latency can translate into lower throughput; in other words, a higher latency
    maps to a higher throughput. For example, if you process 20 batched requests in
    100 ms, the latency is 100 ms, while the throughput is 200 requests per second.
    If you process 60 requests in 200 ms, the latency is 200 ms, while the throughput
    rises to 300 requests per second. Thus, even when batching requests at serving
    time, it’s essential to consider the minimum latency accepted for a good user
    experience.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，事情变得复杂，大多数ML应用采用批处理策略，同时将多个数据样本传递给模型。在这种情况下，较低的延迟可以转化为较低的吞吐量；换句话说，较高的延迟对应于较高的吞吐量。例如，如果您在100毫秒内处理20个批处理请求，延迟是100毫秒，而吞吐量是每秒200个请求。如果您在200毫秒内处理60个请求，延迟是200毫秒，而吞吐量上升到每秒300个请求。因此，即使在服务时间批处理请求时，考虑为良好的用户体验所接受的最低延迟也是至关重要的。
- en: Data
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据
- en: As we know, data is everywhere in an ML system. But when talking about model
    serving, we mostly care about the model’s input and output. This includes the
    format, volume, and complexity of the processed data. Data is the foundation of
    the inference process. The characteristics of the data, such as its size and type,
    determine how the system needs to be configured and optimized for efficient processing.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，数据在ML系统中无处不在。但当我们谈论模型服务时，我们主要关心的是模型的输入和输出。这包括处理数据的格式、体积和复杂性。数据是推理过程的基础。数据的特点，如其大小和类型，决定了系统需要如何配置和优化以实现高效处理。
- en: The type and size of the data directly impact latency and throughput, as more
    complex or extensive data can take longer to process. For example, designing a
    model that takes input structured data and outputs a probability differs entirely
    from an LLM that takes input text (or even images) and outputs an array of characters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的类型和大小直接影响延迟和吞吐量，因为更复杂或更广泛的数据可能需要更长的时间来处理。例如，设计一个接受结构化数据输入并输出概率的模型与接受文本（甚至图像）输入并输出字符数组的LLM完全不同。
- en: Infrastructure
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施
- en: 'Infrastructure refers to the underlying hardware, software, networking, and
    system architecture that supports the deployment and operation of the ML models.
    The infrastructure provides the necessary resources for deploying, scaling, and
    maintaining ML models. It includes computing resources, memory, storage, networking
    components, and the software stack:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施指的是支持ML模型部署和运行的底层硬件、软件、网络和系统架构。基础设施为部署、扩展和维护ML模型提供必要的资源。它包括计算资源、内存、存储、网络组件和软件栈：
- en: For **high throughput**, the systems require scalable infrastructure to manage
    large data volumes and high request rates, possibly through parallel processing,
    distributed systems, and high-end GPUs.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于**高吞吐量**，系统需要可扩展的基础设施来管理大量数据和高速请求，可能通过并行处理、分布式系统和高端GPU。
- en: Infrastructure must be optimized to reduce processing time to achieve **low
    latency**, such as using faster CPUs, GPUs, or specialized hardware. While optimizing
    your system for low latency while batching your requests, you often have to sacrifice
    high throughput in favor of lower latency, which can result in your hardware not
    being utilized at total capacity. As you process fewer requests per second, it
    results in idle computing, which increases the overall cost of processing a request.
    Thus, picking the suitable machine for your requirements is critical in optimizing
    costs.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施必须优化以减少处理时间以实现**低延迟**，例如使用更快的CPU、GPU或专用硬件。在优化系统以实现低延迟并批量处理请求时，你通常必须牺牲高吞吐量以换取低延迟，这可能导致你的硬件无法充分利用其总容量。随着每秒处理的请求数量减少，会导致闲置计算，从而增加处理请求的整体成本。因此，选择适合您需求的机器对于优化成本至关重要。
- en: It is crucial to design infrastructure to meet specific data requirements. This
    includes selecting storage solutions to handle large datasets and implementing
    fast retrieval mechanisms to ensure efficient data access. For example, we mostly
    care about optimizing throughput for offline training, while for online inference,
    we generally care about latency.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 设计满足特定数据要求的基础设施至关重要。这包括选择存储解决方案来处理大型数据集，并实施快速检索机制以确保高效的数据访问。例如，我们主要关注优化离线训练的吞吐量，而对于在线推理，我们通常关注延迟。
- en: 'With this in mind, before picking a specific deployment type, you should ask
    yourself questions such as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，在选择特定的部署类型之前，你应该问自己一些问题，例如：
- en: What are the throughput requirements? You should make this decision based on
    the throughput’s required minimum, average, and maximum statistics.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吞吐量需求是多少？你应该根据吞吐量的所需最小值、平均值和最大值统计数据来做出这个决定。
- en: How many requests the system must handle simultaneously? (1, 10, 1,000, 1 million,
    etc.)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统必须同时处理多少请求？（1，10，1,000，1百万等）
- en: What are the latency requirements? (1 millisecond, 10 milliseconds, 1 second,
    etc.)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 延迟需求是多少？（1毫秒，10毫秒，1秒等）
- en: How should the system scale? For example, we should look at the CPU workload,
    number of requests, queue size, data size, or a combination of them.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统应该如何扩展？例如，我们应该查看CPU的工作负载、请求数量、队列大小、数据大小，或者它们的组合。
- en: What are the cost requirements?With what data do we work with? For example,
    do we work with images, text, or tabular data?
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本需求是什么？我们使用什么数据？例如，我们处理图像、文本还是表格数据？
- en: What is the size of the data we work with? (100 MB, 1 GB, 10 GB)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们处理的数据大小是多少？（100 MB，1 GB，10 GB）
- en: 'Deeply thinking about these questions directly impacts the user experience
    of your application, which ultimately makes the difference between a successful
    product and not. Even if you ship a mind-blowing model, if the user needs to wait
    too long for a response or it often crashes, the user will switch your production
    to something less accurate that works reliably. For example, Google found in a
    2016 study that 53% of visits are abandoned if a mobile site takes longer than
    three seconds to load: [https://www.thinkwithgoogle.com/consumer-insights/consumer-trends/mobile-site-load-time-statistics/](https://www.thinkwithgoogle.com/consumer-insights/consumer-trends/mobile-site-load-time-statistics/).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 深入思考这些问题会直接影响你应用程序的用户体验，这最终决定了产品成功与否。即使你推出一个令人惊叹的模型，如果用户需要等待很长时间才能得到响应，或者它经常崩溃，用户会转向使用更准确但可靠性较低的替代品。例如，谷歌在2016年的一项研究发现，如果移动网站加载时间超过三秒，53%的访问将被放弃：[https://www.thinkwithgoogle.com/consumer-insights/consumer-trends/mobile-site-load-time-statistics/](https://www.thinkwithgoogle.com/consumer-insights/consumer-trends/mobile-site-load-time-statistics/)。
- en: Let’s move on to the three deployment architectures we can leverage to serve
    our models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续探讨我们可以利用的三个部署架构来服务我们的模型。
- en: Understanding inference deployment types
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解推理部署类型
- en: 'As illustrated in *Figure 10.1*, you can choose from three fundamental deployment
    types when serving models:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.1*所示，在服务模型时，你可以从三种基本的部署类型中选择：
- en: Online real-time inference
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线实时推理
- en: Asynchronous inference
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步推理
- en: Offline batch transform
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离线批量转换
- en: When selecting one design over the other, there is a trade-off between latency,
    throughput, and costs. You must consider how the data is accessed and the infrastructure
    you are working with. Another criterion you have to consider is how the user will
    interact with the model. For example, will the user use it directly, like a chatbot,
    or will it be hidden within your system, like a classifier that checks if an input
    (or output) is safe?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择一种设计而不是另一种设计时，需要在延迟、吞吐量和成本之间进行权衡。你必须考虑数据是如何访问的，以及你正在与之合作的基础设施。你必须考虑的另一个标准是用户将如何与模型交互。例如，用户是否会直接使用它，就像聊天机器人一样，或者它是否会被隐藏在你的系统中，就像一个检查输入（或输出）是否安全的分类器？
- en: You have to consider the freshness of the predictions as well. For example,
    serving your model in offline batch mode might be easier to implement if, in your
    use case, it is OK to consume delayed predictions. Otherwise, you have to serve
    your model in real-time, which is more infrastructure-demanding. Also, you have
    to consider your application’s traffic. Ask yourself questions such as, “Will
    the application be constantly used, or will there be spikes in traffic and then
    flatten out?”
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你还必须考虑预测的新鲜度。例如，如果你的用例中可以接受延迟预测，那么在离线批量模式下部署你的模型可能更容易实现。否则，你必须实时部署你的模型，这需要更多的基础设施。此外，你还必须考虑你的应用程序流量。问问自己，例如，“应用程序是否会持续使用，还是会有流量高峰然后平缓下来？”
- en: With that in mind, let’s explore the three major ML deployment types.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，让我们来探讨三种主要的机器学习部署类型。
- en: '![](img/B31105_10_01.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_10_01.png)'
- en: 'Figure 10.1: The three fundamental architectures of inference deployment types'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：推理部署类型的三个基本架构
- en: Online real-time inference
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在线实时推理
- en: In real-time inference, we have a simple architecture based on a server that
    can be accessed through HTTP requests. The most popular options are to implement
    a REST API or gRPC server. The REST API is more accessible but slower, using JSON
    to pass data between the client and server.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在实时推理中，我们有一个基于服务器的简单架构，可以通过HTTP请求访问。最受欢迎的选项是实现REST API或gRPC服务器。REST API更易于访问但速度较慢，使用JSON在客户端和服务器之间传递数据。
- en: This approach is usually taken when serving models outside your internal network
    to the broader public. For example, OpenAI’s API implements a REST API protocol.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当将模型部署到内部网络之外，面向更广泛的公众时，通常会采用这种方法。例如，OpenAI的API实现了REST API协议。
- en: On the other hand, implementing a gRPC makes your ML server faster, though it
    may reduce its flexibility and general applicability. You have to implement `protobuf`
    schemas in your client application, which are more tedious to work with than JSON
    structures. The benefit, however, is that `protobuf` objects can be compiled into
    bites, making the network transfers much faster. Thus, this protocol is often
    adopted for internal services within the same ML system.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，实现gRPC可以使您的机器学习服务器更快，尽管这可能会降低其灵活性和通用性。您必须在客户端应用程序中实现`protobuf`模式，这比JSON结构更繁琐。然而，好处是`protobuf`对象可以编译成字节，使网络传输更快。因此，该协议通常被采用于同一机器学习系统内的内部服务。
- en: Using the real-time inference approach, the client sends an HTTP request to
    the ML service, which immediately processes the request and returns the result
    in the same response. This synchronous interaction means the client waits for
    the result before moving on.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用实时推理方法，客户端向机器学习服务发送HTTP请求，该服务立即处理请求并在同一响应中返回结果。这种同步交互意味着客户端在继续之前必须等待结果。
- en: To make this work efficiently, the infrastructure must support low-latency,
    highly responsive ML services, often deployed on fast, scalable servers. Load
    balancing is crucial to evenly distribute incoming traffic evenly, while autoscaling
    ensures the system can handle varying loads. High availability is also essential
    to keeping the service operational at all times.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一过程高效运行，基础设施必须支持低延迟、高度响应的机器学习服务，通常部署在快速、可扩展的服务器上。负载均衡对于均匀分配传入流量至关重要，而自动扩展确保系统可以处理变化的负载。高可用性对于始终保持服务运行也是必不可少的。
- en: For example, this architecture is often present when interacting with LLMs,
    as when sending a request to a chatbot or API (powered by LLMs), you expend the
    predictions right ahead. LLM services, such as ChatGPT or Claude, often use WebSockets
    to stream each token individually to the end user, making the interaction more
    responsive. Other famous examples are AI services such as embedding or reranking
    models used for **retrieval-augmented generation** (**RAG**) or online recommendation
    engines in platforms like TikTok.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这种架构在与大型语言模型（LLM）交互时经常出现，例如在向聊天机器人或由LLM（如ChatGPT或Claude）驱动的API发送请求时，您可以直接消耗预测。LLM服务，如ChatGPT或Claude，通常使用WebSockets将每个标记单独流式传输到最终用户，这使得交互更加响应。其他著名的例子包括嵌入或重排序模型，这些模型用于**检索增强生成**（RAG）或TikTok等平台上的在线推荐引擎。
- en: The simplicity of real-time inference, with its direct client-server interaction,
    makes it an attractive option for applications that require immediate responses,
    like chatbots or real-time recommendations. However, this approach can be challenging
    to scale and may lead to underutilized resources during low-traffic periods.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 实时推理的简单性，以及其直接的客户端-服务器交互，使其成为需要即时响应的应用程序（如聊天机器人或实时推荐）的吸引人选择。然而，这种方法在扩展上可能具有挑战性，并且在低流量期间可能会导致资源利用率不足。
- en: Asynchronous inference
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步推理
- en: In asynchronous inference, the client sends a request to the ML service, which
    acknowledges the request and places it in a queue for processing. Unlike real-time
    inference, the client doesn’t wait for an immediate response. Instead, the ML
    service processes the request asynchronously. This requires a robust infrastructure
    that queues the messages to be processed by the ML service later on.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在异步推理中，客户端向机器学习服务发送请求，该服务确认请求并将其放入队列以供处理。与实时推理不同，客户端不需要等待即时响应。相反，机器学习服务异步处理请求。这需要一个强大的基础设施，该基础设施将消息排队，以便稍后由机器学习服务处理。
- en: When the results are ready, you can leverage multiple techniques to send them
    to the client. For example, depending on the size of the result, you can put it
    either in a different queue or an object storage dedicated to storing the results.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当结果准备好时，您可以使用多种技术将它们发送到客户端。例如，根据结果的大小，您可以将它放入不同的队列或一个专门用于存储结果的对象存储。
- en: The client can either adopt a polling mechanism that checks on a schedule if
    there are new results or adopt a push strategy and implement a notification system
    to inform the client when the results are ready.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端可以选择采用轮询机制，定期检查是否有新的结果，或者采用推送策略并实现一个通知系统，在结果准备好时通知客户端。
- en: Asynchronous inference uses resources more efficiently. It doesn’t have to process
    all the requests simultaneously but can define a maximum number of machines that
    run in parallel to process the messages. This is possible because the requests
    are stored in the queue until a machine can process them. Another huge benefit
    is that it can handle spikes in requests without any timeouts. For example, let’s
    assume that on an e-shop site, we usually have 10 requests per second handled
    by two machines. Because of a promotion, many people started to visit the site,
    and the number of requests spiked to 100 requests per second. Instead of scaling
    the number of **virtual machines** (**VMs**) by 10, which can add drastic costs,
    the requests are queued, and the same two VMs can process them in their rhythm
    without any failures.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 异步推理更有效地使用资源。它不必同时处理所有请求，但可以定义一个最大数量的机器，这些机器可以并行运行以处理消息。这是可能的，因为请求被存储在队列中，直到有机器可以处理它们。另一个巨大的好处是它可以处理请求的峰值而不会超时。例如，假设在一个电子商务网站上，我们通常每秒由两台机器处理10个请求。由于促销活动，许多人开始访问该网站，请求的数量激增到每秒100个。而不是通过增加10个**虚拟机**（VMs）的数量，这可能会增加巨大的成本，请求被排队，相同的两台VMs可以按其节奏处理它们，而不会出现任何故障。
- en: Another popular advantage for asynchronous architectures is when the requested
    job takes significant time to complete. For example, if the job takes over five
    minutes, you don’t want to block the client waiting for a response.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 异步架构的另一个流行优势是当请求的工作需要很长时间才能完成时。例如，如果工作需要超过五分钟，你不想让客户端等待响应。
- en: While asynchronous inference offers significant benefits, it does come with
    trade-offs. It introduces higher latency, making it less suitable for time-sensitive
    applications. Additionally, it adds complexity to the implementation and infrastructure.
    Depending on your design choices, this architecture type falls somewhere between
    online and offline, offering a balance of benefits and trade-offs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然异步推理提供了显著的好处，但它也带来了一些权衡。它引入了更高的延迟，使其不太适合对时间敏感的应用。此外，它增加了实现和基础设施的复杂性。根据你的设计选择，这种架构类型介于在线和离线之间，提供了利益和权衡的平衡。
- en: For example, this is a robust design where you don’t care too much about the
    latency of the inference but want to optimize costs heavily. Thus, it is a popular
    choice for problems such as extracting keywords from documents, summarizing them
    using LLMs, or running deep-fake models on top of videos. But suppose you carefully
    design the autoscaling system to process the requests from the queue at decent
    speeds. In that case, you can leverage this design for other use cases, such as
    online recommendations for e-commerce. In the end, it sums up how much computing
    power you are willing to pay to meet the expectations of your application.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这是一个健壮的设计，你不太关心推理的延迟，但想大幅度优化成本。因此，它对于诸如从文档中提取关键词、使用大型语言模型进行总结或在上面的视频上运行深度伪造模型等问题来说是一个流行的选择。但是，如果你仔细设计了自动扩展系统，以适中的速度处理队列中的请求，那么你可以利用这种设计来处理其他用例，例如电子商务的在线推荐。最终，它总结了为了满足应用程序的期望，你愿意支付多少计算能力。
- en: Offline batch transform
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离线批处理转换
- en: Batch transform is about processing large volumes of data simultaneously, either
    on a schedule or triggered manually. In a batch transform architecture, the ML
    service pulls data from a storage system, processes it in a single operation,
    and then stores the results in storage. The storage system can be implemented
    as an object storage like AWS S3 or a data warehouse like GCP BigQuery.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理转换是关于同时处理大量数据，无论是按计划还是手动触发。在批处理转换架构中，机器学习服务从存储系统提取数据，一次性处理它，然后将结果存储在存储系统中。存储系统可以实施为对象存储，如AWS
    S3，或数据仓库，如GCP BigQuery。
- en: Unlike the asynchronous inference architecture, a batch transform design is
    optimized for high throughput with permissive latency requirements. When real-time
    predictions are unnecessary, this approach can significantly reduce costs, as
    processing data in big batches is the most economical method. Moreover, the batch
    transform architecture is the simplest way to serve a model, accelerating development
    time.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与异步推理架构不同，批处理转换设计针对高吞吐量和可接受的延迟要求进行了优化。当实时预测不是必需时，这种方法可以显著降低成本，因为批量处理数据是最经济的方法。此外，批处理转换架构是提供模型的最简单方式，可以加速开发时间。
- en: The client pulls the results directly from data storage, decoupling its interaction
    with the ML service. Taking this approach, the client never has to wait for the
    ML service to process its input, but at the same time, it doesn’t have the flexibility
    to ask for new results at any time. You can see the data storage, where the results
    are stored as a large cache, from where the client can take what is required.
    If you want to make your application more responsive, the client can be notified
    when the processing is complete and can retrieve the results.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端直接从数据存储中拉取结果，解耦了它与机器学习服务的交互。采用这种方法，客户端永远不需要等待机器学习服务处理其输入，但与此同时，它也没有在任何时候请求新结果的灵活性。你可以看到数据存储，其中结果作为大缓存存储，客户端可以从那里获取所需的内容。如果你想使你的应用程序更响应，客户端可以在处理完成后被通知并检索结果。
- en: Unfortunately, this approach will always introduce a delay between the time
    the predictions were computed and consumed. That’s why not all applications can
    leverage this design choice. For example, if we implement a recommender system
    for a video streaming application, having a delay of one day for the predicted
    movies and TV shows might work because you don’t consume these products at a high
    frequency. But suppose you make a recommender system for a social media platform.
    In that case, delaying one day or even one hour is unacceptable, as you constantly
    want to provide fresh content to the user.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这种方法总是在预测计算和消费之间引入延迟。这就是为什么并非所有应用程序都能利用这种设计选择。例如，如果我们为视频流应用程序实现一个推荐系统，对于预测的电影和电视节目有一天的延迟可能可行，因为你不会频繁地消费这些产品。但假设你为社交媒体平台实现一个推荐系统。在这种情况下，一天的延迟甚至一小时的延迟都是不可接受的，因为你始终希望向用户提供新鲜的内容。
- en: Batch transform shines in scenarios where high throughput is needed, like data
    analytics or periodic reporting. However, it’s unsuitable for real-time applications
    due to its high latency and requires careful planning and scheduling to manage
    large datasets effectively. That’s why it is an offline serving method.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 批量转换在需要高吞吐量的场景中表现出色，如数据分析或定期报告。然而，由于其高延迟，它不适合实时应用程序，并且需要仔细规划和调度来有效地管理大数据集。这就是为什么它是一种离线服务方法。
- en: To conclude, we examined the three most common architectures for serving ML
    models. We started with online real-time inference, which serves clients when
    they request a prediction. Then, we looked at the asynchronous inference method,
    which sits between online and offline. Ultimately, we presented the offline batch
    transform, which is used to process large amounts of data and store them in data
    storage, from where the client later consumes them.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们探讨了服务机器学习模型最常用的三种架构。我们首先从在线实时推理开始，当客户端请求预测时为客户端提供服务。然后，我们看了异步推理方法，它介于在线和离线之间。最终，我们介绍了离线批量转换，它用于处理大量数据并将它们存储在数据存储中，客户端稍后从那里消费它们。
- en: Monolithic versus microservices architecture in model serving
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型服务中的单体架构与微服务架构
- en: In the previous section, we saw three different methods of deploying the ML
    service. The differences in architecture were mainly based on the interaction
    between the client and the ML service, such as the communication protocol, the
    ML service responsiveness, and prediction freshness.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了部署机器学习服务的三种不同方法。架构上的差异主要基于客户端与机器学习服务之间的交互，例如通信协议、机器学习服务的响应性和预测的新鲜度。
- en: But another aspect to consider is the architecture of the ML service itself,
    which can be implemented as a monolithic server or as multiple microservices.
    This will impact how the ML service is implemented, maintained, and scaled. Let’s
    explore the two options.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 但另一个需要考虑的方面是机器学习服务的架构本身，它可以实现为一个单体服务器或多个微服务。这将影响机器学习服务的实现、维护和扩展方式。让我们来探讨这两种选项。
- en: '![](img/B31105_10_02.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_10_02.png)'
- en: 'Figure 10.2: Monolithic versus microservices architecture in model serving'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：模型服务中的单体架构与微服务架构
- en: Monolithic architecture
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单体架构
- en: The LLM (or any other ML model) and the associated business logic (preprocessing
    and post-processing steps) are bundled into a single service in a monolithic architecture.
    This approach is straightforward to implement at the beginning of a project, as
    everything is placed within one code base. Simplicity makes maintenance easy when
    working on small to medium projects, as updates and changes can be made within
    a unified system.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在单体架构中，LLM（或任何其他ML模型）及其相关的业务逻辑（预处理和后处理步骤）被捆绑成一个单一的服务。这种方法在项目开始时易于实现，因为所有内容都放在一个代码库中。在小到中等规模的项目中，由于更新和更改可以在统一的系统中进行，因此简单性使得维护变得容易。
- en: One key challenge of a monolithic architecture is the difficulty of scaling
    components independently. The LLM typically requires GPU power, while the rest
    of the business logic is CPU and I/O-bound. As a result, the infrastructure must
    be optimized for both GPU and CPU. This can lead to inefficient resource use,
    with the GPU being idle when the business logic is executed and vice versa. Such
    inefficiency can result in additional costs that could be avoided.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 单体架构的一个关键挑战是独立扩展组件的困难。LLM通常需要GPU性能，而其余的业务逻辑是CPU和I/O密集型。因此，基础设施必须针对GPU和CPU进行优化。这可能导致资源使用效率低下，当业务逻辑执行时GPU处于空闲状态，反之亦然。这种低效率可能导致额外的成本，这些成本本可以避免。
- en: Moreover, this architecture can limit flexibility, as all components must share
    the same tech stack and runtime environment. For example, you might want to run
    the LLM using Rust or C++ or compile it with ONNX or TensorRT while keeping the
    business logic in Python. Having all the code in one system makes this differentiation
    difficult. Finally, splitting the work across different teams is complex, often
    leading to bottlenecks and reduced agility.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这种架构可能会限制灵活性，因为所有组件必须共享相同的技栈和运行环境。例如，您可能希望使用Rust或C++运行LLM或使用ONNX或TensorRT进行编译，同时保持业务逻辑在Python中。将所有代码放在一个系统中使得这种差异化变得困难。最后，将工作分配给不同的团队是复杂的，通常会导致瓶颈和敏捷性降低。
- en: Microservices architecture
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微服务架构
- en: A microservices architecture breaks down the inference pipeline into separate,
    independent services—typically splitting the LLM service and the business logic
    into distinct components. These services communicate over a network using protocols
    such as REST or gRPC.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务架构将推理管道分解为独立的、独立的服务——通常将LLM服务和业务逻辑拆分为不同的组件。这些服务通过REST或gRPC等协议在网络中进行通信。
- en: As illustrated in *Figure 10.3*, the main advantage of this approach is the
    ability to scale each component independently. For instance, since the LLM service
    might require more GPU resources than the business logic, it can be scaled horizontally
    without impacting the other components. This optimizes resource usage and reduces
    costs, as different types of machines (e.g., GPU versus CPU) can be used according
    to each service’s needs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.3*所示，这种方法的主要优势在于能够独立扩展每个组件。例如，由于LLM服务可能需要比业务逻辑更多的GPU资源，因此它可以水平扩展而不会影响其他组件。这优化了资源使用并降低了成本，因为可以根据每个服务的需求使用不同类型的机器（例如，GPU与CPU）。
- en: For example, let’s assume that the LLM inference takes longer, so you will need
    more ML service replicas to meet the demand. But remember that GPU VMs are expensive.
    By decoupling the two components, you will run only what is required on the GPU
    machine and not block the GPU VM with other computing that can be done on a much
    cheaper machine.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设LLM推理需要更长的时间，因此您将需要更多的ML服务副本来满足需求。但请记住，GPU虚拟机很昂贵。通过解耦这两个组件，您只需在GPU机器上运行所需的操作，而不会因为其他可以在更便宜的机器上完成的计算而阻塞GPU虚拟机。
- en: Thus, by decoupling the components, you can scale horizontally as required,
    with minimal costs, providing a cost-effective solution to your system’s needs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过解耦组件，您可以按需水平扩展，成本最低，为您的系统需求提供经济有效的解决方案。
- en: '![](img/B31105_10_03.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_10_03.png)'
- en: 'Figure 10.3: Scaling microservices independently based on compute requirements'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：根据计算需求独立扩展微服务
- en: Additionally, each microservice can adopt the most suitable technology stack,
    allowing teams to innovate and optimize independently.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个微服务都可以采用最合适的技术栈，使团队能够独立创新和优化。
- en: However, microservices introduce complexity in deployment and maintenance. Each
    service must be deployed, monitored, and maintained separately, which can be more
    challenging than managing a monolithic system.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，微服务在部署和维护中引入了复杂性。每个服务都必须单独部署、监控和维护，这可能比管理单体系统更具挑战性。
- en: The increased network communication between services can also introduce latency
    and potential points of failure, necessitating robust monitoring and resilience
    mechanisms.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的增加网络通信也可能引入延迟和潜在的故障点，需要强大的监控和弹性机制。
- en: Note that the proposed design for decoupling the ML model and business logic
    into two services can be extended if necessary. For example, you can have one
    service for preprocessing the data, one for the model, and another for post-processing
    the data. Depending on the four pillars (latency, throughput, data, and infrastructure),
    you can get creative and design the most optimal architecture for your application
    needs.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，将机器学习和业务逻辑解耦到两个服务中的设计方案可以根据需要扩展。例如，你可以有一个服务用于数据预处理，一个用于模型，另一个用于数据后处理。根据四个支柱（延迟、吞吐量、数据和基础设施），你可以发挥创意，为你的应用程序需求设计最优化架构。
- en: Choosing between monolithic and microservices architectures
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在单体架构和微服务架构之间进行选择
- en: The choice between monolithic and microservices architectures for serving ML
    models largely depends on the application’s specific needs. A monolithic approach
    might be ideal for smaller teams or more straightforward applications where ease
    of development and maintenance is a priority. It’s also a good starting point
    for projects without frequent scaling requirements. Also, if the ML models are
    smaller, don’t require a GPU, or don’t require smaller and cheaper GPUs, the trade-off
    between reducing costs and complicating your infrastructure is worth considering.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为服务机器学习模型选择单体架构和微服务架构主要取决于应用程序的具体需求。对于小型团队或更简单的应用程序，单体方法可能是理想的，在这些应用程序中，开发和维护的简便性是优先考虑的。它也是没有频繁扩展要求的项目的一个良好起点。此外，如果机器学习模型较小，不需要GPU，或者不需要更小、更便宜的GPU，那么在降低成本和复杂化你的基础设施之间的权衡是值得考虑的。
- en: On the other hand, microservices, with their adaptability and scalability, are
    well suited for larger, more complex systems where different components have varying
    scaling needs or require distinct tech stacks. This architecture is particularly
    advantageous when scaling specific system parts, such as GPU-intensive LLM services.
    As LLMs require powerful machines with GPUs, such as Nvidia A100, V100, or A10g,
    which are incredibly costly, microservices offer the flexibility to optimize the
    system for keeping these machines busy all the time or quickly scaling down when
    the GPU is idle. However, this flexibility comes at the cost of increased complexity
    in both development and operations.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，微服务因其适应性和可扩展性，非常适合更大、更复杂的系统，其中不同的组件有不同的扩展需求或需要不同的技术堆栈。这种架构在扩展特定系统部分，如GPU密集型LLM服务时特别有利。由于LLM需要配备GPU的强大机器，如Nvidia
    A100、V100或A10g，这些机器成本极高，微服务提供了优化系统的灵活性，以保持这些机器始终忙碌或当GPU空闲时快速缩放。然而，这种灵活性是以开发和运营复杂度增加为代价的。
- en: A common strategy is to start with a monolithic design and further decouple
    it into multiple services as the project grows. However, to successfully do so
    without making the transition too complex and costly, you must design the monolith
    application with this in mind. For instance, even if all the code runs on a single
    machine, you can completely decouple the modules of the application at the software
    level. This makes it easier to move these modules to different microservices when
    the time comes. When working with Python, for example, you can implement the ML
    and business logic into two different Python modules that don’t interact with
    each other. Then, you can glue these two modules at a higher level, such as through
    a service class, or directly into the framework you use to expose your application
    over the internet, such as FastAPI.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的策略是在项目增长的过程中，从单一的设计开始，并将其进一步解耦为多个服务。然而，为了在不使过渡过于复杂和昂贵的情况下成功实现这一点，你必须考虑到这一点来设计单体应用程序。例如，即使所有代码都在单个机器上运行，你仍然可以在软件级别完全解耦应用程序的模块。这使得在需要的时候将这些模块移动到不同的微服务中变得更容易。例如，当使用Python时，你可以将机器学习和业务逻辑实现为两个不同的Python模块，这两个模块之间不相互交互。然后，你可以在更高的层次上将这些模块粘合在一起，例如通过服务类，或者直接粘合到你用来在互联网上公开应用程序的框架中，例如FastAPI。
- en: Another option is to write the ML and business logic as two different Python
    packages that you glue together in the same ways as before. This is better because
    it completely enforces a separation between the two but adds extra complexity
    at development time. The main idea, therefore, is that if you start with a monolith
    and down the line you want to move to a microservices architecture, it’s essential
    to design your software with modularity in mind. Otherwise, if the logic is mixed,
    you will probably have to rewrite everything from scratch, adding tons of development
    time, which translates into wasted resources.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是将机器学习和业务逻辑编写为两个不同的Python包，然后以与之前相同的方式将它们粘合在一起。这样做更好，因为它完全强制了两者之间的分离，但在开发时增加了额外的复杂性。因此，主要思想是，如果你从一个单体开始，并且最终想要迁移到微服务架构，那么在设计软件时必须考虑模块化。否则，如果逻辑混合，你可能不得不从头开始重写一切，这将增加大量的开发时间，这转化为浪费的资源。
- en: In summary, monolithic architectures offer simplicity and ease of maintenance
    but at the cost of flexibility and scalability. At the same time, microservices
    provide the agility to scale and innovate but require more sophisticated management
    and operational practices.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，单体架构提供了简单性和易于维护性，但以灵活性和可扩展性为代价。同时，微服务提供了可扩展性和创新的敏捷性，但需要更复杂的管理和运营实践。
- en: Exploring the LLM Twin’s inference pipeline deployment strategy
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索LLM Twin的推理管道部署策略
- en: Now that we’ve understood all the design choices available for implementing
    the deployment strategy of the LLM Twin’s inference pipeline, let’s explore the
    concrete decisions we made to actualize it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了实现LLM Twin推理管道部署策略的所有设计选择，让我们来探讨我们为了实现它所做出的具体决策。
- en: Our primary objective is to develop a chatbot that facilitates content creation.
    To achieve this, we will process requests sequentially, with a strong emphasis
    on low latency. This necessitates the selection of an online real-time inference
    deployment architecture.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要目标是开发一个促进内容创作的聊天机器人。为了实现这一目标，我们将按顺序处理请求，并强调低延迟。这需要选择一个在线实时推理部署架构。
- en: On the monolith versus microservice aspect, we will split the ML service between
    a REST API server containing the business logic and an LLM microservice optimized
    for running the given LLM. As the LLM requires a powerful machine to run the inference,
    and we can further optimize it with various engines to speed up the latency and
    memory usage, it makes the most sense to go with the microservice architecture.
    By doing so, we can quickly adapt the infrastructure based on various LLM sizes.
    For example, if we run an 8B parameter model, the model can run on a single machine
    with a Nivida A10G GPU after quantization. But if we want to run a 30B model,
    we can upgrade to an Nvidia A100 GPU. Doing so allows us to upgrade only the LLM
    microservice while keeping the REST API untouched.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在单体与微服务方面，我们将把机器学习服务分为一个包含业务逻辑的REST API服务器和一个针对运行给定LLM进行优化的LLM微服务。由于LLM需要强大的机器来运行推理，并且我们可以使用各种引擎进一步优化它以加快延迟和内存使用，因此采用微服务架构最有意义。通过这样做，我们可以根据不同的LLM大小快速调整基础设施。例如，如果我们运行一个8B参数模型，模型在量化后可以在单个机器上运行，配备Nivida
    A10G GPU。但如果我们想运行一个30B模型，我们可以升级到Nvidia A100 GPU。这样做允许我们只升级LLM微服务，同时保持REST API不变。
- en: As illustrated in *Figure 10.4*, most business logic is centered around RAG
    in our particular use case. Thus, we will perform RAG’s retrieval and augmentation
    parts within the business microservice. It will also include all the advanced
    RAG techniques presented in the previous chapter to optimize the pre-retrieval,
    retrieval, and post-retrieval steps.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图10.4*所示，在我们的特定用例中，大部分业务逻辑都集中在RAG上。因此，我们将执行RAG的检索和增强部分在业务微服务中。它还将包括上一章中介绍的所有的先进RAG技术，以优化预检索、检索和后检索步骤。
- en: The LLM microservice is strictly optimized for the RAG generation component.
    Ultimately, the business layer will send the prompt trace consisting of the user
    query, prompt, answer, and other intermediary steps to the prompt monitoring pipeline,
    which we will detail in *Chapter 11*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: LLM微服务严格优化了RAG生成组件。最终，业务层将用户查询、提示、答案和其他中间步骤组成的提示跟踪信息发送到提示监控管道，我们将在*第11章*中详细说明。
- en: In summary, our approach involves implementing an online real-time ML service
    using a microservice architecture, which effectively splits the LLM and business
    logic into two distinct services.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的方法涉及通过微服务架构实现一个在线实时ML服务，这有效地将LLM和业务逻辑分为两个不同的服务。
- en: '![](img/B31105_10_04.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_10_04.png)'
- en: 'Figure 10.4: Microservice deployment architecture of the LLM Twin’s inference
    pipeline'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：LLM Twin推理管道的微服务部署架构
- en: 'Let’s review the interface of the inference pipeline, which is defined by the
    **feature/training/inference** (**FTI**) architecture. For the pipeline to run,
    it needs two things:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾推理管道的接口，该接口由**特征/训练/推理**（**FTI**）架构定义。为了使管道运行，它需要两样东西：
- en: Real-time features used for RAG, generated by the feature pipeline, which is
    queried from our online feature store, more concretely from the Qdrant vector
    database (DB)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于RAG的实时特征，由特征管道生成，从我们的在线特征存储中查询，更具体地说，从Qdrant向量数据库（DB）中查询
- en: A fine-tuned LLM generated by the training pipeline, which is pulled from our
    model registry
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由训练管道生成的经过微调的LLM，该LLM从我们的模型注册库中提取
- en: 'With that in mind, the flow of the ML service looks as follows, as illustrated
    in *Figure 10.4*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，ML服务的流程如下，如图10.4所示：
- en: A user sends a query through an HTTP request.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户通过HTTP请求发送查询。
- en: The user’s input retrieves the proper context by leveraging the advanced RAG
    retrieval module implemented in *Chapter 4*.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户输入通过利用第4章中实现的先进RAG检索模块来检索适当的上下文。
- en: The user’s input and retrieved context are packed into the final prompt using
    a dedicated prompt template.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用专用提示模板将用户输入和检索到的上下文打包到最终的提示中。
- en: The prompt is sent to the LLM microservice through an HTTP request.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过HTTP请求将提示发送到LLM微服务。
- en: The business microservices wait for the generated answer.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 业务微服务等待生成的答案。
- en: After the answer is generated, it is sent to the prompt monitoring pipeline
    along with the user’s input and other vital information to monitor.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成答案后，它连同用户输入和其他重要信息一起发送到提示监控管道以进行监控。
- en: Ultimately, the generated answer is sent back to the user.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终，生成的答案发送回用户。
- en: Now, let’s explore what tech stack we used to implement the architecture presented
    in *Figure 10.4*. As we know, we use Qdrant for the vector DB. We will leverage
    Hugging Face for the model registry. By doing so, we can publicly share our model
    with everyone who is testing the code from this book. Thus, you can easily use
    the model we provided if you don’t want to run the training pipeline, which can
    cost up to 100 dollars. As you can see, shareability and accessibility are some
    of the most beautiful aspects of storing your model in a model registry.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索我们用来实现图10.4中展示的架构的技术堆栈。正如我们所知，我们使用Qdrant作为向量数据库。我们将利用Hugging Face进行模型注册。通过这样做，我们可以与测试本书代码的每个人公开共享我们的模型。因此，如果您不想运行训练管道，可以节省高达100美元，您可以使用我们提供的模型。如您所见，可共享性和可访问性是将您的模型存储在模型注册库中的最美丽方面之一。
- en: We will implement the business microservice in FastAPI because it’s popular,
    easy to use, and fast. The LLM microservice will be deployed on AWS SageMaker,
    where we will leverage SageMaker’s integration with Hugging Face’s **Deep Learning
    Containers** (**DLCs**) to deploy the model. We will discuss Hugging Face’s DLCs
    in the next section, but intuitively, it is an inference engine used to optimize
    LLMs at serving time. The prompt monitoring pipeline is implemented using Comet,
    but we will look over that module only in *Chapter 11*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用FastAPI实现业务微服务，因为它受欢迎、易于使用且速度快。LLM微服务将部署在AWS SageMaker上，我们将利用SageMaker与Hugging
    Face的**深度学习容器**（**DLCs**）的集成来部署模型。我们将在下一节讨论Hugging Face的DLCs，但直观地说，它是一个用于在服务时间优化LLM的推理引擎。提示监控管道使用Comet实现，但我们将只在第11章中查看该模块。
- en: 'The SageMaker Inference deployment is composed of the following components
    that we will show you how to implement:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker推理部署由以下组件组成，我们将向您展示如何实现这些组件：
- en: '**SageMaker endpoint**: An endpoint is a scalable and secure API that SageMaker
    hosts to enable real-time predictions from deployed models. It’s essentially the
    interface through which applications interact with your model. Once deployed,
    an application can make HTTP requests to the endpoint to receive real-time predictions.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker端点**：端点是一个可扩展且安全的API，SageMaker托管它以启用从部署模型进行实时预测。它本质上是应用程序与您的模型交互的接口。一旦部署，应用程序就可以向端点发送HTTP请求以接收实时预测。'
- en: '**SageMaker model**: In SageMaker, a model is an artifact that results from
    training an algorithm. It contains the information required to make predictions,
    including the weights and computation logic. You can create multiple models and
    use them in different configurations or for various predictions.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker模型**：在SageMaker中，模型是训练算法后产生的工件。它包含进行预测所需的信息，包括权重和计算逻辑。您可以创建多个模型，并在不同的配置或用于各种预测中使用它们。'
- en: '**SageMaker configuration**: This configuration specifies the hardware and
    software set up to host the model. It defines the resources required for the endpoint,
    such as the type and number of ML compute instances. Endpoint configurations are
    used when creating or updating an endpoint. They allow for flexibility in the
    deployment and scalability of the hosted models.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker配置**：此配置指定了托管模型所需的硬件和软件设置。它定义了端点所需资源，例如ML计算实例的类型和数量。在创建或更新端点时使用端点配置。它们允许在托管模型的部署和可扩展性方面具有灵活性。'
- en: '**SageMaker Inference component**: This is the last piece of the puzzle that
    connects the model and configuration to anendpoint. You can deploy multiple models
    to an endpoint, each with its resource configuration. Once deployed, models are
    easily accessible via the InvokeEndpoint API in Python.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SageMaker推理组件**：这是拼图中最后一块，将模型和配置连接到端点。您可以将多个模型部署到端点，每个模型都有自己的资源配置。一旦部署，模型可以通过Python中的InvokeEndpoint
    API轻松访问。'
- en: Together, these components create a robust infrastructure for deploying and
    managing ML models in SageMaker, enabling scalable, secure, and efficient real-time
    predictions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件共同为在SageMaker中部署和管理机器学习模型提供了一个强大的基础设施，使可扩展、安全且高效的实时预测成为可能。
- en: Other popular cloud platforms offer the exact solutions. For example, you have
    Azure OpenAI instead of Bedrock and Azure ML instead of SageMaker on Azure. The
    list of ML deployment tools, such as Hopsworks, Modal, Vertex AI, Seldon, BentoML,
    and many more, is endless and will probably change. What is essential though is
    to understand your use case requirements and find a tool that fits your needs.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其他流行的云平台提供了确切解决方案。例如，在Azure上，您有Azure OpenAI代替Bedrock，以及Azure ML代替SageMaker。机器学习部署工具的列表，如Hopsworks、Modal、Vertex
    AI、Seldon、BentoML等，是无穷无尽的，并且可能会发生变化。然而，重要的是要了解您的用例需求，并找到一个适合您需求的工具。
- en: The training versus the inference pipeline
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练与推理管道的比较
- en: Understanding the nuances between the training and inference pipelines is crucial
    before we deploy the inference pipeline. While it might seem straightforward that
    the training pipeline is for training and the inference pipeline is for inference,
    there are significant differences that we need to grasp to comprehend the technical
    aspects of our discussion fully.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署推理管道之前，理解训练和推理管道之间的细微差别至关重要。虽然训练管道用于训练，推理管道用于推理可能看起来很简单，但我们需要充分理解这些差异，以全面理解我们讨论的技术方面。
- en: One key difference lies in how data is handled and accessed within each pipeline.
    During training, data is typically accessed from offline storage in batch mode,
    optimized for throughput and ensuring data lineage. For example, our LLM Twin
    architecture uses ZenML artifacts to access, version, and track data fed to the
    training loop in batches. In contrast, the inference pipeline requires an online
    DB optimized for low latency. We will leverage the Qdrant vector DB to grab the
    necessary context for RAG. In this context, the focus shifts from data lineage
    and versioning to quick data access, ensuring a seamless user experience. Additionally,
    the outputs of these pipelines also differ significantly. The training pipeline
    outputs trained model weights stored in the model registry. Meanwhile, the inference
    pipeline outputs predictions served directly to the user.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键的区别在于每个管道内部处理和访问数据的方式。在训练过程中，数据通常以批量模式从离线存储中访问，优化了吞吐量并确保了数据可追溯性。例如，我们的LLM
    Twin架构使用ZenML工件以批量方式访问、版本控制和跟踪训练循环中提供的数据。相比之下，推理管道需要一个针对低延迟优化的在线数据库。我们将利用Qdrant向量数据库来获取RAG所需的必要上下文。在这种情况下，重点从数据可追溯性和版本控制转移到快速数据访问，确保无缝的用户体验。此外，这些管道的输出也显著不同。训练管道输出存储在模型注册表中的训练模型权重。同时，推理管道输出直接提供给用户的预测。
- en: Also, the infrastructure required for each pipeline is different. The training
    pipeline demands more powerful machines equipped with as many GPUs as possible.
    This is because training involves batching data and holding all the necessary
    gradients in memory for optimization steps, making it highly compute-intensive.
    More computational power and VRAM allow larger batches (or throughput), reducing
    training time and enabling more extensive experimentation. On the other hand,
    the inference pipeline typically requires less computation. Inference often involves
    passing a single sample or smaller batches to the model without the need for optimization
    steps.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个管道所需的架构也不同。训练管道需要配备尽可能多的 GPU 的更强大的机器。这是因为训练涉及批量处理数据并在内存中保留所有必要的梯度以进行优化步骤，这使得它非常计算密集。更多的计算能力和
    VRAM 允许更大的批量（或吞吐量），从而减少训练时间并允许更广泛的实验。另一方面，推理管道通常需要的计算较少。推理通常涉及将单个样本或更小的批量传递到模型中，而不需要优化步骤。
- en: Despite these differences, there is some overlap between the two pipelines,
    particularly regarding preprocessing and post-processing steps. Applying the same
    preprocessing and post-processing functions and hyperparameters during training
    and inference is crucial. Any discrepancies can lead to what is known as training-serving
    skew, where the model’s performance during inference deviates from its performance
    during training.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这两个管道之间存在一些差异，但在预处理和后处理步骤方面有一些重叠。在训练和推理过程中应用相同的预处理和后处理函数以及超参数至关重要。任何差异都可能导致所谓的训练-服务偏差，即模型在推理时的性能与其在训练时的性能不一致。
- en: Deploying the LLM Twin service
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 LLM Twin 服务
- en: The last step is implementing the architecture presented in the previous section.
    More concretely, we will deploy the LLM microservice using AWS SageMaker and the
    business microservice using FastAPI. Within the business microservice, we will
    glue the RAG logic written in *Chapter 9* with our fine-tuned LLM Twin, ultimately
    being able to test out the inference pipeline end to end.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是实现前一部分中提出的架构。更具体地说，我们将使用 AWS SageMaker 部署 LLM 微服务，使用 FastAPI 部署业务微服务。在业务微服务中，我们将把第
    9 章中编写的 RAG 逻辑与我们的微调后的 LLM Twin 结合起来，最终能够端到端地测试推理管道。
- en: Serving the ML model is one of the most critical steps in any ML application’s
    life cycle, as users can only interact with our model after this phase is completed.
    If the serving architecture isn’t designed correctly or if the infrastructure
    isn’t working properly, it doesn’t matter that you have implemented a powerful
    and excellent model. As long as the user cannot appropriately interact with it,
    it has near zero value from a business point of view. For example, if you have
    the best code assistant on the market, but the latency to use it is too high,
    or the API calls keep crashing, the user will probably switch to a less performant
    code assistant that works faster and is more stable.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何机器学习应用程序的生命周期中，提供机器学习模型是其中最关键的步骤之一，因为用户只能在完成此阶段后与我们的模型交互。如果服务架构设计不当或基础设施运行不正常，那么你即使实现了强大而优秀的模型也没有意义。只要用户不能适当地与之交互，从商业角度来看，它的价值几乎为零。例如，如果你拥有市场上最好的代码助手，但使用它的延迟太高，或者
    API 调用不断崩溃，用户可能会转而使用性能较差但运行更快且更稳定的代码助手。
- en: 'Thus, in this section, we will show you how to:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本节中，我们将向您展示如何：
- en: Deploy our fined-tuned LLM Twin model to AWS SageMaker
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们的微调后的 LLM Twin 模型部署到 AWS SageMaker
- en: Write an inference client to interact with the deployed model
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写一个推理客户端与部署的模型交互
- en: Write the business service in FastAPI
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 FastAPI 编写业务服务
- en: Integrate our RAG logic with our fine-tuned LLM
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们的 RAG 逻辑与微调后的 LLM 集成
- en: Implement autoscaling rules for the LLM microservice
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为 LLM 微服务实现自动扩展规则
- en: Implementing the LLM microservice using AWS SageMaker
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AWS SageMaker 实现 LLM 微服务
- en: We aim to deploy the LLM Twin model, stored in Hugging Face’s model registry,
    to Amazon SageMaker as an online real-time inference endpoint. We will leverage
    Hugging Face’s specialized inference container, known as the Hugging Face LLM
    **DLC**, to deploy our LLM.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将存储在 Hugging Face 模型注册表中的 LLM Twin 模型部署到 Amazon SageMaker 作为在线实时推理端点。我们将利用
    Hugging Face 的专用推理容器，称为 Hugging Face LLM **DLC**，来部署我们的 LLM。
- en: What are Hugging Face’s DLCs?
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Hugging Face 的 DLC 是什么？
- en: DLCs are specialized Docker images that come pre-loaded with essential deep-learning
    frameworks and libraries, including popular tools like transformers, datasets,
    and tokenizers from Hugging Face. These containers are designed to simplify the
    process of training and deploying models by eliminating the need for complex environment
    setup and optimization. The Hugging Face Inference DLC, in particular, includes
    a fully integrated serving stack, significantly simplifying the deployment process
    and reducing the technical expertise needed to serve deep learning models in production.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: DLC是预装了必要的深度学习框架和库的专用Docker镜像，包括来自Hugging Face的流行工具，如transformers、datasets和tokenizers。这些容器旨在通过消除复杂环境设置和优化的需求来简化模型训练和部署的过程。特别是，Hugging
    Face Inference DLC包括一个完全集成的服务栈，极大地简化了部署过程，并减少了在生产中部署深度学习模型所需的技术专业知识。
- en: 'When it comes to serving models, the DLC is powered by the **Text Generation
    Inference** (**TGI**) engine, made by Hugging Face: [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到模型部署时，DLC由Hugging Face制作的**文本生成推理**（**TGI**）引擎提供支持：[https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference).
- en: 'TGI is an open-source solution for deploying and serving LLMs. It offers high-performance
    text generation using tensor parallelism and dynamic batching for the most popular
    open-source LLMs available on Hugging Face, such as Mistral, Llama, and Falcon.
    To sum up, the most powerful features the DLC image provides are:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: TGI是一个开源解决方案，用于部署和托管LLMs。它提供了使用张量并行性和动态批处理的高性能文本生成，适用于Hugging Face上最流行的开源LLMs，如Mistral、Llama和Falcon。总结一下，DLC镜像提供的最强大功能包括：
- en: '**Tensor parallelism**, thus enhancing the computational efficiency of model
    inference'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tensor并行性**，从而提高模型推理的计算效率'
- en: '**Optimized transformers code for inference**, leveraging flash-attention to
    maximize performance across the most widely used architectures: [https://github.com/Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention
    )'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化后的推理Transformer代码**，利用flash-attention技术以最大化性能，适用于最广泛使用的架构：[https://github.com/Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)'
- en: '**Quantization with** `bitsandbytes` that reduces the model size while maintaining
    performance, making deployments more efficient: [https://github.com/bitsandbytes-foundation/bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes
    )'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用`bitsandbytes`进行量化**，在保持性能的同时减小模型大小，使部署更加高效：[https://github.com/bitsandbytes-foundation/bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes)'
- en: '**Continuous batching of incoming requests**, thus improving throughput by
    dynamically batching requests as they arrive'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对传入请求进行连续批处理**，从而通过动态批处理请求来提高吞吐量'
- en: '**Accelerated weight loading** by utilizing `safetensors` for faster model
    initialization, reducing start-up time: [https://github.com/huggingface/safetensors](https://github.com/huggingface/safetensors
    )'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通过利用`safetensors`加速权重加载**，以更快的模型初始化速度，减少启动时间：[https://github.com/huggingface/safetensors](https://github.com/huggingface/safetensors)'
- en: '**Token streaming** that supports real-time interactions through **Server-Sent
    Events** (**SSE**)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持通过** **Server-Sent Events**（**SSE**）**进行令牌流**，以支持实时交互'
- en: To summarize, our LLM Twin model will run inside DLC Docker images, listening
    to requests, optimizing the LLM for inference, and serving the results in real
    time. The DLC’s Docker images will be hosted on AWS SageMaker under inference
    endpoints that can be accessed through HTTP requests. With that in mind, let’s
    move on to the implementation. We will start by deploying the LLM and then writing
    a wrapper class to interact with the SageMaker Inference endpoint.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的LLM Twin模型将在DLC Docker镜像中运行，监听请求，优化LLM以进行推理，并实时提供服务。DLC的Docker镜像将托管在AWS
    SageMaker的推理端点上，可以通过HTTP请求访问。考虑到这一点，让我们继续实施。我们将从部署LLM开始，然后编写一个包装类来与SageMaker Inference端点交互。
- en: Configuring SageMaker roles
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置SageMaker角色
- en: The first step is to create the proper AWS **Identity and Access Management**
    (**IAM**) users and roles to access and deploy the SageMaker infrastructure. AWS
    IAM controls who can authenticate and what any actor has access to. You can create
    new users (assigned to people) and new roles (assigned to other actors within
    your infrastructure, such as EC2 VMs) through IAM.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建适当的 AWS **身份和访问管理**（**IAM**）用户和角色，以便访问和部署 SageMaker 基础设施。AWS IAM 控制谁可以进行身份验证以及任何行为者可以访问什么。您可以通过
    IAM 创建新的用户（分配给人员）和新的角色（分配给您基础设施中的其他行为者，例如 EC2 虚拟机）。
- en: The whole deployment process is automated. We will have to run a few CLI commands,
    but first, ensure that you have correctly configured the `AWS_ACCESS_KEY`, `AWS_SECRET_KEY`,
    and `AWS_REGION` environmental variables in the `.env` file. At this step, the
    easiest way is to use the credentials attached to an admin role as, in the following
    steps, we will create a set of narrower IAM roles used in the rest of the chapter.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 整个部署过程是自动化的。我们可能需要运行一些 CLI 命令，但首先，请确保您已正确配置 `.env` 文件中的 `AWS_ACCESS_KEY`、`AWS_SECRET_KEY`
    和 `AWS_REGION` 环境变量。在这一步，最简单的方法是使用附加到管理员角色的凭证，因为在接下来的步骤中，我们将创建一组用于本章其余部分的更狭窄的
    IAM 角色。
- en: 'After you configured your `.env` file, we have to:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在您配置了 `.env` 文件之后，我们必须：
- en: 'Create an IAM user restricted to creating and deleting only the resources we
    need for the deployment, such as SageMaker itself, **Elastic Container Registry**
    (**ECR**), and S3\. To make it, run the following:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 IAM 用户，该用户仅限于创建和删除我们部署所需的资源，例如 SageMaker 本身、**弹性容器注册库**（**ECR**）和 S3。要创建它，请运行以下命令：
- en: '[PRE0]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This command will generate a JSON file called `sagemaker_user_credentials.json`
    that contains a new AWS access and secret key. From now on, we will use these
    credentials to deploy everything related to SageMaker to ensure we modify only
    the resources associated with SageMaker. Otherwise, we could accidentally modify
    other AWS resources using an admin account, resulting in additional costs or altering
    other existing projects. Thus, having a narrow role only to your use case is good
    practice.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将生成一个名为 `sagemaker_user_credentials.json` 的 JSON 文件，其中包含新的 AWS 访问密钥和秘密密钥。从现在开始，我们将使用这些凭证来部署与
    SageMaker 相关的所有内容，以确保我们只修改与 SageMaker 相关的资源。否则，我们可能会意外地使用管理员账户修改其他 AWS 资源，导致额外的费用或更改其他现有项目。因此，只针对您的用例拥有一个狭窄的角色是良好的实践。
- en: The last step is to take the new credentials from the JSON file and update the
    `AWS_ACCESS_KEY` and `AWS_SECRET_KEY` variables in your `.env` file. You can check
    out the implementation at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_sagemaker_role.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_sagemaker_role.py).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是从 JSON 文件中获取新的凭证并更新 `.env` 文件中的 `AWS_ACCESS_KEY` 和 `AWS_SECRET_KEY` 变量。您可以在
    [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_sagemaker_role.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_sagemaker_role.py)
    查看实现。
- en: 'Create an IAM execution role. We will attach this role to the SageMaker deployment,
    empowering it to access other AWS resources on our behalf. This is standard practice
    for cloud deployments, as instead of authenticating every machine within your
    credentials, you attach a role that allows them to access only what is necessary
    from your infrastructure. In our case, we will provide SageMaker access to AWS
    S3, CloudWatch, and ECR. To create the role, run the following:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 IAM 执行角色。我们将把这个角色附加到 SageMaker 部署中，使其能够代表我们访问其他 AWS 资源。这是云部署的标准做法，因为您不需要为您的凭证中的每一台机器进行身份验证，而是附加一个角色，允许它们仅从您的基础设施中访问必要的资源。在我们的例子中，我们将为
    SageMaker 提供访问 AWS S3、CloudWatch 和 ECR 的权限。要创建该角色，请运行以下命令：
- en: '[PRE1]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This command will generate a JSON file called `sagemaker_execution_role.json`
    that contains the **Amazon Resource Name** (**ARN**) of the newly created role.
    The ARN is an ID attached to any AWS resource to identify it across your cloud
    infrastructure. Take the ARN value from the JSON file and update the `AWS_ARN_ROLE`
    variable from your `.env` file with it. You can check out the implementation at
    [https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_execution_role.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_execution_role.py).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将生成一个名为`sagemaker_execution_role.json`的JSON文件，其中包含新创建角色的**Amazon资源名称**（**ARN**）。ARN是附加到任何AWS资源上的ID，用于在您的云基础设施中识别它。从JSON文件中获取ARN值，并用它更新`.env`文件中的`AWS_ARN_ROLE`变量。您可以在[https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_execution_role.py](https://github.com/PacktPublishing/LLM-Engineers-Handbook/blob/main/llm_engineering/infrastructure/aws/roles/create_execution_role.py)查看实现。
- en: 'If you have issues, configure the AWS CLI with the same AWS credentials as
    in the `.env` file and repeat the process. Official documentation for installing
    the AWS CLI: [https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遇到问题，请使用与`.env`文件中相同的AWS凭证配置AWS CLI，并重复此过程。安装AWS CLI的官方文档：[https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)。
- en: By setting the IAM user and role in your `.env` file, we will automatically
    load them in the settings Python object and use them throughout the following
    steps. Now, let’s move on to the actual deployment.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`.env`文件中设置IAM用户和角色，我们将自动在Python设置对象中加载它们，并在以下步骤中使用它们。现在，让我们继续实际部署。
- en: Deploying the LLM Twin model to AWS SageMaker
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将LLM Twin模型部署到AWS SageMaker
- en: The deployment of AWS SageMaker is fully automated through a set of Python classes,
    which we will cover in this chapter. This section aims to understand how we configure
    the SageMaker infrastructure directly from Python. Thus, you don’t have to run
    everything step by step, as in a standard tutorial, but only to understand the
    code.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: AWS SageMaker的部署完全通过一组Python类实现自动化，我们将在本章中介绍这些类。本节旨在理解如何直接从Python配置SageMaker基础设施。因此，你不需要像在标准教程中那样一步一步运行所有内容，只需理解代码即可。
- en: 'We can initiate and finalize the entire SageMaker deployment using a simple
    CLI command: `poe deploy-inference-endpoint`. This command will initialize all
    the steps presented in *Figure 10.5*, except for creating the SageMaker AWS IAMs
    we created and configured in the previous step.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用简单的CLI命令`poe deploy-inference-endpoint`启动和完成整个SageMaker部署。此命令将初始化*图10.5*中展示的所有步骤，但不会创建我们在上一步中创建和配置的SageMaker
    AWS IAMs。
- en: In this section, we will walk you through the code presented in *Figure 10.5*
    that helps us fully automate the deployment process, starting with the `create_endpoint()`
    function. Ultimately, we will test the CLI command and check the AWS console to
    see whether the deployment was successful. The SageMaker deployment code is available
    at [https://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy](https://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示*图10.5*中呈现的代码，该代码帮助我们完全自动化部署过程，从`create_endpoint()`函数开始。最终，我们将测试CLI命令并检查AWS控制台，以查看部署是否成功。SageMaker部署代码可在[https://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy](https://github.com/PacktPublishing/LLM-Engineers-Handbook/tree/main/llm_engineering/infrastructure/aws/deploy)找到。
- en: '![](img/B31105_10_05.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_10_05.png)'
- en: 'Figure 10.5: AWS SageMaker deployment steps'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5：AWS SageMaker部署步骤
- en: 'We will take a top-down approach to walk you through the implementation, starting
    with the main function that deploys the LLM Twin model to AWS SageMaker. In the
    function below, we first take the latest version of the Docker DLC image using
    the `get_huggingface_llm_image_uri()` function, which is later passed to the deployment
    strategy class, along with an instance of the resource manager and deployment
    service:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用自上而下的方法向您介绍实现过程，从将LLM Twin模型部署到AWS SageMaker的主函数开始。在下面的函数中，我们首先使用`get_huggingface_llm_image_uri()`函数获取Docker
    DLC镜像的最新版本，该函数随后传递给部署策略类，以及资源管理器和部署服务的实例：
- en: '[PRE2]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We must review the three classes used in the `create_endpoint()` function to
    fully understand the deployment process. Let’s start with the `ResourceManager`
    class. The class begins with the initialization method, establishing the connection
    to AWS SageMaker using boto3, the AWS SDK for Python, which provides the necessary
    functions to interact with various AWS services, including SageMaker.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须回顾 `create_endpoint()` 函数中使用的三个类，以全面了解部署过程。让我们从 `ResourceManager` 类开始。该类从初始化方法开始，使用
    boto3（Python 的 AWS SDK）建立与 AWS SageMaker 的连接，它提供了与各种 AWS 服务（包括 SageMaker）交互所需的必要功能。
- en: '[PRE3]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we implement the `endpoint_config_exists` method, checking whether a
    specific SageMaker endpoint configuration exists:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现了 `endpoint_config_exists` 方法，用于检查是否存在特定的 SageMaker 端点配置：
- en: '[PRE4]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The class also includes the `endpoint_exists` method, which checks the existence
    of a specific SageMaker endpoint:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 该类还包括 `endpoint_exists` 方法，用于检查特定 SageMaker 端点是否存在：
- en: '[PRE5]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s move to the `DeploymentService`. Within the constructor, we set up the
    `sagemaker_client`, which will interface with AWS SageMaker and an instance of
    the `ResourceManager` class we talked about earlier:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转到 `DeploymentService`。在构造函数中，我们设置了 `sagemaker_client`，它将与 AWS SageMaker
    以及我们之前提到的 `ResourceManager` 类实例进行接口：
- en: '[PRE6]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `deploy()` method is the heart of the `DeploymentService` class. This method
    orchestrates the entire process of deploying a model to a SageMaker endpoint.
    It checks whether the necessary configurations are already in place and, if not,
    it triggers the deployment:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`deploy()` 方法是 `DeploymentService` 类的核心。此方法协调将模型部署到 SageMaker 端点的整个过程。它检查是否已设置必要的配置，如果没有，则触发部署：'
- en: '[PRE7]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The deploy method begins by checking whether the endpoint configuration already
    exists using the `resource_manager`. This step is crucial because it avoids unnecessary
    redeployment if the configuration is already set up. The deployment itself is
    handled by calling the `prepare_and_deploy_model()` method, which is responsible
    for the actual deployment of the model to the specified SageMaker endpoint.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: deploy 方法首先使用 `resource_manager` 检查端点配置是否已存在。这一步至关重要，因为它可以避免在配置已设置的情况下进行不必要的重新部署。部署本身是通过调用
    `prepare_and_deploy_model()` 方法来处理的，该方法负责将模型实际部署到指定的 SageMaker 端点。
- en: 'The `prepare_and_deploy_model()` method is a static method within the `DeploymentService`
    class. This method is focused on setting up and deploying the Hugging Face model
    to SageMaker:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`prepare_and_deploy_model()` 方法是 `DeploymentService` 类中的一个静态方法。此方法专注于设置和部署
    Hugging Face 模型到 SageMaker：'
- en: '[PRE8]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This method begins by creating an instance of HuggingFaceModel, a specialized
    model class from SageMaker designed to handle Hugging Face models. The constructor
    for HuggingFaceModel takes several essential parameters, such as the role ARN
    (which gives SageMaker the necessary permissions), the URI of the LLM DLC Docker
    image, and the LLM configuration that specifies what LLM to load from Hugging
    Face and its inference parameters, such as the maximum total of tokens.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法首先创建一个 HuggingFaceModel 实例，这是一个 SageMaker 的专用模型类，用于处理 Hugging Face 模型。HuggingFaceModel
    构造函数接受多个基本参数，例如角色 ARN（它为 SageMaker 提供必要的权限）、LLM DLC Docker 图像的 URI 以及 LLM 配置，该配置指定从
    Hugging Face 加载哪个 LLM 以及其推理参数，例如最大令牌总数。
- en: Once HuggingFaceModel is instantiated, the method deploys it to SageMaker using
    the deploy function. This deployment process involves specifying the type of instance
    used, the number of instances, and whether to update an existing endpoint or create
    a new one. The method also includes optional resources for more complex deployments,
    such as the `initial_instance_count` parameter for multi-model endpoints and tags
    for tracking and categorization.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦实例化了 HuggingFaceModel，该方法就会使用 deploy 函数将其部署到 SageMaker。此部署过程包括指定使用的实例类型、实例数量以及是否更新现有端点或创建新端点。该方法还包括用于更复杂部署的可选资源，例如用于多模型端点的
    `initial_instance_count` 参数和用于跟踪和分类的标签。
- en: The last step is to walk you through the `SagemakerHuggingfaceStrategy` class,
    which aggregates everything we have shown. The class is initialized only with
    an instance of a deployment service, such as the one shown above.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是向您介绍 `SagemakerHuggingfaceStrategy` 类，该类汇总了我们展示的所有内容。该类仅初始化一个部署服务实例，如上面所示。
- en: '[PRE9]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The core functionality of the `SagemakerHuggingfaceStrategy` class is encapsulated
    in its `deploy()` method. This method orchestrates the deployment process, taking
    various parameters that define how the Hugging Face model should be deployed to
    AWS SageMaker:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`SagemakerHuggingfaceStrategy` 类的核心功能封装在其 `deploy()` 方法中。此方法协调部署过程，接受各种参数，这些参数定义了如何将
    Hugging Face 模型部署到 AWS SageMaker：'
- en: '[PRE10]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The parameters passed into the method are crucial to the deployment process:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给方法中的参数对于部署过程至关重要：
- en: '`role_arn`: The AWS IAM role that provides permissions for the SageMaker deployment.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`role_arn`：提供 SageMaker 部署权限的 AWS IAM 角色。'
- en: '`llm_image`: The URI of the DLC Docker image'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`llm_image`：DLC Docker 图像的 URI。'
- en: '`config`: A dictionary containing configuration settings for the model environment.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`config`：包含模型环境配置设置的字典。'
- en: '`endpoint_name` and `endpoint_config_name`: Names for the SageMaker endpoint
    and its configuration, respectively.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`endpoint_name` 和 `endpoint_config_name`：分别为 SageMaker 端点和其配置的名称。'
- en: '`gpu_instance_type`: The type of the GPU EC2 instances used for the deployment.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gpu_instance_type`：用于部署的 GPU EC2 实例的类型。'
- en: '`resources`: Optional resources dictionary used for multi-model endpoint deployments.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resources`：用于多模型端点部署的可选资源字典。'
- en: '`endpoint_type`: This can either be `MODEL_BASED` or `INFERENCE_COMPONENT`,
    determining whether the endpoint includes an inference component.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`endpoint_type`：这可以是 `MODEL_BASED` 或 `INFERENCE_COMPONENT`，决定端点是否包含推理组件。'
- en: The method delegates the actual deployment process to the `deployment_service`.
    This delegation is a critical aspect of the strategy pattern, allowing for flexibility
    in how the deployment is carried out without altering the high-level deployment
    logic.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法将实际的部署过程委托给 `deployment_service`。这种委托是策略模式的关键方面，允许在不改变高级部署逻辑的情况下，灵活地执行部署。
- en: '[PRE11]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Also, let’s review the resource configuration to understand the infrastructure
    better. These resources are leveraged when setting up multi-endpoint configurations
    that use multiple replicas to serve clients while respecting the latency and throughput
    requirements of the application. The `ResourceRequirements` object is initialized
    with a dictionary that specifies various resource parameters. These parameters
    include the number of replicas (copies) of the model to be deployed, the number
    of GPUs required, the number of CPU cores, and the memory allocation in megabytes.
    Each of these parameters plays a crucial role in the performance and scalability
    of the deployed model.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，让我们回顾资源配置，以更好地了解基础设施。当设置使用多个副本来服务客户端并满足应用程序的延迟和吞吐量要求的多端点配置时，这些资源被利用。`ResourceRequirements`
    对象使用一个字典初始化，该字典指定了各种资源参数。这些参数包括要部署的模型副本（副本）数量、所需的 GPU 数量、CPU 核心数以及以兆字节为单位的内存分配。这些参数中的每一个都在部署模型的性能和可扩展性中扮演着至关重要的角色。
- en: '[PRE12]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**copies**: This parameter determines how many instances or replicas of the
    model should be deployed. Having multiple replicas can help in reducing latency
    and increasing throughput.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**copies**：此参数确定应部署多少个模型实例或副本。拥有多个副本可以帮助降低延迟并提高吞吐量。'
- en: '**num_accelerators**: This parameter specifies the number of GPUs to allocate.
    Since LLMs are computationally intensive, multiple GPUs are typically required
    to accelerate inference processes.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**num_accelerators**：此参数指定要分配的 GPU 数量。由于 LLM 计算密集，通常需要多个 GPU 来加速推理过程。'
- en: '**num_cpus:** This defines the number of CPU cores the deployment should have.
    The number of CPUs impacts the model’s ability to handle data preprocessing, post-processing,
    and other tasks that are less GPU-dependent but still essential.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**num_cpus**：这定义了部署应具有的 CPU 核心数。CPU 的数量影响模型处理数据预处理、后处理和其他任务的能力，这些任务虽然不太依赖于
    GPU，但仍然是必不可少的。'
- en: '**memory**: The memory parameter sets the minimum amount of RAM required for
    the deployment. Adequate memory is necessary to ensure the model can load and
    operate without running into memory shortages.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**memory**：内存参数设置部署所需的最低 RAM 量。足够的内存对于确保模型可以加载和运行而不会遇到内存不足至关重要。'
- en: By setting these parameters, the class ensures that it has sufficient resources
    to operate efficiently when the model is deployed to a SageMaker endpoint. The
    precise tuning of these values will vary depending on the LLM’s specific requirements,
    such as its size, the complexity of the tasks it will perform, and the expected
    load. To get a better understanding of how to use them, after deploying the endpoint,
    we suggest modifying them and seeing how the performance of the LLM microservice
    changes.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置这些参数，该类确保当模型部署到 SageMaker 端点时，它有足够的资源来高效运行。这些值的精确调整将根据 LLM 的具体要求而变化，例如其大小、将要执行的任务的复杂性以及预期的负载。为了更好地理解如何使用它们，在部署端点后，我们建议修改它们并观察
    LLM 微服务的性能如何变化。
- en: Ultimately, let’s review the settings configuring the LLM engine. The `HF_MODEL_ID`
    identifies which Hugging Face model to deploy. For example, in the settings class,
    we set it to `mlabonne/TwinLlama-3.1-8B-13` to load our custom LLM Twin model
    stored in Hugging Face. `SM_NUM_GPUS` specifies the number of GPUs allocated per
    model replica, which is crucial for fitting your model into the GPU’s VRAM. `HUGGING_FACE_HUB_TOKEN`
    provides access to the Hugging Face Hub for model retrieval. `HF_MODEL_QUANTIZE`
    specifies what quantization technique to use, while the rest of the variables
    control the LLM token generation process.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们回顾一下配置 LLM 引擎的设置。`HF_MODEL_ID` 识别要部署哪个 Hugging Face 模型。例如，在设置类中，我们将其设置为
    `mlabonne/TwinLlama-3.1-8B-13` 以加载存储在 Hugging Face 中的自定义 LLM Twin 模型。`SM_NUM_GPUS`
    指定每个模型副本分配的 GPU 数量，这对于将模型适配到 GPU 的 VRAM 中至关重要。`HUGGING_FACE_HUB_TOKEN` 提供对 Hugging
    Face Hub 的访问权限以检索模型。`HF_MODEL_QUANTIZE` 指定要使用的量化技术，而其余变量控制 LLM 令牌生成过程。
- en: '[PRE13]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Using these two configurations, we fully control our infrastructure, what LLM
    to use, and how it behaves. To start the SageMaker deployment with the configuration
    shown above, call the `create_endpoint()` function (presented at the beginning
    of the section) as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两个配置，我们完全控制我们的基础设施，选择哪个 LLM 使用，以及它的行为方式。要启动具有上述配置的 SageMaker 部署，请按照以下方式调用
    `create_endpoint()` 函数（本节开头介绍）：
- en: '[PRE14]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For convenience, we also wrapped it up under a `poe` command:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们还将其封装在一个 `poe` 命令下：
- en: '[PRE15]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: That’s all you need to deploy an inference pipeline to AWS SageMaker. The hardest
    part is finding the correct configuration to fit your needs while reducing your
    infrastructure’s costs. Depending on AWS, this will take up to 15-30 minutes to
    deploy. You can always change any value directly from your `.env` file and deploy
    the model with a different configuration without touching the code. For example,
    our default values use a single GPU instance of type `ml.g5.xlargeGPU`. If you
    want more replicas, you can tweak the `GPUS` and `SM_NUM_GPUS` settings or change
    your instance type by changing the `GPU_INSTANCE_TYPE` variable.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 部署推理管道到 AWS SageMaker 所需的步骤就是这些。最困难的部分是找到适合您需求且能降低基础设施成本的正确配置。根据 AWS，这需要 15-30
    分钟才能部署。您可以直接从 `.env` 文件中更改任何值，并使用不同的配置部署模型，而无需修改代码。例如，我们的默认值使用单个 `ml.g5.xlargeGPU`
    类型的 GPU 实例。如果您需要更多副本，可以调整 `GPUS` 和 `SM_NUM_GPUS` 设置，或者通过更改 `GPU_INSTANCE_TYPE`
    变量来更改实例类型。
- en: Before deploying the LLM microservice to AWS SageMaker, ensure that you’ve generated
    a user role by running `poetry poe create-sagemaker-role` and an execution role
    by running `poetry poe create-sagemaker-execution-role`. Also, ensure you update
    your `AWS_*` environment variables in your `.env` file with the credentials generated
    by the two steps. You can find more details on this aspect in the repository’s
    README file.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 LLM 微服务部署到 AWS SageMaker 之前，请确保您已通过运行 `poetry poe create-sagemaker-role`
    生成用户角色，并通过运行 `poetry poe create-sagemaker-execution-role` 生成执行角色。此外，请确保您已更新 `.env`
    文件中的 `AWS_*` 环境变量，以包含由这两个步骤生成的凭证。您可以在存储库的 README 文件中找到更多关于此方面的详细信息。
- en: After deploying the AWS SageMaker Inference endpoint, you can navigate to the
    SageMaker dashboard in AWS to visualize it. First, in the left panel, click on
    **SageMaker dashboard**, and then in the **Inference** column, click on the **Endpoints**
    button, as illustrated in *Figure 10.6*.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署 AWS SageMaker 推理端点后，您可以通过 AWS 中的 SageMaker 仪表板来可视化它。首先，在左侧面板中点击**SageMaker
    仪表板**，然后在**推理**列中点击**端点**按钮，如图 10.6 所示。
- en: '![](img/B31105_10_06.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_10_06.png)'
- en: 'Figure 10.6: AWS SageMaker Inference endpoints example'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6：AWS SageMaker 推理端点示例
- en: After clicking the **Endpoints** button, you will see your **twin** endpoint
    in a **Creating** or **Created** status, as seen in *Figure 10.7*. After clicking
    on it, you can look at the endpoint’s logs in CloudWatch and monitor the CPU,
    memory, disk, and GPU utilization.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**端点**按钮后，您将看到您的**双端点**处于**创建中**或**已创建**状态，如图*图10.7*所示。点击它后，您可以在CloudWatch中查看端点的日志并监控CPU、内存、磁盘和GPU的利用率。
- en: Also, they provide an excellent way to monitor all the HTTP errors, such as
    `4XX` and `5XX`, in one place.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它们提供了一个很好的方法，可以在一个地方监控所有HTTP错误，例如`4XX`和`5XX`。
- en: '![](img/B31105_10_07.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_10_07.png)'
- en: 'Figure 10.7: AWS SageMaker twin inference endpoint example'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7：AWS SageMaker双端点推理端点示例
- en: Calling the AWS SageMaker Inference endpoint
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调用AWS SageMaker Inference端点
- en: 'Now that our LLM service has been deployed on AWS SageMaker, let’s learn how
    to call the service. To do so, we will write two classes that will help us prepare
    the prompt for SageMaker, call the inference endpoint through HTTP requests, and
    decode the results in a way the client can work with. All the AWS SageMaker Inference
    code is available on GitHub at `llm_engineering/model/inference`. It all starts
    with the following example:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将LLM服务部署到AWS SageMaker，让我们学习如何调用该服务。为此，我们将编写两个类，这些类将帮助我们为SageMaker准备提示，通过HTTP请求调用推理端点，并以客户端可以处理的方式解码结果。所有AWS
    SageMaker Inference代码均可在GitHub上找到，位于`llm_engineering/model/inference`。一切始于以下示例：
- en: '[PRE16]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As before, we will walk you through the `LLMInferenceSagemakerEndpoint` and
    `InferenceExecutor` classes. Let’s start with the `LLMInferenceSagemakerEndpoint`
    class, which directly interacts with SageMaker. The constructor initializes all
    the essential attributes necessary to interact with the SageMaker endpoint:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将向您介绍`LLMInferenceSagemakerEndpoint`和`InferenceExecutor`类。让我们从`LLMInferenceSagemakerEndpoint`类开始，该类直接与SageMaker交互。构造函数初始化了与SageMaker端点交互所需的所有基本属性：
- en: '[PRE17]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`endpoint_name` is crucial for identifying the SageMaker endpoint we want to
    request. Additionally, the method initializes the payload using a provided value
    or by calling a method that generates a default payload if none is provided.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`endpoint_name`对于识别我们想要请求的SageMaker端点是至关重要的。此外，该方法使用提供的值初始化有效负载，如果没有提供，则通过调用生成默认有效负载的方法。'
- en: 'One of the key features of the class is its ability to generate a default payload
    for inference requests. This is handled by the `_default_payload()` method:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 该类的一个关键特性是它能够为推理请求生成默认的有效负载。这由`_default_payload()`方法处理：
- en: '[PRE18]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This method returns a dictionary that represents the default structure of the
    payload to be sent for inference. The parameters section includes settings that
    influence the model’s behavior during inference, such as the number of tokens
    to generate, the sampling strategy (`top_p`), and the temperature setting, which
    controls randomness in the output. These parameters are fetched from the application’s
    settings, ensuring consistency across different inference tasks.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法返回一个字典，表示要发送进行推理的有效负载的默认结构。参数部分包括影响推理期间模型行为的设置，例如要生成的标记数、采样策略（`top_p`）和温度设置，该设置控制输出中的随机性。这些参数从应用程序的设置中获取，确保不同推理任务之间的一致性。
- en: 'The class allows customization of the payload through the `set_payload()` method,
    which enables the user to modify the inputs and parameters before sending an inference
    request:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 该类允许通过`set_payload()`方法自定义有效负载，这使用户能够在发送推理请求之前修改输入和参数：
- en: '[PRE19]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This method updates the `inputs` field of the payload with the new input text
    provided by the user. Additionally, it allows for modifying inference parameters
    if any are provided.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将有效负载的`inputs`字段更新为用户提供的新的输入文本。此外，如果提供了任何推理参数，它还允许修改推理参数。
- en: 'Ultimately, we leverage the `inference()` method to call the SageMaker endpoint
    with the customized payload:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们利用`inference()`方法调用SageMaker端点，并使用自定义的有效负载：
- en: '[PRE20]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In this method, the inference method constructs the request to be sent to the
    SageMaker endpoint. The method packages the payload and other necessary details
    into a format SageMaker expects. If an `inference_component_name` is specified,
    it is included in the request, allowing for more granular control over the inference
    process if needed. The request is sent using the `invoke_endpoint()` function,
    and the response is read, decoded, and returned as a JSON object.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方法中，推理方法构建要发送到SageMaker端点的请求。该方法将有效载荷和其他必要细节打包成SageMaker期望的格式。如果指定了`inference_component_name`，它将被包含在请求中，如果需要，允许对推理过程进行更细粒度的控制。请求通过`invoke_endpoint()`函数发送，响应被读取、解码并返回为JSON对象。
- en: Let’s understand how the `InferenceExecutor` uses the `LLMInferenceSagemakerEndpoint`
    class we previously presented to send HTTP requests to the AWS SageMaker endpoint.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解`InferenceExecutor`如何使用我们之前展示的`LLMInferenceSagemakerEndpoint`类来向AWS SageMaker端点发送HTTP请求。
- en: The `InferenceExecutor` class begins with the constructor, which inputs key
    parameters for calling the LLM. The `llm` parameter accepts any instance that
    implements the Inference interface, such as the `LLMInferenceSagemakerEndpoint`
    class, which is used to perform the inference.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`InferenceExecutor`类从构造函数开始，该构造函数输入调用LLM所需的关键参数。`llm`参数接受任何实现Inference接口的实例，例如用于执行推理的`LLMInferenceSagemakerEndpoint`类。'
- en: 'Also, it accepts the query parameter, which represents the user input. Ultimately,
    it takes an optional context field if you want to do RAG, and you can customize
    the prompt template. If no prompt template is provided, it will default to a generic
    version that is not specialized in any LLM:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它接受查询参数，该参数表示用户输入。最终，如果您想进行RAG，它还接受一个可选的上下文字段，并且您可以自定义提示模板。如果没有提供提示模板，它将默认为一种通用的版本，这种版本不针对任何LLM进行专门化：
- en: '[PRE21]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `execute()` method is the key component of the `InferenceExecutor` class.
    This method is responsible for actually performing the inference. When execute
    is called, it prepares the payload sent to the LLM by formatting the prompt with
    the user’s query and context.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`execute()`方法是`InferenceExecutor`类的关键组件。该方法负责实际执行推理。当调用`execute`时，它会通过使用用户的查询和上下文格式化提示来准备发送给LLM的有效载荷。'
- en: Then, it configures several parameters that influence the behavior of the LLM,
    such as the maximum number of new tokens the model is allowed to generate, a repetition
    penalty to discourage the model from generating repetitive text, and the temperature
    setting that controls the randomness of the output.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，它配置了影响LLM行为的一些参数，例如模型允许生成的最大新令牌数、一个重复惩罚以阻止模型生成重复文本，以及控制输出随机性的温度设置。
- en: 'Once the payload and parameters are set, the method calls the `inference` function
    from `LLMInferenceSagemakerEndpoint` and waits for the generated answer:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置好有效载荷和参数，该方法就会调用`LLMInferenceSagemakerEndpoint`中的`inference`函数，并等待生成的答案：
- en: '[PRE22]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: By making the inference through an object that implements the Inference interface
    we decouple, we can easily inject other Inference strategies and the `LLMInferenceSagemakerEndpoint`
    implementation presented above without modifying different parts of the code.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 通过通过实现Inference接口的对象进行推理，我们可以轻松注入其他推理策略和上面展示的`LLMInferenceSagemakerEndpoint`实现，而无需修改代码的不同部分。
- en: 'Running a test example is straightforward. Simply call the following Python
    file, as shown below:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 运行测试示例非常简单。只需调用以下Python文件，如下所示：
- en: '[PRE23]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Also, for convenience, we wrap it under a `poe` command:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了方便，我们将其封装在一个`poe`命令下：
- en: '[PRE24]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now, we must understand how we implement the business microservice using FastAPI.
    This microservice will send HTTP requests to the LLM microservice defined above
    and call the RAG retrieval module implemented in *Chapter 9*.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须了解我们如何使用FastAPI实现业务微服务。这个微服务将向上面定义的LLM微服务发送HTTP请求，并调用在*第9章*中实现的RAG检索模块。
- en: Building the business microservice using FastAPI
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用FastAPI构建业务微服务
- en: 'To implement a simple FastAPI application that proves our deployment strategy,
    we first have to define a FastAPI instance as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现一个简单的FastAPI应用程序来证明我们的部署策略，我们首先必须定义一个FastAPI实例，如下所示：
- en: '[PRE25]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we define the `QueryRequest` and `QueryResponse` classes using Pydantic’s
    `BaseModel`. These classes represent the request and response structure for the
    FastAPI endpoints:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用Pydantic的`BaseModel`定义`QueryRequest`和`QueryResponse`类。这些类代表FastAPI端点的请求和响应结构：
- en: '[PRE26]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now that we’ve defined our FastAPI components and have all the SageMaker elements
    in place, let’s reiterate over the `call_llm_service()` and `rag()` functions
    we’ve presented in *Chapter 9* and couldn’t run because we haven’t deployed our
    fine-tuned LLM. Thus, as a refresher, the `call_llm_service()` function wraps
    the inference logic used to call the SageMaker LLM microservice:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经定义了我们的 FastAPI 组件，并且已经将所有 SageMaker 元素就绪，那么让我们回顾一下在第9章中介绍的 `call_llm_service()`
    和 `rag()` 函数，因为我们尚未部署我们的微调 LLM 而无法运行。因此，作为一个复习，`call_llm_service()` 函数封装了用于调用
    SageMaker LLM 微服务的推理逻辑：
- en: '[PRE27]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we define the `rag()` function that implements all the RAG business logic.
    To avoid repeating ourselves, check *Chapter 9* for the complete function explanation.
    What is important to highlight is that the `rag()` function only implements the
    business steps required to do RAG, which are CPU- and I/O-bounded. For example,
    the `ContextRetriever` class makes API calls to OpenAI and Qdrant, which are network
    I/O bounded, and calls the embedding model, which runs directly on the CPU. Also,
    as the LLM inference logic is moved to a different microservice, the `call_llm_service()`
    function is only network I/O bounded. To conclude, the whole function is light
    to run, where the heavy computing is done on other services, which allows us to
    host the FastAPI server on a light and cheap machine that doesn’t need a GPU to
    run at low latencies:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了一个名为 `rag()` 的函数，该函数实现了所有的 RAG 业务逻辑。为了避免重复，请查看*第9章*以获取完整的函数说明。需要强调的是，`rag()`
    函数仅实现了执行 RAG 所需的业务步骤，这些步骤受 CPU 和 I/O 限制。例如，`ContextRetriever` 类调用 OpenAI 和 Qdrant
    的 API，这些是网络 I/O 限制的，并调用嵌入模型，该模型直接在 CPU 上运行。此外，由于 LLM 推理逻辑被移动到不同的微服务中，`call_llm_service()`
    函数仅受网络 I/O 限制。总之，整个函数运行起来很轻量，重计算在其他服务上完成，这使得我们可以在不需要 GPU 且低延迟的情况下，在轻量且便宜的机器上托管
    FastAPI 服务器：
- en: '[PRE28]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Ultimately, we define the `rag_endpoint()` function, used to expose our RAG
    logic over the internet as an HTTP endpoint. We use a Python decorator to expose
    it as a POST endpoint in the FastAPI application. This endpoint is mapped to the
    `/rag` route and expects a `QueryRequest` as input. The function processes the
    request by calling the rag function with the user’s query. If successful, it returns
    the answer wrapped in a `QueryResponse` object. If an exception occurs, it raises
    an HTTP *500* error with the exception details:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们定义了 `rag_endpoint()` 函数，用于将我们的 RAG 逻辑作为 HTTP 端点暴露到互联网上。我们在 FastAPI 应用程序中使用
    Python 装饰器将其暴露为 POST 端点。此端点映射到 `/rag` 路由，并期望输入一个 `QueryRequest`。该函数通过调用 rag 函数并使用用户的查询来处理请求。如果成功，它将答案封装在
    `QueryResponse` 对象中返回。如果发生异常，它将抛出一个带有异常详细信息的 HTTP *500* 错误：
- en: '[PRE29]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This FastAPI application demonstrates how to effectively integrate an LLM hosted
    on AWS SageMaker into a web service, utilizing RAG to enhance the relevance of
    the model’s responses. The code’s modular design, leveraging custom classes like
    `ContextRetriever`, `InferenceExecutor`, and `LLMInferenceSagemakerEndpoint`,
    allows for easy customization and scalability, making it a powerful tool for deploying
    ML models in production environments.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 FastAPI 应用程序展示了如何有效地将托管在 AWS SageMaker 上的 LLM 集成到 Web 服务中，利用 RAG 来增强模型响应的相关性。代码的模块化设计，利用自定义类如
    `ContextRetriever`、`InferenceExecutor` 和 `LLMInferenceSagemakerEndpoint`，使得它易于定制和扩展，成为在生产环境中部署
    ML 模型的一个强大工具。
- en: 'We will leverage the `uvicorn` web server, the go-to method for FastAPI applications,
    to start the server. To do so, you have to run the following:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用 `uvicorn` Web 服务器，这是 FastAPI 应用程序的常用方法，来启动服务器。为此，您必须运行以下命令：
- en: '[PRE30]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Also, you can run the following `poe` command to achieve the same:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以使用以下 `poe` 命令达到相同的效果：
- en: '[PRE31]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To call the `/rag` endpoint, we can leverage the `curl` CLI command to make
    a POST HTTP request to our FastAPI server, as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 要调用 `/rag` 端点，我们可以利用 `curl` CLI 命令向我们的 FastAPI 服务器发送 POST HTTP 请求，如下所示：
- en: '[PRE32]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As usual, we provided an example using a `poe` command that contains an actual
    user query:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们提供了一个使用 `poe` 命令的示例，其中包含一个实际的用户查询：
- en: '[PRE33]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This FastAPI server runs only locally. The next step would be to deploy it to
    AWS **Elastic Kubernetes Service** (**EKS**), a self-hosted version of Kubernetes
    by AWS. Another option would be to deploy it to AWS **Elastic Container Service**
    (**ECS**), which is similar to AWS EKS but doesn’t use Kubernetes under the hood
    but AWS’s implementation. Unfortunately, this is not specific to LLMs or LLMOps.
    Hence, we won’t go through these steps in this book. But to get an idea of what
    you must do, you must create an AWS EKS/ECS cluster from the dashboard or leverage
    an **infrastructure-as-code** (**IaC**) tool such as Terraform. After that, you
    will have to Dockerize the FastAPI code presented above. Ultimately, you would
    have to push the Docker image to AWS ECR and create an ECS/EKR deployment using
    the Docker image hosted on ECR. If this sounds like a lot, the good news is that
    we will walk you through a similar example in *Chapter 11*, where we will deploy
    the ZenML pipelines to AWS.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 此 FastAPI 服务器仅在本地运行。下一步将是将其部署到 AWS **弹性 Kubernetes 服务**（**EKS**），这是 AWS 的自托管
    Kubernetes 版本。另一个选择是将它部署到 AWS **弹性容器服务**（**ECS**），它与 AWS EKS 类似，但底层不使用 Kubernetes，而是
    AWS 的实现。不幸的是，这并不特定于 LLM 或 LLMOps。因此，我们不会在本书中介绍这些步骤。但为了让你了解你需要做什么，你必须从仪表板创建一个 AWS
    EKS/ECS 集群，或者利用像 Terraform 这样的 **基础设施即代码**（**IaC**）工具。之后，你将不得不将上面展示的 FastAPI 代码
    Docker 化。最终，你必须将 Docker 镜像推送到 AWS ECR，并使用托管在 ECR 上的 Docker 镜像创建 ECS/EKR 部署。如果这听起来很多，好消息是我们在第
    11 章中会带你通过一个类似的例子，我们将部署 ZenML 管道到 AWS。
- en: 'Once you’re done testing your inference pipeline deployment, deleting all your
    AWS SageMaker resources used to deploy the LLM is essential. As almost all AWS
    resources use a pay-as-you-go strategy, using SageMaker for a few hours wouldn’t
    break your wallet, but if you forget and leave it open, in a few days, the costs
    can grow exponentially. Thus, a good rule of thumb is to always delete everything
    after you’re done testing your SageMaker infrastructure (or any AWS resource).
    Luckily, we have provided a script that deletes all the AWS SageMaker resources
    for you:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成测试你的推理管道部署，删除用于部署 LLM 的所有 AWS SageMaker 资源是至关重要的。由于几乎所有 AWS 资源都使用按使用付费的策略，使用
    SageMaker 几小时不会让你的钱包破费，但如果你忘记并让它保持开启状态，几天后，成本可能会呈指数增长。因此，一个好的经验法则是始终在测试完你的 SageMaker
    基础设施（或任何 AWS 资源）后删除所有内容。幸运的是，我们提供了一个脚本，可以为你删除所有 AWS SageMaker 资源：
- en: '[PRE34]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: To ensure everything was correctly deleted, go to your SageMaker dashboard and
    check it yourself.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保一切都被正确删除，请访问您的 SageMaker 仪表板并自行检查。
- en: Autoscaling capabilities to handle spikes in usage
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动扩展功能以处理使用高峰
- en: So far, the SageMaker LLM microservice has used a static number of replicas
    to serve our users, which means that all the time, regardless of the traffic,
    it has the same number of instances up and running. As we highlighted throughout
    this book, machines with GPUs are expensive. Thus, we lose a lot of money during
    downtime when most replicas are idle. Also, if our application has sudden spikes
    in traffic, the application will perform poorly as the server cannot handle the
    number of requests. This is a massive problem for the user experience of our application,
    as in those spikes, we bring in the majority of new users. Thus, if they have
    a terrible impression of our product, we significantly reduce their chance of
    returning to our platform.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，SageMaker LLM 微服务一直使用静态数量的副本为我们用户提供服务，这意味着无论流量如何，它始终有相同数量的实例在运行。正如我们在整本书中强调的那样，带
    GPU 的机器很昂贵。因此，在大多数副本空闲时，我们会在停机期间损失大量资金。此外，如果我们的应用程序出现流量高峰，服务器将无法处理请求的数量，应用程序的表现将很差。这对我们应用程序的用户体验是一个巨大的问题，因为在那些高峰期，我们带来了大部分新用户。因此，如果他们对我们的产品有糟糕的印象，我们将大大降低他们返回我们平台的机会。
- en: 'Previously, we configured our multi-endpoint service using the `ResourceRequirements`
    class from SageMaker. For example, let’s assume we requested four copies (replicas)
    with the following compute requirements:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们使用 SageMaker 的 `ResourceRequirements` 类来配置我们的多端点服务。例如，假设我们请求了以下计算需求的四个副本：
- en: '[PRE35]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Using this configuration, we always have four replicas serving the clients,
    regardless of idle time or spikes in traffic. The solution is to implement an
    autoscaling strategy that scales the number of replicas up and down dynamically
    based on various metrics, such as the number of requests.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此配置，无论空闲时间或流量高峰，我们始终有四个副本在为客户端提供服务。解决方案是实现一个自动扩展策略，根据各种指标（如请求数量）动态地调整副本的数量。
- en: For example, *Figure 10.8* shows a standard architecture where the SageMaker
    Inference endpoints scale in and out based on the number of requests. When there
    is no traffic, we can have one online replica so the server remains responsive
    to new user requests or even scales down to zero if the latency is not super critical.
    Then, let’s assume that when we have around 10 requests per second, we have to
    keep two replicas online, and when the number of requests spikes to 100 per second,
    the autoscaling service should spin up to 20 replicas to keep up with the demand.
    Note that these are fictional numbers that should be adapted to your specific
    use case.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*图 10.8* 展示了一个标准架构，其中 SageMaker Inference 端点根据请求数量进行扩展和缩减。当没有流量时，我们可以有一个在线副本，以便服务器能够响应用户的新请求，或者在延迟不是特别关键的情况下缩减到零。然后，假设当我们每秒有大约
    10 个请求时，我们必须保持两个副本在线，而当请求的数量激增到每秒 100 个时，自动扩展服务应该启动 20 个副本以满足需求。请注意，这些是虚构的数字，应该根据您的具体用例进行调整。
- en: '![](img/B31105_10_08.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_10_08.png)'
- en: 'Figure 10.8: Autoscaling possible use cases'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8：自动扩展的可能用例
- en: Without going into the little details of cloud networking, when working with
    multi-replica systems, between the client and the replicas sits an **Application
    Load Balancer** (**ALB**) or another type of load balancer.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入云网络的小细节，当与多副本系统一起工作时，客户端和副本之间有一个 **应用程序负载均衡器**（**ALB**）或另一种类型的负载均衡器。
- en: All the requests first go to the ALB, which knows to route them to a replica.
    The ALB can adopt various routing strategies, where the simplest one is called
    round robin, which sequentially sends a request to each replica. For example,
    the first request is routed to replica one, the second to replica two, and so
    on. Taking this approach, regardless of how many replicas you have online, the
    endpoint that the client calls is always represented by the load balancer that
    acts as an entry point into your cluster. Thus, adding or removing new replicas
    doesn’t affect the server and client communication protocol.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 所有请求首先发送到 ALB，它知道将它们路由到副本。ALB 可以采用各种路由策略，其中最简单的一种称为轮询，它依次将请求发送到每个副本。例如，第一个请求被路由到副本一，第二个到副本二，依此类推。采用这种方法，无论您有多少在线副本，客户端调用的端点始终由充当集群入口点的负载均衡器表示。因此，添加或删除新副本不会影响服务器和客户端通信协议。
- en: 'Let’s quickly learn how to implement an autoscaling strategy for our AWS SageMaker
    Inference endpoint. SageMaker provides a feature called **Application Auto Scaling**
    that allows you to scale resources dynamically based on pre-defined policies.
    Two foundational steps are involved in effectively leveraging this functionality:
    registering a scalable target and creating a scalable policy.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速学习如何为我们的 AWS SageMaker Inference 端点实现自动扩展策略。SageMaker 提供了一个名为 **应用程序自动扩展**
    的功能，允许您根据预定义的策略动态扩展资源。有效地利用此功能涉及两个基础步骤：注册可扩展目标和创建可扩展策略。
- en: Registering a scalable target
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注册可扩展目标
- en: The first step in enabling autoscaling for your resources is to register a scalable
    target with the **Application Auto Scaling** feature AWS provides. Think of this
    as informing AWS about the specific resource you intend to scale, as well as setting
    the boundaries within which the scaling should occur. However, this step does
    not dictate how or when the scaling should happen.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 启用资源自动扩展的第一步是使用 AWS 提供的 **应用程序自动扩展** 功能注册一个可扩展的目标。想象一下，这是在通知 AWS 关于您打算扩展的具体资源，以及设置扩展应发生的边界。然而，这一步并不规定扩展应该如何或何时发生。
- en: 'For instance, when working with SageMaker Inference components, you’ll define
    the following:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当与 SageMaker Inference 组件一起工作时，您将定义以下内容：
- en: '**Resource ID**: This serves as a unique identifier for the resource you want
    to scale, typically including the name of the SageMaker Inference component.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源 ID**：这作为您想要扩展的资源的一个唯一标识符，通常包括 SageMaker Inference 组件的名称。'
- en: '**Service namespace**: This identifies the AWS service the resource belongs
    to, which, in this case, is **SageMaker**.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务命名空间**：这标识了资源所属的 AWS 服务，在本例中是 **SageMaker**。'
- en: '**Scalable dimension**: This specifies the resources to be scaled, such as
    the desired number of copies.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展维度**：这指定了要扩展的资源，例如所需的副本数量。'
- en: '**MinCapacity and MaxCapacity**: These parameters define the boundaries of
    the autoscaling strategies, such as minimum and maximum limits of the number of
    replicas.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MinCapacity 和 MaxCapacity**：这些参数定义了自动扩展策略的边界，例如副本数量的最小和最大限制。'
- en: By registering a scalable target, you prepare your SageMaker Inference component
    for future scaling actions without determining when or how these actions should
    occur.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 通过注册可缩放目标，你为 SageMaker 推理组件的未来缩放操作做好准备，而无需确定这些操作何时或如何发生。
- en: Creating a scalable policy
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建可缩放策略
- en: Once your scalable target is registered, the next step is defining how the scaling
    should occur. This is where creating a scaling policy comes in. A scaling policy
    defines specific rules that trigger scaling events. When creating policies, you
    have to define metrics to know what to monitor and thresholds to know when to
    emit scaling events.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的可缩放目标注册成功，下一步就是定义缩放应该如何发生。这就是创建缩放策略的地方。缩放策略定义了触发缩放事件的特定规则。在创建策略时，你必须定义指标以了解要监控什么，以及阈值以了解何时触发缩放事件。
- en: 'In the context of our SageMaker Inference component, the scalable policy might
    include the following elements:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 SageMaker 推理组件的上下文中，可缩放策略可能包括以下元素：
- en: '**Policy type**: For instance, you might select **TargetTrackingScaling**,
    a policy that adjusts the resource’s capacity to maintain a specific target value
    for a chosen metric.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略类型**：例如，你可能选择 **TargetTrackingScaling**，这是一种调整资源容量以维持所选指标特定目标值的策略。'
- en: '**Target tracking configuration**: This involves selecting the metric to monitor
    (such as *SageMakerInferenceComponentInvocationsPerCopy*), setting the desired
    target value, and specifying cooldown periods that control how quickly scaling
    actions can occur after previous ones.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标跟踪配置**：这涉及到选择要监控的指标（例如 *SageMakerInferenceComponentInvocationsPerCopy*），设置期望的目标值，并指定冷却期，以控制在前一个操作之后缩放操作可以多快发生。'
- en: The scaling policy defines the rules for your scaling-in and scaling-out strategy.
    It constantly monitors the specified metric, and depending on whether the metric
    exceeds or falls below the target value, it triggers actions to scale the number
    of inference component copies up or down, always within the limits defined by
    the registered scalable target.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放策略定义了你的缩放入和缩放出策略的规则。它持续监控指定的指标，并根据指标是否超过或低于目标值，触发操作以调整推理组件副本的数量上下，始终在注册的可缩放目标定义的范围内。
- en: Let’s explain in more depth how the **TargetTrackingScaling** policy works.
    Imagine you have a metric that represents the ideal average utilization or throughput
    level for your application. With target tracking, you select this metric and set
    a target value that reflects the optimal state for your application. Once defined,
    **Application Auto Scaling** creates and manages the necessary CloudWatch alarms
    to monitor this metric. When deviations occur, scaling actions are triggered,
    similar to how a thermostat adjusts to maintain a consistent room temperature.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地解释一下 **TargetTrackingScaling** 缩放策略是如何工作的。想象一下，你有一个代表应用程序理想平均利用率或吞吐量水平的指标。使用目标跟踪，你选择这个指标并设置一个反映应用程序最佳状态的目标值。一旦定义，**应用程序自动缩放**
    将创建并管理必要的 CloudWatch 警报来监控这个指标。当出现偏差时，会触发缩放操作，类似于恒温器调整以保持室内温度一致。
- en: For instance, consider an application running on SageMaker. Let’s assume we
    set a target of keeping GPU utilization around 70 percent. This target allows
    you to maintain enough headroom to manage sudden traffic spikes while preventing
    the unnecessary cost of idle resources. When GPU usage exceeds the target, the
    system scales out, adding resources to manage the increased load. Conversely,
    when GPU usage drops below the target, the system scales in, reducing capacity
    to minimize costs during quieter periods.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个运行在 SageMaker 上的应用程序。假设我们设定了一个目标，即保持 GPU 利用率在 70% 左右。这个目标允许你在管理突发流量峰值的同时保持足够的余量，同时防止闲置资源的无必要成本。当
    GPU 使用率超过目标时，系统会进行扩展，增加资源以管理增加的负载。相反，当 GPU 使用率低于目标时，系统会进行缩减，减少容量以在较安静的时期最小化成本。
- en: One significant advantage of setting up target tracking policies using Application
    Auto Scaling is that they simplify the scaling process. You no longer need to
    configure CloudWatch alarms and define scaling adjustments manually.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 使用应用程序自动缩放设置目标跟踪策略的一个显著优势是它们简化了缩放过程。你不再需要配置 CloudWatch 警报和手动定义缩放调整。
- en: Minimum and maximum scaling limits
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最小和最大缩放限制
- en: When setting up autoscaling for your SageMaker Inference endpoints, it’s crucial
    to establish your maximum and minimum scaling limits before creating your scaling
    policy. The minimum value represents the least resources your model can operate
    with. This value must be at least 1, ensuring that your model always has some
    capacity.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 当为您的 SageMaker Inference 端点设置自动扩展时，在创建扩展策略之前，建立最大和最小扩展限制至关重要。最小值代表您的模型可以运行的最少资源。此值必须至少为
    1，确保您的模型始终有一定的容量。
- en: Next, configure the maximum value, which defines the upper limit of resources
    your model can scale up to. While the maximum must be equal to or greater than
    the minimum value, it doesn’t impose any upper limit. Thus, you can scale up as
    much as your application needs within the boundaries of what AWS can provide.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，配置最大值，它定义了模型可以扩展到的资源上限。虽然最大值必须等于或大于最小值，但它不设定任何上限。因此，您可以在 AWS 提供的范围内根据应用程序的需求进行扩展。
- en: Cooldown period
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 冷却期
- en: Another important aspect of a scaling policy is the cooldown period, during
    which it’s crucial to maintain a balance between responsiveness and stability.
    This cooldown period acts as a safeguard, ensuring that your system doesn’t overreact
    during scaling events—whether it’s reducing capacity (scaling in) or increasing
    it (scaling out). By introducing a calculated pause, the cooldown period prevents
    rapid fluctuations in the number of instances. Specifically, it delays the removal
    of instances during scale-in requests and restricts the creation of new replicas
    during scale-out requests. This strategy helps maintain a stable and efficient
    environment for LLM service.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展策略的另一个重要方面是冷却期，在此期间，保持响应性和稳定性之间的平衡至关重要。这个冷却期充当一个保护措施，确保在扩展事件（无论是减少容量（扩展入）还是增加容量（扩展出））期间，您的系统不会过度反应。通过引入计算暂停，冷却期防止实例数量的快速波动。具体来说，它延迟了在扩展入请求中移除实例，并限制了在扩展出请求中创建新副本。这种策略有助于为
    LLM 服务维护一个稳定和高效的环境。
- en: These practical basics are used in autoscaling most web servers, including online
    real-time ML servers. Once you understand how to configure scaling policies for
    SageMaker, you can immediately apply the strategies you’ve learned to other popular
    deployment tools like Kubernetes or AWS ECS.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实用的基础知识被广泛应用于大多数自动扩展的 Web 服务器，包括在线实时 ML 服务器。一旦您了解了如何为 SageMaker 配置扩展策略，您就可以立即将您学到的策略应用到其他流行的部署工具，如
    Kubernetes 或 AWS ECS。
- en: 'For a step-by-step guideline on how to configure autoscaling for the AWS SagaMaker
    endpoint implemented in this chapter, you can follow this official tutorial from
    AWS: [https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 关于如何配置本章中实现的 AWS SageMaker 端点的自动扩展的逐步指南，您可以遵循 AWS 的官方教程：[https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling-prerequisites.html)。
- en: Autoscaling is a critical component in any cloud architecture, but there are
    some pitfalls you should be aware of. The first and most dangerous one is over-scaling,
    which directly impacts the costs of your infrastructure. If your scaling policy
    or cooldown period is too sensitive, you may be uselessly spinning up new machines
    that will remain idle or with the resources underused. The second reason is on
    the other side of the spectrum, where your system doesn’t scale enough, resulting
    in a bad user experience for the end user.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 自动扩展是任何云架构中的关键组件，但您应该注意一些陷阱。第一个也是最危险的是过度扩展，这直接影响了您的基础设施成本。如果您的扩展策略或冷却时间太敏感，您可能会无用地启动新的机器，这些机器将保持空闲或资源利用率不足。另一个原因是相反的一端，您的系统扩展不足，导致最终用户体验不佳。
- en: That’s why a good practice is to understand the requirements of your system.
    Based on them, you should tweak and experiment with the autoscaling parameters
    in a dev or test environment until you find the sweet spot (similar to hyperparameter
    tuning when training models). Let’s suppose, for instance, that you expect your
    system to support an average of 100 users per minute and scale up to 10,000 users
    per minute in case of an outlier event such as a holiday. Using this spec, you
    can stress test your system and monitor your resources to find the best trade-off
    between costs, latency, and throughput that supports standard and outlier use
    cases.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个好的做法是了解您系统的需求。基于这些需求，您应该在开发或测试环境中调整和实验自动扩展参数，直到找到最佳平衡点（类似于训练模型时的超参数调整）。例如，假设您预计您的系统每分钟支持平均100个用户，在异常事件（如假日）的情况下，每分钟扩展到10,000个用户。使用这个规范，您可以压力测试您的系统，并监控您的资源，以找到支持标准用例和异常用例的最佳成本、延迟和吞吐量之间的平衡。
- en: Summary
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we learned what design decisions to make before serving an
    ML model, whether an LLM or not, by walking you through the three fundamental
    deployment types for ML models: online real-time inference, asynchronous inference,
    and offline batch transform. Then, we considered whether building our ML-serving
    service as a monolith application made sense or splitting it into two microservices,
    such as an LLM microservice and a business microservice. To do this, we weighed
    the pros and cons of a monolithic versus microservices architecture in model-serving.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过向您介绍ML模型的三个基本部署类型：在线实时推理、异步推理和离线批量转换，学习了在部署ML模型（无论是LLM还是其他类型）之前应做出的设计决策。然后，我们考虑了将我们的ML服务作为单体应用程序构建是否合理，或者将其拆分为两个微服务，例如LLM微服务和业务微服务。为此，我们权衡了在模型服务中单体架构与微服务架构的优缺点。
- en: Next, we walked you through deploying our fine-tuned LLM Twin to an AWS SageMaker
    Inference endpoint. We also saw how to implement the business microservice using
    FastAPI, which consists of all the RAG steps based on the retrieval module implemented
    in *Chapter 9* and the LLM microservice deployed on AWS SageMaker. Ultimately,
    we explored why we have to implement an autoscaling strategy. We also reviewed
    a popular autoscaling strategy that scales in and out based on a given set of
    metrics and saw how to implement it in AWS SageMaker.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们向您展示了如何将我们的微调后的LLM Twin部署到AWS SageMaker推理端点。我们还看到了如何使用FastAPI实现业务微服务，该服务包含基于第9章中实现的检索模块的所有RAG步骤，以及部署在AWS
    SageMaker上的LLM微服务。最终，我们探讨了为什么我们必须实现自动扩展策略。我们还回顾了一种流行的基于给定一组指标进行扩展和缩小的自动扩展策略，并展示了如何在AWS
    SageMaker中实现它。
- en: In the next chapter, we will learn about the fundamentals of MLOps and LLMOps
    and then explore how to deploy the ZenML pipelines to AWS and implement a **continuous
    training**, **continuous integration**, and **continuous delivery** (**CT**/**CI**/**CD**)
    and monitoring pipeline.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习MLOps和LLMOps的基础知识，然后探索如何将ZenML管道部署到AWS，并实现**持续训练**、**持续集成**和**持续交付**（**CT**/**CI**/**CD**）以及监控管道。
- en: References
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'AWS Developers. (2023, September 22). *Machine Learning in 15: Amazon SageMaker
    High-Performance Inference at Low Cost* [Video]. YouTube. [https://www.youtube.com/watch?v=FRbcb7CtIOw](https://www.youtube.com/watch?v=FRbcb7CtIOw
    )'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS开发者。（2023年9月22日）。*15分钟机器学习：Amazon SageMaker低成本高性能推理* [视频]。YouTube。[https://www.youtube.com/watch?v=FRbcb7CtIOw](https://www.youtube.com/watch?v=FRbcb7CtIOw)
- en: 'bitsandbytes-foundation. (n.d.). GitHub—bitsandbytes-foundation/bitsandbytes:
    Accessible large language models via k-bit quantization for PyTorch. GitHub. [https://github.com/bitsandbytes-foundation/bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes
    )'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: bitsandbytes-foundation.（n.d.）。GitHub—bitsandbytes-foundation/bitsandbytes：通过PyTorch的k位量化访问可访问的大语言模型。GitHub。[https://github.com/bitsandbytes-foundation/bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes)
- en: '*Difference between IAM role and IAM user in AWS*. (n.d.). Stack Overflow.
    [https://stackoverflow.com/questions/46199680/difference-between-iam-role-and-iam-user-in-aws](https://stackoverflow.com/questions/46199680/difference-between-iam-role-and-iam-user-in-aws
    )'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AWS中IAM角色与IAM用户的区别*。（n.d.）。Stack Overflow。[https://stackoverflow.com/questions/46199680/difference-between-iam-role-and-iam-user-in-aws](https://stackoverflow.com/questions/46199680/difference-between-iam-role-and-iam-user-in-aws)'
- en: 'Huggingface. (n.d.-a). GitHub—huggingface/safetensors: Simple, safe way to
    store and distribute tensors. GitHub. [https://github.com/huggingface/safetensors](https://github.com/huggingface/safetensors
    )'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huggingface. (n.d.-a). GitHub—huggingface/safetensors: 简单、安全地存储和分发张量。GitHub.
    [https://github.com/huggingface/safetensors](https://github.com/huggingface/safetensors)'
- en: 'Huggingface. (n.d.-b). GitHub—huggingface/text-generation-inference: Large
    Language Model Text Generation Inference. GitHub. [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference
    )'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huggingface. (n.d.-b). GitHub—huggingface/text-generation-inference: 大型语言模型文本生成推理。GitHub.
    [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference)'
- en: Huyen, C. (n.d.). *Designing machine learning systems*. O’Reilly Online Learning.
    [https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/
    )
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huyen, C. (n.d.). *设计机器学习系统*. O’Reilly Online Learning. [https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/](https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/)
- en: Iusztin, P. (2024, August 20). Architect LLM & RAG inference pipelines | Decoding
    ML. *Medium*. [https://medium.com/decodingml/architect-scalable-and-cost-effective-llm-rag-inference-pipelines-73b94ef82a99](https://medium.com/decodingml/architect-scalable-and-cost-effective-llm-rag-inference-pipelines-73b94ef82a99
    )
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iusztin, P. (2024, August 20). 架构 LLM & RAG 推理管道 | 解码机器学习. *Medium*. [https://medium.com/decodingml/architect-scalable-and-cost-effective-llm-rag-inference-pipelines-73b94ef82a99](https://medium.com/decodingml/architect-scalable-and-cost-effective-llm-rag-inference-pipelines-73b94ef82a99)
- en: Lakshmanan, V., Robinson, S., and Munn, M. (n.d.). *Machine Learning design
    patterns*. O’Reilly Online Learning. [https://www.oreilly.com/library/view/machine-learning-design/9781098115777/](https://www.oreilly.com/library/view/machine-learning-design/9781098115777/
    )
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakshmanan, V., Robinson, S., and Munn, M. (n.d.). *机器学习设计模式*. O’Reilly Online
    Learning. [https://www.oreilly.com/library/view/machine-learning-design/9781098115777/](https://www.oreilly.com/library/view/machine-learning-design/9781098115777/)
- en: Mendoza, A. (2024, August 21). *Best tools for ML model Serving*. neptune.ai.
    [https://neptune.ai/blog/ml-model-serving-best-tools](https://neptune.ai/blog/ml-model-serving-best-tools  )
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mendoza, A. (2024, August 21). *机器学习模型部署的最佳工具*. neptune.ai. [https://neptune.ai/blog/ml-model-serving-best-tools](https://neptune.ai/blog/ml-model-serving-best-tools)
- en: Join our book’s Discord space
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llmeng](https://packt.link/llmeng)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llmeng](https://packt.link/llmeng)'
- en: '![](img/QR_Code79969828252392890.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code79969828252392890.png)'
