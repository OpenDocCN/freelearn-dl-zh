- en: Word Representation Using word2vec
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用word2vec进行词表示
- en: Our *Python Deep Learning Projects* team is doing good work, and our (hypothetical)
    business use case has expanded! In the last project, we were asked to accurately
    classify handwritten digits to generate a phone number so that an *available table
    notification* text could be sent out to patrons of a restaurant chain. What we
    learned after the project was that the text that the restaurant sent out had a
    message that was friendly and well received. The restaurant was actually getting
    texts back!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的*Python深度学习项目*团队做得很好，我们（假设的）业务用例已经扩展！在上一个项目中，我们被要求准确地对手写数字进行分类，以生成电话号码，以便向餐饮连锁的顾客发送*可用桌位通知*短信。我们从项目中学到的是，餐厅发送的短信内容既友好又容易被接受。餐厅实际上收到了顾客的短信回复！
- en: 'The notification text was: *We''re excited that you''re here and your table
    is ready! See the greeter, and we''ll seat you now.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通知文本是：*我们很高兴你来了，你的桌子已经准备好了！请见迎宾员，我们现在为您安排座位。*
- en: Response texts were varied and usually short, but the responses were noticed
    by the greeter and the restaurant management, who started thinking that maybe
    they could use this simple system to get feedback on the dining experience. This
    feedback would provide useful business intelligence on how the food tasted, how
    the service was delivered, and the overall quality of the experience.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 回复文本内容各异且通常较短，但迎宾员和餐厅管理层注意到了这些回复，并开始考虑也许可以利用这个简单的系统来获取就餐体验的反馈。这些反馈可以提供有价值的商业智能，了解食物的味道、服务的质量以及整体就餐体验的好坏。
- en: '**Define success**: The goal of this project is to build a computational linguistic
    model, using word2vec, that can take a text response (as identified in our hypothetical
    use case for this chapter) and output a sentiment classification that is meaningful.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义成功**：本项目的目标是构建一个计算语言学模型，使用word2vec，它可以接受文本回复（如我们本章假设的用例所示），并输出一个有意义的情感分类。'
- en: In this chapter, we introduce the foundational knowledge of **deep learning**
    (**DL**) for computational linguistics.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍**深度学习**（**DL**）在计算语言学中的基础知识。
- en: We present the role of the dense vector representation of words in various computational
    linguistic tasks and how to construct them from an unlabeled monolingual corpus.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了词的稠密向量表示在各种计算语言学任务中的作用，以及如何从无标签的单语语料库中构建这些表示。
- en: We'll then present the role of language models in various computational linguistic
    tasks, such as text classification, and how to construct them from an unlabeled
    monolingual corpus using **convolutional neural networks** (**CNNs**). We'll also
    explore CNN architecture for language modeling.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将展示语言模型在各种计算语言学任务中的作用，如文本分类，以及如何使用**卷积神经网络**（**CNNs**）从无标签的单语语料库中构建它们。我们还将探讨用于语言建模的CNN架构。
- en: While working with machine learning/DL, the structure of data is very important. Unfortunately,
    raw data is often very unclean and unstructured, especially in the practice of
    **natural language processing** (**NLP**). When working with textualdata, we cannot
    feed strings as input in most DL algorithms; hence, **word embedding** methods
    come to the rescue. Word embedding is used to transform the textual data into
    dense vector (tensors) form, which we can feed to the learning algorithm.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行机器学习/深度学习（DL）时，数据的结构非常重要。不幸的是，原始数据通常非常脏且无结构，特别是在**自然语言处理**（**NLP**）的实践中。处理文本数据时，我们不能将字符串直接输入大多数深度学习算法；因此，**词嵌入**方法应运而生。词嵌入用于将文本数据转换为稠密的向量（张量）形式，我们可以将其输入到学习算法中。
- en: There are several ways in which we can perform word embeddings, such as one-hot
    encoding, GloVe, word2vec, and many more, and each of them have their own pros
    and cons. Our current favorite is word2vec because it has been proven to be the
    most efficient approach when it comes to learning high quality features.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以进行词嵌入，如独热编码、GloVe、word2vec等，每种方法都有其优缺点。我们目前最喜欢的是word2vec，因为它已被证明是学习高质量特征时最有效的方法。
- en: If you have ever worked on a use case where the input data is in text form,
    then you know that it's a really messy affair because you have to teach a computer
    about the irregularities of human language. Language has lots of ambiguities,
    and you have to teach sort of like hierarchical and the sparse nature of grammar.
    So these are the kinds of problems that word vectors solve by removing the ambiguities
    and making all different kinds of concepts similar.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经处理过输入数据为文本的案例，那么你就知道这是一件非常杂乱的事，因为你需要教计算机理解人类语言的种种不规则性。语言有很多歧义，你需要像处理层级结构一样教会计算机，语法是稀疏的。这些就是词向量解决的问题，通过消除歧义并使不同种类的概念变得相似。
- en: In this chapter, we will learn how to build word2vec models and analyze what
    characteristics we can learn about the provided corpus. Also, we will learn how
    to build a language model that utilizes a CNN with trained word vectors.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何构建 word2vec 模型，并分析我们可以从提供的语料库中学到哪些特征。同时，我们将学习如何构建一个利用卷积神经网络（CNN）和训练好的词向量的语言模型。
- en: Learning word vectors
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习词向量
- en: 'To implement a fully functional word embedding model, we will perform the following steps:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现一个完全功能的词嵌入模型，我们将执行以下步骤：
- en: Loading all the dependencies
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载所有依赖项
- en: Preparing the text corpus
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备文本语料库
- en: Defining the model
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型
- en: Training the model
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Analyzing the model
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析模型
- en: Plotting the word cluster using the **t-Distributed Stochastic Neighbor Embedding**
    (**t-SNE**) algorithm
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 **t-分布随机邻域嵌入**（**t-SNE**）算法绘制词汇集群
- en: Plotting the model on TensorBoard
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 TensorBoard 上绘制模型
- en: Let's make some world-class word embedding models!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起制作世界级的词嵌入模型！
- en: The code for this section is available at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter03/create_word2vec.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter03/create_word2vec.ipynb).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的代码可以在 [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter03/create_word2vec.ipynb](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/blob/master/Chapter03/create_word2vec.ipynb)
    找到。
- en: Loading all the dependencies
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载所有依赖项
- en: In this chapter, we will be using the `gensim` module ([https://github.com/RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim))
    to train our `word2vec` model. Gensim provides large-scale multi-core processing
    support to many popular algorithms, including **Latent Dirichlet Allocation**
    (**LDA**), **Hierarchical Dirichlet** **Process** (**HDP**), and word2vec. There
    are other approaches that we could take, such as the use of TensorFlow ([https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_optimized.py](https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_optimized.py))
    to define our own computation graph and build the model—this is something that
    we will look into later on.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 `gensim` 模块（[https://github.com/RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim)）来训练我们的
    `word2vec` 模型。Gensim 为许多流行算法提供了大规模多核处理支持，包括 **潜在狄利克雷分配**（**LDA**）、**层次狄利克雷过程**（**HDP**）和
    word2vec。我们还可以采取其他方法，例如使用 TensorFlow ([https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_optimized.py](https://github.com/tensorflow/models/blob/master/tutorials/embedding/word2vec_optimized.py))
    定义我们自己的计算图并构建模型——这将是我们稍后要探讨的内容。
- en: Know the code! Python dependencies are quite manageable. You can learn more
    at [https://packaging.python.org/tutorials/managing-dependencies/](https://packaging.python.org/tutorials/managing-dependencies/).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 了解代码！Python 的依赖项相对容易管理。你可以在 [https://packaging.python.org/tutorials/managing-dependencies/](https://packaging.python.org/tutorials/managing-dependencies/)
    了解更多内容。
- en: This tutorial walks you through the use of Pipenv to manage dependencies for
    an application. It will show you how to install and use the necessary tools and
    make strong recommendations on best practices. Keep in mind that Python is used
    for a great many different purposes, and precisely how you want to manage your
    dependencies may change based on how you decide to publish your software. The
    guidance presented here is most directly applicable to the development and deployment
    of network services (including web applications), but is also very well suited
    to managing development and testing environments for any kind of project.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将引导你通过使用 Pipenv 来管理应用程序的依赖项。它会展示如何安装和使用必要的工具，并给出关于最佳实践的强烈建议。请记住，Python 被广泛用于各种不同的目的，你管理依赖项的方式可能会根据你决定如何发布软件而发生变化。这里的指导最直接适用于网络服务（包括
    Web 应用程序）的开发和部署，但也非常适合用于任何类型项目的开发和测试环境的管理。
- en: 'We will be using the `seaborn` package to plot the word clusters, `sklearn`
    to implement the t-SNE algorithm, and `tensorflow` for building TensorBoard plots:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`seaborn`包来绘制词汇集群，使用`sklearn`来实现t-SNE算法，并使用`tensorflow`来构建TensorBoard图表：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Preparing the text corpus
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备文本语料库
- en: 'We will use the previously trained **Natural Language Toolkit** (**NLTK**)
    tokenizer ([http://www.nltk.org/index.html](http://www.nltk.org/index.html)) and
    stop words for the English language to clean our corpus and extract relevant unique
    words from the corpus. We will also create a small module to clean the provided
    collection, with a list of unprocessed sentences, to output the list of words:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前训练过的**自然语言工具包**（**NLTK**）分词器（[http://www.nltk.org/index.html](http://www.nltk.org/index.html)）和英语的停用词，来清理我们的语料库，并从中提取相关的唯一单词。我们还将创建一个小模块来清理提供的集合，该集合包含未处理的句子列表，最终输出单词列表：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since we haven't yet captured the data from the text responses in our hypothetical
    business use case, let's collect a good quality dataset that's available on the
    web. Demonstrating our understanding and skills with this corpus will prepare
    us for the hypothetical business use case data. You can also use your own dataset,
    but it's important to have a huge amount of words so that the `word2vec` model
    can generalize well. So, we will load our data from the Project Gutenberg website,
    available at [Gutenberg.org](http://Gutenberg.org).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们还没有从假设的业务案例中的文本响应捕获数据，先让我们收集一个高质量的数据集，这些数据集可以从网络上获得。通过使用这个语料库来展示我们的理解和技能，将为我们准备处理假设的业务用例数据。你也可以使用自己的数据集，但重要的是拥有大量的单词，这样`word2vec`模型才能很好地进行泛化。因此，我们将从古腾堡项目网站加载我们的数据，网址是[Gutenberg.org](http://Gutenberg.org)。
- en: 'Then we tokenize the raw corpus into the list of unique clean words, as shown
    in the following diagram:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将原始语料库分词成唯一且干净的单词列表，如下图所示：
- en: '![](img/e713b0f9-7853-4d45-bffa-b1c79dae435e.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e713b0f9-7853-4d45-bffa-b1c79dae435e.png)'
- en: This process depicts the data transformation, from raw data, to the list of
    words that will be fed into the word2vec model
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程展示了数据转化的过程，从原始数据到将输入到word2vec模型中的单词列表。
- en: 'Here we will download the text data from the URL and process them as shown
    in the preceding figure:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从URL下载文本数据并按照前面图示的方式处理它们：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Defining our word2vec model
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义我们的word2vec模型
- en: Now let's use `gensim` in our definition of the `word2vec` model. To begin,
    let's define a few hyperparameters for our model, such as the dimension, which
    means how many low-level features we want to learn. Each dimension will learn
    a unique concept of gender, objects, age, and so on.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在定义`word2vec`模型时使用`gensim`。首先，我们定义模型的一些超参数，例如维度，它决定了我们希望学习多少低级特征。每个维度将学习一个独特的概念，比如性别、物体、年龄等。
- en: '**Computational linguistics model tip #1**: Increasing the number of dimensions
    leads to better generalization... but it also adds more computational complexity.
    The right number is an empirical question for you to determine as an applied AI
    deep learning engineer!'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算语言学模型小贴士 #1**：增加维度数量可以提高模型的泛化能力……但同时也会增加计算复杂度。正确的维度数是一个经验性问题，需要你作为应用人工智能深度学习工程师来决定！'
- en: '**Computational linguistics model tip #2**: Pay attention to `context_size`*. *This
    is important because it sets the upper limit for the distance between the current
    and target word prediction within a sentence. This helps the model in learning
    the deeper relationships between a word and the other nearby words.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算语言学模型小贴士 #2**：注意`context_size`。*这很重要，因为它设定了当前单词与目标单词预测之间的最大距离限制。这个参数帮助模型学习单词与其他相邻单词之间的深层关系。*'
- en: 'Using the `gensim` instance, we will define our model, including all the hyperparameters:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`gensim`实例，我们将定义我们的模型，包括所有超参数：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Training the model
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Once we have configured the `gensim word2vec` object, we need to give the model
    some training. Be prepared, as this might take some time depending on the amount
    of data and the computation power you have. In this process, we have to define
    the number of epochswe need to run, which can vary depending on your data size.
    You can play around with these values and evaluate your `word2vec` model's performance.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们配置了`gensim word2vec`对象，我们需要给模型一些训练。请做好准备，因为根据数据量和计算能力的不同，这可能需要一些时间。在此过程中，我们需要定义训练的轮次（epoch），这可以根据数据的大小而有所不同。你可以调整这些值并评估`word2vec`模型的性能。
- en: 'Also, we will save the trained model so that we can use it later on while building
    our language models:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将保存训练好的模型，以便在构建语言模型时能够稍后使用：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Once the training process is complete, you can see a binary file stored in `/trained/sample.w2v`.
    You can share the `sample.w2v` file with others and they can use this word vectors
    in their NLP usecases and load it later into any other NLP task.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练过程完成，你可以看到一个二进制文件存储在`/trained/sample.w2v`中。你可以将`sample.w2v`文件分享给他人，他们可以在他们的NLP应用中使用这些词向量，并将其加载到任何其他NLP任务中。
- en: Analyzing the model
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析模型
- en: 'Now that we have trained our `word2vec` model, let''s explore what our model
    was able to learn. We will use `most_similar()` to explore the relations between
    various words. In the following example, you see that the model was able to learn
    that the word `earth` is related to `crust`, `globe`, and other words. It is interesting
    to see that we only provided the raw data and the model was able to learn all
    of these relations and concepts automatically! The following is the example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了`word2vec`模型，让我们来探索一下模型能够学到什么。我们将使用`most_similar()`来探索不同单词之间的关系。在下面的例子中，你会看到模型学到了`earth`这个词与`crust`、`globe`等词是相关的。很有趣的是，我们只提供了原始数据，模型就能够自动学习到这些关系和概念！以下是这个例子：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s try to find words related to `human` and see what the model has learned:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着找出与`human`相关的词，并看看模型学到了什么：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Critical thinking tip**: It''s interesting to observe that `art`, `race`,
    and `industry` are the most similar outputs. Remember that these similarities
    are based on the corpus of text that we used for training, and they should be
    thought of in that context. Generalization, and its unwanted sidekick, bias, can
    come into play when similarities from outdated or dissimilar training corpora
    are used to train a model that is applied to a new set of language data or cultural
    norms.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**批判性思维提示**：有趣的是，`art`、`race`和`industry`是最相似的输出。请记住，这些相似性是基于我们用来训练的语料库的，应该在这个上下文中加以理解。当过时或不相关的训练语料库的相似性被用于训练应用于新的语言数据或文化规范的模型时，泛化以及它的不受欢迎的伴侣偏见可能会产生影响。'
- en: 'Even when we try to derive an analogy by using two positive vectors, `earth`
    and `moon`, and a negative vector, `orbit`, the model predicts the word `sun`,
    which makes sense because there is a semantic relation between the moon orbiting
    around the earth, and the earth orbiting around the sun:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们试图通过使用两个正向向量`earth`和`moon`，以及一个负向向量`orbit`来推导类比，模型仍然预测出`sun`这个词，这是有道理的，因为月球绕地球公转，地球绕太阳公转之间存在语义关系：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: So, we learned that by using the `word2vec` model we can derive valuable information
    from raw unlabeled data. This process is crucial in terms of learning the grammar
    of a language and the semantic correlations between words.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们了解到，通过使用`word2vec`模型，我们可以从原始未标记数据中提取有价值的信息。这个过程在学习语言的语法和词语之间的语义关联方面至关重要。
- en: Later, we will learn how to use these `word2vec` features as an input for the
    classification model, which helps in boosting the model's accuracy and performance.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，我们将学习如何将这些`word2vec`特征作为分类模型的输入，这有助于提高模型的准确性和性能。
- en: Plotting the word cluster using the t-SNE algorithm
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用t-SNE算法绘制词汇聚类
- en: So, after our analysis, we know that our `word2vec` model has learned some concepts
    from the provided corpus, but how do we visualize it? Because we have created
    a 300-dimensional space to learn the features, it's practically impossible for
    us to visualize. To make it possible, we will use a dimension reduction algorithm,
    called t-SNE, which is very well known for reducing a high dimensional space into
    more humanly understandable two or three-dimensional space.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在我们的分析之后，我们知道我们的`word2vec`模型从提供的语料库中学到了一些概念，但我们如何可视化它呢？因为我们已经创建了一个300维的空间来学习特征，实际上我们不可能直接进行可视化。为了使其成为可能，我们将使用一个降维算法，叫做t-SNE，它在将高维空间降到更容易理解的二维或三维空间方面非常有名。
- en: '"t-Distributed Stochastic Neighbor Embedding (t-SNE) ([https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/))
    is a (prize-winning) technique for dimensionality reduction that is particularly
    well suited for the visualization of high-dimensional datasets. The technique
    can be implemented via Barnes-Hut approximations, allowing it to be applied on
    large real-world datasets. We applied it on data sets with up to 30 million examples."'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '"t-分布随机邻居嵌入（t-SNE）([https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/))是一种（获奖的）降维技术，特别适合于高维数据集的可视化。该技术可以通过Barnes-Hut近似实现，从而可以应用于大型真实世界数据集。我们将其应用于包含多达3000万个示例的数据集。"'
- en: – Laurens van der Maaten
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: – Laurens van der Maaten
- en: To implement this, we will use the `sklearn` package, and define `n_components=2`,
    which means we want to have 2-D space as the output. Next, we will perform the
    transformation by feeding the word vectors into the t-SNE object.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将使用`sklearn`包，并定义`n_components=2`，这意味着我们希望输出的是二维空间。接下来，我们将通过将单词向量输入到t-SNE对象中来进行变换。
- en: 'After this step, we now have a set of values for each word that we can use
    as `x` and `y` coordinates, respectively, to plot it on the 2D plane. Let''s prepare
    a `DataFrame` to store all the words and their `x` and `y` coordinates in the
    same variable, as shown in the following screenshot*,* and take data from there
    to create a scatter plot:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步之后，我们现在有一组每个单词的值，可以分别作为`x`和`y`坐标，用于在二维平面上绘制它。让我们准备一个`DataFrame`来存储所有单词及其`x`和`y`坐标在同一个变量中，如下图所示，并从中获取数据来创建散点图：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is our `DataFrame` containing words and coordinates for both `x` and `y`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的`DataFrame`，包含了单词及其`x`和`y`坐标：
- en: '![](img/f8c999a1-a465-405a-8e08-a79a7a592cbd.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8c999a1-a465-405a-8e08-a79a7a592cbd.png)'
- en: Our word list with the coordinate values obtained using t-SNE
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的单词列表及使用t-SNE获得的坐标值。
- en: 'This is what the entire cluster looks like after plotting 425,633 tokens on
    the 2D plane. Each point is positioned after learning the features and correlations
    between the nearby words, as shown:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在二维平面上绘制了425,633个标记后，整个集群的样子。每个点的位置是在学习了附近单词的特征和关联后确定的，如下所示：
- en: '![](img/1bc2606d-b75f-4012-86b4-ef636a7c1485.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1bc2606d-b75f-4012-86b4-ef636a7c1485.png)'
- en: A scatter plot of all the unique words on a 2D plane
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维平面上绘制所有独特单词的散点图
- en: Visualizing the embedding space by plotting the model on TensorBoard
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过在TensorBoard上绘制模型来可视化嵌入空间
- en: There is no benefit to visualization if you cannot make use of it, in terms
    of understanding how and what the model has learned. To gain a better intuition
    of what the model has learned, we will be using TensorBoard.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不能利用可视化来理解模型的学习过程和内容，那么可视化是没有意义的。为了更好地理解模型学到了什么，我们将使用TensorBoard。
- en: TensorBoard is a powerful tool that can be used to build various kinds of plots
    to monitor your models while in the training process, as well as building DL architectures
    and word embeddings. Let's build a TensorBoard embedding projection and make use
    of it to do various kinds of analysis.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard是一个强大的工具，可以用来构建各种类型的图表，以监控模型在训练过程中的状态，并且在构建深度学习架构和词嵌入时非常有用。让我们构建一个TensorBoard嵌入投影，并利用它进行各种分析。
- en: 'To build an embedding plot in TensorBoard, we need to perform the following
    steps:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在TensorBoard中构建嵌入图，我们需要执行以下步骤：
- en: Collect the words and the respective tensors (300-D vectors) that we learned
    in previous steps.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集我们在之前步骤中学习到的单词及其相应的张量（300维向量）。
- en: Create a variable in the graph that will hold the tensors.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图中创建一个变量，用于存储张量。
- en: Initialize the projector.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化投影器。
- en: Include an appropriately named embedding layer.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包含一个合适命名的嵌入层。
- en: Store all the words in a `.tsv` formatted metadata file. These file types are
    used by TensorBoard to load and display words.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有单词存储在一个`.tsv`格式的元数据文件中。这些文件类型由TensorBoard使用，以加载和显示单词。
- en: Link the `.tsv` metadata file to the projector object.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`.tsv`元数据文件链接到投影器对象。
- en: Define a function that will store all of the summary checkpoints.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，用于存储所有的总结检查点。
- en: 'The following is the code to complete the preceding seven steps:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完成前面七个步骤的代码：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once the TensorBoard preparation module is executed, the binaries, metadata,
    and checkpoints get stored in the disk, as shown in the following screenshot:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行了TensorBoard准备模块，二进制文件、元数据和检查点就会存储到磁盘中，如下图所示：
- en: '![](img/716fd34b-b219-42ad-aa15-6a0dfaf88a18.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/716fd34b-b219-42ad-aa15-6a0dfaf88a18.png)'
- en: The outputs created by TensorBoard
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard创建的输出
- en: 'To visualize the TensorBoard, execute the following command in the Terminal:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化TensorBoard，请在终端中执行以下命令：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, in the browser, open `http://localhost:6006/#projector`, you should see
    TensorBoard with all the data points projected in 3D space. You can zoom in, zoom
    out, look for specific words, as well as retrain the model using t-SNE, and visualize
    the cluster formation of the dataset:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在浏览器中打开`http://localhost:6006/#projector`，你应该能看到TensorBoard，其中所有数据点都投影到3D空间中。你可以放大、缩小，查找特定的词语，以及使用t-SNE重新训练模型，查看数据集的聚类形成：
- en: '![](img/8b31c192-5668-4780-b9d5-ecb78d1e7979.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b31c192-5668-4780-b9d5-ecb78d1e7979.png)'
- en: The TensorBoard embedding projection
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard嵌入投影
- en: Data visualization helps you tell your story! TensorBoard is very cool! Your
    business use case stakeholders love impressive dynamic data visualizations. They
    help with your model intuition, and with generating new hypotheses to test.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化帮助你讲述故事！TensorBoard非常酷！你的业务案例相关方喜欢令人印象深刻的动态数据可视化。它们有助于提升你对模型的直觉，并生成新的假设以进行测试。
- en: Building language models using CNN and word2vec
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN和word2vec构建语言模型
- en: Now that we have learned the core concepts of computational linguistics, and
    trained relations from the provided dataset, we can use this learning to implement
    a language model that can perform a task.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了计算语言学的核心概念，并从提供的数据集中训练了关系，我们可以利用这一学习来实现一个可以执行任务的语言模型。
- en: In this section, we will build a text classification model to perform sentiment
    analysis. For classification, we will be using a combination of CNN and a pre-trained
    `word2vec` model, which we learned about in the previous section of this chapter.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个文本分类模型，用于情感分析。对于分类，我们将使用CNN和预训练的`word2vec`模型的结合，这部分我们在本章的前一节中已经学习过。
- en: This task is the simulation of our hypothetical business use case of taking
    text responses from restaurant patrons and classifying what they text back into
    meaningful classes for the restaurant.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务是我们假设的业务案例的模拟，目的是从餐厅顾客的文本反馈中提取信息，并将他们的回复分类为餐厅有意义的类别。
- en: We have been inspired by Denny Britz's ([https://twitter.com/dennybritz](https://twitter.com/dennybritz))
    work on *Implementing a CNN for Text Classification in TensorFlow* ([http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/))
    in our own CNN and text classification build. We invite you to review the blog
    he created to gain a more complete understanding of the internal mechanisms that
    make CNNs useful for text classification.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们受到了Denny Britz的启发（[https://twitter.com/dennybritz](https://twitter.com/dennybritz)），在他关于*在TensorFlow中实现CNN进行文本分类*
    ([http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/))的工作基础上，构建了我们自己的CNN和文本分类模型。我们邀请你查看他创作的博客，以便更全面地了解CNN在文本分类中的内部机制。
- en: As an overview, this architecture starts with an input embedding step, then
    a 2D convolution utilizing max pooling with multiple filters, and a softmax activation
    layer producing the output.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这种架构从输入嵌入步骤开始，然后是使用多个滤波器的最大池化进行的2D卷积，最后是一个softmax激活层输出结果。
- en: Exploring the CNN model
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索CNN模型
- en: You might be asking yourself, how do you use CNNs to classify text when they
    are most commonly used in image processing?
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，CNN在图像处理领域最常用，如何将其用于文本分类呢？
- en: 'There are many discussions in the literature, linked at the bottom of this
    tip, which have proven that CNNs are a generic feature extraction function that
    can compute **location invariance** and **compositionality**. The location invariance
    property helps the model to capture the context of words, irrespective of their
    occurrence in the corpus. Compositionality helps to derive higher-level representations
    using lower-level features:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中有很多讨论（在本提示底部提供了链接），证明了CNN是一个通用的特征提取函数，可以计算**位置不变性**和**组合性**。位置不变性帮助模型捕捉词语的上下文，无论它们在语料库中的出现位置如何。组合性有助于利用低级特征推导出更高级的表示：
- en: Convolutional Neural Networks for Sentence Classification ([https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882))
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于句子分类的卷积神经网络 ([https://arxiv.org/abs/1408.5882](https://arxiv.org/abs/1408.5882))
- en: A CNN Based Scene Chinese Text Recognition Algorithm with Synthetic Data Engine
    ([https://arxiv.org/abs/1604.01891](https://arxiv.org/abs/1604.01891))
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于CNN的场景中文文本识别算法与合成数据引擎 ([https://arxiv.org/abs/1604.01891](https://arxiv.org/abs/1604.01891))
- en: Text-Attentional Convolutional Neural Networks for Scene Text Detection ([https://arxiv.org/pdf/1510.03283.pdf](https://arxiv.org/pdf/1510.03283.pdf))
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本注意卷积神经网络用于场景文本检测([https://arxiv.org/pdf/1510.03283.pdf](https://arxiv.org/pdf/1510.03283.pdf))
- en: So instead of sending pixel values for an image into the model, we feed one-hot
    encoded word vectors or the `word2vec` matrix, which represent a word or a character
    (for character-based models). Denny Britz's implementation has two filters each
    in three region sizes of two, three, and four. The convolution operation is performed
    by these filters as it processes over the sentence matrix to generate feature
    maps. Downsampling is performed by a max pooling operation over each activation
    map. Finally, all the outputs are concatenated and passed into the softmax classifier.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模型接收到的不是图像的像素值，而是一个热编码的词向量或`word2vec`矩阵，代表一个词或一个字符（对于基于字符的模型）。Denny Britz的实现中有两个滤波器，每个滤波器有两个、三个和四个不同的区域大小。卷积操作通过这些滤波器在句子矩阵上进行处理，生成特征图。通过对每个激活图执行最大池化操作来进行下采样。最后，所有输出都被连接并传递到softmax分类器中。
- en: 'Because we are performing sentiment analysis, there will be both a positive
    and a negative output class target. The softmax classifier will output probabilities
    for each class, as shown:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们正在执行情感分析，所以会有正面和负面两个输出类别目标。softmax分类器将输出每个类别的概率，如下所示：
- en: '![](img/7321c34b-1557-403b-9cca-d0aa076c6a3b.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7321c34b-1557-403b-9cca-d0aa076c6a3b.png)'
- en: This diagram is taken from Denny Britz's blog post describing the functioning
    of the CNN language model
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 该图来自Denny Britz的博客文章，描述了CNN语言模型的工作原理
- en: Let's look into the implementation of the model. We have modified the existing
    implementation by adding the input of the previously trained `word2vec` model
    component.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型的实现。我们通过添加先前训练的`word2vec`模型组件的输入，修改了现有的实现。
- en: The code for this project can be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter03/sentiment_analysis](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter03/sentiment_analysis).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的代码可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter03/sentiment_analysis](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter03/sentiment_analysis)找到。
- en: 'The model resides in `text_cnn.py`. We created a class, named `TextCNN`, which
    takes a few parameters as an input for the model''s configuration, also known
    as hyperparameters. The following is a list of hyperparameters:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 模型位于`text_cnn.py`中。我们创建了一个名为`TextCNN`的类，该类接受一些超参数作为模型配置的输入。以下是超参数的列表：
- en: '`sequence_length`: The fixed sentence length'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sequence_length`：固定的句子长度'
- en: '`num_classes`: The number of output classes that will be produced by the softmax
    activation (positive and negative)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_classes`：softmax激活输出的类别数（正面和负面）'
- en: '`vocab_size`: The count of unique words in our embeddings'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vocab_size`：我们词向量中唯一词的数量'
- en: '`embedding_size`: Embedding dimensionality that we created'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_size`：我们创建的嵌入维度'
- en: '`filter_sizes`: The convolutional filter will cover this many words'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_sizes`：卷积滤波器将覆盖这么多的词'
- en: '`num_filters`: Each filter size will have this many filters'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_filters`：每个滤波器大小将有这么多的滤波器'
- en: '`pre_trained`: Integrates the `word2vec` representation that has been previously
    trained'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pre_trained`：集成了先前训练过的`word2vec`表示'
- en: 'Following is the declaration of the `TextCNN()` class with the `init()` function
    initializing all the hyperparameter values:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`TextCNN()`类的声明，`init()`函数初始化了所有超参数值：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The code is divided into six main parts:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 代码被分为六个主要部分：
- en: '**Placeholders for inputs**: All the placeholders that we need to contain the
    input values for our model are defined first. In this case, inputs are the sentence
    vector and associated labels (either positive or negative). `input_x` holds the
    sentence, `input_y` holds the value of label, and we use `dropout_keep_prob` for the
    probability that we keep a neuron in the dropout layer. The following code shows
    an example of this:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**输入的占位符**：我们首先定义了所有需要包含模型输入值的占位符。在这种情况下，输入是句子向量和相关标签（正面或负面）。`input_x`保存句子，`input_y`保存标签值，我们使用`dropout_keep_prob`来表示我们在dropout层中保留神经元的概率。以下代码展示了一个示例：'
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Embedding**: Our model''s first layer, in which we feed the word representations
    learned in the process of training the `word2vec` model, is the embedding layer.
    We will modify the baseline code that''s in the repository to use our pre-trained
    embedding model, instead of learning the embedding from scratch. This will enhance
    the model accuracy. It is also a kind of a `transfer learning`, where we transfer
    the general knowledge learned from a generic Wikipedia or social media corpus.
    The embedding matrix that is initialized with the `word2vec` model is named `W`,
    as seen as follows:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**嵌入层**：我们模型的第一层，是我们将 `word2vec` 模型训练过程中学习到的单词表示输入的嵌入层。我们将修改仓库中的基准代码，使用我们预训练的嵌入模型，而不是从头开始学习嵌入。这将提高模型的准确性。这也是一种
    `迁移学习`，我们将从通用的 Wikipedia 或社交媒体语料库中转移学到的一般知识。通过 `word2vec` 模型初始化的嵌入矩阵命名为 `W`，如下所示：'
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Convolution with maxpooling:** Defining the convolution layer is done with `tf.nn.conv2d()`.
    This takes, as inputs, the previous embedding layer''s weight (`W`—filter matrix)
    and applies a nonlinear ReLU activation function. Further max polling is performed
    over each filter size using `tf.nn.max_pool()`*.* Results are concatenated, creating
    a single vector that will become the inputs for the following layer of the model:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**卷积与最大池化**：定义卷积层是通过 `tf.nn.conv2d()` 完成的。它的输入是前一个嵌入层的权重（`W`—过滤矩阵），并应用一个非线性
    ReLU 激活函数。然后，使用 `tf.nn.max_pool()` 对每个过滤器大小进行进一步的最大池化。结果会被串联起来，形成一个单一的向量，作为下一层模型的输入：'
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Dropout layer**: To regularize CNN and prevent the model from overfitting,
    a minor percentage of signals from neurons are blocked. This forces the model
    to learn more unique or individual features:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Dropout 层**：为了正则化 CNN 并防止模型过拟合，少量来自神经元的信号会被阻断。这迫使模型学习更多独特或个别的特征：'
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Prediction**: A TensorFlow wrapper performs the *W * x+b* metric multiplications,
    where `x` is the output of the previous layer. This computation will compute the
    values for the scores and the predictions will be produced by `tf.argmax()`:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预测**：一个 TensorFlow 包装器执行 *W * x+b* 度量乘法，其中 `x` 是上一层的输出。这个计算将计算分数的值，预测结果将通过
    `tf.argmax()` 产生：'
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '****Accuracy****: We can define the `loss` function with our scores. Remember
    that the measurement of the error our network makes is called **loss**. As good
    DL engineers, we want to minimize this and make our model more accurate. For the
    problem of categorization, the cross-entropy loss ([http://cs231n.github.io/linear-classify/#softmax](http://cs231n.github.io/linear-classify/#softmax))
    is the standard `loss` function used:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '****准确率****：我们可以使用我们的分数定义 `loss` 函数。记住，我们网络所犯的错误的衡量标准叫做 **loss**。作为优秀的深度学习工程师，我们希望最小化它，让我们的模型更加准确。对于分类问题，交叉熵损失
    ([http://cs231n.github.io/linear-classify/#softmax](http://cs231n.github.io/linear-classify/#softmax))
    是标准的 `loss` 函数：'
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'That''s it, we''re done with our model. Let''s use TensorBoard to visualize
    the network and improve our intuition, as shown:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们的模型完成了。让我们使用 TensorBoard 来可视化网络并提高我们的直觉，如下所示：
- en: '![](img/e942cccf-bcff-4240-9126-3807f881ce44.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e942cccf-bcff-4240-9126-3807f881ce44.png)'
- en: The CNN model architecture definition
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 模型架构定义
- en: Understanding data format
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据格式
- en: An interesting dataset, *Movie Review Data* from Rotten Tomatoes ([http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)),
    was used in this case. Half of the reviews are positive, the other half negative,
    and there are about 10,000 sentences in total. There are around 20,000 different
    words in the vocabulary. The dataset is stored in the `data` folder.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例中，使用了一个有趣的数据集，来自 Rotten Tomatoes 的 *电影评论数据* ([http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/))。一半的评论是正面的，另一半是负面的，总共有大约
    10,000 个句子。词汇表中大约有 20,000 个不同的单词。数据集存储在 `data` 文件夹中。
- en: 'It contains two files: one, `rt-polarity.neg`, contains all the negative sentences, and
    another, `rt-polarity.pos`, contains only positive sentences. To perform classification,
    we need to associate them with the labels. Each positive sentence is associated
    with a one-hot encoded label, `[0, 1]`, and each negative sentence is associated
    with `[1, 0]`, as shown in the following screenshot:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含两个文件：一个，`rt-polarity.neg`，包含所有负面句子；另一个，`rt-polarity.pos`，只包含正面句子。为了执行分类，我们需要将它们与标签关联。每个正面句子与一个独热编码标签
    `[0, 1]` 关联，每个负面句子与 `[1, 0]` 关联，如下图所示：
- en: '![](img/4ad74017-2ffb-4d21-acc7-a0f985edfc75.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ad74017-2ffb-4d21-acc7-a0f985edfc75.png)'
- en: A sample of few positive sentences and the label associated with the sentence
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一些正面句子的示例及与这些句子关联的标签
- en: 'Pre-processing the text data is done with these four steps:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据的预处理通过以下四个步骤完成：
- en: '**Load**: Make sure to load both the positive and negative sentence data files'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载**：确保加载正面和负面句子数据文件'
- en: '**Clean**: Use regex to remove punctuation and other special characters'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**清理**：使用正则表达式移除标点符号和其他特殊字符'
- en: '**Pad**: Make each sentence the same size by appending `<PAD>` tokens'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**填充**：通过添加`<PAD>`标记使每个句子大小相同'
- en: '**Index**: Map each word to an integer in an index so that each sentence can
    become a vector of integers'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**索引**：将每个单词映射到一个整数索引，以便每个句子都能变成整数向量'
- en: Now that we have our data formatted as vectors, we can feed them into our model.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据格式化为向量，我们可以将其输入到模型中。
- en: Integrating word2vec with CNN
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将word2vec与CNN结合
- en: So, the last time we created our `word2vec` model, we dumped that model into
    a binary file. Now it's time to use that model as part of our CNN model. We perform
    this by initializing the `W` weights in the embeddings to these values.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，上次我们创建`word2vec`模型时，我们将该模型导出了一个二进制文件。现在是时候将该模型作为CNN模型的一部分来使用了。我们通过将嵌入层中的`W`权重初始化为这些值来实现这一点。
- en: 'Since we trained on a very small corpus in our previous `word2vec` model, let''s
    choose the `word2vec` model that was pre-trained on the huge corpus. A good strategy
    is to use fastText embedding, which is trained on documents available online and
    for 294 languages ([https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)).
    We do this as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在之前的`word2vec`模型中使用了非常小的语料库，让我们选择一个已经在大规模语料库上预训练的`word2vec`模型。一种不错的策略是使用fastText嵌入，它是在互联网上可用的文档和294种语言上训练的（[https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)）。我们按以下步骤进行：
- en: We will download the English Embedding fastText dataset ([https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip))
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将下载英语Embedding fastText数据集（[https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip](https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.zip)）
- en: Next, extract the vocab and embedding vectors into a separate file
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将词汇表和嵌入向量提取到一个单独的文件中
- en: Load them into the `train.py` file
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将它们加载到`train.py`文件中
- en: That's it—by introducing this step, we can now feed the embedding layer with
    the pre-training `word2vec` model. This incorporation of information has a sufficient
    amount of features to improve the learning process of the CNN model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样——通过引入这一步骤，我们现在可以将预训练的`word2vec`模型馈入嵌入层。信息的整合提供了足够的特征，来改善CNN模型的学习过程。
- en: Executing the model
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行模型
- en: Now it's time to train our model with the provided dataset and the pre-trained
    embedding model. A few hyperparameters will need fine-tuning to achieve good results.
    But once we have executed the `train.py` file with reasonably good configurations,
    we can demonstrate that the model is able to distinguish well between the positive
    and negative sentences when classifying.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候使用提供的数据集和预训练的嵌入模型来训练我们的模型了。需要对一些超参数进行微调，以达到良好的结果。但一旦我们使用合理的配置执行`train.py`文件，就可以证明该模型能够在分类时很好地区分正面和负面句子。
- en: 'As we can see in the following graph, the performance metric of accuracy is
    tending towards 1 and the loss factor is reducing towards 0 over each iteration:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，准确率的性能指标逐渐趋近于1，而损失因子在每次迭代中逐渐减少至0：
- en: '![](img/02a5950d-45b2-4310-b7fb-0bce7b42a4ff.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02a5950d-45b2-4310-b7fb-0bce7b42a4ff.png)'
- en: A plot of the performance metrics accuracy and loss of the CNN model during
    the training process
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，CNN模型的性能指标准确率和损失的图表
- en: Voila! We just used the pre-trained embedding model to train our CNN classifier
    with an average loss of 6.9 and accuracy of 72.6%.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们刚刚使用了预训练的嵌入模型来训练我们的CNN分类器，平均损失为6.9，准确率为72.6%。
- en: 'Once the model training is completed successfully, the output of the model
    will have the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练成功完成，模型的输出将包含以下内容：
- en: The checkpoints stored in `/runs/folder`. We will use these checkpoints to make
    predictions.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点存储在`/runs/folder`目录下。我们将使用这些检查点来进行预测。
- en: A summary with all the loss, accuracy, histogram, and gradient value distribution
    captured during the training process. We can visualize it using the TensorBoard.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含所有损失、准确率、直方图和训练过程中捕获的梯度值分布的总结。我们可以使用TensorBoard可视化这些信息。
- en: Deploy the model into production
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型部署到生产环境
- en: Now that we have our model binaries stored in the `/runs/` folder, we just need
    to write a restful API, for which you can use Flask, and then call the `sentiment_engine()`
    defined in the `model_inference.py` code.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的模型二进制文件已存储在`/runs/`文件夹中，我们只需编写一个RESTful API，可以使用Flask，然后调用在`model_inference.py`代码中定义的`sentiment_engine()`。
- en: 'Always make sure that you use the checkpoints of the best model and the correct
    embedding file, which is defined as the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 始终确保使用最佳模型的检查点和正确的嵌入文件，定义如下：
- en: '[PRE18]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Summary
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Today's project was to build a DL computational linguistics model using word2vec to
    accurately classify text in a sentiment analysis paradigm. Our hypothetical use
    case was to apply DL to enable the management of a restaurant chain to understand
    the general sentiment of text responses their customers made, in response to a
    phone text question asking about their experience after dining. Our specific task
    was to build the natural language processing model that would create business
    intelligence from the data obtained in this simple (hypothetical) application.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的项目是使用word2vec构建一个深度学习计算语言学模型，在情感分析范式中准确分类文本。我们的假设使用案例是将深度学习应用于帮助餐厅连锁管理层理解客户通过短信回答问题后的整体情感。我们的具体任务是构建一个自然语言处理模型，从这个简单（假设的）应用中获得数据，生成商业智能。
- en: '**Revisit our success criteria**: How did we do? Did we succeed? What is the
    impact of success? Just as we defined success at the beginning of the project,
    these are the key questions we ask as DL data scientists as we look to wrap up
    a project.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**回顾我们的成功标准**：我们做得怎么样？我们成功了吗？成功的影响是什么？正如我们在项目开始时定义的那样，这些是我们作为深度学习数据科学家在结束项目时会问的关键问题。'
- en: Our CNN model, which was built on the trained `word2vec` model created earlier
    in the chapter, reached an accuracy of 72.6%! This means that we were able to
    reasonably accurately classify the unstructured text sentences as positive or
    negative.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的CNN模型，基于本章前面创建的训练好的`word2vec`模型，达到了72.6%的准确率！这意味着我们能够合理准确地将无结构的文本句子分类为正面或负面。
- en: What are the implications of this accuracy? In our hypothetical example, this
    means that we can take a body of data that is difficult to summarize outside of
    this DL NLP model and summarize it to produce actionable insights for the restaurant
    management. With summary data points of positive or negative sentiment to the
    questions asked in a phone text, the restaurant chain can track performance over
    time, make adjustments, and possibly even reward staff for improvements.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这种准确性的意义是什么？在我们的假设案例中，这意味着我们可以将一堆数据总结出来，而这些数据如果没有这个深度学习自然语言处理模型是很难总结的，并通过总结得出可操作的见解供餐厅管理层使用。通过总结对短信问题的正面或负面情感的数据点，餐厅连锁可以跟踪表现，进行调整，甚至可能奖励员工的改进。
- en: In this chapter's project, we learned how to build `word2vec` models and analyze
    what characteristics we can learn about the provided corpus. We also learned how
    to build a language model with CNN, using the trained word embeddings.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的项目中，我们学习了如何构建`word2vec`模型，并分析了我们可以从提供的语料库中学到哪些特征。我们还学习了如何使用训练好的词嵌入构建一个基于CNN的语言模型。
- en: Finally, we looked at the model performance in testing and determined whether
    we succeeded in achieving our goals. In the next chapter's project, we're going
    to leverage even more power from our computational linguistic skills to create
    a natural language pipeline that will power a chatbot for open domain question
    answering. This is exciting work—let's see what next!
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们查看了模型在测试中的表现，并确定是否成功达成了目标。在下一个章节的项目中，我们将利用更多的计算语言学技能，创建一个自然语言管道，驱动一个开放域问答的聊天机器人。这是一个令人兴奋的工作——让我们看看接下来会发生什么！
