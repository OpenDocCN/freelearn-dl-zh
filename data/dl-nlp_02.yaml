- en: '*Chapter 2*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第二章*'
- en: Applications of Natural Language Processing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理的应用
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够：
- en: Describe POS tagging and its applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述词性标注及其应用
- en: Differentiate between rule-based and stochastic POS taggers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分基于规则的词性标注器和基于随机的词性标注器
- en: Perform POS tagging, chunking, and chinking on text data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对文本数据执行词性标注、词块分析和分块
- en: Perform named entity recognition for information extraction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行命名实体识别进行信息提取
- en: Develop and train your own POS tagger and named entity recognizer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发并训练你自己的词性标注器和命名实体识别器
- en: Use NLTK and spaCy to perform POS tagging, chunking, chinking, and named entity
    recognition
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 NLTK 和 spaCy 执行词性标注（POS tagging）、词块分析（chunking）、分块（chinking）和命名实体识别（Named
    Entity Recognition）
- en: This chapter aims to introduce you to the plethora of applications of NLP and
    the various techniques involved within.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在向你介绍自然语言处理的众多应用及其涉及的各种技术。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: 'This chapter begins with a quick recap of what natural language processing
    is and what services it can help provide. Then, it discusses two applications
    of natural language processing: **Parts of Speech Tagging (POS tagging)** and
    **Named Entity Recognition**. The functioning, necessity, and purposes of both
    of these algorithms are explained. Additionally, there are exercises and activities
    that perform POS tagging and named entity recognition and build and develop these
    algorithms.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从快速回顾自然语言处理是什么以及它可以提供哪些服务开始。接着，讨论自然语言处理的两个应用：**词性标注（POS tagging）**和**命名实体识别（Named
    Entity Recognition）**。解释了这两种算法的功能、必要性和目的。此外，还有一些练习和活动，用于执行词性标注和命名实体识别，并构建和开发这些算法。
- en: Natural language processing consists of aiding machines to understand the natural
    language of humans in order to communicate with them effectively and automate
    a large number of tasks. The previous chapter discussed the applications of natural
    language processing along with examples of real-life use cases where these techniques
    could simplify the lives of humans. This chapter will specifically look into two
    of these algorithms and their real-life applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理旨在帮助机器理解人类的自然语言，以便有效地与人类沟通并自动化大量任务。上一章讨论了自然语言处理的应用以及这些技术如何简化人类生活的实际案例。本章将特别探讨其中的两种算法及其现实应用。
- en: Every aspect of natural language processing can be seen to follow the same analogy
    of teaching a language. In the last chapter, we saw how machines need to be told
    what parts of a corpus to pay attention to and what parts are irrelevant and unimportant.
    They need to be trained to remove stop words and noisy elements and focus on key
    words to reduce various forms of the same word to the word's root form so that
    it's easier to search for and interpret. In a similar fashion, the two algorithms
    discussed in this chapter also teach machines particular things about languages
    in the way we humans have been taught.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的各个方面都可以类比为语言教学。在上一章中，我们看到机器需要被告知关注语料库的哪些部分，哪些部分是无关紧要的。它们需要被训练去去除停用词和噪声元素，专注于关键词，将同一词的不同形式归约为词根，这样就能更容易地进行搜索和解读。以类似的方式，本章讨论的两种算法也教会机器一些关于语言的特定知识，正如我们人类在学习语言时所经历的。
- en: POS Tagging
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词性标注
- en: Before we dive straight into the algorithm, let's understand what parts of speech
    are. Parts of speech are something most of us are taught in our early years of
    learning the English language. They are categories assigned to words based on
    their syntactic or grammatical functions. These functions are the functional relationships
    that exist between different words.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解算法之前，先来了解什么是词性。词性是我们在学习英语的早期阶段被教授的内容。它们是根据单词的句法或语法功能对单词进行分类的方式。这些功能是不同单词之间存在的功能性关系。
- en: Parts of Speech
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词性
- en: 'The English language has nine main parts of speech:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 英语有九大主要词性：
- en: '*Nouns*: Things or people'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*名词*：事物或人'
- en: 'Examples: table, dog, piano, London, towel'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：table（桌子）、dog（狗）、piano（钢琴）、London（伦敦）、towel（毛巾）
- en: '*Pronouns*: Words that replace nouns'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代词*：替代名词的词'
- en: 'Examples: I, you, he, she, it'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：I（我）、you（你）、he（他）、she（她）、it（它）
- en: '*Verbs*: Action words'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动词*：表示动作的词'
- en: 'Examples: to be, to have, to study, to learn, to play'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：to be（是）、to have（有）、to study（学习）、to learn（学习）、to play（玩）
- en: '*Adjectives*: Words that describe nouns'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*形容词*：描述名词的词'
- en: 'Examples: intelligent, small, silly, intriguing, blue'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：intelligent（聪明的）、small（小的）、silly（傻的）、intriguing（有趣的）、blue（蓝色的）
- en: '*Determiners*: Words that limit nouns'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*限定词*：限制名词的词'
- en: 'Examples: a few, many, some, three'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：a few（一些）、many（许多）、some（一些）、three（三个）
- en: Note
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For more examples of determiners, visit https://www.ef.com/in/english-resources/english-grammar/determiners/.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 想了解更多限定词的例子，请访问 [https://www.ef.com/in/english-resources/english-grammar/determiners/](https://www.ef.com/in/english-resources/english-grammar/determiners/)。
- en: '*Adverbs*: Words that describe verbs, adjectives, or adverbs themselves'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*副词*：描述动词、形容词或副词本身的词'
- en: 'Examples: quickly, shortly, very, really, drastically'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：quickly（快速地）、shortly（很快）、very（非常）、really（真的）、drastically（急剧地）
- en: '*Prepositions*: Words that link nouns to other words'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*介词*：将名词与其他词连接的词'
- en: 'Examples: to, on, in, under, beside'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：to（到）、on（在……上）、in（在……里面）、under（在……下）、beside（在……旁边）
- en: '*Conjunctions*: Words that join two sentences or words'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连词*：连接两个句子或词语的词'
- en: 'Examples: and, but, yet'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：and（和）、but（但是）、yet（然而）
- en: '*Interjections*: Words that are exclamations'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*感叹词*：表示惊叹的词语'
- en: 'Examples: ouch! Ow! Wow!'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：ouch!（哎呀！）、Ow!（啊！）、Wow!（哇！）
- en: As you can see, each word falls under a specific Parts of speech tag assigned
    to it that helps us understand the meaning and purpose of the word, enabling us
    to better understand the context in which it is being used.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，每个词都被分配了一个特定的词性标签，这帮助我们理解词的意义和用途，使我们更好地理解其使用的上下文。
- en: POS Tagger
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词性标注器
- en: POS tagging is the process of assigning a tag to a word. This is done by an
    algorithm known as a POS tagger. The aim of the algorithm is really just as simple
    as this.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注是为一个词分配标签的过程。这是通过一种叫做词性标注器的算法完成的。该算法的目标其实就是这么简单。
- en: Most POS taggers are supervised learning algorithms. If you don't remember what
    supervised learning algorithms are, they are machine learning algorithms that
    learn to perform a task based on previously labeled data. The algorithms take
    rows of data as input. This data contains feature columns—data used to predict
    something—and usually one label column—the something that needs to be predicted.
    The models are trained on this input to learn and understand what features correspond
    to which label, thus learning how to perform the task of predicting the labels.
    Ultimately, they are given unlabeled data (data that just consists of feature
    columns), for which they must predict labels.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数词性标注器是监督式学习算法。如果你不记得什么是监督式学习算法，它是基于先前标注的数据学习执行任务的机器学习算法。算法将数据行作为输入。这些数据包含特征列——用于预测某些事物的数据——通常还包括一个标签列——需要预测的事物。模型在这些输入数据上进行训练，学习和理解哪些特征对应于哪些标签，从而学会如何执行预测标签的任务。最终，它们会接收到未标注的数据（仅包含特征列的数据），并根据这些数据预测标签。
- en: 'The following diagram is a general illustration of a supervised learning model:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示是监督式学习模型的一般示意图：
- en: '![Fig 2.1: Supervised learning'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.1：监督式学习'
- en: '](img/C13783_02_01.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_01.jpg)'
- en: 'Fig 2.1: Supervised learning'
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.1：监督式学习
- en: Note
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on supervised learning, go to https://www.packtpub.com/big-data-and-business-intelligence/applied-supervised-learning-python.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解更多关于监督式学习的信息，请访问 [https://www.packtpub.com/big-data-and-business-intelligence/applied-supervised-learning-python](https://www.packtpub.com/big-data-and-business-intelligence/applied-supervised-learning-python)。
- en: Thus, POS taggers hone their predictive abilities by learning from previously
    labeled datasets. In this case, the datasets can consist of a variety of features,
    such as the word itself (obviously), the definition of the word, the relationships
    of the word with its preceding, proceeding, and other related word(s) that are
    present within the same sentence, phrase, or paragraph. These features together
    help the tagger predict what POS tag should be assigned to a word. The corpus
    used to train a supervised POS tagger is known as a pre-tagged corpus. Such corpora
    serve as the basis for the creation of a system for the POS tagger to tag untagged
    words. These systems/types of POS taggers will be discussed in the next section.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，词性标注器通过学习先前标注过的数据集来提升其预测能力。在这种情况下，数据集可以包含各种特征，比如词本身（显然）、词的定义、词与其前后及其他相关词语（在同一句话、短语或段落中出现的）之间的关系。这些特征共同帮助标注器预测应该为一个词分配什么词性标签。用于训练监督式词性标注器的语料库被称为预标注语料库。这样的语料库作为构建系统的基础，帮助词性标注器为未标注的词进行标注。这些系统/类型的词性标注器将在下一节中讨论。
- en: Pre-tagged corpora, however, are not always readily available, and to accurately
    train a tagger, the corpus must be large. Thus, recently there have been iterations
    of the POS tagger that can be considered as unsupervised learning algorithms.
    These are algorithms that take data consisting solely of features as input. These
    features aren't associated with labels and thus the algorithm, instead of predicting
    labels, forms groups or clusters of the input data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，预标注的语料库并不总是现成可用的，而且为了准确训练标注器，语料库必须足够大。因此，最近有一些词性标注器的迭代版本可以被视为无监督学习算法。这些算法以仅包含特征的数据为输入。这些特征不与标签相关联，因此，算法不是预测标签，而是根据输入数据形成分组或聚类。
- en: In the case of POS tagging, the models use computational methods to automatically
    generate sets of POS tags. While, pre-tagged corpora are responsible for aiding
    the process of creating a system for the tagger in the case of supervised POS
    taggers, with unsupervised POS taggers, these computational methods serve as the
    basis for the creation of such systems. The drawback of unsupervised learning
    methods is that the cluster of POS tags generated automatically may not always
    be as accurate as those found in the pre-tagged corpora used to train supervised
    methods.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在词性标注的过程中，模型使用计算方法自动生成一组词性标签。而预标注语料库则在监督式词性标注器的情况下帮助创建标注器的系统，而在无监督词性标注器的情况下，这些计算方法则作为创建此类系统的基础。无监督学习方法的缺点在于，自动生成的词性标签集可能并不像用于训练监督方法的预标注语料库中的标签那么准确。
- en: 'To summarize, the key differences between supervised and unsupervised learning
    methods are as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，监督学习和无监督学习方法的关键区别如下：
- en: Supervised POS taggers take pre-tagged corpora as input to be trained, while
    unsupervised POS taggers take untagged corpora as input to create a set of POS
    tags.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督式词性标注器以预标注的语料库作为输入进行训练，而无监督词性标注器则以未标注的语料库作为输入，创建一组词性标签。
- en: Supervised POS taggers create dictionaries of words with their respective POS
    tags based on the tagged corpora, while unsupervised POS taggers generate these
    dictionaries using the self-created POS tag set.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督式词性标注器根据标注语料库创建包含相应词性标签的单词词典，而无监督式词性标注器则使用自创的词性标签集生成这些词典。
- en: 'Several Python libraries (such as NLTK and spaCy) have trained POS taggers
    of their own. You will learn how to use one in the following sections, but let''s
    understand the input and output of a POS tagger with an example for now. An important
    thing to remember is that since a POS tagger assigns a POS tag to each word in
    the given corpus, the input needs to be in the form of word tokens. Therefore,
    before performing POS tagging, tokenization needs to be carried out on the corpus.
    Let''s say we give the trained POS tagger the following tokens as an input:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 几个 Python 库（如 NLTK 和 spaCy）都有自己训练的词性标注器。你将在接下来的章节中学习如何使用其中一个，但现在先让我们通过一个示例来理解词性标注器的输入和输出。需要记住的一个重要点是，由于词性标注器会为给定语料库中的每个单词分配一个词性标签，因此输入需要以单词标记的形式提供。因此，在进行词性标注之前，需要对语料库进行标记化处理。假设我们给训练过的词性标注器输入以下标记：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After POS tagging, the output would look something like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行词性标注后，输出将类似于以下内容：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, **PRO** = pronoun, **V** = verb, **DT** = determiner, and **N** = noun.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**PRO** = 代词，**V** = 动词，**DT** = 限定词，**N** = 名词。
- en: 'The input and output for both a trained supervised and unsupervised POS tagger
    are the same: tokens, and tokens with POS tags, respectively.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过的监督式和无监督式词性标注器的输入和输出是相同的：分别是标记和带词性标签的标记。
- en: Note
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This is not the exact syntax of the output; you'll see the proper output later
    when you perform the exercise. This is just to give you an idea of what POS taggers
    do.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是输出的确切语法；你将在进行练习时看到正确的输出。这里仅仅是为了让你了解词性标注器的作用。
- en: 'The aforementioned parts of speech are very basic tags, and to ease the process
    of understanding natural language, POS algorithms create much more complicated
    tags that are variations of these basic ones. Here''s a full list of the POS tags
    with their descriptions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的词性只是非常基础的标签，为了简化自然语言理解的过程，词性算法创建了更为复杂的标签，这些标签是这些基础标签的变体。以下是完整的带描述的词性标签列表：
- en: '![Figure 2.2: POS tags with descriptions'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.2：带描述的词性标签'
- en: '](img/C13783_02_02.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_02.jpg)'
- en: 'Figure 2.2: POS tags with descriptions'
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.2：带描述的词性标签
- en: These tags are from the Penn Treebank tagset (https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html),
    which is one of the most popular tagsets. A majority of the pre-trained taggers
    for the English language are trained on this tagset, including NLTK's POS tagger.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标签来自宾夕法尼亚树库标签集（[https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)），它是最流行的标签集之一。大多数英语语言的预训练标注器都使用这个标签集进行训练，包括NLTK的词性标注器。
- en: Applications of Parts of Speech Tagging
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词性标注的应用
- en: Just like text pre-processing techniques help the machine understand natural
    language better by encouraging it to focus on only the important details, POS
    tagging helps the machine actually interpret the context of text and thus make
    sense of it. While text pre-processing is more of a cleaning phase, parts of speech
    tagging is actually the part where the machine is beginning to output valuable
    information about corpora on its own.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 就像文本预处理技术通过促使机器专注于重要细节，帮助机器更好地理解自然语言一样，词性标注帮助机器解读文本的上下文，从而理解它。虽然文本预处理更像是清理阶段，但词性标注实际上是机器开始独立输出关于语料库有价值信息的部分。
- en: 'Understanding what words correspond to which parts of speech can be beneficial
    in processing natural language in several ways for a machine:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 理解哪些词对应哪些词性，对于机器处理自然语言有多种好处：
- en: POS tagging is useful in differentiating between homonyms – words that have
    the same spelling but mean different things. For example, the word "play" can
    mean the verb to play, as in engage in an activity, and also the noun, as in a
    dramatic work to be performed on stage. A POS tagger can help the machine understand
    what context the word "play" is being used in by determining its POS tag.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注对于区分同形异义词非常有用——这些词虽然拼写相同，但意思却不同。例如，单词“play”可以作为动词，表示参与某项活动，也可以作为名词，表示一种在舞台上表演的戏剧作品。词性标注器可以帮助机器通过确定“play”的词性标签，理解该词在上下文中的使用情况。
- en: POS tagging builds on the need for sentence and word segmentation – one of the
    basic tasks of natural language processing.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注建立在句子和单词分割的需求之上——这是自然语言处理的基本任务之一。
- en: POS tags are used in performing higher-level tasks by other algorithms, one
    of which we will be discussing in this chapter, named entity recognition.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标签被用于其他算法执行更高层次的任务，其中之一就是我们将在本章中讨论的命名实体识别。
- en: POS tags contribute to the process of sentiment analysis and question answering
    too. For example, in the sentence "Tim Cook is the CEO of this technology company,"
    you want the machine to be able to replace "this technology company" with the
    name of the company. POS tagging can help the machine recognize that the phrase
    "this technology company" is a determiner ((this) + a noun phrase (technology
    company)). It can use this information to, for example, search articles online
    and check how many times "Tim Cook is the CEO of Apple" appears in them to then
    decide whether Apple is the correct answer.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标签也有助于情感分析和问答的过程。例如，在句子“Tim Cook是这家科技公司的CEO”中，你希望机器能够将“这家科技公司”替换为公司的名称。词性标注可以帮助机器识别出“这家科技公司”是一个限定词（（this）+名词短语（technology
    company））。它可以利用这些信息，例如，搜索网上的文章并检查“Tim Cook是Apple的CEO”出现的次数，进而判断Apple是否是正确答案。
- en: Thus, POS tagging is an important step in the process of understanding natural
    language because it contributes to other tasks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，词性标注是理解自然语言过程中的一个重要步骤，因为它有助于其他任务的完成。
- en: Types of POS Taggers
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词性标注器的类型
- en: As we saw in the previous section, POS taggers can be both of the supervised
    and unsupervised learning type. This difference largely affects how a tagger is
    trained. There is another distinction that impacts how the tagger actually assigns
    a tag to an untagged word, which is the approach used to train the taggers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中所看到的，词性标注器可以是监督学习型或无监督学习型。这一差异在很大程度上影响了标注器的训练方式。还有另一个区分点影响着标注器如何为未标注的词分配标签，这就是用于训练标注器的方法。
- en: The two types of POS taggers are rule-based and stochastic. Let's take a look
    at both of them.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注器有两种类型：基于规则的和随机的。我们来看看这两者。
- en: Rule-Based POS Taggers
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于规则的词性标注器
- en: These POS taggers work pretty much exactly as their name states – by rules.
    The purpose for giving the taggers sets of rules is to ensure that they tag an
    ambiguous/unknown word accurately most of the times, thus most of the rules are
    applied only when the taggers come across an ambiguous/unknown word.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词性标注器的工作方式几乎完全如其名称所示——通过规则。为标注器提供规则集的目的是确保它们在大多数情况下准确标注歧义/未知单词，因此大多数规则仅在标注器遇到歧义/未知单词时应用。
- en: 'These rules are often known as context frame rules and provide the taggers
    with contextual information to understand what tag to give an ambiguous word.
    An example of a rule is as follows: If an ambiguous/unknown word, x, is preceded
    by a determiner and followed by a noun, then assign it the tag of an adjective.
    An example of this would be "one small girl," where "one" is a determiner and
    "girl" is a noun, therefore the tagger will assign adjective to the word "small."'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则通常被称为上下文框架规则，向标注器提供上下文信息，以帮助它们理解给歧义单词分配哪个标签。一个规则的示例是：如果一个歧义/未知单词x前面跟着一个限定词，后面跟着一个名词，则为其分配形容词标签。一个例子是“one
    small girl”，其中“one”是限定词，“girl”是名词，因此标注器会给“small”分配形容词标签。
- en: The rules depend on your theory of grammar. Additionally, they also often include
    rules such as capitalization and punctuation. This can help you recognize pronouns
    and differentiate them from words found at the start of a sentence (following
    a full stop).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 规则取决于你对语法的理论。此外，规则通常还包括诸如大写和标点符号的规则。这可以帮助你识别代词，并将其与句子开头（跟随句号）出现的单词区分开来。
- en: Most rule-based POS taggers are supervised learning algorithms, in order to
    be able to learn the correct rules and apply them to properly tag ambiguous words.
    Recently, though, there have been experiments with training these taggers the
    unsupervised way. Untagged text is given to the tagger to tag, and humans go through
    the output tags, correcting whatever tags are inaccurate. This correctly tagged
    text is then given to the tagger so that it can develop correction rules between
    the two different tagsets and learn how to accurately tag words.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于规则的词性标注器是监督学习算法，旨在学习正确的规则并将其应用于正确标注歧义词。然而，最近也有实验尝试使用无监督的方式训练这些标注器。未标记的文本提供给标注器进行标注，然后人类检查输出的标签，纠正任何不准确的标签。这些正确标注的文本随后会提供给标注器，以便它可以在两个不同的标签集之间发展出修正规则，并学习如何准确标注单词。
- en: An example of this correction rule-based POS tagger is Brill's tagger, which
    follows the process mentioned earlier. Its functioning can be compared with the
    art of painting – when painting a house, it is easier to first paint the background
    of the house (for example, a brown square) and then paint the details, such as
    a door and windows, on top of that background using a finer brush. Similarly,
    Brill's rule-based POS tagger aims to first generally tag an untagged corpus,
    even if some of the tags may be wrong, and then revisit those tags to understand
    why some are wrong and learn from them.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于修正规则的词性标注器的一个例子是布里尔的标注器，它遵循之前提到的过程。它的功能可以与绘画艺术进行比较——在给房子上色时，首先涂上房子的背景色（例如，一个棕色的方形），然后在背景上使用更细的画笔绘制细节，如门和窗户。类似地，布里尔的基于规则的词性标注器的目标是首先一般性地标注一个未标注的语料库，即使某些标签可能是错误的，然后重新检查这些标签，理解哪些标签是错误的并从中学习。
- en: '**Note**'
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Exercises 10-16 can be performed in the same Jupyter Notebook.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 练习10-16可以在同一个Jupyter Notebook中进行。
- en: 'Exercise 10: Performing Rule-Based POS Tagging'
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10：执行基于规则的词性标注
- en: 'NLTK has a POS tagger that is a rule-based tagger. In this exercise, we will
    perform POS tagging using NLTK''s POS tagger. The following steps will help you
    with the solution:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK有一个基于规则的词性标注器。在本练习中，我们将使用NLTK的词性标注器进行词性标注。以下步骤将帮助你解决这个问题：
- en: Open cmd or terminal, depending on your operating system.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开cmd或终端，具体取决于你的操作系统。
- en: 'Navigate to the desired path and use the following command to initiate a `Jupyter`
    Notebook:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到所需路径，并使用以下命令启动`Jupyter` Notebook：
- en: '[PRE2]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Import `nltk` and `punkt`, as shown:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`nltk`和`punkt`，如所示：
- en: '[PRE3]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Store an input string in a variable called `s`, as follows:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入字符串存储在一个名为`s`的变量中，如下所示：
- en: '[PRE4]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Tokenize the sentence, as demonstrated:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如示例所示，对句子进行分词：
- en: '[PRE5]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Apply the POS tagger on the tokens and then print the tagset, as shown:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对词元应用词性标注器，然后打印标签集，如下所示：
- en: '[PRE6]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Your output will look like this:'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你的输出将如下所示：
- en: '![Fig 2.3: Tagged output'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图2.3：标注输出'
- en: '](img/C13783_02_03.jpg)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_02_03.jpg)'
- en: 'Fig 2.3: Tagged output'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.3：标注输出
- en: 'To understand what the "`NN`" POS tag stands for, you can use the following
    line of code:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要理解 "`NN`" 词性标签的含义，你可以使用以下代码行：
- en: '[PRE7]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output will be as follows:'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果将如下所示：
- en: '![Fig 2.4: Noun details'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.4：名词详情'
- en: '](img/C13783_02_04.jpg)'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_02_04.jpg)'
- en: 'Fig 2.4: Noun details'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.4：名词详情
- en: You can do this for each POS tag by substituting "NN" with it.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以通过将“NN”替换为每个词性标签来对每个词性标签进行处理。
- en: Let's try this out with a sentence containing homonyms.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们通过一个包含同义词的句子来尝试一下。
- en: 'Store an input string containing homonyms in a variable called sent:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将包含同义词的输入字符串存储在一个名为 sent 的变量中：
- en: '[PRE8]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Tokenize this sentence and then apply the POS tagger on the tokens, as shown:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对这个句子进行分词处理，并像示例中一样应用词性标注器：
- en: '[PRE9]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Expected output:**'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Fig 2.5: Tagged output'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.5：标注输出'
- en: '](img/C13783_02_05.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_05.jpg)'
- en: 'Fig 2.5: Tagged output'
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.5：标注输出
- en: As you can see, the first instance of the word play has been tagged as '**VB**',
    which stands for verb, base form, and the second instance of the word play has
    been tagged as '**NN**', which stands for noun. Thus, POS taggers are able to
    differentiate between homonyms and different instances of the same word. This
    helps machines understand natural language better.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，第一个“play”实例被标注为“**VB**”，代表动词，基本形式，第二个“play”实例被标注为“**NN**”，代表名词。因此，词性标注器能够区分同义词和同一单词的不同实例。这有助于机器更好地理解自然语言。
- en: Stochastic POS Taggers
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机词性标注器
- en: Stochastic POS taggers are taggers that use any method other than rule-based
    methods to assign tags to words. Thus, there are a large number of approaches
    that fall into the stochastic category. All models that incorporate statistical
    methods, such as probability and frequency, when determining the POS tags for
    words are stochastic models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 随机词性标注器是使用除基于规则的方法之外的任何方法为单词分配标签的标注器。因此，有很多方法属于随机类别。所有在为单词确定词性标签时，采用统计方法（如概率和频率）模型的标注器都是随机模型。
- en: 'We will discuss three models:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论三种模型：
- en: The Unigram or Word Frequency Approach
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单字法或词频方法
- en: The n – gram approach
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n-gram 方法
- en: The hidden Markov Model
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型
- en: '*The Unigram or Word Frequency Approach*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '*单字法或词频方法*'
- en: The simplest stochastic POS taggers assign POS tags to ambiguous words solely
    based on the probability that a word occurs with a tag. This basically means that
    whatever tag the tagger found linked with a word most often in the training set
    is the tag that it will assign to an ambiguous instance of the same word. For
    example, let's say the training set has the word "beautiful" tagged as an adjective
    a majority of the time. When the POS tagger encounters "beaut", it won't be able
    to tag this directly because it isn't a proper word. This will be an ambiguous
    word, and so it will calculate the probability of it being each of the POS tags,
    based on how many times different instances of this word have been tagged with
    each of those POS tags. "beaut" can be seen as an ambiguous form of "beautiful",
    and since "beautiful" has been tagged as an adjective a majority of the time,
    the POS tagger will tag "beaut" as an adjective too. This is called the word frequency
    approach because the tagger is checking the frequency of the POS tags assigned
    to words.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的随机词性标注器仅根据一个词与标签出现的概率来为模糊的单词分配词性标签。这基本上意味着，标注器在训练集中最常与某个单词关联的标签，就是它为该模糊单词实例分配的标签。例如，假设训练集中，“beautiful”一词大多数情况下被标注为形容词。当词性标注器遇到“beaut”时，它不能直接标注该词，因为它不是一个有效的单词。这个词将是一个模糊词，因此它将计算它可能属于每个词性标签的概率，基于该词在不同实例中被标注为每个词性标签的频率。“beaut”可以看作是“beautiful”的模糊形式，并且由于“beautiful”大多数时候被标注为形容词，词性标注器也会将“beaut”标注为形容词。这被称为词频方法，因为标注器正在检查与单词关联的词性标签的频率。
- en: '*The n – gram Approach*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*n-gram 方法*'
- en: This builds on the previous approach. The **n** in the name stands for how many
    words are considered when determining the probability of a word belonging to a
    particular POS tag. In the Unigram tagger, **n = 1**, and thus only the word itself
    is taken into consideration. Increasing the value of n results in taggers calculating
    the probability of a specific sequence of n POS tags occurring together and assigning
    a word a tag based on this probability.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这一方法建立在之前的基础上。名称中的 **n** 代表在确定一个单词属于某一词性标签的概率时考虑的单词数量。在单字法标注器中，**n = 1**，因此只考虑单词本身。增加
    **n** 的值会导致标注器计算特定的 n 个词性标签序列共同出现的概率，并根据该概率为单词分配标签。
- en: When assigning a tag to a word, these POS taggers create a context of the word
    by factoring in the type of token it is, along with the POS tags of the n preceding
    words. Based on the context, the taggers select the tag that is most likely to
    be in sequence with the tags of the preceding words and assigns this to the word
    in question. The most popular n – gram tagger is known as the Viterbi algorithm.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在为单词分配标签时，这些POS标注器通过考虑它的类型和前面n个单词的POS标签来创建单词的上下文。根据上下文，标注器选择最可能与前面单词标签序列一致的标签，并将其分配给所讨论的单词。最流行的n-gram标注器称为维特比算法。
- en: '*Hidden Markov Model*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*隐马尔可夫模型*'
- en: '*The hidden Markov model combines both the word frequency approach and the
    n – gram approach. A Markov model is one that describes a sequence of events or
    states. The probability of each state occurring depends solely on the state attained
    by the previous event. These events are based on observations. The "hidden" aspect
    of the hidden Markov model is that the set of states that an event could possibly
    be is hidden.*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*隐马尔可夫模型结合了词频方法和n-gram方法。马尔可夫模型描述了事件或状态的序列。每个状态发生的概率仅依赖于前一个事件的状态。这些事件基于观察。隐马尔可夫模型的“隐藏”方面在于，事件可能是一组隐藏状态。*'
- en: '*In the case of POS tagging, the observations are the word tokens, and the
    hidden set of states are the POS tags. The way this works is that the model calculates
    the probability of a word having a particular tag based on what the tag of the
    previous word was. For example, P (V | NN) is the probability of the current word
    being a verb given that the previous word is a noun.*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '*在POS标注的情况下，观察结果是单词标记，而隐藏的状态集合是POS标签。工作方式是模型根据前一个词的标签计算当前词具有特定标签的概率。例如，P（V
    | NN）是当前单词是动词的概率，假设前一个单词是名词。*'
- en: '*Note*'
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*注意*'
- en: '*This is a very basic explanation of the hidden Markov model. To learn more,
    go to* https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是对隐马尔可夫模型的非常基本的解释。要了解更多，请访问* https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24。'
- en: To learn more about stochastic models, go to http://ccl.pku.edu.cn/doubtfire/NLP/Lexical_Analysis/Word_Segmentation_Tagging/POS_Tagging_Overview/POS%20Tagging%20Overview.htm.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多有关随机模型的信息，请访问 http://ccl.pku.edu.cn/doubtfire/NLP/Lexical_Analysis/Word_Segmentation_Tagging/POS_Tagging_Overview/POS%20Tagging%20Overview.htm。
- en: The three approaches mentioned earlier have been explained in an order where
    each model builds upon and improves the accuracy of the preceding model. However,
    each model that builds upon a preceding model involves more calculations of probability
    and thus will take more time to perform computations, depending on the size of
    the training corpus. Therefore, the decision of which approach to use depends
    on the size of the corpus.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的三种方法按顺序解释了每个模型是如何在前一个模型的基础上构建并提高精度的。然而，每个建立在前一个模型基础上的模型涉及更多的概率计算，因此将根据训练语料库的大小来进行更多的计算时间。因此，选择使用哪种方法取决于语料库的大小。
- en: 'Exercise 11: Performing Stochastic POS Tagging'
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习11：执行随机POS标注
- en: 'spaCy''s POS tagger is a stochastic one. In this exercise, we will use spaCy''s
    POS tagger on some sentences to see the difference in the results of rule-based
    and stochastic tagging. The following steps will help you with the solution:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy的POS标注器是一种随机标注器。在本练习中，我们将使用spaCy的POS标注器对一些句子进行标记，以查看基于规则和随机标注之间的结果差异。以下步骤将帮助您解决问题：
- en: Note
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'To install spaCy, click on the following link and follow the instructions:
    https://spacy.io/usage'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装spaCy，请点击以下链接并按照说明操作：https://spacy.io/usage
- en: 'Import `spaCy`:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`spaCy`：
- en: '[PRE10]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Load spaCy''s ''`en_core_web_sm`'' model:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载spaCy的'`en_core_web_sm`'模型：
- en: '[PRE11]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: spaCy has models that are specific to different languages. The 'en_core_web_sm'
    model is the English language model and has been trained on written web text,
    such as blogs and news articles, and includes vocabulary, syntax, and entities.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: spaCy有针对不同语言的特定模型。'en_core_web_sm'模型是英语语言模型，已经在书面网络文本（如博客和新闻文章）上进行了训练，包括词汇、句法和实体。
- en: Note
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: To learn more about spaCy models, click on https://spacy.io/models.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要了解更多有关spaCy模型的信息，请访问 https://spacy.io/models。
- en: 'Fit the model on the sentence you want to assign POS tags to. Let''s use the
    sentence we gave NLTK''s POS tagger:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您要为其分配POS标签的句子上拟合模型。让我们使用我们给NLTK的POS标注器的句子：
- en: '[PRE12]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, let''s tokenize this sentence, assign the POS tags, and print them:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们对这个句子进行标记化，分配POS标签，并打印它们：
- en: '[PRE13]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Expected output:**'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 2.6: Output for POS tags'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.6：词性标注输出'
- en: '](img/C13783_02_06.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_06.jpg)'
- en: 'Figure 2.6: Output for POS tags'
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.6：词性标注输出
- en: 'To understand what a POS tag stands for, use the following line of code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 若要理解词性标注的含义，可以使用以下代码：
- en: '[PRE14]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Replace "VBZ" with the POS tag you''d like to know about. In this case, your
    output will be this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 用你想了解的词性标注替换"VBZ"。在这种情况下，你的输出将是：
- en: '[PRE15]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As you can see, the results are pretty much the same as the ones obtained from
    the NLTK POS tagger. This is the case due to the simplicity of our input.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，结果与NLTK词性标注器的输出基本相同。这是因为我们的输入非常简单。
- en: Chunking
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分块处理
- en: POS taggers work on individual tokens of words. Tagging individual words isn't
    always the best way to understand corpora, though. For example, the words 'United'
    and 'Kingdom' don't make a lot of sense when they're separated, but 'United Kingdom'
    together tells the machine that this is a country, thus providing it with more
    context and information. This is where the process of chunking comes into the
    picture.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注器处理单独的词元进行标注。然而，单独对每个词进行标注并不是理解语料库的最佳方式。例如，'United'和'Kingdom'分开时意义不大，但'United
    Kingdom'作为一个整体告诉机器这代表一个国家，从而提供更多的上下文和信息。正是在这个过程中，分块处理发挥了作用。
- en: Chunking is an algorithm that takes words and their POS tags as input. It processes
    these individual tokens and their tags to see whether they can be combined. The
    combination of one or more individual tokens is known as a chunk, and the POS
    tag assigned to such a chunk is known as a chunk tag.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 分块处理是一种算法，它以单词及其词性标注作为输入。它处理这些单独的词元及其标注，查看它们是否可以组合。一个或多个单独的词元的组合被称为一个分块，分配给该分块的词性标注称为分块标记。
- en: 'Chunk tags are combinations of basic POS tags. They are easier to define phrases
    by and are more efficient than simple POS tags. These phrases are chunks. There
    will be instances where a single word is considered a chunk and assigned a chunk
    tag too. There are five major chunk tags:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 分块标记是基本词性标记的组合。通过这些标记更容易定义短语，而且比简单的词性标记更高效。这些短语即为分块。有时单个词也会被视为一个分块并分配分块标记。常见的五种主要分块标记如下：
- en: '*Noun Phrase* (*NP*): These are phrases that have nouns as the head word. They
    act as a subject or an object to the verb or verb phrase.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*名词短语* (*NP*)：这些是以名词为核心词的短语。它们作为动词或动词短语的主语或宾语。'
- en: '*Verb Phrase* (*VP*): These are phrases that have verbs as the head word.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*动词短语* (*VP*)：这些是以动词为核心词的短语。'
- en: '*Adjective Phrase* (*ADJP*): These are phrases that have adjectives as the
    head word. Describing and qualifying nouns or pronouns is the main function of
    adjective phrases. They are found either directly before or after the noun or
    pronoun.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*形容词短语* (*ADJP*)：这些是以形容词为核心词的短语。形容词短语的主要功能是描述和修饰名词或代词。它们通常位于名词或代词之前或之后。'
- en: '*Adverb Phrase* (*ADVP*): These are phrases that have adverbs as the head word.
    They''re used as modifiers for nouns and verbs by providing details that describe
    and qualify them.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*副词短语* (*ADVP*): 这些是以副词为核心词的短语。它们通过提供描述和修饰名词或动词的细节，用来作为这些词的修饰语。'
- en: '*Prepositional Phrase* (*PP*): These are phrases that have prepositions as
    the head word. They position an action or an entity in time or space.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*介词短语* (*PP*)：这些是以介词为核心词的短语。它们用来定位动作或实体在时间或空间中的位置。'
- en: 'For example, in the sentence ''the yellow bird is slow and is flying into the
    brown house'', the following phrases will be assigned the following chunk tags:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在句子“the yellow bird is slow and is flying into the brown house”中，以下短语将被分配相应的分块标记：
- en: '''the yellow bird'' – NP'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '''the yellow bird'' – NP'
- en: '''is'' – VP'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '''is'' – VP'
- en: '''slow'' – ADJP'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '''slow'' – ADJP'
- en: '''is flying'' – VP'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '''is flying'' – VP'
- en: '''into'' – PP'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '''into'' – PP'
- en: '''the brown house'' – NP'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '''the brown house'' – NP'
- en: Thus, chunking is performed after POS tagging has been applied on a corpus.
    This allows the text to be broken down into its simplest form (tokens of words),
    have its structure analyzed, and then be grouped back together into meaningful
    higher-level chunks. Chunking also benefits the process of named entity recognition.
    We'll see how in the coming section.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分块处理是在对语料库应用词性标注（POS tagging）之后进行的。这使得文本能够被分解为最简单的形式（单词的词元），对其结构进行分析，然后再重新组合成有意义的更高级别的分块。分块处理还有助于命名实体识别的过程。我们将在接下来的部分看到具体如何进行。
- en: The chunk parser present within the NLTK library is rule based and thus needs
    to be given a regular expression as a rule to output a chunk with its chunk tag.
    **spaCy** can perform chunking without the presence of rules. Let's take a look
    at both these approaches.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 库内的块解析器是基于规则的，因此需要提供一个正则表达式作为规则来输出具有其块标签的块。**spaCy**可以在没有规则的情况下执行分块。让我们看看这两种方法。
- en: 'Exercise 12: Performing Chunking with NLTK'
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 12：使用 NLTK 执行分块
- en: In this exercise, we will generate chunks and chunk tags. **nltk** has a regular
    expression parser. This requires an input of a regular expression of a phrase
    and the corresponding chunk tag. It then searches the corpus for this expression
    and assigns it the tag.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将生成块和块标签。**nltk**有一个正则表达式解析器。这需要一个短语的正则表达式和相应的块标签作为输入。然后它在语料库中搜索这个表达式并分配标签。
- en: 'Since chunking works with POS tags, we can add on to our code from the POS
    tagging exercise. We saved the tokens with their respective POS tags in ''tagset''.
    Let''s use this. The following steps will help you with the solution:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 由于分块与词性标记一起工作，我们可以在词性标记练习的基础上扩展代码。我们在 'tagset' 中保存了具有各自词性标记的标记。让我们使用它。以下步骤将帮助您解决问题：
- en: 'Create a regular expression that will search for a noun phrase, as shown:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个正则表达式，将搜索一个名词短语，如下所示：
- en: '[PRE16]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This regular expression is searching for a determiner (optional), followed by
    one or more adjectives and then a single noun. This will form a chunk called `Noun
    Phrase`.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此正则表达式搜索一个限定词（可选）、一个或多个形容词，然后是一个名词。这将形成一个称为`名词短语`的块。
- en: Note
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you don''t know how to write Regular Expressions, check out these quick
    tutorials: https://www.w3schools.com/python/python_regex.asp https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您不知道如何编写正则表达式，请查看这些快速教程：https://www.w3schools.com/python/python_regex.asp
    https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/
- en: 'Create an instance of `RegexpParser` and feed it the rule:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`RegexpParser`的实例，并将规则传递给它：
- en: '[PRE17]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Give `chunkParser` the `tagset` containing the tokens with their respective
    POS tags so that it can perform chunking, and then draw the chunks:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `tagset` 包含具有各自词性标记的标记传递给 `chunkParser`，以便它可以执行分块，然后绘制块：
- en: '[PRE18]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: matplotlib needs to be installed on your machine for the `.draw()` function
    to work.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 `.draw()` 函数能正常工作，matplotlib 需要在您的机器上安装。
- en: 'Your output will look something like this:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您的输出将类似于这样：
- en: '![Figure 2.7: Parse tree.'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.7：解析树。'
- en: '](img/C13783_02_07.jpg)'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_02_07.jpg)'
- en: 'Figure 2.7: Parse tree.'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.7：解析树。
- en: This is a parse tree. As you can see, the chunking process has recognized the
    noun phrases and labeled them, and the remaining tokens are shown with their POS
    tags.
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个解析树。如您所见，分块过程已识别出名词短语并加标签，剩余的标记显示它们的词性标签。
- en: 'Let''s try the same thing out with another sentence. Store an input sentence
    in another variable:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用另一句话试试同样的事情。将输入句子存储在另一个变量中：
- en: '[PRE19]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Tokenize the sentence and perform POS tagging using NLTK''s POS tagger:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 NLTK 的 POS 标记器对句子进行标记化和词性标注：
- en: '[PRE20]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Repeat step 3:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复第 3 步：
- en: '[PRE21]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Expected output:**'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**期望的输出：**'
- en: '![Figure 2.8: Output for chunking.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.8：分块的输出。'
- en: '](img/C13783_02_08.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_08.jpg)'
- en: 'Figure 2.8: Output for chunking.'
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.8：分块的输出。
- en: 'Exercise 13: Performing Chunking with spaCy'
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 13：使用 spaCy 执行分块
- en: 'In this exercise, we will implement chunking with spaCy. **spaCy** doesn''t
    require us to formulate rules to recognize chunks; it identifies chunks on its
    own and tells us what the head word is, thus telling us what the chunk tag is.
    Let''s identify some noun chunks using the same sentence from Exercise 12\. The
    following steps will help you with the solution:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 spaCy 实现分块。**spaCy**不需要我们制定规则来识别块；它自动识别块并告诉我们头词是什么，从而告诉我们块标签是什么。让我们使用与练习
    12 相同的句子识别一些名词块。以下步骤将帮助您解决问题：
- en: 'Fit `spaCy`''s English model on the sentence:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个句子上，使用 `spaCy` 的英文模型进行适配：
- en: '[PRE22]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Apply `noun_chunks` on this model, and for each chunk, print the text of the
    chunk, the root word of the chunk, and the dependency relation that connects the
    root word to its head:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个模型上应用 `noun_chunks`，对于每个块，打印块的文本、块的根词和连接根词与其头部的依赖关系：
- en: '[PRE23]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Expected output:**'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**期望的输出：**'
- en: '![Figure 2.9: Output for chunking with spaCy'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.9：使用 spaCy 进行分块的输出'
- en: '](img/C13783_02_09.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_09.jpg)'
- en: 'Figure 2.9: Output for chunking with spaCy'
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.9：使用 spaCy 进行分块的输出
- en: As you can see, chunking with **spaCy** is a lot simpler than with NLTK.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，与 NLTK 相比，spaCy 的分块要简单得多。
- en: Chinking
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Chinking
- en: Chinking is an extension of chunking, as you've probably guessed already from
    its name. It's not a mandatory step in processing natural language, but it can
    be beneficial.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 切除操作是分块的扩展，正如你从它的名字中可能已经猜到的那样。它不是处理自然语言中的必选步骤，但它可以是有益的。
- en: Chinking is performed after chunking. Post chunking, you have chunks with their
    chunk tags, along with individual words with their POS tags. Often, these extra
    words are unnecessary. They don't contribute to the final result or the entire
    process of understanding natural language and thus are a nuisance. The process
    of chinking helps us deal with this issue by extracting the chunks, and their
    chunk tags form the tagged corpus, thus getting rid of the unnecessary bits. These
    useful chunks are called chinks once they have been extracted from the tagged
    corpus.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 切除操作发生在分块之后。分块后，你会得到带有分块标签的块，以及带有词性标签的单个单词。通常，这些多余的单词是没有必要的。它们对最终结果或理解自然语言的整个过程没有贡献，因此会造成干扰。切除操作帮助我们通过提取块及其块标签来处理这个问题，形成标记语料库，从而去除不必要的部分。这些有用的块被称为切除块，一旦它们从标记语料库中提取出来。
- en: For example, if you need only the nouns or noun phrases from a corpus to answer
    questions such as "what is this corpus talking about?", you would apply chinking
    because it would extract just what you want and present it in front of your eyes.
    Let's check this out with an exercise.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你只需要从语料库中提取名词或名词短语来回答诸如“这个语料库在讲什么？”这样的问题，你应该应用切除操作，因为它只会提取你需要的内容，并将其呈现在你眼前。让我们通过一个练习来验证这一点。
- en: 'Exercise 14: Performing Chinking'
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 14：执行切除操作
- en: 'Chinking is basically altering the things that you''re looking for in a corpus.
    Thus, applying chinking involves altering the rule (regular expression) provided
    to `chinkParser`. The following steps will help you with the solution:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 切除操作基本上是在改变你在语料库中寻找的东西。因此，应用切除操作涉及改变提供给 `chinkParser` 的规则（正则表达式）。以下步骤将帮助你完成解决方案：
- en: 'Create a rule that chunks the entire corpus and only creates chinks out of
    the words or phrases tagged as nouns or noun phrases:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个规则，将整个语料库进行分块，并只从标记为名词或名词短语的单词或短语中创建切除块：
- en: '[PRE24]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This rule is in the form of a regular expression. Basically, this regular expression
    is telling the machine to ignore all words that are not nouns or noun phrases.
    When it comes across a noun or a noun phrase, this rule will ensure that it is
    extracted as a chink.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个规则的形式是正则表达式。基本上，这个正则表达式告诉机器忽略所有不是名词或名词短语的单词。当遇到名词或名词短语时，这个规则将确保它被提取为一个切除块。
- en: 'Create an instance of `RegexpParser` and feed it the rule:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 `RegexpParser` 实例并传入规则：
- en: '[PRE25]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Give `chinkParser` the `tagset` containing the tokens with their respective
    POS tags so that it can perform chinking, and then draw the chinks:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给 `chinkParser` 提供包含带有相应 POS 标签的标记的 `tagset`，以便它可以执行切除操作，然后绘制切除块：
- en: '[PRE26]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Expected output:**'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 2.10: Output for chinking'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.10：切除操作的输出'
- en: '](img/C13783_02_10.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_10.jpg)'
- en: 'Figure 2.10: Output for chinking'
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.10：切除操作的输出
- en: As you can see, the chinks have been highlighted and contain only nouns.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，切除块已被高亮显示，并且只包含名词。
- en: 'Activity 2: Building and Training Your Own POS Tagger'
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 2：构建和训练你自己的 POS 标注器
- en: 'We''ve already looked at POS tagging words using the existing and pre-trained
    POS taggers. In this activity, we will train our own POS tagger. This is like
    training any other machine learning algorithm. The following steps will help you
    with the solution:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过了如何使用现有的和预训练的 POS 标注器对单词进行词性标注。在这个活动中，我们将训练我们自己的 POS 标注器。这就像训练任何其他机器学习算法一样。以下步骤将帮助你完成解决方案：
- en: 'Pick a corpus to train the tagger on. You can use the nltk treebank to work
    on. The following code should help you import the treebank corpus:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个语料库来训练标注器。你可以使用 nltk treebank 来进行操作。以下代码应该能帮助你导入 treebank 语料库：
- en: '[PRE27]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Determine what features the tagger will consider when assigning a tag to a word.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定标注器在为单词分配标签时将考虑的特征。
- en: Create a function to strip the tagged words of their tags so that we can feed
    them into our tagger.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，去除带标签的单词的标签，以便我们可以将它们传递给我们的标注器。
- en: Build the dataset and split the data into training and testing sets. Assign
    the features to 'X' and append the POS tags to 'Y'. Apply this function on the
    training set.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建数据集并将数据分为训练集和测试集。将特征赋给 'X'，并将 POS 标签附加到 'Y'。在训练集上应用这个函数。
- en: Use the decision tree classifier to train the tagger.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用决策树分类器训练标注器。
- en: Import the classifier, initialize it, fit the model on the training data, and
    print the accuracy score.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入分类器，初始化它，在训练数据上拟合模型，并打印准确性分数。
- en: Note
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The accuracy score in the output may vary, depending on the corpus used.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出中的准确性分数可能会有所不同，这取决于使用的语料库。
- en: '**Expected output:**'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**期望输出：**'
- en: '![Figure 2.11: Expected accuracy score.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.11：期望的准确性分数。'
- en: '](img/C13783_02_11.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_11.jpg)'
- en: 'Figure 2.11: Expected accuracy score.'
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.11：期望的准确性分数。
- en: Note
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for the activity can be found on page 297.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第297页找到。
- en: Named Entity Recognition
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: This is one of the first steps in the process of information extraction. Information
    extraction is the task of a machine extracting structured information from unstructured
    or semi-structured text. This furthers the comprehension of natural language by
    machines.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这是信息提取过程中的第一步。信息提取是机器从非结构化或半结构化文本中提取结构化信息的任务。这有助于机器理解自然语言。
- en: After text preprocessing and POS tagging, our corpus becomes semi-structured
    and machine-readable. Thus, information extraction is performed after we've readied
    our corpus.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本预处理和词性标注后，我们的语料库变得半结构化且机器可读。因此，信息提取是在我们准备好语料库后进行的。
- en: 'The following diagram is an example of named entity recognition:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示为命名实体识别的示例：
- en: '![Figure 2.12: Example for named entity recognition'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.12：命名实体识别示例'
- en: '](img/C13783_02_12.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_12.jpg)'
- en: 'Figure 2.12: Example for named entity recognition'
  id: totrans-262
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.12：命名实体识别示例
- en: Named Entities
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名实体
- en: Named entities are real-world objects that can be classified into categories,
    such as people, places, and things. Basically, they are words that can be denoted
    by a proper name. Named entities can also include quantities, organizations, monetary
    values, and many more things.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体是可以归类为不同类别的现实世界对象，如人、地点和物品。基本上，它们是可以通过专有名称表示的词汇。命名实体还可以包括数量、组织、货币金额等。
- en: 'Some examples of named entities and the categories they fall under are as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 一些命名实体及其所属类别示例如下：
- en: Donald Trump, person
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 唐纳德·特朗普，人物
- en: Italy, location
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意大利，地点
- en: Bottle, object
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 瓶子，物体
- en: 500 USD, money
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 500 美元，货币
- en: Named entities can be viewed as instances of entities. In the previous examples,
    the categories are basically entities in their own and the named entities are
    instances of those. For example, London is an instance of city, which is an entity.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体可以视为实体的实例。在之前的示例中，类别基本上是实体本身，命名实体是这些实体的实例。例如，伦敦是“城市”的一个实例，而“城市”是一个实体。
- en: 'The most common named entity categories are as listed:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的命名实体类别如下所示：
- en: ORGANIZATION
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织（ORGANIZATION）
- en: PERSON
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人物（PERSON）
- en: LOCATION
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地点（LOCATION）
- en: DATE
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期（DATE）
- en: TIME
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间（TIME）
- en: MONEY
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 货币（MONEY）
- en: PERCENT
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 百分比（PERCENT）
- en: FACILITY
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设施（FACILITY）
- en: GPE (which stands Geo-Political Entity)
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPE（地理政治实体）
- en: Named Entity Recognizers
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名实体识别器
- en: Named entity recognizers are algorithms that identify and extract named entities
    from corpora and assign them a category. The input provided to a trained named
    entity recognizer consists of tokenized words with their respective POS tags.
    The output of named entity recognition is named entities along with their categories,
    among the other tokenized words and their POS tags.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别器是识别和提取语料库中的命名实体并为其分配类别的算法。提供给经过训练的命名实体识别器的输入是标记化的词语及其相应的词性标注。命名实体识别的输出是命名实体及其类别，并与其他标记化的词语及其词性标注一起给出。
- en: 'The problem of named entity recognition takes place in two phases:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别问题分为两个阶段：
- en: Identifying and recognizing named entities (for example, 'London')
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别和识别命名实体（例如，“伦敦”）
- en: Classifying these names entities (for example, 'London' is a 'location')
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对这些命名实体进行分类（例如，“伦敦”是一个“地点”）
- en: The first phase of identifying named entities is quite similar to the process
    of chunking, because the aim is to recognize things that are denoted by proper
    names. The named entity recognizer needs to look out for continuous sequences
    of tokens to be able to correctly spot named entities. For example, 'Bank of America'
    should be identified as a single named entity, despite the phrase containing the
    word 'America', which in itself is a named entity.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段的命名实体识别与分块过程相似，因为其目标是识别由专有名词表示的事物。命名实体识别器需要关注连续的词元序列，以便正确地识别命名实体。例如，“美国银行”应该被识别为一个单一的命名实体，尽管该短语包含了“美国”这个词，而“美国”本身就是一个命名实体。
- en: Much like POS taggers, most named entity recognizers are supervised learning
    algorithms. They are trained on input that contains named entities along with
    the categories that they fall under, thus enabling the algorithm to learn how
    to classify unknown named entities in the future.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 与词性标注器类似，大多数命名实体识别器都是监督学习算法。它们在包含命名实体及其所属类别的输入数据上进行训练，从而使算法能够学习如何在未来对未知命名实体进行分类。
- en: This input containing named entities with their respective categories is often
    known as a knowledge base. Once a named entity recognizer has been trained and
    is given an unrecognized corpus, it refers to this knowledge base to search for
    the most accurate classification to assign to a named entity.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这种包含命名实体及其相应类别的输入数据通常被称为知识库。一旦命名实体识别器经过训练并面对未识别的语料库，它会参考这个知识库，寻找最准确的分类，以分配给命名实体。
- en: However, due to the fact that supervised learning requires an excessive amount
    of labeled data, unsupervised learning versions of named entity recognizers are
    also being researched. These are trained on unlabeled corpora – text that doesn't
    have named entities categorized. Like POS taggers, named entity recognizers categorize
    the named entities, and then the incorrect categories are corrected manually by
    humans. This corrected data is fed back to the NERs so that they can simply learn
    from their mistakes.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于监督学习需要大量标注数据，未监督学习版本的命名实体识别器也正在进行研究。这些模型在未标注的语料库上进行训练——这些文本中没有被分类的命名实体。与词性标注器类似，命名实体识别器会对命名实体进行分类，然后不正确的分类会由人工进行修正。修正后的数据会反馈给命名实体识别器，从而使它们能够从错误中学习。
- en: Applications of Named Entity Recognition
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名实体识别的应用
- en: 'As mentioned earlier, named entity recognition is one of the first steps of
    information extraction and thus plays a major role in enabling machines to understand
    natural language and perform a variety of tasks based on it. Named entity recognition
    is and can be used in various industries and scenarios to simplify and automate
    processes. Let''s take a look at a few use cases:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，命名实体识别是信息提取的第一步，因此在使机器理解自然语言并执行各种基于此的任务中起着重要作用。命名实体识别可以并且已经在多个行业和场景中被使用，以简化和自动化过程。让我们来看几个应用案例：
- en: '*Online content*, including articles, reports, and blog posts, are often tagged
    to enable users to search for it more easily and also to get a quick overview
    of what exactly the content is about. Named entity recognizers can be used to
    scour through this content and extract named entities to automatically generate
    these tags. These tags help categorize articles into predefined hierarchies as
    well.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在线内容*，包括文章、报告和博客文章，通常会被打上标签，以便用户更轻松地进行搜索，并快速了解内容的主要信息。命名实体识别器可以用来扫描这些内容并提取命名实体，以自动生成这些标签。这些标签也有助于将文章归类到预定义的层级中。'
- en: '*Search algorithms* also benefit from these tags. If a user were to enter a
    keyword into a search algorithm, instead of scouring through all the words of
    every article (which will take forever), the algorithm just needs to refer to
    the tags produced by named entity recognition to pull up articles containing or
    pertaining to the entered keyword. This reduces the computational time and operations
    by a lot.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*搜索算法*同样也受益于这些标签。如果用户向搜索算法输入一个关键词，算法不需要遍历每篇文章中的所有单词（这会耗费大量时间），它只需要参考命名实体识别所生成的标签，就可以快速提取出包含或与输入关键词相关的文章。这大大减少了计算时间和操作量。'
- en: Another purpose for these tags is to create an *efficient recommendation system*.
    If you read an article that discusses the current political situation in India,
    and is thus maybe tagged as 'Indian Politics' (this is just an example), the news
    website can use this tag to suggest different articles with the same or similar
    tags. This also works in the case of visual entertainment such as movies and shows.
    Online streaming websites use tags assigned to content (for example, genres such
    as 'action', 'adventure', 'thriller', and so on) to understand your taste better
    and thus recommend similar content to you.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些标签的另一个用途是创建*高效的推荐系统*。如果你阅读了一篇关于印度当前政治局势的文章，文章可能被标记为“印度政治”（这只是一个例子），那么新闻网站可以利用这个标签来推荐具有相同或相似标签的不同文章。在视觉娱乐领域，如电影和电视节目也是如此。在线视频平台会使用分配给内容的标签（例如“动作”、“冒险”、“惊悚”等类型），以更好地了解你的口味，从而向你推荐相似的内容。
- en: '*Customer feedback* is important for any service or product providing company.
    Running customer complaints and reviews through named entity recognizers produces
    tags that can help classify them based on location, type of product, and type
    of feedback (positive or negative). These reviews and complaints can then be sent
    to the people responsible for that particular product or that particular area
    and can be dealt with based on whether the feedback is positive or negative. The
    same thing can be done with tweets, Instagram captions, Facebook posts, and so
    on.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*客户反馈* 对于任何提供服务或产品的公司都至关重要。通过命名实体识别器处理客户投诉和评价，可以生成标签，帮助基于地点、产品类型和反馈类型（正面或负面）对它们进行分类。这些评价和投诉随后可以发送给负责该产品或该领域的人，并根据反馈是正面还是负面进行处理。对推文、Instagram
    标题、Facebook 帖子等也可以进行类似操作。'
- en: As you can see, there are many applications of named entity recognition. Thus,
    it is important to understand how it works and how to implement it.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，命名实体识别有许多应用。因此，理解它是如何工作的，以及如何实现它，非常重要。
- en: Types of Named Entity Recognizers
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 命名实体识别器的类型
- en: 'As is the case with POS taggers, there are two broad methods to design a named
    entity recognizer: a linguistic approach by defining rules to recognize entities,
    or a stochastic approach using statistical models to accurately determine which
    category a named entity falls into best.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 与 POS 标注器一样，设计命名实体识别器有两种主要方法：通过定义规则来识别实体的语言学方法，或者使用统计模型的随机方法来准确确定命名实体属于哪一类别。
- en: Rule-Based NERs
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于规则的 NER
- en: Rule-based NERs work in the same way that rule-based POS taggers do.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的 NER 的工作方式与基于规则的 POS 标注器相同。
- en: Stochastic NERs
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机 NER
- en: 'These include any and all models that use statistics to name and recognize
    entities. There are several approaches to stochastic named entity recognition.
    Let''s take a look at two of them:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包括所有使用统计学命名和识别实体的模型。对于随机命名实体识别，有几种方法。让我们来看一下其中的两种：
- en: '*Maximum Entropy Classification*'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*最大熵分类*'
- en: This is a machine learning classification model. It calculates the probability
    of a named entity falling into a particular category solely on the basis of the
    information provided to it (the corpus).
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个机器学习分类模型。它仅根据提供给它的信息（语料库）计算命名实体落入特定类别的概率。
- en: Note
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on Maximum Entropy Classification, go to http://blog.datumbox.com/machine-learning-tutorial-the-max-entropy-text-classifier/.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如需了解更多关于最大熵分类的信息，请访问 http://blog.datumbox.com/machine-learning-tutorial-the-max-entropy-text-classifier/。
- en: '*Hidden Markov Model*'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*隐马尔可夫模型*'
- en: '*This method is the same as the one explained in the POS tagging section, but
    instead of the hidden set of states being the POS tags, they are the categories
    of the named entities.*'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*这种方法与 POS 标注部分中解释的方法相同，但不同的是，隐藏状态集不再是 POS 标签，而是命名实体的类别。*'
- en: Note
  id: totrans-309
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on stochastic named entity recognition and when to use
    which approach, go to http://www.datacommunitydc.org/blog/2013/04/a-survey-of-stochastic-and-gazetteer-based-approaches-for-named-entity-recognition-part-2.
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如需了解更多关于随机命名实体识别及何时使用哪种方法的信息，请访问 http://www.datacommunitydc.org/blog/2013/04/a-survey-of-stochastic-and-gazetteer-based-approaches-for-named-entity-recognition-part-2。
- en: 'Exercise 15: Perform Named Entity Recognition with NLTK'
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 15：使用 NLTK 执行命名实体识别
- en: 'In this exercise, we''ll use the `ne_chunk` algorithm of `NLTK` to perform
    named entity recognition on a sentence. Instead of using the sentences we used
    in the previous exercises, create a new sentence that contains proper names that
    can be classified into categories so that you can actually see the results:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 `NLTK` 的 `ne_chunk` 算法对一个句子进行命名实体识别。与前几次练习中使用的句子不同，创建一个包含可以分类的专有名词的新句子，这样你就能实际看到结果：
- en: 'Store an input sentence in a variable, as shown:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入句子存储在一个变量中，如下所示：
- en: '[PRE28]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Tokenize the sentence and assign `POS tags` to the tokens:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对句子进行分词，并为标记分配 `POS 标签`：
- en: '[PRE29]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Apply the `ne_chunk()` algorithm on the tagged words and either print or draw
    the results:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对标记过的词语应用 `ne_chunk()` 算法，并打印或绘制结果：
- en: '[PRE30]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Assigning the value of ''`True`'' to the ''`binary`'' parameter tells the algorithm
    to just recognize the named entities and not classify them. Thus, your results
    will look something like this:'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将 `True` 的值赋给 `binary` 参数，告诉算法仅识别命名实体，而不对其进行分类。因此，你的结果将类似于以下内容：
- en: '![Figure 2.13: Output for named entity recognition with POS tags'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.13：带有 POS 标签的命名实体识别输出'
- en: '](img/C13783_02_13.jpg)'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_02_13.jpg)'
- en: 'Figure 2.13: Output for named entity recognition with POS tags'
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.13：带有词性标注的命名实体识别输出
- en: As you can see, the named entities have been highlighted as '`NE`'.
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如你所看到的，命名实体被标记为 '`NE`'。
- en: 'To know which categories the algorithm has assigned to these named entities,
    simply assign the value of ''`False`'' to the ''`binary`'' parameter:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要知道算法为这些命名实体分配了哪些类别，只需将 '`binary`' 参数的值设置为 '`False`'：
- en: '[PRE31]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Expected output:'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出：
- en: '![Figure 2.14: Output with named entities'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.14：带有命名实体的输出'
- en: '](img/C13783_02_14.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_14.jpg)'
- en: 'Figure 2.14: Output with named entities'
  id: totrans-329
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.14：带有命名实体的输出
- en: The algorithm has accurately categorized 'Shubhangi' and 'SpiceJet'. 'Taj Mahal',
    however, shouldn't be an ORGANIZATION, it should be a FACILITY. Thus, NLTK's `ne_chunk()`
    algorithm isn't the best one.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法准确地将 'Shubhangi' 和 'SpiceJet' 分类。'Taj Mahal' 这一项不应该是组织（ORGANIZATION），它应该是设施（FACILITY）。因此，NLTK
    的 `ne_chunk()` 算法并不是最佳选择。
- en: 'Exercise 16: Performing Named Entity Recognition with spaCy'
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 16：使用 spaCy 进行命名实体识别
- en: 'In this exercise, we''ll be implementing **spaCy**''s named entity recognizer
    on the sentence from the previous exercise and compare the results. spaCy has
    several NERs that have been trained on different corpora. Each model has a different
    set of categories; here''s a list of all the categories spaCy can recognize:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将实现 **spaCy** 的命名实体识别器，处理前一个练习中的句子并比较结果。spaCy 有多个命名实体识别模型，这些模型在不同的语料库上进行训练。每个模型有不同的类别集；以下是
    spaCy 可以识别的所有类别的列表：
- en: '![Figure 2.15: Categories of spaCy'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.15：spaCy 的类别'
- en: '](img/C13783_02_15.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_15.jpg)'
- en: 'Figure 2.15: Categories of spaCy'
  id: totrans-335
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.15：spaCy 的类别
- en: 'The following steps will help you with the solution:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你解决问题：
- en: 'Fit `spaCy`''s English model on the sentence we used in the previous exercise:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前一个练习中使用的句子上适配 `spaCy` 的英语模型：
- en: '[PRE32]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'For each entity in this sentence, print the text of the entity and the label:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于该句中的每个实体，打印实体的文本和标签：
- en: '[PRE33]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Your output will look something like this:'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你的输出将像这样：
- en: '![Figure 2.16: Output for named entity'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.16：命名实体输出'
- en: '](img/C13783_02_16.jpg)'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C13783_02_16.jpg)'
- en: 'Figure 2.16: Output for named entity'
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.16：命名实体输出
- en: It's only recognizing 'SpiceJet' and 'Pune' as named entities, and not 'Shubhangi'
    and 'Taj Mahal'. Let's try adding a last name to 'Shubhangi' and check whether
    that makes a difference.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它只识别了 'SpiceJet' 和 'Pune' 作为命名实体，而没有识别 'Shubhangi' 和 'Taj Mahal'。让我们试着给 'Shubhangi'
    加上一个姓氏，看看是否有所不同。
- en: 'Fit the model on the new sentence:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新句子上适配模型：
- en: '[PRE34]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Repeat step 2:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2：
- en: '[PRE35]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '**Expected output:**'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 2.17: Output for named entity recognition with spaCy.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.17：使用 spaCy 进行命名实体识别的输出。'
- en: '](img/C13783_02_17.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_17.jpg)'
- en: 'Figure 2.17: Output for named entity recognition with spaCy.'
  id: totrans-353
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.17：使用 spaCy 进行命名实体识别的输出。
- en: So now that we've added a last name, "Shubhangi Hora" is recognized as a PERSON,
    and "Taj Mahal" is recognized as a **WORK_OF ART**. The latter is incorrect, since
    if you check the table of categories, **WORK_OF_ART** is used to describe songs
    and books.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加上了姓氏，“Shubhangi Hora” 被识别为一个 PERSON，“Taj Mahal” 被识别为 **WORK_OF_ART**。后者是不正确的，因为如果你查看类别表，**WORK_OF_ART**
    用来描述歌曲和书籍。
- en: Thus, the recognition and categorization of named entities strongly depends
    on the data that the recognizer has been trained on. This is something to keep
    in mind when implementing named entity recognition; it is often better to train
    and develop your own recognizer for specific use cases.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，命名实体的识别和分类强烈依赖于识别器所训练的数据。这一点在实现命名实体识别时需要记住；通常来说，为特定的使用案例训练和开发自己的识别器会更好。
- en: 'Activity 3: Performing NER on a Tagged Corpus'
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 3：在标注语料库上执行 NER
- en: 'Now that we''ve seen how to perform named entity recognition on a sentence,
    in this activity, we''ll perform named entity recognition on a corpus that has
    been through POS tagging. Imagine that you''re given a corpus that you''ve identified
    the POS tags for and now your job is to extract entities from it so that you can
    provide an overall summary of what the corpus is discussing. The following steps
    will help you with the solution:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何在句子上执行命名实体识别，在这个活动中，我们将在一个经过词性标注的语料库上执行命名实体识别。假设你有一个语料库，已经为其标注了词性标签，现在你的任务是从中提取实体，以便你能够提供一个关于该语料库讨论内容的总体总结。以下步骤将帮助你解决问题：
- en: Import NLTK and other necessary packages.
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 NLTK 和其他必要的包。
- en: Print `nltk.corpus.treebank.tagged_sents()` to see the tagged corpus that you
    need extract named entities from.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印 `nltk.corpus.treebank.tagged_sents()` 来查看你需要提取命名实体的标注语料库。
- en: Store the first sentence of the tagged sentences in a variable.
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将标注句子的第一句存储到一个变量中。
- en: Use `nltk.ne_chunk` to perform NER on the sentence. Set **binary** to **True**
    and print the named entities.
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nltk.ne_chunk`对句子进行命名实体识别（NER）。将**binary**设置为**True**并打印命名实体。
- en: Repeat steps 3 and 4 on any number of sentences to see the different entities
    that exist in the corpus. Set the **binary** parameter to **False** to see what
    the named entities are categorized as.
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对任意数量的句子重复步骤3和步骤4，查看语料库中存在的不同实体。将**binary**参数设置为**False**，查看命名实体的分类。
- en: '**Expected output:**'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出：**'
- en: '![Figure 2.18: Expected output for NER on tagged corpus'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.18：对标注语料进行NER的预期输出'
- en: '](img/C13783_02_18.jpg)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C13783_02_18.jpg)'
- en: 'Figure 2.18: Expected output for NER on tagged corpus'
  id: totrans-366
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.18：对标注语料进行NER的预期输出
- en: Note
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for the activity can be found on page 300.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第300页找到。
- en: Summary
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Natural language processing enables a machine to understand the language of
    humans, and just as we learned how to comprehend and process language, machines
    are taught as well. Two ways of better understanding language that allow machines
    to contribute to the real world are POS tagging and named entity recognition.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理使机器能够理解人类的语言，就像我们学习如何理解和处理语言一样，机器也在被教导。两种能让机器更好理解语言并为现实世界做出贡献的方法是词性标注和命名实体识别。
- en: The former is the process of assigning POS tags to individual words so that
    the machine can learn context, and the latter is recognizing and categorizing
    named entities to extract valuable information from corpora.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 前者是将词性标签（POS）分配给单个单词，以便机器能够学习上下文，后者则是识别并分类命名实体，从语料库中提取有价值的信息。
- en: 'There are distinctions in the way these processes are performed: the algorithms
    can be supervised or unsupervised, and the approach can be rule-based or stochastic.
    Either way, the goal is the same, that is, to comprehend and communicate with
    humans in their natural language.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 这些过程的执行方式有所不同：算法可以是有监督的或无监督的，方法可以是基于规则的或随机的。无论哪种方式，目标都是一样的，即理解并与人类进行自然语言交流。
- en: In the next chapter, we will be discussing neural networks, how they work, and
    how they can be used for natural language processing.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论神经网络，它们如何工作，以及如何在自然语言处理中使用它们。
