- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Generating Graphs Using Graph Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用图神经网络生成图
- en: 'Graph generation consists of finding methods to create new graphs. As a field
    of study, it provides insights into understanding how graphs work and evolve.
    It also has direct applications in data augmentation, anomaly detection, drug
    discovery, and so on. We can distinguish two types of generation: **realistic
    graph generation**, which imitates a given graph (for example, in data augmentation),
    and **goal-directed graph generation**, which creates graphs that optimize a specific
    metric (for instance, in molecule generation).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 图生成包括寻找创建新图的方法。作为一个研究领域，它为理解图的工作方式和演化过程提供了见解。它在数据增强、异常检测、药物发现等方面有直接应用。我们可以区分两种生成类型：**现实图生成**，它模仿给定的图（例如，在数据增强中），以及**目标导向图生成**，它创建优化特定指标的图（例如，在分子生成中）。
- en: 'In this chapter, we will explore traditional techniques to understand how graph
    generation works. We will focus on two popular algorithms: the **Erdős–Rényi**
    and the **small-world** models. They present interesting properties but also issues
    that motivate the need for GNN-based graph generation. In the second section,
    we will describe three families of solutions: **variational autoencoder** (**VAE**)-based,
    autoregressive, and **GAN**-based models. Finally, we will implement a GAN-based
    framework with **Reinforcement Learning** (**RL**) to generate new chemical compounds.
    Instead of PyTorch Geometric, we will use the **DeepChem** library with TensorFlow.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索传统技术，以了解图生成的工作原理。我们将重点介绍两种流行的算法：**埃尔德什–雷尼**模型和**小世界**模型。它们具有有趣的特性，但也存在一些问题，这些问题促使了基于GNN的图生成方法的需求。在第二部分中，我们将描述三种解决方案：**变分自编码器**（**VAE**）基础的、自动回归的和**GAN**基础的模型。最后，我们将实现一个基于GAN的框架，并结合**强化学习**（**RL**）生成新的化学化合物。我们将使用**DeepChem**库与TensorFlow，而不是PyTorch
    Geometric。
- en: By the end of this chapter, you will be able to generate graphs using traditional
    and GNN-based techniques. You will have a good overview of this field and the
    different applications you can build with it. You will know how to implement a
    hybrid architecture to guide the generation into generating valid molecules with
    your desired properties.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，您将能够使用传统方法和基于GNN的技术生成图。您将对这一领域以及您可以用它构建的不同应用有一个良好的概览。您将知道如何实现混合架构，以引导生成有效的分子，且具备您所期望的属性。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Generating graphs with traditional techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用传统技术生成图
- en: Generating graphs with graph neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用图神经网络生成图
- en: Generating molecules with MolGAN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MolGAN生成分子
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter11](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter11).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码示例都可以在GitHub上找到，网址是[https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter11](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python/tree/main/Chapter11)。
- en: Installation steps required to run the code on your local machine can be found
    in the *Preface* of this book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的*前言*中可以找到在本地计算机上运行代码所需的安装步骤。
- en: Generating graphs with traditional techniques
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用传统技术生成图
- en: Traditional graph generation techniques have been studied for decades. This
    is why they are well understood and can be used as baselines in various applications.
    However, they are often limited in the type of graphs they can generate. Most
    of them are specialized to output certain topologies, which is why they cannot
    simply imitate a given network.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的图生成技术已经研究了数十年。这就是它们被广泛理解并可以在各种应用中作为基准使用的原因。然而，它们在可以生成的图类型上通常有局限性。大多数技术专注于输出特定的拓扑结构，这也是它们无法简单模仿给定网络的原因。
- en: 'In this section, we will introduce two classical techniques: the Erdős–Rényi
    and the small-world models.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍两种经典技术：埃尔德什–雷尼模型和小世界模型。
- en: The Erdős–Rényi model
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 埃尔德什–雷尼模型
- en: 'The Erdős–Rényi model is the simplest and most popular random graph model.
    It was introduced by Hungarian mathematicians Paul Erdős and Alfréd Rényi in 1959
    [1] and was independently proposed by Edgar Gilbert the same year [2]. This model
    has two variants: ![](img/Formula_B19153_11_001.png) and ![](img/Formula_B19153_11_002.png).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Erdős–Rényi模型是最简单且最流行的随机图模型。它由匈牙利数学家保罗·厄尔多斯和阿尔弗雷德·雷尼于1959年提出[1]，并由埃德加·吉尔伯特在同一年独立提出[2]。该模型有两种变体：![](img/Formula_B19153_11_001.png)和![](img/Formula_B19153_11_002.png)。
- en: 'The ![](img/Formula_B19153_11_003.png) model is straightforward: we are given
    ![](img/Formula_B19153_11_004.png) nodes and a probability ![](img/Formula_B19153_11_005.png)
    of connecting a pair of nodes. We try to randomly connect every node to each other
    to create the final graph. It means that there are ![](img/Formula_B19153_11_006.png)
    possible links. Another way of understanding the probability ![](img/Formula_B19153_11_007.png)
    is to consider it as a parameter to change the density of the network.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_B19153_11_003.png)模型很简单：给定![](img/Formula_B19153_11_004.png)个节点和连接一对节点的概率![](img/Formula_B19153_11_005.png)，我们尝试随机地将每个节点与其他节点连接，形成最终图。这意味着存在![](img/Formula_B19153_11_006.png)种可能的链接。另一种理解概率![](img/Formula_B19153_11_007.png)的方法是将其视为一个参数，用于改变网络的密度。'
- en: 'The `networkx` library has a direct implementation of the ![](img/Formula_B19153_11_008.png)
    model:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`networkx`库对![](img/Formula_B19153_11_008.png)模型有直接的实现：'
- en: 'We import the `networkx` library:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入`networkx`库：
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We generate a `G` graph using the `nx.erdos_renyi_graph()` function with `10`
    nodes (![](img/Formula_B19153_11_009.png)) and a probability for edge creation
    of `0.5` (![](img/Formula_B19153_11_010.png)):'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`nx.erdos_renyi_graph()`函数生成一个包含`10`个节点（![](img/Formula_B19153_11_009.png)）和边创建概率为`0.5`（![](img/Formula_B19153_11_010.png)）的`G`图：
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We position the resulting nodes using the `nx.circular_layout()` function.
    Other layouts can be used, but this one is handy for comparing different values
    of ![](img/Formula_B19153_11_011.png):'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`nx.circular_layout()`函数来定位结果节点。虽然可以使用其他布局方式，但这种布局方式对于比较不同的![](img/Formula_B19153_11_011.png)值非常方便：
- en: '[PRE2]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We draw the `G` graph with the `pos` layout using `nx.draw()`. Global heuristics
    are usually more accurate but require knowing the entirety of the graph. However,
    it is not the only way to predict links with this knowledge:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`nx.draw()`和`pos`布局绘制`G`图。全局启发式方法通常更为准确，但需要知道整个图的结构。然而，这并不是唯一的方法来使用这些知识预测链接：
- en: '[PRE3]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This gives us the following graph:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了如下图：
- en: '![Figure 11.1 – An Erdős–Rényi graph with 10 nodes and p=0.5](img/B19153_11_001.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 一个具有10个节点和p=0.5的Erdős–Rényi图](img/B19153_11_001.jpg)'
- en: Figure 11.1 – An Erdős–Rényi graph with 10 nodes and p=0.5
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 一个具有10个节点和p=0.5的Erdős–Rényi图
- en: 'We can repeat this process with a probability of **0.1** and **0.9** to obtain
    the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以**0.1**和**0.9**的概率重复此过程，从而得到以下图示：
- en: '![Figure 11.2 – Erdős–Rényi graphs with different probabilities for edge creation](img/B19153_11_002.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 不同边创建概率的Erdős–Rényi图](img/B19153_11_002.jpg)'
- en: Figure 11.2 – Erdős–Rényi graphs with different probabilities for edge creation
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 不同边创建概率的Erdős–Rényi图
- en: We can see that many nodes are isolated when ![](img/Formula_B19153_11_012.png)
    is low, while the graph is highly interconnected when ![](img/Formula_B19153_11_013.png)
    is high.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当![](img/Formula_B19153_11_012.png)较低时，许多节点是孤立的，而当![](img/Formula_B19153_11_013.png)较高时，图的互联性较强。
- en: 'In the ![](img/Formula_B19153_11_014.png) model, we randomly choose a graph
    from all graphs with ![](img/Formula_B19153_11_015.png) nodes and ![](img/Formula_B19153_11_016.png)
    links. For instance, if ![](img/Formula_B19153_11_017.png) and ![](img/Formula_B19153_11_018.png),
    there are three possible graphs (see *Figure 11**.3*). The ![](img/Formula_B19153_11_019.png)
    model will just randomly select one of these graphs. This is a different approach
    to the same problem, but it is not as popular as the ![](img/Formula_B19153_11_020.png)
    model because it is more challenging to analyze in general:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在![](img/Formula_B19153_11_014.png)模型中，我们从所有具有![](img/Formula_B19153_11_015.png)个节点和![](img/Formula_B19153_11_016.png)个链接的图中随机选择一个。例如，如果![](img/Formula_B19153_11_017.png)和![](img/Formula_B19153_11_018.png)，则有三个可能的图（见*图
    11.3*）。![](img/Formula_B19153_11_019.png)模型将随机选择其中一个。这是解决相同问题的另一种方法，但由于它更难以分析，因此不如![](img/Formula_B19153_11_020.png)模型流行：
- en: '![Figure 11.3 – A set of graphs with three nodes and two links](img/B19153_11_003.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – 一组包含三个节点和两个链接的图](img/B19153_11_003.jpg)'
- en: Figure 11.3 – A set of graphs with three nodes and two links
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 一组包含三个节点和两个链接的图
- en: 'We can also implement the ![](img/Formula_B19153_11_021.png) model in Python
    using the `nx.gnm_random_graph()` function:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用 `nx.gnm_random_graph()` 函数在 Python 中实现 ![](img/Formula_B19153_11_021.png)
    模型：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure 11.4 – A graph randomly sampled from the set of graphs with three
    nodes and two links](img/B19153_11_004.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.4 – 从具有三个节点和两个链接的图集中随机采样的图](img/B19153_11_004.jpg)'
- en: Figure 11.4 – A graph randomly sampled from the set of graphs with three nodes
    and two links
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – 从具有三个节点和两个链接的图集中随机采样的图
- en: The strongest and most interesting assumption made by the ![](img/Formula_B19153_11_022.png)
    model is that links are independent (meaning that they do not interfere with each
    other). Unfortunately, it is not true for most real-world graphs, where we observe
    clusters and communities that contradict this rule.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_B19153_11_022.png) 模型提出的最强大且最有趣的假设是，链接是独立的（意味着它们不会相互干扰）。不幸的是，这对于大多数现实世界的图来说并不成立，在这些图中，我们观察到与此规则相矛盾的簇和社区。'
- en: The small-world model
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 小世界模型
- en: 'Introduced in 1998 by Duncan Watts and Steven Strogatz [3], the small-world
    model tries to imitate the behavior of biological, technological, and social networks.
    The main concept is that real-world networks are not completely random (as in
    the Erdős–Rényi model) but not totally regular either (as in a grid). This kind
    of topology is somewhere in between, which is why we can interpolate it using
    a coefficient. The small-world model produces graphs that have both:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 小世界模型由 Duncan Watts 和 Steven Strogatz 在 1998 年提出 [3]，该模型试图模拟生物、技术和社交网络的行为。其主要概念是，现实世界的网络并非完全随机（如
    Erdős–Rényi 模型），也不是完全规则的（如网格）。这种拓扑结构介于两者之间，因此我们可以使用系数进行插值。小世界模型产生的图既具有：
- en: '**Short paths**: The average distance between any two nodes in the network
    is relatively small, which makes it easy for information to spread quickly throughout
    the network'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**短路径**：网络中任意两个节点之间的平均距离相对较小，这使得信息能够快速传播到整个网络'
- en: '**High clustering coefficients**: Nodes in the network tend to be closely connected
    to one another, creating dense clusters of nodes'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高聚类系数**：网络中的节点往往彼此紧密连接，形成密集的节点簇'
- en: 'Many algorithms display small-world properties. In the following, we will describe
    the original **Watts–Strogatz** model proposed in [3]. It can be implemented using
    the following steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法展示了小世界特性。接下来，我们将描述原始的**Watts–Strogatz**模型，该模型在 [3] 中提出。它可以通过以下步骤实现：
- en: We initialize a graph with ![](img/Formula_B19153_11_023.png) nodes.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化一个具有 ![](img/Formula_B19153_11_023.png) 节点的图。
- en: Each node is connected to its ![](img/Formula_B19153_11_024.png) nearest neighbors
    (or ![](img/Formula_B19153_11_025.png) neighbors if ![](img/Formula_B19153_11_026.png)
    is odd).
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个节点都连接到它的 ![](img/Formula_B19153_11_024.png) 最近邻（或者如果 ![](img/Formula_B19153_11_026.png)
    是奇数，则连接到 ![](img/Formula_B19153_11_025.png) 邻居）。
- en: Each link between nodes ![](img/Formula_B19153_11_027.png) and ![](img/Formula_B19153_11_028.png)
    has a probability ![](img/Formula_B19153_11_029.png) of being rewired between
    ![](img/Formula_B19153_11_030.png) and ![](img/Formula_B19153_11_031.png), where
    ![](img/Formula_B19153_11_032.png) is another random node.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个节点之间的链接 ![](img/Formula_B19153_11_027.png) 和 ![](img/Formula_B19153_11_028.png)
    都有一个重连的概率 ![](img/Formula_B19153_11_029.png)，重连到 ![](img/Formula_B19153_11_030.png)
    和 ![](img/Formula_B19153_11_031.png)，其中 ![](img/Formula_B19153_11_032.png) 是另一个随机节点。
- en: 'In Python, we can implement it by calling the `nx.watts_strogatz_graph()` function:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们可以通过调用 `nx.watts_strogatz_graph()` 函数来实现它：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This produces the following graph:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了如下的图：
- en: '![Figure 11.5 – A small-world network obtained with the Watts–Strogatz model](img/B19153_11_005.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.5 – 使用 Watts–Strogatz 模型获得的小世界网络](img/B19153_11_005.jpg)'
- en: Figure 11.5 – A small-world network obtained with the Watts–Strogatz model
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 使用 Watts–Strogatz 模型获得的小世界网络
- en: 'As with the Erdős–Rényi model, we can repeat the same process with different
    probabilities ![](img/Formula_B19153_11_033.png) to obtain *Figure 11**.6*:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Erdős–Rényi 模型一样，我们可以用不同的重连概率 ![](img/Formula_B19153_11_033.png) 重复相同的过程，以获得
    *图 11.6*：
- en: '![Figure 11.6 – A small-world model with different probabilities for rewiring](img/B19153_11_006.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 – 不同重连概率的小世界模型](img/B19153_11_006.jpg)'
- en: Figure 11.6 – A small-world model with different probabilities for rewiring
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – 不同重连概率的小世界模型
- en: We can see that when ![](img/Formula_B19153_11_034.png), the graph is completely
    regular. On the opposite end, when ![](img/Formula_B19153_11_035.png), the graph
    is completely random as every link has been rewired. We obtain a balanced graph
    between these two extremes with hubs and local clustering.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当![](img/Formula_B19153_11_034.png)时，图形是完全规则的。而在另一端，当![](img/Formula_B19153_11_035.png)时，图形是完全随机的，因为每个连接都被重新连接。我们通过这些极端之间的平衡，得到一个包含枢纽和局部聚类的图形。
- en: Nonetheless, the Watts–Strogatz model does not produce a realistic degree distribution.
    It also requires a fixed number of nodes, which means it cannot be used for network
    growth. In general, classical methods fail to capture real-world graphs’ full
    diversity and complexity. This motivated the creation of a new family of techniques,
    often referred to as deep graph generation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Watts–Strogatz模型并没有生成现实的度分布。它还要求一个固定数量的节点，这意味着它无法用于网络的增长。一般而言，经典方法无法捕捉到真实世界图形的多样性和复杂性。这促使了新一类技术的诞生，通常被称为深度图生成。
- en: Generating graphs with graph neural networks
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用图神经网络生成图形
- en: 'Deep graph generative models are GNN-based architectures that are more expressive
    than traditional techniques. However, it comes at a cost: they are often too complex
    to be analyzed and understood, like classical methods. We list three main families
    of architecture for deep graph generation: VAEs, GANs, and autoregressive models.
    Other techniques exist, such as normalizing flows or diffusion models, but they
    are less popular and mature than these three.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图生成模型是基于GNN的架构，比传统技术更具表现力。然而，这也有代价：它们通常过于复杂，无法像经典方法那样被分析和理解。我们列出了三种主要的深度图生成架构：VAE、GAN和自回归模型。虽然还有其他技术，如规范化流或扩散模型，但它们比这三种技术更不流行且不成熟。
- en: This section will describe how to use VAEs, GANs, and autoregressive models
    to generate graphs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将描述如何使用VAE、GAN和自回归模型生成图形。
- en: Graph variational autoencoders
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图形变分自编码器
- en: 'As seen in the last chapter, VAEs can be used to approximate an adjacency matrix.
    The **Graph Variational Autoencoder** (**GVAE**) model we saw has two components:
    an encoder and a decoder. The encoder uses two GCNs that share their first layer
    to learn the mean and the variance of each latent normal distribution. The decoder
    then samples the learned distributions to perform the inner product between latent
    variables ![](img/Formula_B19153_11_036.png). In the end, we obtained the approximated
    adjacency matrix ![](img/Formula_B19153_11_037.png).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如上一章所见，VAE可以用来逼近邻接矩阵。我们看到的**图形变分自编码器**（**GVAE**）模型有两个组成部分：编码器和解码器。编码器使用两个共享第一层的GCN来学习每个潜在正态分布的均值和方差。解码器然后从学习到的分布中采样，执行潜在变量的内积！[](img/Formula_B19153_11_036.png)。最终，我们得到逼近的邻接矩阵！[](img/Formula_B19153_11_037.png)。
- en: 'In the previous chapter, we used ![](img/Formula_B19153_11_038.png) to predict
    links. However, it is not its only application: it directly gives us the adjacency
    matrix of a network that imitates graphs seen during training. Instead of predicting
    links, we can use this output to generate new graphs. Here is an example of the
    adjacency matrix created by the VGAE model from [*Chapter 10*](B19153_10.xhtml#_idTextAnchor116):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用![](img/Formula_B19153_11_038.png)来预测连接。然而，这并不是它唯一的应用：它直接给出了一个模拟训练期间见过的图形的网络邻接矩阵。我们可以使用这个输出生成新的图形，而不是预测连接。以下是VGAE模型生成的邻接矩阵示例，来自[*第10章*](B19153_10.xhtml#_idTextAnchor116)：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Since 2016, this technique has been expanded beyond the GVAE model to also
    output node and edge features. A good example is one of the most popular VAE-based
    graph generative models: **GraphVAE** [4]. Introduced in 2018 by Simonovsky and
    Komodakis, it is designed to generate realistic molecules. This requires the ability
    to differentiate nodes (atoms) and edges (chemical bonds).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 自2016年以来，这项技术已扩展到GVAE模型之外，还可以输出节点和边的特征。一个很好的例子是最受欢迎的基于VAE的图生成模型之一：**GraphVAE**
    [4]。它由Simonovsky和Komodakis于2018年提出，旨在生成现实的分子。这需要能够区分节点（原子）和边（化学键）。
- en: 'GraphVAE considers graphs ![](img/Formula_B19153_11_039.png), where ![](img/Formula_B19153_11_040.png)
    is the adjacency matrix, ![](img/Formula_B19153_11_041.png) is the edge attribute
    tensor, and ![](img/Formula_B19153_11_042.png) is the node attribute matrix. It
    learns a probabilistic version of the graph ![](img/Formula_B19153_11_043.png)
    with a predefined number of nodes. In this probabilistic version, ![](img/Formula_B19153_11_038.png)
    contains node (![](img/Formula_B19153_11_045.png)) and edge (![](img/Formula_B19153_11_046.png))
    probabilities, ![](img/Formula_B19153_11_047.png) indicates class probabilities
    for edges, and ![](img/Formula_B19153_11_048.png) contains class probabilities
    for nodes. Compared to GVAE, GraphVAE’s encoder is a feed forward network with
    **edge-conditional graph convolutions** (**ECC**), and its decoder is a **multilayer
    perceptron** (**MLP**) with three outputs. The entire architecture is summarized
    in the following figure:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: GraphVAE 考虑图形 ![](img/Formula_B19153_11_039.png)，其中 ![](img/Formula_B19153_11_040.png)
    是邻接矩阵，![](img/Formula_B19153_11_041.png) 是边属性张量，![](img/Formula_B19153_11_042.png)
    是节点属性矩阵。它学习一个具有预定义节点数的图形的概率版本 ![](img/Formula_B19153_11_043.png)。在这个概率版本中，![](img/Formula_B19153_11_038.png)
    包含节点（![](img/Formula_B19153_11_045.png)）和边（![](img/Formula_B19153_11_046.png)）的概率，![](img/Formula_B19153_11_047.png)
    表示边的类别概率，![](img/Formula_B19153_11_048.png) 包含节点的类别概率。与 GVAE 相比，GraphVAE 的编码器是一个前馈网络，具有**边条件图卷积**（**ECC**），其解码器是一个具有三输出的**多层感知机**（**MLP**）。整个架构总结如下图：
- en: '![Figure 11.7 – GraphVAE’s inference process](img/B19153_11_007.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.7 – GraphVAE 的推理过程](img/B19153_11_007.jpg)'
- en: Figure 11.7 – GraphVAE’s inference process
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 – GraphVAE 的推理过程
- en: 'There are many other VAE-based graph generative architectures. However, their
    role is not limited to imitating graphs: they can also embed constraints to guide
    the type of graphs they produce.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他基于 VAE 的图生成架构。然而，它们的作用不限于模仿图形：它们还可以嵌入约束，以引导它们生成的图形类型。
- en: A popular way of adding these constraints is to check them during the decoding
    phase, such as the **Constrained Graph Variational Autoencoder** (**CGVAE**) [5].
    In this architecture, the encoder is a **Gated Graph Convolutional Network** (**GGCN**),
    and the decoder is an autoregressive model. Autoregressive decoders are particularly
    suited for this task, as they can verify every constraint for each step of the
    process. Finally, another technique to add constraints consists of using Lagrangian-based
    regularizers that are faster to compute but less strict in terms of generation
    [6].
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 添加这些约束的一种流行方法是在解码阶段检查它们，例如**受限图变分自编码器**（**CGVAE**）[5]。在该架构中，编码器是**门控图卷积网络**（**GGCN**），解码器是自回归模型。自回归解码器特别适用于这一任务，因为它们可以在过程的每个步骤中验证每个约束。最后，另一种添加约束的技术是使用基于拉格朗日的正则化器，这些正则化器计算速度更快，但在生成方面的严格性较差[6]。
- en: Autoregressive models
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自回归模型
- en: 'Autoregressive models can also be used on their own. The difference with other
    models is that past outputs become part of the current input. In this framework,
    graph generation becomes a sequential decision-making process that considers both
    data and past decisions. For instance, at each step, the autoregressive model
    can create a new node or a new link. Then, the resulting graph is fed to the model
    for the next generation step until we stop it. The following diagram illustrates
    this process:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型也可以单独使用。与其他模型的区别在于，过去的输出成为当前输入的一部分。在这一框架下，图生成变成了一个序列决策过程，同时考虑数据和过去的决策。例如，在每个步骤中，自回归模型可以创建一个新的节点或一个新的连接。然后，将生成的图输入到模型中进行下一步生成，直到我们停止它。下图展示了这个过程：
- en: '![Figure 11.8 – The autoregressive process for graph generation](img/B19153_11_008.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8 – 图生成的自回归过程](img/B19153_11_008.jpg)'
- en: Figure 11.8 – The autoregressive process for graph generation
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 – 图生成的自回归过程
- en: In practice, we use **Recurrent Neural Networks** (**RNNs**) to implement this
    autoregressive ability. In this architecture, previous outputs are used as inputs
    to compute the current hidden state. In addition, they can process inputs of arbitrary
    length, which is crucial for generating graphs iteratively. However, this computation
    is slower than feedforward networks, as the entire sequence must be processed
    to obtain the final output. The two most popular types of RNNs are the **Gated
    Recurrent Unit** (**GRU**) and **Long Short-Term Memory** (**LSTM**) networks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们使用 **递归神经网络**（**RNN**）来实现这种自回归能力。在这个架构中，先前的输出被用作输入，以计算当前的隐藏状态。此外，RNN 可以处理任意长度的输入，这对于迭代生成图非常重要。然而，这种计算比前馈网络要慢，因为必须处理整个序列才能获得最终输出。最流行的两种
    RNN 类型是 **门控递归单元**（**GRU**）和 **长短期记忆**（**LSTM**）网络。
- en: 'Introduced in 2018 by You et al., **GraphRNN** [7] is a direct implementation
    of these techniques for deep graph generation. This architecture uses two RNNs:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**GraphRNN** 由 You 等人于 2018 年提出，[7] 是这些技术在深度图生成中的直接实现。该架构使用了两个 RNN：'
- en: A *graph-level RNN* to generate a sequence of nodes (including the initial state)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *图级 RNN* 用于生成一系列节点（包括初始状态）。
- en: An *edge-level RNN* to predict connections for each newly added node
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *边级 RNN* 用于预测每个新添加节点的连接。
- en: 'The edge-level RNN takes the hidden state of the graph-level RNN as input and
    then feeds it with its own output. This mechanism is illustrated in the following
    diagram at inference time:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 边级 RNN 将图级 RNN 的隐藏状态作为输入，然后通过自己的输出继续输入。这一机制在推理时的示意图如下：
- en: '![Figure 11.9 – GraphRNN’s architecture at inference time](img/B19153_11_009.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.9 – GraphRNN 在推理时的架构](img/B19153_11_009.jpg)'
- en: Figure 11.9 – GraphRNN’s architecture at inference time
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 – GraphRNN 在推理时的架构
- en: 'Both RNNs are actually completing an adjacency matrix: each new node created
    by the graph-level RNN adds a row and a column, which are filled with zeros and
    ones by the edge-level RNN. In summary, GraphRNN performs the following steps:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个 RNN 实际上在完成一个邻接矩阵：每个由图级 RNN 创建的新节点都会添加一行和一列，并且这些行列由边级 RNN 填充为零和一。总的来说，GraphRNN
    执行以下步骤：
- en: '*Add new node*: The graph-level RNN initializes the graph and its output if
    fed to the edge-level RNN.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*添加新节点*：图级 RNN 初始化图形，并将其输出传递给边级 RNN。'
- en: '*Add new connections*: The edge-level RNN predicts if the new node is connected
    to each of the previous nodes.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*添加新连接*：边级 RNN 预测新节点是否与每个先前的节点相连接。'
- en: '*Stop graph generation*: The two first steps are repeated until the edge-level
    RNN outputs an EOS token, marking the end of the process.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*停止图生成*：前两个步骤会重复进行，直到边级 RNN 输出 EOS 令牌，标志着过程的结束。'
- en: The GraphRNN can learn different types of graphs (grids, social networks, proteins,
    and so on) and completely outperform traditional techniques. It is an architecture
    of choice to imitate given graphs that should be preferred to GraphVAE.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: GraphRNN 可以学习不同类型的图（如网格、社交网络、蛋白质等），并且在性能上完全超越传统技术。它是模仿给定图的首选架构，应优先于 GraphVAE。
- en: Generative adversarial networks
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: Like VAEs, GANs are a well-known generative model in **ML**. In this framework,
    two neural networks compete in a zero-sum game with two different goals. The first
    neural network is a generator that creates new data, and the second one is a discriminator
    that classifies each sample as real (from the training set) or fake (made by the
    generator).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与 VAE 类似，GAN 是 **机器学习（ML）** 中一种著名的生成模型。在这个框架中，两个人工神经网络在零和博弈中相互竞争，目标各不相同。第一个神经网络是生成器，用于生成新数据，第二个神经网络是判别器，用于将每个样本分类为真实的（来自训练集）或伪造的（由生成器生成）。
- en: Over the years, two main improvements to the original architecture have been
    proposed. The first one is called the **Wasserstein GAN** (**WGAN**). It improves
    learning stability by minimizing the Wasserstein distance (or Earth Mover’s distance)
    between two probability distributions. This variant is further refined by introducing
    a gradient penalty instead of the original gradient clipping scheme.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，针对原始架构提出了两项主要改进。第一项被称为 **Wasserstein GAN**（**WGAN**）。它通过最小化两个概率分布之间的 Wasserstein
    距离（或地球搬运者距离）来提高学习稳定性。该变种通过引入梯度惩罚，代替了原来的梯度裁剪方案，从而进一步得到改进。
- en: Multiple works applied this framework to deep graph generation. Like previous
    techniques, GANs can imitate graphs or generate networks that optimize certain
    constraints. The latter option is handy in applications such as finding new chemical
    compounds with specific properties. This problem is exceptionally vast (over ![](img/Formula_B19153_11_049.png)
    possible combinations) and complex due to its discrete nature.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 多项研究将这个框架应用于深度图生成。像之前的技术一样，GANs可以模仿图形或生成优化某些约束的网络。后者选项在诸如发现具有特定属性的新化学化合物等应用中非常有用。这个问题异常庞大（超过
    ![](img/Formula_B19153_11_049.png) 种可能的组合）且复杂，原因在于其离散性质。
- en: 'Proposed by De Cao and Kipf in 2018 [8], the **molecular GAN** (**MolGAN**)
    is a popular solution to this problem. It combines a WGAN with a gradient penalty
    that directly processes graph-structured data and an RL objective to generate
    molecules with desired chemical properties. This RL objective is based on the
    **Deep Deterministic Policy Gradient** (**DDPG**) algorithm, an off-policy actor-critic
    model that uses deterministic policy gradients. MolGAN’s architecture is summarized
    in the following diagram:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**分子GAN**（**MolGAN**）由De Cao和Kipf在2018年提出[8]，是解决这一问题的一个流行方法。它结合了WGAN和带有梯度惩罚的网络，直接处理图结构数据，并且通过RL目标生成具有所需化学属性的分子。这个RL目标基于**深度确定性策略梯度**（**DDPG**）算法，一种使用确定性策略梯度的离策略演员-评论员模型。MolGAN的架构总结如下图：'
- en: '![Figure 11.10 – MolGAN’s architecture at inference time](img/B19153_11_010.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图11.10 – MolGAN推理时的架构](img/B19153_11_010.jpg)'
- en: Figure 11.10 – MolGAN’s architecture at inference time
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10 – MolGAN推理时的架构
- en: 'This framework is divided into three main components:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个框架分为三个主要组件：
- en: The **generator** is an MLP that outputs a node matrix ![](img/Formula_B19153_11_050.png)
    containing the atom types and an adjacency matrix ![](img/Formula_B19153_11_051.png),
    which is actually a tensor containing both the edges and bond types. The generator
    is trained using a linear combination of the WGAN and RL loss. We translate these
    dense representations into sparse objects (![](img/Formula_B19153_11_052.png)
    and ![](img/Formula_B19153_11_053.png)) via categorical sampling.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成器**是一个多层感知器（MLP），它输出一个节点矩阵 ![](img/Formula_B19153_11_050.png)，包含原子类型和一个邻接矩阵
    ![](img/Formula_B19153_11_051.png)，该矩阵实际上是一个张量，包含了边和键类型。生成器通过WGAN和RL损失的线性组合进行训练。我们通过类别采样将这些密集表示转换为稀疏对象（![](img/Formula_B19153_11_052.png)
    和 ![](img/Formula_B19153_11_053.png)）。'
- en: The **discriminator** receives graphs from the generator and the dataset and
    learns to distinguish them. It is solely trained using the WGAN loss.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**判别器**接收来自生成器和数据集的图，并学习区分它们。它仅通过WGAN损失进行训练。'
- en: The **reward network** scores each graph. It is trained using the MSE loss based
    on the real score provided by an external system (RDKit in this case).
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励网络**为每个图打分。它通过基于外部系统（在此为RDKit）提供的真实评分，使用均方误差（MSE）损失进行训练。'
- en: 'The discriminator and the reward network use the GNN mode: the Relational-GCN,
    a GCN variant that supports multiple edge types. After several layers of graph
    convolutions, node embeddings are aggregated into a graph-level vector output:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器和奖励网络使用GNN模式：关系图卷积网络（Relational-GCN），一种支持多种边类型的GCN变种。经过几层图卷积后，节点嵌入被聚合成一个图级别的向量输出：
- en: '![](img/Formula_B19153_11_054.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B19153_11_054.jpg)'
- en: Here, ![](img/Formula_B19153_11_055.png) denotes the logistic sigmoid function,
    ![](img/Formula_B19153_11_056.png) and ![](img/Formula_B19153_11_057.png) are
    two MLPs with linear output, and ![](img/Formula_B19153_11_058.png) is the element-wise
    multiplication. A third MLP further processes this graph embedding to produce
    a value between 0 and 1 for the reward network and between ![](img/Formula_B19153_11_059.png)
    and ![](img/Formula_B19153_11_060.png) for the discriminator.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B19153_11_055.png)表示逻辑sigmoid函数，![](img/Formula_B19153_11_056.png)
    和 ![](img/Formula_B19153_11_057.png) 是两个具有线性输出的MLP，![](img/Formula_B19153_11_058.png)是逐元素乘法。第三个MLP进一步处理这个图嵌入，生成一个介于0和1之间的值用于奖励网络，和一个介于
    ![](img/Formula_B19153_11_059.png) 和 ![](img/Formula_B19153_11_060.png) 之间的值用于判别器。
- en: MolGAN produces valid chemical compounds that optimize properties such as drug
    likeliness, synthesizability, and solubility. We will implement this architecture
    in the next section to generate new molecules.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: MolGAN生成有效的化学化合物，优化药物相似性、可合成性和溶解性等属性。我们将在下一节中实现这个架构，以生成新分子。
- en: Generating molecules with MolGAN
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MolGAN生成分子
- en: 'Deep graph generation is not well covered by PyTorch Geometric. Drug discovery
    is the main application of this subfield, which is why generative models can be
    found in specialized libraries. More specifically, there are two popular Python
    libraries for ML-based drug discovery: `DeepChem` and `torchdrug`. In this section,
    we will use DeepChem as it is more mature and directly implements MolGAN.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图生成在PyTorch Geometric中没有得到充分覆盖。药物发现是该子领域的主要应用，这也是为什么生成模型通常会出现在专门的库中。更具体地说，有两个流行的Python库用于基于机器学习的药物发现：`DeepChem`和`torchdrug`。在这一节中，我们将使用DeepChem，因为它更为成熟并且直接实现了MolGAN。
- en: 'Let’s see how we can use it with `DeepChem` and `tensorflow`. The following
    procedure is based on DeepChem’s example:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用`DeepChem`和`tensorflow`。以下过程基于DeepChem的示例：
- en: 'We install `DeepChem` ([https://deepchem.io](https://deepchem.io)), which requires
    the following libraries: `tensorflow`, `joblib`, `NumPy`, `pandas`, `scikit-learn`,
    `SciPy`, and `rdkit`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们安装`DeepChem`（[https://deepchem.io](https://deepchem.io)），它需要以下库：`tensorflow`、`joblib`、`NumPy`、`pandas`、`scikit-learn`、`SciPy`和`rdkit`：
- en: '[PRE7]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we import the required packages:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们导入所需的包：
- en: '[PRE8]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We download the `tox21` (*Toxicology in the 21st Century*) dataset, which comprises
    over 6,000 chemical compounds, to analyze their toxicity. We only need their **simplified
    molecular-input line-entry system** (**SMILES**) representations in this example:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们下载`tox21`（*21世纪毒理学*）数据集，该数据集包含超过6000种化学化合物，用于分析它们的毒性。在这个示例中，我们只需要它们的**简化分子输入线条表示系统**（**SMILES**）表示：
- en: '[PRE9]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is an output of these `smiles` strings:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里是这些`smiles`字符串的输出：
- en: '[PRE10]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We only consider molecules with a maximum number of 15 atoms. We filter our
    dataset and create a `featurizer` to convert the `smiles` strings into input features:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只考虑最多含有15个原子的分子。我们过滤数据集并创建一个`featurizer`，将`smiles`字符串转换为输入特征：
- en: '[PRE11]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We manually loop through our dataset to convert the `smiles` strings:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们手动遍历数据集以转换`smiles`字符串：
- en: '[PRE12]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We remove invalid molecules from the dataset:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从数据集中移除无效的分子：
- en: '[PRE13]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we create the `MolGAN` model. It will be trained with a learning rate
    that has an exponential delay schedule:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建`MolGAN`模型。它将以具有指数衰减调度的学习率进行训练：
- en: '[PRE14]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We create the dataset to feed to `MolGAN` in DeepChem’s format:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建数据集并以DeepChem的格式提供给`MolGAN`：
- en: '[PRE15]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`MolGAN` uses batch training, which is why we need to define an iterable as
    follows:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MolGAN`使用批量训练，这就是我们需要定义一个可迭代对象的原因，如下所示：'
- en: '[PRE16]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We train the model for `25` epochs:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们训练模型`25`个周期：
- en: '[PRE17]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We generate `1000` molecules:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们生成`1000`个分子：
- en: '[PRE18]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we check whether these molecules are valid or not:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们检查这些分子是否有效：
- en: '[PRE19]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We compare them to see how many molecules are unique:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将它们进行比较，看看有多少个分子是独特的：
- en: '[PRE20]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We print the generated molecules in a grid:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将生成的分子打印在一个网格中：
- en: '[PRE21]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Figure 11.11 – Molecules generated with MolGAN](img/B19153_11_011.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图11.11 – 使用MolGAN生成的分子](img/B19153_11_011.jpg)'
- en: Figure 11.11 – Molecules generated with MolGAN
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11 – 使用MolGAN生成的分子
- en: Despite the GAN’s improvements, this training process is quite unstable and
    can fail to produce any meaningful result. The code we presented is sensitive
    to hyperparameter changes and does not generalize well to other datasets, including
    the `QM9` dataset used in the original paper.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GAN有了改进，这个训练过程仍然相当不稳定，并且可能无法产生任何有意义的结果。我们展示的代码对超参数的变化非常敏感，并且不能很好地泛化到其他数据集，包括原论文中使用的`QM9`数据集。
- en: Nonetheless, MolGAN’s concept of mixing RL and GANs can be employed beyond drug
    discovery to optimize any type of graph, such as computer networks, recommender
    systems, and so on.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，MolGAN将强化学习（RL）和生成对抗网络（GAN）的概念结合的方式，不仅限于药物发现，还可以应用于优化任何类型的图结构，如计算机网络、推荐系统等。
- en: Summary
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we saw different techniques to generate graphs. First, we
    explored traditional methods based on probabilities with interesting mathematical
    properties. However, due to their lack of expressiveness, we switched to GNN-based
    techniques that are much more flexible. We covered three families of deep generative
    models: VAE-based, autoregressive, and GAN-based methods. We introduced a model
    from each family to understand how they work in real life.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了生成图结构的不同技术。首先，我们探索了基于概率的传统方法，这些方法具有有趣的数学特性。然而，由于它们表达能力的不足，我们转向了基于图神经网络（GNN）的技术，这些技术更为灵活。我们涵盖了三种类型的深度生成模型：基于变分自编码器（VAE）、自回归和基于GAN的方法。我们从每种方法中引入了一个模型，以了解它们在实际中的工作原理。
- en: Finally, we implemented a GAN-based model that combines a generator, a discriminator,
    and a reward network from RL. Instead of simply imitating graphs seen during training,
    this architecture can also optimize desired properties such as solubility. We
    used DeepChem and TensorFlow to create 24 unique and valid molecules. Nowadays,
    this pipeline is common in the drug discovery industry, where ML can drastically
    speed up drug development.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实现了一个基于 GAN 的模型，结合了生成器、判别器和来自强化学习的奖励网络。这个架构不仅仅是模仿训练过程中看到的图形，它还能够优化如溶解度等期望的属性。我们使用
    DeepChem 和 TensorFlow 创建了 24 个独特且有效的分子。如今，这种流程在药物发现行业中已经非常普遍，机器学习能够显著加快药物开发进程。
- en: In [*Chapter 12*](B19153_12.xhtml#_idTextAnchor144), *Handling Heterogeneous
    Graphs*, we will explore a new kind of graph that we previously encountered in
    recommender systems and molecules. These heterogeneous graphs contain multiple
    types of nodes and/or links, which requires specific processing. They are more
    general than the regular graphs we talked about and particularly useful in applications
    such as knowledge graphs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 12 章*](B19153_12.xhtml#_idTextAnchor144)，*处理异构图* 中，我们将探讨一种新型图，它之前曾出现在推荐系统和分子中。这些异构图包含多种类型的节点和/或链接，需要特定的处理方式。它们比我们之前讨论的常规图更具通用性，特别在知识图谱等应用中非常有用。
- en: Further reading
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[1] P. Erdös and A. Rényi. *On random graphs I*, Publicationes Mathematicae
    Debrecen, vol. 6, p. 290, 1959\. Available at [https://snap.stanford.edu/class/cs224w-readings/erdos59random.pdf](https://snap.stanford.edu/class/cs224w-readings/erdos59random.pdf).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] P. Erdös 和 A. Rényi. *论随机图 I*，Publicationes Mathematicae Debrecen，第 6 卷，第
    290 页，1959。可在 [https://snap.stanford.edu/class/cs224w-readings/erdos59random.pdf](https://snap.stanford.edu/class/cs224w-readings/erdos59random.pdf)
    获取。'
- en: '[2] E. N. Gilbert, *Random Graphs*, The Annals of Mathematical Statistics,
    vol. 30, no. 4, pp. 1141–1144, 1959, DOI: 10.1214/aoms/1177706098\. Available
    at: [https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-30/issue-4/Random-Graphs/10.1214/aoms/1177706098.full](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-30/issue-4/Random-Graphs/10.1214/aoms/1177706098.full).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] E. N. Gilbert, *随机图*，The Annals of Mathematical Statistics，第 30 卷，第 4 期，第
    1141–1144 页，1959，DOI: 10.1214/aoms/1177706098。可在 [https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-30/issue-4/Random-Graphs/10.1214/aoms/1177706098.full](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-30/issue-4/Random-Graphs/10.1214/aoms/1177706098.full)
    获取。'
- en: '[3] Duncan J. Watts and Steven H. Strogatz. *Collective dynamics of small-world
    networks*, Nature, 393, pp. 440–442, 1998\. Available at [http://snap.stanford.edu/class/cs224w-readings/watts98smallworld.pdf](http://snap.stanford.edu/class/cs224w-readings/watts98smallworld.pdf).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Duncan J. Watts 和 Steven H. Strogatz. *小世界网络的集体动力学*，Nature，393，第 440–442
    页，1998。可在 [http://snap.stanford.edu/class/cs224w-readings/watts98smallworld.pdf](http://snap.stanford.edu/class/cs224w-readings/watts98smallworld.pdf)
    获取。'
- en: '[4] M. Simonovsky and N. Komodakis. *GraphVAE: Towards Generation of Small
    Graphs Using Variational Autoencoders* CoRR, vol. abs/1802.03480, 2018, [Online].
    Available at [http://arxiv.org/abs/1802.03480](http://arxiv.org/abs/1802.03480).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] M. Simonovsky 和 N. Komodakis. *GraphVAE: 利用变分自编码器生成小型图*，CoRR，第 abs/1802.03480
    卷，2018，[在线]. 可在 [http://arxiv.org/abs/1802.03480](http://arxiv.org/abs/1802.03480)
    获取。'
- en: '[5] Q. Liu, M. Allamanis, M. Brockschmidt, and A. L. Gaunt. *Constrained Graph
    Variational Autoencoders for Molecule Design*. arXiv, 2018\. DOI: 10.48550/ARXIV.1805.09076\.
    Available at [https://arxiv.org/abs/1805.09076](https://arxiv.org/abs/1805.09076).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Q. Liu, M. Allamanis, M. Brockschmidt 和 A. L. Gaunt. *用于分子设计的约束图变分自编码器*，arXiv，2018。DOI:
    10.48550/ARXIV.1805.09076。可在 [https://arxiv.org/abs/1805.09076](https://arxiv.org/abs/1805.09076)
    获取。'
- en: '[6] T. Ma, J. Chen, and C. Xiao, Constrained Generation of Semantically Valid
    Graphs via Regularizing Variational Autoencoders. arXiv, 2018\. DOI: 10.48550/ARXIV.1809.02630\.
    Available at [https://arxiv.org/abs/1809.02630](https://arxiv.org/abs/1809.02630).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] T. Ma, J. Chen 和 C. Xiao, 通过正则化变分自编码器约束生成语义有效的图。arXiv，2018。DOI: 10.48550/ARXIV.1809.02630。可在
    [https://arxiv.org/abs/1809.02630](https://arxiv.org/abs/1809.02630) 获取。'
- en: '[7] J. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec. *GraphRNN: Generating
    Realistic Graphs with Deep Auto-regressive Models*. arXiv, 2018\. DOI: 10.48550/ARXIV.1802.08773\.
    Available at [https://arxiv.org/abs/1802.08773](https://arxiv.org/abs/1802.08773).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] J. You, R. Ying, X. Ren, W. L. Hamilton 和 J. Leskovec. *GraphRNN: 使用深度自回归模型生成真实图形*，arXiv，2018。DOI:
    10.48550/ARXIV.1802.08773。可在 [https://arxiv.org/abs/1802.08773](https://arxiv.org/abs/1802.08773)
    获取。'
- en: '[8] N. De Cao and T. Kipf. *MolGAN: An implicit generative model for small
    molecular graphs*. arXiv, 2018\. DOI: 10.48550/ARXIV.1805.11973\. Available at
    [https://arxiv.org/abs/1805.11973](https://arxiv.org/abs/1805.11973).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] N. De Cao 和 T. Kipf. *MolGAN：一种用于小分子图的隐式生成模型*。arXiv，2018年。DOI：10.48550/ARXIV.1805.11973。可通过[https://arxiv.org/abs/1805.11973](https://arxiv.org/abs/1805.11973)获取。'
