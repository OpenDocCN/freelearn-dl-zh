- en: 'Chapter 1:'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第1章：
- en: Overview of TensorFlow Enterprise
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow Enterprise 概览
- en: In this introductory chapter, you will learn how to set up and run TensorFlow
    Enterprise in a **Google Cloud Platform** (**GCP**) environment. This will enable
    you to get some initial hands-on experience of how TensorFlow Enterprise integrates
    with other services in GCP. One of the most important improvements in TensorFlow
    Enterprise is the integration with the data storage options in Google Cloud, such
    as Google Cloud Storage and BigQuery.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章节的介绍中，您将学习如何在**Google Cloud Platform**（**GCP**）环境中设置和运行 TensorFlow Enterprise。这将帮助您获得一些关于
    TensorFlow Enterprise 如何与 GCP 中的其他服务集成的初步实践经验。TensorFlow Enterprise 最重要的改进之一是与
    Google Cloud 中的数据存储选项（如 Google Cloud Storage 和 BigQuery）的集成。
- en: This chapter starts by covering how to complete a one-time setup for the cloud
    environment and enable the necessary cloud service APIs. Then we will see how
    easy it is to work with these data storage systems at scale.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先介绍如何完成云环境的一次性设置并启用必要的云服务 API。然后我们将看到如何在大规模下轻松地与这些数据存储系统进行协作。
- en: 'In this chapter, we''ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Understanding TensorFlow Enterprise
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 TensorFlow Enterprise
- en: Configuring cloud environments for TensorFlow Enterprise
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 TensorFlow Enterprise 的云环境
- en: Accessing the data sources
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问数据源
- en: Understanding TensorFlow Enterprise
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 TensorFlow Enterprise
- en: '**TensorFlow** has become an ecosystem consisting of many valuable assets.
    At the core of its popularity and versatility is a comprehensive machine learning
    library and model templates that evolve quickly with new features and capabilities.
    This popularity comes at a cost, and that cost is expressed as complexity, intricate
    dependencies, and API updates or deprecation timelines that can easily break the
    models and workflow that were laboriously built not too long ago. It is one thing
    to learn and use the latest improvement in your code as you build a model to experiment
    with your ideas and hypotheses, but it is quite another if your job is to build
    a model for long-term production use, maintenance, and support.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow** 已经成为一个包含众多有价值资产的生态系统。其受欢迎和多功能的核心在于一个全面的机器学习库和随着新特性和能力迅速发展的模型模板。受欢迎的背后是代价，这个代价表现为复杂性、复杂的依赖关系以及
    API 更新或弃用时间表，这些都可能轻易地破坏那些不久前辛苦建立的模型和工作流。学习并在代码中使用最新改进以构建模型来实验您的想法和假设是一回事，但如果您的工作是为长期生产使用、维护和支持构建模型，那又是另外一回事。'
- en: Another problem associated with early TensorFlow in general concerned its code
    debugging process. In TensorFlow 1, lazy execution makes it rather tricky to test
    or debug your code because the code is not executed unless it is wrapped in a
    *session*, AKA a graph. Starting with TensorFlow 2, eager execution finally becomes
    a first-class citizen. Also, another welcome addition to TensorFlow 2 is the adoption
    of the Keras high-level API. This makes it much easier to code, experiment with,
    and maintain your model. It also improves the readability of your code and its
    training flow.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 与早期的 TensorFlow 相关的另一个问题是代码调试过程。在 TensorFlow 1 中，惰性执行使得测试或调试代码变得相当棘手，因为代码除非被包装在一个*会话*（即图形）中，否则不会执行。从
    TensorFlow 2 开始，急切执行（eager execution）终于成为了一个一等公民。另外，TensorFlow 2 的另一个受欢迎的新增功能是采用了
    Keras 高级 API。这使得编码、实验和维护模型变得更加容易，同时也提高了代码的可读性及训练流程。
- en: 'For enterprise adoption, there are typically these three major challenges that
    are of concern for stakeholders:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业采用来说，通常有三个主要的挑战需要关注：
- en: 'The first challenge is **scale**. A production-grade model has to be trained
    with large amounts of data, and often it is not practical or possible to fit into
    a single-node computer''s memory. This also can be thought of as another problem:
    how do you pass training data to the model? It seems the natural and instinctive
    way is to declare and involve the entire dataset as a Pythonic structure such
    as a **NumPy array** or a **pandas DataFrame**, as we have seen in so many open
    source examples. But if the data is too large, then it seems reasonable to use
    another way of passing data into a model instance, similar to the Python iterator.
    In fact, **TensorFlow.io** and **TensorFlow dataset libraries** are specifically
    provided to address this issue. We will see how they ingest data in batches to
    a model training process in the subsequent chapters.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个挑战是 **规模**。生产级模型必须使用大量数据进行训练，通常将其适配到单节点计算机的内存中并不现实或可能。这也可以被看作是另一个问题：如何将训练数据传递给模型？看起来，自然和本能的做法是声明并使用整个数据集作为类似
    Python 结构的 **NumPy 数组** 或 **pandas DataFrame**，就像我们在许多开源示例中看到的那样。但如果数据量过大，那么使用另一种方式将数据传入模型实例似乎是合理的，这类似于
    Python 迭代器。实际上，**TensorFlow.io** 和 **TensorFlow 数据集库** 专门为了解决这个问题。我们将在后续章节中看到它们如何将数据批量传递给模型训练过程。
- en: The second challenge that typically arises in consideration of enterprise adoption
    of TensorFlow is the **manageability** of the development environment. Backward
    compatibility is not a strength of TensorFlow, because there are historically
    very quick updates to and new releases of APIs that replace or deprecate old ones.
    This includes but is not limited to library version, API signature, and usage
    style deprecation. As you can imagine by now, this is a deal-breaker for development,
    debugging, and maintenance of the codebase; it also doesn't help with managing
    the stability and reproducibility of a production environment and its scoring
    results. It can easily become a nightmare for someone who manages and controls
    a machine learning development infrastructure and the standard practices in an
    enterprise project.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个挑战通常是在考虑企业采用 TensorFlow 时出现的 **可管理性** 问题。TensorFlow 的向后兼容性不是其强项，因为 API 会非常快速地更新和发布新版本，从而替代或淘汰旧版本。这包括但不限于库版本、API
    签名和使用风格的弃用。正如你现在可以想象的，这对于开发、调试和维护代码库来说是一个致命问题；它也不利于管理生产环境的稳定性和可复现性，以及其评分结果。这对于管理和控制机器学习开发基础设施以及企业项目中的标准实践的人来说，很容易变成一场噩梦。
- en: The third challenge is the efforts for API improvements, patch releases, and
    bug fixes. To address this, TensorFlow rolls these efforts into **long-term support**.
    Typically, for any TensorFlow release, Google's TensorFlow team is committed to
    providing these fixes for up to a year only. However, for an enterprise, this
    is too short for them to get a proper return on investment from the development
    cycle. Therefore, for enterprises' mission-critical performance, a longer commitment
    to TensorFlow releases is essential.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个挑战是 API 改进、补丁发布和 bug 修复的工作。为了解决这个问题，TensorFlow 将这些工作纳入了 **长期支持**。通常，对于任何
    TensorFlow 发布版本，Google 的 TensorFlow 团队承诺仅提供最多一年的修复。然而，对于企业来说，这对于他们从开发周期中获得适当的投资回报来说太短。因此，对于企业的关键任务性能，TensorFlow
    发布的长期承诺是至关重要的。
- en: '**TensorFlow Enterprise** was created to address these challenges. TensorFlow
    Enterprise is a special distribution of TensorFlow that is exclusively available
    through Google Cloud''s various services. TensorFlow Enterprise is available through
    the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow Enterprise** 是为了解决这些挑战而创建的。TensorFlow Enterprise 是 TensorFlow
    的一个特殊发行版，仅通过 Google Cloud 的各项服务提供。TensorFlow Enterprise 可以通过以下方式获得：'
- en: Google Cloud AI Notebooks
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud AI Notebooks
- en: Google Cloud AI Deep Learning VMs
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud AI 深度学习虚拟机
- en: Google Cloud AI Deep Learning Containers
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud AI 深度学习容器
- en: Partially available on Google Cloud AI Training
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部分可用于 Google Cloud AI 训练
- en: The dependencies such as drivers and library version compatibility are managed
    by Google Cloud. It also provides optimized connectivity with other Google Cloud
    services, such as Cloud Storage and the data warehouse (**BigQuery**). Currently,
    TensorFlow Enterprise supports versions 1.15, 2.1, and 2.3 of Google Cloud, and
    the GCP and TensorFlow teams will provide long-term support for up to three years,
    including bug fixes and updates.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖项，如驱动程序和库版本兼容性，由 Google Cloud 管理。它还提供与其他 Google Cloud 服务（如 Cloud Storage 和数据仓库（**BigQuery**））的优化连接。目前，TensorFlow
    Enterprise 支持 Google Cloud 的版本 1.15、2.1 和 2.3，GCP 和 TensorFlow 团队将为其提供最长三年的长期支持，包括错误修复和更新。
- en: In addition to these exclusive services and managed features, the TensorFlow
    team also takes enterprise support to another level by offering a **white-glove
    service**. This is a separate service from Google Cloud Support. In this case,
    TensorFlow engineers in Google will work with qualified enterprise customers to
    solve problems or provide bug fixes in cutting edge AI applications.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些独占服务和托管功能外，TensorFlow 团队还通过提供**白手套服务**将企业支持提升到另一个层次。这是与 Google Cloud Support
    独立的服务。在这种情况下，Google 的 TensorFlow 工程师将与合格的企业客户合作，解决问题或提供前沿 AI 应用中的错误修复。
- en: TensorFlow Enterprise packages
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Enterprise 包
- en: 'At the time of writing this book, TensorFlow Enterprise includes the following
    packages:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，TensorFlow Enterprise 包括以下几个包：
- en: '![Figure 1.1 – TensorFlow packages'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.1 – TensorFlow 包'
- en: '](img/Figure_1.1.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.1.jpg)'
- en: Figure 1.1 – TensorFlow packages
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1 – TensorFlow 包
- en: 'We will have more to say about how to launch JupyterLab in Google AI Platform
    in [*Chapter 2*](B16070_02_Final_JM_ePub.xhtml#_idTextAnchor061), *Running TensorFlow
    Enterprise in Google AI Platform*, but for now, as a demonstration, the following
    command can be executed as a CLI command in a **JupyterLab** cell. It will provide
    the version for each package in your instance so that you can be sure of version
    consistency:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第 2 章*](B16070_02_Final_JM_ePub.xhtml#_idTextAnchor061)，*在 Google AI Platform
    上运行 TensorFlow Enterprise* 中详细讲解如何启动 JupyterLab，但目前作为演示，可以在 **JupyterLab** 单元格中执行以下命令作为
    CLI 命令。它将提供您实例中每个包的版本信息，确保版本一致性：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here''s the output:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 1.2 – Google Cloud AI Platform JupyterLab environment'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.2 – Google Cloud AI Platform JupyterLab 环境'
- en: '](img/Figure_1.2.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.2.jpg)'
- en: Figure 1.2 – Google Cloud AI Platform JupyterLab environment
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – Google Cloud AI Platform JupyterLab 环境
- en: We confirmed the environment is running a TensorFlow Enterprise distribution
    and all the library versions. Knowing this would help in future debugging and
    collaboration efforts.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确认环境正在运行 TensorFlow Enterprise 发行版及所有库版本。了解这些信息有助于今后的调试和协作工作。
- en: Configuring cloud environments for TensorFlow Enterprise
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 TensorFlow Enterprise 的云环境
- en: 'Assuming you have a Google Cloud account already set up with a billing method,
    before you can start using TensorFlow Enterprise, there are some one-time setup
    steps that you must complete in Google Cloud. This setup consists of the following
    steps:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经设置了 Google Cloud 账户并配置了计费方式，在开始使用 TensorFlow Enterprise 之前，您需要在 Google
    Cloud 中完成一些一次性设置步骤。这些设置包括以下步骤：
- en: Create a cloud project and enable billing.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建云项目并启用计费。
- en: Create a Google Cloud Storage bucket.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 Google Cloud Storage 存储桶。
- en: Enable the necessary APIs.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用必要的 API。
- en: The following are some quick instructions for these steps.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这些步骤的一些快速说明。
- en: Setting up a cloud environment
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置云环境
- en: Now we are going to take a look at what we need to set up in **Google Cloud**
    before we can start using TensorFlow Enterprise. These setups are needed so that
    essential Google Cloud services can integrate seamlessly into the user tenant.
    For example, the **project ID** is used to enable resource creation credentials
    and access for different services when working with data in the TensorFlow workflow.
    And by virtue of the project ID, you can read and write data into your Cloud Storage
    and data warehouse.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看在开始使用 TensorFlow Enterprise 之前，需要在 **Google Cloud** 中进行哪些设置。这些设置是必要的，以便
    Google Cloud 的关键服务能够无缝集成到用户租户中。例如，**项目 ID** 用于启用资源创建凭证和访问权限，在 TensorFlow 工作流中处理数据时访问不同的服务。通过项目
    ID，您可以将数据读写到 Cloud Storage 和数据仓库中。
- en: Creating a project
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建项目
- en: 'This is the first step. It is needed in order to enable billing so you can
    use nearly all Google Cloud resources. Most resources will ask for a project ID.
    It also helps you organize and track your spending by knowing which services contribute
    to each workload. Let''s get started:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第一步。此步骤是为了启用计费，以便您可以使用几乎所有的谷歌云资源。大多数资源会要求提供项目 ID。它还可以帮助您通过了解哪些服务贡献于每个工作负载，来组织和跟踪您的支出。让我们开始吧：
- en: The URL for the page to create a project ID is [https://console.cloud.google.com/cloud-resource-manager](https://console.cloud.google.com/cloud-resource-manager).
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建项目 ID 的页面 URL 是[https://console.cloud.google.com/cloud-resource-manager](https://console.cloud.google.com/cloud-resource-manager)。
- en: 'After you have signed into the GCP portal, you will see a panel similar to
    this:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在您登录 GCP 门户后，您将看到一个类似于此的面板：
- en: '![Figure 1.3 – Google Cloud’s project creation panel'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.3 – 谷歌云项目创建面板'
- en: '](img/Figure_1.3.jpg)'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.3.jpg)'
- en: Figure 1.3 – Google Cloud's project creation panel
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.3 – 谷歌云项目创建面板
- en: Click on **CREATE PROJECT**:![Figure 1.4 – Creating a new project
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '点击**创建项目**: ![图 1.4 – 创建新项目'
- en: '](img/Figure_1.4.jpg)'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.4.jpg)'
- en: Figure 1.4 – Creating a new project
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.4 – 创建新项目
- en: Then provide a project name, and the platform will instantly generate a project
    ID for you. You can either accept it or edit it. It may give you a warning regarding
    how many projects you can create if you already have a few active projects:![Figure
    1.5 – Project name and project ID assignment
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后提供一个项目名称，平台会立即为您生成一个项目 ID。您可以接受它或进行编辑。如果您已经有一些活动的项目，它可能会给您一个警告，提示您可以创建的项目数量：![图
    1.5 – 项目名称和项目 ID 分配
- en: '](img/Figure_1.5.jpg)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.5.jpg)'
- en: Figure 1.5 – Project name and project ID assignment
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.5 – 项目名称和项目 ID 分配
- en: 'Make a note of the project name and project ID. Keep these handy for future
    use. Hit **CREATE** and soon you will see the platform dashboard for this project:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记下项目名称和项目 ID。将它们保存以供将来使用。点击**创建**，很快您将看到该项目的系统仪表板：
- en: '![Figure 1.6 – The main project management panel'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.6 – 主要项目管理面板'
- en: '](img/Figure_1.6.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.6.jpg)'
- en: Figure 1.6 – The main project management panel
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 – 主要项目管理面板
- en: The project ID will frequently be used when accessing data storage. It is also
    used to keep track of resource consumption and allocation in a cloud tenant.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 项目 ID 在访问数据存储时会频繁使用。它也用于跟踪云租户中的资源消耗和分配。
- en: Creating a Google Cloud Storage bucket
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建谷歌云存储桶
- en: 'A **Google** **Cloud Storage bucket** is a common way to store models and model
    assets from a model training job. Creating a storage bucket is very easy. Just
    look for **Storage** in the left panel and select **Browser**:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**谷歌** **云存储桶**是存储模型和模型资产的一种常见方式，尤其是来自模型训练工作的资产。创建存储桶非常简单。只需在左侧面板中找到**存储**并选择**浏览器**：'
- en: '![Figure 1.7 – Google Cloud’s Storage options'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.7 – 谷歌云存储选项'
- en: '](img/Figure_1.7.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.7.jpg)'
- en: Figure 1.7 – Google Cloud's Storage options
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 谷歌云存储选项
- en: 'Click **CREATE BUCKET**, and follow the instructions as indicated in the panel.
    In all cases, there are default options selected for you:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**创建存储桶**，并按照面板中的指示操作。在所有情况下，系统会为您选择默认选项：
- en: '**Choose where to store your data**. This is a trade-off between cost and availability
    as measured by performance. The default is multi-region to ensure the highest
    availability.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择数据存储位置**。这是成本和可用性之间的权衡，按性能衡量。默认设置是多区域，以确保最高的可用性。'
- en: '**Choose a default storage class for your data**. This choice lets you decide
    on costs related to retrieval operations. The default is the standard level for
    frequently accessed data.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择数据的默认存储类**。此选项让您决定与检索操作相关的成本。默认设置是频繁访问数据的标准级别。'
- en: '**Choose how to control access to objects**. This offers two different access
    levels for the bucket. The default is **object-level permissions (ACLs)** in addition
    to **bucket level permission (IAM)**.'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择如何控制对对象的访问**。这为存储桶提供了两种不同的访问级别。默认设置是**对象级别权限（ACLs）**以及**存储桶级别权限（IAM）**。'
- en: '**Advanced settings (optional)**. Here, you can choose the encryption type,
    bucket retention policy, and any bucket labels. The default is a Google-managed
    key and no retention policy nor labels:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**高级设置（可选）**。在这里，您可以选择加密类型、存储桶保留策略以及任何存储桶标签。默认设置是谷歌托管的密钥，没有保留策略和标签：'
- en: '![Figure 1.8 – Storage bucket creation process and choices'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.8 – 存储桶创建过程和选择'
- en: '](img/Figure_1.8.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.8.jpg)'
- en: Figure 1.8 – Storage bucket creation process and choices
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 – 存储桶创建过程和选择
- en: Enabling APIs
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用 API
- en: 'Now we have a project, but before we start consuming Google Cloud services,
    we need to enable some APIs. This process needs to be done only once, usually
    as the project ID is created:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个项目，但在开始使用 Google Cloud 服务之前，我们需要启用一些 API。这个过程只需要做一次，通常是在创建项目 ID 时完成：
- en: For now, let's enable the Compute Engine API for the project of your choice:![Figure
    1.9 – Google Cloud APIs and Services for the project
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们为你选择的项目启用计算引擎 API：![图 1.9 – 项目的 Google Cloud APIs 和服务
- en: '](img/Figure_1.9.jpg)'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.9.jpg)'
- en: Figure 1.9 – Google Cloud APIs and Services for the project
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.9 – 项目的 Google Cloud APIs 和服务
- en: 'Optional: Then select **ENABLE APIS AND SERVICES**.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可选：然后选择**启用 API 和服务**。
- en: 'You may do it here now, or as you go through the exercises in this book. If
    you need to use a particular cloud service for the first time, you can enable
    the API as you go along:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以现在启用，或者在本书中的练习过程中启用。如果你第一次使用某个特定的云服务，你可以在进行时启用相应的 API：
- en: '![Figure 1.10 – Enabling APIs and Services'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.10 – 启用 API 和服务'
- en: '](img/Figure_1.10.jpg)'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.10.jpg)'
- en: Figure 1.10 – Enabling APIs and Services
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.10 – 启用 API 和服务
- en: In the search box, type `Compute Engine API`:![Figure 1.11 – Enabling the Compute
    Engine API
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索框中输入 `Compute Engine API`：![图 1.11 – 启用计算引擎 API
- en: '](img/Figure_1.11.jpg)'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.11.jpg)'
- en: Figure 1.11 – Enabling the Compute Engine API
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.11 – 启用计算引擎 API
- en: 'You will see the status of the **Compute Engine API** in your project as shown
    in the following screenshot. Enable it if it''s not already enabled:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将看到你项目中**计算引擎 API**的状态，如下图所示。如果没有启用，请启用它：
- en: '![Figure 1.12 – Google Cloud Compute Engine API'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.12 – Google Cloud 计算引擎 API'
- en: '](img/Figure_1.12.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.12.jpg)'
- en: Figure 1.12 – Google Cloud Compute Engine API
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12 – Google Cloud 计算引擎 API
- en: For now, this is good enough. There are more APIs that you'll need as you go
    through the examples in this book; GCP will ask you to enable the API when relevant.
    You can do so at that time.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这已经足够了。在你完成本书中的示例时，GCP 会要求你启用更多的 API，你可以在相关时启用它们。
- en: 'If you wish, you may repeat the preceding procedure to enable several other
    APIs as well: specifically, the *BigQuery API*, *BigQuery Data Transfer API*,
    *BigQuery Connection API*, *Service Usage API*, *Cloud Storage*, and the *Storage
    Transfer API*.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，你可以重复前面的步骤来启用其他几个 API：具体来说，*BigQuery API*、*BigQuery 数据传输 API*、*BigQuery
    连接 API*、*服务使用 API*、*Cloud Storage* 和 *存储传输 API*。
- en: Next, let's take a look at how to move data in a storage bucket into a table
    inside a BigQuery data warehouse.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一下如何将存储桶中的数据移动到 BigQuery 数据仓库中的表格。
- en: Creating a data warehouse
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建数据仓库
- en: We will use a simple example of putting data stored in a Google Cloud bucket
    into a table that can be queried by BigQuery. The easiest way to do so is to use
    the BigQuery UI. Make sure it is in the right project. We will use this example
    to create a dataset that contains one table.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个简单的例子，将存储在 Google Cloud 存储桶中的数据放入一个可以通过 BigQuery 查询的表格中。最简单的方法是使用 BigQuery
    用户界面。确保它处于正确的项目中。我们将使用这个例子来创建一个包含一个表格的数据集。
- en: 'You can navigate to BigQuery by searching for it in the search bar of the GCP
    portal, as in the following screenshot:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在 GCP 门户的搜索栏中搜索 BigQuery 来导航到它，如下图所示：
- en: '![Figure 1.13 – Searching for BigQuery'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.13 – 搜索 BigQuery'
- en: '](img/Figure_1.13.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.13.jpg)'
- en: Figure 1.13 – Searching for BigQuery
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.13 – 搜索 BigQuery
- en: 'You will see **BigQuery** being suggested. Click on it and it will take you
    to the BigQuery portal:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到**BigQuery**被推荐。点击它，它会带你进入 BigQuery 门户：
- en: '![Figure 1.14 – BigQuery and the data warehouse query portal'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.14 – BigQuery 和数据仓库查询门户'
- en: '](img/Figure_1.14.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.14.jpg)'
- en: Figure 1.14 – BigQuery and the data warehouse query portal
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.14 – BigQuery 和数据仓库查询门户
- en: 'Here are the steps to create a persistent table in the BigQuery data warehouse:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是创建 BigQuery 数据仓库中持久表格的步骤：
- en: Select **Create dataset**:![Figure 1.15 – Creating a dataset for the project
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**创建数据集**：![图 1.15 – 为项目创建数据集
- en: '](img/Figure_1.15.jpg)'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.15.jpg)'
- en: Figure 1.15 – Creating a dataset for the project
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.15 – 为项目创建数据集
- en: Make sure you are in the dataset that you just created. Now click **CREATE TABLE**:![Figure
    1.16 – Creating a table for the dataset
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保你位于刚才创建的数据集中。现在点击**创建表格**：![图 1.16 – 为数据集创建表格
- en: '](img/Figure_1.16.jpg)'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.16.jpg)'
- en: Figure 1.16 – Creating a table for the dataset
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.16 – 为数据集创建表格
- en: 'In the **Source** section, under **Source**, in the **Create table from** section,
    select **Google Cloud Storage**:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 **源** 部分，在 **创建表格自** 部分，选择 **Google Cloud Storage**：
- en: '![Figure 1.17 – Populating the table by specifying a data source'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.17 – 通过指定数据源填充表格'
- en: '](img/Figure_1.17.jpg)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.17.jpg)'
- en: Figure 1.17 – Populating the table by specifying a data source
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.17 – 通过指定数据源填充表格
- en: Then it will transition to another dialog box. You may enter the name of the
    file or use the **Browse** option to find the file stored in the bucket. In this
    case, a CSV file has already been put in my Google Cloud Storage bucket. You may
    either put your own CSV file into the storage bucket, or download the one I used
    from [https://data.mendeley.com/datasets/7xwsksdpy3/1](https://data.mendeley.com/datasets/7xwsksdpy3/1).
    Also, enter the column names and datatypes as the schema:![Figure 1.18 – An example
    of populating a table using an existing CSV file stored in the bucket
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后它将过渡到另一个对话框。你可以输入文件名或使用 **浏览** 选项查找存储在存储桶中的文件。在这种情况下，我的 Google Cloud Storage
    存储桶中已经放置了一个 CSV 文件。你可以将自己的 CSV 文件上传到存储桶，或者从 [https://data.mendeley.com/datasets/7xwsksdpy3/1](https://data.mendeley.com/datasets/7xwsksdpy3/1)
    下载我使用的那个文件。另外，输入列名和数据类型作为模式：![图 1.18 – 使用存储在存储桶中的现有 CSV 文件填充表格的示例
- en: '](img/Figure_1.18.jpg)'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.18.jpg)'
- en: Figure 1.18 – An example of populating a table using an existing CSV file stored
    in the bucket
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.18 – 使用存储在存储桶中的现有 CSV 文件填充表格的示例
- en: In the **Schema** section, use **Auto-detect**, and in the **Advanced options**,
    since the first row is an array of column names, we need to tell it to skip the
    first row:![Figure 1.19 – Handling column names for the table
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **模式** 部分，使用 **自动检测**，在 **高级选项** 中，由于第一行是列名数组，我们需要告诉它跳过第一行：![图 1.19 – 处理表格的列名
- en: '](img/Figure_1.19.jpg)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.19.jpg)'
- en: Figure 1.19 – Handling column names for the table
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.19 – 处理表格的列名
- en: 'Once the table is created, you can click **QUERY TABLE** to update the SQL
    query syntax, or just enter this query:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦表格创建完成，你可以点击 **查询表格** 来更新 SQL 查询语法，或者直接输入这个查询：
- en: '[PRE1]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Execute the preceding query and now click on **Run**:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行前面的查询并点击 **运行**：
- en: '![Figure 1.20 – Running a query to examine the table'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.20 – 运行查询以检查表格'
- en: '](img/Figure_1.20.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.20.jpg)'
- en: Figure 1.20 – Running a query to examine the table
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.20 – 运行查询以检查表格
- en: There are many different data source types, as well as many different ways to
    create a data warehouse from raw data. This is just a simple example for structured
    data. For more information on other data sources and types, please refer to the
    BigQuery documentation at [https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#console](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#console).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的数据源类型，也有许多不同的方式将原始数据创建为数据仓库。这只是结构化数据的简单示例。如需有关其他数据源和类型的更多信息，请参考 BigQuery
    文档：[https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#console](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#console)。
- en: Now you have learned how to create a persistent table in your BigQuery data
    warehouse using the raw data in your storage bucket.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学会了如何使用存储桶中的原始数据，在 BigQuery 数据仓库中创建持久表格。
- en: We used a CSV file as an example and added it to BigQuery as a table. In the
    next section, we are going to see how to connect TensorFlow to our data stored
    in BigQuery and the Cloud Storage bucket. Now we are ready to launch an instance
    of TensorFlow Enterprise running on AI Platform.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个 CSV 文件作为示例，并将其作为表格添加到 BigQuery。在接下来的章节中，我们将看到如何将 TensorFlow 连接到存储在 BigQuery
    和 Cloud Storage 存储桶中的数据。现在我们准备启动一个在 AI Platform 上运行的 TensorFlow Enterprise 实例。
- en: Using TensorFlow Enterprise in AI Platform
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 AI Platform 中使用 TensorFlow Enterprise
- en: 'In this section, we are going to see firsthand how easy it is to access data
    stored in one of the Google Cloud Storage options, such as a storage bucket or
    BigQuery. To do so, we need to configure an environment to execute some example
    TensorFlow API code and command-line tools in this section. The easiest way to
    use TensorFlow Enterprise is through the AI Platform Notebook in Google Cloud:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将亲自体验如何轻松访问存储在 Google Cloud Storage 选项中的数据，例如存储桶或 BigQuery。为此，我们需要配置一个环境来执行一些示例
    TensorFlow API 代码和命令行工具。本节中使用 TensorFlow Enterprise 的最简单方法是通过 Google Cloud 中的
    AI Platform Notebook：
- en: In the GCP portal, search for `AI Platform`.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 GCP 门户中，搜索 `AI Platform`。
- en: Then select **NEW INSTANCE**, with **TensorFlow Enterprise 2.3** and **Without
    GPUs**. Then click **OPEN JUPYTERLAB**:![Figure 1.21 – The Google Cloud AI Platform
    and instance creation
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后选择**NEW INSTANCE**，选择**TensorFlow Enterprise 2.3**并选择**Without GPUs**。然后点击**OPEN
    JUPYTERLAB**：![图1.21 – Google Cloud AI Platform与实例创建
- en: '](img/Figure_1.21.jpg)'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.21.jpg)'
- en: Figure 1.21 – The Google Cloud AI Platform and instance creation
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图1.21 – Google Cloud AI Platform与实例创建
- en: 'Click on **Python 3**, and it will provide a new notebook to execute the remainder
    of this chapter''s examples:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**Python 3**，它将提供一个新的笔记本，以执行本章余下的示例：
- en: '![Figure 1.22 – A JupyterLab environment hosted by AI Platform'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.22 – 由AI Platform托管的JupyterLab环境'
- en: '](img/Figure_1.22.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.22.jpg)'
- en: Figure 1.22 – A JupyterLab environment hosted by AI Platform
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.22 – 由AI Platform托管的JupyterLab环境
- en: An instance of TensorFlow Enterprise running on AI Platform is now ready for
    use. Next, we are going to use this platform to perform some data I/O.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个运行在AI Platform上的TensorFlow Enterprise实例现在已准备好使用。接下来，我们将使用该平台进行一些数据I/O操作。
- en: Accessing the data sources
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问数据源
- en: TensorFlow Enterprise can easily access data sources in Google Cloud Storage
    as well as BigQuery. Either of these data sources can easily host gigabytes to
    terabytes of data. Reading training data into the JupyterLab runtime at this magnitude
    of size is definitely out of question, however. Therefore, streaming data as batches
    through training is the way to handle data ingestion. The `tf.data` API is the
    way to build a data ingestion pipeline that aggregates data from files in a distributed
    system. After this step, the data object can go through transformation steps and
    evolve into a new data object for training.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Enterprise可以轻松访问Google Cloud Storage和BigQuery中的数据源。这些数据源可以轻松存储从几GB到TB级别的数据。然而，以这种规模读取训练数据到JupyterLab运行时是不可行的。因此，使用流式数据批次进行训练是处理数据导入的方式。`tf.data`
    API是构建数据导入管道的方式，能够从分布式系统中的文件中聚合数据。经过这一步后，数据对象可以通过变换步骤，演化成一个新的训练数据对象。
- en: 'In this section, we are going to learn basic coding patterns for the following
    tasks:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习以下任务的基本编码模式：
- en: Reading data from a Cloud Storage bucket
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Cloud Storage存储桶中读取数据
- en: Reading data from a BigQuery table
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从BigQuery表中读取数据
- en: Writing data into a Cloud Storage bucket
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据写入Cloud Storage存储桶
- en: Writing data into BigQuery table
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据写入BigQuery表
- en: After this, you will have a good grasp of reading and writing data to a Google
    Cloud Storage option and persisting your data or objects produced as a result
    of your TensorFlow runtime.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这之后，你将能够很好地掌握如何读取和写入Google Cloud Storage选项中的数据，并持久化你的数据或TensorFlow运行时所生成的对象。
- en: Cloud Storage Reader
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cloud Storage读取器
- en: '`tf.data`, so a `tf.data` object can easily access data in Google Cloud Storage.
    For example, the following code snippet demonstrates how to read a `tfrecord`
    dataset:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data`，因此一个`tf.data`对象可以轻松地访问Google Cloud Storage中的数据。例如，下面的代码片段演示了如何读取一个`tfrecord`数据集：'
- en: '[PRE2]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the example preceding pattern, the file stored in the bucket is serialized
    into `tfrecord`, which is a binary format of your original data. This is a very
    common way of storing and serializing large amounts of data or files in the cloud
    for TensorFlow consumption. This format enables a more efficient read for data
    being streamed over a network. We will discuss `tfrecord` in more detail in a
    future chapter.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，存储在存储桶中的文件被序列化为`tfrecord`，它是原始数据的二进制格式。这是一种非常常见的存储和序列化大量数据或文件的方式，适用于TensorFlow在云中使用。这种格式可以提高通过网络流式传输数据时的读取效率。我们将在未来的章节中更详细地讨论`tfrecord`。
- en: BigQuery Reader
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BigQuery读取器
- en: Likewise, **BigQuery Reader** is also integrated into the TensorFlow Enterprise
    environment, so training data or derived datasets stored in BigQuery can be consumed
    by TensorFlow Enterprise.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，**BigQuery读取器**也集成在TensorFlow Enterprise环境中，因此存储在BigQuery中的训练数据或衍生数据集可以被TensorFlow
    Enterprise消费。
- en: There are three commonly used methods to read a table stored in a BigQuery data
    warehouse. The first way is the `%%bigquery` *magic command*. The second way is
    using the *BigQuery API in a general Python runtime*, and the third way is to
    *use TensorFlow I/O*. Each has its advantages.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种常用的方法可以读取存储在BigQuery数据仓库中的表。第一种方法是使用`%%bigquery` *魔法命令*。第二种方法是在*常规Python运行时中使用BigQuery
    API*，第三种方法是*使用TensorFlow I/O*。每种方法都有其优点。
- en: The BigQuery magic command
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery魔法命令
- en: This method is perfect for running SQL statements directly in a JupyterLab cell.
    This is equivalent to switching the cell's command interpreter. The `%%bigquery`
    interpreter executes a standard SQL query and the results are returned as a pandas
    DataFrame.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法非常适合在 JupyterLab 单元中直接运行 SQL 语句。这等同于切换单元的命令解释器。`%%bigquery` 解释器执行标准 SQL
    查询，并将结果作为 pandas DataFrame 返回。
- en: 'The following code snippet shows how to use the `%%bigquery` interpreter and
    assign a pandas DataFrame name to the result. Each step is a JupyterLab cell:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何使用 `%%bigquery` 解释器，并为结果分配一个 pandas DataFrame 名称。每一步都是一个 JupyterLab
    单元：
- en: 'Specify a project ID. This JupyterLab cell uses a default interpreter. Therefore,
    this is a Python variable. If the BigQuery table is in the same project, then
    you don''t need to specify the project ID:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定一个项目 ID。此 JupyterLab 单元使用默认解释器。因此，这里是一个 Python 变量。如果 BigQuery 表与您当前运行的项目相同，那么无需指定项目
    ID：
- en: '[PRE7]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Invoke the `%%bigquery` magic command, and assign the project ID and a DataFrame
    name to hold the result:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `%%bigquery` 魔法命令，并为结果指定项目 ID 和 DataFrame 名称：
- en: '[PRE8]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If the table is in the same project as you currently running from, you don't
    need --project argument.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果表与您当前运行的项目相同，则无需 --project 参数。
- en: 'Verify the result is a pandas DataFrame:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证结果是一个 pandas DataFrame：
- en: '[PRE9]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Show the DataFrame:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示 DataFrame：
- en: '[PRE10]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The complete code snippet for this example is as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的完整代码片段如下：
- en: '![Figure 1.23 – Code snippet for BigQuery and Python runtime integration'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.23 – BigQuery 与 Python 运行时集成的代码片段'
- en: '](img/Figure_1.23.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.23.jpg)'
- en: '[PRE11]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here are the key takeaways:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关键要点：
- en: It is required to have a project ID in order to use the BigQuery API.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 必须提供项目 ID 才能使用 BigQuery API。
- en: You may pass a Python variable such as the project ID as a value into the cell
    that runs the `%%bigquery` interpreter using the `$` prefix.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以将像项目 ID 这样的 Python 变量作为值传递给运行 `%%bigquery` 解释器的单元，使用 `$` 前缀。
- en: In order for the result to be reusable further by the Python preprocessing functionality
    or for TensorFlow consumption, you need to specify a name for the DataFrame that
    will hold the query result.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了便于 Python 预处理功能或 TensorFlow 使用，您需要为保存查询结果的 DataFrame 指定一个名称。
- en: The Python BigQuery API
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Python BigQuery API
- en: The second method by which we can invoke the BigQuery API is through Google
    Cloud's BigQuery client. This will give us direct access to the data, execute
    the query, and allow us to receive the results right away. This method does not
    require the user to know about the table schema. In fact, it simply wraps a SQL
    statement inside the BigQuery client instantiated through a library call.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 Google Cloud 的 BigQuery 客户端调用 BigQuery API。这将直接访问数据，执行查询，并立即返回结果。此方法不需要用户了解表的架构。实际上，它仅仅是通过库调用将
    SQL 语句包装在 BigQuery 客户端中。
- en: 'This code snippet demonstrates how to invoke the BigQuery API and use it to
    return the results in a pandas DataFrame:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码片段演示了如何调用 BigQuery API，并使用它将结果返回为 pandas DataFrame：
- en: '[PRE12]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![Figure 1.24 – Code output'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.24 – 代码输出'
- en: '](img/Figure_1.24.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.24.jpg)'
- en: Figure 1.24 – Code output
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.24 – 代码输出
- en: 'Let''s take a closer look at the preceding code:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看上述代码：
- en: An import of the BigQuery library is required to create a BigQuery client.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要导入 BigQuery 库来创建 BigQuery 客户端。
- en: The project ID is required for using this API to create a BigQuery client.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用该 API 创建 BigQuery 客户端时需要提供项目 ID。
- en: This client wraps a SQL statement and executes it.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该客户端包装了一个 SQL 语句并执行它。
- en: The returned data can be easily converted to a pandas DataFrame right away.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回的数据可以立即轻松地转换为 pandas DataFrame。
- en: 'The pandas DataFrame rendition of the BigQuery table has the following columns:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 表的 pandas DataFrame 版本包含以下列：
- en: '![Figure 1.25 – The pandas DataFrame rendition of the BigQuery table'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.25 – BigQuery 表格的 pandas DataFrame 版本'
- en: '](img/Figure_1.25.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.25.jpg)'
- en: Figure 1.25 – The pandas DataFrame rendition of the BigQuery table
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.25 – BigQuery 表的 pandas DataFrame 版本
- en: This is ready for further consumption. It is now a pandas DataFrame that occupies
    memory space in your Python runtime.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以进一步使用。它现在是一个 pandas DataFrame，占用了您 Python 运行时的内存空间。
- en: This method is very straightforward, as it can help you explore the data schema
    and do simple aggregation and filtering, and since it is basically a SQL statement
    wrapper, it is very easy to just get the data out of the warehouse and start using
    it. You didn't have to know much about the table schema to do this.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法非常直接，因为它可以帮助你探索数据架构并进行简单的聚合和过滤，而且由于它本质上是一个 SQL 语句包装器，你可以非常轻松地将数据从仓库中取出并开始使用。你不需要了解表格架构就能做到这一点。
- en: However, the problem with this approach is when the table is big enough to overflow
    your memory. TensorFlow I/O can help solve this problem.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法的问题在于当表格足够大以至于超出了内存容量时。TensorFlow I/O 可以帮助解决这个问题。
- en: TensorFlow I/O
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow I/O
- en: For TensorFlow consumption of BigQuery data, it is better if we use TensorFlow
    I/O to invoke the BigQuery API. This is because TensorFlow I/O will provide us
    with a dataset object that represents the query results, rather than the entire
    results, as in the previous method. A dataset object is the means to stream training
    data for a model during training. Therefore not all training data has to be in
    memory at once. This complements mini-batch training, which is arguably the most
    common implementation of gradient descent optimization used in deep learning.
    However, this is a bit more complicated than the previous method. It requires
    you to know the schema of the table. This example uses a public dataset hosted
    by Google Cloud.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 TensorFlow 使用 BigQuery 数据，最好使用 TensorFlow I/O 来调用 BigQuery API。因为 TensorFlow
    I/O 会提供一个代表查询结果的数据集对象，而不是像之前的方法那样提供整个结果。数据集对象是训练期间流式传输训练数据的方式。因此，不必一次性将所有训练数据加载到内存中。这与小批量训练互为补充，而小批量训练无疑是深度学习中最常用的梯度下降优化实现。然而，这比前面的方法稍微复杂一些，它要求你了解表格的架构。本示例使用的是
    Google Cloud 托管的公共数据集。
- en: 'We need to start with the columns of our interest from the table. We can use
    the previous method to examine the column names and datatypes, and create a session
    definition:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要从表格中感兴趣的列开始。我们可以使用之前的方法来检查列名和数据类型，并创建会话定义：
- en: 'Load the required libraries and set up the variables as follows:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式加载所需的库并设置变量：
- en: '[PRE28]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Instantiate a BigQuery client and specify the batch size:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个 BigQuery 客户端并指定批次大小：
- en: '[PRE29]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Use the client to create a read session and specify the columns and datatypes
    of interest. Notice that when using the BigQuery client, you need to know the
    correct column names and their respective datatypes:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用客户端创建一个读取会话并指定感兴趣的列和数据类型。注意，在使用 BigQuery 客户端时，你需要知道正确的列名及其相应的数据类型：
- en: '[PRE30]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we can use the session object created to execute a read operation:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用创建的会话对象来执行读取操作：
- en: '[PRE31]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let''s take a look at the dataset with `type()`:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们用`type()`来查看数据集：
- en: '[PRE32]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here''s the output:'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 1.26 – Output'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 1.26 – 输出'
- en: '](img/Figure_1.26.jpg)'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.26.jpg)'
- en: Figure 1.26 – Output
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.26 – 输出
- en: 'In order to actually see the data, we need to convert the dataset ops to a
    Python iterator and use `next()` to see the content of the first batch:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了实际查看数据，我们需要将数据集操作转换为 Python 迭代器，并使用`next()`查看第一批次的内容：
- en: '[PRE33]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output of the preceding command shows it is organized as an ordered dictionary,
    where the keys are column names and the values are Tensors:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令输出显示它被组织为一个有序字典，其中键是列名，值是张量：
- en: '![Figure 1.27 – Raw data as an iterator'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.27 – 原始数据作为迭代器'
- en: '](img/Figure_1.27.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.27.jpg)'
- en: Figure 1.27 – Raw data as an iterator
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.27 – 原始数据作为迭代器
- en: 'Here are the key takeaways:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关键的要点：
- en: TensorFlow I/O's BigQuery Client requires setting up a read session, which consists
    of column names from your table of interest.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow I/O 的 BigQuery 客户端需要设置读取会话，读取会话由你感兴趣的表格的列名组成。
- en: This client then executes a read operation that also includes data batching.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后这个客户端执行一个读取操作，包含数据批次处理。
- en: The output of the read operation is a TensorFlow ops.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取操作的输出是 TensorFlow 操作。
- en: This ops may be converted to a Python iterator, so it can output the actual
    data read by the ops.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些操作可能会被转换为 Python 迭代器，这样它就可以输出操作读取的实际数据。
- en: This improves the efficiency of memory use during training, as data is sent
    for training in batches.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这提高了训练过程中内存使用的效率，因为数据是按批次发送进行训练的。
- en: Persisting data in BigQuery
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 BigQuery 中持久化数据
- en: We have looked at how to read data stored in Google Storage solutions, such
    as Cloud Storage buckets or a BigQuery data warehouse, and how to enable the data
    for consumption by AI Platform's TensorFlow Enterprise instance running in JupyterLab.
    Now let's take a look at some ways to write data back, or persist our working
    data, into our cloud Storage.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了如何读取存储在Google Storage解决方案中的数据，例如Cloud Storage存储桶或BigQuery数据仓库，并如何使AI
    Platform的TensorFlow Enterprise实例可以在JupyterLab中使用这些数据。现在让我们来看一些将数据写回，或将工作数据持久化到云存储中的方法。
- en: 'Our first example concerns writing a file stored in JupyterLab runtime''s directory
    (in some TensorFlow Enterprise documentations, this is also referred to as a *local*
    file). The process in general is as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个示例涉及写入存储在JupyterLab运行时目录中的文件（在一些TensorFlow Enterprise文档中，这也称为*本地*文件）。一般过程如下：
- en: For convenience, execute a BigQuery SQL `read` command on a table from a public
    dataset.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了方便起见，执行一个BigQuery SQL `read`命令，从公共数据集中查询一个表。
- en: Store the result locally as a **comma-separated file** (**CSV**).
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果本地保存为**逗号分隔文件**（**CSV**）。
- en: Write the CSV file to a table in our BigQuery dataset.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将CSV文件写入我们BigQuery数据集中的一个表。
- en: 'Each step is a code cell. The following step-by-step code snippet applies to
    JupyterLab in any of the three AI platforms (AI Notebook, AI Deep Learning VM,
    and Deep Learning Container):'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 每个步骤都是一个代码单元。以下逐步代码片段适用于任何三个AI平台的JupyterLab（AI Notebook，AI Deep Learning VM和Deep
    Learning Container）：
- en: 'Designate a project ID:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定项目ID：
- en: '[PRE34]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Execute the BigQuery SQL command and assign the result to a pandas DataFrame:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行BigQuery SQL命令并将结果分配给pandas DataFrame：
- en: '[PRE35]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The BigQuery results come back as a pandas DataFrame by default. In this case,
    we designate the DataFrame name to be `mydataframe`.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BigQuery的查询结果默认返回为pandas DataFrame。在这种情况下，我们将DataFrame的名称指定为`mydataframe`。
- en: 'Write the pandas DataFrame to a CSV file in a local directory. In this case,
    we used the `/home` directory of this JupyterLab runtime:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将pandas DataFrame写入本地目录中的CSV文件。在这种情况下，我们使用了JupyterLab运行时的`/home`目录：
- en: '[PRE36]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Designate a dataset name:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定数据集名称：
- en: '[PRE37]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Use the BigQuery command-line tool to create an empty table in this project''s
    dataset. This command starts with `!bq`:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用BigQuery命令行工具在此项目的数据集中创建一个空表。该命令以`!bq`开头：
- en: '[PRE38]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This command creates a new dataset. This dataset doesn't have any tables yet.
    We are going to write a new table into this dataset in the next step.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该命令创建了一个新的数据集。这个数据集还没有任何表格。我们将在下一步将一个新表写入该数据集。
- en: 'Write the local CSV file to a new table:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将本地CSV文件写入新表：
- en: '[PRE39]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now you can navigate to the BigQuery portal, and you will find the dataset and
    the table:![Figure 1.28 – The BigQuery portal and navigation to the dataset
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在你可以导航到BigQuery门户，你会看到数据集和表格：[图 1.28 – BigQuery门户和导航到数据集
- en: '](img/Figure_1.28.jpg)'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_1.28.jpg)'
- en: Figure 1.28 – The BigQuery portal and navigation to the dataset
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 1.28 – BigQuery门户和导航到数据集
- en: In this case, we have one dataset, which contains one table.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有一个数据集，包含一个表。
- en: 'Then, execute a simple query, as follows:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，执行一个简单的查询，如下所示：
- en: '![Figure 1.29 – A query for examining the table'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.29 – 用于检查表的查询'
- en: '](img/Figure_1.29.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.29.jpg)'
- en: Figure 1.29 – A query for examining the table
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.29 – 用于检查表的查询
- en: This is a very simple query where we just want to show 1,000 randomly selected
    rows. You can now execute this query and the output will be as shown in the following
    screenshot.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的查询，我们只是想显示1,000条随机选择的行。现在您可以执行此查询，输出将如下面的截图所示。
- en: 'The following query output shows the data from the BigQuery table we just created:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 以下查询输出显示了我们刚刚创建的BigQuery表中的数据：
- en: '![Figure 1.30 – Example table output'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.30 – 示例表格输出'
- en: '](img/Figure_1.30.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.30.jpg)'
- en: Figure 1.30 – Example table output
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.30 – 示例表格输出
- en: 'Here are the key takeaways:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关键要点：
- en: Data generated during the TensorFlow workflow in the AI Platform's JupyterLab
    runtime can be seamlessly persisted as a table in BigQuery.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在AI Platform的JupyterLab运行时中生成的TensorFlow工作流数据，可以无缝地作为表持久化到BigQuery中。
- en: Persisting data in a structured format, such as a pandas DataFrame or a CSV
    file, in BigQuery can easily be done using the BigQuery command-line tool.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据以结构化格式（如pandas DataFrame或CSV文件）持久化到BigQuery中，可以轻松使用BigQuery命令行工具完成。
- en: When you need to move a data object (such as a table) between the JupyterLab
    runtime and BigQuery, use the BigQuery command-line tool with `!bq` to save time
    and effort.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你需要在JupyterLab运行时和BigQuery之间移动数据对象（如表格）时，使用BigQuery命令行工具和`!bq`可以节省时间和精力。
- en: Persisting data in a storage bucket
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在存储桶中持久化数据
- en: In the previous *Persisting data in BigQuery* section, we saw how a structured
    data source such as a CSV file or a pandas DataFrame can be persisted in a BigQuery
    dataset as a table. In this section, we are going to see how to persist working
    data such as a NumPy array. In this case, the suitable target storage is a Google
    Cloud Storage bucket.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 *在 BigQuery 中持久化数据* 部分，我们看到如何将 CSV 文件或 pandas DataFrame 等结构化数据源持久化为 BigQuery
    数据集中的表。在本节中，我们将看到如何将工作数据（如 NumPy 数组）持久化。在这种情况下，适合的目标存储是 Google Cloud Storage 存储桶。
- en: 'The workflow for this demonstration is as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本演示的工作流如下：
- en: For convenience, read a NumPy array from `tf.keras.dataset`.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了方便，从 `tf.keras.dataset` 中读取 NumPy 数组。
- en: 'Save the NumPy array as a pickle (`pkl`) file. (FYI: The pickle file format,
    while convenient and easy to use for serializing Python objects, also has its
    downsides. For one, it may be slow and creates a larger object than the original.
    Second, a pickle file may contain bugs or security risks for any process that
    opens it. It is used only for convenience here.)'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 NumPy 数组保存为 pickle (`pkl`) 文件。（仅供参考：虽然 pickle 文件格式方便且易于用于序列化 Python 对象，但也有其缺点。首先，它可能很慢，并且生成的对象比原始对象大。其次，pickle
    文件可能包含漏洞或安全风险，任何打开它的进程都有可能受到影响。这里只是为了方便使用。）
- en: Use the `!gsutil` storage command-line tool to transfer files from JupyterLab's
    `/home` directory (in some documentation, this is referred to as the *local directory*)
    to the storage bucket.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `!gsutil` 存储命令行工具将文件从 JupyterLab 的 `/home` 目录（在一些文档中，这被称为*本地目录*）传输到存储桶。
- en: Use `!gsutil` to transfer the content in the bucket back to the JupyterLab runtime.
    Since we will use Python with `!gsutil`, we need to keep the content in separate
    cells.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `!gsutil` 将存储桶中的内容传输回 JupyterLab 运行时。由于我们将使用 Python 与 `!gsutil`，因此需要将内容保持在不同的单元格中。
- en: 'Follow these steps to complete the workflow:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成工作流：
- en: 'Let''s use the IMDB dataset because it is already provided in NumPy format:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 IMDB 数据集，因为它已经以 NumPy 格式提供：
- en: '[PRE40]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '`x_train`, `y_train`, `x_test`, and `y_test` are returned as NumPy arrays.
    Let''s use `x_train` for the purposes of this demonstration. The `x_train` array
    is going to be saved as a `pkl` file in the JupyterLab runtime.'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x_train`、`y_train`、`x_test` 和 `y_test` 将作为 NumPy 数组返回。我们将使用 `x_train` 进行本演示。`x_train`
    数组将作为 `pkl` 文件保存在 JupyterLab 运行时中。'
- en: The preceding code opens the IMDB movie review dataset that is distributed as
    a part of TensorFlow. This dataset is formatted as tuples of NumPy arrays and
    separated as training and test partitions. Then we proceed to save the `x_train`
    array as a pickle file in the runtime's `/home` directory. This pickle file will
    then be persisted in a storage bucket in the next step.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码打开了作为 TensorFlow 一部分分发的 IMDB 电影评论数据集。该数据集的格式为 NumPy 数组元组，并分为训练和测试分区。接下来，我们将
    `x_train` 数组保存为运行时 `/home` 目录下的 pickle 文件。然后，在下一步中，该 pickle 文件将被持久化到存储桶中。
- en: 'Designate a name for the new storage bucket:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为新的存储桶指定一个名称：
- en: '[PRE41]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Create a new bucket with the designated name:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指定的名称创建一个新的存储桶：
- en: '[PRE42]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Use `!gsutil` to move the `pkl` file from the runtime to the storage bucket:'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 `!gsutil` 将 `pkl` 文件从运行时移动到存储桶：
- en: '[PRE43]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Read the `pkl` file back:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取 `pkl` 文件：
- en: '[PRE44]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now let''s inspect the Cloud Storage bucket:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们来查看 Cloud Storage 存储桶：
- en: '![Figure 1.31 – Serializing an object in a bucket from the workflow in AI Platform'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 1.31 – 从 AI 平台的工作流中将对象序列化到存储桶中'
- en: '](img/Figure_1.31.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.31.jpg)'
- en: Figure 1.31 – Serializing an object in a bucket from the workflow in AI Platform
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.31 – 从 AI 平台的工作流中将对象序列化到存储桶中
- en: 'Here are the key takeaways:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是关键要点：
- en: Working data generated during the TensorFlow workflow can be persisted as a
    serialized object in the storage bucket.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 工作流中生成的工作数据可以作为序列化对象持久化到存储桶中。
- en: Google AI Platform's JupyterLab environment provides seamless integration between
    the TensorFlow runtime and the Cloud Storage command-line tool, `gsutil`.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google AI Platform 的 JupyterLab 环境提供了 TensorFlow 运行时与 Cloud Storage 命令行工具 `gsutil`
    之间的无缝集成。
- en: When you need to transfer content between Google Cloud Storage and AI Platform,
    use the `!gsutil` command-line tool.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你需要在 Google Cloud Storage 和 AI Platform 之间传输内容时，使用 `!gsutil` 命令行工具。
- en: Summary
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter provided a broad overview of the TensorFlow Enterprise environment
    hosted by Google Cloud AI Platform. We also saw how this platform seamlessly integrates
    specific tools such as command-line APIs to facilitate the easy transfer of data
    or objects between the JupyterLab environment and our storage solutions. These
    tools make it easy to access data stored in BigQuery or in storage buckets, which
    are the two most commonly used data sources in TensorFlow.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了由 Google Cloud AI Platform 托管的 TensorFlow 企业环境。我们还看到该平台如何无缝集成特定工具，例如命令行
    API，以促进数据或对象在 JupyterLab 环境和我们的存储解决方案之间的轻松传输。这些工具使得访问存储在 BigQuery 或存储桶中的数据变得简单，这两个数据源是
    TensorFlow 中最常用的。
- en: 'In the next chapter, we will take a closer look at the three ways available
    in AI Platform to use TensorFlow Enterprise: the Notebook, Deep Learning VM, and
    Deep Learning Containers.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将更详细地了解 AI Platform 中使用 TensorFlow 企业版的三种方式：Notebook、深度学习虚拟机（Deep Learning
    VM）和深度学习容器（Deep Learning Containers）。
