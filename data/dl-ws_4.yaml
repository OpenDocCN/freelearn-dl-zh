- en: 4\. Deep Learning for Text – Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 深度学习与文本——嵌入
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will begin our foray into **Natural Language Processing**
    for text. We will start by using the **Natural Language Toolkit** to perform text
    preprocessing on raw text data, where we will tokenize the raw text and remove
    punctuations and stop words. As we progress through this chapter, we will implement
    classical approaches to text representation, such as one-hot encoding and the
    **TF-lDF** approach. This chapter demonstrates the power of word embeddings and
    explains the popular deep learning-based approaches for embeddings. We will use
    the **Skip-gram** and **Continuous Bag of Words** algorithms to generate our own
    word embeddings. We will explore the properties of the embeddings, the different
    parameters of the algorithms, and generate vectors for phrases. By the end of
    this chapter, you will be able to handle text data and start using word embeddings
    by using pre-trained models, as well as your own embeddings.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始深入探讨**自然语言处理**（NLP）在文本中的应用。我们将通过使用**自然语言工具包**（Natural Language Toolkit）对原始文本数据进行预处理，过程中我们会对原始文本进行分词，并去除标点符号和停用词。随着本章内容的展开，我们将实现经典的文本表示方法，如独热编码（one-hot
    encoding）和**TF-IDF**方法。本章展示了词嵌入（word embeddings）的强大功能，并解释了基于深度学习的嵌入方法。我们将使用**Skip-gram**和**Continuous
    Bag of Words**算法来生成我们自己的词嵌入。我们将探讨嵌入的特性、算法的不同参数，并生成短语的向量。到本章结束时，你将能够处理文本数据，开始使用预训练模型的词嵌入，甚至创建你自己的嵌入模型。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: How does Siri know exactly what to do when you ask her to "*play a mellow song
    from the 80s*"? How does Google find the most relevant results for even your ill-formed
    search queries in a fraction of a second? How does your translation app translate
    text from German to English almost instantly? How does your email client protect
    you and automatically identify all those malicious spam/phishing emails? The answer
    to all these questions, and what powers many more amazing applications, is using
    **Natural Language Processing** (**NLP**).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Siri是如何知道在你要求她“*播放80年代的一首轻柔歌曲*”时该做什么的？谷歌是如何在不到一秒钟的时间里找到与你搜索的关键词最相关的结果的？你的翻译应用是如何几乎瞬间将德语翻译成英语的？你的电子邮件客户端是如何保护你并自动识别所有那些恶意的垃圾邮件/网络钓鱼邮件的？所有这些问题的答案，以及驱动许多其他惊人应用的技术，都归功于**自然语言处理**（**NLP**）。
- en: So far, we've dealt with structured, numeric data – images that were also numeric
    matrices. In this chapter, we'll begin our discussion by talking about handling
    text data and unlock the skills needed to harness this goldmine of unstructured
    information. We will discuss a key idea in this chapter – representation, particularly
    using embeddings. We will discuss the considerations and implement the approaches
    for representation. We will begin with the simplest approaches and end with word
    embeddings – an amazingly powerful approach for representing text data. Word embeddings
    will help you get state-of-the-art results in NLP tasks when coupled with deep
    learning approaches.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理结构化的数字数据——例如作为数字矩阵的图像。在本章中，我们将开始讨论如何处理文本数据，并解锁所需的技能，以利用这片未结构化信息的“金矿”。本章将讨论一个关键的概念——表示，特别是使用嵌入（embeddings）。我们将讨论表示的注意事项并实现相关方法。我们将从最简单的表示方法开始，最终讨论词嵌入——这是一种表示文本数据的强大方法。词嵌入将帮助你在结合深度学习方法时，在NLP任务中获得最先进的结果。
- en: 'NLP is a field concerned with helping machines make sense of natural (human)
    language. As shown in the following figure, NLP resides at the intersection of
    linguistics, computer science, and artificial intelligence:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是一个旨在帮助机器理解自然（人类）语言的领域。如以下图所示，NLP处于语言学、计算机科学和人工智能的交汇点：
- en: '![Figure 4.1: Where NLP fits'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.1：NLP的应用场景'
- en: '](img/B15385_04_01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_01.jpg)'
- en: 'Figure 4.1: Where NLP fits'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：NLP的应用场景
- en: 'It is a vast field – think of all the places language (spoken and written)
    is used. NLP enables and powers the kind of applications listed in the preceding
    figure, including the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个广泛的领域——想一想语言（口语和书面语言）使用的所有地方。NLP使得并推动了前述图中列举的各类应用，包括以下内容：
- en: Classification of documents into categories (text classification)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文档分类到不同类别（文本分类）
- en: Translation between languages, say, German to English (sequence-to-sequence learning)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言之间的翻译，例如，从德语到英语（序列到序列学习）
- en: Automatically classifying the sentiment of a tweet or a movie review (sentiment
    analysis)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动分类推文或电影评论的情感（情感分析）
- en: Chatbots that reply to your query instantly, 24/7
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 24/7即时回复你的查询的聊天机器人
- en: 'Before we go any further, we need to acknowledge and appreciate that NLP isn''t
    easy. Consider the following sentence: "*The boy saw a man with a telescope.*"'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们需要承认并欣赏，NLP 不是一项简单的任务。考虑以下句子：“*The boy saw a man with a telescope.*”
- en: Who had the telescope? Did the boy use a telescope to see the man through it?
    Or was the man carrying a telescope with him? There is an ambiguity that we can't
    resolve with this sentence alone. Maybe some more context will help us figure
    this out.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 谁拿着望远镜？男孩是用望远镜看见那个人的吗？还是那个人身上带着望远镜？这句话本身有歧义，我们无法单凭它解决这个问题，也许更多的上下文能帮助我们搞清楚。
- en: 'Let''s consider this sentence, then: "*Rahim convinced Mohan to buy a television
    for himself.*" Who was the TV bought for – Rahim or Mohan? This is another case
    of ambiguity that we may be able to resolve with more context, but again, it may
    be very difficult for a machine/program.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再考虑一下这个句子：“*Rahim convinced Mohan to buy a television for himself.*” 这台电视是给谁买的——是给
    Rahim 还是 Mohan？这又是一个歧义问题，我们或许可以通过更多的上下文来解决，但对于机器/程序来说，依然可能非常困难。
- en: 'Let''s consider another example: "*Rahim has quit skydiving.*" This sentence
    implies that Rahim did a fair amount of skydiving. There is a presupposition in
    this sentence, which is hard for a machine to infer.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一个例子：“*Rahim has quit skydiving.*” 这句话意味着 Rahim 曾做过相当多的跳伞。句子中有一个假设，这对机器来说很难推断出来。
- en: 'Language is a complex system that uses symbols (words/terms) and combines them
    in many ways to communicate ideas. Making sense of language is not always very
    easy, and there are many reasons for this. Ambiguity is by far the biggest reason:
    words can have different meanings in different contexts. Add to that subtext,
    different perspectives, and so on. We can never be sure if the same words are
    understood the same way by different people. A poem can be interpreted in many
    ways by those who read it, where each reader brings their unique perspective and
    understanding of the world and employs them to make sense of the poem in their
    own way.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 语言是一个复杂的系统，使用符号（单词/术语），并以多种方式将它们组合起来传达思想。理解语言并不总是那么容易，其中有很多原因。歧义无疑是最大的问题：单词在不同的语境中可能有不同的含义。再加上潜台词、不同的视角等等，我们永远无法确定不同的人是否以相同的方式理解同样的词语。一首诗可以被不同的读者以多种方式解读，每个读者都带着自己独特的视角和对世界的理解来解读这首诗。
- en: Deep Learning for Natural Language Processing
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习与自然语言处理
- en: The emergence of deep learning has had a strong positive impact on many fields,
    and NLP is no exception. By now, you can appreciate that deep learning approaches
    have given us accuracies like never before, and this has helped us improve in
    many areas. There are several tasks in NLP that have gained tremendously from
    deep learning approaches. Applications that use sentiment prediction, machine
    translation, and chatbots previously required a lot of manual intervention. With
    deep learning and NLP, these tasks are completely automated and bring with them
    impressive performance. The simple, high-level view shown in *Figure 4.2* shows
    how deep learning can be used for processing natural language. Deep learning provides
    us with not only great representations of natural language that machines can understand
    but also very powerful modeling approaches well suited for tasks in NLP.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的兴起对许多领域产生了积极的影响，NLP（自然语言处理）也不例外。现在你可以理解，深度学习方法为我们带来了前所未有的高准确度，这帮助我们在许多领域取得了进步。NLP
    中有多个任务从深度学习方法中获益匪浅。过去，情感预测、机器翻译和聊天机器人等应用需要大量人工干预。而现在，借助深度学习和 NLP，这些任务已经完全自动化，并且展现出了令人印象深刻的表现。*图
    4.2* 中展示的简单高层次视图表明了深度学习如何用于处理自然语言。深度学习不仅为我们提供了机器可以理解的自然语言的优秀表示，还为 NLP 任务提供了非常强大的建模方法。
- en: '![Figure 4.2: Deep learning for NLP'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.2：深度学习在自然语言处理中的应用'
- en: '](img/B15385_04_02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_02.jpg)'
- en: 'Figure 4.2: Deep learning for NLP'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：深度学习在自然语言处理中的应用
- en: That being said, we need to be cautious to avoid underestimating the difficulty
    of getting machines to perform tasks involving human language and the field of
    NLP. Deep learning hasn't solved all of the challenges in NLP, but it has indeed
    caused a paradigm shift in the way several tasks in NLP are approached and has
    helped advance some applications in this field, making otherwise difficult tasks
    accessible and easy for anyone and everyone. We will perform some of these in
    *Chapter 5*, *Deep Learning for Sequences*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我们需要小心，避免低估让机器执行涉及人类语言任务的难度以及NLP领域的挑战。深度学习并未解决NLP中的所有挑战，但它的确在许多NLP任务的处理方式上带来了范式的转变，并推动了一些应用的发展，使得本来困难的任务变得对任何人来说都易于操作和实现。我们将在*第五章*，*序列的深度学习*中执行其中的一些任务。
- en: One such key task is text data representation – which is, in simple terms, converting
    raw text into something a model would understand. Word embeddings constitute a
    deep learning-based approach that has changed the game and gives a very powerful
    representation of text. We'll discuss embeddings in detail and create our own
    embeddings later in this chapter. First, let's get our hands dirty by working
    with some text and performing some very important data preparation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个关键任务是文本数据表示——简单来说，就是将原始文本转换为模型可以理解的形式。词嵌入是一种基于深度学习的方法，它改变了游戏规则，并提供了非常强大的文本表示。我们将在本章后面详细讨论嵌入，并创建我们自己的嵌入。首先，让我们动手处理一些文本，进行一些非常重要的数据准备。
- en: Getting Started with Text Data Handling
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始处理文本数据
- en: Let's get some test data into Python to begin. First, we'll create some toy
    data of our own and get familiar with the tools. Then, we'll use Lewis Carrol's
    classic work, "*Alice's Adventures in Wonderland*", which is available through
    Project Gutenberg ([gutenberg.org](http://gutenberg.org)). Conveniently enough,
    we have this easily accessible through the **Natural Language ToolKit** (**NLTK**),
    a great library for performing NLP from scratch.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始将一些测试数据导入Python。首先，我们将创建一些自己的玩具数据，熟悉工具。然后，我们将使用路易斯·卡罗尔的经典作品《*爱丽丝梦游仙境*》，该作品可以通过古腾堡计划（[gutenberg.org](http://gutenberg.org)）获取。幸运的是，我们可以通过**自然语言工具包**（**NLTK**）轻松访问它，这是一款用于从零开始进行NLP的极佳库。
- en: Note
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The code implementations for this chapter can be found at [https://packt.live/3gEgkSP](https://packt.live/3gEgkSP).
    All the code in this chapter must be run in a single Jupyter Notebook.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码实现可以在[https://packt.live/3gEgkSP](https://packt.live/3gEgkSP)找到。本章中的所有代码都必须在一个单一的Jupyter
    Notebook中运行。
- en: 'NLTK should come with the Anaconda distribution. If not, you can install NLTK
    by using the following command in the command line:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK应该包含在Anaconda发行版中。如果没有，你可以在命令行中使用以下命令安装NLTK：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This should work on Windows. For macOS and Linux, you can use the following command:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法在Windows上应该可行。对于macOS和Linux，你可以使用以下命令：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Our dummy data can be created using the following command (we''re using Jupyter
    Notebooks here; feel free to use any interface):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的虚拟数据可以通过以下命令创建（这里我们使用Jupyter Notebooks；你也可以使用任何界面）：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We have the text in `raw_txt`, which is a string variable, so now, we're ready
    to start processing it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个名为`raw_txt`的字符串变量，其中存储了文本，因此，现在我们已经准备好开始处理它。
- en: Text Preprocessing
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本预处理
- en: Text preprocessing refers to the process of getting the text data ready for
    your primary analysis/model. Regardless of your end goal – which could be sentiment
    analysis, classification, clustering, or any of the many others – you need to
    get your raw text data cleaned up and ready for analysis. This is the first part
    of any application involving NLP.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 文本预处理是指将文本数据准备好以进行主要分析/建模的过程。无论你的最终目标是什么——可能是情感分析、分类、聚类或其他众多目标——你都需要清理原始文本数据并准备好进行分析。这是任何涉及自然语言处理（NLP）的应用的第一步。
- en: What do we mean by **clean up**, and when is the text data ready? We know that
    the text data we encounter in our day-to-day lives can be very messy (think about
    social media, product reviews, service reviews, and so on) and has various imperfections.
    Depending on the task at hand and the kind of data you're dealing with, the imperfections
    you care about will vary, and **cleaning up** can mean very different things.
    As an example, in some applications, preprocessing could just mean "dividing the
    sentences into individual terms." The steps you take here can and will have an
    impact on the final outcome of your analysis. Let's discuss this in more detail.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的**清理**是什么意思？什么时候文本数据算是准备好了？我们知道，在日常生活中遇到的文本数据往往非常杂乱（想一想社交媒体、产品评论、服务评论等等），并且存在各种各样的不完美之处。根据手头的任务和你正在处理的数据类型，你关心的不完美之处会有所不同，而**清理**的意义也会有所不同。举个例子，在某些应用中，预处理可能仅仅意味着“将句子分割成单独的术语”。你在这里采取的步骤将对最终分析结果产生影响。让我们更详细地讨论这一点。
- en: Tokenization
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分词
- en: The first step in preprocessing is inevitably **tokenization** – splitting the
    raw input text sequence into **tokens**. In simple terms, it is breaking the raw
    text into constituent elements that you want to work on. This token can be a paragraph,
    sentence, word, or even a character. If you want to separate a paragraph into
    sentences, then you would tokenize the paragraph into sentences. If you want to
    separate the words in a sentence, then you would tokenize the sentence into words.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理的第一步不可避免的是**分词**——将原始输入文本序列拆分成**词元**。简单来说，它就是将原始文本拆分成你想要处理的基本元素。这个词元可以是段落、句子、单词，甚至是字符。如果你想将段落拆分成句子，那么你会将段落分词成句子。如果你想将句子中的单词分开，那么你会将句子分词成单词。
- en: For our raw text, first, we want to separate the sentences. To do so, we have
    multiple options in Python – here, we'll use the tokenize API in NLTK.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的原始文本，首先，我们想要将句子分开。为此，我们在Python中有多种选择——这里，我们将使用NLTK中的tokenize API。
- en: Note
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We'll be using Jupyter Notebooks throughout this book, which is something that
    we recommend. However, feel free to use any IDE you wish.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们将始终使用Jupyter Notebooks，这是我们推荐的工具。不过，随时可以使用任何你喜欢的IDE。
- en: 'Before we can use the API, we have to `import nltk` and download the `punkt`
    sentence tokenizer. Then, we need to import the `tokenize` library. All this can
    be done using the following commands:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用API之前，我们必须`import nltk`并下载`punkt`句子分词器。然后，我们需要导入`tokenize`库。所有这些可以通过以下命令完成：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The tokenize API has utilities to extract different levels of tokens (sentences,
    words, or characters) for different types of data (a very handy tweet tokenizer,
    too). We''ll use the `sent_tokenize()` method here. The `sent_tokenize()` method
    breaks input text into constituent sentences. Let''s see it in action:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: tokenize API提供了提取不同级别词元（句子、单词或字符）的工具，适用于不同类型的数据（也有一个非常实用的推文分词器）。我们将在这里使用`sent_tokenize()`方法。`sent_tokenize()`方法将输入文本拆分成句子。让我们看看它的实际效果：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This should give us the following individual sentences:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会给我们以下的独立句子：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Looking at the output, it seems like `sent_tokenize()` is doing a pretty good
    job. It has correctly identified the sentence boundaries and given us the four
    sentences, as expected. Let''s assign the result to a variable for ease of handling
    and let''s check the data type of the result and its constituents:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出结果来看，`sent_tokenize()`似乎做得相当不错。它正确地识别了句子边界，并如预期般给出了四个句子。为了方便处理，我们将结果赋给一个变量，并检查结果及其组成部分的数据类型：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following is the output of the preceding code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we can see, it's a list with four elements, where each element contains the
    sentence as a string.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，它是一个包含四个元素的列表，每个元素都是一个字符串，表示一个句子。
- en: 'We can try breaking sentences into individual words using the `word_tokenize()`
    method. This method breaks a given sentence into its constituent words. It uses
    smart rules to figure out word boundaries. Let''s use list comprehension (comprehensions
    in Python are a concise approach to constructing new sequences) for a bit for
    convenience:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试使用`word_tokenize()`方法将句子拆分为单个单词。这个方法将给定的句子拆分成其组成的单词。它使用智能规则来判断单词的边界。为了方便起见，我们可以使用列表推导（在Python中，推导是一种简洁构建新序列的方法）：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding command gives us the following output:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的命令会给我们以下输出：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as expected – the elements of the resulting list are lists themselves,
    containing the words that form the sentence. Let''s also print out the first two
    elements of the result:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如预期 - 结果列表的元素本身是列表，其中包含组成句子的单词。让我们也打印出结果的前两个元素：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output would be as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The sentences have been broken into individual words. We can also see that contractions
    like "we'll" have been broken into constituents, that is, "we" and "'ll". All
    punctuation (commas, periods, exclamation marks, and so on) are separate tokens.
    This is very convenient for us if we wish to remove them, which we will do later.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 句子已被分解为单独的单词。我们还可以看到像“we'll”这样的缩写已被分解为组成部分，即“we”和“'ll”。所有标点符号（逗号、句号、感叹号等）都是单独的标记。如果我们希望删除它们，这对我们非常方便，我们稍后会这样做。
- en: Normalizing Case
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标准化大小写
- en: Another common step is to normalize case – we usually don't want "car", "CAR",
    "Car", and "caR" to be treated as separate entities. To do so, we typically convert
    all text into lowercase (we could also convert it into uppercase if we wanted).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的步骤是标准化大小写 - 我们通常不希望“car”、“CAR”、“Car”和“caR”被视为不同的实体。为此，我们通常将所有文本转换为小写（如果需要，我们也可以将其转换为大写）。
- en: All strings in Python have a `lower()` method to them, so converting a string
    variable (`strvar`) into lowercase is as simple as `strvar.lower()`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，所有字符串都有一个`lower()`方法，因此将字符串变量（`strvar`）转换为小写就像`strvar.lower()`这样简单。
- en: Note
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We could have used this right in the beginning, before tokenization, and it
    would have been as simple as `raw_txt = raw_txt.lower()`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以在分词之前一开始就使用这个方法，这将非常简单，比如`raw_txt = raw_txt.lower()`。
- en: 'We will normalize the case of our data using the `lower()` method after tokenizing
    into individual sentences. We''ll accomplish this with the following commands:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在将数据分词为单独的句子后使用`lower()`方法来标准化我们的数据。我们将通过以下命令来实现这一点：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let''s print out a couple of sentences to see what the result looks like:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印几个句子，看看结果是什么样的：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output will be as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can see that the output has all the terms in lowercase this time. We've taken
    the raw text, broken it into sentences, normalized the case, and then broken that
    down into words. Now, we have all the tokens that we need, but we still seem to
    have a lot of punctuation marks as tokens that we need to get rid of. Let's go
    ahead and perform more "cleanup".
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，输出这次所有的术语都是小写的。我们已经取得了原始文本，将其分成句子，标准化大小写，然后将其分解成单词。现在，我们拥有了所有我们需要的标记，但我们似乎仍然有很多标点符号作为标记，我们需要摆脱它们。让我们继续进行更多的“清理工作”。
- en: Removing Punctuation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除标点符号
- en: 'We can see that the data currently has all punctuation as separate tokens.
    Again, bear in mind that there could be tasks where punctuations could be important.
    As an example, when performing sentiment analysis, that is, predicting if the
    sentiment in the text is positive or negative, an exclamation can add value. For
    our task, let''s remove these since we''re only interested in representing the
    terms of language. To do so, we need to have a list of all the punctuation marks
    we want to remove. Luckily, we have such a list in the string base library in
    Python, which we can simply import and assign to a list variable:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，数据目前所有标点符号都作为单独的标记存在。再次提醒，可能会有标点符号很重要的任务。例如，在进行情感分析时，也就是预测文本的情感是积极的还是消极的时候，感叹号可能会增加价值。对于我们的任务，让我们去掉这些标点符号，因为我们只关心语言的表达。为此，我们需要一个列表，其中包含我们要删除的所有标点符号。幸运的是，Python的字符串基础库中有这样一个列表，我们可以简单地导入并分配给一个列表变量：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You should get the following output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: All the usual punctuation marks are available here. If there are any additional
    punctuation marks you want to remove, you can simply add them to the `list_punct`
    variable.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所有通常的标点符号都在这里。如果有任何其他要删除的标点符号，您可以简单地将它们添加到`list_punct`变量中。
- en: 'We can define a function to remove punctuation from a given list of tokens.
    This function will expect a list of tokens, from which it will drop the tokens
    that are available in the `list_punct` variable:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义一个函数来从给定的标记列表中删除标点符号。该函数将期望一个标记列表，从中将可用于`list_punct`变量的标记删除：
- en: '[PRE17]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can test this out on some dummy tokens using the following command:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令在一些虚拟标记上测试这个：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We get the following result:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下结果：
- en: '[PRE19]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The function works as intended. Now, we need to pass the `txt_words` variable
    we modified in the previous section to the `drop_punct` function we just created.
    We will store our result in a new variable called `txt_words_nopunct`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该功能按预期工作。现在，我们需要将我们在上一部分中修改的`txt_words`变量传递给刚刚创建的`drop_punct`函数。我们将把结果存储在一个名为`txt_words_nopunct`的新变量中：
- en: '[PRE20]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will get the following output:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '[PRE21]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As you can see from the preceding output, the function we created has removed
    all the punctuation marks from our raw text. Now, the data looks much cleaner
    without the punctuation, but we still need to get rid of non-informative terms.
    We'll discuss that in the next section.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的输出所示，我们创建的函数已经从原始文本中删除了所有标点符号。现在，数据看起来更干净了，因为没有了标点符号，但我们仍然需要去除无信息的词汇。我们将在下一部分讨论这个问题。
- en: Removing Stop Words
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除停用词
- en: 'In day-to-day language, we have a lot of terms that don''t add a lot of information/value*.
    These are typically referred to as "stop words". We can think of these as belonging
    to two broad categories:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在日常语言中，我们有很多词汇，它们并没有增加太多的信息/价值*。这些通常被称为“停用词”。我们可以将这些词分为两大类：
- en: '**General/functional**: These are filler words in the language that don''t
    provide a lot of information but help stitch together other informative words
    to form meaningful sentences, such as "the", "an", "of", and so on.'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**一般/功能性**：这些是语言中的填充词，它们并没有提供很多信息，但帮助连接其他有信息的词汇，以形成有意义的句子，例如“the”、“an”、“of”等等。'
- en: '**Contextual**: These aren''t general functional terms, but given the context,
    don''t add a lot of value. If you''re working with reviews of a mobile phone,
    where all reviews are talking about the phone, the term "phone" itself may not
    add a lot of information.'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**上下文**：这些不是一般的功能性词汇，但鉴于上下文，它们并没有提供很多信息。如果你正在处理关于手机的评论，而所有评论都在讨论手机，那么“手机”这个词本身可能没有太多信息。'
- en: Note
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: '*The notion of "value" changes with each task. Functional words such as "the"
    and "and" may not be important for, say, automatic document categorization into
    subjects, but can be very important for other applications, such as part-of-speech
    tagging (identifying verbs, adjectives, nouns, pronouns, and so on).'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*“价值”的概念在每个任务中有所不同。像“the”和“and”这样的功能性词汇可能对于自动文档分类到主题中并不重要，但对于其他应用，比如词性标注（识别动词、形容词、名词、代词等）则非常重要。*'
- en: 'Functional stop words are conveniently built into NLTK. We just need to import
    them and then we can store them in a variable. Once stored, they can be accessed
    just like any Python list. Let''s import them and see how many of these words
    we have:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 功能性停用词已经方便地集成到NLTK中。我们只需要导入它们，然后可以将它们存储在一个变量中。一旦存储，就可以像访问Python列表一样访问它们。让我们导入它们并看看我们有多少个这样的词：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will see the following output:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下输出：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can see that we have 179 built-in stop words. Let''s also print some of
    them:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们有179个内置的停用词。让我们也打印出其中一些：
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output will be as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE25]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We can see that most of these terms are very commonly used "filler" terms that
    have a "functional" role in the language, and don't add a lot of information.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这些术语大多数是非常常用的“填充”词，它们在语言中有“功能性”作用，并没有提供太多信息。
- en: Now, removing stop words can be done the same way we removed punctuation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，删除停用词的方法与我们删除标点符号的方法相同。
- en: 'Exercise 4.01: Tokenizing, Case Normalization, Punctuation, and Stop Word Removal'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习4.01：分词、大小写规范化、标点符号和停用词移除
- en: In this exercise, we will remove stop words from the data, and also apply everything
    we have learned so far. We'll start by performing tokenization (sentences and
    words); then, we'll perform case normalization, followed by punctuation and stop
    word removal.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将从数据中删除停用词，并应用我们到目前为止学到的所有内容。我们将首先执行分词（句子和单词）；然后执行大小写规范化，接着是标点符号和停用词的删除。
- en: Note
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before commencing this exercise, ensure that you are using a Jupyter Notebook
    where you have downloaded both the `punkt` sentence tokenizer and the `stopwords`
    corpus, as demonstrated in the *Text Preprocessing* section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始这个练习之前，请确保你正在使用一个Jupyter Notebook，并且已经下载了`punkt`句子分词器和`stopwords`语料库，如*文本预处理*部分所示。
- en: 'We''ll keep the code concise this time. We''ll be defining and manipulating
    the `raw_txt` variable. Let''s get started:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们将保持代码简洁。我们将定义和处理`raw_txt`变量。让我们开始吧：
- en: 'Run the following commands to import `nltk` and the `tokenize` module from
    it:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令来导入`nltk`及其`tokenize`模块：
- en: '[PRE26]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define the `raw_txt` variable so that it contains the text "`Welcome to the
    world of deep learning for NLP! We''re in this together, and we''ll learn together.
    NLP is amazing, and deep learning makes it even more fun. Let''s learn!`":'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`raw_txt`变量，使其包含文本"`欢迎来到深度学习与自然语言处理的世界！我们一起前行，一起学习。自然语言处理令人惊叹，深度学习让它更有趣。让我们开始学习！`"：
- en: '[PRE27]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Use the `sent_tokenize()` method to separate the raw text into individual sentences
    and store the result in a variable. Use the `lower()` method to convert the string
    into lowercase before tokenizing:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`sent_tokenize()`方法将原始文本分割为单独的句子，并将结果存储在一个变量中。在分词之前，使用`lower()`方法将字符串转换为小写：
- en: '[PRE28]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The `txt_sents` variable we've just created will be used later on in the chapter
    as well.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们刚创建的`txt_sents`变量将在本章后续部分中继续使用。
- en: 'Using list comprehension, apply the `word_tokenize()` method to separate each
    sentence into its constituent words:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用列表推导式，应用`word_tokenize()`方法将每个句子分解成其组成单词：
- en: '[PRE29]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Import `punctuation` from the `string` module and convert it into a list:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`string`模块导入`punctuation`并将其转换为列表：
- en: '[PRE30]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Import the built-in stop words for English from NLTK and save them in a variable:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从NLTK导入英语的内建停用词，并将其保存到一个变量中：
- en: '[PRE31]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Create a combined list that contains the punctuations as well as the NLTK stop
    words. Note that we can remove them together in one go:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含标点符号和NLTK停用词的组合列表。请注意，我们可以一次性删除它们：
- en: '[PRE32]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Define a function that will remove stop words and punctuation from the input
    sentence, provided as a collection of tokens:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，移除输入句子中的停用词和标点符号，输入为一个标记集合：
- en: '[PRE33]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Remove redundant tokens by applying the function to the tokenized sentences
    and store the result in a variable:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对分词后的句子应用函数来移除冗余的标记，并将结果存储在一个变量中：
- en: '[PRE34]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Print the first cleaned-up sentence from the data:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印数据中第一个清理后的句子：
- en: '[PRE35]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'With the stop words removed, the result will look like this:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 停用词被移除后，结果将如下所示：
- en: '[PRE36]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/2VVNEgf](https://packt.live/2VVNEgf)。
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/38Gr54r](https://packt.live/38Gr54r)上在线运行这个示例。您必须执行整个Notebook才能获得期望的结果。
- en: In this exercise, we performed all the cleanup steps we've learned about so
    far. This time around, we combined certain steps and made the code more concise.
    These are some very common steps that we should apply when dealing with text data.
    You could try to further optimize and modularize by defining a function that returns
    the result after all the processing steps. We encourage you to try it out.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们完成了迄今为止学到的所有清理步骤。这次我们将某些步骤结合起来，使代码更加简洁。这些是处理文本数据时非常常见的步骤。您可以尝试通过定义一个返回处理结果的函数，进一步优化和模块化代码。我们鼓励您尝试一下。
- en: So far, the steps in the cleanup process were steps that got rid of tokens that
    weren't very useful in our assessment. But there are a few more things we could
    do to make our data even better – we can try using our understanding of the language
    to combine tokens, identify tokens that have practically the same meaning, and
    remove further redundancy. A couple of popular approaches are stemming and lemmatization.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，清理过程中的步骤是移除了我们评估中不太有用的标记。但我们还有一些事情可以做，以进一步改善我们的数据——我们可以尝试运用对语言的理解来合并标记，识别具有相同含义的标记，并去除更多冗余。两种常见的做法是词干提取（stemming）和词形还原（lemmatization）。
- en: Note
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The variables we have created in this exercise will be used in later sections
    of the chapter as well. Ensure that you're completing this exercise first before
    moving to the upcoming exercises and activities.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中创建的变量将在本章的后续部分中继续使用。确保先完成本练习再继续进行接下来的练习和活动。
- en: Stemming and Lemmatization
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词干提取与词形还原
- en: '"Eat", "eats", "eating", "ate" – aren''t they all just variations of the same
    word, all referring to the same action? In most text and spoken language, in general,
    we have multiple forms of the same word. Typically, we don''t want these to be
    considered as separate tokens. A search engine would need to return similar results
    if the query is "red shoes" or "red shoe"– it would be a terrible search experience
    otherwise. We acknowledge that such cases are very common and that we need a strategy
    to handle such cases. But what should we do with the variants of a word? A reasonable
    approach is to map them all to a common token so that they are all treated the
    same.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '"Eat"、"eats"、"eating"、"ate"——它们不都是同一个单词的变体吗，都指的是同一个动作？在大多数文本和口语中，我们通常会遇到同一个单词的多个形式。通常情况下，我们不希望这些形式被当作独立的词项来处理。如果查询是"red
    shoes"或"red shoe"，搜索引擎需要返回相似的结果——否则会带来糟糕的搜索体验。我们承认这种情况非常常见，因此我们需要一种策略来处理这些情况。那么，我们应该如何处理一个单词的变体呢？一种合理的方法是将它们映射到一个共同的词项，这样它们就会被视为相同。'
- en: 'Stemming is a rule-based approach to achieve normalization by reducing a word
    to its "stem". The stem is the root of the word before any affixes (an element
    added to make a variant) are added. This approach is rather simple – chop off
    the suffix to get the stem. A popular algorithm is the **Porter stemming** algorithm,
    which applies a series of such rules:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取是一种基于规则的方法，通过将单词简化为其“词干”来实现标准化。词干是单词在加上任何词缀（用于形成变体的元素）之前的根本形式。这种方法相当简单——去掉后缀就得到词干。一个流行的算法是**Porter
    词干提取**算法，它应用一系列这样的规则：
- en: '![Figure 4.3: Examples of the Porter stemming algorithm''s rule-based approach'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.3：Porter 词干提取算法基于规则的方法示例'
- en: '](img/B15385_04_03.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_03.jpg)'
- en: 'Figure 4.3: Examples of the Porter stemming algorithm''s rule-based approach'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：Porter 词干提取算法基于规则的方法示例
- en: Note
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The full set of Porter stemming algorithm rules can be found at [http://snowball.tartarus.org/algorithms/porter/stemmer.html](http://snowball.tartarus.org/algorithms/porter/stemmer.html).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Porter 词干提取算法的完整规则集可以在[http://snowball.tartarus.org/algorithms/porter/stemmer.html](http://snowball.tartarus.org/algorithms/porter/stemmer.html)找到。
- en: 'Let''s look at the Porter stemming algorithm in action. Let''s import the `PorterStemmer`
    function from the `''stem''` module in NLTK and create an instance of it:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Porter 词干提取算法的实际操作。我们从 NLTK 的 `'stem'` 模块导入 `PorterStemmer` 函数并创建一个实例：
- en: '[PRE37]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Note that the stemmer works on individual tokens, not sentences as a whole.
    Let''s see how the stemmer stems the word "`driving`":'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，词干提取器是作用于单个词项的，而不是整个句子的。让我们看看词干提取器如何处理单词"driving"：
- en: '[PRE38]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output will be as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE39]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s see how we can apply this to a whole sentence. Note that we will have
    to tokenize the sentence:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将这个方法应用于整个句子。请注意，我们需要先对句子进行分词：
- en: '[PRE40]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following code is used for tokenizing the sentence and applying the stemmer
    to each term:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于对句子进行分词，并对每个词项应用词干提取器：
- en: '[PRE41]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE42]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We can see that the stemmer has correctly reduced "mustered" to "muster" and
    "driving" to "drive", while "drove" is untouched. Also, note that the result of
    a stemmer need not be a valid English word.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，词干提取器已经正确地将"mustered"减少为"muster"并将"driving"减少为"drive"，而" drove"则没有被改变。此外，请注意，词干提取器的结果不一定是一个有效的英语单词。
- en: 'Lemmatization is a more sophisticated approach that refers to a dictionary
    and finds a valid root form (the lemma) of the word. Lemmatization works best
    when the part of speech of the word is also provided – it considers the role the
    term is playing and returns the appropriate form. The output from a lemmatization
    step is always a valid English word. However, lemmatization is computationally
    very expensive, and for it to work well, it needs the part-of-speech tag, which
    typically isn''t available in the data. Let''s have a brief look at it. First,
    let''s import `WordNetLemmatizer` from `nltk.stem` and instantiate it:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原是一种更复杂的方法，它参考词典并找到单词的有效词根形式（词形）。词形还原在提供单词词性时效果最佳——它考虑到单词的角色并返回适当的形式。词形还原的输出总是有效的英语单词。然而，词形还原的计算代价非常高，而且为了使其效果良好，需要词性标签，而词性标签通常在数据中是不可用的。我们来看一下它的简要使用。首先，从`nltk.stem`导入`WordNetLemmatizer`并实例化它：
- en: '[PRE43]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let''s apply the `lemmatizer` on the term `ponies`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在词项`ponies`上应用`lemmatizer`：
- en: '[PRE44]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The following is the output:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '[PRE45]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: For our discussions, stemming is sufficient. The result from stemming may not
    always be a valid word. For example, `poni` is the stem for `ponies` but isn't
    a valid English word. Also, there may be some inaccuracies, but for the objective
    of mapping to a common word, this crude method works just fine.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的讨论，词干提取就足够了。词干提取的结果可能并不总是有效的单词。例如，`poni`是`ponies`的词干，但它不是一个有效的英语单词。此外，可能会有一些不准确之处，但对于将单词映射到共同词形的目标而言，这种粗略的方法是完全有效的。
- en: 'Exercise 4.02: Stemming Our Data'
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习4.02：对我们的数据进行词干提取
- en: 'In this exercise, we will continue with data preprocessing. We removed the
    stop words and punctuation in the previous exercise. Now, we will use the Porter
    stemming algorithm to stem the tokens. Since we''ll be using the `txt_words_nostop`
    variable we created previously, let''s continue with the same Jupyter Notebook
    we created in *Exercise 4.01*, *Tokenizing, Case Normalization, Punctuation, and
    Stop Word Removal*. The variable, at this point, will contain the following text:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将继续进行数据预处理。在之前的练习中，我们去除了停用词和标点符号。现在，我们将使用Porter词干提取算法来提取词干。由于我们将使用之前创建的`txt_words_nostop`变量，让我们继续使用在*练习4.01*中创建的同一个Jupyter
    Notebook，*标记化、大小写规范化、标点符号和停用词移除*。此时，该变量将包含以下文本：
- en: '[PRE46]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The following are the steps to complete this exercise:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此练习的步骤如下：
- en: 'Import `PorterStemmer` from NLTK using the following command:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令从NLTK导入`PorterStemmer`：
- en: '[PRE47]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Instantiate the stemmer:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化词干提取器：
- en: '[PRE48]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Apply the stemmer to the first sentence in `txt_words_nostop`:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`txt_words_nostop`中的第一句话应用词干提取器：
- en: '[PRE49]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'When we print the result, we get the following output:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们打印结果时，得到以下输出：
- en: '[PRE50]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Apply the stemmer to all the sentences in the data. You could use loops, or
    a nested list comprehension:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据中的所有句子应用词干提取器。你可以使用循环，或者使用嵌套列表推导：
- en: '[PRE51]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Print the output using the following command:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令打印输出：
- en: '[PRE52]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output will be as follows:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE53]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: It looks like plenty of modifications have been made by the stemmer. Many of
    the words aren't valid anymore but are still recognizable, and that's okay.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来词干提取器已经做了很多修改。许多单词已经不再有效，但仍然可以识别，这没关系。
- en: Note
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参考[https://packt.live/2VVNEgf](https://packt.live/2VVNEgf)。
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，访问[https://packt.live/38Gr54r](https://packt.live/38Gr54r)。你必须执行整个Notebook才能获得预期的结果。
- en: In this exercise, we used the Porter stemming algorithm to stem the terms of
    our tokenized data. Stemming works on individual terms, so it needs to be applied
    after tokenizing into terms. Stemming reduced some terms to their base form, which
    weren't necessarily valid English words.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们使用了Porter词干提取算法来提取我们标记化数据的词干。词干提取作用于单个词，因此需要在标记化成词之后进行。词干提取将一些词语缩减为其基本形式，这些形式不一定是有效的英语单词。
- en: Beyond Stemming and Lemmatization
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词干提取与词形还原之外的内容
- en: Beyond stemming and lemmatization, there are many specific approaches to handle
    word variations. We have techniques such as phonetic hashing to identify spelling
    variations of a word induced by pronunciations. Then, there is spelling correction
    to identify and rectify errors in spelling. Another potential step is abbreviation
    handling so that *television* and *TV* are treated the same. The result from these
    steps can be further augmented by performing domain-specific term handling. You
    get the drift… there are a lot of steps possible, and, depending on your data
    and the criticality of your application, you may include some of these in your
    processing.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 除了词干提取和词形还原之外，还有许多特定的方法可以处理单词变体。我们有像语音哈希这样的方法来识别由发音引起的单词拼写变体。接着，还有拼写校正来识别和纠正拼写错误。另一个潜在的步骤是缩写处理，使得*television*和*TV*被视为相同的单词。这些步骤的结果可以通过进行领域特定的术语处理进一步增强。你大概明白了……有很多可能的步骤，具体是否使用，取决于你的数据和应用的关键性。
- en: In general, though, the steps we performed together are largely sufficient –
    case normalization, tokenization, stop word, and punctuation removal, followed
    by stemming/lemmatization. These are some common steps that most NLP applications
    include.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，一般来说，我们一起完成的这些步骤已经足够了——大小写规范化、标记化、停用词和标点符号移除，接着是词干提取/词形还原。这些是大多数NLP应用程序中常见的步骤。
- en: Downloading Text Corpora Using NLTK
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用NLTK下载文本语料库
- en: So far, we've performed these steps on dummy data that we created. Now, it's
    time to try out our newly acquired skills on a larger and more authentic text.
    First, let's acquire that text – Lewis Carrol's classic work, "*Alice's Adventures
    in Wonderland*", which is available through Project Gutenberg and accessible through
    NLTK.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在我们创建的虚拟数据上执行了这些步骤。现在，是时候在一个更大、更真实的文本上尝试我们新学到的技能了。首先，让我们获取这个文本——路易斯·卡罗尔的经典作品《爱丽丝梦游仙境》，它通过古腾堡计划提供并可通过
    NLTK 访问。
- en: 'You may need to download the `''gutenberg''` corpus through NLTK. First, import
    NLTK using the following command:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要通过 NLTK 下载 `'gutenberg'` 语料库。首先，使用以下命令导入 NLTK：
- en: '[PRE54]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then, use the `nltk.download()` command to open up an app, that is, the **NLTK
    Downloader** interface (shown in the following screenshot):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 `nltk.download()` 命令打开一个应用程序，也就是 **NLTK 下载器** 界面（如下截图所示）：
- en: '[PRE55]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We can see that the app has multiple tabs. Click the **Corpora** tab:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到该应用程序有多个标签。点击 **语料库** 标签：
- en: '![Figure 4.4: NLTK Downloader'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.4：NLTK 下载器'
- en: '](img/B15385_04_04.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_04.jpg)'
- en: 'Figure 4.4: NLTK Downloader'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：NLTK 下载器
- en: 'In the `Corpora` tab, scroll down until you reach `gutenberg`. If the status
    is `not installed`, go ahead and click the `Download` button in the lower-left
    corner. That should install the `gutenberg` corpus:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `语料库` 标签中，向下滚动直到找到 `gutenberg`。如果状态是 `未安装`，请点击左下角的 `下载` 按钮。那将安装 `gutenberg`
    语料库：
- en: '![Figure 4.5: NLTK Downloader''s Corpora tab'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.5：NLTK 下载器的语料库标签'
- en: '](img/B15385_04_05.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_05.jpg)'
- en: 'Figure 4.5: NLTK Downloader''s Corpora tab'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：NLTK 下载器的语料库标签
- en: 'Close the interface. Now, you can access some classic texts right from NLTK.
    We''ll read in the text and store it in a variable:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 关闭界面。现在，你可以直接从 NLTK 获取一些经典文本。我们将读取文本并将其存储在一个变量中：
- en: '[PRE56]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The text is stored in `alice_raw`, which is one big character string. Let''s
    have a look at the first few characters of this string:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 该文本存储在 `alice_raw` 中，这是一个大字符串。我们来看一下这个字符串的前几个字符：
- en: '[PRE57]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The output will be as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE58]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: We can see the raw text in the output, which contains the usual imperfections
    that we expect – varying case, stop words, punctuation, and so on.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在输出中看到原始文本，它包含了我们预期的常见不完美——大小写不一、停用词、标点符号等等。
- en: We're ready. Let's test out our skills through an activity.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备好了。让我们通过一个活动来测试我们的技能。
- en: 'Activity 4.01: Text Preprocessing of the ''Alice in Wonderland'' Text'
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 4.01：‘爱丽丝梦游仙境’文本的文本预处理
- en: 'In this activity, you will apply all the preprocessing steps you''ve learned
    about so far to a much larger, real text. We''ll work with the text for Alice
    in Wonderland that we stored in the `alice_raw` variable:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你将把到目前为止学到的所有预处理步骤应用到一个更大、更真实的文本中。我们将处理存储在 `alice_raw` 变量中的《爱丽丝梦游仙境》文本：
- en: '[PRE59]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The text currently looks like this:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的文本看起来是这样的：
- en: '[PRE60]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: By the end of this activity, you will have cleaned and tokenized the data, removed
    a lot of imperfections, removed stop words and punctuation, and have applied stemming
    on the data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动结束时，你将完成数据的清洗和分词，去除许多不完美之处，去除停用词和标点符号，并对数据应用词干提取。
- en: Note
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before beginning this activity, make sure you have the `gutenberg` corpus installed
    and the `alice_raw` variable created, as shown in the previous section titled
    *Downloading Text Corpora Using NLTK*.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始这个活动之前，确保你已经安装了 `gutenberg` 语料库，并创建了 `alice_raw` 变量，正如前一节 *使用 NLTK 下载文本语料库*
    所示。
- en: 'The following are the steps you need to perform:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是你需要执行的步骤：
- en: Continuing in the same Jupyter Notebook, use the raw text in the `'alice_raw'`
    variable. Change the raw text to lowercase.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一个 Jupyter Notebook 中，使用 `'alice_raw'` 变量中的原始文本。将原始文本转换为小写。
- en: Tokenize the sentences.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对句子进行分词。
- en: Import punctuation from the `string` module and the stop words from NLTK.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `string` 模块导入标点符号，从 NLTK 导入停用词。
- en: Create a variable holding the contextual stop words, that is, `--` and `said`.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个变量，用于保存上下文中的停用词，即 `--` 和 `said`。
- en: Create a master list for stop words to remove that contain terms from punctuation,
    NLTK stop words and contextual stop words.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含标点符号、NLTK 停用词和上下文停用词的主停用词列表，以便去除这些词。
- en: Define a function to drop these tokens from any input sentence (tokenized).
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，从任何输入的句子（已分词）中删除这些标记。
- en: Use the `PorterStemmer` algorithm from NLTK to perform stemming on the result.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 NLTK 的 `PorterStemmer` 算法对结果进行词干提取。
- en: Print out the first five sentences from the result.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出结果中的前五个句子。
- en: Note
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 405.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的详细步骤、解决方案和额外的评论内容在第405页上有介绍。
- en: 'The expected output looks like this:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的输出结果是这样的：
- en: '[PRE61]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Let's take a look at what we have achieved so far and what lies ahead.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看到目前为止我们取得了哪些成果，以及未来还有哪些挑战。
- en: So far, we've learned how to perform text preprocessing – the process of getting
    the text data ready for our primary analysis/model. We started with raw text data
    that has, potentially, many imperfections. We learned how to handle many of these
    imperfections and are now at a juncture where we are comfortable with handling
    text data and getting it ready for further analysis. This is an important first
    part in any NLP application. So, we took raw text data and got clean data in return.
    What's next?
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何进行文本预处理——即为我们的主要分析/模型准备文本数据的过程。我们从原始文本数据开始，这些数据可能存在许多不完美的地方。我们学习了如何处理这些不完美，现在已经能够熟练地处理文本数据，并为进一步的分析做好准备。这是任何自然语言处理应用中的重要第一步。因此，我们从原始文本数据开始，得到了清洁的数据。接下来呢？
- en: The next section is a very important one since it has a very strong bearing
    on the quality of your analysis. It is known as representation. Let's discuss
    it.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的部分非常重要，因为它对你的分析质量有着极大的影响。它被称为表示（Representation）。让我们来讨论一下这个问题。
- en: Text Representation Considerations
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文本表示的考虑因素
- en: We have processed our raw input data into cleaned text. Now, we need to transform
    this cleaned text into something a predictive model understands. But what does
    a predictive model understand? Does it understand the different words? Does it
    read a word as we do? Can it work with the text that we supply to it?
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将原始输入数据处理成了干净的文本。现在，我们需要将这些干净的文本转换为预测模型可以理解的格式。那么，预测模型到底能理解什么呢？它能理解不同的单词吗？它能像我们一样阅读单词吗？它能处理我们提供的文本吗？
- en: 'By now, you understand that models work on numbers. The input to a model is
    a stream of numbers. It doesn''t understand images, but it can work with matrices
    and numbers representing those images. For handling images, the key idea is to
    convert them into numbers and generate features out of them. The idea is the same
    for text: we need to convert the text into numbers, which will act as features
    for the model.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该明白模型是通过数字来工作的。模型的输入是一串数字。它不理解图像，但可以处理表示图像的矩阵和数字。处理图像的关键思想是将图像转换为数字，并从中生成特征。对于文本来说，想法是一样的：我们需要将文本转换为数字，这些数字将作为模型的特征。
- en: '**Representation** is all about converting the text into numbers/features that
    the model understands. Doesn''t sound like there is much to it, right? If you
    think that, then here''s something for you to consider: input features are very
    important for any modeling exercise, and representation is the process of creating
    those features. It has a very significant effect on the outcome of your model
    and is a process that you should pay a great deal of attention to.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '**表示（Representation）**的核心就是将文本转换为模型能够理解的数字/特征。这听起来似乎没什么难的，对吧？如果你这么认为，那么请考虑一下：输入特征对于任何建模任务都至关重要，而表示正是创建这些特征的过程。它对模型的结果有着极为重要的影响，是你应当特别关注的一个过程。'
- en: How do you go about text representation, then? What's the "best" way to represent
    text, if there is such a thing at all? Let's discuss a few approaches.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你应该如何进行文本表示呢？如果有的话，什么是“最佳”的文本表示方法？让我们来讨论几种方法。
- en: Classical Approaches to Text Representation
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本表示的经典方法
- en: 'Text representation approaches have evolved significantly over the years, and
    the advent of neural networks and deep neural networks has made a significant
    impact on the way we now represent text (more on that later). We have come a long
    way indeed: from handcrafting features to marking if a certain word is present
    in the text, to creating powerful representations such as word embeddings. While
    there are a lot of approaches, some more suitable for the task than the others,
    we will discuss a few major classical approaches and work with all of them in
    Python.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 文本表示方法随着时间的推移经历了显著的演变，神经网络和深度神经网络的出现对我们当前的文本表示方式产生了重大影响（稍后会详细讲解）。我们确实走过了很长的一段路：从手工构建特征，标记某个单词是否出现在文本中，到创建强大的表示方法，如词嵌入。尽管有许多方法，其中一些更适合特定任务，我们将在
    Python 中讨论几种主要的经典方法，并实际操作它们。
- en: One-Hot Encoding
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独热编码（One-Hot Encoding）
- en: One-hot encoding is, perhaps, one of the most intuitive approaches toward text
    representation. A one-hot encoded feature for a word is a binary indicator of
    the term being present in the text. It's a simple approach that is easy to interpret
    – the presence or absence of a word. To understand this better, let's consider
    our sample text before stemming, and let's see how one-hot encoding works for
    a particular term of interest, say, `nlp`.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 一热编码可能是文本表示中最直观的方法之一。一个词的一热编码特征是该术语是否出现在文本中的二进制指示符。这是一种简单且容易解释的方法——判断一个词是否存在。为了更好地理解这一点，让我们看看我们去除词干之前的示例文本，并观察一热编码如何作用于特定的目标术语，例如
    `nlp`。
- en: 'Let''s see what the text currently looks like using the following command:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下命令查看当前文本的样子：
- en: '[PRE62]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We can see that the text looks like this:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到文本如下所示：
- en: '[PRE63]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Our word of interest is `nlp`. Here''s what the one-hot encoded feature for
    it would look like:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的词是 `nlp`。它的一热编码特征将如下所示：
- en: '![Figure 4.6: One-hot encoded feature for ''nlp'''
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6：''nlp'' 的一热编码特征](img/B15385_04_06.jpg)'
- en: '](img/B15385_04_06.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_06.jpg)'
- en: 'Figure 4.6: One-hot encoded feature for ''nlp'''
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：'nlp' 的一热编码特征
- en: 'We can see that the feature is `1`, but only for the sentences where the term
    `nlp` is present and is `0` otherwise. We can make such indicator variables for
    each word that we''re interested in. So, if we''re interested in three terms,
    we make three such features:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到该特征为 `1`，但仅在包含术语 `nlp` 的句子中为 `1`，否则为 `0`。我们可以为每个我们感兴趣的词创建这样的指示变量。所以，如果我们对三个术语感兴趣，我们可以为每个术语创建这样的特征：
- en: '![Figure 4.7: One-hot encoded features for ''nlp'', ''deep'', and ''learn'''
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7：''nlp''、''deep'' 和 ''learn'' 的一热编码特征](img/B15385_04_07.jpg)'
- en: '](img/B15385_04_07.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_07.jpg)'
- en: 'Figure 4.7: One-hot encoded features for ''nlp'', ''deep'', and ''learn'''
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：'nlp'、'deep' 和 'learn' 的一热编码特征
- en: Let's recreate this using Python in an exercise.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个练习使用 Python 重新创建这个过程。
- en: 'Exercise 4.03: Creating One-Hot Encoding for Our Data'
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.03：为我们的数据创建一热编码
- en: In this exercise, we will replicate the preceding example. The target terms
    are `nlp`, `deep`, and `learn`. We will create a one-hot encoded feature for these
    terms using our own function and store the result in a `numpy` array.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将复制前面的示例。目标术语是 `nlp`、`deep` 和 `learn`。我们将使用我们自己的函数为这些术语创建一热编码特征，并将结果存储在一个
    `numpy` 数组中。
- en: 'Again, we''ll be using the `txt_words_nostop` variable we created in *Exercise
    4.01*, *Tokenizing, Case Normalization, Punctuation, and Stop Word Removal*. So,
    you will need to continue this exercise in the same Jupyter Notebook. Follow these
    steps to complete this exercise:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将使用在 *练习 4.01* 中创建的 `txt_words_nostop` 变量，即 *分词、大小写标准化、标点符号处理和停用词移除*。所以，你需要在同一个
    Jupyter Notebook 中继续进行此练习。按照以下步骤完成本练习：
- en: 'Print out the `txt_words_nostop` variable to see what we''re working with:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出 `txt_words_nostop` 变量，看看我们正在处理的内容：
- en: '[PRE64]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The output will be as follows:'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE65]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Define a list with the target terms, that is, `"nlp", "deep", "learn"`:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个包含目标术语的列表，即 `"nlp"`, `"deep"`, `"learn"`：
- en: '[PRE66]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Define a function that takes in a single tokenized sentence and returns a `0`
    or `1` for each target term, depending on its presence in the text. Note that
    the length of the output is fixed at `3`:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，该函数接受一个单一的分词句子，并根据目标术语是否出现在文本中返回 `0` 或 `1`。注意，输出的长度固定为 `3`：
- en: '[PRE67]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: We're iterating over the target terms and checking if they're available in the
    input sentence.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们正在遍历目标术语，并检查它们是否出现在输入句子中。
- en: 'Apply the function to each sentence in our text and store the result in a variable:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对文本中的每个句子应用该函数，并将结果存储在一个变量中：
- en: '[PRE68]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Import `numpy`, create a `numpy` `array` from the result, and print it:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `numpy`，根据结果创建一个 `numpy` `array`，并打印出来：
- en: '[PRE69]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The array''s output is as follows:'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数组的输出如下：
- en: '[PRE70]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: We can see that the output contains four rows, one for each sentence. Each of
    the columns in the array contains the one-hot encoding for a target term. The
    values for "learn" are 0, 1, 0, 1, which is consistent with our expectations.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到输出包含四行，每行对应一个句子。数组中的每一列包含一个目标术语的一热编码。对于 "learn" 来说，其值是 0, 1, 0, 1，这与我们的预期一致。
- en: Note
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参见 [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf)。
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/38Gr54r](https://packt.live/38Gr54r) 上在线运行这个示例。你必须执行整个笔记本才能获得预期的结果。
- en: In this exercise, we saw how we can generate features from text using one-hot
    encoding. The example used a list of target terms. This may work when you have
    a very specific objective in mind where we know exactly which terms are useful.
    Indeed, this was the method that was heavily employed until a few years ago, where
    people handcrafted features from text. In many situations, this is not feasible
    – since we don't know exactly which terms are important, we use one-hot encoding
    for a large number of terms (5,000, 10,000, or even more).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们展示了如何使用独热编码从文本中生成特征。示例使用了一个目标术语列表。当你有一个非常明确的目标，知道哪些术语是有用的时，这种方法可能会有效。事实上，直到几年前，这正是人们广泛采用的方法，人们从文本中手工提取特征。在许多情况下，这并不可行——因为我们并不确切知道哪些术语是重要的，所以我们对大量术语（5000、10000，甚至更多）使用独热编码。
- en: The other aspect is whether the presence/absence of the term enough for most
    situations. Do we not want to include more information? Maybe the frequency of
    the term instead of just its presence, or maybe even some other smarter measure?
    Let's see how this works.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个方面是术语的存在/缺失是否足以应对大多数情况。我们是否不想包含更多的信息？也许是术语的频率，而不仅仅是它的存在，或者甚至可能是其他更智能的度量？让我们看看这如何工作。
- en: Term Frequencies
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 术语频率
- en: 'We discussed that one-hot encoding merely indicates the presence or absence
    of a term. A reasonable argument here is that the frequency of terms is also important.
    It may be that a term that''s present more times in a document is more important
    for the document. Maybe representing the term by its frequency is a better approach
    than simply the indicator. The frequency approach is straightforward – for each
    term, count the number of times it appears in a particular text. If a term is
    absent from the document/text, it gets a 0\. We do this for all the terms in our
    vocabulary. Therefore, we have as many features as the number of words in our
    vocabulary (something we can choose; this can be thought of as a hyperparameter).
    We should note that after the preprocessing steps, the "*terms*" that we''re working
    with are tokens that may not be valid words in the language:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论过，独热编码只是表示某个术语是否存在。这里有一个合理的观点：术语的频率也很重要。可能在文档中出现次数更多的术语对该文档更为重要。也许通过术语的频率来表示比仅仅通过指示符更好。频率方法很简单——对于每个术语，统计它在特定文本中出现的次数。如果某个术语在文档/文本中缺失，它的频率为0。我们对词汇表中的所有术语都做这个处理。因此，我们的特征数等于词汇表中单词的数量（这个数量我们可以选择；它可以被看作是一个超参数）。我们应该注意，在预处理步骤后，我们正在处理的“*术语*”是可能在语言中不是真正有效单词的标记（token）：
- en: 'Note:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: The *vocabulary* is the superset of all the terms that we'll use in the final
    model. Vocabulary size refers to the number of unique terms in the vocabulary.
    You could have 20,000 unique terms in the raw text but choose to work with the
    most frequent 10,000 terms; this would be the effective vocabulary size.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '*词汇表*是我们最终模型中将使用的所有术语的超集。词汇表大小是指词汇表中唯一术语的数量。你可能在原始文本中有20000个唯一的术语，但选择使用最频繁的10000个术语；这就是有效的词汇表大小。'
- en: Consider the following image; if we had *N* documents and had *V* (`t1, t2,
    t3` … `t`V) words in our working vocabulary, the representation for the data would
    be a matrix of dimensions *N × V*.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下图像；如果我们有*N*个文档，并且在我们的工作词汇表中有*V*（`t1, t2, t3` … `tV`）个词，那么数据的表示将是一个*N × V*维度的矩阵。
- en: '![Figure 4.8: Document-term matrix'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8：文档-术语矩阵'
- en: '](img/B15385_04_08.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_08.jpg)'
- en: 'Figure 4.8: Document-term matrix'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8：文档-术语矩阵
- en: This matrix is our **Document-Term Matrix** (**DTM**) – where each row represents
    a document, and each column represents a term. The values in the cells can represent
    some measure (count, or any other measure). We'll work with term frequencies in
    this section.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵就是我们的**文档-术语矩阵**（**DTM**）——其中每一行代表一个文档，每一列代表一个术语。单元格中的值可以代表某种度量（计数或其他任何度量）。在这一部分，我们将处理术语频率。
- en: 'We could create our own function again, but we have a very handy utility called
    `''CountVectorizer''` for this in `scikit-learn` that we''ll use instead. Let''s
    familiarize ourselves with it, beginning by importing the utility:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次创建自己的函数，但我们有一个非常方便的工具叫做`'CountVectorizer'`，它在`scikit-learn`中可用，我们将使用它。让我们先通过导入该工具来熟悉它：
- en: '[PRE71]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The vectorizer can work with raw text, as well as tokenized data (as in our
    case). To work on the raw text, we would use the following code, where we will
    create a DTM with term frequencies from our raw text (`txt_sents`).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化器可以处理原始文本，也可以处理分词后的数据（如我们的案例）。为了处理原始文本，我们将使用以下代码，在其中我们将从原始文本（`txt_sents`）创建一个包含词频的文档-术语矩阵（DTM）。
- en: 'Before we begin, let''s take a quick look at the contents of this variable:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，先快速查看一下这个变量的内容：
- en: '[PRE72]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output should be as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该如下所示：
- en: '[PRE73]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Note
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If the contents of the `txt_sents` variable have been overwritten while working
    on *Activity 4.01,* *Text Preprocessing of the 'Alice in Wonderland' Text*, you
    can revisit *Step 3* of *Exercise 4.01,* *Tokenizing, Case Normalization, Punctuation,
    and Stop Word Removal* and redefine the variable so that its contents match the
    preceding output.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在处理*活动 4.01*时，`txt_sents`变量的内容被覆盖了，*《爱丽丝梦游仙境》文本的文本预处理*，你可以重新访问*练习 4.01*的*第3步*，*标记化、大写规范化、标点符号和停用词去除*，并重新定义该变量，使其内容与前面的输出匹配。
- en: 'Now, let''s instantiate the vectorizer. Note that we need to provide the vocabulary
    size. This picks the top *n* terms from the data for creating the matrix:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实例化矢量化器。请注意，我们需要提供词汇表的大小。这样会从数据中选择前* n *个术语来创建矩阵：
- en: '[PRE74]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We chose five terms here; the result will contain five columns in the matrix.
    Let''s train (`''fit''`) the vectorizer on the data:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里选择了五个术语；结果将包含矩阵中的五列。让我们在数据上训练（`'fit'`）矢量化器：
- en: '[PRE75]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The vectorizer has now learned a vocabulary – the top five terms – and has
    created an index for each term in the vocabulary. Let''s have a look at the vocabulary:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，矢量化器已经学习了一个词汇表——前五个术语，并为词汇表中的每个术语创建了一个索引。让我们看看这个词汇表：
- en: '[PRE76]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The preceding attribute gives us the following output:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的属性给出了如下输出：
- en: '[PRE77]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: We can see which terms have been picked (the top five).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到已经选择的术语（前五个）。
- en: 'Now, let''s apply the vectorizer to the data to create the DTM. A minor detail:
    the result from a vectorizer is a sparse matrix. To view it, we''ll convert it
    into an array:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将矢量化器应用于数据以创建DTM。一个小细节：矢量化器的结果是一个稀疏矩阵。为了查看它，我们将其转换为数组：
- en: '[PRE78]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Have a look at the output:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下输出：
- en: '[PRE79]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The second document (the second row) has a frequency of `2` for the last two
    terms. What are those terms? Well, indices 3 and 4 are the terms `''together''`
    and `''we''`, respectively. Let''s print out the original text to see if the output
    is as expected:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个文档（第二行）的最后两个术语的频率为`2`。那两个术语是什么呢？索引3和4分别是术语`'together'`和`'we'`。让我们打印出原始文本，看看输出是否符合预期：
- en: '[PRE80]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The output will be as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE81]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: This is just as we expected, and it looks like the count vectorizer works just
    fine.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这正如我们预期的那样，看起来计数矢量化器工作得很好。
- en: 'Notice that the vectorizer tokenizes the sentence as well. If you don''t want
    that and want to use preprocessed tokens instead (`txt_words_stem`), you simply
    need to pass a dummy tokenizer and preprocessor to `CountVectorizer`. Let''s see
    how that works. First, we create a function that does nothing and simply returns
    the tokenized sentence/document:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，矢量化器也对句子进行了标记化。如果你不想这样，并且希望使用预处理过的标记（`txt_words_stem`），只需要传递一个虚拟的标记化器和预处理器给`CountVectorizer`。让我们看看这样做的效果。首先，我们创建一个什么都不做的函数，它只会返回标记化后的句子/文档：
- en: '[PRE82]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Now, we''ll instantiate the vectorizer to use this function as the preprocessor
    and tokenizer:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将实例化矢量化器，使用这个函数作为预处理器和标记化器：
- en: '[PRE83]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Here, we''re fitting and transforming the data in one step using the `fit_transform()`
    method from the tokenizer, and then we view the result. The method identifies
    the unique terms as the *vocabulary* when fitting on the data, then counts and
    returns the occurrence of each term for each document when transforming. Let''s
    see it in action:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用标记化器的`fit_transform()`方法一步完成数据拟合和转换，然后查看结果。该方法在对数据进行拟合时，会将唯一的术语识别为*词汇表*，然后在转换时统计并返回每个文档中每个术语的出现次数。让我们看看它是如何工作的：
- en: '[PRE84]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The output array will be as follows:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 输出数组将如下所示：
- en: '[PRE85]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'We can see that the output is different from that of the previous result. Is
    this difference expected? To understand, let''s look at the vocabulary of the
    vectorizer:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，输出与之前的结果不同。这种差异是预期的吗？为了理解，我们来看一下矢量化器的词汇：
- en: '[PRE86]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The output will be as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE87]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'We''re working with preprocessed data, remember? We have already removed stop
    words and stemmed. Let''s try printing out the input data just to be sure:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在处理预处理后的数据，记得吗？我们已经移除了停用词并进行了词干提取。让我们试着打印一下输入数据，确保没有问题：
- en: '[PRE88]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'The output will be as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE89]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: We can see that the DTM is working according to the new vocabulary and the frequencies
    that were obtained after preprocessing.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，DTM（文档-词项矩阵）是根据新词汇表和预处理后获得的词频进行工作的。
- en: So, this was the second approach of generating features from text data, that
    is, using the frequencies of the terms. In the next section, we will look at another
    very popular method.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这是第二种从文本数据生成特征的方法，即使用术语的频率。在下一部分，我们将介绍另一种非常流行的方法。
- en: The TF-IDF Method
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF方法
- en: Does the high frequency of a term in a document mean that the word is very important
    for the document? Not really. What if that term is very common in all the documents?
    A common assumption that's employed in text data handling is that if a term is
    present in all documents, it may not be very differentiating or important for
    this particular document at hand. Seems like a reasonable assumption. Once more,
    let's consider the example of the term "*mobile*" when we're working with mobile
    phone reviews. The term is likely to be present in a very high proportion of reviews.
    But if your task is identifying the sentiment in the reviews, the term may not
    add a lot of information.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 文档中术语的高频率是否意味着该词对文档非常重要？不一定。如果该术语在所有文档中都很常见呢？在文本数据处理中有一个常见的假设：如果一个术语出现在所有文档中，那么它对于当前文档的区分度可能不大，或者说它对当前文档的意义不大。这个假设看起来是合理的。再举个例子，假设我们在处理手机评论时遇到“*手机*”这个词。这个词可能出现在大多数评论中。但如果你的任务是识别评论中的情感，这个词可能不会提供太多有用的信息。
- en: We can bump up the importance of terms that are present in the document but
    rare in the entire data and decrease the importance of terms that are present
    in most of the documents.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提高在文档中出现但在整个数据中较为罕见的术语的重要性，降低在大多数文档中出现的术语的重要性。
- en: 'The TF-IDF method, which stands for *Term Frequency – Inverse Document Frequency*,
    defines **Inverse Document Frequency** (**IDF**) as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF方法，即*词频-逆文档频率*，将**逆文档频率**（**IDF**）定义如下：
- en: '![Figure 4.9: Equation for TF-IDF'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.9：TF-IDF的公式'
- en: '](img/B15385_04_09.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_09.jpg)'
- en: 'Figure 4.9: Equation for TF-IDF'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9：TF-IDF的公式
- en: '*n* is the total number of documents, while *df(t)* is the number of documents
    where the term *t* occurs. This is used as a factor to adjust the term frequency.
    You can see that it works just as we want it to – it increases the importance
    for rare terms, and decreases it for common terms. Note that there are variations
    to this formula, but we''ll stick with what `scikit-learn` uses. Like `CountVectorizer`,
    the TF-IDF vectorizer tokenizes the sentence and learns the vocabulary, but instead
    of returning the counts for a term in a document, it returns the adjusted (IDF-multiplied)
    counts.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*是文档的总数，而*df(t)*是包含术语*t*的文档数。这作为一个因子来调整术语频率。可以看出，它正如我们所期望的那样工作——它增加了稀有术语的重要性，减少了常见术语的重要性。请注意，这个公式有不同的变体，但我们将坚持使用`scikit-learn`所采用的方法。像`CountVectorizer`一样，TF-IDF向量化器对句子进行分词并学习词汇表，但它返回的是调整后的（乘以IDF的）术语频率，而不是文档中术语的原始计数。'
- en: Now, let's apply this interesting new approach to our data.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将这个有趣的新方法应用到我们的数据上。
- en: 'Exercise 4.04: Document-Term Matrix with TF-IDF'
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.04：使用TF-IDF生成文档-词矩阵
- en: 'In this exercise, we''ll implement the third approach to feature generation
    from text – TF-IDF. We will use scikit-learn''s `TfidfVectorizer` utility and
    create the DTM for our raw text data. Since we''re using the `txt_sents` variable
    we created earlier in this chapter, we''ll need to use the same Jupyter Notebook.
    The text contained in the variable currently looks like this:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将实现从文本生成特征的第三种方法——TF-IDF。我们将使用scikit-learn的`TfidfVectorizer`工具，并为我们的原始文本数据创建文档-词矩阵（DTM）。由于我们使用的是本章之前创建的`txt_sents`变量，因此我们需要使用相同的Jupyter
    Notebook。当前变量中的文本如下所示：
- en: '[PRE90]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Note
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If the contents of the `txt_sents` variable have been overwritten while working
    on *Activity 4.01*, *Text Preprocessing of the 'Alice in Wonderland' Text*, you
    can revisit *Step 3* of *Exercise 4.01*, *Tokenizing, Case Normalization, Punctuation,
    and Stop Word Removal* and redefine the variable so that its contents match the
    preceding output.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在处理*活动4.01*（*《爱丽丝梦游仙境》文本的文本预处理*）时，`txt_sents`变量的内容被覆盖，可以重新访问*练习4.01*的*步骤3*（*分词、大小写标准化、标点符号和停用词移除*），并重新定义该变量，以便其内容与之前的输出匹配。
- en: 'The following are the steps to perform:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是需要执行的步骤：
- en: 'Import the `TfidfVectorizer` utility from `scikit learn`:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`scikit learn`导入`TfidfVectorizer`工具：
- en: '[PRE91]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Instantiate the `vectorizer` with a vocabulary size of `5`:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用词汇表大小为`5`来实例化`vectorizer`：
- en: '[PRE92]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Fit the `vectorizer` on the raw data of `txt_sents`:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在原始数据`txt_sents`上拟合`vectorizer`：
- en: '[PRE93]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Print out the vocabulary learned by the `vectorizer`:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出`vectorizer`学习到的词汇表：
- en: '[PRE94]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The trained vocabulary will look as follows:'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练后的词汇表如下所示：
- en: '[PRE95]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Notice that the vocabulary is the same as that of the count vectorizer. This
    is expected. We're not changing the vocabulary; we're adjusting its importance
    for the documents.
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，词汇表与计数向量化器的词汇表相同。这是预期的。我们并没有改变词汇表；我们只是调整了它在文档中的重要性。
- en: 'Transform the data using the trained vectorizer:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的向量化器转换数据：
- en: '[PRE96]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Print out the resulting DTM:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出结果的文档-术语矩阵（DTM）：
- en: '[PRE97]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'The output will be as follows:'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE98]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: We can clearly see that the output values are different from the frequencies
    and that the values less than 1 indicate that many values have been lowered after
    multiplication with IDF.
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，输出值与频率不同，并且小于1的值表明在与IDF相乘后，许多值已经降低。
- en: 'We also need to see the IDF for each of the terms in the vocabulary to check
    if the factor is indeed working as we expect it to. Print out the IDF values for
    the terms using the `idf_` attribute:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要查看每个词汇表中术语的IDF值，以检查该因子是否如我们预期的那样起作用。使用`idf_`属性打印术语的IDF值：
- en: '[PRE99]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'The output will be as follows:'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE100]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: The terms `'and'`, `'deep'`, and `'learn'` have a lower IDF, while the terms
    `'together'` and `'we'` have a higher IDF. This is just as we expect it to be
    – the terms `'together'` and `'we'` appear only in one document, while the others
    appear in two. So, the TF-IDF scheme is indeed giving more importance to rarer
    words.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 术语`'and'`、`'deep'`和`'learn'`的IDF较低，而术语`'together'`和`'we'`的IDF较高。这正如我们所预期的——术语`'together'`和`'we'`只出现在一个文档中，而其他词汇出现在两个文档中。因此，TF-IDF方法确实赋予了稀有词汇更高的重要性。
- en: Note
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf)。
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行此示例，访问 [https://packt.live/38Gr54r](https://packt.live/38Gr54r)。你必须执行整个Notebook才能获得所需的结果。
- en: In this exercise, we saw how we can represent text using the TF-IDF approach.
    We also saw how the approach downweighs more frequent terms by noticing that the
    IDF values were lower for higher-frequency terms. We ended up with a DTM containing
    the TF-IDF values for the terms.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们看到了如何使用TF-IDF方法来表示文本。我们还看到了该方法如何通过注意到IDF值在高频词汇中的较低值来降低更频繁的术语的重要性。最后，我们得到了一个包含术语TF-IDF值的文档-术语矩阵（DTM）。
- en: Summarizing the Classical Approaches
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结经典方法
- en: We've just looked at three approaches to the classical way of text representation.
    We began with one-hot encoding, where the feature for a term was simply marking
    its presence in a document. The count/frequency-based approach attempted to add
    the importance of the term by using its frequency in a document. The TF-IDF approach
    attempted to use a "normalized" importance value of the term, factoring in how
    common the term is across the documents.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看了三种经典的文本表示方法。我们从独热编码开始，其中术语的特征只是标记其在文档中的存在。基于计数/频率的方法试图通过使用术语在文档中的频率来增加术语的重要性。TF-IDF方法试图使用术语的"归一化"重要性值，考虑术语在文档中的常见程度。
- en: All three approaches that we've discussed so far fall under the "*Bag of Words*"
    approach to representation. So, why are they called "*Bag of Words*"? For a couple
    of reasons. The first reason is that they don't retain the order of the tokens
    – once in the bag, the position of the terms/tokens doesn't matter. The second
    reason is that this approach retains features for individual terms. So, for each
    document, you have, in a way, a "mixed bag of tokens", or a "*bag of words*",
    for simplicity.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止我们讨论的三种方法都属于"*词袋模型*"表示方法。那么，为什么它们被称为"*词袋模型*"呢？原因有几个。第一个原因是它们不会保留词汇的顺序——一旦进入词袋，术语/词汇的位置就不重要。第二个原因是这种方法保留了每个单独术语的特征。所以，对于每个文档来说，从某种意义上讲，你有一个"混合的词汇袋"，或者为了简单起见，称为"*词袋*"。
- en: The result from all three approaches had a dimensionality of *N × V*, where
    *N* is the number of documents and *V* is the vocabulary size. Note that all three
    representations are very sparse – a typical sentence is very short (maybe 20 words),
    but the vocabulary size is typically in the thousands, resulting in most of the
    cells of the DTM being 0\. This doesn't seem ideal. Well, there's this and a few
    more shortcomings of such representations, which we'll see shortly, that have
    led to the success of deep learning-based methods for representation. Let's discuss
    these ideas next.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种方法的结果具有*N × V*的维度，其中*N*是文档数量，*V*是词汇表的大小。请注意，三种表示方式都非常稀疏——一个典型的句子非常短（可能只有20个单词），但词汇表的大小通常在数千个单词左右，导致大多数DTM的单元格都是0。这看起来并不理想。好吧，还有这种表示方法的其他几个缺点，稍后我们会看到，这些缺点导致了基于深度学习的表示方法的成功。接下来，我们来讨论这些思想。
- en: Distributed Representation for Text
  id: totrans-393
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本的分布式表示
- en: Why are word embeddings so popular? Why are we claiming they are amazingly powerful?
    What makes them so special? To understand and appreciate word embeddings, we need
    to acknowledge the shortcomings of the representations so far.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么词嵌入如此受欢迎？为什么我们说它们非常强大？是什么让它们如此特别？为了理解和欣赏词嵌入，我们需要承认迄今为止表示方法的不足之处。
- en: The terms "*footpath*" and "*sidewalk*" are synonyms. Do you think the approaches
    we've discussed so far will be able to capture this information? Well, you could
    manually go in and replace "*sidewalk*" with "*footpath*" so that both have the
    same token eventually, but can you do this for all possible synonyms in the language?
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '"*人行道*"和"*侧道*"是同义词。你认为我们迄今为止讨论的方法能够捕捉到这一信息吗？好吧，你可以手动将"*侧道*"替换为"*人行道*"，这样两者最终就会有相同的词标记，但你能为语言中的所有同义词做这件事吗？'
- en: The terms "*hot*" and "*cold*" are antonyms. Do the previous Bag-of-Words representations
    capture this? What about "*dog*" being a type of "*animal*"? "*Cockpit*" being
    a part of a "*plane*"? Differentiating between a dog's bark and a tree's bark?
    Can you handle all these cases manually?
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '"*热*"和"*冷*"是反义词。之前的词袋模型表示能够捕捉到这一点吗？那么"*狗*"是"*动物*"的一种类型呢？"*驾驶舱*"是"*飞机*"的一部分呢？区分狗的叫声和树皮的声音呢？你能手动处理所有这些情况吗？'
- en: All the preceding are examples of "**semantic associations**" between the terms
    – simply put, their meanings are linked in some way or another. Bag-of-words representations
    can't capture these. This is where the notion of distributional semantics comes
    in. The key idea of distributional semantics is that terms with similar distributions
    have similar meanings.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有例子都是术语之间的“**语义关联**”——简而言之，它们的含义在某种程度上是相互关联的。词袋模型无法捕捉到这些关联。这就是分布式语义学的作用所在。分布式语义学的核心思想是，具有相似分布的词语具有相似的含义。
- en: 'A quick, fun quiz for you: Guess the meaning of the term *furbaby* from the
    following text:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速、有趣的小测验：从以下文本中猜测*小毛孩*这个词的意思：
- en: '"*I adopted a young Persian furbaby a month back. Like all furbabys, it loves
    to scratch its back and hates water, but unlike other furbabys, it miserably fails
    at catching a mouse.*"'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '"*我一个月前收养了一只年轻的波斯小毛孩。像所有的小毛孩一样，它喜欢抓背部，讨厌水，但和其他小毛孩不同，它在抓老鼠方面完全失败了。*"'
- en: 'You may have guessed it right: *furbaby* is referring to a cat. This was easy,
    wasn''t it?'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能猜得对：*小毛孩*是指猫。这个很简单，对吧？
- en: 'But how did you do that? Nowhere has the term cat been used. You looked at
    the context (the terms surrounding it) for "*furbaby*" and, based on your understanding
    of language and the world, you figured that these terms are generally associated
    with cats. You intuitively used this notion: words with similar meaning appear
    in similar contexts. If "*furbaby*" and "*cat*" appeared in similar contexts,
    their meaning must be similar.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 但你是怎么做到的呢？文中从未使用过“猫”这个词。你查看了上下文（围绕“*小毛孩*”的词语），并根据你对语言和世界的理解，推测这些词通常和猫相关。你直觉地运用了这个概念：具有相似意义的词语出现在相似的语境中。如果“*小毛孩*”和“猫”出现在相似的语境中，它们的意思一定相似。
- en: '"*You shall know a word by the company it keeps.*"'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '"*你将通过一个词所处的环境了解这个词的含义。*"'
- en: This famous, and now overused, quote by John Firth captures this idea very well.
    It's overused for all the right reasons. Let's see how this notion is employed
    in word embeddings.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这句由约翰·弗斯（John Firth）所说的名言很经典，现在已被过度使用，但正因为如此它才被过度使用。让我们看看这个概念是如何在词嵌入中运用的。
- en: Word Embeddings and Word Vectors
  id: totrans-404
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词嵌入与词向量
- en: Word embeddings are representations of each term as a vector with low dimensionality.
    The one-hot encoded representation for a term was also a vector, but with dimensionality
    in the several thousands. Word embeddings/word vectors have much lower dimensionality
    and result from distributional semantics-based approaches – essentially, the representation
    captures the notion that words with similar meanings appear in similar contexts.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是将每个术语表示为一个低维度的向量。术语的独热编码表示也是一个向量，但其维度通常达到几千。词嵌入/词向量具有更低的维度，并且来自基于分布语义的方法——本质上，表示捕捉到的是这样一种观念：具有相似意义的词语出现在相似的上下文中。
- en: Word vectors attempt to capture the meanings of terms. This idea makes them
    very powerful, assuming, of course, they have been created correctly. With word
    vectors, vector operations such as adding/subtracting vectors and dot products
    are possible and have some very interesting meanings. There is also this great
    property that items with similar meanings are spatially closer. All of this leads
    to some amazing results.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量试图捕捉术语的含义。这一思想使得它们非常强大，当然，前提是它们已经被正确创建。使用词向量时，像加法/减法向量和点积这样的向量操作是可能的，而且有一些非常有趣的意义。另一个非常棒的特性是，含义相似的项在空间上更接近。这一切都导致了一些令人惊奇的结果。
- en: 'A very interesting result is that word vectors can perform well on analogy
    tasks. Analogy tasks are defined as tasks of the format – "*a* is to *b* as *x*
    is to ?" – that is, find an entity that has the same relation to *x* as *b* has
    to *a*. As an example, if you ask "man is to uncle as a woman is to ?", the result
    would be "`aunt`" (more on this later). You can also find out semantic regularities
    between terms – the relationships between terms and sets of terms. Let''s look
    at the following figure, which is based on word vectors/embeddings, to understand
    this better:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常有趣的结果是，词向量可以在类比任务上表现得非常好。类比任务被定义为这种格式的任务——“*a* 与 *b* 之关系，*x* 与 ? 的关系是？”——也就是说，找到一个与
    *x* 之间关系相同的实体，正如 *b* 与 *a* 之间的关系。举个例子，如果你问“man 对 uncle 如 woman 对 ?”，结果会是 "`aunt`"（稍后会详细讲解）。你还可以发现术语之间的语义规律——术语与术语集之间的关系。让我们看看下图，它基于词向量/嵌入，帮助我们更好地理解：
- en: '![Figure 4.10: Semantic relationships between terms'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.10：术语之间的语义关系'
- en: '](img/B15385_04_10.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_10.jpg)'
- en: 'Figure 4.10: Semantic relationships between terms'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10：术语之间的语义关系
- en: The preceding figure shows some examples. The vectors can have high dimensionality
    (up to 300 or even more), so dimensionality reduction to two dimensions is performed
    to visualize it. The dotted connection between the two terms represents the relation
    between the terms. The direction of this connection is the important bit. On the
    left panel, we can see that the segment connecting `slow` and `slower` is parallel
    to the segment connecting `short` and `shorter`. What does this mean? This means
    that the word embeddings learned that the relationship between `short` and `shorter`
    is the same as the relationship between `slow` and `slower`. Likewise, the embeddings
    learned that the relationship between `clearer` and `clearest` is the same as
    that between `darker` and `darkest`. Pretty neat, right?
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图示展示了一些例子。向量可以具有很高的维度（最多可达 300 甚至更多），因此进行二维的降维处理以便进行可视化。两个术语之间的虚线连接表示术语之间的关系。连接的方向是关键。在左侧面板中，我们可以看到，连接
    `slow` 和 `slower` 的线段与连接 `short` 和 `shorter` 的线段是平行的。这是什么意思？这意味着词嵌入学到的知识是，`short`
    和 `shorter` 之间的关系与 `slow` 和 `slower` 之间的关系是相同的。同样，嵌入学习到的知识是，`clearer` 和 `clearest`
    之间的关系与 `darker` 和 `darkest` 之间的关系是相同的。很有趣吧？
- en: Similarly, the right-hand side of *Figure 4.10* shows that the embeddings learned
    that the relationship between `sir` and `madam` is the same as that between `king`
    and `queen`. Embeddings have also captured other kinds of semantic associations
    between terms, which we discussed in the previous section. Isn't that amazing?
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，*图 4.10* 的右侧展示了嵌入学到的知识：`sir` 和 `madam` 之间的关系与 `king` 和 `queen` 之间的关系是相同的。嵌入还捕捉到了术语之间的其他语义关联，这些我们在上一节中已经讨论过了。是不是很棒？
- en: This would not be possible with the approaches we discussed earlier. Word embeddings
    truly are working around the "meaning" of terms. We hope you can already appreciate
    the utility and power of word vectors. If you're not convinced yet, we'll soon
    be working with them and will see this for ourselves.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我们之前讨论过的方法中是做不到的。词嵌入真正围绕术语的“意义”进行工作。希望你已经能够欣赏到词向量的实用性和强大功能。如果你还不相信，我们很快就会用它们，并亲自看到这一切。
- en: To generate word embeddings, there are several algorithms we can use. We will
    discuss two major approaches and apprise you of some other popular approaches.
    We will see how the distributional semantics approach is leveraged to derive these
    word embeddings.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成词嵌入，我们可以使用几种不同的算法。我们将讨论两种主要的算法，并介绍一些其他常见的算法。我们将看到分布式语义方法是如何被用来推导这些词嵌入的。
- en: word2vec
  id: totrans-415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: word2vec
- en: 'Back in school, to test if we understood the meaning of certain terms, our
    language teachers used a very popular technique: "*fill in the blanks*". Based
    on the words around it, we needed to identify the word that would best fill that
    blank. If you understood the meaning well, you would do well. Think about this
    – isn''t this distributional semantics?'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在学校时，为了测试我们是否理解某些词汇的意思，语言老师常常使用一种非常流行的技巧：“*填空题*”。根据周围的词汇，我们需要找出最适合填补空白的词。如果你理解了词汇的意思，你就能做得很好。想一想——这不正是分布式语义吗？
- en: In the *'furbaby'* example, you could predict the term `'cat'` because you understood
    the contexts and terms it occurs with. The exercise was effectively a "fill-in-the-blank"
    exercise. You could fill the blank only because you understood the meaning of
    'cat'.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *‘furbaby’* 示例中，你可以预测出词汇‘cat’，因为你理解了与之相关的上下文和词汇。这个练习实际上就是一个“填空”练习。你能填补这个空白，因为你理解了‘cat’的意思。
- en: If you can predict a term given some context around it, you understand the meaning
    of the term.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够根据某个词汇周围的上下文预测出该词，那么你就理解了这个词的意思。
- en: 'This simple idea is exactly the formulation behind the `word2vec` algorithm.
    The `word2vec` algorithm/process is a prediction exercise, a massive "fill-in-the-blank"
    exercise, in a way. In short, this is what the algorithm does:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的思想正是`word2vec`算法背后的公式。`word2vec`算法/过程实际上是一个预测练习，一种庞大的“填空”练习。简而言之，算法的工作原理如下：
- en: Given the contextual words, predict the missing target word.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 给定上下文词汇，预测缺失的目标词。
- en: That's all there is to it. The `word2vec` algorithm predicts the target word
    given the context. Let's understand how these are defined.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。`word2vec`算法根据上下文预测目标词。接下来我们来理解这些是如何被定义的。
- en: 'Consider the sentence, "*The Persian cat eats fish and hates bathing*." We
    define context as some fixed number of the terms to the left and right of the
    target word, which is in the center. For our example, let `''cat''` be the target
    word, and let''s take two words on either side of the target as our context:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 以句子“*波斯猫吃鱼并讨厌洗澡*”为例。我们将上下文定义为目标词左右一定数量的词，对于本例来说，假设“'cat'”是目标词，我们选择其左右两边的两个词作为上下文：
- en: '![Figure 4.11: "cat" as the target term'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.11：“cat”作为目标词'
- en: '](img/B15385_04_11.jpg)'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_11.jpg)'
- en: 'Figure 4.11: "cat" as the target term'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11：“cat”作为目标词
- en: 'The five terms together form a `''window''`, which has the target term at the
    center and the context terms around it. In this example, since we are considering
    two terms on either side, the window size is 2 (more on these parameters later).
    The window is a sliding one and moves over the terms in the sentence. The next
    window would have `''eats''` at the center, with `''cat''` now becoming part of
    the context:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 这五个词汇共同形成一个“窗口”，目标词位于中央，上下文词汇则围绕其周围。在这个例子中，由于我们选择了目标词左右各两个词作为上下文，因此窗口大小为2（稍后会详细介绍这些参数）。窗口是滑动的，它会在句子中的词汇上滑动。下一个窗口将以“'eats'”为中心，而“'cat'”则成为新的上下文词汇：
- en: '![Figure 4.12: Windows for the target term'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.12：目标词的窗口'
- en: '](img/B15385_04_12.jpg)'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_12.jpg)'
- en: 'Figure 4.12: Windows for the target term'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12：目标词的窗口
- en: '`C1`, `C2`, `C3`, and `C4` denote the contexts for each window. In `C3`, "fish"
    is the target word, which is predicted using the terms "cat", "eats", "and", and
    "hates". The formulation is clear, but how does the model learn the representations?
    Let''s discuss that next:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '`C1`、`C2`、`C3` 和 `C4` 表示每个窗口的上下文。在 `C3` 中，“fish”是目标词，它是通过“cat”、“eats”、“and”和“hates”这些词汇预测出来的。公式已经很清楚了，但模型是如何学习这些表示的呢？我们接下来讨论这个问题：'
- en: '![Figure 4.13: The CBOW architecture with an example'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.13：带有示例的CBOW架构'
- en: '](img/B15385_04_13.jpg)'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_13.jpg)'
- en: 'Figure 4.13: The CBOW architecture with an example'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13：带有示例的CBOW架构
- en: The model shown in the preceding figure uses a neural network with a single
    hidden layer. The output layer is for the target term and is one-hot encoded with
    *V* outputs, one for each term – the predicted term, which is `'cat'`, is the
    term that gets `'hot'` in the output, of course. The input layer for the context
    terms is also size *V*, but fires for all the terms in the context. The hidden
    layer is of dimensionality *V x D* (where *D* is the dimension of the vectors).
    This hidden layer is where these magical representations of the terms are learned.
    Note that there is just one input layer, as the weights matrix *W* suggests.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 上图所示的模型使用了一个带有单一隐藏层的神经网络。输出层是用于目标词的，并且是通过*V*个输出进行独热编码，每个词一个输出——预测的词是`'cat'`，它会在输出中变成`'hot'`。上下文词的输入层大小也是*V*，但它对上下文中的所有词都会有响应。隐藏层的维度是*V
    x D*（其中*D*是向量的维度）。这个隐藏层就是学习这些神奇词表示的地方。请注意，只有一个输入层，正如权重矩阵*W*所示。
- en: While the network trains, predicting the target word better with each epoch,
    the parameters of the hidden layer are also getting updates. These parameters
    are effectively D-length vectors for each term. This D-length vector for a term
    is our word embedding for that term. After the iterations complete, we would have
    learned our word embeddings for all the terms in the vocabulary. Pretty neat,
    isn't it?
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络训练的过程中，随着每次迭代，预测目标词的准确性逐步提高，隐藏层的参数也在不断更新。这些参数实际上是每个词的D维向量。这个D维向量就是我们该词的词嵌入。当迭代完成后，我们将学到词汇表中所有词的词嵌入。很棒，不是吗？
- en: 'The approach we just discussed is the CBOW approach to training word vectors.
    The context is a simple bag of words (as we discussed in the previous section
    on classical approaches; order doesn''t matter, remember), hence the name. There
    is another popular approach, the Skip-gram approach, which inverts the approach
    of the CBOW method – it predicts the context words based on the center word. This
    approach may seem a little less intuitive initially but works well. We''ll discuss
    the differences between the results from CBOW and Skip-gram later in this chapter:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才讨论的方法是用于训练词向量的CBOW方法。上下文是一个简单的词袋（正如我们在上一节讨论的经典方法中提到的；顺序不重要，记住这一点），因此得名。还有另一种流行的方法，叫做Skip-gram方法，它与CBOW方法相反——它是根据中心词来预测上下文词汇。这个方法一开始可能看起来不太直观，但效果很好。稍后我们会在本章中讨论CBOW和Skip-gram方法的结果差异：
- en: '![Figure 4.14: The Skip-gram architecture'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.14：Skip-gram架构'
- en: '](img/B15385_04_14.jpg)'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_14.jpg)'
- en: 'Figure 4.14: The Skip-gram architecture'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14：Skip-gram架构
- en: Let's see the CBOW approach in action in Python. We'll create our own word embeddings
    and assess if we can indeed get the amazing results we have been claiming so far.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Python中实际操作一下CBOW方法。我们将创建自己的词嵌入，并评估是否能够得到我们之前所声称的惊人结果。
- en: Training Our Own Word Embeddings
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练我们自己的词嵌入
- en: There are many implementations of the `word2vec` algorithm available in different
    packages. We will use the implementation in **Gensim**, which is a great package
    for many NLP tasks. The implementation of word2vec in Gensim is close to the original
    paper by *Mikolov et al.* in 2013 ([https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)).
    Gensim also supports other algorithms for word embeddings; more on this later.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '`word2vec`算法有许多不同的软件包实现。我们将使用**Gensim**中的实现，它是处理许多自然语言处理任务的优秀工具包。Gensim中的word2vec实现接近Mikolov等人在2013年发布的原始论文（[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)）。Gensim还支持其他词嵌入算法；稍后我们会详细讨论。'
- en: 'If you don''t have Gensim installed, you can install it by typing the following
    command into a Jupyter Notebook:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有安装Gensim，可以通过在Jupyter Notebook中输入以下命令来安装：
- en: '[PRE101]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'The dataset we''ll use is the `text8` corpus ([http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)),
    which is the first billion characters from Wikipedia. It should, therefore, cover
    data from a variety of topics, not specific to one domain. Conveniently, Gensim
    has a utility (the `downloader` API) to read in the data. Let''s read in the data
    after importing the `downloader` utility from Gensim:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集是`text8`语料库（[http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)），它来自维基百科的前十亿个字符。因此，它应该涵盖各种主题的数据，而不是某一个特定领域。方便的是，Gensim提供了一个工具（`downloader`
    API）来读取这些数据。在从Gensim导入`downloader`工具后，让我们来读取这些数据：
- en: '[PRE102]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'This step downloads the `text8` data and can take a while, depending on your
    internet connectivity. Alternatively, the data is available here ([https://packt.live/3gKXU2D](https://packt.live/3gKXU2D))
    to be downloaded and read using the `Text8Corpus` utility in Gensim, as shown
    in the following code:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤下载`text8`数据，下载时间可能较长，具体取决于您的网络连接情况。或者，数据可以通过以下链接下载并使用Gensim中的`Text8Corpus`工具读取，如以下代码所示：[https://packt.live/3gKXU2D](https://packt.live/3gKXU2D)。
- en: '[PRE103]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: The `text8` data is now available as an iterable, which can simply be passed
    to the `word2vec` algorithm.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '`text8`数据现在作为一个可迭代对象，可以直接传递给`word2vec`算法。'
- en: 'Before we train the embeddings, to make the results reproducible, let''s set
    the seed as `1` for random number generation using NumPy:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练嵌入之前，为了使结果具有可重复性，我们使用NumPy将随机数生成的种子设置为`1`：
- en: '[PRE104]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Note
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although we have set the seed, there are more causes for variation of results.
    Some of this is because of an internal hash seed that the Python version on your
    system may use. Using multiple cores can also cause the results to vary. In any
    case, while the values you see may be different, and there could be some changes
    in the order of the results, the output you see should largely agree with ours.
    Note that this applies to all the practical elements pertaining to word vectors
    in this chapter.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已设置了种子，但结果的变化仍然有其他原因。其中一些是由于您的系统上Python版本可能使用的内部哈希种子。使用多个核心也可能导致结果变化。无论如何，尽管您看到的值可能不同，结果的顺序可能有所变化，但您看到的输出应该与我们的输出大致一致。请注意，这适用于本章中与词向量相关的所有实际操作。
- en: 'Now, let''s train our first word embedding by using the `word2Vec` method:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过使用`word2Vec`方法来训练我们的第一个词嵌入：
- en: '[PRE105]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'This may take a minute or two, or less, depending on your system. Once complete,
    we will have our trained word vectors in the model and have access to multiple
    handy utilities to work with these word vectors. Let''s access the word vector/embedding
    for a term:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要一两分钟，或者更短，具体取决于您的系统。完成后，我们将得到训练好的词向量，并可以使用多个方便的工具来处理这些词向量。让我们来访问某个术语的词向量/嵌入表示：
- en: '[PRE106]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'The output will be as follows:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '![Figure 4.15: The embedding for "animal"'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.15: "动物"的嵌入表示'
- en: '](img/B15385_04_15.jpg)'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_04_15.jpg)'
- en: 'Figure 4.15: The embedding for "animal"'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '图4.15: "动物"的嵌入表示'
- en: 'You have a series of numbers – the vector for the term. Let''s find the length
    of the vector:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 你拥有一系列的数字——表示该术语的向量。让我们来计算这个向量的长度：
- en: '[PRE107]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'The length of the vector is as follows:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 向量的长度如下所示：
- en: '[PRE108]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'The representation for each term is now a vector of length 100 (the length
    is a hyperparameter we can change; we used the default setting to get started).
    The vector for any term can be accessed as we did previously. Among the other
    handy utilities is the `most_similar()` method, which helps us find the terms
    that are the most similar to a target term. Let''s see it in action:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 每个术语的表示现在是一个长度为100的向量（这个长度是一个超参数，我们可以更改；我们使用了默认设置来开始）。任何术语的向量都可以像之前那样访问。另一个方便的工具是`most_similar()`方法，它帮助我们找到与目标术语最相似的术语。让我们看看它的实际应用：
- en: '[PRE109]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'The output will be as follows:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '[PRE110]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: The output is a list of tuples, with each tuple containing the term and its
    similarity score with the term "animal".
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果是一个元组列表，每个元组包含一个术语及其与“animal”术语的相似度得分。
- en: We can see `insect`, `animals`, `insects`, and `mammal` in the top-most similar
    terms to "animal". This seems like a very good result, right? But how is the similarity
    being calculated? Words are being represented by vectors, and the vectors are
    trying to capture meaning – the similarity between terms is the similarity between
    their corresponding vectors. The `most_similar()` method uses **cosine similarity**
    between the vectors and returns the terms with the highest values. The value corresponding
    to each term in the result is the cosine similarity with the target word's vector.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到“insect”（昆虫）、“animals”（动物）、“insects”（昆虫）、和“mammal”（哺乳动物）是与“animal”（动物）最相似的术语。这看起来是一个非常好的结果，对吧？但相似度是如何计算的呢？单词是通过向量来表示的，而这些向量试图捕捉意义——术语之间的相似度实际上是它们对应向量之间的相似度。`most_similar()`方法使用**余弦相似度**来计算向量之间的相似度，并返回具有最高值的术语。结果中每个术语对应的值是与目标词向量的余弦相似度。
- en: 'Cosine similarity measures are suitable here as we expect terms that are similar
    in meaning to be spatially together. Cosine similarity is the cosine of the angle
    between the vectors. Terms with similar meaning and representation will have an
    angle closer to 0 and a similarity score closer to 1, whereas terms with completely
    unrelated meanings will have an angle closer to 90, and a cosine similarity closer
    to 0\. Let''s see what the model has learned as top terms related to "happiness":'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，余弦相似度测量很合适，因为我们期望在意义上相似的术语在空间上是在一起的。余弦相似度是向量之间夹角的余弦值。具有相似意义和表示的术语将具有接近0的角度和接近1的相似度分数，而完全无关的意义将具有接近90的角度和接近0的余弦相似度。让我们看看模型在"幸福"相关的顶级术语上学到了什么：
- en: '[PRE111]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'The most similar items turn out to be the following (the most similar ones
    are at the top):'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 最相似的项目结果如下（最相似的排在前面）：
- en: '[PRE112]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: Humanity, mankind, goodness, righteousness, and compassion -- we have some life
    lessons here. It seems to have learned what many people seemingly can't figure
    out in their entire lifetime. Remember, it is just a series of matrix multiplications.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 人类，人类，善良，正义和同情 -- 我们在这里学到了一些生活经验。看起来它已经学到了许多人似乎一辈子也搞不清楚的东西。请记住，这只是一系列矩阵乘法。
- en: Semantic Regularities in Word Embeddings
  id: totrans-477
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词嵌入中的语义规律
- en: We mentioned earlier that these representations capture regularities in language
    and are good at solving simple analogy tasks. The offsets between vector embeddings
    seem to capture the analogical relationship between words. So, for example, *"king"
    - "man" + "woman"* is expected to result in "*queen*". Let's see if the model
    that we trained on the `text8` corpus also understands some regularities.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到这些表示捕捉到了语言中的规律，并且很擅长解决简单的类比任务。向量嵌入之间的偏移似乎捕捉到了单词之间的类比关系。因此，例如，*"king" -
    "man" + "woman"* 期望结果是 "*queen*"。让我们看看我们在`text8`语料库上训练的模型是否也理解了一些规律。
- en: 'We''ll use the `most_similar()` method here, which allows us to add and subtract
    vectors from each other. We''ll provide `''king''` and `''woman''` as vectors
    to add to each other, use `''man''` to subtract from the result, and then check
    out the five terms that are the most similar to the resulting vector:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里使用`most_similar()`方法，该方法允许我们相互添加和减去向量。我们将提供'king'和'woman'作为要添加的向量，使用'man'来从结果中减去，然后查看与生成向量最相似的五个术语：
- en: '[PRE113]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'The output will be as follows:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE114]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: The top result is `'queen'`. Looks like the model is capturing these regularities.
    Let's try out another example. "Man" is to "uncle" as "woman" is to ? Or in an
    arithmetic form, what is the vector closest to *uncle - man + woman = ?*
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 最高结果是'queen'。看起来模型捕捉到了这些规律。让我们试试另一个例子。"Man" 对应 "uncle"，就如同 "woman" 对应于？或者用数学形式表达，*uncle
    - man + woman = ?* 的结果是最接近的向量是什么？
- en: '[PRE115]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'The following is the output of the preceding code:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '[PRE116]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: This seems to be working great. Notice that all the top five results are for
    the feminine gender. So, we took `uncle`, removed the masculine elements, added
    feminine elements, and now we have some really good results.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来效果不错。请注意，所有前五个结果都是女性性别的。因此，我们取出了'uncle'，去除了男性元素，添加了女性元素，现在我们得到了一些非常好的结果。
- en: Let's look at some other examples of vector arithmetic. We can take vectors
    for two different terms and average them to arrive at vectors for a phrase as
    well. Let's try it for ourselves.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些其他矢量算术的例子。我们也可以对两个不同术语的矢量进行平均，以得到短语的矢量。让我们自己试试吧。
- en: Note
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Taking the average of individual vectors is just one of the many ways of arriving
    at phrase vectors. Variations range from weighted averages to more complex mathematical
    functions.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对个体向量的平均值来创建短语向量仅仅是到达短语向量的众多方式之一。变化范围从加权平均到更复杂的数学函数。
- en: 'Exercise 4.05: Vectors for Phrases'
  id: totrans-491
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.05：短语的向量
- en: 'In this exercise, we will begin to create vectors for two different phrases,
    `get happy` and `make merry`, by taking the average of the individual vectors.
    We will find a similarity between the representations for the phrases. You will
    need to continue this exercise in the same Jupyter Notebook we have been using
    throughout this chapter. Follow these steps to complete this exercise:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将通过对两个不同短语“get happy”和“make merry”的个体向量取平均值来创建向量。我们将找到这些短语表示之间的相似性。您需要在我们已经在本章节中使用的同一个Jupyter
    Notebook中继续这个练习。按照以下步骤完成此练习：
- en: 'Extract the vector for the term "*get*" and store it in a variable:'
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取术语"*get*"的向量并将其存储在一个变量中：
- en: '[PRE117]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Extract the vector for the term "*happy*" and store it in a variable:'
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取"*happy*"的向量并将其存储在一个变量中：
- en: '[PRE118]'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'Create a vector as the element-wise average of the two vectors, `(v1 + v2)/2`.
    This is our vector for the entire phrase "get happy":'
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两个向量的元素逐项平均，创建一个向量，`(v1 + v2)/2`。这是我们整个短语“get happy”的向量：
- en: '[PRE119]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Similarly, extract vectors for the terms "*make*" and "*merry*":'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，提取"*make*"和"*merry*"的向量：
- en: '[PRE120]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'Create a vector for the phrase by averaging the individual vectors:'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对单个向量取平均值来创建该短语的向量：
- en: '[PRE121]'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Using the `cosine_similarities()` method in the model, find the cosine similarity
    between the two:'
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型中的`cosine_similarities()`方法，计算这两个向量之间的余弦相似度：
- en: '[PRE122]'
  id: totrans-504
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'The cosine similarity comes out as follows:'
  id: totrans-505
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 余弦相似度的结果如下：
- en: '[PRE123]'
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: The result is a cosine similarity of about `0.58`, which is positive and much
    higher than `0`. This means that the model thinks the phrases "get happy" and
    "make merry" are similar in meaning. Not bad, right? Instead of a simple average,
    we could use weighted averages, or come up with more sophisticated methods of
    combining individual vectors.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个大约为`0.58`的余弦相似度，这是一个正值，比`0`要高得多。这意味着模型认为短语“get happy”和“make merry”的含义相似。还不错吧？我们不仅仅用了简单的平均值，还可以使用加权平均，或者提出更复杂的方法来组合单个向量。
- en: Note
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2VVNEgf](https://packt.live/2VVNEgf)。
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址是[https://packt.live/38Gr54r](https://packt.live/38Gr54r)。你必须执行整个笔记本才能获得期望的结果。
- en: In this exercise, we saw how we could use vector arithmetic to represent phrases,
    instead of individual terms, and we saw that meaning is still captured. This brings
    us to a very important lesson – *vector arithmetic on word embeddings has meaning*.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们看到如何使用向量运算来表示短语，而不是单个术语，我们发现含义仍然被捕捉到了。这带来了一个非常重要的教训——*词嵌入的向量运算是有意义的*。
- en: These vector arithmetic operations work on the meaning of terms, resulting in
    some very interesting results.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 这些向量运算操作作用于术语的含义，产生了一些非常有趣的结果。
- en: We hope you now appreciate the power of word embeddings. We realize that these
    results come from just some matrix multiplication and take a minute to train on
    our dataset. Word embeddings are almost magical, and it is pleasantly surprising
    how such a simple prediction formulation results in such a powerful representation.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望你现在能理解词嵌入的强大功能。我们意识到这些结果仅来自一些矩阵乘法，并且只需要花费一分钟时间来训练我们的数据集。词嵌入几乎是神奇的，令人愉快的是，简单的预测公式竟然能产生如此强大的表示。
- en: When we created the word vectors previously, we didn't pay much attention to
    the controls/parameters. There are many, but only some have a significant impact
    on the quality of the representations. We will now come to understand the different
    parameters of the `word2vec` algorithm and see the effect of changing these for
    ourselves.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们之前创建单词向量时，并没有过多关注控制项/参数。它们有很多，但只有一些对表示质量有显著影响。接下来我们将了解`word2vec`算法的不同参数，并亲自查看更改这些参数的效果。
- en: Effect of Parameters – "size" of the Vector
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数的影响 —— 向量的“大小”
- en: 'The `size` parameter of the `word2vec` algorithm is the length of the vector
    for each term. By default, as we saw earlier, this is 100\. We will try reducing
    this parameter and assess the differences, if any, in the results. Let''s retrain
    the word embeddings, with `size` as 30 this time:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '`word2vec`算法的`size`参数是每个术语向量的长度。默认情况下，正如我们之前看到的，这个长度是100。我们将尝试减少这个参数，并评估结果的差异（如果有的话）。这次我们将`size`设置为30并重新训练词嵌入：'
- en: '[PRE124]'
  id: totrans-517
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'Now, let''s check the analogy task from earlier, that is, `king - man + woman`:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查之前的类比任务，也就是`king - man + woman`：
- en: '[PRE125]'
  id: totrans-519
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'This should give us the following output:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会给我们以下输出：
- en: '[PRE126]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: We can see that `queen` isn't present in the top five results. It looks like
    by using a very low dimensionality, we aren't capturing enough information in
    the representation for a term.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`queen`没有出现在前五个结果中。看起来，使用非常低的维度时，我们没有在表示中捕捉到足够的信息。
- en: Effect of Parameters – "window size"
  id: totrans-523
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数的影响 —— “窗口大小”
- en: The `window size` parameter defines the context; concretely, the window size
    is the number of terms to the left and to the right of the target term while building
    the context. The effect of this parameter is not very obvious. The general observation
    is that when you use a higher window size (say, 20), the top similar terms seem
    to be terms that are used along with the target term, not necessarily having a
    similar meaning. On the other hand, reducing the window size (to, say, 2), returns
    the top terms that are very similar in meaning, and are synonyms in many cases.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '`window size` 参数定义了上下文；具体来说，窗口大小是指在构建上下文时，目标词左右两侧的词汇数量。这个参数的影响并不是特别明显。一般观察是，当你使用更大的窗口大小（比如
    20）时，最相似的词似乎是与目标词一起使用的词，并不一定具有相似的意义。另一方面，减少窗口大小（比如设为 2）会返回那些在意义上非常相似的词，并且在很多情况下是同义词。'
- en: Skip-gram versus CBOW
  id: totrans-525
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Skip-gram 与 CBOW
- en: 'Choosing between Skip-gram and CBOW as the learning algorithm is exercised
    by setting `sg = 1` for Skip-gram (the default is `sg = 0`, that is, CBOW). Recall
    that the Skip-gram approach predicts the context words based on the central target
    word. This flips the formulation of CBOW, where the context words are used to
    predict the target word. But how do we choose between the two? What are the benefits
    of one over the other? To see for ourselves, let''s train embeddings using Skip-gram
    and compare some results with what we had for CBOW. To begin, let''s take a particular
    example for CBOW. First, we''ll recreate the CBOW word vectors with the default
    vector size by not specifying the size parameter. Oeuvre is a term for the body
    of work of an artist/performer. We''ll see the most similar terms for the uncommon
    term, `oeuvre`:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 Skip-gram 和 CBOW 作为学习算法的方式是通过设置 `sg = 1` 来选择 Skip-gram（默认值是 `sg = 0`，即 CBOW）。回顾一下，Skip-gram
    方法是基于中心目标词来预测上下文词。而这正好与 CBOW 相反，CBOW 是通过上下文词来预测目标词。那么我们该如何在两者之间做出选择呢？一个比另一个有什么优势吗？为了亲自验证，让我们使用
    Skip-gram 训练词嵌入，并将一些结果与 CBOW 的结果进行比较。首先，拿 CBOW 的一个具体例子来说。我们将通过不指定 size 参数来重新创建默认向量大小的
    CBOW 词向量。Oeuvre 是指艺术家/表演者的创作总和。我们将查看不常见词 `oeuvre` 的最相似词：
- en: '[PRE127]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'The following terms come out as the most similar terms:'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是最相似的词汇：
- en: '[PRE128]'
  id: totrans-529
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'We can see that most results are the names of artists (`swinburne`, `kurosawa`,
    and `baglione`) or food dishes (chateaubriand). None of the top five results are
    close in meaning to the target term. Now, let''s retrain our vectors using the
    Skip-gram method and see the result on the same task:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，大多数结果是艺术家的名字（`swinburne`、`kurosawa` 和 `baglione`）或食物菜肴（chateaubriand）。前五个结果中没有任何一个与目标词的意义相近。现在，让我们使用
    Skip-gram 方法重新训练我们的向量，并查看在同一任务上的结果：
- en: '[PRE129]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'This gives us the following output:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了以下输出：
- en: '[PRE130]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: We can see that the top terms are much closer in meaning (`masterful`, `orchestration`,
    `showcasing`). So, the Skip-gram method seems to work better for rare words.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，最相似的词在意义上要更接近（`masterful`、`orchestration`、`showcasing`）。因此，Skip-gram
    方法似乎对于稀有词更有效。
- en: Why is this so? The CBOW method smooths over a lot of the distributional statistics
    by effectively averaging overall context words (remember, all the context terms
    together go as an input), while Skip-gram does not. When you have a small dataset,
    the smoothing that's done by CBOW is desirable. If you have a small/moderately
    sized dataset, and if you are concerned about the representation of rare terms,
    then Skip-gram is a good option.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会这样呢？CBOW 方法通过有效地对所有上下文词进行平均，平滑了大量的分布统计（记住，所有上下文词一起作为输入），而 Skip-gram 则没有。在数据集较小的情况下，CBOW
    进行的平滑处理是值得期待的。如果你拥有一个小型或中等大小的数据集，并且你关心稀有词的表示，那么 Skip-gram 是一个不错的选择。
- en: Effect of Training Data
  id: totrans-536
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练数据的影响
- en: A very important decision while training your word vectors is the underlying
    data. The patterns and similarities will be learned from the data you supply to
    the algorithm, and we expect the model to learn differently from data from different
    domains, different kinds of settings, and so on. To appreciate this, we load different
    corpora from different contexts and see how the embeddings vary.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练词向量时，一个非常重要的决定是使用的基础数据。模式和相似性将从你提供给算法的数据中学习，我们预期模型会从不同领域、不同设置等的数据中以不同的方式学习。为了更好地理解这一点，我们加载来自不同背景的不同语料库，看看嵌入是如何变化的。
- en: The Brown corpus is a collection of general text, collected from 15 different
    topics to make it general (from politics to religion, books to music, and many
    other themes). It contains 500 text samples and about 1 million words. The "movie"
    corpus contains movie-review data from IMDb. Both of these are available in NLTK.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: Brown 语料库是一个通用文本集合，收集自15个不同的主题，使其具有广泛性（包括政治、宗教、书籍、音乐及其他许多主题）。它包含500个文本样本和约100万字。
    "电影"语料库包含来自 IMDb 的电影评论数据。这两个语料库都可以在 NLTK 中使用。
- en: 'Exercise 4.06: Training Word Vectors on Different Datasets'
  id: totrans-539
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.06：在不同数据集上训练词向量
- en: 'In this exercise, we will train our own word vectors on the Brown corpus and
    the IMDb movie reviews corpus. We will assess the differences in the representations
    learned and the effect of the underlying training data. Follow these steps to
    complete this exercise:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将基于 Brown 语料库和 IMDb 电影评论语料库训练我们自己的词向量。我们将评估所学习的表示法差异及其底层训练数据的影响。按照以下步骤完成本练习：
- en: 'Import the Brown and IMDb movie reviews corpus from NLTK:'
  id: totrans-541
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 NLTK 导入 Brown 和 IMDb 电影评论语料库：
- en: '[PRE131]'
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'The corpora have a convenient method, `sent()`, to extract the individual sentences
    and words (tokenized sentences, which can be directly passed to the `word2vec`
    algorithm). Since both the corpora are rather small, use the Skip-gram method
    to create the embeddings:'
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些语料库提供了一个方便的方法 `sent()` 来提取单独的句子和单词（已标记的句子，可以直接传递给 `word2vec` 算法）。由于这两个语料库比较小，我们使用
    Skip-gram 方法来创建嵌入：
- en: '[PRE132]'
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: We now have two embeddings that have been learned on different contexts for
    the same term. Let's see the most similar terms for `money` from the model on
    the Brown corpus.
  id: totrans-545
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在我们有了两个在不同上下文中学习到的相同术语的词嵌入。让我们看看在 Brown 语料库上训练的模型中，`money` 的最相似词是什么。
- en: 'Print out the *top five terms* most similar to `money` from the model that
    were learned on the Brown corpus:'
  id: totrans-546
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出从 Brown 语料库中学习到的模型中，与 `money` 最相似的 *前五个词*：
- en: '[PRE133]'
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'The following is the output of the preceding code:'
  id: totrans-548
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '[PRE134]'
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: We can see that the top term is `'job'`; fair enough. Let's see what the model
    learned regarding movie reviews.
  id: totrans-550
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，排名第一的词是 `'job'`，这也合理。让我们看看该模型在电影评论中学到了什么。
- en: 'Print out the top five terms most similar to `money` from the model that learned
    from the movie corpus:'
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出与 `money` 最相似的五个词，它们来自从电影语料库学习的模型：
- en: '[PRE135]'
  id: totrans-552
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'The following are the top terms:'
  id: totrans-553
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是排名靠前的词：
- en: '[PRE136]'
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: The top terms are `cash` and `ransom`. Considering the language being used in
    movies, and thus in movie reviews, this isn't very surprising.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 排名靠前的词是 `cash` 和 `ransom`。考虑到电影评论中使用的语言，这一点并不令人惊讶。
- en: Note
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf).
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此部分的源代码，请参阅 [https://packt.live/2VVNEgf](https://packt.live/2VVNEgf)。
- en: You can also run this example online at [https://packt.live/38Gr54r](https://packt.live/38Gr54r).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，网址为 [https://packt.live/38Gr54r](https://packt.live/38Gr54r)。你必须执行整个
    Notebook 才能获得期望的结果。
- en: In this exercise, we created word vectors using different datasets and saw that
    the representations for the same terms and the associations that were learned
    are very affected by the underlying data. So, choose your data wisely.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们使用不同的数据集创建了词向量，并发现相同术语的表示和所学习的关联很大程度上受到底层数据的影响。因此，选择数据时要谨慎。
- en: Using Pre-Trained Word Vectors
  id: totrans-560
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练的词向量
- en: So far, we've trained our own word embeddings using the small datasets we had
    access to. The folks at the Stanford NLP group have trained word embeddings on
    6 billion tokens with 400,000 terms in the vocabulary. Individually, we will not
    have the resources to handle this scale. Fortunately, the Stanford NLP group has
    been benevolent enough to make these trained embeddings available to the general
    public so that people like us can benefit from their work. The trained embeddings
    are available on the GloVe page ([https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)).
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用可访问的小数据集训练了我们自己的词嵌入。斯坦福 NLP 小组已经在60亿个标记和40万个词汇表项上训练了词嵌入。单独来说，我们可能没有足够的资源来处理这样的规模。幸运的是，斯坦福
    NLP 小组慷慨地将这些训练好的词嵌入公开，使像我们这样的人可以从他们的工作中受益。这些训练好的嵌入可以在 GloVe 页面上找到 ([https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/))。
- en: 'A quick note on GloVe: the method that''s used for training is slightly different.
    The objective is modified to make the similar terms occur closer in space, in
    a little more explicit fashion. You can read about the details on the project
    page for GloVe ([https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)),
    which also has a link to the original paper proposing it. The end result, however,
    is very similar in performance to word2vec.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 关于GloVe的简要说明：用于训练的方法略有不同。目标已经修改，使得相似的词汇在空间中更为接近，以一种更为显式的方式。你可以在GloVe项目页面上阅读详细信息（[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)），该页面也链接到原始的提案论文。最终结果，然而，在性能上与word2vec非常相似。
- en: 'We''ll download the `glove.6B.zip` file from the GloVe project page. The file
    contains 50D, 100D, 200D, and 300D vectors. We''ll work with the 100D vectors
    here. Please unzip the file and make sure you have the text files in your working
    directory. The trained vectors are available as a text file, and the format is
    slightly different. We''ll use the `glove2word2vec` utility that''s available
    in Gensim to convert into a format that Gensim can easily load:'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从GloVe项目页面下载`glove.6B.zip`文件。该文件包含50D、100D、200D和300D向量。我们在这里将使用100D向量。请解压文件，并确保你在工作目录中有文本文件。这些训练好的向量以文本文件形式提供，格式略有不同。我们将使用Gensim中可用的`glove2word2vec`工具，将其转换为Gensim可以轻松加载的格式：
- en: '[PRE137]'
  id: totrans-564
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'We specified the input and the output file and ran the `glove2word2vec` utility.
    As the name suggests, the utility takes in word vectors in GloVe format and converts
    them into `word2vec` format. After this, the `word2vec` models can understand
    these embeddings easily. Now, let''s load the `keyed` word vectors from the text
    file (reformatted):'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了输入和输出文件，并运行了`glove2word2vec`工具。顾名思义，该工具将GloVe格式的词向量转换为`word2vec`格式。之后，`word2vec`模型就可以轻松理解这些嵌入。现在，我们来加载从文本文件（已重新格式化）中获取的`keyed`词向量：
- en: '[PRE138]'
  id: totrans-566
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'With this done, we have the GloVe embeddings in the model, along with all the
    handy utilities we had for the embeddings model from word2vec. Let''s check out
    the top terms similar to `"money"`:'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这一步后，我们已经在模型中加入了GloVe嵌入，并且拥有了与word2vec嵌入模型相同的所有实用工具。接下来，我们来看看与“money”最相似的前几个词：
- en: '[PRE139]'
  id: totrans-568
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'The output is as follows:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE140]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: 'For closure, let''s also check how this model performs on the king and queen
    tasks:'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 为了收尾，我们还检查一下这个模型在king和queen任务上的表现：
- en: '[PRE141]'
  id: totrans-572
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'The following is the output of the preceding code:'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '[PRE142]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: Now that we have these embeddings in a model, we can work with them the same
    way we worked with the embeddings we created previously and can benefit from the
    larger dataset and vocabulary and the processing power used by the contributing
    organization.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将这些嵌入加入模型中，我们可以像之前处理我们创建的嵌入那样操作它们，并可以受益于贡献组织提供的更大数据集、词汇库和处理能力。
- en: Bias in Embeddings – A Word of Caution
  id: totrans-576
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入中的偏差——警告
- en: 'When discussing regularities and analogies, we saw the following example:'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论规律性和类比时，我们看到了以下例子：
- en: '*king – man + woman = queen*'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '*king – man + woman = queen*'
- en: 'It''s great that the embeddings are capturing these regularities by learning
    from the text data. Let''s try something similar to a profession. Let''s see the
    term closest to *doctor – man + woman*:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 很高兴看到这些嵌入通过学习文本数据来捕捉这些规律性。我们再试一个与职业相关的例子。让我们看看与*doctor – man + woman*最接近的词：
- en: '[PRE143]'
  id: totrans-580
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'The output regarding the top five results will be as follows:'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 关于前五个结果的输出将如下所示：
- en: '[PRE144]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'That''s not the kind of result we want. Doctors are males, while females are
    nurses? Let''s try another example. This time, let''s try what the model thinks
    regarding females as corresponding to "smart" for "males":'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 结果不是我们想要的。医生是男性，而女性则是护士？我们再试另一个例子。这次，让我们看看模型对于女性和“聪明”对应男性的看法：
- en: '[PRE145]'
  id: totrans-584
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'We get the following top five results:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下前五个结果：
- en: '[PRE146]'
  id: totrans-586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: We can see that the top terms are `'cute'`, `'dumb'`, and `'crazy'`. That's
    not good at all.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，前几个词是'cute'、'dumb'和'crazy'。这实在不好。
- en: What's happening here? Is this seemingly great representation approach sexist?
    Is the word2vec algorithm sexist? There definitely is bias in the resulting word
    vectors, but think about where the bias is coming from. It's the underlying data
    that uses `'nurse'` for females in contexts where `'doctor'` is used for males.
    It is, therefore, the underlying text that contains the bias, not the algorithm.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了什么？这个看似很棒的表示方法是性别歧视吗？word2vec算法是否存在性别歧视？结果的词向量中确实存在偏见，但想想这些偏见是从哪里来的。问题出在基础数据上，它在使用`'nurse'`表示女性的上下文中，而`'doctor'`则用于男性。因此，偏见来源于基础文本，而不是算法本身。
- en: This topic has recently gained significant attention, and there is ongoing research
    around ways to assess and get rid of biases from the learned embeddings, but a
    good approach is to avoid biases in the data to begin with. If you trained word
    embeddings on YouTube comments, don't be surprised if they contain all kinds of
    extreme biases. You're better off avoiding text data that you suspect to have
    biases.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 这个话题最近引起了广泛关注，围绕如何评估并消除学习到的嵌入中的偏见的研究仍在进行中，但一个好的方法是从一开始就避免数据中的偏见。如果你在YouTube评论上训练词嵌入，不要惊讶于它们包含各种极端的偏见。最好避免使用你怀疑包含偏见的文本数据。
- en: Other Notable Approaches to Word Embeddings
  id: totrans-590
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他值得注意的词嵌入方法
- en: 'We worked with the word2vec approach primarily, and we briefly looked at the
    GloVe approach. While these are the most popular approaches, there are a few other
    approaches worth mentioning:'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要使用了word2vec方法，并简要了解了GloVe方法。虽然这些是最流行的方法，但还有一些其他值得一提的方法：
- en: '**FastText**: Created by **Facebook''s AI Research** (**FAIR**) lab, it uses
    subword information to enrich the word embeddings. You can read more about it
    on the official page ([https://research.fb.com/downloads/fasttext/](https://research.fb.com/downloads/fasttext/)).'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '**FastText**：由**Facebook的人工智能研究**（**FAIR**）实验室创建，它利用子词信息来丰富词嵌入。你可以在官方页面上阅读更多内容（[https://research.fb.com/downloads/fasttext/](https://research.fb.com/downloads/fasttext/)）。'
- en: '**WordRank**: Treats the embeddings problem as a word-ranking problem. Its
    performance is similar to word2vec in several tasks. You can read more about this
    at [https://arxiv.org/abs/1506.02761](https://arxiv.org/abs/1506.02761).'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '**WordRank**：将嵌入问题视为词排名问题。它在多个任务中的表现与word2vec相似。你可以在[https://arxiv.org/abs/1506.02761](https://arxiv.org/abs/1506.02761)上阅读更多相关内容。'
- en: Other than these, some popular libraries now have pre-trained embeddings available
    (SpaCy is a good example). The choices are aplenty. We can't do a detailed treatment
    of these choices here, but please do explore the options.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，一些流行的库现在已经提供了预训练的嵌入（SpaCy就是一个很好的例子）。选择很多。我们不能在这里详细讨论这些选择，但请务必探索这些选项。
- en: We've discussed a lot of ideas around representation in this chapter. Now, let's
    implement these ideas with the help of an activity.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了许多关于表示的观点。现在，让我们通过一个活动来实现这些想法。
- en: 'Activity 4.02: Text Representation for Alice in Wonderland'
  id: totrans-596
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 4.02：爱丽丝梦游仙境的文本表示
- en: In the previous activity, we tokenized and performed basic preprocessing of
    the text. In this activity, we will advance this process by using representation
    approaches for the text. You will create your own embeddings from the data and
    see the kind of relations we have. You will also utilize pre-trained embeddings
    to represent the data in the text.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个活动中，我们对文本进行了分词和基本的预处理。在本活动中，我们将通过使用文本表示方法来推进这一过程。你将从数据中创建自己的嵌入，并查看我们所拥有的关系类型。你还将利用预训练的嵌入来表示文本中的数据。
- en: Note
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Note that you'll need to have completed *Activity 4.01*, *Text Preprocessing
    of the 'Alice in Wonderland' Text*, to proceed with this activity. In that activity,
    we performed stop word removal on the text.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在继续本活动之前，你需要完成*活动 4.01*、*《爱丽丝梦游仙境》文本预处理*。在该活动中，我们对文本进行了停用词移除。
- en: 'You need to perform the following steps:'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要执行以下步骤：
- en: We'll continue using the same Jupyter Notebook that we used for *Activity 4.01*,
    *Text Preprocessing of the 'Alice in Wonderland' Text*. We'll work on the result
    of the stop word removal step we got in that activity (let's say it is stored
    in a variable called `alice_words_nostop`). Print the first three sentences from
    the result.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用之前在*活动 4.01*、*《爱丽丝梦游仙境》文本预处理*中使用的相同Jupyter Notebook。我们将处理在该活动中获得的去除停用词步骤的结果（假设它存储在名为`alice_words_nostop`的变量中）。打印出结果中的前三个句子。
- en: Import `word2vec` from Gensim and train your word embeddings with default parameters.
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Gensim中导入`word2vec`并使用默认参数训练你的词嵌入。
- en: Find the terms most similar to `rabbit`.
  id: totrans-603
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找与`rabbit`最相似的词语。
- en: Using a window size 2, retrain the word vectors.
  id: totrans-604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用窗口大小为2，重新训练词向量。
- en: Find the terms most similar to `rabbit`.
  id: totrans-605
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找与`rabbit`最相似的词语。
- en: Retrain the word vectors using the Skip-gram method with a window size of `5`.
  id: totrans-606
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Skip-gram方法，并将窗口大小设为`5`，重新训练词向量。
- en: Find the terms most similar to `rabbit`.
  id: totrans-607
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找与`rabbit`最相似的词语。
- en: Find the representation for the phrase `white rabbit` by averaging the vectors
    for `white` and `rabbit`.
  id: totrans-608
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对`white`和`rabbit`的词向量进行平均，找到`white rabbit`这个短语的表示。
- en: Find the representation for `mad hatter` by averaging the vectors for `mad`
    and `hatter`.
  id: totrans-609
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对`mad`和`hatter`的词向量进行平均，找到`mad hatter`的表示。
- en: Find the cosine similarity between these two phrases.
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算这两个短语之间的余弦相似度。
- en: Load pre-trained GloVe embeddings of size 100D.
  id: totrans-611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载预训练的100维GloVe词嵌入。
- en: Find representations for `white rabbit` and `mad hatter`.
  id: totrans-612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找`white rabbit`和`mad hatter`的表示。
- en: Find the cosine similarity between the two phrases. Has the cosine similarity
    changed?
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算这两个短语之间的余弦相似度。余弦相似度有变化吗？
- en: As a result of this activity, we will have our own word vectors that have been
    trained on "Alice's Adventures in Wonderland" and have representation for the
    terms available in the text.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本次活动，我们将得到自己训练的词向量，这些词向量基于《爱丽丝梦游仙境》进行训练，并且可以表示文本中出现的术语。
- en: Note
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 407.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的详细步骤、解决方案以及额外的评论内容可以在407页找到。
- en: Summary
  id: totrans-617
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we began by discussing the peculiarities of text data and how
    ambiguity makes NLP difficult. We discussed that there are two key ideas in working
    with text – preprocessing and representation. We discussed the many tasks involved
    in preprocessing, that is, getting your data cleaned up and ready for analysis.
    We saw various approaches to removing imperfections from the data.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 本章开始时，我们讨论了文本数据的特殊性，以及歧义性如何使自然语言处理（NLP）变得困难。我们提到，处理文本时有两个关键的概念——预处理和表示。我们讨论了预处理中的许多任务，即如何清理和准备数据以便进行分析。我们还看到了去除数据中不完美部分的各种方法。
- en: Representation was the next big aspect – we understood the considerations in
    representing text and converting text into numbers. We looked at various approaches,
    beginning with classical approaches, which included one-hot encoding, the count-based
    approach, and the TF-IDF method.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 表示是下一个重要方面——我们理解了表示文本时需要考虑的因素，以及如何将文本转化为数字。我们回顾了各种方法，从经典方法开始，包括独热编码、基于计数的方法和TF-IDF方法。
- en: 'Word embeddings are a whole new approach to representing text that leverage
    ideas from distributional semantics – terms that appear in similar contexts have
    similar meanings. The word2vec algorithm smartly exploits this idea by formulating
    a prediction problem: predict a target word given the context. It uses a neural
    network for the prediction and, in the process, learns vector representations
    for the terms.'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是一种全新的文本表示方法，借鉴了分布式语义学的思想——在相似上下文中出现的术语具有相似的含义。word2vec算法巧妙地利用了这个思想，通过设定预测问题来预测目标词语，给定上下文来预测目标词语。它使用神经网络进行预测，并在此过程中学习词汇的向量表示。
- en: We saw that these representations are amazing as they seem to capture meaning,
    and simple arithmetic operations gave some very interesting and meaningful results.
    You can even create representations for phrases or even sentences/documents using
    word vectors. This sets the stage for later when we use word embeddings in more
    sophisticated deep learning architectures for NLP.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现这些表示非常令人惊讶，因为它们似乎能够捕捉到意义，而且简单的算术运算给出了非常有趣且富有意义的结果。你甚至可以通过词向量来创建短语，甚至是句子或文档的表示。这为后续我们在更复杂的深度学习架构中使用词嵌入技术处理NLP任务打下了基础。
- en: In the next chapter, we'll continue our exploration of sequences by applying
    deep learning approaches such as recurrent neural networks and one-dimensional
    convolutions to them.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将继续探索序列，应用深度学习方法，如递归神经网络（RNN）和一维卷积（1D卷积）。
