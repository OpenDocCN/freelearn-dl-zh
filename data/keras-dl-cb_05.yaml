- en: Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归神经网络
- en: Recurrent Neural Networks (**RNNs**) make use of sequential or time series data.
    In a regular neural network, we consider that all inputs and outputs are independent
    of each other. For a task where you want to predict the next word in a given sentence,
    it's better to know which words have come before it. RNNs are recurrent as the
    same task is performed for every element in the sequence where the output is dependent
    on the previous calculations. RNNs can be thought of as having a **memory** that
    captures information about what has been computed so far.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络（**RNN**）利用序列或时间序列数据。在常规神经网络中，我们认为所有的输入和输出是相互独立的。对于预测给定句子中下一个单词的任务，知道它前面的单词更为重要。RNN
    是递归的，因为对序列中的每个元素执行相同的任务，输出依赖于之前的计算。RNN 可以被看作具有 **记忆**，捕捉到迄今为止计算的信息。
- en: Going from feedforward neural networks to recurrent neural networks, we will
    use the concept of sharing parameters across various parts of the model. Parameter
    sharing will make it possible to extend and apply the model to examples of different
    forms (different lengths, here) and generalize across them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从前馈神经网络到递归神经网络，我们将使用跨模型各部分共享参数的概念。参数共享使得模型能够扩展并应用于不同形式（这里是不同长度）的示例，并且能够在这些示例之间进行泛化。
- en: Introduction to RNNs
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 简介
- en: To understand RNNs, we have to understand the basics of feedforward neural networks.
    You can refer to [Chapter 3](73c6bf42-1089-4504-97d5-a7dafae0e59c.xhtml), *Optimization
    for Neural Networks*, for details on feedforward networks. Both feedforward and
    recurrent neural networks are identified from the way they process the information
    or features through a series of mathematical operations performed at the various
    nodes of the network. One feeds information straight through (never touching a
    given node twice), the other cycles it through a loop.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 RNN，我们必须了解前馈神经网络的基本原理。关于前馈网络的详细信息，请参阅 [第 3 章](73c6bf42-1089-4504-97d5-a7dafae0e59c.xhtml)，*神经网络优化*。前馈和递归神经网络的区别在于它们如何通过一系列数学操作处理信息或特征，操作发生在网络的各个节点上。一个是将信息直接传递（每个节点仅处理一次），另一个则是将信息循环处理。
- en: A feedforward neural network is trained on image data until it minimizes the
    loss or error while predicting or classifying the categories for image types.
    With the trained set of hyper parameters or weights, the neural network can classify
    data it has never seen before. A trained feedforward neural network can be shown
    any random collection of images and the first image it classifies will not alter
    how it classifies the other images.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络对图像数据进行训练，直到它在预测或分类图像类型时最小化损失或误差。通过训练后的超参数集或权重，神经网络可以对从未见过的数据进行分类。训练好的前馈神经网络可以展示任何随机的图像集合，它对第一张图像的分类不会改变它对其他图像的分类。
- en: In a nutshell, these networks have no notion of order in time or temporal pattern,
    and the only information they consider is the current example it has been asked
    to classify.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这些网络没有时间或时间模式的顺序概念，它们仅考虑它们被要求分类的当前示例。
- en: 'RNNs take into account the temporal nature of the input data. An input to the
    RNN cell is both from the current timestep and one step back in time. Details
    are presented in the following diagram:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 考虑到输入数据的时间性。RNN 单元的输入既来自当前时刻，又来自前一个时刻。详情请见下图：
- en: '![](img/bb938643-d41f-41f3-91ff-52a9a58becbf.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb938643-d41f-41f3-91ff-52a9a58becbf.png)'
- en: RNNs are recurrent in nature because they perform the same computation for every
    element in a sequence where the output is dependent on the previous computations.
    The other way to think about RNNs is that they have memory that can capture the
    information about what has been computed so far. RNNs can make use of the information
    or knowledge in long sequences but practically they are restricted to looking
    back only a few steps.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 本质上是递归的，因为它们对序列中的每个元素执行相同的计算，输出依赖于之前的计算。另一种看待 RNN 的方式是，它们具有可以捕捉到迄今为止已计算信息的“记忆”。RNN
    可以利用长序列中的信息或知识，但实际上它们只限于回顾几步之前的信息。
- en: 'A typical RNN looks as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的 RNN 如下所示：
- en: '![](img/3b910d0e-fd5f-4fc8-9a87-dc5bfad83414.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3b910d0e-fd5f-4fc8-9a87-dc5bfad83414.png)'
- en: 'An unwrapped version of the RNN is shown in the following image; by unwrapping
    we mean that we write out the neural network for a complete sequence. Consider
    a sequence of five words; the network will be unwrapped into a five-layer neural
    network, one layer for each word:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了未展开版本的RNN；展开的意思是我们将神经网络写出以适应整个序列。考虑一个包含五个单词的序列；该网络将展开成一个五层的神经网络，每一层对应一个单词：
- en: '![](img/1ec29cd0-643a-4f88-ab1f-2e9315c103b6.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ec29cd0-643a-4f88-ab1f-2e9315c103b6.png)'
- en: 'Computations happening in an RNN are as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 中的计算过程如下：
- en: '![](img/e7e0812f-609b-45b5-a109-9e6e6d6f6ae9.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7e0812f-609b-45b5-a109-9e6e6d6f6ae9.png)'
- en: '![](img/64313d85-a18d-42e1-9793-4f567b969e1b.jpg) denotes the input at timestep
    ![](img/2c8b95fa-cac0-44f7-a919-6ad5e8acd3f6.jpg).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/64313d85-a18d-42e1-9793-4f567b969e1b.jpg) 表示时间步 ![](img/2c8b95fa-cac0-44f7-a919-6ad5e8acd3f6.jpg)
    的输入。'
- en: '![](img/87c94d1b-1897-42a5-9e5b-c06d45d23745.jpg) denotes the hidden state
    at timestep ![](img/e0b2da69-d7f9-4c8a-86c7-509f4582749b.jpg). The hidden state
    is the memory of the network. ![](img/deba0343-39ab-45a6-80fa-8e5314797374.jpg) is
    computed based on the previously hidden state and the input at the current step, ![](img/f3bdc7a8-b8f6-4dc4-8921-88c9a6324268.jpg).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/87c94d1b-1897-42a5-9e5b-c06d45d23745.jpg) 表示时间步 ![](img/e0b2da69-d7f9-4c8a-86c7-509f4582749b.jpg)
    的隐藏状态。隐藏状态是网络的记忆。 ![](img/deba0343-39ab-45a6-80fa-8e5314797374.jpg) 是基于先前的隐藏状态和当前步骤的输入计算得出的，
    ![](img/f3bdc7a8-b8f6-4dc4-8921-88c9a6324268.jpg)。'
- en: Function *f* represents non-linearity such as *tanh* or ReLU. The first hidden
    state is typically initialized to all zeroes.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数 *f* 表示非线性，例如 *tanh* 或 ReLU。第一个隐藏状态通常初始化为全零。
- en: '![](img/64fa0205-2db6-439c-9ba1-09e3ad190341.jpg) denotes the output at step
    ![](img/18f8876a-fc08-4bcb-bf7a-6dff44642c3e.jpg). To predict the next word in
    a given sentence, it will be a vector of probabilities across the vocabulary, ![](img/738130f6-dc44-4e1e-a2ff-21b10ec81f36.jpg).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/64fa0205-2db6-439c-9ba1-09e3ad190341.jpg) 表示步骤 ![](img/18f8876a-fc08-4bcb-bf7a-6dff44642c3e.jpg)
    的输出。为了预测给定句子中的下一个单词，它将是一个遍历词汇表的概率向量， ![](img/738130f6-dc44-4e1e-a2ff-21b10ec81f36.jpg)。'
- en: RNN implementation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 实现
- en: Following program processes, a sequence of numbers and the goal is to predict
    the next value, with the previous values provided. The input to the RNN network
    at every time step is the current value and a state vector that represents or
    stores what the neural network has seen at timesteps before. This state-vector
    is the encoded memory of the RNN, initially set to zero.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下程序处理一系列数字，目标是预测下一个值，并提供先前的值。在每个时间步，RNN 网络的输入是当前值和一个状态向量，该向量表示或存储了神经网络在之前时间步看到的内容。这个状态向量是
    RNN 的编码记忆，最初设置为零。
- en: Training data is basically a random binary vector. The output is shifted to
    the right.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据基本上是一个随机的二进制向量。输出向右偏移。
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Computational graph
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算图
- en: 'The computational graph is shown as following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图如下所示：
- en: '![](img/b454f847-90e3-4a82-b40a-e1bcf2589066.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b454f847-90e3-4a82-b40a-e1bcf2589066.png)'
- en: 'The output of the listing is shown as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是列表的输出结果：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: RNN implementation with TensorFlow
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 实现的 RNN
- en: 'We will now use the TensorFlow API; the inner workings of the RNN are hidden
    under the hood. The TensorFlow `rnn` package unrolls the RNN and creates the graph
    automatically so that we can remove the for loop:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用 TensorFlow API；RNN 的内部工作机制隐藏在幕后。TensorFlow 的 `rnn` 包展开了 RNN 并自动创建图形，这样我们就可以去掉
    for 循环：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Computational graph
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算图
- en: 'The following is the image of computational graph:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是计算图的图像：
- en: '![](img/53cc84ec-3960-4956-8431-37935ea2c8a9.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/53cc84ec-3960-4956-8431-37935ea2c8a9.png)'
- en: 'The output of the listing is shown as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是列表的输出结果：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Introduction to long short term memory networks
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆网络简介
- en: The vanishing gradient problem has appeared as the biggest obstacle to recurrent
    networks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失问题已成为循环网络最大的障碍。
- en: As the straight line changes along the *x* axis with a slight change in the
    *y* axis, the gradient shows change in all the weights with regard to change in
    error. If we don't know the gradient, we will not be able to adjust the weights
    in a direction that will reduce the loss or error, and our neural network ceases
    to learn.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随着直线沿 *x* 轴变化，*y* 轴的变化幅度较小，梯度显示了所有权重随误差变化的变化。如果我们不知道梯度，就无法调整权重以减少损失或误差，我们的神经网络将停止学习。
- en: '**Long short term memories** (**LSTMs**) are designed to overcome the vanishing
    gradient problem. Retaining information for a larger duration of time is effectively
    their implicit behavior.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）旨在克服梯度消失问题。保留信息更长时间实际上是它们的隐性行为。'
- en: 'In standard RNNs, the repeating cell will have an elementary structure, such
    as a single **tanh** layer:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的 RNN 中，重复单元将具有一个基础结构，例如单一的 **tanh** 层：
- en: '![](img/3ce2d1f5-54e4-4b79-9bfd-9ec6fac73002.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ce2d1f5-54e4-4b79-9bfd-9ec6fac73002.png)'
- en: 'As seen in the preceding image, LSTMs also have a chain-like structure, but
    the recurrent cell has a different structure:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，LSTM 也具有链式结构，但其递归单元具有不同的结构：
- en: '![](img/d56f78b6-5471-44cd-8c0d-b8d983a62edb.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d56f78b6-5471-44cd-8c0d-b8d983a62edb.png)'
- en: Life cycle of LSTM
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM 的生命周期
- en: 'The key to LSTMs is the cell state that is like a conveyor belt. It moves down
    the stream with minor linear interactions. It''s straightforward for data to flow
    as unchanged:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 的关键是单元状态，它像一个传送带。它通过轻微的线性交互沿着流动，数据可以轻松流动而不改变：
- en: '![](img/fe8f82d3-8908-45b9-9aea-178576cd8070.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe8f82d3-8908-45b9-9aea-178576cd8070.png)'
- en: LSTM networks have the ability to either remove or add information to the cell
    state that is carefully regulated by structures known as gates.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 网络能够有选择地从单元状态中删除或添加信息，而这些信息的操作由称为门的结构精心调控。
- en: 'The first step in an LSTM network is to determine what information we will
    be throwing away from the cell state. The decision is made by a sigmoid layer
    known as the **forget gate** layer. The layer looks at the previous state *h(t-1)*
    and current input *x(t)* and outputs a number between 0 and 1 for each number
    in the cell state *C(t−1)*, where 1 represents **absolutely keep this** while
    a 0 represents **entirely get rid of this**:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM 网络的第一步是确定我们将从单元状态中丢弃哪些信息。这个决策是通过一个称为**忘记门**层的 sigmoid 层来做出的。该层查看前一个状态 *h(t-1)*
    和当前输入 *x(t)*，并为单元状态 *C(t−1)* 中的每个数字输出一个介于 0 和 1 之间的数值，其中 1 表示**绝对保留此信息**，而 0 表示**完全丢弃此信息**：
- en: '![](img/20b9c65d-c1a3-4647-a963-46b538b83bfd.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20b9c65d-c1a3-4647-a963-46b538b83bfd.png)'
- en: The next step is to determine what new information we are going to persist in
    the cell state. Firstly, a sigmoid layer known as the input gate layer decides
    which values will be updated. Secondly, a *tanh* layer generates a vector of new
    candidate values *C̃* that could be added to the state.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是确定我们将要在单元状态中保持哪些新信息。首先，一个称为输入门层的 sigmoid 层决定哪些值会被更新。其次，一个 *tanh* 层生成一个新的候选值向量
    *C̃*，这些值可能会被添加到状态中。
- en: '![](img/b0d4fd69-9c56-44db-8493-4f5120847a16.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0d4fd69-9c56-44db-8493-4f5120847a16.png)'
- en: We will now update the old cell state *C(t−1)* to the new cell state *C(t)*.
    We multiply the old state by *f(t)*, forgetting the things we decided to forget
    earlier. Then we add *i(t) ∗ C̃*; these are the new candidate values scaled by
    the amount we decided to update each state value.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将更新旧的单元状态 *C(t−1)* 为新的单元状态 *C(t)*。我们将旧状态乘以 *f(t)*，忘记我们之前决定要忘记的内容。然后我们添加
    *i(t) ∗ C̃*；这些是新的候选值，按我们决定更新每个状态值的比例进行缩放。
- en: '![](img/c262f1f2-8163-43d6-aa07-67fa578add3d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c262f1f2-8163-43d6-aa07-67fa578add3d.png)'
- en: Finally, we decide on the output, which will be based on our cell state but
    will be a filtered or modified version. Firstly, we execute the sigmoid layer
    that determines what parts of the cell state we're going to output. Following
    which, we put the cell state through tanh to push the values to be between −1
    and 1, and multiply it by the output of the sigmoid gate so that we only output
    the parts we decided to.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们决定输出什么，这将基于我们的单元状态，但将是一个经过滤波或修改的版本。首先，我们执行一个 sigmoid 层来决定我们将输出单元状态的哪些部分。然后，我们将单元状态通过
    tanh 层，以便将值压缩到 −1 和 1 之间，并将其与 sigmoid 门的输出相乘，这样我们只输出我们决定的部分。
- en: '![](img/1f22f442-8d01-441d-9a6a-aea5ec3723e7.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f22f442-8d01-441d-9a6a-aea5ec3723e7.png)'
- en: LSTM implementation
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM 实现
- en: 'LSTMs remember, forget, and pick what to pass on and then output depending
    on the current state and input. An LSTM has many more moving parts, but using
    the native TensorFlow API, it will be quite straightforward:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 会记住、忘记，并根据当前的状态和输入选择要传递和输出的内容。一个 LSTM 具有更多的活动组件，但使用原生的 TensorFlow API，操作起来会非常直接：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Computational graph
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算图
- en: 'The following computational graph from TensorBoard describes the working of
    the LSTM network:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自 TensorBoard 的计算图，描述了 LSTM 网络的工作原理：
- en: '![](img/fe1361fa-f533-4aee-8456-c7543d327f24.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe1361fa-f533-4aee-8456-c7543d327f24.png)'
- en: 'The output of the listing is shown as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的输出如下所示：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Sentiment analysis
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: We will now write an app to predict sentiments of a movie review. Reviews are
    made up of a sequence of words and the order of words encodes very useful information
    to predict sentiment. The first step is to map words to word embeddings. The second
    step is the RNN that receives a sequence of vectors as input and considers the
    order of the vectors to generate the prediction.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将编写一个应用程序来预测电影评论的情感。评论由一系列单词组成，单词的顺序编码了预测情感的非常有用的信息。第一步是将单词映射到词嵌入。第二步是 RNN，它接收一个向量序列作为输入，并考虑向量的顺序来生成预测。
- en: Word embeddings
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词嵌入
- en: We will now train a neural network for word to vector representation. Given
    a particular word in the center of a sentence, which is the input word, we look
    at the words nearby. The network is going to tell us the probability for every
    word in our vocabulary of being the nearby word that we choose.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将训练一个神经网络来进行词到向量的表示。给定句子中心的特定单词，即输入词，我们查看附近的单词。网络将告诉我们，在我们的词汇表中，每个单词作为选择的邻近单词的概率。
- en: '![](img/11250af4-f53c-46cb-bf1e-3db2dd354b11.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11250af4-f53c-46cb-bf1e-3db2dd354b11.png)'
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of the listing is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的输出如下：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Sentiment analysis with an RNN
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RNN 进行情感分析
- en: 'The following example shows the implementation of sentiment analysis using
    an RNN. It has fixed-length movie reviews encoded as integer values, which are
    then converted to word embedding (embedding vectors) passed to LSTM layers in
    a recurrent manner that pick the last prediction as the output sentiment:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用 RNN 实现情感分析。它包含固定长度的电影评论，这些评论被编码为整数值，然后转换为词嵌入（嵌入向量），并以递归方式传递给 LSTM
    层，最后选取最后的预测作为输出情感：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Computational graph
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算图
- en: '![](img/a365b445-98cc-4ede-b781-2a3fd9ea0efc.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a365b445-98cc-4ede-b781-2a3fd9ea0efc.png)'
- en: 'The output of the listing is shown as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的输出如下所示：
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Summary
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned the basics of recurrent neural networks and why
    it is a useful mechanism for time series data processing. You learned about basic
    concepts such as states, word embeddings, and long-term memories. This was followed
    by an example to develop sentiment analysis system. We also implement recurrent
    neural networks using tensorflow.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了递归神经网络的基础知识，以及它为何是处理时间序列数据的有用机制。你学习了基本概念，如状态、词嵌入和长期记忆。接着是一个开发情感分析系统的示例。我们还使用
    TensorFlow 实现了递归神经网络。
- en: In the next chapter, we look at a different kind of neural network called a
    **Generative Model**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将介绍一种不同类型的神经网络，称为**生成模型**。
