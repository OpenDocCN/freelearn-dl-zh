- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Understanding TensorFlow 2
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 TensorFlow 2
- en: 'In this chapter, you will get an in-depth understanding of TensorFlow. This
    is an open source distributed numerical computation framework, and it will be
    the main platform on which we will be implementing all our exercises. This chapter
    covers the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将让你深入理解 TensorFlow。它是一个开源的分布式数值计算框架，也是我们将实现所有练习的主要平台。本章涵盖以下主题：
- en: What is TensorFlow?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow？
- en: The building blocks of TensorFlow (for example, variables and operations)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 的构建模块（例如，变量和操作）
- en: Using Keras for building models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 构建模型
- en: Implementing our first neural network
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现我们的第一个神经网络
- en: We will get started with TensorFlow by defining a simple calculation and trying
    to compute it using TensorFlow. After we complete this, we will investigate how
    TensorFlow executes this computation. This will help us to understand how the
    framework creates a computational graph to compute the outputs and execute this
    graph to obtain the desired outputs. Then we will dive into the details of how
    TensorFlow architecture operates by looking at how TensorFlow executes things,
    with the help of an analogy of how a fancy café works. We will then see how TensorFlow
    1 used to work so that we can better appreciate the amazing features TensorFlow
    2 offers. Note that when we use the word “TensorFlow” by itself, we are referring
    to TensorFlow 2\. We will specifically mention TensorFlow 1 if we are referring
    to TensorFlow 1.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过定义一个简单的计算并尝试使用 TensorFlow 来计算它，开始学习 TensorFlow。完成这一步后，我们将研究 TensorFlow
    如何执行这个计算。这将帮助我们理解框架是如何创建一个计算图来计算输出，并执行该图以获得期望的输出。接着，我们将通过使用一个类比——一个高级咖啡馆是如何运作的——来深入了解
    TensorFlow 架构如何运作，了解 TensorFlow 如何执行任务。然后，我们将回顾 TensorFlow 1 的工作方式，以便更好地理解 TensorFlow
    2 所提供的惊人功能。请注意，当我们单独使用“TensorFlow”这个词时，我们指的是 TensorFlow 2。如果我们提到 TensorFlow 1，则会特别说明。
- en: Having gained a good conceptual and technical understanding of how TensorFlow
    operates, we will look at some of the important computations the framework offers.
    First, we will look at defining various data structures in TensorFlow, such as
    variables and tensors, and we’ll also see how to read inputs through data pipelines.
    Then we will work through some neural network-related operations (for example,
    convolution operation, defining losses, and optimization).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 TensorFlow 的操作有了很好的概念性和技术性理解之后，我们将探讨框架提供的一些重要计算。首先，我们将了解如何在 TensorFlow 中定义各种数据结构，例如变量和张量，并且我们还会看到如何通过数据管道读取输入。接着，我们将学习一些与神经网络相关的操作（例如，卷积操作、定义损失和优化）。
- en: Finally, we will apply this knowledge in an exciting exercise, where we will
    implement a neural network that can recognize images of handwritten digits. You
    will also see that you can implement or prototype neural networks very quickly
    and easily by using a high-level submodule such as Keras.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在一个令人兴奋的练习中应用这些知识，实施一个可以识别手写数字图像的神经网络。你还将看到，通过使用像 Keras 这样的高级子模块，你可以非常快速和轻松地实现或原型化神经网络。
- en: What is TensorFlow?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 TensorFlow？
- en: In *Chapter 1*, *Introduction to Natural Language Processing*, we briefly discussed
    what TensorFlow is. Now let’s take a closer look at it. TensorFlow is an open
    source, distributed numerical computation framework released by Google that is
    mainly intended to alleviate the painful details of implementing a neural network
    (for example, computing derivatives of the weights of the neural network). TensorFlow
    takes this a step further by providing efficient implementations of such numerical
    computations using **Compute Unified Device Architecture** (**CUDA**), which is
    a parallel computational platform introduced by NVIDIA (for more information on
    CUDA, visit [https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/)).
    The **Application Programming Interface** (**API**) of TensorFlow at [https://www.tensorflow.org/api_docs/python/tf/all_symbols](https://www.tensorflow.org/api_docs/python/tf/all_symbols)
    shows that TensorFlow provides thousands of operations that make our lives easier.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 1 章*，*自然语言处理简介* 中，我们简要讨论了什么是 TensorFlow。现在让我们更仔细地了解它。TensorFlow 是由 Google
    发布的一个开源分布式数值计算框架，主要目的是缓解实现神经网络时的痛苦细节（例如，计算神经网络权重的导数）。TensorFlow 通过使用 **计算统一设备架构**（**CUDA**），进一步提供了高效的数值计算实现，CUDA
    是 NVIDIA 推出的并行计算平台（关于 CUDA 的更多信息，请访问 [https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/](https://blogs.nvidia.com/blog/2012/09/10/what-is-cuda-2/)）。TensorFlow
    的 **应用程序编程接口**（**API**）可以在 [https://www.tensorflow.org/api_docs/python/tf/all_symbols](https://www.tensorflow.org/api_docs/python/tf/all_symbols)
    查到，显示了 TensorFlow 提供了成千上万的操作，让我们的生活更轻松。
- en: TensorFlow was not developed overnight. This is a result of the persistence
    of talented, good-hearted developers and scientists who wanted to make a difference
    by bringing deep learning to a wider audience. If you are interested, you can
    take a look at the TensorFlow code at [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow).
    Currently, TensorFlow has around 3,000 contributors, and it sits on top of more
    than 115,000 commits, evolving to be better and better every day.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 不是一夜之间开发出来的。这是由一群有才华、心地善良的开发者和科学家的坚持努力的结果，他们希望通过将深度学习带给更广泛的受众来有所改变。如果你感兴趣，可以查看
    TensorFlow 的代码，地址是 [https://github.com/tensorflow/tensorflow](https://github.com/tensorflow/tensorflow)。目前，TensorFlow
    拥有约 3,000 名贡献者，并且已经有超过 115,000 次提交，每天都在不断发展，变得越来越好。
- en: Getting started with TensorFlow 2
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始使用 TensorFlow 2
- en: 'Now let’s learn about a few essential components in the TensorFlow framework
    by working through a code example. Let’s write an example to perform the following
    computation, which is very common for neural networks:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过一个代码示例来学习 TensorFlow 框架中的一些基本组件。我们来编写一个执行以下计算的示例，这是神经网络中非常常见的操作：
- en: '![](img/B14070_02_002.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_02_002.png)'
- en: 'This computation encompasses what happens in a single layer of a fully connected
    neural network. Here `W` and `x` are matrices and `b` is a vector. Then, “`.`"
    denotes the dot product. sigmoid is a non-linear transformation given by the following
    equation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算涵盖了全连接神经网络中单个层发生的操作。这里 `W` 和 `x` 是矩阵，`b` 是向量。然后，“`.`”表示点积。sigmoid 是一个非线性变换，给定以下方程：
- en: '![](img/B14070_02_001.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_02_001.png)'
- en: We will discuss how to do this computation through TensorFlow step by step.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐步讨论如何通过 TensorFlow 来进行此计算。
- en: 'First, we will need to import TensorFlow and NumPy. NumPy is another scientific
    computation framework that provides various mathematical and other operations
    to manipulate data. Importing them is essential before you run any type of TensorFlow
    or NumPy-related operation in Python:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入 TensorFlow 和 NumPy。NumPy 是另一个科学计算框架，提供了各种数学和其他操作来处理数据。在运行任何与 TensorFlow
    或 NumPy 相关的操作之前，导入它们是必不可少的：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'First, we will write a function that can take the inputs `x`, `W`, and `b`
    and perform this computation for us:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将编写一个函数，该函数可以接收 `x`、`W` 和 `b` 作为输入，并为我们执行这个计算：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we add a Python decorator called `tf.function` as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们添加一个名为 `tf.function` 的 Python 装饰器，如下所示：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Put simply, a Python decorator is just another function. A Python decorator
    provides a clean way to call another function whenever you call the decorated
    function. In other words, every time the `layer()` function is called, `tf.function()`
    is called. This can be used for various purposes, such as:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，Python 装饰器就是另一个函数。Python 装饰器提供了一种干净的方式来调用另一个函数，每次调用被装饰的函数时。换句话说，每次调用 `layer()`
    函数时，都会调用 `tf.function()`。这可以用于多种目的，例如：
- en: Logging the content and operations in a function
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录函数中的内容和操作
- en: Validating the inputs and outputs of another function
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证另一个函数的输入和输出
- en: When the `layer()` function is passing through `tf.function()`, TensorFlow will
    trace the content (in other words, the operations and data) in the function and
    build a computational graph automatically.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `layer()` 函数通过 `tf.function()` 时，TensorFlow 会追踪函数中的内容（换句话说，就是操作和数据），并自动构建计算图。
- en: The computational graph (also known as the dataflow graph) builds a DAG (a directed
    acyclic graph) that shows what kind of inputs are required, and what sort of computations
    need to be done in the program.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图（也称为数据流图）构建一个 DAG（有向无环图），显示程序需要什么样的输入，以及需要进行什么样的计算。
- en: 'In our example, the `layer()` function produces `h` by using inputs `x`, `W`,
    and `b`, and some transformations or operations such as `+` and `tf.matmul()`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，`layer()` 函数通过使用输入 `x`、`W` 和 `b` 以及一些变换或操作（如 `+` 和 `tf.matmul()`）来生成
    `h`：
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_01.jpg](img/B14070_02_01.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_01.jpg](img/B14070_02_01.png)'
- en: 'Figure 2.1: A computational graph of the client'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：客户端的计算图
- en: If we look at an analogy for a DAG, if you think of the output as a *cake*,
    then the *graph* would be the recipe to make that cake using *ingredients* (that
    is, inputs).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一个有向无环图（DAG）的类比，假设输出是一个 *蛋糕*，那么 *图* 就是做这个蛋糕的食谱，其中包含 *原料*（也就是输入）。
- en: The feature that builds this computational graph automatically in TensorFlow
    is known as **AutoGraph**. AutoGraph is not just looking at the operations in
    the passed function; it also scrutinizes the flow of operations. This means that
    you can have `if` statements, or `for`/`while` loops in your function, and AutoGraph
    will take care of those when building the graph. You will see more on AutoGraph
    in the next section.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中自动构建计算图的特性被称为 **AutoGraph**。AutoGraph 不仅查看传递函数中的操作，还会仔细检查操作的流动。这意味着你可以在函数中使用
    `if` 语句或 `for`/`while` 循环，AutoGraph 会在构建图时处理这些情况。你将在下一节中看到更多关于 AutoGraph 的内容。
- en: In TensorFlow 1.x, the user needed to implement the computational graph explicitly.
    This meant the user could not write typical Python code using `if-else` statements
    or `for` loops, but had to explicitly control the flow of operations using special
    bespoke TensorFlow operations such as `tf.cond()` and `tf.control_dependencies()`.
    This is because, unlike TensorFlow 2.x, TensorFlow 1.x did not immediately execute
    operations when you called them. Rather, after they were defined, they needed
    to be executed explicitly using the context of a TensorFlow `Session`. For example,
    when you run the following in TensorFlow 1,
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 1.x 中，用户需要显式地实现计算图。这意味着用户不能像平常那样编写典型的 Python 代码，使用 `if-else` 语句或
    `for` 循环，而必须使用特定的 TensorFlow 操作，如 `tf.cond()` 和 `tf.control_dependencies()`，来显式地控制操作的流。这是因为，与
    TensorFlow 2.x 不同，TensorFlow 1.x 在你调用操作时并不会立即执行它们。相反，在定义它们后，需要通过 TensorFlow `Session`
    上下文显式执行。例如，在 TensorFlow 1 中运行以下代码时，
- en: '`h = tf.nn.sigmoid(tf.matmul(x,W) + b)`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`h = tf.nn.sigmoid(tf.matmul(x,W) + b)`'
- en: '`h` will not have any value until `h` is executed in the context of a `Session`.
    Therefore, `h` could not be treated like any other Python variable. Don’t worry
    if you don’t understand how the `Session` works. It will be discussed in the coming
    sections.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`h` 在 `Session` 上下文中执行之前不会有任何值。因此，`h` 不能像其他 Python 变量一样处理。如果你不理解 `Session`
    是如何工作的，不要担心，我们将在接下来的章节中讨论它。'
- en: 'Next, you can use this function right away, as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以立即使用这个函数，方法如下：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here, `x` is a simple NumPy array:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`x` 是一个简单的 NumPy 数组：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`W` and `b` are TensorFlow variables defined using the `tf.Variable` object.
    `W` and `b` hold tensors. A tensor is essentially an *n*-dimensional array. For
    example, a one-dimensional vector or a two-dimensional matrix are called **tensors**.
    A `tf.Variable` is a mutable structure, which means the values in the tensor stored
    in that variable can change over time. For example, variables are used to store
    neural network weights, which change during the model optimization.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`W` 和 `b` 是使用 `tf.Variable` 对象定义的 TensorFlow 变量。`W` 和 `b` 存储张量。张量本质上是一个 *n*
    维数组。例如，一维向量或二维矩阵都称为 **张量**。`tf.Variable` 是一个可变结构，这意味着存储在该变量中的张量的值可以随时间变化。例如，变量用于存储神经网络的权重，这些权重在模型优化过程中会发生变化。'
- en: 'Also, note that for `W` and `b`, we provide some important arguments, such
    as the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请注意，对于 `W` 和 `b`，我们提供了一些重要的参数，例如以下内容：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'These are called variable initializers and are the tensors that will be assigned
    to the `W` and `b` variables initially. A variable must have an initial value
    provided. Here, `tf.initializers.RandomUniform` means that we uniformly sample
    values between `minval` `(-0.1)` and `maxval` `(0.1)` to assign values to the
    tensors. There are many different initializers provided in TensorFlow ([https://www.tensorflow.org/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)).
    It is also very important to define the *shape* of your initializer when you are
    defining the initializer itself. The `shape` property defines the size of each
    dimension of the output tensor. For example, if `shape` is `[10, 5]`, this means
    that it will be a two-dimensional structure and will have `10` elements on axis
    0 (rows) and `5` elements on axis 1 (columns):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些被称为变量初始化器，是会被初始赋值给 `W` 和 `b` 变量的张量。变量必须提供一个初始值。在这里，`tf.initializers.RandomUniform`
    表示我们在 `minval` `(-0.1)` 和 `maxval` `(0.1)` 之间均匀地抽取值并赋给张量。TensorFlow 提供了许多不同的初始化器（[https://www.tensorflow.org/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)）。在定义初始化器时，定义初始化器的
    *shape*（形状）属性也非常重要。`shape` 属性定义了输出张量的每个维度的大小。例如，如果 `shape` 是 `[10, 5]`，这意味着它将是一个二维结构，在轴
    0（行）上有 `10` 个元素，在轴 1（列）上有 `5` 个元素：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Finally, `h` is called a TensorFlow tensor in general. A TensorFlow tensor is
    an immutable structure. Once a value is assigned to a TensorFlow tensor, it cannot
    be changed.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`h` 通常被称为 TensorFlow 张量。TensorFlow 张量是一个不可变结构。一旦一个值被赋给 TensorFlow 张量，它就不能再被更改。
- en: 'As you can see, the term “tensor” is used in two ways:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，“张量”（tensor）这个术语有两种使用方式：
- en: To refer to an *n*-dimensional array
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要引用一个 *n* 维数组
- en: To refer to an immutable data structure in TensorFlow
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要引用 TensorFlow 中的不可变数据结构
- en: For both, the underlying concept is the same as they hold an *n*-dimensional
    data structure, only differing in the context they are used. The term will be
    used interchangeably to refer to these structures in our discussion.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两者，底层的概念是相同的，因为它们都持有一个 *n* 维的数据结构，只是在使用的上下文上有所不同。我们将在讨论中交替使用这个术语来指代这些结构。
- en: Finally, you can immediately see the value of `h` using,
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以立即看到 `h` 的值，通过以下代码：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: which will give,
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `numpy()` function retrieves the NumPy array from the TensorFlow Tensor
    object. The full code is as below. All the code examples in this chapter will
    be available in the `tensorflow_introduction.ipynb` file in the `ch2` folder:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`numpy()` 函数从 TensorFlow 张量对象中获取 NumPy 数组。完整的代码如下。章节中的所有代码示例都可以在 `ch2` 文件夹中的
    `tensorflow_introduction.ipynb` 文件中找到：'
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For future reference, let’s call our example *the sigmoid example*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 供以后参考，我们称这个示例为 *sigmoid 示例*。
- en: As you can already see, defining a TensorFlow computational graph and executing
    that is very “Pythonic”. This is because TensorFlow executes its operations “eagerly”,
    or immediately after the `layer()` function is called. This is a special mode
    in TensorFlow known as *eager execution* mode. This was an optional mode for TensorFlow
    1, but has been made the default in TensorFlow 2\.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经看到的，定义 TensorFlow 计算图并执行它是非常“Pythonic”的。这是因为 TensorFlow 执行其操作是“急切的”（eager），即在调用`layer()`函数后立即执行。这是
    TensorFlow 中一种特殊模式，称为 *急切执行* 模式。在 TensorFlow 1 中这是一个可选模式，但在 TensorFlow 2 中已经成为默认模式。
- en: Also note that the next two sections will be somewhat complex and technical.
    However, don’t worry if you don’t understand everything completely because the
    explanation will be supplemented with a more digestible, and thorough, real-world
    example that explains how an order is fulfilled in our new-and-improved restaurant,
    *Café Le TensorFlow 2*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 还请注意，接下来的两个章节将会比较复杂且技术性较强。然而，如果你不能完全理解所有内容也不用担心，因为接下来的解释将通过一个更易于理解且全面的实际示例进行补充，这个示例将解释我们改进过的新餐厅
    *Café Le TensorFlow 2* 中如何完成一个订单。
- en: TensorFlow 2 architecture – What happens during graph build?
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 2 架构 – 图构建过程中发生了什么？
- en: Let’s now understand what TensorFlow does when you execute TensorFlow operations.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来了解当你执行 TensorFlow 操作时，TensorFlow 会做些什么。
- en: When you call a function decorated by `tf.function()`, such as the `layer()`
    function, there is quite a bit happening in the background. First, TensorFlow
    will trace all the TensorFlow operations taking place in the function and build
    the computational graph automatically.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当你调用一个由 `tf.function()` 装饰的函数时，比如 `layer()` 函数，后台会发生很多事情。首先，TensorFlow 会追踪函数中所有发生的
    TensorFlow 操作，并自动构建计算图。
- en: In fact, `tf.function()` will return a function that executes the built dataflow
    graph when invoked. Therefore, `tf.function()` is a multi-stage process, where
    it first builds the dataflow graph and then executes it. Additionally, since TensorFlow
    traces each line in the function, if something goes wrong, TensorFlow can point
    to the exact line that is causing the issue.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，`tf.function()` 会返回一个在调用时执行已构建数据流图的函数。因此，`tf.function()` 是一个多阶段的过程，它首先构建数据流图，然后执行它。此外，由于
    TensorFlow 跟踪函数中的每一行代码，如果发生问题，TensorFlow 可以指明导致问题的确切行。
- en: 'In our sigmoid example, the computational, or dataflow, graph would look like
    *Figure 2.2*. A single element or vertex of the graph is called a **node.** There
    are two main types of objects in this graph: *operations* and *tensors*. In the
    preceding example, `tf.nn.sigmoid` is an operation and `h` is a tensor:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 Sigmoid 示例中，计算图或数据流图看起来像*图 2.2*。图的单个元素或顶点称为**节点**。这个图中有两种主要类型的对象：*操作*和*张量*。在前面的示例中，`tf.nn.sigmoid`
    是一个操作，`h` 是一个张量：
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_01.jpg](img/B14070_02_01.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_01.jpg](img/B14070_02_01.png)'
- en: 'Figure 2.2: A computational graph of the client'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：客户端的计算图
- en: The preceding graph shows the order of operations as well as how inputs flow
    through them.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图展示了操作的顺序以及输入如何流经这些操作。
- en: 'Keep in mind that `tf.function()` or AutoGraph is not a silver bullet that
    turns any arbitrary Python function using TensorFlow operations into a computational
    graph; it has its limitations. For example, the current version cannot handle
    recursive calls. To see a full list of the eager mode capabilities, refer to the
    following link: [https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md](https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`tf.function()` 或 AutoGraph 并不是一个万能的解决方案，不能将任何使用 TensorFlow 操作的任意 Python
    函数转换为计算图；它有其局限性。例如，当前版本无法处理递归调用。要查看 eager 模式的完整功能列表，请参考以下链接：[https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md](https://github.com/sourcecode369/tensorflow-1/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md)。
- en: Now we know that TensorFlow is skilled at creating a nice computational graph,
    with all the dependencies and operations so that it knows exactly how, when, and
    where the data flows. However, we did not quite answer how this graph is executed.
    In fact, TensorFlow does quite a bit behind the scenes. For example, the graph
    might be divided into subgraphs, and subsequently into even finer pieces, to achieve
    parallelization. These subgraphs or pieces will then be assigned to workers that
    will perform the assigned task.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道 TensorFlow 擅长创建一个包含所有依赖和操作的漂亮计算图，这样它就能准确知道数据如何、何时以及在哪里流动。然而，我们还没有完全回答这个图是如何执行的。事实上，TensorFlow
    在幕后做了很多工作。例如，图可能会被划分成子图，并进一步拆分成更细的部分，以实现并行化。这些子图或部分将被分配给执行指定任务的工作进程。
- en: TensorFlow architecture – what happens when you execute the graph?
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow 架构——执行图时发生了什么？
- en: The computational graph uses the `tf.GraphDef` protocol to canonicalize the
    dataflow graph and send it to the distributed master. The distributed master would
    perform the actual operation execution and parameter updates in a single-process
    setting. In a distributed setting, the master would delegate these tasks to worker
    processes/devices and manage these worker processes. `tf.GraphDef` is a standardized
    representation of the graph specific to TensorFlow. The distributed master sees
    all computations in the graph and divides the computations into different devices
    (for example, different GPUs and CPUs). TensorFlow operations have multiple kernels.
    A kernel is a device-specific implementation of a certain operation. For example,
    the `tf.matmul()` function will be implemented differently to run on the CPU or
    GPU since, on a GPU, you can achieve much better performance due to more parallelization.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图使用 `tf.GraphDef` 协议来标准化数据流图并将其发送到分布式主节点。分布式主节点将在单进程环境中执行实际的操作执行和参数更新。在分布式环境中，主节点将把这些任务委派给工作进程/设备并管理这些工作进程。`tf.GraphDef`
    是特定于 TensorFlow 的图的标准化表示。分布式主节点可以看到图中的所有计算，并将计算分配到不同的设备（例如，不同的 GPU 和 CPU）。TensorFlow
    操作有多个内核。内核是特定于设备的某个操作的实现。例如，`tf.matmul()` 函数会根据是在 CPU 还是 GPU 上运行进行不同的实现，因为在 GPU
    上可以通过更多的并行化来实现更好的性能。
- en: Next, the computational graph will be broken into subgraphs and pruned by the
    distributed master. Although decomposing the computational graph in *Figure 2.2*
    appears too trivial in our example, the computational graph can exponentially
    grow in real-world solutions with many hidden layers. Additionally, it becomes
    important to break the computational graph into multiple pieces and shave off
    any redundant computations in order to get results faster (for example, in a multi-device
    setting).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，计算图将被分解为子图，并由分布式主节点进行修剪。尽管在我们的示例中，分解*图2.2*中的计算图看起来过于简单，但在实际应用中，计算图可能会在包含多个隐藏层的解决方案中呈指数级增长。此外，为了更快地获得结果（例如，在多设备环境中），将计算图分解成多个部分，并去除任何冗余计算，变得尤为重要。
- en: Executing the graph or a subgraph (if the graph is divided into subgraphs) is
    called a single *task*, where each task is allocated to a single worker (which
    could be a single process or an entire device). These workers can run as a single
    process in a multi-process device (for example, a multi-processing CPU), or run
    on different devices (for example, CPUs and GPUs). In a distributed setting, we
    would have multiple workers executing tasks (for example, multiple workers training
    the model on different batches of data). On the contrary, we have only one set
    of parameters. So how do multiple workers manage to update the same set of parameters?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 执行图或子图（如果图被分为多个子图）称为单个*任务*，每个任务被分配给一个工作进程（该进程可以是一个单独的进程或一个完整的设备）。这些工作进程可以在多进程设备中作为单个进程运行（例如，多核CPU），或在不同的设备上运行（例如，CPU和GPU）。在分布式环境中，我们会有多个工作进程执行任务（例如，多个工作进程在不同的数据批次上训练模型）。相反，我们只有一组参数。那么，多个工作进程如何管理更新同一组参数呢？
- en: 'To solve this, there is one worker that is considered the parameter server
    and will hold the main copy of the parameters. The workers will copy the parameters
    over, update them, and send them back to the parameter server. Typically, the
    parameter server will define some resolution strategy to resolve multiple updates
    coming from multiple workers (for example, taking the mean). These details were
    provided so you can understand the complexity that has gone into TensorFlow. However,
    our book will be based on using TensorFlow in a single-process/worker setting.
    In this setting, the organization of the distributed master, workers, and the
    parameter server is much more straightforward and is absorbed mostly by a special
    session implementation used by TensorFlow. This general workflow of a TensorFlow
    client is depicted in *Figure 2.3*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，会有一个工作进程被视为参数服务器，并持有参数的主要副本。其他工作进程将复制这些参数，更新它们，然后将更新后的参数发送回参数服务器。通常，参数服务器会定义一些解决策略来处理来自多个工作进程的多个更新（例如，取平均值）。这些细节的提供是为了帮助你理解TensorFlow中涉及的复杂性。然而，我们的书籍将基于在单进程/单工作进程设置中使用TensorFlow。在这种设置下，分布式主节点、工作进程和参数服务器的组织方式要简单得多，并且大部分都由TensorFlow使用的特殊会话实现来吸收。TensorFlow客户端的这一通用工作流程在*图2.3*中得到了展示：
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_02.jpg](img/B14070_02_03.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_02.jpg](img/B14070_02_03.png)'
- en: 'Figure 2.3: The generic execution of a TensorFlow client. A TensorFlow client
    starts with a graph that gets sent to the distributed master. The master spins
    up worker processes to perform actual tasks and parameter updates'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3：TensorFlow客户端的通用执行。TensorFlow客户端从一个图开始，图被发送到分布式主节点。主节点启动工作进程来执行实际任务和参数更新。
- en: 'Once the calculation is done, the session brings back the updated data to the
    client from the parameter server. The architecture of TensorFlow is shown in *Figure
    2.4*:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算完成，会话将从参数服务器中将更新后的数据返回给客户端。TensorFlow的架构如*图2.4*所示：
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_04.png](img/B14070_02_04.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_04.png](img/B14070_02_04.png)'
- en: 'Figure 2.4: TensorFlow framework architecture. This explanation is based on
    the official TensorFlow documentation found at: [https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/architecture.md](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/architecture.md)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4：TensorFlow框架架构。此解释基于官方的TensorFlow文档，文档链接为：[https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/architecture.md](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/extend/architecture.md)
- en: Most of the changes introduced in TensorFlow 2 can be attributed to front-end
    changes. That is, how the dataflow graph is built and when the graph is executed.
    The way the graph is executed remains more or less the same in TensorFlow 1 and
    2.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2 中引入的大部分变化都可以归结为前端的变化。也就是说，数据流图是如何构建的，以及何时执行图。图的执行方式在 TensorFlow
    1 和 2 中基本保持不变。
- en: Now we know what happens end-to-end from the moment you execute `tf.function()`,
    but this was a very technical explanation, and nothing explains something better
    than a good analogy. Therefore, we will try to understand TensorFlow 2 with an
    analogy to our new and improved Café Le TensorFlow 2.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了从你执行 `tf.function()` 这一刻起发生的端到端过程，但这只是一个非常技术性的解释，最好的理解方式是通过一个好的类比。因此，我们将尝试通过一个类比来理解
    TensorFlow 2，就像我们对新升级版的 Café Le TensorFlow 2 的理解一样。
- en: Café Le TensorFlow 2 – understanding TensorFlow 2 with an analogy
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Café Le TensorFlow 2 – 通过类比来理解 TensorFlow 2
- en: Let’s say the owners renovated our previous Café Le TensorFlow (this is an analogy
    from the first edition) and reopened it as Café Le TensorFlow 2\. The word around
    the town is that it’s much more opulent than it used to be. Remembering the great
    experience you had before, you book a table instantly and go there to grab a seat.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设老板们对我们之前的 Café Le TensorFlow（这是第一版的类比）进行了翻新，并重新开业为 Café Le TensorFlow 2。镇上传闻它比以前更加奢华。记得之前那次美好的体验后，你立刻预定了座位并赶去那里占个座。
- en: You want to order a *chicken burger with extra cheese and no tomatoes*. And
    you realize the café is indeed fancy. There’re no waiters here, but a voice-enabled
    tablet for each table into which you say what you want. This will get converted
    to a standard format that the chefs will understand (for example, table number,
    menu item ID, quantity, and special requirements).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你想点一份*加奶酪、不加番茄的鸡肉汉堡*。然后你意识到这家咖啡馆确实很高档。这里没有服务员，每桌都有一个语音启用的平板电脑，你可以对它说出你想要的。这会被转化为厨师能理解的标准格式（例如，桌号、菜单项
    ID、数量和特别要求）。
- en: Here, you represent the TensorFlow 2 program. The ability of the voice-enabled
    tablet that converts your voice (or TensorFlow operations) to the standard format
    (or GraphDef format) is analogous to the AutoGraph feature.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，你代表了 TensorFlow 2 程序。将你的语音（或 TensorFlow 操作）转化为标准格式（或 GraphDef 格式）的语音启用平板电脑功能，类似于
    AutoGraph 特性。
- en: Now comes the best part. As soon as you start speaking, a manager will be looking
    at your order and assigning various tasks to chefs. The manager is responsible
    for making sure things happen as quickly as possible. The kitchen manager makes
    decisions, such as how many chefs are required to make the dish and which chefs
    are the best candidates for the job. The kitchen manager represents the distributed
    master.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在到了最精彩的部分。一旦你开始说话，经理就会查看你的订单，并将各种任务分配给厨师。经理负责确保一切尽可能快地完成。厨房经理做出决策，例如需要多少厨师来制作这道菜，哪些厨师是最合适的。厨房经理代表着分布式的主节点。
- en: Each chef has a cook whose responsibility it is to provide the chef with the
    right ingredients, equipment, and so forth. So, the kitchen manager takes the
    order to a single chef and a cook (a burger is not that hard to prepare) and asks
    them to prepare the dish. The chef looks at the order and tells the cook what
    is needed. So, the cook first finds the things that will be required (for example,
    buns, patties, and onions) and keeps them close to fulfill the chef’s requests
    as soon as possible. Moreover, the chef might also ask to keep the intermediate
    results (for example, cut vegetables) of the dish temporarily until the chef needs
    it back again. In our example, the chef is the operation executor, and the cook
    is the parameter server.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 每个厨师都有一个助手，负责为厨师提供所需的食材、设备等。因此，厨房经理会将订单交给一位厨师和一位助手（比如，汉堡的准备并不难），并要求他们制作这道菜。厨师查看订单后，告诉助手需要什么。然后，助手首先找到所需的物品（例如，面包、肉饼和洋葱），并将它们放在手边，以便尽快完成厨师的要求。此外，厨师可能还会要求暂时保存菜肴的中间结果（例如，切好的蔬菜），直到厨师再次需要它们。在我们的例子中，厨师是操作执行者，而助手是参数服务器。
- en: This café is full of surprises. As you are speaking out your order (that is,
    invoking Python functions that have TensorFlow operations), you see it getting
    prepared in real time through the tablet on your table (that is, eager execution).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这家咖啡馆充满了惊喜。当你说出你的订单（也就是说，调用包含 TensorFlow 操作的 Python 函数）时，你通过桌上的平板电脑实时看到订单正在被准备（也就是急切执行）。
- en: 'The best thing about this video feed is that, if you see that the chef did
    not put enough cheese, you know exactly why the burger wasn’t as good as expected.
    So, you can either order another one or provide specific feedback. This is a great
    improvement over how TensorFlow 1 did things, where they would take your order
    and you would not see anything until the full burger had been prepared. This process
    is shown in *Figure 2.5*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个视频教程的最棒之处在于，如果你看到厨师没有放足够的奶酪，你就能立刻明白为什么汉堡不如预期的好。所以，你可以选择再点一个或者给出具体的反馈。这比 TensorFlow
    1 的做法要好得多，因为他们会先接受你的订单，然后你在汉堡准备好之前什么也看不见。这个过程在*图 2.5*中展示：
- en: '![](img/B14070_02_05.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_02_05.png)'
- en: 'Figure 2.5: The restaurant analogy illustrated'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：餐厅类比示意图
- en: Let’s now have a look back at how TensorFlow 1 used to work.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下 TensorFlow 1 的工作方式。
- en: 'Flashback: TensorFlow 1'
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾：TensorFlow 1
- en: We said numerous times that TensorFlow 2 is very different from TensorFlow 1\.
    But we still don’t know what it used to be like. Therefore, let’s now do a bit
    of time traveling to see how the same sigmoid computation could have been implemented
    in TensorFlow 1.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们多次提到过，TensorFlow 2 与 TensorFlow 1 的区别非常大。但我们仍然不知道它以前是怎样的。现在，让我们做一场时光旅行，看看同样的
    sigmoid 计算在 TensorFlow 1 中是如何实现的。
- en: '**Warning**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告**'
- en: You will not be able to execute the following code in TensorFlow 2.x as it stands.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你无法直接在 TensorFlow 2.x 中执行以下代码。
- en: 'First, we’ll define a `graph` object, which we will populate with operations
    and variables later:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个 `graph` 对象，稍后我们将向其中添加操作和变量：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `graph` object contains the computational graph that connects the various
    inputs and outputs we define in our program to get the final desired output. This
    is the same graph we discussed earlier. Also, we’ll define a `session` object
    that takes the defined graph as the input, which executes the graph. In other
    words, compared to TensorFlow 2, the `graph` object and the `session` object do
    what happens when you invoke them decorated by `tf.function()`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`graph` 对象包含了计算图，它将我们程序中定义的各种输入和输出连接起来，从而得到最终期望的输出。这就是我们之前讨论的那个图。同时，我们还会定义一个
    `session` 对象，作为输入传递给已定义的图，用以执行这个图。换句话说，相较于 TensorFlow 2，`graph` 对象和 `session`
    对象做的事情就是当你调用它们并用 `tf.function()` 装饰时发生的事情。'
- en: 'Now we’ll define a few tensors, namely `x`, `W`, `b`, and `h`. There are several
    different ways that you can define tensors in TensorFlow 1\. Here, we will look
    at three such different approaches:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义几个张量，即 `x`、`W`、`b` 和 `h`。在 TensorFlow 1 中，你可以用多种不同的方式定义张量。这里，我们将介绍三种不同的方法：
- en: First, `x` is a placeholder. Placeholders, as the name suggests, are not initialized
    with any value. Rather, we will provide the value on the fly at the time of the
    graph execution. If you remember from the TensorFlow 2 sigmoid exercise, we fed
    `x` (which was a NumPy array) directly to the function `layer(x, w, b)`. Unlike
    in TensorFlow 2, you cannot feed NumPy arrays directly to TensorFlow 1 graphs
    or operations.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，`x` 是一个占位符。占位符，顾名思义，未初始化任何值。相反，我们会在图执行时动态地提供其值。如果你记得 TensorFlow 2 中的 sigmoid
    练习，我们直接将 `x`（它是一个 NumPy 数组）传递给函数 `layer(x, w, b)`。与 TensorFlow 2 不同，在 TensorFlow
    1 中，不能直接将 NumPy 数组传递给图或操作。
- en: Next, we have the variables `W` and `b`. Variables are defined similarly to
    TensorFlow 2 with some minor changes in the syntax.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们有变量 `W` 和 `b`。变量的定义方式与 TensorFlow 2 类似，只是语法上有一些小的变化。
- en: Finally, we have `h`, which is an immutable tensor produced by performing some
    operations on `x`, `W`, and `b`. Note that you will not see the value of `h` immediately
    as you needed to manually execute the graph in TensorFlow 1.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们有 `h`，它是一个不可变的张量，由对 `x`、`W` 和 `b` 执行一些操作生成。请注意，你不会立即看到 `h` 的值，因为在 TensorFlow
    1 中，你需要手动执行图才能查看其值。
- en: 'These tensors are defined as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这些张量的定义如下：
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The lifetime of variables in TensorFlow 1 was managed by the session object,
    meaning that variables lived in memory for as long as the session lived (even
    after losing references to them in the code). However, in TensorFlow 2, variables
    are removed soon after the variables are not referenced in the code, just like
    in Python.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 1 中变量的生命周期由 session 对象管理，这意味着变量在 session 存在期间会一直驻留在内存中（即使代码中不再引用它们）。然而，在
    TensorFlow 2 中，变量会在代码中不再引用后很快被移除，就像在 Python 中一样。
- en: 'Next, we’ll run an initialization operation that initializes the variables
    in the graph, `W` and `b`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将运行一个初始化操作，用于初始化图中的变量 `W` 和 `b`：
- en: '[PRE12]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we will execute the graph to obtain the final output we need, `h`. This
    is done by running `session.run(...)`, where we provide the value to the placeholder
    as an argument of the `session.run()` command:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将执行计算图以获取最终所需的输出 `h`。这通过运行 `session.run(...)` 来完成，在此过程中我们将值提供给占位符作为 `session.run()`
    命令的参数：
- en: '[PRE13]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we close the session, releasing any resources held by the `session`
    object:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们关闭会话，释放 `session` 对象占用的任何资源：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here is the full code of this TensorFlow 1 example:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这个 TensorFlow 1 示例的完整代码：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As you can see, before TensorFlow 2 the user had to:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，在 TensorFlow 2 之前，用户需要：
- en: Define the computational graph using various TensorFlow data structures (for
    example, `tf.placeholder`) and operations (for example, `tf.matmul()`)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种 TensorFlow 数据结构（例如，`tf.placeholder`）和操作（例如，`tf.matmul()`）定义计算图。
- en: Execute the required part of the graph using `session.run()` to fetch the results
    by feeding the correct data into the session
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `session.run()` 执行计算图的相关部分，通过将正确的数据传递给 session 来获取结果。
- en: 'In conclusion, TensorFlow 1.x had several limitations:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，TensorFlow 1.x 存在若干限制：
- en: Coding with TensorFlow 1 did not provide the same intuitive “Pythonic” feeling
    as you needed to define the computational graph first and then invoke the execution
    of it. This is known as declarative programming.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 1 编码并不像使用 Python 那样直观，因为你需要先定义计算图，然后再执行它。这被称为声明式编程。
- en: The design in TensorFlow 1 made it very hard to break the code down into manageable
    functions as the user needed to define the graph fully, before doing any computations.
    This resulted in very large functions or pieces of code containing very large
    computational graphs.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 1 的设计使得将代码拆分成可管理的函数非常困难，因为用户需要在进行任何计算之前完全定义计算图。这导致了包含非常大计算图的非常大的函数或代码块。
- en: It was very difficult to do real-time debugging of the code as TensorFlow had
    its own runtime that used `session.run()`.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 TensorFlow 有自己的运行时，使用 `session.run()` 进行实时调试非常困难。
- en: But also, it was not without some advantages, such as the efficiency brought
    about by declaring the full computational graph upfront. Knowing all the computations
    in advance meant TensorFlow 1 could perform all sorts of optimizations (for example,
    graph pruning) to run the graph efficiently.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 但是，它也有一些优点，例如通过预先声明完整的计算图带来的效率。提前知道所有计算意味着 TensorFlow 1 可以进行各种优化（例如，图修剪），从而高效地运行计算图。
- en: In this part of the chapter, we discussed our first example in TensorFlow2 and
    the architecture of TensorFlow. Finally, we compared and contrasted TensorFlow
    1 and 2\. Next, we will discuss the various building blocks of TensorFlow 2.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的这一部分，我们讨论了 TensorFlow 2 的第一个示例以及 TensorFlow 的架构。最后，我们对比了 TensorFlow 1 和
    2。接下来，我们将讨论 TensorFlow 2 的各种构建模块。
- en: Inputs, variables, outputs, and operations
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入、变量、输出和操作
- en: 'Now we are returning from our journey into TensorFlow 1 and stepping back to
    TensorFlow 2\. Let’s proceed to the most common elements that comprise a TensorFlow
    2 program. If you read any of the millions of TensorFlow clients available on
    the internet, the TensorFlow-related code all falls into one of these buckets:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们从 TensorFlow 1 的旅程中回到 TensorFlow 2，让我们继续探讨构成 TensorFlow 2 程序的最常见元素。如果你浏览互联网上的任意一个
    TensorFlow 客户端代码，所有与 TensorFlow 相关的代码都可以归入以下几类：
- en: '**Inputs**: Data used to train and test our algorithms'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：用于训练和测试我们算法的数据。'
- en: '**Variables**: Mutable tensors, mostly defining the parameters of our algorithms'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变量**：可变张量，主要定义我们算法的参数。'
- en: '**Outputs**: Immutable tensors storing both terminal and intermediate outputs'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**：不可变张量，存储终端和中间输出。'
- en: '**Operations**: Various transformations for inputs to produce the desired outputs'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作**：对输入进行各种变换以产生期望的输出。'
- en: 'In our earlier sigmoid example, we can find instances of all these categories.
    We list the respective TensorFlow elements and the notation used in the sigmoid
    example in *Table 2.1*:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的 sigmoid 示例中，可以找到所有这些类别的实例。我们列出了相应的 TensorFlow 元素以及在 *表 2.1* 中使用的符号：
- en: '| **TensorFlow element** | **Value from example client** |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| **TensorFlow 元素** | **示例客户端中的值** |'
- en: '| Inputs | `x` |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | `x` |'
- en: '| Variables | `W` and `b` |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | `W` 和 `b` |'
- en: '| Outputs | `h` |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | `h` |'
- en: '| Operations | `tf.matmul(...)`, `tf.nn.sigmoid(...)` |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | `tf.matmul(...)`，`tf.nn.sigmoid(...)` |'
- en: 'Table 2.1: The different types of TensorFlow primitives we have encountered
    so far'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1：到目前为止我们遇到的不同类型的 TensorFlow 原语
- en: The following subsections explain each of these TensorFlow elements listed in
    the table in more detail.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节将更详细地解释表中列出的每个 TensorFlow 元素。
- en: Defining inputs in TensorFlow
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中定义输入
- en: 'There are three different ways you can feed data to a TensorFlow program:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将数据传递给 TensorFlow 程序的方式有三种：
- en: Feeding data as NumPy arrays
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据作为 NumPy 数组输入
- en: Feeding data as TensorFlow tensors
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据作为 TensorFlow 张量输入
- en: Using the `tf.data` API to create an input pipeline
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `tf.data` API 创建输入管道
- en: Next, we will discuss a few different ways you can feed data to TensorFlow operations.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论几种不同的方式，你可以将数据传递给 TensorFlow 操作。
- en: Feeding data as NumPy arrays
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据作为 NumPy 数组输入
- en: This is the simplest way to feed data into a TensorFlow program. Here, you pass
    a NumPy array as an input to the TensorFlow operation and the result is executed
    immediately. This is exactly what we did in the sigmoid example. If you look at
    `x`, it is a NumPy array.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将数据传递给 TensorFlow 程序的最简单方式。在这里，你将一个 NumPy 数组作为输入传递给 TensorFlow 操作，结果会立即执行。这正是我们在
    sigmoid 示例中所做的。如果你查看 `x`，它是一个 NumPy 数组。
- en: Feeding data as tensors
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将数据作为张量输入
- en: The second method is like the first one, but the type of data is different.
    Here, we are defining `x` as a TensorFlow tensor.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法与第一种类似，但数据类型不同。在这里，我们将 `x` 定义为一个 TensorFlow 张量。
- en: 'To see this in action, let’s modify our sigmoid example. Remember that we defined
    `x` as:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看这个过程，让我们修改我们的 sigmoid 示例。记得我们将 `x` 定义为：
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Instead, let’s define this as a tensor that contains specific values:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，让我们将其定义为包含特定值的张量：
- en: '[PRE17]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Also, the full code would become as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，完整的代码将如下所示：
- en: '[PRE18]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Let’s now discuss how we can define data pipelines in TensorFlow.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论如何在 TensorFlow 中定义数据管道。
- en: Building a data pipeline using the tf.data API
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 tf.data API 构建数据管道
- en: '`tf.data` provides you with a convenient way to build data pipelines in TensorFlow.
    Input pipelines are designed for more heavy-duty programs that need to process
    a lot of data. For example, if you have a small dataset (for example, the MNIST
    dataset) that fits into the memory, input pipelines would be excessive. However,
    when working with complex data or problems, where you might need to work with
    large datasets that do not fit in memory, augment the data (for example, for adjusting
    image contrast/brightness), numerically transform it (for example, standardize),
    and so on. The `tf.data` API provides convenient functions that can be used to
    easily load and transform your data. Furthermore, it streamlines your data ingestion
    code with the model training.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.data` 为你提供了在 TensorFlow 中构建数据管道的便捷方式。输入管道是为需要处理大量数据的更高负载程序设计的。例如，如果你有一个小数据集（例如
    MNIST 数据集），它能完全加载到内存中，那么输入管道就显得多余了。然而，当处理复杂数据或问题时，可能需要处理不适合内存的大数据集，进行数据增强（例如，调整图像对比度/亮度），进行数值变换（例如，标准化）等。`tf.data`
    API 提供了便捷的函数，可以轻松加载和转换数据。此外，它简化了与模型训练相关的数据输入代码。'
- en: Additionally, the `tf.data` API offers various options to enhance the performance
    of your data pipeline, such as multi-processing and pre-fetching data. Pre-fetching
    refers to bringing data into the memory before it’s required and keeping it ready.
    We will discuss these methods in more detail as they are used in the upcoming
    chapters.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`tf.data` API 提供了多种选项来增强数据管道的性能，例如多进程处理和数据预取。预取指的是在数据需要之前将数据加载到内存中并保持其准备好。我们将在接下来的章节中更详细地讨论这些方法。
- en: 'When creating an input pipeline, we intend to perform the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建输入管道时，我们打算执行以下操作：
- en: Source the data from a data source (for example, an in-memory NumPy array, CSV
    file on disk, or individual files such as images).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据源获取数据（例如，一个内存中的 NumPy 数组、磁盘上的 CSV 文件或单独的文件如图像）。
- en: Apply various transformations to the data (for example, cropping/resizing image
    data).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据应用各种变换（例如，裁剪/调整图像数据的大小）。
- en: Iterate the resulting dataset element/batch-wise. Batching is required as deep
    learning models are trained on randomly sampled batches of data. As the datasets
    these models are trained on are large, they typically do not fit in memory.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按元素/批次迭代结果数据集。由于深度学习模型是基于随机采样的数据批次进行训练的，因此批处理是必要的。由于这些模型训练的数据集通常较大，因此通常无法完全加载到内存中。
- en: Let’s write an input pipeline using TensorFlow’s `tf.data` API. In this example,
    we have three text files (`iris.data.1`, `iris.data.2`, and `iris.data.3`) in
    CSV format, each file having 50 lines and each line having 4 floating-point numbers
    (in other words, various lengths associated with a flower) and a string label
    separated by commas (an example line would be `5.6,2.9,3.6,1.3,Iris-versicolor`).
    We will now use the `tf.data` API to read data from these files. We also know
    that some of this data is corrupted (as with any real-life machine learning project).
    In our case, some data points have negative lengths. So, let’s first write a pipeline
    to go through the data row by row and print the corrupted outputs.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 TensorFlow 的 `tf.data` API 来编写输入管道。在这个示例中，我们有三个文本文件（`iris.data.1`、`iris.data.2`
    和 `iris.data.3`），每个文件都是 CSV 格式，包含 50 行数据，每行有 4 个浮动的数字（也就是花的各种长度）和一个由逗号分隔的字符串标签（例如，一行数据可能是
    `5.6,2.9,3.6,1.3,Iris-versicolor`）。我们将使用 `tf.data` API 从这些文件中读取数据。我们还知道这些数据中有些是损坏的（就像任何现实中的机器学习项目一样）。在我们的例子中，某些数据点的长度是负数。因此，我们首先编写一个管道，逐行处理数据并打印出损坏的数据。
- en: For more information, refer to the official TensorFlow page on importing data
    at [https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参考官方的 TensorFlow 数据导入页面 [https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data)。
- en: 'First, let’s import a few important libraries as before:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，像以前一样导入几个重要的库：
- en: '[PRE19]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we will define a list containing the filenames:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个包含文件名的列表：
- en: '[PRE20]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we will use one of the dataset readers provided in TensorFlow. The dataset
    reader takes in a list of filenames and another list that specifies the data types
    of each column in the dataset. As we saw previously, we have four floating numbers
    and one string:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用 TensorFlow 提供的其中一个数据集读取器。数据集读取器接受一个文件名列表和另一个指定数据集每列数据类型的列表。如我们之前所见，我们有四个浮动数字和一个字符串：
- en: '[PRE21]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we will organize our data into inputs and labels as follows:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将按如下方式组织数据为输入和标签：
- en: '[PRE22]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We are using lambda functions to separate out `x1,x2,x3,x4` into one dataset
    and `y` to another dataset, along with the `dataset.map()` function.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用 lambda 函数将 `x1, x2, x3, x4` 分别提取到一个数据集中，而将 `y` 提取到另一个数据集，并使用 `dataset.map()`
    函数。
- en: 'Lambda functions are a special type of function that allow you to define some
    computations succinctly. With lambda functions, you don’t need to name your function,
    which can be quite handy if you are using a certain function only once in your
    code. The format of the lambda function looks like:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 函数是一种特殊类型的函数，它允许你简洁地定义一些计算。使用 lambda 函数时，你无需为你的函数命名，如果你在代码中只使用某个函数一次，这会非常方便。lambda
    函数的格式如下：
- en: '`lambda <arguments>: <result returned after the computation>`'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`lambda <arguments>: <result returned after the computation>`'
- en: 'For example, if you need to write a function that adds two numbers, simply
    write:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你需要编写一个函数来加两个数字，直接写：
- en: '`lambda x, y: x+y`'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`lambda x, y: x+y`'
- en: Here, `tf.stack()` stacks individual tensors (here, the individual feature)
    to a single tensor. When using the `map` function, you first need to visualize
    what needs to be done to a single item in the dataset (a single item in our case
    is a single row from the dataset), and write the transformation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`tf.stack()` 将单个张量（在这里是单个特征）堆叠为一个张量。当使用 `map` 函数时，你首先需要可视化对数据集中单个项目（在我们的例子中是数据集中的一行）需要做的操作，并编写转换代码。
- en: 'The map function is very simple but powerful. All it does is transform a set
    of given inputs into a new set of values. For example, if you have a list, `xx`,
    that contains a list of numbers and want to convert them to power 2 element-wise,
    you can write something like `xx_pow = map(lambda x: x**2, xx)`. And this can
    be very easily parallelized as there’s no dependency between items.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 'map 函数非常简单但功能强大。它的作用是将一组给定的输入转换为一组新的值。例如，如果你有一个包含数字的列表`xx`，并且想要逐个元素将其转换为平方，你可以写出类似`xx_pow
    = map(lambda x: x**2, xx)`的代码。由于项目之间没有依赖关系，这个过程非常容易并行化。'
- en: 'Next, you can iterate through this dataset, examining individual data points,
    as you would iterate through a normal Python list. Here, we are printing out all
    the corrupted items:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你可以像遍历普通 Python 列表那样遍历这个数据集，检查每个数据点。在这里，我们打印出所有受损的项目：
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Since you don’t want those corrupted inputs in your dataset, you can use the
    `dataset.filter()` function to filter out those corrupted entries as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你不希望数据集中包含那些受损的输入，你可以使用 `dataset.filter()` 函数来过滤掉这些受损的条目，方法如下：
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here we are checking whether the minimum element in `x` is greater than zero;
    if not, those elements will be filtered out of the dataset.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们检查 `x` 中的最小元素是否大于零；如果不是，这些元素将被从数据集中过滤掉。
- en: 'Another useful function is `dataset.batch()`. When training deep neural networks,
    we often traverse the dataset in batches, not individual items. `dataset.batch()`
    provides a convenient way to do that:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的函数是 `dataset.batch()`。在训练深度神经网络时，我们通常以批次而不是单个项遍历数据集。`dataset.batch()`
    提供了一个方便的方式来做到这一点：
- en: '[PRE25]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, if you print the shape of a single element in your dataset, you should
    get the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你打印数据集中单个元素的形状，你应该会得到以下内容：
- en: '[PRE26]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now that we have examined the three different methods you can use to define
    inputs in TensorFlow, let’s see how we can define variables in TensorFlow.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经检查了在 TensorFlow 中定义输入的三种不同方法，接下来让我们看看如何在 TensorFlow 中定义变量。
- en: Defining variables in TensorFlow
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中定义变量
- en: Variables play an important role in TensorFlow. A variable is essentially a
    tensor with a specific shape defining how many dimensions the variable will have
    and the size of each dimension. However, unlike a regular TensorFlow tensor, variables
    are *mutable*; meaning that the value of the variables can change after they are
    defined. This is an ideal property to have to implement the parameters of a learning
    model (for example, neural network weights), where the weights change slightly
    after each step of learning. For example, if you define a variable with `x = tf.Variable(0,dtype=tf.int32)`,
    you can change the value of that variable using a TensorFlow operation such as
    `tf.assign(x,x+1)`. However, if you define a tensor such as `x = tf.constant(0,dtype=tf.int32)`,
    you cannot change the value of the tensor, as you could for a variable. It should
    stay `0` until the end of the program execution.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 变量在 TensorFlow 中扮演着重要角色。变量本质上是一个张量，具有特定的形状，定义了变量将具有多少维度以及每个维度的大小。然而，与常规的 TensorFlow
    张量不同，变量是*可变的*；这意味着在定义后，变量的值可以发生变化。这是实现学习模型参数（例如，神经网络权重）所需的理想特性，因为权重会在每一步学习后略微变化。例如，如果你定义了一个变量
    `x = tf.Variable(0,dtype=tf.int32)`，你可以使用 TensorFlow 操作如 `tf.assign(x,x+1)` 来改变该变量的值。然而，如果你定义了一个张量
    `x = tf.constant(0,dtype=tf.int32)`，你就无法像改变变量那样改变张量的值。它应该保持为 `0`，直到程序执行结束。
- en: 'Variable creation is quite simple. In our sigmoid example, we already created
    two variables, `W` and `b`. When creating a variable, a few things are extremely
    important. We will list them here and discuss each in detail in the following
    paragraphs:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 变量创建非常简单。在我们的 sigmoid 示例中，我们已经创建了两个变量，`W` 和 `b`。在创建变量时，有几个非常重要的事项。我们将在此列出它们，并在接下来的段落中详细讨论每一个：
- en: Variable shape
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量形状
- en: Initial values
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始值
- en: Data type
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据类型
- en: Name (optional)
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名称（可选）
- en: The variable shape is a list of the `[x,y,z,...]` format. Each value in the
    list indicates how large the corresponding dimension or axis is. For instance,
    if you require a 2D tensor with 50 rows and 10 columns as the variable, the shape
    would be equal to `[50,10]`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 变量形状是一个 `[x,y,z,...]` 格式的列表。列表中的每个值表示相应维度或轴的大小。例如，如果你需要一个包含 50 行和 10 列的 2D 张量作为变量，形状将是
    `[50,10]`。
- en: The dimensionality of the variable (that is, the length of the `shape` vector)
    is recognized as the rank of the tensor in TensorFlow. Do not confuse this with
    the rank of a matrix.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的维度（即 `shape` 向量的长度）在 TensorFlow 中被认为是张量的秩。不要将其与矩阵的秩混淆。
- en: Tensor rank in TensorFlow indicates the dimensionality of the tensor; for a
    two-dimensional matrix, *rank* = 2.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中的张量秩表示张量的维度；对于一个二维矩阵，*秩* = 2。
- en: 'Next, a variable requires an *initial* value to be initialized with. TensorFlow
    provides several different initializers for our convenience, including constant
    initializers and normal distribution initializers. Here are a few popular TensorFlow
    initializers you can use to initialize variables:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，变量需要一个*初始*值来初始化。TensorFlow 为我们的便利提供了几种不同的初始化器，包括常量初始化器和正态分布初始化器。以下是你可以用来初始化变量的几种流行的
    TensorFlow 初始化器：
- en: '`tf.initializers.Zeros`'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.initializers.Zeros`'
- en: '`tf.initializers.Constant`'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.initializers.Constant`'
- en: '`tf.initializers.RandomNormal`'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.initializers.RandomNormal`'
- en: '`tf.initializers.GlorotUniform`'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tf.initializers.GlorotUniform`'
- en: 'The shape of the variable can be provided as a part of the initializer as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的形状可以作为初始化器的一部分提供，如下所示：
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The data type plays an important role in determining the size of a variable.
    There are many different data types, including the commonly used `tf.bool`, `tf.uint8`,
    `tf.float32`, and `tf.int32`. Each data type has a number of bits required to
    represent a single value with that type. For example, `tf.uint8` requires 8 bits,
    whereas `tf.float32` requires 32 bits. It is common practice to use the same data
    types for computations, as doing otherwise can lead to data type mismatches. So,
    if you have two different data types for two tensors that you need to transform,
    you have to explicitly convert one tensor to the other tensor’s type using the
    `tf.cast(...)` operation.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型在确定变量的大小时起着重要作用。TensorFlow 有许多不同的数据类型，包括常用的 `tf.bool`、`tf.uint8`、`tf.float32`
    和 `tf.int32`。每种数据类型都需要一定的位数来表示该类型的单个值。例如，`tf.uint8` 需要 8 位，而 `tf.float32` 需要 32
    位。通常建议在计算中使用相同的数据类型，因为使用不同的数据类型可能会导致类型不匹配。因此，如果你有两个不同数据类型的张量需要转换，你必须使用 `tf.cast(...)`
    操作显式地将一个张量转换为另一个张量的数据类型。
- en: The `tf.cast(...)` operation is designed to cope with such situations. For example,
    if you have an `x` variable with the `tf.int32` type, which needs to be converted
    to `tf.float32`, employ `tf.cast(x,dtype=tf.float32)` to convert `x` to `tf.float32`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.cast(...)` 操作旨在处理这种情况。例如，如果你有一个 `x` 变量，类型为 `tf.int32`，并且需要将其转换为 `tf.float32`，可以使用
    `tf.cast(x,dtype=tf.float32)` 将 `x` 转换为 `tf.float32`。'
- en: Finally, the *name* of the variable will be used as an ID to identify that variable
    in the graph. If you ever visualize the computational graph, the variable will
    appear by the argument passed to the `name` keyword. If you do not specify a name,
    TensorFlow will use the default naming scheme.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，变量的*名称*将作为 ID 用于在计算图中标识该变量。如果你曾经可视化过计算图，变量将以传递给 `name` 关键字的参数出现。如果没有指定名称，TensorFlow
    将使用默认的命名方案。
- en: 'Note that the Python variable `tf.Variable` is assigned to is not known by
    the computational graph, and is not a part of TensorFlow variable naming. Consider
    this example where you specify a TensorFlow variable as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Python 变量 `tf.Variable` 被赋值后，在计算图中是不可见的，且不属于 TensorFlow 变量命名的一部分。考虑以下示例，你指定一个
    TensorFlow 变量如下：
- en: '[PRE28]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, the TensorFlow graph will know this variable by the name `b` and not `a`.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，TensorFlow 图会通过名称 `b` 来识别此变量，而不是 `a`。
- en: Moving on, let’s talk about how to define TensorFlow outputs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来讨论如何定义 TensorFlow 输出。
- en: Defining outputs in TensorFlow
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中定义输出
- en: 'TensorFlow outputs are usually tensors, and the result of a transformation
    to either an input, or a variable, or both. In our example, `h` is an output,
    where `h = tf.nn.sigmoid(tf.matmul(x,W) + b)`. It is also possible to give such
    outputs to other operations, forming a chained set of operations. Furthermore,
    they do not necessarily have to be TensorFlow operations. You also can use standard
    Python arithmetic with TensorFlow. Here is an example:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的输出通常是张量，并且是对输入、变量或两者的变换结果。在我们的示例中，`h` 是一个输出，其中 `h = tf.nn.sigmoid(tf.matmul(x,W)
    + b)`。也可以将这种输出传递给其他操作，形成一系列链式操作。此外，它们不一定非得是 TensorFlow 操作，你还可以使用标准的 Python 算术与
    TensorFlow 结合。以下是一个示例：
- en: '[PRE29]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Below, we explain various operations available in TensorFlow and how to use
    them.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面，我们将解释 TensorFlow 中可用的各种操作以及如何使用它们。
- en: Defining operations in TensorFlow
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中定义操作
- en: An operation in TensorFlow takes one or more inputs and produces one or more
    outputs. If you take a look at the TensorFlow API at [https://www.tensorflow.org/api_docs/python/tf](https://www.tensorflow.org/api_docs/python/tf),
    you will see that TensorFlow has a massive collection of operations available.
    Here, we will take a look at a selected few of the myriad TensorFlow operations.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 中的操作接受一个或多个输入，并生成一个或多个输出。如果你查看 TensorFlow API [https://www.tensorflow.org/api_docs/python/tf](https://www.tensorflow.org/api_docs/python/tf)，你会发现
    TensorFlow 提供了大量的操作。在这里，我们将选取一些典型的 TensorFlow 操作进行讲解。
- en: Comparison operations
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较操作
- en: Comparison operations are useful for comparing two tensors. The following code
    example includes a few useful comparison operations.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 比较操作用于比较两个张量。以下代码示例包括一些有用的比较操作。
- en: 'To understand the working of these operations, let’s consider two example tensors,
    `x` and `y`:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这些操作的工作原理，我们来考虑两个示例张量，`x` 和 `y`：
- en: '[PRE30]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Next, let’s look at some mathematical operations.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一些数学运算。
- en: Mathematical operations
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数学运算
- en: 'TensorFlow allows you to perform math operations on tensors that range from
    the simple to the complex. We will discuss a few of the mathematical operations
    made available in TensorFlow. The complete set of operations is available at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 允许你对张量执行从简单到复杂的数学操作。我们将讨论一些在 TensorFlow 中提供的数学操作。完整的操作集可以在[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math)找到：
- en: '[PRE31]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Now, we will look at the scatter operation.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将来看一下散布操作。
- en: Updating (scattering) values in tensors
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更新（散布）张量中的值
- en: A scatter operation, which refers to changing the values at certain indices
    of a tensor, is very common in scientific computing problems. This functionality
    was originally provided through an intimidating `tf.scatter_nd()` function, which
    can be difficult to understand.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 散布操作，指的是更改张量某些索引处的值，在科学计算问题中非常常见。最初，TensorFlow 通过一个令人生畏的`tf.scatter_nd()`函数提供了这一功能，这个函数可能比较难理解。
- en: 'However, in recent TensorFlow versions, you can perform scatter operations
    via array indexing and slicing using NumPy-like syntax. Let’s see a few examples.
    Say you have the TensorFlow variable v, which is a [3,2] matrix:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在最近的 TensorFlow 版本中，你可以通过使用类似于 NumPy 的语法进行数组索引和切片来执行散布操作。让我们看几个例子。假设你有一个
    TensorFlow 变量`v`，它是一个[3,2]的矩阵：
- en: '[PRE32]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You can change the 0^(th) row of this tensor with:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式更改此张量的第0行：
- en: '[PRE33]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'which results in:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致：
- en: '[PRE34]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You can change the value at index [1,1] with:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式更改索引[1,1]处的值：
- en: '[PRE35]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'which results in:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致：
- en: '[PRE36]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You can perform row slicing with:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式进行行切片：
- en: '[PRE37]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'which results in:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致：
- en: '[PRE38]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: It is important to remember that the scatter operation (performed via the `assign()`
    operation) can only be performed on `tf.Variables`, which are mutable structures.
    Remember that `tf.Tensor`/`tf.EagerTensor` are immutable objects.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，散布操作（通过`assign()`操作执行）只能在`tf.Variables`上执行，后者是可变结构。请记住，`tf.Tensor`/`tf.EagerTensor`是不可变对象。
- en: Collecting (gathering) values from a tensor
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从张量中收集（聚集）值
- en: 'A gather operation is very similar to a scatter operation. Remember that scattering
    is about assigning values to tensors, whereas gathering retrieves the values of
    a tensor. Let’s understand this through an example. Say you have a TensorFlow
    tensor, `t`:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 聚集操作与散布操作非常相似。请记住，散布是将值分配给张量，而聚集则是检索张量的值。让我们通过一个例子来理解这一点。假设你有一个 TensorFlow 张量`t`：
- en: '[PRE39]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You can obtain the 0^(th) row of `t` with:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过以下方式获取`t`的第0行：
- en: '[PRE40]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'which will return:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回：
- en: '[PRE41]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You can also perform row-slicing with:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过以下方式进行行切片：
- en: '[PRE42]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'which will return:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回：
- en: '[PRE43]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Unlike the scatter operation, the gather operation works both on `tf.Variable`
    and `tf.Tensor` structures.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 与散布操作不同，聚集操作既适用于`tf.Variable`也适用于`tf.Tensor`结构。
- en: Neural network-related operations
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与神经网络相关的操作
- en: Now, let’s look at several useful neural network-related operations that we
    will use heavily in the following chapters. The operations we will discuss here
    range from simple element-wise transformations (that is, activations) to computing
    partial derivatives of a set of parameters with respect to another value. We will
    also implement a simple neural network as an exercise.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一些我们在接下来的章节中将大量使用的有用的神经网络相关操作。我们将在这里讨论的操作从简单的逐元素变换（即激活）到计算一组参数对另一个值的偏导数。我们还将作为练习实现一个简单的神经网络。
- en: Nonlinear activations used by neural networks
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络使用的非线性激活函数
- en: Nonlinear activations enable neural networks to perform well at numerous tasks.
    Typically, there is a nonlinear activation transformation (that is, activation
    layer) after each layer output in a neural network (except for the last layer).
    A nonlinear transformation helps a neural network to learn various nonlinear patterns
    that are present in data. This is very useful for complex real-world problems,
    where data often has more complex nonlinear patterns, in contrast to linear patterns.
    If not for the nonlinear activations between layers, a deep neural network would
    be a bunch of linear layers stacked on top of each other. Also, a set of linear
    layers can essentially be compressed to a single bigger linear layer.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性激活使神经网络在多个任务上表现出色。通常，在神经网络的每一层输出后（除了最后一层），会有一个非线性激活转换（即激活层）。非线性转换帮助神经网络学习数据中存在的各种非线性模式。这对于复杂的现实问题非常有用，因为这些问题中的数据往往比线性模式更复杂。如果没有层与层之间的非线性激活，深度神经网络将仅仅是堆叠在一起的多个线性层。而且，一组线性层本质上可以压缩成一个更大的线性层。
- en: In conclusion, if not for the nonlinear activations, we cannot create a neural
    network with more than one layer.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，如果没有非线性激活，我们就无法创建一个具有多层的神经网络。
- en: 'Let’s observe the importance of nonlinear activation through an example. First,
    recall the computation for the neural networks we saw in the sigmoid example.
    If we disregard b, it will be this:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来观察非线性激活的重要性。首先，回忆一下我们在sigmoid例子中看到的神经网络计算。如果我们忽略b，它将是：
- en: '`h = sigmoid(W*x)`'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`h = sigmoid(W*x)`'
- en: 'Assume a three-layer neural network (having `W1`, `W2`, and `W3` as layer weights)
    where each layer does the preceding computation; we can summarize the full computation
    as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个三层神经网络（具有`W1`、`W2`和`W3`作为层权重），每一层执行前一层的计算；我们可以将整个计算过程总结如下：
- en: '`h = sigmoid(W3*sigmoid(W2*sigmoid(W1*x)))`'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`h = sigmoid(W3*sigmoid(W2*sigmoid(W1*x)))`'
- en: 'However, if we remove the nonlinear activation (that is, sigmoid), we get this:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们去除非线性激活（即sigmoid），我们将得到如下结果：
- en: '`h = (W3 * (W2 * (W1 *x))) = (W3*W2*W1)*x`'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`h = (W3 * (W2 * (W1 *x))) = (W3*W2*W1)*x`'
- en: So, without the nonlinear activations, the three layers can be brought down
    to a single linear layer.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果没有非线性激活，三层网络可以简化为一个线性层。
- en: 'Now we’ll list two commonly used nonlinear activations in neural networks (in
    other words, sigmoid and ReLU) and how they can be implemented in TensorFlow:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们列出两种在神经网络中常用的非线性激活（即sigmoid和ReLU）以及它们如何在TensorFlow中实现：
- en: '[PRE44]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The functional form of these computations is visualized in *Figure 2.6*:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这些计算的函数形式在*图 2.6*中可视化：
- en: '![](img/B14070_02_06.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_02_06.png)'
- en: 'Figure 2.6: The functional forms of sigmoid (left) and ReLU (right) activations'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：sigmoid（左）和ReLU（右）激活的函数形式
- en: Next, we will discuss the convolution operation.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论卷积操作。
- en: The convolution operation
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积操作
- en: 'A convolution operation is a widely used signal-processing technique. For images,
    convolution is used to produce different effects (such as blurring), or extract
    features (such as edges) from an image. An example of edge detection using convolution
    is shown in *Figure 2.7*. This is achieved by shifting a convolution filter on
    top of an image to produce a different output at each location (see *Figure* *2.8*
    later in this section). Specifically, at each location, we do element-wise multiplication
    of the elements in the convolution filter with the image patch (the same size
    as the convolution filter) that overlaps with the convolution filter and takes
    the sum of the multiplication:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作是一种广泛使用的信号处理技术。对于图像，卷积用于产生不同的效果（如模糊），或从图像中提取特征（如边缘）。使用卷积进行边缘检测的例子如*图 2.7*所示。其实现方式是将卷积滤波器移动到图像上，每个位置产生不同的输出（稍后在本节中会看到*图
    2.8*）。具体而言，在每个位置，我们对卷积滤波器中的元素与图像块（与卷积滤波器大小相同）进行逐元素相乘，并将乘积求和：
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_06.png](img/B14070_02_07.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_06.png](img/B14070_02_07.png)'
- en: 'Figure 2.7: Using the convolution operation for edge detection in an image
    (Source: [https://en.wikipedia.org/wiki/Kernel_(image_processing)](https://en.wikipedia.org/wiki/Kernel_(image_processing)))'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7：使用卷积操作进行图像边缘检测（来源：[https://en.wikipedia.org/wiki/Kernel_(image_processing)](https://en.wikipedia.org/wiki/Kernel_(image_processing)))
- en: 'The following is the implementation of the convolution operation:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是卷积操作的实现：
- en: '[PRE45]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here, the apparently excessive number of square brackets used might make you
    think that the example can be made easy to follow by getting rid of these redundant
    brackets. Unfortunately, that is not the case. For the `tf.nn.conv2d(...)` operation,
    TensorFlow requires `input`, `filters`, and `strides` to be of an exact format.
    We will now go through each argument in `tf.conv2d(input, filters, strides, padding)`
    in more detail:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，显得过多的方括号可能会让你认为通过去掉这些冗余的括号，例子会更容易理解。不幸的是，情况并非如此。对于`tf.nn.conv2d(...)`操作，TensorFlow要求`input`、`filters`和`strides`必须符合精确的格式。我们现在将更详细地介绍`tf.conv2d(input,
    filters, strides, padding)`中的每个参数：
- en: '**input**: This is typically a 4D tensor where the dimensions should be ordered
    as `[batch_size, height, width, channels]`:'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**input**：这通常是一个4D张量，其中各维度应该按顺序排列为`[batch_size, height, width, channels]`：'
- en: '**batch_size**: This is the amount of data (for example, inputs such as images,
    and words) in a single batch of data. We normally process data in batches as large
    datasets are used for learning. At a given training step, we randomly sample a
    small batch of data that approximately represents the full dataset. And doing
    this for many steps allows us to approximate the full dataset quite well. This
    `batch_size` parameter is the same as the one we discussed in the TensorFlow input
    pipeline example.'
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**batch_size**：这是单个数据批次中的数据量（例如图像和单词等输入）。我们通常以批量方式处理数据，因为在学习过程中使用的是大型数据集。在每个训练步骤中，我们会随机抽取一个小批量数据，这些数据大致代表了整个数据集。通过这种方式进行多次步骤，我们可以很好地逼近整个数据集。这个`batch_size`参数与我们在TensorFlow输入管道示例中讨论的参数是一样的。'
- en: '**height and width**: This is the height and the width of the input.'
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**height and width**：这是输入的高度和宽度。'
- en: '**channels**: This is the depth of an input (for example, for an RGB image,
    the number of channels will be 3—a channel for each color).'
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**channels**：这是输入的深度（例如，对于RGB图像，通道数为3—每种颜色一个通道）。'
- en: '**filters**: This is a 4D tensor that represents the convolution window of
    the convolution operation. The filter dimensions should be `[height, width, in_channels,
    out_channels]`:'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**filters**：这是一个4D张量，表示卷积操作的卷积窗口。滤波器的维度应该是`[height, width, in_channels, out_channels]`：'
- en: '**height and width**: This is the height and the width of the filter (often
    smaller than that of the input)'
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**height and width**：这是滤波器的高度和宽度（通常小于输入的高度和宽度）。'
- en: '**in_channels**: This is the number of channels of the input to the layer'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**in_channels**：这是输入层的通道数。'
- en: '**out_channels**: This is the number of channels to be produced in the output
    of the layer'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**out_channels**：这是层输出中产生的通道数。'
- en: '**strides**: This is a list with four elements, where the elements are `[batch_stride,
    height_stride, width_stride, channels_stride]`. The `strides` argument denotes
    how many elements to skip during a single shift of the convolution window on the
    input. Usually, you don’t have to worry about `batch_stride` and `channels_stride`.
    If you do not completely understand what `strides` is, you can use the default
    value of `1`.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**strides**：这是一个包含四个元素的列表，其中元素为`[batch_stride, height_stride, width_stride,
    channels_stride]`。`strides`参数表示在卷积窗口单次移动时跳过的元素数。通常情况下，你不需要担心`batch_stride`和`channels_stride`。如果你不完全理解`strides`，可以使用默认值`1`。'
- en: '**padding**: This can be one of `[''SAME'', ''VALID'']`. It decides how to
    handle the convolution operation near the boundaries of the input. The `VALID`
    operation performs the convolution without padding. If we were to convolve an
    input of *n* length with a convolution window of size *h*, this will result in
    an output of size (*n-h+1 < n*). The diminishing of the output size can severely
    limit the depth of neural networks. `SAME` pads zeros to the boundary such that
    the output will have the same height and width as the input.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**padding**：这可以是`[''SAME'', ''VALID'']`之一。它决定了如何处理卷积操作在输入边界附近的情况。`VALID`操作在没有填充的情况下进行卷积。如果我们用大小为*h*的卷积窗口对长度为*n*的输入进行卷积，那么输出的大小将是（*n-h+1
    < n*）。输出大小的减少可能会严重限制神经网络的深度。`SAME`则会在边界处填充零，使得输出的高度和宽度与输入相同。'
- en: 'To gain a better understanding of what filter size, stride, and padding are,
    refer to *Figure 2.8*:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解滤波器大小、步幅和填充，请参考*图2.8*：
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_07.png](img/B14070_02_08.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_07.png](img/B14070_02_08.png)'
- en: 'Figure 2.8: The convolution operation. Note how the kernel is moved over the
    input to compute values at each position'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：卷积操作。注意卷积核如何在输入上移动，以在每个位置计算值。
- en: Next, we will discuss the pooling operation.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论池化操作。
- en: The pooling operation
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 池化操作
- en: 'A pooling operation behaves similarly to the convolution operation, but the
    final output is different. Instead of outputting the sum of the element-wise multiplication
    of the filter and the image patch, we now take the maximum element of the image
    patch for that location (see *Figure 2.9*):'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 池化操作的行为类似于卷积操作，但最终输出是不同的。我们不再输出滤波器和图像块的逐元素乘积的和，而是对该位置的图像块选择最大元素（参见*图 2.9*）：
- en: '[PRE46]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_08.png](img/B14070_02_09.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH02\B08681_02_08.png](img/B14070_02_09.png)'
- en: 'Figure 2.9: The max-pooling operation'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9：最大池化操作
- en: Defining loss
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义损失
- en: 'We know that, for a neural network to learn something useful, a loss needs
    to be defined. The loss represents how close or far away the predictions are from
    actual targets. There are several functions for automatically calculating the
    loss in TensorFlow, two of which are shown in the following code. The `tf.nn.l2_loss`
    function is the mean squared error loss, and `tf.nn.softmax_cross_entropy_with_logits`
    is another type of loss that actually gives better performance in classification
    tasks. And by logits here, we mean the unnormalized output of the neural network
    (that is, the linear output of the last layer of the neural network):'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，为了让神经网络学到有用的东西，需要定义损失函数。损失函数表示预测值与实际目标之间的差距。TensorFlow 中有几个函数可以自动计算损失，以下代码展示了其中两个。`tf.nn.l2_loss`
    函数是均方误差损失，而 `tf.nn.softmax_cross_entropy_with_logits` 是另一种损失函数，实际上在分类任务中表现更好。这里的
    logits 指的是神经网络的未归一化输出（即神经网络最后一层的线性输出）：
- en: '[PRE47]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Here, we discussed several important operations intertwined with neural networks,
    such as the convolution operation and the pooling operation. We will now discuss
    how a sub-library in TensorFlow known as Keras can be used to build models.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论了与神经网络密切相关的几个重要操作，如卷积操作和池化操作。接下来，我们将讨论如何使用 TensorFlow 中的子库 Keras 来构建模型。
- en: 'Keras: The model building API of TensorFlow'
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras：TensorFlow 的模型构建 API
- en: Keras was developed as a separate library that provides high-level building
    blocks to build models conveniently. It was initially platform-agnostic and supported
    many softwares (for example, TensorFlow and Theano).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 最初作为一个独立的库开发，提供了高层次的构建模块，便于构建模型。它最初是平台无关的，支持多种软件（例如 TensorFlow 和 Theano）。
- en: However, TensorFlow acquired Keras and now is an integral part of TensorFlow
    for building models effortlessly.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，TensorFlow 收购了 Keras，现在它已成为 TensorFlow 中构建模型的一个不可或缺的部分，简化了模型构建过程。
- en: 'Keras’s primary focus is model building. For that, Keras provides several different
    APIs with varying degrees of flexibility and complexity. Choosing the right API
    for the job will require sound knowledge of the limitations of each API as well
    as experience. The APIs provided by Keras are:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的主要关注点是模型构建。为此，Keras 提供了几个不同的 API，具有不同的灵活性和复杂性。选择合适的 API 需要了解每个 API 的局限性，并积累相应的经验。Keras
    提供的 API 包括：
- en: Sequential API – The most easy-to-use API. In this API, you simply stack layers
    on top of each other to create a model.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序 API – 最易于使用的 API。在这个 API 中，你只需将层按顺序堆叠在一起以创建模型。
- en: Functional API – The functional API provides more flexibility by allowing you
    to define custom models that can have multiple input layers/multiple output layers.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能性 API – 功能性 API 通过允许你定义自定义模型，提供了更多的灵活性，这些模型可以有多个输入层/多个输出层。
- en: Sub-classing API – The sub-classing API enables you to define custom reusable
    layers/models as Python classes. This is the most flexible API, but it requires
    strong familiarity with the API and raw TensorFlow operations to use it correctly.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子类化 API – 子类化 API 使你能够将自定义的可重用层/模型定义为 Python 类。这是最灵活的 API，但它需要对 API 和原始 TensorFlow
    操作有深入的了解才能正确使用。
- en: Do not confuse the Keras TensorFlow sub-module ([https://www.tensorflow.org/api_docs/python/tf/keras](https://www.tensorflow.org/api_docs/python/tf/keras))
    with the external Keras library ([https://keras.io/](https://keras.io/)). They
    share roots in terms of where they’ve come from, but they are not the same. You
    will run into strange issues if you treat them as the same during your development.
    In this book, we exclusively use `tf.keras`.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将 Keras 的 TensorFlow 子模块（[https://www.tensorflow.org/api_docs/python/tf/keras](https://www.tensorflow.org/api_docs/python/tf/keras)）与外部的
    Keras 库（[https://keras.io/](https://keras.io/)）混淆。它们在起源上有相似之处，但并不是相同的。如果在开发过程中把它们当成相同的东西，你会遇到奇怪的问题。在本书中，我们专门使用
    `tf.keras`。
- en: One of the most innate concepts in Keras is that a model is composed of one
    or more layers connected in a specific way. Here, we will briefly go through what
    the code looks like, using different APIs to develop models. You are not expected
    to fully understand the code below. Rather, focus on the code style to spot any
    differences between the three methods.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 中最本质的概念之一是模型由一个或多个以特定方式连接的层组成。在这里，我们将简要介绍使用不同 API 开发模型的代码样子。你不需要完全理解下面的代码，而是要关注代码风格，识别三种方法之间的差异。
- en: Sequential API
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 顺序 API
- en: 'When using the Sequential API, you simply define your model as a list of layers.
    Here, the first element in the list is the closest to the input, where the last
    is the output layer:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用顺序 API 时，你只需将模型定义为一个层的列表。在这里，列表中的第一个元素最接近输入，而最后一个元素则是输出层：
- en: '[PRE48]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In the preceding code, we have three layers. The first layer has 500 output
    nodes and takes in a vector of 784 elements as the input. The second layer is
    automatically connected to the first one, whereas the last layer is connected
    to the second layer. All of these layers are fully-connected layers, where all
    input nodes are connected to all output nodes.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们有三层。第一层有 500 个输出节点，并接受一个 784 元素的向量作为输入。第二层自动连接到第一层，而最后一层则连接到第二层。这些层都是全连接层，其中所有输入节点都连接到所有输出节点。
- en: Functional API
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 功能性 API
- en: 'In the Functional API, we do things differently. We first define one or more
    input layers, and other layers that carry computations. Then we connect the inputs
    to outputs ourselves, as shown in the following code:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在功能性 API 中，我们采取不同的做法。我们首先定义一个或多个输入层，以及进行计算的其他层。然后，我们自己将输入与输出连接起来，如下所示的代码所示：
- en: '[PRE49]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'In the code, we start with an input layer that accepts a 784 element-long vector.
    The input is passed to a Dense layer that has 500 nodes. The output of that layer
    is assigned to `out_1`. Then `out_1` is passed to another Dense layer, which outputs
    `out_2`. Next, a Dense layer with 10 nodes outputs the final output. Finally,
    the model is defined as a `tf.keras.models.Model` object that takes two arguments:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们首先定义一个输入层，它接受一个 784 元素长的向量。输入将传递给一个具有 500 个节点的 Dense 层。该层的输出被赋值给`out_1`。然后，`out_1`被传递到另一个
    Dense 层，该层输出`out_2`。接着，一个具有 10 个节点的 Dense 层输出最终的结果。最后，模型被定义为一个`tf.keras.models.Model`对象，它接受两个参数：
- en: inputs – One or more input layers
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: inputs – 一个或多个输入层
- en: outputs – One or more outputs produced by any `tf.keras.layers` type object
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: outputs – 由任何 `tf.keras.layers` 类型对象生成的一个或多个输出
- en: The model is identical to what was defined in the previous section. One of the
    benefits of the Functional API is that you can create far more complex models
    as you’re not bounded to have layers as a list. Because of this freedom, you can
    have multiple inputs connecting to many layers in many different ways and potentially
    produce many outputs as well.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型与上一节中定义的模型相同。功能性 API 的一个好处是，你可以创建更复杂的模型，因为你不再局限于将层作为列表。由于这种灵活性，你可以有多个输入连接到多个层，并以多种不同方式连接，甚至可能产生多个输出。
- en: Sub-classing API
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 子类化 API
- en: 'Finally, we will use the sub-classing API to define a model. With sub-classing,
    you define your model as a Python object that inherits from the base object, `tf.keras.Model`.
    When using sub-classing, you need to define two important functions: `__init__()`,
    which will specify any special parameters, layers, and so on required to successfully
    perform the computations, and `call()`, which defines the computations that need
    to happen in the model:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用子类化 API 来定义一个模型。通过子类化，你将模型定义为一个继承自基础对象`tf.keras.Model`的 Python 对象。在使用子类化时，你需要定义两个重要的函数：`__init__()`，它将指定成功执行计算所需的任何特殊参数、层等；以及`call()`，它定义了模型中需要执行的计算：
- en: '[PRE50]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Here, you can see that our model has three layers, just like all the previous
    models we defined. Next, the call function defines how these layers connect to
    produce the final output. The sub-classing API is considered the most difficult
    to master, mainly due to the freedom allowed by the method. However, the rewards
    are immense once you learn the API as it enables you to define very complex models/layers
    as unit computations that can be reused later. Now that you understand how each
    API works, let’s implement a neural network using Keras and train it on a dataset.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到我们的模型有三层，就像我们定义的所有前置模型一样。接下来，`call`函数定义了这些层如何连接，从而生成最终输出。子类化API被认为是最难掌握的，主要是因为该方法提供了很大的自由度。然而，一旦你掌握了这个API，它的回报是巨大的，因为它使你能够定义非常复杂的模型/层作为单元计算，并且可以在后续重用。现在你已经理解了每个API的工作原理，让我们使用Keras来实现一个神经网络并在数据集上训练它。
- en: Implementing our first neural network
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现我们的第一个神经网络
- en: Great! Now that you’ve learned the architecture and foundations of TensorFlow,
    it’s high time that we move on and implement something slightly more complex.
    Let’s implement a neural network. Specifically, we will implement a fully connected
    neural network model (FCNN), which we discussed in *Chapter 1*, *Introduction
    to Natural Language Processing*.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！现在你已经了解了TensorFlow的架构和基础，是时候继续前进并实现稍微复杂一点的东西了。让我们来实现一个神经网络。具体来说，我们将实现一个全连接神经网络模型（FCNN），这是我们在*第一章*《自然语言处理介绍》中讨论过的内容。
- en: One of the stepping stones to the introduction of neural networks is to implement
    a neural network that is able to classify digits. For this task, we will be using
    the famous MNIST dataset made available at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络引入的一个重要步骤是实现一个能够分类数字的神经网络。为此任务，我们将使用著名的MNIST数据集，可以从[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)下载。
- en: You might feel a bit skeptical regarding our using a computer vision task rather
    than an NLP task. However, vision tasks can be implemented with less preprocessing
    and are easy to understand.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能对我们使用计算机视觉任务而不是自然语言处理任务感到有些怀疑。然而，视觉任务的实现通常需要较少的预处理，且更易于理解。
- en: As this is our first encounter with neural networks, we will see how to implement
    this model using Keras. Keras is the high-level submodule that provides a layer
    of abstraction over TensorFlow. Therefore, you can implement neural networks with
    much less effort with Keras than using TensorFlow’s raw operations. To run the
    examples end to end, you can find the full exercise in the `tensorflow_introduction.ipynb`
    file in the `Ch02-Understanding-TensorFlow` folder. The next step is to prepare
    the data.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是我们第一次接触神经网络，我们将学习如何使用Keras实现这个模型。Keras是一个高层次的子模块，它在TensorFlow之上提供了一个抽象层。因此，使用Keras实现神经网络比使用TensorFlow的原始操作要轻松得多。为了完整运行这些示例，你可以在`Ch02-Understanding-TensorFlow`文件夹中的`tensorflow_introduction.ipynb`文件中找到完整的练习。下一步是准备数据。
- en: Preparing the data
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'First, we need to download the dataset. TensorFlow out of the box provides
    convenient functions to download data and MNIST is one of those supported datasets.
    We will be performing four important steps during the data preparation:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要下载数据集。TensorFlow提供了便捷的函数来下载数据，MNIST就是其中之一。我们将在数据准备过程中执行四个重要步骤：
- en: Downloading the data and storing it as `numpy.ndarray` objects. We will create
    a folder named data within our `ch2` directory and store the data there.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载数据并将其存储为`numpy.ndarray`对象。我们将在`ch2`目录下创建一个名为data的文件夹并将数据存储在其中。
- en: Reshaping the images so that 2D grayscale images in the dataset will be converted
    to 1D vectors.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对图像进行重塑，使得数据集中的2D灰度图像转换为1D向量。
- en: Standardizing the images to have a zero-mean and unit-variance (also known as
    **whitening**).
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对图像进行标准化，使其具有零均值和单位方差（也叫做**白化**）。
- en: One-hot encoding the integer class labels. One-hot encoding refers to the process
    of representing integer class labels as a vector. For example, if you have 10
    classes and a class label of 3 (where labels range from 0-9), your one-hot encoded
    vector will be `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]`.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对整数类别标签进行独热编码。独热编码指的是将整数类别标签表示为一个向量的过程。例如，如果你有10个类别且类别标签为3（标签范围为0-9），那么你的独热编码向量将是`[0,
    0, 0, 1, 0, 0, 0, 0, 0, 0]`。
- en: 'The following code performs these functions for us:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码为我们执行这些功能：
- en: '[PRE51]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You can see that we are using the `tf.keras.datasets.mnist.load_data()` function
    provided by TensorFlow to download the training and testing data. It will be downloaded
    to a folder named `data` within the `Ch02-Understanding-TensorFlow` folder. This
    will provide four output tensors:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，我们使用了TensorFlow提供的`tf.keras.datasets.mnist.load_data()`函数来下载训练和测试数据。数据将下载到`Ch02-Understanding-TensorFlow`文件夹中的一个名为`data`的文件夹内。这将提供四个输出张量：
- en: '`x_train` – A 60000 x 28 x 28 sized tensor where each image is 28 x 28'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x_train` – 一个大小为60000 x 28 x 28的张量，每张图像为28 x 28'
- en: '`y_train` – A 60000 sized vector, where each element is a class label between
    0-9'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_train` – 一个大小为60000的向量，其中每个元素是一个介于0-9之间的类别标签'
- en: '`x_test` – A 10000 x 28 x 28 sized tensor'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x_test` – 一个大小为10000 x 28 x 28的张量'
- en: '`y_test` – A 10000 sized vector'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_test` – 一个大小为10000的向量'
- en: Once the data is downloaded, we reshape the 28 x 28 sized images into a 1D vector.
    This is because we will be implementing a fully connected neural network. Fully
    connected neural networks take a 1D vector as the input. Therefore, all the pixels
    in the image will be arranged as a sequence of pixels in order to feed into the
    model. Finally, if you look at the range of values present in the `x_train` and
    `x_test` tensors, they will be in the range of 0-255 (typical grayscale range).
    We would bring these values to a zero mean unit-variance range by subtracting
    the mean of each image and dividing by the standard deviation.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被下载，我们将28 x 28大小的图像重塑为1D向量。这是因为我们将实现一个全连接神经网络。全连接神经网络将1D向量作为输入。因此，图像中的所有像素将按照像素序列进行排列，以便输入到模型中。最后，如果你查看`x_train`和`x_test`张量中值的范围，它们将处于0-255之间（典型的灰度范围）。我们将通过减去每张图像的均值并除以标准差，将这些值转换为零均值单位方差的范围。
- en: Implementing the neural network with Keras
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras实现神经网络
- en: Let’s now examine how to implement the type of neural network we discussed in
    *Chapter 1, Introduction to Natural Language Processing*, with Keras. The network
    is a fully connected neural network with 3 layers having 500, 250, and 10 nodes,
    respectively. The first two layers will use ReLU activation, whereas the last
    layer uses softmax. To implement this, we are going to use the simplest of the
    Keras APIs available to us – the Sequential API.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何使用Keras实现我们在*第1章，自然语言处理简介*中讨论的那种神经网络。该网络是一个全连接神经网络，具有三层，分别包含500、250和10个节点。前两层将使用ReLU激活函数，而最后一层则使用softmax激活函数。为了实现这一点，我们将使用Keras提供的最简单的API——Sequential
    API。
- en: 'You can find the full exercise in the `tensorflow_introduction.ipynb` file
    in the `Ch02-Understanding-TensorFlow` folder:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在`Ch02-Understanding-TensorFlow`文件夹中的`tensorflow_introduction.ipynb`文件中找到完整的练习：
- en: '[PRE52]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'You can see that all it takes is a single line in the Keras Sequential API
    to define the model we just defined. Keras provides various types of layers. You
    can see the full list of layers available to you at [https://www.tensorflow.org/api_docs/python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers).
    For a fully connected network, we only need Dense layers that mimic the computations
    of a hidden layer in a fully connected network. With the model defined, you need
    to compile this model with an appropriate loss function, an optimizer, and, optionally,
    performance metrics:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，在Keras的Sequential API中只需要一行代码，就能定义我们刚才定义的模型。Keras提供了多种层类型。你可以在[https://www.tensorflow.org/api_docs/python/tf/keras/layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)查看所有可用的层列表。对于全连接网络，我们只需要使用Dense层，它模拟全连接网络中隐藏层的计算。定义好模型后，你需要用适当的损失函数、优化器和可选的性能指标来编译模型：
- en: '[PRE53]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: With the model defined and compiled, we can now train our model on the prepared
    data.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 定义并编译好模型后，我们就可以在准备好的数据上训练我们的模型了。
- en: Training the model
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Training a model could not be easier in Keras. Once the data is prepared, all
    you need to do is call the `model.fit()` function with the required arguments:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中训练模型非常简单。一旦数据准备好，你只需要调用`model.fit()`函数并传入所需的参数：
- en: '[PRE54]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '`model.fit()` accepts several important arguments. We will go through them
    in more detail here:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.fit()`接受几个重要的参数。我们将在这里详细介绍它们：'
- en: '`x` – An input tensor. In our case, this is a 60000 x 784 sized tensor.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x` – 输入张量。在我们的例子中，这是一个大小为60000 x 784的张量。'
- en: '`y` – The one-hot encoded label tensor. In our case, this is a 60000 x 10 sized
    tensor.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y` – 一热编码标签张量。在我们的例子中，这是一个大小为60000 x 10的张量。'
- en: '`batch_size` – Deep learning models are trained with batches of data (in other
    words, stochastically) as opposed to feeding the full dataset at once. The batch
    size defines how many examples are included in a single batch. The larger the
    batch size, the better the accuracy of your model would be generally.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` – 深度学习模型通过批次数据进行训练（换句话说，以随机方式），而不是一次性输入完整数据集。批次大小定义了单个批次中包含多少样本。批次大小越大，通常模型的准确率会越好。'
- en: '`epochs` – Deep learning models iterate through the dataset in batches several
    times. The number of times iterated through the dataset is known as the number
    of epochs. In our example, this is set to 10.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs` – 深度学习模型会多次以批次方式遍历数据集。遍历数据集的次数称为训练周期数。在我们的例子中，这被设置为 10。'
- en: '`validation_split` – When training deep learning models, a validation set is
    used to monitor performance, where the validation set acts as a proxy for real-world
    performance. `validation_split` defines how much of the full dataset is to be
    used as the validation subset. In our example, this is set to 20% of the total
    dataset size.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validation_split` – 在训练深度学习模型时，使用验证集来监控性能，验证集作为真实世界性能的代理。`validation_split`
    定义了要用于验证子集的完整数据集的比例。在我们的例子中，这被设置为总数据集大小的 20%。'
- en: 'Here’s what the training loss and validation accuracy look like over the number
    of epochs we trained the model (*Figure 2.10*):'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们训练模型过程中，训练损失和验证准确率随训练周期变化的情况（*图 2.10*）：
- en: '![](img/B14070_02_10.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_02_10.png)'
- en: 'Figure 2.10: Training loss and validation accuracy over 10 epochs as the model
    is trained'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10：随着模型训练的进行，训练损失和验证准确率在 10 个训练周期中的变化
- en: Next up is testing our model on some unseen data.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是用一些未见过的数据来测试我们的模型。
- en: Testing the model
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试模型
- en: 'Testing the model is also straightforward. During testing, we measure the loss
    and the accuracy of the model on the test dataset. In order to evaluate the model
    on a dataset, Keras models provide a convenient function called `evaluate()`:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 测试模型也很简单。在测试过程中，我们会测量模型在测试数据集上的损失和准确率。为了在数据集上评估模型，Keras 提供了一个方便的函数叫做`evaluate()`：
- en: '[PRE55]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The arguments expected by the `model.evaluate()` function are already covered
    during our discussion of `model.fit()`:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '`model.evaluate()` 函数期望的参数已经在我们讨论 `model.fit()` 时覆盖过了：'
- en: '`x` – An input tensor. In our case, this is a 10000 x 784 sized tensor.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x` – 输入张量。在我们的例子中，这是一个 10000 x 784 的张量。'
- en: '`y` – The one-hot encoded label tensor. In our case, this is a 10000 x 10 sized
    tensor.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y` – 独热编码标签张量。在我们的例子中，这是一个 10000 x 10 的张量。'
- en: '`batch_size` – Batch size defines how many examples are included in a single
    batch. The larger the batch size, the better the accuracy of your model would
    be generally.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` – 批次大小定义了单个批次中包含多少样本。批次大小越大，通常模型的准确率会越好。'
- en: You will get a loss of 0.138 and an accuracy of 98%. You will not get the exact
    same values due to various randomness present in the model, as well as during
    training.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到一个损失为 0.138 和准确率为 98% 的结果。由于模型中以及训练过程中存在的各种随机性，你将不会得到完全相同的值。
- en: In this section, we went through an end-to-end example of training a neural
    network. We prepared the data, trained the model on that data, and finally tested
    it on some unseen data.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们演示了一个从头到尾的神经网络训练示例。我们准备了数据，使用这些数据训练了模型，最后在一些未见过的数据上进行了测试。
- en: Summary
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you took your first steps to solving NLP tasks by understanding
    the primary underlying platform (TensorFlow) on which we will be implementing
    our algorithms. First, we discussed the underlying details of TensorFlow architecture.
    Next, we discussed the essential ingredients of a meaningful TensorFlow program.
    We got to know some new features in TensorFlow 2, such as the AutoGraph feature,
    in depth. We then discussed more exciting elements in TensorFlow such as data
    pipelines and various TensorFlow operations.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你通过理解我们将实现算法的主要基础平台（TensorFlow），迈出了解决 NLP 任务的第一步。首先，我们讨论了 TensorFlow 架构的底层细节。接着，我们讨论了一个有意义的
    TensorFlow 程序的基本组成部分。我们深入了解了 TensorFlow 2 中的一些新特性，比如 AutoGraph 功能。然后，我们讨论了 TensorFlow
    中一些更有趣的元素，如数据管道和各种 TensorFlow 操作。
- en: Specifically, we discussed the TensorFlow architecture by lining up the explanation
    with an example TensorFlow program; the sigmoid example. In this TensorFlow program,
    we used the AutoGraph feature to generate a TensorFlow graph; that is, using the
    `tf.function()` decorator over the function that performs the TensorFlow operations.
    Then, a `GraphDef` object was created representing the graph and sent to the distributed
    master. The distributed master looked at the graph, decided which components to
    use for the relevant computation, and divided it into several subgraphs to make
    the computations faster. Finally, workers executed subgraphs and returned the
    result immediately.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们通过一个 TensorFlow 示例程序（sigmoid 示例）来对 TensorFlow 架构进行解释。在这个 TensorFlow 程序中，我们使用了
    AutoGraph 功能来生成一个 TensorFlow 图；即，通过在执行 TensorFlow 操作的函数上使用 `tf.function()` 装饰器。然后，创建了一个
    `GraphDef` 对象来表示该图，并将其发送到分布式主节点。分布式主节点查看图，决定使用哪些组件进行相关计算，并将其拆分成多个子图，以加速计算。最后，工作节点执行子图并立即返回结果。
- en: 'Next, we discussed the various elements that comprise a typical TensorFlow
    client: inputs, variables, outputs, and operations. Inputs are the data we feed
    to the algorithm for training and testing purposes. We discussed three different
    ways of feeding inputs: using NumPy arrays, preloading data as TensorFlow tensors,
    and using `tf.data` to define an input pipeline. Then we discussed TensorFlow
    variables, how they differ from other tensors, and how to create and initialize
    them. Following this, we discussed how variables can be used to create intermediate
    and terminal outputs.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了构成典型 TensorFlow 客户端的各个元素：输入、变量、输出和操作。输入是我们提供给算法用于训练和测试的数据。我们讨论了三种不同的输入馈送方式：使用
    NumPy 数组、将数据预加载为 TensorFlow 张量，以及使用 `tf.data` 定义输入管道。然后，我们讨论了 TensorFlow 变量，它们与其他张量的不同之处，以及如何创建和初始化它们。接着，我们讨论了如何使用变量创建中间和最终输出。
- en: Finally, we discussed several available TensorFlow operations, including mathematical
    operations, matrix operations, and neural network-related operations that will
    be used later in the book.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了几种可用的 TensorFlow 操作，包括数学运算、矩阵操作和神经网络相关的操作，这些将在本书后续章节中使用。
- en: 'Later, we discussed Keras, a sub-module in TensorFlow that supports building
    models. We learned that there are three different APIs for building models: the
    Sequential API, the Functional API, and the Sub-classing API. We learned that
    the Sequential API is the easiest to use, whereas the Sub-classing API takes much
    more effort. However, the Sequential API is very restrictive in terms of the type
    of models that can be implemented with it.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，我们讨论了 Keras，TensorFlow 中一个支持构建模型的子模块。我们了解到，有三种不同的 API 可以用来构建模型：Sequential
    API、Functional API 和 Sub-classing API。我们得知，Sequential API 是最易用的，而 Sub-classing
    API 则需要更多的工作。然而，Sequential API 在可实现的模型类型上非常有限制。
- en: Finally, we implemented a neural network using all the concepts learned previously.
    We used a three-layer neural network to classify a MNIST digit dataset, and we
    used Keras (a high-level sub-module in TensorFlow) to implement this model.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用之前学到的所有概念实现了一个神经网络。我们使用三层神经网络对 MNIST 手写数字数据集进行了分类，并且使用了 Keras（TensorFlow
    中的高级子模块）来实现该模型。
- en: In the next chapter, we will see how to use the fully connected neural network
    we implemented in this chapter for learning the semantic, numerical word representation
    of words.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何使用本章中实现的全连接神经网络来学习单词的语义和数值表示。
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本书的代码文件，请访问我们的 GitHub 页面：[https://packt.link/nlpgithub](https://packt.link/nlpgithub)
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，结识志同道合的人，并与超过 1000 名成员一起学习： [https://packt.link/nlp](https://packt.link/nlp)
- en: '![](img/QR_Code5143653472357468031.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5143653472357468031.png)'
