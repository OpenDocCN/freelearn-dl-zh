- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Regularization with Tree-Based Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于树的模型的正则化
- en: Tree-based models using ensemble learning such as Random Forest or Gradient
    Boosting are often seen as easy-to-use, state-of-the-art models for regular machine
    learning tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的集成学习模型，如随机森林或梯度提升，通常被视为易于使用的最先进模型，适用于常规机器学习任务。
- en: Many Kaggle competitions have been won with such models, as they can be quite
    robust and efficient at finding complex patterns in data. Knowing how to regularize
    and fine-tune them is key to having the very best performance.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 许多Kaggle竞赛都是通过这样的模型获胜的，因为它们在发现数据中的复杂模式时非常健壮且高效。知道如何正则化和微调它们是获得最佳性能的关键。
- en: 'In this chapter, we’ll look at the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下食谱：
- en: Building a classification tree
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建分类树
- en: Building regression trees
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建回归树
- en: Regularizing a decision tree
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化决策树
- en: Training a Random Forest algorithm
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个随机森林算法
- en: Regularization of Random Forest
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林的正则化
- en: Training a boosting model with XGBoost
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用XGBoost训练提升模型
- en: Regularization with XGBoost
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用XGBoost进行正则化
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, you will train and fine-tune several decision tree-based models,
    as well as visualize a tree. The following libraries will be required for this
    chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将训练和微调几个基于决策树的模型，并可视化一棵树。本章将需要以下库：
- en: NumPy
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: Matplotlib
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: Scikit-learn
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn
- en: Graphviz
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graphviz
- en: XGBoost
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost
- en: pickle
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pickle
- en: Building a classification tree
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建分类树
- en: Decision trees are a separate class of models in machine learning. Although
    a decision tree alone can be considered a weak learner, combined with the power
    of ensemble learning such as bagging or boosting, decision trees get great performances.
    Before digging into ensemble learning models and how to regularize them, in this
    recipe, we will review how decision trees work and how to use them on a classification
    task on the iris dataset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是机器学习中一个独立的模型类别。尽管单独的决策树可以被视为一个弱学习器，但结合集成学习的力量，如袋装法或提升法，决策树能够获得非常好的性能。在深入研究集成学习模型及其如何正则化之前，在这个食谱中，我们将回顾决策树是如何工作的，并且如何在鸢尾花数据集的分类任务中使用它们。
- en: 'To give an intuition of the power of decision trees, let’s consider a use case.
    We would like to know whether to sell ice creams on the beach based on two input
    features: sun and temperature.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了直观地理解决策树的强大能力，我们考虑一个使用场景。我们想知道是否应根据两个输入特征：阳光和温度，在海滩上卖冰激凌。
- en: We have the data in *Figure 4**.1* and would like to train a model on it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*图 4**.1*中有数据，并希望在其上训练一个模型。
- en: '![Figure 4.1 – A circle if we should sell ice creams as a function of sun and
    temperature and a cross if we shouldn’t](img/B19629_04_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – 如果我们应该卖冰激凌，表示为圆圈；如果不应该，表示为叉号](img/B19629_04_01.jpg)'
- en: Figure 4.1 – A circle if we should sell ice creams as a function of sun and
    temperature and a cross if we shouldn’t
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – 如果我们应该卖冰激凌，表示为圆圈；如果不应该，表示为叉号
- en: For a human, this seems quite easy. For a linear model though, not so much.
    If we try to use logistic regression on this data, it will end up drawing a decision
    line such as the left in *Figure 4**.2*. Even with features that are raised to
    a higher power level, the logistic regression would struggle and propose something
    such as the decision line on the right in *Figure 4**.2*.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类来说，这看起来相当简单。但对于线性模型来说就不那么容易了。如果我们尝试在这些数据上使用逻辑回归，它将最终绘制出类似于*图 4**.2*左侧的决策线。即便使用更高次幂的特征，逻辑回归也会遇到困难，并提出类似于*图
    4**.2*右侧的决策线。
- en: '![Figure 4.2 – Potential result of a linear model at classifying this dataset:
    on the left with raw features, on the right with higher power features](img/B19629_04_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2 – 线性模型对该数据集分类的潜在结果：左侧是原始特征，右侧是更高次幂特征](img/B19629_04_02.jpg)'
- en: 'Figure 4.2 – Potential result of a linear model at classifying this dataset:
    on the left with raw features, on the right with higher power features'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2 – 线性模型对该数据集分类的潜在结果：左侧是原始特征，右侧是更高次幂特征
- en: 'In a word, this data is not linearly separable. But it can be divided into
    two separate linearly separable problems:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 总而言之，这些数据是非线性可分的。但它可以被划分为两个独立的线性可分问题：
- en: Is the weather sunny?
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天气晴吗？
- en: Is the temperature warm?
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温度热吗？
- en: 'If we fulfill those two conditions, then we should sell ice cream. This can
    be summarized as the tree in *Figure 4**.3*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们满足这两个条件，那么我们应该卖冰激凌。这可以总结为*图 4**.3*中的树形结构：
- en: '![Figure 4.3 – A decision tree correctly classifying all data points](img/B19629_04_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – 一个正确分类所有数据点的决策树](img/B19629_04_03.jpg)'
- en: Figure 4.3 – A decision tree correctly classifying all data points
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – 一个正确分类所有数据点的决策树
- en: 'Let’s cover a bit of vocabulary here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们要介绍一些词汇：
- en: 'We have **Warm**, which is the first decision node and the root node with two
    branches: **Yes** and **No**.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有**Warm**，这是第一个决策节点，也是根节点，包含两个分支：**Yes** 和 **No**。
- en: We have another decision node in **Sunny**. A decision node is any node containing
    two (sometimes more) branches.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在**Sunny**中有另一个决策节点。决策节点是指包含两个（有时更多）分支的任何节点。
- en: We have three leaves. A leaf does not have any branches and contains a final
    prediction.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有三个叶节点。叶节点没有任何分支，包含最终的预测结果。
- en: Just as for binary trees in computer science, the depth of the tree is the number
    of edges between the root node and the lowest leaf
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就像计算机科学中的二叉树一样，树的深度是根节点和最底部叶节点之间的边数。
- en: 'If we go back to our dataset, the decision line would now look like the one
    in *Figure 4**.4* with not one but two lines combined, providing an effective
    solution:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回到我们的数据集，决策线现在将看起来像*图4.4*那样，提供有效的解决方案，不是一个而是两个组合的线：
- en: '![Figure 4.4 – Result of a decision tree at classifying this dataset](img/B19629_04_04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 决策树在分类此数据集时的结果](img/B19629_04_04.jpg)'
- en: Figure 4.4 – Result of a decision tree at classifying this dataset
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 决策树在分类此数据集时的结果
- en: From now on, any new data will fall into one of those leaves, allowing it to
    be classified correctly.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在开始，任何新数据都将进入这些叶节点之一，从而被正确分类。
- en: 'This is the power of decision trees: they can compute complex, nonlinear rules,
    allowing them great flexibility. A decision tree is trained using a **greedy algorithm**,
    meaning it only tries to optimize one step at a time.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是决策树的强大之处：它们能够计算复杂的非线性规则，赋予它们极大的灵活性。决策树使用**贪婪算法**进行训练，这意味着它每次只尝试优化一个步骤。
- en: More specifically, it means the decision tree is not optimized globally, but
    one node at a time.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，这意味着决策树并不是全局优化的，而是逐个节点进行优化。
- en: 'This can be seen as a recursive algorithm:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以看作是一个递归算法：
- en: Take all samples in a node.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取节点中的所有样本。
- en: Find a threshold in a feature that minimizes the disorder of the splits. In
    other words, find the feature and threshold giving the best class separation.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在某个特征中找到一个阈值，以最小化分裂的无序度。换句话说，找到给出最佳类别分离的特征和阈值。
- en: Split this into two new nodes.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其分裂为两个新的节点。
- en: Go back to *step 1* until your node is pure (meaning that only one class remains)
    or any other condition, and thus a leaf.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到*第1步*，直到你的节点是纯净的（意味着只剩下一个类别），或者满足其他条件，成为叶节点。
- en: But how do we actually choose the splits so that they are optimal? Of course,
    we use a loss function, which uses disorder measurement. Let’s dig into those
    two topics before wrapping it up.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何选择最优的分裂方式呢？当然，我们使用一个损失函数，它使用无序度量。让我们在总结之前先深入探讨这两个话题。
- en: Disorder measurement
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无序度量
- en: For a classification tree to be effective, it must have as little disorder as
    possible in its leaves. Indeed, in the previous example, we assumed all leaves
    are pure. They contain samples from only one class. In reality, leaves may be
    impure and contain samples from several classes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使分类树有效，它必须在其叶节点中尽可能减少无序。实际上，在前面的示例中，我们假设所有的叶节点都是纯净的。它们只包含来自单一类别的样本。实际上，叶节点可能是不纯的，包含来自多个类别的样本。
- en: Note
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If after training a tree a leaf remains impure, we would use the majority class
    of that leaf for classification.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练决策树后，叶节点仍然不纯，我们将使用该叶节点的多数类进行分类。
- en: 'So, the idea is to minimize impurity, but how do we measure it? There are two
    ways: entropy and Gini impurity. Let’s have a look at both.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，基本思路是最小化不纯度，但我们如何度量它呢？有两种方法：熵和基尼不纯度。让我们一起来看看这两种方法。
- en: Entropy
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 熵
- en: 'Entropy is a general word that is used in many contexts, such as physics and
    computer science. The entropy **E** we use here can be defined with the following
    equation, where pi is the proportion of subclass ![](img/Formula_04_001.png) in
    a sample:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 熵是一个广泛使用的术语，应用于多个领域，比如物理学和计算机科学。我们在这里使用的熵**E**可以用以下方程定义，其中pi是样本中子类 ![](img/Formula_04_001.png)
    的比例：
- en: '![](img/Formula_04_002.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_002.jpg)'
- en: 'Let’s consider a concrete example as depicted in *Figure 4**.5*:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看一个具体的例子，如*图4.5*所示：
- en: '![Figure 4.5 – A node with 10 samples of two classes: red and blue](img/B19629_04_05.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 一个包含10个样本，分为红色和蓝色两类的节点](img/B19629_04_05.jpg)'
- en: 'Figure 4.5 – A node with 10 samples of two classes: red and blue'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – 一个包含两类（红色和蓝色）10个样本的节点
- en: 'In this example, the entropy would be the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，熵将是以下内容：
- en: '![](img/Formula_04_003.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_003.jpg)'
- en: 'Indeed, we have the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们有以下结果：
- en: '![](img/Formula_04_004.png)=3/10, since we have three blue samples out of 10'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_04_004.png)=3/10，因为我们有10个样本中有3个是蓝色的'
- en: '![](img/Formula_04_005.png)=7/10, since we have seven blue samples out of 10'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_04_005.png)=7/10，因为我们有10个样本中有7个是蓝色的'
- en: 'If we look at extreme cases, we understand entropy is well suited to compute
    disorder:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们观察极端情况，我们会理解熵非常适合用来计算混乱度：
- en: if ![](img/Formula_04_006.png)=0, then ![](img/Formula_04_007.png)=1 and E =
    0
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 ![](img/Formula_04_006.png)=0，那么 ![](img/Formula_04_007.png)=1 并且 E = 0
- en: if ![](img/Formula_04_008.png)pblue = ![](img/Formula_04_009.png) = 0.5, then
    E = 1
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 ![](img/Formula_04_008.png)pblue = ![](img/Formula_04_009.png) = 0.5, 那么
    E = 1
- en: 'So, we understand that the entropy reaches a maximum value of one when the
    node contains perfectly mixed samples, and the entropy goes to zero when a node
    contains only one class. This is summarized by the curve in *Figure 4**.6*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以理解，当节点包含完全混合的样本时，熵达到最大值1，而当节点只包含一个类别时，熵为零。这个总结可以通过*图 4.6*中的曲线展示：
- en: '![Figure 4.6 – Entropy as a function of p for two classes](img/B19629_04_06.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – 熵作为 p 对于两个类别的函数](img/B19629_04_06.jpg)'
- en: Figure 4.6 – Entropy as a function of p for two classes
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 熵作为 p 对于两个类别的函数
- en: Gini impurity
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基尼不纯度
- en: 'Gini impurity is another way to measure disorder. The formula of Gini impurity
    G is quite simple:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度是另一种衡量混乱度的方法。基尼不纯度 G 的公式非常简单：
- en: '![](img/Formula_04_010.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_010.jpg)'
- en: Again, ![](img/Formula_04_011.png) is the proportion of class in the node.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，![](img/Formula_04_011.png) 是节点中类别的比例。
- en: 'Applied to the example node in *Figure 4**.5*, the computation of the Gini
    impurity would lead to the following result:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于*图 4.5*中的示例节点，基尼不纯度的计算将得出以下结果：
- en: '![](img/Formula_04_012.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_012.jpg)'
- en: 'The result is quite different from entropy, but let’s check that the properties
    remain the same with extreme values:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与熵有很大的不同，但让我们检查极值的情况，以确保属性保持一致：
- en: if ![](img/Formula_04_013.png), then ![](img/Formula_04_014.png) and G = 0
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 ![](img/Formula_04_013.png)，那么 ![](img/Formula_04_014.png) 并且 G = 0
- en: if ![](img/Formula_04_015.png) = 0.5, then G = 0.5
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 ![](img/Formula_04_015.png) = 0.5，那么 G = 0.5
- en: Indeed, the Gini impurity reaches a maximum value of 0.5 when the disorder is
    maximum and is equal to 0 when the node is pure.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，当混乱度最大时，基尼不纯度达到最大值0.5，当节点是纯净的时，基尼不纯度为0。
- en: Entropy or Gini?
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 熵还是基尼？
- en: That said, what should we use? Well, this can be seen as a hyperparameter, and
    scikit-learn’s implementation allows one to choose between entropy and Gini.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们应该使用什么呢？这可以看作是一个超参数，而 scikit-learn 的实现允许选择熵或基尼。
- en: In practice, the results are often the same for both. But Gini impurity is faster
    to compute (entropy involves more expensive log computations), so it is usually
    the first choice.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，两者的结果通常是相同的。但基尼不纯度的计算更快（熵涉及更昂贵的对数计算），所以它通常是首选。
- en: Loss function
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: We have a disorder measurement, but what is the loss we should minimize? The
    ultimate goal is to make splits that minimize the disorder, one node at a time.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个混乱度的度量，但我们应该最小化的损失是什么？最终目标是做出划分，最小化每次划分的混乱度。
- en: 'Considering a decision node always has two children, we can define them as
    left and right nodes. Then, the loss for this node can be written like this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到一个决策节点总是有两个子节点，我们可以将它们定义为左子节点和右子节点。那么，这个节点的损失可以写成这样：
- en: '![](img/Formula_04_016.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_016.jpg)'
- en: 'Let’s break down the formula:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这个公式：
- en: m, ![](img/Formula_04_017.png), and ![](img/Formula_04_018.png) are the number
    of samples in each node respectively
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: m, ![](img/Formula_04_017.png)，以及 ![](img/Formula_04_018.png) 分别是每个节点中样本的数量
- en: '![](img/Formula_04_019.png) and ![](img/Formula_04_020.png) are the Gini impurities
    of the left and right nodes'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_04_019.png) 和 ![](img/Formula_04_020.png) 是左子节点和右子节点的基尼不纯度'
- en: Note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: Of course, this can be computed with entropy instead of Gini impurity.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这也可以通过熵而不是基尼不纯度来计算。
- en: 'Assuming we choose a split decision, we then have a parent node and two children
    nodes defined by the split in *Figure 4**.7*:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们选择了一个划分决策，那么我们就有一个父节点和两个由*图 4.7*中的划分定义的子节点：
- en: '![Figure 4.7 – One parent node and two children nodes, with their respective
    Gini impurities](img/B19629_04_07.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – 一个父节点和两个子节点，它们各自的基尼不纯度](img/B19629_04_07.jpg)'
- en: Figure 4.7 – One parent node and two children nodes, with their respective Gini
    impurities
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 一个父节点和两个子节点，以及它们各自的基尼杂质
- en: 'In this case, the loss **L** would be the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，损失**L**将是如下：
- en: '![](img/Formula_04_021.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_021.jpg)'
- en: Using this loss computation, we are now able to minimize the impurity (thus
    maximizing the purity of a node).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个损失计算方法，我们现在能够最小化杂质（从而最大化节点的纯度）。
- en: Note
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: What we defined here as the loss is only the loss at a node level. Indeed, as
    stated earlier, the decision tree is trained using a greedy approach, not a gradient
    descent.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里定义的损失只是节点层次的损失。实际上，正如前面所说，决策树是通过贪心方法训练的，而不是通过梯度下降。
- en: Getting ready
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Finally, before going into the practical details of this recipe, we need to
    have the following libraries installed: scikit-learn, `graphviz`, and `matplotlib`.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在进入本食谱的实际细节之前，我们需要安装以下库：scikit-learn、`graphviz`和`matplotlib`。
- en: 'They can be installed with the following command line:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可以通过以下命令行安装：
- en: '[PRE0]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it…
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Before actually training a decision tree, let’s quickly go through all the
    steps to train a decision tree:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际训练决策树之前，让我们快速浏览一下训练决策树的所有步骤：
- en: We have a node containing samples of N classes.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有一个包含N个类别样本的节点。
- en: We iterate through all the features and all the possible values of a feature.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们遍历所有特征以及每个特征的所有可能值。
- en: For each feature value, we compute the Gini impurity and the loss.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个特征值，我们计算基尼杂质和损失。
- en: We keep the feature value with the lowest loss and split the node into two children
    nodes.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们保留具有最低损失的特征值，并将节点分裂成两个子节点。
- en: Go back to *step 1* with both nodes until a node is pure (or the stop condition
    is fulfilled).
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到*步骤 1*，对两个节点进行处理，直到某个节点纯净（或者满足停止条件）。
- en: 'Using this approach, the decision tree will eventually find the right set of
    decisions to successfully separate any classes. Then, two cases are possible for
    each leaf:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，决策树最终会找到一组正确的决策，成功地分离各个类别。然后，对于每个叶子节点，有两种可能的情况：
- en: If the leaf is pure, predict this class
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果叶子节点纯净，则预测该类别
- en: If the leaf is impure, predict the most represented class
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果叶子节点不纯净，则预测出现最多的类别
- en: Note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: One way to test all the possible feature values is to use all the existing values
    in the dataset. Another is to use a linear split over the range of existing values
    in the dataset.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 测试所有可能特征值的一种方式是使用数据集中所有现有的值。另一种方法是对数据集中现有值的范围进行线性划分。
- en: 'Let’s now train a decision tree on the iris dataset with scikit-learn:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用scikit-learn在鸢尾花数据集上训练一个决策树：
- en: 'First, we need the required imports: `matplotlib` for data visualization (not
    necessary otherwise), `load_iris` for loading the dataset, `train_test_split`
    for splitting the data into training and test sets, and the `DecisionTreeClassifier`
    decision tree implementation from scikit-learn:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入必需的库：用于数据可视化的`matplotlib`（如果没有其他需求，可以不必使用），用于加载数据集的`load_iris`，用于将数据集分为训练集和测试集的`train_test_split`，以及scikit-learn中实现的`DecisionTreeClassifier`决策树模型：
- en: '[PRE1]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can now load the data using `load_iris`:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以使用`load_iris`加载数据：
- en: '[PRE5]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We split the dataset into training and test sets with `train_test_split`, keeping
    the default parameters and only specifying the random state for reproducibility:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`train_test_split`将数据集分为训练集和测试集，保持默认参数，只指定随机状态以确保可重复性：
- en: '[PRE7]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this step, we display a two-dimensional projection of the data. This is
    just for pedagogical purposes but is not mandatory:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们展示数据的二维投影。这个操作只是为了教学目的，但并非强制要求：
- en: '[PRE10]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here is the plot for it:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这是相应的图表：
- en: '![Figure 4.8 – The three iris classes as a function of the sepal width and
    sepal length (plot produced  by the code)](img/B19629_04_08.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – 三个鸢尾花类别作为萼片宽度和萼片长度的函数（由代码生成的图）](img/B19629_04_08.jpg)'
- en: Figure 4.8 – The three iris classes as a function of the sepal width and sepal
    length (plot produced by the code)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 三个鸢尾花类别作为萼片宽度和萼片长度的函数（由代码生成的图）
- en: 'Instantiate the `DecisionTreeClassifier` model. We use the default parameters
    here:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`DecisionTreeClassifier`模型。我们在这里使用默认参数：
- en: '[PRE15]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Train the model on the training set. We did not prepare the data with any preprocessing
    here because we have only quantitative features, and decision trees are not sensitive
    to scale, unlike linear models. But it would not hurt either to rescale the quantitative
    features:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上训练模型。这里我们没有对数据进行任何预处理，因为我们只有定量特征，而决策树不对特征的尺度敏感，不像线性模型。但如果对定量特征进行缩放也无妨：
- en: '[PRE17]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we evaluate the accuracy of the model on both the training and test
    sets, using the `score()` method of the classification tree:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用分类树的`score()`方法在训练集和测试集上评估模型的准确性：
- en: '[PRE19]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This prints the following output:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出以下输出：
- en: '[PRE24]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We are achieving satisfactory results, even if we are clearly facing overfitting
    with 100% accuracy on the train set.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们在训练集上出现了100%的准确率，显然面临过拟合问题，我们仍然取得了令人满意的结果。
- en: There’s more…
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: Unlike linear models, there are no weights associated with each feature since
    a tree is made up of splits.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性模型不同，树没有与每个特征相关联的权重，因为树是由划分组成的。
- en: 'For visualization purposes, we can display the tree thanks to the `graphviz`
    library. This is mostly for pedagogical use or interest but is not necessarily
    useful otherwise:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化目的，我们可以利用`graphviz`库来展示树。这主要用于教学目的或兴趣，除此之外并不一定有用：
- en: '[PRE25]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here is the tree for it:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是它的树形图：
- en: '![Figure 4.9 – Tree visualization produced by the graphviz library](img/B19629_04_09.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图4.9 – 使用graphviz库生成的树形可视化](img/B19629_04_09.jpg)'
- en: Figure 4.9 – Tree visualization produced by the graphviz library
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 – 使用graphviz库生成的树形可视化
- en: From this tree visualization, we can see that the 37 samples of the setosa class
    are fully classified right away at the first decision node (considering the data
    visualization, this should not be a surprise). The samples of classes virginica
    and versicolor seem to be much more intertwined in the provided features, thus
    the tree requires many more decision nodes to fully discriminate them.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个树形可视化中，我们可以看到，setosa类的37个样本在第一个决策节点就完全被分类（考虑到数据的可视化，这并不令人惊讶）。而virginica和versicolor类的样本在提供的特征中似乎更为交织，因此树需要更多的决策节点才能完全区分它们。
- en: 'Unlike linear models, we do not have weights associated with each feature.
    But we can have a piece of somehow equivalent information, called feature importance,
    available with the `.``feature_importances` attribute:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性模型不同，我们没有与每个特征相关联的权重。但我们可以获得某种等效的信息，称为特征重要性，可以通过`.``feature_importances`属性获取：
- en: '[PRE26]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here is the plot for it:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是它的图示：
- en: '![Figure 4.10 – Feature importance as a function of the feature name (histogram
    produced by the code)](img/B19629_04_10.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图4.10 – 特征重要性与特征名称的关系（由代码生成的直方图）](img/B19629_04_10.jpg)'
- en: Figure 4.10 – Feature importance as a function of the feature name (histogram
    produced by the code)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 – 特征重要性与特征名称的关系（由代码生成的直方图）
- en: This feature importance is relative (meaning the sum of all feature importance
    is equal to 1) and is computed based on the number of samples classified thanks
    to this feature.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 该特征重要性是相对的（意味着所有特征重要性之和等于1），并且是根据通过该特征分类的样本数量来计算的。
- en: Note
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Feature importance is computed based on the amount of reduction of the metric
    used for splitting (for example, Gini impurity or entropy). If one single feature
    allows to make all the splits, then this feature will have an importance of 1.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性是根据用于划分的指标的减少量来计算的（例如，基尼不纯度或熵）。如果某个特征能够做出所有的划分，那么这个特征的重要性将是1。
- en: See also
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The sci-kit learning documentation on classification trees as available at
    the following URL: [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Sci-kit学习文档中关于分类树的说明，请参考以下网址：[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml)。
- en: Building regression trees
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建回归树
- en: Before digging into the regularization of decision trees in general, let’s have
    a recipe for regression trees. Indeed, all the explanations in the previous recipe
    were assuming we have a classification task. Let’s explain how to apply it to
    a regression task and apply it to the California housing dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨决策树的正则化之前，让我们首先了解回归树的使用方法。事实上，之前的所有解释都是假设我们在处理分类任务。现在让我们解释如何将其应用于回归任务，并将其应用于加利福尼亚住房数据集。
- en: 'For regression trees, only a few steps need to be modified compared to classification
    trees: the inference and the loss computation. Besides that, the overall principle
    is the same.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归树，与分类树相比，只需要修改几个步骤：推断和损失计算。除此之外，整体原理是相同的。
- en: The inference
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推断
- en: In order to make an inference, we can no longer use the most represented class
    in a leaf (or in the case of pure leaf, the only class). So, we use the average
    of the labels in each node.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行推理，我们不能再使用叶子节点中最常见的类别（或者在纯叶子的情况下，唯一的类别）。因此，我们使用每个节点中标签的平均值。
- en: In the example proposed in *Figure 4**.11*, assuming this is a leaf, we would
    have an inference value that is the average of those 10 values equal to 14 in
    this case.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 4.11*中提议的示例中，假设这是一个叶子节点，那么推理值将是这10个值的平均数，在这个案例中为14。
- en: '![Figure 4.11 – The example of 10 samples with associated values](img/B19629_04_11.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11 – 具有相关值的10个样本示例](img/B19629_04_11.jpg)'
- en: Figure 4.11 – The example of 10 samples with associated values
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 具有相关值的10个样本示例
- en: The loss
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 损失
- en: 'Instead of using a disorder measurement to compute the loss, in regression
    trees, the mean squared error is used. So, the loss is as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归树中，不使用杂乱度度量来计算损失，而是使用均方误差。所以，损失公式如下：
- en: '![](img/Formula_04_022.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_022.jpg)'
- en: 'Assume again a given split leading to the ![](img/Formula_04_023.png) samples
    in the left node and the ![](img/Formula_04_024.png) samples in the right node.
    The **MSE** for each split is computed using the average of the labels into that
    node:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 假设再次给定一个分割，导致左节点中的样本为![](img/Formula_04_023.png)，右节点中的样本为![](img/Formula_04_024.png)。每个分割的**MSE**通过计算该节点中标签的平均值来得出：
- en: '![Figure 4.12 – Example of a node split on a regression task](img/B19629_04_12.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12 – 回归任务中的节点分割示例](img/B19629_04_12.jpg)'
- en: Figure 4.12 – Example of a node split on a regression task
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – 回归任务中的节点分割示例
- en: 'If we take the example of the proposed split in *Figure 4**.12*, we have all
    we need to compute the L loss:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以*图 4.12*中提出的分割为例，我们已经具备计算L损失所需的一切：
- en: '![](img/Formula_04_025.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_025.jpg)'
- en: Based on those two slight changes, we can train a regression tree with the same
    recursive, greedy algorithm as with classification trees.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这两处轻微的变化，我们可以使用与分类树相同的递归贪心算法来训练回归树。
- en: Getting ready
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Before getting practical, all we need for this recipe is to have the scikit-learn
    library installed. If not yet installed, just type the following command line
    in your terminal:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作之前，我们只需要安装scikit-learn库。如果尚未安装，只需在终端中输入以下命令：
- en: '[PRE27]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: How to do it…
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will train a regression tree using the `DecisionTreeRegressor` class from
    scikit-learn on the California housing dataset:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用scikit-learn中的`DecisionTreeRegressor`类，在加利福尼亚房价数据集上训练回归树：
- en: 'First, the required imports: the `fetch_california_housing` function to load
    the California housing dataset, the `train_test_split` function to split data,
    and the `DecisionTreeRegressor` class with the regression tree implementation:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，所需的导入：`fetch_california_housing`函数用于加载加利福尼亚房价数据集，`train_test_split`函数用于拆分数据，`DecisionTreeRegressor`类包含回归树实现：
- en: '[PRE28]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Load the dataset using the `fetch_california_housing` function:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fetch_california_housing`函数加载数据集：
- en: '[PRE31]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Split the data into training and test sets using the `train_test_split` function:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`函数将数据分为训练集和测试集：
- en: '[PRE32]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Instantiate the `DecisionTreeRegressor` object. We just keep the default parameters
    here, but they can be customized at this point:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`DecisionTreeRegressor`对象。我们在这里保持默认参数，但此时可以自定义它们：
- en: '[PRE34]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Train the regression tree on the training set using the `.fit()` method of
    the `DecisionTreeRegressor` class. Note that we do not apply any specification
    preprocessing to the data because we have only quantitative features, and decision
    trees are not sensitive to feature scale issues:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`DecisionTreeRegressor`类的`.fit()`方法，在训练集上训练回归树。请注意，我们不对数据应用任何特定的预处理，因为我们只有定量特征，而决策树对特征尺度问题不敏感：
- en: '[PRE35]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Evaluate the R2-score of the regression tree on both the training and test
    sets using the built-in `.score()` method of the model class:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型类的内置`.score()`方法，评估回归树在训练集和测试集上的R2分数：
- en: '[PRE37]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This would show something like this:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示类似以下内容：
- en: '[PRE41]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As we can see, we face here a strong overfitting, having a perfect R2-score
    on the training set, while having a much worse (but still decent overall) R2-score
    on the test set.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这里出现了强烈的过拟合，训练集上的R2分数完美，而测试集上的R2分数要差得多（但整体上仍然不错）。
- en: See also
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: 'Look at the official `DecisionTreeRegressor` documentation more information:
    [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.xhtml#sklearn-tree-decisiontreeregressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.xhtml#sklearn-tree-decisiontreeregressor).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 查看官方的 `DecisionTreeRegressor` 文档以获取更多信息：[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.xhtml#sklearn-tree-decisiontreeregressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.xhtml#sklearn-tree-decisiontreeregressor)。
- en: Regularizing a decision tree
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化决策树
- en: In this recipe, we will look at the means to regularize decision trees. We will
    review and comment on a couple of methods for reference and provide a few more
    to be explored.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将探讨正则化决策树的方法。我们将回顾并评论几个方法供参考，并提供一些更多的供进一步探索。
- en: Getting ready
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Obviously, we cannot use L1 or L2 regularization as we did with linear models.
    Since we have no weights for the features and no overall loss such as the mean
    squared error or the binary cross entropy, it is not possible to apply this method
    here.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们无法像在线性模型中使用L1或L2正则化那样使用它。由于我们没有特征的权重，也没有类似均方误差或二元交叉熵的整体损失，因此在这里无法应用这种方法。
- en: But we do have other ways to regularize, such as the max depth of the tree,
    the minimum number of samples per leaf, the minimum number of samples per split,
    the max number of features, or the minimum impurity decrease. In this recipe,
    we will look at those.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们确实有其他的正则化方法，例如树的最大深度、每个叶节点的最小样本数、每次分裂的最小样本数、最大特征数或最小杂质减少量。在这个食谱中，我们将探讨这些方法。
- en: 'To do that, we only need the following libraries: scikit-learn, `matplotlib`
    and `NumPy`. Also, since we will provide some visualization to give some idea
    of regularization, we will use the following `plot_decision_function` function:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们只需要以下库：scikit-learn、`matplotlib` 和 `NumPy`。另外，由于我们将提供一些可视化以帮助理解正则化，我们将使用以下的
    `plot_decision_function` 函数：
- en: '[PRE42]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This function will allow us to visualize the decision function of our decision
    tree and get a better understanding of what is overfitting and regularization
    when it comes to a classification tree.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将帮助我们可视化决策树的决策函数，更好地理解分类树中的过拟合和正则化。
- en: How to do it…
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: We will give a recipe to regularize a decision tree based on the maximum depth,
    and then explore a few others in the *There’s* *more* section.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提供一个基于最大深度来正则化决策树的食谱，然后在 *还有更多* 部分探讨其他一些方法。
- en: The maximum depth is quite often one of the first hyperparameters to fine-tune
    when trying to regularize. Indeed, as we have seen earlier, decision trees can
    learn complex data patterns using more decision nodes. If not stopped, the decision
    trees may tend to overfit the data with too many consecutive decision nodes.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最大深度通常是调整超参数时首先需要调整的一个参数。事实上，正如我们之前看到的，决策树可以使用更多的决策节点学习复杂的数据模式。如果不加限制，决策树可能会因为太多连续的决策节点而过拟合数据。
- en: 'We will now train a classification tree with a limited maximum depth on the
    iris dataset:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在鸢尾花数据集上训练一个限制最大深度的分类树：
- en: 'Make the required imports:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行必要的导入：
- en: The `load_iris` function to load the dataset
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`load_iris` 函数用于加载数据集'
- en: The `train_test_split` function to split the data into training and test sets
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_test_split` 函数用于将数据拆分为训练集和测试集'
- en: 'The `DecisionTreeClassifier` class:'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DecisionTreeClassifier` 类：'
- en: '[PRE43]'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Load the dataset using the `load_iris` function. In order to be able to fully
    visualize the effects of regularization, we also keep only two features out of
    four, so that we can display them on a plot:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `load_iris` 函数加载数据集。为了能够完全可视化正则化的效果，我们还只保留了四个特征中的两个，以便可以将它们显示在图表中：
- en: '[PRE46]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Split the data into training and test sets using the `train_test_split` function.
    We only specify the random state for reproducibility and let the other parameters
    be by default:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `train_test_split` 函数将数据拆分为训练集和测试集。我们只指定随机状态以确保可重复性，其他参数保持默认：
- en: '[PRE49]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Instantiate a decision tree object, limiting the maximum depth to five with
    the `max_depth=5` parameter. We also set the random state to `0` for reproducibility:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个决策树对象，将最大深度限制为五，并使用 `max_depth=5` 参数。我们还将随机状态设置为 `0` 以确保可重复性：
- en: '[PRE51]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Fit the classification tree on the training set using the `.fit()` method.
    As mentioned earlier, since the features are all quantitative and decision trees
    are not sensitive to the features scale, there is no need to apply rescaling:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.fit()`方法将分类树拟合到训练集。正如之前提到的，由于特征都是定量的，而决策树对特征的尺度不敏感，因此无需进行重缩放：
- en: '[PRE53]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Evaluate the model accuracy, using the `.score()` method of the `DecisionTreeClassifier`
    class:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`DecisionTreeClassifier`类的`.score()`方法评估模型准确性：
- en: '[PRE55]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'This would print the following output:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下输出：
- en: '[PRE59]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: How it works…
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'In order to have a better understanding of how it works, let’s look at the
    two dimensions of the iris dataset we retained. We will use the `plot_decision_function()`
    function defined in *Getting ready* to plot the decision function of a decision
    tree with no regularization (that is, default hyperparameters):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解它是如何工作的，我们来看看我们保留的鸢尾花数据集的两个维度。我们将使用*准备工作*中定义的`plot_decision_function()`函数来绘制没有正则化的决策树的决策函数（即，默认超参数）：
- en: '[PRE60]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Here is the plot:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是图形：
- en: '![Figure 4.13 – Decision function of the model as a function of the sepal width
    and sepal length with a very complex and questionable decision function (plot
    produced by the code)](img/B19629_04_13.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.13 – 模型的决策函数作为花萼宽度和花萼长度的函数，具有非常复杂且值得怀疑的决策函数（由代码生成的图）](img/B19629_04_13.jpg)'
- en: Figure 4.13 – Decision function of the model as a function of the sepal width
    and sepal length with a very complex and questionable decision function (plot
    produced by the code)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 – 模型的决策函数作为花萼宽度和花萼长度的函数，具有非常复杂且值得怀疑的决策函数（由代码生成的图）
- en: From this plot, we can deduce we are typically facing overfitting. Indeed, the
    boundaries are really specific, sometimes trying to make a complex pattern for
    only one sample, instead of focusing on the higher-level pattern.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图中，我们可以推断出我们通常面临的是过拟合。事实上，边界非常具体，有时试图为单个样本创建复杂的模式，而不是专注于更高级的模式。
- en: 'Indeed, if we look at the accuracy score for both the training and test set,
    we have the following results:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如果我们查看训练集和测试集的准确率，我们得到以下结果：
- en: '[PRE61]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We’ll get the following output:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下输出：
- en: '[PRE62]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: While the accuracy is about 94% on the training set, it is only about 63% on
    the test set. There is overfitting, and regularization may be helpful.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然训练集的准确率约为94%，但测试集的准确率仅约为63%。存在过拟合，正则化可能有助于缓解。
- en: Note
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The accuracy is far lower than in the first recipe because we use only two features
    for visualization and pedagogical purposes. The reasoning remains true if we keep
    the four features, though.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们仅使用两个特征进行可视化和教学目的，因此准确率远低于第一个示例。然而，如果我们保留四个特征，推理仍然成立。
- en: 'Let’s now add regularization by limiting the maximum depth of the decision
    tree as we did in this recipe:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过限制决策树的最大深度来添加正则化，正如我们在这个示例中所做的那样：
- en: '[PRE63]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: If the maximum depth of the tree is `min_samples_split` samples.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果树的最大深度是`min_samples_split`个样本。
- en: 'It means that by default, the trees are expanded with no limit on the depth.
    The limit is then perhaps set by other factors and may go very deep. If we fix
    that by limiting the depth to `5`, let’s see the impact on the decision function:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着默认情况下，树的深度没有限制地扩展。限制可能由其他因素设置，且深度可能非常大。如果我们通过将深度限制为`5`来解决这个问题，我们来看看对决策函数的影响：
- en: '[PRE64]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Here is the output:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是输出结果：
- en: '![Figure 4.14 – Decision function with maximum depth regularization (plot produced
    by the code)](img/B19629_04_14.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.14 – 最大深度正则化的决策函数（由代码生成的图）](img/B19629_04_14.jpg)'
- en: Figure 4.14 – Decision function with maximum depth regularization (plot produced
    by the code)
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – 最大深度正则化的决策函数（由代码生成的图）
- en: 'By limiting the max depth to `5`, we get a less specific decision function,
    even if there seems to be some overfitting remaining. If we have a look again
    at the accuracy score, we can see that it actually helped slightly:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将最大深度限制为`5`，我们得到了一个不太具体的决策函数，即使似乎仍然存在一些过拟合。如果我们再次查看准确率，我们可以看到它实际上有所帮助：
- en: '[PRE65]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'This would provide the following output:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE66]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Indeed, the accuracy score on the test set climbed from 63% to 66%, while the
    accuracy on the training set decreased from 95% to 87%. This is typically what
    we can expect from regularization: this added bias (and thus decreased training
    set performances) and decreased the variance (and thus allowed us to generalize
    better).'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，测试集的准确率从63%上升到66%，而训练集的准确率从95%下降到87%。这通常是我们从正则化中可以预期的结果：它增加了偏差（因此降低了训练集的表现），并减少了方差（因此使我们能够更好地进行泛化）。
- en: There’s more…
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: The maximum depth hyperparameter is really convenient because it’s easy to understand
    and fine-tune. But, there are many other hyperparameters that can help regularize
    decision trees. Let’s review some of them here. We will focus on the minimum sample
    hyperparameters and then propose a few other hyperparameters.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 最大深度超参数非常方便，因为它容易理解并进行微调。但是，还有许多其他超参数可以帮助正则化决策树。我们将在这里回顾其中的一些。我们将重点介绍最小样本超参数，然后提出一些其他超参数。
- en: Minimum samples
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小样本数
- en: Other hyperparameters allowing us to regularize are the ones controlling the
    minimum number of samples per leaf and the minimum number of samples per split.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 其他允许我们进行正则化的超参数包括控制每个叶子节点的最小样本数和每次分裂的最小样本数。
- en: The idea is rather straightforward and intuitive but more subtle than the maximum
    depth. In the decision tree we visualized earlier in this chapter, we could see
    that the first splits classify substantial amounts of samples. The first split
    successfully classified 37 samples as setosa while keeping 75 in the other split.
    On the other end of the decision tree, the lowest nodes are sometimes splitting
    over only three or four samples.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法相当直接且直观，但比最大深度更微妙。在我们在本章早些时候可视化的决策树中，我们可以看到第一次分裂分类了大量的样本。第一次分裂成功地将37个样本分类为setosa，并将其余75个样本保留在另一个分裂中。在决策树的另一端，最底层的节点有时仅在三或四个样本上进行分裂。
- en: Is splitting over only three samples significant? What if out of these three
    samples, there is an outlier? Generally, it does not sound like a promising idea
    to create a rule for only three samples if the end goal is to have a robust, well-generalizing
    model.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在三条样本上进行分裂是否具有显著意义？如果这三条样本中有一个异常值呢？通常来说，如果最终目标是拥有一个稳健、良好泛化的模型，那么仅基于三条样本创建规则并不是什么值得期待的想法。
- en: 'We have two different but somewhat related hyperparameters that allow us to
    deal with that:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个不同但有些相关的超参数，可以帮助我们处理这个问题：
- en: '`min_samples_split`: The minimum samples required to split an internal node.
    If a float is provided, then it uses a fraction of the total number of samples.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`：进行内部节点分裂所需的最小样本数。如果提供的是浮动值，则表示样本总数的一个比例。'
- en: '`min_samples_leaf`: The minimum samples required to be considered a leaf. If
    a float is provided, then it uses a fraction of the total number of samples.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`：被视为叶子的最小样本数。如果提供的是浮动值，则表示样本总数的一个比例。'
- en: While `min_samples_split` is acting at the decision node level, `min_samples_leaf`
    is acting only at the leaf level.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`min_samples_split`作用于决策节点层面，但`min_samples_leaf`仅作用于叶子层面。
- en: 'Let’s see if that allows us to avoid overfitting in specific regions in our
    case. We set the minimum number of samples per split to 15 (while keeping all
    other parameters to default values). This is expected to regularize, since we
    know from the decision tree visualization some splits were for less than five
    samples:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这是否能帮助我们避免在特定区域的过拟合。在这种情况下，我们将每次分裂的最小样本数设置为15（其余参数保持默认值）。这预计将进行正则化，因为我们从决策树的可视化中知道，有些分裂的样本数少于五个：
- en: '[PRE67]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Here is the output:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 4.15 – Decision function with minimum samples per split regularization
    (plot produced by the code)](img/B19629_04_15.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![图4.15 – 每次分裂正则化的决策函数（图由代码生成）](img/B19629_04_15.jpg)'
- en: Figure 4.15 – Decision function with minimum samples per split regularization
    (plot produced by the code)
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 – 每次分裂正则化的决策函数（图由代码生成）
- en: The resulting decision function is a bit different than when regularizing with
    the maximum depth and seems to be indeed more regularized than without constraint
    on this hyperparameter.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的决策函数与使用最大深度正则化时略有不同，且确实比没有对该超参数进行约束时更加正则化。
- en: 'We can also look at the accuracy score to confirm the regularization was successful:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看准确率，以确认正则化是否成功：
- en: '[PRE68]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We’ll get the following output:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获得以下输出：
- en: '[PRE69]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Compared to default hyperparameters, the accuracy score on the test set climbed
    from 63% to 74%, while the accuracy on the training set decreased from 95% to
    86%. Compared to the maximum depth hyperparameter, we added slightly more regularization,
    and got slightly better results on the test set.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 与默认超参数相比，测试集的准确率从63%上升到74%，而训练集的准确率从95%下降到86%。与最大深度超参数相比，我们增加了些许正则化，并在测试集上获得了略微更好的结果。
- en: In general, the hyperparameters on the number of samples (either per leaf or
    split) may allow a finer regularization than the maximum depth. Indeed, the max
    depth hyperparameter is setting a common hard limit to the whole decision tree.
    But it may happen that two nodes at the same depth level do not carry the same
    number of samples. One node may have hundreds of samples (and then a splitting
    is probably relevant), while another node may have just a few samples.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，关于样本数量的超参数（无论是每个叶子还是每个分裂）可能比最大深度提供更细粒度的正则化。实际上，最大深度超参数为整个决策树设置了一个固定的硬限制。但有可能两个处于相同深度层次的节点包含的样本数不同。一个节点可能有数百个样本（这时分裂可能是相关的），而另一个节点可能只有几个样本。
- en: 'The criterion for a minimum number of samples on its side is more subtle: no
    matter the depth in the tree, if a node does not have enough samples, then we
    decide it is not worth splitting.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 对于最小样本数的准则更为微妙：无论树的深度如何，如果一个节点没有足够的样本，我们认为没有必要进行分裂。
- en: Other hyperparameters
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他超参数
- en: 'Other hyperparameters can be used to regularize. We will not go through all
    the details for each of them, but rather list them and explain them briefly:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 其他超参数可以用于正则化。我们不会逐一讲解每个超参数的细节，而是列出它们并简要解释：
- en: '`max_features`: By default, the decision tree is finding the best split among
    all features. You can choose to add randomness by setting another maximum number
    of features to use at each split. May add regularization by adding noise.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`：默认情况下，决策树会在所有特征中找到最佳分裂。你可以通过设置每次分裂时使用的特征最大数量来增加随机性，可能通过加入噪声来增加正则化。'
- en: '`max_leaf_nodes`: Set a straight limit on the number of leaves in the tree.
    Somewhat like the max depth hyperparameter, it will regularize by limiting the
    number of splits, giving priority to nodes having the highest impurity reduction.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_leaf_nodes`：设置树中叶子的数量上限。它类似于最大深度超参数，通过限制分裂的数量进行正则化，优先选择具有最大纯度减少的节点。'
- en: '`min_impurity_decrease`: This will split a node only if the impurity decrease
    is above the given threshold. This allows us to regularize by selecting highly
    impacting node splits only.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_impurity_decrease`：仅当纯度减少超过给定阈值时，才会分裂节点。这允许我们通过只选择影响较大的节点分裂来进行正则化。'
- en: Note
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although we did not mention regression trees, the behavior and principles are
    analogous, and the same hyperparameters can be fine-tuned with the same behavior.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们没有提到回归树，但其行为和原理是类似的，相同的超参数也可以通过类似的行为进行微调。
- en: See also
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'The scikit-learn documentation is pretty explicit about all the hyperparameters
    and their potential impact: [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml).'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn文档对所有超参数及其潜在影响进行了详细说明：[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml)。
- en: Training the Random Forest algorithm
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练随机森林算法
- en: The Random Forest algorithm is an ensemble learning model, meaning it uses an
    ensemble of decision trees, hence *forest* in its name.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法是一种集成学习模型，意味着它使用多个决策树集成，因此其名字中有*森林*。
- en: In this recipe, we will explain how it works and then train a Random Forest
    model on the California housing dataset.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，我们将解释它是如何工作的，然后在加利福尼亚房价数据集上训练一个随机森林模型。
- en: Getting ready
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Ensemble learning is based somehow on the idea of collective intelligence. Let’s
    do a thought experiment to understand the power of collective intelligence.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习在某种程度上基于集体智能的理念。让我们做一个思维实验，以理解集体智能的力量。
- en: Let’s assume we have a bot that randomly answers correctly to any binary question
    51% of the time. This would be considered inefficient and unreliable.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个机器人，它在任何二元问题上随机回答正确的概率为51%。这会被认为是低效且不可靠的。
- en: But now, let’s also assume we are using not only one but an army of those randomly
    answering bots and use the majority vote as the final answer. If we have 1,000
    of those bots, the majority vote will provide the right answer 75% of the time.
    If we have 10,000 bots, the majority vote will provide the right answer 97% of
    the time. This would turn a low-performing system into a remarkably high-performing
    system.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在，我们假设不仅使用一个，而是使用一支随机回答问题的机器人队伍，并使用多数投票作为最终答案。如果我们有1,000个这样的机器人， majority
    vote将75%的情况下提供正确答案。如果我们有10,000个机器人， majority vote将97%的情况下提供正确答案。这会将一个低效的系统转变为一个极为高效的系统。
- en: Note
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'A strong assumption was made for this example: each bot must be independent
    of the others. Otherwise, this example does not hold true. Indeed, the extreme
    counter-example would be that all bots are answering the same answer to any question,
    in which case, no matter how many bots you use, the accuracy remains at 51%.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个例子做了一个强假设：每个机器人必须独立于其他机器人。否则，这个例子就不成立。事实上，极端的反例是，所有机器人对任何问题都给出相同的答案，在这种情况下，无论你使用多少个机器人，准确率都保持在51%。
- en: This is the idea of collective intelligence, which relates somehow to human
    society. Most of the time, collective knowledge outperforms individual knowledge.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这是集体智慧的概念，它在某种程度上与人类社会相关。大多数情况下，集体知识超过个人知识。
- en: 'This is also the idea behind ensemble models: an ensemble of weak learners
    can become a powerful model. To do that with Random Forest, we need to define
    two key aspects:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是集成模型背后的理念：一组弱学习者可以变成一个强大的模型。为了做到这一点，在随机森林中，我们需要定义两个关键方面：
- en: How to compute the majority vote
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何计算多数投票
- en: How to ensure the independence of each decision tree in our model with randomness
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何确保我们模型中每棵决策树在随机性下的独立性
- en: Majority vote
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多数投票
- en: 'To properly explain majority vote, let’s imagine we have an ensemble of three
    decision trees trained on a binary classification task. On a given sample, the
    predictions are the following:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确解释多数投票，假设我们有一个由三棵决策树组成的集成模型，训练于一个二分类任务。在给定的样本上，预测结果如下：
- en: '| **Tree** | **Predicted probability of** **class 1** | **Class predictions**
    |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| **树** | **类别1的预测概率** | **类别预测** |'
- en: '| Tree 1 | 0.05 | 0 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 树 1 | 0.05 | 0 |'
- en: '| Tree 2 | 0.6 | 1 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 树 2 | 0.6 | 1 |'
- en: '| Tree 3 | 0.55 | 1 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 树 3 | 0.55 | 1 |'
- en: Table 4.1 – Predictions
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 表格4.1 – 预测
- en: 'We have two pieces of information for each decision tree:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为每棵决策树提供了两条信息：
- en: The predicted probability of class 1
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别1的预测概率
- en: The predicted class (usually computed as class 1 if probability > 0.5, class
    0 otherwise)
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测类别（通常在概率>0.5时计算为类别1，否则计算为类别0）
- en: Note
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `DecisionTreeClassifier.predict_proba()` method allows us to get the prediction
    probability. It is computed by using the proportion of the given class in the
    prediction leaf.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '`DecisionTreeClassifier.predict_proba()`方法允许我们获取预测概率。它是通过使用预测叶子中给定类别的比例计算的。'
- en: 'We could come up with many ways to compute the majority vote on such data,
    but let’s explore two, hard vote and soft vote:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以提出许多方法来计算这样的数据的多数投票，但让我们探讨两种方法，硬投票和软投票：
- en: A hard vote is the most intuitive one. This is the simple majority vote of the
    predicted classes. In our case, class 1 is predicted two times out of three. In
    this case, the hard majority vote is class 1.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬投票是最直观的一种。这是简单的多数投票，即预测类别的简单多数。在我们的例子中，类别1被预测了三次中的两次。在这种情况下，硬多数投票的类别为1。
- en: A soft vote uses the average probability and then applies a threshold. In our
    case, the average probability is 0.4, which is below the threshold of 0.5\. In
    that case, the soft majority vote is class 0.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个软投票方法使用平均概率然后应用一个阈值。在我们的例子中，平均概率为0.4，低于0.5的阈值。在这种情况下，软多数投票的类别为0。
- en: It is particularly interesting to note that, even if two out of three trees
    predicted class 1, the only tree that was really confident (having a high probability)
    was the tree predicting class 0.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 特别值得注意的是，即使三棵树中有两棵预测了类别1，唯一一个非常自信（具有高概率）的树却是预测类别0的那棵树。
- en: 'A real-life example would be, when facing a question:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现实中的例子是，当面对一个问题时：
- en: Two friends give answer A, but are unsure
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个朋友给出了A答案，但不确定
- en: One friend gives answer B but is highly confident
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个朋友给出了B答案，但非常自信
- en: 'What would you do in such a case? The odds are you would listen to that highly
    confident friend. This is exactly what a soft majority vote is about: giving more
    power to highly confident trees. Most of the time, the soft vote outperforms the
    hard vote. Fortunately, Random Forest implementation in scikit-learn is based
    on the soft vote.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下你会怎么做呢？你很可能会听从那个非常自信的朋友。这正是软多数投票的含义：赋予自信度高的树更多权重。大多数情况下，软投票优于硬投票。幸运的是，scikit-learn
    中的随机森林实现是基于软投票的。
- en: Bagging
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集成法
- en: Bagging is a key concept in Random Forest to ensure the independence of decision
    trees and is made of bootstrapping and aggregating. Let’s see how those two steps
    are working together to get the best out of ensembling decision trees.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 集成法（Bagging）是随机森林中的一个关键概念，它确保了决策树之间的独立性，并由自助抽样和聚合组成。让我们看看这两个步骤是如何协同工作，充分发挥集成决策树的优势的。
- en: Bootstrapping is random sampling with replacement. In simple terms, if we apply
    bootstrapping to samples with replacement, it means we will randomly pick samples
    in the dataset with replacement. What *with replacement* means is that, once a
    sample has been picked, it is not removed from the dataset and may be picked again.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 自助抽样是有放回的随机抽样。简单来说，如果我们对样本进行自助抽样并允许有放回，则意味着我们会随机从数据集中挑选样本，并且样本在选中后不会从数据集中移除，可能会被重新挑选。
- en: 'Let’s assume we have an initial dataset of 10 samples, either blue or red.
    If we use bootstrapping to select 10 samples in this initial dataset, we may have
    some samples missing, and some samples appearing several times. If we do that
    three independent times, we may have three slightly different datasets, such as
    in *Figure 4**.16*. We call those newly created datasets subsamples:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含 10 个样本的初始数据集，样本为蓝色或红色。如果我们使用自助抽样从该初始数据集中选择 10 个样本，可能会有些样本缺失，某些样本可能会出现多次。如果我们独立进行三次这样的操作，可能会得到三个略有不同的数据集，就像在*图
    4.16*中所示的那样。我们称这些新创建的数据集为子样本：
- en: '![Figure 4.16 – An example of bootstrapping an initial dataset three times
    and selecting 10 samples for replacement (the three created subsamples are slightly
    different)](img/B19629_04_16.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.16 – 对初始数据集进行三次自助抽样并选择 10 个样本进行替换（这三个生成的子样本略有不同）](img/B19629_04_16.jpg)'
- en: Figure 4.16 – An example of bootstrapping an initial dataset three times and
    selecting 10 samples for replacement (the three created subsamples are slightly
    different)
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16 – 对初始数据集进行三次自助抽样并选择 10 个样本进行替换（这三个生成的子样本略有不同）
- en: 'Since those subsamples are slightly different, in Random Forest, a decision
    tree is trained on each of those and ends up with hopefully independent models.
    The next step is the aggregating of those models’ results, through a soft majority
    vote. Once those two steps (bootstrapping and aggregating) are combined, this
    is what we call bagging. *Figure 4**.17* summarizes those two steps:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些子样本略有不同，在随机森林中，会对每个子样本训练一个决策树，并希望得到独立的模型。接下来的步骤是通过软多数投票聚合这些模型的结果。一旦这两个步骤（自助抽样和聚合）结合起来，这就是我们所说的集成法（bagging）。*图
    4.17*总结了这两个步骤：
- en: '![Figure 4.17 – Bootstrapping on the samples and then aggregating the results
    to end with an ensemble model](img/B19629_04_17.jpg)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.17 – 对样本进行自助抽样并聚合结果以得到集成模型](img/B19629_04_17.jpg)'
- en: Figure 4.17 – Bootstrapping on the samples and then aggregating the results
    to end with an ensemble model
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.17 – 对样本进行自助抽样并聚合结果以得到集成模型
- en: 'As we have seen, the *random* in Random Forest comes from the bootstrapping
    of the samples, meaning we randomly select a subsample of the original dataset
    for each trained decision tree. But in reality, other levels of randomness have
    been omitted for pedagogical reasons. Without going into all the details, there
    are three levels of randomness in a Random Forest algorithm:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，随机森林中的*随机*来自于样本的自助抽样（bootstrapping），意味着我们为每个训练的决策树随机选择原始数据集的一个子样本。但实际上，为了教学的需要，其他层次的随机性被省略了。无需进入所有细节，随机森林算法中有三个层次的随机性：
- en: '**Bootstrapping of the samples**: Samples are selected with replacement'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本的自助抽样**：样本是通过有放回的方式进行选择的'
- en: '**Bootstrapping of the features**: Features are selected with replacement'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征的自助抽样**：特征是通过有放回的方式进行选择的'
- en: '**Feature selection of the best split of a node**: By default, in scikit-learn,
    all features are used, thus there is no randomness at this level'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点最佳分割特征选择**：在 scikit-learn 中，默认使用所有特征，因此在这一层次没有随机性'
- en: 'Now that we have a solid enough understanding of how Random Forest works, let’s
    train a Random Forest algorithm on a regression task. To do so, we only need scikit-learn
    to be installed. If this has not already been done, just install it with the following
    command line:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对随机森林的工作原理有了足够清晰的理解，接下来让我们在回归任务上训练一个随机森林算法。为此，我们只需要安装scikit-learn。如果尚未安装，只需使用以下命令行进行安装：
- en: '[PRE70]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: How to do it…
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'As with other machine learning models in scikit-learn, training a Random Forest
    algorithm is quite easy. There are two main classes:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 与scikit-learn中的其他机器学习模型一样，训练随机森林算法也非常简单。主要有两个类：
- en: '`RandomForestRegressor` for regression tasks'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomForestRegressor`用于回归任务'
- en: '`RandomForestClassifier` for classification tasks'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomForestClassifier`用于分类任务'
- en: 'Here, we will use the `RandomForestRegressor` on the California housing dataset:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用`RandomForestRegressor`模型处理加利福尼亚住房数据集：
- en: 'First, let’s make the required imports: `fetch_california_housing` to load
    the data, `train_test_split` for splitting the dataset, and `RandomForestRegressor`
    for the model itself:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们进行必要的导入：`fetch_california_housing`用于加载数据，`train_test_split`用于分割数据集，`RandomForestRegressor`用于模型本身：
- en: '[PRE71]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Load the dataset using `fetch_california_housing`:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fetch_california_housing`加载数据集：
- en: '[PRE74]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Split the data with `train_test_split`. Here, we just use the default parameters
    and set the random state to 0 for reproducibility:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`分割数据。在这里，我们仅使用默认参数，并将随机种子设置为0，以确保结果可复现：
- en: '[PRE75]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Instantiate the `RandomForestRegressor` model. We just keep the default parameters
    of the class here for simplicity; we only specify the random state:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`RandomForestRegressor`模型。为简单起见，我们在此保持类的默认参数，只指定随机种子：
- en: '[PRE77]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Train the model on the training set with the `.fit()` method. This can take
    a few seconds to compute:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.fit()`方法在训练集上训练模型。这可能需要几秒钟来计算：
- en: '[PRE78]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Evaluate the R2-score on both the training and test set using the `.``score()`
    method:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.score()`方法评估训练集和测试集的R2得分：
- en: '[PRE80]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Our output would be as follows:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输出结果如下：
- en: '[PRE83]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: We have an R2-score on the training set of 97%, while on the test set, it is
    only 79%. This means we are facing overfitting, and we will see in the next recipe
    how to add regularization to such models.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集上的R2得分为97%，而在测试集上的得分仅为79%。这意味着我们遇到了过拟合问题，在下一个示例中我们将看到如何为此类模型添加正则化。
- en: See also
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The documentation of this class in scikit-learn: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.xhtml#sklearn-ensemble-randomforestregressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.xhtml#sklearn-ensemble-randomforestregressor)'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: scikit-learn中此类的文档：[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.xhtml#sklearn-ensemble-randomforestregressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.xhtml#sklearn-ensemble-randomforestregressor)
- en: 'Likewise, there is the documentation of the Random Forest classifier for classification
    tasks: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml)'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，针对分类任务，也有随机森林分类器的文档：[https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml)
- en: Regularization of Random Forest
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林的正则化
- en: A Random Forest algorithm shares many hyperparameters with decision trees since
    a Random Forest is made up of trees. But a few more hyperparameters do exist,
    so in this recipe, we will present them and show how to use them to improve results
    on the California housing dataset regression.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法与决策树共享许多超参数，因为随机森林是由多棵树组成的。但还有一些额外的超参数存在，因此在这个示例中，我们将介绍它们，并展示如何使用它们来改善加利福尼亚住房数据集回归任务的结果。
- en: Getting started
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入门
- en: 'Random Forests are known to be quite prone to overfitting. Even if it’s not
    a formal proof, in the previous recipe, we were indeed facing quite strong overfitting.
    But Random Forests, like decision trees, have many hyperparameters allowing us
    to try to reduce overfitting. As for a decision tree, we can use the following
    hyperparameters:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林已知容易过拟合。即使这不是一个正式的证明，在前一个示例中，我们确实遇到了相当强的过拟合问题。但随机森林与决策树一样，拥有许多超参数，允许我们尝试减少过拟合。对于决策树，我们可以使用以下超参数：
- en: Maximum depth
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大深度
- en: Minimum samples per leaf
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个叶子的最小样本数
- en: Minimum samples per split
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个划分的最小样本数
- en: '`max_features`'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`'
- en: '`max_leaf_nodes`'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_leaf_nodes`'
- en: '`min_impurity_decrease`'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_impurity_decrease`'
- en: 'But some other hyperparameters can be fine-tuned too:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 但也可以微调其他一些超参数：
- en: '`n_estimators`: This is the number of decision trees trained in the random
    forest.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：这是在随机森林中训练的决策树的数量。'
- en: '`max_samples`: The number of samples to draw from the given dataset to train
    each decision tree. A lower value would add regularization.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_samples`：从给定数据集中抽取的样本数，用于训练每棵决策树。较低的值会增加正则化。'
- en: Technically speaking, for this recipe, it is assumed that scikit-learn is installed.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，本食谱假设已安装scikit-learn。
- en: How to do it…
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到……
- en: 'In this recipe, we will try to add regularization by limiting the max number
    of features to the log of the total number of features. If you are reusing the
    same environment as for the previous recipe, you can jump directly to *step 4*:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将尝试通过将特征的最大数量限制为特征总数的对数来添加正则化。如果你正在使用与前一个食谱相同的环境，你可以直接跳到*第4步*：
- en: 'As usual, let’s make the required imports: `fetch_california_housing` to load
    the data, `train_test_split` for splitting the dataset, and `RandomForestRegressor`
    for the model itself:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像往常一样，让我们进行所需的导入：`fetch_california_housing`用于加载数据，`train_test_split`用于划分数据集，`RandomForestRegressor`用于模型本身：
- en: '[PRE84]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Load the dataset using `fetch_california_housing`:'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fetch_california_housing`加载数据集：
- en: '[PRE87]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Split the data with `train_test_split`. Here, we just use the default parameters,
    meaning we have a 75%25% split and set the random state to `0` for reproducibility:'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`划分数据。在这里，我们仅使用默认参数，这意味着我们有一个75%和25%的划分，并将随机状态设置为`0`以保证可重复性：
- en: '[PRE88]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Instantiate the `RandomForestRegressor` model. This time, we specify `max_features=''log2''`
    so that for each split, only a random subset (of size `log2(n)`, assuming *n*
    features) of all the features is used:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`RandomForestRegressor`模型。这次，我们指定`max_features='log2'`，这样每次拆分时，只会使用所有特征的一个随机子集（大小为`log2(n)`，假设有*n*个特征）：
- en: '[PRE90]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Train the model on the training set with the `.fit()` method. This may take
    a few seconds to compute:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.fit()`方法在训练集上训练模型。这可能需要几秒钟时间来计算：
- en: '[PRE91]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Evaluate the R2-score on both the training and test set using the `.``score()`
    method:'
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.score()`方法评估训练集和测试集上的R2得分：
- en: '[PRE93]'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'This would return the following:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下内容：
- en: '[PRE97]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Compared to the previous recipe with default hyperparameters, it improved the
    R2-score on the test set from 79% to 81%, while not significantly changing the
    score on the training set.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个使用默认超参数的食谱相比，它将测试集的R2得分从79%提高到了81%，而对训练集得分的影响不大。
- en: Note
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this case, as in many others in machine learning, it might be tricky (or
    sometimes impossible) to have the performances on the train and test set meeting
    halfway, meaning that even if the R2-score is 97% on training and 79% on the test
    set, there is absolutely no guarantee you can improve the R2-score on the test
    set. Sometimes, even the best hyperparameters are not the right key to improve
    performance.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的许多情况下，像这样，可能会遇到一个棘手的情况（有时甚至不可能），即训练集和测试集的性能无法达到平衡。这意味着，即使训练集的R2得分为97%，测试集为79%，也不能保证你能改善测试集上的R2得分。有时，即使是最好的超参数也不是改善性能的正确关键。
- en: In a word, all the regularization rules for decision trees can be applied to
    Random Forests, and a few more are available. As usual, an effective way to find
    the right set of hyperparameters is through hyperparameter optimization. Random
    Forest takes somewhat longer to train than a simple decision tree, so it may take
    some time.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，所有适用于决策树的正则化规则都可以应用于随机森林，并且还提供了更多的规则。像往常一样，找到合适的超参数集的有效方法是通过超参数优化。随机森林的训练时间比简单的决策树要长，因此可能需要一些时间。
- en: Training a boosting model with XGBoost
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用XGBoost训练提升模型
- en: 'Let’s now see another application of decision trees: boosting. While bagging
    (used in Random Forest models) is training several trees in parallel, boosting
    is about training trees sequentially. In this recipe, we will have a quick review
    of what is boosting, and then train a boosting model with XGBoost, a widely used
    boosting library.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看决策树的另一个应用：提升。与并行训练多棵树的包装（在随机森林模型中使用）不同，提升是关于顺序训练树。在这个食谱中，我们将快速回顾什么是提升，然后使用XGBoost，一个广泛使用的提升库，训练一个提升模型。
- en: Getting ready
  id: totrans-439
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Let’s have a look at introducing limits of bagging, then see how boosting may
    address some of those limits and how. Finally, let’s train a model on the already
    prepared Titanic dataset with XGBoost.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下引入包装的局限性，然后看看提升如何解决其中的一些限制，以及如何解决。最后，让我们使用XGBoost在已经准备好的Titanic数据集上训练一个模型。
- en: Limits of bagging
  id: totrans-441
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 包装的局限性
- en: Let’s assume we have a binary classification task, and we trained Random Forest
    in three decision trees on two features. Bagging is expected to perform well if
    anywhere in the feature space, at least two out of three decision trees are right,
    as in *Figure 4**.18*.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个二分类任务，我们在两个特征上训练了三棵决策树的随机森林。如果在特征空间中的任何地方，至少三分之二的决策树是正确的，那么袋装方法预计将表现良好，如*图
    4.18*所示。
- en: '![Figure 4.18 – The absence of overlap in dashed circle areas highlights decision
    tree errors, demonstrating Random Forest’s strong performance](img/B19629_04_18.jpg)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.18 – 虚线圆圈区域的重叠缺失突出显示了决策树的错误，展示了随机森林的强大性能](img/B19629_04_18.jpg)'
- en: Figure 4.18 – The absence of overlap in dashed circle areas highlights decision
    tree errors, demonstrating Random Forest’s strong performance
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.18 – 虚线圆圈区域的重叠缺失突出显示了决策树的错误，展示了随机森林的强大性能
- en: In *Figure 4**.18*, we observe that the areas inside the dashed circles are
    where a decision tree is wrong. Since they don’t overlap, at least two out of
    three decision trees are right everywhere. Thus, Random Forest is performing well.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 4.18*中，我们观察到虚线圆圈内的区域是决策树错误的地方。由于它们没有重叠，三分之二的决策树在任何地方都是正确的。因此，随机森林表现良好。
- en: Unfortunately, always having two out of three decision trees right is a strong
    assumption. What happens if only one or fewer decision tree is right in the feature
    space? As represented in *Figure 4**.19*, the Random Forest algorithm starts performing
    poorly.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，始终有三分之二的决策树正确是一个强假设。如果在特征空间中只有一个或更少的决策树是正确的，会发生什么？如*图 4.19*所示，随机森林算法开始表现不佳。
- en: '![Figure 4.19 – When one or fewer out of three decision trees is right, Random
    Forest is performing poorly](img/B19629_04_19.jpg)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.19 – 当三个决策树中只有一个或更少是正确时，随机森林表现不佳](img/B19629_04_19.jpg)'
- en: Figure 4.19 – When one or fewer out of three decision trees is right, Random
    Forest is performing poorly
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.19 – 当三个决策树中只有一个或更少是正确时，随机森林表现不佳
- en: Let’s see how boosting can fix this issue by sequentially training decision
    trees to each try to fix the errors of the previous one.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看提升如何通过顺序训练决策树来修复这个问题，使每棵树都试图修正前一棵树的错误。
- en: Note
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Those examples are simplified since Random Forest can be using soft vote, and
    thus predict a class that only a minority of trees predicted. But the principle
    remains true overall.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例被简化了，因为随机森林可以使用软投票，因此可以预测只有少数几棵树预测的类别。但总体原理仍然适用。
- en: Gradient boosting principles
  id: totrans-452
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度提升原理
- en: 'Gradient boosting has several implementations with a some differences: XGBoost,
    CatBoost, and LightGBM all have pros and cons, and the details of each are beyond
    the scope of this book. Rather, we will explain some general principles of the
    gradient boosting algorithm, enough to give a high-level understanding of the
    model.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升有几种实现方式，存在一些差异：XGBoost、CatBoost和LightGBM都有各自的优缺点，而每个方法的细节超出了本书的范围。我们将解释梯度提升算法的一些通用原理，足以对模型有一个高层次的理解。
- en: 'The algorithm training can be summarized with the following steps:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 算法训练可以通过以下步骤总结：
- en: Compute an average guess on the ![](img/Formula_04_026.png) training set.
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上计算平均猜测值！[](img/Formula_04_026.png)。
- en: Compute the pseudo-residuals of each sample toward the last guessed prediction,
    ![](img/Formula_04_027.png).
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个样本相对于最后一次猜测预测的伪残差！[](img/Formula_04_027.png)。
- en: Train a decision tree on the pseudo-residuals as labels, allowing to have predictions
    ![](img/Formula_04_028.png).
  id: totrans-457
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在伪残差上训练一个决策树作为标签，从而得到预测值！[](img/Formula_04_028.png)。
- en: Compute the weight ![](img/Formula_04_029.png) of that decision tree based on
    its performance.
  id: totrans-458
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于决策树的性能计算权重！[](img/Formula_04_029.png)。
- en: 'Update the guess with the learning rate 𝜂, 𝛾i and these predicted pseudo-residuals:
    ![](img/Formula_04_030.png)o.'
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用学习率𝜂、𝛾i和这些预测的伪残差更新猜测值：![](img/Formula_04_030.png)o。
- en: Go back to *step 2*. with the updated ![](img/Formula_04_031.png), iterate until
    reaching the maximum number of decision trees or another criterion.
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到*步骤 2*，使用更新后的！[](img/Formula_04_031.png)，迭代直到达到决策树的最大数量或其他标准。
- en: 'In the end, the final prediction will be the following:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，最终预测将是以下内容：
- en: '![](img/Formula_04_032.jpg)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_04_032.jpg)'
- en: The 𝜂 learning rate is a hyperparameter, typically 0.001.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 𝜂学习率是一个超参数，通常为0.001。
- en: Note
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: All boosting implementations are a bit different. For example, not all boosting
    implementations have weights 𝛾 associated with their trees. But since this is
    the case for XGBoost that we will use here, it is worth mentioning it for a better
    understanding.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 所有提升算法的实现都有些不同。例如，并不是所有提升算法的树都与权重 𝛾 相关。但由于这是我们将在此使用的 XGBoost 所具备的特性，因此值得提及，以便更好地理解。
- en: In the end, having enough decision trees allows a model to perform well enough
    in most cases, hopefully. Unlike Random Forest, boosting models tend to avoid
    pitfalls such as having most number of wrong decision trees at the same place,
    since each decision tree is trying to fix remaining errors.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，拥有足够的决策树可以让模型在大多数情况下表现得足够好。与随机森林不同，提升模型通常避免了类似于在同一位置有最多错误决策树的陷阱，因为每棵决策树都在尝试修正剩余的错误。
- en: Also, boosting models tend to be more robust and generalized than Random Forest
    models, making them really powerful in many applications.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，提升模型通常比随机森林模型更具鲁棒性和泛化能力，使其在许多应用中非常强大。
- en: 'Finally, for this recipe, we will need the following libraries to be installed:
    `pickle` and `xgboost`. They can be installed using `pip` with the following command
    line:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于这个食谱，我们需要安装以下库：`pickle` 和 `xgboost`。它们可以通过以下命令行使用 `pip` 安装：
- en: '[PRE98]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'We will also reuse a prepared Titanic dataset from a previous recipe to avoid
    spending too much time on the data preparation. This data can be downloaded at
    [https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_02/prepared_titanic.pkl](https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_02/prepared_titanic.pkl)
    and should be added locally before doing the recipe with the following command
    line:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将重用之前准备好的 Titanic 数据集，以避免花费过多时间在数据准备上。该数据可以在[https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_02/prepared_titanic.pkl](https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_02/prepared_titanic.pkl)下载，并应在进行该食谱之前通过以下命令行本地添加：
- en: '[PRE99]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: How to do it…
  id: totrans-472
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'XGBoost is a very popular implementation of gradient boosting. It can be used
    with the same pattern as models in scikit-learn, using the following methods:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 是一种非常流行的梯度提升实现。它可以像 scikit-learn 中的模型一样使用，使用以下方法：
- en: '`fit(X, y)` to train the model'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit(X, y)` 用于训练模型'
- en: '`predict(X)` to compute predictions'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict(X)` 用于计算预测结果'
- en: '`score(X, y)` to evaluate the model'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`score(X, y)` 用于评估模型'
- en: 'Let’s use it on the Titanic dataset with the default parameters downloaded
    locally:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本地下载的 Titanic 数据集上使用默认参数：
- en: 'The first step is the required imports. Here, we need pickle to read the data
    from the binary format and the `XGBoost` classification model class `XGBClassifier`:'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是所需的导入。在这里，我们需要 pickle 来读取二进制格式的数据，以及 `XGBoost` 分类模型类 `XGBClassifier`：
- en: '[PRE100]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'We load the already prepared data using pickle. Note that we already get a
    split dataset because it was actually saved this way:'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 pickle 加载已经准备好的数据。请注意，我们已经得到一个拆分数据集，因为它实际上是以这种方式保存的：
- en: '[PRE102]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Instantiate the boosting model. We specify `use_label_encoder=False` because
    our qualitative features are actually already encoded with one hot encoding and
    because this feature is about to be deprecated:'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化提升模型。我们指定 `use_label_encoder=False`，因为我们的定性特征已经通过一热编码进行了编码，而且这个特性即将被弃用：
- en: '[PRE104]'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Train the model on the training set using the `.fit()` method, exactly like
    we would do for a scikit-learn model:'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.fit()` 方法在训练集上训练模型，就像我们为 scikit-learn 模型所做的那样：
- en: '[PRE105]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Compute the accuracy of the model on both training and test sets, using the
    `.score()` method. Again, this is the same as in scikit-learn:'
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.score()` 方法计算模型在训练集和测试集上的准确度。同样，这与 scikit-learn 中的做法相同：
- en: '[PRE107]'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'We’ll get the following now:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到以下结果：
- en: '[PRE111]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'We notice overfitting: a 97% accuracy rate on the training set but only 81%
    on the test set. But in the end, the results of the test set are quite good, since
    it is quite hard to have much higher than 85% accuracy on the Titanic dataset.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到过拟合：训练集准确率为 97%，但测试集准确率仅为 81%。但最终，测试集的结果还是相当不错的，因为在 Titanic 数据集上很难达到 85%
    以上的准确度。
- en: See also
  id: totrans-497
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The XGBoost documentation quality is arguably not as good as scikit-learn’s,
    but it is still useful: [https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBClassifier).'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 的文档质量可能不如 scikit-learn，但仍然有用：[https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBClassifier)。
- en: 'There is also the regression counterpart class XGBRegressor and its documentation:
    [https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBRegressor](https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBRegressor).'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有回归对应类XGBRegressor及其文档：[https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBRegressor](https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBRegressor)。
- en: Regularization with XGBoost
  id: totrans-500
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用XGBoost进行正则化
- en: After a recipe introducing boosting and the use of XGBoost for classification,
    let’s now have a look at how to regularize such models. We will be using the same
    Titanic dataset and try to improve test accuracy.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了提升方法和XGBoost分类使用的实例后，接下来我们来看一下如何对这些模型进行正则化。我们将使用相同的Titanic数据集，并尝试提高测试准确度。
- en: Getting ready
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'Just like Random Forest, an XGBoost model is made of decision trees. Consequently,
    it has some hyperparameters such as the maximum depth of trees (`max_depth`) or
    the number of trees (`n_estimators`) that can allow to regularize in the same
    way. It also has several other hyperparameters related to the decision trees that
    can be fine-tuned:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 就像随机森林一样，XGBoost模型由决策树组成。因此，它有一些超参数，例如树的最大深度（`max_depth`）或树的数量（`n_estimators`），这些超参数可以以相同的方式进行正则化。它还有一些与决策树相关的其他超参数可以进行微调：
- en: '`subsample`: The number of samples to randomly draw for training, equivalent
    to `max_sample` for scikit-learn’s decision trees. A smaller value may add regularization.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsample`：用于训练的随机抽取样本数，等同于scikit-learn中决策树的`max_sample`。较小的值可能会增加正则化效果。'
- en: '`colsample_bytree`: The number of features to randomly draw (equivalent to
    scikit-learn’s `max_features`) for each tree. A smaller value may add regularization.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colsample_bytree`：在每棵树中随机抽取的特征数量（等同于scikit-learn的`max_features`）。较小的值可能会增加正则化效果。'
- en: '`colsample_bylevel`: The number of features to randomly draw at the tree level.
    A smaller value may add regularization.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colsample_bylevel`：在树级别随机抽取的特征数量。较小的值可能会增加正则化效果。'
- en: '`colsample_bynode`: The number of features to randomly draw at the node level.
    A smaller value may add regularization.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`colsample_bynode`：在节点级别随机抽取的特征数量。较小的值可能会增加正则化效果。'
- en: 'Finally, more hyperparameters that are not shared with decision trees or Random
    Forest can allow fine-tuning the XGBoost models:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有一些与决策树或随机森林不同的超参数可以用于调优XGBoost模型：
- en: '`learning_rate`: The learning rate. A smaller learning rate may train even
    closer to the training set. Thus, a larger learning rate may regularize, although
    it may also degrade the performances'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：学习率。较小的学习率可能会使训练更加接近训练集。因此，较大的学习率可能会起到正则化作用，尽管它也可能会降低模型性能。'
- en: '`reg_alpha`: The strength of the L1 regularization. A higher value implies
    more regularization.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reg_alpha`：L1正则化的强度。较高的值意味着更强的正则化。'
- en: '`reg_lambda`: The strength of the L2 regularizations. A higher value implies
    more regularization.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reg_lambda`：L2正则化的强度。较高的值意味着更强的正则化。'
- en: Unlike other tree-based models seen so far, XGBoost allows L1 and L2 regularization
    too. Indeed, since each tree has an associated weight of 𝛾i, it is possible to
    add L1 or L2 regularization on those parameters, just the way it is done for linear
    models.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前所见的其他基于树的模型不同，XGBoost也支持L1和L2正则化。实际上，由于每棵树都有一个关联的权重𝛾i，因此可以像在线性模型中一样对这些参数添加L1或L2正则化。
- en: Those are the main hyperparameters to fine-tune to optimize and properly regularize
    an XGBoost whenever needed. Although it’s a powerful, robust, and efficient model,
    it can be hard to fine-tune optimally, since there is quite a large number of
    hyperparameters.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是调优XGBoost时需要微调的主要超参数，以优化并适当正则化模型。虽然它是一个强大、健壮且高效的模型，但由于超参数数量众多，最佳调优可能会比较困难。
- en: More practically, in this recipe, we will only add L1 regularization. To do
    so, all we need is to have XGBoost installed, as well as the Titanic-prepared
    data, as for the previous recipe.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 更实际地说，在这个实例中，我们将只添加L1正则化。为此，我们只需要安装XGBoost，并准备好Titanic数据集，如同上一个实例一样。
- en: How to do it…
  id: totrans-515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: In this recipe, let’s add L1 regularization with the parameter `reg_alpha` in
    order to add bias and hopefully reduce the variance of the model. We will reuse
    the `XGBClassifier` model on the prepared Titanic data, as we did for the previous
    recipe. If your environment is still the same as the previous recipe, you can
    jump to *step 3*.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实例中，我们将使用`reg_alpha`参数添加L1正则化，以便添加偏差并希望减少模型的方差。我们将重新使用之前实例中的`XGBClassifier`模型，使用已经准备好的Titanic数据。如果你的环境与之前一样，可以跳到*步骤3*。
- en: 'As usual, we start with the required imports: `pickle` to read the data from
    the binary format and the `XGBoost` classification model class `XGBClassifier`:'
  id: totrans-517
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 和往常一样，我们从必需的导入开始：使用 `pickle` 从二进制格式读取数据，并使用 `XGBoost` 分类模型类 `XGBClassifier`：
- en: '[PRE112]'
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Then, we load the already prepared data using `pickle`. It assumes the `prepared_titanic.pkl`
    file is locally downloaded:'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用 `pickle` 加载已经准备好的数据。它假设 `prepared_titanic.pkl` 文件已经在本地下载：
- en: '[PRE114]'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Instantiate the boosting model. Besides specifying `use_label_encoder=False`,
    we now specify `reg_alpha=1` to add `L1` regularization:'
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化提升模型。除了指定 `use_label_encoder=False` 外，我们现在还指定 `reg_alpha=1` 来添加 `L1` 正则化：
- en: '[PRE116]'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Train the model on the training set using the `.``fit()` method:'
  id: totrans-525
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.``fit()` 方法在训练集上训练模型：
- en: '[PRE117]'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Finally, compute the accuracy of the model on both training and test sets,
    using the `.``score()` method:'
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用 `.``score()` 方法计算模型在训练集和测试集上的准确率：
- en: '[PRE118]'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'This would print the following:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印以下内容：
- en: '[PRE122]'
  id: totrans-533
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: Compared to the previous recipe with default hyperparameters, adding L1 penalization
    allowed us to improve the results. The accuracy scores are now about 84% on the
    test set, and they lowered to 94% on the training set, effectively adding regularization.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的默认超参数配置相比，加入 L1 惩罚使我们能够改善结果。现在在测试集上的准确率约为 84%，而在训练集上降低到 94%，有效地加入了正则化。
- en: There’s more…
  id: totrans-535
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: Finding the best hyperparameter set with XGBoost can be tricky as there are
    many hyperparameters. Of course, using hyperparameter optimization techniques
    is required to gain some previous time.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 XGBoost 寻找最佳超参数集可能会比较棘手，因为超参数种类繁多。当然，使用超参数优化技术是为了节省一些前期时间。
- en: For regression tasks, as mentioned earlier, the `XGBoost` library has an `XGBRegressor`
    class that makes them possible, with the same hyperparameters having the same
    effects on regularization.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归任务，如前所述，`XGBoost` 库有一个 `XGBRegressor` 类来实现这些任务，并且相同的超参数对正则化有相同的效果。
