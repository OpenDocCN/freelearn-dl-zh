- en: Create and Train Machine Translation Systems
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建并训练机器翻译系统
- en: The objective of this project is to train an **artificial intelligence** (**AI**)
    model to be able to translate between two languages. Specifically, we will see
    an automatic translator which reads German and produces English sentences; although,
    the model and the code developed in this chapter is generic enough for any language
    pair.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的目标是训练一个**人工智能**（**AI**）模型，使其能够在两种语言之间进行翻译。具体来说，我们将看到一个自动翻译器，它读取德语并生成英语句子；不过，本章中开发的模型和代码足够通用，可以应用于任何语言对。
- en: 'The project explored in this chapter has four important sections, as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨的项目有四个重要部分，如下所示：
- en: A walkthrough of the architecture
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构概述
- en: Preprocessing the corpora
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理语料库
- en: Training the machine translator
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练机器翻译器
- en: Testing and translating
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试与翻译
- en: Each of them will describe one key component of the project, and, at the end,
    you'll have a clear picture of what's going on.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 它们每个都会描述项目中的一个关键组件，最后，你将对发生的事情有一个清晰的了解。
- en: A walkthrough of the architecture
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构概述
- en: 'A machine translation system receives as input an arbitrary string in one language
    and produces, as output, a string with the same meaning but in another language.
    Google Translate is one example (but also many other main IT companies have their
    own). There, users are able to translate to and from more than 100 languages.
    Using the webpage is easy: on the left just put the sentence you want to translate
    (for example, Hello World), select its language (in the example, it''s English),
    and select the language you want it to be translated to.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器翻译系统接收一种语言的任意字符串作为输入，并生成一个在另一种语言中具有相同含义的字符串作为输出。Google 翻译就是一个例子（但许多其他大型IT公司也有自己的系统）。在这里，用户可以进行超过100种语言之间的翻译。使用网页非常简单：在左侧输入你想翻译的句子（例如，Hello
    World），选择其语言（在这个例子中是英语），然后选择你希望翻译成的目标语言。
- en: 'Here''s an example where we translate the sentence Hello World to French:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子，我们将句子Hello World翻译成法语：
- en: '![](img/ccea21d1-123d-41a1-b60d-7cae15f0e6cf.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccea21d1-123d-41a1-b60d-7cae15f0e6cf.png)'
- en: Is it easy? At a glance, we may think it's a simple dictionary substitution.
    Words are chunked, the translation is looked up on the specific English-to-French
    dictionary, and each word is substituted with its translation. Unfortunately,
    that's not the case. In the example, the English sentence has two words, while
    the French one has three. More generically, think about phrasal verbs (turn up,
    turn off, turn on, turn down), Saxon genitive, grammatical gender, tenses, conditional
    sentences... they don't always have a direct translation, and the correct one
    should follow the context of the sentence.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这很容易吗？乍一看，我们可能会认为这只是简单的字典替换。单词被分块，翻译通过特定的英法词典查找，每个单词用其翻译进行替换。不幸的是，事实并非如此。在这个例子中，英语句子有两个单词，而法语句子有三个。更一般来说，想想短语动词（turn
    up, turn off, turn on, turn down），萨克森属格，语法性别，时态，条件句……它们并不总是有直接的翻译，正确的翻译应当根据句子的上下文来决定。
- en: 'That''s why, for doing machine translation, we need some artificial intelligence tools.
    Specifically, as for many other **natural language processing** (**NLP**) tasks,
    we''ll be using **recurrent neural networks** (**RNNs**). We introduced RNNs in
    the previous chapter, and the main feature they have is that they work on sequences:
    given an input sequence, they produce an output sequence. The objective of this
    chapter is to create the correct training pipeline for having a sentence as the
    input sequence, and its translation as the output one. Remember also the *no free
    lunch theorem*: this process isn''t easy, and more solutions can be created with
    the same result. Here, in this chapter, we will propose a simple but powerful
    one.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么，在进行机器翻译时，我们需要一些人工智能工具。具体来说，就像许多其他**自然语言处理**（**NLP**）任务一样，我们将使用**循环神经网络**（**RNN**）。我们在上一章介绍了RNN，主要特点是它们处理序列：给定输入序列，产生输出序列。本章的目标是创建正确的训练流程，以使句子作为输入序列，其翻译作为输出序列。还要记住*没有免费的午餐定理*：这个过程并不简单，更多的解决方案可以用相同的结果创造出来。在本章中，我们将提出一个简单而强大的方法。
- en: 'First of all, we start with the corpora: it''s maybe the hardest thing to find,
    since it should contain a high fidelity translation of many sentences from a language
    to another one. Fortunately, NLTK, a well-known package of Python for NLP, contains
    the corpora Comtrans. **Comtrans** is the acronym of **combination approach to
    machine translation**, and contains an aligned corpora for three languages: German,
    French, and English.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从语料库开始：这可能是最难找到的部分，因为它应该包含从一种语言到另一种语言的高保真度翻译。幸运的是，NLTK，一个著名的Python自然语言处理包，包含了Comtrans语料库。**Comtrans**是**机器翻译组合方法**（combination
    approach to machine translation）的缩写，包含了三种语言的对齐语料库：德语、法语和英语。
- en: 'In this project, we will use these corpora for a few reasons, as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将使用这些语料库出于以下几个原因：
- en: It's easy to download and import in Python.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它可以很容易地在Python中下载和导入。
- en: No preprocessing is needed to read it from disk / from the internet. NLTK already
    handles that part.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不需要预处理来从磁盘/互联网读取它。NLTK已经处理了这部分内容。
- en: It's small enough to be used on many laptops (a few dozen thousands sentences).
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它足够小，可以在许多笔记本电脑上使用（只有几万句）。
- en: It's freely available on the internet.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它可以自由地在互联网上获取。
- en: For more information about the Comtrans project, go to [http://www.fask.uni-mainz.de/user/rapp/comtrans/](http://www.fask.uni-mainz.de/user/rapp/comtrans/).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Comtrans项目的更多信息，请访问 [http://www.fask.uni-mainz.de/user/rapp/comtrans/](http://www.fask.uni-mainz.de/user/rapp/comtrans/)。
- en: 'More specifically, we will try to create a machine translation system to translate
    German to English. We picked these two languages at random among the ones available
    in the Comtrans corpora: feel free to flip them, or use the French corpora instead.
    The pipeline of our project is generic enough to handle any combination.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体来说，我们将尝试创建一个机器翻译系统，将德语翻译成英语。我们随机选择了这两种语言，作为Comtrans语料库中可用语言的其中一对：你可以自由选择交换它们，或者改用法语语料库。我们的项目管道足够通用，可以处理任何组合。
- en: 'Let''s now investigate how the corpora is organized by typing some commands:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过输入一些命令来调查语料库的组织结构：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output is as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The pairs of sentences are available using the function `aligned_sents`. The
    filename contains the from and to language. In this case, as for the following
    part of the project, we will translate German (*de*) to English (*en*). The returned
    object is an instance of the class `nltk.translate.api.AlignedSent`. By looking
    at the documentation, the first language is accessible with the attribute `words`,
    while the second language is accessible with the attribute `mots`. So, to extract
    the German sentence and its English translation separately, we should run:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 句子对可以通过函数`aligned_sents`获取。文件名包含源语言和目标语言。在这种情况下，作为项目的后续部分，我们将翻译德语（*de*）到英语（*en*）。返回的对象是类`nltk.translate.api.AlignedSent`的一个实例。从文档中可以看到，第一个语言可以通过属性`words`访问，第二个语言可以通过属性`mots`访问。所以，为了分别提取德语句子和其英语翻译，我们应该运行：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code outputs:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码输出：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How nice! The sentences are already tokenized, and they look as sequences. In
    fact, they will be the input and (hopefully) the output of the RNN which will
    provide the service of machine translation from German to English for our project.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 真棒！这些句子已经被分词，并且看起来像是序列。实际上，它们将是我们项目中RNN的输入和（希望）输出，RNN将为我们提供德语到英语的机器翻译服务。
- en: 'Furthermore, if you want to understand the dynamics of the language, Comtrans
    makes available the alignment of the words in the translation:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你想了解语言的动态，Comtrans还提供了翻译中单词的对齐：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code outputs:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码输出：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The first word in German is translated to the first word in English *(Wiederaufnahme*
    to *Resumption),* the second to the second *(der* to both *of* and *the),* and
    the third (at index 1) is translated with the fourth *(Sitzungsperiode* to *session).*
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 德语中的第一个词被翻译为英语中的第一个词（*Wiederaufnahme*到*Resumption*），第二个词被翻译为第二个词（*der*到*of*和*the*），第三个（索引为1）被翻译为第四个词（*Sitzungsperiode*到*session*）。
- en: Preprocessing of the corpora
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语料库的预处理
- en: The first step is to retrieve the corpora. We've already seen how to do this,
    but let's now formalize it in a function. To make it generic enough, let's enclose
    these functions in a file named `corpora_tools.py`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是获取语料库。我们已经看到过如何做到这一点，但现在让我们将其形式化为一个函数。为了使其足够通用，我们将把这些函数封装在一个名为`corpora_tools.py`的文件中。
- en: 'Let''s do some imports that we will use later on:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们导入一些稍后会用到的内容：
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, let''s create the function to retrieve the corpora:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个函数来获取语料库：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This function has one argument; the file containing the aligned sentences from
    the NLTK Comtrans corpora. It returns two lists of sentences (actually, they're
    a list of tokens), one for the source language (in our case, German), the other
    in the destination language (in our case, English).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数有一个参数；包含来自NLTK Comtrans语料库的对齐句子的文件。它返回两个句子列表（实际上是词汇列表），一个用于源语言（在我们的例子中是德语），另一个用于目标语言（在我们的例子中是英语）。
- en: 'On a separate Python REPL, we can test this function:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个单独的Python REPL中，我们可以测试这个函数：
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code creates the following output:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We also printed the number of sentences in each corpora (33,000) and asserted
    that the number of sentences in the source and the destination languages is the
    same.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还打印了每个语料库中的句子数量（33,000），并确认源语言和目标语言的句子数量相同。
- en: 'In the following step, we want to clean up the tokens. Specifically, we want
    to tokenize punctuation and lowercase the tokens. To do so, we can create a new
    function in `corpora_tools.py`. We will use the `regex` module to perform the
    further splitting tokenization:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们希望清理掉无用的标记。具体来说，我们要对标点符号进行分词处理，并将所有词汇小写。为此，我们可以在`corpora_tools.py`中创建一个新函数。我们将使用`regex`模块来进一步分词：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Again, in the REPL, let''s test the function:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，在REPL中，我们来测试这个函数：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code outputs the same sentence as before, but chunked and cleaned:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出与之前相同的句子，但已分块并清理：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Nice!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！
- en: The next step for this project is filtering the sentences that are too long
    to be processed. Since our goal is to perform the processing on a local machine,
    we should limit ourselves to sentences up to *N* tokens. In this case, we set
    *N*=20, in order to be able to train the learner within 24 hours. If you have
    a powerful machine, feel free to increase that limit. To make the function generic
    enough, there's also a lower bound with a default value set to 0, such as an empty
    token set.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的下一步是筛选出过长的句子，无法进行处理。由于我们的目标是在本地机器上进行处理，我们应该限制句子的长度在*N*个词以内。在这种情况下，我们将*N*设置为20，以便在24小时内能够训练学习器。如果你有一台强大的机器，可以随意提高这个限制。为了使函数足够通用，还设置了一个下限，默认值为0，例如一个空的词汇集。
- en: 'The logic of the function is very easy: if the number of tokens for a sentence
    or its translation is greater than *N*, then the sentence (in both languages)
    is removed:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 函数的逻辑非常简单：如果句子或其翻译的词汇数大于*N*，那么就将该句子（无论源语言还是目标语言）移除：
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Again, let''s see in the REPL how many sentences survived this filter. Remember,
    we started with more than 33,000:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，让我们在REPL中查看有多少句子通过了这个过滤器。记住，我们起始时有超过33,000个句子：
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code prints the following output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码打印出以下输出：
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Almost 15,000 sentences survived, that is, half of the corpora.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 大约15,000个句子存活下来，也就是语料库的一半。
- en: 'Now, we finally move from text to numbers (which AI mainly uses). To do so,
    we shall create a dictionary of the words for each language. The dictionary should
    be big enough to contain most of the words, though we can discard some if the
    language has words with low occourrence. This is a common practice even in the
    tf-idf (term frequency within a document, multiplied by the inverse of the document
    frequency, i.e. in how many documents that token appears), where very rare words
    are discarded to speed up the computation, and make the solution more scalable
    and generic. We need here four special symbols in both dictionaries:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们终于从文本转向数字（AI主要使用这些）。为此，我们将为每种语言创建一个词典。这个词典应该足够大，能够包含大多数词汇，尽管我们可以丢弃一些出现频率很低的词汇。如果某种语言有低频词汇，这是常见做法，就像tf-idf（文档中词频乘以文档频率的倒数，即该词在多少个文档中出现）一样，极为罕见的词汇会被丢弃，以加速计算并使解决方案更加可扩展和通用。在这里，我们需要在两个词典中分别有四个特殊符号：
- en: One symbol for padding (we'll see later why we need it)
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个符号用于填充（稍后我们会看到为什么需要它）
- en: One symbol for dividing the two sentences
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个符号用于分隔两个句子
- en: One symbol to indicate where the sentence stops
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个符号表示句子的结束位置
- en: One symbol to indicate unknown words (like the very rare ones)
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个符号用于表示未知词汇（比如那些非常罕见的词）
- en: 'For doing so, let''s create a new file named `data_utils.py` containing the
    following lines of code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，让我们创建一个新的文件，命名为`data_utils.py`，并包含以下代码行：
- en: '[PRE16]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, back to the `corpora_tools.py` file, let''s add the following function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，返回到`corpora_tools.py`文件中，让我们添加以下函数：
- en: '[PRE17]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This function takes as arguments the number of entries in the dictionary and
    the path of where to store the dictionary. Remember, the dictionary is created
    while training the algorithms: during the testing phase it''s loaded, and the
    association token/symbol should be the same one as used in the training. If the
    number of unique tokens is greater than the value set, only the most popular ones
    are selected. At the end, the dictionary contains the association between a token
    and its ID for each language.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的参数包括字典中的条目数和存储字典的路径。记住，字典是在训练算法时创建的：在测试阶段它会被加载，且令牌/符号的关联应与训练中使用的一致。如果唯一令牌的数量大于设定的值，则只选择最常见的那些。最终，字典包含每种语言中令牌及其ID之间的关联。
- en: After building the dictionary, we should look up the tokens and substitute them
    with their token ID.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建字典之后，我们应该查找令牌并用它们的令牌ID进行替换。
- en: 'For that, we need another function:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们需要另一个函数：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This step is very simple; the token is substituted with its ID. If the token
    is not in the dictionary, the ID of the unknown token is used. Let''s see in the
    REPL how our sentences look after these steps:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步非常简单；令牌会被替换成其ID。如果令牌不在字典中，则使用未知令牌的ID。让我们在REPL中查看经过这些步骤后的句子：
- en: '[PRE19]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This code prints the token and its ID for both the sentences. What''s used
    in the RNN will be just the second element of each tuple, that is, the integer
    ID:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码打印了两个句子的令牌及其ID。RNN中使用的将只是每个元组的第二个元素，也就是整数ID：
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Please also note how frequent tokens, such as *the* and *of* in English, and
    *der* in German, have a low ID. That's because the IDs are sorted by popularity
    (see the body of the function `create_indexed_dictionary`).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 另外请注意，像英语中的*the*和*of*，德语中的*der*等常见令牌，其ID较低。这是因为ID是按流行度排序的（见函数`create_indexed_dictionary`的主体）。
- en: 'Even though we did the filtering to limit the maximum size of the sentences,
    we should create a function to extract the maximum size. For the lucky owners
    of very powerful machines, which didn''t do any filtering, that''s the moment
    to see how long the longest sentence in the RNN will be. That''s simply the function:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们做了过滤以限制句子的最大长度，我们仍然应该创建一个函数来提取最大长度。对于那些拥有非常强大机器的幸运用户，如果没有进行任何过滤，那么现在就是看RNN中最长期限句子多长的时刻。这个函数就是：
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s apply the following to our sentences:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对这些句子应用以下操作：
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As expected, the output is:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，输出为：
- en: '[PRE23]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The final preprocessing step is padding. We need all the sequences to be the
    same length, therefore we should pad the shorter ones. Also, we need to insert
    the correct tokens to instruct the RNN where the string begins and ends.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的预处理步骤是填充。我们需要所有序列具有相同的长度，因此需要填充较短的序列。此外，我们需要插入正确的令牌，指示RNN字符串的开始和结束位置。
- en: 'Basically, this step should:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这一步应该：
- en: Pad the input sequences, for all being 20 symbols long
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充输入序列，使它们都为20个符号长
- en: Pad the output sequence, to be 20 symbols long
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充输出序列，使其为20个符号长
- en: Insert an `_GO` at the beginning of the output sequence and an `_EOS` at the
    end to position the start and the end of the translation
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在输出序列的开头插入一个`_GO`，在结尾插入一个`_EOS`，用以标识翻译的开始和结束
- en: 'This is done by this function (insert it in the `corpora_tools.py`):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过这个函数完成的（将其插入到`corpora_tools.py`中）：
- en: '[PRE24]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To test it, let''s prepare the dataset and print the first sentence:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试它，让我们准备数据集并打印第一句：
- en: '[PRE25]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code outputs the following:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出如下：
- en: '[PRE26]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: As you can see, the input and the output are padded with zeros to have a constant
    length (in the dictionary, they correspond to `_PAD`, see `data_utils.py`), and
    the output contains the markers 1 and 2 just before the start and the end of the
    sentence. As proven effective in the literature, we're going to pad the input
    sentences at the start and the output sentences at the end. After this operation,
    all the input sentences are `20` items long, and the output sentences `22`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，输入和输出都通过零进行填充以保持常数长度（在字典中，它们对应于`_PAD`，见`data_utils.py`），输出中包含标记1和2，分别位于句子的开始和结束之前。根据文献证明的有效方法，我们将填充输入句子的开始，并填充输出句子的结束。完成此操作后，所有输入句子的长度都是`20`，输出句子的长度是`22`。
- en: Training the machine translator
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练机器翻译器
- en: So far, we've seen the steps to preprocess the corpora, but not the model used.
    The model is actually already available on the TensorFlow Models repository, freely
    downloadable from [https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py](https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py.).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了预处理语料库的步骤，但尚未看到使用的模型。实际上，模型已经可以在 TensorFlow Models 仓库中找到，可以从 [https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py](https://github.com/tensorflow/models/blob/master/tutorials/rnn/translate/seq2seq_model.py)
    免费下载。
- en: 'The piece of code is licensed with Apache 2.0\. We really thank the authors
    for having open sourced such a great model. Copyright 2015 The TensorFlow Authors.
    All Rights Reserved. Licensed under the Apache License, Version 2.0 (the License);
    You may not use this file except in compliance with the License. You may obtain
    a copy of the License at: [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)
    Unless required by applicable law or agreed to in writing, software. Distributed
    under the License is distributed on an AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS
    OF ANY KIND, either express or implied. See the License for the specific language
    governing permissions and limitations under the License.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码采用 Apache 2.0 许可证。我们非常感谢作者开源了如此出色的模型。版权所有 2015 TensorFlow 作者。保留所有权利。根据 Apache
    许可证第 2.0 版（许可证）授权；除非符合该许可证，否则不得使用此文件。你可以在此处获得许可证副本：[http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)
    除非适用法律要求或书面同意，否则按“原样”分发软件，且不提供任何形式的保证或条件。有关许可证下特定权限和限制的语言，请参见许可证。
- en: 'We will see the usage of the model throughout this section. First, let''s create
    a new file named `train_translator.py` and put in some imports and some constants.
    We will save the dictionary in the `/tmp/` directory, as well as the model and
    its checkpoints:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中看到模型的使用。首先，让我们创建一个名为 `train_translator.py` 的新文件，并导入一些库和常量。我们将把字典保存在 `/tmp/`
    目录下，以及模型和它的检查点：
- en: '[PRE27]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, let''s use all the tools created in the previous section within a function
    that, given a Boolean flag, returns the corpora. More specifically, if the argument
    is `False`, it builds the dictionary from scratch (and saves it); otherwise, it
    uses the dictionary available in the path:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在一个函数中使用前面部分创建的所有工具，给定一个布尔标志返回语料库。更具体地说，如果参数是`False`，则从头开始构建字典（并保存）；否则，它使用路径中现有的字典：
- en: '[PRE28]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This function returns the cleaned sentences, the dataset, the maximum length
    of the sentences, and the lengths of the dictionaries.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数返回清理后的句子、数据集、句子的最大长度以及字典的长度。
- en: 'Also, we need to have a function to clean up the model. Every time we run the
    training routine we need to clean up the model directory, as we haven''t provided
    any garbage information. We can do this with a very simple function:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要一个清理模型的函数。每次运行训练例程时，我们需要清理模型目录，因为我们没有提供任何垃圾信息。我们可以通过一个非常简单的函数来实现这一点：
- en: '[PRE29]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, let''s create the model in a reusable fashion:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们以可重用的方式创建模型：
- en: '[PRE30]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This function calls the constructor of the model, passing the following parameters:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数调用模型的构造函数，传递以下参数：
- en: The source vocabulary size (German, in our example)
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源语言词汇大小（在我们的示例中是德语）
- en: The target vocabulary size (English, in our example)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标词汇大小（在我们的示例中是英语）
- en: The buckets (in our example is just one, since we padded all the sequences to
    a single size)
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 桶（在我们的示例中只有一个，因为我们已将所有序列填充为单一大小）
- en: The **long short-term memory** (**LSTM**) internal units size
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）内部单元的大小'
- en: The number of stacked LSTM layers
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠的 LSTM 层数
- en: The maximum norm of the gradient (for gradient clipping)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度的最大范数（用于梯度裁剪）
- en: The mini-batch size (that is, how many observations for each training step)
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小批量大小（即每个训练步骤的观察次数）
- en: The learning rate
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率
- en: The learning rate decay factor
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率衰减因子
- en: The direction of the model
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的方向
- en: The type of data (in our example, we will use flat16, that is, float using 2
    bytes)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据类型（在我们的示例中，我们将使用 flat16，即使用 2 个字节的浮动类型）
- en: To make the training faster and obtain a model with good performance, we have
    already set the values in the code; feel free to change them and see how it performs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速训练并获得良好的模型表现，我们已经在代码中设置了这些值；你可以自由更改它们并查看效果。
- en: The final if/else in the function retrieves the model, from its checkpoint,
    if the model already exists. In fact, this function will be used in the decoder
    too to retrieve and model on the test set.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 函数中的最终 if/else 语句会从检查点中检索模型（如果模型已经存在）。事实上，这个函数也会在解码器中使用，以在测试集上检索并处理模型。
- en: 'Finally, we have reached the function to train the machine translator. Here
    it is:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们达到了训练机器翻译器的函数。它是这样的：
- en: '[PRE31]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The function starts by creating the model. Also, it sets some constants on
    the steps per checkpoints and the maximum number of steps. Specifically, in the
    code, we will save a model every 100 steps and we will perform no more than 20,000
    steps. If it still takes too long, feel free to kill the program: every checkpoint
    contains a trained model, and the decoder will use the most updated one.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数通过创建模型开始。此外，它设置了一些常量，用于确定每个检查点的步骤数和最大步骤数。具体来说，在代码中，我们将在每 100 步保存一次模型，并且最多执行
    20,000 步。如果这仍然需要太长时间，可以随时终止程序：每个检查点都包含一个训练好的模型，解码器将使用最新的模型。
- en: At this point, we enter the while loop. For each step, we ask the model to get
    a minibatch of data (of size 64, as set previously). The method `get_batch` returns
    the inputs (that is, the source sequence), the outputs (that is, the destination
    sequence), and the weights of the model. With the method `step`, we run one step
    of the training. One piece of information returned is the loss for the current
    minibatch of data. That's all the training!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，我们进入了 while 循环。每一步，我们要求模型获取一个大小为 64 的小批量数据（如之前设置的）。`get_batch` 方法返回输入（即源序列）、输出（即目标序列）和模型的权重。通过
    `step` 方法，我们执行一步训练。返回的信息之一是当前小批量数据的损失值。这就是所有的训练！
- en: 'To report the performance and store the model every 100 steps, we print the
    average perplexity of the model (the lower, the better) on the 100 previous steps,
    and we save the checkpoint. The perplexity is a metric connected to the uncertainty
    of the predictions: the more confident we''re about the tokens, the lower will
    be the perplexity of the output sentence. Also, we reset the counters and we extract
    the same metric from a single minibatch of the test set (in this case, it''s a
    random minibatch of the dataset), and performances of it are printed too. Then,
    the training process restarts again.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了每 100 步报告性能并保存模型，我们会打印模型在过去 100 步中的平均困惑度（数值越低越好），并保存检查点。困惑度是与预测不确定性相关的指标：我们对单词的信心越强，输出句子的困惑度就越低。此外，我们重置计数器，并从测试集中的一个小批量数据（在这个案例中是数据集中的一个随机小批量）中提取相同的指标，并打印其性能。然后，训练过程会重新开始。
- en: As an improvement, every 100 steps we also reduce the learning rate by a factor.
    In this case, we multiply it by 0.99\. This helps the convergence and the stability
    of the training.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种改进，每 100 步我们还会将学习率降低一个因子。在这种情况下，我们将其乘以 0.99。这有助于训练的收敛性和稳定性。
- en: 'We now have to connect all the functions together. In order to create a script
    that can be called by the command line but is also used by other scripts to import
    functions, we can create a `main`, as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要将所有函数连接在一起。为了创建一个可以通过命令行调用的脚本，同时也可以被其他脚本导入函数，我们可以创建一个 `main` 函数，如下所示：
- en: '[PRE32]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the console, you can now train your machine translator system with a very
    simple command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中，你现在可以使用非常简单的命令来训练你的机器翻译系统：
- en: '[PRE33]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'On an average laptop, without an NVIDIA GPU, it takes more than a day to reach
    a perplexity below 10 (12+ hours). This is the output:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在一台普通的笔记本电脑上，没有 NVIDIA GPU，困惑度降到 10 以下需要一天多的时间（12 个小时以上）。这是输出：
- en: '[PRE34]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Test and translate
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试并翻译
- en: The code for the translation is in the file `test_translator.py`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译的代码在文件 `test_translator.py` 中。
- en: 'We start with some imports and the location of the pre-trained model:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一些导入和预训练模型的位置开始：
- en: '[PRE35]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now, let's create a function to decode the output sequence generated by the
    RNN. Mind that the sequence is multidimensional, and each dimension corresponds
    to the probability of that word, therefore we will pick the most likely one. With
    the help of the reverse dictionary, we can then figure out what was the actual
    word. Finally, we will trim the markings (padding, start, end of string) and print
    the output.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个函数来解码 RNN 生成的输出序列。请注意，序列是多维的，每个维度对应于该单词的概率，因此我们将选择最可能的单词。在反向字典的帮助下，我们可以找出实际的单词是什么。最后，我们将修剪掉标记（填充、开始、结束符号），并打印输出。
- en: 'In this example, we will decode the first five sentences in the training set,
    starting from the raw corpora. Feel free to insert new strings or use different
    corpora:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将解码训练集中的前五个句子，从原始语料库开始。随时可以插入新的字符串或使用不同的语料库：
- en: '[PRE36]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Here, again, we need a `main` to work with the command line, as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们再次需要一个`main`来与命令行配合使用，如下所示：
- en: '[PRE37]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Running the preceding code generates the following output:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码会生成以下输出：
- en: '[PRE38]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: As you can see, the output is mainly correct, although there are still some
    problematic tokens. To mitigate the problem, we'd need a more complex RNN, a longer
    corpora or a more diverse one.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，输出结果主要是正确的，尽管仍然存在一些有问题的标记。为了减轻这个问题，我们需要一个更复杂的RNN、更长的语料库或更多样化的语料库。
- en: Home assignments
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 家庭作业
- en: 'This model is trained and tested on the same dataset; that''s not ideal for
    data science, but it was needed to have a working project. Try to find a longer
    corpora and split it into two pieces, one for training and one for testing:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是在相同的数据集上进行训练和测试的；这对数据科学来说并不理想，但为了有一个可运行的项目，这是必要的。尝试找一个更长的语料库，并将其拆分成两部分，一部分用于训练，另一部分用于测试：
- en: 'Change the settings of the model: how does that impact the performance and
    the training time?'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改模型的设置：这会如何影响性能和训练时间？
- en: Analyze the code in `seq2seq_model.py`. How can you insert the plot of the loss
    in TensorBoard?
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析`seq2seq_model.py`中的代码。如何在TensorBoard中插入损失的图表？
- en: NLTK also contains the French corpora; can you create a system to translate
    them both together?
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLTK还包含法语语料库；你能否创建一个系统，将它们一起翻译？
- en: 'In this chapter we''ve seen how to create a machine translation system based
    on an RNN. We''ve seen how to organize the corpus, how to train it and how to
    test it. In the next chapter, we''ll see another application where RNN can be
    used: chatbots.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经学习了如何基于RNN创建一个机器翻译系统。我们了解了如何组织语料库、如何训练它以及如何测试它。在下一章中，我们将看到RNN的另一个应用：聊天机器人。
