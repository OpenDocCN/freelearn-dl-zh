- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Understanding Autoencoders
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解自编码器
- en: Autoencoders are a type of model that was built mainly to accomplish **representation
    learning**. Representation learning is a type of deep learning task that focuses
    on generating a compact and representative feature to represent any single data
    sample, be it image, text, audio, video, or multimodal data. After going through
    some form of representation learning, a model will be able to map inputs into
    more representable features, which can be used to differentiate itself from other
    sample inputs. The representation obtained will exist in a latent space where
    different input samples will co-exist together. These representations are also
    known as **embeddings**. The applications of autoencoders will be tied closely
    to representation learning applications, and some applications include generating
    predictive features for other subsequent supervised learning objectives, comparing
    and contrasting samples in the wild, and performing effective sample recognition.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种主要用于实现**表示学习**的模型类型。表示学习是一种深度学习任务，专注于生成紧凑且具代表性的特征来表示任何单个数据样本，无论是图像、文本、音频、视频还是多模态数据。经过某种形式的表示学习后，模型将能够将输入映射到更可表示的特征，这些特征可用于区分自身与其他样本输入。所获得的表示将存在于潜在空间中，不同的输入样本将共同存在。这些表示也称为**嵌入**。自编码器的应用将与表示学习应用紧密联系，一些应用包括为其他后续监督学习目标生成预测特征，比较和对比野外样本，以及执行有效的样本识别。
- en: Note that autoencoders are not the only way to execute representation learning.
    The topic of representation learning will be discussed further in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125),
    *Exploring Supervised Deep Learning*, and [*Chapter 9*](B18187_09.xhtml#_idTextAnchor149),
    *Exploring Unsupervised* *Deep Learning*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，自编码器并不是执行表示学习的唯一方式。表示学习的主题将在[*第8章*](B18187_08.xhtml#_idTextAnchor125)，*探索监督深度学习*，以及[*第9章*](B18187_09.xhtml#_idTextAnchor149)，*探索无监督深度学习*中进一步讨论。
- en: Now, we know that an autoencoder learns to generate distinctive representations
    or, in other words, embeddings. But what’s the architecture like? Let’s discover
    a standard form of the architecture and then discover a couple more useful advancements.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道自编码器学习生成独特的表示或嵌入。但架构是什么样的？让我们先了解标准架构，然后再发现一些更有用的进展。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，将涵盖以下主题：
- en: Decoding the standard autoencoder
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码标准自编码器
- en: Exploring autoencoder variations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索自编码器变体
- en: Building a CNN autoencoder
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建 CNN 自编码器
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter includes some practical implementations in the **Python** programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括一些在**Python**编程语言中的实际实现。要完成它，您需要安装以下库的计算机：
- en: '`pandas`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`'
- en: '`Matplotlib`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Matplotlib`'
- en: '`Seaborn`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Seaborn`'
- en: '`scikit-learn`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn`'
- en: '`NumPy`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NumPy`'
- en: '`Keras`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Keras`'
- en: '`PyTorch`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PyTorch`'
- en: 'The code files are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_5](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_5).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 代码文件可以在 GitHub 上找到：[https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_5](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_5).
- en: Decoding the standard autoencoder
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码标准自编码器
- en: Autoencoders are more of a concept than an actual neural network architecture.
    This is due to the fact that they can be based on different base neural network
    layers. When dealing with images, you build CNN autoencoders, and when dealing
    with text, you might want to build RNN autoencoders. When dealing with multimodal
    datasets with images, text, audio, numerical, and categorical data, well, you
    use a combination of different layers as a base. Autoencoders are mainly based
    on three components, called the **encoder**, the **bottleneck layers**, and the
    **decoder**. This is illustrated in *Figure 5**.1*.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器更像是一个概念，而不是一个实际的神经网络架构。这是因为它们可以基于不同的基础神经网络层。处理图像时，您构建CNN自编码器；处理文本时，您可能希望构建RNN自编码器；处理包含图像、文本、音频、数字和分类数据的多模态数据集时，则使用不同层次的组合作为基础。自编码器主要基于三个组件，称为**编码器**、**瓶颈层**和**解码器**。这在*图5**.1*中有所说明。
- en: '![Figure 5.1 – The autoencoder concept](img/B18187_05_001.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 自编码器概念](img/B18187_05_001.jpg)'
- en: Figure 5.1 – The autoencoder concept
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 自编码器概念
- en: The encoder for a standard autoencoder typically takes in high-dimensional data
    and compresses it to an arbitrary scale smaller than the original data dimensions,
    which will result in what is known as a **bottleneck representation**, where it
    ties itself to the bottleneck, signifying a compact representation without any
    useless information. The bottleneck component then gets passed into the decoder,
    where it will expand the dimensionality using the exact opposite of the scale
    used by the encoder, resulting in output with the same dimensions as the input
    data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 标准自编码器的编码器通常接收高维数据，并将其压缩为比原始数据维度更小的任意尺度，这样就会生成所谓的**瓶颈表示**，瓶颈表示紧密联系着瓶颈部分，意味着它具有一个紧凑的表示，并不包含任何无用的信息。瓶颈部分随后会传递给解码器，解码器使用与编码器所用尺度完全相反的方式来扩展维度，最终生成与输入数据相同维度的输出数据。
- en: Note
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Encoder and decoder structures are not exclusive to autoencoders but are also
    utilized in other architectures, such as transformers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器结构并非仅限于自编码器，也被广泛应用于其他架构中，例如变换器（transformers）。
- en: The difference is the bottleneck component that holds the representative features.
    It is commonly compressed and in smaller dimensions, but sometimes, it can be
    made larger to hold more representative features for predictive power.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 区别在于瓶颈部分，它承载着代表性特征。瓶颈通常被压缩为较小的维度，但有时也可以增大瓶颈的尺寸，以容纳更多具有预测能力的代表性特征。
- en: Autoencoders, in general, are trained to reconstruct input data. The training
    process of the autoencoder model involves comparing the distance between the generated
    output data and the input data. After being optimized to generate the input data,
    when the model is capable of reconstructing the original input data completely,
    it can be said that the bottleneck has a more compact and summarized representation
    of the input data than the original input data itself. The compact representation
    can then be used subsequently to achieve other tasks, such as sample recognition,
    or can even be used generally to save space, by storing the smaller bottleneck
    feature instead of the original large input data. The encoder and decoder are
    not constrained to a single layer and can be defined with multiple layers. However,
    the standard autoencoder only has a single bottleneck feature. This is also known
    as **code**, or a **latent feature**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器通常是用来重建输入数据的。自编码器模型的训练过程涉及比较生成的输出数据与输入数据之间的距离。在优化过程中，模型能够生成接近输入数据的输出，当模型能够完全重建原始输入数据时，可以认为瓶颈部分比原始输入数据本身具有更加紧凑和概括性的表示。该紧凑的表示随后可以用来实现其他任务，例如样本识别，或者甚至可以通过存储较小的瓶颈特征来节省空间，而不是存储原始的庞大输入数据。编码器和解码器并不局限于单层结构，可以定义为多层结构。然而，标准自编码器只有一个单一的瓶颈特征，这也被称为**代码**，或**潜在特征**。
- en: Now, let’s explore the different variations of autoencoders.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来探索自编码器的不同变种。
- en: Exploring autoencoder variations
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索自编码器的变种
- en: For tabular data, the network structure can be pretty straightforward. It simply
    uses an MLP with multiple fully connected layers that gradually shrink the number
    of features for the encoder, and multiple fully connected layers that gradually
    increase the data outputs to the same dimension and size as the input for the
    decoder.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于表格数据，网络结构可以非常简单。它仅使用一个包含多个全连接层的多层感知器（MLP），这些层逐渐减少编码器的特征数量，并使用多个全连接层逐渐增加解码器的输出数据，使其与输入数据的维度和大小相同。
- en: For time-series or sequential data, RNN-based autoencoders can be used. One
    of the most cited research projects about RNN-based autoencoders is a version
    where LSTM-based encoders and decoders are used. The research paper is called
    *Sequence to Sequence Learning with Neural Networks* by Ilya Sutskever, Oriol
    Vinyals, and Quoc V. Le ([https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)).
    Instead of stacking encoder LSTMs and decoder LSTMs, using the hidden state output
    sequence of each of the LSTM cells vertically, the decoder layer sequentially
    continues the sequential flow of the encoder LSTM and outputs the reconstructed
    input in reversed order. An additional decoder LSTM layer was also used to concurrently
    optimize, to predict future sequences. This structure is shown in *Figure 5**.2*.
    Note that it is also possible to adapt this to the video image modality using
    raw flattened image pixels as input. The architecture is also called **seq2seq**.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于时间序列或顺序数据，可以使用基于 RNN 的自编码器。关于基于 RNN 的自编码器，最常被引用的研究项目之一是使用基于 LSTM 的编码器和解码器的版本。该研究论文名为
    *Sequence to Sequence Learning with Neural Networks*，作者为 Ilya Sutskever、Oriol
    Vinyals 和 Quoc V. Le（[https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215)）。该研究方法不是堆叠编码器
    LSTM 和解码器 LSTM，而是将每个 LSTM 单元的隐藏状态输出序列垂直使用，解码器层按顺序继续编码器 LSTM 的顺序流，并以反向顺序输出重建的输入。还使用了额外的解码器
    LSTM 层来并行优化，预测未来的序列。这个结构如 *图 5.2* 所示。需要注意的是，这种结构也可以通过使用原始的展平图像像素作为输入，适配到视频图像模态中。该架构也被称为
    **seq2seq**。
- en: '![Figure 5.2 – An LSTM-based autoencoder structure](img/B18187_05_002.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – 基于 LSTM 的自编码器结构](img/B18187_05_002.jpg)'
- en: Figure 5.2 – An LSTM-based autoencoder structure
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – 基于 LSTM 的自编码器结构
- en: The hidden state outputs of the encoder LSTM can be considered to be the latent
    feature of the LSTM autoencoder. Transformers, a new architecture that can also
    deal with sequential data, which will be introduced later in this chapter, have
    some variations that can also be considered a kind of autoencoder – some transformers
    can be autoencoders, but not all transformers are autoencoders.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器 LSTM 的隐藏状态输出可以被认为是 LSTM 自编码器的潜在特征。Transformers 是一种新型架构，也能处理顺序数据，后面将在本章介绍。它有一些变体也可以被视为一种自编码器——一些
    transformers 可以是自编码器，但并非所有 transformers 都是自编码器。
- en: For image data, by using convolutional layers, we can scale down the features
    gradually with multiple convolutional and pooling layers, until the stage where
    a global pooling layer is applied and the data becomes a 1-dimensional feature.
    This represents the encoder of the autoencoder generating the bottleneck feature.
    This workflow is the same as the one we discussed in the previous section on CNN.
    However, for the decoder, to scale up the 1-dimensional pooled feature into 2-dimensional
    image-like data again, a special form of convolution is needed, called the **transpose
    convolution**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像数据，通过使用卷积层，我们可以通过多层卷积和池化层逐渐缩小特征，直到应用全局池化层并将数据转化为 1 维特征。这代表了自编码器的编码器生成瓶颈特征。这一工作流与我们在前一节中讨论的
    CNN 相同。然而，对于解码器来说，为了将 1 维池化特征重新放大为 2 维的类似图像数据，需要一种特殊形式的卷积，称为 **反卷积**。
- en: The variations mentioned in this topic are all about using the standard autoencoder
    structures but implementing them using different neural network types. There are
    also two additional variations of the autoencoder, where one variation is based
    on data input manipulation, and another variation is based on an actual modification
    of the autoencoder structure to achieve the data generation goal.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提到的变体都是关于使用标准的自编码器结构，但通过不同的神经网络类型来实现它们。还有两种额外的自编码器变体，其中一种变体基于数据输入的操作，另一种变体则是通过实际修改自编码器结构来实现数据生成目标。
- en: For the variation used for data manipulation, the idea is to add noise to the
    input data during training and maintain the original input data without the added
    noise, to be used as the target to predictively reconstruct the data. This variation
    of the autoencoder is called a **denoising autoencoder** because its goal is primarily
    to *denoise* the data. Since the goal has shifted from compressing the data to
    denoising the data, the bottleneck features are not restricted to being small
    in size. The features that are utilized afterward are not constrained to just
    the single bottleneck feature but, instead, can be the features from multiple
    intermediate layers in the network, or just simply the denoised reconstructed
    output. This method takes advantage of the innate capability of neural networks
    to perform automated feature engineering. The most notable usage of a denoising
    autoencoder is in the first-place solution of a Kaggle competition based on tabular
    data, hosted by Porto Seguro, an insurance company, where multiple features from
    intermediate layers were fed into a separate MLP to predict whether a driver will
    file an insurance claim in the future ([https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于用于数据处理的变体，其思想是在训练过程中向输入数据添加噪声，并保持原始输入数据不受噪声影响，作为目标用于预测性地重建数据。该自编码器的这种变体被称为**去噪自编码器**，因为其主要目标是*去噪*数据。由于目标已经从压缩数据转变为去噪数据，因此瓶颈特征不再限制于小尺寸。后续利用的特征不再仅仅局限于单一瓶颈特征，而是可以是网络中多个中间层的特征，或者简单地是去噪后的重建输出。该方法利用了神经网络执行自动特征工程的固有能力。去噪自编码器最著名的应用是在一个由葡萄牙保险公司Porto
    Seguro主办的基于表格数据的Kaggle竞赛中获得第一名的解决方案，在这个竞赛中，来自多个中间层的特征被输入到一个单独的多层感知机（MLP）中，以预测驾驶员未来是否会提出保险索赔（[https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629)）。
- en: For the variation that uses a modification of the autoencoder structure, the
    idea is to produce two bottleneck feature vectors that represent a list of standard
    deviation values and mean values so that different bottleneck feature values can
    be sampled, based on the mean and standard deviation values. These sampled bottleneck
    feature values can then be passed into the decoder to generate new random data
    outputs. This variation of the autoencoder is called a **variational autoencoder**.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用自编码器结构修改的变体，其思想是生成两个瓶颈特征向量，分别表示标准差值和均值列表，以便根据均值和标准差值对不同的瓶颈特征值进行采样。这些采样的瓶颈特征值可以传递到解码器中，生成新的随机数据输出。该自编码器的变体被称为**变分自编码器**。
- en: Now that we have covered a good overview of autoencoder variations, let’s dig
    further into CNN autoencoders and build a CNN autoencoder using deep learning
    libraries.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对自编码器的变体有了一个较好的概述，接下来让我们深入研究CNN自编码器，并使用深度学习库构建一个CNN自编码器。
- en: Building a CNN autoencoder
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建CNN自编码器
- en: Let’s start by going through what a **transpose convolution** is. *Figure 5**.3*
    shows an example transpose convolution operation on a 2x2 sized input with a 2x2
    sized convolutional filter, with a stride of 1.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来了解一下什么是**转置卷积**。*图 5.3* 展示了一个在 2x2 大小输入上使用 2x2 大小卷积滤波器，步幅为 1 的转置卷积操作示例。
- en: '![Figure 5.3 – A transposed convolutional filter operation](img/B18187_05_003.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 转置卷积滤波操作](img/B18187_05_003.jpg)'
- en: Figure 5.3 – A transposed convolutional filter operation
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 转置卷积滤波操作
- en: In *Figure 5**.3*, note that each of the 2x2 input data is marked with a number
    from **1** to **4**. These numbers are used to map the output results, presented
    as 3x3 outputs. The convolutional kernel applies each of its weights individually
    to every value in the input data in a sliding window manner, and the outputs from
    the four convolutional operations are presented in the bottom part of the figure.
    After the operation is done, each of the outputs will be elementwise added to
    form the final output and subjected to a bias. This example process depicts how
    a 2x2 input can be scaled up to a 3x3 data size without relying completely on
    padding.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图5.3*中，注意每个2x2的输入数据都标有从**1**到**4**的数字。这些数字用于映射输出结果，输出结果呈现为3x3的形式。卷积核会以滑动窗口的方式将每个权重分别应用到输入数据中的每一个值，四次卷积操作的输出结果会在图的下方呈现。操作完成后，每个输出结果将逐元素相加，形成最终的输出并加上偏置。这个示例过程展示了如何将一个2x2的输入扩展到3x3的输出，而无需完全依赖填充。
- en: 'Let’s implement a convolutional autoencoder model in `Pytorch` below and train
    it on the `Fashion MNIST` image dataset, an image dataset comprising fashion items
    such as shoes, bags, and clothes:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下面实现一个卷积自编码器模型，并在`Fashion MNIST`图像数据集上进行训练。这个数据集包含了鞋子、包包、衣服等时尚商品的图像：
- en: 'Let’s start by importing the necessary libraries:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们开始导入必要的库：
- en: '[PRE0]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will define the overall convolutional autoencoder structure:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义整个卷积自编码器的结构：
- en: '[PRE1]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The code presented here is a convolutional autoencoder structure in `PyTorch`
    with an encoder and decoder variable placeholder. The encoder is responsible for
    taking in an image and reducing its dimensionality until it has a single dimension
    with a small representation footprint – the bottleneck feature. The decoder will
    then take in the bottleneck feature and produce a feature map of the same size
    as the original input image. The encoder and decoder will be defined in the next
    two steps.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里展示的代码是一个基于`PyTorch`的卷积自编码器结构，包含编码器和解码器的变量占位符。编码器负责接收一张图片并逐步减少其维度，直到只剩下一个具有小表示足迹的单维度——瓶颈特征。解码器接着会接收该瓶颈特征，并生成与原始输入图像大小相同的特征图。编码器和解码器将在接下来的两步中定义。
- en: 'The encoder will be designed to take in a grayscale image (one channel) of
    size 28x28\. This image dimension is the default size of the Fashion MNIST image
    dataset. The following logic shows the code to define the encoder, replacing the
    placeholder defined in *step 2*:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码器将被设计为接收一个28x28的灰度图像（单通道）。这个图像尺寸是Fashion MNIST数据集的默认大小。以下逻辑展示了定义编码器的代码，替换掉*步骤2*中定义的占位符：
- en: '[PRE2]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The defined encoder has two convolutional layers, each followed by the non-linear
    activation, `ReLU`, and a pooling layer. The filter sizes of the convolutional
    layers are `16` and `4` The second pooling layer is a global average pooling layer
    meant to reduce the 4x9x9 feature map to 4x1x1, where each channel will have only
    one value to represent itself. This means that the encoder will squeeze the dimensionality
    of the original 28x28 image, which adds up to around 784 pixels for only four
    features, which is a 99.4% compression rate!
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义的编码器有两个卷积层，每个卷积层后面跟着非线性激活函数`ReLU`和一个池化层。卷积层的滤波器大小分别为`16`和`4`。第二个池化层是一个全局平均池化层，用于将4x9x9的特征图减少到4x1x1，每个通道只有一个值来表示自己。这意味着编码器将压缩原始28x28图像的维度，原图的784个像素仅保留四个特征，压缩率高达99.4%！
- en: 'The decoder will then take these four features per image and produce an output
    feature map of 28x28 again, reproducing the original image size. The entire model
    has no padding applied. The decoder will be defined as follows, replacing the
    placeholder decoder defined in *step 2*:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码器将接收每张图像的这四个特征，并再次生成28x28的输出特征图，重建原始的图像大小。整个模型没有应用填充。解码器将被如下定义，替换掉在*步骤2*中定义的占位符解码器：
- en: '[PRE3]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Three convolutional transpose layers are used here. Each convolutional layer
    is followed by a non-linear activation layer, where the first two layers used
    `ReLU` (being the standard non-linear activation) and the last layer used `sigmoid`.
    `sigmoid` is used here, as the `fashion MNIST` data is already normalized to have
    values between `0` and `1`. The convolutional transpose layer defined here adopts
    a similar number of filter configurations from the encoder, from `16` to `4` and
    finally, to `1` filter to produce only one channel grayscale image.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里使用了三层卷积转置层。每个卷积层后面都有一个非线性激活层，前两层使用了`ReLU`（作为标准的非线性激活函数），最后一层使用了`sigmoid`。使用`sigmoid`是因为`fashion
    MNIST`数据集已经被归一化，使得其值介于`0`和`1`之间。这里定义的卷积转置层采用了与编码器类似的滤波器配置，从`16`个滤波器到`4`个，最终使用`1`个滤波器生成单通道灰度图像。
- en: 'Now that we have defined the convolutional autoencoder, let’s load up the fashion
    MNIST data from the `torchvision` library. This tutorial will use the `Catalyst`
    library for ease of training, so let’s take the `fashion MNIST` dataset loader
    and feeder class from `torchvision` and modify it for usage in the `Catalyst`
    library:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了卷积自编码器，接下来让我们从`torchvision`库加载时尚MNIST数据集。本教程将使用`Catalyst`库来简化训练过程，因此我们将从`torchvision`中获取`fashion
    MNIST`数据集加载器和喂入器类，并对其进行修改，以便在`Catalyst`库中使用：
- en: '[PRE4]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The class from `torchvision` already has the necessary logic to download and
    load the `fashion MNIST` dataset. However, the data feeder method, `getitem`,
    is not in the expected format for image generation and, thus, requires this modification
    for this experiment to work.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 来自`torchvision`的类已经具备下载和加载`fashion MNIST`数据集的必要逻辑。然而，数据喂入方法`getitem`的格式不符合图像生成的预期格式，因此需要进行修改，以使本实验能够正常运行。
- en: 'Note that the `Pillow` library is used to load the image in *step 5*. This
    is so that we can easily use the tool from `torchvision` to perform different
    transformation steps, such as image augmentation. However, in this experiment,
    we will directly convert the `pillow` image into `Pytorch` tensors, using the
    transform logic that follows:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，`Pillow`库用于在*第5步*中加载图像。这是为了方便我们使用`torchvision`中的工具执行不同的转换步骤，例如图像增强。然而，在本实验中，我们将直接将`pillow`图像转换为`Pytorch`张量，使用如下的转换逻辑：
- en: '[PRE5]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, let’s load the training and validation datasets of `fashion MNIST`:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们加载`fashion MNIST`的训练和验证数据集：
- en: '[PRE6]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding code downloads the dataset into the `fashion_mnist` folder if
    it doesn’t already exist. Additionally, the `loaders` variable is here so that
    it can be consumed by the `Catalyst` library.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的代码会将数据集下载到`fashion_mnist`文件夹中（如果该文件夹不存在）。此外，`loaders`变量也在这里定义，以便可以被`Catalyst`库使用。
- en: 'Since the optimization goal is to reduce the different of reproduced pixel
    values compared to the target pixel values, we will use the mean squared error
    as the reconstruction loss here:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于优化目标是减少重建像素值与目标像素值之间的差异，因此我们将在这里使用均方误差作为重建损失：
- en: '[PRE7]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'It’s important to note that while reconstruction loss is a common objective
    in unsupervised representation learning, there may be other metrics or objectives
    used, depending on the specific algorithm or approach. For example, in `Catalyst`
    so that we can train our model:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要注意的是，尽管重建损失是无监督表示学习中的常见目标，但根据具体的算法或方法，可能会使用其他的指标或目标。例如，在`Catalyst`中，我们可以用来训练我们的模型：
- en: '[PRE8]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we will define a generally usable function that can make it easy to perform
    multiple training and validation experiments through code:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个通用的函数，使得通过代码执行多个训练和验证实验变得更加简单：
- en: '[PRE9]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: These are just basic training and evaluation boiler code without any tricks
    applied. We will explore in depth the tricks to train supervised models in [*Chapter
    8*](B18187_08.xhtml#_idTextAnchor125), *Exploring Supervised* *Deep Learning*.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些只是基本的训练和评估模板代码，没有应用任何技巧。我们将在[*第8章*](B18187_08.xhtml#_idTextAnchor125)中深入探讨训练监督模型的技巧，*探索监督学习*
    *深度学习*。
- en: 'Now, we are ready to train and evaluate the CNN autoencoder model through the
    following logic:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备通过以下逻辑来训练和评估CNN自编码器模型：
- en: '[PRE10]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The best performing `cnn_autoencoder` weights based on the validation loss will
    be automatically loaded after training on 20 epochs is complete.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于验证损失，表现最好的`cnn_autoencoder`权重将在训练完成20个epoch后自动加载。
- en: 'After training the preceding model with the provided training code, on the
    Fashion MNIST dataset with 1x28x28 image dimensions, you should get something
    similar to the example input/output pairs shown in *Figure 5**.4* through the
    following code:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用提供的训练代码训练前述模型后，在 Fashion MNIST 数据集（图像尺寸为 1x28x28）上，您应该会得到类似于 *图 5.4* 中所示的示例输入/输出对，通过以下代码：
- en: '[PRE11]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Figure 5.4 – Autoencoder sample results on the Fashion MNIST dataset; the
    bottom represents the original image, and the top represents the reproduced image](img/B18187_05_004.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – Autoencoder 在 Fashion MNIST 数据集上的示例结果；底部表示原始图像，顶部表示重建图像](img/B18187_05_004.jpg)'
- en: Figure 5.4 – Autoencoder sample results on the Fashion MNIST dataset; the bottom
    represents the original image, and the top represents the reproduced image
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – Autoencoder 在 Fashion MNIST 数据集上的示例结果；底部表示原始图像，顶部表示重建图像
- en: With the results, it’s clear that the bottleneck features are capable of reproducing
    the entire picture somewhat completely, and they can be treated as more representative
    and compact features for use instead of the original input data. Increasing the
    number of representation values of the bottleneck feature from 4 to something
    like 10 features should also increase the quality of the reproducible image. Feel
    free to try it out and experiment with the parameters!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果来看，可以清楚地看出瓶颈特征能够相对完整地重建整个图像，它们可以作为更具代表性和紧凑的特征，代替原始输入数据使用。将瓶颈特征的表示值从 4 增加到大约
    10 个特征，也应该能提高重建图像的质量。欢迎尝试并实验不同的参数！
- en: Summary
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Autoencoders are considered a fundamental method to achieve representation learning
    across data modalities. Consider the architecture as a shell that you can fit
    in a variety of other neural network components, allowing you to ingest data of
    different modalities or benefit from more advanced neural network components.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Autoencoder 被认为是实现跨数据模态表示学习的基本方法。可以将其架构视为一个框架，您可以在其中嵌入多种神经网络组件，从而允许您处理不同模态的数据或利用更先进的神经网络组件。
- en: However, do note that they are not the only method to learn representative features.
    There are many more applications for autoencoders that primarily revolve around
    different training objectives using the same architecture. Two of these adaptations
    that were briefly introduced in this chapter are denoising autoencoders and variational
    autoencoders, which will be introduced properly in [*Chapter 9*](B18187_09.xhtml#_idTextAnchor149),
    *Exploring Unsupervised Deep Learning*. Now, let’s shift gears again to discover
    the model family of transformers!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，请注意，它们并不是唯一的学习表示特征的方法。Autoencoder 还有许多应用，主要围绕不同的训练目标，使用相同的架构。在本章中简要介绍的两个变体是去噪自编码器和变分自编码器，它们将在
    [*第 9 章*](B18187_09.xhtml#_idTextAnchor149)《探索无监督深度学习》中得到详细介绍。现在，让我们再次转变思路，探索
    Transformer 模型系列！
