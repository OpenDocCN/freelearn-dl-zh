- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Deep Learning Regularization
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习正则化
- en: In this chapter, we will cover several tricks and techniques to regularize neural
    networks. We will reuse the L2 regularization technique, as we did in linear models,
    for example. But there are other techniques not yet presented in this book, such
    as early stopping and dropout, which will be covered in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍几种技巧和方法来正则化神经网络。我们将重用 L2 正则化技术，就像在处理线性模型时一样。但本书中还有其他尚未介绍的技术，比如早停法和
    dropout，这些将在本章中进行讲解。
- en: 'In this chapter, we’ll look at the following recipes:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下食谱：
- en: Regularizing a neural network with L2 regularization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 L2 正则化来正则化神经网络
- en: Regularizing a neural network with early stopping
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用早停法正则化神经网络
- en: Regularization with network architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网络架构进行正则化
- en: Regularizing with dropout
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 dropout 进行正则化
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will train neural networks on various tasks. This will
    require us to use the following libraries:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将训练神经网络来处理各种任务。这将要求我们使用以下库：
- en: NumPy
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: Scikit-learn
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-learn
- en: Matplotlib
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: PyTorch
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch
- en: torchvision
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: torchvision
- en: Regularizing a neural network with L2 regularization
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 L2 正则化来正则化神经网络
- en: Just like a linear model, whether it be a linear regression or a logistic regression,
    neural networks have weights. And so, just like a linear model, L2 penalization
    can be used on those weights to regularize the neural network. In this recipe,
    we will apply L2 penalization to a neural network on the MNIST handwritten digits
    dataset.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 就像线性模型一样，无论是线性回归还是逻辑回归，神经网络都有权重。因此，就像线性模型一样，可以对这些权重使用 L2 惩罚来正则化神经网络。在这个食谱中，我们将在
    MNIST 手写数字数据集上应用 L2 惩罚来正则化神经网络。
- en: As a reminder, when training a neural network on this task in [*Chapter 6*](B19629_06.xhtml#_idTextAnchor162),
    there was a small overfitting after 20 epochs, and the results were an accuracy
    of 97% on the train set and 95% on the test set. Let’s try to reduce this overfitting
    by adding L2 regularization in this recipe.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，当我们在[*第6章*](B19629_06.xhtml#_idTextAnchor162)中训练神经网络时，经过 20 个周期后出现了轻微的过拟合，训练集的准确率为
    97%，测试集的准确率为 95%。让我们通过在本食谱中添加 L2 正则化来减少这种过拟合。
- en: Getting ready
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Just like for linear models, L2 regularization is just adding a new L2 term
    to the loss. Given the weights W=w1,w2,..., the added term to the loss would be
    ![](img/Formula_07_001.png). The consequence of this added term to the loss is
    that the weights are more constrained and must stay close to zero to keep the
    loss small. As a result, it adds bias to the model and then can help regularize
    it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 就像线性模型一样，L2 正则化只是向损失函数中添加一个新的 L2 项。给定权重 W=w1,w2,...，添加到损失函数中的项为 ![](img/Formula_07_001.png)。这个新增的项对损失函数的影响是，权重会受到更多约束，并且必须保持接近零以保持损失函数较小。因此，它为模型添加了偏差，并帮助进行正则化。
- en: Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This notation for the weights is simplified here. Actually, there are weights
    ![](img/Formula_07_002.png) for each unit `i`, each feature `j`, and each layer
    `l`. But in the end, the L2 term remains the sum of all the squared weights.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的权重表示法进行了简化。实际上，每个单元 `i`、每个特征 `j` 和每一层 `l` 都有权重 ![](img/Formula_07_002.png)。但最终，L2
    项仍然是所有权重平方的总和。
- en: 'For this recipe, only three libraries are needed:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，只需要三种库：
- en: '`matplotlib` for plots'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib` 用于绘制图表'
- en: '`pytorch` for deep learning'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytorch` 用于深度学习'
- en: '`torchvision` for the MNIST dataset'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torchvision` 用于 MNIST 数据集'
- en: These can be installed with `pip install matplotlib` `torch torchvision`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `pip install matplotlib` `torch torchvision` 安装这些库。
- en: How to do it...
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: In this recipe, we reuse the exact same code as in the previous chapter when
    training a multiclass classification model on the MNIST dataset. The only difference
    will be at *step 6* – feel free to jump there if needed.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们重新使用了上一章中训练多类分类模型时的相同代码，数据集仍然是 MNIST。唯一的区别将在于 *第 6 步* ——如果需要，可以直接跳到这一步。
- en: 'The input data is the MNIST handwritten dataset: grayscale images of 28x28
    pixels. The data will thus need to be rescaled and flattened before being able
    to train a custom neural network:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据是 MNIST 手写数据集：28x28 像素的灰度图像。因此，在能够训练自定义神经网络之前，数据需要进行重新缩放和展平处理：
- en: 'Import the required libraries. As in previous recipes, we import several useful
    `torch` modules and functions:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。像以前的食谱一样，我们导入了几个有用的 `torch` 模块和函数：
- en: '`torch`'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`'
- en: '`torch.nn` containing required classes for building a neural network'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn` 包含构建神经网络所需的类'
- en: '`torch.nn.functional` for activation functions such as ReLU'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch.nn.functional`用于激活函数，如ReLU：'
- en: '`DataLoader` for handling the data'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于处理数据的`DataLoader`：
- en: 'And we have some imports from `torchvision`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还从`torchvision`中导入了一些模块：
- en: '`MNIST` for loading the dataset'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MNIST`用于加载数据集：'
- en: '`transforms` for transforming the dataset – both rescaling and flattening the
    data:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于数据集转换的`transforms`——既包括缩放也包括扁平化数据：
- en: '[PRE0]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Instantiate the transformations. The `Compose` class is used here to compose
    three transformations:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化转换。此处使用`Compose`类来组合三个转换：
- en: '`transforms.ToTensor()`: Convert the input image in to `torch.Tensor` format'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transforms.ToTensor()`：将输入图像转换为`torch.Tensor`格式'
- en: '`transforms.Normalize()`: Normalize the image with the mean value and standard
    deviation. Will subtract the mean (i.e., `0.1307`) and then divide it by the standard
    deviation (i.e., `0.3081`) for each pixel value.'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transforms.Normalize()`: 使用均值和标准差对图像进行归一化处理。会先减去均值（即`0.1307`），然后将每个像素值除以标准差（即`0.3081`）。'
- en: '`transforms.Lambda(torch.flatten)`: Flatten the 2D tensor in to a 1D tensor:'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transforms.Lambda(torch.flatten)`：将2D张量展平为1D张量：'
- en: 'Here is the code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: Images are commonly normalized with a mean and standard deviation of 0.5\. We
    normalize with those specific values because the dataset is made with specific
    images, but 0.5 would work fine too.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图像通常使用均值和标准差为0.5进行归一化。我们使用这些特定的值进行归一化，因为数据集是由特定的图像构成的，但0.5也可以很好地工作。
- en: 'Load the train and test sets, as well as the train and test data loaders. Using
    the `MNIST` class, we both get the train and test sets using the `train=True`
    and `train=False` parameters, respectively. We apply the previously defined transformations
    directly while loading the data with the `MNIST` class too. Then we instantiate
    the data loaders with a batch size of `64`:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载训练集和测试集，以及训练和测试数据加载器。使用`MNIST`类，我们分别通过`train=True`和`train=False`参数获取训练集和测试集。在加载数据时，我们还直接应用之前定义的转换。然后，使用批量大小为`64`实例化数据加载器：
- en: '[PRE8]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Define the neural network. We define here, by default, a neural network made
    of 2 hidden layers of 24 units. The output layer has 10 units since there are
    10 classes (digits between 0 and 9). Finally, the `softmax` function is applied
    to the output layer, allowing the sum of the 10 units to be strictly equal to
    `1`:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经网络。这里默认定义了一个包含2个隐藏层（每层24个神经元）的神经网络。输出层包含10个神经元，因为有10个类别（数字从0到9）。最后，对输出层应用`softmax`函数，使得10个单元的和严格等于`1`：
- en: '[PRE16]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To check the code, we instantiate the model with the right input shape of `784`
    (28x28 pixels) and check the forward propagation works properly on a given random
    tensor:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了检查代码，我们实例化模型，并使用正确的输入形状`784`（28x28像素），确保在给定的随机张量上正向传播正常工作：
- en: '[PRE34]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The code output would be something like the following (only the sum must be
    equal to 1; other numbers may be different):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出将类似如下（只有总和必须等于1；其他数字可能不同）：
- en: '[PRE41]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Define the loss function as the cross-entropy loss, available as `nn.CrossEntropyLoss()`
    in `pytorch`, and the optimizer as `Adam`. Here we set another parameter to the
    `Adam` optimizer: `weight_decay=0.001`. This parameter is the strength of the
    L2 penalization. By default, `weight_decay` is `0`, meaning there is no L2 penalization.
    A higher value means a higher regularization, just like in linear models in scikit-learn:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数为交叉熵损失，使用`pytorch`中的`nn.CrossEntropyLoss()`，并将优化器定义为`Adam`。在这里，我们给`Adam`优化器设置另一个参数：`weight_decay=0.001`。该参数是L2惩罚的强度。默认情况下，`weight_decay`为`0`，表示没有L2惩罚。较高的值意味着更强的正则化，就像在scikit-learn中的线性模型一样：
- en: '[PRE42]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Instantiate the `epoch_step` helper function allowing to compute forward and
    backward propagation (for the training set only) as well as the loss and accuracy:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化`epoch_step`辅助函数，用于计算正向传播和反向传播（仅限于训练集），以及损失和准确率：
- en: '[PRE45]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: We can finally train the neural network on 20 epochs and compute the loss and
    accuracy for each epoch.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们最终可以在20个epoch上训练神经网络，并计算每个epoch的损失和准确率。
- en: 'Since we both train on the train set and evaluate on the test set, the model
    is switched to `train` mode with `model.train()` before training, whereas before
    evaluating on the test set, it is switched to `eval` mode with `model.eval()`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在训练集上进行训练，并在测试集上进行评估，模型在训练前会切换到`train`模式（`model.train()`），而在评估测试集之前，则切换到`eval`模式（`model.eval()`）：
- en: '[PRE66]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'On the last epoch, the output should look like the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个epoch，输出应该如下所示：
- en: '[PRE67]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'For visualization purposes, we can plot the loss for both the train and test
    sets as a function of the epoch:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了可视化，我们可以绘制训练集和测试集的损失随epoch的变化：
- en: '[PRE68]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Here is the plot for it:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是它的图示：
- en: '![Figure 7.1 – Cross-entropy loss as a function of the epoch; output from the
    previous code](img/B19629_07_01.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 交叉熵损失随epoch的变化；来自前面的代码输出](img/B19629_07_01.jpg)'
- en: Figure 7.1 – Cross-entropy loss as a function of the epoch; output from the
    previous code
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 交叉熵损失随epoch的变化；来自前面的代码输出
- en: We can notice that the loss seems to be almost the same for both the training
    and test set, with no clear divergence. In the previous attempts without L2 penalization,
    the losses were further apart from each other, meaning we effectively regularized
    the model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以注意到，训练集和测试集的损失几乎相同，没有明显的偏离。而在没有L2惩罚的前几次尝试中，损失相差较大，这意味着我们有效地对模型进行了正则化。
- en: 'Showing related results, we can do it with accuracy too:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示相关结果，我们也可以用准确率来表示：
- en: '[PRE74]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Here is the plot for it:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是它的图示：
- en: '![Figure 7.2 – Accuracy as a function of the epoch; output from the previous
    code](img/B19629_07_02.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 准确率随epoch的变化；来自前面的代码输出](img/B19629_07_02.jpg)'
- en: Figure 7.2 – Accuracy as a function of the epoch; output from the previous code
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 准确率随epoch的变化；来自前面的代码输出
- en: At the end, the accuracy is about 96% for both the train set and the test set,
    with no significant overfitting.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，训练集和测试集的准确率都约为96%，且没有明显的过拟合。
- en: There’s more...
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: Even if L2 regularization is a quite common technique to regularize linear models
    such as linear regression and logistic regression, it is not usually the first
    choice with deep learning. Other methods such as early stopping or dropout are
    usually preferred.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 即使L2正则化是正则化线性模型（如线性回归和逻辑回归）中非常常见的技术，它通常不是深度学习中的首选方法。其他方法，如早停或丢弃法，通常更受青睐。
- en: On another note, in this recipe, we keep mentioning only the train and test
    sets. But to optimize the `weight_decay` hyperparameter properly, it is required
    to use a validation set; otherwise, the results will be biased. We have simplified
    this recipe by having only two sets to keep it concise.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，在这个示例中，我们只提到了训练集和测试集。但为了正确优化`weight_decay`超参数，需要使用验证集；否则，结果会产生偏差。为了简洁起见，我们简化了这个示例，只用了两个数据集。
- en: Note
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Generally speaking, in deep learning, any other hyperparameter optimization,
    such as the number of layers, number of units, activation functions, and so on
    must be optimized for the validation set too, not just for the test set.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，在深度学习中，任何其他的超参数优化，如层数、单元数、激活函数等，必须针对验证集进行优化，而不仅仅是测试集。
- en: See also
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: It may seem strange to adjust the L2 penalization through the optimizer of the
    model rather than directly in the loss function, and indeed it is.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过模型的优化器调整L2惩罚，而不是直接在损失函数中进行调整，可能看起来有些奇怪，实际上也是如此。
- en: 'Of course, it would be possible to manually add an L2 penalization, but it
    would probably be suboptimal. See this PyTorch thread for more about this design
    choice, as well as an example of adding L1 penalization: [https://discuss.pytorch.org/t/simple-l2-regularization/139](https://discuss.pytorch.org/t/simple-l2-regularization/139).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，也可以手动添加L2惩罚，但这可能不是最优选择。请查看这个PyTorch讨论帖，了解更多关于这个设计选择的信息，以及如何添加L1惩罚的示例：[https://discuss.pytorch.org/t/simple-l2-regularization/139](https://discuss.pytorch.org/t/simple-l2-regularization/139)。
- en: Regularizing a neural network with early stopping
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用早停法对神经网络进行正则化
- en: 'Early stopping is a commonly employed approach in deep learning to prevent
    the overfitting of models. The concept is straightforward yet effective: if the
    model is overfitting due to prolonged training epochs, we terminate the training
    prematurely to prevent overfitting. We can utilize this technique on the breast
    cancer dataset.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 早停是深度学习中常用的一种方法，用于防止模型过拟合。这个概念简单而有效：如果模型由于过长的训练周期而发生过拟合，我们就提前终止训练，以避免过拟合。我们可以在乳腺癌数据集上使用这一技术。
- en: Getting ready
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: In a perfect world, there is no need for regularization. What that means is
    that for both the train and validation sets, the losses are almost perfectly equal,
    for any number of epochs, as in *Figure 7**.3*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个完美的世界里，是不需要正则化的。这意味着在训练集和验证集中，无论经过多少个epoch，损失几乎完全相等，如*图7.3*所示。
- en: '![Figure 7.3 – Example with no overfitting of train and valid losses as a function
    of the number of epochs](img/B19629_07_03.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 训练和验证损失随着周期数增加但无过拟合的示例](img/B19629_07_03.jpg)'
- en: Figure 7.3 – Example with no overfitting of train and valid losses as a function
    of the number of epochs
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 训练和验证损失随着周期数增加但无过拟合的示例
- en: But it’s not always that perfect. In practice, it may happen that the neural
    network is learning more and more about the data distribution of the train set
    at every epoch, at the cost of the generalization to new data. This case is depicted
    by the example in *Figure 7**.4*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 但现实中并非总是如此完美。在实践中，可能会发生神经网络在每个周期中逐渐学习到更多关于训练集数据分布的信息，这可能会牺牲对新数据的泛化能力。这个情况在*图
    7.4*中有所展示。
- en: '![Figure 7.4 – Example with overfitting of train and valid losses as a function
    of the number of epochs](img/B19629_07_04.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – 训练和验证损失随着周期数增加而过拟合的示例](img/B19629_07_04.jpg)'
- en: Figure 7.4 – Example with overfitting of train and valid losses as a function
    of the number of epochs
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 训练和验证损失随着周期数增加而过拟合的示例
- en: When dealing with such a scenario, a natural solution would be to halt the training
    process once the **valid** loss of the model stops decreasing. Once the validation
    loss of the model stops decreasing, continuing to train the model for additional
    epochs may cause it to become better at memorizing the training data, rather than
    improving its ability to make accurate predictions on new, unseen data. This technique
    is **called early stopping** and allows to prevent a model from overfitting.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当遇到这种情况时，一个自然的解决方案是，在模型的**验证**损失停止下降时暂停训练。一旦模型的验证损失停止下降，继续训练额外的周期可能会导致模型更擅长记忆训练数据，而不是提升其在新数据上的预测准确性。这个技术叫做**早停法**，它可以防止模型过拟合。
- en: '![Figure 7.5 – As soon as the valid loss stops decreasing, we can stop the
    learning and consider the model fully trained; this is early stopping](img/B19629_07_05.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – 一旦验证损失停止下降，我们就可以停止学习并认为模型已经完全训练好；这就是早停法](img/B19629_07_05.jpg)'
- en: Figure 7.5 – As soon as the valid loss stops decreasing, we can stop the learning
    and consider the model fully trained; this is early stopping
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – 一旦验证损失停止下降，我们就可以停止学习并认为模型已经完全训练好；这就是早停法。
- en: Since this recipe will be applied to the breast cancer dataset, scikit-learn
    must be installed, along with `torch` for the models and `matplotlib` for visualization.
    These libraries can be installed with `pip install sklearn` `torch matplotlib`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个示例将应用于乳腺癌数据集，因此必须安装`scikit-learn`，以及用于模型的`torch`和可视化的`matplotlib`。可以通过`pip
    install sklearn` `torch matplotlib`安装这些库。
- en: How to do it...
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: In this recipe, we will first train a neural network on the breast cancer dataset
    and visualize the overfitting effect amplifying with the number of epochs. Then,
    we will implement early stopping, to regularize.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例中，我们将首先在乳腺癌数据集上训练一个神经网络，并可视化随着周期数增加，过拟合效应的加剧。然后，我们将实现早停法来进行正则化。
- en: Regular training
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 正常训练
- en: 'Since the breast cancer dataset is rather small, instead of splitting the dataset
    into train, valid, and test sets, we will consider only the train and valid sets:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于乳腺癌数据集相对较小，我们将只考虑训练集和验证集，而不是将数据集拆分为训练集、验证集和测试集：
- en: 'Import the needed libraries from `scikit-learn`, `matplotlib`, and `torch`:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`scikit-learn`、`matplotlib`和`torch`导入所需的库：
- en: '`load_breast_cancer` to load the dataset'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`load_breast_cancer`加载数据集
- en: '`train_test_split` to split the data into training and validation sets'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`将数据拆分为训练集和验证集
- en: '`StandardScaler` to rescale the quantitative data'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`StandardScaler`对定量数据进行重新缩放
- en: '`accuracy_score` to evaluate the model'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`accuracy_score`评估模型
- en: '`matplotlib` for display'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`matplotlib`进行显示
- en: '`torch` itself'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`torch`本身
- en: '`torch.nn` containing required classes for building a neural network'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`torch.nn`包含构建神经网络所需的类
- en: '`torch.nn.functional` for activation functions such as ReLU'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`torch.nn.functional`实现激活函数，如ReLU
- en: '`Dataset` and `DataLoader` for handling the data'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`Dataset`和`DataLoader`处理数据
- en: 'Here is the code for it:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是实现代码：
- en: '[PRE80]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Load the features and labels with the `load_breast_cancer` function:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`load_breast_cancer`函数加载特征和标签：
- en: '[PRE81]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Split the data into training and validation sets, specifying the random state
    for reproducibility, and convert the features and labels in to `float32` for later
    compatibility with PyTorch:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和验证集，指定随机种子以确保可重复性，并将特征和标签转换为`float32`，以便后续与PyTorch兼容：
- en: '[PRE82]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Create the `Dataset` class for handling the data. We are simply reusing the
    class implemented in the previous chapter:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`Dataset`类来处理数据。我们简单地重用了上一章实现的类：
- en: '[PRE85]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Instantiate the training and validation sets and loaders for PyTorch. Notice
    that we provide the training scaler when instantiating the validation dataset
    to make sure the scaler used with both datasets is the one fitted on the training
    set:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为PyTorch实例化训练集和验证集及其数据加载器。注意，在实例化验证集时，我们提供了训练数据集的缩放器，确保两个数据集使用的缩放器是基于训练集拟合的：
- en: '[PRE100]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Define the neural network architecture – 2 hidden layers of 36 units and an
    output layer with 1 unit with a sigmoid activation function since it’s a binary
    classification task:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经网络架构——2个隐藏层，每个隐藏层有36个单元，输出层有1个单元，并使用sigmoid激活函数，因为这是一个二分类任务：
- en: '[PRE107]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'Instantiate the model with the expected input shape (the number of features).
    Optionally, we can check the forward propagation works properly on a given random
    tensor:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用期望的输入形状（特征数量）实例化模型。我们还可以选择检查给定随机张量的前向传播是否正常工作：
- en: '[PRE126]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'The output of this code is the following (the value itself may change, but
    will be between 0 and 1 since it’s a sigmoid activation function on the last layer):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 该代码的输出如下（具体值可能会有所变化，但由于最后一层使用的是sigmoid激活函数，所以输出值会在0和1之间）：
- en: '[PRE132]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'Define the loss function as the binary cross entropy loss since this is a binary
    classification task. Instantiate the optimizer too:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将损失函数定义为二分类交叉熵损失函数，因为这是一个二分类任务。同时实例化优化器：
- en: '[PRE133]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'Implement a helper function, `epoch_step`, that computes forward propagation,
    backpropagation (for the training set), loss, and accuracy for one epoch:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现一个辅助函数`epoch_step`，该函数计算前向传播、反向传播（对于训练集）、损失和准确率，适用于一个训练周期：
- en: '[PRE136]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: 'Let’s now implement the `train_model` function allowing us to train a model,
    with or without patience. This function stores each epoch and then returns the
    following:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们实现`train_model`函数，以便训练一个模型，无论是否使用耐心。该函数存储每个训练周期的信息，然后返回以下结果：
- en: The loss and accuracy for the train set
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集的损失和准确率
- en: The loss and accuracy for the valid set
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集的损失和准确率
- en: 'Here is the code for the model:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是模型的代码：
- en: '[PRE158]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: 'Let’s now train the neural network on 500 epochs reusing the previously implemented
    `train_model` function. Here is the code for it:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在500个周期上训练神经网络，重用之前实现的`train_model`函数。以下是代码：
- en: '[PRE159]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: 'After 500 epochs, the code output will be something like the following:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在500个周期后，代码输出将类似于以下内容：
- en: '[PRE160]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: 'We can now plot the loss for both training and validation sets, as a function
    of the epoch, and visualize the overfitting effect increasing with the number
    of epochs:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以绘制训练集和验证集的损失图，作为训练周期的函数，并可视化随着周期数增加而加剧的过拟合效应：
- en: '[PRE161]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'Here is the plot for it:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是该损失的图表：
- en: '![Figure 7.6 – Cross-entropy loss as a function of the epoch. (despite a few
    bumps, the training loss keeps decreasing)](img/B19629_07_06.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 交叉熵损失作为训练周期的函数。（尽管有几个波动，训练损失仍然在持续下降）](img/B19629_07_06.jpg)'
- en: Figure 7.6 – Cross-entropy loss as a function of the epoch. (despite a few bumps,
    the training loss keeps decreasing)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 交叉熵损失作为训练周期的函数。（尽管有几个波动，训练损失仍然在持续下降）
- en: We indeed have a training loss that keeps decreasing overall, even reaching
    a value of zero. On the other hand, the valid loss starts decreasing to reach
    a minimum somewhere around epoch 100 and then increases slowly over the epochs.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，我们看到训练损失总体上持续下降，甚至达到了零的值。另一方面，验证损失开始下降并在第100个周期左右达到最小值，然后在接下来的周期中缓慢增加。
- en: 'We can implement early stopping to avoid this situation in several ways:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过多种方式实现早停来避免这种情况：
- en: After the first training, we could retrain the model up to 100 epochs (or any
    identified optimal validation loss), hopefully having the same results. This would
    be a waste of CPU time.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一次训练后，我们可以重新训练模型，最多100个周期（或任何识别的最佳验证损失），希望能够得到相同的结果。这将是浪费CPU时间。
- en: We could save the model at every epoch, and then pick the best one afterward.
    This solution is sometimes implemented but can be a waste of storage memory, especially
    for large models.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在每个周期保存模型，然后在之后挑选最佳模型。这个方法有时会被实现，但可能会浪费存储空间，特别是对于大型模型。
- en: We could automatically stop the training after a given number of epochs not
    improving validation loss. The minimum number of steps without validation loss
    improvement is usually called patience.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在验证损失没有改善时，自动停止训练，这个停止条件通常称为“耐心”。
- en: Let’s now implement the latter solution.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们实现后者的解决方案。
- en: Note
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Using patience is risky too: a too-small patience may get the model stuck in
    a local minimum, while a too-large patience may miss the actual optimal epoch
    by stopping too late.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 使用耐心也有风险：耐心太小可能会让模型陷入局部最小值，而耐心太大可能会错过真正的最优epoch，导致停止得太晚。
- en: Training with patience and early stopping
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用耐心和早停的训练
- en: 'Let’s now retrain a model using early stopping. We first instantiate a fresh
    model to avoid training an already trained model:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用早停重新训练一个模型。我们首先实例化一个新的模型，以避免训练已经训练过的模型：
- en: 'Instantiate a fresh model as well as a fresh optimizer. No need to test it,
    nor to instantiate the loss again if you are using the same notebook kernel. If
    you want to run this code separately, *steps 1* to *8* of the previous recipe
    must be reused:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个新的模型以及一个新的优化器。如果你使用的是相同的笔记本内核，就不需要测试它，也不需要重新实例化损失函数。如果你想单独运行这段代码，*步骤1*到*步骤8*必须重复使用：
- en: '[PRE167]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: 'We now train this model with a patience of `30`. After 30 epochs without improving
    the `val` loss, the training will just stop:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在使用`30`的耐心来训练这个模型。在连续30个epoch内，如果`val`损失没有改善，训练将会停止：
- en: '[PRE170]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: '[PRE173]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE174]'
- en: 'The code output will be something like the following (the total number of epochs
    before reaching the early stopping may vary):'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出将类似以下内容（在达到早停之前的总epoch数量可能会有所不同）：
- en: '[PRE175]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: The training stopped after about 100 epochs (the result may vary since the results
    are not deterministic by default), with a validation accuracy of about 98%, far
    better than the 96% that we got after 500 epochs.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 训练在大约100个epoch后停止（结果可能会有所不同，因为默认情况下结果是非确定性的），验证准确率大约为98%，远远超过我们在500个epoch后得到的96%。
- en: 'Let’s plot the train and validation losses again as a function of the number
    of epochs:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次绘制训练和验证损失，作为epoch数量的函数：
- en: '[PRE176]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: 'Here is the plot for it:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是它的图表：
- en: '![Figure 7.7 – Cross-entropy loss as a function of the epoch](img/B19629_07_07.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 交叉熵损失与epoch的关系](img/B19629_07_07.jpg)'
- en: Figure 7.7 – Cross-entropy loss as a function of the epoch
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 交叉熵损失与epoch的关系
- en: As we can see, the validation loss is already overfitting but did not have time
    to grow too much, preventing further overfitting.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，验证损失已经发生过拟合，但没有时间增长太多，从而避免了进一步的过拟合。
- en: There’s more...
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: As explained earlier in this recipe, for proper evaluation, it would be necessary
    to compute the accuracy (or any selected evaluation metric) on a separate test
    set. Indeed, stopping the training based on the validation set and evaluating
    the model on this same dataset is a biased approach, and may artificially improve
    the evaluation.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如本例中前面所述，为了进行正确的评估，需要在单独的测试集上计算准确率（或任何选定的评估指标）。实际上，基于验证集停止训练并在同一数据集上评估模型是一种偏向的方法，可能会人为提高评估结果。
- en: Regularization with network architecture
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用网络架构进行正则化
- en: 'In this recipe, we will explore a less popular, but still sometimes useful,
    regularization method: adapting the neural network architecture. After reviewing
    why to use this method and when, we will apply it to the California housing dataset,
    a regression task.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将探讨一种不太常见但有时仍然有用的正则化方法：调整神经网络架构。在回顾为何使用此方法以及何时使用后，我们将其应用于加利福尼亚住房数据集，这是一个回归任务。
- en: Getting ready
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备中
- en: 'Sometimes, the best way to regularize is not to use any fancy techniques but
    only common sense. In many cases, it happens that the neural network used is just
    too large for the input task and dataset. An easy rule of thumb is to have a quick
    look at the number of parameters in the network (e.g., weights and biases) and
    compare it to the number of data points: if the ratio is above 1 (i.e., there
    are more parameters than data points), there is a risk of severe overfitting.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，最好的正则化方法不是使用任何花哨的技术，而是常识。在许多情况下，使用的神经网络可能对于输入任务和数据集来说过于庞大。一个简单的经验法则是快速查看网络中的参数数量（例如，权重和偏置），并将其与数据点的数量进行比较：如果比率大于1（即参数多于数据点），则有严重过拟合的风险。
- en: Note
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If transfer learning is used, this rule of thumb no longer applies since the
    network has been trained on a presumably large enough dataset.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用迁移学习，这条经验法则不再适用，因为网络已经在一个假定足够大的数据集上进行了训练。
- en: 'If we take a step back and go back to linear models such as linear regression,
    it is well known that having too many correlated features can deteriorate the
    model’s performance. It can be the same for neural networks: having too many free
    parameters will do no good to the performances. So, depending on the task, it
    is not always required to have dozens of layers; just a few may be enough to get
    the best performances and avoid overfitting. Let’s check that in practice on the
    California dataset.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们退后一步，回到线性模型，比如线性回归，大家都知道，特征之间高度相关会降低模型的性能。神经网络也是如此：过多的自由参数并不会提高性能。因此，根据任务的不同，并不总是需要几十层的网络；只需几层就足以获得最佳性能并避免过拟合。让我们通过加利福尼亚数据集在实践中验证这一点。
- en: To do so, the libraries needed are scikit-learn, Matplotlib, and PyTorch. They
    can be installed with `pip install sklearn` `matplotlib torch`.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，需要使用的库有scikit-learn、Matplotlib和PyTorch。可以通过`pip install sklearn` `matplotlib
    torch`来安装它们。
- en: How to do it...
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'This will be a two-step recipe: first, we will train a large model (compared
    to the dataset) on the data, to expose the effect of the network on overfitting.
    Then, we will train another, more adapted model on this same data, hopefully fixing
    the overfitting issue.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是一个两步的流程：首先，我们将训练一个较大的模型（相对于数据集来说），以揭示网络对过拟合的影响。然后，我们将在相同数据上训练另一个更适配的模型，期望解决过拟合问题。
- en: Training a large model
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练一个大型模型
- en: 'Here are the steps to train a model:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是训练模型的步骤：
- en: 'The following imports are needed first:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先需要导入以下库：
- en: '`fetch_california_housing` to load the dataset'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`fetch_california_housing`来加载数据集
- en: '`train_test_split` to split the data into training and test sets'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`将数据划分为训练集和测试集
- en: '`StandardScaler` to rescale the features'
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`StandardScaler`来重新缩放特征
- en: '`r2_score` to evaluate the model at the end'
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`r2_score`来评估模型的最终表现
- en: '`matplotlib` to display the loss'
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`matplotlib`来显示损失
- en: '`torch` itself for some functions at the lower level of the library'
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`本身提供一些库中低级功能的实现'
- en: '`torch.nn`, which has many useful classes for building a neural network'
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`torch.nn`，它提供了许多用于构建神经网络的实用类
- en: '`torch.nn.functional` for some useful functions'
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`torch.nn.functional`来实现一些有用的函数
- en: '`Dataset` and `DataLoader` for handling the data operations'
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`Dataset`和`DataLoader`来处理数据操作
- en: 'The following is the code for these `import` statements:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这些`import`语句的代码：
- en: '[PRE182]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: 'Load the data using the `fetch_california_housing` function:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fetch_california_housing`函数加载数据：
- en: '[PRE183]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: 'Split the data into training and test sets with a ratio of 80%/20%, using the
    `train_test_split` function. Set a random state for reproducibility. For `pytorch`,
    the data is converted in to `float32` variables:'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`函数以80%/20%的比例将数据划分为训练集和测试集。设置一个随机种子以保证可复现性。对于`pytorch`，数据会被转换成`float32`类型的变量：
- en: '[PRE184]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE184]'
- en: '[PRE185]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE185]'
- en: '[PRE186]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE186]'
- en: 'Rescale the data using the standard scaler:'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准化缩放器对数据进行重新缩放：
- en: '[PRE187]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: 'Create the `CaliforniaDataset` class, allowing to handle the data. The only
    transformation here is the conversion from a `numpy` array to a `torch` tensor:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`CaliforniaDataset`类，用于处理数据。这里唯一的变换是将`numpy`数组转换为`torch`张量：
- en: '[PRE190]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '[PRE192]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE192]'
- en: '[PRE193]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: 'Instantiate the datasets for the train and test sets and the data loaders.
    We define here a batch size of `64` but this can be modified:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化训练集和测试集的数据集以及数据加载器。这里定义了一个批处理大小为`64`，但可以根据需要进行调整：
- en: '[PRE198]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '[PRE199]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: '[PRE200]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '[PRE201]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE201]'
- en: '[PRE202]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE204]'
- en: '[PRE205]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE205]'
- en: 'Create the neural network architecture. We create a large model here on purpose
    considering the dataset – 5 hidden layers of 128 units:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建神经网络架构。考虑到数据集的规模，我们故意创建一个较大的模型——包含5个隐藏层，每个层有128个单元：
- en: '[PRE206]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '[PRE208]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '[PRE209]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE209]'
- en: '[PRE210]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '[PRE211]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE211]'
- en: '[PRE212]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE212]'
- en: '[PRE213]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE214]'
- en: '[PRE215]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '[PRE217]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE218]'
- en: '[PRE219]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '[PRE220]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE221]'
- en: '[PRE222]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE222]'
- en: '[PRE223]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE223]'
- en: '[PRE224]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE224]'
- en: '[PRE225]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE225]'
- en: '[PRE226]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE226]'
- en: '[PRE227]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE227]'
- en: '[PRE228]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE228]'
- en: '[PRE229]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '[PRE230]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE230]'
- en: '[PRE231]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE231]'
- en: '[PRE232]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '[PRE233]'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE233]'
- en: '[PRE234]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE234]'
- en: 'Instantiate the model with the given input shape (the number of features).
    Optionally, we can check the network is correctly created using an input tensor
    of the expected shape (so here is the number of features):'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用给定的输入形状（特征数量）实例化模型。可选地，我们可以使用预期形状的输入张量来检查网络是否正确创建（这里指的是特征数量）：
- en: '[PRE235]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE235]'
- en: '[PRE236]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE236]'
- en: '[PRE237]'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE237]'
- en: '[PRE238]'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE238]'
- en: '[PRE239]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE239]'
- en: '[PRE240]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE240]'
- en: '[PRE241]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE241]'
- en: 'Instantiate the loss to be a mean squared error loss since this is a regression
    task, and define the optimizer to be `Adam`, with a learning rate of `0.001`:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将损失函数实例化为均方误差损失（MSE），因为这是一个回归任务，并定义优化器为`Adam`，学习率为`0.001`：
- en: '[PRE242]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE242]'
- en: '[PRE243]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE243]'
- en: 'Finally, train the neural network on 500 epochs by using the `train_model`
    function. The implementation of this function is similar to previous ones and
    can be found in the GitHub repository. Again, we purposely chose a large number
    of epochs; otherwise, the overfitting could be compensated by early stopping.
    We also store the train and test losses for each epoch, for visualization purposes
    and information:'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`train_model`函数训练神经网络500个纪元。这个函数的实现与之前的类似，可以在GitHub仓库中找到。再次提醒，我们故意选择了一个较大的纪元数；否则，过拟合可能会通过提前停止来得到补偿。我们还会存储每个纪元的训练和测试损失，用于可视化和信息展示：
- en: '[PRE244]'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE244]'
- en: '[PRE245]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE245]'
- en: '[PRE246]'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE246]'
- en: 'After 500 epochs, the final output lines will be like the following:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 经过500个纪元后，最终的输出曲线将如下所示：
- en: '[PRE247]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE247]'
- en: 'Plot the loss for both the train and test set as a function of the epoch:'
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练集和测试集的损失绘制为与纪元相关的函数：
- en: '[PRE248]'
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE248]'
- en: '[PRE249]'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE249]'
- en: '[PRE250]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE250]'
- en: '[PRE251]'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE251]'
- en: '[PRE252]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE252]'
- en: '[PRE253]'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE253]'
- en: 'Here is the plot for it:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这是其绘图：
- en: '![Figure 7.8 – Mean squared error loss as a function of the epoch (note the
    clear divergence between the train and test losses)](img/B19629_07_08.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 平均平方误差损失与纪元的关系（注意训练集和测试集的损失明显分离）](img/B19629_07_08.jpg)'
- en: Figure 7.8 – Mean squared error loss as a function of the epoch (note the clear
    divergence between the train and test losses)
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 平均平方误差损失与纪元的关系（注意训练集和测试集的损失明显分离）
- en: We can notice that the train loss keeps decreasing over and over, while the
    test loss soon reaches a plateau before increasing again. This is a clear sign
    of overfitting. Let’s confirm there is overfitting by computing the R2-scores.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以注意到，训练损失持续下降，而测试损失很快达到一个平台期，然后再次上升。这是过拟合的明显信号。让我们通过计算R2分数来确认是否存在过拟合。
- en: 'Finally, let’s evaluate the model on both the training and test sets with the
    R2-score:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们使用R2分数评估模型在训练集和测试集上的表现：
- en: '[PRE254]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE254]'
- en: '[PRE255]'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE255]'
- en: '[PRE256]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE256]'
- en: '[PRE257]'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE257]'
- en: '[PRE258]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE258]'
- en: '[PRE259]'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE259]'
- en: '[PRE260]'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE260]'
- en: '[PRE261]'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE261]'
- en: '[PRE262]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE262]'
- en: '[PRE263]'
  id: totrans-442
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE263]'
- en: 'This code will output values such as the following:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将输出如下类似的值：
- en: '[PRE264]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE264]'
- en: As expected, we are facing a clear overfitting here, with an almost perfect
    R2-score on the train set, and an R2-score of about 0.76 on the test set.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期所示，我们确实遇到了明显的过拟合，在训练集上几乎达到了完美的R2分数，而在测试集上的R2分数约为0.76。
- en: Note
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This may look like an exaggerated example, but it is fairly easy to choose an
    architecture that is way too large for the task and dataset.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来像是一个夸张的例子，但选择一个对于任务和数据集来说过于庞大的架构其实是相当容易的。
- en: Regularizing with a smaller network
  id: totrans-448
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用更小的网络进行正则化
- en: Let’s now train a more reasonable model and see how this impacts overfitting,
    even with the same number of epochs. The goal is not only to decrease overfitting
    but also to get better performances on the test set.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练一个更合理的模型，看看这如何影响过拟合，即使是使用相同数量的纪元。目标不仅是减少过拟合，还要在测试集上获得更好的表现。
- en: 'If you are using the same kernel, there is no need to redo the first steps.
    Otherwise, *steps 1* to *6* must be redone:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是相同的内核，则不需要重新执行第一步。否则，*步骤 1* 到 *6* 必须重新执行：
- en: 'Define the neural network. This time, we only have two hidden layers of 16
    units each, so this is much smaller than earlier:'
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经网络。这次我们只有两个包含16个单元的隐藏层，因此这个网络比之前的要小得多：
- en: '[PRE265]'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE265]'
- en: '[PRE266]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE266]'
- en: '[PRE267]'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE267]'
- en: '[PRE268]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE268]'
- en: '[PRE269]'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE269]'
- en: '[PRE270]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE270]'
- en: '[PRE271]'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE271]'
- en: '[PRE272]'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE272]'
- en: '[PRE273]'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE273]'
- en: '[PRE274]'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE274]'
- en: '[PRE275]'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE275]'
- en: '[PRE276]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE276]'
- en: '[PRE277]'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE277]'
- en: '[PRE278]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE278]'
- en: '[PRE279]'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE279]'
- en: '[PRE280]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE280]'
- en: '[PRE281]'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE281]'
- en: 'Instantiate the network with the expected number of input features and the
    optimizer:'
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预期数量的输入特征和优化器实例化网络：
- en: '[PRE282]'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE282]'
- en: '[PRE283]'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE283]'
- en: '[PRE284]'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE284]'
- en: '[PRE285]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE285]'
- en: 'Train the neural network over 500 epochs so that we have results that we can
    compare to the previous ones. We will reuse the `train_model` function already
    used earlier in this recipe:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练神经网络500个纪元，以便我们可以将结果与之前的结果进行比较。我们将重新使用之前在本配方中使用的`train_model`函数：
- en: '[PRE286]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE286]'
- en: '[PRE287]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE287]'
- en: '[PRE288]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE288]'
- en: '[PRE289]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE289]'
- en: '[PRE290]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE290]'
- en: 'Plot the loss as a function of the epoch for the train and test sets:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将损失绘制为与纪元相关的函数，分别针对训练集和测试集：
- en: '[PRE291]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE291]'
- en: '[PRE292]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE292]'
- en: '[PRE293]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE293]'
- en: '[PRE294]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE294]'
- en: '[PRE295]'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE295]'
- en: '[PRE296]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE296]'
- en: 'Here is the plot for it:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 这是其绘图：
- en: '![Figure 7.9 – Mean squared error loss as a function of the epoch (note the
    train and test sets almost overlapping)](img/B19629_07_09.jpg)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 平均平方误差损失与纪元的关系（注意训练集和测试集几乎重叠）](img/B19629_07_09.jpg)'
- en: Figure 7.9 – Mean squared error loss as a function of the epoch (note the train
    and test sets almost overlapping)
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 平均平方误差损失与纪元的关系（注意训练集和测试集几乎重叠）
- en: 'As we can see, this time, even with many epochs, there is no strong overfitting:
    the train and test losses remain close to each other no matter the number (except
    for a few noise bumps), even if a small amount of overfitting seems to appear
    over time.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，即使经过许多纪元，这次也没有明显的过拟合：无论纪元数量如何（除了少数噪声波动），训练集和测试集的损失保持接近，尽管随着时间推移，似乎会出现少量过拟合。
- en: 'Let’s again evaluate the model with the R2-score on the training and test sets:'
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次使用R2分数评估模型在训练集和测试集上的表现：
- en: '[PRE297]'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE297]'
- en: '[PRE298]'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE298]'
- en: '[PRE299]'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE299]'
- en: '[PRE300]'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE300]'
- en: '[PRE301]'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE301]'
- en: '[PRE302]'
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE302]'
- en: '[PRE303]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE303]'
- en: '[PRE304]'
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE304]'
- en: '[PRE305]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE305]'
- en: '[PRE306]'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE306]'
- en: 'Here is the typical output of this code:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是此代码的典型输出：
- en: '[PRE307]'
  id: totrans-503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE307]'
- en: While the R2-score on the training set decreased from 0.99 to 0.81, the score
    on the test set increased from 0.76 to 0.79, effectively improving the performance
    of the model.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然训练集上的R2分数从0.99降到了0.81，但测试集上的得分从0.76提高到了0.79，有效地提高了模型的性能。
- en: Even if it was a rather extreme example, the general idea remains true.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这是一个相当极端的例子，整体思路仍然是成立的。
- en: Note
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Early stopping could work well too in this case. The two techniques (early stopping
    and downsizing the network) are not mutually exclusive and can work well together.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，提前停止（early stopping）也可能效果很好。这两种技术（提前停止和缩小网络）并不是相互排斥的，实际上可以很好地协同工作。
- en: There’s more...
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The model complexity can arguably be computed using the number of parameters.
    Even if it’s not a direct measure, it remains a good indicator.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的复杂度可以通过参数数量来计算。即使这不是一个直接的度量，它仍然是一个良好的指示器。
- en: For example, the first neural network used in this recipe, with 10 hidden layers
    of 128 units, had 67,329 trainable parameters. On the other side, the second neural
    network, with only 2 hidden layers of 16 units, had only 433 trainable parameters.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，本食谱中使用的第一个神经网络，具有10个隐藏层和128个单元，共有67,329个可训练参数。另一方面，第二个神经网络，只有2个隐藏层和16个单元，仅有433个可训练参数。
- en: 'The number of parameters in a fully connected neural network is based on the
    number of units and the number of layers: both units and layers do not have to
    be the same on the number of parameters though.'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接神经网络的参数数量是基于单元数量和层数的：不过，单元数和层数并不直接决定参数的数量。
- en: 'To compute the number of trainable parameters in the torch network’s net, we
    can use the following code snippet:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算torch网络中可训练参数的数量，我们可以使用以下代码片段：
- en: '[PRE308]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE308]'
- en: 'To get an idea, let’s take again three examples of neural networks with the
    same number of neurons, but with a different number of layers. Let’s assume they
    all have 10 input features and 1 unit output layer:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解，让我们再看三个神经网络的例子，这三个网络的神经元数量相同，但层数不同。假设它们都具有10个输入特征和1个单元输出层：
- en: 'A neural network with 1 hidden layer of 100 units: 1,201 parameters'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有1个隐藏层和100个单元的神经网络：1,201个参数
- en: 'A neural network with 2 hidden layers of 50 units: 3,151 parameters'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有2个隐藏层和50个单元的神经网络：3,151个参数
- en: 'A neural network with 10 hidden layers of 10 units: 1,111 parameters'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有10个隐藏层和10个单元的神经网络：1,111个参数
- en: So, there is a trade-off between the number of layers and the number of units
    per layer to get the most complex neural network for a given number of neurons.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在层数和每层单元数之间存在权衡，以便在给定神经元数量的情况下构建最复杂的神经网络。
- en: Regularizing with dropout
  id: totrans-519
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用丢弃法进行正则化
- en: A widely used method for regularizing is dropout. Dropout is just randomly setting
    some neurons’ activations to zero during the training phase. Let’s first review
    how this works and then apply it to a multiclass classification task, the `sklearn`
    digits dataset, which is kind of an older and smaller version of the MNIST dataset.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 一种广泛使用的正则化方法是丢弃法（dropout）。丢弃法就是在训练阶段随机将一些神经元的激活值设置为零。让我们首先回顾一下它是如何工作的，然后将其应用于多类分类任务——`sklearn`数字数据集，这是MNIST数据集的一个较旧且较小的版本。
- en: Getting ready
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: Dropout is a widely adopted regularization approach in deep learning, due to
    its simplicity and effectiveness. The technique is easy to understand, yet can
    yield powerful results.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃法是深度学习中广泛采用的正则化方法，因其简单有效而受到青睐。这种技术易于理解，但能够产生强大的效果。
- en: 'The principle is simple – during training, we randomly ignore some units by
    setting their activations to zero, as represented in *Figure 7**.10*:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 原理很简单——在训练过程中，我们随机忽略一些单元，将它们的激活值设置为零，正如*图 7.10*中所示：
- en: '![Figure 7.10 – On the left, a standard neural network with its connections,
    and, on the right, the same neural network with dropout, having, on average, 50%
    of its neurons ignored at training](img/B19629_07_10.jpg)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – 左侧是一个标准的神经网络及其连接，右侧是同一个神经网络应用丢弃法后，训练时平均有50%的神经元被忽略](img/B19629_07_10.jpg)'
- en: Figure 7.10 – On the left, a standard neural network with its connections, and,
    on the right, the same neural network with dropout, having, on average, 50% of
    its neurons ignored at training
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – 左侧是一个标准的神经网络及其连接，右侧是同一个神经网络应用丢弃法后，训练时平均有50%的神经元被忽略
- en: 'Dropout adds one hyperparameter though: the dropout probability. For a 0% probability,
    there is no dropout. For a 50% probability, about 50% of the neurons will be randomly
    selected to be ignored. For a 100% probability, well, there is nothing left to
    learn. The ignored neurons are not always the same: for each new batch size, a
    new set of units is randomly selected to be ignored.'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，dropout 添加了一个超参数：dropout 概率。对于 0% 的概率，没有 dropout；对于 50% 的概率，约 50% 的神经元将被随机选择忽略；对于
    100% 的概率，嗯，那就没有东西可学了。被忽略的神经元并不总是相同的：对于每个新的批量大小，都会随机选择一组新的单元进行忽略。
- en: Note
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The remaining activations are consequently scaled to keep a consistent global
    input for any unit. In practice, for a dropout probability of 1/2, all the neurons
    that are not ignored are scaled by a factor of 2 (i.e., their activations are
    multiplied by 2).
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的激活值会被缩放，以保持每个单元的一致全局输入。实际上，对于 1/2 的 dropout 概率，所有未被忽略的神经元都会被缩放 2 倍（即它们的激活值乘以
    2）。
- en: Certainly, when evaluating or inferring on new data, dropout is deactivated,
    causing all neurons to be activated.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在评估或对新数据进行推理时，dropout 会被停用，导致所有神经元都被激活。
- en: 'But what is the point of doing that? Why would randomly ignoring some neurons
    help? A formal explanation is beyond the scope of this book, but at least we can
    provide some intuition. The idea is to avoid confusing the neural network with
    too much information. As a human, having too much information can hurt more than
    it helps: sometimes, having less information allows you to make better decisions,
    preventing you from being flooded by it. This is the idea of dropout: instead
    of giving the network all the information at once, it is gently trained with less
    information by turning off a few neurons randomly for a short amount of time.
    Hopefully, this will help the network make better decisions in the end.'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 但是这样做的意义何在呢？为什么随机忽略一些神经元会有帮助呢？一个正式的解释超出了本书的范围，但至少我们可以提供一些直观的理解。这个想法是避免给神经网络提供过多信息而导致混淆。作为人类，信息过多有时比有帮助更多：有时，信息更少反而可以帮助你做出更好的决策，避免被信息淹没。这就是
    dropout 的思想：不是一次性给网络所有信息，而是通过在短时间内随机关闭一些神经元，以较少的信息来温和训练网络。希望这能最终帮助网络做出更好的决策。
- en: In this recipe, this will be run on the `digits` dataset of scikit-learn, which
    is just a link to the *Optical Recognition of Handwritten Digits* dataset. A small
    subset of these images is represented in *Figure 7**.11*.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，将使用 scikit-learn 的 `digits` 数据集，该数据集实际上是 *光学手写数字识别* 数据集的一个链接。这些图像的一个小子集展示在
    *图 7.11* 中。
- en: '![Figure 7.11 – A sample of images from the dataset and their labels: each
    image is composed of 8x8 pixels](img/B19629_07_11.jpg)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.11 – 数据集中的一组图像及其标签：每个图像由 8x8 像素组成](img/B19629_07_11.jpg)'
- en: 'Figure 7.11 – A sample of images from the dataset and their labels: each image
    is composed of 8x8 pixels'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 数据集中的一组图像及其标签：每个图像由 8x8 像素组成
- en: Each image is an 8x8-pixel picture of a handwritten digit. Thus, the dataset
    is made up of 10 classes, 1 for each digit.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 每张图像都是一个 8x8 像素的手写数字图像。因此，数据集由 10 个类别组成，每个类别代表一个数字。
- en: To run the code of this recipe, the required libraries are `sklearn`, `matplotlib`,
    and `torch`. They can be installed with `pip install sklearn` `matplotlib torch`.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本教程中的代码，所需的库是 `sklearn`、`matplotlib` 和 `torch`。可以通过 `pip install sklearn`、`matplotlib`、`torch`
    安装这些库。
- en: How to do it...
  id: totrans-536
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'This recipe will comprise two steps:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 本教程将包括两个步骤：
- en: First, we will train a neural network without dropout with a rather large model,
    considering the data.
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将训练一个没有 dropout 的神经网络，使用一个相对较大的模型，考虑到数据的特点。
- en: Then, we will train the same neural network with dropout, hopefully, to improve
    the model’s performance.
  id: totrans-539
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用 dropout 训练相同的神经网络，希望能提高模型的性能。
- en: We will use the same data for both configurations, the same batch size, and
    the same number of epochs, so that we can compare the results.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的数据、相同的批量大小和相同的训练轮次，以便比较结果。
- en: Without dropout
  id: totrans-541
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 没有 dropout
- en: 'Here are the steps to regularize without dropout:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是没有 dropout 的正则化步骤：
- en: 'The following imports must be loaded:'
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 必须加载以下导入：
- en: '`load_digits` from `sklearn` to load the dataset'
  id: totrans-544
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `sklearn` 中的 `load_digits` 加载数据集
- en: '`train_test_split` from `sklearn` to split the dataset'
  id: totrans-545
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `sklearn` 中的 `train_test_split` 来拆分数据集
- en: '`torch`, `torch.nn`, and `torch.nn.functional` for the neural network'
  id: totrans-546
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`torch`、`torch.nn` 和 `torch.nn.functional` 用于神经网络'
- en: '`Dataset` and `DataLoader` from `torch` for the dataset loading in `torch`'
  id: totrans-547
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dataset` 和 `DataLoader` 来自 `torch`，用于在 `torch` 中加载数据集'
- en: '`matplotlib` for the visualization of the loss'
  id: totrans-548
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`用于可视化损失'
- en: 'Here is the code for the `import` statements:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 这是`import`语句的代码：
- en: '[PRE309]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE309]'
- en: 'Load the data. The dataset is made of 1,797 samples, and the images are already
    flattened to 64 values between 0 and 16 for the 8x8 pixels:'
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据。数据集包含1,797个样本，图像已经展平为64个值，范围从0到16，对应8x8像素：
- en: '[PRE310]'
  id: totrans-552
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE310]'
- en: 'Split the data into training and test sets, with 80% in the training set and
    20% in the test set. The features are converted in to `float32`, while the labels
    are converted into `int64` to avoid `torch` errors later:'
  id: totrans-553
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拆分为训练集和测试集，80%的数据用于训练集，20%的数据用于测试集。特征值转换为`float32`，标签值转换为`int64`，以避免后续`torch`错误：
- en: '[PRE311]'
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE311]'
- en: '[PRE312]'
  id: totrans-555
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE312]'
- en: '[PRE313]'
  id: totrans-556
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE313]'
- en: 'Create the `DigitsDataset` class for PyTorch. The only transformation to the
    features, besides converting them into `torch` tensors, is to divide the values
    by 255 to have a range of features in `[``0, 1]`:'
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为PyTorch创建`DigitsDataset`类。除了将特征转换为`torch`张量外，唯一的转换操作是将值除以255，以使特征的范围落在`[0,
    1]`之间：
- en: '[PRE314]'
  id: totrans-558
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE314]'
- en: '[PRE315]'
  id: totrans-559
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE315]'
- en: '[PRE316]'
  id: totrans-560
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE316]'
- en: '[PRE317]'
  id: totrans-561
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE317]'
- en: '[PRE318]'
  id: totrans-562
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE318]'
- en: '[PRE319]'
  id: totrans-563
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE319]'
- en: '[PRE320]'
  id: totrans-564
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE320]'
- en: '[PRE321]'
  id: totrans-565
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE321]'
- en: 'Instantiate the datasets for the train and test sets and the data loaders with
    a batch size of `64`:'
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为训练集和测试集实例化数据集，并使用批次大小`64`实例化数据加载器：
- en: '[PRE322]'
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE322]'
- en: '[PRE323]'
  id: totrans-568
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE323]'
- en: '[PRE324]'
  id: totrans-569
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE324]'
- en: '[PRE325]'
  id: totrans-570
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE325]'
- en: '[PRE326]'
  id: totrans-571
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE326]'
- en: '[PRE327]'
  id: totrans-572
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE327]'
- en: '[PRE328]'
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE328]'
- en: '[PRE329]'
  id: totrans-574
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE329]'
- en: 'Define the neural network architecture – here, there are 3 hidden layers of
    128 units (by default) and a dropout probability set to 25% applied to all the
    hidden layers:'
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经网络架构——这里有3个隐藏层，每层有128个单元（默认为128），并且对所有隐藏层应用了25%的丢弃概率：
- en: '[PRE330]'
  id: totrans-576
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE330]'
- en: '[PRE331]'
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE331]'
- en: '[PRE332]'
  id: totrans-578
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE332]'
- en: '[PRE333]'
  id: totrans-579
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE333]'
- en: '[PRE334]'
  id: totrans-580
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE334]'
- en: '[PRE335]'
  id: totrans-581
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE335]'
- en: '[PRE336]'
  id: totrans-582
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE336]'
- en: '[PRE337]'
  id: totrans-583
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE337]'
- en: '[PRE338]'
  id: totrans-584
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE338]'
- en: '[PRE339]'
  id: totrans-585
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE339]'
- en: '[PRE340]'
  id: totrans-586
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE340]'
- en: '[PRE341]'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE341]'
- en: '[PRE342]'
  id: totrans-588
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE342]'
- en: '[PRE343]'
  id: totrans-589
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE343]'
- en: '[PRE344]'
  id: totrans-590
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE344]'
- en: '[PRE345]'
  id: totrans-591
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE345]'
- en: '[PRE346]'
  id: totrans-592
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE346]'
- en: '[PRE347]'
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE347]'
- en: '[PRE348]'
  id: totrans-594
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE348]'
- en: '[PRE349]'
  id: totrans-595
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE349]'
- en: '[PRE350]'
  id: totrans-596
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE350]'
- en: '[PRE351]'
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE351]'
- en: '[PRE352]'
  id: totrans-598
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE352]'
- en: '[PRE353]'
  id: totrans-599
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE353]'
- en: '[PRE354]'
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE354]'
- en: '[PRE355]'
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE355]'
- en: 'Here, dropout is added in two steps:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，丢弃操作分两步添加：
- en: Instantiate an `nn.Dropout(p=dropout)` class in the constructor, having the
    provided dropout probability
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构造函数中实例化一个`nn.Dropout(p=dropout)`类，传入丢弃概率
- en: Apply the dropout layer (defined in the constructor) after the activation function
    for each hidden layer with `x =` `self.dropout(x)`
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个隐藏层的激活函数之后应用丢弃层（在构造函数中定义），`x =` `self.dropout(x)`
- en: Note
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In the case of a ReLU activation function, setting the dropout before or after
    the activation function won’t change the output. For other activation functions
    such as the sigmoid, though, this makes a difference.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ReLU激活函数来说，将丢弃层设置在激活函数之前或之后不会改变输出。但对于其他激活函数，如sigmoid，这会产生不同的结果。
- en: 'Instantiate the model with the right input shape of `64` (8x8 pixels) and a
    dropout of `0` since we want to check the results without dropout first. Check
    the forward propagation works properly on a given random tensor:'
  id: totrans-607
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正确的输入形状`64`（8x8像素）实例化模型，并且由于我们希望先检查没有丢弃的结果，所以设置丢弃率为`0`。检查前向传播是否在给定的随机张量上正常工作：
- en: '[PRE356]'
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE356]'
- en: '[PRE357]'
  id: totrans-609
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE357]'
- en: '[PRE358]'
  id: totrans-610
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE358]'
- en: '[PRE359]'
  id: totrans-611
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE359]'
- en: '[PRE360]'
  id: totrans-612
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE360]'
- en: '[PRE361]'
  id: totrans-613
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE361]'
- en: '[PRE362]'
  id: totrans-614
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE362]'
- en: 'The output of this code should look like the following:'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码的输出应该如下所示：
- en: '[PRE363]'
  id: totrans-616
  prefs: []
  type: TYPE_PRE
  zh: '[PRE363]'
- en: 'Define the loss function as the cross-entropy loss and the optimizer as `Adam`:'
  id: totrans-617
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将损失函数定义为交叉熵损失，并将优化器设置为`Adam`：
- en: '[PRE364]'
  id: totrans-618
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE364]'
- en: '[PRE365]'
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE365]'
- en: 'Train the neural network on 500 epochs using the `train_model` function available
    in the GitHub repository. For each epoch, we store and compute the loss and the
    accuracy for both the training and test sets:'
  id: totrans-620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用GitHub仓库中可用的`train_model`函数，在500个周期内训练神经网络。每个周期，我们都存储并计算训练集和测试集的损失和精度：
- en: '[PRE366]'
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE366]'
- en: '[PRE367]'
  id: totrans-622
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE367]'
- en: '[PRE368]'
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE368]'
- en: '[PRE369]'
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE369]'
- en: '[PRE370]'
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE370]'
- en: 'After 500 epochs, you should get an output like this:'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 在500个周期后，你应该得到如下输出：
- en: '[PRE371]'
  id: totrans-627
  prefs: []
  type: TYPE_PRE
  zh: '[PRE371]'
- en: 'Plot the cross-entropy loss for both the training and test sets as a function
    of the epoch number:'
  id: totrans-628
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制训练集和测试集的交叉熵损失与周期数的关系：
- en: '[PRE372]'
  id: totrans-629
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE372]'
- en: '[PRE373]'
  id: totrans-630
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE373]'
- en: '[PRE374]'
  id: totrans-631
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE374]'
- en: '[PRE375]'
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE375]'
- en: '[PRE376]'
  id: totrans-633
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE376]'
- en: '[PRE377]'
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE377]'
- en: 'Here is the plot for it:'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是它的图示：
- en: '![Figure 7.12 – Cross-entropy loss as a function of the epoch (note the slight
    divergence between train and test sets)](img/B19629_07_12.jpg)'
  id: totrans-636
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – 交叉熵损失作为周期的函数（注意训练集和测试集之间的轻微偏差）](img/B19629_07_12.jpg)'
- en: Figure 7.12 – Cross-entropy loss as a function of the epoch (note the slight
    divergence between train and test sets)
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – 交叉熵损失作为周期的函数（注意训练集和测试集之间的轻微偏差）
- en: 'Plotting the accuracy will show the equivalent results:'
  id: totrans-638
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制精度图将展示等效结果：
- en: '[PRE378]'
  id: totrans-639
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE378]'
- en: '[PRE379]'
  id: totrans-640
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE379]'
- en: '[PRE380]'
  id: totrans-641
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE380]'
- en: '[PRE381]'
  id: totrans-642
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE381]'
- en: '[PRE382]'
  id: totrans-643
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE382]'
- en: '[PRE383]'
  id: totrans-644
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE383]'
- en: 'Here is the plot for it:'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是它的图示：
- en: '![Figure 7.13 – Accuracy as a function of the epoch; we can again notice the
    overfitting](img/B19629_07_13.jpg)'
  id: totrans-646
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 精度作为周期的函数；我们再次可以看到过拟合现象](img/B19629_07_13.jpg)'
- en: Figure 7.13 – Accuracy as a function of the epoch; we can again notice the overfitting
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 精度作为周期的函数；我们再次可以看到过拟合现象
- en: The final accuracy is about 98% on the train set and only about 95% on the test
    set, showing overfitting. Let’s try now to add dropout to reduce this overfitting.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 最终精度在训练集上大约为98%，而在测试集上仅为95%左右，显示出过拟合现象。现在我们尝试添加丢弃层来减少过拟合。
- en: With dropout
  id: totrans-649
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用丢弃层
- en: 'In this part, we will simply restart from *step 7*, but with dropout, and then
    compare the results:'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分，我们将简单地从*步骤 7*开始，但使用丢弃层，并与之前的结果进行比较：
- en: 'Instantiate the model with an input share of `64` and a dropout probability
    of 25%. A probability of 25% means that during the training, in each of the hidden
    layers, about 32 randomly selected neurons will be ignored. Instantiate a fresh
    optimizer, still using `Adam`:'
  id: totrans-651
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用`64`作为输入共享，25%的dropout概率实例化模型。25%的概率意味着在训练过程中，在每一层隐藏层中，大约会有32个神经元被随机忽略。重新实例化一个新的优化器，依然使用`Adam`：
- en: '[PRE384]'
  id: totrans-652
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE384]'
- en: '[PRE385]'
  id: totrans-653
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE385]'
- en: '[PRE386]'
  id: totrans-654
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE386]'
- en: 'Train the neural network again for 500 epochs, while storing the train and
    test loss and accuracy:'
  id: totrans-655
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次训练神经网络500个周期，同时记录训练和测试的损失值及准确度：
- en: '[PRE387]'
  id: totrans-656
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE387]'
- en: '[PRE388]'
  id: totrans-657
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE388]'
- en: '[PRE389]'
  id: totrans-658
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE389]'
- en: '[PRE390]'
  id: totrans-659
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE390]'
- en: '[PRE391]'
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE391]'
- en: '[PRE392]'
  id: totrans-661
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE392]'
- en: 'Plot the train and test losses again as a function of the epoch:'
  id: totrans-662
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次绘制训练和测试损失随周期变化的图表：
- en: '[PRE393]'
  id: totrans-663
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE393]'
- en: '[PRE394]'
  id: totrans-664
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE394]'
- en: '[PRE395]'
  id: totrans-665
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE395]'
- en: '[PRE396]'
  id: totrans-666
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE396]'
- en: '[PRE397]'
  id: totrans-667
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE397]'
- en: '[PRE398]'
  id: totrans-668
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE398]'
- en: 'Here is the plot for it:'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的图表：
- en: '![Figure 7.14 – Cross-entropy loss as a function of the epoch, with reduced
    divergence thanks to dropout](img/B19629_07_14.jpg)'
  id: totrans-670
  prefs: []
  type: TYPE_IMG
  zh: '![图7.14 – 交叉熵损失随周期变化，得益于dropout减少了发散](img/B19629_07_14.jpg)'
- en: Figure 7.14 – Cross-entropy loss as a function of the epoch, with reduced divergence
    thanks to dropout
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 – 交叉熵损失随周期变化，得益于dropout减少了发散
- en: We face a different behavior here than seen previously. The train and test losses
    do not seem to grow apart too much with the epochs. During the initial 100 epochs,
    the test loss is marginally lower than the train loss, but afterward, the train
    loss decreases further, indicating slight overfitting of the model.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里观察到与之前不同的行为。训练和测试损失似乎不会随着周期的增加而相差太多。在最初的100个周期中，测试损失略低于训练损失，但之后训练损失进一步减少，表明模型轻微过拟合。
- en: 'Finally, plot the train and test accuracy as a function of the epoch:'
  id: totrans-673
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，绘制训练和测试准确度随周期变化的图表：
- en: '[PRE399]'
  id: totrans-674
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE399]'
- en: '[PRE400]'
  id: totrans-675
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE400]'
- en: '[PRE401]'
  id: totrans-676
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE401]'
- en: '[PRE402]'
  id: totrans-677
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE402]'
- en: '[PRE403]'
  id: totrans-678
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE403]'
- en: '[PRE404]'
  id: totrans-679
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE404]'
- en: 'Here is the plot for it:'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的图表：
- en: '![Figure 7.15 – Accuracy as a function of the epoch (the overfitting is largely
    reduced thanks to dropout)](img/B19629_07_15.jpg)'
  id: totrans-681
  prefs: []
  type: TYPE_IMG
  zh: '![图7.15 – 准确度随周期变化（得益于dropout，过拟合大幅度减少）](img/B19629_07_15.jpg)'
- en: Figure 7.15 – Accuracy as a function of the epoch (the overfitting is largely
    reduced thanks to dropout)
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – 准确度随周期变化（得益于dropout，过拟合大幅度减少）
- en: We have a train accuracy of 99% against the 98% seen previously. More interestingly,
    the test accuracy climbed to 97%, from 95% previously, effectively regularizing
    and reducing the overfitting.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练准确度达到了99%，相比之前的98%有所提升。更有趣的是，测试准确度也从之前的95%上升到97%，有效地实现了正则化并减少了过拟合。
- en: There’s more...
  id: totrans-684
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Although dropout is not always foolproof, it has been demonstrated to be an
    effective regularization technique, particularly when training large networks
    on small datasets. More about this can be found in the publication *Improving
    neural networks by preventing co-adaptation of feature detectors*, by Hinton et
    al. This publication can be found here on `arxiv`: [https://arxiv.org/abs/1207.0580](https://arxiv.org/abs/1207.0580).'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管dropout并非万无一失，但它已被证明是一种有效的正则化技术，尤其是在对小数据集进行大规模网络训练时。有关更多内容，可以参考Hinton等人发表的论文《*通过防止特征检测器的共适应来改进神经网络*》。这篇论文可以在`arxiv`上找到：[https://arxiv.org/abs/1207.0580](https://arxiv.org/abs/1207.0580)。
- en: See also
  id: totrans-686
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The official location of the `digits` dataset: [https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits)'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`digits`数据集的官方地址：[https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits)'
- en: 'The PyTorch documentation about dropout: [https://pytorch.org/docs/stable/generated/torch.nn.Dropout.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.xhtml)'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于dropout的PyTorch文档：[https://pytorch.org/docs/stable/generated/torch.nn.Dropout.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.xhtml)
