- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Understanding Key Parameters and Their Impact on Generated Responses
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解关键参数及其对生成响应的影响
- en: In the previous chapter, we learned that the OpenAI API is not just one endpoint
    but also a collection of various endpoints. These endpoints are triggered with
    `model` and `messages` – and we’ve mainly seen how changing the messages parameter
    impacts the generated response. However, there is a vast collection of optional
    parameters that influence the behavior of the API, such as temperature, N, and
    the maximum number of tokens.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学到了 OpenAI API 不仅仅是一个端点，它还是一个由多个端点组成的集合。这些端点通过`model`和`messages`触发——我们主要看到的是如何通过改变消息参数来影响生成的响应。然而，还有许多可选参数影响
    API 的行为，比如温度、N 和最大令牌数。
- en: In this chapter, we will explore these optional key parameters and understand
    how they influence the generated response. **Parameters** are like the dials and
    knobs you’d find on a complex machine. By adjusting these dials and knobs, you
    can change the behavior of the machine to your liking. Similarly, in the realm
    of ChatGPT, parameters allow us to tweak the finer details of the model’s behavior,
    influencing how it processes input and crafts its output. Each of these plays
    a unique role in shaping the response from OpenAI.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索这些可选的关键参数，并理解它们如何影响生成的响应。**参数**就像你在复杂机器上找到的旋钮和按钮。通过调整这些旋钮和按钮，你可以根据自己的喜好改变机器的行为。同样，在
    ChatGPT 的领域中，参数允许我们调整模型行为的细节，影响它如何处理输入并生成输出。每个参数在塑造 OpenAI 的响应中扮演着独特的角色。
- en: By the end of this chapter, you will know how these parameters can be adjusted
    to better suit your specific needs, how they affect the quality, length, and style
    of the output, and how to make effective use of them to get the most desirable
    results. Learning this is important, as these parameters will need to change as
    we begin integrating the API for different use cases in intelligent applications,
    and understanding how generated responses change with these parameters will enable
    us to determine the correct settings.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将知道如何调整这些参数以更好地满足你的特定需求，了解它们如何影响输出的质量、长度和风格，并学会如何有效使用它们以获得最理想的结果。学习这些内容很重要，因为随着我们开始将
    API 集成到智能应用程序的不同用例中，这些参数需要进行调整，理解生成的响应如何随这些参数变化，将帮助我们确定正确的设置。
- en: 'Specifically, we will cover the following recipes, each of which will focus
    on a key parameter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将介绍以下教程，每个教程将聚焦一个关键参数：
- en: Changing the model parameter and understanding its impact on generated responses
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改模型参数并理解其对生成响应的影响
- en: Controlling the number of generated responses using the n parameter
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 n 参数控制生成响应的数量
- en: Determining the randomness and creativity of generated responses using the temperature
    parameter
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用温度参数来确定生成响应的随机性和创造性
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: All the recipes in this chapter require you to have access to the OpenAI API
    (via a generated API key) and have an API client installed, such as Postman. You
    can refer to the [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021) recipe *Making
    OpenAI API requests with Postman* for more information on how to obtain your API
    key and set up Postman.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有教程都要求你能够访问 OpenAI API（通过生成的 API 密钥），并安装了 API 客户端，如 Postman。你可以参考[*第 1
    章*](B21007_01.xhtml#_idTextAnchor021)中的教程*使用 Postman 发起 OpenAI API 请求*，了解如何获取你的
    API 密钥并设置 Postman。
- en: Changing the model parameter and understanding its impact on generated responses
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更改模型参数并理解其对生成响应的影响
- en: In both [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021) and [*Chapter 2*](B21007_02.xhtml#_idTextAnchor044),
    the chat completion requests were made using both the model and messages parameters,
    with `model` always being equal to the `gpt-3.5-turbo` value. We essentially ignored
    the model parameter. However, this parameter likely has the biggest impact on
    the generated responses of any other parameter. Contrary to popular belief, the
    OpenAI API is not just one model; it’s powered by a diverse set of models with
    different capabilities and price points.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 1 章*](B21007_01.xhtml#_idTextAnchor021)和[*第 2 章*](B21007_02.xhtml#_idTextAnchor044)中，聊天完成请求是使用模型参数和消息参数发起的，其中`model`始终等于`gpt-3.5-turbo`的值。我们基本上忽略了模型参数。然而，这个参数可能是所有参数中对生成响应影响最大的。与普遍看法相反，OpenAI
    API 不仅仅是一个模型；它由多个不同能力和价格点的模型组成。
- en: In this recipe, we will cover two main models (*GPT-3.5* and *GPT-4*), learn
    how to change the `model` parameter, and observe how the generated responses vary
    between these two models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将介绍两个主要模型（*GPT-3.5* 和 *GPT-4*），学习如何更改 `model` 参数，并观察这两个模型生成的响应有何不同。
- en: Getting ready
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Ensure you have an OpenAI Platform account with available usage credits. If
    you don’t, please follow the *Setting up your OpenAI Playground environment* recipe
    in [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你有一个具有可用使用额度的 OpenAI 平台账户。如果没有，请参阅 [*第 1 章*](B21007_01.xhtml#_idTextAnchor021)
    中的 *设置 OpenAI Playground 环境* 配方。
- en: Furthermore, ensure that you have Postman installed, that you have created a
    new workspace, that you have created a new HTTP request, and that `Headers` for
    that request are correctly configured. This is important because, without the
    `Authorization` configured, you will not be able to use the API. If you don’t
    have Postman installed and configured as mentioned, follow the *Making OpenAI
    API requests with Postman* recipe in [*Chapter 1*](B21007_01.xhtml#_idTextAnchor021).
    However, if you do not remember, *steps 1–4* in the next section explain the configuration
    process.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请确保你已安装 Postman，已创建一个新的工作区，已创建一个新的 HTTP 请求，并且该请求的 `Headers` 配置正确。这非常重要，因为如果没有正确配置
    `Authorization`，你将无法使用 API。如果你没有按照上述步骤安装和配置 Postman，请参阅 [*第 1 章*](B21007_01.xhtml#_idTextAnchor021)
    中的 *使用 Postman 发送 OpenAI API 请求* 配方。 如果你记不起来了，接下来的 *步骤 1–4* 会解释配置过程。
- en: All the recipes in this chapter will have this same requirement.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有配方都有相同的要求。
- en: How to do it…
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: In your Postman workspace, select the **New** button on the top-left menu bar,
    and then select **HTTP** from the list of options that appears. This will create
    a new **Untitled Request**.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的 Postman 工作区，选择左上角菜单栏中的 **New** 按钮，然后从弹出的选项中选择 **HTTP**。这将创建一个新的 **Untitled
    Request**。
- en: Change the HTTP request type from **GET** to **POST** by selecting the **Method**
    drop-down menu (by default, it will be set to **GET**).
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择 **Method** 下拉菜单（默认设置为 **GET**），将 HTTP 请求类型从 **GET** 更改为 **POST**。
- en: 'Enter the following URL as the endpoint for chat completions: [https://api.openai.com/v1/chat/completions](https://api.openai.com/v1/chat/completions).'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入以下 URL 作为聊天完成的端点：[https://api.openai.com/v1/chat/completions](https://api.openai.com/v1/chat/completions)。
- en: 'Select **Headers** in the sub-menu, and add the following key-value pairs into
    the table below it:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在子菜单中选择 **Headers**，并将以下键值对添加到下方的表格中：
- en: '| *Key* | *Value* |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| *Key* | *Value* |'
- en: '| --- | --- |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `Content-Type` | `application/json` |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `Content-Type` | `application/json` |'
- en: '| `Authorization` | `Bearer <your API` `key here>` |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `Authorization` | `Bearer <your API` `key here>` |'
- en: 'Select **Body** in the sub-menu, and then select **raw** for the request type.
    Enter the following request body, which details to OpenAI the prompt, system message,
    chat log, and the set of other parameters that it needs to use to generate a completion
    response:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在子菜单中选择 **Body**，然后选择 **raw** 作为请求类型。输入以下请求体，这些内容将向 OpenAI 说明提示、系统消息、聊天日志和生成完成响应所需的其他参数：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '5. After sending the HTTP request, you should see the following response from
    the OpenAI API. Note that your response may be different. The section of the HTTP
    response that we particularly want to take note of is the `content` value:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 发送 HTTP 请求后，你应该看到来自 OpenAI API 的以下响应。请注意，你的响应可能会有所不同。我们特别需要注意的 HTTP 响应部分是
    `content` 值：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '6. Let’s now repeat the HTTP request in *step 4* and keep everything else consistent,
    but modify the `model` parameter. Specifically, we will change the value of that
    parameter to `gpt-4`. Enter the following for the endpoint and request body, and
    then click **Send**:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 6. 现在，让我们重复 *步骤 4* 中的 HTTP 请求，并保持其他内容一致，但修改 `model` 参数。具体来说，我们将把该参数的值更改为 `gpt-4`。输入以下端点和请求体，然后点击
    **Send**：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '7. You should see the following similar response from the OpenAI API. Note
    that the response is far different than what we received earlier. Notably, it
    more closely matches the instruction in the prompt of generating six five-letter
    words:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 你应该看到来自 OpenAI API 的类似响应。请注意，这个响应与我们之前收到的响应有很大不同。特别地，它更接近于生成六个五个字母单词的提示要求：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '8. Repeat *steps 1–4*, but change the `content` parameter inside `messages`
    to the following prompt instead: `How many chemicals exist in cigarettes, how
    many of them are known to be harmful, and how many are known to cause cancer?
    Respond with just the numbers,` `nothing else`.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 8. 重复*步骤1-4*，但将`messages`中的`content`参数改为以下提示：`香烟中有多少种化学物质，多少种已知对人体有害，多少种已知会导致癌症？只回答数字，`
    `其他一律不提`。
- en: Again, execute one chat completion request where the `model` parameter is `gpt-3.5-turbo`
    and one where the `model` parameter is `gpt-4`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 再次执行一个聊天完成请求，其中`model`参数为`gpt-3.5-turbo`，另一个请求中`model`参数为`gpt-4`。
- en: '9. The following are extracts of the HTTP response that I received using GPT-3.5-turbo
    and GPT-4:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 9. 以下是我使用GPT-3.5-turbo和GPT-4收到的HTTP响应摘录：
- en: When **model** = **gpt-3.5-turbo:**
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当**model** = **gpt-3.5-turbo**时：
- en: '[PRE4]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When **model** = **gpt-4:**
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当**model** = **gpt-4**时：
- en: '[PRE5]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '10. Repeat *steps 4–7*, but change the `content` parameter inside `messages`
    to the following logical question prompt instead:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 10. 重复*步骤4-7*，但将`messages`中的`content`参数改为以下逻辑问题提示：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that HTTP requests, with the request body being in the JSON format, cannot
    handle multiline strings. As a result, if you need to write multiline strings
    into any of the API parameters (such as `messages` in this case), use the line
    break characters instead (`\n`):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，HTTP请求中，若请求体为JSON格式，无法处理多行字符串。因此，如果需要将多行字符串写入任何API参数（例如此处的`messages`），请改用换行字符（`\n`）：
- en: For example,
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，
- en: '`"`'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`"`'
- en: '`Line 1`'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`Line 1`'
- en: '`Line 2`'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`Line 2`'
- en: '`"`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`"`'
- en: would become
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 会变成
- en: '`"Line` `1\nLine 2"`'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`"Line` `1\nLine 2"`'
- en: '11. The following are extracts of the HTTP response that I received:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 11. 以下是我收到的HTTP响应摘录：
- en: When **model** = **gpt-3.5-turbo:**
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当**model** = **gpt-3.5-turbo**时：
- en: '[PRE7]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When **model** = **gpt-4:**
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当**model** = **gpt-4**时：
- en: '[PRE8]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: How it works…
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'In this recipe, we observed three different examples of how changing the `model`
    parameter affected the generated text. The following table summarizes the different
    responses generated by OpenAI, based on different model parameters:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们观察了三种不同的`model`参数变化如何影响生成的文本。下表总结了OpenAI基于不同模型参数生成的不同响应：
- en: '| *Prompt* | *Response when model =* *gpt-3.5-turbo* | *Response when model
    =* *gpt-4* |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| *提示* | *当model =* *gpt-3.5-turbo*时的响应 | *当model =* *gpt-4*时的响应 |'
- en: '| --- | --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Describe Donald Trump’s time in office in a sentence that has six five-letter
    words. Remember, each word must have five letters |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 用六个五个字母的单词描述唐纳德·特朗普的任期。记住，每个单词必须有五个字母 |'
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| How many chemicals exist in cigarettes, how many of them are known to be
    harmful, and how many are known to cause cancer? Respond with just the numbers,
    nothing else |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 香烟中有多少种化学物质，多少种已知对人体有害，多少种已知会导致癌症？只回答数字，其他一律不提 |'
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Which conclusion follows from the statement with absolute certainty?'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '| 哪个结论可以从该陈述中得到绝对的确定性？'
- en: None of the stamp collectors is an architect.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有一个邮票收藏家是建筑师。
- en: All the drones are stamp collectors.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有的无人机都是邮票收藏家。
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE13]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: In all cases, the `gpt-4` model produced more accurate results than `gpt-3.5-turbo`.
    For example, in the first prompt about describing *Donald Trump’s time in office*,
    the `gpt-3.5-turbo` model did not understand that it should only use five-letter
    words, whereas `gpt-4` was able to answer it successfully.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有情况下，`gpt-4`模型比`gpt-3.5-turbo`模型生成的结果更准确。例如，在关于描述*唐纳德·特朗普任期*的第一个提示中，`gpt-3.5-turbo`模型没有理解应只使用五个字母的单词，而`gpt-4`则能够成功回答。
- en: GPT-4 versus GPT-3.5
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-4与GPT-3.5
- en: Why is that the case? The inner workings of the two models are different. In
    neural network models such as GPT, a parameter is a single numerical value that
    combines with others, which perform calculations that turn inputs (such as a prompt)
    into output data (such as a chat completion response). The larger the number of
    parameters, the greater the capacity for the model to accurately capture patterns
    in the data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会这样呢？这两个模型的内部工作原理不同。在像GPT这样的神经网络模型中，参数是一个单一的数值，它与其他参数组合在一起，通过计算将输入（如提示）转化为输出数据（如聊天完成响应）。参数的数量越大，模型捕捉数据模式的能力越强。
- en: The GPT-3.5 set of models was trained with 175 billion parameters, whereas the
    GPT-4 set of models is estimated to be trained on more than 100 trillion parameters
    (collectively over an ensemble of smaller models), many order of magnitudes higher
    ([https://www.pcmag.com/news/the-new-chatgpt-what-you-get-with-gpt-4-vs-gpt-35](https://www.pcmag.com/news/the-new-chatgpt-what-you-get-with-gpt-4-vs-gpt-35)).
    The neural network behind GPT-4 is far denser, enabling it to understand nuances
    and answer more accurately.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3.5 模型集经过了 1750 亿个参数的训练，而 GPT-4 模型集预计经过了超过 100 万亿个参数的训练（通过多个较小的模型集合），这个数量比之前大了许多倍（[https://www.pcmag.com/news/the-new-chatgpt-what-you-get-with-gpt-4-vs-gpt-35](https://www.pcmag.com/news/the-new-chatgpt-what-you-get-with-gpt-4-vs-gpt-35)）。GPT-4
    背后的神经网络更为密集，使其能够理解更多细微的差异并给出更准确的回答。
- en: GPT models typically struggle with very complex and long instructions. For example,
    in the cigarette question, the instruction was clearly to `respond with just the
    numbers, nothing else`. GPT-3.5 provided a suitable answer, but not in the correct
    format, whereas the answer returned by GPT-4 was in the correct format.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型通常在处理非常复杂和冗长的指令时会遇到困难。例如，在香烟问题中，指令明确要求“`只回复数字，其他不要`”。GPT-3.5 提供了一个合适的答案，但格式不正确，而
    GPT-4 返回的答案则符合正确的格式。
- en: 'In general, GPT-4 is more reliable and can handle much more nuanced instructions
    than GPT-3.5\. It is worth noting that the distinction can be subtle, even non-existent,
    for primarily easy tasks. To discern these differences, the two models were tested
    on a variety of benchmarks and common exams, which demonstrates the power of GPT-4\.
    You can learn about these test results here: [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4).
    Overall, GPT-4 outperformed GPT-3.5 on various standardized exams, such as AP
    calculus, AP English literature, and LSAT.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，GPT-4 更可靠，并且能够处理比 GPT-3.5 更复杂的指令。值得注意的是，这一区别对于主要是简单任务的情况可能很微妙，甚至不存在。为了辨别这些差异，两种模型在多种基准测试和常见考试中进行了测试，结果展示了
    GPT-4 的强大。你可以在这里了解这些测试结果：[https://openai.com/research/gpt-4](https://openai.com/research/gpt-4)。总的来说，GPT-4
    在各种标准化考试中超越了 GPT-3.5，例如 AP 微积分、AP 英语文学和 LSAT。
- en: 'Other differences between GPT-4 and GPT-3.5 include the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 和 GPT-3.5 之间的其他差异包括：
- en: '**Memory and context size**: GPT-4 can retain more memory and has a bigger
    context window ([https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)),
    which means it can accept much larger and more complex prompts than GPT-3.5\.
    The **context window** refers to the amount of recent input (in terms of tokens
    or chunks of text) the model can consider when generating a response. Imagine
    reading a paragraph from the middle of a book; the more sentences you can see
    and remember, the better you understand that paragraph’s context. Similarly, with
    a larger context window, GPT-4 can *see* and *remember* more of the previous input,
    allowing it to generate more contextually relevant responses.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆和上下文窗口**：GPT-4 可以保留更多记忆，并且具有更大的上下文窗口（[https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)），这意味着它可以处理比
    GPT-3.5 更大、更复杂的提示。**上下文窗口**指的是模型在生成回答时，能够考虑的最近输入的数量（以标记或文本块为单位）。可以想象你在阅读一本书中间的一段文字；你能看到和记住的句子越多，你对这段文字的理解就越好。同样，拥有更大的上下文窗口，GPT-4
    可以*看到*并*记住*更多的先前输入，从而生成更具上下文相关性的回答。'
- en: '**Visual input**: GPT-4 can accept both text and images, whereas GPT-3.5 is
    text-only.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视觉输入**：GPT-4 可以同时处理文本和图像，而 GPT-3.5 仅限于文本。'
- en: '**Language**: Both GPT-3.5 and GPT-4 have multilingual capabilities, meaning
    they can understand, interpret, and respond in languages other than English. However,
    while GPT-3.5 can work in multiple languages, GPT-4 offers enhanced linguistic
    finesse and can go beyond simple speech in other languages.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言能力**：GPT-3.5 和 GPT-4 都具有多语言能力，意味着它们可以理解、解释并用英语以外的语言作答。然而，尽管 GPT-3.5 可以处理多种语言，但
    GPT-4 提供了更为精细的语言能力，能够在其他语言中超越简单的语言表达。'
- en: '**Alignment**: GPT-4 has been more *aligned*, meaning it has a bias to not
    provide harmful advice, buggy code, or inaccurate information, from human-based
    adversarial testing. In this context, **alignment** refers to the process of adjusting
    GPT-4’s responses to be more in line with ethical and safety standards, reducing
    the likelihood of it providing harmful advice, buggy code, or inaccurate information.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对齐性**：GPT-4 已经过更多的*对齐*，意味着它倾向于不提供有害的建议、错误的代码或不准确的信息，这得益于基于人类对抗测试的优化。在此背景下，**对齐性**指的是调整
    GPT-4 的回答，使其更符合伦理和安全标准，从而降低提供有害建议、错误代码或不准确信息的可能性。'
- en: Cost considerations
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 成本考虑
- en: One important difference between GPT-4 and GPT-3.5 is cost. GPT-4 charges a
    much higher token rate, which also increases if models with larger context windows
    are chosen.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4 和 GPT-3.5 之间的一个重要区别是费用。GPT-4 的 token 费用高得多，如果选择更大的上下文窗口模型，费用还会增加。
- en: A **token** is a chunk of text that the model reads as input or generates as
    output. These tokens may be a single character, part of a word, or the word itself.
    As a rough rule of thumb, 1 token is equal to 0.75 words ([https://platform.openai.com/docs/introduction/key-concepts](https://platform.openai.com/docs/introduction/key-concepts)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**token** 是模型读取的输入或生成的输出的一部分文本。这些 tokens 可能是一个字符、一个单词的一部分或整个单词。大致来说，1 个 token
    相当于 0.75 个单词（[https://platform.openai.com/docs/introduction/key-concepts](https://platform.openai.com/docs/introduction/key-concepts)）。'
- en: 'When making API requests for chat completions, the response always includes
    the number of tokens that was used in the request, in the `usage` object. For
    example, the following is an excerpt for the response in *step 5*:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行 API 请求以完成聊天时，响应中总是包含请求中使用的 tokens 数量，位于 `usage` 对象中。例如，以下是 *步骤 5* 中响应的摘录：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This tells us that our `Describe Donald Trump''s time in office in a sentence
    that has six five-letter words. Remember, each word must have 5 letters` prompt
    was 33 tokens, and the following response was 12 tokens, making a combined total
    of 45 tokens:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，我们的 `Describe Donald Trump's time in office in a sentence that has six
    five-letter words. Remember, each word must have 5 letters` 提示用了 33 个 tokens，而以下的回应用了
    12 个 tokens，总共 45 个 tokens：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The number of tokens matters for two reasons:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: tokens 数量重要的原因有两个：
- en: Depending on the model chosen, the number of total tokens cannot exceed the
    model’s *max token*, also known as the context window. For GPT-3.5-turbo, this
    is 4,096 tokens. This means that in any API request using that model, the sum
    of *content* in **messages** cannot exceed 4,096 tokens, or approximately 3,000
    words. In comparison, GPT-4 has a sub-model called **gpt-4-32k**, which has a
    context window of 32,768 tokens, or around 24,000 words.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据所选择的模型，总的 tokens 数量不能超过模型的*最大 token*，也称为上下文窗口。对于 GPT-3.5-turbo，该值为 4,096 个
    tokens。这意味着在使用该模型的任何 API 请求中，**消息**的*内容*总和不能超过 4,096 个 tokens，约为 3,000 个单词。相比之下，GPT-4
    有一个子模型叫做 **gpt-4-32k**，其上下文窗口为 32,768 个 tokens，约为 24,000 个单词。
- en: The number of total tokens and the model you use dictates how much you are charged
    for an API request. For example, in *step 5*, we used 45 tokens using the **gpt-3.5-turbo**
    model, which means that the request cost USD 0.0000675\. By comparison, the same
    45 tokens using **gpt-4** would have cost USD 0.00135, which is 20x the cost.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总的 tokens 数量和所使用的模型决定了你为 API 请求付费的金额。例如，在 *步骤 5* 中，我们使用 **gpt-3.5-turbo** 模型时，使用了
    45 个 tokens，这意味着该请求的费用为 0.0000675 美元。相比之下，使用 **gpt-4** 的相同 45 个 tokens 费用为 0.00135
    美元，是原费用的 20 倍。
- en: Decision criteria
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策标准
- en: 'The determination of which model to use in chat completion requests should
    depend on the following factors:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 确定在聊天完成请求中使用哪个模型应考虑以下因素：
- en: '**Context window**: Determine the likely context window of the chat completion
    requests. If your prompts are likely to be over 12,000 words, then you need to
    use GPT-4, as the biggest model underneath GPT-3.5 only has a maximum number of
    16,384 tokens.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文窗口**：确定聊天完成请求的可能上下文窗口。如果你的提示可能超过 12,000 个单词，那么你需要使用 GPT-4，因为 GPT-3.5 以下的最大模型只有
    16,384 个 tokens 的最大值。'
- en: '**Complexity**: Determine the complexity of your chat completion request. In
    general, if it requires nuance understanding and formatting instructions such
    as the first two examples in the recipe, or if it requires complex information
    synthesis and logical problem solving such as the third example in the recipe,
    then you need to use GPT-4\. This is especially the case with any mathematical
    or scientific reasoning – GPT-4 performs far better.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**：确定你的聊天完成请求的复杂性。一般来说，如果它需要细致的理解和格式化指令（如食谱中的前两个示例），或需要复杂的信息综合和逻辑问题解决（如食谱中的第三个示例），那么你需要使用
    GPT-4。这对于任何数学或科学推理尤其如此——GPT-4 的表现要好得多。'
- en: '**Cost**: Evaluate the cost implications of choosing GPT-4 over GPT-3.5\. If
    you use the GPT-4 model with the highest context window, this can be 40x times
    the price of a request using GPT-3.5.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**：评估选择 GPT-4 而非 GPT-3.5 的成本影响。如果你使用具有最大上下文窗口的 GPT-4 模型，这将是使用 GPT-3.5 的请求价格的
    40 倍。'
- en: In general, you should always use and test GPT-3.5 first to see whether it can
    provide suitable chat completions, and then move to GPT-4 if absolutely necessary.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，你应该始终首先使用并测试GPT-3.5，看看它是否能够提供合适的对话补全，然后在绝对必要的情况下再切换到GPT-4。
- en: Overall, the `model` parameter influences the quality of generated responses,
    which is important, as different use cases of API requests will require different
    levels of sophisticated responses.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，`model`参数会影响生成响应的质量，这是非常重要的，因为不同的API请求用例会要求不同层次的复杂响应。
- en: Controlling the number of generated responses using the n parameter
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用n参数控制生成的响应数量
- en: For certain intelligent applications that you build, you want multiple generated
    texts from the same prompt. For example, if we’re building an app that generates
    company slogans, you likely want to generate not just one but also multiple responses
    so that the user can select the best one. The `n` parameter controls how many
    chat completion choices to generate for each input message. It can also control
    the number of images that are generated when using the *Images* endpoint.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于你构建的某些智能应用，你可能需要从相同的提示生成多个文本。例如，如果我们正在构建一个生成公司口号的应用，你可能不只希望生成一个响应，而是多个响应，以便用户可以选择最佳的一个。`n`参数控制每个输入消息生成多少个对话补全选择。当使用*Images*端点时，它也可以控制生成的图像数量。
- en: In this recipe, we will see how the `n` parameter affects the number of generated
    responses and understand the different use cases for it.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将看到`n`参数如何影响生成的响应数量，并理解它的不同用例。
- en: How to do it…
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'In Postman, enter the following URL as the endpoint for chat completions: [https://api.openai.com/v1/chat/completions](https://api.openai.com/v1/chat/completions).'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Postman中，输入以下URL作为对话补全的端点：[https://api.openai.com/v1/chat/completions](https://api.openai.com/v1/chat/completions)。
- en: 'In the request body, type in the following and click **Send**. Note that we
    have added t**h**e **n** parameter and set it to the default value of **1** explicitly:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在请求正文中，输入以下内容并点击**发送**。请注意，我们已添加**h**和**n**参数，并明确将其设置为默认值**1**：
- en: '[PRE17]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After sending the HTTP request, you should see the following (similar, but
    not exact) response from the OpenAI API:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发送HTTP请求后，你应该看到来自OpenAI API的以下响应（相似但不完全相同）：
- en: '[PRE18]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we’ll repeat the request in *step 2*, but let’s change the **n** parameter
    to a value of **3**. After sending the HTTP request, we get the following response.
    Note that there are now three separate objects or responses within **choices**.
    We effectively received three different generated responses to the prompt:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将重复*第2步*中的请求，但将**n**参数改为**3**。发送HTTP请求后，我们得到以下响应。请注意，现在在**choices**中有三个独立的对象或响应。我们实际上收到了三个不同的生成响应：
- en: '[PRE19]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, let’s generate images and observe how the **n** parameter affects the
    number of images returned. In Postman, enter the following for the endpoint: [https://api.openai.com/v1/images/generations](https://api.openai.com/v1/images/generations).
    In the request body, type in the following, and then click **Send**:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们生成图像并观察**n**参数如何影响返回的图像数量。在Postman中，输入以下端点：[https://api.openai.com/v1/images/generations](https://api.openai.com/v1/images/generations)。在请求正文中，输入以下内容，然后点击**发送**：
- en: '[PRE20]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After sending the HTTP request, you should see the following response from
    the OpenAI API. Notably, you should see three different URLs, each corresponding
    to a generated image. The URLs in the following code block have been artificially
    condensed. After copying and pasting the URLs into your browser, you should see
    images of ice cream:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发送HTTP请求后，你应该看到来自OpenAI API的以下响应。特别地，你应该看到三个不同的URL，每个URL对应一个生成的图像。以下代码块中的URL已被人工简化。将这些URL复制并粘贴到浏览器中，你应该会看到冰淇淋的图像：
- en: '[PRE21]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: "![Figure 3.1 – Output of the OpenAI image endpoint (\uFEFFn=3)](img/B21007_03_1.jpg)"
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – OpenAI图像端点的输出（n=3）](img/B21007_03_1.jpg)'
- en: Figure 3.1 – Output of the OpenAI image endpoint (n=3)
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – OpenAI图像端点的输出（n=3）
- en: How it works…
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The `n` parameter simply specifies the number of generated responses from the
    OpenAI API. For chat completions, it can be any integer; this means you can ask
    the API to return thousands and thousands of responses. For image generations,
    this parameter has a max value of *10*, meaning you can only generate up to 10
    images per request.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`n`参数仅指定从OpenAI API生成的响应数量。对于对话补全，它可以是任何整数；这意味着你可以要求API返回成千上万的响应。对于图像生成，这个参数的最大值是*10*，意味着每次请求最多只能生成10张图像。'
- en: Applications of n
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: n的应用
- en: 'The applications of having an `n` parameter are very broad – it’s often useful
    to have a parameter that controls and repeats generations for the same prompt,
    all in one HTTP request. These include the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`n` 参数的应用非常广泛——通常，具有控制并重复生成相同提示的参数非常有用，且所有操作都能在一个 HTTP 请求中完成。包括以下内容：'
- en: '**Creativity**: For creative apps and tasks such as slogan generation, songwriting,
    or brainstorming, providing a richer set of materials to work from makes it easier
    for users'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创造力**：对于创意类应用和任务，如标语生成、歌曲创作或头脑风暴，提供更多的素材集可以帮助用户更轻松地完成任务。'
- en: '**Redundancy**: Since response generations from the OpenAI API with the same
    prompt can differ wildly, it’s useful to create multiple responses and cross-verify
    the information, especially in mission-critical workflows'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冗余**：由于 OpenAI API 在相同提示下生成的响应可能会有很大差异，因此创建多个响应并交叉验证信息非常有用，特别是在关键任务工作流中。'
- en: '**A/B testing**: Very common in marketing, the **n** parameter enables you
    to create multiple responses that users can experiment with to see which one performs
    better'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A/B 测试**：在营销中非常常见，**n** 参数使你可以生成多个响应，用户可以尝试不同的响应，看看哪一个效果更好。'
- en: Considerations of n
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: n 的考虑
- en: However, multiple generations do generally mean a lower speed and higher cost,
    which are considerations that need to be taken into account before deciding what
    value to set for the `n` parameter. For example, in our recipe, when we requested
    one generation, the cost was *33* tokens (as specified in the response). However,
    when `n = 3`, the total number of tokens jumped to *52* tokens. We learned in
    the previous recipe that the OpenAI API charges based on the total number of tokens
    generated.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多个生成通常意味着较低的速度和较高的成本，这些都是在决定 `n` 参数值时需要考虑的因素。例如，在我们的食谱中，当我们请求生成一个响应时，成本为
    *33* token（如响应中所示）。然而，当 `n = 3` 时，总 token 数跳升到 *52* token。我们在前面的食谱中了解到，OpenAI
    API 根据生成的总 token 数收费。
- en: 'Note that the cost increase is not linear – generating three additional responses
    only cost ~60% more tokens, instead of the expected 3x. This is for two reasons:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，成本增加并不是线性的——生成三个额外的响应仅增加约 60% 的 token，而不是预期的 3 倍。这是由于两个原因：
- en: The number of prompt tokens remains fixed no matter how many generations are
    created, whether it’s 1 or 100
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论生成多少次响应，提示 token 的数量保持不变，无论是 1 次还是 100 次。
- en: The model finds computational savings when it knows to produce multiple completions
    instead of one
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型知道需要生成多个完成项而非单一项时，会产生计算节省。
- en: This is also why using the `n` parameter is far better (from a cost point of
    view) than just executing the HTTP request multiple times. Under the hood, when
    you set `n = 3`, the model in parallel processes the requests during a single
    model inference, leveraging inherent efficiencies. We could have, for example,
    run the HTTP request three times instead of one HTTP request where `n = 3`, but
    that would mean spending ~3x more cost and overhead.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是为什么从成本角度来看，使用 `n` 参数比多次执行 HTTP 请求要好得多的原因。在底层，当你设置 `n = 3` 时，模型会在一次模型推理中并行处理请求，从而利用内在的效率。举例来说，我们本可以执行三次
    HTTP 请求，而不是设置 `n = 3` 的一次请求，但那样会花费约 3 倍的成本和开销。
- en: Overall, the `n` parameter impacts the number of generated responses, which
    is tremendously valuable for particular use cases, resulting in lower costs as
    well.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，`n` 参数影响生成的响应数量，对于特定用例来说，这非常有价值，同时也能降低成本。
- en: Determining the randomness and creativity of generated responses using the temperature
    parameter
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用温度参数来确定生成响应的随机性和创造力。
- en: '**Temperature** is likely to be one of the least understood parameters. Overall,
    it controls the creativity or randomness of text generations. The higher the temperature,
    the more diverse and creative the results will be – even for the same input. In
    practice, the temperature is set based on the use case. Applications where consistent
    and standard generations are needed should use a very low temperature, whereas
    solutions that require creative approaches should opt for higher temperatures.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**温度** 可能是最难理解的参数之一。总体来说，它控制文本生成的创造力或随机性。温度越高，结果就会越多样化和富有创意——即使对于相同的输入。在实际应用中，温度根据使用场景来设置。对于需要一致且标准生成的应用，应使用非常低的温度，而对于需要创造性方法的解决方案，应选择较高的温度。'
- en: In this recipe, we will learn about the temperature parameter, observing how
    it can be used to influence the text generations produced by the OpenAI API.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本配方中，我们将了解温度参数，并观察它如何影响OpenAI API生成的文本。
- en: How to do it…
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'In Postman, enter the following for the endpoint: [https://api.openai.com/v1/chat/completions](https://api.openai.com/v1/chat/completions).
    In the request body, type in the following, and then click **Send**. Our prompt
    is **Explain gravity in one sentence**. Note that we have added the **temperature**
    parameter and set it to the value of **0** explicitly. We will repeat this *three*
    times and record the responses of the **content** parameter for each generation:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Postman中，输入以下端点：[https://api.openai.com/v1/chat/completions](https://api.openai.com/v1/chat/completions)。在请求体中，输入以下内容，然后点击**发送**。我们的提示是**用一句话解释引力**。请注意，我们已经添加了**温度**参数，并将其显式设置为**0**。我们将重复此操作*三*次，并记录每次生成的**内容**参数的响应：
- en: '[PRE22]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, let’s edit our request body and change the **temperature** parameter
    to the highest value possible, which is **2**. Click **Send**, and then repeat
    this three times, recording the responses of the **content** parameter for each
    generation:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们编辑请求体，并将**温度**参数更改为可能的最高值**2**。点击**发送**，然后再次重复三次，记录每次生成的**内容**参数的响应：
- en: '[PRE23]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, let’s repeat *steps 1–2* but use a more creative prompt, such as **Create
    a creative tag line for an AI learning book**. Again, we will first perform a
    chat completion with the temperature parameter equal to **0** three times. Then,
    we will increase the temperature parameter to **2** and run the request three
    times again. The responses of the **content** parameter for each generation are
    listed in the following code blocks. Note that yours will likely differ:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们重复*步骤1-2*，但使用一个更具创意的提示，例如**为一本AI学习书籍创建一个创意标语**。同样，我们首先会将温度参数设置为**0**，然后进行三次聊天生成。接着，我们将温度参数提高到**2**，再进行三次请求。每次生成的**内容**参数的响应会列在以下代码块中。请注意，你的结果可能会有所不同：
- en: '[PRE24]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How it works…
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'As we saw in the recipe, the temperature parameter controls the randomness
    and creativity of the text generation. When the temperature was set very low,
    the API produced very consistent and deterministic results for the same prompt.
    In the first example, gravity was explained in the same exact way for each chat
    completion:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在配方中所看到的，温度参数控制着文本生成的随机性和创造性。当温度设置得非常低时，API会为相同的提示生成非常一致和确定的结果。在第一个示例中，引力在每次聊天完成时都以完全相同的方式进行解释：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'When we increased the temperature, we saw very different, more creative, and
    unexpected responses, such as the following:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提高温度时，我们看到了非常不同、更加富有创意和出人意料的响应，例如以下内容：
- en: '[PRE26]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Think of the temperature setting as the dial on a radio. A lower temperature
    is like tuning the radio to a well-established station where the signal is strong
    and clear, and you get a consistent, expected type of music or talk show. This
    is analogous to the model delivering responses that are reliable, straightforward,
    and closely aligned with the most likely answer.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 把温度设置想象成收音机上的旋钮。较低的温度就像将收音机调到一个信号强且清晰的电台，在这里你会获得一种一致且预期的音乐或脱口秀节目。这类似于模型生成的响应是可靠的、直接的，并且与最可能的答案紧密对齐。
- en: Conversely, a higher temperature is similar to tuning the radio to a frequency
    where you might catch a variety of stations, some clear and some static-filled,
    playing an eclectic mix of genres. This creates an environment where unexpected,
    novel, and varied content comes through. In the context of the language model,
    this means generating more creative, diverse, and sometimes unpredictable responses,
    mirroring the eclectic and varied nature of a radio dial turned toward a less
    defined frequency.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，较高的温度类似于调节收音机到一个频率，在这个频率上你可能会接收到多种不同的电台，一些信号清晰，一些信号杂乱，播放着各种各样的音乐风格。这创造了一个环境，在这里意外、新颖和多变的内容会涌现出来。在语言模型的背景下，这意味着生成更加富有创意、多样化，并且有时是不可预测的响应，就像将收音机调到一个不太明确的频率上，从而接收到各种不同的内容。
- en: Temperature inner working
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 温度的内部工作机制
- en: As we discussed before, when a model generates text, it calculates probabilities
    for the next word based on the prompt and response it has built so far. In practice,
    temperature affects the response by changing the probability distribution of the
    next word.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所讨论的，当模型生成文本时，它会根据已构建的提示和响应计算下一个单词的概率。在实践中，温度通过改变下一个单词的概率分布来影响响应。
- en: With a higher temperature, this distribution becomes flatter, meaning less-probable
    words have a higher chance of being selected. At a lower temperature, the distribution
    becomes more pronounced or *sharper*, meaning the most probable words are likely
    to be chosen every time, which reduces randomness.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用较高的温度时，这个分布会变得更平坦，意味着较不可能的词语有更高的概率被选中。较低的温度则使得分布更加突出或*尖锐*，意味着每次选择的都是最可能的词语，从而减少了随机性。
- en: Decision based on use case
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根据用例做决定
- en: The decision on which temperature to use depends solely on the particular use
    case. In general, there are three categories of this parameter.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用哪种温度的决定完全取决于具体的使用场景。一般来说，这个参数可以分为三类。
- en: '**Low-temperature values (0.0 to 0.8)**: These should be used for primarily
    analytical, factual, or logical tasks so that the model is more deterministic
    and focused. In these use cases, traceability and repeatability is also important,
    and so a lower temperature is better, as it reduces randomness. A lower temperature
    also means adhering to established patterns and conventions, leading to more correct
    answers.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低温度值（0.0 到 0.8）**：这些应主要用于分析性、事实性或逻辑性任务，以使模型更加确定性和集中。在这些用例中，追溯性和可重复性也很重要，因此较低的温度更好，因为它减少了随机性。较低的温度也意味着遵循已建立的模式和惯例，从而产生更正确的答案。'
- en: Examples include generating code, performing data analysis, and answering factual
    questions.
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，生成代码、执行数据分析和回答事实性问题。
- en: '**Medium-temperature values (0.8 to 1.2)**: These should be used for general-purpose
    and chatbot-like tasks, where balancing coherence and creativity is critical.
    This enables the model to be flexible and produce new ideas, but it still remains
    focused to the prompt at-hand.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中等温度值（0.8 到 1.2）**：这些应适用于一般用途和类似聊天机器人的任务，其中平衡连贯性和创造性至关重要。这使得模型更加灵活，可以产生新想法，但仍然能集中在当前提示上。'
- en: Examples include chatbots/conversational agents and Q&A systems.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，聊天机器人/对话代理和问答系统。
- en: '**High-temperature values (1.2 to 2.0)**: These should be used for creative
    writing and brainstorming as the model is not constrained to follow established
    patterns and can explore very diverse styles. Here, a *correct* answer does not
    exist, and instead, the purpose is to create varying outputs. This does mean that
    you may get unexpected outputs that do not conform to the actual prompt at all.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高温度值（1.2 到 2.0）**：这些应用于创意写作和头脑风暴，因为模型不受已建立模式的约束，可以探索各种多样的风格。在这里，*正确*的答案是不存在的，目标是创造多样的输出。这意味着你可能会得到完全不符合实际提示的意外输出。'
- en: Examples include storytelling, generating marketing slogans, and brainstorming
    company names.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，讲故事、生成营销口号和头脑风暴公司名称。
- en: In the recipe, a lower temperature was far better when explaining gravity, as
    the prompt encourages a factual and straightforward answer. However, the second
    prompt, about creating a tagline, is far better suited for a higher temperature,
    as this is a task that requires creativity and out-of-the-box thinking.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，当解释重力时，较低的温度更为合适，因为提示鼓励的是事实性且直接的回答。然而，第二个提示关于创建标语的任务，更适合使用较高的温度，因为这是一个需要创造性和跳出思维框架的任务。
- en: Overall, setting a temperature value means performing a trade-off between coherence
    and creativity, which shifts based on how you use the API within your application.
    As a rule of thumb, it’s best to set the temperature to 1 and then modify it in
    increments of 0.2 until you reach your desired output set.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，设置温度值意味着在连贯性和创造性之间进行权衡，这种权衡取决于你如何在应用程序中使用API。经验法则是，最好将温度设置为1，然后以0.2为增量进行调整，直到达到你想要的输出集。
