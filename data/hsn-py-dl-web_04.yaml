- en: Getting Started with Deep Learning Using Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python开始深度学习
- en: In the first chapter, we had a very close look at deep learning and how it is
    related to machine learning and artificial intelligence. In this chapter, we are
    going to delve deeper into this topic. We will start off by learning about what
    sits at the heart of deep learning—namely, neural networks and their fundamental
    components, such as neurons, activation units, backpropagation, and so on.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们深入探讨了深度学习以及它与机器学习和人工智能之间的关系。在本章中，我们将更深入地探讨这一主题。我们将从了解深度学习的核心——神经网络及其基本组成部分开始，包括神经元、激活单元、反向传播等内容。
- en: Note that this chapter is not going to be too math heavy, but at the same time,
    we are not going to cut short the most important formulas that are fundamental
    to the world of neural networks. For a more math-heavy study of the subject, readers
    are encouraged to read the book *Deep Learning* ([deeplearningbook.org](http://deeplearningbook.org))
    by Goodfellow et al.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，本章不会过多涉及数学内容，但同时，我们也不会忽略那些对神经网络世界至关重要的最基本公式。对于更偏重数学的学习，建议读者阅读Goodfellow等人所著的《深度学习》一书（[deeplearningbook.org](http://deeplearningbook.org)）。
- en: 'The following is an overview of what we are going to cover in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将在本章中涵盖内容的概述：
- en: A whirlwind tour of neural networks and their related concepts
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络及其相关概念的快速浏览
- en: Deep learning versus shallow learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习与浅层学习的区别
- en: Different types of neural networks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的神经网络
- en: Setting up a deep-learning-based cloud environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置基于深度学习的云环境
- en: Exploring Jupyter Notebooks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Jupyter Notebooks
- en: Demystifying neural networks
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解开神经网络的谜团
- en: Let's start this section by finding the answers to the question, “Why are neural
    networks called 'neural'?”. What is the significance behind this term?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的开始，我们将寻找问题的答案：“为什么神经网络被称为'神经'网络？”这个术语背后有什么意义？
- en: Our intuition says that it has something to do with our brains, which is correct,
    but only partially. Before we get to the reason why it is only partially correct,
    we need to have some familiarity with the structure of a brain. For this purpose,
    let's look at the anatomy of our own brains.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的直觉告诉我们，它与我们的脑部有关系，这是正确的，但仅仅部分正确。在了解为什么这种理解只部分正确之前，我们需要对大脑的结构有所了解。为此，让我们来看看我们自己大脑的解剖结构。
- en: A human brain is composed of approximately 10 billion *neurons*, each connected
    to about 10,000 other neurons, which gives it a network-like structure. The inputs
    to the neurons are called *dendrites* and the outputs are called *axons*. The
    body of a neuron is called a *soma*. So, on a high level, a particular soma is
    connected to another soma. The word "neural" comes from the word "neuron," and
    in fact, neural is the adjective form of the word "neuron." In our brains, neurons
    are the most granular units that form this dense network we just discussed. We
    are slowly understanding the resemblance of an artificial neural network to the
    brain, and in order to continue our understanding of this similarity, we will
    briefly learn about the functionalities of a neuron.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑由大约100亿个*神经元*组成，每个神经元与大约10,000个其他神经元相连接，形成一种类似网络的结构。神经元的输入被称为*树突*，输出被称为*轴突*。神经元的主体部分称为*胞体*。因此，从高层次看，某个特定的胞体与另一个胞体相连接。
    "神经"一词源自"神经元"一词，实际上，"神经"是"神经元"的形容词形式。在我们的脑中，神经元是形成我们刚刚讨论的这一密集网络的最基本单元。我们正在慢慢理解人工神经网络与大脑之间的相似性，为了进一步理解这种相似性，我们将简要了解神经元的功能。
- en: A network is nothing but a graph-like structure that contains a set of nodes
    and edges that are connected to each other. In the case of our brains, or any
    brain in general, neurons are referred to as nodes and the dendrites are referred
    to as the vertices.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 网络不过是一个类似图形的结构，包含一组节点和彼此连接的边。在我们的脑部，或任何大脑中，神经元被称为节点，树突被称为顶点。
- en: A neuron receives inputs from other neurons via their dendrites. These inputs
    are electrochemical in nature. Not all the inputs are equally powerful. If the
    inputs are powerful enough, then the connected neurons are activated and continue
    the process of passing the input to the other neurons. Their power is determined
    by a predefined threshold that allows the activation process to be selective so
    that it does not activate all the neurons that are present in the network at the
    same time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元通过其树突接收来自其他神经元的输入。这些输入是电化学性质的。并非所有输入的力量相等。如果输入的力量足够强大，则连接的神经元会被激活，并继续将输入传递给其他神经元。它们的强度由一个预设的阈值决定，这个阈值使得激活过程具有选择性，从而避免在同一时间激活网络中所有的神经元。
- en: To summarize, neurons receive a collective sum of inputs from other neurons,
    this sum is compared to a threshold, and the neurons are activated accordingly.
    An **artificial neural network** (**ANN**), or simply a **neural network** (**NN**),
    is based on this important fact, hence the resemblance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，神经元接收来自其他神经元的输入总和，这个总和与一个阈值进行比较，然后神经元根据阈值激活。**人工神经网络**（**ANN**），或者简称**神经网络**（**NN**），就是基于这一重要事实，因此才有了这种相似性。
- en: So, what makes a network a *neural* one? What does it take to form an NN?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是什么让一个网络成为*神经*网络呢？要构成一个NN需要什么？
- en: 'The following quote from the book *Deep Learning For Computer Vision With Python*
    by Adrian Rosebrock answers this question in a very commendable way:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下引用来自Adrian Rosebrock的《Deep Learning For Computer Vision With Python》一书，以非常值得称赞的方式回答了这个问题：
- en: Each node performs a simple computation. Each connection then carries a signal
    (i.e., the output of the computation) from one node to another, labeled by a weight
    indicating the extent to which the signal is amplified or diminished. Some connections
    have large, positive weights that amplify the signal, indicating that the signal
    is very important when making a classification. Others have negative weights,
    diminishing the strength of the signal, thus specifying that the output of the
    node is less important in the final classification. We call such a system an Artificial
    Neural Network if it consists of a graph structure with connection weights that
    are modifiable using a learning algorithm.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点执行简单的计算。然后，每个连接携带一个信号（即计算的输出）从一个节点传递到另一个节点，连接上标有一个权重，表示信号被放大或减弱的程度。有些连接的权重大且为正，放大了信号，表示在分类时这个信号非常重要。其他连接则具有负权重，削弱了信号的强度，说明该节点的输出在最终分类中的重要性较低。如果一个系统由带有可通过学习算法修改的连接权重的图形结构组成，我们就称这样的系统为人工神经网络。
- en: We have learned about the resemblance of neural networks to brains. We will
    now take this information and learn more about the granular units of ANNs. Let's
    start by learning what a simple neuron has to do in an ANN.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了神经网络与大脑的相似性。接下来，我们将利用这些信息进一步了解ANN中的基本单元。让我们从了解ANN中一个简单神经元的作用开始。
- en: Artificial neurons
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经元
- en: 'Let''s call the neurons that are used in ANNs artificial neurons. Broadly speaking,
    artificial neurons can be of two types:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称在ANN中使用的神经元为人工神经元。广义来说，人工神经元可以分为两种类型：
- en: Linear neuron
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性神经元
- en: Nonlinear neuron
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非线性神经元
- en: Anatomy of a linear neuron
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性神经元的结构
- en: 'A neuron is the most granular unit in a neural network. Let''s look at the
    second word of "neural network." A network is nothing but a set of vertices (also
    called nodes) whose edges are connected to each other. In the case of a neural
    network, neurons serve as the nodes. Let''s consider the following neural network
    architecture and try to dissect it piece by piece:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元是神经网络中最基本的单元。让我们看看“神经网络”这个词的第二个词。网络仅仅是一些顶点（也叫节点）的集合，这些顶点的边相互连接。在神经网络中，神经元充当节点。让我们考虑以下神经网络架构，并尝试逐步分析：
- en: '![](img/c5502868-fd41-47b9-859a-f36bcfd8020e.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5502868-fd41-47b9-859a-f36bcfd8020e.png)'
- en: 'What we can see in the preceding diagram is a neural network with two hidden
    layers (in a neural network, a layer is a set of neurons) with a single output.
    In fact, this is called a two-layer neural network. The neural network consists
    of the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的图示中看到的是一个具有两个隐藏层的神经网络（在神经网络中，层是神经元的集合），并且只有一个输出。事实上，这种网络被称为两层神经网络。神经网络由以下几个部分组成：
- en: One single input
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一输入
- en: Two hidden layers, where the first hidden layer has three neurons and the second
    hidden layer contains two neurons
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个隐藏层，其中第一个隐藏层有三个神经元，第二个隐藏层包含两个神经元
- en: One single output
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一输出
- en: There is no deeper psychological significance in calling the layers hidden they
    are called hidden simply because the neurons involved in these layers are neither
    parts of the input nor output. One thing that is very evident here is that there
    is a layer before the first hidden layer. Why are we not counting that layer?
    In the world of neural networks, that initial layer and output are not counted
    in the stack of layers. In simple words, if there are *n* hidden layers, it is
    an *n*-layer neural network.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所谓隐藏层并没有更深的心理学意义，它们被称为隐藏层仅仅是因为这些层中的神经元既不是输入层的一部分，也不是输出层的一部分。这里有一个非常明显的地方，那就是在第一个隐藏层之前有一层。为什么我们不把这一层算在内呢？在神经网络的世界里，初始层和输出层并不算在层堆叠中。简单来说，如果有*n*个隐藏层，那么它就是一个*n*层的神经网络。
- en: The initial layer (also called an input layer) is used for receiving primary
    input to the neural network. After receiving the primary input, the neurons present
    in the input layer pass them to the next set of neurons that are present in the
    subsequent hidden layers. Before this propagation happens, the neurons add weights
    to the inputs and a bias term to the inputs. These inputs can be from various
    domains—for example, the inputs can be the raw pixels of an image, the frequencies
    of an audio signal, a collection of words, and so on. Generally, these inputs
    are given as feature vectors to the neural network. In this case, the input data
    has only one feature.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 初始层（也叫输入层）用于接收神经网络的主要输入。在接收到主要输入后，输入层中的神经元将其传递给后续隐藏层中的神经元。在这一传播过程发生之前，神经元会给输入加上权重，并为输入添加一个偏置项。这些输入可以来自不同领域——例如，输入可以是图像的原始像素、音频信号的频率、一组词语等等。通常，这些输入会作为特征向量传递给神经网络。在这种情况下，输入数据只有一个特征。
- en: Now, what are the neurons from the next two layers doing here? This is an important
    question. We can consider the addition of weights and biases to the inputs as
    the first level/layer of learning (also called the decision making layer). The
    neurons in the initial hidden layer repeat this process, but before sending the
    calculated output to the neurons that are present in the next hidden layer, they
    compare this value to a threshold. If the threshold criteria are satisfied, then
    only the outputs are propagated to the next level. This part of the whole neural
    network learning process bears a solid resemblance to the biological process that
    we discussed earlier. This also supports the philosophy of learning complex things
    in a layered fashion.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，接下来的两层神经元在做什么呢？这是一个重要的问题。我们可以将权重和偏置添加到输入的过程看作是学习的第一层/级（也叫决策层）。初始隐藏层中的神经元重复这个过程，但在将计算结果发送到下一个隐藏层的神经元之前，它们会将这个值与阈值进行比较。如果阈值条件被满足，那么输出才会传播到下一层。这部分整个神经网络学习过程与我们之前讨论的生物学过程有着显著的相似之处。这也支持了以分层的方式学习复杂事物的理念。
- en: A question that is raised here is, "What happens if no hidden layers are used?".
    It turns out that adding more levels of complexity (by adding more layers) in
    a neural network allows it to learn the underlying representations of the input
    data in a more concise manner than a network with just the input layer and the
    output. But how many layers would we need? We will get to that later.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个问题是：“如果不使用隐藏层，会发生什么？”事实证明，在神经网络中添加更多的复杂度（通过增加更多层）可以比仅有输入层和输出层的网络更简洁地学习输入数据的潜在表示。那么，我们需要多少层呢？我们稍后会讲到这个问题。
- en: Let's introduce some mathematical formulas here to formalize what we just studied.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这里引入一些数学公式，来正式化我们刚才学到的内容。
- en: 'We express the input features as *x*, the weights as *w*, and the bias term
    as *b*. The neural network model that we are currently trying to dissect builds
    upon the following rule:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输入特征表示为*x*，权重表示为*w*，偏置项表示为*b*。我们目前要剖析的神经网络模型基于以下规则：
- en: '![](img/73d8417a-6a5d-48b3-9d3c-2df95ad05b55.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73d8417a-6a5d-48b3-9d3c-2df95ad05b55.png)'
- en: 'The rule says that after calculating the sum of weighted input and the bias,
    if the result is greater than 0, then the neuron is going to yield 1, and if the
    result is less than or equal to 0, then the neuron is simply going to produce
    0 in other words, the neuron is not going to fire. In the case of multiple input
    features, the rule remains exactly the same and the multivariate version of the
    rule looks like the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 该规则表示，在计算加权输入和偏置的和之后，如果结果大于0，则神经元输出1；如果结果小于或等于0，则神经元输出0，换句话说，神经元不会激活。对于多个输入特征，规则仍然完全相同，其多元版本如下：
- en: '![](img/fd99fc73-d225-45bc-9dcb-8ae25e9d5647.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd99fc73-d225-45bc-9dcb-8ae25e9d5647.png)'
- en: 'Here, *i* means that we have a total of *i* input features. The preceding rule
    can be broken down as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*i*表示我们有*i*个输入特征。上述规则可以分解如下：
- en: We take the features individually, and then we multiply them by the weights
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们逐个处理特征，然后将它们与权重相乘
- en: After finishing this process for all the individual input features, we take
    all of the weighted inputs and sum them and finally add the bias term.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对所有输入特征完成该过程后，我们将所有加权输入求和，并最终加上偏置项。
- en: The preceding process is continued for the number of layers we have in our network.
    In this case, we have two hidden layers, so the output of one layer would be fed
    to the next.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上述过程会继续进行，直到我们网络中的所有层都完成。在这种情况下，我们有两层隐藏层，因此一层的输出会被输入到下一层。
- en: The elements we just studied were proposed by Frank Rosenblatt in the 1960s.
    The idea of assigning 0 or 1 to the weighted sum of the inputs based on a certain
    threshold is also known as the **step-function**. There are many rules like this
    in the literature, these are called update rules.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚学习的元素是由Frank Rosenblatt在1960年代提出的。将输入的加权和基于某个阈值分配0或1的想法也被称为**阶跃函数**。文献中有许多类似的规则，这些被称为更新规则。
- en: The neurons we studied are **linear neurons** that are capable of learning linear
    functions. They are not suited for learning representations that are nonlinear
    in nature. Practically, almost all the inputs that neural networks are fed with
    are nonlinear in nature. In the next section, we are going to introduce another
    type of neuron that is capable of capturing the nonlinearities that may be present
    in the data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究的神经元是**线性神经元**，它们能够学习线性函数。但它们不适合学习本质上是非线性的表示。实际上，几乎所有神经网络接收的输入都是非线性的。在下一节中，我们将介绍另一种类型的神经元，它能够捕捉数据中可能存在的非线性。
- en: Some of you might be wondering whether this NN model is called an **MLP** (**multilayer**
    **perceptron**). Well, it is. In fact, Rosenblatt proposed this way back in the
    1960s. Then what are neural networks? We are going to learn the answer to this
    shortly.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你们中的一些人可能会想，这个神经网络模型是不是叫做**MLP**（**多层感知机**）。嗯，确实是。实际上，Rosenblatt早在1960年代就提出了这个模型。那么，什么是神经网络呢？我们很快就会学到答案。
- en: Anatomy of a nonlinear neuron
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性神经元的构造
- en: 'A nonlinear neuron means that it is capable of responding to the nonlinearities
    that may be present in the data. Nonlinearity in this context essentially means
    that for a given input, the output does not change in a linear way. Look at the
    following diagrams:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性神经元意味着它能够响应数据中可能存在的非线性。这里的非线性基本上意味着，对于给定的输入，输出不会以线性方式变化。请看下面的图示：
- en: '![](img/43498898-28e2-44bf-a26c-a071493c6de0.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43498898-28e2-44bf-a26c-a071493c6de0.png)'
- en: Both of the preceding figures depict the relationship between the inputs that
    are given to a neural network and the outputs that the network produces. From
    the first figure, it is clear that the input data is linearly separable, whereas
    the second figure tells us that the inputs cannot be linearly separated. In cases
    like this, a linear neuron will miserably fail, hence the need for nonlinear neurons.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两个图都描述了神经网络接收的输入与网络产生的输出之间的关系。从第一个图中可以看出，输入数据是线性可分的，而第二个图则告诉我们，输入数据不能线性分割。在这种情况下，线性神经元将会失败，因此需要非线性神经元。
- en: In the training process of a neural network, conditions can arise where a small
    change in the bias and weight values may affect the output of the neural network
    in a drastic way. Ideally, this should not happen. A small change to either the
    bias or weight values should cause only a small change in the output. When a step
    function is used, the changes in weight and bias terms can affect the output to
    a great extent, hence the need for something other than a step function.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络的训练过程中，可能会出现小幅度改变偏置和权重值会对神经网络的输出产生剧烈影响的情况。理想情况下，这种情况不应发生。对偏置或权重值的微小改变应该只导致输出的微小变化。当使用step函数时，权重和偏置的变化可能会对输出产生较大的影响，因此需要使用其他的函数来替代step函数。
- en: 'Behind the operation of a neuron sits a function. In the case of the linear
    neuron, we saw that its operations were based on a step function. We have a bunch
    of functions that are capable of capturing the nonlinearities. The sigmoid function
    is such a function, and the neurons that use this function are often called sigmoid
    neurons. Unlike the step function, the output in the case of a sigmoid neuron
    is produced using the following rule:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的操作背后有一个函数。在线性神经元的情况下，我们看到它的操作是基于step函数的。我们有一些函数能够捕捉非线性问题。sigmoid函数就是其中之一，使用这种函数的神经元通常被称为sigmoid神经元。与step函数不同，sigmoid神经元的输出是根据以下规则生成的：
- en: '![](img/c07baaa1-6d33-4e1b-b759-e81faa535390.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c07baaa1-6d33-4e1b-b759-e81faa535390.png)'
- en: 'So, our final, updated rule becomes the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们最终的更新规则变为以下形式：
- en: '![](img/f641c5d4-03a7-4cf6-8f46-eacadf6206f4.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f641c5d4-03a7-4cf6-8f46-eacadf6206f4.png)'
- en: 'But why is the sigmoid function better than a step function in terms of capturing
    nonlinearities? Let''s compare their performance in graphical to understand this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么在捕捉非线性时，sigmoid函数比step函数更好呢？让我们通过图形来比较它们的表现，理解这一点：
- en: '![](img/c67899b6-21c7-48ed-adf3-4fd1efd01d29.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c67899b6-21c7-48ed-adf3-4fd1efd01d29.png)'
- en: The preceding two figures give us a clear picture about the two functions regarding
    their intrinsic nature. It is absolutely clear that the sigmoid function is more
    sensitive to the nonlinearities than the step function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前面两张图清晰地展示了这两个函数的内在特性。显而易见，sigmoid函数对非线性问题的敏感度高于step函数。
- en: 'Apart from the sigmoid function, the following are some widely known and used
    functions that are used to give a neuron a nonlinear character:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 除了sigmoid函数，以下是一些广泛知名且常用于赋予神经元非线性特征的函数：
- en: Tanh
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanh
- en: ReLU
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU
- en: Leaky ReLU
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leaky ReLU
- en: In the literature, these functions, along with the two that we have just studied,
    are called activation functions. Currently, ReLU and its variants are by far the
    most successful activation functions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，这些函数以及我们刚刚学习的两个函数，被称为激活函数。目前，ReLU及其变体无疑是最成功的激活函数。
- en: 'We are still left with a few other basic things related to artificial neural
    networks. Let''s summarize what we have learned so far:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然有一些与人工神经网络相关的基本概念没有讲解。让我们总结一下到目前为止学到的内容：
- en: Neurons and their two main types
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经元及其两种主要类型
- en: Layers
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层
- en: Activation functions
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'We are now in a position to draw a line between MLPs and neural networks. Michael
    Nielson in his online book *Neural Networks and Deep Learning* describes this
    quite well:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在多层感知机（MLP）和神经网络之间做出区分。Michael Nielson在他的在线书籍《神经网络与深度学习》中很好地描述了这一点：
- en: Somewhat confusingly, and for historical reasons, such multiple layer networks
    are sometimes called *multilayer perceptrons* or *MLPs*, despite being made up
    of sigmoid neurons, not perceptrons.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有些时候，由于历史原因，这类多层网络有时会被称为*多层感知机（MLP）*，尽管它们是由sigmoid神经元组成，而非感知机。
- en: We are going to use the neural network and deep neural network terminologies
    throughout this book. We will now move forward and learn more about the input
    and output layers of a neural network.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将贯穿始终地使用神经网络和深度神经网络的术语。接下来，我们将继续学习神经网络的输入层和输出层。
- en: A note on the input and output layers of a neural network
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于神经网络的输入层和输出层的说明
- en: It is important to understand what can be given as inputs to a neural network.
    Do we feed raw images or raw text data to a neural network? Or are there other
    ways to provide input to a neural network? In this section, we will learn how
    a computer really interprets an image to show what exactly can be given as input
    to a neural network when it is dealing with images (yes, neural networks are pretty
    great at image processing). We will also learn the ways to show what it takes
    to feed a neural network with raw text data. But before that, we need to have
    a clear understanding of how a regular tabular dataset is given as an input to
    a neural network. Because tabular datasets are everywhere, in the form of SQL
    tables, server logs, and so on.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 了解什么可以作为神经网络的输入非常重要。我们是否将原始图像或原始文本数据输入到神经网络中？还是有其他方式向神经网络提供输入？在本节中，我们将学习计算机如何真正解释图像，以展示在处理图像时，神经网络到底能接收哪些输入（是的，神经网络在图像处理方面非常强大）。我们还将学习如何向神经网络输入原始文本数据。但在此之前，我们需要清楚了解常规的表格数据集是如何作为神经网络的输入的。因为表格数据集无处不在，形式包括SQL表、服务器日志等。
- en: 'We will take the following toy dataset for this purpose:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下玩具数据集进行此任务：
- en: '![](img/92ffcb3f-b5eb-4b0c-8ec6-a364fa7a3775.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92ffcb3f-b5eb-4b0c-8ec6-a364fa7a3775.png)'
- en: 'Take note of the following points regarding this toy dataset:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下几点，关于这个玩具数据集：
- en: It has two predictor variables, *x1* and *x2*, and these predictors are generally
    called input feature vectors.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它有两个预测变量，*x1* 和 *x2*，这些预测变量通常称为输入特征向量。
- en: It is common to assign *x1* and *x2* to a vector, *X* (more on this later).
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常会将 *x1* 和 *x2* 分配给一个向量 *X*（稍后会详细讲解）。
- en: The response variable is *y*.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应变量是 *y*。
- en: We have 10 instances (containing *x1*, *x2*, and *y* attributes) that are categorized
    into two classes, 0 and 1.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有10个实例（包含 *x1*、*x2* 和 *y* 属性），并将其分为两个类别：0 和 1。
- en: Given *x1* and *x2*, our (neural network's) task is to predict *y*, which essentially
    makes this a classification task.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定 *x1* 和 *x2*，我们的（神经网络的）任务是预测 *y*，这本质上是一个分类任务。
- en: When we say that the neural network predicts something, we mean that it is supposed
    to learn the underlying representations of the input data that best approximate
    a certain function (we saw what function plotting look like a while ago).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说神经网络预测某些内容时，我们的意思是它应该学习输入数据的潜在表示，这些表示最好能近似某个特定的函数（我们之前看过函数绘图的样子）。
- en: 'Let''s now see how this data is given as inputs to a neural network. As our
    data has two predictor variables (or two input vectors), the input layer of the
    neural network has to contain two neurons. We will use the following neural network
    architecture for this classification task:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何将这些数据作为输入传递给神经网络。由于我们的数据有两个预测变量（或两个输入向量），所以神经网络的输入层必须包含两个神经元。我们将使用以下神经网络架构来处理这个分类任务：
- en: '![](img/bdab7ceb-ab2d-45c2-8c77-c896a0bfb2c5.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdab7ceb-ab2d-45c2-8c77-c896a0bfb2c5.png)'
- en: The architecture is quite identical to the one that we saw a while ago, but
    in this case, we have an added input feature vector. The rest is exactly the same.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 架构与我们之前看到的几乎相同，但在这个例子中，我们增加了一个输入特征向量。其他部分完全相同。
- en: To keep it simple, we are not considering the data preprocessing that might
    be needed before we feed the data to the network. Now, let's see how the data
    is combined with the weights and the bias term, and how the activation function
    is applied to them.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化起见，我们不考虑在将数据输入网络之前可能需要的数据预处理。现在，让我们看看数据是如何与权重和偏置项结合，以及激活函数如何应用于它们。
- en: 'In this case, the feature vectors and the response variable (which is *y*)
    are interpreted separately by the neural network the response variable is used
    in the later stage in the network''s training process. Most importantly, it is
    used for evaluating how the neural network is performing. The input data is organized
    as a matrix form, like the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，特征向量和响应变量（即 *y*）被神经网络分别处理，响应变量在网络的后期训练过程中使用。最重要的是，它被用来评估神经网络的表现。输入数据以矩阵形式组织，如下所示：
- en: '![](img/75262c2f-fdbc-41c3-b461-9a04b2d24de7.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75262c2f-fdbc-41c3-b461-9a04b2d24de7.png)'
- en: The kind of NN architecture that we are using now is a fully connected architecture,
    which means that all of the neurons in a particular layer are connected to all
    the other neurons in the next layer.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用的神经网络架构是全连接架构，这意味着某一层中的所有神经元都与下一层中的所有神经元相连接。
- en: 'The weight matrix is defined as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵定义如下：
- en: '![](img/6c7cc6d2-b697-492b-b277-1a3217a04fe2.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c7cc6d2-b697-492b-b277-1a3217a04fe2.png)'
- en: 'For now, let''s not bother about the weight values. The dimensions of the weight
    matrix is interpreted as the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们暂时不关心权重值。权重矩阵的维度可以解释为以下内容：
- en: The number of rows equals the number of feature vectors (*x1* and *x2*, in our
    case).
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行数等于特征向量的数量（在我们的案例中是 *x1* 和 *x2*）。
- en: The number of columns equals the number of neurons in the first hidden layer.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列的数量等于第一隐层中神经元的数量。
- en: 'There are some suffixes and superscripts associated with each of the weight
    values in the matrix. If we take the general form of the weight as ![](img/c16cd84f-3e3a-42e5-b619-a628b3160c0c.png),
    then it should be interpreted as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每个矩阵中权重值都与某些后缀和上标相关。如果我们将权重的一般形式表示为 ![](img/c16cd84f-3e3a-42e5-b619-a628b3160c0c.png)，那么它的解释如下：
- en: '*l* denotes the layer from which the weight is coming. In this case, the weight
    matrix that we just saw is going to be associated with the input layer.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*l* 表示权重来源的层。在这种情况下，我们刚刚看到的权重矩阵将与输入层相关联。'
- en: '*j* denotes the position of the neuron in ![](img/ef96922d-fbab-4bf0-bac4-07dc23ece429.png),
    whereas *k* denotes the position of the neuron in the next layer that the value
    is propagated to.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*j* 表示神经元在 ![](img/ef96922d-fbab-4bf0-bac4-07dc23ece429.png) 中的位置，而 *k* 表示神经元在下一层中，值传播到的位置。'
- en: 'The weights are generally randomly initialized, which adds a *stochastic* character
    to the neural network. Let''s randomly initialize a weight matrix for the input
    layer:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 权重通常是随机初始化的，这为神经网络增加了 *随机* 特性。让我们随机初始化输入层的权重矩阵：
- en: '![](img/28fe1ff8-96fa-4c85-ac98-fa1c4d0ec5db.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/28fe1ff8-96fa-4c85-ac98-fa1c4d0ec5db.png)'
- en: 'Now we calculate the values that are to be given to the first hidden layer
    of the NN. This is computed as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们计算将要赋予神经网络第一隐层的值。计算方法如下：
- en: '![](img/4716dd77-2dfc-490d-b796-aa1215cab251.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4716dd77-2dfc-490d-b796-aa1215cab251.png)'
- en: The first matrix contains all the instances from the training set (without the
    response variable *y*) and the second matrix is the weight matrix that we just
    defined. The result of this multiplication is stored in a variable, ![](img/a85b6a85-7af3-46bc-aa93-70b13630a83d.png)
    (this variable can be named anything, and the superscript denotes that it is related
    to the first hidden layer of the network).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个矩阵包含来自训练集的所有实例（不包括响应变量 *y*），第二个矩阵是我们刚定义的权重矩阵。这个乘法的结果存储在一个变量中，![](img/a85b6a85-7af3-46bc-aa93-70b13630a83d.png)（这个变量可以命名为任何名称，上标表示它与网络的第一隐层相关）。
- en: 'We are still left with one more step before we send these results to the neurons
    in the next layer, where the activation functions will be applied. The sigmoid
    activation function and the final output from the input layer would look like
    the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在将这些结果传送到下一层神经元之前，我们还有一步需要完成，在下一层神经元上将应用激活函数。sigmoid 激活函数和输入层的最终输出如下所示：
- en: '![](img/20a23d3e-6c8c-44ac-874b-3e088653fec9.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20a23d3e-6c8c-44ac-874b-3e088653fec9.png)'
- en: Here, *a^((1))* is our final output for the next layer of neurons. Note that
    the sigmoid function is applied to each and every element of the ![](img/ad7faaaf-0c2c-4576-92b4-6b1cf41f2489.png)
    matrix. The final matrix will have a dimension of 10 X 3, where each row is for
    each instance from the training set and each column is for each neuron of the
    first hidden layer.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*a^((1))* 是我们为下一层神经元得到的最终输出。请注意，sigmoid 函数应用于 ![](img/ad7faaaf-0c2c-4576-92b4-6b1cf41f2489.png)
    矩阵的每个元素。最终的矩阵将具有 10 X 3 的维度，其中每一行对应训练集中的一个实例，每一列对应第一隐层中的一个神经元。
- en: 'The whole calculation that we saw is without the bias term, *b*, that we initially
    talked about. Well, that is just a matter the of addition of another dimension
    to the picture. In that case, before we apply the sigmoid function to each of
    the elements of the ![](img/2f62dd1e-ca80-4973-a7da-b0b7622410bc.png) matrix,
    the matrix itself would be changed to something like this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的整个计算没有包含我们最初提到的偏置项 *b*。实际上，这只是向图像中添加另一个维度的问题。在这种情况下，在我们对 ![](img/2f62dd1e-ca80-4973-a7da-b0b7622410bc.png)
    矩阵的每个元素应用 sigmoid 函数之前，矩阵本身会变成如下所示：
- en: '![](img/541cba3d-b30e-416a-8eac-8a9b0ec6f6f1.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/541cba3d-b30e-416a-8eac-8a9b0ec6f6f1.png)'
- en: After this matrix multiplication process, the sigmoid function is applied and
    the output is sent to the neurons in the next layers, and this whole process repeats
    for each hidden layer and output layer that we have in the NN. As we proceed,
    we are supposed to get ![](img/7ca8c778-786e-4432-b25f-0409a6d2ef8c.png) from
    the output layer.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个矩阵乘法过程之后，应用了sigmoid函数，输出会被传送到下一层的神经元，而这个过程会在每一层隐藏层和输出层中重复进行。随着进程的推进，我们应该从输出层获得![](img/7ca8c778-786e-4432-b25f-0409a6d2ef8c.png)。
- en: The sigmoid activation function outputs values ranging from 0–1, but we are
    dealing with a binary classification problem, and we only want 0 or 1 as the final
    output from the NN. We can do this with a little tweak. We can define a threshold
    at the output layer of the NN—for the values that are less than 0.5 they should
    be identified as class 0 and the values that are greater than or equal to 0.5
    should be identified as class 1\. Note that this is called forward pass or forward
    propagation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid激活函数的输出值范围是0到1，但我们处理的是二分类问题，并且我们只希望神经网络的最终输出是0或1。我们可以通过一个小的调整来实现这一点。我们可以在神经网络的输出层定义一个阈值——对于小于0.5的值，它们应被标识为类别0，而对于大于或等于0.5的值，它们应被标识为类别1。请注意，这叫做前向传递或前向传播。
- en: The NN we just saw is referred to as a feed-forward network with no further
    optimization in its learning process. But wait! What does the network even learn?
    Well, an NN typically learns the weights and bias terms so that the final output
    is as accurate as possible. And this happens with gradient descent and backpropagation.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才看到的神经网络被称为前馈网络，在其学习过程中没有进一步的优化。但是等一下！这个网络到底学到了什么？实际上，神经网络通常学习的是权重和偏置项，以使最终输出尽可能准确。这一过程通过梯度下降和反向传播来实现。
- en: Gradient descent and backpropagation
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降和反向传播
- en: Before we start learning about what gradient descent and backpropagation have
    to do in the context of neural networks, let's learn what is meant by an optimization
    problem.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始了解梯度下降和反向传播在神经网络中的作用之前，让我们先了解一下什么是优化问题。
- en: 'An optimization problem, briefly, corresponds to the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，优化问题对应于以下内容：
- en: Minimizing a certain cost
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化某个成本
- en: Maximizing a certain profit
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化某个利润
- en: Let's now try to map this to a neural network. What happens if, after getting
    the output from a feed-forward neural network, we find that its performance is
    not up to the mark (which is the case almost all the time)? How are we going to
    enhance the performance of the NN? The answer is gradient descent and backpropagation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试将其映射到神经网络上。如果在从前馈神经网络获得输出后，我们发现其性能没有达到预期（这几乎是每次都会发生的情况），我们该如何提高神经网络的性能呢？答案就是梯度下降和反向传播。
- en: We are going to optimize the learning process of the neural network with these
    two techniques. But what are we going to optimize? What are we going to minimize
    or maximize? We require a specific type of cost that we will attempt to minimize.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过这两种技术来优化神经网络的学习过程。那么我们要优化什么呢？我们要最小化或最大化什么呢？我们需要一种特定类型的成本函数，我们将尝试最小化它。
- en: We will define the cost in terms of a function. Before we define a cost function
    for the NN model, we will have to decide the parameters of the cost function.
    In our case, the weights and the biases are the parameters of the function that
    the NN is trying to learn to give us accurate results (see the information box
    just before this section). In addition, we will have to calculate the amount of
    loss that the network is inculcating at each step of its training process.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个函数来定义成本。在我们为神经网络模型定义成本函数之前，我们必须先决定成本函数的参数。在我们的例子中，权重和偏置项是神经网络试图学习的函数参数，以便为我们提供准确的结果（请参见本节前面的信息框）。此外，我们还需要计算网络在每次训练步骤中所犯的损失。
- en: For a binary classification problem, a loss function called a **cross-entropy**
    loss function (for a binary classification problem it is called a binary cross
    cross-entropy loss function) is widely used, and we are going to use it. So, what
    does this function look like?
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二分类问题，广泛使用一种称为**交叉熵**的损失函数（对于二分类问题，它被称为二元交叉熵损失函数），我们也将使用它。那么，这个函数是什么样的呢？
- en: '![](img/6d3375fb-d8cf-49aa-ba7e-26efe4d6b342.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d3375fb-d8cf-49aa-ba7e-26efe4d6b342.png)'
- en: Here, *y* denotes the ground truth or true label (remember the response variable,
    *y*, in the training set) of a given instance and ![](img/e50fa991-8408-4440-88e2-cd37d88e29a1.png) denotes
    the output as yielded by the NN model. This function is convex in nature, which
    is just perfect for convex optimizers such as gradient descent.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y* 表示给定实例的真实标签或地面真实值（记住训练集中的响应变量 *y*），而 ![](img/e50fa991-8408-4440-88e2-cd37d88e29a1.png) 表示由神经网络模型输出的结果。这个函数是凸函数，这非常适合使用像梯度下降法这样的凸优化器。
- en: This is one of the reasons that we didn't pick up a simpler and nonconvex loss
    function. (Don't worry if you are not familiar with terms like convex and nonconvex.)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是我们没有选择一个更简单且非凸的损失函数的原因之一。（如果你不熟悉凸和非凸等术语，不必担心。）
- en: We have our loss function now. Keep in mind that this is just for one instance
    of the entire set of data this is not the function on which we are going to apply
    gradient descent. The preceding function is going to help us define the cost function
    that we will eventually optimize using gradient descent. Let's see what that cost
    function looks like.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了损失函数。请记住，这只是针对整个数据集中的一个实例的函数，这不是我们将应用梯度下降法的函数。前述函数将帮助我们定义代价函数，最终我们会通过梯度下降法来优化它。让我们看看那个代价函数是什么样子。
- en: '![](img/2fe2f681-2cd6-4422-b942-fba491e140e1.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe2f681-2cd6-4422-b942-fba491e140e1.png)'
- en: 'Here, *w* and *b* are the weights and biases that the network is trying to
    learn. The letter *m* represents the number of training instances, which is 10
    in this case. The rest seems familiar. Let''s put the original form of the function,
    *L()*, and see what *J()* looks like:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*w* 和 *b* 是网络尝试学习的权重和偏置。字母 *m* 表示训练实例的数量，在本例中为 10。其余部分看起来很熟悉。我们将原始形式的函数
    *L()* 放入其中，看看 *J()* 看起来是什么样子：
- en: '![](img/bc0b7b74-497f-4bae-8f14-a5661a0c48af.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc0b7b74-497f-4bae-8f14-a5661a0c48af.png)'
- en: The function may look a bit confusing, so just slow it down and make sure you
    understand it well.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可能有点让人困惑，所以请慢慢理解，确保你能很好地理解它。
- en: 'We can finally move toward the optimization process. Broadly, gradient descent
    is trying to do the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终可以进入优化过程。广义而言，梯度下降法试图做以下几件事：
- en: Give us a point where the cost function is as minimal as possible (this point
    is called the minima).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给我们一个点，使得代价函数最小化（这个点叫做最小值点）。
- en: Give us the right values of the weights and biases so that the cost function
    reaches that point.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给我们正确的权重和偏置值，使得代价函数达到那个点。
- en: 'To visualize this, let''s take a simple convex function:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这一点，我们先来看一个简单的凸函数：
- en: '![](img/b648cee7-9c85-4912-a310-764c72904530.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b648cee7-9c85-4912-a310-764c72904530.png)'
- en: 'Now, say we start the journey at a random point, such as the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们从一个随机点开始，比如以下的点：
- en: '![](img/b7d2ca94-6643-41d4-ac7d-aa8bdb593cff.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7d2ca94-6643-41d4-ac7d-aa8bdb593cff.png)'
- en: So, the point at the top right corner is the point at which we started. And
    the point (directed by the dotted arrow) is the point we wish to arrive at. So,
    how do we do this in terms of simple computations?
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，右上角的点就是我们开始的点。由虚线箭头指示的点是我们希望到达的点。那么，我们如何通过简单的计算来实现这一点呢？
- en: 'In order to arrive at this point the following update rule is used:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这一点，使用了以下更新规则：
- en: '![](img/932dc7a9-29ef-438c-ae22-d0397b9fab67.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/932dc7a9-29ef-438c-ae22-d0397b9fab67.png)'
- en: 'Here, we are taking the partial derivative of *J(w,b)* with respect to the
    weights. We are taking a partial derivative because *J(w,b)* contains *b* as one
    of the parameters. 𝝰 is the learning rate that speeds up this process. This update
    rule is applied multiple times to find the right values of the weights. But what
    about the bias values? The rule remains exactly the same only the equation is
    changed:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在对 *J(w,b)* 关于权重进行偏导数计算。我们进行偏导数计算是因为 *J(w,b)* 将 *b* 作为参数之一。𝝰 是加速该过程的学习率。这个更新规则会应用多次，以找到合适的权重值。那么偏置值呢？规则完全相同，只是方程发生了变化：
- en: '![](img/0505bb51-e6be-440d-86e5-3774b8532cbe.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0505bb51-e6be-440d-86e5-3774b8532cbe.png)'
- en: These new assignments of weights and biases are essentially referred to as *backpropagation*,
    and it is done in conjunction with *gradient descent*. After computing the new
    values of the weights and the biases, the whole forward propagation process is
    repeated until the NN model generalizes well. Note that these rules are just for
    one single instance, provided that the instance has only one feature. Doing this
    for several instances that contain several features can be difficult, so we are
    going to skip that part however, those who are interested in seeing the fully
    fledged version of this may refer to a lecture online by Andrew Ng.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新的权重和偏置的分配本质上被称为*反向传播*，并且是与*梯度下降*一起完成的。在计算出新的权重和偏置值后，整个前向传播过程将被重复，直到神经网络模型能够很好地泛化。请注意，这些规则仅适用于单一实例，前提是该实例只有一个特征。对于包含多个特征的多个实例来说，做这件事可能会比较困难，因此我们将跳过这一部分，不过那些对完整版本感兴趣的人可以参考Andrew
    Ng的在线讲座。
- en: 'We have covered the necessary fundamental units of a standard neural network,
    and it was not easy at all. We started by defining neurons and we ended with backprop
    (the nerdy term of backpropagation). We have already laid the foundations of a
    deep neural network. Readers might be wondering whether that was a deep neural
    net that we just studied. As **Andriy Burkov** says (from his book titled *The
    Hundred Page Machine-Learning Book*):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了标准神经网络所需的基本单元，这一点并不容易。我们从定义神经元开始，最后讨论了反向传播（反向传播的技术术语）。我们已经为深度神经网络打下了基础。读者可能会想，这是不是我们刚刚学习的那个深度神经网络。正如**安德里·布尔科夫**在他的书《*一百页机器学习书*》中所说：
- en: Deep learning refers to training neural networks with more than two non-output
    layers. … the term “deep learning” refers to training neural networks using the
    modern algorithmic and mathematical toolkit independently of how deep the neural
    network is. In practice, many business problems can be solved with neural networks
    having 2-3 layers between the input and output layers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习指的是训练具有两个以上非输出层的神经网络。...“深度学习”这一术语指的是使用现代的算法和数学工具训练神经网络，而与神经网络的深度无关。实际上，许多商业问题可以通过具有2-3个层次的神经网络来解决，这些层次位于输入层和输出层之间。
- en: In the next sections, we will learn about the difference between deep learning
    and shallow learning. We will also take a look at two different types of neural
    networks—namely, convolutional neural networks and recurrent neural networks.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习深度学习与浅层学习之间的区别。我们还将介绍两种不同类型的神经网络——卷积神经网络和循环神经网络。
- en: Different types of neural network
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同类型的神经网络
- en: So far, we have learned what feed-forward neural networks look like and how
    techniques such as backpropagation and gradient descent are applied to it in order
    to optimize their training process. The binary classification problem we studied
    earlier appears to be too naive and too impractical, doesn't it?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了前馈神经网络的结构以及如何将反向传播和梯度下降等技术应用于它，以优化其训练过程。我们之前学习的二分类问题似乎太过简单和不切实际了，不是吗？
- en: Well, there are many problems that a simple NN model can solve well. But as
    the complexity of the problem increases, improvements to the basic NN model become
    necessary. These complex problems include object detection, object classification,
    image-caption generation, sentiment analysis, fake-news classification, sequence
    generation, speech translation, and so on. For problems like these, a basic NN
    model is not sufficient. It needs some architectural improvements so that it can
    solve these problems. In this section, we are going to study two of the most powerful
    and widely used NN models—convolutional neural networks and recurrent neural networks.
    At the heart of the stunning applications of deep learning that we see nowadays
    sit these NN models.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，有很多问题是简单的神经网络模型能够很好地解决的。但随着问题复杂性的增加，对基本神经网络模型的改进变得必要。这些复杂问题包括物体检测、物体分类、图像说明生成、情感分析、假新闻分类、序列生成、语音翻译等等。像这些问题，单纯的神经网络模型是无法满足的。它需要一些架构上的改进才能解决这些问题。在这一部分，我们将学习两种最强大且广泛使用的神经网络模型——卷积神经网络和循环神经网络。正是这些神经网络模型支撑着当今深度学习令人惊叹的应用。
- en: Convolutional neural networks
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Have you ever uploaded a photo of your friends' group to Facebook? If yes, have
    you ever wondered how Facebook detects all the faces in the photo automatically
    just after the upload finishes? In short, the answer is **convolutional neural
    networks** (**CNNs**).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有将朋友们的合照上传到 Facebook？如果有，你是否曾经想过，在上传完成后，Facebook 是如何自动检测出照片中的所有人脸的？简而言之，答案是**卷积神经网络**（**CNNs**）。
- en: A feed-forward network generally consists of several fully connected layers,
    whereas a CNN consists of several layers of convolution, along with other types
    of sophisticated layers, including fully-connected layers. These fully-connected
    layers are generally placed towards the very end and are typically used for making
    predictions. But what kinds of predictions? In an image-processing and computer-vision
    context, a prediction task can encompass many use cases, such as identifying the
    type of object present in the image that is given to the network. But are CNNs
    only good for image-related tasks? CNNs were designed and proposed for image-processing
    tasks (such as object detection, object classification, and so on) but it has
    found its use in many text-processing tasks as well. We are going to learn about
    CNNs in an image-processing context because CNNs are most popular for the wonders
    they can work in the domains of image processing and computer vision. But before
    we move on to this topic, it would be useful to understand how an image can be
    represented in terms of numbers.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈网络通常由若干全连接层组成，而卷积神经网络（CNN）由若干卷积层以及其他类型的复杂层（包括全连接层）组成。这些全连接层通常位于网络的最后，并通常用于做预测。那么，预测的内容是什么呢？在图像处理和计算机视觉的背景下，预测任务可以涵盖多种使用场景，比如识别图像中给定对象的类型。但是，CNN
    只适用于与图像相关的任务吗？CNN 是为图像处理任务（如目标检测、目标分类等）设计和提出的，但它也在许多文本处理任务中得到了应用。我们将要学习 CNN 在图像处理中的应用，因为
    CNN 在图像处理和计算机视觉领域的奇迹使其最为人们熟知。不过，在我们深入讨论这个话题之前，了解如何将图像以数字的形式表示是非常有用的。
- en: 'An image consists of numerous pixels and dimensions—height x width x depth.
    For a color image, the depth dimension is generally 3, and for a grayscale image,
    the dimension is 1\. Let''s dig a bit deeper into this. Consider the following
    image:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一张图像由大量的像素和维度组成——高度 x 宽度 x 深度。对于彩色图像，深度维度通常是 3，而对于灰度图像，维度是 1。让我们深入了解一下。请看下图：
- en: '![](img/79d278c8-9d3b-409e-ae79-de957bb6251c.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79d278c8-9d3b-409e-ae79-de957bb6251c.png)'
- en: The dimension of the preceding image is 626 x 675 x 3, and numerically, it is
    nothing but a matrix. Each pixel represents a particular intensity of red, green,
    and blue (according to the RGB color system). The image contains a total of 422,550
    pixels (675 x 626).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图像尺寸为 626 x 675 x 3，数字上，它只是一个矩阵。每个像素代表红色、绿色和蓝色的特定强度（根据 RGB 颜色系统）。该图像总共有 422,550
    个像素（675 x 626）。
- en: 'The pixels are denoted by a list of three values of red, green, and blue colors.
    Let''s now see what a pixel (corresponding to the twentieth row and the hundredth
    column in the matrix of 422,550 pixels) looks like in coding terms:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些像素由红色、绿色和蓝色的三个值组成。现在，让我们看看一个像素（对应于422,550个像素矩阵中的第20行和第100列）在编码中的表现：
- en: '[PRE0]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Each value corresponds to a particular intensity of the colors red, green, and
    blue. For the purpose of understanding CNNs, we will look at a much smaller dimensional
    image in grayscale. Keep in mind that each pixel in a grayscale image is between
    0 and 255, where 0 corresponds to black and 255 corresponds to white.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 每个值对应于红色、绿色和蓝色的特定强度。为了便于理解 CNN，我们将查看一个维度较小的灰度图像。请记住，灰度图像中的每个像素值介于 0 和 255 之间，其中
    0 对应黑色，255 对应白色。
- en: 'The following is a dummy matrix of pixels representing a grayscale image (we
    will refer to this as an image matrix):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个代表灰度图像的虚拟像素矩阵（我们将其称为图像矩阵）：
- en: '![](img/a24eda9c-4190-41e1-bc67-d4d3a775f1d9.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a24eda9c-4190-41e1-bc67-d4d3a775f1d9.png)'
- en: 'Before we proceed, let''s think intuitively about how can we train a CNN to
    learn the underlying representations of an image and make it perform some tasks.
    Images have a special property inherent to them: the pixels in an image that contain
    a similar type of information generally remain close to each other. Consider the
    image of a standard human face: the pixels denoting the hair are darker and are
    closely located on the image, whereas the pixels denoting the other parts of the
    face are generally lighter and also stay very close to each other. The intensities
    may vary from face to face, but you get the idea. We can use this spatial relationship
    of the pixels in an image and train a CNN to detect the similar pixels and the
    edges that they create in between them to distinguish between the several regions
    present in an image (in an image of a face, there are arbitrary edges in between
    the hair, eyebrows, and so on). Let''s see how this can be done.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，先直观地思考一下，我们如何训练一个 CNN 来学习图像的潜在表示并使其执行某些任务。图像有一个特殊的固有属性：图像中包含相似类型信息的像素通常会彼此靠近。以标准人脸图像为例：表示头发的像素较暗，且在图像上彼此接近，而表示面部其他部分的像素通常较亮，并且也非常接近。不同面孔的强度可能有所不同，但你能理解这一点。我们可以利用图像中像素的空间关系，训练
    CNN 来检测相似的像素以及它们之间产生的边缘，从而区分图像中的不同区域（在人脸图像中，头发、眉毛等之间有任意的边缘）。让我们看看如何实现这一点。
- en: 'A CNN typically has the following components:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 CNN 通常包含以下组件：
- en: Convolutional layer
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Activation layer
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活层
- en: Pooling layer
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: Fully connected layer
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层
- en: 'At the heart of a CNN sits an operation called convolution (which is also known
    as cross relation in the literature of computer vision and image processing).
    Adrian Rosebrock of PyImageSearch describes the operation as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络（CNN）的核心是一个被称为卷积的操作（在计算机视觉和图像处理的文献中也称为交叉关系）。PyImageSearch 的 Adrian Rosebrock
    如下描述了这一操作：
- en: In terms of deep learning, an (image) convolution is an element-wise multiplication
    of two matrices followed by a sum.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，图像卷积是两个矩阵的逐元素相乘，然后求和。
- en: 'This quote tells us how an (image) convolution operator works. The matrices
    mentioned in the quote are the image matrix itself and another matrix known as
    the kernel. The original image matrix can be higher than the kernel matrix and
    the convolution operation is performed on the image matrix in a left–right top–bottom
    direction. Here is an example of a convolution operation involving the preceding
    dummy matrix and a kernel of size 2 x 2:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这段话告诉我们一个（图像）卷积操作是如何工作的。文中提到的矩阵是图像矩阵本身和另一个被称为卷积核的矩阵。原始图像矩阵可以大于卷积核矩阵，卷积操作是按从左到右、从上到下的方向在图像矩阵上进行的。以下是一个涉及前面虚拟矩阵和一个
    2 x 2 大小的卷积核的卷积操作示例：
- en: '![](img/81b66b3b-aaa5-45a5-82d4-e14bfc38ecc0.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/81b66b3b-aaa5-45a5-82d4-e14bfc38ecc0.png)'
- en: 'The kernel matrix actually serves as the weight matrix for the network, and
    to keep it simple, we ignore the bias term for now. It is also worth noting that
    our favorite image filters (sharpening, blurring, and so on) are nothing but outputs
    of certain kinds of convolution applied to the original images. A CNN actually
    learns these filter (kernel) values so that it can best capture the spatial representation
    of an image. These values can be further optimized using gradient descent and
    backpropagation. The following figure depicts four convolution operations applied
    to the image:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积核矩阵实际上充当了网络的权重矩阵，为了简化起见，我们暂时忽略偏置项。值得注意的是，我们最喜欢的图像滤镜（如锐化、模糊等）实际上就是对原始图像应用某些卷积操作后的输出。CNN
    实际上学习这些滤镜（卷积核）值，从而能够最佳地捕捉图像的空间表示。这些值可以通过梯度下降和反向传播进一步优化。下图展示了对图像应用的四个卷积操作：
- en: '![](img/3f1deaa5-9ebe-413d-8869-8a3b87ddc294.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f1deaa5-9ebe-413d-8869-8a3b87ddc294.png)'
- en: 'Note how the kernel is sliding and how the convoluted pixels are being calculated.
    But if we proceed like this, then the original dimensionality of the image gets
    lost. This can cause information loss. To prevent this, we apply a technique called
    padding and retain the dimensionality of the original image. There are many padding
    techniques, such as replicate padding, zero padding, wrap around, and so on. Zero
    padding is very popular in deep learning. We will now see how zero padding can
    be applied to the original image matrix so that the original dimensionality of
    the image is retained:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意核是如何滑动的，以及如何计算卷积像素。但如果我们继续这样做，那么图像的原始维度就会丢失，这可能会导致信息丢失。为了防止这种情况，我们应用一种叫做填充（padding）的技术，以保持原始图像的维度。填充技术有很多种，例如复制填充、零填充、环绕填充等等。零填充在深度学习中非常流行。现在我们来看一下如何将零填充应用于原始图像矩阵，从而保持图像的原始维度：
- en: '![](img/c102f556-b78a-45cb-b402-302b91561d97.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c102f556-b78a-45cb-b402-302b91561d97.png)'
- en: Zero padding means that the pixel value matrix will be padded by zero on all
    sides, as shown in the preceding image.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 零填充意味着像素值矩阵将在所有边缘用零填充，如前面的图像所示。
- en: 'It is important to instruct the network how it should slide the image matrix.
    This is controlled using a parameter called stride. The choice of stride depends
    on the dataset and the correct use of stride 2 is standard practice in deep learning.
    Let''s see how stride 1 differs from stride 2:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是指导网络如何滑动图像矩阵。这是通过一个叫做步长（stride）的参数来控制的。步长的选择取决于数据集，并且在深度学习中，正确使用步长2是标准做法。让我们看看步长1与步长2的不同：
- en: '![](img/fee0eddc-8450-4997-ab80-615b4b4155c1.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fee0eddc-8450-4997-ab80-615b4b4155c1.png)'
- en: 'A convoluted image typically looks like the following:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一个复杂的图像通常看起来像下面这样：
- en: '![](img/6e10b613-f9a8-49eb-b1d6-d6e2072fcff4.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e10b613-f9a8-49eb-b1d6-d6e2072fcff4.png)'
- en: The convoluted image largely depends on the kernel that is being used. The final
    output matrix is passed to an activation function and the function is applied
    to the matrix's elements. Another important operation in a CNN is pooling, but
    we will skip this for now. By now, you should have a good understanding of how
    a CNN works on a high level, which is sufficient for continuing to follow the
    book. If you want to have a deeper understanding of how a CNN works, then refer
    to the blog post at [https://www.pyimagesearch.com/2018/04/16/keras-and-convolutional-neural-networks-cnns/](https://www.pyimagesearch.com/2018/04/16/keras-and-convolutional-neural-networks-cnns/).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积图像在很大程度上取决于所使用的核。最终输出的矩阵会传递给激活函数，并且该函数会应用于矩阵的元素。卷积神经网络（CNN）中的另一个重要操作是池化，但我们现在先跳过这一部分。到目前为止，您应该对CNN如何在高层次上工作有了较好的理解，这对于继续阅读本书已经足够。如果你想更深入了解CNN的工作原理，可以参考这篇博客：[https://www.pyimagesearch.com/2018/04/16/keras-and-convolutional-neural-networks-cnns/](https://www.pyimagesearch.com/2018/04/16/keras-and-convolutional-neural-networks-cnns/)。
- en: Recurrent neural networks
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: '**Recurrent neural networks (RNNs)** are another type of neural network, and
    are tremendously good at NLP tasks—for example, sentiment analysis, sequence prediction,
    speech-to-text translation, language-to-language translation, and so on. Consider
    an example: you open up Google and you start searching for recurrent neural networks.
    The moment you start typing a word, Google starts giving you a list of suggestions
    which is most likely to be topped by the complete word, or the most commonly searched
    phrase that begins with the letters you have typed by then. This is an example
    of sequence prediction where the task is to predict the next sequence of the given
    phrase.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络（RNNs）**是另一种神经网络类型，非常擅长处理NLP任务——例如情感分析、序列预测、语音转文本翻译、语言翻译等等。考虑一个例子：你打开Google并开始搜索循环神经网络。当你开始输入一个词时，Google会开始给出一系列建议，最有可能的是在你输入的字母后，搜索量最大的完整单词或短语。这是一个序列预测的例子，其中的任务是预测给定短语的下一个序列。'
- en: 'Let''s take another example: you are given a bunch of English sentences containing
    one blank per sentence. Your task is to appropriately fill the gaps with the correct
    words. Now, in order to do this, you will need to use your previous knowledge
    of the English language in general and make use of the context as much as possible.
    To use previously encountered information like this, you use your memory. But
    what about neural networks? Traditional neural networks cannot do this because
    they do not have any memory. This is exactly where RNNs come into the picture.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再来看一个例子：给定一组包含一个空白的英语句子，你的任务是适当地填充这些空白，选择正确的单词。为了做到这一点，你需要使用你对英语语言的先前知识，并尽可能地利用上下文。为了使用之前遇到的信息，你需要用到记忆。那么，神经网络呢？传统的神经网络无法做到这一点，因为它们没有任何记忆。这正是RNN派上用场的地方。
- en: 'The question that we need to answer is how can we empower neural networks with
    memory? An absolutely naive idea would be to do the following:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要回答的问题是，如何赋予神经网络记忆？一个完全天真的想法是做以下的事情：
- en: Feed a certain sequence into a neuron.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一个特定的序列输入到神经元中。
- en: Take the output of the neuron and feed it to the neuron again.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将神经元的输出再输入到该神经元中。
- en: 'It turns out that this idea is not that naive, and in fact constitutes the
    foundation of the RNN. A single layer of an RNN actually looks like the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这个想法并非那么天真，实际上它构成了RNN的基础。一个RNN的单层实际上看起来像下面这样：
- en: '![](img/598b05d6-11ee-4c08-9c52-9bebfd97f229.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/598b05d6-11ee-4c08-9c52-9bebfd97f229.png)'
- en: 'The loop seems to be a bit mysterious. You might already be thinking about
    what happens in each iteration of the loop:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 循环看起来有点神秘。你可能已经在想每次循环迭代时会发生什么：
- en: '![](img/4865606d-533c-4d58-a488-c425003e96e4.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4865606d-533c-4d58-a488-c425003e96e4.png)'
- en: In the preceding diagram, an RNN (the figure on the left) is unrolled to show
    three simple feedforward networks. But what do these unrolled networks do? Let's
    find this out now.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的图示中，RNN（左侧的图形）被展开成三个简单的前馈网络。但这些展开的网络究竟在做什么呢？让我们现在来找找看。
- en: Let's consider the task of sequence prediction. To keep it simple, we will look
    at how an RNN can learn to predict the next letter to complete a word. For example,
    if we train the network with a set of letters, *{w, h, a, t}*, and after giving
    the letters *w,h*, and *a* sequentially, the network should be able to predict
    that the letter should be *t* so that the meaningful word "what" is produced.
    Just like the feed-forward networks we saw earlier, *X* serves as the input vector
    to the network in RNN terminology, this vector is also referred to as the vocabulary
    of the network. The vocabulary of the network is, in this case, *{w, h, a, t}*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一下序列预测任务。为了简单起见，我们将看看RNN如何学习预测下一个字母以完成一个单词。例如，如果我们用一组字母 *{w, h, a, t}*
    来训练网络，依次输入字母 *w, h* 和 *a* 后，网络应该能够预测下一个字母是 *t*，这样就能形成有意义的单词“what”。就像我们之前看到的前馈网络一样，在RNN术语中，*X*
    作为输入向量，这个向量也被称为网络的词汇表。在这种情况下，网络的词汇表是 *{w, h, a, t}*。
- en: 'The network is fed with the letters *w,h*, and *a* sequentially. Let''s try
    to give indices to the letters:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 网络依次输入字母 *w, h* 和 *a*。我们尝试为字母指定索引：
- en: '![](img/454ba3b9-649e-4cae-877b-6b2cde5bf155.png)→ ![](img/f653ecc8-9aae-468b-983e-256285270df6.png)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/454ba3b9-649e-4cae-877b-6b2cde5bf155.png)→ ![](img/f653ecc8-9aae-468b-983e-256285270df6.png)'
- en: '![](img/173cfead-90f6-4776-83e6-208f4919ba20.png)→ ![](img/a2112f9b-9250-42c1-85c0-9be6fab58c48.png)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/173cfead-90f6-4776-83e6-208f4919ba20.png)→ ![](img/a2112f9b-9250-42c1-85c0-9be6fab58c48.png)'
- en: '![](img/3e03023b-b603-4d60-bb97-cb51c112e8c1.png)→ ![](img/2815c5c2-68c2-44ae-a545-8a9227b5685e.png)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/3e03023b-b603-4d60-bb97-cb51c112e8c1.png)→ ![](img/2815c5c2-68c2-44ae-a545-8a9227b5685e.png)'
- en: These indices are known as time-steps (the superscripts in the figure presenting
    the unrolling of an RNN). A recurrent layer makes use of the input that is given
    at previous time-steps, along with a function when operating on the current time-step.
    Let's see how the output is produced by this recurrent layer step by step.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这些索引被称为时间步（图中展示的RNN展开过程中的上标）。一个递归层会利用前一个时间步输入的内容，并结合当前时间步的函数进行操作。让我们一步一步地来看这个递归层是如何产生输出的。
- en: Feeding the letters to the network
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将字母输入到网络中
- en: 'Before we see how a recurrent layer produces the output, it is important to
    learn how we can feed the set of letters to the networks. One-hot encoding lets
    us do this in a very efficient way:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们看到递归层如何产生输出之前，了解如何将字母集输入到网络中是很重要的。独热编码让我们以非常高效的方式做到这一点：
- en: '![](img/82595ac0-af4d-4777-8394-1939beb4c4b7.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82595ac0-af4d-4777-8394-1939beb4c4b7.png)'
- en: So, in one-hot encoding, our input vectors/vocabulary of letters are nothing
    but four 4 x 1 matrices, each denoting a particular letter. One-hot encoding is
    standard practice for these tasks. This step is actually a data-preprocessing
    step.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在独热编码中，我们的输入向量/字母词汇就是四个4 x 1矩阵，每个矩阵表示一个特定字母。独热编码是这些任务的标准做法。此步骤实际上是数据预处理步骤。
- en: Initializing the weight matrix and more
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 初始化权重矩阵及其他
- en: When there are neural networks, there are weights. This is true, right? But
    before we start to deal with the weights for our RNN, let's see exactly where
    they are needed.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及神经网络时，就会有权重。这是对的吗？但在我们开始处理RNN的权重之前，让我们先看一下它们具体在哪里需要。
- en: 'There are two different weight matrices in the case of an RNN—one for the input
    neuron (remember that we feed feature vectors only through neurons) and one for
    the recurrent neuron. A particular state in an RNN is produced using the following
    two equations:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNN中有两个不同的权重矩阵——一个用于输入神经元（记住，我们只通过神经元传递特征向量），一个用于递归神经元。RNN中的特定状态是通过以下两个方程式产生的：
- en: '![](img/0a36036b-7a4b-4307-b60b-5031283c9060.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a36036b-7a4b-4307-b60b-5031283c9060.png)'
- en: 'To understand what each term means in the first equation, refer to the following
    image (don''t worry, we will get to the second equation):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解第一个方程中的每个术语是什么意思，请参考下图（别担心，我们稍后会讨论第二个方程）：
- en: '![](img/8bbaa47e-7702-433d-aac9-870000b96aa9.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8bbaa47e-7702-433d-aac9-870000b96aa9.png)'
- en: 'The first pass of the RNN ![](img/4b9aaa2a-87d5-487b-9d8c-6a54cb705d09.png)is
    the letter *w*. We will randomly initialize the two weight matrices as present
    in the equation (1). Assume that the matrix ![](img/06871357-5a95-4e0b-aee0-90bfb11dcc7c.png)
    after getting initialized looks like the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的第一次传递 ![](img/4b9aaa2a-87d5-487b-9d8c-6a54cb705d09.png) 是字母 *w*。我们将随机初始化方程（1）中出现的两个权重矩阵。假设矩阵
    ![](img/06871357-5a95-4e0b-aee0-90bfb11dcc7c.png) 初始化后的样子如下：
- en: '![](img/94ec44a4-d673-4c34-854b-e1d35c371270.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94ec44a4-d673-4c34-854b-e1d35c371270.png)'
- en: 'The ![](img/6ccce969-b83d-44cd-87d9-647322b6f44a.png) matrix is 3 x 4:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/6ccce969-b83d-44cd-87d9-647322b6f44a.png) 矩阵是3 x 4：'
- en: '*x* = 3, as we have three recurrent neurons in the recurrent layer'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x* = 3，因为我们在递归层中有三个递归神经元'
- en: '*h* = 4, as our vocabulary is 4'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*h* = 4，因为我们的词汇量是4'
- en: The matrix ![](img/a1666498-816b-4b2b-87a6-4c777b0e2976.png) is a 1 x 1 matrix.
    Let's take its value as 0.35028053\. Let's also introduce the bias term *b* here,
    which is also a 1 x 1 matrix, 0.6161462\. In the next step, we will put these
    values together and determine the value of ![](img/8d4bf29d-dca0-4114-9f7b-79469774b4a9.png).
    (We will deal with the second equation later.)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 ![](img/a1666498-816b-4b2b-87a6-4c777b0e2976.png) 是一个 1 x 1 矩阵。我们假设它的值为 0.35028053。我们还在这里引入偏置项
    *b*，它也是一个1 x 1的矩阵，值为 0.6161462。在下一步中，我们将把这些值结合起来并确定 ![](img/8d4bf29d-dca0-4114-9f7b-79469774b4a9.png)
    的值。（我们稍后会处理第二个方程式。）
- en: Putting the weight matrices together
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将权重矩阵组合起来
- en: 'Let''s determine ![](img/c1ccae7d-7e2d-4c5c-9d8f-8f3d7d29a2bf.png) first. ![](img/d9784c61-5546-4ef2-8485-d134d1b8c132.png)is
    a 4 x 1 matrix representing the letter *w*, which we defined earlier. The standard
    rules of matrix multiplication apply here:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先确定 ![](img/c1ccae7d-7e2d-4c5c-9d8f-8f3d7d29a2bf.png)。![](img/d9784c61-5546-4ef2-8485-d134d1b8c132.png)
    是一个 4 x 1 矩阵，表示我们之前定义的字母 *w*。这里适用矩阵乘法的标准规则：
- en: '![](img/234a5eeb-d4fd-4fb8-ad16-468a49bb1010.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/234a5eeb-d4fd-4fb8-ad16-468a49bb1010.png)'
- en: 'Now we will calculate the term ![](img/a1b2ed80-6b11-4cdd-b69f-4dd54bf630b9.png).
    We will shortly see the significance of the bias term. Since *w* is the first
    letter that we are feeding to the network, it does not have any previous state
    therefore, we will take ![](img/47774b6e-33a9-495f-9c90-7a98bc56ad42.png) as a
    matrix of 3 x 1 consisting of zeros:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将计算术语 ![](img/a1b2ed80-6b11-4cdd-b69f-4dd54bf630b9.png)。我们将很快看到偏置项的意义。由于
    *w* 是我们输入到网络中的第一个字母，它没有任何前一个状态，因此，我们将取 ![](img/47774b6e-33a9-495f-9c90-7a98bc56ad42.png)
    作为一个3 x 1的零矩阵：
- en: '![](img/7dcad348-20fc-4827-96c5-f4e62ffd10bb.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7dcad348-20fc-4827-96c5-f4e62ffd10bb.png)'
- en: 'Note that if we didn''t take the bias term, we would have got a matrix consisting
    of only zeros. We will now add these two matrices as per the equation (1). The
    result of this addition is a 3 x 1 matrix and is stored in ![](img/fe80e464-21d6-468f-bbc8-1a23d3a7a9a5.png)
    (which in this case is ![](img/def168f0-bef1-4905-8c0d-1ced2411df48.png)):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们没有考虑偏置项，我们将得到一个全为零的矩阵。现在我们将按照方程（1）将这两个矩阵相加。这次加法的结果是一个3 x 1矩阵，并存储在 ![](img/fe80e464-21d6-468f-bbc8-1a23d3a7a9a5.png)
    中（在此情况下是 ![](img/def168f0-bef1-4905-8c0d-1ced2411df48.png)）：
- en: '![](img/8d9339a0-4050-4c6f-8756-c05e881ccb3c.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d9339a0-4050-4c6f-8756-c05e881ccb3c.png)'
- en: Following the equation (1), all we need to do is apply the activation function
    to this matrix.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 根据公式(1)，我们需要做的就是将激活函数应用于这个矩阵。
- en: Applying activation functions and the final output
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用激活函数和最终输出
- en: 'When it comes to RNNs, ![](img/ff3c61ce-bd96-4088-8fbe-4cf175154e52.png) is
    a good choice as an activation function. So, after applying ![](img/00494d15-6793-4ec3-9fb8-3c3f40d6c9d9.png),
    the matrix looks like the following:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到RNN时，![](img/ff3c61ce-bd96-4088-8fbe-4cf175154e52.png)是一个很好的激活函数选择。所以，在应用![](img/00494d15-6793-4ec3-9fb8-3c3f40d6c9d9.png)之后，矩阵如下所示：
- en: '![](img/fefb7cb2-a21e-4ac2-8aaa-84401ab2bf71.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fefb7cb2-a21e-4ac2-8aaa-84401ab2bf71.png)'
- en: 'We have got the result of ![](img/5a3592f6-3ebd-4459-aa50-dfaa77b7eedd.png).
    *ht* acts as ![](img/1f0a601e-c001-4c1a-86b6-84ffa79cc16e.png) for the next time-step.
    We will now calculate the value of ![](img/62216a8f-4753-45e0-853e-a73540ac5f36.png)
    using equation (2). We will require another weight matrix ![](img/7e9e4820-3c32-47c8-9529-a914a16cdd59.png)
    (of shape 4 x 3) that is randomly initialized:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经得到了![](img/5a3592f6-3ebd-4459-aa50-dfaa77b7eedd.png)的结果。*ht*在下一个时间步长中充当![](img/1f0a601e-c001-4c1a-86b6-84ffa79cc16e.png)。接下来，我们将使用公式(2)计算![](img/62216a8f-4753-45e0-853e-a73540ac5f36.png)的值。我们需要另一个权重矩阵![](img/7e9e4820-3c32-47c8-9529-a914a16cdd59.png)（形状为4
    x 3），它是随机初始化的：
- en: '![](img/281bbcd0-d1ba-46a1-9998-7609b6319c5f.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/281bbcd0-d1ba-46a1-9998-7609b6319c5f.png)'
- en: 'After applying the second equation, the value of ![](img/d7a94c7b-b682-4671-99b8-63180437e306.png)
    becomes a 4 x 1 matrix:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用第二个公式后，![](img/d7a94c7b-b682-4671-99b8-63180437e306.png)的值变成了一个4 x 1的矩阵：
- en: '![](img/0a969531-c833-44fd-8c6f-f034c65b267d.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a969531-c833-44fd-8c6f-f034c65b267d.png)'
- en: 'Now, in order to predict what might be the next letter that comes after *w*
    (remember, we started all our calculations with the letter *w* and we still left
    with the first pass of the RNN) to make a suitable word from the given vocabulary,
    we will apply the softmax function to ![](img/220321b9-76f4-476f-9fac-b77514106737.png).
    This will output a set of probabilities for each of the letters from the vocabulary:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了预测在* w*之后可能出现的下一个字母（记住，我们所有的计算都是从字母*w*开始的，并且我们仍然停留在RNN的第一次传递阶段），为了从给定的词汇中构建合适的单词，我们将对![](img/220321b9-76f4-476f-9fac-b77514106737.png)应用softmax函数。这样会输出一组来自词汇中每个字母的概率：
- en: '![](img/20663bb4-96fe-4e90-bf5e-5ced2e6e9e10.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20663bb4-96fe-4e90-bf5e-5ced2e6e9e10.png)'
- en: If anyone is curious about learning what a softmax function looks like, there
    is an extremely helpful article at [http://bit.ly/softmaxfunc](http://bit.ly/softmaxfunc).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人好奇想了解softmax函数是什么样子，可以阅读这篇非常有帮助的文章：[http://bit.ly/softmaxfunc](http://bit.ly/softmaxfunc)。
- en: So, the RNN tells us that the next letter after *w* is more likely to be an
    ![](img/fdcd5b37-b1e7-4248-8ec0-1569c96a25a8.png). With this, we finish the initial
    pass of the RNN. As an exercise, you can play around with the *ht* value we got
    from this pass and apply it (along with the next letter *h*) to the next pass
    of the RNN to see what happens.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，RNN告诉我们，*w*之后最有可能的字母是![](img/fdcd5b37-b1e7-4248-8ec0-1569c96a25a8.png)。至此，我们完成了RNN的初始传递。作为练习，你可以尝试调整我们从这一传递中得到的*ht*值，并将其（与下一个字母*h*一起）应用于RNN的下一个传递，看看会发生什么。
- en: 'Now, let''s get to the most important question—what is the network learning?
    Again, weights and biases! You might have guessed the next sentence already. These
    weights are further optimized using backpropagation. Now, this backpropagation
    is a little bit different from what we have seen earlier. This version of backpropagation
    is referred to as **backpropagation through time**. We won''t be learning about
    this. Before finishing off this section, let''s summarize the steps (after one-hot
    encoding of the vocabulary) that were performed during the forward pass of the
    RNN:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来回答最重要的问题——网络在学习什么？再次回答，权重和偏置！你可能已经猜到了下一个句子。这些权重通过反向传播进一步优化。现在，这种反向传播与我们之前看到的有所不同。这种版本的反向传播被称为**时间反向传播**。我们将不会学习这个内容。在结束本节之前，让我们总结一下在RNN前向传播过程中执行的步骤（在对词汇进行独热编码之后）：
- en: Initialize the weight matrices randomly.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机初始化权重矩阵。
- en: Calculate ![](img/06efe53c-1ce5-45a5-9526-995ad648f78c.png) using equation (1).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用公式(1)计算![](img/06efe53c-1ce5-45a5-9526-995ad648f78c.png)。
- en: Calculate ![](img/71c99373-2b99-42eb-9199-be1382be273b.png) using equation (2).
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用公式(2)计算![](img/71c99373-2b99-42eb-9199-be1382be273b.png)。
- en: Apply the softmax function to ![](img/fbe962da-d664-4cf8-9c26-31ddc850c175.png)to
    get the probabilities of each of the letters in the vocabulary.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对![](img/fbe962da-d664-4cf8-9c26-31ddc850c175.png)应用softmax函数，得到词汇中每个字母的概率。
- en: It is good to know that apart from CNNs and RNNs, there are other types of neural
    networks, such as auto-encoders, generative adversarial networks, capsule networks,
    and so on. In the previous two sections, we learned about two of the most powerful
    types of neural network in detail. But when we talk about cutting-edge deep-learning
    applications, are these networks good enough to be used? Or do we need more enhancements
    on top of these? It turns out that although these architectures perform well,
    they fail to scale, hence the need for more sophisticated architectures. We will
    get to some of these specialized architectures in the next chapters.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 值得了解的是，除了 CNN 和 RNN 外，还有其他类型的神经网络，如自编码器、生成对抗网络、胶囊网络等。在前两节中，我们详细了解了两种最强大的神经网络类型。但当我们谈论前沿的深度学习应用时，这些网络足够用吗？还是我们需要在这些基础上进行更多的增强？事实证明，尽管这些架构表现良好，但它们未能扩展，因此需要更复杂的架构。在接下来的章节中，我们将介绍一些这些专门的架构。
- en: We have covered a good amount of theory since [Chapter 1](f97d928f-3614-4d12-ad37-d5736008f542.xhtml),
    *Demystifying Artificial Intelligence and Fundamentals of Machine Learning*. In
    the next few sections, we will be diving into some hands-on examples.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 [第一章](f97d928f-3614-4d12-ad37-d5736008f542.xhtml)《揭开人工智能与机器学习基础的神秘面纱》以来，我们已经覆盖了不少理论内容。在接下来的几节中，我们将深入一些实际操作的例子。
- en: Exploring Jupyter Notebooks
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Jupyter Notebooks
- en: While working on a project relating to deep learning, you must deal with a huge
    amount of variables of various types and arrays of various dimensions. Also, since
    the data contained in them is massive and keeps changing after nearly every step,
    we need a tool that helps us to observe the output produced by each step so that
    we can proceed accordingly. A Jupyter Notebook is one such tool. Jupyter Notebooks
    are known for their simplicity, and their wide support of features and platforms
    are currently the standard tool for developing deep-learning solutions. The reasons
    for their popularity can be understood by considering the fact that several of
    the top tech giants offer their own version of the tool, such as Google Colaboratory
    and Microsoft Azure Notebooks. Moreover, the popular code-hosting website GitHub
    has been providing a native rendering of Jupyter Notebook since 2016.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行与深度学习相关的项目时，您必须处理大量各种类型的变量和不同维度的数组。此外，由于它们所包含的数据庞大且几乎在每一步后都会发生变化，我们需要一个工具来帮助我们观察每个步骤产生的输出，以便我们能够据此继续进行。Jupyter
    Notebook 就是这样一个工具。Jupyter Notebooks 因其简单性而闻名，并且其广泛的功能支持和平台兼容性使其成为目前开发深度学习解决方案的标准工具。它们受欢迎的原因可以通过考虑到多个顶级科技巨头都提供了自己版本的工具来理解，例如
    Google Colaboratory 和 Microsoft Azure Notebooks。此外，流行的代码托管网站 GitHub 自 2016 年起便提供了
    Jupyter Notebook 的本地渲染。
- en: Installing Jupyter Notebook
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 Jupyter Notebook
- en: Let's begin with the installation of Jupyter Notebook.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从安装 Jupyter Notebook 开始。
- en: Installation using pip
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 pip 安装
- en: If you already have Python installed on your system, you can install the Jupyter
    package from the `pip` repository to start using Jupyter Notebooks quickly.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的系统已经安装了 Python，您可以从 `pip` 仓库安装 Jupyter 包，以快速开始使用 Jupyter Notebooks。
- en: 'For Python 3, use the following:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Python 3，请使用以下命令：
- en: '[PRE1]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For Python 2, use the following:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Python 2，请使用以下命令：
- en: '[PRE2]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For Mac users, if the `pip` installation is not found, you can download the
    latest Python version, which carries `pip` bundled with it.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Mac 用户，如果找不到 `pip` 安装，您可以下载最新的 Python 版本，其中已包含 `pip`。
- en: Installation using Anaconda
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Anaconda 安装
- en: While it is possible to install Jupyter as a single package from `pip`, it is
    strongly recommended that you install the Anaconda distribution for Python, which
    automatically installs Python, Jupyter, and several other packages required for
    machine learning and data science. Anaconda makes it very easy to deal with the
    various package versions and update dependency packages or dependent packages.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以通过 `pip` 安装 Jupyter 单个包，但强烈建议您安装 Anaconda 发行版，它会自动安装 Python、Jupyter 和机器学习与数据科学所需的其他几个包。Anaconda
    使得处理各种包版本以及更新依赖包或相关包变得非常容易。
- en: Firstly, download the correct Anaconda distribution for your system and requirements
    from [https://www.anaconda.com/downloads](https://www.anaconda.com/downloads)
    and then follow the corresponding installation steps given on the website.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从 [https://www.anaconda.com/downloads](https://www.anaconda.com/downloads)
    下载适合您系统和需求的 Anaconda 发行版，然后按照网站上给出的相应安装步骤进行安装。
- en: Verifying the installation
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证安装
- en: 'To check whether Jupyter has correctly installed, run the following command
    in Command Prompt (Windows) or Terminal (Linux/Mac):'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查 Jupyter 是否正确安装，请在命令提示符（Windows）或终端（Linux/Mac）中运行以下命令：
- en: '[PRE3]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will be able to see some logging output at the terminal (henceforth, the
    default term for Command Prompt on Windows and Terminal on Linux or Mac). After
    that, your default browser, will open up and you will be taken to a link on the
    browser which will resemble the following image:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 你将能够在终端（以下简称 Windows 上的命令提示符或 Linux/Mac 上的终端）看到一些日志输出。之后，你的默认浏览器将打开，并跳转到浏览器中的一个链接，链接内容类似于以下图像：
- en: '![](img/580396c4-cea4-4e26-9fa3-b206c340510a.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/580396c4-cea4-4e26-9fa3-b206c340510a.png)'
- en: Under the Files tab, a basic file manager is provided that the user can use
    to create, upload, rename, delete, and move files.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在 "Files" 标签页下，提供了一个基本的文件管理器，用户可以用它来创建、上传、重命名、删除和移动文件。
- en: The Running tab lists all the currently running Jupyter Notebooks, which can
    be shut down from the listing displayed.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '"Running" 标签页列出了所有当前运行的 Jupyter Notebook，用户可以从列表中关闭它们。'
- en: The Clusters tab provides an overview of all the available IPython clusters.
    In order to use this feature, you are required to install the IPython Parallel
    extension for your Python environment.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '"Clusters" 标签页提供了所有可用 IPython 集群的概览。为了使用此功能，您需要为 Python 环境安装 IPython Parallel
    扩展。'
- en: Jupyter Notebooks
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jupyter Notebooks
- en: 'A Jupyter Notebook by default is identified by the `.ipynb` extension. Upon
    clicking on the name of once such notebook in the file manager provided by Jupyter,
    you''ll be presented with a screen resembling the following:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Jupyter Notebook 的文件扩展名是 `.ipynb`。当你在 Jupyter 提供的文件管理器中点击该类笔记本的名称时，屏幕上将显示一个类似以下的界面：
- en: '![](img/9585b379-18ef-43e1-9a0a-60b00b5e6efb.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9585b379-18ef-43e1-9a0a-60b00b5e6efb.png)'
- en: The topmost section, where you can see a menu bar, a toolbar, and the title
    of the notebook, is called the **header**. On the right side of the header you
    can see the environment in which the notebook is executing, and when any task
    is running, the white circle beside the environment language's name turns gray.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最上方的部分，你可以看到一个菜单栏、工具栏和笔记本标题，称为**标题**。标题的右侧，你可以看到笔记本执行的环境，当有任务正在运行时，环境语言名称旁的白色圆圈会变成灰色。
- en: Below the header is the body of the notebook, which is composed of cells stacked
    vertically. Each cell in the body of the notebook is either a block of code, a
    markdown cell, or a raw cell. A code cell can have an output cell attached below
    it, which the user cannot edit manually. This holds the output produced by the
    code cell associated with it.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在标题下方是笔记本的主体，它由垂直堆叠的单元格组成。笔记本主体中的每个单元格可以是代码块、Markdown 单元格或原始单元格。代码单元格下方可以附加一个输出单元格，用户无法手动编辑该单元格。该单元格保存由其关联的代码单元格生成的输出。
- en: 'In a Jupyter Notebook, the keyboard behaves differently for different **modes**
    of a cell For this reason, these notebooks are called **modal**. There are two
    modes in which a notebook cell can operate: the **command** mode and the **editx**
    mode.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 中，键盘在不同的**模式**下行为不同，因此这些笔记本被称为**模式化**的。一个笔记本的单元格可以操作的模式有两种：**命令**模式和**编辑**模式。
- en: While a cell is in command mode, it has a gray border. In this mode, the cell
    contents cannot be changed. The keys of the keyboard in this mode are mapped to
    several shortcuts that can be used to modify the cell or the notebook as a whole.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 当单元格处于命令模式时，它有一个灰色边框。在此模式下，单元格内容不能更改。在此模式下，键盘上的按键会映射到一些快捷键，用户可以用它们来修改单元格或整个笔记本。
- en: While in command mode, if you press the *Enter* key on the keyboard, the cell
    mode changes to the edit mode. While in this mode, the contents of the cell can
    be changed and the basic keyboard shortcuts that are available in the usual textboxes
    in the browser can be invoked.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令模式下，如果按下键盘上的*Enter*键，单元格模式会切换到编辑模式。在该模式下，可以更改单元格的内容，并可以调用浏览器中常见文本框的基本快捷键。
- en: 'To exit the edit mode, the user can use the *Esc* key. To run the particular
    cell, the user has to input *Shift* + *Return*, which will do one of the following
    in each case:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 要退出编辑模式，用户可以按*Esc*键。要运行特定的单元格，用户需要输入*Shift* + *Return*，这将在每种情况下执行以下操作之一：
- en: For a markdown cell, the rendered markdown shall be displayed.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Markdown 单元格，渲染后的 Markdown 会显示出来。
- en: For a raw cell, the raw text as entered shall be visible.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于原始单元格，输入的原始文本将会显示。
- en: For a code cell, the code will be executed and if it produces some output, an
    output cell attached to the code cell will be created and the output will get
    displayed there. If the code in the cell asks for an input, an input field will
    appear and the cell's code execution stalls until the input is provided.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于代码单元格，代码将被执行，如果产生输出，将创建一个附加到代码单元格的输出单元格，并在那里显示输出。如果单元格中的代码要求输入，将显示一个输入字段，并且该单元格的代码执行将停顿，直到提供输入。
- en: Jupyter also allows the manipulation of text files and Python script files using
    its in-built text editor. It is also possible to invoke the system terminal from
    within the Jupyter environment.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter 还允许使用其内置文本编辑器操作文本文件和 Python 脚本文件。还可以从 Jupyter 环境内调用系统终端。
- en: Setting up a deep-learning-based cloud environment
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置基于深度学习的云环境
- en: Before we begin setting up a cloud-based deep learning environment, we might
    wonder why would we need it or how a cloud-based deep learning environment would
    benefit us. Deep learning requires a massive amount of mathematical calculation.
    At every layer of the neural network, there is a mathematical matrix undergoing
    multiplication with another or several other such matrices. Furthermore, every
    data point itself can be a vector instead of a singular entity. Now, to train
    over several repetitions, such a deep learning model would require a lot of time
    just because of the number of mathematical operations involved.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始设置基于云的深度学习环境之前，我们可能会想知道为什么我们需要它或者云基深度学习环境如何使我们受益。深度学习需要大量的数学计算。在神经网络的每一层，都有一个数学矩阵与另一个或多个这样的矩阵进行乘法运算。此外，每个数据点本身可以是一个向量而不是一个单一实体。现在，为了在多次重复训练过程中训练这样的深度学习模型，将需要大量时间，仅仅是因为涉及的数学运算数量。
- en: A GPU-enabled machine would be much more efficient at executing these operations
    because a GPU is made specifically for high-speed mathematical calculations however,
    GPU-enabled machines are costly and may not be affordable to everyone. Furthermore,
    considering that multiple developers work on the same software in a work environment,
    it might be a very costly option to buy GPU-enabled machines for all the developers
    on the team. For these reasons, the idea of a GPU-enabled cloud computing environment
    has a strong appeal.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GPU 可能会更高效执行这些操作，因为 GPU 专门用于高速数学计算，然而，GPU 启用的机器成本高昂，可能不是每个人都能负担得起。此外，考虑到多个开发者在工作环境中使用同一软件，为团队中所有开发者购买
    GPU 启用的机器可能是一个非常昂贵的选择。因此，GPU 启用的云计算环境的概念非常具有吸引力。
- en: Companies nowadays are increasingly leaning towards the usage of GPU-enabled
    cloud environments for their development teams, which can lead to the creation
    of a common environment for all of the developers as well as the facilitation
    of high-speed computation.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，公司越来越倾向于为其开发团队使用 GPU 启用的云环境，这可以为所有开发者创建一个共同的环境，并促进高速计算。
- en: Setting up an AWS EC2 GPU deep learning environment
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 AWS EC2 GPU 深度学习环境
- en: 'In this section, we will learn how to set up a deep learning specific instance
    on AWS. Before you can begin working with AWS, you will need to create an account
    on the AWS console. To do so, go through the following steps:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何在 AWS 上设置一个特定的深度学习实例。在您开始使用 AWS 之前，您需要在 AWS 控制台上创建一个帐户。为此，请按照以下步骤操作：
- en: Visit [https://console.aws.amazon.com](https://console.aws.amazon.com) and you'll
    be presented with a login/sign up screen.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [https://console.aws.amazon.com](https://console.aws.amazon.com)，您将看到登录/注册屏幕。
- en: If you do not already have an AWS account, click on Create a new AWS account
    and follow the steps to create one, which might require you to enter your debit/credit
    card details to enable billing for your account.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您尚未拥有 AWS 帐户，请点击“创建新的 AWS 帐户”并按照步骤创建一个帐户，这可能需要您输入借记/信用卡详细信息以启用帐单。
- en: 'Upon logging into your account, on the dashboard, click on EC2 in the All services
    section, as shown in the following screenshot:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录您的帐户后，在仪表板上，在“所有服务”部分点击 EC2，如下截图所示：
- en: '![](img/a1e52502-90c0-477b-82fa-70b9ee8e3450.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1e52502-90c0-477b-82fa-70b9ee8e3450.png)'
- en: Once you have reached the EC2 management page within the AWS console, you'll
    need to go through the steps in the following sections to create an instance for
    your deep learning needs.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您进入 AWS 控制台中的 EC2 管理页面，您将需要按照以下部分的步骤来为您的深度学习需求创建实例。
- en: 'Step 1: Creating an EC2 GPU-enabled instance'
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一步：创建一个 EC2 GPU 启用实例
- en: 'First, select the Ubuntu 16.04 or 18.04 LTS AMI:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，选择 Ubuntu 16.04 或 18.04 LTS AMI：
- en: '![](img/41e6aa63-bb63-4a50-bbc7-3ae7249e74f9.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41e6aa63-bb63-4a50-bbc7-3ae7249e74f9.png)'
- en: 'Then, choose a GPU-enabled instance configuration. The `g2.2xlarge` is a good
    choice for a starter deep learning environment:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，选择一个支持 GPU 的实例配置。`g2.2xlarge` 是一个适合初学者的深度学习环境配置：
- en: '![](img/9b74f7f3-e953-46a0-a975-3d9b78497042.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9b74f7f3-e953-46a0-a975-3d9b78497042.png)'
- en: Next, configure the required instance settings or leave them as their default
    and proceed to the storage step. Here, a recommended size of the volume is 30
    GB. You can then proceed to launch the instance with the default options.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，配置所需的实例设置，或者保持默认设置并继续到存储步骤。此处，建议的磁盘大小为 30 GB。然后，您可以使用默认选项继续启动实例。
- en: Assign an EC2 key pair to your instance so that you can access the instance's
    terminal over SSH from your system. If you name the key pair `abc`, then a file
    named `abc.pem` would download automatically to your browser's default download
    location.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 将 EC2 密钥对分配给您的实例，这样您就可以通过 SSH 从您的系统访问实例的终端。如果您将密钥对命名为 `abc`，则名为 `abc.pem` 的文件会自动下载到浏览器的默认下载位置。
- en: 'Step 2: SSHing into your EC2 instance'
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 2：SSH 连接到您的 EC2 实例
- en: Open up a terminal on your system and using `cd`, navigate to the directory
    that your `abc.pem` file is stored in.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在系统上打开终端，使用 `cd` 命令，导航到存储 `abc.pem` 文件的目录。
- en: 'If you''re unfamiliar with the `cd` command, consider a scenario in which you
    are inside a folder named `Folder1`, which has the following contents:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉 `cd` 命令，假设您位于一个名为 `Folder1` 的文件夹中，文件夹内容如下：
- en: '[PRE4]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To access any files inside the folder named `Folder2`, you''ll have to change
    your working directory to that folder. To do so, you can use the following example
    of the `cd` command:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问名为 `Folder2` 的文件夹中的任何文件，您需要将工作目录切换到该文件夹。为此，您可以使用以下 `cd` 命令示例：
- en: '[PRE5]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that this command only works when you're already inside `Folder1`, which
    can be reached with a similar usage of the `cd` command from anywhere on the system.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只有在您已经进入 `Folder1` 文件夹时，此命令才有效，您可以使用类似的 `cd` 命令从系统中的任何位置进入该文件夹。
- en: 'You can read more about the usage of any command on a Linux system by using
    the following command:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令查看 Linux 系统上任何命令的使用方法：
- en: '[PRE6]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For example, you can use the following:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以使用以下内容：
- en: '[PRE7]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, set the permissions required for SSH using the key file by entering the
    following:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过输入以下命令设置所需的 SSH 权限：
- en: '[PRE8]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, to SSH into your instance, you will need its public IP or instance public
    DNS. For example, if the public IP is `1.2.3.4`, then use the following command:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要 SSH 连接到您的实例，您需要其公共 IP 或实例公共 DNS。例如，如果公共 IP 是 `1.2.3.4`，则使用以下命令：
- en: '[PRE9]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The public IP of the AWS instance can be found on the details panel below the
    list of running instances on the AWS console in the EC2 management page.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 实例的公共 IP 可以在 AWS 控制台 EC2 管理页面中运行实例列表下方的详细信息面板中找到。
- en: 'Step 3: Installing CUDA drivers on the GPU instance'
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 3：在 GPU 实例上安装 CUDA 驱动程序
- en: 'First, update/install the NVIDIA graphics drivers:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，更新/安装 NVIDIA 显卡驱动：
- en: '[PRE10]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, `xxx` can be replaced with the graphics hardware version installed on
    your instance, which can be found in the instance details.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`xxx` 可以替换为安装在实例上的显卡硬件版本，您可以在实例详细信息中找到该版本。
- en: 'Next, download the CUDA deb file (this code is for the latest version at the
    time of writing, from Jan, 2019):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，下载 CUDA deb 文件（此代码适用于撰写时的最新版本，2019年1月）：
- en: '[PRE11]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, proceed with the following commands:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，继续执行以下命令：
- en: '[PRE12]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'To verify whether everything was installed successfully, run the following
    commands:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证是否成功安装了所有内容，请运行以下命令：
- en: '[PRE13]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If both the commands produce output without any warnings or errors, then the
    installation is successful.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这两个命令都没有产生任何警告或错误的输出，那么安装成功。
- en: 'Step 4: Installing the Anaconda distribution of Python'
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 4：安装 Anaconda Python 发行版
- en: 'First, download the Anaconda installer script:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，下载 Anaconda 安装脚本：
- en: '[PRE14]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, set the script to executable:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将脚本设置为可执行：
- en: '[PRE15]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, run the installation script:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，运行安装脚本：
- en: '[PRE16]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The installer will ask for several options. To verify successful installation,
    use the following command:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 安装程序将询问一些选项。为了验证安装是否成功，使用以下命令：
- en: '[PRE17]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The Python3 REPL loads into the terminal with a banner reflecting the Anaconda
    distribution version installed on your instance.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: Python3 REPL 会加载到终端，并显示已安装的 Anaconda 版本的横幅。
- en: 'Step 5: Run Jupyter'
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步骤 5：运行 Jupyter
- en: 'Use the following command to get the Jupyter Notebook server started on the
    instance:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令在实例上启动 Jupyter Notebook 服务器：
- en: '[PRE18]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output on the terminal will contain a URL on opening, with which you will
    be able to access the Jupyter Notebook running on your EC2 GPU instance.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 打开终端后，输出将包含一个 URL，你可以通过该 URL 访问运行在 EC2 GPU 实例上的 Jupyter Notebook。
- en: Deep learning on Crestle
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Crestle 上进行深度学习
- en: While a customized deep learning environment can be of use when you need greater
    control over the system—such as when you want to have third-party applications
    working along with your deep learning model—at other times, you may not have such
    needs, and you'll only be interested in performing deep learning on the cloud,
    quickly and in a collaborative manner. In such circumstances, paying the cost
    of an AWS `g2.2xlarge` instance would be much higher than that of paying only
    for computing time or GPU time used.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 定制的深度学习环境在你需要对系统进行更大控制时很有用——比如当你希望第三方应用与深度学习模型一起工作时——但在其他情况下，你可能没有这样的需求，可能只想快速地在云端进行深度学习，并且希望能够协作。在这种情况下，支付
    AWS `g2.2xlarge` 实例的费用会比仅支付计算时间或 GPU 时间的费用要高得多。
- en: 'Crestle is a service that provides GPU-enabled Jupyter Notebooks online at
    very affordable pricing. To begin using Crestle, go through the following steps:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Crestle 是一个提供 GPU 支持的 Jupyter Notebook 在线服务，价格非常实惠。要开始使用 Crestle，请按照以下步骤操作：
- en: Log on to [www.crestle.com](http://www.crestle.com).
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到[www.crestle.com](http://www.crestle.com)。
- en: Click on Sign Up and fill up the sign-up form that appears.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 Sign Up 并填写出现的注册表单。
- en: Check your email for an account confirmation link. Activate your account and
    sign in.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查你的电子邮件，找到帐户确认链接。激活你的帐户并登录。
- en: You'll be taken to the dashboard where you'll find a button reading Start Jupyter.
    You will have the option of using the GPU or keeping it disabled. Click on the
    Start Jupyter button with the GPU option enabled.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将被带到仪表板，在那里你会看到一个按钮，写着“Start Jupyter”。你可以选择使用 GPU 或保持禁用。点击启用 GPU 选项的 Start
    Jupyter 按钮。
- en: You will be presented with a Jupyter environment running on the cloud with GPU
    support. While the pricing is subject to change with the passage of time, it is
    one of the most affordable solutions available on the internet as of January 2020.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 你将进入一个运行在云端的 Jupyter 环境，支持 GPU。虽然价格会随着时间的推移而变化，但截至 2020 年 1 月，它仍是互联网上最实惠的解决方案之一。
- en: Other deep learning environments
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他深度学习环境
- en: As well as the aforementioned ways of performing GPU-enabled deep learning on
    the cloud, you can also, in certain circumstances, choose to use other platforms.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的云端启用 GPU 的深度学习方式外，在某些情况下，你也可以选择使用其他平台。
- en: Google Colaboratory is a freely available Jupyter Notebook service that is accessible
    at [https://colab.research.google.com](https://colab.research.google.com). Colaboratory
    notebooks are stored on the user's Google Drive and so have a storage limit of
    15 GB. It is possible to store large datasets on Google Drive and include them
    in the project with the help of the Google Drive Python API. By default, the GPU
    is disabled on Colaboratory and has to be manually turned on.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colaboratory 是一个免费提供的 Jupyter Notebook 服务，可以在[https://colab.research.google.com](https://colab.research.google.com)访问。Colaboratory
    笔记本存储在用户的 Google Drive 上，因此存储限制为 15 GB。可以通过 Google Drive Python API 将大型数据集存储在
    Google Drive 上，并在项目中使用这些数据集。默认情况下，Colaboratory 禁用 GPU，需要手动开启。
- en: Kaggle is yet another platform that was specifically built to carry out contests
    on data science. It provides a Jupyter-Notebooks-like environment called a **kernel**.
    Each kernel is provided with a large amount of RAM and free GPU power however,
    there are more strict storage limits on Kaggle than on Google Colaboratory, and
    so it is an effective option when the computation is intensive but the data that
    is to be used and the output is not very large.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 是另一个专门为数据科学竞赛而建立的平台。它提供了一个类似 Jupyter Notebook 的环境，称为**kernel**。每个 kernel
    都提供大量的内存和免费的 GPU 计算能力。然而，Kaggle 的存储限制比 Google Colaboratory 更严格，因此当计算密集型任务但数据和输出不大时，它是一个有效的选择。
- en: Exploring NumPy and pandas
  id: totrans-342
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 NumPy 和 pandas
- en: NumPy and pandas are the backbone of nearly every data-science-related library
    available in the Python language. While pandas is built on top of NumPy, NumPy
    itself is a wrapping of Python around high-performance C code to facilitate superior
    mathematical computing in Python than Python itself in its pure form can provide.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 和 pandas 是几乎所有 Python 语言中与数据科学相关的库的基础。虽然 pandas 是建立在 NumPy 之上的，但 NumPy
    本身是将 Python 包装在高性能的 C 代码上，从而在 Python 中提供比纯 Python 更优秀的数学计算能力。
- en: Almost all deep learning software developed in Python in one way or another
    depends upon NumPy and pandas. It is therefore important to have a good understanding
    of both libraries and the features that they can provide.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有使用 Python 开发的深度学习软件，都或多或少依赖于 NumPy 和 pandas。因此，深入理解这两个库及其提供的特性非常重要。
- en: NumPy
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NumPy
- en: 'NumPy is an acronym for **Numerical Python**. Vanilla Python lacks the implementation
    of arrays, which are close analogs of the mathematical matrices used to develop
    machine learning models. NumPy brings to Python support for multidimensional arrays
    and high-performance computing features. It can be included into any Python code
    by using the following import statement:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 是 **Numerical Python**（数值计算 Python）的缩写。普通的 Python 缺乏数组的实现，而数组是用于开发机器学习模型的数学矩阵的近似对象。NumPy
    为 Python 提供了对多维数组和高性能计算特性的支持。可以通过以下导入语句将其包含到任何 Python 代码中：
- en: '[PRE19]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`np` is a commonly used convention for importing NumPy.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '`np` 是导入 NumPy 时常用的约定。'
- en: NumPy arrays
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NumPy 数组
- en: 'There are several methods to create arrays in NumPy. The following are some
    notable ones:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中创建数组有多种方法。以下是一些显著的方法：
- en: '`np.array`: To convert Python lists to NumPy arrays:'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`np.array`：将 Python 列表转换为 NumPy 数组：'
- en: '![](img/bea02ad3-cb06-4762-8ddd-1ecd8acd8a7a.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bea02ad3-cb06-4762-8ddd-1ecd8acd8a7a.png)'
- en: '`np.ones` or `np.zeros`: To create a NumPy array of all 1s or all 0s:'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`np.ones` 或 `np.zeros`：创建一个全为 1 或全为 0 的 NumPy 数组：'
- en: '![](img/be91b2e2-dce0-40eb-ae79-746dd9ddb97c.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be91b2e2-dce0-40eb-ae79-746dd9ddb97c.png)'
- en: '`np.random.rand`: To generate an array of random numbers:'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`np.random.rand`：生成一个随机数数组：'
- en: '![](img/e5956406-688a-4c1b-986c-d73d3ef30bff.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e5956406-688a-4c1b-986c-d73d3ef30bff.png)'
- en: '`np.eye`: To generate an identity matrix of given square matrix dimensions:'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`np.eye`：生成给定方阵维度的单位矩阵：'
- en: '![](img/3468a284-89f9-41d5-ba9e-8cadbfd6ca35.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3468a284-89f9-41d5-ba9e-8cadbfd6ca35.png)'
- en: Let's now look at basic NumPy array operations.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下基本的 NumPy 数组操作。
- en: Basic NumPy array operations
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本的 NumPy 数组操作
- en: NumPy arrays are Python analogues of mathematical matrices, and so they support
    the arithmetic manipulation of all basic types, such as addition, subtraction,
    division, and multiplication.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 数组是 Python 对数学矩阵的类比，因此它们支持所有基本类型的算术操作，如加法、减法、除法和乘法。
- en: 'Let''s declare two NumPy arrays and store them as `array1` and `array2`:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们声明两个 NumPy 数组并将其存储为 `array1` 和 `array2`：
- en: '[PRE20]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now let''s look at some examples of each arithmetic operation on these arrays:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一些关于这些数组的基本算术操作的例子：
- en: '**Addition**:'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加法**：'
- en: '![](img/170428ce-55de-4a81-bf59-4ea0b5e9af4a.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](img/170428ce-55de-4a81-bf59-4ea0b5e9af4a.png)'
- en: '**Subtraction**:'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减法**：'
- en: '![](img/ad215dbc-939b-4eff-a0be-e217c9cc2bf5.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad215dbc-939b-4eff-a0be-e217c9cc2bf5.png)'
- en: '**Multiplication**:'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**乘法**：'
- en: '![](img/780c5000-1e48-466d-8e60-de7561b8d815.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![](img/780c5000-1e48-466d-8e60-de7561b8d815.png)'
- en: '**Division**:'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**除法**：'
- en: '![](img/d1097939-d2ab-4455-a459-056d54e7e468.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1097939-d2ab-4455-a459-056d54e7e468.png)'
- en: Let's now compare NumPy arrays with Python lists.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将 NumPy 数组与 Python 列表进行比较。
- en: NumPy arrays versus Python lists
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NumPy 数组与 Python 列表的比较
- en: Let's now see how NumPy arrays offer advantages over Python lists.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看 NumPy 数组相较于 Python 列表的优势。
- en: Array slicing over multiple rows and columns
  id: totrans-376
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对多个行和列进行数组切片
- en: 'While it is not possible to slice lists of lists in Python in such a way as
    to select a specific number of rows and columns in the list of lists, NumPy array
    slicing works according to the following syntax:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在 Python 中不能以切片的方式对列表中的列表进行操作，从而选择特定的行和列，但 NumPy 数组的切片操作遵循以下语法：
- en: '`Array [ rowStartIndex : rowEndIndex, columnStartIndex : columnEndIndex ]`'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '`Array [ rowStartIndex : rowEndIndex, columnStartIndex : columnEndIndex ]`'
- en: 'Here''s an example:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子：
- en: '![](img/3f6ed47b-cf3a-42f9-9413-9e6a146e5577.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3f6ed47b-cf3a-42f9-9413-9e6a146e5577.png)'
- en: In the preceding example, we are able to select two rows and all elements of
    those rows in NumPy array `a`.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们能够在 NumPy 数组 `a` 中选择两行及该行的所有元素。
- en: Assignment over slicing
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 切片赋值
- en: 'While it is not possible to assign values to slices of Python lists, NumPy
    allows the assignment of values to NumPy arrays. For example, to assign 4 to the
    third to the fifth element of a NumPy one-dimensional array, we can use the following:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然无法给 Python 列表的切片分配值，但 NumPy 允许给 NumPy 数组分配值。例如，要将 4 分配给 NumPy 一维数组的第三到第五个元素，我们可以使用以下方法：
- en: '[PRE21]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Next, we will be looking at pandas.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习 pandas。
- en: Pandas
  id: totrans-386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas
- en: Built on top of NumPy, pandas is one of the most widely used libraries for data
    science using Python. It facilitates high-performance data structures and data-analysis
    methods. Pandas provides an in-memory two-dimensional table object called a DataFrame,
    which in turn is made of a one-dimensional, array-like structure called a series.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 构建于 NumPy 之上，是 Python 中用于数据科学的最广泛使用的库之一。它提供了高性能的数据结构和数据分析方法。pandas 提供了一个内存中的二维表格对象，称为
    DataFrame，DataFrame 又由一个一维的类似数组的结构（称为 series）构成。
- en: Each DataFrame in pandas is in the form of a spreadsheet-like table with row
    labels and column headers. It is possible to carry out row-based or column-based
    operations, or both together. Pandas strongly integrates with matplotlib to provide
    several intuitive visualizations of data that are often very useful when making
    presentations or during exploratory data analysis.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 pandas DataFrame 都是一个类似电子表格的表格，具有行标签和列头。可以执行基于行或列的操作，或者同时进行两者。pandas 与 matplotlib
    强集成，提供了多种直观的数据可视化功能，这些功能在制作演示文稿或进行探索性数据分析时非常有用。
- en: 'To import pandas into a Python project, use the following line of code:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 pandas 导入 Python 项目，使用以下代码：
- en: '[PRE22]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, `pd` is a common name for importing pandas.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`pd` 是导入 pandas 时常用的别名。
- en: 'Pandas provides the following data structures:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 提供了以下数据结构：
- en: '**Series**: One-dimensional array or vector, similar to a column in a table'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Series**：一维数组或向量，类似于表格中的一列'
- en: '**DataFrames**: Two-dimensional table, with table headers and labels for the
    rows'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据框（DataFrames）**：二维表格，具有表头和行标签'
- en: '**Panels**: A dictionary of DataFrames, much like a MySQL database that contains
    several tables inside'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面板（Panels）**：一个 DataFrame 的字典，就像一个包含多个表的 MySQL 数据库'
- en: 'A pandas series can be created using the `pd.Series( )` method, while a DataFrame
    can be created using the `pd.DataFrame( )` method—for example, in the following
    code, we create a pandas DataFrame object using multiple series objects:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `pd.Series( )` 方法创建 pandas series，而使用 `pd.DataFrame( )` 方法创建 DataFrame——例如，在以下代码中，我们使用多个
    series 对象创建一个 pandas DataFrame 对象：
- en: '[PRE23]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出结果如下：
- en: '![](img/0eac47bd-7093-4c7e-a082-545d3e511900.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0eac47bd-7093-4c7e-a082-545d3e511900.png)'
- en: 'Some of the most important methods available for a pandas DataFrame are as
    follows:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: pandas DataFrame 中一些最重要的方法如下：
- en: '`head(n)` or `tail(n)`: To display the top or bottom *n* rows of the DataFrame.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head(n)` 或 `tail(n)`：显示 DataFrame 的前 *n* 行或后 *n* 行。'
- en: '`info( )`: To display information on all the columns, dimensions, and types
    of data in the columns of the DataFrame.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info( )`：显示 DataFrame 中所有列的信息、维度和列的数据类型。'
- en: '`describe( )`: To display handy aggregate and statistical information about
    each of the columns in the DataFrame. Columns that are not numeric are omitted.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`describe( )`：显示 DataFrame 中每一列的便捷聚合和统计信息。非数值列会被省略。'
- en: Summary
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We covered a lot of different things in this chapter. We started by learning
    the basics of a neural network and then we gradually proceeded. We learned the
    two most powerful types of neural networks used today—CNNs and RNNs—and we also
    learned about them on a high level, but without skipping their foundational units.
    We learned that as the complexity in a neural network increases, it requires a
    lot of computational power, which standard computers may fail to cater for we
    saw how this problem can be overcome by configuring a deep learning development
    environment using two different providers—AWS and Crestle. We explored Jupyter
    Notebooks, a powerful tool for performing deep learning tasks. We learned about
    the usage of two very popular Python libraries—NumPy and pandas. Both of these
    libraries are extensively used when performing deep learning tasks.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了许多不同的内容。我们从学习神经网络的基础开始，然后逐步深入。我们学习了今天使用的两种最强大的神经网络类型——CNN和RNN，并且在高层次上了解了它们，但没有跳过它们的基础单元。我们了解到，随着神经网络复杂度的增加，它需要大量的计算能力，而标准计算机可能无法满足这一需求，我们还看到如何通过使用两个不同的服务提供商——AWS和Crestle，来配置深度学习开发环境，从而克服这一问题。我们探索了Jupyter
    Notebooks，这是一款用于执行深度学习任务的强大工具。我们学习了两个非常流行的Python库——NumPy和pandas的使用。这两个库在执行深度学习任务时被广泛使用。
- en: In the next chapter, we will be building applications and integrating deep learning
    to make them perform intelligently. But before we did this, it was important for
    us to know the basics that were covered in this chapter. We are now in a good
    position to move on to the next chapter.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将构建应用程序并集成深度学习，使其具有智能表现。但在此之前，了解本章中涵盖的基础知识对我们来说非常重要。现在我们已经掌握了必要的基础，准备好继续进入下一章。
