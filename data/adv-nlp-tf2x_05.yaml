- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Generating Text with RNNs and GPT-2
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RNN 和 GPT-2 生成文本
- en: When your mobile phone completes a word as you type a message or when Gmail
    suggests a short reply or completes a sentence as you reply to an email, a text
    generation model is working in the background. The Transformer architecture forms
    the basis of state-of-the-art text generation models. BERT, as explained in the previous
    chapter, uses only the encoder part of the Transformer architecture.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的手机在你输入消息时自动完成一个单词，或者 Gmail 在你回复邮件时建议简短回复或自动完成一句话时，背景中正在运行一个文本生成模型。Transformer
    架构构成了最先进的文本生成模型的基础。如前一章所述，BERT 仅使用 Transformer 架构的编码器部分。
- en: However, BERT, being bi-directional, is not suitable for the generation of text.
    A left-to-right (or right-to-left, depending on the language) language model built
    on the decoder part of the Transformer architecture is the foundation of text
    generation models today.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，BERT 是双向的，不适合用于文本生成。基于 Transformer 架构解码器部分的从左到右（或从右到左，取决于语言）语言模型是当今文本生成模型的基础。
- en: 'Text can be generated a character at a time or with words and sentences together.
    Both of these approaches are shown in this chapter. Specifically, we will cover
    the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 文本可以按字符逐个生成，也可以将单词和句子一起生成。这两种方法将在本章中展示。具体来说，我们将涵盖以下主题：
- en: 'Generating text with:'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下方法生成文本：
- en: Character-based RNNs for generating news headlines and completing text messages
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于字符的 RNN 生成新闻标题和完成文本消息
- en: GPT-2 to generate full sentences
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 GPT-2 生成完整的句子
- en: 'Improving the quality of text generation using techniques such as:'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下技术提高文本生成质量：
- en: Greedy search
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贪婪搜索
- en: Beam search
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beam 搜索
- en: Top-K sampling
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Top-K 采样
- en: 'Using advanced techniques such as learning rate annealing and checkpointing
    to enable long training times:'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用学习率退火和检查点等高级技术来支持较长的训练时间：
- en: Details of the Transformer decoder architecture
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer 解码器架构的详细信息
- en: Details of the GPT and GPT-2 models
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPT 和 GPT-2 模型的详细信息
- en: A character-based approach for generating text is shown first. Such models can
    be quite useful for generating completions of a partially typed word in a sentence
    on a messaging platform, for example.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先展示的是基于字符的文本生成方法。例如，这种模型在消息平台中生成部分输入单词的补全时非常有用。
- en: Generating text – one character at a time
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逐字符生成文本
- en: Text generation yields a window into whether deep learning models are learning
    about the underlying structure of language. Text will be generated using two different
    approaches in this chapter. The first approach is an RNN-based model that generates
    a character at a time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成提供了一个窗口，帮助我们了解深度学习模型是否在学习语言的潜在结构。本章中将使用两种不同的方法生成文本。第一种方法是基于 RNN 的模型，它一次生成一个字符。
- en: In the previous chapters, we have seen different tokenization methods based
    on words and sub-words. Text is tokenized into characters, which include capital
    and small letters, punctuation symbols, and digits. There are 96 tokens in total.
    This tokenization is an extreme example to test how much a model can learn about
    the language structure. The model will be trained to predict the next character
    based on a given set of input characters. If there is indeed an underlying structure
    in the language, the model should pick it up and generate reasonable-looking sentences.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们已经看到基于单词和子词的不同分词方法。文本被分解为字符，包括大写字母、小写字母、标点符号和数字。总共有 96 个标记。这个分词方法是一个极端的示例，用来测试模型能在多大程度上学习语言的结构。模型将被训练预测基于给定字符集的下一个字符。如果语言中确实存在某种潜在结构，模型应该能够识别并生成合理的句子。
- en: Generating coherent sentences one character at a time is a very challenging
    task. The model does not have a dictionary or vocabulary, and it has no sense
    of capitalization of nouns or any grammar rules. Yet, we are expecting it to generate
    reasonable-looking sentences. The structure of words and their order in a sentence
    is not random but driven by grammar rules in a language. Words have some structure,
    based on parts of speech and word roots. A character-based model has the smallest
    possible vocabulary, but we hope that the model learns a lot about the use of
    the letters. This may seem like a tall order but be prepared to be surprised.
    Let's get started with the data loading and pre-processing steps.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一次生成一个字符的连贯句子是一项非常具有挑战性的任务。该模型没有字典或词汇表，也不具备名词大写或任何语法规则的概念。然而，我们仍然期望它能生成看起来合理的句子。单词的结构及其在句子中的顺序并非随机，而是受到语言语法规则的驱动。单词具有某种结构，基于词性和词根。基于字符的模型拥有最小的词汇表，但我们希望模型能学到大量关于字母使用的知识。这可能看起来是个艰巨的任务，但请准备好被惊讶。让我们从数据加载和预处理步骤开始。
- en: Data loading and pre-processing
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据加载和预处理
- en: For this particular example, we are going to use data from a constrained domain
    – a set of news headlines. The hypothesis is that news headlines are usually short
    and follow a particular structure. These headlines are usually a summary of an
    article and contain a large number of proper nouns like names of companies and
    celebrities. For this particular task, data from two different datasets are joined
    together and used. The first dataset is called the News Aggregator dataset generated
    by the Artificial Intelligence Lab, part of the Faculty of Engineering at Roma
    Tre University in Italy. The University of California, Irvine, has made the dataset
    available for download from [https://archive.ics.uci.edu/ml/datasets/News+Aggregator](https://archive.ics.uci.edu/ml/datasets/News+Aggregator).
    This dataset has over 420,000 news article titles, URLs, and other information.
    The second dataset is a set of over 200,000 news articles from The Huffington
    Post, called the News Category dataset, collected by Rishabh Mishra and posted
    on Kaggle at [https://www.kaggle.com/rmisra/news-category-dataset](https://www.kaggle.com/rmisra/news-category-dataset).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的示例，我们将使用来自受限领域的数据——一组新闻标题。假设新闻标题通常较短，并遵循特定的结构。这些标题通常是文章的摘要，并包含大量专有名词，例如公司名称和名人姓名。对于这个特定任务，来自两个不同数据集的数据被合并在一起使用。第一个数据集叫做新闻聚合器数据集，由意大利罗马三大学工程学院人工智能实验室生成。加利福尼亚大学欧文分校提供了该数据集的下载链接：[https://archive.ics.uci.edu/ml/datasets/News+Aggregator](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)。该数据集包含超过420,000条新闻文章标题、URL和其他信息。第二个数据集是来自《赫芬顿邮报》的200,000多篇新闻文章，名为新闻类别数据集，由Rishabh
    Mishra收集，并在Kaggle上发布：[https://www.kaggle.com/rmisra/news-category-dataset](https://www.kaggle.com/rmisra/news-category-dataset)。
- en: News article headlines from both datasets are extracted and compiled into one
    file. This step is already done to save time. The compressed output file is called
    `news-headlines.tsv.zip` and is located in the `chapter5-nlg-with-transformer-gpt/char-rnn`
    GitHub folder corresponding to this chapter. The folder is located inside the
    GitHub repository for this book. The format of this file is pretty simple. It
    has two columns separated by a tab. The first column is the original headline,
    and the second column is an uncased version of the same headline. This example
    uses the first column of the file only.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 来自两个数据集的新闻文章标题已被提取并编译成一个文件。此步骤已完成，以节省时间。压缩后的输出文件名为`news-headlines.tsv.zip`，并位于与本章对应的`chapter5-nlg-with-transformer-gpt/char-rnn`
    GitHub文件夹中。该文件夹位于本书的GitHub仓库内。该文件的格式非常简单，包含两列，通过制表符分隔。第一列是原始标题，第二列是该标题的小写版本。本示例仅使用文件的第一列。
- en: However, you can try the uncased version to see how the results differ. Training
    such models usually takes a lot of time, often several hours. Training in an IPython
    notebook can be difficult as a number of issues, such as the loss of the connection
    to the kernel or the kernel process dying, can result in the loss of the trained
    model. What we are attempting to do in this example is akin to training BERT from
    scratch. Don't worry; we train the model for a much shorter time than it took
    to train BERT. Running long training loops runs the risk of training loops crashing
    in the middle. In such a case, we don't want to restart training from scratch.
    The model is checkpointed frequently during training so that the model state can
    be restored from the last checkpoint if a failure occurs. Then, training can be
    restarted from the last checkpoint. Python files executed from the command line
    give the most control when running long training loops.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你可以尝试无大小写版本，看看结果有何不同。训练这类模型通常需要很长时间，往往是几个小时。在 IPython 笔记本中训练可能很困难，因为会遇到很多问题，比如与内核失去连接或内核进程崩溃，可能导致已训练的模型丢失。在本例中，我们尝试做的事情类似于从零开始训练
    BERT。别担心；我们训练模型的时间要比训练 BERT 的时间短得多。长时间的训练循环存在崩溃的风险。如果发生这种情况，我们不想从头开始重新训练。训练过程中模型会频繁保存检查点，以便在发生故障时可以从最后一个检查点恢复模型状态。然后，可以从最后一个检查点重新开始训练。从命令行执行的
    Python 文件在运行长时间训练循环时提供了最大的控制权。
- en: The command-line instructions shown in this example were tested on an Ubuntu
    18.04 LTS machine. These commands should work as is on a macOS command line but
    may need some adjustments. Windows users may need to translate these commands
    for their operating system. Windows 10 power users should be able to use the **Windows
    Subsystem for Linux** (**WSL**) capabilities to execute the same commands.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例中展示的命令行指令已在 Ubuntu 18.04 LTS 机器上进行了测试。这些命令应在 macOS 命令行上直接工作，但可能需要进行一些调整。Windows
    用户可能需要将这些命令翻译为适合他们操作系统的版本。Windows 10 的高级用户应该能够使用 **Windows 子系统 Linux** (**WSL**)
    功能来执行相同的命令。
- en: 'Going back to the data format, all that needs to be done for loading the data
    is to unzip the prepared headline file. Navigate to the folder where the ZIP file
    has been pulled down from GitHub. The compressed file of headlines can be unzipped
    and inspected:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 回到数据格式，加载数据所需做的只是解压准备好的标题文件。导航到从 GitHub 下载的 ZIP 文件所在的文件夹。可以解压并检查该压缩文件中的标题：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s inspect the contents of the file to get a sense of the data:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下文件的内容，以便了解数据的概况：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The model is trained on the headlines shown above. We are ready to move on to
    the next step and load the file to perform normalization and tokenization.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是在上述标题的基础上进行训练的。我们准备好进入下一步，加载文件以执行归一化和标记化操作。
- en: Data normalization and tokenization
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据归一化和标记化
- en: 'As discussed above, this model uses a token per character. So, each letter,
    including punctuation, numbers, and space, becomes a token. Three additional tokens
    are added. These are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，该模型使用每个字符作为一个标记。因此，每个字母，包括标点符号、数字和空格，都变成一个标记。额外增加了三个标记，它们是：
- en: '`<EOS>`: Denotes end of sentences. The model can use this token to indicate
    that the generation of text is complete. All headlines end with this token.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<EOS>`：表示句子的结束。该模型可以使用此标记表示文本生成已完成。所有标题都会以此标记结尾。'
- en: '`<UNK>`: While this is a character-level model, it is possible to have different
    characters from other languages or character sets in the dataset. When a character
    is detected that is not present in our set of 96 characters, this token is used.
    This approach is consistent with word-based vocabulary approaches where it is
    common to replace out-of-vocabulary words with a special token.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<UNK>`：虽然这是一个基于字符的模型，但数据集中可能包含其他语言或字符集的不同字符。当检测到一个不在我们96个字符集中的字符时，会使用此标记。这种方法与基于词汇的词汇表方法一致，在这种方法中，通常会用一个特殊的标记替换词汇表之外的词汇。'
- en: '`<PAD>`: This is a unique padding token used to pad all headlines to the same
    length. Padding is done by hand in this example as opposed to using TensorFlow
    methods, which we have seen previously.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<PAD>`：这是一个独特的填充标记，用于将所有标题填充到相同的长度。在这个例子中，填充是手动进行的，而不是使用 TensorFlow 方法，这些方法我们之前已经见过。'
- en: All the code in this section will refer to the `rnn-train.py` file from the
    `chapter5-nlg-with-transformer-gpt` folder of the GitHub repo of the book. The
    first part of this file has the imports and optional instructions for setting
    up a GPU. Ignore this section if your setup does not use a GPU.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的所有代码将参考来自 GitHub 图书仓库 `chapter5-nlg-with-transformer-gpt` 文件夹中的 `rnn-train.py`
    文件。该文件的第一部分包含导入和设置 GPU 的可选指令。如果您的设置没有使用 GPU，请忽略此部分。
- en: A GPU is an excellent investment for deep learning engineers and researchers.
    A GPU could speed up your training times by orders of magnitude or more! It would
    be worthwhile to outfit your deep learning setup with a GPU like the Nvidia GeForce
    RTX 2070.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 对于深度学习工程师和研究人员来说是一个极好的投资。GPU 可以将训练时间提高几个数量级！因此，配置一台如 Nvidia GeForce RTX
    2070 的 GPU 深度学习设备是值得的。
- en: 'The code for data normalization and tokenization is between lines 32 and 90
    of this file. To start, the tokenization function needs to be set up:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据归一化和标记化的代码位于该文件的第 32 行到第 90 行之间。首先，需要设置标记化函数：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once the token list is ready, methods need to be defined for converting characters
    to tokens and vice versa. Creating mapping is relatively straightforward:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦令牌列表准备好，就需要定义方法将字符转换为令牌，反之亦然。创建映射相对简单：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, the data needs can be read in from the TSV file. A maximum length of 75
    characters is used for the headlines. If the headlines are shorter than this length,
    they are padded. Any headlines longer than 75 characters are snipped. The `<EOS>`
    token is appended to the end of every headline. Let''s set this up:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据需要从 TSV 文件中读取。对于标题，使用 75 个字符的最大长度。如果标题短于此长度，会进行填充。任何超过 75 个字符的标题都会被截断。`<EOS>`
    标记会被附加到每个标题的末尾。我们来设置这个：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'All the data is loaded into a list with the code above. You may be wondering
    about the ground truth here for training as we only have a line of text. Since
    we want this model to generate text, the objective can be reduced to predicting
    the next character given a set of characters. Hence, a trick will be used to construct
    the ground truth – we will just shift the input sequence by one character and
    set it as the expected output. This transformation is quite easy do with `numpy`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据都已通过上述代码加载到列表中。你可能会想，训练的地面真实值是什么，因为我们只有一行文本。由于我们希望这个模型能够生成文本，目标可以简化为根据一组字符预测下一个字符。因此，采用一种技巧来构造真实值——我们只需将输入序列向右移动一个字符，并将其设置为期望输出。这个转换通过
    `numpy` 很容易做到：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With this nifty trick, we have both inputs and expected outputs ready for training.
    The final step is to convert it into `tf.Data.DataSet` for ease of batching and
    shuffling:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个巧妙的技巧，我们准备好了输入和期望的输出用于训练。最后一步是将其转换为 `tf.Data.DataSet`，以便于批处理和洗牌：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now everything is ready to start training.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切准备就绪，可以开始训练了。
- en: Training the model
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'The code for model training starts at line 90 in the `rnn-train.py` file. The
    model is quite simple. It has an embedding layer, followed by a GRU layer and
    a dense layer. The size of the vocabulary, the number of RNN units, and the size
    of the embeddings are set up:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练的代码从 `rnn-train.py` 文件的第 90 行开始。该模型非常简单，包含一个嵌入层、一个 GRU 层和一个全连接层。词汇表的大小、RNN
    单元的数量以及嵌入的大小已经设置好：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With the batch size being defined, training data can be batched and ready for
    use by the model:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了批处理大小后，训练数据可以进行批处理，并准备好供模型使用：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Similar to code in previous chapters, a convenience method to build models
    is defined like so:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章中的代码类似，定义了一个方便的方法来构建模型，如下所示：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'A model can be instantiated with this method:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用此方法实例化模型：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'There are just over 4 million trainable parameters in this model. The Adam
    optimizer, with a sparse categorical loss function, is used for training this
    model:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有超过 400 万个可训练参数。训练该模型时使用了带稀疏分类损失函数的 Adam 优化器：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Since training is potentially going to take a long time, we need to set up
    checkpoints along with the training. If there is any problem in training and training
    stops, these checkpoints can be used to restart the training from the last saved
    checkpoint. A directory is created using the current timestamp for saving these
    checkpoints:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练可能会耗费很长时间，我们需要在训练过程中设置检查点。如果训练过程中出现问题且训练停止，这些检查点可以用来从最后保存的检查点重新开始训练。通过当前的时间戳创建一个目录，用于保存这些检查点：
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'A custom callback that saves checkpoints during training is defined in the
    last line of code above. This is passed to the `model.fit()` function to be called
    at the end of every epoch. Starting the training loop is straightforward:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 上面代码中的最后一行定义了一个在训练过程中保存检查点的自定义回调。这个回调会传递给 `model.fit()` 函数，以便在每个训练周期结束时调用。启动训练循环非常简单：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The model will be trained for 25 epochs. The time taken in training will be
    logged as well in the code above. The final piece of code uses the history to
    plot the loss and save it as a PNG file in the same directory:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将训练 25 个周期。训练所需的时间也会在上面的代码中记录。最后一段代码使用训练历史来绘制损失曲线，并将其保存为 PNG 文件，保存在同一目录下：
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The best way to start training is to start the Python process so that it can
    run in the background without needing a Terminal or command-line. On Unix systems,
    this can be done with the `nohup` command:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 开始训练的最佳方式是启动 Python 进程，使其能够在后台运行，而无需终端或命令行。在 Unix 系统上，可以使用 `nohup` 命令来实现：
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This command line starts the process in a way that disconnecting the Terminal
    would not interrupt the training process. On my machine, this training took approximately
    1 hour and 43 minutes. Let''s check out the loss curve:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令行启动进程，使得断开终端连接不会中断训练过程。在我的机器上，这次训练大约花费了 1 小时 43 分钟。让我们来看一下损失曲线：
- en: '![A close up of a mans face  Description automatically generated](img/B16252_05_01.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![一张近距离的男性面部照片  描述自动生成](img/B16252_05_01.png)'
- en: 'Figure 5.1: Loss curve'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：损失曲线
- en: As we can see, the loss decreases to a point and then shoots up. The standard
    expectation is that loss would monotonically decrease as the model was trained
    for more epochs. In the case shown above, the loss suddenly shoots up. In other
    cases, you may observe a NaN, or Not-A-Number, error. NaNs result from the exploding
    gradient problem during backpropagation through RNNs. The gradient direction causes
    weights to grow very large quickly and overflow, resulting in NaNs. Given how
    prevalent this is, there are quite a few jokes about NLP engineers and Indian
    food to go with the nans (referring to a type of Indian bread).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，损失值会降低到一个点后突然上升。标准的预期是，随着模型训练轮数的增加，损失值应单调下降。在上面展示的情况下，损失值突然上升。在其他情况下，可能会观察到
    NaN（不是一个数字）错误。NaN 错误是由 RNN 反向传播中的梯度爆炸问题引起的。梯度方向使得权重迅速变得非常大并导致溢出，最终产生 NaN 错误。由于这种情况非常普遍，关于
    NLP 工程师和印度食物的笑话也随之而来，"NaN" 这个词也巧妙地指代了一种印度面包。
- en: The primary reason behind these occurrences is gradient descent overshooting
    the minima and starting to climb the slope before reducing again. This happens
    when the steps gradient descent is taking are too large. Another way to prevent
    the NaN issue is gradient clipping where gradients are clipped to an absolute
    maximum, preventing loss from exploding. In the RNN model above, a scheme needs
    to be used that reduces the learning rate over time. Reducing the learning rate
    over epochs reduces the chances for gradient descent to overshoot the minima.
    This technique of reducing the learning rate over time is called **learning rate
    annealing** or **learning rate decay**. The next section walks through implementing
    learning rate decay while training the model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些现象背后的主要原因是梯度下降超越了最小值，并在再次降低之前开始爬升坡度。这发生在梯度下降的步伐过大时。另一种防止 NaN 问题的方法是梯度裁剪，其中梯度被裁剪到一个绝对最大值，从而防止损失爆炸。在上面的
    RNN 模型中，需要使用一种在训练过程中逐步减小学习率的方案。随着训练轮次的增加，减小学习率可以降低梯度下降超越最小值的可能性。这个逐步减小学习率的技巧被称为**学习率退火**或**学习率衰减**。下一部分将介绍如何在训练模型时实现学习率衰减。
- en: Implementing learning rate decay as custom callback
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现自定义学习率衰减回调
- en: 'There are two ways to implement learning rate decay in TensorFlow. The first
    way is to use one of the prebuilt schedulers that are part of the `tf.keras.optimizers.schedulers`
    package and use a configured instance with the optimizer. An example of a prebuilt
    scheduler is `InverseTimeDecay`, and it can be set up as shown below:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中有两种实现学习率衰减的方法。第一种方法是使用 `tf.keras.optimizers.schedulers` 包中预构建的调度器之一，并将配置好的实例与优化器一起使用。一个预构建的调度器实例是
    `InverseTimeDecay`，可以按照如下方式进行设置：
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The first parameter, 0.001 in the example above, is the initial learning rate.
    The number of steps per epoch can be calculated by dividing the number of training
    examples by batch size. The number of decay steps determines how the learning
    rate is reduced. The equation used to compute the learning rate is:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例中的第一个参数0.001是初始学习率。每个周期的步数可以通过将训练样本数量除以批量大小来计算。衰减步数决定了学习率的减少方式。用来计算学习率的公式是：
- en: '![](img/B16252_05_001.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_05_001.png)'
- en: 'After being set up, all this function needs is the step number for computing
    the new learning rate. Once the schedule is set up, it can be passed to the optimizer:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完成后，该函数所需的仅是用于计算新学习率的步数。一旦设置好学习计划，它就可以传递给优化器：
- en: '[PRE18]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: That's it! The rest of the training loop code is unchanged. However, this learning
    rate scheduler starts reducing the learning rate from the first epoch itself.
    A lower learning rate increases the amount of training time. Ideally, we would
    keep the learning rate unchanged for the first few epochs and then reduce it.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！其余的训练循环代码保持不变。然而，这个学习率调度器从第一个周期开始就会减少学习率。较低的学习率会增加训练时间。理想情况下，我们会在前几个周期保持学习率不变，然后再减少它。
- en: Looking at *Figure 5.1* above, the learning rate is probably effective until
    about the tenth epoch. BERT also uses **learning rate warmup** before learning
    rate decay. Learning rate warmup generally refers to increasing the learning rate
    for a few epochs. BERT was trained for 1,000,000 steps, which roughly translates
    to 40 epochs. For the first 10,000 steps, the learning rate was increased, and
    then it was linearly decayed. Implementing such a learning rate schedule is better
    accomplished by a custom callback.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 看看上面的*图5.1*，学习率可能在大约第十个周期之前是有效的。BERT还使用了**学习率预热**，然后才进行学习率衰减。学习率预热通常是指在几个周期内逐渐增加学习率。BERT训练了1000000步，约等于40个周期。在前10,000步中，学习率是逐渐增加的，然后线性衰减。实现这样的学习率计划更好通过自定义回调来完成。
- en: 'Custom callbacks in TensorFlow enable the execution of custom logic at various
    points during training and inference. We saw an example of a prebuilt callback
    that saves checkpoints during training. A custom callback provides hooks that
    enable desired logic that can be executed at various points during training. This
    main step is to define a subclass of `tf.keras.callbacks.Callback`. Then, one
    or more of the following functions can be implemented to hook onto the events
    exposed by TensorFlow:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中的自定义回调函数可以在训练和推理的不同阶段执行自定义逻辑。我们看到过一个预构建的回调函数，它在训练过程中保存检查点。自定义回调函数提供了钩子，使得可以在训练的不同阶段执行所需的逻辑。这个主要步骤是定义`tf.keras.callbacks.Callback`的子类。然后，可以实现以下一个或多个函数来挂钩TensorFlow暴露的事件：
- en: '`on_[train,test,predict]_begin` / `on_[train,test,predict]_end`: This callback
    happens at the start of training or the end of the training. There are methods
    for training, testing, and prediction loops. Names for these methods can be constructed
    using the appropriate stage name from the possibilities shown in brackets. The
    method naming convention is a common pattern across other methods in the rest
    of the list.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_[train,test,predict]_begin` / `on_[train,test,predict]_end`：这个回调函数发生在训练开始时或训练结束时。这里有用于训练、测试和预测循环的方法。这些方法的名称可以使用方括号中显示的适当阶段名称来构造。方法命名约定是整个列表中其他方法的常见模式。'
- en: '`on_[train,test,predict]_batch_begin` / `on_[train,test,predict] _batch_end`:
    These callbacks happen when training for a specific batch starts or ends.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_[train,test,predict]_batch_begin` / `on_[train,test,predict]_batch_end`：这些回调函数在训练特定批次开始或结束时触发。'
- en: '`on_epoch_begin` / `on_epoch_end`: This is a training-specific function called
    at the start or end of an epoch.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_epoch_begin` / `on_epoch_end`：这是一个特定于训练的函数，在每个周期开始或结束时调用。'
- en: 'We will implement a callback for the start of the epoch that adjusts that epoch''s
    learning rate. Our implementation will keep the learning rate constant for a configurable
    number of initial epochs and then reduce the learning rate in a fashion similar
    to the inverse time decay function described above. This learning rate would look
    like the following *Figure 5.2*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个在每个周期开始时调整该周期学习率的回调函数。我们的实现会在可配置的初始周期数内保持学习率不变，然后以类似上述逆时间衰减函数的方式减少学习率。这个学习率图看起来像下面的*图5.2*：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_05_02.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![一张手机的截图，自动生成的描述](img/B16252_05_02.png)'
- en: 'Figure 5.2: Custom learning rate decay function'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：自定义学习率衰减函数
- en: 'First, a subclass is created with the function defined in it. The best place
    to put this in `rnn_train.py` is just around the checkpoint callback, before the
    start of training. This class definition is shown below:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个包含定义函数的子类。将其放置在`rnn_train.py`中最好的位置是在检查点回调附近，训练开始之前。该类定义如下所示：
- en: '[PRE19]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Using this callback in the training loop requires the instantiation of the
    callback. The following parameters are set while instantiating the callback:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环中使用此回调函数需要实例化该回调函数。实例化回调时会设置以下参数：
- en: The initial learning rate is set to 0.001.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始学习率设置为0.001。
- en: The decay rate is set to 4\. Please feel free to play around with different
    settings.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 衰减率设置为4。请随意尝试不同的设置。
- en: The number of steps is set to the number of epochs. The model is trained for
    150 epochs.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步数设置为纪元数。模型被训练了150个纪元。
- en: Learning rate decay should start after epoch 10, so the start epoch is set to
    10.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率衰减应从第10个纪元后开始，因此开始的纪元设置为10。
- en: 'The training loop is updated to include the callback like so:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环已更新，包含回调函数，如下所示：
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Changes are highlighted above. Now, the model is ready to be trained using
    the command shown above. Training 150 epochs took over 10 hours on the GPU-capable
    machine. The loss surface is shown in *Figure 5.3*:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以上变化已高亮显示。现在，模型已经准备好使用上述命令进行训练。训练150个纪元花费了超过10小时的GPU时间。损失曲面见*图5.3*：
- en: '![A close up of a piece of paper  Description automatically generated](img/B16252_05_03.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![A close up of a piece of paper  Description automatically generated](img/B16252_05_03.png)'
- en: 'Figure 5.3: Model loss after learning rate decay'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：学习率衰减后的模型损失
- en: 'In the figure above, the loss drops very fast for the first few epochs before
    plateauing near epoch 10\. Learning rate decay kicks in at that point, and the
    loss starts to fall again. This can be verified from a snippet of the log file:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，损失在前几个纪元中下降得非常快，然后在第10个纪元附近趋于平稳。此时，学习率衰减开始起作用，损失再次开始下降。这可以从日志文件中的一段代码中验证：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note the highlighted loss above. The loss slightly increased around epoch 10
    as learning rate decay kicked in, and the loss started falling again. The small
    bumps in the loss that can be seen in *Figure 5.3* correlate with places where
    the learning rate was higher than needed, and learning rate decay kicked it down
    to make the loss go lower. The learning rate started at 0.001 and ended at a fifth
    of that at 0.0002.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意上图中突出显示的损失。在第10个纪元左右，损失略微增加，原因是学习率衰减开始起作用，然后损失再次开始下降。*图5.3*中可见的损失小波动与学习率高于需求的地方相关，学习率衰减将其降低，促使损失下降。学习率从0.001开始，最终降至0.0002，即其五分之一。
- en: Training this model took much time and advanced tricks like learning rate decay
    to train. But how does this model do in terms of generating text? That is the
    focus of the next section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个模型花费了大量时间和先进的技巧，比如学习率衰减。但这个模型在生成文本方面表现如何呢？这是下一部分的重点。
- en: Generating text with greedy search
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用贪心搜索生成文本
- en: Checkpoints were taken during the training process at the end of every epoch.
    These checkpoints are used to load a trained model for generating text. This part
    of the code is implemented in an IPython notebook. The code for this section is
    found in the `charRNN-text-generation.ipynb` file in this chapter's folder in
    GitHub. The generation of text is dependent on the same normalization and tokenization
    logic used during training. The *Setup Tokenization* section of the notebook has
    this code replicated.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，每个纪元结束时都会进行检查点的保存。这些检查点用于加载已训练的模型以生成文本。这部分代码实现于一个IPython笔记本中。该部分代码位于本章GitHub文件夹中的`charRNN-text-generation.ipynb`文件中。文本生成依赖于训练过程中使用的相同归一化和标记化逻辑。笔记本中的*设置标记化*部分包含了这段代码的复本。
- en: There are two main steps in generating text. The first step is restoring a trained
    model from the checkpoint. The second step is generating a character at a time
    from a trained model until a specific end condition is met.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 生成文本有两个主要步骤。第一步是从检查点恢复训练好的模型。第二步是从训练好的模型中逐个生成字符，直到满足特定的结束条件。
- en: 'The *Load the Model* section of the notebook has the code to define the model.
    Since the checkpoints only stored the weights for the layers, defining the model
    structure is important. The main difference from the training network is the batch
    size. We want to generate a sentence at a time, so we set the batch size as 1:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本的*加载模型*部分包含定义模型的代码。由于检查点仅存储了层的权重，因此定义模型结构至关重要。与训练网络的主要区别在于批量大小。我们希望一次生成一句话，因此将批量大小设置为1：
- en: '[PRE22]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A convenience function for setting up the model structure is defined like so:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型结构的便利函数如下所示：
- en: '[PRE23]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Note that the embedding layer does not use masking because, in text generation,
    we are not passing an entire sequence but only part of a sequence that needs to
    be completed. Now that the model is defined, the weights for the layers can be
    loaded in from the checkpoint. Please remember to replace the checkpoint directory
    with your local directory containing the checkpoints from training:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，嵌入层不使用掩码，因为在文本生成中，我们不是传递整个序列，而只是需要完成的序列的一部分。现在模型已经定义好，可以从检查点中加载层的权重。请记住将检查点目录替换为包含训练检查点的本地目录：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The second main step is to generate text a character at a time. Generating
    text needs a seed or a starting few letters, which are completed by the model
    into a sentence. The process of generation is encapsulated in the function below:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个主要步骤是逐个字符生成文本。生成文本需要一个种子或几个起始字母，这些字母由模型完成成一个句子。生成过程封装在下面的函数中：
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The generation method takes in a seed string that is used as the starting point
    for the generation. This seed string is vectorized. The actual generation happens
    in a loop, where one character is generated at a time and appended to the sequence
    generated. At every point, the character with the highest likelihood is chosen.
    Choosing the next letter with the highest probability is called **greedy search**.
    However, there is a configuration parameter called **temperature**, which can
    be used to adjust the predictability of the generated text.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 生成方法接收一个种子字符串作为生成的起始点。这个种子字符串被向量化。实际的生成过程在一个循环中进行，每次生成一个字符并附加到生成的序列中。在每一步中，选择具有最高概率的字符。选择具有最高概率的下一个字母被称为**贪婪搜索**。然而，有一个配置参数称为**温度**，可以用来调整生成文本的可预测性。
- en: Once probabilities for all characters are predicted, dividing the probabilities
    by the temperature changes the distribution of the generated characters. Smaller
    values of the temperature generate text that is closer to the original text. Larger
    values of the temperature generate more creative text. Here, a value of 0.7 is
    chosen to bias more on the surprising side.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦预测出所有字符的概率，将概率除以温度会改变生成字符的分布。温度较小的值生成更接近原始文本的文本。温度较大的值生成更有创意的文本。在这里，选择了一个值为0.7，更倾向于产生一些令人惊讶的内容。
- en: 'To generate the text, all that is needed is one line of code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 生成文本所需的全部代码只需一行：
- en: '[PRE26]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Each execution of the command may generate slightly different results. The line
    generated above, while obviously nonsensical, is pretty well structured. The model
    has learned capitalization rules and headline structure. Normally, we would not
    generate text beyond the `<EOS>` token, but all 75 characters are generated here
    for the sake of understanding the model output.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 每次执行命令可能会生成略有不同的结果。上面生成的行，虽然显然毫无意义，但结构相当良好。模型已经学习了大写规则和标题结构。通常情况下，我们不会生成超过`<EOS>`标记的文本，但在这里生成了所有75个字符，以便更好地理解模型输出。
- en: Note that the output shown for text generation is indicative. You may see a
    different output for the same prompt. There is some inherent randomness that is
    built into this process, which we can try and control by setting random seeds.
    When a model is retrained, it may end up on a slightly different point on the
    loss surface, where even though the loss numbers look similar, there may be slight
    differences in the model weights. Please take the outputs presented in the entire
    chapter as indicative versus actual.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，文本生成显示的输出是指示性的。对于相同的提示，您可能会看到不同的输出。这个过程内在地包含一些随机性，我们可以通过设置随机种子来尝试控制它。当重新训练模型时，它可能会停留在损失表面上的略有不同的点，即使损失数字看起来相似，模型权重也可能略有不同。请将整个章节中呈现的输出视为指示性的，而不是实际的。
- en: 'Here are some other examples of seed strings and model outputs, snipped after
    the end-of-sentence tag:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一些种子字符串和模型输出的其他示例，这些示例在句子结束标记后被剪辑：
- en: '| Seed | Generated Sentence |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 种子 | 生成的句子 |'
- en: '| S&P | S&P 500 closes above 190<EOS>S&P: Russell Slive to again find any business
    manufacture<EOS>S&P closes above 2000 for first tim<EOS> |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 标普 | 标普500首次突破190<EOS>标普：Russell Slive再次找到任何商业制造商<EOS>标普突破2000点首次突破<EOS>
    |'
- en: '| Beyonce | Beyonce and Solange pose together for ''American Idol'' contes<EOS>Beyonce''s
    sister Solange rules'' Dawn of the Planet of the Apes'' report<EOS>Beyonce & Jay
    Z Get Married<EOS> |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Beyonce | Beyoncé和Solange一起为《美国偶像》比赛拍照<EOS>Beyoncé的妹妹Solange主宰了《猩球崛起》的报告<EOS>Beyoncé和Jay
    Z结婚<EOS> |'
- en: 'Note the model''s use of quotes in the first two sentences for *Beyonce* as
    the seed word. The following table shows the impact of different temperature settings
    for similar seed words:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型在前两句中使用了**Beyonce**作为种子词时的引号。下表展示了不同温度设置对类似种子词的影响：
- en: '| Seed | Temperature | Generated Sentence |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 种子 | 温度 | 生成的句子 |'
- en: '| S&P | 0.10.30.50.9 | S&P 500 Closes Above 1900 For First Tim<EOS>S&P Close
    to $5.7 Billion Deal to Buy Beats Electronic<EOS>S&P 500 index slips to 7.2%,
    signaling a strong retail sale<EOS>S&P, Ack Factors at Risk of what you see This
    Ma<EOS> |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 标普 | 0.10.30.50.9 | 标普500首次突破1900点<EOS>标普接近57亿美元收购Beats Electronics的交易<EOS>标普500指数下跌7.2%，预示着零售销售强劲<EOS>标普，Ack因素面临风险，你在这个市场看到了什么<EOS>
    |'
- en: '| Kim | 0.10.30.50.9 | Kim Kardashian and Kanye West wedding photos release<EOS>Kim
    Kardashian Shares Her Best And Worst Of His First Look At The Met Gala<EOS>Kim
    Kardashian Wedding Dress Dress In The Works From Fia<EOS>Kim Kardashian''s en<EOS>
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Kim | 0.10.30.50.9 | Kim Kardashian和Kanye West的婚礼照片发布<EOS>Kim Kardashian分享她对Met
    Gala首次亮相的最佳和最差看法<EOS>Kim Kardashian婚纱在Fia工作室制作中<EOS>Kim Kardashian的私人生活<EOS> |'
- en: Generally, the quality of the text goes down at higher values of temperature.
    All these examples were generated by passing in the different temperature values
    to the generation function.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，随着温度值的升高，文本的质量会下降。所有这些例子都是通过向生成函数传递不同的温度值生成的。
- en: A practical application of such a character-based model is to complete words
    in a text messaging or email app. By default, the `generate_text()` method is
    generating 75 characters to complete the headline. It is easy to pass in much
    shorter lengths to see what the model proposes as the next few letters or words.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于字符的模型的一个实际应用是完成文本消息或电子邮件应用中的单词。默认情况下，`generate_text()`方法会生成75个字符来完成标题。可以很容易地传入更短的长度，看看模型提出的下几个字母或单词是什么。
- en: 'The table below shows some experiments of trying to complete the next 10 characters
    of text fragments. These completions were generated using:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了一些实验，尝试完成文本片段的下10个字符。这些完成是通过以下方式生成的：
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '| Prompt | Completion |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 完成 |'
- en: '| I need some money from ba | I need some money from bank chairma |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 我需要一些来自银行的钱 | 我需要一些来自银行主席的钱 |'
- en: '| Swimming in the p | Swimming in the profitabili |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 在盈利池中游泳 | 在盈利能力中游泳 |'
- en: '| Can you give me a | Can you give me a Letter to |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 你能给我一封 | 你能给我一封信吗 |'
- en: '| are you fr | are you from around |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 你是从哪儿的 | 你是从附近来的 |'
- en: '| The meeting is | The meeting is back in ex |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 会议是 | 会议恢复了 |'
- en: '| Lets have coffee at S | Lets have coffee at Samsung heaLets have coffee at
    Staples storLets have coffee at San Diego Z |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 我们在S喝咖啡吧 | 我们在三星总部喝咖啡吧 | 我们在Staples商店喝咖啡吧 | 我们在圣地亚哥Z喝咖啡吧 |'
- en: Given that the dataset used was only from news headlines, it is biased toward
    certain types of activities. For example, the second sentence could be completed
    with *pool* instead of the model trying to fill it in with profitability. If a
    more general text dataset was used, then this model could do quite well at generating
    completions for partially typed words at the end of the sentence. However, there
    is one limitation that this text generation method has – the use of the greedy
    search algorithm.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于使用的数据集仅来自新闻标题，它对某些类型的活动存在偏见。例如，第二句话本来可以用*泳池*来完成，而不是模型尝试用盈利能力来填充。如果使用更为通用的文本数据集，那么该模型在生成部分输入词语的完成时可能表现得很好。然而，这种文本生成方法有一个限制——使用了贪心搜索算法。
- en: 'The greedy search process is a crucial part of the text generation above. It
    is one of several ways to generate text. Let''s take an example to understand
    this process. For this example, bigram frequencies were analyzed by Peter Norvig
    and published on [http://norvig.com/mayzner.html](http://norvig.com/mayzner.html).
    Over 743 billion English words were analyzed in this work. With 26 characters
    in an uncased model, there are theoretically 26 x 26 = 676 bigram combinations.
    However, the article reports that the following bigrams were never seen in roughly
    2.8 trillion bigram instances: JQ, QG, QK, QY, QZ, WQ, and WZ.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪搜索过程是上述文本生成中的关键部分。它是生成文本的几种方式之一。我们通过一个例子来理解这个过程。在这个例子中，Peter Norvig 分析了二元组频率，并发布在[http://norvig.com/mayzner.html](http://norvig.com/mayzner.html)上。在这项工作中分析了超过7430亿个英文单词。在一个没有大小写区分的模型中，理论上有26
    x 26 = 676个二元组组合。然而，文章报告称，在大约2.8万亿个二元组实例中，从未见过以下二元组：JQ、QG、QK、QY、QZ、WQ 和 WZ。
- en: The *Greedy Search with Bigrams* section of the notebook has code to download
    and process the full dataset and show the process of greedy search. After downloading
    the set of all n-grams, bigrams are extracted. A set of dictionaries is constructed
    to help look up the highest-probability next letter given a starting letter. Then,
    using some recursive code, a tree is constructed, picking the top three choices
    for the next letter. In the generation code above, only the top letter is chosen.
    However, the top three letters are chosen to show how greedy search works and
    its shortcomings.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本中的*贪婪搜索与二元组*部分包含了下载和处理完整数据集的代码，并展示了贪婪搜索的过程。下载所有n-gram集合后，提取了二元组。构建了一组字典来帮助查找给定起始字母后的最高概率字母。然后，使用一些递归代码构建了一个树，选择下一个字母的前三个选择。在上述生成代码中，仅选择了最顶部的字母。然而，选择了前三个字母来展示贪婪搜索的工作原理及其缺点。
- en: 'Using the nifty `anytree` Python package, a nicely formatted tree can be visualized.
    This tree is shown in the following figure:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用巧妙的`anytree` Python包，可以可视化一个格式化良好的树。这棵树在以下图中展示：
- en: '![A close up of text on a white background  Description automatically generated](img/B16252_05_04.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![一段白色背景上的文本特写，描述自动生成](img/B16252_05_04.png)'
- en: 'Figure 5.4: Greedy search tree starting with WI'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：从WI开始的贪婪搜索树
- en: The algorithm was given the task of completing **WI** in a total of five characters.
    The preceding tree shows cumulative probabilities for a given path. More than
    one path is shown so that the branches not taken by greedy search can also be
    seen. If a three-character word was being built, the highest probability choice
    is **WIN** with a probability of 0.243, followed by **WIS** at 0.01128\. If four-letter
    words are considered, then the greedy search would consider only those words that
    start with **WIN** as that was the path with the highest probability considering
    the first three letters. **WIND** has the highest probability of 0.000329 in this
    path. However, a quick scan across all four-letter words shows that the highest
    probability word should be **WITH** having a probability of 0.000399.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的任务是用总共五个字符完成**WI**。前面的树显示了给定路径的累计概率。显示了多条路径，这样可以看到贪婪搜索没有选择的分支。如果构建一个三字符单词，最高概率选择是**WIN**，概率为0.243，其次是**WIS**，概率为0.01128。如果考虑四个字母的单词，贪婪搜索将只考虑那些以**WIN**开头的单词，因为这是考虑前三个字母后，具有最高概率的路径。在这个路径中，**WIND**的概率为0.000329。然而，快速扫描所有四个字母的单词后发现，概率最高的单词应该是**WITH**，概率为0.000399。
- en: This, in essence, is the challenge of the greedy search algorithm for text generation.
    Higher-probability options considering joint probabilities are hidden due to optimization
    at each character instead of cumulative probability. Whether the text is generated
    a character or a word at a time, greedy search suffers from the same issue.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这就是贪婪搜索算法在文本生成中的挑战。由于每个字符的优化，而不是累计概率，考虑联合概率的高概率选项被隐藏了。无论是按字符还是按词生成文本，贪婪搜索都面临相同的问题。
- en: An alternative algorithm, called **beam search**, allows tracking multiple options,
    and pruning out the lower-probability options as generation proceeds. The tree
    shown in *Figure 5.4* can also be seen as an illustration of tracking beams of
    probabilities. To see the power of this technique, a more sophisticated model
    for generating text would be better. The **GPT-2**, or **Generative Pre-Training**,
    based model published by OpenAI set many benchmarks including in open-ended text
    generation. This is the subject of the next half of this chapter, where the GPT-2
    model is explained first. The next topic is fine-tuning a GPT-2 model for completing
    email messages. Beam search and other options to improve the quality of the generated
    text are also shown.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一种替代算法，称为**束搜索（beam search）**，可以跟踪多个选项，并在生成过程中剔除低概率的选项。如*图 5.4*所示的树形结构也可以看作是跟踪概率束的示意图。为了展示这一技术的威力，使用一个更复杂的生成文本模型会更好。由OpenAI发布的**GPT-2**（Generative
    Pre-Training）模型设立了多个基准，包括在开放式文本生成方面的突破。这是本章下半部分的主题，其中首先解释了GPT-2模型。接下来的话题是对GPT-2模型进行微调，以完成电子邮件消息的生成。束搜索和其他改善生成文本质量的选项也会在接下来的内容中展示。
- en: Generative Pre-Training (GPT-2) model
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成预训练（GPT-2）模型
- en: OpenAI released the first version of the GPT model in June 2018\. They followed
    up with GPT-2 in February 2019\. This paper attracted much attention as full details
    of the large GPT-2 model were not released with the paper due to concerns of nefarious
    uses. The large GPT-2 model was released subsequently in November 2019\. The GPT-3
    model is the most recent, released in May 2020\.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI于2018年6月发布了第一版GPT模型，随后在2019年2月发布了GPT-2。由于担心恶意用途，GPT-2的大规模模型没有与论文一起公开发布，因此引起了广泛关注。之后在2019年11月，OpenAI发布了GPT-2的大型版本。GPT-3模型是最新版本，于2020年5月发布。
- en: '*Figure 5.5* shows the number of parameters in the largest of each of these
    models:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.5*显示了这些模型中最大模型的参数数量：'
- en: '![](img/B16252_05_05.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_05_05.png)'
- en: 'Figure 5.5: Parameters in different GPT models'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：不同GPT模型的参数
- en: The first model used the standard Transformer decoder architecture with twelve
    layers, each with twelve attention heads and 768-dimensional embeddings, for a
    total of approximately 110 million parameters, which is very similar to the BERT
    model. The largest GPT-2 has over 1.5 billion parameters, and the most recently
    released GPT-3 model's largest variant has over 175 billion parameters!
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模型采用了标准的Transformer解码器架构，具有12层，每层12个注意力头和768维的嵌入，总共有约1.1亿个参数，与BERT模型非常相似。最大的GPT-2模型拥有超过15亿个参数，而最近发布的GPT-3模型的最大变体拥有超过1750亿个参数！
- en: '**Cost of training language models**'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**语言模型训练成本**'
- en: As the number of parameters and dataset sizes increase, the time taken for training
    also increases. As per a Lambda Labs article, If the GPT-3 model were to be trained
    on a single Nvidia V100 GPU, it would take 342 years. Using stock Microsoft Azure
    pricing, this would cost over $3 million. GPT-2 model training is estimated to
    run to $256 per hour. Assuming a similar running time as BERT, which is about
    four days, that would cost about $25,000\. If the cost of training multiple models
    during research is factored in, the overall cost can easily increase ten-fold.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 随着参数数量和数据集规模的增加，训练所需的时间也会增加。根据Lambda Labs的一篇文章，如果GPT-3模型仅在单个Nvidia V100 GPU上训练，训练时间将达到342年。使用微软Azure的标准定价，这将花费超过300万美元。GPT-2模型的训练预计每小时花费256美元。假设训练时间与BERT类似（约四天），这将花费约25,000美元。如果在研究过程中需要训练多个模型，整体成本可能会增加十倍。
- en: At such costs, training these models from scratch is out of reach for individuals
    and even most companies. Transfer learning and the availability of pre-trained
    models from companies like Hugging Face make it possible for the general public
    to use these models.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种成本，个人甚至大多数公司无法从头开始训练这些模型。迁移学习和像Hugging Face这样的公司提供的预训练模型使得公众能够使用这些模型。
- en: The base architecture of GPT models uses the decoder part of the Transformer
    architecture. The decoder is a *left-to-right* language model. The BERT model,
    in contrast, is a bidirectional model. A left-to-right model is autoregressive,
    that is, it uses tokens generated thus far to generate the next token. Since it
    cannot see future tokens like a bi-directional model, this language model is ideal
    for text generation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型的基础架构使用了Transformer架构的解码器部分。解码器是一个*从左到右*的语言模型。相比之下，BERT模型是一个双向模型。左到右的模型是自回归的，即它使用到目前为止生成的标记来生成下一个标记。由于它不能像双向模型一样看到未来的标记，这种语言模型非常适合文本生成。
- en: '*Figure 5.6* shows the full Transformer architecture with the encoder blocks
    on the left and decoder blocks on the right:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.6* 显示了完整的Transformer架构，左侧是编码器块，右侧是解码器块：'
- en: '![](img/B16252_05_06.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_05_06.png)'
- en: 'Figure 5.6: Full Transformer architecture with encoder and decoder blocks'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：完整的Transformer架构，包含编码器块和解码器块
- en: The left side of *Figure 5.6* should be familiar – it is essentially *Figure
    4.6* from the *Transformer model* section of the previous chapter. The encoder
    blocks shown are the same as the BERT model. The decoder blocks are very similar
    to the encoder blocks with a couple of notable differences.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.6* 的左侧应该很熟悉——它基本上是上一章*Transformer模型*部分中的*图 4.6*。所示的编码器块与BERT模型相同。解码器块与编码器块非常相似，有几个显著的不同之处。'
- en: In the encoder block, there is only one source of input – the input sequence
    and all of the input tokens are available for the multi-head attention to operate
    on. This enables the encoder to understand the context of the token from both
    the left and right sides.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器块中，只有一个输入源——输入序列，所有输入标记都可以用于多头注意力操作。这使得编码器能够从左右两侧理解标记的上下文。
- en: In the decoder block, there are two inputs to each block. The outputs generated
    by the encoder blocks are available to all the decoder blocks and fed to the middle
    of the decoder block through multi-head attention and layer norms.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码器块中，每个块有两个输入。编码器块生成的输出对所有解码器块可用，并通过多头注意力和层归一化传递到解码器块的中间。
- en: '**What is layer normalization?**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是层归一化？**'
- en: 'Large deep neural networks are trained using the **Stochastic Gradient Descent**
    (**SGD**) optimizer or a variant like Adam. Training large models on big datasets
    can take a significant amount of time for the model to converge. Techniques such
    as weight normalization, batch normalization, and layer normalization are aimed
    at reducing training time by helping models to converge faster while also acting
    as a regularizer. The idea behind layer normalization is to scale the inputs of
    a given hidden layer with the mean and standard deviation of the inputs. First,
    the mean and standard deviation are computed:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 大型深度神经网络使用**随机梯度下降**（**SGD**）优化器或类似的变体如Adam进行训练。在大数据集上训练大型模型可能需要相当长的时间才能使模型收敛。诸如权重归一化、批量归一化和层归一化等技术旨在通过帮助模型更快地收敛来减少训练时间，同时还起到正则化的作用。层归一化的基本思想是根据输入的均值和标准差对给定隐藏层的输入进行缩放。首先，计算均值和标准差：
- en: '![](img/B16252_05_002.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_05_002.png)'
- en: '*H* denotes the number of hidden units in layer *l*. Inputs to the layer are
    normalized using the above-calculated values:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*H* 表示层 *l* 中的隐藏单元数量。层的输入通过上述计算的值进行归一化：'
- en: '![](img/B16252_05_003.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_05_003.png)'
- en: where *g* is a gain parameter. Note that the formulation of the mean and standard
    deviation is not dependent on the size of the mini-batches or dataset size. Hence,
    this type of normalization can be used for RNNs and other sequence modeling problems.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *g* 是一个增益参数。请注意，均值和标准差的公式与小批量的大小或数据集的大小无关。因此，这种类型的归一化可以用于RNN和其他序列建模问题。
- en: However, the tokens generated by the decoder thus far are fed back through a
    masked multi-head self-attention and added to the output from the encoder blocks.
    Masked here refers to the fact that tokens to the right of the token being generated
    are masked, and the decoder cannot see them. Similar to the encoder, there are
    several such blocks stacked on top of each other. However, GPT architecture is
    only one half of the Transformer. This requires some modifications to the architecture.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，到目前为止由解码器生成的标记会通过掩码的多头自注意力回馈，并与来自编码器块的输出相加。这里的掩码指的是生成的标记右侧的标记被掩盖，解码器看不见它们。与编码器类似，这里有多个这样的块堆叠在一起。然而，GPT架构仅是Transformer的一半，这需要对架构进行一些修改。
- en: The modified architecture for GPT is shown in *Figure 5.7*. Since there is no
    encoder block to feed the representation of the input sequence, the multi-head
    layer is no longer required. The outputs generated by the model are recursively
    fed back to generate the next token.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 的修改架构如 *图 5.7* 所示。由于没有编码器块来传递输入序列的表示，因此不再需要多头层。模型生成的输出会递归地反馈，用以生成下一个标记。
- en: The smallest GPT-2 model has twelve layers and 768 dimensions for each token.
    The largest GPT-2 model has 48 layers and 1,600 dimensions per token. To pre-train
    models of this size, the authors of GPT-2 needed to create a new dataset. Web
    pages provide a great source of text, but the text comes with quality issues.
    To solve this challenge, they scraped all outbound links from Reddit, which had
    received at least three karma points. The assumption made by the authors is that
    karma points are an indicator of the quality of the web page being linked. This
    assumption allows scraping a huge set of text data. The resulting dataset was
    approximately 45 million links.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最小的 GPT-2 模型有 12 层，每个标记有 768 个维度。最大的 GPT-2 模型有 48 层，每个标记有 1,600 个维度。为了预训练这种规模的模型，GPT-2
    的作者需要创建一个新的数据集。网页是很好的文本来源，但文本存在质量问题。为了解决这个问题，他们从 Reddit 上抓取了所有至少获得三点 karma 的外部链接。作者的假设是
    karma 点数可以作为网页质量的一个指标。这一假设使得抓取大量文本数据成为可能。最终的数据集大约包含 4500 万个链接。
- en: 'To extract text from the HTML on the web pages, two Python libraries were used:
    Dragnet and Newspaper. After some quality checks and deduplication, the final
    dataset was about 8 million documents with 40 GB of text. One exciting thing that
    the authors did was to remove any Wikipedia documents as they felt many of the
    test datasets used Wikipedia, and adding these pages would cause an overlap between
    test and training data sets. The pre-training objective is a standard LM training
    objective of predicting the next word given a set of previous words:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从网页的 HTML 中提取文本，使用了两个 Python 库：Dragnet 和 Newspaper。经过一些质量检查和去重处理，最终的数据集约有
    800 万份文档，总共 40 GB 的文本数据。令人兴奋的是，作者还去除了所有维基百科文档，因为他们认为许多测试数据集都使用了维基百科，加入这些页面会导致测试和训练数据集的重叠。预训练的目标是一个标准的语言模型训练目标：根据一组前置词预测下一个词：
- en: '![A close up of a sign  Description automatically generated](img/B16252_05_07.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![标志的特写  描述自动生成](img/B16252_05_07.png)'
- en: 'Figure 5.7: GPT architecture (Source: Improving Language Understanding by Generative
    Pre-Training by Radford et al.)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7：GPT 架构（来源：Radford 等人的《通过生成预训练改进语言理解》）
- en: During pre-training, the GPT-2 model is trained with a maximum sequence length
    of 1,024 tokens. A **Byte Pair Encoding** (**BPE**) algorithm is used for tokenization,
    with a vocabulary size of about 50,000 tokens. GPT-2 uses byte sequences rather
    than Unicode code points for the byte pair merges. If GPT-2 only used bytes for
    encoding, then the vocabulary would only be 256 tokens. On the other hand, using
    Unicode code points would yield a vocabulary of over 130,000 tokens. By cleverly
    using bytes in BPE, GPT-2 is able to keep the vocabulary size to a manageable
    50,257 tokens.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练过程中，GPT-2 模型使用最大序列长度 1,024 个标记进行训练。采用 **字节对编码**（**BPE**）算法进行分词，词汇表大小约为 50,000
    个标记。GPT-2 使用字节序列而不是 Unicode 码点进行字节对合并。如果 GPT-2 仅使用字节进行编码，则词汇表的大小将只有 256 个标记。另一方面，使用
    Unicode 码点将导致词汇表超过 130,000 个标记。通过巧妙地使用 BPE 中的字节，GPT-2 能够将词汇表大小控制在一个可管理的 50,257
    个标记。
- en: Another peculiarity of the tokenizer in GPT-2 is that it converts all text to
    lowercase and uses spaCy and `ftfy` tokenizers prior to using BPE. The `ftfy`
    library is quite useful for fixing Unicode issues. If these two are not available,
    then the basic BERT tokenizer is used.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-2 中的分词器还有一个特点，它将所有文本转换为小写字母，并在使用 BPE 之前使用 spaCy 和 `ftfy` 分词器。`ftfy` 库对于修复
    Unicode 问题非常有用。如果这两个库不可用，则会使用基本的 BERT 分词器。
- en: 'There are several ways to encode the inputs to solve various problems, even
    though the left-to-right model may seem limiting. These are shown in *Figure 5.8*:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从左到右的模型可能看起来有限制，但有几种方法可以编码输入以解决不同的问题。这些方法显示在 *图 5.8* 中：
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_05_08.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图  描述自动生成](img/B16252_05_08.png)'
- en: 'Figure 5.8: Input transformations in GPT-2 for different problems (Source:
    Improving Language Understanding by Generative Pre-Training by Radford et al.)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：GPT-2 在不同问题中的输入转换（来源：Radford 等人的《通过生成预训练改进语言理解》）
- en: The figure above shows how a pre-trained GPT-2 model can be used for a variety
    of tasks other than text generation. In each instance, start and end tokens are
    added before and after the input sequence. In all cases, a linear layer is added
    to the end that is trained during model fine-tuning. The major advantage being
    claimed is that many different types of tasks can be accomplished using the same
    architecture. The topmost architecture in *Figure 5.8* shows how it can be used
    for classification. GPT-2 could be used for IMDb sentiment analysis using this
    approach, for example.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了如何使用预训练的GPT-2模型来处理文本生成以外的多种任务。在每个实例中，输入序列的前后分别添加了开始和结束标记。在所有情况下，最后都会添加一个线性层，并且在模型微调时进行训练。所宣称的主要优势是，许多不同类型的任务可以使用相同的架构来完成。*图5.8*中的最上层架构展示了它如何用于分类。例如，GPT-2可以使用这种方法进行IMDb情感分析。
- en: The second example is of textual entailment. Textual entailment is an NLP task
    where the relationship between two fragments of text needs to be established.
    The first text fragment is called a premise, and the second fragment is called
    the hypothesis. Different relationships can exist between the premise and hypothesis.
    The premise can validate or contradict the hypothesis, or they may be unrelated.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个示例是文本蕴含。文本蕴含是一个NLP任务，需要确定两个文本片段之间的关系。第一个文本片段称为前提，第二个片段称为假设。前提和假设之间可以存在不同的关系。前提可以验证或与假设相矛盾，或者它们可能没有任何关系。
- en: Let's say the premise is *Exercising every day is an important part of a healthy
    lifestyle and longevity*. If the hypothesis is *exercise increases lifespan*,
    then the premise *entails* or *validates* the hypothesis. Alternatively, if the
    hypothesis is *Running has no benefits*, then the premise *contradicts* the hypothesis.
    Lastly, if the hypothesis is that *lifting weights can build a six-pack*, then
    the premise neither entails nor contradicts the hypothesis. To perform entailment
    with GPT-2, the premise and hypothesis are concatenated with a delimiter, usually
    `$`, in between them.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 假设前提是*每天锻炼是健康生活方式和长寿的重要组成部分*。如果假设是*锻炼增加寿命*，那么前提*蕴含*或*验证*了假设。另一方面，如果假设是*跑步没有任何好处*，那么前提*与*假设相矛盾。最后，如果假设是*举重可以锻炼出六块腹肌*，那么前提既不蕴含也不与假设相矛盾。为了使用GPT-2进行蕴含推理，前提和假设被用分隔符通常是`$`连接起来。
- en: For text similarity, two input sequences are constructed, one with the first
    text sequence first and the second with the second text sequence first. The output
    from the GPT model is added together and fed to the linear layer. A similar approach
    is used for multiple-choice questions. However, our focus in this chapter is text
    generation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本相似性，构造两个输入序列，一个将第一个文本序列放在前面，另一个将第二个文本序列放在前面。GPT模型的输出结果相加并传入线性层。类似的方法也可以用于多项选择题。然而，本章的重点是文本生成。
- en: Generating text with GPT-2
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GPT-2生成文本
- en: 'Hugging Face''s transformers library simplifies the process of generating text
    with GPT-2\. Similar to the pre-trained BERT model, as shown in the previous chapter,
    Hugging Face provides pre-trained GPT and GPT-2 models. These pre-trained models
    are used in the rest of the chapter. Code for this and the rest of the sections
    of this chapter can be found in the IPython notebook named `text-generation-with-GPT-2.ipynb`.
    After running the setup, scoot over to the *Generating Text with GPT-2* section.
    A section showing the generation of text with GPT is also provided for reference.
    The first step in generating text is to download the pre-trained model, and its
    corresponding tokenizer:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face的transformers库简化了使用GPT-2生成文本的过程。类似于前一章所示的预训练BERT模型，Hugging Face提供了预训练的GPT和GPT-2模型。这些预训练模型将在本章的其余部分中使用。此代码和本章其余部分的代码可以在名为`text-generation-with-GPT-2.ipynb`的IPython笔记本中找到。运行设置后，转到*使用GPT-2生成文本*部分。还提供了一个展示如何使用GPT生成文本的部分作为参考。生成文本的第一步是下载预训练模型及其相应的tokenizer：
- en: '[PRE30]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This may take a few minutes as the models need to be downloaded. You may see
    a warning if spaCy and `ftfy` are not available in your environment. These two
    libraries are not mandatory for text generation. The following code can be used
    to generate text using a greedy search algorithm:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要几分钟时间，因为模型需要下载。如果在您的环境中没有找到spaCy和`ftfy`，您可能会看到警告。这两个库对于文本生成并不是强制性的。以下代码可以使用贪婪搜索算法来生成文本：
- en: '[PRE31]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: A prompt was supplied for the model to complete. The model started in a promising
    manner but soon resorted to repeating the same output.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了一个提示词供模型完成。模型开始时表现不错，但很快开始重复相同的输出。
- en: Note that the output shown for text generation is indicative. You may see different
    outputs for the same prompt. There are a few different reasons for this. There
    is some inherent randomness that is built into this process, which we can try
    and control by setting random seeds. The models themselves may be retrained periodically
    by the Hugging Face team and may evolve with newer versions.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，生成文本的输出只是一个示例。对于相同的提示，您可能会看到不同的输出。这有几个不同的原因。此过程本身具有一定的随机性，我们可以通过设置随机种子来尝试控制它。Hugging
    Face团队可能会定期重新训练模型，并且模型可能会随着新版本的发布而发生变化。
- en: Issues with the greedy search were noted in the previous section. Beam search
    can be considered as an alternative. At each step of generating a token, a set
    of top probability tokens are kept as part of the beam instead of just the highest-probability
    token. The sequence with the highest overall probability is returned at the end
    of the generation. *Figure 5.4*, in the previous section with a greedy search,
    can be considered as the output of a beam search algorithm with a beam size of
    3\.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节中提到了贪婪搜索的问题。束搜索可以作为一种替代方案。在生成每个标记的步骤中，会保留一组具有最高概率的标记作为束的一部分，而不仅仅是保留最高概率的标记。在生成结束时，会返回具有最高总体概率的序列。前一节中的*图5.4*（使用贪婪搜索）可以视为束搜索算法的输出，束的大小为3\。
- en: 'Generating text using beam search is trivial:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 使用束搜索生成文本是很简单的：
- en: '[PRE33]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Qualitatively, the first sentence makes a lot more sense than the one generated
    by the greedy search. The `early_stopping` parameter signals generation to stop
    when all beams reach the EOS token. However, there is still much repetition going
    on. One parameter that can be used to control the repetition is by setting a limit
    on n-grams being repeated:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 从质量上讲，第一句比贪婪搜索生成的句子更有意义。`early_stopping`参数会在所有束到达EOS标记时指示停止生成。然而，仍然存在很多重复的情况。控制重复的一个参数是通过设置限制，防止n-grams重复：
- en: '[PRE35]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This has made a considerable difference in the quality of the generated text.
    The `no_repeat_ngram_size` parameter prevents the model from generating any 3-grams
    or triplets of tokens more than once. While this improves the quality of the text,
    using the n-gram constraint can have a significant impact on the quality of the
    generated text. If the generated text is about *The White House*, then these three
    words can only be used once in the entire generated text. In such a case, using
    the n-gram constraint will be counter-productive.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这对生成文本的质量产生了相当大的影响。`no_repeat_ngram_size`参数可以防止模型生成任何3-gram或三元组的重复。虽然这提高了文本的质量，但使用n-gram约束可能会对生成文本的质量产生显著影响。如果生成的文本是关于*白宫*的，那么这三个词只能在整个生成文本中使用一次。在这种情况下，使用n-gram约束会适得其反。
- en: '**To beam or not to beam**'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**是否使用束搜索**'
- en: 'Beam search works well in cases where the generated sequence is of a restricted
    length. As the length of the sequence increases, the number of beams to be maintained
    and computed increases significantly. Consequently, beam search works well in
    tasks like summarization and translation but performs poorly in open-ended text
    generation. Further, beam search, by trying to maximize the cumulative probability,
    generates more predictable text. The text feels less natural. The following piece
    of code can be used to get a feel for the various beams being generated. Just
    make sure that the number of beams is greater than or equal to the number of sequences
    to be returned:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当生成的序列长度受到限制时，束搜索效果很好。随着序列长度的增加，需要维护和计算的束数量显著增加。因此，束搜索适用于像总结和翻译这样的任务，但在开放式文本生成中表现较差。此外，束搜索通过尝试最大化累积概率，生成了更多可预测的文本。这使得文本感觉不太自然。以下代码可以用来感受生成的不同束。确保束的数量大于或等于返回的序列数：
- en: '[PRE37]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'There is another method for improving the coherence and creativity of the text
    being generated called Top-K sampling. This is the preferred method in GPT-2 and
    plays an essential role in the success of GPT-2 in story generation. Before explaining
    how this works, let''s try it out and see the output:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一种方法可以提高生成文本的连贯性和创造性，这种方法叫做Top-K采样。这是GPT-2中首选的方法，在GPT-2生成故事的成功中起着至关重要的作用。在解释这个方法如何工作之前，我们先试试看，并看看生成的输出：
- en: '[PRE39]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The above sample was generated by selecting a high temperature value. A random
    seed was set to ensure repeatable results. The Top-K sampling method was published
    in a paper titled *Hierarchical Neural Story Generation* by Fan Lewis and Dauphin
    in 2018\. The algorithm is relatively simple – at every step, it picks a token
    from the top *K* highest probability tokens. If *K* is set to 1, then this algorithm
    is identical to the greedy search.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例是通过选择一个高温值生成的。设置了一个随机种子，以确保结果可重复。Top-K采样方法在2018年由Fan Lewis和Dauphin在论文《层级神经故事生成》中发布。这个算法相对简单——每一步，它从概率最大的*K*个tokens中挑选一个。如果*K*设置为1，那么该算法就与贪心搜索相同。
- en: 'In the code example above, the model looks at the 25 top tokens out of the
    50,000+ tokens while generating text. Then, it picks a random word from these
    and continues the generation. Choosing larger values will result in more surprising
    or creative text. Choosing lower values of *K* will result in more predictable
    text. If you are a little underwhelmed by the results thus far, that is because
    the prompt selected is a really tough one. Consider this output generated with
    Top-K of 50 for the prompt *In the dark of the night, there was a*:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码示例中，模型在生成文本时会查看50,000+个tokens中的前25个最顶端的tokens。然后，从这些tokens中随机选择一个，并继续生成文本。选择更大的值会产生更惊讶或更富有创意的文本。选择较低的*K*值则会生成更可预测的文本。如果到目前为止你觉得结果有些让人失望，那是因为所选择的提示确实很难。请考虑这是使用Top-K为50时，为提示*在黑夜的深处，突然出现了一*生成的输出：
- en: '*In the dark of the night, there was a sudden appearance of light.*'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '*在黑夜的深处，突然出现了一束光。*'
- en: '*Sighing, Xiao Chen slowly stood up and looked at Tian Cheng standing over.
    He took a step to look closely at Tian Cheng''s left wrist and frowned.*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*叹了口气，萧辰慢慢站起身，看着站在他面前的天承。他迈步走近，仔细观察天承的左腕，眉头紧皱。*'
- en: '*Lin Feng was startled, and quickly took out a long sword!*'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '*林峰吓了一跳，迅速抽出一把长剑！*'
- en: '*Lin Feng didn''t understand what sort of sword that Long Fei had wielded in
    the Black and Crystal Palace!*'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '*林峰不明白龙飞在黑晶宫里挥舞的是什么样的剑！*'
- en: '*The Black and Crystal Palace was completely different than his original Black
    Stone City. Long Fei carried a sword as a souvenir, which had been placed on the
    back of his father''s arm by Tian Cheng.*'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*黑晶宫与他原本的黑石城完全不同。龙飞带着一把剑做为纪念，这把剑是天承将其放置在他父亲的手臂上的。*'
- en: '*He drew the sword from his dad''s arm again!*'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '*他又从父亲的手臂上拔出了那把剑！*'
- en: '*The black blade was one of the most valuable weapons within the Black and
    Crystal Palace. The sword was just as sharp as the sharpest of all weapons, which
    had been placed on Long Fei''s father''s arm by the Black Stone City''s Black
    Ice, for him to*'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*这把黑色的剑刃是黑晶宫中最有价值的武器之一。这把剑锋利得像所有武器中最锋利的一把，它被黑石城的黑冰放置在龙飞父亲的手臂上，供他使用。*'
- en: The above longer form text was generated by the smallest GPT-2 model, which
    has roughly 124 million parameters. Several different settings and model sizes
    are available for you to now play with. Remember, with great power comes great
    responsibility.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 上述较长的文本是由最小的GPT-2模型生成的，该模型大约有1.24亿个参数。目前有多个不同的设置和模型大小供你使用。记住，强大的能力伴随巨大的责任。
- en: Between the last chapter and this one, we have covered both the encoder and
    decoder parts of the Transformer architecture conceptually. Now, we are ready
    to put both parts together in the next chapter. Let's quickly review what we covered
    in this chapter.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章和这一章之间，我们已经从概念上涵盖了Transformer架构中的编码器和解码器部分。现在，我们准备在下一章将这两部分结合起来。让我们快速回顾一下本章的内容。
- en: Summary
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Generating text is a complicated task. There are practical uses that can make
    typing text messages or composing emails easier. On the other hand, there are
    creative uses, like generating stories. In this chapter, we covered a character-based
    RNN model to generate headlines one character at a time and noted that it picked
    up the structure, capitalization, and other things quite well. Even though the
    model was trained on a particular dataset, it showed promise in completing short
    sentences and partially typed words based on the context. The next section covered
    the state-of-the-art GPT-2 model, which is based on the Transformer decoder architecture.
    The previous chapter had covered the Transformer encoder architecture, which is
    used by BERT.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 生成文本是一个复杂的任务。它有实际的用途，可以使打字文本消息或撰写电子邮件变得更加容易。另一方面，还有创造性的用途，如生成故事。在本章中，我们介绍了基于字符的RNN模型，逐个字符生成标题，并注意到它在结构、大写和其他方面表现出色。尽管模型是在特定数据集上训练的，但它在根据上下文完成短句和部分打字词方面表现出了潜力。接下来的部分介绍了基于Transformer解码器架构的最先进GPT-2模型。前一章已经介绍了Transformer编码器架构，BERT使用了该架构。
- en: Generating text has many knobs to tune like temperature to resample distributions,
    greedy search, beam search, and Top-K sampling to balance the creativity and predictability
    of the generated text. We saw the impact of these settings on text generation
    and used a pre-trained GPT-2 model provided by Hugging Face to generate text.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 生成文本有许多可以调节的参数，如重新采样分布的温度、贪婪搜索、波束搜索和Top-K采样，以平衡生成文本的创造性和可预测性。我们看到了这些设置对文本生成的影响，并使用了Hugging
    Face提供的预训练GPT-2模型来生成文本。
- en: Now that both the encoder and decoder parts of the Transformer architecture
    have been covered, the next chapter will use the full Transformer to build a text
    summarization model. Text summarization is at the cutting edge of NLP today. We
    will build a model that will read news articles and summarize them in a few sentences.
    Onward!
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了Transformer架构的编码器和解码器部分，下一章将使用完整的Transformer构建一个文本摘要模型。文本摘要技术正处于自然语言处理的前沿。我们将建立一个模型，能够阅读新闻文章并用几句话总结出来。继续前进！
