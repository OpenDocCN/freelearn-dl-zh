- en: Deep Q-Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q网络
- en: So far, we've approached and developed reinforcement learning algorithms that
    learn about a value function, *V*, for each state, or an action-value function,
    *Q*, for each action-state pair. These methods involve storing and updating each
    value separately in a table (or an array). These approaches do not scale because,
    for a large number of states and actions, the table's dimensions increase exponentially
    and can easily exceed the available memory capacity.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经接触并开发了强化学习算法，这些算法学习每个状态的价值函数*V*，或每个动作-状态对的动作价值函数*Q*。这些方法涉及单独存储和更新每个值在一个表格（或数组）中。由于状态和动作的数量庞大，这些方法无法扩展，因为表格的维度会呈指数增长，容易超出可用的内存容量。
- en: In this chapter, we will introduce the use of function approximation in reinforcement
    learning algorithms to overcome this problem. In particular, we will focus on
    deep neural networks that are applied to Q-learning. In the first part of this
    chapter, we'll explain how to extend Q-learning with function approximation to
    store Q values, and we'll explore some major difficulties that we may face. In
    the second part, we will present a new algorithm called **Deep Q-network** (**DQN**),
    which using new ideas, offers an elegant solution to some challenges that are
    found in the vanilla version of Q-learning with neural networks. You'll see how
    this algorithm achieves surprising results on a wide variety of games that learn
    only from pixels. Moreover, you'll implement this algorithm and apply it to Pong,
    and see some of its strengths and vulnerabilities for yourself.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍在强化学习算法中使用函数逼近来克服这个问题。特别是，我们将关注应用于Q-learning的深度神经网络。本章的第一部分，我们将解释如何使用函数逼近扩展Q-learning来存储Q值，并探讨我们可能面临的一些主要困难。在第二部分，我们将介绍一种新算法——**深度Q网络**（**DQN**），它利用新的思想，为在传统Q-learning与神经网络结合中遇到的一些挑战提供了优雅的解决方案。你将看到该算法如何在仅从像素学习的多种游戏中取得令人惊讶的成果。此外，您将实现该算法并将其应用于Pong，亲自体验它的一些优点和弱点。
- en: Since DQN was proposed, other researchers have proposed many variations that
    provide more stability and efficiency for the algorithm. We'll quickly look at
    and implement some of them so that we have a better understanding of the weaknesses
    of the basic version of DQN and so that we can provide you with some ideas so
    that you can improve it yourself.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 自从提出DQN以来，其他研究人员提出了许多变种，这些变种为算法提供了更好的稳定性和效率。我们将快速浏览并实现其中一些，以便更好地理解基础版本DQN的弱点，并为您提供一些思路，以便您自己改进它。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Deep neural networks and Q-learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络与Q-learning
- en: DQN
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN
- en: DQN applied to Pong
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN应用于Pong
- en: DQN variations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN变种
- en: Deep neural networks and Q-learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络与Q-learning
- en: 'The Q-learning algorithm, as we saw in [Chapter 4](6364d5e9-40e9-4991-a88f-1ce5b0f0a6e9.xhtml),
    *Q-Learning and SARSA Applications*, has many qualities that enable its application
    in many real-world contexts. A key ingredient of this algorithm is that it makes
    use of the Bellman equation for learning the Q-function. The Bellman equation, as
    used by the Q-learning algorithm, enables the updating of Q-values from subsequent
    state-action values. This makes the algorithm able to learn at every step, without
    waiting until the trajectory is completed. Also, every state or action-state pair
    has its own values stored in a lookup table that saves and retrieves the corresponding
    values. Being designed in this way, Q-learning converges to optimal values as
    long as all the state-action pairs are repeatedly sampled. Furthermore, the method
    uses two policies: a non-greedy behavior policy to gather experience from the
    environment (for example, ![](img/2456b0f9-50be-4a72-8670-0095bae6292b.png)-greedy)
    and a target greedy policy that follows the maximum Q-value.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第4章](6364d5e9-40e9-4991-a88f-1ce5b0f0a6e9.xhtml)《Q-Learning和SARSA应用》中看到的那样，Q-learning算法具有许多优点，使其能够应用于许多现实世界的场景。该算法的一个关键要素是它利用Bellman方程来学习Q函数。Q-learning算法使用的Bellman方程能够从后续的状态-动作值中更新Q值。这使得该算法能够在每一步学习，而无需等待轨迹完成。此外，每个状态或动作-状态对都有自己存储的值，保存在查找表中，以便保存和检索相应的值。正因为如此，Q-learning只要反复采样所有状态-动作对，就会收敛到最优值。此外，该方法使用两种策略：非贪婪行为策略用于从环境中收集经验（例如，![](img/2456b0f9-50be-4a72-8670-0095bae6292b.png)-贪婪），以及遵循最大Q值的目标贪婪策略。
- en: Maintaining a tabular representation of values can be contraindicated and in
    some cases, harmful. That's because most problems have a very high number of states
    and actions. For example, images (including small ones) have more state than the
    atoms in the universe. You can easily guess that, in this situation, tables cannot
    be used. Besides the infinite memory that the storage of such a table requires,
    only a few states will be visited more than once, making learning about the Q-function
    or V-function extremely difficult. Thus, we may want to generalize across states.
    In this case, generalization means that we are not only interested in the precise
    value of a state, *V(s)*, but also in the values in similar and near states. If
    a state has never been visited, we could approximate it with the value of a state
    near it. Generally speaking, the concept of generalization is incredibly important
    in all machine learning, including reinforcement learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 维持表格形式的值表示可能是不可取的，有时甚至是有害的。因为大多数问题都有非常高的状态和动作数量。例如，图像（包括小图像）所包含的状态比宇宙中的原子还要多。你可以轻易猜到，在这种情况下，无法使用表格。除了这种表格所需的无限存储空间之外，只有少数状态会被访问多次，这使得关于Q函数或V函数的学习变得极为困难。因此，我们可能希望对状态进行泛化。在这种情况下，泛化意味着我们不仅仅关注一个状态的精确值，*V(s)*，还关注相似和邻近状态的值。如果一个状态从未被访问过，我们可以用一个接近它的状态的值来逼近它。一般来说，泛化的概念在所有机器学习中都极为重要，包括强化学习。
- en: The concept of generalization is fundamental in circumstances where the agent
    doesn't have a complete view of the environment. In this case, the full state
    of the environment will be hidden by the agent that has to make decisions based
    solely on a restricted representation of the environment. This is known as **observation**.
    For example, think about a humanoid agent that deals with basic interactions in
    the real world. Obviously, it doesn't have a view of the complete state of the
    universe and of all the atoms. It only has a limited viewpoint, that is, observation,
    which is perceived by its sensors (such as video cameras). For this reason, the
    humanoid agent should generalize what's happening around it and behave accordingly.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化的概念在代理无法全面了解环境的情况下是至关重要的。在这种情况下，环境的完整状态将被代理隐藏，代理只能基于环境的有限表示做出决策。这被称为**观察**。例如，想象一个类人代理处理现实世界中的基本交互。显然，它无法看到整个宇宙的状态以及所有原子的情况。它只有一个有限的视角，也就是通过其传感器（例如视频摄像头）感知到的观察。正因如此，类人代理应该对周围发生的事情进行泛化，并相应地作出行为。
- en: Function approximation
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 函数逼近
- en: Now that we have talked about the main constraints of tabular algorithms and
    expressed the need for generalization capabilities in RL algorithms, we have to
    deal with the tools that allow us to get rid of these tabular constraints and
    address the generalization problem.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了表格算法的主要约束，并表达了在强化学习（RL）算法中对泛化能力的需求，我们需要处理一些工具，帮助我们摆脱这些表格约束并解决泛化问题。
- en: We can now dismiss tables and represent value functions with a function approximator.
    Function approximation allows us to represent value functions in a constraint
    domain using only a fixed amount of memory. Resource allocation is only dependent
    on the function that's used to approximate the problem. The choice of function
    approximator is, as always, task-dependent. Examples of function approximation
    are linear functions, decision trees, nearest neighbor algorithms, artificial
    neural networks, and so on. As you may expect, artificial neural networks are
    preferred over all the others – it is not a coincidence that it is widespread
    across all kinds of RL algorithms. In particular, deep artificial neural networks,
    or for brevity, **deep neural networks** (**DNNs**), are used. Their popularity
    is due to their efficiency and ability to learn features by themselves, creating
    a hierarchical representation as the hidden layers of the network increase. Also,
    deep neural networks, and in particular, **convolutional neural networks** (**CNNs**),
    deal incredibly well with images, as demonstrated by recent breakthroughs, especially
    in supervised tasks. But despite the fact that almost all studies of deep neural
    networks have been done in supervised learning, their integration in an RL framework
    has produces very interesting results. However, as we'll see shortly, this is
    not easy.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以放弃表格，并用函数逼近器来表示值函数。函数逼近使我们能够在约束域内使用固定量的内存表示值函数。资源分配仅取决于用于逼近问题的函数。函数逼近器的选择，如同以往，依赖于任务的具体需求。函数逼近的例子有线性函数、决策树、最近邻算法、人工神经网络等等。正如你所预料的，人工神经网络被优先选择，因为它们在所有其他方法中占据主导地位——这并非偶然，它已广泛应用于各种强化学习算法中。特别地，深度人工神经网络，或者简言之，**深度神经网络**（**DNNs**），被广泛使用。它们的流行源于其高效性和能够自我学习特征的能力，随着网络隐藏层的增加，能够创建层次化的表示。此外，深度神经网络，特别是**卷积神经网络**（**CNNs**），在处理图像方面表现得极为出色，正如近期的突破所示，特别是在监督学习任务中。但尽管几乎所有关于深度神经网络的研究都集中在监督学习中，它们在强化学习框架中的应用却产生了非常有趣的结果。然而，正如我们稍后会看到的，这并不容易。
- en: Q-learning with neural networks
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络的Q学习
- en: 'In Q-learning, a deep neural network learns a set of weights to approximate
    the Q-value function. Thereby, the Q-value function is parametrized by ![](img/47f90968-1930-465a-95df-834ba7ae1cde.png) (the
    weights of the network) and written as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在Q学习中，深度神经网络通过学习一组权重来近似Q值函数。因此，Q值函数通过 ![](img/47f90968-1930-465a-95df-834ba7ae1cde.png) （网络的权重）来参数化，并写作如下：
- en: '![](img/98a40f0c-b8e7-4983-a2ba-d19839d56bf4.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98a40f0c-b8e7-4983-a2ba-d19839d56bf4.png)'
- en: To adapt Q-learning with deep neural networks (this combination takes the name
    of deep Q-learning), we have to come up with a loss function (or objective) to
    minimize.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将Q学习与深度神经网络结合（这一组合被称为深度Q学习），我们必须提出一个损失函数（或目标）来进行最小化。
- en: 'As you may recall, the tabular Q-learning update is as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所记得，表格形式的Q学习更新如下：
- en: '![](img/e222333f-62b2-47e8-80ec-cd3e387e4699.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e222333f-62b2-47e8-80ec-cd3e387e4699.png)'
- en: Here, ![](img/e4c0615d-ded4-4546-802e-410419febed6.png) is the state at the
    next step. This update is done online on each sample that's collected by the behavior
    policy.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里， ![](img/e4c0615d-ded4-4546-802e-410419febed6.png) 是下一步的状态。此更新在每个通过行为策略收集到的样本上进行在线更新。
- en: Compared to the previous chapters, to simplify the notation, here, we refer
    to ![](img/00457ffe-7e2c-485e-adbf-7a8aff8e0bc5.png) as the state and action in
    the present step, while ![](img/1a6b4da8-b62e-4792-9d4d-94a718870eed.png) is referred
    to as the state and action in the next step.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章相比，为了简化符号表示，在这里我们将 ![](img/00457ffe-7e2c-485e-adbf-7a8aff8e0bc5.png) 称为当前步骤中的状态和动作，而 ![](img/1a6b4da8-b62e-4792-9d4d-94a718870eed.png) 则称为下一步中的状态和动作。
- en: 'With the neural network, our objective is to optimize the weight, ![](img/db3c4a2b-d406-4101-9489-dbdbefc7ac01.png), so
    that ![](img/8b1968e5-7d3b-4b46-985f-cbb14efcb476.png) resembles the optimal Q-value
    function. But since we don''t have the optimal Q-function, we can only make small
    steps toward it by minimizing the Bellman error for one step, ![](img/0a5374d6-404d-45e1-8c8e-1fd5288dbab8.png).
    This step is similar to what we''ve done in tabular Q-learning. However, in deep
    Q-learning, we don''t update the single value, ![](img/55fd7330-0019-4509-a791-cb1f3a7960a8.png).
    Instead, we take the gradient of the Q-function with respect to the parameters, ![](img/a480d51f-1eb1-463f-9270-59e39cd03366.png):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中，我们的目标是优化权重 ![](img/db3c4a2b-d406-4101-9489-dbdbefc7ac01.png)，使得 ![](img/8b1968e5-7d3b-4b46-985f-cbb14efcb476.png)
    更接近最优的 Q 值函数。但由于我们没有最优的 Q 函数，我们只能通过最小化 Bellman 错误来朝着最优 Q 函数迈出小步伐，这个 Bellman 错误是一步的误差，即
    ![](img/0a5374d6-404d-45e1-8c8e-1fd5288dbab8.png)。这一步类似于我们在表格 Q 学习中所做的。然而，在深度
    Q 学习中，我们不更新单一的值 ![](img/55fd7330-0019-4509-a791-cb1f3a7960a8.png)。相反，我们对 Q 函数相对于参数
    ![](img/a480d51f-1eb1-463f-9270-59e39cd03366.png) 进行梯度计算：
- en: '![](img/231422ae-3ae5-47e8-b3a1-cf18a09cf8af.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/231422ae-3ae5-47e8-b3a1-cf18a09cf8af.png)'
- en: Here, ![](img/bdd2ace6-17d7-4f48-a39e-ed1808930af8.png) is the partial derivate
    of ![](img/e941b29d-7599-4d23-81bc-a654b086904f.png) with respect to ![](img/9205cd8e-8704-4fbb-a34d-9063dcc45453.png) . ![](img/9f063db9-8f96-4021-9a6a-5a2adfd0af21.png) is
    called the learning rate, which is the size of the step to take toward the gradient.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/bdd2ace6-17d7-4f48-a39e-ed1808930af8.png) 是关于 ![](img/e941b29d-7599-4d23-81bc-a654b086904f.png)
    相对于 ![](img/9205cd8e-8704-4fbb-a34d-9063dcc45453.png) 的偏导数。![](img/9f063db9-8f96-4021-9a6a-5a2adfd0af21.png)
    被称为学习率，它表示朝着梯度方向迈出的步伐大小。
- en: 'In reality, the smooth transition that we just saw from tabular Q-learning
    to deep Q-learning doesn''t yield a good approximation. The first fix involves
    the use of the **Mean Square Error** (**MSE**) as a loss function (instead of
    the Bellman error). The second fix is to migrate from an online Q-iteration to
    a batch Q-iteration. This means that the parameters of the neural network are
    updated using multiple transitions at once (such as using a mini-batch of size
    greater than 1 in supervised settings). These changes produce the following loss
    function:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们刚才从表格 Q 学习到深度 Q 学习的平滑过渡，并没有产生一个良好的近似。第一个修复方法是使用**均方误差**（**MSE**）作为损失函数（而不是
    Bellman 错误）。第二个修复方法是将在线 Q 迭代迁移到批量 Q 迭代。这意味着神经网络的参数会通过一次性使用多个过渡来更新（例如，在监督学习设置中使用大于
    1 的小批量）。这些更改产生了以下损失函数：
- en: '![](img/8b4a9659-3b2d-47fc-956b-91e4dfbe7805.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b4a9659-3b2d-47fc-956b-91e4dfbe7805.png)'
- en: 'Here, ![](img/8e2d8ca5-6815-4a91-822f-23c145349a67.png) isn''t the true action-value
    function since we haven''t used it. Instead, it is the Q-target value:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/8e2d8ca5-6815-4a91-822f-23c145349a67.png) 不是实际的动作价值函数，因为我们并没有使用它。相反，它是
    Q 目标值：
- en: '![](img/89094609-0115-4184-84e1-040099cdd2b1.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89094609-0115-4184-84e1-040099cdd2b1.png)'
- en: 'Then, the network parameter, ![](img/af8c754c-5f09-4058-901d-a6cbb18fc3c6.png), is
    updated by gradient descent on the MSE loss function, ![](img/42695026-a675-49e2-b51e-d2b9526c6926.png):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，网络参数 ![](img/af8c754c-5f09-4058-901d-a6cbb18fc3c6.png) 通过在均方误差（MSE）损失函数 ![](img/42695026-a675-49e2-b51e-d2b9526c6926.png)
    上进行梯度下降来更新：
- en: '![](img/38257569-a20d-4b7b-b4f6-23fac713e899.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38257569-a20d-4b7b-b4f6-23fac713e899.png)'
- en: It's very important to note that ![](img/f31318a0-7dcd-43eb-8eac-270a9914848a.png) is treated
    as a constant and that the gradient of the loss function isn't propagated further.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 非常重要的一点是，![](img/f31318a0-7dcd-43eb-8eac-270a9914848a.png) 被视为常数，损失函数的梯度不会进一步传播。
- en: Since, in the previous chapter, we introduced MC algorithms, we want to highlight
    that these algorithms can also be adapted to work with neural networks. In this
    case, ![](img/162cb052-26ad-42c8-b542-2c45e9dd0fe6.png) will be the return, ![](img/73088934-4648-4058-b4bd-266e60d814c0.png). Since
    the MC update isn't biased, it's asymptotically better than TD, but the latter
    has better results in practice.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在前一章中，我们介绍了蒙特卡洛（MC）算法，我们希望强调这些算法也可以调整为与神经网络一起使用。在这种情况下，![](img/162cb052-26ad-42c8-b542-2c45e9dd0fe6.png)
    将是回报，![](img/73088934-4648-4058-b4bd-266e60d814c0.png)。由于 MC 更新没有偏差，它在渐近意义上优于时序差分（TD），但在实践中，后者的表现更好。
- en: Deep Q-learning instabilities
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度 Q 学习的不稳定性
- en: With the loss function and the optimization technique we just presented, you
    should be able to develop a deep Q-learning algorithm. However, the reality is
    much more subtle. Indeed, if we try to implement it, it probably won't work. Why?
    Once we introduce neural networks, we can no longer guarantee improvement. Although
    tabular Q-learning has convergence capabilities, its neural network counterpart
    does not.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们刚刚提出的损失函数和优化技术，你应该能够开发出深度Q学习算法。然而，现实要复杂得多。实际上，如果我们尝试实现它，可能无法成功。为什么？一旦引入神经网络，我们就无法保证算法的改进。尽管表格型Q学习具有收敛能力，但其神经网络版本并不具备。
- en: 'Sutton and Barto in *Reinforcement Learning: An Introduction*, introduced a
    problem called the deadly triad, which arises when the following three factors
    are combined:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Sutton和Barto在*《强化学习：导论》*中提出了一个问题，叫做致命三合一问题，它出现在以下三种因素结合时：
- en: Function approximation
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数逼近
- en: Bootstrapping (that is, the update used by other estimates)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自举法（即，由其他估计值进行的更新）
- en: Off-policy learning (Q-learning is an off-policy algorithm since its update
    is independent on the policy that's being used)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离策略学习（Q学习是一种离策略算法，因为它的更新与所使用的策略无关）
- en: But these are exactly the three main ingredients of the deep Q-learning algorithm.
    As the authors noted, we cannot get rid of bootstrapping without affecting the
    computational cost or data efficiency. Moreover, off-policy learning is important
    for creating more intelligent and powerful agents. And clearly, without deep neural
    networks, we'll lose an extremely important component. Therefore, it is very important
    to design algorithms that preserve these three components but at the same time
    mitigate the deadly triad problem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些恰恰是深度Q学习算法的三个主要成分。正如作者所指出的那样，我们无法摆脱自举法，而不影响计算成本或数据效率。此外，离策略学习对于创建更智能、更强大的代理至关重要。显然，没有深度神经网络，我们将失去一个非常重要的组件。因此，设计能够保持这三种成分的算法，同时减轻致命三合一问题，是非常重要的。
- en: Besides, from equations (5.2) and (5.3), the problem may seem similar to supervised
    regression, but it's not. In supervised learning, when performing SGD, the mini-batches are
    always sampled randomly from a dataset to make sure that they are **independent
    and identically distributed** (**IID**). In RL, it is the policy that gathers
    the experience. And because the states are sequential and strongly related to
    each other, the i.i.d assumption is immediately lost, causing severe instabilities
    when performing SGD.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，从方程（5.2）和（5.3）来看，问题可能看起来类似于有监督回归，但它并非如此。在有监督学习中，进行SGD时，迷你批次总是从数据集中随机抽样，以确保它们是**独立同分布**（**IID**）的。而在强化学习中，经验是由策略收集的。由于状态是顺序的并且相互关联，i.i.d
    假设立即失效，这在执行SGD时会导致严重的不稳定性。
- en: Another cause of instability is due to the non-stationarity of the Q-learning
    process. From equation, (5.2) and (5.3), you can see that the same neural network
    that is updated is also the one that computes the target values, ![](img/5867eec4-30f5-46f8-a86b-5809cb70bf16.png).
    This is dangerous, considering that the target values will also be updated during
    training. It's like shooting at a moving circular target without taking into consideration
    its movement. These behaviors are only due to the generalization capabilities
    of the neural network; in fact, they are not a problem in a tabular case.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个不稳定性的原因是Q学习过程的非平稳性。从方程（5.2）和（5.3）中可以看出，同一个神经网络在更新时也是计算目标值的网络，![](img/5867eec4-30f5-46f8-a86b-5809cb70bf16.png)。考虑到目标值在训练过程中也会被更新，这一点是危险的。这就像是射击一个移动的圆形目标，却没有考虑到它的运动。这些行为仅仅是神经网络的泛化能力所致；实际上，在表格型情况下并不是问题。
- en: Deep Q-learning is poorly understood theoretically but, as we'll soon see, there
    is an algorithm that deploys a few tricks to increase the i.i.d of the data and
    alleviate the moving target problem. These tricks make the algorithm much more
    stable and flexible.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q学习在理论上理解较少，但正如我们很快会看到的那样，有一种算法通过使用一些技巧来增加数据的i.i.d性并缓解目标移动问题。这些技巧使得算法更加稳定和灵活。
- en: DQN
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN
- en: DQN, which was introduced for the first time in the paper *Human-level control
    through deep reinforcement learning*by Mnih and others from DeepMind, is the first
    scalable reinforcement learning algorithm that combines Q-learning with deep neural
    networks. To overcome stability issues, DQN adopts two novel techniques that turned
    out to be essential for the balance of the algorithm.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: DQN首次出现在Mnih等人（DeepMind）发表的论文*通过深度强化学习实现人类水平控制*中，是第一个将Q学习与深度神经网络结合的可扩展强化学习算法。为了克服稳定性问题，DQN采用了两种新的技术，这些技术对算法的平衡至关重要。
- en: DQN has proven itself to be the first artificial agent capable of learning in
    a diverse array of challenging tasks. Furthermore, it has learned how to control
    many tasks using only high-dimensional row pixels as input and using an end-to-end
    RL approach.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: DQN已经证明自己是第一个能够在多种具有挑战性的任务中学习的人工智能代理。此外，它已经学会了如何仅通过高维度行像素作为输入，并采用端到端的强化学习方法来控制多个任务。
- en: The solution
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决方案
- en: The key innovations brought by DQN involve a **replay buffer** to get over the
    data correlation drawback, and a separate *target network* to get over the non-stationarity
    problem.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: DQN带来的关键创新包括**回放缓冲区**，以克服数据相关性问题，以及一个独立的*目标网络*，以解决非平稳性问题。
- en: Replay memory
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回放记忆
- en: To use more IID data during SGD iterations, DQN introduced a replay memory (also
    called experienced replay) to collect and store the experience in a large buffer.
    This buffer ideally contains all the transitions that have taken place during
    the agent's lifetime. When doing SGD, a random mini-batch will be gathered from
    the experienced replay and used in the optimization procedure. Since the replay
    memory buffer holds varied experience, the mini-batch that's sampled from it will
    be diverse enough to provide independent samples. Another very important feature
    behind the use of an experience replay is that it enables the reusability of the
    data as the transitions will be sampled multiple times. This greatly increases
    the data efficiency of the algorithm.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在SGD迭代中使用更多的IID数据，DQN引入了回放记忆（也叫经验回放）来收集和存储在一个大缓冲区中的经验。这个缓冲区理想情况下包含了智能体在其生命周期中发生的所有转移。在执行SGD时，将从经验回放中随机抽取一个小批量样本，并用于优化过程。由于回放记忆缓冲区存储了多样化的经验，从中抽取的小批量样本将具有足够的多样性，提供独立的样本。使用经验回放的另一个非常重要的特点是，它使得数据的可重用性得以实现，因为这些转移将被多次抽取。这大大提高了算法的数据效率。
- en: The target network
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标网络
- en: The moving target problem is due to continuously updating the network during
    training, which also modifies the target values. Nevertheless, the neural network
    has to update itself in order to provide the best possible state-action values.
    The solution that's employed in DQNs is to use two neural networks. One is called
    the *online network*, which is constantly updated, while the other is called the *target
    network*, which is updated only every *N* iterations (with *N* usually being between
    1,000 and 10,000). The online network is used to interact with the environment
    while the target network is used to predict the target values. In this way, for
    *N* iterations, the target values that are produced by the target network remain
    fixed, preventing the propagation of instabilities and decreasing the risk of
    divergence. A potential disadvantage is that the target network is an old version
    of the online network. Nonetheless, in practice, the advantages greatly outweigh
    the disadvantages and the stability of the algorithm will improve significantly.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 移动目标问题是由于在训练过程中持续更新网络，这也修改了目标值。然而，神经网络必须不断更新自己，以提供最佳的状态-动作值。DQNs中采用的解决方案是使用两个神经网络。一个被称为*在线网络*，它不断更新，而另一个被称为*目标网络*，它只在每隔*N*次迭代后更新（*N*通常在1,000到10,000之间）。在线网络用于与环境交互，而目标网络用于预测目标值。通过这种方式，对于*N*次迭代，目标网络生成的目标值保持不变，防止了不稳定性的传播并降低了发散的风险。一个潜在的缺点是目标网络是在线网络的旧版本。然而，在实践中，优点远远大于缺点，算法的稳定性将显著提高。
- en: The DQN algorithm
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN算法
- en: The introduction of a replay buffer and of a separate target network in a deep
    Q-learning algorithm has been able to control Atari games (such as Space Invaders,
    Pong, and Breakout) from nothing but images, a reward, and a terminal signal.
    DQN learns completely end to end with a combination of CNN and fully connected
    neural networks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度Q学习算法中引入重放缓冲区和单独的目标网络，使得从只有图像、奖励和终止信号开始，便能控制Atari游戏（如《太空侵略者》、《乒乓》和《打砖块》）。DQN通过结合卷积神经网络（CNN）和全连接神经网络，完全端到端地进行学习。
- en: DQN has been trained separately on 49 Atari games with the same algorithm, network
    architecture, and hyperparameters. It performed better than all the previous algorithms,
    achieving a level comparable to or better than professional gamers on many games.
    The Atari games are not easy to solve and many of them demand complex planning
    strategies. Indeed, a few of them (such as the well-known Montezuma's Revenge)
    required a level that even DQN hasn't been able to achieve.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: DQN已经在49个Atari游戏上分别进行了训练，使用相同的算法、网络架构和超参数。它表现得比所有以前的算法更好，在许多游戏中达到了与专业玩家相当甚至更好的水平。Atari游戏并不容易解决，其中许多需要复杂的规划策略。实际上，其中一些（如著名的《蒙特祖马的复仇》）要求的水平甚至是DQN尚未能够达到的。
- en: A particularity of these games is that, as they provide only images to the agent,
    they are partially observable. They don't show the full state of the environment.
    In fact, a single image isn't enough to fully understand the current situation.
    For example, can you deduce the direction of the ball in the following image?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些游戏的一个特点是，由于它们仅向代理提供图像，因此它们是部分可观测的。它们没有显示环境的完整状态。事实上，一张图像不足以完全理解当前的情况。例如，你能从以下图像中推断出球的方向吗？
- en: '![](img/35491dfb-ad59-49c7-aa1b-84480f56a173.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35491dfb-ad59-49c7-aa1b-84480f56a173.png)'
- en: Figure 5.1\. Rendering of pong
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1\. Pong的渲染
- en: You can't, and neither can the agent. To overcome this situation, at each point
    in time, a sequence of the previous observations is considered. Usually the last
    two to five frames are used, and in most cases, they give a pretty accurate approximation
    of the actual overall state.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你做不到，代理也做不到。为了克服这种情况，在每个时间点，都会考虑一系列先前的观察结果。通常会使用最后两到五帧，在大多数情况下，它们能较为准确地近似实际的整体状态。
- en: The loss function
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数
- en: 'The deep Q-network is trained by minimizing the loss function (5.2) that we
    have already presented, but with the further employment of a separate Q-target
    network, ![](img/3804d279-d7a4-4dab-b5d4-2f9287074dad.png), with a weight, ![](img/cbd02dbe-17b9-4d28-a7ed-961ac768c913.png),
    putting everything together, the loss function becomes:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 深度Q网络通过最小化我们已经介绍的损失函数(5.2)来进行训练，但进一步使用了一个单独的Q目标网络，![](img/3804d279-d7a4-4dab-b5d4-2f9287074dad.png)，并且带有权重，![](img/cbd02dbe-17b9-4d28-a7ed-961ac768c913.png)，将一切整合起来，损失函数变为：
- en: '![](img/e80dd0ac-24a7-43fd-8d6a-2b528a551450.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e80dd0ac-24a7-43fd-8d6a-2b528a551450.png)'
- en: Here, ![](img/9420e431-c0af-40fd-8207-eec9aba0ffcb.png) is the parameters of
    the online network.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/9420e431-c0af-40fd-8207-eec9aba0ffcb.png)是在线网络的参数。
- en: 'The optimization of the differentiable loss function (5.4) is performed with
    our favorite iterative method, namely mini-batch gradient descent. That is, the
    learning update is applied to mini-batches that have been drawn uniformly from
    the experienced buffer. The derivative of the loss function is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 可微分损失函数(5.4)的优化使用了我们最喜欢的迭代方法，即小批量梯度下降。也就是说，学习更新应用于从经验缓冲区均匀抽取的小批量数据。损失函数的导数如下：
- en: '![](img/06c6fb1b-1b4d-4ff4-8f33-c1a63bfe59fd.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06c6fb1b-1b4d-4ff4-8f33-c1a63bfe59fd.png)'
- en: Unlike the problem framed in the case of deep Q-learning, in DQN, the learning
    process is more stable. Furthermore, because the data is more i.i.d. and the target
    is (somehow) fixed, it's very similar to a regression problem. But on the other
    hand, the targets still depend on the network weights.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度Q学习中的问题框架不同，在DQN中，学习过程更加稳定。此外，由于数据更加独立同分布（i.i.d.），并且目标（以某种方式）是固定的，因此它与回归问题非常相似。但另一方面，目标仍然依赖于网络权重。
- en: If you optimize the loss function (5.4) at each step and only on a single sample,
    you would obtain the Q-learning algorithm with function approximation.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在每一步都优化损失函数(5.4)，并且只对单一样本进行优化，你将得到带有函数逼近的Q学习算法。
- en: Pseudocode
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伪代码
- en: Now that all the components of DQN have been explained, we can put all the pieces
    together and show you the pseudocode version of the algorithm to clarify any uncertainties
    (don't worry if it doesn't – in the next section, you'll implement it and everything
    will be clearer).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，DQN的所有组件都已被解释清楚，我们可以将所有部分结合起来，向你展示该算法的伪代码版本，以澄清任何不确定的地方（如果你还是不明白，也不用担心——在下一节，你将实现它，一切都会变得更清晰）。
- en: 'The DQN algorithm involves three main parts:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: DQN算法涉及三个主要部分：
- en: Data collection and storage. The data is collected by following a behavior policy
    (for example, ![](img/93596807-c9b3-4293-b605-b0981360a525.png)-greedy).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集和存储。数据通过遵循行为策略（例如，![](img/93596807-c9b3-4293-b605-b0981360a525.png)-贪心）进行收集。
- en: Neural network optimization (performing SGD on mini-batches that have been sampled
    from the buffer).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络优化（对从缓冲区中采样的小批量数据执行SGD）。
- en: Target update.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标更新。
- en: 'The pseudocode of DQN is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: DQN的伪代码如下：
- en: '[PRE0]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, `d` is a flag that's returned by the environment that signals whether
    the environment is in its final state. If `d=True`, that is, the episode has ended,
    the environment has to be reset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`d`是由环境返回的标志，指示环境是否处于最终状态。如果`d=True`，即表示这一回合结束，环境需要重置。
- en: '![](img/d100d047-49cf-46bc-9477-439651e55000.png) is a preprocessing step that
    changes the images to reduce their dimensionality (it converts the images into
    grayscale and resizes them into smaller images) and adds the last `n` frames to
    the current frame. Usually, `n` is a value between 2 and 4\. The preprocessing
    part will be explained in more detail in the next section, where we''ll implement
    DQN.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/d100d047-49cf-46bc-9477-439651e55000.png)是一个预处理步骤，它通过降低图像的维度来改变图像（将图像转换为灰度图并将其调整为更小的尺寸），并将最后`n`帧添加到当前帧。通常，`n`的值介于2和4之间。预处理部分将在下一节中详细解释，我们将在那里实现DQN。'
- en: In DQN, the experienced replay, ![](img/16e1586f-bf6d-42a9-9cb4-b16f41425950.png), is
    a dynamic buffer that stores a limited number of frames. In the paper, the buffer
    contains the last 1 million transitions and when it exceeds this dimension, it
    discards the older experiences.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在DQN中，经验回放，![](img/16e1586f-bf6d-42a9-9cb4-b16f41425950.png)，是一个动态缓冲区，用来存储有限数量的帧。在论文中，缓冲区包含了最后100万次转换，当它超过这个数量时，会丢弃旧的经验。
- en: All the other parts have already been described. If you are wondering why the
    target value, ![](img/928f3116-b6f5-4883-a7b1-6bbf04bbfea4.png), takes the ![](img/010c7f88-1ae7-4ef4-be70-385d773d785b.png) if ![](img/d963cc36-1c16-4e04-aa06-86afe819f31a.png) value,
    it is because there won't be any other interactions with the environment after
    and so ![](img/11b366a5-5a09-4f41-9fa0-9acef6d01b9a.png) is its actual unbiased
    Q-value.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其他部分已经描述过。如果你在想为什么目标值，![](img/928f3116-b6f5-4883-a7b1-6bbf04bbfea4.png)，会取![](img/010c7f88-1ae7-4ef4-be70-385d773d785b.png)的值，原因是之后不会再与环境进行交互，因此![](img/11b366a5-5a09-4f41-9fa0-9acef6d01b9a.png)是其实际的无偏Q值。
- en: Model architecture
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型架构
- en: So far, we have talked about the algorithm itself, but we haven't explained
    the architecture of the DQN. Besides the new ideas that have been adopted to stabilize
    its training, the architecture of the DQN plays a crucial role in the final performance
    of the algorithm. In the *DQN* paper, a single model architecture is used in all
    of the Atari environments. It combines CNNs and FNNs. In particular, as observation
    images are given as input, it employs a CNN to learn about feature maps from those
    images. CNNs have been widely used with images for their translation invariance characteristics
    and for their property of sharing weights, which allows the network to learn with
    fewer weights compared to other deep neural network types.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了算法本身，但还没有解释DQN的架构。除了为稳定训练而采用的新思想外，DQN的架构在算法的最终表现中也起着至关重要的作用。在*DQN*论文中，所有的Atari环境都使用了单一的模型架构。它结合了CNN和FNN。特别地，作为输入的观察图像使用CNN来学习这些图像的特征图。CNN由于具有平移不变性特征和共享权重的属性，已经广泛应用于图像处理中，这使得网络能够比其他类型的深度神经网络使用更少的权重进行学习。
- en: The output of the model corresponds to the state-action values, with one for
    each action. Thus, to control an agent with five actions, the model will output
    a value for each of those five actions. Such a model architecture allows us to
    compute all the Q-values with only one forward pass.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输出对应于状态-动作值，每个动作有一个输出值。因此，为了控制一个具有五个动作的智能体，模型将为每个动作输出一个值。这种模型架构允许我们仅通过一次前向传递就计算出所有的Q值。
- en: 'There are three convolutional layers. Each layer includes a convolution operation
    with an increasing number of filters and a decreasing dimension, as well as a
    non-linear function. The last hidden layer is a fully connected layer, followed
    by a rectified activation function and a fully-connected linear layer with an
    output for each action. A simple representation of this architecture is shown
    in the following illustration:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个卷积层。每一层包括一个卷积操作，过滤器数量逐渐增加，维度逐渐减小，同时还有一个非线性函数。最后一个隐藏层是一个全连接层，后接一个修正激活函数和一个全连接的线性层，每个动作都有一个输出。以下是该架构的简要表示：
- en: '![](img/01c3e6d6-1c32-4f3c-bef6-e4aa962214f0.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01c3e6d6-1c32-4f3c-bef6-e4aa962214f0.png)'
- en: Figure 5.2\. Illustration of a DNN architecture for DQN composed with a CNN
    and FNN
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2\. DQN 的 DNN 架构示意图，由 CNN 和 FNN 组成
- en: DQN applied to Pong
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN 应用于 Pong
- en: Equipped with all the technical knowledge about Q-learning, deep neural networks,
    and DQN, we can finally put it to work and start to warm up the GPU. In this section,
    we will apply DQN to an Atari environment, Pong. We have chosen Pong rather than
    all the other Atari environments because it's simpler to solve and thus requires
    less time, computational power, and memory. That being said, if you have a decent
    GPU available, you can apply the same exact configuration to almost all the other
    Atari games (some may require a little bit of fine-tuning). For the same reason,
    we adopted a lighter configuration compared to the original DQN paper, both in
    terms of the capacity of the function approximator (that is, fewer weights) and
    hyperparameters such as a smaller buffer size. This does not compromise the results
    rather on Pong but might degrade the performance of other games.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了关于 Q-learning、深度神经网络和 DQN 的所有技术知识后，我们终于可以开始实际应用并启动 GPU 了。在本节中，我们将 DQN 应用到
    Atari 环境 Pong。我们选择 Pong 而非其他所有 Atari 环境，因为它更容易解决，因此需要更少的时间、计算能力和内存。话虽如此，如果你有一个不错的
    GPU，可以将相同的配置应用于几乎所有其他 Atari 游戏（有些可能需要稍微调整）。出于同样的原因，我们采用了比原始 DQN 论文中更轻量的配置，既包括函数逼近器的容量（即更少的权重），也包括超参数如较小的缓存大小。这不会影响
    Pong 的结果，但可能会影响其他游戏的表现。
- en: First, we will briefly introduce the Atari environment and the preprocessing
    pipeline before moving on to the DQN implementation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将简要介绍 Atari 环境和预处理管道，然后再继续进行 DQN 实现。
- en: Atari games
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Atari 游戏
- en: 'Atari games became a standard testbed for deep RL algorithms since their introduction
    in the DQN paper. These were first provided in the **Arcade Learning Environment**
    (**ALE**) and subsequently wrapped by OpenAI Gym to provide a standard interface.
    ALE (and Gym) includes 57 of the most popular Atari 2600 video games, such as
    Montezuma''s Revenge, Pong, Breakout, and Space Invaders, as shown in the following
    illustration. These games have been widely used in RL research for their high-dimensional
    state space (210 x 160 pixels) and their task diversity between games:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 DQN 论文中首次介绍 Atari 游戏以来，它们就成为了深度强化学习算法的标准测试平台。这些游戏最初通过 **Arcade Learning Environment**（**ALE**）提供，随后被
    OpenAI Gym 封装，提供了标准接口。ALE（和 Gym）包括 57 款最受欢迎的 Atari 2600 视频游戏，如 Montezuma's Revenge、Pong、Breakout
    和 Space Invaders，如下图所示。这些游戏由于其高维状态空间（210 x 160 像素）以及游戏之间的任务多样性，已广泛应用于强化学习研究：
- en: '![](img/4252c8ec-b509-42d2-93d8-d1cfc94e9e4b.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4252c8ec-b509-42d2-93d8-d1cfc94e9e4b.png)'
- en: Figure 5.3 The Montezuma's Revenge, Pong, Breakout, and Space Invaders environments
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 Montezuma's Revenge、Pong、Breakout 和 Space Invaders 环境
- en: A very important note about Atari environments is that they are deterministic,
    meaning that, given a fixed set of actions, the results will be the same across
    multiple matches. From an algorithm perspective, this determinism holds true until
    all the history is used to choose an action from a stochastic policy.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Atari 环境的一个非常重要的说明是，它们是确定性的，这意味着在给定一组固定的动作时，多个比赛中的结果将是相同的。从算法的角度来看，这种确定性在所有历史数据用于从随机策略中选择动作之前都是成立的。
- en: Preprocessing
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理
- en: 'The frames in Atari are 210 x 160 pixels with RGB color, thus having an overall
    size of 210 x 160 x 3\. If a history of 4 frames was used, the input would have
    a dimension of 210 x 160 x 12\. Such dimensionality can be computationally demanding
    and it could be difficult to store a large number of frames in the experienced
    buffer. Therefore, a preprocessing step to reduce the dimensionality is necessary.
    In the original DQN implementation, the following preprocessing pipeline is used:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Atari中的每一帧为210 x 160像素，采用RGB颜色，因此其整体尺寸为210 x 160 x 3。如果使用4帧的历史数据，则输入的维度为210
    x 160 x 12。如此高的维度计算量大，而且在经验缓存中存储大量帧可能会变得困难。因此，必须进行预处理以减少维度。在原始的DQN实现中，使用了以下预处理管道：
- en: RGB colors are converted into grayscale
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RGB颜色被转换为灰度图。
- en: The images are downsampled to 110 x 84 and then cropped to 84 x 84
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像被下采样至110 x 84，然后裁剪为84 x 84。
- en: The last three to four frames are concatenated to the current frame
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后三到四帧会与当前帧拼接。
- en: The frames are normalized
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对帧进行归一化处理。
- en: Furthermore, because the games are run at a high frame rate, a technique called
    frame-skipping is used to skip ![](img/c56514f6-eae8-4703-b818-0b357b964b3e.png) consecutive frames.
    This technique allows the agent to store and train on fewer frames for each game
    without significantly degrading the performance of the algorithms. In practice,
    with the frame-skipping technique, the agent selects an action every ![](img/6b679333-18df-4201-a115-199619733e58.png) frames and repeats
    the action on the skipped frames.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于游戏以高帧率运行，采用了一种称为“跳帧”的技术，用于跳过![](img/c56514f6-eae8-4703-b818-0b357b964b3e.png)连续帧。这项技术使得智能体能够在每局游戏中存储和训练较少的帧，而不会显著降低算法的性能。实际上，使用跳帧技术时，智能体每隔![](img/6b679333-18df-4201-a115-199619733e58.png)帧选择一个动作，并在跳过的帧上重复该动作。
- en: In addition, in some environments, at the start of each game, the agent has
    to push the fire button in order to start the game. Also, because of the determinism
    of the environment, some no-ops are taken on the reset of the environment to start
    the agent in a random position.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在某些环境中，每局游戏开始时，智能体必须按下开火按钮才能开始游戏。另外，由于环境的确定性，在重置环境时，某些无操作（no-ops）会被执行，以便将智能体随机放置在一个位置。
- en: 'Luckily for us, OpenAI released an implementation of the preprocessing pipeline
    that is compatible with the Gym interface. You can find it in this book''s GitHub
    repository in the `atari_wrappers.py` file. Here, we will give just a brief explanation
    of the implementation:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，OpenAI发布了与Gym接口兼容的预处理管道实现。你可以在本书的GitHub仓库中的`atari_wrappers.py`文件中找到它。在这里，我们仅简要解释一下该实现：
- en: '`NoopResetEnv(n)`: Takes `n` no-ops on reset of the environment to provide
    a random starting position for the agent.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NoopResetEnv(n)`：在重置环境时执行`n`次无操作（no-op），以为智能体提供一个随机的起始位置。'
- en: '`FireResetEnv()`: Fires on reset of the environment (required only in some
    games).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FireResetEnv()`：在环境重置时开火（仅在某些游戏中需要）。'
- en: '`MaxAndSkipEnv(skip)`: Skips `skip` frames while taking care of repeating the
    actions and summing the rewards.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MaxAndSkipEnv(skip)`：跳过`skip`帧，并确保重复动作和累计奖励。'
- en: '`WarpFrame()`: Resizes the frame to 84 x 84 and converts it into grayscale.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WarpFrame()`：将帧大小调整为84 x 84并转换为灰度图。'
- en: '`FrameStack(k)`: Stacks the last `k` frames.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FrameStack(k)`：将最近的`k`帧堆叠在一起。'
- en: 'All of these functions are implemented as a wrapper. A wrapper is a way to
    easily transform an environment by adding a new layer on top of it. For example,
    to scale the frames on Pong, we would use the following code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些功能都作为包装器实现。包装器是一种通过在环境上方添加新层，来轻松转换环境的方式。例如，要在Pong中缩放帧，我们会使用以下代码：
- en: '[PRE1]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A wrapper has to inherit the `gym.Wrapper` class and override at least one
    of the following methods: `__init__(self, env)`, `step`, `reset`, `render`, `close`,
    or `seed`.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 包装器必须继承`gym.Wrapper`类，并且至少重载以下方法之一：`__init__(self, env)`、`step`、`reset`、`render`、`close`或`seed`。
- en: 'We won''t show the implementation of all the wrappers listed here as they are
    outside of the scope of this book, but we will use `FireResetEnv `and `WrapFrame` as
    examples to give you a general idea of their implementation. The complete code
    is available in this book''s GitHub repository:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们不会展示所有列出的包装器的实现，因为它们超出了本书的范围，但我们将使用`FireResetEnv`和`WrapFrame`作为示例，给你一个大致的实现概念。完整的代码可以在本书的GitHub仓库中找到：
- en: '[PRE2]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: First, `FireResetEnv` inherits the `Wrapper` class from Gym. Then, during the
    initialization, it checks the availability of the `fire` action by unwrapping
    the environment through `env.unwrapped`. The function overrides the `reset` function
    by calling `reset`, which was defined in the previous layer with `self.env.reset`,
    then takes a fire action by calling `self.env.step(1)` and an environment-dependent
    action, `self.env.step(2)`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`FireResetEnv`继承自Gym的`Wrapper`类。然后，在初始化时，通过`env.unwrapped`解包环境，检查`fire`动作的可用性。该函数通过调用在前一层中定义的`self.env.reset`来重写`reset`函数，接着通过调用`self.env.step(1)`执行一个开火动作，并执行一个依赖于环境的动作`self.env.step(2)`。
- en: '`WrapFrame` has a similar definition:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`WrapFrame`具有类似的定义：'
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This time, `WarpFrame` inherits the properties from `gym.ObservationWrapper`
    and creates a `Box` space with values between 0 and 255 and with the shape 84
    x 84\. When `observation()` is called, it converts the RGB frames into grayscale
    and resizes the images to the chosen shape.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，`WarpFrame`继承自`gym.ObservationWrapper`，并创建了一个维度为84 x 84、值范围在0到255之间的`Box`空间。当调用`observation()`时，它将RGB帧转换为灰度图像，并将图像调整为所选形状。
- en: 'We can then create a function, `make_env`, to apply every wrapper to an environment:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以创建一个函数`make_env`，将每个包装器应用到环境中：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The only preprocessing step that is missing is the scaling of the frame. We'll
    take care of scaling immediately before giving the observation frame as input
    to the neural network. This is because `FrameStack` uses a particular memory-efficient
    array called a lazy array, which is lost whenever scaling is applied as a wrapper.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一缺失的预处理步骤是帧的缩放。我们将在将观察帧作为输入传递给神经网络之前处理缩放。这是因为`FrameStack`使用了一种特定的节省内存的数组，称为懒数组，每当缩放作为包装器应用时，这种数组就会丢失。
- en: DQN implementation
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN实现
- en: Though DQN is a pretty simple algorithm, it requires particular attention when
    it comes to its implementation and design choices. This algorithm, like every
    other deep RL algorithm, is not easy to debug and tune. Therefore, throughout
    this book, we'll give you some techniques and suggestions for how to do this.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管DQN是一个相当简单的算法，但在实现和设计选择时需要特别注意。与其他深度强化学习算法一样，这个算法并不容易调试和调整。因此，在本书中，我们将为您提供一些技巧和建议，帮助您完成这些任务。
- en: 'The DQN code contains four main components:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: DQN代码包含四个主要组件：
- en: DNNs
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNNs
- en: An experienced buffer
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个经验缓冲区
- en: A computational graph
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个计算图
- en: A training (and evaluation) loop
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个训练（和评估）循环
- en: The code, as usual, is written in Python and TensorFlow, and we'll use TensorBoard
    to visualize the training and the performance of the algorithm.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 代码，像往常一样，使用Python和TensorFlow编写，我们将使用TensorBoard来可视化训练过程和算法的性能。
- en: All the code is available in this book's GitHub repository. Make sure to check
    it out there. We don't provide the implementation of some simpler functions here
    to avoid weighing down the code.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码都可以在本书的GitHub仓库中找到。务必去那里查看。为了避免使代码过于冗长，我们没有提供一些简单函数的实现。
- en: 'Let''s immediately jump into the implementation by importing the required libraries:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们立即开始实现，导入所需的库：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`atari_wrappers` includes the `make_env` function we defined previously.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`atari_wrappers`包含我们之前定义的`make_env`函数。'
- en: DNNs
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNNs
- en: 'The DNN architecture is as follows (the components are built in sequential
    order):'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: DNN架构如下（组件按顺序构建）：
- en: A convolution of 16 filters of dimension 8 x 8 with 4 strides and rectifier
    nonlinearity.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个16个过滤器的卷积，过滤器维度为8 x 8，步幅为4，采用整流非线性激活函数。
- en: A convolution of 32 filters of dimension 4 x 4 with 2 strides and rectifier
    nonlinearity.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个32个过滤器的卷积，过滤器维度为4 x 4，步幅为2，采用整流非线性激活函数。
- en: A convolution of 32 filters of dimension 3 x 3 with 1 strides and rectifier
    nonlinearity.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个32个过滤器的卷积，过滤器维度为3 x 3，步幅为1，采用整流非线性激活函数。
- en: A dense layer of 128 units and ReLU activation.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个128单元的全连接层，使用ReLU激活函数。
- en: A dense layer with a number of units equal to the actions that are allowed in
    the environment and a linear activation.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个全连接层，其单元数等于环境中允许的动作数量，并采用线性激活函数。
- en: 'In `cnn`, we define the first three convolutional layers, while in `fnn`, we
    define the last two dense layers:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在`cnn`中，我们定义了前三个卷积层，而在`fnn`中，我们定义了最后两个全连接层：
- en: '[PRE6]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code, `hidden_layers` is a list of integer values. In our implementation,
    this is `hidden_layers=[128]`. On the other hand, `output_layer` is the number
    of agent actions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`hidden_layers`是一个整数值的列表。在我们的实现中，它是`hidden_layers=[128]`。另一方面，`output_layer`表示代理动作的数量。
- en: 'In `qnet`,the CNN and FNN layers are connected with a layer that flattens the
    2D output of the CNN:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在`qnet`中，CNN和FNN层通过一个层连接，该层将CNN的二维输出展平：
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The deep neural network is now fully defined. All we need to do is connect it
    to the main computational graph.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络现在已经完全定义。我们需要做的就是将其连接到主计算图。
- en: The experienced buffer
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经验缓冲区
- en: 'The experienced buffer is a class of the `ExperienceBuffer` type and stores
    a queue of type **FIFO** (**First In**, **First Out**) for each of the following
    components: observation, reward, action, next observation, and done. FIFO means
    that once it reaches the maximum capacity specified by `maxlen`, it discards the
    elements starting from the oldest one. In our implementation, the capacity is
    `buffer_size`:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 经验缓冲区是`ExperienceBuffer`类型的类，并存储一个**FIFO**（**先进先出**）类型的队列，用于存储以下每个组件：观察、奖励、动作、下一观察和完成。FIFO意味着，一旦达到`maxlen`指定的最大容量，它将从最旧的元素开始丢弃。在我们的实现中，容量为`buffer_size`：
- en: '[PRE8]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `ExperienceBuffer` class also manages the sampling of mini-batches, which
    are used to train the neural network. These are uniformly sampled from the buffer
    and have a predefined `batch_size` size:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExperienceBuffer`类还管理小批量的采样，这些小批量用于训练神经网络。这些小批量是从缓冲区中均匀采样的，并且具有预定义的`batch_size`大小：'
- en: '[PRE9]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Lastly, we override the `_len` method to provide the length of the buffers.
    Note that because every buffer is the same size as the others, we only return
    the length of `self.obs_buf`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们重写了`_len`方法，以提供缓冲区的长度。请注意，由于每个缓冲区的大小相同，我们只返回`self.obs_buf`的长度：
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The computational graph and training loop
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算图和训练循环
- en: 'The core of the algorithm, namely the computational graph and the training
    (and evaluation) loop, is implemented in the `DQN` function, which takes the name
    of the environment and all the other hyperparameters as arguments:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的核心，即计算图和训练（及评估）循环，已经在`DQN`函数中实现，该函数将环境的名称和所有其他超参数作为参数：
- en: '[PRE11]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the first few lines of the preceding code, two environments are created:
    one for training and one for testing. Moreover, `gym.wrappers.Monitor` is a Gym
    wrapper that saves the games of an environment in video format, while `video_callable`
    is a function parameter that establishes how often the videos are saved, which
    in this case is every 20 episodes.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码的前几行中，创建了两个环境：一个用于训练，另一个用于测试。此外，`gym.wrappers.Monitor`是一个Gym封装器，它将环境的游戏保存为视频格式，而`video_callable`是一个函数参数，用于确定视频保存的频率，在这种情况下是每20集保存一次。
- en: 'Then, we can reset the TensorFlow graph and create placeholders for the observations,
    the actions, and the target values. This is done with the following lines of code:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以重置 TensorFlow 图并为观察值、动作和目标值创建占位符。这可以通过以下代码行完成：
- en: '[PRE12]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we can create a target and an online network by calling the `qnet` function
    that we defined previously. Because the target network has to update itself sometimes
    and take the parameters of the online network, we create an operation called `update_target_op`,
    which assigns every variable of the online network to the target network. This
    assignment is done by the TensorFlow `assign` method. `tf.group`, on the other
    hand, aggregates every element of the `update_target` list as a single operation.
    The implementation is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过调用之前定义的`qnet`函数来创建目标网络和在线网络。由于目标网络需要时不时更新自己并采用在线网络的参数，因此我们创建了一个名为`update_target_op`的操作，用于将在线网络的每个变量分配给目标网络。这一分配通过
    TensorFlow 的`assign`方法完成。另一方面，`tf.group`将`update_target`列表中的每个元素聚合为一个操作。实现如下：
- en: '[PRE13]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we have defined the placeholder that's created the deep neural network
    and defined the target update operation, all that remains is to define the loss
    function. The loss function is ![](img/a831e7fc-45db-4469-a046-032ff17c8b3d.png) (or,
    equivalently, (5.5)). It requires the target values, ![](img/88f8b378-b9c3-4831-a529-36fd4cdca94e.png),
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经定义了创建深度神经网络的占位符，并定义了目标更新操作，剩下的就是定义损失函数。损失函数是![](img/a831e7fc-45db-4469-a046-032ff17c8b3d.png)（或者等价地，(5.5)）。它需要目标值，![](img/88f8b378-b9c3-4831-a529-36fd4cdca94e.png)，
- en: 'computed as they are in formula (5.6), which are passed through the `y_ph` placeholder
    and the Q-values of the online network, ![](img/4c3d2453-13e9-4a90-a0f6-4f70c3870bce.png).
    A Q-value is dependent on the action, ![](img/75e3b146-8d3a-44a9-a4d5-e68839ae0317.png),
    but since the online network outputs a value for each action, we have to find
    a way to retrieve only the Q-value of ![](img/75e3b146-8d3a-44a9-a4d5-e68839ae0317.png) while
    discarding the other action-values. This operation can be achieved by using a
    one-hot encoding of the action, ![](img/75e3b146-8d3a-44a9-a4d5-e68839ae0317.png), and
    then multiplying it by the output of the online network. For example, if there
    are five possible actions and ![](img/2fd1422c-a331-4a0b-8e78-ea40cba7152e.png),
    then the one-hot encoding will be ![](img/45cdd9fa-4046-428e-beb0-144c41ed74eb.png). Then,
    supposing that the network outputs ![](img/5fa70caf-4f92-4cdc-9301-28ecd0a56d0e.png),
    the results of the multiplication with the one-hot encoding will be ![](img/3e063c4c-06eb-47aa-9f2a-a80a98004442.png).
    After, the q-value is obtained by summing this vector. The result will be ![](img/8578bb25-bfaf-444e-b319-116517194c6c.png).
    All of this is done in the following three lines of code:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 按照公式(5.6)计算，它们通过`y_ph`占位符和在线网络的Q值传递，![](img/4c3d2453-13e9-4a90-a0f6-4f70c3870bce.png)。Q值依赖于动作，![](img/75e3b146-8d3a-44a9-a4d5-e68839ae0317.png)，但由于在线网络为每个动作输出一个值，我们必须找到一种方法来仅提取![](img/75e3b146-8d3a-44a9-a4d5-e68839ae0317.png)的Q值，同时丢弃其他动作值。这个操作可以通过使用动作的独热编码（one-hot
    encoding）来实现，![](img/75e3b146-8d3a-44a9-a4d5-e68839ae0317.png)，然后将其与在线网络的输出相乘。例如，如果有五个可能的动作且![](img/2fd1422c-a331-4a0b-8e78-ea40cba7152e.png)，那么独热编码将是![](img/45cdd9fa-4046-428e-beb0-144c41ed74eb.png)。然后，假设网络输出![](img/5fa70caf-4f92-4cdc-9301-28ecd0a56d0e.png)，与独热编码的乘积结果将是![](img/3e063c4c-06eb-47aa-9f2a-a80a98004442.png)。接下来，通过对这个向量求和，获得q值。结果将是![](img/8578bb25-bfaf-444e-b319-116517194c6c.png)。这一切在以下三行代码中完成：
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To minimize the loss function we just defined, we will use Adam, a variant
    of SGD:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化我们刚才定义的损失函数，我们将使用Adam，这是SGD的一种变体：
- en: '[PRE15]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This concludes the creation of the computation graph. Before going through
    the main DQN cycle, we have to prepare everything so that we can save the scalars
    and the histograms. By doing this, we will be able to visualize them later in
    TensorBoard:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了计算图的创建。在进入主要的DQN循环之前，我们需要准备好一切，以便可以保存标量和直方图。这样做后，我们就能够在TensorBoard中查看它们：
- en: '[PRE16]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Everything is quite self-explanatory. The only things that you may question
    are the `mr_v` and `ml_v` variables. These are variables we want to track with
    TensorBoard. However, because they aren't defined internally by the computation
    graph, we have to declare them separately and assign them in `session.run` later.
    `FileWriter` is created with a unique name and associated with the default graph.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都非常直观。唯一可能让你疑问的是`mr_v`和`ml_v`这两个变量。它们是我们希望通过TensorBoard跟踪的变量。然而，由于它们没有在计算图中内部定义，我们必须单独声明它们，并在稍后的`session.run`中赋值。`FileWriter`是用一个唯一的名字创建的，并与默认图相关联。
- en: 'We can now define the `agent_op` function that computes the forward pass on
    a scaled observation. The observation has already passed through the preprocessing
    pipeline (built in the environment with the wrappers), but we left the scaling
    aside:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义`agent_op`函数，用于计算缩放后的观察值的前向传递。该观察值已经通过了预处理管道（在环境中通过包装器构建），但我们将缩放操作放到了一边：
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, the session is created, the variables are initialized, and the environment
    is reset:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，创建会话，初始化变量，并重置环境：
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The next move involves instantiating the replay buffer, updating the target
    network so that it has the same parameters as the online network, and initializing
    the decay rate with `eps_decay`. The policy for the epsilon decay is the same
    as the one that was adopted in the DQN paper. A decay rate has been chosen so
    that, when it''s applied linearly to the `eps` variable, it reaches a terminal
    value, `end_explor`, in about `explor_steps` steps. For example, if you want to
    decrease from 1.0 to 0.1 in 1,000 steps, you have to decrement the variable by
    a value equal to ![](img/951d4743-6522-4fda-a081-4e6b31ab9563.png) on each step.
    All of this is accomplished in the following lines of code:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤包括实例化回放缓冲区，更新目标网络使其具有与在线网络相同的参数，并用`eps_decay`初始化衰减率。epsilon衰减的策略与DQN论文中采用的相同。衰减率的选择使得当它线性应用于`eps`变量时，它将在大约`explor_steps`步内达到终值`end_explor`。例如，如果你想在1000步内从1.0降低到0.1，那么你必须在每一步减小一个等于![](img/951d4743-6522-4fda-a081-4e6b31ab9563.png)的值。所有这一切都在以下几行代码中完成：
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As you may recall, the training loop comprises two inner cycles: the first
    iterates across the epochs while the other iterates across each transition of
    the epoch. The first part of the innermost cycle is quite standard. It selects an
    action following an ![](img/d28bbae0-9b7d-46ba-b063-afde075d2465.png)-greedy behavior
    policy that uses the online network, takes a step in the environment, adds the
    new transition to the buffer, and finally, updates the variables:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所记得，训练循环由两个内层循环组成：第一个循环遍历训练周期，第二个循环遍历每个周期的过渡阶段。最内层循环的第一部分是相当标准的。它根据使用在线网络的![](img/d28bbae0-9b7d-46ba-b063-afde075d2465.png)-贪婪行为策略选择一个动作，执行一步环境操作，将新的过渡加入缓冲区，并最终更新变量：
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding code, `obs` takes the value of the next observation and the
    cumulative game reward is incremented.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`obs`获取下一个观察值，累计的游戏奖励会增加。
- en: 'Then, in the same cycle, `eps` is decayed and if some of the conditions are
    met, it trains the online network. These conditions make sure that the buffer
    has reached a minimal size and that the neural network is trained only once every
    `update_freq` steps. To train the online network, first, a minibatch is sampled from
    the buffer and the target values are calculated. Then, the session is run to minimize
    the loss function, `v_loss`, which feeds the dictionary with the target values,
    the actions, and the observations of the minibatch. While the session is running,
    it also returns `v_loss` and `scalar_summary` for statistics purposes. `scalar_summary`
    is then added to `file_writer` to be saved in the TensorBoard logging file. Finally,
    every `update_target_net` epochs, the target network is updated. A summary with
    the mean losses is also run and added to the TensorBoard logging file. All of
    this is done by the following snippet of code:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在相同的循环中，`eps`会衰减，如果满足某些条件，则训练在线网络。这些条件确保缓冲区已达到最小大小，并且神经网络仅在每`update_freq`步时训练一次。为了训练在线网络，首先，从缓冲区中采样一个小批量，并计算目标值。然后，运行会话以最小化损失函数`v_loss`，并将目标值、动作和小批量的观察值传入字典。在会话运行期间，它还会返回`v_loss`和`scalar_summary`以供统计使用。接着，`scalar_summary`会被添加到`file_writer`中保存到TensorBoard日志文件。最后，每经过`update_target_net`个周期，目标网络会更新。一个包含平均损失的总结也会被运行并添加到TensorBoard日志文件中。所有这些操作通过以下代码片段完成：
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'When an epoch terminates, the environment is reset, the total reward of the
    game is appended to `batch_rew`, and the latter is set to zero. Moreover, every
    `test_frequency` epochs, the agent is tested for 10 games, and the statistics
    are added to `file_writer`. At the end of the training, the environments and the
    writer are closed. The code is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个周期结束时，环境会被重置，游戏的总奖励被添加到`batch_rew`中，并将后者设为零。此外，每隔`test_frequency`个周期，代理会在10局游戏中进行测试，统计数据会被添加到`file_writer`中。在训练结束时，环境和写入器将被关闭。代码如下：
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'That''s it. We can now call the `DQN` function with the name of the Gym environment
    and all the hyperparameters:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样。我们现在可以调用`DQN`函数，传入Gym环境的名称和所有超参数：
- en: '[PRE23]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: There's one last note before reporting the results. The environment that's being
    used here isn't the default version of `Pong-v0` but a modified version of it.
    The reason for this is that in the regular version, each action is performed 2,
    3, or 4 times where this number is sampled uniformly. But because we want to skip
    a fixed number of times, we opted for the version without the built-in skip feature, `NoFrameskip`,
    and added the custom `MaxAndSkipEnv` wrapper.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在报告结果之前，还有最后一点需要说明。这里使用的环境并非默认版本的`Pong-v0`，而是其修改版。之所以这样做，是因为在常规版本中，每个动作会执行2、3或4次，且这个次数是均匀采样的。但由于我们希望跳过固定次数，因此选择了没有内置跳过功能的版本`NoFrameskip`，并添加了自定义的`MaxAndSkipEnv`包装器。
- en: Results
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: Evaluating the progress of an RL algorithm is very challenging. The most obvious
    way to do this is to keep track of its end goal; that is, monitoring the total
    reward that's accumulated during the epochs. This is a good metric. However, training
    the average reward can be very noisy due to changes in the weights. This leads
    to large changes in the distribution of the state that's being visited.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 评估强化学习算法的进展非常具有挑战性。最明显的方式是追踪其最终目标；即，监控在周期中积累的总奖励。这是一个很好的指标。然而，由于权重的变化，训练过程中的平均奖励可能非常嘈杂。这会导致状态分布发生较大的变化。
- en: 'For these reasons, we evaluated the algorithm on 10 test games every 20 training
    epochs and kept track of the average of the total (non-discounted) reward that
    was accumulated throughout the games. Moreover, because of the determinism of
    the environment, we tested the agent on an ![](img/3e83ab04-37b2-43ca-961d-e824a111b62b.png)-greedy
    policy (with ![](img/ddd88359-b1c5-411d-a2a1-3141eceef4d2.png)) so that we have
    a more robust evaluation. The scalar summary is called `test_rew`. You can see
    it in TensorBoard if you access the directory where the logs have been saved,
    and execute the following command:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些原因，我们在每 20 次训练周期后评估算法在 10 个测试游戏上的表现，并跟踪整个游戏过程中累积的总（非折扣）奖励的平均值。此外，由于环境的确定性，我们使用 ![](img/3e83ab04-37b2-43ca-961d-e824a111b62b.png)-贪婪策略（与 ![](img/ddd88359-b1c5-411d-a2a1-3141eceef4d2.png)）进行测试，以便进行更稳健的评估。这个标量总结称为
    `test_rew`。你可以通过访问保存日志的目录并执行以下命令，在 TensorBoard 中查看它：
- en: '[PRE24]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The plot, which should be similar to yours (if you run the DQN code), is shown
    in the following diagram. The x axis represents the number of steps. You can see
    that it reaches a steady score of ![](img/18045cf6-a26c-47a2-86c0-b0b4a12d89be.png) after
    a linear increase in the first 250,000 steps and a more significant growth in
    the next 300,000 steps:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图示应该与你的图示类似（如果你运行了 DQN 代码）。它显示在下图中，x 轴表示步数。你可以看到，在前 250,000 步的线性增加之后，Q 值在接下来的
    300,000 步内出现了更显著的增长，最终达到了一个稳定的得分 ![](img/18045cf6-a26c-47a2-86c0-b0b4a12d89be.png)：
- en: '![](img/ca751dbb-bf2d-4ab7-b33d-844493dd0249.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca751dbb-bf2d-4ab7-b33d-844493dd0249.png)'
- en: Figure 5.4\. A plot of the mean total reward across 10 games. The x axis represents
    the number of steps
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4\. 10 场游戏的平均总奖励图示。x 轴表示步数
- en: Pong is a relatively simple task to complete. In fact, our algorithm has been
    trained on around 1.1 million steps, whereas in the DQN paper, all the algorithms
    were trained on 200 million steps.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Pong 是一个相对简单的任务。事实上，我们的算法已经训练了大约 1.1 百万步，而在 DQN 论文中，所有的算法都是在 2 亿步上进行训练的。
- en: 'An alternative way to evaluate the algorithm involves the estimated action-values. Indeed,
    the estimated action-values are a valuable metric because they measure the belief
    of the quality of the state-action pair. Unfortunately, this option is not optimal
    as some algorithms tend to overestimate the Q-values, as we will soon learn. Despite
    this, we tracked it during training. The plot is visible in the following diagram
    and, as we expected, the Q-value increases throughout the training in a similar
    way to the plot in the preceding diagram:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 评估算法的另一种方式是通过估计的动作值。事实上，估计的动作值是一个有价值的指标，因为它们衡量了状态-动作对的质量的信念。不幸的是，这个选项并不最优，因为一些算法倾向于高估
    Q 值，正如我们很快会学到的那样。尽管如此，我们在训练过程中仍然追踪了它。图示如下所示，正如我们预期的那样，Q 值在训练过程中不断增加，与前面的图示相似：
- en: '![](img/ef3a10c0-7a08-4540-8edc-131c01d9ec65.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef3a10c0-7a08-4540-8edc-131c01d9ec65.png)'
- en: Figure 5.5\. A plot of the estimated training Q-values. The x axis represents
    the number of steps
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5\. 估计的训练 Q 值图示。x 轴表示步数
- en: 'Another important plot, shown in the following diagram, shows the loss function
    through time. It''s not as useful as in supervised learning as the target values
    aren''t the ground truth, but it can always provide a good insight into the quality
    of the model:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的图示，如下图所示，展示了损失函数随时间变化的情况。尽管它不像监督学习中那么有用，因为目标值并不是实际的真实值，但它仍然可以提供对模型质量的良好洞察：
- en: '![](img/d2981b99-48bc-461e-b12f-4d3e28b04cd3.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d2981b99-48bc-461e-b12f-4d3e28b04cd3.png)'
- en: Figure 5.6\. A plot of the loss function
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6\. 损失函数的图示
- en: DQN variations
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN 变体
- en: Following the amazing results of DQN, many researchers have studied it and come
    up with integrations and changes to improve its stability, efficiency, and performance.
    In this section, we will present three of these improved algorithms, explain the
    idea and solution behind them, and provide their implementation. The first is
    Double DQN or DDQN, which deals with the over-estimation problem we mentioned
    in the DQN algorithm. The second is Dueling DQN, which decouples the Q-value function
    in a state value function and an action-state advantage value function. The third
    is n-step DQN, an old idea taken from TD algorithms, which spaces the step length
    between one-step learning and MC learning.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在DQN取得惊人效果之后，许多研究人员对其进行了研究，并提出了整合与改进，以提高其稳定性、效率和性能。在这一部分，我们将介绍三种改进的算法，解释它们背后的思想和解决方案，并提供它们的实现。第一种是双重DQN（Double
    DQN或DDQN），它解决了我们在DQN算法中提到的过度估计问题。第二种是对抗DQN（Dueling DQN），它将Q值函数解耦为状态值函数和动作-状态优势值函数。第三种是n步DQN（n-step
    DQN），它借鉴了TD算法中的一个旧思想，用于扩展一步学习与蒙特卡洛学习之间的步长。
- en: Double DQN
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双重DQN
- en: 'The over-estimation of the Q-values in Q-learning algorithms is a well-known
    problem. The cause of this is the max operator, which over-estimates the actual
    maximum estimated values. To comprehend this problem, let''s assume that we have
    noisy estimates with a mean of 0 but a variance different from 0, as shown in
    the following illustration. Despite the fact that, asymptotically, the average
    value is 0, the max function will always return values greater than 0:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习算法中的Q值过度估计是一个众所周知的问题。其原因是最大值操作符，它会过度估计实际的最大估计值。为了理解这个问题，假设我们有噪声估计，其均值为0，但方差不为0，如下图所示。尽管从渐近上看，平均值为0，但max函数总是会返回大于0的值：
- en: '![](img/68325277-2d84-44bc-b051-22073024f483.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68325277-2d84-44bc-b051-22073024f483.png)'
- en: Figure 5.7\. Six values sampled from a normal distribution with a mean of 0
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7. 从均值为0的正态分布中采样的六个值
- en: In Q-learning, this over-estimation is not a real problem until the higher values
    are uniformly distributed. If, however, the over-estimation is not uniform and
    the error differs from states and actions, this over-estimation negatively affects
    the DQN algorithm, which degrades the resulting policy.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在Q学习中，过度估计通常不是一个实际问题，直到较高的值均匀分布。然而，如果过度估计并不均匀，并且误差在状态和动作间存在差异，这种过度估计会对DQN算法产生负面影响，从而降低生成的策略的质量。
- en: 'To address this problem, in the paper [*Deep Reinforcement Learning with Double
    Q-learning*](https://arxiv.org/abs/1509.06461), the authors suggest using two
    different estimators (that is, two neural networks): one for the action selection
    and one for the Q-values estimation. But instead of using two different neural
    networks and increasing the complexity, the paper proposes the use of the online
    network to choose the best action with the max operation, and the use of the target
    network to compute its Q-values. With this solution, the target value, ![](img/e442e4a2-47fe-4157-ba8b-6712adf8ce79.png),
    will change from being as follows for standard Q-learning:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，在论文[*深度强化学习与双Q学习*](https://arxiv.org/abs/1509.06461)中，作者建议使用两种不同的估计器（即两个人工神经网络）：一个用于动作选择，另一个用于Q值估计。但论文并没有使用两个不同的神经网络来增加复杂度，而是提出了使用在线网络来通过最大操作选择最佳动作，使用目标网络来计算其Q值。通过这个解决方案，目标值，![](img/e442e4a2-47fe-4157-ba8b-6712adf8ce79.png)，将会改变，从标准Q学习中的如下形式：
- en: '![](img/adee62f0-f82e-408b-8ad3-7712a033f964.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/adee62f0-f82e-408b-8ad3-7712a033f964.png)'
- en: 'Now, it''s as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，它是如下所示：
- en: '![](img/7c3cadf2-80f3-4ec7-817f-a2e26452e0ba.png) (5.7)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c3cadf2-80f3-4ec7-817f-a2e26452e0ba.png) (5.7)'
- en: This uncoupled version significantly reduces over-estimation problems and improves
    the stability of the algorithm.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解耦版本显著减少了过度估计问题，并提高了算法的稳定性。
- en: DDQN implementation
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DDQN实现
- en: 'From an implementation perspective, the only change to make in order to implement
    DDQN is in the training phase. You just need to replace the following lines of
    code in the DDQN implementation itself:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 从实现的角度来看，实现DDQN的唯一变化是在训练阶段。只需要在DDQN实现中替换以下代码行：
- en: '[PRE25]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Replace this with the following code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码替换：
- en: '[PRE26]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Here, `double_q_target_values` is a function that computes (5.7) for each transition
    of the minibatch.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`double_q_target_values`是一个计算每个小批量转换的(5.7)的函数。
- en: Results
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'To see if DQN actually overestimates the Q-values in respect to DDQN, we reported
    the Q-value plot in the following diagram. We also included the results of DQN
    (the orange line) so that we have a direct comparison between the two algorithms:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证DQN是否真的高估了Q值，相对于DDQN，我们在下图中展示了Q值的图表。我们还包括了DQN（橙线）的结果，以便进行两种算法的直接比较：
- en: '![](img/159e4130-41ea-412a-81a0-053dc4d830f7.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/159e4130-41ea-412a-81a0-053dc4d830f7.png)'
- en: Figure 5.8\. A plot of the estimated training Q-values. The DDQN values are
    plotted in blue and the DQN values are plotted in orange. The x axis represents
    the number of steps
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8. 估计训练Q值的图表。DDQN的值用蓝色表示，DQN的值用橙色表示。x轴表示步骤数量
- en: 'The performance of both DDQN (the blue line) and DQN (the orange line), which
    are represented by the average reward of the test games, is as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: DDQN（蓝线）和DQN（橙线）的表现，通过测试游戏的平均奖励表示，结果如下：
- en: For all the color references mentioned in the chapter, please refer to the color
    images bundle at [http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 关于本章节中提到的所有颜色参考，请参考[颜色图片包](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf)。
- en: '![](img/e28c62d0-1fad-4ade-82ac-871bcd133565.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e28c62d0-1fad-4ade-82ac-871bcd133565.png)'
- en: Figure 5.9\. A plot of the mean test rewards. The DDQN values are plotted in
    blue and the DQN values are plotted in orange. The x axis represents the number
    of steps
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9. 平均测试奖励的图表。DDQN的值用蓝色表示，DQN的值用橙色表示。x轴表示步骤数量
- en: As we expected, the Q-values are always smaller in DDQN than in DQN, meaning
    that the latter was actually over-estimating the values. Nonetheless, the performance
    on the test games doesn't seem to be impacted, meaning that those over-estimations
    were probably not hurting the performance of the algorithm. However, be aware
    that we only tested the algorithm on Pong. The effectiveness of an algorithm shouldn't
    be evaluated in a single environment. In fact, in the paper, the authors apply
    it to all 57 ALE games and reported that DDQN not only yields more accurate value
    estimates but leads to much higher scores on several games.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，DDQN中的Q值始终小于DQN，这意味着后者实际上在高估Q值。尽管如此，测试游戏的表现似乎并未受到影响，这意味着这些高估可能并未对算法的性能造成伤害。然而，请注意，我们只在Pong游戏上测试了该算法。一个算法的有效性不应该仅在单一环境中进行评估。事实上，在论文中，作者将其应用于所有57个ALE游戏，并报告称DDQN不仅能提供更准确的值估计，还在多个游戏中获得了更高的得分。
- en: Dueling DQN
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗DQN
- en: 'In the paper *Dueling Network Architectures for Deep Reinforcement Learning*
    ([https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581)), a novel
    neural network architecture with two separate estimators was proposed: one for
    the state value function and the other for the state-action advantage value function.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文《Dueling Network Architectures for Deep Reinforcement Learning》([https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581))中，提出了一种新型的神经网络架构，包含两个独立的估计器：一个用于状态值函数，另一个用于状态-动作优势值函数。
- en: 'The advantage function is used everywhere in RL and is defined as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 优势函数在强化学习中无处不在，其定义如下：
- en: '![](img/64a74303-d3e7-43c5-8947-1545ae6515f9.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64a74303-d3e7-43c5-8947-1545ae6515f9.png)'
- en: The advantage function tells us the improvement of an action, ![](img/6f4dc9fe-7775-44c2-977d-dadd3a3638d7.png), compared
    to the average action in a given state, ![](img/4f1e190b-1fd1-4e81-a14d-2b65d067daf1.png). Thus,
    if ![](img/f25da7b2-1b41-4b79-8c00-c07aeb8e8e68.png) is a positive value, this
    means that the action, ![](img/f6f612ff-260f-44df-bc3d-9f6a1f847c4c.png), is better
    then the average action in the state, ![](img/3046c2b4-7e2a-4d4c-bf7f-2045d93b9a47.png). On
    the contrary, if ![](img/3ab03d6c-2d75-41c1-a225-da18a7ccf1eb.png) is a negative
    value, this means that ![](img/b6cfea02-e054-4d3c-bdcf-86bced05dc87.png) is worse
    than the average action in the state, ![](img/dd4b1b99-d383-470b-86a8-6e807cf54980.png).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 优势函数告诉我们某个动作的改善程度，![](img/6f4dc9fe-7775-44c2-977d-dadd3a3638d7.png)，与在给定状态下的平均动作，![](img/4f1e190b-1fd1-4e81-a14d-2b65d067daf1.png)相比。因此，如果![](img/f25da7b2-1b41-4b79-8c00-c07aeb8e8e68.png)是正值，这意味着该动作，![](img/f6f612ff-260f-44df-bc3d-9f6a1f847c4c.png)，优于该状态下的平均动作，![](img/3046c2b4-7e2a-4d4c-bf7f-2045d93b9a47.png)。相反，如果![](img/3ab03d6c-2d75-41c1-a225-da18a7ccf1eb.png)是负值，这意味着![](img/b6cfea02-e054-4d3c-bdcf-86bced05dc87.png)比该状态下的平均动作，![](img/dd4b1b99-d383-470b-86a8-6e807cf54980.png)更差。
- en: 'Thus, estimating the value function and the advantage function separately,
    as done in the paper, allows us to rebuild the Q-function, like so:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，像论文中所做的那样分别估计价值函数和优势函数，使我们能够重建 Q 函数，如下所示：
- en: '![](img/666038cd-facd-41ff-9d71-c791f381e9fd.png) (5.8)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/666038cd-facd-41ff-9d71-c791f381e9fd.png) (5.8)'
- en: Here, the mean of the advantage has been added to increase the stability of
    the DQN.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，已添加优势的均值，以提高 DQN 的稳定性。
- en: 'The architecture of Dueling DQN consists of two heads (or streams): one for
    the value function and one for the advantage function, all while sharing a common
    convolutional module. The authors reported that this architecture can learn which
    states are or are not valuable, without having to learn the absolute value of
    each action in a state. They tested this new architecture on the Atari games and
    obtained considerable improvements regarding their overall performance.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗 DQN 的架构由两个头（或流）组成：一个用于价值函数，一个用于优势函数，且都共享一个公共的卷积模块。作者报告称，这种架构可以学习哪些状态是有价值的，哪些不是，而无需学习每个动作在某一状态下的绝对值。他们在
    Atari 游戏上测试了这一新架构，并在整体性能方面取得了显著的提升。
- en: Dueling DQN implementation
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗 DQN 实现
- en: 'One of the benefits of this architecture and of formula (5.8) is that it doesn''t
    impose any changes on the underlying RL algorithm. The only changes are in the
    construction of the Q-network. Thus, we can replace `qnet` with the `dueling_qnet`
    function, which can be implemented as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构以及公式 (5.8) 的一个好处是，它不会对底层的强化学习算法施加任何更改。唯一的变化是在 Q 网络的构建上。因此，我们可以用 `dueling_qnet`
    函数替换 `qnet`，其实现方法如下：
- en: '[PRE27]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Two forward neural networks are created: one with only one output (for the
    value function) and one with as many outputs as the actions of the agent (for
    the state-dependent action advantage function). The last line returns formula
    (5.8).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了两个前馈神经网络：一个只有一个输出（用于价值函数），另一个具有与智能体动作数相同的输出（用于状态依赖的动作优势函数）。最后一行返回公式 (5.8)。
- en: Results
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: 'The results of the test rewards, as shown in the following diagram, are promising,
    proving a clear benefit in the use of a dueling architecture:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，测试奖励的结果很有前景，证明了使用对抗架构的明显好处：
- en: '![](img/ecc126ee-5f2f-4add-8071-4079f554f481.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ecc126ee-5f2f-4add-8071-4079f554f481.png)'
- en: Figure 5.10\. A plot of the test rewards. The dueling DQN values are plotted
    in red and the DQN values are plotted in orange. The x axis represents the number
    of steps
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10。测试奖励的绘图。对抗 DQN 的值以红色绘制，DQN 的值以橙色绘制。x 轴表示步数。
- en: N-step DQN
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N 步 DQN
- en: The idea behind n-step DQN is old and comes from the shift between temporal
    difference learning and Monte Carlo learning. These algorithms, which were introduced
    in [Chapter 4](6364d5e9-40e9-4991-a88f-1ce5b0f0a6e9.xhtml), *Q-Learning and SARSA
    Applications*, are at the opposite extremes of a common spectrum. TD learning
    learns from a single step, while MC learns from the complete trajectory. TD learning
    exhibits a minimal variance but a maximal bias, where as MC exhibits high variance
    but a minimal bias. The variance-bias problem can be balanced using an n-step
    return. An n-step return is a return computed after `n` steps. TD learning can
    be viewed as a 0-step return while MC can be viewed as a ![](img/d58022ac-8a1b-4abc-9928-056dba5f837b.png)-step
    return.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: n 步 DQN 背后的思想很久以前就有了，它源于时间差分学习（TD）与蒙特卡罗学习（MC）之间的转变。这些算法在 [第 4 章](6364d5e9-40e9-4991-a88f-1ce5b0f0a6e9.xhtml)中介绍过，*Q-learning
    和 SARSA 应用*，是一个常见谱系的两个极端。TD 学习从单一步骤中学习，而 MC 从完整的轨迹中学习。TD 学习表现出最小的方差，但最大偏差，而 MC
    则表现出较大的方差，但最小偏差。方差-偏差问题可以通过使用 n 步返回来平衡。n 步返回是经过 `n` 步后计算的回报。TD 学习可以视为 0 步返回，而
    MC 可以视为一个 ![](img/d58022ac-8a1b-4abc-9928-056dba5f837b.png)-步返回。
- en: 'With the n-step return, we can update the target value, as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 n 步返回，我们可以更新目标值，具体如下：
- en: '![](img/a3b14f9a-3c6d-470a-a97b-c39b5b2c7083.png) (5.9)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a3b14f9a-3c6d-470a-a97b-c39b5b2c7083.png) (5.9)'
- en: Here, ![](img/8d681a90-0f01-4b60-9ad8-7e758e2fe930.png) is the number of steps.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/8d681a90-0f01-4b60-9ad8-7e758e2fe930.png) 是步数。
- en: An n-step return is like looking ahead `n` steps, but in practice, as it's impossible
    to actually look into the future, it's done in the opposite way, that is, by computing
    the ![](img/1735a426-eb5c-412e-9704-af27fbe339ca.png) value of n-steps ago. This
    leads to values that are only available at time ![](img/e0bc4154-db21-474c-ad06-9e3669b80c3c.png),
    delaying the learning process.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: n 步回报就像向前看 `n` 步，但实际上，由于不可能真正预见未来，它是以相反的方式进行的，即通过计算 n 步前的值。这导致值仅在时间 `t - n`
    处可用，从而延迟了学习过程。
- en: The main advantage of this approach is that the target values are less biased
    and this can lead to faster learning. An important problem that arises is that
    the target values that are calculated in this way are correct, but only when the
    learning is on-policy (DQN is off-policy). This is because formula (5.9) assumes
    that the policy that the agent will follow for the next n-steps is the same policy that
    collected the experience. There are some ways to adjust for the off-policy case,
    but they are generally complicated to implement and the best general practice
    is just to keep a small `n` and ignore the problem.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要优点在于目标值偏差较小，这可能导致更快的学习。一个重要的问题是，这种方式计算的目标值只有在学习是 on-policy 时才正确（DQN 是
    off-policy）。这是因为公式（5.9）假设代理将在接下来的 n 步中遵循相同的策略来收集经验。虽然有一些方法可以调整 off-policy 情况，但一般来说实现起来比较复杂，最好的一般实践是保持一个小的
    `n` 并忽略这个问题。
- en: Implementation
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施
- en: To implement n-step DQN, only a few changes in the buffer are required. When
    sampling from the buffer, the n-step reward, the n-step next state, and the n-step
    done flag have to be returned. We will not provide the implementation here as
    it is quite simple but you can look at it in the code provided in this book's
    GitHub repository. The code to support n-step return is in the `MultiStepExperienceBuffer` class.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现 n 步 DQN，只需要对缓冲区进行少量更改。在从缓冲区采样时，需要返回 n 步回报、n 步下一个状态和 n 步完成标志。我们不会在此提供实现，因为它相当简单，但你可以在本书的
    GitHub 存储库中查看代码。支持 n 步回报的代码在 `MultiStepExperienceBuffer` 类中。
- en: Results
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结果
- en: For off-policy algorithms (such as DQN), n-step learning works well with small
    values of `n`. In DQN, it has been shown that the algorithm works well with values
    of `n` between 2 and 4, leading to improvements in a wide range of Atari games.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离线策略算法（如 DQN），n 步学习在小 `n` 值时效果显著。在 DQN 中，已经证明算法在 2 到 4 的 n 值范围内表现良好，从而改进了广泛的
    Atari 游戏。
- en: 'In the following graph, the results of our implementation are visible. We tested
    DQN with a three-step return. From the results, we can see that it requires more
    time before taking off. Afterward, it has a steeper learning curve but with an
    overall similar learning curve compared to DQN:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们的实现结果可见。我们使用三步回报测试了 DQN。从结果来看，在起飞之前需要更多时间。之后，它的学习曲线更陡，但与 DQN 整体类似：
- en: '![](img/4826853c-0cde-4d5d-9c63-6c4edb58eeda.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4826853c-0cde-4d5d-9c63-6c4edb58eeda.png)'
- en: Figure 5.11\. A plot of the mean test total reward. The three-step DQN values
    are plotted in violet and the DQN values are plotted in orange. The x axis represents
    the number of steps
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11. 测试平均总奖励的图表。三步 DQN 值以紫色绘制，DQN 值以橙色绘制。x 轴表示步数
- en: Summary
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we went further into RL algorithms and talked about how these
    can be combined with function approximators so that RL can be applied to a broader
    variety of problems. Specifically, we described how function approximation and
    deep neural networks can be used in Q-learning and the instabilities that derive
    from it. We demonstrated that, in practice, deep neural networks cannot be combined
    with Q-learning without any modifications.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们进一步探讨了强化学习算法，并讨论了如何将其与函数逼近器结合，以便将强化学习应用于更广泛的问题。具体来说，我们描述了如何在 Q-learning
    中使用函数逼近和深度神经网络，以及由此引起的不稳定性。我们证明了，在实践中，深度神经网络不能在不进行任何修改的情况下与 Q-learning 结合使用。
- en: The first algorithm that was able to use deep neural networks in combination
    with Q-learning was DQN. It integrates two key ingredients to stabilize learning
    and control complex tasks such as Atari 2600 games. The two ingredients are the
    replay buffer, which is used to store the old experience, and a separate target
    network, which is updated less frequently than the online network. The former
    is employed to exploit the off-policy quality of Q-learning so that it can learn
    from the experiences of different policies (in this case, old policies) and to
    sample more i.i.d mini-batches from a larger pool of data to perform stochastic
    gradient descent. The latter is introduced to stabilize the target values and
    reduce the non-stationarity problem.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 能够将深度神经网络与Q-learning结合使用的第一个算法是DQN。它集成了两个关键要素，以稳定学习并控制复杂任务，例如Atari 2600游戏。这两个要素分别是回放缓冲区，用于存储旧经验，以及更新频率低于在线网络的独立目标网络。前者用于利用Q-learning的离政策性质，以便它可以从不同策略（在这种情况下是旧策略）的经验中学习，并从更大的数据池中采样更多的独立同分布（i.i.d）小批量数据，以执行随机梯度下降。后者的引入是为了稳定目标值并减少非平稳性问题。
- en: After this formal introduction to DQN, we implemented it and tested it on Pong,
    an Atari game. Moreover, we showed more practical aspects of the algorithm, such
    as the preprocessing pipeline and the wrappers. Following the publication of DQN,
    many other variations have been introduced to improve the algorithm and overcome
    its instabilities. We took a look at them and implemented three variations, namely
    Double DQN, Dueling DQN, and n-step DQN. Despite the fact that, in this chapter,
    we applied these algorithms exclusively to Atari games, they can be employed in
    many real-world problems.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在对DQN进行了正式介绍后，我们将其实现并在Pong（一款Atari游戏）上进行了测试。此外，我们还展示了该算法的更多实际方面，例如预处理管道和包装器。在DQN发布之后，许多其他变种被提出以改进该算法并克服其不稳定性。我们研究了这些变种，并实现了三种变体，即Double
    DQN、Dueling DQN和n-step DQN。尽管在本章中，我们仅将这些算法应用于Atari游戏，但它们可以用于解决许多现实世界中的问题。
- en: In the next chapter, we'll introduce a different category of deep RL algorithms
    called policy gradient algorithms. These are on-policy and, as we'll soon see,
    they have some very important and unique characteristics that widen their applicability
    to a larger set of problems.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍另一类深度强化学习算法——策略梯度算法。这些算法是基于策略的，正如我们很快将看到的，它们具有一些非常重要和独特的特性，使得它们能够应用于更广泛的问题领域。
- en: Questions
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the cause of the deadly triad problem?
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 致命三合一问题的原因是什么？
- en: How does DQN overcome instabilities?
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DQN是如何克服不稳定性的？
- en: What's the moving target problem?
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是移动目标问题？
- en: How is the moving target problem mitigated in DQN?
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DQN是如何缓解移动目标问题的？
- en: What's the optimization procedure that's used in DQN?
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DQN中使用的优化过程是什么？
- en: What's the definition of a state-action advantage value function?
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 状态-动作优势值函数的定义是什么？
- en: Further reading
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: For a comprehensive tutorial regarding OpenAI Gym wrappers, read the following
    article: [https://hub.packtpub.com/openai-gym-environments-wrappers-and-monitors-tutorial/](https://hub.packtpub.com/openai-gym-environments-wrappers-and-monitors-tutorial/).
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关OpenAI Gym包装器的全面教程，请阅读以下文章：[https://hub.packtpub.com/openai-gym-environments-wrappers-and-monitors-tutorial/](https://hub.packtpub.com/openai-gym-environments-wrappers-and-monitors-tutorial/)。
- en: For the original *Rainbow* paper, go to [https://arxiv.org/abs/1710.02298](https://arxiv.org/abs/1710.02298).
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要查看原始的*Rainbow*论文，请访问[https://arxiv.org/abs/1710.02298](https://arxiv.org/abs/1710.02298)。
