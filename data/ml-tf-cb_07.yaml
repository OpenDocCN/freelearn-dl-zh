- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Predicting with Tabular Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用表格数据进行预测
- en: Most of the available data that can be easily found is not composed of images
    or text documents, but it is instead made of relational tables, each one possibly
    containing numbers, dates, and short text, which can be all joined together. This
    is because of the widespread adoption of database applications based on the relational
    paradigm (data tables that can be combined together by the values of certain columns
    that act as joining keys). These tables are the main source of tabular data nowadays
    and because of that, there are certain challenges.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 目前可以轻松找到的大多数数据并不是由图像或文本文件组成，而是由关系型表格构成，每个表格可能包含数字、日期和简短文本，这些数据可以结合在一起。这是因为基于关系模型（可以通过某些列的值作为连接键将数据表结合在一起）的数据库应用得到了广泛采用。如今，这些表格是表格数据的主要来源，因此也带来了一些挑战。
- en: 'Here are the challenges commonly faced by **Deep Neural Networks** (**DNNs**)
    when applied to tabular data:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是应用**深度神经网络**（**DNNs**）于表格数据时常见的挑战：
- en: Mixed features data types
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混合特征数据类型
- en: Data in a sparse format (there are more zeros than non-zero data), which is
    not the best for a DNN converging to an optimum solution
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据以稀疏格式呈现（零值数据多于非零值数据），这对于DNN找到最优解并不是最理想的情况。
- en: No state-of-the-art architecture has emerged yet, there are just some various
    best practices
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前尚未出现最先进的架构，只有一些不同的最佳实践。
- en: Less data is available for a single problem than in a usual image recognition
    problem
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对单一问题可用的数据比常见的图像识别问题要少。
- en: There's suspicion from non-technical people because DNNs are less interpretable
    than simpler machine learning algorithms for tabular data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非技术人员会产生怀疑，因为与更简单的机器学习算法相比，DNN在表格数据上的可解释性较差。
- en: Often, DNNs are not the best-in-class solution for tabular data, because gradient
    boosting solutions (such as LightGBM, XGBoost, and CatBoost) might perform better
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常情况下，DNN并不是表格数据的最佳解决方案，因为梯度提升方法（如LightGBM、XGBoost和CatBoost）可能表现得更好。
- en: 'Even if these challenges seem quite difficult, simply do not get discouraged.
    The challenges when applying DNNs to tabular data are certainly serious, but on
    the other hand, so are the opportunities. Andrew Ng, Adjunct Professor at Stanford
    University and deep learning expert ([https://www.coursera.org/instructor/andrewng](https://www.coursera.org/instructor/andrewng)),
    recently stated: *"Deep learning has seen tremendous adoption in consumer Internet
    companies with a huge number of users and thus big data, but for it to break into
    other industries where datasets sizes are smaller, we now need better techniques
    for small data."*'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这些挑战看起来相当困难，也请不要气馁。将深度神经网络（DNN）应用于表格数据时的挑战确实是严峻的，但另一方面，机会也同样巨大。斯坦福大学兼职教授、深度学习专家[安德鲁·吴](https://www.coursera.org/instructor/andrewng)最近表示：“**深度学习在用户众多的消费互联网公司中得到了极大的应用，因而产生了大量数据，但要突破到其他行业，那里的数据集较小，我们现在需要针对小数据的更好技术。**”
- en: 'In this chapter, we introduce you to some of the best recipes for handling
    small, tabular data with TensorFlow. In doing so, we will be using TensorFlow,
    Keras, and two specialized machine learning packages: pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/))
    and scikit-learn ([https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html)).
    In the previous chapters, we often used TensorFlow Datasets ([https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets))
    and specialized layers for feature columns ([https://www.tensorflow.org/api_docs/python/tf/feature_column](https://www.tensorflow.org/api_docs/python/tf/feature_column)).
    We could have reused them for this chapter, but then we would have missed some
    interesting transformations that only scikit-learn can provide, and doing cross-validation
    would have proved difficult.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向你介绍一些处理小型表格数据的最佳方法，使用的是TensorFlow。在此过程中，我们将使用TensorFlow、Keras，以及两个专门的机器学习包：pandas（[https://pandas.pydata.org/](https://pandas.pydata.org/)）和scikit-learn（[https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html)）。在前几章中，我们经常使用TensorFlow
    Datasets（[https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)）和专门用于特征列的层（[https://www.tensorflow.org/api_docs/python/tf/feature_column](https://www.tensorflow.org/api_docs/python/tf/feature_column)）。我们本可以在这一章中重复使用它们，但那样我们就会错过一些只有scikit-learn才能提供的有趣转换，而且交叉验证也会变得困难。
- en: Consider moreover that using scikit-learn makes sense if you are comparing the
    performance of different algorithms on a problem, and you need to standardize
    a data preparation pipeline not only for the TensorFlow model but also for other
    more classical machine learning and statistical models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑到使用 scikit-learn 如果你在比较不同算法在某个问题上的表现，并且需要标准化一个数据准备管道，不仅适用于 TensorFlow 模型，还适用于其他更经典的机器学习和统计模型。
- en: 'In order to install pandas and scikit-learn (if you are using Anaconda, they
    should already be on your system), please follow these guidelines:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安装 pandas 和 scikit-learn（如果你使用的是 Anaconda，它们应该已经安装在你的系统中），请按照以下指南操作：
- en: 'For pandas: [https://pandas.pydata.org/docs/getting_started/install.html](https://pandas.pydata.org/docs/getting_started/install.html)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于 pandas: [https://pandas.pydata.org/docs/getting_started/install.html](https://pandas.pydata.org/docs/getting_started/install.html)'
- en: 'For scikit-learn: [https://scikit-learn.org/stable/install.html](https://scikit-learn.org/stable/install.html)'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对于 scikit-learn: [https://scikit-learn.org/stable/install.html](https://scikit-learn.org/stable/install.html)'
- en: In this chapter, we will deal with a series of recipes focused on learning from
    tabular data, which is data arranged in the form of a table, where rows represent
    observations and columns are the observed values for each feature.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论一系列专注于从表格数据中学习的方法，这些数据以表格形式组织，行表示观察结果，列表示每个特征的观察值。
- en: Tabular data is the common input data for most machine learning algorithms,
    but not a usual one for DNNs, since DNNs excel with other kinds of data, such
    as images and text.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 表格数据是大多数机器学习算法的常见输入数据，但对于 DNN 来说并不常见，因为 DNN 在处理其他类型的数据（如图像和文本）时表现更好。
- en: Recipes for deep learning for tabular data require solving problems, such as
    data heterogeneity, which are not mainstream, and they require using many common
    machine learning strategies, such as cross-validation, which are not currently
    implemented in TensorFlow.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 针对表格数据的深度学习方法需要解决一些问题，比如数据异质性，这些问题并非主流，并且需要使用许多常见的机器学习策略，比如交叉验证，而这些策略在 TensorFlow
    中目前尚未实现。
- en: 'By the end of this chapter, you should have knowledge of the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你应当掌握以下内容：
- en: Processing numerical data
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数值数据
- en: Processing dates
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理日期数据
- en: Processing categorical data
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理类别数据
- en: Processing ordinal data
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理有序数据
- en: Processing high-cardinality categorical data
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理高卡特性类别数据
- en: Wrapping up all the processing
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完成所有处理步骤
- en: Setting up a data generator
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置数据生成器
- en: Creating custom activations for tabular data
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为表格数据创建自定义激活函数
- en: Running a test run on a difficult problem
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对难题进行测试运行
- en: Let's start immediately with how to deal with numerical data. You will be amazed
    by how these recipes can be effective with many tabular data problems.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即开始学习如何处理数值数据。你会惊讶于这些方法在许多表格数据问题中的有效性。
- en: Processing numerical data
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理数值数据
- en: 'We will start by preparing numerical data. You have numerical data when:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从准备数值数据开始。你有数值数据时，数据是：
- en: Your data is expressed by a floating number
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的数据是浮动数字表示的
- en: Your data is an integer and it has a certain number of unique values (otherwise
    if there are only few values in sequence, you are dealing with an ordinal variable,
    such as a ranking)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的数据是整数，并且有一定数量的唯一值（否则，如果只有少数值按顺序排列，你正在处理的是有序变量，如排名）。
- en: Your integer data is not representing a class or label (otherwise you are dealing
    with a categorical variable)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的整数数据并不代表一个类别或标签（否则你正在处理一个**类别变量**）。
- en: 'When working with numerical data, a few situations may affect the performance
    of a DNN when processing such data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数值数据时，有几种情况可能会影响 DNN 处理数据时的性能：
- en: Missing data (NULL or NaN values, or even INF values) that will prevent your
    DNN from working at all
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失数据（NULL 或 NaN 值，甚至 INF 值）会导致 DNN 完全无法工作。
- en: Constant values that will make computations slower and interfere with the bias
    each neuron in the network is already providing
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常数值会导致计算变慢，并干扰每个神经元已经提供的偏差。
- en: Skewed distribution
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏斜分布
- en: Non-standardized data, especially data with extreme values
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非标准化数据，尤其是带有极端值的数据
- en: Before feeding numerical data to your neural network, you have to be sure that
    all these issues have been properly dealt with or you may encounter errors or
    a learning process that will not work.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在将数值数据输入神经网络之前，你必须确保所有这些问题已经得到妥善处理，否则你可能会遇到错误或学习过程无法正常进行。
- en: Getting ready
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In order to address all the potential issues, we will mostly be using specialized
    functions from scikit-learn. Before starting our recipe, we will import them into
    our environment:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决所有潜在问题，我们将主要使用来自 scikit-learn 的专门函数。在开始我们的配方之前，我们将把它们导入到环境中：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In order to test our recipe we will use a simple 3x4 table, with some columns
    containing NaN values, and some constant columns that contain no NaN values:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的配方，我们将使用一个简单的 3x4 表格，其中一些列包含 NaN 值，还有一些常数列不包含 NaN 值：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How to do it…
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Our recipe will build a scikit-learn pipeline, based on our indications relative
    to:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的配方将基于我们相对于以下内容的指示构建一个 scikit-learn 管道：
- en: The minimum acceptable variance for a feature to be kept, or you may just be
    introducing unwanted constants into your network that may hinder the learning
    process (the `variance_threshold` parameter)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个特征被保留的最小可接受方差，否则你可能会将不需要的常量引入网络，进而阻碍学习过程（`variance_threshold` 参数）
- en: What to use as a baseline strategy for imputing missing values (the `imputer`
    parameter, by default set to replace missing values with the mean of the feature)
    so that your input matrix will be completed and matrix multiplication will be
    possible (the basic computation in a neural network)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用作填充缺失值的基准策略是什么（`imputer` 参数，默认设置为用特征的均值替代缺失值），以便完成输入矩阵，使矩阵乘法成为可能（这是神经网络中的基本计算）
- en: Whether we should use a more sophisticated imputation strategy based on the
    missing values of all the numeric data (the `multivariate_imputer` parameter),
    because sometimes points are not missing at random and other variables may supply the
    information you need for a proper estimation
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否应该使用基于所有数值数据缺失值的更复杂的填充策略（`multivariate_imputer` 参数），因为有时候数据点并非随机缺失，其他变量可能提供你所需的信息以进行正确的估计
- en: Whether to add a binary feature denoting for each feature where the missing
    values were, which is a good strategy because you often find information also
    on missing patterns (the `add_indicator` parameter)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否添加一个二进制特征，标示每个特征的缺失值位置，这是一个好的策略，因为你通常可以通过缺失模式找到有用的信息（`add_indicator` 参数）
- en: Whether to transform the distribution of variables in order to force them to
    resemble a symmetric distribution (`quantile_transformer` parameter, set to `normal`
    by default) because your network will learn better from symmetrical data distributions
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否对变量的分布进行转换，以强制它们接近对称分布（`quantile_transformer` 参数，默认为 `normal`），因为网络将从对称的数据分布中学习得更好
- en: Whether we should rescale our output based on the statistical normalization,
    that is, dividing by the standard deviation after having removed the mean (the
    `scaler` parameter, set to `True` by default)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否应该基于统计归一化来重新缩放输出，即在去除均值后除以标准差（`scaler` 参数，默认为 `True`）
- en: 'Now, bearing all that in mind, let''s build our pipeline as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑到这些，让我们按照以下方式构建我们的管道：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now create our numerical pipeline by specifying our transformation preferences:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过指定我们的转换偏好来创建我们的数值管道：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can immediately try our new function on the example by applying first the
    `fit` and then the `transform` methods:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以立即在示例上尝试我们新的函数，首先应用`fit`方法，然后应用`transform`方法：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is the resulting output NumPy array:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成的输出 NumPy 数组：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see all the original data has been completely transformed, with all
    the missing values replaced.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，所有原始数据已经完全转换，所有缺失值都已被替换。
- en: How it works…
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'As we previously mentioned, we are using scikit-learn for comparability with
    other machine learning solutions and because there are a few unique scikit-learn
    functions involved in the building of this recipe:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用 scikit-learn 以便与其他机器学习解决方案进行比较，并且因为在构建此配方时涉及到一些独特的 scikit-learn 函数：
- en: '`VarianceThreshold` ([https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html))'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VarianceThreshold` ([https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html))'
- en: '`IterativeImputer` ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html))'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IterativeImputer` ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html))'
- en: '`SimpleImputer` ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html))'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SimpleImputer` ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html))'
- en: '`QuantileTransformer` ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html))'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`QuantileTransformer` ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html))'
- en: '`StandardScaler` ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html))'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StandardScaler` ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html))'
- en: '`Pipeline` ([https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html))'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pipeline` ([https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html))'
- en: For each function, you will find a link pointing to the scikit-learn documentation
    with detailed information on how the function works. It is paramount to explain
    why the scikit-learn approach is so important for this recipe (and for the others
    you will find in this chapter).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个函数，您会找到一个指向 scikit-learn 文档的链接，提供关于该函数如何工作的详细信息。解释为什么 scikit-learn 方法对于这个配方（以及本章中您将找到的其他配方）如此重要至关重要。
- en: When processing images or text, you usually don't need to define specific processes
    for respectively training and testing data. That's because you apply deterministic
    transformations to both. For instance, in images, you just divide the pixels'
    values by 255 in order to normalize them.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 处理图像或文本时，通常不需要为训练数据和测试数据定义特定的处理过程。这是因为您对两者应用的是确定性的转换。例如，在图像处理中，您只需将像素值除以 255
    来进行标准化。
- en: However, with tabular data you need transformations that are more complex and
    not deterministic at all because they involve learning and memorizing specific
    parameters. For instance, when imputing a missing value for a feature by using
    the mean, you have first to compute the mean from your training data. Then you
    have to reuse that exact value for any other new data you will apply the same
    imputation on (it won't work to compute again the mean on any new data because
    it could be from a slightly different distribution and may not match what your
    DNN has learned).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于表格数据，您需要更复杂的转换，并且这些转换完全不是确定性的，因为它们涉及学习和记住特定的参数。例如，当使用均值填充一个特征的缺失值时，您首先需要从训练数据中计算均值。然后，您必须对任何新的数据应用相同的填充值（不能重新计算新数据的均值，因为这些新数据可能来自稍有不同的分布，可能与您的
    DNN 所学的值不匹配）。
- en: All of this involves keeping track of many parameters learned from your training
    data. scikit-learn may help you in that because when you use the `fit` method,
    it learns and stores away all the parameters it derives from training data. Using
    the `transform` method, you will apply the transformations with the learned-by-fit
    parameters on any new data (or on the very same training data).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都涉及跟踪从训练数据中学习到的许多参数。scikit-learn 可以帮助您，因为当您使用 `fit` 方法时，它会学习并存储从训练数据中推导出的所有参数。使用
    `transform` 方法，您将使用通过 fit 学到的参数对任何新数据（或相同的训练数据）应用转换。
- en: There's more…
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'scikit-learn functions usually return a NumPy array. It is not a problem to
    label the resulting array using the input columns, if no further feature creation
    has occurred. Unfortunately, this is not the case because of the transformation
    pipeline we created:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 函数通常返回一个 NumPy 数组。如果没有进行进一步的特征创建，使用输入列标记返回的数组并没有问题。不幸的是，由于我们创建的转换管道，这种情况并不成立：
- en: The variance threshold will remove features that are not useful
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方差阈值将移除无用的特征
- en: Missing value imputation will create missing binary indicators
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值填充将创建缺失的二进制指示器
- en: 'We can actually explore this by inspecting the fitted pipeline and finding
    out which columns have been removed and what has been added from the original
    data. A function can be created to do just that for us automatically:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以通过检查拟合的管道，找出哪些列已经被移除，哪些内容已经从原始数据中添加。可以创建一个函数来自动执行这一操作：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When we try it on our example:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在示例中尝试时：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We obtain a pandas index containing the remaining columns and the binary indicators
    (denoted by the name of the original feature and the `_missing` suffix):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得一个包含剩余列和二进制指示符的 pandas 索引（由原始特征的名称和 `_missing` 后缀表示）：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Keeping track of your columns as you transform them can help you when you need
    to debug your transformed data and if you need to explain how your DNN works using
    tools such as shap ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))
    or lime ([https://github.com/marcotcr/lime](https://github.com/marcotcr/lime)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪你在转换列时的操作，可以帮助你在需要调试转换后的数据时，以及在使用像 shap ([https://github.com/slundberg/shap](https://github.com/slundberg/shap))
    或 lime ([https://github.com/marcotcr/lime](https://github.com/marcotcr/lime))
    等工具解释 DNN 工作原理时提供帮助。
- en: This recipe should suffice for all your needs with regard to numerical data.
    Now let's proceed to examine dates and times.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方应该能满足你关于数值数据的所有需求。现在，让我们继续探索日期和时间。
- en: Processing dates
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理日期
- en: Dates are common in databases and, especially when processing the forecasting
    of future estimates (such as in sales forecasting), they can prove indispensable.
    Neural networks cannot process dates as they are, since they are often expressed
    as strings. Hence, you have to transform them by separating their numerical elements,
    and once you have split a date into its components, you have just numbers that
    can easily be dealt with by any neural network. Certain time elements, however,
    are cyclical (days, months, hours, days of the week) and lower and higher numbers
    are actually contiguous. Consequently, you need to use sine and cosine functions,
    which will render such cyclical numbers in a format that can be both understood
    and correctly interpreted by a DNN.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 日期在数据库中非常常见，特别是在处理未来估算的预测（如销售预测）时，它们显得不可或缺。神经网络无法直接处理日期，因为它们通常以字符串形式表示。因此，你必须通过分离日期的数值元素来转换它们，一旦你将日期拆分成它的组成部分，你就得到了一些数字，任何神经网络都能轻松处理这些数字。然而，某些时间元素是周期性的（例如天、月、小时、星期几），低数字和高数字实际上是相邻的。因此，你需要使用正弦和余弦函数，这将使这些周期性数字以一种
    DNN 可以理解和正确解释的格式呈现。
- en: Getting ready
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Since we need to code a class operating using the fit/transform operations
    that are typical of scikit-learn, we import the `BaseEstimator` and `TransformerMixin`
    classes from scikit-learn to inherit from. This inheritance will help us to make
    our recipe perfectly compatible with all other functions from scikit-learn:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要编写一个使用 fit/transform 操作的类，这是 scikit-learn 中典型的操作方式，我们从 scikit-learn 导入
    `BaseEstimator` 和 `TransformerMixin` 类进行继承。这个继承将帮助我们使我们的代码与 scikit-learn 的所有其他函数完美兼容：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'For testing purposes, we also prepare an example dataset of dates in string
    form, using the day/month/year format:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试，我们还准备了一个包含日期的字符串形式的示例数据集，采用日/月/年格式：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The provided example is quite short and simplistic, but it should illustrate
    all the relevant points as we work through it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的示例非常简短和简单，但它应该能说明我们在处理时的所有相关要点。
- en: How to do it…
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: This time we will design a class of our own, `DateProcessor`. After being initialized,
    instances of this class can pick a pandas DataFrame and filter and process each
    date into a new DataFrame that can be processed by a DNN.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次我们将设计我们自己的类 `DateProcessor`。实例化该类后，它可以选择一个 pandas DataFrame，并将每个日期筛选并处理成一个新的
    DataFrame，供 DNN 处理。
- en: 'The process focuses on one date at a time, extracting days, days of the week,
    months, and years (additionally also hours and minutes), and transforming all
    cyclical time measures using sine and cosine transformations:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程一次处理一个日期，提取日期、星期几、月份和年份（另外，还包括小时和分钟），并使用正弦和余弦变换对所有周期性时间进行转换：
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now that we have scripted down the recipe in the form of a `DateProcessor` class,
    let's explore more of its inner workings.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将配方编写成 `DateProcessor` 类的形式，让我们进一步探索它的内部工作原理。
- en: How it works…
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: The key to the entire class is the transformation operated by the pandas `to_datetime`
    function, which turns any string representing a date into the `datetime64[ns]`
    type.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 整个类的关键是通过 pandas `to_datetime` 函数进行的转换，它将任何表示日期的字符串转换为 `datetime64[ns]` 类型。
- en: '`to_datetime` works because you provide it a template (the `format` parameter)
    for turning strings into dates. For a complete guide on how to define such a template,
    please visit [https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_datetime`之所以有效，是因为你提供了一个模板（`format`参数），用来将字符串转换为日期。有关如何定义该模板的完整指南，请访问[https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior)。'
- en: 'When you need to fit and transform your data, the class will automatically
    process all the dates into the right format and furthermore, perform transformations
    using sine and cosine functions:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要拟合和转换数据时，类会自动将所有日期处理成正确的格式，并进一步使用正弦和余弦函数进行变换：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Some resulting transformations will be obvious, but some others related to cyclical
    time may appear puzzling. Let's spend a bit of time exploring how they work and
    why.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一些变换结果是显而易见的，但有些与周期时间相关的变换可能会让人感到困惑。我们花点时间探讨它们如何运作以及为什么会这样。
- en: There's more…
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'The class doesn''t return the raw extraction of time elements such as the hour,
    the minute, or the day, but it transforms them using first a sine, then a cosine
    transformation. Let''s plot how it transforms the 24 hours in order to get an
    better understanding of this recipe:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类不会返回时间元素的原始提取结果，如小时、分钟或天数，而是首先通过正弦变换，然后是余弦变换来转换它们。让我们绘制出它如何转换24小时，以便更好地理解这个方法：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is the plot that you will obtain:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你将得到的图：
- en: '![](img/B16254_07_01.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_07_01.png)'
- en: 'Figure 7.1: Plotting of hourly time after sine and cosine transformations'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：正弦和余弦变换后的小时时间绘图
- en: 'From the plot, we can figure out how the start and end of the day coincide,
    thus closing the time cycle. Each transformation also returns the same value for
    a couple of different hours. That''s the reason why we should pick both sine and
    cosine together; if you use both, each point in time has a different tuple of
    sine and cosine values, and so you can detect exactly where you are in continuous
    time. This can also be explained visually by plotting the sine and cosine values
    in a scatter plot:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中，我们可以看出一天的开始和结束是如何重合的，从而完成时间周期的闭环。每个变换也会返回相同的值，对于几个不同的小时来说都是如此。这就是为什么我们应该同时使用正弦和余弦的原因；如果你同时使用这两者，每个时间点都会有一对不同的正弦和余弦值，因此你可以精确地检测你在连续时间中的位置。通过将正弦和余弦值绘制成散点图，这一点也可以通过可视化方式进行解释：
- en: '[PRE14]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here is the result:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![](img/B16254_07_02.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_07_02.png)'
- en: 'Figure 7.2: Combining the sine and cosine transformations of hourly time into
    a scatter plot'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：将小时时间的正弦和余弦变换结合到散点图中
- en: As in a clock, the hours are plotted in a circle, each one separate and distinct,
    yet in full cyclical continuity.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 就像时钟一样，小时被绘制在一个圆圈中，每个小时是分开的、独立的，但却是完整的周期连续体。
- en: Processing categorical data
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理分类数据
- en: 'Strings usually represent categorical data in tabular data. Each unique value
    in a categorical feature represents a quality that refers to the example we are
    examining (hence, we consider this information to be **qualitative** whereas numerical
    information is **quantitative**). In statistical terms, each unique value is called
    a **level** and the categorical feature is called a **factor**. Sometimes you
    can find numeric codes used as categorical (identifiers), when the qualitative
    information has been previously encoded into numbers, but the way to deal with
    them doesn''t change: the information is in numeric values but it should be treated
    as categorical.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串通常在表格数据中表示分类数据。分类特征中的每个唯一值代表一个质量，指的是我们正在检查的示例（因此，我们认为这些信息是**定性的**，而数字信息是**定量的**）。从统计学角度看，每个唯一值被称为**水平**，而分类特征被称为**因子**。有时你会看到用于分类的数字代码（标识符），当定性信息之前已被编码为数字时，但处理方式不会改变：信息是数字值，但应该当作分类数据处理。
- en: Since you don't know how each unique value in a categorical feature is related
    to every other value present in the feature (if you jump ahead and group values
    together or order them you are basically expressing a hypothesis you have about
    the data), you can treat each of them as a value in itself. Hence, you can derive
    the idea of creating a binary feature from each unique categorical value. This
    process is called one-hot encoding and it is the most common data processing approach
    that can make categorical data usable by DNNs and other machine learning algorithms.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你不知道每个分类特征中每个唯一值与特征中其他值的关系（如果你提前将值分组或排序，实际上是在表达你对数据的假设），你可以将每个唯一值视为一个独立的值。因此，你可以从每个唯一的分类值中推导出创建二进制特征的想法。这个过程被称为独热编码（one-hot
    encoding），它是最常见的数据处理方法，可以使得分类数据适用于深度神经网络（DNN）和其他机器学习算法。
- en: 'For instance, if you have a categorical variable containing the unique values
    of red, blue, and green, you can turn it into three distinct binary variables,
    each one representing uniquely a single value, as represented in the following
    schema:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，如果你有一个分类变量，其中包含红色、蓝色和绿色这些唯一值，你可以将它转化为三个独立的二进制变量，每个变量唯一地表示一个值，如下图所示：
- en: '![](img/B16254_07_03.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_07_03.png)'
- en: This approach presents a problem for DNNs, though. When your categorical variable
    has too many levels (conventionally more than 255), the resulting binary derived
    features are not only too numerous, making your dataset huge, but also carry little
    information since most of the numerical values will be just zeros (we call this
    situation **sparse data**). Sparse data is somewhat problematic for a DNN because
    backpropagation doesn't work optimally when there are too many zeros in the data
    since the lack of information can stop the signal from making a meaningful difference
    as it's sent back through the network.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法对 DNN 来说存在一个问题。当你的分类变量拥有过多的取值（通常超过 255 个）时，所得到的二进制派生特征不仅数量过多，导致数据集变得庞大，而且还携带很少的信息，因为大部分数值将是零（我们称这种情况为**稀疏数据**）。稀疏数据对
    DNN 有一定问题，因为当数据中有过多的零时，反向传播效果不好，因为信息的缺失会导致信号在网络中传播时无法产生有意义的变化。
- en: We therefore distinguish between low-cardinality and high-cardinality categorical
    variables, on the basis of their number of unique values and process (by one-hot
    encoding) only those categorical variables that we consider to have low cardinality
    (conventionally if there are less than 255 unique values, but you can choose a
    lower threshold, such as 64, 32, or even 24).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们根据分类变量的唯一值数量区分低基数和高基数的分类变量，并且只对我们认为基数较低的分类变量进行处理（通常如果唯一值少于 255 个，我们认为它是低基数，但你也可以选择一个更低的阈值，比如
    64、32，甚至是 24）。
- en: Getting ready
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We import the scikit-learn function for one-hot encoding and we prepare a simple
    example dataset containing categorical data both in string and numerical form:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入 scikit-learn 的独热编码函数，并准备一个简单的示例数据集，其中包含字符串和数值形式的分类数据：
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now we can proceed to the recipe.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续执行方案了。
- en: How to do it…
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: We prepare a class that can turn numbers to strings, so, after using it, every
    numerical categorical feature will be processed in the same way as the strings.
    We then prepare our recipe, which is a scikit-learn pipeline that combines our
    string converter and one-hot encoding together (we won't forget to automatically
    deal with any missing values by converting them into unique values).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备一个可以将数字转换为字符串的类，使用它后，每个数值型分类特征将与字符串一样进行处理。然后，我们准备好我们的方案，这个方案是一个 scikit-learn
    管道，结合了我们的字符串转换器和独热编码（我们不会忘记通过将缺失值转换为唯一值来自动处理缺失数据）。
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Though the code snippet is short, it indeed achieves quite a lot. Let's understand
    how it works.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代码片段很简短，但它实际上实现了很多功能。我们来理解它是如何工作的。
- en: How it works…
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Like the other methods we''ve seen, we just fit and transform our example:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 像我们之前看到的其他方法一样，我们只需要拟合并转换我们的示例：
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Since the returned array will be sparse (a special format for datasets where
    zero values prevail), we can convert it back to our usual NumPy array format using
    the `.todense` method.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于返回的数组是稀疏的（即在数据集中零值占主导的特殊格式），我们可以使用 `.todense` 方法将其转换回我们常用的 NumPy 数组格式。
- en: There's more…
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: 'One-hot encoding, by converting every categorical unique value into a variable
    of its own, produces many new features. In order to label them we have to inspect
    the scikit-learn one-hot encoding instance we used and extract the labels from
    it:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码通过将每个类别的唯一值转换为自己的变量，生成许多新特征。为了给它们打标签，我们必须检查我们使用的 scikit-learn 独热编码实例，并从中提取标签：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'For instance, in our example, now we can figure out what each new feature represents
    by calling the following function:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我们的示例中，现在我们可以通过调用以下函数来弄清楚每个新特征所代表的含义：
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The results provide us indication about both the original feature and the unique
    value represented by the binary variable:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 结果为我们提供了有关原始特征和由二元变量表示的独特值的指示：
- en: '[PRE20]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As you can see, the results provide an indication of both the original feature
    and the unique value represented by the binary variable.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，结果同时提供了原始特征和由二元变量表示的独特值的指示。
- en: Processing ordinal data
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理序数数据
- en: Ordinal data (for instance, rankings or star values in a review) is certainly
    more similar to numerical data than it is to categorical data, yet we have to
    first consider certain differences before dealing with it plainly as a number.
    The problem with categorical data is that you can process it as numerical data,
    but probably the distance between one point and the following one in the scale
    is different than the distance between the following one and the next (technically
    the steps could be different). This is because ordinal data doesn't represent
    quantities, but just ordering. On the other hand, we also treat it as categorical
    data, because categories are independent and we will lose the information implied
    in the ordering. The solution for ordinal data is simply to treat it as both a
    numerical and a categorical variable.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 序数数据（例如，排名或评论中的星级）无疑更像数字数据，而不是类别数据，但我们必须首先考虑一些差异，然后才能将其直接作为数字来处理。类别数据的问题在于，你可以将其作为数字数据处理，但在标度中一个点与下一个点之间的距离，可能不同于下一个点与再下一个点之间的距离（从技术上讲，步骤可能不同）。这是因为序数数据并不代表数量，而只是表示顺序。另一方面，我们也将它视为类别数据，因为类别是独立的，而这样做会丧失顺序中隐含的信息。处理序数数据的解决方案就是将它视为数字和类别变量的组合。
- en: Getting ready
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'First, we need to import the `OrdinalEncoder` function from scikit-learn, which
    will help us in numerically recoding ordinal values, even when they are textual
    (such as the ordinal scale bad, neutral, and good):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从 scikit-learn 导入`OrdinalEncoder`函数，它将帮助我们对序数值进行数字化编码，即使它们是文本形式（例如，序数等级“差、中等、好”）：
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can then prepare our example using two features containing ordinal information
    recorded as strings:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用包含按序信息并记录为字符串的两个特征来准备我们的示例：
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Again, the example is just a toy dataset, but it should allow us to test the
    functionalities demonstrated by this recipe.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，示例只是一个玩具数据集，但它应该能帮助我们测试这个配方所展示的功能。
- en: How to do it…
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'At this point, we can prepare two pipelines. The first pipeline will be working
    on the ordinal data by turning it into ordered numeric (this transformation will
    preserve the ordering of the original feature). The second transformation one-hot
    encodes the ordinal data (a transformation that will preserve the step information
    between ordinal grades, but not their ordering). As with the date transformation
    in the recipe *Processing dates*, earlier in this chapter, just two pieces of
    information derived from your original data will be enough for you to process
    ordinal data in a DNN:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个阶段，我们可以准备两个流水线。第一个流水线将处理序数数据，将其转换为有序的数字（该转换将保留原始特征的顺序）。第二个转换将对序数数据进行独热编码（这种转换将保留序数等级之间的步长信息，但不保留它们的顺序）。正如在本章前面“处理日期”配方中所述，对于你要在
    DNN 中处理序数数据来说，来自原始数据的仅有两部分信息就足够了：
- en: '[PRE23]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As this recipe is mainly composed of a scikit-learn pipeline, it should be quite
    familiar to you. Let's delve into it to understand more of its workings.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个配方主要由 scikit-learn 流水线组成，所以它对你来说应该是相当熟悉的。让我们深入了解它，了解更多的工作原理。
- en: How it works…
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'All you have to do is to operate the transformations separately and then stack
    the resulting vectors together:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你所需要做的就是单独操作这些转换，然后将生成的向量堆叠在一起：
- en: '[PRE24]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here is the result from our example:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们示例的结果：
- en: '[PRE25]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Columns can be easily derived using the `derive_ohe_columns` function that
    we have seen before:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列可以通过之前看到的`derive_ohe_columns`函数轻松推导出来：
- en: '[PRE26]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here is the list containing the transformed column names:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这是包含转换后的列名的列表：
- en: '[PRE27]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: By combining the variables covering the numerical part and the unique values
    of an ordinal variable, we should now be able to utilize all the real information
    from our data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将覆盖数值部分的变量与有序变量的唯一值结合起来，我们现在应该能够利用来自数据的所有真实信息。
- en: Processing high-cardinality categorical data
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理高基数类别数据
- en: When processing high-cardinality categorical features, we can use the previously
    mentioned one-hot encoding strategy. However, we may encounter problems because
    the resulting matrix is too sparse (many zero values), thus preventing our DNN
    from converging to a good solution, or making the dataset unfeasible to handle
    (because sparse matrices made dense can occupy a large amount of memory).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理高基数类别特征时，我们可以使用前面提到的独热编码策略。然而，我们可能会遇到一些问题，因为生成的矩阵过于稀疏（许多零值），从而阻碍了我们的深度神经网络（DNN）收敛到一个好的解，或者使数据集变得不可处理（因为稀疏矩阵变为密集矩阵后可能占用大量内存）。
- en: The best solution instead is to pass them to our DNN as numerically labeled
    features and let a Keras embedding layer take care of them ([https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)).
    An embedding layers is just a matrix of weights that can convert the high-cardinality
    categorical input into a lower-dimensionality numerical output. It is basically
    a weighted linear combination whose weights are optimized to convert categories
    into numbers that can best help the prediction process.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的解决方案是将它们作为数值标签特征传递给我们的深度神经网络（DNN），并让 Keras 嵌入层来处理它们（[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)）。嵌入层实际上是一个权重矩阵，可以将高基数类别输入转换为低维度的数值输出。它本质上是一个加权线性组合，其权重经过优化，以将类别转换为最佳帮助预测过程的数字。
- en: Under the hood, the embedding layer converts your categorical data into one-hot-encoded
    vectors that become the input of a small neural network. The purpose of this small
    neural network is just to mix and combine the inputs together into a smaller output
    layer. The one-hot encoding performed by the layer works only on numerically labeled
    categories (no strings), so it is paramount to transform our high-cardinality
    categorical data in the correct way.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，嵌入层将你的类别数据转换为独热编码向量，这些向量成为一个小型神经网络的输入。这个小型神经网络的目的是将输入混合并组合成一个较小的输出层。该层执行的独热编码仅适用于数值标签的类别（不适用于字符串），因此正确转换我们的高基数类别数据是至关重要的。
- en: The scikit-learn package provides the `LabelEncoder` function as a possible
    solution, but this method presents some problems, because it cannot handle previously
    unseen categories, nor can it properly work in a fit/transform regime. Our recipe
    has to wrap it up and make it suitable for producing the correct input and information
    for a Keras embedding layer.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 包提供了 `LabelEncoder` 函数作为一种可能的解决方案，但这种方法存在一些问题，因为它无法处理之前未见过的类别，也无法在拟合/转换模式下正常工作。我们的方案需要将其封装并使其适用于为
    Keras 嵌入层生成正确的输入和信息。
- en: Getting ready
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this recipe, we will need to redefine the `LabelEncoder` function from scikit-learn
    and make it suitable for a fit/transform process:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们需要重新定义 scikit-learn 中的 `LabelEncoder` 函数，并使其适用于拟合/转换过程：
- en: '[PRE28]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Since we need to simulate a high-cardinality categorical variable, we will
    use random unique values (made of letters and digits) created by a simple script.
    That will allow us to test a larger number of examples, too:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要模拟一个高基数类别变量，我们将使用一个简单脚本创建的随机唯一值（由字母和数字组成）。这将使我们能够测试更多的示例：
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This is the output of our random example generator:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们随机示例生成器的输出：
- en: '![](img/B16254_07_04.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_07_04.png)'
- en: The first column contains a two-letter code, the second uses three letters,
    and the last one four letters.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列包含一个两字母代码，第二列使用三个字母，最后一列使用四个字母。
- en: How to do it…
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点……
- en: 'In this recipe, we will prepare another scikit-learn class. It extends the
    existing `LabelEncoder` function because it automatically handles missing values.
    It keeps records of the mapping between the original categorical values and their
    resulting numeric equivalents and at transformation time, it can handle previously
    unseen categories, labeling them as unknown:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们将准备另一个 scikit-learn 类。它扩展了现有的 `LabelEncoder` 函数，因为它能够自动处理缺失值。它记录了原始类别值与其对应的数值之间的映射关系，并且在转换时，它能够处理之前未见过的类别，将它们标记为未知：
- en: '[PRE30]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Like the other classes we've seen so far, `LEncoder` has a fitting method that
    stores information for future uses and a transform method that applies transformations
    based on the information previously stored after fitting it to the training data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们迄今为止看到的其他类一样，`LEncoder`有一个拟合方法，能够存储信息以供将来使用，还有一个转换方法，基于之前拟合到训练数据时存储的信息应用转换。
- en: How it works…
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'After instancing our label encoder, we simply fit and transform our example,
    turning each categorical feature into a sequence of numeric labels:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化标签编码器后，我们只需拟合并转换我们的示例，将每个类别特征转化为一系列数字标签：
- en: '[PRE31]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: After all the coding to complete the recipe, the execution of this class is
    indeed simple and straightforward.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有编码以实现配方后，这个类的执行确实简单明了。
- en: There's more…
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'In order for the Keras embeddings layers to work properly, we need to specify
    the input size of our high-cardinality categorical variable. By accessing the
    `le.dictionary_size` in our examples, we had `412`, `497`, and `502` distinct
    values in our example variables:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让Keras嵌入层正常工作，我们需要指定高基数类别变量的输入大小。通过访问我们示例中的`le.dictionary_size`，我们在示例变量中有`412`、`497`和`502`个不同的值：
- en: '[PRE32]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In our examples, we had `412`, `497`, and `502` distinct values, respectively,
    in our example variables:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，示例变量分别有`412`、`497`和`502`个不同的值：
- en: '[PRE33]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This number includes the **missing** and **unknown** labels, even if there were
    no missing or unknown elements in the examples we fitted.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数字包括**缺失**和**未知**标签，即使在我们拟合的示例中没有缺失或未知元素。
- en: Wrapping up all the processing
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 完成所有处理
- en: Now that we have completed the recipes relating to processing different kinds
    of tabular data, in this recipe we will be wrapping everything together in a class
    that can easily handle all the fit/transform operations with a pandas DataFrame
    as input and explicit specifications of what columns to process and how.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了处理不同类型表格数据的配方，在本配方中，我们将把所有内容封装到一个类中，这个类可以轻松处理所有的fit/transform操作，输入为pandas
    DataFrame，并明确指定要处理的列以及处理方式。
- en: Getting ready
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备好
- en: 'Since we will combine multiple transformations, we will take advantage of the
    `FeatureUnion` function from scikit-learn, a function that can concatenate them
    together easily:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将结合多个转换，我们将利用scikit-learn的`FeatureUnion`函数，这是一个可以轻松地将它们拼接在一起的函数：
- en: '[PRE34]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'As a testing dataset, we will then simply combine all our previously used test
    data:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 作为测试数据集，我们将简单地合并之前使用过的所有测试数据：
- en: '[PRE35]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As for as our toy dataset, we just combine all the datasets we have used up
    to now.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 至于我们的玩具数据集，我们只需将迄今为止使用过的所有数据集合并在一起。
- en: How to do it…
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到……
- en: 'The wrapper class of this recipe has been split into parts, in order to help
    you to inspect and study the code better. The first part comprises the initialization,
    which effectively incorporates all the recipes we have seen so far in this chapter:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本配方的包装类已拆分为多个部分，以帮助您更好地检查和学习代码。第一部分包含初始化，它有效地整合了本章迄今为止所有看到的配方：
- en: '[PRE36]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: After having recorded all the key parameters of the wrappers, we proceed to
    examine all the individual parts of it. Please don't forget that all these code
    snippets are part of the same `__init__` method and that we are simply re-using
    the recipes we have seen previously, therefore for any details of these code snippets,
    just refer to the previous recipes.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在记录了所有包装器的关键参数之后，我们继续检查它的所有独立部分。请不要忘记，这些代码片段都属于同一个`__init__`方法，我们仅仅是重新使用之前看到的配方，因此关于这些代码片段的任何细节，请参考之前的配方。
- en: 'Here we record the numeric pipeline:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们记录了数字管道：
- en: '[PRE37]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'After that, we record the pipeline processing time-related features:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们记录与管道处理时间相关的特征：
- en: '[PRE38]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now it is the turn of ordinal variables:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在轮到有序变量了：
- en: '[PRE39]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We close with the categorical pipelines, both the low-and high-categorical
    ones:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以分类管道作为结尾，包括低类别和高类别的管道：
- en: '[PRE40]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The next part regards the fitting. Depending on the different variable types
    available, the appropriate fit process will be applied and the newly processed
    or generated columns will be recorded in the `.columns` index list:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分涉及拟合。根据不同的变量类型，将应用相应的拟合过程，新的处理或生成的列将记录在`.columns`索引列表中：
- en: '[PRE41]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The `transform` method provides all the transformations and matrix joining
    in order to return a list of arrays containing, as their first element, the numerical
    parts of the processed data, followed by the numerical label vectors representing
    the high-cardinality categorical variables:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`transform`方法提供了所有的转换和矩阵连接，以返回一个包含处理后数据的数组列表，第一个元素是数值部分，后面是代表高基数类别变量的数值标签向量：'
- en: '[PRE42]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, we set the `fit_transform` method, which sequentially executes the
    fit and transform operations:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们设置了`fit_transform`方法，它依次执行fit和transform操作：
- en: '[PRE43]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now that we have finished wrapping everything together, we can take a look at
    how it works.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了所有的封装工作，可以看看它是如何工作的。
- en: How it works…
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'In our test, we assign the list of column names to variables depending on their
    type:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的测试中，我们根据列的类型将列名的列表赋值给变量：
- en: '[PRE44]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'After having instantiated the `TabularTransformer`, and mapped the variables
    we need to be processed to their type, we proceed to fit and transform our example
    dataset:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化了`TabularTransformer`并将需要处理的变量映射到它们的类型后，我们开始拟合并转换我们的示例数据集：
- en: '[PRE45]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The result is a list of NumPy arrays. We can iterate through them and print
    their shape in order to check how the output is composed:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个NumPy数组的列表。我们可以通过遍历它们并打印它们的形状，来检查输出的组成：
- en: '[PRE46]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The printed result reports a larger array as its first element (the combined
    result of all processes except the high-cardinality categorical one):'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的结果显示第一个元素是一个较大的数组（所有过程的结合结果，除了高基数类别变量的部分）：
- en: '[PRE47]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Our DNN can now expect a list as input, where the first element is a numerical
    matrix and the following elements are vectors to be sent to categorical embeddings
    layers.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的DNN现在可以期望一个列表作为输入，其中第一个元素是数值矩阵，接下来的元素是要传递给类别嵌入层的向量。
- en: There's more…
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: 'In order to be able to retrace each column and vector name, the `TabularTransformer`
    has a `columns` method, `tt.columns`, that can be invoked. The `TabularTransformer`
    can also call `tt.vocabulary` for information about the dimensionality of the
    categorical variables, which is necessary in order to correctly set the input
    shape of the embeddings layers in the network. The returned result is a dictionary
    in which the column name is the key and the dictionary size is the value:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够追溯每个列和向量的名称，`TabularTransformer`有一个`columns`方法，`tt.columns`，可以调用。`TabularTransformer`还可以调用`tt.vocabulary`来获取关于类别变量维度的信息，这对于正确设置网络中嵌入层的输入形状是必需的。返回的结果是一个字典，其中列名是键，字典的大小是值：
- en: '[PRE48]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now that we have these two methods for tracking down variable names (`tt.columns`)
    and defining the vocabulary of high-cardinality variables (`tt.vocabulary`), we
    are just a step away from a complete deep leaning framework for deep learning
    processing of tabular data.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了这两个方法来追踪变量名（`tt.columns`）和定义高基数变量的词汇（`tt.vocabulary`），我们距离一个完整的深度学习框架已经只差一步，用于处理表格数据的深度学习。
- en: Setting up a data generator
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置数据生成器
- en: We are just missing one key ingredient before we try our framework out on a
    difficult test task. The previous recipe presented a `TabularTransformer` that
    can effectively turn a pandas DataFrame into numerical arrays that a DNN can process.
    Yet, the recipe can only deal with all the data at once. The next step is to provide
    a way to create batches of the data of different sizes. This could be accomplished
    using `tf.data` or a Keras generator and, since previously in the book we have
    already explored quite a few examples with `tf.data`, this time we will prepare
    the code for a Keras generator that's capable of generating random batches on
    the fly when our DNN is learning.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们尝试在一个难度较大的测试任务上使用我们的框架之前，还缺少一个关键成分。前面的步骤展示了一个`TabularTransformer`，它可以有效地将pandas
    DataFrame转化为DNN可以处理的数值数组。然而，这个步骤只能一次性处理所有数据。下一步是提供一种方法，能够创建不同大小的数据批次。这可以通过使用`tf.data`或Keras生成器来实现，并且由于我们在本书之前已经探讨了许多`tf.data`的例子，这次我们将为Keras生成器准备代码，使其能够在DNN学习时动态生成随机批次。
- en: Getting ready
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Our generator will inherit from the `Sequence` class:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的生成器将继承自`Sequence`类：
- en: '[PRE49]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The `Sequence` class is the base object for fitting a sequence of data and it
    requires you to implement custom `__getitem__` (which will return a complete batch)
    and `__len__` (which will report how many batches are necessary to complete an
    epoch) methods.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sequence` 类是拟合数据序列的基础对象，要求你实现自定义的 `__getitem__`（返回完整批次）和 `__len__`（报告完成一个周期所需的批次数）方法。'
- en: How to do it…
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'We now script a new class called `DataGenerator` that inherits from the Keras
    `Sequence` class:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在编写一个名为 `DataGenerator` 的新类，继承自 Keras 的 `Sequence` 类：
- en: '[PRE50]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The generator is now set up. Let's proceed to the next section and explore how
    it works in more detail.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器已经设置好。接下来让我们进入下一部分，详细探索它是如何工作的。
- en: How it works…
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Apart from the `__init__` method, which instantiates the internal variables
    of the class, the `DataGenerator` class consists of these methods:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `__init__` 方法，该方法实例化类的内部变量外，`DataGenerator` 类还包括以下方法：
- en: '`_build_index`: This creates an index of the provided data'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_build_index`：用于创建提供数据的索引'
- en: '`on_epoch_end`: At the end of each epoch, this method will randomly shuffle
    the data'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`on_epoch_end`：在每个周期结束时，这个方法将随机打乱数据'
- en: '`__len__`: This reports how many batches are required to complete an epoch'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__len__`：报告完成一个周期所需的批次数'
- en: '`__iter__`: This renders the class an iterable'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__iter__`：使该类成为可迭代对象'
- en: '`__next__`: This calls the next batch'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__next__`：调用下一个批次'
- en: '`__call__`: This returns the `__iter__` method call'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__call__`：返回 `__iter__` 方法的调用'
- en: '`__data_generation`: Where the `TabularTransformer` operates on data batches,
    returning the transformed output (returning it as a list of arrays or as a dictionary
    of arrays)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__data_generation`：在这里，`TabularTransformer` 对数据批次进行操作，返回转换后的输出（作为数组列表或数组字典返回）'
- en: '`__getitem__`: This splits the data into batches and calls the `__data_generation`
    method for the transformations'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`__getitem__`：将数据拆分成批次，并调用 `__data_generation` 方法进行转换'
- en: This completes the final piece of the puzzle. Using the last two recipes you
    can fully transform and deliver to a TensorFlow model any mixed variable tabular
    dataset to a TensorFlow model, just by filling in a few parameters. In the next
    two recipes we will provide you with some specific tricks to make our DNN work
    better with tabular data, and we'll look at a fully fledged example from a famous
    Kaggle competition.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了最后一块拼图。通过使用最后两个配方，你只需填写几个参数，就可以将任何混合变量的表格数据完全转换并交付给 TensorFlow 模型。在接下来的两个配方中，我们将为你提供一些特定的技巧，以帮助我们的
    DNN 更好地处理表格数据，并且我们将展示一个来自著名 Kaggle 竞赛的完整示例。
- en: Creating custom activations for tabular data
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为表格数据创建自定义激活函数
- en: With images and text, it is more difficult to backpropagate errors in DNNs working
    on tabular data because the data is sparse. While the ReLU activation function
    is used widely, new activation functions have been found to work better in such
    cases and can improve the network performances. These activations functions are
    SeLU, GeLU, and Mish. Since SeLU is already present in Keras and TensorFlow (see
    [https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu)
    and [https://www.tensorflow.org/api_docs/python/tf/nn/selu](https://www.tensorflow.org/api_docs/python/tf/nn/selu)),
    in this recipe we'll use the GeLU and Mish activation functions.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像和文本数据来说，由于数据是稀疏的，在 DNN 处理表格数据时反向传播误差更加困难。虽然 ReLU 激活函数广泛使用，但已经发现新的激活函数在这种情况下表现更好，可以提高网络性能。这些激活函数包括
    SeLU、GeLU 和 Mish。由于 SeLU 已经包含在 Keras 和 TensorFlow 中（参见 [https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu](https://www.tensorflow.org/api_docs/python/tf/keras/activations/selu)
    和 [https://www.tensorflow.org/api_docs/python/tf/nn/selu](https://www.tensorflow.org/api_docs/python/tf/nn/selu)），在这个配方中我们将使用
    GeLU 和 Mish 激活函数。
- en: Getting ready
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You need the usual imports:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要导入常见的模块：
- en: '[PRE51]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: We've added `matplotlib`, so we can plot how these new activation functions
    work and get an idea of the reason for their efficacy.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经添加了 `matplotlib`，可以绘制这些新激活函数的工作效果，并了解它们有效性的原因。
- en: How to do it…
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'GeLU and Mish are defined by their mathematics, which you can find in their
    original papers:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: GeLU 和 Mish 的数学公式已在它们的原始论文中定义，您可以在其中找到详细信息：
- en: '*Gaussian Error Linear Units (GELUs)*: [https://arxiv.org/abs/1606.08415](https://arxiv.org/abs/1606.08415)'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高斯误差线性单元（GELUs）*：[https://arxiv.org/abs/1606.08415](https://arxiv.org/abs/1606.08415)'
- en: '*Mish, A Self Regularized* *Non-Monotonic Neural Activation Function*: [https://arxiv.org/abs/1908.08681](https://arxiv.org/abs/1908.08681)'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Mish, 一种自正则化的* *非单调神经激活函数*：[https://arxiv.org/abs/1908.08681](https://arxiv.org/abs/1908.08681)'
- en: 'Here are the formulas translated into code:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是翻译成代码的公式：
- en: '[PRE52]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The interesting part of the recipe is that `get_custom_objects` is a function
    that allows you to record your new functions in custom TensorFlow objects and
    then easily recall them as strings in layer parameters. You can find more information
    about how custom objects work in Keras by having a look at the TensorFlow documentation:
    [https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_custom_objects](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_custom_objects).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法有趣的地方在于 `get_custom_objects` 是一个函数，允许你在自定义的 TensorFlow 对象中记录新的函数，并且可以轻松地在层参数中将其作为字符串调用。你可以通过查看
    TensorFlow 文档了解更多关于自定义对象的工作原理：[https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_custom_objects](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_custom_objects)。
- en: How it works…
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'We can get an idea of how these two activation functions work by plotting positive
    and negative inputs against their outputs. A few commands from matplotlib will
    help us with the visualization:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过绘制正负输入与其输出的关系来了解这两个激活函数是如何工作的。使用 matplotlib 的几个命令将帮助我们进行可视化：
- en: '[PRE53]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'After running the code, you should get the following plot:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，你应该得到以下图形：
- en: '![](img/B16254_07_05.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_07_05.png)'
- en: 'Figure 7.3: GeLU and Mish activation functions mapped from inputs to outputs'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：GeLU 和 Mish 激活函数从输入到输出的映射
- en: As with the ReLU activation function, inputs from zero onward are just identically
    mapped as output (preserving linearity in the positive activations). The interesting
    thing happens when the input is below zero, actually, because it is not suppressed
    as happens with ReLU. In both the GeLU and Mish activation functions, the output
    is a dampened transformation of the negative input that recedes to zero when the
    input is very negative. This prevents both the case of dying neurons, because
    negative inputs can still pass information, and the case of saturated neurons,
    because overly negative values are turned off.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 与 ReLU 激活函数类似，输入从零开始到正数的部分会被直接映射为输出（保持正激活时的线性关系）。有趣的部分出现在输入小于零时，实际上，因为它不像 ReLU
    那样被抑制。在 GeLU 和 Mish 激活函数中，负输入的输出是经过抑制的变换，当输入非常负时会趋近于零。这避免了神经元“死亡”的问题，因为负输入仍然可以传递信息，也避免了神经元饱和的问题，因为过于负的值会被关闭。
- en: With different strategies, negative input is therefore processed and propagated
    both by the GeLU and Mish activations functions. This allows a defined gradient
    from negative inputs, which doesn't cause harm to the network.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 通过不同的策略，GeLU 和 Mish 激活函数都能处理和传播负输入。这使得负输入的梯度是有定义的，并且不会对网络造成伤害。
- en: Running a test on a difficult problem
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在一个困难问题上进行测试
- en: Throughout the chapter, we have provided recipes to handle tabular data in a
    successful way. Each recipe is not actually a solution in itself, but a piece
    of a puzzle. When the pieces are combined you can get excellent results and in
    this last recipe, we will demonstrate how to assemble all the recipes together
    to successfully complete a difficult Kaggle challenge.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们提供了一些有效处理表格数据的技巧。每个技巧本身并不是一个完整的解决方案，而是一个拼图的一部分。当这些部分组合在一起时，你可以获得出色的结果。在这最后一个技巧中，我们将展示如何将所有技巧结合起来，成功完成一个困难的
    Kaggle 挑战。
- en: The Kaggle competition, *Amazon.com – Employee Access Challenge* ([https://www.kaggle.com/c/amazon-employee-access-challenge](https://www.kaggle.com/c/amazon-employee-access-challenge)),
    is a competition that's notable for the high-cardinality variables involved and
    is a solid benchmark that's used to compare gradient boosting algorithms. The
    aim of the competition is to develop a model that can predict whether an Amazon
    employee should be given access to a specific resource based on their role and
    activities. The answer should be given as likelihood. As predictors, you have
    different ID codes corresponding to the type of resource you are evaluating access
    to, the role of the employee in the organization, and the referring manager.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 竞赛 *Amazon.com – Employee Access Challenge* ([https://www.kaggle.com/c/amazon-employee-access-challenge](https://www.kaggle.com/c/amazon-employee-access-challenge))
    是一个因涉及高基数变量而著名的竞赛，也是比较梯度提升算法的一个重要基准。该竞赛的目的是开发一个模型，根据员工的角色和活动预测是否应该给予其访问特定资源的权限。结果应该以可能性形式给出。作为预测因子，你将使用不同的
    ID 代码，这些代码对应于你正在评估访问权限的资源类型、员工在组织中的角色以及推荐经理。
- en: Getting ready
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'As usual, we start by importing TensorFlow and Keras:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们先导入 TensorFlow 和 Keras：
- en: '[PRE54]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Using sequential-based data generators may trigger some errors in TensorFlow
    2.2\. This is due to eager execution and, as a precaution, we have to disable
    it for this recipe:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于顺序的数据生成器可能会触发TensorFlow 2.2中的一些错误。这是由于启用执行（eager execution），因此作为预防措施，我们必须为此配方禁用它：
- en: '[PRE55]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'In order to get hold of the Amazon dataset, the best and fastest way is to
    install **CatBoost**, a gradient boosting algorithm that uses the dataset as a
    benchmark. If it is not already present in your installed environment, you easily
    install it using the `pip install catboost` command:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取亚马逊数据集，最佳且最快的方法是安装**CatBoost**，一种使用数据集作为基准的梯度提升算法。如果它还没有安装在你的环境中，你可以通过运行`pip
    install catboost`命令轻松安装：
- en: '[PRE56]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Since the test data (uploaded into the `Xt` variable) has an unlabeled target
    variable, we will be using just the training data in the `X` variable.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 由于测试数据（已上传至`Xt`变量）包含未标记的目标变量，我们将只使用`X`变量中的训练数据。
- en: How to do it…
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: As a first step, we will define the DNN architecture for this problem. Since
    the problem involves only categorical variables with high cardinality, we start
    setting an input and an embedding layer for each feature.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义此问题的DNN架构。由于问题只涉及具有高基数的分类变量，我们从为每个特征设置输入和嵌入层开始。
- en: 'We first define an input for each feature, where the data flows into the network,
    and then each input is directed into its respective embedding layer. The size
    of the input is based on the number of unique values of the feature, and the size
    of the output is based on the logarithm of the input size. The output of each
    embedding is then passed to a spatial dropout (since the embedding layer will
    return a matrix, the spatial dropout will blank out entire columns of the matrix)
    and then flattened. Finally, all the flattened results are concatenated into a
    single layer. From there on, the data has to pass through two dense layers with
    dropout before reaching the output response node, a sigmoid activated node that
    will return a probability as an answer:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先为每个特征定义一个输入，其中数据流入网络，然后每个输入被导入到其各自的嵌入层。输入的大小根据特征的唯一值数量来确定，输出的大小则基于输入大小的对数。每个嵌入层的输出随后传递到空间丢弃层（由于嵌入层会返回一个矩阵，空间丢弃层会将整个矩阵的列置空），然后进行展平。最后，所有展平后的结果被连接成一个单一的层。从此，数据必须通过两个带丢弃层的全连接层，最终到达输出响应节点，该节点是一个经过sigmoid激活的节点，返回一个概率作为答案：
- en: '[PRE57]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The architecture works only with categorical data. It takes each categorical
    input (expecting a single integer code) and fits it into an embedding layer, whose
    output is a reduced dimensionality vector (whose dimensions are computed using
    the heuristic `int(np.log1p(category_counts)+1)`). It applies a `SpatialDropout1D`
    and finally it flattens the output. `SpatialDropout1D` removes all the connections
    in a row of the output matrix from all channels, thus effectively dropping some
    information from the embedding. All the outputs of all the categorical variables
    are then concatenated and passed on to a series of dense layers with GeLU activations
    and dropout. It all ends with a single sigmoid node (so you can get the emission
    of a probability in the range [0,1]).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构仅适用于分类数据。它将每个分类输入（预期为单个整数编码）传入一个嵌入层，嵌入层的输出是一个降维向量（其维度通过启发式方法`int(np.log1p(category_counts)+1)`计算得出）。接着应用`SpatialDropout1D`，最后将输出展平。`SpatialDropout1D`会去除输出矩阵一行中的所有连接，去掉所有通道中的一些信息。所有分类变量的输出将被连接并传递到一系列带GeLU激活和丢弃层的全连接层。最后，输出一个单一的sigmoid节点（因此你可以得到[0,1]范围内的概率输出）。
- en: After defining the architecture, we define the score functions, taking them
    from scikit-learn and converting them for use in Keras using the `tf.py_function`
    from TensorFlow ([https://www.tensorflow.org/api_docs/python/tf/py_function](https://www.tensorflow.org/api_docs/python/tf/py_function)),
    a wrapper that can turn any function into a once-differentiable TensorFlow operation
    that can be executed eagerly.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 定义完架构后，我们将定义评分函数，取自scikit-learn，并使用TensorFlow中的`tf.py_function`将它们转换为Keras可以使用的函数（[https://www.tensorflow.org/api_docs/python/tf/py_function](https://www.tensorflow.org/api_docs/python/tf/py_function)），这是一个包装器，可以将任何函数转化为一个一次可微分的TensorFlow操作，并可以进行即时执行。
- en: As score functions, we use the average precision and the ROC AUC. Both of these
    can help us figure out how we are performing on a binary classification by telling
    us how closely the predicted probabilities resemble the true values. More on ROC
    AUC and average precision can be found in the scikit-learn documentation at [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)
    and [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 作为得分函数，我们使用平均精度和ROC AUC。这两者都能帮助我们了解在二分类问题上的表现，告诉我们预测的概率与真实值之间的接近程度。有关ROC AUC和平均精度的更多信息，请参见scikit-learn文档中的[https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)和[https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)。
- en: 'We also instantiate a simple plotting function that can plot selected error
    and score measures as recorded during the training both on the training and validation
    sets:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实例化了一个简单的绘图函数，可以绘制在训练过程中记录的选定误差和得分度量，既适用于训练集也适用于验证集：
- en: '[PRE58]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: At this point, you need to set up the training phase. Given the limited number
    of examples and your need to test your solution, using cross-validation is the
    best choice. The `StratifiedKFold` function from scikit-learn will provide you
    with the right tool for the job.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，你需要设置训练阶段。考虑到样本数量有限以及你需要测试你的解决方案，使用交叉验证是最好的选择。scikit-learn中的`StratifiedKFold`函数将为你提供完成此任务的正确工具。
- en: In `StratifiedKFold`, your data is randomly (you can provide a seed value for
    reproducibility) split into *k* parts, each one with the same proportion of the
    target variable as is found in the original data.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在`StratifiedKFold`中，你的数据会随机（你可以提供一个种子值以确保可重复性）被划分为*k*个部分，每一部分的目标变量比例与原始数据中的比例相同。
- en: These *k* splits are used to generate *k* training tests that can help you infer
    the performance of the DNN architecture you have set up. In fact, *k* times over,
    *all but one* of the splits are used to train your model and the one kept apart
    is left out for testing each time. This ensures that you have *k* tests made on
    splits that have not been used for training.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这些*k*拆分被用来生成*k*个训练测试，这些测试可以帮助你推断你所设置的DNN架构的表现。事实上，在进行*k*次实验时，*除了一个*拆分，其余的都被用来训练模型，而那个被保留的拆分则每次都用于测试。这确保了你有*k*个测试，都是在未用于训练的拆分上进行的。
- en: This approach, especially when dealing with only a few training examples, is
    preferable to picking up a single test set to verify your models on, because by
    sampling a test set you could find a sample that is differently distributed from
    your train set. Moreover, by using a single test set, you also risk overfitting
    your test set. If you repeatedly test different solutions, eventually you may
    find a solution that fits the test set very well but is not a generalizable solution
    in itself.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法，尤其是在训练样本较少时，比选择单一的测试集来验证模型更为优越，因为通过抽取测试集，你可能会发现一个与训练集分布不同的样本。此外，使用单一的测试集也有可能导致过拟合。如果你反复测试不同的解决方案，最终可能会找到一个非常适合测试集的解决方案，但这个解决方案并不一定具有泛化能力。
- en: 'Let''s put it into practice here:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这里实践一下：
- en: '[PRE59]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The script runs a training and validation test for each fold and stores the
    results that will help you correctly evaluate the performances of your DNN for
    tabular data.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本会为每个折叠执行训练和验证测试，并存储结果，这些结果将帮助你正确评估你在表格数据上的深度神经网络（DNN）表现。
- en: How it works…
  id: totrans-304
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Each fold will print a plot detailing how the DNN performed, both on log-loss
    and ROC AUC, for the training and the validation sample:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 每个折叠将打印出一张图，详细展示DNN在训练集和验证集上的表现，包括对数损失和ROC AUC：
- en: '![](img/B16254_07_06.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_07_06.png)'
- en: 'Figure 7.4: DNN performance on the training set and the validation set'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：DNN在训练集和验证集上的表现
- en: All the folds have a similar trajectory, with a significant decoupling of the
    train and validation curves after 5 epochs and a widening gap after 15 epochs,
    implying a certain overfitting during the training phase. By modifying your DNN
    architecture, and changing parameters such as the learning rate or the optimization
    algorithm, you can safely experiment to try to achieve better results because
    the cross-validation procedure ensures that you are making the right decisions.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的折叠都有类似的轨迹，在训练和验证曲线在5个epoch之后明显分离，并且在15个epoch后差距扩大，暗示训练阶段可能存在一定的过拟合。通过修改你的DNN架构，并改变如学习率或优化算法等参数，你可以放心地进行实验，尝试获得更好的结果，因为交叉验证过程可以确保你做出正确的决策。
