- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Transformers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformers
- en: Transformers are deep learning architectures introduced by Google in 2017 that
    are designed to process sequential data for downstream tasks such as translation,
    question answering, or text summarization. In this manner, they aim to solve a
    similar problem to RNNs discussed in *Chapter 9*, *Recurrent Neural Networks*,
    but Transformers have a significant advantage as they do not require processing
    the data in order. Among other advantages, this allows a higher degree of parallelization
    and therefore faster training.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers是Google在2017年提出的深度学习架构，旨在处理序列数据，用于下游任务，如翻译、问答或文本摘要。这样，它们旨在解决与*第9章*《*循环神经网络*》中讨论的RNNs类似的问题，但Transformers具有显著优势，因为它们不需要按顺序处理数据。除此之外，这使得更高程度的并行化成为可能，从而加速了训练过程。
- en: Due to their flexibility, Transformers can be pretrained on large bodies of
    unlabeled data and then finetuned for other tasks. Two main groups of such pretrained
    models are **Bidirectional Encoder Representations from Transformers** (**BERTs**)
    and **Generative Pretrained Transformers** (**GPTs**).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其灵活性，Transformers可以在大量未标记的数据上进行预训练，然后再针对其他任务进行微调。这些预训练模型的两大主要类型是**双向编码器表示的Transformers**（**BERT**）和**生成预训练Transformers**（**GPT**）。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Text generation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本生成
- en: Sentiment analysis
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感分析
- en: 'Text classification: sarcasm detection'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类：讽刺检测
- en: Question answering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答
- en: We'll begin by demonstrating the text generation capabilities of GPT-2 – one
    of the most popular Transformer architectures usable by a broader audience. While
    sentiment analysis can be handled by RNNs as well (as demonstrated in the previous
    chapter), it is the generative capabilities that most clearly demonstrate the
    impact of introducing Transformers into the Natural Language Processing stack.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先展示GPT-2的文本生成能力——这是最受广大用户使用的Transformers架构之一。虽然情感分析也可以由RNNs处理（如前一章所示），但正是生成能力最能清晰地展示Transformers在自然语言处理中的影响。
- en: Text generation
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本生成
- en: 'The first GPT model was introduced in a 2018 paper by Radford et al. from OpenAI
    – it demonstrated how a generative language model can acquire knowledge and process
    long-range dependencies thanks to pretraining on a large, diverse corpus of contiguous
    text. Two successor models (trained on more extensive corpora) were released in
    the following years: GPT-2 in 2019 (1.5 billion parameters) and GPT-3 in 2020
    (175 billion parameters). In order to strike a balance between demonstration capabilities
    and computation requirements, we will be working with GPT-2 – as of the time of
    writing, access to the GPT-3 API is limited.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个GPT模型是在2018年由OpenAI的Radford等人发布的论文中介绍的——它展示了生成性语言模型如何通过在大量多样的连续文本语料库上进行预训练，获得知识并处理长程依赖关系。随后几年发布了两个继任模型（在更大语料库上训练）：2019年的GPT-2（15亿参数）和2020年的GPT-3（1750亿参数）。为了在演示能力和计算需求之间取得平衡，我们将使用GPT-2——截至本文编写时，GPT-3的API访问受到限制。
- en: We'll begin by demonstrating how to generate your own text based on a prompt
    given to the GPT-2 model without any finetuning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过展示如何基于给定提示生成自己的文本，来开始使用GPT-2模型，而不进行任何微调。
- en: How do we go about it?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们该如何进行？
- en: We will be making use of the excellent Transformers library created by Hugging
    Face ([https://huggingface.co/](https://huggingface.co/)). It abstracts away several
    components of the building process, allowing us to focus on the model performance
    and intended performance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用由Hugging Face创建的优秀Transformers库（[https://huggingface.co/](https://huggingface.co/)）。它抽象了构建过程中的多个组件，使我们能够专注于模型性能和预期表现。
- en: 'As usual, we begin by loading the required packages:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们首先加载所需的包：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'One of the advantages of the Transformers library – and a reason for its popularity,
    undoubtedly – is how easily we can download a specific model (and also define
    the appropriate tokenizer):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers库的一个优势——也是其流行的原因之一——是我们可以轻松下载特定模型（并且还可以定义合适的分词器）：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It is usually a good idea to fix the random seed to ensure the results are
    reproducible:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，固定随机种子是一个好主意，以确保结果的可重复性：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For a proper description of the decoder architecture within Transformers, please
    refer to the *See also* section at the end of this section – for now, let us focus
    on the fact that how we decode is one of the most important decisions when using
    a GPT-2 model. Below, we review some of the methods that can be utilized.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Transformer中解码器架构的详细描述，请参阅本节末尾的*另见*部分——现在，我们将重点关注解码方式，它是使用GPT-2模型时最重要的决策之一。下面，我们将回顾一些可用的方法。
- en: 'With **greedy search**, the word with the highest probability is predicted
    as the next word in the sequence:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**贪心搜索**，预测具有最高概率的单词作为序列中的下一个单词：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once we have our input sequence, we encode it and then call a `decode` method:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了输入序列，就将其编码，然后调用`decode`方法：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As you can see, the results leave some room for improvement: the model starts
    repeating itself, because the high-probability words mask the less-likely ones
    so they cannot explore more diverse combinations.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，结果仍有改进空间：模型开始自我重复，因为高概率词汇掩盖了低概率词汇，使其无法探索更多样化的组合。
- en: 'A simple remedy is **beam search**: we keep track of the alternative variants,
    so that more comparisons are possible:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的解决方法是**束搜索**：我们跟踪备选变体，从而使得更多的比较成为可能：
- en: '[PRE5]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is definitely more diverse – the message is the same, but at least the
    formulations look a little different from a style point of view.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实更加多样化——信息相同，但至少从风格上来看，表达方式有所不同。
- en: 'Next, we can explore sampling – indeterministic decoding. Instead of following
    a strict path to find the end text with the highest probability, we rather randomly
    pick the next word by its conditional probability distribution. This approach
    risks producing incoherent ramblings, so we make use of the `temperature` parameter,
    which affects the probability mass distribution:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以探索采样——不确定性解码。我们并不严格按照路径来找到具有最高概率的最终文本，而是根据条件概率分布随机选择下一个单词。这个方法有可能生成不连贯的胡言乱语，因此我们使用`temperature`参数，它会影响概率质量分布：
- en: '[PRE6]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Waxing poetic a bit. What happens if we increase the temperature?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微有点诗意地说，若我们提高温度，会发生什么呢？
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is getting more interesting, although it still feels a bit like a train
    of thought – which is perhaps to be expected, given the content of our prompt.
    Let's explore some more ways to tune the output.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这变得更加有趣，尽管它仍然有点像思路流——这或许是可以预见的，考虑到我们提示词的内容。让我们探索更多调优输出的方法。
- en: 'In **Top-K sampling**, the top *k* most likely next words are selected and
    the entire probability mass is shifted to these *k* words. So instead of increasing
    the chances of high-probability words occurring and decreasing the chances of
    low-probability words, we just remove low-probability words altogether:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在**Top-K采样**中，选择最有可能的前*k*个单词，并将整个概率质量转移到这*k*个单词上。因此，我们并不增加高概率词汇出现的机会或减少低概率词汇的机会，而是直接将低概率词汇完全移除：
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This seems like a step in the right direction. Can we do better?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是朝着正确方向迈出的步伐。我们能做得更好吗？
- en: 'Top-P sampling (also known as nucleus sampling) is similar to Top-K, but instead
    of choosing the top *k* most likely words, we choose the smallest set of words
    whose total probability is larger than *p*, and then the entire probability mass
    is shifted to the words in this set. The main difference here is that with Top-K
    sampling, the size of the set of words is static (obviously), whereas in Top-P
    sampling, the size of the set can change. To use this sampling method, we just
    set `top_k = 0` and choose a `top_p` value:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Top-P采样（也叫做核采样）类似于Top-K采样，但不是选择最有可能的前*k*个单词，而是选择概率总和大于*p*的最小单词集合，然后将整个概率质量转移到该集合中的单词上。这里的主要区别是，Top-K采样中，单词集合的大小是静态的（显然），而在Top-P采样中，集合的大小可以变化。要使用这种采样方法，我们只需设置`top_k
    = 0`并选择一个`top_p`值：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can combine both approaches:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以结合这两种方法：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Clearly, the more-sophisticated method''s settings can give us pretty impressive
    results. Let''s explore this avenue more – we''ll use the prompts taken from OpenAI''s
    GPT-2 website, where they feed them to a full-sized GPT-2 model. This comparison
    will give us an idea of how well we are doing with a local (smaller) model compared
    to a full one that was used for the original demos:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，更复杂的方法设置可以给我们带来相当令人印象深刻的结果。让我们进一步探索这个方向——我们将使用从OpenAI的GPT-2网站上获取的提示词，并将其输入完整的GPT-2模型。这种对比将让我们了解本地（较小）模型与用于原始演示的大型模型之间的表现差异：
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For comparison, this is the output from a complete model:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对比，这是完整模型的输出：
- en: 'Output:'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出：
- en: '0*: In a* *shocking finding, scientist discovered a herd of unicorns living
    in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising
    to the researchers was the fact that the unicorns spoke perfect English.*'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '0*: 在一项* *震惊的发现中，科学家们在安第斯山脉的一个偏远、以前未被探索的山谷中发现了一群独角兽。更令研究人员惊讶的是，这些独角兽能说一口流利的英语。*'
- en: '*"This is not only a scientific finding; it is also a very important finding
    because it will enable us to further study the* *phenomenon," said Dr. Jorge Llamas,
    from the* **National Institute of Anthropology and History** *(***INAH***) in
    Colombia, in a statement.*'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*“这不仅是一个科学发现，它也是一个非常重要的发现，因为它将使我们能够进一步研究这个* *现象，”哥伦比亚**国家人类学与历史研究所**（**INAH**）的乔治·利亚马斯博士在一份声明中说道。*'
- en: '*"We have previously found that humans have used human voices to communicate
    with the animals. In this case, the animals are communicating with us. In other
    words, this is a breakthrough in the field of animal communication," added Llamas...*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*“我们之前发现人类曾用人类的声音与动物交流。在这种情况下，动物正在与我们交流。换句话说，这是动物交流领域的一个突破，”利亚马斯补充道……*'
- en: '*In another example, it seems like the trepidations of the model authors were
    justified: GPT-2 can in fact generate fake news stories.*'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*在另一个例子中，似乎模型作者的担忧是有道理的：GPT-2确实能够生成假新闻故事。*'
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Output:'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出：
- en: '*0: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood
    Boulevard today. The star* *was spotted trying on three dresses before attempting
    to walk out of the store.*'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*0：麦莉·赛勒斯今天在好莱坞大道上被抓到从阿贝克朗比和费奇商店偷东西。有人看到她试穿了三件裙子，然后试图走出商店。*'
- en: '*Abercrombie is one of a number of stores the star has frequented.*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*阿贝克朗比是这位明星常去的几家店之一。*'
- en: '*The singer was spotted walking into Abercrombie & Fitch in West Hollywood
    just after noon this afternoon before leaving the store.*'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*这位歌手在今天中午过后被看到走进位于西好莱坞的阿贝克朗比与费奇商店，随后离开了商店。*'
- en: '*The star is currently in the middle of a tour of Australia and New Zealand
    for her X Factor appearance on February 28....*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*这位明星目前正在澳大利亚和新西兰进行巡演，参加2月28日的《X Factor》演出……*'
- en: What about riffing off literature classics like Tolkien?
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，像托尔金这样的文学经典，怎么样？
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Output:'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出：
- en: '*0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing
    war cry.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*0：莱戈拉斯和金雳带着一声刺耳的战吼向兽人进军，举起了他们的武器。*'
- en: '*Then the orcs* *made their move.*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*然后，兽人* *发动了进攻。*'
- en: '*The Great Orc Warband advanced at the sound of battle. They wore their weapons
    proudly on their chests, and they looked down upon their foes.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*伟大的兽人战争团在战斗声中前进。他们骄傲地佩戴着武器，俯视着他们的敌人。*'
- en: '*In the distance, the orcs could be heard shouting their orders in a low voice.*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*远处，可以听见兽人低声喊着命令。*'
- en: '*But the battle was not yet over. The orcs'' axes and hammers slammed into
    the enemy ranks as though they were an army of ten thousand warriors, and their
    axes made the orcs bleed.*'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*但战斗还没有结束。兽人的斧头和战锤猛烈地击打敌军，就像他们是一支万人的军队一样，斧头砍中了兽人，使他们流血。*'
- en: '*In the midst of the carnage, the Elven leader Aragorn cried out: "Come, brave.
    Let us fight the orcs!"*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这场大屠杀中，精灵领袖阿拉贡大喊道：“来吧，勇士们，让我们与兽人战斗！”*'
- en: 'As you can see from the examples above, a GPT-2 model working out of the box
    (without finetuning) can already generate plausible-looking long-form text. Assessing
    the future impact of this technology on the field of communication remains an
    open and highly controversial issue: on the one hand, there is fully justified
    fear of fake news proliferation (see the Miley Cyrus story above). This is particularly
    concerning because large-scale automated detection of generated text is an extremely
    challenging topic. On the other hand, GPT-2 text generation capabilities can be
    helpful for creative types: be it style experimentation or parody, an AI-powered
    writing assistant can be a tremendous help.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如上面的例子所示，一个未经微调的GPT-2模型（即开箱即用）已经能够生成看似合理的长篇文本。评估这种技术未来对传播领域的影响仍然是一个开放且极具争议的问题：一方面，对于假新闻泛滥的恐惧是完全有理由的（见上面的麦莉·赛勒斯故事）。这尤其令人担忧，因为大规模自动检测生成文本是一个极具挑战性的话题。另一方面，GPT-2的文本生成能力对于创意工作者可能非常有帮助：无论是风格实验还是讽刺，一个由AI驱动的写作助手可以提供巨大的帮助。
- en: See also
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'There are multiple excellent resources online for text generation with GPT-2:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 网络上有多个关于GPT-2文本生成的优秀资源：
- en: 'The original OpenAI post that introduced the model:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍该模型的原始OpenAI文章：
- en: '[https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/
    )'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://openai.com/blog/better-language-models/](https://openai.com/blog/better-language-models/
    )'
- en: 'Top GPT-2 open source projects:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顶级 GPT-2 开源项目：
- en: '[https://awesomeopensource.com/projects/gpt-2](https://awesomeopensource.com/projects/gpt-2
    )'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://awesomeopensource.com/projects/gpt-2](https://awesomeopensource.com/projects/gpt-2
    )'
- en: 'Hugging Face documentation:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face 文档：
- en: '[https://huggingface.co/blog/how-to-generate](https://huggingface.co/blog/how-to-generate
    )'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/blog/how-to-generate](https://huggingface.co/blog/how-to-generate
    )'
- en: '[https://huggingface.co/transformers/model_doc/gpt2.html](https://huggingface.co/transformers/model_doc/gpt2.html
    )'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://huggingface.co/transformers/model_doc/gpt2.html](https://huggingface.co/transformers/model_doc/gpt2.html
    )'
- en: Sentiment analysis
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析
- en: 'In this section, we''ll demonstrate how DistilBERT – a lightweight version
    of BERT – can be used to handle a common problem of sentiment analysis. We will
    be using data from a Kaggle competition ([https://www.kaggle.com/c/tweet-sentiment-extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)):
    given a tweet and the sentiment (positive, neutral, or negative), participants
    needed to identify the part of the tweet that defines that sentiment. Sentiment
    analysis is typically employed in business as part of a system that helps data
    analysts gauge public opinion, conduct detailed market research, and track customer
    experience. An important application is medical: the effect of different treatments
    on patients'' moods can be evaluated based on their communication patterns.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何使用DistilBERT（BERT的轻量版本）来处理情感分析中的常见问题。我们将使用Kaggle比赛中的数据（[https://www.kaggle.com/c/tweet-sentiment-extraction](https://www.kaggle.com/c/tweet-sentiment-extraction)）：给定一条推文及其情感（积极、中立或消极），参与者需要识别出定义该情感的推文部分。情感分析通常在商业中应用，作为帮助数据分析师评估公众舆论、进行详细市场调研和跟踪客户体验的系统的一部分。一个重要的应用领域是医疗：可以根据患者的交流模式评估不同治疗对其情绪的影响。
- en: How do we go about it?
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们该如何进行？
- en: As usual, we begin by loading the necessary packages.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们首先加载必要的包。
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In order to streamline the code, we define some helper functions for cleaning
    the text: we remove website links, starred-out NSFW terms, and emojis.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化代码，我们定义了一些帮助函数来清理文本：去除网站链接、星号遮蔽的NSFW词汇和表情符号。
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Load the data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据。
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_10_1.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_10_1.png)'
- en: 'Figure 10.1: Sample of the tweet sentiment analysis data'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1：推文情感分析数据示例
- en: 'The snapshot above demonstrates a sample of the data we will focus our analysis
    on: the complete text, the key phrase, and its associated sentiment (positive,
    negative, or neutral).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的快照展示了我们将关注分析的一个数据样本：完整文本、关键短语及其相关的情感（积极、消极或中立）。
- en: 'We proceed with fairly standard preprocessing of the data:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续进行相对标准的数据预处理：
- en: '`basic_cleaning` – to remove website URLs and non-characters and to replace
    * swear words with the word swear.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`basic_cleaning` – 用于去除网站URL和非字符内容，并将*脏话替换为 swear 一词。'
- en: '`remove_html`.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`remove_html`。'
- en: '`remove_emojis`.'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`remove_emojis`。'
- en: '`remove_multiplechars` – this is for when there are more than 3 characters
    in a row in a word, for example, wayyyyy. The function removes all but one of
    the letters.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`remove_multiplechars` – 该函数用于处理当一个单词中有超过 3 个连续字符时，例如，wayyyyy。该函数会去除其中的多余字母，保留一个。'
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As for labels, we one-hot encode the targets, tokenize them, and convert them
    into sequences.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标签，我们进行独热编码，将它们分词，并转换为序列。
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'DistilBERT is a light version of BERT: it has 40 pct fewer parameters, but
    achieves 97% of the performance. For the purpose of this recipe, we will use it
    primarily for its tokenizer and an embedding matrix. Although the matrix is trainable,
    we shall not utilize this option, in order to reduce the training time.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: DistilBERT 是 BERT 的轻量版本：它的参数比 BERT 少 40%，但性能达到 BERT 的 97%。对于本例，我们主要使用它的分词器和嵌入矩阵。虽然该矩阵是可训练的，但为了减少训练时间，我们不使用这个选项。
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We proceed with the usual steps for defining a model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按常规步骤定义模型。
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can now fit the model:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以拟合模型了：
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As we can see from the above output, the model converges quite rapidly and achieves
    a reasonable accuracy of 76% on the validation set already after 10 iterations.
    Further finetuning of hyperparameters and longer training can improve the performance,
    but even at this level, a trained model – for example, through the use of TensorFlow
    Serving – can provide a valuable addition to the sentiment analysis logic of a
    business application.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述输出可以看出，模型收敛得相当快，并且在验证集上的准确率在10次迭代后已经达到了76%。进一步微调超参数和更长时间的训练可以提高性能，但即使在这个水平，经过训练的模型——例如，通过TensorFlow
    Serving——也能为商业应用中的情感分析逻辑提供有价值的补充。
- en: See also
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'The best starting point is the documentation by Hugging Face: [https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/](https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的起点是Hugging Face的文档：[https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/](https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)。
- en: Open-domain question answering
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开放领域问题回答
- en: Given a passage of text and a question related to that text, the idea of **Question
    Answering** (**QA**) is to identify the subset of the passage that answers the
    question. It is one of many tasks where Transformer architectures have been applied
    successfully. The Transformers library has a number of pretrained models for QA
    that can be applied even in the absence of a dataset to finetune on (a form of
    zero-shot learning).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一段文本及与该文本相关的问题，**问题回答**（**QA**）的概念是确定回答该问题的文本子集。这是应用Transformer架构成功的许多任务之一。Transformers库有多个预训练的QA模型，可以在没有数据集进行微调的情况下应用（这是一种零样本学习形式）。
- en: 'However, different models might fail at different examples and it might be
    useful to examine the reasons. In this section, we''ll demonstrate the TensorFlow
    2.0 GradientTape functionality: it allows us to record operations on a set of
    variables we want to perform automatic differentiation on. To explain the model''s
    output on a given input, we can:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不同的模型在不同的示例上可能会失败，了解原因可能是有益的。在本节中，我们将展示TensorFlow 2.0的GradientTape功能：它允许我们记录对一组变量进行自动微分的操作。为了解释模型在给定输入上的输出，我们可以：
- en: One-hot encode the input – unlike integer tokens (typically used in this context),
    a one-hot-encoding representation is differentiable
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对输入进行一热编码——与整数令牌（通常在此上下文中使用）不同，一热编码表示是可微分的
- en: Instantiate GradientTape and watch our input variable
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化GradientTape并监视我们的输入变量
- en: Compute a forward pass through the model
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算通过模型的前向传播
- en: Get the gradients of the output of interest (for example, a specific class logit)
    with respect to the *watched* input
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取我们感兴趣的输出的梯度（例如，特定类别的logit），相对于*监视的*输入
- en: Use the normalized gradients as explanations
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标准化梯度作为解释
- en: 'The code in this section is adapted from the results published by Fast Forward
    Labs: [https://experiments.fastforwardlabs.com/](https://experiments.fastforwardlabs.com/).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码改编自Fast Forward Labs发布的结果：[https://experiments.fastforwardlabs.com/](https://experiments.fastforwardlabs.com/)。
- en: How do we go about it?
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何开始？
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As usual, we need some boilerplate: begin with a function for fetching pretrained
    QA models.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们需要一些样板代码：首先编写一个用于获取预训练QA模型的函数。
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Identify the span of the answer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 确定答案的范围。
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We need some functions for data preparation.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一些用于数据准备的函数。
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And finally plotting:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后进行绘图：
- en: '[PRE26]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We'll compare the performance of a small set of models across a range of questions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将比较一小部分模型在不同问题上的表现。
- en: '[PRE27]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Format the results for easier inspection.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 格式化结果以便于检查。
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_10_2.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![包含桌子的图像  描述由系统自动生成](img/B16254_10_2.png)'
- en: 'Figure 10.2: Sample records demonstrating the answers generated by different
    models'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：展示不同模型生成的答案的样本记录
- en: 'As we can observe from the results data, even on this sample dataset there
    are marked differences between the models:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果数据中我们可以观察到，即使在这个样本数据集上，模型之间也存在明显的差异：
- en: DistilBERT (SQUAD1) can answer 5/8 questions, 2 correct
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistilBERT（SQUAD1）能回答5/8个问题，正确2个
- en: DistilBERT (SQUAD2) can answer 7/8 questions, 7 correct
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DistilBERT（SQUAD2）能回答7/8个问题，正确7个
- en: BERT base can answer 5/8 questions, 5 correct
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT base能回答5/8个问题，正确5个
- en: BERT large can answer 7/8 questions, 7 correct
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT large能回答7/8个问题，正确7个
- en: '[PRE29]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Based on the results above, we can gain some insight into the workings of BERT-based
    QA models:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述结果，我们可以对基于BERT的问答模型的工作原理获得一些洞见：
- en: In a situation where a BERT model fails to produce an answer (for example, it
    only gives CLS), almost none of the input tokens have high normalized gradient
    scores. This suggests room for improvement in terms of the metrics used – going
    beyond explanation scores and potentially combining them with model confidence
    scores to gain a more complete overview of the situation.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在BERT模型无法生成答案的情况下（例如，它只给出CLS标记），几乎没有输入的标记具有高的归一化梯度分数。这表明在使用的指标方面还有改进的空间——可以超越解释分数，并可能将其与模型置信度分数结合，以获得更完整的情境概览。
- en: Analyzing the performance difference between the base and large variants of
    the BERT model suggests that the trade-off (better performance versus longer inference
    time) should be investigated further.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析基础版和大版本BERT模型的性能差异表明，应该进一步研究性能提升与推理时间延长之间的权衡（更好的性能对比更长的推理时间）。
- en: Taking into account the potential issues with our selection of the evaluation
    dataset, a possible conclusion is that DistilBERT (trained on SQuAD2) performs
    better than base BERT – which highlights issues around using SQuAD1 as a benchmark.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到我们在选择评估数据集时可能存在的问题，一个可能的结论是，DistilBERT（在SQuAD2上训练）比基础BERT表现更好——这凸显了使用SQuAD1作为基准存在的问题。
