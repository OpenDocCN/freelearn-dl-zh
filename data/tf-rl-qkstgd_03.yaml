- en: Deep Q-Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q网络
- en: '**Deep Q-Networks** (**DQNs**) revolutionized the field of **reinforcement
    learning** (**RL**). I am sure you have heard of Google DeepMind, which used to
    be a British company called DeepMind Technologies until Google acquired it in
    2014\. DeepMind published a paper in 2013 titled *Playing Atari with Deep RL*,
    where they used **Deep Neural Networks** (**DNNs**) in the context of RL, or DQNs
    as they are referred to – which is an idea that is seminal to the field. This
    paper revolutionized the field of deep RL, and the rest is history! Later, in
    2015, they published a second paper, titled *Human Level Control Through Deep
    RL*, in *Nature*, where they had more interesting ideas that further improved
    the former paper. Together, the two papers led to a Cambrian explosion in the
    field of deep RL, with several new algorithms that have improved the training
    of agents using neural networks, and have also pushed the limits of applying deep
    RL to interesting real-world problems.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度Q网络**（**DQN**）革新了**强化学习**（**RL**）领域。我相信你一定听说过Google DeepMind，它曾是一个名为DeepMind
    Technologies的英国公司，直到2014年被Google收购。DeepMind在2013年发布了一篇论文，标题为*Playing Atari with
    Deep RL*，在这篇论文中，他们在强化学习的背景下使用了**深度神经网络**（**DNN**），或者被称为DQN——这是该领域的奠基性想法。这篇论文彻底革新了深度强化学习的领域，接下来的发展也成为了历史！后来，在2015年，他们又发布了第二篇论文，标题为*Human
    Level Control Through Deep RL*，发表于《自然》杂志，提出了一些更有趣的想法，进一步改进了前一篇论文。两篇论文一起引发了深度强化学习领域的“寒武纪大爆发”，出现了多个新算法，改善了使用神经网络训练智能体的效果，也推动了深度强化学习在实际问题中的应用。'
- en: In this chapter, we will investigate a DQN and also code it using Python and
    TensorFlow. This will be our first use of deep neural networks in RL. It will
    also be our first effort in this book to use deep RL to solve real-world control
    problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将研究DQN，并使用Python和TensorFlow进行编码。这将是我们在强化学习中第一次使用深度神经网络。同时，这也是我们本书中第一次尝试通过深度强化学习解决实际的控制问题。
- en: 'In this chapter, the following topics will be covered:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，将涉及以下主题：
- en: Learning the theory behind a DQN
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习DQN背后的理论
- en: Understanding target networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解目标网络
- en: Learning about replay buffer
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解回放缓冲区
- en: Getting introduced to the Atari environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍Atari环境
- en: Coding a DQN in TensorFlow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TensorFlow中编写DQN
- en: Evaluating the performance of a DQN on Atari Breakout
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估DQN在Atari Breakout上的表现
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Knowledge of the following will help you to better understand the concepts
    presented in this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 了解以下内容将有助于你更好地理解本章介绍的概念：
- en: Python (2 and above)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python（2及以上版本）
- en: NumPy
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: TensorFlow (version 1.4 or higher)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow（版本1.4或更高）
- en: Learning the theory behind a DQN
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习DQN背后的理论
- en: In this section, we will look at the theory behind a DQN, including the math
    behind it, and learn the use of neural networks to evaluate the `value` function.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨DQN背后的理论，包括其中的数学原理，并学习如何使用神经网络来评估`value`函数。
- en: Previously, we looked at Q-learning, where *Q(s,a)* was stored and evaluated
    as a multi-dimensional array, with one entry for each state-action pair. This
    worked well for grid-world and cliff-walking problems, both of which are low-dimensional
    in both state and action spaces. So, can we apply this to higher dimensional problems?
    Well, no, due to the *curse of dimensionality*, which makes it unfeasible to store
    very large number states and actions. Moreover, in continuous control problems,
    the actions vary as a real number in a bounded range, although an infinite number
    of real numbers are possible, which cannot be represented as a tabular *Q* array.
    This gave rise to function approximations in RL, particularly with the use of
    DNNs – that is, DQNs. Here, *Q(s,a)* is represented as a DNN that will output
    the value of *Q*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论了Q学习，其中*Q(s,a)*被存储并作为一个多维数组进行评估，每个状态-动作对有一个条目。这在网格世界和悬崖行走问题中效果良好，因为这两个问题在状态和动作空间中都是低维的。那么，我们能将此方法应用于高维问题吗？答案是否定的，因为存在*维度灾难*，使得存储大量的状态和动作变得不可行。此外，在连续控制问题中，动作作为一个实数变化，虽然可能有无限多个实数值，这些值无法表示为表格型的*Q*数组。这就催生了强化学习中的函数逼近方法，尤其是使用深度神经网络（DNNs）的方法——也就是DQN。在这里，*Q(s,a)*被表示为一个DNN，输出的是*Q*的值。
- en: 'The following are the steps that are involved in a DQN:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是DQN涉及的步骤：
- en: 'Update the state-action value function using a Bellman equation, where (*s,
    a*) are the states and actions at a time, *t*, *s''* and *a''* are respectively
    the states and actions at the subsequent time *t+1, *and *γ* is the discount factor:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Bellman方程更新状态-动作价值函数，其中(*s, a*)是某时刻的状态和动作，*t*、*s'*和*a'*分别是下一个时刻*t+1*的状态和动作，*γ*是折扣因子：
- en: '![](img/5766505f-bb33-4f83-9824-7a86ff366bd4.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5766505f-bb33-4f83-9824-7a86ff366bd4.png)'
- en: 'We then define a loss function at iteration step *i* to train the Q-network
    as follows:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们在第*i*步定义一个损失函数来训练Q网络，如下所示：
- en: '![](img/ed6fd3dd-bb31-457d-919a-312b4088734c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed6fd3dd-bb31-457d-919a-312b4088734c.png)'
- en: The preceding parameters are are the neural network parameters, which are represented
    as *θ*, hence the Q-value is written as *Q(s, a; θ).*
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 上述参数是神经网络参数，用*θ*表示，因此Q值写作*Q(s, a; θ)*。
- en: '*y[i]* is the target for iteration *i,* and is given by the following equation:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*y[i]*是第*i*次迭代的目标，给定的方程如下：'
- en: '![](img/e97f346e-e71f-4954-bba0-58dd07edb1a2.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e97f346e-e71f-4954-bba0-58dd07edb1a2.png)'
- en: We then train the neural network on the DQN by minimizing this loss function
    *L(θ)* using optimization algorithms, such as gradient descent, RMSprop, and Adam.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们通过最小化损失函数*L(θ)*，使用优化算法如梯度下降、RMSprop和Adam来训练DQN神经网络。
- en: We used the least squared loss previously for the DQN loss function, also referred
    to as the L2 loss. You can also consider other losses, such as the Huber loss,
    which combines the L1 and L2 losses, with the L2 loss in the vicinity of zero
    and L1 in regions far away. The Huber loss is less sensitive to outliers than
    the L2 loss.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前使用了最小二乘损失来计算DQN的损失函数，也叫做L2损失。你也可以考虑其他损失函数，如Huber损失，它结合了L1和L2损失，L2损失在接近零的地方，而L1损失在远离零的区域。Huber损失对异常值比L2损失更不敏感。
- en: We will now look at the use of target networks. This is a very important concept,
    required to stabilize training.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在来看一下目标网络的使用。这是一个非常重要的概念，对于稳定训练是必不可少的。
- en: Understanding target networks
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解目标网络
- en: An interesting feature of a DQN is the utilization of a second network during
    the training procedure, which is referred to as the target network. This second
    network is used for generating the target-Q values that are used to compute the
    loss function during training. Why not use just use one network for both estimations,
    that is, for choosing the action *a* to take, as well as updating the Q-network?
    The issue is that, at every step of training, the Q-network's values change, and
    if we use a constantly changing set of values to update our network, then the
    estimations can easily become unstable – the network can fall into feedback loops
    between the target and estimated Q-values. In order to mitigate this instability,
    the target network's weights are fixed – that is, slowly updated to the primary
    Q-network's values. This leads to training that is far more stable and practical.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DQN的一个有趣特点是在训练过程中使用第二个网络，称为目标网络。这个第二个网络用于生成目标-Q值，这些值在训练过程中用于计算损失函数。为什么不只用一个网络进行两个估算，也就是说既用来选择行动*a*，又用来更新Q网络呢？问题在于，在训练的每一步，Q网络的值都会发生变化。如果我们使用不断变化的值来更新网络，那么估算可能很容易变得不稳定——网络可能会在目标Q值和估算Q值之间进入反馈回路。为了减少这种不稳定性，目标网络的权重是固定的——即，慢慢地更新为主Q网络的值。这使得训练变得更加稳定和实际。
- en: 'We have a second neural network, which we will refer to as the target network.
    It is identical in architecture to the primary Q-network, although the neural
    network parameter values are different. Once every *N* steps, the parameters are
    copied from the Q-network to the target network. This results in stable training.
    For example, *N* = 10,000 steps can be used. Another option is to slowly update
    the weights of the target network (here, *θ* is the Q-network''s weights, and
    *θ^t* is the target network''s weights):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有第二个神经网络，我们称之为目标网络。它的架构与主Q网络相同，尽管神经网络的参数值不同。每经过*N*步，参数会从Q网络复制到目标网络。这会导致训练更加稳定。例如，可以使用*N*
    = 10,000步。另一种选择是慢慢更新目标网络的权重（这里，*θ*是Q网络的权重，*θ^t*是目标网络的权重）：
- en: '![](img/17b0450e-4576-4258-b434-66eced448b6d.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17b0450e-4576-4258-b434-66eced448b6d.png)'
- en: Here, *τ* is a small number, say, 0.001\. This latter approach of using an exponential
    moving average is the preferred choice in this book.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*τ*是一个小的数值，比如0.001。使用指数移动平均的方法是本书推荐的选择。
- en: Let's now learn about the use of replay buffer in off-policy algorithms.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们了解一下在离策略算法中回放缓冲区的使用。
- en: Learning about replay buffer
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解回放缓冲区
- en: We need the tuple (`s`, `a`, `r`, `s'`, `done`) for updating the DQN, where
    `s` and `a` are respectively the state and actions at time `t`; `s'` is the new
    state at time *t+1*; and `done` is a Boolean value that is `True` or `False` depending
    on whether the episode is not completed or has ended, also referred to as the
    terminal value in the literature. This Boolean `done` or `terminal` variable is
    used so that, in the Bellman update, the last terminal state of an episode is
    properly handled (since we cannot do an *r + γ max Q(s',a')* for the terminal
    state). One problem in DQNs is that we use contiguous samples of the (`s`, `a`,
    `r`, `s'`, `done`) tuple, they are correlated, and so the training can overfit.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要元组（`s`，`a`，`r`，`s'`，`done`）来更新DQN，其中`s`和`a`分别是时间`t`的状态和动作；`s'`是时间*t+1*的新状态；`done`是一个布尔值，根据回合是否结束，它是`True`或`False`，也就是文献中所称的终止值。这个布尔变量`done`或`terminal`用于在Bellman更新中正确处理回合的最后终止状态（因为我们无法对终止状态做`*r
    + γ max Q(s',a')*`）。DQNs中的一个问题是，我们使用连续的（`s`，`a`，`r`，`s'`，`done`）元组样本，这些样本是相关的，因此训练可能会过拟合。
- en: To mitigate this issue, a replay buffer is used, where the tuple (`s`, `a`,
    `r`, `s'`, `done`) is stored from experience, and a mini-batch of such experiences
    are randomly sampled from the replay buffer and used for training. This ensures
    that the samples drawn for each mini-batch are **independent and identically distributed**
    (**IID**). Usually, a large-size replay buffer is used, say, 500,000 to 1 million
    samples. At the beginning of the training, the replay buffer is filled to a sufficient
    number of samples and populated with new experiences. Once the replay buffer is
    filled to a maximum number of samples, the older samples are discarded one by
    one. This is because the older samples were generated from an inferior policy,
    and are not desired for training at a later stage as the agent has advanced in
    its learning.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这个问题，使用了回放缓冲区，其中元组（`s`，`a`，`r`，`s'`，`done`）从经验中存储，且从回放缓冲区中随机采样一个这样的经验小批量用于训练。这确保了每个小批量采样的样本是**独立同分布**（**IID**）的。通常使用较大的回放缓冲区，例如500,000到100万个样本。在训练开始时，回放缓冲区会填充到足够数量的样本，并且会随着新经验的到来而更新。一旦回放缓冲区填满到最大样本数量，旧样本会被一个一个地丢弃。这是因为旧样本是由劣质策略生成的，在后期训练时不再需要，因为智能体在学习过程中已经进步。
- en: In a more recent paper, DeepMind came up with a prioritized replay buffer, where
    the absolute value of the temporal difference error is used to give importance
    to a sample in the buffer. Thus, samples with higher errors have a higher priority
    and so have a bigger chance of being sampled. This prioritized replay buffer results
    in faster learning than the vanilla replay buffer. However, it is slightly harder
    to code, as it uses a SumTree data structure, which is a binary tree where the
    value of every parent node is the sum of the values of its two child nodes. This
    prioritized experience replay will not be discussed further for now!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的一篇论文中，DeepMind提出了一种优先回放缓冲区，其中使用时间差分误差的绝对值来为缓冲区中的样本赋予重要性。因此，误差较大的样本具有更高的优先级，因此被采样的几率更大。这种优先回放缓冲区比普通的回放缓冲区学习更快。然而，它的编码稍微困难一些，因为它使用了SumTree数据结构，这是一个二叉树，其中每个父节点的值是其两个子节点值的总和。此优先经验回放将在此处不再深入讨论！
- en: 'The prioritized experience replay buffer is based on this DeepMind paper: [https://arxiv.org/abs/1511.05952](https://arxiv.org/abs/1511.05952)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 优先经验回放缓冲区基于这篇DeepMind论文：[https://arxiv.org/abs/1511.05952](https://arxiv.org/abs/1511.05952)
- en: We will now look into the Atari environment. If you like playing video games,
    you will love this section!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将研究Atari环境。如果你喜欢玩视频游戏，你一定会喜欢这一部分！
- en: Getting introduced to the Atari environment
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Atari 环境
- en: The Atari 2600 game suite was originally released in the 1970s, and was a big
    hit at that time. It involves several games that are played by users using the
    keyboard to enter actions. These games were a big hit back in the day, and inspired
    many computer game players of the 1970s and 1980s, but are considered too primitive
    by today's video game players' standards. However, they are popular today in the
    RL community as a portal to games that can be trained by RL agents.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Atari 2600 游戏套件最早于1970年代发布，并在当时非常火爆。它包括几个游戏，用户通过键盘输入动作进行游戏。这些游戏在当时非常受欢迎，激励了1970年代和1980年代的许多电脑游戏玩家，但按照今天视频游戏玩家的标准来看，它们显得过于原始。然而，它们今天在强化学习（RL）社区中仍然很受欢迎，因为它们是可以被RL智能体训练的游戏入口。
- en: Summary of Atari games
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Atari游戏总结
- en: Here is a summary of a select few games from Atari (we won't present screenshots
    of the games for copyright reasons, but will provide links to them).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是Atari部分精选游戏的总结（由于版权原因，我们不会展示游戏截图，但会提供链接）。
- en: Pong
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pong
- en: Our first example is a ping pong game called Pong, which allows the user to
    move up or down to hit a ping pong ball to an opponent, which is the computer.
    The first one to score 21 points is the winner of the game. A screenshot of the
    Pong game from Atari can be found at [https://gym.openai.com/envs/Pong-v0/](https://gym.openai.com/envs/Pong-v0/).
    [](https://gym.openai.com/envs/Pong-v0/)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个例子是一个叫做Pong的乒乓球游戏，用户可以上下移动击打乒乓球，将其击打给对方，另一方是计算机。首先得到21分的人将赢得比赛。可以在[https://gym.openai.com/envs/Pong-v0/](https://gym.openai.com/envs/Pong-v0/)找到来自Atari的Pong游戏截图。
    [](https://gym.openai.com/envs/Pong-v0/)
- en: Breakout
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Breakout
- en: In another game, called Breakout, the user must move a paddle to the left or
    right to hit a ball that then bounces off a set of blocks at the top of the screen.
    The higher the number of blocks hit, the more points or rewards the player can
    accrue. There are a total of five lives per game, and if the player misses the
    ball, it results in the loss of a life. A screenshot of the Breakout game from
    Atari can be found at [https://gym.openai.com/envs/Breakout-v0/](https://gym.openai.com/envs/Breakout-v0/).
    [](https://gym.openai.com/envs/Breakout-v0/)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个游戏中，叫做Breakout，用户必须左右移动挡板来击打球，球随后会弹跳并撞击屏幕顶部的方块。击中方块的数量越多，玩家能够获得的积分或奖励就越高。每局游戏总共有五条生命，如果玩家未能击中球，就会失去一条生命。可以在[https://gym.openai.com/envs/Breakout-v0/](https://gym.openai.com/envs/Breakout-v0/)找到来自Atari的Breakout游戏截图。
    [](https://gym.openai.com/envs/Breakout-v0/)
- en: Space Invaders
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Space Invaders
- en: If you like shooting space aliens, then Space Invaders is the game for you.
    In this game, wave after wave of space aliens descend from the top, and the goal
    is to shoot them using a laser beam, accruing points. The link to this can be
    found at [https://gym.openai.com/envs/SpaceInvaders-v0/](https://gym.openai.com/envs/SpaceInvaders-v0/).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢射击外星人，那么Space Invaders就是你的游戏。在这个游戏中，一波接一波的外星人从顶部降下，目标是用激光束射击它们，获得积分。你可以在[https://gym.openai.com/envs/SpaceInvaders-v0/](https://gym.openai.com/envs/SpaceInvaders-v0/)找到该游戏的链接。
- en: LunarLander
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LunarLander
- en: Or, if you are fascinated by space travel, then LunarLander is about landing
    a spacecraft (which resembles the Apollo 11 Eagle) on the surface of the moon.
    For each level, the surface of the landing zone changes and the goal is to guide
    the spacecraft to land on the lunar surface between two flags. A screenshot of
    LunarLander from Atari can be found at [https://gym.openai.com/envs/LunarLander-v2/](https://gym.openai.com/envs/LunarLander-v2/).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你对太空旅行感兴趣，那么LunarLander是关于将太空船（类似于阿波罗11号的“鹰”号）着陆在月球表面。在每个关卡中，着陆区的表面会发生变化，目标是引导太空船在两个旗帜之间的月球表面着陆。可以在[https://gym.openai.com/envs/LunarLander-v2/](https://gym.openai.com/envs/LunarLander-v2/)找到LunarLander的Atari截图。
- en: The Arcade Learning Environment
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经典街机学习环境
- en: Over 50 such games exist in Atari. They are now part of the **Arcade Learning
    Environment** (**ALE**), which is an object-oriented framework built on top of
    Atari. OpenAI's gym is used to invoke Atari games these days so that RL agents
    can be trained to play these games. For instance, you can import `gym` in Python
    and play them as follows.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Atari上有超过50款此类游戏，它们现在是**经典街机学习环境**（**ALE**）的一部分，ALE是一个面向对象的框架，建立在Atari之上。如今，OpenAI的gym被用于调用Atari游戏，以便训练强化学习（RL）代理来玩这些游戏。例如，你可以在Python中导入`gym`并按如下方式进行游戏。
- en: 'The `reset()` function resets the game environment, and `render()` renders
    the screenshot of the game:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset()`函数重置游戏环境，`render()`函数渲染游戏截图：'
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We will now code a DQN in TensorFlow and Python to train an agent on how to
    play Atari Breakout.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用TensorFlow和Python编写一个DQN，来训练一个代理如何玩Atari Breakout。
- en: Coding a DQN in TensorFlow
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用TensorFlow编写DQN
- en: 'Here, we will code a DQN using TensorFlow and play Atari Breakout. There are
    three Python files that we will use:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用TensorFlow编写一个DQN并玩Atari Breakout。我们将使用三个Python文件：
- en: '`dqn.py`: This file will have the main loop, where we explore the environment
    and call the update functions'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dqn.py`：这个文件将包含主循环，在这里我们将探索环境并调用更新函数。'
- en: '`model.py`: This file will have the class for the DQN agent, where we will
    have the neural network and the functions we require to train it'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.py`：这个文件将包含DQN代理的类，在这里我们将构建神经网络并实现训练所需的函数。'
- en: '`funcs.py`: This file will involve some utility functions—for example, to process
    the image frames, or to populate the replay buffer'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`funcs.py`：此文件将涉及一些工具函数——例如，用于处理图像帧或填充重放缓冲区'
- en: Using the model.py file
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 model.py 文件
- en: 'Let''s first code the `model.py` file. The steps involved in this are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先编写 `model.py` 文件。相关步骤如下：
- en: '**Import the required packages**:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入所需的包**：'
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Choose the** **bigger** **or** **smaller** **network**: We will use two neural
    network architectures, one called `bigger` and the other `smaller`. Let''s use
    the `bigger` network for now; the interested user can later change the network
    to the `smaller` option and compare performance:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择** **更大** **或** **更小** **的网络**：我们将使用两种神经网络架构，一种称为 `bigger`，另一种称为 `smaller`。暂时使用
    `bigger` 网络，感兴趣的用户可以稍后将网络更改为 `smaller` 选项并比较性能：'
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Choose the** **loss** **function (L2 loss or the Huber loss)**: For the Q-learning
    `loss` function, we can use either the L2 loss or the Huber loss. Both options
    will be used in the code. We will choose `huber` for now:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择** **损失** **函数（L2 损失或 Huber 损失）**：对于 Q-learning 的`loss`函数，我们可以选择 L2 损失或
    Huber 损失。两者都会在代码中使用。我们暂时选择`huber`：'
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Define neural network weights initialization**: We will then specify a weights
    initializer for the neural network weights. `tf.variance_scaling_initializer(scale=2)`
    is used for He initialization. Xavier initialization can also be used, and is
    provided as a comment. The interested user can compare the performance of both
    the He and Xavier initializers later:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义神经网络权重初始化**：接下来，我们将为神经网络的权重指定一个初始化器。`tf.variance_scaling_initializer(scale=2)`
    用于 He 初始化。也可以使用 Xavier 初始化，并且已提供为注释。感兴趣的用户稍后可以比较 He 和 Xavier 初始化器的性能：'
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Define the** **QNetwork()** **class**: We will then define the `QNetwork()`
    class as follows. It will have an `__init__()` constructor and the `_build_model()`,
    `predict()`, and `update()` functions. The `__init__` constructor is shown as
    follows:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义** **QNetwork()** **类**：然后我们将按如下方式定义 `QNetwork()` 类。它将包含一个 `__init__()`
    构造函数，以及 `_build_model()`、`predict()` 和 `update()` 函数。`__init__` 构造函数如下所示：'
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Complete the** **_build_model()** **function**: `In _build_model()`, we first
    define the TensorFlow `tf_X, tf_Y`, and `tf_actions` placeholders. Note that the
    image frames are stored in `uint8` format in the replay buffer to save memory,
    and so they are normalized by converting them to `float` and then dividing them
    by `255.0` to put the `X` input in the 0-1 range:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**完成** **_build_model()** **函数**：在 `_build_model()` 中，我们首先定义 TensorFlow 的 `tf_X`、`tf_Y`
    和 `tf_actions` 占位符。请注意，图像帧在重放缓冲区中以 `uint8` 格式存储，以节省内存，因此它们通过转换为 `float` 并除以 `255.0`
    来进行归一化，以将 `X` 输入值压缩到 0-1 范围内：'
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '**Defining convolutional layers**: As mentioned earlier, we have a `bigger`
    and a `smaller` neural network option. The `bigger` network has three convolutional
    layers, followed by a fully connected layer. The `smaller` network only has two
    convolutional layers, followed by a fully connected layer. We can define convolutional
    layers in TensorFlow using `tf.contrib.layers.conv2d()`, and fully connected layers
    using `tf.contrib.layers.fully_connected()`. Note that, after the last convolutional
    layer, we need to flatten the output before passing it to the fully connected
    layer, for which we will use `tf.contrib.layers.flatten()`. We use the `winit`
    object as our weights initializer, which we defined earlier:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义卷积层**：如前所述，我们有两个神经网络选项，`bigger` 和 `smaller`。`bigger` 网络有三个卷积层，后面跟着一个全连接层。`smaller`
    网络只有两个卷积层，后面跟着一个全连接层。我们可以使用 `tf.contrib.layers.conv2d()` 定义卷积层，使用 `tf.contrib.layers.fully_connected()`
    定义全连接层。请注意，在最后一个卷积层之后，我们需要先将输出展平，然后再传递给全连接层，展平操作将使用 `tf.contrib.layers.flatten()`。我们使用之前定义的
    `winit` 对象作为权重初始化器：'
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '**Defining the fully connected layer**: Finally, we have a fully connected
    layer sized according to the number of actions, which is specified using `len(self.VALID_ACTIONS)`.
    The output of this last fully connected layer is stored in `self.predictions`,
    and represents *Q(s,a)*, which we saw in the equations presented earlier, in the
    *Learning the theory behind a DQN* section. The actions we pass to this function
    (`self.tf_actions`) have to be converted to one-hot format, for which we use `tf.one_hot()`.
    Note that `one_hot` is a way to represent the action number as a binary array
    with zero for all actions, except for one action, for which we store *a* as `1.0`.
    Then, we multiply the predictions with the one-hot actions using `self.predictions
    * action_one_hot`, which is summed over using `tf.reduce_sum()`; this is stored
    in the `self.action_predictions` variable:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义全连接层**：最后，我们有一个全连接层，其大小根据动作的数量来确定，使用`len(self.VALID_ACTIONS)`来指定。这个最后的全连接层的输出存储在`self.predictions`中，表示*Q(s,a)*，我们在之前的*学习DQN背后的理论*部分的方程中看到过。传递给这个函数的动作（`self.tf_actions`）需要转换为独热编码格式，我们使用`tf.one_hot()`。请注意，`one_hot`是一种表示动作编号的方式，它将所有动作的值设为零，除了某个动作外，该动作对应的值为`1.0`。然后，我们使用`self.predictions
    * action_one_hot`将预测与独热编码的动作相乘，最后使用`tf.reduce_sum()`对其求和；这被存储在`self.action_predictions`变量中：'
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Computing loss for training the Q-network**: We compute the loss for training
    the Q-network, stored in `self.loss`, using either the L2 loss or the Huber loss,
    which is determined using the `LOSS` variable. For L2 loss, we use the `tf.squared_difference()`
    function; for the Huber loss, we use `huber_loss()`, which we will soon define.
    The loss is averaged over many samples, and for this we use the `tf.reduce_mean()`
    function. Note that we will compute the loss between the `tf_y` placeholder that
    we defined earlier and the `action_predictions` variable that we obtained in the
    previous step:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算训练Q网络的损失**：我们通过使用L2损失或Huber损失来计算训练Q网络的损失，存储在`self.loss`中，损失的类型由`LOSS`变量决定。对于L2损失，我们使用`tf.squared_difference()`函数；对于Huber损失，我们使用`huber_loss()`，我们很快会定义它。损失会在多个样本上取平均，为此我们使用`tf.reduce_mean()`函数。请注意，我们将计算之前定义的`tf_y`占位符和前一步获得的`action_predictions`变量之间的损失：'
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Using the optimizer**: We use either the RMSprop or Adam optimizer, and store
    it in `self.optimizer`. Our learning objective is to minimize `self.loss`, and
    so we use `self.optimizer.minimize()`. This is stored in `self.train_op`:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用优化器**：我们使用RMSprop或Adam优化器，并将其存储在`self.optimizer`中。我们的学习目标是最小化`self.loss`，因此我们使用`self.optimizer.minimize()`。这被存储在`self.train_op`中：'
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Define the** **predict()** **function for the class**: In the `predict()`
    function, we run the `self.predictions` function defined earlier using TensorFlow''s
    `sess.run()`, where `sess` is the `tf.Session()` object that is passed to this
    function. The states are passed as an argument to this function in the `s` variable,
    which is passed on to the TensorFlow placeholder, `tf_X`:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为类定义** **predict()** **函数**：在`predict()`函数中，我们使用TensorFlow的`sess.run()`运行之前定义的`self.predictions`函数，其中`sess`是传递给此函数的`tf.Session()`对象。状态作为参数传递给此函数，存储在`s`变量中，之后传递给TensorFlow占位符`tf_X`：'
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Define the** **update()** **function for the class**: Finally, in the `update()`
    function, we call the `train_op` and `loss` objects, and feed the `a` dictionary
    to the placeholders involved in performing these operations, which we call `feed_dict`.
    The states are stored in `s`, the actions in `a`, and the targets in `y`:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为类定义** **update()** **函数**：最后，在`update()`函数中，我们调用`train_op`和`loss`对象，并将字典`a`传递给参与执行这些操作的占位符，我们称之为`feed_dict`。状态存储在`s`中，动作存储在`a`中，目标存储在`y`中：'
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Define the** **huber_loss()** **function outside the class**: The last thing
    to complete `model.py` is the definition of the Huber loss function, which is
    a blend of L1 and L2 losses. Whenever the input is `< 1.0`, the L2 loss is used,
    and the L1 loss otherwise:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在类外定义** **huber_loss()** **函数**：完成`model.py`的最后一项任务是定义Huber损失函数，它是L1和L2损失的结合。当输入小于`1.0`时，使用L2损失，其他情况使用L1损失：'
- en: '[PRE13]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Using the funcs.py file
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用`funcs.py`文件
- en: 'We will next code `funcs.py` by completing the following steps:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过完成以下步骤来编写`funcs.py`：
- en: '**Import packages**: First, we import the required packages:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入包**：首先，我们导入所需的包：'
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '**Complete the** **ImageProcess()** **class**: Then, convert the 210 x 160
    x 3 RGB image from the Atari emulator to an 84 x 84 grayscale image. For this,
    we create an `ImageProcess()` class and use TensorFlow utility functions, such
    as `rgb_to_grayscale()` to convert RGB to grayscale, `crop_to_bounding_box()`
    to crop the image to the region of interest, `resize_images()` to resize the image
    to the desired 84 x 84 size, and `squeeze()` to remove a dimension from the input.
    The `process()` function of the class will carry out the operations by invoking
    the `sess.run()` function on `self.output`; note that we pass the `state` variable
    as a dictionary:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**完成** **ImageProcess()** **类**：接下来，将来自Atari模拟器的210 x 160 x 3 RGB图像转换为84 x
    84的灰度图像。为此，我们创建了一个`ImageProcess()`类，并使用TensorFlow的实用函数，如`rgb_to_grayscale()`将RGB转换为灰度，`crop_to_bounding_box()`将图像裁剪到感兴趣的区域，`resize_images()`将图像调整为所需的84
    x 84大小，以及`squeeze()`去除输入中的维度。该类的`process()`函数通过调用`self.output`上的` sess.run()`函数来执行这些操作；请注意，我们将`state`变量作为字典传递。'
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Copy model parameters from one network to another**: The next step is to
    write a function called `copy_model_parameters()`, which will take as arguments
    the `tf.Session()` object `sess`, and two networks (in this case, the Q-network
    and the target network). Let''s call them `qnet1` and `qnet2`. The function will
    copy the parameter values from `qnet1` to `qnet2`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从一个网络复制模型参数到另一个网络**：下一步是编写一个名为`copy_model_parameters()`的函数，该函数将接受`tf.Session()`对象`sess`和两个网络（在本例中为Q网络和目标网络）作为参数。我们将它们命名为`qnet1`和`qnet2`。该函数将把`qnet1`的参数值复制到`qnet2`：'
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Write a function to use ε-greedy strategy to explore or exploit**: We will
    then write a function called `epsilon_greedy_policy()`, which will either explore
    or exploit depending on whether a random real number computed using NumPy''s `np.random.rand()`
    is less than `epsilon`, the parameter described earlier for the ε-greedy strategy.
    For exploration, all actions have equal probabilities and equal one/`(num_actions)`,
    where `num_actions` is the number of actions (which is four for Breakout). On
    the other hand, for exploiting, we use Q-network''s `predict()` function to obtain
    Q values and identify which action has the highest Q value with the use of NumPy''s
    `np.argmax()` function. The output of this function is the probability of each
    of the actions, which, for exploitation, will have all `0` actions except the
    one action corresponding to the largest Q value, for which the probability is
    assigned `1.0`:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编写一个使用 ε-贪婪策略进行探索或开发的函数**：我们将编写一个名为`epsilon_greedy_policy()`的函数，该函数将根据通过NumPy的`np.random.rand()`计算得到的随机实数是否小于`epsilon`（之前描述的ε-贪婪策略参数）来决定是进行探索还是开发。对于探索，所有的动作具有相等的概率，每个动作的概率为`1/(num_actions)`，其中`num_actions`是动作的数量（在Breakout游戏中为四个）。另一方面，对于开发，我们使用Q网络的`predict()`函数获取Q值，并通过NumPy的`np.argmax()`函数确定具有最高Q值的动作。该函数的输出是每个动作的概率，在开发的情况下，除了与最大Q值对应的那个动作，其他动作的概率都为`0`，对应的最大Q值动作的概率被赋为`1.0`：'
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Write a function to populate the replay memory**: Finally, we will write
    the `populate_replay_mem` function to populate the replay buffer with `replay_memory_init_size`
    number of samples. First, we reset the environment using `env.reset()`. Then,
    we process the state obtained from the reset. We need four frames for each state,
    as the agent otherwise has no way of determining which way the ball or the paddle
    are moving, their speed and/or acceleration (in the Breakout game; for other games,
    such as Space Invaders, similar reasoning applies to determine when and where
    to fire). For the first frame, we stack up four copies. We also compute `delta_epsilon`,
    which is the amount epsilon is decreased per time step. The replay memory is initialized
    as an empty list:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编写一个函数来填充回放记忆**：最后，我们将编写`populate_replay_mem`函数，用于将`replay_memory_init_size`数量的样本填充到回放缓冲区。首先，我们使用`env.reset()`重置环境。然后，我们处理从重置中获得的状态。每个状态需要四帧，因为代理否则无法判断球或挡板的运动方向、速度和/或加速度（在Breakout游戏中；对于其他游戏，如《太空侵略者》，类似的推理也适用于确定何时以及在哪里开火）。对于第一帧，我们堆叠四个副本。我们还计算`delta_epsilon`，即每个时间步长ε的减少量。回放记忆被初始化为空列表：'
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Computing action probabilities**: Then, we loop over `replay_memory_init_size`
    four times, decrease epsilon by `delta_epsilon`, and compute the action probabilities,
    stored in the `action_probs` variable, using `policy()`, which was passed as an
    argument. The exact action is determined from the `action_probs` variable by sampling
    using NumPy''s `np.random.choice`. Then, `env.render()` renders the environment,
    and then we pass the action to `env.step()`, which outputs the next state (stored
    in `next_state`), the reward for the transition, and whether the episode terminated,
    which is stored in the Boolean `done` variable.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算动作概率**：然后，我们将循环`replay_memory_init_size`四次，按`delta_epsilon`减少epsilon，并使用传入的`policy()`计算动作概率，这些概率存储在`action_probs`变量中。通过使用NumPy的`np.random.choice`从`action_probs`变量中采样确定具体的动作。然后，`env.render()`渲染环境，接着将动作传递给`env.step()`，它输出下一个状态（存储在`next_state`中）、该转移的奖励，以及该回合是否结束，这些信息存储在布尔变量`done`中。'
- en: '**Append to replay buffer**: We then process the next state and append it to
    the replay memory, the tuple (`state`, `action`, `reward`, `next_state`, `done`).
    If the episode is done, we reset the environment to a new round of the game, process
    the image and stack up four times, as done earlier. If the episode is not yet
    complete, the new state becomes the current state for the next time step, and
    we proceed this way on and on until the loop finishes:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**追加到重放缓冲区**：我们接着处理下一个状态，并将其追加到重放记忆中，格式为元组（`state`、`action`、`reward`、`next_state`、`done`）。如果回合结束，我们将重置环境，进入新一轮游戏，处理图像并像之前一样叠加四次。如果回合尚未结束，新的状态将成为下一个时间步的当前状态，我们将继续这样进行，直到循环结束：'
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This completes `funcs.py`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了`funcs.py`。
- en: Using the dqn.py file
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用dqn.py文件
- en: 'We will first code the `dqn.py` file. This requires the following steps:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先编写`dqn.py`文件。这需要以下步骤：
- en: '**Import the necessary packages**: We will import the required packages as
    follows:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入必要的包**：我们将按如下方式导入所需的包：'
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Set the game and choose the valid actions**: We will then set the game. Let''s
    choose the `BreakoutDeterministic-v4` game for now, which is a later version of
    Breakout v0\. This game has four actions, numbered zero to three, and they represent
    `0`: no-operation (`noop`), `1`: `fire`, `2`: move left, and `3`: move right:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置游戏并选择有效的动作**：然后，我们将设置游戏。现在选择`BreakoutDeterministic-v4`游戏，它是Breakout v0的后续版本。该游戏有四个动作，编号从零到三，分别表示`0`：无操作（`noop`）、`1`：开火、`2`：向左移动、`3`：向右移动：'
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Set the mode (train/test) and the start iterations**: We will then set the
    mode in the `train_or_test` variable. Let''s start with `train` to begin with
    (you can later set it to `test` to evaluate the model after the training is complete).
    We will also train from scratch from the `0` iteration:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置模式（训练/测试）和初始迭代次数**：我们接下来将设置`train_or_test`变量中的模式。首先我们选择`train`开始（训练完成后，你可以将其设置为`test`来评估模型）。我们还将从`0`迭代开始训练：'
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Create environment**: We will create the environment `env` object, which
    will create the `GAME` game. `env.action_space.n` will print the number of actions
    in this game. `env.reset()` will reset the game and output the initial state/observation
    (note that state and observation in RL parlance are the same and are interchangeable).
    `observation.shape` will print the shape of the state space:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建环境**：我们将创建`env`环境对象，它将创建`GAME`游戏。`env.action_space.n`将打印该游戏中的动作数量。`env.reset()`将重置游戏并输出初始状态/观察（请注意，在强化学习术语中，状态和观察是相同的，可以互换）。`observation.shape`将打印状态空间的形状：'
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Create paths and directories for storing checkpoint files**: We will then
    create the paths for storing the checkpoint model files and create the directory:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建存储检查点文件的路径和目录**：接下来，我们将创建存储检查点模型文件的路径并创建目录：'
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Define the** **deep_q_learning()** **function**: We will next create the
    `deep_q_learning()` function, which will take a long list of arguments that involve
    the TensorFlow session object, the environment, the *Q* and target network objects,
    and so on. The policy to be followed is `epsilon_greedy_policy()`:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义** **deep_q_learning()** **函数**：接下来我们将创建`deep_q_learning()`函数，该函数将接受一个包含多个参数的长列表，涉及TensorFlow会话对象、环境、*Q*和目标网络对象等。要遵循的策略是`epsilon_greedy_policy()`：'
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**Populate the replay memory with experiences encountered with initial random
    actions**: Then, we populate the replay memory with the initial samples:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**用初始随机动作遇到的经验填充重放记忆**：然后，我们用初始样本填充重放记忆：'
- en: '[PRE27]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '**Set the epsilon values**: Next, we will set the `epsilon` values. Note that
    we have a double linear function, which will decrease the value of `epsilon`,
    first from 1 to 0.1, and then from 0.1 to 0.01, in as many steps, specified in
    `epsilon_decay_steps`:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置epsilon值：** 接下来，我们将设置`epsilon`值。注意，我们有一个双线性函数，它将首先把`epsilon`的值从1减少到0.1，然后从0.1减少到0.01，步数由`epsilon_decay_steps`指定：'
- en: '[PRE28]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will then set the total number of time steps:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将设置总的时间步数：
- en: '[PRE29]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, the main loop starts over the episodes from the start to the total number
    of episodes. We reset the episode, process the first frame, and stack it up `4`
    times. Then, we will initialize `loss`, `time_steps`, and `episode_rewards` to
    `0`. The total number of lives per episode for Breakout is `5`, and so we keep
    count of it in the `ale_lives` variable. The total number of time steps in this
    life of the agent is initialized to a large number:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，主循环从开始到总集数开始遍历每一集。我们重置集数，处理第一帧，并将其堆叠`4`次。接着，我们将`loss`、`time_steps`和`episode_rewards`初始化为`0`。对于Breakout游戏，每集的总生命数为`5`，因此我们在`ale_lives`变量中跟踪这一数据。该代理在这一生命阶段的总时间步数初始化为一个较大的数字：
- en: '[PRE30]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '**Keeping track of time steps:** We will use an inner `while` loop to keep
    track of the time steps in a given episode (note: the outer `for` loop is over
    episodes, and this inner `while` loop is over time steps in the current episode).
    We will decrease `epsilon` accordingly, depending on whether it is in the 0.1
    to 1 range or in the 0.01 to 0.1 range, both of which have different `delta_epsilon`
    values:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**跟踪时间步数：** 我们将使用一个内部`while`循环来跟踪给定集中的时间步数（注意：外部`for`循环是针对集数的，而这个内部`while`循环是针对当前集中的时间步数的）。我们将根据`epsilon`的值是否处于0.1到1的范围内，或0.01到0.1的范围内，来相应地减少`epsilon`，两者有不同的`delta_epsilon`值：'
- en: '[PRE31]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '**Updating the target network:** We update the target network if the total
    number of time steps so far is a multiple of `update_target_net_every`, which
    is a user-defined parameter. This is accomplished by calling the `copy_model_parameters()`
    function:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新目标网络：** 如果迄今为止的总时间步数是`update_target_net_every`的倍数，我们将更新目标网络，这个值是一个用户定义的参数。这通过调用`copy_model_parameters()`函数来实现：'
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: At the start of every new life of the agent, we undertake a no-op (corresponding
    to action probabilities [1, 0, 0, 0]) a random number of times between zero and
    seven to make the episode different from past episodes, so that the agent gets
    to see more variations when it explores and learns the environment. This was also
    done in the original DeepMind paper, and ensures that the agent learns better,
    since this randomness will ensure that more diversity is experienced. Once we
    are outside this initial randomness cycle, the actions are taken as per the `policy()`
    function.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次代理的新生命开始时，我们进行一次无操作（对应动作概率[1, 0, 0, 0]），次数是零到七之间的随机数，以使得该集与过去的集不同，这样代理在探索和学习环境时能看到更多的变化。这在原始的DeepMind论文中也有采用，确保代理能够更好地学习，因为这种随机性确保了体验到更多的多样性。一旦我们脱离了这个初始的随机性循环，接下来的动作将按照`policy()`函数执行。
- en: 'Note that we still need to take one fire operation (action probabilities [0,
    1, 0, 0]) for one time step at the start of every new life to kick-start the agent.
    This is a requirement for the ALE framework, without which the frames will freeze.
    Thus, the life cycle evolves as a `1` fire operation, followed by a random number
    (between zero and seven) of no-ops, and then the agent uses the `policy` function:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，我们仍然需要在每次新生命开始时进行一次射击操作（对应的动作概率是[0, 1, 0, 0]），这对于启动代理是必需的。对于ALE框架来说，如果没有这一步，画面将会冻结。因此，生命周期的演变是：首先进行一次射击操作，然后进行一个随机次数（介于零到七之间）的无操作，再然后代理使用`policy`函数：
- en: '[PRE33]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will then take the action using NumPy''s `random.choice`, which will use
    the `action_probs` probabilities. Then, we render the environment and take one
    `step`. `info[''ale.lives'']` will let us know the number of lives remaining for
    the agent, from which we can ascertain whether the agent lost a life in the current
    time step. In the DeepMind paper, the rewards were set to `+1` or `-1` depending
    on the sign of the reward, so as to be able to compare the different games. This
    is accomplished using `np.sign(reward)`, which we will not use for now. We will
    then process `next_state_img` to convert to grayscale of the desired size, which
    is then appended to the `next_state` vector, which maintains a sequence of four
    contiguous frames. The rewards obtained are used to increment `episode_rewards`,
    and we also increment `time_steps`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将使用NumPy的`random.choice`根据`action_probs`的概率选择行动。接着，我们渲染环境并执行一步操作。`info['ale.lives']`将告知我们代理剩余的生命数，从而帮助我们确定代理是否在当前时间步丧失了生命。在DeepMind的论文中，奖励被设定为`+1`或`-1`，具体取决于奖励的符号，以便比较不同的游戏。这可以通过`np.sign(reward)`实现，但我们现在不使用它。然后，我们将处理`next_state_img`，将其转换为所需大小的灰度图像，接着将其追加到`next_state`向量中，该向量保持四帧连续的图像。获得的奖励将用于增加`episode_rewards`，同时我们也增加`time_steps`：
- en: '[PRE34]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Updating the networks:** Next, if we are in training mode, we update the
    networks. First, we pop the oldest element in the replay memory if the size is
    exceeded. Then, we append the recent tuple (`state`, `action`, `reward`, `next_state`,
    `done`) to the replay memory. Note that, if we have lost a life, we treat `done
    = True` in the last time step so that the agent learns to avoid losses of lives;
    without this, `done = True` is experienced only when the episode ends, that is,
    when all lives are lost. However, we also want the agent to be self-conscious
    of the loss of lives.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新网络：** 接下来，如果我们处于训练模式，我们更新网络。首先，如果回放记忆的大小超过限制，我们弹出最旧的元素。然后，我们将最近的元组（`state`，`action`，`reward`，`next_state`，`done`）追加到回放记忆中。请注意，如果我们丧失了一条生命，我们会在最后一个时间步将`done
    = True`，这样代理可以学习避免丧失生命；否则，`done = True`仅在回合结束时才会被体验到，也就是当所有生命都丧失时。然而，我们也希望代理能够意识到生命的损失。'
- en: '**Sampling a mini-batch from the replay buffer:** We sample a mini-batch from
    the replay buffer of `batch_size`. We calculate the *Q* values of the next state
    (`q_values_next`) using the target network, and use it to compute the greedy *Q*
    value, which is used to compute the target (*y* in the equation presented earlier).
    Once every four time steps, we update the Q-network using `q_net.update()`; this
    update frequency is once every four, as it is known to be more stable:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**从回放缓冲区采样一个小批次：** 我们从回放缓冲区中采样一个小批次，大小为`batch_size`。我们使用目标网络计算下一个状态的*Q*值（`q_values_next`），并利用它计算贪婪的*Q*值，该值用于计算目标（公式中之前提到的*y*）。每四个时间步更新一次Q网络，使用`q_net.update()`；这种更新频率是每四次，因为已知这样更稳定：'
- en: '[PRE35]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We exit the inner `while` loop if `done = True`, otherwise, we proceed to the
    next time step, where the state will now be `new_state` from the previous time
    step. We can also print on the screen the episode number, time steps for the episode,
    the total rewards earned in the episode, the current `epsilon`, and the replay
    buffer size at the end of the episode. These values are also useful for analysis
    later, and so we store them in a text file called `performance.txt`:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`done = True`，我们将退出内部的`while`循环，否则，我们将继续到下一个时间步，此时状态将是来自上一时间步的`new_state`。我们还可以在屏幕上打印出回合号、该回合的时间步数、回合中获得的总奖励、当前的`epsilon`以及回放缓冲区的大小。这些值对于后续分析也很有用，因此我们将它们存储在名为`performance.txt`的文本文件中：
- en: '[PRE36]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The next few lines of code will complete `dqn.py`. We reset the TensorFlow
    graph to begin with using `tf.reset_default_graph()`. Then, we create two instances
    of the `QNetwork` class, the `q_net` and `target_net` objects. We create a `state_processor`
    object of the `ImageProcess` class and also create the TensorFlow `saver` object:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来的几行代码将完成`dqn.py`。首先，我们使用`tf.reset_default_graph()`重置TensorFlow图。然后，我们创建`QNetwork`类的两个实例，分别是`q_net`和`target_net`对象。我们还创建了一个`state_processor`对象，属于`ImageProcess`类，并创建了TensorFlow的`saver`对象：
- en: '[PRE37]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We will now execute the TensorFlow graph by calling `tf.Session()`. If we are
    starting from scratch in training mode, we have to initialize the variables, which
    is accomplished by calling `sess.run()` on `tf.global_variables_initializer()`.
    Otherwise, if we are in test mode, or in training mode but not starting from scratch,
    we load the latest checkpoint file by calling `saver.restore()`.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将通过调用 `tf.Session()` 来执行 TensorFlow 图。如果我们是从头开始进行训练模式，则必须初始化变量，这可以通过调用
    `sess.run()` 执行 `tf.global_variables_initializer()` 来完成。否则，如果我们处于测试模式，或者在训练模式下但不是从头开始，则通过调用
    `saver.restore()` 来加载最新的检查点文件。
- en: 'The `replay_memory_size` parameter is limited by the size of RAM you have at
    your disposal. The present simulations were undertaken in a 16 GB RAM computer,
    where `replay_memory_size = 300000` was the limit. If the reader has access to
    more RAM, a larger value can be used for this parameter. For your information,
    DeepMind used a replay memory size of 1,000,000\. A larger replay memory size
    is good, as it helps to provide more diversity in training when a mini-batch is
    sampled:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`replay_memory_size` 参数的大小受限于可用的内存大小。本次模拟是在一台 16GB 内存的计算机上进行的，`replay_memory_size
    = 300000` 是该计算机的内存限制。如果读者有更大的内存，可以为此参数使用更大的值。供参考，DeepMind 使用了 1,000,000 的重放记忆大小。更大的重放记忆大小有助于提供更多的多样性，从而在采样迷你批次时有更好的训练效果：'
- en: '[PRE38]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: That's it—this completes `dqn.py`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样——这就完成了 `dqn.py`。
- en: We will now evaluate the performance of the DQN on Atari Breakout.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将评估 DQN 在 Atari Breakout 上的表现。
- en: Evaluating the performance of the DQN on Atari Breakout
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估 DQN 在 Atari Breakout 上的表现
- en: 'Here, we will plot the performance of our DQN algorithm on Breakout using the
    `performance.txt` file that we wrote in the code previously. We will use `matplotlib`
    to plot two graphs as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用之前在代码中编写的 `performance.txt` 文件，绘制 DQN 算法在 Breakout 上的表现。我们将使用 `matplotlib`
    绘制如下两张图：
- en: Number of time steps per episode versus episode number
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个回合的时间步数与回合数的关系
- en: Total episode reward versus time step number
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总回合奖励与时间步数的关系
- en: 'The steps involved in this are as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这其中涉及的步骤如下：
- en: '**Plot number of time steps versus episode number for Atari Breakout using
    DQN**: First, we plot the number of time steps the agent lasted per episode of
    training in the following diagram. As we can see, after about 10,000 episodes,
    the agent has learned to survive for a peak of 2,000 time steps per episode (blue
    curve). We also plot the exponentially weighted moving average with the degree
    of weighing, *α* = 0.02, in orange. The average number of time steps lasted is
    approximately 1,400 per episode at the end of the training:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**使用 DQN 绘制 Atari Breakout 的时间步数与回合数的关系图**：首先，我们在以下图表中绘制了每个训练回合中代理持续的时间步数。如图所示，经过大约
    10,000 个回合后，代理已经学会在每个回合中生存 2,000 个时间步数（蓝色曲线）。我们还绘制了使用权重系数 *α* = 0.02 的指数加权移动平均（橙色曲线）。在训练结束时，平均每个回合持续的时间步数大约为
    1,400：'
- en: '![](img/9ddf9707-46e5-42f9-ab20-162ef909927c.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ddf9707-46e5-42f9-ab20-162ef909927c.png)'
- en: 'Figure 1: Number of time steps lasted per episode for Atari Breakout using
    the DQN'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：使用 DQN 在 Atari Breakout 中每回合持续的时间步数
- en: '**Plot episode reward versus time step number**: In the following graph, we
    plot the total episode reward versus time time step for Atari Breakout using the
    DQN algorithm. As we can see, the peak episode rewards are close to 400 (blue
    curve), and the exponentially weighted moving average is approximately 160 to
    180 toward the end of the training. We used a replay memory size of 300,000, which
    is fairly small by modern standards, due to RAM limitations. If a bigger replay
    memory size was used, a higher average episode reward could be obtained. This
    is left for experimentation by the reader:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**绘制回合奖励与时间步数的关系图**：在下图中，我们绘制了使用 DQN 算法的 Atari Breakout 中，总回合奖励与时间步数的关系。如图所示，回合奖励的峰值接近
    400（蓝色曲线），并且在训练结束时，指数加权移动平均大约在 160 到 180 之间。我们使用的重放记忆大小为 300,000，由于内存限制，这个值相对较小。如果使用更大的重放记忆大小，可能会获得更高的平均回合奖励。这个可以留给读者进行实验：'
- en: '![](img/5b3eabb5-0f16-45ea-8bb8-25fcf5fc278a.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b3eabb5-0f16-45ea-8bb8-25fcf5fc278a.png)'
- en: 'Figure 2: Total episode reward versus time step number for Atari Breakout using
    the DQN'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用 DQN 在 Atari Breakout 中的总回合奖励与时间步数的关系
- en: This concludes this chapter on DQN.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容至此结束，关于 DQN 的讨论完结。
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at our very first deep RL algorithm, DQN, which is
    probably the most popular RL algorithm in use today. We learned the theory behind
    a DQN, and also looked at the concept and use of target networks to stabilize
    training. We were also introduced to the Atari environment, which is the most
    popular environment suite for RL. In fact, many of the RL papers published today
    apply their algorithms to games from the Atari suite and report their episodic
    rewards, comparing them with corresponding values reported by other researchers
    who use other algorithms. So, the Atari environment is a natural suite of games
    to train RL agents and compare them to ascertain the robustness of algorithms.
    We also looked at the use of a replay buffer, and learned why it is used in off-policy
    algorithms.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了第一个深度强化学习算法——DQN，它可能是当今最流行的强化学习算法。我们了解了DQN背后的理论，也探讨了目标网络的概念及其在稳定训练中的应用。我们还介绍了Atari环境，这是目前最流行的强化学习环境套件。事实上，今天发布的许多强化学习论文都将其算法应用于Atari套件中的游戏，并报告其每回合的奖励，与使用其他算法的研究人员报告的相应值进行比较。因此，Atari环境是训练强化学习智能体并与之比较、验证算法稳健性的自然游戏套件。我们还研究了回放缓冲区的使用，并了解了为什么它在脱离策略算法中很有用。
- en: This chapter has laid the foundation for us to delve deeper into deep RL (no
    pun intended!). In the next chapter, we will look at other DQN extensions, such
    as DDQN, dueling network architectures, and rainbow networks.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为我们深入研究深度强化学习奠定了基础（没有双关的意思！）。在下一章中，我们将研究其他DQN的扩展，如DDQN、对抗网络结构和彩虹网络。
- en: Questions
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Why is a replay buffer used in a DQN?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在DQN中使用回放缓冲区？
- en: Why do we use target networks?
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们要使用目标网络？
- en: Why do we stack four frames into one state? Will one frame alone suffice to
    represent one state?
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们要将四帧图像堆叠成一个状态？仅使用一帧图像能否足以表示一个状态？
- en: Why is the Huber loss sometimes preferred over L2 loss?
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么有时Huber损失比L2损失更受欢迎？
- en: We converted the RGB input image into grayscale. Can we instead use the RGB
    image as input to the network? What are the pros and cons of using RGB images
    instead of grayscale?
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将RGB输入图像转换为灰度图像。我们能否直接使用RGB图像作为网络的输入？使用RGB图像而不是灰度图像有什么优缺点？
- en: Further reading
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Playing Atari with Deep Reinforcement Learning*, by* Volodymyr Mnih, Koray
    Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
    Martin Riedmiller, arXiv*:1312.5602: [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过深度强化学习玩Atari*，作者为*Volodymyr Mnih、Koray Kavukcuoglu、David Silver、Alex Graves、Ioannis
    Antonoglou、Daan Wierstra 和 Martin Riedmiller*，*arXiv*:1312.5602: [https://arxiv.org/abs/1312.5602](https://arxiv.org/abs/1312.5602)'
- en: '*Human-level control through deep reinforcement learning* by *Volodymyr Mnih,
    Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare,
    Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
    Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran,
    Daan Wierstra, Shane Legg, and Demis Hassabis*, *Nature*, 2015: [https://www.nature.com/articles/nature14236](https://www.nature.com/articles/nature14236)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过深度强化学习实现人类水平的控制*，作者为*Volodymyr Mnih、Koray Kavukcuoglu、David Silver、Andrei
    A. Rusu、Joel Veness、Marc G. Bellemare、Alex Graves、Martin Riedmiller、Andreas K.
    Fidjeland、Georg Ostrovski、Stig Petersen、Charles Beattie、Amir Sadik、Ioannis Antonoglou、Helen
    King、Dharshan Kumaran、Daan Wierstra、Shane Legg 和 Demis Hassabis*，*Nature*，2015:
    [https://www.nature.com/articles/nature14236](https://www.nature.com/articles/nature14236)'
