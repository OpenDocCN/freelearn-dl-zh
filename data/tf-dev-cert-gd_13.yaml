- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Time Series, Sequences, and Prediction with TensorFlow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 进行时间序列、序列和预测
- en: Welcome to the final chapter in our journey with TensorFlow. In the last chapter,
    we closed on a high by applying neural networks such as DNNs to forecast time
    series data effectively. In this chapter, we will be exploring an array of advanced
    ideas, such as integrating learning rate schedulers into our workflow to dynamically
    adapt our learning rate and accelerate the process of model training. In previous
    chapters, we emphasized the need for and importance of finding the optimal learning
    rate. When building models with learning rate schedulers, we can achieve this
    in a dynamic way either using inbuilt learning rate schedulers in TensorFlow or
    by crafting our own custom-made learning rate scheduler.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到我们与 TensorFlow 旅程的最后一章。在上一章中，我们通过应用神经网络（如 DNN）有效地预测时间序列数据，达到了一个高潮。本章中，我们将探索一系列高级概念，例如将学习率调度器集成到我们的工作流中，以动态调整学习率，加速模型训练过程。在前几章中，我们强调了找到最佳学习率的必要性和重要性。在构建带有学习率调度器的模型时，我们可以使用
    TensorFlow 中内置的学习率调度器，或者通过制作自定义学习率调度器，以动态方式实现这一目标。
- en: Next, we will discuss Lambda layers and how these arbitrary layers can be applied
    in our model architecture to enhance quick experimentation, enabling us to embed
    custom functions seamlessly into our model’s architecture, especially when working
    with LSTMs and RNNs. We will switch over from building time series models with
    DNNs to more complex architectures such as CNNs, RNNs, LSTMs, and CNN-LSTM networks.
    We will apply these networks to our sales dataset case study. To conclude this
    chapter, we will extract Apple stock closing price data from Yahoo Finance and
    apply these models to build a forecasting model to predict future stock prices.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论 Lambda 层，以及如何将这些任意层应用到我们的模型架构中，以增强快速实验的能力，使我们能够无缝地将自定义函数嵌入到模型架构中，尤其是在处理
    LSTM 和 RNN 时。我们将从使用 DNN 构建时间序列模型转向更复杂的架构，如 CNN、RNN、LSTM 和 CNN-LSTM 网络。我们将把这些网络应用到我们的销售数据集案例研究中。最后，我们将从
    Yahoo Finance 提取苹果股票的收盘价数据，并应用这些模型构建预测模型，预测未来的股价。
- en: 'In this chapter, we will be covering the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Understanding and applying learning rate schedulers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解并应用学习率调度器
- en: Utilizing Lambda layers in TensorFlow
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中使用 Lambda 层
- en: Employing RNNs, LSTMs, and CNNs for time series forecasting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 RNN、LSTM 和 CNN 进行时间序列预测
- en: Apple stock price prediction using neural networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络进行苹果股价预测
- en: By the end of this chapter, you will have a deeper understanding of time series
    forecasting with TensorFlow, along with hands-on experience in applying different
    techniques in building time series forecasting models for real-world projects.
    Let’s get started.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将对使用 TensorFlow 进行时间序列预测有更深入的理解，并拥有将不同技术应用于构建时间序列预测模型的实践经验，适用于真实世界的项目。让我们开始吧。
- en: Understanding and applying learning rate schedulers
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解并应用学习率调度器
- en: 'In [*Chapter 12*](B18118_12.xhtml#_idTextAnchor291), *Introduction to Time
    Series, Sequences,* and Predictions. we built a DNN that achieved a `LearningRateScheduler`
    callback from TensorFlow, we can dynamically adjust the learning rate during training
    using some built-in techniques. Let us examine some built-in learning rate schedulers:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 12 章*](B18118_12.xhtml#_idTextAnchor291)，《时间序列、序列和预测简介》中，我们构建了一个 DNN，成功使用了
    TensorFlow 中的 `LearningRateScheduler` 回调函数，我们可以通过一些内置技术在训练过程中动态调整学习率。让我们来看看一些内置的学习率调度器：
- en: '`ExponentialDecay`: This starts with a specified learning rate and decreases
    exponentially after a certain number of steps.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ExponentialDecay`：从指定的学习率开始，在经过一定步数后以指数方式下降。'
- en: '`PiecewiseConstantDecay`: This provides a piecewise constant learning rate,
    where you specify boundaries and learning rates to divide the training process
    into several stages with different learning rates.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PiecewiseConstantDecay`：提供分段常数学习率，你可以指定边界和学习率，将训练过程划分为多个阶段，每个阶段使用不同的学习率。'
- en: '`PolynomialDecay`: This learning rate is a function of the iteration number
    in this schedule. It starts with the initial learning rate and decreases it to
    the end learning rate as per the polynomial function specified.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PolynomialDecay`：该学习率是调度中迭代次数的函数。它从初始学习率开始，根据指定的多项式函数，将学习率逐渐降低至最终学习率。'
- en: 'Let’s add a learning rate scheduler to the feedforward network we used in [*Chapter
    12*](B18118_12.xhtml#_idTextAnchor291)*, Introduction to Time Series, Sequences,
    and Predictions*. We are using the same sales data, but this time we will be applying
    different learning rate schedulers to improve our model’s performance. Let’s begin:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为[*第12章*](B18118_12.xhtml#_idTextAnchor291)《时间序列、序列和预测导论》中使用的前馈神经网络添加一个学习率调度器。我们使用相同的销售数据，但这次我们将应用不同的学习率调度器来提高模型的性能。让我们开始吧：
- en: 'We begin by importing the libraries for this project:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从导入该项目所需的库开始：
- en: '[PRE0]'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, let us load our dataset:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们加载我们的数据集：
- en: '[PRE4]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We load the sales data from the GitHub repo for this book and put the CSV data
    into a DataFrame.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们从本书的GitHub仓库加载销售数据，并将CSV数据放入DataFrame中。
- en: 'Now, we convert the `Date` column into datetime and make it the index:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将`Date`列转换为日期时间格式并将其设置为索引：
- en: '[PRE8]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first line of code converts the date column into a datetime format. We do
    this to easily perform time series operations. Next, we change the date column
    by setting it as the index of our DataFrame. This makes it easier to slice and
    dice our data using dates.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一行代码将日期列转换为日期时间格式。我们这样做是为了方便进行时间序列操作。接下来，我们将日期列设置为DataFrame的索引，这使得使用日期切片和处理数据变得更容易。
- en: 'Let’s extract the sales values from the DataFrame:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从DataFrame中提取销售值：
- en: '[PRE10]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we are extracting the sales values from our sales DataFrame and converting
    them into a NumPy array. We will use this NumPy array to create our sliding window
    data.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们从销售DataFrame中提取销售值并将其转换为NumPy数组。我们将使用这个NumPy数组来创建我们的滑动窗口数据。
- en: 'Next, we’ll create a sliding window:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个滑动窗口：
- en: '[PRE11]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Just as we did in [*Chapter 12*](B18118_12.xhtml#_idTextAnchor291), *Introduction
    to Time Series, Sequences, and Predictions,* we are using the sliding window technique
    to convert our time series into a supervised learning problem made up of features
    and labels. Here, the window size of 20 serves as our `X` feature, which contains
    20 consecutive sales values, and our `y` is the next immediate value after those
    first 20 sales values. Here, we use the first 20 values to predict the next value.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们在[*第12章*](B18118_12.xhtml#_idTextAnchor291)《时间序列、序列和预测导论》中所做的那样，我们使用滑动窗口技术将时间序列转换为包含特征和标签的监督学习问题。在这里，大小为20的窗口作为我们的`X`特征，包含20个连续的销售值，而我们的`y`是这20个销售值之后的下一个即时值。这里，我们使用前20个值来预测下一个值。
- en: 'Now, let’s split our data into training and validation sets:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将数据拆分为训练集和验证集：
- en: '[PRE16]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We convert our data into NumPy array and split our data into training and validation
    sets. We use 80 percent of our data for training and 20 percent of our data for
    the validation set that we will use to evaluate our model.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将数据转换为NumPy数组，并将数据拆分为训练集和验证集。我们使用80%的数据用于训练，20%的数据用于验证集，我们将用它来评估我们的模型。
- en: 'Our next goal is to build out a TensorFlow dataset, which is a more efficient
    format for training models in TensorFlow:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的下一个目标是构建一个TensorFlow数据集，这是训练TensorFlow模型时更高效的格式：
- en: '[PRE21]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We apply the `from_tensor_slices()` method to make a dataset from the NumPy
    arrays. After this, we use the `cache()` method to speed up training by caching
    our dataset in memory. We apply the `shuffle(buffer_size)` method to randomly
    shuffle our training data to prevent issues such as sequential bias. Then we use
    the `batch(batch_size)` method to split our data into batches of a specified size;
    in this case, batches of 128 are fed into our model during training. Next, we
    use the `prefetch` method to ensure our GPU/CPU will always have data ready for
    processing, reducing the waiting time between the processing of one batch and
    the next. We pass in the `tf.data.experimental.AUTOTUNE` argument to tell TensorFlow
    to automatically determine the optimal number of batches to prefetch. This makes
    our training process smoother and faster.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们应用`from_tensor_slices()`方法从NumPy数组创建数据集。之后，我们使用`cache()`方法通过将数据集缓存到内存中来加速训练。接着，我们使用`shuffle(buffer_size)`方法随机打乱训练数据，以防止诸如顺序偏差等问题。然后，我们使用`batch(batch_size)`方法将数据拆分成指定大小的批次；在这个例子中，训练过程中每次将128个样本输入到模型中。接下来，我们使用`prefetch`方法确保GPU/CPU始终有数据准备好进行处理，从而减少一个批次处理完与下一个批次之间的等待时间。我们传入`tf.data.experimental.AUTOTUNE`参数，告诉TensorFlow自动确定预取的最佳批次数量。这使得我们的训练过程更加顺畅和快速。
- en: Our data is now ready for modeling. Let us explore using this data with in-built
    learning rate schedulers from TensorFlow, after which we will look at how to find
    the optimal learning rate with a custom learning rate scheduler.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在已经准备好进行建模。接下来，我们将使用 TensorFlow 内置的学习率调度器探索这个数据，然后我们将探讨如何使用自定义学习率调度器找到最佳学习率。
- en: In-built learning rate schedulers
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内置学习率调度器
- en: 'We will be using the same model as we did in [*Chapter 12*](B18118_12.xhtml#_idTextAnchor291)*,
    Introduction to Time Series, Sequences, and Predictions*. Let’s define our model
    and explore the inbuilt learning rate schedulers:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与 [*第 12 章*](B18118_12.xhtml#_idTextAnchor291)* 《时间序列、序列与预测入门》 中相同的模型。让我们定义模型并探索内置的学习率调度器：
- en: 'We’ll start with the model definition:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从模型定义开始：
- en: '[PRE28]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here, we are using three dense layers.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用了三个密集层。
- en: 'Next, we’ll use the exponential decay learning rate scheduler:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用指数衰减学习率调度器：
- en: '[PRE34]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The exponential decay learning rate scheduler sets up a learning rate that decays
    exponentially over time. In this experiment, the initial learning rate is set
    to `0.1`. This learning rate will undergo an exponential decay at a rate of 0.96
    for every 100 steps, as defined by the `decay_steps` parameter. Next, we assign
    our exponential learning rate to our optimizer and compile the model. After this,
    we fit the model for 100 epochs.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指数衰减学习率调度器设置了一个学习率，该学习率随着时间的推移按指数方式衰减。在此实验中，初始学习率设置为`0.1`。这个学习率将在每 100 步时按 0.96
    的衰减速率进行指数衰减，这是由`decay_steps`参数定义的。接下来，我们将我们的指数学习率分配给优化器并编译模型。之后，我们将模型拟合 100 个
    epochs。
- en: 'Next, we’ll evaluate the performance of the model using the MAE and **mean
    squared error** (**MSE**) and plot the validation forecast against the true values:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 MAE 和 **均方误差** (**MSE**) 评估模型的性能，并将验证预测与真实值进行比较：
- en: '[PRE42]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This will generate the following output:'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '![Figure 13.1 – True forecast versus the validation forecast using exponential
    decay (zoomed in)](img/B18118_13_001.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.1 – 使用指数衰减的真实预测与验证预测（放大版）](img/B18118_13_001.jpg)'
- en: Figure 13.1 – True forecast versus the validation forecast using exponential
    decay (zoomed in)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 – 使用指数衰减的真实预测与验证预测（放大版）
- en: When we run the code block, we get an MAE of around 5.31 and an MSE of 43.18,
    and from the zoomed-in plot in *Figure 13**.1*, we see our model is following
    the actual sales validation data closely. However, the result is no better than
    what we achieved in [*Chapter 12*](B18118_12.xhtml#_idTextAnchor291)*, Introduction
    to Time Series, Sequences, and Predictions*. Next, let us experiment with `PiecewiseConstantDecay`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码块时，得到的 MAE 约为 5.31，MSE 为 43.18，并且从*图 13.1* 中的放大图可以看到，我们的模型紧密跟踪实际的销售验证数据。然而，结果并没有比我们在
    [*第 12 章*](B18118_12.xhtml#_idTextAnchor291)* 《时间序列、序列与预测入门》中取得的更好。接下来，我们将尝试使用
    `PiecewiseConstantDecay`。
- en: 'Let’s use the `PiecewiseConstantDecay` learning rate scheduler:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用 `PiecewiseConstantDecay` 学习率调度器：
- en: '[PRE53]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The `PiecewiseConstantDecay` learning rate scheduler allows us the flexibility
    to define specific learning rates for different periods during training. In our
    case, we specified 30 and 60 steps as our boundaries; this means that for the
    first 30 steps, we apply a learning rate of `0.1`, from 30 to 60, we apply a learning
    rate of `0.01`, and from 61 to the end of training, we apply a learning rate of
    `0.001`. To `PiecewiseConstantDecay`, the number of learning rates should be one
    more than the number of boundaries applied. For example, in our case, we have
    two boundaries (`[30, 60]`) and three learning rates (`[0.1, 0.01, 0.001]`). Once
    we set up the scheduler, we use the same optimizer and compile and fit the model
    as we did with the exponential decay learning rate scheduler. Then, we evaluate
    the performance of the model and generate the following validation plot:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`PiecewiseConstantDecay` 学习率调度器允许我们在训练过程中为不同的时期定义特定的学习率。在我们的例子中，我们将 30 和 60
    步设为边界；这意味着在前 30 步中，我们应用学习率 `0.1`，从第 30 步到第 60 步，我们应用学习率 `0.01`，从第 61 步到训练结束，我们应用学习率
    `0.001`。对于 `PiecewiseConstantDecay`，学习率的数量应该比应用的边界数多一个。例如，在我们的例子中，我们有两个边界（`[30,
    60]`）和三个学习率（`[0.1, 0.01, 0.001]`）。设置好调度器后，我们使用相同的优化器，并像使用指数衰减学习率调度器一样编译和拟合模型。然后，我们评估模型的性能并生成以下验证图：'
- en: '![Figure 13.2 – True forecast versus the validation forecast using exponential
    decay](img/B18118_13_002.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.2 – 使用指数衰减的真实预测与验证预测](img/B18118_13_002.jpg)'
- en: Figure 13.2 – True forecast versus the validation forecast using exponential
    decay
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 – 使用指数衰减的真实预测与验证预测
- en: In this experiment, we achieved an MAE of 4.87 and an MSE of 36.97\. This is
    an improved performance. Again, the forecast in *Figure 13**.2* nicely follows
    the true values. Let us zoom in for clarity’s sake.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次实验中，我们实现了4.87的MAE和36.97的MSE。这是一次改进的表现。再次，*图13.2*中的预测很好地跟随了真实值。为了清晰起见，我们放大了一些。
- en: '![Figure 13.3 – Zoomed-in plots for our first two experiments](img/B18118_13_003.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图13.3 – 我们前两个实验的放大图](img/B18118_13_003.jpg)'
- en: Figure 13.3 – Zoomed-in plots for our first two experiments
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 – 我们前两个实验的放大图
- en: You can see from *Figure 13**.3*, in which we zoomed into the first 200 days,
    when we applied polynomial decay, the forecasted plot maps better to the true
    values in comparison to when we used exponential decay for our learning rate scheduler.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图13.3*中可以看到，我们在应用多项式衰减时，将视图放大到前200天，相较于使用指数衰减学习率调度器时，预测图更好地与真实值匹配。
- en: 'Let’s now apply `PolynomialDecay`:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在让我们应用`PolynomialDecay`：
- en: '[PRE61]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: In this experiment, we set `initial_learning_rate` to `0.1`; this serves as
    our starting learning rate. We set the `decay_steps` parameter to `100`, indicating
    that the learning rate will decay over these 100 steps. Next, we set our `end_learning_rate`
    to `0.01`; this means that by the conclusion of our `decay_steps`, the learning
    rate will have reduced to this value. The `power` parameter controls the exponent
    to which the step decay is raised. In this experiment, we set the `power` value
    to `1.0`, resulting in linear decay.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个实验中，我们将`initial_learning_rate`设置为`0.1`，它作为我们的初始学习率。我们将`decay_steps`参数设置为`100`，表示学习率将在这100步中衰减。接下来，我们将`end_learning_rate`设置为`0.01`，这意味着在`decay_steps`结束时，学习率将降到此值。`power`参数控制步长衰减的指数。在本实验中，我们将`power`值设置为`1.0`，实现线性衰减。
- en: When we evaluate the model’s performance, we see that we achieved our best result
    so far with an MAE of 4.72 and an MSE of 34.49\. From *Figure 13**.4*, we can
    see that it also closely follows the data, even more accurately than when we used
    `PiecewiseConstantDecay`.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们评估模型的表现时，我们发现目前为止我们取得了最佳结果，MAE为4.72，MSE为34.49。从*图13.4*中可以看到，预测结果与数据非常接近，比我们使用`PiecewiseConstantDecay`时更为准确。
- en: '![Figure 13.4 – True forecast versus the validation forecast using PolynomialDecay
    (zoomed in)](img/B18118_13_004.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图13.4 – 使用PolynomialDecay的真实预测与验证预测（放大）](img/B18118_13_004.jpg)'
- en: Figure 13.4 – True forecast versus the validation forecast using PolynomialDecay
    (zoomed in)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4 – 使用PolynomialDecay的真实预测与验证预测（放大）
- en: Now that you have a good idea of how to apply these learning rate schedulers,
    it is a good idea to tweak the values and see whether you can achieve a much lower
    MAE and MSE. When you are done, let’s look at a custom learning rate scheduler.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经大致了解了如何应用这些学习率调度器，接下来可以调整值，看看是否能实现更低的MAE和MSE。当你完成后，我们来看看一个自定义学习率调度器。
- en: By simply tweaking the learning rate, we can see our `PiecewiseConstantDecay`
    learning rate scheduler won this battle, not only with the other learning rates
    but also, it outperforms the simple DNN model with the same architecture that
    we used in [*Chapter 12*](B18118_12.xhtml#_idTextAnchor291)*, Introduction to
    Time Series, Sequences, and Predictions*. You can read more about the learning
    rate scheduler from the document [https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler)
    or using this excellent Medium article [https://rmoklesur.medium.com/learning-rate-scheduler-in-keras-cc83d2f022a6](https://rmoklesur.medium.com/learning-rate-scheduler-in-keras-cc83d2f022a6)
    by Moklesur Rahman.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单地调整学习率，我们可以看到我们的`PiecewiseConstantDecay`学习率调度器在这场竞争中获胜了，不仅超越了其他学习率，还优于我们在[*第12章*](B18118_12.xhtml#_idTextAnchor291)《时间序列、序列和预测导论》中使用的相同架构的简单DNN模型。你可以通过文档[https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler)或Moklesur
    Rahman在Medium上的这篇优质文章[https://rmoklesur.medium.com/learning-rate-scheduler-in-keras-cc83d2f022a6](https://rmoklesur.medium.com/learning-rate-scheduler-in-keras-cc83d2f022a6)了解更多关于学习率调度器的内容。
- en: Custom learning rate scheduler
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义学习率调度器
- en: 'Beyond using built-in learning rate schedulers, TensorFlow provides us with
    an easy way to build a custom learning rate scheduler to help us find the optimal
    learning rate. Let’s do this next:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用内置的学习率调度器外，TensorFlow还提供了一种简便的方法来构建自定义学习率调度器，帮助我们找到最佳学习率。接下来我们来实现这一点：
- en: 'Let’s start by defining the custom learning rate scheduler:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从定义自定义学习率调度器开始：
- en: '[PRE67]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Here, we start with a small learning rate (1×10−7) and we increase this learning
    rate exponentially with each passing epoch. We use `10**(epoch / 10)` to determine
    the rate at which the learning rate increases.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们从一个较小的学习率（1×10−7）开始，并随着每个 epoch 的增加，学习率呈指数增长。我们使用 `10**(epoch / 10)` 来确定学习率增长的速度。
- en: 'We define the optimizer with the initial learning rate:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用初始学习率定义优化器：
- en: '[PRE70]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Here, we used an SGD with a starting learning rate of 1×10−7 and a momentum
    of `0.9`. Momentum helps accelerate the optimizer in the right direction and also
    dampens oscillations.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，我们使用了一个学习率为 1×10−7 的 SGD 优化器，并设置动量为 `0.9`。动量帮助加速优化器朝正确的方向前进，同时也减少震荡。
- en: 'Next, we compile the model with the defined optimizer and set our loss as MSE:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们用定义好的优化器编译模型，并将损失设置为 MSE：
- en: '[PRE73]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now, we train the model:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们开始训练模型：
- en: '[PRE74]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: We train the model for 200 epochs and then pass the learning rate scheduler
    as a callback. This way, the learning rate is adjusted based on the customization
    when defining our custom learning rate scheduler. We also set `verbose=0` so we
    don’t print the training process.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们训练模型 200 个 epoch，然后将学习率调度器作为回调传入。这样，学习率将根据定义的自定义学习率调度器进行调整。我们还设置了 `verbose=0`，以避免打印训练过程。
- en: 'Calculate the learning rates for each epoch:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个 epoch 的学习率：
- en: '[PRE76]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: We use this code to calculate the learning rate per epoch and it gives us an
    array of learning rates.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用这段代码来计算每个 epoch 的学习率，并且它会给我们一个学习率数组。
- en: 'We plot the model loss against the learning rate:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们绘制模型损失与学习率的关系图：
- en: '[PRE77]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: This plot is an effective way of selecting the optimal learning rate.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个图是选择最佳学习率的有效方法。
- en: '![Figure 13.5 – The learning rate loss curve](img/B18118_13_005.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.5 – 学习率损失曲线](img/B18118_13_005.jpg)'
- en: Figure 13.5 – The learning rate loss curve
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 – 学习率损失曲线
- en: To find the optimal learning rate, we are on the lookout for where the loss
    is decreasing most rapidly before it begins to increase again. From the plot,
    we can see the learning rate falls and settles at around 3x10-5, after which it
    begins to rise again. So, we will pick this value as our ideal learning rate for
    this experiment. Now we will retrain our model using this new learning rate as
    our fixed learning rate. When we run the code, we get an MAE of 5.96 and an MSE
    of 55.08.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最佳的学习率，我们需要找出损失最迅速下降的地方，在其开始再次上升之前。从图中可以看到，学习率下降并在大约 3x10-5 附近稳定，然后开始再次上升。因此，我们将选择这个值作为本次实验的理想学习率。接下来，我们将使用这个新的学习率作为固定学习率来重新训练模型。当我们运行代码时，我们得到
    MAE 为 5.96，MSE 为 55.08。
- en: We have now seen how to use both in-built learning rate schedulers and custom
    schedulers. Let us now switch our attention to using CNNs for time series forecasting.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到如何使用内置的学习率调度器和自定义调度器。接下来，让我们将注意力转向使用 CNN 进行时间序列预测。
- en: CNNs for time series forecasting
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于时间序列预测的 CNN
- en: CNNs have recorded remarkable success in image classification tasks due to their
    ability to detect localized patterns within grid-like data structures. This idea
    can also be applied to time series forecasting. By viewing a time series as a
    sequence of temporal intervals, CNNs can extract and recognize patterns that are
    predictive of future trends. Another important strength of CNNs is their translation-invariant
    nature. This means once they learn a pattern in one segment, the network is well
    equipped to recognize it everywhere else it occurs within the series. This comes
    in handy in detecting reoccurring patterns across time steps.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 在图像分类任务中取得了显著的成功，因为它们能够检测网格状数据结构中的局部模式。这一思想同样可以应用于时间序列预测。通过将时间序列视为一系列时间间隔，CNN
    可以提取和识别有助于预测未来趋势的模式。CNN 的另一个重要优势是它们的平移不变性。这意味着，一旦它们在一个片段中学习到某个模式，网络就能在序列中出现该模式的任何地方进行识别。这对于在时间步长之间检测重复模式非常有用。
- en: The setup of a CNN also helps to automatically reduce the dimensionality of
    our input data with the aid of the pooling layers. Hence, the convolution and
    pooling operations in a CNN transform the input series into a streamlined form
    that captures the core features while ensuring computational efficiency. Unlike
    with images, here we use a 1D convolutional filter because of the nature of time
    series data (singular dimension). This filter slides across the time dimension,
    observing localized windows of values as input. It detects informative patterns
    within these intervals through repeated element-wise multiplications and summations
    between its weights and the input windows.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的设置还通过池化层的帮助，自动减少输入数据的维度。因此，CNN中的卷积和池化操作将输入序列转化为一种简化的形式，捕捉核心特征，同时确保计算效率。与图像不同，在这里我们使用1D卷积滤波器，因为时间序列数据的特性（单一维度）。这个滤波器沿着时间维度滑动，观察作为输入的局部值窗口。它通过在每个元素上的乘法和加和操作，检测这些区间中的有用模式。
- en: Multiple filters are learned to extract diverse predictive signals – trends,
    seasonal fluctuations, cycles, and more. Similar to patterns within images, CNNs
    can recognize translated versions of these temporal motifs throughout the series.
    When we apply successive convolutional and pooling layers, the network composes
    these low-level features into higher-level representations, progressively condensing
    the series into its most salient components. Fully connected layers ultimately
    use these learned features to make forecasts.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 多个滤波器被用来提取多样的预测信号——趋势、季节性波动、周期等。类似于图像中的模式，卷积神经网络（CNN）能够识别这些时间序列中的变换版本。当我们应用连续的卷积层和池化层时，网络将这些低级特征组合成更高级的表示，逐步将序列浓缩成其最显著的组成部分。最终，完全连接层利用这些学习到的特征进行预测。
- en: Let us return to our notebooks and apply a 1D CNN in modeling our sales data.
    We already have our training and test data. Now, to model our data with a CNN,
    we need to carry out an extra step, which involves reshaping our data to meet
    the expected input shape a CNN expects. In [*Chapter 7*](B18118_07.xhtml#_idTextAnchor146)*,*
    *Image Classification with Convolutional Neural Networks*, we saw how CNNs required
    3D data in comparison to the 2D data we use when modeling with a DNN; the same
    applies here.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到笔记本，并在我们的销售数据建模中应用1D CNN。我们已经有了训练数据和测试数据。现在，为了使用CNN建模我们的数据，我们需要进行一个额外的步骤，即调整数据形状以满足CNN所期望的输入形状。在[*第7章*](B18118_07.xhtml#_idTextAnchor146)*,*
    *使用卷积神经网络进行图像分类*中，我们看到CNN需要3D数据，而我们使用DNN建模时则使用2D数据；这里也是同样的情况。
- en: A CNN requires a batch size, a window size, and the number of features. The
    batch size is the first dimension of our input shape; it refers to the number
    of sequences we feed into the CNN. We set the window size value to `20`, and the
    number of features here refers to the number of distinct features at each time
    step. For a univariate time series, this value will be `1`; for a multivariate
    time series, the value will be `2` or more.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: CNN需要一个批量大小、一个窗口大小和特征数量。批量大小是输入形状的第一维，它表示我们输入CNN的序列数量。我们将窗口大小设置为`20`，特征数量指的是每个时间步的独立特征数。对于单变量时间序列，这个值是`1`；对于多变量时间序列，这个值将是`2`或更多。
- en: 'Since we are dealing with a univariate time series in our case study, our input
    shape needs to look like (`128,` `20, 1`):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在案例研究中处理的是单变量时间序列，因此我们的输入形状需要类似于（`128,` `20, 1`）：
- en: 'Let’s prepare our data for modeling with a CNN with the right shape:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们准备数据以适应CNN模型的正确形状：
- en: '[PRE83]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: Most of the code in this code block is the same. The key step here is the `reshape`
    step, which we use to achieve the input shape required for CNN modeling.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码中的大部分内容是相同的。这里的关键步骤是`reshape`步骤，我们用它来实现CNN建模所需的输入形状。
- en: 'Let’s build our model:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们构建我们的模型：
- en: '[PRE106]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: In our model, we apply 1D convolutional layers due to the single-dimensional
    nature of time series data, unlike the 2D CNNs we employed for image classification,
    due to the 2D structure of images. Here, our model is made up of two 1D convolutional
    layers, each followed by a max pooling layer. In our first convolutional layer,
    we have 64 filters to learn various data patterns with a filter size of `3`, which
    allows it to recognize patterns spanning three time steps. We use a stride of
    `1`; this means that our filter traverses the data one step at a time, and to
    ensure nonlinearity, we use ReLU as our activation function. Notice that we are
    using a new type of padding, called causal padding. This choice is strategic as
    causal padding ensures that the model’s output for a particular time step is influenced
    only by that time step and its predecessors, never by future data. By adding padding
    to the start of the sequence, causal padding respects the natural temporal sequence
    of our data. This is essential to prevent our model from inadvertently “looking
    ahead,” ensuring forecasts rely solely on past and current information.
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的模型中，由于时间序列数据是单维的，我们采用了1D卷积层，而不是像用于图像分类的2D卷积神经网络（CNN），因为图像具有二维结构。我们模型由两个1D卷积层组成，每个卷积层后跟一个最大池化层。在我们的第一个卷积层中，我们使用了64个滤波器来学习各种数据模式，滤波器大小为`3`，这使得它能够识别跨越三个时间步的模式。我们使用了步幅`1`，这意味着我们的滤波器每次遍历数据时只跨一步，为了确保非线性，我们使用了ReLU作为激活函数。请注意，我们使用了一种新的填充方式，称为因果填充。这种选择是有战略意义的，因为因果填充确保模型在某个时间步的输出仅受该时间步及其前置时间步的影响，永远不会受到未来数据的影响。通过在序列的开头添加填充，因果填充尊重了我们数据的自然时间序列。这对于防止模型“不经意地提前预测”至关重要，确保预测仅依赖于过去和当前的信息。
- en: We earlier outlined that we need 3D input-shaped data to be fed into our CNN
    model made up of the batch size, window size, and number of features. Here, we
    used `input_shape=(window_size, 1)`. We did not state the batch size in the input
    shape definition. This means the model can take batches of different sizes since
    we did not hardcode any batch size. Also, we only have one feature since we are
    dealing with a univariate time series, and that’s why we have specified `1` in
    the input shape along with the window size. The max pooling layer reduces the
    dimensionality of our data. Next, we reach the second convolutional layer; this
    time we use 32 filters, again with a kernel size of `3`, causal padding, and ReLU
    as the activation function. Next, the max pooling layer samples the data again.
    After this, the data is flattened and fed into the fully connected layers to make
    predictions based on the patterns learned from our sales data.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们之前提到，我们需要3D形状的输入数据来馈送到由批量大小、窗口大小和特征数量组成的CNN模型中。在这里，我们使用了`input_shape=(window_size,
    1)`。我们没有在输入形状定义中说明批量大小。这意味着模型可以接受不同大小的批量，因为我们没有硬编码任何批量大小。另外，由于我们处理的是单变量时间序列，所以我们只有一个特征，这就是为什么在输入形状中指定了`1`以及窗口大小的原因。最大池化层减少了我们数据的维度。接下来，我们进入第二个卷积层，这次我们使用了32个滤波器，核大小为`3`，依然使用因果填充和ReLU作为激活函数。然后，最大池化层再次对数据进行采样。之后，数据被展平并输入到全连接层中，根据我们从销售数据中学到的模式进行预测。
- en: 'Let’s compile and fit the model for 100 epochs:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编译并训练模型100个周期：
- en: '[PRE122]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'Finally, let’s evaluate the performance of our model:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们评估一下我们模型的性能：
- en: '[PRE125]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: We evaluate the model on the validation set by generating the MAE and MSE. When
    we run the code, we achieve an MAE of 5.37 and an MSE of 44.28\. Here, you have
    the opportunity to see whether you can achieve a much lower MAE by tweaking the
    number of filters, sizes of filters, and so on.
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过生成MAE和MSE在验证集上评估模型。当我们运行代码时，获得了5.37的MAE和44.28的MSE。在这里，你有机会通过调整滤波器数量、滤波器的大小等来看看是否能够获得更低的MAE。
- en: Next, let us see how we can use the RNN family in forecasting time series data.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用RNN系列模型来进行时间序列数据预测。
- en: RNNs in time series forecasting
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列预测中的RNN
- en: Time series forecasting poses a unique challenge in the world of machine learning,
    involving the prediction of future values based on previously observed sequential
    data. An intuitive way of thinking about this is to consider a sequence of past
    data points. The question then becomes, given this sequence, how can we predict
    the next data point or sequence of data points? This is where RNNs demonstrate
    their efficacy. RNNs are a specific type of neural network developed to process
    sequential data. They maintain an internal state or “memory” that holds information
    about the elements of the sequence observed thus far. This internal state is updated
    at each step of the sequence, amalgamating information from the new input and
    the previous state. As an example, while predicting sales, an RNN may retain data
    regarding the sales trends from the previous months, the overall trend across
    the past year, and the seasonality effects, among others.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列预测在机器学习领域提出了一个独特的挑战，它涉及基于先前观察到的顺序数据预测未来的值。一种直观的思考方式是考虑一系列过去的数据点。问题就变成了，给定这个序列，我们如何预测下一个数据点或数据点序列？这正是RNN展示其有效性的地方。RNN是一种专门为处理序列数据而开发的神经网络。它们保持一个内部状态或“记忆”，存储迄今为止观察到的序列元素的信息。这个内部状态会在序列的每个步骤中更新，将新的输入信息与之前的状态合并。例如，在预测销售时，RNN可能会保留有关前几个月销售趋势、过去一年的总体趋势以及季节性效应等数据。
- en: 'However, standard RNNs exhibit a significant limitation: the problem of “vanishing
    gradients.” This problem results in difficulty in maintaining and utilizing information
    from earlier steps in the sequence, especially as the sequence length increases.
    To overcome this hurdle, the deep learning community introduced advanced architectures.
    LSTMs and GRUs are specialized types of RNNs designed explicitly to counteract
    the vanishing gradient issue. These types of RNNs are capable of learning long-term
    dependencies due to their in-built gating mechanisms, which control the flow of
    information in and out of the memory state.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，标准的RNN存在一个显著的局限性：“梯度消失”问题。这个问题导致在序列中很难保持和利用来自早期步骤的信息，特别是当序列长度增加时。为了解决这个问题，深度学习社区引入了先进的架构。LSTM和GRU是专门设计用来解决梯度消失问题的RNN变种。由于它们内置的门控机制，这些类型的RNN能够学习长期依赖关系，控制信息在记忆状态中进出的流动。
- en: Thus, RNNs, LSTMs, and GRUs can be potent tools for time series forecasting
    because they inherently incorporate the temporal dynamics of the problem. For
    instance, while predicting sales, these models can factor in seasonal patterns,
    holidays, weekends, and more by maintaining information about previous sales periods,
    which could lead to more accurate forecasts.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RNN、LSTM和GRU可以是强大的时间序列预测工具，因为它们本身就包含了问题的时间动态。例如，在预测销售时，这些模型可以通过保持对先前销售时期的记忆来考虑季节性模式、假期、周末等因素，从而提供更准确的预测。
- en: 'Let’s put a simple RNN into action here and see how it will perform on our
    dataset:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这里运行一个简单的RNN，看看它在我们的数据集上的表现如何：
- en: 'Let’s start by preparing our data:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从准备数据开始：
- en: '[PRE133]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: '[PRE141]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: '[PRE144]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: '[PRE148]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: '[PRE149]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: Here, you will observe we are preparing our data in the same way as we did when
    using the DNN and we do not reshape it as we just did with our CNN model. There
    is a simple trick ahead that will help you do this in our model architecture.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，你会看到我们准备数据的方式与使用DNN时相同，并且我们没有像在CNN模型中那样重新塑形数据。接下来会有一个简单的技巧，帮助你在我们的模型架构中实现这一点。
- en: 'Let’s define our model architecture:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义我们的模型架构：
- en: '[PRE151]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: '[PRE152]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: '[PRE156]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: '[PRE159]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: When modeling with RNNs, just like we saw with CNNs, we need to reshape our
    data as our model expects 3D-shaped input data as well. However, in scenarios
    where you would like to keep your original input shape intact for various experiments
    with different models, we can resort to a simple yet effective solution – the
    Lambda layer. This layer is a powerful tool in our toolbox that lets us perform
    simple, arbitrary functions on the input data, making it an excellent instrument
    for quick experimentation.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用RNN建模时，就像我们在使用CNN时所看到的那样，我们需要将数据重新塑形，因为我们的模型也需要3D形状的输入数据。然而，在你希望保持原始输入形状不变，以便进行不同模型的各种实验时，我们可以采用一个简单而有效的解决方案——Lambda层。这个层是我们工具箱中的一个强大工具，允许我们对输入数据执行简单的任意函数，使其成为快速实验的绝佳工具。
- en: With Lambda layers, we can execute element-wise mathematical operations, such
    as normalization, linear scaling, and simple arithmetic operations. For instance,
    in our case, we utilize a Lambda layer to expand the dimension of our 2D input
    data to fit the 3D input requirement (`batch_size`, `time_steps`, and `features`)
    of RNNs. In TensorFlow, you can leverage the Keras API’s `tf.keras.layers.Lambda`
    to create a Lambda layer. A Lambda layer serves as an adapter, allowing us to
    make minor tweaks to the data, ensuring it’s in the right format for our model,
    while keeping the original data intact for other uses. Next, we come across two
    simple RNN layers of 40 units each. It is important to note that in the first
    RNN, we included `return_sequence =True`. We use this in RNNs and LSTMs when the
    output of one RNN or LSTM layer is fed into another RNN or LSTM layer. We set
    this parameter to ensure the first RNN layer will return an output for each input
    in the sequence. The output is then fed into the second RNN layer; this layer
    will only return the output of the final step, which is then fed into the dense
    layers, which outputs the predicted value for each sequence. Then, we come across
    another Lambda layer that multiplies the output by 100\. We use this to expand
    the output values.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用 Lambda 层，我们可以执行逐元素的数学操作，例如归一化、线性缩放和简单的算术运算。例如，在我们的案例中，我们使用 Lambda 层将 2D 输入数据的维度扩展，以符合
    RNN 的 3D 输入要求（`batch_size`、`time_steps` 和 `features`）。在 TensorFlow 中，您可以利用 Keras
    API 的 `tf.keras.layers.Lambda` 来创建 Lambda 层。Lambda 层作为适配器，允许我们对数据进行微小调整，确保其格式适合我们的模型，同时保持原始数据不变，供其他用途。接下来，我们遇到两个每个有
    40 个单元的简单 RNN 层。需要注意的是，在第一个 RNN 中，我们设置了 `return_sequence=True`。我们在 RNN 和 LSTM
    中使用此设置，当一个 RNN 或 LSTM 层的输出需要输入到另一个 RNN 或 LSTM 层时。我们设置此参数，以确保第一个 RNN 层会为序列中的每个输入返回一个输出。然后将输出传递到第二个
    RNN 层，该层仅返回最终步骤的输出，然后将其传递到密集层，密集层输出每个序列的预测值。接下来，我们遇到了另一个 Lambda 层，它将输出乘以 100。我们使用这个操作来扩展输出值。
- en: 'Let’s compile and fit our model:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们编译并拟合我们的模型：
- en: '[PRE161]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: '[PRE163]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: Here, we achieve an MAE of 4.84 and an MSE of 35.65 on the validation set. This
    result is slightly worse than the result we achieved when we used the `PolynomialDecay`
    learning rate scheduler. Perhaps here, you have an opportunity to try our different
    learning rate schedulers to achieve a lower MAE.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们在验证集上获得了 4.84 的 MAE 和 35.65 的 MSE。这个结果稍微比我们使用 `PolynomialDecay` 学习率调度器时的结果要差一些。也许在这里，您有机会尝试不同的学习率调度器，以实现更低的
    MAE。
- en: Next, let us explore LSTMs.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索 LSTM。
- en: LSTMs in time series forecasting
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列预测中的 LSTM
- en: 'In the NLP sections, we discussed the capabilities of LSTMs and their improvements
    over RNNs by mitigating issues such as the vanishing gradient problem, enabling
    the model to learn longer sequences. In the context of time series forecasting,
    LSTM networks can be quite powerful. Let’s see how we can apply LSTMs to our sales
    dataset:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NLP 部分，我们讨论了 LSTM 的能力及其相对于 RNN 的改进，解决了如梯度消失问题等问题，使得模型能够学习更长的序列。在时间序列预测的背景下，LSTM
    网络可以非常强大。让我们看看如何将 LSTM 应用于我们的销售数据集：
- en: 'Let’s begin by preparing our data:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先准备我们的数据：
- en: '[PRE165]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: '[PRE166]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: '[PRE170]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE172]'
- en: '[PRE173]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '[PRE175]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: '[PRE177]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: '[PRE178]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: '[PRE179]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE179]'
- en: '[PRE180]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: '[PRE181]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: '[PRE182]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE182]'
- en: '[PRE183]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: '[PRE184]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE184]'
- en: '[PRE185]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE185]'
- en: '[PRE186]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE186]'
- en: Notice that we use the `reshape` step as we do not use the Lambda layers, to
    avoid repeating this code block. Note that we will be using it for this experiment
    and the next experiment using the CNN-LSTM architecture.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，由于我们不使用 Lambda 层，因此我们在这里使用 `reshape` 步骤，以避免重复此代码块。请注意，我们将在此次实验和下一次使用 CNN-LSTM
    架构的实验中使用它。
- en: 'Next, let’s define our model:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们定义我们的模型：
- en: '[PRE187]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE187]'
- en: '[PRE188]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: '[PRE189]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: '[PRE190]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: '[PRE191]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE191]'
- en: '[PRE192]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE192]'
- en: The architecture we use here is an RNN structure using LSTM cells – the first
    layer is an LSTM layer of 50 neurons, and it has the `return_sequence` parameter
    set to `True` to ensure the output returned is the complete sequence, which is
    fed into the final LSTM layer. Here, we also use an input shape of `[None, 1]`.
    The next layer also has 50 neurons, and it outputs a single value since we did
    not set the `return_sequence` parameter to `True` here and this is fed into the
    dense layer for predictions.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在这里使用的架构是一个使用 LSTM 单元的 RNN 结构——第一层是一个具有 50 个神经元的 LSTM 层，它的 `return_sequence`
    参数设置为 `True`，以确保返回的输出是完整的序列，这些输出会传递到最终的 LSTM 层。在这里，我们还使用了 `[None, 1]` 的输入形状。下一层也有
    50 个神经元，并且输出一个单一的值，因为我们没有在此处设置 `return_sequence` 参数为 `True`，该输出将传递到密集层进行预测。
- en: The next step is to compile the model. We compile and fit our model as before,
    then we evaluate it. Here we achieved an MAE of 4.56 and an MSE of 32.42, our
    lowest so far.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是编译模型。我们像以前一样编译并拟合我们的模型，然后对其进行评估。在这里，我们获得了 4.56 的 MAE 和 32.42 的 MSE，这是迄今为止最好的结果。
- en: '![Figure 13.6 – True forecast versus the validation forecast using an LSTM
    (zoomed in)](img/B18118_13_006.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图13.6 – 使用LSTM的真实预测与验证预测（放大显示）](img/B18118_13_006.jpg)'
- en: Figure 13.6 – True forecast versus the validation forecast using an LSTM (zoomed
    in)
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6 – 使用LSTM的真实预测与验证预测（放大显示）
- en: From the plot, we see that the predicted values and the true values are much
    more in sync than any other experiment we have carried out so far. Let’s see whether
    we can improve this result using a CNN-LSTM architecture for our next experiment.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，预测值和真实值比我们迄今为止进行的任何其他实验都更加一致。让我们看看是否可以使用CNN-LSTM架构来改进我们的结果，进行下一个实验。
- en: CNN-LSTM architecture for time series forecasting
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于时间序列预测的CNN-LSTM架构
- en: Deep learning has offered compelling solutions for time series forecasting,
    and one of the notable architectures in this space is the CNN-LSTM model. This
    model leverages the strengths of CNNs and LSTM networks, providing an effective
    framework for handling the unique characteristics of time series data. CNNs are
    renowned for their performance in image processing tasks due to their ability
    to learn spatial patterns in images, while in sequential data, they can learn
    local patterns. The convolutional layers within the network apply a series of
    filters to the data, learning and extracting significant local and global temporal
    patterns and trends. These features act as a compressed representation of the
    original data, retaining essential information while reducing dimensionality.
    The reduction in dimensionality leads to a more efficient representation that
    captures relevant patterns.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习为时间序列预测提供了引人注目的解决方案，而这一领域中的一个显著架构是CNN-LSTM模型。该模型利用了CNN和LSTM网络的优势，为处理时间序列数据的独特特性提供了有效的框架。CNN因其在图像处理任务中的表现而闻名，主要得益于其在图像中学习空间模式的能力，而在顺序数据中，它们能够学习局部模式。网络中的卷积层通过一系列滤波器对数据进行处理，学习并提取显著的局部和全局时间模式及趋势。这些特征作为原始数据的压缩表示，保留了关键信息，同时减少了维度。维度的减少导致更高效的表示，能够捕捉到相关模式。
- en: 'Once significant features have been extracted through the convolutional layers,
    these features become inputs to the LSTM layer(s) of the network. CNN-LSTM models
    have an advantage in their capacity for end-to-end learning. In this architecture,
    CNN and LSTM have complementary roles. The CNN layer captures local patterns and
    the LSTM layers learn temporal relationships from these patterns. This joint optimization
    is central to the performance gains of the CNN-LSTM model in comparison to independent
    architectures. Let us see how to apply this architecture to our sales data. Here
    we are going straight to the model architecture as we have already looked at the
    data preparation steps several times before:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦通过卷积层提取了显著特征，这些特征将作为网络LSTM层的输入。CNN-LSTM模型在端到端学习能力上具有优势。在这种架构中，CNN和LSTM各自扮演着互补的角色。CNN层捕捉局部模式，而LSTM层从这些模式中学习时间关系。与独立架构相比，这种联合优化是CNN-LSTM模型性能提升的关键。让我们看看如何将这种架构应用到我们的销售数据中。由于我们之前已经多次查看过数据准备步骤，因此这里直接进入模型架构：
- en: 'Let’s build our model using convolution layers:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用卷积层来构建我们的模型：
- en: '[PRE193]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE193]'
- en: '[PRE194]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: '[PRE195]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: '[PRE196]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE196]'
- en: '[PRE197]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: '[PRE198]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: '[PRE199]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: We use our 1D convolutional layer to detect patterns from the sequence of values
    as we did in our CNN forecasting experiment. Remember to set `padding` to `causal`
    to ensure the output size remains the same as the input. We set the input shape
    to `[window_size, 1]`. Here, `window_size` represents the number of time steps
    in each input sample. `1` means we are working with a univariate time series.
    For example, if we set `window_size` to `7`, this will mean we are using a week’s
    worth of data for forecasting.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用1D卷积层从值序列中检测模式，就像我们在CNN预测实验中做的那样。记得将`padding`设置为`causal`，以确保输出大小与输入保持一致。我们将输入形状设置为`[window_size,
    1]`。这里，`window_size`代表每个输入样本中的时间步长数量。`1`表示我们正在处理单变量时间序列。例如，如果我们将`window_size`设置为`7`，这意味着我们使用一周的数据进行预测。
- en: 'Next, our data reaches the LSTM layers, which are made up of 2 LSTM layers,
    each made up of 64 neurons:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们的数据进入LSTM层，由2个LSTM层组成，每个LSTM层包含64个神经元：
- en: '[PRE200]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: '[PRE201]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE201]'
- en: 'Then, we have the dense layers:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们有密集层：
- en: '[PRE202]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: '[PRE203]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE203]'
- en: '[PRE204]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE204]'
- en: The LSTM layer adds temporal context to the features extracted by the CNN layers,
    and the dense layers generate the final predictions. Here, we use three fully
    connected layers and output the final forecasted values. With this architecture,
    we achieve an MAE of 4.98 and an MSE of 40.71\. Not bad, but worse than the LSTM
    standalone model.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LSTM 层为 CNN 层提取的特征添加了时间上下文，而全连接层则生成最终的预测值。在这里，我们使用了三层全连接层，并输出最终的预测结果。通过这种架构，我们实现了
    4.98 的 MAE 和 40.71 的 MSE。效果不错，但不如单独使用 LSTM 模型的结果。
- en: Tuning the hyperparameters to optimize our model’s performance is a good idea
    here. By adjusting parameters such as the learning rate, batch size, or optimizers,
    we may be able to improve the model’s capabilities. We will not be going into
    this here as you are already well equipped to do this. Let us move on to the Apple
    stock price data and use all we have learned to create a series of experiments
    to forecast the future prices of Apple stocks and see which architecture will
    come out on top.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 调整超参数以优化模型性能在这里是个好主意。通过调整学习率、批处理大小或优化器等参数，我们可能能够提升模型的能力。我们不会在这里详细讨论，因为你已经具备了处理这些的能力。接下来让我们继续探讨苹果股票价格数据，并运用我们所学的知识，创建一系列实验，预测苹果股票的未来价格，看看哪种架构最终会脱颖而出。
- en: Forecasting Apple stock price data
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测苹果股票价格数据
- en: 'We have now covered everything we need to know about time series for the TensorFlow
    Developer Certificate exam. Let us round off this chapter and the book with a
    real-world use case on time series. For this exercise, we will be working with
    a real-world dataset (Apple closing day stock price). Let’s see how we can do
    this next. The Jupyter notebook for this exercise can be found here: [https://github.com/PacktPublishing/TensorFlow-Developer-Certificate-Guide](https://github.com/PacktPublishing/TensorFlow-Developer-Certificate-Guide).
    Let’s begin:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了 TensorFlow 开发者证书考试中关于时间序列的所有内容。让我们用一个实际的时间序列用例来结束这一章和本书。在这个练习中，我们将使用一个真实世界的数据集（苹果股票的收盘日价格）。接下来让我们看看如何进行操作。这个练习的
    Jupyter notebook 可以在这里找到：[https://github.com/PacktPublishing/TensorFlow-Developer-Certificate-Guide](https://github.com/PacktPublishing/TensorFlow-Developer-Certificate-Guide)。让我们开始：
- en: 'We start by importing the required libraries:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入所需的库：
- en: '[PRE205]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE205]'
- en: '[PRE206]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE206]'
- en: '[PRE207]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE207]'
- en: '[PRE208]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE208]'
- en: '[PRE209]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE209]'
- en: Here, we are using a new library called `yfinance`. This lets us access the
    Apple stock data for our case study.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里我们使用了一个新的库 `yfinance`。它让我们能够访问苹果股票数据，用于我们的案例分析。
- en: Note
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You may want to run `pip install yfinance` to get it working if the import fails.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果导入失败，你可能需要运行`pip install yfinance`来让其正常工作。
- en: 'Create a DataFrame:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 DataFrame：
- en: '[PRE210]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE210]'
- en: '[PRE211]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE211]'
- en: '[PRE212]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE212]'
- en: We start by creating a DataFrame using the `AAPL` ticker symbol, which represents
    Apple (the company) in the stock market. To do this, we use the `yf.Ticker` function
    from the `yfinance` library to access Apple’s historical data from Yahoo Finance.
    We apply the `history` method to our `Ticker` object to access the historical
    market data for Apple. Here, we set the period to `1d`, which means daily data
    should be accessed. We also set the `start` and `end` parameters to define the
    date range we want to access; in this case, we are collecting 10 years of data
    from the first day of January 2013 to the last day of January 2023.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过使用 `AAPL` 股票代码来创建一个 DataFrame，`AAPL`代表苹果公司（Apple）在股票市场中的代号。为此，我们使用来自 `yfinance`
    库的 `yf.Ticker` 函数来访问苹果的历史数据，该数据来自 Yahoo Finance。我们将 `history` 方法应用到我们的 `Ticker`
    对象上，以访问苹果的历史市场数据。在这里，我们将 `period` 设置为 `1d`，意味着获取的是每日数据。同时，我们也设置了 `start` 和 `end`
    参数，定义了我们想要访问的日期范围；在此案例中，我们收集了从2013年1月1日到2023年1月31日的10年数据。
- en: Next, we use `df.head()` to get a snapshot of our DataFrame. We can see the
    dataset is made up of seven columns (`Open`, `High`, `Low`, `Close`, `Volume`,
    `Dividends`, and `Stock Splits`), as shown in the following screenshot.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `df.head()` 来查看我们 DataFrame 的快照。可以看到数据集由七列组成（`Open`，`High`，`Low`，`Close`，`Volume`，`Dividends`，和
    `Stock Splits`），如下图所示。
- en: '![Figure 13.7 – A snapshot of the Apple stock data](img/B18118_13_007.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![图13.7 – 苹果股票数据快照](img/B18118_13_007.jpg)'
- en: Figure 13.7 – A snapshot of the Apple stock data
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7 – 苹果股票数据快照
- en: 'Let’s understand what these columns mean:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来理解这些列的含义：
- en: '`Open` stands for the opening price for the trading day.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Open`表示交易日的开盘价格。'
- en: '`High` is the highest price at which stocks were traded during the day.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`High`表示交易日内股票交易的最高价格。'
- en: '`Low` is the lowest price at which stocks were traded during the day.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Low`表示交易日内股票交易的最低价格。'
- en: '`Close` stands for the closing price for the trading day.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Close`表示交易日的收盘价格。'
- en: '`Volume` signifies the number of shares that changed hands during the course
    of the trading day. This can serve as an indicator of the market strength.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`交易量`表示在交易日内成交的股票数量。这可以作为市场强度的一个指标。'
- en: '`Dividends` represents how the company’s earnings are shared among shareholders.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`股息`表示公司如何将其收益分配给股东。'
- en: '`Stock Splits` can be viewed as an act of the corporation that increased the
    number of the company’s outstanding shares by splitting each share.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`股票分割`可以看作是公司通过拆分每一股股票，从而增加公司流通股本的一种行为。'
- en: 'Let’s plot the daily closing price:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们绘制每日收盘价：
- en: '[PRE213]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE213]'
- en: '[PRE214]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE214]'
- en: '[PRE215]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE215]'
- en: '[PRE216]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE216]'
- en: '[PRE217]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE217]'
- en: '[PRE218]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE218]'
- en: '[PRE219]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE219]'
- en: '[PRE220]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE220]'
- en: '[PRE221]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE221]'
- en: 'When we run the code, we get the following plot:'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当我们运行代码时，我们得到以下图表：
- en: '![Figure 13.8 – A plot showing the Apple stock closing price between January
    2013 and January 2023](img/B18118_13_008.jpg)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.8 – 显示2013年1月到2023年1月期间苹果股票收盘价的图表](img/B18118_13_008.jpg)'
- en: Figure 13.8 – A plot showing the Apple stock closing price between January 2013
    and January 2023
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8 – 显示2013年1月到2023年1月期间苹果股票收盘价的图表
- en: From the plot in *Figure 13**.8*, we can see that the stock has a positive upward
    trend with occasional dips.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图13.8*的图表中，我们可以看到股票呈现出正向上升的趋势，偶尔会有下跌。
- en: 'Convert the data into a NumPy array:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据转换为NumPy数组：
- en: '[PRE222]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE222]'
- en: For this exercise, we will be forecasting the daily stock closing price. Hence,
    we take the closing price column and convert it into a NumPy array. This way,
    we create a univariate time series for our experimentation.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将预测每日的股票收盘价。因此，我们取收盘价列并将其转换为NumPy数组。这样，我们为实验创建了一个单变量时间序列。
- en: 'Prepare the windowed dataset:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备窗口数据集：
- en: '[PRE223]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE223]'
- en: '[PRE224]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE224]'
- en: '[PRE225]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE225]'
- en: '[PRE226]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE226]'
- en: '[PRE227]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE227]'
- en: '[PRE228]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE228]'
- en: '[PRE229]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE229]'
- en: '[PRE230]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE230]'
- en: '[PRE231]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE231]'
- en: '[PRE232]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE232]'
- en: '[PRE233]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE233]'
- en: '[PRE234]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE234]'
- en: '[PRE235]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE235]'
- en: '[PRE236]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE236]'
- en: '[PRE237]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE237]'
- en: '[PRE238]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE238]'
- en: '[PRE239]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE239]'
- en: '[PRE240]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE240]'
- en: '[PRE241]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE241]'
- en: '[PRE242]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE242]'
- en: 'We are now familiar with this code block, which we use to prepare our data
    for modeling. Next, we will use the same set of architectures to carry out our
    experiment with our Apple stock dataset. The results are as follows:'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们现在已经熟悉了这个代码块，它用于准备数据以进行建模。接下来，我们将使用相同的架构进行苹果股票数据集的实验。结果如下：
- en: '| **Model** | **MAE** |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **MAE** |'
- en: '| DNN | 4.56 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| DNN | 4.56 |'
- en: '| RNN | 2.24 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| RNN | 2.24 |'
- en: '| LSTM | 3.02 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | 3.02 |'
- en: '| CNN-LSTM | 18.75 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| CNN-LSTM | 18.75 |'
- en: Figure 13.9 – A table showing the MAE across various models
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9 – 显示各种模型的MAE表格
- en: From our results. we see that we achieved the best-performing model with our
    RNN architecture, having an MAE of 2.24\. You can now save your best model, which
    can be used to predict future stock values or applied to other forecasting problems.
    You can also tweak the hyperparameters further to see whether you can achieve
    a lower MAE.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的结果来看，我们使用RNN架构取得了最佳表现的模型，MAE为2.24。现在你可以保存你最好的模型，它可以用来预测未来的股票价值，或应用于其他预测问题。你也可以进一步调整超参数，看看是否能达到更低的MAE。
- en: Note
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: This model is an illustration of what is possible with forecasting with neural
    networks. However, we must be aware of its limitations. Please refrain from making
    financial decisions using this model as real-world stock market predictions capture
    complex relationships such as economic indicators, market sentiment, and other
    interdependencies, which our basic model does not take into consideration.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型展示了利用神经网络进行预测的可能性。然而，我们必须意识到它的局限性。请不要使用此模型做出金融决策，因为现实世界的股市预测涉及复杂的关系，如经济指标、市场情绪以及其他相互依赖性，而我们的基础模型并未考虑这些因素。
- en: With this, we have come to the end of this chapter and the book.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就来到了本章和整本书的结尾。
- en: Summary
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 总结
- en: In this final chapter, we explored some advanced concepts for working with time
    series forecasting with TensorFlow. We saw both how to use in-built learning rate
    schedulers as well as designing custom-made schedulers tailored to our needs.
    Then, we used more specialized models, such as RNNs, LSTM networks, and the combination
    of CNNs and LSTM. We also saw how we could apply Lambda layers to implement custom
    operations and add flexibility to our network architecture.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们探讨了一些使用TensorFlow进行时间序列预测的高级概念。我们了解了如何使用内建的学习率调度器，也学习了如何设计定制的调度器来满足我们的需求。接着，我们使用了更专业的模型，如RNN、LSTM网络，以及CNN和LSTM的组合。我们还学习了如何应用Lambda层来实现自定义操作并为我们的网络架构增加灵活性。
- en: To conclude the chapter, we worked on forecasting the Apple stock closing price.
    By the end of this chapter, you should have a good understanding of applying concepts
    such as learning rate schedulers and Lambda layers and effectively building time
    series forecasting models using various architectures in readiness for your exam.
    Good luck!
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结时，我们进行了苹果股票收盘价的预测。到本章结束时，你应该已经充分理解如何应用学习率调度器、Lambda 层等概念，并且能够使用不同架构有效地构建时间序列预测模型，为你的考试做好准备。祝你好运！
- en: Note from the author
  id: totrans-405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作者的话
- en: It gives me great pleasure to see you move from the very fundamentals of machine
    learning to building various projects using TensorFlow. You have now explored
    building models with different neural network architectures. You now have a solid
    foundation upon which you can, and should, build an incredible career as a certified
    TensorFlow developer. You can only become a top developer by building solutions.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 看到你从机器学习的基础知识到使用 TensorFlow 构建各种项目，我感到非常高兴。你现在已经探索了如何使用不同的神经网络架构来构建模型。你已经拥有了坚实的基础，可以并且应该在此基础上，作为一名认证的
    TensorFlow 开发者，构建一个令人惊叹的职业生涯。你只有通过构建解决方案，才能成为顶尖开发者。
- en: Everything we have covered in this book will serve you well as you finalize
    your preparation for the TensorFlow Developer Certificate exam and beyond. I want
    to congratulate you for not giving up and working through all the concepts and
    projects in this book and the exercises. I would like to encourage you to continue
    learning, experimenting, and keeping tabs on the latest developments in the field
    of machine learning. I wish you success in your exam and your career ahead.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中涵盖的所有内容将帮助你在最终准备 TensorFlow 开发者证书考试以及更远的未来中取得成功。我想祝贺你没有放弃，成功完成了本书中的所有概念、项目和练习。我鼓励你继续学习、实验，并保持对机器学习领域最新发展的关注。祝你考试顺利，并在未来的职业生涯中取得成功。
- en: Questions
  id: totrans-408
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Load the Google stock data from Yahoo Finance for 01-01-2015 to 01-01-2020.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Yahoo Finance 加载 2015-01-01 到 2020-01-01 的 Google 股票数据。
- en: Create training, forecasting, and plotting functions.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练、预测和绘图函数。
- en: Prepare the data for training.
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数据以进行训练。
- en: Build DNN, CNN, LSTM, and CNN-LSTM models to model the data.
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建 DNN、CNN、LSTM 和 CNN-LSTM 模型来建模数据。
- en: Evaluate the models using MAE and MSE.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 MAE 和 MSE 评估模型。
- en: References
  id: totrans-414
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Shi, X., Chen, Z., Wang, H., Yeung, D. Y., Wong, W. K., & Woo, W. C. (2015).
    *Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting*.
    In Advances in Neural Information Processing Systems (pp. 802–810) [https://papers.nips.cc/paper/2015/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.xhtml](https://papers.nips.cc/paper/2015/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.xhtml%0D)'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi, X., Chen, Z., Wang, H., Yeung, D. Y., Wong, W. K., & Woo, W. C. (2015).
    *卷积 LSTM 网络：一种用于降水短期预测的机器学习方法*。在神经信息处理系统的进展（第802–810页）[https://papers.nips.cc/paper/2015/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.xhtml](https://papers.nips.cc/paper/2015/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.xhtml%0D)
- en: Karim, F., Majumdar, S., Darabi, H., & Harford, S. (2019). *LSTM fully convolutional
    networks for time series classification*. IEEE Access, 7, 1662-1669
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karim, F., Majumdar, S., Darabi, H., & Harford, S. (2019). *用于时间序列分类的 LSTM 全卷积网络*。IEEE
    Access, 7, 1662-1669
- en: Siami-Namini, S., Tavakoli, N., & Siami Namin, A. (2019). *The Performance of
    LSTM and BiLSTM in Forecasting Time Series*. In 2019 IEEE International Conference
    on Big Data
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siami-Namini, S., Tavakoli, N., & Siami Namin, A. (2019). *LSTM 和 BiLSTM 在时间序列预测中的表现*。2019
    IEEE 大数据国际会议
- en: 'TensorFlow learning rate scheduler: [https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler%0D)'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 学习率调度器：[https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler%0D)
- en: '*Lambda* *layers*: [https://keras.io/api/layers/core_layers/lambda/](https://keras.io/api/layers/core_layers/lambda/)'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Lambda* *层*：[https://keras.io/api/layers/core_layers/lambda/](https://keras.io/api/layers/core_layers/lambda/)'
- en: '*Time series* *forecasting*: [https://www.tensorflow.org/tutorials/structured_data/time_series](https://www.tensorflow.org/tutorials/structured_data/time_series%0D)'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间序列* *预测*：[https://www.tensorflow.org/tutorials/structured_data/time_series](https://www.tensorflow.org/tutorials/structured_data/time_series%0D)'
- en: '*tf.data: Build TensorFlow input* *pipelines*. [https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data%0D)'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*tf.data：构建 TensorFlow 输入* *管道*。[https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data%0D)'
- en: '*Windowed datasets for time* *series*: [https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing](https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing%0D)'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间序列的窗口数据集*：[https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing](https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing%0D)'
