- en: Building Your First GAN with PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch构建您的第一个GAN
- en: In previous chapters, we covered the idea of using adversarial learning to generate
    simple signals with NumPy and learned about the new features and capabilities
    of PyTorch 1.3\. It's time for us to use PyTorch to train a GAN model for generating
    interesting samples.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讲解了使用对抗学习生成简单信号的概念，并学习了PyTorch 1.3的新特性和能力。现在是时候使用PyTorch来训练一个GAN模型，生成有趣的样本了。
- en: 'In this chapter, we will introduce you to a classic and well-performing GAN
    model, called DCGAN, to generate 2D images. You will learn the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将向您介绍一个经典且表现优异的GAN模型，称为DCGAN，用于生成2D图像。您将学习以下内容：
- en: The architecture of DCGANs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DCGAN的架构
- en: The training and evaluation of DCGANs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DCGAN的训练与评估
- en: Using a DCGAN to generate handwritten digits, human faces
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DCGAN生成手写数字、人脸图像
- en: Having fun with the generator network by performing image interpolation and
    arithmetic calculation on the latent vectors to change the image attributes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对潜在向量进行图像插值和算术计算，改变图像属性，与生成器网络一起玩乐
- en: By the end of this chapter, you will have grasped the core architecture design
    of GAN models for generating image data and have a better understanding of the
    relationship between latent vectors and generated samples.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将掌握GAN模型生成图像数据的核心架构设计，并更好地理解潜在向量与生成样本之间的关系。
- en: Introduction to Deep Convolutional GANs
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积生成对抗网络（DCGAN）简介
- en: '**DCAGN** (**Deep Convolutional Generative Adversarial ****Network**) is one
    of the early well-performing and stable approaches to generate images with adversarial
    training. Let''s take a look back at the simple example in [Chapter 1](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml), *Generative
    Adversarial Networks Fundamentals*.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**DCGAN**（**深度卷积生成对抗网络**）是早期表现良好且稳定的生成图像的对抗训练方法之一。让我们回顾一下[第1章](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml)中的简单例子，*生成对抗网络基础*。'
- en: 'Here, even when we only train a GAN to manipulate 1D data, we have to use multiple
    techniques to ensure a stable training. A lot of things could go wrong in the
    training of GANs. For example, either a generator or a discriminator could overfit
    if one or the other does not converge. Sometimes, the generator only generates
    a handful of sample varieties. This is called **mode collapse**. The following
    is an example of mode collapse, where we want to train a GAN with some popular
    meme images in China called **Baozou**. We can see that our GAN is only capable
    of generating one or two memes at a time. Problems that commonly occur in other
    machine learning algorithms such as gradient vanishing/explosion and underfitting
    can also look familiar in the training of GANs. Therefore, just replacing 1D data
    with 2D images won''t easily guarantee successful training:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，即使我们只训练一个GAN来处理1D数据，我们也必须使用多种技术来确保训练的稳定性。在GAN的训练过程中，很多问题都可能出现。例如，如果生成器或判别器没有收敛，其中一个可能会过拟合。有时，生成器只能生成少数几种样本变体，这被称为**模式崩溃**。以下是模式崩溃的一个例子，我们想要训练一个GAN来生成一些中国的热门表情包，叫做**暴走**。我们可以看到，GAN只能一次生成一两个表情包。其他机器学习算法中常见的梯度消失/爆炸和欠拟合等问题，在GAN训练中也很常见。因此，单纯将1D数据替换为2D图像并不能轻易保证训练成功：
- en: '![](img/6e458f91-d634-486e-abc7-1b48a8d0150c.png)![](img/668363a6-4abe-4656-9aff-65bb9c3a0a9c.png)![](img/949bf1ca-dc4d-461d-9c55-1a5134f91129.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e458f91-d634-486e-abc7-1b48a8d0150c.png)![](img/668363a6-4abe-4656-9aff-65bb9c3a0a9c.png)![](img/949bf1ca-dc4d-461d-9c55-1a5134f91129.png)'
- en: 'Mode collapse in GAN training (left: some training samples; middle: results
    at 492nd iteration; right: results at 500th iteration)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GAN训练中的模式崩溃（左：一些训练样本；中：第492次迭代的结果；右：第500次迭代的结果）
- en: 'To ensure the stable training of GANs on image data like this, a DCGAN uses
    three techniques:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保GAN在此类图像数据上的稳定训练，DCGAN使用了三种技术：
- en: Getting rid of fully connected layers and only using convolution layers
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摒弃全连接层，仅使用卷积层
- en: Using strided convolution layers to perform downsampling, instead of using pooling
    layers
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用步长卷积层进行下采样，而不是使用池化层
- en: Using ReLU/leakyReLU activation functions instead of Tanh between hidden layers
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ReLU/leakyReLU激活函数替代隐藏层之间的Tanh
- en: In this section, we will introduce the architectures of the generator and discriminator
    of the DCGAN and learn how to generate images with it. We'll use MNIST ([http://yann.lecun.com/exdb/mnist](http://yann.lecun.com/exdb/mnist))
    samples to illustrate the architecture of a DCGAN and use it to train the model
    in the next two sections.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍 DCGAN 的生成器和判别器架构，并学习如何使用它生成图像。我们将使用 MNIST ([http://yann.lecun.com/exdb/mnist](http://yann.lecun.com/exdb/mnist))
    样本来说明 DCGAN 的架构，并在接下来的两节中使用它来训练模型。
- en: The architecture of generator
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器的架构
- en: 'The generator network of a DCGAN contains 4 hidden layers (we treat the input
    layer as the 1^(st) hidden layer for simplicity) and 1 output layer. Transposed
    convolution layers are used in hidden layers, which are followed by batch normalization
    layers and ReLU activation functions. The output layer is also a transposed convolution
    layer and Tanh is used as the activation function. The architecture of the generator
    is shown in the following diagram:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN 的生成器网络包含 4 个隐藏层（为简化起见，我们将输入层视为第 1 个隐藏层）和 1 个输出层。隐藏层中使用转置卷积层，后面跟随批量归一化层和
    ReLU 激活函数。输出层也是一个转置卷积层，使用 Tanh 作为激活函数。生成器的架构如下图所示：
- en: '![](img/383259d3-c12d-4c77-91b4-7286a0fbe20d.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/383259d3-c12d-4c77-91b4-7286a0fbe20d.png)'
- en: Generator architecture in DCGAN
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN 中的生成器架构
- en: The 2^(nd), 3^(rd),and 4^(th) hidden layers and the output layer have a stride
    value of 2\. The 1^(st) layer has a padding value of 0 and the other layers have
    a padding value of 1\. As the image (feature map) sizes increase by two in deeper
    layers, the numbers of channels are decreasing by half. This is a common convention
    in the architecture design of neural networks. All kernel sizes of transposed
    convolution layers are set to 4 x 4\. The output channel can be either 1 or 3,
    depending on whether you want to generate grayscale images or color images.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 第 2、3、4 个隐藏层和输出层的步幅值为 2。第 1 层的填充值为 0，其余层的填充值为 1。随着图像（特征图）尺寸在更深的层中增加一倍，通道数减少一半。这是神经网络架构设计中的一种常见约定。所有转置卷积层的卷积核大小都设置为
    4 x 4。输出通道可以是 1 或 3，具体取决于你是想生成灰度图像还是彩色图像。
- en: The transposed convolution layer can be considered as the **reverse process**
    of a normal convolution. It was once called by some a deconvolution layer, which
    is misleading because the transposed convolution is not the **inverse** of convolution.
    Most convolution layers are not invertible, because they are ill-conditioned (have
    extremely large condition numbers) from the linear algebra perspective, which
    makes their pseudoinverse matrices unfit for representing the inverse process.
    If you are interested in finding the inverse of a convolution kernel, you can
    search for numerical deconvolution methods on the internet.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积层可以被看作是普通卷积的**逆过程**。它曾一度被一些人称为反卷积层，这种叫法具有误导性，因为转置卷积并不是卷积的**逆**操作。从线性代数的角度来看，大多数卷积层是不可逆的，因为它们是病态的（具有极大的条件数），这使得它们的伪逆矩阵不适合表示逆过程。如果你有兴趣寻找卷积核的逆运算方法，可以在互联网上搜索数值反卷积方法。
- en: The architecture of a discriminator
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别器的架构
- en: 'The discriminator network of a DCGAN consists of 4 hidden layers (again, we
    treat the input layer as the 1^(st) hidden layer) and 1 output layer. Convolution
    layers are used in all layers, which are followed by batch normalization layers
    except that the first layer does not have batch normalization. LeakyReLU activation
    functions are used in the hidden layers and Sigmoid is used for the output layer.
    The architecture of the discriminator is shown in the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN 的判别器网络由 4 个隐藏层（同样，我们将输入层视为第 1 个隐藏层）和 1 个输出层组成。所有层中都使用卷积层，后面跟随批量归一化层，除了第一层没有批量归一化。隐藏层中使用
    LeakyReLU 激活函数，输出层使用 Sigmoid 激活函数。判别器的架构如下所示：
- en: '![](img/0a1cf07f-9051-42ec-ad32-e29f8eae7b55.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a1cf07f-9051-42ec-ad32-e29f8eae7b55.png)'
- en: Discriminator architecture in DCGAN
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN 中的判别器架构
- en: The input channel can be either 1 or 3, depending on whether you are dealing
    with grayscale images or color images. All hidden layers have a stride value of
    2 and a padding value of 1 so that their output image sizes will be half the input
    images. As image sizes increase in deeper layers, the numbers of channels are
    increasing by twice. All kernels in convolution layers are of a size of 4 x 4. The
    output layer has a stride value of 1 and a padding value of 0\. It maps 4 x 4
    feature maps to single values so that the Sigmoid function can transform the value
    into prediction confidence.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 输入通道可以是 1 或 3，具体取决于你处理的是灰度图像还是彩色图像。所有隐藏层的步幅值为 2，填充值为 1，因此它们的输出图像尺寸将是输入图像的一半。随着图像在更深层次的尺寸增大，通道的数量会翻倍。卷积层中的所有卷积核大小为
    4 x 4。输出层的步幅值为 1，填充值为 0。它将 4 x 4 的特征图映射为单一值，以便 Sigmoid 函数能够将该值转换为预测置信度。
- en: Creating a DCGAN with PyTorch
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch 创建 DCGAN
- en: Let's start writing PyTorch code to create a DCGAN model. Here, we assume that
    you are using the Python 3.7 environment in Ubuntu 18.04\. If not, please refer
    to [Chapter 2](4459c703-9610-43e7-9eda-496d63a45924.xhtml), *Getting Started with
    PyTorch 1.3*, to learn how to create an Anaconda environment.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始编写 PyTorch 代码来创建一个 DCGAN 模型。在这里，我们假设你正在使用 Ubuntu 18.04 的 Python 3.7
    环境。如果不是，请参考 [第 2 章](4459c703-9610-43e7-9eda-496d63a45924.xhtml)，*PyTorch 1.3 入门*，了解如何创建
    Anaconda 环境。
- en: 'First, let''s create a Python source file called `dcgan.py` and import the
    packages that we need:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个名为 `dcgan.py` 的 Python 源文件，并导入我们需要的包：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, NumPy is only used to initialize a random seed. If you don't have NumPy
    installed, simple replace `np.random` with `random` and insert the `import random`
    line after `import os`. In the last line of code, we import a module called `utils`,
    which is a custom utility package defined in the `utils.py` file. The full source
    code of `utils.py` is available under the code repository for this chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，NumPy 仅用于初始化随机种子。如果你没有安装 NumPy，只需将 `np.random` 替换为 `random`，并在 `import os`
    后插入 `import random`。在代码的最后一行，我们导入了一个名为 `utils` 的模块，它是一个自定义的实用程序包，定义在 `utils.py`
    文件中。`utils.py` 的完整源代码可以在本章节的代码仓库中找到。
- en: In this book, we will put most of the PyTorch-independent helper functions (including
    file organization, learning rate adjustment, logging, tensor visualization, and
    so on) in this `utils.py` file. Therefore, we will also come across this module
    in future chapters. Don't forget to update this file as we move on to later chapters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将把大部分与 PyTorch 无关的辅助函数（包括文件组织、学习率调整、日志记录、张量可视化等）放在 `utils.py` 文件中。因此，在未来的章节中我们还会遇到这个模块。随着章节的推进，别忘了更新这个文件。
- en: 'Then, we define the output path and hyperparameters. Note that here we set
    the minimal channel size of hidden layers in both the generator and discriminator
    to `64`, because we find that the value of `128` as we previously show could lead
    to the overfitting of the discriminator:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义输出路径和超参数。请注意，这里我们将生成器和判别器中隐藏层的最小通道大小设置为 `64`，因为我们发现之前展示的 `128` 可能导致判别器的过拟合：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you don't have a CUDA-enabled graphics card and want to train the networks
    on the CPU, you can change `CUDA` to `False`. `DATA_PATH` points to the root directory
    of the MNIST dataset. If you haven't downloaded and properly preprocessed MNIST
    yet, simply point it to any directory (such as `'.'`) and we can download it later. `BATCH_SIZE`
    has a major impact on how much GPU memory your code will consume. If you are not
    sure what batch size is appropriate for your system, you can start at a small
    value, train your model for 1 epoch, and double the batch size until errors pop
    up.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有 CUDA 支持的显卡，并且想在 CPU 上训练网络，你可以将 `CUDA` 设置为 `False`。`DATA_PATH` 指向 MNIST
    数据集的根目录。如果你还没有下载并正确预处理 MNIST，只需将其指向任意目录（例如 `'.'`），稍后我们可以下载数据。`BATCH_SIZE` 会对代码消耗的
    GPU 内存量产生重大影响。如果你不确定哪个批量大小适合你的系统，可以从一个较小的值开始，训练模型 1 个 epoch，然后将批量大小加倍，直到出现错误。
- en: For MNIST, setting `BATCH_SIZE` to 128 should be good enough and it costs less
    than 1 GB of GPU memory. `IMAGE_CHANNEL` describes the number of color channels
    of image samples. Since all images in MNIST are single-channel, we should set
    it to 1. `EPOCH_NUM` has a great impact on the training time of neural networks.
    If you want better results, setting a larger epoch number and small learning rates
    is almost always a good strategy. We set `seed=1` so that your results should
    look exactly the same as what we get in this book.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MNIST，设置 `BATCH_SIZE` 为 128 应该足够，而且 GPU 内存消耗不到 1GB。`IMAGE_CHANNEL` 描述图像样本的颜色通道数。由于
    MNIST 中的所有图像都是单通道的，因此我们应该将其设置为 1。`EPOCH_NUM` 对神经网络的训练时间有很大的影响。如果你希望得到更好的结果，通常将
    epoch 数量设置大一点，学习率设置小一点是个不错的策略。我们将 `seed=1`，这样你的结果应该和我们在本书中得到的结果完全一致。
- en: 'Next, we need to do some preparation before creating the networks:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在创建网络之前，我们需要做一些准备工作：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, `utils.clear_folder(OUT_PATH)` will empty the output folder for us and
    create one if it doesn't exist. `sys.stdout = utils.StdOut(LOG_FILE)` will redirect
    all messages from `print` to the log file and show these messages in the console
    at the same time. Refer to the `utils.py` file if you are interested in the implementations. `cudnn.benchmark
    = True` will tell cuDNN to choose the best set of algorithms for your model if
    the size of input data is fixed; otherwise, cuDNN will have to find the best algorithms
    at each iteration.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`utils.clear_folder(OUT_PATH)` 将清空输出文件夹，并在该文件夹不存在时创建一个。`sys.stdout = utils.StdOut(LOG_FILE)`
    将把所有 `print` 的消息重定向到日志文件，并同时在控制台显示这些消息。如果你对实现感兴趣，可以参考 `utils.py` 文件。`cudnn.benchmark
    = True` 将告诉 cuDNN 为你的模型选择最优的算法集，如果输入数据的大小是固定的；否则，cuDNN 将在每次迭代时都寻找最佳算法。
- en: If you have previously done some training tasks on CNNs with PyTorch, you might
    notice that, sometimes, setting `cudnn.benchmark = True` will dramatically increase
    the GPU memory consumption, especially when your model architectures are changed
    during training and you are doing both training and evaluation in your code. Change
    it to `False` if you encounter strange **OOM** (**Out-Of-Memory**) issues.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前在使用 PyTorch 进行 CNN 训练时遇到过问题，你可能会注意到，有时候设置 `cudnn.benchmark = True` 会显著增加
    GPU 内存消耗，特别是在模型架构在训练过程中发生变化且你在代码中同时进行训练和评估时。如果遇到奇怪的 **OOM** (**内存溢出**) 问题，请将其改为
    `False`。
- en: Generator network
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成器网络
- en: 'Now, let''s define the generator network with PyTorch:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 PyTorch 来定义生成器网络：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the output layer does not have a batch normalization layer connected
    to it.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出层没有连接批归一化层。
- en: 'Let''s create a `helper` function to initialize the network parameters:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个 `helper` 函数来初始化网络参数：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There are only two types of layers in the generator network that contain trainable
    parameters: transposed convolution layers and batch normalization layers. Here,
    we initialize the convolution kernels based on the Gaussian distribution (normal
    distribution) with a mean of 0 and a standard deviation of 0.02\. We also need
    to initialize the affine parameters (scaling factors) in batch normalization.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成器网络中，只有两种类型的层包含可训练的参数：转置卷积层和批归一化层。在这里，我们根据高斯分布（正态分布）初始化卷积核，均值为 0，标准差为 0.02。我们还需要初始化批归一化中的仿射参数（缩放因子）。
- en: 'Now, we can create a `Generator` object, as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以按照以下方式创建一个 `Generator` 对象：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can check what modules are contained in the generator network by directly
    printing it. We won't show the output of it considering its length.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过直接打印生成器网络来检查其中包含的模块。考虑到输出的长度，我们不会显示它的输出。
- en: Discriminator network
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别器网络
- en: 'Now, let''s define the discriminator network:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义判别器网络：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that the input layer does not have a batch normalization layer connected
    to it. This is because, when applying batch normalization to all layers, it could
    lead to sample oscillation and model instability, as pointed out in the original
    paper.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输入层没有连接批归一化层。这是因为，当将批归一化应用到所有层时，可能会导致样本震荡和模型不稳定，正如原始论文中所指出的那样。
- en: 'Similarly, we can create a `Discriminator` object as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以按以下方式创建一个 `Discriminator` 对象：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Model training and evaluation
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练与评估
- en: We will use Adam as the training method for both the generator and discriminator
    networks. If you are interested in the details of gradient descent methods, please
    refer to [Chapter 3](8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml), *Best Practices
    for Model Design and Training**,* to learn more about the common training methods.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Adam作为生成器和判别器网络的训练方法。如果你对梯度下降方法的细节感兴趣，请参考[第3章](8aa2141f-1f14-405f-a5e6-31daf5f4163a.xhtml)，*模型设计与训练最佳实践*，以了解更多常见的训练方法。
- en: 'Let''s first define the loss function for the discriminator network and `optimizers` for
    both of the networks:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先为判别器网络定义损失函数，并为两个网络定义`optimizers`：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, `nn.BCELoss()` represents the Binary Cross-Entropy loss function, which
    we previously used in [Chapter 1](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml), *Generative
    Adversarial Networks Fundamentals*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`nn.BCELoss()`表示二元交叉熵损失函数，我们在[第1章](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml)，*生成对抗网络基础*中曾经使用过。
- en: 'Next, let''s load the MNIST dataset to the GPU memory:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将MNIST数据集加载到GPU内存中：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can also add a `pin_memory=True` argument when calling `torch.utils.data.DataLoader()` on
    small datasets, which will make sure data is stored at fixed GPU memory addresses
    and thus increase the data loading speed during training.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理小数据集时，你还可以在调用`torch.utils.data.DataLoader()`时添加`pin_memory=True`参数，这样可以确保数据存储在固定的GPU内存地址，从而加快训练时的数据加载速度。
- en: Training iteration
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练迭代
- en: 'The training procedure is basically the same as in the simple example in [Chapter
    1](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml), *Generative Adversarial Networks
    Fundamentals*:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程与[第1章](66a945c3-9fd3-4d27-a6ec-b47d2e299e84.xhtml)中的简单示例基本相同，*生成对抗网络基础*：
- en: Train the discriminator with the real data and recognize it as real.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用真实数据训练判别器，并将其识别为真实数据。
- en: Train the discriminator with the fake data and recognize it as fake.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用虚假数据训练判别器，并将其识别为虚假数据。
- en: Train the generator with the fake data and recognize it as real.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用虚假数据训练生成器，并将其识别为真实数据。
- en: 'The first two steps let the discriminator learn how to tell the difference
    between real data and fake data. The third step teaches the generator how to confuse
    the discriminator with generated samples:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个步骤让判别器学习如何区分真实数据和虚假数据。第三步教会生成器如何用生成的样本混淆判别器：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we create the `real_label` and `fake_label` tensors in real time because
    there is no guarantee that all sample batches will have the same size (the last
    batch is often smaller depending on the batch size and the total number of training
    samples).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们实时创建`real_label`和`fake_label`张量，因为不能保证所有样本批次的大小相同（最后一个批次通常较小，具体取决于批量大小和训练样本的总数）。
- en: Visualizing generated samples
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化生成的样本
- en: 'It''s better if we can check how well the generator is trained. Therefore,
    we need to export the generated images during training. Add these lines at the
    end of the `if` scope:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够检查生成器的训练效果会更好。因此，我们需要在训练过程中导出生成的图像。请在`if`语句的末尾添加以下代码行：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, your DCGAN is ready for training. Open the Terminal, `activate` the Anaconda
    environment and start training DCGAN:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你的DCGAN准备好进行训练了。打开终端，`activate` Anaconda环境并开始训练DCGAN：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The training takes about 13 minutes on a GTX 1080Ti graphics card. If you don't
    like the generated samples even before the training is finished, you can always
    press <q>Ctrl</q> + <q>C</q> to cancel the training. The generated images after
    the 1^(st) and 25^(th) epoch are shown in the following. Note that we only show
    halves of the generated images (that is, 64 samples).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在GTX 1080Ti显卡上训练大约需要13分钟。如果你在训练未完成前就不喜欢生成的样本，你可以随时按<q>Ctrl</q> + <q>C</q>来取消训练。第1轮和第25轮后的生成图像如下所示。请注意，我们只展示生成图像的一半（即64个样本）。
- en: 'We can see that the DCGAN does a good job at generating handwritten digits:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，DCGAN在生成手写数字方面表现不错：
- en: '![](img/301cf81f-ede8-408f-884f-9d4db4c6ac17.png)![](img/9d465db8-37e7-4265-ba85-5008aaa3ce4c.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/301cf81f-ede8-408f-884f-9d4db4c6ac17.png)![](img/9d465db8-37e7-4265-ba85-5008aaa3ce4c.png)'
- en: Generated images by the DCGAN from MNIST after the 1st and 25th epoch
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN在MNIST数据集上经过第1轮和第25轮后的生成图像
- en: 'For your reference, here is a list of GPU memory consumption with different
    `BATCH_SIZE` values. Note that no matter how large the batch size is, the total
    training time is almost unchanged, since the total workload of the computation
    is basically the same:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 供你参考，以下是不同`BATCH_SIZE`值下GPU内存消耗的列表。请注意，无论批量大小如何，训练的总时间几乎不变，因为计算的总工作量基本相同：
- en: '| Batch size | 128 | 256 | 512 | 1024 | 2048 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 128 | 256 | 512 | 1024 | 2048 |'
- en: '| GPU memory | 939 MB | 1283 MB | 1969 MB | 3305 MB | 6011 MB |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| GPU内存 | 939 MB | 1283 MB | 1969 MB | 3305 MB | 6011 MB |'
- en: Checking GPU usage information
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查GPU使用信息
- en: Here, we will talk about how to check GPU usage along with other hardware usage
    information in Windows 10 and Ubuntu 18.04.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论如何在Windows 10和Ubuntu 18.04中查看GPU使用情况以及其他硬件使用信息。
- en: In Windows 10, the easiest way to check hardware usage (including GPU usage)
    is using Task Manager. You can open it by pressing <q>Ctrl</q> + <q>Shift</q>
    + <q>Esc</q>, and switch to the Performance panel. All hardware usage information
    is available to you now.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows 10中，检查硬件使用情况（包括GPU使用情况）最简单的方法是使用任务管理器。你可以通过按<q>Ctrl</q> + <q>Shift</q>
    + <q>Esc</q>打开任务管理器，并切换到性能面板。现在，你可以查看所有硬件使用情况信息。
- en: In Ubuntu 18.04, you can check CPU, RAM, and drive usage with **GNOME System
    Monitor**, which is shipped with the system. You can search for the system monitor
    in the Application menu, or run `gnome-system-monitor` in a Terminal to open it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在Ubuntu 18.04中，你可以使用**GNOME系统监视器**查看CPU、RAM和磁盘使用情况，它是系统自带的。你可以在应用菜单中搜索系统监视器，或者在终端中运行`gnome-system-monitor`来打开它。
- en: 'Alternatively, you can install a GNOME extension to illustrate the usage graphs
    in the status bar. We recommend that you use the** system-monitor extension**
    ([https://extensions.gnome.org/extension/120/system-monitor](https://extensions.gnome.org/extension/120/system-monitor))
    for this purpose. To install it, you first need to install several prerequisites:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以安装一个GNOME扩展来在状态栏中显示使用图表。我们建议你使用**system-monitor扩展**（[https://extensions.gnome.org/extension/120/system-monitor](https://extensions.gnome.org/extension/120/system-monitor)）来实现这个目的。要安装它，你首先需要安装几个先决条件：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Then, open a Firefox browser and navigate to this site, [https://addons.mozilla.org/en-US/firefox/addon/gnome-shell-integration](https://addons.mozilla.org/en-US/firefox/addon/gnome-shell-integration/),
    to install the browser extension for easy installation of GNOME extensions provided
    by [http://gnome.org](http://gnome.org). You also need to run `sudo apt-get install
    chrome-gnome-shell` in Terminal.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，打开Firefox浏览器，访问这个网站，[https://addons.mozilla.org/en-US/firefox/addon/gnome-shell-integration](https://addons.mozilla.org/en-US/firefox/addon/gnome-shell-integration/)，安装浏览器扩展程序，以便轻松安装[http://gnome.org](http://gnome.org)提供的GNOME扩展。你还需要在终端中运行`sudo
    apt-get install chrome-gnome-shell`。
- en: Next, open the web page, [https://extensions.gnome.org/extension/120/system-monitor](https://extensions.gnome.org/extension/120/system-monitor),
    with the Firefox browser; you'll see a switch button on the right side of the
    extension title. Click it to switch it to `ON` and you will be prompted to install
    the system-monitor extension.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开网页，[https://extensions.gnome.org/extension/120/system-monitor](https://extensions.gnome.org/extension/120/system-monitor)，使用Firefox浏览器；你将看到扩展标题右侧的开关按钮。点击它将开关切换到`ON`，然后你将被提示安装system-monitor扩展。
- en: Finally, press *Alt* + *F2*, type in `r`, and then press <q>Enter</q>. This
    will restart the GNOME shell so that the system-monitor extension will be activated.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，按下*Alt* + *F2*，输入`r`，然后按<q>Enter</q>。这将重新启动GNOME shell，从而激活system-monitor扩展。
- en: 'To check GPU usage in Ubuntu, you can run this script in the Terminal to show
    it in real time:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Ubuntu中检查GPU使用情况，你可以在终端中运行这个脚本，实时显示GPU使用情况：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: You can also create a `.sh` file in a convenient directory, for example, `~/gpu.sh`: copy
    the script into this file, then run `chmod +x ~/.gpu.sh`. Then, you can simply
    run `./gpu.sh` in the Terminal whenever you need to check GPU usage.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在一个方便的目录中创建一个`.sh`文件，例如`~/gpu.sh`：将脚本复制到此文件中，然后运行`chmod +x ~/.gpu.sh`。然后，你可以在终端中简单地运行`./gpu.sh`，每当需要检查GPU使用情况时。
- en: Alternatively, there are many other tools you can use on Ubuntu, for example,
    NVTOP ([https://github.com/Syllo/nvtop](https://github.com/Syllo/nvtop)).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，Ubuntu上还有许多其他工具可以使用，例如NVTOP（[https://github.com/Syllo/nvtop](https://github.com/Syllo/nvtop)）。
- en: Moving to larger datasets
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移到更大的数据集
- en: Generating digits is fun. We can have way more fun generating other stuff, such
    as human faces and bedroom photos. To generate good complex images like these,
    we need more training samples than the 60,000 samples that MNIST offers. In this
    section, we will download two much larger datasets (CelebA and LSUN) and train
    the DCGAN on them to get more complex generated samples.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 生成数字是有趣的。我们可以通过生成其他东西，比如人脸和卧室照片，获得更多的乐趣。为了生成像这样的复杂图像，我们需要比MNIST提供的60,000个样本更多的训练样本。在本节中，我们将下载两个更大的数据集（CelebA和LSUN），并在其上训练DCGAN，以获得更复杂的生成样本。
- en: Generating human faces from the CelebA dataset
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从CelebA数据集中生成人脸
- en: The CelebFaces Attributes (**CelebA**, [http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html))
    dataset is a large-scale face attributes dataset with more than 200,000 celebrity
    images, each with 40 attribute annotations. We need to download the cropped and
    aligned images. We won't need any attribute annotation here so we only need to
    download the file named `img_align_celeba.zip`, which is no more than 2 GB in
    size.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: CelebFaces 属性（**CelebA**，[http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)）数据集是一个大规模的面部属性数据集，包含超过
    200,000 张名人图像，每张图像有 40 个属性标注。我们需要下载裁剪和对齐过的图像。由于我们在这里不需要任何属性标注，所以只需要下载名为`img_align_celeba.zip`的文件，文件大小不超过
    2 GB。
- en: If you can't download the CelebA dataset from official links, try these links
    provided by Kaggle and the official PyTorch tutorial: [https://www.kaggle.com/jessicali9530/celeba-dataset](https://www.kaggle.com/jessicali9530/celeba-dataset)
    and [https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8](https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8).
    Note that you only need to download `Img/img_align_celeba.zip` from the Google
    Drive link.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你无法从官方链接下载 CelebA 数据集，可以尝试以下 Kaggle 提供的链接和 PyTorch 官方教程：[https://www.kaggle.com/jessicali9530/celeba-dataset](https://www.kaggle.com/jessicali9530/celeba-dataset)
    和 [https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8](https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8)。请注意，你只需要从
    Google Drive 链接下载`Img/img_align_celeba.zip`文件。
- en: Extract the downloaded images to a directory, for example, `~/Data/CelebA`.
    Make sure all your images are contained in an individual directory inside this
    root directory so that the images are stored at a location such as `~/Data/CelebA/img_align_celeba/000001.png`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 将下载的图像提取到一个目录中，例如`~/Data/CelebA`。确保所有的图像都存储在这个根目录下的独立子目录中，以便图像存储在像`~/Data/CelebA/img_align_celeba/000001.png`这样的路径下。
- en: If you have a **Solid**-**State Drive** (**SSD**) with enough space plugged
    in your machine, we highly recommend you move all of your training samples to
    the SSD, especially when you have a powerful graphics card. Because when you are
    training neural networks on a very large dataset, which cannot fit in the GPU
    memory, the reading speed from physical drives could be the bottleneck of your
    training performance. Sometimes, the speed-up of SSD (reading samples at 50 MB/s)
    over the traditional hard drive (5 MB/s) can save you a big chunk of training
    time.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的机器上有足够空间的**固态硬盘**（**SSD**），我们强烈建议将所有训练样本移至 SSD，特别是当你有一块强力的显卡时。因为在训练非常大的数据集时，如果数据集无法完全加载到
    GPU 内存中，从物理硬盘读取数据的速度可能会成为训练性能的瓶颈。有时，SSD（50 MB/s）的读取速度相比传统硬盘（5 MB/s）可以大大缩短训练时间。
- en: 'We only need to alter 3 different parts of code in the previous section to
    train the DCGAN on the CelebA dataset:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要在上一节中修改 3 个不同的代码部分，就可以在 CelebA 数据集上训练 DCGAN：
- en: 'Change the dataset root directory:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改数据集根目录：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If you are not sure what absolute path you're currently at in the file manager
    on Ubuntu, simply press *Ctrl* + *L* and the full path will show up.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定在 Ubuntu 文件管理器中当前的绝对路径，可以按 *Ctrl* + *L*，此时完整路径会显示出来。
- en: 'Change the image channel number:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改图像通道数：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Redefine the `dataset` object:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新定义`dataset`对象：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, let''s run `python dcgan.py` in the Terminal and wait for a while. It
    takes about 88 minutes to finish 25 epochs of training on a GTX 1080Ti graphics
    card. The generated images after the 1^(st) epoch and the 25^(th) epoch are shown
    in the following. Again, we only show 64 generated samples:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在终端中运行`python dcgan.py`并等待一段时间。在 GTX 1080Ti 显卡上，完成 25 轮训练大约需要 88 分钟。生成的图像分别是第
    1 轮和第 25 轮训练后的结果。这里我们仅展示 64 张生成的样本：
- en: '![](img/ea55d23e-5e10-419c-a8ca-ddae7ed7a0f6.png)![](img/8ecc9a1e-f556-43f7-924d-87d57677506c.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea55d23e-5e10-419c-a8ca-ddae7ed7a0f6.png)![](img/8ecc9a1e-f556-43f7-924d-87d57677506c.png)'
- en: Generated images by the DCGAN from CelebA after the 1st and 25th epoch
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN 在 CelebA 数据集上经过第 1 轮和第 25 轮训练后生成的图像
- en: 'Here is a list of GPU memory consumption with different `BATCH_SIZE` values:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是不同`BATCH_SIZE`值下的 GPU 内存使用情况：
- en: '| Batch size | 64 | 128 | 256 | 512 | 1024 | 2048 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 64 | 128 | 256 | 512 | 1024 | 2048 |'
- en: '| GPU memory | 773 MB | 963 MB | 1311 MB | 2029 MB | 3441 MB | 6283 MB |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| GPU 内存 | 773 MB | 963 MB | 1311 MB | 2029 MB | 3441 MB | 6283 MB |'
- en: Generating bedroom photos from the LSUN dataset
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 LSUN 数据集生成卧室照片
- en: 'LSUN (Large-scale Scene Understanding, [https://www.yf.io/p/lsun](https://www.yf.io/p/lsun))
    is a large image dataset with 10 scene categories and 20 object categories. You
    can get the downloading toolkit from [https://github.com/fyu/lsun](https://github.com/fyu/lsun).
    We will use the `bedroom` category to train our DCGAN, which has more than 3 million
    bedroom photos:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: LSUN（大规模场景理解，[https://www.yf.io/p/lsun](https://www.yf.io/p/lsun)）是一个包含10个场景类别和20个物体类别的大型图像数据集。你可以从[https://github.com/fyu/lsun](https://github.com/fyu/lsun)获取下载工具包。我们将使用`bedroom`类别来训练我们的DCGAN，它包含超过300万张卧室照片：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You can also export the images as individual files with `python data.py export
    bedroom_train_lmdb --out_dir` `bedroom_train_img` so that you can easily use these
    images for other projects. But try not to directly open the image folder with
    your file manager. It will take a lot of RAM and time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用`python data.py export bedroom_train_lmdb --out_dir` `bedroom_train_img`将图像导出为单独的文件，这样你就可以轻松地将这些图像用于其他项目。但尽量不要直接通过文件管理器打开图像文件夹，因为这会占用大量内存和时间。
- en: The dataset is contained in an **LMDB** (**Lightning Memory-Mapped Database
    Manager**) database file, which is about 54 GB in size. Make sure the database
    files are located in the `bedroom_train_lmdb` directory so that PyTorch's data
    loader can recognize it when the root directory is specified.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集保存在**LMDB**（**Lightning Memory-Mapped Database Manager**）数据库文件中，文件大小约为54GB。确保数据库文件位于`bedroom_train_lmdb`目录下，以便PyTorch的数据加载器在指定根目录时能够识别它。
- en: 'Similarly, we only need to change 3 parts of the code to use the LSUN dataset
    for our model:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们只需要更改代码中的3个部分，就能使用LSUN数据集来训练我们的模型：
- en: 'Change the dataset root directory:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改数据集根目录：
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Change the image channel number:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更改图像通道数：
- en: '[PRE20]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Redefine the `dataset` object:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新定义`dataset`对象：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And don''t forget to install the `lmdb` library for Python so that we can read
    the database file:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 别忘了为Python安装`lmdb`库，这样我们才能读取数据库文件：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, let''s save the source file and run `python dcgan.py` in the Terminal.
    Since there are way more samples in the LSUN dataset, we don''t have to train
    the model for 25 epochs. Some of the generated images are already impressive even
    after the 1^(st) epoch of training. It takes about 5 hours to train for 5 epochs
    on a GTX 1080Ti graphics card. The generated images after the 1^(st) epoch and
    the 25^(th) epoch are shown in the following. Here, we only show 64 generated
    samples. We will not show the GPU memory consumption for LSUN because it''s almost
    the same as CelebA since the input images are both 3-channel and the network structure
    is not changed:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，保存源文件并在终端中运行`python dcgan.py`。由于LSUN数据集中有更多的样本，我们不需要训练25个epoch。一些生成的图像在第1个epoch训练后就已经相当出色了。在GTX
    1080Ti显卡上，训练5个epoch大约需要5小时。以下是第1个epoch和第25个epoch生成的图像。这里我们只展示64个生成样本。由于LSUN和CelebA的输入图像都是3通道，并且网络结构没有变化，我们不展示LSUN的GPU内存消耗，它几乎与CelebA相同：
- en: '![](img/9163ef11-96c4-42ca-8a45-89e602f19f80.png)![](img/71e1ad9d-6e46-4777-81a9-62fabc9f9df1.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9163ef11-96c4-42ca-8a45-89e602f19f80.png)![](img/71e1ad9d-6e46-4777-81a9-62fabc9f9df1.png)'
- en: Generated images by the DCGAN from LSUN after the 1st and 5th epochs
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN生成的LSUN图像，分别是在第1和第5个epoch后生成的
- en: 'Again, we''d like to point out that if you plan on training GANs on a large
    dataset, always consider using powerful GPUs and putting your dataset on an SSD.
    Here, we give two sets of performance comparisons. In the first configuration,
    we use an NVIDIA GTX 960 graphics card and put the training set on an **HDD**
    (**hard disk drive**). In the second configuration, we use an NVIDIA GTX 1080Ti
    graphics card and put the training set on an SSD. We can see the speedup of the
    powerful platform is life changing:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，如果你计划在大数据集上训练GAN，务必考虑使用强大的GPU，并将数据集放在SSD上。这里，我们提供了两组性能对比。在第一种配置中，我们使用NVIDIA
    GTX 960显卡，并将训练集放在**HDD**（**硬盘驱动器**）上。在第二种配置中，我们使用NVIDIA GTX 1080Ti显卡，并将训练集放在SSD上。我们可以看到强大平台的加速效果堪称改变人生：
- en: '| Dataset | CelebA | LSUN |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | CelebA | LSUN |'
- en: '| GTX 960 + HDD | 2 hours/epoch | 16.6 hours/epoch |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| GTX 960 + HDD | 2小时/epoch | 16.6小时/epoch |'
- en: '| GTX 1080Ti + SSD | 3.5 minutes/epoch | 53 minutes/epoch |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| GTX 1080Ti + SSD | 3.5分钟/epoch | 53分钟/epoch |'
- en: '| Speedup | 34X | 19X |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 加速 | 34X | 19X |'
- en: Having fun with the generator network
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 玩转生成器网络
- en: Now that our first image generator is trained, aren't you curious about what
    it is capable of and how images are generated from random noise vectors? In this
    section, we will have some fun with the generator network. First, we will choose
    two random vectors and calculate the interpolation between them to see what images
    will be generated. Second, we will choose some exemplary vectors and perform arithmetic
    calculations on them to find out what changes appear in the generated samples.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的第一个图像生成器已经训练完成，你是否好奇它能做些什么，以及如何从随机噪声向量生成图像？在本节中，我们将通过生成器网络来进行一些有趣的操作。首先，我们将选择两个随机向量并计算它们之间的插值，看看会生成什么样的图像。其次，我们将选择一些典型的向量并对其进行算术运算，看看生成的样本中会出现什么变化。
- en: First, we need a test version of the DCGAN code.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一个测试版本的DCGAN代码。
- en: 'Copy your original `dcgan.py` file to `dcgan_test.py`. Next, we need to make
    some changes to our new file. First, we need to replace these lines of just the
    `Generator` class:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始的 `dcgan.py` 文件复制到 `dcgan_test.py`。接下来，我们需要对新文件进行一些修改。首先，我们需要替换掉这些仅包含 `Generator`
    类的行：
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We replace them with the following lines (you can either delete them or simply
    comment them out):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将它们替换为以下几行（你可以删除它们，或者只需注释掉它们）：
- en: '[PRE24]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Next, we need to remove (or comment out) the `weights_init`, `Discriminator`,
    `dataset`, `dataloader`, `criterion`, and `optimizer` objects.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要移除（或注释掉）`weights_init`、`Discriminator`、`dataset`、`dataloader`、`criterion`
    和 `optimizer` 对象。
- en: 'Next, we need to replace the entire training iteration section with this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将整个训练迭代部分替换为以下内容：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We''re almost done. We need to add the following code at the end:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们快完成了。我们需要在最后添加以下代码：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, go back to the top of the code file and add a line in the `import` section:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到代码文件的顶部，在 `import` 部分添加一行：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And finally, we need to add a line to the variable definitions. Just after
    the line that says `CUDA = True`, add this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要在变量定义中添加一行代码。在写着 `CUDA = True` 的那一行之后，添加以下内容：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The values for `VIZ_MODE` are 0 for random, 1 for interpolation, and 2 for semantic
    calculation. This will be used as we move forward through the three sets of code.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`VIZ_MODE` 的值为 0 代表随机生成，1 代表插值，2 代表语义计算。这将在我们继续进行三组代码时使用。'
- en: We need to export the input vector and the generated images to file. The full
    code for the DCGAN testing is available under the code repository for this chapter,
    which is called `dcgan_test.py`. And don't forget to delete or comment out the `utils.clear_folder(OUT_PATH)` line; otherwise,
    all your training results will be deleted, which would be a bad thing.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将输入向量和生成的图像导出到文件。DCGAN测试的完整代码可以在本章的代码库中找到，文件名为 `dcgan_test.py`。别忘了删除或注释掉
    `utils.clear_folder(OUT_PATH)` 这一行，否则你所有的训练结果会被删除，这将是个坏消息。
- en: Image interpolation
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像插值
- en: The generator network maps the input random vector (the latent vector) to a
    generated image. If we perform linear interpolation on the latent vectors, the
    corresponding output images also obey the interpolation relation. Let's take the
    trained model on CelebA, for example.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络将输入的随机向量（潜在向量）映射到生成的图像。如果我们对潜在向量进行线性插值，相应的输出图像也会遵循插值关系。我们以在CelebA上训练的模型为例：
- en: 'First, let''s randomly choose two vectors that generate clean images. Here,
    we set `BATCH_SIZE=10` for simplicity. We''ll also add the beginnings of an `if`
    conditional to allow easy selection of what parts of the code to run:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们随机选择两个生成干净图像的向量。这里为了简便，我们将 `BATCH_SIZE=10`。我们还将添加一个 `if` 条件语句的开始部分，以便轻松选择要运行的代码部分：
- en: '[PRE29]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The generated images may look like the following. And the latent vectors for
    these images are exported to a file (for example, `vec_20190317-223131.txt`):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像可能如下所示。这些图像的潜在向量将导出到一个文件（例如，`vec_20190317-223131.txt`）：
- en: '![](img/dd07d801-4807-49ee-868b-e5d3b3d01a51.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd07d801-4807-49ee-868b-e5d3b3d01a51.png)'
- en: Randomly generated images
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 随机生成的图像
- en: 'Assume that we choose the 3^(rd) and the last images for interpolation. Now,
    let''s perform linear interpolation on their latent vectors with SciPy (replace
    the previous line starting with `viz_tensor = ...` with the following lines).
    Be sure to change the filename to the one that was just generated on your system:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们选择第3张和最后一张图像进行插值。现在，让我们用SciPy对它们的潜在向量进行线性插值（用以下几行代码替换之前以 `viz_tensor = ...`
    开头的那一行）。确保将文件名改为刚刚在系统上生成的文件名：
- en: '[PRE30]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You will also need to change the `VIZ_MODE` flag from `0` to `1` for interpolation:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要将 `VIZ_MODE` 标志从 `0` 改为 `1`，以便进行插值：
- en: '[PRE31]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, run your changed source code. The corresponding generated images are as
    follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行你修改后的源代码。对应生成的图像如下：
- en: '![](img/bdb7d87f-3f8b-4aef-99a8-d4d3e5ef1b6a.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdb7d87f-3f8b-4aef-99a8-d4d3e5ef1b6a.png)'
- en: Image interpolation
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图像插值
- en: We can see that the image on the left is smoothly transformed into the one on
    the right. Therefore, we know that the interpolation of the latent vectors leads
    to the interpolation of generated images.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，左边的图像平滑地转变为右边的图像。因此，我们知道潜在向量的插值会导致生成图像的插值。
- en: Semantic vector arithmetic
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义向量算术
- en: Linear interpolation is one of the basic methods in linear algebra. We can do
    a lot more with arithmetic calculations on the latent vectors.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 线性插值是线性代数中的基本方法之一。我们可以通过对潜在向量进行算术计算，做更多的事情。
- en: Take the randomly generated images from previous steps. We notice that some
    images are of smiling women (the 1^(st), 7^(th), and 9^(th) images), some women's
    images are not smiling (the 2^(nd), 3^(rd), and 5^(th) images), and none of the
    men in the images are smiling. Man, aren't they serious! How do we put a smile
    on a man's face without regenerating a new set of random vectors?
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 取前面步骤中随机生成的图像。我们注意到有些图像是微笑的女性（第1、7、9张图像），有些女性图像并没有微笑（第2、3、5张图像），而这些图像中的男性没有一个是在微笑。天哪，他们不是很严肃吗！我们怎么能在不重新生成一组新的随机向量的情况下，让一个男人微笑呢？
- en: 'Well, imagine we can solve it with arithmetic calculations:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，想象一下我们可以通过算术计算来解决这个问题：
- en: <q>[smiling woman] - [woman] = [smile]</q>
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: <q>[微笑的女性] - [女性] = [微笑]</q>
- en: <q>[smile] + [man] = [smiling man]</q>
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: <q>[微笑] + [男性] = [微笑的男性]</q>
- en: Can we do that? Let's try it!
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做到吗？让我们试试吧！
- en: 'First, set the `VIS_MODE` flag again, this time to `2` for semantic calculations:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，再次设置`VIS_MODE`标志，这次设置为`2`进行语义计算：
- en: '[PRE32]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, continue the `if` conditional with the following code. Once again, use
    the filename that was created earlier:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，继续用以下代码执行`if`条件。再次使用之前创建的文件名：
- en: '[PRE33]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here, by performing `z1-z2`, we get a smiling vector. And `z3` gives us a man
    vector. Adding them together will give us the following results. We use the mean
    vector of 3 different latent vectors for more stable results and we add small
    random values to the arithmetic results to introduce a slight randomness:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，通过执行`z1-z2`，我们得到一个微笑向量。而`z3`则给我们一个男性向量。将它们加在一起将得到以下结果。我们使用3个不同潜在向量的均值向量来获得更稳定的结果，并将小的随机值加入算术结果中，以引入轻微的随机性：
- en: '![](img/1f894278-6379-4d9b-b801-301b8781e45d.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f894278-6379-4d9b-b801-301b8781e45d.png)'
- en: Vector arithmetic on latent vectors
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在向量的向量算术
- en: 'The vector arithmetic calculation process can be described as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 向量算术计算过程可以描述如下：
- en: '![](img/94aa789f-d6b6-4411-a4b2-4452eb8e0740.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94aa789f-d6b6-4411-a4b2-4452eb8e0740.png)'
- en: Vector arithmetic
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 向量算术
- en: Out of curiosity, we directly generate images based on *z1-z2*, which gives
    us the sample at the bottom right in the previous screenshot. We can tell it's
    a smiling face, but the rest of the face is rather unnatural. It looks like the
    face of a strange dude.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，我们直接基于*z1-z2*生成图像，这给我们带来了前面截图右下角的样本。我们可以看出它是一个微笑的面孔，但面部其他部分显得相当不自然。看起来像是一个奇怪的人的脸。
- en: Now, we have unlocked the potential of GANs on manipulating the attributes of
    the generated images. However, the results are not natural and authentic enough.
    In the next chapter, we will learn how to generate samples with the exact attributes
    we desire.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经解锁了GAN在操作生成图像属性方面的潜力。然而，结果仍然不够自然和真实。在下一章中，我们将学习如何生成具有我们想要的确切属性的样本。
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We spent a tremendous amount of time learning about Deep Convolutional GANs
    in this chapter. We dealt with the MNIST dataset as well as two huge datasets
    in the form of the CelebA and LSUN datasets. We also consumed a large number of
    computing cycles. Hopefully, you have a good grasp of DCGANs at this point.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们花费了大量时间学习深度卷积生成对抗网络（DCGAN）。我们处理了MNIST数据集，以及形式为CelebA和LSUN的大型数据集。我们还消耗了大量的计算资源。希望到现在为止，你对DCGAN已经有了较好的掌握。
- en: Next, we'll look at a **Conditional GAN** (**CGAN**) and how to add label information
    during the training process. Let's get going!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习**条件生成对抗网络**（**CGAN**），以及如何在训练过程中添加标签信息。开始吧！
- en: References and useful reading list
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献及有用的阅读清单
- en: Hui J. (2018, Jun 21). GAN — *Why it is so hard to train Generative Adversarial
    Networks!*. Retrieved from [https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Hui J. (2018年6月21日). GAN — *为什么训练生成对抗网络这么困难！* 从[https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b](https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b)获取。
