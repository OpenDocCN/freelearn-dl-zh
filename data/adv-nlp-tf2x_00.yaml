- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: 2017 was a watershed moment for **Natural Language Processing** (**NLP**), with
    Transformer-and attention-based networks coming to the fore. The past few years
    have been as transformational for NLP as AlexNet was for computer vision in 2012\.
    Tremendous advances in NLP have been made, and we are now moving from research
    labs into applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年是**自然语言处理**（**NLP**）的一个分水岭，基于Transformer和注意力机制的网络崭露头角。过去几年对于NLP的变革性影响，就像2012年AlexNet对计算机视觉的影响一样巨大。NLP取得了巨大的进展，我们现在正在从研究实验室走向应用。
- en: These advances span the domains of **Natural Language Understanding** (**NLU**),
    **Natural Language Generation** (**NLG**), and **Natural Language Interaction**
    (**NLI**). With so much research in all of these domains, it can be a daunting
    task to understand the exciting developments in NLP.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这些进展跨越了**自然语言理解**（**NLU**）、**自然语言生成**（**NLG**）和**自然语言互动**（**NLI**）等领域。鉴于这些领域中有如此多的研究，要理解NLP的激动人心的进展可能是一项艰巨的任务。
- en: This book is focused on cutting-edge applications in the fields of NLP, language
    generation, and dialog systems. It covers the concepts of pre-processing text
    using techniques such as tokenization, **parts-of-speech** (**POS**) tagging,
    and lemmatization using popular libraries such as Stanford NLP and spaCy. **Named
    Entity Recognition** (**NER**) models are built from scratch using **Bi-directional
    Long Short-Term Memory networks** (**BiLSTMs**), **Conditional Random Fields**
    (**CRFs**), and Viterbi decoding. Taking a very practical, application-focused
    perspective, the book covers key emerging areas such as generating text for use
    in sentence completion and text summarization, multi-modal networks that bridge
    images and text by generating captions for images, and managing the dialog aspects
    of chatbots. It covers one of the most important reasons behind recent advances
    of NLP – transfer learning and fine tuning. Unlabeled textual data is easily available
    but labeling this data is costly. This book covers practical techniques that can
    simplify the labeling of textual data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本书专注于NLP、语言生成和对话系统领域的前沿应用。它涵盖了使用流行库（如Stanford NLP和spaCy）对文本进行预处理的概念，如分词、**词性**（**POS**）标注和词形还原。**命名实体识别**（**NER**）模型通过构建**双向长短期记忆网络**（**BiLSTMs**）、**条件随机场**（**CRFs**）和维特比解码器从零开始实现。采用非常实用、聚焦应用的视角，本书还涵盖了生成文本以用于句子补全和文本摘要、多模态网络通过为图像生成说明文字连接图像和文本、以及管理聊天机器人对话方面的内容。本书还讲解了NLP近期进展背后的一个重要原因——迁移学习和微调。未标注的文本数据容易获取，但对这些数据进行标注是昂贵的。本书提供了简化文本数据标注的实用技巧。
- en: By the end of the book, I hope you will have advanced knowledge of the tools,
    techniques, and deep learning architectures used to solve complex NLP problems.
    The book will cover encoder-decoder networks, **Long Short-Term Memory networks**
    (**LSTMs**) and BiLSTMs, CRFs, BERT, GPT-2, GPT-3, Transformers, and other key
    technologies using TensorFlow.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本书结束时，我希望您能掌握用于解决复杂NLP问题的工具、技术和深度学习架构的高级知识。本书将涵盖编码器-解码器网络、**长短期记忆网络**（**LSTMs**）和BiLSTMs、CRFs、BERT、GPT-2、GPT-3、Transformer等关键技术，并使用TensorFlow进行讲解。
- en: 'Advanced TensorFlow techniques required for building advanced models are also
    covered:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本书还涵盖了构建高级模型所需的高级TensorFlow技术：
- en: Building custom models and layers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建自定义模型和层
- en: Building custom loss functions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建自定义损失函数
- en: Implementing learning rate annealing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现学习率退火
- en: Using `tf.data` for loading data efficiently
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`tf.data`高效加载数据
- en: Checkpointing models to enable long training times (usually several days)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型检查点以支持长时间训练（通常需要几天）
- en: This book contains working code that can be adapted to your own use cases. I
    hope that you will even be able to do novel state-of-the-art research using the
    skills you'll gain as you progress through the book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书包含可根据您自身使用场景进行调整的可工作代码。我希望通过本书的学习，您甚至能够利用所学技能进行前沿的研究。
- en: Who this book is for
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适合哪些人群
- en: 'This book assumes that the reader has some familiarity with the basics of deep
    learning and the fundamental concepts of NLP. This book focuses on advanced applications
    and building NLP systems that can solve complex tasks. All kinds of readers will
    be able to follow the content of the book, but readers who can benefit the most
    from this book include:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本书假设读者对深度学习的基础知识和自然语言处理（NLP）的基本概念有所了解。本书重点讲解高级应用和构建能够解决复杂任务的NLP系统。各种读者都能够跟随本书的内容，但从本书中受益最大的读者包括：
- en: Intermediate **Machine Learning** (**ML**) developers who are familiar with
    the basics of supervised learning and deep learning techniques
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉监督学习和深度学习技术基础的中级**机器学习**（**ML**）开发者
- en: Professionals who already use TensorFlow/Python for purposes such as data science,
    ML, research, analysis, etc., and can benefit from a more solid understanding
    of advanced NLP techniques
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经使用TensorFlow/Python进行数据科学、机器学习、研究、分析等工作的专业人士，并能从更扎实的高级NLP技术理解中获益
- en: What this book covers
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容概述
- en: '*Chapter 1*, *Essentials of NLP*, provides an overview of various topics in
    NLP such as tokenization, stemming, lemmatization, POS tagging, vectorization,
    etc. An overview of common NLP libraries like spaCy, Stanford NLP, and NLTK, with
    their key capabilities and use cases, will be provided. We will also build a simple
    classifier for spam.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*第1章*，*NLP基础知识*，概述了NLP中的各种话题，如分词、词干提取、词形还原、词性标注、向量化等。本章将介绍常见的NLP库，如spaCy、Stanford
    NLP和NLTK，并提供它们的关键功能和使用场景。我们还将构建一个简单的垃圾邮件分类器。'
- en: '*Chapter 2*, *Understanding Sentiment in Natural Language with BiLSTMs*, covers
    the NLU use case of sentiment analysis with an overview of **Recurrent Neural
    Networks** (**RNNs**), LSTMs, and BiLSTMs, which are the basic building blocks
    of modern NLP models. We will also use `tf.data` for efficient use of CPUs and
    GPUs to speed up data pipelines and model training.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*第2章*，*使用BiLSTM理解自然语言中的情感*，涵盖了情感分析的自然语言理解（NLU）用例，介绍了**循环神经网络**（**RNNs**）、LSTM和BiLSTM，它们是现代NLP模型的基本构建模块。我们还将使用`tf.data`高效利用CPU和GPU，提升数据流水线和模型训练的速度。'
- en: '*Chapter 3*, *Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi
    Decoding*, focuses on the key NLU problem of NER, which is a basic building block
    of task-oriented chatbots. We will build a custom layer for CRFs for improving
    the accuracy of NER and the Viterbi decoding scheme, which is often applied to
    a deep model to improve the quality of the output.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*第3章*，*使用BiLSTM、CRF和维特比解码的命名实体识别（NER）*，聚焦于命名实体识别（NER）这一自然语言理解（NLU）的核心问题，NER是任务导向型聊天机器人的基本构建模块。我们将为CRF构建一个自定义层，以提高NER的准确性，并介绍维特比解码方案，它通常应用于深度模型中，以提高输出质量。'
- en: '*Chapter 4*, *Transfer Learning with BERT*, covers a number of important concepts
    in modern deep NLP such as types of transfer learning, pre-trained embeddings,
    an overview of Transformers, and BERT and its application in improving the sentiment
    analysis task introduced in *Chapter 2*, *Understanding Sentiment in Natural Language
    with BiLSTMs*.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*第4章*，*使用BERT进行迁移学习*，涵盖了现代深度NLP中的若干重要概念，如迁移学习的类型、预训练词向量、Transformer的概述，以及BERT及其在改进*第2章*中情感分析任务中的应用，*使用BiLSTM理解自然语言中的情感*。'
- en: '*Chapter 5*, *Generating Text with RNNs and GPT-2*, focuses on generating text
    with a custom character-based RNN and improving it with Beam Search. We will also
    cover the GPT-2 architecture and touch upon GPT-3.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*第5章*，*使用RNN和GPT-2生成文本*，专注于使用自定义的基于字符的RNN生成文本，并通过Beam Search进行改进。我们还将介绍GPT-2架构，并简要讨论GPT-3。'
- en: '*Chapter 6*, *Text Summarization with Seq2seq Attention and Transformer Networks*,
    takes on the challenging task of abstractive text summarization. BERT and GPT
    are two halves of the full encoder-decoder model. We put them together to build
    a seq2seq model for summarizing news articles by generating headlines for them.
    How ROUGE metrics are used for the evaluation of summarization is also covered.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*第6章*，*使用Seq2seq注意力机制和Transformer网络进行文本摘要*，挑战性地探讨了抽象文本摘要任务。BERT和GPT是完整编码器-解码器模型的两部分。我们将它们结合在一起，构建一个用于生成新闻文章标题的seq2seq模型。还会介绍如何使用ROUGE指标评估摘要效果。'
- en: '*Chapter 7*, *Multi-Modal Networks and Image Captioning with ResNets and Transformers*,
    combines computer vision and NLP together to see if a picture is indeed worth
    a thousand words! We will build a custom Transformer model from scratch and train
    it to generate captions for images.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*第7章*，*使用ResNet和Transformer进行多模态网络和图像描述*，将计算机视觉和NLP结合在一起，看看一张图片是否真能表达千言万语！我们将从零开始构建一个自定义的Transformer模型，并训练它生成图像的描述。'
- en: '*Chapter 8*, *Weakly Supervised Learning for Classification with Snorkel*,
    focuses on a key problem – labeling data. While NLP has a lot of unlabeled data,
    labeling it is quite an expensive task. This chapter introduces the `snorkel`
    library and shows how massive amounts of data can be quickly labeled.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*第8章*，*使用Snorkel进行分类的弱监督学习*，聚焦于一个关键问题——数据标注。虽然NLP领域有大量未标注的数据，但标注数据是一项非常昂贵的任务。本章介绍了`snorkel`库，并展示如何快速标注大量数据。'
- en: '*Chapter 9*, *Building Conversational AI Applications with Deep Learning*,
    combines the various techniques covered throughout the book to show how different
    types of chatbots, such as question-answering or slot-filling bots, can be built.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 9 章*，*使用深度学习构建对话式 AI 应用程序*，结合了书中讲解的各种技术，展示了如何构建不同类型的聊天机器人，如问答机器人或槽位填充机器人。'
- en: '*Chapter 10*, *Installation and Setup Instructions for Code*, walks through
    all the instructions required to install and configure a system for running the
    code supplied with the book.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 10 章*，*代码安装和配置说明*，介绍了安装和配置系统以运行本书随附代码的所有必要步骤。'
- en: To get the most out of this book
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为了最大限度地利用本书
- en: It would be a good idea to get a background on the basics of deep learning models
    and TensorFlow.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解深度学习模型和 TensorFlow 的基础知识是个不错的主意。
- en: The use of a GPU is highly recommended. Some of the models, especially in the
    later chapters, are pretty big and complex. They may take hours or days to fully
    train on CPUs. RNNs are very slow to train without the use of GPUs. You can get
    access to free GPUs on Google Colab, and instructions for doing so are provided
    in the first chapter.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强烈建议使用 GPU。一些模型，尤其是后续章节中的模型，规模较大且复杂。它们在 CPU 上训练可能需要数小时甚至数天。没有 GPU 的情况下，RNN 的训练速度非常慢。你可以在
    Google Colab 上免费访问 GPU，相关操作说明在第一章中提供。
- en: Download the example code files
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: The code bundle for the book is hosted on GitHub at [https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-TensorFlow-2](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-TensorFlow-2).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的代码包托管在 GitHub 上，网址为 [https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-TensorFlow-2](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-TensorFlow-2)。我们还提供了来自我们丰富图书和视频目录的其他代码包，网址为
    [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)。请查看！
- en: Download the color images
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载彩色图片
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781800200937_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781800200937_ColorImages.pdf).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一个 PDF 文件，包含本书中使用的截图/图表的彩色图片。你可以在这里下载：[https://static.packt-cdn.com/downloads/9781800200937_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781800200937_ColorImages.pdf)。
- en: Conventions used
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用的约定
- en: There are a number of text conventions used throughout this book.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了许多文本约定。
- en: '`CodeInText`: Indicates code words in text, database table names, folder names,
    filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles.
    For example: "In the `num_capitals()` function, substitutions are performed for
    the capital letters in English."'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`CodeInText`：表示文本中的代码词、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟网址、用户输入和 Twitter 用户名。例如：“在
    `num_capitals()` 函数中，执行了对英语中大写字母的替换。”'
- en: 'A block of code is set as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块如下所示：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望引起你对代码块中特定部分的注意时，相关的行或项将以粗体显示：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出都将如下所示：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Bold**: Indicates a new term, an important word, or words that you see on
    the screen, for example, in menus or dialog boxes, also appear in the text like
    this. For example: "Select **System info** from the **Administration** panel."'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示一个新术语、一个重要的词或你在屏幕上看到的词，例如在菜单或对话框中，也会以这种方式出现在文本中。例如：“从 **管理** 面板中选择
    **系统信息**。”'
- en: Warnings or important notes appear like this.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 警告或重要说明将以这种形式出现。
- en: Tips and tricks appear like this.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 提示和技巧将以这种形式出现。
- en: Get in touch
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我们
- en: Feedback from our readers is always welcome.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终欢迎读者的反馈。
- en: '**General feedback**: If you have questions about any aspect of this book,
    mention the book title in the subject of your message and email Packt at `customercare@packtpub.com`.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**一般反馈**：如果你对本书的任何方面有疑问，请在邮件主题中提及书名，并通过电子邮件联系 Packt，邮箱地址是 `customercare@packtpub.com`。'
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you could report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata),
    select your book, click on the **Errata Submission Form** link, and enter the
    details.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误表**：尽管我们已尽一切努力确保内容准确无误，但错误偶尔也会发生。如果你在本书中发现了错误，我们将不胜感激您向我们报告。请访问 [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata)，选择您的书籍，点击
    **Errata Submission Form** 链接并输入详细信息。'
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the Internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packtpub.com` with a
    link to the material.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**盗版**：如果你在任何形式的互联网上发现我们作品的非法副本，我们将不胜感激您提供位置地址或网站名称。请通过链接`copyright@packtpub.com`与我们联系。'
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [http://authors.packtpub.com](http://authors.packtpub.com).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你对成为一名作者感兴趣**：如果你有专业知识并且有意编写或贡献书籍，请访问 [http://authors.packtpub.com](http://authors.packtpub.com)。'
- en: Reviews
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评论
- en: Please leave a review. Once you have read and used this book, why not leave
    a review on the site that you purchased it from? Potential readers can then see
    and use your unbiased opinion to make purchase decisions, we at Packt can understand
    what you think about our products, and our authors can see your feedback on their
    book. Thank you!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请留下你的评论。阅读并使用本书后，请考虑在购买书籍的网站上留下您的评论。潜在读者可以看到并使用你的客观意见来做出购买决定，我们在 Packt 可以了解到你对我们产品的看法，而我们的作者也能看到你对他们书籍的反馈。谢谢！
- en: For more information about Packt, please visit [packtpub.com](http://packtpub.com).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多有关 Packt 的信息，请访问 [packtpub.com](http://packtpub.com)。
