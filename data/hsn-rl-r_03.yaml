- en: Building Blocks of Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的构建模块
- en: The main objective of algorithms based on reinforcement learning is to learn
    and adapt to environmental changes. To do this, we use external feedback signals
    (reward signals) generated by the environment according to the choices made by
    the algorithm. In this context, the right choice that's suggested by the algorithm
    will provide a reward while a wrong choice will result in a penalty. All of this
    is done in order to achieve the best result possible. In this chapter, you will
    discover agent-environment interface concepts for building models. By the end
    of this chapter, you will be ready to dive into working on the Markov decision
    process. We will also discover the fundamental concepts of the policy and how
    to improve the results by applying a policy gradient.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 基于强化学习的算法的主要目标是学习并适应环境的变化。为了实现这一目标，我们利用环境根据算法做出的选择生成的外部反馈信号（奖励信号）。在这种情况下，算法推荐的正确选择将会获得奖励，而错误的选择将会受到惩罚。所有这些操作都是为了实现最佳的结果。在本章中，你将会了解构建模型的智能体与环境接口的概念。在本章结束时，你将准备好深入研究马尔可夫决策过程。我们还将探索策略的基本概念，并通过应用策略梯度来提升结果。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Agent-environment interface
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体与环境的接口
- en: Understanding the Markov decision process
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解马尔可夫决策过程
- en: Explaining the policy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释策略
- en: Exploring policy gradient methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索策略梯度方法
- en: Agent-environment interface
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能体与环境接口
- en: In reinforcement learning, the agent learns and adapts to environmental changes.
    The basis of this programming technique arises from the concept of receiving external
    stimuli according to the choices of the algorithm. A correct choice returns a
    reward while an incorrect choice returns a penalty. The objective of the system
    is to achieve the best possible result.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，智能体学习并适应环境的变化。这种编程技术的基础源于根据算法选择接收外部刺激的概念。正确的选择会带来奖励，而错误的选择会带来惩罚。系统的目标是实现最佳的结果。
- en: These mechanisms derive from the basic concepts of machine learning (learning
    from experience) in an attempt to simulate human reasoning. In fact, in our mind,
    we activate brain mechanisms that lead us to chase and repeat what produces feelings
    of gratification and well-being. Whenever we experience moments of pleasure (food,
    music, art, and so on), our brain produces some substances that work by reinforcing
    that same stimulus, emphasizing it. Along with this mechanism of neurochemical
    reinforcement, our memory remembers this experience so that we can recreate how
    we feel in the future. Evolution has provided us with this mechanism so that we
    can repeat experiences that are rewarding to us.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机制源自机器学习的基本概念（从经验中学习），旨在模拟人类推理。事实上，在我们的脑海中，我们激活了大脑机制，促使我们追逐并重复那些带来满足感和幸福感的体验。每当我们经历愉悦的时刻（如食物、音乐、艺术等），我们的脑部会分泌一些物质，这些物质通过强化同样的刺激来加深这种体验。伴随着这种神经化学强化机制，我们的记忆会记住这一经历，以便将来能够重新体验这些感觉。进化为我们提供了这种机制，使我们能够重复对我们有益的经历。
- en: This is why we remember the important experiences of our lives, especially those
    that are powerfully rewarding, are part of our memories, and condition our future
    explorations. Learning from experience can be simulated by a numerical algorithm
    in various ways, depending on the nature of the signal that's used for learning
    or the type of feedback that's returned by the system.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么我们记得生活中重要的经历，特别是那些带来巨大奖励的经历，它们成为我们记忆的一部分，并影响我们未来的探索。根据所使用的学习信号的性质或系统返回的反馈类型，学习经验可以通过数值算法以不同方式进行模拟。
- en: 'In supervised learning, there is a teacher who tells the system what the correct
    output is. In reality, it isn''t always possible to have a tutor who guides us
    in our choices. Often, we only have qualitative information that gives us feedback
    on how the environment responds to our actions. This information is called reinforcement
    signals. The system doesn''t give us any information about how to update the agent''s
    behavior (that is, the weights). You cannot define a cost function or a gradient.
    The goal of the system is to create smart agents that are able to learn from their
    experience. In the following diagram, we can see a flowchart that displays how
    reinforcement learning interacts with the environment:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，系统有一个教师，告诉它正确的输出是什么。实际上，并不总是有一个导师引导我们做出选择。通常，我们只能获得定性的反馈信息，了解环境如何回应我们的行为。这些信息被称为强化信号。系统不会提供关于如何更新智能体行为（即权重）的任何信息。你不能定义代价函数或梯度。系统的目标是创建能够从经验中学习的智能代理。在下图中，我们可以看到展示强化学习如何与环境交互的流程图：
- en: '![](img/b2b34caf-debe-423d-ad95-4a606f973573.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2b34caf-debe-423d-ad95-4a606f973573.png)'
- en: 'Scientific literature has taken an uncertain stance on the classification of
    learning by reinforcement as a paradigm. In fact, in its initial phase, it was
    considered as a special case of supervised learning before it was fully promoted
    as the third paradigm of machine learning algorithms. It is applied in different
    contexts in which supervised learning is inefficient: the problems of interacting
    with the environment is a clear example of this.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 科学文献对于强化学习是否作为一种范式的分类持不确定态度。事实上，在其初期阶段，它被视为监督学习的一个特例，直到完全被推广为机器学习算法的第三大范式。它应用于监督学习效率低下的不同场景：与环境交互的问题就是一个明显的例子。
- en: 'We need to follow these steps to correctly apply a reinforcement learning algorithm:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要遵循以下步骤来正确应用强化学习算法：
- en: Prepare the agent.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备智能体。
- en: Observe the environment.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察环境。
- en: Select the optimal strategy.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最佳策略。
- en: Execute the actions.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作。
- en: Calculate the corresponding reward (or penalty).
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算相应的奖励（或惩罚）。
- en: Develop updated strategies (if necessary).
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发更新的策略（如果需要）。
- en: Repeat *steps 2 - 5* iteratively until the agent learns the optimal strategies.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反复执行*步骤2 - 5*，直到智能体学会最佳策略。
- en: Reinforcement learning is based on the psychological theory that came about
    due to a series of experiments that were performed on animals. In particular,
    Edward Thorndike (an American psychologist) noted that if a cat is given a reward
    immediately after it behaves in a way that's considered correct, it increases
    the probability that the cat will repeat this behavior. In the face of unwanted
    behavior, however, applying punishment decreases the probability that the cat
    will repeat the behavior.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习基于一个心理学理论，该理论源自一系列对动物进行的实验。特别是，美国心理学家爱德华·桑代克（Edward Thorndike）指出，如果一只猫在表现出被认为正确的行为后立即获得奖励，那么猫重复这一行为的概率会增加。然而，面对不想要的行为时，施加惩罚则会降低猫重复该行为的概率。
- en: On the basis of this theory, reinforcement learning tries to maximize the rewards
    that are received when an action or set of actions are executed so that a certain
    goal can be reached.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一理论，强化学习试图最大化执行某个行为或一组行为时获得的奖励，以便实现某个特定目标。
- en: Reinforcement learning can be seen as a special case of the interaction problem
    for achieving a goal. The entity that must reach the goal is called an agent.
    The entity that the agent must interact with is called the environment, which
    corresponds to everything that is external to the agent.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习可以看作是实现目标的交互问题的特例。必须实现目标的实体被称为智能体。智能体必须与之交互的实体被称为环境，它对应着智能体外部的一切。
- en: So far, we have focused on the term agent, but what does it represent? The agent
    (software) is a software entity that performs services on behalf of another program,
    usually automatically and invisibly. These pieces of software are also called
    smart agents.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直关注术语“智能体”，但它代表什么呢？智能体（软件）是一个代表其他程序执行服务的软件实体，通常是自动和隐形的。这些软件也被称为智能代理。
- en: 'The following are the most important features of an agent:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是智能体最重要的特征：
- en: It can choose an action to perform on the environment that's either continuous
    or discrete.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以选择对环境执行的动作，动作可以是连续的或离散的。
- en: The action that's performed depends on the situation. The situation is summarized
    in the system state.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行的动作取决于情况。情况在系统状态中得到总结。
- en: The agent continuously monitors the environment (input) and continuously changes
    the status.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理持续监控环境（输入），并不断改变状态。
- en: The choice of the action isn't trivial and requires a certain degree of "intelligence".
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择动作并非 trivial（微不足道），需要一定程度的“智能”。
- en: The agent has a smart memory.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理具有智能记忆。
- en: The agent has a goal-directed behavior, but how it acts in an uncertain environment
    isn't known beforehand. An agent learns by interacting with the environment. Planning
    can be developed while the agent is learning about the environment through the
    measurements it makes. This strategy is close to the trial-and-error theory.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 代理有目标导向的行为，但它在不确定环境中如何行动是事先无法知道的。代理通过与环境的互动来学习。规划可以在代理通过其测量了解环境时逐步开发。这一策略与试错理论接近。
- en: The trial and error theory is a crucial method of problem-solving. The trial
    is repeated until the agent is successful or until the agent stops trying.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 试错理论是解决问题的一个关键方法。试验会不断重复，直到代理成功或停止尝试。
- en: The agent-environment interaction is continuous since the agent chooses an action
    to be taken and, in response, the environment changes state by presenting a new
    situation that the agent will be faced with.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 代理与环境的互动是持续的，因为代理选择要采取的行动，并且环境会改变状态，呈现出代理将面临的新情况。
- en: In the case of reinforcement learning, the environment provides the agent with
    a reward. It is essential that the source of the reward is the environment to
    avoid the formation of a personal reinforcement mechanism that would compromise
    learning.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习的情况下，环境为代理提供奖励。奖励的来源必须是环境，以避免形成个人强化机制，这样会妨碍学习。
- en: The value of the reward is proportional to the influence that the action has
    in reaching the objective, so it is positive or high in the case of a correct
    action or negative or low for an incorrect action.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励的值与行动在达成目标方面的影响成正比，因此在正确的行动下奖励是正向或高的，在错误的行动下奖励是负向或低的。
- en: 'The following are some examples from real life where an agent and environment
    have interacted to solve a certain problem:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些现实生活中的例子，其中代理和环境通过互动解决了特定问题：
- en: A chess player, where each move provides information about the possible countermoves
    of the opponent can make.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个棋手，每一步都能提供关于对手可能采取反击的提示。
- en: A little giraffe that, in a few hours, can learn to get up and run at 50 km/h.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一只小长颈鹿，几小时内可以学会站起来并以每小时50公里的速度奔跑。
- en: A truly autonomous robot learns to move in a room so that it can get out of
    it.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台真正的自主机器人学会在房间里移动，从而能够走出房间。
- en: The parameters of a refinery (oil pressure, flow, and so on) are set in real-time
    so that we can obtain the maximum yield or maximum quality.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 炼油厂的参数（如油压、流量等）实时设置，以便我们获得最大产量或最高质量。
- en: 'All of these examples have the following characteristics in common:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些例子有以下共同特点：
- en: Interaction with the environment
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与环境的互动
- en: The objective of the agent
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理的目标
- en: Uncertainty or partial knowledge of the environment
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境的不确定性或部分知识
- en: 'From this, it is possible to make the following observations:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从中可以得出以下观察：
- en: The agent learns from its own experience
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理从自身经验中学习
- en: The actions change the status (the situation) and how many changes can be made
    in the future (delayed reward).
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行为会改变状态（情况）以及未来可以做出多少变化（延迟奖励）。
- en: The effect of an action cannot be completely predicted.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一项行为的效果无法完全预测。
- en: The agent has a global assessment of its behavior.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理具有对其行为的全局评估。
- en: The agent must exploit this information to improve its choices. These choices
    improve with experience.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理必须利用这些信息来改善其选择。这些选择随着经验的积累而改进。
- en: Problems can have a finite or infinite time horizon.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题可以有有限或无限的时间范围。
- en: Essentially, the agent receives sensations from the environment through its
    sensors. Depending on its feelings, the agent decides what actions to take in
    the environment. Based on the immediate result of its actions, the agent may be
    rewarded.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，代理通过其传感器从环境中接收感知。根据其感受，代理决定在环境中采取什么行动。根据其行动的即时结果，代理可能会得到奖励。
- en: If you want to use an automatic learning method, you need to give a formal description
    of the environment. It isn't important to know exactly how the environment is
    made – what's interesting is to make general assumptions about the properties
    that the environment has. In reinforcement learning, it is usually assumed that
    the environment can be described by a Markov decision process. Let's learn how.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用自动学习方法，你需要给出环境的正式描述。知道环境的具体构成并不重要——有趣的是对环境的属性做出一般性假设。在强化学习中，通常假设环境可以用马尔可夫决策过程来描述。让我们来学习一下。
- en: Understanding the Markov decision process
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解马尔可夫决策过程
- en: To avoid load problems and computational difficulties, the agent-environment
    interaction is considered as a Markov decision process. A **Markov decision process**
    is a discrete time stochastic control process.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免负载问题和计算困难，代理-环境交互被视为马尔可夫决策过程。**马尔可夫决策过程**是一个离散时间随机控制过程。
- en: '**Stochastic processes** are mathematical models that are used to study the
    evolution of phenomena following random or probabilistic laws. In all-natural
    phenomena, it is known that, by their very nature and by the observation errors,
    a random or accidental component is present. This component states that at every
    instant, *t*, the result of observing the phenomenon is a random number or random
    variables, *st*: it isn''t possible to predict with certainty what the result
    will be; we can only state that it will take one of several possible values, each
    of which has a given probability.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机过程**是用于研究遵循随机或概率法则的现象演化的数学模型。在所有自然现象中，已知由于其本质以及观察误差，存在随机或偶然成分。这个成分表明，在每一时刻
    *t*，观察现象的结果是一个随机数或随机变量 *st*：无法确定地预测结果是什么；我们只能说它将取多个可能值中的一个，每个值都有一个给定的概率。'
- en: A stochastic process is deemed Markovian when a certain instant, *t*, of the
    observation is chosen, the evolution of the process, starting with *t*, depends
    only on *t* while it doesn't depend on the previous instants in any way. Thus,
    a process is Markov when, given the moment of observation, the instant determines
    the future evolution of the process, while this evolution doesn't depend on the
    past.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当选择某一时刻 *t* 的观察时，若一个随机过程被认为是马尔可夫过程，则从 *t* 开始，过程的演变仅依赖于 *t*，而与之前的时刻无关。因此，马尔可夫过程指的是在给定观察时刻的情况下，时刻决定了过程的未来演变，而这一演变不依赖于过去。
- en: In a Markov process, at each time step, the process is in some state, *s ∈ S*,
    and the agent may choose any action, *a ∈ A*, that is available in state *s*.
    The process responds at the next time step by randomly moving into a new state,
    *s'*, and giving the agent a corresponding reward *r(s,s')*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在马尔可夫过程中，每个时间步，过程处于某一状态 *s ∈ S*，代理可以选择状态 *s* 中可用的任何动作 *a ∈ A*。在下一时间步，过程会通过随机移动到新状态
    *s'*，并给予代理一个相应的奖励 *r(s,s')*。
- en: 'In the following diagram, we can see the agent-environment interaction in a
    Markov decision process:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图示中，我们可以看到马尔可夫决策过程中代理与环境的交互：
- en: '![](img/86d11815-26cd-481c-a7b4-5808afc8b778.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86d11815-26cd-481c-a7b4-5808afc8b778.png)'
- en: 'The agent-environment interaction shown in the preceding diagram can be summarized
    as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 前面图示中的代理-环境交互可以总结如下：
- en: The agent and the environment interact at discrete intervals over time, t =
    0, 1, 2… n.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理与环境在离散的时间间隔上进行交互，t = 0, 1, 2… n。
- en: At each interval, the agent receives a representation of the state *st* of the
    environment.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个间隔时，代理接收环境状态的表示 *st*。
- en: Each element *st S*, where *S* is the set of possible states.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个元素 *st S*，其中 *S* 是可能状态的集合。
- en: Once the state is recognized, the agent must take an action at *A(st)*, where
    *A(st)* is the set of possible actions in the state, *st*.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦状态被识别，代理必须在 *A(st)* 中采取一个动作，其中 *A(st)* 是状态 *st* 中可能的动作集合。
- en: The choice of the action to be taken depends on the objective to be achieved
    and is mapped through the policy indicated with the symbol *π* (discounted cumulative
    reward), which associates the action with *A(s)* for each state, *s*. The term
    *πt(s,a)* represents the probability that action *a* is carried out in the state,
    *s*.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采取行动的选择依赖于要实现的目标，并通过策略 *π*（折扣累积奖励）进行映射，该策略将每个状态 *s* 中的动作与 *A(s)* 关联。术语 *πt(s,a)*
    表示在状态 *s* 中执行动作 *a* 的概率。
- en: During the next time interval, *t + 1*, as part of the consequence of the action,
    the agent receives a numerical reward, *rt + 1 R*, corresponding to the action
    that was taken previously.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在接下来的时间间隔 *t + 1* 中，作为行动的结果，代理会收到一个数值奖励，*rt + 1 R*，对应于之前采取的行动。
- en: Now, the consequence of the action represents the new state, *st*. At this point,
    the agent must code the state and make a choice in terms of the action that will
    take place.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，行动的结果代表新的状态 *st*。此时，代理必须对状态进行编码，并根据将要采取的行动做出选择。
- en: This iteration repeats itself so that the objective is achieved by the agent.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个迭代会不断重复，以便代理达到目标。
- en: 'The definition of the status *st + 1* depends on the previous state and the
    action taken (MDP), as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 *st + 1* 的定义取决于前一个状态和所采取的行动（MDP），如下所示：
- en: '![](img/86ddc6a6-2400-4f4b-a2ae-8365595f33de.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86ddc6a6-2400-4f4b-a2ae-8365595f33de.png)'
- en: In the formula, δ represents the status function.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式中，δ 代表状态函数。
- en: 'In summary, we can state the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们可以陈述以下内容：
- en: In a Markov decision process, the agent can perceive the status, *s S* that
    they're in and has a set of actions at their disposal.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在马尔可夫决策过程中，代理可以感知它所处的状态 *s S*，并且有一组可用的行动。
- en: At each discrete interval *t* of time, the agent detects the current status
    *st* and decides to implement an action at *A*.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个离散时间间隔 *t* 中，代理会检测当前状态 *st* 并决定执行某个动作 *A*。
- en: The environment responds by providing a reward (a reinforcement) *rt = r (st,
    at)* and moving into the state *st + 1 = δ (st, at)*.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境通过提供奖励（强化）*rt = r (st, at)* 并进入状态 *st + 1 = δ (st, at)* 来做出回应。
- en: The r and δ functions are part of the environment; they only depend on the current
    state and action (not the previous ones) and are not necessarily known to the
    agent.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: r 和 δ 函数是环境的一部分；它们仅依赖于当前的状态和行动（而非之前的状态和行动），且不一定为代理所知。
- en: The goal of reinforcement learning is to learn a policy that, for each state,
    *s,* in which the system is located, specifies an action to the agent so that
    it can maximize the total reinforcement it receives during the entire action sequence.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的目标是学习一个策略，对于系统所处的每个状态 *s*，为代理指定一个行动，以便它能够最大化在整个行动序列中获得的总强化。
- en: 'Let''s talk about some of the terms we used in more detail:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论一下我们使用的一些术语：
- en: A reward function defines the goal in a reinforcement learning problem. It maps
    the detected states of the environment into a single number, thus defining a reward.
    As we mentioned previously, the only goal is to maximize the total reward it receives
    in the long term. The reward function decides which actions are positive and negative
    for the agent. The reward function has the need to be correct, and it can be used
    as a basis for changing the policy. If an action that's suggested by the policy
    returns a low reward, in the next step, the policy can be modified to suggest
    other actions in the same situation.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励函数定义了强化学习问题中的目标。它将环境检测到的状态映射为一个数值，从而定义了奖励。正如我们之前提到的，唯一的目标是最大化它在长期内获得的总奖励。奖励函数决定了哪些行动对代理是正面的，哪些是负面的。奖励函数需要是正确的，并且可以作为改变策略的基础。如果策略建议的行动返回较低的奖励，那么在下一步中，可以修改策略以在相同情况下建议其他行动。
- en: A policy defines the behavior of the learning agent at a given time. It maps
    both the detected states of the environment and the actions to take when they
    are in those states. This corresponds to what would be called a set of rules or
    associations of stimulus response in psychology. The policy is the fundamental
    part of a reinforcing learning agent in the sense that it alone is enough to determine
    behavior.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略定义了学习代理在给定时间的行为。它将检测到的环境状态和在这些状态下应采取的行动进行映射。这对应于心理学中所谓的刺激反应规则或关联。策略是强化学习代理的核心部分，因为它足以决定行为。
- en: A value function represents how good a state is for an agent. It is equal to
    the total reward that's expected for an agent from the status, *s*. The value
    function depends on the policy that the agent selects for the actions to be performed
    on.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值函数表示某个状态对于代理的价值。它等于从状态 *s* 开始，代理预期获得的总奖励。值函数依赖于代理为所要执行的动作选择的策略。
- en: An action-value function returns the value, that is, the expected return (overall
    reward) for using action, *a*, in a certain state, *s*, following a policy.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作值函数返回一个值，即在某一状态 *s* 下，按照策略执行动作 *a* 后的预期回报（总体奖励）。
- en: In the next section, we will learn how to maximize the total reinforcement that's
    received during the learning process.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何最大化在学习过程中获得的总强化。
- en: Discounted cumulative reward
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 折扣累积奖励
- en: In the *Markov decision process* section, we mentioned that the goal of reinforcement
    learning is to learn a policy that, for each state, *s*, in which the system is
    located, specifies an action to the agent so that it can maximize the total reinforcement
    they receive during the entire action sequence. How can we maximize the total
    reinforcement that's received during the entire sequence of actions?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在*马尔可夫决策过程*部分，我们提到过，强化学习的目标是学习一个策略，对于每个状态*s*，在该状态下，指定一个动作给代理，以便它能够最大化在整个动作序列中获得的总强化。那么，我们如何在整个动作序列中最大化获得的总强化呢？
- en: 'The total reinforcement that''s derived from the policy is calculated as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从策略中获得的总强化计算如下：
- en: '![](img/ce3c5ab6-1900-46e3-aa37-fcbac40d0e9c.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce3c5ab6-1900-46e3-aa37-fcbac40d0e9c.png)'
- en: Here, r[T] represents the reward of the action that drives the environment in
    the terminal state, s[T].
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，r[T]表示驱动环境进入终止状态s[T]的动作的奖励。
- en: A possible solution to this problem is to associate the action that provides
    the highest reward to each individual state; that is, we must determine an optimal
    policy so that the previous quantity is maximized.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一个可能方法是将提供最高奖励的动作与每个状态关联；也就是说，我们必须确定一个最优策略，以便最大化前述量。
- en: For problems that don't reach the goal or terminal state in a finite number
    of steps (continuing tasks), Rt continues to infinity.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在有限步数内无法到达目标或终止状态的问题（持续任务），Rt会趋向于无限大。
- en: In these cases, the sum of the rewards that we want to maximize diverges at
    the infinite, so this approach is not applicable. Due to this, it is necessary
    to develop an alternative reinforcement technique.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，我们希望最大化的奖励和会在无穷大处发散，因此这种方法不可行。由于这个原因，有必要开发一种替代的强化技术。
- en: 'The technique that best suits the reinforcement learning paradigm turns out
    to be the discounted cumulative reward, which tries to maximize the following
    quantity:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最适合强化学习范式的技巧是折扣累积奖励，它试图最大化以下量：
- en: '![](img/a4f284ea-0813-4784-815b-48af5f19b057.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4f284ea-0813-4784-815b-48af5f19b057.png)'
- en: 'Here, γ is called a discount factor and represents the importance of future
    rewards. This parameter can take the values 0 ≤  γ ≤ 1, which have the following
    meanings:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，γ被称为折扣因子，代表未来奖励的重要性。该参数可以取值0 ≤ γ ≤ 1，具有以下含义：
- en: If γ <1, the sequence, *rt*, will converge to a finite value.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果γ < 1，序列*rt*将收敛到一个有限值。
- en: If γ = 0, the agent will have no interest in future rewards but will try to
    maximize the reward for the current state.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果γ = 0，代理将不关心未来的奖励，而是尝试最大化当前状态的奖励。
- en: If γ = 1, the agent will try to increase future rewards, even at the expense
    of the immediate ones.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果γ = 1，代理将尽力增加未来奖励，即使这会以当前的奖励为代价。
- en: The discount factor can be modified during the learning process to highlight
    particular actions or states. An optimal policy can lead to the reinforcement
    that's obtained when performing a single action to be low (or even negative),
    provided that this leads to greater reinforcement. Exploring the environment is
    the right approach when you want to gather useful information. However, in some
    cases, it is also a computationally expensive process, so let's look at how to
    deal with this problem.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 折扣因子可以在学习过程中进行修改，以突出特定的动作或状态。一个最优策略可以导致执行单一动作时获得的强化较低（甚至为负），前提是这会导致更大的强化。当你希望收集有用信息时，探索环境是一种正确的方式。然而，在某些情况下，这也是一个计算开销较大的过程，所以我们来看一下如何处理这个问题。
- en: Exploration versus exploitation
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索与利用
- en: Ideally, the agent must associate with each action at the respective reward,
    *r*, in order to choose the most rewarded behavior for achieving the goal. This
    approach is impractical for complex problems in which the number of states is
    particularly high and the possible associations increase exponentially.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，代理必须将每个动作与相应的奖励*r*相关联，以选择最有奖励的行为来实现目标。对于复杂问题，这种方法是不实际的，因为状态数量特别高，且可能的关联以指数级增长。
- en: This problem is called the **exploration-exploitation** dilemma. Ideally, the
    agent must explore all the possible actions for each state and find the one that
    is rewarded the most when it's exploited.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题被称为**探索-开发**困境。理想情况下，代理必须探索每个状态下所有可能的行为，并找到当其被开发时奖励最多的行为。
- en: 'Thus, decision-making involves a fundamental choice:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，决策涉及一个根本性的选择：
- en: '**Exploitation**: This makes the best decision given the current information.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发**：根据当前信息做出最佳决策。'
- en: '**Exploration**: This collects more information.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索**：这会收集更多信息。'
- en: In this process, the best long-term strategy can lead to considerable sacrifices
    in the short term. Therefore, it is necessary to gather enough information to
    make the best decisions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，最好的长期策略可能需要在短期内做出相当大的牺牲。因此，有必要收集足够的信息来做出最佳决策。
- en: 'The exploration-exploitation dilemma has something to offer whenever we try
    to learn something new. Often, we have to decide whether to choose what we already
    know (exploitation), thus leaving our cultural baggage unaltered, or choose something
    new and learn in this way instead (exploration). The second choice risks us making
    the wrong choices. This is an experience that we have often faced; think, for
    example, about the choices we make in a restaurant when we are asked to choose
    between the dishes on the menu:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 探索-开发困境在我们尝试学习新事物时总会有所启示。通常，我们必须决定是选择我们已经知道的东西（开发），从而让我们的文化包袱保持不变，还是选择新的东西，转而通过这种方式来学习（探索）。第二种选择有可能使我们做出错误的决策。这是我们经常面临的一个经历；例如，想一想我们在餐厅选择菜单时所做的决策：
- en: We can choose something that we already know about and that, in the past, has
    given us back a known reward with gratification (exploitation), such as pizza
    (who doesn't know the goodness of a Margherita pizza?).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以选择已经了解的东西，过去曾给我们带来已知的满足感和奖励（开发），比如比萨饼（谁不知道玛格丽塔比萨的美味呢？）。
- en: We can try something new that we have never tasted before and see what we get
    (exploration), such as lasagne (alas, not everyone knows the magical taste of
    a lasagne bowl).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以尝试一些以前从未品尝过的新东西，看看会有什么收获（探索），比如千层面（唉，并不是每个人都知道千层面碗中的神奇味道）。
- en: 'The choice we make will depend on many boundary conditions: the price of the
    dishes, our level of hunger, our knowledge of dishes, and so on. What''s important
    is that studying the best way to make this kind of choice has demonstrated that
    optimal learning requires that we sometimes make bad choices. This means that,
    sometimes, you have to choose to avoid the action you deem the most rewarding
    and take an action that you feel is less rewarding. The logic is that these actions
    are necessary to obtain a long-term benefit: sometimes, you need to get your hands
    dirty to learn more.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做出的选择将取决于许多边界条件：菜肴的价格、我们的饥饿程度、我们对菜肴的了解等等。重要的是，研究如何做出这种选择的最佳方法表明，最佳学习有时需要我们做出错误的选择。这意味着，有时你必须选择避免你认为最有回报的行动，而选择一个你认为回报较少的行动。其逻辑是，这些行动对于获得长期利益是必要的：有时候，你需要弄脏你的双手才能学到更多。
- en: 'Here are some more examples of adopting this technique in real-life cases:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些在实际案例中采用此技术的更多示例：
- en: 'Selecting a store:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择商店：
- en: '**Exploitation**: Go to your favorite store'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发**：去你最喜欢的商店'
- en: '**Exploration**: Try a new store'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索**：尝试新商店'
- en: 'Choosing a route:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择路线：
- en: '**Exploitation**: Choose the best route you know of'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发**：选择你已知的最佳路线'
- en: '**Exploration**: Try a new route'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索**：尝试一条新路线'
- en: 'In practice, in very complex problems, converging a very good strategy would
    be too slow. A good solution to this problem is to find a balance between exploration
    and exploitation:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，对于非常复杂的问题，收敛到一个非常好的策略可能太慢。解决这个问题的一个好方法是找到探索和开发之间的平衡：
- en: An agent who limits itself to exploring will always act in a casual way in every
    state and it is evident that converging to an optimal strategy is impossible.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个仅限于探索的代理将在每个状态下始终采取随意的行动，显然收敛到一个最优策略是不可能的。
- en: If an agent explores a little, it will always use the actions it would always
    use, which may not be optimal.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个代理仅仅进行少量探索，它将总是使用它通常会使用的行为，这可能不是最优的。
- en: At every step, the agent has to choose between repeating what they have done
    so far or trying out new movements that could achieve better results.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一步，代理都必须在重复迄今为止的做法和尝试新的方法之间做出选择，后者可能会获得更好的结果。
- en: Policies are essential when it comes to choosing an action to perform. In the
    next section, we will look at this further by analyzing the different approaches
    that can be used while searching for the best possible policy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择要执行的动作时，策略至关重要。在下一节中，我们将通过分析在寻找最佳策略时可以使用的不同方法来进一步探讨这一点。
- en: Explaining the policy
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释策略
- en: As we mentioned in the *Markov decision process* section, a policy defines the
    behavior of the learning agent at a given time. It maps both the detected states
    of the environment and the actions to take when they are in those states. The
    policy is the fundamental part of a reinforcing learning agent in the sense that
    it alone is enough to determine behavior. The policy is essential in the choices
    that the agent is required to make. In fact, once the observation has been obtained,
    the decision on what to do next is made on the basis of the policy. In this case,
    we don't need the value of the state or a particular action – we simply need the
    policy that considers the total reward.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*马尔可夫决策过程*部分提到的，策略定义了学习代理在某一时刻的行为。它将环境中检测到的状态与在这些状态下采取的行动对应起来。策略是强化学习代理的核心部分，因为仅凭策略就足以决定行为。策略在代理需要做出选择时至关重要。事实上，一旦获得观察结果，接下来的决策就是基于策略来做出的。在这种情况下，我们不需要状态的值或特定动作的值——我们只需要考虑能够实现总奖励的策略。
- en: Policies can be deterministic when the same action is taken for a given state,
    or probabilistic when the action is chosen based on some distribution calculation
    between the shares and the given state.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 策略可以是确定性的，即对于给定状态采取相同的动作；也可以是概率性的，即基于某些分布计算在状态之间选择的动作。
- en: 'To fully understand the meaning of the policy, let''s look at an example. Suppose
    we need to implement an algorithm to drive a delivery vehicle from the shop to
    the customer''s home. We can define the following elements:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解策略的含义，我们来看一个例子。假设我们需要实现一个算法，让送货车辆从商店开到顾客家里。我们可以定义以下元素：
- en: The road map is the environment.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 道路地图是环境。
- en: The current position of the vehicle is a state.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车辆的当前位置是一个状态。
- en: The policy is what the agent does to accomplish this task.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略就是代理为完成任务所采取的行动。
- en: 'Now, we provide some examples of policies that our vehicle could adopt in order
    to complete a task, such as the delivery of goods:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们提供一些我们的车辆可以采用的策略示例，以完成任务，比如送货：
- en: '**Policy 1**: Uncontrolled vehicles would move randomly until they accidentally
    end up in the right place (the customer''s home). Here, it is possible that a
    lot of fuel will be consumed and that the delivery process will last a long time.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略 1**：不受控制的车辆将随机移动，直到它们偶然到达正确的地方（顾客家）。在这里，可能会消耗大量燃料，且送货过程会持续很长时间。'
- en: '**Policy 2**: Other vehicles could learn to travel on only the main roads,
    thus traveling more.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略 2**：其他车辆可以学会只在主干道上行驶，从而行驶更长的距离。'
- en: '**Policy 3**: The controlled vehicles will plan the route by choosing a route
    that will take them to their destination so that they drive fewer roads.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略 3**：受控车辆将通过选择一条能将它们带到目的地的路线来规划路线，从而行驶较少的道路。'
- en: Obviously, some policies are better than others and there are many ways to evaluate
    them, namely the function value state and the function's value-action. The goal
    is to learn the best policy. The policy can be approached in two ways: **policy
    iteration** and **policy search**. The main difference between these two techniques
    is in the use of the value function. In the upcoming sections, we will analyze
    both of these approaches in detail.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，一些策略优于其他策略，且有许多方法可以评估它们，即状态的函数值和值-行动函数。目标是学习最佳策略。策略可以通过两种方式来接近：**策略迭代**和**策略搜索**。这两种技术的主要区别在于价值函数的使用。在接下来的部分中，我们将详细分析这两种方法。
- en: Policy iteration
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略迭代
- en: Policy iteration is a dynamic programming algorithm that uses a value function
    to model the expected return for each pair of action-states. Many techniques in
    reinforcement learning are based on this technique, including Q-learning, TD-learning,
    SARSA, QV-learning, and more. These techniques update the value functions using
    the immediate reward and the (discounted) value of the next state in a process
    called **bootstrap**. Therefore, they imply the storage of *Q (s, a)* in tables
    or with approximate function techniques.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代是一种动态规划算法，使用价值函数来建模每对动作-状态的期望回报。强化学习中的许多技术都基于这一技术，包括Q学习、TD学习、SARSA、QV学习等。这些技术通过立即奖励和下一个状态的（折扣后）价值来更新价值函数，这一过程称为**引导法**。因此，它们隐含了将*Q(s,
    a)*存储在表格中或使用近似函数技术的做法。
- en: Policy iteration is generally applied to discrete Markov decision processes,
    where both the state space *S* and the action space *A* are discrete and finite
    sets.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 策略迭代通常应用于离散的马尔可夫决策过程，其中状态空间*S*和动作空间*A*都是离散且有限的集合。
- en: 'Starting from an initial P[0] policy, the iteration of the policy alternates
    between the following two phases:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从初始的P[0]策略开始，策略迭代在以下两个阶段之间交替进行：
- en: '**Policy evaluation**: Given the current policy P, estimate the action-value
    function, Q[P].'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**策略评估**：给定当前策略P，估计动作价值函数Q[P]。'
- en: '**Policy improvement**: Calculate a better policy P '' based on Q[P], then
    set P'' as the new policy and return to the previous step.'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**策略改进**：基于Q[P]计算更好的策略P''，然后将P''设为新的策略并返回到上一步。'
- en: When the action-value function can be calculated for each action-state pair,
    the policy iteration with the greedy policy improvement leads to convergence by
    returning the optimal policy. Essentially, repeatedly executing these two processes
    converges the general process toward the optimal solution.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当可以计算每个动作-状态对的动作价值函数时，采用贪婪策略改进的策略迭代会通过返回最优策略而收敛。实质上，反复执行这两个过程将一般过程收敛到最优解。
- en: Unfortunately, the Q[P] value function can't always be calculated exactly; generally,
    it can be estimated from samples. In these cases, it is necessary to include a
    certain degree of randomness in the policy to ensure sufficient exploration of
    the state's space of action. These algorithms store the value function in a finite
    table (tabular approach). The limitation of these algorithms is that they cannot
    be applied to the case of continuous Markov decision processes. Moreover, these
    methods can be unusable in some discrete cases where the cardinality of the state-action
    space is too high. Let's look at another approach to this problem.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Q[P]价值函数并非总能被精确计算；通常只能通过样本进行估计。在这种情况下，必须在策略中引入一定程度的随机性，以确保对状态空间的充分探索。这些算法将价值函数存储在有限的表格中（表格方法）。这些算法的局限性在于，它们无法应用于连续马尔可夫决策过程的情况。此外，在某些离散情况下，如果状态-动作空间的基数过高，这些方法也可能无法使用。让我们看看另一种解决这一问题的方法。
- en: Policy search
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略搜索
- en: In the policy search approach, a parameterized policy is stored, but no value
    function is used or estimated. Policy search methods can rely on roll-out policies
    that use trajectory-based sampling. Other policy search methods use optimization
    techniques such as evolutionary algorithms to search for optimal policy parameters.
    Policy gradient methods are examples of policy search. Let's look at these in
    detail.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在策略搜索方法中，存储的是一个参数化的策略，但不使用或估计价值函数。策略搜索方法可以依赖于使用基于轨迹的采样的展开策略。其他策略搜索方法则使用如进化算法等优化技术来搜索最优策略参数。策略梯度方法就是策略搜索的一个例子。让我们详细看看这些方法。
- en: Exploring policy gradient methods
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索策略梯度方法
- en: The policy gradient is a class of reinforcement learning algorithms based on
    the use of parameterized policies. The idea is to calculate the expected return
    gradient (reward) with respect to each parameter in order to change the parameters
    in a direction that increases the performance of the same. This method doesn't
    show the problems of traditional reinforcement learning such as the lack of guarantees
    of a value function, the problem resulting from the uncertainty of the state,
    and the complexities that arise from states and actions in continuous spaces.
    In the policy search method, no value function is used or estimated. The value
    function can be used to learn the policy parameter; however, it won't necessary
    for action selection. Policy gradient methods bypass all the problems that are
    connected to value function-based techniques by searching for the optimal policy
    directly.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度是一类基于参数化策略的强化学习算法。其思想是计算每个参数相对于预期回报梯度（奖励），从而在有利的方向上调整参数，以提高性能。该方法没有传统强化学习中的问题，如缺乏值函数保证、状态不确定性带来的问题以及连续空间中状态和动作的复杂性。在策略搜索方法中，不使用或估计值函数。值函数可以用来学习策略参数；然而，动作选择时并不一定需要它。策略梯度方法通过直接寻找最优策略，绕过了与基于值函数的技术相关的所有问题。
- en: 'The pros of applying policy gradient methods are as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 应用策略梯度方法的优点如下：
- en: Continuous states and actions can be treated as discrete cases and learning
    performance is often increased.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续状态和动作可以视作离散情况处理，学习性能通常会提高。
- en: There's a great variety of different algorithms in the literature that have
    strong theoretical bases.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文献中有各种不同的算法，它们具有强大的理论基础。
- en: State uncertainty doesn't degrade the learning process, even if no particular
    state estimator is used.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使没有使用特定的状态估计器，状态不确定性也不会降低学习过程的效果。
- en: 'The cons of applying policy gradient methods are as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 应用策略梯度方法的缺点如下：
- en: The learning rate can decide on the order of magnitude of the convergence speed.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率可以决定收敛速度的数量级。
- en: They need to update the data very quickly to avoid the introduction of errors
    in the gradient estimator. This means that the use of sample data is not very
    efficient.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们需要非常快速地更新数据，以避免在梯度估计器中引入错误。这意味着样本数据的使用效率不高。
- en: In practice, a policy gradient returns the direction in which we have to modify
    the parameters of our algorithm to improve the policy in order to maximize the
    total accumulated rewards. The gradient is equal to the gradient of the logarithmic
    probability of the action that's chosen. In other words, we are looking to increase
    the probability of actions that return a good total reward and to reduce the probability
    of actions with negative final results – we keep what works and leave out what
    it doesn't.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 实际中，策略梯度返回的是我们需要修改算法参数的方向，以改善策略并最大化总的累积奖励。这个梯度等于所选动作的对数概率的梯度。换句话说，我们希望增加那些能带来良好总奖励的动作的概率，并减少那些带来负面结果的动作的概率——我们保留有效的部分，剔除无效的部分。
- en: As we mentioned in the *Markov decision process* section, the choice of the
    action to be taken depends on the objective to be achieved and is mapped through
    the policy indicated with the symbol π (discounted cumulative reward), which associates
    the action with a ∈ A(s) for each state, s. The term πt(s,a) represents the probability
    that action *a* is carried out in the state, *s*.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*马尔可夫决策过程*部分提到的，选择要执行的动作取决于要实现的目标，并通过表示为π的策略（折扣累积奖励）映射， π将每个状态s下的动作与a ∈
    A(s)关联。术语πt(s,a)表示在状态*s*下执行动作*a*的概率。
- en: The purpose of this procedure is to parameterize the policy using a θ parameter,
    which allows us to determine the best action, *a*, in a state, *s*. This policy
    function will be defined as πθ (s,a).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的目的是通过使用θ参数对策略进行参数化，这样我们就可以确定在状态*s*下的最佳动作*a*。这个策略函数将定义为πθ(s,a)。
- en: To find the optimal policy, we will use a neural network that will input the
    state and output the probability of each action in that state. This probability
    will be used to sample an action from this distribution and perform that action
    in the state. Nothing tells us that the sampled action is the correct action to
    perform in the state. Then, we perform the action and keep the reward. This procedure
    will be repeated for each state. The data that's obtained will be our training
    data. At this point, to update the gradients, we will use an algorithm based on
    the descent of the gradient.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最优策略，我们将使用一个神经网络，该网络将输入状态并输出该状态下每个动作的概率。这个概率将用于从该分布中采样一个动作，并在该状态下执行该动作。没有任何信息告诉我们，所采样的动作是执行该状态下的正确动作。然后，我们执行该动作并记录奖励。这个过程将对每个状态重复进行。获取的数据将作为我们的训练数据。在此时，为了更新梯度，我们将使用一个基于梯度下降的算法。
- en: By doing this, the actions that return a high reward in a state will present
    a high probability, while the actions with a low reward will have a low probability.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，在某一状态下返回高奖励的动作将具有较高的概率，而低奖励的动作将具有较低的概率。
- en: 'Like in all gradient descent methods, the parameter vector is updated in the
    direction of the gradient of a performance measurement. In this case, the measurement
    for performance is given by the following formula:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有梯度下降方法一样，参数向量按照性能度量的梯度方向进行更新。在此情况下，性能的度量由以下公式给出：
- en: '![](img/a0d4a2fb-5f46-4f71-b547-677149d2a4a0.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0d4a2fb-5f46-4f71-b547-677149d2a4a0.png)'
- en: 'In the preceding formula, *E* is the expected return and *r* is the reward.
    Our aim is to learn a policy that maximizes the cumulative future reward. The
    gradient of the expected return is called the policy gradient, and we''ll use
    it to update the θ parameter, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，*E* 是期望回报，*r* 是奖励。我们的目标是学习一个能够最大化累积未来奖励的策略。期望回报的梯度被称为策略梯度，我们将利用它来更新 θ
    参数，如下所示：
- en: '![](img/8a939a7b-ece1-42c3-be19-90a141b1981f.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a939a7b-ece1-42c3-be19-90a141b1981f.png)'
- en: 'Here, we have the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: θ is the parameter
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: θ 是参数
- en: ∇ is the gradient of the expected return
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ∇ 是期望回报的梯度
- en: α is the learning rate
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α 是学习率
- en: A parameter update is performed at each learning iteration. The gradient descent
    algorithm guarantees convergence to at least one local optimum.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 每次学习迭代时都会执行参数更新。梯度下降算法保证至少会收敛到一个局部最优解。
- en: 'A general policy gradient algorithm can be summarized as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的策略梯度算法可以总结如下：
- en: '[PRE0]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the next section, we will analyze some methods based on the policy gradient,
    that is, the **Monte Carlo policy gradient** and **actor-critic methods**.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将分析一些基于策略梯度的方法，即**蒙特卡洛策略梯度**和**演员-评论员方法**。
- en: The Monte Carlo policy gradient
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蒙特卡洛策略梯度
- en: The Monte-Carlo policy gradient, also called **REINFORCE**, is a class of associative
    RL algorithms for networks connected to stochastic units. It has been shown that,
    in tasks with immediate reinforcement and, with some limitations, even in some
    tasks with delayed reinforcement, these algorithms perform parameter adjustments
    in a direction that lies along the gradient of the expected reinforcement. Because
    of their nature, these algorithms can easily be integrated with other gradient
    descent methods and with backpropagation in particular. The major drawback is
    that they are unable to distinguish between local (global) maximums (minimum)
    and don't have a general convergence theory.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特卡洛策略梯度，亦称为**REINFORCE**，是一类用于与随机单元连接的网络的关联强化学习算法。研究表明，在具有即时强化的任务中，且在一些限制条件下，甚至在一些具有延迟强化的任务中，这些算法能够在期望强化的梯度方向上进行参数调整。由于其特性，这些算法能够与其他梯度下降方法，尤其是反向传播方法，轻松结合。主要的缺点是它们无法区分局部（全局）最大值（最小值），且没有通用的收敛理论。
- en: In this family of algorithms, the agent generates a trajectory of an episode
    using its current policy and uses it to update the policy parameter. This algorithm
    provides an off-policy update since a complete trajectory must be completed to
    build a sample space.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这类算法中，智能体通过当前策略生成一个情节轨迹，并利用该轨迹来更新策略参数。该算法提供了一种脱离策略的更新，因为必须完成完整的轨迹才能构建样本空间。
- en: 'The following code is the pseudocode for the REINFORCE algorithm:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是 REINFORCE 算法的伪代码：
- en: '[PRE1]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we can see, no explicit exploration is required. In this case, having calculated
    the probabilities with a neural network, the exploration is performed automatically.
    First, random weights are used to initialize the network and a uniform probability
    distribution is returned. This distribution is equivalent to the behavior of a
    random agent.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，不需要显式的探索。在这种情况下，通过神经网络计算概率后，探索会自动进行。首先，使用随机权重初始化网络，并返回均匀概率分布。该分布等价于随机智能体的行为。
- en: To reduce the variance of the gradient estimation while maintaining the bias,
    you can make a change to the REINFORCE algorithm by subtracting a base value from
    the return.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少梯度估计的方差同时保持偏差，可以通过从回报中减去基准值来修改 REINFORCE 算法。
- en: Now, let's look at another method based on the policy gradient.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看基于策略梯度的另一种方法。
- en: Actor-critic methods
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Actor-critic 方法
- en: Actor-critic methods use policy search and value function estimation in order
    to carry out low-variance gradient updates. These methods separate the memory
    structure to make the policy independent of the value function. The policy block
    is known as an **actor** because it chooses actions, while the estimated value
    function block is known as a **critic** in the sense that it criticizes the actions
    that are performed by the policy that is being followed. From this, it is clear
    that learning is an on-policy type where the critic learns and criticizes the
    work of politics.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-critic 方法结合了策略搜索和价值函数估计，以便执行低方差的梯度更新。这些方法将记忆结构分开，使得策略与价值函数相互独立。策略模块被称为**演员**，因为它选择动作，而估计的价值函数模块被称为**评论员**，因为它对由策略执行的动作进行评估。从这一点来看，可以明确地看出学习是一种基于策略的方法，在这种方法中，评论员学习并评估所跟随的策略的执行结果。
- en: Typically, the critic is a function of state evaluation. After each selection
    of an action, the critic assesses the new state to determine whether things have
    gone better or worse than expected.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，评论员是状态评估函数。每次选择动作后，评论员评估新状态，以确定事情是否比预期更好或更糟。
- en: 'The agent performs two jobs and performs two roles with two different networks:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体执行两项任务，并通过两种不同的网络扮演两个角色：
- en: The **actor** network is the one that establishes the action to be performed
    in a certain state by updating the policy parameters in the direction proposed
    by the critic.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**演员** 网络是根据评论员建议的方向，通过更新策略参数来确定在某一状态下需要执行的动作。'
- en: The **critic** network is the one that evaluates the consequences of this action,
    modifying the function value of the next time step, and updating the value function
    parameters.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评论员** 网络是评估该动作后果的网络，修改下一时间步的函数值，并更新价值函数参数。'
- en: The environment is perceived and measured by input sensors. The system processes
    these inputs within the evaluation/estimation of the state. The status estimate
    and any rewards are then communicated to the agent.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 环境通过输入传感器进行感知和测量。系统在状态的评估/估计过程中处理这些输入。然后，状态估计和任何奖励会传递给智能体。
- en: 'The following is the pseudocode for the actor-critic algorithm:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 Actor-critic 算法的伪代码：
- en: '[PRE2]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see the critic network and the actor network are updated at each step.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，评论员网络和演员网络在每一步都进行更新。
- en: Summary
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we have learned about the basics of building a model based
    on Markov processes. A Markov decision-making process is a stochastic process
    characterized by five elements: epoch, states, actions, transition probabilities,
    and rewards. There''s also an agent present that controls the path of the stochastic
    process. At a certain point in the path and at a certain point, *t*, the agent
    intervenes and makes a decision that will influence the future evolution of the
    process. These moments are called epochs of decision, while the decisions that
    are made assume the connotation of actions.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何基于马尔可夫过程构建模型的基础知识。马尔可夫决策过程是一个由五个元素组成的随机过程：时刻、状态、动作、转移概率和奖励。过程的路径由一个控制的智能体来决定。在路径的某个特定点和时刻，*t*，智能体介入并做出决定，这个决定将影响过程未来的演变。这些时刻称为决策的时刻，而所做的决策则具有动作的含义。
- en: 'Then, we discovered the policy that defines the behavior of the learning agent
    at a given time. It maps both the detected states of the environment and the actions
    to take when they are in those states. The policy is the fundamental part of a
    reinforcing learning agent in the sense that it alone is enough to determine behavior.
    The policy can be approached in two ways: **policy iteration** and **policy search**.
    The main difference between these two techniques is in their use of the value
    function.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们发现了定义学习智能体在特定时刻行为的策略。它映射了环境的检测状态以及在这些状态下应该采取的行动。策略是强化学习智能体的核心部分，因为单凭它就足以决定行为。策略可以通过两种方式来处理：**策略迭代**和**策略搜索**。这两种技术的主要区别在于它们对价值函数的使用。
- en: Finally, we learned how to build a model based on policy gradient methods, that
    is, a class of reinforcement learning algorithms based on the use of parameterized
    policies. The idea is to calculate the expected return gradient (reward) with
    respect to each parameter in order to change the parameters in a direction that
    increases the performance of the same.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了如何基于策略梯度方法构建模型，也就是基于使用参数化策略的一类强化学习算法。其核心思想是计算相对于每个参数的期望回报梯度（奖励），以便在某个方向上调整参数，从而提高性能。
- en: In the next chapter, we will see the Markov decision processes in action. We
    will get to grips with the concepts of the Markov decision process and understand
    the agent-environment interaction process. Then, we will learn how to use Bellman
    equations as consistency conditions for the optimal value functions to determine
    the optimal policy. Finally, we will discover and implement Markov chains and
    learn how to simulate random walks using them.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将看到马尔可夫决策过程的实际应用。我们将深入理解马尔可夫决策过程的概念，并理解智能体与环境的交互过程。接下来，我们将学习如何使用贝尔曼方程作为最优价值函数的一致性条件，从而确定最优策略。最后，我们将发现并实现马尔可夫链，并学习如何使用它们来模拟随机游走。
