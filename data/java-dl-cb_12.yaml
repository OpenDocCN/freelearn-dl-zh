- en: Benchmarking and Neural Network Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试与神经网络优化
- en: Benchmarking is a standard against which we compare solutions to find out whether
    they are good or not. In the context of deep learning, we might set benchmarks
    for an existing model that is performing pretty well. We might test our model
    against factors such as accuracy, the amount of data handled, memory consumption,
    and JVM garbage collection tuning. In this chapter, we briefly talk about the
    benchmarking possibilities with your DL4J applications. We will start with general
    guidelines and then move on to more DL4J-specific benchmarking settings. At the
    end of the chapter, we will look at a hyperparameter tuning example that shows
    how to find the best neural network parameters in order to yield the best results.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试是我们用来比较解决方案的标准，以判断它们是否优秀。在深度学习的背景下，我们可能会为表现相当不错的现有模型设定基准。我们可能会根据准确率、处理的数据量、内存消耗和JVM垃圾回收调优等因素来测试我们的模型。本章简要讨论了DL4J应用程序中的基准测试可能性。我们将从一般指南开始，然后转向更具体的DL4J基准测试设置。在本章的最后，我们将介绍一个超参数调优示例，展示如何找到最佳的神经网络参数，以获得最佳的结果。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下内容：
- en: DL4J/ND4J specific configuration
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J/ND4J 特定配置
- en: Setting up heap spaces and garbage collection
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置堆空间和垃圾回收
- en: Using asynchronous ETL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用异步ETL
- en: Using arbiter to monitor neural network behavior
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用arbiter监控神经网络行为
- en: Performing hyperparameter tuning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行超参数调优
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter is located at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java)。
- en: After cloning our GitHub repository, navigate to the `Java-Deep-Learning-Cookbook/12_Benchmarking_and_Neural_Network_Optimization/sourceCode`
    directory. Then import the `cookbookapp` project as a Maven project by importing `pom.xml`.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆我们的GitHub仓库后，导航到`Java-Deep-Learning-Cookbook/12_Benchmarking_and_Neural_Network_Optimization/sourceCode`目录。然后通过导入`pom.xml`将`cookbookapp`项目作为Maven项目导入。
- en: 'The following are links to two examples:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是两个示例的链接：
- en: 'Hyperparameter tuning example: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java/HyperParameterTuning.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java/HyperParameterTuning.java)'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数调优示例：[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java/HyperParameterTuning.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java/HyperParameterTuning.java)
- en: Arbiter UI example: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java/HyperParameterTuningArbiterUiExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java/HyperParameterTuningArbiterUiExample.java)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arbiter UI 示例：[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java/HyperParameterTuningArbiterUiExample.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/12_Benchmarking_and_Neural_Network_Optimization/sourceCode/cookbookapp/src/main/java/HyperParameterTuningArbiterUiExample.java)
- en: This chapter's examples are based on a customer churn dataset ([https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/resources](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/resources)).
    This dataset is included in the project directory.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例基于一个客户流失数据集（[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/resources](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/03_Building_Deep_Neural_Networks_for_Binary_classification/sourceCode/cookbookapp/src/main/resources)）。该数据集包含在项目目录中。
- en: 'Although we are explaining DL4J/ND4J-specific benchmarks in this chapter, it
    is recommended you follow general benchmarking guidelines. The following some important
    generic benchmarks that are common for any neural network:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在本章中解释了DL4J/ND4J特定的基准测试，但我们建议您遵循一般的基准测试指南。以下是一些常见的神经网络通用基准：
- en: '**Perform warm-up iterations before the actual benchmark task**: Warm-up iterations
    refer to a set of iterations performed on benchmark tasks before commencing the
    actual ETL operation or network training. Warm up iterations are important because
    the execution of the first few iterations will be slow. This can add to the total
    duration of the benchmark tasks and we could end up with wrong/inconsistent conclusions.
    The slow execution of the first few iterations may be because of the compilation
    time taken by JVM, the lazy-loading approach of DL4J/ND4J libraries, or the learning
    phase of DL4J/ND4J libraries. This learning phase refers to the time taken to
    learn the memory requirements for execution.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在实际基准任务之前进行预热迭代**：预热迭代指的是在开始实际ETL操作或网络训练之前，在基准任务上执行的一组迭代。预热迭代非常重要，因为最初的几次执行会很慢。这可能会增加基准任务的总时长，并可能导致错误或不一致的结论。最初几次迭代的缓慢执行可能是由于JVM的编译时间，DL4J/ND4J库的延迟加载方式，或DL4J/ND4J库的学习阶段所致。学习阶段是指执行过程中用于学习内存需求的时间。'
- en: '**Perform benchmark tasks multiple times**: To make sure that benchmark results
    are reliable, we need to run benchmark tasks multiple times. The host system may
    have multiple apps/processes running in parallel apart from the benchmark instance.
    So, the runtime performance will vary over time. In order to assess this situation,
    we need to run benchmark tasks multiple times.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多次执行基准任务**：为了确保基准结果的可靠性，我们需要多次执行基准任务。主机系统可能除了基准实例外还在并行运行多个应用程序/进程。因此，运行时性能会随着时间变化。为了评估这种情况，我们需要多次运行基准任务。'
- en: '**Understand where you set the benchmarks and why**: We need to assess whether
    we are setting the right benchmarking. If we target operation a, then make sure
    that only operation a is being timed for benchmark. Also, we have to make sure
    that we are using the right libraries for the right situation. The latest versions
    of libraries are always preferred. It is also important to assess DL4J/ND4J configurations
    used in our code. The default configurations may suffice in regular scenarios,
    but manual configuration may be required for optimal performance. The following
    some of the default configuration options for reference:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**了解基准设置的目的和原因**：我们需要评估是否设置了正确的基准。如果我们的目标是操作a，那么确保只针对操作a进行基准测试。同时，我们还必须确保在适当的情况下使用正确的库。始终推荐使用库的最新版本。评估代码中使用的DL4J/ND4J配置也非常重要。默认配置在常规情况下可能足够，但为了获得最佳性能，可能需要手动配置。以下是一些默认配置选项，供参考：'
- en: Memory configurations (heap space setup).
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存配置（堆空间设置）。
- en: Garbage collection and workspace configuration (changing the frequency at which
    the garbage collector is called).
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垃圾回收和工作区配置（更改垃圾回收器调用的频率）。
- en: Add cuDNN support (utilizing a CUDA-powered GPU machine with better performance).
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加cuDNN支持（利用CUDA加速的GPU机器以获得更好的性能）。
- en: Enable DL4J cache mode (to bring in cache memory for the training instance).
    This will be a DL4J-specific change.
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用DL4J缓存模式（为训练实例引入缓存内存）。这将是DL4J特定的更改。
- en: We discussed cuDNN in [Chapter 1](f88b350b-16e2-425b-8425-4631187c7803.xhtml), *Introduction
    to Deep Learning in Java*, while we talked about DL4J in GPU environments. These
    configuration options will be discussed further in upcoming recipes.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第1章](f88b350b-16e2-425b-8425-4631187c7803.xhtml)中讨论了cuDNN，*Java中的深度学习介绍*，同时谈到了GPU环境下的DL4J。这些配置选项将在接下来的教程中进一步讨论。
- en: '**Run the benchmark on a range of sizes**: It is important to run the benchmark
    on multiple different input sizes/shapes to get a complete picture of its performance.
    Mathematical computations such as matrix multiplications vary over different dimensions.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在不同规模的任务上运行基准**：在多个不同的输入大小/形状上运行基准非常重要，以全面了解其性能。像矩阵乘法这样的数学计算在不同维度下会有所不同。'
- en: '**Understand the hardware**: The training instance with the smallest minibatch
    size will perform better on a CPU than on a GPU system. When we use a large minibatch
    size, the observation will be exactly the opposite. The training instance will
    now be able to utilize GPU resources. In the same way, a large layer size can
    better utilize GPU resources. Writing network configurations without understanding
    the underlying hardware will not allow us to exploit its full capabilities.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**了解硬件**：使用最小批次大小的训练实例在 CPU 上的表现会比在 GPU 系统上更好。当我们使用较大的批次大小时，观察到的情况恰恰相反。此时，训练实例能够利用
    GPU 资源。同样，较大的层大小也能更好地利用 GPU 资源。不了解底层硬件就编写网络配置，将无法发挥其全部潜力。'
- en: '**Reproduce the benchmarks and understand their limits**: In order to troubleshoot
    performance bottlenecks against a set benchmark, we always need to reproduce them.
    It is helpful to assess the circumstance under which poor performance occurs. On
    top of that, we also need to understand the limitations put on certain benchmarks.
    Certain benchmarks set on a specific layer won''t tell you anything about the
    performance factor of other layers.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重现基准测试并理解其局限性**：为了排查性能瓶颈，我们总是需要重现基准测试。评估性能不佳的情况时，了解其发生的环境非常有帮助。除此之外，我们还需要理解某些基准测试的限制。针对特定层设置的基准测试不会告诉你其他层的性能因素。'
- en: '**Avoid common benchmark mistakes**:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免常见的基准测试错误**：'
- en: Consider using the latest version of DL4J/ND4J. To apply the latest performance
    improvements, try snapshot versions.
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑使用最新版本的 DL4J/ND4J。为了应用最新的性能改进，可以尝试使用快照版本。
- en: Pay attention to the types of native libraries used (such as cuDNN).
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意使用的本地库类型（例如 cuDNN）。
- en: Run enough iterations and with a reasonable minibatch size to yield consistent
    results.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行足够多的迭代，并使用合理的批次大小以获得一致的结果。
- en: Do not compare results across hardware without accounting for the differences.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对硬件差异未进行考虑的情况下，不要跨硬件进行结果比较。
- en: In order to benefit from the latest fixes for performance issues, you need to
    have latest version in your local. If you want to run the source on the latest
    fix and if the new version hasn't been released, then you can make use of snapshot
    versions. To find out more about working with snapshot versions, go to [https://deeplearning4j.org/docs/latest/deeplearning4j-config-snapshots](https://deeplearning4j.org/docs/latest/deeplearning4j-config-snapshots).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从最新的性能修复中受益，您需要在本地使用最新版本。如果您想在最新修复上运行源代码，并且新版本尚未发布，那么可以使用快照版本。有关如何使用快照版本的详细信息，请访问
    [https://deeplearning4j.org/docs/latest/deeplearning4j-config-snapshots](https://deeplearning4j.org/docs/latest/deeplearning4j-config-snapshots)。
- en: DL4J/ND4J-specific configuration
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DL4J/ND4J 特定配置
- en: Apart from general benchmarking guidelines, we need to follow additional benchmarking
    configurations that are DL4J/ND4J-specific. These are important benchmarking configurations
    that target the hardware and mathematical computations.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了常规的基准测试指南外，我们还需要遵循一些特定于 DL4J/ND4J 的附加基准测试配置。这些是针对硬件和数学计算的重要基准测试配置。
- en: Because ND4J is the JVM computation library for DL4J, benchmarks mostly target
    mathematical computations. Any benchmarks discussed with regard to ND4J can then
    also be applied to DL4J. Let's discuss DL4J/ND4J-specific benchmarks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ND4J 是 DL4J 的 JVM 计算库，因此基准测试主要针对数学计算。任何关于 ND4J 的基准测试都可以同样应用于 DL4J。让我们来讨论
    DL4J/ND4J 特定的基准测试。
- en: Getting ready
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Make sure you have downloaded cudNN from the following link: [https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn). Install
    it before attempting to configure it with DL4J. Note that cuDNN doesn't come as
    a bundle with CUDA. So, adding the CUDA dependency alone will not be enough.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 确保已经从以下链接下载了 cudNN：[https://developer.nvidia.com/cudnn](https://developer.nvidia.com/cudnn)。在尝试将其与
    DL4J 配置之前，请先安装它。请注意，cuDNN 并不包含在 CUDA 中，因此仅添加 CUDA 依赖并不足够。
- en: How to do it...
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Detach the `INDArray` data to use it across workspaces:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分离 `INDArray` 数据以便在不同工作区间使用：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Remove all workspaces that were created during training/evaluation in case
    they are running short of RAM:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除训练/评估过程中创建的所有工作区，以防它们内存不足：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Leverage an array instance from another workspace in the current workspace
    by calling `leverageTo()`:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `leverageTo()` 在当前工作区使用来自其他工作区的数组实例：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Track the time spent on every iteration during training using `PerformanceListener`:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `PerformanceListener` 跟踪每次迭代时花费的时间：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Add the following Maven dependency for cuDNN support:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为支持 cuDNN 添加以下 Maven 依赖：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Configure DL4J/cuDNN to favor performance over memory:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置DL4J/cuDNN以优先考虑性能而非内存：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Configure `ParallelWrapper` to support multi-GPU training/inferences:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置`ParallelWrapper`以支持多GPU训练/推理：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Configure `ParallelInference` as follows:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式配置`ParallelInference`：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: How it works...
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: A workspace is a memory management model that enables the reuse of memory for
    cyclic workloads without having to introduce a JVM garbage collector. `INDArray` memory
    content is invalidated once in every workspace loop. Workspaces can be integrated
    for training or inference.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 工作空间是一种内存管理模型，它使得在无需引入JVM垃圾回收器的情况下，实现对循环工作负载的内存重用。每次工作空间循环时，`INDArray`的内存内容都会失效。工作空间可以用于训练或推理。
- en: In step 1, we start with workspace benchmarking. The `detach()` method will
    detach the specific INDArray from the workspace and will return a copy. So, how
    do we enable workspace modes for our training instance?  Well, if you're using
    the latest DL4J version (from 1.0.0-alpha onwards), then this feature is enabled
    by default. We target version 1.0.0-beta 3 in this book.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们从工作空间基准测试开始。`detach()`方法将从工作空间中分离出特定的`INDArray`并返回一个副本。那么，我们如何为训练实例启用工作空间模式呢？如果你使用的是最新的DL4J版本（从1.0.0-alpha版本起），那么此功能默认已启用。本书中我们使用的目标版本是1.0.0-beta
    3。
- en: 'In step 2, we removed workspaces from the memory, as shown here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2步中，我们从内存中移除了工作空间，如下所示：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This will destroy workspaces from the current running thread only. We can release
    memory from workspaces in this way by running this piece of code in the thread
    in question.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这将仅销毁当前运行线程中的工作空间。通过在相关线程中运行这段代码，我们可以释放工作空间的内存。
- en: 'DL4J also lets you implement your own workspace manager for layers. For example,
    activation results from one layer during training can be placed in one workspace,
    and the results of the inference can be placed in another workspace. This is possible
    using DL4J''s  `LayerWorkspaceMgr`, as mentioned in step 3\. Make sure that the
    returned array (`myArray` in step 3) is defined as `ArrayType.ACTIVATIONS`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J还允许你为层实现自定义的工作空间管理器。例如，训练期间某一层的激活结果可以放在一个工作空间中，而推理的结果则可以放在另一个工作空间中。这可以通过DL4J的`LayerWorkspaceMgr`来实现，如第3步所述。确保返回的数组（第3步中的`myArray`）被定义为`ArrayType.ACTIVATIONS`：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is fine to use different workspace modes for training/inference. But it is
    recommended you use `SEPARATE` mode for training and `SINGLE` mode for inference
    because inference only involves a forward pass and doesn't involve backpropagation. However,
    for training instances with high resource consumption/memory, it might be better
    to go for `SEPARATE` workspace mode because it consumes less memory. Note that
    `SEPARATE` is the default workspace mode in DL4J.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练/推理，使用不同的工作空间模式是可以的。但推荐在训练时使用`SEPARATE`模式，在推理时使用`SINGLE`模式，因为推理只涉及前向传播，不涉及反向传播。然而，对于资源消耗/内存较高的训练实例，使用`SEPARATE`工作空间模式可能更合适，因为它消耗的内存较少。请注意，`SEPARATE`是DL4J中的默认工作空间模式。
- en: 'In step 4, two attributes are used while creating `PerformanceListener`: `reportScore`
    and `frequency`. `reportScore` is a Boolean variable and `frequency` is the iteration
    count by which time needs to be tracked. If `reportScore` is `true`, then it will
    report the score (just like in `ScoreIterationListener`) along with information
    on the time spent on each iteration.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步中，创建`PerformanceListener`时使用了两个属性：`reportScore`和`frequency`。`reportScore`是一个布尔变量，`frequency`是需要追踪时间的迭代次数。如果`reportScore`为`true`，则会报告得分（就像在`ScoreIterationListener`中一样），并提供每次迭代所花费时间的信息。
- en: In step 7, we used `ParallelWrapper` or `ParallelInference` for multi-GPU devices. Once
    we have created a neural network model, we can create a parallel wrapper using
    it. We specify the count of devices, a training mode, and the number of workers
    for the parallel wrapper.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7步中，我们使用了`ParallelWrapper`或`ParallelInference`来支持多GPU设备。一旦我们创建了神经网络模型，就可以使用它创建一个并行包装器。我们需要指定设备数量、训练模式以及并行包装器的工作线程数。
- en: We need to make sure that our training instance is cost-effective. It is not
    feasible to spend a lot adding multiple GPUs and then utilizing one GPU in training.
    Ideally, we want to utilize all GPU hardware to speed up the training/inference
    process and get better results. `ParallelWrapper` and `ParallelInference` serve
    this purpose.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保训练实例是具备成本效益的。将多个GPU添加到系统中并在训练时仅使用一个GPU是不现实的。理想情况下，我们希望充分利用所有GPU硬件来加速训练/推理过程，并获得更好的结果。`ParallelWrapper`和`ParallelInference`正是为了这个目的。
- en: 'The following some configurations supported by `ParallelWrapper` and `ParallelInference`:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`ParallelWrapper`和`ParallelInference`支持的一些配置：
- en: '`prefetchBuffer(deviceCount)`: This parallel wrapper method specifies dataset
    prefetch options. We mention the number of devices here.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prefetchBuffer(deviceCount)`：此并行包装方法指定数据集预取选项。我们在此提到设备的数量。'
- en: '`trainingMode(mode)`: This parallel wrapper method specifies the distributed
    training method. `SHARED_GRADIENTS` refers to the gradient sharing method for
    distributed training.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trainingMode(mode)`：此并行包装方法指定分布式训练方法。`SHARED_GRADIENTS`指的是分布式训练中的梯度共享方法。'
- en: '`workers(Nd4j.getAffinityManager().getNumberOfDevices())`: This parallel wrapper
    method specifies the number of workers. We set the number of workers to the number
    of available systems.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`workers(Nd4j.getAffinityManager().getNumberOfDevices())`：此并行包装方法指定工作者的数量。我们将工作者的数量设置为可用系统的数量。'
- en: '`inferenceMode(mode)`: This parallel inference method specifies the distributed
    inference method. `BATCHED` mode is an optimization. If a large number of requests
    come in, it will process them in batches. If there is a small number of requests,
    then they will be processed as usual without batching. As you might have guessed,
    this is the perfect option if you''re in production.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inferenceMode(mode)`：此并行推理方法指定分布式推理方法。`BATCHED`模式是一种优化方式。如果大量请求涌入，它会将请求批量处理。如果请求较少，则会按常规处理，不进行批处理。正如你可能猜到的，这是生产环境中的最佳选择。'
- en: '`batchLimit(batchSize)`: This parallel inference method specifies the batch
    size limit and is only applicable if you use `BATCHED` mode in `inferenceMode()`.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batchLimit(batchSize)`：此并行推理方法指定批处理大小限制，仅在使用`inferenceMode()`中的`BATCHED`模式时适用。'
- en: There's more...
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'The performance of ND4J operations can also vary upon input array ordering. ND4J
    enforces the ordering of arrays. Performance in mathematical operations (including
    general ND4J operations) depends on the input and result array orders. For example,
    performance in operations such as simple addition, such as *z = x + y*, will vary in
    line with the input array orders. It happens due to memory striding: it is easier
    to read the memory sequence if they''re close/adjacent to each other than when
    they''re spread far apart. ND4J is faster on computations with larger matrices.
    By default, ND4J arrays are C-ordered. IC ordering refers to row-major ordering
    and the memory allocation resembles that of an array in C:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ND4J操作的性能还可能受到输入数组排序的影响。ND4J强制执行数组的排序。数学运算（包括一般的ND4J操作）的性能取决于输入数组和结果数组的排序。例如，像*z
    = x + y*这样的简单加法操作的性能会根据输入数组的排序有所变化。这是因为内存步幅的原因：如果内存序列靠得很近，读取它们会更容易，而不是分布得很远。ND4J在处理更大的矩阵时运算速度更快。默认情况下，ND4J数组是C-顺序的。IC排序指的是行主序排序，内存分配类似于C语言中的数组：
- en: '![](img/fd7db9f6-a996-485b-87e1-3f535f59a786.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fd7db9f6-a996-485b-87e1-3f535f59a786.png)'
- en: '(Image courtesy: Eclipse Deeplearning4j Development Team. Deeplearning4j: Open-source
    distributed deep learning for the JVM, Apache Software Foundation License 2.0. http://deeplearning4j.org)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: （图片由Eclipse Deeplearning4j开发团队提供。Deeplearning4j：用于JVM的开源分布式深度学习，Apache软件基金会许可证2.0。http://deeplearning4j.org）
- en: ND4J supplies the `gemm()` method for advanced matrix multiplication between
    two INDArrays depending on whether we require multiplication after transposing it.
    This method returns the result in F-order, which means the memory allocation resembles
    that of an array in Fortran. F-ordering refers to column-major ordering. Let's
    say we have passed a C-ordered array to collect the results from the `gemm()` method;
    ND4J automatically detects it, creates an F-ordered array, and then passes the
    result to a C-ordered array.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ND4J提供了`gemm()`方法，用于在两个INDArray之间进行高级矩阵乘法，具体取决于是否需要在转置后进行乘法运算。此方法返回F顺序的结果，这意味着内存分配类似于Fortran中的数组。F顺序指的是列主序排序。假设我们传递了一个C-顺序的数组来收集`gemm()`方法的结果；ND4J会自动检测它，创建一个F-顺序数组，然后将结果传递给一个C-顺序数组。
- en: To learn more about array ordering and how ND4J handles array ordering, go to [https://deeplearning4j.org/docs/latest/nd4j-overview](https://deeplearning4j.org/docs/latest/nd4j-overview).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于数组排序以及ND4J如何处理数组排序的信息，请访问[https://deeplearning4j.org/docs/latest/nd4j-overview](https://deeplearning4j.org/docs/latest/nd4j-overview)。
- en: It is also critical to assess the minibatch size used for training. We need
    to experiment with different minibatch sizes while performing multiple training
    sessions by acknowledging the hardware specs, data, and evaluation metrics. For
    a CUDA-enabled GPU environment, the minibatch size will have a big role to play
    with regard to benchmarks if you use a large enough value. When we talk about
    a large minibatch size, we are referring to a minibatch size that can be justified
    against the entire dataset. For very small minibatch sizes, we won't observe any
    noticeable performance difference with the CPU/GPU after the benchmarks. At the
    same time, we need to watch out for changes in model accuracy as well. An ideal
    minibatch size is when we utilize the hardware to its full ability without affecting
    model accuracy. In fact, we aim for better results with better performance (shorter
    training time).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 评估用于训练的迷你批次大小也是至关重要的。我们需要在进行多次训练时，尝试不同的迷你批次大小，并根据硬件规格、数据和评估指标进行调整。在启用CUDA的GPU环境中，如果使用一个足够大的值，迷你批次大小将在基准测试中起到重要作用。当我们谈论一个大的迷你批次大小时，我们是指可以根据整个数据集来合理化的迷你批次大小。对于非常小的迷你批次大小，我们在基准测试后不会观察到CPU/GPU有明显的性能差异。与此同时，我们还需要关注模型准确度的变化。理想的迷你批次大小是当我们充分利用硬件性能的同时，不影响模型准确度。事实上，我们的目标是在更好的性能（更短的训练时间）下获得更好的结果。
- en: Setting up heap spaces and garbage collection
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置堆空间和垃圾回收
- en: Memory heap spaces and garbage collection are frequently discussed yet are often
    the most frequently ignored benchmarks. With DL4J/ND4J, you can configure two
    types of memory limit: on-heap memory and off-heap memory. Whenever an INDArray is
    collected by the JVM garbage collector, the off-heap memory will be de-allocated,
    assuming that it is not being used anywhere else. In this recipe, we will set
    up heap spaces and garbage collection for benchmarking.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 内存堆空间和垃圾回收是经常被讨论的话题，但却往往是最常被忽略的基准测试。在使用DL4J/ND4J时，你可以配置两种类型的内存限制：堆内存和非堆内存。每当JVM垃圾回收器回收一个`INDArray`时，非堆内存将被释放，前提是它不在其他地方使用。在本教程中，我们将设置堆空间和垃圾回收以进行基准测试。
- en: How to do it...
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Add the required VM arguments to the Eclipse/IntelliJ IDE, as shown in the
    following example:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向Eclipse/IntelliJ IDE中添加所需的VM参数，如以下示例所示：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'For example, in IntelliJ IDE, we can add VM arguments to the runtime configuration:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在IntelliJ IDE中，我们可以将VM参数添加到运行时配置中：
- en: '![](img/7aef487e-7668-429d-9bce-b73c51d3c941.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7aef487e-7668-429d-9bce-b73c51d3c941.png)'
- en: 'Run the following command after changing the memory limits to suit your hardware
    (for command-line executions):'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在更改内存限制以适应硬件后，运行以下命令（用于命令行执行）：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Configure a server-style generational garbage collector for JVM:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置JVM的服务器风格代际垃圾回收器：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Reduce the frequency of garbage collector calls using ND4J:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ND4J减少垃圾回收器调用的频率：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Disable garbage collector calls instead of step 4:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 禁用垃圾回收器调用，而不是执行第4步：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Allocate memory chunks in memory-mapped files instead of RAM:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在内存映射文件中分配内存块，而不是使用RAM：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: How it works...
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In step 1, we performed on-heap/off-heap memory configurations. On-heap memory
    simply means the memory that is managed by the JVM heap (garbage collector). Off-heap
    memory refers to memory that is not managed directly, such as that used with INDArrays. Both
    off-heap and on-heap memory limits can be controlled using the following VM options
    in Java command-line arguments:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1步中，我们进行了堆内存/非堆内存配置。堆内存指的是由JVM堆（垃圾回收器）管理的内存。非堆内存则是指不被直接管理的内存，例如INDArrays使用的内存。通过以下Java命令行选项，我们可以控制堆内存和非堆内存的限制：
- en: '`-Xms`: This defines how much memory will be consumed by the JVM heap at application
    startup.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-Xms`：此选项定义了应用启动时JVM堆将消耗的内存量。'
- en: '`-Xmx`: This defines the maximum memory that can be consumed by the JVM heap
    at any point in runtime. This involves allotting memory only up to this point
    when it is required.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-Xmx`：此选项定义了JVM堆在运行时可以消耗的最大内存。它仅在需要时分配内存，且不会超过此限制。'
- en: '`-Dorg.bytedeco.javacpp.maxbytes`: This specifies the off-heap memory limit.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-Dorg.bytedeco.javacpp.maxbytes`：此选项指定非堆内存的限制。'
- en: '`-Dorg.bytedeco.javacpp.maxphysicalbytes`: This specifies the maximum number
    of bytes that can be allotted to the application at any given time. Usually, this
    takes a larger value than `-Xmx` and `maxbytes` combined.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-Dorg.bytedeco.javacpp.maxphysicalbytes`：此选项指定可以分配给应用程序的最大字节数。通常，这个值比`-Xmx`和`maxbytes`的组合值要大。'
- en: 'Suppose we want to configure 1 GB initially on-heap, 6 GB max on-heap, 16 GB
    off-heap, and 20 GB maximum for processes; the VM arguments will look as follows,
    and as shown in step 1:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要在堆内最初配置1 GB，在堆内最大配置6 GB，在堆外配置16 GB，并在进程的最大内存为20 GB，VM参数将如下所示，并如步骤1所示：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that you will need to adjust this in line with the memory available in
    your hardware.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您需要根据硬件可用内存进行相应调整。
- en: It is also possible to set up these VM options as an environment variable. We
    can create an environment variable named `MAVEN_OPTS` and put VM options there.
    You can choose either step 1 or step 2, or set them up with an environment variable.
    Once this is done, you can skip to step 3.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以将这些VM选项设置为环境变量。我们可以创建一个名为`MAVEN_OPTS`的环境变量并将VM选项放置在其中。您可以选择步骤1或步骤2，或者设置环境变量。完成此操作后，可以跳转到步骤3。
- en: In steps 3, 4, and 5, we discussed memory automatically using some tweaks in
    garbage collection. The garbage collector manages memory management and consumes on-heap
    memory. DL4J is tightly coupled with the garbage collector. If we talk about ETL,
    every `DataSetIterator` object takes 8 bytes of memory. The garbage collector
    can induce further latency in the system. To that end, we configure **G1GC** (short
    for **Garbage First Garbage Collector**) tuning in step 3.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤3、4和5中，我们讨论了通过一些垃圾收集优化自动管理内存。垃圾收集器管理内存管理并消耗堆内内存。DL4J与垃圾收集器紧密耦合。如果我们谈论ETL，每个`DataSetIterator`对象占用8字节内存。垃圾收集器可能会进一步增加系统的延迟。为此，我们在步骤3中配置了**G1GC**（即**Garbage
    First Garbage Collector**）调优。
- en: If we pass 0 ms (milliseconds) as an attribute to the `setAutoGcWindow()` method,
    as in step 4, it will just disable this particular option. `getMemoryManager()` will
    return a backend-specific implementation of `MemoryManager` for lower-level memory
    management.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将0毫秒（毫秒）作为属性传递给`setAutoGcWindow()`方法（如步骤4所示），它将只是禁用此特定选项。`getMemoryManager()`将返回一个用于更低级别内存管理的后端特定实现的`MemoryManager`。
- en: In step 6, we discussed configuring memory-mapped files to allocate more memory
    for INDArrays. We have created a 1 GB memory map file in step 4. Note that memory-mapped
    files can be created and supported only when using the `nd4j-native` library.
    Memory mapped files are slower than memory allocation in RAM. Step 4 can be applied
    if the minibatch size memory requirement is higher than the amount of RAM available.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤6中，我们讨论了配置内存映射文件以为**INDArrays**分配更多内存。我们在步骤4中创建了一个1 GB的内存映射文件。请注意，只有使用`nd4j-native`库时才能创建和支持内存映射文件。内存映射文件比RAM中的内存分配速度慢。如果小批量大小的内存需求高于可用RAM量，则可以应用步骤4。
- en: There's more...
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这还不是全部……
- en: DL4J has a dependency with JavaCPP that acts as a bridge between Java and C++: [https://github.com/bytedeco/javacpp](https://github.com/bytedeco/javacpp).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J与JavaCPP有依赖关系，后者充当Java和C++之间的桥梁：[https://github.com/bytedeco/javacpp](https://github.com/bytedeco/javacpp)。
- en: JavaCPP works on the basis of the `-Xmx` value set on the heap space (off-heap
    memory) and the overall memory consumption will not exceed this value. DL4J seeks
    help from the garbage collector and JavaCPP to deallocate memory.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: JavaCPP基于堆空间（堆外内存）上设置的`-Xmx`值运行。DL4J寻求垃圾收集器和JavaCPP的帮助来释放内存。
- en: For training sessions with large amounts of data involved, it is important to
    have more RAM for the off-heap memory space than for on-heap memory (JVM). Why?
    Because our datasets and computations are involved with INDArrays and are stored
    in the off-heap memory space.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于涉及大量数据的训练会话，重要的是为堆外内存空间（JVM）提供比堆内内存更多的RAM。为什么？因为我们的数据集和计算涉及到**INDArrays**，并存储在堆外内存空间中。
- en: 'It is important to identify the memory limits of running applications. The
    following some instances where the memory limit needs to be properly configured:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 识别运行应用程序的内存限制非常重要。以下是需要正确配置内存限制的一些情况：
- en: For GPU systems, `maxbytes` and `maxphysicalbytes` are the important memory
    limit settings. We are dealing with off-heap memory here. Allocating reasonable
    memory to these settings allows us to consume more GPU resources.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于GPU系统，`maxbytes`和`maxphysicalbytes`是重要的内存限制设置。这里我们处理的是堆外内存。为这些设置分配合理的内存允许我们使用更多的GPU资源。
- en: For `RunTimeException` that refer to memory allocation issues, one possible
    reason may be the unavailability of off-heap memory spaces. If we don't use the
    memory limit (off-heap space) settings discussed in the *Setting up heap space
    and garbage collection* recipe, the off-heap memory space can be reclaimed by
    the JVM garbage collector. This can then cause memory allocation issues.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于涉及内存分配问题的 `RunTimeException`，一个可能的原因是堆外内存空间不可用。如果我们没有使用 *设置堆空间和垃圾回收* 章节中讨论的内存限制（堆外内存空间）设置，堆外内存空间可能会被
    JVM 垃圾回收器回收，从而导致内存分配问题。
- en: If you have limited-memory environments, then it is not recommended to use large
    values for the `-Xmx` and `-Xms` options. For instance, if we use `-Xms6G` for
    an 8 GB RAM system, we leave only 2 GB for the off-heap memory space, the OS,
    and for other processes.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的环境内存有限，建议不要为 `-Xmx` 和 `-Xms` 选项设置过大的值。例如，如果我们为 8 GB 内存的系统使用 `-Xms6G`，那么仅剩下
    2 GB 的内存空间用于堆外内存、操作系统和其他进程。
- en: See also
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: If you're interested in knowing more about G1GC garbage collector tuning, you
    can read about it here: [https://www.oracle.com/technetwork/articles/java/g1gc-1984535.html](https://www.oracle.com/technetwork/articles/java/g1gc-1984535.html)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解更多关于 G1GC 垃圾回收器的调优内容，可以阅读以下链接：[https://www.oracle.com/technetwork/articles/java/g1gc-1984535.html](https://www.oracle.com/technetwork/articles/java/g1gc-1984535.html)
- en: Using asynchronous ETL
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用异步 ETL
- en: We use synchronous ETL for demonstration purposes. But for production, asynchronous
    ETL is preferable. In production, the existence of a single low-performance ETA
    component can cause a performance bottleneck. In DL4J, we load data to the disk
    using `DataSetIterator`. It can load the data from disk or, memory, or simply
    load data asynchronously. Asynchronous ETL uses an asynchronous loader in the
    background. Using multithreading, it loads data into the GPU/CPU and other threads
    take care of compute tasks. In the following recipe, we will perform asynchronous
    ETL operations in DL4J.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用同步 ETL 进行演示。但在生产环境中，推荐使用异步 ETL。在生产环境中，一个低性能的 ETA 组件可能会导致性能瓶颈。在 DL4J 中，我们使用
    `DataSetIterator` 将数据加载到磁盘。它可以从磁盘、内存中加载数据，或者简单地异步加载数据。异步 ETL 在后台使用异步加载器。通过多线程，它将数据加载到
    GPU/CPU 中，其他线程负责计算任务。在下面的操作步骤中，我们将在 DL4J 中执行异步 ETL 操作。
- en: How to do it...
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create asynchronous iterators with asynchronous prefetch:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用异步预取创建异步迭代器：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Create asynchronous iterators with synchronous prefetch:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用同步预取创建异步迭代器：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, we created an iterator using `AsyncMultiDataSetIterator`. We can
    use  `AsyncMultiDataSetIterator` or `AsyncDataSetIterator` to create asynchronous
    iterators. There are multiple ways in which you can configure an `AsyncMultiDataSetIterator`.
    There are multiple ways to create `AsyncMultiDataSetIterator` by passing further
    attributes such as `queSize` (the number of mini-batches that can be prefetched
    at once) and `useWorkSpace` (a Boolean type indicating whether workspace configuration
    should be used). While using `AsyncDataSetIterator`, we use the current dataset
    before calling `next()` to get the next dataset. Also note that we should not
    store datasets without the `detach()` call. If you do, then the memory used by
    INDArray data inside the dataset will eventually be overwritten within  `AsyncDataSetIterator`. For
    custom iterator implementations, make sure you don't initialize something huge
    using the `next()` call during training/evaluation. Instead, keep all such initialization
    inside the constructor to avoid undesired workspace memory consumption.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，我们使用 `AsyncMultiDataSetIterator` 创建了一个迭代器。我们可以使用 `AsyncMultiDataSetIterator`
    或 `AsyncDataSetIterator` 来创建异步迭代器。`AsyncMultiDataSetIterator` 有多种配置方式。你可以通过传递其他属性来创建
    `AsyncMultiDataSetIterator`，例如 `queSize`（一次可以预取的迷你批次的数量）和 `useWorkSpace`（布尔类型，表示是否应该使用工作区配置）。在使用
    `AsyncDataSetIterator` 时，我们会在调用 `next()` 获取下一个数据集之前使用当前数据集。还需要注意的是，在没有调用 `detach()`
    的情况下不应存储数据集。如果这样做，数据集中 INDArray 数据使用的内存最终会在 `AsyncDataSetIterator` 中被覆盖。对于自定义迭代器实现，确保你在训练/评估过程中不要通过
    `next()` 调用初始化大型对象。相反，应将所有初始化工作放在构造函数内，以避免不必要的工作区内存消耗。
- en: In step 2, we created an iterator using `AsyncShieldDataSetIterator`. To opt
    out of asynchronous prefetch, we can use `AsyncShieldMultiDataSetIterator` or
    `AsyncShieldDataSetIterator`. These wrappers will prevent asynchronous prefetch
    in data-intensive operations such as training, and can be used for debugging purposes.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，我们使用`AsyncShieldDataSetIterator`创建了一个迭代器。要选择退出异步预取，我们可以使用`AsyncShieldMultiDataSetIterator`或`AsyncShieldDataSetIterator`。这些包装器将在数据密集型操作（如训练）中防止异步预取，可以用于调试目的。
- en: 'If the training instance performs ETL every time it runs, we are basically
    recreating the data every time it runs. Eventually, the whole process (training
    and evaluation) will get slower. We can handle this better using a pre-saved dataset.
    We discussed pre-save using `ExistingMiniBatchDataSetIterator` in the previous
    chapter, when we pre-saved feature data and then later loaded it using `ExistingMiniBatchDataSetIterator`.
    We can convert it to an asynchronous iterator (as in step 1 or step 2) and kill
    two birds with one stone: pre-saved data with asynchronous loading. This is essentially a
    performance benchmark that further optimizes the ETL process.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练实例每次运行时都执行ETL操作，实际上我们每次都在重新创建数据。最终，整个过程（训练和评估）会变得更慢。我们可以通过使用预先保存的数据集来更好地处理这一点。我们在上一章中讨论了使用`ExistingMiniBatchDataSetIterator`进行预保存，当时我们预保存了特征数据，并随后使用`ExistingMiniBatchDataSetIterator`加载它。我们可以将其转换为异步迭代器（如步骤1或步骤2所示），一举两得：使用异步加载的预保存数据。这本质上是一个性能基准，进一步优化了ETL过程。
- en: There's more...
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Let's say our minibatch has 100 samples and we specify `queSize` as `10`; 1,000
    samples will be prefetched every time. The memory requirement of the workspace
    depends on the size of the dataset, which arises from the underlying iterator.
    The workspace will be adjusted for varying memory requirements (for example, time
    series with varying lengths). Note that asynchronous iterators are internally
    supported by `LinkedBlockingQueue`. This queue data structure orders elements
    in **First In First Out** (**FIFO**) mode. Linked queues generally have more throughput
    than array-based queues in concurrent environments.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的迷你批次有100个样本，并且我们将`queSize`设置为`10`；每次将预取1,000个样本。工作区的内存需求取决于数据集的大小，这来自于底层的迭代器。工作区将根据不同的内存需求进行调整（例如，长度变化的时间序列）。请注意，异步迭代器是通过`LinkedBlockingQueue`在内部支持的。这个队列数据结构以**先进先出**（**FIFO**）模式对元素进行排序。在并发环境中，链式队列通常比基于数组的队列有更高的吞吐量。
- en: Using arbiter to monitor neural network behavior
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用arbiter监控神经网络行为
- en: Hyperparameter optimization/tuning is the process of finding the optimal values
    for hyperparameters in the learning process. Hyperparameter optimization partially
    automates the process of finding optimal hyperparameters using certain search
    strategies. Arbiter is part of the DL4J deep learning library and is used for
    hyperparameter optimization. Arbiter can be used to find high-performing models
    by tuning the hyperparameters of the neural network. Arbiter has a UI that visualizes
    the results of the hyperparameter tuning process.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数优化/调优是寻找学习过程中超参数的最优值的过程。超参数优化部分自动化了使用某些搜索策略来寻找最佳超参数的过程。Arbiter是DL4J深度学习库的一部分，用于超参数优化。Arbiter可以通过调整神经网络的超参数来找到高性能的模型。Arbiter有一个用户界面，用于可视化超参数调优过程的结果。
- en: In this recipe, we will set up arbiter and visualize the training instance to
    take a look at neural network behavior.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将设置arbiter并可视化训练实例，观察神经网络的行为。
- en: How to do it...
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Add the arbiter Maven dependency in `pom.xml`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`pom.xml`中添加arbiter Maven依赖：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Configure the search space using `ContinuousParameterSpace`:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ContinuousParameterSpace`配置搜索空间：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Configure the search space using `IntegerParameterSpace`:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`IntegerParameterSpace`配置搜索空间：
- en: '[PRE21]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Use `OptimizationConfiguration` to combine all components required to execute
    the hyperparameter tuning process:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`OptimizationConfiguration`来结合执行超参数调优过程所需的所有组件：
- en: '[PRE22]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: How it works...
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In step 2, we created  `ContinuousParameterSpace` to configure the search space
    for hyperparameter optimization:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，我们创建了`ContinuousParameterSpace`来配置超参数优化的搜索空间：
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In the preceding case, the hyperparameter tuning process will select continuous
    values in the range (0.0001, 0.01) for the learning rate. Note that arbiter doesn't
    really automate the hyperparameter tuning process. We still need to specify the
    range of values or a list of options by which the hyperparameter tuning process
    takes place. In other words, we need to specify a search space with all the valid
    values for the tuning process to pick the best combination that can produce the
    best results. We have also mentioned `IntegerParameterSpace`, where the search
    space is an ordered space of integers between a maximum/minimum value.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述情况下，超参数调优过程将选择学习率在(0.0001, 0.01)范围内的连续值。请注意，仲裁者并不会自动化超参数调优过程。我们仍然需要指定值的范围或选项列表，以便超参数调优过程进行。换句话说，我们需要指定一个搜索空间，其中包含所有有效的值，供调优过程选择最佳组合，从而获得最佳结果。我们还提到了`IntegerParameterSpace`，它的搜索空间是一个整数的有序空间，位于最大/最小值之间。
- en: Since there are multiple training instances with different configurations, it
    takes a while  to finish the hyperparameter optimization-tuning process. At the
    end, the best configuration will be returned.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有多个不同配置的训练实例，因此超参数优化调优过程需要一段时间才能完成。最后，将返回最佳配置。
- en: In step 2, once we have defined our search space using `ParameterSpace` or `OptimizationConfiguration`,
    we need to add it to `MultiLayerSpace` or `ComputationGraphSpace`. These are the
    arbiter counterparts of DL4J's `MultiLayerConfiguration` and `ComputationGraphConfiguration`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，一旦我们使用`ParameterSpace`或`OptimizationConfiguration`定义了搜索空间，我们需要将其添加到`MultiLayerSpace`或`ComputationGraphSpace`中。这些是DL4J的`MultiLayerConfiguration`和`ComputationGraphConfiguration`的仲裁者对应物。
- en: Then we added `candidateGenerator` using the `candidateGenerator()` builder
    method. `candidateGenerator` chooses candidates (various combinations of hyperparameters)
    for hyperparameter tuning. It can use different approaches, such as random search
    and grid search, to pick the next configuration for hyperparameter tuning.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`candidateGenerator()`构建方法添加了`candidateGenerator`。`candidateGenerator`为超参数调优选择候选者（各种超参数组合）。它可以使用不同的方法，如随机搜索和网格搜索，来选择下一个用于超参数调优的配置。
- en: '`scoreFunction()` specifies the evaluation metrics used for evaluation during
    the hyperparameter tuning process.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`scoreFunction()`指定在超参数调优过程中用于评估的评估指标。'
- en: '`terminationConditions()` is used to mention all termination conditions for
    the training instance. Hyperparameter tuning will then proceed with the next configuration
    in the sequence.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`terminationConditions()`用于指定所有的训练终止条件。超参数调优随后将进行下一个配置。'
- en: Performing hyperparameter tuning
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行超参数调优
- en: Once search spaces are defined using `ParameterSpace` or `OptimizationConfiguration`, with
    a possible range of values, the next step is to complete network configuration
    using  `MultiLayerSpace` or `ComputationGraphSpace`. After that, we start the
    training process. We perform multiple training sessions during the hyperparameter
    tuning process.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用`ParameterSpace`或`OptimizationConfiguration`定义了搜索空间，并且有可能的值范围，下一步是使用`MultiLayerSpace`或`ComputationGraphSpace`完成网络配置。之后，我们开始训练过程。在超参数调优过程中，我们会执行多个训练会话。
- en: In this recipe, we will perform and visualize the hyperparameter tuning process.
    We will be using `MultiLayerSpace` for the demonstration.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将执行并可视化超参数调优过程。我们将在演示中使用`MultiLayerSpace`。
- en: How to do it...
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Add a search space for the layer size using `IntegerParameterSpace`:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`IntegerParameterSpace`为层大小添加搜索空间：
- en: '[PRE24]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Add a search space for the learning rate using `ContinuousParameterSpace`:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ContinuousParameterSpace`为学习率添加搜索空间：
- en: '[PRE25]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Use `MultiLayerSpace` to build a configuration space by adding all the search
    spaces to the relevant network configuration:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`MultiLayerSpace`通过将所有搜索空间添加到相关的网络配置中来构建配置空间：
- en: '[PRE26]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create `candidateGenerator` from `MultiLayerSpace`:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`MultiLayerSpace`创建`candidateGenerator`：
- en: '[PRE27]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Create a data source by implementing the `DataSource` interface:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过实现`DataSource`接口来创建数据源：
- en: '[PRE28]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will need to implement four methods: `configure()`, `trainData()`, `testData()`,
    and `getDataType()`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要实现四个方法：`configure()`、`trainData()`、`testData()`和`getDataType()`：
- en: 'The following is an example implementation of `configure()`:'
  id: totrans-169
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是`configure()`的示例实现：
- en: '[PRE29]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here''s an example implementation of `getDataType()`:'
  id: totrans-171
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是`getDataType()`的示例实现：
- en: '[PRE30]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here''s an example implementation of `trainData()`:'
  id: totrans-173
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是`trainData()`的示例实现：
- en: '[PRE31]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here''s an example implementation of `testData()`:'
  id: totrans-175
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是`testData()`的示例实现：
- en: '[PRE32]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create an array of termination conditions:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个终止条件数组：
- en: '[PRE33]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Calculate the score of all models that were created using different combinations
    of configurations:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算使用不同配置组合创建的所有模型的得分：
- en: '[PRE34]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Create `OptimizationConfiguration` and add termination conditions and the score
    function:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`OptimizationConfiguration`并添加终止条件和评分函数：
- en: '[PRE35]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Create `LocalOptimizationRunner` to run the hyperparameter tuning process:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`LocalOptimizationRunner`以运行超参数调优过程：
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Add listeners to `LocalOptimizationRunner` to ensure events are logged properly
    (skip to step 11 to add `ArbiterStatusListener`):'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向`LocalOptimizationRunner`添加监听器，以确保事件正确记录（跳到第11步添加`ArbiterStatusListener`）：
- en: '[PRE37]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Execute the hyperparameter tuning by calling the `execute()` method:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`execute()`方法执行超参数调优：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Store the model configurations and replace `LoggingStatusListener` with `ArbiterStatusListener`:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储模型配置并将`LoggingStatusListener`替换为`ArbiterStatusListener`：
- en: '[PRE39]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Attach the storage to `UIServer`:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将存储附加到`UIServer`：
- en: '[PRE40]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Run the hyperparameter tuning session and go to the following URL to view the
    visualization:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行超参数调优会话，并访问以下URL查看可视化效果：
- en: '[PRE41]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Evaluate the best score from the hyperparameter tuning session and display
    the results in the console:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估超参数调优会话中的最佳得分，并在控制台中显示结果：
- en: '[PRE42]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'You should see the output shown in the following snapshot. The model''s best
    score, the index where the best model is located, and the number of configurations
    evaluated in the process are displayed:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到以下快照中显示的输出。显示了模型的最佳得分、最佳模型所在的索引以及在过程中过滤的配置数量：
- en: '![](img/8f118f92-bd5f-475e-a313-f071925878ed.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f118f92-bd5f-475e-a313-f071925878ed.png)'
- en: How it works...
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 4, we set up a strategy by which the network configurations will be
    picked up from the search space. We use `CandidateGenerator` for this purpose.
    We created a parameter mapping to store all data mappings for use with the data
    source and passed it to `CandidateGenerator`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步中，我们设置了一种策略，通过该策略从搜索空间中选择网络配置。我们为此目的使用了`CandidateGenerator`。我们创建了一个参数映射来存储所有数据映射，以便与数据源一起使用，并将其传递给`CandidateGenerator`。
- en: 'In step 5, we implemented the `configure()` method along with three other methods
    from the `DataSource` interface. The `configure()` method accepts a `Properties` attribute,
    which has all parameters to be used with the data source. If we want to pass `miniBatchSize` as
    a property, then we can create a `Properties` instance as shown here:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5步中，我们实现了`configure()`方法以及来自`DataSource`接口的另外三个方法。`configure()`方法接受一个`Properties`属性，其中包含所有要与数据源一起使用的参数。如果我们想传递`miniBatchSize`作为属性，则可以创建一个`Properties`实例，如下所示：
- en: '[PRE43]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Note that the minibatch size needs to be mentioned as a string: `"64"` and
    not `64`.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，迷你批量大小需要作为字符串 `"64"` 提供，而不是 `64`。
- en: The custom `dataPreprocess()` method pre-processes data. `dataSplit()` creates
    `DataSetIteratorSplitter` to generate train/test iterators for training/evaluation.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义的`dataPreprocess()`方法对数据进行预处理。`dataSplit()`创建`DataSetIteratorSplitter`来生成训练/评估的迭代器。
- en: In step 4, `RandomSearchGenerator` generates candidates for hyperparameter tuning
    at random. If we explicitly mention a probability distribution for the hyperparameters,
    then the random search will favor those hyperparameters according to their probability. 
    `GridSearchCandidateGenerator` generates candidates using a grid search. For discrete
    hyperparameters, the grid size is equal to the number of hyperparameter values.
    For integer hyperparameters, the grid size is the same as `min(discretizationCount,max-min+1)`.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4步中，`RandomSearchGenerator`通过随机方式生成超参数调优的候选项。如果我们明确提到超参数的概率分布，那么随机搜索将根据其概率偏向这些超参数。`GridSearchCandidateGenerator`通过网格搜索生成候选项。对于离散型超参数，网格大小等于超参数值的数量。对于整数型超参数，网格大小与`min(discretizationCount,max-min+1)`相同。
- en: In step 6, we defined termination conditions. Termination conditions control
    how far the training process should progress. Termination conditions could be
    `MaxTimeCondition`, `MaxCandidatesCondition`, or we can define our own termination
    conditions.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6步中，我们定义了终止条件。终止条件控制训练过程的进展程度。终止条件可以是`MaxTimeCondition`、`MaxCandidatesCondition`，或者我们可以定义自己的终止条件。
- en: In step 7, we created a score function to mention how each and every model is
    evaluated during the hyperparameter optimization process.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7步中，我们创建了一个评分函数，用于说明在超参数优化过程中如何评估每个模型。
- en: 'In step 8, we created `OptimizationConfiguration` comprising these termination
    conditions. Apart from termination conditions, we also added the following configurations
    to `OptimizationConfiguration`:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 8 步中，我们创建了包含这些终止条件的 `OptimizationConfiguration`。除了终止条件外，我们还向 `OptimizationConfiguration`
    添加了以下配置：
- en: The location at which the model information has to be stored
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型信息需要存储的位置
- en: The candidate generator that was created earlier
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前创建的候选生成器
- en: The data source that was created earlier
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前创建的数据源
- en: The type of evaluation metrics to be considered
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要考虑的评估指标类型
- en: '`OptimizationConfiguration` ties all the components together to execute the
    hyperparameter optimization. Note that the `dataSource()` method expects two attributes:
    one is the class type of your data source class, the other is the data source
    properties that we want to pass on (`minibatchSize` in our example). The `modelSaver()`
    builder method requires you to mention the location of the model being trained.
    We can store model information (model score and other configurations) in the resources
    folder, and then we can create a `ModelSaver` instance as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`OptimizationConfiguration` 将所有组件结合起来执行超参数优化。请注意，`dataSource()` 方法需要两个属性：一个是数据源类的类类型，另一个是我们想要传递的数据源属性（在我们的示例中是
    `minibatchSize`）。`modelSaver()` 构建方法要求你指定正在训练的模型的存储位置。我们可以将模型信息（模型评分及其他配置）存储在资源文件夹中，然后创建一个
    `ModelSaver` 实例，如下所示：'
- en: '[PRE44]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In order to visualize the results using arbiter, skip step 10, follow step 12,
    and then execute the visualization task runner.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用裁判进行可视化，跳过第 10 步，按照第 12 步操作，然后执行可视化任务运行器。
- en: 'After following the instructions in steps 13 and 14,  you should be able to
    see arbiter''s UI visualization, as shown here:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在遵循第 13 和第 14 步的指示之后，你应该能够看到裁判的 UI 可视化，如下所示：
- en: '![](img/a25f2b28-308c-483a-a17a-027792c1020b.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a25f2b28-308c-483a-a17a-027792c1020b.png)'
- en: It is very intuitive and easy to figure out the best model score from the arbiter
    visualization. If you run multiple sessions of hyperparameter tuning, then you
    can select a particular session from the drop-down list at the top. Further important
    information displayed on the UI is pretty self-explanatory at this stage.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 从裁判可视化中找出最佳模型评分非常直观且容易。如果你运行了多个超参数调优的会话，你可以从顶部的下拉列表中选择特定的会话。此时，UI上显示的其他重要信息也非常易于理解。
