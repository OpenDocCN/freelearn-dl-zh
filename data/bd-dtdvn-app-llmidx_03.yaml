- en: '<html:html><html:head><html:title>Ingesting Data into Our RAG Workflow</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-70">Ingesting
    Data into Our RAG Workflow</html:h1> <html:div id="_idContainer039"><html:p>We’ve
    taken a good look at the overall structure of LlamaIndex from afar. It’s now time
    to get much closer and understand the small details of this framework. It’s bound
    to get more technical but also more intriguing as we <html:span class="No-Break">go
    further.</html:span></html:p> <html:p>Ready to go deeper down the rabbit hole?
    <html:span class="No-Break">Follow me!</html:span></html:p> <html:p>In this chapter,
    we will learn about <html:span class="No-Break">the following:</html:span></html:p>
    <html:ul><html:li>Using the LlamaHub connectors to ingest <html:span class="No-Break">our
    data</html:span></html:li> <html:li>Taking advantage of the many text-chunking
    tools <html:span class="No-Break">in LlamaIndex</html:span></html:li> <html:li>Infusing
    our nodes with metadata <html:span class="No-Break">and relationships</html:span></html:li>
    <html:li>Keeping our data private and our <html:span class="No-Break">budget safe</html:span></html:li>
    <html:li>Creating ingestion pipelines for better efficiency and <html:span class="No-Break">lower
    costs</html:span></html:li></html:ul> <html:a id="_idTextAnchor070"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Technical
    requirements</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-71">Technical requirements</html:h1> <html:div id="_idContainer039"><html:p>You
    will need to install the following Python libraries in your environment to be
    able to run the examples included in <html:span class="No-Break">this chapter:</html:span></html:p>
    <html:ul><html:li><html:span class="No-Break"><html:strong class="bold">LangChain</html:strong></html:span>
    <html:span class="No-Break">:</html:span> <html:a><html:span class="No-Break">https://www.langchain.com/</html:span></html:a></html:li>
    <html:li><html:span class="No-Break"><html:strong class="bold">Py-Tree-Sitter</html:strong></html:span>
    <html:span class="No-Break">:</html:span> <html:a><html:span class="No-Break">https://pypi.org/project/tree-sitter/</html:span></html:a></html:li></html:ul>
    <html:p>In addition, several LlamaIndex Integration packages will <html:span class="No-Break">be
    required:</html:span></html:p> <html:ul><html:li><html:strong class="bold">Entity</html:strong>
    <html:span class="No-Break"><html:strong class="bold">extractor</html:strong></html:span>
    <html:span class="No-Break">:</html:span> <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-extractors-entity/</html:span></html:a></html:li>
    <html:li><html:strong class="bold">Hugging Face</html:strong> <html:span class="No-Break"><html:strong
    class="bold">LLMs</html:strong></html:span> <html:span class="No-Break">:</html:span>
    <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-llms-huggingface/</html:span></html:a></html:li>
    <html:li><html:strong class="bold">Database</html:strong> <html:span class="No-Break"><html:strong
    class="bold">reader</html:strong></html:span> <html:span class="No-Break">:</html:span>
    <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-readers-database/</html:span></html:a></html:li>
    <html:li><html:strong class="bold">Web</html:strong> <html:span class="No-Break"><html:strong
    class="bold">reader</html:strong></html:span> <html:span class="No-Break">:</html:span>
    <html:a><html:span class="No-Break">https://pypi.org/project/llama-index-readers-web/</html:span></html:a></html:li></html:ul>
    <html:p>All the code examples in this chapter can be found in the <html:em class="italic">ch4</html:em>
    subfolder of this book’s GitHub <html:span class="No-Break">repository:</html:span>
    <html:a><html:span class="No-Break">https://github.com/PacktPublishing/Building-Data-Driven-Applications-with-LlamaIndex</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:a id="_idTextAnchor071"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Ingesting
    data via LlamaHub</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-72">Ingesting data via LlamaHub</html:h1> <html:div id="_idContainer039"><html:p>As
    we saw in <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    3</html:em></html:span></html:a> , <html:em class="italic">Kickstarting Your Journey
    with LlamaIndex</html:em> , one of the first steps in a RAG workflow is to ingest
    and process our proprietary data. We already discovered <html:a id="_idIndexMarker200"></html:a>the
    concepts of documents and nodes, which are used to organize the data and prepare
    it for indexing. I’ve also briefly introduced <html:a id="_idIndexMarker201"></html:a>the
    LlamaHub data loaders as a way to easily ingest data into LlamaIndex. It’s time
    to examine these steps in more detail and gradually learn how to infuse LLM applications
    with our own, proprietary knowledge. Before we continue, though, I’d like to emphasize
    some very common challenges encountered at <html:span class="No-Break">this step:</html:span></html:p>
    <html:ol><html:li>No matter how effective our RAG pipeline is, at the end of the
    day, the quality of the final result will largely depend on the quality of the
    initial data. To overcome this challenge, make sure you start by cleaning up your
    data first. Eliminate potential duplicates and errors. While not exactly duplicates,
    redundant information can also clutter your knowledge base and confuse the RAG
    system. Be on the lookout for ambiguous, biased, incomplete, or outdated information.
    I’ve seen many cases of poorly structured and insufficiently maintained knowledge
    repositories that were completely useless for users looking for quick and accurate
    answers. Ask yourself this question: <html:em class="italic">If I were to manually
    search through this data, how easy would it be to find the information I need?</html:em>
    Before moving on with building the pipeline, do yourself a favor and prepare your
    data thoroughly until you’re satisfied with the answer to <html:span class="No-Break">that
    question.</html:span></html:li> <html:li>Our data is dynamic. An organizational
    knowledge repository is rarely a static, permanent data source. It evolves with
    the business, reflecting new insights, discoveries, and changes in the external
    environment. Recognizing this fluid nature is key to maintaining a relevant and
    effective system. To overcome this challenge, in a production RAG application,
    you’ll have to implement a systematic method for periodically reviewing and updating
    the content, ensuring that new information is incorporated and outdated or incorrect
    data <html:span class="No-Break">is removed.</html:span></html:li> <html:li>Data
    comes in many flavors, shapes, and sizes. Sometimes, it’s structured, sometimes
    not. A well-built RAG system should be able to properly ingest all kinds of <html:a
    id="_idIndexMarker202"></html:a>formats and document types. While LlamaIndex provides
    a huge number of data loaders for many different APIs, databases, and document
    types, building an automated ingestion system can still prove to <html:a id="_idIndexMarker203"></html:a>be
    challenging. To overcome this particular challenge, later in this section, we’ll
    cover <html:strong class="bold">LlamaParse</html:strong> – an innovative hosted
    service designed to automatically ingest and process data from different <html:span
    class="No-Break">data sources.</html:span></html:li></html:ol> <html:p>Now that
    we know what kind of problems await along the way, let’s start our journey by
    first discussing the simplest ways of ingesting the data into the RAG pipeline
    – by using the available LlamaHub <html:span class="No-Break">data loaders.</html:span></html:p>
    <html:a id="_idTextAnchor072"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>An
    overview of LlamaHub</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-73">An overview of LlamaHub</html:h1> <html:div id="_idContainer039"><html:p>LlamaHub
    is an extensive library of integrations that augments the capabilities of the
    core framework. Among many <html:a id="_idIndexMarker204"></html:a>other types
    of integrations, LlamaHub <html:a id="_idIndexMarker205"></html:a>contains numerous
    <html:strong class="bold">connectors</html:strong> – also known as <html:strong
    class="bold">data readers</html:strong> or <html:strong class="bold">data loaders</html:strong>
    – specially built to allow seamless integration of external data with LlamaIndex.
    There are over 180 readily available data readers spanning a wide range of data
    <html:a id="_idIndexMarker206"></html:a>sources and formats, and the list is <html:span
    class="No-Break">constantly increasing.</html:span></html:p> <html:p>These connectors
    act as a standard way to ingest data, extracting data from sources such as databases,
    APIs, files, and websites and converting it into LlamaIndex <html:code class="literal">Document</html:code>
    objects. This relieves you from the burden of writing customized parsers and connectors
    for every new data source. But of course, if you’re not satisfied with the existing
    connectors, you can always build your own and contribute to <html:span class="No-Break">the
    collection.</html:span></html:p> <html:p>LlamaHub empowers you to tap into diverse
    data sources with just a few lines of code. The resulting Document objects can
    then be parsed into nodes and indexed as required by your application. The unified
    output as LlamaIndex <html:code class="literal">Document</html:code> objects means
    your core business <html:a id="_idIndexMarker207"></html:a>logic does not have
    to worry about handling various data types. The complexity is abstracted by <html:span
    class="No-Break">the framework.</html:span></html:p> <html:p class="callout-heading">Why
    do we need so many integrations?</html:p> <html:p class="callout">In <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 2</html:em></html:span></html:a>
    , <html:em class="italic">LlamaIndex: The Hidden Jewel - An Introduction to the
    LlamaIndex Ecosystem</html:em> , in the <html:em class="italic">Familiarizing
    ourselves with the structure of the LlamaIndex code repository</html:em> section,
    I explained the motives behind the framework’s modular architecture. Because of
    this modular architecture, many RAG components provided by LlamaIndex are not
    included in the core elements that are installed together with the rest of the
    framework. This means that before using any data loader for the first time, we
    have to install the corresponding integration package. Once the package has been
    installed, we’ll be able to import the reader into our code and use its functionality.
    Some readers also utilize specialized libraries and tools tailored to each data
    type. For example, <html:code class="literal">PDFReader</html:code> leverages
    Camelot and Tika for parsing PDF content. <html:code class="literal">AirbyteSalesforceReader</html:code>
    uses the Salesforce API client, and so on. This allows us to efficiently adapt
    to the format and interface of each source but may require us to install additional
    packages in our <html:span class="No-Break">development environment.</html:span></html:p>
    <html:p>All available readers are listed on the LlamaHub website and usually come
    with detailed documentation and usage samples. Therefore, I’ll briefly cover just
    a few examples to give a general idea of how you can use them in <html:span class="No-Break">your
    applications.</html:span></html:p> <html:p>I strongly encourage you to take your
    time and go through the entire list of data readers when building your LlamaIndex
    apps instead of spending valuable time building one from scratch. Chances are
    you’ll just be reinventing <html:span class="No-Break">the wheel.</html:span></html:p>
    <html:p>If you’re looking to consult the source code for the readers, you’ll find
    them all included in the Llama-index GitHub repository, under the <html:code class="literal">llama-index-integrations/readers</html:code>
    <html:span class="No-Break">subfolder:</html:span> <html:a><html:span class="No-Break">https://github.com/run-llama/llama_index/tree/main/llama-index-integrations/readers</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>The LlamaHub documentation
    for each data reader lists its installation requirements and usage guidance, so
    before trying to use them, make sure you also install any additional dependencies
    required by specific connectors you want <html:span class="No-Break">to use.</html:span></html:p>
    <html:a id="_idTextAnchor073"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Using
    the LlamaHub data loaders to ingest content</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-74">Using the LlamaHub data loaders
    to ingest content</html:h1> <html:div id="_idContainer039">pip install llama-index-readers-web
    from llama_index.readers.web import SimpleWebPageReader urls = ["https://docs.llamaindex.ai"]
    documents = SimpleWebPageReader().load_data(urls) for doc in documents:     print(doc.text)
    pip install llama-index-readers-database from llama_index.readers.database import
    DatabaseReader reader = DatabaseReader(     uri="sqlite:///files/db/example.db"
    ) query = " from llama_index.core import SimpleDirectoryReader reader = SimpleDirectoryReader(
        input_dir="files",     recursive=True ) documents = reader.load_data() for
    doc in documents:     print(doc.metadata) files = [" from llama_parse import LlamaParse
    from llama_index.core import SimpleDirectoryReader from llama_index.core import
    VectorStoreIndex parser = LlamaParse(result_type="text") file_extractor = {".pdf":
    parser} reader = SimpleDirectoryReader(     "./files/pdf",     file_extractor=file_extractor
    ) docs = reader.load_data() index = VectorStoreIndex.from_documents(docs) qe =
    index.as_query_engine() response = qe.query(     " Started parsing the file under
    job_id <…> <html:p>Apart from the <html:em class="italic">Wikipedia</html:em>
    reader that we discussed in the previous chapter, to get a better <html:a id="_idIndexMarker208"></html:a>understanding
    of how data readers work, let’s look at a few more examples of LlamaHub readers
    that we can use to <html:span class="No-Break">ingest data.</html:span></html:p>
    <html:a id="_idTextAnchor074"></html:a><html:h2 id="_idParaDest-75">Ingesting
    data from a web page</html:h2> <html:p><html:code class="literal">SimpleWebPageReader</html:code>
    can extract text content from <html:span class="No-Break">web pages.</html:span></html:p>
    <html:p>To use it, we <html:a id="_idIndexMarker209"></html:a>must first install
    the <html:span class="No-Break">corresponding integration:</html:span></html:p>
    <html:p>Once installed, it’s really easy <html:span class="No-Break">to use:</html:span></html:p>
    <html:p>This loads and displays the text content of the specified web pages <html:span
    class="No-Break">into documents.</html:span></html:p> <html:p>At its core, <html:code
    class="literal">SimpleWebPageReader</html:code> serves as a bridge between the
    vast, unstructured world of the internet and the structured environment of the
    LlamaIndex RAG pipeline. To better understand its inner workings, let’s explore
    what happens under the hood when it extracts text content from <html:span class="No-Break">web
    pages.</html:span></html:p> <html:p>When loading the data, <html:code class="literal">SimpleWebPageReader</html:code>
    iterates over a list of URLs provided by the user. For each URL, it performs a
    web request to fetch the page content. The response, initially in HTML format,
    can be transformed into plain text if the <html:code class="literal">html_to_text</html:code>
    flag is set to <html:code class="literal">True</html:code> . This transformation
    strips away the HTML tags and converts the web page content into a more digestible
    text format. However, remember what I’ve said about external dependencies for
    these readers? In this case, the HTML-to-text conversion feature requires the
    <html:code class="literal">html2text</html:code> package, which has to be <html:span
    class="No-Break">installed first.</html:span></html:p> <html:p>Another significant
    aspect of this reader is its ability to attach metadata to the scraped documents.
    Through the <html:code class="literal">metadata_fn</html:code> parameter, we can
    pass a custom function that takes a URL as input and returns a dictionary of metadata.
    This flexibility allows for the enrichment of documents with additional information
    or any relevant tags that might be <html:a id="_idIndexMarker210"></html:a>useful
    in categorizing and understanding the context of the data better. Should the user
    provide a <html:code class="literal">metadata_fn</html:code> parameter, the reader
    then applies this function to the current URL to extract metadata, enriching the
    final <html:code class="literal">Document</html:code> object with this additional
    layer <html:span class="No-Break">of information.</html:span></html:p> <html:p
    class="callout-heading">A practical use case for the metadata_fn function</html:p>
    <html:p class="callout">We could, for example, use a function that simply returns
    the current date and time. That way, we could ingest the same URL at different
    moments and build a chronological timeline highlighting different versions of
    that page at various points in time. This could prove useful in scenarios such
    as browsing a code repository or answering questions about a developing <html:span
    class="No-Break">news story.</html:span></html:p> <html:p>Finally, each web page’s
    content, along with its URL and optionally added metadata, is encapsulated in
    a <html:code class="literal">Document</html:code> object. These objects are then
    collected into a list, providing a structured representation of the text content
    and metadata extracted from each <html:span class="No-Break">web page.</html:span></html:p>
    <html:p class="callout-heading">One thing to keep in mind</html:p> <html:p class="callout">As
    its name suggests, this reader is a simple tool. While it can be effective for
    reading simple web pages, for more advanced cases such as pages requiring interaction
    (for example, navigating a login process or handling JavaScript-rendered content),
    <html:code class="literal">SimpleWebPageReader</html:code> might not be sufficient.
    Websites that dynamically generate content based on user interactions or rely
    heavily on client-side scripting can pose challenges that this basic scraper is
    not designed <html:span class="No-Break">to handle.</html:span></html:p> <html:p>Through
    <html:code class="literal">SimpleWebPageReader</html:code> , the task of ingesting
    and structuring basic web content is simplified. The great thing about these readers
    is that they allow us to focus on building <html:a id="_idIndexMarker211"></html:a>and
    enhancing the logic of our RAG applications instead of spending precious time
    on building compatible ingestion tools for each type of data in our <html:span
    class="No-Break">knowledge base.</html:span></html:p> <html:a id="_idTextAnchor075"></html:a><html:h2
    id="_idParaDest-76">Ingesting data from a database</html:h2> <html:p>Using databases
    is not only a common practice but also a highly efficient method for managing
    and retrieving structured information. Databases offer a robust platform for storing
    <html:a id="_idIndexMarker212"></html:a>a vast array of data types, from simple
    text to complex relationships between entities, making them an indispensable asset
    in <html:span class="No-Break">data management.</html:span></html:p> <html:p>The
    <html:code class="literal">DatabaseReader</html:code> connector allows querying
    many database systems. First, we need to install the necessary <html:span class="No-Break">integration
    package:</html:span></html:p> <html:p>Here’s an example of how you can easily
    fetch the contents of an <html:span class="No-Break">SQLite database:</html:span></html:p>
    <html:p>Under the hood, <html:code class="literal">DatabaseReader</html:code>
    connects to various databases to fetch data and transform it <html:a id="_idIndexMarker213"></html:a>into
    a format usable by the RAG pipeline. It supports connection through a <html:code
    class="literal">SQLDatabase</html:code> instance, a <html:strong class="bold">SQLAlchemy
    Engine</html:strong> , a connection URI, or a set of database credentials – provided
    through the <html:code class="literal">scheme</html:code> , <html:code class="literal">host</html:code>
    , <html:code class="literal">port</html:code> , <html:code class="literal">user</html:code>
    , <html:code class="literal">password</html:code> , and <html:code class="literal">dbname</html:code>
    arguments. Once set up, it executes a provided SQL query to retrieve data. After
    connecting to the database, the reader executes the provided <html:code class="literal">query</html:code>
    . The resulting rows are then converted into Document objects, with each row from
    the query result forming a single Document. The conversion process involves concatenating
    each column-value pair into a string, which is then assigned as the text of <html:span
    class="No-Break">a document.</html:span></html:p> <html:p>The example I have provided
    executes the SQL query against an SQLite database stored in the <html:code class="literal">ch4/files/db</html:code>
    folder, loads each returned row as a Document, and displays the results. You can
    find a more general example on the official project documentation <html:span class="No-Break">website:</html:span>
    <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/data_connectors/DatabaseReaderDemo.html</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Alright – I think you
    understand the workflow now. As you’ve probably noticed, the approach for using
    LlamaHub readers is very straightforward. In all the examples, first, we install
    <html:a id="_idIndexMarker214"></html:a>the required integration package, as described
    on LlamaHub, and then use it to import and load data from the reader. Apart from
    the examples I have provided, you’ll find a huge number of data readers available
    on LlamaHub. From Office documents, Gmail accounts, videos and images, YouTube
    videos, and RSS feeds to GitHub repositories and Discord chats, pretty much every
    popular data format <html:span class="No-Break">is supported.</html:span></html:p>
    <html:p>But apart from reading individual files using dedicated data readers,
    in the next section, we will also explore more efficient methods that can be used
    for ingesting multiple documents <html:span class="No-Break">at once.</html:span></html:p>
    <html:a id="_idTextAnchor076"></html:a><html:h2 id="_idParaDest-77">Bulk-ingesting
    data from sources with multiple file formats</html:h2> <html:p>Loading data into
    LlamaIndex is a crucial first step. But sifting through the wide range of data
    <html:a id="_idIndexMarker215"></html:a>loaders in LlamaHub and figuring out how
    to configure each one can feel overwhelming early on. That’s why I’m going to
    show you two different methods that can greatly simplify and reduce the burden
    of data ingestion for your <html:span class="No-Break">RAG systems.</html:span></html:p>
    <html:p>We’ll start with the simple <html:span class="No-Break">method first.</html:span></html:p>
    <html:h3>Using SimpleDirectoryReader to ingest multiple data formats</html:h3>
    <html:p>When <html:a id="_idIndexMarker216"></html:a>you just want to get started
    fast or have a simple use case, <html:code class="literal">SimpleDirectoryReader</html:code>
    comes to the rescue. Think of this reader as your trusty pocketknife for bulk
    data ingestion. It’s easy to use, requires minimal setup, and automatically adapts
    to different file types. To load data, you simply point the reader to a folder
    or list of files. Loading a folder <html:a id="_idIndexMarker217"></html:a>containing
    PDFs, Word docs, plain text files, and CSVs is very straightforward. Here’s <html:span
    class="No-Break">a demonstration:</html:span></html:p> <html:p class="callout-heading">Under
    the hood</html:p> <html:p class="callout"><html:code class="literal">SimpleDirectoryReader</html:code>
    has built-in methods to determine which reader works best for <html:a id="_idIndexMarker218"></html:a>each
    file type. You don’t need to worry about those details. It will automatically
    detect formats such as PDF, DOCX, CSV, plain text, and others based on the file
    extensions. Then, it chooses the best tool to extract the content into Document
    objects. For plain text files, it simply reads the text content. For binary files
    such as PDFs and Office docs, it uses libraries such as PyPDF and Pillow to extract
    <html:span class="No-Break">the text.</html:span></html:p> <html:p><html:code
    class="literal">SimpleDirectoryReader</html:code> effortlessly handles the different
    files and returns the parsed content as documents. By default, it only processes
    files in the directory’s top level. To include subdirectories, you can set the
    <html:code class="literal">recursive</html:code> parameter <html:span class="No-Break">to</html:span>
    <html:span class="No-Break"><html:code class="literal">True</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>You can also pass in
    a list of specific files to load, <html:span class="No-Break">like this:</html:span></html:p>
    <html:p>The result is a batch of Document objects ready for indexing in just a
    few lines of code. No <html:a id="_idIndexMarker219"></html:a>headaches setting
    up separate data readers for each file type. When you want quick and easy data
    ingestion without the complexity, let <html:code class="literal">SimpleDirectoryReader</html:code>
    handle the hard work! It’s versatile <html:span class="No-Break">and automated.</html:span></html:p>
    <html:h3>Parsing like a pro with the help of LlamaParse</html:h3> <html:p>While
    <html:code class="literal">SimpleDirectoryReader</html:code> is great for quick
    and easy data ingestion, sometimes, you need more advanced parsing capabilities,
    especially for complex file formats. Most of <html:a id="_idIndexMarker220"></html:a>the
    time, we have to deal with complex file structures containing a mix of data. For
    example, a PDF file may include images, charts, code snippets, mathematical formulas,
    and other elements alongside its text content. The naive readers included in the
    LlamaHub integration library will be overwhelmed by such cases. They would most
    probably fail to extract the entire content or – even worse – mess up the extracted
    data and complicate its <html:span class="No-Break">further processing.</html:span></html:p>
    <html:p>This is where <html:a id="_idIndexMarker221"></html:a>LlamaParse shines.
    Provided through the LlamaCloud enterprise platform ( <html:a>https://cloud.llamaindex.ai/parse</html:a>
    ), this reader is implemented through a cutting-edge hosted service that integrates
    seamlessly with the other components of the framework. It uses multi-modal capabilities
    and LLM intelligence under the hood to provide industry-leading document parsing,
    including exceptional support for tricky formats such as PDFs containing tables,
    figures, <html:span class="No-Break">and equations.</html:span></html:p> <html:p>One
    of the standout features of <html:code class="literal">LlamaParse</html:code>
    is that it allows you to provide natural language instructions to guide the parsing
    by using the <html:code class="literal">parsing_instruction</html:code> parameter.
    Since you know your documents best, you can tell <html:code class="literal">LlamaParse</html:code>
    exactly what kind of output you need and how that information should be extracted
    from <html:span class="No-Break">the files.</html:span></html:p> <html:p class="callout-heading">For
    instance:</html:p> <html:p class="callout">When parsing a technical whitepaper,
    you could instruct it to extract all the section headings, ignore the footnotes,
    and output any code snippets in markdown format. <html:code class="literal">LlamaParse</html:code>
    will follow your instructions to parse the <html:span class="No-Break">document
    accurately.</html:span></html:p> <html:p>In addition to the instruction-guided
    parsing mode, <html:code class="literal">LlamaParse</html:code> also offers a
    JSON output mode that provides rich structured data about the parsed document,
    including marking tables, headings, extracting images, and more. Also, for bulk-ingesting
    entire folders in one go, <html:code class="literal">LlamaParse</html:code> can
    be used in combination with <html:code class="literal">SimpleDirectoryReader</html:code>
    , as you will see in the next example. This gives you full flexibility to build
    custom RAG applications over a complex collection of documents. You could also
    accomplish this manually by using specialized data readers for each file format
    in your <html:a id="_idIndexMarker222"></html:a>collection of data. However, using
    <html:code class="literal">LlamaParse</html:code> will greatly simplify this process,
    improve the overall quality, and save you a lot <html:span class="No-Break">of
    time.</html:span></html:p> <html:p><html:code class="literal">LlamaParse</html:code>
    supports a wide and expanding range of file types beyond just PDFs, including
    Word docs, PowerPoint, RTF, ePub, and many more. It offers a generous free tier
    to <html:span class="No-Break">get started.</html:span></html:p> <html:p>The necessary
    <html:code class="literal">LlamaParse</html:code> integration package should already
    be installed along with the LlamaIndex components, so no additional installation
    is required to run the code example in <html:span class="No-Break">this section.</html:span></html:p>
    <html:p>The next step is to create a free account on <html:a>https://cloud.llamaindex.ai</html:a>
    and obtain an API key. Once you have obtained the key, you can use it directly
    in your code, but for a more secure approach, I strongly encourage you to follow
    the same steps we followed in <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    2</html:em></html:span></html:a> , <html:em class="italic">LlamaIndex: The Hidden
    Jewel - An Introduction to the LlamaIndex Ecosystem</html:em> , and add the key
    as a variable in your local environment under the name <html:code class="literal">LLAMA_CLOUD_API_KEY</html:code>
    . To demonstrate the capabilities of this tool, I’ve designed a sample PDF with
    a more complex structure, as can be seen in <html:span class="No-Break"><html:em
    class="italic">Figure 4</html:em></html:span> <html:span class="No-Break"><html:em
    class="italic">.1</html:em></html:span> <html:span class="No-Break">:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 4.1 – A sample PDF containing
    multiple articles, images, and tables</html:p> <html:p>Here’s a basic code example
    that uses <html:code class="literal">LlamaParse</html:code> to ingest <html:span
    class="No-Break">this PDF:</html:span></html:p> <html:p>The first part of the
    code imported the necessary modules. Next, we’ll configure <html:code class="literal">LlamaParse</html:code>
    and pass it to <html:code class="literal">SimpleDirectoryReader</html:code> as
    a <html:span class="No-Break"><html:code class="literal">file_extractor</html:code></html:span>
    <html:span class="No-Break">argument:</html:span></html:p> <html:p>Once the PDF
    <html:a id="_idIndexMarker223"></html:a>content has been ingested into a new Document
    object, it’s time to build an index and run a query against <html:span class="No-Break">our
    data:</html:span></html:p> <html:p>The output of this script should be similar
    to <html:span class="No-Break">the following:</html:span></html:p> <html:p class="callout-heading">Important
    note</html:p> <html:p class="callout">One important consideration with using a
    hosted service such as <html:code class="literal">LlamaParse</html:code> is data
    privacy. Before submitting your proprietary data through the API, be sure to carefully
    review their privacy policy to ensure it aligns with your data protection requirements.
    While the service offers powerful parsing capabilities, it’s crucial to safeguard
    <html:span class="No-Break">sensitive information.</html:span></html:p> <html:p>Keep
    in mind that this is a paid service. The great news, however, is that you can
    take advantage of their generous <html:strong class="bold">free tier</html:strong>
    . For higher volume needs, the current pricing can be found on the website. If
    you want to unlock the full potential of <html:code class="literal">LlamaParse</html:code>
    to build advanced <html:a id="_idIndexMarker224"></html:a>document retrieval systems
    or deploy it on your private cloud for maximal data security, that option is available
    <html:span class="No-Break">as well.</html:span></html:p> <html:p>For professional,
    production-ready applications, <html:code class="literal">LlamaParse</html:code>
    is a powerful tool that puts you in full control of parsing your data to maximize
    the quality of your knowledge base and <html:span class="No-Break">RAG applications.</html:span></html:p>
    <html:p>Now that we’ve got the data, let’s make it easier to handle by breaking
    it down into <html:span class="No-Break">smaller pieces.</html:span></html:p>
    <html:a id="_idTextAnchor077"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Parsing
    the documents into nodes</html:title></html:head> <html:body><html:div class="epub-source"><html:h1
    id="_idParaDest-78">Parsing the documents into nodes</html:h1> <html:div id="_idContainer039">from
    llama_index.core.node_parser import < for node in nodes:     print(f"Metadata
    {node.metadata} \nText: {node.text}") splitter = TokenTextSplitter(     chunk_size
    = 70,     chunk_overlap = 2,     separator = " ",     backup_separators = [".",
    "!", "?"] ) nodes = splitter.get_nodes_from_documents(document) pip install tree_sitter
    pip install tree_sitter_languages code_splitter = CodeSplitter.from_defaults(
        language = ''python'',     chunk_lines = 5,     chunk_lines_overlap = 2,     max_chars
    = 150 ) nodes = code_splitter.get_nodes_from_documents(document) parser = SentenceWindowNodeParser.from_defaults(
        window_size=2,     window_metadata_key="text_window",     original_text_metadata_key="original_sentence"
    ) nodes = parser.get_nodes_from_documents(document) pip install langchain from
    langchain.text_splitter import CharacterTextSplitter from llama_index.core.node_parser
    import LangchainNodeParser parser = LangchainNodeParser(CharacterTextSplitter())
    nodes = parser.get_nodes_from_documents(document) parser = SimpleFileNodeParser()
    nodes = parser.get_nodes_from_documents(documents) my_tags = [" parser = MarkdownNodeParser.from_defaults()
    nodes = parser.get_nodes_from_documents(document) json_parser = JSONNodeParser.from_defaults()
    nodes = json_parser.get_nodes_from_documents(document) hierarchical_parser = HierarchicalNodeParser.from_defaults(
        chunk_sizes=[128, 64, 32],     chunk_overlap=0, ) nodes = hierarchical_parser.get_nodes_from_documents(document)
    node_parser = SentenceWindowNodeParser.from_defaults(   include_prev_next_rel=True
    ) node1.relationships[PREVIOUS] = RelatedNodeInfo(node_id=node0.node_id) node2.relationships[NEXT]
    = RelatedNodeInfo(node_id=node3.node_id) <html:p>As we saw in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 3</html:em></html:span></html:a>
    , <html:em class="italic">Kickstarting Your Journey with LlamaIndex</html:em>
    , the next step is to split the documents into nodes. In many cases, documents
    tend to be very large, so we need to break <html:a id="_idIndexMarker225"></html:a>them
    down into smaller units called nodes. Working at this granular level allows for
    better handling of our content while maintaining an accurate <html:a id="_idIndexMarker226"></html:a>representation
    of its internal structure. This is the basic mechanism that LlamaIndex uses to
    manage our proprietary data content <html:span class="No-Break">more easily.</html:span></html:p>
    <html:p>Now is the <html:a id="_idIndexMarker227"></html:a>time to understand
    how nodes can be generated in LlamaIndex and what customization opportunities
    we have along the way. In the previous chapter, we talked about how to manually
    create nodes. But that was merely a way to simplify the explanation and help you
    better understand their mechanics. In a real application, most likely, we will
    want to use some automatic methods to generate them from the ingested documents.
    So, that’s what we’ll focus on <html:span class="No-Break">going forward.</html:span></html:p>
    <html:p>In this section, we will discover different ways of chunking a document.
    We’ll start by understanding <html:a id="_idIndexMarker228"></html:a>simple <html:strong
    class="bold">text splitters</html:strong> – which operate on raw text – and then
    we’ll cover <html:a id="_idIndexMarker229"></html:a>the more advanced <html:strong
    class="bold">node parsers</html:strong> – which are capable of interpreting more
    complex formats and following the document structure when extracting <html:span
    class="No-Break">the nodes.</html:span></html:p> <html:a id="_idTextAnchor078"></html:a><html:h2
    id="_idParaDest-79">Understanding the simple text splitters</html:h2> <html:p><html:strong
    class="bold">Text splitters</html:strong> <html:em class="italic">break down</html:em>
    the document into smaller pieces operating at the raw text level. They are useful
    when the content has a <html:em class="italic">flat</html:em> structure and does
    not come in a <html:span class="No-Break">specific format.</html:span></html:p>
    <html:p>To run the <html:a id="_idIndexMarker230"></html:a>following examples,
    make sure you add the necessary imports and the document reading logic, using
    <html:code class="literal">FlatReader</html:code> , at the beginning of your code
    for <html:span class="No-Break">all examples:</html:span></html:p> <html:p>Also,
    if you want to see the actual nodes generated by the code, you can add something
    like this <html:em class="italic">after</html:em> running <html:span class="No-Break">the
    parsers:</html:span></html:p> <html:p>Alright. Let’s see what’s in store in the
    <html:em class="italic">text</html:em> <html:span class="No-Break"><html:em class="italic">splitter</html:em></html:span>
    <html:span class="No-Break">category.</html:span></html:p> <html:h3>SentenceSplitter</html:h3>
    <html:p>This one splits text <html:a id="_idIndexMarker231"></html:a>while maintaining
    sentence boundaries, providing <html:a id="_idIndexMarker232"></html:a>nodes containing
    groups of sentences. You saw an example of using this parser in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 3</html:em></html:span></html:a>
    , <html:em class="italic">Kickstarting Your Journey with LlamaIndex</html:em>
    , in the <html:em class="italic">Automatically extracting nodes from documents
    using</html:em> <html:span class="No-Break"><html:em class="italic">splitters</html:em></html:span>
    <html:span class="No-Break">section.</html:span></html:p> <html:h3>TokenTextSplitter</html:h3>
    <html:p>This splitter <html:a id="_idIndexMarker233"></html:a>breaks down text
    while respecting sentence <html:a id="_idIndexMarker234"></html:a>boundaries to
    create suitable nodes for further natural language processing. It operates at
    the <html:span class="No-Break">token level.</html:span></html:p> <html:p>A typical
    usage in code would look <html:span class="No-Break">like this:</html:span></html:p>
    <html:p>Here are some notes on the parameters of <html:span class="No-Break">this
    splitter:</html:span></html:p> <html:ul><html:li><html:code class="literal">chunk_size</html:code>
    : This sets the maximum number of tokens for <html:span class="No-Break">each
    chunk</html:span></html:li> <html:li><html:code class="literal">chunk_overlap</html:code>
    : This defines the overlap in tokens between <html:span class="No-Break">consecutive
    chunks</html:span></html:li> <html:li><html:code class="literal">separator</html:code>
    : This is used to determine the primary <html:span class="No-Break">token boundary</html:span></html:li>
    <html:li><html:code class="literal">backup_separators</html:code> : These can
    be used for additional splitting points if the primary separator doesn’t split
    the <html:span class="No-Break">text sufficiently</html:span></html:li></html:ul>
    <html:h3>CodeSplitter</html:h3> <html:p>This smart <html:a id="_idIndexMarker235"></html:a>splitter
    knows how to interpret source code. It splits text based on <html:a id="_idIndexMarker236"></html:a>programming
    language and is ideal for managing technical documentation or source code. Before
    running the example, make sure you install the <html:span class="No-Break">necessary
    libraries:</html:span></html:p> <html:p>Let’s have a look at an example of how
    to use this splitter in <html:span class="No-Break">your code:</html:span></html:p>
    <html:p>As you <html:a id="_idIndexMarker237"></html:a>can see, there are several
    parameters you can tune with <html:a id="_idIndexMarker238"></html:a><html:span
    class="No-Break">this splitter:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">language</html:code> : This specifies the language of <html:span
    class="No-Break">the code</html:span></html:li> <html:li><html:code class="literal">chunk_lines</html:code>
    : This defines the number of lines <html:span class="No-Break">per chunk</html:span></html:li>
    <html:li><html:code class="literal">chunk_lines_overlap</html:code> : This defines
    the lines overlap <html:span class="No-Break">between chunks</html:span></html:li>
    <html:li><html:code class="literal">max_chars</html:code> : This defines the maximum
    characters <html:span class="No-Break">per chunk</html:span></html:li></html:ul>
    <html:p class="callout-heading">Quick side note on CodeSplitter</html:p> <html:p
    class="callout">This splitter is <html:a id="_idIndexMarker239"></html:a>cleverly
    built around a concept called <html:strong class="bold">abstract syntax tree</html:strong>
    ( <html:strong class="bold">AST</html:strong> ). An AST is a key idea in computer
    science that’s mainly used in creating programs that translate or interpret code.
    It’s like a branching diagram that shows the basic structure of the code written
    in a programming language. Each point on the diagram represents a different part
    or piece of the code. Because of this splitter’s awareness of AST, when you’re
    splitting code, you keep related statements together as much as possible, which
    is vital when you need to maintain the logical flow of code to understand or process
    <html:span class="No-Break">it later.</html:span></html:p> <html:a id="_idTextAnchor079"></html:a><html:h2
    id="_idParaDest-80">Using more advanced node parsers</html:h2> <html:p>Text splitters
    <html:a id="_idIndexMarker240"></html:a>only provide basic logic for breaking
    down text, mostly by using simple rules. We also have more advanced tools for
    chunking text into nodes. These are designed to process various standard file
    formats or can be used for more specific types <html:span class="No-Break">of
    content.</html:span></html:p> <html:p>Before we continue, keep in mind that all
    the node parsers that we will discuss here are derived from the generic class
    called <html:code class="literal">NodeParser</html:code> . Each parser has various
    parameters that can be <html:a id="_idIndexMarker241"></html:a>configured according
    to the use case, but at the base, three common elements can be customized for
    <html:span class="No-Break">all parsers:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">include_metadata</html:code> : This determines whether the parser
    should take into account the metadata or not. By default, this is set <html:span
    class="No-Break">to</html:span> <html:span class="No-Break"><html:code class="literal">True</html:code></html:span></html:li>
    <html:li><html:code class="literal">Include_prev_next_rel</html:code> : This determines
    whether the parser should automatically include <html:strong class="bold">prev/next</html:strong>
    type relationships between nodes. Again, the default value <html:span class="No-Break">is</html:span>
    <html:span class="No-Break"><html:code class="literal">True</html:code></html:span></html:li>
    <html:li><html:code class="literal">Callback_manager</html:code> : This can <html:a
    id="_idIndexMarker242"></html:a>be used to define a specific <html:strong class="bold">callback
    function</html:strong> . These functions can be used for debugging, tracing, and
    cost analysis, among other functions. We will talk more about them in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 10</html:em></html:span></html:a>
    , <html:em class="italic">Prompt Engineering Guidelines and</html:em> <html:span
    class="No-Break"><html:em class="italic">Best Practices</html:em></html:span></html:li></html:ul>
    <html:p>Apart from these three general options, each parser provides specific
    parameters to customize. You can get a complete list of configurable parameters
    for each parser by consulting the <html:span class="No-Break">official documentation.</html:span></html:p>
    <html:p>Let’s explore the node parsers available <html:span class="No-Break">in
    LlamaIndex.</html:span></html:p> <html:h3>SentenceWindowNodeParser</html:h3> <html:p>Based
    on the simple <html:strong class="bold">SentenceSplitter</html:strong> , this
    parser <html:a id="_idIndexMarker243"></html:a>splits text into individual <html:a
    id="_idIndexMarker244"></html:a>sentences and also <html:a id="_idIndexMarker245"></html:a>includes
    a <html:em class="italic">window</html:em> of surrounding sentences in the metadata
    of each node. It is useful for building more context around each sentence. During
    the querying process, that context will be fed into the LLM and allow for better
    responses. We can use it <html:span class="No-Break">like this:</html:span></html:p>
    <html:p>For this <html:a id="_idIndexMarker246"></html:a>parser, three specific
    parameters <html:a id="_idIndexMarker247"></html:a>can <html:span class="No-Break">be
    customized:</html:span></html:p> <html:ul><html:li><html:code class="literal">Window_size</html:code>
    : This defines the number of sentences on each side to include in <html:span class="No-Break">the
    window</html:span></html:li> <html:li><html:code class="literal">window_metadata_key</html:code>
    : This defines the metadata key for the <html:span class="No-Break">window sentences</html:span></html:li>
    <html:li><html:code class="literal">original_text_metadata_key</html:code> : This
    defines the metadata key for the <html:span class="No-Break">original sentence</html:span></html:li></html:ul>
    <html:h3>LangchainNodeParser</html:h3> <html:p>If you prefer <html:a id="_idIndexMarker248"></html:a>using
    the LangChain splitters, this parser allows <html:a id="_idIndexMarker249"></html:a>using
    any text splitter from the Langchain collection, extending the parsing options
    offered <html:span class="No-Break">by LlamaIndex.</html:span></html:p> <html:p>As
    a prerequisite for the next example, you’ll have to install the <html:span class="No-Break"><html:code
    class="literal">LangChain</html:code></html:span> <html:span class="No-Break">library:</html:span></html:p>
    <html:p>Here’s a simple example of how to use <html:span class="No-Break">this
    parser:</html:span></html:p> <html:p class="callout-heading">A quick note on LangChain</html:p>
    <html:p class="callout">The LangChain framework is similar in purpose to LlamaIndex
    and provides a versatile toolkit specialized <html:a id="_idIndexMarker250"></html:a>in
    advanced natural language processing capabilities. Its collection of text segmentation,
    summarization, and language understanding models assist in splitting and digesting
    textual data into coherent chunks ready for indexing in a similar way to LlamaIndex.
    When dealing with large data sources requiring nuanced linguistic analysis, LangChain
    empowers users to finely control the breakdown and ingestion of text - ensuring
    context and clarity are retained for downstream retrieval and querying. As you
    can see, the two can complement each other in a RAG scenario. Want to know more?
    Check <html:span class="No-Break">out</html:span> <html:a><html:span class="No-Break">https://www.langchain.com/</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Let’s see what other
    parsers we <html:span class="No-Break">have available.</html:span></html:p> <html:h3>SimpleFileNodeParser</html:h3>
    <html:p>This one automatically decides which of the following three node parsers
    should be used based <html:a id="_idIndexMarker251"></html:a>on file types. It
    can automatically handle these file formats <html:a id="_idIndexMarker252"></html:a>and
    transform them into nodes, simplifying the process of interacting with various
    types <html:span class="No-Break">of content:</html:span></html:p> <html:p>You
    can simply rely on <html:code class="literal">FlatReader</html:code> to load the
    file into your <html:code class="literal">Document</html:code> object; <html:code
    class="literal">SimpleFileNodeParser</html:code> will know what to do <html:span
    class="No-Break">from there.</html:span></html:p> <html:h3>HTMLNodeParser</html:h3>
    <html:p>This parser uses <html:strong class="bold">Beautiful Soup</html:strong>
    to parse HTML files and convert them into nodes based on <html:a id="_idIndexMarker253"></html:a>selected
    HTML tags. This parser simplifies <html:a id="_idIndexMarker254"></html:a>the
    HTML file by extracting text from standard text elements and merging adjacent
    nodes of the same type. The parser can be used <html:span class="No-Break">like
    this:</html:span></html:p> <html:p>As you <html:a id="_idIndexMarker255"></html:a>can
    see, you have the option to customize <html:a id="_idIndexMarker256"></html:a>the
    HTML <html:code class="literal">tags</html:code> from where you want to <html:span
    class="No-Break">retrieve content.</html:span></html:p> <html:h3>MarkdownNodeParser</html:h3>
    <html:p>This parser <html:a id="_idIndexMarker257"></html:a>processes raw <html:em
    class="italic">markdown</html:em> text and generates nodes reflecting its structure
    and content. The markdown node parser divides the content into <html:a id="_idIndexMarker258"></html:a>nodes
    for each header encountered in the file and incorporates the header hierarchy
    into the metadata. Here’s how to <html:span class="No-Break">use</html:span> <html:span
    class="No-Break"><html:code class="literal">MarkdownNodeParser</html:code></html:span>
    <html:span class="No-Break">:</html:span></html:p> <html:h3>JSONNodeParser</html:h3>
    <html:p>This parser <html:a id="_idIndexMarker259"></html:a>is specialized in
    processing and querying <html:a id="_idIndexMarker260"></html:a>structured data
    in JSON format. In a similar way to the Markdown parser, the JSON parser can be
    used <html:span class="No-Break">like this:</html:span></html:p> <html:a id="_idTextAnchor080"></html:a><html:h2
    id="_idParaDest-81">Using relational parsers</html:h2> <html:p><html:strong class="bold">Relational
    parsers</html:strong> parse <html:a id="_idIndexMarker261"></html:a>information
    into nodes that are linked to <html:a id="_idIndexMarker262"></html:a>each other
    through relationships. Relationships add a whole new dimension to our data and
    allow for more advanced retrieval techniques in our <html:span class="No-Break">RAG
    workflow.</html:span></html:p> <html:h3>HierarchicalNodeParser</html:h3> <html:p>This
    parser organizes the nodes into hierarchies across multiple levels. It will generate
    a hierarchy <html:a id="_idIndexMarker263"></html:a>of nodes, starting with top-level
    nodes with larger section sizes, down to child nodes with smaller section sizes,
    where each <html:a id="_idIndexMarker264"></html:a>child node has a parent node
    with a larger section size ( <html:span class="No-Break"><html:em class="italic">Figure
    4</html:em></html:span> <html:em class="italic">.1</html:em> ). By default, the
    parser uses <html:code class="literal">SentenceSplitter</html:code> to chunk text.
    The node hierarchy looks <html:span class="No-Break">like this:</html:span></html:p>
    <html:ul><html:li><html:em class="italic">Level 1</html:em> : Section <html:span
    class="No-Break">size 2,048</html:span></html:li> <html:li><html:em class="italic">Level
    2</html:em> : Section <html:span class="No-Break">size 512</html:span></html:li>
    <html:li><html:em class="italic">Level 3</html:em> : Section <html:span class="No-Break">size
    128</html:span></html:li></html:ul> <html:p>The top-level nodes, with larger sections,
    can provide high-level summaries, while the lower nodes can allow for a more detailed
    analysis of text sections. Have a look at <html:span class="No-Break"><html:em
    class="italic">Figure 4</html:em></html:span> <html:em class="italic">.2</html:em>
    for a visual representation of <html:span class="No-Break">this concept:</html:span></html:p>
    <html:p class="IMG---Caption" lang="en-US">Figure 4.2 – Hierarchical nodes of
    2,048, 512, and 128 chunk sizes</html:p> <html:p>In this way, the different node
    levels can be used to adjust the accuracy and depth of search results, allowing
    users to find information at different granularity levels. Here’s an example of
    how to use this parser in <html:span class="No-Break">your code:</html:span></html:p>
    <html:p>There <html:a id="_idIndexMarker265"></html:a>are two <html:a id="_idIndexMarker266"></html:a>specific
    parameters to customize for <html:span class="No-Break">this parser:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">chunk_sizes</html:code> : The values
    in this list define your hierarchy levels based on <html:span class="No-Break">content
    size</html:span></html:li> <html:li><html:code class="literal">chunk_overlap</html:code>
    : This defines the overlap size <html:span class="No-Break">between chunks</html:span></html:li></html:ul>
    <html:h3>UnstructuredElementNodeParser</html:h3> <html:p>I left this one last
    because it is used for more special situations. Sometimes, our documents <html:a
    id="_idIndexMarker267"></html:a>may include a mix of text and data tables, which
    can make parsing inefficient by <html:span class="No-Break">conventional methods.</html:span></html:p>
    <html:p>This parser <html:a id="_idIndexMarker268"></html:a>can process and split
    these documents into interpretable nodes, distinguishing between text sections
    and other embedded structures such as tables. We’ll talk about it in more detail
    toward the end of <html:span class="No-Break">this chapter.</html:span></html:p>
    <html:a id="_idTextAnchor081"></html:a><html:h2 id="_idParaDest-82">Confused about
    node parsers and text splitters?</html:h2> <html:p>You may have noticed that I
    use the two terms quite loosely. Categorizing parsing modules into <html:a id="_idIndexMarker269"></html:a>these
    two groups might initially cause some confusion. To simplify, a node <html:a id="_idIndexMarker270"></html:a>parser
    is a more sophisticated mechanism than a simple splitter. While both serve the
    same basic function and operate at different levels of complexity, they differ
    in <html:span class="No-Break">their implementations.</html:span></html:p> <html:p>Text
    splitters such as <html:code class="literal">SentenceSplitter</html:code> can
    divide long flat texts into nodes, based on certain rules or limitations, such
    as <html:strong class="bold">chunk_size</html:strong> or <html:strong class="bold">chunk_overlap</html:strong>
    . The nodes could represent lines, paragraphs, or sentences, and may also include
    additional metadata or links to the <html:span class="No-Break">original document.</html:span></html:p>
    <html:p>Node parsers <html:a id="_idIndexMarker271"></html:a>are more sophisticated
    and can involve additional data <html:a id="_idIndexMarker272"></html:a>processing
    logic. Beyond simply dividing text into nodes, they can perform extra tasks, such
    as analyzing the structure of HTML or JSON files and producing nodes enriched
    with <html:span class="No-Break">contextual information.</html:span></html:p>
    <html:a id="_idTextAnchor082"></html:a><html:h2 id="_idParaDest-83">Understanding
    chunk_size and chunk_overlap</html:h2> <html:p>As you have probably understood
    by now, text splitters are a basic but important component. They <html:a id="_idIndexMarker273"></html:a>control
    how text in documents gets split into nodes during parsing. For each text splitter
    type, LlamaIndex provides several parameters to customize the <html:a id="_idIndexMarker274"></html:a>text
    <html:span class="No-Break">splitting behavior.</html:span></html:p> <html:p>Probably
    two of the most important parameters for a text splitter are <html:code class="literal">chunk_size</html:code>
    and <html:code class="literal">chunk_overlap</html:code> . The text splitters
    themselves, such as <html:code class="literal">SentenceSplitter</html:code> ,
    <html:code class="literal">TokenTextSplitter</html:code> , <html:code class="literal">TextSplitter</html:code>
    , and others, take in the <html:code class="literal">chunk_size</html:code> and
    <html:code class="literal">chunk_overlap</html:code> arguments to control how
    they break text into smaller chunks during node creation. <html:code class="literal">chunk_size</html:code>
    controls the maximum length of text chunks in nodes. This is useful for ensuring
    nodes don’t take long for the LLM to process. Note that in LlamaIndex, the default
    <html:code class="literal">chunk_size</html:code> is 1,024, while the default
    <html:code class="literal">chunk_overlap</html:code> <html:span class="No-Break">is
    20.</html:span></html:p> <html:p><html:strong class="bold">chunk size</html:strong>
    is an important setting when building an RAG system. If chunks are too small,
    important context may be lost and the quality of the LLM response will be lower.
    On the other hand, large chunks increase the size of the prompts, increasing both
    computational cost and response generation time. An experimental approach was
    used when the default values were selected for <html:span class="No-Break">LlamaIndex:</html:span>
    <html:a><html:span class="No-Break">https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p><html:code class="literal">chunk_overlap</html:code>
    creates overlapping nodes by re-including some tokens from the previous Node.
    This helps provide context so that the LLM can understand the continuity of ideas
    when processing <html:span class="No-Break">adjacent Nodes.</html:span></html:p>
    <html:p><html:span class="No-Break"><html:em class="italic">Figure 4</html:em></html:span>
    <html:em class="italic">.3</html:em> provides a visual representation of <html:span
    class="No-Break">this concept:</html:span></html:p> <html:p class="IMG---Caption"
    lang="en-US">Figure 4.3 – chunk_size and chunk_overlap explained</html:p> <html:p>The
    concept <html:a id="_idIndexMarker275"></html:a>is similar to the way <html:code
    class="literal">SentenceWindowNodeParser</html:code> works – that is, it <html:a
    id="_idIndexMarker276"></html:a>extracts a <html:em class="italic">window</html:em>
    of context for each sentence. For example, with <html:code class="literal">chunk_size=100</html:code>
    and <html:code class="literal">chunk_overlap=10</html:code> , let’s say we had
    the <html:span class="No-Break">following text:</html:span></html:p> <html:p><html:em
    class="italic">Gardening is not only a relaxing hobby but also an art form. Cultivating
    plants, designing landscapes, and nurturing nature bring a sense of accomplishment.
    Many find it therapeutic and rewarding, especially when they see their</html:em>
    <html:span class="No-Break"><html:em class="italic">garden flourish</html:em></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>It would get split
    into the <html:span class="No-Break">following areas:</html:span></html:p> <html:ul><html:li><html:em
    class="italic">Node 1 (first 100 characters)</html:em> : “Gardening is not only
    a relaxing hobby but also an art form. Cultivating plants, designing <html:span
    class="No-Break">landscapes, an”</html:span></html:li> <html:li><html:em class="italic">Node
    2 (starts from the 75th character, next 100 characters)</html:em> : “designing
    landscapes, and nurturing nature bring a sense of accomplishment. Many find it
    therapeutic <html:span class="No-Break">and re”</html:span></html:li> <html:li><html:em
    class="italic">Node 3 (starts from the 150th character to the end of the text)</html:em>
    : “Many find it therapeutic and rewarding, especially when they see their <html:span
    class="No-Break">garden flourish.”</html:span></html:li></html:ul> <html:p>In
    this setup, the overlap between node 1 and node 2 is “ <html:code class="literal">designing</html:code>
    landscapes, an,” whereas the overlap between node 2 and node 3 is “Many find it
    therapeutic <html:span class="No-Break">and re.”</html:span></html:p> <html:p>These
    overlaps mean that one node re-includes parts of the previous node. This mechanism
    ensures continuity and context between the chunks, making each part more meaningful
    when <html:a id="_idIndexMarker277"></html:a>read in sequence. Of course, choosing
    the right values for <html:a id="_idIndexMarker278"></html:a>these two parameters
    is very important. The biggest impact will be on creating vector indexes. We’ll
    talk about them later, during <html:a><html:span class="No-Break"><html:em class="italic">Chapter
    5</html:em></html:span></html:a> , <html:em class="italic">Indexing</html:em>
    <html:span class="No-Break"><html:em class="italic">with LlamaIndex</html:em></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Next, let’s have a
    quick overview of <html:span class="No-Break">node relationships.</html:span></html:p>
    <html:a id="_idTextAnchor083"></html:a><html:h2 id="_idParaDest-84">Including
    relationships with include_prev_next_rel</html:h2> <html:p>Let’s talk a bit <html:a
    id="_idIndexMarker279"></html:a>about another important parameter that can dictate
    the behavior of our parsers: the <html:code class="literal">include_prev_next_rel</html:code>
    option. When set to <html:code class="literal">True</html:code> , this option
    makes the parser automatically add <html:code class="literal">NEXT</html:code>
    and <html:code class="literal">PREVIOUS</html:code> relationships between consecutive
    nodes. Here’s <html:span class="No-Break">an example:</html:span></html:p> <html:p>This
    helps capture the sequencing between nodes. Then, later when querying, you can
    optionally retrieve previous or next nodes for more context using features such
    as <html:code class="literal">PrevNextNodePostprocessor</html:code> . More on
    that in <html:a><html:span class="No-Break"><html:em class="italic">Chapter 6</html:em></html:span></html:a>
    , <html:em class="italic">Querying Our Data, Part 1 –</html:em> <html:span class="No-Break"><html:em
    class="italic">Context Retrieval</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>The relationships get added to the <html:code class="literal">.relationships</html:code>
    dictionary on <html:span class="No-Break">each node.</html:span></html:p> <html:p>So,
    node 1 would now be <html:span class="No-Break">as follows:</html:span></html:p>
    <html:p>Node 2 would be <html:span class="No-Break">as follows:</html:span></html:p>
    <html:p>Capturing these sequences helps provide contextual continuity in long
    documents and brings a lot of other benefits that I listed in more detail in the
    <html:span class="No-Break">previous chapter.</html:span></html:p> <html:p>Among
    other things, having a previous-next relationship enables <html:em class="italic">cluster
    retrieval</html:em> : you can get a cluster of related nodes by following the
    relationships to fetch nearby connected nodes. This provides a more focused context
    instead of randomly scattered nodes. Maintaining a <html:a id="_idIndexMarker280"></html:a>cohesive
    narrative thread through the content when following a story or a dialogue is another
    good reason for having these relationships <html:span class="No-Break">between
    nodes.</html:span></html:p> <html:p>Next, let’s have a look at how to use these
    parsers and splitters in <html:span class="No-Break">our workflow.</html:span></html:p>
    <html:a id="_idTextAnchor084"></html:a><html:h2 id="_idParaDest-85">Practical
    ways of using these node creation models</html:h2> <html:p>How you implement the
    node parsers or text splitters in your code depends on how much you <html:a id="_idIndexMarker281"></html:a>want
    to customize the process but, in the end, it all comes down to three <html:span
    class="No-Break">main options:</html:span></html:p> <html:ol><html:li>Using them
    standalone by calling from llama_index.core import Document from llama_index.core.node_parser
    import SentenceWindowNodeParser doc = Document(     text="Sentence 1\. Sentence
    2\. Sentence 3." ) parser = SentenceWindowNodeParser.from_defaults(     window_size=2  ,
        window_metadata_key="ContextWindow",     original_text_metadata_key="node_text"
    ) nodes = parser.get_nodes_from_documents([doc]) Node ID: 0715876a-61e6-4e77-95ba-b93e10de1c67
    Text: Sentence 2. {''ContextWindow'': ''Sentence 1.  Sentence 2.  Sentence 3.'',
    ''node_text'': ''Sentence 2\. ''} <html:code class="literal">get_nodes_from_documents()</html:code>
    , like in <html:span class="No-Break">this example:</html:span> <html:p class="list-inset">This
    code will produce three nodes. If we look at the second node, for example, by
    running <html:code class="literal">print(nodes[1])</html:code> , we’ll get the
    <html:span class="No-Break">following output:</html:span></html:p> <html:p class="list-inset">As
    you can see, the parser extracted the second sentence and allocated a random ID
    to the node. But if we take a peek at the node’s metadata by running <html:code
    class="literal">print(nodes[1].metadata)</html:code> , we’ll also see the context
    it gathered, using the keys <html:span class="No-Break">we specified:</html:span></html:p>
    <html:p class="list-inset">This metadata <html:a id="_idIndexMarker282"></html:a>can
    later be used when building queries to provide more context for each sentence
    and improve the <html:span class="No-Break">LLM responses.</html:span></html:p>
    <html:p class="list-inset">We’ll explore this in more detail during <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 6</html:em></html:span></html:a>
    , <html:em class="italic">Querying Our Data, Part 1 –</html:em> <html:span class="No-Break"><html:em
    class="italic">Context Retrieval</html:em></html:span> <html:span class="No-Break">.</html:span></html:p></html:li>
    <html:li>Configuring them from llama_index.core import Settings, Document,     VectorStoreIndex
    from llama_index.core.node_parser import     SentenceWindowNodeParser doc = Document(
        text="Sentence 1\. Sentence 2\. Sentence 3." ) text_splitter = SentenceWindowNodeParser.from_defaults(
        window_size=2  ,     window_metadata_key="ContextWindow",     original_text_metadata_key="node_text"
    ) Settings.text_splitter = text_splitter index = VectorStoreIndex.from_documents([doc])
    <html:span class="No-Break">in</html:span> <html:span class="No-Break"><html:code
    class="literal">Settings</html:code></html:span> <html:span class="No-Break">.</html:span>
    <html:p class="list-inset">The second option is a bit more general and convenient
    when you need to automatically use the same parser for multiple purposes in <html:span
    class="No-Break">your app:</html:span></html:p> <html:p class="list-inset">This
    time, after we define and configure our custom <html:code class="literal">text_splitter</html:code>
    , we pre-load it in <html:code class="literal">Settings</html:code> . From this
    point forward, whenever we call any function that relies on text splitting, our
    custom <html:code class="literal">text_splitter</html:code> will be used <html:span
    class="No-Break">by default.</html:span></html:p> <html:p class="list-inset">Of
    course, this actual example is a bit of overkill. You’ve probably noticed that
    I’ve used a node <html:a id="_idIndexMarker283"></html:a>parser in place of a
    simple text splitter. The index we’re building with the nodes won’t benefit in
    any way from the additional context metadata created by the parser. I just wanted
    to emphasize my previous point regarding parsers <html:span class="No-Break">and
    splitters.</html:span></html:p></html:li> <html:li>Defining the parsers as a <html:strong
    class="bold">transformation</html:strong> step in an <html:span class="No-Break"><html:strong
    class="bold">ingestion pipeline</html:strong></html:span> <html:span class="No-Break">.</html:span>
    <html:p class="list-inset">An ingestion pipeline is an automatic and structured
    process for ingesting data. It’s running the <html:a id="_idIndexMarker284"></html:a>data
    through a series of steps (called <html:strong class="bold">transformations</html:strong>
    ) one <html:span class="No-Break">by one.</html:span></html:p> <html:p class="list-inset">I
    will explain how this works and what it can be used for later in this chapter,
    in the <html:em class="italic">Using the ingestion pipeline to increase efficiency</html:em>
    section. You’ll also get to see the code for implementing the parser as a transformation
    in <html:span class="No-Break">the pipeline.</html:span></html:p></html:li></html:ol>
    <html:p>Next, we’ll talk about metadata and how metadata can be used to improve
    our <html:span class="No-Break">RAG application.</html:span></html:p> <html:a
    id="_idTextAnchor085"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Working
    with metadata to improve the context</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-86">Working with metadata to improve
    the context</html:h1> <html:div id="_idContainer039">document.metadata = {     "report_name":
    "Sales Report April 2022",     "department": "Sales",     "author": "Jane Doe"
    } From llama_index.core import SimpleDirectoryReader from llama_index.core.node_parser
    import SentenceSplitter reader = SimpleDirectoryReader(''files'') documents =
    reader.load_data() parser = SentenceSplitter(include_prev_next_rel=True) nodes
    = parser.get_nodes_from_documents(documents) from llama_index.core.extractors
    import SummaryExtractor summary_extractor = SummaryExtractor(summaries=["prev",
    "self",     "next"]) metadata_list = summary_extractor.extract(nodes) print(metadata_list)
    from llama_index.core.extractors import QuestionsAnsweredExtractor qa_extractor
    = QuestionsAnsweredExtractor(questions=5) metadata_list = qa_extractor.extract(nodes)
    print(metadata_list) from llama_index.core.extractors import TitleExtractor title_extractor
    = TitleExtractor () metadata_list = title_extractor.extract(nodes) print(metadata_list)
    from llama_index.core.extractors import EntityExtractor entity_extractor = EntityExtractor
    (     label_entities = True,     device = "cpu" ) metadata_list = entity_extractor.extract(nodes)
    print(metadata_list) from llama_index.core.extractors import KeywordExtractor
    key_extractor = KeywordExtractor (keywords=3) metadata_list = key_extractor.extract(nodes)
    print(metadata_list) from llama_index.core.extractors import BaseExtractor from
    typing import List, Dict class CustomExtractor(BaseExtractor):     async def aextract(self,
    nodes) -> List[Dict]:         metadata_list = [             {                 "node_length":  str(len(node.text))
                }             for node in nodes         ]         return metadata_list
    document.excluded_llm_metadata_keys = ["file_name"] document.excluded_embed_metadata_keys
    = ["file_name"] document.metadata_template = "{key}::{value}" print(document.get_content(metadata_mode=MetadataMode.LLM))
    <html:p>What is <html:strong class="bold">metadata</html:strong> ? It’s simply
    <html:a id="_idIndexMarker285"></html:a>additional information we can attach to
    our documents <html:a id="_idIndexMarker286"></html:a>and nodes. This extra context
    helps LlamaIndex better understand our data. It provides additional context about
    data and can be customized in terms of visibility <html:span class="No-Break">and
    format.</html:span></html:p> <html:p>For example, let’s say you’ve <html:em class="italic">ingested</html:em>
    some PDF reports as documents. You could then simply add some metadata <html:span
    class="No-Break">like this:</html:span></html:p> <html:p>This metadata gives vital
    clues when querying the data later. In this example, we can use it to locate reports
    by <html:a id="_idIndexMarker287"></html:a>department or author. You can store
    anything useful as metadata – categories, timestamps, locations, <html:span class="No-Break">and
    more.</html:span></html:p> <html:p>And here’s a neat trick – any metadata you
    set on a document automatically flows down to child nodes! So, if I set an <html:code
    class="literal">author</html:code> field on a document, all nodes derived from
    that document will inherit the <html:code class="literal">author</html:code> metadata.
    This propagation saves time and prevents duplicating metadata <html:span class="No-Break">across
    nodes.</html:span></html:p> <html:p>There are multiple ways of <html:span class="No-Break">defining
    metadata:</html:span></html:p> <html:ol><html:li>Setting the metadata values directly
    in the document = Document(     text="",     metadata={"author": "John Doe"} )
    <html:code class="literal">Document</html:code> constructor <html:span class="No-Break">as
    follows:</html:span></html:li> <html:li>Adding the metadata after document.metadata
    = {"category": "finance"} <html:span class="No-Break">document creation:</html:span></html:li>
    <html:li>Automatically setting metadata in the ingestion process, when using data
    connectors such def set_metadata(filename):     return {"file_name": filename}
    documents = SimpleDirectoryReader(     "./data",     file_metadata=set_metadata("file1.txt")
    ).load_data() <html:span class="No-Break">as</html:span> <html:span class="No-Break"><html:code
    class="literal">SimpleDirectoryReader</html:code></html:span> <html:span class="No-Break">:</html:span></html:li>
    <html:li>Using standalone, dedicated extractors provided by LlamaIndex. <html:strong
    class="bold">Metadata</html:strong> <html:strong class="bold">extractors</html:strong>
    are a <html:a id="_idIndexMarker288"></html:a>powerful way to generate relevant
    metadata from text using the power of LLMs. This extracted metadata can then be
    attached to documents and nodes to provide <html:span class="No-Break">additional
    context</html:span></html:li> <html:li>Defining the <html:a id="_idIndexMarker289"></html:a>extractor
    as a <html:em class="italic">transformation</html:em> step in an ingestion pipeline.
    Just like in the case of node parsers, extractors can also become part of the
    pipeline. We’ll cover this approach later in this chapter in the <html:em class="italic">Using
    the ingestion pipeline to increase</html:em> <html:span class="No-Break"><html:em
    class="italic">efficiency</html:em></html:span> <html:span class="No-Break">section</html:span></html:li></html:ol>
    <html:p>But first, let’s put our magnifying glass on these specialized metadata
    extractors to better understand how <html:span class="No-Break">they work.</html:span></html:p>
    <html:p>Before you go further, if you want to run the following code examples,
    make sure to include the necessary imports, document ingestion, and node parsing
    logic at the beginning of your code by adding the <html:span class="No-Break">following
    lines:</html:span></html:p> <html:p>This boilerplate code prepares your data –
    ingested from the <html:code class="literal">files</html:code> subfolder – and
    puts everything you need into <html:code class="literal">Nodes</html:code> . We’ll
    store our metadata in a variable called <html:code class="literal">metadata_list</html:code>
    . I’ve added <html:code class="literal">print(metadata_list)</html:code> at the
    end of each example so that we’ll see an output of the extracted metadata. Apart
    from describing their logic, I’ve also highlighted practical uses for each one
    of <html:span class="No-Break">the extractors.</html:span></html:p> <html:a id="_idTextAnchor086"></html:a><html:h2
    id="_idParaDest-87">SummaryExtractor</html:h2> <html:p>This extractor generates
    summaries of the text contained by the node. Optionally, it can generate <html:a
    id="_idIndexMarker290"></html:a>summaries for the previous and next adjacent nodes.
    Here’s <html:span class="No-Break">an example:</html:span></html:p> <html:p>This
    extractor <html:a id="_idIndexMarker291"></html:a>generates concise summaries
    for each node or adjacent node. These are essential during the retrieve phase
    in an RAG architecture. This ensures that the search can consider the summary
    of the documents without having to process the entirety of <html:span class="No-Break">their
    content.</html:span></html:p> <html:p class="callout-heading">Practical use case</html:p>
    <html:p class="callout">Imagine a customer support knowledge base on which <html:code
    class="literal">SummaryExtractor</html:code> can provide summaries of customer
    issues and resolutions. Then, when a new support request comes in, our app can
    retrieve the most relevant past cases to help generate a detailed and <html:span
    class="No-Break">contextual solution.</html:span></html:p> <html:p>You can customize
    the type of <html:code class="literal">summaries</html:code> to generate by setting
    the values in the summaries list and the actual prompt that will be used with
    the LLM by defining the prompt in the <html:span class="No-Break"><html:code class="literal">prompt_template</html:code></html:span>
    <html:span class="No-Break">parameter.</html:span></html:p> <html:a id="_idTextAnchor087"></html:a><html:h2
    id="_idParaDest-88">QuestionsAnsweredExtractor</html:h2> <html:p>This extractor
    <html:a id="_idIndexMarker292"></html:a>generates a specified number of questions
    <html:a id="_idIndexMarker293"></html:a>the node text <html:span class="No-Break">can
    answer.</html:span></html:p> <html:p>The following example should give you a <html:span
    class="No-Break">usage guideline:</html:span></html:p> <html:p>This extractor
    identifies questions that the text is uniquely positioned to answer, allowing
    the retrieval process to focus on nodes that explicitly address <html:span class="No-Break">specific
    inquiries.</html:span></html:p> <html:p class="callout-heading">Practical use
    case</html:p> <html:p class="callout">For an FAQ system, the extractor identifies
    unique questions answered by articles, making it easier to find precise answers
    to <html:span class="No-Break">user queries.</html:span></html:p> <html:p>You
    can <html:a id="_idIndexMarker294"></html:a>customize the number of questions
    it <html:a id="_idIndexMarker295"></html:a>generates but also the actual prompt
    that will be used with the LLM – by setting the <html:code class="literal">prompt_template</html:code>
    parameter. There is also an <html:code class="literal">embedding_only</html:code>
    Boolean parameter that – if set to <html:code class="literal">True</html:code>
    – will make the metadata available only for embeddings. More on that in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 5</html:em></html:span></html:a>
    , <html:em class="italic">Indexing</html:em> <html:span class="No-Break"><html:em
    class="italic">with LlamaIndex</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:a id="_idTextAnchor088"></html:a><html:h2 id="_idParaDest-89">TitleExtractor</html:h2>
    <html:p>This one <html:a id="_idIndexMarker296"></html:a>extracts a title for
    the text. Here’s <html:span class="No-Break">an example:</html:span></html:p>
    <html:p><html:code class="literal">TitleExtractor</html:code> specializes in pulling
    out meaningful titles from larger texts, assisting in the <html:a id="_idIndexMarker297"></html:a>quick
    identification and retrieval of documents. In digital libraries, for example,
    <html:code class="literal">TitleExtractor</html:code> can help categorize documents
    by extracting titles from untitled texts, making retrieval more efficient when
    titles are used as search keywords. There are several parameters you can tweak
    for <html:span class="No-Break">this extractor:</html:span></html:p> <html:ul><html:li><html:code
    class="literal">nodes</html:code> : This sets the <html:a id="_idIndexMarker298"></html:a>number
    of nodes to use for <html:span class="No-Break">title extraction</html:span></html:li>
    <html:li><html:code class="literal">node_template</html:code> : This changes the
    default prompt template that’s used for extracting <html:span class="No-Break">the
    titles</html:span></html:li> <html:li><html:code class="literal">combine_template</html:code>
    : This changes the prompt template for combining multiple <html:a id="_idIndexMarker299"></html:a>node-level
    titles in a <html:span class="No-Break">document-wide title</html:span></html:li></html:ul>
    <html:p>Now, let’s look <html:span class="No-Break">at</html:span> <html:span
    class="No-Break"><html:code class="literal">EntityExtractor</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:a id="_idTextAnchor089"></html:a><html:h2
    id="_idParaDest-90">EntityExtractor</html:h2> <html:p>This will <html:a id="_idIndexMarker300"></html:a>extract
    entities such as people, locations, organizations, and more from <html:a id="_idIndexMarker301"></html:a>the
    node text by using the <html:strong class="bold">span-marker</html:strong> package.
    This <html:a id="_idIndexMarker302"></html:a>package is installed automatically
    together with the <html:code class="literal">EntityExtractor</html:code> integration,
    so no <html:a id="_idIndexMarker303"></html:a>additional installations are required.
    It provides the ability to perform <html:strong class="bold">named entity recognition</html:strong>
    ( <html:strong class="bold">NER</html:strong> ) and relies on a <html:a id="_idIndexMarker304"></html:a>tokenizer
    provided by the <html:strong class="bold">Natural Language Toolkit</html:strong>
    ( <html:strong class="bold">NLTK</html:strong> ) <html:span class="No-Break">package:</html:span>
    <html:a><html:span class="No-Break">https://www.nltk.org/</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p class="callout-heading">A
    quick note on NER</html:p> <html:p class="callout">NER is a technique that’s used
    by computers to identify and label specific entities in text, such as people’s
    names, company names, places, and dates. This helps the computer to better understand
    the content and provides useful context in an <html:span class="No-Break">RAG
    scenario.</html:span></html:p> <html:p>Here’s a code example for using <html:span
    class="No-Break">this extractor:</html:span></html:p> <html:p>The extractor identifies
    named entities from the text, labels them, and adds them to the metadata, enabling
    a retrieval system to focus on nodes with <html:span class="No-Break">specific
    references.</html:span></html:p> <html:p class="callout-heading">Practical use
    case</html:p> <html:p class="callout">Imagine a legal document archive having
    this metadata attached to each node. This extractor <html:a id="_idIndexMarker305"></html:a>could
    ease the retrieval of documents mentioning particular people, locations, or organizations,
    thus providing the best context for <html:span class="No-Break">our query.</html:span></html:p>
    <html:p>There’s a <html:a id="_idIndexMarker306"></html:a>long list of parameters
    that you can tune for <html:span class="No-Break">this extractor:</html:span></html:p>
    <html:ul><html:li><html:code class="literal">model_name</html:code> : This sets
    the name of the model to be used <html:span class="No-Break">by</html:span> <html:span
    class="No-Break"><html:code class="literal">SpanMarker</html:code></html:span></html:li>
    <html:li><html:code class="literal">prediction_threshold</html:code> : This changes
    the default 0.5 minimum prediction threshold for named entities. As you may have
    guessed, entity recognition is usually not a 100% accurate process. However, you
    can experiment with different values here until you find the <html:span class="No-Break">best
    compromise</html:span></html:li> <html:li><html:code class="literal">span_joiner</html:code>
    : This changes the default string used to join <html:span class="No-Break">the
    spans</html:span></html:li> <html:li><html:code class="literal">label_entities</html:code>
    : If set to <html:code class="literal">True</html:code> , it will make the extractor
    label every entity name with an entity type. This could be useful later, in the
    retrieval and querying phase. By default, this is set <html:span class="No-Break">to</html:span>
    <html:span class="No-Break"><html:code class="literal">False</html:code></html:span></html:li>
    <html:li><html:code class="literal">device</html:code> : This controls the device
    on which the model runs. It defaults to <html:code class="literal">cpu</html:code>
    , but if your system allows, it can be set <html:span class="No-Break">to</html:span>
    <html:span class="No-Break"><html:code class="literal">cuda</html:code></html:span></html:li>
    <html:li><html:code class="literal">entity_map</html:code> : This allows you to
    customize the labels for each entity type. The extractor comes with a predefined
    entity map that includes labels for people, organizations, places, events, and
    <html:span class="No-Break">many others</html:span></html:li> <html:li><html:code
    class="literal">Tokenizer</html:code> : This allows you to change the default
    tokenizer function – which defaults to the <html:span class="No-Break">NLTK tokenizer</html:span></html:li></html:ul>
    <html:p>Now, let’s discuss how to extract keywords <html:span class="No-Break">with</html:span>
    <html:span class="No-Break"><html:code class="literal">KeywordExtractor</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:a id="_idTextAnchor090"></html:a><html:h2
    id="_idParaDest-91">KeywordExtractor</html:h2> <html:p>This extractor <html:a
    id="_idIndexMarker307"></html:a>extracts important keywords from the text. Let’s
    have a look at <html:span class="No-Break">an example:</html:span></html:p> <html:p>This
    one <html:a id="_idIndexMarker308"></html:a>identifies important words or phrases,
    making it an invaluable tool for retrieving the most relevant nodes based on <html:span
    class="No-Break">user queries.</html:span></html:p> <html:p class="callout-heading">Practical
    use case</html:p> <html:p class="callout">Integrating <html:code class="literal">KeywordExtractor</html:code>
    into a content recommendation engine can significantly enhance its effectiveness.
    By aligning the keywords extracted from content nodes with the terms used in user
    searches, the engine can more accurately match and recommend content that aligns
    with user interests. This keyword-based matching ensures that recommendations
    are not only relevant but also tailored to the specific inquiries or topics users
    <html:span class="No-Break">are exploring.</html:span></html:p> <html:p>You can
    customize the number of keywords it generates by changing the <html:code class="literal">keywords</html:code>
    parameter to a <html:span class="No-Break">specific value.</html:span></html:p>
    <html:a id="_idTextAnchor091"></html:a><html:h2 id="_idParaDest-92">PydanticProgramExtractor</html:h2>
    <html:p>This extractor <html:a id="_idIndexMarker309"></html:a>extracts metadata
    using a <html:strong class="bold">Pydantic</html:strong> structure. Have <html:a
    id="_idIndexMarker310"></html:a>a look here for a complete example of using this
    <html:span class="No-Break">extractor:</html:span> <html:a><html:span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/PydanticExtractor.html#pydantic-extractor</html:span></html:a>
    <html:span class="No-Break">.</html:span></html:p> <html:p>This <html:em class="italic">Swiss
    Army knife</html:em> enables the creation of complex and structured metadata schemas
    with a single LLM call by making use of Pydantic models. One of the main advantages
    it has over other extractors is that it can pull multiple fields of data using
    a single LLM call making it a very efficient way to extract metadata. This data
    will be nicely organized in a model of <html:span class="No-Break">our design.</html:span></html:p>
    <html:p class="callout-heading">A quick introduction to Pydantic models</html:p>
    <html:p class="callout">A <html:strong class="bold">Pydantic model</html:strong>
    is like a <html:a id="_idIndexMarker311"></html:a>blueprint or a set of rules
    that you define as a class in a Python program. It helps you make sure that the
    data you receive or work with follows certain rules and is in the right format.
    Think of it as a way to define how your data should look – Pydantic helps you
    enforce those rules and make sure the data fits in your <html:span class="No-Break">desired
    structure.</html:span></html:p> <html:p>For example, imagine you have a program
    that deals with user data such as names, ages, and email addresses. You can create
    a Pydantic model that specifies that a user’s name should <html:a id="_idIndexMarker312"></html:a>be
    a string, their age should be a number, and their <html:a id="_idIndexMarker313"></html:a>email
    address should be a valid email format. If input data doesn’t follow these rules,
    Pydantic will raise an error, telling you that the data is not correct. LlamaIndex
    embraces this mechanism whenever it needs to ensure the consistency and correctness
    of the data it handles, especially as it often works with complex structures and
    <html:span class="No-Break">interrelated data.</html:span></html:p> <html:a id="_idTextAnchor092"></html:a><html:h2
    id="_idParaDest-93">MarvinMetadataExtractor</html:h2> <html:p>This <html:a id="_idIndexMarker314"></html:a>extractor
    extracts metadata using the <html:strong class="bold">Marvin AI engineering framework</html:strong>
    <html:span class="P---URL"></html:span>(https://www.askmarvin.ai/). Taking advantage
    of the <html:a id="_idIndexMarker315"></html:a>Marvin AI engineering framework,
    this extractor is <html:a id="_idIndexMarker316"></html:a>capable of trustworthy
    and scalable metadata extraction and augmentation. Its sophistication lies in
    providing type-safe schemas for text – similar to Pydantic models - but also supporting
    business logic transformations. You can find a detailed example <html:span class="No-Break">here:</html:span>
    <html:span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/MarvinMetadataExtractorDemo.html</html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:a id="_idTextAnchor093"></html:a><html:h2
    id="_idParaDest-94">Defining your custom extractor</html:h2> <html:p>Just in case
    <html:a id="_idIndexMarker317"></html:a>none of these ready-made extractors satisfies
    <html:a id="_idIndexMarker318"></html:a>your needs, you can always define your
    own extractor function. Here is a simple example of how to define a <html:span
    class="No-Break">custom extractor:</html:span></html:p> <html:p>This basic <html:a
    id="_idIndexMarker319"></html:a>extractor measures the length in characters for
    <html:a id="_idIndexMarker320"></html:a>each node and saves these values in the
    metadata. Of course, you could replace that with any logic required by <html:span
    class="No-Break">your app.</html:span></html:p> <html:p>Having so many tools and
    methods available at our disposal is a great thing. But then a new question arises:
    <html:em class="italic">do we need that much metadata?</html:em> Let’s find out
    <html:span class="No-Break">the answer.</html:span></html:p> <html:a id="_idTextAnchor094"></html:a><html:h2
    id="_idParaDest-95">Is having all that metadata always a good thing?</html:h2>
    <html:p>Not necessarily. A key detail is that metadata gets injected into the
    text that’s sent to the LLM and <html:a id="_idIndexMarker321"></html:a>embedding
    model. This can potentially induce some bias in the models. This means that sometimes,
    you may not want all metadata to be visible. For example, filenames may help embeddings
    but may <html:em class="italic">distract</html:em> the LLM because the LLM might
    not understand them as filenames but as other entities instead, and also because
    the filenames may have no relevance in the context of the prompt. You can selectively
    hide metadata with the <html:span class="No-Break">following command:</html:span></html:p>
    <html:p>This hides <html:code class="literal">file_name</html:code> from the LLM.
    You can also hide metadata from embeddings if <html:span class="No-Break">you
    want:</html:span></html:p> <html:p>Also, you can customize the metadata format
    <html:span class="No-Break">like this:</html:span></html:p> <html:p>Here is a
    pro tip when dealing with metadata mode. LlamaIndex has an enum called <html:code
    class="literal">MetadataMode</html:code> that controls <html:span class="No-Break">metadata
    visibility:</html:span></html:p> <html:ul><html:li><html:code class="literal">MetadataMode.ALL</html:code>
    : Shows <html:span class="No-Break">all metadata</html:span></html:li> <html:li><html:code
    class="literal">MetadataMode.LLM</html:code> : Only metadata visible to <html:span
    class="No-Break">the LLM</html:span></html:li> <html:li><html:code class="literal">MetadataMode.EMBED</html:code>
    : Only metadata visible <html:span class="No-Break">to embeddings</html:span></html:li></html:ul>
    <html:p>You can test <html:a id="_idIndexMarker322"></html:a>the visibility of
    metadata with the <html:span class="No-Break">following command:</html:span></html:p>
    <html:p>So, in summary, metadata gives your data much-needed context. You have
    full control over its format and visibility to different models. These customizations
    let you mold metadata to match your <html:span class="No-Break">use case!</html:span></html:p>
    <html:p>With that topic exhausted, it’s time to talk <html:span class="No-Break">about
    money.</html:span></html:p> <html:a id="_idTextAnchor095"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Estimating
    the potential cost of using metadata extractors</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-96">Estimating the potential cost
    of using metadata extractors</html:h1> <html:div id="_idContainer039">from llama_index.core
    import ettings from llama_index.core.extractors import QuestionsAnsweredExtractor
    from llama_index.core.llms.mock import MockLLM from llama_index.core.schema import
    TextNode from llama_index.core.callbacks import (     CallbackManager,     TokenCountingHandler
    ) llm = MockLLM(max_tokens=256) counter = TokenCountingHandler(verbose=False)
    callback_manager = CallbackManager([counter]) Settings.llm = llm Settings.callback_manager
    = CallbackManager([counter]) sample_text = (     "LlamaIndex is a powerful tool
    used "     "to create efficient indices from data." ) nodes= [TextNode(text=sample_text)]
    extractor = QuestionsAnsweredExtractor(     show_progress=False ) Questions_metadata
    = extractor.extract(nodes) print(f"Prompt Tokens: {counter.prompt_llm_token_count}")
    print(f"Completion Tokens: {counter.completion_llm_token_count}") print(f"Total
    Token Count: {counter.total_llm_token_count}") <html:p>A key consideration when
    utilizing the various metadata extractors in LlamaIndex is the associated <html:a
    id="_idIndexMarker323"></html:a>LLM compute costs. As mentioned earlier, most
    of these extractors rely on LLMs under the hood to analyze text and generate <html:span
    class="No-Break">descriptive metadata.</html:span></html:p> <html:p>Repeatedly
    calling LLMs to process large volumes of text can quickly add up in charges. For
    example, if you are extracting summaries and keywords from thousands of document
    nodes using <html:code class="literal">SummaryExtractor</html:code> and <html:code
    class="literal">KeywordExtractor</html:code> , those constant LLM invocations
    will carry a <html:span class="No-Break">significant cost.</html:span></html:p>
    <html:a id="_idTextAnchor096"></html:a><html:h2 id="_idParaDest-97">Follow these
    simple best practices to minimize your costs</html:h2> <html:p>Let’s talk <html:a
    id="_idIndexMarker324"></html:a>about some common best practices for minimizing
    your <html:span class="No-Break">LLM costs:</html:span></html:p> <html:ul><html:li>Batch
    content into fewer LLM calls instead of individual calls per node. This amortizes
    the overhead because you consume fewer tokens compared to multiple separate calls.
    Using the Pydantic extractor is very useful for this purpose since it generates
    multiple fields in a single <html:span class="No-Break">LLM call</html:span></html:li>
    <html:li>Use cheaper LLM models with lower compute requirements if full accuracy
    is not necessary. However, be careful – you may introduce errors in your data,
    and these errors have the bad habit of propagating and <html:span class="No-Break">amplifying
    downstream</html:span></html:li> <html:li>Cache previous extractions and reuse
    them without having to re-invoke LLMs every time. I’m going to show you how to
    accomplish that using <html:em class="italic">ingestion pipelines</html:em> later
    in this chapter, in the <html:em class="italic">Using the ingestion pipeline to
    increase</html:em> <html:span class="No-Break"><html:em class="italic">efficiency</html:em></html:span>
    <html:span class="No-Break">section</html:span></html:li> <html:li>Restrict metadata
    extraction only to select subsets of critical nodes rather than full coverage.
    This may be difficult to implement in an <html:span class="No-Break">automated
    scenario</html:span></html:li> <html:li>Consider offline LLMs to eliminate cloud
    costs. Depending on your hardware, this may or may not be <html:span class="No-Break">a
    solution</html:span></html:li></html:ul> <html:p>While these guidelines should
    help you greatly reduce the extraction costs, it’s still a good idea to make sure
    you run some estimates before processing <html:span class="No-Break">large datasets.</html:span></html:p>
    <html:a id="_idTextAnchor097"></html:a><html:h2 id="_idParaDest-98">Estimate your
    maximal costs before running the actual extractors</html:h2> <html:p>Here is a
    <html:a id="_idIndexMarker325"></html:a>basic example of how we can estimate LLM
    <html:a id="_idIndexMarker326"></html:a>costs by using a <html:strong class="bold">MockLLM</html:strong>
    before running the extractor on the <html:span class="No-Break">real one:</html:span></html:p>
    <html:p>You’ll notice that we’re using some specialized tools to run the actual
    estimation. Let’s have a quick overview of the code. <html:code class="literal">MockLLM</html:code>
    – as its name implies – is a stand-in LLM that simulates the behavior of an LLM
    without any actual <html:span class="No-Break">API calls.</html:span></html:p>
    <html:p>When you create a <html:code class="literal">MockLLM</html:code> instance,
    you have the option to set a <html:code class="literal">max_tokens</html:code>
    parameter. This parameter represents the maximum number of tokens that the mock
    model is <html:a id="_idIndexMarker327"></html:a>supposed to generate for any
    given prompt, mirroring the behavior you’d expect from a real language model –
    but without actually generating any <html:span class="No-Break">meaningful content.</html:span></html:p>
    <html:p class="callout-heading">How does the max_token parameter work?</html:p>
    <html:p class="callout">The goal here is to predict a <html:em class="italic">worst-case</html:em>
    scenario, but your actual cost will vary depending <html:a id="_idIndexMarker328"></html:a>on
    the LLM response size and in most regular scenarios should be lower than the <html:code
    class="literal">max_tokens</html:code> value. It’s still a very useful tool because
    it helps you understand how different metadata extraction strategies applied to
    different datasets can affect your total cost. For metadata extraction, this total
    cost will depend on the prompt and response size multiplied by the total number
    of calls the <html:span class="No-Break">extractor performs.</html:span></html:p>
    <html:p><html:strong class="bold">CallbackManager</html:strong> is a debugging
    <html:a id="_idIndexMarker329"></html:a>mechanism that’s implemented in LlamaIndex
    that we will cover in more detail in <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 10</html:em></html:span></html:a> , <html:em class="italic">Prompt
    Engineering Guidelines and Best Practices</html:em> . In our example, <html:code
    class="literal">CallbackManager</html:code> is used in <html:a id="_idIndexMarker330"></html:a>combination
    with the <html:strong class="bold">TokenCountingHandler</html:strong> module,
    which is specialized in counting the tokens that are used for various operations
    involving an LLM. When defining <html:code class="literal">TokenCountingHandler</html:code>
    , you can also specify a <html:span class="No-Break"><html:code class="literal">tokenizer</html:code></html:span>
    <html:span class="No-Break">parameter.</html:span></html:p> <html:p class="callout-heading">What
    is the tokenizer and why do we need it?</html:p> <html:p class="callout">The <html:strong
    class="bold">tokenizer</html:strong> is responsible for <html:em class="italic">tokenization</html:em>
    of the text – that is, converting it into tokens – since LLMs work <html:a id="_idIndexMarker331"></html:a>with
    tokens and also measure their usage using tokens. When running a cost prediction
    for a specific prompt on a specific LLM, it’s important to use a tokenizer that
    is compatible with that specific LLM. Each LLM is often trained with a particular
    tokenizer, which determines how the text is split into tokens. Using the correct
    tokenizer is important if you want to make more accurate cost predictions. By
    default, LlamaIndex uses the <html:code class="literal">CL100K</html:code> tokenizer,
    which is specific for GPT-4\. So, if you plan on using other LLMs, you may want
    to customize the tokenizer. More on this topic and on how we can optimize the
    costs of our RAG app will be covered in <html:a><html:span class="No-Break"><html:em
    class="italic">Chapter 10</html:em></html:span></html:a> , <html:em class="italic">Prompt
    Engineering Guidelines and</html:em> <html:span class="No-Break"><html:em class="italic">Best
    Practices</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>Going back to our example, what happens under the hood is that when we
    run the extractor, it uses <html:code class="literal">MockLLM</html:code> – so,
    everything stays locally. Then, <html:code class="literal">TokenCountingHandler</html:code>
    <html:em class="italic">intercepts</html:em> both the prompt and the response
    from this <html:code class="literal">MockLLM</html:code> and counts the actual
    number of <html:span class="No-Break">tokens used.</html:span></html:p> <html:p>We
    will discuss a similar mechanism that can be used for estimating the costs of
    generating <html:a id="_idIndexMarker332"></html:a>certain types of Indexes and
    running queries later in <html:em class="italic">Chapters 5</html:em> <html:span
    class="No-Break">and</html:span> <html:span class="No-Break"><html:em class="italic">6</html:em></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>In this example, I’ve
    shown you how to estimate the cost for only one type of extractor, <html:code
    class="literal">QuestionsAnsweredExtractor</html:code> . If you need to estimate
    the individual cost for more than one extractor in the same run, you can use the
    <html:code class="literal">token_counter.reset_counts()</html:code> method to
    reset the counters to zero before running the next <html:span class="No-Break">extraction
    round.</html:span></html:p> <html:p class="callout-heading">The main lesson of
    this section</html:p> <html:p class="callout">While rich metadata unlocks many
    capabilities, overuse without conscious optimization can negatively impact operating
    costs and ruin your day. Make sure you take that into account. Apply best practices
    to minimize the costs and always estimate before running extractors on <html:span
    class="No-Break">large datasets.</html:span></html:p> <html:p>Next, let’s talk
    about another very important aspect to consider <html:span class="No-Break">data
    privacy.</html:span></html:p> <html:a id="_idTextAnchor098"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Preserving
    privacy with metadata extractors, and not only</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-99">Preserving privacy with metadata
    extractors, and not only</html:h1> <html:div id="_idContainer039">pip install
    llama-index-llms-huggingface from llama_index.core.postprocessor import NERPIINodePostprocessor
    from llama_index.llms.huggingface import HuggingFaceLLM from llama_index.core.schema
    import NodeWithScore, TextNode original = (     "Dear Jane Doe. Your address has
    been recorded in "     "our database. Please confirm it is valid: 8804 Vista "
        "Serro Dr. Cabo Robles, California(CA)." ) node = TextNode(text=original)
    processor = NERPIINodePostprocessor() clean_nodes = processor.postprocess_nodes(
        [NodeWithScore(node=node)] ) print(clean_nodes[0].node.get_text()) Dear [PER_5].
    Your address has been recorded in our database. Please confirm it is valid: 8804
    [LOC_95] Dr. [LOC_111], [LOC_124]([LOC_135]). <html:p>Augmenting LLMs with your
    proprietary data – which, by the way, may belong to your customers in <html:a
    id="_idIndexMarker333"></html:a>many instances – can prove to be a challenging
    task in terms of <html:strong class="bold">data privacy</html:strong> . While
    a cloud based LLM solution can enrich your proprietary data and offer numerous
    advantages, <html:em class="italic">uncontrolled data sharing with external parties
    can quickly turn into a legal, security, and</html:em> <html:span class="No-Break"><html:em
    class="italic">regulatory nightmare</html:em></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>Although the topic of data privacy is more stringent in the case of indexing
    and querying, utilizing metadata extractors can also raise potential privacy concerns
    to be aware of. Therefore, I believe a brief warning is <html:span class="No-Break">required
    already.</html:span></html:p> <html:p>Since most extractors rely on processing
    content via LLMs to generate metadata, this means your actual data gets transmitted
    to and analyzed by external <html:span class="No-Break">cloud services.</html:span></html:p>
    <html:p>There is a risk <html:a id="_idIndexMarker334"></html:a>of exposure or
    mishandling of any personal or confidential information contained in this data,
    whether due to security lapses, insider risks at the LLM vendor, or <html:span
    class="No-Break">malicious activities.</html:span></html:p> <html:p class="callout-heading">It’s
    not just OUR privacy at stake here</html:p> <html:p class="callout">Speaking of
    privacy issues, remember the example LlamaHub connectors we discussed earlier?
    Ingesting messages with <html:code class="literal">DiscordReader</html:code> transfers
    data from Discord servers. Given that Discord messages may contain private conversations,
    there is a potential privacy concern, especially if Discord’s terms of service
    and the expectations of the message senders are not taken into account. So, if
    your data includes private identities, healthcare details, financial information,
    and so on, allowing unrestrained extraction workflows could <html:span class="No-Break">be
    problematic.</html:span></html:p> <html:p>Here are some ways to mitigate <html:span
    class="No-Break">privacy risks:</html:span></html:p> <html:ul><html:li>Scrubbing
    personal data before ingestion into LlamaIndex using, for example, <html:code
    class="literal">PIINodePostprocessor</html:code> in combination with a local LLM.
    Check out the next section for a simple implementation guideline for <html:span
    class="No-Break">this option</html:span></html:li> <html:li>Restricting metadata
    extraction to only non-sensitive subsets of nodes. Of course, this assumes that
    you manually classify the sensitivity of each Node. That would be impractical
    for automated <html:span class="No-Break">processing pipelines</html:span></html:li>
    <html:li>Running LLMs locally instead of in the cloud where possible to limit
    external exposure. That depends, of course, on your available hardware and <html:span
    class="No-Break">model choice</html:span></html:li> <html:li>Enabling encryption
    mechanisms if such features are available with certain LLM vendors. If privacy
    is a big concern in your implementation, you might want to consider <html:a id="_idIndexMarker335"></html:a>and
    read more about <html:strong class="bold">fully homomorphic encryption</html:strong>
    ( <html:span class="No-Break"><html:strong class="bold">FHE</html:strong></html:span>
    <html:span class="No-Break">):</html:span> <html:a><html:span class="No-Break">https://huggingface.co/blog/encrypted-llm</html:span></html:a></html:li></html:ul>
    <html:p>These concerns and best practices apply to any type of interaction with
    an LLM. This subject has been discussed and analyzed in many available lectures
    and articles, so I’m not going <html:a id="_idIndexMarker336"></html:a>to go into
    further detail here. But that doesn’t mean it’s <html:span class="No-Break">not
    important!</html:span></html:p> <html:p class="callout-heading">Key message</html:p>
    <html:p class="callout">What you should understand is that using an LLM already
    poses a privacy risk for your data. Augmenting that LLM with an additional framework
    such as LlamaIndex means also augmenting the privacy <html:span class="No-Break">risks
    involved.</html:span></html:p> <html:p>In essence, additional diligence is needed
    when dealing with private data to ensure convenience does not override <html:span
    class="No-Break">security requirements.</html:span></html:p> <html:a id="_idTextAnchor099"></html:a><html:h2
    id="_idParaDest-100">Scrubbing personal data and other sensitive information</html:h2>
    <html:p>In a world filled with nosy onlookers and data rulebooks, it’s crucial
    to be as cautious with your <html:a id="_idIndexMarker337"></html:a>data as a
    squirrel guarding its acorns in a crowded park! The good news is that there are
    solutions for ensuring privacy. And a convenient one <html:a id="_idIndexMarker338"></html:a>is
    already provided by the <html:span class="No-Break">LlamaIndex framework.</html:span></html:p>
    <html:p><html:strong class="bold">Node post-processors</html:strong> can solve
    this problem <html:span class="No-Break">for us.</html:span></html:p> <html:p>In
    the <html:a id="_idIndexMarker339"></html:a>previous chapter, we discovered how
    node post-processors are used in a query engine. They are applied to the nodes
    that are returned from a retriever, before the response synthesis step, to apply
    different transformations on the nodes or node data itself. This is, at least,
    their most common <html:span class="No-Break">use case.</html:span></html:p> <html:p
    class="callout-heading">But there’s also another reason to use them</html:p> <html:p
    class="callout">It turns out we can also use node processors outside of the query
    engine. Among other things, they can be used to clean up any sensitive data before
    extracting metadata using external LLMs, <html:span class="No-Break">for example.</html:span></html:p>
    <html:p>There are two methods available: <html:code class="literal">PIINodePostprocessor</html:code>
    and <html:code class="literal">NERPIINodePostprocessor</html:code> . The first
    one is designed to work with any local LLM that you may have on hand, while the
    other is customized for using a specialized NER model. In case <html:a id="_idIndexMarker340"></html:a>you’re
    not familiar with the acronym, <html:strong class="bold">PII</html:strong> stands
    for <html:strong class="bold">Personally</html:strong> <html:span class="No-Break"><html:strong
    class="bold">Identifiable Information</html:strong></html:span> <html:span class="No-Break">.</html:span></html:p>
    <html:p>Here’s a simple example of using <html:code class="literal">NERPIINodePostprocessor</html:code>
    to clean up the data. This method uses a NER model from <html:strong class="bold">Hugging
    Face</html:strong> to do the job. Because I wanted to keep it simple, I didn’t
    specify a particular model. Therefore, you may expect a warning and the HuggingFaceLLM
    will probably default to using the <html:code class="literal">dbmdz/bert-large-cased-finetuned-conll03-english</html:code>
    model, as documented <html:span class="No-Break">here:</html:span> <html:span
    class="No-Break">https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english</html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>Make sure <html:a id="_idIndexMarker341"></html:a>you
    install the corresponding integration <html:span class="No-Break">package first:</html:span></html:p>
    <html:p>Also, on the first run, the code will download the model from Hugging
    Face and you’ll need to make sure you have at least 1.5 GB of free space available
    on <html:span class="No-Break">your machine.</html:span></html:p> <html:p>Here
    is <html:span class="No-Break">the code:</html:span></html:p> <html:p>The output
    should be similar <html:span class="No-Break">to this:</html:span></html:p> <html:p>Looking
    at the results, we can see that the names have been replaced with placeholders
    so that the <html:a id="_idIndexMarker342"></html:a>data can now be safely passed
    to any external LLM. The beauty of this method is that, on return, the answer
    can be processed back and the placeholders can be replaced with the original data,
    resulting in a seamless <html:span class="No-Break">user experience.</html:span></html:p>
    <html:p>The actual mapping between placeholders and real data will be stored in
    <html:code class="literal">clean_nodes[0].node.metadata</html:code> . This metadata
    will not be sent to the LLM and can later be used to produce the original names
    during <html:span class="No-Break">response synthesis.</html:span></html:p> <html:p>Next,
    we’ll discuss how to improve the efficiency of the <html:span class="No-Break">ingestion
    pipeline.</html:span></html:p> <html:a id="_idTextAnchor100"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Using
    the ingestion pipeline to increase efficiency</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-101">Using the ingestion pipeline
    to increase efficiency</html:h1> <html:div id="_idContainer039">from llama_index.core
    import SimpleDirectoryReader from llama_index.core.extractors import SummaryExtractor,QuestionsAnsweredExtractor
    from llama_index.core.node_parser import TokenTextSplitter from llama_index.core.ingestion
    import IngestionPipeline, IngestionCache from llama_index.core.schema import TransformComponent
    class CustomTransformation(TransformComponent):   def __call__(self, nodes, **kwargs):
        # run any node transformation logic here     return nodes reader = SimpleDirectoryReader(''files'')
    documents = reader.load_data() try:     cached_hashes = IngestionCache.from_persist_path(
    "./ingestion_cache.json" )     print("Cache file found. Running using cache")
    except:     cached_hashes = ""     print("No cache file found. Running without
    cache") pipeline = IngestionPipeline(     transformations = [         CustomTransformation(),
            TokenTextSplitter(             separator=" ",             chunk_size=512,
                chunk_overlap=128),         SummaryExtractor(),         QuestionsAnsweredExtractor(
                questions=3         )     ],     cache=cached_hashes ) nodes = pipeline.run(
        documents=documents,     show_progress=True, ) pipeline.cache.persist("./ingestion_cache.json")
    print("All documents loaded") from llama_index.core import Settings Settings.transformations
    = [     CustomTransformation(),     TokenTextSplitter(         separator=" ",
            chunk_size=512,         chunk_overlap=128     ),     SummaryExtractor(),
        QuestionsAnsweredExtractor(         questions=3     ) ] <html:p>Starting with
    <html:code class="literal">version 0.9</html:code> , the LlamaIndex framework
    introduced a really neat concept: the <html:a id="_idIndexMarker343"></html:a>so-called
    <html:span class="No-Break"><html:strong class="bold">ingestion pipeline</html:strong></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p class="callout-heading">A
    simple analogy</html:p> <html:p class="callout">An ingestion pipeline is a bit
    like a conveyor belt in a factory. In the context of LlamaIndex, it’s a setup
    that takes your raw data and gets it ready to be integrated into your RAG workflow.
    It does <html:a id="_idIndexMarker344"></html:a>this by running the data through
    a series of steps – called <html:strong class="bold">transformations</html:strong>
    – one by one. The key idea is to break the ingestion process into a series of
    reusable transformations that are applied to input data. This helps standardize
    and customize ingestion flows for different use cases. Think of transformations
    as different workstations along this conveyor belt. As your raw data moves along,
    it hits different stations where something specific happens. It might be split
    into sentences at one station – that’s your <html:code class="literal">SentenceSplitter</html:code>
    – and have a title extracted at another – such as <html:span class="No-Break">using</html:span>
    <html:span class="No-Break"><html:code class="literal">TitleExtractor</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>If the factory’s default
    workstations don’t quite cut it for you, no worries! Let’s say you have this special
    tool you want to use on your raw data. LlamaIndex makes it easy to plug in your
    custom tool – <html:strong class="bold">custom transformation</html:strong> .
    Just say what your tool does – for example, replacing acronyms with complete names
    using a dictionary – and LlamaIndex will happily add it <html:a id="_idIndexMarker345"></html:a>to
    your pipeline. <html:span class="No-Break"><html:em class="italic">Figure 4</html:em></html:span>
    <html:em class="italic">.4</html:em> provides an ingestion <html:span class="No-Break">pipeline
    schematic:</html:span></html:p> <html:p class="IMG---Caption" lang="en-US">Figure
    4.4 – An ingestion pipeline at work</html:p> <html:p>The most important thing
    about the ingestion pipeline is that <html:em class="italic">it remembers the
    data it has already processed</html:em> . It runs a hashing function on the combination
    of each node data and each transformation run. On any future runs of the same
    transformation on the same nodes, the hashes will be identical, so the cached,
    already processed data will be used instead of re-running <html:span class="No-Break">the
    transformation.</html:span></html:p> <html:p class="callout-heading">What does
    this mean for me?</html:p> <html:p class="callout">If you send the same document
    through the pipeline again, it’s like having a fast-track lane where it skips
    the line because it’s already been handled. This is cool because it saves you
    both time and money by avoiding useless multiple processing of the <html:span
    class="No-Break">same data.</html:span></html:p> <html:p>By default, the cache
    is stored locally but you can customize the storage options and use any external
    database provider you prefer. Let’s cover an example of how the pipeline could
    be implemented. I will explain the code section by section to make it easier <html:span
    class="No-Break">to follow.</html:span></html:p> <html:p>Let’s start with the
    first section of <html:span class="No-Break">the code:</html:span></html:p> <html:p>After
    taking care <html:a id="_idIndexMarker346"></html:a>of the required imports, to
    show you how to customize your pipeline, I have defined a class called <html:code
    class="literal">CustomTransformation</html:code> . This will <html:a id="_idIndexMarker347"></html:a>be
    fed into the pipeline later. In my example, no actual processing takes place,
    so this will return the <html:span class="No-Break">nodes unchanged.</html:span></html:p>
    <html:p>Let’s continue with the <html:span class="No-Break">second section:</html:span></html:p>
    <html:p>The preceding code is <html:a id="_idIndexMarker348"></html:a>responsible
    for fetching all content of the <html:code class="literal">files</html:code> subfolder
    into documents. Next, the code checks if the cache file already exists and attempts
    to load it into memory. Remember, the cache file contains the hashes and the results
    generated by the previous runs. The first time you run the code, there will be
    no file, so the code won’t load any <html:span class="No-Break">cached values.</html:span></html:p>
    <html:p>Let’s move on to the <html:span class="No-Break">third section:</html:span></html:p>
    <html:p>This is the part where we define our pipeline. As you can see, it will
    contain four transformations. The first is our <html:code class="literal">CustomTransformation</html:code>
    , followed by <html:code class="literal">TokenTextSplitter</html:code> , which
    is responsible for breaking each document into smaller chunks and generating nodes.
    The third transformation extracts the summaries metadata and the last one extracts
    a set of questions that each node <html:span class="No-Break">can answer.</html:span></html:p>
    <html:p>If you want to take a peek at the result, you could add <html:code class="literal">print(nodes[0])</html:code>
    at the end of the entire script. Notice that, in the <html:code class="literal">cache</html:code>
    parameter, we also specify the source of the cache for the pipeline. If that is
    empty, it will be ignored; otherwise, it will be used to avoid any <html:a id="_idIndexMarker349"></html:a>unnecessary
    processing by retrieving values from <html:span class="No-Break">the cache.</html:span></html:p>
    <html:p>And now, the <html:span class="No-Break">final part:</html:span></html:p>
    <html:p>This is where we run the pipeline with the <html:code class="literal">show_progress</html:code>
    option set to <html:code class="literal">True</html:code> . This will make the
    pipeline’s progress visible and help you better understand what’s happening in
    the background. In the end, we save the results in the cache file to avoid re-processing
    in the <html:span class="No-Break">next run.</html:span></html:p> <html:p class="callout-heading">A
    quick side note:</html:p> <html:p class="callout">Even if you saved a cache file,
    any changes you make in your pipeline logic will not be cached and will have to
    be processed at the <html:span class="No-Break">next run.</html:span></html:p>
    <html:p>You should also know there is an alternative to manually defining and
    running the pipeline every time you want to ingest more data. Just like with the
    node parsers, we can define the transformations inside <html:code class="literal">Settings</html:code>
    <html:span class="No-Break">like this:</html:span></html:p> <html:p>In conclusion,
    an ingestion pipeline is a super-efficient way to get your data automatically
    <html:a id="_idIndexMarker350"></html:a>prepped and polished by running it through
    customizable sets of transformations until it’s just right for your app <html:span
    class="No-Break">or database.</html:span></html:p> <html:p>As we build up the
    PITS tutoring app, we’ll leverage ingestion pipelines and you’ll get the opportunity
    to experiment more with <html:span class="No-Break">this concept.</html:span></html:p>
    <html:p>Next, let’s talk about more <html:span class="No-Break">complex scenarios.</html:span></html:p>
    <html:a id="_idTextAnchor101"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Handling
    documents that contain a mix of text and tabular data</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-102">Handling
    documents that contain a mix of text and tabular data</html:h1> <html:div id="_idContainer039"><html:p>Data
    is not always simple. Many real-world documents, such as research papers, financial
    reports, and others, contain a mix of unstructured text, as well as structured
    tabular data <html:a id="_idIndexMarker351"></html:a>in tables. Ingesting such
    heterogeneous <html:a id="_idIndexMarker352"></html:a>documents presents an additional
    challenge - we need to not only extract text but also identify, parse, and process
    tables embedded within the text. Because, sometimes you get tables, sometimes
    you get text and sometimes you have to deal with a mix <html:span class="No-Break">of
    both.</html:span></html:p> <html:p>LlamaIndex provides <html:code class="literal">UnstructuredElementNodeParser</html:code>
    to tackle such documents containing both free-form text as well as tables and
    other structured elements. It leverages the <html:code class="literal">Unstructured</html:code>
    library to analyze the document layout and delineate text sections <html:span
    class="No-Break">from tables.</html:span></html:p> <html:p>This parser works exclusively
    on HTML files and can extract two types <html:span class="No-Break">of nodes:</html:span></html:p>
    <html:ul><html:li><html:strong class="bold">Text nodes</html:strong> : Containing
    <html:a id="_idIndexMarker353"></html:a>the <html:span class="No-Break">text</html:span>
    <html:span class="No-Break"><html:a id="_idIndexMarker354"></html:a></html:span><html:span
    class="No-Break">chunks</html:span></html:li> <html:li><html:strong class="bold">Table
    nodes</html:strong> : Containing <html:a id="_idIndexMarker355"></html:a>the table
    data and metadata, such <html:a id="_idIndexMarker356"></html:a><html:span class="No-Break">as
    coordinates</html:span></html:li></html:ul> <html:p>Storing these elements as
    separate nodes allows more modular and meaningful processing later in the RAG
    workflow. The text can be indexed and searched normally with elements like keywords.
    The tables can be loaded into a <html:strong class="bold">pandas DataFrame</html:strong>
    or any structured database for SQL-based access. So, in complex cases involving
    mixed data types, leveraging <html:code class="literal">UnstructuredElementNodeParser</html:code>
    before ingestion enables better <html:span class="No-Break">data organization.</html:span></html:p>
    <html:p>You can find <html:a id="_idIndexMarker357"></html:a>a complete demo for
    using <html:code class="literal">UnstructuredElementNodeParser</html:code> in
    the official LlamaIndex <html:span class="No-Break">documentation:</html:span>
    <html:span class="No-Break">https://docs.llamaindex.ai/en/stable/examples/query_engine/sec_tables/tesla_10q_table.html</html:span>
    <html:span class="No-Break">.</html:span></html:p> <html:p>With these new concepts
    in our toolbox, let’s continue building our <html:span class="No-Break">tutoring
    project.</html:span></html:p> <html:a id="_idTextAnchor102"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Hands-on
    – ingesting study materials into our PITS</html:title></html:head> <html:body><html:div
    class="epub-source"><html:h1 id="_idParaDest-103">Hands-on – ingesting study materials
    into our PITS</html:h1> <html:div id="_idContainer039"><html:p>It’s time for some
    practice. We now have everything we need to continue building our project. Let’s
    write the <html:span class="No-Break"><html:code class="literal">documend_uploader.py</html:code></html:span>
    <html:span class="No-Break">module.</html:span></html:p> <html:p>This module <html:a
    id="_idIndexMarker358"></html:a>will take care of ingesting and preparing our
    available study material. The user can upload any available books, technical documentation,
    or existing articles to provide more context to <html:span class="No-Break">our
    tutor.</html:span></html:p> <html:ol><html:li>First, we have from global_settings
    import STORAGE_PATH, CACHE_FILE from logging_functions import log_action from
    llama_index import SimpleDirectoryReader, VectorStoreIndex from llama_index.ingestion
    import IngestionPipeline, IngestionCache from llama_index.text_splitter import
    TokenTextSplitter from llama_index.extractors import SummaryExtractor from llama_index.embeddings
    import OpenAIEmbedding <html:span class="No-Break">the imports:</html:span></html:li>
    <html:li>Next, we must define the main function that’s responsible for handling
    the ingestion process. You’ll notice that it uses an ingestion pipeline to both
    streamline the def ingest_documents():     documents = SimpleDirectoryReader(
            STORAGE_PATH,         filename_as_id = True     ).load_data()     for
    doc in documents:         print(doc.id_)         log_action(             f"File
    ''{doc.id_}'' uploaded user",             action_type="UPLOAD"         ) <html:a
    id="_idIndexMarker359"></html:a>code but also benefit <html:span class="No-Break">from
    caching:</html:span> <html:ul><html:li>The function loads all readable documents
    available in <html:code class="literal">STORAGE_PATH</html:code> , which was defined
    <html:span class="No-Break">in</html:span> <html:span class="No-Break"><html:code
    class="literal">global_settings.py</html:code></html:span> <html:span class="No-Break">.</html:span></html:li>
    <html:li>For each document processed, a new event is stored in our log file using
    <html:code class="literal">log_action</html:code> <html:span class="No-Break">from</html:span>
    <html:span class="No-Break"><html:code class="literal">logging_functions.py</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:li></html:ul></html:li> <html:li>Next,
    the function checks if there’s already cached pipeline data     try:         cached_hashes
    = IngestionCache.from_persist_path(             CACHE_FILE             )         print("Cache
    file found. Running using cache")     except:         cached_hashes = ""         print("No
    cache file found. Running without") <html:span class="No-Break">to use:</html:span></html:li>
    <html:li>The next step     pipeline = IngestionPipeline(         transformations=[
                TokenTextSplitter(                 chunk_size=1024,                 chunk_overlap=20
                ),             SummaryExtractor(summaries=[''self'']),             OpenAIEmbedding()
            ],         cache=cached_hashes     )     nodes = pipeline.run(documents=documents)
        pipeline.cache.persist(CACHE_FILE)     return nodes <html:a id="_idIndexMarker360"></html:a>is
    to define and run the pipeline. If hashes from the cache file correspond, no operations
    should be processed; instead, the values will be directly loaded from <html:span
    class="No-Break">the cache:</html:span> <html:p class="list-inset">We run three
    transformations in <html:span class="No-Break">the pipeline:</html:span></html:p>
    <html:ol><html:li class="upper-roman">Basic chunking <html:span class="No-Break">using</html:span>
    <html:span class="No-Break"><html:code class="literal">TokenTextSplitter</html:code></html:span>
    <html:span class="No-Break">.</html:span></html:li> <html:li class="upper-roman">A
    metadata extractor that summarizes <html:span class="No-Break">each node.</html:span></html:li>
    <html:li class="upper-roman">Embedding generation using <html:code class="literal">OpenAIEmbedding</html:code>
    . Don’t worry about this step for now. I will explain it thoroughly in <html:a><html:span
    class="No-Break"><html:em class="italic">Chapter 5</html:em></html:span></html:a>
    , <html:em class="italic">Indexing</html:em> <html:span class="No-Break"><html:em
    class="italic">with LlamaIndex</html:em></html:span> <html:span class="No-Break">.</html:span></html:li></html:ol></html:li>
    <html:li>In the end, the function <html:a id="_idIndexMarker361"></html:a>saves
    the current data in the cache file and returns the <html:span class="No-Break">processed
    nodes.</html:span></html:li></html:ol> <html:p>That’s it for now. We have now
    uploaded and prepared the study materials for future processing. We’ll continue
    with the indexing part in the <html:span class="No-Break">next chapter.</html:span></html:p>
    <html:a id="_idTextAnchor103"></html:a></html:div></html:div></html:body></html:html><html:html><html:head><html:title>Summary</html:title></html:head>
    <html:body><html:div class="epub-source"><html:h1 id="_idParaDest-104">Summary</html:h1>
    <html:div id="_idContainer039"><html:p>LlamaHub offers a variety of pre-built
    data loaders, streamlining the process of importing data from various sources
    as documents. This eliminates the need for creating unique parsers for different
    <html:span class="No-Break">data formats.</html:span></html:p> <html:p>After data
    is imported, it undergoes further processing into nodes, and we discussed various
    customization <html:span class="No-Break">options available.</html:span></html:p>
    <html:p>There’s a broad range of options for metadata extraction, and the parsing
    process can be tailored to meet <html:span class="No-Break">specific requirements.</html:span></html:p>
    <html:p>Developing pipelines for data ingestion is an invaluable tool for enhancing
    the efficiency, both in terms of cost and time, of our RAG applications. It’s
    also vital to keep privacy considerations <html:span class="No-Break">in mind.</html:span></html:p>
    <html:p>With data ingestion completed, let’s continue our journey and discover
    the indexing powers <html:span class="No-Break">of LlamaIndex.</html:span></html:p></html:div></html:div></html:body></html:html>'
  prefs: []
  type: TYPE_NORMAL
