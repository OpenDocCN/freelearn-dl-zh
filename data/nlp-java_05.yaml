- en: Chapter 2. Finding and Working with Words
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：查找和处理词语
- en: 'In this chapter, we cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍以下配方：
- en: Introduction to tokenizer factories – finding words in a character stream
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词器工厂简介——在字符流中查找单词
- en: Combining tokenizers – lowercase tokenizer
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合分词器——小写字母分词器
- en: Combining tokenizers – stop word tokenizers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合分词器——停用词分词器
- en: Using Lucene/Solr tokenizers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Lucene/Solr 分词器
- en: Using Lucene/Solr tokenizers with LingPipe
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Lucene/Solr 分词器与 LingPipe
- en: Evaluating tokenizers with unit tests
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单元测试评估分词器
- en: Modifying tokenizer factories
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改分词器工厂
- en: Finding words for languages without white spaces
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找没有空格的语言的单词
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: An important part of building NLP systems is to work with the appropriate unit
    for processing. This chapter addresses the abstraction layer associated with the
    word level of processing. This is called tokenization, which amounts to grouping
    adjacent characters into meaningful chunks in support of classification, entity
    finding, and the rest of NLP.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 NLP 系统的重要部分是使用适当的处理单元。本章讨论的是与词级处理相关的抽象层次。这个过程称为分词，它将相邻字符分组为有意义的块，以支持分类、实体识别和其他
    NLP 任务。
- en: LingPipe provides a broad range of tokenizer needs, which are not covered in
    this book. Look at the Javadoc for tokenizers that do stemming, Soundex (tokens
    based on what English words sound like), and more.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe 提供了广泛的分词器需求，这些需求在本书中没有涵盖。请查阅 Javadoc 以了解执行词干提取、Soundex（基于英语发音的标记）等的分词器。
- en: Introduction to tokenizer factories – finding words in a character stream
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词器工厂简介——在字符流中查找单词
- en: LingPipe tokenizers are built on a common pattern of a base tokenizer that can
    be used on its own, or can be as the source for subsequent filtering tokenizers.
    Filtering tokenizers manipulate the tokens/white spaces provided by the base tokenizer.
    This recipe covers our most commonly used tokenizer, `IndoEuropeanTokenizerFactory`,
    which is good for languages that use the Indo-European style of punctuation and
    word separators—examples include English, Spanish, and French. As always, the
    Javadoc has useful information.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe 分词器建立在一个通用的基础分词器模式上，基础分词器可以单独使用，也可以作为后续过滤分词器的来源。过滤分词器会操作由基础分词器提供的标记和空格。本章节涵盖了我们最常用的分词器
    `IndoEuropeanTokenizerFactory`，它适用于使用印欧语言风格的标点符号和词汇分隔符的语言——例如英语、西班牙语和法语。和往常一样，Javadoc
    中包含了有用的信息。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '`IndoEuropeanTokenizerFactory` creates tokenizers with built-in support for
    alpha-numerics, numbers, and other common constructs in Indo-European languages.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '`IndoEuropeanTokenizerFactory` 创建具有内建支持的分词器，支持印欧语言中的字母数字、数字和其他常见构造。'
- en: The tokenization rules are roughly based on those used in MUC-6 but are necessarily
    more fine grained, because the MUC tokenizers are based on lexical and semantic
    information, such as whether a string is an abbreviation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 分词规则大致基于 MUC-6 中使用的规则，但由于 MUC 分词器基于词汇和语义信息（例如，字符串是否为缩写），因此这些规则必须更为精细。
- en: MUC-6 refers to the Message Understanding Conference that originated the idea
    of government-sponsored competitions between contractors in 1995\. The informal
    term was *Bake off*, in reference to the Pillsbury Bake-Off that started in 1949,
    and one of the authors was a participant as postdoc in MUC-6\. MUC drove much
    of the innovation in the evaluation of NLP systems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: MUC-6 指的是1995年发起的消息理解会议，它创立了政府资助的承包商之间的竞争形式。非正式的术语是 *Bake off*，指的是1949年开始的比尔斯伯里烘焙大赛，且其中一位作者在
    MUC-6 中作为博士后参与了该会议。MUC 对自然语言处理系统评估的创新起到了重要推动作用。
- en: LingPipe tokenizers are built using the LingPipe `TokenizerFactory` interface,
    which provides a way of invoking different types of tokenizers using the same
    interface. This is very useful in creating filtered tokenizers, which are constructed
    as a chain of tokenizers and modify their output in some way. A `TokenizerFactory`
    instance might be created either as a basic tokenizer, which takes simple parameters
    in its construction, or as a filtered tokenizer, which takes other tokenizer factory
    objects as parameters. In either case, an instance of `TokenizerFactory` has a
    single `tokenize()` method, which takes input as a character array, a start index,
    and the number of characters to process and outputs a `Tokenizer` object. The
    `Tokenizer` object represents the state of tokenizing a particular slice of string
    and provides a stream of tokens. While `TokenizerFactory` is thread safe and/or
    serializable, tokenizer instances are typically neither thread safe nor serializable.
    The `Tokenizer` object provides methods to iterate over the tokens in the string
    and to provide token positions of the tokens in the underlying text.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe标记器是使用LingPipe的`TokenizerFactory`接口构建的，该接口提供了一种方法，可以使用相同的接口调用不同类型的标记器。这在创建过滤标记器时非常有用，过滤标记器是通过一系列标记器链构建的，并以某种方式修改其输出。`TokenizerFactory`实例可以作为基本标记器创建，它在构造时接受简单的参数，或者作为过滤标记器创建，后者接受其他标记器工厂对象作为参数。在这两种情况下，`TokenizerFactory`的实例都有一个`tokenize()`方法，该方法接受输入字符数组、起始索引和要处理的字符数，并输出一个`Tokenizer`对象。`Tokenizer`对象表示标记化特定字符串片段的状态，并提供标记符流。虽然`TokenizerFactory`是线程安全和/或可序列化的，但标记器实例通常既不线程安全也不具备序列化功能。`Tokenizer`对象提供了遍历字符串中标记符的方法，并提供标记符在底层文本中的位置。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Download the JAR file and source for the book if you have not already done so.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有下载书籍的JAR文件和源代码，请先下载。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'It is all pretty simple. The following are the steps to get started with tokenization:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都很简单。以下是开始标记化的步骤：
- en: 'Go to the `cookbook` directory and invoke the following class:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到`cookbook`目录并调用以下类：
- en: '[PRE0]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will lead us to a command prompt, which asks us to type in some text:'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将带我们进入命令提示符，提示我们输入一些文本：
- en: '[PRE1]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'If we type a sentence such as: `It''s no use growing older if you only learn
    new ways of misbehaving yourself`, we will get the following output:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们输入如下句子：`It's no use growing older if you only learn new ways of misbehaving
    yourself`，我们将得到以下输出：
- en: '[PRE2]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Examine the output and note what the tokens and white spaces are. The text is
    from the short story, *The Stampeding of Lady Bastable*, by Saki.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看输出并注意标记符和空格的内容。文本摘自萨基的短篇小说《*巴斯特布尔夫人的冲击*》。
- en: How it works...
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The code is so simple that it can be included in its entirety as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 代码非常简单，可以完整地如下包含：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This recipe starts with the creation of `TokenizerFactory tokFactory` in the
    first statement of the `main()` method. Note that a singleton `IndoEuropeanTokenizerFactory.INSTANCE`
    is used. The factory will produce tokenizers for a given string, which is evident
    in the line, `Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(),
    0, input.length())`. The entered string is converted to a character array with
    `input.toCharArray()` as the first argument to the `tokenizer` method and the
    start and finish offsets provided into the created character array.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例从在`main()`方法的第一行创建`TokenizerFactory tokFactory`开始。注意使用了单例`IndoEuropeanTokenizerFactory.INSTANCE`。该工厂会为给定的字符串生成标记器，这一点在这一行中有所体现：`Tokenizer
    tokenizer = tokFactory.tokenizer(input.toCharArray(), 0, input.length())`。输入的字符串通过`input.toCharArray()`转换为字符数组，并作为`tokenizer`方法的第一个参数，起始和结束偏移量传入到生成的字符数组中。
- en: 'The resulting `tokenizer` provides tokens for the provided slice of character
    array, and the white spaces and tokens are printed out in the `while` loop. Calling
    the `tokenizer.nextToken()` method does a few things:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 结果`tokenizer`为提供的字符数组片段提供标记符，空格和标记符将在`while`循环中打印出来。调用`tokenizer.nextToken()`方法执行了几个操作：
- en: The method returns the next token or null if there is no next token. The null
    then ends the loop; otherwise, the loop continues.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法返回下一个标记符，如果没有下一个标记符，则返回null。此时循环结束；否则，循环继续。
- en: The method also increments the corresponding white space. There is always a
    white space with a token, but it might be the empty string.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该方法还会递增相应的空格。每个标记符后面都会有一个空格，但它可能是空字符串。
- en: '`IndoEuropeanTokenizerFactory` assumes a fairly standard abstraction over characters
    that break down as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`IndoEuropeanTokenizerFactory`假设有一个相当标准的字符抽象，其分解如下：'
- en: Characters from the beginning of the `char` array to the first token are ignored
    and not reported as white space
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`char`数组的开头到第一个分词的字符会被忽略，并不会被报告为空格。
- en: Characters from the end of the last token to the end of the `char` array are
    reported as the next white space
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从上一个分词的末尾到`char`数组末尾的字符被报告为下一个空格。
- en: White spaces can be the empty string because of two adjoining tokens—note the
    apostrophe in the output and corresponding white spaces
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空格可能是空字符串，因为有两个相邻的分词——注意输出中的撇号和相应的空格。
- en: This means that it is not possible to reconstruct the original string necessarily
    if the input does not start with a token. Fortunately, tokenizers are easily modified
    for customized needs. We will see this later in the chapter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果输入不以分词开始，则可能无法重建原始字符串。幸运的是，分词器很容易根据自定义需求进行修改。我们将在本章后面看到这一点。
- en: There's more…
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: Tokenization can be arbitrarily complex. The LingPipe tokenizers are intended
    to cover most common uses, but you might need to create your own tokenizer to
    have fine-grained control, for example, Victoria's Secret with "Victoria's" as
    the token. Consult the source for `IndoEuropeanTokenizerFactory` if such customization
    is needed, to see how arbitrary tokenization is done here.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 分词可能会非常复杂。LingPipe分词器旨在覆盖大多数常见用例，但你可能需要创建自己的分词器以进行更精细的控制，例如，将“Victoria's Secret”中的“Victoria's”作为一个分词。如果需要这样的自定义，请查阅`IndoEuropeanTokenizerFactory`的源码，了解这里是如何进行任意分词的。
- en: Combining tokenizers – lowercase tokenizer
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合分词器——小写分词器
- en: We mentioned in the previous recipe that LingPipe tokenizers can be basic or
    filtered. Basic tokenizers, such as the Indo-European tokenizer, don't need much
    in terms of parameterization, none at all as a matter of fact. However, filtered
    tokenizers need a tokenizer as a parameter. What we're doing with filtered tokenizers
    is invoking multiple tokenizers where a base tokenizer is usually modified by
    a filter to produce a different tokenizer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的配方中提到过，LingPipe分词器可以是基本的或过滤的。基本分词器，例如Indo-European分词器，不需要太多的参数化，事实上根本不需要。然而，过滤分词器需要一个分词器作为参数。我们使用过滤分词器的做法是调用多个分词器，其中一个基础分词器通常会被过滤器修改，产生一个不同的分词器。
- en: LingPipe provides several basic tokenizers, such as `IndoEuropeanTokenizerFactory`
    or `CharacterTokenizerFactory`. A complete list can be found in the Javadoc for
    LingPipe. In this section, we'll show you how to combine an Indo-European tokenizer
    with a lowercase tokenizer. This is a fairly common process that many search engines
    implement for Indo-European languages.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LingPipe提供了几种基本的分词器，例如`IndoEuropeanTokenizerFactory`或`CharacterTokenizerFactory`。完整的列表可以在LingPipe的Javadoc中找到。在本节中，我们将向你展示如何将Indo-European分词器与小写分词器结合使用。这是许多搜索引擎为印欧语言实现的一个常见过程。
- en: Getting ready
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to download the JAR file for the book and have Java and Eclipse
    set up so that you can run the example.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要下载书籍的JAR文件，并确保已经设置好Java和Eclipse，以便能够运行示例。
- en: How to do it...
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'This works just the same way as the previous recipe. Perform the following
    steps:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这与前面的配方完全相同。请按照以下步骤操作：
- en: 'Invoke the `RunLowerCaseTokenizerFactory` class from the command line:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行调用`RunLowerCaseTokenizerFactory`类：
- en: '[PRE4]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, in the command prompt, let''s use the following example:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在命令提示符下，我们使用以下示例：
- en: '[PRE5]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: You can see in the preceding output that all the tokens are converted to lowercase,
    including the word `UPPERCASE`, which was typed in uppercase. As this example
    uses an Indo-European tokenizer as its base tokenizer, you can see that the number
    4.5 is retained as `4.5` instead of being broken up into 4 and 5.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在前面的输出中看到，所有的分词都被转换成小写，包括大写输入的单词`UPPERCASE`。由于这个示例使用了Indo-European分词器作为基础分词器，你可以看到数字4.5被保留为`4.5`，而不是分解为4和5。
- en: 'The way we put tokenizers together is very simple:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们组合分词器的方式非常简单：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we created a tokenizer that returns case and white space normalized tokens
    produced using an Indo-European tokenizer. The tokenizer created from the tokenizer
    factory is a filtered tokenizer that starts with the Indo-European base tokenizer,
    which is then modified by `LowerCaseTokenizer` to produce the lowercase tokenizer.
    This is then once again modified by `WhiteSpaceNormTokenizerFactory` to produce
    a lowercase, white space-normalized Indo-European tokenizer.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建了一个分词器，该分词器返回通过印欧语言分词器产生的大小写和空格标准化的标记。通过分词器工厂创建的分词器是一个过滤的分词器，它从印欧基础分词器开始，然后由`LowerCaseTokenizer`修改为小写分词器。接着，它再次通过`WhiteSpaceNormTokenizerFactory`进行修改，生成一个小写且空格标准化的印欧分词器。
- en: Case normalization is applied where the case of words doesn't matter much; for
    example, search engines often store case-normalized words in their indexes. Now,
    we will use case-normalized tokens in the upcoming examples on classifiers.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在对单词大小写不太重要的地方应用大小写标准化；例如，搜索引擎通常会将大小写标准化的单词存储在索引中。现在，我们将在接下来的分类器示例中使用大小写标准化的标记。
- en: See also
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: For more details on how filtered tokenizers are built, see the Javadoc for the
    abstract class, `ModifiedTokenizerFactory.`
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关如何构建过滤分词器的更多细节，请参见抽象类`ModifiedTokenizerFactory`的 Javadoc。
- en: Combining tokenizers – stop word tokenizers
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合分词器 – 停用词分词器
- en: Similarly to the way in which we put together a lowercase and white space normalized
    tokenizer, we can use a filtered tokenizer to create a tokenizer that filters
    out stop words. Once again, using search engines as our example, we can remove
    commonly occurring words from our input set so as to normalize the text. The stop
    words that are typically removed convey very little information by themselves,
    although they might convey information in context.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们如何构建一个小写和空格标准化的分词器，我们可以使用一个过滤的分词器来创建一个过滤掉停用词的分词器。再次以搜索引擎为例，我们可以从输入集中删除常见的词汇，以便规范化文本。通常被移除的停用词本身传达的信息很少，尽管它们在特定上下文中可能会有意义。
- en: The input is tokenized using whatever base tokenizer is set up, and then, the
    resulting tokens are filtered out by the stop tokenizer to produce a token stream
    that is free of the stop words specified when the stop tokenizer is initialized.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输入会通过所设置的基础分词器进行分词，然后由停用词分词器过滤掉，从而生成一个不包含初始化时指定的停用词的标记流。
- en: Getting ready
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to download the JAR file for the book and have Java and Eclipse
    set up so that you can run the example.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要下载书籍的 JAR 文件，并确保已经安装 Java 和 Eclipse，以便能够运行示例。
- en: How to do it...
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'As we did earlier, we will go through the steps of interacting with the tokenizer:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将通过与分词器交互的步骤进行演示：
- en: 'Invoke the `RunStopTokenizerFactory` class from the command line:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行调用`RunStopTokenizerFactory`类：
- en: '[PRE7]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, in the prompt, let''s use the following example:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在提示中，让我们使用以下示例：
- en: '[PRE8]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that we lose adjacency information. In the input, we have `fox is jumping`,
    but the tokens came out as `fox` followed by `jumping`, because `is` was filtered.
    This can be a problem for token-based processes that need accurate adjacency information.
    In the *Foreground- or background-driven interesting phrase detection* recipe
    of [Chapter 4](ch04.html "Chapter 4. Tagging Words and Tokens"), *Tagging Words
    and Tokens*, we will show a length-based filtering tokenizer that preserves adjacency.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，我们失去了邻接信息。在输入中，我们有`fox is jumping`，但分词后变成了`fox`后跟`jumping`，因为`is`被过滤掉了。对于那些需要准确邻接信息的基于分词的过程，这可能会成为一个问题。在[第4章](ch04.html
    "第4章 标签化单词和标记")的*前景驱动或背景驱动的有趣短语检测*配方中，我们将展示一个基于长度过滤的分词器，它保留了邻接信息。
- en: How it works...
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'The stop words used in this `StopTokenizerFactory` filter are just a very short
    list of words, `is`, `of`, `the`, and `to`. Obviously, this list can be much longer
    if required. As you saw in the preceding output, the words `the` and `is` have
    been removed from the tokenized output. This is done with a very simple step:
    we instantiate `StopTokenizerFactory` in `src/com/lingpipe/cookbook/chapter2/RunStopTokenizerFactory.java`.
    The relevant code is:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个`StopTokenizerFactory`过滤器中使用的停用词仅是一个非常简短的单词列表，包括`is`、`of`、`the`和`to`。显然，如果需要，这个列表可以更长。如你在前面的输出中看到的，单词`the`和`is`已经从分词输出中移除了。这通过一个非常简单的步骤完成：我们在`src/com/lingpipe/cookbook/chapter2/RunStopTokenizerFactory.java`中实例化了`StopTokenizerFactory`。相关代码如下：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As we're using `LowerCaseTokenizerFactory` as one of the filters in the tokenizer
    factory, we can get away with the stop words that contain only lowercase words.
    If we want to preserve the case of the input tokens and continue to remove the
    stop words, we will need to add uppercase or mixed-case versions as well.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用`LowerCaseTokenizerFactory`作为分词器工厂中的一个过滤器，我们可以忽略只包含小写字母的停用词。如果我们想保留输入标记的大小写并继续删除停用词，我们还需要添加大写或混合大小写版本。
- en: See also
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: The complete list of filtered tokenizers provided by LingPipe can be found on
    the Javadoc page at [http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/ModifyTokenTokenizerFactory.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/ModifyTokenTokenizerFactory.html)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由LingPipe提供的过滤器分词器的完整列表可以在Javadoc页面找到，链接为[http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/ModifyTokenTokenizerFactory.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/tokenizer/ModifyTokenTokenizerFactory.html)
- en: Using Lucene/Solr tokenizers
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Lucene/Solr分词器
- en: The very popular search engine, Lucene, includes many analysis modules, which
    provide general purpose tokenizers as well as language-specific tokenizers from
    Arabic to Thai. As of Lucene 4, most of these different analyzers can be found
    in separate JAR files. We will cover Lucene tokenizers, because they can be used
    as LingPipe tokenizers, as you will see in the next recipe.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 备受欢迎的搜索引擎Lucene包含许多分析模块，提供通用的分词器以及从阿拉伯语到泰语的语言特定分词器。从Lucene 4开始，这些不同的分析器大多可以在单独的JAR文件中找到。我们将讲解Lucene分词器，因为它们可以像LingPipe分词器一样使用，正如你将在下一个配方中看到的那样。
- en: Much like the LingPipe tokenizers, Lucene tokenizers also can be split into
    basic tokenizers and filtered tokenizers. Basic tokenizers take a reader as input,
    and filtered tokenizers take other tokenizers as input. We will look at an example
    of using a standard Lucene analyzer along with a lowercase-filtered tokenizer.
    A Lucene analyzer essentially maps a field to a token stream. So, if you have
    an existing Lucene index, you can use the analyzer with the field name instead
    of the raw tokenizer, as we will show in the later part of this chapter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 就像LingPipe分词器一样，Lucene分词器也可以分为基础分词器和过滤分词器。基础分词器以读取器为输入，过滤分词器则以其他分词器为输入。我们将看一个示例，演示如何使用标准的Lucene分析器和一个小写过滤分词器。Lucene分析器本质上是将字段映射到一个标记流。因此，如果你有一个现有的Lucene索引，你可以使用分析器和字段名称，而不是使用原始的分词器，正如我们在本章后面的部分所展示的那样。
- en: Getting ready
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You will need to download the JAR file for the book and have Java and Eclipse
    set up so that you can run the example. Some of the Lucene analyzers used in the
    examples are part of the `lib` directory. However, if you'd like to experiment
    with other language analyzers, download them from the Apache Lucene website at
    [https://lucene.apache.org](https://lucene.apache.org).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要下载本书的JAR文件，并配置Java和Eclipse，以便运行示例。示例中使用的一些Lucene分析器是`lib`目录的一部分。然而，如果你想尝试其他语言的分析器，可以从Apache
    Lucene官网[https://lucene.apache.org](https://lucene.apache.org)下载它们。
- en: How to do it...
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Remember that we are not using a LingPipe tokenizer in this recipe but introducing
    the Lucene tokenizer classes:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这个配方中我们没有使用LingPipe分词器，而是介绍了Lucene分词器类：
- en: 'Invoke the `RunLuceneTokenizer` class from the command line:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行调用`RunLuceneTokenizer`类：
- en: '[PRE10]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, in the prompt, let''s use the following example:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在提示中，我们使用以下示例：
- en: '[PRE11]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How it works...
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Let''s review the following code to see how the Lucene tokenizers differ in
    invocation from the previous examples—the relevant part of the code from `src/com/lingpipe/cookbook/chapter2/RunLuceneTokenizer.java`
    is:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾以下代码，看看Lucene分词器如何与前面的示例中的调用不同——`src/com/lingpipe/cookbook/chapter2/RunLuceneTokenizer.java`中相关部分的代码是：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding snippet sets up `BufferedReader` from the command line and starts
    a perpetual `while()` loop. Next, the prompt is provided, the `input` is read,
    and it is used to construct a `Reader` object:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段从命令行设置`BufferedReader`并启动一个永久的`while()`循环。接下来，提供了提示，读取`input`，并用于构造`Reader`对象：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'All the input is now wrapped up, and it is time to construct the actual tokenizer:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所有输入现在都已封装，可以构造实际的分词器了：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The input text is used to construct `StandardTokenizer` with Lucene's versioning
    system supplied—this produces an instance of `TokenStream`. Then, we used `LowerCaseFilter`
    to create the final filtered `tokenStream` with the base `tokenStream` as an argument.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输入文本用于构造`StandardTokenizer`，并提供Lucene的版本控制系统——这会生成一个`TokenStream`实例。接着，我们使用`LowerCaseFilter`创建最终的过滤`tokenStream`，并将基础`tokenStream`作为参数传入。
- en: 'In Lucene, we need to attach the attributes we''re interested in from the token
    stream; this is done by the `addAttribute` method:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在Lucene中，我们需要从token流中附加我们感兴趣的属性；这可以通过`addAttribute`方法完成：
- en: '[PRE15]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Note that in Lucene 4, once the tokenizer has been instantiated, the `reset()`
    method must be called before using the tokenizer:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在Lucene 4中，一旦tokenizer被实例化，必须在使用tokenizer之前调用`reset()`方法：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `tokenStream` is wrapped up with the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`tokenStream`用以下方式进行包装：'
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: See also
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参见
- en: An excellent introduction to Lucene is in *Text Processing with Java*, *Mitzi
    Morris*, *Colloquial Media Corporation*, where the guts of what we explained earlier
    are made clearer than what we can provide in a recipe.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Lucene的一个优秀入门书籍是*Text Processing with Java*，*Mitzi Morris*，*Colloquial Media
    Corporation*，其中我们之前解释的内容比我们在此提供的食谱更清晰易懂。
- en: Using Lucene/Solr tokenizers with LingPipe
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将Lucene/Solr的tokenizers与LingPipe一起使用
- en: We can use these Lucene tokenizers with LingPipe; this is useful because Lucene
    has such a rich set of them. We are going to show how to wrap a Lucene `TokenStream`
    into a LingPipe `TokenizerFactory` by extending the `Tokenizer` abstract class.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些Lucene的tokenizers与LingPipe一起使用；这是非常有用的，因为Lucene拥有一套非常丰富的tokenizers。我们将展示如何通过扩展`Tokenizer`抽象类将Lucene的`TokenStream`封装成LingPipe的`TokenizerFactory`。
- en: How to do it...
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'We will shake things up a bit and have a recipe that is not interactive. Perform
    the following steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将稍微改变一下，提供一个非交互式的示例。请执行以下步骤：
- en: 'Invoke the `LuceneAnalyzerTokenizerFactory` class from the command line:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从命令行调用`LuceneAnalyzerTokenizerFactory`类：
- en: '[PRE18]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `main()` method in the class specifies the input:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类中的`main()`方法指定了输入：
- en: '[PRE19]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding snippet creates a Lucene `StandardAnalyzer` and uses it to construct
    a LingPipe `TokenizerFactory`. The output is as follows—the `StandardAnalyzer`
    filters stop words, so the token `are` is filtered:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的代码片段创建了一个Lucene的`StandardAnalyzer`并用它构建了一个LingPipe的`TokenizerFactory`。输出如下——`StandardAnalyzer`过滤了停用词，因此单词`are`被过滤掉了：
- en: '[PRE20]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The white spaces report as `default` because the implementation does not accurately
    provide white spaces but goes with a default. We will discuss this limitation
    in the *How it works…* section.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 空格报告为`default`，因为实现没有准确提供空格，而是使用了默认值。我们将在*它是如何工作的……*部分讨论这个限制。
- en: How it works...
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Let''s take a look at the `LuceneAnalyzerTokenizerFactory` class. This class
    implements the LingPipe `TokenizerFactory` interface by wrapping a Lucene analyzer.
    We will start with the class definition from `src/com/lingpipe/cookbook/chapter2/LuceneAnalyzerTokenizerFactory.java`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下`LuceneAnalyzerTokenizerFactory`类。这个类通过封装一个Lucene分析器实现了LingPipe的`TokenizerFactory`接口。我们将从`src/com/lingpipe/cookbook/chapter2/LuceneAnalyzerTokenizerFactory.java`中的类定义开始：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The constructor stores the analyzer and the name of the field as private variables.
    As this class implements the `TokenizerFactory` interface, we need to implement
    the `tokenizer()` method:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数将分析器和字段名作为私有变量存储。由于该类实现了`TokenizerFactory`接口，我们需要实现`tokenizer()`方法：
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `tokenizer()` method creates a new character-array reader and passes it
    to the Lucene analyzer to convert it to a `TokenStream`. An instance of `LuceneTokenStreamTokenizer`
    is created based on the token stream. `LuceneTokenStreamTokenizer` is a nested
    static class that extends LingPipe''s `Tokenizer` class:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`tokenizer()`方法创建一个新的字符数组读取器，并将其传递给Lucene分析器，将其转换为`TokenStream`。根据token流创建了一个`LuceneTokenStreamTokenizer`的实例。`LuceneTokenStreamTokenizer`是一个嵌套的静态类，继承自LingPipe的`Tokenizer`类：'
- en: '[PRE23]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The constructor stores `TokenStream` and attaches the term and the offset attributes.
    In the previous recipe, we saw that the term and the offset attributes contain
    the token string, and the token start and end offsets into the input text. The
    token offsets are also initialized to `-1` before any tokens are found:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数存储了`TokenStream`并附加了术语和偏移量属性。在前面的食谱中，我们看到术语和偏移量属性包含token字符串，以及输入文本中的token起始和结束偏移量。token偏移量在找到任何tokens之前也被初始化为`-1`：
- en: '[PRE24]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We will implement the `nextToken()` method and use the `incrementToken()` method
    of the token stream to retrieve any tokens from the token stream. We will set
    the token start and end offsets using `OffsetAttribute`. If the token stream is
    finished or the `incrementToken()` method throws an I/O exception, we will end
    and close the `TokenStream`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现`nextToken()`方法，并使用token流的`incrementToken()`方法从token流中获取任何tokens。我们将使用`OffsetAttribute`来设置token的起始和结束偏移量。如果token流已经结束，或者`incrementToken()`方法抛出I/O异常，我们将结束并关闭`TokenStream`。
- en: 'The `nextWhitespace()` method has some limitations, because `offsetAttribute`
    is focused on the current token where LingPipe tokenizers quantize the input into
    the next token and next offset. A general solution here will be quite challenging,
    because there might not be any well-defined white spaces between tokens—think
    character ngrams. So, the `default` string is supplied just to make it clear.
    The method is:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`nextWhitespace()`方法有一些局限性，因为`offsetAttribute`聚焦于当前标记，而LingPipe分词器会将输入量化为下一个标记和下一个偏移量。这里的一个通用解决方案将是相当具有挑战性的，因为标记之间可能没有明确的空格——可以想象字符n-grams。因此，`default`字符串仅供参考，以确保清楚表达。该方法如下：'
- en: '[PRE25]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The code also covers how to serialize the tokenizer, but we will not cover this
    in the recipe.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 代码还涵盖了如何序列化分词器，但我们在本步骤中不做详细讨论。
- en: Evaluating tokenizers with unit tests
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用单元测试评估分词器
- en: 'We will not evaluate Indo-European tokenizers like the other components of
    LingPipe with measures such as precision and recall. Instead, we will develop
    them with unit tests, because our tokenizers are heuristically constructed and
    expected to perform perfectly on example data—if a tokenizer fails to tokenize
    a known case, then it is a bug, not a reduction in performance. Why is this? There
    are a few reasons:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会像对LingPipe的其他组件一样，用精确度和召回率等度量标准来评估印欧语言分词器。相反，我们会通过单元测试来开发它们，因为我们的分词器是启发式构建的，并预计在示例数据上能完美执行——如果分词器未能正确分词已知案例，那就是一个BUG，而不是性能下降。为什么会这样呢？有几个原因：
- en: Many tokenizers are very "mechanistic" and are amenable to the rigidity of the
    unit test framework. For example, the `RegExTokenizerFactory` is obviously a candidate
    to unit test rather than an evaluation harness.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多分词器非常“机械化”，适合于单元测试框架的刚性。例如，`RegExTokenizerFactory`显然是一个单元测试的候选对象，而不是评估工具。
- en: The heuristic rules that drive most tokenizers are very general, and there is
    no issue of over-fitting training data at the expense of a deployed system. If
    you have a known bad case, you can just go and fix the tokenizer and add a unit
    test.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动大多数分词器的启发式规则是非常通用的，并且不存在以牺牲已部署系统为代价的过拟合训练数据的问题。如果你遇到已知的错误案例，你可以直接修复分词器并添加单元测试。
- en: 'Tokens and white spaces are assumed to be semantically neutral, which means
    that tokens don''t change depending on context. This is not totally true with
    our Indo-European tokenizer, because it treats `.` differently if it is part of
    a decimal or at the end of a sentence, for example, `3.14 is pi.`:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设标记和空格在语义上是中性的，这意味着标记不会根据上下文而变化。对于我们的印欧语言分词器来说，这并不完全正确，因为它会根据上下文的不同（例如，`3.14
    is pi.`中的`.`与句末的`.`）处理`.`。
- en: '[PRE26]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It might be appropriate to use an evaluation metric for statistics-based tokenizers;
    this is discussed in the *Finding words for languages without white spaces* recipe
    in this chapter. See the *Evaluation of sentence detection* recipe in [Chapter
    5](ch05.html "Chapter 5. Finding Spans in Text – Chunking"), *Finding Spans in
    Text – Chunking*, for appropriate span-based evaluation techniques.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于统计的分词器，使用评估指标可能是合适的；这一点在本章的*为没有空格的语言寻找单词*的步骤中进行了讨论。请参阅[第5章](ch05.html "第5章.
    文本中的跨度查找 – 分块")中的*句子检测评估*步骤，了解适合的基于跨度的评估技术。
- en: How to do it...
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'We will forgo running the code step and just get right into the source to put
    together a tokenizer evaluator. The source is in `src/com/lingpipe/chapter2/TestTokenizerFactory.java`.
    Perform the following steps:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将跳过代码步骤，直接进入源代码，构建分词器评估器。源代码在`src/com/lingpipe/chapter2/TestTokenizerFactory.java`。请执行以下步骤：
- en: 'The following code sets up a base tokenizer factory with a regular expression—look
    at the Javadoc for the class if you are not clear about what is being constructed:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码设置了一个基础的分词器工厂，使用正则表达式——如果你对构建的内容不清楚，请查看该类的Javadoc：
- en: '[PRE27]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `checkTokens` method takes `TokenizerFactory`, an array of `String` that
    is the desired tokenization, and `String` that is to be tokenized. It follows:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`checkTokens`方法接受`TokenizerFactory`、一个期望的分词结果的`String`数组，以及一个待分词的`String`。具体如下：'
- en: '[PRE28]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The method is quite intolerant of errors, because it exits the program if the
    token arrays are not of the same length or if any of the tokens are not equal.
    A proper unit test framework such as JUnit will be a better framework, but that
    is beyond the scope of the book. You can look at the LingPipe unit tests in `lingpipe.4.1.0`/`src/com/aliasi/test`
    for how JUnit is used.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该方法对错误的容忍度很低，因为如果标记数组的长度不相同，或者某些标记不相等，它会退出程序。一个像 JUnit 这样的单元测试框架会是一个更好的框架，但这超出了本书的范围。你可以查看
    `lingpipe.4.1.0`/`src/com/aliasi/test` 中的 LingPipe 单元测试，了解如何使用 JUnit。
- en: The `checkTokensAndWhiteSpaces()` method checks white spaces as well as tokens.
    It follows the same basic ideas of `checkTokens()`, so we leave it unexplained.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`checkTokensAndWhiteSpaces()` 方法检查空格以及标记。它遵循与 `checkTokens()` 相同的基本思路，因此我们将其略去不做解释。'
- en: Modifying tokenizer factories
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改标记器工厂
- en: In this recipe, we will describe a tokenizer that modifies the tokens in the
    token stream. We will extend the `ModifyTokenTokenizerFactory` class to return
    text that is rotated by 13 places in the English alphabet, also known as rot-13\.
    Rot-13 is a very simple substitution cipher, which replaces a letter with the
    letter that follows after 13 places. For example, the letter `a` will be replaced
    by the letter `n`, and the letter `z` will be replaced by the letter `m`. This
    is a reciprocal cypher, which means that applying the same cypher twice recovers
    the original text.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇中，我们将描述一个修改标记流中标记的标记器。我们将扩展 `ModifyTokenTokenizerFactory` 类，返回一个经过 13 位旋转的英文字符文本，也叫做
    rot-13。Rot-13 是一种非常简单的替换密码，它将一个字母替换为向后 13 个位置的字母。例如，字母 `a` 会被替换成字母 `n`，字母 `z`
    会被替换成字母 `m`。这是一个互逆密码，也就是说，应用两次同样的密码可以恢复原文。
- en: How to do it...
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现……
- en: 'We will invoke the `Rot13TokenizerFactory` class from the command line:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过命令行调用 `Rot13TokenizerFactory` 类：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You can see that the input text, which was mixed case and in normal English,
    has been transformed into its Rot-13 equivalent. You can see that the second time
    around, we passed the Rot-13 modified text as input and got the original text
    back, except that it was all lowercase.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到输入的文本，原本是大小写混合并且是正常的英文，已经转变为其 Rot-13 等价物。你可以看到第二次，我们将 Rot-13 修改过的文本作为输入，返回了原始文本，只是它变成了全小写。
- en: How it works...
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理是……
- en: '`Rot13TokenizerFactory` extends the `ModifyTokenTokenizerFactory` class. We
    will override the `modifyToken()` method, which operates a token at a time and,
    in this case, converts the token to its Rot-13 equivalent. There is a similar
    `modifyWhiteSpace` (String) method, which modifies the white spaces if required:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`Rot13TokenizerFactory` 扩展了 `ModifyTokenTokenizerFactory` 类。我们将重写 `modifyToken()`
    方法，它一次处理一个标记，在这个例子中，它将标记转换为其 Rot-13 等价物。还有一个类似的 `modifyWhiteSpace`（字符串）方法，如果需要，它会修改空格：'
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The start and end offsets of the tokens themselves remain the same as that of
    the underlying tokenizer. Here, we will use an Indo-European tokenizer as our
    base tokenizer. Filter it once through `LowerCaseTokenizer` and then through `Rot13Tokenizer`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 标记的起始和结束偏移量与底层标记器保持一致。在这里，我们将使用印欧语言标记器作为基础标记器。先通过 `LowerCaseTokenizer` 过滤一次，然后通过
    `Rot13Tokenizer` 过滤。
- en: 'The `rot13` method is:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`rot13` 方法是：'
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Finding words for languages without white spaces
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为没有空格的语言找到单词
- en: Languages such as Chinese do not have word boundaries. For example, 木卫三是围绕木星运转的一颗卫星，公转周期约为7天
    from Wikipedia is a sentence in Chinese that translates roughly into "Ganymede
    is running around Jupiter's moons, orbital period of about seven days" as done
    by the machine translation service at [https://translate.google.com](https://translate.google.com).
    Notice the absence of white spaces.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 像中文这样的语言没有单词边界。例如，木卫三是围绕木星运转的一颗卫星，公转周期约为7天，来自维基百科，这句话大致翻译为“Ganymede is running
    around Jupiter's moons, orbital period of about seven days”，这是机器翻译服务在 [https://translate.google.com](https://translate.google.com)
    上的翻译。注意到没有空格。
- en: Finding tokens in this sort of data requires a very different approach that
    is based on character-language models and our spell-checking class. This recipe
    encodes finding words by treating untokenized text as *misspelled* text, where
    the *correction* inserts a space to delimit tokens. Of course, there is nothing
    misspelled about Chinese, Japanese, Vietnamese, and other non-word delimiting
    orthographies, but we have encoded it in our spelling-correction class.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种数据中找到标记需要一种非常不同的方法，这种方法基于字符语言模型和我们的拼写检查类。这个方法通过将未标记的文本视为*拼写错误*的文本来编码查找单词，其中*修正*操作是在标记之间插入空格。当然，中文、日文、越南语和其他非单词分隔的书写系统并没有拼写错误，但我们已经在我们的拼写修正类中进行了编码。
- en: Getting ready
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will approximate non-word delimiting orthographies with de-white spaced English.
    This is sufficient to understand the recipe and can be easily modified to the
    actual language when needed. Get a 100,000 or so words of English and get them
    to the disk in UTF-8 encoding. The reason for fixing the encoding is that the
    input is assumed to be UTF-8—you can change it by changing the encoding and recompiling
    the recipe.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过去除空格来近似非单词分隔的书写系统。这足以理解这个方法，并且在需要时可以轻松修改为实际的语言。获取大约 100,000 个英文单词并将它们存储在
    UTF-8 编码的磁盘中。固定编码的原因是输入假定为 UTF-8——你可以通过更改编码并重新编译食谱来修改它。
- en: We used *A Connecticut Yankee in King Arthur's Court* by Mark Twain, downloaded
    from Project Gutenberg ([http://www.gutenberg.org/](http://www.gutenberg.org/)).
    Project Gutenberg is an excellent source of texts that are in the public domain,
    and Mark Twain is fine writer—we highly recommend the book. Place your selected
    text in the cookbook directory or work with our default.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了马克·吐温的《康涅狄格州的国王亚瑟宫廷人》（*A Connecticut Yankee in King Arthur's Court*），从古腾堡项目（[http://www.gutenberg.org/](http://www.gutenberg.org/)）下载。古腾堡项目是一个很好的公共领域文本来源，马克·吐温是位杰出的作家——我们强烈推荐这本书。将你选定的文本放在食谱目录中，或者使用我们的默认设置。
- en: How to do it...
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will run a program, play with it a bit, and explain what it does and how
    it does it, using the following steps:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行一个程序，稍微玩一下它，并解释它是如何工作的，使用以下步骤：
- en: 'Type the following command:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入以下命令：
- en: '[PRE32]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following is the output:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '[PRE33]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You might not get the perfect output. How good is Mark Twain at recovering
    proper white space from the Java program that generated it? Let''s find out:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能不会得到完美的输出。马克·吐温从生成它的 Java 程序中恢复正确空格的能力有多强呢？我们来看看：
- en: '[PRE34]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding way was not very good, but we are not being very fair; let''s
    use the concatenated source of LingPipe as training data:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之前的方法不是很好，但我们并不公平；让我们使用 LingPipe 的连接源作为训练数据：
- en: '[PRE35]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This is the perfect space insertion.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是完美的空格插入。
- en: How it works...
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'For all the fun and games, there is very little code involved. The cool thing
    is that we are building on the character-language models from [Chapter 1](ch01.html
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*. The source is in `src/com/lingpipe/chapter2/TokenizeWithoutWhiteSpaces.java`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有的有趣操作中，涉及的代码非常少。酷的是，我们在 [第 1 章](ch01.html "Chapter 1. Simple Classifiers")，*简单分类器*
    中的字符语言模型基础上构建。源代码位于 `src/com/lingpipe/chapter2/TokenizeWithoutWhiteSpaces.java`：
- en: '[PRE36]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The `main()` method starts up with the creation of `NgramProcessLM`. Next up,
    we will access a class for edit distance that is designed to only add spaces to
    a character stream. That's it. `Editdistance` is typically a fairly crude measure
    of string similarity that scores how many edits need to happen to to `string1`
    to make it the same as `string2`. A lot of information on this is Javadoc `com.aliasi.spell`.
    For example, `com.aliasi.spell.EditDistance` has an excellent discussion of the
    basics.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`main()` 方法通过创建 `NgramProcessLM` 开始。接下来，我们将访问一个只添加空格到字符流的编辑距离类。就这样。`Editdistance`
    通常是衡量字符串相似度的一个粗略指标，它计算将 `string1` 转换为 `string2` 所需的编辑次数。关于这一点的很多信息可以在 Javadoc
    `com.aliasi.spell` 中找到。例如，`com.aliasi.spell.EditDistance` 对基础概念有很好的讨论。'
- en: Note
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `EditDistance` class implements the standard notion of edit distance, with
    or without transposition. The distance without transposition is known as the Levenshtein
    distance, and with transposition, it is known as the Damerau-Levenstein distance.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`EditDistance` 类实现了标准的编辑距离概念，支持或不支持交换操作。不支持交换的距离被称为 Levenshtein 距离，支持交换的距离被称为
    Damerau-Levenshtein 距离。'
- en: Read the Javadoc with LingPipe; it has a lot of useful of information that we
    don't have space for in this book.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读 LingPipe 的 Javadoc；它包含了很多有用的信息，这些信息在本书中没有足够的空间介绍。
- en: 'So far we configured and constructed a `TrainSpellChecker` class. The next
    step is to naturally train it:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经配置并构建了 `TrainSpellChecker` 类。下一步自然是对其进行训练：
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We slurped up a text file, assuming it is UTF-8; if not, correct the character
    encoding and recompile. Then, we replaced all the multiple white spaces with a
    single one. This might not be the best move if multiple white spaces have meaning.
    This is followed by training, just like we trained language models in [Chapter
    1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了一个文本文件，假设它是 UTF-8 编码；如果不是，就需要纠正字符编码并重新编译。然后，我们将所有多余的空格替换为单一空格。如果多个空格有特殊意义，这可能不是最好的做法。接着，我们进行了训练，正如我们在
    [第1章](ch01.html "第1章。简单分类器")、*简单分类器* 中训练语言模型时所做的那样。
- en: 'Next up, we will compile and configure the spell checker:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编译和配置拼写检查器：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The next interesting line compiles `spellChecker`, which translates all the
    counts in the underlying language model to precomputed probabilities, which is
    much faster. The compilation step can write to a disk, so it can be used later
    without training; however, visit the Javadoc for `AbstractExternalizable` on how
    to do this. The next lines configure `CompiledSpellChecker` to only consider the
    edits that insert characters and to check for the exact string matches, but it
    forbids deletions, substitutions, and transpositions. Finally, only one insert
    is allowed. It should be clear that we are using a very limited portion of the
    capabilities of `CompiledSpellChecker`, but this is exactly what is called for—insert
    a space or don't.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行编译 `spellChecker`，它将基础语言模型中的所有计数转换为预计算的概率，这样会更快。编译步骤可以将数据写入磁盘，以便后续使用而不需要重新训练；不过，访问
    Javadoc 中关于 `AbstractExternalizable` 的部分，了解如何操作。接下来的几行配置 `CompiledSpellChecker`
    只考虑插入字符的编辑，并检查是否有完全匹配的字符串，但它禁止删除、替换和变换操作。最后，仅允许进行一次插入。显然，我们正在使用 `CompiledSpellChecker`
    的一个非常有限的功能集，但这正是我们需要的——要么插入空格，要么不插入。
- en: 'Last up is our standard I/O routine:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是我们的标准 I/O 例程：
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The mechanics of the `CompiledSpellChecker` and `WeightedEditDistance` classes
    are better described in either the Javadoc or the *Using edit distance and language
    models for spelling correction* recipe in [Chapter 6](ch06.html "Chapter 6. String
    Comparison and Clustering"), *String Comparison and Clustering*. However, the
    basic idea is that the string entered is compared to the language model just trained,
    resulting in a score that shows how good a fit this string is to the model. This
    string is going to be one huge word without any white spaces—but note that there
    is no tokenizer at work here, so the spell checker starts inserting spaces and
    reassessing the score of the resulting sequence. It keeps these sequences where
    insertion of spaces increases the score of the sequence.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`CompiledSpellChecker` 和 `WeightedEditDistance` 类的具体机制可以在 Javadoc 或者《*使用编辑距离和语言模型进行拼写纠正*》一书中的
    [第6章](ch06.html "第6章。字符串比较与聚类")、*字符串比较与聚类* 中得到更好的描述。然而，基本思想是：输入的字符串与刚训练好的语言模型进行比较，从而得到一个分数，表明该字符串与模型的契合度。这个字符串将是一个没有空格的大单词——但请注意，这里没有使用分词器，所以拼写检查器会开始插入空格，并重新评估生成序列的分数。它会保留那些插入空格后，分数提高的序列。'
- en: Remember that the language model was trained on text with white spaces. The
    spell checker tries to insert a space everywhere it can and keeps a set of "best
    so far" insertions of white spaces. In the end, it returns the best scoring series
    of edits.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，语言模型是在带有空格的文本上训练的。拼写检查器会尽量在每个可能的位置插入空格，并保持一组“当前最佳”的空格插入结果。最终，它会返回得分最高的编辑序列。
- en: Note that to complete the tokenizer, the appropriate `TokenizerFactory` needs
    to be applied to the white space-modified text, but this is left as an exercise
    for the reader.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，要完成分词器，必须对修改过空格的文本应用合适的 `TokenizerFactory`，但这留给读者作为练习。
- en: There's more...
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: '`CompiledSpellChecker` allows for an *n*-best output as well; this allows for
    multiple possible analyses of the text. In a high-coverage/recall situation such
    as a research search engine, it might serve to allow the application of multiple
    tokenizations. Also, the edit costs can be manipulated by extending the `WeightedEditDistance`
    class directly to tune the system.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`CompiledSpellChecker` 也支持 *n* 最优输出；这允许对文本进行多种可能的分析。在高覆盖率/召回率的场景下，比如研究搜索引擎，它可能有助于应用多个分词方式。此外，可以通过直接扩展
    `WeightedEditDistance` 类来调整编辑成本，从而调节系统的表现。'
- en: See also
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: It will be unhelpful to not actually provide non-English resources for this
    recipe. We built and evaluated a Chinese tokenizer using resources available on
    the web for research use. Our tutorial on Chinese word segmentation covers this
    in detail. You can find the Chinese word segmentation tutorial at [http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有实际提供非英语资源来支持这个配方，那么是没有帮助的。我们使用互联网上可用的资源为研究用途构建并评估了一个中文分词器。我们的中文分词教程详细介绍了这一点。你可以在[http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/chineseTokens/read-me.html)找到中文分词教程。
