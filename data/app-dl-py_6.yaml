- en: Model Evaluation and Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估与优化
- en: This chapter focuses on how to evaluate a neural network model. Different than
    working with other kinds of models, when working with neural networks, we modify
    the network's hyper parameters to improve its performance. However, before altering
    any parameters, we need to measure how the model performs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点讨论如何评估神经网络模型。与其他类型的模型不同，使用神经网络时，我们会调整网络的超参数以提高其性能。然而，在修改任何参数之前，我们需要先衡量模型的表现。
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Evaluate a model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型
- en: Explore the types of problems addressed by neural networks
  id: totrans-4
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索神经网络处理的不同类型问题
- en: Explore loss functions, accuracy, and error rates
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索损失函数、准确率和错误率
- en: Use TensorBoard
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorBoard
- en: Evaluate metrics and techniques
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估指标和技术
- en: Hyperparameter optimization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数优化
- en: Add layers and nodes
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加层和节点
- en: Explore and add epochs
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索并添加训练轮次
- en: Implement activation functions
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现激活函数
- en: Use regularization strategies
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则化策略
- en: Model Evaluation
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: 'In machine learning, it is common to define two distinct terms: parameter and
    hyper **parameter**. Parameters are properties that affect how a model makes predictions
    from data. Hyper parameters refer to how a model learns from data. Parameters
    can be learned from the data and modified dynamically. Hyper parameters are higher-level
    properties and are not typically learned from data. For a more detailed overview,
    refer to the book Python Machine Learning, by Sebastian Raschka and Vahid Mirjalili
    (Packt, 2017).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，常常定义两个不同的术语：参数和超**参数**。参数是影响模型如何从数据中做出预测的属性。超参数则是指模型如何从数据中学习。参数可以从数据中学习并动态修改，而超参数则是更高级的属性，通常不会从数据中学习。如需更详细的概述，请参考
    Sebastian Raschka 和 Vahid Mirjalili 所著的《Python 机器学习》（Packt，2017年）。
- en: Problem Categories
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题分类
- en: 'Generally, there are two categories of problems solved by neural networks:
    classification and regression. Classification problems regard the prediction of
    the right categories from data; for instance, if the temperature is hot or cold.
    Regression problems are about the prediction of values in a continuous scalar;
    for instance, what the actual temperature value is?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，神经网络解决的问题分为两类：分类和回归。分类问题是关于从数据中预测正确的类别；例如，温度是热还是冷。回归问题则是关于预测连续标量中的值；例如，实际的温度值是多少？
- en: 'Problems in these two categories are characterized by the following properties:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这两类问题的特点如下：
- en: '**Classification**: Problems that are characterized by categories. The categories
    can be different, or not; they can also be about a binary problem. However, they
    must be clearly assigned to each data element. An example of a classification
    problem would be to assign the label *car* or *not car* to an image using a Convolutional
    Neural Network. The MNIST example explored in C*hapter 4*, *Introduction to Neural
    Networks and Deep Learning*, is another example of a classification problem.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：以类别为特征的问题。类别可以不同，也可以相同；它们还可以是二分类问题。然而，每个数据元素必须清晰地被分配到某个类别。一个分类问题的例子是，使用卷积神经网络为图像分配标签
    *车* 或 *非车*。在第4章《神经网络与深度学习简介》中探索的 MNIST 示例是另一个分类问题的例子。'
- en: '**Regression**: Problems that are characterized by a continuous variable (that
    is, a scalar). These problems are measured in terms of ranges, and their evaluations
    regard how close to the real values the network is. An example is a time-series
    classification problem in which a Recurrent Neural Network is used to predict
    the future temperature values. The Bitcoin price-prediction problem is another
    example of a regression problem.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：以连续变量（即标量）为特征的问题。这些问题通过范围来衡量，并评估网络与真实值的接近程度。例如，一个时间序列分类问题，其中使用循环神经网络预测未来的温度值。比特币价格预测问题是另一个回归问题的例子。'
- en: While the overall structure of how to evaluate these models is the same for
    both of these problem categories, we employ different techniques for evaluating
    how models perform. In the following section, we explore these techniques for
    either classification or regression problems.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管评估这两类问题模型的整体结构相同，但我们会采用不同的技术来评估模型的表现。在接下来的部分中，我们将探讨分类或回归问题的评估技术。
- en: All of the code snippets in this chapter are implemented in *Activities 6 and
    7*. Feel free to follow along, but don't feel that it is mandatory, given that
    they will be repeated in more detail during the activities.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有代码片段都实现于*活动6和7*。欢迎跟着一起做，但不必感到强制性，因为它们将在活动中更详细地重复。
- en: Loss Functions, Accuracy, and Error Rates
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数、准确度和误差率
- en: Neural networks utilize functions that measure how the networks perform when
    compared to a validation set—that is, a part of the data separated to be used
    as part of the training process. These functions are called **loss functions**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络利用衡量网络与验证集（即从数据中分离出来作为训练过程一部分的部分数据）比较时的表现的函数。这些函数称为**损失函数**。
- en: Loss functions evaluate how *wrong* a neural network's predictions are; then
    they will propagate those errors back and make adjustments to the network, modifying
    how individual neurons are activated. Loss functions are key components of neural
    networks, and choosing the right loss function can have a significant impact on
    how the network performs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数评估神经网络预测的*错误*程度；然后它们会将这些错误反向传播并调整网络，改变单个神经元的激活方式。损失函数是神经网络的关键组件，选择合适的损失函数对网络性能有着重要影响。
- en: How are errors propagated to each neuron in a network?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 错误是如何传播到网络中的每个神经元的？
- en: Errors are propagated via a process called back propagation. Back propagation
    is a technique for propagating the errors returned by the loss function back to
    each neuron in a neural network. Propagated errors affect how neurons activate,
    and ultimately, how they influence the output of that network.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 错误通过一个称为反向传播的过程进行传播。反向传播是一种将损失函数返回的错误传播到神经网络中每个神经元的技术。传播的错误会影响神经元的激活方式，最终影响该网络的输出。
- en: Many neural network packages, including Keras, use this technique by default.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 包括Keras在内的许多神经网络包默认使用此技术。
- en: For more information about the mathematics of backpropagation, please refer
    to *Deep Learning* by Ian Goodfellow et. al., MIT Press, 2016.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于反向传播的数学内容，请参考*深度学习*（作者：Ian Goodfellow等，麻省理工学院出版社，2016年）。
- en: We use different loss functions for regression and classification problems.
    For classification problems, we use accuracy functions (that is, the proportion
    of times the predictions were correct). While for regression problems, we use
    error rates (that is, how close the predicted values were to the observed ones).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对回归和分类问题使用不同的损失函数。对于分类问题，我们使用准确率函数（即预测正确的比例）。而对于回归问题，我们使用误差率（即预测值与观测值的接近程度）。
- en: 'The following table provides a summary of common loss functions to utilize,
    alongside their common applications:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下表提供了常见损失函数的总结，并列出了它们的常见应用：
- en: '| **Problem Type**  | **Loss Function**  | **Problem**  | **Example**  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **问题类型**  | **损失函数**  | **问题**  | **示例**  |'
- en: '| Regression  | Mean Squared Error (MSE)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '| 回归  | 均方误差（MSE）'
- en: '| Predicting a continuous function. That is, predicting value within a range of
    values.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '| 预测一个连续函数。即在一系列值的范围内预测值。'
- en: '| Predicting the temperature in the future using temperature measurements from
    the past.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '| 使用过去的温度测量预测未来的温度。'
- en: '|'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Regression  | Root Mean Squared Error(RMSE)  | Same as preceding, but deals
    with negative values. RMSE typically provides more interpretable results.  | Same
    as preceding.  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 回归  | 均方根误差（RMSE）  | 与前面相同，但处理负值。RMSE通常提供更具可解释性的结果。  | 与前面相同。  |'
- en: '| Regression  | Mean Absolute Percentage Error'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '| 回归  | 平均绝对百分比误差'
- en: (MAPE)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (MAPE)
- en: '| Prediction continuous  functions. Has better in performance when working with
    de-normalized ranges.  | Predicting the sales for a product using the product properties
    (for example, price, type, target audience, market conditions).'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '| 预测连续函数。与去归一化范围工作时，表现更好。  | 使用产品属性（例如，价格、类型、目标受众、市场条件）预测产品的销售。'
- en: '|'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Classification  | Binary Cross entropy  |  Classification between two  categories
    or between two'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '| 分类  | 二元交叉熵  |  对两个类别或两者之间的分类'
- en: values (that is, `true`   or `false`).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 值（即，`true` 或 `false`）。
- en: '| Predicting if the visitor of a website is male or female based on their browser
    activity.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '| 根据网站访问者的浏览器活动预测其性别。'
- en: '|'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Classification  | Categorical Cross-entropy'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '| 分类  | 分类交叉熵'
- en: '| Classification between many categories from a known set'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '| 从已知类别集中分类多类问题'
- en: of categories.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 分类的类别数。
- en: '| Predicting the nationality of a speaker based on their accent when speaking
    a sentence in English.  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 根据说话者的口音预测其国籍，条件是说出一段英语句子。 |'
- en: For regression problems, the MSE function is the most common choice. While for
    classification problems, Binary Cross-entropy (for binary category problems) and
    Categorical Cross-entropy (for multi-category problems) are common choices. It
    is advised to start with these loss functions, then experiment with other functions
    as you evolve your neural network, aiming to gain performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，MSE 函数是最常用的选择。而对于分类问题，二元交叉熵（用于二分类问题）和多类别交叉熵（用于多分类问题）是常见的选择。建议从这些损失函数开始，然后在神经网络的发展过程中，尝试其他函数，以提升性能。
- en: For regression problems, the MSE function is the most common choice. While for
    classification problems, Binary Cross-entropy (for binary category problems) and
    Categorical Cross-entropy (for multi-category problems) are common choices. It
    is advised to start with these loss functions, then experiment with other functions
    as you evolve your neural network, aiming to gain performance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归问题，MSE 函数是最常用的选择。而对于分类问题，二元交叉熵（用于二分类问题）和多类别交叉熵（用于多分类问题）是常见的选择。建议从这些损失函数开始，然后在神经网络的发展过程中，尝试其他函数，以提升性能。
- en: The network we develop in C*hapter 5,* *Model Architecture*, uses the MSE as
    its loss function. In the following section, we explore how that function performs
    as the network trains.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在**第5章**中开发的网络使用MSE作为其损失函数。在接下来的章节中，我们将探讨该函数在网络训练过程中的表现。
- en: Different Loss Functions, Same Architecture
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同的损失函数，相同的架构
- en: Before moving ahead to the next section, let's explore, in practical terms,
    how these problems are different in the context of neural networks.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续进入下一节之前，让我们从实践角度探讨一下，这些问题在神经网络中的差异。
- en: 'The TensorFlow Playground application is made available by the TensorFlow team
    to help us understand how neural networks work. Here, we see a neural network
    represented with its layers: input (on the left), hidden layers (in the middle),
    and output (on the right).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Playground 应用程序由 TensorFlow 团队提供，帮助我们理解神经网络是如何工作的。在这里，我们看到一个神经网络，其中的层包括：输入层（左侧）、隐藏层（中间）和输出层（右侧）。
- en: We can also choose different sample datasets to experiment with on the far-left
    side. And, finally, on the far-right side, we see the output of the network.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以选择不同的样本数据集进行实验，位于最左侧。最后，在最右侧，我们看到网络的输出。
- en: '![](img/f7d64db8-e329-4ca4-9b37-4101dc07416b.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7d64db8-e329-4ca4-9b37-4101dc07416b.png)'
- en: 'Figure 1: TensorFlow Playground web application. Take the parameters for a
    neural network in this visualization to gain some'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：TensorFlow Playground 网页应用程序。在这个可视化中，使用神经网络的参数来获取一些
- en: intuition on how each parameter affects the model results.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地了解每个参数如何影响模型结果。
- en: 'This application helps us explore the different problem categories we discussed
    in our previous section. When we choose **Classification** as the **Problem type**
    (upper right-hand corner), the dots in the dataset are colored with only two color
    values: either blue or orange.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序帮助我们探索在前一节中讨论的不同问题类别。当我们选择**分类**作为**问题类型**（右上角）时，数据集中的点只有两种颜色值：蓝色或橙色。
- en: 'When we choose **Regression**, the colors of the dots are colored in a range
    of color values between orange and blue. When working on classification problems,
    the network evaluates its loss function based on how many blues and oranges the
    network has gotten wrong; and when working on classification problems, it checks
    how far away to the right color values for each dot the network was, as shown
    in the following image:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们选择**回归**时，点的颜色会在橙色和蓝色之间的色值范围内变化。在处理分类问题时，网络根据错误的蓝色和橙色数量来评估其损失函数；在处理分类问题时，它会检查每个点距离正确色值的远近，如下图所示：
- en: '![](img/2eaeef80-5b83-4c9b-b41d-5ac426e2dddb.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2eaeef80-5b83-4c9b-b41d-5ac426e2dddb.png)'
- en: 'Figure 2: Detail of the TensorFlow Playground application. Different color
    values are assigned to the dots,'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：TensorFlow Playground 应用程序的细节。不同的色值分配给点，
- en: depending on the problem type.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于问题类型。
- en: After clicking on the play button, we notice that the numbers in the Training
    loss area keep going down as the network continuously trains. The numbers are
    very similar in each problem category because the loss functions play the same
    role in both neural networks. However, the actual loss function used for each
    category is different, and is chosen depending on the problem type.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 点击播放按钮后，我们会注意到训练损失区域的数字随着网络不断训练而持续下降。每个问题类别中的数字非常相似，因为损失函数在两个神经网络中扮演着相同的角色。然而，每个类别所使用的实际损失函数是不同的，且根据问题类型选择。
- en: Using TensorBoard
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorBoard
- en: Evaluating neural networks is where TensorBoard excels. As explained in C*hapter
    4*, *Introduction to Neural Networks and Deep Learning*, TensorBoard is a suite
    of visualization tools shipped with TensorFlow. Among other things, one can explore
    the results of loss function evaluations after each epoch. A great feature of
    TensorBoard is that one can organize the results of each run separately and compare
    the resulting loss function metrics for each run. One can then make a decision
    on which hyper parameters to tune and have a general sense of how the network
    is performing. The best part is that it is all done in real time.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 评估神经网络是 TensorBoard 的强项。如同在*C*hapter 4*《神经网络与深度学习简介》中所解释的，TensorBoard 是随 TensorFlow
    一起提供的一套可视化工具。其中之一的功能是，在每个 epoch 后，可以探索损失函数评估结果。TensorBoard 的一个伟大特点是，用户可以单独组织每次运行的结果，并比较每次运行的损失函数指标。之后，用户可以决定需要调整哪些超参数，并对网络的表现有一个大致的了解。最棒的是，这一切都可以实时完成。
- en: 'In order to use TensorBoard with our model, we will use a Keras callback function.
    We do that by importing the `TensorBoard` callback and passing it to our model
    when calling its`fit()` function. The following code shows an example of how it
    would be implemented in the Bitcoin model created in our preceding chapters:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的模型中使用 TensorBoard，我们将使用 Keras 回调函数。我们通过导入 `TensorBoard` 回调函数，并在调用 `fit()`
    函数时将其传递给模型。以下代码展示了如何在我们之前章节中创建的比特币模型中实现：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*Snippet 1*: Snippet that implements a TensorBoard callback in our LSTM model'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段 1*：在我们的 LSTM 模型中实现 TensorBoard 回调函数的代码片段'
- en: Keras callback functions are called at the end of each epoch run. In this case,
    Keras calls the TensorBoard callback to store the results from each run on the
    disk. There are many other useful callback functions available, and one can create
    custom ones using the Keras API.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 回调函数在每个 epoch 运行结束时被调用。在这种情况下，Keras 调用 TensorBoard 回调函数，将每次运行的结果存储到磁盘中。还有许多其他有用的回调函数可以使用，用户也可以使用
    Keras API 创建自定义回调函数。
- en: Please refer to the Keras callback documentation ([https://keras.io/ callbacks/](https://keras.io/callbacks/))
    for more information.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息请参考 Keras 回调文档（[https://keras.io/callbacks/](https://keras.io/callbacks/)）。
- en: After implementing the TensorBoard callback, the `loss` function metrics are
    now available in the TensorBoard interface. You can now run a TensorBoard process
    (`with tensorboard --logdir=./logs`) and leave it running while you train your
    network with `fit()` . The main graphic to evaluate is typically called *loss*.
    One can add more metrics by passing known metrics to the metrics parameter in
    the `fit()` function; these will then be available for visualization in TensorBoard,
    but will not be used to adjust the network weights.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现 TensorBoard 回调函数后，`loss` 函数的指标现在可以在 TensorBoard 界面中查看。你可以运行 TensorBoard
    进程（`with tensorboard --logdir=./logs`）并在训练网络时保持它运行（使用 `fit()`）。评估的主要图形通常称为*损失*。用户可以通过将已知指标传递给
    `fit()` 函数中的 metrics 参数来添加更多指标，这些指标将在 TensorBoard 中进行可视化，但不会用于调整网络权重。
- en: The interactive graphics will continue to update in real time, which allows
    you to understand what is happening on every epoch.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式图形将继续实时更新，帮助你理解每个 epoch 中发生的情况。
- en: '![](img/0edbd6fa-b1b7-4401-a6ab-1be71880004c.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0edbd6fa-b1b7-4401-a6ab-1be71880004c.png)'
- en: 'Figure 3: Screenshot of a TensorBoard instance showing the loss function results
    alongside other metrics added to the metrics parameter'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：TensorBoard 实例的截图，显示了损失函数结果以及添加到指标参数中的其他指标
- en: Implementing Model Evaluation Metrics
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现模型评估指标
- en: 'In both regression and classification problems, we split the input dataset
    into three other datasets: train, validation, and test. Both the train and the
    validation sets are used to train the network. The train set is used by the network
    as an input, and the validation set is used by the loss function to compare the
    output of the neural network to the real data, computing how wrong the predictions
    are. Finally, the test set is used after the network has been trained to measure
    how the network can perform on data it has never seen before.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归和分类问题中，我们将输入数据集拆分为三个数据集：训练集、验证集和测试集。训练集和验证集用于训练网络。训练集作为输入提供给网络，验证集由损失函数用来将神经网络的输出与真实数据进行比较，并计算预测的误差。最后，测试集在网络训练完毕后用于评估网络在未见过的数据上的表现。
- en: There isn't a clear rule for determining how the train, validation, and test
    datasets must be divided. It is a common approach to divide the original dataset
    as 80 percent train and 20 percent test, then to further divide the train dataset
    into 80 percent train and 20 percent validation. For more information about this
    problem, please refer to the book *Python Machine Learning*, by Sebastian Raschka
    and Vahid Mirjalili (Packt, 2017).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 并没有明确的规则来确定训练集、验证集和测试集应如何划分。常见的方法是将原始数据集划分为 80% 的训练集和 20% 的测试集，然后将训练集进一步划分为
    80% 的训练集和 20% 的验证集。有关此问题的更多信息，请参考 Sebastian Raschka 和 Vahid Mirjalili 合著的《*Python
    机器学习*》（Packt，2017）。
- en: 'In classification problems, you pass both the data and the labels to the neural
    network as related but distinct data. The network then learns how data is related
    to each label. In regression problems, instead of passing data and labels, one
    passes the variable of interest as one parameter and the variables used for learning
    patterns as another. Keras provides an interface for both of those use cases with
    the `fit()` method. See *Snippet 2* for an example:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类问题中，您将数据和标签作为相关但不同的数据传递给神经网络。网络随后学习数据如何与每个标签相关。在回归问题中，您不传递数据和标签，而是将感兴趣的变量作为一个参数传递，将用于学习模式的变量作为另一个参数。Keras
    为这两种用例提供了接口，即 `fit()` 方法。请参见 *代码片段 2* 了解示例：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*Snippet 2*: Snippet that illustrates how to use the `validation_split and validation_data`
    parameters'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段 2*：演示如何使用 `validation_split` 和 `validation_data` 参数的代码片段'
- en: The `fit()` method can use either the `validation_split` or the `validation_data`
    parameter, but not both at the same time.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()` 方法可以使用 `validation_split` 或 `validation_data` 参数，但不能同时使用这两个参数。'
- en: Loss functions evaluate the progress of models and adjust their weights on every
    run. However, loss functions only describe the relationship between training data
    and validation data. In order to evaluate if a model is performing correctly,
    we typically use a third set of data—which is not used to train the network—and
    compare the predictions made by our model to the values available in that set
    of data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数评估模型的进展并在每次运行时调整其权重。然而，损失函数仅描述训练数据和验证数据之间的关系。为了评估模型是否正确执行，我们通常使用第三组数据——即未用于训练网络的数据——并将我们模型的预测结果与该数据集中的实际值进行比较。
- en: 'That is the role of the test set. Keras provides the method `model.evaluate()`,
    which makes the process of evaluating a trained neural network against a test
    set easy. See the following code for an example:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是测试集的作用。Keras 提供了 `model.evaluate()` 方法，使得将训练好的神经网络与测试集进行评估的过程变得简单。请参见以下代码了解示例：
- en: '[PRE2]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Snippet 3*: Snippet that illustrates how to use the `evaluate()` method'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段 3*：演示如何使用 `evaluate()` 方法的代码片段'
- en: The `evaluate()` method returns both the results of the loss function and the
    results of the functions passed to the `metrics` parameter. We will be using that
    function frequently in the Bitcoin problem to test how the model performs on the
    test set.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate()` 方法返回损失函数的结果以及传递给 `metrics` 参数的函数结果。在比特币问题中，我们将频繁使用该方法来测试模型在测试集上的表现。'
- en: You will notice that the Bitcoin model looks a bit different than the example
    above. That is because we are using an LSTM architecture. LSTMs are designed to
    predict sequences. Because of that, we do not use a set of variables to predict
    a different single variable—even if it is a regression problem. Instead, we use
    previous observations from a single variable (or set of variables) to predict
    future observations of that same variable (or set). The `y` parameter on `Keras.fit()`
    contains the same variable as the `x` parameter, but only the predicted sequences.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，比特币模型看起来与上述示例有些不同。这是因为我们使用了LSTM架构。LSTM被设计用来预测序列。因此，我们不使用一组变量来预测另一个单一变量——即使它是回归问题。相反，我们使用单一变量（或一组变量）的先前观察值来预测该变量（或变量组）未来的观察值。在`Keras.fit()`的`y`参数包含了与`x`参数相同的变量，只不过是预测的序列。
- en: Evaluating the Bitcoin Model
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估比特币模型
- en: We created a test set during our activities in C*hapter 4*, *Introduction to
    Neural Networks and Deep Learning*. That test set has 19 weeks of Bitcoin daily
    price observations, which is equivalent to about 20 percent of the original dataset.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*C*hapter 4*中创建了一个测试集，*神经网络与深度学习简介*。该测试集包含了19周的比特币每日价格观察数据，相当于原始数据集的约20％。
- en: We have also trained our neural network using the other 80 percent of data (that
    is, the train set with 56 weeks of data, minus one for the validation set) in
    C*hapter 5*, *Model Architecture*, and stored the trained network on disk (`bitcoin_lstm_v0`).
    We can now use the `evaluate()` method in each one of the 19 weeks of data from
    the test set and inspect how that fist neural network performs.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用数据集的其他80％（即包含56周数据的训练集，减去一个用于验证集的数据）在*C*hapter 5*中训练了神经网络，*模型架构*，并将训练好的网络存储在磁盘上（`bitcoin_lstm_v0`）。现在，我们可以在测试集的每一周（共19周）使用`evaluate()`方法，查看该神经网络的表现。
- en: 'In order to do that, though, we have to provide 76 preceding weeks. We have
    to do this because our network has been trained to predict one week of data using
    exactly 76 weeks of continuous data (we will deal with this behavior by re-training
    our network periodically with larger periods in C*hapter 7*, *Productization*,
    when we deploy a neural network as a web application):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要做到这一点，我们必须提供前76周的数据。这是因为我们的网络被训练为通过连续的76周数据预测一个周的数据（我们将在*C*hapter 7*中重新训练我们的网络，以使用更长时间的数据周期，并在*产品化*章节中讨论将神经网络部署为Web应用程序时处理此行为）：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Snippet 4*: Snippet that implements the `evaluate()` method to evaluate the
    performance of our model in a test dataset'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*Snippet 4*：实现`evaluate()`方法以评估我们模型在测试数据集上表现的代码片段'
- en: 'In the preceding code, we evaluate each week using Keras'' `model.evaluate()`
    , then store its output in the variable evaluated_weeks. We then plot the resulting
    MSE for each week in the following figure:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用Keras的`model.evaluate()`评估每一周的数据，然后将输出存储在变量`evaluated_weeks`中。接着，我们将每周的MSE结果绘制在下图中：
- en: '![](img/8466d5dd-ea61-4ad9-8b40-71ce88f7b87e.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8466d5dd-ea61-4ad9-8b40-71ce88f7b87e.png)'
- en: 'Figure 4: MSE for each week in the test set; notice that in week 5, the model
    predictions are worse than in any other week'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：测试集每周的MSE；注意，在第5周，模型预测的结果比其他任何一周都要差。
- en: The resulting MSE from our model suggests that our model performs well during
    most weeks, except for week 5, when its value increases to about `0.08`. Our model
    seems to be performing well for almost all of the other test weeks
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的MSE结果表明，大多数周我们的模型表现良好，除了第5周，其MSE值上升至约`0.08`。除了第5周，我们的模型似乎在几乎所有其他测试周的表现都很好。
- en: Overfitting
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合
- en: Our first trained network (`bitcoin_lstm_v0`) may be suffering from a phenomenon
    known as overfitting. Overfitting is when a model is trained to optimize a validation
    set, but it does so at the expense of more generalizable patterns from the phenomenon
    we are interested in predicting. The main issue with overfitting is that a model
    learns how to predict the validation set, but fails to predict new data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个训练过的网络（`bitcoin_lstm_v0`）可能正遭受一种叫做过拟合的现象。过拟合是指模型在训练时优化验证集，但这样做牺牲了从我们感兴趣的现象中提取更具普适性的模式。过拟合的主要问题是，模型学会了如何预测验证集数据，但无法预测新的数据。
- en: 'The loss function used in our model reaches very low levels (about 2.9 * 10-6)
    at the end of our training process. Not only that, but this happens early: the
    MSE loss function used to predict the last week in our data decreases to a stable
    plateau in about epoch 30\. This means that our model is predicting the data from
    week 77 almost perfectly, using the preceding 76 weeks. Could this be the result
    of overfiting?'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型中使用的损失函数在训练结束时达到了非常低的水平（约为2.9 * 10^-6）。不仅如此，这个过程发生得很早：用于预测数据中最后一周的MSE损失函数在大约第30个训练周期时降低到了一个稳定的水平。这意味着我们的模型几乎完美地预测了第77周的数据，使用了前76周的数据。难道这可能是过拟合的结果吗？
- en: Let's look at *Figure 4* again. We know that our LSTM model reaches extremely
    low values in our validation set (about 2.9 * 10-6), yet it also reaches low values
    in our test set. The key difference, however, is in the scale. The MSE for each
    week in our test set is about 4,000 times bigger (on average) than in the test
    set. This means that the model is performing much worse in our test data than
    in the validation set. This is worth considering.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再看一下*图4*。我们知道我们的LSTM模型在验证集上达到了极低的值（约为2.9 * 10^-6），但它在测试集上也达到了较低的值。然而，关键的区别在于尺度。我们测试集中每一周的MSE大约比验证集高出4,000倍（平均而言）。这意味着模型在我们的测试数据上的表现比在验证集上要差得多。这一点值得关注。
- en: 'The scale, though, hides the power of our LSTM model: even performing much
    worse in our test set, the predictions'' MSE errors are still very, very low.
    That suggests that our model may be learning patterns from the data.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尺度掩盖了我们LSTM模型的能力：即使在测试集上的表现要差得多，预测的MSE误差仍然非常非常低。这表明我们的模型可能正在从数据中学习到模式。
- en: Model Predictions
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型预测
- en: One thing is to measure our model comparing MSE errors, and another is to be
    able to interpret its results intuitively.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面是通过比较MSE误差来衡量我们的模型，另一方面是能够直观地解释其结果。
- en: 'Using the same model, let''s now create a series of predictions for the following
    weeks, using 76 weeks as input. We do that by sliding a window of 76 weeks over
    the complete series (that is, train plus test sets), and making predictions for
    each of those windows. Predictions are done using the Keras `model.predict()`
    method:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的模型，接下来我们将使用76周的数据作为输入，为接下来的几周生成一系列预测。我们通过将76周的滑动窗口应用到完整的数据序列上（即训练集加测试集），并为每个窗口做出预测来实现。预测是通过Keras的`model.predict()`方法完成的：
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '*Snippet 5*: Snippet that uses the `model.predict()` method for making predictions for
    all the weeks of the test dataset'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段5*：使用`model.predict()`方法为测试数据集中的所有周做出预测的代码片段'
- en: 'In the preceding code, we make predictions using `model.predict()`, then store
    these predictions in the `predicted_weeks` variable. We then plot the resulting
    predictions, making the following figure:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用`model.predict()`进行预测，然后将这些预测存储在`predicted_weeks`变量中。接着我们绘制了结果预测图，得到了以下图形：
- en: '![](img/7155c427-aa64-4b61-b128-876340bf24dc.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7155c427-aa64-4b61-b128-876340bf24dc.png)'
- en: '*Figure 5*: MSE for each week in the test set. Notice that in week 5, the model
    predictions are worse than in any other week.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5*：测试集中每一周的MSE。请注意，在第5周，模型的预测比其他任何一周都要差。'
- en: The results of our model (as shown in *Figure 5*) suggest that its performance
    isn't all that bad. By observing the pattern from the Predicted line, one can
    notice that the network has identifiled a fluctuating pattern happening on a weekly
    basis, in which the normalized prices go up in the middle of the week, then down
    by the end of it. With the exception of a few weeks—most notably week 5, the same
    from our previous MSE analysis—most weeks fall close to the correct values.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的结果（如*图5*所示）表明，它的表现并没有那么糟糕。通过观察预测线的模式，可以发现网络已经识别出一个每周波动的模式，其中标准化的价格在周中会上升，然后在周末下降。除了几个星期——最显著的是第5周，与我们之前的MSE分析相同——大多数周的数据都接近正确值。
- en: 'Let''s now denormalize the predictions so that we can investigate the prediction
    values using the same scale as the original data (that is, US Dollars). We can
    do this by implementing a denormalization function that uses the day index from
    the predicted data to identify the equivalent week on the test data. After that
    week is identified, the function then takes the fist value of that week and uses
    that value to denormalize the predicted values by using the same point-relative
    normalization technique, but inverted:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们去归一化预测结果，以便使用与原始数据相同的尺度（即美元）来调查预测值。我们可以通过实现一个去归一化函数来做到这一点，该函数利用预测数据中的日期索引来识别测试数据中相应的一周。确定该周后，函数会取该周的第一个值，并使用该值通过倒置的点相对归一化技术去归一化预测值：
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Snippet 6*: De-normalization of data using an inverted point-relative normalization
    technique. The `denormalize()` function takes the first closing price from the
    test''s first day of an equivalent week.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段 6*：使用倒置的点相对归一化技术对数据进行去归一化处理。`denormalize()`函数取自测试集第一天的第一个收盘价，作为与之对应的那一周的数据。'
- en: Our results now compare the predicted values with the test set, using US Dollars.
    As seen in Figure 5, the `bitcoin_lstm_v0` model seems to perform quite well in
    predicting the Bitcoin prices for the following seven days. But, how can we measure
    that performance in interpretable terms?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果现在通过美元与测试集进行对比。如图 5 所示，`bitcoin_lstm_v0` 模型在预测未来七天比特币价格方面表现得相当不错。但是，我们如何用易于理解的方式来衡量这个表现呢？
- en: '![](img/cd53113f-988e-4517-8fbf-de72c73fcf2f.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd53113f-988e-4517-8fbf-de72c73fcf2f.png)'
- en: 'Figure 6: MSE for each week in the test set; notice that in week 5, the model
    predictions are worse than in any other week'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：测试集中每周的均方误差（MSE）；注意到在第5周，模型预测的结果比其他任何一周都要差。
- en: Interpreting Predictions
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释预测
- en: Our last step is to add interpretability to our predictions. Figure 6 seems
    to show that our model prediction matches the test data somewhat closely, but
    how closely?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一步是为我们的预测增加可解释性。图 6 显示我们的模型预测与测试数据相对接近，但到底有多接近呢？
- en: Keras' `model.evaluate()` function is useful for understanding how a model is
    performing at each evaluation step. However, given that we are typically using
    normalized datasets to train neural networks, the metrics generated by the `model.evaluate()`
    method are also hard to interpret.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 的 `model.evaluate()` 函数对于理解模型在每次评估步骤中的表现非常有用。然而，鉴于我们通常使用归一化数据集来训练神经网络，`model.evaluate()`
    方法生成的指标也很难解释。
- en: 'In order to solve that problem, we can collect the complete set of predictions
    from our model and compare it with the test set using two other functions from
    *Table 1* that are easier to interpret: MAPE and RMSE, implemented as `mape()`
    and `rmse()` , respectively:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以收集模型的完整预测集，并使用 *表 1* 中的另外两个更容易解释的函数将其与测试集进行比较：分别是 `mape()` 和 `rmse()`，它们分别表示
    MAPE 和 RMSE：
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '*Snippet 7*: Implementation of the *mape()* and *rmse()* functions'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段 7*：`mape()` 和 `rmse()` 函数的实现'
- en: These functions are implemented using `NumPy`. Original implementations come
    from [https://stats.stackexchange.com/ questions/58391/mean-absolute-percentage-error-mapein-scikit-learn](https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn)
    (MAPE) and [https://stackoverflow.com/ questions/16774849/mean-squared-error-in-numpy](https://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy)
    (RMSE).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数是使用 `NumPy` 实现的。原始实现来自 [https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn](https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn)（MAPE）和
    [https://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy](https://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy)（RMSE）。
- en: 'After comparing our test set with our predictions using both of those functions,
    we have the following results:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这两个函数将我们的测试集与预测结果进行比较后，我们得到了以下结果：
- en: 'Denormalized RMSE: $399.6'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去归一化后的 RMSE：$399.6
- en: 'Denormalized MAPE: 8.4 percent'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去归一化后的 MAPE：8.4%
- en: This indicates that our predictions differ, on average, about $399 from real
    data. That represents a difference of about 8.4 percent from real Bitcoin prices.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的预测与真实数据的差异平均约为 $399。这相当于与实际比特币价格的差异大约为 8.4%。
- en: These results facilitate the understanding of our predictions. We will continue
    to use the model.evaluate() method to keep track of how our LSTM model is improving,
    but will also compute both `rmse()` and `mape()` on the complete series on every
    version of our model to interpret how close we are to predicting Bitcoin prices.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果有助于理解我们的预测。我们将继续使用`model.evaluate()`方法来跟踪我们的LSTM模型如何改进，同时也会计算每个版本模型在完整系列上的`rmse()`和`mape()`，以解释我们在预测比特币价格时的准确度。
- en: Activity:Creating an Active Training Environment
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动：创建一个主动的训练环境
- en: In this activity, we create a training environment for our neural network that
    facilitates both its training and evaluation. This environment is particularly
    important to our next chapter, in which we search for an optimal combination of
    hyperparameters. F
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，我们为神经网络创建了一个训练环境，促进其训练和评估。这个环境对于下一章尤为重要，在那一章中，我们将寻找最佳的超参数组合。
- en: First, we will start both a Jupyter Notebook instance and a TensorBoard instance.
    Both of these instances can remain open for the remainder of this activity.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将启动一个Jupyter Notebook实例和一个TensorBoard实例。接下来的活动中，这两个实例可以保持打开状态。
- en: 'Using your terminal, navigate to the directory chapter_6/activity_6 and execute the
    following code to start a Jupyter Notebook instance:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用终端，导航到目录`chapter_6/activity_6`，并执行以下代码以启动Jupyter Notebook实例：
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Open the URL provided by the application in your browser and open the Jupyter
    Notebook named  `Activity_6_Creating_an_active_training_environment. ipynb`:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中打开应用程序提供的URL，并打开名为`Activity_6_Creating_an_active_training_environment.ipynb`的Jupyter
    Notebook：
- en: '![](img/f9ad3585-3a3c-440d-b284-4d7acf35007b.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9ad3585-3a3c-440d-b284-4d7acf35007b.png)'
- en: 'Figure 7: Jupyter Notebook highlighting the section Evaluate LSTM Model'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：Jupyter Notebook中高亮显示的“评估LSTM模型”部分
- en: 'Also using your terminal, start a TensorBoard instance by executing the following command:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，使用终端启动一个TensorBoard实例，执行以下命令：
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Open the URL that appears on the screen and leave that browser tab open, as
    well.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开屏幕上出现的URL，并保持该浏览器标签页打开。
- en: Now, load both the training (`train_dataset.csv`) and the test set (`test_dataset.
    csv`), and also our previously compiled model (`bitcoin_lstm_v0.h5`), into the
    Notebook.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将训练集（`train_dataset.csv`）和测试集（`test_dataset.csv`）以及我们之前编译的模型（`bitcoin_lstm_v0.h5`）加载到Notebook中。
- en: 'Load the train and test datasets in the Jupyter Notebook instance using:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将训练集和测试集加载到Jupyter Notebook实例中：
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Also, load our previously compiled model using the following command:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，使用以下命令加载我们之前编译的模型：
- en: '[PRE10]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let us now evaluate how our model performed against test data. Our model is
    trained using 76 weeks to predict a week into the future—that is, the following
    sequence of seven days. When we built our first model, we divided our original
    dataset between a training and a test set. We will now take a combined version
    of both datasets (let's call it combined set) and move a sliding window of 76
    weeks. At each window, we execute Keras' `model.evaluate()` method to evaluate
    how the network performed on that specific week.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估模型在测试数据上的表现。我们的模型使用76周的数据来预测未来一周的情况，即接下来的七天。当我们构建第一个模型时，我们将原始数据集分为训练集和测试集。现在，我们将合并这两个数据集（我们称之为合并集），并滑动一个76周的窗口。在每个窗口中，我们执行Keras的`model.evaluate()`方法，评估网络在该特定周的表现。
- en: 'Execute the cells under the header Evaluate LSTM Model. The key concept of
    these cells it to call the model.evaluate() method for each of the weeks in the
    test set. This line is the most important:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行“评估LSTM模型”标题下的单元格。这些单元格的关键概念是对测试集中的每一周调用`model.evaluate()`方法。以下这一行是最重要的：
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Each evaluation result is now stored in the variable `evaluated_weeks`. That
    variable is a simple array containing the sequence of MSE predictions for every
    week in the test set. Go ahead and also plot these results:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个评估结果现在存储在变量`evaluated_weeks`中。这个变量是一个简单的数组，包含测试集中每一周的MSE预测结果。现在可以继续绘制这些结果：
- en: '![](img/20414581-6c03-4a10-8498-f1fda7532ca2.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20414581-6c03-4a10-8498-f1fda7532ca2.png)'
- en: As discussed during our chapter, the MSE loss function is difficult to interpret.
    To facilitate our understanding of how our model is performing, we also call the
    method `model.predict()` on each week from the test set and compare its predicted
    results with the set's values.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在章节中讨论的那样，MSE损失函数很难解释。为了便于理解模型的表现，我们还会对测试集中的每一周调用`model.predict()`方法，并将其预测结果与实际值进行比较。
- en: 'Navigate to the section **Interpreting Model** Results and execute the code
    cells under the sub-header **Make Predictions**. Notice that we are calling the
    method `model.predict()` , but with a slightly different combination of parameters.
    Instead of using both `X` and `Y` values, we only use `X`:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到**解释模型**结果部分并执行**做出预测**子标题下的代码单元格。请注意，我们正在调用`model.predict()`方法，但使用的是稍有不同的参数组合。我们只使用`X`，而不是同时使用`X`和`Y`值：
- en: '[PRE12]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'At each window, we will issue predictions for the following week and store
    the results. We can now plot the normalized results alongside the normalized values from
    the test set, as shown in the following figure:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个窗口，我们将对下一周进行预测并存储结果。我们现在可以将归一化结果与测试集中的归一化值进行比较，如下图所示：
- en: '![](img/778403d5-d6b1-4c38-8d19-f54824595bcd.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/778403d5-d6b1-4c38-8d19-f54824595bcd.png)'
- en: 'Figure 9: Plotting the normalized values returned from *model.predict()* for
    each week of the test set'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：绘制从*model.predict()*返回的每周归一化值
- en: We will also make the same comparisons but using de-normalized values. In order
    to de-normalize our data, we must first identify the equivalent week between the
    test set and the predictions. Then, we take the first price value for that week
    and use it to reverse the point-relative normalization equation from C*hapter
    5*, *Model Architecture*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也将进行相同的比较，但使用去归一化后的值。为了去归一化我们的数据，我们首先需要识别测试集和预测结果之间的等效周。然后，我们取该周的第一个价格值，并用它来反转第*5章*中的基于点的归一化方程，*模型架构*。
- en: Navigate to the header Denormalizing Predictions and execute all cells under
    that header.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到标题“去归一化预测”并执行该标题下的所有单元格。
- en: 'In this section, we defiled the function `denormalize()`, which performs the
    complete de-normalization process. Different than other functions, this function
    takes in a Pandas DataFrame instead of a NumPy array. We do so for using dates
    as an index. This is the most relevant cell block from that header:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一部分，我们定义了`denormalize()`函数，它执行完整的去归一化过程。与其他函数不同，这个函数接受的是一个Pandas DataFrame，而不是NumPy数组。我们这样做是为了使用日期作为索引。这是该部分标题下最相关的代码块：
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Our de-normalized results (as seen in the following figure) show that our model
    makes predictions that are close to the real Bitcoin prices. But how close?
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的去归一化结果（如以下图所示）显示，我们的模型做出的预测与实际比特币价格非常接近。但到底有多接近呢？
- en: '![](img/1ac188a1-60c6-4533-ab9f-48d4675e0268.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ac188a1-60c6-4533-ab9f-48d4675e0268.png)'
- en: 'Figure 10: Plotting the de-normalized values returned from `model.predict()`
    for each week of the test set'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：绘制从`model.predict()`返回的每周去归一化值
- en: The LSTM network uses MSE values as its loss function. However, as discussed
    during the chapter, MSE values are difficult to interpret. To solve that, we implement
    two functions (loaded from the `script utilities.py`) that implement the functions
    RMSE and MAPE. Those functions add interpretability to our model by returning
    a measurement in the same scale that our original data used, and by comparing
    the difference in scale as a percentage.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM网络使用均方误差（MSE）值作为其损失函数。然而，正如在章节中讨论的，MSE值难以解释。为了解决这个问题，我们实现了两个函数（从`script
    utilities.py`中加载），它们分别实现了RMSE和MAPE函数。这些函数通过返回与我们原始数据使用相同量纲的度量，并通过将量纲差异作为百分比进行比较，从而为我们的模型增加了解释性。
- en: 'Navigate to the header De-normalizing Predictions and load two functions from
    the `utilities.py` script:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到“去归一化预测”标题并从`utilities.py`脚本中加载两个函数：
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The functions from the script are actually really simple:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本中的函数实际上非常简单：
- en: '[PRE15]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Each function is implemented using NumPy's vector-wise operations. They work
    well in vectors of the same length. They are designed to be applied on a complete
    set of results.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 每个函数都是通过NumPy的向量化操作来实现的。它们在相同长度的向量上运行良好。它们被设计用于应用于完整的结果集。
- en: Using the `mape()` function, we can now understand that our model predictions
    are about 8.4 percent away from the prices from the test set. This is equivalent
    to a root mean squared error (calculated using the `rmse()` function) of about
    $399.6.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`mape()`函数，我们现在可以理解，我们的模型预测结果与测试集的价格相差大约8.4%。这相当于根均方误差（使用`rmse()`函数计算）大约为399.6美元。
- en: Before moving on to the next section, go back into the Notebook and find the
    header **Re-train Model with TensorBoard**. You may have noticed that we created
    a helper function called `train_model()` . This function is a wrapper around our
    model that trains (`using model.fit()` ) our model, storing its respective results
    under a new directory. Those results are then used by TensorBoard as a discriminator,
    in order to display statistics for different models.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入下一节之前，回到Notebook中找到标题为**使用TensorBoard重新训练模型**的部分。你可能已经注意到我们创建了一个名为`train_model()`的辅助函数。这个函数是我们模型的封装器，它训练（`using
    model.fit()`）我们的模型，并将其结果存储在一个新的目录下。TensorBoard随后将这些结果作为判别器，显示不同模型的统计数据。
- en: 'Go ahead and modify some of the values for the parameters passed to the `model.
    fit()` function (try epochs, for instance). Now, run the cells that load the model
    into memory from disk (this will replace your trained model):'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请修改传递给`model.fit()`函数的一些参数值（例如试试epochs）。现在，运行从磁盘加载模型到内存的单元（这将替换你训练过的模型）：
- en: '[PRE16]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, run the `train_model()` function again, but with different parameters,
    indicating a new run version:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，再次运行`train_model()`函数，但使用不同的参数，表示一个新的运行版本：
- en: '[PRE17]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this section, we learned how to evaluate a network using loss functions.
    We learned that loss functions are key elements of neural networks, as they evaluate
    the performance of a network at each epoch and are the starting point for the
    propagation of adjustments back into layers and nodes. We also explored why some
    loss functions can be difficult to interpret (for instance, the MSE) and developed
    a strategy using two other functions— RMSE and MAPE—to interpret the predicted
    results from our LSTM model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用损失函数评估网络。我们了解到，损失函数是神经网络的关键元素，它们评估网络在每个epoch的表现，并且是将调整回传到层和节点的起点。我们还探讨了为什么一些损失函数可能难以解释（例如MSE），并通过使用另外两个函数——RMSE和MAPE——来解释我们LSTM模型的预测结果。
- en: Most importantly, this chapter concludes with an active training environment.
    We now have a system that can train a deep learning model and evaluate its results
    continuously. This will be key when we move to optimizing our network in the next
    session.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，本章以一个主动训练环境作为结尾。我们现在拥有一个能够持续训练深度学习模型并评估其结果的系统。这将在我们下一节优化网络时发挥关键作用。
- en: Hyperparameter Optimization
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数优化
- en: We have trained a neural network to predict the next seven days of Bitcoin prices
    using the preceding 76 weeks of prices. On average, that model issues predictions
    that are about 8.4 percent distant from real Bitcoin prices.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练了一个神经网络，利用前76周的比特币价格预测接下来七天的比特币价格。平均来说，这个模型给出的预测值与实际比特币价格之间的误差约为8.4%。
- en: 'This section describes common strategies for improving the performance of neural
    network models:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了提高神经网络模型性能的常见策略：
- en: Adding or removing layers and changing the number of nodes
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加或移除层并更改节点数量
- en: Increasing or decreasing the number of training epochs
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加或减少训练的epoch次数
- en: Experimenting with different activation functions
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的激活函数
- en: Using different regularization strategies
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的正则化策略
- en: We will evaluate each modification using the same active learning environment
    developed by the end of the *Model Evaluation* section, measuring how each one
    of these strategies may help us develop a more precise model.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用到目前为止在*模型评估*部分开发的相同主动学习环境来评估每个修改，衡量这些策略如何帮助我们开发出更精确的模型。
- en: Layers and Nodes - Adding More Layers
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层和节点 - 添加更多层
- en: 'Neural networks with single hidden layers can perform fairly well on many problems.
    Our fist Bitcoin model (`bitcoin_lstm_v0`) is a good example: it can predict the
    next seven days of Bitcoin prices (from the test set) with error rates of about
    8.4 percent using a single LSTM layer. However, not all problems can be modeled
    with single layers.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 单隐层神经网络在许多问题上可以表现得相当好。我们的第一个比特币模型（`bitcoin_lstm_v0`）就是一个很好的例子：它使用单个LSTM层，能够预测接下来七天的比特币价格（来自测试集），误差率约为8.4%。然而，并不是所有问题都能用单层模型建模。
- en: The more complex the function that you are working to predict is, the higher
    the likelihood that you will need to add more layers. A good intuition to determine
    whether adding new layers is a good idea is to understand what their role in a
    neural network is.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你要预测的函数越复杂，你需要添加更多层的可能性就越高。判断是否应该添加新层的一个好直觉是了解它们在神经网络中的作用。
- en: Each layer creates a model representation of its input data. Earlier layers
    in the chain create lower-level representations, and later layers, higher-level.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层都会创建输入数据的模型表示。链条中的早期层创建较低级别的表示，而后期层则创建更高级别的表示。
- en: 'While that description may be difficult to translate into real-world problems,
    its practical intuition is simple: when working with complex functions that have
    different levels of representation, you may want to experiment with the addition
    of layers.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个描述可能难以转化为现实世界的问题，但它的实际直觉很简单：在处理具有不同表示级别的复杂函数时，你可能想要尝试添加层。
- en: Adding More Nodes
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加更多节点
- en: The number of neurons that your layer requires is related to how both the input
    and output data are structured.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 层所需的神经元数量与输入和输出数据的结构有关。
- en: For instance, if you are working to classify a 4 x 4 pixel image into one of
    two categories, one can start with a hidden layer that has 12 neurons (one for
    each available pixel) and an output layer that has only two (one for each predicted
    class).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你正在将一张4 x 4像素的图像分类到两个类别中的一个，你可以从一个具有12个神经元的隐藏层开始（每个神经元对应一个像素），然后再加一个只有两个神经元的输出层（每个神经元对应一个预测类别）。
- en: It is common to add new neurons alongside the addition of new layers. Then,
    one can add a layer that has either the same number of neurons as the previous
    one, or a multiple of the number of neurons from the previous layer. For instance,
    if your fist hidden layer has 12 neurons, you can experiment with adding a second
    layer that has either 12, 6, or 24.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加新层时，通常会添加新的神经元。然后，可以添加一个层，该层的神经元数量与前一层相同，或者是前一层神经元数量的倍数。例如，如果你的第一个隐藏层有12个神经元，你可以尝试添加一个第二层，它的神经元数量可以是12、6或24。
- en: Adding layers and neurons can have significant performance limitations. Feel
    free to experiment with adding layers and nodes. It is common to start with a
    smaller network (that is, a network with a small number of layers and neurons),
    then grow according to its performance gains.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 添加层和神经元可能会导致性能的显著限制。可以随意尝试添加层和节点。通常的做法是从较小的网络开始（即网络中有少量的层和神经元），然后根据其性能的提升逐渐增长。
- en: If the above comes across as imprecise, your intuition is right. To quote Aurélien
    Géron, YouTube's former lead for video classification, *Finding the perfect amount
    of neurons is still somewhat of a black art*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果上面的内容听起来不够精确，你的直觉是对的。引用YouTube前视频分类负责人Aurélien Géron的话，*找到合适数量的神经元仍然有些像黑魔法*。
- en: Hands-on Machine Learning with Scikit-Learn and TensorFlow, by Aurelién Géron,
    published by O'Reilly, March 2017.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 《动手学机器学习》 by Aurelién Géron，O'Reilly出版，2017年3月。
- en: 'Finally, a word of caution: the more layers you add, the more hyper parameters
    you have to tune—and the longer your network will take to train. If your model
    is performing fairly well and not overfitting your data, experiment with the other
    strategies outlined in this chapter before adding new layers to your network.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，提醒一句：你添加的层越多，你需要调整的超参数也就越多——并且训练网络所需的时间也会更长。如果你的模型表现不错，并且没有对数据过拟合，可以在添加新层之前，先尝试本章中提到的其他策略。
- en: Layers and Nodes - Implementation
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层和节点 - 实现
- en: We will now modify our original LSTM model by adding more layers. In LSTM models,
    one typically adds LSTM layers in a sequence, making a chain between LSTM layers.
    In our case, the new LSTM layer has the same number of neurons as the original
    layer, so we don't have to configure that parameter.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过添加更多层来修改我们原来的LSTM模型。在LSTM模型中，通常会按顺序添加LSTM层，在LSTM层之间建立链条。在我们的案例中，新的LSTM层具有与原始层相同的神经元数量，因此我们不需要配置该参数。
- en: We will name the modifiled version of our model `bitcoin_lstm_v1`. It is good
    practice to name each one of the models in which one is attempting different hyperparameter
    configurations differently. This helps you to keep track of how each different
    architecture performs, and also to easily compare model differences in TensorBoard.
    We will compare all the different modifiled architectures at the end of this chapter.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将修改后的模型命名为`bitcoin_lstm_v1`。将每个尝试不同超参数配置的模型命名为不同的名称是一个好习惯。这有助于你跟踪每种不同架构的表现，并在TensorBoard中轻松比较模型之间的差异。我们将在本章末尾比较所有不同的修改过的架构。
- en: Before adding a new LSTM layer, we need to modify the parameter `return_sequences`
    to True on the fist LSTM layer. We do this because the fist layer expects a sequence
    of data with the same input as that of the fist layer. When this parameter is
    set to `False`, the LSTM layer outputs the predicted parameters in a different,
    incompatible output.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加新的 LSTM 层之前，我们需要将第一个 LSTM 层的 `return_sequences` 参数修改为 True。这样做是因为第一个层期望的数据输入是一个序列，这与第一个层的数据输入格式相同。当这个参数设置为
    `False` 时，LSTM 层会输出不兼容的预测参数。
- en: 'Consider the following code example:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下代码示例：
- en: '[PRE18]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '*Snippet 8*: Adding a second LSTM layer to the original `bitcoin_lstm_v0 model`,
    making it `bitcoin_lstm_v1`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段 8*：向原始 `bitcoin_lstm_v0` 模型添加第二个 LSTM 层，使其变为 `bitcoin_lstm_v1`。'
- en: Epochs
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 轮次
- en: Epochs are the number of times the network adjust its weights in response to
    data passing through and its loss function. Running a model for more epochs can
    allow it to learn more from data, but you also run the risk of overfitting.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 轮次是网络在响应数据传递和损失函数时调整权重的次数。运行更多轮次的模型可以让它从数据中学习更多，但也会面临过拟合的风险。
- en: When training a model, prefer to increase the epochs exponentially until the
    loss function starts to plateau. In the case of the `bitcoin_lstm_v0` model, its
    loss function plateaus at about 100 epochs.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，建议以指数方式增加轮次，直到损失函数开始趋于平稳。对于 `bitcoin_lstm_v0` 模型，其损失函数大约在 100 个轮次时趋于平稳。
- en: Our LSTM model uses a small amount of data to train, so increasing the number
    of epochs does not affect its performance in significant ways. For instance, if
    one attempts to train it at 103 epochs, the model barely gains any improvements.
    This will not be the case if the model being trained uses enormous amounts of
    data. In those cases, a large number of epochs is crucial to achieve good performance.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 LSTM 模型使用的数据量较小，因此增加训练轮次对性能几乎没有显著影响。例如，如果尝试在 103 个轮次下训练该模型，模型几乎没有任何改进。但如果训练的模型使用的是大量数据，情况则会有所不同。在这种情况下，大量的轮次对模型的良好性能至关重要。
- en: 'I suggest you use the following association: the larger the date used to train
    your model, the more epochs it will need to achieve good performance.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你使用以下关联：用于训练模型的数据量越大，所需的轮次就越多，以实现良好的性能。
- en: Epochs - Implementation
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 轮次 - 实现
- en: 'Our Bitcoin dataset is rather small, so increasing the epochs that our model
    trains may have only a marginal effect on its performance. In order to have the
    model train for more epochs, one only has to change the epochs parameter in `model.fit()`
    :'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的比特币数据集相对较小，因此增加模型训练的轮次可能对性能的提升影响不大。为了让模型训练更多轮次，只需在 `model.fit()` 中更改轮次参数：
- en: '[PRE19]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*Snippet 9*: Changing the number of epochs that our model trains for, making
    it `bitcoin_lstm_v2`'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段 9*：改变我们模型训练的轮次，使其变为`bitcoin_lstm_v2`。'
- en: That change bumps our model to v2, effectively making it `bitcoin_lstm_v2`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更改将我们的模型升级到了 v2，实际上使其变为 `bitcoin_lstm_v2`。
- en: Activation Functions
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: Activation functions evaluate how much you need to activate individual neurons.
    They determine the value that each neuron will pass to the next element of the
    network, using both the input from the previous layer and the results from the
    loss function—or if a neuron should pass any values at all.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数评估了需要激活每个神经元的程度。它们决定了每个神经元将传递给网络下一个元素的值，使用来自前一层的输入和损失函数的结果——或者决定神经元是否应当传递任何值。
- en: Activation functions are a topic of great interest in the scientific community
    researching neural networks. For an overview of research currently being done
    on the topic and a more detailed review on how activation functions work, please
    refer to *Deep Learning* by Ian Goodfellow et. al., MIT Press, 2017.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是神经网络研究领域的一个重要话题。如果你想了解当前关于此主题的研究概况，以及对激活函数工作原理的更详细回顾，请参考 Ian Goodfellow
    等人撰写的《Deep Learning》，MIT出版社，2017年。
- en: TensorFlow and Keras provide many activation functions—and new ones are occasionally added.
    As an introduction, three are important to consider; let's explore each of them.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 和 Keras 提供了许多激活函数——并且偶尔会新增一些。作为介绍，有三个函数非常重要，值得我们关注；我们将逐一探索它们。
- en: 'This section has been greatly inspired by the article *Understanding Activation
    Functions in Neural Networks* by Avinash Sharma V, available at: [https://medium.com/the-theory-of-everything/
    understanding-activation-functions-in-neural-networks- 9491262884e0](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容深受 Avinash Sharma V 的文章 *理解神经网络中的激活函数* 启发，文章链接为：[https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)。
- en: Linear (Identity)
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性（恒等函数）
- en: 'Linear functions only activate a neuron based on a constant value. They are
    defined by:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 线性函数仅根据常数值激活神经元，其定义如下：
- en: '![](img/7261435b-074a-4e48-a878-ddf7399d9454.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7261435b-074a-4e48-a878-ddf7399d9454.png)'
- en: 'When c = 1, neurons will pass the values as-is, without modification by the
    activation function. The issue with using linear functions is that, due to the
    fact that neurons are activated linearly, chained layers now function as a single
    large layer. In other words, one loses the ability to construct networks with
    many layers, in which the output of one influences the other:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 当 c = 1 时，神经元将按原样传递值，而不受激活函数的修改。使用线性函数的问题在于，由于神经元是线性激活的，链式层次现在作为一个大的单一层来工作。换句话说，失去了构建具有多层的网络的能力，其中一层的输出影响另一层：
- en: '![](img/9929f033-6fa9-461f-8f45-21db82d42fb8.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9929f033-6fa9-461f-8f45-21db82d42fb8.png)'
- en: 'Figure 11: Illustration of a linear function'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：线性函数示意图
- en: The use of linear functions is generally considered obsolete for most networks.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 线性函数的使用通常被认为在大多数网络中已经过时。
- en: Hyperbolic Tangent (Tanh)
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 双曲正切（Tanh）
- en: 'Tanh is a non-linear function, and is represented by the following formula:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Tanh 是一个非线性函数，其公式如下：
- en: '![](img/6e1f0388-c59a-4d99-9d23-5722290e7cbe.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e1f0388-c59a-4d99-9d23-5722290e7cbe.png)'
- en: 'This means that the effect they have on nodes is evaluated continuously. Also,
    because of its non-linearity, one can use this function to change how one layer
    influences the next layer in the chain. When using non-linear functions, layers
    activate neurons in different ways, making it easier to learn different representations
    from data. However, they have a sigmoid-like pattern which penalizes extreme node
    values repeatedly, causing a problem called vanishing gradients. Vanishing gradients
    have negative effects on the ability of a network to learn:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它们对节点的影响是持续评估的。而且，由于其非线性特性，可以使用此函数改变一层如何影响链中下一层。当使用非线性函数时，各层以不同方式激活神经元，从数据中学习不同的表示变得更容易。然而，它们具有类似
    Sigmoid 的模式，会反复惩罚极端的节点值，造成一种称为“梯度消失”的问题。梯度消失对网络学习能力产生负面影响：
- en: '![](img/8d1b34ee-905d-4c13-affc-466e98ed9533.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d1b34ee-905d-4c13-affc-466e98ed9533.png)'
- en: 'Figure 12: Illustration of a `tanh` function'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：`tanh` 函数示意图
- en: Tanhs are popular choices, but due to fact that they are computationally expensive,
    ReLUs are often used instead.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Tanhs 是常用的选择，但由于其计算开销较大，通常会使用 ReLU 作为替代。
- en: Rectifid Linear Unit
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修正线性单元
- en: 'ReLUs have non-linear properties. They are defined by:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 具有非线性特性，其定义如下：
- en: '![](img/e34ccbc9-8077-452b-8ea7-7fa1695be15c.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e34ccbc9-8077-452b-8ea7-7fa1695be15c.png)'
- en: '![](img/69e63f3a-a36c-4cf8-8480-e36c00578657.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69e63f3a-a36c-4cf8-8480-e36c00578657.png)'
- en: 'Figure 13: Illustration of a ReLU function'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：ReLU 函数示意图
- en: ReLU functions are often recommended as great starting points before trying
    other functions. ReLUs tend to penalize negative values. So, if the input data
    (for instance, normalized between -1 and 1) contains negative values, those will
    now be penalized by ReLUs. That may not be the intended behavior.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 函数通常被推荐作为在尝试其他函数之前的一个很好的起点。ReLU 倾向于惩罚负值。因此，如果输入数据（例如，归一化至 -1 到 1 之间）包含负值，这些值将会受到
    ReLU 的惩罚。这可能不是预期的行为。
- en: We will not be using ReLU functions in our network because our normalization
    process creates many negative values, yielding a much slower learning model.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在网络中将不会使用 ReLU 函数，因为我们的归一化过程会生成许多负值，从而导致学习模型的速度大大减慢。
- en: Activation Functions - Implementation
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数 - 实现
- en: The easiest way to implement activation functions in Keras is by instantiating
    the Activation() class and adding it to the `Sequential()` model. `Activation()`
    can be instantiated with any activation function available in Keras (for a complete
    list, see [https://keras.io/activations/](https://keras.io/activations/)). In
    our case, we will use the `tanh` function.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中实现激活函数的最简单方法是实例化`Activation()`类，并将其添加到`Sequential()`模型中。`Activation()`可以使用Keras中提供的任何激活函数进行实例化（完整列表请见[https://keras.io/activations/](https://keras.io/activations/)）。在我们的例子中，我们将使用`tanh`函数。
- en: 'After implementing an activation function, we bump the version of our model
    to `v2`, making it `bitcoin_lstm_v3`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现激活函数后，我们将模型的版本提升为`v2`，使其成为`bitcoin_lstm_v3`：
- en: '[PRE20]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '*Snippet 10*: Adding the activation function `tanh` to the `bitcoin_lstm_v2`
    model, making it `bitcoin_lstm_v3`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段 10*：将激活函数`tanh`添加到`bitcoin_lstm_v2`模型中，更新为`bitcoin_lstm_v3`'
- en: There are a number of other activation functions worth experimenting with. Both
    TensorFlow and Keras provide a list of implemented functions in their respective
    official documentations. Before implementing your own, start with the ones already
    implemented in both TensorFlow and Keras.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他值得尝试的激活函数。TensorFlow和Keras都在各自的官方文档中提供了已实现的函数列表。在实现自己的激活函数之前，最好先从TensorFlow和Keras中已经实现的函数开始。
- en: Regularization Strategies
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化策略
- en: Neural networks are particularly prone to overfitting. Overfitting happens when
    a network learns the patterns of the Neural networks are particularly prone to
    overfitting. Overfitting happens when a network learns the patterns of the training
    data but is unable to find generalizable patterns that can also be applied to
    the test data.he training data but is unable to find generalizable patterns that
    can also be applied to the test data.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络尤其容易出现过拟合。过拟合发生在网络学习了训练数据的模式，但无法找到可以应用于测试数据的可泛化模式。
- en: 'Regularization strategies refer to techniques that deal with the problem of
    overfitting by adjusting how the network learns. In this book, we discuss two
    common strategies: L2 and Dropout.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化策略是指通过调整网络学习的方式来处理过拟合问题的技术。在本书中，我们讨论了两种常见的策略：L2 正则化和Dropout。
- en: L2 Regularization
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L2 正则化
- en: L2 regularization (or weight decay) is a common technique for dealing with overfiting
    models. In some models, certain parameters vary in great magnitudes. The L2 regularization
    penalizes such parameters, reducing the effect of these parameters on the network.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化（或称权重衰减）是一种常见的解决过拟合模型的技术。在一些模型中，某些参数的变化幅度较大。L2 正则化会对这些参数进行惩罚，从而减少这些参数对网络的影响。
- en: L2 regularizations use the![](img/01dfbb24-b718-40fc-840f-8e8529604999.png)
    parameter to determine how much to penalize a model neuron. One typically sets
    that to a very low value (that is, `0.0001`); otherwise, one risks eliminating
    the input from a given neuron completely.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化使用![](img/01dfbb24-b718-40fc-840f-8e8529604999.png)参数来决定惩罚模型神经元的程度。通常将该值设置为非常小的数值（即`0.0001`）；否则，可能会完全消除某个神经元的输入。
- en: Dropout
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout（丢弃法）
- en: 'Dropout is a regularization technique based on a simple question: if one randomly
    takes away a proportion of nodes from layers, how will the other node adapt? It
    turns out that the remaining neurons adapt, learning to represent patterns that
    were previously handled by those neurons that are missing.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 是一种基于简单问题的正则化技术：如果从层中随机移除一部分节点，剩下的节点会如何适应？事实证明，剩余的神经元会适应，学习表示那些之前由缺失神经元处理的模式。
- en: The dropout strategy is simple to implement and is typically very effective
    to avoid overfitting. This will be our preferred regularization.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 策略实现起来简单，通常在避免过拟合方面非常有效。这将是我们首选的正则化策略。
- en: Regularization Strategies – Implementation
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化策略 – 实现
- en: In order to implement the dropout strategy using Keras, we import the `Dropout()`
    class and add it to our network immediately after each LSTM layer.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用Keras实现Dropout策略，我们导入`Dropout()`类，并将其添加到每个LSTM层之后的网络中。
- en: 'This addition effectively makes our network `bitcoin_lstm_v4`:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这一添加有效地将我们的网络变为`bitcoin_lstm_v4`：
- en: '[PRE21]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*Snippet 11*: In this snippet, we add the `Dropout()` step to our model (`bitcoin_lstm_v3`),
    making it `bitcoin_lstm_v4`'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码片段 11*：在此代码片段中，我们将`Dropout()`步骤添加到我们的模型（`bitcoin_lstm_v3`）中，更新为`bitcoin_lstm_v4`'
- en: One could have used the L2 regularization instead of Dropout. In order to do
    that, simply instantiate the `ActivityRegularization()` class with the L2 parameter
    set to a low value (`0.0001`, for instance). Then, place it in the place where
    the Dropout() class is added to the network. Feel free to experiment by adding
    that to the network while keeping both `Dropout()` steps, or simply replace all
    the `Dropout()` instances with `ActivityRegularization()` instead.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用L2正则化代替Dropout。为此，只需实例化`ActivityRegularization()`类，并将L2参数设置为较小的值（例如`0.0001`）。然后，将其放置在网络中添加Dropout()类的位置。可以通过将其添加到网络中，同时保留两个`Dropout()`步骤，或简单地将所有`Dropout()`实例替换为`ActivityRegularization()`来进行实验。
- en: Optimization Results
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化结果
- en: All in all, we have created four versions of our model. Three of these versions
    were created by the application of different optimization techniques outlined
    in this chapter.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们已经创建了四个版本的模型。这些版本中的三个是通过应用本章所述的不同优化技术创建的。
- en: 'After creating all these versions, we now have to evaluate which model performs
    best. In order to do that, we use the same metrics used in our fist model: MSE,
    RMSE, and MAPE. MSE is used to compare the error rates of the model on each predicted
    week. RMSE and MAPE are computed to make the model results easier to interpret.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建了所有这些版本后，我们现在需要评估哪个模型表现最好。为此，我们使用与第一个模型相同的指标：MSE、RMSE和MAPE。MSE用于比较模型在每一周预测中的误差率，RMSE和MAPE用于使模型结果更易于解释。
- en: '| **Model**  | **MSE (last epoch)**  | **RMSE  (whole series)**  | **MAPE  (whole
    series)**  | **Training Time**  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| **模型**  | **MSE（最后一轮）**  | **RMSE（整个序列）**  | **MAPE（整个序列）**  | **训练时间**  |'
- en: '| bitcoin_lstm_v0  | **-** |  399.6  |  8.4 percent  | ** -** |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| bitcoin_lstm_v0  | **-** |  399.6  |  8.4%  | ** -** |'
- en: '| bitcoin_lstm_v1  |  7.15*10^(-6)  |  419.3  |  8.8 percent  | 49.3 s  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| bitcoin_lstm_v1  |  7.15*10^(-6)  |  419.3  |  8.8%  | 49.3秒  |'
- en: '| bitcoin_lstm_v2  |  3.55*10^(-6)  |  425.4  |  9.0 percent  | 1 min 13s  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| bitcoin_lstm_v2  |  3.55*10^(-6)  |  425.4  |  9.0%  | 1分13秒  |'
- en: '| bitcoin_lstm_v3  |  2.8*10^(-4)  |  423.9  |  8.8 percent  | 1 min 19s  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| bitcoin_lstm_v3  |  2.8*10^(-4)  |  423.9  |  8.8%  | 1分19秒  |'
- en: '| bitcoin_lstm_v4  |  4.8*10^(-7)  |  442.4  |  8.8 percent  | 1 min 20s  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| bitcoin_lstm_v4  |  4.8*10^(-7)  |  442.4  |  8.8%  | 1分20秒  |'
- en: 'Table 2: Model results for all models'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：所有模型的结果
- en: Interestingly, our fist model (`bitcoin_lstm_v0`) performed the best in nearly
    all defiled metrics. We will be using that model to build our web application
    and continuously predict Bitcoin prices.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们的第一个模型（`bitcoin_lstm_v0`）在几乎所有的指标中表现最好。我们将使用该模型来构建我们的Web应用程序，并持续预测比特币价格。
- en: Activity:Optimizing a Deep Learning Model
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动：优化深度学习模型
- en: In this activity, we implement different optimization strategies to the model
    created in C*hapter 5*, *Model Architecture* (`bitcoin_lstm_v0`). That model achieves
    a MAPE performance on the complete de-normalization test set of about 8.4 percent.
    We will try to reduce that gap.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们对第5章*模型架构*（`bitcoin_lstm_v0`）中创建的模型应用了不同的优化策略。该模型在完整的去归一化测试集上的MAPE性能约为8.4%。我们将尝试减少这个差距。
- en: 'Using your terminal, start a TensorBoard instance by executing the following command:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用终端，通过执行以下命令启动TensorBoard实例：
- en: '[PRE22]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Open the URL that appears on the screen and leave that browser tab open, as
    well. Also, start a Jupyter Notebook instance with:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开屏幕上出现的URL，并保持该浏览器标签页打开。同时，启动一个Jupyter Notebook实例：
- en: '[PRE23]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Open the URL that appears in a different browser window.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 打开出现在另一个浏览器窗口中的URL。
- en: Now, open the Jupyter Notebook called `Activity_7_Optimizing_a_deep_ learning_model.ipynb`
    and navigate to the title of the Notebook and import all required libraries. We
    will load the train and test data like in previous activities. We will also split
    it into train and test groups using the utility   function `split_lstm_input()`
    .
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，打开名为`Activity_7_Optimizing_a_deep_learning_model.ipynb`的Jupyter Notebook，导航到Notebook的标题并导入所有所需的库。我们将像之前的活动一样加载训练和测试数据。我们还将使用实用函数`split_lstm_input()`将其分割为训练组和测试组。
- en: In each section of this Notebook, we will implement new optimization techniques
    in our model. Each time we do so, we   train a fresh model and store its trained
    instance in a variable that describes the model version. For instance, our first 
    model, `bitcoin_lstm_v0`, is called`model_v0` in this Notebook. At the very end
    of the Notebook, we evaluate all  models using MSE, RMSE, and MAPE.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在Notebook的每个部分，我们都会在模型中实现新的优化技术。每次我们这样做时，都会训练一个全新的模型，并将其训练后的实例存储在一个描述模型版本的变量中。例如，我们的第一个模型`bitcoin_lstm_v0`在Notebook中被称为`model_v0`。在Notebook的最后，我们使用MSE、RMSE和MAPE评估所有模型。
- en: Now, in the open Jupyter Notebook, navigate to the header **Adding Layers**
    and **Nodes**.You will recognize our fist model in the next cell. This is the
    basic LSTM network that we built in C*hapter 5*, *Model Architecture*. Now, we
    have to add a new LSTM layer to this network.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在打开的Jupyter Notebook中，导航到**Adding Layers**（添加层）和**Nodes**（节点）部分。你将在下一个单元格中看到我们第一个模型。这是我们在**第5章**
    *模型架构* 中构建的基础LSTM网络。现在，我们需要向这个网络添加一个新的LSTM层。
- en: Using knowledge from this chapter, go ahead and add a new LSTM layer, compile,
    and train the model. While training your models, remember to frequently visit
    the running TensorBoard instance.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 利用本章的知识，继续添加一个新的LSTM层，编译并训练模型。在训练模型时，请记得经常访问正在运行的TensorBoard实例。
- en: 'You will be able to see each model run and compare the results of their loss
    functions there:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 你将能够看到每个模型的运行并比较它们的损失函数结果：
- en: '![](img/c919b2a6-3e37-4d14-90ce-3748169adc08.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c919b2a6-3e37-4d14-90ce-3748169adc08.png)'
- en: 'Figure 14: Running the TensorBoard instance, which is displaying many different
    model runs. TensorBoard is really'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：运行TensorBoard实例，显示多个不同的模型运行。TensorBoard实际上是一个非常
- en: useful for tracking model training in real time.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 有助于实时跟踪模型训练进展。
- en: 'Now, navigate to the header Epochs. In this section, we are interested in exploring
    different magnitudes of **epochs**. Use the utility function`train_model()` to
    name different model versions and runs:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，导航到**Epochs**（训练轮次）部分。在这一部分，我们将探索不同规模的**epochs**。使用工具函数`train_model()`来命名不同的模型版本和运行：
- en: '[PRE24]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Train the model with a few different epoch parameters.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的epoch参数训练模型。
- en: At this point, you are interested in making sure the model doesn't overfit the
    training data. You want to avoid this, because if it does, it will not be able
    to predict patterns that are represented in the training data but have different
    representations in the test data.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你需要确保模型不会过拟合训练数据。你希望避免这种情况，因为如果模型过拟合，它将无法预测训练数据中表现出的模式，而这些模式在测试数据中可能会有不同的表现形式。
- en: 'After you are done experimenting with epochs, move to the next optimization
    technique: activation functions.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在你完成对epoch的实验后，继续进行下一个优化技术：激活函数。
- en: 'Now, navigate to the header **Activation Functions** in the Notebook. In this
    section, you only need to change the following variable:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，导航到Notebook中的**Activation Functions**（激活函数）部分。在这一部分，你只需要更改以下变量：
- en: '[PRE25]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We have used the `tanh` function in this section, but feel free to try other
    activation functions. Review the list available at [https://keras.io/activations/](https://keras.io/activations/)
    and try other possibilities.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中我们使用了`tanh`函数，但你可以尝试其他激活函数。查看[https://keras.io/activations/](https://keras.io/activations/)中列出的激活函数，并尝试其他可能的选项。
- en: Our final option is to try different regularization strategies. This is notably
    more complex and may take a few iterations to notice any gains—especially with
    so little data. Also, adding regularization strategies typically increases the
    training time of your network.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终选择是尝试不同的正则化策略。这通常更为复杂，可能需要多次迭代才能看到任何改进——特别是在数据量如此之少的情况下。此外，添加正则化策略通常会增加网络的训练时间。
- en: Now, navigate to the header **Regularization Strategies** in the Notebook. In
    this section, you need to implement the `Dropout()` regularization strategy. Find
    the right place to place that step and implement it in our model.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，导航到Notebook中的**Regularization Strategies**（正则化策略）部分。在这一部分，你需要实现`Dropout()`正则化策略。找到合适的位置将此步骤加入，并在我们的模型中实现。
- en: You can also try the L2 regularization here, as well (or combine both). Do the
    same as with `Dropout()` , but now using `ActivityRegularization`(`l2=0.0001`)
    .
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以尝试L2正则化（或者两者结合使用）。和`Dropout()`一样，使用`ActivityRegularization`（`l2=0.0001`）进行操作。
- en: Now, navigate to the header **Evaluate Models** in the Notebook. In this section,
    we will evaluate the model predictions for the next 19 weeks of data in the test
    set. Then, we will compute the RMSE and MAPE of the predicted series versus the
    test series.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，导航到笔记本中的**评估模型**部分。在这一部分，我们将评估模型对测试集未来19周数据的预测。然后，我们将计算预测系列与测试系列之间的RMSE和MAPE。
- en: We have implemented the same evaluation techniques from Activity 6, all wrapped in
    utility functions. Simply run all the   cells from this section until the end
    of the notebook to see the results.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经实现了与第6个活动相同的评估技术，所有功能都封装在实用函数中。只需运行本节的所有单元格直到笔记本结束，即可查看结果。
- en: Take this opportunity to tweak the values for the preceding optimization techniques
    and attempt to beat the performance of that model.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 抓住这个机会调整前面提到的优化技术的值，尝试超越该模型的性能。
- en: Summary
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to evaluate our model using the metrics mean
    squared error (MSE), squared mean squared error (RMSE), and mean averaged percentage
    error (MAPE). We computed the latter two metrics in a series of 19 week predictions
    made by our fist neural network model. We then learned that it was performing
    well.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章节中，我们学习了如何使用均方误差（MSE）、均方根误差（RMSE）和平均绝对百分比误差（MAPE）来评估我们的模型。我们在由我们的第一个神经网络模型进行的19周预测系列中计算了后两个指标。然后，我们了解到该模型表现良好。
- en: We also learned how to optimize a model. We looked at optimization techniques
    typically used to increase the performance of neural networks. Also, we implemented
    a number of these techniques and created a few more models to predict Bitcoin
    prices with different error rates.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了如何优化模型。我们查看了通常用于提高神经网络性能的优化技术。此外，我们实现了其中的一些技术，并创建了几个不同的模型来预测比特币价格，具有不同的误差率。
- en: 'In the next chapter, we will be turning our model into a web application that
    does two things: re-trains our model periodically with new data, and is able to
    make predictions using an HTTP API interface.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章节中，我们将把我们的模型转化为一个Web应用，完成两件事：定期使用新数据重新训练我们的模型，并能够通过HTTP API接口进行预测。
