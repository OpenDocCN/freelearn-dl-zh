- en: '*Chapter 7*: Policy-Based Methods'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第七章*：基于策略的方法'
- en: Value-based methods, which we covered in the previous chapter, achieve great
    results in many environments with discrete control spaces. However, a lot of applications,
    such as robotics, require continuous control. In this chapter, we'll go into another
    important class of algorithms, called policy-based methods, which enable us to
    solve continuous-control problems. In addition, these methods directly optimize
    a policy network and hence stand on a stronger theoretical foundation. Finally,
    policy-based methods are able to learn truly stochastic policies, which are needed
    in partially observable environments and games, and which value-based methods
    cannot learn. All in all, policy-based approaches complement value-based methods
    in many ways. This chapter goes into the details of policy-based methods, so you
    will gain a strong understanding of how they work.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中我们讨论的基于价值的方法在许多具有离散控制空间的环境中取得了良好的效果。然而，许多应用，如机器人技术，需要连续控制。在本章中，我们将深入讨论另一类重要的算法——基于策略的方法，这些方法使我们能够解决连续控制问题。此外，这些方法直接优化策略网络，因此站在更强的理论基础上。最后，基于策略的方法能够学习真正的随机策略，这是部分可观察环境和游戏中所需的，而基于价值的方法无法学习。总的来说，基于策略的方法在许多方面补充了基于价值的方法。本章将深入探讨基于策略的方法，因此你将深入理解它们的工作原理。
- en: 'In particular, we''ll discuss the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论以下主题：
- en: Why should we use policy-based methods?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们应该使用基于策略的方法？
- en: Vanilla policy gradient
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础策略梯度
- en: Actor-critic methods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actor-critic 方法
- en: Trust-region methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信任区域方法
- en: Off-policy methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离策略方法
- en: A comparison of the policy-based methods in Lunar Lander
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在《月球着陆》中的基于策略方法的比较
- en: How to pick the right algorithm
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何选择合适的算法
- en: Open source implementations of policy-based methods
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开源的基于策略方法的实现
- en: Let's dive right in!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接开始吧！
- en: Why should we use policy-based methods?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们应该使用基于策略的方法？
- en: We'll start this chapter by first discussing why we need policy-based methods
    as we have already introduced many value-based methods. Policy-based methods i)
    are arguably more principled as they directly optimize based on the policy parameters,
    ii) allow us to use continuous action spaces, and iii) are able to learn truly
    random stochastic policies. Let's now go into the details of each of these points.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从本章开始讨论为什么我们需要基于策略的方法，因为我们已经介绍了许多基于价值的方法。基于策略的方法 i) 可以说更有原则，因为它们直接基于策略参数进行优化，ii)
    允许我们使用连续动作空间，iii) 能够学习真正的随机策略。现在让我们深入探讨这些要点的细节。
- en: A more principled approach
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种更有原则的方法
- en: In Q-learning, a policy is obtained in an indirect manner by learning action
    values, which are then used to determine the best action(s). But do we really
    need to know the value of an action? Most of the time we don't, as they are only
    proxies to get us to optimal policies. Policy-based methods learn function approximations
    that directly give policies without such an intermediate step. This is arguably
    a more principled approach because we can take gradient steps directly to optimize
    the policy, not the proxy action-value representation. The latter is especially
    inefficient when there are many actions with similar values, perhaps all uninteresting
    to us because they are all bad actions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在Q学习中，策略是通过学习动作值间接获得的，然后用这些值来确定最佳动作。然而，我们真的需要知道一个动作的值吗？大多数时候我们不需要，因为它们只是帮助我们获得最优策略的代理。基于策略的方法通过学习函数逼近来直接给出策略，而不需要这样的中间步骤。这可以说是一种更有原则的方法，因为我们可以直接通过梯度步骤来优化策略，而不是代理的动作值表示。当存在许多具有相似值的动作时，后者尤其低效，可能这些动作对我们来说都没有吸引力，因为它们都是糟糕的动作。
- en: The ability to use continuous action spaces
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用连续动作空间的能力
- en: All of the methods we mentioned in the previous section about value-based methods
    worked with discrete action spaces. On the other hand, there are many use cases
    for which we need to use continuous action spaces, such as robotics, where the
    discretization of actions results in poor agent behavior. But what is the issue
    with using continuous action spaces with value-based methods? Neural networks
    can certainly learn value representations for continuous actions – after all,
    we don't have such a restriction on states.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节提到的所有基于价值的方法都适用于离散动作空间。另一方面，有许多应用场景需要使用连续动作空间，例如机器人技术，其中动作的离散化会导致代理行为变差。那么，使用基于价值的方法与连续动作空间有什么问题呢？神经网络当然可以为连续动作学习价值表示——毕竟，状态并没有这样的限制。
- en: 'However, remember how we used maximization over actions while calculating the
    target values:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，记住我们在计算目标值时是如何进行动作最大化的：
- en: '![](img/Formula_07_001.png) and while obtaining the best action to act in the
    environment using ![](img/Formula_07_002.png). It is not very straightforward
    to do these maximizations over continuous action spaces, although we can make
    it work using approaches such as the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_07_001.png)并在使用![](img/Formula_07_002.png)时获得在环境中执行的最佳动作。虽然我们可以通过以下方法使这些最大化在连续动作空间上有效，但实现起来并不简单：'
- en: During maximization, sample discrete actions from the continuous action space
    and use the action with the maximum value. Alternatively, fit a function to the
    values of the sampled actions and do the maximization over that function, which
    is called the **cross-entropy method (CEM)**.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最大化过程中，从连续动作空间中采样离散动作，并使用具有最大值的动作。或者，可以拟合一个函数来表示采样动作的值，并在该函数上进行最大化，这被称为**交叉熵方法（CEM）**。
- en: Instead of using a neural network, use a function approximator such as a function
    quadratic in actions, the maximum of which can be analytically calculated. An
    example of this is **Normalized Advantage Functions (NAF)** (*Gu et al, 2016*
    **).
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 替代使用神经网络，采用一个如动作二次函数的函数逼近器，其最大值可以通过解析方法计算得出。一个例子是**归一化优势函数（NAF）**（*Gu et al,
    2016*）。
- en: Learn a separate function approximation to obtain the maximum, such as in the
    **Deep Deterministic Policy Gradient (DDPG)** algorithm.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学习一个单独的函数逼近器来获得最大值，例如在**深度确定性策略梯度（DDPG）**算法中。
- en: Now, the downside of CEM and NAF is that they are less powerful compared to
    a neural network that directly represents a continuous-action policy. DDPG, on
    the other hand, is still a competitive alternative, which we will cover later
    in the chapter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，CEM和NAF的缺点是与直接表示连续动作策略的神经网络相比，它们的效果较差。另一方面，DDPG仍然是一个有竞争力的替代方案，我们将在本章后续部分讨论。
- en: Info
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: Most policy-based methods work with both discrete and continuous action spaces.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于策略的方法同时适用于离散和连续动作空间。
- en: The ability to learn truly random stochastic policies
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习真正随机的随机策略的能力
- en: Throughout Q-learning, we used soft-policies such as ![](img/Formula_05_272.png)-greedy
    to enable the agent to explore the environment during training. Although this
    approach works pretty well in practice, and it can be made more sophisticated
    by annealing the ![](img/Formula_07_004.png), it is still not a learned parameter.
    Policy-based methods can learn random policies that lead to a more principled
    exploration during training.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个Q学习过程中，我们使用了软策略，如![](img/Formula_05_272.png)-贪婪策略，以便在训练期间让智能体探索环境。尽管这种方法在实践中效果相当不错，而且可以通过退火![](img/Formula_07_004.png)来使其更复杂，但它仍然不是一个学习到的参数。基于策略的方法可以学习随机策略，从而在训练过程中实现更有原则的探索。
- en: 'Perhaps a bigger issue is that we may need to learn random policies not just
    for training but also for inference. There are two reasons why we might want to
    do this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 也许更大的问题是，我们可能不仅仅为了训练需要学习随机策略，推理时也可能需要这样做。我们可能想要这样做的原因有两个：
- en: 'In **partially observable environments** (**POMDPs**), we may have what are
    called **aliased states**, which emit the same observation although the states
    themselves are different, for which the optimal actions might be different. Consider
    the following example:'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**部分可观察环境**（**POMDPs**）中，我们可能会遇到所谓的**同态状态**，这些状态虽然不同，但发出的观察是相同的，对于这些状态，最佳动作可能是不同的。考虑以下示例：
- en: '![Figure 7.1 – Robot in a partially observable environment'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.1 – 在部分可观察环境中的机器人'
- en: '](img/B14160_07_1.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_07_1.jpg)'
- en: Figure 7.1 – Robot in a partially observable environment
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 在部分可观察环境中的机器人
- en: The agent only observes the shapes of the states it is in but cannot tell what
    the state is. The agent is randomly initialized in a state other than 3, and its
    goal is to reach the coins in state 3 in the minimum number of steps by going
    left or right. The optimal action when the agent observes a hexagon is a random
    one because a deterministic policy (say, to always go left) would make the agent
    get stuck either between 1 and 2 (if left is always chosen) or 4 and 5.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体只观察它所在状态的形态，但无法判断状态是什么。智能体随机初始化在除状态3以外的状态中，目标是通过左右移动，在最少步数内到达状态3中的硬币。当智能体观察到六边形时，最佳动作是随机的，因为一个确定性的策略（比如始终向左走）会让智能体在1和2之间（如果始终选择左走）或4和5之间卡住。
- en: In game settings with adversarial agents, there could be cases where the only
    optimal policy is a random one. The canonical example for this is rock-paper-scissors,
    where the optimal policy is to select an action uniformly at random. Any other
    policy could be exploited by the opponents in the environment.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在对抗性智能体的游戏设置中，可能会出现唯一最优策略是随机策略的情况。经典的例子是石头剪子布，在这种情况下，最优策略是随机选择一个动作。任何其他策略都可能被环境中的对手利用。
- en: Value-based methods don't have the ability to learn such random policies for
    inference whereas policy-based methods allow that.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于价值的方法无法学习这种随机策略进行推断，而基于策略的方法则可以。
- en: Tip
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: If the environment is fully observable, and it is not a game setting, there
    is always a deterministic policy that is optimal (although there could be more
    than one optimal policy and some of them could be random). In such cases, we don't
    need a random policy during inference.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果环境是完全可观察的，并且不是游戏环境，那么总会存在一个确定性的最优策略（尽管可能有多个最优策略，而且其中一些可能是随机的）。在这种情况下，我们在推断过程中不需要随机策略。
- en: With this introduction, let's dive into the most popular policy-based methods.
    Next, we will give an overview of the vanilla policy gradient to set the stage
    for more complex algorithms that we will cover later.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个介绍，让我们深入了解最流行的基于策略的方法。接下来，我们将概述基础的策略梯度方法，为我们稍后将要介绍的更复杂的算法做铺垫。
- en: The vanilla policy gradient
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 原始策略梯度方法
- en: 'We''ll start by discussing the policy-based methods with the most fundamental
    algorithm: a vanilla policy gradient approach. Although such an algorithm is rarely
    useful in realistic problem settings, it is very important to understand it to
    build a strong intuition and a theoretical background for the more complex algorithms
    we will cover later.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从讨论最基础的算法——原始策略梯度方法开始。尽管这种算法在实际问题中很少有用，但理解它对于建立强大的直觉和理论基础至关重要，以便我们能够理解后面将要介绍的更复杂的算法。
- en: The objective in policy gradient methods
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略梯度方法中的目标
- en: 'In value-based methods, we focused on finding good estimates for action values,
    with which we then obtained policies. Policy gradient methods, on the other hand,
    directly focus on optimizing the policy with respect to the reinforcement learning
    objective – although, we will still make use of value estimates. If you don''t
    remember what this objective was, it is the expected discounted return:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于价值的方法中，我们专注于找到对动作值的良好估计，之后利用这些估计来获得策略。而策略梯度方法则直接专注于根据强化学习目标优化策略——尽管我们仍然会利用价值估计。如果你不记得这个目标是什么，它是期望的折扣回报：
- en: '![](img/Formula_07_005.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_005.jpg)'
- en: 'This is a slightly more rigorous way of writing this objective compared to
    how we wrote it before. Let''s unpack what we have here:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是比之前写法稍微严谨一些的目标表达方式。让我们来解读一下：
- en: The objective is denoted by ![](img/Formula_07_006.png) and it is a function
    of the policy at hand, ![](img/Formula_07_007.png).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标由 ![](img/Formula_07_006.png) 表示，它是当前策略的一个函数，![](img/Formula_07_007.png)。
- en: The policy itself is parametrized by ![](img/Formula_07_008.png), which we are
    trying to determine.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略本身由 ![](img/Formula_07_008.png) 参数化，我们正在尝试确定这个参数。
- en: The trajectory the agent observes, ![](img/Formula_07_009.png), is a random
    one, with a probability distribution ![](img/Formula_07_010.png). It is, as you
    would expect, a function of the policy, hence a function of ![](img/Formula_07_011.png).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体观察到的轨迹 ![](img/Formula_07_009.png) 是随机的，其概率分布为 ![](img/Formula_07_010.png)。正如你所期待的，它是策略的一个函数，因此也是
    ![](img/Formula_07_011.png) 的函数。
- en: '![](img/Formula_07_012.png) is a function (unknown to the agent) that gives
    a reward based on the environment dynamics given the state ![](img/Formula_07_013.png)
    and the action ![](img/Formula_07_014.png).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_07_012.png) 是一个函数（对智能体未知），根据环境动态、给定的状态 ![](img/Formula_07_013.png)
    和动作 ![](img/Formula_07_014.png) 给予奖励。'
- en: 'Now, we have an objective function ![](img/Formula_07_015.png) that we want
    to maximize, which depends on parameter ![](img/Formula_07_016.png), which we
    can control. A natural thing to do is to take a gradient step in the ascending
    direction:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个目标函数 ![](img/Formula_07_015.png)，我们希望最大化它，这个目标函数依赖于我们可以控制的参数 ![](img/Formula_07_016.png)。一种自然的做法是朝上升方向采取梯度步伐：
- en: '![](img/Formula_07_017.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_017.jpg)'
- en: where ![](img/Formula_07_018.png) is some step size. That's the main idea behind
    policy gradient methods, which, again, directly optimize the policy.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_07_018.png) 是某个步长。这就是策略梯度方法的主要思想，正如之前所说，它直接优化策略。
- en: Now, the million-dollar question (okay, maybe not that much) is how to figure
    out the gradient term. Let's look into it next.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，百万美元的问题（好吧，也许没那么多）是如何求出梯度项。接下来我们来看看怎么解决。
- en: Figuring out the gradient
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算梯度
- en: Understanding how the gradient of the objective function with respect to the
    policy parameters, ![](img/Formula_07_019.png), is obtained is important to get
    the idea behind different variants of policy gradient methods. Let's derive what
    we use in the vanilla policy gradient step by step.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 理解如何从目标函数对策略参数的梯度，![](img/Formula_07_019.png)，中获得梯度对于理解不同的策略梯度方法变种至关重要。接下来让我们一步步推导在经典策略梯度中使用的公式。
- en: A different way of expressing the objective function
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标函数的另一种表示方式
- en: 'First, let''s express the objective function slightly differently:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们稍微不同地表示目标函数：
- en: '![](img/Formula_07_020.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_020.png)'
- en: '![](img/Formula_07_021.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_021.png)'
- en: 'Here, we just expressed the trajectory and the reward that corresponds to it
    as a whole rather than individual state-action pairs, ![](img/Formula_07_022.png).
    Then, we used the definition of an expectation to write it as an integral (with
    a slight abuse of notation since we use the same ![](img/Formula_07_023.png) both
    to denote the random variable and a realization of it, but it should be apparent
    from the context). Keep in mind that the probability of observing a particular
    trajectory ![](img/Formula_07_024.png) is the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是将轨迹和与之对应的奖励整体表示出来，而不是逐个状态-动作对表示，![](img/Formula_07_022.png)。然后，我们使用期望的定义将其写成一个积分（稍微有点滥用符号，因为我们用相同的![](img/Formula_07_023.png)来表示随机变量和它的一个实现，但从上下文中应该能明显看出来）。请记住，观察到特定轨迹![](img/Formula_07_024.png)的概率是以下形式：
- en: '![](img/Formula_07_025.jpg)![](img/Formula_07_026.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_025.jpg)![](img/Formula_07_026.jpg)'
- en: This is simply a chain of products for probabilities of observing a state, taking
    a particular action with the policy given the state and observing the next state.
    Here, ![](img/Formula_07_027.png) denotes the environment transition probabilities.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个关于观察某个状态、在给定状态下采取特定动作并观察下一个状态的概率链条。这里，![](img/Formula_07_027.png)表示环境转移概率。
- en: Next, we use this to find a convenient formula for the gradient.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们用这个来找到一个便捷的梯度公式。
- en: Coming up with a convenient expression for the gradient
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 得出便捷的梯度表达式
- en: 'Now let''s go back to the objective function. We can write the gradient of
    it as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到目标函数。我们可以将其梯度表示为：
- en: '![](img/Formula_07_028.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_028.jpg)'
- en: '![](img/Formula_07_029.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_029.png)'
- en: 'Now we have the term ![](img/Formula_07_030.png) we need to deal with. We will
    do a simple trick to get rid of it:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了需要处理的项![](img/Formula_07_030.png)。我们将做一个简单的技巧来消除它：
- en: '![](img/Formula_07_031.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_031.jpg)'
- en: 'which just follows from the definition of ![](img/Formula_07_032.png). Putting
    it back to the integral, we end up with an expectation for the gradient of the
    objective (be careful – not for the objective itself):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这直接来自![](img/Formula_07_032.png)的定义。将其代回积分后，我们得到了目标函数梯度的期望值（小心—不是目标函数本身的期望值）：
- en: '![](img/Formula_07_033.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_033.jpg)'
- en: '![](img/Formula_07_034.png) ![](img/Formula_07_035.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_034.png) ![](img/Formula_07_035.png)'
- en: Now, this will turn out to be a very convenient formula for the gradient. Taking
    a step back, we now have an expectation for the gradient. Of course, we cannot
    fully evaluate it because we don't know ![](img/Formula_07_036.png), but we can
    take samples from the environment.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这将变成一个非常便捷的梯度公式。退后一步，我们现在得到了梯度的期望。当然，我们不能完全评估它，因为我们不知道![](img/Formula_07_036.png)，但我们可以从环境中获取样本。
- en: Tip
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Whenever you see an expectation in RL formulations, you can reasonably expect
    that we will use samples from the environment to evaluate it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 每当你在强化学习公式中看到期望值时，你可以合理地预期我们将使用来自环境的样本来进行评估。
- en: This formulation forms the essence of the policy gradient methods. Next, let's
    see how we can conveniently do it.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式构成了策略梯度方法的核心。接下来，让我们看看我们如何能更便捷地进行。
- en: Obtaining the gradient
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获得梯度
- en: 'Before going into estimating the gradient from samples, we need to get rid
    of one more term, ![](img/Formula_07_037.png), because we don''t really know what
    that is. It turns out that we can do so by writing the explicit probability product
    for the trajectory:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始从样本中估计梯度之前，我们需要消除另外一个项![](img/Formula_07_037.png)，因为我们并不真正知道它是什么。结果证明，我们可以通过为轨迹写出明确的概率乘积来解决这个问题：
- en: '![](img/Formula_07_038.jpg)![](img/Formula_07_039.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_038.jpg)![](img/Formula_07_039.jpg)'
- en: 'When we take the gradient with respect to ![](img/Formula_06_096.png), the
    first and last terms in the sum drop since they don''t depend on ![](img/Formula_07_041.png).
    With that, we can write this gradient in terms of what we know, namely, ![](img/Formula_07_042.png),
    the policy that we possess:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对 ![](img/Formula_06_096.png) 求梯度时，求和中的第一个和最后一个项会被去掉，因为它们不依赖于 ![](img/Formula_07_041.png)。因此，我们可以用已知的内容来表示这个梯度，即我们拥有的策略
    ![](img/Formula_07_042.png)：
- en: '![](img/Formula_07_043.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_043.jpg)'
- en: 'We can then estimate the gradient from a batch of ![](img/Formula_07_044.png)
    trajectories as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式从一批 ![](img/Formula_07_044.png) 轨迹中估计梯度：
- en: '![](img/Formula_07_045.jpg)![](img/Formula_07_046.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_045.jpg)![](img/Formula_07_046.jpg)'
- en: This gradient aims to increase the likelihood of trajectories that have high
    total rewards, and reduce the ones (or increase them less) with low total rewards.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 该梯度旨在增加具有高总奖励的轨迹的可能性，并减少那些低总奖励的轨迹（或增加它们的可能性较少）。
- en: This gives us all the ingredients to put together a policy gradient algorithm,
    namely REINFORCE, which we turn to next.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了构建一个策略梯度算法的所有要素，即 REINFORCE，接下来我们将继续介绍。
- en: REINFORCE
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: REINFORCE
- en: The REINFORCE algorithm, one of the earliest policy gradient methods, uses the
    ingredients we presented above. We will need a lot of improvements on top of this
    to come up with methods with which we can attack realistic problems. On the other
    hand, understanding REINFORCE is useful to formalize these ideas in the context
    of an algorithm.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 算法是最早的策略梯度方法之一，使用了我们上面介绍的内容。为了能够解决现实问题，我们需要在此基础上做很多改进。另一方面，理解 REINFORCE
    对于在算法的背景下形式化这些想法是有用的。
- en: 'REINFORCE works as follows in finite horizon problems with the discount factors
    ignored:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在忽略折扣因子的有限时间问题中，REINFORCE的工作原理如下：
- en: 'Initialize a policy ![](img/Formula_07_047.png):'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个策略 ![](img/Formula_07_047.png)：
- en: '*while some stopping criterion is not met do:*'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*当某些停止准则未满足时，执行：*'
- en: Collect ![](img/Formula_07_048.png) trajectories ![](img/Formula_07_049.png)
    from the environment using ![](img/Formula_07_050.png).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ![](img/Formula_07_050.png) 从环境中收集 ![](img/Formula_07_048.png) 轨迹 ![](img/Formula_07_049.png)。
- en: Calculate
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算
- en: '![](img/Formula_07_051.png).'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![](img/Formula_07_051.png)。'
- en: 'Update ![](img/Formula_07_052.png):'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 ![](img/Formula_07_052.png)：
- en: '*end while*'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*结束时*'
- en: The REINFORCE algorithm simply suggests sampling trajectories from the environment
    using the policy on hand, then estimating the gradient using these samples and
    taking a gradient step to update the policy parameters.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 算法简单地建议使用当前的策略从环境中采样轨迹，然后使用这些样本估计梯度，并采取梯度步骤来更新策略参数。
- en: Info
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: The fact that the sampled trajectories are used to obtain a gradient estimate
    for the policy parameters at hand makes policy gradient methods **on-policy**.
    Therefore, we cannot use samples obtained under a different policy to improve
    the existing policy, unlike in value-based methods. Having said that, we will
    have a section at the end of the chapter to discuss several off-policy approaches.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于采样的轨迹用于获取当前策略参数的梯度估计，策略梯度方法是**基于策略的**。因此，我们不能使用在不同策略下获得的样本来改善现有策略，这与基于价值的方法不同。话虽如此，本章最后会有一节讨论几种基于策略外的方法。
- en: The REINFORCE algorithm requires complete trajectories for network updates,
    therefore it is a Monte Carlo method.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 算法要求使用完整的轨迹进行网络更新，因此它是一种蒙特卡洛方法。
- en: Next, let's discuss why we need improvements on top of REINFORCE.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论为什么我们需要对 REINFORCE 进行改进。
- en: The problem with REINFORCE and all policy gradient methods
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: REINFORCE 和所有策略梯度方法的问题
- en: 'The most important issue with the policy gradient algorithms, in general, is
    the high variance in the ![](img/Formula_07_053.png) estimates. If you think about
    it, there are many factors contributing to this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，策略梯度算法的最重要问题是 ![](img/Formula_07_053.png) 估计的高方差。如果你仔细思考，实际上有许多因素导致了这一点：
- en: '**Randomness in the environment** could lead to many different trajectories
    for the agent even with the same policy, whose gradients are likely to vary significantly.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境中的随机性**可能导致即使使用相同策略，代理也会有许多不同的轨迹，这些轨迹的梯度可能会有很大差异。'
- en: '**The length of sample trajectories** could vary significantly, resulting in
    very different sums for the log and reward terms.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本轨迹的长度**可能会有很大差异，导致对数和奖励项的总和差异很大。'
- en: '**Environments with sparse rewards** could be especially problematic (by the
    definition of sparse reward).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏奖励的环境**可能尤其成问题（根据稀疏奖励的定义）。'
- en: '**The size of** ![](img/Formula_07_054.png) is usually kept at a few thousand
    to make the learning practical, but it may not be enough to capture the full distribution
    of trajectories.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大小** ![](img/Formula_07_054.png) 通常保持在几千左右，以使学习过程可行，但它可能不足以捕捉完整的轨迹分布。'
- en: As a result, the gradient estimates we obtain from samples could have a high
    variance, which is likely to destabilize the learning. Reducing this variance
    is an important goal to make learning feasible, and we employ various tricks towards
    this end. Next, we cover the first of those tricks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们从样本中获得的梯度估计可能具有较高的方差，这可能会导致学习不稳定。减少这种方差是使学习可行的一个重要目标，我们为此采用了各种技巧。接下来，我们将介绍其中的第一个技巧。
- en: Replacing the reward sum with reward-to-go
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用前往奖励替换奖励总和
- en: 'Let''s first rearrange the terms in the gradient estimation as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先将梯度估计中的项重新排列如下：
- en: '![](img/Formula_07_055.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_055.jpg)'
- en: 'This original form implies that each of the ![](img/Formula_07_056.png) terms
    are weighed by the total reward obtained throughout the entire trajectory. Intuition,
    on the other hand, tells us that we should be weighing the log term only by the
    sum of rewards following that state-action pair as they cannot affect what came
    before it (causality). More formally, we can write the gradient estimate as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这种原始形式意味着每个 ![](img/Formula_07_056.png) 项都会根据整个轨迹获得的总奖励进行加权。然而直觉告诉我们，我们应该只根据跟随该状态-动作对的奖励总和来加权对数项，因为它们无法影响之前发生的事情（因果关系）。更正式地，我们可以将梯度估计写为如下：
- en: '![](img/Formula_07_057.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_057.jpg)'
- en: It turns out that this still gives us an unbiased estimate of the gradient.
    The variance also reduces as the sums get smaller as we add fewer reward terms,
    and as a result, the weights that multiply the log terms get smaller. The ![](img/Formula_07_058.png)
    is called the reward-to-go at time ![](img/Formula_07_059.png). Notice how this
    is actually an estimate for ![](img/Formula_07_060.png). We will make use of it
    later in the actor-critic algorithms.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这仍然能为我们提供一个无偏的梯度估计。随着我们减少奖励项的数量，方差也会减小，结果是乘以对数项的权重变得更小。![](img/Formula_07_058.png)
    被称为时间 ![](img/Formula_07_059.png) 的前往奖励。请注意，这实际上是对 ![](img/Formula_07_060.png)
    的估计。我们将在后续的演员-评论员算法中使用它。
- en: This improvement over the REINFORCE algorithm is a form of a *vanilla policy
    gradient* method. Next, we'll show how to use the RLlib vanilla policy gradient
    implementation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对 REINFORCE 算法的改进是一种 *普通策略梯度* 方法。接下来，我们将展示如何使用 RLlib 的普通策略梯度实现。
- en: Vanilla policy gradient using RLlib
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 RLlib 的普通策略梯度
- en: RLlib allows us to use the vanilla policy gradient with multiple rollout workers
    (actors) to parallelize the sample collection. You will notice that, unlike in
    value-based methods, the sample collection will be synchronized with network weight
    updates as the vanilla policy gradient algorithm is an on-policy method.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib 允许我们使用普通策略梯度与多个回合工作者（演员）一起并行化样本收集。你会注意到，与基于值的方法不同，样本收集将与网络权重更新同步，因为普通策略梯度算法是一种基于策略的方法。
- en: Info
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: Since policy gradient methods are on-policy, we need to make sure that the samples
    we use to update the neural network parameters (weights) come from the existing
    policy suggested by the network. This dictates synchronizing the policy in use
    across the rollout workers.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于策略梯度方法是基于当前策略的，我们需要确保用于更新神经网络参数（权重）的样本来自网络建议的现有策略。这就要求在所有回合工作者中同步使用的策略。
- en: 'The overall architecture of the parallelized vanilla policy gradient is therefore
    as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化的普通策略梯度的总体架构如下所示：
- en: '![Figure 7.2 – Vanilla policy gradient architecture'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2 – 普通策略梯度架构'
- en: '](img/B14160_07_2.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_07_2.jpg)'
- en: Figure 7.2 – Vanilla policy gradient architecture
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 普通策略梯度架构
- en: At this point, it is worth noting that RLlib implementation transmits samples
    as ![](img/Formula_07_061.png) from actors to the learner and concatenates them
    in the learner to restore the full trajectories.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，值得注意的是 RLlib 实现将样本作为 ![](img/Formula_07_061.png) 从演员传输到学习者，并在学习者中将它们连接起来以恢复完整的轨迹。
- en: Using the vanilla policy gradient in RLlib is pretty simple and very similar
    to how we used value-based methods in the previous chapter. Let's train a model
    for the OpenAI Lunar Lander environment with continuous action space. Follow along!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RLlib 中使用原始策略梯度非常简单，且与我们在上一章中使用基于值的方法非常相似。让我们为 OpenAI Lunar Lander 环境（具有连续动作空间）训练一个模型。请跟着一起操作！
- en: 'First, to avoid running into issues with the Gym and Box2D packages, install
    Gym using the following:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，为了避免与 Gym 和 Box2D 包发生冲突，请使用以下命令安装 Gym：
- en: '[PRE0]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then go to implementing the Python code. Import the packages we need for argument
    parsing, `ray` and `tune`:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后开始实现 Python 代码。导入我们需要的包，用于参数解析的 `ray` 和 `tune`：
- en: '[PRE1]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Import the vanilla **policy** **gradient** (**PG**) trainer class and the corresponding
    config dictionary:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入原始 **策略** **梯度**（**PG**）训练器类以及相应的配置字典：
- en: '[PRE2]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that this part will be different when we want to use different algorithms.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，当我们想使用不同的算法时，这部分会有所不同。
- en: 'Create a main function, which receives the Gym environment name as an argument:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个主函数，该函数接收 Gym 环境名称作为参数：
- en: '[PRE3]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Modify the config dictionary with the number of GPUs we want to use for training
    and the number of CPUs we want to use for sample collection and evaluation:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改配置字典，设置我们希望用于训练的 GPU 数量以及我们希望用于样本收集和评估的 CPU 数量：
- en: '[PRE4]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `print` statement is for you to see what other configurations are available
    to you if you want to change it. You can modify things like the learning rate,
    for example. For now, we are not going into such hyperparameter optimization details.
    And for the vanilla policy gradient, the number of hyperparameters is considerably
    less than a more sophisticated algorithm would involve. One final note: the reason
    we set a separate set of evaluation workers is to make the training consistent
    with the off-policy algorithms we will introduce later. Normally, we don''t need
    to do that since on-policy methods follow the same policy during training and
    evaluation.'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`print` 语句是让您查看如果需要更改配置时，还有哪些其他配置可供选择。例如，您可以修改学习率。现在，我们暂时不讨论这种超参数优化的细节。对于原始策略梯度，超参数的数量远少于更复杂算法所涉及的数量。最后一点说明：我们设置单独的评估工作者是为了使训练与我们后续介绍的离策略算法保持一致。通常，我们不需要这么做，因为在策略方法中，训练和评估过程中都遵循相同的策略。'
- en: 'Implement the section to initialize `ray` and train the agent for a given number
    of iterations:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现这一部分，初始化 `ray` 并为给定的迭代次数训练代理：
- en: '[PRE5]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Save this code in a Python file, say, `pg_agent.py`. You can then train the
    agent as follows:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此代码保存在一个 Python 文件中，例如 `pg_agent.py`。然后，您可以按如下方式训练代理：
- en: '[PRE6]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Monitor the training on TensorBoard:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 TensorBoard 上监控训练过程：
- en: '[PRE7]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The training progress will look like the following:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练进度将如下所示：
- en: '![Figure 7.3 – Training progress for a vanilla policy gradient agent in Gym''s
    continuous Lunar Lander'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.3 – Gym 中连续 Lunar Lander 环境下原始策略梯度代理的训练进度'
- en: '](img/B14160_07_3.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_07_3.jpg)'
- en: Figure 7.3 – Training progress for a vanilla policy gradient agent in Gym's
    continuous Lunar Lander
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – Gym 中连续 Lunar Lander 环境下原始策略梯度代理的训练进度
- en: 'That''s it about the vanilla policy gradient method! Not a bad performance
    for an algorithm without many of the improvements that we will introduce in the
    upcoming sections. Feel free to try this algorithm with the other Gym environments.
    Hint: the Pendulum environment could give you some headaches.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是关于原始策略梯度方法的全部内容！对于一个没有许多改进的算法来说，表现还不错，我们将在接下来的章节中引入这些改进。欢迎尝试在其他 Gym 环境中使用此算法。提示：Pendulum
    环境可能会让您头疼。
- en: Tip
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'To save the best performing models while training the model with Tune, you
    will need to write a simple wrapper training function, which is described here:
    [https://github.com/ray-project/ray/issues/7983](https://github.com/ray-project/ray/issues/7983).
    You save the model whenever you observe an improvement in the evaluation score.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Tune 训练模型时，为了保存最佳表现的模型，您需要编写一个简单的包装训练函数，详细描述可以参考这里：[https://github.com/ray-project/ray/issues/7983](https://github.com/ray-project/ray/issues/7983)。每当您观察到评估得分的提升时，就保存模型。
- en: 'Next, we''ll cover a more powerful class of algorithms: actor-critic methods.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一种更强大的算法类别：演员-评论家方法。
- en: Actor-critic methods
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 演员-评论家方法
- en: Actor-critic methods propose further remedies to the high variance problem in
    the policy gradient algorithm. Just like REINFORCE and other policy gradient methods,
    actor-critic algorithms have been around for decades now. Combining this approach
    with deep reinforcement learning, however, has enabled them to solve more realistic
    RL problems. We'll start this section by presenting the ideas behind the actor-critic
    approach, and later, we'll define them in more detail.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 演员-评论家方法为解决策略梯度算法中的高方差问题提出了进一步的补救措施。就像REINFORCE和其他策略梯度方法一样，演员-评论家算法已经存在了几十年。然而，将这种方法与深度强化学习结合，使其能够解决更现实的RL问题。我们将在本节开始时介绍演员-评论家方法背后的思想，随后会更详细地定义它们。
- en: Further reducing the variance in policy-based methods
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步减少基于策略方法中的方差
- en: Remember that, earlier, to reduce the variance in gradient estimates, we replaced
    the reward sum obtained in a trajectory with a reward-to-go term. Although a step
    in the right direction, it is usually not enough. We'll now introduce two more
    methods to further reduce this variance.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，之前为了减少梯度估计中的方差，我们将轨迹中获得的奖励总和替换为奖励累积项。虽然这朝着正确的方向迈出了第一步，但通常还不够。我们现在将介绍两种方法，进一步减少这个方差。
- en: Estimating the reward-to-go
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 估计奖励的累积值
- en: The reward-to-go term, ![](img/Formula_07_062.png), obtained in a trajectory
    is an estimate of the action-value ![](img/Formula_07_063.png) under the existing
    policy ![](img/Formula_07_064.png).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在一条轨迹中获得的奖励累积项，![](img/Formula_07_062.png)，是现有策略下的动作值 ![](img/Formula_07_063.png)
    的估计值 ![](img/Formula_07_064.png)。
- en: Info
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: Notice the difference between the action-value estimate we use here, ![](img/Formula_07_065.png),
    and what the Q-learning methods estimate, ![](img/Formula_07_066.png). The former
    estimates the action-value under the existing behavior policy ![](img/Formula_05_046.png),
    whereas the latter estimates the action-value under the target policy that is
    ![](img/Formula_07_068.png).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们在此使用的动作值估计值 ![](img/Formula_07_065.png) 和 Q-learning 方法估计的值 ![](img/Formula_07_066.png)
    之间的区别。前者估计的是现有行为策略下的动作值 ![](img/Formula_05_046.png)，而后者估计的是目标策略下的动作值，即 ![](img/Formula_07_068.png)。
- en: Now, every trajectory that visits a particular ![](img/Formula_07_069.png) pair
    is likely to yield a different reward-to-go estimate. This adds to the variance
    in gradient estimates. What if we could use a single estimate for a given ![](img/Formula_07_070.png)
    in a policy update cycle? That would eliminate the variance caused by those noisy
    reward-to-go (action-value) estimates. But how do we obtain such an estimate?
    The answer is to train a neural network that will generate that estimate for us.
    We then train this network using the sampled reward-to-go values. When we query
    it to obtain an estimate for a specific state-action pair, it gives us a single
    number rather than many different estimates, which, in turn, reduces the variance.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每条访问特定 ![](img/Formula_07_069.png) 对的轨迹可能会产生不同的奖励累积估计值。这会增加梯度估计中的方差。如果我们能够在每次策略更新周期中使用一个固定的估计值来表示给定的
    ![](img/Formula_07_070.png) 呢？这将消除由那些噪声奖励累积（动作值）估计值引起的方差。但是我们如何获得这样的估计值呢？答案是训练一个神经网络来为我们生成该估计值。然后我们使用采样的奖励累积值来训练这个网络。当我们查询它以获取特定状态-动作对的估计时，它会给我们一个单一的数字，而不是多个不同的估计，这反过来减少了方差。
- en: In essence, what such a network does is it evaluates the policy, which is why
    we call it the **critic**, while the policy network tells the agent how to **act**
    in the environment – hence the name **actor-critic**.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这种网络的作用是评估策略，这就是为什么我们称之为**评论家**，而策略网络则告诉智能体如何在环境中**行动**——因此得名**演员-评论家**。
- en: Info
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: We don't train the critic network from scratch after each policy update. Instead,
    just like the policy network, we apply gradient updates with new sampled data.
    As a result, the critic is biased towards the old policies. However, we are willing
    to make this trade-off to reduce the variance.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在每次策略更新后从头开始训练评论家网络。相反，像策略网络一样，我们使用新的采样数据进行梯度更新。因此，评论家会偏向于旧的策略。然而，我们愿意做出这个权衡，以减少方差。
- en: Last but not least, we use baselines to reduce the variance, which we turn to
    next.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们使用基准线来减少方差，接下来我们将详细讲解这一点。
- en: Using a baseline
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用基准线
- en: 'The intuition behind the policy gradient methods is that we would like to adjust
    the parameters of the policy so that actions that result in high-reward trajectories
    become more likely, and ones that led to low-reward trajectories become less likely:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 策略梯度方法背后的直觉是，我们希望调整策略的参数，使得产生高奖励轨迹的动作变得更加可能，而产生低奖励轨迹的动作变得不太可能：
- en: '![](img/Formula_07_071.jpg)![](img/Formula_07_072.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_071.jpg)![](img/Formula_07_072.jpg)'
- en: 'One shortcoming in this formulation is that the direction and the magnitude
    of the gradient steps are heavily determined by the total reward in the trajectory,
    ![](img/Formula_07_073.png). Consider the following two examples:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这种公式的一个缺点是，梯度步长的方向和大小在很大程度上由轨迹中的总奖励！[](img/Formula_07_073.png)决定。考虑以下两个例子：
- en: A maze environment that the agent tries to exit in the minimum time. The reward
    is the negative of the time elapsed until the exit is reached.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个迷宫环境，智能体试图在最短时间内退出。奖励是直到到达出口所用的时间的负值。
- en: The same maze environment but the reward is $1M, minus a dollar penalty for
    each second that passes until the exit is reached.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同的迷宫环境，但奖励为$1M，每经过一秒钟就会扣除一美元的惩罚，直到到达出口。
- en: Mathematically, these two are the same optimization problems. Now, think about
    what gradient steps a particular trajectory obtained under some policy ![](img/Formula_07_074.png)
    would lead to under these two different reward structures. The first reward structure
    would lead to a negative gradient step for all trajectories (although some smaller
    than others) regardless of the quality of the policy, and the second reward structure
    would (almost certainly) lead to a positive gradient step. Moreover, under the
    latter, the impact of the elapsed seconds would be negligible since the fixed
    reward is too big, making the learning for the policy network very difficult.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，这两者是相同的优化问题。现在，考虑某个策略！[](img/Formula_07_074.png)下，特定轨迹会导致什么样的梯度步长，假设奖励结构有所不同。第一种奖励结构会导致所有轨迹的负梯度更新（尽管有些比其他的更小），而不管策略的质量如何；而第二种奖励结构几乎肯定会导致正梯度更新。此外，在后者的情况下，经过的时间的影响可以忽略不计，因为固定奖励太大，这使得策略网络的学习变得非常困难。
- en: 'Ideally, we want to measure the **relative** reward performance observed in
    a particular trajectory as a result of the sequence of actions taken in it compared
    to the other trajectories. This way, we can take positive gradient steps in the
    direction of the parameters that result in high-reward trajectories and take negative
    gradient steps for the others. To measure the relative performance, a simple trick
    is to subtract a baseline ![](img/Formula_07_075.png) from the reward sum:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望衡量在特定轨迹中观察到的**相对**奖励表现，即相对于其他轨迹，某个动作序列所带来的奖励。这样，我们就可以朝着那些产生高奖励轨迹的参数方向进行正梯度更新，而对于其他轨迹则进行负梯度更新。为了衡量相对表现，一个简单的技巧是从奖励和中减去基准！[](img/Formula_07_075.png)：
- en: '![](img/Formula_07_076.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_076.jpg)'
- en: 'The most obvious choice for the baseline is the average trajectory reward,
    sampled as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最直观的基准选择是平均轨迹奖励，样本采样如下：
- en: '![](img/Formula_07_077.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_077.jpg)'
- en: It turns out that subtracting such a term still gives an unbiased estimate for
    the gradient, but with less variance, and the difference could be dramatic in
    some settings. So, using baselines is almost always good.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，减去这样的项仍然可以给出梯度的无偏估计，但方差较小，而且在某些情况下，差异可能非常显著。因此，使用基准几乎总是好的。
- en: 'When combined with using reward-to-go estimates, a natural choice for the baseline
    is the state value, which results in the following gradient estimate:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当结合使用奖励未来预估时，基准的自然选择是状态价值，这样就得到了以下的梯度估计：
- en: '![](img/Formula_07_078.jpg)![](img/Formula_07_079.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_078.jpg)![](img/Formula_07_079.jpg)'
- en: where ![](img/Formula_07_080.png) is the **advantage** term, which measures
    how much the agent is better off by taking action ![](img/Formula_07_081.png)
    in state ![](img/Formula_07_082.png) as opposed to following the existing policy.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中！[](img/Formula_07_080.png)是**优势**项，表示智能体通过在状态！[](img/Formula_07_082.png)采取动作！[](img/Formula_07_081.png)相较于遵循现有策略，能够获得多少收益。
- en: Having a critic estimating the advantage, directly or indirectly, gives rise
    to the **advantage actor-critic** algorithms, which we'll cover next.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 由评论员估计优势，直接或间接地，产生了**优势演员-评论员**算法，接下来我们将介绍这一部分。
- en: Advantage Actor-Critic – A2C
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势演员-评论员 – A2C
- en: What we have covered so far is almost sufficient to put together what is known
    as the A2C algorithm. Let's discuss in a bit more detail how to estimate the advantage
    term before going into the full algorithm and the RLlib implementation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所涵盖的内容几乎足以组成所谓的 A2C 算法。让我们在详细讨论完整的算法和 RLlib 实现之前，再详细讨论一下如何估计优势项。
- en: How to estimate the advantage
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何估计优势
- en: 'There are different ways of estimating the advantage using a critic. The critic
    network could do the following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用评论家估计优势有不同的方法。评论家网络可以执行以下操作：
- en: Directly estimate ![](img/Formula_07_083.png).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接估计 ![](img/Formula_07_083.png)。
- en: Estimate ![](img/Formula_07_084.png), from which we can recover ![](img/Formula_07_085.png).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计 ![](img/Formula_07_084.png)，从中我们可以恢复 ![](img/Formula_07_085.png)。
- en: 'Note that both of these approaches involve maintaining a network that gives
    outputs that depend on both the state and the action. However, we can get away
    with a simpler structure. Remember the definition of action-value:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这两种方法都涉及到维护一个网络，其输出依赖于状态和动作。然而，我们可以用一个更简单的结构。记住动作值的定义：
- en: '![](img/Formula_07_086.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_086.jpg)'
- en: 'When we sample a transition of a single step, we already observe the reward
    and the next state and obtain a tuple ![](img/Formula_07_087.png). We can therefore
    obtain the estimate ![](img/Formula_07_088.png) as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们采样单步转移时，我们已经观察到了奖励和下一个状态，并获得了一个元组 ![](img/Formula_07_087.png)。因此，我们可以按如下方式获得估计值
    ![](img/Formula_07_088.png)：
- en: '![](img/Formula_07_089.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_089.jpg)'
- en: where ![](img/Formula_07_090.png) is some estimate of the true state value ![](img/Formula_05_021.png).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_07_090.png) 是真实状态值 ![](img/Formula_05_021.png) 的某个估计值。
- en: Info
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: Notice the nuances in the notation. ![](img/Formula_07_092.png) and ![](img/Formula_07_093.png)
    represent the true values where ![](img/Formula_07_094.png) and ![](img/Formula_07_095.png)
    are their estimates. ![](img/Formula_07_096.png), and ![](img/Formula_07_097.png)
    are random variables, whereas ![](img/Formula_07_098.png), and ![](img/Formula_07_099.png)
    are their realizations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意符号中的细微差别。![](img/Formula_07_092.png) 和 ![](img/Formula_07_093.png) 表示真实值，而
    ![](img/Formula_07_094.png) 和 ![](img/Formula_07_095.png) 是它们的估计值。![](img/Formula_07_096.png)
    和 ![](img/Formula_07_097.png) 是随机变量，而 ![](img/Formula_07_098.png) 和 ![](img/Formula_07_099.png)
    是它们的实现。
- en: 'We can finally estimate the advantage as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终可以按如下方式估计优势：
- en: '![](img/Formula_07_100.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_100.jpg)'
- en: This allows us to use a neural network that simply estimates the state values
    to obtain an advantage estimate. To train this network, we can do bootstrapping
    to obtain the target values for state values. So, using a sampled tuple ![](img/Formula_07_101.png),
    the target for ![](img/Formula_07_102.png) is calculated as ![](img/Formula_07_103.png)
    (the same as the ![](img/Formula_07_104.png) estimate because we happen to obtain
    the actions from the existing stochastic policy).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够使用一个仅仅估计状态值的神经网络来获得优势估计。为了训练这个网络，我们可以使用引导法来获得状态值的目标值。因此，使用采样元组 ![](img/Formula_07_101.png)，![](img/Formula_07_102.png)
    的目标被计算为 ![](img/Formula_07_103.png)（与 ![](img/Formula_07_104.png) 的估计相同，因为我们碰巧从现有的随机策略中获得了动作）。
- en: Before presenting the full A2C algorithm, let's take a look at the implementation
    architecture.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示完整的 A2C 算法之前，让我们先来看一下实现架构。
- en: A2C architecture
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A2C 架构
- en: A2C suggests a synchronized sample collection between different agents, that
    is, all of the rollout workers use the same policy network at a given time to
    collect samples. Those samples are then passed to the learner to update the actor
    (policy network) and the critic (value network). In this sense, the architecture
    is pretty much the same with the vanilla policy gradient, for which we provided
    the schema above. Except, this time, we have a critic network. Then, the question
    is how to bring the critic network in.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: A2C 提出不同代理之间同步采样，也就是说，所有的回合工作者在同一时间使用相同的策略网络来收集样本。然后，这些样本被传递给学习者，更新演员（策略网络）和评论家（价值网络）。从这个意义上讲，架构和普通的策略梯度基本相同，关于这一点我们上面已经给出了示意图。唯一的区别是，这次我们有一个评论家网络。那么，问题是如何将评论家网络引入其中。
- en: 'The design of the actor and critic can range from completely isolated neural
    networks, as shown in the following figure on the left, to a completely shared
    design (except the last layers):'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 演员和评论家的设计可以从完全隔离的神经网络（如下图左侧所示）到完全共享的设计（除了最后几层）不等：
- en: '![Figure 7.4 – Isolated versus shared neural networks'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.4 – 隔离神经网络与共享神经网络'
- en: '](img/B14160_07_4.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_07_4.jpg)'
- en: Figure 7.4 – Isolated versus shared neural networks
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 独立与共享神经网络
- en: The advantage of the isolated design is that it is usually more stable. This
    is because the variance and the magnitude of the target values of the actor and
    critic targets could be very different. Having them share the neural network requires
    careful tuning of hyperparameters such as the learning rate, otherwise, the learning
    would be unstable. On the other hand, using a shared architecture has the advantage
    of cross-learning and using common feature extraction capabilities. This could
    be especially handy when feature extraction is a major part of training, such
    as when observations are images. Of course, any architecture in between is also
    possible.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 独立设计的优势在于它通常更稳定。这是因为演员和评论员目标的方差和目标值的大小可能非常不同。如果它们共享神经网络，就需要仔细调整学习率等超参数，否则学习可能会不稳定。另一方面，使用共享架构的优势在于可以进行交叉学习，并利用共同的特征提取能力。当特征提取是训练的重要部分时（例如当观测值为图像时），这一点尤其有用。当然，介于两者之间的任何架构也是可能的。
- en: Finally, it is time to present the A2C algorithm.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是时候介绍 A2C 算法了。
- en: The A2C algorithm
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A2C 算法
- en: 'Let''s put all these ideas together and form the A2C algorithm:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有这些思想汇集在一起，形成 A2C 算法：
- en: Initialize the actor and critic network(s), ![](img/Formula_07_105.png) and
    ![](img/Formula_07_106.png).
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化演员和评论员网络，![](img/Formula_07_105.png) 和 ![](img/Formula_07_106.png)。
- en: '*while some stopping criterion is not met do:*'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*当某些停止准则未被满足时，执行以下操作：*'
- en: Collect a batch of ![](img/Formula_07_107.png) samples ![](img/Formula_07_108.png)
    from the (parallel) environment(s) using ![](img/Formula_07_109.png).
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ![](img/Formula_07_109.png) 从（并行）环境收集一批 ![](img/Formula_07_107.png) 样本 ![](img/Formula_07_108.png)。
- en: Obtain the state-value targets ![](img/Formula_07_110.png).
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取状态值目标 ![](img/Formula_07_110.png)。
- en: Use gradient descent to update ![](img/Formula_07_111.png) with respect to a
    loss function ![](img/Formula_07_112.png), such as squared loss.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降更新 ![](img/Formula_07_111.png)，以优化损失函数 ![](img/Formula_07_112.png)，例如平方损失。
- en: 'Obtain the advantage value estimates:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取优势值估计：
- en: '![](img/Formula_07_113.png).'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![](img/Formula_07_113.png)。'
- en: Calculate
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算
- en: '![](img/Formula_07_114.png).'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![](img/Formula_07_114.png)。'
- en: Update
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新
- en: '![](img/Formula_07_115.png).'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![](img/Formula_07_115.png)。'
- en: Broadcast the new ![](img/Formula_07_116.png) to the rollout workers.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新的 ![](img/Formula_07_116.png) 广播到回滚工作者。
- en: '*end while*'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*结束时*'
- en: Note that we could also use multi-step learning rather than using single-step
    estimation for advantage estimates and state value targets. We will present a
    generalized version of multi-step learning at the end of the actor-critic section.
    But now, let's see how you can use RLlib's A2C algorithm.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们也可以使用多步学习，而不是使用单步估计来计算优势估计和状态值目标。我们将在演员-评论员部分的最后呈现多步学习的广义版本。但现在，让我们来看一下如何使用
    RLlib 的 A2C 算法。
- en: A2C using RLlib
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 RLlib 的 A2C
- en: 'To train an RL agent in RLlib using A2C is very similar to how we did it for
    the vanilla policy gradient. Therefore, rather than presenting the full flow again,
    we''ll just describe what the difference is. The main difference is importing
    the A2C class:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RLlib 中使用 A2C 训练 RL 代理与我们为传统策略梯度所做的非常相似。因此，我们不会再次呈现完整的流程，而是简要描述它们之间的区别。主要的区别在于导入
    A2C 类：
- en: '[PRE8]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can then train the agent the same way as the vanilla policy gradient agent.
    Instead of presenting the results from our training here, we will compare all
    the algorithms at the end of this chapter.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以像传统策略梯度代理一样训练该代理。我们不会在这里展示我们的训练结果，而是会在本章结束时对所有算法进行比较。
- en: 'Next, we present another famous actor-critic algorithm: A3C.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍另一种著名的演员-评论员算法：A3C。
- en: 'Asynchronous Advantage Actor-Critic: A3C'
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步优势演员评论员：A3C
- en: A3C is pretty much the same as A2C in terms of the loss functions and how it
    uses the critic. In fact, A3C is a predecessor of A2C, although we presented them
    here in reverse order for pedagogical reasons. The differences between A2C and
    A3C are architectural and how the gradients are calculated and applied. Next,
    let's discuss the A3C architecture.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: A3C 在损失函数和如何使用评论员方面与 A2C 非常相似。事实上，A3C 是 A2C 的前身，尽管我们为了教学目的将它们按相反顺序呈现。A2C 和 A3C
    之间的区别在于架构，以及梯度的计算和应用方式。接下来，让我们讨论 A3C 的架构。
- en: A3C architecture
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A3C 架构
- en: 'A3C architecture differs from that of A2C as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: A3C 架构与 A2C 的不同之处如下：
- en: The asynchrony in A3C is due to the fact the rollout workers pull the ![](img/Formula_07_117.png)
    parameters from the main policy network at their own pace, not in sync with other
    workers.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A3C中的异步性是因为回滚工作者以自己的节奏从主策略网络拉取参数！[](img/Formula_07_117.png)，而不是与其他工作者同步。
- en: As a result, the workers are likely to be using different policies at the same
    time.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果是，工作者们可能会同时使用不同的策略。
- en: To avoid calculating gradients at the central learner using samples that are
    likely to have been obtained under different policies, the gradients are calculated
    by the rollout workers with respect to the policy parameters in use at that time
    in the worker.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免在中央学习者计算梯度时使用可能已经在不同策略下获得的样本，梯度是由回滚工作者根据当时在工作者中使用的策略参数计算的。
- en: What is passed to the learner is therefore not the samples but the gradients.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，传递给学习者的不是样本，而是梯度。
- en: Those gradients are applied to the main policy network, again, asynchronously
    as they arrive.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些梯度被应用到主策略网络，再次是异步地应用，随着它们到达。
- en: 'The following diagram depicts the A3C architecture:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 下图描绘了A3C架构：
- en: '![Figure 7.5 – A3C architecture'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.5 – A3C架构'
- en: '](img/B14160_07_5.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_07_5.jpg)'
- en: Figure 7.5 – A3C architecture
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – A3C架构
- en: 'There are two important downsides to A3C:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: A3C有两个重要的缺点：
- en: The gradients that update the main policy network are likely to be obsolete
    and obtained under a different ![](img/Formula_07_011.png) than what is in the
    main policy network. This is theoretically problematic as those are not the true
    gradients for the policy network parameters.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新主策略网络的梯度可能已经过时，并且是在与主策略网络中不同的！[](img/Formula_07_011.png)下获得的。这在理论上是有问题的，因为这些不是策略网络参数的真实梯度。
- en: Passing around gradients, which could be a large vector of numbers, especially
    when the neural network is big, could create a significant communication overhead
    compared to passing only samples.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传递梯度，尤其是当神经网络很大时，梯度可能是一个巨大的数字向量，这可能会产生相对于仅传递样本的显著通信开销。
- en: The main motivation behind A3C despite these disadvantages is obtaining decorrelated
    samples and gradient updates, similar to the role played by experience replay
    in deep Q-learning. On the other hand, in many experiments, people found A2C to
    be as good as A3C, and sometimes even better. As a result, A3C is not very commonly
    used. Still, we have presented it so you understand how these algorithms have
    evolved and what are the key differences between them. Let's also look into how
    you can use RLlib's A3C module.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些缺点，A3C的主要动机是获得不相关的样本和梯度更新，类似于经验回放在深度Q学习中的作用。另一方面，在许多实验中，人们发现A2C与A3C一样好，有时甚至更好。因此，A3C并不常用。不过，我们展示它是为了让你理解这些算法是如何发展的，以及它们之间的关键区别。让我们也看看如何使用RLlib的A3C模块。
- en: A3C using RLlib
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用RLlib的A3C
- en: 'RLlib''s A3C algorithm can simply be accessed by importing the corresponding
    trainer class:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: RLlib的A3C算法可以通过导入相应的训练器类轻松访问：
- en: '[PRE9]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then, you can train an agent following the code we provided earlier in the section.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以按照我们之前提供的代码训练一个代理。
- en: Finally, we'll close the discussion in this section with a generalization of
    multi-step RL in the context of policy gradient methods.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在本节结束时讨论政策梯度方法背景下的多步RL的概括。
- en: Generalized advantage estimators
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义优势估计器
- en: 'We previously mentioned that you can use multi-step estimates for the advantage
    function. Namely, instead of using only a single step transition as in the following:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，您可以使用多步估计来计算优势函数。也就是说，不仅仅使用单步过渡，如下所示：
- en: '![](img/Formula_07_119.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_119.jpg)'
- en: 'using ![](img/Formula_05_231.png)-step transitions could yield a more accurate
    estimate of the advantage function:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用！[](img/Formula_05_231.png)-步过渡可能会提供更准确的优势函数估计：
- en: '![](img/Formula_07_121.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_121.jpg)'
- en: Of course, when ![](img/Formula_07_122.png), we are simply back to using sampled
    reward-to-go, which we had abandoned to reduce the variance in the advantage estimation.
    On the other hand, ![](img/Formula_07_123.png) is likely to introduce too much
    bias towards the existing ![](img/Formula_07_124.png) estimates. Therefore, the
    hyper-parameter ![](img/Formula_07_125.png) is a way to control the bias-variance
    trade-off while estimating the advantage.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，当 ![](img/Formula_07_122.png) 时，我们实际上回到了使用采样奖励的方法，这个方法我们曾经放弃过，以减少优势估计中的方差。另一方面，
    ![](img/Formula_07_123.png) 可能会对现有的 ![](img/Formula_07_124.png) 估计引入过多的偏差。因此，超参数
    ![](img/Formula_07_125.png) 是控制偏差-方差权衡的一种方式，同时估计优势。
- en: 'A natural question is then whether we have to use a "single" ![](img/Formula_05_193.png)
    in the advantage estimation. For example, we can calculate advantage estimates
    using ![](img/Formula_07_127.png), ![](img/Formula_07_128.png), and ![](img/Formula_07_129.png),
    and take their average. What about taking a weighted average (convex combination)
    of all possible ![](img/Formula_07_130.png), up to infinity? That is exactly what
    the **generalized advantage estimator (GAE)** does. More specifically, it weighs
    the ![](img/Formula_07_131.png) terms in an exponentially decaying fashion:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 自然的问题是，是否必须在优势估计中使用“单一的” ![](img/Formula_05_193.png)。例如，我们可以使用 ![](img/Formula_07_127.png)、
    ![](img/Formula_07_128.png) 和 ![](img/Formula_07_129.png) 计算优势估计，并取其平均值。那么，取所有可能的
    ![](img/Formula_07_130.png) 的加权平均（凸组合）怎么样呢？这正是**广义优势估计器（GAE）**所做的。更具体地说，它以指数衰减的方式加权
    ![](img/Formula_07_131.png) 项：
- en: '![](img/Formula_07_132.jpg)![](img/Formula_07_133.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_132.jpg)![](img/Formula_07_133.jpg)'
- en: where ![](img/Formula_07_134.png) and ![](img/Formula_07_135.png) is a hyperparameter.
    Therefore, GAE gives us another knob to control the bias-variance trade-off. Specifically,
    ![](img/Formula_07_136.png) results in ![](img/Formula_07_137.png), which has
    high bias; and ![](img/Formula_07_138.png) results in ![](img/Formula_07_139.png),
    which is equivalent to using the sampled reward-to-to minus the baseline, which
    has high bias. Any value of ![](img/Formula_07_140.png) is a compromise between
    the two.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_07_134.png) 和 ![](img/Formula_07_135.png) 是超参数。因此，GAE为我们提供了另一个控制偏差-方差权衡的调节器。具体来说，
    ![](img/Formula_07_136.png) 会导致 ![](img/Formula_07_137.png)，这具有较高的偏差；而 ![](img/Formula_07_138.png)
    导致 ![](img/Formula_07_139.png)，这相当于使用采样奖励减去基线，这具有较高的偏差。任何值的 ![](img/Formula_07_140.png)
    都是两者之间的折衷。
- en: Let's close this section by noting that you can turn on or off the GAE in RLlib's
    actor-critic implementations using the config flag `"use_gae"` along with `"lambda"`.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过注意到，你可以通过配置标志`"use_gae"`和`"lambda"`来开启或关闭RLlib中的GAE（广义优势估计）功能，这适用于其演员-评论家实现。
- en: This concludes our discussion on actor-critic functions. Next, we'll look into
    a recent approach called **trust-region methods**, which have resulted in significant
    improvements over A2C and A3C.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对演员-评论家函数的讨论。接下来，我们将研究一种名为**信赖域方法**的最新方法，它已在A2C和A3C之上带来了显著的改进。
- en: Trust-region methods
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信赖域方法
- en: One of the important developments in the world of policy-based methods has been
    the evolution of trust-region methods. In particular, TRPO and PPO algorithms
    have led to a significant improvement over algorithms such as A2C and A3C. For
    example, the famous Dota 2 AI agent, which reached expert-level performance in
    competitions, was trained using PPO and GAE. In this section, we'll go into the
    details of those algorithms to help you gain a solid understanding of how they
    work.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于策略的方法中，一个重要的发展是信赖域方法的演变。特别是，TRPO和PPO算法相较于A2C和A3C等算法带来了显著的改进。例如，著名的Dota 2
    AI代理，达到了比赛中的专家级表现，就是使用PPO和GAE进行训练的。在这一部分，我们将深入探讨这些算法的细节，帮助你更好地理解它们是如何工作的。
- en: Info
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: Prof. Sergey Levine, who co-authored the TRPO and PPO papers, goes deep into
    the details of the math behind these methods in his online lecture more than we
    do in this section. That lecture is available at [https://youtu.be/uR1Ubd2hAlE](https://youtu.be/uR1Ubd2hAlE)
    and I highly recommend you watch it to improve your theoretical understanding
    of these algorithms.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Sergey Levine教授，TRPO和PPO论文的共同作者，在他的在线讲座中详细讲解了这些方法背后的数学内容，讲解的深度超过了我们在本节中的内容。该讲座可通过[https://youtu.be/uR1Ubd2hAlE](https://youtu.be/uR1Ubd2hAlE)观看，我强烈推荐你观看它，以提升你对这些算法的理论理解。
- en: Without further ado, let's dive in!
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 不再赘述，让我们直接深入探讨！
- en: Policy gradient as policy iteration
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 策略梯度作为策略迭代
- en: 'In the earlier chapters, we described how most of the RL algorithms can be
    thought of as a form of policy iteration, alternating between policy evaluation
    and improvement. You can think of policy gradient methods in the same context:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们描述了大多数 RL 算法可以看作是一种策略迭代的形式，在策略评估和改进之间交替进行。你可以在相同的背景下理解策略梯度方法：
- en: 'Sample collection and advantage estimation: policy evaluation'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本收集和优势估计：策略评估
- en: 'Gradient step: policy improvement'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度步长：策略改进
- en: Now we will use a policy iteration point of view to set the stage for the upcoming
    algorithms. First, let's look at how we can quantify the improvement in the RL
    objective.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用策略迭代的观点来为即将到来的算法做铺垫。首先，让我们看看如何量化 RL 目标中的改进。
- en: Quantifying the improvement
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 量化改进
- en: 'In general, a policy improvement step aims to improve the policy on hand as
    much as possible. More formally, the objective is the following:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，策略改进步骤的目标是尽可能改进现有策略。更正式地说，目标如下：
- en: '![](img/Formula_07_141.jpg)![](img/Formula_07_142.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_141.jpg)![](img/Formula_07_142.jpg)'
- en: 'where ![](img/Formula_06_036.png) is the existing policy. Using the definition
    of ![](img/Formula_07_144.png) and some algebra, we can show the following:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_06_036.png) 是现有的策略。利用 ![](img/Formula_07_144.png) 的定义和一些代数运算，我们可以推导出以下结果：
- en: '![](img/Formula_07_145.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_145.jpg)'
- en: 'Let''s unpack what this equation tells us:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解读一下这个公式告诉我们的信息：
- en: The improvement obtained by a new policy ![](img/Formula_07_146.png) over an
    existing policy ![](img/Formula_06_154.png) can be quantified using the advantage
    function under the existing policy.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新策略 ![](img/Formula_07_146.png) 相对于现有策略 ![](img/Formula_06_154.png) 的改进可以通过现有策略下的优势函数来量化。
- en: The expectation operation we need for this calculation is under the new policy
    ![](img/Formula_07_148.png).
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在这个计算中需要的期望运算是在新策略下的 ![](img/Formula_07_148.png)。
- en: Remember that it is almost never practical to fully calculate such expectations
    or advantage functions. We always estimate them using the policy on hand, ![](img/Formula_07_149.png)
    in this case, and interacting with the environment. Now, the former is a happy
    point since we know how to estimate advantages using samples – the previous section
    was all about this. We also know that we can collect samples to estimate expectations,
    which is what we have in ![](img/Formula_07_150.png). The problem here, though,
    is that the expectation is with respect to a new policy ![](img/Formula_07_151.png).
    We don't know what ![](img/Formula_07_152.png) is, in fact, that is what we are
    trying to find out. So we cannot collect samples from the environment using ![](img/Formula_07_153.png).
    Everything from here on will be about how to get around this problem so that we
    can iteratively improve the policy.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，完全计算这样的期望或优势函数几乎从来不可行。我们总是使用现有策略进行估计，在这个案例中是 ![](img/Formula_07_149.png)，并与环境进行交互。现在，前者是一个愉快的点，因为我们知道如何利用样本估计优势——上一节讲的正是这个内容。我们也知道，我们可以收集样本来估计期望，这就是我们在
    ![](img/Formula_07_150.png) 中所做的。然而，问题在于，这个期望是相对于一个新策略 ![](img/Formula_07_151.png)
    计算的。我们并不知道 ![](img/Formula_07_152.png) 是什么，事实上，这正是我们试图找出的问题。因此，我们不能使用 ![](img/Formula_07_153.png)
    从环境中收集样本。从现在开始，我们将讨论如何绕过这个问题，以便我们能够逐步改进策略。
- en: Getting rid of ![](img/Formula_07_154.png)
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 去除 ![](img/Formula_07_154.png)
- en: 'Let''s expand this expectation and write it down in terms of the marginal probabilities
    that compose ![](img/Formula_07_155.png):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们展开这个期望，并用组成 ![](img/Formula_07_155.png) 的边际概率表示它：
- en: '![](img/Formula_07_156.jpg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_156.jpg)'
- en: 'which uses the following:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用以下公式：
- en: '![](img/Formula_07_157.jpg)![](img/Formula_07_158.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_157.jpg)![](img/Formula_07_158.jpg)'
- en: 'We can get rid of the ![](img/Formula_07_159.png) in the inner expectation
    using importance sampling:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过重要性采样去除内层期望中的 ![](img/Formula_07_159.png)：
- en: '![](img/Formula_07_160.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_160.jpg)'
- en: Now, getting rid of the ![](img/Formula_07_153.png) in the outer expectation
    is the challenging part. *The key idea is to stay "sufficiently close" to the
    existing policy during the optimization, that is,* ![](img/Formula_07_162.png).
    In that case, it can be shown that ![](img/Formula_07_163.png), and we can replace
    the former with the latter.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，去除外层期望中的 ![](img/Formula_07_153.png) 是最具挑战性的部分。*关键思想是在优化过程中保持与现有策略“足够接近”，也就是说，*
    ![](img/Formula_07_162.png)。在这种情况下，可以证明 ![](img/Formula_07_163.png)，并且我们可以将前者替换为后者。
- en: One of the key questions here is how to measure the "closeness" of the policies
    so that we can make sure the preceding approximation is valid. A popular choice
    for such measurements, due to its nice mathematical properties, is the **Kullback-Leibler**
    (**KL**) divergence.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个关键问题是如何衡量策略的“接近度”，以确保前面的近似是有效的。由于其良好的数学性质，一个常用的度量方法是**库尔巴克-莱布勒**（**KL**）散度。
- en: Info
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'If you are not too familiar with the KL divergence or if you need to refresh
    your mind, a nice explanation of it is available here: [https://youtu.be/2PZxw4FzDU?t=226](https://youtu.be/2PZxw4FzDU?t=226).'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对 KL 散度不太熟悉，或者需要复习一下，可以参考这里的一个很好的解释：[https://youtu.be/2PZxw4FzDU?t=226](https://youtu.be/2PZxw4FzDU?t=226)。
- en: 'Using the approximation due to the closeness of the policies and bounding this
    closeness results in the following optimization function:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 利用策略之间接近的近似，并界定这种接近性，结果得到了以下优化函数：
- en: '![](img/Formula_07_164.jpg)![](img/Formula_07_165.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_164.jpg)![](img/Formula_07_165.jpg)'
- en: where ![](img/Formula_05_272.png) is some bound.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_05_272.png) 是某个界限。
- en: Next, let's see how we can use other approximations to further simplify this
    optimization problem.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用其他近似方法来进一步简化这个优化问题。
- en: Using Taylor series expansion in the optimization
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在优化中使用泰勒级数展开
- en: Now that we have a function ![](img/Formula_07_167.png) and we know that we
    evaluate it in close proximity to another point ![](img/Formula_07_168.png), this
    should ring bells to use Taylor series expansion.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个函数 ![](img/Formula_07_167.png)，并且我们知道它是在接近另一个点 ![](img/Formula_07_168.png)
    处被评估的，这应该让你想到使用泰勒级数展开。
- en: Info
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'If you need to refresh your mind on Taylor series, or deepen your intuition,
    a great resource is this video: [https://youtu.be/3d6DsjIBzJ4](https://youtu.be/3d6DsjIBzJ4).
    I also recommend subscribing to the channel – 3Blue1Brown is one of the best resources
    to visually internalize many math concepts.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要复习泰勒级数，或者加深你的直觉，一个很好的资源是这个视频：[https://youtu.be/3d6DsjIBzJ4](https://youtu.be/3d6DsjIBzJ4)。我还推荐订阅该频道——3Blue1Brown
    是一个非常好的视觉化数学概念资源。
- en: 'The first order expansion at ![](img/Formula_07_169.png) is as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ![](img/Formula_07_169.png) 处的一阶展开如下：
- en: '![](img/Formula_07_170.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_170.png)'
- en: Notice that the first term does not depend on ![](img/Formula_07_171.png), so
    we can get rid of it in the optimization. Also note that the gradient term is
    with respect to ![](img/Formula_07_172.png) rather than ![](img/Formula_07_173.png),
    which should come in handy.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一项与 ![](img/Formula_07_171.png) 无关，因此我们可以在优化中去掉它。还要注意，梯度项是相对于 ![](img/Formula_07_172.png)
    而不是 ![](img/Formula_07_173.png) 的，这应该会很有帮助。
- en: 'Our goal then becomes the following:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标变成了以下内容：
- en: '![](img/Formula_07_174.jpg)![](img/Formula_07_175.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_174.jpg)![](img/Formula_07_175.jpg)'
- en: Finally, let's look into how we can calculate the gradient term.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看如何计算梯度项。
- en: Calculating ![](img/Formula_07_176.png)
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算 ![](img/Formula_07_176.png)
- en: 'First, let''s look at what ![](img/Formula_07_177.png) looks like. Remember
    that we can write the following:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看 ![](img/Formula_07_177.png) 长什么样。记住，我们可以写出以下公式：
- en: '![](img/Formula_07_178.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_178.jpg)'
- en: 'because, by definition, ![](img/Formula_07_179.png). Then we can write:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 因为根据定义，![](img/Formula_07_179.png)。然后我们可以写成：
- en: '![](img/Formula_07_180.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_180.jpg)'
- en: 'Now, remember that we are looking for ![](img/Formula_07_181.png), not ![](img/Formula_07_182.png).
    Replacing all ![](img/Formula_07_183.png) with ![](img/Formula_07_184.png) results
    in the following:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，记住我们在寻找的是 ![](img/Formula_07_181.png)，而不是 ![](img/Formula_07_182.png)。将所有的
    ![](img/Formula_07_183.png) 替换为 ![](img/Formula_07_184.png)，得到如下结果：
- en: '![](img/Formula_07_185.jpg)![](img/Formula_07_186.jpg)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_185.jpg)![](img/Formula_07_186.jpg)'
- en: 'We have arrived at a result that should look familiar! The ![](img/Formula_07_187.png)
    is exactly what goes into the gradient estimate ![](img/Formula_07_188.png) in
    advantage actor-critic. The objective of maximizing the policy improvement between
    policy updates has led us to the same objective with a regular gradient ascent
    approach. Of course, we should not forget the constraint. So, the optimization
    problem we aim to solve has become the following:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个看起来应该很熟悉的结果！![](img/Formula_07_187.png) 正是进入优势行为者-批评方法中梯度估计 ![](img/Formula_07_188.png)
    的内容。最大化策略更新间的政策改进目标使我们得到了一个与常规梯度上升方法相同的目标。当然，我们不能忘记约束条件。所以，我们要解决的优化问题变成了以下内容：
- en: '![](img/Formula_07_189.jpg)![](img/Formula_07_190.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_189.jpg)![](img/Formula_07_190.jpg)'
- en: 'This is a key result! Let''s unpack what we have obtained so far:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个关键结果！让我们梳理一下目前为止得到的内容：
- en: Regular actor-critic with gradient ascent and trust-region methods have the
    same objective of moving in the direction of the gradient.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常规的演员-评论员方法与梯度上升法和信任域方法具有相同的目标，即朝着梯度的方向移动。
- en: The trust-region approach aims to stay close to the existing policy by limiting
    the KL divergence.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信任域方法的目标是通过限制KL散度，保持接近现有策略。
- en: Regular gradient ascent, on the other hand, moves in the direction of the gradient
    by a particular step size, as in ![](img/Formula_07_191.png).
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，常规的梯度上升法则是按照特定的步长朝着梯度的方向移动，就像在![](img/Formula_07_191.png)中那样。
- en: Regular gradient ascent, therefore, aims to keep ![](img/Formula_07_192.png)
    closer to ![](img/Formula_07_193.png), as opposed to keeping ![](img/Formula_07_194.png)
    close to ![](img/Formula_07_195.png).
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，常规梯度上升法的目标是将![](img/Formula_07_192.png)保持得尽可能接近![](img/Formula_07_193.png)，而不是将![](img/Formula_07_194.png)保持得接近![](img/Formula_07_195.png)。
- en: Using a single step size for all dimensions in ![](img/Formula_07_196.png),
    as regular gradient ascent does, may result in very slow convergence, or not converging
    at all, since some dimensions in the parameter vector could have a much greater
    impact on the policy (change) than others.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在![](img/Formula_07_196.png)中为所有维度使用相同的步长，就像常规梯度上升法那样，可能会导致收敛速度非常慢，或者根本不收敛，因为参数向量中的某些维度可能对策略（变化）有比其他维度更大的影响。
- en: Tip
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示
- en: The key objective in the trust-region methods is to stay sufficiently close
    to the existing policy ![](img/Formula_07_197.png) while updating the policy to
    some ![](img/Formula_07_198.png). This is different than simply aiming to keep
    ![](img/Formula_07_199.png) closer to ![](img/Formula_07_200.png), which is what
    regular gradient ascent does.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信任域方法的关键目标是在更新策略到某个![](img/Formula_07_198.png)时，保持足够接近现有策略![](img/Formula_07_197.png)。这不同于简单地将![](img/Formula_07_199.png)保持得接近![](img/Formula_07_200.png)，这是常规梯度上升法的做法。
- en: So, we know that ![](img/Formula_07_201.png) and ![](img/Formula_07_202.png)
    should be close, but have not discussed how to achieve this. In fact, two different
    algorithms, TRPO and PPO will handle this requirement differently. Next, we turn
    to details of the TRPO algorithm.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们知道![](img/Formula_07_201.png)和![](img/Formula_07_202.png)应该是接近的，但我们尚未讨论如何实现这一点。事实上，TRPO和PPO这两种不同的算法将以不同的方式处理这一要求。接下来，我们将深入讨论TRPO算法的细节。
- en: TRPO – Trust Region Policy Optimization
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TRPO – 信任域策略优化
- en: TRPO is an important algorithm that precedes PPO. Let's understand in this section
    how it handles the optimization problem we arrived at above and what the challenges
    are with the TRPO solution.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO是一个重要的算法，它在PPO之前。让我们在本节中理解它如何处理我们上面提到的优化问题，以及TRPO解决方案的挑战是什么。
- en: Handling the KL divergence
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理KL散度
- en: 'The TRPO algorithm approximates the KL divergence constraint with its second-order
    Taylor expansion:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: TRPO算法通过其二阶泰勒展开近似KL散度约束：
- en: '![](img/Formula_07_203.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_203.jpg)'
- en: 'where ![](img/Formula_07_204.png) is called the Fisher information matrix and
    defined as follows:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![](img/Formula_07_204.png)被称为费舍尔信息矩阵，其定义如下：
- en: '![](img/Formula_07_205.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_205.jpg)'
- en: where the expectation is to be estimated from the samples. Note that if ![](img/Formula_07_206.png)
    is an ![](img/Formula_07_207.png) dimensional vector, ![](img/Formula_07_208.png)
    becomes an ![](img/Formula_07_209.png) matrix.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 其中期望值需要从样本中估计。请注意，如果![](img/Formula_07_206.png)是一个![](img/Formula_07_207.png)维的向量，那么![](img/Formula_07_208.png)就会变成一个![](img/Formula_07_209.png)矩阵。
- en: Info
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'The Fisher information matrix is an important concept that you might want to
    learn more about, and the Wikipedia page is a good start: [https://en.wikipedia.org/wiki/Fisher_information](https://en.wikipedia.org/wiki/Fisher_information).'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 费舍尔信息矩阵是一个重要的概念，你可能会想了解更多，维基百科页面是一个很好的起点：[https://en.wikipedia.org/wiki/Fisher_information](https://en.wikipedia.org/wiki/Fisher_information)。
- en: 'This approximation results in the following gradient update step (we omit the
    derivation):'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这种近似方法导致了以下的梯度更新步骤（我们省略了推导过程）：
- en: '![](img/Formula_07_210.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_210.jpg)'
- en: where ![](img/Formula_07_211.png) and ![](img/Formula_05_254.png) are hyperparameters,
    ![](img/Formula_07_213.png) and ![](img/Formula_07_214.png).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![](img/Formula_07_211.png)和![](img/Formula_05_254.png)是超参数，![](img/Formula_07_213.png)和![](img/Formula_07_214.png)。
- en: If this looks scary to you, you are not alone! TRPO is indeed not the easiest
    algorithm to implement. Let's next look into what kind of challenges TRPO involves.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这让你感到害怕，你并不孤单！TRPO确实不是一个最容易实现的算法。接下来，让我们看看TRPO所涉及的挑战。
- en: Challenges with TRPO
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TRPO的挑战
- en: 'Here are some of the challenges involved in implementing TRPO:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是实现TRPO时遇到的一些挑战：
- en: Since the KL divergence constraint is approximated with its second-order Taylor
    expansion, there could be cases where the constraint is violated.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于KL散度约束是通过其二阶泰勒展开式近似的，因此可能会出现违反约束的情况。
- en: 'This is where the ![](img/Formula_07_215.png) term comes in: It shrinks the
    magnitude of the gradient update until the constraint is satisfied. To this end,
    there is a line search followed once ![](img/Formula_07_216.png) and ![](img/Formula_07_217.png)
    are estimated: Starting with ![](img/Formula_07_218.png), ![](img/Formula_07_219.png)
    increases by one until the magnitude of the update shrinks enough so that the
    constraint is satisfied.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这时，![](img/Formula_07_215.png)项就发挥作用了：它会缩小梯度更新的幅度，直到满足约束条件。为此，在估算![](img/Formula_07_216.png)和![](img/Formula_07_217.png)之后，会进行一次线搜索：从![](img/Formula_07_218.png)开始，![](img/Formula_07_219.png)每次增加1，直到更新的幅度足够小，以使约束条件得到满足。
- en: Remember that ![](img/Formula_07_220.png) is an ![](img/Formula_07_221.png)
    matrix, which could be huge depending on the size of the policy network and therefore
    expensive to store.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请记住，![](img/Formula_07_220.png)是一个![](img/Formula_07_221.png)矩阵，这个矩阵的大小取决于策略网络的大小，因此可能会非常庞大，存储起来也非常昂贵。
- en: Since ![](img/Formula_07_222.png) is estimated through samples, given its size,
    there could be a lot of inaccuracies introduced during the estimation.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于![](img/Formula_07_222.png)是通过样本估算的，鉴于其大小，估算过程中可能会引入很多不准确性。
- en: Calculating and storing ![](img/Formula_07_223.png) is an even more painful
    step.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算和存储![](img/Formula_07_223.png)是一个更加痛苦的步骤。
- en: To avoid the complexity of dealing with ![](img/Formula_07_224.png) and ![](img/Formula_07_225.png),
    the authors use the conjugate gradient algorithm, which allows you to take gradient
    steps without building the whole matrix and taking the inverse.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免处理![](img/Formula_07_224.png)和![](img/Formula_07_225.png)的复杂性，作者使用了共轭梯度算法，这允许你在不构建整个矩阵和求逆的情况下进行梯度更新。
- en: As you can see, implementing TRPO could be complex and we have omitted the details
    of its implementation. That is why a simpler algorithm that works along the same
    lines, PPO, is more popular and more widely used, which we'll cover next.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，实施TRPO可能会很复杂，我们省略了其实现的细节。这就是为什么沿着相同思路工作的更简单算法PPO更加流行和广泛使用的原因，接下来我们会介绍它。
- en: PPO – Proximal Policy Optimization
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PPO – 近端策略优化
- en: 'PPO is again motivated by maximizing the policy improvement objective:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: PPO再次通过最大化策略改进目标来激励：
- en: '![](img/Formula_07_226.jpg)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_226.jpg)'
- en: 'while keeping ![](img/Formula_07_227.png) close to ![](img/Formula_07_228.png).
    There are two variants of PPO: PPO-Penalty and PPO-Clip. The latter is simpler,
    which we''ll focus on here.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 同时保持![](img/Formula_07_227.png)接近![](img/Formula_07_228.png)。PPO有两个变种：PPO-Penalty和PPO-Clip。后者更简单，我们将在这里重点介绍它。
- en: PPO-clip surrogate objective
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PPO-clip代理目标
- en: 'A simpler way of achieving the closeness between the old and new policy compared
    to TRPO is to clip the objective so that deviating from the existing policy would
    not bring in additional benefits. More formally, PPO-clip maximizes the surrogate
    objective here:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 相比TRPO，通过剪辑目标函数来实现旧策略和新策略之间的接近度是一种更简单的方法，这样偏离现有策略就不会带来额外的好处。更正式地说，PPO-clip在这里最大化代理目标：
- en: '![](img/Formula_07_229.jpg)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_229.jpg)'
- en: 'Where ![](img/Formula_07_230.png) is defined as follows:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![](img/Formula_07_230.png)定义如下：
- en: '![](img/Formula_07_231.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_231.jpg)'
- en: 'This simply says, if the advantage ![](img/Formula_07_232.png) is positive,
    then the minimization takes the form:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这简单地说，如果优势![](img/Formula_07_232.png)是正的，那么最小化过程就会变为：
- en: '![](img/Formula_07_233.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_233.jpg)'
- en: which therefore clips the maximum value the ratio can take. This means even
    if the tendency is to increase the likelihood of taking action ![](img/Formula_07_234.png)
    in state ![](img/Formula_07_235.png) because it corresponds to a positive advantage,
    we clip how much this likelihood can deviate from the existing policy. As a result,
    further deviation does not contribute to the advantage.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这会限制比率能达到的最大值。这意味着，即使趋势是增加在状态![](img/Formula_07_235.png)中采取行动的可能性![](img/Formula_07_234.png)，因为它对应于一个正的优势，我们也会限制这种可能性偏离现有策略的程度。因此，进一步的偏离不会对优势产生贡献。
- en: 'Conversely, if the advantage is negative, the expression instead becomes:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果优势是负的，表达式将变为：
- en: '![](img/Formula_07_236.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_236.jpg)'
- en: which in similar ways, limits how much the likelihood of taking action ![](img/Formula_07_237.png)
    in state ![](img/Formula_07_238.png) can decrease. This ratio is bounded by ![](img/Formula_07_239.png).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，它限制了在状态 ![](img/Formula_07_238.png) 下采取行动的可能性 ![](img/Formula_07_237.png)
    的减少。这一比率受 ![](img/Formula_07_239.png) 的限制。
- en: Next, let's lay out the full PPO algorithm.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们列出完整的PPO算法。
- en: The PPO algorithm
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PPO算法
- en: 'The PPO algorithm works as follows:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: PPO算法的工作流程如下：
- en: Initialize the actor and critic network(s), ![](img/Formula_07_117.png) and
    ![](img/Formula_07_241.png).
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化演员和评论员网络，![](img/Formula_07_117.png) 和 ![](img/Formula_07_241.png)。
- en: '*while some stopping criterion is not met do:*'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*当某个停止准则未满足时，执行以下操作：*'
- en: Collect a batch of ![](img/Formula_07_242.png) samples ![](img/Formula_07_243.png)
    from the (parallel) environment(s) using ![](img/Formula_07_244.png).
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从（并行）环境中收集一批 ![](img/Formula_07_242.png) 样本 ![](img/Formula_07_243.png)，使用 ![](img/Formula_07_244.png)。
- en: Obtain the state-value targets ![](img/Formula_07_245.png).
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取状态值目标 ![](img/Formula_07_245.png)。
- en: Use gradient descent to update ![](img/Formula_07_246.png) with respect to a
    loss function ![](img/Formula_07_247.png), such as squared loss.
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用梯度下降法更新 ![](img/Formula_07_246.png)，以最小化损失函数 ![](img/Formula_07_247.png)，例如平方损失。
- en: Obtain the advantage value estimates
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取优势值估计
- en: '![](img/Formula_07_248.png).'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![](img/Formula_07_248.png)。'
- en: Take a gradient ascent step towards maximizing the surrogate objective function![](img/Formula_07_249.jpg)
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 朝着最大化代理目标函数的方向执行梯度上升步骤 ![](img/Formula_07_249.jpg)
- en: and update ![](img/Formula_07_250.png). Although we don't provide the explicit
    form of this gradient update, it can be easily achieved through packages such
    as TensorFlow.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并更新 ![](img/Formula_07_250.png)。虽然我们没有提供这种梯度更新的显式形式，但可以通过如TensorFlow等包轻松实现。
- en: Broadcast the new ![](img/Formula_06_096.png) to the rollout workers.
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新的 ![](img/Formula_06_096.png) 广播到执行的工作者。
- en: '*end while*'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*当满足某停止准则时结束*'
- en: Finally, note that the architecture of a PPO implementation would be very similar
    to that of A2C with synchronous sampling and policy updates in the rollout workers.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，PPO实现的架构与A2C非常相似，采用同步采样和策略更新的执行工作者。
- en: PPO using RLlib
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用RLlib的PPO
- en: 'Very similar to how we imported the agent trainer classes for the earlier algorithms,
    the PPO class can be imported as follows:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 非常类似于我们如何为早期算法导入代理训练器类，PPO类可以通过以下方式导入：
- en: '[PRE10]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Again, we will present the training results later in the chapter.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后展示训练结果。
- en: That concludes our discussion on trust-region methods. The last class of algorithms
    we'll present in this chapter is off-policy approaches for policy-based methods.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们关于信任域方法讨论的结束。本章最后一类算法将介绍基于策略的离策略方法。
- en: Off-policy methods
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离策略方法
- en: 'One of the challenges with policy-based methods is that they are on-policy,
    which requires collecting new samples after every policy update. If it is costly
    to collect samples from the environment, then training on-policy methods could
    be really expensive. On the other hand, the value-based methods we covered in
    the previous chapter are off-policy but they only work with discrete action spaces.
    Therefore, there is a need for a class of methods that work with continuous action
    spaces and off-policy. In this section, we''ll cover such algorithms. Let''s start
    with the first one: Deep Deterministic Policy Gradient.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的方法的一个挑战是它们是基于策略的，这要求在每次策略更新后收集新的样本。如果从环境中收集样本的成本很高，那么基于策略的方法的训练可能会非常昂贵。另一方面，我们在上一章中讨论的基于价值的方法是离策略的，但它们仅适用于离散的动作空间。因此，需要一种适用于连续动作空间并且是离策略的方法。在本节中，我们将介绍这类算法。让我们从第一个算法开始：深度确定性策略梯度（DDPG）。
- en: DDPG – Deep Deterministic Policy Gradient
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DDPG – 深度确定性策略梯度
- en: DDPG, in some sense, is an extension of deep Q-learning to continuous action
    spaces. Remember that deep Q-learning methods learn a representation for action
    values,![](img/Formula_07_252.png). The best action is then given by ![](img/Formula_07_253.png)
    in a given state ![](img/Formula_05_010.png). Now, if the action space is continuous,
    learning the action-value representation is not a problem. However, then, it would
    be quite cumbersome to execute the max operation to get the best action over a
    continuous action space. DDPG addresses this issue. Let's see how next.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 从某种意义上讲，DDPG 是深度 Q 学习对连续动作空间的扩展。记住，深度 Q 学习方法学习动作值的表示，![](img/Formula_07_252.png)。然后，在给定状态
    ![](img/Formula_05_010.png) 中，最佳动作由 ![](img/Formula_07_253.png) 给出。现在，如果动作空间是连续的，学习动作值的表示就不是问题。然而，接着，执行最大操作来获取连续动作空间中的最佳动作会非常繁琐。DDPG
    解决了这个问题。接下来我们来看它是如何处理的。
- en: How DDPG handles continuous action spaces
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DDPG 如何处理连续动作空间
- en: 'DDPG simply learns another approximation, ![](img/Formula_07_255.png), that
    estimates the best action given the state. If you wonder why this works, consider
    the following thought process:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 只是学习另一个近似，即 ![](img/Formula_07_255.png)，它估计在给定状态下的最佳动作。如果你想知道为什么这样可行，可以考虑以下的思考过程：
- en: DDPG assumes that the continuous action space is differentiable with respect
    to ![](img/Formula_07_256.png).
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDPG 假设连续动作空间对 ![](img/Formula_07_256.png) 可微。
- en: For a moment, also assume that the action values, ![](img/Formula_07_257.png),
    are known or have been learned.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 暂时假设动作值 ![](img/Formula_07_257.png) 是已知的或已经学习过的。
- en: Then the problem simply becomes learning a function approximation, ![](img/Formula_07_258.png),
    whose input is ![](img/Formula_05_048.png), output is ![](img/Formula_07_260.png),
    and the parameters are ![](img/Formula_07_261.png). The "reward" for the optimization
    procedure is simply provided by ![](img/Formula_07_262.png).
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，问题就简单变成了学习一个函数近似，即 ![](img/Formula_07_258.png)，其输入为 ![](img/Formula_05_048.png)，输出为
    ![](img/Formula_07_260.png)，参数为 ![](img/Formula_07_261.png)。优化过程的“奖励”则由 ![](img/Formula_07_262.png)
    提供。
- en: We can therefore use a gradient ascent method to optimize ![](img/Formula_07_246.png).
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，我们可以使用梯度上升方法来优化 ![](img/Formula_07_246.png)。
- en: Over time, the learned action values will hopefully converge and be less of
    a moving target for the policy function training.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着时间的推移，学习到的动作值希望能够收敛，并且成为策略函数训练中的不变目标。
- en: Info
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息
- en: Because DDPG assumes the differentiability of the policy function with respect
    to action, it can only be used with continuous action spaces.
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于 DDPG 假设策略函数相对于动作是可微的，因此它只能用于连续的动作空间。
- en: Next, let's look into a few more details about the DDPG algorithm.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们深入了解一些关于 DDPG 算法的更多细节。
- en: The DDPG algorithm
  id: totrans-392
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DDPG 算法
- en: 'Since DDPG is an extension of deep Q-learning, plus learning a policy function,
    we don''t need to write the full algorithm here. In addition, many approaches
    we discussed in the previous chapter, such as prioritized experience replay and
    multi-step learning, can be used to form a DDPG variate. The original DDPG algorithm,
    on the other hand, is closer to the DQN algorithm and uses the following:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 DDPG 是深度 Q 学习的扩展，并且学习了一个策略函数，因此我们不需要在这里写出完整的算法。此外，我们在上一章讨论的许多方法，如优先经验回放和多步学习，可以用来构成
    DDPG 的变种。而原始的 DDPG 算法则更接近 DQN 算法，并使用以下内容：
- en: A **replay buffer** to store experience tuples, from which the sampling is done
    uniformly at random.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**回放缓冲区**用于存储经验元组，并从中均匀地随机进行采样。
- en: A **target** **network**, which is updated using Polyak averaging as in ![](img/Formula_07_264.png),
    rather than syncing it with the behavior network every ![](img/Formula_07_265.png)
    steps.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**目标** **网络**，它通过使用 Polyak 平均方法更新，如 ![](img/Formula_07_264.png)，而不是每隔 ![](img/Formula_07_265.png)
    步骤与行为网络同步。
- en: 'DDPG then replaces the target calculation in the DQN algorithm:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: DDPG 然后用以下公式替换了 DQN 算法中的目标计算：
- en: '![](img/Formula_07_266.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_266.jpg)'
- en: 'with this one:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 替换成这个：
- en: '![](img/Formula_07_267.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_267.jpg)'
- en: 'Another important difference between DQN and DDPG is that DQN uses ![](img/Formula_07_269.png)-greedy
    actions during training. The policy network in DDPG, however, provides a deterministic
    action for a given state, hence the word "deterministic" in the name. To enable
    exploration during training, some noise is added to the action. More formally,
    the action is obtained as follows:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: DQN和DDPG之间的另一个重要区别在于，DQN在训练过程中使用![](img/Formula_07_269.png)-贪心策略。然而，DDPG中的策略网络为给定状态提供一个确定性的动作，因此名字中有“deterministic”一词。为了在训练过程中进行探索，会向动作中添加一些噪声。更正式地说，动作的获取方式如下：
- en: '![](img/Formula_07_270.jpg)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_270.jpg)'
- en: where ![](img/Formula_05_272.png) can be chosen as white noise (although the
    original implementation uses what is called OU noise). In this operation, ![](img/Formula_07_272.png)
    and ![](img/Formula_07_273.png) represent the boundaries of the continuous action
    space.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![](img/Formula_05_272.png)可以选择为白噪声（尽管原始实现使用了所谓的OU噪声）。在此操作中，![](img/Formula_07_272.png)和![](img/Formula_07_273.png)代表连续动作空间的边界。
- en: That's what DDPG is about. Let's look into how it can be parallelized next.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是DDPG的核心内容。接下来，我们将探讨如何对其进行并行化。
- en: Ape-X DDPG
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Ape-X DDPG
- en: Given the similarity between deep Q-learning and DDPG, parallelization for DDPG
    can easily be achieved using the Ape-X framework. In fact, the original Ape-X
    paper presents DDPG next to DQN in their implementation. It improves the regular
    DDPG performance by several orders of magnitude in some benchmarks. The authors
    also show that the wall-clock time performance consistently increased with the
    increased number of rollout workers (actors).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于深度Q学习和DDPG之间的相似性，DDPG的并行化可以通过Ape-X框架轻松实现。事实上，原始的Ape-X论文将DDPG与DQN一起展示在他们的实现中。它在某些基准测试中将常规DDPG的性能提高了几个数量级。作者还表明，随着回滚工人（演员）数量的增加，实际运行时间的性能持续提高。
- en: DDPG and Ape-X DPG using RLlib
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用RLlib的DDPG和Ape-X DPG
- en: 'The trainer class and the configs for DDPG can be imported as follows:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 可以按如下方式导入DDPG的训练器类和配置：
- en: '[PRE11]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Similarly, for Ape-X DDPG, we import the following:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于Ape-X DDPG，我们引入如下内容：
- en: '[PRE12]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: That's it! The rest is pretty much the same as the training flow we described
    at the beginning of the chapter. Now, before going into algorithms that improve
    DDPG, let's discuss where the DDPG algorithm falls short.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！其余部分与我们在章节开始时描述的训练流程几乎相同。现在，在深入讨论改进DDPG的算法之前，我们先来探讨一下DDPG算法的不足之处。
- en: DDPG's shortcomings
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DDPG的缺点
- en: 'Despite its initial popularity, there are several issues that the algorithm
    runs into:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管最初受到欢迎，但该算法仍然存在几个问题：
- en: It can be quite sensitive to hyperparameter selections.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对超参数的选择可能非常敏感。
- en: It runs into the issue of maximization bias while learning action values.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在学习行动价值时会遇到最大化偏差的问题。
- en: Spikes in the action-value estimates (which are potentially erroneous) are exploited
    by the policy network and derail the learning.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行动价值估计中的峰值（可能是错误的）被策略网络利用，进而干扰学习过程。
- en: Next, we'll look into the TD3 algorithm, which introduces a set of improvements
    to address these issues.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将研究TD3算法，它引入了一系列改进来解决这些问题。
- en: TD3 – Twin Delayed Deep Deterministic Policy Gradient
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TD3 – 双延迟深度确定性策略梯度
- en: The deal with the TD3 algorithm is that it addresses the function approximation
    errors in DDPG. As a result, it greatly outperforms DDPG, PPO, TRPO, and SAC in
    terms of the maximum reward in OpenAI's continuous control benchmarks. Let's look
    into what TD3 proposes.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: TD3算法的关键在于，它解决了DDPG中的函数逼近误差。因此，它在OpenAI的连续控制基准测试中，大大超过了DDPG、PPO、TRPO和SAC，达到了最大奖励。让我们看看TD3提出了什么改进。
- en: TD3 improvements over DDPG
  id: totrans-420
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TD3相较于DDPG的改进
- en: 'There are three main improvements in TD3 over DDPG:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: TD3相较于DDPG有三个主要改进：
- en: 'It learns two (twin) Q networks rather than one, which in turn creates two
    target Q networks. The ![](img/Formula_07_274.png) targets are then obtained using
    the following:'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它学习两个（双重）Q网络，而不是一个，从而创建两个目标Q网络。然后，![](img/Formula_07_274.png)目标使用以下方式获得：
- en: '![](img/Formula_07_275.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_275.jpg)'
- en: where ![](img/Formula_07_276.png) is the target action for a given state ![](img/Formula_07_277.png).
    This is a form of double Q-learning to overcome the maximization bias.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![](img/Formula_07_276.png)是给定状态![](img/Formula_07_277.png)的目标动作。这是一种双Q学习的形式，用于克服最大化偏差。
- en: During training, the policy and the target networks are updated more slowly
    compared to the Q network updates, where the recommended cycle is to have a policy
    update for every two Q network updates, hence the word "delayed" in the algorithm
    name.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中，策略和目标网络的更新速度比 Q 网络更新慢，推荐的周期是每进行两次 Q 网络更新后进行一次策略更新，因此算法名称中有“延迟”一词。
- en: 'The target action, ![](img/Formula_07_278.png), is obtained after some noise
    is added to the policy network outcome:'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标动作，![](img/Formula_07_278.png)，是在策略网络的输出中添加一些噪声后得到的：
- en: '![](img/Formula_07_279.jpg)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_279.jpg)'
- en: where ![](img/Formula_05_272.png) is some white noise. Note that this is different
    noise than what is used to explore in the environment. The role of this noise
    is to act as a regularizer and prevent the policy network from exploiting some
    action values that are incorrectly estimated by the Q network as very high and
    non-smooth in its region.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_05_272.png) 是一些白噪声。请注意，这与用于环境探索的噪声不同。此噪声的作用是作为正则化项，防止策略网络利用
    Q 网络错误估算为非常高且在其区域内不平滑的某些动作值。
- en: Info
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: Just like DDPG, TD3 can only be used with continuous action spaces.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 DDPG 一样，TD3 只能用于连续动作空间。
- en: Next, let's see how you can train an RL agent using RLlib's TD3 implementation.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用 RLlib 的 TD3 实现来训练一个 RL 代理。
- en: TD3 using RLlib
  id: totrans-432
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 RLlib 的 TD3
- en: 'The TD3 trainer class can be imported as follows:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 可以按如下方式导入 TD3 训练器类：
- en: '[PRE13]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: On the other hand, if you look into the code in the `td3.py` module of RLlib,
    you will see that it simply modifies the default DDPG configs and uses the DDPG
    trainer class under the hood. This means that TD3 improvements are optionally
    available in the DDPG trainer class, and you can modify them to obtain an Ape-X
    TD3 variant.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你查看 RLlib 中 `td3.py` 模块的代码，你会发现它只是修改了默认的 DDPG 配置，并在后台使用了 DDPG 训练器类。这意味着
    TD3 的改进可以在 DDPG 训练器类中选择性地使用，你可以修改它们来获得 Ape-X TD3 变体。
- en: That's it with TD3\. Next, we'll discuss SAC.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 TD3 的全部内容。接下来，我们将讨论 SAC。
- en: SAC – Soft actor-critic
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SAC – Soft actor-critic（软演员-评论家）
- en: 'SAC is another popular algorithm that brings even further improvements to TD3\.
    It uses entropy as part of the reward to encourage exploration:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: SAC 是另一个流行的算法，它对 TD3 进行了进一步的改进。它使用熵作为奖励的一部分来鼓励探索：
- en: '![](img/Formula_07_281.jpg)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_07_281.jpg)'
- en: where ![](img/Formula_07_282.png) is the entropy term and ![](img/Formula_07_283.png)
    is the corresponding weight.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_07_282.png) 是熵项，![](img/Formula_07_283.png) 是相应的权重。
- en: Tip
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: SAC can be used both with continuous and discrete action spaces.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: SAC 可以同时用于连续和离散动作空间。
- en: 'To import the SAC trainer, use the following code:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 要导入 SAC 训练器，可以使用以下代码：
- en: '[PRE14]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The final algorithm we will discuss is IMPALA.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的最后一个算法是 IMPALA。
- en: IMPALA – Importance Weighted Actor-Learner Architecture
  id: totrans-446
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IMPALA – 基于重要性加权的演员-学习者架构
- en: 'IMPALA is an algorithm that is of the policy-gradient type, as opposed to DDPG,
    TD3, and SAC, which are essentially value-based methods. As a result, IMPALA is
    not completely an off-policy method. Actually, it is similar to A3C but with the
    following key differences:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: IMPALA 是一种基于策略梯度的算法，而 DDPG、TD3 和 SAC 本质上是基于值的方法。因此，IMPALA 并不完全是一个离策略方法。实际上，它类似于
    A3C，但有以下几个关键区别：
- en: Unlike A3C, it sends sampled experiences (asynchronously) to the learner(s)
    rather than parameter gradients. This significantly reduces the communication
    overhead.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 A3C 不同，它发送采样经验（异步）给学习者，而不是参数梯度。这显著减少了通信开销。
- en: When a sample trajectory arrives, chances are it was obtained under a policy
    that is several updates behind the policy in the learner. IMPALA uses truncated
    importance sampling to account for the policy lag while calculating the value
    function targets.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个样本轨迹到达时，它很可能是通过一个比学习者当前策略更新滞后几个步骤的策略获得的。IMPALA 使用截断的重要性采样来考虑策略滞后，同时计算价值函数目标。
- en: IMPALA allows multiple synchronous worker learners to calculate gradients from
    samples.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IMPALA 允许多个同步的工作学习者从样本中计算梯度。
- en: 'The IMPALA trainer class can be imported in to RLlib as follows:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 可以按如下方式将 IMPALA 训练器类导入到 RLlib 中：
- en: '[PRE15]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This concludes our discussion on algorithms. Now the fun part! Let's compare
    their performance in OpenAI's continuous-control Lunar Lander environment!
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们关于算法讨论的总结。接下来是有趣的部分！让我们在 OpenAI 的连续控制 Lunar Lander 环境中比较它们的表现！
- en: A comparison of the policy-based methods in Lunar Lander
  id: totrans-454
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lunar Lander 中基于策略的方法对比
- en: 'Here is a comparison of evaluation reward performance progress for different
    policy-based algorithms over a single training session in the Lunar Lander environment:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是不同基于策略的算法在单次训练过程中在月球着陆器环境中的评估奖励表现进展对比：
- en: '![Figure 7.6 – Lunar Lander training performance of various policy-based algorithms'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6 – 各种基于策略的算法在月球着陆器训练中的表现'
- en: '](img/B14160_07_6.jpg)'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_07_6.jpg)'
- en: Figure 7.6 – Lunar Lander training performance of various policy-based algorithms
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 各种基于策略的算法在月球着陆器训练中的表现
- en: 'To also give a sense of how long each training session took and what the performance
    at the end of the training was, here is a TensorBoard tooltip for the preceding
    plot:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你更清楚每次训练耗时以及训练结束时的表现，以下是前述图表的 TensorBoard 工具提示：
- en: '![Figure 7.7 – Wall-clock time and end-of-training performance comparisons'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.7 – 实际时间和训练结束时的性能对比'
- en: '](img/B14160_07_7.jpg)'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_07_7.jpg)'
- en: Figure 7.7 – Wall-clock time and end-of-training performance comparisons
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 实际时间和训练结束时的性能对比
- en: 'Before going into further discussions, let''s make the following disclaimer:
    The comparisons here should not be taken as a benchmark of different algorithms
    for multiple reasons:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步讨论之前，让我们先做以下免责声明：这里的对比不应被视为不同算法的基准测试，原因有多方面：
- en: We did not perform any hyperparameter tuning,
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有进行任何超参数调整，
- en: The plots come from a single training trial for each algorithm. Training an
    RL agent is a highly stochastic process and a fair comparison should include the
    average of at least 5-10 training trials.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些图表来自每个算法的单次训练实验。训练一个强化学习（RL）智能体是一个高度随机的过程，公平的比较应包括至少 5-10 次训练实验的平均结果。
- en: We use RLlib's implementations of these algorithms, which could be less or more
    efficient than other open source implementations.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用的是 RLlib 中这些算法的实现，这些实现可能比其他开源实现效率更高或更低。
- en: 'After this disclaimer, let''s discuss what we observe in these results:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个免责声明之后，让我们讨论一下这些结果中的观察：
- en: PPO attains the highest reward at the end of the training.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PPO 在训练结束时获得了最高奖励。
- en: The vanilla policy gradient algorithm is the fastest (in terms of wall-clock
    time) to reach a "reasonable" reward.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统的策略梯度算法在达到“合理”奖励时是最快的（从实际时间角度来看）。
- en: TD3 and DDPG are really slow in terms of wall-clock time, although they achieve
    higher rewards than A2C and IMPALA.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD3 和 DDPG 在实际时间方面非常慢，尽管它们的奖励高于 A2C 和 IMPALA。
- en: The TD3 training graph is significantly more unstable compared to other algorithms.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比其他算法，TD3 的训练图表明显更不稳定。
- en: At some point, TD3 achieved higher rewards than PPO with the same number of
    samples.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些时刻，TD3 在相同样本数量下的奖励超过了 PPO。
- en: IMPALA was super-fast to reach (and go beyond) 2M samples.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IMPALA 在达到（并超越）2M 样本时非常迅速。
- en: You can extend this list, but the idea here is that different algorithms could
    have different advantages and disadvantages. Next, let's discuss what criteria
    you should consider in picking an algorithm for your application.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以扩展这个列表，但这里的核心思想是，不同算法可能有不同的优缺点。接下来，我们来讨论你在为应用选择算法时应该考虑哪些标准。
- en: How to pick the right algorithm
  id: totrans-475
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何选择合适的算法
- en: As in all machine learning domains, there is no silver bullet in terms of which
    algorithm to use for different applications. There are many criteria you should
    consider, and in some cases, some of them will be more important than others.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 和所有机器学习领域一样，针对不同应用的算法选择没有“银弹”。你应该考虑多个标准，在某些情况下，某些标准可能比其他标准更为重要。
- en: 'Here are different dimensions of algorithm performance that you should look
    into when picking your algorithm:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 选择算法时，以下是你应该关注的不同算法表现维度：
- en: '**Highest reward**: When you are not bound by compute and time resources and
    your goal is simply to train the best possible agent for your application, the
    highest reward is the criterion you should pay attention to. PPO and SAC are promising
    alternatives here.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最高奖励**：当你不受计算和时间资源限制，且目标仅仅是为你的应用训练出最优秀的智能体时，最高奖励是你应关注的标准。在这种情况下，PPO 和 SAC
    是很有前景的选择。'
- en: '**Sample efficiency**: If your sampling process is costly/time-consuming, then
    sample efficiency (achieving higher rewards using fewer samples is important).
    When this is the case, you should look into off-policy algorithms as they reuse
    past experience for training as on-policy methods are often incredibly wasteful
    in how they consume samples. SAC is a good starting point in this case.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本效率**：如果您的采样过程耗费时间或成本高昂，那么样本效率（使用较少样本实现更高奖励）就变得很重要。在这种情况下，您应该考虑离策略算法，因为它们可以重用过去的经验进行训练，而在策略方法中，样本的使用通常是极其浪费的。在这种情况下，SAC是一个很好的起点。'
- en: '**Wall-clock time efficiency**: If your simulator is fast and/or you have resources
    to do massive parallelization, PPO, IMPALA, and Ape-X SAC are often good choices.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**墙钟效率**：如果您的模拟器速度快和/或您有资源进行大规模并行化，那么PPO、IMPALA和Ape-X SAC通常是一个不错的选择。'
- en: '**Stability**: Your ability to achieve good rewards without running many trials
    with the same algorithm and them improving consistently during training is also
    important. Off-policy algorithms could be hard to stabilize. PPO is often a good
    choice in this respect.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稳定性**：在不运行多次试验的情况下获得良好的奖励，并且在训练过程中稳步提高，也是很重要的。离策略算法可能难以稳定。在这方面，PPO通常是一个不错的选择。'
- en: '**Generalization**: If an algorithm requires extensive tuning for each environment
    you train it for, this could cost you a lot of time and resources. SAC, due to
    its use of entropy in its reward, is known to be less sensitive to hyperparameter
    choices.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泛化能力**：如果一个算法需要为每个训练环境进行广泛调整，这可能会耗费大量时间和资源。由于其在奖励中使用熵，SAC被认为对超参数选择不太敏感。'
- en: '**Simplicity**: Having an algorithm that is easy to implement is important
    to avoid bugs and ensure maintainability. That is the reason TRPO has been abandoned
    in favor of PPO.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单性**：拥有一个易于实现的算法对于避免错误和确保可维护性至关重要。这就是为什么TRPO被PPO所取代而被放弃的原因。'
- en: That is the end of the discussion on the algorithm picking criteria. Lastly,
    let's go into some resources where you can find easy-to-understand implementations
    of these algorithms.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是算法选择标准讨论的结束。最后，让我们进入一些资源，您可以找到这些算法易于理解的实现。
- en: Open source implementations of policy-gradient methods
  id: totrans-485
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 政策梯度方法的开源实现
- en: In this chapter, we have covered many algorithms. It is not quite possible to
    explicitly implement all these algorithms given the space limitations here. We
    instead relied on RLlib implementations to train agents for our use case. RLlib
    is open source, so you can go to [https://github.com/ray-project/ray/tree/ray-0.8.5/rllib](https://github.com/ray-project/ray/tree/ray-0.8.5/rllib)
    and dive into implementations of these algorithms.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了许多算法。由于空间限制，我们无法显式实现所有这些算法。相反，我们依赖RLlib的实现来训练我们的用例代理。RLlib是开源的，因此您可以访问[https://github.com/ray-project/ray/tree/ray-0.8.5/rllib](https://github.com/ray-project/ray/tree/ray-0.8.5/rllib)，并深入研究这些算法的实现。
- en: Having said that, RLlib implementations are built for production systems and
    therefore involve many other implementations regarding error-handling and preprocessing.
    In addition, there is a lot of code reuse, resulting in implementations with multiple
    class inheritances. A much easier set of implementations is provided by OpenAI's
    Spinning Up repo at [https://github.com/openai/spinningup](https://github.com/openai/spinningup).
    I highly recommend you go into that repo and dive into the implementation details
    of the algorithms we discussed in this chapter.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，RLlib的实现是为生产系统而构建的，因此涉及许多其他关于错误处理和预处理的实现。此外，还有很多代码复用，导致了具有多个类继承的实现。OpenAI的Spinning
    Up库提供了一个更简单的实现集合，可访问[https://github.com/openai/spinningup](https://github.com/openai/spinningup)，我强烈推荐你进入该库，并深入研究我们在本章讨论的算法的实现细节。
- en: Info
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: OpenAI's Spinning Up is also a great resource to see an overview of RL topics
    and algorithms, available at [https://spinningup.openai.com](https://spinningup.openai.com).
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的Spinning Up也是一个很好的资源，可以查看强化学习主题和算法的概述，可访问[https://spinningup.openai.com](https://spinningup.openai.com)。
- en: That's it! We have come a long way and covered policy-based methods in depth.
    Congratulations on reaching this important milestone!
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们已经走过了很长的路，并深入讨论了基于策略的方法。祝贺您达到了这个重要的里程碑！
- en: Summary
  id: totrans-491
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered an important class of algorithms called policy-based
    methods. These methods directly optimize a policy network, unlike the value-based
    methods we covered in the previous chapter. As a result, they have a stronger
    theoretical foundation. In addition, they can be used with continuous action spaces.
    With this, we have covered model-free approaches in detail. In the next chapter,
    we go into model-based methods, which aim to learn the dynamics of the environment
    the agent is in.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一类重要的算法，称为基于策略的方法。这些方法直接优化策略网络，与我们在上一章介绍的基于价值的方法不同。因此，它们具有更强的理论基础。此外，它们可以用于连续的动作空间。通过这一部分，我们详细介绍了无模型方法。在下一章中，我们将深入探讨基于模型的方法，旨在学习智能体所处环境的动态。
- en: References
  id: totrans-493
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Kinds of RL Algorithms*: [https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html)'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*强化学习算法的种类*：[https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html)'
- en: '*Policy Gradient Methods for Reinforcement Learning with Function Approximation*,
    Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour: [https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*带函数逼近的强化学习策略梯度方法*，理查德·S·萨顿，戴维·麦卡莱斯特，萨廷德·辛格，伊沙伊·曼索尔：[https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf)'
- en: '*Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement
    Learning*, Ronald J. Williams: [https://link.springer.com/content/pdf/10.1007/BF00992696.pdf](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf)'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连接主义强化学习的简单统计梯度跟踪算法*，罗纳德·J·威廉姆斯：[https://link.springer.com/content/pdf/10.1007/BF00992696.pdf](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf)'
- en: '*Dota 2 with Large Scale Deep Reinforcement Learning*, Christopher Berner,
    Greg Brockman, et. al: [https://arxiv.org/pdf/1912.06680.pdf](https://arxiv.org/pdf/1912.06680.pdf)'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dota 2与大规模深度强化学习*，克里斯托弗·伯纳，格雷格·布洛克曼等：[https://arxiv.org/pdf/1912.06680.pdf](https://arxiv.org/pdf/1912.06680.pdf)'
