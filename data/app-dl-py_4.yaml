- en: Introduction to Neural Networks and Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与深度学习简介
- en: The MNIST dataset does not contain numbers on the edges of images. Hence, neither
    network assigns relevant values to the pixels located in that region. Both networks
    are much better at classifying numbers correctly if we draw them closer to the
    center of the designated area. This shows that neural networks can only be as
    powerful as the data that is used to train them. If the data used for training
    is very different than what we are trying to predict, the network will most likely
    produce disappointing results.In this chapter, we will cover the basics of neural
    networks and how to set up a deep learning programming environment. We will also
    explore the common components of a neural network and its essential operations.
    We will conclude this chapter by exploring a trained neural network created using
    TensorFlow.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集的图像边缘没有包含数字。因此，两个网络都不会给位于该区域的像素分配相关的值。如果我们将数字画得更靠近指定区域的中心，两个网络在分类数字时的准确率都明显提高。这表明，神经网络的强大程度取决于用来训练它们的数据。如果用于训练的数据与我们尝试预测的数据有很大不同，那么网络很可能会产生令人失望的结果。本章将涵盖神经网络的基础知识以及如何设置深度学习编程环境。我们还将探索神经网络的常见组件及其基本操作。最后，我们将通过探索使用TensorFlow创建的训练神经网络来结束本章内容。
- en: This chapter is about understanding what neural networks can do. We will not
    cover mathematical concepts underlying deep learning algorithms, but will instead
    describe the essential pieces that make a deep learning system. We will also look
    at examples where neural networks have been used to solve real-world problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的内容是理解神经网络能够做什么。我们不会涉及深度学习算法背后的数学概念，而是描述构成深度学习系统的基本元素。我们还将探讨神经网络在解决实际问题中的应用实例。
- en: This chapter will give you a practical intuition on how to engineer systems
    that use neural networks to solve problems—including how to determine if a given
    problem can be solved at all with such algorithms. At its core, this chapter challenges
    you to think about your problem as a mathematical representation of ideas. By
    the end of this chapter, you will be able to think about a problem as a collection
    of these representations and then start to recognize how these representations
    may be learned by deep learning algorithms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将为你提供一个关于如何设计使用神经网络解决问题的系统的实际直觉——包括如何判断一个给定的问题是否可以用这些算法解决。其核心是，本章挑战你将问题看作是思想的数学表示。到本章结束时，你将能够将问题看作这些表示的集合，并开始识别这些表示如何通过深度学习算法进行学习。
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将能够：
- en: Cover the basics of neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探讨神经网络的基础知识
- en: Set up a deep learning programming environment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置深度学习编程环境
- en: Explore the common components of a neural network and its essential operations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索神经网络的常见组件及其基本操作
- en: Conclude this chapterby exploring a trained neural network created using TensorFlow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过探索使用TensorFlow创建的训练神经网络，结束本章内容
- en: What are Neural Networks?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是神经网络？
- en: Neural networks—also known as **Artificial Neural Networks**—were fist proposed
    in the 40s by MIT professors Warren McCullough and Walter Pitts.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络——也称为**人工神经网络**——最早由麻省理工学院教授沃伦·麦卡洛克和沃尔特·皮茨于20世纪40年代提出。
- en: 'For more information refer, Explained: Neural networks. MIT News Office,April
    14, 2017\. Available at:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参阅《解释：神经网络》。麻省理工学院新闻办公室，2017年4月14日。可在以下网址获取：
- en: '[http://news.mit.edu/2017/explained-neural-networksdeep-learning-0414](http://news.mit.edu/2017/explained-neural-networks-deep-learning-0414).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://news.mit.edu/2017/explained-neural-networksdeep-learning-0414](http://news.mit.edu/2017/explained-neural-networks-deep-learning-0414)。'
- en: Inspired by advancements in neuroscience, they proposed to create a computer
    system that reproduced how the brain works (human or otherwise). At its core was
    the idea of a computer system that worked as an interconnected network. That is,
    a system that has many simple components. These components both interpret data
    and influence each other on how to interpret data. This same core idea remains
    today.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 受神经科学进展的启发，他们提议创建一个能够再现大脑工作方式（无论是人类还是其他）的计算机系统。其核心思想是一个作为相互连接网络运作的计算机系统。也就是说，一个由许多简单组件组成的系统，这些组件既解释数据，又相互影响如何解释数据。这个核心思想至今依然存在。
- en: Deep learning is largely considered the contemporary study of neural networks.
    Think of it as a current name given to neural networks. The main difference is
    that the neural networks used in deep learning are typically far greater in size—that
    is, they have many more nodes and layers—than earlier neural networks. Deep learning
    algorithms and applications typically require resources to achieve success, hence
    the use of the word *deep* to emphasize its size and the large number of interconnected
    components.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在很大程度上被认为是现代神经网络的研究。可以把它看作是神经网络的一个现代名称。主要的区别在于，深度学习中使用的神经网络通常要大得多——即，它们有更多的节点和层——相比早期的神经网络。深度学习算法和应用通常需要资源才能取得成功，因此使用“深度”一词来强调它的规模和大量的相互连接的组件。
- en: Successful Applications
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成功的应用
- en: Neural networks have been under research since their inception in the 40s in
    one form or another. It is only recently, however, that deep learning systems
    have been successfully used in large-scale industry applications.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络自从20世纪40年代起便开始研究，虽然形式各异。但直到最近，深度学习系统才在大规模工业应用中取得了成功。
- en: Contemporary proponents of neural networks have demonstrated great success in
    speech recognition, language translation, image classification, and other feilds.
    Its current prominence is backed by a significant increase in available computing
    power and the emergence of **Graphic Processing Units** (**GPUs**) and **Tensor
    Processing Units** (**TPUs**)— which are able to perform many more simultaneous
    mathematical operations than regular CPUs, as well as a much greater availability
    of data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当代神经网络的倡导者在语音识别、语言翻译、图像分类和其他领域取得了巨大的成功。它如今的显著地位得益于计算能力的大幅提升以及**图形处理单元**（**GPU**）和**张量处理单元**（**TPU**）的出现——它们能够进行比普通
    CPU 更多的同时数学运算，并且数据的可用性也大大增加。
- en: Power consumption of different AlphaGo algorithms. AlphaGo is an initiative
    by DeepMind to develop a series of algorithms to beat the game Go. It is considered
    a prime example of the power of deep learning. TPUs are a chipset developed by
    Google for the use in deep learning programs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 不同 AlphaGo 算法的功耗。AlphaGo 是 DeepMind 提出的一个项目，旨在开发一系列击败围棋的算法。它被认为是深度学习强大能力的一个典型例子。TPU
    是由 Google 开发的一种芯片组，专门用于深度学习程序。
- en: The graphic depicts the number of GPUs and TPUs used to train different versions
    of the AlphaGo algorithm. Source: [https://deepmind.com/blog/alphago-zero-learning-scratch/](https://deepmind.com/blog/alphago-zero-learning-scratch/)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图示展示了用于训练不同版本 AlphaGo 算法的 GPU 和 TPU 数量。来源：[https://deepmind.com/blog/alphago-zero-learning-scratch/](https://deepmind.com/blog/alphago-zero-learning-scratch/)
- en: In this book, we will not be using GPUs to fulfil our activities. GPUs are not
    required to work with neural networks. In a number of simple examples—like the
    ones provided in this book—all computations can be performed using a simple laptop's
    CPU. However, when dealing with very large datasets, GPUs can be of great help
    given that the long time to train a neural network would be unpractical.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们不会使用 GPU 来完成我们的活动。使用神经网络并不需要 GPU。在本书提供的一些简单示例中——所有计算都可以通过普通笔记本电脑的 CPU
    完成。然而，当处理非常大的数据集时，GPU 会非常有帮助，因为训练神经网络所需的长时间将变得不切实际。
- en: 'Here are a few examples of fields in which neural networks have had great impact:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是神经网络在一些领域取得巨大影响的例子：
- en: '**Translating text**: In 2017, Google announced that it was releasing a new
    algorithm for its translation service called **Transformer**. The algorithm consisted
    of a recurrent neural network (LSTM) that is trained used bilingual text. Google
    showed that its algorithm had gained notable accuracy when comparing to industry
    standards (BLEU) and was also computationally efficient. At the time of writing,
    Transformer is reportedly used by Google Translate as its main translation algorithm.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**翻译文本**：2017年，Google 宣布推出一种新的翻译算法，称为 **Transformer**。该算法由一个递归神经网络（LSTM）构成，使用双语文本进行训练。Google
    表示，与行业标准（BLEU）相比，该算法在准确性方面表现突出，并且计算效率也很高。在撰写本文时，Transformer 被报告为 Google Translate
    的主要翻译算法。'
- en: 'Google Research Blog. Transformer: A Novel Neural Network Architecture for
    Language Understanding. August 31, 2017\. Available at: [https://research.googleblog.com/2017/08/transformernovel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Google Research Blog。Transformer：一种用于语言理解的新型神经网络架构。2017年8月31日。网址：[https://research.googleblog.com/2017/08/transformernovel-neural-network.html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)。
- en: '**Self-driving vehicles**: Uber, NVIDIA, and Waymo are believed to be using
    deep learning models to control different vehicle functions that control driving.
    Each company is researching a number of possibilities, including training the
    network using humans, simulating vehicles driving in virtual environments, and
    even creating a small city-like environment in which vehicles can be trained based
    on expected and unexpected events.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动驾驶车辆**：Uber、NVIDIA 和 Waymo 被认为正在使用深度学习模型来控制驾驶相关的各项车辆功能。每家公司都在研究多个可能性，包括利用人类训练网络、在虚拟环境中模拟车辆驾驶，甚至创建一个类似小城市的环境，在其中车辆可以根据预期和意外事件进行训练。'
- en: 'Alexis C. Madrigal: Inside Waymo''s Secret World for Training SelfDriving Cars.
    The Atlantic. August 23, 2017\. Available at: [https://](https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Alexis C. Madrigal：深入Waymo的秘密世界，了解自动驾驶汽车的训练。The Atlantic。2017年8月23日。网址：[https://](https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/)
- en: '[www.theatlantic.com/technology/archive/2017/08/ inside-waymos-secret-testing-and-simulationfacilities/537648/](https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulationfacilities/537648/](https://www.theatlantic.com/technology/archive/2017/08/inside-waymos-secret-testing-and-simulation-facilities/537648/)。'
- en: 'NVIDIA: *End-to-End Deep Learning for Self-Driving Cars*. August 17, 2016\.
    Available at: [https://devblogs.nvidia.com/](https://devblogs.nvidia.com/deep-learning-self-driving-cars/)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA：*端到端深度学习用于自动驾驶汽车*。2016年8月17日。网址：[https://devblogs.nvidia.com/](https://devblogs.nvidia.com/deep-learning-self-driving-cars/)
- en: '[parallelforall/deep-learning-self-driving-cars/](https://devblogs.nvidia.com/deep-learning-self-driving-cars/).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '[parallelforall/deep-learning-self-driving-cars/](https://devblogs.nvidia.com/deep-learning-self-driving-cars/)。'
- en: 'Dave Gershgorn: Uber''s new AI team is looking for the shortest route to self-driving
    cars. Quartz. December 5, 2016\. Available at: [https://qz.com/853236/ubers-new-ai-team-is-looking-for-theshortest-route-to-self-driving-cars/](https://qz.com/853236/ubers-new-ai-team-is-looking-for-the-shortest-route-to-self-driving-cars/)[.](https://qz.com/853236/ubers-new-ai-team-is-looking-for-the-shortest-route-to-self-driving-cars/)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Dave Gershgorn：Uber的新AI团队正在寻找通往自动驾驶汽车的最短路径。Quartz。2016年12月5日。网址：[https://qz.com/853236/ubers-new-ai-team-is-looking-for-theshortest-route-to-self-driving-cars/](https://qz.com/853236/ubers-new-ai-team-is-looking-for-the-shortest-route-to-self-driving-cars/)
- en: '**Image recognition**: Facebook and Google use deep learning models to identify
    entities in images and automatically tag these entities as persons from a set
    of contacts. In both cases, the networks are trained with previously tagged images
    as well as with images from the target friend or contact. Both companies report
    that the models are able to suggest a friend or contact with a high level of accuracy
    in most cases.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像识别**：Facebook 和 Google 使用深度学习模型来识别图像中的实体，并自动将这些实体标记为联系人中的人物。在这两种情况下，网络都是通过已标记的图像以及来自目标朋友或联系人的图像来进行训练的。两家公司都报告称，这些模型在大多数情况下能够高精度地推荐朋友或联系人。'
- en: While there are many more examples in other industries, the application of deep
    learning models is still in its infancy. Many more successful applications are
    yet to come, including the ones that you create.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在其他行业中还有更多的例子，但深度学习模型的应用仍处于起步阶段。许多成功的应用尚未到来，包括你自己创造的那些。
- en: Why Do Neural Networks Work So Well?
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么神经网络工作得如此出色？
- en: Why are neural networks so powerful? Neural networks are powerful because they
    can be used to predict any given function with reasonable approximation. If one
    is able to represent a problem as a mathematical function and also has data that
    represents that function correctly, then a deep learning model can, in principle—and
    given enough resources—be able to approximate that function. This is typically
    called the *universality principle of neural networks*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么神经网络如此强大？神经网络之所以强大，是因为它们可以用来预测任何给定的函数，并给出合理的逼近。如果某人能够将一个问题表示为数学函数，并且拥有正确表示该函数的数据，那么深度学习模型原则上——在有足够资源的情况下——能够逼近该函数。这通常被称为*神经网络的普适性原理*。
- en: 'For more information refer, Michael Nielsen: Neural Networks and Deep Learning:
    A visual proof that neural nets can compute any function. Available at: [http://neuralnetworksanddeeplearning.com/chap4.html](http://neuralnetworksanddeeplearning.com/chap4.html).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参考Michael Nielsen的《神经网络与深度学习：神经网络能够计算任何函数的可视化证明》。可访问：[http://neuralnetworksanddeeplearning.com/chap4.html](http://neuralnetworksanddeeplearning.com/chap4.html)。
- en: 'We will not be exploring mathematical proofs of the universality principle
    in this book. However, two characteristics of neural networks should give you
    the right intuition on how to understand that principle: representation learning
    and function approximation.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中不会深入探讨普适性原理的数学证明。然而，神经网络的两个特性应该能够给你正确的直觉，帮助你理解这一原理：表示学习和函数逼近。
- en: For more information refer, Kai Arulkumaran, Marc Peter Deisenroth,Miles Brundage,
    and Anil Anthony Bharath. A Brief Survey of Deep Reinforcement Learning. arXiv.
    September 28, 2017\. Available at: [https://www.arxiv-vanity.com/papers/1708.05866/
    .](https://www.arxiv-vanity.com/papers/1708.05866/)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参考Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage和Anil Anthony
    Bharath的文章《深度强化学习简短调查》。arXiv. 2017年9月28日。可访问：[https://www.arxiv-vanity.com/papers/1708.05866/](https://www.arxiv-vanity.com/papers/1708.05866/)
- en: Representation Learning
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表示学习
- en: The data used to train a neural network contains representations (also known
    as *features*) that explain the problem you are trying to solve. For instance,
    if one is interested in recognizing faces from images, the color values of each
    pixel from a set of images that contain faces will be used as a starting point.
    The model will then continuously learn higher-level representations by combining
    pixels together as it goes through its training process.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练神经网络的数据包含表示（也称为*特征*），这些表示解释了你试图解决的问题。例如，如果某人想从图像中识别人脸，那么包含人脸的图像中每个像素的颜色值将作为起点。然后，模型将在训练过程中不断地通过组合像素来学习更高层次的表示。
- en: '![](img/c5bc5ed5-3b48-435d-88ab-dbec13f2bed6.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5bc5ed5-3b48-435d-88ab-dbec13f2bed6.png)'
- en: 'Figure 1: Series of higher-level representations that begin on input data.
    Derivate image based on original image from: Yann LeCun, Yoshua Bengio & Geoffrey
    Hinton. "Deep Learning". Nature 521, 436–444 (28 May 2015) doi:10.1038/ nature14539'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：从输入数据开始的一系列更高级的表示。图像衍生自原始图像，来源于：Yann LeCun, Yoshua Bengio & Geoffrey Hinton.
    "Deep Learning". Nature 521, 436–444 (2015年5月28日) doi:10.1038/nature14539
- en: In formal words, neural networks are computation graphs in which each step computes
    higher abstraction representations from input data.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 用正式的语言来说，神经网络是计算图，其中每一步从输入数据中计算出更高层次的抽象表示。
- en: 'Each one of these steps represents a progression into a different abstraction
    layer. Data progresses through these layers, building continuously higher-level
    representations. The process finishes with the highest representation possible:
    the one the model is trying to predict.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步都代表着进入不同抽象层次的进展。数据通过这些层次，逐步构建出更高层次的表示。该过程最终完成时会得到最高层次的表示：即模型试图预测的那个表示。
- en: Function Approximation
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 函数逼近
- en: When neural networks learn new representations of data, they do so by combining
    weights and biases with neurons from different layers. They adjust the weights
    of these connections every time a training cycle takes place using a mathematical
    technique called back propagation. The weights and biases improve at each round,
    up to the point that an optimum is achieved.This means that a neural network can
    measure how wrong it is on every training cycle, adjust the weights and biases
    of each neuron, and try again. If it determines that a certain modification produces
    better results than the previous round, it will invest in that modification until
    an optimal solution is achieved.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络学习数据的新表示时，它们通过将权重和偏差与不同层次的神经元结合来实现这一过程。每次训练周期进行时，它们都会使用一种叫做反向传播的数学技术调整这些连接的权重。每一轮的权重和偏差都会得到改进，直到达到最佳状态。这意味着神经网络可以在每个训练周期中衡量它的错误，调整每个神经元的权重和偏差，并重新尝试。如果它确定某个修改比上一次的结果更好，它会继续强化这个修改，直到达到最佳解决方案。
- en: 'In a nutshell, that procedure is the reason why neural networks can approximate
    functions. However, there are many reasons why a neural network may not be able
    to predict a function with perfection, chief among them being that:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这个过程就是神经网络能够近似函数的原因。然而，有许多原因可能导致神经网络无法完美地预测一个函数，其中最主要的原因是：
- en: Many functions contain stochastic properties (that is, random properties)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多函数包含随机特性（即随机性）
- en: There may be overfitting to peculiarities from the training data
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会过拟合训练数据中的特殊性
- en: There may be a lack of training data
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能缺乏训练数据
- en: In many practical applications, simple neural networks are able to approximate
    a function with reasonable precision. These sorts of applications will be our
    focus.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际应用中，简单的神经网络能够以合理的精度近似一个函数。这类应用将是我们的重点。
- en: Limitations of Deep Learning
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的局限性
- en: Deep learning techniques are best suited to problems that can be defiled with
    formal mathematical rules (that is, as data representations). If a problem is
    hard to define this way, then it is likely that deep learning will not provide
    a useful solution. Moreover, if the data available for a given problem is either
    biased or only contains partial representations of the underlying functions that
    generate that problem, then deep learning techniques will only be able to reproduce
    the problem and not learn to solve it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术最适用于那些可以用正式数学规则定义的问题（即作为数据表示）。如果一个问题很难以这种方式定义，那么深度学习很可能无法提供有用的解决方案。此外，如果用于某个问题的数据有偏差，或者只包含生成该问题的潜在函数的部分表示，那么深度学习技术只能复制该问题，而无法学会如何解决它。
- en: 'Remember that deep learning algorithms are learning different representations
    of data to approximate a given function. If data does not represent a function
    appropriately, it is likely that a function will be incorrectly represented by
    a neural network. Consider the following analogy: you are trying to predict the
    national prices of gasoline (that is, fuel) and create a deep learning model.
    You use your credit card statement with your daily expenses on gasoline as an
    input data for that model. The model may eventually learn the patterns of your
    gasoline consumption, but it will likely misrepresent price fluctuations of gasoline
    caused by other factors only represented weekly in your data such as government
    policies, market competition, international politics, and so on. The model will
    ultimately yield incorrect results when used in production.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，深度学习算法通过学习数据的不同表示来近似给定的函数。如果数据没有恰当地代表一个函数，那么神经网络很可能会错误地表示该函数。考虑以下类比：你正在尝试预测全国的汽油价格（即燃料价格），并创建一个深度学习模型。你使用信用卡账单中关于日常汽油消费的支出数据作为该模型的输入数据。模型可能最终学会你汽油消费的模式，但它很可能无法准确地表示由其他因素引起的汽油价格波动，这些因素在你的数据中每周才出现一次，比如政府政策、市场竞争、国际政治等。最终，模型在生产环境中使用时会得出错误的结果。
- en: To avoid this problem, make sure that the data used to train a model represents
    the problem the model is trying to address as accurately as possible.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这个问题，确保用于训练模型的数据尽可能准确地代表模型试图解决的问题。
- en: For an in-depth discussion of this topic, refer to François Chollet's upcoming
    book Deep Learning with Python. François is the creator of Keras, a Python library
    used in this book. The chapter, The limitations of deep learning, is particularly
    important for understanding this topic. The working version of that book is available
    at: [https://blog.keras.io/the-limitations-of-deeplearning.html](https://blog.keras.io/the-limitations-of-deep-learning.html).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入讨论这个话题，请参考François Chollet即将出版的书籍《深度学习与Python》。François是Keras的创始人，Keras是本书中使用的Python库。章节《深度学习的局限性》对理解这一话题尤其重要。该书的工作版本可通过以下链接访问：[https://blog.keras.io/the-limitations-of-deep-learning.html](https://blog.keras.io/the-limitations-of-deep-learning.html)。
- en: Inherent Bias and Ethical Considerations
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 固有偏见和伦理考虑
- en: Researchers have suggested that the use of the deep learning model without considering
    the inherent bias in the training data can lead not only to poor performing solutions,
    but also to ethical complications.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员建议，如果在没有考虑训练数据中固有偏见的情况下使用深度学习模型，不仅可能导致效果不佳的解决方案，还可能引发伦理上的复杂问题。
- en: For instance, in late 2016, researchers from the Shanghai Jiao Tong University
    in China created a neural network which correctly classified criminals using only
    pictures from their faces. The researchers used 1,856 images of Chinese men in
    which half had been convicted.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在2016年末，来自中国上海交通大学的研究人员创建了一个神经网络，通过仅仅使用面部照片就能够正确分类犯罪分子。研究人员使用了1,856张中国男性的图片，其中一半是已被定罪的。
- en: 'Their model identifiled inmates with 89.5 percent accuracy. ([https:// blog.keras.io/the-limitations-of-deep-learning.html](https://blog.keras.io/the-limitations-of-deep-learning.html)).
    MIT Technology Review. Neural Network Learns to Identify Criminals by Their Faces.
    November 22, 2016\. Available at: [https://www.technologyreview.com/s/602955/neuralnetwork-learns-to-identify-criminals-by-their-faces/](https://www.technologyreview.com/s/602955/neural-network-learns-to-identify-criminals-by-their-faces/).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的模型以89.5%的准确率识别出了囚犯。([https:// blog.keras.io/the-limitations-of-deep-learning.html](https://blog.keras.io/the-limitations-of-deep-learning.html))。《麻省理工科技评论》。神经网络通过面部识别来识别犯罪分子。2016年11月22日。可通过以下链接访问：[https://www.technologyreview.com/s/602955/neural-network-learns-to-identify-criminals-by-their-faces/](https://www.technologyreview.com/s/602955/neural-network-learns-to-identify-criminals-by-their-faces/)。
- en: 'The paper resulted in great furor within the scientific community and popular
    media. One key issue with the proposed solution is that it fails to properly recognize
    the bias inherent in the input data. Namely, the data used in this study came
    from two different sources: one for criminals and one for non-criminals. Some
    researchers suggest that their algorithm identifies patterns associated with the
    different data sources used in the study instead of identifying relevant patterns
    from people''s faces. While there are technical considerations one can make about
    the reliability of the model, the key criticism is on ethical grounds: one ought
    to clearly recognize the inherent bias in input data used by deep learning algorithms
    and consider how its application will have an impact on people''s lives.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文在科学界和大众媒体中引发了极大的争议。该方案的一个关键问题在于未能正确识别输入数据中固有的偏见。也就是说，本研究中使用的数据来自两个不同的来源：一个是犯罪分子，另一个是非犯罪分子。一些研究人员建议，他们的算法识别的是与研究中使用的不同数据来源相关的模式，而不是从人脸中识别出相关的模式。尽管可以从技术角度讨论模型的可靠性，但关键的批评还是从伦理角度出发：我们应该清楚地识别深度学习算法所使用的输入数据中的固有偏见，并考虑其应用将如何影响人们的生活。
- en: 'Timothy Revell. Concerns as face recognition tech used to ''identify'' criminals.
    New Scientist. December 1, 2016\. Available at: [https://www.newscientist.com/article/2114900-concernsas-face-recognition-tech-used-to-identify-criminals/](https://www.technologyreview.com/s/602955/neural-network-learns-to-identify-criminals-by-their-faces/).
    For understanding more about the topic of ethics in learning algorithms (including
    deep learning), refer to the work done by the AI Now Institute ([https://ainowinstitute.org/](https://ainowinstitute.org/)),
    an organization created for the understanding of the social implications of intelligent
    systems.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Timothy Revell。使用面部识别技术“识别”犯罪分子的担忧。《新科学家》2016年12月1日。可通过以下链接访问：[https://www.newscientist.com/article/2114900-concernsas-face-recognition-tech-used-to-identify-criminals/](https://www.technologyreview.com/s/602955/neural-network-learns-to-identify-criminals-by-their-faces/)。有关学习算法（包括深度学习）伦理问题的更多了解，请参考AI
    Now研究所的工作([https://ainowinstitute.org/](https://ainowinstitute.org/))，该机构旨在理解智能系统的社会影响。
- en: Common Components and Operations of Neural Networks
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的常见组成部分和操作
- en: 'Neural networks have two key components: layers and nodes. Nodes are responsible
    for specific operations, and layers are groups of nodes used to differentiate
    different stages of the system.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络有两个关键组件：层和节点。节点负责特定的操作，而层是由多个节点组成的，用于区分系统的不同阶段。
- en: 'Typically, neural networks have the following three categories of layers:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，神经网络有以下三类层：
- en: '**Input**: Where the input data is received and first interpreted'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入**：接收输入数据并进行初步解释的地方'
- en: '**Hidden**: Where computations take place, modifying data as it goes through'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏**：计算发生的地方，数据在这里被修改并传递'
- en: '**Output**: Where an output is assembled and evaluated'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出**：输出被组装和评估的地方'
- en: '![](img/45a615a6-4d0b-4b53-aeec-f523b88145a9.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45a615a6-4d0b-4b53-aeec-f523b88145a9.png)'
- en: 'Figure 2: Illustration of the most common layers in a neural network. By Glosser.ca
    - Own work, Derivative of File: Artificial neural network.svg, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=24913461'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：神经网络中最常见层的示意图。由Glosser.ca - 自主创作，衍生自文件：人工神经网络.svg，CC BY-SA 3.0，https://commons.wikimedia.org/w/index.php?curid=24913461
- en: Hidden layers are the most important layers in neural networks. They are referred
    to as *hidden* because the representations generated in them are not available
    in the data, but are learned from it. It is within these layers where the main
    computations take place in neural networks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层是神经网络中最重要的层。它们被称为*隐藏*层，因为在这些层中生成的表示在数据中不可用，而是从数据中学习到的。正是在这些层中，神经网络的主要计算过程发生。
- en: 'Nodes are where data is represented in the network. There are two values associated
    with nodes: biases and weights. Both of these values affect how data is represented
    by the nodes and passed on to other nodes. When a network learns, it effectively
    adjusts these values to satisfy an optimization function.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 节点是数据在网络中表示的地方。与节点相关联的有两个值：偏差和权重。这两个值会影响数据如何被节点表示并传递给其他节点。当网络学习时，它会有效地调整这些值以满足优化函数。
- en: Most of the work in neural networks happens in the hidden layers. Unfortunately,
    there isn't a clear rule for determining how many layers or nodes a network should
    have. When implementing a neural network, one will probably spend time experimenting
    with different combinations of layers and nodes. It is advised to always start
    with a single layer and also with a number of nodes that reflect the number of
    features the input data has (that is, how many *columns* are available in a dataset).
    One will then continue to add layers and nodes until satisfactory performance
    is achieved—or whenever the network starts over fitting to the training data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的大部分工作发生在隐藏层中。不幸的是，目前并没有明确的规则来确定一个网络应该有多少层或节点。在实现神经网络时，人们通常会花时间尝试不同层和节点的组合。建议总是从一个单层开始，并且节点的数量应该反映输入数据的特征数（即数据集中有多少*列*）。然后，继续添加层和节点，直到达到满意的性能——或者当网络开始过拟合训练数据时。
- en: Contemporary neural network practice is generally restricted to the experimentation
    with the number of nodes and layers (for example, how deep the network is), and
    the kinds of operations performed at each layer. There are many successful instances
    in which neural networks outperformed other algorithms simply by adjusting these
    parameters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当代神经网络实践通常仅限于实验节点和层的数量（例如，网络的深度）以及每层执行的操作类型。许多成功的案例表明，神经网络仅通过调整这些参数就能超越其他算法。
- en: As an intuition, think about data entering a neural network system via the input
    layer, then moving through the network from node to node. The path that data takes
    will depend on how interconnected the nodes are, the weights and the biases of
    each node, the kind of operations that are performed in each layer, and the state
    of data at the end of such operations. Neural networks often require many "runs"
    (or epochs) in order to keep tuning the weights and biases of nodes, meaning that
    data flaws over the different layers of the graph multiple times.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作为直观的理解，想象数据通过输入层进入神经网络系统，然后在网络中通过节点逐一传递。数据所走的路径将取决于节点的互联程度、每个节点的权重和偏差、每层执行的操作类型以及数据在这些操作后的状态。神经网络通常需要多次“运行”（或训练周期），以不断调整节点的权重和偏差，这意味着数据在图的不同层之间会多次传递。
- en: 'This section offered you an overview of neural networks and deep learning.
    Additionally, we discussed a starter''s intuition to understand the following
    key concepts:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本节为您提供了神经网络和深度学习的概述。此外，我们讨论了初学者理解以下关键概念的直觉：
- en: Neural networks can, in principle, approximate most functions, given that it
    has enough resources and data.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在原则上，神经网络可以近似大多数函数，只要有足够的资源和数据。
- en: Layers and nodes are the most important structural components of neural networks.
    One typically spends a lot of time altering those components to find a working
    architecture.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层和节点是神经网络的最重要结构组件。通常，人们花费大量时间来修改这些组件，以找到适用的架构。
- en: Weights and biases are the key properties that a network "learns" during its
    training process.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重和偏差是网络在训练过程中“学习”的关键属性。
- en: Those concepts will prove useful in our next section as we explore a real-world
    trained neural network and make modifications to train our own.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念将在我们下一节中证明其有用性，因为我们将探索一个实际训练过的神经网络，并对其进行修改以训练我们自己的网络。
- en: Configuring a Deep Learning Environment
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置深度学习环境
- en: Before we finish this chapter, we want you to interact with a real neural network.
    We will start by covering the main software components used throughout this book
    and make sure that they are properly installed. We will then explore a pretrained
    neural network and explore a few of the components and operations discussed earlier
    in the What are Neural Networks? section.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成本章之前，我们希望您与一个真实的神经网络进行交互。我们将首先介绍本书中使用的主要软件组件，并确保它们已正确安装。然后，我们将探索一个预训练的神经网络，并探讨之前讨论的一些组件和操作，这些操作位于“什么是神经网络？”部分。
- en: Software Components for Deep Learning
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于深度学习的软件组件
- en: 'We''ll use the following software components for deep learning:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在深度学习中使用以下软件组件：
- en: Python 3
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 3
- en: We will be using Python 3 . Python is a general-purpose programming language
    which is very popular with the scientific community—hence its adoption in deep
    learning. Python 2 is not supported in this book but can be used to train neural
    networks instead of Python 3\. Even if you chose to implement your solutions in
    Python 2, consider moving to Python 3 as its modern feature set is far more robust
    than that of its predecessor.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Python 3。Python 是一种通用编程语言，在科学界非常流行，因此在深度学习中被广泛采用。本书不支持 Python 2，但可以用它来训练神经网络，而不是
    Python 3。即使选择在 Python 2 中实现解决方案，考虑迁移到 Python 3，因为其现代功能集比前者更为强大。
- en: TensorFlow
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow
- en: TensorFlow is a library used for performing mathematical operations in the form
    of graphs. TensorFlow was originally developed by Google and today it is an open-source
    project with many contributors. It has been designed with neural networks in mind
    and is among the most popular choices when creating deep learning algorithms.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是一个用于执行图形形式的数学操作的库。TensorFlow 最初由 Google 开发，今天是一个拥有许多贡献者的开源项目。它专为神经网络而设计，是创建深度学习算法时最受欢迎的选择之一。
- en: TensorFlow is also well-known for its production components. It comes with TensorFlow
    Serving [(https://github.com/tensorflow/serving),](https://github.com/tensorflow/serving)
    a high-performance system for serving deep learning models. Also, trained TensorFlow
    models can be consumed in other high-performance programming languages such as
    Java, Go, and C. This means that one can deploy these models in anything from
    a micro-computer (that is, a RaspberryPi) to an Android device.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 也以其生产组件而闻名。它附带 TensorFlow Serving [(https://github.com/tensorflow/serving),](https://github.com/tensorflow/serving)
    这是一个用于提供深度学习模型的高性能系统。此外，经过训练的 TensorFlow 模型可以在其他高性能编程语言（如 Java、Go 和 C）中使用。这意味着可以在从微型计算机（即
    RaspberryPi）到 Android 设备的任何设备上部署这些模型。
- en: Keras
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras
- en: In order to interact efficiently with TensorFlow, we will be using Keras ([https://keras.io/](https://keras.io/)),
    a Python package with a high-level API for developing neural networks. While TensorFlow
    focuses on components that interact with each other in a computational graph,
    Keras focuses specifically on neural networks. Keras uses TensorFlow as its backend
    engine and makes developing such applications much easier.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与 TensorFlow 高效交互，我们将使用 Keras ([https://keras.io/](https://keras.io/))，一个提供高级
    API 以开发神经网络的 Python 包。虽然 TensorFlow 专注于组件之间的计算图交互，但 Keras 则专注于神经网络。Keras 使用 TensorFlow
    作为其后端引擎，使得开发这类应用程序更加容易。
- en: As of November 2017 (TensorFlow version 1.4), Keras is distributed as part of
    TensorFlow. It is available under the `tf.keras` namespace. If you have TensorFlow
    1.4 or higher installed, you already have Keras available in your system.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2017年11月（TensorFlow 版本 1.4），Keras 作为 TensorFlow 的一部分进行分发。它在 `tf.keras` 命名空间下可用。如果你已经安装了
    TensorFlow 1.4 或更高版本，你的系统中已经包含了 Keras。
- en: TensorBoard
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorBoard
- en: TensorBoard is a data visualization suite for exploring TensorFlow models and
    is natively integrated with TensorFlow. TensorBoard works by consuming checkpoint
    and summary files created by TensorFlow as it trains a neural network. Those can
    be explored either in near real-time (with a 30 second delay) or after the network
    has finished training.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 是一个数据可视化工具套件，用于探索 TensorFlow 模型，并与 TensorFlow 原生集成。TensorBoard 通过消耗
    TensorFlow 在训练神经网络时创建的检查点和摘要文件来工作。这些文件可以在接近实时（延迟 30 秒）或者在网络训练完成后进行探索。
- en: TensorBoard makes the process of experimenting and exploring a neural network
    much easier—plus it's quite exciting to follow the training of your network!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard 使得实验和探索神经网络的过程变得更加容易——而且追踪你的网络训练过程非常令人兴奋！
- en: Jupyter Notebooks, Pandas, and NumPy
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jupyter Notebooks，Pandas 和 NumPy
- en: 'When working to create deep learning models with Python, it is common to start
    working interactively, slowly developing a model that eventually turns into more
    structured software. Three Python packages are used frequently during that process:
    Jupyter Notebooks, Pandas, and `NumPy`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Python 创建深度学习模型时，通常会从交互式工作开始，慢慢开发出一个最终变成更结构化软件的模型。在这个过程中，经常使用三个 Python 包：Jupyter
    Notebooks，Pandas 和 `NumPy`：
- en: Jupyter Notebooks create interactive Python sessions that use a web browser
    as its interface
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter Notebooks 创建使用 web 浏览器作为界面的交互式 Python 会话
- en: Pandas is a package for data manipulation and analysis
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas 是一个用于数据处理和分析的包
- en: NumPy is frequently used for shaping data and performing numerical computations
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy 经常用于数据的形状变换和执行数值计算
- en: These packages are used occasionally. They typically do not form part of a production
    system, but are often used when exploring data and starting to build a model.
    We focus on the other tools in much more detail.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这些包偶尔使用。它们通常不构成生产系统的一部分，但在探索数据和开始构建模型时常被使用。我们更详细地关注其他工具。
- en: The book *Learning Pandas* by Michael Heydt (June 2017, Packt Publishing) and
    *Learning Jupyter* by Dan Toomey (November 2016, Packt Publishing) offer a comprehensive
    guide on how to use these technologies. These books are good references for continuing
    to learn more.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Michael Heydt（2017年6月，Packt出版）的《*学习Pandas*》和Dan Toomey（2016年11月，Packt出版）的《*学习Jupyter*》提供了如何使用这些技术的全面指南。这些书籍是继续深入学习的好参考。
- en: '| **Component**  | **Description**  | **Minimum Version**  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| **组件**  | **描述**  | **最低版本**  |'
- en: '| Python  | General-purpose programming language. Popular language   used in 
    the development of deep learning applications.  |  3.6  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Python  | 通用编程语言。流行的用于开发深度学习应用程序的语言。  |  3.6  |'
- en: '| TensorFlow  | Open-source graph computation Python package typically used
    for  developing deep learning systems.  |  1.4  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| TensorFlow  | 开源图计算 Python 包，通常用于开发深度学习系统。  |  1.4  |'
- en: '| Keras  | Python package that provides a high-level interface to TensorFlow.  |
     2.0.8-tf (distributed with TensorFlow)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '| Keras  | 提供高层次接口到 TensorFlow 的 Python 包。  |  2.0.8-tf（与 TensorFlow 一起分发）'
- en: '|'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| TensorBoard  | Browser-based software for visualizing neural network statistics.  |
     0.4.0  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| TensorBoard  | 基于浏览器的软件，用于可视化神经网络统计数据。  |  0.4.0  |'
- en: '| Jupyter Notebook  | Browser-based software for working interactively  with
    Python sessions.  |  5.2.1  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Jupyter Notebook  | 基于浏览器的软件，用于交互式工作与 Python 会话。  |  5.2.1  |'
- en: '| Pandas  | Python package for analyzing and manipulating data.  |  0.21.0  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Pandas  | 用于分析和处理数据的 Python 包。  |  0.21.0  |'
- en: '| NumPy  | Python package for high-performance numerical computations.  |  1.13.3  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| NumPy  | 用于高性能数值计算的 Python 包。  |  1.13.3  |'
- en: 'Table 1: Software components necessary for creating a deep learning environment'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：创建深度学习环境所需的软件组件
- en: Activity:Verifying Software Components
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动：验证软件组件
- en: Before we explore a trained neural network, let's verify that all the software
    components that we need are available. We have included a script that verifies
    these components work. Let's take a moment to run the script and deal with any
    eventual problems we may find.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索一个训练好的神经网络之前，让我们验证所有必需的软件组件是否已准备好。我们提供了一个脚本来验证这些组件的可用性。让我们花点时间运行脚本，并处理可能遇到的任何问题。
- en: We will now be testing if the software components required for this book are
    available in your working environment. First, we suggest the creation of a Python
    virtual environment using Python's native module `venv`. Virtual environments
    are used for managing project dependencies. We advise each project you create
    to have its own virtual environments. Let's create one now.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将测试是否所有本书所需的软件组件都在您的工作环境中可用。首先，我们建议使用 Python 的原生模块 `venv` 创建一个 Python 虚拟环境。虚拟环境用于管理项目依赖项。我们建议您为每个创建的项目配置独立的虚拟环境。现在让我们创建一个。
- en: If you are more comfortable with `conda` environments, feel free to use those
    instead.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您更喜欢使用 `conda` 环境，可以随意使用它们。
- en: 'A Python virtual environment can be created by using the following command:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用以下命令创建一个 Python 虚拟环境：
- en: '[PRE0]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The latter command will append the string (`venv`) to the beginning of your command
    line. Use the following command to deactivate your virtual environment:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 后者命令会将字符串（`venv`）添加到命令行的开头。使用以下命令来停用您的虚拟环境：
- en: '[PRE1]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Make sure to always activate your Python virtual environment when working on
    a project.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在处理项目时始终激活您的 Python 虚拟环境。
- en: 'After activating your virtual environment, make sure that the right components
    are installed by executing pip over the file `requirements.txt`. This will attempt
    to install the models used in this book in that virtual environment. It will do
    nothing if they are already available:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 激活虚拟环境后，通过执行 pip 对 `requirements.txt` 文件进行操作，确保正确的组件已安装。这将尝试在该虚拟环境中安装本书使用的模型。如果它们已存在，则不会执行任何操作：
- en: '![](img/0d4be57f-1d5b-4db6-8117-43b5b8dde66b.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d4be57f-1d5b-4db6-8117-43b5b8dde66b.png)'
- en: 'Figure 3: Image of a terminal running pip to install dependencies from requirements.txt'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：终端运行 pip 安装 requirements.txt 中的依赖项的图片
- en: 'Install dependencies by running the following command:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令来安装依赖项：
- en: '[PRE2]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will install all required dependencies for your system. If they are already
    installed, this command should simply inform you.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为您的系统安装所有必需的依赖项。如果它们已经安装，该命令将简单地通知您。
- en: These dependencies are essential for working with all code activities.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些依赖项对于所有代码活动的正常运行至关重要。
- en: As a final step on this activity, let's execute the script `test_stack.py`.
    That script formally verifies that all the required packages for this book are
    installed and available in your system.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 作为此活动的最后一步，让我们执行脚本 `test_stack.py`。该脚本正式验证此书所需的所有包是否已安装并在您的系统中可用。
- en: 'Students, run the script `Chapter_4/activity_1/test_stack.py` to check if the
    dependencies Python 3, TensorFlow, and Keras are available. Use the following
    command:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 学生们，运行脚本 `Chapter_4/activity_1/test_stack.py` 检查 Python 3、TensorFlow 和 Keras
    是否可用。使用以下命令：
- en: '[PRE3]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The script returns helpful messages stating what is installed and what needs
    to be installed.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本返回有用的消息，说明已安装什么以及需要安装什么。
- en: 'Run the following script command in your terminal:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的终端中运行以下脚本命令：
- en: '[PRE4]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You should see a help message that explains what each command does. If you
    do not see that message – or see an error message instead – please ask for assistance from
    your instructor:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到一条帮助信息，解释每个命令的作用。如果没有看到该消息，或者看到错误消息，请向您的讲师寻求帮助：
- en: '![](img/3898b1a8-36ce-43d2-bb14-5b5b79874129.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3898b1a8-36ce-43d2-bb14-5b5b79874129.png)'
- en: 'Figure 4: Image of a terminal running `python3 test_stack.py`. The script returns
    messages informing that all dependencies are installed correctly.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：终端运行 `python3 test_stack.py` 的图片。脚本返回消息，告知所有依赖项已正确安装。
- en: 'If a similar message to the following appears, there is no need to worry:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出现类似以下消息，无需担心：
- en: 'Runtime Warning: compile time version 3.5 of module ''`tensorflow.python.framework.fast_tensor_util`''does
    not match runtime version 3.6 return f(*args, **kwds)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时警告：模块'`tensorflow.python.framework.fast_tensor_util`'的编译时版本 3.5 与运行时版本 3.6
    不匹配，返回 f(*args, **kwds)
- en: That message appears if you are running Python 3.6 and the distributed
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行的是 Python 3.6 并且分发
- en: TensorFlow wheel was compiled under a different version (in this case, 3.5).
    You can safely ignore that message.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow wheel 是在不同版本（在此情况下为 3.5）下编译的。您可以安全地忽略该消息。
- en: Once we have verified that Python 3, TensorFlow, Keras, TensorBoard, and the
    packages outlined in `requirements.txt` have been installed, we can continue to
    a demo on how to train a neural network and then go on to explore a trained network
    using these same tools.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确认安装了Python 3、TensorFlow、Keras、TensorBoard以及`requirements.txt`中列出的软件包，我们就可以继续进行演示，了解如何训练神经网络，然后使用这些工具来探索已经训练好的网络。
- en: Exploring a Trained Neural Network
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索训练好的神经网络
- en: In this section, we explore a trained neural network. We do that to understand
    how a neural network solves a real-world problem (predict handwritten digits)
    and also to get familiar with the TensorFlow API. When exploring that neural network,
    we will recognize many components introduced in previous sections such as nodes
    and layers, but we will also see many that we don't recognize (such as activation
    functions)—we will explore those in further sections. We will then walk through
    an exercise on how that neural network was trained and then train that same network
    on our own.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索一个已经训练好的神经网络。我们这么做是为了理解神经网络如何解决一个实际问题（预测手写数字），同时熟悉TensorFlow的API。在探索这个神经网络时，我们会看到许多在前面章节中介绍过的组件，如节点和层，但我们也会看到许多不太熟悉的组件（例如激活函数）——我们将在后续章节中进一步探讨这些内容。然后，我们将通过一个练习，讲解该神经网络是如何训练的，并且尝试自己训练这个网络。
- en: The network that we will be exploring has been trained to recognize numerical
    digits (integers) using images of handwritten digits. It uses the MNIST dataset
    ([http:// yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)), a classic
    dataset frequently used for exploring pattern  recognition tasks.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探索的网络已经经过训练，可以识别手写数字（整数）。它使用了MNIST数据集（[http:// yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)），这是一个经典的数据集，常用于探索模式识别任务。
- en: MNIST Dataset
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST数据集
- en: The **Modifiled National Institute of Standards and Technology** (**MNIST**)
    dataset contains a training set of 60,000 images and a test set of 10,000 images.
    Each image contains a single handwritten number. This dataset—which is a derivate
    from one created by the US Government—was originally used to test different approaches
    to the problem of recognizing handwritten text by computer systems. Being able
    to do that was important for the purpose of increasing the performance of postal
    services, taxation systems, and government services. The MNIST dataset is considered
    too naïve for contemporary methods. Different and more recent datasets are used
    in contemporary research (for example, CIFAR). However, the MNIST dataset is still
    very useful for understanding how neural networks work because known models can
    achieve a high level of accuracy with great efficiency.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**修改版国家标准与技术研究院**（**MNIST**）数据集包含一个包含60,000张图像的训练集和一个包含10,000张图像的测试集。每张图像包含一个手写数字。这个数据集最初是由美国政府创建的，用来测试不同的计算机系统识别手写文字的方法。能够做到这一点对于提高邮政服务、税收系统和政府服务的效率具有重要意义。由于MNIST数据集对现代方法来说过于简单，因此现在的研究通常使用不同的、更新的数据集（例如CIFAR）。然而，MNIST数据集仍然非常有助于理解神经网络的工作原理，因为已知的模型可以高效地实现较高的准确率。'
- en: 'The CIFAR dataset is a machine learning dataset that contains images organized
    in different classes. Different than the MNIST dataset, the CIFAR dataset contains
    classes in many different areas such as animals, activities, and objects. The
    CIFAR dataset is available at: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR数据集是一个机器学习数据集，包含按照不同类别组织的图像。与MNIST数据集不同，CIFAR数据集包含多个领域的类别，如动物、活动和物体。CIFAR数据集可以在以下链接找到：[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)。
- en: '![](img/5bd59dd7-5cc3-4825-8b1e-02a29a48e636.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bd59dd7-5cc3-4825-8b1e-02a29a48e636.png)'
- en: 'Figure 5: Excerpt from the training set of the MNIST dataset. Each image is
    a separate 20x20 pixels image with a single handwritten digit. The original dataset
    is available at: http://yann.lecun.com/exdb/mnist/.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：MNIST数据集训练集的摘录。每张图像是一个单独的20x20像素图像，包含一个手写数字。原始数据集可以在以下链接找到：http://yann.lecun.com/exdb/mnist/。
- en: Training a Neural Network with TensorFlow
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow训练神经网络
- en: Now, let's train a neural network to recognize new digits using the MNIST dataset.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练一个神经网络，使用MNIST数据集来识别新的数字。
- en: 'We will be implementing a special-purpose neural network called "Convolutional
    Neural Network" to solve this problem (we will discuss those in more detail in
    further sections). Our network contains three hidden layers: two fully connected
    layers and a convolutional layer. The convolutional layer is defiled by the following
    TensorFlow snippet of Python code:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一种特殊用途的神经网络，称为“卷积神经网络”，来解决这个问题（我们将在后续章节中更详细地讨论这些内容）。我们的网络包含三层隐藏层：两层全连接层和一层卷积层。卷积层由以下TensorFlow的Python代码片段定义：
- en: '[PRE5]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We execute that snippet of code only once during the training of our network.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练网络时只执行这段代码一次。
- en: The variables W and B stand for weights and biases. These are the values used
    by the nodes within the hidden layers to alter the network's interpretation of
    the data as it passes through the network. Do not worry about the other variables
    for now.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 变量W和B代表权重和偏置。这些值是隐藏层中的节点用来改变网络对数据的解释的，数据在网络中传递时会被这些值所修改。暂时不要担心其他变量。
- en: 'The **fully connected layers** are defiled by the following snippet of Python
    code:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**全连接层**由以下Python代码片段定义：'
- en: '[PRE6]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we also have the two TensorFlow variables W and B. Notice how simple
    the initialization of these variables is: W is initialized as a random value from
    a pruned Gaussian distribution (pruned with `size_in and size_out`) with a standard
    deviation of 0.1, and B (the bias term) is initialized as `0.1,` a constant. Both
    these values will continuously change during each run. That snippet is executed
    twice, yielding two fully connected networks— one passing data to the other.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们还有两个TensorFlow变量W和B。请注意这些变量的初始化有多简单：W被初始化为一个来自剪枝高斯分布的随机值（剪枝范围为`size_in
    和 size_out`），标准差为0.1，B（偏置项）被初始化为`0.1`，这是一个常数。这两个值会在每次运行时不断变化。这段代码执行两次，产生两个全连接网络——一个将数据传递给另一个。
- en: Those 11 lines of Python code represent our complete neural network. We will
    go into a lot more detail in *Chapter 5*, *Model Architecture* about each one
    of those components using Keras. For now, focus on understanding that the network
    is altering the values of W and B in each layer on every run and how these snippets
    form different layers. These 11 lines of Python are the culmination of dozens
    of years of neural network research.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这11行Python代码代表了我们的完整神经网络。在*第五章*，*模型架构*中，我们将详细讲解每个组件如何使用Keras实现。目前，请重点理解网络如何在每次运行时改变每一层中W和B的值，以及这些代码片段如何构成不同的层。这11行Python代码是数十年神经网络研究的结晶。
- en: Let's now train that network to evaluate how it performs in the MNIST dataset.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练这个网络，评估它在MNIST数据集上的表现。
- en: Training a Neural Network
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: 'Follow the following steps to set up this exercise:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤设置本次练习：
- en: Open two terminal instances.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开两个终端实例。
- en: In both of them, navigate to the directory `chapter_4/exercise_a`.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在两个终端中，导航到`chapter_4/exercise_a`目录。
- en: In both of them, make sure that your Python 3 virtual environment is active
    and that the requirements outlined in `requirements.txt` are installed.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在两个终端中，确保你的Python 3虚拟环境是激活状态，并且`requirements.txt`中列出的依赖已安装。
- en: 'In one of them, start a TensorBoard server with the following command:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其中之一是通过以下命令启动TensorBoard服务器：
- en: '`$ tensorboard --logdir=mnist_example/`'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`$ tensorboard --logdir=mnist_example/`'
- en: In the other, run the `train_mnist.py` script from within that directory.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个终端中，从该目录中运行`train_mnist.py`脚本。
- en: Open your browser in the TensorBoard URL provided when you start the server.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中打开当你启动服务器时提供的TensorBoard URL。
- en: In the terminal that you ran the script `train_mnist.py`, you will see a progress
    bar with the epochs of the model. When you open the browser page, you will see
    a couple of graphs. Click on the one that reads `Accuracy`, enlarge it and let
    the page refresh (or click on the `refresh` button). You will see the model gaining
    accuracy as epochs go by.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在你运行`train_mnist.py`脚本的终端中，你将看到一个包含模型训练进度的进度条。当你打开浏览器页面时，你会看到几个图表。点击显示`Accuracy`的图表，放大它并让页面刷新（或者点击`refresh`按钮）。你将看到随着训练轮次的增加，模型的准确度逐渐提高。
- en: Use that moment to explain the power of neural networks in reaching a high level
    of accuracy very early in the training process.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这个时刻，解释神经网络在训练过程早期迅速达到高精度的强大能力。
- en: We can see that in about 200 epochs (or steps), the network surpassed 90 percent
    accuracy. That is, the network is getting 90 percent of the digits in the test
    set correctly. The network continues to gain accuracy as it trains up to the 2,000th
    step, reaching a 97 percent accuracy at the end of that period.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在大约 200 个周期（或步骤）后，网络超过了 90% 的准确率。也就是说，网络在测试集上正确预测了 90% 的数字。随着训练的进行，网络继续提高准确率，直到第
    2000 步，最终达到 97% 的准确率。
- en: Now, let's also test how well those networks perform with unseen data. We will
    use an open-source web application created by Shafeen Tejani to explore if a trained
    network correctly predicts handwritten digits that we create.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们还将测试这些网络在未见数据上的表现。我们将使用由 Shafeen Tejani 创建的一个开源 Web 应用，探索训练好的网络是否能正确预测我们创建的手写数字。
- en: Testing Network Performance with Unseen Data
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用未见数据测试网络性能
- en: 'Visit the website [http://mnist-demo.herokuapp.com/](http://mnist-demo.herokuapp.com/)
    in your browser and draw a number between 0 and 9 in the designated white box:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中访问 [http://mnist-demo.herokuapp.com/](http://mnist-demo.herokuapp.com/)
    并在指定的白色框中绘制一个 0 到 9 之间的数字：
- en: '![](img/a788e857-da7e-40ac-9b39-b83d78219375.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a788e857-da7e-40ac-9b39-b83d78219375.png)'
- en: 'Figure 6: Web application in which we can manually draw digits and test the
    accuracy of two trained networks'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：我们可以手动绘制数字并测试两个训练网络准确性的 Web 应用
- en: 'Source: [https://github.com/ShafeenTejani/mnist-demo](https://github.com/ShafeenTejani/mnist-demo)
    .'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://github.com/ShafeenTejani/mnist-demo](https://github.com/ShafeenTejani/mnist-demo)。
- en: 'In the application, you can see the results of two neural networks. The one
    that we have trained is on the left (called CNN). Does it classify all your handwritten
    digits correctly? Try drawing numbers at the edge of the designated area. For
    instance, try drawing the number **1** close to the right edge of that area:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序中，你可以看到两个神经网络的结果。我们训练的那个在左边（叫做 CNN）。它能正确分类你所有的手写数字吗？试着在指定区域的边缘绘制数字。例如，试着在该区域的右边缘附近绘制数字**1**：
- en: '![](img/2bf7e60d-4981-42f6-97df-64bbc1d1fe72.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2bf7e60d-4981-42f6-97df-64bbc1d1fe72.png)'
- en: 'Figure 7: Both networks have a difficult time estimating values drawn on the
    edges of the area'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：两个网络都难以估算绘制在区域边缘的值
- en: In this example, we see the number **1** drawn to the right side of the drawing
    area. The probability of this number being a **1** is **0** in both networks.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们看到数字**1**被绘制在绘图区域的右侧。在两个网络中，这个数字是**1**的概率都是**0**。
- en: The MNIST dataset does not contain numbers on the edges of images. Hence, neither
    network assigns relevant values to the pixels located in that region. Both networks
    are much better at classifying numbers correctly if we draw them closer to the
    center of the designated area. This shows that neural networks can only be as
    powerful as the data that is used to train them. If the data used for training
    is very different than what we are trying to predict, the network will most likely
    produce disappointing results.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集不包含图像边缘的数字。因此，两个网络都没有为该区域内的像素分配相关的值。如果我们将数字绘制得离指定区域的中心更近，这两个网络在分类数字时会表现得更好。这表明神经网络的强大程度仅取决于用来训练它们的数据。如果训练数据与我们试图预测的数据相差甚远，网络很可能会产生令人失望的结果。
- en: 'Activity: Exploring a Trained Neural Network'
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 活动：探索训练过的神经网络
- en: In this section, we will explore the neural network that we have trained during
    our exercise. We will also train a few other networks by altering hyper parameters
    from our original one. Let's start by exploring the network trained in our exercise.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索我们在练习中训练的神经网络。我们还将通过调整超参数来训练一些其他的网络。让我们先来探索在练习中训练的网络。
- en: We have provided that same trained network as binary files in the directory.
    Let's open that trained network using TensorBoard and explore its components.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将训练好的网络作为二进制文件提供在该目录下。让我们使用 TensorBoard 打开这个训练好的网络，并探索其组成部分。
- en: 'Using your terminal, navigate to the directory `chapter_4/activity_2` and execute
    the following command to start TensorBoard:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你的终端，导航到目录 `chapter_4/activity_2` 并执行以下命令启动 TensorBoard：
- en: '[PRE7]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, open the URL provided by TensorBoard in your browser. You should be able
    to see the TensorBoard scalars page:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在浏览器中打开 TensorBoard 提供的 URL。你应该能够看到 TensorBoard 的标量页面：
- en: '![](img/82521ed5-42e0-4c0b-8c13-3d47a26ca2b2.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82521ed5-42e0-4c0b-8c13-3d47a26ca2b2.png)'
- en: 'Figure 8: Image of a terminal after starting a TensorBoard instance'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：启动 TensorBoard 实例后的终端图像
- en: 'After you open the URL provided by the `tensorboard` command, you should be
    able to see the following TensorBoard page:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在你打开`tensorboard`命令提供的网址后，你应该能够看到以下TensorBoard页面：
- en: '![](img/52461105-3b8e-47e5-9bef-def7c8431481.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52461105-3b8e-47e5-9bef-def7c8431481.png)'
- en: 'Figure 9: Image of the TensorBoard landing page'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：TensorBoard登录页面的图像
- en: Let's now explore our trained neural network and see how it performed.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来探索我们训练好的神经网络，看看它的表现如何。
- en: On the TensorBoard page, click on the **Scalars** page and enlarge the **Accuracy**
    graph. Now, move the **Smoothing** slider to **0.9**.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorBoard页面上，点击**Scalars**页面，并放大**准确率**图表。现在，将**平滑**滑块移到**0.9**。
- en: The accuracy graph measures how accurate the network was able to guess the labels
    of a test set. At fist, the network guesses those labels completely wrong. This
    happens because we have initialized the weights and biases of our network with
    random values, so its fist attempts are a guess. The network will then change
    the weights and biases of its layers on a second run; the network will continue
    to invest in the nodes that give positive results by altering their weights and
    biases, and penalize those that don't by gradually reducing their impact on the
    network (eventually reaching 0). As you can see, this is a really efficient technique
    that quickly yields great results.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率图表衡量了网络在测试集上猜测标签的准确性。刚开始时，网络的标签猜测完全错误。这是因为我们将网络的权重和偏置初始化为随机值，所以它的第一次尝试只是一个猜测。然后，网络将在第二次运行时调整其层的权重和偏置；网络将通过改变权重和偏置来投资于那些给出正面结果的节点，而通过逐渐减少它们对网络的影响（最终达到0）来惩罚那些表现不佳的节点。正如你所看到的，这是一种非常高效的技巧，可以快速得到良好的结果。
- en: Let's focus our attention on the **Accuracy** graph. See how the algorithm manages
    to reach great accuracy (> 95 percent) after around 1,000 epochs? What happens
    between 1,000 and 2,000 epochs?
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将注意力集中在**准确率**图表上。看看算法是如何在大约1,000个epoch后达到很高的准确率（> 95%）的？在1,000到2,000个epoch之间发生了什么？
- en: Would it get more accurate if we continued to train with more epochs? Between
    1,000 and 2,000 is when the accuracy of the network continues to improve, but
    at a decreasing rate. The network may improve slightly if trained with more epochs,
    but it will not reach 100 percent accuracy with the current architecture.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们继续训练更多的epoch，网络会变得更准确吗？在1,000到2,000个epoch之间，网络的准确率持续提高，但提升的速度在减慢。如果继续训练，网络可能会有轻微的改进，但在当前架构下，它无法达到100%的准确率。
- en: 'The script is a modified version of an official Google script that was created
    to show how TensorFlow works. We have divided the script into functions that are
    easier to understand and added many comments to guide your learning. Try running
    that script by modifying the variables at the top of the script:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本是一个修改版的官方Google脚本，旨在展示TensorFlow是如何工作的。我们将脚本分成了更易理解的函数，并添加了许多注释来引导你的学习。尝试通过修改脚本顶部的变量来运行这个脚本：
- en: '[PRE8]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, try running that script by modifying the values of those variables. For
    instance, try modifying the learning rate to **0.1** and the epochs to **100**.
    Do you think the network can achieve comparable results?
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，尝试通过修改这些变量的值来运行那个脚本。例如，试着将学习率修改为**0.1**，将epoch设置为**100**。你认为网络能够获得相似的结果吗？
- en: There are many other parameters that you can modify in your neural network.
    For now, experiment with the epochs and the learning rate of your network. You
    will notice that those two on their own can greatly change the output of your
    network—but only by so much. Experiment to see if you can train this network faster
    with the current architecture just by altering those two parameters.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你的神经网络中有许多其他参数可以修改。现在，尝试调整网络的epoch和学习率。你会发现，这两个参数本身就能极大地改变网络的输出——但也有其限制。尝试看看通过仅仅改变这两个参数，是否能够使当前架构下的网络训练更快。
- en: Verify how your network is training using TensorBoard. Alter those parameters
    a few more times by multiplying the starting values by 10 until you notice that
    the network is improving. This process of tuning the network and finding improved
    accuracy is similar to what is used in industry applications today to improve
    existing neural network models.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorBoard验证你的网络训练情况。通过将起始值乘以10，多次调整这些参数，直到你注意到网络有所改善。这种调整网络并找到更好准确率的过程类似于今天在工业应用中用来改进现有神经网络模型的方法。
- en: Summary
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored a TensorFlow-trained neural network using TensorBoard
    and trained our own modified version of that network with different epochs and
    learning rates. This gave you hands-on experiences on how to train a highly performant
    neural network and also allowed you to explore some of its limitations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用 TensorBoard 探索了一个基于 TensorFlow 训练的神经网络，并用不同的训练轮次和学习率训练了我们自己的修改版网络。这为你提供了如何训练一个高效神经网络的实际操作经验，并且让你有机会探索其一些局限性。
- en: Do you think we can achieve similar accuracy with real Bitcoin data? We will
    attempt to predict future Bitcoin prices using a common neural network algorithm
    during C*hapter 5*, *Model Architecture*. In C*hapter 6*, *Model Evaluation and
    Optimization*, we will evaluate and improve that model and, finally, in C*hapter
    7*, *Productization*, we will create a program that serves the prediction of that
    system via a HTTP API.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为我们能否在真实的比特币数据上实现类似的准确度？我们将在**第5章**，*模型架构*中尝试使用一种常见的神经网络算法来预测未来的比特币价格。在**第6章**，*模型评估与优化*中，我们将评估并改进该模型，最后在**第7章**，*产品化*中，我们将创建一个通过
    HTTP API 提供该系统预测的程序。
