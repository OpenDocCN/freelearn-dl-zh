- en: Handwritten Digits Classification Using ConvNets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ConvNets进行手写数字分类
- en: Welcome to this chapter on using **convolution neural networks** (**ConvNets**)
    for the classification of handwritten digits. In [Chapter 2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training
    NN for Prediction Using Regression,* we built a simple neural network for classifying
    handwritten digits. This was 87% accurate, but we were not happy with its performance.
    In this chapter, we will understand what convolution is and build a ConvNet for
    classifying the handwritten digits to help the restaurant chain become more accurate
    in sending text messages to the right person. If you have not been through [Chapter
    2](https://cdp.packtpub.com/python_deep_learning_projects/wp-admin/post.php?post=31&action=edit#post_25), *Training
    NN for Prediction Using Regression*, please go through it once so that you can
    get an understanding of the use case.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到本章，介绍如何使用**卷积神经网络**（**ConvNets**）对手写数字进行分类。在[第二章](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml)，《使用回归训练神经网络进行预测》中，我们建立了一个简单的神经网络来分类手写数字。这个网络的准确率为87%，但我们对其表现并不满意。在本章中，我们将了解卷积的概念，并构建一个ConvNet来对手写数字进行分类，帮助餐饮连锁公司更准确地将短信发送给正确的人。如果你还没有学习过[第二章](https://cdp.packtpub.com/python_deep_learning_projects/wp-admin/post.php?post=31&action=edit#post_25)《使用回归训练神经网络进行预测》，请先学习它，这样你能更好地理解案例背景。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Convolution
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积
- en: Pooling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化
- en: Dropout
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dropout（丢弃法）
- en: Training the model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型
- en: Testing the model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试模型
- en: Building deeper models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建更深的模型
- en: It would be better if you implement the code snippets as you go through this
    chapter, either in a Jupyter Notebook or any source code editor. This will make
    it easier for you to follow along as well as understand how the different sections
    of the code work.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在学习本章时边做边实现代码片段，最好使用Jupyter Notebook或任何源代码编辑器。这将帮助你更轻松地跟进并理解代码的不同部分是如何工作的。
- en: All of the Python files and the Jupyter Notebook files for this chapter can
    be found at [https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter08](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter08).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所有Python文件和Jupyter Notebook文件可以在[https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter08](https://github.com/PacktPublishing/Python-Deep-Learning-Projects/tree/master/Chapter08)找到。
- en: Code implementation
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码实现
- en: In this exercise, we will be using the Keras deep learning library, which is
    a high-level neural network API capable of running on top of TensorFlow, Theano,
    and CNTK.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用Keras深度学习库，它是一个高层次的神经网络API，能够在TensorFlow、Theano和CNTK之上运行。
- en: Know the code! We will not spend time understanding how Keras works, but if
    you are interested, refer to this easy-to-understand official documentation from
    Keras at [https://keras.io/](https://keras.io/).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 了解代码！我们不会花时间去理解Keras是如何工作的，但如果你感兴趣，可以参考Keras官方提供的这份简单易懂的文档：[https://keras.io/](https://keras.io/)。
- en: Importing all of the dependencies
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入所有依赖项
- en: 'We will be using the `numpy`, `matplotlib`, `keras`, `scipy`, and `tensorflow`
    packages in this exercise. Here, TensorFlow is used as the backend for Keras.
    You can install these packages with `pip`. For the MNIST data, we will be using
    the dataset available in the `keras` module with a simple `import`:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用`numpy`、`matplotlib`、`keras`、`scipy`和`tensorflow`这些包。这里，TensorFlow作为Keras的后端。你可以通过`pip`安装这些包。对于MNIST数据集，我们将使用`keras`模块中的数据集，并通过简单的`import`导入：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It is important that you set `seed` for reproducibility:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 设置`seed`以确保结果可复现非常重要：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Exploring the data
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据
- en: 'Let''s import the `mnist` module that''s available in `keras` with the following
    code:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用以下代码导入`keras`中的`mnist`模块：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, unpack the `mnist` train and test images with the following code:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用以下代码解压`mnist`的训练和测试图片：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that the data has been imported, let''s explore these digits:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已导入，让我们来探索这些数字：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following is the output of the preceding code:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/6f46b58b-f424-4863-a117-e6ee356075d7.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f46b58b-f424-4863-a117-e6ee356075d7.png)'
- en: 'Figure 8.1: Printout information of the data'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：数据的打印信息
- en: From the preceding screenshot, we can see that we have `60000` train images,
    `10000` test images with each image being `28`*`28` in size, and a total of `10`
    predictable classes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的截图中，我们可以看到，我们有`60000`张训练图片，`10000`张测试图片，每张图片的大小为`28`*`28`，总共有`10`个可预测类别。
- en: 'Now, let''s plot `9` handwritten digits. Before that, we will need to import
    `matplotlib` for plotting:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制`9`个手写数字。在此之前，我们需要导入`matplotlib`库用于绘图：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following is the output of the preceding code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '![](img/86089bae-b734-4ffe-b382-c4e84ea7b84e.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86089bae-b734-4ffe-b382-c4e84ea7b84e.png)'
- en: 'Figure 8.2: Visualizing MNIST digits'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：可视化 MNIST 数字
- en: 'Print out the maximum and minimum pixel value of the pixels in the `training_set`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 打印出`training_set`中像素的最大值和最小值：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following is the output of the preceding code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '![](img/93e4e3f7-85d4-48ed-9cba-d22009aecff7.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93e4e3f7-85d4-48ed-9cba-d22009aecff7.png)'
- en: 'Figure 8.3: Printout of the maximum and minimum pixel value in the data'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：数据中最大和最小像素值的打印输出
- en: We can see that the maximum and minimum pixel values in the training set are
    `255` and `0`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，训练集中像素的最大值和最小值分别为`255`和`0`。
- en: Defining the hyperparameters
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义超参数
- en: 'The following are some of the hyperparameters that we will be using throughout
    our code. These are totally configurable:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将在代码中使用的一些超参数。这些参数都是可配置的：
- en: '[PRE7]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you look back at [Chapter 2](https://cdp.packtpub.com/python_deep_learning_projects/wp-admin/post.php?post=31&action=edit#post_25), *Training
    NN for Prediction Using Regression*, you'll see that the `optimizer` used there
    was `Adam`. Therefore, we will import the `Adam` optimizer from the `keras` module
    and set its learning rate, as shown in the preceding code. For most cases that
    will follow, we will be training for `20` `epochs` for ease of comparison.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果回顾一下[第 2 章](https://cdp.packtpub.com/python_deep_learning_projects/wp-admin/post.php?post=31&action=edit#post_25)，*使用回归训练神经网络进行预测*，你会看到当时使用的`optimizer`是`Adam`。因此，我们将从`keras`模块导入`Adam`优化器，并设置其学习率，如前面的代码所示。在接下来的大多数情况中，我们将训练`20`个`epochs`，以便于比较。
- en: To learn more about the `optimizers` and their APIs in Keras, visit [https://keras.io/optimizers/](https://keras.io/optimizers/).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 Keras 中的`optimizers`及其 API，请访问 [https://keras.io/optimizers/](https://keras.io/optimizers/)。
- en: Experiment with different learning rates, optimizers, and batch sizes to see
    how these factors affect the quality of your model. If you get better results,
    show this to the deep learning community.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试不同的学习率、优化器和批量大小，看看这些因素如何影响模型的质量。如果你得到更好的结果，可以向深度学习社区展示。
- en: Building and training a simple deep neural network
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建和训练一个简单的深度神经网络
- en: Now that we have loaded the data into memory, we need to build a simple neural
    network model to predict the MNIST digits. We will use the same architecture we
    used in [Chapter 2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training NN
    for Prediction Using Regression*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据加载到内存中，接下来需要构建一个简单的神经网络模型来预测 MNIST 数字。我们将使用与[第 2 章](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml)中相同的架构，*使用回归训练神经网络进行预测*。
- en: 'We will be building a `Sequential` model. So, let''s import it from Keras and
    initialize it with the following code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个`Sequential`模型。所以，让我们从 Keras 中导入它，并用以下代码初始化它：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To learn more about the Keras Model API, visit [https://keras.io/models/model/](https://keras.io/models/model/).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 Keras 模型 API 的信息，请访问 [https://keras.io/models/model/](https://keras.io/models/model/)。
- en: 'The next thing that we need to do is define the `Dense`/Perceptron layer. In
    Keras, this can be done by importing the `Dense` layer, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义`Dense`/感知机层。在 Keras 中，可以通过导入`Dense`层来完成，如下所示：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we need to add the `Dense` layer to the `Sequential` model as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要将`Dense`层添加到`Sequential`模型中，如下所示：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To learn more about the Keras `Dense` API call, visit [https://keras.io/layers/core/](https://keras.io/layers/core/).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 Keras `Dense` API 的信息，请访问 [https://keras.io/layers/core/](https://keras.io/layers/core/)。
- en: The `add` command performs the job of appending a layer to the `Sequential`
    model, in this case, `Dense`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`add`命令用于将一个层追加到`Sequential`模型中，在这种情况下是`Dense`层。'
- en: In the `Dense` layer in the preceding code, we have defined the number of neurons
    in the first hidden layer, which is `300`. We have also defined the `input_shape` parameter
    as being equal to `(784,)` to indicate to the model that it will be accepting
    input arrays of the shape `(784,)`. This means that the input layer will have
    `784` neurons.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中的`Dense`层中，我们定义了第一隐藏层的神经元数量，即`300`。我们还将`input_shape`参数定义为`(784,)`，以便告诉模型它将接受形状为`(784,)`的输入数组。这意味着输入层将有`784`个神经元。
- en: The type of activation function that needs to be applied to the result can be
    defined with the `activation` parameter. In this case, this is `relu`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 需要应用于结果的激活函数类型可以通过 `activation` 参数定义。在本例中，这是 `relu`。
- en: 'Add another `Dense` layer of `300` neurons by using the following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码添加另一个包含 `300` 个神经元的 `Dense` 层：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And the final `Dense` layer with the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以及具有以下代码的最终 `Dense` 层：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, the final layer has `10` neurons as we need it to predict scores for `10`
    classes. The `activation` function that has been chosen here is `softmax` so that
    we can limit the scores between 0 and 1, and the sum of scores to 1.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，最终层有 `10` 个神经元，因为我们需要它预测 `10` 个类别的分数。这里选择的 `activation` 函数是 `softmax`，以便我们将分数限制在
    0 到 1 之间，并使所有分数之和为 1。
- en: 'Compiling the model in Keras is super-easy and can be done with following code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中编译模型非常简单，可以通过以下代码完成：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: All you need to do to compile the model is call the `compile` method of the
    model and specify the `loss`, `optimizer`, and `metrics` parameters, which in
    this case are `sparse_categorical_crossentropy`, `Adam`, and `['accuracy']`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 编译模型时，所需要做的就是调用模型的 `compile` 方法，并指定 `loss`、`optimizer` 和 `metrics` 参数，在本例中分别为
    `sparse_categorical_crossentropy`、`Adam` 和 `['accuracy']`。
- en: To learn more about the Keras Model's `compile` method, visit [https://keras.io/models/model/](https://keras.io/models/model/).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 Keras Model 的 `compile` 方法，请访问 [https://keras.io/models/model/](https://keras.io/models/model/)。
- en: The metrics that need to be monitored during this learning process must be specified
    as a list to the `metrics` parameter of the `compile` method.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中需要监控的指标必须作为列表传递给 `compile` 方法的 `metrics` 参数。
- en: 'Print out the summary of the model with the following code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码打印出模型的摘要：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is the output of the preceding code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/d0f371cb-8471-40ad-8f5e-77dc01bf564d.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0f371cb-8471-40ad-8f5e-77dc01bf564d.png)'
- en: 'Figure 8.4: Summary of the multilayer Perceptron model'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：多层感知器模型的摘要
- en: Notice that this model has `328,810` trainable parameters, which is reasonable.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，该模型有 `328,810` 个可训练参数，这是合理的。
- en: 'Now, split the train data into train and validation data by using the `train_test_split`
    function that we imported from `sklearn`:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用从 `sklearn` 导入的 `train_test_split` 函数，将训练数据拆分为训练和验证数据：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We have split the data so that we end up with 55,000 training examples and 5,000
    validation examples.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据分割成了 55,000 个训练示例和 5,000 个验证示例。
- en: You will also see that we have reshaped the arrays so that each image is of
    shape `(784,)`. This is because we have defined the model to accept images/arrays
    of shape `(784,)`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会看到我们已经重塑了数组，使得每个图像的形状为 `(784,)`。这是因为我们已经定义了模型接受形状为 `(784,)` 的图像/数组。
- en: Like we did in [Chapter 2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training
    NN for Prediction Using Regression*, we will now train our model on 55,000 training
    examples, validate on 5,000 examples, and test on 10,000 examples.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [第 2 章](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml) 中所做的，*使用回归进行预测训练神经网络*，我们现在将在
    55,000 个训练示例上训练模型，在 5,000 个示例上验证，并在 10,000 个示例上测试。
- en: Assigning the fit to a variable stores relevant information inside it, such
    as train and validation loss and accuracy at each `epoch`, which can then be used
    for plotting the learning process.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 将拟合结果赋值给一个变量，会存储相关信息，如每个 `epoch` 的训练和验证损失及准确率，之后可以用来绘制学习过程。
- en: Fitting a model
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'To fit a model in Keras, along with train digits and train labels, call the
    `fit` method of the model with the following parameters:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中拟合模型时，结合训练数字和训练标签，调用模型的 `fit` 方法，并传入以下参数：
- en: '`epochs`: The number of epochs'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`：训练的轮数'
- en: '`batch_size`: The number of images in each batch'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`：每个批次中的图像数量'
- en: '`validation_data`: The tuple of validation images and validation labels'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validation_data`：验证图像和验证标签的元组'
- en: 'Look at the *Defining the hyperparameters* section of the chapter for the defined
    values of `epochs` and `batch_size`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 查看本章的 *定义超参数* 部分，获取 `epochs` 和 `batch_size` 的定义值：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following is the output of the preceding code:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/d4bbaf1a-b09d-4eb6-9936-4013b7c625de.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4bbaf1a-b09d-4eb6-9936-4013b7c625de.png)'
- en: 'The following is the output at the end of the code''s execution:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码执行结束时的输出：
- en: '![](img/60b81072-408b-47a5-a4fc-2f2b7bd14c23.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60b81072-408b-47a5-a4fc-2f2b7bd14c23.png)'
- en: 'Figure 8.5: Metrics printed out during the training of MLP'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5：在 MLP 训练过程中打印出的指标
- en: Evaluating a model
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'To evaluate the model on test data, you can call the `evaluate` method of the
    `model` by feeding the test images and test labels:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在测试数据上评估模型，你可以通过将测试图像和测试标签传入`model`的`evaluate`方法来进行评估：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following is the output of the preceding code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '![](img/eb0eebef-2954-4f4d-a944-9046e9d05718.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb0eebef-2954-4f4d-a944-9046e9d05718.png)'
- en: 'Figure 8.6: Printout of the evaluation of MLP'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6：MLP评估的打印输出
- en: From the validation and test accuracy, we can see that after 20 epochs of training,
    we have reached the same level of accuracy as we did in [Chapter 2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training
    NN for Prediction Using Regression,* but with very few lines of code.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 从验证和测试准确率来看，我们可以看到，在经过20个周期的训练后，我们达到了与[第2章](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml)中*使用回归训练神经网络进行预测*相同的准确率，但代码量非常少。
- en: 'Now, let''s define a function to plot the train and validation loss and accuracy
    that we have stored in the `history` variable:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义一个函数，绘制我们存储在`history`变量中的训练和验证损失及准确率：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following is the output of the preceding code:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '![](img/9f2661fe-de56-4d6f-af06-92ef81b6086d.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f2661fe-de56-4d6f-af06-92ef81b6086d.png)'
- en: 'Figure 8.7: MLP loss/accuracy plot during training'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7：训练过程中MLP损失/准确率图
- en: MLP – Python file
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP – Python文件
- en: 'This module implements training and evaluation of a simple MLP:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块实现了一个简单MLP的训练和评估：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Convolution
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积
- en: Convolution can be defined as the process of striding a small kernel/filter/array
    over a target array and obtaining the sum of element-wise multiplication between
    the kernel and a subset of equal size of the target array at that location.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积可以定义为将一个小的卷积核/滤波器/数组沿着目标数组滑动，并在每个位置计算卷积核与目标数组相同大小子集的逐元素相乘的和。
- en: 'Consider the following example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, you have a target `array` of length 10 and a `kernel` of length 3.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，你有一个长度为10的目标`array`和一个长度为3的`kernel`。
- en: 'When you start the convolution, implement the following steps:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始卷积时，执行以下步骤：
- en: The `kernel` will be multiplied with the subset of the target `array` within
    indices 0 through 2\. This will be between [-1,1,0] (kernel) and [0,1,0] (from
    index 0 through to 2 of the target array). The result of this element-wise multiplication
    will then be summed up to obtain what is called the result of convolution.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kernel`将与目标`array`中索引0到2的子集进行相乘。这将是[-1,1,0]（卷积核）与[0,1,0]（目标数组索引0到2的部分）。这个逐元素相乘的结果将被求和，得到所谓的卷积结果。'
- en: The `kernel` will then be stridden by 1 unit and then multiplied with the subset
    of the target `array` within the indices 1 through 3, just like in *Step 1*, and
    the result is obtained.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，`kernel`将按1单位步幅移动，并与目标`array`中索引1到3的子集相乘，像*步骤1*那样，得到结果。
- en: '*Step 2* is repeated until a subset equal to the length of the `kernel` is
    not possible at a new stride location.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤2*会重复执行，直到在新的步幅位置无法提取出与`kernel`长度相等的子集。'
- en: The result of convolution at each stride is stored in an `array`. This `array` that's
    holding the result of the convolution is called the feature map. The length of
    the 1-D feature map (with step/stride of 1) is equal to the difference in length
    of the `kernel` and the target `array` plus 1.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步卷积的结果都存储在一个`array`中。这个存储卷积结果的`array`被称为特征图。1维特征图的长度（步幅为1时）等于`kernel`和目标`array`长度的差值再加1。
- en: 'Only in this case, we need to take the following equation into account:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在这种情况下，我们需要考虑以下方程：
- en: '*length of the feature map = length of the target array - length of the kernel
    + 1*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征图的长度 = 目标数组的长度 - 卷积核的长度 + 1*'
- en: 'Here is a code snippet implementing 1-D convolution:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个实现1维卷积的代码片段：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following is the output of the preceding code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '![](img/19786906-1ec7-4677-8f4d-83fabcddbfd4.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19786906-1ec7-4677-8f4d-83fabcddbfd4.png)'
- en: 'Figure 8.8: Printout of example feature map'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8：示例特征图的打印输出
- en: Convolution in Keras
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的卷积
- en: Now that you have an understanding of how convolution works, let's put it into
    use and build a CNN classifier on MNIST digits.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了卷积是如何工作的，让我们将其付诸实践，构建一个基于MNIST数字的CNN分类器。
- en: 'For this, import the `Conv2D` API from the `layers` module of Keras. You can
    do this with the following code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，导入Keras的`layers`模块中的`Conv2D` API。你可以使用以下代码来实现：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Since the convolution will be defined to accept images of shape `28`*`28`*`1`,
    we need to reshape all the images to be of `28`*`28`*`1`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于卷积会被定义为接受`28`*`28`*`1`形状的图像，因此我们需要将所有图像重塑为`28`*`28`*`1`：
- en: '[PRE23]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following is the output of the preceding code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/b66504ca-6c9c-4ed8-8308-8e13e21102db.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b66504ca-6c9c-4ed8-8308-8e13e21102db.png)'
- en: 'Figure 8.9: Shape of data after reshaping'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9：重塑后的数据形状
- en: 'To build the `model`, just like we did previously, we need to initialize the
    `model` as `Sequential`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建 `model`，就像之前那样，我们需要将 `model` 初始化为 `Sequential`：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, add the `Conv2D` layer to the `model` with the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用以下代码将 `Conv2D` 层添加到 `model` 中：
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In the `Conv2D` API, we have defined the following parameters:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `Conv2D` API 中，我们定义了以下参数：
- en: '`units`: `32` (number of kernels/filters)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`units`：`32`（卷积核/滤波器的数量）'
- en: '`kernel_size`: `(3,3)` (size of each kernel)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel_size`：`(3,3)`（每个卷积核的大小）'
- en: '`input_shape`: `28*28*1` (shape of the input array it will receive)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_shape`：`28*28*1`（它将接收的输入数组的形状）'
- en: '`activation`: `relu`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation`：`relu`'
- en: For additional information on the `Conv2D` API, visit [https://keras.io/layers/convolutional/](https://keras.io/layers/convolutional/).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 `Conv2D` API 的更多信息，请访问 [https://keras.io/layers/convolutional/](https://keras.io/layers/convolutional/)。
- en: 'The result of the preceding convolution is `32` feature maps of size 26*26\.
    These 2-D feature maps now have to be converted into a 1-D feature map. This can
    be done in Keras with the following code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 前述卷积操作的结果是 `32` 个 26*26 的特征图。现在，这些 2-D 特征图需要转换成 1-D 特征图。这可以通过以下 Keras 代码完成：
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The result of the preceding snippet is just like a layer of neurons in a simple
    neural network. The `Flatten` function converts all of the 2-D feature maps into
    a single `Dense` layer. In this layer, will we add a `Dense` layer with `128`
    neurons:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码片段的结果就像一个简单神经网络中的神经元层。`Flatten` 函数将所有的 2-D 特征图转换为一个单一的 `Dense` 层。在此层中，我们将添加一个包含
    `128` 个神经元的 `Dense` 层：
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Since we need to get scores for each of the `10` possible classes, we must
    add another `Dense` layer with `10` neurons, with `softmax` as the `activation`
    function:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要获取每个 `10` 个可能类别的分数，我们必须添加另一个包含 `10` 个神经元的 `Dense` 层，并使用 `softmax` 作为 `activation`
    函数：
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, just like in the case of the simple dense neural network we built in the
    preceding code, we will compile and fit the model:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，就像我们在前述代码中构建的简单全连接神经网络一样，我们将编译并拟合模型：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following is the output of the preceding code:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/057e49ab-6aaa-44dc-a757-f0963c239c92.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/057e49ab-6aaa-44dc-a757-f0963c239c92.png)'
- en: 'Figure 8.10: Summary of the convolution classifier'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10：卷积分类器的总结
- en: From the model's summary, we can see that this convolution classifier has `2,770,634`
    parameters. This is a lot of parameters compared to the Perceptron model. Let's
    fit this model and evaluate its performance.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型的总结中，我们可以看到，这个卷积分类器有 `2,770,634` 个参数。与感知机模型相比，这是一个非常庞大的参数数量。让我们拟合这个模型并评估其性能。
- en: Fitting the model
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'Fit the convolution neural network model on the data with the following code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码将卷积神经网络模型拟合到数据：
- en: '[PRE30]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following is the output of the preceding code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/8ddd7b69-c41e-4470-af04-6baa6893e70d.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ddd7b69-c41e-4470-af04-6baa6893e70d.png)'
- en: 'The following is the output from the end of the code''s execution:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码执行结束时的输出：
- en: '![](img/f7b06e8e-39a6-4e6b-bb51-28bb823a12d8.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7b06e8e-39a6-4e6b-bb51-28bb823a12d8.png)'
- en: 'Figure 8.11: Metrics printed out during the training of the convolution classifier'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11：卷积分类器训练过程中打印出的指标
- en: We can see that the convolution classifier's accuracy is 97.72% on the validation
    data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，卷积分类器在验证数据上的准确率为 97.72%。
- en: Evaluating the model
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'You can evaluate the convolution model on the test data with the following
    code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下代码评估卷积模型在测试数据上的表现：
- en: '[PRE31]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following is the output of the preceding code:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/5f297d6d-c560-43fc-a4aa-6695398f9c48.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f297d6d-c560-43fc-a4aa-6695398f9c48.png)'
- en: 'Figure 8.12: Printout of the evaluation of the convolution classifier'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12：卷积分类器评估的打印输出
- en: We can see that the model is 97.92% accurate on test data, 97.72% on validation
    data, and 99.71% on train data. It is clear from the loss as well that the model
    is slightly overfitting on the train data. We will talk about how to handle overfitting
    later.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，模型在测试数据上的准确率为 97.92%，在验证数据上的准确率为 97.72%，在训练数据上的准确率为 99.71%。从损失情况来看，模型在训练数据上有些过拟合。我们稍后将讨论如何处理过拟合问题。
- en: 'Now, let''s plot the train and validation metrics to see how the training has
    progressed:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制训练和验证指标，以查看训练进展情况：
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following is the output of the preceding code:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/0b4f6081-a092-4032-a404-6fd8ee593819.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b4f6081-a092-4032-a404-6fd8ee593819.png)'
- en: 'Figure 8.13: Loss/accuracy plot of the convolution classifier during training'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13：卷积分类器训练过程中的损失/准确率图
- en: Convolution – Python file
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积 – Python 文件
- en: 'This module implements the training and evaluation of a convolution classifier:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块实现了卷积分类器的训练和评估：
- en: '[PRE33]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Pooling
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化
- en: Max pooling can be defined as the process of summarizing a group of values with
    the maximum value within that group. Similarly, if you computed the average, it
    would be average pooling. Pooling operations are usually performed on the generated
    feature maps after convolution to reduce the number of parameters.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化可以定义为将一组值用该组中的最大值进行总结的过程。类似地，如果计算平均值，那么就是平均池化。池化操作通常在卷积后生成的特征图上执行，以减少参数的数量。
- en: 'Let''s take the example array we considered for convolution:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑卷积时使用的示例数组：
- en: '[PRE34]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now, if you were to perform max pooling on this `array` with the pool size set
    to size 1*2 and a stride of 2, the result would be an array of [1,1,1,1,1]. The
    `array` of size 1*10 has been reduced to a size of 1*5 due to max pooling.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你在这个 `array` 上执行最大池化，池大小设置为 1*2，步幅为 2，结果将是数组 [1,1,1,1,1]。这个 1*10 的 `array`
    由于最大池化，已经被缩小为 1*5。
- en: Here, since the pool size is of shape 1*2, you would take the subset of the
    target `array` from index 0 to index 2, which will be [0,1], and compute the maximum
    of this subset as 1\. You would do the same for the subset from index 2 to index
    4, from index 4 to index 6, index 6 to index 8, and finally index 8 to 10.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，由于池大小为 1*2，你将从索引 0 到索引 2 提取目标 `array` 的子集，即 [0,1]，然后计算该子集的最大值为 1。对于从索引 2
    到索引 4、从索引 4 到索引 6、从索引 6 到索引 8，最后从索引 8 到 10，你都会执行相同的操作。
- en: Similarly, average pooling can be implemented by computing the average value
    of the pooled section. In this case, it would result in the array [0.5, 0.5, 0.5,
    0.5, 0.5].
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，平均池化可以通过计算池化部分的平均值来实现。在这种情况下，结果数组将是 [0.5, 0.5, 0.5, 0.5, 0.5]。
- en: 'The following are a couple of code snippets that are implementing max and average
    pooling:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是几个实现最大池化和平均池化的代码片段：
- en: '[PRE35]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following is the output of the preceding code:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '![](img/733c7c85-bf2a-4c0e-b81d-3b731c007ceb.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/733c7c85-bf2a-4c0e-b81d-3b731c007ceb.png)'
- en: 'Figure 8.14: Max pooling operation''s result on an array'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14：最大池化操作在数组上的结果
- en: 'The following is the code snippet for average pooling:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是平均池化的代码片段：
- en: '[PRE36]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following is the output of the preceding code:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '![](img/3631da1b-3ee5-4b59-ab05-926159042499.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3631da1b-3ee5-4b59-ab05-926159042499.png)'
- en: 'Figure 8.15: Average pooling operation''s result on an array'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15：平均池化操作在数组上的结果
- en: 'The following is a diagram explaining the max pooling operation:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是解释最大池化操作的图示：
- en: '![](img/ae6d1b2c-13c4-478e-85c1-a416714eaa3e.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae6d1b2c-13c4-478e-85c1-a416714eaa3e.png)'
- en: 'Figure 8.16: 2*2 max pooling with stride 2 (Source: https://en.wikipedia.org/wiki/Convolutional_neural_network)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16：2*2 最大池化，步幅为 2（来源：https://en.wikipedia.org/wiki/Convolutional_neural_network）
- en: 'Consider the following code for a digit:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑以下关于数字的代码：
- en: '[PRE37]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following is the output of the preceding code:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '![](img/523de311-01e5-4497-9088-248fc21f24f3.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/523de311-01e5-4497-9088-248fc21f24f3.png)'
- en: 'Figure 8.17: Random MNIST digit'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17：随机 MNIST 数字
- en: This image is of shape `28`*`28`. Now, if you were to perform a 2*2 max pooling
    operation of this, the resulting image would have a shape of `14`*`14`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 该图像的形状是 `28`*`28`。现在，如果你对其执行 2*2 最大池化操作，结果图像的形状将变为 `14`*`14`。
- en: 'Now, let''s write a function to implement a 2*2 max pooling operation on a
    MNIST digit:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写一个函数来实现 MNIST 数字的 2*2 最大池化操作：
- en: '[PRE38]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following is the output of the preceding code:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出：
- en: '![](img/7ab0b5fc-261c-4e9e-a7e5-b1a429a169b0.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ab0b5fc-261c-4e9e-a7e5-b1a429a169b0.png)'
- en: 'Figure 8.18: Random MNIST digit after max pooling'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.18：最大池化后的随机 MNIST 数字
- en: You may have noticed that the convolution classifier that we built in the previous
    section has around 2.7 million parameters. It has been proven that having a lot
    of parameters can lead to overfitting in a lot of cases. This is where pooling
    comes in. It helps us to retain the important features in the data as well as reduce
    the number of parameters.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，我们在上一节中构建的卷积分类器大约有 270 万个参数。已经证明，在许多情况下，拥有大量参数会导致过拟合。此时池化操作就显得至关重要。它帮助我们保留数据中的重要特征，并减少参数的数量。
- en: Now, let's implement a convolution classifier with max pooling.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现一个带有最大池化的卷积分类器。
- en: 'Import the max pool operation from Keras with the following code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码从 Keras 导入最大池化操作：
- en: '[PRE39]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Then, define and compile the model:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，定义并编译模型：
- en: '[PRE40]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The following is the output of the preceding code:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/a0eef9af-8c2a-479a-95f7-4aa3e2f29ef7.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0eef9af-8c2a-479a-95f7-4aa3e2f29ef7.png)'
- en: 'Figure 8.19: Summary of the convolution classifier with max pooling'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.19：带最大池化的卷积分类器概述
- en: From the summary, we can see that with a pooling filter of 2*2 with stride 2,
    the number of parameters has come down to `693,962`, which is 1/4^(th) of the
    number of parameters in the convolution classifier.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 从摘要中，我们可以看到，使用步长为2的2*2池化滤波器后，参数数量已降至`693,962`，是卷积分类器参数数量的1/4。
- en: Fitting the model
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'Now, let''s fit the model on the data:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在数据上拟合模型：
- en: '[PRE41]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The following is the output of the preceding code:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/37aaddc3-716d-47fb-92ea-405aa4daa2cf.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37aaddc3-716d-47fb-92ea-405aa4daa2cf.png)'
- en: 'The following is the output at the end of the code''s execution:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码执行完毕后的输出：
- en: '![](img/6e318c23-82ba-4a2f-8a78-827d1400e94d.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e318c23-82ba-4a2f-8a78-827d1400e94d.png)'
- en: 'Figure 8.20: Metrics printed out during the training of the convolution classifier
    with max pooling'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.20：带最大池化的卷积分类器训练过程中打印出的指标
- en: We can see that the convolution classifier with max pooling has an accuracy
    of 97.72% on the validation data.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，带最大池化的卷积分类器在验证数据上的准确率为97.72%。
- en: Evaluating the model
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Now, evaluate the convolution model with max pooling on the test data:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在测试数据上评估带最大池化的卷积模型：
- en: '[PRE42]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The following is the output of the preceding code:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/af78f6d0-bf8f-4991-b522-a8bc839f5652.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af78f6d0-bf8f-4991-b522-a8bc839f5652.png)'
- en: 'Figure 8.21: Printout of the evaluation of the convolution classifier with
    max pooling'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.21：带最大池化的卷积分类器评估的打印输出
- en: We can see that the model is 97.88% accurate on the test data, 97.72% on the
    validation data, and 99.74% on the train data. The convolution model with pooling
    gives the same level of performance as the convolution model without pooling,
    but with four times less parameters.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，模型在测试数据上的准确率为97.88%，在验证数据上的准确率为97.72%，在训练数据上的准确率为99.74%。带池化的卷积模型与不带池化的卷积模型表现相同，但参数减少了四倍。
- en: In this case, we can clearly see from the loss that the model is slightly overfitting
    on the train data.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以从损失值中清楚地看到，模型在训练数据上略微过拟合。
- en: 'Just like we did previously, plot the train and validation metrics to see how
    the training has progressed:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前做的那样，绘制训练和验证指标，查看训练的进展：
- en: '[PRE43]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The following is the output of the preceding code:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/ecc6dcb1-e810-4ef6-9d75-7bb64805b8f2.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ecc6dcb1-e810-4ef6-9d75-7bb64805b8f2.png)'
- en: 'Figure 8.22: Loss/accuracy plot of the convolution classifier with max pooling
    during training'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.22：带最大池化的卷积分类器训练过程中的损失/准确率图
- en: Convolution with pooling – Python file
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积与池化 – Python 文件
- en: 'This module implements the training and evaluation of a convolution classifier
    with the pooling operation:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 本模块实现了带池化操作的卷积分类器的训练与评估：
- en: '[PRE44]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Dropout
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: 'Dropout is a regularization technique used to prevent overfitting. During training,
    it is implemented by randomly sampling a neural network from the original neural
    network during each forward and backward propagation, and then training this subset
    network on the batch of input data. During testing, no dropout is implemented.
    The test results are obtained as an ensemble of all of the sampled networks:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout 是一种正则化技术，用于防止过拟合。在训练过程中，它通过在每次前向和反向传播时随机从原始神经网络中采样一个神经网络，然后在输入数据的批次上训练这个子集网络来实现。测试时不进行
    dropout。测试结果作为所有采样网络的集合获得：
- en: '![](img/73b7aca8-0bd1-4833-89c6-70da2ebdacf1.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73b7aca8-0bd1-4833-89c6-70da2ebdacf1.png)'
- en: 'Figure 8.23: Dropout, as shown in the Dropout: A Simple Way to Prevent Neural
    Networks from'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.23：Dropout，如在《Dropout：防止神经网络过拟合的一种简单方法》所示
- en: 'Overfitting paper (Source: http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合论文（来源：http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf）
- en: 'In Keras, implementing `Dropout` is easy. First, import it from the `layers`
    module of `keras`:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，实现`Dropout`非常简单。首先，从`keras`的`layers`模块中导入它：
- en: '[PRE45]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, place the layer where needed. In the case of our CNN, we will place one
    after the max pool operation and one after the `Dense` layer, as shown in the
    following code:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将该层放置在需要的地方。对于我们的 CNN，我们将在最大池化操作后放置一个层，在`Dense`层后放置另一个层，如以下代码所示：
- en: '[PRE46]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The following is the output of the preceding code:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/4647d988-3e13-49f7-a703-ec54f465ee6c.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4647d988-3e13-49f7-a703-ec54f465ee6c.png)'
- en: 'Figure 8.24: Summary of the convolution classifier'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.24：卷积分类器总结
- en: Since `Dropout` is a regularization technique, adding it to a model will not
    result in a change in the number of trainable parameters.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`Dropout`是一种正则化技术，将其添加到模型中不会改变可训练参数的数量。
- en: Fitting the model
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'Again, train the model on the standard 20 `epochs`:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 再次在标准的20个`epochs`上训练模型：
- en: '[PRE47]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The following is the output of the preceding code:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/d19d9627-f78b-4236-aab5-ca2f67b12a81.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d19d9627-f78b-4236-aab5-ca2f67b12a81.png)'
- en: 'The following is the output at the end of the code''s execution:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码执行结束时的输出：
- en: '![](img/8b9bfe45-5557-4d4d-bd77-0d80bad973e4.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b9bfe45-5557-4d4d-bd77-0d80bad973e4.png)'
- en: 'Figure 8.25: Metrics printed out during the training of the convolution classifier
    with max pooling and dropout'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.25：训练过程中打印出的卷积分类器的指标，使用最大池化和丢弃法
- en: We see that the convolution classifier with max pooling and dropout is 98.52%
    accurate on the validation data.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到使用最大池化和丢弃法的卷积分类器在验证数据上的准确率为98.52%。
- en: Evaluating the model
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Now, let’s evaluate the model and capture the loss and the accuracy:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估模型并捕捉损失和准确率：
- en: '[PRE48]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The following is the output of the preceding code:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/b3216a23-ca5a-470c-b77c-2a48a300d96e.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b3216a23-ca5a-470c-b77c-2a48a300d96e.png)'
- en: 'Figure 8.26: Printout of the evaluation of the convolution classifier with
    max pooling and dropout'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.26：使用最大池化和丢弃法评估卷积分类器的输出
- en: We can see that the model is 98.42% accurate on the test data, 98.52% on the
    validation data, and 99.26% on the train data. The convolution model with pooling
    and dropout gives the same level of performance as the convolution model without
    pooling, but with four times fewer parameters. If you look at the `loss` as well,
    this model was able to reach a better minima than the other models we have trained
    before.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，模型在测试数据上的准确率为98.42%，在验证数据上为98.52%，在训练数据上为99.26%。带有池化和丢弃法的卷积模型提供了与没有池化的卷积模型相同的性能水平，但参数量减少了四倍。如果你也查看`loss`，你会发现这个模型比我们之前训练的其他模型达到了一个更好的最小值。
- en: 'Plot the metrics to understand how the training has progressed:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制指标图以了解训练进度：
- en: '[PRE49]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The following is the output of the preceding code:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/4c0d735f-5b28-4408-97f2-8e48e40a59da.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c0d735f-5b28-4408-97f2-8e48e40a59da.png)'
- en: 'Figure 8.27: Loss/accuracy plot of the convolution classifier with max pooling
    and dropout during training'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.27：卷积分类器训练过程中，使用最大池化和丢弃法的损失/准确度曲线
- en: Convolution with pooling – Python file
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用池化的卷积 – Python 文件
- en: 'This module implements the training and evaluation of a convolution classifier
    with the max pool and `Dropout` operations:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 本模块实现了一个带有最大池化和`Dropout`操作的卷积分类器的训练与评估：
- en: '[PRE50]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Going deeper
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向更深的网络进发
- en: The convolution classifier with max pooling and dropout seems to be the best
    classifier so far. However, we also noticed that there was a slight amount of
    overfitting on the train data.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 使用最大池化和丢弃法的卷积分类器似乎是迄今为止最好的分类器。然而，我们也注意到训练数据上存在轻微的过拟合现象。
- en: Let's build a deeper model to see if we can create a classifier that is more
    accurate than the other models we have trained so far, and see if we can get it
    to reach an even better minima.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个更深的模型，看看能否创建一个比我们迄今训练的其他模型更准确的分类器，并看看能否让它达到一个更好的最小值。
- en: 'We will build a deeper model by adding two more convolution layers to our best
    model so far:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过向目前最好的模型中添加两个卷积层来构建一个更深的模型：
- en: The first layer is a convolution 2-D layer with 32 filters of size 3*3 with
    `activation` as `relu`, followed by downsampling with max pooling of size 2*2,
    followed by `Dropout` as the regularizer
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层是一个卷积2D层，包含32个大小为3*3的滤波器，`activation`使用`relu`，接着进行大小为2*2的最大池化下采样，最后使用`Dropout`作为正则化方法
- en: The second layer is a convolution 2-D layer with 64 filters of size 3*3 with
    `activation` as `relu`, followed by downsampling with max pooling of size 2*2,
    followed by `Dropout` as the regularizer
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层是一个卷积2D层，包含64个大小为3*3的滤波器，`activation`使用`relu`，接着进行大小为2*2的最大池化下采样，最后使用`Dropout`作为正则化方法
- en: The third layer is a convolution 2-D layer with 128 filters of size 3*3 with
    `activation` as `relu`, followed by downsampling with max pooling of size 2*2,
    followed by `Dropout` as the regularizer
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层是一个卷积二维层，具有128个3*3大小的滤波器，`activation`为`relu`，接着是2*2大小的最大池化进行下采样，最后是`Dropout`作为正则化器
- en: Compiling the model
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'The following is the code for the deeper model:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是更深层模型的代码：
- en: '[PRE51]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The following is the output of the preceding code:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/4c6da246-d210-45da-8a69-311a1f58142d.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c6da246-d210-45da-8a69-311a1f58142d.png)'
- en: 'Figure 8.28: Summary of the deep convolution classifier'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.28：深度卷积分类器的总结
- en: From the summary, we can see that the deeper model has only `110,474` parameters.
    Now, let's see if a deeper model with fewer parameters can do a better job than
    we have done so far.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 从总结中可以看到，更深层的模型仅有`110,474`个参数。现在，让我们看看一个具有更少参数的更深层模型是否能比我们到目前为止做得更好。
- en: Fitting the model
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'Just like we did previously, fit the model, but with `epochs` set as `40` instead
    of 20, since the deeper model takes longer to learn. Try training the model for
    20 epochs first to see what happens:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 就像之前一样，拟合模型，但将`epochs`设置为`40`，因为更深的模型需要更长时间来学习。首先尝试训练20个epoch，看看会发生什么：
- en: '[PRE52]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The following is the output of the preceding code:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/6b362dee-0822-44f6-9e85-93421de78866.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b362dee-0822-44f6-9e85-93421de78866.png)'
- en: 'The following is the output at the end of the code''s execution:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码执行结束时的输出：
- en: '![](img/01f12117-7e01-4ade-8fcc-45cb8d3c09dd.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01f12117-7e01-4ade-8fcc-45cb8d3c09dd.png)'
- en: 'Figure 8.29: Metrics printed out during the training of the deep convolution
    classifier'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.29：深度卷积分类器训练过程中的度量输出
- en: Evaluating the model
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Now, evaluate the model with the following code:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用以下代码评估模型：
- en: '[PRE53]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The following is the output of the preceding code:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/6c817d9c-5f72-48fe-b3c4-bc89d13f8b3f.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c817d9c-5f72-48fe-b3c4-bc89d13f8b3f.png)'
- en: 'Figure 8.30: Printout of the evaluation of the deep convolution classifier'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.30：深度卷积分类器评估结果输出
- en: 'We can see that the model is 99.01% accurate on the test data, 98.84% on the
    validation data, and 98.38% on the train data. The deeper convolution model with pooling
    and dropout gives a much better performance with just 110,000 parameters. If you
    look at the `loss` as well, this model was able to reach a better minima than
    the other models that we trained previously:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，模型在测试数据上的准确率为99.01%，在验证数据上的准确率为98.84%，在训练数据上的准确率为98.38%。具有池化和Dropout的更深层卷积模型仅有110,000个参数，却提供了更好的性能。如果你也查看`loss`，这个模型能够达到比我们之前训练的其他模型更好的最小值：
- en: 'Plot the metrics to understand how the training has progressed:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制度量图表，以了解训练进度：
- en: '[PRE54]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The following is the output of the preceding code:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/e92baa61-56d7-43d5-bfbd-2aae16e39bfb.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e92baa61-56d7-43d5-bfbd-2aae16e39bfb.png)'
- en: 'Figure 8.31: Loss/accuracy plot of the deep convolution classifier during training'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.31：深度卷积分类器训练过程中的损失/准确率图
- en: This is one of the best training plots you can get. We can see no overfitting
    at all.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你可以获得的最佳训练图之一。我们可以看到完全没有过拟合。
- en: Convolution with pooling and Dropout – Python file
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积与池化以及Dropout – Python文件
- en: 'This module implements the training and evaluation of a deep convolution classifier
    with the max pool and `Dropout` operations:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块实现了一个深度卷积分类器的训练与评估，包含最大池化和`Dropout`操作：
- en: '[PRE55]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Data augmentation
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: Imagine a situation where you might want to build a convolution classifier on
    a small set of images. The problem here is that the classifier will easily overfit
    on this small set of data. The reason why the classifier will overfit is that
    there are very few images that are similar. That is, there are not a lot of variations
    for the model to capture within a specific class so that it can be robust and
    perform well on new data.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个场景，你可能希望在一小组图像上构建一个卷积分类器。这里的问题是，分类器很容易在这小部分数据上过拟合。分类器之所以过拟合，是因为相似的图像非常少。也就是说，模型在特定类别中捕捉到的变化不多，因此很难在新的数据上表现得既强大又精准。
- en: Keras provides a preprocessing utility called `ImageDataGenerator` that can
    be used to augment image data with simple configuration.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了一个名为`ImageDataGenerator`的预处理工具，可以通过简单的配置来增强图像数据。
- en: 'Its capabilities include the following:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 它的功能包括以下内容：
- en: '`zoom_range`: Randomly zoom in on images to a given zoom level'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zoom_range`：随机对图像进行缩放，至给定的缩放级别'
- en: '`horizontal_flip`: Randomly flip images horizontally'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`horizontal_flip`：随机水平翻转图像'
- en: '`vertical_flip`: Randomly flip images vertically'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vertical_flip`：随机垂直翻转图像'
- en: '`rescale`: Multiply the data with the factor provided'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rescale`：用提供的因子乘以数据'
- en: It also includes capabilities for random rotations, random shear, and many more.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 它还包括随机旋转、随机剪切等多种功能。
- en: Visit the official Keras documentation ([https://keras.io/preprocessing/image/](https://keras.io/preprocessing/image/))
    to learn more about some of the additional functionalities of the `image_data_generator` API.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 访问官方 Keras 文档（[https://keras.io/preprocessing/image/](https://keras.io/preprocessing/image/)）以了解更多关于
    `image_data_generator` API 的附加功能。
- en: Using ImageDataGenerator
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ImageDataGenerator
- en: The `image_data_generator` API transforms and augments the data in batches on
    the go, and is also super easy to use.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '`image_data_generator` API 会在进行过程中按批次转换和增强数据，而且使用起来非常简单。'
- en: 'First, import the `ImageDataGenerator`:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入 `ImageDataGenerator`：
- en: '[PRE56]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Implement a random horizontal flip augmenter:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 实现一个随机水平翻转增强器：
- en: '[PRE57]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Fit the augmenter on the train data:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练数据上拟合增强器：
- en: '[PRE58]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: After the fit, we usually use the `transform` command. Here, instead of `transform`,
    we have the `flow` command. It accepts the images and its corresponding labels,
    and then generates batches of transformed data of the specified batch size.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合后，我们通常使用 `transform` 命令。在这里，我们使用 `flow` 命令代替 `transform`。它接受图像及其对应标签，然后生成指定批次大小的转换数据。
- en: 'Let''s transform a bunch of images and look at the result:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们转换一批图像并查看结果：
- en: '[PRE59]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The following is the output of the preceding code:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/6a115712-ae5d-4032-9f25-b05d9ac6cae3.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a115712-ae5d-4032-9f25-b05d9ac6cae3.png)'
- en: 'Figure 8.32: Digits after horizontal flip augmentation'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.32：水平翻转增强后的数字
- en: 'Similarly, we can implement a random zoom augmenter, like so:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以实现一个随机缩放增强器，如下所示：
- en: '[PRE60]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The following is the output of the preceding code:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/ce701893-0f28-45d4-96c9-83e7cd303fdb.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce701893-0f28-45d4-96c9-83e7cd303fdb.png)'
- en: Figure 8.33: Digits after zoom augmentation
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.33：缩放增强后的数字
- en: Fitting ImageDataGenerator
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合 ImageDataGenerator
- en: Now, let's build a classifier using the same architecture as the deep convolution
    model with pooling and Dropout, but on augmented data.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们用与深度卷积模型相同的架构（带池化和 Dropout）构建一个分类器，但使用增强数据。
- en: 'First, define the features of the `ImageDataGenerator`, as follows:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，定义 `ImageDataGenerator` 的特性，如下所示：
- en: '[PRE61]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We have defined that the `ImageDataGenerator` can perform the following operations
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了 `ImageDataGenerator` 可以执行以下操作
- en: Rescaling
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重标定
- en: Random zoom
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机缩放
- en: Random horizontal flip
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机水平翻转
- en: The rescaling operation scales the pixel values to a range between 0 and 1.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 重标定操作将像素值缩放到 0 到 1 的范围内。
- en: 'The next step is to fit this generator on the train data:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将此生成器拟合到训练数据上：
- en: '[PRE62]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Compiling the model
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'We need to define and compile the deep convolution model like so:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要这样定义并编译深度卷积模型：
- en: '[PRE63]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Fitting the model
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'Finally, we need to fit the model:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要拟合模型：
- en: '[PRE64]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The following is the output of the preceding code:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/60dafac5-58d2-45dd-9a27-3f75978b2878.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60dafac5-58d2-45dd-9a27-3f75978b2878.png)'
- en: 'The following is the output at the end of the code''s execution:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 代码执行结束时的输出如下：
- en: '![](img/f5fae92c-b6f1-4ae0-addc-46b9e2630e1d.png)'
  id: totrans-371
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f5fae92c-b6f1-4ae0-addc-46b9e2630e1d.png)'
- en: Figure 8.34: Metrics printed out during the training of the deep convolution
    classifier on augmented data
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.34：深度卷积分类器在增强数据上训练过程中的打印指标
- en: Evaluating the model
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Now, we need to evaluate the model:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要评估模型：
- en: '[PRE65]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The following is the output of the preceding code:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/9d368191-8ef0-4800-912b-8cf6cfc6245a.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d368191-8ef0-4800-912b-8cf6cfc6245a.png)'
- en: 'Figure 8.35: Printout of the evaluation of the deep convolution classifier
    on augmented data'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.35：深度卷积分类器在增强数据上的评估打印输出
- en: 'Then, we need to plot the deep convolution classifier:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要绘制深度卷积分类器：
- en: '[PRE66]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The following is the output of the preceding code:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/6c796851-f809-4951-99ca-b22b888d51c7.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c796851-f809-4951-99ca-b22b888d51c7.png)'
- en: 'Figure 8.36: Loss/accuracy plot of the deep convolution classifier during training
    on augmented data'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.36：深度卷积分类器在增强数据上的训练损失/准确率图
- en: Augmentation – Python file
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 增强 – Python 文件
- en: 'This module implements the training and evaluation of a deep convolution classifier
    on augmented data:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 本模块实现了深度卷积分类器在增强数据上的训练和评估：
- en: '[PRE67]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Additional topic – convolution autoencoder
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附加主题 – 卷积自编码器
- en: 'An autoencoder is a combination of two parts: an encoder and a decoder. The
    encoder and decoder of a simple autoencoder are usually made up of dense layers,
    whereas in a convolution autoencoder, they are made of convolution layers:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是由两部分组成的：编码器和解码器。简单自编码器的编码器和解码器通常由全连接层构成，而卷积自编码器则由卷积层构成：
- en: '![](img/ca01c5ff-38df-424d-b324-035f7f964ac9.png)'
  id: totrans-389
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca01c5ff-38df-424d-b324-035f7f964ac9.png)'
- en: 'Figure 8.37: The structure of an autoencoder (image source: Wikipedia)'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.37：自编码器的结构（图像来源：维基百科）
- en: The encoder part of the autoencoder accepts an image and compresses it into
    a smaller size with the help of a pooling operation. In our case, this is max
    pooling. The decoder accepts the input of the encoder and learns to expand the
    image to our desired size by using convolution and upsampling.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器的编码器部分接受图像并通过池化操作压缩为更小的尺寸。在我们的例子中，这是最大池化。解码器接受编码器的输入，并通过使用卷积和上采样来学习扩展图像到我们想要的尺寸。
- en: 'Imagine a situation where you want to build high-resolution images out of blurred
    images:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你想从模糊的图像中构建高分辨率图像的情况：
- en: '![](img/3792613d-9ac2-4ab4-964e-678ad0ad96c5.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3792613d-9ac2-4ab4-964e-678ad0ad96c5.png)'
- en: 'Figure 8.38: Low-resolution digits on the left and high-resolution digits on
    the right'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.38：左侧是低分辨率数字，右侧是高分辨率数字。
- en: Convolution autoencoders are capable of doing this job very well. The preceding
    high-resolution digits that you can see were actually generated using convolution
    autoencoders.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积自编码器能够非常好地完成这项任务。你看到的前述高分辨率数字实际上是使用卷积自编码器生成的。
- en: By the end of this section, you will have built a convolution autoencoder that
    accepts low-resolution 14*14*1 MNIST digits and generates high-resolution 28*28*1
    digits.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 到本节结束时，你将构建一个卷积自编码器，它接受低分辨率的14*14*1 MNIST数字并生成高分辨率的28*28*1数字。
- en: Importing the dependencies
  id: totrans-397
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入依赖项
- en: 'Consider restarting your session before starting this section:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本节之前，请考虑重新启动会话：
- en: '[PRE68]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Generating low-resolution images
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成低分辨率图像
- en: 'To generate low-resolution images, define a function called `reshape()` that
    will resize the input image/digit to size `14`*`14`. After defining this, we will
    use the `reshape()` function to generate low-resolution train and test images:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成低分辨率图像，定义一个名为`reshape()`的函数，该函数将输入的图像/数字调整为`14`*`14`的大小。定义完成后，我们将使用`reshape()`函数生成低分辨率的训练和测试图像：
- en: '[PRE69]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '`XX_train` and `XX_test` will be the images that we will feed into the encoder,
    and `X_train` and `X_test` will be the targets.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '`XX_train`和`XX_test`将是我们输入到编码器中的图像，`X_train`和`X_test`将是目标。'
- en: Scaling
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩放
- en: 'Scale the train input, test input, and target images to range between 0 and
    1 so that the learning process is faster:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练输入、测试输入和目标图像缩放到0到1之间，这样学习过程会更快：
- en: '[PRE70]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Defining the autoencoder
  id: totrans-407
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义自编码器
- en: 'The convolution autoencoder we are going to build will accept 14*14*1 images
    as input with 28*28*1 images as the targets, and will have the following characteristics:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要构建的卷积自编码器将接受14*14*1的图像作为输入，28*28*1的图像作为目标，并将具有以下特点：
- en: 'In the encoder:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器中：
- en: The first layer is a convolution 2-D layer with 64 filters of size 3*3, followed
    by batch normalization, with `activation` as `relu`, followed by downsampling
    with `MaxPooling2D` of size 2*2
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层是一个卷积二维层，包含64个大小为3*3的滤波器，接着是批量归一化，`activation`为`relu`，然后是使用大小为2*2的`MaxPooling2D`进行下采样。
- en: The second layer, or the final layer in this encoder part, is again a convolution
    2-D layer with 128 filters of size 3*3, batch normalization, with `activation`
    as `relu`
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层，即编码器部分的最后一层，再次是一个卷积二维层，包含128个大小为3*3的滤波器，进行批量归一化，`activation`为`relu`。
- en: 'In the decoder:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码器中：
- en: The first layer is a convolution 2-D layer with 128 filters of size 3*3 with
    `activation` as `relu`, followed by upsampling that's performed with `UpSampling2D `
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一层是一个卷积二维层，包含128个大小为3*3的滤波器，`activation`为`relu`，接着是通过`UpSampling2D`进行上采样。
- en: The second layer is a convolution 2-D layer with 64 filters of size 3*3 with
    `activation` as `relu`, followed by upsampling with `UpSampling2D`
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二层是一个卷积二维层，包含64个大小为3*3的滤波器，`activation`为`relu`，接着是使用`UpSampling2D`进行上采样。
- en: The third layer, or the final layer in this decoder part, is again a convolution
    2-D layer with 1 filter of size 3*3 with `activation` as `sigmoid`
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三层，或解码器部分的最后一层，是一个卷积二维层，包含1个大小为3*3的滤波器，`activation`为`sigmoid`。
- en: 'The following is the code for our autoencoder:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们的自编码器代码：
- en: '[PRE71]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The following is the output of the preceding code:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/f2158296-4726-440e-8dbb-18237991eae7.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2158296-4726-440e-8dbb-18237991eae7.png)'
- en: 'Figure 8.39: Autoencoder summary'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.39：自编码器摘要
- en: We are using `mean_squared_error` as the `loss`, as we want the model to predict
    the pixel values.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`mean_squared_error`作为`loss`，因为我们希望模型预测像素值。
- en: If you take a look at the summary, the input image of size 14*14*1 is compressed
    along the width and the height dimensions to a size of 7*7, but is expanded along
    the channel dimension from 1 to 128\. These small/compressed feature maps are
    then fed to the decoder to learn the mappings that are required to generate high-resolution
    images of the defined dimension, which in this case is 28*28*1.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看摘要，输入的图像大小为14*14*1，在宽度和高度维度上被压缩到7*7的大小，但在通道维度上从1扩展到128。这些小/压缩的特征图被输入到解码器中，学习生成高分辨率图像所需的映射，这些图像的定义尺寸在此为28*28*1。
- en: If you have any questions about the usage of he Keras API, please visit the
    Keras official documentation at [https://keras.io/](https://keras.io/)[.](https://keras.io/)
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对Keras API的使用有任何疑问，请访问Keras官方文档：[https://keras.io/](https://keras.io/)。
- en: Fitting the autoencoder
  id: totrans-424
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合自编码器
- en: 'Like any regular model fit, fit the autoencoder:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 像任何常规模型拟合一样，拟合自编码器：
- en: '[PRE72]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The following is the output of the preceding code:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/aedb2da6-ebc4-4b7d-ba04-c87d3b280378.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aedb2da6-ebc4-4b7d-ba04-c87d3b280378.png)'
- en: 'The following is the output at the end of the code''s execution:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码执行结束时的输出：
- en: '![](img/19eeb2fe-1fa9-46d1-8115-f9821562ed81.png)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19eeb2fe-1fa9-46d1-8115-f9821562ed81.png)'
- en: 'Figure 8.40: Printout during the training of the autoencoder'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.40：自编码器训练过程中的打印输出
- en: You will notice that inside the fit, we have specified a parameter called `validation_split`
    and that we have set it to `0.2`. This will split the train data into train and
    validation data, with validation data having 20% of the original train data.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在拟合过程中，我们指定了一个叫做`validation_split`的参数，并将其设置为`0.2`。这会将训练数据分割为训练数据和验证数据，其中验证数据占原始训练数据的20%。
- en: Loss plot and test results
  id: totrans-433
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失图和测试结果
- en: 'Now, let''s get to plotting the train and validation loss progression during
    training. We will also plot the high-resolution image result from the model by
    feeding the test images:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制训练过程中训练和验证损失的变化曲线。同时，我们还将通过将测试图像输入模型，绘制模型生成的高分辨率图像结果：
- en: '[PRE73]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The following is the output of the preceding code:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前述代码的输出：
- en: '![](img/f0c7474d-8cf7-4c5e-915b-25802632cb27.png)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0c7474d-8cf7-4c5e-915b-25802632cb27.png)'
- en: 'Figure 8.41: Train/val loss plot'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.41：训练/验证损失图
- en: 'The following is the output of high-resolution images that have been generated
    from low-resolution images:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从低分辨率图像生成的高分辨率图像的输出：
- en: '![](img/ee3eda9b-96b0-4237-9a71-29cbe5ac921c.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee3eda9b-96b0-4237-9a71-29cbe5ac921c.png)'
- en: 'Figure 8.42: High-resolution test (28*28) images generated from low-resolution
    test (14*14) images'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.42：从低分辨率测试（14*14）图像生成的高分辨率测试（28*28）图像
- en: Autoencoder – Python file
  id: totrans-442
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器 – Python 文件
- en: 'This module implements training an autoencoder on MNIST data:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 本模块实现了在MNIST数据上训练自编码器：
- en: '[PRE74]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Conclusion
  id: totrans-445
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This project was all about building a CNN classifier to classify handwritten
    digits better than we did in [Chapter 2](https://cdp.packtpub.com/python_deep_learning_projects/wp-admin/post.php?post=31&action=edit#post_25), *Training
    NN for Prediction Using Regression,* with a multilayer Perceptron.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的目标是构建一个CNN分类器，用于比我们在[第二章](https://cdp.packtpub.com/python_deep_learning_projects/wp-admin/post.php?post=31&action=edit#post_25)中的*使用回归训练神经网络进行预测*章节中通过多层感知机分类手写数字更好的模型。
- en: Our deep convolution neural network classifier with max pooling and dropout
    hit 99.01% accuracy on a test set of 10,000 images/digits. This is good. This
    is almost 12% better than our multilayer Perceptron model.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的深度卷积神经网络分类器，使用最大池化和丢弃法，在10000张图像/数字的测试集上达到了99.01%的准确率。这是一个很好的成绩，比我们的多层感知机模型高出几乎12%。
- en: However, there are some implications. What are the implications of this accuracy?
    It is important that we understand this. Just like we did in [Chapter 2](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml), *Training
    NN for Prediction Using Regression,* let's calculate the incidence of an error
    occurring that would result in a customer service issue.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一些影响。这个准确率的含义是什么？理解这一点很重要。就像我们在[第二章](027b6171-1cf7-4589-b9a2-e417dbe53d8b.xhtml)中的*使用回归训练神经网络进行预测*一章一样，让我们计算一下导致客户服务问题的错误发生率。
- en: Just to refresh our memory, in this hypothetical use case, we assumed that the restaurant
    has an average of 30 tables at each location, and that those tables turn over
    two times per night during the rush hour when the system is likely to be used,
    and finally that the restaurant chain has 35 locations. This means that each day
    of operation, there are approximately 21,000 handwritten numbers being captured
    (30 tables x 2 turns/day x 35 locations x 10-digit phone number).
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们回忆，在这个假设的用例中，我们假设每个餐厅地点平均有30张桌子，并且这些桌子在高峰时段（系统可能会被使用时）每晚翻台两次，最后，假设该餐厅连锁有35个地点。这意味着每天运营时，大约会捕捉到21,000个手写数字（30张桌子
    x 2次翻台/天 x 35个地点 x 10位电话号码）。
- en: The ultimate goal is to classify all of the digits properly, since even a single-digit
    misclassification will result in a failure. With the classifier that we have built,
    it would improperly classify 208 digits per day. If we consider the worst case
    scenario, out of the 2,100 patrons, 208 phone numbers would be misclassified.
    That is, even in the worst case, 90.09% ((2,100-208)/2,100) of the time, we would
    be sending the text to the right patron.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 最终目标是正确分类所有数字，因为即使是一个数字的误分类也会导致失败。使用我们构建的分类器，每天会错误分类208个数字。如果我们考虑最坏的情况，在2,100个顾客中，将会有208个电话号码被误分类。也就是说，即使在最坏的情况下，我们仍然有90.09%的时间（(2,100-208)/2,100）会将短信发送给正确的顾客。
- en: The best case scenario would be that if all ten digits were misclassified in
    each phone number, we would only be improperly classifying 21 phone numbers. This
    means that we would have a failure rate of ((2,100-21)/2,100) 1%. This is as good
    as it gets.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的情况是，如果每个电话号码中的所有十个数字都被误分类，那么我们只会错误分类21个电话号码。这意味着我们的失败率为((2,100-21)/2,100)
    1%。这已经是最理想的情况了。
- en: Unless you aim at reducing that 1% error...
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你想减少那个1%的错误率……
- en: Summary
  id: totrans-453
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we understood how to implement a convolution neural network
    classifier in Keras. You now have a brief understanding of what convolution, average,
    max pooling, and dropout are, and you also built a deep model. You understood
    how to reduce overfitting as well as how to generate more/validation in data to
    build a generalizable model when you have less data than you need. Finally, we
    assessed the model's performance on test data and determined that we succeeded
    in achieving our goal. We ended this chapter by introducing you to autoencoders.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们了解了如何在Keras中实现卷积神经网络分类器。现在你对卷积、平均池化、最大池化和丢弃法有了简要的理解，并且你还构建了一个深度模型。你了解了如何减少过拟合，以及在数据量不足时，如何生成更多的训练/验证数据来构建一个可泛化的模型。最后，我们评估了模型在测试数据上的表现，并确认我们成功达到了目标。本章的结尾，我们介绍了自编码器。
