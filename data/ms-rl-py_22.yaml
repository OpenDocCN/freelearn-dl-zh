- en: '*Chapter 18*: Challenges and Future Directions in Reinforcement Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第18章*：强化学习中的挑战与未来方向'
- en: 'In this last chapter, we will summarize the end of our journey in this book:
    you have done a lot, so think of this as a celebration as well as a bird''s-eye
    view of your achievement. On the other hand, when you take your learnings to use
    **reinforcement learning** (**RL**) in real-world problems, you will likely encounter
    multiple challenges. After all, deep RL is still a fast-moving field making a
    lot of progress in solving these challenges. We have already mentioned most of
    them in the book and proposed solution approaches. We will briefly recap them
    and talk about additional future directions for RL. Finally, we will go over some
    resources and strategies for you to become an RL expert, which you are very well
    positioned for by coming this far.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章中，我们将总结我们在本书中的旅程：你已经做了很多，所以把它当作一次庆祝，也是对你成就的全景回顾。另一方面，当你将所学应用于解决实际问题时，你可能会遇到许多挑战。毕竟，深度RL仍然是一个快速发展的领域，正在取得大量进展以解决这些挑战。我们在书中已经提到了大部分挑战，并提出了解决方法。接下来，我们将简要回顾这些内容，并讨论RL的其他未来方向。最后，我们将介绍一些资源和策略，帮助你成为RL专家，你已经在这条道路上走得非常远。
- en: 'So, here is what we will cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这一章我们将讨论的内容如下：
- en: What you have achieved with this book
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在本书中取得的成就
- en: Challenges and future directions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挑战与未来方向
- en: Suggestions for aspiring RL experts
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对有志成为RL专家的建议
- en: Final words
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结束语
- en: What you have achieved with this book
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你在本书中取得的成就
- en: 'First of all, congratulations! You have come a long way beyond the fundamentals,
    acquiring the skills and the mindset to apply RL in the real world. Here is what
    we have done together so far:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，恭喜你！你已经远远超越了基础，掌握了在现实世界中应用强化学习（RL）所需的技能和心态。至此，我们已经共同完成了以下内容：
- en: We spent a fair amount of time on bandit problems, which have a tremendous number
    of applications, not just in industry but also in academia as an auxiliary tool
    to solve other problems.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们花费了相当多的时间讨论赌博问题，这些问题不仅在工业界有着广泛的应用，而且在学术界也作为辅助工具来解决其他问题。
- en: We went deeper into the theory than a typical applied book would to strengthen
    your foundations of RL.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们比一般的应用书籍更深入地探讨了理论，以加强你在RL方面的基础。
- en: We covered many of the algorithms and architectures behind the most successful
    applications of RL.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们涵盖了很多RL最成功应用背后的算法和架构。
- en: We discussed advanced training strategies to get the most out of the advanced
    RL algorithms.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们讨论了先进的训练策略，以最大化地发挥高级RL算法的效果。
- en: We did hands-on work with realistic case studies.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过现实案例进行过实际操作。
- en: Throughout this journey, we both implemented our versions of some of the algorithms,
    as well as utilized libraries, such as Ray and RLlib, which power many teams and
    platforms at the top tech companies for their RL applications.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个过程中，我们不仅实现了我们各自版本的一些算法，还利用了如Ray和RLlib等库，这些库为顶尖科技公司中的许多团队和平台提供了RL应用的技术支持。
- en: You absolutely deserve to take a moment to celebrate your success!
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你完全值得花点时间庆祝你的成功！
- en: 'Now, once you are back, it is time to talk about what is ahead of you. RL is
    at the beginning of its rise. This means multiple things:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦你回过神来，是时候讨论一下你面前的道路了。RL正处于崛起的初期阶段。这意味着多方面的含义：
- en: First, it is an opportunity. You are now ahead of the game by making this investment
    and coming this far.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，这也是一个机会。通过做出这一投资并走到今天，你已经走在了前沿。
- en: Second, since this is cutting edge, there are many challenges to be solved before
    RL becomes a mature, easy-to-use technology.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，由于这是前沿技术，RL在成为成熟、易用的技术之前，还面临着许多需要解决的挑战。
- en: 'In the next section, we will discuss what those challenges are. That way, you
    will recognize them when you see them, know that you are not alone, and can set
    your expectations accordingly in terms of what is needed (data, time, compute
    resource, and so on) to solve your problem. But you shouldn''t worry! RL is a
    very active and accelerating area of research, so our arsenal to tackle those
    challenges is getting stronger by the day. See the number of papers submitted
    to the NeurIPS conference on RL over the years, compiled and presented by Katja
    Hofmann, a prominent RL researcher, during the conference in 2019:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论这些挑战是什么。这样，当你遇到它们时，你就能识别出来，知道自己并不孤单，并且能够根据解决问题所需的资源（数据、时间、计算资源等）设定合理的期望。但你不必担心！RL是一个非常活跃且快速发展的研究领域，因此我们应对这些挑战的工具和方法日渐丰富。看看Katja
    Hofmann，这位著名的RL研究员，在2019年NeurIPS会议上整理并展示的关于RL在NeurIPS会议上提交的论文数量：
- en: '![Figure 18.1 – Number of RL contributions to the NeurIPS conference, compiled
    and presented by Katja Hofmann (source: Hofmann, 2019)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图18.1 – RL贡献者在NeurIPS会议上的数量，由Katja Hofmann整理并展示（来源：Hofmann，2019）'
- en: '](img/B14160_18_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_18_01.jpg)'
- en: 'Figure 18.1 – Number of RL contributions to the NeurIPS conference, compiled
    and presented by Katja Hofmann (source: Hofmann, 2019)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.1 – RL贡献者在NeurIPS会议上的数量，由Katja Hofmann整理并展示（来源：Hofmann，2019）
- en: Therefore, while we talk about the challenges, we also talk about the related
    research directions, so you will know where to look for the answers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们讨论这些挑战的同时，我们也会讨论相关的研究方向，这样你就能知道应该在哪里寻找答案。
- en: Challenges and future directions
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战与未来方向
- en: 'You may be wondering why we are back to talking about RL challenges after finishing
    an advanced-level book on this topic. Indeed, throughout the book, we presented
    many approaches to mitigate them. On the other hand, we cannot claim these challenges
    are solved. So, it is important to call them out and discuss future directions
    for each to provide you with a compass to navigate them. Let''s start our disccusion
    with one of the most important challenges: sample efficiency.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么在完成一本高级强化学习书籍后，我们又回到讨论RL挑战的问题。确实，在全书中，我们提出了许多缓解这些挑战的方法。另一方面，我们不能声称这些挑战已经被解决。因此，指出这些挑战并讨论每个挑战的未来发展方向是非常重要的，这样可以为你提供一把指南针，帮助你应对这些问题。让我们从其中一个最重要的挑战开始讨论：样本效率。
- en: Sample efficiency
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 样本效率
- en: As you are now well aware, it takes a lot of data to train an RL model. OpenAI
    Five, which became a world-class player in the strategy game Dota 2, took 128,000
    CPUs and GPUs to train, over many months, collecting a total of 900 years' worth
    of game experience **per day** (OpenAI, 2018). RL algorithms are benchmarked on
    their performances after they are trained on over 10 billion Atari frames (Kapturowski,
    2019). This is certainly a lot of compute and resources just to play games. So,
    sample efficiency is one of the biggest challenges that face real-world RL applications.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你现在已经清楚地知道，训练一个强化学习（RL）模型需要大量的数据。OpenAI Five，这个在战略游戏《Dota 2》中成为世界级玩家的AI，训练过程中使用了128,000个CPU和GPU，持续了好几个月，每天收集相当于900年的游戏经验**每一天**（OpenAI，2018）。RL算法的性能通常是在它们已经训练过超过100亿帧Atari游戏画面后进行基准测试的（Kapturowski，2019）。这无疑需要大量的计算和资源，仅仅是为了玩游戏。所以，样本效率是现实世界RL应用中面临的最大挑战之一。
- en: Let's discuss the overall directions to mitigate this problem.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下缓解这个问题的总体方向。
- en: Sample-efficient algorithms
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 样本高效算法
- en: An obvious direction is to try to create algorithms that are more sample efficient.
    Indeed, there is a big push in the research community to this end. We will increasingly
    compare the algorithms not just based on their best possible performance but also
    on how quickly and efficiently they reach those performance levels.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一个显而易见的方向是尝试创建更加样本高效的算法。实际上，研究界对此方向有很大的推动。我们将越来越多地比较算法，不仅仅是基于它们的最佳性能，还会评估它们在达到这些性能水平时的速度和效率。
- en: 'To this end, we will likely talk more and more about the following algorithm
    classes:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将可能越来越多地讨论以下几种算法类别：
- en: '**Off-policy methods**, which don''t require data to be collected under the
    most recent policy, giving them an edge over policy methods in terms of sample
    efficiency.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**脱策略方法**，不要求数据必须是在最新策略下收集的，这使它们在样本效率上相较于策略方法具有优势。'
- en: '**Model-based methods**, which can be orders of magnitude more efficient than
    their model-free counterparts by leveraging the information they possess on environment
    dynamics.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于模型的方法**，通过利用它们对环境动态的了解，比起无模型方法能在效率上高出几个数量级。'
- en: '**Models with informed priors**, which limit the hypothesis space of the models
    to a plausible set. Examples of this class use neural ordinary differential equations
    and Lagrangian neural networks in RL models (Du, 2020; Shen, 2020).'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有先验知识的模型**，这些模型将假设空间限制为一个合理的集合。这类模型的例子包括使用神经常微分方程和拉格朗日神经网络的RL模型（Du, 2020；Shen,
    2020）。'
- en: Specialized hardware and software architectures for distributed training
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于分布式训练的专用硬件和软件架构
- en: 'We can expect the progress on the algorithmic frontier to be gradual and slow.
    For those of us who are excited and impatient enthusiasts, a quicker solution
    is to dump more compute resources into RL projects, get the most out of the existing
    ones. and train bigger and bigger models. So, it is only reasonable to expect
    what happened in the **natural language processing** (**NLP**) space to happen
    to RL too: NLP models went from being 8billion-parameter models to 17billion,
    then to 175billion in size with OpenAI''s GPT-3 in less than a year, thanks to
    the optimizations in training architectures and, of course, the supercomputers
    dedicated to the task:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以预期，算法前沿的进展将是渐进且缓慢的。对于那些充满激情、迫不及待的爱好者来说，更快速的解决方案是向RL项目投入更多的计算资源，充分利用现有资源，训练越来越大的模型。因此，完全合理地预期，在**自然语言处理**（**NLP**）领域发生的事情也会发生在RL中：NLP模型的规模从80亿参数增加到170亿参数，再到1750亿参数，OpenAI的GPT-3在不到一年的时间内达到了这一规模，这要归功于训练架构的优化，当然还有专门用于此任务的超级计算机。
- en: '![Figure 18.2 – Growth in the biggest NLP model sizes. The vertical axis is
    the number of parameters. Image modified from Rosset, 2020'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 18.2 – 最大的NLP模型规模的增长。纵轴是参数的数量。图像修改自Rosset, 2020'
- en: '](img/B14160_18_02.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_18_02.jpg)'
- en: Figure 18.2 – Growth in the biggest NLP model sizes. The vertical axis is the
    number of parameters. Image modified from Rosset, 2020
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.2 – 最大的NLP模型规模的增长。纵轴是参数的数量。图像修改自Rosset, 2020
- en: In addition, innovations in RL training architectures, such as in Ape-X and
    SEED RL (*Espeholt, 2019*), help existing RL algorithms to run more efficiently,
    a direction we can expect to see more progress in.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RL训练架构的创新，如Ape-X和SEED RL中的创新（*Espeholt, 2019*），帮助现有的RL算法运行得更加高效，这是一个我们可以期待看到更多进展的方向。
- en: Machine teaching
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器教学
- en: Machine teaching approaches, such as curriculum learning, reward shaping, and
    demonstration learning, aim to infuse context and expertise into RL training.
    They often lead to significant sample efficiency during training and in some cases
    are needed to make learning even more feasible. Machine teaching approaches will
    become increasingly popular in the near future to increase the sample efficiency
    in RL.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 机器教学方法，如课程学习、奖励塑形和示范学习，旨在将上下文和专业知识融入到RL训练中。这些方法通常在训练过程中显著提高样本效率，在某些情况下，它们是让学习变得更加可行所必需的。机器教学方法将在不久的将来变得越来越流行，以提高RL中的样本效率。
- en: Multi-task/meta/transfer learning
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多任务/元学习/迁移学习
- en: Since training an RL model from scratch could be very expensive, it only makes
    sense to reuse the models that are trained on other relevant tasks. Today, when
    we want to develop an application that involves image classification, for example,
    it is rare that we train a model from scratch. Instead, we use one of the pre-trained
    models and fine-tune it for our application.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于从头开始训练一个RL模型可能非常昂贵，因此复用在其他相关任务上训练过的模型才是明智的做法。例如，当我们想开发一个涉及图像分类的应用时，现在很少有情况下我们从头开始训练模型。相反，我们会使用一个预训练的模型，并根据我们的应用需求进行微调。
- en: Info
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'The ONNX Model Zoo is a collection of pre-trained, state-of-the-art models
    in an open-standard format for popular tasks such as image classification and
    machine translation, which I highly recommend you take a look at: [https://github.com/onnx/models](https://github.com/onnx/models).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX模型库是一个预训练的、先进的模型集合，采用开放标准格式，适用于图像分类和机器翻译等流行任务，我强烈建议你查看一下：[https://github.com/onnx/models](https://github.com/onnx/models)。
- en: We can expect to see similar repositories for RL tasks as well. On a relevant
    note, approaches such as multi-task learning, which is about training models for
    more than one task, and meta-learning, which is about training models that can
    be efficiently transferred to new tasks, will gain momentum and broader adaptation
    among RL researchers and practitioners.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以预期在RL任务中也会看到类似的资源库。在相关的背景下，像多任务学习（训练多个任务的模型）和元学习（训练能够高效转移到新任务的模型）等方法，将在RL研究人员和实践者中获得更多的关注和更广泛的应用。
- en: RL-as-a-service
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RL即服务
- en: If you need to translate texts in your app programmatically, one approach is,
    as we mentioned, to use a pre-trained model. But what if you don't want to invest
    in maintaining these models? The answer is often to buy it as a service from companies
    such as Microsoft, Google, and Amazon. These companies have access to huge amounts
    of data and compute resources, and they constantly upgrade their models using
    cutting-edge machine learning approaches. Doing the same could be a daunting,
    even infeasible, or simply impractical undertaking for other businesses without
    the same resources and focus. We can expect to see RL-as-a-service trend in the
    industry.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要在应用程序中以编程方式翻译文本，正如我们之前提到的，一种方法是使用预训练模型。但是，如果你不想投入精力去维护这些模型呢？答案通常是从微软、谷歌、亚马逊等公司购买该服务。这些公司拥有大量的数据和计算资源，并且他们通过尖端的机器学习方法不断升级自己的模型。对于其他没有相同资源和聚焦的企业来说，做到同样的事情可能是一个艰巨的任务，甚至是不可行的，或者根本就是不实际的。我们可以预见到，RL即服务（RL-as-a-service）将成为行业中的一种趋势。
- en: 'Sample efficiency is a tough nut to crack, but there are developments in multiple
    frontiers to achieve it, as we have summarized. Next, let''s talk about another
    major challenge: the need for good simulation models.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 样本效率（sample efficiency）是一个难题，但在多个领域已经有了进展，正如我们所总结的那样。接下来，我们将讨论另一个主要挑战：对良好仿真模型的需求。
- en: Need for high-fidelity and fast simulation models
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对高精度和快速仿真模型的需求
- en: One of the biggest barriers in the way of the broad adaptation of RL in industry
    is the absence of simulations of processes that companies are interested in optimizing,
    either altogether or at sufficient fidelity. Creating these simulation models
    often requires a lot of investment, and in some complex tasks, the fidelity of
    a simulation model won't be high enough for a typical RL approach. To overcome
    these challenges, we can expect to see more developments in the following areas.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习在工业中广泛应用的最大障碍之一是缺乏能够有效仿真公司希望优化的过程的模型，或者这些模型的逼真度不足以达到要求。创建这些仿真模型通常需要大量投资，在一些复杂任务中，仿真模型的逼真度可能不足以支撑典型的强化学习方法。为了克服这些挑战，我们可以预见到在以下几个领域会有更多的进展。
- en: Offline RL to learn directly from data
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 离线强化学习从数据中直接学习
- en: Although most processes in industry don't have simulation models, it is much
    more common to have logs describing what happened in it. Offline RL approaches,
    which aim to learn policies directly from data, will become of higher and higher
    importance as RL finds its way into real-world applications.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管工业中的大多数过程没有仿真模型，但更常见的是会有日志记录描述过程中的发生事件。离线强化学习（Offline RL）方法旨在直接从数据中学习策略，随着强化学习逐渐进入现实世界应用，这种方法的重要性将变得越来越大。
- en: Approaches to deal with generalization, partial observability, and non-stationarity
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理泛化、部分可观测性和非平稳性的方法
- en: Even in cases where a simulation model of a process exists, it is quite rare
    for such a model to be of enough fidelity to naively train an RL model that would
    work in the real-world without additional considerations. This sim2real gap can
    be thought of as a form of partial observability, which is often handled through
    memory in the RL model architecture. Combined with generalization techniques,
    such as domain randomization and regularization, we are already seeing very successful
    applications where policies trained in a simulation are transferred to the real
    world. Dealing with non-stationarity is also closely related to the generalization
    capabilities of RL algorithms.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在存在过程仿真模型的情况下，这种模型通常也很难达到足够的逼真度，无法直接训练一个可以在现实世界中工作的强化学习模型，而不需要额外的考虑。这个“仿真到现实”的差距可以看作是部分可观测性的一种形式，通常通过强化学习模型架构中的记忆机制来处理。结合领域随机化和正则化等泛化技术，我们已经看到了一些非常成功的应用场景，其中在仿真中训练的策略被转移到现实世界。处理非平稳性（non-stationarity）问题也与强化学习算法的泛化能力密切相关。
- en: Online learning and fine-tuning on the edge
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在线学习和边缘设备上的微调
- en: One of the important capabilities that will enable the successful use of RL
    approaches will be to be able to continue training after the deployment of a model
    on the edge. With that, we will be able to fine-tune the models that are trained
    on a simulation with actual data. In addition, this capability will help RL policies
    to adapt to the changing conditions in the environment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使强化学习方法成功应用的一个重要能力是，在将模型部署到边缘设备后仍能继续进行训练。这样，我们将能够用实际数据对在仿真中训练的模型进行微调。此外，这一能力将帮助强化学习策略适应环境中变化的条件。
- en: To summarize, we will witness a shift from RL being a tool to use in video games
    to an alternative to the traditional control and decision-making methods, which
    will be facilitated by approaches that will remove the dependency on high-fidelity
    simulations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们将看到强化学习（RL）从视频游戏中的工具转变为传统控制和决策方法的替代方案，这一转变将通过去除对高保真模拟的依赖的方式得到促进。
- en: High-dimensional action spaces
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高维动作空间
- en: CartPole, the iconic testbed for RL, has only a few elements in its action space,
    like in most RL environments used in RL research. Real-life problems, however,
    can be quite complex in terms of the number of available actions to the agent,
    which also often depends on the state the agent is in. Approaches such as action
    embeddings, action masking, and action elimination will come in handy to tackle
    this challenge in such realistic scenarios.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: CartPole，作为RL的标志性测试平台，其动作空间只有少数几个元素，就像大多数用于RL研究的RL环境一样。然而，现实问题通常在可用动作数量上非常复杂，这也常常取决于智能体所处的状态。在这种现实场景中，像动作嵌入、动作屏蔽和动作淘汰等方法将帮助应对这一挑战。
- en: Reward function fidelity
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励函数的保真度
- en: Crafting the right reward function that leads the agent to the desired behavior
    is a notoriously difficult undertaking in RL. Inverse RL approaches, which learn
    the reward from demonstrations, and curiosity-driven learning, which relies on
    intrinsic rewards, are promising methods to reduce the dependency on hand-engineering
    the reward function.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 制定正确的奖励函数，使智能体实现期望行为，是RL中公认的困难任务。逆向RL方法（通过示范学习奖励）和基于好奇心的学习（依赖内在奖励）是有前景的减少手工设计奖励函数依赖性的方法。
- en: The challenge with reward function engineering is exacerbated when there are
    multiple and qualitative objectives. There is growing literature on multi-objective
    RL approaches that either deal with each subproblem individually or produce policies
    for a given mixture of objectives.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数工程的挑战在目标多样且具有定性时更加复杂。关于多目标RL方法的文献越来越多，这些方法要么分别处理每个子问题，要么为给定的目标组合生成策略。
- en: Another challenge with respect to reward signals in RL environments is the delay
    and sparsity of rewards. For example, a marketing strategy controlled by an RL
    agent might observe rewards days, weeks, or even months after the action is taken.
    Approaches that deal with causality and credit assignment in RL are critical to
    be able to leverage RL in these environments.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 关于RL环境中奖励信号的另一个挑战是奖励的延迟和稀疏性。例如，一个由RL智能体控制的营销策略可能会在行动后几天、几周甚至几个月才观察到奖励。解决因果关系和信用分配问题的RL方法在这些环境中至关重要，能够让RL在这些环境中得以有效应用。
- en: These are all important branches to keep an eye on since real-world RL problems
    rarely have well-defined, dense, and scalar objectives.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是值得关注的重要分支，因为现实世界中的RL问题很少具有明确、密集且标量的目标。
- en: Safety, behavior guarantees, and explainability
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全性、行为保证和可解释性
- en: When training RL agents in a computer simulation, it is okay, in fact, needed,
    to try random and crazy actions to figure out better policies. For an RL model
    competing against world-class players in a board game, the worst scenario that
    can happen is to lose the game, perhaps embarrassingly. The risks are of a different
    category when an RL agent is in charge of a chemical process or a self-driving
    car. These are safety-critical systems where the room for error is little to none.
    In fact, this is one of the biggest disadvantages of RL methods compared to traditional
    control theory approaches that often come with theoretical guarantees and a solid
    understanding of the expected behavior. Research on constrained RL and safe exploration
    is, therefore, crucial to be able to use RL in such systems.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机仿真中训练RL智能体时，尝试随机和疯狂的动作以找到更好的策略是完全可以接受的，事实上，这是必须的。对于与世界级选手在棋盘游戏中竞争的RL模型来说，最糟糕的情况就是输掉比赛，或许是有些尴尬。当RL智能体负责化学过程或自动驾驶汽车时，风险则是完全不同的类别。这些都是安全关键系统，容错空间几乎为零。事实上，这是RL方法相比于传统控制理论方法的一大劣势，后者通常伴随着理论保证和对预期行为的深刻理解。因此，约束RL和安全探索的研究对于在此类系统中使用RL至关重要。
- en: Even when the system is not safety-critical, such as in inventory replenishment
    problems, a related challenge is the explainability of the actions suggested by
    the RL agent in charge. Experts who oversee the decisions in these processes often
    demand explanations, especially when the suggested actions are counterintuitive.
    People tend to trade accuracy for explanation, which puts black-box approaches
    at a disadvantage. Deep learning has come a long way in terms of explainability,
    and RL will surely benefit from it. On the other hand, this will be an ongoing
    challenge for machine learning at large.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 即便系统并非安全至关重要的场景，例如在库存补充问题中，相关的挑战仍然是RL智能体所建议的行动的可解释性。在这些过程中，监督决策的专家往往需要解释，尤其是当建议的行动违背直觉时。人们往往会为了可解释性而牺牲准确性，这使得黑箱方法处于不利地位。深度学习在可解释性方面已经取得了长足的进展，而RL无疑会从中受益。另一方面，这将是机器学习面临的持续挑战。
- en: Reproducibility and sensitivity to hyperparameter choices
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 再现性和对超参数选择的敏感性
- en: It is one thing to train an RL model with the close oversight and guidance of
    many experts for a specific task and after many iterations, yet it is another
    thing to deploy multiple RL models in production for various environments, which
    are to be re-trained periodically and hands-off-the-wheel as new data come in.
    The consistency and resiliency of RL algorithms in terms of producing successful
    policies under a variety of conditions will be an increasingly important factor
    when benchmarking them for the research community, as well as for the practitioners
    who will get to deal with these models and their maintenance in real life.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个特定任务，通过众多专家的密切监督和指导训练一个RL模型，经过多次迭代，这是一回事；而在多个生产环境中部署多个RL模型，并定期进行再训练，同时在新数据到来时不需人工干预，这又是另一回事。在各种条件下，RL算法生成成功策略的一致性和韧性将成为评估这些算法时越来越重要的因素，既适用于研究社区，也适用于实际操作中需要处理这些模型及其维护的从业者。
- en: Robustness and adversarial agents
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 鲁棒性与对抗性智能体
- en: Deep learning is known to be brittle about its representations. This lack of
    robustness allows adversarial agents to manipulate systems that rely on deep learning.
    This is a major concern and a very active area of research in the machine learning
    community. RL will surely piggyback on the developments in the broader machine
    learning research to address robustness issues in this field.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在其表示方面是脆弱的。这种缺乏鲁棒性使得对抗性智能体能够操控依赖深度学习的系统。这是一个重大问题，也是机器学习社区中非常活跃的研究领域。强化学习（RL）肯定会借助广泛的机器学习研究成果，解决该领域的鲁棒性问题。
- en: These challenges in RL are important to be aware of, especially for practitioners
    who want to use these tools to solve real-world problems, and this recap will
    hopefully help with that. We covered many of the solution approaches in the book
    and mentioned the overall directions in this section as well, so you know where
    to look for solutions. All of these are active areas of research, so whenever
    you encounter these challenges, it is a good idea to take a fresh look at the
    literature.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些RL挑战对于希望使用这些工具解决现实世界问题的从业者来说非常重要，且这一总结希望能够帮助到你。我们在书中覆盖了许多解决方案方法，并在本节中也提到了整体的方向，所以你知道该从哪里寻找解决方案。所有这些都是活跃的研究领域，因此，每当你遇到这些挑战时，回顾文献是一个很好的做法。
- en: Before we wrap up, I would like to offer my two cents to aspiring RL experts.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束之前，我想对有志成为RL专家的人提供一些个人看法。
- en: Suggestions for aspiring RL experts
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对有志成为RL专家的建议
- en: This book is designed for an audience who already know the fundamentals of RL.
    Now that you have finished this book too, you are well-positioned to be an expert
    in RL. Having said that, RL is a big topic, and this book is really meant to be
    a compass and kickstarter for you. At this point, if you decide to go deeper to
    become an RL expert, I have some suggestions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本书面向的读者群体是那些已经掌握了强化学习基础的人。既然你已经读完了这本书，你已经具备了成为RL专家的基础条件。话虽如此，RL是一个庞大的话题，本书的真正目的是为你提供一份指南和启动器。如果你决定深入研究，成为一名RL专家，我有一些建议。
- en: Go deeper into the theory
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解理论
- en: 'In machine learning, models often fail to produce the expected level of results,
    at least at the beginning. One big factor that will help you pass these obstacles
    is to have a good foundation for the math behind the algorithms you are using
    to solve your problems. This will help you better understand the limitations and
    assumptions of those algorithms and identify whether they conflict with the realities
    of the problem at hand. To this end, here is some advice:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，模型往往无法在一开始就产生预期的结果。有一个重要因素可以帮助你克服这些障碍，那就是为你用来解决问题的算法背后的数学打下坚实的基础。这将帮助你更好地理解这些算法的局限性和假设，并能识别它们是否与当前问题的实际情况冲突。为此，以下是一些建议：
- en: It is never a bad idea to deepen your understanding of probability and statistics.
    Don't forget that all these algorithms are essentially statistical models.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入理解概率论和统计学永远不是坏主意。别忘了，所有这些算法本质上都是统计模型。
- en: 'A solid understanding of the basic ideas in RL, such as Q-learning and the
    Bellman equation, is critical to have a good foundation to build modern RL on.
    This book serves this purpose to some extent. However, I highly recommend you
    read, multiple times, Rich Sutton and Andrew Barto''s book *Reinforcement Learning:
    An Introduction*, which is essentially the Bible of traditional RL.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '对强化学习的基本概念有扎实的理解，如 Q-learning 和贝尔曼方程，对于构建现代强化学习是至关重要的。本书在某种程度上服务于这个目的。然而，我强烈推荐你多次阅读
    Rich Sutton 和 Andrew Barto 的书籍 *Reinforcement Learning: An Introduction*，这本书基本上是传统强化学习的圣经。'
- en: Professor Sergey Levine's UC Berkeley course on deep RL, which this book benefited
    greatly from, is an excellent resource to go deeper into RL. This course is available
    at [http://rail.eecs.berkeley.edu/deeprlcourse/](http://rail.eecs.berkeley.edu/deeprlcourse/).
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赛尔吉·莱文教授的 UC Berkeley 深度强化学习课程，是本书在此领域受益匪浅的一个重要资源。这个课程可以通过 [http://rail.eecs.berkeley.edu/deeprlcourse/](http://rail.eecs.berkeley.edu/deeprlcourse/)
    访问。
- en: Another great resource, specific to multi-tasking and meta-learning, is Professor
    Chelsea Finn's Stanford course at [https://cs330.stanford.edu/](https://cs330.stanford.edu/).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个很棒的资源，专门涉及多任务学习和元学习，是切尔西·芬恩教授的斯坦福课程，网址是 [https://cs330.stanford.edu/](https://cs330.stanford.edu/)。
- en: 'The Deep RL Bootcamp taught by the experts in the field is another excellent
    resource: [https://sites.google.com/view/deep-rl-bootcamp/home](https://sites.google.com/view/deep-rl-bootcamp/home).'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由该领域专家教授的 Deep RL Bootcamp 是另一个优秀的资源：[https://sites.google.com/view/deep-rl-bootcamp/home](https://sites.google.com/view/deep-rl-bootcamp/home)。
- en: As you go through these resources, and refer back to them from time to time,
    you will notice your understanding of the topic becomes much deeper.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当你浏览这些资源，并不时参考它们时，你会发现自己对该主题的理解变得更加深入。
- en: Follow good practitioners and research labs
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关注优秀的从业者和研究实验室
- en: 'There are excellent research labs focusing on RL, who also publish their findings
    in detailed blog posts that contain a lot of theoretical and practical insights.
    Here are some examples:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些优秀的研究实验室专注于强化学习（RL），并且他们也通过详细的博客文章发布他们的研究成果，这些文章包含了很多理论和实践的见解。以下是一些例子：
- en: 'OpenAI blog: [https://openai.com/blog/](https://openai.com/blog/)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI 博客: [https://openai.com/blog/](https://openai.com/blog/)'
- en: 'DeepMind blog: [https://deepmind.com/blog](https://deepmind.com/blog)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DeepMind 博客: [https://deepmind.com/blog](https://deepmind.com/blog)'
- en: '**Berkeley AI Research** (**BAIR**) blog: [https://bair.berkeley.edu/blog](https://bair.berkeley.edu/blog)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伯克利人工智能研究**（**BAIR**）博客: [https://bair.berkeley.edu/blog](https://bair.berkeley.edu/blog)'
- en: 'Microsoft Research RL group: [https://www.microsoft.com/en-us/research/theme/reinforcement-learning-group/](https://www.microsoft.com/en-us/research/theme/reinforcement-learning-group/)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '微软研究院强化学习小组: [https://www.microsoft.com/en-us/research/theme/reinforcement-learning-group/](https://www.microsoft.com/en-us/research/theme/reinforcement-learning-group/)'
- en: 'Google AI blog: [https://ai.googleblog.com/](https://ai.googleblog.com/)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Google AI 博客: [https://ai.googleblog.com/](https://ai.googleblog.com/)'
- en: Even if you don't read every single post, it is a good idea to monitor them
    regularly to stay synced with the trends of RL research.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你不阅读每一篇帖子，定期查看它们也是个好主意，这样可以保持与强化学习研究趋势同步。
- en: Learn from papers and their good explanations
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从论文和它们的优秀解释中学习
- en: 'A single year in AI research is like a year in dog years: a lot happens in
    it. So, the best way to stay up to date is really to follow the research in the
    area. This will also expose you to the theory and rigorous explanations of the
    methods. Now, there are two challenges that come with this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能（AI）研究中，一年就像是狗年：发生了很多事。所以，保持最新的最好的方式就是关注该领域的研究。这也会让你接触到方法的理论和严格的解释。现在，这样做有两个挑战：
- en: There are a ton of papers published every year, which makes it impossible to
    read them all.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每年发布的论文数量庞大，几乎不可能阅读所有论文。
- en: It could be daunting to read equations and proofs.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读公式和证明可能会让人感到望而生畏。
- en: To address the former, I suggest you focus on papers accepted to conferences
    such as NeurIPS, ICLR, ICML, and AAAI. This will still amount to a lot, so you
    will still have to develop your own thresholds about what to read.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决前者，我建议你关注像NeurIPS、ICLR、ICML和AAAI等会议上接收的论文。这仍然会是一大堆论文，因此你需要制定自己的筛选标准，决定哪些论文值得阅读。
- en: 'To address the latter, you can check whether there are good blog posts explaining
    the papers you would like to understand better. Some high-quality blogs (not specific
    to RL) to follow are the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决后者，你可以查看是否有优秀的博客文章解释你想更好理解的论文。以下是一些高质量的博客（不限于RL）：
- en: 'Lilian Weng''s blog: [https://lilianweng.github.io/lil-log/](https://lilianweng.github.io/lil-log/)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lilian Weng的博客: [https://lilianweng.github.io/lil-log/](https://lilianweng.github.io/lil-log/)'
- en: 'Distill: [https://distill.pub/](https://distill.pub/)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Distill: [https://distill.pub/](https://distill.pub/)'
- en: 'The Gradient: [https://thegradient.pub/](https://thegradient.pub/)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'The Gradient: [https://thegradient.pub/](https://thegradient.pub/)'
- en: 'Adrian Colyer''s The Morning Paper: [https://blog.acolyer.org/](https://blog.acolyer.org/)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Adrian Colyer的《晨间论文》: [https://blog.acolyer.org/](https://blog.acolyer.org/)'
- en: 'Jay Alammar''s blog: [http://jalammar.github.io/](http://jalammar.github.io/)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jay Alammar的博客: [http://jalammar.github.io/](http://jalammar.github.io/)'
- en: 'Christopher Olah''s blog (who is also in the Distill team): [https://colah.github.io/](https://colah.github.io/)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Christopher Olah的博客（他也是Distill团队的一员）: [https://colah.github.io/](https://colah.github.io/)'
- en: 'Jian Zhang''s blog: [https://medium.com/@jianzhang_23841](mailto:https://medium.com/@jianzhang_23841)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jian Zhang的博客: [https://medium.com/@jianzhang_23841](mailto:https://medium.com/@jianzhang_23841)'
- en: I have personally learned a lot from these blogs, and still continue to learn
    from them.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我个人从这些博客中学到了很多，并且仍然在持续学习。
- en: Stay up to date with trends in other fields of deep learning
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关注深度学习其他领域的趋势
- en: Most major developments in deep learning, such as the Transformer architecture,
    take only months to find their way into RL. Therefore, staying up to date with
    major trends in the broader machine learning and deep learning research will help
    you predict what is upcoming for RL. The blogs we listed in the previous section
    are a good way to follow these trends.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的许多重大进展，如Transformer架构，仅需几个月时间就能进入强化学习（RL）领域。因此，保持对广泛的机器学习和深度学习研究中的主要趋势的关注，将帮助你预测RL领域即将到来的发展。我们在前一部分列出的博客是跟踪这些趋势的好方法。
- en: Read open source repos
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阅读开源代码库
- en: 'At this point, there are just too many algorithms in RL to explain line by
    line in a book. So, at some point, you need to develop that literacy and directly
    read good implementations of these algorithms. Here are my suggestions:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，强化学习中的算法实在是太多了，无法在书中逐行解释。因此，你需要在某个阶段培养这种素养，直接阅读这些算法的优质实现。以下是我的建议：
- en: The OpenAI Spinning Up website, [https://spinningup.openai.com/](https://spinningup.openai.com/),
    and repo, [https://github.com/openai/spinningup](https://github.com/openai/spinningup)
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI Spinning Up网站: [https://spinningup.openai.com/](https://spinningup.openai.com/)，以及代码库:
    [https://github.com/openai/spinningup](https://github.com/openai/spinningup)'
- en: 'OpenAI Baselines: [https://github.com/openai/baselines](https://github.com/openai/baselines)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI Baselines: [https://github.com/openai/baselines](https://github.com/openai/baselines)'
- en: 'Stable Baselines: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Stable Baselines: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)'
- en: 'DeepMind OpenSpiel: [https://github.com/deepmind/open_spiel](https://github.com/deepmind/open_spiel)'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DeepMind OpenSpiel: [https://github.com/deepmind/open_spiel](https://github.com/deepmind/open_spiel)'
- en: 'Ray and RLlib: [https://github.com/ray-project/ray](https://github.com/ray-project/ray)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ray和RLlib: [https://github.com/ray-project/ray](https://github.com/ray-project/ray)'
- en: 'The *Hands-On* *Deep Reinforcement Learning* book repo by Maxim Lapan: [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Maxim Lapan的《动手深度强化学习》书籍代码库: [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On)'
- en: In addition to these, many papers now come with their own repos, as we used
    in some chapters in this book. There is a very nice website, [https://paperswithcode.com/](https://paperswithcode.com/),
    which you can use to identify such papers.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，许多论文现在也有自己的代码库，就像我们在本书的某些章节中使用过的那样。有一个非常好的网站，[https://paperswithcode.com/](https://paperswithcode.com/)，你可以用来查找这类论文。
- en: Practice!
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实践！
- en: Regardless of how much you read, you will truly learn only by practicing. So,
    try to get your hands dirty wherever possible. This could be through reproducing
    RL papers and algorithms, or even better, doing your own RL projects. The benefit
    you will get by going into the guts of an implementation cannot be replaced by
    anything else.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你读了多少内容，真正的学习只有通过实践才能实现。所以，尽可能亲自动手去做。你可以通过复现强化学习论文和算法，或者更好的是，进行自己的强化学习项目。深入理解实现的内部细节所带来的益处是任何其他方式都无法替代的。
- en: I hope this set of resources is helpful to you. To be clear, this is a lot to
    consume. It will take time to go over these, so set your targets realistically.
    Moreover, you will have to be selective about what to read and follow, a habit
    that you will develop over time.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这套资源对你有帮助。需要明确的是，这些内容量很大，消化这些信息需要时间，因此请设定实际的目标。此外，你还需要在阅读和跟随内容时有所选择，这个习惯会随着时间的推移而逐步养成。
- en: Final words
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最后的话
- en: Well, it is time to wrap up. I would like to thank you for investing your time
    and effort into reading this book. I hope it has been beneficial for you. As a
    last word, I would like to emphasize that getting good at something takes a long
    time and there is no limit to how good you can become. Nobody is an expert at
    everything, even for subdisciplines such as RL or computer vision. you need to
    run. Consistency and continuity of your efforts will make the difference, no matter
    what your goal is. I wish you the best on this journey.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，是时候总结了。感谢你投入时间和精力阅读这本书。希望它对你有所帮助。最后我想强调的是，掌握一项技能需要很长时间，而且你能达到的水平是没有限制的。没有人是所有领域的专家，即使是强化学习或计算机视觉这样的小领域也是如此。你需要不断实践。无论你的目标是什么，持续性和一致性将决定你的成功。祝你在这条路上一路顺利。
- en: References
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Hofmann, K. (2019). *Reinforcement Learning: Past, Present, and Future Perspectives*.
    Conference on Neural Information Processing Systems, Vancouver, Canada. URL: [https://slideslive.com/38922817/reinforcement-learning-past-present-and-future-perspectives](https://slideslive.com/38922817/reinforcement-learning-past-present-and-future-perspectives)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hofmann, K. (2019). *强化学习：过去、现在和未来的视角*。神经信息处理系统会议，温哥华，加拿大。网址：[https://slideslive.com/38922817/reinforcement-learning-past-present-and-future-perspectives](https://slideslive.com/38922817/reinforcement-learning-past-present-and-future-perspectives)
- en: 'OpenAI (2018). *OpenAI Five*. OpenAI Blog. URL: [https://openai.com/blog/openai-five/](https://openai.com/blog/openai-five/)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2018). *OpenAI Five*。OpenAI博客。网址：[https://openai.com/blog/openai-five/](https://openai.com/blog/openai-five/)
- en: Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J., & Munos R. (2019). *Recurrent
    Experience Replay in Distributed Reinforcement Learning*. In International Conference
    on Learning Representations
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J., & Munos R. (2019). *分布式强化学习中的递归经验回放*。国际学习表征大会
- en: 'Espeholt, L., Marinier, R., Stanczyk, P., Wang, K., & Michalski, M. (2019).
    *SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference*.
    arXiv.org, [http://arxiv.org/abs/1910.06591](http://arxiv.org/abs/1910.06591)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Espeholt, L., Marinier, R., Stanczyk, P., Wang, K., & Michalski, M. (2019).
    *SEED RL：具有加速中央推理的可扩展高效深度强化学习*。arXiv.org，[http://arxiv.org/abs/1910.06591](http://arxiv.org/abs/1910.06591)
- en: Du, J., Futoma, J., & Doshi-Velez, F. (2020). *Model-based Reinforcement Learning
    for Semi-Markov Decision Processes with Neural ODEs*. arXiv.org, [https://arxiv.org/abs/2006.16210](https://arxiv.org/abs/2006.16210)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du, J., Futoma, J., & Doshi-Velez, F. (2020). *基于模型的强化学习在神经ODE下的半马尔可夫决策过程中的应用*。arXiv.org，
    [https://arxiv.org/abs/2006.16210](https://arxiv.org/abs/2006.16210)
- en: 'Shen, P. (2020). *Neural ODE for Differentiable Reinforcement Learning and
    Optimal Control: Cartpole Problem Revisited*. The startup. URL: [https://bit.ly/2RROQi3](https://bit.ly/2RROQi3)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen, P. (2020). *神经ODE在可微分强化学习与最优控制中的应用：Cartpole问题再探*。The startup。网址：[https://bit.ly/2RROQi3](https://bit.ly/2RROQi3)
- en: 'Rosset, C. (2020). *Turing-NLG: A 17-billion-parameter language model by Microsoft*.
    Microsoft Research blog. URL: [https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosset, C. (2020). *Turing-NLG：微软的17亿参数语言模型*。微软研究博客。网址：[https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)
- en: Dulac-Arnold, G., et al. (2020). *An Empirical Investigation of the Challenges
    of Real-World Reinforcement Learning*. arXiv:2003.11881 [Cs]. arXiv.org, [http://arxiv.org/abs/2003.11881](http://arxiv.org/abs/2003.11881)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dulac-Arnold, G. 等人（2020）。*现实世界强化学习挑战的实证研究*。arXiv:2003.11881 [Cs]。arXiv.org，[http://arxiv.org/abs/2003.11881](http://arxiv.org/abs/2003.11881)
- en: Dulac-Arnold, G., et al. (2019). *Challenges of Real-World Reinforcement Learning*.
    arXiv:1904.12901 [Cs, Stat]. arXiv.org, [http://arxiv.org/abs/1904.12901](http://arxiv.org/abs/1904.12901)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dulac-Arnold, G. 等人（2019）。*现实世界强化学习的挑战*。arXiv:1904.12901 [Cs, Stat]。arXiv.org，[http://arxiv.org/abs/1904.12901](http://arxiv.org/abs/1904.12901)
- en: Irpan, A. (2018). *Deep Reinforcement Learning Doesn't Work Yet*. [http://www.alexirpan.com/2018/02/14/rl-hard.html](http://www.alexirpan.com/2018/02/14/rl-hard.html)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Irpan, A.（2018）。*深度强化学习尚未奏效*。[http://www.alexirpan.com/2018/02/14/rl-hard.html](http://www.alexirpan.com/2018/02/14/rl-hard.html)
- en: Levine, S. (2019). Deep Reinforcement Learning – CS285 Fa19 11/18/19, YouTube,
    [https://youtu.be/tzieElmtAjs?t=3336](https://youtu.be/tzieElmtAjs?t=3336). Accessed
    26 September 2020.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levine, S.（2019）。深度强化学习 – CS285 Fa19 11/18/19，YouTube，[https://youtu.be/tzieElmtAjs?t=3336](https://youtu.be/tzieElmtAjs?t=3336)。访问日期：2020年9月26日。
- en: 'Hoffmann, K. et al. (2020). *Challenges & Opportunities in Lifelong Reinforcement
    Learning*. ICML 2020\. URL: [https://slideslive.com/38930956/challenges-opportunities-in-lifelong-reinforcement-learning?ref=speaker-16425-latest](https://slideslive.com/38930956/challenges-opportunities-in-lifelong-reinforcement-learning?ref=speaker-16425-latest)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann, K. 等人（2020）。*终身强化学习中的挑战与机遇*。ICML 2020。URL：[https://slideslive.com/38930956/challenges-opportunities-in-lifelong-reinforcement-learning?ref=speaker-16425-latest](https://slideslive.com/38930956/challenges-opportunities-in-lifelong-reinforcement-learning?ref=speaker-16425-latest)
