- en: '*Chapter 9*: Localizing Elements in Images with Object Detection'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：通过目标检测在图像中定位元素'
- en: Object detection is one of the most common yet challenging tasks in computer
    vision. It's a natural evolution of image classification, where our goal is to
    work out what is in an image. On the other hand, object detection is not only
    concerned with the content of an image but also with the location of elements
    of interest in a digital image.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是计算机视觉中最常见但最具挑战性的任务之一。它是图像分类的自然演变，我们的目标是识别图像中的内容。另一方面，目标检测不仅关注图像的内容，还关注数字图像中感兴趣元素的位置。
- en: As with many other well-known tasks in computer vision, object detection has
    long been addressed with a wide array of techniques, ranging from naïve solutions
    (such as object matching) to machine learning-based ones (such as Haar Cascades).
    Nonetheless, the most effective detectors nowadays are powered by deep learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与计算机视觉中的许多其他知名任务一样，目标检测已经通过各种技术得到解决，从简单的解决方案（如目标匹配）到基于机器学习的解决方案（如Haar级联）。尽管如此，如今最有效的检测器都由深度学习驱动。
- en: Implementing state-of-the-art object detectors (such as **You Only Look Once**
    (**YOLO**) and **Fast Region-based Convolutional Neural Network** (**Fast R-CNN**)
    from scratch is a very challenging task. However, there are many pre-trained solutions
    we can leverage, not only to make predictions but also to train our own models
    from zero, as we'll discover in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始实现最先进的目标检测器（如**YOLO（一次看全）**（**YOLO**）和**快速区域卷积神经网络**（**Fast R-CNN**））是一个非常具有挑战性的任务。然而，我们可以利用许多预训练的解决方案，不仅可以进行预测，还可以从零开始训练我们自己的模型，正如本章所介绍的那样。
- en: 'Here is a list of the recipes we''ll be working on in no time:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这里列出了我们将要快速处理的配方：
- en: Creating an object detector with image pyramids and sliding windows
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用图像金字塔和滑动窗口创建目标检测器
- en: Detecting objects with YOLOv3
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用YOLOv3进行目标检测
- en: Training your own object detector with TensorFlow's Object Detection **Application
    Programming Interface** (**API**)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TensorFlow的目标检测**应用程序编程接口**（**API**）训练你自己的目标检测器
- en: Detecting objects using **TensorFlow Hub** (**TFHub**)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**TensorFlow Hub**（**TFHub**）进行目标检测
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Given the complexity of object detectors, having access to a **Graphics Processing
    Unit** (**GPU**) is a great idea. There are many cloud providers you can use to
    run the recipes in this chapter, my favorite being FloydHub, but you can use whichever
    you like the most! Of course, do keep in mind of the fees if you don't want any
    surprises! In the *Getting ready* sections, you'll find the preparatory steps
    for each recipe. The code for this chapter is available at [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch9](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch9).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于目标检测器的复杂性，拥有**图形处理单元**（**GPU**）是个不错的选择。你可以使用许多云服务商来运行本章中的配方，我个人最喜欢的是FloydHub，但你可以使用你最喜欢的任何服务！当然，如果你不想有意外费用，记得关注费用问题！在*准备工作*部分，你将找到每个配方的准备步骤。本章的代码可以在[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch9](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch9)上找到。
- en: 'Check out the following link to see the Code in Action video:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接，观看代码实战视频：
- en: '[https://bit.ly/39wInla](https://bit.ly/39wInla).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/39wInla](https://bit.ly/39wInla)。'
- en: Creating an object detector with image pyramids and sliding windows
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用图像金字塔和滑动窗口创建目标检测器
- en: Traditionally, object detectors have worked following an iterative algorithm
    whereby a window is slid across the image, at different scales, in order to detect
    potential objects at every location and perspective. Although this approach is
    outdated due to its noticeable drawbacks (which we'll talk more about in the *How
    it works…* section), it has the great advantage of being agnostic about the type
    of image classifier we use, meaning we can use it as a framework to turn any classifier
    into an object detector. This is precisely what we'll do in this first recipe!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，目标检测器通过一种迭代算法工作，该算法将窗口以不同的尺度滑过图像，以检测每个位置和视角下的潜在目标。尽管这种方法因其明显的缺点（我们将在*工作原理...*部分中进一步讨论）而已过时，但它的一个重要优点是它对我们使用的图像分类器类型没有偏见，这意味着我们可以将其作为一个框架，将任何分类器转变为目标检测器。这正是我们在第一个配方中所做的！
- en: Let's begin.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Getting ready
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We need to install a couple of external libraries, such as `OpenCV`, `Pillow`,
    and `imutils`, which can easily be accomplished with this command:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要安装一些外部库，比如`OpenCV`、`Pillow`和`imutils`，可以通过以下命令轻松完成：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We'll use a pre-trained model to power our object detector; therefore, we don't
    need any data for this recipe.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个预训练模型来为我们的物体检测器提供支持，因此我们不需要为此食谱提供任何数据。
- en: How to do it…
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Follow these steps to complete the recipe:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成食谱：
- en: 'Import the necessary dependencies:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的依赖项：
- en: '[PRE1]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, let''s define our `ObjectDetector()` class, starting with the constructor:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义我们的`ObjectDetector()`类，从构造函数开始：
- en: '[PRE2]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `classifier` is just a trained network we'll use to classify each window,
    while `preprocess_fn` is the function used to process each window prior to passing
    it to the classifier. `confidence` is the minimum probability we'll allow detections
    to have in order to consider them valid. The remaining parameters will be explained
    in the next steps.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`classifier`只是一个经过训练的网络，我们将用它来分类每个窗口，而`preprocess_fn`是用于处理每个窗口的函数，在将其传递给分类器之前进行处理。`confidence`是我们允许检测结果的最低概率，只有达到这个概率才能认为检测结果有效。剩余的参数将在下一步中解释。'
- en: 'Now, let''s define a `sliding_window()` method, which extracts portions of
    the input image, with dimensions equal to `self.roi_size`. It''s going to be slid
    across the image, both horizontally and vertically, at a rate of `self.window_step_size`
    pixels at a time (notice the use of `yield` instead of `return`—that''s because
    this is a generator):'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义一个`sliding_window()`方法，该方法提取输入图像的部分区域，尺寸等于`self.roi_size`。它将在图像上水平和垂直滑动，每次移动`self.window_step_size`像素（注意使用了`yield`而不是`return`——这是因为它是一个生成器）：
- en: '[PRE3]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, define the `pyramid()` method, which generates smaller and smaller copies
    of the input image, until a minimum size is met (akin to the levels of a pyramid):'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义`pyramid()`方法，该方法会生成输入图像的越来越小的副本，直到达到最小尺寸（类似于金字塔的各个层级）：
- en: '[PRE4]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Because sliding a window across the same image at different scales is very
    prone to producing many detections related to the same object, we need a way to
    keep duplicates at a minimum. That''s the purpose of our next method, `non_max_suppression()`:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为在不同尺度上滑动窗口会很容易产生与同一物体相关的多个检测结果，我们需要一种方法来将重复项保持在最低限度。这就是我们下一个方法`non_max_suppression()`的作用：
- en: '[PRE5]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We start by computing the area of all bounding boxes, and also sort them by
    their probability, in increasing order. Now, we''ll pick the index of the bounding
    box with the highest probability, and add it to our final selection (`pick`) until
    we have `indexes` left to trim down:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先计算所有边界框的面积，并按概率升序对它们进行排序。接下来，我们将选择具有最高概率的边界框的索引，并将其添加到最终选择中（`pick`），直到剩下`indexes`个边界框需要进行修剪：
- en: '[PRE6]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We compute the overlap between the picked bounding box and the other ones,
    and then get rid of those boxes where the overlap is higher than `self.nms_threshold`,
    which means that they probably refer to the same object:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算选中的边界框与其他边界框之间的重叠部分，然后剔除那些重叠部分超过`self.nms_threshold`的框，这意味着它们很可能指的是同一个物体：
- en: '[PRE7]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Return the picked bounding boxes:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回选中的边界框：
- en: '[PRE8]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `detect()` method ties the object detection algorithm together. We start
    by defining a list of `rois`) and their corresponding `locations` (coordinates
    in the original image):'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`detect()`方法将物体检测算法串联在一起。我们首先定义一个`rois`列表及其对应的`locations`（在原始图像中的坐标）：'
- en: '[PRE9]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we''ll generate different copies of the input image at several scales
    using the `pyramid()` generator, and at each level, we''ll slide a window (with
    the `sliding_windows()` generator) to extract all possible ROIs:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`pyramid()`生成器在多个尺度上生成输入图像的不同副本，并在每个层级上，我们将通过`sliding_windows()`生成器滑动窗口，提取所有可能的ROI：
- en: '[PRE10]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Pass all ROIs through the classifier at once:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一次性通过分类器传递所有的ROI：
- en: '[PRE11]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Build a `dict` to map each label produced by the classifier to all the bounding
    boxes and their probabilities (notice we only keep those bounding boxes with a
    probability of at least `self.confidence`):'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个`dict`来将分类器生成的每个标签映射到所有的边界框及其概率（注意我们只保留那些概率至少为`self.confidence`的边界框）：
- en: '[PRE12]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Instantiate an `InceptionResnetV2` network trained on ImageNet to use as our
    classifier and pass it to a new `ObjectDetector`. Notice that we''re also passing
    the `preprocess_function` as input:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个在ImageNet上训练的`InceptionResnetV2`网络，作为我们的分类器，并将其传递给新的`ObjectDetector`。注意，我们还将`preprocess_function`作为输入传递：
- en: '[PRE13]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Load the input image, resize it to a width of 600 pixels maximum (the height
    will be computed accordingly to preserve the aspect ratio), and run it through
    the object detector:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载输入图像，将其最大宽度调整为600像素（高度将相应计算以保持宽高比），并通过物体检测器进行处理：
- en: '[PRE14]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Go over all the detections corresponding to each label, and first draw all
    the bounding boxes:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历所有对应每个标签的检测结果，首先绘制所有边界框：
- en: '[PRE15]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, use **Non-Maximum Suppression** (**NMS**) to get rid of duplicates and
    draw the surviving bounding boxes:'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，使用**非最大抑制**（**NMS**）去除重复项，并绘制剩余的边界框：
- en: '[PRE16]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here''s the result without NMS:'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是没有应用NMS的结果：
- en: '![Figure 9.1 – Overlapping detections of the same dog'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1 – 同一只狗的重叠检测'
- en: '](img/B14768_09_001.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_09_001.jpg)'
- en: Figure 9.1 – Overlapping detections of the same dog
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 同一只狗的重叠检测
- en: 'And here''s the result after applying NMS:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是应用NMS后的结果：
- en: '![Figure 9.2 – With NMS, we got rid of the redundant detections'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2 – 使用NMS后，我们去除了冗余的检测'
- en: '](img/B14768_09_002.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_09_002.jpg)'
- en: Figure 9.2 – With NMS, we got rid of the redundant detections
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 使用NMS后，我们去除了冗余的检测
- en: Although we successfully detected the dog in the previous photos, we notice
    that the bounding box doesn't tightly wrap the object as nicely as we might have
    expected. Let's talk about this and other issues regarding old-school object detection
    in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在前面的照片中成功检测到了狗，但我们注意到边界框并没有像我们预期的那样紧密包裹住物体。让我们在接下来的章节中讨论这个问题以及传统物体检测方法的其他问题。
- en: How it works…
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we implemented a reusable class that easily allows us to turn
    any image classifier into an object detector, by leveraging the iterative approach
    of extracting ROIs (sliding windows) at different levels of perspective (image
    pyramid) and passing them to such a classifier to determine where objects are
    in a photo, and what they are. Also, we used NMS to reduce the amount of non-informative,
    duplicate detections that are characteristic of this strategy.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们实现了一个可重用的类，利用迭代方法在不同的视角层次（图像金字塔）提取ROI（滑动窗口），并将其传递给图像分类器，从而确定照片中物体的位置和类别。我们还使用了**非最大抑制**（NMS）来减少这种策略所特有的冗余和重复检测。
- en: 'Although this a great first attempt at creating an object detector, it has
    its flaws:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是创建对象检测器的一个很好的初步尝试，但它仍然存在一些缺陷：
- en: It's incredibly slow, which makes it unusable in real-time situations.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它非常慢，这使得它在实时场景中不可用。
- en: The accuracy of the bounding boxes depends heavily on the parameter selection
    for the image pyramid, the sliding window, and the ROI size.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边界框的准确性很大程度上取决于图像金字塔、滑动窗口和ROI大小的参数选择。
- en: The architecture is not end-to-end trainable, which means that errors in bounding-box
    predictions are not backpropagated through the network in order to produce better,
    more accurate detections in the future, by updating its weights. Instead, we're
    stuck with pre-trained models that limit themselves to infer but not to learn
    because the framework does not allow them to.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该架构不是端到端可训练的，这意味着边界框预测中的误差不会通过网络反向传播，以便通过更新权重来产生更好、更准确的检测结果。相反，我们只能使用预训练模型，这些模型仅限于推断，而无法学习，因为框架不允许它们学习。
- en: However, don't rule out this approach yet! If you're working with images that
    present very little variation in size and perspective, and your application definitely
    doesn't operate in a real-time context, the strategy implemented in this recipe
    can work wonders for your project!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，别急着排除这种方法！如果你处理的图像在尺寸和视角上变化很小，且你的应用程序绝对不在实时环境中运行，那么本方案中实现的策略可能会对你的项目大有裨益！
- en: See also
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'You can read more about NMS here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于NMS的内容：
- en: '[https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c](https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c)'
- en: Detecting objects with YOLOv3
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用YOLOv3检测物体
- en: In the *Creating an object detector with image pyramids and sliding windows*
    recipe, we learned how to turn any image classifier into an object detector, by
    embedding it in a traditional framework that relies on image pyramids and sliding
    windows. However, we also learned that this approach isn't ideal because it doesn't
    allow the network to learn from its mistakes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在*使用图像金字塔和滑动窗口创建物体检测器*的实例中，我们学会了如何通过将任何图像分类器嵌入到依赖于图像金字塔和滑动窗口的传统框架中，来将其转变为物体检测器。然而，我们也学到，这种方法并不理想，因为它无法让网络从错误中学习。
- en: The reason why deep learning has conquered the field of object detection is
    due to its end-to-end approach. The network not only figures out how to classify
    an object, but also discovers how to produce the best bounding box possible to
    locate each element in the image.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习之所以在物体检测领域占据主导地位，是因为它的端到端方法。网络不仅能弄清楚如何对物体进行分类，还能发现如何生成最佳的边界框来定位图像中的每个元素。
- en: On top of this, thanks to this end-to-end strategy, a network can detect a myriad
    objects in a single pass! Of course, this makes such object detectors incredibly
    efficient!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个端到端的策略，网络可以在一次遍历中检测到无数个物体！当然，这也使得这样的物体检测器极为高效！
- en: One of the seminal end-to-end object detectors is YOLO, and in this recipe,
    we'll learn how to detect objects with a pre-trained YOLOv3 model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO是开创性的端到端物体检测器之一，在这个实例中，我们将学习如何使用预训练的YOLOv3模型进行物体检测。
- en: Let's begin!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始吧！
- en: Getting ready
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'First, install `tqdm`, as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 首先安装`tqdm`，如下所示：
- en: '[PRE17]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Our implementation is heavily inspired by the amazing `keras-yolo3` repository
    implemented by *Huynh Ngoc Anh (on GitHub as experiencor)*, which you can consult
    here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现深受精彩的`keras-yolo3`库的启发，该库由*Huynh Ngoc Anh（GitHub上的experiencor）*实现，你可以在这里查看：
- en: '[https://github.com/experiencor/keras-yolo3](https://github.com/experiencor/keras-yolo3)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/experiencor/keras-yolo3](https://github.com/experiencor/keras-yolo3)'
- en: 'Because we''ll use a pre-trained YOLO model, we need to download the weights.
    They''re available here: [https://pjreddie.com/media/files/yolov3.weights](https://pjreddie.com/media/files/yolov3.weights).
    For the purposes of this tutorial, we assume they''re inside the `ch9/recipe2/resources`
    folder, in the companion repository, as `yolov3.weights`. These weights are the
    same ones used by the original authors of YOLO. Refer to the *See also* section
    to learn more about YOLO.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们将使用预训练的YOLO模型，所以需要下载权重文件。它们可以在这里获取：[https://pjreddie.com/media/files/yolov3.weights](https://pjreddie.com/media/files/yolov3.weights)。在本教程中，我们假设这些权重文件位于伴随代码库中的`ch9/recipe2/resources`文件夹内，名为`yolov3.weights`。这些权重与YOLO的原作者使用的是相同的。更多关于YOLO的内容，请参考*另见*部分。
- en: We are good to go!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪！
- en: How to do it…
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Follow these steps to complete the recipe:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成该实例：
- en: 'Start by importing the relevant dependencies:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先导入相关的依赖：
- en: '[PRE18]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Define a `WeightReader()` class that automatically loads the YOLO weights in
    whichever format the original authors used. Notice that this is a very low-level
    solution, but we don''t need to understand it fully in order to leverage it. Let''s
    begin with the constructor:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个`WeightReader()`类，自动加载YOLO的权重，无论原作者使用了什么格式。请注意，这是一个非常底层的解决方案，但我们不需要完全理解它就可以加以利用。让我们从构造函数开始：
- en: '[PRE19]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, define a method to read a given number of bytes from the `weights` file:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定义一个方法来从`weights`文件中读取指定数量的字节：
- en: '[PRE20]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `load_weights()` method loads the weights for each of the 106 layers that
    comprise the YOLO architecture:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`load_weights()`方法加载了组成YOLO架构的106层每一层的权重：'
- en: '[PRE21]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Load the weights of the convolutional layers:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载卷积层的权重：
- en: '[PRE22]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define a method to reset the offset:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个方法来重置偏移量：
- en: '[PRE23]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define a `BoundBox()` class that encapsulates the vertices of a bounding box,
    along with the confidence that the enclosed elements are an object (`objness`):'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个`BoundBox()`类，封装边界框的顶点，以及该框中元素为物体的置信度（`objness`）：
- en: '[PRE24]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Define a `YOLO()` class that encapsulates both the construction of the network
    and the detection logic. Let''s begin with the constructor:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个`YOLO()`类，封装网络的构建和检测逻辑。让我们从构造函数开始：
- en: '[PRE25]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The output of YOLO is a set of encoded bounding boxes defined in the context
    of anchor boxes that were carefully chosen by the authors of YOLO. This is based
    on an analysis of the size of objects in the `COCO` dataset. That's why we store
    the anchors in `self.anchors`, and `COCO`'s labels in `self.labels`. Also, we
    rely on the `self._load_yolo()` method (defined later) to build the model.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: YOLO 的输出是一组在锚框上下文中定义的编码边界框，这些锚框是由 YOLO 的作者精心挑选的。这是基于对 `COCO` 数据集中物体大小的分析。因此，我们将锚框存储在
    `self.anchors` 中，`COCO` 的标签存储在 `self.labels` 中。此外，我们依赖于 `self._load_yolo()` 方法（稍后定义）来构建模型。
- en: 'YOLO is comprised of a series of convolutional blocks and optional skip connections.
    The `_conv_block()` helper method allows us to instantiate such blocks easily:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: YOLO 由一系列卷积块和可选的跳跃连接组成。 `_conv_block()` 辅助方法允许我们轻松地实例化这些块：
- en: '[PRE26]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Check if we need to add batch normalization, leaky ReLU activations, and skip
    connections:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查是否需要添加批量归一化、leaky ReLU 激活和跳跃连接：
- en: '[PRE27]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `_make_yolov3_architecture()` method, defined as follows, builds the YOLO
    network by stacking a series of convolutional blocks, using the `_conv_block()`
    method defined previously:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`_make_yolov3_architecture()` 方法，如下所示，通过堆叠一系列卷积块来构建 YOLO 网络，使用先前定义的 `_conv_block()`
    方法：'
- en: '[PRE28]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Because this method is quite large, please refer to the companion repository
    for the full implementation.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因为这个方法比较大，请参考附带的代码库获取完整实现。
- en: 'The `_load_yolo()` method creates the architecture, loads the weights, and
    instantiates a trained YOLO model in a format TensorFlow understands:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`_load_yolo()` 方法创建架构、加载权重，并实例化一个 TensorFlow 可理解的训练过的 YOLO 模型：'
- en: '[PRE29]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define a static method to compute the Sigmoid value of a tensor:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个静态方法来计算张量的 Sigmoid 值：
- en: '[PRE30]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `_decode_net_output()` method decodes the candidate bounding boxes and
    class predictions produced by YOLO:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`_decode_net_output()` 方法解码 YOLO 产生的候选边界框和类别预测：'
- en: '[PRE31]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We skip those bounding boxes that don''t confidently describe an object:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们跳过那些不能自信地描述物体的边界框：
- en: '[PRE32]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We extract the coordinates and classes from the network output, and use them
    to create `BoundBox()` instances:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从网络输出中提取坐标和类别，并使用它们来创建 `BoundBox()` 实例：
- en: '[PRE33]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The `_correct_yolo_boxes()` method rescales the bounding boxes to the dimensions
    of the original image:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`_correct_yolo_boxes()` 方法将边界框调整为原始图像的尺寸：'
- en: '[PRE34]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We''ll perform NMS in a bit, in order to reduce the number of redundant detections.
    For that matter, we need a way to compute the amount of overlap between two intervals:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们稍后会执行 NMS，以减少冗余的检测。为此，我们需要一种计算两个区间重叠量的方法：
- en: '[PRE35]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we can calculate the `_interval_overlap()` method defined before:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以计算前面定义的 `_interval_overlap()` 方法：
- en: '[PRE36]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Armed with these methods, we can apply NMS to the bounding boxes in order to
    keep the number of duplicate detections to a minimum:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这些方法，我们可以对边界框应用 NMS，从而将重复检测的数量降到最低：
- en: '[PRE37]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `_get_boxes()` method keeps only those boxes with a confidence score higher
    than the `self.class_threshold` method defined in the constructor (0.6 or 60%
    by default):'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`_get_boxes()` 方法仅保留那些置信度高于构造函数中定义的 `self.class_threshold` 方法（默认值为 0.6 或 60%）的框：'
- en: '[PRE38]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`_draw_boxes()` plots the most confident detections in an input image, which
    means that each bounding box is accompanied by its class label and its probability:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`_draw_boxes()` 在输入图像中绘制最自信的检测结果，这意味着每个边界框都会显示其类别标签及其置信度：'
- en: '[PRE39]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The only public method in the `YOLO()` class is `detect()`, which implements
    the end-to-end logic to detect objects in an input image. First, it passes the
    image through the model:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`YOLO()` 类中的唯一公共方法是 `detect()`，它实现了端到端的逻辑，用于检测输入图像中的物体。首先，它将图像传入模型：'
- en: '[PRE40]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Then, it decodes the outputs of the network:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它解码网络的输出：
- en: '[PRE41]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, it corrects the boxes so that they have proper proportions in relation
    to the input image. It also applies NMS to get rid of redundant detections:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，它修正这些框，使它们与输入图像的比例正确。它还应用 NMS 来去除冗余的检测结果：
- en: '[PRE42]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Lastly, it gets the valid bounding boxes and draws them in the input image:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，它获取有效的边界框，并将其绘制到输入图像中：
- en: '[PRE43]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'With the `YOLO()` class defined, we can instantiate it as follows:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义了 `YOLO()` 类后，我们可以按如下方式实例化它：
- en: '[PRE44]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The final step is to iterate over all test images and run the model on them:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是遍历所有测试图像，并在其上运行模型：
- en: '[PRE45]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Here''s the first example:'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是第一个示例：
- en: '![Figure 9.3 – YOLO detected the dog, with a very high confidence score'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3 – YOLO 检测到狗，具有非常高的置信度]'
- en: '](img/B14768_09_003.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_09_003.jpg)'
- en: Figure 9.3 – YOLO detected the dog, with a very high confidence score
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – YOLO 以非常高的置信度检测到了这只狗
- en: 'We can observe that YOLO confidently detected my dog as such, with a confidence
    score of 94.5%! Awesome! Let''s look at the second test image:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，YOLO 非常自信地检测到了我的狗，并且置信度高达 94.5%！太棒了！接下来看看第二张测试图像：
- en: '![Figure 9.4 – YOLO detected multiple objects at varying scales in a single
    pass'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.4 – YOLO 在一次处理过程中检测到了不同尺度的多个物体'
- en: '](img/B14768_09_004.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_09_004.jpg)'
- en: Figure 9.4 – YOLO detected multiple objects at varying scales in a single pass
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – YOLO 在一次处理过程中检测到了不同尺度的多个物体
- en: Even though the result is crowded, a quick glance reveals the network was able
    to identify both cars in the foreground, as well as the people in the background.
    This is an interesting example because it demonstrates the incredible power of
    YOLO as an end-to-end object detector, which in a single pass was capable of classifying
    and localizing many different objects, at varying scales. Impressive, isn't it?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管结果很拥挤，但快速一瞥便能看出，网络成功识别了前景中的两辆车，以及背景中的人。这是一个有趣的例子，因为它展示了 YOLO 作为端到端物体检测器的强大能力，它能够在一次处理过程中对许多不同的物体进行分类和定位，且尺度各异。是不是很令人印象深刻？
- en: Let's head to the *How it works…* section to connect the dots.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们前往*如何工作...*部分，来连接这些点。
- en: How it works…
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作…
- en: 'In this recipe, we discovered the immense power of end-to-end object detectors—
    particularly, one of the most famous and impressive of all: YOLO.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们发现了端到端物体检测器的巨大威力——特别是其中最著名和最令人印象深刻的一个：YOLO。
- en: Although YOLO was originally implemented in C++, we leveraged the fantastic
    Python adaptation by *Huynh Ngoc Anh* to perform object detection in our own images
    using a pre-trained version (specifically, version 3) of this architecture on
    the seminal `COCO` dataset.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 YOLO 最初是用 C++ 实现的，但我们利用了*Huynh Ngoc Anh* 的精彩 Python 适配，使用这个架构的预训练版本（特别是第
    3 版）在开创性的 `COCO` 数据集上进行物体检测。
- en: As you might have noticed, YOLO and many other end-to-end object detectors are
    very complex networks, but their advantage over traditional approaches such as
    image pyramids and sliding windows is evident. Not only are the results way better,
    but they also come through faster thanks to the ability of YOLO to look once at
    the input image in order to produce all the relevant detections.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的那样，YOLO 和许多其他端到端物体检测器都是非常复杂的网络，但它们相对于传统方法（如图像金字塔和滑动窗口）有明显的优势。结果不仅更好，而且得益于
    YOLO 能够一次性查看输入图像并产生所有相关检测的能力，处理速度也更快。
- en: But what if you want to train an end-to-end object detector on your own data?
    Are you doomed to rely on out-of-the-box solutions? Do you need to spend hours
    deciphering cryptic papers in order to implement such networks?
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你想在自己的数据上训练一个端到端的物体检测器呢？难道你只能依赖现成的解决方案吗？你需要花费数小时去解读难懂的论文，才能实现这些网络吗？
- en: Well, that's one option, but there's another one, which we'll explore in the
    next recipe, and it entails the TensorFlow Object Detection API, an experimental
    repository of state-of-the-art architectures that will ease and boost your object
    detection endeavors!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，那是一个选项，但还有另一个，我们将在下一个配方中探讨，它涉及 TensorFlow 物体检测 API，这是一个实验性仓库，汇集了最先进的架构，能够简化并提升你的物体检测工作！
- en: See also
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'YOLO is a milestone when it comes to deep learning and object detection, so
    reading the paper is a pretty smart time investment. You can find it here:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 是深度学习和物体检测领域的一个里程碑，因此阅读这篇论文是一个非常明智的时间投资。你可以在这里找到它：
- en: '[https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640)'
- en: 'You can learn more about YOLO directly from the author''s website, here:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接从作者的网站了解更多关于 YOLO 的信息，网址如下：
- en: '[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)'
- en: 'If you are interested in exploring `keras-yolo3`, the tool we based our implementation
    on, refer to this link:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解我们基于其实现的`keras-yolo3`工具，可以参考这个链接：
- en: '[https://github.com/experiencor/keras-yolo3](https://github.com/experiencor/keras-yolo3)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/experiencor/keras-yolo3](https://github.com/experiencor/keras-yolo3)'
- en: Training your own object detector with TensorFlow's Object Detection API
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的物体检测 API 训练你自己的物体检测器
- en: It's no secret that modern object detectors rank among the most complex and
    challenging architectures to implement and get it right! However, that doesn't
    mean we can't take advantage of the most recent advancements in this domain in
    order to train object detectors on our own datasets. *How?*, you ask. Enter TensorFlow's
    Object Detection API!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现代物体检测器无疑是实现和调试最复杂、最具挑战性的架构之一！然而，这并不意味着我们不能利用这个领域的最新进展，在我们自己的数据集上训练物体检测器。*怎么做？*你问。那就让我们来了解一下
    TensorFlow 的物体检测 API！
- en: In this recipe, we'll install this API, prepare a custom dataset for training,
    tweak a couple of configuration files, and use the resulting model to localize
    objects on test images. This recipe is a bit different from the ones you've worked
    on so far, because we'll be switching back and forth between Python and the command
    line.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将安装这个 API，准备一个自定义数据集进行训练，调整几个配置文件，并使用训练好的模型在测试图像上定位物体。这个食谱与你之前做过的有所不同，因为我们将在
    Python 和命令行之间来回切换。
- en: Are you ready? Then let's get started.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你准备好了吗？那就让我们开始吧。
- en: Getting ready
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'There are several dependencies we need to install for this recipe to work.
    Let''s begin with the most important one: the TensorFlow Object Detection API.
    First, `cd` to a location of your preference and clone the `tensorflow/models`
    repository:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个依赖项需要安装才能使这个食谱工作。让我们从最重要的开始：TensorFlow 物体检测 API。首先，`cd`到你喜欢的位置并克隆 `tensorflow/models`
    仓库：
- en: '[PRE46]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, install the TensorFlow Object Detection API, like this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，像这样安装 TensorFlow 物体检测 API：
- en: '[PRE47]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'For the purposes of this recipe, we''ll assume it''s installed at the same
    level as the `ch9` folder (https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch9).
    Now, we must install `pandas` and `Pillow`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 就本食谱而言，我们假设它与`ch9`文件夹位于同一层级（https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch9）。现在，我们必须安装`pandas`和`Pillow`：
- en: '[PRE48]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The dataset we will use is `Fruit Images for Object Detection`, hosted on Kaggle,
    which you can access here: [https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection).
    Log in or sign up and download the data to a location of your preference as `fruits.zip`
    (the data is available in the `ch9/recipe3` folder in the companion repository
    for this book). Finally, decompress it:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的数据集是`Fruit Images for Object Detection`，托管在 Kaggle 上，你可以通过以下链接访问：[https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection](https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection)。登录或注册后，下载数据并保存到你喜欢的位置，文件名为`fruits.zip`（数据可以在本书配套仓库的`ch9/recipe3`文件夹中找到）。最后，解压缩它：
- en: '![Figure 9.5 – Sample images of the three classes in the dataset: apple, orange,
    and banana'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.5 – 数据集中三类样本图像：苹果、橙子和香蕉'
- en: '](img/B14768_09_005.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_09_005.jpg)'
- en: 'Figure 9.5 – Sample images of the three classes in the dataset: apple, orange,
    and banana'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 数据集中三类样本图像：苹果、橙子和香蕉
- en: The labels in this dataset are in **Pascal VOC** format, where **VOC** stands
    for **Visual Object Classes**. Refer to the *See also…* section to learn more
    about it.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集的标签采用**Pascal VOC**格式，其中**VOC**代表**视觉物体类别**。请参考*另见…*部分了解更多信息。
- en: Now, we're all set! Let's begin implementing.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好了！让我们开始实现。
- en: How to do it…
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何进行操作……
- en: 'By the end of these steps, you''ll have trained your own state-of-the-art object
    detector using the TensorFlow Object Detection API:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，你将使用 TensorFlow 物体检测 API 训练出你自己的最先进物体检测器：
- en: 'We''ll work with two files in this recipe: the first one is used to prepare
    the data (you can find it as `prepare.py` in the repository), and the second one
    is used to make inferences with the object detector (`inference.py` in the repository).
    Open `prepare.py` and import all the needed packages:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将处理两个文件：第一个用于准备数据（你可以在仓库中找到它，名为`prepare.py`），第二个用于使用物体检测器进行推理（在仓库中为`inference.py`）。打开`prepare.py`并导入所有需要的包：
- en: '[PRE49]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Define the `encode_class()` function, which maps the text labels to their integer
    counterparts:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`encode_class()`函数，将文本标签映射到它们的整数表示：
- en: '[PRE50]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Define a function to split a dataframe of labels (which we''ll create later)
    into groups:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将标签的数据框（我们稍后会创建）拆分成组：
- en: '[PRE51]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The TensorFlow Object Detection API works with a data structure known as `tf.train.Example`.
    The next function takes the path to an image and its label (which is the set of
    bounding boxes and the ground-truth classes of all objects contained in it) and
    creates the corresponding `tf.train.Example`. First, load the image and its properties:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorFlow目标检测API使用一种名为`tf.train.Example`的数据结构。下一个函数接收图像的路径及其标签（即包含的所有对象的边界框集和真实类别），并创建相应的`tf.train.Example`。首先，加载图像及其属性：
- en: '[PRE52]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, store the dimensions of the bounding boxes, along with the classes of
    each object contained in the image:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，存储边界框的维度以及图像中每个对象的类别：
- en: '[PRE53]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Create a `tf.train.Features` object that will contain relevant information
    about the image and its objects:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`tf.train.Features`对象，包含图像及其对象的相关信息：
- en: '[PRE54]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Return a `tf.train.Example` structure initialized with the features created
    previously:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回一个用先前创建的特征初始化的`tf.train.Example`结构：
- en: '[PRE55]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Define a function to transform an **Extensible Markup Language** (**XML**)
    file—with information about the bounding boxes in an image—to an equivalent one
    in **Comma-Separated Values** (**CSV**) format:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将包含图像边界框信息的**可扩展标记语言**（**XML**）文件转换为等效的**逗号分隔值**（**CSV**）格式文件：
- en: '[PRE56]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Iterate over the `test` and `train` subsets in the `fruits` folder, converting
    the labels from CSV to XML:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历`fruits`文件夹中的`test`和`train`子集，将标签从CSV转换为XML：
- en: '[PRE57]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Then, use the same labels to produce the `tf.train.Examples` corresponding
    to the current subset of data being processed:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用相同的标签生成与当前正在处理的数据子集对应的`tf.train.Examples`：
- en: '[PRE58]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: After running the `prepare.py` script implemented in *Step 1* through *Step
    10*, you'll have the data in the necessary shape for the TensorFlow Object Detection
    API to train on it. The next step is to download the weights of `EfficientDet`,
    a state-of-the-art architecture we'll fine-tune shortly. Download the weights
    from this `Desktop` folder.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行*第1步*至*第10步*中实现的`prepare.py`脚本后，你将获得适合TensorFlow目标检测API训练的数据形状。下一步是下载`EfficientDet`的权重，这是我们将要微调的最先进架构。从`Desktop`文件夹下载权重。
- en: 'Create a file to map the classes to integers. Name it `label_map.txt` and place
    it inside `ch9/recipe3/resources`:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个文件，将类别映射到整数。命名为`label_map.txt`并将其放在`ch9/recipe3/resources`中：
- en: '[PRE59]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Next, we must change the configuration file for this network to adapt it to
    our dataset. You can either locate it in `models/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config`
    (assuming you installed the TensorFlow Object Detection API at the same level
    of the `ch9` folder in the companion repository), or download it directly from
    this URL: [https://github.com/tensorflow/models/blob/master/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config](https://github.com/tensorflow/models/blob/master/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config).
    Whichever option you choose, place a copy inside `ch9/recipe3/resources` and modify
    *line 13* to reflect the number of classes in our dataset:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们必须更改该网络的配置文件，以使其适应我们的数据集。你可以将其放置在`models/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config`（假设你已将TensorFlow目标检测API安装在与`ch9`文件夹同一级别的伴随库中），或者直接从以下网址下载：[https://github.com/tensorflow/models/blob/master/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config](https://github.com/tensorflow/models/blob/master/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config)。无论你选择哪种方式，请将文件复制到`ch9/recipe3/resources`中，并修改*第13行*，以反映我们数据集中类别的数量：
- en: '[PRE60]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Then, modify *line 140* to point to the `EfficientDet` weights we downloaded
    in *Step 7*:'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，修改*第140行*，使其指向我们在*第7步*中下载的`EfficientDet`权重：
- en: '[PRE61]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Change `fine_tune_checkpoint_type` from `classification` to `detection` on
    *line 143*:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在*第143行*将`fine_tune_checkpoint_type`从`classification`改为`detection`：
- en: '[PRE62]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Modify *line 180* to point to the `label_map.txt` file created in *Step 8*:'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 修改*第180行*，使其指向*第8步*中创建的`label_map.txt`文件：
- en: '[PRE63]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Modify *line 182* to point to the `train.record` file created in *Step 11*,
    corresponding to the prepared training data:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 修改*第182行*，使其指向*第11步*中创建的`train.record`文件，该文件对应于已准备好的训练数据：
- en: '[PRE64]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Modify *line 193* to point to the `label_map.txt` file created in *Step 12*:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 修改*第193行*，使其指向*第12步*中创建的`label_map.txt`文件：
- en: '[PRE65]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Modify *line 197* to point to the `test.record` file created in *Step 11*,
    corresponding to the prepared test data:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 修改*第197行*，使其指向*第11步*中创建的`test.record`文件，该文件对应于已准备好的测试数据：
- en: '[PRE66]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Time to train the model! First, assuming you''re at the root level of the companion
    repository, `cd` into the `object_detection` folder in the TensorFlow Object Detection
    API:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到了训练模型的时候！首先，假设你在配套仓库的根目录下，`cd`进入TensorFlow对象检测API中的`object_detection`文件夹：
- en: '[PRE67]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Then, train the model with this command:'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，使用以下命令训练模型：
- en: '[PRE68]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Here, we are training the model for `10000` steps. Also, we'll save the results
    in the `training` folder inside `ch9/recipe3`. Finally, we're specifying the location
    of the configuration file with the `--pipeline_config_path` option. This step
    will take several hours.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们正在训练模型进行`10000`步训练。此外，我们将把结果保存在`ch9/recipe3`中的`training`文件夹内。最后，我们通过`--pipeline_config_path`选项指定配置文件的位置。这个步骤将持续几个小时。
- en: 'Once the network has been fine-tuned, we must export it as a frozen graph in
    order to use it for inference. For that matter, `cd` once again to the `object_detection`
    folder in the TensorFlow Object Detection API:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦网络进行了精调，我们必须将其导出为冻结图，以便用于推理。为此，再次`cd`进入TensorFlow对象检测API中的`object_detection`文件夹：
- en: '[PRE69]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now, execute the following command:'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，执行以下命令：
- en: '[PRE70]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The `trained_checkpoint_dir` parameter is used to point to the location where
    the trained model is, while `pipeline_config_path` points to the model's configuration
    file. Finally, the frozen inference graph will be saved inside the `ch9/recipe3/resources/inference_graph`
    folder, as stated by the `output_directory` flag.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`trained_checkpoint_dir`参数用于指定训练好的模型所在的位置，而`pipeline_config_path`则指向模型的配置文件。最后，冻结的推理图将保存在`ch9/recipe3/resources/inference_graph`文件夹中，正如`output_directory`标志所指定的那样。'
- en: 'Open a file named `inference.py`, and import all the relevant dependencies:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个名为`inference.py`的文件，并导入所有相关的依赖项：
- en: '[PRE71]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Define a function to load an image from disk as a NumPy array:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，从磁盘加载图像并将其转换为 NumPy 数组：
- en: '[PRE72]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Define a function to run the model on a single image. First, convert the image
    into a tensor:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，在单张图像上运行模型。首先，将图像转换为张量：
- en: '[PRE73]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Pass the tensor to the network, extract the number of detections, and keep
    as many values in the resulting dictionary as there are detections:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将张量传递给网络，提取检测的数量，并在结果字典中保留与检测数量相等的值：
- en: '[PRE74]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'If there are detection masks present, reframe them to image masks and return
    the results:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果有检测掩膜存在，将它们重框为图像掩膜并返回结果：
- en: '[PRE75]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Create a category index from the `label_map.txt` file we created in *Step 12*,
    and also load the model from the frozen inference graph produced in *Step 15*:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们在*步骤12*中创建的`label_map.txt`文件中创建类别索引，同时从*步骤15*中生成的冻结推理图中加载模型：
- en: '[PRE76]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Pick three random test images:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择三张测试图像：
- en: '[PRE77]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Run the model over the sample images, and save the resulting detections:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在样本图像上运行模型，并保存结果检测：
- en: '[PRE78]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We see the results in *Figure 9.6*:'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以在*图9.6*中看到结果：
- en: '![Figure 9.6 – EfficientDet detection results on a random sample of test images'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.6 – EfficientDet 在随机样本测试图像上的检测结果'
- en: '](img/B14768_09_006.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_09_006.jpg)'
- en: Figure 9.6 – EfficientDet detection results on a random sample of test images
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – EfficientDet 在随机样本测试图像上的检测结果
- en: We can see in *Figure 9.6* that our fine-tuned network produced fairly accurate
    and confident detections. Considering we only concerned ourselves with data preparation
    and inference, and that regarding the architecture itself we just adapted a configuration
    file to our needs, the results are pretty impressive!
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图9.6*中看到，我们精调后的网络产生了相当准确且自信的检测结果。考虑到我们仅关注数据准备和推理，并且在架构方面我们只是根据需要调整了配置文件，结果相当令人印象深刻！
- en: Let's move on to the *How it works…* section.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续阅读*如何工作...*部分。
- en: How it works…
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作...
- en: In this recipe, we discovered that training an object detector is a hard and
    challenging feat. The good news, however, is that we have the TensorFlow Object
    Detection API at our disposal to train a wide range of vanguardist networks.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱中，我们发现训练一个物体检测器是一个艰难且富有挑战的任务。然而，好消息是，我们可以使用TensorFlow对象检测API来训练各种前沿网络。
- en: Because the TensorFlow Object Detection API is an experimental tool, it uses
    different conventions than regular TensorFlow, and therefore in order to use it,
    we need to perform a little bit of processing work on the input data to put it
    into a shape that the API understands. This is done by converting the labels in
    the `Fruits for Object Detection` dataset (originally in XML format) to CSV and
    then into serialized `tf.train.Example` objects.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 由于TensorFlow物体检测API是一个实验性工具，它使用与常规TensorFlow不同的约定，因此，为了使用它，我们需要对输入数据进行一些处理，将其转化为API可以理解的格式。这是通过将`Fruits
    for Object Detection`数据集中的标签（最初是XML格式）转换为CSV，再转为序列化的`tf.train.Example`对象来完成的。
- en: Then, to use the trained model, we exported it as an inference graph using the
    `exporter_main_v2.py` script and leveraged some of the visualization tools in
    the API to display the detections on the sample test images.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了使用训练好的模型，我们通过`exporter_main_v2.py`脚本将其导出为推理图，并利用API中的一些可视化工具显示样本测试图像上的检测结果。
- en: 'What about the training? This is arguably the easiest part, entailing three
    major steps:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，训练呢？可以说这是最简单的部分，包含三个主要步骤：
- en: Creating a mapping from text labels to integers (*Step 12*)
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建从文本标签到整数的映射（*步骤12*）
- en: Modifying the configuration file corresponding to the model to fine-tune it
    in all the relevant places (*Step 13*)
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改与模型对应的配置文件，以便在所有相关位置进行微调（*步骤13*）
- en: Running the `model_main_tf2.py` file to train the network, passing it the proper
    parameters (*Step 14*)
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行`model_main_tf2.py`文件来训练网络，并传递正确的参数（*步骤14*）
- en: This recipe provides you with a template you can tweak and adapt to train virtually
    any modern object detector (supported by the API) on any dataset of your choosing.
    Pretty cool, right?
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方案为你提供了一个模板，你可以对其进行调整和适应，以便在任何你选择的数据集上训练几乎所有现代物体检测器（API支持的）。相当酷，对吧？
- en: See also
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'You can learn more about the TensorFlow Object Detection API here:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里了解更多关于TensorFlow物体检测API的信息：
- en: '[https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/tensorflow/models/tree/master/research/object_detection](https://github.com/tensorflow/models/tree/master/research/object_detection)'
- en: 'Also, I encourage you to read this great article to learn more about `EfficientDet`:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我鼓励你阅读这篇精彩的文章，了解更多关于`EfficientDet`的信息：
- en: '[https://towardsdatascience.com/a-thorough-breakdown-of-efficientdet-for-object-detection-dc6a15788b73](https://towardsdatascience.com/a-thorough-breakdown-of-efficientdet-for-object-detection-dc6a15788b73)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/a-thorough-breakdown-of-efficientdet-for-object-detection-dc6a15788b73](https://towardsdatascience.com/a-thorough-breakdown-of-efficientdet-for-object-detection-dc6a15788b73)'
- en: 'If you want to learn a great deal about the **Pascal VOC** format, then you
    must watch this video:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想深入了解**Pascal VOC**格式，那么你一定要观看这个视频：
- en: '[https://www.youtube.com/watch?v=-f6TJpHcAeM](https://www.youtube.com/watch?v=-f6TJpHcAeM)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=-f6TJpHcAeM](https://www.youtube.com/watch?v=-f6TJpHcAeM)'
- en: Detecting objects using TFHub
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TFHub进行物体检测
- en: TFHub is a cornucopia of state-of-the-art models when it comes to object detection.
    As we'll discover in this recipe, using them to spot elements of interest in our
    images is a fairly straightforward task, especially considering they've been trained
    on the gigantic `COCO` dataset, which make them an excellent choice for out-of-the-box
    object detection.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: TFHub是物体检测领域的一个丰富宝库，充满了最先进的模型。正如我们在这个方案中将发现的那样，使用它们来识别图像中的感兴趣元素是一项相当直接的任务，尤其是考虑到它们已经在庞大的`COCO`数据集上进行了训练，这使得它们成为现成物体检测的绝佳选择。
- en: Getting ready
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'First, we must install `Pillow` and TFHub, as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须安装`Pillow`和TFHub，步骤如下：
- en: '[PRE79]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Also, because some visualization tools we''ll use live in the TensorFlow Object
    Detection API, we must install it. First, `cd` to a location of your preference
    and clone the `tensorflow/models` repository:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于我们将使用的一些可视化工具位于TensorFlow物体检测API中，我们必须先安装它。首先，`cd`到你喜欢的位置，并克隆`tensorflow/models`仓库：
- en: '[PRE80]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Next, install the TensorFlow Object Detection API, like this:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，安装TensorFlow物体检测API，像这样：
- en: '[PRE81]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: That's it! Let's get started.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！让我们开始吧。
- en: How to do it…
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Follow these steps to learn how to use TFHub to detect objects in your own
    photos:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤学习如何使用TFHub检测你自己照片中的物体：
- en: 'Import the packages we''ll need:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入我们需要的包：
- en: '[PRE82]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Define a function to load an image into a NumPy array:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将图像加载到NumPy数组中：
- en: '[PRE83]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Define a function to make predictions with a model, and save the results to
    disk. Start by loading the image and passing it through the model:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，通过模型进行预测，并将结果保存到磁盘。首先加载图像并将其传入模型：
- en: '[PRE84]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Convert the results to NumPy arrays:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果转换为NumPy数组：
- en: '[PRE85]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Create a visualization of the detections with their boxes, scores, and classes:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含检测框、得分和类别的可视化结果：
- en: '[PRE86]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Save the result to disk:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果保存到磁盘：
- en: '[PRE87]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Load `COCO`''s category index:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`COCO`的类别索引：
- en: '[PRE88]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Load Faster R-CNN from TFHub:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从TFHub加载Faster R-CNN：
- en: '[PRE89]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Run Faster R-CNN over all test images:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在所有测试图像上运行Faster R-CNN：
- en: '[PRE90]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'After a while, the labeled images should be in the `output` folder. The first
    example showcases the power of the network, which detected with 100% confidence
    the two elephants in the photo:'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一段时间后，标注过的图像应该会出现在`output`文件夹中。第一个示例展示了网络的强大能力，它以100%的信心检测到了照片中的两只大象：
- en: '![Figure 9.7 – Both elephants were detected, with a perfect score'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.7 – 两只大象被检测到，且得分完美'
- en: '](img/B14768_09_007.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_09_007.jpg)'
- en: Figure 9.7 – Both elephants were detected, with a perfect score
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 两只大象被检测到，且得分完美
- en: 'However, there are instances where the model makes some mistakes, like this:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有模型出现一些错误的情况，像这样：
- en: '![Figure 9.8 – The network mistakenly detected a person in the tablecloth'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.8 – 网络错误地将桌布中的一个人检测出来'
- en: '](img/B14768_09_008.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_09_008.jpg)'
- en: Figure 9.8 – The network mistakenly detected a person in the tablecloth
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – 网络错误地将桌布中的一个人检测出来
- en: In this example, the network detected a person in the tablecloth, with 42% certainty,
    although it correctly identified my dog as a Pug, with 100% accuracy. This, and
    other false positives, can be prevented by increasing the `min_score_thresh` value
    passed to the `visualize_boxes_and_labels_on_image_array()` method in *Step 5*.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，网络将桌布中的一个人检测出来，置信度为42%，虽然它正确识别了我的狗是巴哥犬，准确率为100%。通过提高传递给`visualize_boxes_and_labels_on_image_array()`方法的`min_score_thresh`值，可以防止这种误报和其他假阳性。
- en: Let's head to the next section.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进入下一部分。
- en: How it works…
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: In this recipe, we leveraged the ease of use of the powerful models that live
    in TFHub to perform out-of-the-box object detection with fairly good results.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们利用了TFHub中强大模型的易用性，进行开箱即用的物体检测，并取得了相当不错的结果。
- en: Why should we consider TFHub a viable option to satisfy our object detection
    needs? Well, the vast majority of the models there are really challenging to implement
    when starting from scratch, let alone training them to achieve decent results.
    On top of this, these complex architectures have been trained on `COCO`, a massive
    corpus of images tailored for object detection and image segmentation tasks. Nevertheless,
    we must keep in mind that we cannot retrain these networks and, therefore, they
    will work best on images containing objects that exist in `COCO`. If we need to
    create our own custom object detectors, the other strategies covered in this chapter
    should suffice.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们应该将TFHub视为满足物体检测需求的可行选择呢？好吧，那里绝大多数模型在从零开始时实现起来非常具有挑战性，更不用说训练它们以达到可接受的结果了。除此之外，这些复杂的架构是在`COCO`上训练的，`COCO`是一个庞大的图像数据集，专门用于物体检测和图像分割任务。然而，我们必须牢记，无法重新训练这些网络，因此它们最适用于包含`COCO`中已有物体的图像。如果我们需要创建自定义物体检测器，本章中介绍的其他策略应该足够了。
- en: See also
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You can access the list of all available object detectors in TFHub here:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此访问TFHub中所有可用物体检测器的列表：
- en: '[https://tfhub.dev/tensorflow/collections/object_detection/1](https://tfhub.dev/tensorflow/collections/object_detection/1)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://tfhub.dev/tensorflow/collections/object_detection/1](https://tfhub.dev/tensorflow/collections/object_detection/1)'
