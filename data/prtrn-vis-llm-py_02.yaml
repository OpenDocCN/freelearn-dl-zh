- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: 'Dataset Preparation: Part One'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集准备：第一部分
- en: In this chapter, we will begin to discuss what you’ll need in your dataset to
    start a meaningful pretraining project. This is the first of two parts on dataset
    preparation. It opens with some business guidance on finding a good use case for
    foundation modeling, where the data becomes instrumental. Then, focusing on the
    content of your dataset, we use qualitative and quantitative measures to compare
    it with datasets used to pretrain other top models. You’ll learn how to determine
    whether your datasets are “large enough” and “good enough” to boost accuracy while
    pretraining. We discuss bias identification and mitigation, along with multilingual
    and multimodal solutions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开始讨论启动有意义的预训练项目所需的数据集内容。这是关于数据集准备的两部分内容中的第一部分。首先，我们提供一些关于如何为基础建模找到合适用例的业务指导，其中数据起着至关重要的作用。然后，专注于数据集的内容，我们使用定性和定量的标准将其与用于预训练其他顶级模型的数据集进行比较。你将学会如何判断你的数据集是否“足够大”和“足够好”，以在预训练时提高准确性。我们还将讨论偏差识别与缓解，以及多语言和多模态解决方案。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: A business-level discussion on finding datasets and use cases for foundation
    modeling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论如何为基础建模寻找数据集和用例的业务层面内容
- en: Evaluating your dataset by comparing it to ones available in the open source
    research community
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将你的数据集与开源研究社区中可用的数据集进行对比来评估它
- en: Using scaling laws to size your dataset appropriately
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用扩展定律来合理规划你的数据集规模
- en: Bias detection and mitigation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差检测与缓解
- en: Dataset enhancements – multilingual and augmentation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集增强——多语言和数据增强
- en: Finding a dataset and use case for foundation modeling
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为基础建模寻找数据集和用例
- en: Datasets – we love them, we struggle with them, we rely on them, and we ignore
    them, oftentimes all at once. Every transaction, every digital moment, every archive,
    and every snapshot is a candidate for inclusion in a dataset. If your organization
    has already gone through a digital transformation, or if you’re digitally native,
    odds are you are already heavily invested in some data storage solution. Whether
    it’s on-premises or in the cloud, every organization needs a secure, reliable,
    operational, and robust solution to store countless types of data. The major question
    for you right now is, how can I monetize that? How can I lean into what is most
    unique about my organization’s history and strengths, and capitalize on it to
    develop net-new capabilities that further my own competitive advantage?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集——我们热爱它们，挣扎于它们，依赖它们，也常常忽视它们，往往是同时发生的。每一笔交易、每一个数字瞬间、每一个档案、每一张快照都可能成为数据集的候选项。如果你的组织已经经历了数字化转型，或者你是数字原生企业，那么很可能你已经大量投资于某些数据存储解决方案。无论是本地存储还是云存储，每个组织都需要一个安全、可靠、可操作且强大的解决方案来存储各种数据类型。此时你面临的主要问题是，我如何通过这些数据进行盈利？我如何利用我组织历史和优势中最独特的部分，并通过它开发新的能力，进一步提升我的竞争优势？
- en: For companies that already have machine learning models deployed into production
    applications, an easy way to find a candidate dataset for a foundation modeling
    project is to ask yourself, what is the single common denominator in all of my
    models? What domains, what modalities, and what signals do those models rely on,
    and what resources can I draw on to increase the overall intelligence of these
    models?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于已经在生产应用中部署机器学习模型的公司，找到一个适合基础建模项目的候选数据集的简单方法是问自己，所有模型的共同点是什么？这些模型依赖于哪些领域、哪些模式、哪些信号，哪些资源可以用来提高这些模型的整体智能？
- en: 'One common mental exercise is to consider the interactions that are core to
    your business. From search to customer support, supply chain, product development
    and maintenance, marketing, and so on, each of your lines of business involve
    decision making on a regular basis. Now, ask yourself the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的思维练习是考虑与业务核心相关的互动。从搜索到客户支持、供应链、产品开发和维护、营销等，你的每一条业务线都涉及到定期的决策过程。现在，问问自己以下问题：
- en: What if I could improve the accuracy of this decision making by 1%?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我能将决策准确性提高1%呢？
- en: What if I could increase my marketing lead conversion by 1%?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我能将我的营销潜在客户转化率提高1%呢？
- en: What if I could recommend better content to customers by 1%?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我能为客户推荐更好的内容，提高1%呢？
- en: What if I could increase my operational efficiency by 1%?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我能将我的运营效率提高1%呢？
- en: What if I could answer questions more accurately by 1%?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我能更准确地回答问题，提高1%呢？
- en: What if I could deliver my products faster by 1%?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我能让我的产品交付速度提高1%，会怎样呢？
- en: Once you’ve found a certain aspect of your business that’s most interesting
    to you or where you think the impact of your investment could be the highest,
    try to quantify this number. Will an increase in accuracy by 1% give you $50,000?
    What about $500,000? Maybe even $1,000,000? Many multiples of that? I’m stating
    the obvious here, but all things being equal, higher is clearly better. You want
    to pick an area you think will have the absolute maximum return on your investment.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你找到自己最感兴趣的业务领域，或者你认为投资影响最大的一块，尝试量化这个数字。准确度提高1%会为你带来$50,000吗？那$500,000呢？甚至$1,000,000？或者更多倍的收益？我说的显而易见，但所有条件相同的情况下，显然数字越高越好。你要选择一个你认为能带来最大投资回报的领域。
- en: Now, once you have that area of your organization identified, take 10% of the
    total estimated earnings, or some other low percentage you feel more comfortable
    with. That is your maximum compute budget. Now, don’t worry – we’re not going
    to blow through that all at once. You may not even need to spend it all. As we
    step through this book, I’ll help you figure out how to get early signals that
    your project is going to be successful, such as training on 1% of your data to
    ensure the performance is better than open source models. You want to hit key
    milestones throughout your pretraining project, and as you hit those milestones,
    you will get closer to achieving your end goal. That overall goal is also the
    same number you’ll use to figure out how much time it’s worth for you to spend
    on this project, how many people you’ll want to pull in, how many sprints to use,
    and so on.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦你确定了组织中的那个领域，拿出总预估收入的10%，或者是你觉得更合适的其他低百分比。这就是你的最大计算预算。别担心——我们不会一次性花光这些预算。你甚至可能根本不需要花完它。随着我们逐步阅读本书，我将帮助你弄清楚如何通过早期信号判断你的项目是否会成功，例如在`1%`的数据上进行训练，以确保性能优于开源模型。在你的预训练项目中，你需要达到关键的里程碑，随着你完成这些里程碑，你会越来越接近最终目标。这个总体目标也将是你用来决定花费多少时间、需要多少人手、要使用多少次冲刺等的数字。
- en: Once you have this target application in mind, along with both the estimated
    return and costs, you are ready to start bringing it to life! Start listing any
    datasets your organization already stores that are related to the application
    you want to build. Do you have relational databases with transactions relevant
    to this? Customer history? Click-stream data? Search results? What about images?
    Do you have any videos? Any audio files? Any experimental results? Push yourself
    to be creative in listing as many candidate datasets as you have. Consider taking
    a solid hour just to look around and see what your organization already has stored
    around this candidate application area. If it’s already mission-critical, then
    most likely you have quite a bit stored already.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了这个目标应用，并且清楚了预估的回报和成本，你就可以开始将其付诸实践了！开始列出你的组织中已经存储的、与您想要构建的应用相关的任何数据集。你是否有与此相关的事务型关系数据库？客户历史记录？点击流数据？搜索结果？那图像呢？有没有视频？有没有音频文件？有没有实验结果？尽量发挥创意，列出你所拥有的尽可能多的候选数据集。考虑花一个小时四处看看，看看你的组织已经存储了哪些与这个候选应用领域相关的数据。如果它已经是关键任务系统，那么很可能你已经存储了不少数据。
- en: If you don’t already have at least a few GB of data or, even better, a few 10s
    of GB, then you might want to consider gathering a new dataset from open source
    solutions. These might include any combination of over 6,000 datasets available
    through the *Papers With Code* site *(1)*. You can also look at the 8,000 datasets
    available from the *Hugging Face Hub* *(2)*. Remember, these datasets are available
    at no cost! Open source datasets are an excellent way to start proving the concept
    of your idea. Common datasets for language pretraining are *The Pile*, *Common
    Crawl*, *Wikipedia*, *CodeParrot*, and so on *(3)*. You can also look at the OSCAR
    corpus for multimodal pretraining. The **Vision Transformer** (**ViT**) *(4)*
    model was trained from scratch on ImageNet. You have plenty of options! You can
    also use all of these open source datasets to enhance your original dataset, using
    the best of both open source and proprietary options.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有至少几个GB的数据，或者更好的是，几十GB的数据，那么你可能需要考虑从开源解决方案中收集一个新的数据集。这些可能包括通过*Papers With
    Code*网站*(1)*提供的6000多个数据集的任何组合。你也可以查看*Hugging Face Hub*提供的8000个数据集*(2)*。记住，这些数据集是免费的！开源数据集是开始验证你想法概念的绝佳方式。常见的语言预训练数据集包括*The
    Pile*、*Common Crawl*、*Wikipedia*、*CodeParrot*等*(3)*。你还可以查看用于多模态预训练的OSCAR语料库。**视觉变换器**（**ViT**）*(4)*模型就是从头开始在ImageNet上训练的。你有很多选择！你也可以利用所有这些开源数据集来增强你的原始数据集，结合开源和专有选项的最佳部分。
- en: Something else to remember is that *pretraining explicitly benefits from unlabeled
    data*. The best pretraining projects happen when they leverage large volumes of
    unlabeled data and smaller volumes of labeled data. This is largely why pretraining
    foundation models is popular – most data in the world isn’t labeled. However,
    when we use a pretraining objective, as we learned about in the first chapter,
    we can easily train models to learn about this. Then, we fine-tune them using
    supervised data, which is usually smaller in quantity.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要记住的事情是，*预训练明显受益于未标记数据*。最好的预训练项目是在利用大量未标记数据和较少量的标记数据时发生的。这也是为什么预训练基础模型如此流行的原因——世界上大多数数据都没有标记。然而，当我们使用预训练目标时，正如我们在第一章中学到的，我们可以轻松地训练模型来学习这些数据。然后，我们使用监督数据对模型进行微调，通常这些数据量较少。
- en: So, if you find yourself in a scenario with multiple hundreds of GBs of data,
    such as images, files, records, transactions, metadata, and time-series data,
    you may want to consider that as a top candidate for custom pretraining.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你发现自己处于一个拥有数百GB数据的场景中，例如图像、文件、记录、交易、元数据和时间序列数据，那么你可能需要考虑将其作为定制预训练的首选候选。
- en: Top pretraining use cases by industry
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 按行业划分的顶级预训练应用场景
- en: 'Now, let’s highlight a few top use cases to pretrain custom foundational models
    by industry. These are areas in our world where pretraining and finetuning are
    already having an impact today:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们突出展示一些按行业定制预训练基础模型的顶级应用场景。这些是我们世界中，预训练和微调已经在今天产生影响的领域：
- en: '**Software** **and internet**:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软件** **和互联网**：'
- en: Search and discovery
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索与发现
- en: Generating documentation and code
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成文档和代码
- en: Questioning/answering
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问答
- en: '**Hospitality** **and travel**:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**酒店业** **和旅游业**：'
- en: Customer support and service
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户支持和服务
- en: Booking recommendations
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预定推荐
- en: '**Consumer electronics**:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费电子**：'
- en: Design automation
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计自动化
- en: Fashion design automation
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时尚设计自动化
- en: '**Financial services**:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融服务**：'
- en: Document summarization and generation
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档总结和生成
- en: Multimodal forecasts
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态预测
- en: '**Media** **and entertainment**:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**媒体** **和娱乐**：'
- en: Creativity enhancement
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创意增强
- en: Speeding up the generation of creatives (new images, new movies, better artifacts,
    and so on)
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速创意生成（新图像、新电影、更好的艺术品等）
- en: Identifying the best creative to move to finals (best shot, best sequence, best
    melody, and so on)
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定最佳创意进入决赛（最佳镜头、最佳镜头序列、最佳旋律等）
- en: '**Health care and** **life sciences**:'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医疗保健和** **生命科学**：'
- en: Protein modeling, drug discovery, and experimental prioritization
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蛋白质建模、药物发现和实验优先排序
- en: Notes synthesis and diagnosis confirmation
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录综述和诊断确认
- en: Visual results confirmation and experiment result prioritization
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉结果确认和实验结果优先排序
- en: Scientific literature synthesis and experimental design suggestion
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 科学文献综述和实验设计建议
- en: '**Manufacturing** **and agriculture**:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**制造业** **和农业**：'
- en: Part defects and building error detection
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部件缺陷和构建错误检测
- en: Overall design automation of parts and products
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部件和产品的整体设计自动化
- en: Autonomous product design
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自主产品设计
- en: '**Public** **sector governance**:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**公共** **部门治理**：'
- en: Policy impact analysis automation
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政策影响分析自动化
- en: Policy suggestion automation
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政策建议自动化
- en: Political and philosophical difference reconciliation
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政治和哲学差异调解
- en: Budget impact assessment and analysis automation
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预算影响评估和分析自动化
- en: Let’s now move on to see how different your dataset is.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续看看你的数据集有多不同。
- en: Delta – how different is your dataset?
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Delta ——你的数据集有多不同？
- en: 'Now that you have some idea of what use case you are most interested in, and
    what datasets will give your organization the most value, it’s time to understand
    how unique your dataset is. This analysis matters because it will answer two questions:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经有了对最感兴趣的应用场景的某种理解，并且知道哪些数据集能为你的组织带来最大价值，现在是时候了解你的数据集有多独特了。这一分析很重要，因为它将回答两个问题：
- en: First, which models are already on the table for you to use, due to having been
    trained on similar data?
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，哪些模型已经可以供你使用，因为它们已经在类似的数据上训练过？
- en: Second, how well have those models performed?
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，这些模型的表现如何？
- en: This insight will start to give you a clue toward what performance you can hope
    to achieve on your datasets as a best-case scenario. Then, we’ll plug that expected
    performance number back into our total project value and make sure we’re still
    on track. The next chapter is completely dedicated to answering those questions.
    Here, we’ll learn how to pick apart your dataset. This is a good section for those
    who are new to data analysis.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这一见解将开始为你提供一个线索，指引你在最佳情况下能期望在数据集上实现的性能。然后，我们将把这个预期的性能数值重新带入我们整体项目的价值中，确保我们仍然在正确的轨道上。下一章将完全致力于回答这些问题。在这里，我们将学习如何分析你的数据集。这一部分对于那些刚接触数据分析的人来说非常有帮助。
- en: First, it’s always a good idea to spend time really analyzing any dataset you’re
    touching. Whether you are starting from something custom in your own database,
    or working with an open source option, anticipate spending at least a few hours
    getting to know it in little detail. Probably the best phrase I’ve heard to inspire
    this process is, *a good data science team asks more questions than they answer*.
    This is because *the act of analyzing a dataset is a living process, not a finite
    state*. Getting to know a dataset is a little bit like getting to know a person;
    it’s just that the way you ask questions and make observations is totally different.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，花时间真正分析你正在处理的任何数据集总是一个好主意。无论你是从自己数据库中的定制数据开始，还是使用开源选项，预计至少会花上几个小时来详细了解它。我听过的最能激励这个过程的短语是，*一个好的数据科学团队提出的问题比他们回答的问题还要多*。这是因为*分析数据集的过程是一个动态的过程，而不是一个有限的状态*。了解一个数据集有点像了解一个人；只不过你提问和观察的方式完全不同。
- en: Start by verbally describing the contours of your dataset. How many rows does
    it have? How many columns? How large are the images? How many tokens does it have?
    What features does it have? Are they numeric or categorical? Is it based on time?
    What metadata does it have? Make sure you have a good picture in your mind of
    what this dataset looks like. Talk with other people on your team about it until
    you feel confident and can answer questions quickly about the basics of your dataset
    composition. Use common data analysis techniques, such as Jupyter notebooks, to
    produce summary statistics and charts, and perform exploratory data analysis.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从口头描述你的数据集的轮廓开始。它有多少行？有多少列？图像有多大？它有多少个标记？它有哪些特征？这些特征是数值型的还是类别型的？它是基于时间的吗？它有哪些元数据？确保你对这个数据集的样貌有清晰的认知。和你团队中的其他人讨论，直到你感到自信，并能快速回答有关数据集组成的基本问题。使用常见的数据分析技术，如Jupyter笔记本，生成总结性统计和图表，并进行探索性数据分析。
- en: Critically, ask yourself, what real-world process was this dataset drawn from?
    How was this dataset acquired? We call this a sampling mechanism. If you are new
    to data analysis, and especially new to data analysis in applied settings outside
    of theoretical research, the first thing you’ll need to understand is that “not
    all sampling mechanisms are perfect.” To put it another way, you should get into
    the practice of assuming that there may be something wrong with your dataset.
    You need to critically evaluate the way your dataset was developed. Was it randomly
    collected? Or does all of the data have some underlying similarities? Any errors?
    The most important part of your data analysis process is to disabuse yourself
    of any underlying errors, inconsistencies, oddities, and faults in the raw data
    itself. You need to gain certainty that the data itself is indeed valid and reliable.
    Why? Because this certainty serves as a fundamental guarantee for everything you
    produce from this dataset. If the data isn’t reliable, your work can never be
    reliable.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是要问自己，这个数据集是从哪个现实世界过程中抽取出来的？这个数据集是如何获取的？我们称这为抽样机制。如果您是数据分析新手，尤其是在应用设置中进行数据分析而不是在理论研究中，您首先需要了解的是“并非所有抽样机制都是完美的”。换句话说，您应该习惯于假设您的数据集可能存在问题。您需要批判性地评估数据集的开发方式。它是随机收集的吗？或者所有数据都具有某些潜在的相似性？有任何错误吗？数据分析过程中最重要的部分是消除原始数据本身的任何潜在错误、不一致性、奇怪之处和缺陷。您需要确信数据本身确实是有效且可靠的。为什么？因为这种确定性是您从这些数据集产生的一切内容的基本保证。如果数据不可靠，您的工作永远不可能是可靠的。
- en: When you have an idea about your dataset, before that idea is proven true by
    the results you empirically observe, it’s called a **hypothesis**. A hypothesis
    is a concept you believe may be true about your dataset, or about any real-world
    process. However, because you currently lack empirical evidence validating the
    certainty of this hypothesis, you can’t state at the current time that it is objectively
    true. That’s why we call it a hypothesis!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当您对数据集有了想法，但在您通过实证观察到的结果证明之前，这被称为**假设**。假设是您认为可能适用于您的数据集或任何现实世界过程的概念。然而，由于目前缺乏实证证据来验证此假设的确定性，因此您不能断言它在当前时间是客观真实的。这就是为什么我们称其为假设！
- en: A core part of the scientific process, and as a corollary, your own development
    in machine learning is learning how to state this hypothesis clearly. You can
    phrase it as a simple question, something as basic as “which model solves this
    problem the best?”, “what does it mean to solve this type of problem optimally?”,
    or even, “how can we improve upon the state of the art in this area?”
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 科学过程的核心部分，以及作为推论的，您在机器学习中自身的发展，是学会如何清晰地陈述这一假设。您可以将其表述为一个简单的问题，例如“哪种模型最好地解决了这个问题？”，“什么是最优解决这种类型问题的意义？”，或者甚至，“我们如何在这个领域改进现有技术水平？”
- en: Once you have a hypothesis, also called a **research objective**, clearly stated,
    you then want to learn “how to design experiments that answer this question.”
    Experimental design is a surprisingly challenging skill! This includes the work
    of evaluating current research in certain areas, considering open questions and
    results others have demonstrated, and attempting to build upon them empirically.
    At the end of your project, you want to have clear empirical results you can point
    to that validate your work. We’ll discuss this more in the following chapters
    on model evaluation, but it’s a critical topic to keep in mind.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您有了假设，也称为**研究目标**，清晰地表达出来，接下来您需要学习“如何设计实验来回答这个问题”。实验设计是一项令人惊讶地具有挑战性的技能！这包括评估某些领域的当前研究工作，考虑开放性问题和他人展示的结果，并试图从经验上构建。在项目结束时，您希望有明确的经验性结果可以证明您的工作。我们将在关于模型评估的后续章节中进一步讨论这一点，但这是一个需要牢记的关键话题。
- en: Next, let’s learn about sizing our datasets.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们学习如何调整我们的数据集大小。
- en: Use the scaling laws to size your datasets
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用缩放定律来确定您的数据集大小。
- en: At this point, you should have identified your datasets, have a basic understanding
    of them, and be able to describe how they are similar to and different from previous
    datasets and research work in your chosen domain. It’s helpful to have at least
    a handful of papers to refer to so that you can do so when you’re stuck.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您应该已经确定了您的数据集，并对其有了基本的理解，能够描述它们与以前的数据集及您选择的领域中的研究工作的相似之处和不同之处。有几篇论文可以参考会很有帮助，这样在遇到困难时就能派上用场。
- en: In this section, we’ll explore how large your dataset should be in order to
    produce the expected results on a pretraining or fine-tuning project, which clearly
    validates the time and compute expenses you’ll be racking up. We’ll also discuss
    certain characteristics you’ll want this dataset to have, such as sufficient variety,
    quality, and lack of duplicates. The entirety of the next chapter is dedicated
    to picking the right model, including the size and scope, but for now, we’ll focus
    on the dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨数据集应该有多大，才能在预训练或微调项目中产生预期的结果，从而清楚地验证你将投入的时间和计算开销。我们还将讨论你希望这个数据集具备的某些特征，比如足够的多样性、质量和没有重复项。下一章的内容将完全专注于选择合适的模型，包括模型的大小和范围，但现在我们将重点讨论数据集。
- en: First, it’s helpful to know that there is a very large gray area between so-called
    large and small models and the corresponding size in datasets that they tend to
    run on. Under no circumstances should you think that you only need multiple terabytes
    and/or petabytes to think about pretraining, or even models that don’t fit on
    a single GPU. You can produce meaningful results with unsupervised data simply
    by continuing to pretrain your model, rather than necessarily starting pretraining
    from scratch, and still hit your business and intellectual goals. Depending on
    your project, and how niche and interesting it may be, you can easily showcase
    some useful work on just under 1 GB of data. So, don’t hesitate just because you
    aren’t sitting on the Fort Knox of all web data; just start from where you are!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，了解所谓的大模型和小模型之间，以及它们通常运行的数据集大小之间有一个非常大的灰色地带是很有帮助的。在任何情况下，你都不应该认为，只有多达数TB和/或PB的数据才能考虑预训练，或者认为即使是无法放入单个GPU的模型也不值得考虑。你可以通过继续预训练你的模型，而不必从零开始预训练，单凭无监督数据就能产生有意义的结果，并且仍然可以实现你的商业和知识目标。根据你的项目以及它可能的独特性和有趣程度，你完全可以在不到1
    GB的数据上展示一些有用的工作。因此，不要因为没有大量网络数据而犹豫不决；从你现有的资源开始吧！
- en: Next, you’ll need to understand something called the scaling laws *(6)*. These
    are a set of theories and formulas about how large models behave at different
    scales, notably as power laws. These formulas themselves are derived from empirical
    behavior at varying scales. You can use them to determine what model and dataset
    sizes are optimal for a given compute budget, and vice versa. To some degree,
    these laws, and their updated versions as presented in *Chinchilla*, are independent
    of the model architecture itself. This implies that the biggest way to improve
    model accuracy is scaling up the size, rather than alterations in the model architecture
    itself. Kaplan originally presented scaling laws explicitly within the context
    of language models leveraging the transformer model architecture. However, given
    the 10x increase in accuracy that this validated hypothesis gave rise to in the
    GPT-3 paper, I and many others *(7)* believe there is a reason to explore this
    basic relationship outside of **large language models** (**LLMs**), including
    vision and multimodal especially.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要理解一种叫做扩展法则的东西 *(6)*。这些是关于大模型在不同规模下如何表现的理论和公式，尤其是作为幂律。这些公式本身是通过不同规模的经验行为推导出来的。你可以用它们来确定给定计算预算下，什么样的模型和数据集大小是最优的，反之亦然。在某种程度上，这些法则，以及在*Chinchilla*中提出的更新版本，与模型架构本身是独立的。这意味着，提高模型准确性的最大方式是扩大规模，而不是改变模型架构本身。Kaplan最初在语言模型的背景下，明确提出了扩展法则，并利用了变换器模型架构。然而，考虑到这一被验证的假设在GPT-3论文中引发了准确度的10倍提升，我和许多其他人
    *(7)* 相信，有理由在**大语言模型**（**LLMs**）之外探索这种基本关系，特别是在视觉和多模态领域。
- en: You might be thinking, so what? Why is this such a big deal? Obviously, there’s
    some balance you’d want to achieve across your dataset, compute size, and model,
    so what gives? The reason Kaplan’s work was such a breakthrough is that *having
    a valid formula to quantify the optimal compute, data, and model values lets you
    estimate what range of loss your model might achieve*. To put it another way,
    now that we have scaling laws, we can figure out mathematically what loss we should
    expect at the end of our model training run, within a given range. And for training
    runs that can send compute costs into the hundreds of thousands of dollars, if
    not millions, this knowledge is incredibly valuable. OpenAI has validated this
    in its GPT-4 technical report, claiming to be able to accurately forecast its
    model’s loss given changes in scale.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，那又怎样？为什么这会是一个大问题？显然，你会希望在数据集、计算资源和模型之间取得某种平衡，那么，问题出在哪里呢？Kaplan 的研究之所以具有突破性，是因为*拥有一个有效的公式来量化最优的计算、数据和模型值，可以让你估算出模型可能达到的损失范围*。换句话说，现在我们有了尺度定律，我们可以通过数学推导出在给定范围内，模型训练结束时应该预期的损失是多少。对于那些计算成本可能达到数十万美元，甚至数百万美元的训练任务来说，这种知识极为宝贵。OpenAI
    在其 GPT-4 技术报告中验证了这一点，声称能够根据规模变化准确预测其模型的损失。
- en: This opens a new area of questions. What other aspects of machine learning have
    empirically observable laws? In what other ways can we be inspired by physics
    to discover formulaic patterns that rely on mathematical relationships, beyond
    the inner workings of the model itself? This matters because, today, the vast
    majority of machine learning is trial and error. We hope something works, we try
    it out, learn from our experiment, and then take another step. However, I believe
    scaling laws point to a future where machine learning is increasingly enhanced
    with simple, efficient, and fast checks, rather than long-running computational
    experiments. What if we’ve simply been thinking about this in the wrong way for
    decades?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们开辟了一个新的问题领域。机器学习的其他哪些方面也有经验可观测的定律？除了模型本身的内部运作外，我们还能从物理学中获得哪些灵感，发现依赖于数学关系的公式化模式？这个问题非常重要，因为如今，绝大多数机器学习仍然是试验和错误的过程。我们希望某种方法能奏效，尝试后从实验中学习，然后再采取下一步。然而，我认为尺度定律指向了一个未来，在这个未来中，机器学习将越来越多地通过简单、高效、快速的检查来增强，而不是依赖漫长的计算实验。如果我们这些年来一直在错误地思考这一问题呢？
- en: Fundamentals – scaling laws of neural language models
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础知识——神经语言模型的尺度定律
- en: 'If you take a look at the original paper: *Scaling Laws of Neural Language
    Laws*, you’ll find out that one core concept is central to their analysis – proportionality.
    Kaplan et al here argue that changes in your dataset size or model size should
    be accompanied by proportional changes in the companion quantity. To put it another
    way, if you use a bigger dataset, you should use a bigger model, and vice versa.
    Now, exactly how strong this relationship is, what describes it, what constants
    are involved, and precisely how much scaling should be undertaken up or down is
    entirely at the heart of their paper.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看原始论文：《神经语言法则的尺度定律》（*Scaling Laws of Neural Language Laws*），你会发现有一个核心概念贯穿整个分析——比例性。Kaplan
    等人在文中指出，当数据集大小或模型大小发生变化时，伴随其变化的数量也应该成比例变化。换句话说，如果你使用更大的数据集，就应该使用更大的模型，反之亦然。那么，这种关系的强度究竟有多大？是什么描述了它？涉及哪些常数？应该做多大的缩放调整，这些问题恰恰是他们论文的核心内容。
- en: 'While it is helpful to know that a relationship is proportional, it is insufficient.
    Kaplan et al suggest and find empirically that the optimal scaling of neural language
    models follows a power law. Power laws are actually quite simple; they’re just
    about exponents at the end of the day. If two quantities follow a power law, you
    can assume that one side of the equation follows exponential change:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管知道关系是成比例的很有帮助，但这仍然不够。Kaplan 等人建议并通过经验发现，神经语言模型的最优尺度遵循幂律。幂律实际上非常简单；归根结底，它们只是关于指数的。如果两个量遵循幂律，你可以假设方程的一侧遵循指数变化：
- en: '![](img/B18942_02_001.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18942_02_001.jpg)'
- en: To estimate the early-stopped test loss of a transformer-based training regime,
    given the size of both a dataset and a model, Kaplan et al suggest the following.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估算基于 Transformer 的训练方案中，早停时的测试损失，考虑到数据集和模型的大小，Kaplan 等人提出了以下建议。
- en: 'Let’s try to unpack this in very simple terms – first, the left-hand side:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试用非常简单的术语来解释——首先是左侧部分：
- en: '**L** = the final loss of your model, stopped early, on your test set'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**L** = 模型在测试集上早停时的最终损失'
- en: '**N** = the number of trainable parameters in your model'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**N** = 你模型中可训练参数的数量'
- en: '**D** = the size of your dataset in tokens (for language)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**D** = 数据集中令牌的大小（对于语言）'
- en: Now, you understand that the entire equation is about computing the potential
    loss of your model, which would be great to know ahead of time! The rest of the
    terms on the right-hand side are about how to get there. All four of ![](img/B18942_02_002.png),
    ![](img/B18942_02_003.png), ![](img/B18942_02_004.png), and ![](img/B18942_02_005.png)
    describe constants that must be discovered from the dataset and training regime.
    Think of these as hyperparameters; we want to find some constant terms that describe
    our specific dataset and model. In many cases, however, we can simply use the
    constants as presented in their work.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经理解整个方程是关于计算模型的潜在损失，这样你就可以提前知道了！右侧的其余项是关于如何达到这个目标的。所有这四个项：![](img/B18942_02_002.png)、![](img/B18942_02_003.png)、![](img/B18942_02_004.png)
    和 ![](img/B18942_02_005.png) 都描述了必须从数据集和训练过程中发现的常数。可以把它们看作超参数；我们希望找到一些常数项，用以描述我们的特定数据集和模型。然而，在许多情况下，我们可以直接使用他们工作中呈现的常数。
- en: Kaplan et al found constant values for each of these in their training runs
    by fitting the loss curves with their scaling law functions. Using mathematical
    extensions of their core equations, they were able to accurately fit their learning
    curves. Making that fit helped them discover constants that proved useful throughout
    the rest of their studies.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Kaplan 等人在他们的训练过程中通过拟合损失曲线与其缩放法则函数，发现了每个常数的值。利用他们核心方程的数学扩展，他们能够准确地拟合学习曲线。进行这种拟合帮助他们发现了在其余研究中都非常有用的常数。
- en: In the real world, once you’ve performed some preliminary data analysis and
    have a good idea of what characteristics you’ll need to train an adequate model,
    most data science teams will immediately move on to training your first model.
    This is because the machine learning process is generally iterative; you’ll test
    a variety of methods, see which ones are the most promising at a given point in
    time, scale and evaluate, and then try again. For the purposes of a larger book
    on the topic, I’ll go into more detail on two key topics that can help you improve
    your dataset. These are steps you probably wouldn’t implement right at the beginning
    of your data science journey but that you should come back to over time to increase
    the overall quality of your work. The first is bias detection and mitigation,
    and the second is dataset enhancements.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，一旦你进行了一些初步的数据分析，并对你需要的特征有了清晰的了解，大多数数据科学团队会立即开始训练你的第一个模型。这是因为机器学习过程通常是迭代的；你会测试多种方法，看看在特定时间点哪些方法最有前景，然后扩大规模并进行评估，接着再尝试。为了更全面地阐述这个主题，我将详细介绍两个可以帮助你改善数据集的关键话题。这些步骤你可能不会在数据科学旅程的最初就实施，但随着时间的推移，你应该不断回顾它们，以提高工作整体质量。第一个是偏差检测与缓解，第二个是数据集增强。
- en: Bias detection and mitigation
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差检测与缓解
- en: The trajectory of the word “bias” is interesting in that, in the last 15 years,
    it’s come full circle. Originally, *bias* was arguably a statistical term. Formally,
    it implied that a sample size was improperly constructed, giving excessive weight
    to certain variables. Statisticians developed numerous methods to identify and
    reduce bias to evaluate studies properly, such as those used in randomized control
    trials in public health or policy evaluations in econometrics. Basic tactics include
    making sure that the treatment and control groups are roughly the same size and
    have roughly the same characteristics. Without a guarantee of that basic mathematical
    equivalence, or more realistically as close to it as the research team can get,
    it’s difficult to trust that the results of a study are truly valid. The results
    themselves are subject to bias, simply indicating the presence or absence of basic
    characteristics, rather than implying anything meaningful about the treatment
    itself.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: “偏差”一词的轨迹非常有趣，因为在过去的15年里，它已经走完了一圈。最初，*偏差*无疑是一个统计学术语。正式来说，它意味着样本大小不当，给某些变量赋予了过大的权重。统计学家们开发了许多方法来识别和减少偏差，以正确评估研究，例如公共卫生中的随机对照试验或计量经济学中的政策评估所使用的方法。基本策略包括确保处理组和控制组大致相同大小，并且具有大致相同的特征。如果没有保证这种基本的数学等价性，或者更现实地说，研究团队尽可能接近它，就很难相信研究结果是真正有效的。结果本身会受到偏差的影响，简单地指示基本特征的存在与否，而不是暗示关于治疗本身的任何有意义的信息。
- en: In the last 5 years, however, numerous studies have demonstrated the inability
    of machine learning models to perform adequately for certain groups of people
    under certain scenarios. The most egregious examples include facial recognition,
    image detection, employment, judicial decision-making, and countless others. Large
    technology companies have been the first to come under fire here, with financial
    institutions and even public policy organizations also coming in tow. These accusations
    are valid. While bias in datasets has always been a big problem in machine learning,
    the impact on human lives across the world is now so obvious that it deserves
    significant dialogue, discussion, solutioning, and monitoring. If bias is present
    in any dataset, it is almost certain to creep into the model itself. Models certainly
    are not objective; they are effectively children of the datasets they were trained
    on. Bias has now come full circle, starting in statistics, resonating with human
    rights, and now driving machine learning research.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在过去五年里，许多研究已经证明，机器学习模型在特定情境下无法对某些群体的人群做出适当的表现。最严重的例子包括面部识别、图像检测、就业、司法决策等无数案例。大型科技公司首先受到指责，金融机构甚至公共政策组织也纷纷受到波及。这些指控是有根据的。虽然数据集中的偏见一直是机器学习中的一个大问题，但如今它对全球人类生活的影响如此显著，值得进行充分的对话、讨论、解决和监控。如果某个数据集中存在偏见，那么几乎可以肯定它会渗透到模型本身中。模型当然不是客观的；它们实际上是它们所训练的那些数据集的“孩子”。偏见已经走到了一个完整的循环，最初在统计学中出现，与人权相关联，现在推动着机器学习研究。
- en: '*The word bias has now come full circle; starting in statistics, resonating
    with human rights, and now driving machine* *learning research.*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*“偏见”一词现在已经走到了一个完整的循环；最初在统计学中出现，与人权相关联，现在推动着机器* *学习研究。*'
- en: For the purposes of developing and attempting to deploy a machine learning model,
    and especially a large one with its own pretraining regime, you need to know a
    few things. First, the most reliable way to mitigate bias is by increasing and
    decreasing the different aspects of your datasets. This is especially obvious
    in computer vision. If you add more images of certain groups – for example, African
    Americans – your model will be able to recognize them. If you don’t have those
    images in sufficient numbers, your model won’t be able to recognize them in applications.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发和尝试部署一个机器学习模型，尤其是一个具有自己预训练机制的大型模型，你需要知道一些事情。首先，最可靠的减少偏见的方法是通过增加或减少数据集中的不同方面。特别是在计算机视觉领域，这一点尤为明显。如果你增加某些群体的图片——例如，非裔美国人——你的模型将能够识别他们。如果没有足够数量的这些图片，你的模型在应用中就无法识别这些群体。
- en: For natural language, this question ends up being even more challenging. This
    is because most of the data in language isn’t already tabulated into different
    social categories, such as gender, race, religion, and sexuality. For all of those
    types that we care about and know we want to protect, we need to introduce our
    own methods to identify, compare, and synthesize them across our datasets. Just
    doing this alone is tough, as you can imagine.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自然语言而言，这个问题变得更加具有挑战性。这是因为语言中的大部分数据并没有以性别、种族、宗教和性取向等不同的社会类别进行分类。对于我们关心的、并且知道需要保护的所有这些类别，我们需要引入自己的方法，以便在我们的数据集中识别、比较和综合它们。仅仅做到这一点就已经很难了，正如你可以想象的那样。
- en: 'Identifying bias is the first critical step in your journey toward responsible
    ML. Right at the beginning, you need to be able to answer two critical questions
    about your dataset:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 识别偏见是你迈向负责任机器学习之旅的第一步。从一开始，你就需要能够回答关于你的数据集的两个关键问题：
- en: First, what types of bias are present in my dataset currently?
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我的数据集中目前存在哪些类型的偏见？
- en: Second, how much risk does this bias expose to my organization?
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，这种偏见给我的组织带来了多大的风险？
- en: Think about risk in terms of impact on your customers, particularly the predictions
    from a biased ML model. If your model has the potential to cause harm to your
    customers, such as denying a loan, downgrading an employment submission, recommending
    harmful content, or even denying bail or other legal sentencing, then by all means
    make bias detection and mitigation your highest priority!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 从对客户的影响角度来思考风险，特别是来自有偏见的机器学习模型的预测。如果你的模型可能会对客户造成伤害，比如拒绝贷款、降低就业申请、推荐有害内容，甚至拒绝保释或其他法律判决，那么毫无疑问，应该把偏见检测和缓解作为你的最高优先事项！
- en: 'While there are a variety of frameworks concerning responsible AI, I like to
    boil these down to four key actions to take. In terms of bias in ML models trained
    on biased datasets, your four key steps are **expect**, **identify**, **mitigate**,
    and **monitor**:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然关于负责任的人工智能有各种框架，我喜欢将其归结为四个关键步骤。在训练偏差数据集的机器学习模型中，你的四个关键步骤是**预期**、**识别**、**缓解**和**监控**：
- en: '**Expect**: When picking ML projects and datasets, expect that every dataset
    will have some type of bias at the root. Ask yourself, what problem is my ML model
    trying to solve, and what issues will I run into if I don’t have enough of certain
    types of data?'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预期**：在选择机器学习项目和数据集时，预期每个数据集的根本上都会有某种偏差。问问自己，我的机器学习模型试图解决什么问题，如果我没有足够的某些类型数据，会遇到哪些问题？'
- en: '**Identify**: Then, use a variety of techniques to identify the bias present
    in your dataset. Make sure you know right at the outset how many attributes within
    certain groups you do or do not have. Keep working on this until you can quantify
    at least a handful of different types of bias metrics. See the following note
    box for some suggestions on how to identify bias.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**识别**：然后，使用各种技术来识别数据集中存在的偏差。确保从一开始就知道某些组中你是否有某些属性。不断进行这个过程，直到你能够量化出至少几种不同类型的偏差指标。请参见以下提示框，了解如何识别偏差的一些建议。'
- en: '**Mitigate**: Once you’ve identified the bias in your dataset numerically,
    take steps to mitigate the bias. Increase or decrease certain aspects of your
    dataset. Use augmentation, up- or down-sampling, and data transformations to drive
    down your bias metrics until they hit a less dangerous threshold.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**缓解**：一旦你在数据集中识别出偏差，采取措施来缓解它。增加或减少数据集中的某些方面。使用增强、上采样或下采样以及数据转换来降低偏差指标，直到它们达到一个较低的危险阈值。'
- en: '**Monitor**: Once you’ve deployed your ML model, the adventure continues. You
    can use the same bias detection methods you leveraged in *step 2* to monitor the
    model deployed in your application. Ensure that your application and overall system
    design include statistical monitoring and set thresholds for acceptable statistical
    levels. When the model starts to meet or exceed your thresholds, start manually
    reviewing the model predictions and initiate your training pipeline. Keeping humans
    in the loop, particularly those who are both knowledgeable and caring, is the
    best way to reduce the risk of biased predictions.'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监控**：一旦你部署了机器学习模型，冒险仍在继续。你可以使用在*步骤 2*中采用的相同偏差检测方法来监控你在应用程序中部署的模型。确保你的应用程序和整体系统设计包括统计监控，并为可接受的统计水平设置阈值。当模型开始达到或超过这些阈值时，开始手动审查模型预测并启动你的训练管道。保持人为干预，特别是那些既有知识又有责任心的人，是减少偏见预测风险的最佳方法。'
- en: How to do bias detection and monitoring
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如何进行偏差检测和监控
- en: 'Now that we know bias is important, how do we find it mathematically? And once
    we’ve done that, how do we mitigate and monitor? There are many ways of doing
    this, and we can categorize these in their respective domains:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道偏差很重要，那么我们如何在数学上找到它呢？一旦做到了这一点，我们如何缓解并监控它？有很多方法可以做到这一点，我们可以根据它们各自的领域将这些方法进行分类：
- en: '**Tabular**: Detecting bias in tabular data amounts to computing some statistics.
    First, you’ll need to have some ground truth label in your dataset, indicating
    the status inside or outside of a certain group. Notably, for many teams, this
    alone presents a sizeable problem. However, the logical counter to this is simple.
    Expect your data to be biased, regardless of whether or not you have a column
    labeling it as members of certain groups. Introducing this label *is the only
    way to identify bias intrinsic to your dataset and, ultimately, remove it*. Otherwise,
    try to use a proxy, although these are known to be faulty.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**表格数据**：检测表格数据中的偏差等同于计算一些统计数据。首先，你需要在数据集中有一些真实标签，指示某个组内或组外的状态。值得注意的是，对于许多团队来说，仅此一点就已是一个相当大的问题。然而，针对这一点的合理反应非常简单。预期你的数据会有偏差，无论你是否有一个列标记它为某些组的成员。引入这个标签*是识别数据集中固有偏差并最终消除它的唯一方法*。否则，尝试使用代理，但这些已知是有缺陷的。'
- en: Assuming you have a label, such as gender or race, then you have two types of
    metrics – pretraining and post-training metrics. One simple pretraining statistic
    is **class imbalance**. Class imbalance is simply the number of observations from
    your advantaged group, minus the number in the disadvantaged group, divided by
    your overall dataset size. If your class imbalance is too high, your dataset and
    subsequent model are certain to be biased.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个标签，例如性别或种族，那么你就有两种度量标准——预训练和后训练度量。一个简单的预训练统计量是**类别不平衡**。类别不平衡指的是来自你的优势群体的观测数，减去来自劣势群体的观测数，再除以整体数据集的大小。如果你的类别不平衡过高，那么你的数据集以及随后的模型肯定会存在偏差。
- en: One common post-training metric is disparate impact, which is defined simply
    as the number of positive predicted labels in your disadvantaged group, divided
    by the same in your advantaged group. Intuitively, this measures your model’s
    likelihood of predicting positive for different groups, which as you can imagine
    is critical in certain domains such as employment or law. There is some legal
    precedent for using 4/5, or 80%, as the lower threshold here.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的后训练度量标准是**不平等待遇影响**，它的定义非常简单，即在你的劣势群体中预测为正标签的数量，除以在你的优势群体中预测为正标签的数量。直观地说，这衡量了你的模型在不同群体中预测为正的可能性，正如你所想象的，这在某些领域（如就业或法律）是至关重要的。在此，有一些法律先例表明使用4/5或80%作为此处的下限阈值。
- en: '**Vision and language**: Lastly, both vision and language have different approaches.
    In language, it’s common to evaluate a language model’s learned preference to
    suggest a given category under certain conditions, such as placing “he” or “her”
    under some employment criteria.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉与语言**：最后，视觉与语言有不同的处理方式。在语言领域，通常会评估语言模型在特定条件下提出某个类别的偏好，例如将“he”或“her”归类于某些就业标准下。'
- en: With vision, you might use a pretrained text classifier to ensure that datasets
    are balanced before training. Also, you can clearly indicate a model’s poor behavior
    in detecting certain classes – for example, certain groups in image recognition.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉领域，你可以使用一个预训练的文本分类器来确保数据集在训练前是平衡的。此外，你还可以清楚地指出模型在检测某些类别时的不足表现——例如，在图像识别中的某些群体。
- en: Enhancing your dataset – multilingual, multimodal, and augmentations
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升你的数据集——多语言、多模态和增强技术
- en: Finally, now that you’ve learned how to pick a dataset, compare it with research
    datasets, determine the right approximate size, and evaluate bias, let’s dive
    into enhancing the dataset. In particular, we’ll look at a few dimensions – **multilingual**,
    **multimodal**, and **augmentations**. All three of these typically come a bit
    later in your ML projects, especially after the first few versions of your models
    have been trained and you’re looking for the next idea to give you a boost.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，既然你已经学会了如何选择数据集，如何与研究数据集进行比较，如何确定合适的大致大小，以及如何评估偏差，那么我们就可以开始提升数据集了。特别是，我们将关注几个维度——**多语言**、**多模态**和**增强技术**。这三者通常会在你的机器学习项目的稍后阶段出现，特别是在模型的前几个版本训练完成后，你会寻找下一个能够给你带来提升的思路。
- en: Personally, I think there are few applications in the world where multilingually
    *isn’t* a strong added value. *Multilingual* just means multiple languages. While
    many of the state-of-the-art language models were originally trained on English-only
    text, researchers in the last few years have made strong efforts to increase the
    lingual diversity of these corpora. That means they’re adding support for a lot
    of languages. In 2022, Hugging Face led a massive worldwide effort to democratize
    the creation of large language models, calling their program *Big Science* *(8)*.
    This led to the creation of a novel model they named the **BigScience Open-Science
    Open-Access Multilingual Language Model** (**BLOOM**). Hugging Face hopes to improve
    upon the state of the art in multilingual use cases especially, such as zero-shot
    language translation. However, the model was shown to perform worse than GPT-3
    in many cases, leading us to believe that the best models may be single-language
    only.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 个人而言，我认为在世界上很少有应用场景是多语言*不是*一个强有力的附加价值的。*多语言*仅仅意味着多种语言。尽管许多最先进的语言模型最初是基于仅英文文本进行训练的，但近年来研究人员已在大力提升这些语料库的语言多样性。这意味着他们正在为许多语言增加支持。2022年，Hugging
    Face发起了一项全球性的大规模努力，旨在民主化大语言模型的创建，称其项目为*Big Science* *(8)*。这促成了一个新模型的创建，命名为**BigScience开放科学开放访问多语言语言模型**（**BLOOM**）。Hugging
    Face希望在多语言应用场景中，尤其是在零样本语言翻译等任务上，能改进现有技术。然而，该模型在许多情况下表现不如GPT-3，这让我们认为最优秀的模型可能还是仅限于单一语言。
- en: Frankly, being multilingual is just good business. For any product you develop,
    any program you run, and any service you offer, you are limited in interacting
    with your potential consumer through language at the end of the day. Think of
    a language as a market. While you’re developing your product, you want to bring
    it to as many markets as you can. Ultimately, that means as many languages as
    you can. For this reason, I’m optimistic that the industry will find a better
    way to incorporate multiple languages in possibly the same model without worsening
    results. Perhaps this is as simple as formatting a dataset appropriately, as in
    the case of chain-of-thought or instruction tuning.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 坦率地说，多语种化就是好的商业。对于你开发的任何产品，运行的任何程序，以及提供的任何服务，最终，你在与潜在消费者互动时都受到语言的限制。将语言视为一个市场。在你开发产品时，你希望将其推向尽可能多的市场。最终，这意味着尽可能多的语言。因此，我对行业能够找到更好的方法，将多种语言融入到可能是同一个模型中，而不导致结果恶化感到乐观。也许这仅仅是像链式思维或指令调整那样适当格式化数据集的问题。
- en: Briefly, let’s explore adding additional modalities. Simply put, this means
    different types of datasets, such as adding vision to text, or vice versa. I introduced
    this concept in more detail at the close of [*Chapter 1*](B18942_01.xhtml#_idTextAnchor016).
    Here, I’d like to simply point out that *if you have text with images, or images
    with your text, try to use it*. Once you’re invested in a project, with many hours
    spent on analyzing the data, training, evaluating models, deploying these, and
    so on, why would you not go the extra mile to explore adding other modalities?
    Particularly when it has the potential of raising accuracy, which it does. From
    the perspective of a model, another modality is just another type of embedding.
    You’ll most likely want to use some model pretrained elsewhere to convert a raw
    image into embeddings – that is, before adding them as another input to your model.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，让我们探讨一下增加其他模态的问题。简单来说，这意味着不同类型的数据集，例如将视觉与文本结合，或者反过来。我在[*第一章*](B18942_01.xhtml#_idTextAnchor016)的结尾部分更详细地介绍了这一概念。在这里，我只是想指出，*如果你的文本中有图片，或者你的图片中有文本，尽量加以利用*。一旦你投入到一个项目中，花费了大量时间分析数据、训练、评估模型、部署模型等，为什么不再进一步探索增加其他模态呢？特别是当它有提升准确性的潜力时，确实如此。从模型的角度来看，另一种模态仅仅是另一种嵌入类型。你最可能需要使用一些其他地方预训练的模型，将原始图像转换成嵌入——也就是说，在将其作为模型的另一个输入之前。
- en: There are trade-offs here; increasing the size of your model will increase its
    runtime. Increasing your dataset also increases your data transformation costs.
    Adding another step in the data transformation makes hosting more complex, meaning
    you may need to revisit the system design to deploy your model. All of these trade-offs
    are worthy of discussion, and ultimately, you’ll need to prioritize the projects
    that add the most value for your teams and your customers, which could very well
    include language-only models.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些权衡；增加模型的大小会增加其运行时间。增加数据集的大小也会增加数据转换的成本。增加数据转换的步骤使得托管变得更复杂，这意味着你可能需要重新考虑系统设计，以便部署你的模型。所有这些权衡都是值得讨论的，最终，你需要优先考虑那些为你的团队和客户带来最大价值的项目，而这其中可能非常包含仅限语言的模型。
- en: The other reason I’m optimistic about multimodal projects generally, as opposed
    to language-only projects, is that the visual domain carries so much information
    to humans. Humans learn to see before they learn to speak, and so many of our
    experiences and knowledge are gathered visually. For this reason, I believe foundation
    models will continue to converge around joint vision and language tasks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我对多模态项目一般来说持乐观态度，而不是仅限语言的项目的另一个原因是，视觉领域对人类传达了如此多的信息。人类在学会说话之前学会了看东西，我们的许多经验和知识都是通过视觉收集的。基于这个原因，我相信基础模型将继续朝着联合视觉与语言任务的方向收敛。
- en: Finally, data augmentation is a simple and easy step to improve the accuracy
    of your models without adding a ton of extra work to get it. The core idea is
    that you’re adding some degree of variety in your dataset and slight changes in
    the provided samples, which will help your model learn the difference between
    signal and noise. Both text and vision have well-tested methods for augmentation.
    With vision, this is frequently as simple as pixel manipulations, light color
    manipulations, or image rotations.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，数据增强是一个简单易行的步骤，可以提高模型的准确性，而不需要额外付出大量工作。其核心思想是向你的数据集中添加一定程度的多样性和对提供样本的轻微变化，这将帮助模型学习信号和噪声之间的区别。无论是文本还是视觉，都有经过验证的增强方法。在视觉方面，这通常仅仅是像素操作、轻微的颜色变化或图像旋转。
- en: With text, this can be substituting synonyms, sentence-level reconstruction,
    or lightweight punctuation modifications. The trick is that you don’t want to
    change the basic mechanism you are trying to learn. If you’re training an image
    detection model, don’t modify any of the images so that you can’t detect the images.
    If you’re training a text classifier, don’t alter the text so much that it moves
    into a different class.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本来说，这可能是替换同义词、句子层面的重构或轻微的标点修改。关键是你不想改变你正在尝试学习的基本机制。如果你在训练一个图像检测模型，就不要修改任何图像，以免无法检测到它们。如果你在训练一个文本分类器，也不要修改文本到它变成一个不同的类别。
- en: Augmentation is usually less of an issue in large-scale pretraining, where most
    datasets are so large, as they already include more than enough noise and variation.
    It does, however, seem like a promising avenue for bias reduction especially.
    Another key technique for pretraining is reducing duplicate text. This is especially
    key in web data, where memes, comments, and threads can easily render the same
    text many hundreds of times across platforms and users.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模预训练中，数据增强通常不是一个问题，因为大多数数据集非常庞大，已经包含了足够的噪声和变化。然而，这似乎是一个特别有前景的偏差减少途径。预训练的另一个关键技术是减少重复文本。这在网络数据中尤为重要，因为表情包、评论和讨论线程很容易在平台和用户之间反复出现同一段文本。
- en: Now that you’ve learned all about the early stages of preparing your data, let’s
    do a quick recap of what you just learned before we move on to preparing your
    model!
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了数据准备的早期阶段，让我们在继续准备模型之前，快速回顾一下你刚刚学到的内容！
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced a wide variety of use cases for foundation modeling,
    encompassing scenarios where you can fine-tune an existing foundation model and
    where pretraining itself is competitive. We provided a simple economics framework
    to help you make the case for your pretraining project, notably by tying it to
    how much you expect your business to increase based on a more accurate model.
    After that, we talked about evaluating your dataset, comparing it to research
    datasets, and learning how to think critically about its sampling mechanism. We
    set up some basic ideas to use this critical thinking for framing experiments,
    which we’ll continue in the next chapter. We learned about the scaling laws and
    presented an open source notebook you can use to find which dataset size will
    help you hit performance levels, given fixed model and compute budgets. We talked
    about detecting and mitigating bias in your datasets, along with enhancing these
    with augmentation, modalities, and languages.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了各种基础模型的使用案例，包括你可以微调现有基础模型的场景以及预训练本身具有竞争力的场景。我们提供了一个简单的经济学框架，帮助你为预训练项目提供依据，特别是通过将其与你预计基于更准确的模型，业务将如何增长相联系。之后，我们讨论了评估你的数据集，将其与研究数据集进行比较，并学习如何批判性地思考其采样机制。我们提出了一些基本的想法，用于将这种批判性思维应用于实验框架的设定，这将在下一章继续讨论。我们了解了扩展法则，并提供了一个开源笔记本，你可以用来找到在固定模型和计算预算下，哪个数据集大小能帮助你达到预期的性能水平。我们还讨论了如何检测和减轻数据集中的偏差，并通过增强、模态和语言进行改善。
- en: Next up is model preparation!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是模型准备！
- en: References
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Please go through the following content for more information on a few of the
    topics covered in this chapter:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请阅读以下内容，了解本章中涉及的一些主题：
- en: '*Papers With* *Code*: [https://paperswithcode.com/datasets](https://paperswithcode.com/datasets).'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Papers With* *Code*: [https://paperswithcode.com/datasets](https://paperswithcode.com/datasets).'
- en: '*Hugging Face* *Hub*: [https://huggingface.co/datasets](https://huggingface.co/datasets)'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Hugging Face* *Hub*: [https://huggingface.co/datasets](https://huggingface.co/datasets)'
- en: '*Hugging* *Face*: [https://huggingface.co/datasets?task_ids=task_ids:language-modeling&sort=downloads](https://huggingface.co/datasets?task_ids=task_ids:language-modeling&sort=downloads)'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Hugging* *Face*：[https://huggingface.co/datasets?task_ids=task_ids:language-modeling&sort=downloads](https://huggingface.co/datasets?task_ids=task_ids:language-modeling&sort=downloads)'
- en: '*AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT* *SCALE*:
    [https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*一张图胜过16x16个词：用于图像识别的变换器模型（Transformer）*：[https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)'
- en: 'Scaling Laws for Neural Language Models: [https://arxiv.org/pdf/2001.08361.pdf](https://arxiv.org/pdf/2001.08361.pdf)'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经语言模型的扩展法则：[https://arxiv.org/pdf/2001.08361.pdf](https://arxiv.org/pdf/2001.08361.pdf)
- en: 'Training Compute-Optimal Large Language Models: [https://arxiv.org/pdf/2203.15556.pdf](https://arxiv.org/pdf/2203.15556.pdf)'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练计算最优的大型语言模型：[https://arxiv.org/pdf/2203.15556.pdf](https://arxiv.org/pdf/2203.15556.pdf)
- en: '*BigScience Episode #5 – Challenges & Perspectives in Creating Large Language*
    *Models*: [https://bigscience.huggingface.co/acl-2022](https://bigscience.huggingface.co/acl-2022)'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*BigScience 第五集 – 创建大型语言模型的挑战与视角*：[https://bigscience.huggingface.co/acl-2022](https://bigscience.huggingface.co/acl-2022)'
