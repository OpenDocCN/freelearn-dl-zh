- en: Training a Prediction Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练预测模型
- en: This chapter shows you how to build and train basic neural networks in R through
    hands-on examples and shows how to evaluate different hyper-parameters for models
    to find the best set. Another important issue in deep learning is dealing with overfitting,
    which is when a model performs well on the data it was trained on but poorly on
    unseen data. We will briefly look at this topic in this chapter, and cover it
    in more depth in [Chapter 3](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml), *Deep
    Learning Fundamentals*. The chapter closes with an example use case classifying
    activity data from a smartphone as walking, going up or down stairs, sitting,
    standing, or lying down.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过实际示例向你展示如何在R中构建和训练基础的神经网络，并展示如何评估不同的超参数来找到最佳的模型设置。深度学习中的另一个重要问题是过拟合，即模型在训练数据上表现良好，但在未见过的数据上表现差。本章简要介绍了这个问题，我们将在[第3章](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml)，*深度学习基础*中深入探讨。最后通过一个实际用例，使用智能手机的活动数据分类为步行、上下楼梯、坐着、站着或躺着。
- en: 'This chapter covers the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖以下主题：
- en: Neural networks in R
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R中的神经网络
- en: Binary classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二分类
- en: Visualizing a neural network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化神经网络
- en: Multi-classification using the nnet and RSNNS packages
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用nnet和RSNNS包进行多分类
- en: The problem of overfitting data—the consequences explained
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据过拟合问题——解释其后果
- en: Use case—building and applying a neural network
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例——构建和应用神经网络
- en: Neural networks in R
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R中的神经网络
- en: We will build several neural networks in this section. First, we will use the
    neuralnet package to create a neural network model that we can visualize. We will
    also use the `nnet` and `RSNNS` (Bergmeir, C., and Benítez, J. M. (2012)) packages.
    These are standard R packages and can be installed by the `install.packages` command
    or from the packages pane in RStudio. Although it is possible to use the `nnet`
    package directly, we are going to use it through the `caret` package, which is
    short for **Classification and Regression Training**. The `caret` package provides
    a standardized interface to work with many **machine learning** (**ML**) models
    in R, and also has some useful features for validation and performance assessment
    that we will use in this chapter and the next.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建几个神经网络。首先，我们将使用neuralnet包创建一个神经网络模型，并且可以可视化。我们还将使用`nnet`和`RSNNS`（Bergmeir,
    C., 和 Benítez, J. M.（2012））包。这些是标准的R包，可以通过`install.packages`命令或RStudio中的包面板安装。虽然可以直接使用`nnet`包，但我们将通过`caret`包来使用它，`caret`是**分类和回归训练**（**Classification
    and Regression Training**）的缩写。`caret`包为R中的许多**机器学习**（**ML**）模型提供了一个标准化的接口，并且还具备一些用于验证和性能评估的实用功能，我们将在本章和下章中使用这些功能。
- en: 'For our first examples of building neural networks, we will use the `MNIST`
    dataset, which is a classic classification problem: recognizing handwritten digits
    based on pictures. The data can be downloaded from the Apache MXNet site ([https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip](https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip)).
    It is in the CSV format, where each column of the dataset, or feature, represents a
    pixel from the image. Each image has 784 pixels (28 x 28) and the pixels are in
    grayscale and range from 0 to 255\. The first column contains the digit label,
    and the rest are pixel values, to be used for classification.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们构建神经网络的第一个示例，我们将使用`MNIST`数据集，这是一个经典的分类问题：根据图片识别手写数字。数据可以从Apache MXNet网站下载（[https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip](https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip)）。数据格式为CSV，其中每一列数据或特征表示图片中的一个像素。每张图片有784个像素（28
    x 28），像素是灰度的，范围从0到255。第一列包含数字标签，其余列是像素值，用于分类。
- en: Building neural network models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建神经网络模型
- en: The code is in the `Chapter2` folder of the code for this book. If you have
    not already downloaded and unzipped the code, go back to [Chapter 1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml),
    *Getting Started with Deep Learning*, for the link to download the code. Unzip
    the code into a folder in your machine, and you will see folders for different
    chapters. The code we will be following is `Chapter2\chapter2.R`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 代码位于本书`Chapter2`文件夹中。如果你还没有下载并解压代码，请回到[第1章](00c01383-1886-46d0-9435-29dfb3e08055.xhtml)，*深度学习入门*，获取下载代码的链接。将代码解压到你的机器中的一个文件夹，你将看到不同章节的文件夹。我们将使用的代码是`Chapter2\chapter2.R`。
- en: 'We will use the `MNIST` dataset to build some neural network models. The first
    few lines in the script look to see whether the data file (`train.csv`) is in
    the data directory. If the file already exists in the data directory then it proceeds;
    if it isn''t, it downloads a ZIP file from [https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip](https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip),
    and unzips it into the data folder. This check means that you don''t have to download
    the data manually and the program only downloads the file once. Here is the code
    to download the data:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`MNIST`数据集来构建一些神经网络模型。脚本的前几行查看数据文件（`train.csv`）是否在数据目录中。如果文件已经存在于数据目录中，则程序继续执行；如果不存在，则从[https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip](https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/data/mnist_csv.zip)下载ZIP文件，并解压到数据文件夹中。这个检查意味着您不必手动下载数据，并且程序只下载文件一次。以下是下载数据的代码：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As an alternative, the `MNIST` data is also available in Keras, so we can download
    it from that library and save it as a CSV file:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为替代方案，`MNIST`数据也可以在Keras中获得，因此我们可以从该库中下载并保存为CSV文件：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When you load any new dataset for the first time, the first thing you should
    do is a quick check on the data to ensure that the number of rows and columns
    are as expected, as shown in the following code:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当您第一次加载任何新数据集时，您应该快速检查数据，以确保行数和列数符合预期，如以下代码所示：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The data looks OK, we have `42000` rows and `785` columns. The header was imported
    correctly and the values are numeric. Now that we have loaded the data and performed
    some validation checks on it, we can move on to modeling. Our first model will
    use the `neuralnet` library as this allows us to visualize the neural net. We
    will select only the rows where the label is either 5 or 6, and create a binary
    classification task to differentiate between them. Of course, you can pick any
    digits you choose, but using 5 and 6 is a good choice because they are similar
    graphically, and therefore our model will have to work harder than if we picked
    two digits that were not so similar, for example, 1 and 8\. We rename the labels
    as 0 and 1 for modeling and then separate that data into a train and a test split.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 数据看起来不错，我们有`42000`行和`785`列。头部被正确导入，值是数值型的。现在我们已经加载了数据并对其进行了一些验证检查，我们可以继续建模。我们的第一个模型将使用`neuralnet`库，因为这允许我们可视化神经网络。我们将仅选择标签为5或6的行，并创建一个二元分类任务来区分它们。当然，您可以选择任何您喜欢的数字，但选择5和6是一个不错的选择，因为它们在图形上相似，因此我们的模型将比选择不那么相似的两个数字（例如1和8）更难工作。我们将标签重命名为0和1进行建模，然后将该数据分成训练集和测试集。
- en: We then perform dimensionality-reduction using **principal components analysis** (**PCA**)
    on the training data—we use PCA because we want to reduce the number of predictor
    variables in our data to a reasonable number for plotting. PCA requires that we
    remove columns that have zero variance; these are the columns that have the same
    value for each instance. In our image data, there is a border around all images,
    that is, the values are all zero. Note how we find the columns that have zero
    variance using only the data used to train the model; it would be incorrect to
    apply this check first and then split the data for modelling.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对训练数据使用**主成分分析**（**PCA**）进行降维——我们使用PCA是因为我们希望将数据中的预测变量数量减少到一个合理的数量以进行绘图。PCA要求我们移除具有零方差的列；这些列在每个实例中具有相同的值。在我们的图像数据中，所有图像周围都有一个边框，即所有值都为零。请注意，我们如何使用仅用于训练模型的数据找到具有零方差的列；先应用此检查然后再拆分数据进行建模是不正确的。
- en: '**Dimensionality-reduction**: Our image data is grayscale data with values
    from 0 (black) to 255 (white). These values are highly correlated, that is, if
    a pixel is black (that is, 0), it is likely that the pixels around it are either
    black or dark gray. Similarly if a pixel is white (255), it is likely that the
    pixels around it are either white or light gray. Dimensionality-reduction is an
    unsupervised machine learning technique that takes an input dataset and produces
    an output dataset with the same'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**降维**：我们的图像数据是灰度数据，取值范围从0（黑色）到255（白色）。这些值高度相关，即如果一个像素是黑色（即0），则其周围的像素很可能是黑色或深灰色。类似地，如果一个像素是白色（255），则其周围的像素很可能是白色或浅灰色。降维是一种无监督机器学习技术，它接受一个输入数据集并生成一个具有相同特性的输出数据集。'
- en: number of rows but fewer columns. Crucially though, these fewer columns can
    explain most of the signal in the input dataset. PCA is one dimensionality-reduction
    algorithm. We use it here because we want to create a dataset with a small number
    of columns to plot the network, but we still want our algorithm to produce good
    results.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的操作将会减少数据的行数，但列数更少。不过，关键在于，这些更少的列可以解释输入数据集中的大部分信号。PCA是一种降维算法。我们在这里使用它是因为我们想要创建一个列数较少的数据集来绘制网络，但我们仍然希望我们的算法能够产生良好的结果。
- en: 'The following code selects the rows where the label is either 5 or 6 and creates
    a train/test split. It also removes columns where the variance is zero; these
    are columns that have the same value for every row:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码选择了标签为5或6的行，并创建了一个训练/测试数据集的拆分。它还去除了方差为零的列，这些列在每一行中的值都相同：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We have reduced the number of column data from `784` to `624`, that is, `160`
    columns had the same value for all rows. Now, we perform PCA on the data and plot
    the cumulative sum of the variances:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将列数据的数量从`784`减少到了`624`，也就是说，`160`列在所有行中具有相同的值。现在，我们对数据进行PCA分析并绘制方差的累积和：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The cumulative sum of PCA explained variance shows how many principal components
    are needed to explain the proportion of variance in the input data. In layman''s
    terms, this plot shows that we can use the first 100 variables (the *principal
    components*) and this will account for over 80% of the variance in the original
    data:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: PCA解释方差的累积和展示了需要多少主成分才能解释输入数据中的方差比例。通俗来说，这张图表明我们可以使用前100个变量（*主成分*），这就能解释原始数据中超过80%的方差：
- en: '![](img/de8c020f-25b5-4642-a63a-171364740a40.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de8c020f-25b5-4642-a63a-171364740a40.png)'
- en: 'Figure 2.1: Cumulative sum of the Principal Components explained variance.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：主成分解释方差的累积和。
- en: 'The next code block selects out the principal components that account for 50%
    of our variance and use those variables to create a neural network:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码块选择了解释我们50%方差的主成分，并使用这些变量来创建神经网络：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can see that 50% of the variance in the original data can be accounted by
    only 23 principal components. Next, we plot the neural network by calling the
    `plot` function:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，原始数据中50%的方差只需23个主成分就能解释。接下来，我们通过调用`plot`函数绘制神经网络：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This produces a plot similar to the following screenshot. We can see the input
    variables (**PC1** to **PC23**), the hidden layers and biases, and even the network
    weights:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成一个类似于以下屏幕截图的图。我们可以看到输入变量（**PC1**到**PC23**）、隐藏层和偏置，甚至是网络权重：
- en: '![](img/bc6d5871-c471-4093-9ff1-75faca8dcfed.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc6d5871-c471-4093-9ff1-75faca8dcfed.png)'
- en: 'Figure 2.2: An example of a neural network with weights and biases'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：带有权重和偏置的神经网络示例
- en: We selected 23 principal components to use as predictors for our neural network
    library. We chose to use two hidden layers, the first with four nodes and the
    second with two nodes. The plot outputs the coefficients, which are not all decipherable
    from the plot, but there are functions to access them if required.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了23个主成分作为我们神经网络库的预测变量。我们决定使用两个隐藏层，第一个有四个节点，第二个有两个节点。该图输出了系数，虽然从图中并不容易解读，但如果需要，还是可以使用相关函数访问它们。
- en: 'Next, we will create predictions on a holdout or test dataset that was not
    used to build either the dimensionality-reduction or the neural network model.
    We have to first pass the test data into the `predict` function, passing in the
    `df.pca` object created earlier, to get the principal components for the test
    data. We can then pass this data into the neural network prediction (filtering
    the columns to the first 23 principal components) and then show the confusion
    matrix and overall accuracy:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在一个未参与构建降维或神经网络模型的保留数据集或测试数据集上进行预测。我们需要先将测试数据传递给`predict`函数，传入之前创建的`df.pca`对象，以获取测试数据的主成分。然后，我们可以将这些数据传入神经网络预测中（将列过滤到前23个主成分），最后显示混淆矩阵和总体准确率：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We achieved `97.86%` accuracy, which is not bad considering we only used 23
    principal components in our neural network. It is important to note that these
    23 variables are not directly comparable to any columns in the input dataset or
    each other. In fact, the whole point of PCA, or any dimensionality-reduction algorithm,
    is to produce columns that are not correlated with each other.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们达到了`97.86%`的准确率，考虑到我们仅使用了23个主成分在神经网络中，这个结果相当不错。需要注意的是，这些23个变量并不能直接与输入数据集中的任何列或彼此进行比较。事实上，PCA（主成分分析）或任何降维算法的关键就在于生成相互之间没有相关性的列。
- en: 'Next, we will move on to create models that perform multi-classification, that
    is, they can classify digits 0-9\. We will convert the labels (the digits 0 to
    9) to a factor so R knows that this is a classification not a regression problem.
    For real-world problems, you should use all the data available, but if we used
    all 42,000 rows, it would take a very long time to train using the neural network
    packages in R. We will select 5,000 rows for training and 1,000 rows for test
    purposes. We should select the rows at random and ensure that there is no overlap
    between the rows in our training and test datasets. We also separate the data
    into the features or predictors (`digits.x`) and the outcome (`digits.Y`). We
    are using all the columns except the labels as the predictors here:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建执行多分类的模型，也就是说，它们可以对数字 0-9 进行分类。我们将标签（数字 0 到 9）转换为因子，以便 R 知道这是一个分类问题而不是回归问题。对于实际问题，应该使用所有可用的数据，但如果我们使用所有
    42,000 行数据，使用 R 中的神经网络包进行训练将需要很长时间。因此，我们将选择 5,000 行用于训练，1,000 行用于测试。我们应随机选择这些行，并确保训练集和测试集之间没有重复的行。我们还将数据分为特征或预测变量（`digits.x`）和结果（`digits.Y`）。在这里，我们使用除了标签之外的所有列作为预测变量：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, before we get started building our neural network, let''s quickly
    check the distribution of the digits. This can be important as, for example, if
    one digit occurs very rarely, we may need to adjust our modeling approach to ensure
    that, it''s given enough weight in the performance evaluation if we care about
    accurately predicting that specific digit. The following code snippet creates
    a bar plot showing the frequency of each digit label:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在开始构建我们的神经网络之前，让我们快速检查一下数字的分布情况。这一点很重要，例如，如果某个数字非常少见，我们可能需要调整建模方法，以确保在性能评估中对该数字给予足够的权重，特别是当我们希望准确预测该特定数字时。以下代码片段创建了一个条形图，展示了每个数字标签的频率：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can see from the plot that the categories are fairly evenly distributed
    so there is no need to increase the weight or importance given to any particular
    one:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，各类别的分布相对均匀，因此无需增加任何特定类别的权重或重要性：
- en: '![](img/8a41bd71-dbfe-46fd-ac22-d442ffbfc1e9.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a41bd71-dbfe-46fd-ac22-d442ffbfc1e9.png)'
- en: 'Figure 2.3: Distribution of *y* values for train dataset'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：训练数据集的 *y* 值分布
- en: Now let's build and train our first neural network using the `nnet` package
    through the `caret` package wrapper. First, we use the `set.seed()` function and
    specify a specific seed so that the results are reproducible. The exact seed is
    not important, what matters is that the same seed is used each time you run the
    script. The `train()` function first takes the feature or predictor data (`x`),
    and then the outcome variable (`y`), as arguments. The `train()` function can
    work with a variety of models, determined via the method argument. Although many
    aspects of machine learning models are learned automatically, some parameters
    have to be set. These vary by the method used; for example, in neural networks,
    one parameter is the number of hidden units. The `train()` function provides an
    easy way to try a variety of these tuning parameters as a named data frame to
    the `tuneGrid` argument. It returns the performance measures for each set of tuning
    parameters and returns the best trained model. We will start with just five hidden
    neurons in our model, and a modest decay rate. The learning rate controls how
    much each iteration or step can influence the current weights. The decay rate
    is the regularization hyper-parameter, which is used to prevent the model from
    overfitting. Another argument, `trcontrol`, controls additional aspects of `train()`,
    and is used, when a variety of tuning parameters are being evaluated, to tell
    the caret package how to validate and pick the best tuning parameter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过`caret`包的包装器使用`nnet`包来构建和训练我们的第一个神经网络。首先，我们使用`set.seed()`函数并指定一个特定的种子，以确保结果是可重复的。具体的种子值并不重要，重要的是每次运行脚本时使用相同的种子。`train()`函数首先接受特征或预测数据（`x`），然后是结果变量（`y`）作为参数。`train()`函数可以与多种模型一起使用，具体模型通过`method`参数确定。尽管机器学习模型的许多方面是自动学习的，但有些参数需要手动设置。这些参数因使用的方法而异；例如，在神经网络中，一个参数是隐藏单元的数量。`train()`函数提供了一种简单的方式，将各种调优参数作为命名的数据框传递给`tuneGrid`参数。它返回每组调优参数的性能测量值，并返回最佳训练模型。我们将从仅有五个隐藏神经元的模型开始，并使用适中的衰减率。学习率控制每次迭代或步骤对当前权重的影响程度。衰减率是正则化超参数，用于防止模型过拟合。另一个参数`trcontrol`控制`train()`的其他方面，当评估多种调优参数时，用于告知`caret`包如何验证并选择最佳调优参数。
- en: 'For this example, we will set the method for training control to *none* as
    we only have one set of tuning parameters being used here. Finally, at the end,
    we can specify additional, named arguments that are passed on to the actual `nnet()`
    function (or whatever algorithm is specified). Because of the number of predictors
    (`784`), we increase the maximum number of weights to 10,000 and specify a maximum
    of 100 iterations. Due to the relatively small amount of data, and the paucity
    of hidden neurons, this first model does not take too long to run:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将训练控制方法设置为*none*，因为这里只有一组调优参数在使用。最后，我们可以指定额外的命名参数，这些参数会传递给实际的`nnet()`函数（或指定的任何算法）。由于预测变量的数量（`784`），我们将最大权重数增加到10,000，并指定最大迭代次数为100。由于数据量相对较小，且隐藏神经元较少，这个初始模型运行时间并不长：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `predict()` function generates a set of predictions for the data. We will
    use the test dataset to evaluate the model; this contains records that were not
    used to train the model. We examine the distribution of the predicted digits in
    the following diagram.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()`函数为数据生成一组预测值。我们将使用测试数据集来评估模型；该数据集包含未用于训练模型的记录。我们将在下图中检查预测数字的分布。'
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'It is clear that this is not a good model because the distribution of the predicted
    values is very different from the distribution of the actual values:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这不是一个好的模型，因为预测值的分布与实际值的分布差异很大：
- en: '![](img/fad60498-d289-4519-af5e-4e4398bccfbc.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fad60498-d289-4519-af5e-4e4398bccfbc.png)'
- en: 'Figure 2.4: Distribution of *y* values from prediction model'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：预测模型中*y*值的分布
- en: 'The `barplot` is a simple check of the predictions, and shows us that our model
    is not very accurate. We can also calculate the accuracy by finding the percentage
    of rows from the predictions that match the actual value. The accuracy for this
    model is `54.8%`, which is not good. A more formal evaluation of the model''s
    performance is possible using the `confusionMatrix()` function in the `caret`
    package. Because there is a function by the same name in the `RSNNS` package,
    they are masked so we call the function using `caret::confusionMatrix` to ensure
    the function from the `caret` package is used. The following code shows the confusion
    matrix and performance metrics on the test set:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`barplot`是对预测结果的简单检查，显示了我们的模型并不十分准确。我们还可以通过计算预测结果中与实际值匹配的行的百分比来计算准确率。该模型的准确率为`54.8%`，并不理想。通过`caret`包中的`confusionMatrix()`函数，可以更正式地评估模型的性能。由于在`RSNNS`包中也有同名函数，它们被掩盖了，因此我们使用`caret::confusionMatrix`来确保调用的是`caret`包中的函数。以下代码展示了混淆矩阵和测试集上的性能指标：'
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Because we had multiple digits, there are three main sections to the performance
    output. First, the actual frequency cross tab is shown. Correct predictions are
    on the diagonal, with various frequencies of misclassification on the off diagonals.
    Next are the overall statistics, which refer to the model's performance across
    all classes. Accuracy is simply the proportion of cases correctly classified,
    along with a 95% confidence interval, which can be useful, especially for smaller
    datasets where there may be considerable uncertainty in the estimate.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有多个数字，性能输出分为三个主要部分。首先，显示的是实际频率的交叉表。正确的预测位于对角线，误分类的不同频率出现在非对角线上。接下来是总体统计数据，表示模型在所有类别上的性能。准确率仅仅是正确分类的案例比例，并附带一个95%的置信区间，这对于较小的数据集尤为有用，因为在这些数据集中估计值可能存在较大的不确定性。
- en: '`No Information Rate` refers to what accuracy could be expected without any
    information by merely guessing the most frequent class, in this case, 1, which
    occurred 11.16% of the time. The p-value tests whether the observed accuracy (44.3%)
    is significantly different from `No Information Rate` (11.2% ). Although statistically
    significant, this is not very meaningful for digit-classification, where we would
    expect to do far better than simply guessing the most frequent digit! Finally,
    individual performance metrics for each digit are shown. These are based on calculating
    that digit versus every other digit, so that each is a binary comparison.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`No Information Rate`指的是如果没有任何信息，仅通过猜测最常见的类别来期望的准确率，在此例中为1，该类别出现的频率为11.16%。p值测试观察到的准确率（44.3%）是否与`No
    Information Rate`（11.2%）显著不同。尽管在统计学上显著，这对数字分类并没有太大意义，因为我们期望比仅仅猜测最常见的数字要好得多！最后，显示了每个数字的个体性能指标。这些指标是通过计算该数字与其他每个数字的比较来得出的，因此每个指标都是一个二分类比较。'
- en: 'Now that we have some basic understanding of how to set up, train, and evaluate
    model performance, we will try increasing the number of hidden neurons, which
    is one key way to improve model performance, at the cost of greatly increasing
    the model complexity. Recall from [Chapter 1](00c01383-1886-46d0-9435-29dfb3e08055.xhtml), *Getting
    Started with Deep Learning*, that every predictor or feature connects to each
    hidden neuron, and each hidden neuron connects to each outcome or output. With
    `784` features, each additional hidden neuron adds a substantial number of parameters,
    which also results in longer run times. Depending on your computer, be prepared
    to wait a number of minutes for these next model to finish:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对如何设置、训练和评估模型性能有了一些基本了解，我们将尝试增加隐藏神经元的数量，这是提高模型性能的一个关键方式，但代价是大幅增加模型复杂性。回想一下[第1章](00c01383-1886-46d0-9435-29dfb3e08055.xhtml)，*深度学习入门*，每个预测器或特征都会连接到每个隐藏神经元，而每个隐藏神经元又与每个输出或结果相连接。对于`784`个特征，每增加一个隐藏神经元就会增加大量参数，这也会导致运行时间延长。根据你的计算机配置，准备好等待几分钟，直到下一个模型运行完成：
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This model is better than the previous model but the distribution of the predicted
    values is still uneven:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型比上一个模型要好，但预测值的分布仍然不均匀：
- en: '![](img/5a9739c7-1c4e-4251-bea0-3786fd878cf6.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a9739c7-1c4e-4251-bea0-3786fd878cf6.png)'
- en: Figure 2.5: Distribution of *y* values from prediction model
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：预测模型的*y*值分布
- en: 'Increasing the number of hidden neurons from 5 to 10 improved our in-sample
    performance from an overall accuracy of `54.8%` to `66.3%`, but this is still
    quite some way from ideal (imagine character-recognition software that mixed up
    over 30% of all the characters!). We increase again, this time to 40 hidden neurons,
    and wait even longer for the model to finish training:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 将隐藏神经元的数量从 5 增加到 10，提高了我们的样本内表现，整体准确率从`54.8%`提升到`66.3%`，但这仍然距离理想状态有一定差距（想象一下，一个字符识别软件会混淆超过
    30% 的所有字符！）。我们再次增加数量，这次增加到 40 个隐藏神经元，并且让模型等待更长时间才能完成训练：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The distribution of the predicted values is even in this model, which is what
    we are looking for. However the accuracy is still only at 82.2%, which is quite
    low:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，预测值的分布是均匀的，这是我们所追求的。然而，准确率仍然只有 82.2%，这相当低：
- en: '![](img/b4d0b983-1da7-448d-831c-9e7ec71fe882.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b4d0b983-1da7-448d-831c-9e7ec71fe882.png)'
- en: Figure 2.6: Distribution of *y* values from prediction model
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：*y* 值的分布来自预测模型
- en: Using 40 hidden neurons has improved accuracy to `82.2%` overall and it took
    over 40 minutes to run on an i5 computer. Model performance for some digits is
    still not great. If this were a real research or business problem, we might continue
    trying additional neurons, tuning the decay rate, or modifying features in order
    to try to boost model performance further, but for now we will move on.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 40 个隐藏神经元后，整体准确率提高到了`82.2%`，且在一台 i5 电脑上运行耗时超过 40 分钟。某些数字的模型表现仍然不理想。如果这是一个真实的研究或商业问题，我们可能会继续尝试更多的神经元、调整衰减率或修改特征，以进一步提高模型表现，但现在我们将继续进行下一步。
- en: Next, we will take a look at how to train neural networks using the RSNNS package.
    This package provides an interface to a variety of possible models using the **Stuttgart
    Neural Network Simulator** (**SNNS**) code; however, for a basic, single­ hidden-layer,
    feed-forward neural network, we can use the `mlp()` convenience wrapper function,
    which stands for multi-layer perceptron. The RSNNS package is a bit trickier to
    use than the convenience of nnet via the `caret` package, but one benefit is that
    it can be far more flexible and allows for many other types of neural network
    architecture to be trained, including recurrent neural networks, and also has
    a greater variety of training strategies.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何使用 RSNNS 包来训练神经网络。该包提供了一个接口，能够使用**斯图加特神经网络模拟器**（**SNNS**）代码来实现多种可能的模型；然而，对于基本的单隐层前馈神经网络，我们可以使用`mlp()`便捷包装函数，这代表了多层感知机。RSNNS
    包的使用比通过`caret`包中的 nnet 更加复杂，但其一个优势是，它可以更灵活，允许训练许多其他类型的神经网络架构，包括递归神经网络，同时也提供了更多的训练策略。
- en: 'One difference between the nnet and RSNNS packages is that, for multi-class
    outcomes (such as digits), RSNNS requires a dummy encoding (that is, one-hot encoding),
    where each possible class is represented as a column coded as 0/1\. This is facilitated
    using the `decodeClassLabels()` function, as shown in the following code snippet:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: nnet 和 RSNNS 包之间的一个区别是，对于多类别的输出（例如数字），RSNNS 需要进行虚拟编码（即独热编码），每个可能的类别作为一列，使用 0/1
    编码。这可以通过使用`decodeClassLabels()`函数来实现，如下所示的代码片段：
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Since we had reasonably good success with 40 hidden neurons, we will use the
    same size here. Rather than standard propagation as the learning function, we
    will use resilient propagation, based on the work of Riedmiller, M., and Braun,
    H. (1993). Resilient back-propagation is an **optimization** to standard back-propagation
    that applies faster weight-update mechanisms. One of the problems that occurs
    as the neural network increases in complexity is that they take a long time to
    train. We will discuss this in depth in subsequent chapters, but for now, you
    just need to know that this neural network is faster because it keeps track of
    past derivatives and takes bigger steps if they were in the same direction during
    back-propagation. Note also that, because a matrix of outcomes is passed, although
    the predicted probability will not exceed 1 for any single digit, the sum of predicted
    probabilities across all digits may exceed 1 and also may be less than 1 (that
    is, for some cases, the model may not predict they are very likely to represent
    any of the digits). The `predict` function returns a matrix where each column
    represents a single digit, so we use the `encodeClassLabels()` function to convert
    back into a single vector of digit labels to plot and evaluate the model''s performance:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在使用40个隐藏神经元时取得了相当不错的效果，所以这里我们将使用相同的规模。我们将使用基于Riedmiller, M.和Braun, H.（1993）工作的**弹性传播**，而不是标准传播作为学习函数。弹性反向传播是标准反向传播的**优化**方法，它应用了更快速的权重更新机制。随着神经网络复杂度的增加，训练时间通常较长，这是一个常见的问题。我们将在后续章节中深入讨论这一点，但目前，你只需要知道这个神经网络更快，因为它会跟踪过去的导数，如果它们在反向传播过程中方向相同，它会采取更大的步伐。还要注意，因为传递的是一个结果矩阵，尽管单个数字的预测概率不会超过1，但所有数字的预测概率总和可能超过1，也可能小于1（也就是说，对于某些情况，模型可能并不认为它们非常可能代表任何数字）。`predict`函数返回一个矩阵，其中每一列代表一个单独的数字，因此我们使用`encodeClassLabels()`函数将其转换回一个数字标签的单一向量，以便绘制和评估模型的性能：
- en: '[PRE16]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following bar plot shows that the predicted values are relatively evenly
    distributed among the categories. This matches the distribution of the actual
    category values:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下图条形图显示了预测值在各个类别之间相对均匀分布。这与实际类别值的分布相匹配：
- en: '![](img/9ce6c7c5-8599-4a8c-a1ba-1890a556db51.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ce6c7c5-8599-4a8c-a1ba-1890a556db51.png)'
- en: Figure 2.7: Distribution of *y* values from prediction model
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：来自预测模型的*y*值分布
- en: The accuracy is 81.70% and it ran in 3 minutes on my computer. This is only
    slightly lower than when we used nnet with 40 hidden nodes, which took 40 minutes
    on the same machine! This demonstrates the importance of using an optimizer, which
    we will see in subsequent chapters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率为81.70%，且在我的计算机上运行了3分钟。这仅比我们使用40个隐藏节点的nnet稍低，当时在同一台机器上花费了40分钟！这展示了使用优化器的重要性，我们将在后续章节中看到这一点。
- en: Generating predictions from a neural network
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从神经网络生成预测
- en: For any given observation, there can be a probability of membership in any of
    a number of classes (for example, an observation may have a 40% chance of being
    a *5*, a 20% chance of being a *6*, and so on). To evaluate the performance of
    the model, some choices have to be made about how to go from the probability of
    class membership to a discrete classification. In this section, we will explore
    a few of these options in more detail.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何给定的观察值，它可能属于多个类别中的某一个（例如，一个观察值可能有40%的概率是*5*，20%的概率是*6*，依此类推）。为了评估模型的性能，必须做出一些选择，将类别成员资格的概率转化为离散分类。在本节中，我们将更详细地探讨其中的一些选项。
- en: 'As long as there are no perfect ties, the simplest method is to classify observations
    based on the highest predicted probability. Another approach, which the RSNNS
    package calls the **winner takes all** (**WTA**) method, chooses the class with
    the highest probability, provided the following conditions are met:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 只要没有完美的平局，最简单的方法就是基于最高预测概率来分类观察值。另一种方法是RSNNS包称之为**赢家通吃**（**WTA**）方法，选择具有最高概率的类别，前提是满足以下条件：
- en: There are no ties for highest probabilities
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有最高概率的平局情况
- en: The highest probability is above a user-defined threshold (the threshold could
    be zero)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最高概率超过用户定义的阈值（阈值可以为零）
- en: The remaining classes all have a predicted probability under the maximum minus
    another user-defined threshold
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其余类别的预测概率都低于最大值减去另一个用户定义的阈值
- en: Otherwise, observations are classified as unknown. If both thresholds are zero
    (the default), this equates to saying that there must be one unique maximum. The
    advantage of such an approach is that it provides some quality control. In the
    digit-classification example we have been exploring, there are 10 possible classes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，观测值将被分类为未知。如果两个阈值都为零（默认值），这相当于说必须有一个唯一的最大值。这种方法的优点是提供了一些质量控制。在我们一直在探讨的数字分类示例中，有10个可能的类别。
- en: Suppose 9 of the digits had a predicted probability of 0.099, and the remaining
    class had a predicted probability of 0.109\. Although one class is technically
    more likely than the others, the difference is fairly trivial and we may conclude
    that the model cannot with any certainty classify that observation. A final method,
    called 402040, classifies if only one value is above a user-defined threshold,
    and all other values are below another user-defined threshold; if multiple values
    are above the first threshold, or any value is not below the second threshold,
    it treats the observation as unknown. Again, the goal here is to provide some
    quality control.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 假设9个数字的预测概率为0.099，其余类别的预测概率为0.109。尽管某个类别在技术上比其他类别更可能，但差异相当微小，我们可以得出结论，模型无法以任何确定性分类该观测值。一种叫做402040的最终方法，只有当一个值超过用户定义的阈值，且所有其他值低于另一个用户定义的阈值时，才进行分类；如果多个值超过第一个阈值，或者任何值不低于第二个阈值，它将该观测值视为未知。再次强调，这里的目标是提供某种质量控制。
- en: It may seem like this is unnecessary because uncertainty in predictions should
    come out in the model performance. However, it can be helpful to know if your
    model was highly certain in its prediction and right or wrong, or uncertain and
    right or wrong.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来似乎是不必要的，因为预测的不确定性应该已经体现在模型的表现中。然而，知道你的模型在预测时是否高度确信并且预测正确，或者不确定且预测正确或错误，还是很有帮助的。
- en: Finally, in some cases, not all classes are equally important. For example,
    in a medical context where a variety of biomarkers and genes are collected on
    patients and used to classify whether they are, at risk of cancer, or at risk
    of heart disease, even a 40% chance of having cancer may be enough to warrant
    further investigation, even if they have a 60% chance of being healthy. This has
    to do with the performance measures we saw earlier where, beyond overall accuracy,
    we can assess aspects such as sensitivity, specificity, and positive and negative
    predictive values. There are cases where overall accuracy is less important than
    making sure no one is missed.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在某些情况下，并非所有类别都同样重要。例如，在一个医学背景中，收集患者的多种生物标志物和基因，并用来分类他们是否面临癌症风险或心脏病风险，即使患者有40%的癌症风险，可能也足以进一步调查，尽管他们有60%的健康概率。这与我们之前看到的性能度量有关，除了整体准确度外，我们还可以评估诸如敏感性、特异性以及阳性和阴性预测值等方面。有些情况下，整体准确度并不如确保没有遗漏任何人更为重要。
- en: 'The following code shows the raw probabilities for the in-sample data, and
    the impact these different choices have on the predicted values:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码显示了样本数据的原始概率，以及这些不同选择对预测值的影响：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We now proceed to examine problems related to overfitting the data and the impact
    on the evaluation of the model's performance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在开始检查与过拟合数据相关的问题，以及这些问题对模型性能评估的影响。
- en: The problem of overfitting data – the consequences explained
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合数据的问题——后果解释
- en: A common issue in machine learning is overfitting data. Generally, overfitting
    is used to refer to the phenomenon where the model performs better on the data
    used to train the model than it does on data not used to train the model (holdout
    data, future real use, and so on). Overfitting occurs when a model memorizes part
    of the training data and fits what is essentially noise in the training data.
    The accuracy in the training data is high, but because the noise changes from
    one dataset to the next, this accuracy does not apply to unseen data, that is,
    we can say that the model does not generalize very well.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的一个常见问题是过拟合数据。一般来说，过拟合是指模型在用于训练模型的数据上的表现优于在未用于训练模型的数据（如留出数据、未来的实际使用等）上的表现。过拟合发生在模型记住了部分训练数据，并拟合了训练数据中的噪声。训练数据中的准确度很高，但由于噪声在不同数据集之间变化，这种准确度并不适用于未见过的数据，也就是说，我们可以说模型的泛化能力不强。
- en: Overfitting can occur at any time, but tends to become more severe as the ratio
    of parameters to information increases. Usually, this can be thought of as the
    ratio of parameters to observations, but not always. For example, suppose we have
    a very imbalanced dataset where the outcome we want to predict is a rare event
    that occurs in 1 in 5 million cases. In that case, a sample size of 15 million
    may only have 3 positive cases. Even though the sample size is large, the information
    is low. To consider a simple-but-extreme case, imagine fitting a straight line
    to two data points. The fit will be perfect, and in those two training data, your
    linear-regression model will appear to have fully accounted for all variations
    in the data. However, if we then applied that line to another 1,000 cases, it
    might not fit very well at all.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合随时可能发生，但随着参数与信息的比率增加，过拟合的情况往往会变得更严重。通常，这可以被视为参数与观测值的比率，但并不总是如此。例如，假设我们有一个非常不平衡的数据集，目标预测的结果是一个罕见事件，发生的概率为每五百万个案例中有一个。在这种情况下，1500万的样本中可能只有3个正例。即使样本量很大，但信息量却很低。为了考虑一个简单但极端的情况，想象一下将一条直线拟合到两个数据点上。拟合会非常完美，在这两个训练数据上，你的线性回归模型似乎完全解释了数据中的所有变异性。然而，如果我们将这条线应用到另外1000个案例上，它可能就不太适用了。
- en: 'In the previous sections, we generated out-of-sample predictions for the our
    models, that is, we evaluated accuracy on test (or holdout) data. But we never
    checked whether our models were overfitting, that is, the accuracy levels on the
    test data. We can examine how well the model generalizes by checking the accuracy
    on the in-sample predictions. We can see that the accuracy on the in-sample data
    is 84.7%, compared to 81.7% on the holdout data. There is a 3.0% loss; or, put
    differently, using training data to evaluate model performance resulted in an
    overly optimistic estimate of the accuracy, and that overestimate was 3.0%:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们生成了模型的样本外预测，也就是说，我们评估了在测试（或保留）数据上的准确性。但我们从未检查过模型是否出现了过拟合，也就是测试数据上的准确性。我们可以通过检查样本内预测的准确性来检验模型的泛化能力。我们可以看到，样本内数据的准确性为84.7%，而保留数据的准确性为81.7%。有一个3.0%的损失；换句话说，使用训练数据来评估模型表现导致了对准确性的过度乐观估计，这一过高估计为3.0%：
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since we fitted several models earlier of varying complexity, we could examine
    the degree of overfitting or overly optimistic accuracy from in-sample versus
    out­ of-sample performance measures across them. The code here should be easy
    enough to follow. We call the predict function for our models and do not pass
    in any new data; this returns the predictions for the data the model was trained
    with. The rest of the code is boilerplate code to create the graphic plot.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们之前拟合了多个不同复杂度的模型，我们可以通过比较样本内与样本外的表现衡量，检查过拟合或过度乐观的准确性程度。这里的代码应该足够简单易懂。我们调用模型的预测函数，并且不传入任何新数据；这将返回模型所训练的数据的预测结果。其余的代码是标准的代码，用来生成图形图表。
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The code produces the following plot, which shows the accuracy metrics and
    the confidence intervals for those metrics. One thing we notice from this plot
    is that, as the models get more complex, the gap between performance on the in-sample performance
    measures and the out-sample performance measures increases. This highlights that
    more complex models tend to overfit, that is, they perform better on the in-sample data
    than the unseen out-sample data:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成如下图表，展示了准确性指标及这些指标的置信区间。从这张图表中我们注意到，随着模型变得更加复杂，样本内和样本外表现衡量之间的差距逐渐增大。这突出显示了复杂模型容易过拟合，也就是说，它们在样本内数据上的表现优于未见过的样本外数据：
- en: '![](img/88f817b7-5f97-42d6-8606-09e4ab39f888.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88f817b7-5f97-42d6-8606-09e4ab39f888.png)'
- en: 'Figure 2.8: In-sample and out-sample performance measures for on neural network
    models'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：神经网络模型的样本内和样本外表现衡量
- en: Use case – building and applying a neural network
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例 – 构建并应用神经网络
- en: To close the chapter, we will discuss a more realistic use case for neural networks.
    We will use a public dataset by Anguita, D., Ghio, A., Oneto, L., Parra, X., and
    Reyes-Ortiz, J. L. (2013) that uses smartphones to track physical activity. The
    data can be downloaded at [https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones).
    The smartphones had an accelerometer and gyroscope from which 561 features from
    both time and frequency were used.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最后，我们将讨论神经网络的一个更现实的应用场景。我们将使用Anguita, D., Ghio, A., Oneto, L., Parra, X.,
    和 Reyes-Ortiz, J. L.（2013）发布的公开数据集，该数据集使用智能手机来追踪身体活动。数据可以从[https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones)下载。这些智能手机配备了加速度计和陀螺仪，从中提取了561个特征，涵盖了时域和频域数据。
- en: The smartphones were worn during walking, walking upstairs, walking downstairs,
    standing, sitting, and lying down. Although this data came from phones, similar
    measures could be derived from other devices designed to track activity, such
    as various fitness-tracking watches or bands. So this data can be useful if we
    want to sell devices and have them automatically track how many of these different
    activities the wearer engages in.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 智能手机在步行、走楼梯、下楼梯、站立、坐着和躺下时佩戴。虽然这些数据来自手机，但类似的度量可以从其他设计用于追踪活动的设备中得出，例如各种健身追踪手表或腕带。因此，如果我们想销售设备并自动跟踪佩戴者进行的这些不同活动，这些数据就非常有用。
- en: 'This data has already been normalized to range from -1 to + 1; usually we might
    want to perform some normalization if it has not already been applied. Download
    the data from the link and unzip it into the data folder that is on the same level
    as the chapter folder; we will use it in later chapters as well. We can import the
    training and testing data, as well as the labels. We will then take a quick look
    at the distribution of the outcome variable in the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据已经归一化，范围从 -1 到 +1；通常，如果数据没有归一化，我们可能会进行一些归一化操作。请从链接下载数据，并将其解压到与章节文件夹同级的“data”文件夹中；我们在后续章节中也将使用这些数据。我们可以导入训练和测试数据，以及标签。接下来，我们可以通过以下代码快速查看结果变量的分布：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This produces the following bar plot, which shows that the categories are relatively
    evenly balanced:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下条形图，显示类别的分布相对均衡：
- en: '![](img/b105e064-6747-4bd6-97b7-2aaf6d21844d.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b105e064-6747-4bd6-97b7-2aaf6d21844d.png)'
- en: 'Figure 2.9: Distribution of *y* values for UCI HAR dataset'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：UCI HAR 数据集的 *y* 值分布
- en: We are going to evaluate a variety of tuning parameters to show how we might
    experiment with different approaches to try to get the best possible model. We
    will use different hyper-parameters and evaluate which model performs the best.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将评估各种调优参数，以展示如何尝试不同的方法来获得最佳的模型。我们将使用不同的超参数，并评估哪个模型表现最好。
- en: Because the models can take some time to train and R normally only uses a single
    core, we will use some special packages to enable us to run multiple models in parallel.
    These packages are `parallel`, `foreach`, and `doSNOW`, which should have been
    loaded if you ran the script from the first line.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型的训练可能需要一些时间，并且R通常只使用单核处理器，我们将使用一些特殊的包来实现并行运行多个模型。这些包是`parallel`、`foreach`和`doSNOW`，如果你从脚本的第一行运行，它们应该已经被加载。
- en: 'Now we can pick our tuning parameters and set up a local cluster as the backend
    for the `foreach` R package for parallel for loops. Note that if you do this on
    a machine with fewer than five cores, you should change `makeCluster(5)` to a
    lower number:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以选择调优参数，并设置一个本地集群作为`foreach` R包的后台，以支持并行for循环。请注意，如果你在少于五个核心的机器上执行此操作，应将`makeCluster(5)`更改为较低的数字：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we are ready to train all the models. The following code shows a parallel
    for loop, using code that is similar to what we have already seen, but this time
    setting some of the arguments based on the tuning parameters we previously stored
    in the list:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备训练所有模型。以下代码展示了一个并行for循环，使用的代码与之前看到的类似，但这次根据我们先前存储在列表中的调优参数设置了一些参数：
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Because generating out-of-sample predictions can also take some time, we will
    do that in parallel as well. However, first we need to export the model results
    to each of the workers on our cluster, and then we can calculate the predictions:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于生成样本外的预测也可能需要一些时间，我们将并行处理这个步骤。然而，首先我们需要将模型结果导出到集群中的每个工作节点，然后我们可以计算预测值：
- en: '[PRE23]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we can merge the actual and fitted or predicted values together into
    a dataset, calculate performance measures on each one, and store the overall results
    together for examination and comparison. We can use almost identical code to the
    code that follows to generate out-of-sample performance measures. That code is
    not shown in the book, but is available in the code bundle provided with the book.
    Some additional data-management is required here as sometimes a model may not
    predict each possible response level, but this can make for non-symmetrical frequency
    cross tabs, unless we convert the variable to a factor and specify the levels.
    We also drop `o` values, which indicate the model was uncertain about how to classify
    an observation:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将实际值和拟合值或预测值合并成一个数据集，计算每个值的表现度量，并将整体结果一起存储以供检查和比较。我们可以使用与接下来的代码几乎相同的代码来生成样本外的表现度量。该代码没有在书中显示，但可以在与书一起提供的代码包中找到。这里需要进行一些额外的数据管理，因为有时模型可能无法预测每个可能的响应级别，但这可能会导致非对称的频率交叉表，除非我们将变量转换为因子并指定级别。我们还会删除`o`值，这表示模型对如何分类一个观测结果存在不确定性：
- en: '[PRE24]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If we print the in-sample and out-of-sample performance, we can see how each
    of our models did and the effect of varying some of the tuning parameters. The
    output is shown in the following code. The fourth column (null accuracy) is dropped
    as it is not as important for this comparison:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印出样本内和样本外的表现，我们可以看到每个模型的表现以及调整某些调参参数的效果。输出结果如下所示。第四列（null accuracy）被删除，因为它在这次比较中不那么重要：
- en: '[PRE25]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As a reminder, the in-sample results evaluate the predictions on the training
    data and the out-sample results evaluate the predictions on the holdout (or test)
    data. The best set of hyper-parameters is the last set, where we get an accuracy
    of 93.8% on unseen data. This shows that we are able to classify the types of
    activity people are engaged in quite accurately based on the data from their smartphones.
    We can also see that the more complex models perform better on the in-sample data,
    which is not always the case with out-of-sample performance measures.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒一下，样本内结果评估的是训练数据上的预测，而样本外结果评估的是保留数据（或测试数据）上的预测。最佳的超参数集是最后一组，其中我们在未见过的数据上获得了93.8%的准确率。这表明我们能够基于智能手机的数据相当准确地分类人们从事的活动类型。我们还可以看到，复杂模型在样本内数据上的表现更好，但在样本外表现度量上并不总是如此。
- en: For each model, we have large differences between the accuracy for the in-sample
    data against the out-of-sample data; the models clearly overfit. We will get into
    ways to combat this overfitting in [Chapter 3](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml),* Deep
    Learning Fundamentals*, as we train deep neural networks with multiple hidden
    layers.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个模型，我们在样本内数据和样本外数据的准确率之间存在很大差异；这些模型显然存在过拟合问题。我们将在[第3章](6e6dd858-9f00-454a-8434-a95c59e85b25.xhtml)中探讨如何应对这种过拟合问题，*深度学习基础*，因为我们将在那里训练具有多个隐藏层的深度神经网络。
- en: Despite the slightly worse out-of-sample performance, the models still do well – far
    better than chance alone – and, for our example use case, we could pick the best
    model and be quite confident that using this will provide a good classification
    of a user's activities.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管样本外的表现稍微差一些，但这些模型仍然表现良好——远远超过仅凭运气的表现——对于我们的示例用例，我们可以选择最佳模型，并且可以非常有信心地认为，使用这个模型可以提供准确的用户活动分类。
- en: Summary
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter showed how to get started building and training neural networks
    to classify data, including image recognition and physical activity data. We looked
    at packages that can visualize a neural network and we created a number of models
    to perform classification on data with 10 different categories. Although we only
    used some neural network packages rather than deep learning packages, our models
    took a long time to train and we had issues with overfitting.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示了如何开始构建和训练神经网络来对数据进行分类，包括图像识别和身体活动数据。我们查看了可以可视化神经网络的包，并创建了多个模型，以对具有10个不同类别的数据进行分类。尽管我们只使用了一些神经网络包，而不是深度学习包，但我们的模型训练时间很长，并且遇到了过拟合问题。
- en: Some of the basic neural network models in this chapter took a long time to
    train, even though we did not use all the data available. For the MNIST data,
    we used approx. 8,000 rows for our binary classification task and only 6,000 rows
    for our multi-classification task. Even so, one model took almost an hour to train.
    Our deep learning models will be much more complicated and should be able to process
    millions of records. You can now see why specialist hardware is required for training
    deep learning models.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的一些基本神经网络模型训练时间较长，尽管我们并没有使用所有可用的数据。对于MNIST数据集，我们在二分类任务中使用了大约8,000行数据，在多分类任务中只用了6,000行数据。即便如此，一个模型的训练时间仍然接近一个小时。我们的深度学习模型将更为复杂，并且应该能够处理数百万条记录。现在你应该明白为什么训练深度学习模型需要专门的硬件了。
- en: Secondly, we see that a potential pitfall in machine learning is that more complex
    models will be more likely to overfit the training data, so that evaluating performance
    in the same data used to train the model results in biased, overly optimistic
    estimates of the model performance. Indeed, this can even make a difference as
    to which model is chosen as the best. Overfitting is also an issue for deep neural
    networks. In the next chapter, we will discuss various techniques used to prevent
    overfitting and obtain more accurate estimates of model performance.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们看到机器学习中的一个潜在陷阱是，复杂的模型更容易发生过拟合，因此，在使用训练数据评估模型性能时，可能会得到偏向的、过于乐观的性能估计。事实上，这甚至可能影响到选择哪种模型作为最优模型。过拟合对于深度神经网络也是一个问题。在下一章中，我们将讨论防止过拟合的各种技术，并获得更准确的模型性能评估。
- en: In the next chapter we will look at building a neural network from scratch and
    see how it applies to deep learning. We will also discuss some methods to deal
    with overfitting.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将从零开始构建一个神经网络，并了解它如何应用于深度学习。我们还将讨论一些应对过拟合的方法。
