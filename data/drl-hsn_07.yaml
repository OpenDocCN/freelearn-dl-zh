- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Higher-Level RL Libraries
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级 RL 库
- en: In Chapter [6](#), we implemented the deep Q-network (DQN) model published by
    DeepMind in 2015 [[Mni+15](#)]. This paper had a significant effect on the RL
    field by demonstrating that, despite common belief, it’s possible to use nonlinear
    approximators in RL. This proof of concept stimulated great interest in the deep
    Q-learning field and in deep RL in general.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[6](#)章中，我们实现了由 DeepMind 于 2015 年发布的深度 Q 网络（DQN）模型[[Mni+15](#)]。这篇论文通过证明尽管普遍认为不可能，但在
    RL 中使用非线性近似器是可行的，极大地影响了 RL 领域。这一概念验证激发了深度 Q 学习领域和深度 RL 一般领域的极大兴趣。
- en: In this chapter, we will take another step toward a practical RL by discussing
    higher-level RL libraries, which will allow you to build your code from higher-level
    blocks and focus on the details of the method that you are implementing, avoiding
    reimplementing the same logic multiple times. Most of the chapter will describe
    the PyTorch AgentNet (PTAN) library, which will be used in the rest of the book
    to prevent code repetition, so will be covered in detail.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过讨论高级 RL 库迈向实际应用的 RL，这将使你能够从更高级的模块构建代码，专注于你正在实现方法的细节，避免重复实现相同的逻辑。本章大部分内容将介绍
    PyTorch AgentNet（PTAN）库，它将在本书的其余部分中使用，以防止代码重复，因此会进行详细介绍。
- en: 'We will cover:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下内容：
- en: The motivation for using high-level libraries, rather than reimplementing everything
    from scratch
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高级库的动机，而不是从头重新实现所有内容
- en: The PTAN library, including coverage of the most important parts, which will
    be illustrated with code examples
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PTAN 库，包括最重要部分的覆盖，代码示例将加以说明
- en: DQN on CartPole, implemented using the PTAN library
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 CartPole 上实现的 DQN，使用 PTAN 库
- en: Other RL libraries that you might consider
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可能考虑的其他 RL 库
- en: Why RL libraries?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择 RL 库？
- en: Our implementation of basic DQN in Chapter [6](#) wasn’t very, long and complicated—about
    200 lines of training code plus 50 lines in environment wrappers. When you are
    becoming familiar with RL methods, it is very useful to implement everything yourself
    to understand how things actually work. However, the more involved you become
    in the field, the more often you will realize that you are writing the same code
    over and over again.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[6](#)章中实现的基本 DQN 并不长且复杂——大约 200 行训练代码，加上 50 行环境包装代码。当你开始熟悉 RL 方法时，自己实现所有内容非常有用，可以帮助你理解事物是如何运作的。然而，随着你在该领域的深入，你会越来越频繁地意识到自己在一遍又一遍地编写相同的代码。
- en: This repetition comes from the generality of RL methods. As we discussed in
    Chapter [1](ch005.xhtml#x1-190001), RL is quite flexible, and many real-life problems
    fall into the environment-agent interaction scheme. RL methods don’t make many
    assumptions about the specifics of observations and actions, so code implemented
    for the CartPole environment will be applicable to Atari games (maybe with some
    minor tweaks).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重复来自 RL 方法的通用性。正如我们在第[1](ch005.xhtml#x1-190001)章中讨论的，RL 非常灵活，许多现实问题都可以归结为环境-代理互动模型。RL
    方法对观察和动作的具体内容不做过多假设，因此为 CartPole 环境编写的代码可以适用于 Atari 游戏（可能需要做一些小的调整）。
- en: Writing the same code over and over again is not very efficient, as bugs might
    be introduced every time, which will cost you time for debugging and understanding.
    In addition, carefully designed code that has been used in several projects usually
    has a higher quality in terms of performance, unit tests, readability, and documentation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一遍又一遍地编写相同的代码效率不高，因为每次可能都会引入 bug，这将花费你大量的调试和理解时间。此外，经过精心设计并在多个项目中使用的代码通常在性能、单元测试、可读性和文档方面具有更高的质量。
- en: 'The practical applications of RL are quite young by computer science standards,
    so in comparison to other more mature domains, you might not have that rich a
    choice of approaches. For example, in web development, even if you limit yourself
    to just Python, you have hundreds of very good libraries of all sorts: Django
    for heavyweight, fully functional websites; Flask for light Web Server Gateway
    Interface (WSGI) apps; and much more, large and small.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于其他更加成熟的领域，RL 的实际应用在计算机科学标准下相对较年轻，因此你可能没有那么丰富的选择。例如，在 Web 开发中，即便你只使用 Python，你也有数百个非常优秀的库可供选择：Django
    用于重型、功能齐全的网站；Flask 用于轻量级的 Web 服务器网关接口（WSGI）应用；还有许多其他大小不一的库。
- en: RL is not as mature as web frameworks, but still, you can choose from several
    projects that are trying to simplify RL practitioners’ lives. In addition, you
    can always write your own set of tools, as I did several years ago. The tool I
    created is a library called PTAN, and, as mentioned, it will be used in the rest
    of the book to illustrate examples.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）不像 Web 框架那样成熟，但你仍然可以从几个简化强化学习实践者生活的项目中进行选择。此外，你始终可以像我几年前那样编写一套自己的工具。我创建的工具是一个名为
    PTAN 的库，正如前面所提到的，它将在本书的后续部分中用于演示实例。
- en: The PTAN library
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PTAN 库
- en: 'This library is available on GitHub: [https://github.com/Shmuma/ptan](https://github.com/Shmuma/ptan).
    All the subsequent examples were implemented using version 0.8 of PTAN, which
    can be installed in your virtual environment by running the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该库可以在 GitHub 上找到：[https://github.com/Shmuma/ptan](https://github.com/Shmuma/ptan)。所有后续示例都是使用
    PTAN 0.8 版本实现的，可以通过以下命令在虚拟环境中安装：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The original goal of PTAN was to simplify my RL experiments, and it tries to
    keep the balance between two extremes:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: PTAN 的最初目标是简化我的强化学习实验，它试图在两种极端之间找到平衡：
- en: Import the library and then write just a couple of lines with tons of parameters
    to train one of the provided methods, like DQN (a very vivid example is the OpenAI
    Baselines and Stable Baselines3 projects). This first approach is very inflexible.
    It works well when you are using the library the way it is supposed to be used.
    But if you want to do something fancy, you will quickly find yourself hacking
    the library and fighting with the constraints it imposes, rather than solving
    the problem you want to solve.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入库后，只需写几行包含大量参数的代码，就可以训练提供的某个方法，例如 DQN（一个非常生动的例子是 OpenAI Baselines 和 Stable
    Baselines3 项目）。这种方法非常不灵活。当你按照库的预期使用时，它能很好地工作。但如果你想做一些复杂的操作，很快你就会发现自己在破解库并与其施加的约束作斗争，而不是解决你想解决的问题。
- en: Implement all the method’s logic from scratch. This second extreme gives too
    much freedom and requires implementing replay buffers and trajectory handling
    over and over again, which is error-prone, boring, and inefficient.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始实现所有方法的逻辑。第二种极端方式提供了过多的自由度，需要一遍又一遍地实现重放缓冲区和轨迹处理，这既容易出错，又乏味且低效。
- en: PTAN tries to balance those extremes, providing high-quality building blocks
    to simplify your RL code, but at the same time being flexible and not limiting
    your creativity.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PTAN 尝试在这两种极端之间找到平衡，提供高质量的构建块来简化你的强化学习代码，同时保持灵活性，不限制你的创造力。
- en: 'At a high level, PTAN provides the following entities:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，PTAN 提供了以下实体：
- en: 'Agent: A class that knows how to convert a batch of observations to a batch
    of actions to be executed. It can contain an optional state, in case you need
    to track some information between consequent actions in one episode. (We will
    use this approach in Chapter [15](ch019.xhtml#x1-27200015), in the deep deterministic
    policy gradient (DDPG) method, which includes the Ornstein–Uhlenbeck random process
    for exploration.) The library provides several agents for the most common RL cases,
    but you always can write your own subclass of BaseAgent if none of the predefined
    classes are meeting your needs.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agent：一个知道如何将一批观察转化为一批待执行动作的类。它可以包含一个可选的状态，以便在一个回合内跟踪连续动作之间的信息。（我们将在第[15](ch019.xhtml#x1-27200015)章的深度确定性策略梯度（DDPG）方法中使用这种方法，其中包括用于探索的
    Ornstein–Uhlenbeck 随机过程。）该库为最常见的强化学习案例提供了多个代理，但如果没有预定义的类能满足你的需求，你始终可以编写自己的 BaseAgent
    子类。
- en: 'ActionSelector: A small piece of logic that knows how to choose the action
    from some output of the network. It works in tandem with the Agent class.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ActionSelector：一小段逻辑，知道如何从网络的某些输出中选择动作。它与 Agent 类协同工作。
- en: 'ExperienceSource and subclasses: The Agent instance and a Gym environment object
    can provide information about the trajectory of the agent during the episodes.
    In its simplest form, it is one single (a, r, s′) transition at a time, but its
    functionality goes beyond this.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExperienceSource 及其子类：Agent 实例和 Gym 环境对象可以提供有关代理在回合中轨迹的信息。它的最简单形式是一次性提供一个（a,
    r, s′）过渡，但它的功能不仅限于此。
- en: 'ExperienceSourceBuffer and subclasses: Replay buffers with various characteristics.
    They include a simple replay buffer and two versions of prioritized replay buffers.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExperienceSourceBuffer 及其子类：具有各种特征的重放缓冲区。它们包括一个简单的重放缓冲区和两个版本的优先级重放缓冲区。
- en: 'Various utility classes: An example is TargetNet and wrappers for time-series
    preprocessing (used for tracking training progress in TensorBoard).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种实用工具类：例如，TargetNet和用于时间序列预处理的包装器（用于在TensorBoard中跟踪训练进度）。
- en: 'PyTorch Ignite helpers: These can be used to integrate PTAN into the Ignite
    framework.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch Ignite助手：可以用来将PTAN集成到Ignite框架中。
- en: 'Wrappers for Gym environments: For example, wrappers for Atari games (very
    similar to the wrappers we described in Chapter [6](#)).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gym环境的包装器：例如，针对Atari游戏的包装器（与我们在第[6](#)章中描述的包装器非常相似）。
- en: And that’s basically it. In the following sections, we will take a look at these
    entities in detail.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上就是这样。在接下来的章节中，我们将详细了解这些实体。
- en: Action selectors
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作选择器
- en: 'In PTAN terminology, an action selector is an object that helps with going
    from network output to concrete action values. The most common cases include:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在PTAN术语中，动作选择器是一个帮助从网络输出到具体动作值的对象。最常见的情况包括：
- en: 'Greedy (or argmax): Commonly used by Q-value methods when the network predicts
    Q-values for a set of actions and the desired action is the action with the largest
    Q(s,a).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贪婪（或argmax）：Q值方法常用的，当网络为一组动作预测Q值时，所需的动作是具有最大Q(s,a)的动作。
- en: 'Policy-based: The network outputs the probability distribution (in the form
    of logits or normalized distribution), and an action needs to be sampled from
    this distribution. You have already seen this in Chapter [4](ch008.xhtml#x1-740004),
    when we discussed the cross-entropy method.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于策略：网络输出概率分布（以logits或归一化分布的形式），需要从该分布中采样一个动作。你在第[4](ch008.xhtml#x1-740004)章讨论交叉熵方法时已经看过这个。
- en: 'An action selector is used by the Agent and rarely needs to be customized (but
    you have the option). The concrete classes provided by the library are:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 动作选择器由代理使用，通常不需要自定义（但你有这个选项）。库提供的具体类包括：
- en: 'ArgmaxActionSelector: Applies argmax on the second axis of a passed tensor.
    It assumes a matrix with batch dimension along the first axis.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ArgmaxActionSelector：在传入张量的第二个维度上应用argmax。它假设矩阵的第一维是batch维度。
- en: 'ProbabilityActionSelector: Samples from the probability distribution of a discrete
    set of actions.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ProbabilityActionSelector：从离散动作集的概率分布中采样。
- en: 'EpsilonGreedyActionSelector: Has the epsilon parameter, which specifies the
    probability of a random action being taken. It also holds another ActionSelector
    instance, which is used when we’re not sampling random actions.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EpsilonGreedyActionSelector：具有epsilon参数，指定随机动作被执行的概率。它还持有另一个ActionSelector实例，当我们不采样随机动作时使用它。
- en: 'All the classes assume that NumPy arrays will be passed to them. The complete
    example from this section can be found in Chapter07/01_actions.py. Here, I’m going
    to show you how these classes are supposed to be used:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所有类假设会将NumPy数组传递给它们。此章节的完整示例可以在Chapter07/01_actions.py中找到。这里，我将向你展示如何使用这些类：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, the selector returns indices of actions with the largest values.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，选择器返回具有最大值的动作的索引。
- en: 'The next action selector is EpisilonGreedyActionSelector, which ”wraps” another
    action selector and, depending on the epsilon parameter, either uses the wrapped
    action selector or takes the random action. This action selector is used during
    training to introduce randomness to the agent’s actions. If epsilon is 0.0, no
    random actions are taken:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个动作选择器是EpisilonGreedyActionSelector，它“包装”另一个动作选择器，并根据epsilon参数，使用包装的动作选择器或采取随机动作。这个动作选择器在训练过程中用于为代理的动作引入随机性。如果epsilon是0.0，则不会采取随机动作：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we change epsilon to 1, actions will be random:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将epsilon更改为1，动作将变为随机：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can also change the value of epsilon by assigning the action selector’s
    attribute:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以通过为动作选择器的属性赋值来更改epsilon的值：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Working with ProbabilityActionSelector is the same, but the input needs to
    be a normalized probability distribution:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ProbabilityActionSelector的方法是一样的，但输入需要是一个归一化的概率分布：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the preceding example, we sample from three distributions (as we have three
    rows in the passed matrix):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们从三个分布中进行采样（因为我们在传入的矩阵中有三行）：
- en: The first is defined by the vector [0.1, 0.8, 0.1]; as a result, the action
    with index 1 is chosen with probability 80%
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个由向量[0.1, 0.8, 0.1]定义；因此，索引为1的动作以80%的概率被选中
- en: The vector [0.0, 0.0, 1.0] always gives us an action with index 2
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量[0.0, 0.0, 1.0]总是给我们动作2的索引
- en: The distribution [0.5, 0.5, 0.0] produces actions 0 and 1 with 50% chance
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布[0.5, 0.5, 0.0]以50%的几率产生动作0和动作1
- en: The agent
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理
- en: The agent entity provides an unified way of bridging observations from the environment
    and the actions that we want to execute. So far, you have seen only a simple,
    stateless DQN agent that uses a neural network (NN) to obtain action values from
    the current observation and behaves greedily on those values. We have used epsilon-greedy
    behavior to explore the environment, but this doesn’t change the picture much.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体实体提供了一种统一的方式，将来自环境的观察与我们想要执行的动作连接起来。到目前为止，你只看到了一个简单的无状态DQN智能体，该智能体使用神经网络（NN）从当前观察中获取动作值，并根据这些值贪婪地做出决策。我们使用epsilon-贪婪策略来探索环境，但这并没有显著改变局面。
- en: In the RL field, this could be more complicated. For example, instead of predicting
    the values of the actions, our agent could predict a probability distribution
    over the actions. Such agents are called policy agents, and we will talk about
    those methods in Part 3 of the book.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）领域，这可能会更加复杂。例如，我们的智能体可能不是预测动作的值，而是预测动作上的概率分布。这样的智能体被称为策略智能体，我们将在本书的第三部分讨论这些方法。
- en: In some situations, it might be neccesary for the agent to keep state between
    observations. For example, very often, one observation (or even the k last observations)
    is not enough to make a decision about the action, and we want to keep some memory
    in the agent to capture the necessary information. There is a whole subdomain
    of RL that tries to address this complication with partially observable Markov
    decision process (POMDP) formalism, which we briefly mentioned in Chapter [6](#)
    but is not covered extensively in the book.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，智能体可能需要在观察之间保持状态。例如，通常情况下，单一的观察（甚至是最近的k个观察）不足以做出关于动作的决策，我们希望在智能体中保留一些记忆，以捕捉必要的信息。强化学习中有一个子领域试图通过部分可观察马尔可夫决策过程（POMDP）来解决这个问题，我们在第[6](#)章中简要提到过，但在本书中没有广泛覆盖。
- en: The third variant of the agent is very common in continuous control problems,
    which will be discussed in Part 4 of the book. For now, it suffices to say that
    in such cases, actions are not discrete anymore but continuous values, and the
    agent needs to predict them from the observations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体的第三种变体在连续控制问题中非常常见，这将在本书的第四部分讨论。目前，只需说在这种情况下，动作不再是离散的，而是连续的值，智能体需要根据观察来预测这些值。
- en: To capture all those variants and make the code flexible, the agent in PTAN
    is implemented as an extensible hierarchy of classes with the ptan.agent.BaseAgent
    abstract class at the top. From a high level, the agent needs to accept the batch
    of observations (in the form of a NumPy array or a list of NumPy arrays) and return
    the batch of actions that it wants to take. The batch is used to make the processing
    more efficient, as processing several observations in one pass in a graphics processing
    unit (GPU) is frequently much faster than processing them individually.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉所有这些变体并使代码具有灵活性，PTAN中的智能体是通过一个可扩展的类层次结构实现的，ptan.agent.BaseAgent抽象类位于顶部。从高层来看，智能体需要接受一批观察（以NumPy数组或NumPy数组列表的形式），并返回它想要执行的动作批次。批次的使用可以使处理更加高效，因为在图形处理单元（GPU）中一次性处理多个观察通常比逐个处理更快。
- en: 'The abstract base class doesn’t define the types of input and output, which
    makes it very flexible and easy to extend. For example, in the continuous domain,
    our actions will no longer be indices of discrete actions, but float values. In
    any case, the agent can be seen as something that knows how to convert observations
    into actions, and it’s up to the agent how to do this. In general, there are no
    assumptions made on observation and action types, but the concrete implementation
    of agents is more limiting. PTAN provides two of the most common ways to convert
    observations into actions: DQNAgent and PolicyAgent. We will explore these in
    subsequent sections.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象基类没有定义输入和输出的类型，这使得它非常灵活且易于扩展。例如，在连续域中，我们的动作不再是离散动作的索引，而是浮动值。在任何情况下，智能体可以被视为一种知道如何将观察转换为动作的实体，如何做到这一点由智能体决定。通常，对于观察和动作类型没有假设，但智能体的具体实现则更具限制性。PTAN提供了两种将观察转换为动作的常见方法：DQNAgent和PolicyAgent。我们将在后续章节中探讨这些方法。
- en: 'However, in real problems, a custom agent is often needed. These are some of
    the reasons:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实际问题中，通常需要定制的智能体。这些是一些原因：
- en: The architecture of the NN is fancy—its action space is a mixture of continuous
    and discrete and it has multimodal observations (text and pixels, for example),
    or something like that.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的架构很复杂——它的动作空间是连续和离散的混合，并且它有多模态的观察（例如文本和像素），或者类似的东西。
- en: You want to use non-standard exploration strategies, for example, the Ornstein–Uhlenbeck
    process (a very popular exploration strategy in the continuous control domain).
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你想使用非标准的探索策略，例如 Ornstein–Uhlenbeck 过程（在连续控制领域中是一种非常流行的探索策略）。
- en: You have a POMDP environment, and the agent’s decision is not fully defined
    by observations, but by some internal agent state (which is also the case for
    Ornstein–Uhlenbeck exploration).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有一个 POMDP 环境，智能体的决策不仅仅由观察定义，还由某些内部状态（这对于 Ornstein–Uhlenbeck 探索也是如此）决定。
- en: All those cases are easily supported by subclassing the BaseAgent class, and
    in the rest of the book, several examples of such redefinition will be given.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些情况都可以通过子类化 BaseAgent 类轻松支持，书中的后续部分将给出几个这样的重定义示例。
- en: 'Let’s now check the standard agents provided by the library: DQNAgent and PolicyAgent.
    The complete example is in Chapter07/02_agents.py.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看库中提供的标准智能体：DQNAgent 和 PolicyAgent。完整示例在 Chapter07/02_agents.py 中。
- en: DQNAgent
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DQNAgent
- en: This class is applicable in Q-learning when the action space is not very large,
    which covers Atari games and lots of classical problems. This representation is
    not universal and, later in the book, you will see ways of dealing with that.
    DQNAgent takes a batch of observations as input (as a NumPy array), applies the
    network to them to get Q-values, and then uses the provided ActionSelector to
    convert Q-values to indices of actions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类适用于 Q-learning，当动作空间不是很大的时候，涵盖了 Atari 游戏和许多经典问题。这个表示方式不是普适的，后面书中会介绍如何处理这种情况。DQNAgent
    接收一批观察数据作为输入（作为 NumPy 数组），将网络应用到这些数据上以获得 Q 值，然后使用提供的 ActionSelector 将 Q 值转换为动作的索引。
- en: Let’s consider a small example. For simplicity, our network always produces
    the same output for the input batch.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的例子。为了简化起见，我们的网络始终为输入批次产生相同的输出。
- en: 'First, we define the NN class, which is supposed to convert observations to
    actions. In our example, it doesn’t use NNs at all and always produces the same
    output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义神经网络类，它应该将观察转换为动作。在我们的例子中，它根本不使用神经网络，并始终产生相同的输出：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once we have defined the model class, we can use it as a DQN model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了模型类，就可以将其用作 DQN 模型：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We start with the simple argmax policy (which returns the action with the largest
    value), so the agent will always return actions corresponding to ones in the network
    output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从简单的 argmax 策略开始（该策略返回值最大的动作），因此智能体将始终返回与网络输出中对应的动作：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the input, a batch of two observations, each having five values, was given,
    and in the output, the agent returned a tuple of two objects:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入中，给定了一批两条观察数据，每条包含五个值；在输出中，智能体返回了一个包含两个对象的元组：
- en: An array with actions to be executed for our batch. In our case, this is action
    0 for the first batch sample and action 1 for the second.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个数组，表示我们批次中要执行的动作。在我们的例子中，对于第一批样本是动作 0，第二批样本是动作 1。
- en: A list with the agent’s internal state. This is used for stateful agents and
    is a list of None in our case. As our agent is stateless, you can ignore it.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含智能体内部状态的列表。对于有状态的智能体，这个列表很有用，而在我们的例子中，它是一个包含 None 的列表。由于我们的智能体是无状态的，可以忽略它。
- en: 'Now, let’s make the agent with an epsilon-greedy exploration strategy. To do
    this, we just need to pass a different action selector:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使智能体具备 epsilon-greedy 探索策略。为此，我们只需要传递一个不同的动作选择器：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As epsilon is 1.0, all the actions will be random, regardless of the network’s
    output. But we can change the epsilon value on the fly, which is very handy during
    the training when we anneal epsilon over time:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 epsilon 为 1.0，所有动作都会是随机的，与网络的输出无关。但是我们可以在训练过程中动态地更改 epsilon 的值，这在逐步降低 epsilon
    时非常方便：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: PolicyAgent
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PolicyAgent
- en: PolicyAgent expects the network to produce a policy distribution over a discrete
    set of actions. The policy distribution could be either logits (unnormalized)
    or a normalized distribution. In practice, you should always use logits to improve
    the numeric stability of the training process.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: PolicyAgent 期望网络为离散的动作集生成一个策略分布。策略分布可以是 logits（未归一化的）或者归一化的分布。实际上，你应该始终使用 logits
    以提高训练过程的数值稳定性。
- en: Let’s reimplement our previous example, but now, the network will produce a
    probability.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新实现之前的例子，但这次网络将产生一个概率值。
- en: 'We begin by defining the following class:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义以下类：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The class above could be used to get the action logits for a batch of observations:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 上述类可用于获取一批观察的动作对数：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we can use PolicyAgent in combination with ProbabilityActionSelector.
    As the latter expects normalized probabilities, we need to ask PolicyAgent to
    apply softmax to the network’s output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将 PolicyAgent 与 ProbabilityActionSelector 结合使用。由于后者期望的是归一化的概率，我们需要要求
    PolicyAgent 对网络的输出应用 softmax：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Please note that the softmax operation produces non-zero probabilities for
    zero logits, so our agent can still select actions with zero logit values:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，softmax 操作会对零对数值产生非零概率，因此我们的代理仍然可以选择具有零对数值的动作。
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Experience source
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经验源
- en: The agent abstraction described in the previous section allows us to implement
    environment communications in a generic way. These communications happen in the
    form of trajectories, produced by applying the agent’s actions to the Gym environment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节中描述的代理抽象使我们能够以通用的方式实现环境通信。这些通信通过应用代理的动作到 Gym 环境中，形成轨迹。
- en: 'At a high level, experience source classes take the agent instance and environment
    and provide you with step-by-step data from the trajectories. The functionality
    of those classes includes:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，经验源类获取代理实例和环境，并为你提供来自轨迹的逐步数据。这些类的功能包括：
- en: Support for multiple environments being communicated at the same time. This
    allows efficient GPU utilization as a batch of observations is being processed
    by the agent at once.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持同时与多个环境进行通信。这使得在一次处理一批观察时能够有效利用 GPU。
- en: A trajectory can be preprocessed and presented in a convenient form for further
    training. For example, there is an implementation of subtrajectory rollouts with
    accumulation of the reward. That preprocessing is convenient for DQN and n-step
    DQN, when we are not interested in individual intermediate steps in subtrajectories,
    so they can be dropped. This saves memory and reduces the amount of code we need
    to write.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轨迹可以预处理并以方便的形式呈现以供进一步训练。例如，有一种实现子轨迹回滚并累积奖励的方式。对于 DQN 和 n 步 DQN 来说，这种预处理非常方便，因为我们不关心子轨迹中的个别中间步骤，所以可以省略它们。这样可以节省内存，并减少我们需要编写的代码量。
- en: Support for vectorized environments from Gymnasium (classes AsyncVectorEnv and
    SyncVectorEnv). We will cover this in Chapter [17](ch021.xhtml#x1-31100017).
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持来自 Gymnasium 的向量化环境（AsyncVectorEnv 和 SyncVectorEnv 类）。我们将在第[17章](ch021.xhtml#x1-31100017)讨论这个话题。
- en: So, the experience source classes act as a ”magic black box” to hide the environment
    interaction and trajectory handling complexities from the library user. But the
    overall PTAN philosophy is to be flexible and extensible, so if you want, you
    can subclass one of the existing classes or implement your own version as needed.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，经验源类充当了一个“魔法黑箱”，隐藏了环境交互和轨迹处理的复杂性，让库用户不必处理这些问题。但整体的 PTAN 哲学是灵活和可扩展的，所以如果你愿意，你可以子类化现有的类或根据需要实现自己的版本。
- en: 'Three classes are provided by the system:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 系统提供了三个类：
- en: 'ExperienceSource: Using the agent and the set of environments, it produces
    n-step subtrajectories with all intermediate steps.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExperienceSource：通过使用代理和环境集，它生成包含所有中间步骤的 n 步子轨迹。
- en: 'ExperienceSourceFirstLast: This is the same as ExperienceSource, but instead
    of the full subtrajectory (with all steps), it keeps only the first and last steps,
    with proper reward accumulation in between. This can save a lot of memory in the
    case of n-step DQN or advantage actor-critic (A2C) rollouts.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExperienceSourceFirstLast：这与 ExperienceSource 相同，但它仅保留第一步和最后一步的子轨迹，并在两者之间进行适当的奖励累积。对于
    n 步 DQN 或优势演员-评论家（A2C）回滚，这可以节省大量内存。
- en: 'ExperienceSourceRollouts: This follows the asynchronous advantage actor-critic
    (A3C) rollouts scheme described in Mnih’s paper about Atari games (we will discuss
    this topic in Chapter [12](ch016.xhtml#x1-20300012)).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExperienceSourceRollouts：这遵循 Mnih 在关于 Atari 游戏的论文中描述的异步优势演员-评论家（A3C）回滚方案（我们将在第[12章](ch016.xhtml#x1-20300012)讨论这个话题）。
- en: All the classes are written to be efficient both in terms of central processing
    unit (CPU) and memory, which is not very important for toy problems, but will
    become relevant in the next chapter when we get to Atari games with much larger
    amounts of data to be stored and processed.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的类都被编写得既高效地使用中央处理单元（CPU），也高效地使用内存，这对于玩具问题来说并不重要，但在下一章当我们进入 Atari 游戏时，涉及到需要存储和处理大量数据的问题，这一点就显得非常重要。
- en: Toy environment
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 玩具环境
- en: 'For demonstration, we will implement a very simple Gym environment with a small
    predictable observation state to show how ExperienceSource classes work. This
    environment has integer observation, which increases from 0 to 4, integer action,
    and a reward equal to the action given. All episodes produced by the environment
    always have 10 steps:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，我们将实现一个非常简单的Gym环境，具有一个小而可预测的观察状态，来展示`ExperienceSource`类如何工作。这个环境的观察值是整数，从0到4，动作也是整数，奖励等于给定的动作。环境产生的所有回合总是有10个步骤：
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In addition to this environment, we will use an agent that always generates
    fixed actions regardless of observations:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个环境，我们还将使用一个代理，它会根据观察结果始终生成固定的动作：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Both classes are defined in the Chapter07/lib.py module. Now that we have defined
    the agent, let’s talk about the data it produces.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个类都定义在`Chapter07/lib.py`模块中。现在我们已经定义了代理，接下来我们讨论它产生的数据。
- en: The ExperienceSource class
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`ExperienceSource`类'
- en: 'The first class we will discuss is ptan.experience.ExperienceSource, which
    generates chunks of agent trajectories of the given length. The implementation
    automatically handles the end-of-episode situation (when the step() method in
    the environment returns is_done=True) and resets the environment. The constructor
    accepts several arguments:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第一个类是`ptan.experience.ExperienceSource`，它生成给定长度的代理轨迹片段。实现会自动处理回合结束的情况（即环境中的`step()`方法返回`is_done=True`），并重置环境。构造函数接受多个参数：
- en: The Gym environment to be used. Alternatively, it could be the list of environments.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将要使用的Gym环境。或者，也可以是环境列表。
- en: The agent instance.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理实例。
- en: 'steps_count=2: The length of subtrajectories to be generated.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`steps_count=2`：要生成的子轨迹的长度。'
- en: 'The class instance provides the standard Python iterator interface, so you
    can just iterate over it to get subtrajectories:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 该类实例提供标准的Python迭代器接口，因此你可以直接迭代它以获取子轨迹：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'On every iteration, ExperienceSource returns a piece of the agent’s trajectory
    in environment communication. It might look simple, but there are several things
    happening under the hood of our example:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次迭代中，`ExperienceSource`返回代理在与环境交互时的一段轨迹。它看起来可能很简单，但我们的示例背后有几件事在发生：
- en: reset() was called in the environment to get the initial state.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用了环境中的`reset()`以获取初始状态。
- en: The agent was asked to select the action to execute from the state returned.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理被要求从返回的状态中选择要执行的动作。
- en: The step() method was executed to get the reward and the next state.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`step()`方法以获得奖励和下一个状态。
- en: This next state was passed to the agent for the next action.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个下一个状态被传递给代理，以供其执行下一个动作。
- en: Information about the transition from one state to the next state was returned.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回了从一个状态到下一个状态的转移信息。
- en: If the environment returns the end-of-episode flag, we emit the rest of the
    trajectory and reset the environment to start over.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果环境返回回合结束标志，我们就会输出剩余的轨迹并重置环境以重新开始。
- en: The process continues (from step 3) during the iteration over the experience
    source.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在对经验源的迭代过程中，过程继续（从第3步开始）。
- en: If the agent changes the way it generates actions (we can get this by updating
    the network weights, decreasing epsilon, or by some other means), it will immediately
    affect the experience trajectories that we get.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果代理改变了它生成动作的方式（我们可以通过更新网络权重、减少epsilon或其他方法来实现），它将立即影响我们获得的经验轨迹。
- en: 'The ExperienceSource instance returns tuples with lengths equal to or less
    than the step_count argument passed on construction. In our case, we asked for
    two-step subtrajectories, so tuples will be of length 2 or 1 (at the end of episodes).
    Every object in a tuple is an instance of the ptan.experience.Experience class,
    which is a dataclass with the following fields:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExperienceSource`实例返回的元组的长度等于或小于构造时传入的`step_count`参数。在我们的例子中，我们要求的是两个步骤的子轨迹，因此元组的长度为2或1（在回合结束时）。元组中的每个对象都是`ptan.experience.Experience`类的实例，这是一个包含以下字段的数据类：'
- en: 'state: The state we observed before taking the action'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`state`：我们在采取行动前观察到的状态'
- en: 'action: The action we completed'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`action`：我们完成的动作'
- en: 'reward: The immediate reward we got from env'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reward`：我们从环境中获得的即时奖励'
- en: 'done_trunc: Whether the episode was done or truncated'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`done_trunc`：回合是否结束或被截断'
- en: 'If the episode reaches the end, the subtrajectory will be shorter and the underlying
    environment will be reset automatically, so we don’t need to bother with this
    and can just keep iterating:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果回合结束，子轨迹将会更短，且底层环境会自动重置，因此我们无需担心这个问题，可以继续迭代：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can ask ExperienceSource for subtrajectories of any length:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以向 ExperienceSource 请求任意长度的子轨迹：
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can pass it several instances of gym.Env. In that case, they will be used
    in a round-robin fashion:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以传递多个 gym.Env 实例。在这种情况下，它们将按轮流方式使用：
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Please note that when you’re passing several environments to the ExperienceSource,
    they have to be independent instances and not a single environment instance, otherwise
    your observations will become a mess.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当你将多个环境传递给 ExperienceSource 时，它们必须是独立的实例，而不是单一环境实例，否则你的观察将变得混乱。
- en: The ExperienceSourceFirstLast Class
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ExperienceSourceFirstLast 类
- en: The ExperienceSource class provides us with full subtrajectories of the given
    length as a list of (s, a, r) objects. The next state, s′, is returned in the
    next tuple, which is not always convenient. For example, in DQN training, we want
    to have tuples (s, a, r, s′) at once to do one-step Bellman approximation during
    the training. In addition, some extension of DQN, like n-step DQN, might want
    to collapse longer sequences of observations into (first-state, action, total-reward-for-n-steps,
    state-after-step-n).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ExperienceSource 类为我们提供了指定长度的完整子轨迹，作为 (s, a, r) 对象的列表。下一个状态 s′ 会在下一个元组中返回，这有时不太方便。例如，在
    DQN 训练中，我们希望一次性获得 (s, a, r, s′) 元组，以便在训练过程中进行一步 Bellman 近似。此外，DQN 的一些扩展，如 n 步
    DQN，可能希望将更长的观察序列合并为 (first-state, action, total-reward-for-n-steps, state-after-step-n)。
- en: 'To support this in a generic way, a simple subclass of ExperienceSource is
    implemented: ExperienceSourceFirstLast. It accepts almost the same arguments in
    the constructor, but returns different data:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以通用的方式支持这一点，已经实现了一个 ExperienceSource 的简单子类：ExperienceSourceFirstLast。它在构造函数中接受几乎相同的参数，但返回不同的数据：
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, instead of the tuple, it returns a single object on every iteration, which
    is again a dataclass with the following fields:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，它不再返回元组，而是每次迭代返回一个单一的对象，这个对象也是一个数据类，包含以下字段：
- en: 'state: The state we used to decide on the action to take.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'state: 我们用来决定采取什么动作的状态。'
- en: 'action: The action we took at this step.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'action: 我们在这一步骤采取的动作。'
- en: 'reward: The partial accumulated reward for steps_count (in our case, steps_count=1,
    so it is equal to the immediate reward).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'reward: 对于 steps_count（在我们的案例中，steps_count=1，因此它等于即时奖励）的部分累计奖励。'
- en: 'last_state: The state we got after executing the action. If our episode ends,
    we have None here.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'last_state: 执行动作后得到的状态。如果我们的回合结束，这里是 None。'
- en: This data is much more convenient for DQN training, as we can apply Bellman
    approximation directly to it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据对于 DQN 训练更为方便，因为我们可以直接应用 Bellman 近似。
- en: 'Let’s check the result with a larger number of steps:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下使用更多步数时的结果：
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'So, now we are collapsing two steps on every iteration and calculating the
    immediate reward (that’s why reward=2.0 for most of the samples). More interesting
    samples are at the end of the episode:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们在每次迭代中合并了两步，并计算即时奖励（这就是为什么大多数样本的 reward=2.0）。回合结束时更有趣的样本：
- en: '[PRE23]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As the episode ends, we have last_state=None in those samples, but additionally,
    we calculate the reward for the tail of the episode. Those tiny details are very
    easy to implement wrongly if you are doing all the trajectory handling yourself.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当回合结束时，我们在这些样本中将 last_state=None，但此外，我们会计算回合尾部的奖励。如果你自己处理所有的轨迹，这些细节非常容易出错。
- en: Experience replay buffers
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经验回放缓存
- en: In DQN, we rarely deal with immediate experience samples, as they are heavily
    correlated, which leads to instability in the training. Normally, we have large
    replay buffers, which are populated with experience pieces. Then the buffer is
    sampled (randomly or with priority weights) to get the training batch. The replay
    buffer normally has a maximum capacity, so old samples are pushed out when the
    replay buffer reaches the limit.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DQN 中，我们很少处理即时经验样本，因为它们高度相关，这会导致训练的不稳定。通常，我们有一个大的回放缓存，它充满了经验片段。然后从缓存中进行采样（随机或带优先级权重），以获取训练批次。回放缓存通常有一个最大容量，因此当回放缓存达到上限时，旧样本会被推送出去。
- en: 'There are several implementation tricks here, which become extremely important
    when you need to deal with large problems:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个实现技巧，当你需要处理大型问题时，这些技巧非常重要：
- en: How to efficiently sample from a large buffer
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何高效地从大缓存中采样
- en: How to push old samples from the buffer
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何从缓存中推送旧样本
- en: In the case of a prioritized buffer, how priorities need to be maintained and
    handled in the most efficient way
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在优先级缓存的情况下，如何以最有效的方式维护和处理优先级
- en: All this becomes a quite non-trivial task if you want to deal with Atari games,
    keeping 10-100M samples, where every sample is an image from the game. A small
    mistake can lead to a 10-100x memory increase and major slowdowns in the training
    process.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想处理 Atari 游戏，保持 10-100M 样本，其中每个样本都是游戏中的一张图片，这一切就变成了一项相当复杂的任务。一个小错误可能导致 10-100
    倍的内存增加，并且会严重拖慢训练过程。
- en: 'PTAN provides several variants of replay buffers, which integrate simply with
    the ExperienceSource and Agent machinery. Normally, what you need to do is ask
    the buffer to pull a new sample from the source and sample the training batch.
    The provided classes are:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: PTAN 提供了几种重放缓冲区的变体，它们可以与 ExperienceSource 和 Agent 架构轻松集成。通常，您需要做的是请求缓冲区从源中提取一个新样本并采样训练批次。提供的类包括：
- en: 'ExperienceReplayBuffer: A simple replay buffer of a predefined size with uniform
    sampling.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ExperienceReplayBuffer：一个简单的、大小预定义的重放缓冲区，采用均匀采样。
- en: 'PrioReplayBufferNaive: A simple, but not very efficient, prioritized replay
    buffer implementation. The complexity of sampling is O(n), which might become
    an issue with large buffers. This version has the advantage over the optimized
    class, having much easier code. For medium-sized buffers the performance is still
    acceptable, so we will use it in some examples.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PrioReplayBufferNaive：一种简单但效率不高的优先级重放缓冲区实现。采样复杂度为 O(n)，对于大缓冲区来说可能成为一个问题。这个版本相比优化后的类，代码更简单。对于中等大小的缓冲区，性能仍然可以接受，因此我们会在一些示例中使用它。
- en: 'PrioritizedReplayBuffer: Uses segment trees for sampling, which makes the code
    cryptic, but with O(log(n)) sampling complexity.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PrioritizedReplayBuffer：使用线段树进行采样，这使得代码变得晦涩，但采样复杂度为 O(log(n))。
- en: 'The following shows how the replay buffer could be used:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了如何使用重放缓冲区：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'All replay buffers provide the following interface:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 所有重放缓冲区提供以下接口：
- en: A Python iterator interface to walk over all the samples in the buffer
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Python 迭代器接口，用于遍历缓冲区中的所有样本。
- en: The populate(N) method to get N samples from the experience source and put them
    into the buffer
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: populate(N) 方法用于从经验源中获取 N 个样本并将它们放入缓冲区。
- en: The method sample(N) to get the batch of N experience objects
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方法 sample(N) 用于获取 N 个经验对象的批次。
- en: 'So, the normal training loop for DQN looks like an infinite repetition of the
    following steps:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，DQN 的正常训练循环看起来像是以下步骤的无限重复：
- en: Call buffer.populate(1) to get a fresh sample from the environment.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 buffer.populate(1) 从环境中获取一个新样本。
- en: Call batch = buffer.sample(BATCH_SIZE) to get the batch from the buffer.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 batch = buffer.sample(BATCH_SIZE) 从缓冲区中获取批次。
- en: Calculate the loss on the sampled batch.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所采样批次的损失。
- en: Backpropagate.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播。
- en: Repeat until convergence (hopefully).
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复直到收敛（希望如此）。
- en: 'All the rest happens automatically-—resetting the environment, handling subtrajectories,
    buffer size maintenance, and so on:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的过程自动完成——重置环境、处理子轨迹、维护缓冲区大小等：
- en: '[PRE25]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The TargetNet class
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TargetNet 类
- en: We mentioned the bootstrapping problem in the previous chapter, when the network
    used for the next state evaluation becomes influenced by our training process.
    This was solved by disentangling the currently trained network from the network
    used for next-state Q-values prediction.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一章中提到过自举问题，当用于下一个状态评估的网络受到我们训练过程的影响时，这一问题会出现。通过将当前训练的网络与用于预测下一个状态 Q 值的网络分离，解决了这个问题。
- en: 'TargetNet is a small but useful class that allows us to synchronize two NNs
    of the same architecture. This class supports two modes of such synchronization:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: TargetNet 是一个小巧但有用的类，允许我们同步两个相同架构的神经网络。此类支持两种同步模式：
- en: 'sync(): Weights from the source network are copied into the target network.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sync()：源网络的权重被复制到目标网络中。
- en: 'alpha_sync(): The source network’s weights are blended into the target network
    with some alpha weight (between 0 and 1).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: alpha_sync()：源网络的权重通过一个 alpha 权重（介于 0 和 1 之间）融合到目标网络中。
- en: 'The first mode is the standard way to perform a target network sync in discrete
    action space problems, like Atari and CartPole, as we did in Chapter [6](#). The
    latter mode is used in continuous control problems, which will be described in
    Part 4 of the book. In such problems, the transition between two networks’ parameters
    should be smooth, so alpha blending is used, given by the formula w[i] = w[i]α
    + s[i](1 −α), where w[i] is the target network’s i-th parameter and s[i] is the
    source network’s weight. The following is a small example of how TargetNet should
    be used in code. Let’s assume we have the following network:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模式是执行离散动作空间问题（如 Atari 和 CartPole）中的目标网络同步的标准方法，正如我们在第[6章](#)中所做的那样。后者模式用于连续控制问题，这将在本书第
    4 部分中描述。在此类问题中，两个网络参数之间的过渡应是平滑的，因此使用了 alpha 混合，公式为 w[i] = w[i]α + s[i](1 −α)，其中
    w[i] 是目标网络的第 i 个参数，s[i] 是源网络的权重。以下是如何在代码中使用 TargetNet 的一个小示例。假设我们有以下网络：
- en: '[PRE26]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The target network could be created as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络可以通过以下方式创建：
- en: '[PRE27]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The target network contains two fields: model, which is the reference to the
    original network, and target_model, which is a deep copy of it. If we examine
    both networks’ weights, they will be the same:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 目标网络包含两个字段：model，它是对原始网络的引用；target_model，它是原始网络的深拷贝。如果我们检查这两个网络的权重，它们将是相同的：
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'They are independent of each other, however, just having the same architecture:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 它们相互独立，然而，仅仅有相同的架构：
- en: '[PRE29]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To synchronize them again, the sync() method can be used:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 要再次同步它们，可以使用 sync() 方法：
- en: '[PRE30]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'For the blended sync, you can use the alpha_sync() method:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于混合同步，你可以使用 alpha_sync() 方法：
- en: '[PRE31]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Ignite helpers
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ignite 辅助工具
- en: 'PyTorch Ignite was briefly discussed in Chapter [3](ch007.xhtml#x1-530003),
    and it will be used in the rest of the book to reduce the amount of training loop
    code. PTAN provides several small helpers to simplify integration with Ignite,
    which reside in the ptan.ignite package:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Ignite 在第[3章](ch007.xhtml#x1-530003)中简要讨论过，之后在本书的其余部分将用于减少训练循环代码的量。PTAN
    提供了几个小的辅助工具，以简化与 Ignite 的集成，这些工具位于 ptan.ignite 包中：
- en: 'EndOfEpisodeHandler: Attached to ignite.Engine, it emits an EPISODE_COMPLETED
    event and tracks the reward and number of steps in the event in the engine’s metrics.
    It also can emit an event when the average reward for the last episodes reaches
    the predefined boundary, which is supposed to be used to stop the training when
    some goal reward has been reached.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EndOfEpisodeHandler：附加到 ignite.Engine，触发 EPISODE_COMPLETED 事件，并在事件中跟踪奖励和步骤数，记录到引擎的指标中。它还可以在最后几集的平均奖励达到预定义边界时触发事件，预定用于在达到某个目标奖励时停止训练。
- en: 'EpisodeFPSHandler: Tracks the number of interactions between the agent and
    environment that are performed and calculates performance metrics as frames per
    second. It also tracks the number of seconds passed since the start of the training.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EpisodeFPSHandler：跟踪代理与环境之间执行的交互次数，并计算每秒帧数的性能指标。它还跟踪从训练开始到现在经过的秒数。
- en: 'PeriodicEvents: Emits corresponding events every 10, 100, or 1,000 training
    iterations. It is useful for reducing the amount of data being written into TensorBoard.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PeriodicEvents：每 10、100 或 1,000 次训练迭代时触发相应事件。它有助于减少写入 TensorBoard 的数据量。
- en: A detailed illustration of how these classes can be used will be given in the
    next chapter, when we will use them to reimplement the DQN training from Chapter [6](#),
    and then check several DQN extensions and tweaks to improve basic DQN convergence.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中将详细说明如何使用这些类，当时我们将用它们重新实现第[6章](#)中的 DQN 训练，然后检查几个 DQN 扩展和调整，以提高基础 DQN 的收敛性。
- en: The PTAN CartPole solver
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PTAN CartPole 解算器
- en: 'Let’s now take the PTAN classes (without Ignite so far) and try to combine
    everything to solve our first environment: CartPole. The complete code is in Chapter07/06_cartpole.py.
    I will show only the important parts of the code related to the material that
    we have just covered.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看 PTAN 类（目前没有 Ignite），并尝试将所有内容结合起来解决我们的第一个环境：CartPole。完整的代码位于 Chapter07/06_cartpole.py。这里只展示与我们刚刚讨论的材料相关的重要部分代码。
- en: 'First, we create the NN (the simple two-layer feed-forward NN that we used
    for CartPole before) and target the NN epsilon-greedy action selector and DQNAgent.
    Then, the experience source and replay buffer are created:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建神经网络（之前用于 CartPole 的简单两层前馈神经网络）并将其目标设为 NN epsilon-greedy 动作选择器和 DQNAgent。接着，创建经验源和回放缓冲区：
- en: '[PRE32]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: With these few lines, we have finished with our data pipeline.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这几行代码，我们已经完成了数据管道的创建。
- en: 'Now, we just need to call populate() on the buffer and sample training batches
    from it:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要在缓冲区上调用`populate()`并从中采样训练批次：
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: At the beginning of every training loop iteration, we ask the buffer to fetch
    one sample from the experience source and then check for the finished episode.
    The pop_rewards_steps() method in the ExperienceSource class returns the list
    of tuples with information about episodes completed since the last call to the
    method.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次训练循环的开始，我们要求缓冲区从经验源中获取一个样本，然后检查是否有已完成的episode。`pop_rewards_steps()`方法在`ExperienceSource`类中返回一个元组列表，其中包含自上次调用该方法以来完成的episodes信息。
- en: 'Later in the training loop, we convert a batch of ExperienceFirstLast objects
    into tensors suitable for DQN training:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练循环后期，我们将一批`ExperienceFirstLast`对象转换为适合DQN训练的张量：
- en: '[PRE34]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We calculate the loss and do a backpropagation step. Finally, we decay epsilon
    in our action selector (with the hyperparameters used, epsilon decays to zero
    at training step 500) and ask the target network to sync every 10 training iterations.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算损失并进行一次反向传播步骤。最后，我们在我们的动作选择器中衰减epsilon（使用的超参数使得epsilon在第500步训练时衰减为零），并要求目标网络每10次训练迭代同步一次。
- en: 'The unpack_batch method is the last piece of our implementation:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`unpack_batch`方法是我们实现的最后一部分：'
- en: '[PRE35]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It takes a sampled batch of ExperienceFirstLast objects and converts them into
    three tensors: states, actions, and target Q-values. The code should converge
    in 2,000-3,000 training iterations:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 它接受一批采样的`ExperienceFirstLast`对象，并将它们转换为三个张量：状态、动作和目标Q值。代码应该在2000到3000次训练迭代中收敛：
- en: '[PRE36]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Other RL libraries
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他的RL库
- en: 'As we discussed earlier, there are several RL-specific libraries available.
    A few years ago, TensorFlow was more popular than PyTorch, but nowadays, PyTorch
    is dominating the field, and there is a recent trend of JAX being used as it provides
    better performance. The following is my recommended list of libraries you might
    want to take into consideration for your projects:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，市面上有几种专门用于RL的库。几年前，TensorFlow比PyTorch更流行，但如今，PyTorch在该领域占据主导地位，并且最近JAX的使用趋势正在上升，因为它提供了更好的性能。以下是我推荐的一些你可能想要考虑在项目中使用的库：
- en: 'stable-baselines3: We mentioned this library when we discussed Atari wrappers.
    This is a fork of the OpenAI Stable Baselines repository, and the main idea is
    to have an optimized and reproducible set of RL algorithms that you can use to
    check your methods ( [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: stable-baselines3：我们在讨论Atari包装器时提到了这个库。它是OpenAI Stable Baselines库的一个分支，主要目的是提供一个经过优化且可复现的RL算法集，你可以用它来验证你的方法（[https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)）。
- en: 'TorchRL: RL extensions for PyTorch. This library is relatively young-—the first
    release was at the end of 2022—but provides rich set of helper classes for RL.
    Its design philosophy is very close to PTAN—a Python-first set of flexible classes
    that you can combine and extend to build your system—so I highly recommend that
    you learn this library. In the rest of the book, we’ll use this library’s classes.
    Most likely, examples in the next edition of this book (unless we reach ”AI Singularity”
    and books become obsolete, like clay tablets) will not be based on PTAN but on
    TorchRL, which is better maintained. Documentation: [https://pytorch.org/rl/](https://pytorch.org/rl/),
    source code: [https://github.com/pytorch/rl](https://github.com/pytorch/rl).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TorchRL：PyTorch的RL扩展。这个库相对较新——它的第一个版本发布于2022年底——但提供了丰富的RL帮助类。它的设计理念与PTAN非常接近——一个以Python为主的灵活类集合，你可以将它们组合和扩展来构建你的系统——所以我强烈推荐你学习这个库。在本书的剩余部分，我们将使用这个库的类。很可能，本书下一版的示例（除非我们迎来了“人工智能奇点”，书籍变得像粘土板一样过时）将不再基于PTAN，而是基于TorchRL，它维护得更好。文档：[https://pytorch.org/rl/](https://pytorch.org/rl/)，源代码：[https://github.com/pytorch/rl](https://github.com/pytorch/rl)。
- en: 'Spinning Up: Another repo from OpenAI, but with a different goal in mind: providing
    valuable and clean education materials about state-of-the-art methods. This repo
    hasn’t been updated for several years (the last commit was in 2020), but still
    provides very valuable materials about the methods. Documentation: [https://spinningup.openai.com/](https://spinningup.openai.com/).
    Code: [https://github.com/openai/spinningup](https://github.com/openai/spinningup).'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spinning Up：这是OpenAI的另一个库，但目标不同：提供关于最先进方法的有价值且简洁的教育材料。这个库已经有几年没有更新了（最后的提交是在2020年），但仍然提供了关于这些方法的非常有价值的材料。文档：[https://spinningup.openai.com/](https://spinningup.openai.com/)。代码：[https://github.com/openai/spinningup](https://github.com/openai/spinningup)。
- en: 'Keras-RL: Started by Matthias Plappert in 2016, this includes basic deep RL
    methods. As suggested by the name, this library was implemented using Keras, which
    is a high-level wrapper around TensorFlow ([https://github.com/keras-rl/keras-rl](https://github.com/keras-rl/keras-rl)).
    Unfortunately, the last commit was in 2019, so the project has been abandoned.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras-RL：由Matthias Plappert于2016年启动，包含基本的深度强化学习方法。正如名称所示，该库是使用Keras实现的，Keras是一个高层次的TensorFlow封装器（[https://github.com/keras-rl/keras-rl](https://github.com/keras-rl/keras-rl)）。不幸的是，最后一次提交是在2019年，因此该项目已被废弃。
- en: 'Dopamine: A library from Google published in 2018\. It is TensorFlow-specific,
    which is not surprising for a library from Google ([https://github.com/google/dopamine](https://github.com/google/dopamine)).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dopamine：谷歌于2018年发布的库。它是TensorFlow特定的，这对于谷歌发布的库来说并不令人惊讶（[https://github.com/google/dopamine](https://github.com/google/dopamine)）。
- en: 'Ray: A library for distributed execution of machine learning code. It includes
    RL utilities as part of the library ( [https://github.com/ray-project/ray](https://github.com/ray-project/ray)).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ray：一个用于分布式执行机器学习代码的库。它包含作为库一部分的强化学习工具（[https://github.com/ray-project/ray](https://github.com/ray-project/ray)）。
- en: 'TF-Agents: Another library from Google published in 2018 ( [https://github.com/tensorflow/agents](https://github.com/tensorflow/agents)).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-Agents：谷歌于2018年发布的另一个库（[https://github.com/tensorflow/agents](https://github.com/tensorflow/agents)）。
- en: 'ReAgent: A library from Facebook Research. It uses PyTorch internally and uses
    a declarative style of configuration (when you are creating a JSON file to describe
    your problem), which limits extensibility. But, of course, as it is open source,
    you can always extend the functionality ([https://github.com/facebookresearch/ReAgent](https://github.com/facebookresearch/ReAgent)).
    Recently, ReAgent was archived and replaced by the Pearl library from the same
    team: [https://github.com/facebookresearch/Pearl/](https://github.com/facebookresearch/Pearl/).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReAgent：来自Facebook Research的库。它内部使用PyTorch，并采用声明式配置风格（当你创建JSON文件来描述问题时），这限制了可扩展性。但当然，由于它是开源的，你总是可以扩展功能（[https://github.com/facebookresearch/ReAgent](https://github.com/facebookresearch/ReAgent)）。最近，ReAgent已经被归档，并由同一团队的Pearl库所替代：[https://github.com/facebookresearch/Pearl/](https://github.com/facebookresearch/Pearl/)。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we talked about higher-level RL libraries, their motivation,
    and their requirements. Then, we took a deep look at the PTAN library, which will
    be used in the rest of the book to simplify example code. This focus on the details
    of the methods rather than implementation will be extremely useful for you in
    later chapters of this book, as you progress further with RL.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讨论了更高层次的强化学习库、它们的动机和要求。接着，我们深入了解了PTAN库，它将在本书的其余部分中用于简化示例代码。专注于方法的细节而非实现，这对于你在本书后续章节学习强化学习时会非常有帮助。
- en: In the next chapter, we will return to DQN methods by exploring extensions that
    researchers and practitioners have discovered since the classic DQN introduction
    to improve the stability and performance of the method.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将通过探索研究人员和实践者自经典DQN方法引入以来，为了提高方法的稳定性和性能所发现的扩展，重新回到DQN方法。
