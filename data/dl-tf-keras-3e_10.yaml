- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Self-Supervised Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自监督学习
- en: Imagine that you are in the middle of the ocean, and you are thirsty. There
    is water all around you, but you cannot drink any of it. But what if you had the
    resources to boil the salt out of the water and thereby make it drinkable? Of
    course, the energy costs associated with the process can be quite high, so you
    will likely use the process in moderation. However, if your energy costs effectively
    became free, for example, if you were harnessing the power of the sun, the process
    might be more attractive for you to do on a larger scale.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你处于海洋中央，渴得无法忍受。周围是水，但你不能喝其中任何一滴。但是，如果你有能力将水中的盐分煮掉，使它变得可以饮用呢？当然，这个过程的能源成本可能相当高，因此你可能会适度使用这一过程。然而，如果你的能源成本变得几乎为零，例如，你利用太阳能，这个过程可能会变得更具吸引力，甚至可以在更大规模上进行。
- en: In our somewhat simplistic situation described above, the first scenario is
    roughly analogous to supervised learning, and the second to the class of unsupervised
    / semi-supervised learning techniques we will cover in this chapter. The biggest
    problem with supervised learning techniques is the time and expense associated
    with the collection of labeled training data. As a result, labeled datasets are
    often relatively small.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们上面描述的这个较为简化的情况中，第一个场景大致类似于监督学习，而第二个场景则与我们将在本章中讲解的无监督/半监督学习技术类别相似。监督学习技术的最大问题是与收集标签训练数据相关的时间和费用。因此，带标签的数据集通常相对较小。
- en: Deep learning trades off computation against manual feature engineering, and
    while this can be very effective, deep learning models typically need more data
    to train than traditional (non-deep learning) models. Deep learning models tend
    to be more complex and have more learnable parameters, which results in them performing
    better at various tasks. However, more complex models also require more data to
    train. Because the creation of training data is expensive, this effectively limits
    us from scaling up Deep learning models using supervised learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在计算与手动特征工程之间进行权衡，虽然这种方法非常有效，但深度学习模型通常需要比传统（非深度学习）模型更多的数据来进行训练。深度学习模型通常更加复杂，具有更多可学习的参数，这使得它们在各种任务中表现得更好。然而，更复杂的模型也需要更多的数据来训练。由于训练数据的创建成本较高，这实际上限制了我们使用监督学习来扩展深度学习模型。
- en: Unfortunately, completely unsupervised learning techniques that do not need
    labeled data have had limited success so far. Self-supervised techniques that
    leverage the structure of data in the wild to create labeled data to feed supervised
    learning models offer a middle ground. In this chapter, we will learn about various
    self-supervised techniques and some of their applications in the areas of natural
    language processing, computer vision, and audio signal processing.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，完全不需要标签数据的无监督学习技术至今尚未取得太大成功。自监督技术通过利用野外数据的结构来创建标签数据，以供监督学习模型使用，从而提供了一种折衷方案。在本章中，我们将学习各种自监督技术及其在自然语言处理、计算机视觉和音频信号处理等领域的应用。
- en: 'The chapter covers the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Previous work
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前的工作
- en: Self-supervised learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自监督学习
- en: Self-prediction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自预测
- en: Contrastive learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对比学习
- en: Pretext tasks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前置任务
- en: All the code files for this chapter can be found at https://packt.link/dltfchp10
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在 https://packt.link/dltfchp10 找到
- en: Self-supervised learning is the process of imaginatively reusing labels that
    already exist implicitly in your data. In this chapter, we will learn about some
    common strategies for self-supervised learning and examples of their use to solve
    real-life problems. Let’s begin.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习是通过富有创意地重新利用数据中已经隐含存在的标签的过程。在本章中，我们将学习一些常见的自监督学习策略，并举例说明它们在解决现实问题中的应用。让我们开始吧。
- en: Previous work
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 之前的工作
- en: Self-supervised learning is not a new concept. However, the term became popular
    with the advent of transformer-based models such as BERT and GPT-2, which were
    trained in a semi-supervised manner on large quantities of unlabeled text. In
    the past, self-supervised learning was often labeled as unsupervised learning.
    However, there were many earlier models that attempted to leverage regularities
    in the input data to produce results comparable to that using supervised learning.
    You have encountered some of them in previous chapters already, but we will briefly
    cover them again in this section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习并不是一个新概念。然而，随着基于变换器的模型（如BERT和GPT-2）的出现，这一术语变得广为人知，这些模型通过半监督的方式在大量未标记的文本上进行训练。过去，自监督学习通常被归类为无监督学习。然而，早期有许多模型尝试利用输入数据中的规律性来生成与使用监督学习相当的结果。你在前面的章节中已经接触过其中的一些，但我们将在本节中简要介绍它们。
- en: The **Restricted Boltzmann Machine** (**RBM**) is a generative neural model
    that can learn a probability distribution over its inputs. It was invented in
    1986 and subsequently improved in the mid-2000s. It can be trained in either supervised
    or unsupervised mode and can be applied to many downstream tasks, such as dimensionality
    reduction, classification, etc.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**限制玻尔兹曼机**（**RBM**）是一个生成神经网络模型，可以学习输入的概率分布。它于1986年发明，并在2000年代中期得到了改进。它可以在监督或无监督模式下进行训练，并可以应用于许多下游任务，如降维、分类等。'
- en: '**Autoencoders** (**AEs**) are unsupervised learning models that attempt to
    learn an efficient latent representation of input data by learning to reconstruct
    its input. The latent representation can be used to encode the input for downstream
    tasks. There are several variants of the model. Sparse, denoising, and contrastive
    AEs are effective in learning representations for downstream classification tasks,
    whereas variational AEs are more useful as generative models.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码器**（**AEs**）是无监督学习模型，旨在通过学习重建输入数据，来学习其有效的潜在表示。潜在表示可用于对输入进行编码，以便用于下游任务。该模型有多种变体。稀疏、自噪声和对比AEs在学习下游分类任务的表示方面非常有效，而变分AEs作为生成模型更为有用。'
- en: The Word2Vec model is another great example of what we would now call self-supervised
    learning. The CBOW and skip-gram models used to build the latent representation
    of words in a corpus, attempt to learn mappings of neighbors to words and words
    to neighbors respectively. However, the latent representation can now be used
    as word embeddings for a variety of downstream tasks. Similarly, the GloVe model
    is also a self-supervised model, which uses word co-occurrences and matrix factorization
    to generate word embeddings useful for downstream tasks.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec模型是另一个很好的例子，我们现在称之为自监督学习。CBOW和skip-gram模型用于构建词语的潜在表示，分别尝试学习邻居到词语和词语到邻居的映射。然而，潜在表示现在可以作为词嵌入，应用于各种下游任务。同样，GloVe模型也是一个自监督模型，利用词汇共现和矩阵分解来生成用于下游任务的词嵌入。
- en: '**Autoregressive** (**AR**) models predict future behavior based on past behavior.
    We cover them in this chapter in the *Self-prediction* section. However, AR models
    have their roots in time series analysis in statistics, hidden Markov models in
    pre-neural natural language processing, **Recurrent Neural Networks** (**RNNs**)
    in neural (but pre-transformer) NLP.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**自回归**（**AR**）模型根据过去的行为预测未来的行为。我们在本章的*自我预测*部分进行了讨论。然而，AR模型源自统计学中的时间序列分析，来自神经（但在变换器之前的）自然语言处理中的隐马尔可夫模型，以及**递归神经网络**（**RNNs**）。'
- en: '**Contrastive Learning** (**CL**) models try to learn representations whereby
    similar pairs of items cluster together and dissimilar pairs are pushed far apart.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**对比学习**（**CL**）模型试图学习表示，使得相似的项目对聚集在一起，而不相似的项目对被推得很远。'
- en: CL models are also covered in this chapter in the *Contrastive learning* section.
    However, **Self Organizing Maps** (**SOMs**) and Siamese networks use very similar
    ideas and may have been a precursor of current CL models.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的*对比学习*部分也介绍了CL模型。然而，**自组织映射**（**SOMs**）和孪生网络使用非常相似的思想，可能是当前CL模型的前身。
- en: Self-supervised learning
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自监督学习
- en: In self-supervised learning, the network is trained using supervised learning,
    but the labels are obtained in an automated manner by leveraging some property
    of the data and without human labeling effort. Usually, this automation is achieved
    by leveraging how parts of the data sample interact with each other and learning
    to predict that. In other words, the data itself provides the supervision for
    the learning process.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在自监督学习中，网络通过监督学习进行训练，但标签是通过利用数据的某些属性以自动化方式获得的，而无需人工标注。通常，这种自动化是通过利用数据样本的不同部分如何相互作用并学习预测这种关系来实现的。换句话说，数据本身为学习过程提供了监督。
- en: One class of techniques involves leveraging co-occurrences within parts of the
    same data sample or co-occurrences between the same data sample at different points
    in time. These techniques are discussed in more detail in the *Self-prediction*
    section.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一类技术涉及利用同一数据样本内部的共现或在不同时间点的同一数据样本之间的共现。这些技术将在*自预测*部分中更详细地讨论。
- en: Another class of techniques involves leveraging co-occurring modality for a
    given data sample, for example, between a piece of text and its associated audio
    stream, or an image and its caption. Examples of this technique are discussed
    in the sections on joint learning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类技术涉及利用给定数据样本中的共现模态，例如文本与其相关的音频流，或图像与其标题之间的关系。此技术的示例将在联合学习部分讨论。
- en: Yet another class of self-supervised learning techniques involves exploiting
    relationships between pairs of data samples. These pairs are selected from the
    dataset based on some domain-level heuristic. Examples of these techniques are
    covered in the *Contrastive learning* section.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类自监督学习技术涉及利用数据样本对之间的关系。这些对是根据某些领域级启发式从数据集中选择的。这些技术的示例在*对比学习*部分中有详细介绍。
- en: These techniques can either be used to train a model to learn to solve a business
    task (such as sentiment analysis, classification, etc.) directly or to learn a
    latent (embedding) representation of the data that can then be used to generate
    features to learn to solve a downstream business task. The latter class of tasks
    that are used to indirectly learn the latent representation of the data are called
    pretext tasks. The *Pretext tasks* section will cover this subject, with examples,
    in more detail.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术可以直接用来训练模型解决业务任务（例如情感分析、分类等），也可以用来学习数据的潜在（嵌入）表示，然后用于生成特征，学习解决下游业务任务。后一类任务是用来间接学习数据的潜在表示的，称为前置任务。*前置任务*部分将更详细地讨论这一主题，并提供示例。
- en: The advantages of self-supervised learning are twofold. First, as noted already,
    supervised learning involves the manual labeling of data, which is very expensive
    to create, and therefore it is difficult to get high-quality labeled data. Second,
    self-supervised tasks may not address a business task directly but can be used
    to learn a good representation of the data, which can then be applied to transfer
    this information to actual business tasks downstream.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 自监督学习的优势有两个方面。首先，正如前面所提到的，监督学习涉及数据的人工标注，这种标注过程成本非常高，因此很难获得高质量的标注数据。其次，自监督任务可能不会直接解决业务任务，但可以用来学习数据的良好表示，然后可以将此信息转移到实际的下游业务任务中。
- en: Self-prediction
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自预测
- en: The idea behind self-prediction is to predict one part of a data sample given
    another part. For the purposes of prediction, we pretend that the part to be predicted
    is hidden or missing and learn to predict it. Obviously, both parts are known,
    and the part to be predicted serves as the data label. The model is trained in
    a supervised manner, using the non-hidden part as the input and the hidden part
    as the label, learning to predict the hidden part accurately. Essentially, it
    is to pretend that there is a part of the input that you don’t know and predict
    that.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自预测的思路是给定数据样本的一部分，预测另一部分。为了进行预测，我们假装要预测的部分是隐藏的或缺失的，并学习如何预测它。显然，两个部分都是已知的，而要预测的部分作为数据标签。模型以监督的方式进行训练，使用非隐藏部分作为输入，隐藏部分作为标签，学习准确地预测隐藏部分。本质上，就是假装输入的某个部分是未知的，并进行预测。
- en: The idea can also be extended to reversing the pipeline, for example, deliberately
    adding noise to an image and using the original image as the label and the corrupted
    image as the input.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思路也可以扩展到反向管道，例如故意向图像中添加噪声，并使用原始图像作为标签，将损坏的图像作为输入。
- en: Autoregressive generation
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自回归生成
- en: '**Autoregressive** (**AR**) models attempt to predict a future event, behavior,
    or property based on past events, behavior, or properties. Any data that comes
    with some innate sequential order can be modeled using AR generation. Unlike latent
    variable models such as VAEs or GANs, AR models make no assumptions of independence.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**自回归**（**AR**）模型试图根据过去的事件、行为或特性来预测未来的事件、行为或特性。任何具有固有顺序的数据都可以使用AR生成进行建模。与变分自编码器（VAE）或生成对抗网络（GAN）等潜在变量模型不同，AR模型不假设独立性。'
- en: PixelRNN
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PixelRNN
- en: 'The PixelRNN [1] AR model uses two-dimensional **Recurrent Neural Networks**
    (**RNNs**) to model images on a large scale. The idea is to learn to generate
    a pixel by conditioning on all pixels to the left and above it. A convolution
    operation is used to compute all the states along each dimension at once. The
    LSTM layers used in PixelRNN are one of two types – the Row LSTM and the Diagonal
    BiLSTM. In the row LSTM, the convolution is applied along each row, and in the
    Diagonal BiLSTM, the convolutions are applied along the diagonals of the image:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: PixelRNN [1] AR模型使用二维**循环神经网络**（**RNNs**）对图像进行大规模建模。其思想是通过依赖左边和上方所有像素来学习生成一个像素。通过卷积操作可以一次性计算每个维度上的所有状态。PixelRNN中使用的LSTM层有两种类型——行LSTM和对角双向LSTM。在行LSTM中，卷积操作沿每一行应用；而在对角双向LSTM中，卷积操作则沿图像的对角线应用：
- en: '| ![](img/B18331_10_01.png) | ![](img/B18331_10_001.png) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/B18331_10_01.png) | ![](img/B18331_10_001.png) |'
- en: 'Figure 10.1: PixelRNN tries to predict a pixel by conditioning on all pixels
    to the left and above it. From the paper Pixel Recurrent Neural Networks [1]'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：PixelRNN通过依赖左侧和上方的所有像素来预测一个像素。来自论文《像素递归神经网络》[1]
- en: Image GPT (IPT)
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图像GPT（IPT）
- en: 'Image GPT (IPT) [14] is similar to PixelRNN except it works on patches, and
    each patch is treated as a word. The Image GPT is based on the Transformer model
    and is trained on images from the ImageNet dataset. The images are corrupted in
    multiple different ways (super-resolution, bicubic interpolation, adding noise,
    etc.) and pretrained to predict the original image. The core of the IPT model
    consisted of a transformer encoder decoder pair but had multiple heads and tails
    to extract features from the corrupted input image and format the decoder output
    into the output image respectively. The multiple heads and tails were specialized
    for each of the different tasks IPT is trained to do (denoising, de-raining, x2
    and x4 super-resolution, etc.):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图像GPT（IPT）[14]与PixelRNN类似，只是它在图像块上工作，并且每个图像块被视为一个词。图像GPT基于变换器模型，使用ImageNet数据集上的图像进行训练。这些图像通过多种方式（超分辨率、双三次插值、添加噪声等）被损坏，然后进行预训练以预测原始图像。IPT模型的核心包括变换器的编码器-解码器对，但具有多个头部和尾部，分别用于从损坏的输入图像中提取特征，并将解码器输出格式化为输出图像。这些多个头部和尾部专门用于IPT训练的不同任务（去噪、去雨、x2和x4超分辨率等）：
- en: '![](img/B18331_10_02.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_02.png)'
- en: 'Figure 10.2: Architecture of the Image GPT (IPT) AR model. From the paper Pre-trained
    Image Processing Transformer [14]'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：图像GPT（IPT）AR模型的架构。来自论文《预训练图像处理变换器》[14]
- en: GPT-3
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-3
- en: The GPT-3, or Generative Pre-trained Transformer [9] model from OpenAI is an
    AR language model that can generate human-like text. It generates sequences of
    words, code, and other data, starting from a human-provided prompt. The first
    version of GPT used 110 million learning parameters, GPT-2 used 1.5 billion, and
    GPT-3 used 175 billion parameters. The model is trained on unlabeled text such
    as Wikipedia that is readily available on the internet, initially in English but
    later in other languages as well. The GPT-3 model has a wide variety of use cases,
    including summarization, translation, grammar correction, question answering,
    chatbots, and email composition.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3（或称生成预训练变换器）[9]是OpenAI推出的一种自回归（AR）语言模型，能够生成类似人类的文本。它根据人类提供的提示生成单词、代码和其他数据的序列。GPT的第一个版本使用了1.1亿个学习参数，GPT-2使用了15亿个，GPT-3使用了1750亿个参数。该模型使用了互联网上现成的无标签文本进行训练，如维基百科，最初是英文的，后来也包括了其他语言。GPT-3模型有广泛的应用场景，包括摘要生成、翻译、语法修正、问答、聊天机器人和电子邮件写作等。
- en: The popularity of GPT-3 has given rise to a new profession called prompt engineering
    [39], which is basically to create the most effective prompts to start GPT-3 on
    various tasks. A partial list of possible applications for GPT-3 can be found
    on the OpenAI GPT-3 examples page ([https://beta.openai.com/examples/](https://beta.openai.com/examples/)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-3 的流行催生了一种新职业，称为提示工程 [39]，其基本任务是为 GPT-3 创建最有效的提示，以便启动它执行各种任务。GPT-3 的一些可能应用可以在
    OpenAI GPT-3 示例页面找到（[https://beta.openai.com/examples/](https://beta.openai.com/examples/)）。
- en: XLNet
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XLNet
- en: XLNet [38] is similar to GPT-3 in that it is a generalized AR model. However,
    it leverages both AR language modeling and **AutoEncoding** while avoiding their
    limitations. Instead of using only tokens from the left or right context to predict
    the next token, it uses all possible permutations of the tokens from the left
    and right contexts, thus using tokens from both the left and right contexts for
    prediction. Secondly, unlike AE approaches such as BERT, it does not depend on
    input corruption (as in masked language modeling) since it is a generalized AR
    language model. Empirically, under comparable experimental settings, XLNet consistently
    outperforms BERT on a wide spectrum of tasks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: XLNet [38] 类似于 GPT-3，都是一种广义的自回归模型。然而，它同时利用了自回归语言建模和**自编码**，并避免了它们的局限性。它不是仅使用来自左侧或右侧上下文的标记来预测下一个标记，而是使用左侧和右侧上下文中所有可能的标记排列，从而使用来自左右两侧上下文的标记进行预测。其次，不同于像
    BERT 这样的自编码方法，它不依赖于输入的损坏（如掩码语言建模中那样），因为它是一个广义的自回归语言模型。经验上，在可比较的实验设置下，XLNet 在广泛的任务上始终优于
    BERT。
- en: WaveNet
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WaveNet
- en: WaveNet [3] is an AR generative model based on PixelCNN’s architecture but operates
    on the raw audio waveform. As with PixelCNN, an audio sample at a particular point
    in time is conditioned on the samples at all previous timesteps. The conditional
    probability distribution is modeled as a stack of convolutional layers. The main
    ingredient of the WaveNet is causal convolutions. The predictions emitted by the
    model at a time step cannot depend on any future timesteps. When applied to text
    to speech, WaveNet yields state-of-the-art performance, with human listeners rating
    it as significantly more natural sounding for English and Mandarin than comparable
    text-to-speech models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: WaveNet [3] 是一种基于 PixelCNN 架构的自回归生成模型，但它在原始音频波形上进行操作。与 PixelCNN 一样，WaveNet 在特定时间点的音频样本依赖于所有前一个时间步的样本。条件概率分布被建模为一堆卷积层。WaveNet
    的核心成分是因果卷积。模型在某一时间步产生的预测不能依赖于任何未来的时间步。应用于语音合成时，WaveNet 展现出最先进的性能，人工听众评定其在英语和普通话语音合成方面，比其他类似的文本到语音模型自然得多。
- en: WaveRNN
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WaveRNN
- en: WaveRNN [28] is an AR generative model that learns the joint probability of
    the data by factorizing the distribution into a product of conditional probabilities
    over each sample. The convolutional layers of the WaveNet architecture are replaced
    with a single-layer RNN. It also uses more efficient sampling techniques that,
    overall, reduce the number of operations to perform and result in approximately
    4x speedup over WaveNet.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: WaveRNN [28] 是一种自回归生成模型，通过将数据的分布分解为每个样本的条件概率的乘积来学习数据的联合概率。WaveNet 架构中的卷积层被单层
    RNN 替代。它还使用了更高效的采样技术，整体上减少了需要执行的操作次数，并使得 WaveRNN 比 WaveNet 提速约 4 倍。
- en: Masked generation
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 掩码生成
- en: Masked generation models mask some random portion of themselves and pretend
    it is missing, and the models learn to predict the masked information using the
    unmasked information available to them. Unlike autoregressive models, in the case
    of masked generation models, there is no need for the masked information to be
    located before or after the unmasked information; it can be anywhere in the input.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码生成模型会随机遮蔽部分自身，并假装这些部分是缺失的，模型通过利用未遮蔽的信息来预测这些遮蔽的信息。与自回归模型不同，掩码生成模型中，遮蔽的信息不需要位于未遮蔽信息的前后，它可以出现在输入的任何位置。
- en: BERT
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT
- en: '**BERT** [16], or **Bidirectional Encoder Representation from Transformers**,
    is a transformer-based language model that was trained using text from the internet
    by a team from Google. It uses two objectives during the pretraining phase – **Masked
    Language Modeling** (**MLM**) and **Next Sentence Prediction** (**NSP**). During
    training, 15% of the input tokens are masked and the model learns to predict the
    masked token. Since the model is transformer based, it can use context from anywhere
    in the sentence to help with predicting the masked tokens. BERT models, once pretrained,
    can be fine-tuned with smaller supervised datasets for a variety of downstream
    tasks such as classification, sentiment analysis, textual entailment, etc. BERT
    is covered in more depth in *Chapter 6*, *Transformers*.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT** [16]，即**双向编码器表示模型**，是一种基于Transformer的语言模型，由谷歌的团队使用互联网文本进行训练。在预训练阶段，BERT使用两个目标——**掩码语言建模**（**MLM**）和**下一个句子预测**（**NSP**）。在训练过程中，15%的输入标记会被掩码，模型需要学习预测被掩码的标记。由于BERT是基于Transformer的，它可以使用句子中任何位置的上下文信息来帮助预测被掩码的标记。BERT模型在预训练完成后，可以通过较小的有监督数据集进行微调，用于各种下游任务，如分类、情感分析、文本蕴含等。**BERT**将在*第6章*，*Transformer*中详细介绍。'
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Somewhat predictably, the output of this code block is `"Paris"`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有些时候可以预测，这段代码块的输出是`"Paris"`。
- en: Stacked denoising autoencoder
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆叠去噪自编码器
- en: A stacked denoising autoencoder (AE) [29] adds random noise to images and uses
    them as input to a denoising AE to predict the original image. Multiple layers
    of denoising AEs are each individually trained and stacked. This results in the
    composition of several levels of non-linearity and is key to achieving better
    generalization performance on difficult image recognition tasks. Higher-level
    representations learned in this purely unsupervised manner can be used as image
    features to boost the performance of downstream SVM based image classifiers. Each
    layer functions like a regular AE, i.e., it takes an image as input and tries
    to reconstruct it after it passes through a “bottleneck” layer. The bottleneck
    layer learns a compact feature representation of the input image. Unfortunately,
    AEs usually end up only learning how to compress the image without learning a
    semantically meaningful representation. Denoising AEs address this issue by corrupting
    the input and requiring the network to undo the corruption and hence learn a better
    semantic representation of the input image.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠去噪自编码器（AE）[29]向图像中添加随机噪声，并使用它们作为去噪自编码器的输入，来预测原始图像。多个去噪自编码器层分别进行训练并堆叠。这样形成了多个非线性组合，并且是实现困难图像识别任务更好泛化性能的关键。通过这种纯无监督方式学习的更高层次的表示，可以作为图像特征，提升下游基于SVM的图像分类器的性能。每一层的功能类似于普通的自编码器，即它以图像为输入，并在经过一个“瓶颈”层后尝试重建图像。瓶颈层学习输入图像的紧凑特征表示。不幸的是，自编码器通常只会学习如何压缩图像，而不会学习语义上有意义的表示。去噪自编码器通过破坏输入并要求网络恢复破坏，从而学会更好的语义表示输入图像。
- en: Context autoencoder
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文自编码器
- en: The context autoencoder [12] masks out a region of the image and uses it to
    train a convolutional neural network (the context AE) to regress the missing pixel
    values to predict the original image. The task of a context AE is even harder
    than that of a denoising AE since it has to fill in larger missing areas and cannot
    use information from immediately neighboring pixels. This requires a much deeper
    semantic understanding of the image, and the ability to generate high-level features
    over large spatial areas. In a sense, the context AE is a more powerful generative
    model since it needs to fill in the missing region while maintaining coherence
    with the supplied context.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文自编码器[12]将图像的一个区域进行遮掩，并使用它来训练卷积神经网络（上下文自编码器，Context AE），以回归缺失的像素值，进而预测原始图像。上下文自编码器的任务比去噪自编码器的任务更难，因为它需要填补更大范围的缺失区域，而且不能使用来自相邻像素的信息。这需要对图像有更深层次的语义理解，并且能够在大范围的空间区域上生成高级特征。从某种意义上说，上下文自编码器是一种更强大的生成模型，因为它不仅需要填补缺失的区域，还需要与已提供的上下文保持一致。
- en: 'For that reason, the context AE is trained to reconstruct a combination of
    reconstruction loss and adversarial loss. This results in sharper predictions
    than training on reconstruction (L2) loss alone:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，上下文自编码器的训练目标是重建损失和对抗损失的结合。这比仅仅训练重建（L2）损失的模型产生更锐利的预测：
- en: '![](img/B18331_10_03.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_03.png)'
- en: 'Figure 10.3: Qualitative illustration of the context encoder task (from Context
    Encoders: Feature Learning by Inpainting [10])'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：上下文编码器任务的定性说明（来自《上下文编码器：通过修复学习特征》[10]）
- en: Context does not have to be image features, it could also be color, as we will
    see in the next section.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文不一定是图像特征，也可以是颜色，正如我们将在下一节看到的那样。
- en: Colorization
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 色彩化
- en: The paper *Colorization as a Proxy Task for Visual Understanding* [12] uses
    colorization as a way to learn image representations. Color images are converted
    to their grayscale equivalent, which is then used as input to predict the original
    color image. The model can be used to automatically colorize grayscale images,
    as well as learn a representation that can help in downstream tasks such as image
    classification and segmentation. In functional terms, the model predicts the *a*
    and *b* (color information) channels in their *Lab* encoding given their *L* (grayscale)
    channel. Experiments on the ImageNet dataset by the authors of this paper have
    resulted in models that produce state-of-the-art results against datasets for
    semantic segmentation and image classification for models that don’t use ImageNet
    labels, and even surpass some earlier models that have been trained on ImageNet
    using supervised learning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 论文*《色彩化作为视觉理解的代理任务》* [12] 使用色彩化作为学习图像表示的一种方式。彩色图像被转换为其灰度等效图像，然后作为输入来预测原始的彩色图像。该模型可以自动为灰度图像上色，同时学习一种表示方式，能够帮助进行下游任务，如图像分类和分割。从功能上讲，该模型根据*L*（灰度）通道预测其*Lab*编码中的*a*和*b*（颜色信息）通道。本文作者在ImageNet数据集上进行的实验结果表明，该模型在没有使用ImageNet标签的情况下，能够在语义分割和图像分类数据集上产生最先进的结果，甚至超过一些早期的、在ImageNet上使用监督学习训练的模型。
- en: Innate relationship prediction
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 固有关系预测
- en: Models using this technique attempt to learn visual common-sense tasks by leveraging
    innate relationships between parts of an input image. Weights from these learned
    models could be used to generate semantic representations of images for other
    downstream tasks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该技术的模型尝试通过利用输入图像各部分之间的固有关系来学习视觉常识任务。这些学习到的模型的权重可以用来为其他下游任务生成图像的语义表示。
- en: Relative position
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相对位置
- en: 'The paper *Unsupervised Visual Representation Learning by Context Prediction*
    [8] predicts the relative position of one patch in an image with respect to another.
    Effectively, this approach uses spatial context as a source of self-supervision
    for training visual representations. Given a large unlabeled image collection,
    random pairs of patches are extracted from each image as shown in *Figure 10.4*.
    Each pair is labeled depending on the orientation of the second patch with respect
    to the central one. A convolutional network is trained to predict the position
    of the second patch relative to the first. The feature representation learned
    is found to capture the notion of visual similarity across images. Using this
    representation, it has been shown to aid in visual data mining, i.e., discovering
    image fragments that depict the same semantic object, against the Pascal VOC 2007
    dataset:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 论文*《通过上下文预测进行无监督视觉表示学习》* [8] 预测图像中一个区域相对于另一个区域的相对位置。实际上，这种方法使用空间上下文作为自监督的来源来训练视觉表示。给定一个大型未标记的图像集合，从每张图像中提取出随机的区域对，如*图10.4*所示。每对区域根据第二个区域相对于中心区域的方向进行标注。训练一个卷积神经网络来预测第二个区域相对于第一个区域的位置。学习到的特征表示能够捕捉到图像间的视觉相似性概念。利用这种表示，已证明它有助于视觉数据挖掘，即发现描绘相同语义对象的图像片段，在Pascal
    VOC 2007数据集上效果尤为显著：
- en: '![](img/B18331_10_04.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_04.png)'
- en: 'Figure 10.4: Illustration of relative position prediction. The model must predict
    the configuration of the second patch relative to the (central) first patch. From
    the paper Unsupervised Visual Representation Learning by Context Prediction [8]'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：相对位置预测的说明。模型必须预测第二个区域相对于（中心）第一个区域的配置。来自论文《通过上下文预测进行无监督视觉表示学习》[8]
- en: Solving jigsaw puzzles
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解谜拼图
- en: 'The paper *Unsupervised Learning of Visual Representations by Solving Jigsaw
    Puzzles* [26] describes an approach somewhat similar to the previous approach
    of predicting relative position. This method attempts to learn the visual representation
    of images by solving jigsaw puzzles of natural images. Patches are extracted from
    the input image and shuffled to form a jigsaw puzzle. The network learns to reconstruct
    the original image from the jigsaw puzzle, i.e., to solve the jigsaw puzzle. The
    network used is a **Context Free Network** (**CFN**), an n-way Siamese network.
    Each patch corresponds to a column in the n-way CFN. The shared layers in each
    column are implemented exactly as in AlexNet. The classification head predicts
    the original index of the patch (before shuffling). On the Pascal VOC dataset,
    it outperforms all previous self-supervised models in image classification and
    object detection tasks:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 论文《通过解拼图实现无监督视觉表征学习》[26]描述了一种与之前的相对位置预测方法相似的方法。该方法尝试通过解决自然图像的拼图来学习图像的视觉表征。首先从输入图像中提取小块并打乱，形成拼图。网络学习从拼图中重建原始图像，即解开拼图。所用的网络是**上下文自由网络**（**CFN**），一种
    n 路并行的孪生网络。每个小块对应于 n 路 CFN 中的一列。每列中的共享层与 AlexNet 中的实现完全相同。分类头预测小块的原始索引（即打乱前的位置）。在
    Pascal VOC 数据集上，它在图像分类和物体检测任务中超越了所有先前的自监督模型：
- en: '![](img/B18331_10_05.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_05.png)'
- en: 'Figure 10.5: The image is split up into patches and shuffled, and the model
    learns to put the shuffled patches back in the correct order. From the paper Unsupervised
    Learning of Visual Representations [26]'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：图像被分割成多个小块并打乱，模型学习将这些打乱的小块重新排列回正确的顺序。摘自论文《无监督视觉表征学习》[26]
- en: Rotation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 旋转
- en: The RotNet model [34] learns an image representation by using rotation as a
    self-supervision signal. Input images are rotated by 0, 90, 180, and 270 degrees,
    and a convolutional network (RotNet) is trained to learn to predict the rotation
    angle as one of 4 target classes. It turns out that this apparently simple task
    provides a very powerful supervisory signal for semantic feature learning. RotNet
    features were used as input for image classification against the CIFAR-10 dataset
    and resulted in classification accuracy of only 1.6% less than the state-of-the-art
    result obtained using supervised learning. It also obtained state-of-the-art results
    at the time for some classification tasks against ImageNet, and some classification
    and object detection tasks against Pascal VOC.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: RotNet 模型 [34] 通过使用旋转作为自监督信号来学习图像表征。输入图像分别旋转 0、90、180 和 270 度，并训练一个卷积网络（RotNet）来预测旋转角度，作为
    4 个目标类之一。事实证明，这个看似简单的任务为语义特征学习提供了一个非常强大的监督信号。RotNet 特征被用作图像分类的输入，针对 CIFAR-10 数据集，分类精度仅比使用监督学习获得的最先进结果低
    1.6%。在当时，它还在一些分类任务中取得了 ImageNet 的最先进结果，并且在一些分类和物体检测任务中也取得了 Pascal VOC 的优秀表现。
- en: Hybrid self-prediction
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 混合自预测
- en: With hybrid self-prediction models, self-prediction is achieved using not one
    but multiple self-prediction strategies. For example, our first two examples,
    Jukebox and DALL-E, achieve self-prediction by first reducing the input data to
    a more manageable format using one self-supervision technique (VQ-VAE or Vector
    Quantized Variational AutoEncoder [35]) and then use another (AR) on the reduced
    image to produce the final prediction. In our third example, the predictions from
    the VQ-VAE component are further refined using a discriminator trained in an adversarial
    manner.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用混合自预测模型，利用多个自预测策略实现自预测，而不是只使用一种策略。例如，我们的前两个示例，Jukebox 和 DALL-E，通过首先使用一种自监督技术（VQ-VAE
    或向量量化变分自编码器 [35]）将输入数据简化为更易处理的格式，然后使用另一种（AR）在简化后的图像上生成最终预测，从而实现自预测。在我们的第三个示例中，VQ-VAE
    组件的预测通过使用对抗训练的判别器进一步精细化。
- en: VQ-VAE
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VQ-VAE
- en: Since the VQ-VAE is common to all our hybrid self-prediction models, let us
    try to understand what it does at a high level. You have already read about autoencoders
    and variational autoencoders in *Chapter 8*, *Autoencoders*. Autoencoders try
    to learn to reconstruct their input by first encoding the input onto a smaller
    dimension and then decoding the output of the smaller dimension. However, autoencoders
    typically just end up compressing the input and do not learn a good semantic representation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 VQ-VAE 是我们所有混合自预测模型的共同组成部分，我们可以尝试从高层次理解它的工作原理。你已经在 *第8章*《自编码器》中阅读过自编码器和变分自编码器。自编码器试图通过首先将输入编码为更小的维度，然后解码该较小维度的输出，来学习重构其输入。然而，自编码器通常最终只是压缩输入，而未能学习到好的语义表示。
- en: '**Variational Autoencoders** (**VAEs**) can do better in this respect by enforcing
    a probabilistic prior, generally in the form of a standard Gaussian distribution,
    and by minimizing not only the reconstruction loss but also the KL divergence
    between the prior distribution and posterior distribution (the actual distribution
    in the latent space).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**变分自编码器**（**VAE**）在这方面可以做得更好，因为它通过强制使用概率先验，通常采用标准高斯分布的形式，并通过最小化重构损失以及先验分布与后验分布（潜在空间中的实际分布）之间的
    KL 散度来进行优化。'
- en: While the VAE learns a continuous latent distribution, the VQ-VAE learns a discrete
    latent distribution. This is useful because transformers are designed to take
    discrete data as input. VQ-VAE extends VAE by adding a discrete codebook component
    to the network, which is used to quantize the latent vectors output by the encoder
    by choosing the vector in the codebook that is closest to each latent vector by
    Euclidean distance. The VQ-VAE decoder is then tasked with reconstructing the
    input from the discretized latent vector.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 VAE 学习的是连续的潜在分布，但 VQ-VAE 学习的是离散的潜在分布。这是有用的，因为 transformers 被设计为接受离散数据作为输入。VQ-VAE
    通过向网络中添加一个离散代码本组件来扩展 VAE，该组件用于通过选择与每个潜在向量在欧几里得距离上最接近的代码本向量来量化编码器输出的潜在向量。VQ-VAE
    解码器的任务是从离散化的潜在向量重构输入。
- en: Jukebox
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jukebox
- en: Our first example is the Jukebox paper [32], which is a generative model for
    music, similar to how GPT-3 is a generative model for text and Image-GPT is a
    generative model for images. That is, given a musical (voice and music) prompt,
    Jukebox can create the music that might follow this prompt. Early attempts at
    generative models for audio attempted symbolic music generation in the form of
    a piano roll, since the problem with generating raw audio directly is the extremely
    large amount of information it contains and consequently, the extreme long-range
    dependencies that need to be modeled. The VQ-VAE addresses this problem by learning
    a lower-dimensional encoding of the audio with the goal of losing the least important
    information but retaining most of the useful information.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个示例是 Jukebox 论文[32]，这是一个音乐生成模型，类似于 GPT-3 是一个文本生成模型，Image-GPT 是一个图像生成模型。也就是说，给定一个音乐（声音和音乐）提示，Jukebox
    可以生成可能跟随此提示的音乐。早期的音频生成模型尝试通过钢琴卷轴的形式生成符号化的音乐，因为直接生成原始音频的问题在于其包含的信息量极大，因此需要建模极长的依赖关系。VQ-VAE
    通过学习音频的低维编码来解决这个问题，目的是尽量减少丢失不重要的信息，同时保留大部分有用信息。
- en: Jukebox uses hierarchical VQ-VAEs to discretize the input signal into different
    temporal resolutions, then generates a new sequence at each resolution, and finally
    combines the generated sequence at each level into the final prediction.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Jukebox 使用层次化的 VQ-VAE 将输入信号离散化为不同的时间分辨率，然后在每个分辨率下生成一个新的序列，最后将每个层级生成的序列组合成最终的预测。
- en: DALL-E
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DALL-E
- en: Our second example of hybrid prediction models is the DALL-E model [5] from
    OpenAI. DALL-E can also be classified as a joint learning (multimodal) model,
    since it attempts to learn to create images from text captions, using pairs of
    text and image as training input. However, we classify it here as a hybrid prediction
    model because, like Jukebox, it attempts to address the high dimensionality of
    image information (compared with the dimensionality of the associated text) using
    a VQ-VAE.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个混合预测模型示例是 OpenAI 的 DALL-E 模型[5]。DALL-E 也可以归类为联合学习（多模态）模型，因为它试图从文本描述中学习生成图像，使用文本和图像的配对作为训练输入。然而，我们在这里将其归类为混合预测模型，因为与
    Jukebox 一样，它试图通过使用 VQ-VAE 来解决图像信息的高维度问题（与相关文本的维度相比）。
- en: DALL-E receives text and images as a single stream of data. DALL-E uses a two-stage
    training regime. In the first stage, a VQ-VAE is trained to compress each input
    RGB image of size (256, 256, 3) into a grid of image tokens of size (32, 32),
    each element of which can assume one of 8,192 possible discrete values. This reduces
    the size of the image input by a factor of 192 without a corresponding loss in
    image quality.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: DALL-E 将文本和图像作为单一数据流接收。DALL-E 使用了两阶段的训练方案。在第一阶段，训练了一个 VQ-VAE 将每个输入的大小为 (256,
    256, 3) 的 RGB 图像压缩成大小为 (32, 32) 的图像令牌网格，其中的每个元素可以取 8,192 个可能的离散值之一。这使得图像输入的大小减小了
    192 倍，而图像质量没有相应的损失。
- en: In the second stage, the text is BPE-encoded and truncated to 256 tokens. **Byte
    Pair Encoding** (**BPE**) is a hybrid character/word encoding that can represent
    large corpora using a relatively small vocabulary by encoding common byte pairs.
    This encoding is then concatenated with the flattened sequence of 1,024 (32 x
    32) image tokens. This combined sequence is used to train an autoregressive transformer
    to model the joint distribution over the text and image tokens. The first stage
    learns the visual codebook in the VQ-VAE and the second stage learns the prior
    of the discrete latent distribution over the text and image tokens. The trained
    DALL-E model can then be used to generate images given a text prompt.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，文本被 BPE 编码并截断为 256 个令牌。**字节对编码**（**BPE**）是一种混合字符/词编码，可以使用相对较小的词汇表来表示大语料库，通过编码常见的字节对。然后，此编码与扁平化的
    1,024 个（32 x 32）图像令牌序列串联。使用这个联合序列来训练自回归变换器，以建模文本和图像令牌的联合分布。第一阶段学习了 VQ-VAE 中的视觉码书，第二阶段学习了文本和图像令牌离散潜在分布的先验。训练好的
    DALL-E 模型可以根据文本提示生成图像。
- en: Text-to-image generation is getting quite popular. A newer version of DALL-E,
    called DALL-E 2, was recently released by OpenAI. It has 35 billion parameters
    compared to DALL-E’s 12 billion. Even though they are named similarly, DALL-E
    is a version of GPT-3 trained to generate images from text descriptions, and DALL-E
    2 is an encoder-decoder pipeline that uses CLIP to encode the text description
    into a CLIP embedding, and then decode the embedding back to an image using a
    diffusion model that you learned about in *Chapter 9*, *Generative Models*. As
    expected, DALL-E 2 generates more realistic and accurate images than DALL-E.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 文本生成图像正变得相当流行。最近，OpenAI 发布了名为 DALL-E 2 的更新版本，它拥有 350 亿个参数，而原版 DALL-E 只有 120
    亿个参数。尽管它们的命名相似，DALL-E 是 GPT-3 的一个版本，专门用于根据文本描述生成图像，而 DALL-E 2 则是一个编码器-解码器管道，使用
    CLIP 将文本描述编码为 CLIP 嵌入，然后使用你在《第9章》《生成模型》中学到的扩散模型将嵌入解码为图像。不出所料，DALL-E 2 生成的图像比 DALL-E
    更真实、更准确。
- en: Even more recently, Google Research has released Imagen, another model in this
    space that competes with DALL-E 2\. Like DALL-E 2, Imagen uses a T5-XXL encoder
    to map input text into embeddings and a diffusion model to decode the embedding
    into an image.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 更近期，Google Research 发布了 Imagen，这是这一领域的另一个与 DALL-E 2 竞争的模型。与 DALL-E 2 类似，Imagen
    使用了 T5-XXL 编码器将输入文本映射到嵌入，并使用扩散模型将嵌入解码为图像。
- en: VQ-GAN
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VQ-GAN
- en: The VQ-GAN [30] uses an encoder-decoder framework where the encoder uses a VQ-VAE
    style encoder that learns a discrete latent representation, but the decoder is
    a discriminator component of a **Generative Adversarial Network** (**GAN**). Instead
    of the L2 loss used in the VQ-VAE, the VQ-GAN uses a combination of perceptual
    loss and discriminator loss, which helps in keeping good perceptual quality at
    increased compression rates. The use of a GAN architecture rather than a traditional
    VAE decoder helps with training efficiency.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: VQ-GAN [30] 使用了一个编码器-解码器框架，其中编码器使用了 VQ-VAE 风格的编码器，学习了一个离散潜在表示，但解码器是一个 **生成对抗网络**（**GAN**）的鉴别器组件。VQ-GAN
    不使用 VQ-VAE 中使用的 L2 损失，而是使用感知损失和鉴别器损失的组合，这有助于在增加压缩率时保持良好的感知质量。与传统的 VAE 解码器相比，使用
    GAN 结构有助于训练效率。
- en: 'Like VQ-VAE, the VQ-GAN learns a codebook of context-rich visual components,
    which are used to compose sequences for training the autoregressive component.
    The VQ-GAN has been found to outperform the VQ-VAE-2 model on images from ImageNet
    using the **Fréchet Inception Distance** (**FID**), which measures the distance
    between feature vectors of real versus fake images) metric, even though it uses
    approximately 10x fewer parameters:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 VQ-VAE，VQ-GAN 学习了一个上下文丰富的视觉组件的码书，这些组件用于组合序列以训练自回归组件。使用 **Fréchet Inception
    Distance**（**FID**）指标发现，VQ-GAN 在使用 ImageNet 图像上优于 VQ-VAE-2 模型，尽管它使用的参数大约少了10倍：
- en: '![](img/B18331_10_06.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_06.png)'
- en: 'Figure 10.6: Architecture of the VQ-GAN. From the paper: Taming Transformers
    for High Resolution Image Synthesis [30]'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：VQ-GAN架构。来自论文：《驯服变换器以进行高分辨率图像合成》[30]
- en: Next, we will look at another popular self-supervised technique called contrastive
    learning.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将了解另一种流行的自监督技术——对比学习。
- en: Contrastive learning
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对比学习
- en: '**Contrastive Learning** (**CL**) tries to predict the relationship between
    a pair of input samples. The goal of CL is to learn an embedding space where pairs
    of similar samples are pulled close together and dissimilar samples are pushed
    far apart. Inputs to train CL models are in the form of *pairs of data points*.
    CL can be used in both supervised and unsupervised settings.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**对比学习**（**CL**）试图预测一对输入样本之间的关系。CL的目标是学习一个嵌入空间，使得相似的样本对被拉近，而不相似的样本对被推远。训练CL模型的输入是以*数据点对*的形式出现的。CL可以在有监督和无监督的设置中使用。'
- en: When used in an unsupervised setting, it can be a very powerful self-supervised
    learning approach. Similar pairs are found from existing data in a self-supervised
    manner, and dissimilar pairs are found from pairs of similar pairs of data. The
    model learns to predict if a pair of data points are similar or different.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督设置中使用时，它可以成为一种非常强大的自监督学习方法。通过自监督的方式从现有数据中找到相似对，从相似对的数据对中找到不相似的对。模型学习预测一对数据点是否相似或不同。
- en: A taxonomy of CL can be derived by considering the techniques used to generate
    contrastive examples. Before we do that, we will take a brief detour to explore
    the various training objectives that are popular in CL.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过考虑用于生成对比样本的技术来推导出CL的分类法。在我们这么做之前，我们将简单地探索一下在CL中流行的各种训练目标。
- en: Training objectives
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练目标
- en: Early CL models used data points consisting of a single positive and a single
    negative example to learn from. However, the trend in more recent CL models is
    to learn from multiple positive and negative samples in a single batch. In this
    section, we will cover some training objectives (also called loss functions) that
    are commonly used for training CL models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的CL模型使用由一个正样本和一个负样本组成的数据点来学习。然而，最近的CL模型趋势是从单个批次中的多个正负样本中学习。在本节中，我们将介绍一些常用于训练CL模型的训练目标（也叫损失函数）。
- en: Contrastive loss
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对比损失
- en: 'Contrastive loss [35] is one of the earliest training objectives to be used
    for learning using CL techniques. It tries to encode data into an embedding space
    such that examples from the same class have similar embeddings and examples from
    different classes have dissimilar embeddings. Thus, given two data pairs, (*x*[i]*,
    y*[i]) and (*x*[j]*, y*[j]), the contrastive loss objective is described using
    the following formula:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对比损失[35]是使用CL技术进行学习的最早训练目标之一。它试图将数据编码到嵌入空间中，使得同一类别的样本具有相似的嵌入，而不同类别的样本具有不相似的嵌入。因此，给定两个数据对（*x*[i]*,
    y*[i]）和（*x*[j]*, y*[j]），对比损失目标可以通过以下公式来描述：
- en: '![](img/B18331_10_002.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_002.png)'
- en: The first term is activated when the pairs *i* and *j* are similar, and the
    second term is activated when the pair is dissimilar. The objective is designed
    to maximize the square of the differences in the first term and minimize the square
    of differences in the second term (thus maximizing the second term in the case
    of dissimilar pairs). The ![](img/B18331_10_003.png) is a hyperparameter and represents
    a margin of the minimum allowable distance between samples of different classes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 当对*数据对*i和*j*相似时，第一个项被激活，当数据对不相似时，第二个项被激活。目标是最大化第一个项中的差异的平方，并最小化第二项中的差异的平方（从而在不相似的样本对情况下最大化第二项）。！[](img/B18331_10_003.png)是一个超参数，表示不同类别样本之间允许的最小距离的边界。
- en: Triplet loss
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 三元组损失
- en: 'Triplet loss [11] is an enhancement of contrastive loss in that it uses three
    data points instead of two – the anchor point, the positive point, and the negative
    point. Thus, given an anchor point *x*, we select a positive sample ![](img/B18331_10_004.png)
    and one negative sample ![](img/B18331_10_005.png), where *x* and ![](img/B18331_10_004.png)
    belong to the same class and *x* and ![](img/B18331_10_005.png) belong to different
    classes. Triplet loss learns to minimize the distance between the anchor *x* and
    positive sample ![](img/B18331_10_004.png) and maximize the distance between *x*
    and negative sample ![](img/B18331_10_005.png). This is illustrated in *Figure
    10.7*:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 三元组损失 [11] 是对比损失的增强版，它使用三个数据点而不是两个——锚点、正样本和负样本。因此，给定一个锚点 *x*，我们选择一个正样本 ![](img/B18331_10_004.png)
    和一个负样本 ![](img/B18331_10_005.png)，其中 *x* 和 ![](img/B18331_10_004.png) 属于同一类，而
    *x* 和 ![](img/B18331_10_005.png) 属于不同类。三元组损失学习最小化锚点 *x* 与正样本 ![](img/B18331_10_004.png)
    之间的距离，并最大化 *x* 与负样本 ![](img/B18331_10_005.png) 之间的距离。这个过程在 *图 10.7* 中有示意图：
- en: '![Diagram  Description automatically generated](img/B18331_10_07.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![示意图 说明自动生成](img/B18331_10_07.png)'
- en: 'Figure 10.7: Illustration of triplet loss. Based on the paper: FaceNet: A Unified
    Embedding for Face Recognition and Clustering [11]'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.7：三元组损失的示意图。基于论文：FaceNet: A Unified Embedding for Face Recognition and
    Clustering [11]'
- en: 'The equation for triplet loss is shown below. As with contrastive loss, the
    ![](img/B18331_10_003.png) is a hyperparameter representing the minimum allowed
    difference between distances between similar and dissimilar pairs. Triplet loss-based
    models typically need challenging values for ![](img/B18331_10_005.png), the so-called
    hard negatives, to provide good representations:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 三元组损失的公式如下所示。与对比损失一样，![](img/B18331_10_003.png) 是一个超参数，表示相似对与不相似对之间的最小允许差异。基于三元组损失的模型通常需要具有挑战性的值，如
    ![](img/B18331_10_005.png)，即所谓的硬负样本，以提供良好的表示：
- en: '![](img/B18331_10_012.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_012.png)'
- en: N-pair loss
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: N-pair 损失
- en: 'N-pair loss [21] generalizes triplet loss to incorporate comparison with multiple
    negative samples instead of just one. Thus, given an *(N+1)* tuple of training
    samples, {*x*, *x*^+, *x*[1]^-, *x*[2]^-, …, *x*[N+1]^-}, where there is one positive
    sample and *N-1* negative ones, the N-pair loss is defined using the following
    equation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: N-pair 损失 [21] 是三元组损失的泛化，它将与多个负样本的比较引入，而不仅仅是与一个负样本进行比较。因此，给定一个 *(N+1)* 元组的训练样本，{*x*,
    *x*^+, *x*[1]^-, *x*[2]^-, …, *x*[N+1]^-}，其中有一个正样本和 *N-1* 个负样本，N-pair 损失使用以下公式定义：
- en: '![](img/B18331_10_013.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_013.png)'
- en: Lifted structural loss
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升的结构损失
- en: 'Lifted structured loss [15] is another generalization of triplet loss where
    it uses all pairwise edges within a training batch. This leads to better training
    performance. *Figure 10.8* illustrates the idea behind lifted structural loss,
    and how it evolved from contrastive and triplet loss. Red edges connect similar
    pairs and blue edges connect dissimilar pairs:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 提升的结构损失 [15] 是三元组损失的另一种泛化，它使用训练批次中的所有成对边。这可以带来更好的训练效果。*图 10.8* 说明了提升结构损失的原理，以及它如何从对比损失和三元组损失演变而来。红色边连接相似的对，蓝色边连接不相似的对：
- en: '![Chart, diagram, schematic  Description automatically generated](img/B18331_10_08.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图表、图示、示意图 说明自动生成](img/B18331_10_08.png)'
- en: 'Figure 10.8: Illustration of the idea of Lifted Structured Loss. Based on the
    paper: Deep Metric Learning via Lifted Structured Feature Embedding [15]'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8：提升结构损失的原理示意图。基于论文：Deep Metric Learning via Lifted Structured Feature
    Embedding [15]
- en: NCE loss
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NCE 损失
- en: '**Noise Contrastive Estimation** (**NCE**) loss [27] uses logistic regression
    to distinguish positive and negative (noise) examples. The NCE loss attempts to
    maximize the log odds (logits) of positive examples *x* and minimize the log odds
    of negative examples ![](img/B18331_10_014.png) . The equation for NCE loss is
    shown below:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**噪声对比估计** (**NCE**) 损失 [27] 使用逻辑回归来区分正例和负例（噪声）。NCE 损失试图最大化正例 *x* 的对数几率（logits），并最小化负例的对数几率
    ![](img/B18331_10_014.png)。NCE 损失的公式如下所示：'
- en: '![](img/B18331_10_015.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_015.png)'
- en: InfoNCE loss
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: InfoNCE 损失
- en: InfoNCE loss [2] was inspired by NCE loss (described in the previous section)
    and uses categorical cross-entropy loss to identify the positive sample from the
    set of unrelated noise samples. Given some context vector *c*, the positive sample
    should be drawn from the conditional probability distribution *p(x|c)*, while
    the *N-1* negative examples can be drawn from the distribution *p(x)* independent
    of the context *c*. The InfoNCE loss optimizes the negative log probability of
    classifying the positive sample correctly.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: InfoNCE损失[2]受到NCE损失（在前一节中描述）的启发，使用分类交叉熵损失从无关噪声样本的集合中识别正样本。给定一些上下文向量*c*，正样本应从条件概率分布*p(x|c)*中抽取，而*N-1*个负样本则可以从与上下文*c*无关的分布*p(x)*中抽取。InfoNCE损失优化的是正确分类正样本的负对数概率。
- en: 'The InfoNCE loss is given by the following equation, where *f(x, c)* estimates
    the density ratio *p(x|c) / p(x)*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: InfoNCE损失通过以下方程给出，其中*f(x, c)*估计密度比*p(x|c) / p(x)*：
- en: '![](img/B18331_10_016.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_016.png)'
- en: Soft nearest neighbors loss
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Soft最近邻损失
- en: 'Soft nearest neighbors loss [33] further extends the idea of contrastive loss
    to include multiple positive samples given known labels. Given a batch of samples,
    ![](img/B18331_10_017.png) where *y*[i] is the class label of *x*[i], and a similarity
    function *f* that measures similarity between two inputs, the soft nearest neighbor
    loss is given by the equation:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Soft最近邻损失[33]进一步扩展了对比损失的思想，包含了已知标签下的多个正样本。给定一批样本，![](img/B18331_10_017.png)，其中*y*[i]是*x*[i]的类别标签，以及一个相似性函数*f*，该函数衡量两个输入之间的相似性，Soft最近邻损失由以下方程给出：
- en: '![](img/B18331_10_018.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_018.png)'
- en: The temperature ![](img/B18331_10_019.png) is a hyperparameter and is used for
    tuning how concentrated the features are in the representation space. Thus, at
    low temperatures, the contribution of faraway points in the representation space
    to the soft nearest neighbors loss is also low.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 温度![](img/B18331_10_019.png)是一个超参数，用于调整表示空间中特征的集中程度。因此，在低温度下，表示空间中远离点对Soft最近邻损失的贡献也较低。
- en: Instance transformation
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实例转换
- en: CL models that use instance transformation generally rely on data augmentation
    techniques to generate positive pairs and negative mining to generate negative
    pairs from pairs of positive pairs. Many such models rely on generating in-batch
    negative and innovative techniques for mining hard negatives.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用实例转换的CL模型通常依赖数据增强技术生成正样本对，并通过负样本挖掘技术从正样本对中生成负样本对。许多这样的模型依赖于生成批次内的负样本和创新的硬负样本挖掘技术。
- en: Data augmentation techniques are used to create pairs of the original data point
    and its noisy version. This introduces non-essential variation into the examples
    without modifying semantic meaning, which the model then learns during training.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强技术用于创建原始数据点及其噪声版本的样本对。这将非本质的变化引入样本中，而不修改语义意义，模型随后在训练过程中学习这些变化。
- en: In-batch negative sampling is a technique for generating negative samples by
    combining information from examples within a single batch. For each positive pair
    (*x*[i]*, y*[i]) in the batch, all pairs (*x*[i]*, y*[j]) and (*x*[j]*, y*[i])
    for all ![](img/B18331_10_020.png) can be considered as negative pairs. In effect,
    negative pairs are created by combining elements from two random positive pairs
    in the same batch. This technique is practical and can be implemented efficiently
    on GPUs and is therefore widely used.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 批内负采样是一种通过结合单个批次内示例的信息来生成负样本的技术。对于批次中的每个正样本对(*x*[i]*, y*[i])，所有对(*x*[i]*, y*[j])和(*x*[j]*,
    y*[i])都可以视为负样本对。实际上，负样本对是通过在同一批次内将两个随机正样本对的元素组合来创建的。该技术实用且能够在GPU上高效实现，因此广泛使用。
- en: Some models require hard negative samples to learn how to perform their tasks
    well. Hard negatives are pairs that have different labels, but whose embedding
    features are very close to each other. You can visualize them as points that lie
    very close to each other in the embedding space but on opposite sides of the decision
    boundary. Identifying hard negatives for a given task is relatively easy for supervised
    learning. For unsupervised learning, one approach is to increase the batch size,
    which will introduce more hard negative samples. Another technique [19] is to
    increase the sampling probability of the candidate negative sample by its similarity
    with the anchor sample.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型需要硬负样本来学习如何执行任务。硬负样本是标签不同但嵌入特征非常接近的样本对。你可以将它们视为在嵌入空间中非常接近但位于决策边界两侧的点。对于有监督学习，识别硬负样本相对简单。对于无监督学习，一种方法是增加批量大小，从而引入更多硬负样本。另一种技术[19]是根据候选负样本与锚点样本的相似性来增加其采样概率。
- en: SimCLR
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SimCLR
- en: The SimCLR model [36] presents a simple framework for contrastive learning of
    visual representations. Each input image (*x*) is augmented in two different ways
    (*x*[i] and *x*[j]) using the same family of image augmentation strategies, resulting
    in *2N* positive samples.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: SimCLR模型[36]提供了一个用于对比学习视觉表征的简单框架。每个输入图像（*x*）通过使用相同的图像增强策略家族，以两种不同的方式（*x*[i]和*x*[j]）进行增强，从而产生*2N*个正样本。
- en: 'In-batch negative sampling is used, so for each positive example, we have *(2N-1)*
    negative samples. A base encoder (*f*) is applied to the pair of data points in
    each example, and a projection head (*g*) attempts to maximize the agreement for
    positive pairs and minimize it for negative pairs. For good performance, SimCLR
    needs to use large batch sizes so as to incorporate enough negative examples in
    the training regime. SimCLR achieved state-of-the-art results for self-supervised
    and semi-supervised models on ImageNet and matches the performance of a supervised
    ResNet-50\. *Figure 10.9* shows the architecture of the SimCLR model:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用批内负样本采样，因此对于每个正样本，我们有*(2N-1)*个负样本。对于每个示例中的数据点对，应用一个基础编码器（*f*），然后一个投影头（*g*）尝试最大化正样本对的相似度，并最小化负样本对的相似度。为了获得良好的性能，SimCLR需要使用大批量大小，以便在训练过程中包含足够的负样本。SimCLR在ImageNet上实现了自监督和半监督模型的最新成果，并与监督学习的ResNet-50表现相匹配。*图10.9*展示了SimCLR模型的架构：
- en: '![Diagram  Description automatically generated](img/B18331_10_09.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图示 说明自动生成](img/B18331_10_09.png)'
- en: 'Figure 10.9: Architecture of the SimCLR model. From the paper: A Simple Framework
    for Contrastive Learning of Visual Representations [36]'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：SimCLR模型的架构。摘自论文：A Simple Framework for Contrastive Learning of Visual
    Representations [36]
- en: Barlow Twins
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Barlow Twins
- en: 'The idea behind the Barlow Twins [20] model has its roots in neuroscience,
    i.e., the goal of sensory processing is to re-code highly redundant sensory inputs
    into a factorial code, or a code with statistically independent components. In
    this model, an image is distorted into two versions of itself. The distorted versions
    are fed into the same network to extract features and learn to make the cross-correlation
    matrix between these two features as close to the identity matrix as possible.
    In line with the neuroscience idea, the goal of this model is to reduce the redundancy
    between the two distorted versions of the sample by reducing the redundancy between
    these vectors. This is reflected in its somewhat unique loss function – in the
    first equation, the first term represents the difference between the identity
    matrix and the cross-correlation matrix, and the second term represents the redundancy
    reduction term. The second equation defines each element of the cross-correlation
    matrix *C*:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Barlow Twins模型[20]的思想源自神经科学，即感知处理的目标是将高度冗余的感官输入重新编码为因子化代码，或具有统计独立成分的代码。在此模型中，图像被扭曲成两个版本。将这两个扭曲版本输入相同的网络以提取特征，并学习使这两个特征之间的交叉相关矩阵尽可能接近单位矩阵。与神经科学的理念一致，该模型的目标是通过减少这些向量之间的冗余，来减少样本的两个扭曲版本之间的冗余。这体现在其独特的损失函数中——在第一个公式中，第一项表示身份矩阵与交叉相关矩阵之间的差异，第二项表示冗余减少项。第二个公式定义了交叉相关矩阵*C*的每个元素：
- en: '![](img/B18331_10_021.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_021.png)'
- en: '![](img/B18331_10_022.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_022.png)'
- en: Some notable differences between the Barlow Twins model and other models in
    this genre are that the Barlow Twins model doesn’t require a large number of negative
    samples and can thus operate on smaller batches, and that it benefits from high-dimensional
    embeddings. The Barlow Twins model outperforms some previous semi-supervised models
    trained on ImageNet and is on par with some supervised ImageNet models.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Barlow Twins 模型与该领域其他模型的一些显著区别在于，Barlow Twins 模型不需要大量负样本，因此可以在较小的批次上运行，并且受益于高维嵌入。Barlow
    Twins 模型在 ImageNet 上超越了一些先前的半监督模型，并与一些监督学习的 ImageNet 模型持平。
- en: BYOL
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BYOL
- en: 'The **Bootstrap Your Own Latent** (**BYOL**) model [17] is unique in that it
    does not use negative samples at all. It relies on two neural networks, the online
    and target networks, that interact and learn from each other. The goal of BYOL
    is to learn a representation ![](img/B18331_10_023.png)that can be used for downstream
    tasks. The online network is parameterized by a set of weights ![](img/B18331_10_024.png)
    and comprises three stages – an encoder ![](img/B18331_10_025.png), a projector
    ![](img/B18331_10_026.png), and a predictor ![](img/B18331_10_027.png) The target
    network has the same architecture as the online network but uses a different set
    of weights ![](img/B18331_10_028.png). The target network provides the regression
    targets to train the online network, and its parameters ![](img/B18331_10_028.png)
    are an exponential moving average of the online parameters ![](img/B18331_10_024.png)
    After every training step, the following update is performed:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bootstrap Your Own Latent**（**BYOL**）模型 [17] 的独特之处在于它完全不使用负样本。它依赖于两个相互作用并彼此学习的神经网络——在线网络和目标网络。BYOL
    的目标是学习一个可以用于下游任务的表示 ![](img/B18331_10_023.png)。在线网络由一组权重 ![](img/B18331_10_024.png)
    参数化，并包括三个阶段——编码器 ![](img/B18331_10_025.png)、投影器 ![](img/B18331_10_026.png) 和预测器
    ![](img/B18331_10_027.png)。目标网络的架构与在线网络相同，但使用一组不同的权重 ![](img/B18331_10_028.png)。目标网络提供回归目标来训练在线网络，其参数
    ![](img/B18331_10_028.png) 是在线网络参数 ![](img/B18331_10_024.png) 的指数移动平均。在每次训练步骤之后，执行以下更新：'
- en: '![](img/B18331_10_031.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_031.png)'
- en: BYOL produces two augmented views of each image. From the first augmented view,
    the online network outputs a representation ![](img/B18331_10_032.png) and a projection
    ![](img/B18331_10_033.png). Similarly, the target network outputs a representation
    ![](img/B18331_10_034.png) and a projection ![](img/B18331_10_035.png) BYOL attempts
    to minimize the error between the L2 normalized online and target projections
    ![](img/B18331_10_033.png) and ![](img/B18331_10_035.png). At the end of the training,
    we only retain the online network (the encoder).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: BYOL 为每个图像生成两个增强视图。对于第一个增强视图，在线网络输出一个表示 ![](img/B18331_10_032.png) 和一个投影 ![](img/B18331_10_033.png)。同样，目标网络输出一个表示
    ![](img/B18331_10_034.png) 和一个投影 ![](img/B18331_10_035.png)。BYOL 尝试最小化 L2 归一化的在线投影和目标投影之间的误差
    ![](img/B18331_10_033.png) 和 ![](img/B18331_10_035.png)。在训练结束时，我们只保留在线网络（编码器）。
- en: BYOL achieves competitive results against semi-supervised or transfer learning
    models on ImageNet. It is also less sensitive to changes in batch size and the
    type of image augmentations used compared to other models in this genre. However,
    later work [4] indicates that the batch normalization component in BYOL may implicitly
    cause a form of contrastive learning by implicitly creating negative samples as
    a result of data redistribution it causes.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: BYOL 在 ImageNet 上与半监督或迁移学习模型相比，取得了竞争性的结果。与该领域的其他模型相比，它对批量大小和图像增强类型的变化不太敏感。然而，后续研究
    [4] 表明，BYOL 中的批归一化组件可能通过隐式创建负样本，作为数据重分布的结果，从而隐式地引发了一种对比学习形式。
- en: Feature clustering
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征聚类
- en: Feature clustering involves finding similar data samples by clustering them.
    This can be useful when data augmentation techniques are not feasible. The idea
    here is to use clustering algorithms to assign pseudo-labels to samples such that
    we can run intra-sample CL. Although similar, feature clustering differs from
    CL in that it relaxes the instance discrimination problem – rather than learn
    to distinguish between a pair of transformations on a single input image, feature
    clustering learns to discriminate between groups of images with similar features.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 特征聚类涉及通过聚类找到相似的数据样本。当数据增强技术不可行时，这非常有用。这里的想法是使用聚类算法为样本分配伪标签，以便我们可以进行样本内对比学习（CL）。尽管相似，特征聚类与对比学习不同，它放宽了实例区分问题——与其学习区分单一输入图像的多个变换，特征聚类则学习区分具有相似特征的图像组。
- en: DeepCluster
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepCluster
- en: The DeepCluster [24] paper is predicated on the fact that datasets for supervised
    learning such as ImageNet are “too small” to account for general-purpose features
    that go beyond image classification. For learning general-purpose features, it
    is necessary to train on billions of images at internet scales. However, labeling
    such large datasets is not feasible, so DeepCluster presents a clustering method
    that jointly learns the parameters of the neural network and the cluster assignments
    of the resulting features. DeepCluster iteratively groups these features using
    the K-Means clustering algorithm and uses the cluster assignments as pseudo labels
    to learn the parameters of the ConvNet. The end product of the training is the
    weights of the ConvNet. These weights have been shown to be useful general-purpose
    visual features and have outperformed the best published numbers on many downstream
    tasks regardless of the dataset.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: DeepCluster [24]论文的前提是，像ImageNet这样的监督学习数据集“过于小”，无法考虑超出图像分类的一般目的特征。为了学习一般目的特征，需要在互联网规模的数十亿图像上进行训练。然而，标注如此大规模的数据集并不可行，因此DeepCluster提出了一种聚类方法，联合学习神经网络的参数和生成特征的聚类分配。DeepCluster通过K-Means聚类算法迭代地对这些特征进行分组，并使用聚类分配作为伪标签来学习ConvNet的参数。训练的最终产物是ConvNet的权重。这些权重已被证明是有用的通用视觉特征，并且在许多下游任务中超越了在不同数据集上发布的最佳结果。
- en: SwAV
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SwAV
- en: In the **SwAV** (**SWapping Assignments between multiple Views**) [25] model,
    features are learned by predicting the cluster assignment (pseudo-label) for a
    view from the representation of another view. SwAV uses a variant of the architecture
    used in CL models. The images *x*[1] and *x*[2] are transformations of the same
    input image *x*, which are sent through an encoder ![](img/B18331_10_025.png)
    to produce a representation *z*[1] and *z*[2]. In the case of SwAV, *z*[1] and
    *z*[2] are used to compute *q*[1] and *q*[2] by matching their features to a set
    of *K* prototype vectors *{c*[1]*, …, c*[K]*}*, which are then used to predict
    the cluster assignment for *x*[2] and *x*[1] respectively.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**SwAV**（**SWapping Assignments between multiple Views**）[25]模型通过预测来自另一个视图的表示来学习特征，从而预测某一视图的聚类分配（伪标签）。SwAV使用了与CL模型中相似的架构变体。图像*x*[1]和*x*[2]是同一输入图像*x*的变换，经过编码器
    ![](img/B18331_10_025.png) 生成表示*z*[1]和*z*[2]。在SwAV的情况下，*z*[1]和*z*[2]用于通过将其特征与一组*K*原型向量*{c*[1]*,
    …, c*[K]*}*进行匹配，从而计算*q*[1]和*q*[2]，这些向量随后用于分别预测*x*[2]和*x*[1]的聚类分配。'
- en: Unlike DeepCluster, SwAV does online clustering (clustering of data that arrives
    continuously in a streaming manner and is not known before the clustering process
    begins) and can therefore scale to potentially unlimited amounts of data. SwAV
    also works well with both large and small batch sizes. The SwAV paper also proposes
    a new multi-crop strategy to increase the number of views of an image with no
    computational or memory overhead. It achieves 75% top-1 accuracy on ImageNet with
    ResNet50 (a supervised learning method) as well as surpassing results of supervised
    pretraining in all the considered transfer tasks.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与DeepCluster不同，SwAV进行在线聚类（即对持续流式到达的数据进行聚类，这些数据在聚类过程开始之前并不为人所知），因此能够扩展到潜在的无限数据量。SwAV还适用于大批量和小批量大小。SwAV论文还提出了一种新的多裁剪策略，可以在没有计算或内存开销的情况下增加图像的视图数量。它在ImageNet上使用ResNet50（监督学习方法）取得了75%的Top-1准确率，并且在所有考虑的迁移任务中超越了监督预训练的结果。
- en: InterCLR
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: InterCLR
- en: InterCLR [18] is a hybrid model that jointly learns a visual representation
    by leveraging intra-image as well as inter-image invariance. It has two invariance
    learning branches in its pipeline, one for intra-image, and the other for inter-image.
    The intra-image branch constructs contrastive pairs by standard CL methods such
    as generating a pair of transformations from an input image. The inter-image branch
    constructs contrastive pairs using pseudo-labels obtained from clustering – two
    items within the same cluster constitute a positive pair, and two items from different
    clusters form a negative pair.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: InterCLR [18]是一个混合模型，通过利用图像内和图像间的不变性来共同学习视觉表示。它在其流程中有两个不变性学习分支，一个用于图像内，一个用于图像间。图像内分支通过标准CL方法构造对比对，例如从输入图像生成一对变换。图像间分支使用通过聚类获得的伪标签构造对比对——同一聚类中的两个项构成正对，来自不同聚类的两项构成负对。
- en: 'A variant of the InfoNCE loss function is used to compute the contrastive loss
    and the network is trained through back-propagation:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 InfoNCE 损失函数的变体来计算对比损失，并通过反向传播训练网络：
- en: '![Diagram  Description automatically generated](img/B18331_10_10.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图示  描述自动生成](img/B18331_10_10.png)'
- en: 'Figure 10.10: Architecture of the InterCLR model. From the paper: Delving into
    Inter-Image Invariance for Unsupervised Visual Representation [18]'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10：InterCLR 模型的架构。摘自论文：深入探讨无监督视觉表示的图像间不变性 [18]
- en: The InterCLR paper also addresses some special considerations around pseudo
    label maintenance, sampling strategy, and decision boundary design for the inter-image
    branch, which we will skip here in the interests of space. The InterCLR model
    shows many improvements over state-of-the-art intra-image invariance learning
    methods on multiple standard benchmarks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: InterCLR 论文还讨论了伪标签维护、采样策略和图像间分支的决策边界设计等一些特别考虑因素，但为了节省篇幅，我们在此略过。InterCLR 模型在多个标准基准测试中，相较于现有的图像内不变性学习方法，展示了许多改进。
- en: Multiview coding
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多视图编码
- en: Multiview coding has become a mainstream CL method in recent years and involves
    constructing positive contrastive examples using two or more views of the same
    object. The objective is to maximize the mutual information between the representations
    of the multiple views of the data for positive examples and minimize it for negative
    examples. This requires the model to learn higher-level features whose influence
    spans multiple views.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 多视图编码近年来已成为主流的 CL 方法，涉及使用同一物体的两个或多个视角构建正对比样本。目标是最大化多个视角的表示之间的互信息，对于正例，负例则最小化这一互信息。这要求模型学习跨越多个视角的高阶特征。
- en: AMDIM
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AMDIM
- en: '**Augmented Multiscale Deep InfoMax** (**AMDIM**) [31] is a model for self-supervised
    representational learning based on an earlier local Deep InfoMax method, which
    attempts to maximize the mutual information between a global summary feature that
    depends on the entire input, and a collection of local features that are extracted
    from intermediate layers in the encoder. AMDIM extends DIM by predicting features
    across independently augmented features of each input and simultaneously across
    multiple scales, as well as using a more powerful encoder.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '**增强多尺度深度信息最大化**（**AMDIM**）[31] 是一个基于早期局部深度信息最大化方法的自监督表征学习模型，旨在最大化一个全局摘要特征与一组从编码器中间层提取的局部特征之间的互信息。全局摘要特征依赖于整个输入，局部特征则来自中间层。AMDIM
    通过预测每个输入的独立增强特征，以及跨多个尺度的特征，扩展了 DIM，并使用了更强大的编码器。'
- en: The paper also considers other ways of producing contrastive pairs, such as
    instance transformation and multimodal (discussed in the next section), but it
    is described here because it also considers constructing contrastive pairs using
    multiview coding. The model beats several benchmarks for self-supervised learning
    objectives.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 论文还考虑了生成对比对的其他方法，如实例变换和多模态（将在下一节讨论），但在此描述是因为它也考虑了使用多视图编码构建对比对。这一模型在多个自监督学习目标的基准测试中超越了若干标准。
- en: CMC
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CMC
- en: The **Contrastive Multiview Coding** (**CMC**) [37] model is based on the idea
    that when an object is represented by multiple views, each of these views is noisy
    and incomplete, but important factors such as the physics, geometry, and semantics
    of the object are usually shared across all the views. The goal of CMC is to learn
    a compact representation of the object that captures these important factors.
    CMC achieves this by using CL to learn a representation such that views of the
    same scene map to nearby points, whereas views of different scenes map to distant
    points.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**对比多视图编码**（**CMC**）[37] 模型基于这样一个理念：当一个物体由多个视角表示时，这些视角每一个都带有噪声且不完整，但物体的物理、几何和语义等重要因素通常在所有视角中是共享的。CMC
    的目标是学习物体的紧凑表示，捕捉这些重要因素。CMC 通过使用 CL 来学习一个表示，使得同一场景的视角映射到相近的点，而不同场景的视角则映射到远离的点。'
- en: Multimodal models
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多模态模型
- en: The class of models covered in this section includes models that use paired
    inputs from two or more modalities of the same data. The input to such a model
    could be an image and a caption, a video and text, an audio clip and its transcript,
    etc. These models learn a joint embedding across multiple modalities. In this
    class of models, we will cover the CLIP [6] and CodeSearchNet [13] models as examples.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖的模型类别包括使用来自两个或更多模态的配对输入的模型。这些模型的输入可以是图像和字幕、视频和文本、音频片段及其转录本等。这些模型学习跨多个模态的联合嵌入。在这一类模型中，我们将以
    CLIP [6] 和 CodeSearchNet [13] 模型为例进行讲解。
- en: Another class of multimodal models is frameworks that can be used to do self-supervised
    learning across multiple modalities. The Data2Vec [7] model is an example of such
    a model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类多模态模型是可以用于跨多个模态进行自监督学习的框架。Data2Vec [7] 模型就是这样一个模型的例子。
- en: CLIP
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CLIP
- en: The CLIP model [6] learns image representations by learning to predict which
    image goes with which caption. It is pretrained with 400 million image-text pairs
    from the internet. After pretraining, the model can use natural language queries
    to refer to learned visual concepts. CLIP can be used in zero-shot mode for downstream
    tasks such as image classification, text-to-image, and image-to-image image search.
    The model is competitive for natural images with a fully supervised baseline without
    the need for any additional fine-tuning. For example, CLIP can match the accuracy
    of the original ResNet50 on ImageNet in zero-shot mode, i.e., without additional
    fine-tuning. CLIP can also be fine-tuned with specialized image datasets for specific
    downstream tasks, such as learning visual representations for satellite imagery
    or tumor detection.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 模型 [6] 通过学习预测哪些图像与哪些字幕匹配来学习图像表示。它在互联网上预训练了 4 亿对图像-文本数据。预训练之后，该模型可以使用自然语言查询来引用已学习的视觉概念。CLIP
    可以在零-shot 模式下用于下游任务，如图像分类、文本到图像生成以及图像到图像的搜索。该模型在自然图像的表现上与完全监督基准具有竞争力，且无需额外的微调。例如，CLIP
    在零-shot 模式下可以与原始 ResNet50 在 ImageNet 上的准确度相匹配，即无需额外微调。CLIP 还可以通过使用特定的图像数据集进行微调，来应对特定的下游任务，例如学习卫星图像的视觉表示或肿瘤检测。
- en: '*Figure 10.11* shows the architecture of the CLIP model for training and inference.
    Both image and text encoders are transformer-based encoders. The objective of
    pretraining is to solve the task of predicting which text as a whole is paired
    with which image. Thus, given a batch of *N* image-text pairs, CLIP learns to
    predict which of the *N* x *N* possible image-text pairs across the batch actually
    occurred. CLIP learns a multi-modal joint embedding space by maximizing the cosine
    similarity of the image and text embeddings of the N real pairs in the batch while
    minimizing the cosine similarity of the rest of the *N*² *- N* incorrect pairs.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.11* 展示了 CLIP 模型的训练与推理架构。图像和文本编码器都是基于 Transformer 的编码器。预训练的目标是解决预测哪些文本与哪些图像整体匹配的任务。因此，给定一批
    *N* 对图像-文本对，CLIP 学习预测在 *N* x *N* 的所有可能图像-文本对中，实际发生的图像-文本对是哪一对。CLIP 通过最大化批次中 N
    对真实图像-文本嵌入的余弦相似度，同时最小化剩余的 *N*² *- N* 错误对的余弦相似度，来学习一个多模态联合嵌入空间。'
- en: 'During inference, the input of one modality can be used to predict the output
    of the other, i.e., given an image, it can predict the image class as text:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理时，一个模态的输入可以用来预测另一个模态的输出，即，给定一个图像，它可以预测该图像的类别为文本：
- en: '![](img/B18331_10_11.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_10_11.png)'
- en: 'Figure 10.11: Architecture of the CLIP model. From the paper: Learning Transferable
    Visual Models from Natural Language Supervision [34x]'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11：CLIP 模型的架构。来自论文：《从自然语言监督中学习可迁移的视觉模型》 [34x]
- en: '[PRE2]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The CLIP model does this by projecting both text and image to a single embedding
    space. Using this common embedding approach, CLIP is also able to compute the
    similarity between two images and a text. It also offers the ability to extract
    encodings of text and images.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 模型通过将文本和图像投影到单一的嵌入空间来实现这一点。通过这种共同嵌入的方法，CLIP 还能够计算两张图像和一段文本之间的相似度。它还提供了提取文本和图像编码的能力。
- en: CodeSearchNet
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CodeSearchNet
- en: '[PRE4]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Data2Vec
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Data2Vec
- en: Data2Vec [7] is a little different in that it proposes a common framework to
    do self-supervised learning across multiple modalities. It uses masked prediction
    to apply the same learning method for either speech, language, or computer vision.
    The core idea is to predict latent representations of the full input based on
    a masked view of the input. Instead of predicting modality-specific targets such
    as words, visual tokens, etc., it predicts contextualized latent representations
    that contain information for the entire input. It uses a teacher-student architecture
    – first, a representation of the full input data is built, which serves as the
    target for the learning task (teacher mode). Then a masked version of the input
    sample is encoded, with which the full data representation is predicted (student
    mode). The teacher’s parameters are updated using exponentially decaying average
    weights of the student. At the end of the training, the teacher’s weights are
    used as the learned embedding.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Data2Vec [7] 有些不同，它提出了一个通用框架，用于跨多种模态进行自监督学习。它使用掩蔽预测，将相同的学习方法应用于语音、语言或计算机视觉。核心思想是基于输入的掩蔽视图预测整个输入的潜在表示。它不是预测特定模态的目标，如单词、视觉标记等，而是预测包含整个输入信息的上下文化潜在表示。它采用教师-学生架构——首先，构建完整输入数据的表示，作为学习任务的目标（教师模式）。然后对输入样本进行掩蔽编码，并预测完整的数据表示（学生模式）。教师的参数通过使用学生的指数衰减平均权重来更新。在训练结束时，教师的权重作为学习到的嵌入被使用。
- en: 'Experiments using this framework against major benchmarks in speech recognition,
    image classification, and natural language understanding show either state-of-the-art
    performance or competitive performance to popular approaches:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用该框架在语音识别、图像分类和自然语言理解等主要基准上的实验显示，无论是达到最先进的性能，还是与流行方法竞争，结果都非常出色：
- en: '![Diagram  Description automatically generated](img/B18331_10_12.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](img/B18331_10_12.png)'
- en: 'Figure 10.12: Architecture of the Data2Vec model. From the paper: data2vec:
    A General Framework for Self-supervised Learning in Speech, Vision and Language
    [7]'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10.12：Data2Vec 模型的架构。来自论文：data2vec: A General Framework for Self-supervised
    Learning in Speech, Vision and Language [7]'
- en: Pretext tasks
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前置任务
- en: Pretext tasks are tasks that self-supervised learning models attempt to solve
    by leveraging some pattern inherent in the unlabeled data they train on. Such
    tasks are not necessarily useful in and of themselves, but they help the system
    learn a useful latent representation, or embeddings, that can then be used, either
    as-is or after fine-tuning, on some other downstream tasks. Training to solve
    pretext tasks usually happens as a precursor to building the actual model, and
    for that reason, it is also referred to as pretraining.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 前置任务是自监督学习模型试图通过利用无标签数据中固有的某些模式来解决的任务。这些任务本身不一定有用，但它们帮助系统学习一个有用的潜在表示，或者嵌入，这些表示可以直接使用，或在微调后用于其他下游任务。训练以解决前置任务通常作为构建实际模型的前奏，因此它也被称为预训练。
- en: Almost all the techniques we have discussed in this chapter have been pretext
    tasks. While some tasks may end up being useful in and of themselves, such as
    colorization or super-resolution, they also result in embeddings that end up learning
    the semantics of the data distribution of the unlabeled data that it was trained
    on, in the form of learned weights. These weights can then be applied to downstream
    tasks.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中讨论的几乎所有技术都是前置任务。虽然一些任务本身可能最终会变得有用，例如色彩化或超分辨率，但它们也会产生嵌入，这些嵌入会学习所训练的无标签数据分布的语义，以学习到的权重的形式存在。这些权重随后可以应用于下游任务。
- en: This is not a new concept – for example, the Word2Vec algorithm, which is widely
    used for finding “synonyms,” is based on an embedding space where words used in
    similar contexts cluster together. It is trained using either the skip-gram or
    CBOW algorithm, which attempt to predict a context word given a word, or vice
    versa. Neither of these objectives are useful in and of themselves, but in the
    process, the network ends up learning a good latent representation of the words
    in the input data. This representation can then be directly used to find “synonyms”
    for words or do word analogies, as well as being used to produce useful vector
    representations of words and sequences of words (such as sentences and documents)
    for downstream tasks, such as text classification or sentiment analysis.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个新概念——例如，广泛用于寻找“同义词”的Word2Vec算法，是基于一个嵌入空间，在该空间中，语境相似的词汇聚在一起。它使用skip-gram或CBOW算法进行训练，这两种算法试图在给定一个词的情况下预测上下文词，反之亦然。这些目标本身没有直接的实用价值，但在这个过程中，网络最终会学习到输入数据中单词的良好潜在表示。这个表示可以直接用于寻找单词的“同义词”或进行单词类比，也可以用来生成有用的单词和单词序列（例如句子和文档）的向量表示，进而用于下游任务，例如文本分类或情感分析。
- en: The biggest advantage of pretext tasks is that the training of models for downstream
    tasks can be done with relatively smaller amounts of labeled data. The model learns
    a lot about the domain (the broad strokes) based on solving the pretext task using
    large quantities of readily available unlabeled data. It requires relatively smaller
    amounts of labeled data to learn to solve more specific downstream tasks based
    on what it already knows about the domain. Because labeled data is hard to come
    by and expensive to create, this two-step approach can often make some machine
    learning models possible, if not more practical.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 前置任务的最大优势在于，使用较少量的标注数据即可训练下游任务的模型。模型通过解决前置任务，利用大量现成的未标注数据，学习到有关领域的知识（大致的概况）。它只需要较少量的标注数据就能根据已经掌握的领域知识，学习解决更具体的下游任务。由于标注数据难以获得且成本较高，这种两步法通常能够使某些机器学习模型成为可能，甚至更具实用性。
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: In this chapter, we saw various self-supervised strategies for leveraging data
    to learn the data distribution in the form of specialized embedding spaces, which
    in turn can be used for solving downstream tasks. We have looked at self-prediction,
    contrastive learning, and pretext tasks as specific approaches for self-supervision.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了多种自监督策略，如何利用数据学习数据分布，并以专门的嵌入空间形式展示，这些嵌入空间又可以用来解决下游任务。我们讨论了自预测、对比学习和前置任务作为自监督的具体方法。
- en: In the next chapter, we will look at reinforcement learning, an approach that
    uses rewards as a feedback mechanism to train models for specific tasks.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论强化学习，这是一种通过奖励作为反馈机制来训练特定任务模型的方法。
- en: References
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Aaron van den Oord, Nal Kalchbrenner, and Koray Kavucuoglu (2016). Pixel Recurrent
    Neural Networks Proceedings MLR Press: [http://proceedings.mlr.press/v48/oord16.pdf](http://proceedings.mlr.press/v48/oord16.pdf)'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Aaron van den Oord, Nal Kalchbrenner 和 Koray Kavucuoglu（2016）。像素递归神经网络 会议论文
    MLR Press: [http://proceedings.mlr.press/v48/oord16.pdf](http://proceedings.mlr.press/v48/oord16.pdf)'
- en: 'Aaron van den Oord, Yazhe Li, and Oriol Vinyals. *Representation Learning with
    Contrastive Predictive Coding*. Arxiv Preprint, arXiv 1807.03748 [cs.LG]: [https://arxiv.org/pdf/1807.03748.pdf](https://arxiv.org/pdf/1807.03748.pdf)'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Aaron van den Oord, Yazhe Li, 和 Oriol Vinyals. *对比预测编码的表示学习*。Arxiv预印本，arXiv
    1807.03748 [cs.LG]: [https://arxiv.org/pdf/1807.03748.pdf](https://arxiv.org/pdf/1807.03748.pdf)'
- en: 'Aaron van den Oord, et al. (2016). *WaveNet: A Generative Model for Raw Audio*.
    Arxiv Preprint, arXiv:1609.03499v2 [cs.SD]: [https://arxiv.org/pdf/1609.03499.pdf](https://arxiv.org/pdf/1609.03499.pdf)'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Aaron van den Oord 等人（2016）。*WaveNet：一种原始音频的生成模型*。Arxiv预印本，arXiv:1609.03499v2
    [cs.SD]: [https://arxiv.org/pdf/1609.03499.pdf](https://arxiv.org/pdf/1609.03499.pdf)'
- en: 'Abe Fetterman and Josh Albrecht. (2020). *Understanding Self-Supervised and
    Contrastive Learning* with “Bootstrap your Own Latent” (BYOL). Blog post: [https://generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/](https://generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/)'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Abe Fetterman 和 Josh Albrecht（2020）。*理解自监督和对比学习*与“自举你的潜在表示”（BYOL）。博客文章：[https://generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/](https://generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/)
- en: 'Aditya Ramesh, et al. *Zero Shot Text to Image generation*. Arxiv Preprint,
    arXiv 2102.12092v2 [cs.CV]: [https://arxiv.org/pdf/2102.12092.pdf](https://arxiv.org/pdf/2102.12092.pdf)'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Aditya Ramesh 等人。*零样本文本到图像生成*。Arxiv 预印本，arXiv 2102.12092v2 [cs.CV]：[https://arxiv.org/pdf/2102.12092.pdf](https://arxiv.org/pdf/2102.12092.pdf)
- en: 'Alec Radford, et al. (2021). *Learning Transferable Visual Models from Natural
    Language Supervision*. Proceedings of Machine Learning Research (PMLR): [http://proceedings.mlr.press/v139/radford21a/radford21a.pdf](http://proceedings.mlr.press/v139/radford21a/radford21a.pdf)'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alec Radford 等人（2021）。*从自然语言监督中学习可迁移的视觉模型*。机器学习研究论文集（PMLR）：[http://proceedings.mlr.press/v139/radford21a/radford21a.pdf](http://proceedings.mlr.press/v139/radford21a/radford21a.pdf)
- en: 'Alexei Baevsky, et al. (2022). *data2vec: A General Framework for Self-Supervised
    Learning in Speech, Vision and Language*. Arxiv Preprint, arXiv 2202.03555v1 [cs.LG]:
    [https://arxiv.org/pdf/2202.03555.pdf](https://arxiv.org/pdf/2202.03555.pdf)'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Alexei Baevsky 等人（2022）。*data2vec：一个通用框架，用于语音、视觉和语言的自监督学习*。Arxiv 预印本，arXiv 2202.03555v1
    [cs.LG]：[https://arxiv.org/pdf/2202.03555.pdf](https://arxiv.org/pdf/2202.03555.pdf)
- en: 'Carl Doersch, Abhinav Gupta and Alexei Efros. (2015). *Unsupervised Visual
    Representation by Context Prediction*. International Conference on Computer Vision
    (ICCV): [https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf)'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Carl Doersch、Abhinav Gupta 和 Alexei Efros（2015）。*通过上下文预测进行无监督视觉表示学习*。计算机视觉国际会议（ICCV）：[https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf)
- en: 'Chuan Li. (2020). *OpenAI’s GPT-3 Language Model – a Technical Overview*. LambdaLabs
    Blog post: [https://lambdalabs.com/blog/demystifying-gpt-3/](https://lambdalabs.com/blog/demystifying-gpt-3/)'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chuan Li（2020）。*OpenAI的GPT-3语言模型——技术概述*。LambdaLabs 博客文章：[https://lambdalabs.com/blog/demystifying-gpt-3/](https://lambdalabs.com/blog/demystifying-gpt-3/)
- en: 'Deepak Pathak, et al. (2016). *Context Encoders: Feature Learning by Inpainting*:
    [https://openaccess.thecvf.com/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_CVPR_2016_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_CVPR_2016_paper.pdf)'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Deepak Pathak 等人（2016）。*上下文编码器：通过填充学习特征*：[https://openaccess.thecvf.com/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_CVPR_2016_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_CVPR_2016_paper.pdf)
- en: 'Florian Schroff, Dmitry Kalenichenko and James Philbin. (2025). *FaceNet: A
    Unified Embedding for Face Recognition and Clustering*. ArXiv Preprint, arXiv
    1503.03832 [cs.CV]: [https://arxiv.org/pdf/1503.03832.pdf](https://arxiv.org/pdf/1503.03832.pdf)'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Florian Schroff、Dmitry Kalenichenko 和 James Philbin（2025）。*FaceNet：一种统一的面部识别与聚类嵌入*。ArXiv
    预印本，arXiv 1503.03832 [cs.CV]：[https://arxiv.org/pdf/1503.03832.pdf](https://arxiv.org/pdf/1503.03832.pdf)
- en: 'Gustav Larsson, Michael Maire and Gregory Shakhnarovich. (2017). *Colorization
    as a Proxy Task for Visual Understanding*: [https://openaccess.thecvf.com/content_cvpr_2017/papers/Larsson_Colorization_as_a_CVPR_2017_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2017/papers/Larsson_Colorization_as_a_CVPR_2017_paper.pdf)'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gustav Larsson、Michael Maire 和 Gregory Shakhnarovich（2017）。*将图像上色作为视觉理解的代理任务*：[https://openaccess.thecvf.com/content_cvpr_2017/papers/Larsson_Colorization_as_a_CVPR_2017_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2017/papers/Larsson_Colorization_as_a_CVPR_2017_paper.pdf)
- en: 'Hamel Husain, et al. (2020). *CodeSearchNet Challenge: Evaluating the State
    of Semantic Code Search*. Arxiv Preprint, arXiv: 1909.09436 [cs.LG]: [https://arxiv.org/pdf/1909.09436.pdf](https://arxiv.org/pdf/1909.09436.pdf)'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hamel Husain 等人（2020）。*CodeSearchNet 挑战：评估语义代码搜索的现状*。Arxiv 预印本，arXiv: 1909.09436
    [cs.LG]：[https://arxiv.org/pdf/1909.09436.pdf](https://arxiv.org/pdf/1909.09436.pdf)'
- en: 'Hanting Chen, et al. (2021). *Pre-trained Image Processing Transformer*. Conference
    on Computer Vision and Pattern Recognition (CVPR): [https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.pdf)'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hanting Chen 等人（2021）。*预训练图像处理变换器*。计算机视觉与模式识别会议（CVPR）：[https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.pdf)
- en: 'Hyun Oh Song, Yu Xiang, Stefanie Jegelka and Silvio Savarese. (2015). *Deep
    Metric Learning via Lifted Structured Feature Embedding*. Arxiv Preprint, arXiv
    1511.06452 [cs.CV]: [https://arxiv.org/pdf/1511.06452.pdf](https://arxiv.org/pdf/1511.06452.pdf)'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hyun Oh Song, Yu Xiang, Stefanie Jegelka 和 Silvio Savarese. (2015). *通过提升结构化特征嵌入进行深度度量学习*.
    Arxiv 预印本, arXiv 1511.06452 [cs.CV]: [https://arxiv.org/pdf/1511.06452.pdf](https://arxiv.org/pdf/1511.06452.pdf)'
- en: 'Jacob Devlin, et al. (2019). *BERT: Pre-training of Deep Bidirectional Transformers
    for Language Understanding*. Arxiv Preprint, arXiv: 1810.04805v2 [cs.CL]: [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Jacob Devlin 等. (2019). *BERT：用于语言理解的深度双向变换器预训练*. Arxiv 预印本, arXiv: 1810.04805v2
    [cs.CL]: [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)'
- en: 'Jean-Bastien Grill, et al. (2020). *Bootstrap your own latent: A new approach
    to self-supervised learning*. Arxiv Preprint, arXiv 2006.07733 [cs.LG]: [https://arxiv.org/pdf/2006.07733.pdf](https://arxiv.org/pdf/2006.07733.pdf)'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Jean-Bastien Grill 等. (2020). *Bootstrap你的潜在空间：一种新的自监督学习方法*. Arxiv 预印本, arXiv
    2006.07733 [cs.LG]: [https://arxiv.org/pdf/2006.07733.pdf](https://arxiv.org/pdf/2006.07733.pdf)'
- en: 'Jiahao Xie, et al. (2021). *Delving into Inter-Image Invariance for Unsupervised
    Visual Representations*. Arxiv Preprint, arXiv: 2008.11702 [cs.CV]: [https://arxiv.org/pdf/2008.11702.pdf](https://arxiv.org/pdf/2008.11702.pdf)'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Jiahao Xie 等. (2021). *深入探讨无监督视觉表示的图像间不变性*. Arxiv 预印本, arXiv: 2008.11702 [cs.CV]:
    [https://arxiv.org/pdf/2008.11702.pdf](https://arxiv.org/pdf/2008.11702.pdf)'
- en: 'Joshua Robinson, Ching-Yao Chuang, Suvrit Sra and Stefanie Jegelka. (2021).
    *Contrastive Learning with Hard Negative Samples*. Arxiv Preprint, arXiv 2010.04592
    [cs.LG]: [https://arxiv.org/pdf/2010.04592.pdf](https://arxiv.org/pdf/2010.04592.pdf)'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Joshua Robinson, Ching-Yao Chuang, Suvrit Sra 和 Stefanie Jegelka. (2021). *使用困难负样本进行对比学习*.
    Arxiv 预印本, arXiv 2010.04592 [cs.LG]: [https://arxiv.org/pdf/2010.04592.pdf](https://arxiv.org/pdf/2010.04592.pdf)'
- en: 'Jure Zobontar, et al. (2021). Barlow Twins: Self-Supervised Learning via Redundancy
    Reduction. Arxiv Preprint, arXiv 2103.03230 [cs.CV]: [https://arxiv.org/pdf/2103.03230.pdf](https://arxiv.org/pdf/2103.03230.pdf)'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Jure Zobontar 等. (2021). Barlow Twins：通过冗余减少进行自监督学习. Arxiv 预印本, arXiv 2103.03230
    [cs.CV]: [https://arxiv.org/pdf/2103.03230.pdf](https://arxiv.org/pdf/2103.03230.pdf)'
- en: 'Kihyuk Sohn. (2016). *Improved Deep Metric Learning with Multi-class N-pair
    Loss Objective*. Advances in Neural Information Processing Systems: [https://proceedings.neurips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf](https://proceedings.neurips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf)'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Kihyuk Sohn. (2016). *改进的深度度量学习与多类 N-pair 损失目标*. 神经信息处理系统进展: [https://proceedings.neurips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf](https://proceedings.neurips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf)'
- en: 'Lilian Weng and Jong Wook Kim. (2021). *Self-supervised Learning: Self Prediction
    and Contrastive Learning*. NeurIPS Tutorial: [https://neurips.cc/media/neurips-2021/Slides/21895.pdf](https://neurips.cc/media/neurips-2021/Slides/21895.pdf)'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Lilian Weng 和 Jong Wook Kim. (2021). *自监督学习：自预测与对比学习*. NeurIPS教程: [https://neurips.cc/media/neurips-2021/Slides/21895.pdf](https://neurips.cc/media/neurips-2021/Slides/21895.pdf)'
- en: 'Lilian Weng. (Blog post 2021). Contrastive Representation Learning: [https://lilianweng.github.io/posts/2021-05-31-contrastive/](https://lilianweng.github.io/posts/2021-05-31-contrastive/)'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Lilian Weng. (博客文章 2021). 对比表示学习: [https://lilianweng.github.io/posts/2021-05-31-contrastive/](https://lilianweng.github.io/posts/2021-05-31-contrastive/)'
- en: 'Mathilde Caron, Piotr Bojanowsky, Armand Joulin and Matthijs Douze. (2019).
    *Deep Clustering for Unsupervised Learning of Visual Features*. Arxiv Preprint,
    arXiv: 1807.05520 [cs.CV]: [https://arxiv.org/pdf/1807.05520.pdf](https://arxiv.org/pdf/1807.05520.pdf)'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Mathilde Caron, Piotr Bojanowsky, Armand Joulin 和 Matthijs Douze. (2019). *深度聚类：用于无监督学习视觉特征*.
    Arxiv 预印本, arXiv: 1807.05520 [cs.CV]: [https://arxiv.org/pdf/1807.05520.pdf](https://arxiv.org/pdf/1807.05520.pdf)'
- en: 'Mathilde Caron, et al. (2020). *Unsupervised Learning of Visual Features by
    Contrasting Cluster Assignments*. Arxiv Preprint, arXiv: 2006.099882 [cs.CV]:
    [https://arxiv.org/pdf/2006.09882.pdf](https://arxiv.org/pdf/2006.09882.pdf)'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Mathilde Caron 等. (2020). *通过对比聚类分配进行无监督的视觉特征学习*. Arxiv 预印本, arXiv: 2006.099882
    [cs.CV]: [https://arxiv.org/pdf/2006.09882.pdf](https://arxiv.org/pdf/2006.09882.pdf)'
- en: 'Mehdi Noroozi and Paolo Favaro. (2016). *Unsupervised Learning of Visual Representations
    by solving Jigsaw Puzzles*. European Conference on Computer Vision: [https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5](https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5)'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Mehdi Noroozi 和 Paolo Favaro. (2016). *通过解拼图来进行无监督的视觉表示学习*. 欧洲计算机视觉大会: [https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5](https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5)'
- en: 'Michael Gutmann, Aapo Hyvarinen. (2010). *Noise-contrastive estimation: A new
    estimation principle for unnormalized statistical models*. Proceedings of Machine
    Learning Research (PMLR): [http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Michael Gutmann 和 Aapo Hyvarinen（2010）。*噪声对比估计：一种新的无标准化统计模型估计原理*。机器学习研究会议论文集（PMLR）：[http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf)
- en: 'Nal Kalchbrenner, et al. (2018). *Efficient Neural Audio Synthesis*. Proceedings
    MLR Press: [http://proceedings.mlr.press/v80/kalchbrenner18a/kalchbrenner18a.pdf](http://proceedings.mlr.press/v80/kalchbrenner18a/kalchbrenner18a.pdf)'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Nal Kalchbrenner 等人（2018）。*高效的神经音频合成*。机器学习研究会议论文集（MLR）：[http://proceedings.mlr.press/v80/kalchbrenner18a/kalchbrenner18a.pdf](http://proceedings.mlr.press/v80/kalchbrenner18a/kalchbrenner18a.pdf)
- en: 'Pascal Vincent, et al. (2010). *Stacked Denoising Autoencoders: Learning Useful
    Representations in a Deep Network with a Local Denoising Criterion*. Journal of
    Machine Learning Research (JMLR): [https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com](https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com)'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pascal Vincent 等人（2010）。*堆叠去噪自编码器：通过局部去噪标准在深度网络中学习有用表示*。机器学习研究期刊（JMLR）：[https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com](https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com)
- en: 'Patrick Esser, Robin Rombach and Bjorn Ommer. (2021). Taming Transformers for
    High-Resolution Image Synthesis. Computer Vision and Pattern Recognition (CVPR):
    [https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf)'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Patrick Esser、Robin Rombach 和 Bjorn Ommer（2021）。*为高分辨率图像合成驯服变换器*。计算机视觉与模式识别（CVPR）：[https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf](https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf)
- en: 'Philip Bachman, R Devon Hjelm and William Buchwalter. (2019). *Learning Representations
    by Maximizing Mutual Information across Views. Advances in Neural Information
    Processing Systems (NeurIPS)*: [https://proceedings.neurips.cc/paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf)'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Philip Bachman、R Devon Hjelm 和 William Buchwalter（2019）。*通过最大化不同视图之间的互信息学习表示*。神经信息处理系统进展（NeurIPS）：[https://proceedings.neurips.cc/paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf)
- en: 'Prafulla Dhariwal, et al. (2020). Jukebox: *A Generative Model for Music*.
    Arxiv Preprint, arXiv 2005.00341v1 [eess.AS]: [https://arxiv.org/pdf/2005.00341.pdf](https://arxiv.org/pdf/2005.00341.pdf)'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prafulla Dhariwal 等人（2020）。Jukebox：*一种生成音乐的模型*。Arxiv 预印本，arXiv 2005.00341v1
    [eess.AS]：[https://arxiv.org/pdf/2005.00341.pdf](https://arxiv.org/pdf/2005.00341.pdf)
- en: 'Ruslan Salakhutdinov and Geoff Hinton. (2007). *Learning a Nonlinear Embedding
    by Preserving Class Neighborhood Structure.* Proceedings of Machine Learning Research
    (PMLR): [http://proceedings.mlr.press/v2/salakhutdinov07a/salakhutdinov07a.pdf](http://proceedings.mlr.press/v2/salakhutdinov07a/salakhutdinov07a.pdf)'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ruslan Salakhutdinov 和 Geoff Hinton（2007）。*通过保持类别邻域结构学习非线性嵌入*。机器学习研究会议论文集（PMLR）：[http://proceedings.mlr.press/v2/salakhutdinov07a/salakhutdinov07a.pdf](http://proceedings.mlr.press/v2/salakhutdinov07a/salakhutdinov07a.pdf)
- en: 'Spyros Gidaris, Praveer Singh and Nicos Komodakis. (2018). *Unsupervised Representation
    Learning by Predicting Image Rotations*. Arxiv Preprint, arXiv 1803.07728v1 [cs.CV]:
    [https://arxiv.org/pdf/1803.07728.pdf](https://arxiv.org/pdf/1803.07728.pdf)'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spyros Gidaris、Praveer Singh 和 Nicos Komodakis（2018）。*通过预测图像旋转进行无监督表示学习*。Arxiv
    预印本，arXiv 1803.07728v1 [cs.CV]：[https://arxiv.org/pdf/1803.07728.pdf](https://arxiv.org/pdf/1803.07728.pdf)
- en: 'Sumit Chopra, et al. (2005). *Learning a Similarity Metric Discriminatively,
    with application to Face Verification*. IEEE Computer Society: [http://www.cs.utoronto.ca/~hinton/csc2535_06/readings/chopra-05.pdf](http://www.cs.utoronto.ca/~hinton/csc2535_06/readings/chopra-05.pdf)'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sumit Chopra 等人（2005）。*通过判别学习相似度度量，应用于人脸验证*。IEEE 计算机学会：[http://www.cs.utoronto.ca/~hinton/csc2535_06/readings/chopra-05.pdf](http://www.cs.utoronto.ca/~hinton/csc2535_06/readings/chopra-05.pdf)
- en: 'Ting Chen, Simon Kornblith, Mohammed Norouzi and Geoffrey Hinton. (2020). *A
    Simple Framework for Contrastive Learning*. Arxiv Preprint, arXiv 2002.05709 [cs.LG]:
    [https://arxiv.org/pdf/2002.05709.pdf](https://arxiv.org/pdf/2002.05709.pdf)'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ting Chen, Simon Kornblith, Mohammed Norouzi 和 Geoffrey Hinton. (2020). *对比学习的简单框架*。Arxiv
    预印本，arXiv 2002.05709 [cs.LG]：[https://arxiv.org/pdf/2002.05709.pdf](https://arxiv.org/pdf/2002.05709.pdf)
- en: 'Yonglong Tian, Dilip Krishnan and Philip Isola. (2020). *Contrastive Multiview
    Coding*. Arxiv Preprint, arXiv: 1906.05849 [cs.CV]: [https://arxiv.org/pdf/1906.05849.pdf?ref=https://githubhelp.com](https://arxiv.org/pdf/1906.05849.pdf?ref=https://githubhelp.com)'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Yonglong Tian, Dilip Krishnan 和 Philip Isola. (2020). *对比多视角编码*。Arxiv 预印本，arXiv:
    1906.05849 [cs.CV]：[https://arxiv.org/pdf/1906.05849.pdf?ref=https://githubhelp.com](https://arxiv.org/pdf/1906.05849.pdf?ref=https://githubhelp.com)'
- en: 'Zhilin Yang, et al. (2019). *XLNet: Generalized Autoregressive Pre-training
    for Language Understanding*: [https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhilin Yang 等. (2019). *XLNet：用于语言理解的广义自回归预训练*：[https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)
- en: '*Prompt Engineering*. (7th July 2022). Wikipedia, Wikimedia Foundation: [https://en.wikipedia.org/wiki/Prompt_engineering](https://en.wikipedia.org/wiki/Prompt_engineering)'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*提示工程*。(2022年7月7日)。维基百科，维基媒体基金会：[https://en.wikipedia.org/wiki/Prompt_engineering](https://en.wikipedia.org/wiki/Prompt_engineering)'
- en: Join our book’s Discord space
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人一起学习，和超过 2000 名成员共同成长：[https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
