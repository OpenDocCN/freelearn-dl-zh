- en: 7\. Generative Adversarial Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7\. 生成对抗网络
- en: Introduction
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 引言
- en: 'In this chapter, you will embark on another interesting topic within the deep
    learning domain: **Generative Adversarial Networks** (**GANs**). You will get
    introduced to GANs and their basic components, along with some of their use cases.
    This chapter will give you hands-on experience of creating a GAN to generate a
    data distribution produced by a sine function. You will also be introduced to
    deep convolutional GANs and will perform an exercise to generate an MNIST data
    distribution. By the end of this chapter, you will have tested your understanding
    of GANs by generating the MNIST fashion dataset.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将探索深度学习领域中另一个有趣的主题：**生成对抗网络**（**GANs**）。您将介绍GANs及其基本组件，以及它们的一些用例。本章将通过创建一个GAN来生成由正弦函数产生的数据分布，为您提供实践经验。您还将介绍深度卷积GANs，并进行一个生成MNIST数据分布的练习。通过本章的学习，您将测试您对GANs的理解，生成MNIST时尚数据集。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: The power of creativity was always the exclusive domain of the human mind. This
    was one of the facts touted as one of the major differences between the human
    mind and the artificial intelligence domain. However, in the recent past, deep
    learning has been making baby steps in the path to being creative. Imagine you
    were at the Sistine Chapel in the Vatican and were looking up with bewilderment
    at the frescos immortalized by Michelangelo, wishing your deep learning models
    were able to recreate something like that. Well, maybe 10 years back, people would
    have scoffed at your thought. Not anymore, though – deep learning models have
    made great strides in regenerating immortal works. Applications like these are
    made possible by a class of networks called **Generative Adversarial Networks**
    (**GANs**).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 创造力的力量一直是人类思维的专属领域。这是被宣称为人类思维与人工智能领域之间主要差异之一的事实。然而，在最近的过去，深度学习一直在迈向创造力之路上迈出婴儿步伐。想象一下，你在梵蒂冈的西斯廷教堂，目瞪口呆地仰望米开朗基罗永恒不朽的壁画，希望你的深度学习模型能够重新创作出类似的作品。嗯，也许10年前，人们会嗤之以鼻。但现在不同了——深度学习模型在重现不朽作品方面取得了巨大进展。这类应用得益于一类称为**生成对抗网络**（**GANs**）的网络。
- en: 'Many applications have been made possible with GANs. Take a look at the following image:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用都依靠GANs实现。请看下面的图像：
- en: '![Figure 7.1: Image translation using GANs'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.1：使用GANs进行图像翻译'
- en: '](img/B15385_07_01.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_01.jpg)'
- en: 'Figure 7.1: Image translation using GANs'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1：使用GANs进行图像翻译
- en: 'Note:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: 'The preceding image is sourced from the research paper titled *Image-to-Image
    Translation with Conditional Adversarial Networks*: Phillip Isola, Jun-Yan Zhu,
    Tinghui Zhou, Alexei A. Efros, available at [https://arxiv.org/pdf/1611.07004.pdf](https://arxiv.org/pdf/1611.07004.pdf).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像来源于标题为《带条件对抗网络的图像到图像翻译》的研究论文：Phillip Isola、Jun-Yan Zhu、Tinghui Zhou、Alexei
    A. Efros，详见[https://arxiv.org/pdf/1611.07004.pdf](https://arxiv.org/pdf/1611.07004.pdf)。
- en: The preceding image demonstrates how the input image, which has a very different
    color scheme, has been transformed by the GAN into an image that looks very similar
    to the real one. This application of GANs is called image translation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像展示了输入图像如何通过GAN转换为看起来与真实图像非常相似的图像。这种GAN的应用称为图像翻译。
- en: 'In addition to these examples, many other use cases are finding traction. Some
    of the notable ones are as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些例子，许多其他用例也开始受到关注。其中一些显著的用例如下：
- en: Synthetic data generation for data augmentation
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于数据增强的合成数据生成
- en: Generating cartoon characters
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成卡通人物
- en: Text to image translation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本到图像翻译
- en: Three-dimensional object generation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三维对象生成
- en: The list goes on. As the days go by, applications of GANs increasingly become mainstream.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 无论时间如何流逝，GANs的应用越来越成为主流。
- en: So, what exactly are GANs? What are the inner dynamics of GANs? How do you generate
    images or other data distributions from totally unconnected distributions? In
    this chapter, we'll find out the answers to those questions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，GANs究竟是什么？GANs的内在动态是什么？如何从完全不相关的分布中生成图像或其他数据分布？在本章中，我们将找到这些问题的答案。
- en: In the previous chapter, we learned about **recurrent neural networks** (**RNNs**),
    a class of deep learning networks used for sequence data. In this chapter, we
    will embark on a fascinating safari to the world of GANs. First, we will start
    with an introduction to GANs. Then, we'll focus on generating a data distribution
    that is similar to a known mathematical expression. We'll then move on to **deep
    convolutional GANs** (**DCGANs**). To see how well our generative models work,
    we will generate a data distribution similar to the MNIST handwritten digits.
    We'll start this journey by learning about GANs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们学习了**循环神经网络**（**RNNs**），一种用于序列数据的深度学习网络。在本章中，我们将开启一场关于GAN的精彩探索之旅。首先，我们将介绍GAN。然后，我们将重点介绍生成一个与已知数学表达式相似的数据分布。接下来，我们将介绍**深度卷积GANs**（**DCGANs**）。为了验证我们的生成模型的效果，我们将生成一个类似于MNIST手写数字的数据分布。我们将从学习GAN开始这段旅程。
- en: Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Depending on your system configuration, some of the exercises and activities
    in this chapter may take quite a long time to execute.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的系统配置，本章中的一些练习和活动可能需要较长时间才能执行完成。
- en: Key Components of Generative Adversarial Networks
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成对抗网络的关键组件
- en: GANs are used to create a data distribution from random noise data and make
    it look similar to a real data distribution. GANs are a family of deep neural
    networks that comprise two networks that are competing against each other. One
    of these networks is called the **generator network**, while the other is called
    the **discriminator network**. The functions of these two networks are to compete
    against each other to generate a probability distribution that closely mimics
    an existing probability distribution. To state an example of generating a new
    probability distribution, let's say we have a collection of images of cats and
    dogs (real images). Using a GAN, we can generate a different set of images (fake
    images) of cats and dogs from a very random distribution of numbers. The success
    of a GAN is in generating the best set of cat and dog images to the point that
    it is difficult for people to differentiate between the fake ones and the real
    ones.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: GANs用于从随机噪声数据中创建一个数据分布，并使其看起来类似于真实的数据分布。GANs是一类深度神经网络，由两个相互竞争的网络组成。其中一个网络叫做**生成器网络**，另一个叫做**判别器网络**。这两个网络的功能是相互竞争，生成一个尽可能接近真实概率分布的概率分布。举一个生成新概率分布的例子，假设我们有一组猫狗的图片（真实图片）。使用GAN，我们可以从一个非常随机的数字分布中生成一组不同的猫狗图片（虚假图片）。GAN的成功之处在于生成最佳的猫狗图片集，以至于人们很难分辨虚假图片和真实图片。
- en: Another example where GANs can become useful is in data privacy. The data of
    companies, especially in domains such as finance and healthcare, is extremely
    sensitive. However, there might be instances where data has to be shared with
    third parties for research purposes. In such scenarios, to maintain the confidentiality
    of data, companies can use GANs to generate datasets that are similar in nature
    to their existing datasets. There is a multitude of such business use cases where
    GANs can come in really handy.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个GAN可以派上用场的例子是在数据隐私领域。尤其是在金融和医疗等领域，公司的数据非常敏感。然而，在某些情况下，可能需要将数据共享给第三方进行研究。为了保持数据的机密性，公司可以使用GAN生成与现有数据集相似的数据集。在许多业务场景中，GAN都可以发挥重要作用。
- en: 'Let''s understand GANs better by mapping out some of their components, as shown
    in the following diagram:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过绘制一些GAN的组件来更好地理解GAN，如下图所示：
- en: '![Figure 7.2: Example of GAN structure'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2：GAN结构示例](img/B15385_07_02.jpg)'
- en: '](img/B15385_07_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_02.jpg)'
- en: 'Figure 7.2: Example of GAN structure'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2：GAN结构示例
- en: 'The preceding figure provides a concise overview of the components of a GAN
    and how they come in handy in generating fake images from real ones. Let''s understand
    the process in the context of the preceding diagram:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图提供了GAN组件的简要概述，并展示了它们如何帮助从真实图片生成虚假图片。让我们在前述图的背景下理解这个过程：
- en: The set of images at the top-left corner of the preceding figure represents
    a probability distribution of real data (for example, MNIST, images of cats and
    dogs, pictures of human faces, and more).
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面图中左上角的图片集合代表了真实数据的概率分布（例如，MNIST、猫狗图片、人脸图片等）。
- en: The generative network shown in the bottom-left part of the diagram generates
    fake images (probability distributions) from a random noise distribution.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图表左下部分显示的生成网络从随机噪声分布中生成虚假图像（概率分布）。
- en: The trained discriminative network classifies whether the image that is fed
    in is fake or real.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 经过训练的鉴别网络对输入的图像进行分类，判断其真伪。
- en: A feedback loop (the diamond-shaped box) working through the backpropagation
    algorithm gives feedback to the generator network, thereby refining the parameters
    of the generator model.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反馈循环（菱形框）通过反向传播算法向生成器网络提供反馈，从而优化生成模型的参数。
- en: The parameters continue to be refined until the discriminator network can't
    discriminate between the fake images and the real ones.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参数会持续优化，直到鉴别网络无法区分虚假图像和真实图像为止。
- en: Now that we have an overview of each of the components, let's dive deeper and
    understand them better through a problem statement.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对每个组件有了概述，让我们通过问题陈述更深入地理解它们。
- en: Problem Statement – Generating a Distribution Similar to a Given Mathematical Function
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述 – 生成与给定数学函数类似的分布
- en: In this problem, we will use GANs to generate a distribution that is similar
    to a data distribution from a mathematical function. The function we will be using
    to generate the real data is a simple **sine wave**. We will train a GAN to generate
    a fake distribution of data that will be similar to the data we generated from
    the known mathematical function. We will progressively build each component that's
    required while we traverse the solution for this problem statement.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个问题中，我们将使用GAN生成类似于数学函数数据分布的分布。我们将使用简单的**正弦波**函数来生成真实数据，训练GAN生成与从已知数学函数生成的数据类似的虚假数据分布。在解决这个问题陈述的过程中，我们将逐步构建每个所需的组件。
- en: 'The process we will follow is explained in the following figure. We will follow
    a pedagogical approach as per the steps detailed in this figure:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下图解中详细说明的步骤，采用教学方法来执行该过程：
- en: '![Figure 7.3: Four-step process to building a GAN from a known function'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.3：从已知函数构建GAN的四步过程'
- en: '](img/B15385_07_03.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_03.jpg)'
- en: 'Figure 7.3: Four-step process to building a GAN from a known function'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：从已知函数构建GAN的四步过程
- en: Now, let's explore each of these processes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐个探索这些过程。
- en: Process 1 – Generating Real Data from the Known Function
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过程1 – 从已知函数生成真实数据
- en: To begin our journey, we need a real distribution of data. This distribution
    of data will comprise two features – the first one is the sequence and the second
    one is the sine of the sequence. The first feature is a sequence of data points
    spaced at equal intervals. To generate this sequence, we need to randomly generate
    a data point from a normal distribution and then find other numbers spaced in
    sequence at equal intervals. The second feature will be the `sine()` of the first
    feature. Both these features will form our real data distribution. Before we get
    into an exercise that generates the real dataset, let's look at some of the functions
    in the `numpy` library we will use in this process.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始我们的旅程，我们需要一个真实数据分布。这个数据分布将包括两个特征 – 第一个是序列，第二个是序列的正弦值。第一个特征是等间隔的数据点序列。为了生成这个序列，我们需要从正态分布随机生成一个数据点，然后找到等间隔序列中的其他数值。第二个特征将是第一个特征的`sine()`。这两个特征将构成我们的真实数据分布。在进入生成真实数据集的练习之前，让我们看一下我们将在此过程中使用的`numpy`库中的一些函数。
- en: '**Random Number Generation**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机数生成**'
- en: 'First, we will generate a random number from a normal distribution using the
    following function:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用以下函数从正态分布中生成一个随机数：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This function takes three arguments:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受三个参数：
- en: '`loc`: This is the mean of the data distribution.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loc`: 这是数据分布的均值。'
- en: '`scale`: This is the standard deviation of the data distribution.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scale`: 这是数据分布的标准差。'
- en: '`size`: This defines the number of data points we want.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`size`: 这定义了我们想要的数据点数量。'
- en: '**Arranging the Data into a Sequence**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**将数据排列成序列**'
- en: 'To arrange data in a sequence, we use the following function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据排列成序列，我们使用以下函数：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The arguments are the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 参数如下：
- en: '`start`: This is the point that the sequence should start from.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start`: 序列应从此点开始。'
- en: '`end`: The point where the sequence ends.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`end`: 序列结束的点。'
- en: '`spacing`: The frequency between each successive number in the sequence. For
    example, if we start off with 1 and generate a series with a spacing of `0.1`,
    the series will look as follows:'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spacing`：序列中每个连续数字之间的间隔。例如，如果我们从1开始，生成一个间隔为`0.1`的序列，那么序列将如下所示：'
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Generating the Sine Wave**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成正弦波**'
- en: 'To generate the sine of a number, we use the following command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成一个数字的正弦值，我们使用以下命令：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's use these concepts in the following exercise and learn how to generate
    a real data distribution.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的练习中使用这些概念，学习如何生成一个真实的数据分布。
- en: 'Exercise 7.01: Generating a Data Distribution from a Known Function'
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.01：从已知函数生成数据分布
- en: 'In this exercise, we will generate a data distribution from a simple sine function.
    By completing this exercise, you will learn how to generate a random number from
    a normal distribution and create a sequence of equally spaced data with the random
    number as its center. This sequence will be the first feature. The second feature
    will be created by calculating the `sine()` for the first feature. Follow these
    steps to complete this exercise:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将从一个简单的正弦函数生成数据分布。通过完成此练习，你将学习如何从正态分布中生成一个随机数，并使用这个随机数作为中心生成一个等间隔的数据序列。这个序列将是第一个特征。第二个特征将通过计算第一个特征的`sin()`值来创建。按照以下步骤完成此练习：
- en: 'Open a new Jupyter Notebook and name it `Exercise 7.01`. Run the following
    command to import the necessary library packages:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook，并将其命名为`Exercise 7.01`。运行以下命令以导入必要的库：
- en: '[PRE4]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Generate a random number from a normal distribution that has a mean of 3 and
    a standard deviation of 1:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从均值为3、标准差为1的正态分布中生成一个随机数：
- en: '[PRE5]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Using the previously generated random number as a midpoint, we will generate
    equal sequences of numbers to the right and left of the midpoint. We will generate
    a batch of 128 numbers. So, we take 64 numbers each to the right and left of the
    midpoint with a spacing of 0.1\. The following code generates a sequence to the
    right of the midpoint:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用之前生成的随机数作为中点，我们将在中点的左右两侧生成相等数量的数字序列。我们将生成128个数字。因此，我们在中点的左右两侧各取64个数字，间隔为0.1。以下代码生成了中点右侧的序列：
- en: '[PRE6]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Generate 64 numbers to the left of the midpoint:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成中点左侧的64个数字：
- en: '[PRE7]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Concatenate both these sequences to generate the first feature:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这两个序列连接起来，生成第一个特征：
- en: '[PRE8]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should get an output similar to the one shown here:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到类似下面的输出：
- en: '![Figure 7.4: Sequence of numbers with equal spacing'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.4：等间隔数字序列](img/B15385_07_04.jpg)'
- en: '](img/B15385_07_04.jpg)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_04.jpg)'
- en: 'Figure 7.4: Sequence of numbers with equal spacing'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.4：等间隔数字序列
- en: The preceding is the distribution of `128` numbers equally spaced from one another.
    This sequence will be our first feature for the data distribution.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述是`128`个等间隔数字的分布。这个序列将成为我们数据分布的第一个特征。
- en: 'Generate the second feature, which is the `sine()` of the first feature:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成第二个特征，即第一个特征的`sin()`值：
- en: '[PRE9]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Plot the distribution:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制分布图：
- en: '[PRE10]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You should get the following output:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 7.5: Plot for the sine function'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.5：正弦函数的图表](img/B15385_07_05.jpg)'
- en: '](img/B15385_07_05.jpg)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_05.jpg)'
- en: 'Figure 7.5: Plot for the sine function'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.5：正弦函数的图表
- en: The preceding plot shows the distribution that you would be trying to mimic
    using GANs.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述图表显示了你试图使用GANs模仿的分布。
- en: 'Reshape each feature before concatenating them:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在连接这些特征之前，先对每个特征进行重塑：
- en: '[PRE11]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Concatenate both features to form a single DataFrame:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这两个特征连接起来，形成一个单一的DataFrame：
- en: '[PRE12]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You should get the following output:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE13]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3gHhv42](https://packt.live/3gHhv42).
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3gHhv42](https://packt.live/3gHhv42)。
- en: You can also run this example online at [https://packt.live/2O62M6r](https://packt.live/2O62M6r).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2O62M6r](https://packt.live/2O62M6r)在线运行此示例。你必须执行整个Notebook才能得到期望的结果。
- en: In this exercise, we created a data distribution from a mathematical function.
    We will be using this data distribution later to train the GAN to generate a distribution
    similar to this. In a production environment, you will be provided with a real
    dataset, similar to the MNIST or `Imagenet` datasets. In this case, our real dataset
    is a known mathematical function. Later in this chapter, we will use some random
    noise data and train the GAN to make that random noise data similar to this real
    data distribution.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个练习中，我们从一个数学函数创建了一个数据分布。稍后我们将使用这个数据分布来训练GAN生成类似的分布。在生产环境中，你会获得一个真实的数据集，类似于MNIST或`Imagenet`数据集。在本例中，我们的真实数据集是一个已知的数学函数。稍后在本章中，我们将使用一些随机噪声数据，并训练GAN使这些随机噪声数据类似于这个真实的数据分布。  '
- en: Now that we have seen the real data distribution, the next section will be all
    about creating a basic generative network.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '现在我们已经看到真实数据分布，接下来的部分将专注于创建一个基本的生成网络。  '
- en: Process 2 – Creating a Basic Generative Network
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '过程2 – 创建一个基本的生成网络  '
- en: In the previous process, we worked on an example that will generate a distribution
    from a known function. As we mentioned earlier, the purpose of the generative
    network is to sample data from any arbitrary distribution and then transform that
    data into generative samples that look similar to the known distribution.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '在之前的过程里，我们做了一个例子来生成来自已知函数的分布。正如我们之前提到的，生成网络的目的是从任意分布中采样数据，并将这些数据转换为生成的样本，使其看起来与已知分布相似。  '
- en: The way the generative network achieves this is through the dynamics of the
    generator, the discriminator, and the training process. The success of the generative
    network relies on its ability to create data distributions that the discriminator
    can't differentiate between – in other words, it can't determine whether the distribution
    is fake or not. This ability of the generative network to create distributions
    that can fool the discriminator is acquired by the training process. We will talk
    more about the discriminator and the training process later in this chapter. For
    now, let's see how a generator network can be constructed to generate fake data
    distributions from some random distribution.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 生成网络实现这一点的方式是通过生成器、判别器和训练过程的动态关系。生成网络的成功取决于它能否创建出判别器无法区分的数据分布——换句话说，判别器无法判断这个分布是否是假的。生成网络能够创建可以欺骗判别器的分布的能力，是通过训练过程获得的。我们稍后会详细讲解判别器和训练过程。现在，让我们看看如何构建一个生成器网络，以便从某些随机分布中生成虚假的数据分布。
- en: Building the Generative Network
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '构建生成网络  '
- en: Generative networks are neural networks that are trained to transform an arbitrary
    distribution so that it looks similar to the known distribution. We can use any
    type of neural network for this, such as **multi-layer perceptrons** (**MLPs**),
    **convolutional neural networks** (**CNNs**), and more, to build the generator
    network. The input data to these networks are the samples that we take from any
    arbitrary distribution. In this example, we will be using an MLP to build a generative
    network. Before we start building the network, let's revisit some of the building
    blocks of a neural network that you will have learned about in the previous chapters.
    We will be building the network using the Keras library.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '生成网络是通过训练神经网络将任意分布转换为类似已知分布的网络。我们可以使用任何类型的神经网络来构建这个生成器网络，比如**多层感知机**（**MLPs**）、**卷积神经网络**（**CNNs**）等。输入这些网络的数据是我们从任意分布中采样得到的样本。在这个例子中，我们将使用MLP来构建生成网络。在我们开始构建网络之前，让我们回顾一下你在前几章中学到的一些神经网络的基本构件。我们将使用Keras库来构建网络。  '
- en: Sequential()
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`Sequential()`  '
- en: 'As you might already know, a neural network consists of different layers of
    nodes that have connections between them. The `Sequential()` API is the mechanism
    through which you can create those layers in Keras. The `Sequential()` API is
    instantiated using the following code:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '如你所知，神经网络由不同层次的节点组成，这些节点之间有连接。`Sequential()` API是你在Keras中创建这些层的机制。`Sequential()`
    API可以通过以下代码实例化：  '
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the first part of the code, the `Sequential()` class is imported from the
    `tensorflow.Keras` module. It is then instantiated as a variable model in the
    second line of code.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '在代码的第一部分，`Sequential()` 类从 `tensorflow.Keras` 模块中导入。然后，它在第二行代码中被实例化为变量model。  '
- en: Kernel Initializers
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '核心初始化器  '
- en: In *Chapter 2*, *Neural Networks*, you learned that the training process involves
    updating the weights and biases of a neural network so that the function that
    maps the inputs to the outputs is learned effectively. As a first step in the
    training process, we initialize some values for the weights and biases. These
    get updated more during the backpropagation stage. The initialization of the weights
    and biases is done through a parameter called the `he_uniform` in the exercise
    that follows. A kernel initializer will be added as a parameter within the network.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2章*，*神经网络*中，你已经学习过训练过程涉及更新神经网络的权重和偏置，以便有效地学习将输入映射到输出的函数。在训练过程的第一步，我们初始化权重和偏置的某些值。这些值将在反向传播阶段进一步更新。在接下来的练习中，权重和偏置的初始化通过一个名为`he_uniform`的参数完成。一个核初始化器将作为参数添加到网络中。
- en: Dense Layers
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密集层
- en: 'The basic dynamics within each layer in a neural network is the matrix multiplication
    (dot product) between the weights for the layer and the input to the layer, and
    the further addition of a bias. This is represented by the `dot(X,W) + B` equation,
    where `X` is the input to the layer, `W` is the weight or the kernel, and `B`
    is the bias. This operation of the neural network is done using the dense layer
    in Keras. This is implemented in code as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中每一层的基本动态是层的权重与层输入的矩阵乘法（点积），并进一步加上偏置。这可以通过`dot(X,W) + B`方程表示，其中`X`是输入，`W`是权重或核，`B`是偏置。神经网络的这个操作是通过Keras中的密集层实现的，代码实现如下：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The above code block is solely meant to explain how the code is implemented.
    It may not result in a desirable output when run in its current form. For now,
    try to understand the syntax completely; we will be putting this code into practice
    in *Exercise 7.02*, *Building a Generative Network*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码块仅用于解释代码是如何实现的。以当前形式运行时，可能无法得到理想的输出。目前，尝试完全理解语法；我们将在*练习 7.02*中实际应用这段代码，*构建生成网络*。
- en: 'As you can see, we add a dense layer to the instantiation of the `Sequential()`
    class (`Genmodel`) we created earlier. Some of the key parameters that need to
    be given to define a dense layer are as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们在之前创建的`Sequential()`类（`Genmodel`）实例化时添加了一个密集层。定义一个密集层时需要给出的关键参数如下：
- en: '`(hidden_layer)`: As you might know, hidden layers are the intermediate layers
    in a neural network. The number of nodes of a hidden layer is defined as the first
    parameter.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(hidden_layer)`：如你所知，隐藏层是神经网络中的中间层。隐藏层的节点数定义为第一个参数。'
- en: '`(activation)`: The other parameter is the type of activation function that
    will be used. Activation functions will be discussed in detail in the next section.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(activation)`：另一个参数是将要使用的激活函数类型。激活函数将在下一节中详细讨论。'
- en: '`(kernel_initializer)`: The kind of kernel initializer that is used for the
    layer is defined within the dense layer.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(kernel_initializer)`：用于层的核初始化器的种类在密集层中定义。'
- en: '`(input_dim)`: For the first layer of the network, we have to define the dimensions
    of the input (`input_dim`). For the subsequent layers, this is deduced automatically
    based on the output dimensions of each layer.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(input_dim)`：对于网络的第一层，我们必须定义输入的维度（`input_dim`）。对于后续的层，这个值会根据每层的输出维度自动推导出来。'
- en: Activation Functions
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'As you might know, activation functions introduce non-linearity to the outputs
    of a neuron. In a neural network, activation functions are introduced just after
    the dense layer. The output of the dense layer is the input of the activation
    function. Different activation functions will be used within the following exercise.
    They are as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，激活函数为神经元的输出引入了非线性。在神经网络中，激活函数会紧跟在密集层之后。密集层的输出就是激活函数的输入。接下来的练习中会使用不同的激活函数，具体如下：
- en: '**ReLU**: This stands for **Rectified Linear Unit**. This activation function
    only outputs positive values. All negative values will be output as zero. This
    is one of the most widely used activation functions.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ReLU**：这代表**修正线性单元**。这种激活函数只输出正值，所有负值将输出为零。这是最广泛使用的激活函数之一。'
- en: '**ELU**: This stands for **Exponential Linear Unit**. This is very similar
    to ReLU except for the fact that it outputs negative values as well.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ELU**：这代表**指数线性单元**。它与ReLU非常相似，不同之处在于它也能输出负值。'
- en: '**Linear**: This is a straight-line activation function. In this function,
    the activations are proportional to the inputs.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性**：这是一种直线型激活函数。在这个函数中，激活值与输入成正比。'
- en: '**SELU**: This stands for **Scaled Exponential Linear Unit**. This activation
    function is a relatively lesser-used one. It enables an idea called internal normalization,
    which ensures that the mean and variance from the previous layers are maintained.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SELU**：这代表**缩放指数线性单元**。这种激活函数是一种相对较少使用的函数。它启用了一个叫做内部归一化的理念，确保前一层的均值和方差保持不变。'
- en: '**Sigmoid**: This is a very standard activation function. A sigmoid function
    squashes any input into a value between 0 and 1\. Therefore, the output from a
    sigmoid function can also be treated as a probability distribution as the values
    are between 0 and 1.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sigmoid**：这是一种非常标准的激活函数。Sigmoid 函数将任何输入压缩到 0 和 1 之间。因此，Sigmoid 函数的输出也可以被视为概率分布，因为其值在
    0 和 1 之间。'
- en: Now that we have seen some of the basic building blocks of the network, let's
    go ahead and build our generative network in the next exercise.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些网络的基本构建模块，接下来让我们在下一个练习中构建我们的生成网络。
- en: Before we start the exercise, let's see where the next exercise lies in the
    overall scheme of things. In *Exercise 7.01*, *Generating a Data Distribution
    from a Known Function*, we created a data distribution from a known mathematical
    function, which is a `sine()` function. We created the entire distribution by
    arranging the first feature with equal intervals and then creating the second
    feature by taking the `sine()` function of the first feature. So, we literally
    controlled the entire process of creating this dataset. That's why this is called
    a real data distribution because the data is created from a known function. The
    ultimate aim of a GAN is to transform a random noise distribution and make it
    look like a real data distribution; that is, make a random distribution look like
    the structured `sine()` distribution. This will be achieved in later exercises.
    However, as a first step, we will create a generative network that will create
    a random noise distribution. This is what we will do in the next exercise.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始练习之前，让我们看看下一个练习在整体结构中所处的位置。在*练习 7.01*，*从已知函数生成数据分布*中，我们从已知的数学函数（即 `sine()`
    函数）生成了数据分布。我们通过将第一个特征按等间距排列，然后使用第一个特征的 `sine()` 函数创建第二个特征，从而生成了整个分布。所以我们实际上控制了这个数据集的创建过程。这就是为什么这被称为真实数据分布，因为数据是从已知函数生成的。生成对抗网络（GAN）的最终目标是将随机噪声分布转化为看起来像真实数据分布的东西；即将一个随机分布转化为结构化的
    `sine()` 分布。这将在以后的练习中实现。然而，作为第一步，我们将创建一个生成网络，用于生成随机噪声分布。这就是我们在下一个练习中要做的事情。
- en: 'Exercise 7.02: Building a Generative Network'
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.02：构建生成网络
- en: 'In this exercise, we will build a generative network. The purpose of the generative
    network is to generate fake data distribution from a random noise data. We''ll
    do this by generating random data points as input to the generator network. Then,
    we''ll build a six-layer network, layer by layer. Finally, we''ll predict the
    output from the network and plot the output distribution. This data distribution
    will be our fake distribution. Follow these steps to complete this exercise:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将构建一个生成网络。生成网络的目的是从随机噪声数据生成虚假的数据分布。我们将通过生成随机数据点作为生成器网络的输入来实现这一点。然后，我们将逐层构建一个六层网络。最后，我们将预测网络的输出并绘制输出分布。这些数据分布将是我们的虚假分布。按照以下步骤完成此练习：
- en: 'Open a new Jupyter Notebook and name it `Exercise 7.02`. Import the following
    library packages:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook，并将其命名为 `练习 7.02`。导入以下库包：
- en: '[PRE16]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In this step, we define the number of input features and output features for
    the network:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步，我们定义网络的输入特征和输出特征的数量：
- en: '[PRE17]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We will have 10 features as input and the output will be two features. The input
    features of `10` are arbitrarily selected. The output features of `2` are selected
    because our real dataset contains two features.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将有 10 个特征作为输入，输出将是两个特征。输入特征 `10` 是随意选择的。输出特征 `2` 是根据我们的真实数据集选择的，因为我们的真实数据集包含两个特征。
- en: 'Now, we will generate a batch of random numbers. Our batch size will be `128`:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将生成一批随机数。我们的批处理大小将为 `128`：
- en: '[PRE18]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can select any batch size. A batch size of `128` is selected so that we take
    cognizance of the computation resources we have. Since the input size is 10, we
    should generate 128 × 10 random numbers. Also, in the preceding code, `randn()`
    is the function to generate random numbers. Inside the function, we specify how
    many data points we want, which is (128 × 10) in our case.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以选择任何批次大小。选择 `128` 作为批次大小，以考虑到我们现有的计算资源。由于输入大小为 10，我们应生成 128 × 10 个随机数。此外，在前面的代码中，`randn()`
    是生成随机数的函数。在该函数内部，我们指定需要多少数据点，在我们的例子中是 (128 × 10)。
- en: 'Let''s reshape the random data into the input format we want using the following code:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用以下代码将随机数据重新塑形为我们想要的输入格式：
- en: '[PRE19]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You should get the following output:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '[PRE20]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In this step, we will define the generator. This network will have six layers:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此步骤中，我们将定义生成器。该网络将包含六层：
- en: '[PRE21]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: From the network, we can see that, in the first layer, we define the dimension
    of the input, which is 10, and in the last layer, we define the output dimension,
    which is 2\. This is based on the input data dimensions that we generated in *Step
    4* (10) and the output features that we want, which is similar to the number of
    features of the real data distribution.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从网络中，我们可以看到，在第一层，我们定义了输入的维度，即 10；而在最后一层，我们定义了输出维度，即 2\。这基于我们在 *步骤 4* 中生成的输入数据维度（10）以及我们想要的输出特征，这与真实数据分布的特征数量相似。
- en: 'We can see the summary of this network by using the `model.summary()` function
    call:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用 `model.summary()` 函数调用查看该网络的概述：
- en: '[PRE22]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You should get the following output:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '![Figure 7.6: Summary of the generator network'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.6：生成器网络概述'
- en: '](img/B15385_07_06.jpg)'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_06.jpg)'
- en: 'Figure 7.6: Summary of the generator network'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.6：生成器网络概述
- en: From the summary, you can see the shapes of the output from each layer. For
    example, the output from the dense layer has a shape of (*size of batch*, `32`)
    since the first hidden layer has `32` neurons. `None` in the shape layer denotes
    the number of examples, which in this case means the input batch size. The figure
    of 352 for the first layer is the size of the parameters, which includes both
    the weights and bias. The weight matrix will have a size of (10 × 32) as the number
    of inputs to the first layer is 10 and the next layer (hidden layer 1) has 32
    neurons. The number of bias will be (32 × 1), which will be equivalent to the
    number of hidden layer neurons in the first layer. So, in total, there are 320
    + 32 = 352 parameters. The second layer would be (32 × 32) + ( 32 × 1) = 1,056
    and so on for all subsequent layers.
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从总结中，你可以看到每一层输出的形状。例如，密集层的输出形状为 (*批次大小*，`32`)，因为第一隐藏层有 `32` 个神经元。形状层中的 `None`
    表示示例的数量，在本例中指的是输入批次大小。第一层的 352 是参数的大小，包括权重和偏差。权重矩阵的大小为 (10 × 32)，因为第一层的输入数量是 10，而下一层（隐藏层
    1）有 32 个神经元。偏差的数量是 (32 × 1)，这将等于第一层中隐藏层神经元的数量。因此，总共有 320 + 32 = 352 个参数。第二层则为
    (32 × 32) + (32 × 1) = 1,056，以此类推，后续层也是如此。
- en: 'Now that we have defined the generator network, let''s generate the output
    from the network. We can do that using the `predict()` function:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了生成器网络，接下来让我们从网络中生成输出。我们可以使用 `predict()` 函数来实现：
- en: '[PRE23]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You should get the following output:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会得到以下输出：
- en: '[PRE24]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We can see that the output from the generator function generates a sample with
    two features and several examples equal to the batch size we have given.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，生成器函数的输出会生成一个具有两个特征的样本，以及与给定批次大小相等的多个示例。
- en: 'Plot the distribution:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制分布图：
- en: '[PRE25]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You should get an output similar to the following. Please note that modeling
    will be stochastic in nature and therefore you might not get the same output:'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到类似于以下的输出。请注意，建模将是随机的，因此你可能不会得到相同的输出：
- en: '![Figure 7.7: Plot of the fake data distribution'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.7：虚假数据分布图'
- en: '](img/B15385_07_07.jpg)'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_07.jpg)'
- en: 'Figure 7.7: Plot of the fake data distribution'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7：虚假数据分布图
- en: As we can see, very random data has been generated. As you will see in upcoming
    exercises, this random data will be transformed so that it looks like the real
    data distribution.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，已经生成了非常随机的数据。正如你将在接下来的练习中看到的，这些随机数据将被转换，以便看起来像真实的数据分布。
- en: Note
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2W0FxyZ](https://packt.live/2W0FxyZ).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该部分的源代码，请参阅 [https://packt.live/2W0FxyZ](https://packt.live/2W0FxyZ)。
- en: You can also run this example online at [https://packt.live/2WhZpOn](https://packt.live/2WhZpOn).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2WhZpOn](https://packt.live/2WhZpOn)上在线运行这个示例。你必须执行整个Notebook才能得到预期的结果。
- en: In this exercise, we defined the generator network, which had six layers and
    then generated the first fake samples from the generator network. You may be wondering
    how we arrived at those six layers. What about the choice of the activation functions?
    Well, the network architecture was arrived at after a lot of experimentation for
    this problem statement. There are no real shortcuts in terms of finding the right
    architecture. We have to arrive at the most optimal architecture after experimenting
    with different parameters such as the number of layers, type of activations, and more.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们定义了一个包含六层的生成器网络，并从生成器网络中生成了第一个假样本。你可能在想我们是如何得到这六层的。那么，激活函数的选择呢？其实，网络架构是在大量实验之后得出的，目的是解决这个问题。找到合适的架构没有真正的捷径。我们必须通过实验不同的参数，如层数、激活函数类型等，来确定最优架构。
- en: Setting the Stage for the Discriminator Network
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为判别器网络奠定基础
- en: 'In the previous exercise, we defined the generator network. Now, it is time
    to set the stage before we define the discriminator network. Looking at the output
    we got from the generator network, we can see that the data points are randomly
    distributed. Let''s take a step back and assess where we are really headed. In
    our introduction to generative networks, we stated that we want the output from
    the generative network to be similar to the real distribution we are trying to
    mimic. In other words, we want the output from the generative network to look
    similar to the output from the real distribution, as shown in the following plot:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的练习中，我们定义了生成器网络。现在，在定义判别器网络之前，是时候为此做一些准备了。看一下我们从生成器网络得到的输出，可以看到数据点是随机分布的。让我们退后一步，评估一下我们真正的方向。在我们介绍生成网络时，我们提到我们希望生成网络的输出与我们试图模仿的真实分布相似。换句话说，我们希望生成网络的输出看起来与真实分布的输出相似，如下图所示：
- en: '![Figure 7.8: Real data distribution'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.8：真实数据分布'
- en: '](img/B15385_07_08.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_08.jpg)'
- en: 'Figure 7.8: Real data distribution'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8：真实数据分布
- en: 'We can see that the current distribution that has been generated by the generator
    network is nowhere near the distribution we want to mimic. Why do you think this
    is happening? Well, the reason is quite obvious; we have not done any training
    yet. You will also have noticed that we don''t have an optimizer function as part
    of the network. The optimizer function in Keras is defined using the `compile()`
    function, as shown in the following code, where we define the type of loss function
    and what kind of optimizers we want to adopt:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，生成器网络当前生成的分布与我们希望模仿的分布相差甚远。你认为这是什么原因呢？嗯，原因很明显；我们还没有进行任何训练。你可能也注意到，网络中没有包含优化器函数。Keras中的优化器函数是通过`compile()`函数定义的，如以下代码所示，其中我们定义了损失函数的类型以及我们希望采用的优化器：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We have excluded the `compile()` function on purpose. Later, when we are introduced
    to the GAN model, we will use the `compile()` function to optimize the generator
    network. So, hang on until then. For now, we will go ahead with the next step
    of the process, which is defining the discriminator network.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们故意排除了`compile()`函数。稍后，当我们介绍GAN模型时，我们将使用`compile()`函数来优化生成器网络。所以，请耐心等待。现在，我们将继续进行过程的下一步，即定义判别器网络。
- en: Process 3 – Discriminator Network
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过程 3 – 判别器网络
- en: In the previous process, we were introduced to the generative network, a neural
    network that generated fake samples. The discriminator network is also another
    neural network, albeit with different functionality from the generator network.
    The purpose of the discriminator function is to identify whether a given example
    is a real one or a fake one. Using an analogy, if the generator network is a conman
    who makes fake currency, then the discriminator network is the super cop who identifies
    that the currency is fake. Once caught by the super cop, the conman will try to
    perfect their craft to make better counterfeits so that they can fool the super
    cop. However, the super cop will also undergo lots of training to know the nuances
    of different currencies and work toward perfecting the craft of catching whatever
    the conman generates. We can see here that both these protagonists are in adversarial
    positions all the time. This is the reason why the network is called a *generative
    adversarial network.*
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的过程中，我们介绍了生成网络，一个生成假样本的神经网络。鉴别器网络也是另一个神经网络，尽管它的功能与生成器网络不同。鉴别器的作用是识别给定样本是真实的还是假的。用一个类比来说，如果生成器网络是一个制造假币的骗子，那么鉴别器网络就是识别假币的超级警察。一旦被超级警察抓住，骗子将尝试完善他们的伎俩，制作更好的伪币，以便能欺骗超级警察。然而，超级警察也会接受大量训练，了解不同货币的细微差别，并努力完善识别假币的技能。我们可以看到，这两个主角始终处于对立的状态。这也是为什么这个网络被称为*生成对抗网络*的原因。
- en: Taking a cue from the preceding analogy, training a discriminator is similar
    to the super cop undergoing more training to identify fake currency. The discriminator
    network is like any binary classifier you would have learned about in machine
    learning. As part of the training process, the discriminator will be provided
    with two classes of examples, one generated from the real distribution and the
    other from the generator distribution. Each of these sets of examples will have
    their respective labels too. The real distribution will have a label of "1", while
    the fake distribution will have a label of "0". The discriminator, after being
    trained, will have to correctly classify whether an example is real or fake, which
    is a typical binary classification problem.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的类比中获得启示，训练鉴别器类似于超级警察经过更多训练来识别假币。鉴别器网络就像你在机器学习中学到的任何二分类器。在训练过程中，鉴别器将提供两类样本，一类来自真实分布，另一类来自生成器分布。每一类样本都会有各自的标签。真实分布的标签是“1”，而假分布的标签是“0”。经过训练后，鉴别器必须正确地分类样本是来自真实分布还是假分布，这是一个典型的二分类问题。
- en: Implementing the Discriminator Network
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现鉴别器网络
- en: 'The core structure of the discriminator network would be similar to the generator
    network we implemented in the previous section. The complete process behind building
    the discriminator network is as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器网络的核心结构与我们在前一部分实现的生成器网络相似。构建鉴别器网络的完整过程如下：
- en: Generate batches of real distribution.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成真实分布的批次。
- en: Using the generator network, it generates batches of fake distribution.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成器网络，生成假分布的批次。
- en: Train the discriminator network with examples of both these distributions. The
    real distribution will have a label of 1, while the fake distribution will have
    a label of 0.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用这两种分布的样本训练鉴别器网络。真实分布的标签是 1，而假分布的标签是 0。
- en: Evaluate the performance of the discriminator.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估鉴别器的性能。
- en: In *Steps 1* and *2*, we have to generate batches of both the real and fake
    distributions. This will necessitate making use of the real distribution we built
    in *Exercise 7.01*, *Generating a Data Distribution from a Known Function,* and
    the generator network we developed in *Exercise 7.02,* *Building a Generative
    Network*. Since we have to use these two distributions, it would be convenient
    to package them into two types of functions to efficiently train the discriminator
    network. Let's look at the two types of functions we will build.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 1* 和 *步骤 2* 中，我们需要生成真实和假分布的批次。这就需要利用我们在*练习 7.01*中构建的真实分布，*从已知函数生成数据分布*，以及我们在*练习
    7.02*中开发的生成器网络，*构建生成网络*。由于我们需要使用这两种分布，因此将它们打包成两种函数类型，以便高效地训练鉴别器网络会更方便。让我们来看看我们将要构建的两种函数类型。
- en: Function to Generate Real Samples
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成真实样本的函数
- en: 'The content of this function, which is used to generate real samples, is the
    same as the code we developed in *Exercise 7.01,* *Generating a Data Distribution
    from a Known Function*. The only notable addition is the label for the input data.
    As we stated earlier, the real samples will have a label of 1\. So, as labels,
    we will generate an array of 1s with the same size as the batch size. There is
    a utility function in `numpy` that can be used to generate a series of 1s called
    `np.ones((batch,1)`. This will generate an array of 1s whose size is equal to
    the batch size. Let''s revisit the different steps in this function:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这个用于生成真实样本的函数内容与我们在 *练习 7.01* 中开发的代码相同，*从已知函数生成数据分布*。唯一需要注意的补充内容是输入数据的标签。正如我们之前所述，真实样本的标签将是
    1。因此，作为标签，我们将生成一个大小与批量大小相同的 1 的数组。`numpy` 中有一个实用函数可以用来生成一系列 1，叫做 `np.ones((batch,1))`。这将生成一个大小等于批量大小的
    1 的数组。我们来回顾一下这个函数中的不同步骤：
- en: Generate equally spaced numbers to the right and left of a random number.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成均匀分布的数字，位于随机数的左右两侧。
- en: Concatenate both sets to get a series that is equal in length to the batch size
    we require. This is our first feature.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两组特征合并，得到一个与我们所需批量大小相等的序列。这是我们的第一个特征。
- en: Generate the second feature by taking the `sine()` function of the first feature
    we generated in *Step 2*.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过对我们在 *步骤 2* 中生成的第一个特征应用 `sine()` 函数来生成第二个特征。
- en: Reshape both features so their size is equal to `(batch,1)` and then concatenate
    them along the columns. This will result in an array of shape `(batch,2)`.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两个特征的形状调整为 `(batch,1)`，然后沿列连接它们。这将得到一个形状为 `(batch,2)` 的数组。
- en: Generate the labels using the `np.ones((batch,1))` function. The label array
    will have a dimension of `(batch,1)`.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `np.ones((batch,1))` 函数生成标签。标签数组的维度将是 `(batch,1)`。
- en: The arguments that we will provide to the function are the random number and
    the batch size. One subtle change to note in *Step 1* is that since we want a
    series equal in length to the batch size, we will take equally spaced numbers
    to the left and right equal to half of the batch size (batch size /2). In this
    way, when we combine both series to the left and right, we get a series equal
    to the batch size we want.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提供给函数的参数是随机数和批量大小。在 *步骤 1* 中需要注意一个细微的变化，由于我们希望生成一个长度等于批量大小的序列，我们将取与批量大小的一半（batch
    size /2）等长的均匀分布的数字。这样，当我们将左右两边的序列组合起来时，我们就得到了一个等于所需批量大小的序列。
- en: Functions to Generate Fake Samples
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成假样本的函数
- en: 'The function(s) to generate fake samples will be the same as what we developed
    in *Exercise 7.02,* *Building a Generative Network*. However, we will have to
    divide this into three separate functions. The reason for dividing the code we
    implemented in *Exercise 7.02,* *Building a Generative Network* into three separate
    functions is for convenience and efficiency during the training process. Let''s
    take a look at these three functions:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 生成假样本的函数将与我们在 *练习 7.02* 中开发的函数相同，*构建生成对抗网络*。不过，我们将把它分成三个独立的函数。将 *练习 7.02* 中的代码分成三个独立的函数是为了在训练过程中提高便利性和效率。我们来看看这三个函数：
- en: '`randn()` function. The output will be an array of size (`batch,input features`).
    The arguments to this function are `batch size` and `input feature size`.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`randn()` 函数。输出将是一个形状为 (`batch,input features`) 的数组。该函数的参数是 `batch size` 和
    `input feature size`。'
- en: '`input feature size` and `output feature size`.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`输入特征大小` 和 `输出特征大小`。'
- en: '`numpy` to generate 0s called `np.zeros((batch,1))`.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `numpy` 生成 0，即 `np.zeros((batch,1))`。
- en: 'Let''s look at the complete process for these three functions:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看这三个函数的完整过程：
- en: Generate fake inputs using *function 1*.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *函数 1* 生成假输入。
- en: Use the generator model function (*function 2*) to predict a fake output.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成器模型函数（*函数 2*）来预测假输出。
- en: Generate labels, which is a series of 0s, using the `np.zeros()` function. This
    is part of *function 3*.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `np.zeros()` 函数生成标签，即一系列 0。这是 *函数 3* 的一部分。
- en: The arguments to the third function are `generator model`, `batch size`, and
    `input feature size`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个函数的参数是 `生成器模型`、`批量大小` 和 `输入特征大小`。
- en: Building the Discriminator Network
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建判别器网络
- en: The discriminator network will be built along the same lines as the generator
    network; that is, it will be created using the `Sequential()` class, the dense
    layer, and the activation and initialization functions. The only notable exception
    is that we will also have the optimization layer in the form of the `compile()`
    function. In the optimization layer, we will define the loss function, which in
    this case will be `binary_crossentropy` as the discriminator network is a binary
    classification network. For the optimizer, we will be using the `adam optimizer`
    as this is found to be very efficient and is a very popular choice.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络将按照与生成器网络相同的方式构建；也就是说，它将使用`Sequential()`类、全连接层、激活函数和初始化函数来创建。唯一值得注意的例外是，我们还将在`compile()`函数中添加优化层。在优化层中，我们将定义损失函数，这里将使用`binary_crossentropy`，因为判别器网络是一个二分类网络。对于优化器，我们将使用`adam
    optimizer`，因为它非常高效且是一个非常流行的选择。
- en: Training the Discriminator Network
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练判别器网络
- en: 'Now that we have gone through all the components for implementing the discriminator
    network, let''s look at the steps involved in training the discriminator network:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了实现判别器网络的所有组件，接下来让我们看看训练判别器网络的步骤：
- en: Generate a random number and then generate a batch of real samples and its labels
    using the function to generate real samples.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机数，然后使用生成真实样本的函数生成一批真实样本及其标签。
- en: Generate a batch of fake samples and its labels using the third function described
    to generate fake samples. The third function will use both the other functions
    to generate the fake samples.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第三个函数生成虚假样本及其标签，该函数将使用前面定义的其他函数来生成虚假样本。
- en: Train the discriminator model using the `train_on_batch()` function with the
    batch of real samples and fake samples.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_on_batch()`函数，用一批真实样本和虚假样本来训练判别器模型。
- en: Steps *1* to *3* are repeated for the number of epochs we want the training
    to run for. This is done through a `for` loop over the number of epochs.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤*1*到*3*会针对我们希望训练运行的轮数重复执行。这是通过一个`for`循环来实现的，循环的次数是训练的轮数。
- en: At every intermediate step, we calculate the accuracy of the model on the fake
    samples and real samples using the `evaluate()` function. The accuracy of the
    model is printed.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个中间步骤中，我们使用`evaluate()`函数计算模型在虚假样本和真实样本上的准确性，并打印出模型的准确度。
- en: Now that we have seen the steps involved in implementing the discriminator network,
    we'll implement this in the next exercise.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了实现判别器网络的步骤，接下来我们将在下一个练习中实现它。
- en: 'Exercise 7.03: Implementing the Discriminator Network'
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.03：实现判别器网络
- en: 'In this exercise, we will build the discriminator network and train the network
    on both the real samples and fake samples. Follow these steps to complete this
    exercise:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将构建判别器网络，并在真实样本和虚假样本上训练该网络。请按照以下步骤完成此练习：
- en: 'Open a new Jupyter Notebook and name it `Exercise 7.03`. Import the following
    library packages:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook，并命名为`Exercise 7.03`。导入以下库包：
- en: '[PRE27]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let''s define a function that will generate the features of our real data distribution.
    The return values of this function will be the real dataset and its label:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一个函数，生成我们真实数据分布的特征。此函数的返回值将是真实数据集及其标签：
- en: '[PRE28]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The function we are defining here comprises code that was used to generate the
    `sine()` wave dataset in *Exercise 7.01*, *Generating a Data Distribution from
    a Known Function*. The inputs to this function are the random number and the batch
    size. Once the random number is provided, the series is generated with the same
    process we followed in *Exercise 7.01*, *Generating a Data Distribution from a
    Known Function*. We also generate the labels for the real data distribution, which
    will be 1\. The final return value will be the two features and the label.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在这里定义的函数包含了在*练习 7.01*中用于生成`sine()`波形数据集的代码，*练习 7.01*是*从已知函数生成数据分布*部分的内容。这个函数的输入是随机数和批次大小。一旦提供了随机数，系列将按照我们在*练习
    7.01*中遵循的相同过程生成。我们还会为真实数据分布生成标签，这些标签为1。最终的返回值将是两个特征和标签。
- en: 'Let''s define a function called `fakeInputs` to generate inputs for the generator
    function (this is *function 1*, which we explained in the *Functions to Generate
    Fake Samples* section):'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一个名为`fakeInputs`的函数，用于为生成器函数生成输入（这是*函数 1*，我们在*生成虚假样本的函数*部分中进行了说明）：
- en: '[PRE29]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In this function, we're generating random numbers in the format we want `([batch
    size , input features])`. This function generates the fake data that was sampled
    from the random distribution as the return value.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个函数中，我们正在生成我们需要的格式的随机数 `([batch size , input features])`。这个函数生成的是从随机分布中采样的伪数据作为返回值。
- en: 'Next, we''ll be defining a function that will return a generator model:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个函数，该函数将返回一个生成器模型：
- en: '[PRE30]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This is the same model that we implemented in *Exercise 7.02,* *Building a Generative
    Network*. The return value for this function will be the generator model.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这与我们在 *练习 7.02* 中实现的模型相同，*构建生成对抗网络*。该函数的返回值将是生成器模型。
- en: 'The following function will be used to create fake samples using the generator model:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下函数将用于使用生成器模型创建伪样本：
- en: '[PRE31]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: In the preceding code, we are implementing *function 3*, which we covered in
    the *Functions to Generate Fake Samples* section. As you can see, we call the
    generator model we defined in *Step 4* as input, along with the batch size and
    the input features. The return values for this function are the fake data that's
    generated, along with its label (`0`).
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上述代码中，我们正在实现 *函数 3*，该函数在 *生成伪样本的函数* 部分中已经介绍过。如你所见，我们调用了在 *步骤 4* 中定义的生成器模型作为输入，同时包括了批次大小和输入特征。这个函数的返回值是生成的伪数据及其标签（`0`）。
- en: 'Now, let''s define the parameters to be used in the functions we have just created:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义在刚刚创建的函数中将使用的参数：
- en: '[PRE32]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s build the discriminator model using the following code:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用以下代码构建判别器模型：
- en: '[PRE33]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The mode of construction for the discriminator model is similar to what we did
    in the generator network. Please note that the activation function for the last
    layer will be a sigmoid as we need a probability regarding whether the output
    is a real network or a fake network.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 判别模型的构建方式类似于我们在生成器网络中所做的。请注意，最后一层的激活函数将是 Sigmoid，因为我们需要一个关于输出是实际网络还是伪网络的概率。
- en: 'Print the summary of the discriminator network:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印判别网络的摘要：
- en: '[PRE34]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You should get the following output:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该获得以下输出：
- en: '![Figure 7.9: Model summary'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.9：模型摘要'
- en: '](img/B15385_07_09.jpg)'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_09.jpg)'
- en: 'Figure 7.9: Model summary'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.9：模型摘要
- en: From the summary, we can see the size of the network based on the architecture
    we defined. We can see that the first three dense layers have 16 neurons each,
    which we defined in *Step 7* when we built the discriminator network. The final
    layer will only have one output as this is a sigmoid layer. This outputs the probability
    of whether the data distribution is real (`1`) or fake (`0`).
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从摘要中，我们可以看到基于我们定义的架构，网络的大小。我们可以看到前面三层全连接层每层有 16 个神经元，这是我们在 *步骤 7* 中构建判别网络时定义的。最后一层只有一个输出，因为这是一个
    Sigmoid 层。它输出的是数据分布是否真实的概率（`1` 表示真实，`0` 表示伪造）。
- en: 'Invoke the generator model function to be used in the training process:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用将在训练过程中使用的生成器模型函数：
- en: '[PRE35]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You should get the following output:'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该获得以下输出：
- en: '![Figure 7.10: Model summary'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.10：模型摘要'
- en: '](img/B15385_07_10.jpg)'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_10.jpg)'
- en: 'Figure 7.10: Model summary'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.10：模型摘要
- en: You will notice that the architecture is the same as what we developed in *Exercise
    7.02,* *Building a Generative Network*.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你会注意到，架构与我们在 *练习 7.02* 中开发的完全相同，*构建生成对抗网络*。
- en: 'Now, we need to define the number of epochs to train the network for, as follows:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要定义训练网络的周期数，如下所示：
- en: '[PRE36]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, let''s start training the discriminator network:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始训练判别网络：
- en: '[PRE37]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Here, we iterate the training of the model on both the real and fake data for
    20,000 epochs. The number of epochs is arrived at after some level of experimentation.
    We should try this out with different values for the number of epochs until we
    get some good accuracy figures. For every 4,000 epochs, we print the accuracy
    of the model on both the real dataset and the fake dataset. The printing frequency
    is arbitrary and is based on the number of plots you want to see to check the
    progress of the training process. After training, you will see that the discriminator
    achieves very good accuracy levels.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对真实数据和伪数据的训练进行 20,000 次周期的迭代。周期数是通过一定的实验得出的。我们应该尝试不同的周期值，直到得到较好的准确度。每经过
    4,000 次周期，我们打印模型在真实数据集和伪数据集上的准确度。打印频率是任意的，基于你希望看到的图表数量，以检查训练进度。训练后，你将看到判别器达到非常好的准确率。
- en: 'You should get an output similar to the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该获得类似于以下的输出：
- en: '[PRE38]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Note
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Since we are working with random values here, the output you get may vary from
    the one you see here. It will also vary with every run.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在这里使用的是随机值，因此你得到的输出可能与这里看到的不同，每次运行结果也会有所不同。
- en: From the accuracy levels, we can see that the discriminator was very good (accuracy
    = 1) at identifying the real dataset initially and shows relatively poor accuracy
    levels for the fake dataset. After around 4,000 epochs, we can see that the discriminator
    has become good at identifying both the fake and real datasets as both the accuracies
    are near 1.0.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 从准确度水平来看，我们可以看到判别器最初在识别真实数据集时非常优秀（准确度 = 1），而对假数据集的准确度较低。经过大约4,000个周期后，我们看到判别器已经能够很好地区分假数据集和真实数据集，因为它们的准确度都接近1.0。
- en: Note
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fe02j3](https://packt.live/3fe02j3).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问这个特定部分的源代码，请参考[https://packt.live/3fe02j3](https://packt.live/3fe02j3)。
- en: You can also run this example online at [https://packt.live/2ZYiYMG](https://packt.live/2ZYiYMG).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问[https://packt.live/2ZYiYMG](https://packt.live/2ZYiYMG)。你必须执行整个Notebook才能获得预期的结果。
- en: In this exercise, we defined different helper functions and also built the discriminator
    function. Finally, we trained the discriminator model on real data and fake data.
    At the end of the training process, we saw that the discriminator learned to discriminate
    between the real dataset and fake dataset really well. Having trained the discriminator
    network, it's now time to move on to the climax, which is building the GAN.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们定义了不同的辅助函数，并构建了判别器函数。最后，我们在真实数据和假数据上训练了判别器模型。在训练过程结束时，我们看到判别器能够非常好地区分真实数据集和假数据集。训练完判别器网络后，接下来就是构建GAN的高潮部分。
- en: Process 4 – Implementing the GAN
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过程 4 – 实现GAN
- en: We have finally arrived at the moment we have been waiting for all this while.
    In the previous three processes, we have been progressively building all the building
    blocks for the GAN, such as the fake data generator, real data generator, generator
    network, and discriminator network. The GAN is, in fact, the integration of all
    these building blocks. The real game in the GAN is the process in which we integrate
    these components with each other. Let's address this right away.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于到了我们一直在等待的时刻。在之前的三个过程中，我们逐步构建了GAN的所有构建模块，如假数据生成器、真实数据生成器、生成器网络和判别器网络。GAN实际上是这些构建模块的整合。GAN中的真正难题是我们如何将这些组件互相整合。我们现在就来处理这个问题。
- en: Integrating All the Building Blocks
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 整合所有构建模块
- en: 'When building the discriminator network, we generated real samples and fake
    samples and fed them to the discriminator during training. The training process
    made the discriminator "smart", which enabled it to correctly identify what is
    fake and what is real. In probability terms, this would mean that when the discriminator
    gets a fake sample, it will predict a probability close to "0" and when the sample
    is real, it will predict a probability close to "1". However, getting the discriminator
    to be smart is not our end objective. Our end objective is to get the generator
    model smart so that it starts generating examples that look like real samples
    and, in the process, fools the discriminator. This can be achieved by training
    the generator and updating its parameters (that is, the weights and bias) to enable
    it to generate samples that look like real samples. However, there is still a
    problem, because in the generator network, we did not include an optimizer step
    and therefore the generator network by itself cannot be trained. The way to get
    around this problem is by building another network (let''s call it **Ganmodel**)
    that connects the generator and discriminator in sequence and then include an
    optimizer function in the new network so that it goes and updates the parameters
    of its constituents when backpropagation happens. In terms of pseudocode, this
    network will look something like this:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建鉴别器网络时，我们生成了真实样本和假样本，并在训练过程中将它们输入到鉴别器中。训练过程使得鉴别器变得“聪明”，从而能够正确地识别什么是假的，什么是真的。从概率的角度来看，这意味着当鉴别器接收到假样本时，它会预测接近“0”的概率，而当样本是真实的时，它会预测接近“1”的概率。然而，使鉴别器变聪明并不是我们的最终目标。我们的最终目标是让生成器模型变聪明，使它开始生成看起来像真实样本的例子，并在此过程中欺骗鉴别器。这可以通过训练生成器并更新其参数（即权重和偏置）来实现，使它能够生成看起来像真实样本的样本。然而，仍然存在一个问题，因为在生成器网络中，我们没有包括优化器步骤，因此生成器网络本身无法进行训练。解决这个问题的方法是构建另一个网络（我们称之为**Ganmodel**），它将生成器和鉴别器按顺序连接起来，然后在新网络中包含一个优化器函数，使其在反向传播时更新其组成部分的参数。用伪代码表示，这个网络大致如下：
- en: '[PRE39]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In this model, the generator model will generate fake samples that are fed into
    the discriminator model, which in turn will then generate a probability as to
    whether the example is fake or real. Based on the label of the example, it will
    have a certain loss that will be propagated through the discriminator to the generator,
    updating the parameters of both the models. In other words, based on the loss,
    the backpropagation algorithm will update each parameter based on the gradient
    of the parameter with respect to the loss. So, this will solve our problem of
    not having defined an optimizer function for the generator.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，生成器模型将生成假样本并输入到鉴别器模型中，鉴别器模型将根据样本生成一个概率，判断该样本是假的还是现实的。根据样本的标签，它会有一定的损失，这个损失会通过鉴别器传播到生成器，更新两个模型的参数。换句话说，基于损失，反向传播算法将根据损失对每个参数的梯度来更新该参数。因此，这将解决我们没有为生成器定义优化器函数的问题。
- en: 'However, there is one more catch to this network. Our discriminator network
    has already been trained and was made really smart when we trained the discriminator
    network separately. We don''t want to train the discriminator model again in this
    new network and make it smarter. This can be solved by defining that we don''t
    want to train the discriminator parameters in the network. With this new change,
    the **Ganmodel** would look as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个网络还有一个问题。我们的鉴别器网络已经训练过，并且在我们单独训练鉴别器网络时已经变得非常聪明。我们不希望在这个新网络中再次训练鉴别器模型并让它变得更聪明。这个问题可以通过定义我们不希望训练鉴别器参数来解决。经过这个新修改，**Ganmodel**将如下所示：
- en: '[PRE40]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: By making `Discmodel.trainable = False`, we're telling the network that we don't
    want to update the parameters of the discriminator network during backpropagation.
    So, the discriminator network will act as a conduit to pass on the error during
    the backpropagation stage to the generator network.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置 `Discmodel.trainable = False`，我们告诉网络在反向传播时不更新鉴别器网络的参数。因此，鉴别器网络将在反向传播阶段充当传递错误的中介，传递给生成器网络。
- en: If you think all our problems have been solved, you are in for a rude awakening.
    We know that when the discriminator model is presented with a fake distribution,
    it will predict the probability to a value very close to `0`. We also know that
    the labels of the fake dataset are also `0`. So, in terms of loss, there would
    be very minimal loss being propagated back to the generator. With such a minuscule
    loss, the subsequent update to the parameters of the generator model will also
    be very minuscule. This will not enable the generator to generate samples that
    are like the real samples. The generator will only be able to learn if a large
    loss is generated and propagated to it so that its parameters are updated in the
    direction of real parameters. So, how do we get the loss to be high? What if,
    instead of defining the labels of the fake samples as `0`, we define them as `1`?
    If we do this, the discriminator model, as usual, will predict a probability close
    to 0 for fake examples. However, we now have a situation where the loss function
    would be large because the labels are 1\. When this large loss function gets propagated
    back to the generator network, the parameters will be updated significantly, which
    will enable it to be smarter. Subsequently, what will happen is the generator
    will start generating samples that look more like the real samples, and they would
    meet our objective.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你认为我们所有的问题都已经解决，那你可能会大吃一惊。我们知道，当判别器模型遇到假分布时，它会将概率预测为接近`0`的值。我们也知道，假数据集的标签也是`0`。所以，从损失的角度来看，传播回生成器的损失会非常小。由于损失非常小，随后的生成器模型参数更新也会非常微小。这将无法使生成器生成类似真实样本的样本。生成器只有在生成了较大损失并将其传播到生成器时，才能学习，从而使得其参数朝着真实参数的方向更新。那么，如何使损失变大呢？如果我们不将假样本的标签定义为`0`，而是将其定义为`1`，会怎么样呢？如果我们这么做，判别器模型像往常一样会预测假样本的概率接近0。然而，现在我们有了一个情况，即损失函数会变大，因为标签是1。当这个大的损失函数被传播回生成器网络时，参数会显著更新，这将使生成器变得更聪明。随后，生成器将开始生成更像真实样本的样本，这也就达到了我们的目标。
- en: 'This concept can be explained with the following figure. Here, we can see that
    at the initial level of training, the probability for the fake data is close to
    zero (`0.01`) and the label that we''ve given for the fake data is `1`. This will
    ensure that we get a large loss that gets backpropagated to the generator network:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念可以通过下图来解释。在这里，我们可以看到，在训练的初始阶段，假数据的概率接近零（`0.01`），而我们为假数据指定的标签是`1`。这将确保我们获得一个较大的损失，这个损失会反向传播到生成器网络：
- en: '![Figure 7.11: GAN process'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.11：GAN 过程'
- en: '](img/B15385_07_11.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_11.jpg)'
- en: 'Figure 7.11: GAN process'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：GAN 过程
- en: Now that we have seen the dynamics of the GAN model, let's tie all the pieces
    together to define the process we will follow in order to build the GAN.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过了GAN模型的动态，接下来我们将把所有部分结合起来，定义我们将遵循的过程，以构建GAN。
- en: Process for Building the GAN
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建GAN的过程
- en: 'The complete process for the GAN is all about tying together the pieces we
    have built into a logical order. We will use all the functions we built when we
    defined the discriminator function. In addition, we will also make new functions;
    for instance, a function for the discriminator network and another function for
    the GAN model. All these functions will be called at specific points to make the
    GAN model. The end-to-end process will be as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的完整过程就是将我们构建的所有部分结合成一个合乎逻辑的顺序。我们将使用我们在定义判别器函数时构建的所有函数。此外，我们还将创建新的函数，例如用于判别器网络的函数和用于GAN模型的另一个函数。所有这些函数将在特定的时刻被调用，以构建GAN模型。端到端的过程将如下所示：
- en: Define the function to generate a real data distribution. This function is the
    same function we developed in *Exercise 7.03*, *Implementing the Discriminator
    Network* for the discriminator network.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成真实数据分布的函数。这个函数与我们在*练习 7.03*中为判别器网络开发的**判别器网络实现**函数相同。
- en: Define the three functions that were created for generating fake samples. These
    are a function for generating fake inputs, a function for the generator network,
    and a function for generating fake samples and labels. All these functions are
    the same as the ones we developed in *Exercise 7.03*, *Implementing the Discriminator
    Network* for the discriminator network.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义为生成假样本创建的三个函数。这些函数包括用于生成假输入的函数、用于生成器网络的函数，以及用于生成假样本和标签的函数。这些函数与我们在*练习 7.03*中为判别器网络开发的**判别器网络实现**函数相同。
- en: Create a new function for the discriminator network, just like we created in
    *Exercise 7.03*, *Implementing the Discriminator Network*. This function will
    have the output features (2) as its input as both the real dataset and fake dataset
    have two features. This function will return the discriminator model.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的判别器网络函数，就像我们在*Exercise 7.03*中创建的*判别器网络的实现*一样。此函数将以输出特征（2）作为输入，因为真实数据集和假数据集都有两个特征。此函数将返回判别器模型。
- en: Create a new function for the GAN model as per the pseudocode we developed in
    the previous section (*Process 4 – Building a GAN*). This function will have the
    generator model and the discriminator model as its inputs.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照我们在上一节中开发的伪代码，创建一个新的GAN模型函数（*过程 4 – 构建 GAN*）。此函数将以生成器模型和判别器模型作为输入。
- en: Start the training process.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动训练过程。
- en: The Training Process
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练过程
- en: 'The training process here is similar to the process we implemented in *Exercise
    7.03*, *Implementing the Discriminator Network* for the discriminator network.
    The steps for the training process are as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的训练过程类似于我们在*Exercise 7.03*中实现的*判别器网络的实现*，用于判别器网络的训练过程。训练过程的步骤如下：
- en: Generate a random number and then generate a batch of real samples and its labels
    using the function to generate real samples.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机数，然后使用生成真实样本的函数生成一批真实样本及其标签。
- en: Generate a batch of fake samples and its labels using the third function we
    described regarding the functions for generating fake samples. The third function
    will use both the other functions to generate the fake samples.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们描述的关于生成假样本的第三个函数，生成一批假样本及其标签。第三个函数将使用其他函数来生成假样本。
- en: Train the discriminator model using the `train_on_batch()` function using the
    batch of real samples and fake samples.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_on_batch()`函数训练判别器模型，使用真实样本和假样本的批次。
- en: Generate another batch of fake inputs to train the GAN model. These fake samples
    are generated using *function 1* in the fake sample generation process.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成另一批假输入来训练GAN模型。这些假样本是通过假样本生成过程中的*函数 1*生成的。
- en: Generate the labels for the fake samples that are intended to fool the discriminator.
    These labels will be 1s instead of 0s.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为假样本生成标签，目的是欺骗判别器。这些标签将是1，而不是0。
- en: Train the GAN model using the `train_on_batch()` function using the fake samples
    and its labels, as described in *Steps 4* and *5*.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_on_batch()`函数训练GAN模型，使用假样本及其标签，如*步骤 4*和*5*中所述。
- en: '*Steps 1* to *6* are repeated for the number of epochs we want the training
    to run for. This is done through a `for` loop over the number of epochs.'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤 1* 到 *6* 会根据我们希望训练运行的周期数重复。这是通过在周期数上使用`for`循环完成的。'
- en: At every intermediate step, we calculate the accuracy of the model on the fake
    samples and real samples using the `evaluate()` function. The accuracy of the
    model is also printed.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个中间步骤中，我们使用`evaluate()`函数计算模型在假样本和真实样本上的准确性。模型的准确度也会打印出来。
- en: We also generate output plots at certain epochs.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还会在某些时期生成输出图。
- en: Now that we have seen the complete process behind training a GAN, let's dive
    into *Exercise 7.04*, *Implementing the GAN*, which implements this process.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了训练GAN的完整过程，让我们深入到*Exercise 7.04*，*实现GAN*，并实现这一过程。
- en: 'Exercise 7.04: Implementing the GAN'
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.04：实现 GAN
- en: 'In this exercise, we will build and train the GAN by implementing the process
    we discussed in the previous section. Follow these steps to complete this exercise:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将通过实现我们在上一节讨论的过程来构建和训练GAN。按照以下步骤完成此练习：
- en: 'Open a new Jupyter Notebook and name it `Exercise 7.04`. Import the following
    library packages:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook 并命名为`Exercise 7.04`。导入以下库：
- en: '[PRE41]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let''s create a function to generate the real samples:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个函数来生成真实样本：
- en: '[PRE42]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The function we're creating here follows the same process we implemented in
    *Exercise 7.01*, *Generating a Data Distribution from a Known Function*. The inputs
    to this function are the random number and the batch size. We get the real data
    distribution with both our features, along with the label for the real data distribution
    as return values, from this function. The return values from this function are
    the real dataset and its label.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在这里创建的函数遵循与我们在*Exercise 7.01*中实现的*从已知函数生成数据分布*相同的过程。此函数的输入是随机数和批次大小。我们从此函数中获得包含我们特征的真实数据分布，以及真实数据分布的标签作为返回值。此函数的返回值是实际数据集及其标签。
- en: 'Here, let''s define the function to generate inputs for the generator network:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，让我们定义一个生成输入给生成器网络的函数：
- en: '[PRE43]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This function generates the fake data that was sampled from the random distribution
    as output.
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数生成从随机分布采样的虚假数据作为输出。
- en: 'Now, let''s go ahead and define the function for building the generator network:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们继续定义构建生成器网络的函数：
- en: '[PRE44]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: This is the same function we built in *Exercise 7.02,* *Building a Generative
    Network*. This function returns the generator model.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是我们在*练习 7.02*，*构建生成网络*中构建的相同函数。此函数返回生成器模型。
- en: 'In this step, we will define the function that will create fake samples using
    the generator network:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此步骤中，我们将定义一个函数，使用生成器网络创建虚假样本：
- en: '[PRE45]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The function we are defining here takes the random data distribution as input
    (to the generator network we defined in the previous step) and generates the fake
    distribution. The label for the fake distribution, which is 0, is also generated
    within the function. In other words, the outputs from this function are the fake
    dataset and its label.
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在这里定义的函数将随机数据分布作为输入（传递给我们在前一步定义的生成器网络），并生成虚假分布。虚假分布的标签是0，它也在函数内部生成。换句话说，来自此函数的输出是虚假数据集及其标签。
- en: 'Now, let''s define the parameters that we will be using within the different functions:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义将在不同函数中使用的参数：
- en: '[PRE46]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, let''s build the discriminator model as a function:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们将鉴别器模型构建为一个函数：
- en: '[PRE47]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The network architecture will be like the one we developed in *Exercise 7.03*,
    *Implementing the Discriminator Network*. This function will return the discriminator.
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网络架构将类似于我们在*练习 7.03*，*实现鉴别器网络*中开发的架构。此函数将返回鉴别器。
- en: 'Print the summary of the discriminator network:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印鉴别器网络的摘要：
- en: '[PRE48]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You should get the following output:'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 7.12: Discriminator model summary'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.12：鉴别器模型摘要'
- en: '](img/B15385_07_12.jpg)'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_12.jpg)'
- en: 'Figure 7.12: Discriminator model summary'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.12：鉴别器模型摘要
- en: This output is the same as the one we received for the network we implemented
    in *Exercise 7.03*, *Implementing the Discriminator Network*, where we defined
    the discriminator function.
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该输出与我们在*练习 7.03*，*实现鉴别器网络*中实现的网络所收到的输出相同，我们在其中定义了鉴别器函数。
- en: 'Invoke the generator model function for use in the training process:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用生成器模型函数以供训练过程使用：
- en: '[PRE49]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You should get the following output:'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 7.13: Generator model summary'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.13：生成器模型摘要'
- en: '](img/B15385_07_13.jpg)'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_13.jpg)'
- en: 'Figure 7.13: Generator model summary'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.13：生成器模型摘要
- en: You will notice that the architecture is the same as what we developed in *Exercise
    7.02,* *Building a Generative Network*.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你会注意到，架构与我们在*练习 7.02*，*构建生成网络*中开发的相同。
- en: 'Before we begin training, let''s visualize the fake data distribution. For
    this, we generate the fake dataset using the `fakedataGenerator()` function and
    then visualize it using `pyplot`:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始训练之前，让我们可视化虚假数据分布。为此，我们使用`fakedataGenerator()`函数生成虚假数据集，然后使用`pyplot`可视化它：
- en: '[PRE50]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You should get an output similar to the following. Please note that data generation
    is stochastic in nature (random) and that you might not get the same plot:'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到类似于以下的输出。请注意，数据生成本质上是随机的，因此你可能不会得到相同的图：
- en: '![Figure 7.14: Plot from the fake input distribution'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.14：来自虚假输入分布的图'
- en: '](img/B15385_07_14.jpg)'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_14.jpg)'
- en: 'Figure 7.14: Plot from the fake input distribution'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.14：来自虚假输入分布的图
- en: From the preceding plot, you can see that the data distribution is quite random.
    We need to convert this random data into a form similar to the sine wave, which
    was our real data distribution.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从前面的图表中可以看到，数据分布相当随机。我们需要将这些随机数据转换为类似于正弦波的形式，这才是我们的真实数据分布。
- en: 'Now, let''s define the GAN model as a function. This function is similar to
    the pseudocode we developed in *Process 4*, where we defined the GAN. The GAN
    is a wrapper model around the generator model and the discriminator model. Please
    note that we define the discriminator model as **not trainable** within this function:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将GAN模型定义为一个函数。该函数类似于我们在*过程 4*中开发的伪代码，在那里我们定义了GAN。GAN是生成器模型和鉴别器模型的包装模型。请注意，我们在此函数中将鉴别器模型定义为**不可训练**：
- en: '[PRE51]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This function will return the GAN model.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数将返回GAN模型。
- en: 'Now, let''s invoke the GAN function. Please note that the inputs to the GAN
    model are the previously defined generator model and the discriminator model:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们调用GAN函数。请注意，GAN模型的输入是先前定义的生成器模型和判别器模型：
- en: '[PRE52]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Print the summary of the GAN model:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印GAN模型的总结：
- en: '[PRE53]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'You should get the following output:'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 7.15: Summary of the GAN model'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.15：GAN模型的总结'
- en: '](img/B15385_07_15.jpg)'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_15.jpg)'
- en: 'Figure 7.15: Summary of the GAN model'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.15：GAN模型的总结
- en: Note that the parameters of each layer of the GAN model are equivalent to the
    parameters of the generator and discriminator models. The GAN model is just a
    wrapper around these two models we defined earlier.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，GAN模型中每一层的参数等同于生成器和判别器模型的参数。GAN模型仅仅是对这两个模型的包装。
- en: 'Let''s define the number of epochs to train the network:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义训练网络的轮数（epochs）：
- en: '[PRE54]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, we start the process of training the network:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们开始训练网络的过程：
- en: '[PRE55]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: It needs to be noted here that the training of the discriminator model with
    the fake and real samples and the training of the GAN model happens concurrently.
    The only difference is that training the GAN model proceeds without updating the
    parameters of the discriminator model. The other thing to note is that, inside
    the GAN, the labels for the fake samples would be 1\. This is to generate large
    loss terms that will be backpropagated through the discriminator network to update
    the generator parameters.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，判别器模型与假样本和真实样本的训练，以及GAN模型的训练是同时进行的。唯一的区别是，训练GAN模型时不会更新判别器模型的参数。另一个需要注意的是，在GAN内部，假样本的标签将为1。这是为了生成大的损失项，这些损失项会通过判别器网络进行反向传播，从而更新生成器的参数。
- en: 'Note:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: 'Please note that the third line of code from the bottom (`filename = ''GAN_Training_Plot%03d.png''
    % (i)`) saves a plot once every 2,000 epochs. The plots will be saved in the same
    folder that your Jupyter Notebook is located in. You can also specify the path
    you want to save the plots at. This can be done as follows:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，底部倒数第三行代码（`filename = 'GAN_Training_Plot%03d.png' % (i)`）每隔2,000个轮次保存一次图表。图表将保存在与你的Jupyter
    Notebook文件位于同一文件夹中。你也可以指定保存图表的路径。可以通过以下方式完成：
- en: '`filename = ''D:/Project/GAN_Training_Plot%03d.png'' % (i)`'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '`filename = ''D:/Project/GAN_Training_Plot%03d.png'' % (i)`'
- en: You can access the plots that were generated through this exercise at [https://packt.live/2W1FjaI](https://packt.live/2W1FjaI).
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过[https://packt.live/2W1FjaI](https://packt.live/2W1FjaI)访问通过本练习生成的图表。
- en: 'You should get an output similar to the one shown here. Since the predictions
    are stochastic in nature (that is to say, they''re random), you might not get
    the same plots shown in this example. Your values may vary; however, they will
    be similar to what''s shown here:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到类似以下所示的输出。由于预测是随机的（也就是说，它们是随机的），你可能无法得到与本示例中相同的图表。你的值可能会有所不同；然而，它们会与这里显示的结果相似：
- en: '[PRE56]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'From the preceding output, you can see that the real dataset accuracy levels
    are progressively going down and that the fake dataset''s accuracy is going up.
    In ideal situations, the accuracy of the discriminator network has to be around
    the 0.5 level, which indicates that the discriminator is really confused as to
    whether a sample is fake or real. Now, let''s look at some of the plots that were
    generated at different epoch levels as to how the data points are converging to
    look like the real function. The following plot is the distribution of the random
    data point before it was fed into the GAN (*Step 10*):'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出可以看出，真实数据集的准确率逐渐下降，而假数据集的准确率则在上升。在理想情况下，判别器网络的准确率应该在0.5左右，这意味着判别器对于一个样本是假的还是现实的感到非常困惑。现在，让我们看看在不同轮次下生成的一些图表，了解数据点如何收敛并趋近于真实的函数。以下是输入到生成对抗网络（GAN）之前的随机数据点的分布图（*步骤10*）：
- en: '![Figure 7.16: Plot from the fake input distribution'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.16：来自假输入分布的图表'
- en: '](img/B15385_07_16.jpg)'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_16.jpg)'
- en: 'Figure 7.16: Plot from the fake input distribution'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16：来自假输入分布的图表
- en: Notice the distribution of the data where the data points are mostly centered
    on a mean of 0\. This is because the random points are generated from a normal
    distribution that has a mean of 0 and a standard deviation of 1\. Now, using the
    raw data, let's study the progression of the fake dataset as the generator is
    trained.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 注意数据的分布，数据点大多集中在均值为0的地方。这是因为这些随机点是从均值为0、标准差为1的正态分布中生成的。现在，使用这些原始数据，让我们研究随着生成器训练，假数据集的变化过程。
- en: Note
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3iIJHVS](https://packt.live/3iIJHVS).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此部分的源代码，请参考[https://packt.live/3iIJHVS](https://packt.live/3iIJHVS)。
- en: You can also run this example online at [https://packt.live/3gF5DPW](https://packt.live/3gF5DPW).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个例子，网址是[https://packt.live/3gF5DPW](https://packt.live/3gF5DPW)。你必须执行整个Notebook才能获得期望的结果。
- en: The three plots shown below map the progression of the fake data distribution
    vis-a-vis the real data distribution. The *x* axis represents feature 1, while
    the *y* axis represents feature 2\. In the plots, the red points pertain to the
    data from the real distribution and the blue plots pertain to the data from the
    fake distribution. From the following plot, we can see that at epoch `2000`, the
    fake plots are within the domain; however, they are not aligned to the shape of
    the real data distribution.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 以下三个图展示了虚假数据分布与真实数据分布的进展情况。*x*轴表示特征1，而*y*轴表示特征2。在图中，红色点表示真实数据分布的数据，蓝色点表示虚假数据分布的数据。从以下图中可以看到，在`2000`代时，虚假数据的分布已经进入了该范围，但仍未与真实数据分布的形态对齐。
- en: '![Figure 7.17: Plot of fake data distribution vis-à-vis the real data distribution
    at epoch 2000'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.17：在`2000`代时，虚假数据分布与真实数据分布的对比图'
- en: '](img/B15385_07_17.jpg)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_17.jpg)'
- en: 'Figure 7.17: Plot of fake data distribution vis-à-vis the real data distribution
    at epoch 2000'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17：在`2000`代时，虚假数据分布与真实数据分布的对比图
- en: 'By epoch `10000`, which is when the generator has been trained almost halfway,
    there is a consolidation nearer to the real data distribution:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 到了`10000`代时，即生成器已经训练到一半左右，数据已经趋向与真实数据分布的汇聚：
- en: '![Figure 7.18: Plot of fake data distribution vis-à-vis the real data distribution
    at epoch 10000'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.18：在`10000`代时，虚假数据分布与真实数据分布的对比图'
- en: '](img/B15385_07_18.jpg)'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_18.jpg)'
- en: 'Figure 7.18: Plot of fake data distribution vis-à-vis the real data distribution
    at epoch 10000'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18：在`10000`代时，虚假数据分布与真实数据分布的对比图
- en: By epoch `18000`, we can see that most of the points are aligned to the real
    data distribution, which is an indicator that the GAN has been trained reasonably
    well.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 到了`18000`代时，我们可以看到大多数数据点已经与真实数据分布对齐，这表明GAN模型的训练效果相当不错。
- en: '![Figure 7.19: Plot of fake data distribution vis-à-vis the real data distribution
    at epoch 18000'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.19：在`18000`代时，虚假数据分布与真实数据分布的对比图'
- en: '](img/B15385_07_19.jpg)'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_19.jpg)'
- en: 'Figure 7.19: Plot of fake data distribution vis-à-vis the real data distribution
    at epoch 18000'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19：在`18000`代时，虚假数据分布与真实数据分布的对比图
- en: However, you can see that the data points after `x = 4` have a lot more noise
    than the ones on the left. One reason for this could be the random data distribution
    we generated before we trained the *GAN(Step 10)* contains data that is distributed
    predominantly between `-2` and `4`. Such data is aligning well to the target distribution
    (sine wave) within the same range and is a little wobbly around the target distribution
    to the right of `x = 4`. However, you should also note that getting 100% alignment
    to the target distribution is an extremely difficult proposition that would involve
    experimenting with different model architectures and more experiments. We encourage
    you to experiment and be innovative with different components within the architecture
    to get the distribution more aligned.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以看到在`x = 4`之后的数据点比左边的更多噪声。造成这种情况的原因之一可能是我们在训练*GAN（第10步）*之前生成的随机数据分布，数据主要集中在`-2`和`4`之间。这些数据与目标分布（正弦波）在相同范围内对齐，并且在`x
    = 4`右侧目标分布的对齐稍显不稳定。然而，你还应注意，完全对齐目标分布是一个极其困难的任务，涉及到不同模型架构的实验以及更多的实验。我们鼓励你在架构的不同组件中进行实验和创新，以使分布更好地对齐。
- en: Note
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The results we have gotten in the above exercise will vary every time we run
    the code.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上述实验中获得的结果每次运行代码时都会有所不同。
- en: This brings us to the end of the complete process of progressively building
    a GAN. Through a series of exercises, we have learned what a GAN is, its constituents,
    and how all of them are tied together to train a GAN. We will take what we've
    learned forward and develop more advanced GANs using different datasets.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们逐步构建GAN的完整过程的结束。通过一系列的实验，我们了解了GAN的定义、组成部分，以及它们如何协同工作来训练一个GAN。接下来，我们将运用所学的知识，利用不同的数据集开发更先进的GAN。
- en: Deep Convolutional GANs
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积GAN
- en: In the previous sections, where we implemented a GAN, we made use of an architecture
    based on the **Multi-Layer Perceptron** (**MLP**). As you may recall from the
    previous chapters, MLPs have fully connected layers. This implies that all the
    neurons in each layer have connections to all the neurons of the subsequent layer.
    For this reason, MLPs are also called fully connected layers. The GAN that we
    developed in the previous section can also be called a **Fully Connected GAN**
    (**FCGAN**). In this section, we will learn about another architecture called
    **Deep Convolutional GANs** (**DCGANS**). As the name implies, this is based on
    the **Convolutional Neural Network** (**CNN**) architecture that you learned about
    in *Chapter 4*, *Deep Learning for Text – Embeddings*. Let's revisit some of the
    building blocks of DCGANs.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们实现了一个基于**多层感知器**（**MLP**）的生成对抗网络（GAN）。如你在前几章中所学，MLP具有全连接层，这意味着每一层的所有神经元都与下一层的所有神经元相连接。因此，MLP也被称为全连接层。我们在前一节开发的GAN也可以称为**全连接生成对抗网络**（**FCGAN**）。在本节中，我们将学习另一种架构，称为**深度卷积生成对抗网络**（**DCGANs**）。顾名思义，这种架构基于你在*第4章*《文本深度学习——嵌入》中学到的**卷积神经网络**（**CNN**）。让我们重新回顾一下DCGAN的一些基本构建块。
- en: Building Blocks of DCGANs
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DCGAN的构建块
- en: Most of the building blocks of DCGANs are similar to what you learned about
    when you were introduced to CNNs in *Chapter 3*, *Image Classification with Convolutional
    Neural Networks*. Let's revisit some of the important ones.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN的大多数构建块与在*第3章*《卷积神经网络的图像分类》中引入CNN时学到的类似。让我们回顾一下其中的一些重要内容。
- en: '**Convolutional Layers**'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积层**'
- en: 'As you learned in *Chapter 3*, *Image Classification with Convolutional Neural
    Networks*, convolutional operations involve filters or kernels moving over the
    input image to generate a set of feature maps. The convolutional layer can be
    implemented in Keras using the following line of code:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在*第3章*《卷积神经网络的图像分类》中所学，卷积操作涉及过滤器或卷积核在输入图像上移动，以生成一组特征图。在Keras中，卷积层可以通过以下代码实现：
- en: '[PRE57]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The above code block is solely meant to explain how the code is implemented.
    It may not result in a desirable output when run in its current form. For now,
    try to understand the syntax completely; we will be putting this code into practice
    soon.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码块仅用于解释代码是如何实现的。以当前的形式运行时，可能不会得到理想的输出。目前，尽量完全理解语法，我们很快将把这段代码付诸实践。
- en: In the first part of the preceding code, the `Sequential()` class is imported
    from the `tensorflow.keras` module. It is then instantiated to a variable model
    in the second line of code. The convolutional layer is added to the `Sequential()`
    class by defining the number of filters, kernel size, the required strides, and
    the padding indicators. In the preceding line of code, 64 indicates the number
    of feature maps. A `kernel_size` value of `(5,5)` indicates the size of the filters
    that will be convolved over the input image to generate the feature maps. The
    `strides` value of `(2,2)` indicates that the filters will move two cells at a
    time, both horizontally and vertically, in the process of generating the feature
    maps. `padding = 'same'` indicates that we want the output of the convolutional
    operation to be of the same size as the input.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`Sequential()`类是从`tensorflow.keras`模块中导入的。然后，在第二行代码中将其实例化为一个变量model。通过定义过滤器的数量、核的大小、所需的步幅和填充指示器，将卷积层添加到`Sequential()`类中。在前面的代码行中，64表示特征图的数量。`kernel_size`值为`(5,5)`表示将用于卷积输入图像并生成特征图的过滤器的大小。`strides`值为`(2,2)`表示在生成特征图的过程中，过滤器每次水平和垂直地移动两个单元格。`padding
    = 'same'`表示我们希望卷积操作的输出与输入具有相同的大小。
- en: 'Note:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: The choice of architecture to use, such as the number of filters, size of kernels,
    stride, and more, is an art and can be mastered with lots of experimentation on
    the domain.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 选择使用的架构，例如过滤器的数量、卷积核的大小、步幅等，是一种艺术形式，可以通过大量的实验在特定领域中掌握。
- en: '**Activation Functions**'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**'
- en: In the previous section, we implemented some activation functions such as ReLU,
    ELU, SELU, and linear. In this section, we will be introduced to another activation
    function called LeakyReLU. LeakyReLU is another variation of ReLU. Unlike ReLU,
    which doesn't allow any negative values, LeakyReLU allows a small non-zero gradient
    that is controlled by a factor, `α`. This factor, `α`, controls the slope of the
    gradient for the negative values.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们实现了一些激活函数，如 ReLU、ELU、SELU 和线性激活函数。在本节中，我们将介绍另一种叫做 LeakyReLU 的激活函数。LeakyReLU
    是 ReLU 的另一种变体。与 ReLU 不允许任何负值不同，LeakyReLU 允许一个由因子`α`控制的小的非零梯度。这个因子`α`控制负值的梯度斜率。
- en: '**Upsampling Operation**'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '**上采样操作**'
- en: In a CNN, an image gets down-sampled to lower dimensions by operations such
    as max pooling and convolutional operations. However, in a GAN, the dynamics of
    a generator network operate in a direction opposite to the convolutional operation;
    that is, from lower or coarser dimensions, we have to transform an image to a
    denser form (that is, with more dimensions). One way to do that is through an
    operation called `UpSampling`. In this operation, the input dimensions are doubled.
    Let's understand this operation in more detail using a small example.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CNN 中，图像通过最大池化和卷积操作等方式被下采样到更低的维度。然而，在 GAN 中，生成器网络的动态运作方向与卷积操作相反；也就是说，从较低或较粗的维度开始，我们需要将图像转换为更密集的形式（即，更多的维度）。一种实现方法是通过一种叫做`UpSampling`的操作。在此操作中，输入维度被加倍。让我们通过一个小例子来更详细地了解这个操作。
- en: 'The following code can be used to import the required library files. The function
    that''s specific for `UpSampling` is `UpSampling2D` from `keras.layers`:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码可用于导入所需的库文件。专门用于`UpSampling`的函数是`keras.layers`中的`UpSampling2D`：
- en: '[PRE58]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The following code creates a simple model that takes an array of shape `(3,3,1)`
    as input in the `UpSampling` layer:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码创建了一个简单的模型，在`UpSampling`层中接受形状为`(3,3,1)`的数组作为输入：
- en: '[PRE59]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output will be as follows:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 7.20: Model summary for UpSampling2D'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.20：UpSampling2D 模型摘要'
- en: '](img/B15385_07_20.jpg)'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_20.jpg)'
- en: 'Figure 7.20: Model summary for UpSampling2D'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20：UpSampling2D 模型摘要
- en: 'From the summary, we can see that the output has been doubled to `(None, 6,6,1)`,
    wherein the middle two dimensions have been doubled. To understand what change
    this makes to an array of shape `(3,3,1)`, we will need to define an array of
    size `(3,3)`, as follows:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 从摘要中，我们可以看到输出已经被加倍至`(None, 6,6,1)`，其中中间的两个维度被加倍。为了理解这一变化如何影响形状为`(3,3,1)`的数组，我们需要定义一个`(3,3)`大小的数组，如下所示：
- en: '[PRE60]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output will be as follows:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE61]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The array we''ve defined has only two dimensions. However, the input to the
    model we defined needs four dimensions, where the dimensions are in the order
    (`examples, width, height, channels`). We can create the additional dimensions
    using the `reshape()` function, as follows:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的数组只有两个维度。然而，模型输入需要四个维度，维度的顺序是（`examples, width, height, channels`）。我们可以使用`reshape()`函数来创建额外的维度，如下所示：
- en: '[PRE62]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output will be as follows:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE63]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We can use the following code to make some predictions with the `UpSampling`
    model we created and observe the dimensions of the resultant array:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码通过我们创建的`UpSampling`模型进行一些预测，并观察结果数组的维度：
- en: '[PRE64]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The output will be as follows:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 7.21: Output shape of the unsampled model'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.21：未采样模型的输出形状'
- en: '](img/B15385_07_21.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_21.jpg)'
- en: 'Figure 7.21: Output shape of the unsampled model'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.21：未采样模型的输出形状
- en: From the preceding output, we can see how the resultant array has been transformed.
    As we can see, each of the inputs has been doubled to get the resultant array.
    We will be using the `UpSampling` method in *Exercise 7.05*, *Implementing the DCGAN*.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以看到结果数组是如何被转换的。正如我们所见，每个输入都被加倍以得到结果数组。我们将在*练习 7.05*中使用`UpSampling`方法，*实现
    DCGAN*。
- en: '**Transpose Convolution**'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '**转置卷积**'
- en: Transpose convolution is different from the `UpSampling` method we just saw.
    `UpSampling` was more or less a naïve doubling of the input values. However, transpose
    convolutions have weights that are learned during the training phase. Transpose
    convolutions work similarly to convolutional operations but in reverse. Instead
    of reducing the dimensions, transpose convolutions expand the dimensions of the
    input through a combination of the kernel size and its strides. As learned in
    *Chapter 3*, *Image Processing with Convolutional Neural Networks*, strides are
    the step sizes where we convolve or move the filters over the image to get an
    output. We also control the output of transpose convolutions with the `padding
    = 'same'` parameter, just like we do in convolutional operations.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积与我们刚才看到的`UpSampling`方法不同。`UpSampling`更多的是输入值的简单翻倍。然而，转置卷积在训练阶段学习到的权重。转置卷积的工作方式类似于卷积操作，但方向相反。转置卷积通过核大小和步幅的组合来扩展输入的维度，而不是减小维度。正如*第3章*《*卷积神经网络的图像处理*》中所学，步幅是我们卷积或移动过滤器以获得输出时的步长。我们还可以通过`padding
    = 'same'`参数来控制转置卷积的输出，就像在卷积操作中一样。
- en: Let's take a look at a code example of how transpose convolutions work.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个代码示例，了解转置卷积是如何工作的。
- en: 'First, we will need to import the necessary library files. The function that''s
    specific to transpose convolution operations is `Conv2DTranspose` from `keras.layers`:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要导入必要的库文件。与转置卷积操作相关的函数是`keras.layers`中的`Conv2DTranspose`：
- en: '[PRE65]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now, we can create a simple model that takes an image of shape `(3,3,1)` in
    the transpose convolution layer:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个简单的模型，在转置卷积层中处理形状为`(3,3,1)`的图像：
- en: '[PRE66]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'In the transpose convolution layer, the first parameter `(1)` is the number
    of filters. The second one `(4,4)` is the size of kernel and the last one `(2,2)`
    is the strides. With `padding = ''same''`, the output will not be dependent on
    the size of the kernel but will be multiples of the stride and the input dimension.
    The summary that will be generated by the preceding code will be as follows:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在转置卷积层中，第一个参数`(1)`是滤波器的数量。第二个参数`(4,4)`是核的大小，最后一个参数`(2,2)`是步幅。使用`padding = 'same'`时，输出将不依赖于核的大小，而是步幅和输入维度的倍数。前面的代码生成的摘要如下：
- en: '![Figure 7.22: Summary of the model'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.22：模型摘要](img/B15385_07_22.jpg)'
- en: '](img/B15385_07_22.jpg)'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_22.jpg)'
- en: 'Figure 7.22: Summary of the model'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.22：模型摘要
- en: From the summary, we can see that the output has been doubled to `(None, 6,6,1)`,
    which would work like the multiplying the strides by the input dimensions (None,
    2 × 3, 2 × 3, 1).
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 从摘要中，我们可以看到输出已经翻倍为`(None, 6,6,1)`，这就像是将步幅与输入维度相乘一样（None, 2 × 3, 2 × 3, 1）。
- en: 'Now, let''s see what changes occur to a real array of shape `(1,3,3,1)`. Remember
    that we also created this array earlier:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个形状为`(1,3,3,1)`的实际数组发生了什么变化。记住，我们之前也创建过这个数组：
- en: '[PRE67]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output is as follows:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE68]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'To generate the transposed array, we need to make some predictions using the
    transpose convolution model we created. By printing the shape, we can also observe
    the dimensions of the resultant array:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成转置数组，我们需要使用我们创建的转置卷积模型进行一些预测。通过打印形状，我们还可以观察到结果数组的维度：
- en: '[PRE69]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The output will be as follows:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.23: Transformed array'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.23：变换后的数组](img/B15385_07_23.jpg)'
- en: '](img/B15385_07_23.jpg)'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_22.jpg)'
- en: 'Figure 7.23: Transformed array'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.23：变换后的数组
- en: Note
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The output you get may vary from the one we have shown above.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 你得到的输出可能与我们上面展示的不同。
- en: From the preceding output, we can see how the resultant array has been transformed.
    The values in the generated array are the end result of the dynamics between the
    weights of the kernel on the input image.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以看到生成的数组是如何被转化的。生成数组中的值是内核权重与输入图像之间动态关系的最终结果。
- en: Now that we have seen some of the basic building blocks of a DCGAN, we'll go
    ahead and build it in the next exercise.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过一些 DCGAN 的基本构建块，接下来我们将在下一个练习中构建它。
- en: Generating Handwritten Images Using DCGANs
  id: totrans-454
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 DCGAN 生成手写图像
- en: Now, we will try to generate a data distribution similar to the data pertaining
    to handwritten digits using a DCGAN. We will be using the MNIST handwritten digits
    dataset as the real dataset. This dataset has a training set of 60,000 examples,
    all of which are handwritten images of digits from 0 to 9\. The implementation
    process for this GAN will be similar to *Exercise 7.04*, *Implementing the GAN*,
    where we implemented the GAN for the known function. Let's look at the steps we
    will follow for this problem statement.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将尝试使用 DCGAN 生成一个类似于手写数字数据的数据分布。我们将使用 MNIST 手写数字数据集作为真实数据集。这个数据集包含 60,000
    个训练样本，所有样本都是从 0 到 9 的手写数字图像。这个 GAN 的实现过程将与*练习 7.04*，*实现 GAN*中的过程相似，我们在其中实现了已知函数的
    GAN。让我们来看一下我们将遵循的步骤来解决这个问题。
- en: 'First, we''ll need to define the function that will be used to generate a real
    data distribution:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义一个用于生成真实数据分布的函数：
- en: '[PRE70]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: The preceding function will generate the real data distribution from the MNIST
    dataset. The train and test sets can be generated using the `mnist.load_data()`
    function. Using this function, we get all the related datasets in the form `(X_train,y_train)`,`(X_test,y_test)`.
    Since we only require the `X_train` data, we do not store the other datasets in
    variables.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数将从 MNIST 数据集生成真实数据分布。训练集和测试集可以通过 `mnist.load_data()` 函数生成。使用该函数，我们将获得所有相关数据集，格式为
    `(X_train,y_train)`,`(X_test,y_test)`。由于我们只需要 `X_train` 数据，因此我们不会将其他数据集存储在变量中。
- en: 'The MNIST data is two-dimensional; that is, (width, height). Since we require
    three-dimensional data (width, height, channel) for convolutional operations,
    we need to create the third dimension as 1 using the `np.newaxis` function. Please
    note that the first dimensions will be the number of examples:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据是二维的，即 (宽度，高度)。由于我们需要三维数据 (宽度，高度，通道) 进行卷积操作，因此我们需要使用 `np.newaxis` 函数创建第三维。请注意，第一个维度将是样本的数量：
- en: '[PRE71]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The other process is to generate batches of the training data. To generate
    batches of data, we sample some integers between 0 and the number of examples
    in the training set. The sample''s size will be equal to the batch size we want.
    This is implemented as follows:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个过程是生成训练数据的批次。为了生成数据批次，我们需要在 0 和训练集中的样本数量之间随机抽取一些整数。样本的大小将等于我们希望的批次大小。其实现如下：
- en: '[PRE72]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: We will only be returning the `X` variable. The labels that are batches of 1s
    will be separately generated during the training process.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只会返回 `X` 变量。批次的标签值将是 1，并将在训练过程中单独生成。
- en: Then, we need to define the three functions that will be used for generating
    fake samples. These are a function for generating fake inputs, a function for
    the generator network, and a function for generating fake samples and labels.
    Most of these functions are the same as what we developed in *Exercise 7.04*,
    *Implementing the GAN*. The generator model will be constructed as a convolutional
    model with intermittent use of **Up-Sampling/Converse2Dtranspose** operations.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要定义三个函数，用于生成假样本。这些函数包括生成假输入的函数、生成器网络的函数以及生成假样本和标签的函数。大多数这些函数与我们在*练习 7.04*，*实现
    GAN*中开发的函数相同。生成器模型将被构建为一个卷积模型，并间歇性地使用**Up-Sampling/Converse2Dtranspose**操作。
- en: Next, we need to create a new function for the discriminator network. This discriminator
    model will, again, be a convolutional model with the final layer as a sigmoid
    layer where we output a probability, that is, the probability of an image being
    real or fake. The input dimensions to the discriminator model will be the dimensions
    of the images generated from MNIST and the fake images, which will be (batch size,
    28,28,1).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要为判别器网络创建一个新函数。这个判别器模型将再次是一个卷积模型，最后一层是一个 sigmoid 层，在其中输出一个概率值，即图像为真实图像还是假图像的概率。判别器模型的输入维度将是从
    MNIST 生成的图像和假图像的维度，这些维度为(batch size, 28, 28, 1)。
- en: The GAN model will be the same as the one we created in *Exercise 7.04*, *Implementing
    the GAN.* This function will have the generator model and the discriminator model
    as its inputs.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: GAN 模型将与我们在*练习 7.04*，*实现 GAN*中创建的模型相同。此函数将以生成器模型和判别器模型作为输入。
- en: The Training Process
  id: totrans-467
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练过程
- en: 'The training process will be similar to the process we implemented in *Exercise
    7.04,* *Implementing the GAN*. The steps for the training process are as follows:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程将与我们在*练习 7.04*，*实现 GAN*中实现的过程类似。训练过程的步骤如下：
- en: Generate a batch of MNIST data using the function to generate a real dataset.
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成真实数据集的函数，生成一批 MNIST 数据。
- en: Generate a batch of fake samples using *function 3* described in the functions
    for generating fake samples.
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*函数 3*，生成一个假样本批次，假样本生成函数中有详细描述。
- en: Concatenate the real samples and fake samples into one DataFrame. This will
    be the input variable for the discriminator model.
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将真实样本和假样本拼接成一个 DataFrame。这将成为判别模型的输入变量。
- en: The labels will be a series of 1s and 0s corresponding to the real data and
    fake data that was concatenated earlier.
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签将是一系列的 1 和 0，对应于之前拼接的真实数据和假数据。
- en: Train the discriminator model using the `train_on_batch()` function using the
    `X` variable and the labels.
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `train_on_batch()` 函数，通过`X`变量和标签训练判别模型。
- en: Generate another batch of fake inputs for training the GAN model. These fake
    samples are generated using *function 1* in the fake sample generation process.
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为训练 GAN 模型生成另一个批次的假输入。这些假样本是使用假样本生成过程中*函数 1*生成的。
- en: Generate the labels for the fake samples that are intended to fool the discriminator.
    These labels will be 1s instead of 0s.
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为假样本生成标签，这些标签的目的是欺骗判别器。这些标签将是 1，而不是 0。
- en: Train the GAN model using the `train_on_batch()` function using the fake samples
    and its labels, as described in *Steps 6* and *7*.
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_on_batch()`函数，利用假样本及其标签训练 GAN 模型，如*步骤 6*和*7*所述。
- en: '*Steps 1* to *8* are repeated for the number of epochs we want the training
    to run for. This is done through a `for` loop over the number of epochs.'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤 1*到*步骤 8*会根据我们希望训练运行的 epoch 数量重复。这是通过对 epoch 数量进行 `for` 循环实现的。'
- en: At every intermediate step, we calculate the accuracy of the discriminator model.
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个中间步骤中，我们都会计算判别模型的准确性。
- en: We also generate output plots at certain epochs.
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还会在某些 epoch 时生成输出图。
- en: Now that we have seen the complete process behind training a DCGAN, let's dive
    into the next exercise, which implements this process.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了训练 DCGAN 背后的完整过程，让我们深入到下一个练习中，实际实现这个过程。
- en: 'Exercise 7.05: Implementing the DCGAN'
  id: totrans-481
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.05：实现 DCGAN
- en: 'In this exercise, we will build and train the DCGAN on the MNIST dataset. We
    will use the MNIST dataset as the real data distribution. We will then generate
    fake data from a random distribution. After that, we will train the GAN to generate
    data that is similar to the MNIST dataset''s. Follow these steps to complete this
    exercise:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将构建并训练一个 DCGAN，使用 MNIST 数据集。我们将使用 MNIST 数据集作为真实数据分布，然后从随机分布中生成假数据。之后，我们将训练
    GAN 生成类似 MNIST 数据集的内容。按照以下步骤完成本练习：
- en: 'Open a new Jupyter Notebook and name it `Exercise 7.05`. Import the following
    library packages and the MNIST dataset:'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook，并命名为`练习 7.05`。导入以下库包和 MNIST 数据集：
- en: '[PRE73]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Define the function that will be used to generate real datasets. The real dataset
    is generated from the MNIST data:'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，用于生成真实数据集。真实数据集是从 MNIST 数据中生成的：
- en: '[PRE74]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The return value from this function is the batch of MNIST data. Note that we
    normalize the input data by subtracting `127.5`, which is half the maximum pixel
    values (255), and divide by the same amount. This will help with converging the
    solution faster.
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数的返回值是 MNIST 数据的批次。请注意，我们通过减去`127.5`（这是像素值的最大值 255 的一半）并除以相同的数值来对输入数据进行归一化。这有助于更快地收敛解决方案。
- en: 'Now, let''s generate a set of images from the MNIST dataset:'
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们从 MNIST 数据集中生成一组图像：
- en: '[PRE75]'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Next, let''s visualize the plots using `matplotlib`:'
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们使用 `matplotlib` 可视化这些图：
- en: '[PRE76]'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'You should get an output similar to the following:'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该会得到类似于以下的输出：
- en: '![Figure 7.24: Visualized data – digits from the dataset'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.24：可视化数据 – 来自数据集的数字'
- en: '](img/B15385_07_24.jpg)'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_24.jpg)'
- en: 'Figure 7.24: Visualized data – digits from the dataset'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.24：可视化数据 – 来自数据集的数字
- en: From the output, we can see the visualization of some of the digits. We can
    see that the image is centrally positioned within a white background.
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从输出中，我们可以看到一些数字的可视化。我们可以看到图像在白色背景中居中显示。
- en: Note
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The digits that are visualized when you run the code will differ from the ones
    we've shown here.
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你运行代码时，所显示的数字会与我们在这里展示的有所不同。
- en: 'Define the function to generate inputs for the generator network. The fake
    data will be random data points generated from a uniform distribution:'
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，用于为生成器网络生成输入。假数据将是从均匀分布中生成的随机数据点：
- en: '[PRE77]'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let''s define the function for building the generator network. Building the
    generator network is similar to building any CNN network. In this generator network,
    we will use the `UpSampling` method:'
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义构建生成器网络的函数。构建生成器网络类似于构建任何CNN网络。在这个生成器网络中，我们将使用`UpSampling`方法：
- en: '[PRE78]'
  id: totrans-502
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: In the model, we can see the progressive use of the transpose convolution operation.
    The initial input has the dimensions of 100\. This is progressively increased
    to the desired image size of batch size x 28 x 28 through a series of transpose
    convolution operations.
  id: totrans-503
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在模型中，我们可以看到逐步使用转置卷积操作。初始输入的维度为100。通过一系列转置卷积操作，这个维度逐渐增加，最终达到所需的图像大小：batch size
    x 28 x 28。
- en: 'Next, we define the function to create fake samples. In this function, we only
    return the `X` variable:'
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数来创建假样本。在这个函数中，我们只返回`X`变量：
- en: '[PRE79]'
  id: totrans-505
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: The return value from this function is the fake dataset.
  id: totrans-506
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个函数的返回值是伪造的数据集。
- en: 'Define the parameters that we will use, along with the summary of the generator network:'
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们将使用的参数，并给出生成器网络的总结：
- en: '[PRE80]'
  id: totrans-508
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'You should get the following output:'
  id: totrans-509
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 7.25 Summary of the model'
  id: totrans-510
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.25 模型总结'
- en: '](img/B15385_07_25.jpg)'
  id: totrans-511
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_25.jpg)'
- en: Figure 7.25 Summary of the model
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.25 模型总结
- en: 'From the summary, please note how the dimension of the input changes with each
    transpose convolution operation. Finally, we get an output that is equal in dimension
    to the real data set: `(None,28 ,28,1)`.'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从总结中，请注意每次转置卷积操作后输入维度的变化。最终，我们得到了一个与真实数据集维度相同的输出：(None, 28, 28, 1)。
- en: 'Let''s use the generator function to generate a fake sample before training:'
  id: totrans-514
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练之前，让我们使用生成器函数生成一个假样本：
- en: '[PRE81]'
  id: totrans-515
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'You should get the following output:'
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '[PRE82]'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Now, let''s plot the generated fake sample:'
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们绘制生成的假样本：
- en: '[PRE83]'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'You should get an output similar to the one shown here:'
  id: totrans-520
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到一个类似下面的输出：
- en: '![Figure 7.26: Plot of the fake sample image'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.26：假样本图像的绘制'
- en: '](img/B15385_07_26.jpg)'
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_26.jpg)'
- en: 'Figure 7.26: Plot of the fake sample image'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.26：假样本图像的绘制
- en: This is the plot of the fake sample before training. After training, we want
    samples like these to look like the MNIST samples we visualized earlier in this exercise.
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是训练前假样本的图像。训练后，我们希望这些样本看起来像我们之前在本练习中可视化的MNIST样本。
- en: 'Now, let''s build the discriminator model as a function. The network will be
    a CNN network like the one you learned about in *Chapter 3*, *Image Classification
    with Convolutional Neural Networks*:'
  id: totrans-525
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们把判别器模型作为函数来构建。这个网络将是一个CNN网络，就像你在*第 3 章*中学习的内容，*使用卷积神经网络进行图像分类*：
- en: '[PRE84]'
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: In the discriminator network, we have included all the necessary layers, such
    as the convolutional operations and LeakyReLU activations. Please note that the
    last layer is a sigmoid layer as we want the output as a probability of the sample
    to be real (1) or fake (0).
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在判别器网络中，我们包含了所有必要的层，比如卷积操作和LeakyReLU激活函数。请注意，最后一层是一个sigmoid层，因为我们希望输出是一个样本为真实（1）或假（0）的概率。
- en: 'Print the summary of the discriminator network:'
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印判别器网络的总结：
- en: '[PRE85]'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'You should get the following output:'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 7.27: Summary of the model architecture'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.27：模型架构总结'
- en: '](img/B15385_07_27.jpg)'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_27.jpg)'
- en: 'Figure 7.27: Summary of the model architecture'
  id: totrans-533
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.27：模型架构总结
- en: The preceding screenshot shows the summary of the model architecture. This is
    based on the different layers we implemented using the `Sequential` class. For
    example, we can see that the first layer has 32 filter maps, the second layer
    has 64 filter maps, and the last layer has one output that corresponds to the
    sigmoid activation.
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上面的截图展示了模型架构的总结。这是基于我们使用`Sequential`类实现的不同层。例如，我们可以看到第一层有32个滤波器图，第二层有64个滤波器图，最后一层有一个输出，对应于sigmoid激活函数。
- en: 'Next, define the GAN model as a function:'
  id: totrans-535
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将GAN模型定义为一个函数：
- en: '[PRE86]'
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The structure of the GAN model is similar to the one we developed in *Exercise
    7.04*, *Implementing the GAN*.
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GAN模型的结构与我们在*练习 7.04*中开发的结构类似，*实现GAN*。
- en: 'Now, it''s time to invoke the GAN function. Please note that the inputs to
    the GAN model are the previously defined generator and discriminator models:'
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，是时候调用GAN函数了。请注意，GAN模型的输入是之前定义的生成器和判别器模型：
- en: '[PRE87]'
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'From the preceding code, we can see that the inputs to the GAN model are the
    previously defined generator and discriminator models. You should get the following
    output:'
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以看到GAN模型的输入是先前定义的生成器和判别器模型。你应该得到如下输出：
- en: '![Figure 7.28: Model summary'
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.28：模型摘要'
- en: '](img/B15385_07_28.jpg)'
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_28.jpg)'
- en: 'Figure 7.28: Model summary'
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.28：模型摘要
- en: Please note that the parameters of each layer of the GAN model are equivalent
    to the parameters of the generator and discriminator models. The GAN model is
    just a wrapper around the models we defined earlier.
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，GAN模型每一层的参数等同于生成器和判别器模型的参数。GAN模型只是我们先前定义的模型的一个封装。
- en: 'Define the number of epochs to train the network:'
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练网络的周期数：
- en: '[PRE88]'
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Now, let''s train the GAN:'
  id: totrans-547
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们训练GAN：
- en: '[PRE89]'
  id: totrans-548
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: From the preceding code, we can see that the training of the discriminator model
    with the fake and real samples and the training of the GAN model happens concurrently.
    The only difference is the training of the GAN model proceeds without updating
    the parameters of the discriminator model. The other thing to note is that, inside
    the GAN, the labels for the fake samples would be 1 to generate large loss terms
    that will be backpropagated through the discriminator network to update the generator
    parameters. We also display the predicted probability of the GAN for every 10
    epochs. When calculating the probability, we combine a sample of real data and
    fake data and then take the mean of the predicted probability. We also save a
    copy of the generated image.
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以看到，判别器模型使用假样本和真实样本进行训练，而GAN模型的训练是同时进行的。唯一的区别是GAN模型的训练在不更新判别器模型参数的情况下继续进行。另一个需要注意的点是，在GAN内部，假样本的标签将是1，以生成较大的损失项，这些损失项将通过判别器网络反向传播以更新生成器参数。我们还显示了每10个训练周期后GAN的预测概率。在计算概率时，我们将真实数据和假数据样本结合在一起，然后取预测概率的平均值。我们还保存了一份生成图像的副本。
- en: '[PRE90]'
  id: totrans-550
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Note
  id: totrans-551
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The output for the preceding code may not be an exact match with what you get
    when you run the code.
  id: totrans-552
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面代码的输出可能与您运行代码时得到的结果不完全一致。
- en: From the predicted probability of the test data, we can see that the values
    are hovering around the `.55` mark. This is an indication that the discriminator
    is confused about whether the image is fake or real. If the discriminator were
    sure that an image was real, it would predict a probability close to 1, while
    it would predict a probability close to 0 if it were sure that the image was fake.
    In our case, we can see that the probability is around the .55 mark, which indicates
    that the generator is learning to generate images similar to the real images.
    This has confused the discriminator. *A value close to 50% accuracy for the discriminator
    is the desired value.*
  id: totrans-553
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从测试数据的预测概率中，我们可以看到，值徘徊在`.55`附近。这表明判别器对于图像是假的还是实的感到困惑。如果判别器确定图像是真的，它会预测接近1的概率，而如果确定图像是假的，它则会预测接近0的概率。在我们的案例中，概率大约在.55附近，表明生成器正在学习生成与真实图像相似的图像，这使得判别器感到困惑。*判别器的准确率接近50%的值是理想值。*
- en: 'Now, let''s generate fake images after the training process and visualize them:'
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们在训练过程后生成假图像并可视化它们：
- en: '[PRE91]'
  id: totrans-555
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The output will be as follows:'
  id: totrans-556
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.29: Predicted image post training'
  id: totrans-557
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.29：训练后预测的图像'
- en: '](img/B15385_07_29.jpg)'
  id: totrans-558
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_07_29.jpg)'
- en: 'Figure 7.29: Predicted image post training'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.29：训练后预测的图像
- en: We can see that the generated images from the trained generator model closely
    resonate with the real handwritten digits.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，从训练后的生成器模型生成的图像与真实的手写数字非常相似。
- en: Note
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZPg8cJ](https://packt.live/2ZPg8cJ).
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此部分的源代码，请参考[https://packt.live/2ZPg8cJ](https://packt.live/2ZPg8cJ)。
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线交互示例，需在本地运行。
- en: In this exercise, we developed a GAN to generate distributions similar to MNIST
    handwritten digits. In the section that follows, we will analyze the images that
    were generated at each epoch during this exercise.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们开发了一个GAN，用于生成类似MNIST手写数字的分布。在接下来的部分，我们将分析在每个训练周期生成的图像。
- en: Analysis of Sample Plots
  id: totrans-565
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 样本图分析
- en: 'Now, let''s look at the output sample plots from the previous exercise to see
    what the generated images look like. By completing the previous exercise, these
    should have been saved in the same path where your Jupyter Notebook is located,
    under a subfolder called `handwritten`:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看前一个练习的输出样本图，看看生成的图像是什么样子的。完成前一个练习后，这些图像应该已经保存在与你的 Jupyter Notebook 位于同一路径下的一个名为
    `handwritten` 的子文件夹中：
- en: '![Figure 7.30: Sample plot after 10 iterations'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.30: 经过 10 次迭代后的样本图'
- en: '](img/B15385_07_30.jpg)'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_30.jpg)'
- en: 'Figure 7.30: Sample plot after 10 iterations'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7.30: 经过 10 次迭代后的样本图'
- en: 'The preceding images are those that were generated after 10 iterations. We
    can see that these images look more like random noise. However, we can also see
    that there are traces of white patches forming within the image, which indicates
    the GAN is learning some of the features of the real image:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像是经过 10 次迭代后的生成结果。我们可以看到这些图像更像是随机噪声。然而，我们也可以看到图像中开始形成一些白色斑块，这表明 GAN 正在学习真实图像的一些特征：
- en: '![ Figure 7.31: Sample plot after 500 iterations'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.31: 经过 500 次迭代后的样本图'
- en: '](img/B15385_07_31.jpg)'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_31.jpg)'
- en: 'Figure 7.31: Sample plot after 500 iterations'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7.31: 经过 500 次迭代后的样本图'
- en: 'The preceding images are the plots after 500 iterations. From these images,
    we can see some semblance of the real image. We can see that the white background
    of the real images is being formed. We can also see the distribution getting aggregated
    at the center of the image:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像是经过 500 次迭代后的结果。从这些图像中，我们可以看到一些类似真实图像的特征。我们可以看到，真实图像的白色背景正在形成。我们还可以看到图像中的分布开始集中在图像的中央：
- en: '![ Figure 7.32: Sample plot after 2,000 iterations'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.32: 经过 2,000 次迭代后的样本图'
- en: '](img/B15385_07_32.jpg)'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_32.jpg)'
- en: 'Figure 7.32: Sample plot after 2,000 iterations'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7.32: 经过 2,000 次迭代后的样本图'
- en: 'The preceding image is after 2,000 iterations. We can see that many digits
    have started to form; for example, 8, 5 ,3 ,9 ,4, 7, 0, and so on. We can also
    see that the dark shades of the images have started to become more pronounced.
    Now, let''s look at the images that were generated during the last iteration:'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 上图是经过 2,000 次迭代后的结果。我们可以看到，许多数字开始出现；例如，8、5、3、9、4、7、0 等等。我们还可以看到，图像的暗色区域开始变得更加明显。现在，让我们看看最后一次迭代生成的图像：
- en: '![Figure 7.33: Sample plot after 5,000 iterations'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.33: 经过 5,000 次迭代后的样本图'
- en: '](img/B15385_07_33.jpg)'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_33.jpg)'
- en: 'Figure 7.33: Sample plot after 5,000 iterations'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7.33: 经过 5,000 次迭代后的样本图'
- en: A question to ask at this stage is, are these images perfect? Absolutely not.
    Would running the training for more epochs improve the results further? Not necessarily.
    Getting those perfect images would entail hours of training and experimentation
    with different model architectures. You can take this as a challenge to improve
    the output through your choices of architecture and the parameters within the
    model.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段需要问的一个问题是，这些图像完美吗？绝对不是。继续训练更多的轮次会进一步改善结果吗？不一定。要获得完美的图像需要长时间的训练，并且需要对不同的模型架构进行实验。你可以将此作为一个挑战，通过选择架构和模型参数来改善输出。
- en: GANs are a really active area of research and the possibilities they are opening
    up point to the direction of computers slowly becoming creative. However, there
    are some common problems when implementing GANs. Let's look at some of them.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 是一个非常活跃的研究领域，它们所打开的可能性指向了计算机逐渐变得具有创造力的方向。然而，在实现 GANs 时存在一些常见问题。让我们来看一下其中的一些。
- en: Common Problems with GANs
  id: totrans-584
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GANs 的常见问题
- en: GANs are difficult networks to train and stabilize. There are different failure
    modes for GANs. Let's get a perspective of some of the common failure modes.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 是难以训练和稳定的网络。GANs 有不同的失败模式。让我们来看看一些常见的失败模式。
- en: Mode Collapse
  id: totrans-586
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模式崩溃
- en: A very common failure mode of GANs, especially on multi-modal data, is a situation
    called **mode collapse**. This refers to a situation where the generator learns
    only some specific variety of the distribution within the data. For example, in
    an MNIST data distribution, if the GAN generates only one particular digit (say,
    5) after training, then this is a case of mode collapse.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 中一种非常常见的失败模式，尤其是在多模态数据上，是所谓的 **模式崩溃**。这指的是生成器仅学习数据分布中的某些特定种类。例如，在 MNIST
    数据分布中，如果 GAN 训练后只生成一个特定的数字（比如 5），那么这就是模式崩溃的表现。
- en: One way to combat mode collapse is to group data according to the different
    classes and train the discriminator accordingly. This will give the discriminator
    the ability to identify different modes that are present in the data.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗模式崩溃的一种方法是根据不同的类别对数据进行分组，并相应地训练判别器。这样可以使判别器具备识别数据中不同模式的能力。
- en: Convergence Failure
  id: totrans-589
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收敛失败
- en: Another prominent failure mode in GANs is convergence failure. In this failure
    mode, the network fails to converge with the loss as it never settles during the
    training phase. Some methods that researchers have used to get over this problem
    include adding noise to discriminatory networks and penalizing discriminator weights
    through regularization techniques.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: GAN中的另一个显著故障模式是收敛失败。在这种故障模式下，网络无法收敛，损失值在训练阶段始终无法稳定。一些研究人员为解决这一问题，采用的方法包括向判别网络添加噪声，并通过正则化技术对判别器权重进行惩罚。
- en: Notwithstanding the numerous challenges inherent in training and building GANs,
    it still remains one of the most active areas of research within the deep learning
    community. The promises and the applications that are made possible by GANs are
    what make this area one of the most sought-after domains in deep learning. Now
    that we have laid some of the foundations for GANs, let's use what we've learned
    to build a GAN for a different dataset.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管训练和构建GAN面临许多挑战，但它仍然是深度学习领域最活跃的研究领域之一。GAN所带来的承诺和应用使其成为深度学习中最受追捧的领域之一。现在我们已经为GAN奠定了一些基础，让我们用所学的知识为一个不同的数据集构建一个GAN。
- en: 'Activity 7.01: Implementing a DCGAN for the MNIST Fashion Dataset'
  id: totrans-592
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动7.01：为MNIST时尚数据集实现DCGAN
- en: 'In this activity, you will implement a DCGAN to generate images similar to
    the ones found in the MNIST fashion dataset. The MNIST fashion dataset is similar
    to the handwritten digital images dataset that you implemented in *Exercise 7.05*,
    *Implementing the DCGAN*. This dataset consists of grayscale images of 10 different
    fashion accessories and comprises 60,000 training samples. The following is a
    sample of the images included in this dataset:'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你将实现一个DCGAN来生成类似于MNIST时尚数据集中图像的图像。MNIST时尚数据集与*练习7.05*中实现的手写数字图像数据集类似。该数据集由10种不同的时尚配饰的灰度图像组成，共有60,000个训练样本。以下是该数据集中包含的图像样本：
- en: '![Figure 7.34: Sample of the MNIST fashion dataset'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.34：MNIST时尚数据集样本'
- en: '](img/B15385_07_34.jpg)'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_34.jpg)'
- en: 'Figure 7.34: Sample of the MNIST fashion dataset'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.34：MNIST时尚数据集样本
- en: The objective of this activity is to build a GAN and generate images similar
    to the fashion dataset. The high-level steps for this activity will be similar
    to *Exercise 7.05*, *Implementing the DCGAN*, where you implemented a DCGAN for
    handwritten digits. You will be completing this activity in two parts, first by
    creating the relevant functions and then by training the model.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的目标是构建一个GAN并生成类似于时尚数据集的图像。这个活动的高层次步骤将类似于*练习7.05*中的步骤，即你实现了一个用于手写数字的DCGAN。你将分两部分完成这个活动，首先是创建相关函数，然后是训练模型。
- en: '**Generating Key Functions**: Here, you will be creating the required functions,
    such as the generator function and the discriminator function:'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成关键函数**：在这里，你将创建所需的函数，如生成器函数和判别器函数：'
- en: 'Define the function that will generate a real data distribution. This function
    has to generate the real data distribution from the MNIST fashion dataset. The
    fashion dataset can be imported using the following code:'
  id: totrans-599
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个生成真实数据分布的函数。这个函数必须从MNIST时尚数据集中生成真实的数据分布。可以使用以下代码导入时尚数据集：
- en: '[PRE92]'
  id: totrans-600
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: The training set can be generated using the `fashion_mnist.load_data()` function.
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练集可以使用`fashion_mnist.load_data()`函数生成。
- en: 'Note:'
  id: totrans-602
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意：
- en: Alternatively, you can download the dataset from [https://packt.live/2X4xeCL](https://packt.live/2X4xeCL).
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，你可以从[https://packt.live/2X4xeCL](https://packt.live/2X4xeCL)下载数据集。
- en: Define the three functions that will be used to generate fake samples; that
    is, the function for generating fake inputs, the function for the generator network,
    and the function for generating fake samples and labels. Use *Converse2Dtranspose*
    operations within the generator function.
  id: totrans-604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义三个函数，用于生成假样本；即生成假输入的函数、生成器网络的函数以及生成假样本和标签的函数。在生成器函数中使用*Converse2Dtranspose*操作。
- en: Create a new function for the discriminator network.
  id: totrans-605
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为判别网络创建一个新函数。
- en: Create the GAN model. You can take cues from *Exercise 7.05*, *Implementing
    the DCGAN*, on how to do this.
  id: totrans-606
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建GAN模型。你可以参考*练习7.05*，*实现DCGAN*，了解如何实现这一点。
- en: '**The Training Process**: You will follow a process similar to the one in *Exercise
    7.05*, *Implementing the DCGAN*:'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练过程**：你将遵循与*练习7.05*，*实现DCGAN*类似的过程：'
- en: Generate a batch of MNIST data using the function for generating a real dataset.
  id: totrans-608
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成真实数据集的函数生成一批MNIST数据。
- en: Generate a batch of fake samples using the third function described in the functions
    for generating fake samples.
  id: totrans-609
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成假样本的第三个函数生成一批假样本。
- en: Concatenate the real samples and fake samples into one DataFrame and generate
    their labels.
  id: totrans-610
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将真实样本和假样本合并成一个DataFrame，并生成它们的标签。
- en: Train the discriminator model using the `train_on_batch()` function using the
    `X` variable and the labels.
  id: totrans-611
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_on_batch()`函数训练判别器模型，使用`X`变量和标签。
- en: Generate another batch of fake inputs for training the GAN model, along with
    their labels.
  id: totrans-612
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成另一批用于训练GAN模型的假输入，以及它们的标签。
- en: Train the GAN model using the `train_on_batch()` function using the fake samples
    and their labels.
  id: totrans-613
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_on_batch()`函数训练GAN模型，使用假样本及其标签。
- en: Repeat the training for around 5,000 epochs.
  id: totrans-614
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复训练约5,000个周期。
- en: At every intermediate step, calculate the accuracy of the discriminator model.
  id: totrans-615
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每一个中间步骤中，计算判别器模型的准确性。
- en: 'The discriminator probabilities you''ll get should be around `0.5`. The expected
    output will be a generated image that looks similar to the one shown here:'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 你得到的判别器概率应该接近`0.5`。预期的输出将是一个生成的图像，看起来与这里展示的图像相似：
- en: '![Figure 7.35: Expected output for this activity'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.35：此活动的预期输出'
- en: '](img/B15385_07_35.jpg)'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_07_35.jpg)'
- en: 'Figure 7.35: Expected output for this activity'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.35：此活动的预期输出
- en: 'Note:'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 426.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的详细步骤，包含解决方案和额外的注释，见第426页。
- en: Summary
  id: totrans-622
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: You have come a long way from being introduced to one of the most promising
    areas in deep learning. Let's revisit some of the concepts that we learned about
    in this chapter.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经从了解深度学习中最有前景的领域之一，走了很长一段路。让我们回顾一下本章中学到的一些概念。
- en: We started this chapter by understanding what GANs are and their major applications.
    We then went on to understand the various building blocks of GANs, such as the
    real datasets, fake datasets, the discriminator operation, the generator operation,
    and the GAN operation.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从了解GAN是什么及其主要应用开始了这一章。然后，我们进一步了解了GAN的各种构建模块，如真实数据集、假数据集、判别器操作、生成器操作和GAN操作。
- en: We executed a problem statement to progressively build a **fully connected GAN**
    (**FCGAN**) to solve a real function. In the process of building the GAN, we also
    implemented exercises for creating real datasets, creating fake datasets, creating
    a generator network, creating a discriminator network, and finally combining all
    these individual components to create the GAN. We visualized the different plots
    and understood how the generated data distribution mimicked the real data distribution.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行了一个问题陈述，逐步构建了一个**全连接GAN**（**FCGAN**）来解决一个实际的函数问题。在构建GAN的过程中，我们还实现了创建真实数据集、创建假数据集、创建生成器网络、创建判别器网络的练习，最后将所有这些单独的组件组合成GAN。我们可视化了不同的图形，并理解了生成的数据分布如何模仿真实数据分布。
- en: In the next section, we understood the concept of DCGANs. We also visited some
    of the unique concepts in DCGANs such as upsampling and transpose convolutions.
    We implemented a GAN for the MNIST digital handwritten images and visualized the
    fake images we generated using a DCGAN. Finally, we also implemented a DCGAN for
    the MNIST fashion dataset in an activity.
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们了解了DCGAN的概念。我们还学习了DCGAN中的一些独特概念，如上采样和转置卷积。我们实现了一个用于MNIST数字手写图像的GAN，并使用DCGAN可视化了我们生成的假图像。最后，我们还在一个活动中为MNIST时尚数据集实现了DCGAN。
- en: Having laid the foundations, the next question is, where do we go from here?
    GANs are a large area by itself and there's quite a lot of buzz around it these
    days. To start with, it would be good to tweak the models you have already learned
    by tweaking the architecture and activation functions and trying out other parameters
    such as batch normalization. After playing around with different variations of
    the current models, it will be time to explore other networks such as the **Least
    Squares GAN** (**LSGAN**) and **Wasserstein GAN** (**WGAN**). Then, there is this
    large playing field of conditional GANs such as the **Conditional GAN** (**cGan**),
    InfoGAN, **Auxiliary Classifier GAN** (**AC-GAN**), and **Semi-Supervised GAN**
    (**SGAN**). Once you've done this, you'll have set the stage for advanced topics
    such as CycleGAN, BigGAN, and StyleGAN.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 在打下基础后，接下来的问题是，我们从哪里开始？生成对抗网络（GAN）本身就是一个庞大的领域，最近围绕它有很多热议。首先，调整你已经学习过的模型会是一个不错的起点，可以通过调整架构、激活函数以及尝试其他参数（如批量归一化）来进行优化。在尝试不同变种的现有模型后，接下来就可以探索其他网络，例如**最小二乘生成对抗网络**（**LSGAN**）和**瓦瑟斯坦生成对抗网络**（**WGAN**）。接着，还有一个大领域是条件生成对抗网络（conditional
    GAN），比如**条件生成对抗网络**（**cGan**）、InfoGAN、**辅助分类器生成对抗网络**（**AC-GAN**）和**半监督生成对抗网络**（**SGAN**）。完成这些后，你将为学习更高级的主题，如CycleGAN、BigGAN和StyleGAN，奠定基础。
- en: This chapter also brings down the curtain on the amazing journey you've made
    throughout this book. First, you were introduced to what deep learning is and
    the different use cases that are possible with deep learning. Subsequently, you
    learned the basics of neural networks, which are the foundations of deep learning.
    From there, you went on to learn about advanced techniques such as CNNs, which
    are the workhorses for use cases such as image recognition. Along with that, you
    learned about recurrent neural networks, which can be used for sequence data.
    Finally, you were introduced to GANs, a class of networks that's making lots of
    waves within the domain. Having equipped yourself with this set of tools now is
    the time to apply your learning to your domain. The possibilities and opportunities
    are endless. All we need to do is consolidate our current learning and move ahead
    step by step. I wish you all the best on your journey in scaling new peaks in
    the deep learning domain.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 本章也为你在本书中的精彩旅程画上了句号。首先，你了解了什么是深度学习以及深度学习可以实现的不同应用场景。随后，你学习了神经网络的基础知识，神经网络是深度学习的基础。从那里，你开始学习诸如卷积神经网络（CNN）等高级技术，它们是图像识别等应用场景的主要技术。与此同时，你还学习了循环神经网络（RNN），它们可以用于处理序列数据。最后，你接触了生成对抗网络（GAN），这一类网络正在该领域掀起波澜。现在，你已经掌握了这套工具，是时候将所学应用到你的领域了。可能性和机会是无穷无尽的。我们需要做的就是巩固当前的学习，并一步步向前迈进。祝你在深度学习领域的旅程中不断攀登新高峰，取得成功。
