- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Accelerate Data Engineering on AWS
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速AWS上的数据工程
- en: 'In this chapter, we will look at the following key topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下关键主题：
- en: Code assistance options with AWS services
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与AWS服务的代码辅助选项
- en: Code assistance integration with AWS Glue
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与AWS Glue的代码辅助集成
- en: Code assistance integration with Amazon EMR
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Amazon EMR的代码辅助集成
- en: Code assistance integration with AWS Lambda
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与AWS Lambda的代码辅助集成
- en: Code assistance integration with Amazon SageMaker
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Amazon SageMaker的代码辅助集成
- en: Code assistance integration with Amazon Redshift
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Amazon Redshift的代码辅助集成
- en: In the previous part of the book, we explored auto-code generation techniques
    and the integration of a code companion with **integrated development environments**
    (**IDEs**) and provided examples using JetBrains PyCharm IDE with Amazon Q Developer
    for different languages that developers use very often. In this chapter, we will
    specifically focus on how Amazon is expanding in the area of assisting code developers
    by integrating with core AWS services.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的前一部分，我们探讨了自动代码生成技术以及代码伴侣与**集成开发环境**（**IDEs**）的集成，并提供了使用JetBrains PyCharm
    IDE和Amazon Q Developer对不同语言进行示例的例子。在本章中，我们将特别关注Amazon如何通过与其核心AWS服务的集成来扩展其在辅助代码开发者领域的努力。
- en: Code assistance options with AWS services
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与AWS服务的代码辅助选项
- en: AWS users select diverse services, considering factors such as the unique requirements
    of their projects, use cases, developers’ technical needs, developer preferences,
    and the characteristics of AWS services. To cater to various developer personas,
    such as data engineers, data scientists, application developers, and so on, AWS
    has integrated code assistance with many of its code services. If you are an application
    builder, software developer, data engineer, or data scientist working with AWS
    services, you would frequently use builder-friendly tools such as Amazon SageMaker
    as a platform for building AI / **machine learning** (**ML**) projects, Amazon
    EMR as a platform for building big data processing projects, AWS Glue for building
    **extract, transform, and load** (**ETL**) pipelines, AWS Lambda as a serverless
    compute service for application development. All these services provide tools
    that help builders and developers write code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: AWS用户会根据他们项目的独特需求、用例、开发者的技术需求、开发者偏好以及AWS服务的特点来选择多样化的服务。为了满足各种开发者角色，例如数据工程师、数据科学家、应用开发者等，AWS已经将其代码服务与代码辅助功能集成。如果你是使用AWS服务的应用构建者、软件开发者、数据工程师或数据科学家，你将经常使用Amazon
    SageMaker作为构建AI/**机器学习**（**ML**）项目的平台，Amazon EMR作为构建大数据处理项目的平台，AWS Glue用于构建**提取、转换和加载**（**ETL**）管道，AWS
    Lambda作为应用开发的无服务器计算服务。所有这些服务都提供了帮助构建者和开发者编写代码的工具。
- en: '![Figure 14.1 – Code assistance options with AWS services](img/B21378_14_1.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图14.1 – 与AWS服务相关的代码辅助选项](img/B21378_14_1.jpg)'
- en: Figure 14.1 – Code assistance options with AWS services
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 – 与AWS服务相关的代码辅助选项
- en: As of the writing of this book, AWS has integrated Amazon Q Developer with AWS
    Glue, Amazon EMR, AWS Lambda, Amazon SageMaker, and Amazon Redshift. However,
    we anticipate that the list of services benefiting from code assistance, such
    as Amazon Q Developer, will continue to expand in the future.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本书编写时，AWS已经将Amazon Q Developer与AWS Glue、Amazon EMR、AWS Lambda、Amazon SageMaker和Amazon
    Redshift集成。然而，我们预计受益于代码辅助的服务列表，如Amazon Q Developer，将在未来继续扩展。
- en: In the following sections, we will dive deep into each of these services, examining
    their integration with Amazon Q in detail. We will provide examples that will
    be helpful for data engineers to accelerate development on AWS.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将深入探讨这些服务的每一个，详细检查它们与Amazon Q的集成情况。我们将提供有助于数据工程师在AWS上加速开发的示例。
- en: Note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**Large language models** (**LLMs**), by nature, are non-deterministic, so
    you may not get the same code blocks shown in the code snapshots. However, logically,
    the generated code should meet the requirements.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型**（**LLMs**）本质上是非确定性的，因此你可能不会得到代码快照中显示的相同代码块。然而，从逻辑上讲，生成的代码应该满足要求。'
- en: '**CodeWhisperer** is a legacy name from a service that merged with Amazon Q
    Developer. As of the time of writing this book, some of the integrations are still
    referred to as CodeWhisperer in the AWS console, which may change in the future.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**CodeWhisperer**是合并到Amazon Q Developer服务中的一个旧名称。截至本书编写时，AWS控制台中的一些集成仍然被称为CodeWhisperer，这可能在将来发生变化。'
- en: Code assistance integration with AWS Glue
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与AWS Glue的代码辅助集成
- en: 'Before we start diving deep into code assistance support for AWS Glue service,
    let’s quickly go through an overview of AWS Glue. **AWS Glue** is a serverless
    data integration service designed to simplify the process of discovering, preparing,
    moving, and integrating data from diverse sources, catering to analytics, ML,
    and application development needs. At the very high level, AWS Glue has the following
    major components, and each of them has multiple features to support data engineers:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨AWS Glue服务的代码辅助支持之前，让我们快速浏览AWS Glue的概述。**AWS Glue**是一个无服务器数据集成服务，旨在简化从各种来源发现、准备、移动和集成数据的过程，满足分析、机器学习和应用开发的需求。在非常高的层面上，AWS
    Glue具有以下主要组件，每个组件都有多个功能来支持数据工程师：
- en: '**Glue Data Catalog**: It’s a centralized technical metadata repository. It
    stores metadata about data sources, transformations, and targets, providing a
    unified view of the data.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Glue数据目录**：它是一个集中的技术元数据存储库。它存储有关数据源、转换和目标的数据，提供了一个统一的数据视图。'
- en: '**Glue Studio**: AWS Glue Studio offers a graphical interface that facilitates
    the seamless creation, execution, and monitoring of data integration jobs within
    AWS Glue. Additionally, it provides Jupyter notebooks for advanced developers.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Glue Studio**：AWS Glue Studio提供了一个图形界面，它简化了在AWS Glue中创建、执行和监控数据集成作业的过程。此外，它还为高级开发者提供了Jupyter笔记本。'
- en: AWS Glue Studio is seamlessly integrated with Amazon Q Developer. Let’s explore
    the further functionality by considering a very common use case of data enrichment.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue Studio无缝集成到Amazon Q Developer。让我们通过考虑数据丰富化的一个非常常见的用例来探索其进一步的功能。
- en: Use case for AWS Glue
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Glue的应用场景
- en: Features and functionalities of any service or tool are best understood when
    we have a use case to solve. So, let’s start with one of the easy and widely used
    use cases of data enrichment using lookups.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有用例要解决时，我们才能最好地理解任何服务或工具的功能和特性。因此，让我们从一个简单且广泛使用的使用查找表进行数据丰富化的用例开始。
- en: '**Data enrichment using lookup**: In a typical scenario, business analysts
    often require data enrichment by incorporating details associated with codes/IDs
    found in a column through a lookup table. The desired result is a comprehensive
    and denormalized record containing both the code and corresponding details in
    the same row. To address this specific use case, data engineers develop ETL jobs
    to join the tables, creating the final structure with a denormalized dataset.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用查找表进行数据丰富化**：在典型场景中，业务分析师通常需要通过查找表将列中找到的代码/ID的相关细节合并到数据中，以实现数据丰富化。期望的结果是在同一行中包含代码和相应的详细信息，形成一个全面且去规范化（denormalized）的记录。为了解决这个特定的用例，数据工程师开发ETL作业来连接表，创建包含去规范化数据集的最终结构。'
- en: To illustrate this use case, we will use yellow taxi trip records that encompass
    details such as the date and time of pick-up and drop-off, the locations for pick-up
    and drop-off, the trip distance, comprehensive fare breakdowns, various rate types,
    utilized payment methods, and passenger counts reported by the driver. Additionally,
    trip information incorporates passenger location codes for both pick-up and drop-off.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个用例，我们将使用包含诸如接车和下车的日期和时间、接车和下车的位置、行程距离、详细的费用分解、各种费率类型、使用的支付方式和司机报告的乘客数量等详细信息的黄色出租车行程记录。此外，行程信息还包括接车和下车的乘客位置代码。
- en: The business objective is to enhance the dataset with zone information based
    on the pick-up location code.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 业务目标是根据接车位置代码增强数据集的区域信息。
- en: To meet this requirement, data engineers must develop a PySpark ETL script.
    This script should perform a lookup for zone information corresponding to the
    pick-up location code. Subsequently, the engineers create denormalized/enriched
    data by amalgamating yellow taxi trip data with detailed pick-up zone information
    and save the result as a file.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这一需求，数据工程师必须开发一个PySpark ETL脚本。此脚本应执行查找与接车位置代码对应的区域信息。随后，工程师通过将黄色出租车行程数据与详细的接车区域信息合并来创建去规范化/丰富化的数据，并将结果保存为文件。
- en: As a code developer / data engineer, you will need to convert the preceding
    business objectives into technical requirements.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作为代码开发者/数据工程师，您需要将前面的业务目标转换为技术需求。
- en: Solution blueprint
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案蓝图
- en: Write a PySpark code to handle technical requirements.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写PySpark代码以处理技术需求。
- en: Read the `yellow_tripdata_2023-01.parquet` file from the S3 location in a DataFrame
    and display a sample of 10 records.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从S3位置读取`yellow_tripdata_2023-01.parquet`文件到DataFrame中，并显示10条记录的样本。
- en: Read the `taxi+_zone_lookup.csv` file from the S3 location in a DataFrame and
    display a sample of 10 records.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从S3位置读取`taxi+_zone_lookup.csv`文件到DataFrame中，并显示10条记录的样本。
- en: Perform a left outer join on `yellow_tripdata_2023-01.parquet` and `taxi+_zone_lookup.csv`
    on `PULocationID = LocationID` to gather pick-up zone information.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`PULocationID = LocationID`上对`yellow_tripdata_2023-01.parquet`和`taxi+_zone_lookup.csv`执行左外连接，以收集接车区域信息。
- en: Save the preceding dataset as a CSV file in the preceding Amazon S3 bucket in
    a new `glue_notebook_yellow_pick_up_zone_output` folder.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将前面提到的数据集作为CSV文件保存在前面的Amazon S3桶中的新`glue_notebook_yellow_pick_up_zone_output`文件夹中。
- en: For verification, download and check the files from the `glue_notebook_yellow_pick_up_zone_output`
    folder.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为验证，从`glue_notebook_yellow_pick_up_zone_output`文件夹下载并检查文件。
- en: Now that we have a use case defined, let’s go through the step-by-step solution
    for it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了一个use case，让我们逐步通过它的解决方案。
- en: Data preparation
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: The first step will be to prepare the data. To illustrate its functionality,
    in the following sections, we will utilize the publicly available NY Taxi dataset
    from TLC Trip Record Data. [https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步将是准备数据。为了说明其功能，在接下来的章节中，我们将利用公开的NY出租车数据集（TLC行程记录数据）。[https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)。
- en: 'Firstly, we will download the required files on a local machine and then upload
    them in one of Amazon’s S3 buckets:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将在本地机器上下载所需的文件，然后上传到Amazon的一个S3桶中：
- en: Download the Yellow Taxi Trip Records data for the Jan 2023 Parquet file (`yellow_tripdata_2023-01.parquet`)
    on a local machine from [https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet](https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet).
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet](https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet)下载2023年1月的Yellow
    Taxi行程记录数据Parquet文件(`yellow_tripdata_2023-01.parquet`)到本地机器。
- en: '![Figure 14.2 – The Yellow Taxi Trip Records data for Jan 2023 Parquet file](img/B21378_14_2.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图14.2 – 2023年1月Parquet文件的Yellow Taxi行程记录数据](img/B21378_14_2.jpg)'
- en: Figure 14.2 – The Yellow Taxi Trip Records data for Jan 2023 Parquet file
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.2 – 2023年1月Parquet文件的Yellow Taxi行程记录数据
- en: Download the Taxi Zone Lookup Table CSV file (`taxi+_zone_lookup.csv`) on a
    local machine from [https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv](https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv).
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv](https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv)下载Taxi
    Zone Lookup Table CSV文件(`taxi+_zone_lookup.csv`)到本地机器。
- en: '![Figure 14.3 – The Zone Lookup Table CSV file](img/B21378_14_3.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图14.3 – Zone Lookup Table CSV文件](img/B21378_14_3.jpg)'
- en: Figure 14.3 – The Zone Lookup Table CSV file
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3 – Zone Lookup Table CSV文件
- en: Create the two `yellow_taxi_trip_records` and `zone_lookup` folders in Amazon
    S3, which we can reference in our Glue notebook job.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Amazon S3中创建两个`yellow_taxi_trip_records`和`zone_lookup`文件夹，我们可以在我们的Glue笔记本作业中引用它们。
- en: '![Figure 14.4 – S3 folders structure](img/B21378_14_04.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图14.4 – S3文件夹结构](img/B21378_14_04.jpg)'
- en: Figure 14.4 – S3 folders structure
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4 – S3文件夹结构
- en: Upload the `yellow_tripdata_2023-01.parquet` file to the `yellow_taxi_trip_records`
    folder.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`yellow_tripdata_2023-01.parquet`文件上传到`yellow_taxi_trip_records`文件夹。
- en: '![Figure 14.5 – The yellow_taxi_tripdata_record file](img/B21378_14_05.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图14.5 – yellow_taxi_tripdata_record文件](img/B21378_14_05.jpg)'
- en: Figure 14.5 – The yellow_taxi_tripdata_record file
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5 – yellow_taxi_tripdata_record文件
- en: Upload the `taxi+_zone_lookup.csv` file to the `zone_lookup` folder.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`taxi+_zone_lookup.csv`文件上传到`zone_lookup`文件夹。
- en: '![Figure 14.6 – The zone_lookup file](img/B21378_14_06.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图14.6 – zone_lookup文件](img/B21378_14_06.jpg)'
- en: Figure 14.6 – The zone_lookup file
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6 – zone_lookup文件
- en: Note
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will use the same dataset and use case to discover solutions using AWS Glue
    and Amazon EMR. For illustrative purposes, we have prepared the data manually.
    However, in a production environment, file transfers can be automated by leveraging
    various AWS services and/or third-party software.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的dataset和use case，通过AWS Glue和Amazon EMR来发现解决方案。为了说明目的，我们已经手动准备了数据。然而，在生产环境中，可以通过利用各种AWS服务和/或第三方软件来自动化文件传输。
- en: Now, let’s dive deep into a detailed exploration of the solution using the integration
    of Amazon Q Developer with an AWS Glue Studio notebook for the preceding use case.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨使用Amazon Q开发者与AWS Glue Studio笔记本集成来解决前面用例的详细解决方案。
- en: Solution – Amazon Q Developer with an AWS Glue Studio notebook
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案 – 使用AWS Glue Studio笔记本的Amazon Q开发者
- en: Let’s first enable Amazon Q Developer with an AWS Glue Studio notebook.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先启用 Amazon Q Developer 与 AWS Glue Studio 笔记本。
- en: Prerequisites to enable Amazon Q Developer with an AWS Glue Studio notebook
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用 AWS Glue Studio 笔记本以使用 Amazon Q Developer 的先决条件
- en: The developer is required to modify the **identity and access management** (**IAM**)
    policy associated with the IAM user or role to grant permissions for Amazon Q
    Developer to initiate recommendations in a Glue Studio notebook. Reference [*Chapter
    2*](B21378_02.xhtml#_idTextAnchor022) for the details to enable Amazon Q Developer
    with an AWS Glue Studio notebook.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者需要修改与 IAM 用户或角色关联的 **身份和访问管理**（**IAM**）策略，以授予 Amazon Q Developer 在 Glue Studio
    笔记本中启动建议的权限。参考 [*第 2 章*](B21378_02.xhtml#_idTextAnchor022) 了解启用 Amazon Q Developer
    与 AWS Glue Studio 笔记本的相关细节。
- en: To fulfill the previously mentioned solution blueprint, we will use various
    auto-code generation techniques that were discussed in [*Chapter 3*](B21378_03.xhtml#_idTextAnchor060).
    Mainly, we will focus on single-line prompts, multi-line prompts, and chain-of-thought
    prompts for auto-code generation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现之前提到的解决方案蓝图，我们将使用在 [*第 3 章*](B21378_03.xhtml#_idTextAnchor060) 中讨论的各种自动代码生成技术。主要，我们将专注于单行提示、多行提示和思维链提示用于自动代码生成。
- en: Let’s use Amazon Q Developer to auto-generate an end-to-end script in an AWS
    Glue Studio notebook. Here is the step-by-step solution walk-through for the previously
    defined solution blueprint.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Amazon Q Developer 在 AWS Glue Studio 笔记本中自动生成端到端脚本。以下是之前定义的解决方案蓝图的逐步解决方案说明。
- en: Requirement 1
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求 1
- en: First, you need to write some PySpark code.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要编写一些 PySpark 代码。
- en: While creating a Glue Studio notebook, select the **Spark (Python)** engine
    and the role that has the Amazon Q Developer policy attached.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 Glue Studio 笔记本时，选择 **Spark (Python**) 引擎和附加了 Amazon Q Developer 策略的角色。
- en: '![Figure 14.7 – Create a Glue Studio notebook with PySpark](img/B21378_14_7.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.7 – 使用 PySpark 创建 Glue Studio 笔记本](img/B21378_14_7.jpg)'
- en: Figure 14.7 – Create a Glue Studio notebook with PySpark
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7 – 使用 PySpark 创建 Glue Studio 笔记本
- en: Once you create the notebook, observe the kernel named `Glue PySpark`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建笔记本，观察名为 `Glue PySpark` 的内核。
- en: '![Figure 14.8 – A Glue Studio notebook with the Glue PySpark kernel](img/B21378_14_08.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.8 – 带有 Glue PySpark 内核的 Glue Studio 笔记本](img/B21378_14_08.jpg)'
- en: Figure 14.8 – A Glue Studio notebook with the Glue PySpark kernel
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8 – 带有 Glue PySpark 内核的 Glue Studio 笔记本
- en: Requirement 2
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求 2
- en: Read the `yellow_tripdata_2023-01.parquet` file from the S3 location in a DataFrame
    and display a sample of 10 records.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从 S3 位置读取 `yellow_tripdata_2023-01.parquet` 文件到 DataFrame 中，并显示 10 条记录的样本。
- en: 'Let’s use a chain-of-thought prompt technique with multiple single-line prompts
    in different cells to achieve the preceding requirement:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一系列思维提示技术，在多个单元格中使用多个单行提示来实现前面的要求：
- en: '[PRE0]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Figure 14.9 – PySpark code to read the Yellow Taxi Trip Records data using
    single-line prompts](img/B21378_14_09.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.9 – 使用单行提示读取 Yellow Taxi Trip Records 数据的 PySpark 代码](img/B21378_14_09.jpg)'
- en: Figure 14.9 – PySpark code to read the Yellow Taxi Trip Records data using single-line
    prompts
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9 – 使用单行提示读取 Yellow Taxi Trip Records 数据的 PySpark 代码
- en: Observe that upon entering the Amazon Q Developer-enabled Glue Studio notebook
    prompt, it initiates code recommendations. Q Developer recognizes the file format
    as Parquet and suggests using the `spark.read.parquet` method. You can directly
    execute each cell/code from the notebook. Furthermore, as you move to the next
    cell, Q Developer utilizes “line-by-line recommendations” to suggest displaying
    the schema.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当输入启用 Amazon Q Developer 的 Glue Studio 笔记本提示时，它将启动代码建议。Q Developer 识别文件格式为
    Parquet，并建议使用 `spark.read.parquet` 方法。你可以直接从笔记本中执行每个单元格/代码。此外，当你移动到下一个单元格时，Q Developer
    使用“逐行建议”来建议显示模式。
- en: '![Figure 14.10 – Line-by-line recommendations to display schema](img/B21378_14_10.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.10 – 显示模式的逐行建议](img/B21378_14_10.jpg)'
- en: Figure 14.10 – Line-by-line recommendations to display schema
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10 – 显示模式的逐行建议
- en: Requirement 3
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求 3
- en: Read the `taxi+_zone_lookup.csv` file from the S3 location in a DataFrame and
    display a sample of 10 records.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 从 S3 位置读取 `taxi+_zone_lookup.csv` 文件到 DataFrame 中，并显示 10 条记录的样本。
- en: 'We already explored the chain-of-thought prompt technique with multiple single-line
    prompts for *Requirement 2*. Now, let’s try with a multi-line prompt to achieve
    the preceding requirement and we will try to customize the code for the DataFrame
    name:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了使用多个单行提示的思维链提示技术来满足 *要求 2*。现在，让我们尝试使用多行提示来实现前面的要求，并且我们将尝试为 DataFrame
    名称定制代码：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Figure 14.11 – PySpark code to read the Zone Lookup file using a multi-line
    prompt](img/B21378_14_11.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.11 – 使用多行提示读取区域查找文件的 PySpark 代码](img/B21378_14_11.jpg)'
- en: Figure 14.11 – PySpark code to read the Zone Lookup file using a multi-line
    prompt
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.11 – 使用多行提示读取区域查找文件的 PySpark 代码
- en: Observe that Amazon Q Developer understood the context behind the multi-line
    prompt and also the specific DataFrame name instructed in the prompt. It auto-generated
    multiple lines of code with the DataFrame name as `zone_df` and file format as
    CSV, suggesting the use of the `spark.read.csv` method to read CSV files. You
    can directly execute each cell/code from the notebook.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Amazon Q 开发者理解了多行提示背后的上下文，以及提示中指定的特定 DataFrame 名称。它自动生成了多行代码，DataFrame 名称作为
    `zone_df`，文件格式为 CSV，建议使用 `spark.read.csv` 方法读取 CSV 文件。您可以直接从笔记本中执行每个单元格/代码。
- en: Requirement 4
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求 4
- en: Perform a left outer join on `yellow_tripdata_2023-01.parquet` and `taxi+_zone_lookup.csv`
    on `pulocationid = LocationID` to gather pick-up zone information.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `pulocationid = LocationID` 上对 `yellow_tripdata_2023-01.parquet` 和 `taxi+_zone_lookup.csv`
    执行左外连接以收集取货区域信息。
- en: 'We will continue using multi-line prompts and some code customization to achieve
    the preceding requirement:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用多行提示和一些代码定制来实现前面的要求：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Figure 14.12 – Left outer join df and dataframe zone_df – multi-line prompt](img/B21378_14_12.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.12 – 左外连接 df 和 dataframe zone_df – 多行提示](img/B21378_14_12.jpg)'
- en: Figure 14.12 – Left outer join df and dataframe zone_df – multi-line prompt
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.12 – 左外连接 df 和 dataframe zone_df – 多行提示
- en: Now, let’s review the schema of the DataFrame returns by the code execution.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾代码执行返回的 DataFrame 的模式。
- en: '![Figure 14.13 – Left outer join df and dataframe zone_df – display schema](img/B21378_14_13.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.13 – 左外连接 df 和 dataframe zone_df – 显示模式](img/B21378_14_13.jpg)'
- en: Figure 14.13 – Left outer join df and dataframe zone_df – display schema
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.13 – 左外连接 df 和 dataframe zone_df – 显示模式
- en: Observe that, as instructed in the multi-line prompt, Amazon Q Developer understood
    the context and auto-generated error-free code with the exact specifications we
    provided related to the DataFrame name of `yellow_pu_zone_df`. You can directly
    execute each cell/code from the notebook.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，正如多行提示中所述，Amazon Q 开发者理解了上下文，并自动生成了无错误的代码，与我们提供的有关 `yellow_pu_zone_df` DataFrame
    名称的精确规格完全一致。您可以直接从笔记本中执行每个单元格/代码。
- en: Requirement 5
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求 5
- en: Save the preceding dataset as a CSV file in the preceding Amazon S3 bucket in
    a new folder called `glue_notebook_yellow_pick_up_zone_output`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 将前面的数据集保存为 CSV 文件，在前面 Amazon S3 桶中的新文件夹 `glue_notebook_yellow_pick_up_zone_output`
    中。
- en: 'Since the preceding requirement is straightforward and can be encapsulated
    in a single sentence, we will use a single-line prompt to generate the code, and
    we will also include a header to facilitate easy verification:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前面的要求很简单，可以封装在单个句子中，我们将使用单行提示来生成代码，并且我们还将包括标题以方便验证：
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Figure 14.14 – Save the CSV file with enrichment pick-up location data](img/B21378_14_14.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.14 – 保存包含丰富取货位置数据的 CSV 文件](img/B21378_14_14.jpg)'
- en: Figure 14.14 – Save the CSV file with enrichment pick-up location data
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.14 – 保存包含丰富取货位置数据的 CSV 文件
- en: Requirement 6
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求 6
- en: For verification, download and check files from the `glue_notebook_yellow_pick_up_zone_output`
    folder.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证，从 `glue_notebook_yellow_pick_up_zone_output` 文件夹下载并检查文件。
- en: Let’s go to the Amazon S3 console to verify the files. Select one of the files
    and click **Download**.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们去 Amazon S3 控制台验证文件。选择其中一个文件并点击 **下载**。
- en: '![Figure 14.15 – Save a CSV file with enrichment pick-up location data](img/B21378_14_15.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.15 – 保存包含丰富取货位置数据的 CSV 文件](img/B21378_14_15.jpg)'
- en: Figure 14.15 – Save a CSV file with enrichment pick-up location data
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.15 – 保存包含丰富取货位置数据的 CSV 文件
- en: After downloading the file, you can use any text editor to review the file contents.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 下载文件后，您可以使用任何文本编辑器来查看文件内容。
- en: '![Figure 14.16 – Verify the CSV file with enrichment pick-up location data](img/B21378_14_16.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.16 – 验证包含丰富取货位置数据的 CSV 文件](img/B21378_14_16.jpg)'
- en: Figure 14.16 – Verify the CSV file with enrichment pick-up location data
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.16 – 验证包含丰富取货位置数据的 CSV 文件
- en: Observe that the CSV file has additional columns with zone information based
    on the pick-up location ID. In the next section, we will explore Amazon Q Developer
    integration with AWS Glue and use the chat assistant technique.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 CSV 文件根据取货位置 ID 有额外的区域信息列。在下一节中，我们将探索 Amazon Q 开发者与 AWS Glue 的集成，并使用聊天助手技术。
- en: Think challenge
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战思考
- en: To fulfill *Requirement 6*, if you are interested, attempt to utilize the same
    Glue Studio notebook for reading a CSV file, displaying sample records, and adding
    a header.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足*要求6*，如果你感兴趣，尝试使用相同的Glue Studio笔记本来读取CSV文件，显示样本记录，并添加标题。
- en: '**Hint**: Use the multi-line prompt technique, similar to the one we used when
    reading the Zone Lookup file.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示**：使用多行提示技术，类似于我们在读取区域查找文件时使用的技术。'
- en: Solution – Amazon Q Developer with AWS Glue
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案 – Amazon Q开发者与AWS Glue
- en: Amazon Q Developer provides a chat-style interface in the AWS Glue console.
    Now, let’s explore the integration between Amazon Q Developer and AWS Glue for
    the same use case and solution blueprint that we handled using Amazon Q Developer
    and an AWS Glue Studio notebook integration.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Q开发者提供了AWS Glue控制台中的聊天式界面。现在，让我们探索Amazon Q开发者与AWS Glue之间的集成，以及我们使用Amazon
    Q开发者和AWS Glue Studio笔记本集成所处理的相同用例和解决方案蓝图。
- en: Let’s now look at the prerequisites to enable Amazon Q with AWS Glue.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看启用Amazon Q与AWS Glue的先决条件。
- en: To enable Amazon Q Developer integration with AWS Glue, we will need to update
    the IAM policy. Please refer to [*Chapter 2*](B21378_02.xhtml#_idTextAnchor022)
    for additional details on initiating interaction with Amazon Q in AWS Glue.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用Amazon Q开发者与AWS Glue的集成，我们需要更新IAM策略。请参阅[*第2章*](B21378_02.xhtml#_idTextAnchor022)以获取在AWS
    Glue中启动与Amazon Q交互的更多详细信息。
- en: Now, let’s dive deep into a detailed exploration of the integration of Amazon
    Q Developer with AWS Glue Studio for the preceding use case.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨Amazon Q开发者与AWS Glue Studio集成的前述用例。
- en: To fulfill the mentioned requirements, we will mainly use the chat companion
    that was discussed in [*Chapter 3*](B21378_03.xhtml#_idTextAnchor060).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足提到的要求，我们将主要使用在第[*第3章*](B21378_03.xhtml#_idTextAnchor060)中讨论的聊天伴侣。
- en: 'Here is a step-by-step solution walk-through that we’ll use as a prompt for
    all of the preceding requirements:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个逐步的解决方案演示，我们将使用它作为前面所有要求的提示：
- en: '[PRE4]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure 14.17 – The AWS Glue ETL code suggested by Amazon Q Developer – part
    1](img/B21378_14_17.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图14.17 – Amazon Q开发者建议的AWS Glue ETL代码 – 第1部分](img/B21378_14_17.jpg)'
- en: Figure 14.17 – The AWS Glue ETL code suggested by Amazon Q Developer – part
    1
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.17 – Amazon Q开发者建议的AWS Glue ETL代码 – 第1部分
- en: You can see that, based on the instruction provided to Amazon Q, it generated
    the skeleton on the ETL code. It generated code structure with Glue-PySpark libraries,
    a s3node with create dynamic dataframe to read parquet file, and a s3node with
    write dynamic dataframe to write CSV file.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，根据提供给Amazon Q的指令，它生成了ETL代码的框架。它生成了使用Glue-PySpark库的代码结构，一个创建动态dataframe以读取parquet文件的s3node，以及一个写入动态dataframe以写入CSV文件的s3node。
- en: '![Figure 14.18 – AWS Glue ETL code suggested by Amazon Q Developer – part 2](img/B21378_14_18.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图14.18 – Amazon Q开发者建议的AWS Glue ETL代码 – 第2部分](img/B21378_14_18.jpg)'
- en: Figure 14.18 – AWS Glue ETL code suggested by Amazon Q Developer – part 2
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.18 – Amazon Q开发者建议的AWS Glue ETL代码 – 第2部分
- en: Observe that Amazon Q also provided technical details to explain the script
    flow. This can also be used to meet the in-script documentation needs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到Amazon Q还提供了技术细节来解释脚本流程。这也可以用来满足脚本中的文档需求。
- en: '![Figure 14.19 – AWS Glue ETL code suggested by Amazon Q Developer – script
    summary](img/B21378_14_19.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图14.19 – Amazon Q开发者建议的AWS Glue ETL代码 – 脚本摘要](img/B21378_14_19.jpg)'
- en: Figure 14.19 – AWS Glue ETL code suggested by Amazon Q Developer – script summary
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.19 – Amazon Q开发者建议的AWS Glue ETL代码 – 脚本摘要
- en: Data engineers with coding experience can easily reference the script summary
    and script skeleton to write end-to-end scripts to meet the solution blueprint.
    LLMs, by nature, are non-deterministic, so you may not get the same code blocks
    shown in the code snapshots.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 具有编码经验的数据工程师可以轻松地参考脚本摘要和脚本框架来编写端到端的脚本以满足解决方案蓝图。由于LLMs本质上是非确定性的，所以你可能不会得到代码快照中显示的相同代码块。
- en: Based on the preceding use case illustration, AWS Glue integration with Amazon
    Q Developer with prompting techniques can be used by data engineers at a relatively
    lower experience level, while AWS Glue integration with Amazon Q Developer using
    the chat assistant can be utilized by ETL developers with relatively more experience.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的用例说明，AWS Glue与Amazon Q开发者集成并使用提示技术可以由经验相对较低的数据工程师使用，而使用聊天助手与AWS Glue集成的Amazon
    Q开发者可以由经验相对较多的ETL开发者利用。
- en: Summary – Amazon Q Developer with an AWS Glue Studio notebook
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要 – Amazon Q开发者与AWS Glue Studio笔记本
- en: As illustrated, we can automatically generate end-to-end, error-free, and executable
    code simply by providing prompts with specific requirements. Amazon Q Developer,
    integrated with an AWS Glue Studio notebook, comprehends the context and automatically
    generates PySpark code that can be run directly from the notebook without the
    need to provision any hardware upfront. This marks a significant advancement for
    many data engineers, relieving them from concerns about the technical intricacies
    associated with PySpark libraries, methods, and syntax.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，我们只需提供具有特定要求的提示，就可以自动生成端到端、无错误且可执行的代码。Amazon Q 开发者与 AWS Glue Studio 笔记本集成，理解上下文并自动生成可以直接从笔记本中运行的
    PySpark 代码，无需预先配置任何硬件。这对许多数据工程师来说是一个重大的进步，使他们摆脱了与 PySpark 库、方法和语法相关的技术复杂性。
- en: Next, we will explore code assistance integration with Amazon EMR.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨与 Amazon EMR 的代码辅助集成。
- en: Code assistance integration with Amazon EMR
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 Amazon EMR 的代码辅助集成
- en: 'Before we dive deep into the details of code assistance support for Amazon
    EMR, let’s quickly go through an overview of Amazon EMR. **Amazon EMR** is a cloud-based
    big data platform that simplifies the deployment, management, and scaling of various
    big data frameworks such as Apache Hadoop, Apache Spark, Apache Hive, and Apache
    HBase. At a high level, Amazon EMR comprises the following major components, each
    with multiple features to support data engineers and data scientists:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨 Amazon EMR 的代码辅助支持的细节之前，让我们快速浏览一下 Amazon EMR 的概述。**Amazon EMR** 是一个基于云的大数据平台，简化了各种大数据框架（如
    Apache Hadoop、Apache Spark、Apache Hive 和 Apache HBase）的部署、管理和扩展。在较高层次上，Amazon
    EMR 包含以下主要组件，每个组件都有多个功能来支持数据工程师和数据科学家：
- en: '**EMR on EC2/EKS**: The Amazon EMR service provides two options, EMR on EC2
    and EMR on EKS, allowing customers to provision clusters. Amazon EMR streamlines
    the execution of batch jobs and interactive workloads for data analysts and engineers.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EMR on EC2/EKS**：Amazon EMR 服务提供了两种选项，即 EMR on EC2 和 EMR on EKS，允许客户配置集群。Amazon
    EMR 简化了数据分析师和工程师执行批量作业和交互式工作负载的过程。'
- en: '**EMR Serverless**: Amazon EMR Serverless is a serverless alternative within
    Amazon EMR. With Amazon EMR Serverless, users can access the full suite of features
    and advantages offered by Amazon EMR, all without requiring specialized expertise
    for cluster planning and management.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EMR Serverless**：Amazon EMR Serverless 是 Amazon EMR 内的一个无服务器替代方案。使用 Amazon
    EMR Serverless，用户可以访问 Amazon EMR 提供的完整功能集和优势，而无需对集群规划和管理工作具有专业知识。'
- en: '**EMR Studio**: EMR Studio supports data engineers and data scientists in developing,
    visualizing, and debugging applications within an IDE. It also provides a Jupyter
    Notebook environment for interactive coding.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EMR Studio**：EMR Studio 支持数据工程师和数据科学家在 IDE 内开发、可视化和调试应用程序。它还提供了一个 Jupyter
    Notebook 环境用于交互式编码。'
- en: Use case for Amazon EMR Studio
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon EMR Studio 的用例
- en: For simplicity and ease of following Amazon Q Developer integration with Amazon
    EMR, we will use the same use case and data that we used in this chapter under
    the *Code assistance integration with AWS Glue* section. Refer to the *Use case
    for AWS Glue* section, which covers details related to the solution blueprint
    and data preparation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单和便于跟踪 Amazon Q 开发者与 Amazon EMR 的集成，我们将使用本章在 *代码辅助集成与 AWS Glue* 部分中使用的相同用例和数据。请参阅
    *AWS Glue 的用例* 部分，该部分涵盖了有关解决方案蓝图和数据准备的相关细节。
- en: Solution – Amazon Q Developer with Amazon EMR Studio
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案 – Amazon Q 开发者与 Amazon EMR Studio
- en: Let’s first enable Amazon Q Developer with Amazon EMR Studio. To enable Amazon
    Q Developer integration with Amazon EMR Studio, we will need to update the IAM
    policy.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先启用 Amazon Q 开发者与 Amazon EMR Studio。要启用 Amazon Q 开发者与 Amazon EMR Studio 的集成，我们需要更新
    IAM 策略。
- en: Prerequisite to enable Amazon Q Developer with Amazon EMR Studio
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用 Amazon Q 开发者与 Amazon EMR Studio 的先决条件
- en: The developer is required to modify the IAM policy associated with the role
    to grant permissions for Amazon Q Developer to initiate recommendations in EMR
    Studio. Please reference [*Chapter 2*](B21378_02.xhtml#_idTextAnchor022) for additional
    details on initiating interaction with Amazon Q Developer in Amazon EMR Studio.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者需要修改与该角色关联的 IAM 策略，以授予 Amazon Q 开发者在 EMR Studio 中发起推荐的权限。请参考[*第 2 章*](B21378_02.xhtml#_idTextAnchor022)以获取有关在
    Amazon EMR Studio 中与 Amazon Q 开发者进行交互的更多详细信息。
- en: To fulfill the mentioned requirements, we will use various auto-code generation
    techniques that were discussed in [*Chapter 3*](B21378_03.xhtml#_idTextAnchor060).
    Mainly, we will focus on single-line prompts, multi-line prompts, and chain-of-thought
    prompts for auto-code generation techniques.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足提到的需求，我们将使用在 [*第 3 章*](B21378_03.xhtml#_idTextAnchor060) 中讨论的各种自动代码生成技术。主要，我们将重点关注单行提示、多行提示和思考链提示用于自动代码生成技术。
- en: Let’s use Amazon Q Developer to auto-generate end-to-end scripts, which can
    achieve the following requirements in Amazon EMR Studio. Here is a step-by-step
    solution walk-through of the solution.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Amazon Q 开发者来自动生成端到端脚本，这些脚本可以在 Amazon EMR Studio 中实现以下需求。以下是解决方案的逐步解决方案概述。
- en: Note
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 备注
- en: You can observe lots of similarities between a Glue Studio notebook and an EMR
    Studio notebook when it comes to code recommended by Amazon Q Developer.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到 Amazon Q 开发者推荐代码时，你可以观察到 Glue Studio 笔记本和 EMR Studio 笔记本之间有很多相似之处。
- en: Requirement 1
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 需求 1
- en: You will need to write a PySpark code to handle technical requirements.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你将需要编写 PySpark 代码来处理技术需求。
- en: Once you open Amazon EMR Studio, use **Launcher** to select **PySpark** from
    the **Notebook** section.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你打开 Amazon EMR Studio，使用 **启动器** 从 **笔记本** 部分选择 **PySpark**。
- en: '![Figure 14.20 – Create an EMR Studio notebook with PySpark](img/B21378_14_20.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.20 – 使用 PySpark 创建 EMR Studio 笔记本](img/B21378_14_20.jpg)'
- en: Figure 14.20 – Create an EMR Studio notebook with PySpark
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.20 – 使用 PySpark 创建 EMR Studio 笔记本
- en: Once you create the notebook, you can see a kernel named `PySpark`. The kernel
    is a standalone process that runs in the background and executes the code you
    write in your notebooks. For more information, refer to the *References* section
    at the end of the chapter.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建笔记本，你将看到一个名为 `PySpark` 的内核。内核是一个在后台运行的独立进程，它执行你在笔记本中编写的代码。有关更多信息，请参阅本章末尾的
    *参考文献* 部分。
- en: '![Figure 14.21 – The EMR Studio notebook with a PySpark kernel](img/B21378_14_21.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.21 – 带有 PySpark 内核的 EMR Studio 笔记本](img/B21378_14_21.jpg)'
- en: Figure 14.21 – The EMR Studio notebook with a PySpark kernel
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.21 – 带有 PySpark 内核的 EMR Studio 笔记本
- en: I have already attached a cluster to my notebook, but you can explore different
    options to attach the compute to EMR studio in AWS documentation at [https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经将一个集群附加到我的笔记本上了，但你可以在 AWS 文档中探索不同的选项来将计算附加到 EMR Studio，请参阅[https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html)。
- en: Requirement 2
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 需求 2
- en: Read the `yellow_tripdata_2023-01.parquet` file from the S3 location in a DataFrame
    and display a sample of 10 records.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从 S3 位置读取 `yellow_tripdata_2023-01.parquet` 文件到 DataFrame 中，并显示 10 条记录的样本。
- en: 'Let’s use a chain-of-thought prompts technique with multiple single-line prompts
    in different cells to achieve this requirement:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个在不同单元格中包含多个单行提示的思考链提示技术来实现这个需求：
- en: '[PRE5]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Figure 14.22 – PySpark code to read the Yellow Taxi Trip Records data using
    single-line prompts](img/B21378_14_22.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.22 – 使用单行提示读取 Yellow Taxi Trip Records 数据的 PySpark 代码](img/B21378_14_22.jpg)'
- en: Figure 14.22 – PySpark code to read the Yellow Taxi Trip Records data using
    single-line prompts
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.22 – 使用单行提示读取 Yellow Taxi Trip Records 数据的 PySpark 代码
- en: Observe that upon entering the Amazon Q Developer-enabled EMR Studio notebook
    prompt, it initiates code recommendations. Amazon Q Developer recognizes the file
    format as Parquet and suggests using the `spark.read.parquet` method. You can
    directly execute each cell/code from the notebook. Furthermore, as you move to
    the next cell, Amazon Q Developer utilizes “line-by-line recommendations” to suggest
    displaying the schema.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当输入 Amazon Q 开发者启用的 EMR Studio 笔记本提示时，它将启动代码推荐。Amazon Q 开发者识别文件格式为 Parquet，并建议使用
    `spark.read.parquet` 方法。你可以直接从笔记本中执行每个单元格/代码。此外，当你移动到下一个单元格时，Amazon Q 开发者利用“逐行推荐”来建议显示模式。
- en: Requirement 3
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 需求 3
- en: Read the `taxi+_zone_lookup.csv` file from the S3 location in a DataFrame and
    display a sample of 10 records.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从 S3 位置读取 `taxi+_zone_lookup.csv` 文件到 DataFrame 中，并显示 10 条记录的样本。
- en: 'We already explored the chain-of-thought prompts technique with multiple single-line
    prompts for *Requirement 2*. Now, let’s try a multi-line prompt to achieve this
    requirement and we will try to customize the code for the DataFrame name:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了使用多个单行提示的思考链提示技术来处理 *需求 2*。现在，让我们尝试一个多行提示来实现这个需求，并且我们将尝试为 DataFrame 名称定制代码：
- en: '[PRE6]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Figure 14.23 – PySpark code to read the Zone Lookup file using a multi-line
    prompt](img/B21378_14_23.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图14.23 – 使用多行提示读取区域查找文件的PySpark代码](img/B21378_14_23.jpg)'
- en: Figure 14.23 – PySpark code to read the Zone Lookup file using a multi-line
    prompt
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.23 – 使用多行提示读取区域查找文件的PySpark代码
- en: Observe that Amazon Q Developer understood the context behind the multi-line
    prompt and also the specific DataFrame name instructed in the prompt. It auto-generated
    multiple lines of code with the DataFrame name of `zone_df` and file format as
    CSV, suggesting the use of the `spark.read.csv` method to read CSV files. You
    can directly execute each cell/code from the notebook.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到Amazon Q Developer理解了多行提示背后的上下文，以及提示中指定的特定DataFrame名称。它自动生成了多行代码，DataFrame名称为`zone_df`，文件格式为CSV，建议使用`spark.read.csv`方法读取CSV文件。您可以直接从笔记本中执行每个单元格/代码。
- en: Requirement 4
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 需求4
- en: Perform a left outer join on `yellow_tripdata_2023-01.parquet` and `taxi+_zone_lookup.csv`
    on `pulocationid = LocationID` to gather pick-up zone information.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在`yellow_tripdata_2023-01.parquet`和`taxi+_zone_lookup.csv`上执行左外连接，基于`pulocationid
    = LocationID`来收集接车区域信息。
- en: 'We will continue using multi-line prompts and some code customization to achieve
    the preceding requirement:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用多行提示和一些代码定制来实现上述需求：
- en: '[PRE7]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 14.24 – Left outer join df and dataframe zone_df – multi-line prompt](img/B21378_14_24.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图14.24 – 左外连接df和dataframe zone_df – 多行提示](img/B21378_14_24.jpg)'
- en: Figure 14.24 – Left outer join df and dataframe zone_df – multi-line prompt
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.24 – 左外连接df和dataframe zone_df – 多行提示
- en: Now, let’s review the schema of the DataFrame printed by the code.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下代码打印出的DataFrame的架构。
- en: '![Figure 14.25 – Left outer join df and dataframe zone_df – display schema](img/B21378_14_25.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图14.25 – 左外连接df和dataframe zone_df – 显示架构](img/B21378_14_25.jpg)'
- en: Figure 14.25 – Left outer join df and dataframe zone_df – display schema
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.25 – 左外连接df和dataframe zone_df – 显示架构
- en: Observe that, as instructed in the multi-line prompt, Amazon Q Developer understood
    the context and auto-generated error-free code with the exact specifications we
    provided related to the DataFrame named `yellow_pu_zone_df`. You can directly
    execute each cell/code from the notebook.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到，正如多行提示中所述，Amazon Q Developer理解了上下文，并自动生成了无错误的代码，这些代码与我们提供的有关名为`yellow_pu_zone_df`的DataFrame的精确规格完全一致。您可以直接从笔记本中执行每个单元格/代码。
- en: Requirement 5
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 需求5
- en: Save the preceding dataset as a CSV file in the previous Amazon S3 bucket in
    a new folder called `glue_notebook_yellow_pick_up_zone_output`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 将上述数据集保存为CSV文件，存放在之前Amazon S3桶中的新文件夹`glue_notebook_yellow_pick_up_zone_output`中。
- en: 'Since this requirement is straightforward and can be encapsulated in a single
    sentence, we will use a single-line prompt to generate the code, and we will also
    include a header to facilitate easy verification:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个需求很简单，可以封装在单个句子中，我们将使用单行提示来生成代码，并且我们还将包括一个标题以方便验证：
- en: '[PRE8]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Figure 14.26 – Save the CSV file with enrichment pick-up location data](img/B21378_14_26.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图14.26 – 保存包含丰富接车位置数据的CSV文件](img/B21378_14_26.jpg)'
- en: Figure 14.26 – Save the CSV file with enrichment pick-up location data
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.26 – 保存包含丰富接车位置数据的CSV文件
- en: Requirement 6
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 需求6
- en: For verification, download and check files from the `glue_notebook_yellow_pick_up_zone_output`
    folder.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证，从`glue_notebook_yellow_pick_up_zone_output`文件夹下载并检查文件。
- en: Let’s go to the Amazon S3 console to verify the files. Select one of the files
    and click **Download**.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们去Amazon S3控制台验证文件。选择其中一个文件，然后点击**下载**。
- en: '![Figure 14.27 – Verify final result set – Amazon Q Developer with Amazon EMR
    Studio](img/B21378_14_27.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图14.27 – 验证最终结果集 – Amazon Q Developer与Amazon EMR Studio](img/B21378_14_27.jpg)'
- en: Figure 14.27 – Verify final result set – Amazon Q Developer with Amazon EMR
    Studio
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.27 – 验证最终结果集 – Amazon Q Developer与Amazon EMR Studio
- en: After downloading, you can use a text editor to review the file contents.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 下载后，您可以使用文本编辑器来查看文件内容。
- en: '![Figure 14.28 – Verify the CSV file contents of Amazon Q Developer with Amazon
    EMR Studio](img/B21378_14_28.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图14.28 – 验证Amazon Q Developer与Amazon EMR Studio的CSV文件内容](img/B21378_14_28.jpg)'
- en: Figure 14.28 – Verify the CSV file contents of Amazon Q Developer with Amazon
    EMR Studio
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.28 – 验证Amazon Q Developer与Amazon EMR Studio的CSV文件内容
- en: Observe that the CSV file has additional columns with zone information based
    on the pick-up location ID.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到CSV文件有基于接车位置ID的额外区域信息列。
- en: Summary – Amazon Q Developer with Amazon EMR Studio
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要 – Amazon Q Developer与Amazon EMR Studio
- en: As illustrated, we can automatically generate end-to-end, error-free, and executable
    code simply by providing prompts with specific requirements. Amazon Q Developer,
    integrated with an Amazon EMR Studio notebook, comprehends the context and automatically
    generates PySpark code that can be run directly from the notebook. This marks
    a significant advancement for many data engineers, relieving them from concerns
    about the technical intricacies associated with PySpark libraries, methods, and
    syntax.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，我们只需提供具有特定要求的提示，就可以自动生成端到端、无错误且可执行的代码。Amazon Q 开发者，与 Amazon EMR Studio 笔记本集成，理解上下文并自动生成可以直接从笔记本运行的
    PySpark 代码。这对许多数据工程师来说是一个重大进步，使他们免于担心与 PySpark 库、方法和语法相关的技术复杂性。
- en: Think challenge
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战思考
- en: To fulfill *Requirement 6*, if you are interested, attempt to utilize the same
    EMR Studio notebook for reading a CSV file, displaying sample records, and adding
    a header.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 要满足 *要求 6*，如果您感兴趣，尝试利用相同的 EMR Studio 笔记本读取 CSV 文件、显示样本记录并添加标题。
- en: '**Hint**: Use the multi-line prompt technique, similar to the one we used when
    reading the Zone Lookup file.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**提示**：使用多行提示技术，类似于我们在读取 Zone Lookup 文件时使用的技术。'
- en: In the next section, we will consider an application developer persona to explore
    code assistance integration with AWS Lambda.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将考虑应用程序开发人员的角色，以探索 AWS Lambda 与代码辅助的集成。
- en: Code assistance integration with AWS Lambda
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS Lambda 与代码辅助的集成
- en: Before we start diving deep into code assistance support for the AWS Lambda
    service, let’s quickly go through an overview of AWS Lambda. **AWS Lambda** is
    a serverless computing service that allows users to run code without provisioning
    or managing servers. With Lambda, you can upload your code or use the available
    editor from the Lambda console. During the runtime of the code, based on the provided
    configurations, the service automatically takes care of the compute resources
    needed for execution. It is designed to be highly scalable, cost effective, and
    suitable for event-driven applications.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨 AWS Lambda 服务代码辅助支持之前，让我们快速了解一下 AWS Lambda 的概述。**AWS Lambda** 是一种无服务器计算服务，允许用户在不配置或管理服务器的情况下运行代码。使用
    Lambda，您可以从 Lambda 控制台上传您的代码或使用可用的编辑器。在代码运行期间，根据提供的配置，服务会自动处理执行所需的计算资源。它旨在高度可扩展、成本效益高，且适用于事件驱动型应用。
- en: AWS Lambda supports multiple programming languages, including Node.js, Python,
    Java, Go, and .NET Core, allowing you to choose the language that best fits your
    application. Lambda can be easily integrated with other AWS services, enabling
    you to build complex and scalable architectures. It works seamlessly with services
    such as Amazon S3, DynamoDB, and API Gateway.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Lambda 支持多种编程语言，包括 Node.js、Python、Java、Go 和 .NET Core，允许您选择最适合您应用程序的语言。Lambda
    可以轻松集成到其他 AWS 服务中，使您能够构建复杂且可扩展的架构。它与 Amazon S3、DynamoDB 和 API Gateway 等服务无缝协作。
- en: The AWS Lambda console is integrated with Amazon Q Developer to make it easy
    for developers to get coding assistance/recommendations.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Lambda 控制台与 Amazon Q 开发者集成，使开发者能够轻松获得编码辅助/建议。
- en: Use case for AWS Lambda
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Lambda 的用例
- en: Let’s start with one of the easy and widely used use cases of converting file
    format.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从转换文件格式的一个简单且广泛使用的用例开始。
- en: '**File format conversion**: In a typical scenario, once a file is received
    from an external team and/or source, it may not be in the target location and
    have the required name expected by the application. In that case, AWS Lambda can
    be used to quickly copy the file from the source location to the target location
    and rename the file at the target location.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**文件格式转换**：在典型场景中，一旦从外部团队和/或来源收到文件，它可能不在目标位置，也不具有应用程序期望的所需名称。在这种情况下，可以使用 AWS
    Lambda 快速将文件从源位置复制到目标位置，并在目标位置重命名文件。'
- en: To illustrate this use case, let’s copy the NY Taxi Zone lookup file from the
    source location (`s3://<your-bucket-name>/zone_lookup/`) to the target location
    (`s3://<your-bucket-name>/source_lookup_file/`). Also, remove the special character
    (`+`) from the filename to save it as `taxi_zone_lookup.csv`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个用例，让我们将 NY Taxi Zone 查找文件从源位置 (`s3://<your-bucket-name>/zone_lookup/`)
    复制到目标位置 (`s3://<your-bucket-name>/source_lookup_file/`)。同时，从文件名中删除特殊字符 (`+`)，将其保存为
    `taxi_zone_lookup.csv`。
- en: To meet this requirement, application developers must develop a Python script.
    This script should copy and rename the Zone Lookup file from the source to the
    target location.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这一要求，应用程序开发者必须开发一个 Python 脚本。此脚本应将区域查找文件从源位置复制并重命名到目标位置。
- en: As a code developer / data engineer, you will need to convert the preceding
    business objectives into the solution blueprint.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 作为代码开发者/数据工程师，您需要将前面的业务目标转换为解决方案蓝图。
- en: Solution blueprint
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案蓝图
- en: Write a Python script to handle technical requirements.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个 Python 脚本来处理技术需求。
- en: Copy the `taxi+_zone_lookup.csv` file from S3 to the `zone_lookup` folder to
    the `source_lookup_file` folder.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `taxi+_zone_lookup.csv` 文件从 S3 复制到 `zone_lookup` 文件夹，然后到 `source_lookup_file`
    文件夹。
- en: During copying, change `taxi+_zone_lookup.csv` to `taxi_zone_lookup.csv` in
    the target `source_lookup_file` folder.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在复制过程中，将目标 `source_lookup_file` 文件夹中的 `taxi+_zone_lookup.csv` 改为 `taxi_zone_lookup.csv`。
- en: For verification, check the contents of the `source_lookup_file/taxi_zone_lookup.csv`
    file.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了验证，检查 `source_lookup_file/taxi_zone_lookup.csv` 文件的内容。
- en: Now that we have a use case defined, let’s go through the step-by-step solution
    for it.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经定义了用例，让我们逐步分析其解决方案。
- en: Data preparation
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: We are using the same lookup file that we provisioned in this chapter under
    the *Code assistance integration with AWS Glue* section. Please refer to the *Use
    case for AWS Glue* section, which covers details related to data preparation.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用本章在“*与 AWS Glue 集成的代码辅助*”部分中配置的相同查找文件。请参阅“*AWS Glue 用例*”部分，该部分涵盖了与数据准备相关的详细信息。
- en: Solution – Amazon Q Developer with AWS Lambda
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案 – Amazon Q 开发者与 AWS Lambda
- en: Let’s first enable Amazon Q Developer with the AWS Lambda console. To enable
    Amazon Q Developer integration with AWS Lambda, we will need to update the IAM
    policy.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先启用 AWS Lambda 控制台中的 Amazon Q 开发者。为了启用 AWS Lambda 与 Amazon Q 开发者的集成，我们需要更新
    IAM 策略。
- en: Prerequisite to enable Amazon Q Developer with AWS Lambda
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用 Amazon Q 开发者与 AWS Lambda 的先决条件
- en: The developer is required to modify the IAM policy associated with the IAM user
    or role to grant permissions for Amazon Q Developer to initiate recommendations
    in the AWS Lambda console. Please reference [*Chapter 2*](B21378_02.xhtml#_idTextAnchor022)
    for additional details on initiating interaction with Amazon Q Developer in AWS
    Lambda.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者需要修改与 IAM 用户或角色关联的 IAM 策略，以授予 Amazon Q 开发者通过 AWS Lambda 控制台发起推荐的权限。请参考[*第
    2 章*](B21378_02.xhtml#_idTextAnchor022)以获取有关在 AWS Lambda 中与 Amazon Q 开发者进行交互的更多详细信息。
- en: To let Amazon Q Developer start code suggestions, make sure to choose **Tools**
    | **Amazon CodeWhisperer** **Code Suggestions**.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要让 Amazon Q 开发者开始代码建议，请确保选择**工具** | **Amazon CodeWhisperer 代码建议**。
- en: '![Figure 14.29 – AWS Lambda console with Amazon Q Developer for Python runtime](img/B21378_14_29.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.29 – AWS Lambda 控制台中的 Amazon Q 开发者用于 Python 运行时](img/B21378_14_29.jpg)'
- en: Figure 14.29 – AWS Lambda console with Amazon Q Developer for Python runtime
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.29 – AWS Lambda 控制台中的 Amazon Q 开发者用于 Python 运行时
- en: To fulfill the mentioned requirements, we will use auto-code generation techniques
    that were discussed in [*Chapter 3*](B21378_03.xhtml#_idTextAnchor060). Mainly,
    we will focus on the multi-line prompt for auto-code generation. Let’s use Amazon
    Q Developer to auto-generate an end-to-end script that can achieve the following
    requirements in AWS Lambda Console and EMR Studio. Here is a step-by-step solution
    walk-through of the solution.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足提到的要求，我们将使用在第 [*第 3 章*](B21378_03.xhtml#_idTextAnchor060) 中讨论的自动代码生成技术。主要，我们将关注多行提示自动代码生成。让我们使用
    Amazon Q 开发者来自动生成一个端到端脚本，该脚本可以在 AWS Lambda 控制台和 EMR Studio 中实现以下要求。以下是解决方案的逐步解决方案概述。
- en: Requirement 1
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 需求 1
- en: You need to write a Python script to handle technical requirements.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要编写一个 Python 脚本来处理技术需求。
- en: Once you open the AWS Lambda console, use the launcher to select a Python runtime.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦打开 AWS Lambda 控制台，使用启动器选择一个 Python 运行时。
- en: '![Figure 14.30 – Create a Python runtime from AWS Lambda](img/B21378_14_30.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.30 – 从 AWS Lambda 创建 Python 运行时](img/B21378_14_30.jpg)'
- en: Figure 14.30 – Create a Python runtime from AWS Lambda
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.30 – 从 AWS Lambda 创建 Python 运行时
- en: Once you successfully create a Lambda function, observe that AWS Lambda creates
    a `lambda_function.py` file with some sample code. We can safely delete the sample
    code for this exercise, as we will use Amazon Q Developer to generate end-to-end
    code.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成功创建 Lambda 函数，您会注意到 AWS Lambda 创建了一个包含一些示例代码的 `lambda_function.py` 文件。对于这个练习，我们可以安全地删除示例代码，因为我们将会使用
    Amazon Q 开发者来生成端到端代码。
- en: '![Figure 14.31 – The AWS Lambda console with Amazon Q Developer for Python
    runtime](img/B21378_14_31.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.31 – AWS Lambda 控制台与 Python 运行时的 Amazon Q 开发者](img/B21378_14_31.jpg)'
- en: Figure 14.31 – The AWS Lambda console with Amazon Q Developer for Python runtime
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.31 – AWS Lambda 控制台与 Python 运行时的 Amazon Q 开发者
- en: Let’s combine *Requirements 2* and *3*, as we are planning to use a multi-line
    prompt.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 *要求 2* 和 *3* 结合起来，因为我们计划使用多行提示。
- en: Requirements 2 and 3
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求 2 和 3
- en: Copy the `taxi+_zone_lookup.csv` file from S3 to the `zone_lookup` folder to
    the `source_lookup_file` folder.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `taxi+_zone_lookup.csv` 文件从 S3 复制到 `source_lookup_file` 文件夹中的 `zone_lookup`
    文件夹。
- en: During copying, change the file name from `taxi+_zone_lookup.csv` to `taxi_zone_lookup.csv`
    in the target `source_lookup_file` folder.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在复制过程中，将目标 `source_lookup_file` 文件夹中的文件名从 `taxi+_zone_lookup.csv` 更改为 `taxi_zone_lookup.csv`。
- en: 'Let’s use multi-line prompts to auto-generate the code:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用多行提示来自动生成代码：
- en: '[PRE9]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Figure 14.32 – Amazon Q Developer generated code for the AWS Lambda console](img/B21378_14_32.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.32 – Amazon Q 开发者为 AWS Lambda 控制台生成的代码](img/B21378_14_32.jpg)'
- en: Figure 14.32 – Amazon Q Developer generated code for the AWS Lambda console
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.32 – Amazon Q 开发者为 AWS Lambda 控制台生成的代码
- en: Observe that Amazon Q Developer created a `lambda_handler` function and added
    `return code of 200` with a success message.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到 Amazon Q 开发者创建了一个 `lambda_handler` 函数，并添加了 `返回代码 200` 和成功消息。
- en: Requirement 4
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求 4
- en: For verification, check the contents of the `source_lookup_file/taxi_zone_lookup.csv`
    file.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证，请检查 `source_lookup_file/taxi_zone_lookup.csv` 文件的内容。
- en: Let’s deploy and use a test event to run Lambda code generated by Amazon Q Developer.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们部署并使用测试事件来运行 Amazon Q 开发者生成的 Lambda 代码。
- en: '![Figure 14.33 – Deploy AWS Lambda code](img/B21378_14_33.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.33 – 部署 AWS Lambda 代码](img/B21378_14_33.jpg)'
- en: Figure 14.33 – Deploy AWS Lambda code
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.33 – 部署 AWS Lambda 代码
- en: Now, let’s test the code by going to the **Test** tab and clicking the **Test**
    button. Since we are not passing any values to this Lambda function, the JSON
    event values from the **Test** tab do not matter in our case.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过转到 **测试** 选项卡并点击 **测试** 按钮来测试代码。由于我们没有向此 Lambda 函数传递任何值，因此 **测试** 选项卡中的
    JSON 事件值在我们的情况下并不重要。
- en: '![Figure 14.34 – Test AWS Lambda code](img/B21378_14_34.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.34 – 测试 AWS Lambda 代码](img/B21378_14_34.jpg)'
- en: Figure 14.34 – Test AWS Lambda code
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.34 – 测试 AWS Lambda 代码
- en: Once the Lambda code executes successfully, it will provide you with the details
    of the execution. Observe that the code is executed successfully and displays
    the returned code with a success message.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Lambda 代码成功执行，它将为您提供执行详情。观察代码执行成功，并显示带有成功消息的返回代码。
- en: '![Figure 14.35 – AWS Lambda code execution](img/B21378_14_35.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.35 – AWS Lambda 代码执行](img/B21378_14_35.jpg)'
- en: Figure 14.35 – AWS Lambda code execution
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.35 – AWS Lambda 代码执行
- en: Let’s use the Amazon S3 console to download and verify `s3://<your-bucket-name>/source_lookup_file/taxi_zone_lookup.csv`.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Amazon S3 控制台下载并验证 `s3://<your-bucket-name>/source_lookup_file/taxi_zone_lookup.csv`。
- en: '![Figure 14.36 – Target lookup file from Amazon S3](img/B21378_14_36.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.36 – 来自 Amazon S3 的目标查找文件](img/B21378_14_36.jpg)'
- en: Figure 14.36 – Target lookup file from Amazon S3
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.36 – 来自 Amazon S3 的目标查找文件
- en: '![Figure 14.37 – The Zone Lookup file](img/B21378_14_37.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.37 – 区域查找文件](img/B21378_14_37.jpg)'
- en: Figure 14.37 – The Zone Lookup file
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.37 – 区域查找文件
- en: Summary – Amazon Q Developer with AWS Lambda
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要 – Amazon Q 开发者与 AWS Lambda
- en: As illustrated, we can automatically generate end-to-end, error-free, and executable
    code simply by providing prompts with specific requirements. Amazon Q Developer,
    integrated with AWS Lambda, automatically generates the `lambda_handle``r` `()`
    function with return code based on the Lambda runtime environment selected. This
    integration can assist application developers with relatively limited coding experience
    in automatically generating Lambda functions with minor to no code changes.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如上图所示，我们只需提供具有特定要求的提示，就可以自动生成端到端、无错误且可执行的代码。Amazon Q 开发者，与 AWS Lambda 集成，根据所选的
    Lambda 运行时环境自动生成基于返回代码的 `lambda_handle()` 函数。这种集成可以帮助那些编码经验相对有限的开发者自动生成 Lambda
    函数，且代码更改最小或无更改。
- en: Continuing with the application developer persona, next, we will explore the
    data scientist persona to investigate code assistance integration with Amazon
    SageMaker.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 继续以应用程序开发者的身份，接下来，我们将探索数据科学家角色，以研究代码辅助与 Amazon SageMaker 的集成。
- en: Code assistance integration with Amazon SageMaker
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 Amazon SageMaker 的代码辅助集成
- en: Before we start diving deep into code assistance support for the Amazon SageMaker
    service, let’s quickly go through an overview of Amazon SageMaker. **Amazon SageMaker**
    is a fully managed service that simplifies the process of building, training,
    and deploying ML models at scale. It is designed to make it easier for developers
    and data scientists to build, train, and deploy ML models without the need for
    extensive expertise in ML or deep learning. It has multiple features such as end-to-end
    workflow, built-in algorithms, custom model training, automatic model tuning,
    ground truth, edge manager, augmented AI, and managed notebooks, just to name
    a few. Amazon SageMaker integrates with other AWS services, such as Amazon S3
    for data storage, AWS Lambda for serverless inference, and Amazon CloudWatch for
    monitoring.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始深入研究Amazon SageMaker服务的代码辅助支持之前，让我们快速浏览一下Amazon SageMaker的概述。**Amazon SageMaker**是一个完全托管的服务，简化了在规模上构建、训练和部署机器学习模型的过程。它旨在使开发人员和数据科学家更容易构建、训练和部署机器学习模型，而无需在机器学习或深度学习方面具有广泛的专长。它具有多个功能，如端到端工作流程、内置算法、自定义模型训练、自动模型调优、真实数据、边缘管理器、增强人工智能和托管笔记本等，仅举几例。Amazon
    SageMaker与其他AWS服务集成，例如Amazon S3用于数据存储、AWS Lambda用于无服务器推理和Amazon CloudWatch用于监控。
- en: Amazon SageMaker Studio hosts the managed notebooks, which are integrated with
    Amazon Q Developer.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker Studio托管托管笔记本，这些笔记本与Amazon Q Developer集成。
- en: Use case for Amazon SageMaker
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon SageMaker的应用场景
- en: Let’s use a very common business use case related to churn prediction for which
    data scientists use the XGBoost algorithm.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用一个与客户流失预测相关的非常常见的商业用例，数据科学家使用XGBoost算法。
- en: '**Churn prediction** in business involves utilizing data and algorithms to
    forecast which customers are at risk of discontinuing their usage of a product
    or service. The term “churn” commonly denotes customers ending subscriptions,
    discontinuing purchases, or ceasing service utilization. The primary objective
    of churn prediction is to identify these customers before they churn, enabling
    businesses to implement proactive measures for customer retention.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户流失预测**在商业中涉及利用数据和算法来预测哪些客户有风险停止使用产品或服务。术语“流失”通常表示客户结束订阅、停止购买或停止服务使用。客户流失预测的主要目标是识别这些客户在流失之前，使企业能够实施主动措施以保留客户。'
- en: We will use publicly available direct marketing bank data to illustrate the
    support provided by Amazon Q Developer for milestone steps such as data collection,
    feature engineering, model training, and model deployment using Amazon SageMaker.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用公开可用的直接营销银行数据来展示Amazon Q Developer对数据收集、特征工程、模型训练和模型部署等里程碑步骤的支持。
- en: Typically, data scientists need to write a complex script to carry out all of
    the preceding milestone steps from an Amazon SageMaker Studio notebook.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，数据科学家需要编写一个复杂的脚本，从Amazon SageMaker Studio笔记本中执行所有上述里程碑步骤。
- en: Solution blueprint
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案蓝图
- en: Set up an environment with the required set of libraries.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个包含所需库集合的环境。
- en: '**Data collection**: Download and unzip direct marketing bank data from [https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip).'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据收集**：从[https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip)下载并解压缩直接营销银行数据。'
- en: '**Feature engineering**: To demonstrate the functionality, we will carry out
    the following commonly used feature engineering steps:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征工程**：为了展示功能，我们将执行以下常用的特征工程步骤：'
- en: Manipulate column data using default values
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用默认值操纵列数据
- en: Drop extra columns
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除额外列
- en: Carry out one-hot encoding
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行独热编码
- en: '**Model training**: Let’s use the XGBoost algorithm:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型训练**：让我们使用XGBoost算法：'
- en: Rearrange data to create training, validation, and test datasets/files
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新排列数据以创建训练、验证和测试数据集/文件
- en: Use XGBoost algorithms to train the model using the training dataset
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用XGBoost算法使用训练数据集训练模型
- en: '**Model deployment**: Deploy the model as an endpoint to allow inferences.'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型部署**：将模型作为端点部署以允许推理。'
- en: In the preceding solution blueprint, we illustrate the integration of Amazon
    Q Developer with Amazon SageMaker by handling commonly used milestone steps. However,
    based on the complexity of your data and enterprise needs, there might be additional
    steps required.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的解决方案蓝图示例中，我们展示了通过处理常用里程碑步骤来集成 Amazon Q 开发者与 Amazon SageMaker。然而，根据您数据和企业需求的不同复杂度，可能需要额外的步骤。
- en: Data preparation
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: We will utilize an AWS dataset publicly hosted for direct marketing bank data.
    The complete dataset is available at [https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip).
    All the data preparation steps will be conducted in the SageMaker Studio notebook
    as part of the data collection requirement.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用公开托管在 AWS 上的直接营销银行数据集。完整数据集可在 [https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip)
    获取。所有数据准备步骤都将作为数据收集要求的一部分在 SageMaker Studio 笔记本中执行。
- en: Solution – Amazon Q with Amazon SageMaker Studio
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案 – Amazon Q 与 Amazon SageMaker Studio
- en: Let’s first enable Amazon Q Developer with Amazon SageMaker Studio. The following
    prerequisites are needed to allow Amazon Q Developer to auto-generate code inside
    Amazon SageMaker studio.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们启用 Amazon Q 开发者与 Amazon SageMaker Studio。以下先决条件是必需的，以便允许 Amazon Q 开发者在
    Amazon SageMaker Studio 内自动生成代码。
- en: Prerequisite to enable Amazon Q Developer with Amazon SageMaker Studio
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用 Amazon Q 开发者与 Amazon SageMaker Studio 的先决条件
- en: The developer is required to modify the IAM policy associated with the IAM user
    or role to grant permissions for Amazon Q Developer to initiate recommendations
    in for Amazon SageMaker Studio notebook. Refer [*Chapter 2*](B21378_02.xhtml#_idTextAnchor022)
    for the details to enable Amazon Q Developer with Amazon SageMaker Studio notebook.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者需要修改与 IAM 用户或角色关联的 IAM 策略，以授予 Amazon Q 开发者在 Amazon SageMaker Studio 笔记本中启动推荐的权限。请参阅
    [*第 2 章*](B21378_02.xhtml#_idTextAnchor022) 了解启用 Amazon Q 开发者与 Amazon SageMaker
    Studio 笔记本的详细信息。
- en: Once the Amazon Q Developer is activated for Amazon SageMaker Studio notebook,
    select **Create notebook** from the **Launcher** to verify that Amazon Q Developer
    is enabled.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Amazon Q 开发者为 Amazon SageMaker Studio 笔记本激活，从 **启动器**中选择 **创建笔记本**以验证 Amazon
    Q 开发者是否已启用。
- en: '![Figure 14.38 –An Amazon Q Developer-enabled notebook from SageMaker Studio](img/B21378_14_38.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.38 – SageMaker Studio 中启用了 Amazon Q 开发者的笔记本](img/B21378_14_38.jpg)'
- en: Figure 14.38 –An Amazon Q Developer-enabled notebook from SageMaker Studio
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.38 – SageMaker Studio 中启用了 Amazon Q 开发者的笔记本
- en: To fulfill the mentioned requirements, we will use auto-code generation techniques
    that were discussed in [*Chapter 4*](B21378_04.xhtml#_idTextAnchor081). Mainly,
    we will focus on single-line prompts, multi-line prompts, and chain-of-thought
    prompts for auto-code generation techniques.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足上述要求，我们将使用在第 [*第 4 章*](B21378_04.xhtml#_idTextAnchor081) 中讨论的自动代码生成技术。主要，我们将关注单行提示、多行提示和思维链提示，以实现自动代码生成技术。
- en: Requirement 1
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求 1
- en: Set up an environment with the required set of libs.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 设置包含所需库的环境。
- en: 'Let’s use single-line prompts:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用单行提示：
- en: '[PRE10]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Figure 14.39 –Amazon Q Developer – SageMaker Studio setup environment](img/B21378_14_39.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.39 – Amazon Q 开发者 – SageMaker Studio 设置环境](img/B21378_14_39.jpg)'
- en: Figure 14.39 –Amazon Q Developer – SageMaker Studio setup environment
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.39 – Amazon Q 开发者 – SageMaker Studio 设置环境
- en: Observe that, based on our prompts, Amazon Q Developer generated code with a
    default set of libraries and variables. However, based on your needs, and account
    setup, you may need to update/add the code.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，根据我们的提示，Amazon Q 开发者生成了带有默认库和变量的代码。然而，根据您的需求和账户设置，您可能需要更新/添加代码。
- en: Requirement 2
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求 2
- en: For data collection, download and unzip direct marketing bank data from [https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据收集，从 [https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip)
    下载并解压直接营销银行数据。
- en: 'We will focus on multi-line prompts to achieve this requirement:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于多行提示来实现这一要求：
- en: '[PRE11]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Figure 14.40 – Amazon Q Developer – SageMaker Studio data collection](img/B21378_14_40.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.40 – Amazon Q 开发者 – SageMaker Studio 数据收集](img/B21378_14_40.jpg)'
- en: Figure 14.40 – Amazon Q Developer – SageMaker Studio data collection
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.40 –亚马逊Q开发者 – SageMaker Studio数据收集
- en: Requirement 3
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求3
- en: 'To demonstrate the functionality of feature engineering, we will carry out
    the following commonly used feature engineering steps, which will help us improve
    the model accuracy:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示特征工程的功能，我们将执行以下常用的特征工程步骤，这将帮助我们提高模型精度：
- en: Manipulate column data using default values.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认值操纵列数据。
- en: Drop extra columns.
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除额外列。
- en: Carry out one-hot encoding.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行独热编码。
- en: 'We will focus on multi-line prompts to achieve this requirement:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于多行提示来实现这一要求：
- en: '[PRE12]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We get the following screen.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下屏幕。
- en: '![Figure 14.41 –Amazon Q Developer – SageMaker Studio feature engineering](img/B21378_14_41.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![图14.41 –亚马逊Q开发者 – SageMaker Studio特征工程](img/B21378_14_41.jpg)'
- en: Figure 14.41 –Amazon Q Developer – SageMaker Studio feature engineering
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.41 –亚马逊Q开发者 – SageMaker Studio特征工程
- en: 'Now, let’s proceed with the generated code for the model training, testing,
    and validation:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续使用生成的代码进行模型训练、测试和验证：
- en: '[PRE13]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Figure 14.42 –Amazon Q Developer – SageMaker Studio feature engineering](img/B21378_14_42.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![图14.42 –亚马逊Q开发者 – SageMaker Studio特征工程](img/B21378_14_42.jpg)'
- en: Figure 14.42 –Amazon Q Developer – SageMaker Studio feature engineering
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.42 –亚马逊Q开发者 – SageMaker Studio特征工程
- en: Note that for both single-line and multi-line prompts, we needed to provide
    much more specific details to generate the code as expected.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于单行和多行提示，我们需要提供更多具体的细节来生成预期的代码。
- en: Requirement 4
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求4
- en: '**Model training**: Let’s use the XGBoost algorithm:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型训练**：让我们使用XGBoost算法：'
- en: Rearrange data to create training, validation, and test datasets/files
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重新排列数据以创建训练、验证和测试数据集/文件
- en: Use XGBOOST algorithms to train the model using a training dataset
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用XGBOOST算法使用训练数据集训练模型
- en: 'We will focus on multi-line prompts to achieve this requirement to start the
    model training activity:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于多行提示来实现这一要求，以启动模型训练活动：
- en: '[PRE14]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Figure 14.43 – Amazon Q Developer – SageMaker Studio model training](img/B21378_14_43.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图14.43 –亚马逊Q开发者 – SageMaker Studio模型训练](img/B21378_14_43.jpg)'
- en: Figure 14.43 – Amazon Q Developer – SageMaker Studio model training
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.43 –亚马逊Q开发者 – SageMaker Studio模型训练
- en: Requirement 5
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求5
- en: Deploy the model as an endpoint to allow inferences.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型部署为端点以允许推理。
- en: 'We will focus on single-line prompts to achieve this requirement:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于单行提示来实现这一要求：
- en: '[PRE15]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Figure 14.44 – Amazon Q Developer -SageMaker studio model training](img/B21378_14_44.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![图14.44 –亚马逊Q开发者 -SageMaker studio模型训练](img/B21378_14_44.jpg)'
- en: Figure 14.44 – Amazon Q Developer -SageMaker studio model training
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.44 –亚马逊Q开发者 -SageMaker studio模型训练
- en: Observe that Amazon Q Developer uses a default configuration for `instance_type`
    and `initial_instance_count`. You can check the hosted model from the Amazon SageMaker
    console by clicking the **Inference** dropdown and selecting the **Endpoints**
    option.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到亚马逊Q开发者使用默认配置的`instance_type`和`initial_instance_count`。您可以通过点击亚马逊SageMaker控制台中的**推理**下拉菜单并选择**端点**选项来检查托管模型。
- en: In the preceding examples, we extensively used inline prompts with single-line
    prompting, multi-line prompting, and chain of thought prompting techniques. If
    you wish to use a chat-style interface, you can leverage the Amazon Q Developer
    chat-style interface, as shown in the following screenshot.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们广泛使用了单行提示、多行提示和思维链提示技术。如果您想使用聊天式界面，可以利用以下截图所示的亚马逊Q开发者聊天式界面。
- en: '![Figure 14.45 –Amazon Q Developer -SageMaker studio chat style interface](img/B21378_14_45.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![图14.45 –亚马逊Q开发者 -SageMaker studio聊天式界面](img/B21378_14_45.jpg)'
- en: Figure 14.45 –Amazon Q Developer -SageMaker studio chat style interface
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.45 –亚马逊Q开发者 -SageMaker studio聊天式界面
- en: Summary – Amazon Q Developer with Amazon SageMaker
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要 – 亚马逊Q开发者与亚马逊SageMaker
- en: As demonstrated, Amazon Q Developer seamlessly integrated with the Amazon SageMaker
    Studio notebook IDE, enables the automatic generation of end-to-end, error-free,
    and executable code. By supplying prompts with specific requirements, Q Developer
    can auto-generate code for essential milestone steps, including data collection,
    feature engineering, model training, and model deployment within the SageMaker
    Studio notebook.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，亚马逊Q开发者与亚马逊SageMaker Studio笔记本IDE无缝集成，能够自动生成端到端、无错误且可执行的代码。通过提供具有特定要求的提示，Q开发者可以在SageMaker
    Studio笔记本中自动生成代码，包括数据收集、特征工程、模型训练和模型部署等关键里程碑步骤。
- en: While data scientists can utilize this integration to produce code blocks, customization
    may be necessary. Specific details must be provided in prompts to tailor the code.
    In some instances, adjustments may be required to align with enterprise standards,
    business requirements, and configurations. Users should possess expertise in prompt
    engineering, familiarity with scripting, and conduct thorough testing to ensure
    the scripts meet business requirements before deploying them into production.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据科学家可以利用此集成来生成代码块，但可能需要进行定制。必须在提示中提供特定细节以调整代码。在某些情况下，可能需要调整以符合企业标准、业务需求和配置。用户应具备提示工程的专业知识、熟悉脚本编写，并在部署到生产之前进行彻底测试，以确保脚本满足业务需求。
- en: Now, let’s dive deep to see how data analysts can use code assistance while
    working with Amazon Redshift.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨数据分析师如何在处理Amazon Redshift时使用代码辅助。
- en: Code assistance integration with Amazon Redshift
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与Amazon Redshift的代码辅助集成
- en: Before we start diving deep into code assistance support for the Amazon Redshift
    service, let’s quickly go through an overview of AWS Redshift. **Amazon Redshift**
    is an AI-powered, fully managed, cloud-based data warehouse service. It is designed
    for high-performance analysis and the processing of large datasets using standard
    SQL queries.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨Amazon Redshift服务的代码辅助支持之前，让我们快速浏览一下AWS Redshift的概述。**Amazon Redshift**是一个由人工智能驱动的、完全托管、基于云的数据仓库服务。它旨在使用标准SQL查询进行高性能分析和处理大量数据集。
- en: Amazon Redshift is optimized for data warehousing, providing a fast and scalable
    solution for processing and analyzing large volumes of structured data. It uses
    columnar storage and **massively parallel processing** (**MPP**) architecture,
    distributing data and queries across multiple nodes to deliver high performance
    for complex queries. This architecture allows it to easily scale from a few hundred
    gigabytes to petabytes of data, enabling organizations to grow their data warehouse
    as their needs evolve. It integrates with various data sources, allowing you to
    load data from multiple sources, including Amazon S3, Amazon DynamoDB, and Amazon
    EMR.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift针对数据仓库进行了优化，提供了一种快速且可扩展的解决方案，用于处理和分析大量结构化数据。它使用列式存储和**大规模并行处理**（**MPP**）架构，将数据和查询分布在多个节点上，以提供复杂查询的高性能。这种架构允许它轻松地从几百吉字节扩展到拍字节的数据，使组织能够随着需求的变化扩展其数据仓库。它集成了各种数据源，允许您从多个来源加载数据，包括Amazon
    S3、Amazon DynamoDB和Amazon EMR。
- en: Note
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'To query the data, Amazon Redshift also provides a query editor. The Redshift
    query editor v2 has two modes to interact with databases: **Editor** and **Notebook**.
    Code assistance is integrated with the Notebook mode of the Redshift query editor
    v2.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查询数据，Amazon Redshift还提供了一个查询编辑器。Redshift查询编辑器v2有两种与数据库交互的模式：**编辑器**和**笔记本**。代码辅助集成在Redshift查询编辑器v2的笔记本模式下。
- en: Use case for Amazon Redshift
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon Redshift的用例
- en: Let’s start with one of the easy and widely used use cases of converting file
    format.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从转换文件格式的一个简单且广泛使用的用例开始。
- en: '**Identifying top performers**: In a typical business use case, analysts are
    interested in identifying top performers based on certain criteria.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '**识别顶尖表现者**：在典型的业务用例中，分析师对根据某些标准识别顶尖表现者感兴趣。'
- en: To illustrate this use case, we will be using the publicly available `tickit`
    database, which is readily available with Amazon Redshift. For more information
    about the `tickit` database, refer to the *References* section at the end of the
    chapter.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个用例，我们将使用公开可用的`tickit`数据库，该数据库与Amazon Redshift一起提供。有关`tickit`数据库的更多信息，请参阅本章末尾的*参考文献*部分。
- en: Analysts want to identify the top state where most of the venues are.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 分析师希望识别大多数场馆所在的最顶级州。
- en: To meet this requirement, analyst developers must develop SQL queries to interact
    with different tables from the `tickit` database.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这一需求，分析师开发者必须开发SQL查询来与`tickit`数据库中的不同表进行交互。
- en: Solution blueprint
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案蓝图
- en: 'As we are considering the data analyst persona and using code assistance to
    generate the code, we do not need to further break down the business ask into
    the solution blueprint. This makes it easy for analysts to interact with databases
    without getting involved in table structures and relationship details:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在考虑数据分析师的角色并使用代码辅助来生成代码，我们不需要进一步将业务需求分解为解决方案蓝图。这使得分析师能够与数据库交互，而无需涉及表结构和关系细节：
- en: Write SQL to identify the top state where most of the venues are
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写 SQL 语句以识别大多数场馆所在的顶级州
- en: Data preparation
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'We will be using the publicly available `tickit` database, which comes with
    Amazon Redshift. Let’s import the data using Redshift query editor v2:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用公开可用的 `tickit` 数据库，该数据库随 Amazon Redshift 一起提供。让我们使用 Redshift 查询编辑器 v2 导入数据：
- en: Connect to your Amazon Redshift cluster or Serverless endpoint from Redshift
    query editor 2.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Redshift 查询编辑器 2 连接到您的 Amazon Redshift 集群或无服务器端点。
- en: Then, choose `sample_data_dev` and click on `tickit`.
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，选择 `sample_data_dev` 并点击 `tickit`。
- en: '![Figure 14.46 – Import the tickit database using Amazon Redshift](img/B21378_14_46.jpg)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.46 – 使用 Amazon Redshift 导入 tickit 数据库](img/B21378_14_46.jpg)'
- en: Figure 14.46 – Import the tickit database using Amazon Redshift
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.46 – 使用 Amazon Redshift 导入 tickit 数据库
- en: Solution – Amazon Q with Amazon Redshift
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案 – Amazon Q 与 Amazon Redshift
- en: Let’s first enable Amazon Q with Amazon Redshift. To allow Amazon Q to generate
    SQL inside Amazon Redshift, the admin needs to enable the **Generative SQL** option
    inside **Notebook** of Redshift query editor v2\. Please reference [*Chapter 3*](B21378_03.xhtml#_idTextAnchor060)
    for additional details on initiating interaction with Amazon Q in Amazon Redshift.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们启用 Amazon Q 与 Amazon Redshift。为了允许 Amazon Q 在 Amazon Redshift 内生成 SQL，管理员需要在
    Redshift 查询编辑器 v2 的 **笔记本** 中启用 **生成 SQL** 选项。请参考 [*第 3 章*](B21378_03.xhtml#_idTextAnchor060)
    获取有关在 Amazon Redshift 中启动与 Amazon Q 交互的更多详细信息。
- en: Prerequisite to enable Amazon Q with Amazon Redshift
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用 Amazon Q 与 Amazon Redshift 的先决条件
- en: Let’s walk through the steps needed to enable the **Generative SQL** option
    inside **Notebook** of the Redshift query editor v2.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解启用 Redshift 查询编辑器 v2 中 **笔记本** 内的 **生成 SQL** 选项所需的步骤。
- en: Log in with admin privileges to connect to your Amazon Redshift cluster or Serverless
    endpoint.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用管理员权限登录以连接到您的 Amazon Redshift 集群或无服务器端点。
- en: Choose **Notebook**.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **笔记本**。
- en: '![Figure 14.47 – Notebook using Redshift query editor v2](img/B21378_14_47.jpg)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.47 – 使用 Redshift 查询编辑器 v2 的笔记本](img/B21378_14_47.jpg)'
- en: Figure 14.47 – Notebook using Redshift query editor v2
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.47 – 使用 Redshift 查询编辑器 v2 的笔记本
- en: Choose **Generative SQL**, then check the **Generative SQL** box, and click
    **Save**.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **生成 SQL**，然后勾选 **生成 SQL** 复选框，并点击 **保存**。
- en: '![Figure 14.48 – Enable Generative SQL using Redshift query editor v2](img/B21378_14_48.jpg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.48 – 使用 Redshift 查询编辑器 v2 启用生成 SQL](img/B21378_14_48.jpg)'
- en: Figure 14.48 – Enable Generative SQL using Redshift query editor v2
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.48 – 使用 Redshift 查询编辑器 v2 启用生成 SQL
- en: To fulfill the mentioned requirements, we will use auto-code generation techniques
    that were discussed in [*Chapter 4*](B21378_04.xhtml#_idTextAnchor081). Mainly,
    we will focus on the chat companion for auto-code generation.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足上述要求，我们将使用在第 [*第 4 章*](B21378_04.xhtml#_idTextAnchor081) 中讨论的自动代码生成技术。主要，我们将关注自动代码生成的聊天伴侣。
- en: Requirement 1
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求 1
- en: Write SQL to identify the top state where most of the venues are.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 编写 SQL 语句以识别大多数场馆所在的顶级州。
- en: 'Use Amazon Q’s interactive session to ask the following question:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Amazon Q 的交互会话提出以下问题：
- en: '[PRE16]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Observe that we did not provide database details to the Amazon Q Developer,
    but it was still able to identify the required table, `tickit.venue`. It generated
    the fully executable end-to-end query with `Group by`, `Order by`, and `Limit`
    to meet the requirements. To make it easy for analysts to run the queries, code
    assistance is integrated with the notebook. Just by clicking **Add to notebook**,
    the SQL code will be available in a notebook cell that users can run directly.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们没有向 Amazon Q 开发者提供数据库详细信息，但它仍然能够识别所需的表，`tickit.venue`。它生成了包含 `Group by`、`Order
    by` 和 `Limit` 的完整可执行端到端查询以满足要求。为了使分析师更容易运行查询，代码辅助已集成到笔记本中。只需点击 **添加到笔记本**，SQL
    代码就会在用户可以直接运行的笔记本单元格中可用。
- en: '![Figure 14.49 – Interact with code assistance from Amazon Redshift](img/B21378_14_49.jpg)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.49 – 使用 Amazon Redshift 代码辅助进行交互](img/B21378_14_49.jpg)'
- en: Figure 14.49 – Interact with code assistance from Amazon Redshift
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.49 – 使用 Amazon Redshift 代码辅助进行交互
- en: Summary – Amazon Q with Amazon Redshift
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要 – Amazon Q 与 Amazon Redshift
- en: As demonstrated, we can effortlessly generate end-to-end, error-free, and executable
    SQL by interacting with Amazon Q through a chat-style interface. Amazon Q seamlessly
    integrates with notebooks in the Amazon Redshift query editor v2\. Users are not
    required to provide database and/or table details to the code assistant. It autonomously
    identifies the necessary tables and generates SQL code to fulfill the specified
    requirements in the prompt. Furthermore, to facilitate analysts in running queries,
    it is directly integrated with the notebook. Amazon Q, in conjunction with Amazon
    Redshift, proves to be a valuable asset for data analysts. In many cases, data
    analysts do not need to translate business requirements into technical steps.
    They can leverage the auto-generate SQL feature, bypassing the need to delve deep
    into database and table details.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 如演示所示，我们可以通过聊天式界面与 Amazon Q 交互，轻松生成端到端、无错误且可执行的 SQL。Amazon Q 与 Amazon Redshift
    查询编辑器 v2 中的笔记本无缝集成。用户无需向代码助手提供数据库和/或表详情。它自动识别必要的表并生成满足提示中指定要求的 SQL 代码。此外，为了方便分析师运行查询，它直接集成到笔记本中。Amazon
    Q 与 Amazon Redshift 结合，证明是数据分析师的有价值资产。在许多情况下，数据分析师无需将业务需求转换为技术步骤。他们可以利用自动生成 SQL
    的功能，无需深入了解数据库和表详情。
- en: Summary
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we initially covered the integration of different AWS services
    with code companions to assist users in auto-code generation. Then, we explored
    the integration of Amazon Q Developer with some of the core services, such as
    AWS Glue, Amazon EMR, AWS Lambda, Amazon Redshift, and Amazon SageMaker, commonly
    used by application developers, data engineers, and data scientists.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们最初介绍了不同 AWS 服务与代码伴侣的集成，以帮助用户自动生成代码。然后，我们探讨了 Amazon Q 开发者与一些核心服务的集成，例如
    AWS Glue、Amazon EMR、AWS Lambda、Amazon Redshift 和 Amazon SageMaker，这些服务通常被应用开发者、数据工程师和数据科学家使用。
- en: We then discussed, in the prerequisites, the in-depth integration with sample
    common use cases and corresponding solution walk-throughs for various integrations.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在先决条件中讨论了与示例常见用例的深入集成以及各种集成的相应解决方案讲解。
- en: AWS Glue integration with Amazon Q Developer, aiding data engineers in generating
    and executing ETL scripts using the AWS Glue Studio notebook environment. This
    includes a skeletal outline of a full end-to-end Glue ETL job using AWS Glue Studio.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue 与 Amazon Q 开发者集成，帮助数据工程师在 AWS Glue Studio 笔记本环境中生成和执行 ETL 脚本。这包括使用
    AWS Glue Studio 的一个完整的端到端 Glue ETL 作业的骨架概要。
- en: AWS EMR integration with Amazon Q Developer to assist data engineers in generating
    and executing ETL scripts using the AWS EMR Studio notebook environment.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: AWS EMR 与 Amazon Q 开发者集成，帮助数据工程师在 AWS EMR Studio 笔记本环境中生成和执行 ETL 脚本。
- en: AWS Lambda console IDE integration with Amazon Q Developer, supporting application
    engineers in generating and executing end-to-end Python-based applications for
    file movement.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Lambda 控制台 IDE 与 Amazon Q 开发者集成，支持应用工程师生成和执行基于 Python 的端到端应用程序，用于文件移动。
- en: Amazon SageMaker studio notebook integration with Amazon Q Developer to help
    data scientists achieve major milestone steps in data collection, feature engineering,
    model training, and model deployment using different prompting techniques.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker Studio 笔记本与 Amazon Q 开发者集成，帮助数据科学家使用不同的提示技术实现数据收集、特征工程、模型训练和模型部署的重大里程碑步骤。
- en: Amazon Redshift integration with Amazon Q to aid business analysts in generating
    SQL queries by simply providing business requirements. Users are not required
    to provide database and/or table details to the code assistant.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 与 Amazon Q 集成，帮助业务分析师通过简单地提供业务需求来生成 SQL 查询。用户无需向代码助手提供数据库和/或表详情。
- en: In the next chapter, we will look at how you can use Amazon Q Developer to get
    AWS-specific guidance and recommendations, either from the AWS console or from
    the documentation on a variety of topics such as architecture and best practices
    support.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何使用 Amazon Q 开发者获取 AWS 特定的指导和推荐，无论是从 AWS 控制台还是从有关架构和最佳实践支持的文档等各个主题的文档中。
- en: References
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'AWS Prescriptive Guidance - Data engineering: [https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-caf-platform-perspective/data-eng.html](https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-caf-platform-perspective/data-eng.html)'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 预设指导 - 数据工程：[https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-caf-platform-perspective/data-eng.html](https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-caf-platform-perspective/data-eng.html)
- en: 'Jupyter kernel: [https://docs.jupyter.org/en/latest/projects/kernels.html](https://docs.jupyter.org/en/latest/projects/kernels.html)'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter 内核：[https://docs.jupyter.org/en/latest/projects/kernels.html](https://docs.jupyter.org/en/latest/projects/kernels.html)
- en: 'Amazon Q Developer with AWS Glue Studio: [https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/glue-setup.html](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/glue-setup.html)'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Q 开发者与 AWS Glue Studio：[https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/glue-setup.html](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/glue-setup.html)
- en: 'TLC Trip Record Data: [https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TLC 行程记录数据：[https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
- en: 'Setting up Amazon Q data integration in AWS Glue: [https://docs.aws.amazon.com/glue/latest/dg/q-setting-up.html](https://docs.aws.amazon.com/glue/latest/dg/q-setting-up.html)'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AWS Glue 中设置 Amazon Q 数据集成：[https://docs.aws.amazon.com/glue/latest/dg/q-setting-up.html](https://docs.aws.amazon.com/glue/latest/dg/q-setting-up.html)
- en: 'Setting up Amazon Q Developer data integration in Amazon EMR: [https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/emr-setup.html](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/emr-setup.html)'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Amazon EMR 中设置 Amazon Q 开发者数据集成：[https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/emr-setup.html](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/emr-setup.html)
- en: 'Attach a compute to an EMR Studio Workspace: [https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html)'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将计算资源附加到 EMR Studio 工作区：[https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-use-clusters.html)
- en: 'Using Amazon Q Developer with AWS Lambda: [https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/lambda-setup.html](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/lambda-setup.html)'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS Lambda 与 Amazon Q 开发者：[https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/lambda-setup.html](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/lambda-setup.html)
- en: 'Interacting with query editor v2 generative SQL: [https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-generative-ai.html](https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-generative-ai.html)'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与查询编辑器 v2 生成式 SQL 交互：[https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-generative-ai.html](https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor-v2-generative-ai.html)
- en: 'Amazon Redshift “tickit” database: [https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html](https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html)'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Redshift “tickit” 数据库：[https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html](https://docs.aws.amazon.com/redshift/latest/dg/c_sampledb.html)
- en: 'Direct marketing bank data: [https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip)'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接营销银行数据：[https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip](https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip)
- en: 'Amazon SageMaker Studio: [https://aws.amazon.com/sagemaker/studio/](https://aws.amazon.com/sagemaker/studio/)'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon SageMaker Studio：[https://aws.amazon.com/sagemaker/studio/](https://aws.amazon.com/sagemaker/studio/)
