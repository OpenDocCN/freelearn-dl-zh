- en: Predicting Apple Stock Market Cost with LSTM
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LSTM预测苹果股票市场成本
- en: Stock market predictions have been going on for many years and it has spawned
    an entire industry of prognosticators. It shouldn't come as a surprise since it
    can turn a significant profit if predicted properly. Understanding when is a good
    time to buy or sell a stock is key to getting the upper hand on Wall Street. This
    chapter will focus on creating a deep learning model using LSTM on Keras to predict
    the stock market quote of AAPL.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 股票市场预测已经进行多年，并且催生了一个完整的预测行业。考虑到如果预测准确，它可以带来可观的利润，这一点不应令人惊讶。理解何时是买入或卖出股票的最佳时机是掌握华尔街的关键。本章将重点介绍如何使用Keras上的LSTM创建深度学习模型，以预测苹果公司（AAPL）的股市报价。
- en: 'The following recipes will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下食谱：
- en: Downloading stock market data for Apple
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载苹果公司的股市数据
- en: Exploring and visualizing stock market data for Apple
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索并可视化苹果公司的股市数据
- en: Preparing stock data for model performance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备股票数据以供模型性能评估
- en: Building the LSTM model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建LSTM模型
- en: Evaluating the LSTM model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估LSTM模型
- en: Downloading stock market data for Apple
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载苹果公司的股市数据
- en: There are many resources for downloading stock market data for Apple. For our
    purposes, we will be using the Yahoo! Finance website.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多资源可以下载苹果公司的股市数据。为了我们的目的，我们将使用Yahoo! Finance网站。
- en: Getting ready
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'This section will require initializing a Spark cluster that will be used for
    all recipes in this chapter. A Spark notebook can be initialized in the terminal
    using `sparknotebook`, as seen in the following screenshot:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分需要初始化一个Spark集群，供本章所有食谱使用。可以通过终端使用`sparknotebook`来初始化Spark笔记本，如下图所示：
- en: '![](img/c9cd9f51-9207-4cc3-8288-355f18c47dff.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9cd9f51-9207-4cc3-8288-355f18c47dff.png)'
- en: 'A `SparkSession` can be initialized in a Jupyter notebook using the following
    script:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下脚本在Jupyter笔记本中初始化`SparkSession`：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it...
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: The following section walks through the steps for downloading historical stock
    market data for Apple.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将介绍如何下载苹果公司历史股市数据的步骤。
- en: Visit the following website to track the daily historical adjusted closing stock
    value for Apple, which has a stock ticker value of AAPL: [https://finance.yahoo.com/quote/AAPL/history](https://finance.yahoo.com/quote/AAPL/history)
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问以下网站，以追踪苹果公司（股票代码：AAPL）的每日历史调整收盘股价：[https://finance.yahoo.com/quote/AAPL/history](https://finance.yahoo.com/quote/AAPL/history)
- en: 'Set and apply the following parameters to the Historical Data tab:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“历史数据”标签中设置并应用以下参数：
- en: 'Time Period: Jan 01, 2000 - Apr 30, 2018.'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 时间范围：2000年1月1日 - 2018年4月30日。
- en: 'Show: Historical prices.'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示：历史价格。
- en: 'Frequency: Daily.'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 频率：每日。
- en: 'Download the dataset with the specified parameter to a `.csv` file by clicking
    on the Download Data link, as seen in the following screenshot:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击下载数据链接，将数据集下载为`.csv`文件，如下图所示：
- en: '![](img/75a7842c-6816-4485-8815-282c4f04f4c0.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75a7842c-6816-4485-8815-282c4f04f4c0.png)'
- en: 'Download the file, `AAPL.csv`, and then upload the same dataset to a Spark
    dataframe using the following script:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载文件`AAPL.csv`，然后使用以下脚本将相同的数据集上传到Spark数据框：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How it works...
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: The following section explains how the stock market data is incorporated into
    a Jupyter notebook.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分将解释如何将股市数据整合到Jupyter笔记本中。
- en: Yahoo! Finance is a great source for stock market quotes for publicly traded
    companies. The stock quote for Apple, AAPL, is traded on NASDAQ and the historical
    quotes can be captured for model development and analysis purposes. Yahoo! Finance
    gives you the option to capture stock quotes on a daily, weekly, or monthly snapshot.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yahoo! Finance是一个提供上市公司股市报价的好来源。苹果公司的股市报价（AAPL）在纳斯达克交易，历史报价可以用于模型开发和分析。Yahoo!
    Finance提供了按日、周或月捕捉股市报价的选项。
- en: The purpose of this chapter is to forecast stock at a daily level, as that would
    pull in the most amount of data into our training model. We can do this by tracing
    data back to January 1, 2000, all the way to April 30, 2018.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本章的目的是按日预测股票，因为这将拉入最多的数据来训练我们的模型。我们可以通过追溯数据从2000年1月1日到2018年4月30日来实现这一点。
- en: Once our parameters are set for download, we receive a nicely formatted comma-separated
    value file from Yahoo! Finance that can be easily converted into a Spark dataframe
    with minimal issues.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们设置好下载的参数，我们就可以从Yahoo! Finance获取格式良好的CSV文件，该文件可以轻松地转换为Spark数据框，几乎没有问题。
- en: 'The dataframe will allow us to view the Date, Open, High, Low, Close, Adj Close,
    and Volume of the stock on a daily basis. The columns in the dataframe track the
    opening and closing stock values as well as the highest and lowest values traded
    during that day. The number of shares traded during the day is also captured.
    The output of the Spark dataframe, `df`, can be shown by executing `df.show()`,
    as you can see in the following screenshot:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据框将允许我们查看股票的日期、开盘价、最高价、最低价、收盘价、调整后收盘价和成交量。数据框中的列跟踪当天的开盘和收盘股价，以及当天交易的最高和最低股价。当天交易的股票数量也会被记录下来。执行`df.show()`可以显示Spark数据框`df`的输出，具体如以下截图所示：
- en: '![](img/5f01e2e7-00ae-4314-886f-bec781945611.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f01e2e7-00ae-4314-886f-bec781945611.png)'
- en: There's more...
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Python had stock market APIs that allowed you to automatically connect and pull
    back stock market quotes for publicly traded companies such as Apple.   You would
    be required to input parameters and retrieve the data that can be stored in a
    dataframe. However, as of April 2018, the *Yahoo! Finance* API is no longer operational
    and therefore not a reliable solution for extracting data for this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Python曾经有股票市场API，允许你自动连接并获取像苹果公司这样的上市公司股票报价。你需要输入参数并检索可以存储在数据框中的数据。然而，从2018年4月起，*Yahoo!
    Finance* API已不再运行，因此不再是提取本章数据的可靠解决方案。
- en: See also
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: '`Pandas_datareader` is a very powerful library for extracting data from websites
    such as Yahoo! Finance. To learn more about the library and how it may connect
    back to Yahoo! Finance once it is back online, visit the following website:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`Pandas_datareader`是一个非常强大的库，用于从Yahoo! Finance等网站提取数据。要了解更多关于该库的信息，以及它如何在Yahoo!
    Finance重新上线后连接到该网站，请访问以下网站：'
- en: '[https://github.com/pydata/pandas-datareader](https://github.com/pydata/pandas-datareader)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/pydata/pandas-datareader](https://github.com/pydata/pandas-datareader)'
- en: Exploring and visualizing stock market data for Apple
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索和可视化苹果公司股票市场数据
- en: Before any modeling and predictions are performed on the data, it is important
    to first explore and visualize the data at hand for any hidden gems.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据进行任何建模和预测之前，首先探索并可视化手头的数据，以发现任何潜在的亮点是非常重要的。
- en: Getting ready
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'We will perform transformations and visualizations on the dataframe in this
    section. This will require importing the following libraries in Python:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将对数据框进行转换和可视化操作。这将需要在Python中导入以下库：
- en: '`pyspark.sql.functions`'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pyspark.sql.functions`'
- en: '`matplotlib`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`'
- en: How to do it...
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: The following section walks through the steps to explore and visualize the stock
    market data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分介绍了探索和可视化股票市场数据的步骤。
- en: 'Transform the `Date` column in the dataframe by removing the timestamp using
    the following script:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本转换数据框中的`Date`列，通过去除时间戳：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create a for-cycle to add three additional columns to the dataframe. The loop
    breaks apart the `date` field into `year`, `month`, and `day`, as seen in the
    following script:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个for循环，向数据框中添加三个额外的列。该循环将`date`字段拆分为`year`、`month`和`day`，如下脚本所示：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Save a subset of the Spark dataframe to a `pandas` dataframe called `df_plot`
    using the following script: `df_plot = df.select('year', 'Adj Close').toPandas()`.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将Spark数据框的子集保存到名为`df_plot`的`pandas`数据框中：`df_plot = df.select('year', 'Adj
    Close').toPandas()`。
- en: 'Graph and visualize the `pandas` dataframe, `df_plot`, inside of the notebook
    using the following script:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本在笔记本中绘制并可视化`pandas`数据框`df_plot`：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Calculate the row and column count of our Spark dataframe using the following
    script: `df.toPandas().shape`.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本计算我们的Spark数据框的行数和列数：`df.toPandas().shape`。
- en: Execute the following script to determine null values in the dataframe: `df.dropna().count()`.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本来确定数据框中的空值：`df.dropna().count()`。
- en: 'Execute the following script to pull back statistics on `Open`, `High`, `Low`,
    `Close`, and `Adj Close`:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本以提取`Open`、`High`、`Low`、`Close`和`Adj Close`的统计数据：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The following section explains the techniques used and insights gained from
    exploratory data analysis.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分解释了探索性数据分析中使用的技术和获得的洞察。
- en: 'The date column in the dataframe is more of a date-time column with the time
    values all ending in 00:00:00\. This is unnecessary for what we will need during
    our modeling and therefore can be removed from the dataset. Luckily for us, PySpark
    has a `to_date` function that can do this quite easily. The dataframe, `df`, is
    transformed using the `withColumn()` function and now only shows the date column
    without the timestamp, as seen in the following screenshot:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据框中的`date`列更像是一个日期时间列，所有时间值都以`00:00:00`结尾。对于我们在建模过程中所需的内容，这是不必要的，因此可以从数据集中删除。幸运的是，PySpark有一个`to_date`函数，可以轻松地完成这项操作。数据框`df`使用`withColumn()`函数进行转换，现在只显示没有时间戳的日期列，如以下截图所示：
- en: '![](img/7e4db493-73b8-4a4d-a52d-c918ea13998d.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e4db493-73b8-4a4d-a52d-c918ea13998d.png)'
- en: 'For analysis purposes, we want to extract the `day`, `month`, and `year` from
    the `date` column. We can do this by enumerating through a custom list, `date_breakdown`,
    to split the date by a `-` and then adding a new column for the year, month, and
    day using the `withColumn()` function. The updated dataframe with the newly added
    columns can be seen in the following screenshot:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了分析目的，我们希望从`date`列中提取`day`、`month`和`year`。我们可以通过遍历自定义列表`date_breakdown`，以`-`分隔日期，然后使用`withColumn()`函数为年份、月份和日期添加新列。添加新列后的更新数据框可以在以下截图中看到：
- en: '![](img/f9a7a8a6-a99e-4f5c-8416-cdab2bc52514.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9a7a8a6-a99e-4f5c-8416-cdab2bc52514.png)'
- en: One important takeaway is that `PySpark` also has a SQL function for dates that
    can extract the day, month, or year from a date timestamp. For example, if we
    were to add a month column to our dataframe, we would use the following script: `df.withColumn("month",f.month("date")).show()`.
    This is to highlight the fact that there are multiple ways to transform data within
    Spark.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的结论是，`PySpark`也有一个SQL函数用于日期，可以从日期时间戳中提取天、月或年。例如，如果我们要向数据框添加一个月份列，可以使用以下脚本：`df.withColumn("month",f.month("date")).show()`。这突出了在Spark中转换数据的多种方法。
- en: Spark dataframes are more limited in visualization features than `pandas` dataframes.
    Therefore, we will subset two columns from the Spark dataframe, `df`, and convert
    them into a `pandas` dataframe for plotting a line or time-series chart. The y-axis
    will be the adjusted close of the stock and the x-axis will be the year of the
    date.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark数据框在可视化功能上不如`pandas`数据框。因此，我们将从Spark数据框`df`中提取两列，并将它们转换为`pandas`数据框，用于绘制线性图或时间序列图。y轴将是股票的调整后收盘价，x轴将是日期的年份。
- en: 'The pandas dataframe, df_plot, is ready to be plotted using matplotlib once
    some formatting features are set, such as the grid visibility, the figure size
    of the plot, and the labels for the title and axes. Additionally, we explicitly
    state that the index of the dataframe needs to point to the year column. Otherwise,
    the default index will appear on the x-axis and not the year. The final time-series
    plot can be seen in the following screenshot:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: pandas数据框`df_plot`准备好用于绘制图表，一旦设置了一些格式化功能，如网格可见性、图表大小和标题及坐标轴标签。此外，我们明确指定数据框的索引需要指向年份列。否则，默认的索引会出现在x轴上，而不是年份。最终的时间序列图可以在以下截图中看到：
- en: '![](img/27142593-3862-40c6-80df-ce03e2e90c81.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/27142593-3862-40c6-80df-ce03e2e90c81.png)'
- en: Apple has experienced extensive growth over the last 18 years. While a few years
    saw some downward dips, the overall trend has been a steady upward move with the
    last couple of year's stock quotes hovering between $150 and $175.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 苹果在过去18年经历了广泛的增长。虽然有几年出现了下滑，但总体趋势是稳定向上的，近几年股价徘徊在$150到$175之间。
- en: 'We have made some changes to our dataframe so far, so it is important to get
    an inventory count of the rows and columns total as this will affect how the dataset
    is broken up for testing and training purposes later on in the chapter. As can
    be seen in the following screenshot, we have a total of 10 columns and 4,610 rows:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经对数据框做了一些更改，因此重要的是要获取行和列的总数，这会影响数据集稍后如何分割用于测试和训练。如以下截图所示，我们总共有10列和4,610行：
- en: '![](img/8a8ef246-4ce2-414d-8973-3b7d20f45564.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a8ef246-4ce2-414d-8973-3b7d20f45564.png)'
- en: When executing `df.dropna().count()`, we can see that the row count is still
    4,610, which is identical to the row count from the previous step, indicating
    that none of the rows have any null values.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行`df.dropna().count()`时，我们可以看到行数仍为4,610，这与上一步的行数相同，表明没有任何行包含空值。
- en: 'Finally, we can get a good read on the row count, mean, standard deviation,
    minimum, and maximum values of each of the columns that will be used in the model.
    This can help to identify whether there are anomalies in the data. One important
    thing to note is that each of the five fields that will be used in the model has
    a standard deviation higher than the mean value, indicating that the data is more
    spread out and not so clustered around the mean. The statistics for Open, High,
    Low, Close, and Adj Close can be seen in the following screenshot:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以对每个将用于模型的列进行行数、均值、标准差、最小值和最大值的统计。这有助于识别数据中是否存在异常。需要注意的一点是，所有五个将在模型中使用的字段的标准差都高于均值，这表明数据分布较广，并非都集中在均值附近。以下截图显示了Open、High、Low、Close和Adj
    Close的统计数据：
- en: '![](img/2df0f754-2582-417d-8429-2fc79b0cd4dd.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2df0f754-2582-417d-8429-2fc79b0cd4dd.png)'
- en: There's more...
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: While dataframes in Spark do not have the same native visualization features
    that are found in `pandas` dataframes, there are companies that manage Spark for
    enterprise solutions that allow for advanced visualization capabilities through
    notebooks without having to use libraries such as `matplotlib`. Databricks is
    one such company that offers this feature.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Spark中的数据框没有`pandas`数据框那样的本地可视化功能，但有些公司提供企业级Spark管理服务，允许通过笔记本进行高级可视化，而无需使用`matplotlib`等库。Databricks就是其中一家提供此功能的公司。
- en: 'The following is an example of a visualization using the built-in features
    available in notebooks from Databricks:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用Databricks笔记本内置功能进行可视化的示例：
- en: '![](img/fc9a047b-540b-473d-902e-cd83c02c0743.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc9a047b-540b-473d-902e-cd83c02c0743.png)'
- en: See also
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: To learn more about Databricks in general, visit the following website: [https://databricks.com/](https://databricks.com/).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于Databricks的信息，请访问以下网站：[https://databricks.com/](https://databricks.com/)。
- en: To learn more about visualizations in Databricks notebooks, visit the following
    website: [https://docs.databricks.com/user-guide/visualizations/index.html](https://docs.databricks.com/user-guide/visualizations/index.html).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于Databricks笔记本中可视化的信息，请访问以下网站：[https://docs.databricks.com/user-guide/visualizations/index.html](https://docs.databricks.com/user-guide/visualizations/index.html)。
- en: 'To learn more about accessing Databricks through a Microsoft Azure subscription,
    visit the following website:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何通过Microsoft Azure订阅访问Databricks，请访问以下网站：
- en: '[https://azure.microsoft.com/en-us/services/databricks/](https://azure.microsoft.com/en-us/services/databricks/)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://azure.microsoft.com/en-us/services/databricks/](https://azure.microsoft.com/en-us/services/databricks/)'
- en: Preparing stock data for model performance
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为模型性能准备股票数据
- en: We are almost ready to build a prediction algorithm for the stock value performance
    of Apple. The remaining task at hand is to prepare the data in a manner that ensures
    the best possible predictive outcome.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好为苹果股票的价值表现构建预测算法了。剩下的任务是以确保最佳预测结果的方式准备数据。
- en: Getting ready
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'We will perform transformations and visualizations on the dataframe in this
    section. This will require importing the following libraries in Python:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对数据框执行转换和可视化操作。这需要导入以下Python库：
- en: '`numpy`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`'
- en: '`MinMaxScaler()`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MinMaxScaler()`'
- en: How to do it...
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作…
- en: This section walks through the steps for preparing the stock market data for
    our model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍为我们的模型准备股票市场数据的步骤。
- en: 'Execute the following script to group the year column by the `Adj Close` count:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本，通过`Adj Close`计数对年份列进行分组：
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Execute the following script to create two new dataframes for training and
    testing purposes:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本以创建两个新的数据框，用于训练和测试：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Convert the two new dataframes  to `pandas` dataframes to get row and column
    counts with `toPandas()` using the following script:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将两个新的数据框转换为`pandas`数据框，以便使用`toPandas()`获取行列计数：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As we did previously with `df`, we visualize `trainDF` and `testDF` using the
    following script:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正如我们之前对`df`做的那样，我们使用以下脚本对`trainDF`和`testDF`进行可视化：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We create two new arrays, `trainArray` and `testArray`, based on the dataframes
    with the exception of the date columns using the following script:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据数据框（日期列除外）创建两个新数组，`trainArray`和`testArray`，使用以下脚本：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In order to scale the arrays between 0 and 1, import `MinMaxScaler` from `sklearn` and
    create a function call, `MinMaxScale`, using the following script:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了将数组缩放到0和1之间，导入`sklearn`中的`MinMaxScaler`，并使用以下脚本创建一个名为`MinMaxScale`的函数：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`MinMaxScaler` is then fit on the `trainArray` and used to create two new arrays
    that are scaled to fit using the following script:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将 `MinMaxScaler` 应用于 `trainArray`，并使用以下脚本创建两个经过缩放的新数组：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Split both `testingArray` and `trainingArray`  into features, `x`, and label,
    `y`, using the following script:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将 `testingArray` 和 `trainingArray` 都拆分为特征 `x` 和标签 `y`：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Execute the following script to retrieve a final inventory of the shape of
    all four arrays:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本来检索所有四个数组形状的最终清单：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Execute the following script to plot the training array for the quotes `open`,
    `high`, `low`, and `close` :'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下脚本绘制报价 `open`、`high`、`low` 和 `close` 的训练数组：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Additionally, we plot the training array for `volume` using the following script:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们使用以下脚本绘制 `volume` 的训练数组：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works...
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section explains the transformations needed on the data to be used in the
    model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了模型中所需的数据转换。
- en: 'One of the first steps to building a model is splitting the data into a training
    and test dataset for model evaluation purposes. Our goal is to use all of the
    stock quotes from 2000 through 2016 to predict stock trends in 2017-2018\. We
    know from previous sections that we have a total of 4,610 days of stock quotes,
    but we don''t know exactly how many fall in each year. We can use the `groupBy()`
    function within the dataframe to get a unique count of stock quotes per year,
    as can be seen in the following screenshot:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型的第一步是将数据分割成训练数据集和测试数据集，以进行模型评估。我们的目标是使用 2000 年到 2016 年的所有股票报价来预测 2017-2018
    年的股票趋势。我们从之前的部分得知，我们总共有 4,610 天的股票报价，但我们并不确切知道每年有多少数据。我们可以在数据框中使用 `groupBy()`
    函数来获取每年股票报价的唯一计数，详情请参见以下截图：
- en: '![](img/94a6f06e-e420-4c98-af2d-47734b6fe9b4.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94a6f06e-e420-4c98-af2d-47734b6fe9b4.png)'
- en: 2016 and 2017's combined data represents approximately 7% of the total data,
    which is a bit small for a testing dataset. However, for the purposes of this
    model, it should be sufficient. The remaining 93% of the dataset will be used
    for training purposes between 2000 and 2016\. Therefore, two dataframes are created
    using a filter to determine whether to include or exclude rows before or after
    2016.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2016 年和 2017 年的合并数据大约占总数据的 7%，这个测试数据集的比例稍显较小。然而，出于此模型的目的，这应该足够。剩余的 93% 数据集将用于
    2000 到 2016 年之间的训练。因此，通过过滤器创建了两个数据框，以确定是否包含或排除 2016 年之前或之后的行。
- en: 'We can now see that the test dataset, `testDF`, contains 333 rows and that
    the training dataset, `trainDF`, contains 4,277 rows. When both are combined,
    we reach our total row count from our original dataframe, `df`, of 4,610\. Finally,
    we see that `testDF` is comprised of 2017 and 2018 data only, which is 251 rows
    for 2017 and 82 rows for 2018 for a total of 333 rows, as can be seen in the following
    screenshot:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以看到，测试数据集 `testDF` 包含 333 行，而训练数据集 `trainDF` 包含 4,277 行。当两者合并时，我们得到了来自原始数据框
    `df` 的总行数 4,610。最后，我们看到 `testDF` 仅由 2017 年和 2018 年的数据组成，其中 2017 年有 251 行，2018
    年有 82 行，总共 333 行，详情请参见以下截图：
- en: '![](img/f249625c-a120-4cd6-91a0-59562546f47d.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f249625c-a120-4cd6-91a0-59562546f47d.png)'
- en: Please note that anytime we are converting a Spark dataframe to a `pandas` dataframe
    it may not always scale for big data.  While it will work for our specific example
    as we are using a relatively small dataset, the conversion to a `pandas` dataframe
    means that all of the data is loaded into the memory of the driver.  Once this
    conversion occurs, the data is not stored in the Spark worker nodes but is instead
    to the main driver node.  This is not optimal and may produce an out of memory
    error.  If you find that you need to convert to a `pandas` dataframe from Spark
    to visualize data it is recommended to pull a random sample from Spark or to aggregate
    the spark data to a more manageable dataset and then visualize in `pandas`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每当我们将 Spark 数据框转换为 `pandas` 数据框时，它可能并不总是适合大数据。虽然在我们使用相对较小的数据集时它可以正常工作，但转换为
    `pandas` 数据框意味着所有数据都将加载到主驱动程序的内存中。一旦发生此转换，数据不会存储在 Spark 工作节点中，而是传输到主驱动节点。这并不是最优的，可能会导致内存溢出错误。如果你发现需要从
    Spark 转换为 `pandas` 数据框来可视化数据，建议从 Spark 中抽取一个随机样本，或将 Spark 数据聚合成一个更易处理的数据集，然后在
    `pandas` 中进行可视化。
- en: 'Both testing and training dataframes can be visualized using `matplotlib` once
    a subset of the data is converted using `toPandas()` to leverage the built-in
    graphing capabilities of `pandas`. Visualizing the dataframes side by side showcases
    how the graphs appear to be similar when the y-axis for adjusted close is not
    scaled. In reality, we can see that `trainDF_plot` starts close to 0, but `testDF_plot`
    starts closer to 110, as seen in the following two screenshots:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据的子集通过`toPandas()`转换，我们就可以使用`matplotlib`来可视化测试和训练数据框，利用`pandas`内建的图形功能。将数据框并排显示可以展示出，在未缩放的情况下，调整后的收盘价的y轴看起来相似。实际上，我们可以看到`trainDF_plot`接近于0开始，而`testDF_plot`则从接近110的位置开始，如下两张截图所示：
- en: '![](img/bad4a77f-af78-4ec0-8600-1ce08fbd58df.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bad4a77f-af78-4ec0-8600-1ce08fbd58df.png)'
- en: '![](img/d0c2e709-8238-47a5-8624-9875b5aaea6b.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0c2e709-8238-47a5-8624-9875b5aaea6b.png)'
- en: 'Our stock values, as they stand, don''t lend themselves well to deep learning
    modeling because there isn''t a baseline for normalization or standardization.
    When working with neural networks, it is best to keep the values between 0 and
    1 to match outcomes found in sigmoid or step functions that are used for activation.
    In order for us to accomplish this, we must first convert our `pyspark` dataframes,
    `trainDF` and `testDF`, into `numpy` arrays, these being `trainArray` and `testArray`.
    As these are now arrays and not dataframes, we will not be using the date column
    as the neural network is only interested in numerical values. The first values
    in each can be seen in the following screenshot:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前我们的股票值并不适合深度学习建模，因为没有用于归一化或标准化的基准。在使用神经网络时，最好将值保持在0和1之间，以匹配在用于激活的sigmoid或step函数中找到的结果。为了实现这一点，我们必须首先将`pyspark`数据框`trainDF`和`testDF`转换为`numpy`数组，即`trainArray`和`testArray`。由于它们现在是数组而不是数据框，我们将不使用日期列，因为神经网络只对数值感兴趣。每个数组的第一个值可以在以下截图中看到：
- en: '![](img/839b0580-face-47d5-b89e-2d0401361448.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/839b0580-face-47d5-b89e-2d0401361448.png)'
- en: 'There are many ways to scale array values to a range between 0 and 1\. It involves
    using the following formula: `scaled array value = (array value - min array value)
    / (max array value - min array value)`. Fortunately, we do not need to manually
    make this calculation on arrays. We can leverage the `MinMaxScaler()` function
    from `sklearn` to scale down both arrays.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放数组值到0和1之间有很多种方法。它涉及使用以下公式：`scaled array value = (array value - min array value)
    / (max array value - min array value)`。幸运的是，我们无需手动进行这个计算。我们可以利用`sklearn`中的`MinMaxScaler()`函数来缩放这两个数组。
- en: 'The `MinMaxScaler()` function is fit on the training array, `trainArray`, and
    is then applied to create two brand new arrays, `trainingArray` and `testingArray`,
    that are scaled to values between 0 and 1\. The first row for each array can be
    seen in the following screenshot:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MinMaxScaler()`函数会在训练数组`trainArray`上进行拟合，然后应用于创建两个新的数组，`trainingArray`和`testingArray`，它们的值会被缩放到0到1之间。每个数组的第一行可以在以下截图中看到：'
- en: '![](img/0ac5852c-0e03-464c-b5cd-3074bf12f275.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ac5852c-0e03-464c-b5cd-3074bf12f275.png)'
- en: 'We are now ready to set our label and feature variables by slicing up the array
    into x and y for both testing and training purposes. The first five elements in
    the array are the features or the x values and the last element is the label or
    y value. The features are composed of the values from Open, High, Low, Close,
    and Volume. The label is composed of Adj Close. The breakout of the first row
    for `trainingArray` can be seen in the following screenshot:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备通过将数组切分成x和y来设置标签和特征变量，供训练和测试使用。数组中的前五个元素是特征或x值，最后一个元素是标签或y值。特征包括Open、High、Low、Close和Volume的值。标签由Adj
    Close组成。`trainingArray`的第一行分解可以在以下截图中看到：
- en: '![](img/a2e0b0e3-f4b8-49a0-83b8-163b4fcb4f1f.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2e0b0e3-f4b8-49a0-83b8-163b4fcb4f1f.png)'
- en: 'A final look at the shape of the four arrays that we will be using in the model
    can be used to confirm that we have 4,227 matrix rows of training data, 333 matrix
    rows of test data, 5 elements for features (`x`), and 1 element for the label
    (`y`), as can be seen in the following screenshot:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看我们将在模型中使用的四个数组的形状，可以确认我们有4,227行训练数据、333行测试数据、5个特征元素（`x`）和1个标签元素（`y`），如以下截图所示：
- en: '![](img/b6e38d99-ebe8-48a3-a186-3089a8074a93.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6e38d99-ebe8-48a3-a186-3089a8074a93.png)'
- en: 'The values for the training array, `xtrain`, for open, low, high, and close
    can be plotted using the newly adjusted scales between 0 and 1 for the quotes,
    as shown in the following screenshot:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`xtrain`训练数组的值，包括开盘价、最低价、最高价和收盘价，可以使用新的调整后的0到1之间的比例绘制，如下图所示：'
- en: '![](img/240dd4a3-03f5-48ad-8ec5-079c745060b9.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/240dd4a3-03f5-48ad-8ec5-079c745060b9.png)'
- en: 'Additionally, to volume can also be plotted with the scaled volume scores between
    0 and 1, as shown in the following screenshot:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，体积也可以使用归一化后的体积得分（在0和1之间）进行绘制，如下图所示：
- en: '![](img/0e927f61-bd08-42be-9487-cac94c83908f.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e927f61-bd08-42be-9487-cac94c83908f.png)'
- en: There's more...
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: While we did use `MinMaxScaler` from `sklearn`, it is also important to understand
    that there is also a `MinMaxScaler` function that is available directly through
    `pyspark.ml.feature`. It works exactly the same way by rescaling each feature
    to a value between 0 and 1\. Had we used a machine learning library natively through
    PySpark in this chapter to make our prediction, we would have used `MinMaxScaler`
    from `pyspark.ml.feature`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们确实使用了来自`sklearn`的`MinMaxScaler`，但同样重要的是要理解，`pyspark.ml.feature`中也有一个`MinMaxScaler`函数。它的功能完全相同，都是将每个特征重新缩放到0到1之间。如果我们在本章中通过PySpark原生的机器学习库进行预测，我们将使用来自`pyspark.ml.feature`的`MinMaxScaler`。
- en: See also
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'To learn more about `MinMaxScaler` from `sklearn`, visit the following website:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`sklearn`中`MinMaxScaler`的信息，请访问以下网站：
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html.](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html.](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)'
- en: 'To learn more about `MinMaxScaler` from `pyspark`, visit the following website:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于`pyspark`中`MinMaxScaler`的信息，请访问以下网站：
- en: '[https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler.](https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler.](https://spark.apache.org/docs/2.2.0/ml-features.html#minmaxscaler)'
- en: Building the LSTM model
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建LSTM模型
- en: The data is now in a format compatible with model development in Keras for LSTM
    modeling. Therefore, we will spend this section setting up and configuring the
    deep learning model for predicting stock quotes for Apple in 2017 and 2018.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现在已转换为与Keras中的LSTM模型开发兼容的格式。因此，我们将在本节中设置并配置深度学习模型，以预测2017年和2018年苹果公司的股票报价。
- en: Getting ready
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will perform model management and hyperparameter tuning of our model in
    this section. This will require importing the following libraries in Python:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中对模型进行管理和超参数调优。这将需要在Python中导入以下库：
- en: '[PRE17]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How to do it...
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: This section walks through the steps to setting up and tuning the LSTM model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将引导您完成设置和调优LSTM模型的步骤。
- en: 'Import the following libraries from `keras` using the following script:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本从`keras`导入以下库：
- en: '[PRE18]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Build a `Sequential` model using the following script:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本构建一个`Sequential`模型：
- en: '[PRE19]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Transform the testing and training data sets into three-dimensional arrays
    using the following script:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将测试集和训练集转换为三维数组：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Fit the  `model` using a variable called `loss` with the following script:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一个名为`loss`的变量通过以下脚本拟合`model`：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Create a new array, `predicted`, using the following script:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本创建一个新的数组`predicted`：
- en: '[PRE22]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Combine the `predicted` and `ytest` arrays into a single unified array, `combined_array`,
    using the following script:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本将`predicted`和`ytest`数组合并为一个统一的数组`combined_array`：
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: How it works...
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section explains how the LSTM neural network model is configured to train
    on our dataset.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了如何配置LSTM神经网络模型以在我们的数据集上进行训练。
- en: Most of the functionality from `keras` used to build the LSTM model will come
    from `models` and `layers`.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建LSTM模型时，大部分功能来自于`keras`的`models`和`layers`模块。
- en: The `LSTM` model that has been built will be defined using a `Sequential` class
    that works well with time series that are sequence dependent. The LSTM model has
    an `input_shape = (1,5)` for one dependent variable and five independent variables
    in our training dataset. Only one `Dense` layer will be used to define the neural
    network as we are looking to keep the model simple. A loss function is required
    when compiling a model in keras, and since we are performing it on a recurrent
    neural network, a `mean_squared_error` calculation is best to determine how close
    the predicted value is to the actual value. Finally, an optimizer is also defined
    when the model is compiled to adjust the weights in the neural network. `adam`
    has given good results, especially when being used with recurrent neural networks.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 已构建的`LSTM`模型将使用`Sequential`类进行定义，这种类非常适合处理具有序列依赖的时间序列数据。LSTM模型的`input_shape
    = (1,5)`表示我们的训练数据集中有一个因变量和五个自变量。由于我们希望保持模型简单，因此只会使用一个`Dense`层来定义神经网络。编译keras模型时需要指定损失函数，因为我们正在处理一个循环神经网络，使用`mean_squared_error`（均方误差）计算最能反映预测值与实际值之间的差距。最后，模型编译时还需要定义优化器，以调整神经网络中的权重。`adam`优化器给出了很好的结果，尤其是在与循环神经网络配合使用时。
- en: 'Our current arrays, `xtrain` and `xtest`, are currently two-dimensional arrays;
    however, to incorporate them into the LSTM model, they will need to be converted
    to three-dimensional arrays using `reshape()`, as shown in the following screenshot:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们当前的数组`xtrain`和`xtest`是二维数组；然而，为了将它们整合进LSTM模型，它们需要使用`reshape()`转换为三维数组，具体如以下截图所示：
- en: '![](img/ed6bb442-5b2e-4367-b453-589ade0a6c89.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed6bb442-5b2e-4367-b453-589ade0a6c89.png)'
- en: The LSTM model is fit with `xtrain` and `ytrain` and the batch size is set to
    10 with 100 epochs. The batch size is the setting that defines the number of objects
    that are trained together. We can go as low or as high as we like in terms of
    setting the batch size, keeping in mind that the lower the number of batches,
    the more memory is required. Additionally, an epoch is a measurement of how often
    the model goes through the entire dataset. Ultimately, these parameters can be
    tuned based on time and memory allotment.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LSTM模型使用`xtrain`和`ytrain`进行拟合，批量大小设定为10，训练100个epochs。批量大小是定义一起训练的对象数量的设置。我们可以根据需要设置较低或较高的批量大小，但需要注意的是，批量数越少，所需的内存越多。此外，epoch是衡量模型通过整个数据集的次数。最终，这些参数可以根据时间和内存分配进行调整。
- en: 'The mean squared error loss in each epoch is captured and visualized. After
    the fifth or sixth epoch, we can see that the loss tapers off, as shown in the
    following screenshot:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 每个epoch中的均方误差损失都被捕获并可视化。经过第五或第六个epoch后，我们可以看到损失逐渐减小，如以下截图所示：
- en: '![](img/5227945e-55d2-4d8f-869c-bb319f9a88b8.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5227945e-55d2-4d8f-869c-bb319f9a88b8.png)'
- en: We can now create a new array, `predicted`, based on the fitted model applied
    on `xtest` and then combine it with `ytest` to compare them side by side for accuracy
    purposes.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以创建一个新的数组`predicted`，它基于应用于`xtest`的拟合模型，然后将其与`ytest`合并，进行并排比较，以便进行准确性验证。
- en: See also
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: To learn more about parameter tuning models within keras, visit the following
    website: [https://keras.io/models/model/](https://keras.io/models/model/)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于keras中参数调整模型的内容，请访问以下网站：[https://keras.io/models/model/](https://keras.io/models/model/)
- en: Evaluating the model
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Here''s the moment of truth: we are going to see if our model is able to give
    us a good prediction for the AAPL stock in 2017 and 2018.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在到了关键时刻：我们将看看我们的模型是否能够为2017年和2018年的AAPL股票提供准确的预测。
- en: Getting ready
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We will perform a model evaluation using the mean squared error. Therefore,
    we will need to import the following library:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用均方误差进行模型评估。因此，我们需要导入以下库：
- en: '[PRE24]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How to do it...
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: This section walks through visualizing and calculating the predicted vs. actual
    stock quotes for Apple in 2017 and 2018.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本节演示了如何可视化并计算2017年和2018年苹果股票的实际与预测股价。
- en: 'Plot a side by side comparison of `Actual` versus `Predicted` stock to compare
    trends using the following script:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本绘制`实际`与`预测`股票的并排比较图，以便比较趋势：
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Calculate the mean squared error between the actual `ytest` versus `predicted` stock
    using the following script:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下脚本计算实际`ytest`与`predicted`（预测的）股票之间的均方误差：
- en: '[PRE26]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: This section explains the results of the LSTM model's evaluation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了LSTM模型评估的结果。
- en: 'From a graphical perspective, we can see that our predictions were close to
    the actual stock quotes from 2017-2018, as shown in the following screenshot:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从图形的角度来看，我们可以看到我们的预测与2017年至2018年的实际股票报价非常接近，如下面的屏幕截图所示：
- en: '![](img/59aa966a-f8d8-4990-83ca-adb1e7eacc6c.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59aa966a-f8d8-4990-83ca-adb1e7eacc6c.png)'
- en: 'Our model shows that the predicted values are closer to the actual values earlier
    on in the days for 2017 and 2018 than later on.  Overall, while it seems that
    our predicted and actual scores are very close, it would be best to get a mean
    squared error calculation to understand how much deviation is between the two.
    As we can see, we have a mean squared error of 0.05841 or approximately 5.8%:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的模型显示，预测值与2017年和2018年较早的实际值更接近。总体而言，虽然我们预测的分数与实际分数非常接近，但最好还是进行均方误差计算，以了解两者之间的偏差。正如我们所看到的，我们的均方误差为0.05841，或约为5.8%：
- en: '![](img/38579afd-997d-47c8-a733-538b06ae4ba8.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38579afd-997d-47c8-a733-538b06ae4ba8.png)'
- en: See also
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'In order to learn more about how the mean squared error is calculated within
    sklearn, visit the following website:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解在sklearn中如何计算均方误差，请访问以下网站：
- en: '[http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html).'
