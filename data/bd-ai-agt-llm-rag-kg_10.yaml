- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Building an AI Agent Application
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建人工智能智能体应用
- en: 'In the previous chapter, we discussed how an LLM can extend its capabilities
    by using other tools. We also saw some examples of how the use of multiple agents
    at the same time (instead of one) can be used to solve more complex tasks. We
    extensively discussed how these approaches can be used in various industries and
    how they can be revolutionary for so many applications. However, we also highlighted
    two of the limitations of agents: scalability and the complexity of connecting
    an agent with different tools.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们讨论了大型语言模型如何通过使用其他工具来扩展其功能。我们还看到了一些示例，说明了同时使用多个智能体（而不是一个）如何解决更复杂的问题。我们广泛讨论了这些方法如何在各个行业中使用，以及它们如何对许多应用产生革命性的影响。然而，我们也指出了智能体的两个局限性：可扩展性和将智能体与不同工具连接的复杂性。
- en: In this chapter, we will expand on these challenges and show how we can overcome
    them. We will pick up from these two limitations. So far, we have treated multi-agent
    systems as standalone entities running on a personal computer. In the final section
    of the previous chapter, we explored the exciting new business paradigms emerging
    with AI. Agents are poised to play a significant role across industries in the
    future, but for that to happen, agent systems must be ready for production deployment.
    Getting a multi-agent system into production means we’ll have to solve the previously
    mentioned scalability and complexity issues to avoid harming the customer experience.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将进一步探讨这些挑战，并展示如何克服它们。我们将从这两个限制开始。到目前为止，我们将多智能体系统视为在个人计算机上独立运行的实体。在前一章的最后部分，我们探讨了随着人工智能的出现而兴起的令人兴奋的新商业模式。智能体在未来各个行业中都准备发挥重要作用，但为了实现这一点，智能体系统必须为生产部署做好准备。将多智能体系统投入生产意味着我们必须解决之前提到的可扩展性和复杂性等问题，以避免损害客户体验。
- en: We will follow a progressive approach in this chapter. We will use Streamlit,
    which is a simple but flexible framework that allows us to manage the entire process
    of creating an application around our agents. It allows us to conduct rapid prototyping
    of our application, testing different options until we reach a proof of concept.
    With Streamlit, we can seamlessly work with both the backend, where agents operate,
    and the frontend, which shapes the user experience—all within a single framework.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将采用渐进式方法。我们将使用 Streamlit，这是一个简单但灵活的框架，允许我们管理围绕智能体创建应用程序的整个流程。它允许我们快速原型设计我们的应用程序，测试不同的选项，直到达到概念验证。使用
    Streamlit，我们可以无缝地与后端（智能体运行的地方）和前端（塑造用户体验的地方）一起工作，所有这些都在一个框架内完成。
- en: Next, we will discuss in more detail the whole set of operations that are necessary
    to make an LLM and agents functional. Irrespective of whether you have the opportunity
    to train a model from scratch, this section will help you understand how to improve
    scalability and how the industry is handling the complexity of the process. In
    addition, we will address asynchronous programming and containerization, two concepts
    that are useful for scaling not only a multi-agent application but any machine
    learning project.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将更详细地讨论使大型语言模型和智能体功能所需的全部操作。无论您是否有机会从头开始训练模型，本节都将帮助您了解如何提高可扩展性以及行业如何处理过程的复杂性。此外，我们将讨论异步编程和容器化，这两个概念不仅对扩展多智能体应用，而且对任何机器学习项目都很有用。
- en: 'In this chapter, we''ll be covering the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to Streamlit
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Streamlit 简介
- en: Developing our frontend with Streamlit
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Streamlit 开发我们的前端
- en: Creating an application with Streamlit and AI agents
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Streamlit 和人工智能智能体创建应用程序
- en: Machine learning operations and LLM operations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习操作和大型语言模型操作
- en: Asynchronous programming
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步编程
- en: Docker
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Most of the code in this chapter can be run on CPUs. The *Introduction to Streamlit*
    and *Frontend with Streamlit* sections do not require GPUs. The libraries to install
    are as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的大部分代码都可以在 CPU 上运行。*Streamlit 简介*和*Streamlit 前端*部分不需要 GPU。需要安装的库如下：
- en: '**Streamlit**: For managing the frontend and backend of our app'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Streamlit**: 用于管理我们应用程序的前端和后端'
- en: '**pandas**: For handling DataFrames'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pandas**: 用于处理 DataFrame'
- en: '**Matplotilib**: For plotting graphs'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Matplotlib**: 用于绘制图表'
- en: '**Folium**: For plotting maps'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Folium**: 用于绘制地图'
- en: '**time**: For monitoring runtime'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**time**: 用于监控运行时间'
- en: '**NumPy**: For numerical computation'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NumPy**: 用于数值计算'
- en: '**pydeck**: For map representation'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pydeck**: 用于地图表示'
- en: '**OpenAI**: For building agents using its LLMs'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenAI**：用于使用其 LLMs 构建代理'
- en: '**Sentence Transformer**: To conduct embeddings'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sentence Transformer**：用于执行嵌入'
- en: The *Creating an application with Streamlit and AI agents* section can be run
    on a CPU, but it would be preferred if it were run on a GPU.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: “使用 Streamlit 和 AI 代理创建应用程序”这一部分可以在 CPU 上运行，但最好是在 GPU 上运行。
- en: 'The OpenAI library requires the use of an OpenAI token, and you should register
    with OpenAI to obtain it. The next sections can be run on CPUs and are mainly
    based on the use of the AsyncIO library. The code can be found on GitHub: [https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr10](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr10).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 库需要使用 OpenAI 令牌，您应该注册 OpenAI 以获取它。接下来的部分可以在 CPU 上运行，主要基于 AsyncIO 库的使用。代码可以在
    GitHub 上找到：[https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr10](https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr10)。
- en: Introduction to Streamlit
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Streamlit 简介
- en: If readers are familiar with Streamlit, they can move on to the *Creating an
    application with Streamlit and AI agents* section directly.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果读者熟悉 Streamlit，他们可以直接跳转到“使用 Streamlit 和 AI 代理创建应用程序”这一部分。
- en: Companies have invested heavily in data science and AI. The models that are
    trained can guide business decisions and provide different insights. Training
    a model, using it, and extracting insights requires expertise that not everyone
    has. A model that is truly useful for a company must provide results that must
    then be used by other stakeholders as well. For example, when you train a model,
    it should generate results that are usable by other people. It is possible to
    create static visualizations of the data (exporting graphs), but they convey only
    limited information. One could provide information in a Jupyter notebook but not
    everyone is capable of using such a tool. One option that might allow easier access
    by others is to create a dashboard or web application.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 公司在数据科学和人工智能方面投入了大量资金。训练的模型可以指导业务决策并提供不同的见解。训练模型、使用它和提取见解需要专业知识，并非每个人都有。一个真正对公司有用的模型必须提供结果，然后其他利益相关者也可以使用。例如，当您训练一个模型时，它应该生成其他人可用的结果。可以创建数据的静态可视化（导出图表），但它们只能传达有限的信息。有人可能会提供信息在
    Jupyter 笔记本中，但并非每个人都能使用这样的工具。一个可能允许其他人更容易访问的选项是创建仪表板或网络应用程序。
- en: This is where Streamlit comes in.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是 Streamlit 发挥作用的地方。
- en: Starting with Streamlit
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 Streamlit 开始
- en: Streamlit is a web application framework that allows one to easily and intuitively
    create web applications with Python. Its library provides a number of built-in
    components for both the backend and the frontend. It is also compatible with leading
    machine learning, graph, and plotting libraries.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Streamlit 是一个网络应用程序框架，允许用户使用 Python 轻松直观地创建网络应用程序。它的库为后端和前端提供了许多内置组件。它还与领先的机器学习、图形和绘图库兼容。
- en: The objective of this section is to understand how Streamlit works and how it
    can be a powerful tool.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目标是了解 Streamlit 的工作原理以及它如何成为一个强大的工具。
- en: 'One of the advantages of Streamlit is its ease of use and installation. Streamlit
    can simply be installed from the terminal and is present in Anaconda distributions:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Streamlit 的一个优点是它易于使用和安装。Streamlit 可以简单地从终端安装，并在 Anaconda 分发中可用：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Organizing an app with Streamlit is a simple Python script that typically contains
    both the backend and the frontend. This script can then be run either locally
    or in the cloud. For example, `my_app.py` should contain within it all the elements
    to build a web app. In the simplest cases, with just a few lines of code, we can
    build a web app. Once we define our app, running it locally is really simple:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Streamlit 组织应用程序是一个简单的 Python 脚本，通常包含后端和前端。这个脚本可以在本地或云端运行。例如，`my_app.py`
    应该包含构建网络应用程序所需的所有元素。在最简单的情况下，只需几行代码，我们就可以构建一个网络应用程序。一旦我们定义了我们的应用程序，在本地运行它就非常简单：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'What we need to do is call Streamlit and the name of our app (obviously, if
    we are using a terminal, we need to be in the right directory). Actually, the
    script does not have to be in your local directory; it can be on the internet.
    For example, our script is in our repository on GitHub, and we want to run it
    locally:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的是调用 Streamlit 和我们应用程序的名称（显然，如果我们使用终端，我们需要在正确的目录中）。实际上，脚本不必在您的本地目录中；它可以在互联网上。例如，我们的脚本在我们的
    GitHub 仓库中，我们想在本地运行它：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Under the hood, Streamlit runs through the file and executes the elements it
    finds sequentially. After that is done, a local Streamlit server will be initialized
    and your app will open in a new tab in your default web browser. Note that everything
    we write is in Python, and no other language is required. When we make a change,
    we must save our source. Streamlit detects any modifications and prompts us to
    rerun the app. This allows for quick iterations while immediately observing the
    effects, ensuring a seamless feedback loop between writing and running the application.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，Streamlit 会遍历文件并按顺序执行它找到的元素。完成后，将初始化一个本地 Streamlit 服务器，并在默认的网页浏览器的新标签页中打开你的应用程序。请注意，我们编写的一切都是
    Python 代码，不需要其他语言。当我们进行更改时，我们必须保存我们的源代码。Streamlit 会检测任何修改并提示我们重新运行应用程序。这允许我们快速迭代，并立即观察效果，确保编写和运行应用程序之间的无缝反馈循环。
- en: 'An example of a simple app is the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 简单应用程序的一个例子如下：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The app we generated simply does three things: it creates a DataFrame with
    pandas, plots it, and then produces a box plot. In a few lines of code, we have
    created a mini web application that is accessible on our browser. Once we have
    written it, we just have to run it and then Streamlit takes care of everything
    else.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成的应用程序简单地做了三件事：使用 pandas 创建一个 DataFrame，绘制它，然后生成箱线图。在几行代码中，我们创建了一个可在浏览器上访问的微型网络应用程序。一旦编写完成，我们只需运行它，然后
    Streamlit 就会处理其他所有事情。
- en: '![Figure 10.1 – Example of a web application](img/B21257_10_01.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – 网络应用程序示例](img/B21257_10_01.jpg)'
- en: Figure 10.1 – Example of a web application
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 网络应用程序示例
- en: 'Let’s look at the code block in a bit more detail:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看代码块：
- en: '`st.title`: This is a text element that allows us to display the title of our
    app. It is a good idea to always include it in an app.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.title`：这是一个文本元素，允许我们显示应用程序的标题。在应用程序中始终包含它是好主意。'
- en: '`st.write`: This is considered the Swiss army knife of Streamlit. Its main
    purpose is to write both textual and other elements. In this case, we have shown
    how passing a DataFrame is written to the app in nice formatting. In addition,
    this element is interactive. In other words, its behavior depends on the input
    given to it. The `write()` function is not limited to text but can be used with
    images, other Python elements (such as lists and dictionaries), templates, and
    so on. It also allows us to insert commands with HTML if we want to edit our text.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.write`：这被认为是 Streamlit 的瑞士军刀。其主要目的是写入文本和其他元素。在这种情况下，我们展示了如何将 DataFrame
    以良好的格式写入到应用程序中。此外，这个元素是交互式的。换句话说，它的行为取决于它所接收的输入。`write()` 函数不仅限于文本，还可以与图像、其他 Python
    元素（如列表和字典）、模板等一起使用。它还允许我们在想要编辑文本时插入 HTML 命令。'
- en: '`st.pyplot`: This displays a Matplotlib figure – in our case, a box plot. As
    you can see, we generated our figure first and then called `pyplot()` for the
    subsequent plotting. The figure is generated before being actually shown. In other
    words, the figure is already present in memory; we need `pyplot()` to display
    the figure to the user in the app. Actually, we could also call plotting directly
    with Matplotlib, but this is not recommended because it could lead to unexpected
    behavior.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.pyplot`：这显示了一个 Matplotlib 图形——在我们的例子中是一个箱线图。正如你所见，我们首先生成了图形，然后调用 `pyplot()`
    进行后续绘图。图形在显示之前就已经生成。换句话说，图形已经在内存中；我们需要 `pyplot()` 来在应用程序中向用户显示图形。实际上，我们也可以直接使用
    Matplotlib 进行绘图，但这样做不推荐，因为它可能导致意外的行为。'
- en: 'Note that we have only shown some basic commands, but Streamlit is quite flexible.
    For example, the DataFrame can be written to the app in different ways. Using
    `st.write()` is just one way: `st.dataframe()` does the same as `st.write()`,
    `st.table()` allows us to render the table statically, and writing `''df''` directly
    acts as if we were using `st.write()`. It is recommended to use one of the built-in
    methods because the behavior is known and we can also use additional arguments
    to handle the output.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们只展示了基本命令，但 Streamlit 非常灵活。例如，DataFrame 可以以不同的方式写入到应用程序中。使用 `st.write()`
    只是一种方法：`st.dataframe()` 与 `st.write()` 做同样的事情，`st.table()` 允许我们静态渲染表格，直接写入 `'df'`
    就像我们使用 `st.write()` 一样。建议使用内置方法，因为其行为已知，我们还可以使用额外的参数来处理输出。
- en: 'For example, we can use the flexibility provided by the built-in method, `st.dataframe()`,
    to highlight elements in our DataFrame:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用内置方法 `st.dataframe()` 提供的灵活性来突出显示 DataFrame 中的元素：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure 10.2 – Change in style in the DataFrame rendering](img/B21257_10_02.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.2 – DataFrame 渲染风格的变化](img/B21257_10_02.jpg)'
- en: Figure 10.2 – Change in style in the DataFrame rendering
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – DataFrame 渲染风格的变化
- en: 'In addition, Streamlit also makes it easy to add maps to our application. Just
    provide the coordinates, and `st.map()` magically allows us to have a map in our
    application (a map that we can enlarge and move). In this case, we provided the
    coordinates of some Sicilian cities:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Streamlit 还使得将地图添加到我们的应用变得容易。只需提供坐标，`st.map()` 就能神奇地让我们在应用中拥有地图（一个可以放大和移动的地图）。在这种情况下，我们提供了西西里岛一些城市的坐标：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Figure 10.3 – Plotting a map with Streamlit](img/B21257_10_03.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.3 – 使用 Streamlit 绘制地图](img/B21257_10_03.jpg)'
- en: Figure 10.3 – Plotting a map with Streamlit
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 使用 Streamlit 绘制地图
- en: As can be seen, we have added some elements and made some changes to our app
    (adding a map). Whenever we modify the code, we should remember to save the changes
    to the script; then, we go to our app and press the *R* key, which will reload
    the app with the updates.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，我们在我们的应用中添加了一些元素并做了一些修改（添加了地图）。每次我们修改代码时，都应该记得保存脚本中的更改；然后，我们进入我们的应用并按 *R*
    键，这将使用更新重新加载应用。
- en: 'If there are any errors, Streamlit will provide us with error messages indicating
    what we need to correct. An example of an error is shown in the following figure
    (in this case, about the variable name to use):'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有任何错误，Streamlit 将提供错误信息，指示我们需要纠正的内容。以下图示了一个错误示例（在这种情况下，关于要使用的变量名）：
- en: '![Figure 10.4 – Example of an error](img/B21257_10_04.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.4 – 错误示例](img/B21257_10_04.jpg)'
- en: Figure 10.4 – Example of an error
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – 错误示例
- en: 'For debugging, we use `st.write()` extensively; this simple function can print
    almost any Python object by guiding us to understand what the error is. For example,
    we can use it in this case. As we can see, we have an error in the column names
    (*Latitude* should be lowercase; so, we substitute it with the correct name):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于调试，我们广泛使用 `st.write()`；这个简单的函数可以通过引导我们理解错误来打印几乎任何 Python 对象。例如，我们可以在这个情况下使用它。如我们所见，我们在列名中有一个错误（*Latitude*
    应该是小写；因此，我们用正确的名称替换它）：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Figure 10.5 – Using st.write() to debug](img/B21257_10_05.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.5 – 使用 st.write() 调试](img/B21257_10_05.jpg)'
- en: Figure 10.5 – Using st.write() to debug
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – 使用 st.write() 调试
- en: Caching the results
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存结果
- en: 'Caching allows our app to remain performant even if data is loaded from the
    web (we will discuss how to add data from the web or the user later). It also
    allows it to manipulate large datasets or use machine learning models. So far,
    we have been using small datasets and hence we could load anything, but what if
    we start putting models of millions of parameters inside our app? Our app might
    crash. If we use models or other elements that require long computations, we need
    to focus on optimizing our app’s efficiency by caching results in memory and avoiding
    redundant calculations. We can see the cache as a kind of short-term memory, where
    we keep information that we use often or think will be useful to safeguard. Caching
    allows us to reuse this information and save computation. If we have a function
    that performs a large computation, we can use two alternatives:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存允许我们的应用即使在从网络加载数据时也能保持性能（我们将在后面讨论如何从网络或用户那里添加数据）。它还允许它操作大型数据集或使用机器学习模型。到目前为止，我们一直在使用小型数据集，因此我们可以加载任何东西，但如果我们开始在应用中放置数百万参数的模型呢？我们的应用可能会崩溃。如果我们使用需要长时间计算的计算模型或其他元素，我们需要通过在内存中缓存结果并避免重复计算来优化我们应用的高效性。我们可以将缓存视为一种短期记忆，其中我们保存经常使用或认为有用的信息以保护。缓存允许我们重用这些信息并节省计算。如果我们有一个执行大型计算的功能，我们可以使用两种替代方案：
- en: '`st.cache_data`: This is a decorator in Streamlit that is used to cache the
    results of a function so that the function need not be recomputed every time the
    app is rerun (such as when a user interacts with widgets or the app reloads).
    This decorator is recommended for cache computations that return data. One should
    use `st.cache_data` when a function returns a serializable data object (e.g.,
    `str`, `int`, `float`, `DataFrame`, `dict`, or `list`). When a function is wrapped
    with `@st.cache_data`, the first time the function is called, Streamlit stores
    the result in memory or a disk cache, depending on the configuration. On subsequent
    calls with the same arguments, Streamlit returns the cached result, which is much
    faster than recomputing it. It speeds up the app by preventing redundant work,
    especially for functions that take a long time to execute. If the inputs to the
    function change, Streamlit will invalidate the cache and recompute the function.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.cache_data`: 这是 Streamlit 中的一个装饰器，用于缓存函数的结果，以便在应用程序重新运行时（例如，当用户与小部件交互或应用程序重新加载时）无需重新计算函数。此装饰器建议用于缓存返回数据的计算。当函数返回可序列化的数据对象（例如，`str`、`int`、`float`、`DataFrame`、`dict`
    或 `list`）时，应使用 `st.cache_data`。当函数用 `@st.cache_data` 包装时，第一次调用该函数时，Streamlit 会根据配置将其结果存储在内存或磁盘缓存中。在后续调用中，如果使用相同的参数，Streamlit
    会返回缓存的結果，这比重新计算它要快得多。它通过防止重复工作来加快应用程序的速度，特别是对于执行时间较长的函数。如果函数的输入发生变化，Streamlit
    将使缓存失效并重新计算函数。'
- en: '`st.cache_resource`: This is another decorator in Streamlit, introduced to
    handle the caching of resources – specifically, objects or expensive operations
    that do not depend on the function arguments but instead represent reusable resources
    that can be cached for the lifetime of the app. While `st.cache_data` is used
    for caching the results of computations or data loads based on the inputs, `st.cache_resource`
    is designed to cache resources such as database connections, model objects, or
    any other object that is expensive to create or initialize but doesn’t change
    with each function call. Use this for caching resources such as database connections,
    machine learning models, network connections, or any expensive resource that needs
    to be reused across multiple runs of the app. If an object or resource (e.g.,
    a pre-trained model) is expensive to create, you can use `st.cache_resource` to
    avoid reloading or reinitializing it multiple times.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.cache_resource`: 这是在 Streamlit 中引入的另一个装饰器，用于处理资源的缓存——具体来说，是指那些不依赖于函数参数但代表可重用资源的对象或昂贵的操作，这些资源可以在应用程序的生命周期内进行缓存。虽然
    `st.cache_data` 用于缓存基于输入的计算结果或数据加载，但 `st.cache_resource` 被设计用来缓存诸如数据库连接、模型对象或其他创建或初始化成本高昂但每次函数调用不发生变化的任何对象。使用此装饰器来缓存数据库连接、机器学习模型、网络连接或任何需要在应用程序多次运行中重复使用的昂贵资源。如果一个对象或资源（例如，一个预训练模型）创建成本高昂，可以使用
    `st.cache_resource` 来避免多次重新加载或重新初始化它。'
- en: 'For example, for `st.cache_data`, in the following code, we are simulating
    a slow operation and showing how caching is saving time:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于 `st.cache_data`，在以下代码中，我们模拟了一个慢速操作，并展示了缓存如何节省时间：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding snippet, under the hood, before running a function, Streamlit
    checks its cache for a previously saved result. If it finds one, it uses that
    instead of running the function; if it doesn’t find it, it runs the function and
    saves it in the cache. The cache is updated during execution, especially if the
    code changes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，在运行函数之前，Streamlit 会检查其缓存以查找之前保存的结果。如果找到了，它会使用该结果而不是运行函数；如果没有找到，它会运行函数并将其保存到缓存中。缓存会在执行过程中更新，尤其是在代码发生变化时。
- en: By default, Streamlit doesn’t save the information between app reruns, but with
    each rerun, it reruns the app from top to bottom. Normally, Streamlit reruns the
    entire script whenever there’s an interaction (e.g., when a user adjusts a slider
    or clicks a button). With session state, you can store data that persists during
    these reruns so you don’t lose values when the script reruns. Each user gets their
    own independent session state, so data stored in the session state is isolated
    from other users. You can use the session state to store things such as form inputs,
    counters, authentication data, or intermediate computation results.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Streamlit不会在应用程序重新运行之间保存信息，但每次重新运行时，它都会从头到尾重新运行应用程序。通常，Streamlit在每次交互时都会重新运行整个脚本（例如，当用户调整滑块或点击按钮时）。使用会话状态，您可以存储在重新运行期间持续存在的数据，这样在脚本重新运行时就不会丢失值。每个用户都有自己的独立会话状态，因此会话状态中存储的数据与其他用户隔离。您可以使用会话状态来存储诸如表单输入、计数器、身份验证数据或中间计算结果等。
- en: 'Let’s try building an app that makes a shopping list; we will show how to save
    information about the session:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试构建一个制作购物清单的应用程序；我们将展示如何保存会话信息：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is our initial app; we will see immediately afterward how we can view
    information saved by the user:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们最初的应用程序；我们将在之后立即看到我们如何查看用户保存的信息：
- en: '![Figure 10.6 – Example of grocery list app](img/B21257_10_06.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – 购物清单应用示例](img/B21257_10_06.jpg)'
- en: Figure 10.6 – Example of grocery list app
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 购物清单应用示例
- en: 'If we add objects by clicking on **Add Item**, they will be added to the list
    (at this time, the information is not saved; it remains only for the session):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过点击**添加项目**来添加对象，它们将被添加到列表中（此时，信息尚未保存；它仅保留在当前会话中）：
- en: '![Figure 10.7 – Example of adding objects to the grocery list app](img/B21257_10_07.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7 – 购物清单应用中添加对象的示例](img/B21257_10_07.jpg)'
- en: Figure 10.7 – Example of adding objects to the grocery list app
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 购物清单应用中添加对象的示例
- en: However, if we press *R* and rerun our app, we will lose this information, and
    the elements will disappear (because the information is not saved anywhere).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们按下*R*并重新运行我们的应用程序，我们将丢失这些信息，元素将消失（因为信息没有保存到任何地方）。
- en: 'Now, let’s try `session_state`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试`session_state`：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: When we use `st.session_state`, the items we add will be preserved during the
    current session. On the first run, the list will contain the initial elements,
    and as the user adds more items, the list will grow accordingly.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用`st.session_state`时，我们添加的项目将在当前会话中保留。在第一次运行时，列表将包含初始元素，随着用户添加更多项目，列表将相应增长。
- en: However, once the page is reloaded or the session ends, the list will reset
    unless we store the data in a persistent location (e.g., a file or database).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一旦页面重新加载或会话结束，除非我们将数据存储在持久位置（例如文件或数据库），否则列表将重置。
- en: '![Figure 10.8 – Updated list](img/B21257_10_08.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.8 – 更新后的列表](img/B21257_10_08.jpg)'
- en: Figure 10.8 – Updated list
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – 更新后的列表
- en: While using `st.session_state` allows temporary storage of values during a user
    session—gradually filling up as interactions occur—this data is lost upon a full
    page reload or app restart. In contrast, `st.connection` enables Streamlit to
    maintain persistent access to external resources, ensuring that data remains available
    across sessions and reloads. This makes it ideal for applications that require
    consistent interaction with long-lived data, overcoming the limitations of in-memory
    session state. `st.connection` allows the connection to external services to be
    maintained and reused and does so efficiently with each user interaction.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`st.session_state`允许在用户会话期间临时存储值——随着交互的发生逐渐填满——但在完整页面重新加载或应用程序重启时，这些数据会丢失。相比之下，`st.connection`使Streamlit能够维持对外部资源的持久访问，确保数据在会话和重新加载之间保持可用。这使得它非常适合需要与长期数据持续交互的应用程序，克服了内存会话状态的限制。`st.connection`允许维护和重用与外部服务的连接，并且在与每个用户交互时都高效地做到这一点。
- en: 'Let’s see how `st.connection` works in practice:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`st.connection`在实际中的工作方式：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this section, we discussed the main components of a Streamlit application.
    In the next one, we will discuss how to beautify our app and make the user experience
    better.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了Streamlit应用程序的主要组件。在下一节中，我们将讨论如何美化我们的应用程序并改善用户体验。
- en: Developing our frontend with Streamlit
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Streamlit开发我们的前端
- en: In this section, we will begin to discuss some of the elements that allow us
    to improve the user experience when interacting with our app.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开始讨论一些允许我们改善与我们的应用程序交互时用户体验的元素。
- en: We will show the various frontend elements and how to combine them for complex
    apps.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将展示各种前端元素以及如何将它们组合成复杂的应用程序。
- en: Adding the text elements
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加文本元素
- en: 'To improve our user experience, we can start by improving the text elements.
    The first elements we add are the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了改善用户体验，我们可以从改善文本元素开始。我们首先添加的元素如下：
- en: '`st.title()`: This sets the main title of your Streamlit app. It’s the largest
    text element and is typically used for the main heading of your app. Every app
    should have at least one title, and this is shown in the GitHub-flavored Markdown.
    This function obviously takes a string.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.title()`: 这个函数设置Streamlit应用的主标题。它是最大的文本元素，通常用于应用的主要标题。每个应用都应该至少有一个标题，这在GitHub风格的Markdown中也有显示。这个函数显然接受一个字符串。'
- en: '`st.header()`: This adds a header to your app. It’s smaller than the title
    but still stands out as an important section heading. This also has a counterpart
    in GitHub and is similar in purpose. One attribute you can add is `divider`, which
    shows a colored divider below the header (we can specify a color). Also, we can
    add a `help` string that provides a tooltip next to the header.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.header()`: 这个函数为您的应用添加标题。它比标题小，但仍然作为一个重要的部分标题突出。这也在GitHub中有对应的功能，目的相似。您可以添加一个属性`divider`，在标题下方显示一个彩色分隔符（我们可以指定颜色）。我们还可以添加一个`help`字符串，在标题旁边提供工具提示。'
- en: '`st.subheader()`: This adds a subheader, which is smaller than the header and
    is typically used for subsections or to provide additional structure to the content.
    The subheader can also have a colored divider if you want one. A help `string`
    is also possible.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.subheader()`: 这个函数添加一个副标题，它比标题小，通常用于子部分或为内容提供额外的结构。副标题也可以有一个彩色分隔符，如果您需要的话。还可以添加一个`help`字符串。'
- en: 'Here are some examples of how to insert these elements:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些如何插入这些元素的示例：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we can test them directly in our app:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在我们的应用中直接测试它们：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This code shows how to start inserting stylistic elements into our app. The
    following figure shows the result after these improvements:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码展示了如何将风格元素开始添加到我们的应用中。以下图显示了这些改进后的结果：
- en: '![Figure 10.9 – Updated app](img/B21257_10_09.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图10.9 – 更新后的应用](img/B21257_10_09.jpg)'
- en: Figure 10.9 – Updated app
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 – 更新后的应用
- en: Inserting images in a Streamlit app
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Streamlit应用中插入图片
- en: 'Next, we begin the customization of our app, adding both a logo and an image.
    To do this, we will use several elements:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开始定制我们的应用，添加一个标志和一张图片。为此，我们将使用几个元素：
- en: '`st.set_page_config(...)`: This function is used to configure the Streamlit
    app’s page settings, such as the title of the page, favicon (icon in the browser
    tab), and layout preferences. In this case, we will use it to add a small icon
    that will be seen as a browser tab element.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.set_page_config(...)`: 这个函数用于配置Streamlit应用的页面设置，例如页面的标题、favicon（浏览器标签中的图标）和布局偏好。在这种情况下，我们将使用它来添加一个小的图标，它将作为浏览器标签元素显示。'
- en: '`st.image(...)`: This function displays an image in the Streamlit app. It takes
    the URL or path of the image and can adjust its width to fit the screen with `use_column_width=True`.
    As input, `st.image` takes either a URL (as we are doing in this case) or a path
    to a local image or `numpy.array` (the image can be in number format).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.image(...)`: 这个函数在Streamlit应用中显示图片。它接受图片的URL或路径，并且可以通过设置`use_column_width=True`来调整图片宽度以适应屏幕。作为输入，`st.image`可以接受URL（正如我们在这个例子中所做的那样）或指向本地图片的路径或`numpy.array`（图片可以以数字格式存在）。'
- en: One of the keywords is `caption`, which allows us to provide a caption for the
    image directly. In our case, however, we will add the caption separately.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关键词之一是 `caption`，它允许我们直接为图片提供说明。然而，在我们的情况下，我们将单独添加说明。
- en: '`st.caption(...)`: This function adds a small caption or descriptive text below
    elements, such as images or charts. In our app, it provides the image credit.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.caption(...)`: 这个函数为元素（如图片或图表）添加下方的简短说明或描述性文本。在我们的应用中，它提供图片的来源信息。'
- en: '`st.sidebar.image(...)`: This places an image in the sidebar, which will be
    the collapsible menu on the left side of the app. The sidebar is useful for placing
    navigation, settings, or additional content.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.sidebar.image(...)`: 这个函数在侧边栏中放置一个图片，它将是应用左侧的可折叠菜单。侧边栏对于放置导航、设置或附加内容非常有用。'
- en: 'We will now insert an image:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将插入一张图片：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code shows how to insert an image with the proper caption. The
    following figure shows the results:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码展示了如何使用适当的说明插入图片。以下图显示了结果：
- en: '![Figure 10.10 – Changes in appearance in the app](img/B21257_10_10.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图10.10 – 应用外观的变化](img/B21257_10_10.jpg)'
- en: Figure 10.10 – Changes in appearance in the app
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 – 应用外观变化
- en: 'Here is our browser icon:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的浏览器图标：
- en: '![Figure 10.11 – The browser icon](img/B21257_10_11.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.11 – 浏览器图标](img/B21257_10_11.jpg)'
- en: Figure 10.11 – The browser icon
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 – 浏览器图标
- en: Thus far, we have explored the basic features of Streamlit and used them to
    build a simple and static app. Now it’s time to move beyond and start exploring
    what makes a Streamlit app dynamic, responsive, and connected to real use.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探索了 Streamlit 的基本功能，并使用它们构建了一个简单且静态的应用程序。现在，是时候超越这些功能，开始探索使 Streamlit
    应用程序动态、响应和与实际使用相连的因素。
- en: Creating a dynamic app
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建动态应用程序
- en: 'We can further modify our app to make it more dynamic. So far, our user can
    only add items to their list, and then the list is shown. This app is of little
    use, so we want to make it more dynamic and allow the user to add quantities.
    So, we’re going to do the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步修改我们的应用程序，使其更加动态。到目前为止，我们的用户只能将项目添加到列表中，然后显示列表。这个应用程序用处不大，所以我们希望使其更加动态，并允许用户添加数量。因此，我们将执行以下操作：
- en: Allow the user to add an item to buy. Once the item is added, two sliders are
    created that represent the quantity the user has at home and how much they have
    to buy. To avoid creating an endless list, we will use two columns. In addition,
    we will add a button to select whether or not the user has taken the ingredient.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许用户添加要购买的项目。一旦添加了项目，就会创建两个滑块，分别代表用户家中拥有的数量和需要购买的数量。为了避免创建无限长的列表，我们将使用两列。此外，我们还将添加一个按钮来选择用户是否已取用该成分。
- en: Make an interactive display of a table with ingredients, showing how much to
    buy, how much was taken, and whether it was taken, as well as a completion bar
    that shows how many items have been taken and how many are missing.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制作一个交互式的表格显示，显示成分，包括购买的数量、已取用的数量以及是否已取用，以及一个显示已取用项目和缺失项目的完成条。
- en: In the sidebar, add a button to download the list.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在侧边栏中添加一个按钮以下载列表。
- en: 'Let’s start by displaying the grocery list items in a structured manner using
    two columns, ensuring a more compact and visually balanced layout:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从使用两列以结构化的方式显示杂货清单项目开始，确保更紧凑和视觉上平衡的布局：
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For each item, we determine whether it should be placed in the first column
    (`col1`) or the second column (`col2`) based on whether the index, `i`, is even
    or odd. This ensures that items are distributed evenly between the two columns,
    preventing a long vertical list.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个项目，我们根据索引 `i` 是偶数还是奇数来确定它是否应该放在第一列 (`col1`) 或第二列 (`col2`) 中。这确保了项目在两列之间均匀分布，防止出现长的垂直列表。
- en: 'Inside the selected column, the item name is displayed in bold using `st.markdown()`.
    Below the name, two sliders are created: one for the quantity the user has at
    home and another for the quantity they need to take. Each slider is assigned a
    unique key based on the item name to ensure proper tracking and persistence of
    values. The values from these sliders are stored back into the session state so
    they remain updated across app interactions. In addition, a checkbox is included
    for each item. The collected data for each item, including its name, the selected
    quantities, and whether it has been taken or not, is appended to the data list.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在选定的列中，项目名称使用 `st.markdown()` 以粗体显示。名称下方创建了两个滑块：一个用于用户家中拥有的数量，另一个用于需要携带的数量。每个滑块根据项目名称分配一个唯一的键，以确保正确跟踪和持久化值。这些滑块的值存储回会话状态，以便在应用交互中保持更新。此外，每个项目还包括一个复选框。每个项目的收集数据，包括其名称、选定的数量以及是否已取用，被附加到数据列表中。
- en: '![Figure 10.12 – Restyling of the app](img/B21257_10_12.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.12 – 应用重设计](img/B21257_10_12.jpg)'
- en: Figure 10.12 – Restyling of the app
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 – 应用重设计
- en: 'Notice that the app is interactive (we can interact with sliders):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到应用程序是交互式的（我们可以与滑块交互）：
- en: '![Figure 10.13 – Interactive elements](img/B21257_10_13.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.13 – 交互元素](img/B21257_10_13.jpg)'
- en: Figure 10.13 – Interactive elements
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13 – 交互元素
- en: The preceding figure shows us how to insert interactive elements and how we
    can interact with them. Streamlit allows this in the background, without the need
    for us to code these complex elements, and we can use simple commands.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图示展示了如何插入交互元素以及我们如何与之交互。Streamlit 在后台允许这样做，无需我们编写这些复杂元素，我们可以使用简单的命令。
- en: 'We can then display the table:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以显示表格：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Figure 10.14 – Table obtained](img/B21257_10_14.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.14 – 获得表格](img/B21257_10_14.jpg)'
- en: Figure 10.14 – Table obtained
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14 – 获得表格
- en: 'At this point, we can create our progress bar:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以创建我们的进度条：
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Figure 10.15 – Progress bar](img/B21257_10_15.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.15 – 进度条](img/B21257_10_15.jpg)'
- en: Figure 10.15 – Progress bar
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.15 – 进度条
- en: 'Next, we define a function, `generate_pdf()`, which creates a PDF document
    containing the grocery list data and allows users to download it:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义了一个函数 `generate_pdf()`，它创建一个包含购物清单数据的 PDF 文档，并允许用户下载它：
- en: '[PRE17]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: First, we initialize an `FPDF` object with automatic page breaks and add a new
    page. The font is set to `Arial` with a size of `12` for consistent formatting.
    To enhance the PDF visually, the `generate_pdf()` function downloads a logo from
    a specified URL, saves it locally as `logo.jpg`, and embeds it in the top-left
    corner of the page. A centered title, `Grocery List`, is added, followed by some
    spacing to ensure the text does not overlap with the logo. The function then iterates
    through the grocery list stored in `DataFrame (df)`, adding each item’s name,
    quantities at home and to take, and whether the item has been marked as taken.
    Once the document is populated, it is saved in the current working directory as
    `grocery_list.pdf` and returned.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用自动分页的 `FPDF` 对象初始化一个新的页面，并将字体设置为 `Arial`，字号为 `12` 以保持格式的一致性。为了增强 PDF
    的视觉效果，`generate_pdf()` 函数从指定的 URL 下载一个标志，将其本地保存为 `logo.jpg`，并将其嵌入到页面的左上角。接着添加一个居中的标题“购物清单”，并留出一些空白以确保文本不会与标志重叠。然后，该函数遍历存储在
    `DataFrame (df)` 中的购物清单，添加每个项目的名称、家中和要携带的数量，以及该项目是否已被标记为已取。一旦文档内容填充完毕，它就被保存在当前工作目录下，文件名为
    `grocery_list.pdf`，并返回。
- en: '![Figure 10.16 – The PDF button](img/B21257_10_16.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.16 – PDF 按钮](img/B21257_10_16.jpg)'
- en: Figure 10.16 – The PDF button
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.16 – PDF 按钮
- en: 'Here is the generated PDF:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是生成的 PDF：
- en: '![Figure 10.17 – The obtained PDF file](img/B21257_10_17.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.17 – 获得的 PDF 文件](img/B21257_10_17.jpg)'
- en: Figure 10.17 – The obtained PDF file
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.17 – 获得的 PDF 文件
- en: 'Our users may want to add notes; for this, we can take advantage of the fact
    that Streamlit allows other pages to be added to create a section for notes. Note
    that we now have a second page that we can access through our sidebar. This way,
    we can enter notes and then save them:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的用户可能想要添加备注；为此，我们可以利用 Streamlit 允许添加其他页面以创建备注部分的事实。请注意，我们现在有一个可以通过侧边栏访问的第二页。这样，我们可以输入备注并保存它们：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Figure 10.18 – Adding another page to the app](img/B21257_10_18.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.18 – 向应用添加另一页](img/B21257_10_18.jpg)'
- en: Figure 10.18 – Adding another page to the app
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.18 – 向应用添加另一页
- en: 'Now, we can also note that the information has been updated in our PDF:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们还可以注意到信息已经更新到我们的 PDF 中：
- en: '![Figure 10.19 – Updated PDF](img/B21257_10_19.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.19 – 更新的 PDF](img/B21257_10_19.jpg)'
- en: Figure 10.19 – Updated PDF
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.19 – 更新的 PDF
- en: 'If our users want to know where the nearest supermarkets are, we could add
    the following functionality to our app:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的用户想知道最近的超市在哪里，我们可以在我们的应用中添加以下功能：
- en: '[PRE19]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the code, we are adding a new page to the Streamlit app where users can find
    nearby supermarkets using `Nominatim` geocoder from the `geopy` library to convert
    the location input into latitude and longitude coordinates. If a valid location
    is found, we confirm this to the user and create an interactive map centered at
    the given coordinates using Folium. A marker is added to indicate the user’s location.
    Next, we use the Overpass API, which queries OSM data, to find supermarkets within
    a 5-kilometer radius. We send a request to the Overpass API and parse the JSON
    response to extract the coordinates and names of nearby supermarkets. Each supermarket
    is then added as a green marker on the map. Finally, we display the generated
    map inside the Streamlit app using `folium_static`. If the location input is invalid
    or not found, we show an error message prompting the user to try again.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们向 Streamlit 应用添加了一个新页面，用户可以使用 `geopy` 库中的 `Nominatim` 地理编码器将位置输入转换为经纬度坐标，以查找附近的超市。如果找到有效的位置，我们向用户确认，并使用
    Folium 创建一个以给定坐标为中心的交互式地图。添加一个标记来指示用户的位置。接下来，我们使用 Overpass API，该 API 查询 OSM 数据，以找到半径为
    5 公里的超市。我们向 Overpass API 发送请求，并解析 JSON 响应以提取附近超市的坐标和名称。然后，每个超市都被添加到地图上的绿色标记。最后，我们使用
    `folium_static` 在 Streamlit 应用中显示生成的地图。如果位置输入无效或未找到，我们显示一个错误消息，提示用户再次尝试。
- en: '![Figure 10.20 – Find Supermarkets page](img/B21257_10_20.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.20 – 查找超市页面](img/B21257_10_20.jpg)'
- en: Figure 10.20 – Find Supermarkets page
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.20 – 查找超市页面
- en: 'When we click **Find Supermarkets**, we get the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们点击 **查找超市** 时，我们得到以下内容：
- en: '![Figure 10.21 – Supermarket map](img/B21257_10_21.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.21 – 超市地图](img/B21257_10_21.jpg)'
- en: Figure 10.21 – Supermarket map
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.21 – 超市地图
- en: Now that we know how to build an app, we can build one with agents.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何构建应用程序，我们可以使用代理来构建一个。
- en: Creating an application with Streamlit and AI agents
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Streamlit和AI代理创建应用程序
- en: In this section, we will look at integrating the multi-agent system described
    in [*Chapter 9*](B21257_09.xhtml#_idTextAnchor156) into an app with Streamlit.
    Here, we will describe only the code parts we change; the structure remains the
    same. In the previous chapter, we built a script that allowed a travel program
    to be defined; in this chapter, the output is the same, but the system is encapsulated
    in an app. In other words, our app will run in the browser and can be used even
    by a user who does not know programming.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨将第9章中描述的多代理系统集成到Streamlit应用程序中。在这里，我们只描述我们更改的代码部分；结构保持不变。在前一章中，我们构建了一个脚本，允许定义旅行程序；在本章中，输出相同，但系统被封装在应用程序中。换句话说，我们的应用程序将在浏览器中运行，甚至可以供不懂得编程的用户使用。
- en: 'As a brief recap, the multi-model *Travel Planning System* is an AI-driven
    assistant that integrates multiple specialized models to generate personalized
    travel plans. It consists of four key agents:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 作为简要回顾，多模型*旅行规划系统*是一个由AI驱动的助手，它集成了多个专业模型以生成个性化的旅行计划。它包括四个关键代理：
- en: '`WeatherAnalysisAgent`: Predicts the best travel months using historical weather
    data'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WeatherAnalysisAgent`：使用历史天气数据预测最佳的旅行月份'
- en: '`HotelRecommenderAgent`: Uses a transformer model to find accommodations that
    match user preferences'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HotelRecommenderAgent`：使用转换器模型找到符合用户偏好的住宿'
- en: '`ItineraryPlannerAgent`: Employs GPT-2 to generate detailed day-by-day travel
    plans'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ItineraryPlannerAgent`：使用GPT-2生成详细的每日旅行计划'
- en: '`SummaryAgent`: Creates professional trip summaries and cost estimates'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SummaryAgent`：创建专业的旅行总结和成本估算'
- en: The system follows a structured data flow, where the user inputs their destination,
    preferences, and duration, and the agents collaborate to deliver a complete travel
    plan. The core AI models include `RandomForestRegressor` for weather predictions,
    `SentenceTransformer` for hotel recommendations, and GPT-2 for itinerary and summary
    generation.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 系统遵循结构化的数据流，其中用户输入他们的目的地、偏好和持续时间，代理协作以提供完整的旅行计划。核心AI模型包括用于天气预测的`RandomForestRegressor`、用于酒店推荐的`SentenceTransformer`以及用于行程和总结生成的GPT-2。
- en: 'To better understand the internal structure of the *Travel Planning System*,
    this section provides three UML diagrams. These visualizations illustrate the
    architecture, execution flow, and system interactions of the application described
    in this chapter:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解*旅行规划系统*的内部结构，本节提供了三个UML图。这些可视化展示了本章所述应用程序的架构、执行流程和系统交互：
- en: '`WeatherAnalysisAgent` and `ItineraryPlannerAgent`), the underlying models
    (`RandomForest`, `SentenceTransformer`, and `OpenAI` GPT), and the Streamlit app
    that connects the user interface to the backend logic:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WeatherAnalysisAgent`和`ItineraryPlannerAgent`）、底层模型（`RandomForest`、`SentenceTransformer`和`OpenAI`
    GPT）以及连接用户界面到后端逻辑的Streamlit应用程序： '
- en: '![Figure 10.22 – Structural UML Diagram for the multi-model Travel Planning
    System](img/B21257_10_22.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图10.22 – 多模型旅行规划系统的结构UML图](img/B21257_10_22.jpg)'
- en: Figure 10.22 – Structural UML Diagram for the multi-model Travel Planning System
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.22 – 多模型旅行规划系统的结构UML图
- en: '**Activity diagram**: The activity diagram describes the control flow of the
    application, starting from user input collection through to the generation of
    a complete travel plan. It illustrates how each agent is triggered and how their
    outputs are merged:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活动图**：活动图描述了应用程序的控制流程，从用户输入收集到生成完整的旅行计划。它说明了每个代理是如何被触发的以及它们的输出是如何合并的：'
- en: '![Figure 10.23 – UML activity diagram for the multi-model Travel Planning System](img/B21257_10_23.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图10.23 – 多模型旅行规划系统的UML活动图](img/B21257_10_23.jpg)'
- en: Figure 10.23 – UML activity diagram for the multi-model Travel Planning System
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.23 – 多模型旅行规划系统的UML活动图
- en: '**Sequence diagram**: Finally, the sequence diagram outlines the time-based
    interactions between the Streamlit frontend, the database, and the AI agents.
    It shows the order of method calls, the data exchanged, and the points where the
    system waits for responses. It makes clear when and how each agent is called:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序列图**：最后，序列图概述了Streamlit前端、数据库和AI代理之间的基于时间的交互。它显示了方法调用的顺序、交换的数据以及系统等待响应的点。它清楚地说明了每个代理何时以及如何被调用：'
- en: '![Figure 10.24 – UML sequence diagram for the multi-model Travel Planning System](img/B21257_10_24.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图10.24 – 多模型旅行规划系统的UML序列图](img/B21257_10_24.jpg)'
- en: Figure 10.24 – UML sequence diagram for the multi-model Travel Planning System
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.24 – 多模型旅行规划系统的UML序列图
- en: 'First, we start by importing the libraries we need:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们开始导入所需的库：
- en: '[PRE20]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`streamlit`: Our library to create the interactive web application'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`streamlit`: 我们用于创建交互式Web应用的库'
- en: '`numpy`: A library for all the numerical operations'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy`: 用于所有数值运算的库'
- en: '`pandas`: A library to handle DataFrames'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`: 用于处理数据框的库'
- en: '`pydeck`: A visualization library built on top of Deck.gl, specifically for
    rendering large-scale geographical data'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pydeck`: 基于Deck.gl构建的可视化库，专门用于渲染大规模地理数据'
- en: '`openai`: The OpenAI Python library, which provides access to models such as
    GPT-3.5 and GPT-4 for **natural language processing** (**NLP**) tasks.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openai`: 提供对GPT-3.5和GPT-4等模型访问的OpenAI Python库，用于**自然语言处理**（**NLP**）任务。'
- en: '`RandomForestRegressor`: The scikit-learn model we use in our app'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RandomForestRegressor`: 我们在应用中使用的scikit-learn模型'
- en: '`SentenceTransformer`: The library for the embeddings (see the previous chapter)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SentenceTransformer`: 用于嵌入的库（参见上一章）'
- en: 'The code for agents is the same, except for `ItineraryPlannerAgent`. For a
    better and smoother response, we use OpenAI’s GPT-4 model here:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的代码相同，除了`ItineraryPlannerAgent`。为了获得更好的响应，我们在这里使用OpenAI的GPT-4模型：
- en: '[PRE21]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The operation is the same: it takes in a travel destination, the best time
    to visit, a recommended hotel, and the trip duration, following which a structured
    itinerary is generated. Note that we need to use an API key to authenticate requests
    to OpenAI’s API. Again, the agent does nothing more than generate an itinerary
    based on the same inputs: travel location, the best months to travel, hotel details,
    and the number of travel days. GPT-4 also works similarly to GPT-2: we have to
    provide a prompt with the information and the model then autoregressively generates
    the travel itinerary'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 操作方式相同：它接受旅行目的地、最佳旅行时间、推荐酒店和旅行时长，随后生成一个结构化的行程。请注意，我们需要使用API密钥来验证对OpenAI API的请求。同样，代理所做的只是根据相同的输入生成行程：旅行地点、最佳旅行月份、酒店详情和旅行天数。GPT-4也类似于GPT-2：我们必须提供一个包含信息的提示，然后模型会自回归地生成旅行行程
- en: 'Here again, we provide the same data that we provided to our system previously
    (you can find it in the repository):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供与之前提供给系统相同的数据（你可以在存储库中找到它）：
- en: '![Figure 10.25 – Screenshot of the code](img/B21257_10_25.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图10.25 – 代码截图](img/B21257_10_25.jpg)'
- en: Figure 10.25 – Screenshot of the code
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.25 – 代码截图
- en: 'At this point, we can initialize our agents, each with its own different purpose:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以初始化我们的代理，每个代理都有自己的不同目的：
- en: '[PRE22]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note, `openai_api_key = st.secrets["general"]["openai_api_key"]` uses Streamlit’s
    secrets manager to securely access the OpenAI API key. In fact, `st.secrets` is
    a way to store and retrieve sensitive credentials in Streamlit apps. The API key
    is stored under `st.secrets["general"]["openai_api_key"]`, indicating it is saved
    inside a `"general"` section within the `secrets` configuration. The purpose of
    `st.secrets` is to prevent sensitive credentials from being hardcoded in the script,
    reducing the risk of privacy breaches.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`openai_api_key = st.secrets["general"]["openai_api_key"]`使用Streamlit的密钥管理器来安全地访问OpenAI
    API密钥。实际上，`st.secrets`是一种在Streamlit应用中存储和检索敏感凭证的方式。API密钥存储在`st.secrets["general"]["openai_api_key"]`下，表示它被保存在`secrets`配置中的`"general"`部分内。`st.secrets`的目的在于防止敏感凭证在脚本中以硬编码的形式存储，从而降低隐私泄露的风险。
- en: 'Now, let’s start building our interface:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始构建我们的界面：
- en: '[PRE23]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'First, we add a title: `st.title()` sets the title of the Streamlit web app.
    This title will appear at the top of the page. At this point, we use `st.write()`
    to give a brief explanation of the app’s purpose. Next, `st.text_input()` is used
    to create a box where the user can enter their destination. Note that we are providing
    a hint about what the user can enter – `"Enter your destination (e.g., Rome):"`
    – and there is a default value of `"Rome"` (if the user doesn’t input anything,
    it defaults to `Rome`). `st.text_area()` creates a multi-line text box where users
    can describe their ideal hotel. We use `text_area` to allow users to provide detailed
    hotel preferences. `st.slider()` creates a slider input for selecting the trip
    duration (there are parameters that define a minimum duration of `1` day and a
    maximum of `14`, with a `5`-day trip being the default duration).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们添加一个标题：`st.title()` 设置Streamlit web应用的标题。这个标题将显示在页面顶部。在这个阶段，我们使用 `st.write()`
    提供对应用目的的简要说明。接下来，`st.text_input()` 用于创建一个用户可以输入目的地的框。请注意，我们提供了一个关于用户可以输入的内容的提示——“`输入你的目的地（例如，罗马）：`”——并且有一个默认值“`罗马`”（如果用户没有输入任何内容，则默认为`罗马`）。`st.text_area()`
    创建一个多行文本框，用户可以在其中描述他们理想的酒店。我们使用 `text_area` 允许用户提供详细的酒店偏好。`st.slider()` 创建一个滑块输入，用于选择旅行时长（有参数定义了最小时长为`1`天，最大为`14`天，默认时长为`5`天）。
- en: '![Figure 10.26 – Input preferences in the app](img/B21257_10_26.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图10.26 – 应用中的输入偏好](img/B21257_10_26.jpg)'
- en: Figure 10.26 – Input preferences in the app
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.26 – 应用中的输入偏好
- en: At this point, we will deal with what happens after the user adds the information
    and presses a button. To recap, the system predicts the best travel months based
    on weather conditions (through the use of historical data and random forest algorithms),
    finds a hotel that matches the user’s preferences (using data on hotels and similarity
    of embeddings), and finally, creates a personalized itinerary using OpenAI’s GPT-4.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们将处理用户添加信息并按下按钮之后发生的事情。为了回顾，系统根据天气条件（通过使用历史数据和随机森林算法）预测最佳旅行月份，找到符合用户偏好的酒店（使用酒店数据和嵌入相似度），最后，使用OpenAI的GPT-4创建个性化行程。
- en: 'We have created the framework to be able to visualize the results: the best
    months to visit, the recommended hotel, the AI-generated itinerary, and finally,
    a map visualization of the destination. All this happens only when our user presses
    the button, which we will create next:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了框架来可视化结果：最佳旅行月份、推荐酒店、AI生成的行程，最后是目的地的地图可视化。所有这些只有在我们的用户按下按钮时才会发生，我们将在下一部分创建这个按钮：
- en: '[PRE24]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`if st.button("Generate Travel Plan` `✨``"):` creates an interactive button
    labeled `best_months = weather_agent.predict_best_time({''latitude'': 41.9028,
    ''longitude'': 12.4964})`. Note that we entered the destination’s latitude (`41.9028`)
    and longitude (`12.4964`) for Rome, got our best months based on the weather score,
    and selected the best month. At this point, we identify the best hotels based
    on our user’s preferences with `hotel_agent.find_hotels(preferences)`. This agent
    will return a list of hotels matching the user’s description.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`if st.button("生成旅行计划✨"):` 创建了一个交互式按钮，标签为 `best_months = weather_agent.predict_best_time({''latitude'':
    41.9028, ''longitude'': 12.4964})`。请注意，我们输入了罗马的目的地纬度（`41.9028`）和经度（`12.4964`），根据天气得分获得了最佳月份，并选择了最佳月份。在这个阶段，我们使用
    `hotel_agent.find_hotels(preferences)` 根据用户的偏好确定最佳酒店。这个代理将返回一个与用户描述相匹配的酒店列表。'
- en: Since we have all the details, we can generate our itinerary. `itinerary = itinerary_agent.create_itinerary(destination,
    best_month, recommended_hotels[0], duration)` does exactly that; it takes the
    inputs defined earlier and produces a structured AI-generated itinerary. Once
    we have our itinerary, we start the display of it for the user. We use `st.subheader("``📆`
    `Best Months to Visit")` to create a subsection and then iterate over `best_months`
    and print each month with its weather score. At this point, we show the best hotels
    in an additional subsection after `st.subheader("``🏨` `Recommended Hotel")`. Finally,
    `st.subheader("``📜` `Generated Itinerary")` allows us to create a subsection where
    our itinerary will be inserted. In the last part, we show the city map.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们拥有所有详细信息，我们可以生成我们的行程。`itinerary = itinerary_agent.create_itinerary(destination,
    best_month, recommended_hotels[0], duration)` 正是如此操作；它接受之前定义的输入并生成一个结构化的 AI 生成的行程。一旦我们有了行程，我们就开始向用户展示它。我们使用
    `st.subheader("``📆` `最佳访问月份")` 创建一个子部分，然后遍历 `best_months` 并打印出每个月及其天气评分。在此阶段，我们在
    `st.subheader("``🏨` `推荐酒店")` 之后显示最佳酒店。最后，`st.subheader("``📜` `生成的行程")` 允许我们在其中插入行程的子部分。在最后一部分，我们展示城市地图。
- en: '![Figure 10.27 – Generated output (part 1)](img/B21257_10_27.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.27 – 生成的输出（第 1 部分）](img/B21257_10_27.jpg)'
- en: Figure 10.27 – Generated output (part 1)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.27 – 生成的输出（第 1 部分）
- en: '![Figure 10.28 – Generated output (part 2)](img/B21257_10_28.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.28 – 生成的输出（第 2 部分）](img/B21257_10_28.jpg)'
- en: Figure 10.28 – Generated output (part 2)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.28 – 生成的输出（第 2 部分）
- en: '![Figure 10.29 – Generated output (part 3)](img/B21257_10_29.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.29 – 生成的输出（第 3 部分）](img/B21257_10_29.jpg)'
- en: Figure 10.29 – Generated output (part 3)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.29 – 生成的输出（第 3 部分）
- en: In this section, we created a multi-agent system and embedded it within an app.
    In this way, even users with no programming knowledge can interact with our system.
    The system can be run by a user by clicking a simple button.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们创建了一个多智能体系统并将其嵌入到应用程序中。这样，即使是没有编程知识的用户也可以与我们的系统交互。用户可以通过点击一个简单的按钮来运行系统。
- en: We discussed an app as an isolated system; in the next section, we will see
    how a model is not an isolated concept but part of an ecosystem. This complexity
    must be taken into account, and in the next section, we will discuss the life
    cycle of a model, from conception to deployment.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了应用程序作为一个孤立系统；在下一节中，我们将看到模型不是一个孤立的概念，而是生态系统的一部分。必须考虑到这种复杂性，在下一节中，我们将讨论模型的生命周期，从构思到部署。
- en: Machine learning operations and LLM operations
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习操作和 LLM 操作
- en: We have seen how to create an app containing a multi-agent system. When we create
    a script with Python, we create an element that can run on our computer, but this
    is not a product. Turning a script into an app allows a user to be able to interact
    with our app even if they do not know how to program. Streamlit allows us to be
    able to run a quick prototype of our app. This is not optimal for a product, especially
    if it is to be used by several users. In this section, we will discuss all those
    operations necessary to make our model function as a product.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何创建一个包含多智能体系统的应用程序。当我们用 Python 编写脚本时，我们创建了一个可以在我们的计算机上运行的元素，但这不是一个产品。将脚本转换成应用程序允许用户即使不知道如何编程也能与我们的应用程序交互。Streamlit
    允许我们快速原型化我们的应用程序。这并不适合产品，尤其是当它要被多个用户使用时。在本节中，我们将讨论所有必要的操作，以使我们的模型作为一个产品运行。
- en: '**Machine Learning Operations** (**MLOps**) is a set of practices and tools
    designed to streamline and manage the life cycle of **machine learning** (**ML**)
    models in production. It combines ML, DevOps, and data engineering practices to
    ensure the **continuous integration/continuous delivery** (**CI/CD**), monitoring,
    and scaling of ML systems.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习操作**（**MLOps**）是一套旨在简化和管理生产中机器学习模型生命周期的实践和工具。它结合了机器学习、DevOps 和数据工程实践，以确保机器学习系统的**持续集成/持续交付**（**CI/CD**）、监控和扩展。'
- en: '![Figure 10.30 – MLOps combination (https://arxiv.org/pdf/2202.10169)](img/B21257_10_30.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.30 – MLOps 组合](img/B21257_10_30.jpg)'
- en: Figure 10.30 – MLOps combination ([https://arxiv.org/pdf/2202.10169](https://arxiv.org/pdf/2202.10169))
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.30 – MLOps 组合 ([https://arxiv.org/pdf/2202.10169](https://arxiv.org/pdf/2202.10169))
- en: 'MLOps plays a key role in turning a model into a useful application in the
    real world. In short, MLOps encompass the development, monitoring, and maintenance
    of models in a production environment, enabling the transition from a research
    product to a functional product. Here are the various stages involved:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps在将模型转化为现实世界中有用的应用中发挥着关键作用。简而言之，MLOps涵盖了在生产环境中模型的发展、监控和维护，使得从研究产品到功能产品的转变成为可能。以下是涉及的各个阶段：
- en: '**Model development**: This is the first step, in which an ML model is designed
    and trained. Typically, at this stage, both data scientists and data engineers
    collaborate on the choice of model, datasets, and training and testing process.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型开发**：这是第一步，其中设计并训练了机器学习模型。通常，在这个阶段，数据科学家和数据工程师在模型选择、数据集以及训练和测试过程中进行合作。'
- en: '**Testing**: Normally, the testing phase is part of model development; however,
    today, there is a greater emphasis on testing the model. Hence, we consider it
    a separate stage. In fact, complex models in particular can exhibit unexpected
    behaviors, so testing is often considered a separate phase.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试**：通常，测试阶段是模型开发的一部分；然而，今天，对模型的测试更加重视。因此，我们将其视为一个单独的阶段。实际上，特别是复杂模型可能会表现出意外的行为，因此测试通常被视为一个单独的阶段。'
- en: '**Deployment**: Once the model has been developed and tested, it can be deployed
    in a production environment. This delicate step requires that the model be integrated
    with other existing systems (which have been developed previously) and that it
    can be used in real time.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署**：一旦模型开发和测试完成，它可以在生产环境中部署。这一微妙的步骤要求模型与其他现有系统（先前已开发）集成，并且能够实时使用。'
- en: '**Monitoring and maintenance**: Once the model is deployed, we must ensure
    its performance doesn’t degrade and prevent operational problems. At the same
    time, we may need to update the model or ensure compatibility with new system
    elements.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**监控和维护**：一旦模型部署，我们必须确保其性能不会下降，并防止运营问题。同时，我们可能需要更新模型或确保与新的系统元素兼容。'
- en: '![Figure 10.31 – High-level process view of MLOps (https://arxiv.org/pdf/2202.10169)](img/B21257_10_31.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图10.31 – MLOps的高级流程视图 (https://arxiv.org/pdf/2202.10169)](img/B21257_10_31.jpg)'
- en: Figure 10.31 – High-level process view of MLOps ([https://arxiv.org/pdf/2202.10169](https://arxiv.org/pdf/2202.10169))
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.31 – MLOps的高级流程视图 ([https://arxiv.org/pdf/2202.10169](https://arxiv.org/pdf/2202.10169))
- en: '**Large Language Model Operations** (**LLMOps**) is an extension of MLOps specifically
    focused on the deployment, maintenance, and management of LLMs. It incorporates
    the principles of MLOps but also addresses the unique challenges and needs associated
    with working with large-scale NLP models.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型操作**（**LLMOps**）是MLOps的一个扩展，专门关注LLMs的部署、维护和管理。它结合了MLOps的原则，但也解决了与大规模NLP模型工作相关的独特挑战和需求。'
- en: 'However, LLMOps adds additional complexity. Here’s why:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLMOps增加了额外的复杂性。原因如下：
- en: '**Model size and complexity**: In MLOps, models can vary in size and complexity,
    but they typically don’t require as much computational power or memory as LLMs.
    Models may include traditional ML algorithms, smaller deep learning models, or
    specialized models for structured data. LLMs can be in the order of billions of
    parameters and thus require optimized infrastructure (often involving specialized
    hardware such as GPUs or TPUs) or distributed training. This means more expertise
    and dedicated infrastructure (dedicated hardware and storage), which can be very
    expensive.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型大小和复杂性**：在MLOps中，模型的大小和复杂性可能有所不同，但通常它们不需要像LLMs那样多的计算能力或内存。模型可能包括传统的机器学习算法、较小的深度学习模型或针对结构化数据的专用模型。LLMs可能有数十亿个参数，因此需要优化的基础设施（通常涉及专门的硬件，如GPU或TPU）或分布式训练。这意味着需要更多的专业知识和专用基础设施（专用硬件和存储），这可能非常昂贵。'
- en: '**Training and fine-tuning**: In MLOps, training is much more manageable. Many
    of the models are small in size and can therefore be easily retrained. Retraining
    itself can be conducted programmatically. Fine-tuning LLMs is more complex and
    resource-intensive. Collecting and processing the datasets needed for an LLM is
    resource-intensive.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练和微调**：在MLOps中，训练要容易得多。许多模型体积较小，因此可以轻松重新训练。重新训练本身可以程序化进行。微调LLMs更复杂且资源密集。收集和处理LLM所需的数据集是资源密集型的。'
- en: '**Scalability and deployment**: In MLOps, deploying models to production is
    usually straightforward. Scaling LLMs, on the other hand, requires dedicated infrastructure
    that can ensure necessary support when there is high demand. In fact, latency
    can increase considerably when there are many users. Optimizing latency during
    inference can be a delicate process that risks degrading performance.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性和部署**：在MLOps中，将模型部署到生产环境通常是直接的。另一方面，扩展LLM需要能够确保在高需求时提供必要支持的专用基础设施。实际上，当有大量用户时，延迟可能会显著增加。优化推理过程中的延迟可能是一个微妙的过程，可能会降低性能。'
- en: '**Monitoring and maintenance**: Monitoring ML models in production involves
    tracking key metrics such as accuracy, precision, and recall, as well as model
    drift or data drift. Monitoring LLMs involves not only the usual performance metrics
    but also the quality of text generation, user feedback, and ethical concerns such
    as biased or harmful outputs. While it is straightforward to evaluate an output
    in terms of accuracy, it is more complex to assess whether an LLM produces hallucinations
    or inappropriate or harmful content. Some biases might be subtle but still be
    noticed by users.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控和维护**：在生产环境中监控ML模型涉及跟踪关键指标，如准确率、精确率和召回率，以及模型漂移或数据漂移。监控LLM不仅包括通常的性能指标，还包括文本生成的质量、用户反馈以及如偏见或有害输出等伦理问题。虽然从准确率的角度评估输出是直接的，但评估LLM是否产生幻觉或不适或有害的内容则更为复杂。一些偏见可能很微妙，但仍可能被用户注意到。'
- en: '**Model governance and compliance**: While governance and compliance are critical
    in any ML deployment, MLOps primarily focuses on ensuring data privacy and model
    transparency, especially when dealing with sensitive or regulated data. For LLMOps,
    there is not only privacy, but it can also be used to generate text on a wide
    variety of topics with the risk of generating inappropriate content. With regulations
    in development, assessing bias, fairness, and ethical issues is complex and evolving.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型治理和合规性**：虽然治理和合规性在任何ML部署中都至关重要，但MLOps主要关注确保数据隐私和模型透明度，尤其是在处理敏感或受监管的数据时。对于LLMOps，不仅涉及隐私问题，而且还可以用于生成涉及广泛主题的文本，存在生成不适当内容的风险。随着法规的发展，评估偏见、公平性和伦理问题既复杂又不断演变。'
- en: Here’s an example of the added complexity involved when performing LLMOps. If
    we wanted to train a model from scratch, we would have to retrieve a corpus of
    at least 1B tokens. These tokens would have to be collected from different sources
    (books, websites, articles, code repositories, and so on). In MLOPs, we usually
    create a model when a dataset is already present (e.g., through user interactions
    with our site). The steps of preprocessing a dataset for a classical model (images
    or tabular) are much simpler than a large corpus (steps such as debiasing, eliminating
    duplicates, and so on). Also, since our dataset can be over hundreds of terabytes
    in size, there is more complexity. While we can train an ML model easily (even
    on a consumer computer), this is no longer possible with an LLM. Especially for
    larger ones, we have to use dedicated infrastructure, and we cannot do many experiments
    (testing different hyperparameters or different architecture combinations). Similarly,
    fine-tuning will be preferred to having to retrain our model.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行LLMOps时，涉及到的附加复杂性可以通过以下例子来展示。如果我们想要从头开始训练一个模型，我们就需要检索至少包含10亿个标记的语料库。这些标记需要从不同的来源收集（如书籍、网站、文章、代码库等）。在MLOps中，我们通常在数据集已经存在的情况下创建模型（例如，通过用户与我们的网站互动）。为经典模型（图像或表格）预处理数据集的步骤比大型语料库（如去偏、消除重复等步骤）要简单得多。此外，由于我们的数据集可能超过数百个TB，因此复杂性也更大。虽然我们可以轻松地训练一个ML模型（甚至是在消费级计算机上），但LLM则不再如此。特别是对于更大的模型，我们必须使用专用基础设施，并且无法进行许多实验（测试不同的超参数或不同的架构组合）。同样，微调将比重新训练我们的模型更受欢迎。
- en: Testing also no longer relies on simple measures (such as accuracy) but requires
    human-in-the-loop evaluations. Given the language-centric nature of the system,
    a metric such as accuracy gives us only partial information about the output of
    our model. Only humans (even if we use other LLMs to check at scale) can evaluate
    the output of an LLM in terms of creativity, bias, quality, and the presence of
    inappropriate content. Also, after pre-training, there is usually a step where
    human feedback is used to be able to further improve the output of a model. In
    addition, we must then continue to evaluate our LLM, because the traffic may grow
    or there may be evolutions in the language and knowledge that our model must have.
    For example, an LLM for medical use needs to be updated on new therapies.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 测试也不再依赖于简单的度量（如准确率），而是需要人工参与的评价。鉴于系统的语言中心特性，准确率这样的指标只能为我们提供关于模型输出的部分信息。只有人类（即使我们使用其他大型语言模型进行大规模检查）才能从创造力、偏见、质量和不适当内容的存在等方面评估大型语言模型的输出。此外，在预训练之后，通常有一个步骤会使用人类反馈来进一步提高模型的输出。此外，我们还必须继续评估我们的LLM，因为流量可能会增长，或者我们的模型必须具备的语言和知识可能会发生变化。例如，用于医疗用途的LLM需要更新新的治疗方法。
- en: In the next section, we will start with the complexities of developing a model
    as complex as an LLM.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将从开发像LLM这样复杂的模型所面临的复杂性开始。
- en: Model development
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型开发
- en: 'The development of a model starts with the collection of a corpus. This collection
    is generally divided into two types: general data and specialized data. General
    data represents data such as web pages, books, and conversational text. Specialized
    data, on the other hand, is data that is designed for a specific task, such as
    multilingual data, scientific data, and code:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的开发始于语料库的收集。这种收集通常分为两种类型：通用数据和专用数据。通用数据代表网页、书籍和对话文本等数据。另一方面，专用数据是为特定任务设计的，例如多语言数据、科学数据和代码：
- en: '**General data**: Considering the large amount of data on the internet, it
    is now common for data collection to start with using datasets of downloaded pages
    or even conducting crawling to collect new data. In addition, there are also datasets
    of conversations (such as discussions on Reddit or other platforms), chats with
    LLMs, and other sources. Books are another popular source for training, as they
    generally contain coherent, quality text on disparate topics. These datasets contain
    a mixture of quality data (such as Wikipedia and blog posts) but also a large
    amount of data that needs to be removed, such as spam, toxic posts, and so on.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用数据**：考虑到互联网上数据量巨大，现在数据收集通常从使用下载的页面数据集或进行爬取以收集新数据开始。此外，还有对话数据集（如Reddit或其他平台上的讨论）、与LLM的聊天和其他来源。书籍也是训练的另一个流行来源，因为它们通常包含关于不同主题的连贯、高质量的文本。这些数据集包含质量数据（如维基百科和博客文章）的混合，但也包含大量需要删除的数据，如垃圾邮件、有毒帖子等。'
- en: '**Specialized text data**: Today, it is common to add a multilingual corpus
    to improve the language capabilities of LLMs (e.g., PaLM covers 122 languages
    due to the addition of a multilingual corpus).'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**专用文本数据**：今天，向LLM添加多语言语料库以改善其语言能力是很常见的（例如，由于添加了多语言语料库，PaLM覆盖了122种语言）。'
- en: Adding scientific text enables improved performance in scientific and reasoning
    tasks. Huge datasets of articles exist today that are ready to use and can be
    directly added. Almost all modern pre-training datasets also insert code. The
    addition of code and other structured data appears to be related to an increase
    in performance in some reasoning tasks.
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 添加科学文本可以改善科学和推理任务的表现。今天存在大量可供使用的文章数据集，可以直接添加。几乎所有现代预训练数据集也插入代码。代码和其他结构化数据的添加似乎与某些推理任务性能的提高有关。
- en: '![Figure 10.32 – Ratios of various data sources in the pre-training data for
    existing LLMs (https://arxiv.org/pdf/2303.18223)](img/B21257_10_32.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图10.32 – 现有LLM预训练数据中各种数据源的比例（https://arxiv.org/pdf/2303.18223）](img/B21257_10_32.jpg)'
- en: Figure 10.32 – Ratios of various data sources in the pre-training data for existing
    LLMs ([https://arxiv.org/pdf/2303.18223](https://arxiv.org/pdf/2303.18223))
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.32 – 现有LLM预训练数据中各种数据源的比例（[https://arxiv.org/pdf/2303.18223](https://arxiv.org/pdf/2303.18223)）
- en: Once the data has been collected, it must be preprocessed to remove unnecessary
    tokens such as HTML tags or other presentation elements, reduce text variation,
    and eliminate duplicate data. Today, we try to eliminate data that is of low quality,
    using either heuristic algorithms or classifiers. For example, we can train a
    classifier on quality data such as Wikipedia to recognize what content we want
    to preserve. Heuristic algorithms, on the other hand, rely on a set of rules that
    are defined upstream (such as statistical properties, the presence or absence
    of keywords, and so on). Deduplication is an important step because it impacts
    model diversity and training stability. Typically, different granularities, such
    as sentence or document level, are used to avoid repetitive word patterns. In
    addition, another common step today is privacy reduction, in which an attempt
    is made to remove **personally identifiable information** (**PII**), often through
    a set of rules that are defined upstream. Once these steps are conducted, tokenization
    can be done. Tokenization is considered a crucial step because it largely impacts
    model performance. **Byte-pair encoding** (**BPE**) tokenization is generally
    one of the most widely used methods.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集完毕后，必须进行预处理以去除不必要的标记，如HTML标签或其他展示元素，减少文本变化，并消除重复数据。如今，我们尝试使用启发式算法或分类器消除低质量数据。例如，我们可以在维基百科等质量数据上训练一个分类器，以识别我们想要保留的内容。另一方面，启发式算法依赖于上游定义的一系列规则（如统计属性、关键词的存在或不存在等）。去重是一个重要的步骤，因为它会影响模型多样性和训练稳定性。通常，使用不同的粒度，如句子或文档级别，以避免重复的单词模式。此外，今天另一个常见的步骤是隐私降低，其中试图通过一系列上游定义的规则来移除**个人身份信息**（**PII**）。一旦进行这些步骤，就可以进行分词。分词被认为是一个关键步骤，因为它在很大程度上影响模型性能。**字节对编码**（**BPE**）分词通常是最广泛使用的方法之一。
- en: '![Figure 10.33 – Illustration of a typical data preprocessing pipeline for
    pre-training LLMs (https://arxiv.org/pdf/2303.18223)](img/B21257_10_33.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图10.33 – 预训练LLMs典型数据预处理流程的示意图](https://arxiv.org/pdf/2303.18223)(img/B21257_10_33.jpg)'
- en: Figure 10.33 – Illustration of a typical data preprocessing pipeline for pre-training
    LLMs ([https://arxiv.org/pdf/2303.18223](https://arxiv.org/pdf/2303.18223))
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.33 – 预训练LLMs典型数据预处理流程的示意图([https://arxiv.org/pdf/2303.18223](https://arxiv.org/pdf/2303.18223))
- en: 'Once we have preprocessed the corpus, we can train the model in the next phase.
    To train the model, we need to define a strategy to schedule the multi-sources
    (different types of data such as Wikipedia, text from the internet, books, etc.)
    previously introduced. In fact, two important aspects are decided: the proportion
    of each data source (data mixture) and the order in which each data source is
    scheduled for training (data curriculum). Since each type of data has an impact
    on performance, the data must be mixed in a precise distribution. This distribution
    can be global or local (at certain training steps). To do this, we can then decide
    to conduct upsampling and downsampling of the various sources in order to respect
    the mixture we have decided on. For example, in the case of LLaMA pre-training,
    the authors chose to train with the following proportion (based on experimental
    results, which have shown that this proportion works well): 80% web pages, 6.5%
    code-related data from GitHub and Stack Exchange, 4.5% from books, and 2.5% of
    scientific data sourced from arXiv. These values do not sum to exactly 100%, as
    the remaining portion includes other minor sources not explicitly detailed in
    the original paper. Today, this recipe has been used for many different types
    of LLMs, while LLMs with a specific purpose have a different proportion of code
    and scientific articles.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们预处理了语料库，我们就可以在下一阶段训练模型。为了训练模型，我们需要定义一个策略来安排之前引入的多源（如维基百科、互联网上的文本、书籍等不同类型的数据）。实际上，有两个重要方面被确定：每个数据源的比例（数据混合）以及每个数据源在训练中安排的顺序（数据课程）。由于每种类型的数据都会影响性能，数据必须按照精确的分布混合。这种分布可以是全局的或局部的（在特定的训练步骤）。为此，我们可以决定对各种来源进行上采样和下采样，以尊重我们决定的数据混合。例如，在LLaMA预训练的情况下，作者选择以下比例进行训练（基于实验结果，表明这个比例效果良好）：80%网页，6.5%来自GitHub和Stack
    Exchange的与代码相关的数据，4.5%来自书籍，以及2.5%来自arXiv的科研数据。这些值加起来并不正好是100%，因为剩余的部分包括原始论文中未明确详细说明的其他一些小来源。如今，这个配方已被用于许多不同类型的LLMs，而具有特定目的的LLMs则有不同的代码和科学文章的比例。
- en: Generally, a heterogeneous corpus is preferred, as diversity enhances a model’s
    ability to generalize across domains. In contrast, an overly homogeneous dataset
    can hinder generalization. Additionally, the sequence in which data is presented—often
    referred to as a data curriculum—is crucial. The training data is thus typically
    organized to first develop foundational skills, followed by more specialized capabilities.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，异构语料库更受欢迎，因为多样性增强了模型跨领域的泛化能力。相比之下，过度同质的数据集可能会阻碍泛化。此外，数据呈现的顺序——通常被称为数据课程——至关重要。因此，训练数据通常组织起来，首先发展基础技能，然后是更专业的技能。
- en: 'To do this, you first use easy/general examples and then add examples that
    are more complex or more specific. For example, for models that are code-specific
    such as `CodeLLaMA-Python`, the order is as follows: 2T general tokens, 500B code-heavy
    tokens, and 100B Python-heavy tokens.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，你首先使用简单/通用的例子，然后添加更复杂或更具体的例子。例如，对于像`CodeLLaMA-Python`这样的代码特定模型，顺序如下：2T通用标记，500B代码密集型标记，以及100BPython密集型标记。
- en: In general, it is important that we create pipelines that allow us to collect
    and organize data. Generally, these kinds of pipelines are called **extract, transform,
    load** (**ETL**) pipelines. So, if we want to download a set of web pages, we
    will need to create an ETL pipeline that allows us to download the pages and load
    them into a database along with a set of metadata. The metadata will then be used
    both to clean the data and for data scheduling. Once the data is downloaded it
    needs to be transformed. Because our corpus contains different types of data,
    it is good to have different pipelines for preprocessing the different types (removing
    HTML tags from web pages, removing comments from code, and so on).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，创建允许我们收集和组织数据的管道非常重要。通常，这类管道被称为**提取、转换、加载**（**ETL**）管道。因此，如果我们想下载一组网页，我们需要创建一个ETL管道，使我们能够下载页面并将它们以及一组元数据加载到数据库中。这些元数据将用于数据清洗和数据调度。一旦数据下载完毕，就需要对其进行转换。因为我们的语料库包含不同类型的数据，所以为不同类型的数据预处理设置不同的管道是很好的（例如，从网页中删除HTML标签，从代码中删除注释等）。
- en: In addition, data is an important resource, and access must be controlled. Indeed,
    we need to prevent data leakage and ensure that our corpus complies with regulations
    such as the **General Data Protection Regulation** (**GDPR**). Often, **role-based
    access control** (**RBAC**) is also implemented, where different users have control
    over a different corpus of data. For example, administrators or analysts may have
    different privileges so as to avoid contamination or problems with the data.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据是一个重要的资源，必须控制访问。确实，我们需要防止数据泄露并确保我们的语料库符合如**通用数据保护条例**（**GDPR**）等规定。通常，还会实施**基于角色的访问控制**（**RBAC**），其中不同用户控制不同的数据语料库。例如，管理员或分析师可能拥有不同的权限，以避免数据污染或数据问题。
- en: Once we have our data and have cleaned it, we create features (i.e., the data
    that will be used for training). The feature store is typically a database that
    is optimized to enable training. The idea is to have a dedicated database that
    we can efficiently use for training.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了数据并且已经清理了它，我们就创建特征（即用于训练的数据）。特征存储通常是优化以支持训练的数据库。想法是拥有一个专门的数据库，我们可以高效地用于训练。
- en: '![Figure 10.34 – Automation of the ML pipeline for continuous training (https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)](img/B21257_10_34.jpg)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图10.34 – 持续训练的机器学习管道自动化（https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning）](img/B21257_10_34.jpg)'
- en: Figure 10.34 – Automation of the ML pipeline for continuous training ([https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning))
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.34 – 持续训练的机器学习管道自动化（[https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)）
- en: Model training
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'Once we have our features, we need to decide what our foundation model will
    be. There are two alternatives: use an LLM that has already been trained or conduct
    fine-tuning of an already trained model. In the first case, most models today
    are causal decoders (as we saw in *Chapters 2* and *3*). Although the structure
    remains the base, there are now different alternatives and modifications (such
    as the mixture of experts architecture) and modifications to the attention mechanism
    to increase context and reduce computational cost. Training an LLM from scratch
    is very expensive, however, so most companies focus on using a pre-trained model
    and conducting fine-tuning.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了特征，我们需要决定我们的基础模型将是什么。有两种选择：使用已经训练好的LLM或者对已经训练好的模型进行微调。在前一种情况下，今天的大多数模型都是因果解码器（正如我们在**第2章**和**第3章**中看到的）。尽管结构仍然是基础，但现在有不同替代方案和修改（例如专家混合架构）以及修改注意力机制以增加上下文和减少计算成本。然而，从头开始训练一个LLM是非常昂贵的，因此大多数公司专注于使用预训练模型并进行微调。
- en: Therefore, choosing the foundation model will be an important task. First, we
    must choose a model that has the desired performance in terms of output quality.
    Obviously, the chosen model must be compatible with the resources available to
    us (hardware and cost). In addition, we may want to choose a model that exhibits
    lower performance on general benchmarks but superior performance on some other
    aspects. For example, if our application focuses on having a coding assistant,
    it is better to have an LLM with superior performance on coding benchmarks than
    an LLM that has better wide-ranging capabilities.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，选择基础模型将是一项重要任务。首先，我们必须选择一个在输出质量方面具有所需性能的模型。显然，所选模型必须与我们可用的资源（硬件和成本）兼容。此外，我们可能希望选择在通用基准测试中表现较低但在其他方面表现优异的模型。例如，如果我们的应用侧重于拥有代码助手，那么拥有在编码基准测试中表现优异的LLM（大型语言模型）比拥有更广泛能力的LLM更好。
- en: When choosing a model, we need to take into account that its size impacts both
    its memory footprint and its storage. A larger size means higher costs in general,
    especially if we use a cloud provider. Also, not all models can be used for all
    applications (for example, we cannot use large models for specific devices). In
    addition, a larger model also has higher latency (the time to process an input
    and produce an output). A high latency disrupts the user experience and may lead
    the user to choose a competitor. As we saw in [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042),
    techniques (distillation, quantization, and pruning) to reduce model size while
    maintaining performance exist today. Another important point is the licensing
    of the model. Not all models have an open source license; some models may be available
    in repositories but may not be commercially usable.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择模型时，我们需要考虑到其大小会影响其内存占用和存储。更大的尺寸通常意味着更高的成本，尤其是如果我们使用云服务提供商。此外，并非所有模型都适用于所有应用（例如，我们不能使用大型模型针对特定设备）。此外，更大的模型也有更高的延迟（处理输入并产生输出的时间）。高延迟会破坏用户体验，并可能导致用户选择竞争对手。正如我们在[*第3章*](B21257_03.xhtml#_idTextAnchor042)中看到的，目前存在一些技术（蒸馏、量化和剪枝）可以在保持性能的同时减少模型大小。另一个重要点是模型的许可。并非所有模型都有开源许可；一些模型可能在存储库中可用，但可能不可用于商业用途。
- en: 'Fine-tuning is intended to enable the model to acquire specific skills or some
    particular knowledge. In the former case, it is often referred to as instruction
    tuning. Instruction tuning is a subcategory of the supervised training process
    that aims to make the model more capable of following instructions or being trained
    for specific tasks. In repositories, there are often models that have been simply
    pre-trained or ones that have already undergone an instruction-tuning step. If
    we want the model to acquire a specific set of skills, it might be more interesting
    for us to collect a dataset for instruction tuning. Again, some caveats apply:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的目的是使模型能够获得特定的技能或某些特定的知识。在前一种情况下，它通常被称为指令微调。指令微调是监督训练过程的一个子类别，旨在使模型更能够遵循指令或为特定任务进行训练。在存储库中，通常有只是预训练或已经经过指令微调步骤的模型。如果我们希望模型获得一组特定的技能，那么收集用于指令微调的数据集可能对我们更有趣。再次提醒，以下是一些注意事项：
- en: '**Data** **distribution**: Instruction tuning considers a mix of different
    tasks, so our dataset should respect this principle and contain several examples.
    Ideally, these examples should be of different topics, different contexts, different
    lengths, different styles, and different types of tasks.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分布**：指令微调考虑了不同任务混合，因此我们的数据集应该遵循这一原则，并包含多个示例。理想情况下，这些示例应该涉及不同主题、不同上下文、不同长度、不同风格和不同类型的任务。'
- en: '**Dataset quality**: Generally, in this step (quality check), it is important
    to use examples that are correct not only in terms of factual correctness but
    also in terms of ensuring that the task is done correctly and is well explained.
    For example, chain-of-thought examples are used today, where the intermediate
    thinking is explained instead of just the solution. The examples are human-generated;
    however, to save costs, a larger model can be used initially to create the dataset
    for instruction tuning. For instance, a 70-billion-parameter model could be used
    to prepare the dataset for tuning a 7-billion-parameter model.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集质量**：通常，在这一步（质量检查）中，使用不仅在实际正确性方面正确，而且确保任务完成正确且解释充分的示例非常重要。例如，目前使用的是思维链示例，其中解释的是中间思考过程，而不是仅仅解决方案。这些示例是由人类生成的；然而，为了节省成本，最初可以使用较大的模型来创建指令微调的数据集。例如，一个70亿参数的模型可以用来准备调整70亿参数模型的数据集。'
- en: '**Complexity**: In general, we want our model to acquire capabilities. Through
    simple examples, the model will learn structure and gain a general understanding
    of the task. However, there should also be examples in the dataset that are difficult,
    require multi-step reasoning, or are complex in nature. These examples reflect
    the complexity of real-world problems and have been seen to help the model improve
    its reasoning skills.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**：一般来说，我们希望我们的模型获得能力。通过简单的示例，模型将学习结构并获得对任务的总体理解。然而，数据集中也应该包含一些困难的示例，需要多步推理或本质上复杂。这些示例反映了现实世界问题的复杂性，并且已被证明有助于模型提高其推理技能。'
- en: '**Quantity**: There is also a discourse associated with quantity. According
    to some studies, larger models need fewer examples. For example, models with 70
    billion parameters might require as few as 1,000 quality examples. In contrast,
    smaller models might need many more examples. Smaller models may need many examples
    just to understand the task and many more to master it. A 7 billion model may
    use up to a million examples.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数量**：与数量相关联的也有一个论点。根据一些研究，较大的模型可能需要更少的示例。例如，具有700亿参数的模型可能只需要1000个高质量的示例。相比之下，较小的模型可能需要更多的示例。较小的模型可能需要很多示例来理解任务，而要掌握它可能需要更多。一个70亿参数的模型可能需要多达一百万个示例。'
- en: Building a dataset of thousands of examples can be particularly expensive. In
    many studies, only a small portion is created by humans. To reach the desired
    number of examples, one can either use a model to generate them or integrate already
    available datasets. Hugging Face contains many datasets for instruction tuning,
    for both general purposes as well as specific domains.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 构建包含数千个示例的数据集可能特别昂贵。在许多研究中，只有一小部分是由人类创建的。为了达到所需的示例数量，可以使用模型生成它们或整合现有的数据集。Hugging
    Face包含许多用于指令微调的数据集，包括通用目的和特定领域的数据集。
- en: '![Figure 10.35 – Constructing an instruction-tuning dataset (https://arxiv.org/pdf/2303.18223)](img/B21257_10_35.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图10.35 – 构建指令微调数据集 (https://arxiv.org/pdf/2303.18223)](img/B21257_10_35.jpg)'
- en: Figure 10.35 – Constructing an instruction-tuning dataset ([https://arxiv.org/pdf/2303.18223](https://arxiv.org/pdf/2303.18223))
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.35 – 构建指令微调数据集 ([https://arxiv.org/pdf/2303.18223](https://arxiv.org/pdf/2303.18223))
- en: The construction of these datasets, especially for particular domains, also
    requires the presence of experts (for example, if the dataset is for finance or
    medicine, collaboration with experts in the field or other institutions is common).
    Similar to a pre-training dataset, this dataset will undergo preprocessing. For
    example, examples of poor quality will be filtered out (one of the most commonly
    used methods is to have a list of keywords that indicate inappropriate content,
    off-topic examples, and so on), and filters will be used for length (e.g., examples
    that are too short or too long for the model) and for format (for some tasks,
    examples are formatted in a particular way, and examples that do not comply are
    removed). This dataset will also be deduplicated, and examples that are too similar
    are also often removed (if you ask an LLM to generate the examples, it might happen
    that examples that are too similar are generated). Patterns such as embeddings
    can be used for this task, where examples that have too high a similarity are
    filtered out. **MinHash** is another popular alternative to reduce the computational
    cost of the task. MinHash generates compact representations of patterns (of vectors),
    which are then compared with a similarity function.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集的构建，尤其是针对特定领域，也需要专家的参与（例如，如果数据集是针对金融或医学领域，与该领域或其它机构的专家合作是常见的）。与预训练数据集类似，这个数据集也将进行预处理。例如，低质量示例将被过滤掉（最常用的方法之一是有一个关键词列表，这些关键词表示不适当的内容、离题示例等），并且将使用长度（例如，对于模型来说太短或太长的示例）和格式（对于某些任务，示例以特定方式格式化，不符合格式的示例将被移除）过滤器。此数据集也将进行去重，过于相似的示例也经常被移除（如果你让一个大型语言模型生成示例，可能会发生生成过于相似示例的情况）。可以使用嵌入等模式进行这项任务，其中过滤掉相似度过高的示例。**MinHash**是减少任务计算成本的另一种流行选择。MinHash生成模式的紧凑表示（向量的模式），然后通过相似度函数进行比较。
- en: 'Because we are interested in model performance for specific tasks, an additional
    step is also conducted: **data decontamination**. This is a process in which we
    ensure that our instruction-tuning dataset does not contain examples that are
    the same or too similar to those in the evaluation or test set. In fact, once
    we have instruction-tuned our model, we want to test it on test sets that we set
    aside. If there were examples that were too similar, we could not verify overfitting
    or storage phenomena. Data decontamination is conducted with techniques similar
    to data deduplication.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对特定任务的模型性能感兴趣，因此还会进行一个额外的步骤：**数据净化**。这是一个确保我们的指令微调数据集不包含与评估或测试集中相同或过于相似的示例的过程。实际上，一旦我们对模型进行了指令微调，我们希望在预留的测试集上对其进行测试。如果有过于相似的示例，我们就无法验证过拟合或存储现象。数据净化使用与数据去重类似的技术进行。
- en: Before proceeding to the actual training, an additional step, **data quality
    evaluation**, is usually conducted. The dataset is evaluated for several criteria
    such as quality, accuracy, and complexity. Usually, some statistical parameters
    (such as the loss) are calculated and some examples are manually inspected. Recently,
    it has become increasingly popular to use **LLM-as-a-judge**, a strategy in which
    an LLM evaluates the quality of some examples. In such cases, an LLM is given
    a kind of template to check the quality of the examples by providing a score.
    Alternatively, today, there are also specific templates trained to provide a quality
    score. For example, reward models such as **ArmoRM-Llama3-8B-v0.1** are trained
    to produce an output that represents the quality of a text in terms of helpfulness,
    correctness, coherence, complexity, and verbosity.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行实际训练之前，通常还会进行一个额外的步骤，即**数据质量评估**。数据集将根据质量、准确性、复杂性等几个标准进行评估。通常，会计算一些统计参数（如损失）并对一些示例进行人工检查。最近，使用**LLM作为评判者**的策略越来越受欢迎，这是一种LLM评估某些示例质量的方法。在这种情况下，LLM被提供一种模板，通过提供分数来检查示例的质量。或者，今天也有专门训练的模板来提供质量分数。例如，**ArmoRM-Llama3-8B-v0.1**等奖励模型被训练来生成一个输出，该输出代表文本的质量，从有用性、正确性、连贯性、复杂性和冗长性等方面来衡量。
- en: Model testing
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型测试
- en: Once we have our dataset, we can conduct fine-tuning. Fine-tuning allows us
    to steer the capabilities and knowledge of our LLM. We must keep in mind that
    fine-tuning is not a magic potion; it has both risks and benefits. For example,
    fine-tuning exploits pre-existing knowledge of the model, but also conducts a
    refocus for a specific domain. This can lead to performance degradation and hallucinations.
    For this reason, in *Chapters 5*–*7*, we looked at alternatives (RAG and GraphRAG).
    In [*Chapter 3*](B21257_03.xhtml#_idTextAnchor042), we saw that there are now
    also efficient fine-tuning techniques such as LoRA and QLoRA that make the process
    much less expensive. Today, different libraries can conduct fine-tuning of these
    models, such as TRL (a library created by Hugging Face), Unsloth, and Axolotl
    based on Unsloth; these libraries also have additional features.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了我们的数据集，我们就可以进行微调。微调允许我们引导LLM的能力和知识。我们必须记住，微调不是灵丹妙药；它既有风险也有好处。例如，微调利用了模型预先存在的知识，但也为特定领域进行了重新聚焦。这可能导致性能下降和幻觉。因此，在*第5*章到*第7*章中，我们探讨了替代方案（RAG和GraphRAG）。在[*第3*章](B21257_03.xhtml#_idTextAnchor042)中，我们看到了现在也有像LoRA和QLoRA这样的高效微调技术，这使得整个过程成本大大降低。今天，不同的库可以对这些模型进行微调，例如由Hugging
    Face创建的TRL库、基于Unsloth的Unsloth和Axolotl；这些库还具有其他功能。
- en: 'After training, the key step is LLM evaluation. In general, evaluation is carried
    out in three stages:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 训练之后，关键步骤是LLM评估。通常，评估分为三个阶段：
- en: '**During** **pre-training**: During this step, the training of the model is
    monitored, and, in general, metrics such as training loss (a metric based on cross-entropy),
    loss on the validation set, perplexity (the exponential of training loss, one
    of the most commonly used metrics), and gradient norm (which indicates whether
    there were any instabilities in the training) are evaluated.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在预训练期间**：在这一步中，监控模型的训练，通常评估的指标包括训练损失（基于交叉熵的指标）、验证集上的损失、困惑度（训练损失的指数，最常用的指标之一）和梯度范数（指示训练中是否存在任何不稳定性）。'
- en: '**After** **pre-training**: Once pre-training is completed, a capability analysis
    is conducted on the benchmark datasets. In these datasets, both model knowledge
    and the ability to solve certain problems are evaluated. For example, MMLU tests
    model knowledge on a large number of domains, while datasets such as HellaSwag
    test the model on reasoning skills.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在预训练之后**：一旦预训练完成，就会在基准数据集上对能力进行分析。在这些数据集中，评估了模型的知识和解决某些问题的能力。例如，MMLU测试了模型在大量领域上的知识，而像HellaSwag这样的数据集则测试了模型的推理技能。'
- en: '**After fine-tuning**: After instruction tuning, qualities such as the LLM’s
    ability to follow instructions, converse, and use tools, for example, are usually
    evaluated. Since fine-tuning allows you to adapt the model to a specialized domain,
    it is beneficial to use specialized benchmarks in such cases. For example, for
    medical knowledge, a dataset such as Open Medical-LLM Leaderboard can be used,
    or for coding skills, BigCodeBench Leaderboard is a popular choice.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在微调之后**：在指令微调之后，通常会对LLM的遵循指令、对话和使用工具等能力进行评估。由于微调允许您将模型适应特定领域，在这种情况下使用专门的基准是有益的。例如，对于医学知识，可以使用Open
    Medical-LLM Leaderboard这样的数据集，或者对于编码技能，BigCodeBench Leaderboard是一个流行的选择。'
- en: '![Figure 10.36 – Taxonomy of LLM evaluation (https://arxiv.org/pdf/2310.19736)](img/B21257_10_36.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图10.36 – LLM评估分类](https://arxiv.org/pdf/2310.19736)(img/B21257_10_36.jpg)'
- en: Figure 10.36 – Taxonomy of LLM evaluation ([https://arxiv.org/pdf/2310.19736](https://arxiv.org/pdf/2310.19736))
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.36 – LLM评估分类([https://arxiv.org/pdf/2310.19736](https://arxiv.org/pdf/2310.19736))
- en: The last two steps (*After* *pre-training* and *After fine-tuning*) can also
    be conducted by manual inspection or using LLM-as-a-judge. For example, for open-ended
    text generation, it is more difficult to evaluate the capabilities of a model
    with standard metrics. Moreover, evaluating a model’s capabilities in a specific
    domain requires more in-depth analysis.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个步骤（*在预训练之后*和*在微调之后*）也可以通过人工检查或使用LLM作为裁判来进行。例如，对于开放式文本生成，使用标准指标评估模型的能力更为困难。此外，在特定领域评估模型的能力需要更深入的分析。
- en: If our LLM is a component of a system such as RAG, not only should the capabilities
    of the LLM be evaluated but the whole system as well. Indeed, we can evaluate
    the reasoning or hallucination capabilities of a model alone, but since the model
    will then be part of a system, we need to evaluate the whole product. For example,
    we should evaluate the whole RAG system for accuracy in retrieval and response
    generation. Even for RAG, there are both metrics and specific libraries for evaluating
    the system. For example, RAGAS (Retrieval-Augmented Generation Assessment) uses
    an LLM to evaluate the RAG response. ARES (Automatic RAG Evaluation through Synthetic
    data) is a comprehensive tool that takes advantage of synthetic data generation
    to assess model quality.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的 LLM 是 RAG 等系统的一个组件，那么不仅应该评估 LLM 的能力，还应该评估整个系统。实际上，我们可以单独评估模型的推理或幻觉能力，但由于模型将成为系统的一部分，我们需要评估整个产品。例如，我们应该评估
    RAG 系统在检索和响应生成方面的准确性。即使是对于 RAG，也有评估系统的指标和特定库。例如，RAGAS（检索增强生成评估）使用 LLM 来评估 RAG
    响应。ARES（通过合成数据自动评估 RAG）是一个综合工具，它利用合成数据生成来评估模型质量。
- en: Inference optimization
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理优化
- en: 'Our LLM has to be deployed and will consume resources; our goal now is to optimize
    the inference process to avoid users encountering latency and reduce costs for
    us. Basically, three processes occur in inference:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 LLM 必须部署并消耗资源；我们的目标现在是将推理过程优化，以避免用户遇到延迟并减少我们的成本。基本上，推理过程中发生三个过程：
- en: '**Tokenization and embedding**: Input is transformed into a numerical representation
    and then vector.'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分词和嵌入**：输入被转换为数值表示，然后转换为向量。'
- en: '**Computation**: A key and value are computed for each multi-head attention.'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**计算**：为每个多头注意力计算一个键和一个值。'
- en: '**Generation**: Output is produced sequentially.'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成**：输出是顺序产生的。'
- en: The first two steps are expensive but are easily parallelized on GPUs. The third
    step, on the other hand, is sequential because each output token depends on the
    previous token. The purpose of inference optimization is to speed up these three
    steps, and in this subsection, we will look at some techniques.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个步骤成本较高，但可以在 GPU 上轻松并行化。另一方面，第三个步骤是顺序的，因为每个输出标记都依赖于前一个标记。推理优化的目的是加快这三个步骤，在本小节中，我们将探讨一些技术。
- en: Model inference optimization
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型推理优化
- en: 'To produce a token output, we need all the previous context. For example, for
    the 15th token produced, we should calculate the **key-value** (**KV**) product
    of all tokens, 1 through 14\. This makes the process very slow, reducing over
    time such that the attention has a quadratic cost (*O(n²)*). The KV cache caches
    and reuses the key (*K*) and value (*V*) tensors from previous tokens, allowing
    faster computation of attention scores. This reduces memory and computational
    cost, enabling near-linear time (*O(n)*) inference. Typically, the process works
    like this: for the first token, we compute and store *(K,V)*. For the second,
    we find *(K,V)* again and add *K,V*. In other words, attention is applied only
    to the new tokens. As we saw in [*Chapter 2*](B21257_02.xhtml#_idTextAnchor032),
    this is the calculation of attention:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成一个标记输出，我们需要所有先前的上下文。例如，对于产生的第 15 个标记，我们应该计算所有标记（1 到 14）的 **键值**（**KV**）乘积。这使得整个过程非常缓慢，随着时间的推移，注意力的成本呈二次方增长（*O(n²)*）。KV
    缓存缓存并重用先前标记的键（*K*）和值（*V*）张量，从而允许更快地计算注意力分数。这减少了内存和计算成本，使得推理接近线性时间（*O(n)*)。通常，这个过程是这样的：对于第一个标记，我们计算并存储
    *(K,V)*。对于第二个标记，我们再次找到 *(K,V)* 并添加 *K,V*。换句话说，注意力只应用于新标记。正如我们在 [*第 2 章*](B21257_02.xhtml#_idTextAnchor032)
    中看到的，这是注意力的计算：
- en: '![Figure 10.37 – Attention calculation](img/B21257_10_37.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.37 – 注意力计算](img/B21257_10_37.jpg)'
- en: Figure 10.37 – Attention calculation
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.37 – 注意力计算
- en: In the KV cache, we calculate the KV product, and then we save the product result
    in memory. At the time of a new token, we retrieve this information (the KV product)
    and calculate the KV product only for that token.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在 KV 缓存中，我们计算 KV 乘积，然后将乘积结果保存在内存中。在新的标记出现时，我们检索这条信息（KV 乘积），并只为该标记计算 KV 乘积。
- en: '![Figure 10.38 – KV cache process](img/B21257_10_38.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.38 – KV 缓存过程](img/B21257_10_38.jpg)'
- en: Figure 10.38 – KV cache process
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.38 – KV 缓存过程
- en: The KV cache speeds up inference by eliminating some redundant computation (it
    prevents us from reprocessing all the previous parts of the sequence), scales
    well with long context windows, and is now optimized for major libraries and hardware.
    Of course, using the KV cache means we use more memory. In fact, it means that
    we have to keep in memory each KV cache per token, per attention head, and per
    layer. This, in practice, also places a limit on the size of the context window
    we can use. Obviously, during model training, it is of little use because we have
    to conduct parameter updates. Therefore, today, there are approaches that try
    to compress the KV cache so as to reduce the cost in terms of memory.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: KV缓存通过消除一些冗余计算（它阻止我们重新处理序列的所有先前部分）来加速推理，与长上下文窗口具有良好的扩展性，并且现在已针对主要库和硬件进行了优化。当然，使用KV缓存意味着我们使用更多的内存。实际上，这意味着我们必须为每个标记、每个注意力头和每层保留每个KV缓存的内存。在实践中，这也限制了我们可以使用的上下文窗口的大小。显然，在模型训练期间，它几乎没有用处，因为我们必须进行参数更新。因此，今天，有一些方法试图压缩KV缓存，以减少内存成本。
- en: 'Another technique used to speed up inference is **continuous batching**. The
    main purpose of this technique is to parallelize the various queries, then divide
    the model memory cost by the batch and transfer more data to the GPU. Traditional
    batching leads to slower input processing and is not optimized for inference,
    where the various queries may differ in size. Continuous batching, on the other
    hand, allows multiple user requests to be handled dynamically, allowing multiple
    inference requests to be processed in parallel, even if they arrive at different
    times. Requests that arrive at a different time are dynamically grouped into a
    series of batches, instead of having a fixed batch to fill. A batching engine
    merges multiple users’ prompts into a single batch. Instead of waiting for an
    entire batch, new tokens are processed when resources are available. This technique
    also works well with the KV cache; some tokens may have already been processed
    and we can recall what is in memory to further speed up the process. Continuous
    batching thus allows lower latency, allows streaming for several users at the
    same time, and improves resource utilization. Of course, it is more complex than
    the standard implementation of attention and requires a different implementation:
    we have to manage users optimally, and numerous requests are made to the KV cache.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 用于加速推理的另一种技术是**连续批处理**。这种技术的主要目的是并行化各种查询，然后将模型内存成本分摊到批处理中，并将更多数据传输到GPU。传统的批处理会导致输入处理速度变慢，并且不适合推理，因为各种查询的大小可能不同。相反，连续批处理允许动态处理多个用户请求，允许并行处理多个推理请求，即使它们在不同时间到达。不同时间到达的请求会动态地组合成一系列批次，而不是有一个固定的批次来填充。批处理引擎将多个用户的提示合并成一个批次。不需要等待整个批次，当资源可用时就会处理新的标记。这种技术也与KV缓存兼容；一些标记可能已经被处理，我们可以从内存中召回它们以进一步加快处理速度。连续批处理因此允许更低的延迟，允许同时为多个用户提供流式传输，并提高资源利用率。当然，它比标准的注意力实现更复杂，需要不同的实现：我们必须优化用户管理，并对KV缓存进行大量请求。
- en: '**Speculative decoding** is another optimization technique used in autoregressive
    language models to accelerate text generation. Classic LLMs generate only one
    token at a time, and token generation is not parallelizable, leading to inefficient
    inference. In speculative decoding, we have two models working together:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '**推测解码**是自回归语言模型中用于加速文本生成的另一种优化技术。经典的LLM一次只生成一个标记，标记生成不可并行化，导致推理效率低下。在推测解码中，我们有两个模型协同工作：'
- en: A small, faster “draft” model that generates multiple candidate tokens
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个小型、快速的“草案”模型，生成多个候选标记
- en: The main, larger LLM that verifies the candidates and either accepts or corrects
    them
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要的、更大的LLM用于验证候选者，并接受或纠正它们
- en: The draft model (a small model of the same LLM architecture as the main one,
    but with fewer parameters) generates multiple speculative tokens at once. The
    main LLM checks these proposed tokens; if they match those of the larger LLM’s
    output, they are accepted. If, however, there is no match, the LLM discards them
    and continues to generate. The process is iterative until the output is finished.
    Speculative decoding makes it possible to reduce the number of sequential steps
    in inference, speed up the response, and maximize GPU consumption without losing
    quality. Of course, the draft model must generate good candidates; if the small
    model is not accurate, we lose the advantage in speedup, which means we would
    require another model. This approach works better with long-form than small outputs.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 草稿模型（与主模型具有相同LLM架构但参数更少的小模型）一次生成多个推测性标记。主LLM检查这些提议的标记；如果它们与较大LLM的输出匹配，则被接受。然而，如果没有匹配，LLM将丢弃它们并继续生成。这个过程会迭代进行，直到输出完成。推测性解码使得减少推理中的顺序步骤数量、加快响应速度以及在不降低质量的情况下最大化GPU消耗成为可能。当然，草稿模型必须生成好的候选者；如果小模型不准确，我们将失去加速的优势，这意味着我们需要另一个模型。这种方法在长文本输出上比在小输出上效果更好。
- en: Another way to speed up inference is to use specific forms of attention. **Paged
    attention** is an optimized memory management technique for handling large KV
    caches efficiently during LLM inference. It works like a virtual memory system
    by dynamically managing memory allocation and preventing fragmentation. It is
    inspired by the management of memory systems in computers, and instead of storing
    KV caches in a continuous memory block (which can lead to fragmentation), it stores
    them in smaller memory pages. This allows faster retrieval of information (and
    only necessary information) from the KV cache. Paged attention thus prevents GPU
    memory fragmentation, makes the system more efficient for long context (reduces
    memory consumption for long chats between the user and the system), and decreases
    latency by allowing easier fetching from the KV cache. **FlashAttention** is another
    way to make the inference process more efficient, allowing faster processing of
    attention with decreased memory consumption. It achieves this by processing attention
    in small blocks instead of storing large intermediate matrices. In this way, it
    makes more efficient use of GPU resources. In FlashAttention, only small blocks
    of various tokens are stored in the RAM. Today, many models use forms of attention
    during training that are aimed at faster reasoning. **Multi-grouped attention**
    (**MGA**) is a hybrid between **multi-head attention** (**MHA**) and sparse attention.
    Instead of each attention head attending to all tokens, MGA groups multiple heads
    together to enable more efficient computation. In MGA, the heads are not separated
    but grouped into specific clusters and process a group of characters. This makes
    it possible to reduce computational costs, is more flexible for sparse attention
    forms, and makes it possible to speed up training and reasoning. Another popular
    alternative is **multi-head latent attention** (**MLA**), which is used in modern
    LLMs. In standard MHA, we explicitly compute attention for all heads. In MLA,
    we use latent heads that indirectly encode relationships between tokens without
    the need for a full pairwise computation of attention. In this way, the model
    has better generalization by learning a compressed representation without sacrificing
    accuracy. This requires less attention during inference and saves memory.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 加速推理的另一种方法是使用特定的注意力形式。**分页注意力**是一种优化内存管理技术，用于在LLM推理过程中高效地处理大型KV缓存。它通过动态管理内存分配和防止碎片化，像虚拟内存系统一样工作。它受到计算机内存系统管理的启发，而不是将KV缓存存储在连续的内存块中（这可能导致碎片化），而是将它们存储在较小的内存页中。这使得从KV缓存中更快地检索信息（以及仅必要的信息）成为可能。因此，分页注意力可以防止GPU内存碎片化，使系统在长上下文中更加高效（减少用户和系统之间长聊的内存消耗），并通过允许更容易地从KV缓存中获取信息来降低延迟。**FlashAttention**是另一种使推理过程更有效的方法，通过减少内存消耗来加快注意力的处理速度。它通过处理小块中间矩阵而不是存储大型中间矩阵来实现这一点。这样，它更有效地利用了GPU资源。在FlashAttention中，只有各种标记的小块存储在RAM中。如今，许多模型在训练期间使用旨在加快推理的注意力形式。**多组注意力**（**MGA**）是**多头注意力**（**MHA**）和稀疏注意力之间的混合。MGA不是每个注意力头都关注所有标记，而是将多个头组合在一起以实现更有效的计算。在MGA中，头不是分开的，而是分组到特定的簇中，并处理一组字符。这使得降低计算成本成为可能，对于稀疏注意力形式更加灵活，并使加速训练和推理成为可能。另一种流行的替代方案是**多头潜在注意力**（**MLA**），它用于现代LLM中。在标准的MHA中，我们明确计算所有头的注意力。在MLA中，我们使用潜在的头，这些头间接编码标记之间的关系，而无需进行完整的成对注意力计算。通过这种方式，模型通过学习压缩表示来提高泛化能力，而不会牺牲准确性。这需要推理期间更少的注意力，并节省内存。
- en: '![Figure 10.39 – Overview of methods for speeding inference (https://arxiv.org/pdf/2407.18003)](img/B21257_10_39.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![图10.39 – 加速推理方法概述](https://arxiv.org/pdf/2407.18003)(img/B21257_10_39.jpg)'
- en: Figure 10.39 – Overview of methods for speeding inference ([https://arxiv.org/pdf/2407.18003](https://arxiv.org/pdf/2407.18003))
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.39 – 加速推理方法概述([https://arxiv.org/pdf/2407.18003](https://arxiv.org/pdf/2407.18003))
- en: These techniques, as illustrated in *Figure 10**.39*, demonstrate how inference
    efficiency can be improved across multiple stages—compression, caching, and memory
    optimization. With this foundation, we can now explore how such optimizations
    are applied in real-world deployment scenarios.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术，如图*图10**.39*所示，展示了如何通过多个阶段——压缩、缓存和内存优化——提高推理效率。有了这个基础，我们现在可以探索这些优化如何在现实世界的部署场景中应用。
- en: Data, pipeline, and tensor parallelism
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据、管道和张量并行性
- en: 'Another way to make training more efficient is to parallelize it. **Model parallelism**
    for a neural network is to distribute the model across multiple devices (such
    as GPUs or TPUs) to overcome memory and computation limitations. While this can
    be useful to speed up training, in other cases, it is necessary because the model
    is too large to fit on a single device. There are several ways to parallelize
    a model, as we will see next:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种使训练更有效的方法是并行化。对于神经网络，**模型并行**是将模型分布到多个设备（如 GPU 或 TPU）以克服内存和计算限制。虽然这可以用来加速训练，但在其他情况下，这是必要的，因为模型太大，无法适应单个设备。有几种方法可以并行化模型，我们将在下面看到：
- en: '**Data parallelism** is considered the simplest approach, in which replicas
    of the model are distributed across multiple computing devices (e.g., GPUs, TPUs,
    or even different machines), and different subsets of the training dataset are
    fed into each replica. During training, averaging of the gradients of the various
    GPUs is conducted; this is used for model updates. Then, each model is replicated
    across workers (GPUs/TPUs), and the input data batch is split into mini-batches
    assigned to different workers. During the forward pass, each worker computes predictions
    and losses for its mini-batch. Subsequently, each worker calculates gradients
    for its assigned data. These gradients are aggregated either by averaging or using
    a more complex method, and the aggregated gradients are used to update all model
    replicas, ensuring synchronization across workers. Data parallelism can be implemented
    in several ways, the most common being synchronous data parallelism, in which
    all devices compute the gradient before synchronization. Once all gradients are
    available, averaging is conducted. Although this approach ensures that there is
    consistency, a worker can slow down the training. To overcome this, we have asynchronous
    data parallelism, where each device conducts the local model update independently,
    at the risk of introducing stale gradients (outdated updates). An intermediate
    approach (stale-sync data parallelism) is also available, where workers perform
    multiple local updates before synchronizing with others. Data parallelism can
    also be centralized with a central server or decentralized with the various workers
    exchanging gradients in a ring topology. Data parallelism allows the workload
    to be distributed among different devices, increasing the speed of training, scales
    well when you have several devices, is not complex to implement, and is efficient
    because the model stays on the various devices and is not swapped. On the other
    hand, gradient synchronization can be slow due to communication overhead, especially
    if communication is inefficient. Variations in device speed, such as using different
    hardware or GPU versions, can further exacerbate this issue. Additionally, large
    batch sizes may cause convergence problems, and managing synchronization becomes
    increasingly complex as the number of devices grows.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据并行**被认为是 simplest 的方法，其中模型的副本被分布到多个计算设备（例如，GPU、TPU 或甚至不同的机器）上，不同的训练数据集子集被输入到每个副本中。在训练过程中，对各种
    GPU 的梯度进行平均；这用于模型更新。然后，每个模型在工人（GPU/TPU）之间进行复制，输入数据批次被分割成分配给不同工人的小批次。在正向传播过程中，每个工人计算其小批次的预测和损失。随后，每个工人为其分配的数据计算梯度。这些梯度可以通过平均或使用更复杂的方法进行聚合，聚合后的梯度用于更新所有模型副本，确保工人在同步。数据并行可以以多种方式实现，最常见的是同步数据并行，其中所有设备在同步之前计算梯度。一旦所有梯度都可用，就进行平均。尽管这种方法确保了一致性，但一个工人可能会减慢训练速度。为了克服这一点，我们有了异步数据并行，其中每个设备独立进行本地模型更新，这可能会引入过时的梯度（过时的更新）。还有一种中间方法（过时同步数据并行），其中工人在与其他人同步之前执行多个本地更新。数据并行也可以通过中央服务器集中化，或者通过各种工人在环形拓扑中交换梯度进行去中心化。数据并行允许将工作负载分配到不同的设备上，从而提高训练速度，当有多个设备时扩展良好，实现起来不复杂，并且效率高，因为模型保持在各种设备上，而不需要交换。另一方面，由于通信开销，梯度同步可能会很慢，尤其是如果通信效率不高。设备速度的变化，例如使用不同的硬件或
    GPU 版本，可能会进一步加剧这个问题。此外，大批次大小可能会导致收敛问题，随着设备数量的增加，管理同步变得越来越复杂。'
- en: '![Figure 10.40 – Processing of mini-batches over time in data parallelism.
    Each GPU has a copy of all the layers (shown in different colors) and different
    mini-batches (numbered) are processed by different GPUs (https://arxiv.org/pdf/2111.04949)](img/B21257_10_40.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![图10.40 – 数据并行中随时间处理的迷你批次。每个GPU拥有所有层的副本（以不同颜色显示）以及不同编号的迷你批次由不同的GPU处理（https://arxiv.org/pdf/2111.04949）](img/B21257_10_40.jpg)'
- en: Figure 10.40 – Processing of mini-batches over time in data parallelism. Each
    GPU has a copy of all the layers (shown in different colors) and different mini-batches
    (numbered) are processed by different GPUs ([https://arxiv.org/pdf/2111.04949](https://arxiv.org/pdf/2111.04949))
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.40 – 数据并行中随时间处理的迷你批次。每个GPU拥有所有层的副本（以不同颜色显示）以及不同编号的迷你批次由不同的GPU处理([https://arxiv.org/pdf/2111.04949](https://arxiv.org/pdf/2111.04949))
- en: '**Pipeline parallelism** is a distributed training technique where different
    layers of a deep learning model are assigned to different devices (e.g., GPUs
    or TPUs), and mini-batches are processed sequentially through the pipeline. This
    technique helps in training extremely large models that do not fit into a single
    device’s memory. Pipeline parallelism is commonly used in transformer models such
    as GPT-3, GPT-4, LLaMA, and DeepSeek, where model sizes exceed the memory capacity
    of a single GPU. The model is divided into multiple stages, where each stage represents
    a subset of consecutive layers and is assigned to a different GPU. A batch is
    split into mini-batches, and a mini-batch is split into micro-batches. One micro-batch
    is then processed from the first stage and passed to the next. The second micro-batch
    starts being processed before the first micro-batch has finished all the stages
    (as soon as the first stage clears, it can start processing the second micro-batch,
    without the first micro-batch having to pass all the layers, thus allowing the
    process to be parallelized in an efficient manner). The backward pass follows
    the same pipeline as the forward pass but in reverse order; the gradient starts
    from the last stages to the first stages. Once all micro-batches are completed,
    the model update can be conducted.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管道并行**是一种分布式训练技术，其中深度学习模型的不同层被分配到不同的设备（例如，GPU或TPU），并且迷你批次通过管道顺序处理。这项技术有助于训练不适合单个设备内存的极大型模型。管道并行通常用于GPT-3、GPT-4、LLaMA和DeepSeek等转换器模型，其中模型大小超过了单个GPU的内存容量。模型被分成多个阶段，每个阶段代表连续层的子集，并分配给不同的GPU。一个批次被分成迷你批次，而一个迷你批次被分成微批次。然后，一个微批次从第一阶段开始处理并传递到下一阶段。在第一个微批次完成所有阶段之前，第二个微批次就开始处理（一旦第一阶段清除，就可以开始处理第二个微批次，而第一个微批次不需要通过所有层，从而允许以高效的方式并行化处理）。反向传播遵循正向传播相同的管道，但顺序相反；梯度从最后阶段到第一阶段。一旦所有微批次都完成，就可以进行模型更新。'
- en: '![Figure 10.41 – Forward and backward update for a single micro-batch (https://arxiv.org/pdf/2403.03699v1)](img/B21257_10_41.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![图10.41 – 单个微批次的正向和反向更新](https://arxiv.org/pdf/2403.03699v1)(img/B21257_10_41.jpg)'
- en: Figure 10.41 – Forward and backward update for a single micro-batch ([https://arxiv.org/pdf/2403.03699v1](https://arxiv.org/pdf/2403.03699v1))
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.41 – 单个微批次的正向和反向更新([https://arxiv.org/pdf/2403.03699v1](https://arxiv.org/pdf/2403.03699v1))
- en: '![Figure 10.42 – Forward and backward update for two micro-batches in parallel
    (https://arxiv.org/pdf/2403.03699v1)](img/B21257_10_42.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![图10.42 – 并行处理两个微批次](https://arxiv.org/pdf/2403.03699v1)(img/B21257_10_42.jpg)'
- en: Figure 10.42 – Forward and backward update for two micro-batches in parallel
    ([https://arxiv.org/pdf/2403.03699v1](https://arxiv.org/pdf/2403.03699v1))
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.42 – 并行处理两个微批次([https://arxiv.org/pdf/2403.03699v1](https://arxiv.org/pdf/2403.03699v1))
- en: Pipeline parallelism can be conducted in different manners such as **one forward,
    one backward** (**1F1B**) scheduling, in which each GPU conducts one forward pass
    and one backward pass at the same time. Alternatively, each device could contain
    multiple model partitions and thus conduct more flexible scheduling. Pipeline
    parallelism allows the training of very large models that do not fit into a single
    GPU, allows better utilization of the various devices (each device constantly
    processes micro-batches), reduces the risk of memory bottlenecks, and is well
    adapted to transformers. On the other hand, it is a more complex system, where
    one has to manage the stages so that some of them do not have more computation-heavy
    layers and thus become bottlenecks (careful layer partitioning to balance the
    workload among the various devices). In the first iterations, the system is less
    efficient as it waits to be filled with micro-batches (the first stage starts
    working before the other stages), communication is more complex due to gradient
    aggregation, and there is increased complexity in designing the system.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 管道并行可以在不同的方式下进行，例如**前向一次，后向一次**（**1F1B**）调度，其中每个GPU同时进行一次前向传播和一次后向传播。或者，每个设备可以包含多个模型分区，从而进行更灵活的调度。管道并行允许训练不适合单个GPU的非常大的模型，允许更好地利用各种设备（每个设备持续处理微批），降低内存瓶颈的风险，并且非常适合Transformer。另一方面，它是一个更复杂的系统，其中必须管理阶段，以确保其中一些阶段没有计算密集型层，从而成为瓶颈（仔细分层以平衡各种设备之间的工作负载）。在最初的迭代中，系统效率较低，因为它等待被微批填充（第一阶段在其它阶段之前开始工作），由于梯度聚合，通信更加复杂，并且在设计系统时增加了复杂性。
- en: '**Tensor parallelism** is a model parallelism technique where individual weight
    tensors (matrices) within a model are split across multiple GPUs. Unlike traditional
    model parallelism, which assigns entire layers to different GPUs, tensor parallelism
    breaks down the computations within a single layer and distributes them across
    multiple devices. This approach is particularly useful for large-scale transformer
    models where certain operations (such as matrix multiplications in attention layers)
    require enormous memory and computational power. Instead of computing and storing
    entire weight matrices on a single GPU, tensor parallelism divides them among
    multiple GPUs. For example, a fully connected layer applies a weight matrix, *W*,
    to an input, *X*, to obtain an output, *Y*. If *W* is too large for a single GPU,
    we can divide it among multiple GPUs. Each GPU will then conduct only part of
    the computation, producing part of the output, which is then later aggregated.
    Similarly, during the backward pass, we must then redistribute the gradient computation
    to allow proper updates of the weights of the various matrices, *W*. Column-wise
    tensor parallelism is among the most widely used for transformers, where the weight
    matrix is split column-wise across GPUs, and each GPU then computes part of the
    output, which is then concatenated. Considering the self-attention mechanism of
    a model, the query (*Q*), key (*K*), and value (*V*) matrices are split column-wise
    across multiple GPUs. Each GPU then computes a partial attention score, following
    which the various results are aggregated across GPUs to reconstruct the finished
    output. The advantage of this approach is that instead of storing entire weight
    matrices, each GPU stores only a portion. Also, the multiplication of large matrices
    can be distributed and thus make the computation faster, making it particularly
    efficient for large models. On the other hand, there is always the risk of communication
    overhead (GPUs must frequently exchange partial results, which can slow down training),
    it can be complex to implement, and it is not worthwhile except for large models.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量并行**是一种模型并行技术，其中模型中单个权重张量（矩阵）被分割到多个GPU上。与传统模型并行不同，后者将整个层分配到不同的GPU上，张量并行将单个层内的计算分解并分布到多个设备上。这种方法对于大规模的转换器模型特别有用，因为某些操作（如注意力层中的矩阵乘法）需要巨大的内存和计算能力。张量并行不是在单个GPU上计算和存储整个权重矩阵，而是将它们分割到多个GPU上。例如，一个全连接层将权重矩阵*W*应用于输入*X*以获得输出*Y*。如果*W*太大以至于无法在一个GPU上处理，我们可以将其分割到多个GPU上。然后每个GPU将只进行部分计算，产生部分输出，之后再将这些输出聚合。同样，在反向传播过程中，我们必须重新分配梯度计算，以便正确更新各种矩阵*W*的权重。列向张量并行是转换器中最广泛使用的一种，其中权重矩阵按列分割到多个GPU上，然后每个GPU计算部分输出，之后再将这些输出连接。考虑到模型的自我注意力机制，查询(*Q*)、键(*K*)和值(*V*)矩阵按列分割到多个GPU上。然后每个GPU计算部分注意力分数，随后将各种结果在GPU之间聚合以重建最终的输出。这种方法的优点是，不需要存储整个权重矩阵，每个GPU只存储一部分。此外，大型矩阵的乘法可以分布，从而加快计算速度，这使得它对于大型模型特别高效。另一方面，始终存在通信开销的风险（GPU必须频繁交换部分结果，这可能会减慢训练速度），实现起来可能很复杂，并且除非是大型模型，否则不值得。'
- en: '![Figure 10.43 – Tensor parallelism (https://arxiv.org/pdf/2311.01635)](img/B21257_10_43.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![图10.43 – 张量并行 (https://arxiv.org/pdf/2311.01635)](img/B21257_10_43.jpg)'
- en: Figure 10.43 – Tensor parallelism ([https://arxiv.org/pdf/2311.01635](https://arxiv.org/pdf/2311.01635))
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.43 – 张量并行 ([https://arxiv.org/pdf/2311.01635](https://arxiv.org/pdf/2311.01635))
- en: 'The following table compares tensor parallelism, data parallelism, and pipeline
    parallelism across key dimensions such as memory usage, communication overhead,
    and complexity:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 下表比较了张量并行、数据并行和流水线并行在内存使用、通信开销和复杂性等关键维度上的差异：
- en: '| **Feature** | **Tensor parallelism** | **Data parallelism** | **Pipeline
    parallelism** |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| **特性** | **张量并行** | **数据并行** | **流水线并行** |'
- en: '| **How** **it works** | Splits individual tensors across GPUs | Replicates
    full model on each device; splits data | Splits model layers across GPUs |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| **如何工作** | 在GPU之间分割单个张量 | 在每个设备上复制完整模型；分割数据 | 在GPU之间分割模型层 |'
- en: '| **Memory usage** | Low (weights are sharded) | High (full model stored on
    each GPU) | Medium (layers distributed) |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| **内存使用** | 低（权重被分割） | 高（完整模型存储在每个GPU上） | 中等（层分布） |'
- en: '| **Communication** **overhead** | High (frequent cross-GPU communication)
    | High (gradient synchronization) | Moderate (micro-batch passing) |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| **通信开销** | 高（频繁的跨GPU通信） | 高（梯度同步） | 中等（微批处理传递） |'
- en: '| **Best for** | Very large models with huge weight matrices | Medium-sized
    models with large datasets | Deep models such as transformers |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| **最佳适用** | 非常大的模型，具有巨大的权重矩阵 | 中等大小的模型，具有大量数据集 | 深度模型，如变换器 |'
- en: '| **Complexity** | High | Low | Medium |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| **复杂性** | 高 | 低 | 中等 |'
- en: Table 10.1 – Comparison of tensor, data, and pipeline parallelism in large-scale
    model training
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.1 – 大规模模型训练中张量、数据和管道并行性的比较
- en: '**Hybrid parallelism** integrates different types of parallelism trying to
    optimize training across multiple GPUs. Generally, the various approaches can
    be combined, although this requires more complexity. For example, data parallelism
    ensures that GPUs process different batches while model parallelism (tensor or
    pipeline parallelism) ensures that the model is optimized across multiple GPUs.
    For example, if the model is too large for a single GPU, we can use model parallelism
    and split the model across multiple GPUs. We can then use 16 GPUs to split a batch
    of data across 4 copies of the model.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合并行性**通过整合不同类型的并行性，试图优化跨多个GPU的训练。通常，可以将各种方法结合起来，尽管这需要更多的复杂性。例如，数据并行性确保GPU处理不同的批次，而模型并行性（张量或管道并行性）确保模型在多个GPU上得到优化。例如，如果模型太大，无法在一个GPU上运行，我们可以使用模型并行性并将模型分割到多个GPU上。然后我们可以使用16个GPU将数据批次分割到模型的4个副本中。'
- en: So far, we have explored how to build a fully working AI-driven Streamlit app
    that integrates multiple agents and external APIs such as OpenAI. However, when
    an application moves from development to production, some important challenges
    need to be taken into account.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了如何构建一个完全工作的AI驱动Streamlit应用程序，该应用程序集成了多个代理和外部API，如OpenAI。然而，当应用程序从开发阶段转移到生产阶段时，需要考虑一些重要的挑战。
- en: Handling errors in production
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产环境中的错误处理
- en: 'In this section, we’ll explore some of the approaches we can adopt to handle
    issues that may occur when an application moves from development to production.
    Typical problems you might encounter include:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨我们可以采用的一些方法来处理当应用程序从开发阶段转移到生产阶段时可能遇到的问题。你可能会遇到的一些典型问题包括：
- en: The OpenAI API is temporarily unavailable
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI API暂时不可用
- en: Intermittent network failures or exceeding rate limits
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 间歇性网络故障或超过速率限制
- en: Incomplete or missing logging system
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不完整或缺失的日志系统
- en: 'Let’s see how we can mitigate these issues effectively:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何有效地缓解这些问题：
- en: '`try`/`except` blocks. Here’s an example of how you can handle different types
    of errors when calling the OpenAI API:'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`try`/`except` 块。以下是如何在调用OpenAI API时处理不同类型错误的示例：'
- en: '[PRE25]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Temporary issues**: When there are intermittent network failures or momentary
    unavailability of external APIs, instead of immediately failing, the app can retry
    the operation a few times:'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**临时问题**：当出现间歇性网络故障或外部API暂时不可用时，应用程序不应立即失败，而是可以尝试操作几次：'
- en: '[PRE26]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`st.write()` is fine for quick debugging, but in production, you need a more
    persistent and structured way to track what’s happening in your app.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.write()` 对于快速调试来说很好，但在生产环境中，你需要一种更持久和结构化的方式来跟踪应用程序中发生的事情。'
- en: 'A basic logging system helps you record important events and catch errors that
    may not appear in the UI:'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基本的日志系统可以帮助你记录重要事件并捕获可能不在UI中出现的错误：
- en: '[PRE27]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Security considerations for production
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产环境的安全考虑
- en: Applications deployed in production often involve API keys and potentially sensitive
    user data, so security must be carefully addressed from the beginning.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 部署在生产环境中的应用程序通常涉及API密钥和可能敏感的用户数据，因此必须从一开始就仔细处理安全问题。
- en: One of the most fundamental practices is to avoid hardcoding credentials such
    as API keys directly into the source code. Instead, credentials should be managed
    securely using environment variables or a dedicated secrets management system.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的实践之一是避免将凭证（如API密钥）直接硬编码到源代码中。相反，应使用环境变量或专门的密钥管理系统安全地管理凭证。
- en: 'Security in production typically involves three key areas:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 生产环境中的安全通常涉及三个关键领域：
- en: Managing secrets
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理密钥
- en: Data exposure prevention
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止数据泄露
- en: Securing your deployment environment
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护你的部署环境
- en: Let’s discuss these next.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论这些问题。
- en: Managing secrets in production
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生产环境中的密钥管理
- en: 'There are two common ways to securely manage secrets in production environments:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中安全地管理密钥的两种常见方法：
- en: '`st.secrets`: This is ideal for applications deployed on Streamlit Cloud'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.secrets`：这对于部署在Streamlit Cloud上的应用程序来说很理想'
- en: '**Using environment variables**: This is recommended for Docker containers
    or local server deployments'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用环境变量**：这适用于Docker容器或本地服务器部署'
- en: Both approaches allow you to keep sensitive information out of your source code,
    but the right choice depends on your deployment context.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都可以让你将敏感信息从源代码中排除出去，但正确的选择取决于你的部署环境。
- en: 'Here are some examples for each method:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这里为每种方法提供了一些示例：
- en: '`st.secrets`: When using Streamlit, create a `.streamlit/secrets.toml` file
    that lets you define secrets into it. Here is an example:'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`st.secrets`：当使用Streamlit时，创建一个`.streamlit/secrets.toml`文件，让你可以将其中的秘密定义到其中。以下是一个示例：'
- en: '[PRE28]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Access it in your code like this:'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在你的代码中这样访问它：
- en: '[PRE29]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '**Using environment variables**: For Dockerization or local deployments, it
    is recommended to store secrets as environment variables, keeping them separate
    from the source code. To use environment variables, you must define them in your
    terminal or deployment environment before running your application.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用环境变量**：对于Docker化或本地部署，建议将秘密作为环境变量存储，将它们与源代码分开。要使用环境变量，你必须在运行应用程序之前在终端或部署环境中定义它们。'
- en: 'For example, in a Unix-based terminal (Linux, macOS, or WSL), you can define
    the variable like this:'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，在基于Unix的终端（Linux、macOS或WSL）中，你可以这样定义变量：
- en: '[PRE30]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, in your Python code, access the variable as follows:'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，在你的Python代码中，如下访问变量：
- en: '[PRE31]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `export` command sets an environment variable only for the current terminal
    session. This means it will remain active only until you close the terminal. To
    launch your app using the variable, you must run it in the same shell session:'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`export`命令只为当前终端会话设置环境变量。这意味着它只在你关闭终端时保持活动状态。要使用变量启动应用程序，你必须在该shell会话中运行它：'
- en: '[PRE32]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Then, save and close it. From now on, your app will automatically find the API
    key every time it is launched from a new terminal session.
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，保存并关闭它。从现在开始，每次从新的终端会话启动应用程序时，你的应用程序将自动找到API密钥。
- en: Data exposure prevention
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 防止数据泄露
- en: In production, one of the most overlooked security risks is the unintentional
    exposure of sensitive data through logging, error messages, or misconfigured URLs.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，最容易被忽视的安全风险之一是通过日志记录、错误消息或配置错误的URL无意中泄露敏感数据。
- en: While logging is essential for debugging and observability, it can easily become
    a liability if secrets, tokens, or user data are captured without proper filtering.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然日志记录对于调试和可观察性至关重要，但如果在不适当的过滤下捕获了秘密、令牌或用户数据，它很容易变成一种负担。
- en: 'Here are a few best practices to minimize the risk:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些最佳实践，以最大限度地降低风险：
- en: '**Avoid logging secrets**: Never print API keys, access tokens, or passwords
    to logs, even in debug mode. This applies to both client-side and server-side
    logs.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**避免记录秘密**：即使在调试模式下，也永远不要将API密钥、访问令牌或密码打印到日志中。这适用于客户端和服务器端日志。'
- en: '**Sanitize user data**: If your application logs inputs or error traces that
    include user-provided data (e.g., form submissions, headers, and payloads), be
    sure to mask or strip sensitive fields (such as email addresses, credit card numbers,
    or personal identifiers).'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清理用户数据**：如果你的应用程序记录了包含用户提供的输入或错误跟踪的日志（例如，表单提交、头信息和有效负载），请确保对敏感字段（如电子邮件地址、信用卡号码或个人标识符）进行遮蔽或删除。'
- en: '`INFO`, `WARNING`, `ERROR`, or `DEBUG`) and restrict debug-level logs in production.
    Enable only what is necessary to diagnose issues without overexposing internals.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`INFO`、`WARNING`、`ERROR`或`DEBUG`）并限制生产环境中的调试级别日志。仅启用诊断问题所必需的内容，而不过度暴露内部信息。'
- en: '**Handle errors**: Avoid sending raw stack traces or system error messages
    directly to users. These can leak details about your backend, framework, or database.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理错误**：避免直接向用户发送原始堆栈跟踪或系统错误消息。这些可能会泄露有关你的后端、框架或数据库的详细信息。'
- en: Preventing data exposure is about designing systems that assume that secrets
    and user data must always be protected, even in edge cases or failures.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 防止数据泄露是设计系统，假设秘密和用户数据必须始终受到保护，即使在边缘情况或故障中也是如此。
- en: Securing your deployment environment
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保护你的部署环境
- en: Even if your code avoids data exposure and your secrets are properly managed,
    your application can still be vulnerable if the environment in which it runs is
    misconfigured.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你的代码避免了数据泄露并且你的秘密得到了适当的管理，如果你的应用程序运行的环境配置不当，它仍然可能存在漏洞。
- en: For example, in modern workflows, containerization is one of the most common
    ways to package and deploy applications. In fact, containers offer portability
    and consistency across environments, but they also introduce specific security
    risks.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在现代工作流程中，容器化是打包和部署应用程序最常见的方式之一。实际上，容器提供了跨环境的可移植性和一致性，但它们也引入了特定的安全风险。
- en: 'A wrong or poor Dockerfile configuration can introduce multiple vulnerabilities,
    such as the following:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 错误或较差的Dockerfile配置可能会引入多个漏洞，例如以下内容：
- en: Increased exposure to known exploits if the image includes unnecessary packages
    or tools
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果镜像包含不必要的软件包或工具，则会增加对已知漏洞的暴露。
- en: Credential leaks if secrets are stored directly in the image
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果秘密直接存储在镜像中，则可能发生凭证泄露
- en: Privilege escalation if the container runs as the root user
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果容器以root用户运行，则可能发生权限提升
- en: Unsafe access to host resources if volumes are not properly restricted
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果卷没有被适当限制，则可能对主机资源进行不安全访问
- en: 'To mitigate these risks, it is important to follow a set of container security
    best practices. Let’s look at a few simple guidelines to make your Docker-based
    deployment more secure and production-ready:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这些风险，遵循一系列容器安全最佳实践非常重要。让我们看看一些简单的指南，以使你的基于Docker的部署更加安全并准备好生产：
- en: '`python:3.11` image instead of `python:3.11-slim` can include dozens of unnecessary
    system tools. If any of these have known vulnerabilities, they become an unintentional
    attack, even if your app doesn’t use them.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`python:3.11`镜像而不是`python:3.11-slim`可能包含数十个不必要的系统工具。如果其中任何一个有已知的漏洞，它们就会成为意外的攻击，即使你的应用程序没有使用它们。
- en: '`.env` files into the Docker image allows anyone with access to the image to
    extract and appropriate them.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`.env`文件放入Docker镜像允许任何有权访问镜像的人提取并使用它们。
- en: '`root``root`, and combined with an exploit in a Python dependency, this could
    give an attacker full control of the container and possibly the host too.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`root`用户和`root`权限，如果与Python依赖项中的漏洞利用相结合，这可能会使攻击者完全控制容器，甚至可能控制主机。'
- en: '`/`: Root of the host filesystem. Grants full access to the entire filesystem
    of the host, including sensitive system directories, user data, and configuration
    files.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/`：主机文件系统的根。授予对整个主机文件系统的完全访问权限，包括敏感的系统目录、用户数据和配置文件。'
- en: '`/etc`: System configuration directory. Contains critical configuration files,
    including `/etc/passwd`, `/etc/shadow`, network settings, and user permissions.
    Exposing this can allow manipulation of how the host system behaves.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/etc`：系统配置目录。包含关键配置文件，包括`/etc/passwd`、`/etc/shadow`、网络设置和用户权限。暴露这些文件可能允许操纵主机系统的行为。'
- en: '`/var/run/docker.sock`: Docker daemon socket. Gives the container direct control
    over the Docker engine running on the host. This lets the container start, stop,
    and manage other containers, including mounting volumes and executing code on
    the host.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/var/run/docker.sock`：Docker守护进程套接字。赋予容器对在主机上运行的Docker引擎的直接控制权。这允许容器启动、停止和管理其他容器，包括挂载卷和在主机上执行代码。'
- en: 'Here is an example of a minimal and secure Dockerfile:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个最小化和安全的Dockerfile示例：
- en: '[PRE33]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To inject secrets securely at runtime, use environment variables passed with
    `docker run` or use secret management tools such as **Docker secrets**:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在运行时安全地注入秘密，请使用`docker run`传递的环境变量或使用如**Docker secrets**之类的秘密管理工具：
- en: '[PRE34]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: MLOPs and LLMOPs are important concepts for anyone who wants to use an ML model
    or LLM in production. In the next section, we will discuss other important concepts
    in production deployment, such as asynchronous programming, which allows us to
    handle multiple concurrent user requests.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: MLOPs和LLMOPs是任何希望在生产中使用ML模型或LLM的人的重要概念。在下一节中，我们将讨论生产部署中的其他重要概念，例如异步编程，它允许我们处理多个并发用户请求。
- en: Asynchronous programming
  id: totrans-405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步编程
- en: So far, you’ve seen examples where tasks are executed one after the other. But
    what if some tasks don’t need to block the flow of the entire program while waiting?
    That’s where asynchronous programming comes in.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到了任务依次执行示例。但如果某些任务在等待时不需要阻塞整个程序的流程呢？这就是异步编程发挥作用的地方。
- en: Asynchronous programming allows tasks to cooperatively share the CPU. Instead
    of each task waiting for the previous one to finish, tasks can voluntarily pause
    and let others run, making better use of the single processor’s time. This does
    not imply simultaneous execution; instead, it indicates a smart interleaving of
    their operations.; this is especially useful when waiting for things such as I/O
    operations.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 异步编程允许任务协作共享CPU。不是每个任务都等待前一个任务完成，任务可以自愿暂停并让其他任务运行，从而更好地利用单个处理器的时间。这并不表示同时执行；相反，它表示它们操作的智能交织；这在等待I/O操作等事物时特别有用。
- en: Think of it as multiple conversations happening with one person switching between
    them, efficiently and politely. In Python, this is achieved using the `asyncio`
    module, which supports cooperative multitasking on a single CPU.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，一个人在与多个人进行多个对话时切换，高效且礼貌。在Python中，这是通过使用`asyncio`模块实现的，该模块支持单个CPU上的协作多任务。
- en: As you’ll see in the comparison table, asynchronous code is different from using
    threads or multiple processes. It runs on just one core, but it can still feel
    fast, especially when dealing with many I/O-bound tasks.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在比较表中看到的，异步代码与使用线程或多个进程不同。它只在单个核心上运行，但仍然可以感觉很快，尤其是在处理许多I/O-bound任务时。
- en: '| **Python module** | **Number** **of CPUs** | **Task** **switching style**
    | **Switching decision** |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| **Python模块** | **CPU数量** | **任务切换风格** | **切换决策** |'
- en: '| `asyncio` | Single | Cooperative multitasking | Tasks yield control voluntarily
    through the `await` keyword |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| `asyncio` | 单一 | 协作多任务 | 任务通过`await`关键字自愿交出控制权 |'
- en: '| `threading` | Single | Preemptive multitasking | OS decides when to switch
    threads |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| `threading` | 单一 | 预先多任务 | 操作系统决定何时切换线程 |'
- en: '| `multiprocessing` | Multiple | Preemptive multitasking | Separate processes
    run independently, but on the same machine; the OS decides when to switch |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| `multiprocessing` | 多个 | 预先多任务 | 独立运行的进程，但位于同一台机器上；操作系统决定何时切换 |'
- en: 'Table 10.2 – Concurrency mechanisms in Python: differences between asyncio,
    threading, and multiprocessing'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.2 – Python中的并发机制：asyncio、线程和进程之间的差异
- en: 'Concurrency is particularly useful in two types of scenarios: when a program
    is waiting for responses from external systems (I/O-bound), and when it is handling
    a high computational workload (CPU-bound).'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 并发在两种类型的场景中特别有用：当程序等待外部系统的响应（I/O-bound）时，以及当它处理高计算负载（CPU-bound）时。
- en: In I/O-bound situations, a script spends most of its time waiting for data to
    arrive from a source, such as a filesystem, a network connection, a database,
    or an API. During this time, the CPU is often idle, making it a perfect opportunity
    to run other tasks concurrently.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在I/O-bound情况下，脚本的大部分时间都在等待从文件系统、网络连接、数据库或API等来源的数据到达。在这段时间里，CPU通常处于空闲状态，这使得并行运行其他任务成为完美的机会。
- en: In contrast, CPU-bound tasks keep the processor fully occupied with calculations
    such as rendering images, parsing large datasets, or performing cryptographic
    operations. In these cases, concurrency helps by distributing the workload across
    multiple CPU cores, enabling true parallel execution. This form of concurrency
    (better described as parallelism) can significantly reduce total processing time
    for heavy computations.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，CPU-bound任务使处理器完全忙于计算，如渲染图像、解析大型数据集或执行加密操作。在这些情况下，并发通过在多个CPU核心上分配工作负载，实现真正的并行执行。这种并发（更准确地说是并行）可以显著减少重计算的总处理时间。
- en: '| **Type** **of task** | **Main limitation** | **Examples** | **Concurrency
    benefit** | **Execution style** |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| **任务类型** | **主要限制** | **示例** | **并发优势** | **执行风格** |'
- en: '| I/O-bound | Slow external systems | Reading files, API requests, database
    queries | Keeps CPU busy while waiting for I/O | Cooperative (`asyncio`) |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| I/O-bound | 慢速外部系统 | 读取文件、API请求、数据库查询 | 在等待I/O时保持CPU忙碌 | 协作（`asyncio`） |'
- en: '| CPU-bound | Intensive computation | Data crunching, image processing, encryption
    | Distributes load across multiple cores for real parallelism | Preemptive (`threading`,
    `multiprocessing`) |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| CPU-bound | 繁重计算 | 数据处理、图像处理、加密 | 在多个核心上分配负载以实现真正的并行性 | 预先（`threading`、`multiprocessing`）
    |'
- en: 'Table 10.3 – I/O-bound vs. CPU-bound: task types and optimal concurrency models'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 表10.3 – I/O-bound与CPU-bound：任务类型和最佳并发模型
- en: The following diagram illustrates how task execution differs between synchronous
    and asynchronous when dealing with I/O-bound operations.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了在处理I/O-bound操作时，同步和异步任务执行的不同之处。
- en: '![Figure 10.44 – Comparison of blocking vs non-blocking I/O execution](img/B21257_10_44.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.44 – 阻塞与非阻塞 I/O 执行的比较](img/B21257_10_44.jpg)'
- en: Figure 10.44 – Comparison of blocking vs non-blocking I/O execution
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.44 – 阻塞与非阻塞 I/O 执行的比较
- en: In the first row, each request blocks the CPU until the I/O completes. In the
    second row (async), the CPU switches between tasks during I/O wait times, improving
    efficiency on a single core and minimizing the idle time.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行，每个请求会阻塞 CPU 直到 I/O 完成。在第二行（异步），CPU 在 I/O 等待时间内在任务之间切换，提高单核效率并最小化空闲时间。
- en: When multiple I/O-bound requests arrive in sequence, using a single thread to
    handle each of them one after the other would block the program during I/O waits.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 当多个 I/O 密集型请求按顺序到达时，使用单个线程逐个处理每个请求会在 I/O 等待时阻塞程序。
- en: To improve responsiveness, the `threading` module can be used to delegate each
    request to a separate thread.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高响应性，可以使用 `threading` 模块将每个请求委托给单独的线程。
- en: 'In the following diagram, each incoming request is assigned to one of four
    worker threads. The actual workload (T1, T2, T3, ...) represents short bursts
    of CPU activity interleaved with I/O waits:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，每个进入的请求被分配给四个工作线程之一。实际的工作负载（T1, T2, T3, ...）代表短期的 CPU 活动与 I/O 等待的交错：
- en: '![Figure 10.45 – Concurrent request handling with four worker threads and interleaved
    CPU/I/O workloads](img/B21257_10_45.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.45 – 使用四个工作线程和交错 CPU/I/O 工作负载的并发请求处理](img/B21257_10_45.jpg)'
- en: Figure 10.45 – Concurrent request handling with four worker threads and interleaved
    CPU/I/O workloads
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.45 – 使用四个工作线程和交错 CPU/I/O 工作负载的并发请求处理
- en: This pattern is useful when your program must remain responsive while interacting
    with slow external systems such as APIs, databases, filesystems, or even GUIs.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的程序在与慢速的外部系统（如 API、数据库、文件系统或甚至 GUI）交互时必须保持响应性时，这种模式很有用。
- en: Asynchronous programming is a type of parallel programming that allows programs
    to perform tasks concurrently, without blocking the main execution thread. For
    example, when we have multiple users interacting with the system at the same time,
    we will have more tasks to handle at the same time, with some tasks taking more
    time by blocking our agent. In traditional synchronous programming, tasks are
    executed one after the other, where each task must wait for the previous one to
    finish before it can begin; tasks are executed sequentially in the order in which
    they are written. Each task must complete fully before the next begins, which
    can lead to delays if a task involves waiting, such as for file I/O or network
    operations. Asynchronous programming, on the other hand, allows tasks that may
    block execution to be initiated and handled concurrently. Instead of waiting for
    a task to finish, the program can move on to other tasks, returning to the blocked
    task once it is ready. This approach improves efficiency by making better use
    of system resources, particularly in scenarios involving high-latency operations,
    such as web requests or database queries, enabling more responsive and performant
    applications.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 异步编程是一种并行编程类型，它允许程序在不阻塞主执行线程的情况下并发执行任务。例如，当有多个用户同时与系统交互时，我们将同时处理更多任务，其中一些任务由于阻塞我们的代理而需要更多时间。在传统的同步编程中，任务一个接一个地执行，每个任务必须等待前一个任务完成才能开始；任务按照它们被编写的顺序顺序执行。每个任务必须完全完成，下一个才开始，如果任务涉及等待，如文件
    I/O 或网络操作，这可能会导致延迟。另一方面，异步编程允许可能阻塞执行的任务被并发启动和处理。程序不需要等待任务完成，可以继续执行其他任务，一旦准备就绪，再回到阻塞的任务。这种方法通过更好地利用系统资源，尤其是在涉及高延迟操作的场景中（如网络请求或数据库查询），提高了效率，使应用程序更加响应和高效。
- en: 'There are some key concepts for discussing asynchronous programming:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论异步编程有一些关键概念：
- en: '**Concurrency**: This refers to the ability to handle different tasks at the
    same time; however, this does not mean that tasks are handled simultaneously.
    Tasks are started and completed in overlapping time periods, but not simultaneously.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并发性**：这指的是同时处理不同任务的能力；然而，这并不意味着任务会同时处理。任务是在重叠的时间段内启动和完成的，但不是同时。'
- en: '**Parallelism**: This refers to the ability to accomplish tasks at exactly
    the same time, usually by using multiple processors or cores. While concurrency
    may or may not involve parallelism, parallelism always involves concurrency.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行性**：这指的是同时完成任务的能力，通常是通过使用多个处理器或核心来实现的。虽然并发性可能涉及或不涉及并行性，但并行性总是涉及并发性。'
- en: '**Blocking operations**: These are operations that wait for a task to complete
    before starting a new operation (e.g., reading a file from disk before starting
    to process text).'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阻塞操作**：这些是在开始新操作之前等待任务完成的操作（例如，在开始处理文本之前从磁盘读取文件）。'
- en: '**Non-blocking operations**: This refers to the ability to start a task and
    continue the program with other tasks without waiting for the task to complete
    (making an HTTP request and continuing to generate more text with an LLM while
    waiting for the response).'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非阻塞操作**：这指的是启动一个任务并继续程序执行其他任务，而无需等待该任务完成的能力（例如，在等待HTTP请求响应的同时，使用大型语言模型生成更多文本）。'
- en: '**Callbacks**: These are functions passed as arguments to other functions that
    are executed when a task completes.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回调**：这些是作为参数传递给其他函数的函数，在任务完成时执行。'
- en: '**Promises and futures**: These are abstractions that represent the eventual
    result of an asynchronous operation. A promise is a value (or result) that may
    be unavailable at that time but will be available at some later point. A future
    is the same thing but is commonly used in languages such as Python and Java.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**承诺和未来**：这些是表示异步操作最终结果的抽象。承诺是一个值（或结果），在那时可能不可用，但将在稍后的某个时刻可用。未来与承诺相同，但在Python和Java等语言中更常用。'
- en: '**Event loop**: This is the fundamental component of an asynchronous program,
    where tasks, events, or signals are listed and scheduled for execution when the
    resources are available. In other words, we use an event loop to allow tasks to
    run without blocking the main program. The event loop waits for an event to occur
    and calls an appropriate callback function at this point.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件循环**：这是异步程序的基本组件，其中任务、事件或信号被列出并安排在资源可用时执行。换句话说，我们使用事件循环来允许任务在没有阻塞主程序的情况下运行。事件循环等待事件发生，并在此时调用适当的回调函数。'
- en: '**Coroutines**: These are special functions that can be paused and then resumed
    during their execution. In other words, a function can start and then be paused
    to wait for the result of another task. For example, when we start an analysis
    of some documents, the function pauses while we conduct an HTTP request to find
    more information that is needed to accomplish our function. When the results of
    the HTTP request arrive, the function resumes.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协程**：这些是可以在执行过程中暂停和恢复的特殊函数。换句话说，一个函数可以开始，然后暂停以等待另一个任务的结果。例如，当我们开始分析某些文档时，函数会暂停，以便我们进行HTTP请求以找到完成函数所需的信息。当HTTP请求的结果到达时，函数会继续执行。'
- en: It may seem counterintuitive how asynchronous programming makes code execution
    faster (after all, no additional resources are being used). I have made extensive
    changes here for conciseness and clarity. Please confirm whether your intended
    meaning has been retained. In the synchronous format, she completes each game
    one at a time before moving to the next. With each move taking her 10 seconds
    and her opponent 60 seconds, a full game of 30 moves per player (60 moves total)
    takes 2,100 seconds. Playing all 24 games sequentially requires 50,400 seconds,
    or roughly 14 hours.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 异步编程如何使代码执行更快可能看起来有些反直觉（毕竟，没有使用额外的资源）。我为了简洁和清晰在这里做了大量的修改。请确认你的意图是否得到了保留。在同步格式中，她在开始下一轮游戏之前，会先完成每一轮游戏。每次移动需要她10秒，而对手需要60秒，每位玩家30步（总共60步）的完整游戏需要2,100秒。连续玩24场比赛需要50,400秒，即大约14小时。
- en: In contrast, the asynchronous format has Judit moving from board to board, making
    one move per game while each opponent thinks during her rotation. One full round
    of 24 moves takes 240 seconds, and since each player takes 60 seconds to respond,
    Judit returns to each board just as the opponent is ready. Over 30 rounds, the
    entire session lasts only 7,200 seconds, or approximately 2 hours—making asynchronous
    play significantly more time-efficient.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，异步格式中，Judit 在棋盘之间移动，每轮游戏移动一次，而对手在她轮换时思考。一整轮24步需要240秒，由于每位玩家响应需要60秒，Judit
    在对手准备好的时候回到每个棋盘。在30轮比赛中，整个会话仅持续7,200秒，即大约2小时——这使得异步游戏在时间效率上显著更高。
- en: In async programming, we do exactly the same, the event loop allows us to manage
    the various tasks in an optimal time management manner. A function that would
    block other tasks can be optimally blocked when we need to run other tasks, allowing
    optimized management of the entire program. Here, we do not want to optimize the
    time of each game but the whole performance.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在异步编程中，我们做的是完全相同的，事件循环允许我们以最优的时间管理方式管理各种任务。当需要运行其他任务时，一个可能会阻塞其他任务的功能可以被最优地阻塞，从而优化整个程序的管理。在这里，我们不想优化每场比赛的时间，而是整个表演的时间。
- en: 'We can then manage multiple processes at the same time in different ways:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以不同的方式同时管理多个进程：
- en: '**Multiple processes**: A process is an independent program in execution. Each
    process has its own memory, resources, and execution context. In the simplest
    way, we can manage different processes at the same time (for example, several
    players playing the 24 games is a simple example of multiple processes occurring
    at the same time during performance). In the case of programming, this means that
    different scripts or processes can run at the same time (e.g., four functions
    and each of them runs on a different CPU). However, this approach is very inefficient.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个进程**：进程是执行中的独立程序。每个进程都有自己的内存、资源和执行上下文。以最简单的方式，我们可以同时管理不同的进程（例如，几个玩家同时玩24游戏是性能期间同时发生多个进程的简单例子）。在编程的情况下，这意味着不同的脚本或进程可以同时运行（例如，四个函数，每个函数在不同的CPU上运行）。然而，这种方法非常低效。'
- en: '**Multiple threads**: This is a variation of the previous approach. A thread
    is the smallest unit of execution within a process. Multiple threads can be within
    the same process and share the same memory, but each thread has its own execution
    stack. In this case, several threads are executed at the same time.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个线程**：这是先前方法的变体。线程是进程内的最小执行单元。多个线程可以位于同一进程内并共享相同的内存，但每个线程都有自己的执行栈。在这种情况下，同时执行多个线程。'
- en: '`asyncio` does exactly this by exploiting coroutines and futures to simplify
    asynchronous code.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`asyncio` 通过利用协程和未来来简化异步代码，正是这样做的。'
- en: Asynchronous programming, therefore, improves performance when some tasks are
    time-consuming and can block the execution of a program. In this way, the system
    can continue executing other tasks while it waits for them to complete. It also
    allows better utilization of system resources (for example, while waiting for
    a network request, the program can perform calculations or handle other requests).
    Asynchronous programming also helps to achieve systems that are more scalable
    and can handle multiple requests in parallel, reducing the number of threads.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，异步编程在有些任务耗时且可能阻塞程序执行时可以提高性能。这样，系统可以在等待它们完成的同时继续执行其他任务。它还允许更好地利用系统资源（例如，在等待网络请求时，程序可以进行计算或处理其他请求）。异步编程还有助于实现更可扩展的系统，可以并行处理多个请求，减少线程数量。
- en: asyncio
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: asyncio
- en: '`async`/`await` syntax. It provides a framework for running asynchronous operations,
    without relying on multithreading or multiprocessing. The heart of `asyncio` is
    the event loop, which schedules and executes asynchronous tasks (called coroutines)
    in the background. A coroutine is similar to a generator in Python: it can pause
    execution and let other tasks run and then resume later. It is the event loop
    that tracks the state of these coroutines and their results, which are presented
    as `futures`.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '`async`/`await` 语法。它提供了一个运行异步操作的框架，而不依赖于多线程或多进程。`asyncio` 的核心是事件循环，它在后台调度和执行异步任务（称为协程）。协程在
    Python 中类似于生成器：它可以暂停执行，让其他任务运行，然后稍后继续。事件循环跟踪这些协程及其结果的状态，这些结果以 `futures` 的形式呈现。'
- en: 'Here is a basic example of a coroutine:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个协程的基本示例：
- en: '[PRE35]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: While this code shows how to define and run a coroutine by using the event loop,
    it does not yet take advantage of concurrent execution. In fact, to execute multiple
    asynchronous tasks concurrently, we can use either `asyncio.gather()` or `asyncio.create_task()`.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这段代码展示了如何通过事件循环定义和运行协程，但它还没有利用并发执行。事实上，为了并发执行多个异步任务，我们可以使用 `asyncio.gather()`
    或 `asyncio.create_task()`。
- en: While `gather()` is useful when you want to run several coroutines and wait
    for all of them to finish together, `create_task()` provides more flexibility.
    It allows you to launch coroutines in the background and decide when (or whether)
    to await their results later in your program. Let’s look at some examples together.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`gather()`在你想要运行多个协程并等待它们全部完成时很有用，但`create_task()`提供了更多的灵活性。它允许你在后台启动协程，并在你的程序中决定何时（或是否）等待它们的结果。让我们一起来查看一些示例。
- en: 'The following example uses `asyncio.gather()` to execute multiple coroutines
    concurrently:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用`asyncio.gather()`来并发执行多个协程：
- en: '[PRE36]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In this case, both tasks are executed concurrently, and the total execution
    time will be close to 2 seconds: the time taken by the longest task.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，两个任务都是并发执行的，总执行时间将接近2秒：最长任务的执行时间。
- en: We can achieve the same result using `asyncio.create_task()`, which offers more
    control over task scheduling. Unlike `asyncio.gather()`, which groups coroutines
    and waits for all of them together, `create_task()` lets us launch coroutines
    individually and decide when to await their results. This is particularly useful
    when we want to run background tasks while doing other work.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`asyncio.create_task()`达到相同的结果，它提供了对任务调度的更多控制。与将协程分组并一起等待所有协程的`asyncio.gather()`不同，`create_task()`允许我们单独启动协程，并决定何时等待它们的结果。这在我们需要在执行其他工作的同时运行后台任务时特别有用。
- en: 'Here is the same example rewritten with `create_task()`:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用`create_task()`重写的相同示例：
- en: '[PRE37]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Each call to `create_task()` returns a `Task` object, which represents the running
    coroutine and can be awaited, cancelled, or monitored.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 每次调用`create_task()`都会返回一个`Task`对象，它代表正在运行的协程，可以被等待、取消或监控。
- en: 'The result is the same: both tasks run concurrently, and the output is printed
    once each finish. However, with `create_task()`, we gain more flexibility.'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是相同的：两个任务都是并发运行的，每个任务完成时都会打印输出。然而，使用`create_task()`，我们获得了更多的灵活性。
- en: For example, we can start several background tasks and continue executing other
    logic in `main()`. Then, we can await only the results we need at a specific point
    in the workflow. This flexibility makes `create_task()` especially useful in complex
    workflows where not all tasks are equally important or time-sensitive.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以启动几个后台任务，并在`main()`中继续执行其他逻辑。然后，我们可以在工作流程的特定点等待我们需要的仅有的结果。这种灵活性使得`create_task()`在复杂的工作流程中特别有用，在这些工作流程中，并非所有任务都同等重要或时间敏感。
- en: To better understand the real-world impact of asynchronous programming, let’s
    compare an example of synchronous versus asynchronous execution. Specifically,
    we will simulate fetching data from a website using HTTP requests by using Python
    `requests` library. This will highlight how asynchronous code can significantly
    improve performance when dealing with I/O-bound tasks such as network calls.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解异步编程在现实世界中的影响，让我们比较一下同步与异步执行的示例。具体来说，我们将通过使用Python `requests`库模拟从网站获取数据来使用HTTP请求。这将突出异步代码在处理I/O密集型任务（如网络调用）时如何显著提高性能。
- en: 'Here is the synchronous code:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是同步代码：
- en: '[PRE38]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Here is the asynchronous code:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是异步代码：
- en: '[PRE39]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The following figures show the output of the synchronous and asynchronous implementations
    described above, respectively. As we can see, the synchronous version performs
    the HTTP requests one after the other, resulting in a longer total execution time.
    The asynchronous version, on the other hand, sends all requests concurrently,
    significantly reducing the total time required.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了上述所述的同步和异步实现的输出，分别。如图所示，同步版本依次执行HTTP请求，导致总执行时间更长。另一方面，异步版本并发发送所有请求，显著减少了所需的总时间。
- en: '![Figure 10.46 – Synchronous result](img/B21257_10_46.jpg)'
  id: totrans-471
  prefs: []
  type: TYPE_IMG
  zh: '![图10.46 – 同步结果](img/B21257_10_46.jpg)'
- en: Figure 10.46 – Synchronous result
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.46 – 同步结果
- en: '![Figure 10.47 – Asynchronous result](img/B21257_10_47.jpg)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![图10.47 – 异步结果](img/B21257_10_47.jpg)'
- en: Figure 10.47 – Asynchronous result
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.47 – 异步结果
- en: Asynchronous programming and ML
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异步编程与机器学习
- en: 'Conjugating asynchronous programming with ML in Python can be a powerful combination.
    Asynchronous programming can improve performance by allowing non-blocking operations,
    such as loading large datasets, running hyperparameter tuning, or interacting
    with APIs. For example, we can see different possibilities:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中将异步编程与机器学习（ML）结合可以是一种强大的组合。异步编程可以通过允许非阻塞操作来提高性能，例如加载大型数据集、运行超参数调整或与API交互。例如，我们可以看到不同的可能性：
- en: '**Data loading**: In ML workflows, especially when working with large datasets,
    loading and preprocessing data can often be a bottleneck. Asynchronous programming
    can help speed this up by loading different parts of the data concurrently. For
    example, you can asynchronously load multiple chunks of a dataset while concurrently
    performing some I/O-bound tasks (such as data augmentation, cleaning, or transformation).'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据加载**：在机器学习工作流程中，尤其是在处理大型数据集时，加载数据和预处理数据往往是一个瓶颈。异步编程可以通过并发加载数据的不同部分来加速这个过程。例如，您可以在同时执行一些I/O密集型任务（如数据增强、清理或转换）的同时异步加载数据集的多个块。'
- en: '**Hyperparameter tuning**: The tuning of hyperparameters is one of the most
    time-consuming and slowest processes, which can benefit from conducting some tasks
    asynchronously. For example, when performing a grid or random search on hyperparameters,
    different configurations can be evaluated simultaneously rather than sequentially.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数调整**：超参数调整是耗时最长且速度最慢的过程之一，可以从异步执行一些任务中受益。例如，当对超参数进行网格或随机搜索时，可以同时评估不同的配置，而不是按顺序进行。'
- en: '**Asynchronous inference**: You can use asynchronous programming to create
    a non-blocking API to serve trained ML models. This is especially useful when
    deploying a model for real-time inference and wanting to handle multiple queries
    simultaneously.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步推理**：您可以使用异步编程创建一个非阻塞API来服务训练好的机器学习模型。这在部署模型进行实时推理并希望同时处理多个查询时特别有用。'
- en: '**Model training**: Although training is usually conducted on different GPUs/CPUs
    in parallel, asynchronous scheduling can be conjugated to allow better loading
    and preprocessing of data while training appears in parallel. This is particularly
    useful when we have different data to retrieve.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练**：尽管训练通常是在不同的GPU/CPUs上并行进行的，但异步调度可以结合使用，以在训练并行进行的同时更好地加载数据和预处理数据。这在我们需要检索不同数据时特别有用。'
- en: We can observe a classic example of hyperparameter tuning. In this simple example
    with the classic Iris dataset and a simple model, we’ll show how using `asyncio`
    saves some time.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到超参数调整的经典示例。在这个使用经典Iris数据集和简单模型的简单例子中，我们将展示如何使用`asyncio`节省一些时间。
- en: 'Here is the synchronous code:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是同步代码：
- en: '[PRE40]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In the preceding script, we run an ML model and search for the best parameters.
    This script shows how even a small model takes a lot of time to be executed.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的脚本中，我们运行了一个机器学习模型并搜索最佳参数。这个脚本展示了即使是小型模型也需要花费很多时间来执行。
- en: '![Figure 10.48 – Synchronous result](img/B21257_10_48.jpg)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
  zh: '![图10.48 – 同步结果](img/B21257_10_48.jpg)'
- en: Figure 10.48 – Synchronous result
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.48 – 同步结果
- en: 'Here is the asynchronous code:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是异步代码：
- en: '[PRE41]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In this case, we trained the same model using asynchronous programming. This
    approach allowed us to save time and thus reduce the execution time.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用异步编程训练了相同的模型。这种方法使我们能够节省时间，从而减少执行时间。
- en: '![Figure 10.49 – Asynchronous result](img/B21257_10_49.jpg)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![图10.49 – 异步结果](img/B21257_10_49.jpg)'
- en: Figure 10.49 – Asynchronous result
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.49 – 异步结果
- en: This can also be applied to an LLM as an agent. Traditionally, function calls
    block LLM inference, making the process inefficient as each function call must
    complete before moving to the next. Some authors propose instead to implement
    an async approach even with LLMs (or generate tokens and execute function calls
    concurrently) when tools are connected like in agents. For example, one can consider
    interruptible LLM decoding, where the function executor notifies the LLM asynchronously,
    allowing it to continue generating tokens while waiting for function call results.
    The purpose of this approach is to reduce latency by conducting an overlap of
    function execution and token generation.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以应用于作为代理的LLM。传统上，函数调用会阻塞LLM推理，使得过程效率低下，因为每个函数调用必须完成才能进行下一个。一些作者提出，即使是在代理工具连接的情况下，也可以为LLM（或生成令牌和执行函数调用）实现异步方法。例如，可以考虑可中断的LLM解码，其中函数执行器异步地通知LLM，允许它在等待函数调用结果的同时继续生成令牌。这种方法的目的是通过重叠函数执行和令牌生成来减少延迟。
- en: '![Figure 10.50 – Synchronous vs. asynchronous function calling (https://arxiv.org/pdf/2412.07017)](img/B21257_10_50.jpg)'
  id: totrans-493
  prefs: []
  type: TYPE_IMG
  zh: '![图10.50 – 同步与异步函数调用](https://arxiv.org/pdf/2412.07017)(img/B21257_10_50.jpg)'
- en: Figure 10.50 – Synchronous vs. asynchronous function calling ([https://arxiv.org/pdf/2412.07017](https://arxiv.org/pdf/2412.07017))
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.50 – 同步与异步函数调用([https://arxiv.org/pdf/2412.07017](https://arxiv.org/pdf/2412.07017))
- en: 'So, in theory, we can have three approaches for an LLM agent:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从理论上讲，我们可以为LLM代理采用三种方法：
- en: '**Synchronous LLM function calling**: Each function is executed one after the
    other. An LLM must wait for each function to complete before it can continue with
    the next one. This approach is the simplest, but it adds latency to the system
    since it must wait for each operation to finish (e.g., reading HTML, reading XLS
    files, generating tokens, etc.) before it can continue. This leads to high inefficiency,
    especially if there are many functions or some functions lose a lot of time.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步LLM函数调用**：每个函数依次执行。LLM必须等待每个函数完成，才能继续执行下一个。这种方法是最简单的，但它会增加系统的延迟，因为它必须等待每个操作完成（例如，读取HTML、读取XLS文件、生成令牌等）才能继续。这导致效率低下，尤其是在有多个函数或某些函数花费大量时间的情况下。'
- en: '**Synchronous LLM function calling with parallel optimization**: This process
    tries to optimize each task in parallel (e.g., reading HTML, reading XLS, and
    reading text simultaneously), but each task still blocks the next one. The advantage
    over the previous approach is that each function can be conducted concurrently,
    with an increase in speed over the previous one. Synchronization is required to
    conduct the tasks in the right order. Although the tasks are optimized, they are
    still synchronous, so we have to wait for a function to finish before completing
    some tasks.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步LLM函数调用与并行优化**：这个过程试图并行优化每个任务（例如，同时读取HTML、读取XLS和读取文本），但每个任务仍然会阻塞下一个任务。与之前的方法相比，优势在于每个函数可以并发执行，速度比之前的方法有所提高。需要同步以正确顺序执行任务。尽管任务已优化，但它们仍然是同步的，因此我们必须等待一个函数完成才能完成某些任务。'
- en: "**Asynchronous LLM function calling**: In this approach, tasks are executed\
    \ asynchronously, meaning that functions do not block one another. The system\
    \ can read HTML, read XLS, and read text while simultaneously performing other\
    \ operations (such as summarizing or saving data). This leads to a noticeable\
    \ improvement in latency, improving the use of resources. The system ensures that\
    \ dependent tasks (e.g., summarizing and saving PDFs) are only performed once\
    \ the necessary data (e.g., reading text) is available. Dependencies are managed\
    \ dynamically without halting other operations. Multiprocessing parallelization\
    \ (the previous approach) creates different processes or threads in order to handle\
    \ tasks concurrently, thus allocating resources and memory. This leads to more\
    \ resource consumption than in an asynchronous version, and consumption can explode\
    \ depending on how many functions we have. Also, this approach is more scalable.![Figure\
    \ 10.51 – Comparison of LLM-executor interactions (https:\uFEFF//arxiv.org/pdf/2412.07017)](img/B21257_10_51.jpg)"
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步LLM函数调用**：在这种方法中，任务异步执行，意味着函数不会相互阻塞。系统可以在同时执行其他操作（如总结或保存数据）的同时读取HTML、读取XLS和读取文本。这导致延迟显著改善，提高了资源利用率。系统确保在必要数据（例如，读取文本）可用后，才执行依赖任务（例如，总结和保存PDF）。依赖关系动态管理，而不会停止其他操作。多进程并行化（先前的方案）创建不同的进程或线程以并发处理任务，从而分配资源和内存。这导致比异步版本更多的资源消耗，并且消耗量会根据我们拥有的函数数量而爆炸式增长。此外，这种方法更具可扩展性。![图10.51
    – LLM执行器交互比较 (https://arxiv.org/pdf/2412.07017)](img/B21257_10_51.jpg)'
- en: Figure 10.51 – Comparison of LLM-executor interactions ([https://arxiv.org/pdf/2412.07017](https://arxiv.org/pdf/2412.07017))
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.51 – LLM执行器交互比较([https://arxiv.org/pdf/2412.07017](https://arxiv.org/pdf/2412.07017))
- en: Once we have made our system (our application) efficient, it should be placed
    in isolation to avoid external problems. In the next section, we will explain
    in detail exactly how Docker allows us to do this.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们使我们的系统（我们的应用程序）高效，它应该被放置在隔离的环境中，以避免外部问题。在下一节中，我们将详细解释Docker如何使我们能够做到这一点。
- en: Docker
  id: totrans-501
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Docker
- en: '**Docker** is an open source platform that enables developers and system administrators
    to create, deploy, and run applications in containers. Containers allow software
    to be packaged along with all its dependencies (such as libraries, configurations,
    etc.) and run consistently across different environments, whether it’s a developer’s
    laptop, a test server, or a production machine.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '**Docker**是一个开源平台，它使开发人员和系统管理员能够在容器中创建、部署和运行应用程序。容器允许软件及其所有依赖项（如库、配置等）打包在一起，并在不同的环境中一致运行，无论是开发人员的笔记本电脑、测试服务器还是生产机器。'
- en: Containers can then be viewed as virtual machines, allowing for reduced overhead
    and better utilization of resources and the system itself (especially if we have
    to use a single model on several systems). The idea is that our software (of which
    our model or an LLM plus agents is a component) can run in isolation to prevent
    problems from arising that impact its execution and performance. The use of virtual
    machines is an example of how a system can run in a guest **operating system**
    (**OS**) and use resources. Optimizing a system for a guest OS, however, requires
    considerable resources. Containers try to reduce resource consumption and overhead
    in order to run the application. Containers offer a way to package an application
    to make it abstract from the environment in which it runs. This decoupling then
    allows a container to run in any target environment, predictably and isolated
    from other applications. At the same time, the container provides the ability
    to control our environment in a granular manner. Docker containers are lightweight
    and portable and ensure that the application behaves the same way everywhere.
    Given these benefits, Docker containers have been adopted by many companies.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 容器可以被视为虚拟机，从而减少开销，更好地利用资源和系统本身（尤其是如果我们需要在多个系统上使用单个模型时）。其理念是，我们的软件（其中我们的模型或一个LLM加上代理是一个组件）可以独立运行，以防止出现影响其执行和性能的问题。虚拟机的使用是系统如何在客户操作系统（**OS**）中运行并使用资源的例子。然而，为虚拟机优化系统需要相当多的资源。容器试图减少资源消耗和开销，以便运行应用程序。容器提供了一种打包应用程序的方法，使其从运行环境抽象出来。这种解耦使得容器可以在任何目标环境中运行，可预测且与其他应用程序隔离。同时，容器提供了以细粒度控制我们环境的能力。Docker容器轻量级且便携，确保应用程序在任何地方的行为都相同。鉴于这些好处，许多公司已经采用了Docker容器。
- en: 'Docker is based on a few main concepts:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: Docker基于几个主要概念：
- en: '**Containers**: These are the basic units of Docker and contain an application
    and its dependencies in a single package that can be easily moved between environments.
    A container also contains the OS kernels, to reduce the resources needed. Unlike
    a virtual machine that contains the entire OS, Docker containers contain only
    the information needed to run the application. This makes Docker containers much
    faster and more efficient to run.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器**：这些是Docker的基本单元，包含一个应用程序及其依赖项的单个包，可以在不同环境中轻松移动。容器还包含操作系统内核，以减少所需资源。与包含整个操作系统的虚拟机不同，Docker容器只包含运行应用程序所需的信息。这使得Docker容器运行得更快、更高效。 '
- en: '**Images**: An image is a read-only template used to create containers. It
    contains the application code, runtime, libraries, and environment variables.
    Docker images contain the blueprint of the application – all the information to
    be able to execute a code. There are many ready-made images in Docker Hub that
    can be used to efficiently create containers and reduce the need to start from
    scratch.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**镜像**：镜像是一个只读模板，用于创建容器。它包含应用程序代码、运行时、库和环境变量。Docker镜像包含应用程序的蓝图——所有能够执行代码所需的信息。Docker
    Hub中有许多现成的镜像，可以用来高效地创建容器，并减少从头开始的需求。'
- en: '**Docker Engine**: This is the component responsible for managing and running
    containers (runtime environment for Docker). Docker Engine runs on both Linux
    and Windows OSs.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker Engine**：这是负责管理和运行容器（Docker的运行环境）的组件。Docker Engine在Linux和Windows操作系统上运行。'
- en: '**Dockerfile**: A Dockerfile is a script containing instructions on how to
    build a Docker image. This file specifies which base image to use, how to install
    dependencies, environment configurations, and other details.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dockerfile**：Dockerfile是一个包含构建Docker镜像指令的脚本。此文件指定了要使用的基础镜像、如何安装依赖项、环境配置和其他细节。'
- en: '**Docker Compose**: This is a tool for defining and running multi-container
    Docker applications.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Docker Compose**：这是一个用于定义和运行多容器Docker应用程序的工具。'
- en: 'Docker containers thus have a number of advantages:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: Docker容器因此具有许多优势：
- en: '**Portability**: Docker containers encapsulate an application and its dependencies
    in a single, portable unit. In this way, the system abstracts away differences
    between environments, making it more reliable and consistent in deployment.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可移植性**：Docker容器将应用程序及其依赖项封装在一个单一、可移植的单元中。通过这种方式，系统抽象了环境之间的差异，使得部署更加可靠和一致。'
- en: '**Efficiency**: The system is more efficient compared to traditional virtual
    machines. By using only the kernel, the system uses far fewer resources, thus
    making it easier to deploy and more scalable. Docker integrates well with other
    orchestration tools, such as Kubernetes and Docker Swarm, making it easier to
    scale an application both horizontally (more containers) and vertically (increasing
    the resources available to containers).'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率**：与传统的虚拟机相比，该系统更加高效。通过仅使用内核，系统使用 fewer 资源，因此更容易部署和扩展。Docker与Kubernetes和Docker
    Swarm等其他编排工具集成良好，这使得水平扩展（更多容器）和垂直扩展（增加容器可用的资源）应用程序变得更加容易。'
- en: '**Isolation**: Docker provides strong isolation between containers, allowing
    them to run independently and not interfere with each other, thus improving security
    and avoiding conflicts between different applications.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隔离**：Docker在容器之间提供了强大的隔离，允许它们独立运行，不会相互干扰，从而提高安全性并避免不同应用程序之间的冲突。'
- en: '**Version control and reproducibility**: A container allows you to store, share,
    and deploy specific versions of an application, ensuring that a single version
    is used in different environments, thus improving reproducibility.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**版本控制和可重复性**：容器允许你存储、共享和部署应用程序的特定版本，确保在不同环境中使用的是单个版本，从而提高可重复性。'
- en: 'Like any system, there are also disadvantages:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何系统一样，也存在一些缺点：
- en: '**Security concerns**: It can introduce some vulnerabilities, especially if
    you’re not shrewd and handy with Docker. It has a certain learning curve, especially
    if you want to use the system efficiently.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全担忧**：它可能会引入一些漏洞，尤其是如果你不熟悉且不擅长使用Docker。它有一定的学习曲线，尤其是如果你想高效地使用系统的话。'
- en: '**Data management**: Containers are ephemeral by design, meaning that any data
    inside a container will be lost if the container is destroyed. Although there
    are solutions to this problem, it requires more complexity than traditional systems.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据管理**：容器的设计是短暂的，这意味着如果容器被销毁，容器内的任何数据都将丢失。尽管有解决这个问题的方案，但它比传统系统需要更多的复杂性。'
- en: '**Complexity**: Docker makes deploying and managing individual containers easy;
    scaling and orchestrating large numbers of containers across many nodes can become
    complex. Docker’s networking model, while flexible, can be difficult to set up
    and manage, particularly when containers are spread across multiple hosts. In
    addition, complexities increase if there are several containers and associated
    tools. Also, the OS kernel is limited, making debugging and implementing certain
    features more complex.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性**：Docker使得部署和管理单个容器变得容易；在许多节点上跨大量容器进行扩展和编排可能会变得复杂。Docker的网络模型虽然灵活，但设置和管理可能很困难，尤其是当容器分布在多个主机上时。此外，如果有多个容器和相关工具，复杂性会增加。还有，操作系统内核有限，使得调试和实现某些功能变得更加复杂。'
- en: While Docker containers offer many advantages such as portability, efficiency,
    isolation, and scalability, they also come with challenges, especially related
    to security, complexity, and data management.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Docker容器提供了许多优势，如便携性、效率、隔离性和可伸缩性，但它们也带来了挑战，尤其是与安全、复杂性和数据管理相关。
- en: Sometimes, our system can be particularly complex and have more than one container;
    in the following subsection, we will discuss Kubernetes, which allows us to orchestrate
    multiple containers.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们的系统可能特别复杂，并且有多个容器；在以下小节中，我们将讨论Kubernetes，它允许我们编排多个容器。
- en: Kubernetes
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes
- en: '**Kubernetes** is an open source container orchestration platform that automates
    the deployment, scaling, management, and operation of containerized applications.
    It manages and orchestrates containers in production environments.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubernetes**是一个开源的容器编排平台，它自动化了容器化应用程序的部署、扩展、管理和操作。它在生产环境中管理和编排容器。'
- en: In Kubernetes, a Pod is a group of one or more containers that are tied together
    and share resources such as networks and storage. The containers in a Pod are
    always deployed together and share the same environment. A Service is an abstraction
    that defines a logical set of Pods and a policy to access them. Services allow
    us to manage how our Pods are connected internally or are open to the outside
    world (during production deployment). A node, on the other hand, is a physical
    or virtual machine that runs containers in the Kubernetes cluster. Each node in
    the cluster runs at least one kubelet (the agent that runs containers) and a kube-proxy
    (networking proxy for managing communication between containers). A group of nodes
    is called a cluster, and clusters are the backbone of a Kubernetes environment
    that provides resources such as CPU, memory, and storage to applications. Kubernetes
    facilitates the deployment and maintenance of containers, allowing easier scaling
    and production of applications. It also allows us to better manage sensitive data
    configuration and data management in general.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，Pod是一组一个或多个相互关联并共享资源（如网络和存储）的容器。Pod中的容器总是一起部署并共享相同的环境。服务是一种抽象，它定义了一组Pod的逻辑集合以及访问它们的策略。服务允许我们管理Pod如何在内部连接或对外部世界（在生产部署期间）开放。另一方面，节点是一个运行Kubernetes集群中容器的物理或虚拟机。集群中的每个节点至少运行一个kubelet（运行容器的代理）和一个kube-proxy（用于管理容器之间通信的网络代理）。节点的一组被称为集群，集群是提供CPU、内存和存储等资源给应用的Kubernetes环境的骨干。Kubernetes简化了容器的部署和维护，允许更容易地扩展和生成应用。它还允许我们更好地管理敏感数据配置以及一般的数据管理。
- en: Kubernetes is widely used to deploy, manage, and scale microservices-based applications.
    It is also a popular choice in DevOps practices due to its ability to automate
    deployments and scale applications.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes被广泛用于部署、管理和扩展基于微服务的应用。它也是DevOps实践中的一种流行选择，因为它能够自动化部署并扩展应用。
- en: Docker with ML
  id: totrans-525
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Docker与机器学习
- en: Over the years, Docker has been extensively used with ML models, both for running
    models and for ML-based creations. It allows you to set up a workspace that is
    ready to code, where all the dependencies needed are managed so that the process
    of using a model is expedited. Docker also allows for improved reproducibility
    of models, both for training and inference.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，Docker在机器学习模型方面得到了广泛的应用，无论是用于运行模型还是用于基于机器学习的创作。它允许你设置一个准备好编码的工作空间，其中所有需要的依赖项都得到管理，从而加快了使用模型的过程。Docker还允许提高模型的复现性，无论是训练还是推理。
- en: '![Figure 10.52 – Overview of the purposes of using Docker for ML-based software
    projects (https://arxiv.org/pdf/2206.00699)](img/B21257_10_52.jpg)'
  id: totrans-527
  prefs: []
  type: TYPE_IMG
  zh: '![图10.52 – 使用Docker进行基于机器学习的软件项目目的概述 (https://arxiv.org/pdf/2206.00699)](img/B21257_10_52.jpg)'
- en: Figure 10.52 – Overview of the purposes of using Docker for ML-based software
    projects ([https://arxiv.org/pdf/2206.00699](https://arxiv.org/pdf/2206.00699))
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.52 – 使用Docker进行基于机器学习的软件项目目的概述 ([https://arxiv.org/pdf/2206.00699](https://arxiv.org/pdf/2206.00699))
- en: Docker can be used with any ML application, including using LLMs and agents.
    For example, Ollama has its own Docker image available on Docker Hub, thus making
    it easy to create applications with LLMs and be able to directly deploy them to
    a server. Our application can also contain RAG or other components.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: Docker可以与任何机器学习（ML）应用一起使用，包括使用大型语言模型（LLMs）和代理。例如，Ollama在Docker Hub上提供了自己的Docker镜像，这使得创建使用LLMs的应用变得容易，并且可以直接部署到服务器上。我们的应用也可以包含RAG或其他组件。
- en: Also, as Docker containers are now used in various applications and LLMs are
    used to generate code, an LLM can be used to address the challenges of environment
    configuration in software development, particularly when using Docker for containerization.
    In fact, many software repositories require specific dependencies to function
    properly, and setting up the environment correctly is error-prone, time-consuming,
    and difficult for users. Docker allows the process to be more robust and reproducible,
    but Dockerfiles must be configured manually and can be complex when a project
    has many dependencies or when the configuration involves multiple steps that need
    to be executed in a specific order. Therefore, it was proposed to use an LLM to
    act as an intelligent agent that understands the dependencies and requirements
    of a repository and can generate a fully automated configuration that works in
    a Docker container. **Repo2Run** is an approach that leverages an LLM as an agent
    to control the process and ensure that the environment is properly configured
    before being deployed.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于 Docker 容器现在被用于各种应用程序，而大型语言模型（LLM）被用于生成代码，因此 LLM 可以用于解决软件开发中环境配置的挑战，尤其是在使用
    Docker 进行容器化时。事实上，许多软件仓库需要特定的依赖项才能正常运行，正确设置环境可能会出错、耗时且对用户来说困难重重。Docker 允许过程更加稳健和可重复，但
    Dockerfile 必须手动配置，当项目有多个依赖项或配置涉及需要按特定顺序执行的多步操作时，可能会变得复杂。因此，有人提出使用 LLM 作为智能代理，理解仓库的依赖项和需求，并能够生成在
    Docker 容器中工作的完全自动化的配置。**Repo2Run** 是一种利用 LLM 作为代理来控制过程并确保在部署前环境得到正确配置的方法。
- en: Repo2Run automatically generates Dockerfiles, which are used to configure Docker
    containers. Dockerfiles contain a set of instructions for setting up a Docker
    container environment, including installing dependencies and setting up necessary
    configurations. The system inspects a given Python repository, detects its dependencies
    (e.g., from files such as `requirements.txt` or Pipfile), and then formulates
    a Dockerfile to recreate the necessary environment. The core innovation in Repo2Run
    lies in its use of LLMs to drive the configuration process. The LLM intelligently
    understands the structure of the repository and its dependencies, reducing the
    need for manual intervention. It automates steps that are traditionally tedious
    and prone to errors, such as dependency resolution and configuration setup.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: Repo2Run 自动生成 Dockerfile，用于配置 Docker 容器。Dockerfile 包含设置 Docker 容器环境的指令集，包括安装依赖项和设置必要的配置。系统检查给定的
    Python 仓库，检测其依赖项（例如，来自 `requirements.txt` 或 Pipfile 等文件），然后制定一个 Dockerfile 来重新创建必要的环境。Repo2Run
    的核心创新在于其使用 LLM 驱动配置过程。LLM 智能地理解仓库的结构及其依赖项，减少了手动干预的需求。它自动化了传统上繁琐且容易出错的步骤，例如依赖项解析和配置设置。
- en: '![Figure 10.53 – Example process of Repo2Run (https://www.arxiv.org/pdf/2502.13681)](img/B21257_10_53.jpg)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.53 – Repo2Run 的示例过程](img/B21257_10_53.jpg)'
- en: Figure 10.53 – Example process of Repo2Run ([https://www.arxiv.org/pdf/2502.13681](https://www.arxiv.org/pdf/2502.13681))
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.53 – Repo2Run 的示例过程([https://www.arxiv.org/pdf/2502.13681](https://www.arxiv.org/pdf/2502.13681))
- en: Moving Docker containers to Kubernetes requires a set of configuration files
    that describe how applications run within Kubernetes clusters (Kubernetes manifests).
    This migration can be complex, especially for large applications that contain
    several containers and services. Conducting this process can be error-prone, time-consuming,
    and difficult to manage, especially for teams without in-depth Kubernetes expertise.
    Therefore, some works (such as Ueno, 2024) propose to use an LLM to assist in
    this process and generate the manifest.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Docker 容器迁移到 Kubernetes 需要一组配置文件，这些文件描述了应用程序如何在 Kubernetes 集群（Kubernetes 清单）中运行。这种迁移可能很复杂，尤其是对于包含多个容器和服务的应用程序。执行此过程可能会出错，耗时且难以管理，尤其是对于缺乏深入
    Kubernetes 专业知识的小组。因此，一些研究（如 Ueno，2024）建议使用大型语言模型（LLM）来协助此过程并生成清单。
- en: The **LLMSecConfig** framework aims to address a critical problem in the security
    of containerized applications and **container orchestrators** (**COs**) such as
    Kubernetes. CO tools are used to manage the deployment, scaling, and networking
    of containerized applications. However, due to their complexity, many possible
    misconfigurations can expose security vulnerabilities. For instance, misconfigured
    access controls, improper resource limitations, or insecure network policies can
    leave applications open to attacks.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLMSecConfig**框架旨在解决容器化应用程序和**容器编排器**（**COs**）如Kubernetes的安全中的关键问题。CO工具用于管理容器化应用程序的部署、扩展和网络。然而，由于它们的复杂性，许多可能的配置错误可能会暴露安全漏洞。例如，配置错误的访问控制、不恰当的资源限制或不安全的网络策略可能会使应用程序容易受到攻击。'
- en: These misconfigurations are common because the process requires a high level
    of expertise and is manual. **Static analysis tools** (**SATs**) are used to detect
    misconfigurations by analyzing the configuration files of containerized applications,
    such as Kubernetes YAML files or Dockerfiles. Although SATs are a good solution
    for detecting vulnerabilities, they lack automation and require manual effort.
    LLMSecConfig proposes to use RAG and LLMs to find relevant information from external
    sources to identify misconfigurations. The goal then is to make the process automated,
    in which vulnerabilities are identified and fixed at the same time while maintaining
    operational containers.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 这些配置错误很常见，因为该过程需要高水平的专业知识和手动操作。**静态分析工具**（**SATs**）通过分析容器化应用程序的配置文件，如Kubernetes
    YAML文件或Dockerfile，来检测配置错误。尽管SATs是检测漏洞的好方法，但它们缺乏自动化，需要手动操作。LLMSecConfig提出使用RAG和LLM从外部来源查找相关信息以识别配置错误。目标是将过程自动化，在这个过程中，同时识别和修复漏洞，同时保持操作容器的正常运行。
- en: '![Figure 10.54 – Architecture overview of the LLMSecConfig framework for automated
    Kubernetes security configuration (https://arxiv.org/pdf/2502.02009)](img/B21257_10_54.jpg)'
  id: totrans-537
  prefs: []
  type: TYPE_IMG
  zh: '![图10.54 – LLMSecConfig框架的自动化Kubernetes安全配置架构概述](https://arxiv.org/pdf/2502.02009)(img/B21257_10_54.jpg)'
- en: Figure 10.54 – Architecture overview of the LLMSecConfig framework for automated
    Kubernetes security configuration ([https://arxiv.org/pdf/2502.02009](https://arxiv.org/pdf/2502.02009))
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.54 – LLMSecConfig框架的自动化Kubernetes安全配置架构概述([https://arxiv.org/pdf/2502.02009](https://arxiv.org/pdf/2502.02009))
- en: These approaches show that not only can Docker be used for LLM applications
    but also, conversely, LLMs can be used to enhance the use of containers, especially
    when the application goes into production.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法表明，不仅Docker可以用于LLM应用，相反，LLM也可以用来增强容器使用，尤其是在应用程序进入生产阶段时。
- en: Summary
  id: totrans-540
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter focused on an important aspect of how we plan a multi-agent system.
    Whatever form our system takes, it must eventually go into production and be used
    by users. The experience for users is pivotal to whatever project we have in mind.
    That is why we started by using Streamlit, a framework that allows us to experiment
    quickly and get an initial proof of concept. Being able to get a prototype of
    our system allows us to understand both strengths and weaknesses before investing
    large resources in scaling. The advantage of Streamlit is that it allows us to
    analyze both the backend and the frontend, enabling us to interact with an application
    as if we were one of the users. Streamlit allows us to test what a complete product
    may look like before we conduct scaling and system optimization.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍了我们规划多代理系统的一个重要方面。无论我们的系统采取何种形式，它最终都必须进入生产阶段并供用户使用。用户的使用体验对我们心中的任何项目都至关重要。这就是为什么我们首先使用Streamlit，这是一个允许我们快速实验并获得初步概念验证的框架。能够获得我们系统的原型使我们能够在投入大量资源进行扩展之前，了解其优势和劣势。Streamlit的优势在于它允许我们分析后端和前端，使我们能够像用户一样与应用程序互动。Streamlit允许我们在进行扩展和系统优化之前测试一个完整产品可能的样子。
- en: Obviously, an application will then have to pass this prototype stage to enter
    production. This step requires that we conduct scaling of our application. LLMs
    are complex products that need a lot of resources, so during the second half of
    the chapter, we dealt with all those operations that enable the training and what
    happens afterward. Although we kept a main focus on LLMs, everything we saw can
    be useful for any ML application.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，一个应用必须通过这个原型阶段才能进入生产。这一步要求我们对我们应用进行扩展。LLMs是复杂的产物，需要大量资源，因此在本章的后半部分，我们处理了所有那些使训练成为可能以及之后发生的事情的操作。尽管我们主要关注LLMs，但我们看到的一切都可以用于任何ML应用。
- en: In the next and final chapter of the book, we will discuss the perspectives
    of a field that is constantly evolving. We will discuss some important open questions
    and some of the future opportunities and developments that the exciting field
    of agents holds for us.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的下一章和最后一章中，我们将讨论一个不断发展的领域的视角。我们将讨论一些重要的开放性问题，以及这个令人兴奋的代理领域为我们带来的未来机会和发展。
- en: Further reading
  id: totrans-544
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Hewage, *Machine Learning Operations: A Survey on MLOps Tool Support*, 2022,
    [https://arxiv.org/abs/2202.10169](https://arxiv.org/abs/2202.10169)'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赫瓦格，**机器学习操作：关于MLOps工具支持的综述**，2022，[https://arxiv.org/abs/2202.10169](https://arxiv.org/abs/2202.10169)
- en: 'Park, *LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to
    Small-Scale Local LLMs*, 2024, [https://arxiv.org/abs/2408.13467](https://arxiv.org/abs/2408.13467)'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 帕克，**LlamaDuo：无缝迁移从服务LLMs到小型本地LLMs的LLMOps管道**，2024，[https://arxiv.org/abs/2408.13467](https://arxiv.org/abs/2408.13467)
- en: Zhao, *A Survey of Large Language Models*, 2023, [https://arxiv.org/abs/2303.18223](https://arxiv.org/abs/2303.18223)
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵一，**大型语言模型综述**，2023，[https://arxiv.org/abs/2303.18223](https://arxiv.org/abs/2303.18223)
- en: Chang, *A Survey on Evaluation of Large Language Models*, 2023, [https://arxiv.org/abs/2307.03109](https://arxiv.org/abs/2307.03109)
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张，**关于大型语言模型评估的综述**，2023，[https://arxiv.org/abs/2307.03109](https://arxiv.org/abs/2307.03109)
- en: 'IBM, *LLM evaluation: Why* *Testing AI Models* *Matters*, [https://www.ibm.com/think/insights/llm-evaluation](https://www.ibm.com/think/insights/llm-evaluation)'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM，**LLM评估：为什么** **测试AI模型** **很重要**，[https://www.ibm.com/think/insights/llm-evaluation](https://www.ibm.com/think/insights/llm-evaluation)
- en: 'Guo, *Evaluating Large Language Models: A Comprehensive Survey*, 2023, [https://arxiv.org/abs/2310.19736](https://arxiv.org/abs/2310.19736)'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭一鸣，**评估大型语言模型：全面综述**，2023，[https://arxiv.org/abs/2310.19736](https://arxiv.org/abs/2310.19736)
- en: 'Shi, *Keep the Cost Down: A Review on Methods to Optimize LLM’s KV-Cache Consumption*,
    2024, [https://arxiv.org/abs/2407.18003](https://arxiv.org/abs/2407.18003)'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 石，**降低成本：关于优化LLM的KV-Cache消耗方法的综述**，2024，[https://arxiv.org/abs/2407.18003](https://arxiv.org/abs/2407.18003)
- en: Li, *A Survey on Large Language Model Acceleration based on KV Cache Management*,
    2024, [https://arxiv.org/abs/2412.19442](https://arxiv.org/abs/2412.19442)
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李，**基于KV缓存管理的大型语言模型加速综述**，2024，[https://arxiv.org/abs/2412.19442](https://arxiv.org/abs/2412.19442)
- en: Zhou, *A Survey on Efficient Inference for Large Language Models*, 2024, [https://arxiv.org/abs/2404.14294](https://arxiv.org/abs/2404.14294)
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周一，**关于大型语言模型高效推理的综述**，2024，[https://arxiv.org/abs/2404.14294](https://arxiv.org/abs/2404.14294)
- en: Leviathan, *Looking* *Back at Speculative Decoding,* 2024, [https://research.google/blog/looking-back-at-speculative-decoding/](https://research.google/blog/looking-back-at-speculative-decoding/)
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leviathan，**回顾** **投机解码**，2024，[https://research.google/blog/looking-back-at-speculative-decoding/](https://research.google/blog/looking-back-at-speculative-decoding/)
- en: Determined AI, *Tensor Parallelism in Three Levels of* *Difficulty*, [https://www.determined.ai/blog/tp](https://www.determined.ai/blog/tp)
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定性AI，**三个难度级别的** **张量并行**，[https://www.determined.ai/blog/tp](https://www.determined.ai/blog/tp)
- en: Geeksforgeeks, *asyncio in* *Python*, [https://www.geeksforgeeks.org/asyncio-in-python/](https://www.geeksforgeeks.org/asyncio-in-python/)
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geeksforgeeks，**Python中的** **asyncio**，[https://www.geeksforgeeks.org/asyncio-in-python/](https://www.geeksforgeeks.org/asyncio-in-python/)
- en: Gim, *Asynchronous LLM Function Calling*, 2024, [https://arxiv.org/abs/2412.07017](https://arxiv.org/abs/2412.07017)
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吉姆，**异步LLM函数调用**，2024，[https://arxiv.org/abs/2412.07017](https://arxiv.org/abs/2412.07017)
- en: '*Asynchronous* *Computation*, [https://d2l.ai/chapter_computational-performance/async-computation.html](https://d2l.ai/chapter_computational-performance/async-computation.html)'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异步** **计算**，[https://d2l.ai/chapter_computational-performance/async-computation.html](https://d2l.ai/chapter_computational-performance/async-computation.html)'
- en: Openja, *Studying the Practices of Deploying Machine Learning Projects on Docker*,
    2022, [https://arxiv.org/abs/2206.00699](https://arxiv.org/abs/2206.00699)
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Openja，**研究在Docker上部署机器学习项目的实践**，2022，[https://arxiv.org/abs/2206.00699](https://arxiv.org/abs/2206.00699)
- en: 'Muzumdar, *Navigating the Docker Ecosystem: A Comprehensive Taxonomy and Survey*,
    2024, [https://arxiv.org/abs/2403.17940](https://arxiv.org/abs/2403.17940)'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muzumdar，*导航 Docker 生态系统：一个全面的分类和调查*，2024，[https://arxiv.org/abs/2403.17940](https://arxiv.org/abs/2403.17940)
- en: Saha, *Evaluation of Docker Containers for Scientific Workloads in the Cloud*,
    2019, [https://arxiv.org/abs/1905.08415](https://arxiv.org/abs/1905.08415)
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saha，*评估云中科学工作负载的 Docker 容器*，2019，[https://arxiv.org/abs/1905.08415](https://arxiv.org/abs/1905.08415)
- en: Ru, *An LLM-based Agent for Reliable Docker Environment Configuration*, 2025,
    [https://www.arxiv.org/abs/2502.13681](https://www.arxiv.org/abs/2502.13681)
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鲁某，*基于语言学习的可靠 Docker 环境配置代理*，2025，[https://www.arxiv.org/abs/2502.13681](https://www.arxiv.org/abs/2502.13681)
- en: Ueno, *Migrating Existing Container Workload to Kubernetes -- LLM Based Approach
    and Evaluation*, 2024, [https://arxiv.org/abs/2408.11428v1](https://arxiv.org/abs/2408.11428v1)
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上野，*将现有容器工作负载迁移到 Kubernetes —— 基于语言学习的迁移方法与评估*，2024，[https://arxiv.org/abs/2408.11428v1](https://arxiv.org/abs/2408.11428v1)
- en: 'Ye, *LLMSecConfig: An LLM-Based Approach for Fixing Software Container Misconfigurations*,
    2025, [https://arxiv.org/abs/2502.02009](https://arxiv.org/abs/2502.02009)'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 叶，*LLMSecConfig：一种基于语言学习的修复软件容器配置错误的方案*，2025，[https://arxiv.org/abs/2502.02009](https://arxiv.org/abs/2502.02009)
- en: 'Docker, *LLM Everywhere: Docker for Local and Hugging Face* *Hosting*, [https://www.docker.com/blog/llm-docker-for-local-and-hugging-face-hosting/](https://www.docker.com/blog/llm-docker-for-local-and-hugging-face-hosting/)'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker，*LLM 到处都是：Docker 用于本地和 Hugging Face 存储*，[https://www.docker.com/blog/llm-docker-for-local-and-hugging-face-hosting/](https://www.docker.com/blog/llm-docker-for-local-and-hugging-face-hosting/)
