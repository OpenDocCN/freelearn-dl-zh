- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Building Sustainable Enterprise-Grade AI Platforms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建可持续的企业级AI平台
- en: The primary objective of this chapter is to inform you about sustainability
    best practices, along with model governance techniques, to align them with your
    organizational goals and initiatives. You will be made aware of the environmental
    consequences of training **Deep Learning (DL)** models and possible remediating
    actions that you could take. You will also become accustomed to different metrics
    of sustainability for different platforms, which will lead to sustainable model
    training and deployment.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要目标是向你介绍可持续性最佳实践以及模型治理技术，以便将其与组织目标和计划对齐。你将了解到训练**深度学习（DL）**模型的环境影响以及你可以采取的可能补救措施。你还将熟悉不同平台的可持续性指标，这将有助于实现可持续的模型训练和部署。
- en: By the end of this chapter, you will have been equipped with the collaborative
    and decentralized learning techniques involved in **Federated Learning (FL)**,
    whether model training happens on the network, at the edge, or in the cloud. Finally,
    you will also be aware of the tools that can help to track carbon emission statistics.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将掌握**联邦学习（FL）**中的协作和去中心化学习技术，无论是模型训练发生在网络、边缘还是云端。最后，你还将了解帮助跟踪碳排放统计数据的工具。
- en: 'In this chapter, these topics will be covered in the following sections:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下几个主题：
- en: The key to sustainable enterprise-grade AI platforms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可持续企业级AI平台的关键
- en: Sustainability practices and metrics across different cloud platforms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨不同云平台的可持续实践和指标
- en: Carbon emission trackers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 碳排放追踪器
- en: Adopting sustainable model training and deployment with FL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用FL进行可持续模型训练和部署
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter requires you to have Python 3.8, along with some Python packages,
    listed as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求你安装Python 3.8，并包含以下一些Python包：
- en: Keras 2.7.0 and TensorFlow 2.7.0
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 2.7.0和TensorFlow 2.7.0
- en: '`pip` `install CodeCarbon`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pip` `install CodeCarbon`'
- en: The key to sustainable enterprise-grade AI platforms
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可持续企业级AI平台的关键
- en: Sustainability has a very important role to play in the era of ethical AI, as
    the ability to predict energy emissions can support initiatives to protect the
    environment and conserve resources. AI-enabled platforms can facilitate emission
    reductions and carbon dioxide (CO2) removal, which can foster greener transportation
    networks, as well as monitor and control deforestation. Thus, by effectively using
    AI solutions in a sustainable manner, we can try to prevent extreme weather conditions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可持续性在伦理AI时代中扮演着非常重要的角色，因为预测能源排放的能力可以支持保护环境和节约资源的倡议。AI平台可以促进排放减少和二氧化碳（CO2）去除，这有助于建立更绿色的交通网络，并监测与控制森林砍伐。因此，通过以可持续的方式有效利用AI解决方案，我们可以努力预防极端天气情况。
- en: As a first step, we should understand why organizations have set up a vision
    to go carbon free. Along with this, it is equally important to understand the
    leadership vision to properly align teams with the sustainable mission goals set
    forth by their senior leadership and CXOs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们应当理解为什么组织会设定无碳愿景。同时，理解领导层的愿景同样重要，以便将团队与高层领导和CXO们设定的可持续使命目标对齐。
- en: Hence, as the first step, we will look at the motivation of organizations to
    restructure their roadmaps toward building sustainable AI solutions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作为第一步，我们将探讨组织重构其路线图以构建可持续AI解决方案的动机。
- en: Sustainable solutions with AI as an organizational roadmap
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将AI作为组织路线图的可持续解决方案
- en: We should be extremely careful to build solutions that are environmentally friendly.
    Substantial research indicates that just 40 days of research training can emit
    96 tons of CO2\. The amount is so large that it can be compared to 1,000 hours
    of air travel, roughly equivalent to the carbon footprint of 23 American homes
    ([https://www.analyticsinsight.net/new-mit-neural-network-architecture-may-reduce-carbon-footprint-ai/](https://www.analyticsinsight.net/new-mit-neural-network-architecture-may-reduce-carbon-footprint-ai/),
    [https://inhabitat.com/mit-moves-toward-greener-more-sustainable-artificial-intelligence/](https://inhabitat.com/mit-moves-toward-greener-more-sustainable-artificial-intelligence/)).
    The carbon emissions involved in running a neural network search have been as
    high as 600,000 CO2e (lbs), equivalent to the lifetime of 5 cars. Training algorithms
    such as large **Natural Language Processing (NLP)** models (or transformers) generate
    so much energy that data scientists need to be careful when designing the architectures
    of this kind of DL neural network ([https://aclanthology.org/P19-1355.pdf](https://aclanthology.org/P19-1355.pdf)).
    This kind of training can be threatening to the environment. If we continue to
    build these AI solutions in this way, future generations will face adverse environmental
    consequences.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应非常小心地构建环保的解决方案。大量研究表明，仅仅40天的研究训练就能排放96吨CO2。这一数值非常庞大，相当于1,000小时的航空旅行，约等于23个美国家庭的碳足迹（[https://www.analyticsinsight.net/new-mit-neural-network-architecture-may-reduce-carbon-footprint-ai/](https://www.analyticsinsight.net/new-mit-neural-network-architecture-may-reduce-carbon-footprint-ai/)，[https://inhabitat.com/mit-moves-toward-greener-more-sustainable-artificial-intelligence/](https://inhabitat.com/mit-moves-toward-greener-more-sustainable-artificial-intelligence/))。进行神经网络搜索的碳排放量曾高达600,000
    CO2e（磅），相当于5辆汽车的生命周期。训练像**自然语言处理（NLP）**模型（或transformer）这样的算法会消耗大量能量，因此数据科学家在设计此类DL神经网络架构时需要格外小心（[https://aclanthology.org/P19-1355.pdf](https://aclanthology.org/P19-1355.pdf)）。这种训练方式对环境可能构成威胁。如果我们继续以这种方式构建AI解决方案，未来的世代将面临不利的环境后果。
- en: Organizations that want to apply AI to sustainability should not only focus
    on sustainable banking, energy consumption, or healthcare but also develop best
    practices to quantify the CO2e by measuring carbon footprints and the computational
    power required to train and evaluate the methods for managing data centers efficiently.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 希望将AI应用于可持续发展的组织，不仅应关注可持续的银行业务、能源消费或医疗保健，还应开发最佳实践，通过衡量碳足迹和计算训练及评估数据中心管理方法所需的计算能力来量化CO2e。
- en: In addition, data scientists and engineers should first evaluate the need for
    DL models before choosing them. For certain use cases, similar performance can
    be guaranteed with a standard model without needing to train a DL model. So, correct
    practices and audits need to be established in an organization before training
    DL models, to avoid a large carbon footprint.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据科学家和工程师应首先评估是否需要使用DL模型。对于某些应用场景，使用标准模型即可保证相似的性能，而无需训练DL模型。因此，在训练DL模型之前，组织需要建立正确的实践和审核机制，以避免造成过大的碳足迹。
- en: Once the data science team finalizes the model (traditional versus DL in either
    centralized or FL setups), the selection of the number of hyperparameters and
    careful tuning play important roles in carbon emissions. Tuning hyperparameters
    for FL can become expensive in terms of energy consumed, as this leads to tuning
    hundreds of different models (with local models in the clients).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据科学团队最终确定了模型（无论是在集中式还是FL设置中使用传统模型还是DL模型），超参数的选择和精确调优在碳排放中起着重要作用。对于FL来说，调优超参数可能在能耗方面变得非常昂贵，因为这会导致调优数百个不同的模型（包括客户端中的本地模型）。
- en: The tuning process in FL may become increasingly complex due to parameterization
    in the aggregation strategy, as well as heterogeneity in the individual datasets
    of the clients. The complexity of hyperparameter tuning in both types of learning
    must be carefully designed to minimize CO2e release.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: FL中的调优过程可能会变得越来越复杂，这主要是由于聚合策略中的参数化以及客户端各个数据集的异质性。两种学习方式中超参数调优的复杂性必须仔细设计，以尽量减少CO2e排放。
- en: The team should be aware of the choice of the dataset and the extent of preprocessing
    and feature engineering involved. One such example is when feature engineering
    techniques become more exhaustive and expensive in developing recommender systems
    using financial and retail data rather than using only retail data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 团队应该注意数据集的选择以及所涉及的预处理和特征工程的程度。一个例子是，使用金融和零售数据来开发推荐系统时，特征工程技术变得更加详尽和昂贵，而不仅仅是使用零售数据。
- en: Let us study the organizational standards that can enable the building of such
    frameworks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们研究能够支持这些框架构建的组织标准。
- en: Organizational standards for sustainable frameworks
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可持续框架的组织标准
- en: CXOs and leadership must also be involved in mandating sustainable data and
    model practices as organizational objectives. These sustainable practices for
    scalable cloud platforms originate from setting the right objectives and **Service-Level
    Agreements** (**SLAs**) based on the current business requirements. This may require
    selecting the right trade-off to affect sustainability metrics by prioritizing
    business-critical functions and allowing lower service levels for non-critical
    functions. Critically, businesses should handle siloed data across departments
    by defining architectural design patterns that are suitable for sustainable model
    training and deployment.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 高管和领导层必须参与将可持续数据和模型实践作为组织目标的强制性要求。这些可伸缩云平台的可持续实践源于设定正确的目标和基于当前业务需求的服务级别协议（SLAs）。这可能需要通过优先考虑业务关键功能并允许非关键功能的较低服务级别来影响可持续性指标，来选择正确的权衡。至关重要的是，企业应通过定义适用于可持续模型训练和部署的架构设计模式来处理跨部门的孤立数据。
- en: Now, let us begin looking into the metrics measured across different cloud platforms.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始研究在不同云平台上测量的指标。
- en: Sustainability practices and metrics across different cloud platforms
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同云平台的可持续实践和指标
- en: 'In this section, we will explore how we can evaluate the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何评估以下内容：
- en: Useful emission metrics on Google Cloud
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌云的有用排放指标
- en: Best practices and strategies for carbon-free energy
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无碳能源的最佳实践和策略
- en: The energy efficiency of data centers
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中心的能源效率
- en: First of all, we will discuss some of the metrics of Google Cloud and also cite
    some of the initiatives and tools promoted by Microsoft.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将讨论谷歌云的一些指标，并引用微软推广的一些倡议和工具。
- en: Google’s role in sustainable cloud solutions has been overwhelming, and it has
    been the leading cloud provider in terms of purchasing sufficient renewable energy,
    more than any other organization. Starting with a pledge in 2007 to become the
    first major carbon-neutral company by 2017, 100% of its electricity consumption
    now comes from renewable energy. In the absence of enough wind and solar power
    or renewable resources, Google draws power from the local grid to run operations
    at a local data center. Conversely, when sufficient power is available, the excess
    is fed back to the local grid to be used elsewhere. With a vision of 24/7 carbon-free
    energy by 2030, it uses the guidelines mentioned in the following sections to
    make its data centers and electricity grids carbon free.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌在可持续云解决方案中的角色是巨大的，它一直是领先的云服务提供商，在购买足够的可再生能源方面超过任何其他组织。从2007年宣布成为到2017年第一家主要碳中和公司的承诺开始，现在其所有电力消耗都来自可再生能源。在没有足够的风能和太阳能或可再生资源的情况下，谷歌从当地电网获取电力来运行本地数据中心的操作。相反，当有足够的电力可用时，多余的电力会反馈到当地电网用于其他用途。以2030年实现全天候无碳能源的愿景，谷歌使用以下各节提到的指南使其数据中心和电网无碳。
- en: Microsoft also has its own Power BI application for Azure, popularly known as
    the Microsoft Sustainability Calculator, which provides insights into the amount
    of carbon emissions associated with the data centers in each region. Even initiatives
    such as the purchase of carbon-neutral renewable energy help to drive its annual
    cloud consumption.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 微软还为Azure推出了其Power BI应用程序，被称为Microsoft可持续性计算器，该应用程序提供了有关每个区域数据中心碳排放量的见解。即使是购买碳中和的可再生能源等举措也有助于推动其年度云消费。
- en: Emission metrics on Google Cloud
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谷歌云的排放指标
- en: 'To move to carbon-free energy by 2030, Google Cloud has come up with an initiative
    to empower its customers to leverage carbon-free energy 24/7\. Customers can consider
    the carbon impact when designing their solutions by analyzing a metric called
    **CFE%**. This metric quantifies the amount of energy consumed every hour that
    is carbon free by broadly dividing it into two main categories: CFE% and carbon
    intensity. CFE% is calculated based on the generated energy that feeds the grid
    at any time, along with the clean energy attributions (through the supply of renewable
    energy resources) made available by Google that are applied to the grid:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 2030 年实现碳自由能源，Google Cloud 提出了一个倡议，帮助客户实现 24/7 利用碳自由能源。客户可以在设计解决方案时考虑碳影响，分析一种叫做
    **CFE%** 的指标。该指标量化了每小时消耗的碳自由能源，通过将其大致分为两大类：CFE% 和碳强度。CFE% 的计算是基于任何时候向电网供应的能源以及
    Google 提供的可再生能源资源供给所做的碳自由能源贡献。
- en: '**Google CFE%**: This provides a measure of the average percentage of carbon-free
    energy. This measure, when computed per location on an hourly basis, provides
    customers with an estimate of the amount of time that their apps can run on carbon-free
    energy, depending on the investments made into carbon-free energy that apply to
    the grid at the same location.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google CFE%**：该指标提供了碳自由能源的平均百分比。按小时计算时，这一指标为客户提供了一个估算，表明他们的应用程序能够在多长时间内使用碳自由能源，这取决于该地区电网投资的碳自由能源比例。'
- en: '**Grid carbon intensity (gCO**2**eq/kWh)**: This metric quantifies the average
    life cycle of gross emissions per unit of energy from a grid at a specific location.
    With it, we can compare the carbon intensity of the electricity at different locations
    within a local grid that behave similarly. Using this, we can select the region
    of deployment of production apps when two or more locations demonstrate a similar
    CFE%. To cite an example, we will often see that Frankfurt and the Netherlands
    exhibit similar CFE% scores, while the Netherlands experiences higher rates of
    emissions.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电网碳强度（gCO**2**eq/kWh）**：该指标量化了在特定位置从电网获取的单位能源的平均生命周期排放。通过这个指标，我们可以比较在同一电网中表现相似的不同位置的电力碳强度。利用这一点，当两个或更多地点的
    CFE% 相似时，我们可以选择一个生产应用的部署区域。例如，我们经常会看到法兰克福和荷兰的 CFE% 得分相似，但荷兰的排放率较高。'
- en: Now, let's discuss the best practices involved in utilizing carbon-free energy.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，来讨论一下如何利用碳自由能源的最佳实践。
- en: Best practices and strategies for carbon-free energy
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 碳自由能源的最佳实践和策略
- en: 'As Google Cloud Platform actively concentrates on increasing the CFE% for each
    of the Google Cloud regions, a higher percentage of carbon-free energy increases
    the sustainability of deployments. Some of the unique propositions for cloud AI
    specialists and architects are as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Google Cloud Platform 积极专注于提升每个 Google Cloud 区域的 CFE%（碳自由能源比例），碳自由能源的比例增加有助于提升部署的可持续性。以下是针对云
    AI 专家和架构师的一些独特建议：
- en: '**Selecting a lower-carbon region**: Building and running new applications
    in low-carbon regions (such as Finland or Sweden) with a higher CFE% helps to
    achieve the target carbon-free energy emissions.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择低碳区域**：在低碳区域（如芬兰或瑞典）建立并运行新应用程序，且该区域具有较高的 CFE%，有助于实现目标碳自由能源排放。'
- en: '**Running batch jobs in a low-carbon region**: Running batch workloads through
    careful planning can help to maximize the use of carbon-free energy. This means
    we don''t need to run low-to-medium workload jobs separately and ensures high
    CPU utilization by combining jobs that have high (as well as low) workloads.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在低碳区域运行批处理任务**：通过精心规划运行批处理工作负载，可以最大限度地利用碳自由能源。这意味着我们无需将低至中等负载的任务分开运行，通过将高负载和低负载的任务结合起来，可以确保高
    CPU 利用率。'
- en: '**Driving organizational policies for greener cloud applications**: Leadership
    teams and organizations should set priorities to allow the usage of resources
    and services in certain regions while restricting access and usage in other regions.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推动绿色云应用的组织政策**：领导团队和组织应该设定优先级，允许在特定区域使用资源和服务，同时限制在其他区域的访问和使用。'
- en: '**Efficient use of services**: Effectively strategizing the VM sizing, along
    with the use of serverless products such as Cloud Run and Cloud Functions, can
    further reduce carbon emissions, as they auto-scale based on workload and conserve
    energy as much as possible.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高效使用服务**：有效地规划虚拟机大小，并结合使用无服务器产品如 Cloud Run 和 Cloud Functions，可以进一步减少碳排放，因为它们会根据工作负载自动扩展，并尽可能节省能源。'
- en: The energy efficiency of data centers
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据中心的能源效率
- en: To run highly efficient data centers, we need to carefully plan and select the
    placement of VMs in a multisite cloud. In addition to selecting the right data
    center for hosting a VM, VM sizing is equally important, as it prevents the wastage
    of additional computational resources and improves the execution speed of an application.
    We can increase a data center’s efficiency by allowing parallelization and employing
    the right techniques to consolidate the cloud resources to reduce energy consumption.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行高效的数据中心，我们需要精心规划并选择虚拟机在多站点云中的部署位置。除了选择合适的数据中心来托管虚拟机外，虚拟机的大小同样重要，因为它可以防止额外计算资源的浪费，并提高应用程序的执行速度。通过允许并行化并采用合适的技术来合并云资源，我们可以提高数据中心的效率，从而减少能源消耗。
- en: '**Physical Machines** (**PMs**) constitute 35% of the total energy consumption
    of data centers. The energy consumed by them can be broken into **static** (the
    energy consumed when the VM is idle) and **dynamic** parts. While the dynamic
    power consumed primarily depends on the utilization of each component and constitutes
    more than 50% of the maximum power consumed, the idle power consumed remains lower
    than 50% of the maximum power consumed. In addition, a nominal amount of power
    is also consumed in sleep mode. We need the right balance between the amount of
    idle and dynamic power consumed to achieve high utilization workloads so that
    PMs contribute to a high energy efficiency ratio by maintaining energy consumption
    proportional to the utilization load. We need to consider optimization to reduce
    energy consumption so that idle power consumption is at 0 W (when PMs are not
    in use), and at other times, it corresponds with an increasing workload until
    the maximum utilization load is achieved. Virtualized resources have the flexibility
    to consolidate user tasks into fewer PMs. We have to also carefully decide how
    we reduce the usage of PMs so that the idle power and, consequently, total consumption
    are reduced. Besides PMs, while determining the overall efficiency of a data center,
    some of the metrics of interest are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**物理机器**（**PMs**）占数据中心总能源消耗的 35%。它们消耗的能量可以分为**静态**（虚拟机空闲时消耗的能量）和**动态**两部分。动态功率消耗主要依赖于各个组件的使用情况，占最大功率消耗的
    50%以上，而空闲功率消耗则保持在最大功率消耗的 50% 以下。此外，睡眠模式下也会消耗少量电力。我们需要在空闲和动态功率消耗之间找到合适的平衡，以实现高利用率的工作负载，从而使
    PMs 通过保持与利用负载成比例的能耗，贡献于高能源效率比。我们需要考虑优化以减少能耗，确保空闲功率消耗为 0 W（当 PMs 未使用时），在其他时候，根据工作负载的增加来相应调整，直到达到最大利用负载。虚拟化资源具有将用户任务合并到较少
    PMs 中的灵活性。我们还必须仔细决定如何减少 PMs 的使用，以减少空闲功率，从而降低整体消耗。除了 PMs，在确定数据中心的整体效率时，以下是一些需要关注的指标：'
- en: The **Power Usage Effectiveness** (**PUE**) metric is a variable factor dependent
    on the cloud provider that signifies the energy efficiency of a data center. It
    estimates the overhead per computation cycle, which is obtained by evaluating
    the fraction of the total amount of power entering a data center required to make
    IT equipment fully functional. A highly efficient data center would have an ideal
    value close to 1.0.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**功率使用效率**（**PUE**）指标是一个依赖于云服务提供商的可变因素，表示数据中心的能源效率。它通过评估进入数据中心的总电力中，用于使 IT
    设备完全正常运行的部分，来估算每个计算周期的开销。一个高效的数据中心将具有接近 1.0 的理想值。'
- en: The **Total Power Usage Effectiveness** (**TUE**) metric serves as an offset
    to reduce the gaps created by the PUE metric. The TUE is obtained by taking the
    ratio of the total energy consumed by a data center to the energy consumed by
    the individual compute elements. It is thus more helpful for assessing environmental
    impact as it only considers the energy consumed for executed workloads instead
    of the whole IT facility.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总功率使用效率**（**TUE**）指标用于弥补由PUE指标产生的差距。TUE通过计算数据中心消耗的总能量与单个计算元素消耗的能量之比来获得。因此，它在评估环境影响时更有帮助，因为它只考虑执行工作负载所消耗的能量，而不是整个IT设施的能耗。'
- en: The **Green Energy Coefficient** (**GEC**) metric gives an estimate as a percentage
    of the energy consumed that is obtained from green energy sources, some of which
    are dependent on the weather, such as solar panels and wind turbines. Renewable-energy-driven
    data centers have a lower detrimental impact on the environment and are hence
    more desirable.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**绿色能源系数**（**GEC**）指标通过百分比估算来自绿色能源来源的能量，其中一些来源受天气条件影响，如太阳能电池板和风力发电机。使用可再生能源的驱动数据中心对环境的负面影响较小，因此更具吸引力。'
- en: 'Ensuring a data center’s efficiency also requires following some of the best
    practices or techniques related to how we structure our programs to run on the
    cloud, and how we measure power consumption. It is worthwhile following some of
    the best practices mentioned here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 确保数据中心的高效性还需要遵循一些与我们如何构建程序以在云端运行，以及如何衡量电力消耗相关的最佳实践或技术。以下是值得遵循的一些最佳实践：
- en: DevOps and cloud architects need not take into consideration the startup costs
    of the VMs as it is negligible.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DevOps和云架构师无需考虑虚拟机的启动成本，因为这几乎可以忽略不计。
- en: Developers should ensure that all programs exploit all the cores available in
    the VMs automatically.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发人员应确保所有程序自动利用虚拟机中可用的所有核心。
- en: Specifying the compute resources is highly recommended at each step of the workflow
    to provision the creation of a VM. The best way to achieve this is to ensure sufficient
    disk space and memory availability to support the execution of the program. Allow
    applications to start processing whenever the available resources are ready to
    execute the tasks.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在工作流的每个步骤中，强烈建议指定计算资源，以便为创建虚拟机（VM）提供支持。实现这一目标的最佳方法是确保有足够的磁盘空间和内存可用，以支持程序的执行。当可用资源准备好执行任务时，允许应用程序开始处理。
- en: Allow submitted applications to execute all the steps of the task until the
    end.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许提交的应用程序执行任务的所有步骤，直到任务结束。
- en: Enable **Dynamic Voltage and Frequency Scaling** (**DVFS**) for PMs that host
    VMs to reduce power consumption by dynamically adjusting the voltage and frequency
    of the CPU.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用**动态电压和频率调整**（**DVFS**）以减少PM（物理机）上虚拟机的功耗，通过动态调整CPU的电压和频率来实现。
- en: Forbid the over-commitment of resources on PMs so that they do not disturb the
    execution speed of resource-intensive applications.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 禁止在PM上过度分配资源，以免影响资源密集型应用程序的执行速度。
- en: Now let us estimate how we can track the amount of carbon emitted.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们估算如何追踪碳排放量。
- en: Carbon emission trackers
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 碳排放跟踪器
- en: With a thorough understanding of the carbon metrics on Google Cloud, our next
    lesson will focus on the computation mechanisms for the energy utilization of
    individual VMs, which we have illustrated with an example on GitHub. Our next
    step is to account for the carbon emissions by embedding carbon trackers, which
    can give us a detailed analysis of the emission statistics from within our source
    code.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深入了解Google Cloud上的碳度量，我们的下一课将集中在单个虚拟机（VM）能效计算机制上，我们已经通过GitHub上的示例进行了说明。我们的下一步是通过嵌入碳排放跟踪器来计算碳排放量，碳排放跟踪器可以提供来自源代码中的详细排放统计分析。
- en: Now, let us explore a few emission tools that can be used for FL and centralized
    learning.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索一些可以用于联邦学习（FL）和集中学习的排放工具。
- en: The FL carbon calculator
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FL碳排放计算器
- en: 'The amount of carbon emitted during FL from a pool of devices can be tracked
    using a kind of FL carbon calculator ([https://mlsys.cst.cam.ac.uk/carbon_fl/](https://mlsys.cst.cam.ac.uk/carbon_fl/),
    [https://github.com/mlco2/codecarbon](https://github.com/mlco2/codecarbon)), where
    the following parameters need to be specified:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在一组设备进行联邦学习（FL）时，碳排放量可以通过一种FL碳排放计算器来追踪（[https://mlsys.cst.cam.ac.uk/carbon_fl/](https://mlsys.cst.cam.ac.uk/carbon_fl/)，[https://github.com/mlco2/codecarbon](https://github.com/mlco2/codecarbon)），其中需要指定以下参数：
- en: '**Devices**: The type of hardware being used by the devices.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备**：设备使用的硬件类型。'
- en: '**Country**: The level of energy production resulting from the burning of fossil
    fuels is dependent on a country. The first parameter helps to access the electricity/carbon
    conversion rate for the country while the second parameter is needed to estimate
    the amount of carbon emissions resulting from communications between the client
    and the server.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**国家**: 由燃烧化石燃料产生的能源生产量取决于国家。第一个参数用于获取该国的电力/碳转化率，第二个参数用于估算客户端与服务器之间通信所产生的碳排放量。'
- en: '**Dataset**: This helps to specify balanced, non-**Independent Identical Distribution**
    (**IID**) datasets such as ImageNet and CIFAR-10\. Unlike IID datasets, non-IID
    datasets have random variables that are not mutually independent on each other,
    nor are they identically distributed. This makes them quite different in that
    they do not seem to come from the same distribution.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**: 用于指定平衡的、非**独立同分布** (**IID**) 数据集，如 ImageNet 和 CIFAR-10。与 IID 数据集不同，非
    IID 数据集包含随机变量，这些变量之间没有相互独立性，也不是同分布的。这使得它们与 IID 数据集有很大不同，因为它们似乎并非来自同一分布。'
- en: '**The number of rounds**: This is used to specify the total number of iterations
    used to build the global model by the central server through the aggregation process.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轮次数**: 用于指定中央服务器通过聚合过程构建全局模型所使用的总迭代次数。'
- en: '**The number of Local Epochs** (**LEs**): This is used to specify the iterations
    at each client end to train their local model before sending them to the server.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地纪元数** (**LEs**): 用于指定每个客户端在将本地模型发送到服务器之前进行训练的迭代次数。'
- en: '**The number of active devices**: This is used to specify the count of active
    client devices in each round, which is usually a fraction of the total devices
    used.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活跃设备数量**: 用于指定每一轮中活跃客户端设备的数量，这通常是所使用的设备总数的一部分。'
- en: '**Network**: This helps to define the internet upload/download speeds for a
    given set of devices.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络**: 用于定义给定设备集的互联网上传/下载速度。'
- en: '*Figure 12**.1* demonstrates the amount of carbon emitted by 5 clients overall
    at the rate of 2.51 gCO2eq, with 10 LEs and 10 rounds in an FL setup. Here, CO2eq
    is responsible for measuring the energy utilization from hardware such as the
    CPU and GPU, and determining where the hardware is located. This, along with the
    region’s average CO2 emission (measured in gCO2eq/KWh) and the ML model’s architecture,
    helps finalize the region of the deployment. For example, deployments with high
    energy requirements can choose to select regions such as Quebec, Canada, but in
    certain other situations, they may be driven to deploy their solutions in Iowa,
    US, which has CO2 emissions as high as 735.6 gCO2eq/KWh:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12.1* 演示了在一个具有 5 个客户端、10 个本地纪元和 10 轮训练的 FL 设置中，总共排放的碳量为 2.51 gCO2eq。这里，CO2eq
    用于衡量硬件（如 CPU 和 GPU）的能耗，并确定硬件的位置。结合该地区的平均 CO2 排放（以 gCO2eq/KWh 为单位）和 ML 模型架构，可以帮助确定部署的地区。例如，对于需要高能耗的部署，可以选择像加拿大魁北克这样的地区，但在某些其他情况下，可能会被迫将解决方案部署在美国爱荷华州，该地区的
    CO2 排放高达 735.6 gCO2eq/KWh：'
- en: '![Figure 12.1 – Carbon emissions from an FL setup with 5 clients with 10 LEs
    and 10 rounds](img/Figure_12.1_B18681.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.1 – 来自一个具有 5 个客户端、10 个本地纪元和 10 轮训练的 FL 设置的碳排放](img/Figure_12.1_B18681.jpg)'
- en: Figure 12.1 – Carbon emissions from an FL setup with 5 clients with 10 LEs and
    10 rounds
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – 来自一个具有 5 个客户端、10 个本地纪元和 10 轮训练的 FL 设置的碳排放
- en: Now, let us see how we compute the CO2e for centralized learning.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何计算集中式学习的 CO2 排放。
- en: Centralized learning carbon emissions calculator
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集中式学习碳排放计算器
- en: Just as with FL, we can do the same for centralized learning, as illustrated
    in *Figure 12**.1*, using an ML CO2 impact calculator ([https://mlco2.github.io/impact/](https://mlco2.github.io/impact/)
    or [https://github.com/mlco2/codecarbon](https://github.com/mlco2/codecarbon))
    for different cloud platforms.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 FL 一样，我们也可以对集中式学习做同样的计算，正如 *图 12.1* 所示，使用 ML CO2 影响计算器（[https://mlco2.github.io/impact/](https://mlco2.github.io/impact/)
    或 [https://github.com/mlco2/codecarbon](https://github.com/mlco2/codecarbon)）来计算不同云平台的碳排放。
- en: 'We also demonstrate in the following example how to measure carbon emissions
    using the CodeCarbon tool, which can easily be integrated with a natural workflow.
    It captures emission metrics from within the code and can help developers to track
    metrics at the function or module level:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了以下示例，说明如何使用 CodeCarbon 工具来衡量碳排放，该工具可以轻松地集成到自然工作流中。它从代码内部捕捉排放指标，并帮助开发人员在函数或模块级别跟踪这些指标：
- en: 'As the first step, let us import all the necessary libraries:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入所有必要的库：
- en: '[PRE0]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the next step, the following code demonstrates training a DL model using
    the MNIST dataset:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，以下代码演示了使用MNIST数据集训练深度学习模型：
- en: '[PRE1]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Lastly, with the following code, we can track the emission numbers for training
    the DL code:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用以下代码，我们可以追踪训练深度学习代码时的排放数据：
- en: '[PRE2]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is the output in kg CO2:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是以千克二氧化碳（kg CO2）为单位的输出：
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can see that the DL model produces .0001063 kg of CO2 emissions.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，深度学习（DL）模型产生了0.0001063千克的二氧化碳排放。
- en: We are now equipped with sustainable ML metrics for controlling our energy consumption.
    However, research has revealed that FL plays an important role in controlling
    CO2e emissions in different circumstances. Let us now see how we can incorporate
    sustainability during model training and deployment using FL.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经具备了可持续机器学习的指标来控制我们的能耗。然而，研究表明，联邦学习在不同情况下对控制二氧化碳当量（CO2e）排放起着重要作用。让我们现在来看如何在使用联邦学习进行模型训练和部署时融入可持续性。
- en: Adopting sustainable model training and deployment with FL
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 采用可持续的模型训练和部署方式，使用联邦学习（FL）
- en: With an exponential rise in ML training, over 300,000x from 2012 to 2018 – that
    is, with a 3-4-month doubling period (well exceeding Moore’s 2-year doubling period)
    – data scientists and algorithm researchers have increasingly investigated decentralized
    approaches of model training to try and curb the tremendous heat generated from
    DL models running on specialized hardware accelerators in data centers. This specialized
    hardware uses enormous amounts of energy (200 **Terawatt-Hours** (**TWh**)), higher
    than the national electricity consumption of some countries and contributing to
    0.3% of global carbon emissions (as cited in the journal *Nature* in 2018). For
    example, Google’s AlphaGo Zero and training NLP models have released tons of CO2e,
    demonstrating the urgency of adopting a decentralized mechanism.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习训练量的指数级增长，从2012年到2018年增长超过300,000倍——即3-4个月翻倍（远超摩尔定律的2年翻倍周期）——数据科学家和算法研究人员越来越多地研究去中心化的模型训练方法，以试图遏制深度学习模型在数据中心的专用硬件加速器上运行时产生的巨大热量。这些专用硬件消耗大量能源（200
    **太瓦时**（**TWh**）），高于一些国家的国家电力消费，并占全球碳排放的0.3%（2018年《自然》期刊中引用）。例如，谷歌的AlphaGo Zero和训练自然语言处理（NLP）模型已经排放了大量的二氧化碳当量，突显了采用去中心化机制的紧迫性。
- en: The exponential rise in the number of mobile and IoT devices means that FL and
    its collaborative method of training have been instrumental in lowering power
    consumption and carbon emissions. Furthermore, it has been found that the design
    of FL has a greener environmental impact due to fewer carbon emissions when compared
    with centralized learning.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 移动设备和物联网设备数量的指数级增长意味着联邦学习及其协作训练方法在降低能耗和碳排放方面发挥了重要作用。此外，研究发现，与集中式学习相比，联邦学习的设计具有更绿色的环境影响，因为它的碳排放较少。
- en: Now, let us understand the emission metrics in the model training process.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们理解模型训练过程中的排放指标。
- en: CO2e emission metrics
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二氧化碳当量（CO2e）排放指标
- en: 'To quantify the effect of training DL models in data centers or edge servers,
    the true environmental impact can only be understood when we account for the following
    metrics, as energy consumption is truly translated into CO2e emissions based on
    geographical location:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要量化在数据中心或边缘服务器中训练深度学习模型的环境影响，只有考虑以下指标时，才能真正理解其环境影响，因为能源消耗会根据地理位置转化为二氧化碳当量（CO2e）排放：
- en: The total amount of energy consumed by the hardware for both centralized learning
    and FL systems, along with the communication energy for FL systems
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中式学习和联邦学习系统硬件消耗的总能量，以及联邦学习系统的通信能量
- en: The amount of energy consumed by the data centers due to cooling effects, particularly
    in the case of centralized learning
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中心因冷却效应而消耗的能量，特别是在集中式学习的情况下
- en: Now, let us isolate the factors that are responsible for energy consumption
    for centralized learning and FL-based training factors.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们隔离出导致集中式学习和基于联邦学习的训练系统能耗的因素。
- en: Comparing emission factors – centralized learning versus FL
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较排放因子——集中式学习与联邦学习（FL）
- en: The following factors summarize the principal differentiating points between
    energy consumption in centralized learning versus FL.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下因素总结了集中式学习与联邦学习在能源消耗上的主要区别点。
- en: Hardware dependency
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件依赖性
- en: The energy consumption in the training process for centralized learning can
    be derived by sampling the GPU and CPU power consumption at training time. The
    NVIDIA System Management Interface can be queried to average the GPU power consumption
    of all samples during training. However, for FL, besides the aggregation server
    and clients equipped with a GPU, this part of energy consumption can be safely
    eliminated.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 集中式学习训练过程中的能耗可以通过在训练期间采样GPU和CPU的功耗来推算。可以查询NVIDIA系统管理界面，以平均所有样本在训练期间的GPU功耗。然而，对于FL来说，除了配备GPU的聚合服务器和客户端之外，这部分能耗可以安全地忽略。
- en: In an FL environment with a distributed training setup and multiple communication
    rounds from heterogeneous edge devices, the training time can far exceed that
    of centralized training. This is because centralized training converges much more
    quickly. For FL, the training time, and hence the energy consumption, is closely
    related to the number of clients selected in each iteration, the data distribution
    of clients (which is most often non-IID), and the heterogeneous client devices
    with varying computational power that take part in the training process, as clients
    exhibit varying computational power.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个具有分布式训练设置和来自异构边缘设备的多个通信轮次的FL环境中，训练时间可能远远超过集中式训练的时间。这是因为集中式训练收敛得更快。对于FL来说，训练时间以及能耗与每次迭代中选择的客户端数量、客户端的数据分布（通常是非IID的）以及参与训练过程的具有不同计算能力的异构客户端设备密切相关，因为客户端的计算能力各不相同。
- en: In addition to these factors, the actual energy consumed is also affected by
    the hardware devices (RAM or HDD) employed at each client device, the infrastructure,
    and the device distribution. When scaling FL setups, we need to carefully evaluate
    all the possible options before making the proper selection for the FL client’s
    hardware options. Hence, to differentiate the energy consumption metrics for centralized
    learning and FL, we need to benchmark the hardware for FL clients to validate
    the comparison metrics.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些因素外，实际消耗的能源还受到每个客户端设备所使用的硬件设备（RAM或HDD）、基础设施和设备分布的影响。在扩展FL设置时，我们需要在做出正确的硬件选择之前，仔细评估所有可能的选项。因此，为了区分集中式学习和FL的能耗指标，我们需要基准测试FL客户端的硬件，以验证比较指标。
- en: Data center cooling
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据中心冷却
- en: 'The data center cooling process plays a significant role in overall energy
    utilization, with contributions reaching as high as 40% of the total energy consumed
    (Capozzoli and Primiceri, 2015: [https://www.researchgate.net/publication/290010399_Cooling_Systems_in_Data_Centers_State_of_Art_and_Emerging_Technologies0](https://www.researchgate.net/publication/290010399_Cooling_Systems_in_Data_Centers_State_of_Art_and_Emerging_Technologies0)).
    It is primarily dictated by the data center’s efficiency and estimated using the
    PUE ratio, which records an average of 1.67 for 2019 at a global level and is
    known for its variation across different cloud providers (recorded as 1.11 by
    Google, 1.2 by Amazon, and 1.125 by Microsoft in 2020). The FL training process
    does not have an associated cooling process. However, the central aggregation
    server can also be deployed with a cooling functionality.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '数据中心的冷却过程在整体能源利用中起着重要作用，其贡献高达总能源消耗的40%（Capozzoli 和 Primiceri, 2015: [https://www.researchgate.net/publication/290010399_Cooling_Systems_in_Data_Centers_State_of_Art_and_Emerging_Technologies0](https://www.researchgate.net/publication/290010399_Cooling_Systems_in_Data_Centers_State_of_Art_and_Emerging_Technologies0)）。这一过程主要由数据中心的效率决定，并通过PUE比率进行估算，2019年全球PUE的平均值为1.67，并且在不同的云服务提供商之间有所不同（2020年Google记录为1.11，Amazon为1.2，Microsoft为1.125）。FL训练过程没有关联的冷却过程。然而，中央聚合服务器也可以配备冷却功能。'
- en: Certain data center cooling techniques employ optimal control mechanisms through
    the use of hot/cold aisle arrangement (not separating hot and cold aisles to promote
    free air mixing), containment (by isolating hot and cold air), rack placement
    (to promote heat circulation from rack hotspots), cable organization (to allow
    uninterrupted airflow in the data centers), and the usage of blanking panels (to
    block hot air from entering the data center’s airflow). These are efficient cooling
    methods that sustain the temperatures in the data centers. Even deploying monitoring
    tools to manage the data center’s airflow, humidity levels, temperature, air pressure,
    and hotspots can result in greater efficiency due to better temperature and pressure
    control of the data center.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据中心的冷却技术通过使用最佳控制机制来实现，如热冷通道布局（不分隔冷热通道以促进空气自由混合）、封闭（通过隔离冷热空气）、机架摆放（促进机架热点的热循环）、电缆整理（确保数据中心内空气流畅通行）以及使用空白面板（防止热空气进入数据中心的气流）。这些高效的冷却方法有助于维持数据中心内的温度。甚至部署监控工具来管理数据中心的气流、湿度、温度、气压和热点，也能通过更好的温度和压力控制提高效率。
- en: Energy utilization during data exchange
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据交换过程中的能量利用率
- en: When training happens on a central server, there is certainly no data exchange
    involved. In contrast, in an FL ecosystem, the data and models are transferred,
    downloaded, or uploaded between the central aggregation server and the distributed
    clients. In addition to this, energy utilization also takes into consideration
    the energy utilized by routers and hardware on account of downloads and uploads.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练发生在中央服务器上时，肯定不涉及数据交换。相比之下，在FL生态系统中，数据和模型会在中央聚合服务器与分布式客户端之间传输、下载或上传。除此之外，能量利用率还考虑了由于下载和上传，路由器和硬件所消耗的能量。
- en: Illustrating how FL works better than centralized learning
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 说明FL比集中学习更有效
- en: 'The result of experiments done by the Xinchi group ([https://arxiv.org/pdf/2102.07627.pdf](https://arxiv.org/pdf/2102.07627.pdf))
    explains, compares, and evaluates the impact of centralized learning versus FL:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Xinchi团队进行的实验结果（[https://arxiv.org/pdf/2102.07627.pdf](https://arxiv.org/pdf/2102.07627.pdf)）解释、比较并评估了集中学习与FL的影响：
- en: The number of target communication rounds needed to attain the target accuracy
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 达到目标精度所需的通信轮次
- en: The energy associated with both centralized learning and FL
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中学习和FL相关的能量
- en: The training times for both centralized learning and FL
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中学习和FL的训练时间
- en: The different training optimizers used with FL
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与FL一起使用的不同训练优化器
- en: In the FL setup, each client was engaged in training a small dataset with low-power,
    GPU-enabled edge devices. The setup achieved the target accuracy within one or
    five LEs. In contrast, in a centralized training environment, one LE translates
    to a standard epoch, which was applied during the training process on the entire
    dataset.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL设置中，每个客户端都使用低功耗、支持GPU的边缘设备进行小数据集的训练。该设置在一个或五个LE（训练周期）内达到了目标精度。相比之下，在集中训练环境中，一个LE相当于标准的训练周期，这一周期应用于整个数据集的训练过程中。
- en: Furthermore, it has been demonstrated that the energy utilization is even lower
    in an FL setup than in a centralized learning setup using an energy-efficient
    training optimizer such as FedAdam. FedAdam is well known for demonstrating faster
    initial convergence than **Federated Averaging** (**FedAvg**) and even performing
    better than FedAvg along with non-adaptive optimizers. This adaptive optimization
    technique does not result in additional client storage or communication costs
    but rather ensures compatibility with cross-device FL.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究表明，在FL（联邦学习）设置中，能量利用率甚至比使用节能训练优化器（如FedAdam）的集中学习设置还要低。FedAdam因其在初始收敛速度上表现优于**联邦平均法**（**FedAvg**）而广为人知，甚至在与非自适应优化器的比较中也表现更好。这种自适应优化技术不会导致额外的客户端存储或通信成本，而是确保与跨设备FL的兼容性。
- en: In FedAvg, clients engaged in training do so through several **Stochastic Gradient
    Descent** (**SGD**) steps to share local updates to the server. This method underperforms
    in heterogeneous settings (such as NLP domains with different users and different
    vocabularies) as it uses a simple average of the shared update to upgrade the
    initial model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在FedAvg中，参与训练的客户端通过多个**随机梯度下降**（**SGD**）步骤将本地更新共享给服务器。这种方法在异构环境（如具有不同用户和词汇的NLP领域）下表现不佳，因为它使用共享更新的简单平均值来升级初始模型。
- en: A gradient-based optimization technique such as FedAdam focuses on an adaptive
    server optimization procedure using per-coordinate methods to average the clients’
    model updates. Here, the optimization at the server end aims to optimize the aggregated
    model from a global perspective using FedAvg and server momentum. On the other
    end, individual clients use a client optimizer over multiple epochs during local
    training. In this way, it minimizes the overall loss of local data during its
    limited participation process.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一种基于梯度的优化技术，如FedAdam，专注于通过每个坐标的方法来平均客户端的模型更新，从而实现自适应服务器优化过程。在这里，服务器端的优化旨在通过FedAvg和服务器动量，从全局视角优化聚合后的模型。另一方面，个别客户端在本地训练期间通过多次迭代使用客户端优化器。通过这种方式，它在有限参与过程中尽可能减少本地数据的总体损失。
- en: In FL, more often than centralized learning, a higher number of LEs results
    in the faster convergence of ML models in fewer FL rounds. Even though it does
    not ensure lower energy utilization, adaptive aggregation strategies such as FedAdam
    work better in terms of global model convergence speed.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL中，相比集中式学习，更多的LE通常会导致ML模型在较少的FL轮次中更快地收敛。尽管这并不能确保更低的能耗，但像FedAdam这样的自适应聚合策略在全局模型收敛速度上表现更好。
- en: In addition, non-IID datasets need more FL rounds than IID datasets, which converge
    more quickly than non-IID datasets.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，非IID数据集需要比IID数据集更多的FL轮次，而IID数据集的收敛速度比非IID数据集更快。
- en: The effectiveness of the FedADAM optimizer is evident in the Xinchi group study
    (the experiments illustrated are taken from the research results of *A First Look
    into the Carbon Footprint of Federated Learning*, [https://arxiv.org/pdf/2102.07627.pdf](https://arxiv.org/pdf/2102.07627.pdf)),
    where we see that it performed better than FedAvg for both the CIFAR-10 and SpeechCmd
    datasets. Furthermore, using five LEs, the amount of CO2e emissions using FedAdam
    was lower than for centralized learning and FL using FedAvg. For ImageNet experiments,
    to achieve the required test accuracy in the non-IID data case, the emissions
    were higher in FedAdam than FedAvg. This happened because the dataset is naturally
    unbalanced, which leads to rounds of training being required to reach the target
    test accuracy. This contributes to longer training times, leading to a higher
    level of CO2e emissions. For example, the IID SpeechCmd dataset exhibits significantly
    lower emissions using the FedAdam optimizer.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: FedADAM优化器的有效性在Xinchi小组研究中得到了验证（实验结果摘自《*Federated Learning碳足迹初探*》的研究结果，[https://arxiv.org/pdf/2102.07627.pdf](https://arxiv.org/pdf/2102.07627.pdf)），我们看到它在CIFAR-10和SpeechCmd数据集上都表现优于FedAvg。此外，使用五个LE时，FedAdam的CO2e排放量低于集中式学习和使用FedAvg的FL。对于ImageNet实验，为了在非IID数据情况下达到所需的测试准确率，FedAdam的排放量高于FedAvg。这是因为该数据集天然不平衡，导致需要多轮训练才能达到目标测试准确度，这导致了更长的训练时间，从而产生更高的CO2e排放。例如，使用FedAdam优化器时，IID
    SpeechCmd数据集的排放量显著更低。
- en: Thus we see energy efficiency is dependent on a number of factors in both centralized
    learning and FL, a primary one being the optimizer used in FL. It is often possible,
    with non-IID datasets (for example, SpeechCmd) with the FedAdam optimizer, less
    complex model architectures, and fewer communication rounds, that FL yields lower
    CO2 emissions than centralized learning. Thus we can conclude that with lightweight
    neural networks employed, FL appears to be a better choice in terms of energy
    efficiency than centralized learning.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到能效取决于集中式学习和FL中的多个因素，其中主要因素是FL中使用的优化器。在非IID数据集（例如，SpeechCmd）中，使用FedAdam优化器、较简单的模型架构以及较少的通信轮次，FL的CO2排放量往往低于集中式学习。因此，我们可以得出结论，使用轻量级神经网络时，FL在能效方面似乎优于集中式学习。
- en: Let us understand how we can bring down CO2 emissions by making a trade-off
    and optimizing the following factors in FL.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解如何通过权衡并优化FL中的以下因素来减少CO2排放。
- en: The CO2 footprint of FL
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FL的CO2足迹
- en: 'Some of the leading factors in deciding the CO2 footprint of centralized learning
    and FL training are as follows, resulting in varying levels of CO2e emissions:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 决定集中式学习和FL训练CO2足迹的一些主要因素如下，这些因素导致了不同水平的CO2e排放：
- en: '**Geolocation of hardware**: For example, training in France has been found
    to produce the lowest CO2e emissions, as nuclear energy is the main source of
    energy in France, which consequently leads to the lowest energy-to-CO2e conversion
    rate.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件的地理定位**：例如，研究发现，在法国进行训练时会产生最低的CO2e排放，因为核能是法国的主要能源来源，这也导致了最低的能量转CO2e的转换率。'
- en: '**The type of DL model, specifically the model architecture**: For example,
    FedAvg-based FL optimizations have been found to emit more CO2 for image-based
    tasks using ResNet-18 than modern GPUs and centralized training. The same FL-based
    optimizations have been found to emit less CO2 on the SpeechCmd dataset when training
    an LSTM model. If the local training tasks on FL are lightweight, with less communication
    and data exchange, studies demonstrate that it will lead to lower CO2 emissions.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习模型的类型，特别是模型架构**：例如，基于FedAvg的联邦学习优化在使用ResNet-18进行图像任务时比现代GPU和集中式训练排放更多CO2。而在SpeechCmd数据集上训练LSTM模型时，相同的联邦学习优化反而排放较少的CO2。如果联邦学习中的本地训练任务较轻，通信和数据交换较少，研究表明，这将导致更低的CO2排放。'
- en: '**Hardware efficiency**: Chips such as Tegra X2 are likely to be embedded into
    smartphones, tablets, and other IoT devices. FL will continue to reduce emissions
    when using this type of advanced chip.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件效率**：像Tegra X2这样的芯片可能会嵌入到智能手机、平板电脑和其他物联网设备中。当使用这种先进的芯片时，联邦学习将继续减少排放。'
- en: '**Cooling needs and availability in data centers**: FL does not have a centralized
    cooling process in a data center; those cooling requirements are distributed across
    the devices included in the federation and vary based on each device’s needs.
    Distributed setups of centralized learning need a cooling facility. Advanced GPUs
    or **TPUs** (short for **Tensor Processing Units**) requiring high computational
    power demand more requirements for cooling and, consequently, high energy utilization.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据中心的冷却需求和可用性**：联邦学习在数据中心没有集中式的冷却过程；这些冷却需求分布在联合中的各个设备上，并根据每个设备的需求而变化。集中式学习的分布式设置需要冷却设施。高性能GPU或**TPU**（**张量处理单元**）需要大量计算能力，要求更高的冷却需求，从而导致更高的能量消耗。'
- en: '**Communication rounds for FL**: One LE in centralized learning yields more
    CO2 than five LEs, irrespective of the aggregation strategy or the device in place.
    This happens due to the fact that a lower number of local training epochs causes
    increased time for the model to converge. This ultimately leads to more data exchange
    and communication rounds between the local clients and the global server. With
    five LEs, individual devices train for longer, leading to fewer communication
    rounds and lower emissions.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**联邦学习中的通信轮次**：在集中式学习中，一个LE产生的CO2比五个LE更多，无论是聚合策略还是设备类型。这是因为较少的本地训练epoch会导致模型收敛所需时间增加，从而导致更多的数据交换和本地客户端与全球服务器之间的通信轮次。使用五个LE时，单个设备训练时间更长，导致较少的通信轮次和较低的排放。'
- en: '**Data exchanges in FL**: As we have seen before, the percentage of CO2e emission
    is also primarily driven by **Wide-Area Networking** (**WAN**) emissions due to
    exchanges between datasets and FL setups. For example, the energy utilization
    of communication may yield up to 0.4% (ImageNet with five LEs) and 95% (CIFAR-10
    with one LE) of total emissions.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**联邦学习中的数据交换**：正如我们之前所看到的，CO2e排放的百分比也主要由**广域网络**（**WAN**）排放所驱动，这些排放源自数据集和联邦学习设置之间的交换。例如，通信的能量利用可能占总排放的0.4%（五个LE的ImageNet）和95%（一个LE的CIFAR-10）。'
- en: 'To bring parity and equivalency when comparing CO2e emissions, the communication
    rounds in FL are converted into centralized epochs. CO2e emissions are directly
    correlated to the number of centralized epochs. Some of the key factors in the
    increase in CO2 emissions put forward by the Xinchi group are as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在比较CO2e排放时达到对等性和一致性，联邦学习中的通信轮次被转换为集中式训练的epoch。CO2e排放与集中式epoch的数量直接相关。Xinchi团队提出的一些CO2排放增加的关键因素如下：
- en: The number of centralized epochs
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集中式epoch的数量
- en: The thermal design power of hardware
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件的热设计功耗
- en: An increase in the model size, increasing the energy used for communication
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型大小的增加，增加了通信所消耗的能量
- en: The size of the training dataset
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据集的大小
- en: The use of a better DL-based optimizer, such as FedAdam, which can outperform
    centralized learning
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更好的基于深度学习的优化器，例如FedAdam，它能够优于集中式学习
- en: We know how the devastating impacts of climate change over the last decade have
    required us to rethink and reconsider our emission metrics before deploying an
    architecture. Climate change and concern for a greener planet will drive research
    and innovation when designing new metrics. All these factors, when assembled,
    motivate us to choose better hardware that can provide visibility into the CO2
    footprint and offer recommendations for possible remediations. It is now important
    for us to know how we can deploy popular design patterns for training FL models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，过去十年气候变化的毁灭性影响迫使我们在部署架构之前重新思考和审视我们的排放指标。气候变化和对更绿色星球的关注将推动设计新指标时的研究和创新。所有这些因素汇聚在一起，促使我们选择更好的硬件，以便能够看到CO2足迹并提供可能的修正建议。现在，我们必须了解如何部署用于训练FL模型的流行设计模式。
- en: We should also look at other ways of generating energy from renewable sources,
    which can play an important role in convincing organizations to control their
    emissions and encourage investment in renewable energy.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应考虑其他可再生能源发电方式，这些方式可以在说服组织控制排放并鼓励投资可再生能源方面发挥重要作用。
- en: Hence, let us walk through ways to compensate for CO2e emissions.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们探讨如何补偿CO2e排放。
- en: How to compensate for equivalent CO2e emissions
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何补偿等效CO2e排放
- en: 'To compensate for CO2e emissions, we need to understand how the information
    regarding the energy grid or the conversion rate from energy into CO2e can be
    handled. As there is also a dearth of public sources of information, there are
    certain recommended practices to bear in mind:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了补偿CO2e排放，我们需要理解如何处理有关能源电网的信息或将能源转化为CO2e的转换率。由于公共信息来源的匮乏，有一些推荐的实践方法需要牢记：
- en: We must move with the assumption that all data centers and edge devices connected
    to a local grid are directly associated with their physical location. Electricity-specific
    CO2e emission factors are expressed in kg CO2e/kWh, with varying factors associated
    with each country, such as in France (0.0790), the US (0.5741), and China (0.9746).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须假设，所有连接到本地电网的数据中心和边缘设备都直接与其物理位置相关联。特定于电力的CO2e排放因子以kg CO2e/kWh表示，不同国家的排放因子各不相同，例如法国（0.0790）、美国（0.5741）和中国（0.9746）。
- en: The emission technology (with suitable metrics to compute the energy lost when
    transmitting and distributing electricity) and the throughput or productivity
    of heat plants can impact the overall energy utilization.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排放技术（包括计算电力传输和分配时能量损失的适当指标）以及热电厂的吞吐量或生产力可能会影响整体能源利用情况。
- en: Evaluating the conversion factor for both FL and centralized learning with the
    aforementioned assumptions in mind can help us compute and target an energy utilization
    metric that has the least CO2 emissions. In FL, the energy used for communication
    varies based on the type of data partitions (more energy is required for IID datasets),
    the number of communication rounds, the hardware type, the location, and other
    factors that we have discussed.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑到上述假设，评估FL和集中式学习的转换因子可以帮助我们计算并设定一个具有最少CO2排放的能源利用指标。在FL中，通信所需的能源因数据分区类型（IID数据集需要更多能源）、通信轮数、硬件类型、位置及我们已经讨论过的其他因素而有所不同。
- en: Our next goal as sustainability experts is to compensate for the resultant CO2
    emissions by allocating renewable energy sources through buying **Renewable Energy
    Credits** (**RECs**) in the US or **Tradable Green Certificates** (**TGCs**) in
    the EU. Even initiatives to support environment-friendly projects, such as renewable
    energy initiatives or massive tree-planting activities, can be encouraged.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为可持续发展专家，我们的下一个目标是通过购买美国的**可再生能源凭证**（**RECs**）或欧盟的**可交易绿色证书**（**TGCs**）来补偿由此产生的CO2排放。即使是支持环保项目的倡议，如可再生能源项目或大规模植树活动，也可以得到鼓励。
- en: We now need to consider how we train FL models by considering different parameters
    that impact CO2 emissions.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要考虑如何通过考虑不同的参数来训练FL模型，这些参数会影响CO2排放。
- en: Design patterns of FL-based model training
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于FL的模型训练设计模式
- en: Till now, we have been concentrating on how FL is a great methodology from a
    sustainability standpoint. However, apart from sustainability, FL also offers
    a great benefit from a data privacy standpoint, in relation to Responsible AI.
    In FL, clients are able to share anonymized learning from their local data, instead
    of having to share potentially sensitive data with a centralized process. We will
    learn more about this in this section as we learn more about the different design
    patterns of FL.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在集中讨论从可持续性角度看，FL是一种非常好的方法。然而，除了可持续性之外，FL还从数据隐私的角度提供了巨大的好处，这与负责任的人工智能（Responsible
    AI）相关。在FL中，客户端能够分享来自本地数据的匿名化学习，而无需将潜在敏感的数据与集中式处理过程共享。在这一部分中，我们将进一步了解这一点，并深入了解FL的不同设计模式。
- en: 'FL has different design and deployment strategies that will impact its CO2
    emission metrics. Some of the key metrics are as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: FL有不同的设计和部署策略，这些策略将影响其二氧化碳排放指标。以下是一些关键指标：
- en: The size of the non-IID datasets in each client
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个客户端中非IID数据集的大小
- en: The number of participating clients
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参与的客户端数量
- en: The wait time before clients engage in training that impacts the model convergence
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户端参与训练之前的等待时间，这影响模型的收敛性
- en: The training time for local clients and the time to aggregate the global model
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地客户端的训练时间以及聚合全局模型的时间
- en: Let us now see the different modes of FL-based training.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下基于FL的训练的不同模式。
- en: 'When we think about model training and data processing patterns using FL, we
    need to consider the type of data, along with the motivation of the clients to
    participate in local training. Hence, we can primarily divide them into three
    types:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑使用FL进行模型训练和数据处理模式时，我们需要考虑数据类型，以及客户端参与本地训练的动机。因此，我们可以将它们主要分为三种类型：
- en: A multi-task model trainer to train non-IID datasets
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个多任务模型训练器，用于训练非独立同分布（non-IID）数据集
- en: A heterogeneous data handler known for training heterogeneous datasets
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种用于训练异构数据集的异构数据处理器
- en: An incentive registry, which motivates clients through a reward system
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个激励注册系统，通过奖励机制激励客户端
- en: 'In each of the illustrated figures corresponding to each of the design patterns,
    we used the sequence of FL, which is as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个与设计模式对应的示意图中，我们使用了以下的FL序列：
- en: A generic global model is trained by the central server.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个通用的全局模型由中央服务器训练。
- en: Each client selected in the specific training round downloads the global model
    and kicks off the local training process.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在特定训练轮次中选择的每个客户端下载全局模型，并启动本地训练过程。
- en: The locally trained model from the client end is then updated on the global
    server.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端本地训练的模型随后会在全球服务器上进行更新。
- en: The server aggregates the global models using the FedAvg algorithm to improve
    the shared version of the model.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务器使用FedAvg算法聚合全局模型，以改进共享版本的模型。
- en: The local devices are then updated with the newly retrained global model, which
    prepares them for local retraining and updates them for successive iterations.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，本地设备将被更新为最新的重训练全局模型，这为它们的本地重训练做准备，并更新它们以进行后续迭代。
- en: The multi-task model trainer
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多任务模型训练器
- en: 'This mode of training has the primary objective of improving learning efficiency
    and model performance metrics. Furthermore, it is most suitable for training separate
    related models on local devices. Specifically, we use this when the data distribution
    patterns of clients differ and the global model falls short of representing the
    data pattern exhibited by every client. For example, as the following figure illustrates,
    training separate models on computer vision, reinforcement learning, speech recognition,
    and NLP is essential for modeling and demonstrating the purpose of each model:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这种训练模式的主要目标是提高学习效率和模型性能指标。此外，它最适合在本地设备上训练独立的相关模型。具体而言，当客户端的数据分布模式有所不同，并且全局模型无法代表每个客户端所展示的数据模式时，我们会使用这种方法。例如，正如下图所示，在计算机视觉、强化学习、语音识别和自然语言处理（NLP）上训练独立的模型，对于建模和展示每个模型的目的至关重要：
- en: '![Figure 12.2 – FL model training design pattern – a multi-task model trainer](img/Figure_12.02_B18681.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2 – FL模型训练设计模式 – 一个多任务模型训练器](img/Figure_12.02_B18681.jpg)'
- en: Figure 12.2 – FL model training design pattern – a multi-task model trainer
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – FL模型训练设计模式 – 一个多任务模型训练器
- en: One real-world example would be to use ML to solve a next-word prediction task
    (performs related machine learning tasks involving NLP) by using device data such
    as text messages, web browser search strings, and emails.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现实世界的例子是使用机器学习（ML）解决下一个词预测任务（涉及自然语言处理的相关机器学习任务），通过使用设备数据，如文本消息、网页浏览器搜索字符串和电子邮件。
- en: To obtain higher accuracy, this design involves more training time, computation,
    and energy resources in every round, as expected with other conventional FL techniques.
    The major challenge of this method is the vulnerability of local clients to data
    privacy threats and the limitations of the training to convex loss functions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更高的准确性，该设计每一轮都涉及更多的训练时间、计算和能源资源，这与其他传统联邦学习（FL）技术的期望一致。该方法的主要挑战是本地客户容易受到数据隐私威胁的影响，并且训练只能应用于凸损失函数。
- en: A heterogeneous data handler
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个异构数据处理器
- en: 'This mode of training preserves data privacy and finds better vanilla FL usage
    with non-IID and skewed data distributions by applying special processing techniques
    such as data augmentation and adversarial training with a generative adversarial
    network:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这种训练模式通过应用诸如数据增强和使用生成对抗网络进行对抗训练等特殊处理技术，保持数据隐私，并在数据分布非独立同分布（non-IID）和偏斜的情况下，找到更好的基本FL使用方法：
- en: '![Figure 12.3 – FL model training design pattern – a heterogeneous data handler](img/Figure_12.03_B18681.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3 – FL模型训练设计模式 – 一个异构数据处理器](img/Figure_12.03_B18681.jpg)'
- en: Figure 12.3 – FL model training design pattern – a heterogeneous data handler
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – FL模型训练设计模式 – 一个异构数据处理器
- en: As the personalized data of clients results in imbalanced and skewed data distributions,
    local models trained on devices tend to lower the global model accuracy upon aggregation.
    Hence, it becomes essential to promote data efficiency by plugging in a heterogeneous
    data handler that can correctly augment and distill the federated data and still
    preserve data privacy. The distillation process equips the client devices to gather
    information from other participating devices at intervals without any direct access
    to other clients’ data.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于客户的个性化数据导致数据分布不均衡和偏斜，在设备上训练的本地模型在聚合时往往会降低全局模型的准确性。因此，必须通过插入一个异构数据处理器来提升数据效率，该处理器可以正确地增强和提炼联邦数据，同时仍然保持数据隐私。提炼过程使客户端设备能够定期从其他参与设备收集信息，而无需直接访问其他客户的数据。
- en: This mechanism tries to yield better model performance metrics at the cost of
    the training time and computational resources, which, ultimately, results in lower
    energy efficiency and a lower sustainability metric.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 该机制试图以训练时间和计算资源为代价，提供更好的模型性能指标，最终导致较低的能源效率和较低的可持续性指标。
- en: An incentive registry
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个激励登记
- en: 'The training model shown in the following figure rewards each participating
    client based on their contribution in terms of data volume, model performance,
    and computation resources, among other things. This is a measure to motivate clients
    and improve the performance of the global model. The following figure demonstrates
    a blockchain and a smart-contract-based incentive mechanism:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示的训练模型根据参与客户在数据量、模型性能、计算资源等方面的贡献进行奖励。这是一种激励客户并提高全局模型性能的措施。下图展示了基于区块链和智能合约的激励机制：
- en: '![Figure 12.4 – FL model training design pattern – an incentive registry](img/Figure_12.04_B18681.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图12.4 – FL模型训练设计模式 – 一个激励登记](img/Figure_12.04_B18681.jpg)'
- en: Figure 12.4 – FL model training design pattern – an incentive registry
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 – FL模型训练设计模式 – 一个激励登记
- en: Motivation energizes more clients to participate, as this training strategy
    is not driven by the complete participation of each and every client in every
    iteration. Moreover, the incentive scheme needs a mutual agreement between the
    clients and the learning coordinator to decide the evaluation criteria. Some of
    the common ways to formulate incentive schemes are reinforcement learning, blockchain/smart
    contracts, and the Stackelberg game model. One specific employment of blockchain-based
    FL incorporating incentives is FLChain, which supports collaborative training
    and a marketplace for model trading. This model also suffers from the challenge
    of long training times and computation resources.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 激励机制能激励更多客户端参与，因为这种训练策略并不是通过每个客户端在每次迭代中都完全参与来驱动的。此外，激励机制需要客户端和学习协调者之间达成共识，以决定评估标准。制定激励机制的一些常见方法包括强化学习、区块链/智能合约和斯塔克尔伯格博弈模型。基于区块链的FL激励机制的一个具体应用是FLChain，它支持协作训练和模型交易市场。该模型同样面临着长时间训练和计算资源挑战的问题。
- en: In this section, we learned about important factors that control the FL training
    setup, including the parameters, the number of clients, time, and the epoch period
    of training. All these factors contribute to the energy used in training environments,
    which contributes to global warming. Beyond model training, let us also learn
    how different modes of model deployment also have an impact on energy emissions.
    In the following section, we will primarily discuss a strategy in an FL environment
    in which only selected clients are engaged in training. Although we will refer
    to individual clients taking part in the training process, our main goal is to
    understand the operational or deployment modes that enable clients to trigger
    the local training process.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了控制FL训练设置的重要因素，包括参数、客户端数量、时间和训练的时期周期。所有这些因素都影响着训练环境中的能量消耗，从而加剧了全球变暖。除了模型训练，我们还将了解不同的模型部署模式如何影响能源排放。在接下来的章节中，我们将主要讨论一种FL环境中的策略，其中只有选定的客户端参与训练。尽管我们将提到参与训练过程的个别客户端，但我们的主要目标是理解那些能促使客户端启动本地训练过程的操作或部署模式。
- en: Sustainability in model deployments
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型部署中的可持续性
- en: In the previous section, we learned how FL plays an important role in sustainability.
    Now, let us explore a sustainable ML framework for FL. We will see how to use
    rechargeable devices that are capable of accumulating energy from the ambient
    environment and effectively utilizing it during intermittent training periods.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解了FL在可持续性方面的重要作用。现在，让我们探讨一个可持续的FL ML框架。我们将看到如何使用能够从环境中积累能量并在间歇性训练期间有效利用这些能量的可充电设备。
- en: 'This kind of framework can be extended to cross-device and cross-silo FL settings,
    including FL in wireless edge networks, IoT, and the **Internet of Medical Devices**
    (**IoMD**). Individual local device training can drive model convergence, as well
    as make it adaptable to the following settings:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这种框架可以扩展到跨设备和跨信息孤岛的FL设置，包括无线边缘网络中的FL、物联网和**医疗设备互联网**（**IoMD**）。单个本地设备的训练可以推动模型的收敛，并使其适应以下设置：
- en: Enabling the random selection of a small number of clients to minimize the communication
    overhead during every iteration.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过允许随机选择少量客户端来最小化每次迭代中的通信开销。
- en: Enabling the selection of clients to maximize the learning rate to speed up
    the convergence rate.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启用客户端选择，以最大化学习速率，加速收敛速度。
- en: Enabling a client selection process based on the energy arrival process at each
    client. This is to address intermittent and non-homogeneous energy arrival patterns
    to effectively manage clients dropping out.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于每个客户端的能源到达过程启用客户端选择过程。这是为了应对间歇性和非均匀的能源到达模式，以有效管理客户端掉线。
- en: This deployment framework can rightly fit scenarios in which heterogeneous devices
    are available and each training round works independently. This makes the framework
    much more flexible, as each device participating in the training process in one
    round is not held accountable or dependent on future consecutive rounds. The training
    process (driven by random energy availability) is coordinated by the central server,
    which aggregates the local models of the clients to send them the updated global
    model. The individual clients are engaged in the local training with their own
    datasets, as well as updating the global model received using multiple SGD iterations
    over their local dataset. The framework not only benefits from energy-efficient
    sustainable FL training but also minimizes the total energy cost of training in
    scenarios where devices generate energy through an intermittent and non-homogeneous
    renewal process.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 该部署框架非常适合异构设备可用且每轮训练独立进行的场景。这使得框架更加灵活，因为每个参与训练过程的设备在某一轮中不会被未来的连续轮次所约束或依赖。训练过程（由随机的能量供应驱动）由中央服务器协调，服务器汇总客户端的本地模型并将更新的全局模型发送给它们。各个客户端使用自己的数据集进行本地训练，并通过多次SGD迭代更新收到的全局模型。该框架不仅有助于节能的可持续FL训练，而且在设备通过间歇性和非同质的能量更新过程生成能量的场景下，能够最小化训练的总能量成本。
- en: 'The framework allows clients of different types to have varying levels of energy
    generation to participate in training, with two different configurations:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架允许不同类型的客户端具有不同的能量生成水平以参与训练，提供两种不同的配置：
- en: A biased model prediction strategy, where the global model is biased toward
    clients that have more frequent energy availability. This kind of model sees a
    performance loss in the accuracy of the predicted outcomes but yields a better
    convergence rate.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种有偏的模型预测策略，其中全局模型偏向于那些能量供应更频繁的客户端。这种模型在预测结果的准确性上会有性能损失，但能够提供更好的收敛速度。
- en: An unbiased model prediction strategy, where there is a wait associated with
    letting all the clients generate enough energy before each iteration of participating
    in the training process. This kind of model suffers from the longest wait time
    based on the slowest client. However, this kind of training has better performance
    metrics, despite having a slow convergence rate.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种无偏的模型预测策略，其中允许所有客户端在每次参与训练过程之前，等待足够的能量生成。这种模型基于最慢客户端的等待时间，会有最长的等待时间。然而，这种训练尽管收敛速度较慢，但在性能指标上表现更好。
- en: 'The proposed framework is equipped with four different entities, as described
    here, to facilitate an energy-aware client scheduling and training strategy:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的框架配备了四个不同的实体，如下所述，以促进一个能量感知的客户端调度和训练策略：
- en: '**The energy profile of clients**: Accounting for the energy availability of
    all clients is the most important factor, as it governs the client participation
    based on the energy received from the ambient environment, such as solar, kinetic,
    ambient light, or RF energy. Profiling is a mechanism for studying energy patterns
    to infer the availability of sufficient energy to train the local model and send
    updates to the central server. Any participating client starting the training
    process at a global round starting at the initial time instant *t* ensures the
    client participates for the whole duration of that global round, *{t, . . ., t+T
    −1}*, where the client is engaged in training the local model. In addition, clients
    taking part in the training process for any global round remain constant during
    the entire duration of training for that specified global round.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户端的能量配置**：考虑所有客户端的能量供应是最重要的因素，因为它决定了客户端基于从环境中获得的能量（如太阳能、动能、环境光或射频能量）参与训练的情况。能量配置是一种研究能量模式的机制，用于推断是否有足够的能量来训练本地模型并将更新发送给中央服务器。任何参与的客户端从初始时刻
    *t* 开始参与全球轮次的训练过程，确保客户端在该全球轮次的整个时段内参与训练，*{t, . . ., t+T −1}*，期间客户端正在训练本地模型。此外，参与任何全球轮次训练过程的客户端，在该指定的全球轮次训练的整个过程中保持不变。'
- en: '**Client scheduling**: The framework comes with the flexibility of allowing
    clients to decide whether they want to participate in the training process at
    that specific iteration based on their energy profile. The stochastic participation
    process incorporates clients after locally estimating the energy arrival process
    to maximize the convergence rate or reduce the communication overhead of training.
    The scheduling process does not necessitate coordination among clients, making
    it easier to scale in large networks.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户端调度**：该框架具有灵活性，允许客户端根据其能量状态决定是否参与当前迭代的训练过程。随机参与过程在客户端本地估计能量到达过程后，将其纳入调度，以最大化收敛速度或减少训练的通信开销。调度过程无需客户端之间的协调，使得在大规模网络中更容易扩展。'
- en: '**Local training at the client**: This phase allows participating clients to
    train their local datasets using SGD and then update them on the central server.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户端本地训练**：此阶段允许参与的客户端使用 SGD 训练本地数据集，然后将其更新到中央服务器。'
- en: '**Model updates by the server**: This phase allows the server to aggregate
    the local models to consolidate the global model to be sent to individual devices.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务器更新模型**：此阶段允许服务器聚合本地模型，以整合全球模型并发送到各个设备。'
- en: Having studied the principal influencing factors behind energy-intelligent client
    scheduling, here, we will discuss how the scheduling process works based on the
    energy availability of clients.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究了影响能量智能客户端调度的主要因素后，我们将在此讨论基于客户端能量可用性如何进行调度的过程。
- en: '*Figure 12**.5* illustrates the two different prediction strategies, where
    clients either take part as soon as energy arrives or wait until all clients have
    accumulated the energy required to start training. The left-hand part of the figure
    is optimistic about model convergence and starts client scheduling immediately
    when the energy is available (as represented by different colors of *t*1, *t*2,
    *t*3, and *t*4). On the other hand, the right-hand subfigure is conservative and
    waits until energy is available for all the clients to schedule them together,
    as represented by them all being the same color, orange, where *t*1, *t*2, *t*3,
    and *t*4 are different time instances:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12.5* 说明了两种不同的预测策略，其中客户端要么在能量到达时立即参与，要么等到所有客户端积累了启动训练所需的能量后再开始。图的左侧部分对模型收敛持乐观态度，在能量可用时立即开始客户端调度（由不同颜色的
    *t*1、*t*2、*t*3 和 *t*4 表示）。而右侧子图则比较保守，等待所有客户端都具备可用能量后再统一调度，它们的颜色都是相同的橙色，其中 *t*1、*t*2、*t*3
    和 *t*4 表示不同的时间点：'
- en: '![Figure 12.5 – Client scheduling based on energy arrival patterns](img/Figure_12.05_B18681.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.5 – 基于能量到达模式的客户端调度](img/Figure_12.05_B18681.jpg)'
- en: Figure 12.5 – Client scheduling based on energy arrival patterns
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5 – 基于能量到达模式的客户端调度
- en: In this section, we have learned how sustainable FL can be deployed in large-scale
    networks using stochastic energy arrival processes. Furthermore, adapting model
    quantization and compression techniques with a completely random energy availability
    among clients promises a better characterization of the relationship between the
    energy renewal processes and the training performance.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们已经学习了如何通过使用随机能量到达过程，在大规模网络中部署可持续的 FL。此外，结合模型量化和压缩技术，利用客户端间完全随机的能量可用性，能够更好地描述能量更新过程与训练性能之间的关系。
- en: We have understood scheduling the client’s training process based on energy.
    Now, let us see how these training patterns can be scheduled according to the
    architecture and how to scale FL models.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经理解了基于能量调度客户端的训练过程。现在，让我们看看这些训练模式如何根据架构进行调度，以及如何扩展 FL 模型。
- en: Design patterns of FL-based model deployments
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于 FL 的模型部署设计模式
- en: 'Let us deep-dive into how FL models can be managed and aggregated once they
    are live in production. Here, we will discuss two types of architectural designs
    that come into play, mainly from the perspective of data communication, model
    management, and governance, and how these models are aggregated and distributed
    to local clients:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨一旦 FL 模型在生产环境中上线后，如何管理和聚合它们。在这里，我们将讨论两种架构设计，主要从数据通信、模型管理和治理的角度出发，探讨这些模型如何被聚合并分发给本地客户端：
- en: '**FL-based model management patterns**: We need the model management patterns
    to establish the rules and processes related to the local client’s data or model
    size. The size plays a critical role in the data or model exchange and, hence,
    the amount of energy consumed. In addition, the frequency of replacing the model
    and updating the global model serves as a deciding factor in the CO2e and the
    sustainability of FL.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于FL的模型管理模式**：我们需要模型管理模式来建立与本地客户端的数据或模型大小相关的规则和流程。大小在数据或模型交换中起着至关重要的作用，因此影响能耗。此外，替换模型的频率以及更新全局模型的频率是决定CO2e排放量和FL可持续性的关键因素。'
- en: '**FL-based model aggregation patterns**: We need model aggregation patterns
    in FL to consolidate learning from individual clients to create an updated global
    model. The mode of the m­odel aggregation process from the deployed clients –
    be it asynchronous, hierarchical, or decentralized – impacts the timing and the
    latency. All these factors contribute to varying levels of CO2e.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于FL的模型聚合模式**：我们需要FL中的模型聚合模式，将来自单个客户端的学习整合起来，以创建更新后的全局模型。模型聚合过程的模式——无论是异步的、分层的，还是去中心化的——都会影响时机和延迟。所有这些因素共同作用，导致不同水平的CO2e排放。'
- en: Let us discuss these designs in the following sections.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的部分中讨论这些设计。
- en: FL-based model management patterns
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于FL的模型管理模式
- en: 'Model management patterns are responsible for model transmission, deployment,
    and governance. Based on how messages are transferred or models are stored locally
    by clients, we can divide them into four broad categories, as described here:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 模型管理模式负责模型的传输、部署和治理。根据消息传输方式或客户端如何在本地存储模型，我们可以将它们分为四个大类，如下所述：
- en: A message compressor, which reduces the transmitted message size
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息压缩器，减小传输消息的大小
- en: A model co-versioning registry, involved in model version management by receiving
    model updates from clients, helping to facilitate model aggregation
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型共版本注册表，参与模型版本管理，通过接收客户端的模型更新，帮助促进模型聚合
- en: A model replacement unit, which monitors the global model’s performance and
    initiates new training when the model starts to show a reduced performance
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个模型替换单元，监控全局模型的性能，并在模型性能下降时启动新的训练
- en: A deployment selector, which pushes the improved global model to the clients
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署选择器，将改进后的全局模型推送到客户端
- en: Let us discuss each of these four categories in detail.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论这四个类别中的每一个。
- en: A message compressor
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 消息压缩器
- en: 'This design pattern, as demonstrated in *Figure 12**.6*, compresses the message
    to reduce its data size during the model exchange process during every round:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图12.6*所示，这一设计模式在每轮模型交换过程中压缩消息，以减小数据大小：
- en: '![ Figure 12.6 – FL model deployment design pattern – message compressor](img/Figure_12.06_B18681.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图12.6 – FL模型部署设计模式 – 消息压缩器](img/Figure_12.06_B18681.jpg)'
- en: Figure 12.6 – FL model deployment design pattern – message compressor
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6 – FL模型部署设计模式 – 消息压缩器
- en: This process helps to increase the efficiency of communication by effectively
    compressing model parameters or gradients in limited-bandwidth scenarios. However,
    the pattern incurs additional computation costs when the server must aggregate
    sizeable model parameters. Moreover, it also adds an overhead for message compression
    and decompression and may involve the loss of essential information.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程通过有效地压缩模型参数或梯度，帮助在带宽有限的情况下提高通信效率。然而，当服务器必须聚合大量的模型参数时，该模式会带来额外的计算成本。此外，它还会增加消息压缩和解压缩的开销，可能会丢失一些关键信息。
- en: A model co-versioning registry
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个模型共版本注册表
- en: 'The model co-versioning registry, as shown in *Figure 12**.7*, aids the local
    model governance process by tracking each client’s model version and aligning
    it with the global model of that corresponding iteration:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 模型共版本注册表，如*图12.7*所示，通过跟踪每个客户端的模型版本并将其与相应迭代的全局模型对齐，帮助本地模型治理过程：
- en: '![Figure 12.7 – FL model deployment design pattern – model co-versioning registry](img/Figure_12.07_B18681.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图12.7 – FL模型部署设计模式 – 模型共版本注册表](img/Figure_12.07_B18681.jpg)'
- en: Figure 12.7 – FL model deployment design pattern – model co-versioning registry
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7 – FL模型部署设计模式 – 模型共版本注册表
- en: This facilitates synchronous and asynchronous model updates, client dropouts,
    model selection, and stopping complex model training early. Furthermore, this
    pattern has the unique advantage of tracking the model quality (including global
    models, client device updates, and the version of the application or the device
    OS/firmware), and adversarial activities by clients (where a client acts as an
    adversary) to strengthen the accountability of the system. The registry is used
    to collect local model updates and map them to the global model so that the model
    version number and client IDs stay immutable. This pattern can also be implemented
    using a blockchain to provide model provenance and co-versioning. The added storage
    volume required by this architecture to store all versions of the global and local
    models comes with the increased benefit of offering system security to detect
    dishonest clients that may cause a system failure. One prime example of this pattern
    is the MLflow Model Registry built on Databricks, which has an efficient centralized
    model store for tracking the chronological model lineage, versioning, and stage
    transitions.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于同步和异步模型更新、客户端掉线、模型选择以及提前停止复杂模型训练。此外，这种模式具有独特的优势，可以跟踪模型质量（包括全局模型、客户端设备更新以及应用程序或设备操作系统/固件的版本），以及客户端的对抗性行为（当客户端充当对手时），以增强系统的问责性。注册表用于收集本地模型更新并将其映射到全局模型，从而确保模型版本号和客户端
    ID 保持不变。此模式还可以通过区块链实现，以提供模型来源和共同版本管理。该架构所需的额外存储量，用于存储所有版本的全局和本地模型，带来了额外的好处，即为系统安全提供保障，能够检测出可能导致系统故障的不诚实客户端。这个模式的一个典型例子是基于
    Databricks 构建的 MLflow 模型注册表，它提供了一个高效的集中式模型存储，用于跟踪模型的时间线、版本管理和阶段过渡。
- en: A model replacement trigger
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个模型替换触发器
- en: This FL design pattern initiates the replacement of the model by setting a trigger
    on a new model task whenever the model’s performance degrades below an acceptable
    threshold. To address issues such as a drop in the model’s accuracy, this pattern
    provides mechanisms to investigate the reason for this reduced accuracy before
    establishing the new model training process. The retraining process is set only
    when the degradation in model performance is noticed for a few consecutive rounds
    to strongly infer that the performance reduction is global. Retraining the model
    adds a cost both in terms of communication and computation but is effective in
    handling clients in an FL environment with heterogenous non-IID datasets where
    clients have personalized datasets and exhibit faster decays in the global model's
    performance over successive iterations. Microsoft Azure Machine Learning designer
    and Amazon SageMaker are well-known platforms for triggering model retraining
    based on the degradation of model performance.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这个联邦学习设计模式通过在新的模型任务上设置触发器来启动模型的替换，当模型的性能降到可接受阈值以下时。为了解决模型准确度下降等问题，这个模式提供了机制，在建立新的模型训练过程之前，先调查这种准确度下降的原因。只有在连续几轮注意到模型性能退化时，才会启动重新训练过程，从而强烈推断性能下降是全局性的。重新训练模型会增加通信和计算成本，但在处理具有异质非独立同分布（non-IID）数据集的联邦学习环境中的客户端时非常有效，因为这些客户端拥有个性化数据集，并且随着多轮迭代，全球模型的性能会更快衰退。微软
    Azure 机器学习设计器和 Amazon SageMaker 是基于模型性能退化触发模型重新训练的知名平台。
- en: A deployment selector
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个部署选择器
- en: 'This deployment strategy, as demonstrated in *Figure 12**.8*, enables the convergence
    of the global model and pushes the global model to selected clients based on the
    application on which they run. Hence, different groups of clients receive different
    versions of converged models, and this aims to improve the performance metrics
    of the models. The model selection addresses the non-IID data distribution among
    clients so that different customized converged models serve different groups of
    clients better. However, this procedure comes with added overhead on the server,
    as the central server needs to properly identify the clients to provide them with
    different versions of the global model and, at the same time, train and store
    different models for diverse clients. Despite the increased training cost, we
    get better-generalized models based on the ML task of the clients. This strategy
    needs to have extra privacy measures built in to prevent privacy leakage when
    the server tries to group clients. This type of FL training is available on Amazon
    SageMaker and Google Cloud, which trains and manages multiple model versions and
    deploys them at different endpoints:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图12.8*所示，这种部署策略能够使全局模型收敛，并根据所运行的应用将全局模型推送到选定的客户端。因此，不同组的客户端接收到不同版本的收敛模型，目的是提高模型的性能指标。模型选择解决了客户端之间非IID数据分布的问题，使得不同定制的收敛模型能够更好地服务于不同的客户端组。然而，这一过程会增加服务器的开销，因为中央服务器需要正确识别客户端，以便为它们提供不同版本的全局模型，同时训练和存储适用于不同客户端的不同模型。尽管训练成本增加，但我们根据客户端的ML任务得到了更好的泛化模型。此策略需要内置额外的隐私保护措施，以防止在服务器尝试分组客户端时出现隐私泄露。这种FL训练可以在Amazon
    SageMaker和Google Cloud上进行，它们可以训练和管理多个模型版本，并将其部署到不同的端点：
- en: '![Figure 12.8 – FL model deployment design pattern – model deployment selector](img/Figure_12.08_B18681.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图12.8 – FL模型部署设计模式 – 模型部署选择器](img/Figure_12.08_B18681.jpg)'
- en: Figure 12.8 – FL model deployment design pattern – model deployment selector
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8 – FL模型部署设计模式 – 模型部署选择器
- en: FL-based model aggregation patterns
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于FL的模型聚合模式
- en: 'This design pattern employs different tactics for model aggregation to reduce
    aggregation latency and increase the system’s efficiency, reliability, and accountability.
    The primary objective is to improve the model performance metrics by effectively
    utilizing optimal resources. They can be broadly divided into the following four
    types of patterns:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计模式采用不同的策略进行模型聚合，以减少聚合延迟并提高系统的效率、可靠性和可追溯性。主要目标是通过有效利用最佳资源来改善模型的性能指标。它们可以大致分为以下四种模式：
- en: An asynchronous secure aggregator
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步安全聚合器
- en: A decentralized aggregator
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去中心化聚合器
- en: A security aggregator
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全聚合器
- en: A hierarchical aggregator
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层聚合器
- en: We have illustrated the four patterns in the following subsections; the last
    two patterns can be combined into a hybrid hierarchical secure aggregator.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下小节中展示了四种模式；最后两种模式可以组合成一个混合的分层安全聚合器。
- en: An asynchronous aggregator
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 异步聚合器
- en: This asynchronous global model aggregation strategy, as shown in the following
    figure, enables us to speed up the model aggregation with the arrival of a new
    model update without waiting for the models that are trained locally the clients.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这种异步全局模型聚合策略，如下图所示，使我们能够在新的模型更新到来时加速模型聚合，而无需等待在客户端本地训练的模型。
- en: '![Figure 12.9 – FL model deployment design pattern – asynchronous aggregator](img/Figure_12.09_B18681.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图12.9 – FL模型部署设计模式 – 异步聚合器](img/Figure_12.09_B18681.jpg)'
- en: Figure 12.9 – FL model deployment design pattern – asynchronous aggregator
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9 – FL模型部署设计模式 – 异步聚合器
- en: This mechanism differs from conventional FL by allowing clients to skip the
    first aggregation round, where clients engage in updating models asynchronously
    during the next (second or successive) aggregation phase. Hence, it accommodates
    the variability of clients in terms of their computational resources, bandwidth
    availability, and communication capacities. The clients enjoy a definite advantage
    when participating in global model aggregation where the model convergence process
    by the server is not impacted by delays.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这一机制与传统的FL有所不同，它允许客户端跳过第一次聚合轮次，在下一次（第二次或后续）聚合阶段中客户端异步地更新模型。因此，它能够适应客户端在计算资源、带宽可用性和通信能力方面的差异。客户端在参与全局模型聚合时享有明显优势，因为服务器的模型收敛过程不受延迟的影响。
- en: The challenge of employing this strategy results in a bias in the global model,
    which may impact the model’s quality if important information is eliminated. In
    addition, the maximum aggregation latency at the server is dictated by when the
    slowest client sends the updates, leading to slow model convergence.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此策略所面临的挑战会导致全局模型产生偏差，如果重要信息被消除，可能会影响模型的质量。此外，服务器上的最大聚合延迟由最慢客户端发送更新的时间决定，从而导致模型收敛速度缓慢。
- en: However, some of the key benefits include centralized aggregation happening
    at each round instead of waiting for the slowest client. We also see reduced bandwidth
    usage during each round, as few clients send their updates asynchronously.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法的一些主要优点包括在每轮聚合中都进行集中聚合，而不是等待最慢的客户端。我们还看到，每轮的带宽使用量减少，因为只有少数客户端异步发送它们的更新。
- en: Examples of design patterns include **Asynchronous Online Federated Learning**
    and **Asynchronous** **Federated Optimization**.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 设计模式的例子包括**异步在线联邦学习**和**异步** **联邦优化**。
- en: A decentralized aggregator
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 去中心化聚合器
- en: This aggregation strategy, as demonstrated in the following figure, follows
    a decentralized FL approach by removing the dependency on a central server that
    turns out to be the single point of failure. Here, the central server may experience
    additional loads, as it receives and aggregates updates from all clients. The
    figure illustrates how a blockchain and a smart contract can be used to receive
    model updates when updates are shared with neighboring devices. In conventional
    FL training, the system may suffer from privacy limitations when not all participating
    clients trust the central server.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，这种聚合策略遵循去中心化联邦学习（FL）方法，通过去除对中央服务器的依赖，避免了单点故障的风险。在这种架构中，中央服务器可能会遇到额外的负载，因为它需要接收并聚合来自所有客户端的更新。图示展示了区块链和智能合约如何在更新共享给相邻设备时接收模型更新。在传统的FL训练中，当并非所有参与的客户端都信任中央服务器时，系统可能会受到隐私限制的影响。
- en: '![Figure 12.10 – FL model deployment design pattern – decentralized aggregator](img/Figure_12.10_B18681.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图12.10 – FL模型部署设计模式 – 去中心化聚合器](img/Figure_12.10_B18681.jpg)'
- en: Figure 12.10 – FL model deployment design pattern – decentralized aggregator
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10 – FL模型部署设计模式 – 去中心化聚合器
- en: This decentralized approach is recommended where model updates are allowed between
    peer client devices. However, this architecture also requires a proper definition
    of a decentralized model management system, where peer systems can collect, store,
    examine, and aggregate the local models. Moreover, we also need to define system
    ownership, through which a random client is selected to perform the aggregation
    from the local nearby clients and send the aggregated model to the client network.
    The blockchain serves as one of the best options for decentralized FL, where it
    can store immutable models and the learning coordinator can maintain the blockchain.
    A blockchain mechanism is reliable, accountable, and trustworthy, thereby increasing
    resiliency to adversarial attacks, trust, and transparency.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐使用这种去中心化方法，其中允许客户端设备之间进行模型更新。然而，这种架构还需要合理定义去中心化的模型管理系统，在该系统中，同行设备能够收集、存储、检查并聚合本地模型。此外，我们还需要定义系统所有权，通过该方式随机选择一个客户端执行来自本地相邻客户端的聚合操作，并将聚合后的模型发送到客户端网络。区块链作为去中心化FL的最佳选项之一，可以存储不可变的模型，学习协调者可以维护区块链。区块链机制可靠、可追溯且值得信赖，从而增强了对抗攻击的韧性、信任度和透明度。
- en: One of the major drawbacks of this system is the latency involved due to the
    blockchain consensus protocols during the model aggregation process. In addition,
    client devices may also experience power drainage due to their parallel participation
    in training and model aggregation. Even this kind of decentralized architecture
    runs the risk of clients exposing sensitive information about their peers to others.
    One practical example of this mode of peer-to-peer learning is BrainTorrent, in
    which clients engage in direct learning and communication with each other.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 该系统的主要缺点之一是由于区块链共识协议在模型聚合过程中的延迟。此外，客户端设备还可能因为参与训练和模型聚合的并行性而导致电池耗电。即便是这种去中心化架构，也存在客户端可能泄露其同伴的敏感信息的风险。此类点对点学习模式的一个实际例子是BrainTorrent，在该模式中，客户端之间直接进行学习和交流。
- en: A security aggregator
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安全聚合器
- en: This aggregation strategy supports built-in security protocols to protect the
    model better than vanilla FL without any support for data encryption. This is
    in contrast to conventional FL, which does not take data encryption and the secure
    exchange of parameters into consideration and leaves room for unauthorized data
    access. To prevent malicious clients from joining the training process and curb
    poisoning attacks on the data/model, we need to ensure that the proper security
    protocols are in place as and when models send updates to the central server.
    The best way to ensure that model parameters and gradients are not accessible
    to third parties is to employ secure, multi-party computation for model exchanges
    and aggregations to guarantee that each participating client is aware of its model
    input and output. We can also employ homomorphic encryption to allow the client
    to encrypt and the server to decrypt the model. Application-level security mechanisms
    such as pairwise masking and differential privacy help reduce data leakages to
    external adversaries.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这种聚合策略支持内置的安全协议，比传统的联邦学习（FL）更好地保护模型，且不需要数据加密的支持。这与传统的联邦学习形成对比，后者没有考虑数据加密和参数的安全交换，容易留下未经授权的数据访问空间。为了防止恶意客户端加入训练过程并遏制对数据/模型的污染攻击，我们需要确保在模型向中央服务器发送更新时，合适的安全协议已到位。确保模型参数和梯度不被第三方访问的最佳方式是采用安全的多方计算来进行模型交换和聚合，确保每个参与客户端都能意识到自己的模型输入和输出。我们还可以使用同态加密，使客户端能够加密，服务器能够解密模型。应用级安全机制，如成对遮蔽和差分隐私，有助于减少数据泄露给外部对手的风险。
- en: Some of the drawbacks of this mode of architecture include the reduction in
    system efficiency and lower model accuracy at the cost of the addition of extra
    security protocols and encryption methods. Hence, we need to choose the right
    privacy thresholds to balance the trade-off between the model’s performance and
    privacy. The Secure Aggregation protocol (developed by Google) serves as one of
    the prime examples of a secured aggregation protocol in FL.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构模式的一些缺点包括由于增加额外的安全协议和加密方法，导致系统效率降低和模型准确性下降。因此，我们需要选择合适的隐私阈值，以平衡模型性能和隐私之间的权衡。谷歌开发的安全聚合协议（Secure
    Aggregation）就是联邦学习中一种安全聚合协议的典型例子。
- en: A hierarchical aggregator
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层次化聚合器
- en: This aggregation technique adds an extra hierarchical aggregator layer (such
    as an edge server) to reduce the impact of non-IID on the global model and increase
    the system’s efficiency. The following figure illustrates how we might employ
    edge network 1 and edge network 2 to carry out partial aggregation from nearby
    client devices before triggering the global aggregation process. This design pattern,
    with intermediate servers at the edge networks, has been introduced to handle
    slow communication between clients and distant servers to improve the system’s
    efficiency. Involving a hierarchical aggregator in the process helps scale FL
    systems and facilitates a better global model aggregation, as the server can assemble
    local models from clients that have similar data heterogeneity.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这种聚合技术增加了一个额外的层次化聚合器层（如边缘服务器），以减少非独立同分布（non-IID）对全局模型的影响，并提高系统效率。下图展示了我们如何使用边缘网络1和边缘网络2在触发全局聚合过程之前，先从附近的客户端设备进行部分聚合。这种设计模式引入了位于边缘网络的中间服务器，用于处理客户端与远程服务器之间的慢速通信，从而提高系统效率。引入层次化聚合器有助于扩展联邦学习系统，并促进更好的全局模型聚合，因为服务器可以将来自数据异质性相似的客户端的本地模型进行组合。
- en: The main drawback of this kind of architecture is the system reliability when
    devices are disconnected from the edge servers, which impacts the model training
    and performance. In addition, we need to be extra cautious about the security
    protocols, as edge servers can have security breaches and are more often subjected
    to network security threats.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构的主要缺点是当设备与边缘服务器断开连接时，系统的可靠性受到影响，进而影响模型的训练和性能。此外，我们还需要特别小心安全协议，因为边缘服务器可能会出现安全漏洞，并且更容易受到网络安全威胁。
- en: 'One real-world example of an FL model that directly uses this architecture
    is Hierarchical FedAvg, where multiple edge servers are employed for partial model
    aggregation depending on updates that are incrementally received from the clients:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 一个直接使用这种架构的联邦学习模型的实际例子是层次化联邦平均（Hierarchical FedAvg），在该模型中，根据从客户端逐步接收的更新，采用多个边缘服务器进行部分模型聚合：
- en: '![Figure 12.11 – FL model deployment design pattern – hierarchical aggregator](img/Figure_12.11_B18681.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.11 – FL模型部署设计模式 – 层级聚合器](img/Figure_12.11_B18681.jpg)'
- en: Figure 12.11 – FL model deployment design pattern – hierarchical aggregator
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.11 – FL模型部署设计模式 – 层级聚合器
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned about the rising importance of sustainability
    when developing AI solutions, as well as the desired cloud metrics and architectural
    and operational know-how that can help us to reduce CO2 emissions. We saw a detailed
    overview of how FL models can be trained based on energy availability at the clients'
    end to reduce the detrimental impact of higher CO2 emissions. We are now aware
    of efficient FL-based design patterns, whether training, model management, or
    model aggregation patterns.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了在开发AI解决方案时可持续性日益重要的背景，以及可以帮助我们减少二氧化碳排放的所需云计算指标、架构和操作技能。我们详细概述了如何根据客户端的能源可用性训练FL模型，以减少较高二氧化碳排放的负面影响。现在我们已经了解了高效的基于FL的设计模式，无论是训练、模型管理，还是模型聚合模式。
- en: In the context of training and deployment strategies for FL, we also got the
    chance to explore the benefits of different DL-based training optimizers and their
    impact on the sustainability of solutions. Furthermore, we walked through the
    sustainability factors in centralized learning versus FL, which can help us to
    estimate the CO2 footprint of the GPU compute using the specification of the type
    of hardware, its efficiency, the active runtime period, the model architecture,
    cooling needs, the cloud provider, and the region. We explored how to calculate
    the total energy consumed, the different emission metrics we can use, and the
    best practices we can follow to maximize our data centers’ efficiency.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在联邦学习（FL）的训练和部署策略的背景下，我们还获得了探索基于深度学习（DL）的不同训练优化器的优势及其对解决方案可持续性影响的机会。此外，我们还探讨了集中式学习与联邦学习（FL）中的可持续性因素，这有助于我们使用硬件规格、效率、活动运行时、模型架构、冷却需求、云服务提供商和地区等信息来估算GPU计算的二氧化碳排放量。我们还探索了如何计算总能耗、可以使用的不同排放指标，以及我们可以遵循的最佳实践，以最大化数据中心的效率。
- en: In the next chapter, we will go deeper into exploring how to ensure sustainability
    while creating feature stores.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨在创建特征存储时如何确保可持续性。
- en: Further reading
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine*
    *Learning*: [https://jmlr.org/papers/volume21/20-312/20-312.pdf](https://jmlr.org/papers/volume21/20-312/20-312.pdf)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*朝着系统化报告机器* *学习的能量和碳足迹*: [https://jmlr.org/papers/volume21/20-312/20-312.pdf](https://jmlr.org/papers/volume21/20-312/20-312.pdf)'
- en: '*How Can I Calculate CO*2*eq emissions for my Azure* *VM?*: [https://devblogs.microsoft.com/sustainable-software/how-can-i-calculate-CO2eq-emissions-for-my-azure-vm/](https://devblogs.microsoft.com/sustainable-software/how-can-i-calculate-CO2eq-emissions-for-my-azure-vm/)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如何计算我的Azure* *虚拟机的CO2等效排放量？*: [https://devblogs.microsoft.com/sustainable-software/how-can-i-calculate-CO2eq-emissions-for-my-azure-vm/](https://devblogs.microsoft.com/sustainable-software/how-can-i-calculate-CO2eq-emissions-for-my-azure-vm/)'
- en: '*A Framework for Sustainable Federated Learning,* B. Güler and A. Yener: [https://dl.ifip.org/db/conf/wiopt/wiopt2021/WiOpt_2021_paper_100-invited.pdf](https://dl.ifip.org/db/conf/wiopt/wiopt2021/WiOpt_2021_paper_100-invited.pdf),
    2021 19th International Symposium on Modeling and Optimization in Mobile, Ad hoc,
    and Wireless Networks (WiOpt)'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可持续联邦学习框架,* B. Güler 和 A. Yener: [https://dl.ifip.org/db/conf/wiopt/wiopt2021/WiOpt_2021_paper_100-invited.pdf](https://dl.ifip.org/db/conf/wiopt/wiopt2021/WiOpt_2021_paper_100-invited.pdf),
    2021 第19届国际移动、自治和无线网络建模与优化研讨会（WiOpt）'
- en: '*Sustainable federated* *learning*: [https://arxiv.org/pdf/2102.11274.pdf](https://arxiv.org/pdf/2102.11274.pdf)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可持续的联邦* *学习*: [https://arxiv.org/pdf/2102.11274.pdf](https://arxiv.org/pdf/2102.11274.pdf)'
- en: '*Cloud* *sustainability*: [https://cloud.google.com/sustainability](https://cloud.google.com/sustainability)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*云* *可持续性*: [https://cloud.google.com/sustainability](https://cloud.google.com/sustainability)'
- en: '*How carbon-free is your cloud? New data lets you* *know*: [https://cloud.google.com/blog/topics/sustainability/sharing-carbon-free-energy-percentage-for-google-cloud-regions](https://cloud.google.com/blog/topics/sustainability/sharing-carbon-free-energy-percentage-for-google-cloud-regions)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你的云服务有多无碳？新的数据让你* *知道*: [https://cloud.google.com/blog/topics/sustainability/sharing-carbon-free-energy-percentage-for-google-cloud-regions](https://cloud.google.com/blog/topics/sustainability/sharing-carbon-free-energy-percentage-for-google-cloud-regions)'
- en: '*CO*2 *in Federated* *Learning*: [https://mlsys.cst.cam.ac.uk/carbon_fl/](https://mlsys.cst.cam.ac.uk/carbon_fl/)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*联邦* *学习中的CO2*: [https://mlsys.cst.cam.ac.uk/carbon_fl/](https://mlsys.cst.cam.ac.uk/carbon_fl/)'
- en: '*Can Federated Learning Save The* *Planet?* [https://arxiv.org/pdf/2010.06537.pdf](https://arxiv.org/pdf/2010.06537.pdf)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*联邦学习能拯救* *地球吗？* [https://arxiv.org/pdf/2010.06537.pdf](https://arxiv.org/pdf/2010.06537.pdf)'
- en: '*Architectural Patterns for the Design of Federated Learning* *Systems*: [https://www.researchgate.net/publication/348316341_Architectural_Patterns_for_the_Design_of_Federated_Learning_Systems](https://www.researchgate.net/publication/348316341_Architectural_Patterns_for_the_Design_of_Federated_Learning_Systems)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*联邦学习系统设计的架构模式*：[https://www.researchgate.net/publication/348316341_Architectural_Patterns_for_the_Design_of_Federated_Learning_Systems](https://www.researchgate.net/publication/348316341_Architectural_Patterns_for_the_Design_of_Federated_Learning_Systems)'
