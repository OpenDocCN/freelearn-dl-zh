- en: Exploiting ML-Agents
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用 ML-Agents
- en: At some point, we need to move beyond building and training agent algorithms
    and explore building our own environments. Building your own environments will
    also give you more experience in making good reward functions. We have virtually
    omitted this important question in **Reinforcement Learning** (**RL**) and **Deep
    Reinforcement Learning** (**DRL**) and that is what makes a good reward function.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时候，我们需要超越构建和训练代理算法，探索构建我们自己的环境。构建自己的环境也将使你在制作良好的奖励函数方面获得更多经验。我们在**强化学习**（**RL**）和**深度强化学习**（**DRL**）中几乎忽略了这个问题，而这正是制作良好奖励函数的关键。
- en: In this chapter, we will look to answer the question of what makes a good reward
    function or what a reward function is. We will talk about reward functions by
    building new environments with the Unity game engine. We will start by installing
    and setting up Unity ML-Agents, an advanced DRL kit for building agents and environments.
    From there, we will look at how to build one of the standard Unity demo environments
    for our use with our PyTorch models. Conveniently, this leads us to working with
    the ML-Agents toolkit for using a Unity environment from Python and PyTorch with
    our previously explored Rainbow DQN model. After that, we will look at creating
    a new environment, and then finish this chapter by looking at advances Unity has
    developed for furthering RL.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨什么使奖励函数良好，或者奖励函数是什么。我们将通过使用 Unity 游戏引擎构建新环境来讨论奖励函数。我们将首先安装和设置 Unity
    ML-Agents，这是一个用于构建代理和环境的高级 DRL 工具包。从那里，我们将探讨如何构建一个标准的 Unity 示例环境，以便我们使用 PyTorch
    模型。方便的是，这使我们能够使用 ML-Agents 工具包从 Python 和 PyTorch 中使用 Unity 环境，与之前探索的 Rainbow DQN
    模型一起。之后，我们将探讨创建一个新环境，然后通过探讨 Unity 为推进 RL 所开发的进步来结束本章。
- en: 'Here are the main topics we will cover in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Installing ML-Agents
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 ML-Agents
- en: Building a Unity environment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建 Unity 环境
- en: Training a Unity environment with Rainbow
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Rainbow 训练 Unity 环境
- en: Creating a new environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个新环境
- en: Advancing RL with ML-Agents
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ML-Agents 推进 RL
- en: Unity is the largest and most frequently used game engine for game development.
    You likely already know this if you are a game developer. The game engine itself
    is developed with C++ but it provides a scripting interface in C# that 99% of
    its game developers use. As such, we will need to expose you to some C# code in
    this chapter, but just a tiny amount.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Unity 是游戏开发中最大和最常用的游戏引擎。如果你是游戏开发者，你很可能已经知道这一点。游戏引擎本身是用 C++ 开发的，但它提供了一个 C# 脚本接口，99%
    的游戏开发者都在使用它。因此，在本章中，我们需要向您展示一些 C# 代码，但只是很少量。
- en: In the next section, we'll install Unity and the ML-Agents toolkit.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将安装 Unity 和 ML-Agents 工具包。
- en: Installing ML-Agents
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 ML-Agents
- en: Installing Unity, the game engine itself, is not very difficult, but when working
    with ML-Agents, you need to be careful when you pick your version. As such, the
    next exercise is intended to be more configurable, meaning you may need to ask/answer
    questions while performing the exercise. We did this to make this exercise longer
    lasting since this toolkit has been known to change frequently with many breaking
    changes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 安装游戏引擎本身（Unity）并不困难，但在与 ML-Agents 一起工作时，您在选择版本时需要小心。因此，下一个练习旨在更具可配置性，这意味着您在执行练习时可能需要提问/回答问题。我们这样做是为了使这个练习更持久，因为这个工具包已知经常发生许多破坏性更改。
- en: 'Unity will run on any major desktop computer (Windows, Mac, or Linux), so open
    your development computer and follow along with the next exercise to install Unity
    and the ML-Agents toolkit:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Unity 可以在任何主流桌面计算机（Windows、Mac 或 Linux）上运行，所以打开您的开发计算机，按照下一个练习安装 Unity 和 ML-Agents
    工具包：
- en: Before installing Unity, check the ML-Agents GitHub installation page ([https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md)) and
    confirm which version of Unity is currently supported. At the time of writing,
    this is 2017.4, and we will prefer to use only that version even though the documentation suggests
    later versions are supported.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在安装 Unity 之前，请检查 ML-Agents GitHub 安装页面 ([https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md))
    并确认当前支持哪个版本的 Unity。在撰写本文时，这是 2017.4 版本，尽管文档建议支持后续版本，但我们更倾向于只使用那个版本。
- en: You can download and install Unity directly or through the Unity Hub. Since
    managing multiple versions of Unity is so common, Unity built a management app,
    the Unity Hub, for this purpose.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以直接下载并安装 Unity，或者通过 Unity Hub 进行安装。由于管理多个版本的 Unity 非常常见，Unity 为此目的构建了一个管理应用程序，即
    Unity Hub。
- en: Download and install the required minimum version of Unity. If you have never
    installed Unity, you will need to create a user account and verify their license
    agreement. After you create a user account, you will be able to run the Unity
    editor.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并安装所需的最低版本的 Unity。如果您从未安装过 Unity，您将需要创建一个用户帐户并验证其许可协议。创建用户帐户后，您将能够运行 Unity
    编辑器。
- en: 'Open a Python/Anaconda command shell and make sure to activate your virtual
    environment with the following command:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开 Python/Anaconda 命令行，并确保使用以下命令激活您的虚拟环境：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Install the Unity Gym wrapper with the following command:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装 Unity Gym 包装器：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Change to a root working folder, preferably `C:` or `/`, and create a directory
    for cloning the ML-Agents toolkit into with the following command:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到根工作文件夹，最好是 `C:` 或 `/`，并使用以下命令创建一个目录以克隆 ML-Agents 工具包：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, assuming you have `git` installed, use `git` to pull down the ML-Agents
    toolkit with this:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，假设您已安装 `git`，使用以下命令使用 `git` 拉取 ML-Agents 工具包：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The reason we prefer a root folder is that the ML-Agents directory structure
    can get quite deep and this may cause too long filename errors on some operating
    systems.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更喜欢根目录的原因是 ML-Agents 目录结构可能相当深，这可能在某些操作系统中导致文件名过长错误。
- en: Testing the entire installation is best done by consulting the current Unity
    docs and using their most recent guide. A good place to start is the first example
    environment, the 3D Balance Ball. You can find this document at [https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md).
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过查阅当前 Unity 文档并使用他们最新的指南来测试整个安装效果最佳。一个好的起点是第一个示例环境，3D 平衡球。您可以在[https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md)找到此文档。
- en: Take some time and explore the ML-Agents toolkit on your own. It is meant to
    be quite accessible and if your only experience in DRL is this book, you should
    have plenty of background by now to understand the general gist of running Unity
    environments. We will review some of these procedures but there are plenty of
    other helpful guides out there that can help you run ML-Agents in Unity. Our priority
    here will be using Unity to build environments and possibly new environments we
    can use to test our models on. While we won't use the ML-Agents toolkit to train
    agents, we will use the Gym wrappers, which do require knowledge of what a brain
    or academy is.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 抽出一些时间，自己探索 ML-Agents 工具包。它旨在易于访问，如果您在 DRL 方面的唯一经验是这本书，那么您现在应该有足够的背景知识来理解运行
    Unity 环境的一般概念。我们将回顾一些这些流程，但还有许多其他有用的指南可以帮助您在 Unity 中运行 ML-Agents。我们在这里的优先事项将是使用
    Unity 构建环境，以及可能的新环境，我们可以使用这些环境来测试我们的模型。虽然我们不会使用 ML-Agents 工具包来训练智能体，但我们将使用 Gym
    包装器，这确实需要了解大脑或学院是什么。
- en: Adam Kelly has an excellent blog, Immersive limit ([http://www.immersivelimit.com/tutorials/tag/Unity+ML+Agents](http://www.immersivelimit.com/tutorials/tag/Unity+ML+Agents)),
    devoted to machine learning and DRL with a specialization of creating very cool
    ML-Agents environments and projects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Adam Kelly 拥有一个优秀的博客，沉浸式极限 ([http://www.immersivelimit.com/tutorials/tag/Unity+ML+Agents](http://www.immersivelimit.com/tutorials/tag/Unity+ML+Agents))，专注于机器学习和
    DRL，并专注于创建非常酷的 ML-Agents 环境和项目。
- en: ML-Agents currently uses PPO and Soft Actor-Critic methods to train agents.
    It also provides several helpful modules for state encoding using convolutional
    and recurrent networks, hence allowing for visual observation encoding and memory
    or context. Additionally, it provides methods for defining discrete or continuous
    action spaces, as well as enabling mixing action or observation types. The toolkit
    is extremely well done but, with the rapidly changing landscape of ML, it has
    become quickly outdated and/or perhaps just out-hyped. In the end, it also appears
    that most researchers or serious practitioners of DRL just want to build their
    own frameworkll for now.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents 目前使用 PPO 和 Soft Actor-Critic 方法来训练代理。它还提供了使用卷积和循环网络进行状态编码的几个有用模块，从而允许进行视觉观察编码和记忆或上下文。此外，它提供了定义离散或连续动作空间的方法，以及启用混合动作或观察类型。这个工具包做得非常好，但随着机器学习领域的快速变化，它已经变得很快过时，或者可能只是被炒作得过多。最终，似乎大多数研究人员或严肃的
    DRL 实践者现在只想构建自己的框架。
- en: While DRL is quite complicated, the amount of code to make something powerful
    is still quite small. Therefore, we will likely see a plethora of RL frameworks
    trying to gain a foothold on the space. Whether you decide to use one of these
    frameworks or build your own is up to you. Just remember that frameworks come
    and go, but the more underlying knowledge you have on a topic, the better your
    ability to guide future decisions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 DRL 非常复杂，但制作出强大功能的代码量仍然相当小。因此，我们可能会看到许多 RL 框架试图在这个领域站稳脚跟。无论您决定使用这些框架中的哪一个，还是构建自己的框架，这完全取决于您。只需记住，框架有来有去，但您对某个主题的底层知识越多，您指导未来决策的能力就越强。
- en: Regardless of whether you decide to use the ML-Agents framework for training
    DRL agents, use another framework, or build your own, Unity provides you with
    an excellent opportunity to build new and more exciting environments. We learn how
    to build a Unity environment we can train with DRL in the next section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您决定是否使用 ML-Agents 框架来训练 DRL 代理，使用其他框架，还是构建自己的框架，Unity 都为您提供了构建新的、更令人兴奋的环境的绝佳机会。我们将在下一节学习如何构建一个可以使用
    DRL 训练的 Unity 环境。
- en: Building a Unity environment
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建 Unity 环境
- en: The ML-Agents toolkit provides not only a DRL training framework but also a
    mechanism to quickly and easily set up AI agents within a Unity game. Those agents
    can then be externally controlled through a Gym interface—yes, that same interface
    we used to train most of our previous agent/algorithms. One of the truly great
    things about this platform is that Unity provides several new demo environments
    that we can explore. Later, we will look at how to build our own environments
    for training agents.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents 工具包不仅提供了一个 DRL 训练框架，还提供了一种在 Unity 游戏中快速轻松地设置 AI 代理的机制。这些代理可以通过 Gym
    接口外部控制——是的，就是我们在训练大多数先前代理/算法时使用的同一个接口。这个平台真正伟大的地方之一是 Unity 提供了几个我们可以探索的新演示环境。稍后，我们将探讨如何为训练代理构建自己的环境。
- en: The exercises in this section are meant to summarize the setup steps required
    to build an executable environment to train with Python. They are intended for
    newcomers to Unity who don't want to learn all about Unity to just build a training
    environment. If you encounter issues using these exercises, it is likely the SDK
    may have changed. If that is the case, then just revert back and consult the full
    online documentation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的练习旨在总结构建用于 Python 训练的可执行环境所需的设置步骤。它们是为那些只想构建训练环境而不想学习 Unity 所有知识的 Unity
    新手准备的。如果您在使用这些练习时遇到问题，那么 SDK 可能已经发生了变化。如果是这种情况，那么只需回退并查阅完整的在线文档。
- en: 'Building a Unity environment for agent training requires a few specialized
    steps we will cover in this exercise:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 构建用于代理训练的 Unity 环境需要一些专门的步骤，我们将在本练习中介绍：
- en: First, open the Unity editor, either through the Unity Hub or just Unity itself.
    Remember to use a version that supports the ML-Agents toolkit.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，打开 Unity 编辑器，无论是通过 Unity Hub 还是直接使用 Unity 本身。请记住使用支持 ML-Agents 工具包的版本。
- en: 'Using the Unity Hub, we can add the project using the **Add** button, as shown
    in the following screenshot:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Unity Hub，我们可以通过以下截图所示的 **添加** 按钮添加项目：
- en: '![](img/d317976a-e439-4a1f-b61f-a33e2ea27a23.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d317976a-e439-4a1f-b61f-a33e2ea27a23.png)'
- en: Adding a project in the Unity Hub
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Unity Hub 中添加项目
- en: After you click **Add**, you will be prompted to locate the project folder.
    Use the dialog to find and select the `UnitySDK` project folder we pulled down
    with `git` in the previous exercise. This folder should be located in your `/mlagents/ml-agents/UnitySDK`
    folder.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**添加**后，您将被提示定位项目文件夹。使用对话框找到并选择我们在之前的练习中用`git`下载的`UnitySDK`项目文件夹。这个文件夹应该位于您的`/mlagents/ml-agents/UnitySDK`文件夹中。
- en: When the project has been added, it will also be added to the top of the list
    of projects. You will likely see a warning icon indicating you need to select
    the version number. Select a version of Unity that coincides with the ML-Agents
    toolkit and then select the project to launch it in the editor.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当项目被添加后，它也将被添加到项目列表的顶部。您可能会看到一个警告图标，表示您需要选择版本号。选择与ML-Agents工具包相匹配的Unity版本，然后选择项目以在编辑器中启动它。
- en: You may be prompted to **Upgrade** the project. If you are prompted, then select
    **Yes** to do so. If the upgrade fails or the project won't run right, then you
    can just delete all of the old files and try again with a different version of
    Unity. Loading this project may take some time, so be patient, grab a drink, and
    wait.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可能会被提示**升级**项目。如果您被提示，请选择**是**以进行升级。如果升级失败或项目无法正常运行，您可以简单地删除所有旧文件，并尝试使用不同版本的Unity。加载此项目可能需要一些时间，所以请耐心等待，拿杯饮料，然后等待。
- en: After the project finishes loading and the editor opens, open the scene for
    the `3DBall` environment located in the `Assets/ML-Agents/Examples/Hallway/Scenes`
    folder by double-clicking on the `3DBall` scene file.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 项目加载完成并打开编辑器后，通过双击位于`Assets/ML-Agents/Examples/Hallway/Scenes`文件夹中的`3DBall`场景文件来打开`3DBall`环境的场景。
- en: 'We need to set the Academy to control the brain, that is, allow the brain to
    be trained. To do that, select the **Academy**, then locate the **Hallway Academy**
    component in the **Inspector** window, and select the **Control** option, as shown
    in the following screenshot:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要将学院设置为控制大脑，也就是说，允许大脑进行训练。为此，选择**学院**，然后在**检查器**窗口中找到**走廊学院**组件，并选择**控制**选项，如图所示：
- en: '![](img/d5f6d096-ee5f-4a4a-ae6e-bd4c2e820467.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d5f6d096-ee5f-4a4a-ae6e-bd4c2e820467.png)'
- en: Setting the academy to control the brain
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将学院设置为控制大脑
- en: 'Next, we need to modify the run parameters for the environment. The idea here
    is that we will build the Unity environment as an executable game that we can
    then use the wrappers on to train an agent to play. However, to do that, we need
    to make some assumptions about the game:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要修改环境的运行参数。这里的想法是，我们将构建Unity环境作为一个可执行的游戏，然后我们可以使用包装器来训练智能体进行游戏。但是，为了做到这一点，我们需要对游戏做一些假设：
- en: The game is windowless and runs in the background.
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 游戏是无窗口的，在后台运行。
- en: Any player actions need to be controlled by the agent. A dialog prompts for
    warnings, errors, or anything else that must be avoided.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何玩家动作都需要由智能体控制。对话框会提示警告、错误或其他必须避免的事情。
- en: Make sure that the training scene is loaded first.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保首先加载训练场景。
- en: Before finishing that though, turn the **Control** option back off or on for
    the academy and run the scene by pressing the **Play** button at the top of the
    interface. You will be able to observe an already trained agent play through the
    scene. Make sure to turn the **Control** option back on when you are done viewing
    the agent play.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完成那之前，请将学院的**控制**选项关闭或打开，然后通过界面顶部的**播放**按钮运行场景。您将能够观察到一个已经训练好的智能体在场景中玩耍。完成观看后，请确保将**控制**选项重新打开。
- en: Now, the ML-Agents toolkit will allow you to train directly from here by just
    running a separate Python command shell and script controlling the editor. As
    of yet, this is not possible and our only way to run an environment is with wrappers.
    In the next section, we will finish setting up the environment by setting some
    final parameters and building them.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，ML-Agents工具包将允许您通过运行一个单独的Python命令行和脚本控制编辑器来直接从这里进行训练。到目前为止，这还不可能，我们运行环境的唯一方法是通过包装器。在下一节中，我们将通过设置一些最终参数并构建它们来完成环境的设置。
- en: Building for Gym wrappers
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为Gym包装器构建
- en: 'Configuring the setup of an environment just requires setting a few additional
    parameters. We will learn how to do this in the following exercise:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 配置环境的设置只需设置一些额外的参数。我们将在接下来的练习中学习如何做到这一点：
- en: From the editor menu, select **Edit | Project Settings... **to open the **Project
    Settings** window. You can anchor this window or close it after you've finished
    editing.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从编辑菜单中选择**编辑 | 项目设置...**以打开**项目设置**窗口。您可以在编辑完成后锚定此窗口或关闭它。
- en: Select the **Player** option. Player, in this case, denotes the player or game
    runner—not to be confused with an actual human player. Change the text in the
    **Company Name** and **Product Name** fields to `GameAI`.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**玩家**选项。在这里，玩家指的是玩家或游戏运行者——不要与实际的人类玩家混淆。将**公司名称**和**产品名称**字段中的文本更改为`GameAI`。
- en: 'Open the **Resolution and Presentation** section and then be sure that **Run
    In Background*** is checked and **Display Resolution Dialog** is set to **Disabled**,
    as shown in the following screenshot:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开**分辨率和显示**部分，并确保**在后台运行**被勾选，**显示分辨率对话框**设置为**禁用**，如下面的截图所示：
- en: '![](img/61c81f5c-cebf-46de-82d9-f7e36ab8e030.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/61c81f5c-cebf-46de-82d9-f7e36ab8e030.png)'
- en: Setting the Player settings
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 设置玩家设置
- en: Close the dialog or anchor it and then, from the menu, select **File | Build
    Settings**.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭对话框或将其锚定，然后从菜单中选择**文件 | 构建设置**。
- en: 'Click the **Add Open Scene** button and be sure to select your default platform
    for training. This should be a desktop environment you can easily run with Python.
    The following screenshot shows the Windows option by default:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**添加打开场景**按钮，并确保选择你的默认训练平台。这应该是一个你可以轻松用 Python 运行的桌面环境。下面的截图显示了默认的 Windows
    选项：
- en: '![](img/8ebf8c6e-436c-42ef-8df8-901d5cdd67c5.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8ebf8c6e-436c-42ef-8df8-901d5cdd67c5.png)'
- en: Building the scene into a game environment
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 将场景构建成游戏环境
- en: Click the Build button at the bottom of the dialog to build the executable environment.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击对话框底部的构建按钮来构建可执行环境。
- en: You will be prompted to save the output to a folder. Be sure to note the location
    of this folder and/or save it someplace accessible. A good suggested location
    is the `mlagents` root folder in `/mlagents`. Create a new folder called `desktop`and
    save the output there.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将被提示将输出保存到文件夹中。请确保注意此文件夹的位置，并将其保存在可访问的地方。一个建议的位置是`/mlagents`中的`mlagents`根文件夹。创建一个名为`desktop`的新文件夹并将输出保存在那里。
- en: The environment should be built and runnable now as a Gym environment. We will
    set up this environment and start training it as an environment in the next section.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 环境现在应该作为 Gym 环境构建并可运行。我们将在下一节中设置此环境并开始对其进行训练。
- en: Training a Unity environment with Rainbow
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Rainbow 训练 Unity 环境
- en: Training an agent to learn a Unity environment is not unlike much of the training
    we have already done. There are a few slight changes to the way we interact and
    set up the environment but overall it is much the same, which makes it a further
    plus for us because now we can go back and train several different agents/algorithms
    on completely new environments that we can even design. Furthermore, we now can
    use other DRL frameworks to train agents with Python— outside the ML-Agents agents,
    that is. We will cover more on using other frameworks in [Chapter 12](6d061d35-176a-421a-9b62-aed35f48a6b7.xhtml),
    *DRL Frameworks*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个智能体来学习 Unity 环境并不像我们已经做过的许多训练那样。我们在交互和设置环境的方式上做了一些细微的调整，但总体上还是相同的，这使我们受益匪浅，因为我们现在可以回到过去，在完全新的环境中训练几个不同的智能体/算法，甚至是我们自己设计的。此外，我们现在可以使用其他
    DRL 框架用 Python 训练智能体——即除了 ML-Agents 智能体之外。我们将在第 12 章[**DRL 框架**](6d061d35-176a-421a-9b62-aed35f48a6b7.xhtml)中详细介绍如何使用其他框架。
- en: 'In the next exercise, we see how to convert one of our latest and most state-of-the-art
    samples, `Chapter_10_Rainbow.py`, and turn it into `Chapter_11_Unity_Rainbow.py`.
    Open `Chapter_11_Unity_Rainbow.py` and follow the next exercise:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将看到如何将我们最新且最先进的样本之一`Chapter_10_Rainbow.py`转换为`Chapter_11_Unity_Rainbow.py`。打开`Chapter_11_Unity_Rainbow.py`并按照下一个练习进行操作：
- en: We first need to copy the output folder from the last build, the desktop folder,
    and place it in the same folder as this chapter's source code. This will allow
    us to launch that build as the environment our agent will train on.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先需要从最后的构建中复制输出文件夹，即桌面文件夹，并将其放置在与本章源代码相同的文件夹中。这将允许我们以该构建作为智能体训练的环境。
- en: 'Since you will likely want to convert a few of our previous samples to run
    Unity environments, we will go through the required changes step by step, starting
    first with the new import, as follows:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于你可能会想要将我们之前的一些样本转换为运行 Unity 环境，我们将逐步介绍所需的变化，首先从新的导入开始，如下所示：
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This imports the `UnityEnviroment` class, which is a Gym adapter to Unity.
    We next use this class to instantiate the `env` environment, like so; note that
    we have placed commented lines for other operating systems:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这导入了`UnityEnvironment`类，这是一个Unity的Gym适配器。接下来，我们使用这个类来实例化`env`环境，如下所示；注意，我们为其他操作系统放置了注释行：
- en: '[PRE5]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we get `brain` and `brain_name` from the environment. Unity uses the
    concept of a brain to control agents. We will explore agent brains in a later
    section. For now, realize that we just take the first available brain with the
    following code:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们从环境中获取`brain`和`brain_name`。Unity使用大脑的概念来控制智能体。我们将在后面的章节中探讨智能体大脑。现在，请记住我们只是用以下代码获取第一个可用的大脑：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, we extract the action (`action_size`) and state size (`state_size`) from
    the brain and use these as inputs to construct our `RainbowDQN` models, like so:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们从大脑中提取动作大小（`action_size`）和状态大小（`state_size`），并使用这些作为输入来构建我们的`RainbowDQN`模型，如下所示：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The last part we need to worry about is down in the training code and has to
    do with how the environment is reset. Unity allows for multiple agents/brains
    to run in concurrent environments concurrently, either as a way mechanism for
    A2C/A3C or other mechanisms. As such, it requires a bit more care as to which
    specific brain and mode we want to reset. The following code shows how we reset
    the environment when training Unity:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要关注的最后一部分是在训练代码中，与环境的重置方式有关。Unity允许多个智能体/大脑在并发环境中并发运行，这可以是A2C/A3C或其他机制的方式机制。因此，我们需要更加小心地确定我们想要重置的具体大脑和模式。以下代码展示了我们在训练Unity时如何重置环境：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As mentioned, the purpose of the slightly confusing indexing has to do with
    which brain/agent you want to pull the state from. Unity may have multiple brains
    training multiple agents in multiple sub-environments, all either working together
    or against each other. We will cover more about training multiple agent environments
    in [Chapter 14](a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml), *From DRL to AGI*.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，这种稍微有些令人困惑的索引目的在于确定你想要从哪个大脑/智能体中提取状态。Unity可能拥有多个大脑在多个子环境中训练多个智能体，这些智能体要么协同工作，要么相互竞争。我们将在第14章中详细介绍多个智能体环境的训练，即从DRL到AGI。
- en: 'We also have to change any other occurrences of when the environment may reset
    itself like in the following example when the algorithm checks whether the episode
    is done, with the following code:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还必须更改任何其他可能重置环境的情况，如下例所示，当算法检查是否完成一个回合时，使用以下代码：
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Run the code and watch the agent train. You won't see any visuals other than
    TensorBoard output, assuming you go through the steps and run TB in another shell,
    which you can likely do on your own by now.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行代码并观察智能体训练过程。除非你在另一个shell中运行TensorBoard并完成所有步骤，否则你将看不到除了TensorBoard输出之外的任何视觉内容，现在你很可能可以自己做到这一点。
- en: This example may be problematic to run due to API compatibility issues. If you
    encounter problems when running the sample, then try and set up a whole new virtual
    environment and install everything again. If the issue continues, then check online
    for help in places such as Stack Overflow or GitHub. Be sure to also refer to
    the latest Unity documentation on ML-Agents.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子可能由于API兼容性问题而难以运行。如果你在运行示例时遇到问题，那么尝试设置一个全新的虚拟环境并重新安装所有内容。如果问题仍然存在，那么在网上寻找帮助，例如在Stack
    Overflow或GitHub上。务必参考最新的Unity文档关于ML-Agents的部分。
- en: The real benefit of plugging in and using Unity is the ability to construct
    your own environments and then use those new environments with your own or another
    RL framework. In the next section, we will look at the basics of building your
    own RL environment with Unity ML-Agents.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 将Unity连接并使用它的真正好处是能够构建自己的环境，然后使用这些新环境与自己的或另一个RL框架一起使用。在下一节中，我们将探讨使用Unity ML-Agents构建自己的RL环境的基本知识。
- en: Creating a new environment
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个新环境
- en: The great thing about the ML-Agents toolkit is the ability it provides for creating
    new agent environments quickly and simply. You can even transform existing games
    or game projects into training environments for a range of purposes, from building
    full robotic simulations to simple game agents or even game agents that play as
    non-player characters. There is even potential to use DRL agents for game quality
    assurance testing. Imagine building an army of game testers that learn to play
    your game with just trial and error. The possibilities are endless and Unity is
    even building a full cloud-based simulation environment for running or training
    these agents in the future.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents工具包的伟大之处在于它提供了快速简单地创建新智能体环境的能力。你甚至可以将现有的游戏或游戏项目转换成用于各种目的的训练环境，从构建完整的机器人模拟到简单的游戏智能体，甚至扮演非玩家角色的游戏智能体。甚至有潜力使用深度强化学习智能体进行游戏质量保证测试。想象一下，建立一个游戏测试员团队，他们只通过试错来学习玩你的游戏。可能性是无限的，Unity甚至正在构建一个完整的基于云的模拟环境，用于未来运行或训练这些智能体。
- en: In this section, we will walk through using a game project as a new training
    environment. Any environment you create in Unity would be best tested with the
    ML-Agents toolkit before you set up your own Python code. DRL agents are masters
    at finding bugs and/or cheats. As such, you will almost always want to test the
    environment first with ML-Agents before training it with your own code. I already
    recommended that you go through the process of setting up and running the ML-Agents
    Python code to train agents. Remember that once you export an environment for
    Gym training, it becomes windowless and you will not have any knowledge of how
    well the agent trains or performs in the environment. If there are any cheats
    or bugs to be found, the agent will most assuredly find them. After all, your
    agent will attempt millions of different trial and error combinations trying to
    find how to play the game.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过使用游戏项目作为新的训练环境来演示。在设置自己的Python代码之前，你最好使用ML-Agents工具包测试你在Unity中创建的任何环境。深度强化学习智能体是发现错误和/或作弊的大师。因此，你几乎总是希望在用你的代码训练环境之前，先使用ML-Agents测试环境。我已经推荐你通过设置和运行ML-Agents
    Python代码来训练智能体的过程。记住，一旦你导出环境用于Gym训练，它就变成了无窗口的，你将无法了解智能体在环境中的训练或表现情况。如果有任何作弊或错误需要被发现，智能体几乎肯定能找到。毕竟，你的智能体将尝试数百万种不同的试错组合，试图找到如何玩游戏的方法。
- en: 'We are going to look at the Basic ML-Agents environment as a way of understanding
    how to build our own extended or new environment. The ML-Agents documentation
    is an excellent source to fall back on if the information here is lacking. This
    exercise is intended to get you up to speed building your own environments quickly:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看基本ML-Agents环境，以了解如何构建我们自己的扩展或新环境。如果这里的信息不足，ML-Agents文档是一个很好的备选资源。这个练习的目的是让你快速掌握构建自己的环境：
- en: Open up the **Unity Editor** to the **UnitySDK** ML-Agents project we previously
    had open. Locate and open (double-click) the **Basic** scene at `Assets/ML-Agents/Examples/Basic/Scenes`.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开**Unity编辑器**到我们之前打开的**UnitySDK** ML-Agents项目。在`Assets/ML-Agents/Examples/Basic/Scenes`中找到并打开（双击）**基本**场景。
- en: 'At the center of any environment is the Academy. Locate and select the **Academy**
    object in the **Hierarchy** window and then view the properties in the **Inspector**
    window, as shown in the screenshot:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任何环境的中心都是学院。在**层次结构**窗口中找到并选择**学院**对象，然后在**检查器**窗口中查看属性，如图所示：
- en: '![](img/8bfa360d-b17b-416c-9a01-b67c806967ba.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8bfa360d-b17b-416c-9a01-b67c806967ba.png)'
- en: Inspecting the Academy
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 检查学院
- en: 'Click on and select the **BasicLearning (LearningBrain)** brain in the **Basic
    Academy | Broadcast Hub | Brains** entry. This will highlight the entry in the
    **Project** window. Select the **BasicLearning** brain in the **Project** window
    and view the brain setup in the **Inspector** window, as shown in the screenshot:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**基本学院 | 广播中心 | 大脑**条目中点击并选择**基本学习（学习大脑）**大脑。这将突出显示**项目**窗口中的条目。在**项目**窗口中选择**基本学习**大脑，并在**检查器**窗口中查看大脑设置，如图所示：
- en: '![](img/db8d17df-1a65-491d-adf8-4c14fb9f2e8b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/db8d17df-1a65-491d-adf8-4c14fb9f2e8b.png)'
- en: Inspecting the Learning Brain
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 检查学习大脑
- en: We can see a few things about the brain here. A brain controls an agent so the
    brain's observation and action space effectively become the same as the agent's.
    In the **Inspector** window, you can see there are 20 vector observations and
    an action space of three discrete actions. For this environment, the actions are
    left or right and null. The 0 action becomes a null or pause action.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里我们可以看到关于大脑的一些信息。大脑控制智能体，因此大脑的观察和动作空间实际上与智能体的相同。在**检查器**窗口中，你可以看到有20个向量观察和一个包含三个离散动作的动作空间。对于这个环境，动作是左移或右移以及空操作。0动作变为空操作或暂停操作。
- en: 'Next, we want to inspect the agent itself. Click on and expand the **Basic**
    object in the **Hierarchy** window. Select the **BasicAgent** object and then
    review the **Inspector** window, as the screenshot shows:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们想要检查智能体本身。在**层次结构**窗口中点击并展开**基本**对象。选择**BasicAgent**对象，然后根据截图查看**检查器**窗口：
- en: '![](img/6eefe3a4-eb8a-4652-bf53-e35f3c2bb995.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6eefe3a4-eb8a-4652-bf53-e35f3c2bb995.png)'
- en: Inspecting the Basic Agent
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 检查基本智能体
- en: Inspect the Basic Agent component and you can see the **Brain** is set to the **BasicLearning**
    brain and there are other properties displayed here. Note how the Reset On Done
    and On Demand Decisions are both checked. **Reset On Done** enables the environment
    to reset itself when an episode is complete—what you would expect is the default
    behavior but is not. **On Demand Decisions** equate to using on- versus off-policy
    models and is more relevant when training with ML-Agents toolkit.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查基本智能体组件，你可以看到**大脑**设置为**BasicLearning**大脑，并且这里还显示了其他属性。注意重置完成和按需决策都被勾选了。**重置完成**使环境在完成一个回合后自动重置——这是你期望的默认行为，但实际上并非如此。**按需决策**等同于使用在线策略模型和离线策略模型，在用ML-Agents工具包训练时更为相关。
- en: Pressing Play will show you the agent playing the game. Watch how the agent
    plays and while the agent moves around, be sure to select and inspect objects
    in the editor. Unity is great for seeing how your game mechanics work and this
    comes in especially handy when building your own agent environments.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按下播放键将显示智能体在玩游戏。观察智能体的游戏过程，当智能体移动时，确保在编辑器中选择并检查对象。Unity非常适合查看你的游戏机制如何工作，这在构建自己的智能体环境时尤其有用。
- en: The academy, brain, and agent are the main elements you will need to consider
    when building any new environment. As long as you follow this basic example, you
    should be able to construct a simple working environment quickly. The other tricky
    part of building your own environment is the special coding you may have to do
    and we will cover that in the next section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建任何新的环境时，你需要考虑的主要元素是学院、大脑和智能体。只要遵循这个基本示例，你应该能够快速构建一个简单的可工作环境。构建自己的环境时另一个棘手的部分是你可能需要做的特殊编码，我们将在下一节中介绍。
- en: Coding an agent/environment
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写智能体/环境代码
- en: Unity provides an excellent interface for prototyping and building commercial
    games. You can actually get quite far with very little coding. Unfortunately,
    that is currently not the case when building new ML-Agents environments.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Unity提供了一个出色的界面用于原型设计和构建商业游戏。实际上，你可以用很少的代码走得很远。不幸的是，目前构建新的ML-Agents环境并不是这样。
- en: 'As such, we will explore the important coding parts in the next exercise:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将在下一项练习中探索重要的编码部分：
- en: Next, locate and open the **Scripts** folder under `Assets/ML-Agents/Examples/Basic`
    and inside that double-click to open `BasicAgent.cs`. This is a C# (CSharp) file
    and it will open in the default editor.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，定位并打开位于`Assets/ML-Agents/Examples/Basic`下的**脚本**文件夹，并在其中双击打开`BasicAgent.cs`。这是一个C#（CSharp）文件，它将在默认编辑器中打开。
- en: 'At the top of the file, you will note that this `BasicAgent` class is extended
    from `Agent` and not `MonoBehaviour`, which is the Unity default. `Agent` is a
    special class in Unity, which as you likely guessed, defines an agent that is
    capable of exploring the environment. However, in this case, agent refers more
    to a worker as in a worker in asynchronous or synchronous actor-critic. This means
    a single brain may control multiple agents, which is often the case:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文件顶部，你会注意到这个`BasicAgent`类是从`Agent`扩展的，而不是Unity默认的`MonoBehaviour`。`Agent`是Unity中的一个特殊类，正如你可能猜到的，它定义了一个能够探索环境的智能体。然而，在这种情况下，智能体更多地指的是异步或同步actor-critic中的工作者。这意味着单个大脑可以控制多个智能体，这通常是这种情况：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Skipping down past the fields, we will jump to the method definitions starting
    with `CollectObservations`, shown here:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跳过字段，我们跳到以`CollectObservations`开始的方法定义，如下所示：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Inside this method, we can see how the agent/brain collects observations from
    the environment. In this case, the observation is added using `AddVectorObs`,
    which adds the observation as a one-hot encoded vector of the required size. In
    this case, the vector size is 20, the same as the brain's state size.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个方法内部，我们可以看到代理/大脑如何从环境中收集观察结果。在这种情况下，观察是通过 `AddVectorObs` 添加的，它将观察结果添加为所需大小的
    one-hot 编码向量。在这种情况下，向量大小是 20，与大脑的状态大小相同。
- en: One-hot encoding is a method by which we encode can encode class information
    in terms of binary values inside a vector. Hence, if a one-hot encoded vector
    denoting class or position 1 was active, it would be written as [0,1,0,0].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: One-hot 编码是一种方法，通过在向量内部使用二进制值来编码类信息。因此，如果一个表示类别或位置 1 的一热编码向量是激活的，它将被写成 [0,1,0,0]。
- en: 'The main action method is the `AgentAction` method. This where the agent performs
    actions in the environment, be these actions moving or something else:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主要的动作方法是 `AgentAction` 方法。这是代理在环境中执行动作的地方，无论是移动还是其他动作：
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The first part of this code just determines how the agent moves based on the
    action it has taken. You can see how the code adjusts the agent''s position based
    on its move. Then, we see the following line of code:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码的第一部分只是根据代理所采取的动作确定其移动方式。您可以看到代码如何根据移动调整代理的位置。然后，我们看到以下代码行：
- en: '[PRE13]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This line adds a step reward, meaning it always adds this reward every step.
    It does this as a way of limiting the agent's moves. Hence, the longer the agent
    takes to make the wrong decisions, the less the reward. We sometimes use a step
    reward but it can also have negative effects and it often makes sense to eliminate
    a step reward entirely.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这行代码添加了一个步骤奖励，意味着它会在每一步都添加这个奖励。它这样做是为了限制代理的移动。因此，代理做出错误决策所需的时间越长，奖励就越少。我们有时会使用步骤奖励，但它也可能有负面影响，并且通常有理由完全消除步骤奖励。
- en: 'At the bottom of the `AgentAction` method, we can see what happens when the
    agent reaches the small or large goal. If the agent reaches the large goal it
    gets a reward of 1 and .1 if it makes the small goal. With that, we can also see
    that, when it reaches a goal, the episode terminates using a call to `Done()`:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `AgentAction` 方法的底部，我们可以看到当代理达到小目标或大目标时会发生什么。如果代理达到大目标，它会获得 1 的奖励，如果达到小目标，它会获得
    0.1 的奖励。有了这些，我们还可以看到，当它达到目标时，通过调用 `Done()` 来终止游戏：
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Reverse the numbers for the rewards, save the code, and return to the editor.
    Set the **Academy** to Control the brain and then train the agent with the ML-Agents
    or the code we developed earlier. You should very clearly see the agent having
    a preference for the smaller goal.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将奖励的数字反转，保存代码，然后返回编辑器。将 **Academy** 设置为控制大脑，然后使用 ML-Agents 或我们之前开发的代码训练代理。您应该非常清楚地看到代理对较小目标的偏好。
- en: Extending these concepts and building your own environment now will be up to
    you. The sky really is the limit and Unity provides several excellent examples
    to work with and learn from. In the next section, we will take the opportunity
    to look at the advances ML-Agents provides as mechanisms to enhance your agents
    or even explore new ways to train.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些概念扩展并构建您自己的环境现在将取决于您。天空真的是无极限，Unity 提供了几个出色的示例供您学习和使用。在下一节中，我们将有机会查看 ML-Agents
    提供的作为增强您的代理或探索新的训练方式的机制。
- en: Advancing RL with ML-Agents
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ML-Agents 推进强化学习
- en: The ML-Agents toolkit, the part that allows you to train DRL agents, is considered
    one of the more serious and top-end frameworks for training agents. Since the
    framework was developed on top of Unity, it tends to perform better on Unity-like
    environments. However, not unlike many others who spend time training agents,
    the Unity developers realized early on that some environments present such difficult
    challenges as to require us to assist our agents.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents 工具包，允许您训练深度强化学习（DRL）代理的部分，被认为是训练代理中较为严肃和高端的框架之一。由于该框架是在 Unity 上开发的，因此在类似
    Unity 的环境中表现通常更好。然而，与许多花费时间训练代理的人一样，Unity 开发者早期就意识到，某些环境提出了如此困难的挑战，以至于我们需要协助我们的代理。
- en: 'Now, this assistance is not so much direct but rather indirect and often directly
    relates to how easy or difficult it is for an agent to find rewards. This, in
    turn, directly relates to how well the environment designer can build a reward
    function that an agent can use to learn an environment. There are also the times
    when an environment''s state space is so large and not obvious that creating a
    typical reward function is just not possible. With all that in mind, Unity has
    gone out of its way to enhance the RL inside ML-Agents with the following new
    forms of learning:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这种辅助作用不是那么直接，而是间接的，并且通常直接关系到代理找到奖励的难易程度。这反过来又直接关系到环境设计师能否构建一个代理可以用来学习环境的奖励函数。也有时候，环境的状态空间如此之大且不明显，以至于创建一个典型的奖励函数根本不可能。考虑到所有这些，Unity
    已经竭尽全力通过以下新的学习形式来增强 ML-Agents 内的强化学习：
- en: Curriculum learning
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程学习
- en: Behavioral cloning (imitation learning)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行为克隆（模仿学习）
- en: Curiosity learning
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好奇心学习
- en: Training generalized RL agents
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练通用强化学习代理
- en: We will cover each form of learning in a quick example using the Unity environment.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用 Unity 环境的快速示例来介绍每种学习形式。
- en: Curriculum learning
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程学习
- en: Curriculum learning allows you to train an agent by increasing the complexity
    of the task as the agent learns. This is fairly intuitive and likely very similar
    to the way we learn various tasks from math to programming.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习允许你通过随着代理的学习增加任务的复杂性来训练代理。这相当直观，可能非常类似于我们从数学到编程学习各种任务的方式。
- en: 'Follow the exercise to quickly see how you can set up for curriculum learning:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 按照练习快速了解如何为课程学习进行设置：
- en: Open the `WallJump` scene located in the `Assets/ML-Agents/Examples/WallJump/Scenes`
    folder.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于 `Assets/ML-Agents/Examples/WallJump/Scenes` 文件夹中的 `WallJump` 场景。
- en: 'Select the **Academy** in the scene and review the settings of the **Wall Jump
    Academy** component in the **Inspector** window and as shown in the screenshot:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在场景中选择 **Academy**，并在 **Inspector** 窗口中查看 **Wall Jump Academy** 组件的设置，如图所示：
- en: '![](img/3c4c96dc-e061-470a-ac33-738fb0274f84.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c4c96dc-e061-470a-ac33-738fb0274f84.png)'
- en: Inspecting the WallJump Academy
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 WallJump 学院
- en: Inside the academy is an expanded section called **Reset Parameters**. These
    parameters represent training level parameters for various training states we
    want to put the agent through.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在学院内部有一个扩展的部分，称为 **Reset Parameters**。这些参数代表我们想要让代理经历的各个训练状态的训练级别参数。
- en: 'These parameters now need to be configured in a configuration file the ML-Agents
    toolkit will use to train the agent with curriculum. The contents of this file
    can be found or created at `config/curricula/wall-jump/` and consist of the following:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些参数现在需要在配置文件中进行配置，ML-Agents 工具包将使用该配置文件来训练使用课程的代理。该文件的 内容可以在 `config/curricula/wall-jump/`
    中找到或创建，并包括以下内容：
- en: '[PRE15]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Understanding these parameters can be best done by referring back to the ML-Agents
    docs. Basically, the idea here is that these parameters control the wall height
    which is increased over time. Hence, the agent needs to learn to move the block
    over to jump over the wall as it gets harder and harder.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过参考 ML-Agents 文档可以最好地理解这些参数。基本上，这里的想法是这些参数控制着随时间增加的墙壁高度。因此，代理需要学会移动方块跳过墙壁，随着难度越来越大。
- en: 'Set the **Control** flag on the **Academy** brains and then run an ML-Agents
    session in a Python shell with the following:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **Academy** 脑上设置 **Control** 标志，然后在 Python shell 中运行以下命令以启动 ML-Agents 会话：
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Assuming the configuration files are in the correct place, you will be prompted
    to run the editor and watch the agent train in the environment. The results of
    this example are shown here:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设配置文件位于正确的位置，你将被提示运行编辑器并观察代理在环境中的训练。此示例的结果如下所示：
- en: '![](img/308194a0-9602-4a54-aae2-4c80548cb9ee.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/308194a0-9602-4a54-aae2-4c80548cb9ee.png)'
- en: The output of curriculum training example
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 课程训练示例的输出
- en: Curriculum learning solves the problem of an environment not having an obvious
    answer in a novel way. In this case, the agent's goal is to make the target square.
    However, if the wall started out very high and the agent needed to move the block
    there to jump over it, it likely won't even understand it needs to get to a block.
    Therefore, we help it to train by first allowing it to get to the goal but then
    make it gradually harder to do so. As the difficulty increases, the agent learns
    how to use the block to jump over the wall.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习以新颖的方式解决了环境在没有明显答案的问题。在这种情况下，智能体的目标是使目标方块。然而，如果墙壁一开始就很高，智能体需要移动方块到那里以跳过它，它可能甚至不会理解它需要到达方块。因此，我们通过首先允许它到达目标，然后逐渐使其更难做到这一点来帮助它进行训练。随着难度的增加，智能体学会如何使用方块跳过墙壁。
- en: In the next section, we look to another method that helps agents to solve tasks
    with difficult to find or what we call sparse rewards.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨另一种帮助智能体解决具有难以找到的或我们称之为稀疏奖励的任务的方法。
- en: Behavioral cloning
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 行为克隆
- en: 'Behavioral cloning is sometimes also referred to as imitation learning. While
    not exactly both the same, we will use the terms interchangeably here. In RL,
    we use the term sparse rewards or rewards sparsity for any environment where it
    is difficult for an agent to just finish a task by trial and error and perhaps
    luck. The larger an environment is, the more sparse the rewards and in many cases,
    the observation space can be so large that any hope of training an agent at all
    is extremely difficult. Fortunately, a method called behavioral cloning or imitation
    learning can solve the problem of sparse rewards by using the observations of
    humans as previous sampled observations. Unity provides three methods to generate
    previous observations and they are as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 行为克隆有时也被称为模仿学习。虽然这两个术语并不完全相同，但在这里我们将交替使用这两个术语。在强化学习（RL）中，我们使用“稀疏奖励”或“奖励稀疏性”这个术语来描述任何对智能体来说仅通过试错和可能的好运很难完成任务的环境。环境越大，奖励越稀疏，在许多情况下，观察空间可能如此之大，以至于训练智能体的任何希望都极其困难。幸运的是，一种称为行为克隆或模仿学习的方法可以通过使用人类的观察作为先前采样的观察来解决稀疏奖励的问题。Unity
    提供了三种生成先前观察的方法，如下所示：
- en: '**Generative Adversarial Imitation Learning** (**GAIL**): You can use something
    called the GAIL reward signal to enhance learning rewards from a few observations.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗模仿学习**（**GAIL**）：你可以使用称为 GAIL 奖励信号的东西来增强从少量观察中学习到的奖励。'
- en: '**Pretraining**: This allows you to use prerecorded demonstrations likely from
    a human and use those to bootstrap the learning of the agent. If you use pretraining,
    you also need to provide a configuration section in your ML-Agents config file
    like so:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练**：这允许你使用预先录制的人类演示，并利用这些演示来启动智能体的学习。如果你使用预训练，你还需要在你的 ML-Agents 配置文件中提供一个配置部分，如下所示：'
- en: '[PRE17]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Behavioral Cloning** (**BC**): In this training, setup happens directly in
    the Unity editor. This is great for environments where small demonstrations can
    help to increase an agent''s learning. BC does not work so well on larger environments
    with a large observation state space.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行为克隆**（**BC**）：在这个训练中，设置直接在 Unity 编辑器中进行。这对于小演示可以帮助智能体增加学习的环境来说非常好。BC 在具有大观察状态空间的大型环境中效果不佳。'
- en: These three methods can be combined in a variety of configurations and used
    together in the case of pretraining and GAIL with other methods such as curiosity
    learning, which we will see later.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种方法可以以各种配置组合在一起，并在预训练和 GAIL 与其他方法（如好奇心学习，我们将在后面看到）一起使用的情况下一起使用。
- en: 'It can be especially entertaining to train an agent in real time with BC, as
    we''ll see in the next exercise. Follow the next exercise to explore using the
    BC method of demonstrating to an agent:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 BC 在实时中训练智能体特别有趣，正如我们将在下一个练习中看到的那样。遵循下一个练习来探索使用 BC 方法向智能体演示：
- en: Open the **TennisIL** scene located in the `Assets/ML-Agents/Examples/Tennis/Scenes`
    folder.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于 `Assets/ML-Agents/Examples/Tennis/Scenes` 文件夹中的 **TennisIL** 场景。
- en: This environment is an example of a sparse rewards environment, whereby the
    agent needs to find and hit the ball back to its opponent. This environment makes
    for an excellent example to test BC with on.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个环境是一个稀疏奖励环境的例子，其中智能体需要找到并击打球回对手。这个环境是测试 BC 的绝佳例子。
- en: 'Select the **Academy** object in the **Hierarchy** window and then check the
    **Control** option of **TennisLearning (LearningBrain)** in the **Inspector**
    window, as shown in the screenshot here:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**层次结构**窗口中选择**Academy**对象，然后在**检查器**窗口中检查**TennisLearning (LearningBrain)**的**控制**选项，如图中所示：
- en: '![](img/f4b71a0c-c0d9-4e85-bad5-6437fccc68c2.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f4b71a0c-c0d9-4e85-bad5-6437fccc68c2.png)'
- en: Turning on the learning brain to control
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 打开学习大脑以进行控制
- en: 'As you can see, there are two brains in this scene: one student brain— the
    learning brain, and one teacher brain—the player brain. The teacher brain, controlled
    by the human player, won''t control an actual agent but rather just take direct
    inputs from the player. The student brain observes the teacher''s actions and
    uses those as samples in its policy. In a basic sense, this becomes the teacher
    working from the human policy that the target policy, the agent, needs to learn.
    This is really no different than us having current and target networks.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，在这个场景中有两个大脑：一个学生大脑——学习大脑，和一个教师大脑——玩家大脑。教师大脑由人类玩家控制，不会控制实际的代理，而是直接从玩家那里获取输入。学生大脑观察教师的行为，并使用这些行为作为其策略的样本。从基本意义上讲，这变成了教师根据目标策略，即代理需要学习的策略，从人类策略中工作。这实际上与我们拥有当前网络和目标网络并没有太大的区别。
- en: 'The next thing we have to do is customize the ML-Agents hyperparameters config
    file. We customize the file by adding the following entry for `StudentBrain`:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接下来要做的事情是自定义ML-Agents的超参数配置文件。我们通过为`StudentBrain`添加以下条目来自定义文件：
- en: '[PRE18]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The highlighted elements in the preceding configuration show the `trainer:`
    set to `imitation` and `brain_to_imitate:` as `TeacherBrain`. Plenty of more information
    about setting up the configuration for ML-Agents can be found with the online
    docs.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的配置中，突出显示的元素显示了`trainer:`设置为`imitation`和`brain_to_imitate:`为`TeacherBrain`。有关设置ML-Agents配置的更多信息，可以在在线文档中找到。
- en: 'Next, you need to open a Python/Anaconda shell and change to the `mlagents`
    folder. After that, run the following command to start training:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您需要打开Python/Anaconda shell并切换到`mlagents`文件夹。之后，运行以下命令以开始训练：
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This will start the trainer and in short while you will be prompted to start
    the Unity editor in **Play** mode.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将启动训练器，不久之后您将被提示以**播放**模式启动Unity编辑器。
- en: 'Press **Play** to put the editor in play mode and use the *WASD* controls to
    maneuver the paddle to play tennis against the agent. Assuming you do well, the
    agent will also improve. A screenshot of this training is shown here:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按**播放**将编辑器置于播放模式，并使用*WASD*控制来操纵球拍与代理进行网球比赛。假设您做得很好，代理也会提高。这次训练的截图如下所示：
- en: '![](img/e6f26fd0-b5df-429d-a9cc-5962d065294e.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e6f26fd0-b5df-429d-a9cc-5962d065294e.png)'
- en: Training the tennis agent with BC
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用BC训练网球代理
- en: Imitation learning was a key ingredient in training the agent, AlphaStar. AlphaStar
    was shown to beat human players at a very complex real-time strategy game called
    *StarCraft 2*. It has many keen benefits in getting agents past the sparse rewards
    problem. However, there are many in the RL community that want to avoid IL or
    BC because it can introduce human bias. Human bias has been shown to decrease
    agent performance when compared to agents trained entirely without BC. In fact,
    AlphaStar was trained to a sufficient enough level of playability before it was
    trained on itself. It was this self-training that is believed to be responsible
    for the innovation that allowed it to beat human players.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习是训练代理AlphaStar的关键因素。AlphaStar被证明在一种非常复杂的实时策略游戏*星际争霸2*中击败了人类玩家。它在帮助代理克服稀疏奖励问题方面有许多显著优势。然而，在强化学习（RL）社区中，许多人希望避免模仿学习（IL）或行为克隆（BC），因为这可能会引入人类偏见。研究表明，与完全未经BC训练的代理相比，人类偏见会降低代理的性能。实际上，AlphaStar在开始自我训练之前已经达到了足够的可玩性水平。正是这种自我训练被认为使其能够击败人类玩家。
- en: In the next section, we look at another exciting way Unity has tried to capture
    a method to counter sparse reward problems.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨Unity尝试捕捉解决稀疏奖励问题的一种另类激动人心的方法。
- en: Curiosity learning
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 好奇心学习
- en: Up until now, we have only ever considered external rewards given to the agent
    from the environment. Yet, we and other animals receive a wide variety of external
    and internal rewards. Internal rewards are often characterized by emotion or feeling.
    An agent could have an internal reward that gives it +1 every time it looks to
    some face, perhaps denoting some internal love or infatuation reward. These types
    of rewards are called intrinsic rewards and they represent rewards that are internal
    or self-derived by the agent. This has some powerful capabilities for everything
    from creating interesting motivated agents to enhancing an agent's learning ability.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了环境给予智能体的外部奖励。然而，我们和其他动物会接收到各种各样的外部和内部奖励。内部奖励通常由情绪或感觉来表征。智能体可能有一种内部奖励，每次它看向某个面孔时都会得到
    +1，这或许代表了一些内在的爱或迷恋奖励。这类奖励被称为内在奖励，它们代表了智能体内部或自我产生的奖励。这为从创建有趣的动机智能体到增强智能体的学习能力等方面提供了强大的能力。
- en: It is the second way in which Unity introduced curiosity learning or the internal
    curiosity reward system as a way of letting agents explore more when they get
    surprised. That is, whenever an agent is surprised by an action, its curiosity
    increases and hence it needs to explore the state actions in the space that surprised
    it.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Unity 引入好奇心学习或内部好奇心奖励系统的第二种方式，作为一种让智能体在感到惊讶时探索更多的方式。也就是说，每当智能体对某个动作感到惊讶时，它的好奇心就会增加，因此它需要探索使其感到惊讶的状态动作空间。
- en: 'Unity has produced a very powerful example of curiosity learning in an environment
    called Pyramids. It is the goal of the agent in this environment to find a pile
    of yellow blocks with a gold block on top. Knock over the pile of blocks and then
    get the gold block. The problem is that there are piles of boxes in many rooms
    at the start but none start yellow. To turn the blocks yellow, that agent needs
    to find and press a button. Finding this sequence of tasks using straight RL could
    be problematic and/or time-consuming. Fortunately, with CL, we can improve this
    performance dramatically. We will look at how to use CL in the next section to
    train the Pyramids environment:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Unity 在一个名为 Pyramids 的环境中产生了一个非常强大的好奇心学习示例。在这个环境中，智能体的目标是找到一堆带有金色方块在顶部的黄色方块。推倒方块堆，然后获取金色方块。问题是，一开始在许多房间里都有盒子堆，但没有一个是黄色的。要使方块变黄，智能体需要找到并按下按钮。使用直接强化学习（RL）找到这个任务序列可能会出现问题，或者会耗费时间。幸运的是，有了
    CL，我们可以显著提高这种性能。我们将在下一节中探讨如何使用 CL 来训练 Pyramids 环境：
- en: Open the **Pyramids** scene located in the `Assets/ML-Agents/Examples/Pyramids/Scenes`
    folder.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于 `Assets/ML-Agents/Examples/Pyramids/Scenes` 文件夹中的 **Pyramids** 场景。
- en: 'Press Play to run the default agent; this will be one trained with Unity. When
    you run the agent, watch it play through the environment and you will see the
    agent first find the button, press it, then locate the pile of blocks it needs
    to knock over. It will knock over the boxes as shown in the sequence of screenshots
    here:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按下播放按钮运行默认智能体；这将是一个使用 Unity 训练的智能体。当你运行智能体时，观察它如何在环境中玩耍，你会看到智能体首先找到按钮，按下它，然后定位到它需要推倒的方块堆。它将像这里截图序列中所示那样推倒盒子：
- en: '![](img/bd8c5703-0567-418a-aa63-667979789af6.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bd8c5703-0567-418a-aa63-667979789af6.png)'
- en: Pyramids agent playing the environment
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Pyramids 智能体在玩环境
- en: Training an agent with curiosity just requires setting the Academy to control
    the brain and running the ML-Agents trainer with the proper configuration. This
    documentation to drive CL has changed several times over the course of ML-Agents
    development. Therefore, it is recommended you consult the ML-Agents docs for the
    most recent documentation.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅需将学院设置为控制大脑，并使用适当的配置运行 ML-Agents 训练器，就可以训练一个好奇心的智能体。这份关于驱动 CL 的文档在 ML-Agents
    开发过程中已经更改了好几次。因此，建议您查阅 ML-Agents 文档以获取最新的文档信息。
- en: CL can be very powerful and the whole concept of intrinsic rewards has some
    fun and interesting application towards games. Imagine being able to power internal
    reward systems for enemy agents that may play to greed, power, or some other evil
    trait. In the next section, we finish out this chapter with a look at training
    generalized reinforcement learning agents.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 'CL 可以非常强大，内在奖励的整个概念在游戏中有一些有趣和有趣的应用。想象一下，能够为可能表现出贪婪、权力或其他邪恶特质的敌方智能体提供内部奖励系统。在下一节中，我们将通过探讨训练通用强化学习智能体来结束这一章。 '
- en: Training generalized reinforcement learning agents
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练通用强化学习智能体
- en: We often have to remind ourselves that RL is just a derivation of data science
    best practices and we often have to consider how could you fix a training issue
    with data science. In the case of RL, we see the same issues we see in data science
    and machine learning only at different scales and exposed in another manner. One
    example of this is when an agent is overfitted to an environment that we then
    try to apply to other general variations of that environment. For instance, imagine
    the Frozen Lake environment that could be various sizes or even provide random
    starting points or other variations. By introducing these types of variations,
    we allow our agent to better generalize to a wider variety of similar environments.
    It is this generalization that we want to introduce into our environment.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常需要提醒自己，强化学习只是数据科学最佳实践的衍生，我们经常需要考虑如何使用数据科学来修复训练问题。在强化学习的案例中，我们看到与数据科学和机器学习相同的问题，只是在不同的规模和不同的方式下暴露出来。一个例子是，当代理过度拟合到我们试图应用于该环境其他一般变体的环境中时。例如，想象一下可能具有不同大小或甚至提供随机起点或其他变体的
    Frozen Lake 环境。通过引入这些类型的变体，我们允许我们的代理更好地泛化到更广泛的类似环境中。我们希望将这种泛化引入到我们的环境中。
- en: '**AGI** or **Artificial General Intelligence** is the concept of generalized
    training agents to the *n*^(th) degree. It is expected that a truly AGI agent
    would be able to be placed in any environment and learn to solve the task. This
    could take an amount of training but ideally, no other hyperparameters or other
    human intervention should be required.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**AGI** 或 **通用人工智能** 是将通用训练代理扩展到 *n* 次方的概念。预期一个真正的 AGI 代理能够被放置在任何环境中并学会解决任务。这可能需要一定量的训练，但理想情况下，不应需要其他超参数或其他人为干预。'
- en: 'By making the environment stochastic, we are essentially increasing the likelihood
    of our methods that use distributional RL and noisy networks will also become
    more powerful. Unfortunately, enabling these types of parameters with other training
    code, or our PyTorch code, is not currently available. In the next exercise, we''ll
    learn how to set up a generalized training environment:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使环境具有随机性，我们实际上增加了我们使用分布式强化学习和有噪声网络的方法的效力。不幸的是，启用这些类型的参数与其他训练代码或我们的 PyTorch
    代码目前不可用。在下一个练习中，我们将学习如何设置一个通用的训练环境：
- en: Open the **WallJump** scene located in the `Assets/ML-Agents/Examples/WallJump/Scenes`
    folder.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于 `Assets/ML-Agents/Examples/WallJump/Scenes` 文件夹中的 **WallJump** 场景。
- en: '**WallJump** is already set up and configured with several reset parameters
    we looked at earlier when we reviewed curriculum learning. This time, instead
    of progressively changing those parameters, we are going to have the environment
    sample them randomly.'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**WallJump** 已经设置并配置了我们在审查课程学习时看到的几个重置参数。这次，我们不是逐步改变这些参数，而是让环境随机采样它们。'
- en: The parameters we want to resample are based on this sample. We can create a
    new generalized YAML file called `walljump_generalize.yaml` in the config folder.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们想要重新采样的参数基于这个样本。我们可以在配置文件夹中创建一个新的通用 YAML 文件，命名为 `walljump_generalize.yaml`。
- en: 'Open and put the following text in this file and then save it:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开文件，将以下文本放入其中，然后保存：
- en: '[PRE20]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This sets up the sampling distributions for how we will sample the values.
    The values for the environment can then be sampled with the following code:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这设置了我们将如何采样值的采样分布。然后，可以使用以下代码对环境的值进行采样：
- en: '[PRE21]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can also define new sampler types or ways of sampling data values using
    a custom sampler that our classes place in the `sample_class.py` file in the ML-Agents
    code. The following is an example of a custom sampler:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以定义新的采样器类型或使用自定义采样器以自定义方式采样数据值，该采样器由我们在 ML-Agents 代码中的 `sample_class.py`
    文件中的类放置。以下是一个自定义采样器的示例：
- en: '[PRE22]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, you can configure the config file to run this sampler like so:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以配置配置文件以运行此采样器，如下所示：
- en: '[PRE23]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Remember that you still need to sample the values and modify the environment's
    configuration when the agent resets. This will require modifying the code to sample
    the inputs using the appropriate samplers.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请记住，当代理重置时，您仍然需要采样值并修改环境的配置。这需要修改代码以使用适当的采样器采样输入。
- en: 'You can then run the Unity ML-Agents trainer code with the following command:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用以下命令运行 Unity ML-Agents 训练器代码：
- en: '[PRE24]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Being able to train agents in this manner allows your agents to be more robust
    and able to tackle various incarnations of your environments. If you are building
    a game that needs a practical agent, you will most likely need to train your agents
    in a generalized manner. Generalized agents will generally be able to adapt to
    unforeseen changes in the environment far better than an agent trained otherwise.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 能够以这种方式训练代理可以使您的代理更加健壮，能够应对各种环境形态。如果您正在构建需要实用代理的游戏，您很可能会需要以通用方式训练您的代理。通用代理通常能够更好地适应环境中不可预见的变化，比其他方式训练的代理要好得多。
- en: That about does it for this chapter and, in the next section, we'll look at
    gaining further experience with the sample exercises for this chapter.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本章的内容，在下一节中，我们将探讨如何通过本章的示例练习获得更多经验。
- en: Exercises
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'The exercises in this section are intended to introduce you to Unity ML-Agents
    in more detail. If your preference is not to use ML-Agents as a training framework,
    then move on to the next section and the end of this chapter. For those of you
    still here, ML-Agents on its own is a powerful toolkit for quickly exploring DRL
    agents. The toolkit hides most of the details of DRL but that should not be a
    problem for you to figure out by now:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的练习旨在更详细地向您介绍Unity ML-Agents。如果您不打算使用ML-Agents作为训练框架，那么请继续下一节和本章的结尾。对于那些仍然在这里的人，ML-Agents本身是一个强大的工具包，可以快速探索DRL代理。该工具包隐藏了DRL的大部分细节，但您现在应该能够自己找出这些细节：
- en: Set up and run one of the Unity ML-Agents sample environments in the editor
    to train an agent. This will require that you consult the Unity ML-Agents documentation.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编辑器中设置并运行Unity ML-Agents的一个示例环境来训练代理。这需要您查阅Unity ML-Agents文档。
- en: Tune the hyperparameters of a sample Unity environment.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整示例Unity环境的超参数。
- en: Start TensorBoard and run it so that it collects logs from the Unity runs folder.
    This will allow you to watch the training performance of the agents being trained
    with ML-Agents.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动TensorBoard并运行它，以便从Unity运行文件夹中收集日志。这将允许您查看使用ML-Agents训练的代理的训练性能。
- en: Build a Unity environment and train it with the Rainbow DQN example.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用彩虹DQN示例构建Unity环境并进行训练。
- en: Customize one of the existing Unity environments by changing the setup, parameters,
    reset parameters, and/or reward function. That is, change the reward feedback
    the agent receives when completing actions or tasks.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过更改设置、参数、重置参数和/或奖励函数来自定义现有的Unity环境。也就是说，改变代理在完成动作或任务时收到的奖励反馈。
- en: Set up and train an agent with pretrained data. This will require you to set
    up a player brain to record demonstrations. Play the game to record those demonstrations
    and then set the game for training with a learning brain.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练数据设置和训练代理。这需要您设置一个玩家大脑来记录演示。玩游戏以记录这些演示，然后设置游戏以学习大脑进行训练。
- en: Train an agent with behavioral cloning using the tennis environment.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用网球环境通过行为克隆训练代理。
- en: Train an agent with curiosity learning using the Pyramids scene.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用金字塔场景通过好奇心学习训练代理。
- en: Set up and run a Unity environment for generalized training. Use the sampling
    to pull stochastic values from distributions for the environment. What effect
    do different distributions have on the agent's training performance?
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置并运行一个用于通用训练的Unity环境。使用采样从分布中提取环境的随机值。不同的分布对代理的训练性能有什么影响？
- en: Convert a PG method example such as PPO so that you can run a Unity environment.
    How does the performance compare with Rainbow DQN?
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将PG方法示例（如PPO）转换为可以在Unity环境中运行。其性能与彩虹DQN相比如何？
- en: Use these examples to familiarize yourself with Unity ML-Agents and more advanced
    concepts in RL. In the next section, we will summarize and complete this chapter.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些示例熟悉Unity ML-Agents和RL中的更高级概念。在下一节中，我们将总结并完成本章。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we took a diversion and built our own DRL environments for
    training with our own code, or another framework, or using the ML-Agents framework
    from Unity. At first, we looked at the basics of installing the ML-Agents toolkit
    for the development of environments, training, and training with our own code.
    Then, we looked at how to build a basic Unity environment for training from a
    Gym interface like we have been doing throughout this whole book. After that,
    we learned how our RainbowDQN sample could be customized to train an agent. From
    there, we looked at how we can create a brand new environment from the basics.
    We finished this chapter by looking at managing rewards in environments and the
    set of tools ML-Agents uses to enhance environments with sparse rewards. There,
    we looked at several methods Unity has added to ML-Agents to assist with difficult
    environments and sparse rewards.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们偏离了主线，构建了我们自己的DRL环境，用于使用自己的代码、另一个框架或使用Unity的ML-Agents框架进行训练。起初，我们探讨了安装ML-Agents工具包的基本知识，用于开发环境、训练以及使用自己的代码进行训练。然后，我们了解了如何从Gym界面构建一个基本的Unity环境，就像我们在整本书中一直做的那样。之后，我们学习了如何定制我们的RainbowDQN示例以训练一个智能体。从那里，我们探讨了如何从基础创建一个全新的环境。我们通过查看如何在环境中管理奖励以及ML-Agents用于增强具有稀疏奖励的环境的工具集来结束本章。在那里，我们探讨了Unity为ML-Agents添加的几种方法，以帮助处理困难的环境和稀疏奖励。
- en: Moving on from this chapter, we will continue to explore other DRL frameworks
    that can be used to train agents. ML-Agents is one of many powerful frameworks
    that can be used to train agents, as we will soon see.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章继续前进，我们将继续探索其他可用于训练智能体的DRL框架。ML-Agents是许多强大的框架之一，可用于训练智能体，正如我们很快将看到的。
