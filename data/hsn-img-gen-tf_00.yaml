- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Any sufficiently advanced technology is indistinguishable from magic.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 任何足够先进的技术都无法与魔法区分开来。
- en: – Arthur C. Clarke
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ——亚瑟·C·克拉克
- en: This phrase best describes image generation using **artificial** **intelligence**
    (**AI**). The field of deep learning—a subset of artificial intelligence—has been
    developing rapidly in the last decade. Now we can generate artificial but faces
    that are indistinguishable from real people's faces, and to generate realistic
    paintings from simple brush strokes. Most of these abilities are owed to a type
    of deep neural network known as a **generative** **adversarial** **network** (**GAN**).
    With this hands-on book, you'll not only develop image generation skills but also
    gain a solid understanding of the underlying principles.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这句话最能描述使用**人工****智能**（**AI**）进行图像生成的过程。深度学习——作为人工智能的一个子集——在过去十年中发展迅速。现在，我们可以生成与真实人脸无法区分的人工人脸，并且能将简单的笔触转化为逼真的画作。这些能力大多归功于一种深度神经网络，称为**生成****对抗****网络**（**GAN**）。通过这本动手实践的书籍，你不仅将开发图像生成技能，还能深入理解背后的原理。
- en: The book starts with an introduction to the fundamentals of image generation
    using TensorFlow covering variational autoencoders and GANs. As you progress through
    the chapters, you'll learn to build models for different applications for performing
    face swaps using deep fakes, neural style transfer, image-to-image translation,
    turning simple images into photorealistic images, and much more. You'll also understand
    how and why to construct state-of-the-art deep neural networks using advanced
    techniques such as spectral normalization and self-attention layer before working
    with advanced models for face generation and editing. You'll also be introduced
    to photo restoration, text-to-image synthesis, video retargeting, and neural rendering.
    Throughout the book, you'll learn to implement models from scratch in TensorFlow
    2.x, including PixelCNN, VAE, DCGAN, WGAN, pix2pix, CycleGAN, StyleGAN, GauGAN,
    and BigGAN.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本书从使用TensorFlow生成图像的基本原理开始，涵盖了变分自编码器和生成对抗网络（GAN）。随着章节的进展，你将学习如何为不同的应用构建模型，包括使用深度伪造技术进行换脸、神经风格迁移、图像到图像的转换、将简单图像转化为逼真的图像等。你还将了解如何以及为什么使用先进的技术，如谱归一化和自注意力层，来构建最先进的深度神经网络，然后再使用这些技术处理面部生成和编辑的高级模型。本书还将介绍照片修复、文本到图像的合成、视频重新定向和神经渲染等内容。在整个书籍中，你将学习如何从零开始在TensorFlow
    2.x中实现模型，包括PixelCNN、VAE、DCGAN、WGAN、pix2pix、CycleGAN、StyleGAN、GauGAN和BigGAN。
- en: By the end of this book, you'll be well-versed in TensorFlow and image generative
    technologies.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本书结束时，你将熟练掌握TensorFlow和图像生成技术。
- en: Who this book is for
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适合的人群
- en: This book is for deep learning engineers, practitioners, and researchers who
    have basic knowledge of convolutional neural networks and want to use it to learn
    various image generation techniques using TensorFlow 2.x. You'll also find this
    book useful if you are an image processing professional or computer vision engineer
    looking to explore state-of-the-art architectures to improve and enhance images
    and videos. Knowledge of Python and TensorFlow is required to get the best out
    of the book.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本书面向具有卷积神经网络基础知识的深度学习工程师、从业者和研究人员，旨在帮助你使用TensorFlow 2.x学习各种图像生成技术。如果你是图像处理专业人士或计算机视觉工程师，想要探索最先进的架构以改善和增强图像和视频，本书对你也非常有用。为了从本书中获得最大收益，要求你具备Python和TensorFlow的知识。
- en: How to use this book
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用本书
- en: There are many online tutorials available teaching the basics of GANs. However,
    the models tend to be rather simple and suitable only for toy datasets. At the
    other end of the spectrum, there are also free codes available for state-of-the-art
    models to generate realistic images. Nevertheless, the code tends to be complex,
    and the lack of explanation makes it difficult for beginners to understand. Many
    of the “Git cloners” who downloaded the codes had no clue how to tweak the models
    to make them work for their applications. This book aims to bridge that gap.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 网上有很多教程教授GAN的基础知识。然而，这些模型通常较为简单，仅适用于玩具数据集。在另一端，也有一些免费的代码可供生成逼真图像的最先进模型使用。然而，这些代码往往复杂，且缺乏解释，使得初学者很难理解。许多下载代码的“Git克隆者”根本不知道如何调整模型以使其适用于自己的应用。本书旨在弥补这一差距。
- en: We will start with learning the basic principles and immediately implement the
    code to put them to the test. You'll be able to see the result of your work instantly.
    All the necessary code to build a model is laid bare in a single Jupyter notebook.
    This is to make it easier for you to go through the flow of the code and to modify
    and test the code in an interactive manner. I believe writing from scratch is
    the best way to learn and master deep learning. There are between one to three
    models in each chapter, and we will write all of them from scratch. When you finish
    this book, not only will you be familiar with image generation but you will also
    be an expert in TensorFlow 2.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从学习基本原理开始，并立即实现代码来验证它们。你将能够即时看到你的工作成果。构建模型所需的所有代码都展示在一个 Jupyter notebook
    中。这是为了让你更容易理解代码流程，并以交互方式修改和测试代码。我相信从头开始编写代码是学习和掌握深度学习的最佳方式。每一章包含一到三个模型，我们将从零开始编写所有这些模型。完成本书后，你不仅会熟悉图像生成，还将成为
    TensorFlow 2 的专家。
- en: The chapters are arranged in roughly chronological order of the history of GANs,
    where the chapters may build upon knowledge from previous chapters. Therefore,
    it is best to read the chapters in order, especially the first three chapters,
    which cover the fundamentals. After that, you may jump to chapters that interest
    you more. Should you feel confused by the acronyms during the reading, you can
    refer to the summary of GAN techniques listed in the last chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 各章节大致按 GAN 历史的时间顺序排列，每一章的内容可能会建立在前一章的知识基础上。因此，最好按顺序阅读这些章节，特别是前面三章，它们涵盖了基础知识。之后，你可以跳到自己感兴趣的章节。如果在阅读过程中对缩写感到困惑，可以参考最后一章中列出的
    GAN 技术总结。
- en: What this book covers
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容概览
- en: '[*Chapter 1*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017), *Getting Started
    with Image Generation Using TensorFlow*, walks through the basics of pixel probability
    and uses it to build our first model to generate handwritten digits.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第1章*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017)，*使用 TensorFlow 进行图像生成入门*，讲解了像素概率的基础知识，并利用它来构建我们第一个生成手写数字的模型。'
- en: '[*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039), *Variational
    Autoencoder*, explains how to build a **variational autoencoder** (**VAE**) and
    use it to generate and edit faces.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第2章*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)，*变分自编码器*，讲解了如何构建**变分自编码器**（**VAE**），并使用它来生成和编辑面部图像。'
- en: '[*Chapter 3*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060), *Generative
    Adversarial Network*, introduces the fundamentals of GANs and builds a DCGAN to
    generate photorealistic images. We''ll then learn about new adversarial loss to
    stabilize the training.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第3章*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060)，*生成对抗网络*，介绍了 GAN 的基本原理，并构建了一个
    DCGAN 来生成逼真的图像。然后我们将学习新的对抗性损失函数，以稳定训练过程。'
- en: '[*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084), *Image-to-Image
    Translation*, covers a lot of models and interesting applications. We will first
    implement pix2pix to convert sketches to photorealistic photos. Then we''ll use
    CycleGAN to transform a horse to a zebra. Lastly, we will use BicycleGAN to generate
    a variety of shoes.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第4章*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)，*图像到图像的转换*，涵盖了许多模型和有趣的应用。我们将首先实现
    pix2pix，将素描转换为逼真的照片。接着我们将使用 CycleGAN 将马变成斑马。最后，我们将使用 BicycleGAN 生成各种鞋子。'
- en: '[*Chapter 5*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104), *Style Transfer*,
    explains how to extract the style from a painting and transfer it into a photo.
    We''ll also learn advanced techniques to make neural style transfer run faster
    in runtime, and to use it in state-of-the-art GANs.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第5章*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)，*风格迁移*，解释了如何从一幅画中提取风格并将其转移到照片上。我们还将学习一些先进的技术，以加速神经风格迁移的运行速度，并将其应用于最前沿的
    GANs。'
- en: '[*Chapter 6*](B14538_06_Final_JM_ePub.xhtml#_idTextAnchor124), *AI Painter*,
    goes through the underlying principles of image editing and transformation using
    **interactive GAN** (**iGAN**) as an example. Then we will build a GauGAN to create
    photorealistic building facades from a simple segmentation map.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第6章*](B14538_06_Final_JM_ePub.xhtml#_idTextAnchor124)，*AI 绘画师*，讲解了使用 **交互式
    GAN**（**iGAN**）作为示例的图像编辑和变换的基本原理。接着我们将构建一个 GauGAN，从简单的分割图生成逼真的建筑外立面。'
- en: '[*Chapter 7*](B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136), *High Fidelity
    Face Generation*, shows how to build a StyleGAN using techniques from style transfer.
    However, before that, we will learn to grow the network layer progressively using
    a Progressive GAN.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 7 章*](B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136)，*高保真面部生成*，展示了如何利用风格迁移技术构建
    StyleGAN。然而，在此之前，我们将学习如何使用渐进式 GAN 逐步增加网络层。'
- en: '[*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156), *Self-Attention
    for Image Generation*, shows how to build self-attention into a **Self-Attention
    GAN** (**SAGAN**) and a BigGAN for conditional image generation.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 8 章*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)，*图像生成中的自注意力*，展示了如何将自注意力机制构建到
    **自注意力 GAN**（**SAGAN**）和 BigGAN 中，用于条件图像生成。'
- en: '[*Chapter 9*](B14538_09_Final_JM_ePub.xhtml#_idTextAnchor175), *Video Synthesis*,
    demonstrates how to use autoencoders to create a deepfake video. Along the way,
    we''ll learn how to use OpenCV and dlib for face processing.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 9 章*](B14538_09_Final_JM_ePub.xhtml#_idTextAnchor175)，*视频合成*，演示了如何使用自编码器创建深度伪造视频。在此过程中，我们将学习如何使用
    OpenCV 和 dlib 进行面部处理。'
- en: '[*Chapter 10*](B14538_10_Final_JM_ePub.xhtml#_idTextAnchor195), *Road Ahead*,
    reviews and summarizes the generative techniques we have learned. Then we will
    look at how they are used as the basis of up-and-coming applications, including
    text-to-image-synthesis, video compression, and video retargeting.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第 10 章*](B14538_10_Final_JM_ePub.xhtml#_idTextAnchor195)，*前路*，回顾并总结了我们所学的生成技术。接着，我们将探讨它们如何作为即将到来的应用的基础，包括文本到图像合成、视频压缩和视频重定向。'
- en: To get the most out of this book
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为了最大限度地利用本书
- en: Readers should have basic knowledge of deep learning training pipelines, such
    as training convolutional neural networks for image classification. This book
    will mainly use high-level Keras APIs in TensorFlow 2, which is easy to learn.
    Should you need to refresh or learn TensorFlow 2, there are many free tutorials
    available online, such as the one on the official TensorFlow website, [https://www.tensorflow.org/tutorials/keras/classification](https://www.tensorflow.org/tutorials/keras/classification).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应具备深度学习训练管道的基本知识，如训练卷积神经网络进行图像分类。本书将主要使用 TensorFlow 2 中的高级 Keras API，这些 API
    容易学习。如果你需要刷新或学习 TensorFlow 2，有许多免费的在线教程可供参考，如官方 TensorFlow 网站上的教程：[https://www.tensorflow.org/tutorials/keras/classification](https://www.tensorflow.org/tutorials/keras/classification)。
- en: '![](img/01.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01.jpg)'
- en: Training deep neural networks is computationally intensive. You can train the
    first few simple models using the CPU only. However, as we progress to more complex
    models and datasets in later chapters, the model training could take a few days
    before you start to see satisfactory results. To get the most out of this book,
    you should have access to the GPU to accelerate the model training time. There
    are also free cloud services, such as Google's Colab, that provide GPUs on which
    you can upload and run the code.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度神经网络需要大量的计算资源。你可以仅使用 CPU 来训练前几个简单的模型。然而，随着我们进入后续章节，更复杂的模型和数据集，模型训练可能需要几天时间才能看到满意的结果。为了最大限度地利用本书，你应该有
    GPU 来加速模型训练时间。也有一些免费的云服务，比如 Google 的 Colab，提供 GPU，你可以在其上上传并运行代码。
- en: '**If you are using the digital version of this book, we advise you to type
    the code yourself or access the code via the GitHub repository (link available
    in the next section). Doing so will help you avoid any potential errors related
    to the copying and pasting of code.**'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你使用的是本书的数字版，我们建议你自己输入代码或通过 GitHub 仓库访问代码（链接将在下一节提供）。这样可以帮助你避免复制粘贴代码时可能出现的任何错误。**'
- en: Download the example code files
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0).
    In case there's an update to the code, it will be updated on the existing GitHub
    repository.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 GitHub 下载本书的示例代码文件，链接：[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0)。如果代码有更新，它将会在现有的
    GitHub 仓库中进行更新。
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有其他代码包，来自我们丰富的书籍和视频目录，可以在 [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)
    获取。快来看看吧！
- en: Download the color images
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载彩色图像
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781838826789_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781838826789_ColorImages.pdf).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一份PDF文件，包含本书中使用的截图/图表的彩色图片。你可以在这里下载：[https://static.packt-cdn.com/downloads/9781838826789_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781838826789_ColorImages.pdf)。
- en: Conventions used
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的约定
- en: There are a number of text conventions used throughout this book.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了若干文本约定。
- en: '`Code in text`: Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: “This is done using `tf.gather(self.beta, labels)`,
    which is conceptually equivalent to `beta = self.beta[labels]`, as follows.”'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`文本中的代码`：表示文本中的代码词、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟网址、用户输入和推特用户名。以下是一个例子：“这是通过`tf.gather(self.beta,
    labels)`完成的，概念上等同于`beta = self.beta[labels]`，如下所示。”'
- en: 'A block of code is set as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一段代码如下所示：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望引起你对代码块中特定部分的注意时，相关的行或项会以粗体显示：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出都以以下方式书写：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For example, words in menus or dialog boxes appear in the text like this. Here
    is an example: “From the preceding architecture diagram, we can see that **G1**''s
    encoder output concatenates with **G1**''s features and feeds into the decoder
    part of **G2** to generate high-resolution images.”'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示新术语、重要单词或你在屏幕上看到的单词。例如，菜单或对话框中的单词在文本中会以这种形式出现。以下是一个例子：“从前面的架构图中，我们可以看到**G1**的编码器输出与**G1**的特征连接，并输入到**G2**的解码器部分以生成高分辨率图像。”'
- en: Tips or important notes
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 提示或重要事项
- en: Appear like this.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式出现。
- en: Get in touch
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我们
- en: Feedback from our readers is always welcome.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终欢迎读者的反馈。
- en: '`customercare@packtpub.com`.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`customercare@packtpub.com`。'
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit www.packtpub.com/support/errata,
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误表**：尽管我们已经尽一切努力确保内容的准确性，但难免会有错误。如果你发现本书中的错误，我们非常感激你能报告给我们。请访问 www.packtpub.com/support/errata，选择你的书籍，点击“勘误表提交表单”链接，并输入相关细节。'
- en: '`copyright@packt.com` with a link to the material.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`copyright@packt.com`，并附有相关材料的链接。'
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit authors.packtpub.com.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你有兴趣成为作者**：如果你在某个领域有专长，并且有兴趣写作或贡献内容到书籍中，请访问 authors.packtpub.com。'
- en: Reviews
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 书评
- en: Please leave a review. Once you have read and used this book, why not leave
    a review on the site that you purchased it from? Potential readers can then see
    and use your unbiased opinion to make purchase decisions, we at Packt can understand
    what you think about our products, and our authors can see your feedback on their
    book. Thank you!
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请留下评论。一旦你阅读并使用了本书，为什么不在你购买书籍的站点上留下评论呢？潜在的读者可以参考你的公正意见来做出购买决定，我们Packt可以了解你对我们产品的看法，我们的作者也能看到你对他们书籍的反馈。谢谢！
- en: For more information about Packt, please visit packt.com.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于Packt的信息，请访问 packt.com。
