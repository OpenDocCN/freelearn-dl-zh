- en: Chapter 3. Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 自编码器
- en: In the previous chapter, [Chapter 2](ch02.html "Chapter 2. Deep Neural Networks"),
    *Deep Neural Networks*, you were introduced to the concepts of deep neural networks.
    We're now going to move on to look at autoencoders, which are a neural network
    architecture that attempts to find a compressed representation of the given input
    data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，[第2章](ch02.html "第2章 深度神经网络")，*深度神经网络*中，您已介绍了深度神经网络的概念。现在我们将继续研究自编码器，这是一种神经网络架构，旨在找到给定输入数据的压缩表示。
- en: Similar to the previous chapters, the input data may be in multiple forms including,
    speech, text, image, or video. An autoencoder will attempt to find a representation
    or code in order to perform useful transformations on the input data. As an example,
    in denoising autoencoders, a neural network will attempt to find a code that can
    be used to transform noisy data into clean ones. Noisy data could be in the form
    of an audio recording with static noise which is then converted into clear sound.
    Autoencoders will learn the code automatically from the data alone without human labeling.
    As such, autoencoders can be classified under **unsupervised** learning algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，输入数据可以是多种形式，包括语音、文本、图像或视频。自编码器将尝试找到一种表示或编码，以便对输入数据执行有用的变换。例如，在去噪自编码器中，神经网络将尝试找到一个可以将噪声数据转换为干净数据的编码。噪声数据可能是带有静态噪音的音频录音，然后将其转换为清晰的声音。自编码器将自动从数据中学习编码，无需人工标注。因此，自编码器可以归类为**无监督**学习算法。
- en: In later chapters of this book, we will look at **Generative Adversarial Networks**
    (**GANs**) and **Variational Autoencoders** (**VAEs**) which are also representative
    forms of unsupervised learning algorithms. This is in contrast to the supervised
    learning algorithms we discussed in the previous chapters where human annotations were
    required.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后续章节中，我们将介绍**生成对抗网络**（**GANs**）和**变分自编码器**（**VAEs**），它们也是无监督学习算法的代表形式。这与我们在前几章讨论的监督学习算法不同，后者需要人工标注。
- en: In its simplest form, an autoencoder will learn the representation or code by
    trying to copy the input to output. However, using an autoencoder is not as simple
    as copying the input to output. Otherwise, the neural network would not be able
    to uncover the hidden structure in the input distribution.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的形式中，自编码器将通过尝试将输入复制到输出的方式来学习表示或编码。然而，使用自编码器并不是简单地将输入复制到输出。否则，神经网络将无法揭示输入分布中的隐藏结构。
- en: An autoencoder will encode the input distribution into a low-dimensional tensor,
    which usually takes the form of a vector. This will approximate the hidden structure
    that is commonly referred to as the latent representation, code, or vector. This
    process constitutes the encoding part. The latent vector will then be decoded
    by the decoder part to recover the original input.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器将输入分布编码成低维张量，通常表现为一个向量。这将近似于通常称为潜在表示、编码或向量的隐藏结构。这个过程构成了编码部分。然后，潜在向量将通过解码器部分被解码，以恢复原始输入。
- en: As a result of the latent vector being a low-dimensional compressed representation
    of the input distribution, it should be expected that the output recovered by
    the decoder can only approximate the input. The dissimilarity between the input
    and the output can be measured by a loss function.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 由于潜在向量是输入分布的低维压缩表示，因此应当预期通过解码器恢复的输出只能近似输入。输入和输出之间的不相似度可以通过损失函数来度量。
- en: But why would we use autoencoders? Simply put, autoencoders have practical applications
    both in their original form or as part of more complex neural networks. They're
    a key tool in understanding the advanced topics of deep learning as they give
    you a low-dimensional latent vector. Furthermore, it can be efficiently processed
    to perform structural operations on the input data. Common operations include
    denoising, colorization, feature-level arithmetic, detection, tracking, and segmentation,
    to name just a few.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们要使用自编码器呢？简单来说，自编码器在其原始形式或作为更复杂神经网络的一部分都有实际应用。它们是理解深度学习高级主题的关键工具，因为它们提供了一个低维的潜在向量。此外，它可以高效处理，用于对输入数据执行结构性操作。常见的操作包括去噪、着色、特征级运算、检测、跟踪和分割，仅举几例。
- en: 'In summary, the goal of this chapter is to present:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章的目标是呈现：
- en: The principles of autoencoders
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器的原理
- en: How to implement autoencoders into the Keras neural network library
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将自编码器实现到 Keras 神经网络库中
- en: The main features of denoising and colorization autoencoders
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去噪和颜色化自编码器的主要特征
- en: Principles of autoencoders
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器原理
- en: In this section, we're going to go over the principles of autoencoders. In this
    section, we're going to be looking at autoencoders with the MNIST dataset, which
    we were first introduced to in the previous chapters.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍自编码器的原理。在本节中，我们将查看使用 MNIST 数据集的自编码器，这是我们在前几章中首次介绍的。
- en: 'Firstly, we need to be made aware that an autoencoder has two operators, these
    are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要意识到自编码器有两个操作符，它们是：
- en: '**Encoder**: This transforms the input, *x*, into a low-dimensional latent
    vector, *z = f(x)*. Since the latent vector is of low dimension, the encoder is
    forced to learn only the most important features of the input data. For example,
    in the case of MNIST digits, the important features to learn may include writing
    style, tilt angle, roundness of stroke, thickness, and so on. Essentially, these
    are the most important information needed to represent digits zero to nine.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：它将输入 *x* 转换为低维潜在向量 *z = f(x)*。由于潜在向量维度较低，编码器被迫仅学习输入数据的最重要特征。例如，在 MNIST
    数字的情况下，重要的特征可能包括书写风格、倾斜角度、笔画圆度、粗细等。本质上，这些是表示数字零到九所需的最重要信息。'
- en: '**Decoder**: This tries to recover the input from the latent vector,![Principles
    of autoencoders](img/B08956_03_001.jpg)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：它尝试从潜在向量恢复输入，![自编码器原理](img/B08956_03_001.jpg)'
- en: . Although the latent vector has a low dimension, it has a sufficient size to allow
    the decoder to recover the input data.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 。尽管潜在向量维度较低，但它具有足够的大小，使解码器能够恢复输入数据。
- en: The goal of the decoder is to make
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的目标是使
- en: '![Principles of autoencoders](img/B08956_03_002.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_002.jpg)'
- en: as close as possible to *x*. Generally, both the encoder and decoder are non-linear
    functions. The dimension of *z* is a measure of the number of salient features
    it can represent. The dimension is usually much smaller than the input dimensions
    for efficiency and in order to constrain the latent code to learn only the most
    salient properties of the input distribution[1].
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能接近 *x*。通常，编码器和解码器都是非线性函数。*z* 的维度是它能表示的显著特征数量的度量。为了提高效率并限制潜在编码仅学习输入分布的最显著特性，维度通常远小于输入维度[1]。
- en: The autoencoder has the tendency to memorize the input when the dimension of
    the latent code is significantly bigger than *x*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当潜在编码的维度明显大于 *x* 时，自编码器倾向于记住输入。
- en: A suitable loss function,
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 适当的损失函数，
- en: '![Principles of autoencoders](img/B08956_03_003.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_003.jpg)'
- en: ', is a measure of how dissimilar the input, *x*, from the output which is the
    recovered input,'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ，是输入 *x* 和输出（即恢复的输入）之间不相似度的度量，
- en: '![Principles of autoencoders](img/B08956_03_004.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_004.jpg)'
- en: '. As shown in the following equation, the **Mean Squared Error** (**MSE**)
    is an example of such a loss function:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如下方的方程所示，**均方误差**（**MSE**）是这种损失函数的一个例子：
- en: '![Principles of autoencoders](img/B08956_03_005.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_005.jpg)'
- en: (Equation 3.1.1)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (方程 3.1.1)
- en: In this example, *m* is the output dimensions (For example, in MNIST *m = width
    × height × channels = 28 × 28 × 1 = 784*).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，*m* 是输出维度（例如，在 MNIST 中 *m = 宽度 × 高度 × 通道 = 28 × 28 × 1 = 784*）。
- en: '![Principles of autoencoders](img/B08956_03_006.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_006.jpg)'
- en: and
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![Principles of autoencoders](img/B08956_03_007.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_007.jpg)'
- en: are the elements of *x* and
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 是 *x* 的元素，并且
- en: '![Principles of autoencoders](img/B08956_03_008.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_008.jpg)'
- en: respectively. Since the loss function is a measure of dissimilarity between
    the input and output, we're able to use alternative reconstruction loss functions
    such as the binary cross entropy or **structural similarity index** (**SSIM**).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 分别。由于损失函数是输入和输出之间不相似度的度量，我们可以使用其他重建损失函数，如二元交叉熵或**结构相似性指数**（**SSIM**）。
- en: '![Principles of autoencoders](img/B08956_03_01.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_01.jpg)'
- en: 'Figure 3.1.1: Block diagram of an autoencoder'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1.1：自编码器的框图
- en: '![Principles of autoencoders](img/B08956_03_02.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_02.jpg)'
- en: 'Figure 3.1.2: An autoencoder with MNIST digit input and output. The latent
    vector is 16-dim.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1.2：带有 MNIST 数字输入和输出的自编码器。潜在向量是 16 维的。
- en: To put the autoencoder in context, *x* can be an MNIST digit which has a dimension
    of 28 × 28 × 1 = 784\. The encoder transforms the input into a low-dimensional
    *z* that can be a 16-dimension latent vector. The decoder will attempt to recover
    the input in the form of
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使自编码器具有上下文，*x* 可以是一个 MNIST 数字，其维度为 28 × 28 × 1 = 784。编码器将输入转换为一个低维度的 *z*，它可以是一个
    16 维的潜在向量。解码器将尝试以以下形式恢复输入
- en: '![Principles of autoencoders](img/B08956_03_010.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_010.jpg)'
- en: from *z*. Visually, every MNIST digit *x* will appear similar to
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *z* 中获取。直观地看，每个 MNIST 数字 *x* 将与
- en: '![Principles of autoencoders](img/B08956_03_011.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_011.jpg)'
- en: . *Figure 3.1.2* demonstrates this autoencoding process to us. We can observe
    that the decoded digit 7, while not exactly the same remains close enough.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: . *图 3.1.2* 向我们展示了这个自编码过程。我们可以观察到，解码后的数字 7，虽然不是完全相同，但足够接近。
- en: Since both encoder and decoder are non-linear functions, we can use neural networks
    to implement both. For example, in the MNIST dataset, the autoencoder can be implemented
    by MLP or CNN. The autoencoder can be trained by minimizing the loss function
    through backpropagation. Similar to other neural networks, the only requirement
    is that the loss function must be differentiable.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于编码器和解码器都是非线性函数，我们可以使用神经网络来实现它们。例如，在 MNIST 数据集中，可以通过 MLP 或 CNN 来实现自编码器。自编码器可以通过最小化损失函数并通过反向传播进行训练。与其他神经网络类似，唯一的要求是损失函数必须是可微的。
- en: If we treat the input as a distribution, we can interpret the encoder as an
    encoder of distribution,
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将输入视为一个分布，我们可以将编码器解释为分布的编码器，
- en: '![Principles of autoencoders](img/B08956_03_012.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_012.jpg)'
- en: and the decoder, as the decoder of distribution,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器作为分布的解码器，
- en: '![Principles of autoencoders](img/B08956_03_013.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_013.jpg)'
- en: '. The loss function of the autoencoder is expressed as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: . 自编码器的损失函数表示如下：
- en: '![Principles of autoencoders](img/B08956_03_014.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_014.jpg)'
- en: (Equation 3.1.2)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: (公式 3.1.2)
- en: 'The loss function simply means that we would like to maximize the chances of
    recovering the input distribution given the latent vector distribution. If the
    decoder output distribution is assumed to be Gaussian, then the loss function
    boils down to MSE since:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数简单地意味着我们希望在给定潜在向量分布的情况下，最大化恢复输入分布的机会。如果假设解码器输出分布是高斯分布，那么损失函数就简化为 MSE，因为：
- en: '![Principles of autoencoders](img/B08956_03_015.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_015.jpg)'
- en: (Equation 3.1.3)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: (公式 3.1.3)
- en: In this example,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，
- en: '![Principles of autoencoders](img/B08956_03_016.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_016.jpg)'
- en: represents a Gaussian distribution with a mean of
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表示均值为的高斯分布
- en: '![Principles of autoencoders](img/B08956_03_017.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_017.jpg)'
- en: and variance of
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 和方差为
- en: '![Principles of autoencoders](img/B08956_03_018.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_018.jpg)'
- en: . A constant variance is assumed. The decoder output
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: . 假设常数方差。解码器输出
- en: '![Principles of autoencoders](img/B08956_03_019.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![自编码器原理](img/B08956_03_019.jpg)'
- en: is assumed to be independent. While *m* is the output dimension.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设是独立的。*m* 是输出维度。
- en: Building autoencoders using Keras
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 构建自编码器
- en: We're now going to move onto something really exciting, building an autoencoder
    using Keras library. For simplicity, we'll be using the MNIST dataset for the
    first set of examples. The autoencoder will then generate a latent vector from
    the input data and recover the input using the decoder. The latent vector in this
    first example is 16-dim.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将进行一个非常激动人心的内容，使用 Keras 库构建一个自编码器。为了简化起见，我们将使用 MNIST 数据集作为第一组示例。自编码器将从输入数据中生成一个潜在向量，并通过解码器恢复输入。第一个示例中的潜在向量是
    16 维的。
- en: Firstly, we're going to implement the autoencoder by building the encoder. *Listing* *3.2.1*
    shows the encoder that compresses the MNIST digit into a 16-dim latent vector.
    The encoder is a stack of two `Conv2D`. The final stage is a `Dense` layer with
    16 units to generate the latent vector. *Figure 3.2.1* shows the architecture
    model diagram generated by `plot_model()` which is the same as the text version
    produced by `encoder.summary()`. The shape of the output of the last `Conv2D`
    is saved to compute the dimensions of the decoder input layer for easy reconstruction
    of the MNIST image.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将通过构建编码器来实现自动编码器。*清单* 3.2.1 显示了编码器，它将 MNIST 数字压缩为一个 16 维的潜在向量。编码器是由两层 `Conv2D`
    堆叠而成。最后一个阶段是一个有 16 个单元的 `Dense` 层，用于生成潜在向量。*图 3.2.1* 显示了 `plot_model()` 生成的架构模型图，它与
    `encoder.summary()` 生成的文本版本相同。最后一个 `Conv2D` 层输出的形状被保存下来，用于计算解码器输入层的维度，以便轻松重建 MNIST
    图像。
- en: 'The following Listing 3.2.1, shows `autoencoder-mnist-3.2.1.py`. This is an autoencoder
    implementation using Keras. The latent vector is 16-dim:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的代码清单 3.2.1 显示了 `autoencoder-mnist-3.2.1.py`。这是一个使用 Keras 实现的自动编码器。潜在向量是 16
    维的：
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Building autoencoders using Keras](img/B08956_03_03.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 构建自动编码器](img/B08956_03_03.jpg)'
- en: 'Figure 3.2.1: The encoder model is a made up of Conv2D(32)-Conv2D(64)-Dense(16)
    in order to generate the low dimensional latent vector'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2.1：编码器模型由 Conv2D(32)-Conv2D(64)-Dense(16) 组成，用于生成低维潜在向量。
- en: The decoder in *Listing* *3.2.1* decompresses the latent vector in order to
    recover the MNIST digit. The decoder input stage is a `Dense` layer that will
    accept the latent vector. The number of units is equal to the product of the saved
    `Conv2D` output dimensions from the encoder. This is done so we can easily resize
    the output of the `Dense` layer for `Conv2DTranspose` to finally recover the original
    MNIST image dimensions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单* 3.2.1 中的解码器将潜在向量解压缩，以恢复 MNIST 数字。解码器的输入阶段是一个 `Dense` 层，用于接受潜在向量。单元的数量等于编码器中
    `Conv2D` 输出维度的乘积。这样做是为了方便将 `Dense` 层的输出调整为 `Conv2DTranspose` 的输入，最终恢复原始的 MNIST
    图像维度。'
- en: The decoder is made of a stack of three `Conv2DTranspose`. In our case, we're
    going to use a **Transposed CNN** (sometimes called deconvolution), which is more
    commonly used in decoders. We can imagine transposed CNN (`Conv2DTranspose`) as
    the reversed process of CNN. In a simple example, if the CNN converts an image
    to feature maps, the transposed CNN will produce an image given feature maps.
    *Figure 3.2.2* shows the decoder model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器由三层 `Conv2DTranspose` 堆叠而成。在我们的例子中，我们将使用 **反向卷积神经网络**（有时称为反卷积），这种结构在解码器中更为常见。我们可以把反向卷积神经网络（`Conv2DTranspose`）想象成卷积神经网络的反向过程。在一个简单的例子中，如果卷积神经网络将图像转换为特征图，反向卷积神经网络则会根据特征图生成图像。*图
    3.2.2* 显示了解码器模型。
- en: '![Building autoencoders using Keras](img/B08956_03_04.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 构建自动编码器](img/B08956_03_04.jpg)'
- en: 'Figure 3.2.2: The decoder model is made of a Dense(16)-Conv2DTranspose(64)
    -Conv2DTranspose(32)-Conv2DTranspose(1). The input is the latent vector decoded
    to recover the original input.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2.2：解码器模型由 Dense(16)-Conv2DTranspose(64)-Conv2DTranspose(32)-Conv2DTranspose(1)
    组成。输入是潜在向量，被解码以恢复原始输入。
- en: By joining the encoder and decoder together, we're able to build the autoencoder.
    *Figure 3.2.3* illustrates the model diagram of the autoencoder. The tensor output
    of the encoder is also the input to a decoder which generates the output of the
    autoencoder. In this example, we'll be using the MSE loss function and Adam optimizer.
    During training, the input is the same as the output, `x_train`. We should note
    that in our example, there are only a few layers which are sufficient enough to
    drive the validation loss to 0.01 in one epoch. For more complex datasets, you
    may need a deeper encoder, decoder as well as more epochs of training.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将编码器和解码器结合在一起，我们可以构建自动编码器。*图 3.2.3* 说明了自动编码器的模型图。编码器的张量输出也是解码器的输入，解码器生成自动编码器的输出。在这个示例中，我们将使用
    MSE 损失函数和 Adam 优化器。在训练过程中，输入和输出相同，即 `x_train`。我们应该注意，在这个示例中，只有少数几层就足够使验证损失在一轮训练中降到
    0.01。对于更复杂的数据集，可能需要更深的编码器、解码器以及更多的训练轮次。
- en: '![Building autoencoders using Keras](img/B08956_03_05.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 构建自动编码器](img/B08956_03_05.jpg)'
- en: 'Figure 3.2.3: The autoencoder model is built by joining an encoder model and
    a decoder model together. There are 178k parameters for this autoencoder.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2.3：自动编码器模型是通过将编码器模型和解码器模型结合在一起构建的。这个自动编码器有 178k 个参数。
- en: After training the autoencoder for one epoch with a validation loss of 0.01,
    we're able to verify if it can encode and decode the MNIST data that it has not
    seen before. *Figure 3.2.4* shows us eight samples from the test data and the
    corresponding decoded images. Except for minor blurring in the images, we're able
    to easily recognize that the autoencoder is able to recover the input with good
    quality. The results will improve as we train for a larger number of epochs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在对自编码器训练一个周期后，验证损失为 0.01，我们能够验证它是否可以编码和解码之前未见过的 MNIST 数据。*图 3.2.4*展示了来自测试数据的八个样本及其对应的解码图像。除了图像中的轻微模糊外，我们可以轻松识别出自编码器能够以良好的质量恢复输入数据。随着训练轮数的增加，结果将会得到改善。
- en: '![Building autoencoders using Keras](img/B08956_03_06.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 构建自编码器](img/B08956_03_06.jpg)'
- en: 'Figure 3.2.4: Prediction of the autoencoder from the test data. The first 2
    rows are the original input test data. The last 2 rows are the predicted data.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2.4：从测试数据中预测的自编码器输出。前两行是原始输入的测试数据，后两行是预测的数据。
- en: At this point, we may be wondering how we can visualize the latent vector in
    space. A simple method for visualization is to force the autoencoder to learn
    the MNIST digits features using a 2-dim latent vector. From there, we're able
    to project this latent vector on a 2D space in order to see how the MNIST codes
    are distributed. By setting the `latent_dim = 2` in `autoencoder-mnist-3.2.1.py`
    code and by using the `plot_results()` to plot the MNIST digit as a function of
    the 2-dim latent vector, *Figure 3.2.5* and *Figure 3.2.6* shows the distribution
    of MNIST digits as a function of latent codes. These figures were generated after
    20 epochs of training. For convenience, the program is saved as `autoencoder-2dim-mnist-3.2.2.py`
    with the partial code shown in *Listing 3.2.2*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可能会想，如何可视化潜在向量空间呢？一种简单的可视化方法是强迫自编码器通过使用 2 维潜在向量来学习 MNIST 数字的特征。这样，我们就能够将这个潜在向量投影到
    2D 空间中，从而看到 MNIST 编码的分布情况。通过在 `autoencoder-mnist-3.2.1.py` 代码中设置 `latent_dim =
    2`，并使用 `plot_results()` 绘制 MNIST 数字与 2 维潜在向量的关系，*图 3.2.5* 和 *图 3.2.6* 展示了 MNIST
    数字在潜在编码上的分布情况。这些图形是在训练 20 个周期后生成的。为了方便起见，程序已保存为 `autoencoder-2dim-mnist-3.2.2.py`，其部分代码在*代码清单
    3.2.2*中显示。
- en: Following is Listing 3.2.2, `autoencoder-2dim-mnist-3.2.2.py`, which shows the
    function for visualization of the MNIST digits distribution over 2-dim latent
    codes. The rest of the code is practically similar to *Listing 3.2.1* and no longer
    shown here.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是代码清单 3.2.2，`autoencoder-2dim-mnist-3.2.2.py`，它展示了用于可视化 MNIST 数字在 2 维潜在编码上的分布的函数。其余代码实际上与*代码清单
    3.2.1*类似，这里不再展示。
- en: '[PRE1]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Building autoencoders using Keras](img/B08956_03_07.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 构建自编码器](img/B08956_03_07.jpg)'
- en: 'Figure 3.2.5: A MNIST digit distribution as a function of latent code dimensions,
    *z*[0] and *z*[1]. Original color photo can be found on the book GitHub repository,
    https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter3-autoencoders/README.md.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2.5：MNIST 数字分布与潜在编码维度 *z*[0] 和 *z*[1] 的关系。原始彩色照片可在书籍的 GitHub 仓库中找到，https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter3-autoencoders/README.md。
- en: '![Building autoencoders using Keras](img/B08956_03_08.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Keras 构建自编码器](img/B08956_03_08.jpg)'
- en: 'Figure 3.2.6: Digits generated as the 2-dim latent vector space is navigated'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2.6：当导航 2 维潜在向量空间时生成的数字
- en: In *Figure 3.2.5*, we'll be able to see that the latent codes for a specific
    digit are clustering on a region in space. For example, digit 0 is on the lower
    left quadrant, while digit 1 is on the upper right quadrant. Such clustering is
    mirrored in *Figure 3.2.6*. In fact, the same figure shows the result of navigating
    or generating new digits from the latent space as shown in the *Figure 3.2.5*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 3.2.5*中，我们能够看到特定数字的潜在编码在空间中的某个区域聚集。例如，数字 0 位于左下象限，而数字 1 位于右上象限。这种聚集在*图 3.2.6*中得到了呈现。事实上，同一图形展示了从潜在空间中导航或生成新数字的结果，正如在*图
    3.2.5*中所示。
- en: For example, starting from the center and varying the value of a 2-dim latent
    vector towards the lower left quadrant, shows us that the digit changes from 2
    to 0\. This is expected since from *Figure 3.2.5*, we're able to see that the
    codes for the digit 2 clusters are near the center, and as discussed digit 0 codes
    cluster in the lower left quadrant. For *Figure 3.2.6*, we've only explored the
    regions between -4.0 and +4.0 for each latent dimension.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，从中心开始，并改变一个二维潜在向量的值朝向左下象限，显示出数字从2变为0。这是可以预期的，因为从*图 3.2.5*，我们可以看到数字2的代码簇接近中心，而数字0的代码簇则位于左下象限。如*图
    3.2.6*所示，我们只探索了每个潜在维度在-4.0到+4.0之间的区域。
- en: As can be seen in *Figure 3.2.5*, the latent code distribution is not continuous
    and ranges beyond
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*图 3.2.5*所示，潜在代码分布是不连续的，并且超出了
- en: '![Building autoencoders using Keras](img/B08956_03_020.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras构建自编码器](img/B08956_03_020.jpg)'
- en: . Ideally, it should look like a circle where there are valid values everywhere.
    Because of this discontinuity, there are regions where if we decode the latent
    vector, hardly recognizable digits will be produced.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 。理想情况下，它应该像一个圆形，那里每个地方都有有效的值。由于这种不连续性，存在一些区域，如果我们解码潜在向量，可能会生成几乎无法识别的数字。
- en: Denoising autoencoder (DAE)
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 去噪自编码器（DAE）
- en: 'We''re now going to build an autoencoder with a practical application. Firstly,
    let''s paint a picture and imagine that the MNIST digits images were corrupted
    by noise, thus making it harder for humans to read. We''re able to build a **Denoising
    Autoencoder** (**DAE**) to remove the noise from these images. *Figure 3.3.1*
    shows us three sets of MNIST digits. The top rows of each set (for example, MNIST
    digits 7, 2, 1, 9, 0, 6, 3, 4, 9) are the original images. The middle rows show
    the inputs to DAE, which are the original images corrupted by noise. The last
    rows show the outputs of DAE:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在要构建一个具有实际应用的自编码器。首先，让我们画个图，假设MNIST数字图像被噪声污染，这样人类阅读起来会更困难。我们可以构建一个**去噪自编码器**（**DAE**）来去除这些图像中的噪声。*图
    3.3.1*展示了三组MNIST数字。每组的顶部行（例如，MNIST数字 7、2、1、9、0、6、3、4、9）是原始图像。中间行显示了DAE的输入，即被噪声污染的原始图像。底部行显示了DAE的输出：
- en: '![Denoising autoencoder (DAE)](img/B08956_03_09.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_09.jpg)'
- en: 'Figure 3.3.1: Original MNIST digits (top rows), corrupted original images (middle
    rows) and denoised images (last rows)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3.1：原始MNIST数字（顶部行），被污染的原始图像（中间行）和去噪图像（底部行）
- en: '![Denoising autoencoder (DAE)](img/B08956_03_10.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_10.jpg)'
- en: 'Figure 3.3.2: The input to the denoising autoencoder is the corrupted image.
    The output is the clean or denoised image. The latent vector is assumed to be
    16-dim.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3.2：去噪自编码器的输入是被污染的图像，输出是干净的或去噪的图像。假设潜在向量为16维。
- en: 'As shown in *Figure 3.3.2*, the denoising autoencoder has practically the same
    structure as the autoencoder for MNIST that we presented in the previous section.
    The input is defined as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 3.3.2*所示，去噪自编码器的结构实际上与我们在前一部分中介绍的MNIST自编码器相同。输入定义为：
- en: '![Denoising autoencoder (DAE)](img/B08956_03_021.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_021.jpg)'
- en: (Equation 3.3.1)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: （方程式 3.3.1）
- en: In this formula,
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，
- en: '![Denoising autoencoder (DAE)](img/B08956_03_022.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_022.jpg)'
- en: represents the original MNIST image corrupted by *noise*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表示被*噪声*污染的原始MNIST图像。
- en: The objective of the encoder is to discover how to produce the latent vector,
    *z*, that will enable the decoder to recover
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的目标是发现如何生成潜在向量*z*，使得解码器能够恢复
- en: '![Denoising autoencoder (DAE)](img/B08956_03_023.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_023.jpg)'
- en: 'by minimizing the dissimilarity loss function such as MSE, as shown here:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化失真损失函数（如均方误差 MSE），如图所示：
- en: '![Denoising autoencoder (DAE)](img/B08956_03_024.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_024.jpg)'
- en: (Equation 3.3.2)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: （方程式 3.3.2）
- en: In this example, *m* is the output dimensions (for example, in MNIST *m = width
    × height × channels = 28 × 28 × 1 = 784*).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，*m* 是输出维度（例如，在MNIST中，*m = 宽度 × 高度 × 通道 = 28 × 28 × 1 = 784*）。
- en: '![Denoising autoencoder (DAE)](img/B08956_03_025.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_025.jpg)'
- en: and
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![Denoising autoencoder (DAE)](img/B08956_03_026.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_026.jpg)'
- en: are the elements of
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 是…的元素
- en: '![Denoising autoencoder (DAE)](img/B08956_03_027.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_027.jpg)'
- en: and
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![Denoising autoencoder (DAE)](img/B08956_03_028.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_028.jpg)'
- en: ', respectively.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ，分别。
- en: To implement DAE, we're going to need to make a few changes on the autoencoder
    presented in the previous section. Firstly, the training input data should be
    corrupted MNIST digits. The training output data is the same original clean MNIST
    digits. This is like telling the autoencoder what the corrected images should
    be or asking it to figure out how to remove noise given a corrupted image. Lastly,
    we must validate the autoencoder on the corrupted MNIST test data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现DAE，我们需要对上一节中的自编码器做一些修改。首先，训练输入数据应该是损坏的MNIST数字。训练输出数据与原始清晰的MNIST数字相同。这就像是告诉自编码器正确的图像应该是什么，或者要求它在给定损坏图像的情况下找出如何去除噪声。最后，我们必须在损坏的MNIST测试数据上验证自编码器。
- en: The MNIST digit 7 shown on the left of *Figure 3.3.2* is an actual corrupted
    image input. The one on the right is the clean image output of a trained denoising
    autoencoder.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.3.2*左侧展示的MNIST数字7是一个实际的损坏图像输入。右侧的是经过训练的去噪自编码器输出的清晰图像。'
- en: '*Listing 3.3.1* shows the denoising autoencoder which has been contributed
    to the Keras GitHub repository. Using the same MNIST dataset, we''re able to simulate
    corrupted images by adding random noise. The noise added is a Gaussian distribution
    with a mean,'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单3.3.1*展示了已贡献至Keras GitHub仓库的去噪自编码器。使用相同的MNIST数据集，我们能够通过添加随机噪声来模拟损坏的图像。添加的噪声是一个高斯分布，均值为，'
- en: '![Denoising autoencoder (DAE)](img/B08956_03_029.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_029.jpg)'
- en: and standard deviation of
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 和标准差
- en: '![Denoising autoencoder (DAE)](img/B08956_03_030.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_030.jpg)'
- en: . Since adding random noise may push the pixel data into invalid values of less
    than 0 or greater than 1, the pixel values are clipped to [0.1, 1.0] range.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 。由于添加随机噪声可能会将像素数据推向无效值（小于0或大于1），因此像素值会被裁剪到[0.1, 1.0]范围内。
- en: Everything else will remain practically the same as the autoencoder from the
    previous section. We'll use the same MSE loss function and Adam optimizer as the
    autoencoder. However, the number of epoch for training has increased to 10\. This
    is to allow sufficient parameter optimization.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其他部分将与上一节中的自编码器几乎相同。我们将使用相同的MSE损失函数和Adam优化器。然而，训练的epoch数增加到了10。这是为了允许足够的参数优化。
- en: '*Figure 3.3.1* shows actual validation data with both the corrupted and denoised
    test MNIST digits. We''re even able to see that humans will find it difficult
    to read the corrupted MNIST digits. *Figure 3.3.3* shows a certain level of robustness
    of DAE as the level of noise is increased from'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.3.1*展示了实际的验证数据，包含损坏和去噪后的MNIST测试数字。我们甚至能够看到，人类会发现很难读取损坏的MNIST数字。*图3.3.3*展示了DAE在噪声水平提高时的某种鲁棒性。'
- en: '![Denoising autoencoder (DAE)](img/B08956_03_031.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_031.jpg)'
- en: to
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 到
- en: '![Denoising autoencoder (DAE)](img/B08956_03_032.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_032.jpg)'
- en: and
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '![Denoising autoencoder (DAE)](img/B08956_03_033.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_033.jpg)'
- en: . At
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 。在
- en: '![Denoising autoencoder (DAE)](img/B08956_03_034.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_034.jpg)'
- en: ', DAE is still able to recover the original images. However, at'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ，DAE仍然能够恢复原始图像。然而，在
- en: '![Denoising autoencoder (DAE)](img/B08956_03_035.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_035.jpg)'
- en: ', a few digits such as 4 and 5 in the second and third sets can no longer be
    recovered correctly.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ，在第二组和第三组中，像数字4和5这样的几个数字已经无法正确恢复。
- en: '![Denoising autoencoder (DAE)](img/B08956_03_11.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![去噪自编码器（DAE）](img/B08956_03_11.jpg)'
- en: 'Figure 3.3.3: Performance of denoising autoencoder as the noise level is increased'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3.3：去噪自编码器在噪声水平增加时的表现
- en: 'As seen in Listing 3.3.1, `denoising-autoencoder-mnist-3.3.1.py` shows us a Denoising
    autoencoder:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如清单3.3.1所示，`denoising-autoencoder-mnist-3.3.1.py`向我们展示了一个去噪自编码器：
- en: '[PRE2]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Automatic colorization autoencoder
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动着色自编码器
- en: We're now going to work on another practical application of autoencoders. In
    this case, we're going to imagine that we have a grayscale photo and that we want
    to build a tool that will automatically add color to them. We would like to replicate
    the human abilities in identifying that the sea and sky are blue, the grass field
    and trees are green, while clouds are white, and so on.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在要进行自编码器的另一个实际应用。在这个案例中，我们假设有一张灰度照片，我们希望构建一个能够自动为其添加颜色的工具。我们希望模拟人类的能力，能够识别出大海和天空是蓝色的，草地和树木是绿色的，而云是白色的，等等。
- en: As shown in *Figure 3.4.1*, if we are given a grayscale photo of a rice field
    on the foreground, a volcano in the background and sky on top, we're able to add
    the appropriate colors.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图3.4.1*所示，如果我们得到一张前景为稻田、背景为火山、顶部为天空的灰度照片，我们能够为其添加适当的颜色。
- en: '![Automatic colorization autoencoder](img/B08956_03_12.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![自动颜色化自编码器](img/B08956_03_12.jpg)'
- en: 'Figure 3.4.1: Adding color to a grayscale photo of the Mayon Volcano. A colorization
    network should replicate human abilities by adding color to a grayscale photo.
    Left photo is grayscale. The right photo is color. Original color photo can be
    found on the book GitHub repository, https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter3-autoencoders/README.md.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4.1：为梅雁火山的灰度照片添加颜色。颜色化网络应该模仿人类的能力，通过给灰度照片添加颜色。左图是灰度图，右图是彩色图。原始的彩色照片可以在本书的GitHub仓库中找到，链接为
    https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter3-autoencoders/README.md。
- en: A simple automatic colorization algorithm seems like a suitable problem for
    autoencoders. If we can train the autoencoder with a sufficient number of grayscale
    photos as input and the corresponding colored photos as output, it could possibly
    discover the hidden structure on properly applying colors. Roughly, it is the
    reverse process of denoising. The question is, can an autoencoder add color (good
    noise) to the original grayscale image?
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的自动颜色化算法似乎是自编码器的合适问题。如果我们可以用足够多的灰度照片作为输入，并将对应的彩色照片作为输出进行训练，它可能会发现隐藏的结构并正确地应用颜色。大致来说，它是去噪的反向过程。问题是，自编码器能否为原始灰度图像添加颜色（良好的噪声）？
- en: '*Listing* *3.4.1* shows the colorization autoencoder network. The colorization
    autoencoder network is a modified version of denoising autoencoder that we used for
    the MNIST dataset. Firstly, we need a dataset of grayscale to colored photos.
    The CIFAR10 database, which we have used before, has 50,000 training and 10,000
    testing 32 × 32 RGB photos that can be converted to grayscale. As shown in the
    following listing, we''re able to use the `rgb2gray()` function to apply weights
    on R, G, and B components to convert from color to grayscale.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表* *3.4.1* 显示了颜色化自编码器网络。该颜色化自编码器网络是我们之前用于MNIST数据集的去噪自编码器的修改版。首先，我们需要一个灰度到彩色照片的数据集。我们之前使用过的CIFAR10数据库包含50,000张训练照片和10,000张32
    × 32的RGB测试照片，可以转换为灰度图像。如下所示，我们可以使用`rgb2gray()`函数，通过对R、G和B分量加权，将彩色图像转换为灰度图像。'
- en: 'Listing 3.4.1, `colorization-autoencoder-cifar10-3.4.1.py`, shows us a colorization
    autoencoder using the CIFAR10 dataset:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.4.1，`colorization-autoencoder-cifar10-3.4.1.py`，展示了一个使用CIFAR10数据集的颜色化自编码器：
- en: '[PRE3]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We've increased the capacity of the autoencoder by adding one more block of
    convolution and transposed convolution. We've also doubled the number of filters
    at each CNN block. The latent vector is now 256-dim in order to increase the number
    of salient properties it can represent as discussed in the autoencoder section.
    Finally, the output filter size has increased to three, or equal to the number
    of channels in RGB of the expected colored output.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过增加一个卷积和转置卷积块来增加自编码器的容量。我们还在每个CNN块中加倍了滤波器的数量。潜在向量现在是256维，以便增加它可以表示的显著属性的数量，如自编码器部分所讨论的那样。最后，输出滤波器的大小增加到3，即与期望的彩色输出中的RGB通道数相等。
- en: The colorization autoencoder is now trained with the grayscale as inputs and
    original RGB images as outputs. The training will take more epochs and uses the
    learning rate reducer to scale down the learning rate when the validation loss
    is not improving. This can be easily done by telling the `callbacks` argument
    in the Keras `fit()` function to call the `lr_reducer()` function.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色化自编码器现在使用灰度图像作为输入，原始RGB图像作为输出进行训练。训练将需要更多的epoch，并使用学习率调整器，在验证损失没有改善时缩小学习率。这可以通过在Keras的`fit()`函数中告诉`callbacks`参数调用`lr_reducer()`函数来轻松实现。
- en: '*Figure 3.4.2* demonstrates colorization of grayscale images from the test
    dataset of CIFAR10\. *Figure 3.4.3* compares the ground truth with the colorization
    autoencoder prediction. The autoencoder performs an acceptable colorization job.
    The sea or sky is predicted to be blue, animals have varying brown shades, the
    cloud is white, and so on.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.4.2* 演示了CIFAR10测试数据集中的灰度图像颜色化。*图3.4.3* 比较了地面真实值与颜色化自编码器的预测。自编码器完成了一个可接受的颜色化任务。海洋或天空被预测为蓝色，动物具有不同的棕色阴影，云是白色的，等等。'
- en: There are some noticeable wrong predictions like red vehicles have become blue
    or blue vehicles become red, and the occasional green field has been mistaken
    as blue skies, and dark or golden skies are converted to blue skies.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些明显的错误预测，比如红色车辆变成了蓝色，或者蓝色车辆变成了红色，偶尔绿色的田野被误判为蓝天，暗色或金色的天空被转换成了蓝色天空。
- en: '![Automatic colorization autoencoder](img/B08956_03_13.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![自动上色自编码器](img/B08956_03_13.jpg)'
- en: 'Figure 3.4.2: Automatic grayscale to color image conversion using the autoencoder.
    CIFAR10 test grayscale input images (left) and predicted color images (right).
    Original color photo can be found on the book GitHub repository, https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter3-autoencoders/README.md.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4.2：使用自编码器进行自动灰度到彩色图像转换。CIFAR10 测试灰度输入图像（左）和预测的彩色图像（右）。原始彩色照片可以在书籍的GitHub仓库中找到，https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter3-autoencoders/README.md。
- en: '![Automatic colorization autoencoder](img/B08956_03_14.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![自动上色自编码器](img/B08956_03_14.jpg)'
- en: 'Figure 3.4.3: Side by side comparison of ground truth color images and predicted
    colorized images. Original color photos can be found on the book GitHub repository,
    https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter3-autoencoders/README.md.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4.3：实地对比真实的彩色图像和预测的上色图像。原始彩色照片可以在书籍的GitHub仓库中找到，https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter3-autoencoders/README.md。
- en: Conclusion
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we've been introduced to autoencoders, which are neural networks that
    compress input data into low-dimensional codes in order to efficiently perform
    structural transformations such as denoising and colorization. We've laid the
    foundations to the more advanced topics of GANs and VAEs, that we will introduce
    in later chapters, while still exploring how autoencoders can utilize Keras. We've
    demonstrated how to implement an autoencoder from two building block models, both
    encoder and decoder. We've also learned how the extraction of a hidden structure
    of input distribution is one of the common tasks in AI.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了自编码器，它是一种神经网络，将输入数据压缩成低维度的编码，以便高效地执行结构性转换，如去噪和上色。我们为更高级的话题——生成对抗网络（GANs）和变分自编码器（VAEs）奠定了基础，这些将在后续章节中介绍，同时仍然探讨自编码器如何利用Keras。我们展示了如何从两个基本模型（编码器和解码器）实现自编码器。我们还学会了如何提取输入分布的隐藏结构，这是AI中的常见任务之一。
- en: Once the latent code has been uncovered, there are many structural operations
    that can be performed on the original input distribution. In order to gain a better
    understanding of the input distribution, the hidden structure in the form of the
    latent vector can be visualized using low-level embedding similar to what we did
    in this chapter or through more sophisticated dimensionality reduction techniques
    such t-SNE or PCA.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦潜在编码被揭示出来，就可以对原始输入分布执行许多结构性操作。为了更好地理解输入分布，可以通过低级嵌入方式（类似于我们在本章中所做的）或通过更复杂的降维技术，如t-SNE或PCA，来可视化潜在向量形式的隐藏结构。
- en: Apart from denoising and colorization, autoencoders are used in converting input
    distribution to low-dimensional latent codes that can be further processed for
    other tasks such as segmentation, detection, tracking, reconstruction, visual
    understanding, and so on. In [Chapter 8](ch08.html "Chapter 8. Variational Autoencoders
    (VAEs)"), *Variational Autoencoders (VAEs)*, we will discuss VAEs which are structurally
    the same as autoencoder but differ by having an interpretable latent code that
    can produce a continuous latent codes projection. In the next chapter, we will
    embark on one of the most important recent breakthroughs in AI, the introduction
    of GANs where we will learn of the core strengths of GANs and their ability to
    synthesize data or signals that look real.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 除了去噪和上色外，自编码器还用于将输入分布转换为低维度潜在编码，这些编码可以进一步处理用于其他任务，如分割、检测、跟踪、重建、视觉理解等。在[第8章](ch08.html
    "第8章 变分自编码器 (VAEs)")，*变分自编码器（VAEs）*中，我们将讨论与自编码器结构相同，但通过具有可解释的潜在编码来区分的VAEs，这些编码能够生成连续的潜在编码投影。在下一章，我们将介绍AI领域最近最重要的突破之一——生成对抗网络（GANs），在其中我们将学习GANs的核心优势及其合成看似真实的数据或信号的能力。
- en: References
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ian Goodfellow and others. *Deep learning*. Vol. 1\. Cambridge: MIT press,
    2016 ([http://www.deeplearningbook.org/](http://www.deeplearningbook.org/)).'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ian Goodfellow 等人。*深度学习*。第1卷。剑桥：MIT出版社，2016年 ([http://www.deeplearningbook.org/](http://www.deeplearningbook.org/))。
