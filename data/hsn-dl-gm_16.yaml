- en: Obstacle Tower Challenge and Beyond
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 障碍塔挑战及其后续
- en: 'In this chapter, our final one, we will take a look at the current and future
    state of **deep learning** (**DL**) and **deep reinforcement learning** (**DRL**)
    for games. We take an honest and candid look to see whether these technologies
    are ready for prime-time commercial games or whether they are just novelties.
    Are we poised to see DRL agents beating human players at every game imaginable
    a few years from now? While that remains to be seen, and things are changing quickly,
    the question really is this: is DL ready for your game? It likely is a question
    you are asking yourself at this very moment, and it is hopefully one we will answer
    in this chapter.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的最后一章，我们将审视游戏中**深度学习（DL）**和**深度强化学习（DRL）**的当前和未来状态。我们诚实而坦率地看待这些技术是否已经准备好投入商业游戏，或者它们只是新奇玩意。几年后，我们是否会看到DRL代理在每一款游戏中击败人类玩家？尽管这还有待观察，而且事情变化迅速，但真正的问题是：DL是否准备好为您的游戏服务？这可能是您此刻正在问自己的问题，希望我们在本章中能够回答。
- en: 'This chapter will be a mix of hands-on exercises and general discussions with
    unfortunately no exercises. Well, there is one big exercise, but we will get to
    that shortly. Here is what we will cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将是一些实际练习和一般讨论的结合，不幸的是没有练习。好吧，有一个大练习，但我们很快就会谈到。以下是本章将涵盖的内容：
- en: The Unity Obstacle Tower challenge
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Unity障碍塔挑战**'
- en: Deep Learning for your game?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的游戏的深度学习？
- en: Building your game
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制作你的游戏
- en: More foundations of learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多学习的基础知识
- en: This chapter assumes you have covered numerous exercises in this book in order
    to understand the context. We will refer to those sections in order to remind
    the reader, but please don't jump to this chapter first.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假定您已经完成了本书中的众多练习，以便理解上下文。我们将提到这些部分以提醒读者，请不要跳到本章。
- en: The Unity Obstacle Tower Challenge
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Unity障碍塔挑战**'
- en: The **Unity Obstacle Tower Challenge** was introduced in February 2019 as a
    discrete visual learning problem. As we have seen before, this is the holy grail
    of learning for games, robotics, and other simulations. What makes it more interesting
    is this challenge was introduced outside of ML-Agents and requires the challenger
    to write their own Python code from scratch to control the game—something we have
    come close to learning how to do in this book, but we omitted the technical details.
    Instead, we focused on the fundamentals of tuning hyperparameters, understanding
    rewards, and the agent state. All of these fundamentals will come in handy if
    you decide to tackle the tower challenge.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**Unity障碍塔挑战**于2019年2月引入，作为一个离散的视觉学习问题。正如我们之前所见，这是游戏、机器人和其他模拟学习的圣杯。更有趣的是，这一挑战是在ML-Agents之外引入的，并且要求挑战者从头开始编写他们自己的Python代码来控制游戏——这是我们在本书中接近学习如何做到的，但我们省略了技术细节。相反，我们专注于调整超参数、理解奖励和代理状态的基础知识。如果您决定挑战这个塔，所有这些基础知识都将派上用场。'
- en: At the time this book was written, the ML-Agents version used for developing
    was `0.6`. If you have run all the exercises to completion, you will have noticed
    that all of the visual learning environments using a discrete action space suffer
    from a vanishing or exploding gradient problem. What you will see happen is the
    agent essentially learning nothing and performing random actions; this often takes
    several hundred thousand iterations to see. But we don't see this problem in environments
    with a smaller state space using vector observations. In visual environments with
    a large input state, though, the problem can be seen quite regularly. This means
    that, essentially, at the time of writing anyway, you would not want to use the
    Unity code; it currently is a poor visual learner of discrete actions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，用于开发的ML-Agents版本是`0.6`。如果您已经完成了所有的练习，您会注意到，所有使用离散动作空间的视觉学习环境都存在梯度消失或梯度爆炸问题。您将看到的情况是代理基本上学不到东西，并执行随机动作；通常需要数十万次迭代才能看到结果。但在使用矢量观察的状态空间较小的环境中，我们并不会看到这个问题。然而，在具有大输入状态的视觉环境中，这个问题经常会出现。这意味着，基本上在撰写本书时，您不会希望使用Unity代码；它目前是离散动作的可视学习者。
- en: At the time of writing, the Unity Obstacle Tower Challenge has just started,
    and early metrics are already being reported. The current leading algorithm from
    Google, DeepMind, not surprisingly, is an algorithm called **Rainbow**. In short,
    Rainbow is the culmination of many different DRL algorithms and techniques all
    combined to better learn the discrete action visual-learning space that the tower
    so well defines.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在写这篇文章时，Unity Obstacle Tower Challenge刚刚启动，早期的度量指标已经开始报告。目前，谷歌DeepMind提出的领先算法毫不奇怪，就是一个名为**Rainbow**的算法。简而言之，Rainbow是许多不同的深度强化学习（DRL）算法和技术的结合，旨在更好地学习障碍塔所定义的离散动作视觉学习空间。
- en: Now that we have established that you likely want to write your own code, we
    will understand the high-level critical pieces your agent needs to address. It
    likely would take another book to explain how to do the coding and other technical
    aspects of that, so we will instead talk about the overall challenges and the
    critical elements you need to address. Also, the winners will more than likely
    need to use more probabilistic methods in order to address the problem, and that
    is currently not covered very well anywhere.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确认你可能想要编写自己的代码，那么接下来我们将理解你的代理需要解决的高层关键问题。解释如何编写代码以及其他技术细节可能需要另一本书，因此我们将讨论整体挑战和你需要解决的关键要素。此外，获胜者更可能需要使用更多的概率方法来解决问题，而这一点目前在任何地方的讨论都不充分。
- en: 'Let''s set up the challenge and get it running in the next exercise:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的练习中设置挑战并启动它：
- en: Download the Obstacle Tower Environment as a binary from [https://github.com/Unity-Technologies/obstacle-tower-env](https://github.com/Unity-Technologies/obstacle-tower-env).
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://github.com/Unity-Technologies/obstacle-tower-env](https://github.com/Unity-Technologies/obstacle-tower-env)下载Obstacle
    Tower环境的二进制文件。
- en: Follow the instructions and download the zip file for your environment as directed.
    On most systems, this just requires downloading and unzipping the file into a
    folder you will execute from later.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照指示操作，下载适合你环境的压缩文件。在大多数系统上，这只需要下载并解压到一个文件夹，稍后你将在该文件夹中执行文件。
- en: Unzip the file into a well-known folder.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文件解压到一个常用的文件夹中。
- en: 'Launch the program by double-clicking on it (Windows) to enter the name in
    a console. After you launch the challenge, you can actually play it as a human.
    Play the game and see how many floors you can climb. An example of the running
    challenge is shown in the following screenshot:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过双击程序（Windows）或在控制台中输入名称来启动程序。启动挑战后，你实际上可以像人类一样参与其中。玩这个游戏，看看你能爬到多少楼层。以下截图展示了正在运行的挑战示例：
- en: '![](img/4dbf2be9-5c36-431e-bdd0-504185403822.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4dbf2be9-5c36-431e-bdd0-504185403822.png)'
- en: The Obstacle Tower Challenge in player mode
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 玩家模式下的Obstacle Tower挑战
- en: One of the first things you will learn as you progress through the game is that
    the game starts out quite simply, but on the later floors, it gets quite difficult,
    even for a human.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你在游戏过程中会学到的第一件事之一是，游戏开始时相对简单，但在后面的楼层，难度会增加，甚至对人类来说也很困难。
- en: 'Now, as we mentioned, solving this challenge is well beyond the scope of this
    book, but hopefully you can now appreciate some of the complexities that currently
    stifle the field of deep reinforcement learning. We have reviewed the major challenges
    that you will face when undertaking this method in the following table:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，解决这个挑战超出了本书的范围，但希望你现在能理解一些目前制约深度强化学习领域的复杂性。我们已经在下表中回顾了你在进行此方法时将面临的主要挑战：
- en: '| **Problem** | **Chapter** | **Current** **Status** | **Future** |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **问题** | **章节** | **当前** **状态** | **未来** |'
- en: '| Visual observation state—you will need to build a complex enough CNN and
    possibly recurrent networks to encode enough details in the visual state. | [Chapter
    7](9b7b6ff8-8daa-42bd-a80f-a7379c37c011.xhtml), *Agent and the Environment* |
    The current Unity visual encoder is far from acceptable. | Fortunately, there
    is plenty of work always being done with CNN and recurrent networks for analysis
    of videos. Remember, you don''t just want to capture static images; you also want
    to encode the sequence of the images. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 视觉观测状态——你需要构建一个足够复杂的卷积神经网络（CNN），并可能需要递归神经网络（RNN）来编码视觉状态中的足够细节。 | [第7章](9b7b6ff8-8daa-42bd-a80f-a7379c37c011.xhtml)，*代理与环境*
    | 当前的Unity视觉编码器远未达标。 | 幸运的是，CNN和递归网络在视频分析中已有大量研究。记住，你不仅仅是想捕捉静态图像；你还需要编码图像的序列。
    |'
- en: '| DQN, DDQN, or Rainbow | [Chapter 5](6ca7a117-1a8c-49f9-89c0-ee2f2a1e8baf.xhtml),
    *Introducing DRL* | Rainbow is currently the best, and it is available on the
    GCP. | As we have seen in this book, PPO only performs well on continuous action
    spaces. In order to tackle the discrete action space, we look back to more fundamental
    methods such as DQN or the newcomer Rainbow, which is the summation of all base
    methods. We will also discuss future ways in which further use of deep probabilistic
    methods may be the answer. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| DQN, DDQN 或 Rainbow | [第5章](6ca7a117-1a8c-49f9-89c0-ee2f2a1e8baf.xhtml),
    *介绍深度强化学习* | Rainbow目前是最好的，并且可以在GCP上使用。 | 正如我们在本书中看到的，PPO仅在连续动作空间上表现良好。为了应对离散动作空间的问题，我们回顾了更基础的方法，如DQN或新兴的Rainbow，它是所有基本方法的汇总。我们还将讨论未来可能通过进一步使用深度概率方法来解决当前问题的途径。
    |'
- en: '| Intrinsic rewards | [Chapter 9](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml),
    *Rewards and Reinforcement Learning* | The use of an intrinsic reward system shows
    promise for exploration. | Being able to introduce intrinsic reward systems such
    as **Curiosity Learning** allows the agent to explore new environments based on
    some expectation of state. This method will be essential for any algorithm that
    plans to reach the higher levels of the tower. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 内在奖励 | [第9章](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml), *奖励与强化学习* | 使用内在奖励系统在探索方面表现出色。
    | 引入像**好奇心学习**这样的内在奖励系统，可以让智能体根据某种对状态的期望来探索新环境。这种方法将对任何计划达到塔楼更高层次的算法至关重要。 |'
- en: '| Understanding | [Chapter 6](b422aff5-b743-4696-ba80-e0a222ea5b4d.xhtml),
    *Unity ML-Agents* | Unity provides an excellent sample environment to build and
    test models on. | You can easily build and test a similar environment in Unity
    quite quickly and on your own. It is no wonder Unity never released the raw Unity
    environment as a project. This was more than likely because this would have attracted
    many novices, thinking they could overcome the problem with just training. Sometimes,
    training is just not the answer. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 理解 | [第6章](b422aff5-b743-4696-ba80-e0a222ea5b4d.xhtml), *Unity ML-Agents*
    | Unity提供了一个出色的示范环境，用于构建和测试模型。 | 你可以很容易地在Unity中快速构建并独立测试一个类似的环境。难怪Unity从未发布过原始的Unity环境作为项目。这很可能是因为这会吸引许多初学者，他们以为仅凭训练就能解决问题。但有时候，训练并不是答案。
    |'
- en: '| Sparse rewards | [Chapter 9](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml),
    *Rewards and Reinforcement Learning*[Chapter 10](1525f2f4-b9e1-4b7f-ac40-33e801c668ed.xhtml),
    *Imitation and Transfer Learning* | Could implement Curriculum or Imitation Learning.
    | We have already covered many examples of ways to manage the sparse rewards problem.
    It will be interesting to see how much the winners depend on one of these methods,
    such as IL, to win. |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏奖励 | [第9章](ae184eca-6c9d-456e-a72b-85274ddcc10c.xhtml), *奖励与强化学习* [第10章](1525f2f4-b9e1-4b7f-ac40-33e801c668ed.xhtml),
    *模仿与迁移学习* | 可以实施课程学习或模仿学习。 | 我们已经讨论了许多管理稀疏奖励问题的示例。看看获胜者是否依赖这些方法中的一种，如模仿学习（IL）来取得胜利，将会非常有趣。
    |'
- en: '| Discrete actions | [Chapter 8](1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml), *Understanding
    PPO* | We learned how PPO allowed continuous action problems to learn, using stochastic
    methods. | As we alluded to before, it will likely take new work into more deep
    probabilistic methods and techniques to work around some of the current problems.
    This will likely require the development of new techniques using new algorithms,
    and how long that takes remains to be seen. |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 离散动作 | [第8章](1393797c-79cd-46c3-8e43-a09a7750fc92.xhtml), *理解PPO* | 我们学会了如何利用PPO通过随机方法解决连续动作问题。
    | 正如我们之前提到的，可能需要通过深度概率方法和技术来解决当前的一些问题。这可能需要新算法的开发，而开发所需的时间仍然需要观察。 |'
- en: Each of the problems highlighted in the preceding table will likely need to
    be solved in part or wholly in order to get an agent from floor 1 to 100 to complete
    the entire challenge. It remains to be seen how this will play out for Unity,
    the winner, and DRL as a whole. In the next section, we discuss the practical
    applications of DL and DRL, and how they can be used for your game.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 前表中突出的每个问题可能需要部分或全部解决，才能让智能体从1层到100层，完成整个挑战。如何在Unity、DRL以及整个深度强化学习领域中发挥作用，还需要进一步观察。在接下来的部分，我们将讨论深度学习和深度强化学习的实际应用，以及它们如何用于你的游戏。
- en: Deep Learning for your game?
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在你的游戏中的应用？
- en: 'It''s likely the reason you picked this book up was to learn about DL and DLR
    for games in the hope of landing your dream job or completing your dream game.
    In either case, we come to a point where you decide whether this technology is
    worth including in your own game and to what extent. The following is a list of
    ten questions you can use to determine whether DL is right for your game:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能是因为希望通过学习深度学习（DL）和深度强化学习（DRL）在游戏中的应用，进而获得理想的工作或完成理想的游戏，才开始阅读这本书。无论如何，你会面临一个问题：决定这项技术是否值得加入自己的游戏，以及在什么程度上加入。以下是十个问题，可以帮助你判断深度学习（DL）是否适合你的游戏：
- en: Have you already made the decision and need to build the game with DL or DRL?
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是否已经决定并需要使用深度学习（DL）或深度强化学习（DRL）来构建游戏？
- en: Yes – 10 points
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是的 – 10分
- en: No – 0 points
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不是 – 0分
- en: Will your game benefit from some form of automation, either through testing
    or managing repetitious player tasks?
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的游戏是否能从某种形式的自动化中受益，无论是通过测试还是管理重复性的玩家任务？
- en: Yes – 10 points
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是的 – 10分
- en: No – 0 points
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不是 – 0分
- en: Do you want to make training and AI or another similar activity part of the
    game?
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是否希望将训练、人工智能或其他类似活动作为游戏的一部分？
- en: Yes – (-5) points. *You may be better off using a more robust from of AI to
    simulate the training. Training DRL takes too many iterations and samples to be
    effective as an inline game-training tool, at least for now.*
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是的 – (-5)分。*你可能更适合使用一种更强大的人工智能来模拟训练。训练DRL需要太多的迭代和样本，至少目前，它作为游戏内训练工具并不高效。*
- en: No – 0 points.
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不是 – 0分。
- en: Do you want cutting-edge AI to feature in your game?
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是否希望在你的游戏中加入前沿的人工智能技术？
- en: Yes – 10 points. *There are certainly ways of layering AI technologies and making
    a DRL solution work. When it comes to current AI, there really is no better cutting-edge
    technology.*
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是的 – 10分。*确实有很多方法可以将人工智能技术叠加，并让深度强化学习（DRL）解决方案奏效。谈到当前的人工智能技术，真的没有比这更前沿的技术了。*
- en: No – 0 points.
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不是 – 0分。
- en: Do you have hours of time to train an AI?
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是否有足够的时间来训练人工智能？
- en: Yes – 10 points
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是的 – 10分
- en: No – (-10) points
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不是 – (-10)分
- en: Have you read a good portion of this book and completed at least a few of the
    exercises?
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是否阅读了这本书的很大一部分，并完成了至少一些练习？
- en: Yes – 10 points, +5 if you completed more than 50%
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是的 – 10分，若完成超过50%则加5分
- en: No – (-10) points; thanks for the honesty
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不是 – (-10)分；感谢你的诚实
- en: Do you have a background or affinity for math?
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是否有数学背景或对数学感兴趣？
- en: Yes – 10 points
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是的 – 10分
- en: No – (-10) points
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不是 – (-10)分
- en: How many papers have you read on reinforcement learning at an academic level?
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你在学术层面阅读了多少关于强化学习的论文？
- en: 10+ – 25 points
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 10+ – 25分
- en: 5–10 – 10 points
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 5–10 – 10分
- en: 1–5 – 5 points
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1–5 – 5分
- en: 0 – 0 points
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 – 0分
- en: What is your completion timeline?
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的完成时间表是什么？
- en: 1–3 months – (-10) points
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1–3个月 – (-10)分
- en: 3–6 months – 0 points
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3–6个月 – 0分
- en: 6–12 months – 10 points
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 6–12个月 – 10分
- en: 1–2+ years – 25 points
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1–2年 – 25分
- en: What is the size of your team?
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的团队规模是多少？
- en: Solo – (-10) points
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单打独斗 – (-10)分
- en: 2–5 – 0 points
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2–5 – 0分
- en: 6–10 – 10 points
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 6–10 – 10分
- en: 11+ – 25 points
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 11+ – 25分
- en: 'Answer all the questions and score your points to determine your full readiness
    score. Consult the following to determine how ready you and/or your team are:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 回答所有问题并评分，以确定你是否完全准备好。请参阅以下内容，了解你和/或你的团队的准备情况：
- en: '**<0 points** - How did you even make it this far into the book? You''re not
    ready, and it''s best you just put this book down.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<0分** - 你是怎么走到这本书的这一部分的？你还没有准备好，最好放下这本书。'
- en: '**0-50** - You certainly show promise, but you are going to need some more
    help; check out the following section on next steps and further areas of learning.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**0-50** - 你显然有潜力，但你还需要更多帮助；查看下一步和进一步学习领域的部分。'
- en: '**50-100** - You certainly are on your way to building the knowledge base and
    implementing some fun DRL in games, but you may still need a little help. Check
    the section on next steps and further areas of learning.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**50-100** - 你显然在构建知识基础和实现有趣的深度强化学习（DRL）方面有了进展，但你可能仍然需要一些帮助。查看下一步和进一步学习领域的部分。'
- en: '**100+** - You are well beyond ready, and we appreciate you taking the time
    to read this book. Perhaps take some of your own personal time and pass your own
    or your team members'' knowledge on to people you know.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**100+** - 你已经完全准备好了，非常感谢你抽出时间阅读这本书。也许可以利用一些个人时间，将你的知识传递给你认识的人或团队成员。'
- en: Of course, there are no absolute rules to the results of the preceding test,
    and you may find that you score quite low but then go on to make the next great
    AI game. How you approach the results is up to you, and how you take your next
    steps is also entirely up to you.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，前述测试结果没有绝对的规则，您可能会发现自己的得分很低，但随后可能会做出下一个伟大的人工智能游戏。您如何看待结果由您决定，下一步如何进行也完全由您决定。
- en: In the next section, we look at the next steps you can take to learn more about
    DRL and how to build better automation and AI in games.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将探讨您可以采取的下一步措施，以便深入了解 DRL，并如何在游戏中构建更好的自动化和人工智能。
- en: Building your game
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建您的游戏
- en: 'Now that you have decided to use deep learning and/or deep reinforcement learning
    for your game, it is time to determine how you plan to implement various functionality
    in your game. In order to do that, we are going to go through a table outlining
    the steps you need to go through in order to build your game''s AI agent:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经决定为您的游戏使用深度学习和/或深度强化学习，是时候确定如何在游戏中实现各种功能了。为了做到这一点，我们将通过一个表格，概述您需要经过的步骤来构建游戏的人工智能代理：
- en: '| **Step** | **Action** | **Summary** |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **步骤** | **行动** | **总结** |'
- en: '| Start | Determine at what level you want the AI in the game to operate, from
    basic, perhaps for just testing and simple automation, to advanced, where the
    AI will complete against the player. | Determine the level of AI. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 启动 | 确定您希望游戏中的人工智能在什么层次上操作，从基础层次（也许只是用于测试和简单的自动化）到高级层次（人工智能与玩家对抗）。 | 确定人工智能的层次。
    |'
- en: '| Resourcing | Determine the amount of resources. Basic AI or automation could
    be handled within the team itself, whereas more complex AI may require one or
    many experienced members of staff. | Team requirements. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 资源配置 | 确定资源的数量。基本的人工智能或自动化可以由团队内部处理，而更复杂的人工智能可能需要一个或多个经验丰富的团队成员。 | 团队需求。
    |'
- en: '| Knowledge | Determine the level of knowledge the team possesses and what
    will be required. It is a given that any team implementing new AI will need to
    learn new skills.  | Knowledge-gap analysis. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 知识 | 确定团队所拥有的知识水平以及所需的知识。显然，任何实施新人工智能的团队都需要学习新技能。 | 知识差距分析。 |'
- en: '| Demonstration | Always start by building a simple but workable proof of concept
    that demonstrates all critical aspects of the system. | Demonstrate the team can
    complete the basic premise. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 演示 | 始终从构建一个简单但可行的概念验证开始，展示系统的所有关键方面。 | 演示团队能够完成基本前提。 |'
- en: '| Implementation | Build the actual system in a way that is simplistic and
    maintainable. Keep all the things you know simple and clean. | Build the system.
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 实施 | 以简洁且可维护的方式构建实际系统。保持所有已知的内容简单清晰。 | 构建系统。 |'
- en: '| Testing | Test the system over and over again. It is critical that the system
    is tested thoroughly, and of course what better way to do that than with a DRL
    automated test system. | Test the system. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 一遍又一遍地测试系统。系统必须彻底测试，当然，最好的测试方法就是使用 DRL 自动化测试系统。 | 测试系统。 |'
- en: '| Fix | As anyone who has developed software for more than a few weeks will
    tell you, the process is build, test, fix, and repeat. That essentially is the
    software development process, so try not to add too many other bells and whistles
    to distract from that. | Fixing the system. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 修复 | 正如任何开发过软件超过几周的人所告诉你的那样，过程是构建、测试、修复并重复。这本质上就是软件开发的过程，因此尽量不要增加太多其他无关的功能，以免分散注意力。
    | 修复系统。 |'
- en: '| Release | Releasing software to users/players is absolutely critical to a
    successful game or software product of any kind. You will always want to release
    early and often, which means your players must be encouraged to test, and to provide
    feedback. | Let the bugs out. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 发布 | 向用户/玩家发布软件对成功的游戏或任何类型的软件产品至关重要。您始终希望尽早发布并频繁发布，这意味着必须鼓励玩家进行测试并提供反馈。 |
    发布错误。 |'
- en: '| Repeat | The cycle is endless and will continue as long as your product/game
    makes money. | Support the system. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 重复 | 这一过程是无止境的，只要您的产品/游戏能带来利润，它就会持续进行。 | 支持系统。 |'
- en: The preceding process is the basic premise and will work for most of your development
    needs. In most cases, you may want to track individual work items such as features
    or bugs on a work or task board. You may want to use a more defined process such
    as Scrum, but often keeping things simple is your best course of action.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 前述过程是基本前提，适用于大多数开发需求。在大多数情况下，您可能希望在工作或任务板上跟踪单个工作项，如功能或错误。您可能希望使用更明确的流程，例如 Scrum，但通常保持简洁是最好的行动方式。
- en: Scrum and other software development processes are great examples to learn from,
    but unless you have formally trained staff, it's better to avoid trying to implement
    these yourself. There are often subtle rules that need to be enforced in these
    processes for them to work as they claim to. Even trained Scrum Masters may need
    to battle daily to enforce these rules in many organizations, and in the end their
    value becomes more management-driven than developer-focused. Use the previous
    table as a guide for the steps you take in building your next game, and always
    remember that build, release, fix, and repeat is the key to good software.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Scrum及其他软件开发流程是很好的学习范例，但除非你有经过正式培训的员工，否则最好避免自己去实施这些流程。这些流程中往往有一些微妙的规则，需要执行才能像它们所声称的那样有效。即使是经过培训的Scrum
    Master，也可能需要在许多组织中每天进行斗争，才能落实这些规则，最终它们的价值变得更多是由管理驱动，而非开发者主导。可以参考前面的表格作为你构建下一个游戏时的步骤指南，始终记住“构建、发布、修复、重复”是做好软件的关键。
- en: In the next section, we will look at other things you can use to expand your
    learning.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将介绍你可以用来扩展学习的其他内容。
- en: More foundations of learning
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多的学习基础
- en: 'There is an ever-growing resource for learning about machine learning, DL,
    and of course DLR. The list is becoming very large, and there are many materials
    to choose from. For that reason, we will now summarize the areas we feel show
    the most promise for developing AI and DL for games:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有着日益增长的学习机器学习、深度学习和深度学习回归（DLR）的资源。这个资源库正在不断扩大，选择材料的余地也越来越多。因此，我们现在总结出我们认为对游戏AI和深度学习最具前景的领域：
- en: '**Basic Data Science Course**: If you have never taken a basic fundamentals
    course on data science, then you certainly should. The foundations of understanding
    the qualities of data, statistics, probability, and variability are too numerous
    to mention. Be sure to cover this foundation first.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础数据科学课程**：如果你从未学习过数据科学的基础课程，那么你肯定应该学习一门。这些课程可以帮助你理解数据的特性、统计学、概率和变异性，这些基础知识多得无法一一列举。务必先打好这个基础。'
- en: '**Probabilistic Programming**: This is a combination of various variational
    inference methods by which to answer problems given a probability of events with
    an answer of the probability that some event may occur. These types of models
    and languages have been used to analyze financial information and risk for years,
    but they are now coming to the forefront in ML technologies.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概率编程**：这是通过变分推断方法的结合，来回答给定事件的概率及某事件可能发生的概率。这些类型的模型和语言已经用于多年来分析金融信息和风险，但它们现在在机器学习技术中逐渐崭露头角。'
- en: '**Deep Probabilistic Programming**: This is the combination of variational
    inference and DL models. Variational inference is the process by which you answer
    a question with a probability given the input of possibly multiple probabilities.
    So, instead of using a series of weights to train a network, we use a series of
    probability distributions. This method has proven to be very effective and has
    recently performed visual image classification tasks with a modified probabilistic
    CNN model.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度概率编程**：这是变分推断与深度学习模型的结合。变分推断是一个过程，你通过给定多个可能概率的输入来回答一个带有概率的问题。因此，我们不是用一系列权重来训练网络，而是使用一系列概率分布。这种方法已经证明非常有效，最近它已经用修改后的概率CNN模型执行了视觉图像分类任务。'
- en: '**Visual state classification and encoding**: A critical aspect to a DL system
    is the development of CNN models to classify images. You will need to understand
    this space very well in order to build the networks for your game environment.
    Recall that different environments may require CNN models.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视觉状态分类与编码**：深度学习系统的一个关键方面是开发卷积神经网络（CNN）模型来分类图像。为了为你的游戏环境构建网络，你需要深入理解这个领域。请记住，不同的环境可能需要使用CNN模型。'
- en: '**Memory**: Memory can of course come in all forms, but the primary one of
    interest is the **recurrent neural network** (**RNN**). Early on in this book,
    we looked at the current standard recurrent network model we use called the **long
    short-term memory** (**LSTM**) **block**. Even at the time of writing, there is
    a renewed interest in the **gated recurrent unit** (**GRU**), a more complex recurrent
    network that has been shown to handle the vanishing gradient problem better. There
    is always an interest in cloud or other supported technologies and how they may
    interact with new DL technologies.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**记忆**：记忆当然可以有多种形式，但最值得关注的主要形式是**递归神经网络**（**RNN**）。在本书的早期，我们介绍了目前使用的标准递归网络模型——**长短时记忆**（**LSTM**）**块**。即使在写作时，关于**门控递归单元**（**GRU**）的兴趣也在重新升温，这是一种更复杂的递归网络，已被证明能更好地解决梯度消失问题。人们始终对云技术或其他支持的技术以及它们如何与新的深度学习技术互动充满兴趣。'
- en: '**DL as a Service**: Companies such as Google, Amazon, Microsoft, OpenAI, and
    others who claim to be all about openness are often far from it. In most cases,
    if you want to incorporate these technologies into your game, you will need to
    subscribe to their service—which of course has its own pluses and minuses. The
    major problem is that if your game becomes popular and if you rely heavily on
    the DL service, your profits will be tied to it. Fortunately, Unity has yet to
    take this approach, but that does remain to be seen depending on how easily the
    community solves the Obstacle Tower Challenge.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习即服务**：像 Google、Amazon、Microsoft、OpenAI 等公司，虽然声称注重开放性，但通常远非如此。在大多数情况下，如果你想将这些技术融入你的游戏，你需要订阅他们的服务——当然，这也有其优缺点。主要问题在于，如果你的游戏变得非常流行，并且你过度依赖深度学习服务，你的利润就会与其挂钩。幸运的是，Unity
    至今还没有采取这种方式，但这一切还得看社区如何顺利解决障碍塔挑战。'
- en: '**Math**: In general, you will want to always advance your math skills whether
    you plan to dig deep into building your own models or not. In the end, your gut
    understanding of the math will provide you with the insights you need to overcome
    these complex technologies.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数学**：一般来说，无论你是否打算深入构建自己的模型，你都需要不断提升自己的数学技能。最终，你对数学的直觉理解将为你提供解决这些复杂技术所需的洞察力。'
- en: '**Perseverance**: Learn to fail, and then move on. This is critical and something
    many new developers often get disgruntled with and then move on to something easier,
    simpler, and less rewarding. Be happy when you fail, as failing is learning to
    understand. If you never fail, you really never learn, so learn to fail.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**毅力**：学会失败，然后继续前行。这是至关重要的，也是许多新开发者常常感到沮丧的地方，他们会选择转向更简单、更容易、更少回报的事情。当你失败时要高兴，因为失败是学习和理解的过程。如果你从未失败过，那你其实从未真正学习过，所以学会去失败。'
- en: A hard-coded list of learning resources would likely get out of date before
    this book is even printed or released. Use the preceding list to generalize your
    learning and broaden your basic machine learning and data science knowledge as
    well. First and foremost, DL is a data science pursuit that serves respect to
    the data; never forget that as well.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 硬编码的学习资源列表很可能在这本书还没有印刷或发布之前就已经过时。请利用前面的列表来概括你的学习，拓宽你在基础机器学习和数据科学方面的知识。最重要的是，深度学习是一项数据科学追求，必须尊重数据；永远不要忘记这一点。
- en: In the next section for our final chapter, we will summarize this chapter and
    the book.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节的最终章中，我们将总结本章内容和整本书。
- en: Summary
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we took a short tour of many basic concepts involving your
    next steps in DL and DRL; perhaps you will decide to pursue the Unity Obstacle
    Tower Challenge and complete that or just use DRL in your own project. We looked
    at simple quizzes in order to evaluate your potential for diving in and using
    DRL in a game. From there, we looked at the next steps in development, and then
    finally we looked at other areas of learning may want to focus on.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要介绍了与深度学习（DL）和深度强化学习（DRL）相关的许多基本概念；也许你会决定参与 Unity 障碍塔挑战并完成它，或者仅在自己的项目中使用
    DRL。我们通过简单的测验来评估你是否有潜力深入学习并在游戏中应用 DRL。之后，我们探讨了开发的下一步，并最终看到了可能想要专注的其他学习领域。
- en: This book was an exercise in understanding how effective DL can be when applied
    to your game project in the future. We explored many areas of basic DL principles
    early on and looked at more specific network types such as CNN and LSTM. Then,
    we looked at how these basics network forms could be applied to applications for
    driving and building a chatbot. From there, we looked at the current king of machine
    learning algorithms, reinforcement and deep reinforcement learning. We then looked
    at one of the current leaders, Unity ML-Agents, and how to implement this technology,
    over several chapters by looking at how simple environments are built to more
    complex multi-agent environments. This also allowed us to explore different forms
    of intrinsic/extrinsic rewards and learning systems, including curriculum, curiosity,
    imitation, and transfer learning.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是一次了解深度学习（DL）在未来应用于游戏项目时如何有效的练习。我们一开始探讨了许多基础的DL原理，并研究了更具体的网络类型，如CNN和LSTM。接着，我们考察了这些基础网络形式如何应用于自动驾驶和构建聊天机器人等应用。之后，我们研究了当前机器学习算法的“王者”——强化学习和深度强化学习。然后，我们研究了当前的领导者之一——Unity
    ML-Agents，并通过多个章节讲解如何实现这一技术，逐步从简单的环境构建到更复杂的多智能体环境。这也使我们有机会探索不同形式的内在/外在奖励和学习系统，包括课程学习、好奇心、模仿学习和迁移学习。
- en: Finally, before finishing this chapter, we completed a long exercise regarding
    using DRL for automatic testing and debugging with the added option of using IL
    as a way of enhancing testing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在完成本章之前，我们进行了一个关于使用深度强化学习（DRL）进行自动化测试和调试的长期练习，并额外提供了使用内在学习（IL）增强测试的选项。
