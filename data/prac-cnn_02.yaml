- en: Introduction to Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络简介
- en: '**Convolutional Neural Networks** (**CNNs**) are everywhere. In the last five
    years, we have seen a dramatic rise in the performance of visual recognition systems
    due to the introduction of deep architectures for feature learning and classification.
    CNNs have achieved good performance in a variety of areas, such as automatic speech
    understanding, computer vision, language translation, self-driving cars, and games
    such as Alpha Go. Thus, the applications of CNNs are almost limitless. DeepMind
    (from Google) recently published WaveNet, which uses a CNN to generate speech
    that mimics any human voice ([https://deepmind.com/blog/wavenet-generative-model-raw-audio/](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）无处不在。在过去五年中，由于引入了深度架构用于特征学习和分类，视觉识别系统的性能得到了显著提升。CNN在多个领域取得了良好的表现，如自动语音理解、计算机视觉、语言翻译、自动驾驶汽车以及类似Alpha
    Go的游戏。因此，CNN的应用几乎是无限的。DeepMind（来自谷歌）最近发布了WaveNet，这是一种利用CNN生成模仿任何人类声音的语音的技术（[https://deepmind.com/blog/wavenet-generative-model-raw-audio/](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)）。'
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: History of CNNs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN的历史
- en: Overview of a CNN
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN概述
- en: Image augmentation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像增强
- en: History of CNNs
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN的历史
- en: 'There have been numerous attempts to recognize pictures by machines for decades.
    It is a challenge to mimic the visual recognition system of the human brain in
    a computer. Human vision is the hardest to mimic and most complex sensory cognitive
    system of the brain. We will not discuss biological neurons here, that is, the
    primary visual cortex, but rather focus on artificial neurons. Objects in the
    physical world are three dimensional, whereas pictures of those objects are two
    dimensional. In this book, we will introduce neural networks without appealing
    to brain analogies. In 1963, computer scientist Larry Roberts, who is also known
    as the **father of computer vision**, described the possibility of extracting
    3D geometrical information from 2D perspective views of blocks in his research
    dissertation titled**BLOCK WORLD**. This was the first breakthrough in the world
    of computer vision. Many researchers worldwide in machine learning and artificial
    intelligence followed this work and studied computer vision in the context of
    BLOCK WORLD. Human beings can recognize blocks regardless of any orientation or
    lighting changes that may happen. In this dissertation, he said that it is important
    to understand simple edge-like shapes in images. He extracted these edge-like
    shapes from blocks in order to make the computer understand that these two blocks
    are the same irrespective of orientation:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年来，机器识别图像的尝试层出不穷。模仿人脑的视觉识别系统在计算机中是一项挑战。人类的视觉是大脑中最难模仿且最复杂的感知认知系统。我们在这里不会讨论生物神经元，即初级视觉皮层，而是专注于人工神经元。物理世界中的物体是三维的，而这些物体的图像是二维的。在本书中，我们将介绍神经网络，而不借用大脑类比。1963年，计算机科学家Larry
    Roberts，也被称为**计算机视觉之父**，在他的研究论文《BLOCK WORLD》中描述了从物体的二维透视图中提取三维几何信息的可能性。这是计算机视觉领域的第一个突破。全球许多研究者在机器学习和人工智能领域跟随这项工作，并在BLOCK
    WORLD的背景下研究计算机视觉。人类能够识别物体，不论物体的朝向或光照发生何种变化。在这篇论文中，他指出，理解图像中的简单边缘形状是很重要的。他从方块中提取这些边缘形状，以使计算机理解这两个方块无论朝向如何都是相同的：
- en: '![](img/7e8981b5-bdf4-4023-a271-417a9c96d29f.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e8981b5-bdf4-4023-a271-417a9c96d29f.png)'
- en: The vision starts with a simple structure. This is the beginning of computer
    vision as an engineering model. David Mark, an MIT computer vision scientist,
    gave us the next important concept, that vision is hierarchical. He wrote a very
    influential book named *VISION*. This is a simple book. He said that an image
    consists of several layers. These two principles form the basis of deep learning
    architecture, although they do not tell us what kind of mathematical model to
    use.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉始于一个简单的结构。这是计算机视觉作为工程模型的起点。MIT计算机视觉科学家David Mark给了我们下一个重要的概念，那就是视觉是层次化的。他写了一本非常有影响力的书，名为*VISION*。这是一本简单的书。他说，一幅图像由几个层次组成。这两个原则构成了深度学习架构的基础，尽管它们并没有告诉我们该使用哪种数学模型。
- en: In the 1970s, the first visual recognition algorithm, known as the **generalized
    cylinder model**, came from the AI lab at Stanford University. The idea here is
    that the world is composed of simple shapes and any real-world object is a combination
    of these simple shapes. At the same time, another model, known as the **pictorial
    structure model**, was published from SRI Inc. The concept is still the same as
    the generalized cylinder model, but the parts are connected by springs; thus,
    it introduced a concept of variability. The first visual recognition algorithm
    was used in a digital camera by Fujifilm in 2006.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1970 年代，第一种视觉识别算法——**广义圆柱模型**，来自斯坦福大学的 AI 实验室。这个模型的思想是，世界由简单的形状构成，任何现实世界的物体都是这些简单形状的组合。同时，SRI
    Inc. 发布了另一种模型——**图像结构模型**。其概念与广义圆柱模型相同，但这些部分通过弹簧连接，因此引入了可变性概念。第一个视觉识别算法在 2006
    年由富士胶片公司在数码相机中使用。
- en: Convolutional neural networks
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: 'CNNs, or ConvNets, are quite similar to regular neural networks. They are still
    made up of neurons with weights that can be learned from data. Each neuron receives
    some inputs and performs a dot product. They still have a loss function on the
    last fully connected layer. They can still use a nonlinearity function. All of
    the tips and techniques that we learned from the last chapter are still valid
    for CNN. As we saw in the previous chapter, a regular neural network receives
    input data as a single vector and passes through a series of hidden layers. Every
    hidden layer consists of a set of neurons, wherein every neuron is fully connected
    to all the other neurons in the previous layer. Within a single layer, each neuron
    is completely independent and they do not share any connections. The last fully
    connected layer, also called the **output layer**, contains class scores in the
    case of an image classification problem. Generally, there are three main layers
    in a simple ConvNet. They are the **convolution layer**, the **pooling layer**,
    and the **fully connected layer**. We can see a simple neural network in the following
    image:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 或者 ConvNet 与常规神经网络非常相似。它们仍然由带有权重的神经元组成，这些权重可以通过数据学习得到。每个神经元接收一些输入并进行点积运算。它们仍然在最后的全连接层上使用损失函数。它们仍然可以使用非线性激活函数。我们在上一章学到的所有技巧和方法对于
    CNN 仍然有效。正如我们在上一章中看到的，常规神经网络将输入数据作为一个单一的向量，经过一系列隐藏层。每个隐藏层由一组神经元组成，每个神经元与前一层的所有神经元完全连接。在单一层内，每个神经元是完全独立的，它们之间没有任何连接。最后一个全连接层，也称为**输出层**，在图像分类问题中包含类别得分。一般来说，一个简单的
    ConvNet 包括三个主要层：**卷积层**、**池化层**和**全连接层**。我们可以在下图中看到一个简单的神经网络：
- en: '![](img/22900b9c-4754-4b53-8023-c1f50bfc7eda.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22900b9c-4754-4b53-8023-c1f50bfc7eda.png)'
- en: A regular three-layer neural network
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常规的三层神经网络
- en: So, what changes? Since a CNN mostly takes images as input, this allows us to
    encode a few properties into the network, thus reducing the number of parameters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么改变了呢？由于 CNN 主要以图像作为输入，这使我们能够在网络中编码一些特性，从而减少了参数的数量。
- en: 'In the case of real-world image data, CNNs perform better than **Multi-Layer
    Perceptrons** (**MLPs**). There are two reasons for this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际的图像数据中，CNN 比**多层感知机**（**MLP**）表现更好。其原因有两个：
- en: 'In the last chapter, we saw that in order to feed an image to an MLP, we convert
    the input matrix into a simple numeric vector with no spatial structure. It has
    no knowledge that these numbers are spatially arranged. So, CNNs are built for
    this very reason; that is, to elucidate the patterns in multidimensional data.
    Unlike MLPs, CNNs understand the fact that image pixels that are closer in proximity
    to each other are more heavily related than pixels that are further apart:'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到，为了将图像输入到 MLP 中，我们将输入矩阵转换为一个简单的数值向量，这个向量没有空间结构。它无法理解这些数字是如何在空间上排列的。因此，CNN
    正是为了解决这个问题而设计的，旨在揭示多维数据中的模式。与 MLP 不同，CNN 理解图像中彼此距离较近的像素之间关系比远离的像素之间的关系更强：
- en: '*CNN = Input layer + hidden layer + fully connected layer*'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*CNN = 输入层 + 隐藏层 + 全连接层*'
- en: 'CNNs differ from MLPs in the types of hidden layers that can be included in
    the model. A ConvNet arranges its neurons in three dimensions: **width**, **height**,
    and **depth**. Each layer transforms its 3D input volume into a 3D output volume
    of neurons using activation functions. For example, in the following figure, the
    red input layer holds the image. Thus its width and height are the dimensions
    of the image, and the depth is three since there are Red, Green, and Blue channels:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN与MLP在模型中包含的隐藏层类型上有所不同。一个ConvNet将其神经元按三维方式排列：**宽度**、**高度**和**深度**。每一层使用激活函数将其三维输入体积转换为三维输出体积。例如，在下图中，红色输入层包含图像。因此，它的宽度和高度是图像的维度，深度为三，因为有红色、绿色和蓝色通道：
- en: '![](img/2909a83d-f11b-4e4f-be2b-ed1365c13eae.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2909a83d-f11b-4e4f-be2b-ed1365c13eae.png)'
- en: ConvNets are deep neural networks that share their parameters across space.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ConvNet是深度神经网络，它们在空间上共享参数。
- en: How do computers interpret images?
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机如何解读图像？
- en: Essentially, every image can be represented as a matrix of pixel values. In
    other words, images can be thought of as a function (*f*) that maps from *R²*
    to *R*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，每张图像可以表示为一个像素值矩阵。换句话说，图像可以看作是一个函数（*f*），它从*R²*映射到*R*。
- en: '*f(x, y)* gives the intensity value at the position *(x, y)*. In practice,
    the value of the function ranges only from *0* to *255*. Similarly, a color image
    can be represented as a stack of three functions. We can write this as a vector
    of:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*f(x, y)* 给出位置*(x, y)*处的强度值。实际上，函数的值范围仅从*0*到*255*。类似地，一张彩色图像可以表示为三个函数的堆叠。我们可以将其写为一个向量：'
- en: '* f( x, y) = [ r(x,y) g(x,y) b(x,y)]*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*f(x, y) = [ r(x,y) g(x,y) b(x,y)]*'
- en: 'Or we can write this as a mapping:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们可以将其写成一个映射：
- en: '*f: R x R --> R3*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*f: R x R --> R3*'
- en: So, a color image is also a function, but in this case, a value at each *(x,y)*
    position is not a single number. Instead it is a vector that has three different
    light intensities corresponding to three color channels. The following is the
    code for seeing the details of an image as input to a computer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一张彩色图像也是一个函数，但在这种情况下，每个*(x,y)*位置的值不是一个单一的数字。相反，它是一个向量，包含三种不同的光强度，对应于三个颜色通道。以下是查看图像细节作为计算机输入的代码。
- en: Code for visualizing an image
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于可视化图像的代码
- en: 'Let''s take a look at how an image can be visualized with the following code:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下如何使用以下代码来可视化一张图像：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We obtain the following image as a result:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果图像：
- en: '![](img/7becc16a-b4b4-4e07-8f41-30ae0768b2aa.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7becc16a-b4b4-4e07-8f41-30ae0768b2aa.png)'
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following result is obtained:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 得到以下结果：
- en: '![](img/896620ef-6e56-4fb5-9e79-9d5aa17f9c09.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/896620ef-6e56-4fb5-9e79-9d5aa17f9c09.png)'
- en: 'In the previous chapter, we used an MLP-based approach to recognize images.
    There are two issues with that approach:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们使用基于MLP的方法来识别图像。该方法存在两个问题：
- en: It increases the number of parameters
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它增加了参数的数量
- en: It only accepts vectors as input, that is, flattening a matrix to a vector
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它只接受向量作为输入，也就是说，将矩阵展平为向量
- en: This means we must find a new way to process images, in which 2D information
    is not completely lost. CNNs address this issue. Furthermore, CNNs accept matrices
    as input. Convolutional layers preserve spatial structures. First, we define a
    convolution window, also called a **filter**, or **kernel**; then slide this over
    the image.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们必须找到一种新的处理图像的方法，在这种方法中，二维信息不会完全丢失。CNN解决了这个问题。此外，CNN接受矩阵作为输入。卷积层保留空间结构。首先，我们定义一个卷积窗口，也叫做**滤波器**，或**卷积核**；然后将其在图像上滑动。
- en: Dropout
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: A neural network can be thought of as a search problem. Each node in the neural
    network is searching for correlation between the input data and the correct output
    data.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以被看作是一个搜索问题。神经网络中的每个节点都在搜索输入数据与正确输出数据之间的相关性。
- en: Dropout randomly turns nodes off while forward-propagating and thus helps ward
    off weights from converging to identical positions. After this is done, it turns
    on all the nodes and back-propagates. Similarly, we can set some of the layer's
    values to zero at random during forward propagation in order to perform dropout
    on a layer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout在前向传播时随机关闭节点，从而帮助防止权重收敛到相同的位置。完成此操作后，它会打开所有节点并进行反向传播。同样，我们也可以在前向传播时将某些层的值随机设置为零，以便对该层执行dropout。
- en: Use dropout only during training. Do not use it at runtime or on your testing
    dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 仅在训练过程中使用dropout。在运行时或测试数据集上不要使用它。
- en: Input layer
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入层
- en: 'The **input layer** holds the image data. In the following figure, the input
    layer consists of three inputs. In a **fully connected layer**, the neurons between
    two adjacent layers are fully connected pairwise but do not share any connection
    within a layer. In other words, the neurons in this layer have full connections
    to all activations in the previous layer. Therefore, their activations can be
    computed with a simple matrix multiplication, optionally adding a bias term. The
    difference between a fully connected and convolutional layer is that neurons in
    a convolutional layer are connected to a local region in the input, and that they
    also share parameters:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入层**保存图像数据。在下图中，输入层由三个输入组成。在**全连接层**中，两个相邻层之间的神经元是完全连接的，但在同一层内的神经元之间没有连接。换句话说，这一层的神经元与上一层的所有激活都有全连接。因此，它们的激活可以通过简单的矩阵乘法计算，可能还需要加上偏置项。全连接层与卷积层的区别在于，卷积层中的神经元仅连接到输入的局部区域，并且它们还共享参数：'
- en: '![](img/d65ee007-a008-4252-88b6-f4ac9d2d13d2.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d65ee007-a008-4252-88b6-f4ac9d2d13d2.png)'
- en: Convolutional layer
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: The main objective of convolution in relation to ConvNet is to extract features
    from the input image. This layer does most of the computation in a ConvNet. We
    will not go into the mathematical details of convolution here but will get an
    understanding of how it works over images.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积在与ConvNet相关的目标是从输入图像中提取特征。这一层在ConvNet中进行大部分计算。我们在这里不会深入讲解卷积的数学细节，而是了解它在图像上的工作原理。
- en: The ReLU activation function is extremely useful in CNNs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU激活函数在CNN中非常有用。
- en: Convolutional layers in Keras
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras中的卷积层
- en: 'To create a convolutional layer in Keras, you must first import the required
    modules as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中创建卷积层之前，你必须首先导入所需的模块，如下所示：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, you can create a convolutional layer by using the following format:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以使用以下格式创建卷积层：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You must pass the following arguments:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须传入以下参数：
- en: '`filters`: The number of filters.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filters`：过滤器的数量。'
- en: '`kernel_size`: A number specifying both the height and width of the (square)
    convolution window. There are also some additional optional arguments that you
    might like to tune.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel_size`：指定卷积窗口（方形）的高度和宽度的数字。你还可以调整一些额外的可选参数。'
- en: '`strides`: The stride of the convolution. If you don''t specify anything, this
    is set to one.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strides`：卷积的步幅。如果你没有指定，默认为1。'
- en: '`padding`: This is either `valid` or `same`. If you don''t specify anything,
    the padding is set to `valid`.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`：这可以是`valid`或`same`。如果没有指定，填充默认为`valid`。'
- en: '`activation`: This is typically `relu`. If you don''t specify anything, no
    activation is applied. You are strongly encouraged to add a ReLU activation function
    to every convolutional layer in your networks.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`activation`：通常是`relu`。如果没有指定，则不会应用激活函数。强烈建议你在每个卷积层中添加ReLU激活函数。'
- en: It is possible to represent both `kernel_size` and `strides` as either a number
    or a tuple.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`kernel_size`和`strides`都可以表示为数字或元组。'
- en: When using your convolutional layer as the first layer (appearing after the
    input layer) in a model, you must provide an additional `input_shape` argument—`input_shape`.
    It is a tuple specifying the height, width, and depth (in that order) of the input.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当将卷积层用作模型中的第一层（位于输入层之后）时，你必须提供一个额外的`input_shape`参数——`input_shape`。它是一个元组，指定输入的高度、宽度和深度（按此顺序）。
- en: Please make sure that the  `input_shape` argument is not included if the convolutional
    layer is not the first layer in your network.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果卷积层不是你网络中的第一层，请确保不包含`input_shape`参数。
- en: 'There are many other tunable arguments that you can set to change the behavior
    of your convolutional layers:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以设置许多其他可调参数，以改变卷积层的行为：
- en: '**Example 1**: In order to build a CNN with an input layer that accepts images
    of 200 x 200 pixels in grayscale. In such cases, the next layer would be a convolutional
    layer of 16 filters with width and height as 2\. As we go ahead with the convolution
    we can set the filter to jump 2 pixels together. Therefore, we can build a convolutional,
    layer with a filter that doesn''t pad the images with zeroes with the following
    code:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例 1**：为了构建一个接受200 x 200像素灰度图像的输入层的CNN。在这种情况下，下一层将是一个具有16个过滤器的卷积层，宽度和高度为2。随着卷积的进行，我们可以设置过滤器每次跳跃2个像素。因此，我们可以使用以下代码构建一个不填充零的卷积层：'
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Example 2**: After we build our CNN model, we can have the next layer in
    it to be a convolutional layer. This layer will have 32 filters with width and
    height as 3, which would take the layer that was constructed in the previous example
    as its input. Here, as we proceed with the convolution, we will set the filter
    to jump one pixel at a time, such that the convolutional layer will be able to
    see all the regions of the previous layer too. Such a convolutional layer can
    be constructed with the help of the following code:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例 2**：在构建完 CNN 模型后，我们可以在模型中加入下一层，通常是卷积层。这个层会有 32 个滤波器，宽度和高度均为 3，它会将前面示例中构建的层作为输入。在进行卷积操作时，我们设置滤波器每次跳跃一个像素，以便卷积层能够查看前一层的所有区域。通过以下代码，我们可以构建这样一个卷积层：'
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Example 3**: You can also construct convolutional layers in Keras of size
    2 x 2, with 64 filters and a ReLU activation function. Here, the convolution utilizes
    a stride of 1 with padding set to `valid` and all other arguments set to their
    default values. Such a convolutional layer can be built using the following code:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**示例 3**：你还可以在 Keras 中构建大小为 2 x 2 的卷积层，使用 64 个滤波器和 ReLU 激活函数。在这里，卷积操作采用步幅为
    1，填充方式为`valid`，其他参数均使用默认值。可以使用以下代码来构建这样一个卷积层：'
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Pooling layer
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: 'As we have seen, a convolutional layer is a stack of feature maps, with one
    feature map for each filter. More filters increase the dimensionality of convolution.
    Higher dimensionality indicates more parameters. So, the pooling layer controls
    overfitting by progressively reducing the spatial size of the representation to
    reduce the number of parameters and computation. The pooling layer often takes
    the convolutional layer as input. The most commonly used pooling approach is **max
    pooling**. In addition to max pooling, pooling units can also perform other functions
    such as **average pooling**. In a CNN, we can control the behavior of the convolutional
    layer by specifying the size of each filter and the number of filters. To increase
    the number of nodes in a convolutional layer, we can increase the number of filters,
    and to increase the size of the pattern, we can increase the size of the filter.
    There are also a few other hyperparameters that can be tuned. One of them is the
    stride of the convolution. Stride is the amount by which the filter slides over
    the image. A stride of 1 moves the filter by 1 pixel horizontally and vertically.
    Here, the convolution becomes the same as the width and depth of the input image.
    A stride of 2 makes a convolutional layer of half of the width and height of the
    image. If the filter extends outside of the image, then we can either ignore these
    unknown values or replace them with zeros. This is known as **padding**. In Keras,
    we can set `padding = ''valid''` if it is acceptable to lose a few values. Otherwise,
    set `padding = ''same''`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，卷积层是由一堆特征图组成的，每个滤波器对应一个特征图。更多的滤波器会增加卷积的维度，而维度越高表示参数越多。因此，池化层通过逐渐减小表示的空间大小来控制过拟合，从而减少参数和计算量。池化层通常以卷积层作为输入。最常用的池化方法是**最大池化**。除了最大池化，池化单元还可以执行其他功能，比如**平均池化**。在
    CNN 中，我们可以通过指定每个滤波器的大小和滤波器的数量来控制卷积层的行为。为了增加卷积层中的节点数，我们可以增加滤波器的数量；为了增大模式的大小，我们可以增大滤波器的尺寸。此外，还有一些其他的超参数可以调整，其中之一是卷积的步幅。步幅是滤波器在图像上滑动的步长。步幅为
    1 时，滤波器会水平和垂直移动 1 个像素。在这种情况下，卷积的输出尺寸将与输入图像的宽度和深度相同。步幅为 2 时，卷积层的输出宽度和高度将为输入图像的一半。如果滤波器超出了图像边界，我们可以选择忽略这些未知值，或者用零来填充它们，这被称为**填充**。在
    Keras 中，如果可以接受丢失少量数据，我们可以设置 `padding = 'valid'`；否则，设置 `padding = 'same'`：
- en: '![](img/ffc13c90-ca46-446d-a5c9-e1c0b2712ba8.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ffc13c90-ca46-446d-a5c9-e1c0b2712ba8.png)'
- en: 'A very simple ConvNet looks like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常简单的卷积网络如下所示：
- en: '![](img/f67c1764-58d0-4d31-b204-34f22089b652.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f67c1764-58d0-4d31-b204-34f22089b652.png)'
- en: Practical example – image classification
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实际示例 – 图像分类
- en: 'The convolutional layer helps to detect regional patterns in an image. The
    max pooling layer, present after the convolutional layer, helps reduce dimensionality.
    Here is an example of image classification using all the principles we studied
    in the previous sections. One important notion is to first make all the images
    into a standard size before doing anything else. The first convolution layer requires
    an additional `input.shape()` parameter. In this section, we will train a CNN
    to classify images from the CIFAR-10 database. CIFAR-10 is a dataset of 60,000
    color images of 32 x 32 size. These images are labeled into 10 categories with
    6,000 images each. These categories are airplane, automobile, bird, cat, dog,
    deer, frog, horse, ship, and truck. Let''s see how to do this with the following
    code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层有助于检测图像中的区域模式。卷积层之后的最大池化层有助于减少维度。这里是一个使用我们在前面几节中学习的所有原则进行图像分类的例子。一个重要的概念是，在做其他操作之前，首先要将所有图像调整为标准大小。第一个卷积层需要一个额外的`input.shape()`参数。在这一节中，我们将训练一个CNN来分类来自CIFAR-10数据库的图像。CIFAR-10是一个包含60,000张32
    x 32大小的彩色图像的数据集。这些图像被标注为10个类别，每个类别有6,000张图像。这些类别是飞机、汽车、鸟、猫、狗、鹿、青蛙、马、船和卡车。让我们看看如何通过以下代码来实现：
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Image augmentation
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像增强
- en: 'While training a CNN model, we do not want the model to change any prediction
    based on the size, angle, and position of the image. The image is represented
    as a matrix of pixel values, so the size, angle, and position have a huge effect
    on the pixel values. To make the model more size-invariant, we can add different
    sizes of the image to the training set. Similarly, in order to make the model
    more rotation-invariant, we can add images with different angles. This process
    is known as **image data augmentation**. This also helps to avoid overfitting.
    Overfitting happens when a model is exposed to very few samples. Image data augmentation
    is one way to reduce overfitting, but it may not be enough because augmented images
    are still correlated. Keras provides an image augmentation class called `ImageDataGenerator`
    that defines the configuration for image data augmentation. This also provides
    other features such as:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练CNN模型时，我们不希望模型根据图像的大小、角度和位置改变任何预测。图像被表示为像素值的矩阵，因此，大小、角度和位置对像素值有很大的影响。为了使模型对尺寸不变性更强，我们可以将不同尺寸的图像添加到训练集中。同样，为了使模型对旋转不变性更强，我们可以添加具有不同角度的图像。这个过程被称为**图像数据增强**。这还有助于避免过拟合。过拟合发生在模型仅接触到非常少的样本时。图像数据增强是减少过拟合的一种方法，但可能还不够，因为增强后的图像之间仍然存在相关性。Keras提供了一个图像增强类，叫做`ImageDataGenerator`，它定义了图像数据增强的配置。它还提供了其他功能，例如：
- en: Sample-wise and feature-wise standardization
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本标准化和特征标准化
- en: Random rotation, shifts, shear, and zoom of the image
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机旋转、平移、剪切和缩放图像
- en: Horizontal and vertical flip
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平和垂直翻转
- en: ZCA whitening
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZCA白化
- en: Dimension reordering
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度重排序
- en: Saving the changes to disk
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将更改保存到磁盘
- en: 'An augmented image generator object can be created as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 可以按照以下方式创建一个增强型图像生成器对象：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This API generates batches of tensor image data in real-time data augmentation,
    instead of processing an entire image dataset in memory. This API is designed
    to create augmented image data during the model fitting process. Thus, it reduces
    the memory overhead but adds some time cost for model training.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 该API通过实时数据增强生成批量张量图像数据，而不是在内存中处理整个图像数据集。这个API旨在在模型拟合过程中创建增强的图像数据。因此，它减少了内存开销，但为模型训练增加了一些时间成本。
- en: 'After it is created and configured, you must fit your data. This computes any
    statistics required to perform the transformations to image data. This is done
    by calling the `fit()` function on the data generator and passing it to the training
    dataset, as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 创建并配置完毕后，您必须对数据进行拟合。这将计算进行图像数据转换所需的任何统计信息。通过在数据生成器上调用`fit()`函数并将其传递给训练数据集，可以完成此操作，如下所示：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The batch size can be configured, the data generator can be prepared, and batches
    of images can be received by calling the `flow()` function:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 可以配置批量大小，准备数据生成器，并通过调用`flow()`函数接收批量图像：
- en: '[PRE11]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, call the `fit_generator()` function instead of calling the `fit()` function
    on the model:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，调用`fit_generator()`函数，而不是在模型上调用`fit()`函数：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let's look at some examples to understand how the image augmentation API in
    Keras works. We will use the MNIST handwritten digit recognition task in these
    examples.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些示例来理解Keras中的图像增强API是如何工作的。在这些示例中，我们将使用MNIST手写数字识别任务。
- en: 'Let''s begin by taking a look at the first nine images in the training dataset:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先来看一下训练数据集中前九张图像：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The following code snippet creates augmented images from the CIFAR-10 dataset.
    We will add these images to the training set of the last example and see how the
    classification accuracy increases:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段从CIFAR-10数据集中创建了增强图像。我们将这些图像添加到上一例中的训练集，并观察分类准确率如何提高：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Summary
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We began this chapter by briefly looking into the history of CNNs. We introduced
    you to the implementation of visualizing images.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过简要回顾卷积神经网络（CNN）的历史开始了这一章。我们向你介绍了可视化图像的实现方法。
- en: We studied image classification with the help of a practical example, using
    all the principles we learned about in the chapter. Finally, we learned how image
    augmentation helps us avoid overfitting and studied the various other features
    provided by image augmentation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个实际的例子学习了图像分类，运用了本章中学到的所有原理。最后，我们了解了图像增强如何帮助我们避免过拟合，并研究了图像增强提供的其他各种功能。
- en: In the next chapter, we will learn how to build a simple image classifier CNN
    model from scratch.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何从零开始构建一个简单的图像分类CNN模型。
