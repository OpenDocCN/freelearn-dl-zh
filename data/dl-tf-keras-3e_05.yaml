- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Recurrent Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: 'In *Chapter 3*, we learned about **Convolutional Neural Networks** (**CNNs**)
    and saw how they exploit the spatial geometry of their inputs. For example, CNNs
    for images apply convolutions to initially small patches of the image, and progress
    to larger and larger areas of the image using pooling operations. Convolutions
    and pooling operations for images are in two dimensions: the width and the height.
    For audio and text streams, one-dimensional convolution and pooling operations
    are applied along the time dimension, and for video streams, these operations
    are applied in three dimensions: along the height, width, and time dimensions.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 3 章*中，我们学习了**卷积神经网络**（**CNNs**），并了解了它们如何利用输入的空间几何特性。例如，CNNs 在图像处理中，最初对图像的小块进行卷积操作，然后通过池化操作逐步扩大到图像的更大区域。图像的卷积和池化操作是二维的：宽度和高度。而对于音频和文本流，则沿时间维度应用一维卷积和池化操作；对于视频流，这些操作是在三个维度上进行的：高度、宽度和时间维度。
- en: In this chapter, we will focus on **Recurrent Neural Networks** (**RNNs**),
    a class of neural networks that is popularly used on text inputs. RNNs are very
    flexible and have been used to solve problems such as speech recognition, language
    modeling, machine translation, sentiment analysis, and image captioning, to name
    a few. RNNs exploit the sequential nature of their input. Sequential inputs could
    be text, speech, time series, and anything else where the occurrence of an element
    in a sequence is dependent on the elements that came before it. In this chapter,
    we will see examples of various RNNs and learn how to implement them with TensorFlow.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍**循环神经网络**（**RNNs**），这是一类广泛应用于文本输入的神经网络。RNN 非常灵活，已被用于解决语音识别、语言建模、机器翻译、情感分析、图像描述等问题。RNN
    充分利用了输入的顺序特性。顺序输入可以是文本、语音、时间序列，或者任何其他元素的出现依赖于前一个元素的序列。在本章中，我们将看到各种 RNN 的示例，并学习如何使用
    TensorFlow 实现它们。
- en: We will first look at the internals of a basic RNN cell and how it deals with
    these sequential dependencies in the input. We will also learn about some limitations
    of the basic RNN cell (implemented as SimpleRNN in Keras) and see how two popular
    variants of the SimpleRNN cell – the **Long Short-Term Memory** (**LSTM**) and
    the **Gated Recurrent Unit** (**GRU**) – overcome this limitation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先了解基本的 RNN 单元及其如何处理输入中的这些顺序依赖关系。我们还将学习基本 RNN 单元（在 Keras 中实现为 SimpleRNN）的一些局限性，并查看两个流行的
    SimpleRNN 变体——**长短期记忆**（**LSTM**）和**门控循环单元**（**GRU**）——是如何克服这些局限性的。
- en: We will then zoom out one level and consider the RNN layer itself, which is
    just the RNN cell applied to every time step. An RNN can be thought of as a graph
    of RNN cells, where each cell performs the same operation on successive elements
    of the sequence. We will describe some simple modifications to improve performance,
    such as making the RNN bidirectional and/or stateful.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将放大一个层次，考虑 RNN 层本身，它就是将 RNN 单元应用于每个时间步。RNN 可以被视为一个由 RNN 单元组成的图，每个单元在序列的连续元素上执行相同的操作。我们将描述一些简单的修改以提高性能，例如使
    RNN 双向和/或有状态。
- en: Finally, we look at some standard RNN topologies and the kind of applications
    they can be used to solve. RNNs can be adapted to different types of applications
    by rearranging the cells in the graph. We will see some examples of these configurations
    and how they are used to solve specific problems. We will also consider the sequence-to-sequence
    (or seq2seq) architecture, which has been used with great success in machine translation
    and various other fields. We will then look at what an attention mechanism is,
    and how it can be used to improve the performance of sequence-to-sequence architectures.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来看一些标准的 RNN 拓扑结构及其可以解决的应用问题。通过重新排列图中的单元，RNN 可以适应不同类型的应用。我们将看到这些配置的一些示例，并了解它们如何用来解决特定问题。我们还将讨论序列到序列（或
    seq2seq）架构，该架构在机器翻译和其他多个领域取得了巨大的成功。然后，我们将探讨什么是注意力机制，以及如何利用它提高序列到序列架构的性能。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下主题：
- en: The basic RNN cell
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本 RNN 单元
- en: RNN cell variants
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 单元变体
- en: RNN variants
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 变体
- en: RNN topologies
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 拓扑结构
- en: Encoder-decoder architectures – seq2seq
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器-解码器架构 – seq2seq
- en: Attention mechanism
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力机制
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp5](https://packt.link/dltfchp5).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在[https://packt.link/dltfchp5](https://packt.link/dltfchp5)找到。
- en: It is often said that a journey of a thousand miles starts with a single step,
    so in that spirit, let’s begin our study of RNNs by first considering the RNN
    cell.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 常说千里之行始于足下，因此，为了体现这一精神，让我们从考虑 RNN 单元开始，来启动我们对 RNN 的学习。
- en: The basic RNN cell
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本的 RNN 单元
- en: Traditional multilayer perceptron neural networks make the assumption that all
    inputs are independent of each other. This assumption is not true for many types
    of sequence data. For example, words in a sentence, musical notes in a composition,
    stock prices over time, or even molecules in a compound are examples of sequences
    where an element will display a dependence on previous elements.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的多层感知器神经网络假设所有输入之间是相互独立的。这一假设对于许多类型的序列数据来说并不成立。例如，句子中的单词、音乐作品中的音符、股票价格随时间变化，甚至化合物中的分子，都是典型的序列数据，其中一个元素会依赖于前面的元素。
- en: 'RNN cells incorporate this dependence by having a hidden state, or memory,
    that holds the essence of what has been seen so far. The value of the hidden state
    at any point in time is a function of the value of the hidden state at the previous
    time step, and the value of the input at the current time step, that is:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 单元通过具有隐藏状态或记忆来实现这一依赖关系，隐藏状态保存了到目前为止所看到的本质信息。任意时刻隐藏状态的值是前一时刻隐藏状态的值和当前时刻输入值的函数，即：
- en: '![](img/B18331_05_001.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_001.png)'
- en: Here, *h*[t] and *h*[t][-1] are the values of the hidden states at the time
    *t* and *t-1* respectively, and *x*[t] is the value of the input at time *t*.
    Notice that the equation is recursive, that is, *h*[t][-1] can be represented
    in terms of *h*[t][-2] and *x*[t-1], and so on, until the beginning of the sequence.
    This is how RNNs encode and incorporate information from arbitrarily long sequences.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*h*[t] 和 *h*[t][-1] 分别是时刻 *t* 和 *t-1* 的隐藏状态值，*x*[t] 是时刻 *t* 的输入值。注意，这个方程是递归的，即
    *h*[t][-1] 可以用 *h*[t][-2] 和 *x*[t-1] 表示，依此类推，直到序列的开始。这就是 RNN 如何编码和融合来自任意长序列的信息。
- en: We can also represent the RNN cell graphically, as shown in *Figure 5.1(a)*.
    At time *t*, the cell has an input *x(t)* and output *y(t)*. Part of the output
    *y(t)* (represented by the hidden state *h*[t]) is fed back into the cell for
    use at a later time step *t+1*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以像 *图 5.1(a)* 中那样图示化地表示 RNN 单元。在时刻 *t*，该单元有一个输入 *x(t)* 和输出 *y(t)*。部分输出 *y(t)*（由隐藏状态
    *h*[t] 表示）会被反馈到单元中，以便在后续的时间步 *t+1* 使用。
- en: 'Just as in a traditional neural network, where the learned parameters are stored
    as weight matrices, the RNN’s parameters are defined by the three weight matrices
    *U*, *V*, and *W*, corresponding to the weights of the input, output, and hidden
    states respectively:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 就像传统的神经网络中，学习到的参数存储为权重矩阵一样，RNN 的参数由三个权重矩阵 *U*、*V* 和 *W* 定义，分别对应输入、输出和隐藏状态的权重：
- en: '![Diagram, schematic  Description automatically generated](img/B18331_05_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图表，示意图 描述自动生成](img/B18331_05_01.png)'
- en: 'Figure 5.1: (a) Schematic of an RNN cell; (b) the RNN cell unrolled'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1： (a) RNN 单元的示意图；(b) 展开视图中的 RNN 单元
- en: '*Figure 5.1(b)* shows the same RNN in an “unrolled view.” Unrolling just means
    that we draw the network out for the complete sequence. The network shown here
    has three time steps, suitable for processing three element sequences. Note that
    the weight matrices *U*, *V*, and *W*, that we spoke about earlier, are shared
    between each of the time steps. This is because we are applying the same operation
    to different inputs at each time step. Being able to share these weights across
    all the time steps greatly reduces the number of parameters that the RNN needs
    to learn.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.1(b)* 显示了一个“展开视图”的相同 RNN。展开只是意味着我们将网络绘制出来，覆盖整个序列。这里显示的网络有三个时间步，适用于处理包含三个元素的序列。请注意，我们之前提到的权重矩阵
    *U*、*V* 和 *W* 在每个时间步之间是共享的。这是因为我们在每个时间步对不同的输入应用相同的操作。能够在所有时间步之间共享这些权重大大减少了 RNN
    需要学习的参数数量。'
- en: We can also describe the RNN as a computation graph in terms of equations. The
    internal state of the RNN at a time *t* is given by the value of the hidden vector
    *h(t)*, which is the sum of the weight matrix *W* and the hidden state *h*[t][-1]
    at time *t-1*, and the product of the weight matrix *U* and the input *x*[t] at
    time *t*, passed through a `tanh` activation function. The choice of `tanh` over
    other activation functions such as sigmoid has to do with it being more efficient
    for learning in practice and helps combat the vanishing gradient problem, which
    we will learn about later in the chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过方程来描述 RNN 作为计算图。RNN 在时间 *t* 时的内部状态由隐藏向量 *h(t)* 的值表示，*h(t)* 是权重矩阵 *W*
    和时间 *t-1* 时刻的隐藏状态 *h*[t][-1] 的和，以及时间 *t* 时刻的输入 *x*[t] 与权重矩阵 *U* 的乘积，再经过 `tanh`
    激活函数。选择 `tanh` 而非其他激活函数（如 sigmoid）是因为它在实际学习中更为高效，并且有助于解决梯度消失问题，后者我们将在本章后面学习到。
- en: 'For notational convenience, in all our equations describing different types
    of RNN architectures in this chapter, we have omitted explicit reference to the
    bias terms by incorporating them within the matrix. Consider the following equation
    of a line in an n-dimensional space. Here, *w*[1] through *w*[n] refer to the
    coefficients of the line in each of the *n* dimensions, and the bias *b* refers
    to the y-intercept along each of these dimensions:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便起见，在本章中描述不同类型的 RNN 架构的所有方程中，我们省略了对偏置项的明确引用，而是将其并入矩阵中。考虑以下一个 n 维空间中的直线方程。这里，*w*[1]
    到 *w*[n] 是每个维度中直线的系数，偏置 *b* 是每个维度上的 y 截距：
- en: '![](img/B18331_05_002.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_002.png)'
- en: 'We can rewrite the equation in matrix notation as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将方程改写为矩阵表示如下：
- en: '![](img/B18331_05_003.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_003.png)'
- en: 'Here, *W* is a matrix of shape (*m, n*) and *b* is a vector of shape (*m, 1*),
    where *m* is the number of rows corresponding to the records in our dataset, and
    *n* is the number of columns corresponding to the features for each record. Equivalently,
    we can eliminate the vector *b* by folding it into our matrix *W* by treating
    the *b* vector as a feature column corresponding to the “unit” feature of *W*.
    Thus:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W* 是一个形状为 (*m, n*) 的矩阵，*b* 是一个形状为 (*m, 1*) 的向量，其中 *m* 是对应于我们数据集中记录的行数，*n*
    是每条记录对应的特征列数。等效地，我们可以通过将 *b* 向量作为 *W* 的“单位”特征列，将其折叠到 *W* 矩阵中，从而去掉向量 *b*。因此：
- en: '![](img/B18331_05_004.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_004.png)'
- en: Here, *W’* is a matrix of shape (*m, n+1*), where the last column contains the
    values of *b*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*W’* 是一个形状为 (*m, n+1*) 的矩阵，最后一列包含偏置 *b* 的值。
- en: The resulting notation ends up being more compact and (we believe) easier to
    comprehend and retain as well.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，这种符号表示方式更为紧凑，并且（我们认为）更容易理解和记忆。
- en: 'The output vector *y*[t] at time *t* is the product of the weight matrix *V*
    and the hidden state *h*[t], passed through a softmax activation, such that the
    resulting vector is a set of output probabilities:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间*t*时刻，输出向量 *y*[t] 是权重矩阵 *V* 和隐藏状态 *h*[t] 的乘积，经过softmax激活函数处理后，得到的向量是一个输出概率集合：
- en: '![](img/B18331_05_005.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_005.png)'
- en: '![](img/B18331_05_006.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_006.png)'
- en: Keras provides the SimpleRNN recurrent layer that incorporates all the logic
    we have seen so far, as well as the more advanced variants such as LSTM and GRU,
    which we will learn about later in this chapter. Strictly speaking, it is not
    necessary to understand how they work to start building with them.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了 SimpleRNN 循环层，它包含了我们到目前为止看到的所有逻辑，以及更高级的变种，如 LSTM 和 GRU，我们将在本章后面学习到。严格来说，理解它们的工作原理并不是构建它们的必要条件。
- en: However, an understanding of the structure and equations is helpful when you
    need to build your own specialized RNN cell to overcome a specific problem.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你需要构建自己的专用 RNN 单元来解决特定问题时，理解结构和方程是非常有帮助的。
- en: Now that we understand the flow of data forward through the RNN cell, that is,
    how it combines its input and hidden states to produce the output and the next
    hidden state, let us now examine the flow of gradients in the reverse direction.
    This is a process called **Backpropagation Through Time** (**BPTT**).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了数据在 RNN 单元中的前向流动，即它如何将输入和隐藏状态结合起来，生成输出和下一个隐藏状态，我们现在来分析梯度在反向传播中的流动。这一过程称为**时间反向传播**（**BPTT**）。
- en: Backpropagation through time (BPTT)
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间反向传播（BPTT）
- en: Just like traditional neural networks, training RNNs also involves the backpropagation
    of gradients. The difference, in this case, is that since the weights are shared
    by all time steps, the gradient at each output depends not only on the current
    time step but also on the previous ones. This process is called backpropagation
    through time [11]. Because the weights *U*, *V*, and *W*, are shared across the
    different time steps in the case of RNNs, we need to sum up the gradients across
    the various time steps in the case of BPTT. This is the key difference between
    traditional backpropagation and BPTT.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的神经网络类似，训练RNN同样涉及梯度的反向传播。不同之处在于，由于权重在所有时间步之间共享，每个输出的梯度不仅依赖于当前时间步，还依赖于之前的时间步。这一过程称为通过时间的反向传播（BPTT）[11]。因为在RNN中，权重*U*、*V*和*W*在不同时间步之间是共享的，所以我们需要将不同时间步的梯度进行累加。这是传统反向传播与BPTT之间的关键区别。
- en: 'Consider the RNN with five time steps shown in *Figure 5.2*. During the forward
    pass, the network produces predictions *ŷ*[t] at time *t* that are compared with
    the label *y*[t] to compute a loss *L*[t]. During backpropagation (shown by the
    dotted lines), the gradients of the loss with respect to the weights *U*, *V*,
    and *W*, are computed at each time step and the parameters updated with the sum
    of the gradients:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图5.2所示的具有五个时间步的RNN。在前向传播过程中，网络在时间*t*生成预测值*ŷ*[t]，并与标签*y*[t]进行比较，以计算损失*L*[t]。在反向传播过程中（由虚线表示），损失对权重*U*、*V*和*W*的梯度在每个时间步计算出来，并通过梯度的累加更新参数：
- en: '![Diagram  Description automatically generated](img/B18331_05_02.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图示  描述自动生成](img/B18331_05_02.png)'
- en: 'Figure 5.2: Backpropagation through time'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：通过时间反向传播
- en: The following equation shows the gradient of the loss with respect to *W*. We
    focus on this weight because it is the cause of the phenomenon known as the vanishing
    and exploding gradient problem.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 下式展示了损失对*W*的梯度。我们关注这个权重，因为它是导致所谓的消失梯度和爆炸梯度问题的原因。
- en: 'This problem manifests as the gradients of the loss approaching either zero
    or infinity, making the network hard to train. To understand why this happens,
    consider the equation of the SimpleRNN we saw earlier; the hidden state *h*[t]
    is dependent on *h*[t][-1], which in turn is dependent on *h*[t][-2], and so on:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题表现为损失的梯度接近零或无穷大，从而使网络难以训练。为了理解为什么会发生这种情况，考虑我们之前看到的简单RNN的方程；隐藏状态*h*[t]依赖于*h*[t][-1]，而*h*[t][-1]又依赖于*h*[t][-2]，依此类推：
- en: '![](img/B18331_05_007.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_007.png)'
- en: 'Let’s now see what happens to this gradient at time step *t=3*. By the chain
    rule, the gradient of the loss with respect to *W* can be decomposed to a product
    of three sub-gradients. The gradient of the hidden state *h*[2] with respect to
    *W* can be further decomposed as the sum of the gradient of each hidden state
    with respect to the previous one. Finally, each gradient of the hidden state with
    respect to the previous one can be further decomposed as the product of gradients
    of the current hidden state against the previous hidden state:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下在时间步*t=3*时，梯度会发生什么变化。根据链式法则，损失对*W*的梯度可以分解为三个子梯度的乘积。隐藏状态*h*[2]相对于*W*的梯度可以进一步分解为每个隐藏状态相对于前一个状态的梯度之和。最终，每个隐藏状态相对于前一个状态的梯度可以进一步分解为当前隐藏状态与前一个隐藏状态梯度的乘积：
- en: '![](img/B18331_05_008.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_008.png)'
- en: Similar calculations are done to compute the gradient of the other losses *L*[0]
    through *L*[4] with respect to *W*, and sum them up into the gradient update for
    *W*. We will not explore the math further in this book, but this WildML blog post
    [12] has a very good explanation of BPTT, including a more detailed derivation
    of the math behind the process.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的计算也用于计算其他损失 *L*[0] 到 *L*[4] 对 *W* 的梯度，并将它们加总成 *W* 的梯度更新。我们在本书中不会进一步探讨这些数学推导，但这篇WildML博客文章[12]对BPTT提供了非常好的解释，包括该过程背后的数学推导。
- en: Vanishing and exploding gradients
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消失梯度和爆炸梯度
- en: The reason BPTT is particularly sensitive to the problem of vanishing and exploding
    gradients comes from the product part of the expression representing the final
    formulation of the gradient of the loss with respect to *W*. Consider the case
    where the individual gradients of a hidden state with respect to the previous
    one are less than 1.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: BPTT特别容易受到消失梯度和爆炸梯度问题的影响，原因在于表达式中代表损失对*W*的梯度最终公式的乘积部分。考虑一下，隐藏状态相对于前一状态的个别梯度小于1的情况。
- en: As we backpropagate across multiple time steps, the product of gradients becomes
    smaller and smaller, ultimately leading to the problem of vanishing gradients.
    Similarly, if the gradients are larger than 1, the products get larger and larger,
    and ultimately lead to the problem of exploding gradients.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在多个时间步上进行反向传播时，梯度的乘积会变得越来越小，最终导致梯度消失问题。类似地，如果梯度大于 1，乘积会变得越来越大，最终导致梯度爆炸问题。
- en: Of the two, exploding gradients are more easily detectable. The gradients will
    become very large and turn into **Not a Number** (**NaN**), and the training process
    will crash. Exploding gradients can be controlled by clipping them at a predefined
    threshold [13]. TensorFlow 2.0 allows you to clip gradients using the `clipvalue`
    or `clipnorm` parameter during optimizer construction, or by explicitly clipping
    gradients using `tf.clip_by_value`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况中，梯度爆炸更容易被检测到。梯度会变得非常大，最终变为 **非数字** (**NaN**)，导致训练过程崩溃。梯度爆炸可以通过将其裁剪到预定义的阈值来控制
    [13]。TensorFlow 2.0 允许在优化器构建期间通过 `clipvalue` 或 `clipnorm` 参数裁剪梯度，或者通过 `tf.clip_by_value`
    显式裁剪梯度。
- en: The effect of vanishing gradients is that gradients from time steps that are
    far away do not contribute anything to the learning process, so the RNN ends up
    not learning any long-range dependencies. While there are a few approaches toward
    minimizing the problem, such as proper initialization of the *W* matrix, more
    aggressive regularization, using ReLU instead of `tanh` activation, and pretraining
    the layers using unsupervised methods, the most popular solution is to use LSTM
    or GRU architectures, both of which will be explained shortly. These architectures
    have been designed to deal with vanishing gradients and learn long-term dependencies
    more effectively.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失的影响是，来自远距离时间步的梯度对学习过程没有任何贡献，因此 RNN 最终无法学习任何长期依赖关系。虽然有一些方法可以最小化这个问题，例如适当初始化
    *W* 矩阵、更加激进的正则化、使用 ReLU 替代 `tanh` 激活函数、以及使用无监督方法对层进行预训练，但最流行的解决方案是使用 LSTM 或 GRU
    架构，这两者将在后续讲解。这些架构被设计用来处理梯度消失问题，并更有效地学习长期依赖关系。
- en: RNN cell variants
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 单元变种
- en: 'In this section, we’ll look at some cell variants of RNNs. We’ll begin by looking
    at a variant of the SimpleRNN cell: the LSTM RNN.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将探讨一些 RNN 的单元变种。我们将首先看看 SimpleRNN 单元的一个变种：LSTM RNN。
- en: Long short-term memory (LSTM)
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）
- en: The LSTM is a variant of the SimpleRNN cell that is capable of learning long-term
    dependencies. LSTMs were first proposed by Hochreiter and SchmidHuber [14] and
    refined by many other researchers. They work well on a large variety of problems
    and are the most widely used RNN variant.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 是一种 SimpleRNN 单元的变种，能够学习长期依赖关系。LSTM 最早由 Hochreiter 和 SchmidHuber [14] 提出，并经过许多其他研究者的改进。它们在多种问题上表现良好，是最广泛使用的
    RNN 变种。
- en: We have seen how the SimpleRNN combines the hidden state from the previous time
    step and the current input through a `tanh` layer to implement recurrence. LSTMs
    also implement recurrence in a similar way, but instead of a single `tanh` layer,
    there are four layers interacting in a very specific way. *Figure 5.3* illustrates
    the transformations that are applied in the hidden state at time step *t*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，SimpleRNN 如何通过一个 `tanh` 层将前一时间步的隐藏状态与当前输入结合来实现递归。LSTM 也以类似的方式实现递归，但它们并非使用单一的
    `tanh` 层，而是有四个层以非常特定的方式相互作用。*图 5.3* 展示了在时间步 *t* 时，隐藏状态所应用的变换。
- en: The diagram looks complicated, but let’s look at it component by component.
    The line across the top of the diagram is the cell state *c*, representing the
    internal memory of the unit.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图示看起来比较复杂，但让我们一个一个地来看。图表顶部的线是单元的细胞状态 *c*，表示单元的内部记忆。
- en: 'The line across the bottom is the hidden state *h*, and the *i*, *f*, *o*,
    and *g* gates are the mechanisms by which the LSTM works around the vanishing
    gradient problem. During training, the LSTM learns the parameters of these gates:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图表底部的线是隐藏状态 *h*，而 *i*、*f*、*o* 和 *g* 门是 LSTM 解决梯度消失问题的机制。在训练过程中，LSTM 会学习这些门的参数：
- en: '![](img/B18331_05_03.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_03.png)'
- en: 'Figure 5.3: An LSTM cell'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：LSTM 单元
- en: An alternative way to think about how these gates work inside an LSTM cell is
    to consider the equations of the cell. These equations describe how the value
    of the hidden state *h*[t] at time *t* is calculated from the value of hidden
    state *h*[t-1] at the previous time step. In general, the equation-based description
    tends to be clearer and more concise and is usually the way a new cell design
    is presented in academic papers. Diagrams, when provided, may or may not be comparable
    to the ones you saw earlier. For these reasons, it usually makes sense to learn
    to read the equations and visualize the cell design. To that end, we will describe
    the other cell variants in this book using equations only.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种理解LSTM单元中这些门如何工作的方式是，考虑该单元的方程式。这些方程描述了如何从先前时间步长的隐藏状态 *h*[t-1] 计算出时间 *t* 时的隐藏状态
    *h*[t]。一般来说，基于方程的描述通常更清晰、更简洁，也是学术论文中展示新单元设计时常用的方式。提供的图示可能与之前看到的有所不同，因此，学习如何阅读方程式并想象单元设计通常更为合理。为此，本书中将仅通过方程式描述其他单元变体。
- en: 'The set of equations representing an LSTM is shown as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 代表LSTM的一组方程式如下所示：
- en: '![](img/B18331_05_009.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_009.png)'
- en: Here, *i*, *f*, and *o* are the input, forget, and output gates. They are computed
    using the same equations but with different parameter matrices *W*[i], *U*[i],
    *W*[f], *U*[f], and *W*[o], *U*[o]. The sigmoid function modulates the output
    of these gates between 0 and 1, so the output vectors produced can be multiplied
    element-wise with another vector to define how much of the second vector can pass
    through the first one.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*i*、*f* 和 *o* 是输入门、遗忘门和输出门。它们是通过相同的方程式计算的，只是参数矩阵不同，分别为 *W*[i]、*U*[i]、*W*[f]、*U*[f]
    和 *W*[o]、*U*[o]。Sigmoid 函数将这些门的输出值调节到0和1之间，因此，产生的输出向量可以逐元素与另一个向量相乘，以定义第二个向量可以通过第一个向量的程度。
- en: The forget gate defines how much of the previous state *h*[t][-1] you want to
    allow to pass through. The input gate defines how much of the newly computed state
    for the current input *x*[t] you want to let through, and the output gate defines
    how much of the internal state you want to expose to the next layer. The internal
    hidden state *g* is computed based on the current input *x*[t] and the previous
    hidden state *h*[t][-1]. Notice that the equation for *g* is identical to that
    of the SimpleRNN, except that in this case, we will modulate the output by the
    output of input vector *i*.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 遗忘门定义了你希望允许多少先前状态 *h*[t][-1] 通过。输入门定义了你希望多少当前输入 *x*[t] 新计算的状态可以通过，而输出门则定义了你希望多少内部状态暴露给下一层。内部隐藏状态
    *g* 是基于当前输入 *x*[t] 和先前隐藏状态 *h*[t][-1] 计算出来的。注意，*g* 的方程式与 SimpleRNN 中的相同，只是这次我们将通过输入向量
    *i* 的输出来调节输出。
- en: Given *i*, *f*, *o*, and *g*, we can now calculate the cell state *c*[t] at
    time *t* as the cell state *c*[t][-1] at time (*t-1*) multiplied by the value
    of the forget gate *g*, plus the state *g* multiplied by the input gate *i*. This
    is basically a way to combine the previous memory and the new input – setting
    the forget gate to 0 ignores the old memory and setting the input gate to 0 ignores
    the newly computed state. Finally, the hidden state *h*[t] at time *t* is computed
    as the memory *c*[t] at time *t*, with the output gate *o*.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 *i*、*f*、*o* 和 *g*，我们现在可以计算在时间 *t* 时的细胞状态 *c*[t]，它等于在时间 (*t-1*) 时的细胞状态 *c*[t][-1]，乘以遗忘门
    *g* 的值，再加上状态 *g* 乘以输入门 *i* 的值。这基本上是一种结合旧记忆和新输入的方法——将遗忘门设置为0会忽略旧记忆，而将输入门设置为0则会忽略新计算的状态。最后，时间
    *t* 时的隐藏状态 *h*[t] 是通过时间 *t* 的记忆 *c*[t] 和输出门 *o* 计算得出的。
- en: One thing to realize is that the LSTM is a drop-in replacement for a SimpleRNN
    cell; the only difference is that LSTMs are resistant to the vanishing gradient
    problem. You can replace an RNN cell in a network with an LSTM without worrying
    about any side effects. You should generally see better results along with longer
    training times.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 需要了解的一点是，LSTM可以作为SimpleRNN单元的直接替代品；唯一的区别是LSTM能够防止梯度消失问题。你可以在网络中用LSTM替换RNN单元，而无需担心任何副作用。通常你会看到更好的结果，尽管训练时间会更长。
- en: TensorFlow 2.0 also provides a ConvLSTM2D implementation based on the paper
    by Shi, et al. [18], where the matrix multiplications are replaced by convolution
    operators.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2.0 还提供了基于 Shi 等人 [18] 论文的 ConvLSTM2D 实现，其中矩阵乘法被卷积运算符取代。
- en: If you would like to learn more about LSTMs, please take a look at the WildML
    RNN tutorial [15] and Christopher Olah’s blog post [16]. The first covers LSTMs
    in somewhat greater detail, and the second takes you step by step through the
    computations in a very visual way.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于 LSTM 的内容，请查看 WildML 的 RNN 教程 [15] 和 Christopher Olah 的博客文章 [16]。第一篇教程详细介绍了
    LSTM，第二篇教程则以非常直观的方式一步一步带您了解计算过程。
- en: Now that we have covered LTSMs, we will cover the other popular RNN cell architecture
    – GRUs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了 LSTM，我们将介绍另一种流行的 RNN 单元架构——GRU。
- en: Gated recurrent unit (GRU)
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 门控循环单元（GRU）
- en: The GRU is a variant of the LSTM and was introduced by Cho, et al [17]. It retains
    the LSTM’s resistance to the vanishing gradient problem, but its internal structure
    is simpler, and is, therefore, faster to train, since fewer computations are needed
    to make updates to its hidden state.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 是 LSTM 的变体，由 Cho 等人 [17] 提出。它保留了 LSTM 对消失梯度问题的抗性，但其内部结构更简单，因此训练速度更快，因为更新隐藏状态所需的计算量较少。
- en: Instead of the input (*i*), forgot (*f*), and output (*o*) gates in the LSTM
    cell, the GRU cell has two gates, an update gate *z* and a reset gate *r*. The
    update gate defines how much previous memory to keep around, and the reset gate
    defines how to combine the new input with the previous memory. There is no persistent
    cell state distinct from the hidden state as it is in LSTM.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LSTM 单元中的输入 (*i*)、遗忘 (*f*) 和输出 (*o*) 门不同，GRU 单元有两个门，一个是更新门 *z*，另一个是重置门 *r*。更新门定义了保留多少先前的记忆，重置门定义了如何将新输入与先前的记忆结合。与
    LSTM 中的隐藏状态不同，GRU 中没有持久的单元状态。
- en: 'The GRU cell defines the computation of the hidden state *h*[t] at time *t*
    from the hidden state *h*[t][-1] at the previous time step using the following
    set of equations:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: GRU 单元定义了通过以下一组方程，从前一时间步的隐藏状态 *h*[t][-1] 计算当前时间 *t* 的隐藏状态 *h*[t]：
- en: '![](img/B18331_05_010.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_010.png)'
- en: The outputs of the update gate *z* and the reset gate *r* are both computed
    using a combination of the previous hidden state *h*[t][-1] and the current input
    *x*[t]. The sigmoid function modulates the output of these functions between 0
    and 1\. The cell state *c* is computed as a function of the output of the reset
    gate *r* and input *x*[t]. Finally, the hidden state *h*[t] at time *t* is computed
    as a function of the cell state *c* and the previous hidden state *h*[t][-1].
    The parameters *W*[z], *U*[z], *W*[r], *U*[r], and *W*[c], *U*[c], are learned
    during training.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 更新门 *z* 和重置门 *r* 的输出都是通过前一隐藏状态 *h*[t][-1] 和当前输入 *x*[t] 的组合来计算的。Sigmoid 函数调节这些函数的输出值在
    0 到 1 之间。单元状态 *c* 是作为重置门 *r* 和输入 *x*[t] 输出的函数来计算的。最后，时间 *t* 的隐藏状态 *h*[t] 是作为单元状态
    *c* 和前一隐藏状态 *h*[t][-1] 的函数来计算的。参数 *W*[z]、*U*[z]、*W*[r]、*U*[r]、*W*[c]、*U*[c] 会在训练过程中进行学习。
- en: Similar to LSTM, TensorFlow 2.0 (`tf.keras`) provides an implementation for
    the basic GRU layer as well, which is a drop-in replacement for the RNN cell.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 LSTM，TensorFlow 2.0（`tf.keras`）也提供了基本 GRU 层的实现，它是 RNN 单元的替代品。
- en: Peephole LSTM
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Peephole LSTM
- en: The peephole LSTM is an LSTM variant that was first proposed by Gers and Schmidhuber
    [19]. It adds “peepholes” to the input, forget, and output gates, so they can
    see the previous cell state *c*[t][-1]. The equations for computing the hidden
    state *h*[t], at time *t*, from the hidden state *h*[t][-1] at the previous time
    step, in a peephole LSTM are shown next.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Peephole LSTM 是一种 LSTM 变体，最早由 Gers 和 Schmidhuber 提出 [19]。它向输入、遗忘和输出门添加了“窥视孔”，因此它们可以看到之前的单元状态
    *c*[t][-1]。计算在 peephole LSTM 中，从前一时间步的隐藏状态 *h*[t][-1] 到当前时间 *t* 的隐藏状态 *h*[t] 的方程式如下所示。
- en: 'Notice that the only difference from the equations for the LSTM is the additional
    *c*[t][-1] term for computing outputs of the input (*i*), forget (*f*), and output
    (*o*) gates:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与 LSTM 方程式的唯一区别是额外的 *c*[t][-1] 项，用于计算输入 (*i*)、遗忘 (*f*) 和输出 (*o*) 门的输出：
- en: '![](img/B18331_05_009.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_009.png)'
- en: 'TensorFlow 2.0 provides an experimental implementation of the peephole LSTM
    cell. To use this in your own RNN layers, you will need to wrap the cell (or list
    of cells) in the RNN wrapper, as shown in the following code snippet:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2.0 提供了一个实验性的 peephole LSTM 单元实现。要在自己的 RNN 层中使用此功能，您需要将该单元（或单元列表）包装在
    RNN 包装器中，如下所示的代码片段所示：
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the previous section, we saw some RNN cell variants that were developed to
    target specific inadequacies of the basic RNN cell. In the next section, we will
    look at variations in the architecture of the RNN network itself, which were built
    to address specific use cases.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们介绍了一些为了针对基本 RNN 单元的特定不足而开发的 RNN 单元变体。在下一节中，我们将探讨 RNN 网络架构本身的变体，这些变体是为了应对特定的使用场景而构建的。
- en: RNN variants
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 变体
- en: In this section, we will look at a couple of variations of the basic RNN architecture
    that can provide performance improvements in some specific circumstances. Note
    that these strategies can be applied to different kinds of RNN cells, as well
    as for different RNN topologies, which we will learn about later.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将介绍一些基本 RNN 架构的变体，这些变体在某些特定情况下可以提供性能改进。请注意，这些策略可以应用于不同种类的 RNN 单元，以及不同的
    RNN 拓扑结构，我们将在后续学习中了解这些拓扑结构。
- en: Bidirectional RNNs
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 双向 RNN
- en: We have seen how, at any given time step *t*, the output of the RNN is dependent
    on the outputs at all previous time steps. However, it is entirely possible that
    the output is also dependent on the future outputs as well. This is especially
    true for applications such as natural language processing where the attributes
    of the word or phrase we are trying to predict may be dependent on the context
    given by the entire enclosing sentence, not just the words that came before it.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，在任何给定的时间步 *t*，RNN 的输出依赖于所有之前时间步的输出。然而，完全有可能输出也依赖于未来的输出。这对于自然语言处理等应用尤为重要，在这些应用中，我们试图预测的单词或短语的属性可能依赖于整个句子所给出的上下文，而不仅仅是前面的单词。
- en: This problem can be solved using a bidirectional LSTM (see *Figure 5.4*), also
    called biLSTM, which is essentially two RNNs stacked on top of each other, one
    reading the input from left to right, and the other reading the input from the
    right to the left.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过使用双向 LSTM 来解决（见 *图 5.4*），它也被称为 biLSTM，实际上是两个 RNN 堆叠在一起，一个从左到右读取输入，另一个从右到左读取输入。
- en: 'The output at each time step will be based on the hidden state of both RNNs.
    Bidirectional RNNs allow the network to place equal emphasis on the beginning
    and end of the sequence, and typically result in performance improvements:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 每个时间步的输出将基于两个 RNN 的隐藏状态。双向 RNN 允许网络对序列的开始和结束部分给予同等的关注，通常会导致性能的提升：
- en: '![Diagram  Description automatically generated](img/B18331_05_04.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![Diagram  Description automatically generated](img/B18331_05_04.png)'
- en: 'Figure 5.4: Bidirectional LSTM'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：双向 LSTM
- en: 'TensorFlow 2.0 provides support for bidirectional RNNs through a bidirectional
    wrapper layer. To make an RNN layer bidirectional, all that is needed is to wrap
    the layer with this wrapper layer, which is shown as follows. Since the output
    of each pair of cells in the left and right LSTM in the biLSTM pair are concatenated
    (see *Figure 5.4*), it needs to return output from each cell. Hence, we set `return_sequences`
    to `True` (the default is `False` meaning that the output is only returned from
    the last cell in the LSTM):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 2.0 通过一个双向包装层支持双向 RNN。为了使 RNN 层变成双向，只需要用这个包装层将其包裹起来，具体如下所示。由于 biLSTM
    中左右 LSTM 的每一对单元的输出是连接在一起的（见 *图 5.4*），因此需要返回每个单元的输出。因此，我们将 `return_sequences` 设置为
    `True`（默认值是 `False`，意味着只返回 LSTM 中最后一个单元的输出）：
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The next major RNN variation we will look at is the Stateful RNN.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的下一个主要 RNN 变体是有状态的 RNN。
- en: Stateful RNNs
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有状态的 RNN
- en: RNNs can be stateful, which means that they can maintain state across batches
    during training. That is, the hidden state computed for a batch of training data
    will be used as the initial hidden state for the next batch of training data.
    However, this needs to be explicitly set, since TensorFlow 2.0 (`tf.keras`) RNNs
    are stateless by default, and resets the state after each batch. Setting an RNN
    to be stateful means that it can build state across its training sequence and
    even maintain that state when doing predictions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 可以是有状态的，这意味着它们可以在训练期间跨批次保持状态。也就是说，为一批训练数据计算的隐藏状态将作为下一批训练数据的初始隐藏状态。然而，这需要显式设置，因为
    TensorFlow 2.0（`tf.keras`）中的 RNN 默认是无状态的，并且在每个批次之后会重置状态。将 RNN 设置为有状态意味着它可以在训练序列中构建状态，甚至在进行预测时保持该状态。
- en: The benefits of using stateful RNNs are smaller network sizes and/or lower training
    times. The disadvantage is that we are now responsible for training the network
    with a batch size that reflects the periodicity of the data and resetting the
    state after each epoch. In addition, data should not be shuffled while training
    the network since the order in which the data is presented is relevant for stateful
    networks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用有状态RNN的好处是网络规模更小和/或训练时间更短。缺点是我们现在需要使用一个反映数据周期性的批量大小来训练网络，并在每个训练周期后重置状态。此外，在训练网络时，数据不应被打乱，因为数据的呈现顺序对有状态网络来说是相关的。
- en: To set an RNN layer as stateful, set the named variable stateful to `True`.
    In our example of a one-to-many topology for learning how to generate text, we
    provide an example of using a stateful RNN. Here, we train using data consisting
    of contiguous text slices, so setting the LSTM to stateful means that the hidden
    state generated from the previous text chunk is reused for the current text chunk.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要将RNN层设置为有状态，请将命名变量stateful设置为`True`。在我们关于学习如何生成文本的单对多拓扑示例中，我们提供了一个使用有状态RNN的示例。在这里，我们使用由连续文本切片组成的数据进行训练，因此将LSTM设置为有状态意味着从前一个文本片段生成的隐藏状态会被重用于当前的文本片段。
- en: In the next section on RNN topologies, we will look at different ways to set
    up the RNN network for different use cases.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节关于RNN拓扑结构中，我们将探讨如何为不同的使用案例设置RNN网络。
- en: RNN topologies
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN拓扑结构
- en: We have seen examples of how MLP and CNN architectures can be composed to form
    more complex networks. RNNs offer yet another degree of freedom, in that they
    allow sequence input and output. This means that RNN cells can be arranged in
    different ways to build networks that are adapted to solve different types of
    problems. *Figure 5.5* shows five different configurations of inputs, hidden layers,
    and outputs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过如何将MLP和CNN架构组合成更复杂的网络。RNN提供了另一个自由度，它允许序列的输入和输出。这意味着RNN单元可以以不同的方式排列，构建出适应于解决不同类型问题的网络。*图5.5*展示了输入、隐藏层和输出的五种不同配置。
- en: Of these, the first one (one-to-one) is not interesting from a sequence processing
    point of view, as it can be implemented as a simple dense network with one input
    and one output.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，第一个（单对单）从序列处理的角度来看并不有趣，因为它可以通过一个简单的密集网络实现，只有一个输入和一个输出。
- en: 'The one-to-many case has a single input and outputs a sequence. An example
    of such a network might be a network that can generate text tags from images [6],
    containing short text descriptions of different aspects of the image. Such a network
    would be trained with image input and labeled sequences of text representing the
    image tags:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 单对多的案例有一个输入并输出一个序列。这样一个网络的示例可能是从图像中生成文本标签的网络[6]，这些标签包含图像不同方面的简短文本描述。这样的网络将使用图像输入和表示图像标签的标注文本序列进行训练：
- en: '![Diagram  Description automatically generated](img/B18331_05_05.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图表 描述自动生成](img/B18331_05_05.png)'
- en: 'Figure 5.5: Common RNN topologies'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：常见的RNN拓扑结构
- en: The many-to-one case is the reverse; it takes a sequence of tensors as input
    but outputs a single tensor. Examples of such networks would be a sentiment analysis
    network [7], which takes as input a block of text such as a movie review and outputs
    a single sentiment value.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 多对一的案例则相反；它接受一个序列的张量作为输入，但输出一个单一的张量。这样的网络示例可能是情感分析网络[7]，它以一段文本（如电影评论）为输入，并输出一个单一的情感值。
- en: The many-to-many use case comes in two flavors. The first one is more popular
    and is better known as the seq2seq model. In this model, a sequence is read in
    and produces a context vector representing the input sequence, which is used to
    generate the output sequence.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 多对多的使用案例有两种变体。第一种更为流行，更广为人知的是seq2seq模型。在这个模型中，一个序列被读取并生成一个上下文向量，代表输入序列，用于生成输出序列。
- en: The topology has been used with great success in the field of machine translation,
    as well as problems that can be reframed as machine translation problems. Real-life
    examples of the former can be found in [8, 9], and an example of the latter is
    described in [10].
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 该拓扑结构在机器翻译领域取得了巨大成功，也适用于可以重新框架为机器翻译问题的问题。前者的现实生活示例可以在[8, 9]中找到，后者的示例描述在[10]中。
- en: The second many-to-many type has an output cell corresponding to each input
    cell. This kind of network is suited for use cases where there is a 1:1 correspondence
    between the input and output, such as time series. The major difference between
    this model and the seq2seq model is that the input does not have to be completely
    encoded before the decoding process begins.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种多对多类型的网络中，每个输入单元都有一个对应的输出单元。这种网络适用于输入和输出之间有1:1对应关系的用例，例如时间序列。这个模型与seq2seq模型的主要区别在于，在解码过程开始之前，输入不必完全编码。
- en: In the next three sections, we provide examples of a one-to-many network that
    learns to generate text, a many-to-one network that does sentiment analysis, and
    a many-to-many network of the second type, which predicts **Part-of-Speech** (**POS**)
    for words in a sentence. Because of the popularity of the seq2seq network, we
    will cover it in more detail later in this chapter.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三个部分中，我们提供一个学习生成文本的一对多网络示例，一个进行情感分析的多对一网络示例，以及第二类型的多对多网络示例，该网络预测句子中单词的词性。由于seq2seq网络的普及，我们稍后会在本章节中更详细地介绍它。
- en: Example ‒ One-to-many – Learning to generate text
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 ‒ 一对多 – 学习生成文本
- en: RNNs have been used extensively by the **Natural Language Processing** (**NLP**)
    community for various applications. One such application is to build language
    models. A language model is a model that allows us to predict the probability
    of a word in a text given previous words. Language models are important for various
    higher-level tasks such as machine translation, spelling correction, and so on.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: RNN在自然语言处理（NLP）社区被广泛用于各种应用中。其中一种应用是构建语言模型。语言模型允许我们预测给定先前单词的文本中下一个单词的概率。语言模型对于各种高级任务如机器翻译、拼写校正等非常重要。
- en: The ability of a language model to predict the next word in a sequence makes
    it a generative model that allows us to generate text by sampling from the output
    probabilities of different words in the vocabulary. The training data is a sequence
    of words, and the label is the word appearing at the next time step in the sequence.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型预测序列中下一个单词的能力使其成为一个生成模型，通过从词汇表中不同单词的输出概率中进行采样，我们可以生成文本。训练数据是单词序列，标签是在序列的下一个时间步出现的单词。
- en: For our example, we will train a character-based RNN on the text of the children’s
    stories *Alice in Wonderland* and its sequel *Through the Looking Glass* by Lewis
    Carroll. We have chosen to build a character-based model because it has a smaller
    vocabulary and trains quicker. The idea is the same as training and using a word-based
    language model, except we will use characters instead of words. Once trained,
    the model can be used to generate some text in the same style.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，我们将在Lewis Carroll的儿童故事《爱丽丝梦游仙境》及其续集《爱丽丝镜中奇遇记》的文本上训练一个基于字符的RNN。我们选择建立一个基于字符的模型，因为它具有较小的词汇量并且训练速度更快。其思想与训练和使用基于单词的语言模型相同，只是我们将使用字符而不是单词。训练完成后，该模型可以用于以相同风格生成一些文本。
- en: The data for our example will come from the plain texts of two novels on the
    Project Gutenberg website [36]. Input to the network are sequences of 100 characters,
    and the corresponding output is another sequence of 100 characters, offset from
    the input by 1 position.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们示例中的数据将来自Project Gutenberg网站[36]上两部小说的纯文本。网络的输入是100个字符的序列，对应的输出是输入序列后移一个位置的另一个100个字符的序列。
- en: That is, if the input is the sequence [*c*[1], *c*[2], …, *c*[n]], the output
    will be [*c*[2], *c*[3], …, *c*[n+1]]. We will train the network for 50 epochs,
    and at the end of every 10 epochs, we will generate a fixed-size sequence of characters
    starting with a standard prefix. In the following example, we have used the prefix
    “Alice”, the name of the protagonist in our novels.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果输入是序列[*c*[1], *c*[2], …, *c*[n]]，输出将是[*c*[2], *c*[3], …, *c*[n+1]]。我们将训练网络50个epochs，在每个10个epochs的末尾，我们将生成一个以标准前缀开始的固定大小字符序列。在以下示例中，我们使用了前缀“爱丽丝”，这是我们小说中主人公的名字。
- en: 'As always, we will first import the necessary libraries and set up some constants.
    Here, `DATA_DIR` points to a data folder under the location where you downloaded
    the source code for this chapter. `CHECKPOINT_DIR` is the location, a folder of
    checkpoints under the data folder, where we will save the weights of the model
    at the end of every 10 epochs:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们将首先导入必要的库并设置一些常量。这里，`DATA_DIR`指向您下载本章源代码所在位置下的数据文件夹。`CHECKPOINT_DIR`是该数据文件夹下的一个存储每10个epochs结束时模型权重的检查点文件夹：
- en: '[PRE2]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we download and prepare the data for our network to consume. The texts
    of both books are publicly available from the Project Gutenberg website. The `tf.keras.utils.get_file()`
    function will check to see whether the file has already been downloaded to your
    local drive, and if not, it will download to a `datasets` folder under the location
    of the code. We also preprocess the input a little here, removing newline and
    byte order mark characters from the text. This step will create the `texts` variable,
    a flat list of characters for these two books:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将下载并准备网络所需的数据。这两本书的文本可以从 Project Gutenberg 网站上公开获取。`tf.keras.utils.get_file()`
    函数会检查文件是否已经下载到本地磁盘，如果没有，它将下载到代码所在位置的 `datasets` 文件夹下。我们还会稍微预处理一下输入，去除文本中的换行符和字节顺序标记字符。此步骤将创建
    `texts` 变量，它是这两本书的一个字符扁平化列表：
- en: '[PRE3]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next, we will create our vocabulary. In our case, our vocabulary contains 90
    unique characters, composed of uppercase and lowercase alphabets, numbers, and
    special characters. We also create some mapping dictionaries to convert each vocabulary
    character into a unique integer and vice versa. As noted earlier, the input and
    output of the network is a sequence of characters.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建我们的词汇表。在我们的案例中，词汇表包含 90 个独特的字符，由大小写字母、数字和特殊字符组成。我们还创建了一些映射字典，将每个词汇字符转换为唯一整数，并且反之亦然。正如前面所提到的，网络的输入和输出是字符序列。
- en: 'However, the actual input and output of the network are sequences of integers,
    and we will use these mapping dictionaries to handle this conversion:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，网络的实际输入和输出是整数序列，我们将使用这些映射字典来处理这种转换：
- en: '[PRE4]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next step is to use these mapping dictionaries to convert our character
    sequence input into an integer sequence and then into a TensorFlow dataset. Each
    of our sequences is going to be 100 characters long, with the output being offset
    from the input by 1 character position. We first batch the dataset into slices
    of 101 characters, then apply the `split_train_labels()` function to every element
    of the dataset to create our sequences dataset, which is a dataset of tuples of
    two elements, with each element of the tuple being a vector of size 100 and type
    `tf.int64`. We then shuffle these sequences and create batches of 64 tuples for
    each input to our network. Each element of the dataset is now a tuple consisting
    of a pair of matrices, each of size (64, 100) and type `tf.int64`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用这些映射字典将我们的字符序列输入转换为整数序列，然后转换为 TensorFlow 数据集。每个序列将包含 100 个字符，输出将比输入偏移
    1 个字符位置。我们首先将数据集批量化为 101 个字符的切片，然后对数据集的每个元素应用 `split_train_labels()` 函数，以创建我们的序列数据集，该数据集由包含两个元素的元组组成，每个元素是一个大小为
    100、类型为 `tf.int64` 的向量。然后我们对这些序列进行洗牌，并为每个输入到网络的序列创建 64 个元组的批次。现在，数据集的每个元素都是一个由两个矩阵组成的元组，每个矩阵的大小为
    (64, 100)，类型为 `tf.int64`：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We are now ready to define our network. As before, we define our network as
    a subclass of `tf.keras.Model`, as shown next. The network is fairly simple; it
    takes as input a sequence of integers of size 100 (`num_timesteps`) and passes
    them through an embedding layer so that each integer in the sequence is converted
    into a vector of size 256 (`embedding_dim`). So, assuming a batch size of 64,
    for our input sequence of size (64, 100), the output of the embedding layer is
    a matrix of shape (64, 100, 256).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好定义我们的网络。像之前一样，我们将网络定义为 `tf.keras.Model` 的子类，如下所示。网络结构相对简单；它以大小为 100（`num_timesteps`）的整数序列作为输入，并通过一个嵌入层将每个整数转换为大小为
    256（`embedding_dim`）的向量。因此，假设批次大小为 64，对于大小为 (64, 100) 的输入序列，嵌入层的输出将是一个形状为 (64,
    100, 256) 的矩阵。
- en: The next layer is an RNN layer with 100 time steps. The implementation of RNN
    chosen is a GRU. This GRU layer will take, at each of its time steps, a vector
    of size (256,) and output a vector of shape (1024,) (`rnn_output_dim`). Note also
    that the RNN is stateful, which means that the hidden state output from the previous
    training epoch will be used as input to the current epoch. The `return_sequences=True`
    flag also indicates that the RNN will output at each of the time steps rather
    than an aggregate output at the last time steps.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 下一层是一个具有 100 个时间步的 RNN 层。选择的 RNN 实现是 GRU。该 GRU 层将在每个时间步接收一个大小为 (256,) 的向量，并输出一个形状为
    (1024,) 的向量（`rnn_output_dim`）。还需要注意的是，RNN 是有状态的，这意味着从前一训练周期输出的隐藏状态将作为当前周期的输入。`return_sequences=True`
    标志还表示 RNN 会在每个时间步输出，而不是仅在最后一个时间步输出聚合结果。
- en: 'Finally, each of the time steps will emit a vector of shape (1024,) into a
    dense layer that outputs a vector of shape (90,) (`vocab_size`). The output from
    this layer will be a tensor of shape (64, 100, 90). Each position in the output
    vector corresponds to a character in our vocabulary, and the values correspond
    to the probability of that character occurring at that output position:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，每个时间步都会发出一个形状为(1024,)的向量，进入一个密集层，该层输出一个形状为(90,)的向量（`vocab_size`）。该层的输出将是一个形状为(64,
    100, 90)的张量。输出向量中的每个位置对应于我们词汇表中的一个字符，值对应于该字符在该输出位置出现的概率：
- en: '[PRE6]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we define a loss function and compile our model. We will use the sparse
    categorical cross-entropy as our loss function because that is the standard loss
    function to use when our inputs and outputs are sequences of integers. For the
    optimizer, we will choose the Adam optimizer:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个损失函数并编译我们的模型。我们将使用稀疏类别交叉熵作为损失函数，因为当输入和输出是整数序列时，这是标准的损失函数。对于优化器，我们将选择Adam优化器：
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Normally, the character at each position of the output is found by computing
    the argmax of the vector at that position, that is, the character corresponding
    to the maximum probability value. This is known as greedy search. In the case
    of language models where the output of one time step becomes the input to the
    next time step, this can lead to a repetitive output. The two most common approaches
    to overcome this problem are either to sample the output randomly or to use beam
    search, which samples from *k* the most probable values at each time step. Here,
    we will use the `tf.random.categorical()` function to sample the output randomly.
    The following function takes a string as a prefix and uses it to generate a string
    whose length is specified by `num_chars_to_generate`. The temperature parameter
    is used to control the quality of the predictions. Lower values will create a
    more predictable output.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，输出中每个位置的字符是通过计算该位置向量的argmax来找到的，也就是说，找到与最大概率值对应的字符。这被称为贪婪搜索。在语言模型中，其中一个时间步的输出作为下一个时间步的输入，这可能会导致输出重复。克服这个问题的两种常见方法是要么随机抽样输出，要么使用束搜索，在每个时间步从最可能的*k*个值中进行抽样。在这里，我们将使用`tf.random.categorical()`函数来随机抽样输出。以下函数接受一个字符串作为前缀，并利用它生成一个长度由`num_chars_to_generate`指定的字符串。温度参数用于控制预测的质量。较低的值会生成更可预测的输出。
- en: 'The logic follows a predictable pattern. We convert the sequence of characters
    in our `prefix_string` into a sequence of integers, then `expand_dims` to add
    a batch dimension so the input can be passed into our model. We then reset the
    state of the model. This is needed because our model is stateful, and we don’t
    want the hidden state of the first time step in our prediction run to be carried
    over from the one computed during training. We then run the input through our
    model and get back a prediction. This is the vector of shape (90,) representing
    the probabilities of each character in the vocabulary appearing at the next time
    step. We then reshape the prediction by removing the batch dimension and dividing
    by the temperature, and then randomly sampling from the vector. We then set our
    prediction as the input of the next time step. We repeat this for the number of
    characters we need to generate, converting each prediction back into character
    form and accumulating them in a list, and returning the list at the end of the
    loop:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑遵循一个可预测的模式。我们将`prefix_string`中的字符序列转换为整数序列，然后通过`expand_dims`添加一个批次维度，以便将输入传递到我们的模型中。接着我们重置模型的状态。这是必要的，因为我们的模型是有状态的，我们不希望预测过程中的第一个时间步的隐藏状态被训练时计算出的状态所继承。然后，我们将输入传递通过模型并得到预测结果。这是一个形状为(90,)的向量，表示词汇表中每个字符在下一时间步出现的概率。接下来，我们通过去除批次维度并除以温度参数来重塑预测结果，然后从向量中随机抽样。我们将预测结果作为下一时间步的输入。我们重复这一过程，直到生成所需数量的字符，并将每次预测转换回字符形式，积累到一个列表中，最后在循环结束时返回该列表：
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we are ready to run our training and evaluation loop. As mentioned
    earlier, we will train our network for 50 epochs, and at every 10-epoch interval,
    we will try to generate some text with the model trained so far. Our prefix at
    each stage is the string `"Alice "`. Notice that in order to accommodate a single
    string prefix, we save the weights after every 10 epochs and build a separate
    generative model with these weights but with an input shape with a batch size
    of 1\. Here is the code to do this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们准备好运行训练和评估循环。如前所述，我们将训练网络 50 个周期，每隔 10 个周期，我们将尝试用迄今为止训练的模型生成一些文本。每个阶段的前缀是字符串
    `"Alice "`。请注意，为了适应单个字符串前缀，我们会在每隔 10 个周期保存一次权重，并使用这些权重构建一个单独的生成模型，但输入形状的批量大小为
    1。以下是执行此操作的代码：
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output after the very first epoch of training contains words that are completely
    undecipherable:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的第一次周期后的输出包含一些完全无法解读的单词：
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'However, after about 30 epochs of training, we begin to see words that look
    familiar:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在大约训练了 30 个周期后，我们开始看到一些看起来熟悉的单词：
- en: '[PRE11]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After 50 epochs of training, the model still has trouble expressing coherent
    thought but has learned to spell reasonably well. What is amazing here is that
    the model is character-based and has no knowledge of words, yet it learns to spell
    words that look like they might have come from the original text:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 经过 50 个周期的训练，模型仍然很难表达连贯的思想，但已经学会了合理地拼写单词。令人惊讶的是，尽管模型是基于字符的，并且不了解单词，但它学会了拼写看起来可能来源于原始文本的单词：
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Generating the next character or next word in the text isn’t the only thing
    you can do with this sort of model. Similar models have been built to make stock
    price predictions [3] or generate classical music [4]. Andrej Karpathy covers
    a few other fun examples, such as generating fake Wikipedia pages, algebraic geometry
    proofs, and Linux source code in his blog post [5].
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 生成文本中的下一个字符或单词并不是你可以使用这种模型做的唯一事情。类似的模型已被构建用来预测股票价格 [3] 或生成古典音乐 [4]。Andrej Karpathy
    在他的博客文章 [5] 中介绍了一些有趣的例子，比如生成假维基百科页面、代数几何证明和 Linux 源代码。
- en: 'The full code for this example is available in `alice_text_generator.py` in
    the source code folder for this chapter. It can be run from the command line using
    the following command:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子的完整代码可以在本章的源代码文件夹中的 `alice_text_generator.py` 找到。可以通过以下命令在命令行中运行：
- en: '[PRE13]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Our next example will show an implementation of a many-to-one network for sentiment
    analysis.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个例子将展示一个用于情感分析的多对一网络的实现。
- en: Example ‒ Many-to-one – Sentiment analysis
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 ‒ 多对一 – 情感分析
- en: In this example, we will use a many-to-one network that takes a sentence as
    input and predicts its sentiment as being either positive or negative. Our dataset
    is the Sentiment-labeled sentences dataset on the UCI Machine Learning Repository
    [20], a set of 3,000 sentences from reviews on Amazon, IMDb, and Yelp, each labeled
    with 0 if it expresses a negative sentiment, or 1 if it expresses a positive sentiment.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用一个多对一的网络，它以一个句子为输入，并预测其情感是积极的还是消极的。我们的数据集是 UCI 机器学习库中的情感标注句子数据集
    [20]，它包含来自亚马逊、IMDb 和 Yelp 的 3,000 个评论句子，每个句子根据其情感标注为 0（表示消极情感）或 1（表示积极情感）。
- en: 'As usual, we will start with our imports:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们首先进行导入：
- en: '[PRE14]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The dataset is provided as a zip file, which expands into a folder containing
    three files of labeled sentences, one for each provider, with one sentence and
    label per line and with the sentence and label separated by the tab character.
    We first download the zip file, then parse the files into a list of `(sentence,
    label)` pairs:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集以压缩文件的形式提供，解压后是一个包含三个人物标注句子文件的文件夹，每行一个句子和标签，句子与标签之间由制表符分隔。我们首先下载压缩文件，然后将文件解析为
    `(sentence, label)` 对的列表：
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Our objective is to train the model so that, given a sentence as input, it learns
    to predict the corresponding sentiment provided in the label. Each sentence is
    a sequence of words. However, to input it into the model, we have to convert it
    into a sequence of integers.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是训练模型，使其能够根据输入的句子，学习预测标签中提供的相应情感。每个句子是一个单词的序列。然而，为了将其输入到模型中，我们必须将其转换为整数序列。
- en: 'Each integer in the sequence will point to a word. The mapping of integers
    to words for our corpus is called a vocabulary. Thus, we need to tokenize the
    sentences and produce a vocabulary. This is done using the following code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 序列中的每个整数都将指向一个单词。我们语料库中整数到单词的映射称为词汇表。因此，我们需要对句子进行分词并生成一个词汇表。这是通过以下代码完成的：
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Our vocabulary consists of 5,271 unique words. It is possible to make the size
    smaller by dropping words that occur fewer than some threshold number of times,
    which can be found by inspecting the `tokenizer.word_counts` dictionary. In such
    cases, we need to add 1 to the vocabulary size for the UNK (unknown) entry, which
    will be used to replace every word that is not found in the vocabulary.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的词汇表包含了5,271个独特的单词。通过丢弃出现次数少于某个阈值的单词，我们可以将词汇表的大小缩小。这个阈值可以通过检查`tokenizer.word_counts`字典来找到。在这种情况下，我们需要将词汇大小加1，以便为UNK（未知）条目预留空间，该条目将用于替代词汇表中找不到的单词。
- en: We also construct lookup dictionaries to convert from the word-to-word index
    and back. The first dictionary is useful during training to construct integer
    sequences to feed the network. The second dictionary is used to convert from the
    word index back into words in our prediction code later.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还构建了查找字典，用于从单词到单词索引的转换以及反向转换。第一个字典在训练期间非常有用，用于构造整数序列以供网络输入。第二个字典则在预测代码中用于将单词索引转换回单词。
- en: 'Each sentence can have a different number of words. Our model will require
    us to provide sequences of integers of identical length for each sentence. To
    support this requirement, it is common to choose a maximum sequence length that
    is large enough to accommodate most of the sentences in the training set. Any
    sentences that are shorter will be padded with zeros, and any sentences that are
    longer will be truncated. An easy way to choose a good value for the maximum sequence
    length is to look at the sentence length (as in the number of words) at different
    percentile positions:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 每个句子的单词数可能不同。我们的模型要求我们为每个句子提供相同长度的整数序列。为了支持这一要求，通常会选择一个足够大的最大序列长度，以容纳大多数训练集中的句子。任何较短的句子将会被零填充，较长的句子将会被截断。选择最大序列长度的一个简单方法是查看不同百分位位置的句子长度（例如，单词数量）：
- en: '[PRE17]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This gives us the following output:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As can be seen, the maximum sentence length is 71 words, but 99% of the sentences
    are under 36 words. If we choose a value of 64, for example, we should be able
    to get away with not having to truncate most of the sentences.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如可以看到，最大句子长度为71个单词，但99%的句子都在36个单词以内。例如，如果我们选择64作为值，我们应该能够避免大多数句子的截断。
- en: The preceding blocks of code can be run interactively multiple times to choose
    good values of vocabulary size and maximum sequence length respectively. In our
    example, we have chosen to keep all the words (so `vocab_size = 5271`), and we
    have set our `max_seqlen` to `64`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码块可以多次交互运行，以分别选择合适的词汇大小和最大序列长度。在我们的示例中，我们选择保留所有单词（因此`vocab_size = 5271`），并将`max_seqlen`设置为64。
- en: Our next step is to create a dataset that our model can consume. We first use
    our trained tokenizer to convert each sentence from a sequence of words (`sentences`)
    into a sequence of integers (`sentences_as_ints`), where each corresponding integer
    is the index of the word in the `tokenizer.word_index`. It is then truncated and
    padded with zeros.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是创建一个模型可以使用的数据集。我们首先使用训练好的分词器，将每个句子从一系列单词（`sentences`）转换为一系列整数（`sentences_as_ints`），其中每个整数对应的是该单词在`tokenizer.word_index`中的索引。然后，它会被截断并用零进行填充。
- en: 'The labels are also converted into a NumPy array `labels_as_ints`, and finally,
    we combine the tensors `sentences_as_ints` and `labels_as_ints` to form a TensorFlow
    dataset:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 标签也被转换为NumPy数组`labels_as_ints`，最后，我们将张量`sentences_as_ints`和`labels_as_ints`结合，形成一个TensorFlow数据集：
- en: '[PRE19]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We want to set aside 1/3 of the dataset for evaluation. Of the remaining data,
    we will use 10% as an inline validation dataset, which the model will use to gauge
    its own progress during training, and the remaining as the training dataset. Finally,
    we create batches of 64 sentences for each dataset:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将数据集的1/3部分留作评估数据。剩余的数据中，我们将10%作为内联验证数据集，模型将在训练过程中使用它来评估自身进度，其余部分作为训练数据集。最后，我们为每个数据集创建64个句子的批次：
- en: '[PRE20]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we define our model. As you can see, the model is fairly straightforward,
    each input sentence is a sequence of integers of size `max_seqlen` (64). This
    is input into an embedding layer that converts each word into a vector given by
    the size of the vocabulary + 1\. The additional word is to account for the padding
    integer 0 that was introduced during the `pad_sequences()` call above. The vector
    at each of the 64 time steps is then fed into a bidirectional LSTM layer, which
    converts each word into a vector of size (64,). The output of the LSTM at each
    time step is fed into a dense layer, which produces a vector of size (64,) with
    ReLU activation. The output of this dense layer is then fed into another dense
    layer, which outputs a vector of (1,) at each time step, modulated through a sigmoid
    activation.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们的模型。如你所见，该模型相当简单，每个输入句子都是一个大小为`max_seqlen`（64）的整数序列。这些输入会传入一个嵌入层，将每个单词转换为一个向量，向量的大小为词汇表大小+1。额外的一个单词是为了考虑在上面的`pad_sequences()`调用中引入的填充整数0。然后，64个时间步的每个向量都会输入到一个双向LSTM层，该层将每个单词转换为大小为（64,）的向量。LSTM在每个时间步的输出会输入到一个密集层，该层产生一个大小为（64,）的向量，并使用ReLU激活函数。该密集层的输出会输入到另一个密集层，该层在每个时间步输出一个大小为（1,）的向量，并通过sigmoid激活进行调节。
- en: 'The model is compiled with the binary cross-entropy loss function and the Adam
    optimizer, and then trained over 10 epochs:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型使用二元交叉熵损失函数和Adam优化器进行编译，并经过10轮训练：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As you can see from the output, the training set accuracy goes to 99.8% and
    the validation set accuracy goes to about 78.5%. Having a higher accuracy over
    the training set is expected since the model was trained on this dataset. You
    can also look at the following loss plot to see exactly where the model starts
    overfitting on the training set. Notice that the training loss keeps going down,
    but the validation loss comes down initially and then starts going up. It is at
    the point where it starts going up that we know that the model overfits on the
    training set:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中你可以看到，训练集的准确率达到了99.8%，验证集的准确率约为78.5%。训练集的准确率较高是预期的，因为模型是在该数据集上训练的。你还可以查看以下的损失图，准确看到模型开始在训练集上过拟合的位置。注意，训练损失持续下降，但验证损失最初下降后开始上升。当验证损失开始上升时，我们就知道模型在训练集上过拟合了：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '*Figure 5.6* shows TensorBoard plots of accuracy and loss for the training
    and validation datasets:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.6* 显示了训练和验证数据集的准确率和损失的 TensorBoard 图：'
- en: '![Chart, line chart, scatter chart  Description automatically generated](img/B18331_05_06.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图，散点图 描述自动生成](img/B18331_05_06.png)'
- en: 'Figure 5.6: Accuracy and loss plots from TensorBoard for sentiment analysis
    network training'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：来自 TensorBoard 的情感分析网络训练准确率和损失图
- en: 'Our checkpoint callback has saved the best model based on the lowest value
    of validation loss, and we can now reload this for evaluation against our held
    out test set:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的检查点回调基于最低的验证损失保存了最佳模型，现在我们可以重新加载该模型，并用它对我们保留的测试集进行评估：
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The easiest high-level way to evaluate a model against a dataset is to use
    the `model.evaluate()` call:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型与数据集的最简单高层方法是使用`model.evaluate()`调用：
- en: '[PRE24]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This gives us the following output:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '[PRE25]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can also use `model.predict()` to retrieve our predictions and compare them
    individually to the labels and use external tools (from scikit-learn, for example)
    to compute our results:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`model.predict()`来获取预测结果，并将其与标签逐一对比，利用外部工具（例如来自scikit-learn的工具）来计算我们的结果：
- en: '[PRE26]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For the first batch of 64 sentences in our test dataset, we reconstruct the
    sentence and display the label (first column) as well as the prediction from the
    model (second column). Here, we show the top 10 sentences. As you can see, the
    model gets it right for most sentences on this list:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们测试数据集中的第一批64个句子，我们重建句子并显示标签（第一列）以及模型的预测（第二列）。这里我们展示前10个句子。如你所见，模型在这个列表中的大多数句子上都预测正确：
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We also report the results across all sentences in the test dataset. As you
    can see, the test accuracy is the same as that reported by the `evaluate` call.
    We have also generated the confusion matrix, which shows that out of 1,000 test
    examples, our sentiment analysis network predicted correctly 782 times and incorrectly
    218 times:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还报告了所有测试数据集句子的结果。如你所见，测试准确率与`evaluate`调用报告的结果相同。我们还生成了混淆矩阵，显示在1,000个测试示例中，我们的情感分析网络正确预测了782次，错误预测了218次：
- en: '[PRE28]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The full code for this example is available in `lstm_sentiment_analysis.py`
    in the source code folder for this chapter. It can be run from the command line
    using the following command:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的完整代码位于此章节源代码文件夹中的`lstm_sentiment_analysis.py`中。可以通过以下命令从命令行运行：
- en: '[PRE29]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Our next example will describe a many-to-many network trained for POS tagging
    English text.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一个例子将描述一个用于词性标注英文文本的多对多网络。
- en: Example ‒ Many-to-many – POS tagging
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 ‒ 多对多 ‒ 词性标注
- en: In this example, we will use a GRU layer to build a network that does **Part
    of Speech** (**POS**) tagging. A POS is a grammatical category of words that are
    used in the same way across multiple sentences. Examples of POS are nouns, verbs,
    adjectives, and so on. For example, nouns are typically used to identify things,
    verbs are typically used to identify what they do, and adjectives are used to
    describe attributes of these things. POS tagging used to be done manually in the
    past, but this is now mostly a solved problem, initially through statistical models,
    and more recently by using deep learning models in an end-to-end manner, as described
    in Collobert, et al. [21].
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用GRU层构建一个网络，用于进行**词性标注**（POS tagging）。词性是跨多个句子使用的词汇类别。词性的例子包括名词、动词、形容词等。例如，名词通常用于识别事物，动词通常用于识别它们的行为，形容词用于描述这些事物的属性。过去词性标注是手动完成的，但现在主要通过统计模型解决，最近则通过端到端的深度学习模型，如Collobert等人[21]所述，进一步解决了这个问题。
- en: For our training data, we will need sentences tagged with POS tags. The Penn
    Treebank [22] is one such dataset; it is a human-annotated corpus of about 4.5
    million words of American English. However, it is a non-free resource. A 10% sample
    of the Penn Treebank is freely available as part of NLTK [23], which we will use
    to train our network.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的训练数据，我们需要标有词性标签的句子。Penn Treebank [22] 是其中之一，它是约450万个美国英语单词的人工注释语料库。但它是非免费资源。Penn
    Treebank的10%样本作为NLTK [23]的一部分免费提供，我们将使用它来训练我们的网络。
- en: Our model will take a sequence of words in a sentence as input, then will output
    the corresponding POS tag for each word. Thus, for an input sequence consisting
    of the words [The, cat, sat. on, the, mat, .], the output sequence should be the
    POS symbols `[DT, NN, VB, IN, DT, NN, .]`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型将接受句子中的单词序列作为输入，然后将为每个单词输出相应的词性标签。因此，对于由单词[The, cat, sat. on, the, mat,
    .]组成的输入序列，输出序列应为词性符号`[DT, NN, VB, IN, DT, NN, .]`。
- en: 'In order to get the data, you need to install the NLTK library if it is not
    already installed (NLTK is included in the Anaconda distribution), as well as
    the 10% treebank dataset (not installed by default). To install NLTK, follow the
    steps on the NLTK install page [23]. To install the treebank dataset, perform
    the following in the Python REPL:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取数据，如果尚未安装NLTK库（NLTK已包含在Anaconda分发中），则需要安装NLTK库（NLTK）。要安装树库数据集，请在Python REPL中执行以下操作：
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Once this is done, we are ready to build our network. As usual, we will start
    by importing the necessary packages:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些步骤后，我们就可以构建我们的网络了。像往常一样，我们将从导入必要的包开始：
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We will lazily import the NLTK treebank dataset into a pair of parallel flat
    files, one containing the sentences and the other containing a corresponding **POS**
    sequence:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将懒惰地将NLTK treebank数据集导入成一对平行的扁平文件，一个包含句子，另一个包含相应的**词性**序列：
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'There are 3,194 sentences in our dataset. The preceding code writes the sentences
    and corresponding tags into parallel files, that is, line 1 in `treebank-sents.txt`
    contains the first sentence, and line 1 in `treebank-poss.txt` contains the corresponding
    POS tags for each word in the sentence. *Table 5.1* shows two sentences from this
    dataset and their corresponding POS tags:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集中有3,194个句子。前面的代码将句子及其对应的标签写入平行文件，即`treebank-sents.txt`中的第一行包含第一句，`treebank-poss.txt`中的第一行包含句子中每个单词的相应词性标签。*表5.1*显示了这个数据集中的两个句子及其相应的词性标签：
- en: '| **Sentences** | **POS Tags** |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **句子** | **词性标签** |'
- en: '| Pierre Vinken, 61 years old, will join the board as a nonexecutive director
    Nov. 29. | NNP NNP , CD NNS JJ , MD VB DT NN IN DT JJ NN NNP CD. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Pierre Vinken, 61 years old, will join the board as a nonexecutive director
    Nov. 29. | NNP NNP , CD NNS JJ , MD VB DT NN IN DT JJ NN NNP CD. |'
- en: '| Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group. | NNP
    NNP VBZ NN IN NNP NNP , DT NNP VBG NN. |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Mr. Vinken is chairman of Elsevier N.V., the Dutch publishing group. | NNP
    NNP VBZ NN IN NNP NNP , DT NNP VBG NN. |'
- en: 'Table 5.1: Sentences and their corresponding POS tags'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1：句子及其相应的词性标签
- en: We will then use the TensorFlow (`tf.keras`) tokenizer to tokenize the sentences
    and create a list of sentence tokens. We reuse the same infrastructure to tokenize
    the POS, although we could have simply split on spaces. Each input record to the
    network is currently a sequence of text tokens, but they need to be a sequence
    of integers. During the tokenizing process, the Tokenizer also maintains the tokens
    in the vocabulary, from which we can build mappings from the token to the integer
    and back.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用TensorFlow（`tf.keras`）的分词器对句子进行分词，并创建句子标记的列表。我们重复使用相同的基础设施对词性进行分词，尽管我们也可以直接按空格分割。每个输入记录当前是一个文本标记序列，但它们需要是一个整数序列。在分词过程中，分词器还会维护词汇表中的标记，从中我们可以建立从标记到整数的映射，并可以进行反向映射。
- en: 'We have two vocabularies to consider, the vocabulary of word tokens in the
    sentence collection and the vocabulary of POS tags in the part-of-speech collection.
    The following code shows how to tokenize both collections and generate the necessary
    mapping dictionaries:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要考虑两个词汇表，一个是句子集合中的词汇表，另一个是词性集合中的词性标签词汇表。以下代码展示了如何对这两个集合进行分词并生成必要的映射字典：
- en: '[PRE33]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Our sentences are going to be of different lengths, although the number of
    tokens in a sentence and their corresponding POS tag sequence are the same. The
    network expects the input to have the same length, so we have to decide how much
    to make our sentence length. The following (throwaway) code computes various percentiles
    and prints sentence lengths at these percentiles to the console:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的句子将具有不同的长度，尽管句子中的标记数量及其相应的词性标签序列是相同的。网络要求输入具有相同的长度，因此我们需要决定句子的长度是多少。以下（临时）代码计算了不同的百分位数，并将这些百分位数的句子长度打印到控制台：
- en: '[PRE34]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We see that we could probably get away with setting the sentence length to around
    100 and have a few truncated sentences as a result. Sentences shorter than our
    selected length will be padded at the end. Because our dataset is small, we prefer
    to use as much of it as possible, so we end up choosing the maximum length.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，设置句子长度为约100并不会有太大问题，尽管会有一些被截断的句子。长度小于我们选择的长度的句子将在末尾进行填充。由于我们的数据集较小，我们希望尽可能多地使用它，因此最终选择了最大长度。
- en: 'The next step is to create the dataset from our inputs. First, we have to convert
    our sequence of tokens and POS tags in our input and output sequences into sequences
    of integers. Second, we have to pad shorter sequences to the maximum length of
    271\. Notice that we do an additional operation on the POS tag sequences after
    padding, rather than keep it as a sequence of integers; we convert it into a sequence
    of one-hot encodings using the `to_categorical()` function. TensorFlow 2.0 does
    provide loss functions to handle outputs as a sequence of integers, but we want
    to keep our code as simple as possible, so we opt to do the conversion ourselves.
    Finally, we use the `from_tensor_slices()` function to create our dataset, shuffle
    it, and split it up into training, validation, and test sets:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是根据我们的输入创建数据集。首先，我们需要将输入和输出序列中的标记和词性标签序列转换为整数序列。其次，我们需要将较短的序列填充到最大长度271\。请注意，在填充之后，我们对词性标签序列进行额外的操作，而不是保持它为整数序列；我们将其转换为一系列独热编码，使用`to_categorical()`函数。TensorFlow
    2.0确实提供了处理输出为整数序列的损失函数，但我们希望尽可能简化代码，因此选择自行进行转换。最后，我们使用`from_tensor_slices()`函数创建数据集，对其进行打乱，并将其划分为训练集、验证集和测试集：
- en: '[PRE35]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Next, we will define our model and instantiate it. Our model is a sequential
    model consisting of an embedding layer, a dropout layer, a bidirectional GRU layer,
    a dense layer, and a softmax activation layer. The input is a batch of integer
    sequences with shape (`batch_size`, `max_seqlen`). When passed through the embedding
    layer, each integer in the sequence is converted into a vector of size (`embedding_dim`),
    so now the shape of our tensor is (`batch_size`, `max_seqlen`, `embedding_dim`).
    Each of these vectors is passed to corresponding time steps of a bidirectional
    GRU with an output dimension of 256\.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们的模型并实例化它。我们的模型是一个顺序模型，由嵌入层、丢弃层、双向GRU层、全连接层和softmax激活层组成。输入是一个形状为（`batch_size`，`max_seqlen`）的整数序列批次。当通过嵌入层时，序列中的每个整数都会被转换为大小为（`embedding_dim`）的向量，因此现在我们的张量形状是（`batch_size`，`max_seqlen`，`embedding_dim`）。这些向量会传递到双向GRU的相应时间步中，GRU的输出维度为256\。
- en: Because the GRU is bidirectional, this is equivalent to stacking one GRU on
    top of the other, so the tensor that comes out of the bidirectional GRU has the
    dimension (`batch_size`, `max_seqlen`, `2*rnn_output_dimension`). Each time step
    tensor of shape (`batch_size`, `1`, `2*rnn_output_dimension`) is fed into a dense
    layer, which converts each time step into a vector of the same size as the target
    vocabulary, that is, (`batch_size`, `number_of_timesteps`, `output_vocab_size`).
    Each time step represents a probability distribution of output tokens, so the
    final softmax layer is applied to each time step to return a sequence of output
    POS tokens.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 因为GRU是双向的，这相当于将一个GRU堆叠在另一个GRU之上，因此从双向GRU中输出的张量具有维度（`batch_size`，`max_seqlen`，`2*rnn_output_dimension`）。每个时间步长的张量形状为（`batch_size`，`1`，`2*rnn_output_dimension`），它会被送入一个全连接层，该层将每个时间步长转换为一个与目标词汇表大小相同的向量，即（`batch_size`，`number_of_timesteps`，`output_vocab_size`）。每个时间步长表示一个输出标记的概率分布，因此最终会对每个时间步应用softmax层，返回一个输出POS标记序列。
- en: 'Finally, we declare the model with some parameters, then compile it with the
    Adam optimizer, the categorical cross-entropy loss function, and accuracy as the
    metric:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们声明模型并设置一些参数，然后使用Adam优化器、分类交叉熵损失函数和准确度作为度量来编译它：
- en: '[PRE36]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Perhaps the best approach is to replace the current loss function with one
    that ignores any matches where both numbers are zero; however, a simpler approach
    is to build a stricter metric and use that to judge when to stop the training.
    Accordingly, we build a new accuracy function `masked_accuracy()` whose code is
    shown as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 或许最好的方法是用一个忽略所有数字为零的匹配项的损失函数来替代当前的损失函数；然而，一个更简单的方法是构建一个更严格的度量，并使用它来判断何时停止训练。因此，我们构建了一个新的准确度函数`masked_accuracy()`，其代码如下所示：
- en: '[PRE37]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We are now ready to train our model. As usual, we set up the model checkpoint
    and TensorBoard callbacks, and then call the `fit()` convenience method on the
    model to train the model with a batch size of 128 for 50 epochs:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备训练我们的模型。像往常一样，我们设置了模型检查点和TensorBoard回调，然后调用模型的`fit()`便捷方法，以批量大小128训练模型50个epoch：
- en: '[PRE38]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'A truncated output of the training is shown as follows. As you can see, the
    `masked_accuracy` and `val_masked_accuracy` numbers seem more conservative than
    the `accuracy` and `val_accuracy` numbers. This is because the masked versions
    do not consider the sequence positions where the input is a PAD character:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的一个截断输出如下所示。如你所见，`masked_accuracy`和`val_masked_accuracy`的数值似乎比`accuracy`和`val_accuracy`的数值更为保守。这是因为掩码版本没有考虑输入为PAD字符的序列位置：
- en: '[PRE39]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here are some examples of POS tags generated for some random sentences in the
    test set, shown together with the POS tags in the corresponding ground truth sentences.
    As you can see, while the metric values are not perfect, it seems to have learned
    to do POS tagging fairly well:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了一些随机句子的POS标签，这些句子来自测试集，并与相应的真实标签句子的POS标签一起展示。如你所见，尽管度量值并不完美，但它似乎已经学会了相当好地进行POS标注：
- en: '[PRE40]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If you would like to run this code yourself, you can find the code in the code
    folder for this chapter. To run it from the command line, enter the following
    command. The output is written to the console:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想自己运行这段代码，你可以在本章的代码文件夹中找到它。要从命令行运行，输入以下命令。输出将写入控制台：
- en: '[PRE41]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Now that we have seen some examples of three common RNN network topologies,
    let’s explore the most popular of them all – the seq2seq model, which is also
    known as the recurrent encoder-decoder architecture.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过了三种常见的RNN网络拓扑结构的示例，让我们来探索其中最流行的一个——seq2seq模型，它也被称为递归编码器-解码器架构。
- en: Encoder-decoder architecture – seq2seq
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器-解码器架构 – seq2seq
- en: The example of a many-to-many network we just saw was mostly similar to the
    many-to-one network. The one important difference was that the RNN returns outputs
    at each time step instead of a single combined output at the end. One other noticeable
    feature was that the number of input time steps was equal to the number of output
    time steps. As you learn about the encoder-decoder architecture, which is the
    “other,” and arguably more popular, style of a many-to-many network, you will
    notice another difference – the output is in line with the input in a many-to-many
    network, that is, it is not necessary for the network to wait until all of the
    input is consumed before generating the output.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才看到的多对多网络示例与多对一网络非常相似。唯一的重要区别是，RNN在每个时间步返回输出，而不是在最后返回一个合并的输出。另一个显著的特点是输入时间步的数量等于输出时间步的数量。当你学习编码器-解码器架构时，这种“另一种”更流行的多对多网络风格，你会注意到另一个区别——在多对多网络中，输出与输入是对齐的，也就是说，网络不需要等到所有输入被处理完后才生成输出。
- en: The encoder-decoder architecture is also called a seq2seq model. As the name
    implies, the network is composed of an encoder and a decoder part, both RNN-based
    and capable of consuming and returning sequences of outputs corresponding to multiple
    time steps. The biggest application of the seq2seq network has been in neural
    machine translation, although it is equally applicable for problems that can be
    roughly structured as translation problems. Some examples are sentence parsing
    [10] and image captioning [24]. The seq2seq model has also been used for time
    series analysis [25] and question answering.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器架构也被称为seq2seq模型。顾名思义，网络由一个编码器和一个解码器组成，二者都基于RNN，并且能够处理并返回对应多个时间步的输出序列。seq2seq网络最大的应用是神经机器翻译，尽管它同样适用于那些大致可以结构化为翻译问题的任务。一些例子包括句子解析[10]和图像标注[24]。seq2seq模型也已被用于时间序列分析[25]和问答系统。
- en: In the seq2seq model, the encoder consumes the source sequence, which is a batch
    of integer sequences. The length of the sequence is the number of input time steps,
    which corresponds to the maximum input sequence length (padded or truncated as
    necessary). Thus, the dimensions of the input tensor are (`batch_size`, `number_of_encoder_timesteps`).
    This is passed into an embedding layer, which will convert the integer at each
    time step into an embedding vector. The output of the embedding is a tensor of
    shape (`batch_size`, `number_of_encoder_timesteps`, `encoder_embedding_dim`).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在seq2seq模型中，编码器处理源序列，这是一批整数序列。序列的长度是输入时间步的数量，对应于最大输入序列长度（根据需要进行填充或截断）。因此，输入张量的维度为（`batch_size`，`number_of_encoder_timesteps`）。这个张量被传递到嵌入层，嵌入层将每个时间步的整数转换为嵌入向量。嵌入层的输出是一个形状为（`batch_size`，`number_of_encoder_timesteps`，`encoder_embedding_dim`）的张量。
- en: This tensor is fed into an RNN, which converts the vector at each time step
    into the size corresponding to its encoding dimension. This vector is a combination
    of the current time step and all previous time steps. Typically, the encoder will
    return the output at the last time step, representing the context or “thought”
    vector for the entire sequence. This tensor has the shape (`batch_size`, `encoder_rnn_dim`).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这个张量被输入到RNN中，RNN将每个时间步的向量转换为与其编码维度相对应的大小。这个向量是当前时间步和所有之前时间步的组合。通常，编码器会返回最后一个时间步的输出，代表整个序列的上下文或“思想”向量。这个张量的形状为（`batch_size`，`encoder_rnn_dim`）。
- en: The decoder network has a similar architecture as the encoder, except there
    is an additional dense layer at each time step to convert the output. The input
    to each time step on the decoder side is the hidden state of the previous time
    step and the input vector that is the token predicted by the decoder of the previous
    time step. For the very first time step, the hidden state is the context vector
    from the encoder, and the input vector corresponds to the token that will initiate
    sequence generation on the target side. For the translation use case, for example,
    it is a **beginning-of-string** (**BOS**) pseudo-token. The shape of the hidden
    signal is (`batch_size`, `encoder_rnn_dim`) and the shape of the input signal
    across all time steps is (`batch_size`, `number_of_decoder_timesteps`).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器网络的架构与编码器相似，唯一不同的是每个时间步上会有一个额外的密集层来转换输出。解码器每个时间步的输入是上一个时间步的隐藏状态和由解码器在上一个时间步预测的标记向量。对于第一个时间步，隐藏状态来自编码器的上下文向量，而输入向量对应于在目标端启动序列生成的标记。例如，在翻译的用例中，它是**开始字符串**（**BOS**）伪标记。隐藏信号的形状是（`batch_size`，`encoder_rnn_dim`），而在所有时间步上的输入信号形状是（`batch_size`，`number_of_decoder_timesteps`）。
- en: 'Once it passes through the embedding layer, the output tensor shape is (`batch_size`,
    `number_of_decoder_timesteps`, `decoder_embedding_dim`). The next step is the
    decoder RNN layer, the output of which is a tensor of shape (`batch_size`, `number_of_decoder_timesteps`,
    `decoder_rnn_dim`). The output at each time step is then sent through a dense
    layer, which converts the vector into the size of the target vocabulary, so the
    output of the dense layer is (`batch_size`, `number_of_decoder_timesteps`, `output_vocab_size`).
    This is basically a probability distribution over tokens at each time step, so
    if we compute the argmax over the last dimension, we can convert it back into
    a predicted sequence of tokens in the target language. *Figure 5.7* shows a high-level
    view of the seq2seq architecture:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦通过嵌入层，输出张量的形状是（`batch_size`，`number_of_decoder_timesteps`，`decoder_embedding_dim`）。下一步是解码器RNN层，其输出是一个形状为（`batch_size`，`number_of_decoder_timesteps`，`decoder_rnn_dim`）的张量。每个时间步的输出会经过一个密集层，该层将向量转换为目标词汇表的大小，因此密集层的输出形状是（`batch_size`，`number_of_decoder_timesteps`，`output_vocab_size`）。这基本上是每个时间步上的标记概率分布，因此如果我们计算最后一维的argmax，我们就可以将其转换回目标语言中的预测标记序列。*图5.7*展示了seq2seq架构的高层次视图：
- en: '![Diagram  Description automatically generated](img/B18331_05_07.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图示  描述自动生成](img/B18331_05_07.png)'
- en: 'Figure 5.7: Seq2seq network data flow. Image Source: Artur Suilin [25]'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：Seq2seq网络数据流。图片来源：Artur Suilin [25]
- en: In the next section, we will look at an example of a seq2seq network for machine
    translation.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将查看一个用于机器翻译的seq2seq网络的示例。
- en: Example ‒ seq2seq without attention for machine translation
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 ‒ 无注意力机制的seq2seq机器翻译
- en: To understand the seq2seq model in greater detail, we will look at an example
    of one that learns how to translate from English to French using the French-English
    bilingual dataset from the Tatoeba Project (1997-2019) [26]. The dataset contains
    approximately 167,000 sentence pairs. To make our training go faster, we will
    only consider the first 30,000 sentence pairs for our training.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更详细地理解seq2seq模型，我们将通过一个例子来学习如何使用Tatoeba项目（1997-2019）的法英双语数据集将英文翻译成法文。[26]该数据集包含大约167,000对句子。为了加快训练速度，我们只会使用前30,000对句子进行训练。
- en: 'As always, we will start with the imports:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们将从导入开始：
- en: '[PRE42]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The data is provided as a remote zip file. The easiest way to access the file
    is to download it from [http://www.manythings.org/anki/fra-eng.zip](http://www.manythings.org/anki/fra-eng.zip)
    and expand it locally using unzip. The zip file contains a tab-separated file
    called `fra.txt`, with French and English sentence pairs separated by a tab, one
    pair per line. The code expects the `fra.txt` file in a dataset folder in the
    same directory as itself. We want to extract three different datasets from it.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 数据作为远程zip文件提供。访问该文件的最简单方法是从[http://www.manythings.org/anki/fra-eng.zip](http://www.manythings.org/anki/fra-eng.zip)下载并使用unzip在本地解压。zip文件包含一个名为`fra.txt`的制表符分隔文件，其中法语和英语的句子对通过制表符分隔，每行一个句子对。代码期望在与自身相同目录的`dataset`文件夹中找到`fra.txt`文件。我们希望从中提取三个不同的数据集。
- en: If you recall the structure of the seq2seq network, the input to the encoder
    is a sequence of English words. On the decoder side, the input is a set of French
    words, and the output is the sequence of French words offset by one time step.
    The following function will download the zip file, expand it, and create the datasets
    described before.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回忆一下 seq2seq 网络的结构，编码器的输入是一个英语单词序列。在解码器一侧，输入是一个法语单词序列，输出是一个时间步偏移的法语单词序列。以下函数将下载压缩文件，解压并创建前面描述的数据集。
- en: The input is preprocessed to *asciify* the characters, separate out specific
    punctuations from their neighboring word, and remove all characters other than
    alphabets and these specific punctuation symbols. Finally, the sentences are converted
    into lowercase. Each English sentence is just converted into a single sequence
    of words. Each French sentence is converted into two sequences, one preceded by
    the BOS pseudo-word and the other followed by the **end-of-sentence** (**EOS**)
    pseudo-word.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据经过预处理，以 *ascii化* 字符，分离出与相邻单词的特定标点符号，并移除除字母和这些特定标点符号之外的所有字符。最后，句子被转换为小写。每个英语句子被转换为一个单一的词序列。每个法语句子被转换为两个序列，一个以
    BOS 伪词开头，另一个以 **句子结束**（**EOS**）伪词结尾。
- en: 'The first sequence starts at position 0 and stops one short of the final word
    in the sentence, and the second sequence starts at position 1 and goes all the
    way to the end of the sentence:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个序列从位置 0 开始，停止于句子中的倒数第二个单词，第二个序列从位置 1 开始，一直延伸到句子的末尾：
- en: '[PRE43]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Our next step is to tokenize our inputs and create the vocabulary. Since we
    have sequences in two different languages, we will create two different tokenizers
    and vocabularies, one for each language.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是对输入进行分词，并创建词汇表。由于我们有两种不同语言的序列，我们将为每种语言分别创建两个不同的分词器和词汇表。
- en: The tf.keras framework provides a very powerful and versatile tokenizer class
    – here, we have set filters to an empty string and `lower` to `False` because
    we have already done what was needed for tokenization in our `preprocess_sentence()`
    function. The Tokenizer creates various data structures from which we can compute
    the vocabulary sizes and lookup tables that allow us to go from word to word index
    and back.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: tf.keras 框架提供了一个非常强大且多功能的分词器类——在这里，我们将过滤器设置为空字符串，`lower` 设置为 `False`，因为我们已经在
    `preprocess_sentence()` 函数中完成了分词所需的操作。分词器创建了各种数据结构，我们可以从中计算词汇表大小和查找表，允许我们从单词到单词索引之间进行转换。
- en: 'Next, we handle different length sequences of words by padding with zeros at
    the end, using the `pad_sequences()` function. Because our strings are fairly
    short, we do not do any truncation; we just pad to the maximum length of sentence
    that we have (8 words for English and 16 words for French):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过使用 `pad_sequences()` 函数在序列末尾填充零来处理不同长度的单词序列。因为我们的字符串比较短，所以我们不进行截断，只对句子的最大长度进行填充（英语为
    8 个单词，法语为 16 个单词）：
- en: '[PRE44]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Finally, we convert the data into a TensorFlow dataset, and then split it into
    a training and test dataset:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将数据转换成 TensorFlow 数据集，然后将其分割为训练数据集和测试数据集：
- en: '[PRE45]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Our data is now ready to be used for training the seq2seq network, which we
    will define next. Our encoder is an embedding layer followed by a GRU layer. The
    input to the encoder is a sequence of integers, which is converted into a sequence
    of embedding vectors of size `embedding_dim`. This sequence of vectors is sent
    to an RNN, which converts the input at each of the `num_timesteps` time steps
    into a vector of size `encoder_dim`. Only the output at the last time step is
    returned, as shown by `return_sequences=False`.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在已经准备好用于训练 seq2seq 网络，接下来我们将定义该网络。我们的编码器是一个嵌入层，后跟一个 GRU 层。编码器的输入是一个整数序列，这个序列会被转换成一个大小为
    `embedding_dim` 的嵌入向量序列。这个向量序列被送入 RNN，在每个 `num_timesteps` 时间步中将输入转换为大小为 `encoder_dim`
    的向量。只有最后一个时间步的输出会被返回，正如 `return_sequences=False` 所示。
- en: The decoder has almost the same structure as the encoder, except that it has
    an additional dense layer that converts the vector of size `decoder_dim`, which
    is output from the RNN, into a vector that represents the probability distribution
    across the target vocabulary. The decoder also returns outputs along with all
    its time steps.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的结构几乎与编码器相同，唯一不同的是它有一个额外的全连接层，用于将从 RNN 输出的大小为 `decoder_dim` 的向量转换为一个表示目标词汇表概率分布的向量。解码器还会返回所有时间步的输出。
- en: 'In our example network, we have chosen our embedding dimension to be 128, followed
    by an encoder and decoder RNN dimension of 1024 each. Note that we have to add
    1 to the vocabulary size for both the English and French vocabularies to account
    for the PAD character that was added during the `pad_sequences()` step:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例网络中，我们选择将嵌入维度设置为 128，然后是每个 1024 的编码器和解码器 RNN 维度。请注意，我们必须为英文和法文词汇表的词汇大小分别加
    1，以便考虑在 `pad_sequences()` 步骤中添加的 PAD 字符：
- en: '[PRE46]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now that we have defined our `Encoder` and `Decoder` classes, let’s revisit
    the dimensions of their inputs and outputs. The following piece of (throwaway)
    code can be used to print out the dimensions of the various inputs and outputs
    of the system. It has been left in for convenience as a commented-out block in
    the code supplied with this chapter:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了 `Encoder` 和 `Decoder` 类，让我们回顾一下它们的输入和输出的维度。以下一段（临时）代码可以用来打印系统中各种输入和输出的维度。为了方便，代码已经作为注释块保留在本章随附的代码中：
- en: '[PRE47]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This produces the following output, which is in line with our expectations.
    The encoder input is a batch of a sequence of integers, each sequence being of
    size 8, which is the maximum number of tokens in our English sentences, so its
    dimension is (`batch_size`, `maxlen_en`).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出，符合我们的预期。编码器输入是一批整数序列，每个序列的大小为 8，这是我们英文句子中最大数量的标记，因此其维度为（`batch_size`，`maxlen_en`）。
- en: The output of the encoder is a single tensor (`return_sequences=False`) of shape
    (`batch_size`, `encoder_dim`) and represents a batch of context vectors representing
    the input sentences. The encoder state tensor has the same dimensions. The decoder
    outputs are also a batch of sequences of integers, but the maximum size of a French
    sentence is 16; therefore, the dimensions are (`batch_size`, `maxlen_fr`).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器的输出是一个形状为（`batch_size`，`encoder_dim`）的单一张量（`return_sequences=False`），表示一批上下文向量，代表输入句子。编码器状态张量具有相同的维度。解码器的输出也是一批整数序列，但法文句子的最大大小为
    16；因此，维度为（`batch_size`，`maxlen_fr`）。
- en: 'The decoder predictions are a batch of probability distributions across all
    time steps; hence, the dimensions are (`batch_size`, `maxlen_fr`, `vocab_size_fr+1`),
    and the decoder state is the same dimension as the encoder state (`batch_size`,
    `decoder_dim`):'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的预测是一个跨所有时间步的概率分布的批次；因此，维度为（`batch_size`，`maxlen_fr`，`vocab_size_fr+1`），解码器状态的维度与编码器状态相同（`batch_size`，`decoder_dim`）：
- en: '[PRE48]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Next, we define the loss function. Because we padded our sentences, we don’t
    want to bias our results by considering the equality of pad words between the
    labels and predictions. Our loss function masks our predictions with the labels,
    so any padded positions on the label are also removed from the predictions, and
    we only compute our loss using the non-zero elements on both the label and predictions.
    This is done as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义损失函数。由于我们对句子进行了填充，我们不想通过考虑标签和预测之间填充词的相等性来偏倚结果。我们的损失函数通过标签对预测进行掩蔽，因此标签上的任何填充位置也会从预测中移除，我们仅使用标签和预测中的非零元素来计算损失。实现如下：
- en: '[PRE49]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Because the seq2seq model is not easy to package into a simple Keras model,
    we have to handle the training loop manually as well. Our `train_step()` function
    handles the flow of data and computes the loss at each step, applies the gradient
    of the loss back to the trainable weights, and returns the loss.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 seq2seq 模型不容易打包成一个简单的 Keras 模型，我们还需要手动处理训练循环。我们的 `train_step()` 函数处理数据流，并在每一步计算损失，应用损失的梯度回到可训练的权重上，并返回损失。
- en: Notice that the training code is not quite the same as what was described in
    our discussion of the seq2seq model earlier. Here, it appears that the entire
    `decoder_input` is fed in one go into the decoder to produce the output offset
    by one time step, whereas in the discussion, we said that this happens sequentially,
    where the token generated in the previous time step is used as the input for the
    next time step.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，训练代码与我们之前讨论的 seq2seq 模型中描述的有所不同。在这里，似乎整个 `decoder_input` 一次性输入到解码器中，以产生偏移一个时间步长的输出，而在讨论中，我们说这发生是按顺序的，其中前一个时间步生成的标记作为下一个时间步的输入。
- en: 'This is a common technique used to train seq2seq networks, which is called
    **Teacher Forcing**, where the input to the decoder is the ground truth output
    instead of the prediction from the previous time step. This is preferred because
    it makes training faster but also results in some degradation in prediction quality.
    To offset this, techniques such as **Scheduled Sampling** can be used, where the
    input is sampled randomly either from the ground truth or the prediction at the
    previous time step, based on some threshold (this depends on the problem, but
    usually varies between 0.1 and 0.4):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种常用的技术，称为**教师强迫（Teacher Forcing）**，其中解码器的输入是实际的输出，而不是来自上一个时间步的预测。这样做的优势是可以加速训练，但也可能导致预测质量的下降。为了解决这个问题，可以使用**定时采样（Scheduled
    Sampling）**等技术，在这种技术中，输入会根据某个阈值从实际输出或上一个时间步的预测中随机选择（这个阈值依赖于问题，通常在0.1到0.4之间）：
- en: '[PRE50]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The `predict()` method is used to randomly sample a single English sentence
    from the dataset and use the model trained so far to predict the French sentence.
    For reference, the label French sentence is also displayed. The `evaluate()` method
    computes the **BiLingual Evaluation Understudy** (**BLEU**) score [35] between
    the labels and predictions across all records in the test set. BLEU scores are
    generally used where multiple ground truth labels exist (we have only one) and
    compare up to 4-grams (n-grams with *n=4*) in both reference and candidate sentences.
    Both the `predict()` and `evaluate()` methods are called at the end of every epoch:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()`方法用于从数据集中随机选择一个英语句子，并利用目前训练的模型预测法语句子。为了参考，标签的法语句子也会显示出来。`evaluate()`方法计算**双语评估准则（BiLingual
    Evaluation Understudy）**（**BLEU**）分数[35]，该分数衡量测试集中的所有记录的标签和预测之间的差异。BLEU分数通常用于多个地面真实标签存在的情况（我们这里只有一个），并比较参考句子和候选句子中的最多4-gram（n-gram中的*n=4*）。`predict()`和`evaluate()`方法在每个epoch结束时都会被调用：'
- en: '[PRE51]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The training loop is shown as follows. We will use the Adam optimizer for our
    model. We also set up a checkpoint so that we can save our model after every 10
    epochs. We then train the model for 250 epochs, and print out the loss, an example
    sentence and its translation, and the BLEU score computed over the entire test
    set:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环如下所示。我们将使用Adam优化器进行模型训练。我们还设置了检查点，以便在每10个epoch后保存我们的模型。然后，我们训练模型250个epoch，并打印出损失、一个示例句子及其翻译，以及在整个测试集上计算的BLEU分数：
- en: '[PRE52]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The results from the first 5 and last 5 epochs of training are shown as follows.
    Notice that the loss has gone down from about 1.5 to around 0.07 in epoch 247\.
    The BLEU scores have also gone up by around 2.5 times. Most impressive, however,
    is the difference in translation quality between the first 5 and last 5 epochs:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是训练的前5个和最后5个epoch的结果。注意到，损失从大约1.5降到247 epoch时的约0.07。BLEU分数也提高了约2.5倍。然而，最令人印象深刻的是前5个和最后5个epoch之间的翻译质量差异：
- en: '| **Epoch-#** | **Loss (Training)** | **BLEU Score (Test)** | **English** |
    **French (true)** | **French (predicted)** |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| **Epoch-#** | **Loss (Training)** | **BLEU Score (Test)** | **英文** | **法文（真实）**
    | **法文（预测）** |'
- en: '| 1 | 1.4119 | 1.957e-02 | tom is special. | tom est special. | elle est tres
    bon. |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1.4119 | 1.957e-02 | 汤姆很特别。 | tom est special. | elle est tres bon. |'
- en: '| 2 | 1.1067 | 2.244e-02 | he hates shopping. | il deteste faire les courses.
    | il est tres mineure. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1.1067 | 2.244e-02 | 他讨厌购物。 | il deteste faire les courses. | il est
    tres mineure. |'
- en: '| 3 | 0.9154 | 2.700e-02 | did she say it? | l a t elle dit? | n est ce pas
    clair? |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.9154 | 2.700e-02 | 她说了吗？ | l a t elle dit? | n est ce pas clair? |'
- en: '| 4 | 0.7817 | 2.803e-02 | i d rather walk. | je prefererais marcher. | je
    suis alle a kyoto. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.7817 | 2.803e-02 | 我宁愿走路。 | je prefererais marcher. | je suis alle
    a kyoto. |'
- en: '| 5 | 0.6632 | 2.943e-02 | i m in the car. | je suis dans la voiture. | je
    suis toujours inquiet. |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.6632 | 2.943e-02 | 我在车里。 | je suis dans la voiture. | je suis toujours
    inquiet. |'
- en: '| ... |  |  |  |  |  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| ... |  |  |  |  |  |'
- en: '| 245 | 0.0896 | 4.991e-02 | she sued him. | elle le poursuivit en justice.
    | elle l a poursuivi en justice. |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 245 | 0.0896 | 4.991e-02 | 她起诉了他。 | elle le poursuivit en justice. | elle
    l a poursuivi en justice. |'
- en: '| 246 | 0.0853 | 5.011e-02 | she isn t poor. | elle n est pas pauvre. | elle
    n est pas pauvre. |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 246 | 0.0853 | 5.011e-02 | 她不穷。 | elle n est pas pauvre. | elle n est pas
    pauvre. |'
- en: '| 247 | 0.0738 | 5.022e-02 | which one is mine? | lequel est le mien? | lequel
    est le mien? |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 247 | 0.0738 | 5.022e-02 | 哪一个是我的？ | lequel est le mien? | lequel est le
    mien? |'
- en: '| 248 | 0.1208 | 4.931e-02 | i m getting old. | je me fais vieux. | je me fais
    vieux. |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 248 | 0.1208 | 4.931e-02 | 我在变老。 | je me fais vieux. | je me fais vieux.
    |'
- en: '| 249 | 0.0837 | 4.856e-02 | it was worth a try. | ca valait le coup d essayer.
    | ca valait le coup d essayer. |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 249 | 0.0837 | 4.856e-02 | 值得一试。 | ca valait le coup d essayer. | ca valait
    le coup d essayer. |'
- en: '| 250 | 0.0967 | 4.869e-02 | don t back away. | ne reculez pas! | ne reculez
    pas! |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 250 | 0.0967 | 4.869e-02 | 不要退缩。 | ne reculez pas! | ne reculez pas! |'
- en: 'Table 5.2: Training results by epoch'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5.2：按训练轮次的训练结果
- en: 'The full code for this example can be found in the source code accompanying
    this chapter. You will need a GPU-based machine to run it, although you may be
    able to run it on the CPU using smaller network dimensions (`embedding_dim`, `encoder_dim`,
    `decoder_dim`), smaller hyperparameters (`batch_size`, `num_epochs`), and a smaller
    number of sentence pairs. To run the code in its entirety, run the following command.
    The output will be written to the console:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子的完整代码可以在本章附带的源代码中找到。你需要一台基于 GPU 的计算机来运行它，尽管通过使用较小的网络维度（`embedding_dim`，`encoder_dim`，`decoder_dim`），较小的超参数（`batch_size`，`num_epochs`）和较少的句子对，可能能够在
    CPU 上运行它。要完整运行代码，执行以下命令。输出将写入控制台：
- en: '[PRE53]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: In the next section, we will look at a mechanism to improve the performance
    of the seq2seq network, by allowing it to focus on certain parts of the input
    more than on others in a data-driven way. This mechanism is known as the attention
    mechanism.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将介绍一种通过让 seq2seq 网络以数据驱动的方式更加关注输入的某些部分，从而提高性能的机制。这种机制被称为注意力机制。
- en: Attention mechanism
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意力机制
- en: In the previous section, we saw how the context or thought vector from the last
    time step of the encoder is fed into the decoder as the initial hidden state.
    As the context flows through the time steps on the decoder, the signal gets combined
    with the decoder output and progressively gets weaker and weaker. The result is
    that the context does not have much effect on the later time steps in the decoder.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分，我们看到来自编码器最后时间步的上下文或思维向量被作为初始隐藏状态输入到解码器中。随着上下文通过解码器的时间步流动，信号与解码器输出结合，并逐渐变弱。结果是，上下文对解码器后续时间步的影响变得较小。
- en: In addition, certain sections of the decoder output may depend more heavily
    on certain sections of the input. For example, consider an input “thank you very
    much,” and the corresponding output “merci beaucoup” for an English-to-French
    translation network such as the one we looked at in the previous section. Here,
    the English phrases “thank you,” and “very much,” correspond to the French “merci”
    and “beaucoup” respectively. This information is also not conveyed adequately
    through the single context vector.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，解码器输出的某些部分可能会更依赖于输入的某些部分。例如，考虑输入“thank you very much”以及相应的输出“merci beaucoup”——这是我们在上一部分看到的英法翻译网络的一个例子。在这里，英语短语“thank
    you”和“very much”分别对应法语的“merci”和“beaucoup”。这些信息通过单一的上下文向量无法得到充分传递。
- en: The attention mechanism provides access to all encoder hidden states at every
    time step on the decoder. The decoder learns which part of the encoder states
    to pay more attention to. The use of attention has resulted in great improvements
    to the quality of machine translation, as well as a variety of standard natural
    language processing tasks.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在解码器的每个时间步提供对所有编码器隐藏状态的访问。解码器学习在编码器状态中该关注哪一部分。使用注意力机制显著提高了机器翻译的质量，并对各种标准的自然语言处理任务产生了巨大改进。
- en: The use of attention is not limited to seq2seq networks. For example, attention
    is a key component in the “Embed, Encode, Attend, Predict” formula for creating
    state-of-the-art deep learning models for NLP [34]. Here, attention has been used
    to preserve as much information as possible when downsizing from a larger to a
    more compact representation, for example, when reducing a sequence of word vectors
    into a single sentence vector.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的使用不限于 seq2seq 网络。例如，注意力是“Embed, Encode, Attend, Predict”公式中的一个关键组成部分，用于创建最先进的深度学习模型进行自然语言处理
    [34]。在这里，注意力机制被用来尽可能保留信息，在从更大的表示缩减到更紧凑的表示时，举例来说，当将一系列词向量缩减为一个单一的句子向量时。
- en: 'Essentially, the attention mechanism provides a way to score tokens in the
    target against all tokens in the source and modify the input signal to the decoder
    accordingly. Consider an encoder-decoder architecture where the input and output
    time steps are denoted by indices *i* and *j* respectively, and the hidden states
    on the encoder and decoder at these respective time steps are denoted by *h*[i]
    and *s*[j]. Inputs to the encoder are denoted by *x*[i], and outputs from the
    decoder are denoted by *y*[j]. In an encoder-decoder network without attention,
    the value of decoder state *s*[j] is given by the hidden state *s*[j][-1] and
    output *y*[j][-1] at the previous time step. The attention mechanism adds a third
    signal *c*[j], known as the attention context. With attention, therefore, the
    decoder’s hidden state *s*[j] is a function of *y*[j][-1], *s*[j][-1], and *c*[j],
    which is shown as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，注意力机制提供了一种为目标中的令牌打分的方式，评估与源中所有令牌的关系，并相应地修改传递给解码器的输入信号。考虑一个编码器-解码器架构，其中输入和输出时间步由索引*i*和*j*表示，编码器和解码器在这些时间步的隐藏状态分别由*h*[i]和*s*[j]表示。编码器的输入由*x*[i]表示，解码器的输出由*y*[j]表示。在没有注意力的编码器-解码器网络中，解码器状态*s*[j]的值由上一时间步的隐藏状态*s*[j][-1]和输出*y*[j][-1]给出。注意力机制添加了一个第三信号*c*[j]，称为注意力上下文。因此，使用注意力后，解码器的隐藏状态*s*[j]是*y*[j][-1]、*s*[j][-1]和*c*[j]*的函数，如下所示：
- en: '![](img/B18331_05_012.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_012.png)'
- en: 'The attention context signal *c*[j] is computed as follows. For every decoder
    step *j*, we compute the alignment between the decoder state *s*[j][-1] and every
    encoder state *h*[i]. This gives us a set of *N* similarity values *e*[ij] for
    each decoder state *j*, which we then convert into a probability distribution
    by computing their corresponding softmax values *b*[ij]. Finally, the attention
    context *c*[j] is computed as the weighted sum of the encoder states *h*[i] and
    their corresponding softmax weights *b*[ij] over all *N* encoder time steps. The
    set of equations shown encapsulates this transformation for each decoder step
    *j*:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力上下文信号*c*[j]的计算方法如下。对于每一个解码器步*j*，我们计算解码器状态*s*[j][-1]与每个编码器状态*h*[i]之间的对齐度。这为每个解码器状态*j*提供了一组*N*个相似度值*e*[ij]，然后我们通过计算它们的softmax值*b*[ij]来将其转换为一个概率分布。最后，注意力上下文*c*[j]作为加权和计算，其中加权系数是编码器状态*h*[i]及其相应的softmax权重*b*[ij]，涵盖了所有*N*个编码器时间步。以下方程组表示了每个解码器步骤*j*的这一转换：
- en: '![](img/B18331_05_013.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_013.png)'
- en: Multiple attention mechanisms have been proposed based on how the alignment
    is done. We will describe a few next. For notational convenience, we will indicate
    the state vector *h*[i] on the encoder side with *h*, and the state vector *s*[j][-1]
    on the decoder side with *s*.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 基于对齐方式，已经提出了多种注意力机制。接下来我们将描述几种。为了便于记法，我们将编码器端的状态向量*h*[i]表示为*h*，将解码器端的状态向量*s*[j][-1]表示为*s*。
- en: 'The simplest formulation of alignment is **content-based attention**. It was
    proposed by Graves, Wayne, and Danihelka [27], and is just the cosine similarity
    between the encoder and decoder states. A precondition for using this formulation
    is that the hidden state vector on both the encoder and decoder must have the
    same dimensions:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐的最简单形式是**基于内容的注意力**。它由Graves、Wayne和Danihelka [27] 提出，简单来说，就是编码器和解码器状态之间的余弦相似度。使用这种形式的先决条件是编码器和解码器的隐藏状态向量必须具有相同的维度：
- en: '![](img/B18331_05_014.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_014.png)'
- en: 'Another formulation, known as **additive** or **Bahdanau attention**, was proposed
    by Bahdanau, Cho, and Bengio [28]. This involves combining the state vectors using
    learnable weights in a small neural network, given by the following equation.
    Here, the *s* and *h* vectors are concatenated and multiplied by the learned weights
    *W*, which is equivalent to using two learned weights *W*[s] and *W*[h] to multiply
    with *s* and *h*, and adding the results:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种形式，称为**加性**或**Bahdanau注意力**，是由Bahdanau、Cho和Bengio [28] 提出的。它通过在一个小型神经网络中使用可学习的权重来组合状态向量，公式如下所示。在这里，*s*和*h*向量被连接并与学习的权重*W*相乘，这等价于使用两个学习的权重*W*[s]和*W*[h]分别与*s*和*h*相乘，并将结果相加：
- en: '![](img/B18331_05_015.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_015.png)'
- en: Luong, Pham, and Manning [29] proposed a set of three attention formulations
    (dot, general, and concat), of which the general formulation is also known as
    the **multiplicative** or **Luong’s attention**.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Luong、Pham和Manning [29] 提出了一组三种注意力形式（点积、一般形式和拼接形式），其中一般形式也称为**乘法**或**Luong的注意力**。
- en: 'The `dot` and `concat` attention formulations are similar to the content-based
    and additive attention formulations discussed earlier. The multiplicative attention
    formulation is given by the following equation:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`dot` 和 `concat` 注意力的公式类似于前面讨论的基于内容和加性注意力的公式。乘性注意力的公式如下所示：'
- en: '![](img/B18331_05_016.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_016.png)'
- en: 'Finally, Vaswani, et al. [30] proposed a variation on content-based attention,
    called the **scaled dot-product attention**, which is given by the following equation.
    Here, *N* is the dimension of the encoder hidden state *h*. Scaled dot-product
    attention is used in transformer architecture, which we will learn about in the
    next chapter:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Vaswani 等人 [30] 提出了基于内容的注意力机制的变种，称为**缩放点积注意力**，其公式如下所示。这里，*N* 是编码器隐藏状态 *h*
    的维度。缩放点积注意力用于 Transformer 架构，我们将在下一章学习：
- en: '![](img/B18331_05_017.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_017.png)'
- en: Attention mechanisms can also be categorized by what they attend to. Using this
    categorization scheme attention mechanisms can be self-attention, global or soft
    attention, and local or hard attention.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制还可以根据其关注的对象进行分类。使用这种分类方案，注意力机制可以是自注意力、全局或软注意力，以及局部或硬注意力。
- en: Self-attention is when the alignment is computed across different sections of
    the same sequence and has been found to be useful for applications such as machine
    reading, abstractive text summarization, and image caption generation.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是指在同一序列的不同部分之间计算对齐，并已被证明对机器阅读、抽象文本摘要和图像字幕生成等应用非常有用。
- en: Soft or global attention is when the alignment is computed over the entire input
    sequence, and hard or local attention is when the alignment is computed over part
    of the sequence. The advantage of soft attention is that it is differentiable;
    however, it can be expensive to compute. Conversely, hard attention is cheaper
    to compute at inference time but is non-differentiable and requires more complicated
    techniques during training.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 软注意力或全局注意力是指对整个输入序列计算对齐，而硬注意力或局部注意力是指对部分序列计算对齐。软注意力的优点是它是可微分的；然而，它的计算代价可能很高。相反，硬注意力在推理时计算更便宜，但它是不可微分的，并且在训练时需要更复杂的技术。
- en: In the next section, we will see how to integrate the attention mechanism with
    a seq2seq network and how it improves performance.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何将注意力机制与 seq2seq 网络集成，以及它如何提高性能。
- en: Example ‒ seq2seq with attention for machine translation
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例 ‒ 带有注意力机制的 seq2seq 用于机器翻译
- en: Let’s look at the same example of machine translation that we saw earlier in
    this chapter, except that the decoder will now attend to the encoder outputs using
    the additive attention mechanism proposed by Bahdanau, et al. [28], and the multiplicative
    one proposed by Luong, et al [29].
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下本章前面看到的相同的机器翻译示例，只不过解码器现在将使用 Bahdanau 等人 [28] 提出的加性注意力机制，以及 Luong 等人 [29]
    提出的乘性注意力机制来关注编码器输出。
- en: 'The first change is to the encoder. Instead of returning a single context or
    thought vector, it will return outputs at every time point, because the attention
    mechanism will need this information. Here is the revised encoder class with the
    change highlighted:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个变化是对编码器的改动。它不再返回单一的上下文或思想向量，而是会在每个时间点返回输出，因为注意力机制需要这些信息。以下是突出显示的修改后的编码器类：
- en: '[PRE54]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The decoder will have bigger changes. The biggest is the declaration of attention
    layers, which need to be defined, so let’s do that first. Let’s first consider
    the class definition for the additive attention proposed by Bahdanau. Recall that
    this combines the decoder hidden state at each time step with all the encoder
    hidden states to produce an input to the decoder at the next time step, which
    is given by the following equation:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器将进行更大的改动。最大的变化是注意力层的声明，需要先定义它们。首先，我们考虑 Bahdanau 提出的加性注意力的类定义。回顾一下，这将解码器在每个时间步的隐藏状态与所有编码器隐藏状态结合，以产生下一时间步解码器的输入，公式如下：
- en: '![](img/B18331_05_015.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_05_015.png)'
- en: 'The *W [s;h]* in the equation is shorthand for two separate linear transformations
    (of the form *y = Wx + b*), one on *s*, and the other on *h*. The two linear transformations
    are implemented as dense layers, as shown in the following implementation. We
    subclass a `tf.keras` Layer object since our end goal is to use this as a layer
    in our network, but it is also acceptable to subclass a Model object. The `call()`
    method takes the query (the decoder state) and values (the encoder states), computes
    the score, then the alignment as the corresponding softmax, and context vector
    as given by the equation, and then returns them. The shape of the context vector
    is given by (`batch_size`, `num_decoder_timesteps`), and the alignments have the
    shape (`batch_size`, `num_encoder_timesteps`, `1`). The weights for the dense
    layer’s `W1`, `W2`, and `V` tensors are learned during training:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 方程中的*W [s;h]*是两个单独线性变换的简写（形式为*y = Wx + b*），一个作用于*s*，另一个作用于*h*。这两个线性变换被实现为全连接层，如下实现所示。我们继承了`tf.keras`的Layer对象，因为我们的最终目标是将其用作网络中的一层，但继承Model对象也是可以接受的。`call()`方法接受查询（解码器状态）和值（编码器状态），计算分数，然后根据对应的softmax计算对齐，并根据方程给出上下文向量，最后返回它们。上下文向量的形状为（`batch_size`，`num_decoder_timesteps`），对齐的形状为（`batch_size`，`num_encoder_timesteps`，`1`）。全连接层的`W1`、`W2`和`V`张量的权重在训练过程中学习：
- en: '[PRE55]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The Luong attention is multiplicative, but the general implementation is similar.
    Instead of declaring three linear transformations `W1`, `W2`, and `V`, we only
    have a single one `W`. The steps in the `call()` method follow the same general
    steps – first, we compute the scores according to the equation for Luong’s attention,
    as described in the last section. Then, we compute the alignments as the corresponding
    softmax version of the scores and then the context vector as the dot product of
    the alignment and the values. Like the weights in the Bahdanau attention class,
    the weight matrices represented by the dense layer `W` are learned during training:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: Luong注意力是乘法形式的，但其一般实现类似。我们不再声明三个线性变换`W1`、`W2`和`V`，而是只有一个`W`。`call()`方法中的步骤遵循相同的一般步骤——首先，我们根据Luong注意力的方程计算分数，如上一节所述。然后，我们计算对齐作为分数的对应softmax版本，再计算上下文向量作为对齐与值的点积。与Bahdanau注意力类中的权重一样，表示全连接层`W`的权重矩阵在训练过程中学习：
- en: '[PRE56]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'To verify that the two classes are drop-in replacements for each other, we
    run the following piece of throwaway code (commented out in the source code for
    this example). We just manufacture some random inputs and send them to both attention
    classes:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这两个类是否可以互相替代，我们运行以下这段临时代码（在本示例的源代码中已被注释掉）。我们只需制造一些随机输入并将其发送给两个注意力类：
- en: '[PRE57]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The preceding code produces the following output and shows, as expected, that
    the two classes produce identically shaped outputs when given the same input.
    Hence, they are drop-in replacements for each other:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码产生了以下输出，并如预期所示，两个类在给定相同输入时生成了相同形状的输出。因此，它们可以互相替代：
- en: '[PRE58]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Now that we have our attention classes, let’s look at the decoder. The difference
    in the `init()` method is the addition of the attention class variable, which
    we have set to the `BahdanauAttention` class. In addition, we have two additional
    transformations, `Wc` and `Ws`, that will be applied to the output of the decoder
    RNN. The first one has a `tanh` activation to modulate the output between -1 and
    +1, and the next one is a standard linear transformation. Compared to the seq2seq
    network without an attention decoder component, this decoder takes an additional
    parameter `encoder_output` in its `call()` method and returns an additional context
    vector:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了注意力类，让我们看看解码器。`init()`方法中的区别在于增加了注意力类变量，我们已将其设置为`BahdanauAttention`类。此外，我们还有两个附加的变换，`Wc`和`Ws`，它们将应用于解码器RNN的输出。第一个变换使用`tanh`激活函数，将输出调节到-1和+1之间，接下来的变换是标准的线性变换。与没有注意力解码器组件的seq2seq网络相比，这个解码器在`call()`方法中接受额外的参数`encoder_output`，并返回一个额外的上下文向量：
- en: '[PRE59]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The training loop is also a little different. Unlike the seq2seq without attention
    network, where we used teacher forcing to speed up training, using attention means
    that we now have to consume the decoder input one by one. This is because the
    decoder output at the previous step influences more strongly, through attention,
    the output at the current time step. Our new training loop is described by the
    `train_step` function below and is significantly slower than the training loop
    on the seq2seq network without attention. However, this kind of training loop
    may be used on the former network as well, especially when we want to implement
    scheduled sampling strategies:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 训练循环也有所不同。与没有注意力机制的seq2seq网络不同，在该网络中我们使用教师强制（teacher forcing）来加速训练，而使用注意力机制意味着我们现在必须逐个消费解码器的输入。这是因为前一步的解码器输出通过注意力机制对当前时间步的输出影响更大。我们新的训练循环由下面的`train_step`函数描述，且比没有注意力机制的seq2seq网络的训练循环要慢得多。然而，这种训练循环也可以用于前述网络，特别是在我们想要实现计划性采样策略时：
- en: '[PRE60]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The `predict()` and `evaluate()` methods also have similar changes, since they
    also implement the new data flow on the decoder side that involves an extra `encoder_out`
    parameter and an extra `context` return value.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()` 和 `evaluate()` 方法也有类似的变化，因为它们同样实现了新的数据流，涉及到额外的 `encoder_out` 参数和额外的
    `context` 返回值。'
- en: 'We trained two versions of the seq2seq network with attention, one with additive
    (Bahdanau) attention and one with multiplicative (Luong) attention. Both networks
    were trained for 50 epochs instead of 250\. However, in both cases, translations
    were produced with a quality similar to that obtained from the seq2seq network
    without attention trained for 250 epochs. The training losses at the end of training
    for the seq2seq networks with either attention mechanism were marginally lower,
    and the BLEU scores on the test sets were slightly higher, compared with the seq2seq
    network without attention:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了两种版本的带注意力机制的seq2seq网络，一种是加性（Bahdanau）注意力机制，另一种是乘性（Luong）注意力机制。两种网络都训练了50个周期，而不是250个周期。然而，在这两种情况下，产生的翻译质量与训练了250个周期的没有注意力机制的seq2seq网络相似。使用注意力机制的seq2seq网络的训练损失稍微低一些，测试集上的BLEU得分略高于没有注意力机制的seq2seq网络：
- en: '| **Network Description** | **Ending Loss (training set)** | **Ending BLEU
    score (test set)** |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| **网络描述** | **训练集最终损失** | **测试集最终BLEU得分** |'
- en: '| seq2seq without attention, trained for 250 epochs | 0.0967 | 4.869e-02 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 没有注意力机制的seq2seq，训练250个周期 | 0.0967 | 4.869e-02 |'
- en: '| seq2seq with additive attention, trained for 50 epochs | 0.0893 | 5.508e-02
    |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 使用加性注意力机制的seq2seq，训练50个周期 | 0.0893 | 5.508e-02 |'
- en: '| seq2seq with multiplicative attention, trained for 50 epochs | 0.0706 | 5.563e-02
    |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 使用乘性注意力机制的seq2seq，训练50个周期 | 0.0706 | 5.563e-02 |'
- en: 'Table 5.3: BLEU scores for the different methods'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5.3: 不同方法的BLEU得分'
- en: 'Here are some examples of the translations produced by the two networks. Epoch
    numbers and the type of attention used are mentioned with each example. Notice
    that even when the translations are not 100% the same as the labels, many of them
    are valid translations of the original:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是由两种网络产生的翻译示例。每个示例都提到训练的周期数和使用的注意力类型。注意，即使翻译结果与标签不完全相同，许多翻译仍然是原文的有效翻译：
- en: '| **Attention Type** | **Epoch-#** | **English** | **French (label)** | **French
    (predicted)** |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| **注意力类型** | **周期数** | **英语** | **法语（标签）** | **法语（预测）** |'
- en: '| Bahdanau | 20 | your cat is fat. | ton chat est gras. | ton chat est mouille.
    |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| Bahdanau | 20 | your cat is fat. | ton chat est gras. | ton chat est mouille.
    |'
- en: '| 25 | i had to go back. | il m a fallu retourner. | il me faut partir. |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 25 | i had to go back. | il m a fallu retourner. | il me faut partir. |'
- en: '| 30 | try to find it. | tentez de le trouver. | tentez de le trouver. |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 30 | try to find it. | tentez de le trouver. | tentez de le trouver. |'
- en: '| Luong | 20 | that s peculiar. | c est etrange. | c est deconcertant. |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Luong | 20 | that s peculiar. | c est etrange. | c est deconcertant. |'
- en: '| 25 | tom is athletic. | thomas est sportif. | tom est sportif. |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 25 | tom is athletic. | thomas est sportif. | tom est sportif. |'
- en: '| 30 | it s dangerous. | c est dangereux. | c est dangereux. |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 30 | it s dangerous. | c est dangereux. | c est dangereux. |'
- en: 'Table 5.4: Examples of English-to-French translations'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5.4: 英语到法语的翻译示例'
- en: 'The full code for the network described here is in the `seq2seq_with_attn.py`
    file in the code folder for this chapter. To run the code from the command line,
    please use the following command. You can switch between Bahdanau (additive) or
    Luong (multiplicative) attention mechanisms by commenting out one or the other
    in the `init()` method of the `Decoder` class:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这里描述的网络的完整代码在本章的代码文件夹中的`seq2seq_with_attn.py`文件中。要从命令行运行代码，请使用以下命令。你可以通过注释掉`Decoder`类的`init()`方法中的一个或另一个来切换使用Bahdanau（加性）或Luong（乘性）注意力机制：
- en: '[PRE61]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Summary
  id: totrans-358
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about RNNs, a class of networks that is specialized
    for dealing with sequences such as natural language, time series, speech, and
    so on. Just like CNNs exploit the geometry of images, RNNs exploit the sequential
    structure of their inputs. We learned about the basic RNN cell, how it handles
    state from previous time steps, and how it suffers from vanishing and exploding
    gradients because of inherent problems with BPTT. We saw how these problems lead
    to the development of novel RNN cell architectures such as LSTM, GRU, and peephole
    LSTMs. We also learned about some simple ways to make your RNN more effective,
    such as making it bidirectional or stateful.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们学习了RNN（递归神经网络），一种专门处理序列数据的网络类型，如自然语言、时间序列、语音等。就像CNN利用图像的几何结构一样，RNN利用其输入的序列结构。我们学习了基本的RNN单元，它如何处理来自前一个时间步的状态，以及由于BPTT（反向传播通过时间）的固有问题，它如何遭遇梯度消失和梯度爆炸的问题。我们看到这些问题导致了新型RNN单元架构的出现，如LSTM、GRU和窥视LSTM。我们还学习了一些让RNN更有效的简单方法，比如使其成为双向或有状态的。
- en: We then looked at different RNN topologies and how each topology is adapted
    to a particular set of problems. After a lot of theory, we finally saw examples
    of three of these topologies. We then focused on one of these topologies, called
    seq2seq, which first gained popularity in the machine translation community, but
    has since been used in situations where the use case can be adapted to look like
    a machine translation problem.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们讨论了不同的RNN拓扑结构，以及每种拓扑如何适应特定问题。经过大量理论讲解后，我们最终看到了这三种拓扑的实例。然后我们专注于其中一种拓扑，称为seq2seq，这一拓扑最初在机器翻译领域获得了广泛应用，但此后也被用于那些可以适应为类似机器翻译问题的场景中。
- en: From here, we looked at attention, which started as a way to improve the performance
    of seq2seq networks but has since been used very effectively in many situations
    where we want to compress the representation while keeping the data loss to a
    minimum. We looked at different kinds of attention and an example of using them
    in a seq2seq network with attention.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了注意力机制，它最初是为了提高seq2seq网络的性能而提出的，但此后它在许多场合中被非常有效地使用，尤其是当我们希望在尽量减少数据丢失的同时压缩表示时。我们学习了不同类型的注意力机制，并且给出了在带有注意力机制的seq2seq网络中使用它们的实例。
- en: In the next chapter, you will learn about transformers, a state-of-the-art encoder-decoder
    architecture where the recurrent layers have been replaced by attention layers.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将学习关于transformers（变压器）的内容，这是一种最先进的编码器-解码器架构，其中递归层已被注意力层替代。
- en: References
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Jozefowicz, R., Zaremba, R. and Sutskever, I. (2015). *An Empirical Exploration
    of Recurrent Neural Network Architectures*. Journal of Machine Learning
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jozefowicz, R., Zaremba, R. 和 Sutskever, I. (2015). *递归神经网络架构的经验探索*。机器学习学报
- en: 'Greff, K., et al. (July 2016). *LSTM: A Search Space Odyssey*. IEEE Transactions
    on Neural Networks and Learning Systems'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Greff, K., 等人. (2016年7月). *LSTM：一个搜索空间的奥德赛*。IEEE神经网络与学习系统学报
- en: Bernal, A., Fok, S., and Pidaparthi, R. (December 2012). *Financial Markets
    Time Series Prediction with Recurrent Neural Networks*
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bernal, A., Fok, S., 和 Pidaparthi, R. (2012年12月). *使用递归神经网络进行金融市场时间序列预测*
- en: 'Hadjeres, G., Pachet, F., and Nielsen, F. (August 2017). *DeepBach: a Steerable
    Model for Bach Chorales Generation*. Proceedings of the 34th International Conference
    on Machine Learning (ICML)'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hadjeres, G., Pachet, F., 和 Nielsen, F. (2017年8月). *DeepBach: 一种可控制的巴赫合唱生成模型*。第34届国际机器学习大会（ICML）论文集'
- en: 'Karpathy, A. (2015). *The Unreasonable Effectiveness of Recurrent Neural Networks*.
    URL: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Karpathy, A. (2015). *递归神经网络的非凡效果*。网址: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
- en: Karpathy, A., Li, F. (2015). *Deep Visual-Semantic Alignments for Generating
    Image Descriptions*. Conference on Pattern Recognition and Pattern Recognition
    (CVPR)
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karpathy, A., Li, F. (2015). *深度视觉-语义对齐用于生成图像描述*。模式识别与计算机视觉会议 (CVPR)
- en: Socher, et al. (2013). *Recursive Deep Models for Sentiment Compositionality
    over a Sentiment Treebank*. Proceedings of the 2013 Conference on Empirical Methods
    in Natural Language Processing (EMNLP)
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Socher, 等. (2013). *情感组成树上情感合成的递归深度模型*。2013年实证方法自然语言处理会议（EMNLP）论文集
- en: 'Bahdanau, D., Cho, K., and Bengio, Y. (2015). *Neural Machine Translation by
    Jointly Learning to Align and Translate*. arXiv: 1409.0473 [cs.CL]'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Bahdanau, D., Cho, K., 和Bengio, Y. (2015). *通过联合学习对齐和翻译进行神经机器翻译*。arXiv: 1409.0473
    [cs.CL]'
- en: 'Wu, Y., et al. (2016). *Google’s Neural Machine Translation System: Bridging
    the Gap between Human and Machine Translation*. arXiv 1609.08144 [cs.CL]'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wu, Y., 等. (2016). *Google的神经机器翻译系统：弥合人类与机器翻译之间的差距*。arXiv 1609.08144 [cs.CL]
- en: Vinyals, O., et al. (2015). *Grammar as a Foreign Language*. Advances in Neural
    Information Processing Systems (NIPS)
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vinyals, O., 等. (2015). *语法作为外语*。神经信息处理系统进展（NIPS）
- en: 'Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1985). *Learning Internal
    Representations by Error Propagation*. Parallel Distributed Processing: Explorations
    in the Microstructure of Cognition'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rumelhart, D. E., Hinton, G. E., 和Williams, R. J. (1985). *通过误差传播学习内部表示*。并行分布式处理：认知微结构探索
- en: 'Britz, D. (2015). *Recurrent Neural Networks Tutorial, Part 3 - Backpropagation
    Through Time and Vanishing Gradients*: [http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Britz, D. (2015). *递归神经网络教程，第3部分 - 通过时间的反向传播和梯度消失问题*: [http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)'
- en: Pascanu, R., Mikolov, T., and Bengio, Y. (2013). *On the difficulty of training
    Recurrent Neural Networks*. Proceedings of the 30th International Conference on
    Machine Learning (ICML)
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pascanu, R., Mikolov, T., 和Bengio, Y. (2013). *训练递归神经网络的困难*。第30届国际机器学习大会（ICML）论文
- en: Hochreiter, S., and Schmidhuber, J. (1997). *LSTM can solve hard long time lag
    problems*. Advances in Neural Information Processing Systems (NIPS)
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hochreiter, S., 和Schmidhuber, J. (1997). *LSTM能够解决困难的长时间延迟问题*。神经信息处理系统进展（NIPS）
- en: 'Britz, D. (2015). *Recurrent Neural Network Tutorial, Part 4 – Implementing
    a GRU/LSTM RNN with Python and Theano*: [http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Britz, D. (2015). *递归神经网络教程，第4部分 - 使用Python和Theano实现GRU/LSTM RNN*: [http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)'
- en: 'Olah, C. (2015). *Understanding LSTM Networks*: [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Olah, C. (2015). *理解LSTM网络*: [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
- en: 'Cho, K., et al. (2014). *Learning Phrase Representations using RNN Encoder-Decoder
    for Statistical Machine Translation*. arXiv: 1406.1078 [cs.CL]'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Cho, K., 等. (2014). *使用RNN编码器-解码器学习短语表示用于统计机器翻译*。arXiv: 1406.1078 [cs.CL]'
- en: 'Shi, X., et al. (2015). *Convolutional LSTM Network: A Machine Learning Approach
    for Precipitation Nowcasting*. arXiv: 1506.04214 [cs.CV]'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Shi, X., 等. (2015). *卷积LSTM网络：一种用于降水短期预报的机器学习方法*。arXiv: 1506.04214 [cs.CV]'
- en: Gers, F.A., and Schmidhuber, J. (2000). *Recurrent Nets that Time and Count*.
    Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks
    (IJCNN)
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gers, F.A., 和Schmidhuber, J. (2000). *时间与计数的递归网络*。IEEE-INNS-ENNS国际神经网络联合会议（IJCNN）会议论文
- en: 'Kotzias, D. (2015). *Sentiment Labeled Sentences Dataset*, provided as part
    of “From Group to Individual Labels using Deep Features” (KDD 2015): [https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kotzias, D. (2015). *情感标注句子数据集*，作为“从群体到个体标签的深度特征”部分提供（KDD 2015）：[https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)
- en: Collobert, R., et al (2011). *Natural Language Processing (Almost) from Scratch*.
    Journal of Machine Learning Research (JMLR)
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Collobert, R., 等 (2011). *从零开始的自然语言处理*。机器学习研究期刊（JMLR）
- en: 'Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1993). *Building a
    large annotated corpus of English: the Penn Treebank*. Journal of Computational
    Linguistics'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Marcus, M. P., Santorini, B., 和 Marcinkiewicz, M. A. (1993). *构建大型英语注释语料库：Penn
    Treebank*。计算语言学杂志
- en: 'Bird, S., Loper, E., and Klein, E. (2009). *Natural Language Processing with
    Python, O’Reilly Media Inc*. Installation: [https://www.nltk.org/install.xhtml](https://www.nltk.org/install.xhtml)'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bird, S., Loper, E., 和 Klein, E. (2009). *使用 Python 进行自然语言处理，O'Reilly Media
    Inc*。安装：[https://www.nltk.org/install.xhtml](https://www.nltk.org/install.xhtml)
- en: 'Liu, C., et al. (2017). *MAT: A Multimodal Attentive Translator for Image Captioning*.
    arXiv: 1702.05658v3 [cs.CV]'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Liu, C., 等人 (2017). *MAT：一种用于图像字幕的多模态注意力翻译器*。arXiv: 1702.05658v3 [cs.CV]'
- en: 'Suilin, A. (2017). *Kaggle Web Traffic Time Series Forecasting*. GitHub repository:
    [https://github.com/Arturus/kaggle-web-traffic](https://github.com/Arturus/kaggle-web-traffic)'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Suilin, A. (2017). *Kaggle 网络流量时间序列预测*。GitHub 仓库：[https://github.com/Arturus/kaggle-web-traffic](https://github.com/Arturus/kaggle-web-traffic)
- en: 'Tatoeba Project. (1997-2019). Tab-delimited Bilingual Sentence Pairs: [http://tatoeba.org](https://tatoeba.org/en/)
    and [http://www.manythings.org/anki](http://www.manythings.org/anki)'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tatoeba Project. (1997-2019). 制表符分隔的双语句对： [http://tatoeba.org](https://tatoeba.org/en/)
    和 [http://www.manythings.org/anki](http://www.manythings.org/anki)
- en: 'Graves, A., Wayne, G., and Danihelka, I. (2014). *Neural Turing Machines*.
    arXiv: 1410.5401v2 [cs.NE]'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Graves, A., Wayne, G., 和 Danihelka, I. (2014). *神经图灵机*。arXiv: 1410.5401v2 [cs.NE]'
- en: 'Bahdanau, D., Cho, K., and Bengio, Y. (2015). *Neural Machine Translation by
    jointly learning to Align and Translate*. arXiv: 1409.0473v7 [cs.CL]'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Bahdanau, D., Cho, K., 和 Bengio, Y. (2015). *通过联合学习对齐和翻译的神经机器翻译*。arXiv: 1409.0473v7
    [cs.CL]'
- en: 'Luong, M., Pham, H., and Manning, C. (2015). *Effective Approaches to Attention-based
    Neural Machine Translation*. arXiv: 1508.04025v5 [cs.CL]'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Luong, M., Pham, H., 和 Manning, C. (2015). *基于注意力的神经机器翻译的有效方法*。arXiv: 1508.04025v5
    [cs.CL]'
- en: Vaswani, A., et al. (2017). *Attention Is All You Need*. 31st Conference on
    Neural Information Processing Systems (NeurIPS)
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Vaswani, A., 等人 (2017). *注意力机制是你所需要的全部*。第31届神经信息处理系统大会（NeurIPS）
- en: 'Zhang, A., Lipton, Z. C., Li, M., and Smola, A. J. (2019). *Dive into Deep
    Learning*: [http://www.d2l.ai](http://www.d2l.ai)'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Zhang, A., Lipton, Z. C., Li, M., 和 Smola, A. J. (2019). *深入学习*： [http://www.d2l.ai](http://www.d2l.ai)
- en: 'Ba, J. L., Kiros, J. R., and Hinton, G. E. (2016). *Layer Normalization*. arXiv:
    1607.06450v1 [stat.ML]'
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Ba, J. L., Kiros, J. R., 和 Hinton, G. E. (2016). *层归一化*。arXiv: 1607.06450v1
    [stat.ML]'
- en: 'Allamar, J. (2018). *The Illustrated Transformer*: [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Allamar, J. (2018). *图解 Transformer*： [http://jalammar.github.io/illustrated-transformer/](http://jalammar.github.io/illustrated-transformer/)
- en: 'Honnibal, M. (2016). *Embed, encode, attend, predict: The new deep learning
    formula for state-of-the-art NLP models*: [https://explosion.ai/blog/deep-learning-formula-nlp](https://explosion.ai/blog/deep-learning-formula-nlp)'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Honnibal, M. (2016). *嵌入、编码、注意、预测：先进自然语言处理模型的新深度学习公式*： [https://explosion.ai/blog/deep-learning-formula-nlp](https://explosion.ai/blog/deep-learning-formula-nlp)
- en: 'Papineni, K., Roukos, S., Ward, T., and Zhu, W. (2002). *BLEU: A Method for
    Automatic Evaluation of Machine Translation*. Proceedings of the 40th Annual Meeting
    for the Association of Computational Linguistics (ACL)'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Papineni, K., Roukos, S., Ward, T., 和 Zhu, W. (2002). *BLEU：一种自动评估机器翻译的方法*。计算语言学协会（ACL）第40届年会论文集
- en: 'Project Gutenberg (2019): [https://www.gutenberg.org/](https://www.gutenberg.org/)'
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Project Gutenberg (2019)： [https://www.gutenberg.org/](https://www.gutenberg.org/)
- en: Join our book’s Discord space
  id: totrans-400
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人交流，并与超过 2000 名成员一起学习： [https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
