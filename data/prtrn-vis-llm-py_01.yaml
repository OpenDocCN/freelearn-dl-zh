- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: An Introduction to Pretraining Foundation Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练基础模型简介
- en: The biggest lesson that can be read from 70 years of AI research is that general
    methods that leverage computation are ultimately the most effective, and by a
    large margin … The only thing that matters in the long run is the leveraging of
    computation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从70年的人工智能研究中可以得到的最大教训是，利用计算的通用方法最终是最有效的，而且差距很大……从长远来看，唯一重要的是利用计算。
- en: – Richard Sutton, “The Bitter Lesson,” 2019 (1)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – Richard Sutton, “The Bitter Lesson,” 2019 (1)
- en: In this chapter, you’ll be introduced to foundation models, the backbone of
    many artificial intelligence and machine learning systems today. In particular,
    we will dive into their creation process, also called pretraining, and understand
    where it’s competitive to improve the accuracy of your models. We will discuss
    the core transformer architecture underpinning state-of-the-art models such as
    Stable Diffusion, BERT, Vision Transformers, OpenChatKit, CLIP, Flan-T5, and more.
    You will learn about the encoder and decoder frameworks, which work to solve a
    variety of use cases.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将接触到基础模型，它们是当今许多人工智能和机器学习系统的核心。特别是，我们将深入了解它们的创建过程，也就是预训练，并了解在哪些情况下，改进模型的准确性具有竞争力。我们将讨论支撑最先进模型的核心Transformer架构，如稳定扩散（Stable
    Diffusion）、BERT、视觉Transformer、OpenChatKit、CLIP、Flan-T5等。你将了解编码器和解码器框架，它们用于解决各种使用案例。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: The art of pretraining and fine-tuning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练与微调的艺术
- en: The Transformer model architecture
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer模型架构
- en: State-of-the-art vision and language models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最先进的视觉与语言模型
- en: Encoders and decoders
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器与解码器
- en: The art of pretraining and fine-tuning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练与微调的艺术
- en: Humanity is one of Earth’s most interesting creatures. We are capable of producing
    the greatest of beauty and asking the most profound questions, and yet fundamental
    aspects about us are, in many cases, largely unknown. What exactly is consciousness?
    What is the human mind, and where does it reside? What does it mean to be human,
    and how do humans learn?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人类是地球上最有趣的生物之一。我们能够创造出最伟大的美丽，并提出最深刻的问题，然而关于我们的基本方面，在许多情况下仍然知之甚少。意识究竟是什么？人类的心智是什么，它存在于何处？成为人类意味着什么？人类是如何学习的？
- en: While scientists, artists, and thinkers from countless disciplines grapple with
    these complex questions, the field of computation marches forward to replicate
    (and in some cases, surpass) human intelligence. Today, applications from self-driving
    cars to writing screenplays, search engines, and question-answering systems have
    one thing in common – they all use a model, and sometimes many different kinds
    of models. Where do these models come from, how do they acquire intelligence,
    and what steps can we take to apply them for maximum impact? Foundation models
    are essentially compact representations of massive sets of data. The representation
    comes about through applying a *pretraining objective* onto the dataset, from
    predicting masked tokens to completing sentences. Foundation models are useful
    because once they have been created, through the process called pretraining, they
    can either be deployed directly or fine-tuned for a downstream task. An example
    of a foundation model deployed directly is **Stable Diffusion**, which was pretrained
    on billions of image-text pairs and generates useful images from text immediately
    after pretraining. An example of a fine-tuned foundation model is **BERT**, which
    was pretrained on large language datasets, but is most useful when adapted for
    a downstream domain, such as classification.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当来自无数学科的科学家、艺术家和思想家在应对这些复杂的问题时，计算领域却在不断推进，试图复制（在某些情况下，超越）人类智能。今天，从自动驾驶汽车到编写剧本、搜索引擎和问答系统的应用都有一个共同点——它们都使用模型，有时是多种不同类型的模型。这些模型从哪里来，如何获取智能，以及我们可以采取哪些步骤来最大化它们的影响力？基础模型本质上是大规模数据集的紧凑表示。这种表示是通过将*预训练目标*应用于数据集来实现的，从预测掩码标记到完成句子。基础模型之所以有用，是因为一旦创建完成（通过预训练过程），它们可以直接部署，也可以为下游任务进行微调。直接部署的基础模型示例是**稳定扩散（Stable
    Diffusion）**，它在数十亿对图像-文本配对上进行了预训练，预训练后即可根据文本生成有用的图像。微调过的基础模型示例是**BERT**，它在大规模语言数据集上进行了预训练，但当其适应于下游领域（如分类）时，效果最佳。
- en: When applied in natural language processing, these models can complete sentences,
    classify text into different categories, produce summarizations, answer questions,
    do basic math, and generate creative artifacts such as poems and titles. In computer
    vision, foundation models are useful everywhere from image classification to generation,
    pose estimation to object detection, pixel mapping, and more.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理应用中，这些模型可以完成句子补全、将文本分类到不同类别、生成摘要、回答问题、进行基础数学运算，以及生成创意作品如诗歌和标题。在计算机视觉中，基础模型在图像分类到生成、姿态估计到物体检测、像素映射等各个领域都非常有用。
- en: This comes about because of defining a **pretraining objective**, which we’ll
    learn about in detail in this book. We’ll also cover its peer method, **fine-tuning**,
    which helps the model learn more about a specific domain. This more generally
    falls under the category of **transfer learning**, the practice of taking a pretrained
    neural network and supplying it with a novel dataset with the hope of enhancing
    its knowledge in a certain dimension. In both vision and language, these terms
    have some overlap and some clear distinctions, but don’t worry, we’ll cover them
    more throughout the chapters. I’m using the term *fine-tuning* to include the
    whole set of techniques to adapt a model to another domain, outside of the one
    where it was trained, not in the narrow, classic sense of the term.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这归因于定义了一个**预训练目标**，我们将在本书中详细学习这一内容。我们还将讨论它的对等方法——**微调**，该方法帮助模型学习特定领域的知识。更广泛地说，这属于**迁移学习**范畴，即将一个预训练的神经网络与一个新数据集结合，期望提升其在某一维度的知识。在视觉和语言领域，这些术语有一些重叠和明显的区别，但别担心，我们将在后续章节中进一步探讨。我使用“*微调*”这个术语，涵盖了适应模型到另一个领域的所有技术，而非狭义上经典的定义。
- en: Fundamentals – pretraining objectives
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基础知识 – 预训练目标
- en: The heart of large-scale pretraining revolves around this core concept. A `[MASK]`
    token in place of certain words, and training the model to fill in those words.
    Others take a different route, using the left-hand side of a given text string
    to attempt to generate the right-hand side.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模预训练的核心围绕这个核心概念展开：用`[MASK]`标记替换某些词语，并训练模型去填充这些词语。其他方法则走另一条路，使用给定文本串的左侧部分来尝试生成右侧部分。
- en: The training process happens through a **forward pass**, sending your raw training
    data through the neural network to produce some output word. The loss function
    then computes the difference between this predicted word and the one found in
    the data. This difference between the predicted values and the actual values then
    serves as the basis for the **backward pass**. The backward pass itself usually
    leverages a type of stochastic gradient descent to update the parameters of the
    neural network with respect to that same loss function, ensuring that, next time
    around, it’s more likely to get a lower loss function.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程通过**前向传播**进行，将原始训练数据输入神经网络，产生输出结果。然后，损失函数计算预测结果与数据中实际值之间的差异。这种预测值与实际值之间的差异将作为**反向传播**的基础。反向传播通常利用一种随机梯度下降方法，根据该损失函数更新神经网络的参数，从而确保下次训练时能够得到更低的损失函数值。
- en: In the case of BERT*(2)*, the pretraining objective is called a **masked token
    loss**. For generative textual models of the GPT *(3)* variety, the pretraining
    objective is called **causal language loss**. Another way of thinking about this
    entire process is **self-supervised learning**, utilizing content already available
    in a dataset to serve as a signal to the model. In computer vision, you’ll also
    see this referred to as a **pretext task**. More on state-of-the-art models in
    the sections ahead!
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以 BERT*(2)* 为例，预训练目标称为**掩码标记损失**。对于 GPT *(3)* 类型的生成文本模型，预训练目标称为**因果语言损失**。另一种看待这一过程的方式是**自监督学习**，利用数据集中已有的内容作为模型的信号。在计算机视觉中，你也会看到它被称为**前置任务**。关于最先进的模型，后续章节会有更多介绍！
- en: Personally, I think pretraining is one of the most exciting developments in
    machine learning research. Why? Because, as Richard Sutton suggests controversially
    at the start of the chapter, it’s computationally efficient. Using pretraining,
    you can build a model from massive troves of information available on the internet,
    then combine all of this knowledge using your own proprietary data and apply it
    to as many applications as you can dream of. On top of that, pretraining opens
    the door for tremendous collaboration across company, country, language, and domain
    lines. The industry is truly just getting started in developing, perfecting, and
    exploiting the pretraining paradigm.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 就个人而言，我认为预训练是机器学习研究中最令人兴奋的进展之一。为什么？因为正如理查德·萨顿在本章开始时有争议地指出的那样，它在计算上非常高效。通过预训练，你可以利用互联网上大量的信息构建一个模型，然后结合所有这些知识，使用你自己的专有数据，将其应用到你能想象到的所有应用中。更重要的是，预训练为跨公司、跨国家、跨语言和跨领域的巨大合作打开了大门。整个行业实际上才刚刚开始开发、完善和利用预训练范式。
- en: We know that pretraining is interesting and effective, but where is it competitive
    in its own right? Pretraining your own model is useful *when your own proprietary
    dataset is very large and different from common research datasets, and primarily
    unlabeled*. Most of the models we will learn about in this book are trained on
    similar corpora – Wikipedia, social media, books, and popular internet sites.
    Many of them focus on the English language, and few of them consciously use the
    rich interaction between visual and textual data. Throughout the book, we will
    learn about the nuances and different advantages of selecting and perfecting your
    pretraining strategies.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道预训练既有趣又有效，但它在什么方面具有竞争力呢？当你的专有数据集非常庞大且与常见研究数据集不同，并且主要是未标注的时，预训练自己的模型是非常有用的。我们在本书中将学习的大多数模型都训练于相似的语料库——维基百科、社交媒体、书籍和流行的互联网网站。它们中的许多集中在英语语言上，且很少有模型有意识地利用视觉和文本数据之间的丰富互动。在整本书中，我们将学习选择和完善预训练策略的细微差别和不同优势。
- en: If your business or research hypothesis hinges on non-standard natural languages,
    such as financial or legal terminology, non-English languages, or rich knowledge
    from another domain, you may want to consider pretraining your own model from
    scratch. The core question you want to ask yourself is, *How valuable is an extra
    one percentage point of accuracy in my model?* If you do not know the answer to
    this question, then I strongly recommend spending some time getting yourself to
    an answer. We will spend time discussing how to do this in [*Chapter 2*](B18942_02.xhtml#_idTextAnchor034).
    Once you can confidently say an increase in the accuracy of my model is worth
    at least a few hundred thousand dollars, and even possibly a few million, then
    you are ready to begin pretraining your own model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的业务或研究假设依赖于非标准自然语言，例如金融或法律术语、非英语语言，或来自其他领域的丰富知识，你可能需要考虑从头开始预训练自己的模型。你需要问自己一个核心问题，*在我的模型中，额外的一个百分点的准确性有多大价值？*
    如果你无法回答这个问题，我强烈建议你花些时间找到答案。我们将在[*第二章*](B18942_02.xhtml#_idTextAnchor034)中详细讨论如何做到这一点。一旦你能自信地说，提高模型的准确性至少值几十万美元，甚至可能值几百万美元，那么你就准备好开始预训练自己的模型了。
- en: Now that we have learned about foundation models, how they come about through
    a process called pretraining, and how to adapt them to a specific domain through
    fine-tuning, let’s learn more about the Transformer model architecture.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了基础模型，它们是如何通过一种称为预训练的过程产生的，以及如何通过微调将其适应特定领域，接下来让我们进一步了解Transformer模型的架构。
- en: The Transformer model architecture and self-attention
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer模型架构与自注意力机制
- en: The Transformer model, presented in the now-famous 2017 paper *Attention is
    all you need*, marked a turning point for the machine learning industry. This
    is primarily because it used an existing mathematical technique, self-attention,
    to solve problems in NLP related to sequences. The Transformer certainly wasn’t
    the first attempt at modeling sequences, previously, **recurrent neural networks**
    (**RNNs**) and even **convolutional neural networks** (**CNNs**) were popular
    in language.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型，最早出现在2017年那篇著名的论文《*Attention is all you need*》中，标志着机器学习行业的一个转折点。这主要是因为它使用了一种现有的数学技术——自注意力机制，解决了与序列相关的自然语言处理（NLP）问题。Transformer当然不是第一个尝试建模序列的模型，之前，**递归神经网络**（**RNNs**）甚至**卷积神经网络**（**CNNs**）在语言处理领域中都很流行。
- en: However, the Transformer made headlines because its training cost was a small
    fraction of the existing techniques. This is because the Transformer is fundamentally
    easier to parallelize, due to its core self-attention process, than previous techniques.
    It also set new world records in machine translation. The original Transformer
    used both an encoder and decoder, techniques we will dive into later throughout
    this chapter. This joint encoder-decoder pattern was followed directly by other
    models focused on similar text-to-text tasks, such as T5.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Transformer 引起轰动是因为其训练成本仅为现有技术的一小部分。这是因为 Transformer 在其核心自注意过程中基本上更易于并行化，比之前的技术更为简单。它还在机器翻译领域刷新了世界记录。原始的
    Transformer 同时使用了编码器和解码器，我们将在本章后续部分深入讨论这些技术。其他专注于类似文本对文本任务的模型，如 T5，直接遵循了这种联合编码器-解码器模式。
- en: In 2018, Alex Radford and his team presented **Generative Pretrained Transformers**,
    a method inspired by the 2017 Transformer, but using only the decoder. Called
    **GPT**, this model handled large-scale unsupervised pretraining well, and it
    was paired with supervised fine-tuning to perform well on downstream tasks. As
    we mentioned previously, this *causal language modeling* technique optimizes the
    log probability of tokens, giving us a left-to-right ability to find the most
    probable word in a sequence.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，Alex Radford 和他的团队提出了**生成式预训练转换器**，这种方法灵感来自于2017年的 Transformer，但仅使用解码器。称为**GPT**，这种模型很好地处理了大规模无监督的预训练，并与监督微调配对以在下游任务中表现良好。正如我们之前提到的，这种*因果语言建模*技术优化了令牌的对数概率，使我们能够从左到右找到序列中最可能的单词。
- en: 'In 2019, Jacob Devlin and his team presented *BERT: Pretraining of Deep Bidirectional
    Transformers*. BERT also adopted the pretraining, fine-tuning paradigm, but implemented
    a masked language modeling loss function that helped the model learn the impact
    of tokens both before and after them. This proved useful in disambiguating the
    meaning of words in different contexts and has aided encoder-only tasks such as
    classification ever since.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '2019年，Jacob Devlin 和他的团队提出了*BERT: Pretraining of Deep Bidirectional Transformers*。BERT
    也采用了预训练和微调范式，但实施了一种掩码语言建模损失函数，帮助模型学习单词之前和之后的影响。这在消除单词在不同语境中的歧义意义方面非常有用，并自那时以来帮助了仅编码器任务如分类。'
- en: Despite their names, neither GPT nor BERT uses the full encoder-decoder as presented
    in the original Transformer paper but instead leverages the **self-attention mechanism**
    as core steps throughout the learning process. Thus, it is in fact the self-attention
    process we should understand.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们的名称如此，GPT 和 BERT 都没有像原始 Transformer 论文中展示的完整编码器-解码器那样使用，而是利用了**自注意机制**作为学习过程中的核心步骤。因此，事实上我们应该理解的是自注意过程。
- en: 'First, remember that each word, or token, is represented as an embedding. This
    embedding is created simply by using a **tokenizer**, a pretrained data object
    for each model that maps the word to its appropriate dense vector. Once we have
    the embedding per token, we use **learnable weights** to generate three new vectors:
    **key**, **query**, and **value**. We then use matrix multiplication and a few
    steps to interact with the key and the query, using the value at the very end
    to determine what was most informative in the sequence overall. Throughout the
    training loop, we update these weights to get better and better interactions,
    as determined by your pretraining objective.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，要记住每个单词或令牌都表示为一个嵌入。这种嵌入是通过使用**分词器**，每个模型的预训练数据对象，将单词映射到适当的密集向量来创建的。一旦我们获得了每个令牌的嵌入，我们使用**可学习权重**生成三个新向量：**关键词**、**查询**和**值**。然后，我们使用矩阵乘法和几个步骤与关键词和查询进行交互，最终使用值来确定整个序列中最具信息量的内容。在整个训练循环中，我们更新这些权重，以便根据您的预训练目标获得更好的交互。
- en: Your pretraining objective serves as a directional guide for how to update the
    model parameters. Said another way, your pretraining objective provides the primary
    signal to your stochastic gradient descent updating procedure, changing the weights
    of your model based on how incorrect your model predictions are. When you train
    for long periods of time, the parameters should reflect a decrease in loss, giving
    you an overall increase in accuracy.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您的预训练目标作为指导如何更新模型参数的方向指南。换句话说，您的预训练目标提供了主要信号给您的随机梯度下降更新过程，根据您的模型预测的错误程度改变模型的权重。当您长时间训练时，参数应反映损失的减少，从而使您的整体精度提高。
- en: 'Interestingly, the type of transformer heads will change slightly based on
    the different types of pretraining objectives you’re using. For example, a normal
    self-attention block uses information from both the left- and right-hand sides
    of a token to predict it. This is to provide the most informative contextual information
    for the prediction and is useful in masked language modeling. In practice, the
    self-attention heads are stacked to operate on full matrices of embeddings, giving
    us multi-head attention. Casual language modeling, however, uses a different type
    of attention head: masked self-attention. This limits the scope of predictive
    information to only the left-hand side of the matrix, forcing the model to learn
    a left-to-right procedure. This is in contrast to the more traditional self-attention,
    which has access to both the left and right sides of the sequence to make predictions.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，变压器头的类型会根据你使用的不同预训练目标略有变化。例如，普通的自注意力模块会使用来自标记左右两侧的信息来进行预测。这是为了提供最有价值的上下文信息，以便进行预测，并且在掩蔽语言建模中非常有用。实际上，自注意力头是堆叠起来的，在嵌入矩阵上操作，从而实现多头注意力。然而，日常语言建模使用的是一种不同类型的注意力头：掩蔽自注意力。这将预测信息的范围限制为仅矩阵的左侧，迫使模型学习从左到右的过程。这与传统的自注意力形成对比，后者可以访问序列的左右两侧进行预测。
- en: Most of the time, in practice, and certainly throughout this book, you won’t
    need to code any transformers or self-attention heads from scratch. Through this
    book, we will, however, be diving into many model architectures, so it’s helpful
    to have this conceptual knowledge as a base.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，实际上，尤其是在本书中，你不需要从头开始编写任何变压器或自注意力头。然而，通过本书，我们将深入探讨许多模型架构，因此拥有这些概念性知识作为基础是很有帮助的。
- en: 'From an intuitive perspective, what you’ll need to understand about transformers
    and self-attention is fewfold:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观的角度来看，关于变压器和自注意力，你需要理解的内容可以概括为几点：
- en: '**The transformer itself is a model entirely built upon a self-attention function**:
    The self-attention function takes a set of inputs, such as embeddings, and performs
    mathematical operations to combine these. When combined with token (word or subword)
    masking, the model can effectively learn how significant certain parts of the
    embeddings, or the sequence, are to the other parts. This is the meaning of self-attention;
    the model is trying to understand which parts of the input dataset are most relevant
    to the other parts.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变压器本身是完全建立在自注意力函数上的模型**：自注意力函数接受一组输入（如嵌入），并执行数学运算将这些输入组合起来。当与标记（单词或子词）掩蔽结合使用时，模型可以有效地学习某些嵌入或序列的部分对其他部分的重要性。这就是自注意力的含义；模型试图理解输入数据集中哪些部分与其他部分最相关。'
- en: '**Transformers perform exceedingly well using sequences**: Most of the benchmarks
    they’ve blown past in recent years are from NLP, for a good reason. The pretraining
    objectives for these include token masking and sequence completion, both of which
    rely on not just individual data points but the stringing of them together, and
    their combination. This is good news for those of you who already work with sequential
    data and an interesting challenge for those who don’t.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变压器在处理序列时表现非常出色**：它们在最近几年突破的许多基准测试主要来自自然语言处理，原因很简单。这些预训练目标包括标记掩蔽和序列补全，二者都不仅依赖于单独的数据点，还依赖于将它们串联起来以及它们的组合。这对于那些已经处理顺序数据的人来说是好消息，对于那些没有的人来说，则是一个有趣的挑战。'
- en: '**Transformers operate very well at large scales**: The underlying attention
    head is easily parallelizable, which gives it a strong leg-up in reference to
    other candidate sequence-based neural network architectures such as RNNs, including
    **Long Short-Term Memory** (**LSTM**) based networks. The self-attention head
    can be set to trainable in the case of pretraining, or untrainable in the case
    of fine-tuning. When attempting to actually train the self-attention heads, as
    we’ll do throughout this book, the best performance you’ll see is when the transformers
    are applied on large datasets. How large they need to be, and what trade-offs
    you can make when electing to fine-tune or pretrain, is the subject of future
    chapters.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformer在大规模应用下表现非常好**：底层的注意力机制（attention head）非常容易并行化，这使得它相对于其他候选的基于序列的神经网络架构（如RNNs，包括**长短期记忆**（**LSTM**）网络）具有明显的优势。自注意力头可以在预训练时设置为可训练，或者在微调时设置为不可训练。在训练自注意力头时，正如我们在本书中所做的，Transformer在大数据集上的表现最为优异。它们需要多大的数据集，以及在选择微调或预训练时可以做出哪些权衡，都是未来章节的内容。'
- en: Transformers are not the only means of pretraining. As we’ll see throughout
    the next section, there are many different types of models, particularly in vision
    and multimodal cases, which can deliver state-of-the-art performance.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer并不是唯一的预训练方法。正如我们在接下来的章节中将看到的，尤其是在视觉和多模态案例中，有许多不同类型的模型可以提供最先进的性能。
- en: State-of-the-art vision and language models
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最先进的视觉和语言模型
- en: If you’re new to machine learning, then there is a key concept you will eventually
    want to learn how to master, that is, **state of the art**. As you are aware,
    there are many different types of machine learning tasks, such as object detection,
    semantic segmentation, pose detection, text classification, and question answering.
    For each of these, there are many different research datasets. Each of these datasets
    provides labels, frequently for train, test, and validation splits. The datasets
    tend to be hosted by academic institutions, and each of these is purpose-built
    to train machine learning models that solve each of these types of problems.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是机器学习的新手，那么有一个关键概念你最终会想要学习并掌握，那就是**最先进技术（state of the art）**。正如你所知道的，机器学习任务有很多不同的类型，比如物体检测、语义分割、姿态检测、文本分类和问答等。每一种任务都有许多不同的研究数据集。这些数据集通常会提供标签，通常用于训练、测试和验证数据集划分。这些数据集通常由学术机构托管，每个数据集都专门用于训练解决这些问题的机器学习模型。
- en: When releasing a new dataset, researchers will frequently also release a new
    model that has been trained on the train set, tuned on the validation set, and
    separately evaluated on the test set. Their evaluation score on a new test set
    establishes a new state of the art for this specific type of modeling problem.
    When publishing certain types of papers, researchers will frequently try to improve
    performance in this area – for example, by trying to increase accuracy by a few
    percentage points on a handful of datasets.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在发布新的数据集时，研究人员通常还会发布一个新的模型，该模型已经在训练集上进行了训练，在验证集上进行了调优，并在测试集上进行了独立评估。该模型在新的测试集上的评估分数为该特定类型建模问题确立了一个新的**最先进技术**。在发布某些类型的论文时，研究人员通常会尝试提高该领域的性能——例如，通过在一些数据集上提高几个百分点的准确率。
- en: The reason state-of-the-art performance matters for you is that it is a strong
    indication of how well your model is likely to perform in the best possible scenario.
    It isn’t easy to replicate most research results, and frequently, labs will have
    developed special techniques to improve performance that may not be easily observed
    and replicated by others. This is especially true when datasets and code repositories
    aren’t shared publicly, as is the case with GPT-3\. This is acutely true when
    training methods aren’t disclosed, as with GPT-4.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**最先进技术**的性能对你来说很重要，因为它是你模型在最佳情况下可能表现如何的强有力指示。大多数研究结果不容易复制，且实验室通常会开发特殊的技术来提高性能，这些技术可能并不容易被其他人观察和复制。尤其是当数据集和代码库没有公开共享时，像GPT-3这样的情况就会变得尤为复杂。而当训练方法未公开时，像GPT-4的情况则更加明显。'
- en: However, given sufficient resources, it is possible to achieve similar performance
    as reported in top papers. An excellent place to find state-of-the-art performance
    at any given point in time is an excellent website, *Papers With Code*, maintained
    by Meta and enhanced by the community. By using this free tool, you can easily
    find top papers, datasets, models, and GitHub sites with example code. Additionally,
    they have great historical views, so you can see how the top models in different
    datasets have evolved over time.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑到足够的资源，确实可以达到类似于顶级论文中所报告的性能。一个很好的地方，可以找到当前最先进的性能，就是由Meta维护并由社区增强的一个出色网站——*Papers
    With Code*。通过使用这个免费的工具，你可以轻松地找到顶级论文、数据集、模型以及带示例代码的GitHub网站。此外，它们还提供了很好的历史视图，因此你可以看到不同数据集中的顶级模型是如何随时间演变的。
- en: In later chapters on preparing datasets and picking models, we’ll go into more
    detail on how to find the right examples for you, including how to determine how
    similar to and different from your own goals they are. Later in the book, we’ll
    also help you determine the optimal models, and sizes for them. Right now, let’s
    look at some models that, as of this writing, are currently sitting at the top
    of their respective leaderboards.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将详细讨论如何准备数据集和选择模型，包括如何确定与你自身目标的相似性和差异性。书中的后续部分，我们还将帮助你确定最佳模型及其大小。目前，让我们来看一些模型，截至目前，它们仍然稳居各自领域的排行榜之首。
- en: Top vision models as of April 2023
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 截至2023年4月的顶级视觉模型
- en: First, let’s take a quick look at the models performing the best today within
    image tasks such as classification and generation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们快速看一下当前在图像任务（如分类和生成）中表现最好的模型。
- en: '| **Dataset** | **Best model** | **From Transformer** | **Performance** |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **最佳模型** | **来自Transformer** | **性能** |'
- en: '| ImageNet | Basic-L (Lion fine-tuned) | Yes | 91.10% top 1% accuracy |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | Basic-L (Lion微调) | 是 | 91.10% 顶级 1% 准确率 |'
- en: '| CIFAR-10 | ViT-H/14 *(1)* | Yes | 99.5% correct |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-10 | ViT-H/14 *(1)* | 是 | 99.5% 正确率 |'
- en: '| COCO | InternImage-H (M3I Pre-training: [https://paperswithcode.com/paper/internimage-exploring-large-scale-vision](https://paperswithcode.com/paper/internimage-exploring-large-scale-vision))
    | No | 65.0 Box AP |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| COCO | InternImage-H (M3I预训练：[https://paperswithcode.com/paper/internimage-exploring-large-scale-vision](https://paperswithcode.com/paper/internimage-exploring-large-scale-vision))
    | 否 | 65.0 Box AP |'
- en: '| STL-10 | Diffusion ProjectedGAN | No | 6.91 FID (generation) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| STL-10 | Diffusion ProjectedGAN | 否 | 6.91 FID（生成） |'
- en: '| ObjectNet | CoCa | Yes | 82.7% top 1% accuracy |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ObjectNet | CoCa | 是 | 82.7% 顶级 1% 准确率 |'
- en: '| MNIST | Heterogeneous ensemble with simple CNN *(1)* | No | 99.91% accuracy
    (0.09% error) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| MNIST | 简单CNN的异构集成 *(1)* | 否 | 99.91% 准确率（0.09% 错误） |'
- en: Table 1.1 – Top image results
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1.1 – 顶级图像结果
- en: At first glance, these numbers may seem intimidating. After all, many of them
    are near or close to 99% accurate! Isn’t that too high of a bar for beginning
    or intermediate machine learning practitioners?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 初看这些数字，可能会让人感到有些畏惧。毕竟，其中许多模型的准确率接近或达到了99%！对于初学者或中级机器学习从业者来说，这是不是太高的标准了？
- en: Before we get too carried away with doubt and fear, it’s helpful to understand
    that most of these accuracy scores came at least five years after the research
    dataset was published. If we analyze the historical graphs available on *Paper
    With Code*, it’s easy to see that when the first researchers published their datasets,
    initial accuracy scores were closer to 60%. Then, it took many years of hard work,
    across diverse organizations and teams, to finally produce models capable of hitting
    the 90s. So, don’t lose heart! If you put in the time, you too can train a model
    that establishes a new state-of-the-art performance in a given area. This part
    is science, not magic.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对疑虑和恐惧过于担忧之前，理解一个关键点是很有帮助的，那就是这些准确度分数大多是在研究数据集发布后至少五年才取得的。如果我们分析*Paper With
    Code*上提供的历史图表，很容易发现，当第一批研究人员发布他们的数据集时，初始的准确度分数通常在60%左右。随后，经过多年各个组织和团队的努力，才最终生产出能够达到90%以上的模型。所以，不要灰心！只要你投入时间，你也可以训练出在特定领域内创造新最优性能的模型。这是科学，而非魔法。
- en: You’ll notice that while some of these models do in fact adopt a Transformer-inspired
    backend, some do not. Upon closer inspection, you’ll also see that some of these
    models rely on the pretrain and fine-tune paradigm we’ll be learning about in
    this book, but not all of them. If you’re new to machine learning, then this discrepancy
    is something to start getting comfortable with! Robust and diverse scientific
    debate, perspectives, insights, and observations are critical aspects of maintaining
    healthy communities and increasing the quality of outcomes across the field as
    a whole. This means that you can, and should, expect some divergence in methods
    you come across, and that’s a good thing.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，尽管这些模型中有一些确实采用了 Transformer 启发式的后端结构，但也有一些并没有。经过仔细观察，你还会发现，这些模型中的一些依赖于我们将在本书中学习的预训练和微调范式，但并不是所有模型都是如此。如果你对机器学习是新手，那么这种差异可能是你需要开始适应的！强大而多元的科学辩论、观点、见解和观察是维持健康社区和提升领域整体成果质量的关键因素。这意味着，你可以并且应该对遇到的方法有所预期，并且这种差异是件好事。
- en: 'Now that you have a better understanding of top models in computer vision these
    days, let’s explore one of the earliest methods combining techniques from large
    language models with vision: contrastive pretraining and natural language supervision.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然你对当前计算机视觉领域的顶尖模型有了更好的理解，让我们来探讨一下将大规模语言模型技术与视觉技术相结合的早期方法：对比预训练和自然语言监督。
- en: Contrastive pretraining and natural language supervision
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对比预训练和自然语言监督
- en: What’s interesting about both modern and classic image datasets, from Fei-Fei
    Li’s 2006 ImageNet to the LAION-5B as used in 2022 Stable Diffusion, is that the
    labels themselves are composed of natural language. Said another way, because
    the scope of the images includes objects from the physical world, the labels necessarily
    are more nuanced than single digits. Broadly speaking, this type of problem framing
    is called **natural** **language supervision**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 关于现代和经典图像数据集的有趣之处在于，从 Fei-Fei Li 于2006年提出的 ImageNet，到 2022年在 Stable Diffusion
    中使用的 LAION-5B，这些数据集的标签本身都是由自然语言构成。换句话说，由于图像的范围包括了物理世界中的物体，因此标签必然比单一数字更为细致。从广义上讲，这种问题框定方式被称为**自然语言监督**。
- en: Imagine having a large dataset of tens of millions of images, each provided
    with captions. Beyond simply naming the objects, a caption gives you more information
    about the content of the images. A caption can be anything from *Stella sits on
    a yellow couch* to *Pepper, the Australian pup*. In just a few words we immediately
    get more context than simply describing the objects. Now, imagine using a pretrained
    model, such as an encoder, to process the language into a dense vector representation.
    Then, combine this with another pretrained model, this time an image encoder,
    to process the image into another dense vector representation. Combine both of
    these in a learnable matrix, and you are on your way to contrastive pretraining!
    Also presented by Alex Radford and the team, just a few years after their work
    on GPT, this method gives us both a way to jointly learn about the relationship
    between both images and language and a model well suited to do so. The model is
    called **Contrastive Language-Image** **Pretraining** (**CLIP**).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，拥有一个包含数千万张图片的大型数据集，每张图片都有对应的描述。除了简单地命名物体之外，描述还会为你提供更多关于图片内容的信息。一个描述可以是*Stella
    坐在一张黄色沙发上*，也可以是*Pepper，这只澳大利亚小狗*。仅用几句话，我们就能获得比单纯描述物体更多的背景信息。现在，假设你使用一个预训练的模型，比如编码器，将语言处理成一个密集的向量表示。然后，将这个向量与另一个预训练模型——这次是图像编码器——结合起来，将图像处理成另一个密集的向量表示。将这两者结合在一个可学习的矩阵中，你就可以开始进行对比预训练了！这一方法同样由
    Alex Radford 和他的团队提出，距离他们在 GPT 上的研究成果仅仅几年的时间，这种方法为我们提供了一个联合学习图像和语言之间关系的方式，同时也给出了一个非常适合执行这一任务的模型。这个模型叫做**对比语言-图像预训练**（**CLIP**）。
- en: CLIP certainly isn’t the only vision-language pretraining task that uses natural
    language supervision. One year earlier, in 2019, a research team from China proposed
    a **Visual-Linguistic BERT** model attempting a similar goal. Since then, the
    joint training of vision-and-language foundation models has become very popular,
    with Flamingo, Imagen, and Stable Diffusion all presenting interesting work.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP 并不是唯一一个使用自然语言监督的视觉-语言预训练任务。早在2019年，一支来自中国的研究团队就提出了**视觉语言 BERT**模型，试图实现类似的目标。从那时起，视觉与语言基础模型的联合训练变得非常流行，Flamingo、Imagen
    和 Stable Diffusion 都展示了有趣的研究成果。
- en: Now that we’ve learned a little bit about joint vision-and-language contrastive
    pretraining, let’s explore today’s top models in language.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了一些关于联合视觉与语言对比预训练的内容，让我们来探讨今天语言领域中的顶级模型。
- en: Top language models as of April 2023
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2023年4月顶级语言模型
- en: 'Now, let’s evaluate some of today’s best-in-class models for a task extremely
    pertinent to foundation models, and thus this book: language modeling. This table
    shows a set of language model benchmark results across a variety of scenarios.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估一些当前最顶尖的模型，针对一个与基础模型极为相关的任务，这也是本书的主题：语言建模。此表展示了一组在多种场景下的语言模型基准测试结果。
- en: '| **Dataset** | **Best model** | **From Transformer** | **Performance** |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| **数据集** | **最佳模型** | **来自 Transformer** | **性能** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| WikiText-103 | Hybrid H3 (2.7B params) | No | 10.60 test perplexity |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| WikiText-103 | 混合 H3 (2.7B 参数) | 否 | 10.60 测试困惑度 |'
- en: '| Penn Treebank (Word Level) | GPT-3 (Zero-Shot) *(1)* | Yes | 20.5 test perplexity
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Penn Treebank (词汇级别) | GPT-3 (零-shot) *(1)* | 是 | 20.5 测试困惑度 |'
- en: '| LAMBADA | PaLM-540B (Few-Shot) *(1)* | Yes | 89.7% accuracy |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| LAMBADA | PaLM-540B (少-shot) *(1)* | 是 | 89.7% 准确率 |'
- en: '| Penn Treebank (Character Level) | Mogrifer LSTM + dynamic eval *(1)* | No
    | 1.083 bit per character |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Penn Treebank (字符级别) | Mogrifer LSTM + 动态评估 *(1)* | 否 | 1.083 每字符比特 |'
- en: '| C4 (Colossal Clean Crawled Corpus) | Primer | No | 12.35 perplexity |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| C4 (庞大清理抓取语料库) | Primer | 否 | 12.35 困惑度 |'
- en: Table 1.2 – Top language modeling results
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.2 – 顶级语言建模结果
- en: 'First, let’s try to answer a fundamental question. What is language modeling,
    and why does it matter? Language modeling as known today appears to have been
    formalized in two cornerstone papers: BERT *(9)* and GPT *(10)*. The core concept
    that inspired both papers is deceptively simple: how do we better use unsupervised
    natural language?'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们尝试回答一个基本问题。什么是语言建模，为什么它如此重要？今天所知的语言建模似乎在两篇基础论文中得到了形式化：BERT *(9)* 和 GPT
    *(10)*。启发这两篇论文的核心概念看似简单：我们如何更好地使用无监督的自然语言？
- en: As is no doubt unsurprising to you, the vast majority of natural language in
    our world has no direct digital label. Some natural language lends itself well
    to concrete labels, such as cases where objectivity is beyond doubt. This can
    include accuracy in answering questions, summarization, high-level sentiment analysis,
    document retrieval, and more.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所料，我们世界上绝大多数自然语言并没有直接的数字标签。一些自然语言适合使用具体标签，例如那些客观性毋庸置疑的情况。这包括回答问题的准确性、总结、情感分析、高级情感分析、文档检索等。
- en: But the process of finding these labels and producing the datasets necessary
    for them can be prohibitive, as it is entirely manual. At the same time, many
    unsupervised datasets get larger by the minute. Now that much of the global dialog
    is online, datasets rich in variety are easy to access. So, how can ML researchers
    position themselves to benefit from these large, unsupervised datasets?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但寻找这些标签并生成所需数据集的过程可能会非常昂贵，因为这一过程完全是手动的。同时，许多无监督数据集在不断增大。现在，由于全球对话大多在线进行，丰富多样的数据集变得容易获取。那么，机器学习研究人员如何定位自己，借助这些庞大且无监督的数据集获益呢？
- en: This is exactly the problem that language modeling seeks to solve. Language
    modeling is a process to apply mathematical techniques on large corpora of unlabelled
    text, relying on a variety of pretraining objectives to enable the model to *teach
    itself* about the text. Also called **self-supervision**, the precise method of
    learning varies based on the model at hand. BERT applies a mask randomly throughout
    the dataset and learns to predict the word hidden by the mask, using an encoder.
    GPT uses a decoder to predict left-to-right, starting at the beginning of a sentence,
    for example, and learning how to predict the end of the sentence. Models in the
    T5 family use both encoders and decoders to learn text-to-text tasks, such as
    translation and search. As proposed in ELECTRA *(11)*, another alternative is
    a **token replacement** objective, which opts to inject new tokens into the original
    text, rather than masking them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是语言建模试图解决的问题。语言建模是将数学技术应用于大规模无标签文本的过程，依赖多种预训练目标，使模型能够*自我学习*这些文本内容。也称为**自监督**，具体的学习方法根据所使用的模型而有所不同。BERT
    在数据集中随机应用掩码，并学习预测被掩码隐藏的单词，使用的是编码器。GPT 使用解码器从左到右进行预测，例如，从句子的开头开始，学习如何预测句子的结尾。T5
    家族的模型使用编码器和解码器共同学习文本到文本的任务，如翻译和搜索。正如在 ELECTRA *(11)* 中提出的，另一种选择是**标记替换**目标，它选择将新标记注入原始文本，而不是对其进行掩码处理。
- en: Fundamentals – fine-tuning
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 基础知识 – 微调
- en: Foundational language models are only useful in applications when paired with
    their peer method, fine-tuning. The intuition behind fine-tuning is very understandable;
    we want to take a foundational model pretrained elsewhere and apply a much smaller
    set of data to make it more focused and useful for our specific task. We can also
    call this **domain adaptation** – adapting a pretrained model to an entirely different
    domain that was not included in its pretraining task.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基础语言模型只有与其配套方法——微调一起使用时，才在应用中发挥作用。微调的直觉非常容易理解；我们希望利用一个在其他地方预训练的基础模型，并应用一个更小的数据集，使其更加专注和适用于我们的具体任务。我们也可以称之为**领域适应**——将一个预训练的模型适应到一个完全不同的领域，这个领域在其预训练任务中并未涉及。
- en: Fine-tuning tasks are everywhere! You can take a base language model, such as
    BERT, and fine-tune it for text classification. Or question answering. Or named
    entity recognition. Or you could take a different model, GPT-2 for example, and
    fine-tune it for summarization. Or you could take something like T5 and fine-tune
    it for translation. The basic idea is that you are leveraging the intelligence
    of the foundation model. You’re leveraging the compute, the dataset, the large
    neural network, and ultimately, the distribution method the researchers leveraged
    simply by inheriting their pretrained artifact. Then, you can optionally add extra
    layers to the network yourself, or more likely, use a software framework such
    as Hugging Face to simplify the process. Hugging Face has done an amazing job
    building an extremely popular open source framework with tens of thousands of
    pretrained models, and we’ll see in future chapters how to best utilize their
    examples to build our own models in both vision and language. There are many different
    types of fine-tuning, from parameter-efficient fine-tuning to instruction-fine-tuning,
    chain of thought, and even methods that don’t strictly update the core model parameters
    such as retrieval augmented generation. We’ll discuss these later in the book.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 微调任务无处不在！你可以使用一个基础语言模型，例如BERT，并将其微调用于文本分类，或者问答，或者命名实体识别。你也可以使用其他模型，例如GPT-2，并将其微调用于摘要生成。或者，你可以使用类似T5的模型并将其微调用于翻译。基本的思路是，你正在利用基础模型的智能。你正在利用计算力、数据集、大型神经网络，以及最终，研究人员通过继承他们的预训练模型所利用的分发方法。然后，你可以选择性地为网络添加额外的层，或者更可能的是，使用像Hugging
    Face这样的软件框架来简化这一过程。Hugging Face已经成功构建了一个极其流行的开源框架，拥有数万个预训练模型，我们将在未来的章节中学习如何最佳利用它们的示例来构建我们自己的视觉和语言模型。微调有许多不同类型，从参数高效的微调到指令微调、思维链，甚至有些方法并不严格更新核心模型参数，比如检索增强生成。我们将在本书的后续章节中讨论这些内容。
- en: As we will discover in future chapters, foundational language and vision models
    are not without their negative aspects. For starters, their extremely large compute
    requirements place significant energy demands on service providers. Ensuring that
    energy is met through sustainable means and that the modeling process is as efficient
    as possible are top goals for the models of the future. These large compute requirements
    are also obviously quite expensive, posing inherent challenges for those without
    sufficient resources. I would argue, however, that the core techniques you’ll
    learn throughout this book are relevant across a wide spectrum of computational
    needs and resourcing. Once you’ve demonstrated success at a smaller scale of pretraining,
    it’s usually much easier to justify the additional ask.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在未来章节中发现的那样，基础语言和视觉模型也并非没有负面影响。首先，它们对计算资源的极大需求对服务提供商造成了显著的能源压力。确保能源通过可持续方式得到满足，并且建模过程尽可能高效，是未来模型的主要目标。这些巨大的计算需求显然也非常昂贵，对资源不足的人来说构成了固有的挑战。然而，我认为，本书中你将学习到的核心技术适用于各种计算需求和资源配置。一旦你在较小规模的预训练中取得成功，通常更容易证明额外资源的需求是合理的。
- en: Additionally, as we will see in future chapters, large models are infamous for
    their ability to inherit social biases present in their training data. From associating
    certain employment aspects with gender to classifying criminal likelihood based
    on race, researchers have identified hundreds *(9)* of ways bias can creep into
    NLP systems. As with all technology, designers and developers must be aware of
    these risks and take steps to mitigate them. In later chapters, I’ll identify
    a variety of steps you can take today to reduce these risks.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如我们将在后续章节中看到的那样，大型模型因能够继承其训练数据中存在的社会偏见而声名狼藉。从将某些就业领域与性别挂钩，到基于种族分类犯罪可能性，研究人员已识别出数百种*(9)*偏见渗入自然语言处理系统的方式。和所有技术一样，设计师和开发人员必须意识到这些风险，并采取措施加以缓解。在后续章节中，我将指出你今天可以采取的各种措施，以减少这些风险。
- en: 'Next, let’s learn about a core technique used in defining appropriate experiments
    for language models: the scaling laws!'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们了解一种用于为语言模型定义适当实验的核心技术：扩展规律！
- en: Language technique spotlight – causal modeling and the scaling laws
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言技术聚焦——因果建模与扩展规律
- en: You’ve no doubt heard of the now-infamous model **ChatGPT**. For a few years,
    a San Francisco-based AI firm, OpenAI, developed research with a mission to improve
    humanity’s outcomes around artificial intelligence. Toward that end, they made
    bold leaps in scaling language models, deriving formulas as one might in physics
    to explain the performance of LLMs at scale. They originally positioned themselves
    as a non-profit, releasing their core insights and the code to reproduce them.
    Four years after its founding, however, they pivoted to cutting exclusive billion-dollar
    deals with Microsoft. Now, their 600-strong R&D teams focus on developing proprietary
    models and techniques, and many open source projects attempt to replicate and
    improve on their offerings. Despite this controversial pivot, the team at OpenAI
    gave the industry a few extremely useful insights. The first is GPT, and the second
    is the scaling laws.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你无疑听说过现在臭名昭著的模型**ChatGPT**。几年来，总部位于旧金山的人工智能公司OpenAI致力于研究，旨在改善人类在人工智能领域的成果。为了实现这一目标，他们在扩展语言模型方面取得了大胆的突破，像物理学中推导公式一样，解释了大规模语言模型（LLM）性能的规律。最初，他们将自己定位为一个非营利组织，发布了他们的核心见解和代码以供复现。然而，创立四年后，他们转向与微软达成独家数十亿美元的合作协议。如今，他们的600人研发团队专注于开发专有模型和技术，许多开源项目则试图复制并改进他们的成果。尽管这一转型备受争议，但OpenAI团队为行业提供了几个极具价值的见解。第一个是GPT，第二个是扩展规律。
- en: 'As mentioned previously, GPT-based models use **causal language modeling**
    to learn how best to complete text. This means using a left-to-right completion
    learning criteria, which updates the model’s learnable parameters until the text
    is completely accurate. While the first GPT model of 2018 was itself useful, the
    real excitement came years later in two phases. First, Jared Kaplan lead a team
    at OpenAI to suggest a novel concept: using formulas inspired by his work in physics
    to estimate the impact the size of the model, dataset, and overall compute environment
    will have on the loss of the model. These *Scaling Laws for Neural Language Models*
    *(9)* suggested that the optimal model size for a given compute environment was
    massive.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，基于GPT的模型使用**因果语言建模**来学习如何最好地完成文本。这意味着使用从左到右的完成学习标准，不断更新模型的可学习参数，直到文本完全准确。虽然2018年的首个GPT模型本身已经很有用，但真正的兴奋出现在几年后的两个阶段。首先，贾里德·凯普兰（Jared
    Kaplan）领导OpenAI的团队提出了一个新概念：利用受到他在物理学研究中的启发的公式，来估算模型、数据集和整体计算环境的规模对模型损失的影响。这些*神经语言模型的扩展规律*
    *(9)* 表明，对于给定的计算环境，最优的模型规模是非常庞大的。
- en: The original GPT model of 2018 was only 117 million parameters, and its second
    version, aptly named GPT-2, increased the model size by up to 10x. This increase
    in parameter size more than doubled the overall accuracy of the model. Encouraged
    by these results, and fuelled by Kaplan’s theoretical and empirical findings,
    OpenAI boldly increased the model parameter size by another 10x, giving us GPT-3.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年发布的原版GPT模型仅包含1.17亿个参数，而它的第二版，恰如其名的GPT-2，将模型规模提高了最多10倍。参数数量的增加使模型的整体准确度翻倍以上。在这些结果的鼓舞下，且受凯普兰（Kaplan）理论和实证研究成果的推动，OpenAI大胆地将模型参数大小再增加了10倍，最终推出了GPT-3。
- en: 'As the model increased in size, from 1.3 billion parameters to 13 billion,
    ultimately hitting 175 billion parameters, accuracy also took a huge leap! This
    result catalyzed the field of NLP, unleashing new use cases and a flurry of new
    work exploring and extending these impacts. Since then, new work has explored
    both larger (PaLM *(9)*) and smaller (Chinchilla *(10)*) models, with Chinchilla
    presenting an update to the scaling laws entirely. Yann LeCunn’s team at Meta
    has also presented smaller models that outperform the larger ones in specific
    areas, such as question-answering (Atlas *(9)*). Amazon has also presented two
    models that outperform GPT-3: the AlexaTM and MM-COT. Numerous teams have also
    undertaken efforts to produce open source versions of GPT-3, such as Hugging Face’s
    BLOOM, EleutherAI’S GPT-J, and Meta’s OPT.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型规模的增大，从13亿个参数到130亿，最终达到1750亿个参数，准确性也取得了巨大的飞跃！这一结果催生了自然语言处理（NLP）领域，释放了新的应用场景并引发了一波新的工作，探索并扩展了这些影响。从那时起，新的研究探索了更大（PaLM
    *(9)*)和更小（Chinchilla *(10)*)的模型，Chinchilla提出了对扩展规律的全新更新。Meta的Yann LeCunn团队也提出了一些小型模型，这些模型在特定领域超越了更大模型，例如问答（Atlas
    *(9)*）。亚马逊也提出了两个超越GPT-3的模型：AlexaTM和MM-COT。许多团队也在努力推出开源版本的GPT-3，例如Hugging Face的BLOOM、EleutherAI的GPT-J和Meta的OPT。
- en: The rest of this book is dedicated to discussing these models – where they come
    from, what they are good for, and especially how to train your own! While much
    excellent work has covered using these pretrained models in production through
    fine-tuning, such as Hugging Face’s own *Natural Language Processing with Transformers*
    (Tunstall et al., 2022), I continue to believe that pretraining your own foundation
    model is probably the most interesting computational intellectual exercise you
    can embark on today. I also believe it’s one of the most profitable. But more
    on that ahead!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的其余部分将专注于讨论这些模型——它们来自哪里、它们的用途，尤其是如何训练你自己的模型！虽然许多优秀的工作已经覆盖了如何通过微调使用这些预训练模型进行生产，例如Hugging
    Face自己的*自然语言处理与变换器*（Tunstall等，2022），但我依然相信，自己进行基础模型的预训练，可能是当今你能参与的最有趣的计算智能挑战。我还认为，这是最具盈利性的之一。但更多内容将在后面展开！
- en: 'Next, let’s learn about two key model components you’ll need to understand
    in detail: encoders and decoders.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们了解两个你需要详细理解的关键模型组件：编码器和解码器。
- en: Encoders and decoders
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码器和解码器
- en: 'Now, I’d like to briefly introduce you to two key topics that you’ll see in
    the discussion of transformer-based models: encoders and decoders. Let’s establish
    some basic intuition to help you understand what they are all about. An encoder
    is simply a computational graph (or neural network, function, or object depending
    on your background), which takes an input with a larger feature space and returns
    an object with a smaller feature space. We hope (and demonstrate computationally)
    that the encoder is able to learn what is most essential about the provided input
    data.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我想简要介绍两个关键的主题，你将在讨论基于变换器的模型时看到：编码器和解码器。我们先建立一些基本的直觉，帮助你理解它们的含义。编码器只是一个计算图（或神经网络、函数或对象，具体取决于你的背景），它接受一个具有较大特征空间的输入，并返回一个具有较小特征空间的对象。我们希望（并通过计算演示）编码器能够学习到输入数据中最重要的部分。
- en: 'Typically, in large language and vision models, the encoder itself is composed
    of a number of multi-head self-attention objects. This means that in transformer-based
    models, an encoder is usually a number of self-attention steps, learning what
    is most essential about the provided input data and passing this onto the downstream
    model. Let’s look at a quick visual:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在大型语言和视觉模型中，编码器本身由多个多头自注意力对象组成。这意味着在基于变换器的模型中，编码器通常是多个自注意力步骤，学习输入数据中最重要的部分，并将其传递给下游模型。我们来看一个简短的图示：
- en: "![Figure 1.1 – \uFEFFEncoders and decoders](img/B18942_01_01.jpg)"
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 – 编码器和解码器](img/B18942_01_01.jpg)'
- en: Figure 1.1 – Encoders and decoders
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 编码器和解码器
- en: Intuitively, as you can see in the preceding figure, the encoder starts with
    a larger input space and iteratively compresses this to a smaller latent space.
    In the case of classification, this is just a classification head with output
    allotted for each class. In the case of masked language modeling, encoders are
    stacked on top of each other to better predict tokens to replace the masks. This
    means the encoders output an embedding, or a numerical representation of that
    token, and after prediction, the tokenizer is reused to translate that embedding
    back into natural language.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，正如您在前面的图中看到的，编码器从较大的输入空间开始，并逐步将其压缩成较小的潜在空间。在分类的情况下，这只是一个分类头，为每个类别分配输出。在掩蔽语言建模的情况下，编码器堆叠在一起，更好地预测要替换的掩蔽标记。这意味着编码器输出的是一个嵌入，即该标记的数值表示，经过预测后，分词器将重新利用该嵌入将其转化回自然语言。
- en: One of the earliest large language models, BERT, is an encoder-only model. Most
    other BERT-based models, such as DeBERTa, DistiliBERT, RoBERTa, DeBERTa, and others
    in this family use encoder-only model architectures. Decoders operate exactly
    in reverse, starting with a compressed representation and iteratively recomposing
    that back into a larger feature space. Both encoders and decoders can be combined,
    as in the original Transformer, to solve text-to-text problems.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最早的“大型语言模型”之一，BERT是一个仅包含编码器的模型。大多数其他基于BERT的模型，如DeBERTa、DistiliBERT、RoBERTa、DeBERTa等，采用的是仅编码器的模型架构。解码器的操作正好相反，从压缩表示开始，逐步将其重新组合回较大的特征空间。编码器和解码器可以结合使用，如原始Transformer中所示，以解决文本到文本的问题。
- en: To make it easier, here’s a short table quickly summarizing the three types
    of self-attention blocks we’ve looked at, encoders, decoders, and their combination.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，下面是一个简短的表格，快速总结了我们所看过的三种自注意力模块：编码器、解码器及其组合。
- en: '| Size of inputs and outputs | Type of self-attention blocks | Machine learning
    tasks | Example models |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 输入和输出的大小 | 自注意力模块类型 | 机器学习任务 | 示例模型 |'
- en: '| Long to short | Encoder | Classification, any dense representation | BERT,
    DeBERTa, DistiliBERT, RoBERTa, XLM, AlBERT, CLIP, VL-BERT, Vision Transformer
    |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 长到短 | 编码器 | 分类，任何稠密表示 | BERT, DeBERTa, DistiliBERT, RoBERTa, XLM, AlBERT,
    CLIP, VL-BERT, Vision Transformer |'
- en: '| Short to long | Decoder | Generation, summarization, question-answering,
    any sparse representation | GPT, GPT-2, GPT-Neo, GPT-J, ChatGPT, GPT-4, BLOOM,
    OPT |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 短到长 | 解码器 | 生成，摘要，问答，任何稀疏表示 | GPT, GPT-2, GPT-Neo, GPT-J, ChatGPT, GPT-4,
    BLOOM, OPT |'
- en: '| Equal | Encoder-decoder | Machine translation, style translation | T5, BART,
    BigBird, FLAN-T5, Stable Diffusion |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 相等 | 编码器-解码器 | 机器翻译，风格转换 | T5, BART, BigBird, FLAN-T5, Stable Diffusion |'
- en: Table 1.3 – Encoders, decoders, and their combination
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.3 – 编码器、解码器及其组合
- en: Now that you have a better understanding of encoders, decoders, and the models
    they create, let’s close out the chapter with a quick recap of all the concepts
    you just learned about.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您对编码器、解码器及它们所创建的模型有了更深入的理解，让我们快速回顾一下您刚刚学习过的所有概念，结束这一章。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We’ve covered a lot in just this first chapter! Let’s quickly recap some of
    the top themes before moving on. First, we looked at the art of pretraining and
    fine-tuning, including a few key pretraining objects such as masked language and
    causal language modeling. We learned about the Transformer model architecture,
    including the core self-attention mechanism with its variant. We looked at state-of-the-art
    vision and language models, including spotlights on contrastive pretraining from
    natural language supervision, and scaling laws for neural language models. We
    learned about encoders, decoders, and their combination, which are useful throughout
    the vision and language domains today.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这一章中已经涵盖了很多内容！在继续之前，让我们快速回顾一些关键主题。首先，我们讨论了预训练和微调的技巧，包括一些关键的预训练对象，如掩蔽语言模型和因果语言建模。我们了解了Transformer模型架构，包括其变种的核心自注意力机制。我们探讨了最先进的视觉和语言模型，重点介绍了来自自然语言监督的对比预训练，以及神经语言模型的扩展规律。我们还了解了编码器、解码器及其组合，这些概念今天在视觉和语言领域中仍然非常有用。
- en: 'Now that you have a great conceptual and applied basis to understand pretraining
    foundation models, let’s look at preparing your dataset: part one.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经具备了理解预训练基础模型的良好概念性和应用基础，让我们来看看如何准备数据集：第一部分。
- en: References
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Please go through the following content for more information on a few topics
    covered in the chapter:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅以下内容，以获取更多有关本章中涵盖的某些主题的信息：
- en: '*The Bitter Lesson*, Rich Sutton, March 13, 2019: [http://www.incompleteideas.net/IncIdeas/BitterLesson.html](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*痛苦的教训*, Rich Sutton, 2019年3月13日: [http://www.incompleteideas.net/IncIdeas/BitterLesson.html](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)'
- en: 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019\. *BERT:
    Pre-training of Deep Bidirectional Transformers for Language Understanding*: [https://aclanthology.org/N19-1423/](https://aclanthology.org/N19-1423/).
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*, pages 4171–4186, Minneapolis, Minnesota. Association for Computational
    Linguistics.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova. 2019\. *BERT:
    深度双向转换器的预训练用于语言理解*: [https://aclanthology.org/N19-1423/](https://aclanthology.org/N19-1423/)。在
    *2019年北美计算语言学学会年会论文集：人类语言技术，第1卷（长文与短文）*，第4171-4186页，明尼阿波利斯，明尼苏达州。计算语言学学会。'
- en: Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan,
    Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry,
    Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger,
    Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel
    and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler,
    Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and
    Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya
    and Amodei, Dario. 2020\. *Language Models are Few-Shot Learners*. In *Advances
    in Neural Information Processing Systems, Volume 33*. Pages 1877-1901\. Curran
    Associates, Inc.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Brown, Tom 和 Mann, Benjamin 和 Ryder, Nick 和 Subbiah, Melanie 和 Kaplan, Jared
    D 和 Dhariwal, Prafulla 和 Neelakantan, Arvind 和 Shyam, Pranav 和 Sastry, Girish
    和 Askell, Amanda 和 Agarwal, Sandhini 和 Herbert-Voss, Ariel 和 Krueger, Gretchen
    和 Henighan, Tom 和 Child, Rewon 和 Ramesh, Aditya 和 Ziegler, Daniel 和 Wu, Jeffrey
    和 Winter, Clemens 和 Hesse, Chris 和 Chen, Mark 和 Sigler, Eric 和 Litwin, Mateusz
    和 Gray, Scott 和 Chess, Benjamin 和 Clark, Jack 和 Berner, Christopher 和 McCandlish,
    Sam 和 Radford, Alec 和 Sutskever, Ilya 和 Amodei, Dario. 2020\. *语言模型是少样本学习者*。在
    *神经信息处理系统进展，第33卷*。第1877-1901页。Curran Associates, Inc.
- en: '*AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT* *SCALE*:
    [https://arxiv.org/pdf/2010.11929v2.pdf](https://arxiv.org/pdf/2010.11929v2.pdf)'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*一张图片等于16x16个词：用于* *规模化图像识别的变换器*: [https://arxiv.org/pdf/2010.11929v2.pdf](https://arxiv.org/pdf/2010.11929v2.pdf)'
- en: '*AN ENSEMBLE OF SIMPLE CONVOLUTIONAL NEURAL NETWORK MODELS FOR MNIST DIGIT*
    *RECOGNITION*: [https://arxiv.org/pdf/2008.10400v2.pdf](https://arxiv.org/pdf/2008.10400v2.pdf)'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*一个简单卷积神经网络模型的集成用于 MNIST 数字* *识别*: [https://arxiv.org/pdf/2008.10400v2.pdf](https://arxiv.org/pdf/2008.10400v2.pdf)'
- en: '*Language Models are Few-Shot* *Learners*: [https://arxiv.org/pdf/2005.14165v4.pdf](https://arxiv.org/pdf/2005.14165v4.pdf
    )'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*语言模型是少样本* *学习者*: [https://arxiv.org/pdf/2005.14165v4.pdf](https://arxiv.org/pdf/2005.14165v4.pdf
    )'
- en: '*PaLM: Scaling Language Modeling with* *Pathways*: [https://arxiv.org/pdf/2204.02311v3.pdf](https://arxiv.org/pdf/2204.02311v3.pdf)'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*PaLM: 使用* *Pathways* 扩展语言建模： [https://arxiv.org/pdf/2204.02311v3.pdf](https://arxiv.org/pdf/2204.02311v3.pdf)'
- en: '*MOGRIFIER* *LSTM*: [https://arxiv.org/pdf/1909.01792v2.pdf](https://arxiv.org/pdf/1909.01792v2.pdf)'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*MOGRIFIER* *LSTM*: [https://arxiv.org/pdf/1909.01792v2.pdf](https://arxiv.org/pdf/1909.01792v2.pdf)'
- en: '*BERT: Pre-training of Deep Bidirectional Transformers for Language* *Understanding*:
    [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*BERT: 深度双向转换器的预训练用于语言* *理解*: [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)'
- en: '*Improving Language Understanding by Generative* *Pre-Training*: [https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*通过生成式* *预训练提升语言理解*: [https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)'
- en: '*ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN* *GENERATORS*:
    [https://arxiv.org/pdf/2003.10555.pdf](https://arxiv.org/pdf/2003.10555.pdf)'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*ELECTRA: 作为判别器而非* *生成器的预训练文本编码器*: [https://arxiv.org/pdf/2003.10555.pdf](https://arxiv.org/pdf/2003.10555.pdf)'
- en: '*Language (Technology) is Power: A Critical Survey of “Bias” in* *NLP*: [https://arxiv.org/pdf/2005.14050.pdf](https://arxiv.org/pdf/2005.14050.pdf)'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*语言（技术）是权力：NLP中“偏见”的批判性调查*: [https://arxiv.org/pdf/2005.14050.pdf](https://arxiv.org/pdf/2005.14050.pdf)'
- en: '*Scaling Laws for Neural Language* *Models*: [https://arxiv.org/pdf/2001.08361.pdf](https://arxiv.org/pdf/2001.08361.pdf)'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*神经语言模型的规模定律*: [https://arxiv.org/pdf/2001.08361.pdf](https://arxiv.org/pdf/2001.08361.pdf)'
- en: '*PaLM: Scaling Language Modeling with* *Pathways*: [https://arxiv.org/pdf/2204.02311.pdf](https://arxiv.org/pdf/2204.02311.pdf)'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*PaLM: 利用路径提升语言建模的规模*: [https://arxiv.org/pdf/2204.02311.pdf](https://arxiv.org/pdf/2204.02311.pdf)'
- en: '*Training Compute-Optimal Large Language* *Models*: [https://arxiv.org/pdf/2203.15556.pdf](https://arxiv.org/pdf/2203.15556.pdf)'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*训练计算优化的大型语言模型*: [https://arxiv.org/pdf/2203.15556.pdf](https://arxiv.org/pdf/2203.15556.pdf)'
- en: '*Atlas: Few-shot Learning with Retrieval Augmented Language* *Models*:[https://arxiv.org/pdf/2208.03299.pdf](
    https://arxiv.org/pdf/2208.03299.pdf)'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Atlas: 使用增强检索语言模型进行少样本学习*: [https://arxiv.org/pdf/2208.03299.pdf](https://arxiv.org/pdf/2208.03299.pdf)'
