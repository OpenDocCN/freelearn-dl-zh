- en: Image Recognition in IoT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网中的图像识别
- en: Many IoT applications, including smart homes, smart cities, and smart healthcare,
    will extensively use image recognition-based decision-making (such as facial recognition
    for a smart door or lock) in the future. **Machine learning** (**ML**) and **deep
    learning** (**DL**) algorithms are useful for image recognition and decision-making.
    Consequently, they are very promising for IoT applications. This chapter will
    cover hands-on DL-based image data processing for IoT applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，许多物联网应用，包括智能家居、智能城市和智能健康等，将广泛使用基于图像识别的决策制定（如智能门锁的面部识别）。**机器学习**（**ML**）和**深度学习**（**DL**）算法对于图像识别和决策制定非常有用，因此它们在物联网应用中具有很大的潜力。本章将涉及基于深度学习的物联网应用图像数据处理的实际操作。
- en: 'The first part of this chapter will briefly describe different IoT applications
    and their image detection-based decision-making. Furthermore, it will briefly
    discuss an IoT application and its image detection-based implementation in a real-world
    scenario. In the second part of the chapter, we shall present a hands-on image
    detection implementation of an application using a DL algorithm. In this chapter,
    we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分将简要描述不同的物联网应用及其基于图像检测的决策制定。此外，还将简要讨论一个物联网应用及其在实际场景中的基于图像检测的实现。在本章的第二部分，我们将介绍使用深度学习算法的图像检测应用的实际操作。本章将涵盖以下主题：
- en: IoT applications and image recognition
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物联网应用与图像识别
- en: 'Use case one: Image-based road fault detection'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例一：基于图像的道路故障检测
- en: 'Use case two: Image-based smart solid waste separation'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例二：基于图像的智能固体废物分离
- en: Implementing the use cases
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现用例
- en: Transfer learning for image recognition in IoT
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物联网中图像识别的迁移学习
- en: CNNs for image recognition in IoT applications
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物联网应用中的CNN图像识别
- en: Collecting data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集
- en: Data pre-processing
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Model training
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练
- en: Evaluating the models
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估
- en: IoT applications and image recognition
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网应用与图像识别
- en: The image recognition landscape in IoT applications is rapidly changing. Significant
    advances in mobile processing power, edge computing, and machine learning are
    paving the way for the widespread use of image recognition in many IoT applications.
    For example, omnipresent mobile devices (which are a key components in many IoT
    applications) equipped with high-resolution cameras facilitate the generation
    of images and videos by everyone, everywhere.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网应用中的图像识别领域正在迅速变化。移动处理能力、边缘计算和机器学习的重大进展正在为图像识别在许多物联网应用中的广泛使用铺平道路。例如，普及的移动设备（它们是许多物联网应用的关键组件）配备高分辨率摄像头，能够让每个人在任何地方生成图像和视频。
- en: 'Moreover, intelligent video cameras, such as IP cameras and Raspberry Pis with
    cameras, are used in many places, such as smart homes, campuses, and factories,
    for different applications. Many IoT applications—including smart cities, smart
    homes, smart health, smart education, smart factories, and smart agriculture—make
    decisions using image recognition/classification. As shown in the following diagram, these
    applications use one or more of the following image recognition services:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，智能视频摄像头，如IP摄像头和带摄像头的树莓派，广泛应用于许多场所，如智能家居、校园和工厂，用于不同的应用。许多物联网应用——包括智能城市、智能家居、智能健康、智能教育、智能工厂和智能农业——都通过图像识别/分类做出决策。如下面的图示所示，这些应用使用以下一种或多种图像识别服务：
- en: '![](img/c5027b9b-10a4-4fb5-9d03-75004e180697.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5027b9b-10a4-4fb5-9d03-75004e180697.png)'
- en: 'Let''s us discuss the previous image in detail:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细讨论前面提到的图像：
- en: '**People Identification**: Generally, secure and friendly access to home, office,
    and any other premises is a challenging task. The use of smart devices, including
    IoT solutions, can offer secure and friendly access to many premises. Let''s consider
    the example of office or home access. We use one or more keys access to our homes
    or offices. If we lose these keys, this could not only inconvenience us but put
    our security at risk if somebody else finds them. In this context, image recognition-based
    people identification can be used as a keyless access method for a smart home
    or office.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人员识别**：通常，家庭、办公室和其他场所的安全且友好的进入方式是一个具有挑战性的任务。使用包括物联网解决方案在内的智能设备可以为许多场所提供安全且友好的访问方式。以办公室或家庭的访问为例，我们通常使用一把或多把钥匙进入家庭或办公室。如果丢失了这些钥匙，除了给我们带来不便，还可能带来安全风险，特别是如果有人捡到它们。在这种情况下，基于图像识别的人员识别可以作为智能家居或办公室的无钥匙进入方式。'
- en: '**Object Identification**: IoT-based automated object identification is highly
    desirable in many domains, including driverless cars, smart cities, and smart
    factories. For example, smart city applications, such as smart vehicle license
    plate recognition and vehicle detection, as well as city-wide public asset monitoring,
    can use image recognition-based object detection services. Similarly, a smart
    factory can use the object detection service for inventory management.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**物体识别**：基于物联网的自动物体识别在许多领域中非常有价值，包括无人驾驶汽车、智能城市和智能工厂。例如，智能城市应用程序，如智能车辆牌照识别和车辆检测，以及全市范围的公共资产监控，可以使用基于图像识别的物体检测服务。类似地，智能工厂可以使用物体检测服务进行库存管理。'
- en: '**Facial Recognition**: The image processing-based facial detection and recognition
    landscape is changing so rapidly that it will be a commodity soon. Smartphones
    with biometrics will then be the norm. Smartphones and IoT-based facial recognition
    can be used in many applications, such as safety and security, and smart education.
    For example, in a smart class (education), a face recognition system can be used
    to identify the response to a lecture.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**面部识别**：基于图像处理的面部检测和识别领域发展迅速，未来它将成为一种商品。配备生物识别功能的智能手机将成为常态。智能手机和基于物联网的面部识别可以应用于许多领域，如安全、智能教育等。例如，在智能课堂（教育）中，可以使用面部识别系统来识别学生对讲座的反应。'
- en: '**Event Detection**: Symptoms of many human diseases (such as  hand foot mouth),
    animal diseases (such as, foot and mouth, and poultry diseases), and plant diseases
    are explicit and visible. These diseases can be digitally detected using IoT solutions
    integrated with DL-based image classification.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事件检测**：许多人类疾病（如手足口病）、动物疾病（如口蹄疫和家禽疾病）以及植物疾病的症状是显而易见的，可以通过物联网解决方案和基于深度学习的图像分类进行数字化检测。'
- en: Use case one – image-based automated fault detection
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例一 – 基于图像的自动故障检测
- en: Public assets (such as roads, public buildings, and tourist places) in a city
    are heterogeneous and distributed within the city. Most cities in the world face
    challenges in monitoring, fault detection, and reporting these assets. For example,
    in many UK cities, citizens often report faults, but the accuracy and efficiency
    of the reporting is an issue in many cases. In a smart city, these assets can
    be monitored, and their faults can be detected and reported through an IoT application.
    For example, a vehicle (such as a city council vehicle) attached with one or more
    sensors (such as a camera or a mic) can be used for the road fault monitoring
    and detection.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 城市中的公共资产（如道路、公共建筑和旅游景点）是异质的，并且分布在城市各处。世界上大多数城市在监控、故障检测和报告这些资产方面面临挑战。例如，在许多英国城市，市民经常报告故障，但在许多情况下，报告的准确性和效率都是一个问题。在智能城市中，这些资产可以通过物联网应用进行监控，故障也可以被检测并报告。例如，一辆附有一个或多个传感器（如摄像头或麦克风）的车辆（例如市政车辆）可以用于道路故障监测和检测。
- en: Roads are important assets in a city, and they have many faults. Potholes, bumps,
    and road roughness are some of the most frustrating hazards and anomalies experienced
    by commuters and vehicles. Importantly, vehicles may frequently face suspension
    problems, steering misalignment, and punctures, which could also lead to accidents.
    The cost of road-fault-related damages is significant. For example, pothole-related
    damage alone cost UK drivers £1.7 billion a year. An IoT application with the
    support of an appropriate DL algorithm can be used to automatically detect these
    faults and report them appropriately. This reduces the number of road-fault-related
    damage in a cost-effective way.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 道路是城市中重要的资产，且存在许多故障。坑洼、颠簸和道路粗糙是通勤者和车辆常遇到的一些最令人沮丧的危害和异常情况。更重要的是，车辆可能经常面临悬挂问题、转向不对准和爆胎，这些问题也可能导致事故。与道路故障相关的损坏成本相当高。例如，仅坑洼造成的损害就让英国司机每年损失17亿英镑。支持适当深度学习算法的物联网应用可以自动检测这些故障并进行适当报告，从而以具有成本效益的方式减少与道路故障相关的损害数量。
- en: Implementing use case one
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施用例一
- en: 'As shown in the following diagram, the implementation of the use case consists
    of three main elements:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，实施此用例包括三个主要元素：
- en: '![](img/c7204812-461d-4169-8ae5-af23821b9b21.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7204812-461d-4169-8ae5-af23821b9b21.png)'
- en: 'Let us learn about the components in detail:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解这些组件：
- en: '**Sensors and data gathering**:The selection of sensors for data gathering
    depends on the assets and fault types. If we use a smartphone as the edge-computing
    device, its camera can be used for sensing and data gathering about road faults.
    On the contrary, if we use Raspberry Pi as the edge-computing device, we need
    to use an external camera, as there is no built-in camera within the Raspberry
    Pi. The preceding diagram shows the Raspberry Pi and camera used for the use case
    implementation. We used a Raspberry Pi 3 model B+ with 1 GB RAM and a 5-megapixel
    sensor with an Omnivision OV5647 sensor in a fixed-focus lens. The sampling or
    photographic rate of the camera will depend on the vehicle''s speed and the availability
    of a road''s faults. For example, if the smartphone camera or the camera installed
    on the vehicle can capture one picture a second, the phone or  Raspberry Pi will
    be able to detect the faults within two seconds if the speed of the vehicle is
    40 km/h or less. Once the image is sensed and captured, it will be sent to the
    detection method.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传感器和数据收集**：数据收集传感器的选择取决于资产和故障类型。如果我们使用智能手机作为边缘计算设备，其摄像头可以用于感知和收集道路故障数据。相反，如果使用树莓派作为边缘计算设备，则需要使用外部摄像头，因为树莓派没有内置摄像头。前面的图示展示了树莓派和摄像头如何用于用例实现。我们使用了一台树莓派
    3 型 B+，配备 1GB RAM 和一款 500 万像素的 Omnivision OV5647 传感器，采用定焦镜头。相机的采样或拍照速率将取决于车辆的速度和道路故障的可用性。例如，如果智能手机摄像头或安装在车辆上的摄像头每秒钟能拍摄一张照片，当车辆速度为
    40 公里/小时或以下时，手机或树莓派将在两秒内检测到故障。一旦图像被感知并捕捉到，它将被发送到检测方法。'
- en: '**Fault detection and reporting**:In this phase, the edge-computing device
    will be installed with one app. The installed app in a smartphone or Raspberry
    Pi will be loaded with pre-trained fault detection and a classification model.
    Once the vehicle’s smartphone or Raspberry Pi camera takes a picture (following
    a sampling rate), these models will detect and classify a potential fault and
    report to the application server (local council).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**故障检测与报告**：在此阶段，边缘计算设备将安装一个应用程序。安装在智能手机或树莓派上的应用程序将加载预训练的故障检测和分类模型。一旦车辆的智能手机或树莓派摄像头拍摄到一张照片（按照采样率），这些模型将检测并分类潜在的故障，并向应用服务器（本地议会）报告。'
- en: '**Council''s server and Fault Detection Model**:The council''s server is responsible
    for the following:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**议会服务器和故障检测模型**：议会的服务器负责以下内容：'
- en: Learning the model for fault detection and classification using reference datasets
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用参考数据集学习故障检测和分类模型
- en: Disseminating and updating the models for the edge-computing device
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传播和更新边缘计算设备的模型
- en: Receiving and storing the fault data
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接收和存储故障数据
- en: Image-based model learning and validation of road's fault detection is at the
    heart of the implementation. The second part (covered in the sections starting
    from *Transfer learning for image recognition in IoT*) of the chapter will describe
    the implementation of the DL-based anomaly detection of the previous use case. All
    the necessary codes are available in the chapter's code folder.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图像的模型学习和道路故障检测的验证是实现的核心。章节的第二部分（从*物联网中图像识别的迁移学习*开始的部分）将描述前述用例中基于深度学习的异常检测实现。所有必要的代码都可以在章节的代码文件夹中找到。
- en: Use case two – image-based smart solid waste separation
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例二 – 基于图像的智能固体废物分拣
- en: Solid waste is a global challenge. The management of solid waste is expensive,
    and improper waste management is also seriously impacting the global economy,
    public health, and the environment. Generally, solid waste, such as plastic, glass
    bottles, and paper, are recyclable, and they need an effective recycling method
    to become economically and environmentally beneficial. However, in most countries,
    the existing recycling processes are done manually. In addition, citizens or consumers
    often become confused about the recycling method.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 固体废物是全球面临的挑战。固体废物的管理成本高昂，不当的废物管理也在严重影响全球经济、公共健康和环境。一般来说，塑料、玻璃瓶和纸张等固体废物是可回收的，它们需要有效的回收方法，才能在经济和环境上带来益处。然而，在大多数国家，现有的回收过程仍是手工操作。此外，市民或消费者往往会对回收方法感到困惑。
- en: In this context, IoT with the support of machine learning and deep learning,
    especially image-based object recognition, can identify the type of waste and
    help sort it accordingly without any human intervention.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，物联网（IoT）借助机器学习和深度学习，尤其是基于图像的物体识别，能够识别废物类型并帮助将其进行分类，无需人工干预。
- en: Implementing use case two
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现用例二
- en: 'The implementation of image-based smart solid waste separation includes two
    key components:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图像的智能固体废物分离的实现包括两个关键组件：
- en: A bin with an individual chamber with a controllable lid for each type of solid
    waste
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个带有独立舱室、每种类型固体废物都有可控盖子的垃圾箱
- en: An IoT infrastructure with a DL model for image recognition
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有深度学习模型的物联网基础设施，用于图像识别
- en: 'The first component of the implementation is not within the scope of this book,
    and we are considering the component as available for this implementation. As
    shown in the following diagram, the IoT implementation of the use case consists
    of two main elements:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的第一个组件不在本书的范围内，我们假设该组件已经可用。如以下图所示，用例的物联网实现由两个主要元素组成：
- en: '![](img/58f00fef-94d2-4c5c-beff-e05f95948f48.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58f00fef-94d2-4c5c-beff-e05f95948f48.png)'
- en: '**Sensors and data gathering**: Selection of sensors for data gathering depends
    on the types of solid waste and their features. For example, many glass and plastic
    bottles are very similar in color and appearance. However, their weights are generally
    distinctly different. For the use case, we are considering two sensors:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传感器和数据收集**：选择传感器进行数据收集取决于固体废物的类型及其特性。例如，许多玻璃瓶和塑料瓶在颜色和外观上非常相似。然而，它们的重量通常差异显著。对于该用例，我们考虑使用两种传感器：'
- en: One or more cameras to capture an image of trash when it enters into a bin through
    the entry point
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个或多个摄像头，用于捕捉垃圾进入垃圾箱的图像
- en: A weight sensor to get the weight of the trash
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个重量传感器，用于测量垃圾的重量
- en: We use Raspberry Pi as the computing platform. The use case was tested using
    a Raspberry Pi 3 model B+ with 1 GB RAM and a 5 megapixel sensor with an Omnivision
    OV5647 sensor in a fixed-focus lens. Once the image and weight are sensed and
    captured, they are sent to the sorting method.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用树莓派作为计算平台。该用例使用了1 GB内存的树莓派3型号B+，并配备了5百万像素的Omnivision OV5647传感器和固定焦距镜头进行测试。一旦图像和重量被感知并捕获，它们将被发送到分类方法。
- en: '**Trash detection and sorting**: This is the key element of the implementation.
    The Raspberry Pi will be loaded with a pretrained trash detection and sorting
    model using DL. Once the detection algorithm detects trash and sorts it, it will
    actuate the control system to open the appropriate lid and move it into the bin.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**垃圾检测与分类**：这是实现的关键元素。树莓派将加载一个经过预训练的垃圾检测与分类模型，采用深度学习技术。一旦检测算法识别出垃圾并进行分类，它将启动控制系统打开适当的盖子，并将垃圾移入垃圾箱。'
- en: The use case scenario is focusing on waste management in urban public areas,
    including parks, tourist attractions, landscaping, and other recreational areas.
    Generally, citizens and/or visitors in these areas individually dispose of their
    waste. Importantly, they dispose of items in small numbers, from single items
    to just a few items.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该用例场景集中于城市公共区域的废物管理，包括公园、旅游景点、绿化带和其他休闲区域。通常，这些区域的市民和/或游客会单独处理他们的废物。值得注意的是，他们丢弃的废物数量较少，从单个物品到几个物品不等。
- en: All of the following sections will describe the implementation of the DL-based
    image recognition needed for the aforementioned use cases. All the necessary codes
    are available in the chapter's code folder.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下各节将描述所需的基于深度学习的图像识别实现，用于上述用例。所有必要的代码可以在本章的代码文件夹中找到。
- en: Transfer learning for image recognition in IoT
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网中的图像识别迁移学习
- en: Generally, transfer learning means transferring pre-trained machine learning
    model representations to another problem. In recent years, this is becoming a
    popular means of applying DL models to a problem, especially in image processing
    and recognition, as it enables training a DL model with comparatively little data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，迁移学习是指将预训练的机器学习模型表示迁移到另一个问题中。近年来，这种方法已成为应用深度学习模型解决问题的一种流行手段，尤其是在图像处理和识别中，因为它能够用相对较少的数据训练深度学习模型。
- en: 'The following diagram shows two models:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了两个模型：
- en: An architecture for a standard DL model (a)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准深度学习模型架构（a）
- en: 'An architecture for a transfer-learning DL model (b):'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习深度学习模型架构（b）：
- en: '![](img/adc41d97-4ee5-461a-8bca-ef6bbabcea35.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/adc41d97-4ee5-461a-8bca-ef6bbabcea35.png)'
- en: As shown in the figure of an architecture for a standard DL model, a fully trained
    neural net takes input values in an initial layer and then sequentially feeds
    this information forward with necessary transformation until the second-to-last
    layer (which is also known as the **bottleneck layer**) has constructed a high-level
    representation of the input that can more easily be transformed into a final output.
    The complete training of the model involves the optimization of weight and bias
    terms used in each connection (labeled in blue). In large and heterogeneous datasets,
    the number of these weight and bias terms could be in the millions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如标准深度学习模型架构图所示，一个完全训练的神经网络在初始层接收输入值，然后将这些信息依次向前传递并进行必要的转换，直到倒数第二层（也称为 **瓶颈层**）构建了输入的高级表示，该表示可以更容易地转化为最终输出。模型的完整训练包括对每个连接（标记为蓝色）的权重和偏置项进行优化。在大型和异构数据集中，这些权重和偏置项的数量可能达到数百万。
- en: In transfer learning, we can use the early and middle layers and only re-train
    the latter layers. One popular approach to transfer learning is to reuse the pre-trained
    weights for the whole network other than the last layer and relearn the weights
    of the last layer or classification part by retraining the network using the new
    dataset. As shown in the diagram of an architecture for a transfer-learning DL
    model, we reused the orange connections and retrained the network using the new
    dataset to learn the last layer’s green connections.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习中，我们可以使用前面和中间层，只重新训练后面的层。迁移学习的一种流行方法是复用整个网络中除最后一层外的预训练权重，并通过使用新数据集重新训练网络来重新学习最后一层或分类部分的权重。如迁移学习深度学习（DL）模型架构图所示，我们复用了橙色连接，并通过使用新数据集重新训练网络以学习最后一层的绿色连接。
- en: 'Many pre-trained DL models, including the Inception-v3 and MobileNets models,
    are available to be used for transfer learning. The Inception-v3 model, which
    was trained for the ImageNet *Large Visual Recognition Challenge*, classifies
    images into 1,000 classes, such as *Zebra*, *Dalmatian*, and *Dishwasher*. Inception-v3 consists
    of two parts:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 许多预训练的深度学习模型，包括 Inception-v3 和 MobileNets 模型，已经可以用于迁移学习。Inception-v3 模型是为 **ImageNet
    大规模视觉识别挑战赛**训练的，它将图像分类为1,000个类别，如 *斑马*、*达尔马提亚犬* 和 *洗碗机*。Inception-v3 由两个部分组成：
- en: A feature extraction part with a convolutional neural network, which extracts
    features from the input
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个使用卷积神经网络（CNN）进行特征提取的部分，它从输入中提取特征
- en: A classification part with fully connected and softmax layers, which classifies
    the input data based on the features identified in part one
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个分类部分，包含全连接层和 softmax 层，它基于第一部分识别的特征对输入数据进行分类
- en: If we want to use Inception-v3, we can reuse the feature extraction part and
    re-train the classification part with our dataset.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想使用 Inception-v3，我们可以复用特征提取部分，并使用我们的数据集重新训练分类部分。
- en: 'Transfer learning offers two benefits:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习提供了两个好处：
- en: Training on new data is faster.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在新数据上的训练速度更快。
- en: The ability to solve a problem with less training data rather than learning
    from scratch.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决问题的能力是在 **较少的训练数据** 上进行训练，而不是从头开始学习。
- en: These features of transfer learning are especially useful for the implementation
    of DL models in IoT's resource-constrained edge devices, as we do not need to
    train the resource-hungry feature extraction part. Thus, the model can be trained
    using less computational resources and time.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的这些特性对物联网（IoT）资源受限的边缘设备中深度学习模型的实现尤为重要，因为我们无需训练资源消耗大的特征提取部分。因此，模型可以使用更少的计算资源和时间进行训练。
- en: CNNs for image recognition in IoT applications
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网应用中的图像识别 CNN
- en: A **Convolutional Neural Network** (**CNN**) has different implementations. **AlexNet**
    is one such implementation, and it won the ImageNet Challenge: ILSVRC 2012\. Since
    then, CNNs have become omnipresent in computer vision and image detection and
    classification. Until April 2017, the general trend was to make deeper and more
    complicated networks to achieve higher accuracy. However, these deeper and complex
    networks offered improved accuracy but did not always make the networks more efficient,
    particularly in terms of size and speed. In many real-world applications, especially
    in IoT applications, such as a self-driving car and patient monitoring, recognition
    tasks need to be accomplished in a timely fashion on a resource-constrained (processing,
    memory) platform.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNN**）有多种实现方式。**AlexNet** 就是其中一种实现，并且在 ImageNet 挑战赛：ILSVRC 2012
    中获得了胜利。从那时起，CNN 已在计算机视觉和图像检测、分类中无处不在。直到2017年4月，普遍趋势是构建更深、更复杂的网络以实现更高的准确率。然而，这些更深、更复杂的网络提高了准确性，但并不总是使网络变得更加高效，特别是在网络的大小和速度方面。在许多实际应用中，尤其是在物联网应用（如自动驾驶汽车和病人监控）中，识别任务需要在资源受限（处理能力、内存）平台上及时完成。'
- en: In this context, MobileNet V1 was introduced in April 2017\. This version of
    Mobilenet was an improvement on its second version (MobileNetV2) in April 2018. **Mobilenets** and
    their variants are the efficient CNN DL model's IoT applications, especially for
    image recognition-based IoT applications. In the following paragraphs, we present
    a brief overview of MobileNets.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种背景下，MobileNet V1 于2017年4月推出。这个版本的 MobileNet 相较于2018年4月推出的第二版（MobileNetV2）有所改进。**Mobilenets**及其变体是高效
    CNN 深度学习模型在物联网（IoT）应用中的重要应用，尤其是在基于图像识别的物联网应用中。在接下来的段落中，我们将简要介绍 MobileNets。
- en: MobileNets are the implementations of most popular and widely used DL models,
    namely CNNs. They are especially designed for resource-constrained mobile devices
    to support classification, detection, and prediction. Personal mobile devices,
    including smartphones, wearable devices, and smartwatches, installed with DL models
    improve user experience, offering any time, anywhere access, with the additional
    benefits of security, privacy, and energy consumption. Importantly, new emerging
    applications in mobile devices will need ever-more efficient neural networks to
    interact with the real world in real time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNets 是最流行和最广泛使用的深度学习（DL）模型的实现，即卷积神经网络（CNNs）。它们特别为资源受限的移动设备设计，以支持分类、检测和预测功能。安装了深度学习模型的个人移动设备，包括智能手机、可穿戴设备和智能手表，能够提升用户体验，提供随时随地的访问，并带来安全、隐私和节能的附加益处。值得注意的是，移动设备中的新兴应用将需要更高效的神经网络，以便实时与现实世界进行交互。
- en: 'The following diagram shows how the standard convolutional filters (figure
    a) are replaced by two layers in Mobilenet V1\. It uses a depthwise convolution
    (figure b) and a pointwise convolution (figure c) to build a depthwise separable
    filter:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示意图展示了标准卷积滤波器（图 a）如何在 MobileNet V1 中被两层所替代。它使用深度卷积（图 b）和点卷积（图 c）来构建深度可分离滤波器：
- en: '![](img/2294acd6-1260-41c1-80fc-d142b3d6cb0a.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2294acd6-1260-41c1-80fc-d142b3d6cb0a.jpg)'
- en: 'The main motivation of MobileNet V1 is that convolutional layers are expensive
    to compute, and they can be replaced by so-called **depthwise separable convolutions**.
    In MobileNet V1, the depthwise convolution process uses a single filter to every
    input channel, and the pointwise convolution then uses a 1 x 1 convolution process
    to the outputs of the earlier depthwise convolution. As shown in the diagram of
    a standard convolution filter, a standard convolution both filters and combines
    inputs into a new set of outputs in one step. Unlike standard CNNs, the depthwise
    separable convolution (factorized) in MobileNets splits this into two layers (as
    shown in the diagram of Mobilenet V1): a layer for filtering and a separate layer
    for combining.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet V1 的主要动机是卷积层计算代价高，它们可以被所谓的**深度可分离卷积**所替代。在 MobileNet V1 中，深度可分离卷积过程使用单个滤波器作用于每个输入通道，然后点卷积使用1x1的卷积过程处理早期深度卷积的输出。如标准卷积滤波器的示意图所示，标准卷积滤波器在一步中既进行滤波又合并输入，生成新的输出。与标准
    CNN 不同，MobileNet 中的深度可分离卷积（分解形式）将这一过程分为两个层次（如 MobileNet V1 的示意图所示）：一个用于滤波，另一个用于合并。
- en: 'The following diagram presents the factorized architecture of Mobilenet V1\.
    This factorization drastically reduces computation and model size as the model
    needs to calculate a significantly smaller number of parameters. For example,
    MobileNet V1 needs to calculate 4.2 million parameters, whereas a full convolution
    network needs to calculate 29.3 million parameters:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了MobileNet V1的因式分解架构。这种因式分解大大减少了计算量和模型大小，因为模型需要计算的参数数量显著减少。例如，MobileNet
    V1需要计算420万个参数，而全卷积网络需要计算2930万个参数：
- en: '![](img/6aac716a-4376-421f-b2ee-b1bf9f081ed6.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6aac716a-4376-421f-b2ee-b1bf9f081ed6.png)'
- en: MobileNet V2 is an updated and significantly improved version of MobileNet V1\.
    It has greatly improved and pushes existing mobile visual recognition, including
    classification, detection, and semantic segmentation. Like MobileNet V1, the MobileNet
    V2 was released as part of the TensorFlow-Slim Image Classification Library. If
    needed, you can explore this in Google's Colaboratory. In addition, MobileNet
    V2 is available as modules on TF-Hub, and pre-trained checkpoints or saved models
    can be found at [https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet) [and
    can be used as transfer learning.](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNet V2是MobileNet V1的更新版，并且在多个方面有了显著改进。它极大地提高了现有的移动视觉识别，包括分类、检测和语义分割。与MobileNet
    V1一样，MobileNet V2作为TensorFlow-Slim图像分类库的一部分发布。如果需要，你可以在Google的Colaboratory中进行探索。此外，MobileNet
    V2作为模块可在TF-Hub上使用，预训练检查点或保存的模型可以在[https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)找到，[并且可以用作迁移学习。](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet)
- en: 'The following diagram presents a simple architecture of MobileNet V2.  MobileNet
    V2  has been developed as an extension of MobileNet V1\. It uses depth-wise separable
    convolution as efficient building blocks. In addition, MobileNet V2 includes two
    new features in the architecture. One is the linear bottlenecks between the layers,
    and the other one is shortcut connections between the bottlenecks:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了MobileNet V2的简化架构。MobileNet V2作为MobileNet V1的扩展进行了开发。它使用深度可分离卷积作为高效的构建模块。此外，MobileNet
    V2在架构中引入了两个新特性。一个是层之间的线性瓶颈，另一个是瓶颈之间的捷径连接：
- en: '![](img/b80688e0-0d3f-465f-b2ff-976064b84f48.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b80688e0-0d3f-465f-b2ff-976064b84f48.png)'
- en: Collecting data for use case one
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集用例一的数据
- en: 'We can collect data using a smartphone camera or a Raspberry Pi camera and
    prepare the dataset by ourselves, or download existing images from the internet
    (that is, via Google, Bing, and so on) and prepare the dataset. Alternatively,
    we can use an existing open source dataset. For use case one, we have used a combination
    of both. We have downloaded an existing dataset on pothole images (one of the
    most common road faults) from and updated the dataset with more images from Google
    images. The open source dataset (`PotDataset`) for pothole recognition was published
    by Cranfield University, UK. The dataset includes images of pothole objects and
    non-pothole objects, including manholes, pavements, road markings, and shadows.
    The images were manually annotated and organized into the following folders:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用智能手机相机或树莓派相机来收集数据，并自己准备数据集，或者从互联网上下载现有的图像（如通过Google、Bing等）并准备数据集。或者，我们可以使用现有的开源数据集。对于用例一，我们使用了两者的结合。我们从互联网上下载了现有的坑洞图像数据集（这是最常见的道路故障之一），并通过Google图像更新了数据集。该开源数据集（`PotDataset`）由英国克兰菲尔德大学发布，用于坑洞识别。数据集包含坑洞物体和非坑洞物体的图像，包括井盖、人行道、道路标记和阴影。这些图像经过手工标注，并整理到以下文件夹中：
- en: Manhole
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 井盖
- en: Pavement
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人行道
- en: Pothole
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坑洞
- en: Road markings
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 道路标记
- en: Shadow
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阴影
- en: Exploring the dataset from use case one
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索用例一中的数据集
- en: 'It is essential to explore the dataset before applying DL algorithms to the
    data. For the exploration, we can run `image_explorer.py` on the dataset as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在将深度学习算法应用于数据之前，探索数据集是非常必要的。在探索过程中，我们可以通过运行`image_explorer.py`来探索数据集，如下所示：
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following diagram presents a snapshot of the data exploration process:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了数据探索过程的快照：
- en: '![](img/07f715ce-8c41-4106-95fe-475b550a1361.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07f715ce-8c41-4106-95fe-475b550a1361.jpg)'
- en: 'As shown in the diagram of data exploration, the differences between pothole
    and non-pothole objects are not always obvious if we are using only the smartphone
    camera. A combination of an IR and smartphone camera can improve the situation.
    In addition, we found that the pothole images we have used here might not be enough
    to cover a wide range of potholes such as the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如数据探索图所示，如果仅使用智能手机相机，坑洞和非坑洞物体之间的区别并不总是显而易见。结合红外线相机和智能手机相机可以改善这种情况。此外，我们发现我们使用的坑洞图像可能不足以覆盖各种类型的坑洞，例如以下内容：
- en: Many images in the used dataset show that the potholes are already maintained/fixed.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已用数据集中的许多图片显示坑洞已经修复/维护。
- en: There are a few images of a large-sized pothole in the used dataset.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已用数据集中有几张大尺寸坑洞的图片。
- en: 'In this context, we decided to update the pothole images dataset by collecting
    more images from the internet. Next, we briefly discuss the data collection process:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，我们决定通过从互联网收集更多的图像来更新坑洞图像数据集。接下来，我们简要讨论数据收集过程：
- en: '**Search**:Use any browser (we used Chrome), go to Google, and search for *pothole
    images* in Google Images. Your search window will look like the following screenshot:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**搜索**：使用任意浏览器（我们使用了Chrome），访问Google，并在Google图片中搜索*pothole images*。你的搜索窗口将显示如下截图：'
- en: You can select copyright-free images by clicking on *Tools* and changing the
    usage rights to *Labeled for reuse with modification.*
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击*工具*并将使用权限更改为*可修改再利用的标注*来选择无版权限制的图片。
- en: '![](img/e95a92bb-cdea-48e0-a81d-acbe3b9b806b.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e95a92bb-cdea-48e0-a81d-acbe3b9b806b.png)'
- en: '**Gathering Images URLs**: This step is to use a few lines of JavaScript code
    to gather the image URLs. The gathered URLs can be used in Python to download
    the images. As shown in the following screenshot, select the JavaScript console
    (assuming you use the Chrome web browser, but you can use Firefox as well) by
    clicking **View** | **Developer **| **JavaScript Console** (in macOS) and customize
    and control **Google Chrome** | **More tools** | **Developer tools** (in Windows
    OS):'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收集图片URLs**：此步骤是使用几行JavaScript代码收集图片的URLs。收集到的URLs可以在Python中用于下载图片。如以下截图所示，选择JavaScript控制台（假设你使用Chrome浏览器，但你也可以使用Firefox），通过点击**查看**|**开发者**|**JavaScript控制台**（在macOS上）或者**Google
    Chrome** | **更多工具** | **开发者工具**（在Windows操作系统上）：'
- en: '![](img/8dc8d211-0d45-458c-a81c-fa9da19a824d.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8dc8d211-0d45-458c-a81c-fa9da19a824d.png)'
- en: 'Once you have selected the JavaScript console, you will see a browser window
    such as the following screenshot, and this will enable you to execute JavaScript
    in a REPL-like manner:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你选择了JavaScript控制台，你将看到类似于以下截图的浏览器窗口，这将允许你像REPL一样执行JavaScript：
- en: '![](img/8dd135fa-09e2-46a8-9a84-986e8a490d14.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8dd135fa-09e2-46a8-9a84-986e8a490d14.png)'
- en: 'Now do the following in order:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在按以下顺序操作：
- en: 'Scroll the page and go down until you have found all useful images (note: please
    use images that are not subject to copyright) for your dataset. After that, you
    need to collect the URLs for the selected images.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动页面，直到找到所有有用的图片（注意：请使用没有版权限制的图片）作为数据集。之后，你需要收集已选图片的URLs。
- en: 'Now move to the JavaScript console and then copy and paste the following JavaScript
    codes into the console:'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在进入JavaScript控制台，并将以下JavaScript代码复制并粘贴到控制台中：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding line of code will pull the jQuery JavaScript library.  Now you
    can use a CSS selector to collect a list of URLs using the following lines of
    code:'
  id: totrans-107
  prefs:
  - PREF_OL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前一行代码将加载jQuery JavaScript库。现在，你可以使用CSS选择器通过以下代码行收集网址列表：
- en: '[PRE2]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Finally, write the URLs to a file (one per line) using the following lines
    of code:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用以下代码行将网址写入文件（每行一个网址）：
- en: '[PRE3]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once you execute the preceding lines of code, you will have a file named `imageurls.txt`
    in your default download directory.  If you want to download them into a specific
    folder, then write `hiddenComponents.download = 'your fooler/imageurls.txt` instead
    of `hiddenComponents.download = 'imageurls.txt'` in the preceding code.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码行后，你将在默认下载目录中获得一个名为`imageurls.txt`的文件。如果你想将其下载到特定文件夹，请在上述代码中将`hiddenComponents.download
    = 'imageurls.txt'`改为`hiddenComponents.download = 'your fooler/imageurls.txt'`。
- en: '**Downloading the images**: Now you are ready to download the running the images
    `download_images.py` (available in code folder of the chapter) in the previously
    downloaded `imageurls.txt`:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载图片**：现在你可以准备下载运行图片`download_images.py`（在章节的代码文件夹中可找到），并使用先前下载的`imageurls.txt`：'
- en: '[PRE4]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Exploration**: Once we have downloaded the images, we need to explore them
    in order delete the irrelevant images. We can do this through a bit of manual
    inspection. After this, we need to resize and convert them into grayscale images
    to match the previously downloaded dataset:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据探索**：下载图像后，我们需要对其进行探索，以删除无关的图像。我们可以通过手动检查来完成这一过程。之后，我们需要调整图像大小，并将其转换为灰度图像，以匹配之前下载的数据集：'
- en: '![](img/46a249b1-8e27-4169-a8c4-50d62acc6e8d.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46a249b1-8e27-4169-a8c4-50d62acc6e8d.png)'
- en: The preceding screenshot shows the folder structure of the pothole and non-pothole
    images datasets.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图显示了坑洞图像和非坑洞图像数据集的文件夹结构。
- en: Collecting data for use case two
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集用例二的数据
- en: 'As is the case with use case one, we can collect data through digital cameras
    or use an existing open source or a combination of both. We are using an existing
    and open source dataset for the implementation of the sorting algorithm. The dataset
    was collected from urban environments of the USA . As solid waste types may vary
    by country, it is better to update the dataset based on the country the use case
    will be used for. The dataset consists of six types of solid wastes: glass, paper,
    cardboard, plastic, metal, and trash. The dataset consists of 2,527 images, and
    they were annotated and organized into the following folders, as shown in the 
    following screenshot:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与用例一类似，我们可以通过数码相机收集数据，或使用现有的开源数据集，或两者结合使用。我们正在使用现有的开源数据集来实现排序算法。该数据集是从美国的城市环境中收集的。由于固体垃圾种类可能因国家而异，因此最好根据实际使用用例的国家来更新数据集。该数据集包含六种类型的固体垃圾：玻璃、纸张、纸板、塑料、金属和垃圾。数据集共包含2,527张图像，这些图像已被注释并按以下文件夹结构整理，如下图所示：
- en: '![](img/688c0162-ec3e-4ecb-97e1-60c59f450432.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/688c0162-ec3e-4ecb-97e1-60c59f450432.png)'
- en: Data exploration of use case two
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例二的数据探索
- en: 'The following presents a snapshot of the data exploration for use case two.
    As we can see, glass and plastic images could be confusing to the sorting algorithm.
    In this context, weight sensor data can be useful for fixing this issue:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用例二的数据探索快照。如图所示，玻璃和塑料图像可能会混淆排序算法。在这种情况下，重量传感器数据可以帮助解决这个问题：
- en: '![](img/1efe9aad-74f3-4cfe-879a-da94f0bf12da.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1efe9aad-74f3-4cfe-879a-da94f0bf12da.png)'
- en: Data pre-processing
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'This is an essential step for a DL pipeline. The existing datasets on pothole
    images and the solid waste images used in the use cases are pre-processed and
    are ready to be used for training, validation, and testing. As shown in the following
    diagram, both the original and modified (additional images downloaded for the
    pothole class) are organized as sub-folders, each named after one of the five
    categories and containing only images from that category. There are a few issues
    to be noted during the training image set preparation:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这是深度学习管道中的一个关键步骤。现有的坑洞图像数据集以及用于用例中的固体垃圾图像数据集已被预处理，并准备好用于训练、验证和测试。如下面的图示所示，原始图像和修改后的图像（为坑洞类下载的附加图像）都被整理成子文件夹，每个文件夹以五个类别之一命名，并且只包含该类别的图像。在准备训练图像集时需要注意以下几个问题：
- en: '**Data size**:We need to collect at least a hundred images for each class to
    train a model that works well. The more we can gather, the better the accuracy
    of the trained model is likely to be. Each of the five categories in the used
    dataset has more than 1,000 sample images. We also made sure that the images are
    a good representation of what our application will actually face in real implementation.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据大小**：我们需要为每个类别收集至少100张图像，以训练一个表现良好的模型。我们收集的数据越多，训练模型的准确率可能就越高。所使用数据集中的每个类别都有超过1,000张样本图像。我们还确保这些图像能够很好地代表我们应用在实际实施中将会遇到的情况。'
- en: '**Data heterogeneity**:Data collected for training should be heterogeneous.
    For example, images about potholes need to be taken in as wide a variety of situations
    as we can, at different times, and with different devices.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据异质性**：用于训练的数据应该是异质的。例如，关于坑洞的图片需要尽可能在各种不同的环境、不同的时间和使用不同设备的情况下拍摄。'
- en: Models training
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: 'As we mentioned earlier, we are using transfer learning that does not require
    training from scratch; retraining of the models with a new dataset will sufficiently
    work in many cases. We retrained two popular architectures or models of CNN, namely
    Incentive V3 and Mobilenet V1, on a desktop computer, which is replicating the
    city council’s server. In both models, it took less than an hour to retrain the
    models, which is an advantage of the transfer learning approach. We need to understand
    the list of key arguments before running the `retrain.py`file, which is in the
    code folder. If we type in our Terminal (in Linux or macOS) or Command Prompt
    (Windows) `python retrain.py -h`, we shall see a window like the following screenshot
    with additional information (that is, an overview of each argument). The compulsory
    argument is the image directory, and it is one of the dataset directories shown
    in the preceding figures on the folder view of the datasets:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用了迁移学习，这不需要从头开始训练；使用新数据集重新训练模型在许多情况下已经足够有效。我们在一台桌面计算机上重新训练了两种流行的 CNN
    架构或模型，即 Incentive V3 和 Mobilenet V1，这台计算机模拟了市议会的服务器。在这两种模型中，重新训练模型的时间都不到一小时，这是迁移学习方法的优势。在运行
    `retrain.py` 文件之前，我们需要了解关键参数的列表，该文件位于代码文件夹中。如果我们在终端（Linux 或 macOS）或命令提示符（Windows）中输入
    `python retrain.py -h`，将会看到一个类似于下图的窗口，其中包含额外的信息（即每个参数的概述）。强制性参数是图像目录，它是之前图中所示的数据集目录之一：
- en: '![](img/716599bf-9f65-4cd9-82e6-b46defa25ad4.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/716599bf-9f65-4cd9-82e6-b46defa25ad4.png)'
- en: 'In the following, we are presenting two examples of command: one to retrain
    the Incentive model V3 and the other to retain Mobilenet V1 on the modified dataset
    (dataset-modified). To retrain Incentive V3, we did not pass the architecture
    argument value as it is the default architecture included in `retrain.py`. For
    the rest of the arguments, including data split ratio among training, validation,
    and test, we used the default values. In this use case, we are using the split
    rule of the data that put 80% of the images into the main training set, keeping
    10% separate for validation during training, and the final 10% of the data as
    a testing set. The testing set is to test the real-world classification performance
    of the classifier:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 下面，我们提供两个命令示例：一个用于重新训练 Incentive V3 模型，另一个用于在修改后的数据集（dataset-modified）上重新训练
    Mobilenet V1。为了重新训练 Incentive V3，我们没有传递架构参数值，因为它是 `retrain.py` 中包含的默认架构。对于其余的参数，包括训练、验证和测试之间的数据划分比例，我们使用了默认值。在这个使用案例中，我们使用了数据拆分规则，将
    80% 的图像放入主训练集，保留 10% 作为训练过程中的验证集，剩余的 10% 作为测试集。测试集用于测试分类器的真实世界分类性能：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To run the training and validation of the Mobilenet V1 model, use the following
    command:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 Mobilenet V1 模型的训练和验证，请使用以下命令：
- en: '[PRE6]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once we run the preceding commands, it will generate the retrain models (`retrained_graph.pb`),
    labels text (`retrained_labels.txt`) in the given directory and summary directory
    consists of train and validation summary information of the models. The summary
    information `(--summaries_dir`  argument with default value `retrain_logs`) can
    be used by TensorBoard to visualize different aspects of the models, including
    the networks, and their performance graphs. If we type the following command in
    the terminal or Command Prompt, it will run TensorBoard:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行上述命令，它将生成重新训练的模型（`retrained_graph.pb`）、标签文本（`retrained_labels.txt`）以及包含训练和验证摘要信息的目录。`(--summaries_dir`
    参数，默认值为 `retrain_logs`)，TensorBoard 可以使用这些摘要信息来可视化模型的不同方面，包括网络结构和性能图表。如果我们在终端或命令提示符中输入以下命令，它将启动
    TensorBoard：
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once TensorBoard is running, navigate your web browser to `localhost:6006` to
    view the TensorBoard and view the network of the corresponding model. The following
    diagrams **(a)** and **(b)** show the network for Incentive V3 and Mobilenet V1
    respectively. The diagram demonstrates the complexity of Incentive V3 compared
    to Mobilenet V1:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 TensorBoard 启动，打开你的网页浏览器并访问`localhost:6006`，即可查看 TensorBoard 并查看相应模型的网络结构。下图
    **(a)** 和 **(b)** 分别展示了 Incentive V3 和 Mobilenet V1 的网络结构。该图展示了与 Mobilenet V1
    相比，Incentive V3 的复杂性：
- en: '![](img/22d6ea4b-2faa-4ca9-8936-08c84bc6711f.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22d6ea4b-2faa-4ca9-8936-08c84bc6711f.png)'
- en: 'In the second use case, we have retrained only the Mobilenet V1 on the solid
    waste dataset. You can retrain the model as mentioned by only providing an image
    or dataset directory as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种使用情况下，我们只在固体废物数据集上重新训练了 Mobilenet V1。你可以通过仅提供图像或数据集目录来重新训练模型，如下所示：
- en: '[PRE8]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Evaluating models
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Firstly, we have identified the size of the retrain models. As shown in the
    following screenshot, Mobilenet V1 requires only 17.1 MB (for both use cases),
    which is than one-fifth of Incentive V3 (92.3 MB), and this model can be easily
    deployed in resource-constrained IoT devices, including Raspberry Pi or smartphones.
    Secondly, we have evaluated the performance of the models. Two levels of performance
    evaluation have been done for the use cases: (i) dataset-wide evaluation or testing
    has been done during the retraining phase on the desktop PC platform/server, and
    (ii) an individual image or sample (real-life image) was tested or evaluated in
    the Raspberry Pi 3 environment:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们已经确定了重新训练模型的大小。如下面的截图所示，Mobilenet V1仅需要17.1 MB（适用于两种用例），是Incentive V3（92.3
    MB）大小的五分之一，并且该模型可以轻松部署到资源受限的物联网设备中，包括树莓派或智能手机。其次，我们评估了模型的性能。针对这些用例，进行了两级性能评估：(i)
    在重新训练阶段，使用桌面PC平台/服务器对整个数据集进行评估或测试，(ii) 在树莓派3环境中对单个图像或样本（现实生活中的图像）进行了测试或评估：
- en: '![](img/89912302-26f7-4677-b93e-6f01168445dd.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89912302-26f7-4677-b93e-6f01168445dd.png)'
- en: Model performance (use case one)
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能（用例一）
- en: All the evaluation performances of use case one are presented in the following
    screenshots. The following six screenshots present the training, validation, and
    testing performances of Incentive V3 and Mobilenet V1 models on the two sets of
    data. The first three screenshots present the results generated in the terminal
    after retraining the models, and the last three screenshots are generated from
    the TensorBoard.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 用例一的所有评估性能都展示在以下截图中。接下来的六张截图展示了Incentive V3和Mobilenet V1模型在两组数据上的训练、验证和测试性能。前三张截图展示了重新训练模型后终端生成的结果，后三张截图则是从TensorBoard生成的结果。
- en: 'The following screenshot presents the evaluation results of Incentive V3 on
    the original dataset:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在原始数据集上对Incentive V3的评估结果：
- en: '![](img/f968d524-a60a-4c52-8817-65c4cf04c4f5.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f968d524-a60a-4c52-8817-65c4cf04c4f5.png)'
- en: 'The following screenshot presents the evaluation results of Incentive V3 on
    the modified dataset:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在修改后的数据集上对Incentive V3的评估结果：
- en: '![](img/5eaee1f3-7570-44ed-bd87-1d28aa93f7f7.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5eaee1f3-7570-44ed-bd87-1d28aa93f7f7.png)'
- en: 'The following screenshot presents the evaluation results of Mobilenet V1 on
    the original dataset:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在原始数据集上对Mobilenet V1的评估结果：
- en: '![](img/cfb61326-9ce7-4ded-97a7-8b0b1c20e223.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cfb61326-9ce7-4ded-97a7-8b0b1c20e223.png)'
- en: 'The following screenshot presents the evaluation results of  Mobilenet V1 on
    the modified dataset:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在修改后的数据集上对Mobilenet V1的评估结果：
- en: '![](img/eaa8056d-ed33-4e1e-9b9e-e57e812d82f1.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eaa8056d-ed33-4e1e-9b9e-e57e812d82f1.png)'
- en: 'The following screenshot presents the evaluation results of Incentive V3 on
    the original dataset generated by TensorBoard:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在原始数据集上对Incentive V3的评估结果，这些结果是通过TensorBoard生成的：
- en: '![](img/943f8ef3-d31f-4d57-87e3-a52137a240c8.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/943f8ef3-d31f-4d57-87e3-a52137a240c8.png)'
- en: 'The following screenshot presents the evaluation results of  Mobilenet V1 on
    the original dataset generated by TensorBoard:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在原始数据集上对Mobilenet V1的评估结果，这些结果是通过TensorBoard生成的：
- en: '![](img/2469605e-54d7-4ff8-8437-fa1e9a518650.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2469605e-54d7-4ff8-8437-fa1e9a518650.png)'
- en: From all the previous model performance screenshots, it is clear that both training
    and validation accuracies are well above 90%, which is enough for fault detection.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前所有的模型性能截图中可以看出，训练和验证的准确率均远高于90%，这对于故障检测已经足够。
- en: The following diagrams show the classification or object detection performances
    on individual samples. For these, we have used two different sets of classification
    code (available in the chapter's code folder).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了对单个样本的分类或目标检测性能。我们使用了两组不同的分类代码（代码在本章的代码文件夹中）。
- en: 'The first screenshot is showing the snapshot of running the classifier for
    Mobilenet V1 on two samples. As we can see from all results, test or evaluation
    accuracy is well above 94%, and with such accuracy, the DL models (CNNs) have
    the potential to detect objects, including potholes, manholes, and other objects
    on the road. However, object detection time on the Pi 3 was in the range of three
    to five seconds, which needs to be improved if we want to use them in real-time
    detection and actuation. In addition, results show that models trained on the
    modified dataset have a good chance to provide high detection or testing accuracy
    in a real environment (shown in the preceding screenshots), especially in detecting
    potholes, as this class of data has been improved by adding diverse images from
    the googled images:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张截图展示了运行Mobilenet V1分类器对两个样本进行分类的快照。从所有结果来看，测试或评估准确率均高于94%，而且凭借这样的准确率，深度学习模型（CNN）具有检测物体的潜力，包括坑洞、井盖以及路上的其他物体。然而，在Pi
    3上进行物体检测的时间大约为三到五秒，如果我们想要在实时检测和执行中使用这些模型，还需要进一步提高检测速度。此外，结果显示，使用修改过的数据集训练的模型在真实环境中有很大机会提供较高的检测或测试准确率（如前面截图所示），尤其是在坑洞检测方面，因为这一类数据通过加入来自谷歌图片的多样化图像得到了改善：
- en: '![](img/8256fc92-b359-45b0-8b38-4f76fc35cd19.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8256fc92-b359-45b0-8b38-4f76fc35cd19.png)'
- en: 'The following screenshot presents the evaluation results of pothole detection
    with the Incentive V3 model trained on the original dataset (Pi 3 B+):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了使用Incentive V3模型在原始数据集（Pi 3 B+）上训练的坑洞检测评估结果：
- en: '![](img/4e62eb2e-fec0-4466-bfb5-c1296717e078.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e62eb2e-fec0-4466-bfb5-c1296717e078.png)'
- en: 'The following diagram presents the evaluation results of manhole detection
    with the Incentive V3 model trained on the original dataset (Pi 3 B+):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了使用Incentive V3模型在原始数据集（Pi 3 B+）上训练的井盖检测评估结果：
- en: '![](img/09716d1d-c32b-400c-a862-160475bb473c.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09716d1d-c32b-400c-a862-160475bb473c.png)'
- en: 'The following diagram presents the evaluation results of  pothole detection
    with the Mobilenet V1 model trained on the original dataset (Pi 3 B+):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了使用Mobilenet V1模型在原始数据集（Pi 3 B+）上训练的坑洞检测评估结果：
- en: '![](img/072508d6-ea2a-4eef-9d33-a287aaec601d.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/072508d6-ea2a-4eef-9d33-a287aaec601d.png)'
- en: 'The following diagram presents the evaluation results of  manhole detection
    with the Mobilenet V1 model trained on the original dataset (Pi 3 B+):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了使用Mobilenet V1模型在原始数据集（Pi 3 B+）上训练的井盖检测评估结果：
- en: '![](img/d0460b70-2622-48f5-aa3e-8fee182a53f4.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0460b70-2622-48f5-aa3e-8fee182a53f4.png)'
- en: Model performance (use case two)
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能（用例二）
- en: 'All the evaluation performances of use case two are presented in the following
    screenshots. For this use case, we are presenting only the results for Mobilenet
    V1 .The following diagrams present the training, validation, and testing performances
    of the Mobilenet V1 models on the two datasets. As we can see from the following
    screenshot, the test accuracy is not that high (77.5%) but good enough for solid
    waste detection and sorting:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 用例二的所有评估性能展示在下面的截图中。对于这个用例，我们只展示Mobilenet V1的结果。以下图表展示了在两个数据集上，Mobilenet V1模型的训练、验证和测试性能。从以下截图可以看出，测试准确率并不是特别高（77.5%），但对于固体废物检测和分类来说，已经足够好：
- en: '![](img/81309be7-f043-4486-ae9f-2243d26318db.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/81309be7-f043-4486-ae9f-2243d26318db.png)'
- en: 'The following screenshot presents the evaluation results of Mobilenet V1 on
    the dataset generated by TensorBoard:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了在TensorBoard生成的数据集上，Mobilenet V1的评估结果：
- en: '![](img/3a8271f6-78a8-4302-8e08-c7c4f63c25ad.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a8271f6-78a8-4302-8e08-c7c4f63c25ad.png)'
- en: 'The following three screenshots show the classification or object (solid waste)
    detection performances on individual samples. The first screenshot presents the
    evaluation results of glass detection:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下三张截图展示了对单个样本的分类或物体（固体废物）检测性能。第一张截图展示了玻璃检测的评估结果：
- en: '![](img/784a2d8a-4dcb-4d74-88d4-725e03dee6ae.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/784a2d8a-4dcb-4d74-88d4-725e03dee6ae.png)'
- en: 'The following screenshot presents the evaluation results of plastic detection:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了塑料检测的评估结果：
- en: '![](img/d037aa38-1eb6-40be-b709-4377b1bb1ee7.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d037aa38-1eb6-40be-b709-4377b1bb1ee7.png)'
- en: 'The following screenshot presents the evaluation results of metal detection
    using Mobilenet V1:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了使用Mobilenet V1进行金属检测的评估结果：
- en: '![](img/0a1d55f8-9114-4ba5-a4c8-39bd2a1cff87.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a1d55f8-9114-4ba5-a4c8-39bd2a1cff87.png)'
- en: Summary
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In the first part of this chapter, we briefly described different IoT applications
    and their image detection-based decision-making. In addition, we briefly discussed
    two use cases: image detection-based road fault detection, and image detection-based
    solid waste sorting. The first application can detect potholes on the road using
    a smartphone camera or a Raspberry Pi camera. The second application detects different
    types of solid waste and sorts them according to smart recycling.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们简要描述了不同的物联网应用及其基于图像检测的决策制定。此外，我们还简要讨论了两个使用案例：基于图像检测的道路故障检测和基于图像检测的固体废物分类。第一个应用可以使用智能手机摄像头或树莓派摄像头检测道路上的坑洼。第二个应用则检测不同类型的固体废物，并根据智能回收进行分类。
- en: In the second part of the chapter, we briefly discussed transfer learning with
    a few example networks, and examined its usefulness in resource-constrained IoT
    applications. In addition, we discussed the rationale behind selecting a CNN,
    including two popular implementations, namely Inception V3 and Mobilenet V1\.
    The rest of the chapter described all the necessary components of the DL pipeline
    for the Inception V3 and Mobilenet V1 models.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们简要讨论了迁移学习和几个示例网络，并研究了其在资源受限的物联网应用中的有效性。此外，我们还讨论了选择CNN的理由，包括两种流行的实现方式，即Inception
    V3和Mobilenet V1。接下来的内容将介绍Inception V3和Mobilenet V1模型所需的所有深度学习流水线组件。
- en: In many IoT applications, image recognition alone may not be enough for object
    and/or subject detection. In this context, sometimes, audio/speech/voice recognition
    can be useful. [Chapter 3](ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml), Audio/Speech/Voice
    Recognition in IoT, will present DL-based speech/voice data analysis and recognition
    in IoT applications.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多物联网应用中，仅凭图像识别可能不足以进行物体和/或主体检测。在这种情况下，有时音频/语音/声音识别可能会有所帮助。[第3章](ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml)，物联网中的音频/语音/声音识别，将介绍基于深度学习的语音/声音数据分析和物联网应用中的识别。
- en: References
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Smart patrolling: An efficient road surface monitoring using smartphone sensors
    and crowdsourcing*, Gurdit Singh, Divya Bansal, Sanjeev Sofat, Naveen Aggarwal, *Pervasive
    and Mobile Computing*, volume 40, 2017, pages 71-88'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*智能巡逻：使用智能手机传感器和众包进行高效的道路表面监测*，Gurdit Singh，Divya Bansal，Sanjeev Sofat，Naveen
    Aggarwal，*普适计算与移动计算*，第40卷，2017年，第71-88页'
- en: '*Road Damage Detection Using Deep Neural Networks with Images Captured Through
    a Smartphone*, Hiroya Maeda, Yoshihide Sekimoto, Toshikazu Seto, Takehiro Kashiyama,
    Hiroshi Omata, arXiv:1801.09454'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用智能手机拍摄图像的深度神经网络道路损伤检测*，Hiroya Maeda，Yoshihide Sekimoto，Toshikazu Seto，Takehiro
    Kashiyama，Hiroshi Omata，arXiv:1801.09454'
- en: '*Potholes cost UK drivers £1.7 billion a year: Here''s how to claim if you
    car is damaged*, Luke John Smith: [https://www.express.co.uk/life-style/cars/938333/pothole-damage-cost-how-to-claim-UK](https://www.express.co.uk/life-style/cars/938333/pothole-damage-cost-how-to-claim-UK)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*坑洼让英国司机每年损失17亿英镑：如果您的车受损，如何索赔*，Luke John Smith：[https://www.express.co.uk/life-style/cars/938333/pothole-damage-cost-how-to-claim-UK](https://www.express.co.uk/life-style/cars/938333/pothole-damage-cost-how-to-claim-UK)'
- en: '*What a Waste: A Global Review of Solid Waste Management*, D Hoornweg and P
    Bhada-Tata, World Bank, Washington, DC, USA, 2012'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*What a Waste: 全球固体废物管理回顾*，D Hoornweg 和 P Bhada-Tata，世界银行，华盛顿特区，美国，2012年'
- en: '*Efficient Convolutional Neural Networks for Mobile Vision Applica**tions*,
    Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias
    Weyand, Marco Andreetto, Hartwig Adam, *MobileNets: *arXiv:1704.04861'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*高效卷积神经网络在移动视觉应用中的应用*，Andrew G Howard，Menglong Zhu，Bo Chen，Dmitry Kalenichenko，Weijun
    Wang，Tobias Weyand，Marco Andreetto，Hartwig Adam，*MobileNets: arXiv:1704.04861*'
- en: '*Imagenet classification with deep convolutional neural networks*, A Krizhevsky,
    I Sutskever, G E Hinton, in *Advances in Neural Information Processing Systems*,
    pages 1,097–1,105, 2012\. 1, 6.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于深度卷积神经网络的Imagenet分类*，A Krizhevsky，I Sutskever，G E Hinton，发表于*神经信息处理系统进展*，第1,097-1,105页，2012年。1，6。'
- en: '*MobileNetV2: Inverted Residuals and Linear Bottlenecks*, Mark Sandler, Andrew
    Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, arXiv:1801.04381.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MobileNetV2：倒残差和线性瓶颈*，Mark Sandler，Andrew Howard，Menglong Zhu，Andrey Zhmoginov，Liang-Chieh
    Chen，arXiv:1801.04381。'
- en: Pothole dataset: [https://cord.cranfield.ac.uk/articles/PotDataset/5999699 ](https://cord.cranfield.ac.uk/articles/PotDataset/5999699)
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坑洼数据集：[https://cord.cranfield.ac.uk/articles/PotDataset/5999699](https://cord.cranfield.ac.uk/articles/PotDataset/5999699)
- en: Trashnet: [https://github.com/garythung/trashnet](https://github.com/garythung/trashnet)
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Trashnet: [https://github.com/garythung/trashnet](https://github.com/garythung/trashnet)'
