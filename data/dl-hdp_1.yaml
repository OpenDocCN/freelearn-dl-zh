- en: Chapter 1. Introduction to Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章 深度学习简介
- en: '|   | *"By far the greatest danger of Artificial Intelligence is that people
    conclude too early that they understand it."* |   |'
  id: totrans-1
  prefs: []
  type: TYPE_TB
  zh: '|   | *“迄今为止，人工智能最大的危险在于人们过早地认为自己理解了它。”* |   |'
- en: '|   | --*Eliezer Yudkowsky* |'
  id: totrans-2
  prefs: []
  type: TYPE_TB
  zh: '|   | --*埃利泽·尤德科夫斯基* |'
- en: Ever thought, why it is often difficult to beat the computer in chess, even
    for the best players of the game? How Facebook is able to recognize your face
    amid hundreds of millions of photos? How can your mobile phone recognize your
    voice, and redirect the call to the correct person, from hundreds of contacts
    listed?
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 是否曾想过，为什么即使是游戏中最优秀的棋手，也常常很难战胜计算机？Facebook是如何在数亿张照片中识别你的面孔的？你的手机是如何识别你的声音，并将电话转接到正确的人那里，尽管通讯录中有成百上千个联系人？
- en: The primary goal of this book is to deal with many of those queries, and to
    provide detailed solutions to the readers. This book can be used for a wide range
    of reasons by a variety of readers, however, we wrote the book with two main target
    audiences in mind. One of the primary target audiences is undergraduate or graduate
    university students learning about deep learning and Artificial Intelligence;
    the second group of readers are the software engineers who already have a knowledge
    of big data, deep learning, and statistical modeling, but want to rapidly gain
    knowledge of how deep learning can be used for big data and vice versa.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的主要目标是解决许多相关问题，并为读者提供详细的解决方案。本书适用于多种不同的读者和用途，然而，我们在撰写本书时主要针对两类受众。一类是学习深度学习和人工智能的本科生或研究生；另一类是已经具备大数据、深度学习和统计建模知识的软体工程师，但他们希望迅速了解如何将深度学习应用于大数据，反之亦然。
- en: 'This chapter will mainly try to set a foundation for the readers by providing
    the basic concepts, terminologies, characteristics, and the major challenges of
    deep learning. The chapter will also put forward the classification of different
    deep network algorithms, which have been widely used by researchers over the last
    decade. The following are the main topics that this chapter will cover:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将主要通过提供深度学习的基本概念、术语、特性以及主要挑战，为读者奠定基础。本章还将提出深度网络算法的分类，这些算法在过去十年里被研究人员广泛使用。本章将涵盖以下主要内容：
- en: Getting started with deep learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始学习深度学习
- en: Deep learning terminologies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习术语
- en: 'Deep learning: A revolution in Artificial Intelligence'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习：人工智能的革命
- en: Classification of deep learning networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习网络的分类
- en: Ever since the dawn of civilization, people have always dreamt of building artificial
    machines or robots which can behave and work exactly like human beings. From the
    Greek mythological characters to the ancient Hindu epics, there are numerous such
    examples, which clearly suggest people's interest and inclination towards creating
    and having an artificial life.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 自人类文明的曙光以来，人们一直梦想着创造能够像人类一样工作和行为的人工机器或机器人。从希腊神话人物到古代印度史诗中，都有许多类似的例子，明显表明了人们对创造和拥有人工生命的兴趣和倾向。
- en: During the initial computer generations, people had always wondered if the computer
    could ever become as intelligent as a human being! Going forward, even in medical
    science, the need of automated machines has become indispensable and almost unavoidable.
    With this need and constant research in the same field, **Artificial Intelligence**
    (**AI**) has turned out to be a flourishing technology with various applications
    in several domains, such as image processing, video processing, and many other
    diagnosis tools in medical science too.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机的初期发展阶段，人们一直在想，计算机是否有可能像人类一样智能！随着时间的推移，即使在医学科学领域，自动化机器的需求也变得不可或缺，几乎是无法避免的。随着这种需求的不断增加以及相关领域的持续研究，**人工智能**（**AI**）已经成为一项蓬勃发展的技术，并在多个领域得到了广泛应用，例如图像处理、视频处理以及医学科学中的许多诊断工具。
- en: 'Although there are many problems that are resolved by AI systems on a daily
    basis, nobody knows the specific rules for how an AI system is programmed! A few
    of the intuitive problems are as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人工智能系统每天解决着许多问题，但没有人知道人工智能系统是如何被编程的！以下是一些直观的问题：
- en: Google search, which does a really good job of understanding what you type or
    speak
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google搜索，能够非常好地理解你输入或说出的内容
- en: As mentioned earlier, Facebook is also somewhat good at recognizing your face,
    and hence, understanding your interests
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，Facebook 在识别你的面孔方面也颇有成效，因此也能了解你的兴趣。
- en: Moreover, with the integration of various other fields, for example, probability,
    linear algebra, statistics, machine learning, deep learning, and so on, AI has
    already gained a huge amount of popularity in the research field over the course
    of time.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着概率、线性代数、统计学、机器学习、深度学习等各个领域的融合，人工智能在研究领域中已经获得了巨大的关注和流行。
- en: One of the key reasons for the early success of AI could be that it basically
    dealt with fundamental problems for which the computer did not require a vast
    amount of knowledge. For example, in 1997, IBM's Deep Blue chess-playing system
    was able to defeat the world champion Garry Kasparov [1]. Although this kind of
    achievement at that time can be considered significant, it was definitely not
    a burdensome task to train the computer with only the limited number of rules involved
    in chess! Training a system with a fixed and limited number of rules is termed
    as *hard-coded knowledge* of the computer. Many Artificial Intelligence projects
    have undergone this hard-coded knowledge about the various aspects of the world
    in many traditional languages. As time progresses, this hard-coded knowledge does
    not seem to work with systems dealing with huge amounts of data. Moreover, the
    number of rules that the data was following also kept changing in a frequent manner.
    Therefore, most of the projects following that system failed to stand up to the
    height of expectation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能早期成功的一个关键原因可能是，它主要处理的是计算机无需大量知识即可解决的基本问题。例如，1997年，IBM的深蓝象棋系统能够击败世界冠军加里·卡斯帕罗夫[1]。尽管当时这种成就可以算得上是重要的，但训练计算机仅用象棋的有限规则，绝对不算是一项繁重的任务！用固定且有限的规则训练系统被称为计算机的*硬编码知识*。许多人工智能项目都经过了这一硬编码知识，涵盖了许多传统语言中关于世界各方面的内容。随着时间的推移，这种硬编码知识似乎无法应对处理大量数据的系统。此外，数据遵循的规则数量也频繁变化。因此，大多数基于这种系统的项目未能达到预期的高度。
- en: 'The setbacks faced by this hard-coded knowledge implied that those artificial
    intelligence systems needed some way of generalizing patterns and rules from the
    supplied raw data, without the need for external spoon-feeding. The proficiency
    of a system to do so is termed as *machine learning*. There are various successful
    machine learning implementations which we use in our daily life. A few of the
    most common and important implementations are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这一硬编码知识的挫折表明，人工智能系统需要某种方式，从提供的原始数据中泛化出模式和规则，而不需要外部的强制输入。系统做到这一点的能力被称为*机器学习*。我们日常生活中有许多成功的机器学习应用。以下是一些最常见和最重要的应用：
- en: '**Spam detection**: Given an e-mail in your inbox, the model can detect whether
    to put that e-mail in spam or in the inbox folder. A common naive Bayes model
    can distinguish between such e-mails.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**垃圾邮件检测**：给定你收件箱中的一封电子邮件，模型可以检测是否将该电子邮件归类为垃圾邮件或收件箱邮件。一种常见的朴素贝叶斯模型可以区分这类邮件。'
- en: '**Credit card fraud detection**: A model that can detect whether a number of
    transactions performed at a specific time interval are carried out by the original
    customer or not.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信用卡欺诈检测**：一个模型可以检测在特定时间间隔内进行的一系列交易是否由原始客户执行。'
- en: One of the most popular machine learning models, given by Mor-Yosef et al in
    1990, used logistic regression, which could recommend whether caesarean delivery
    was needed for the patient or not!
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1990年，Mor-Yosef等人提出了最受欢迎的机器学习模型之一，该模型使用了逻辑回归，可以推荐是否需要为患者进行剖腹产！
- en: There are many such models which have been implemented with the help of machine
    learning techniques.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多这样的模型是通过机器学习技术实现的。
- en: '![Introduction to Deep Learning](img/image_01_001.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习介绍](img/image_01_001.jpg)'
- en: 'Figure 1.1: The figure shows the example of different types of representation.
    Let''s say we want to train the machine to detect some empty spaces in between
    the jelly beans. In the image on the right side, we have sparse jelly beans, and
    it would be easier for the AI system to determine the empty parts. However, in
    the image on the left side, we have extremely compact jelly beans, and hence,
    it will be an extremely difficult task for the machine to find the empty spaces.
    Images sourced from USC-SIPI image database'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.1：该图展示了不同类型表示法的示例。假设我们想训练机器检测果冻豆之间的空隙。在右侧的图像中，我们看到果冻豆分布稀疏，AI 系统更容易确定空白部分。然而，在左侧的图像中，果冻豆分布极为紧密，因此，机器要找到空隙将是一个极为困难的任务。图像来源于
    USC-SIPI 图像数据库。
- en: A large portion of performance of the machine learning systems depends on the
    data fed to the system. This is called *representation* of the data. All the information
    related to the representation is called the *feature* of the data. For example,
    if logistic regression is used to detect a brain tumor in a patient, the AI system
    will not try to diagnose the patient directly! Rather, the concerned doctor will
    provide the necessary input to the systems according to the common symptoms of
    that patient. The AI system will then match those inputs with the already received
    past inputs which were used to train the system.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统的性能很大一部分取决于输入给系统的数据。这被称为数据的*表示*。与表示相关的所有信息都被称为数据的*特征*。例如，如果使用逻辑回归来检测患者的脑肿瘤，AI
    系统并不会直接诊断患者！相反，相关的医生会根据患者的常见症状提供必要的输入给系统。AI 系统随后会将这些输入与已接收到的过去输入进行匹配，这些输入曾被用来训练该系统。
- en: Based on the predictive analysis of the system, it will provide its decision
    regarding the disease. Although logistic regression can learn and decide based
    on the features given, it cannot influence or modify the way features are defined.
    Logistic regression is a type of regression model where the dependent variable
    has a limited number of possible values based on the independent variable, unlike
    linear regression. So, for example, if that model was provided with a caesarean
    patient's report instead of the brain tumor patient's report, it would surely
    fail to predict the correct outcome, as the given features would never match with
    the trained data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 基于系统的预测分析，它会给出关于疾病的决策。尽管逻辑回归可以根据给定的特征进行学习并做出决策，但它无法影响或修改特征的定义方式。逻辑回归是一种回归模型，其中因变量的可能值有限，且根据自变量的不同而有所变化，与线性回归不同。所以，例如，如果该模型被提供的是剖腹产患者的报告，而不是脑肿瘤患者的报告，它肯定无法预测正确的结果，因为给定的特征永远不会与训练数据匹配。
- en: These dependencies of the machine learning systems on the representation of
    the data are not really unknown to us! In fact, most of our computer theory performs
    better based on how the data are represented. For example, the quality of a database
    is considered based on how the schema is designed. The execution of any database
    query, even on a thousand or a million lines of data, becomes extremely fast if
    the table is indexed properly. Therefore, the dependency of the data representation
    of the AI systems should not surprise us.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统对数据表示的依赖并不是什么我们不知道的事情！事实上，我们大多数的计算机理论在数据表示的方式上表现得更好。例如，数据库的质量通常是根据其架构设计来评估的。即使在处理成千上万行数据时，如果表格被正确索引，任何数据库查询的执行速度也会变得极为迅速。因此，AI
    系统对数据表示的依赖性不应让我们感到惊讶。
- en: There are many such examples in daily life too, where the representation of
    the data decides our efficiency. To locate a person amidst 20 people is obviously
    easier than to locate the same person in a crowd of 500 people. A visual representation
    of two different types of data representation is shown in the preceding *Figure
    1.1*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 日常生活中也有很多类似的例子，其中数据的表示方式决定了我们的效率。比如在 20 个人中定位某个特定的人显然比在 500 人的拥挤人群中定位同一个人要容易得多。前面的*图
    1.1*展示了两种不同类型数据表示法的可视化示例。
- en: Therefore, if the AI systems are fed with the appropriate featured data, even
    the hardest problems could be resolved. However, collecting and feeding the desired
    data in the correct way to the system has been a serious impediment for the computer
    programmer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果 AI 系统输入适当特征的数据，即使是最困难的问题也能够得到解决。然而，以正确的方式收集并输入数据一直是计算机程序员面临的严重障碍。
- en: There can be numerous real-time scenarios where extracting the features could
    be a cumbersome task. Therefore, the way the data are represented decides the
    prime factors in the intelligence of the system.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实时场景中，提取特征可能是一项繁琐的任务。因此，数据的表示方式决定了系统智能的主要因素。
- en: Note
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Finding cats amidst a group of humans and cats can be extremely complicated
    if the features are not appropriate. We know that cats have tails; therefore,
    we might like to detect the presence of tails as a prominent feature. However,
    given the different tail shapes and sizes, it is often difficult to describe exactly
    how a tail will look like in terms of pixel values! Moreover, tails could sometimes
    be confused with the hands of humans. Also, overlapping of some objects could
    omit the presence of a cat's tail, making the image even more complicated.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在一群人和猫中寻找猫咪可能会变得非常复杂，特别是当特征不合适时。我们知道猫咪有尾巴，因此，我们可能希望通过检测尾巴的存在作为一个显著的特征。然而，考虑到尾巴的形状和大小各异，往往很难用像素值来准确描述尾巴的外观！此外，尾巴有时可能会与人的手混淆。而且，物体的重叠可能会遮掩猫咪尾巴的存在，使得图像变得更加复杂。
- en: From all the above discussions, it can be concluded that the success of AI systems
    depends mainly on how the data are represented. Also, various representations
    can ensnare and cache the different explanatory factors of all the disparities
    behind the data.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从以上讨论可以得出结论，AI系统的成功主要取决于数据的表示方式。此外，各种表示可以捕捉并缓存数据背后不同的解释因素。
- en: '**Representation learning** is one of the most popular and widely practiced
    learning approaches used to cope with these specific problems. Learning the representations
    of the next layer from the existing representation of data can be defined as representation
    learning. Ideally, all representation learning algorithms have this advantage
    of learning representations, which capture the underlying factors, a subset that
    might be applicable for each particular sub-task. A simple illustration is given
    in the following *Figure 1.2*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**表示学习**是应对这些特定问题的最流行和广泛应用的学习方法之一。表示学习可以定义为从现有数据表示中学习下一层的表示。理想情况下，所有的表示学习算法都有这种学习表示的优势，这些表示能够捕捉潜在因素，并且每个特定子任务可能适用一个子集。下面的*图1.2*提供了一个简单的示例：'
- en: '![Introduction to Deep Learning](img/B05883_01_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习简介](img/B05883_01_02.jpg)'
- en: 'Figure 1.2: The figure illustrates representation learning. The middle layers
    are able to discover the explanatory factors (hidden layers, in blue rectangular
    boxes). Some of the factors explain each task''s target, whereas some explain
    the inputs'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：该图展示了表示学习。中间层能够发现解释因素（蓝色矩形框中的隐藏层）。一些因素解释了每个任务的目标，而另一些则解释了输入。
- en: 'However, dealing with extracting some high-level data and features from a massive
    amount of raw data, which requires some sort of human-level understanding, has
    shown its limitations. There can be many such examples:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从大量原始数据中提取一些高层次的数据和特征，需要某种程度的人类理解，这已经显示出了其局限性。可以举出很多这样的例子：
- en: Differentiating the cry of two similar age babies.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分两个相似年龄婴儿的哭声。
- en: Identifying the image of a cat's eye at both day and night time. This becomes
    clumsy, because a cat's eyes glow at night unlike during the daytime.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别猫咪眼睛的图像在白天和夜晚都会变得繁琐，因为猫咪的眼睛在夜间会发光，而白天则不会。
- en: In all these preceding edge cases, representation learning does not appear to
    behave exceptionally, and shows deterrent behavior.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些前述的边缘情况中，表示学习似乎并没有表现得特别出色，反而显示出了一种阻碍行为。
- en: '**Deep learning**, a sub-field of machine learning, can rectify this major
    problem of representation learning by building multiple levels of representations
    or learning a hierarchy of features from a series of other simple representations
    and features [2] [8].'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**，作为机器学习的一个子领域，可以通过构建多个表示层次或从一系列其他简单的表示和特征中学习特征层次，来解决表示学习中的主要问题[2][8]。'
- en: '![Introduction to Deep Learning](img/B05883_01_03.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习简介](img/B05883_01_03.jpg)'
- en: 'Figure 1.3: The figure shows how a deep learning system can represent the human
    image by identifying various combinations such as corners and contours, which
    can be defined in terms of edges. Image reprinted with permission from Ian Goodfellow,
    Yoshua Bengio, and Aaron Courville, Deep Learning, published by The MIT Press'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：本图展示了深度学习系统如何通过识别诸如角和轮廓之类的各种组合来表征人类形象，这些可以以边缘的术语来定义。图像经Ian Goodfellow、Yoshua
    Bengio和Aaron Courville授权再版，出版者为MIT出版社。
- en: The preceding *Figure 1.3* shows an illustration of a deep learning model. It
    is generally a cumbersome task for the computer to decode the meaning of raw unstructured
    input data, as represented by this image, as a collection of different pixel values.
    A mapping function, which will convert the group of pixels to identify the image,
    is ideally difficult to achieve. Also, to directly train the computer for these
    kinds of mapping is almost insuperable. For these types of tasks, deep learning
    resolves the difficulty by creating a series of subsets of mappings to reach the
    desired output. Each subset of mappings corresponds to a different set of layer
    of the model. The input contains the variables that one can observe, and hence
    , are represented in the visible layers. From the given input we can incrementally
    extract the abstract features of the data. As these values are not available or
    visible in the given data, these layers are termed as hidden layers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的*图1.3*显示了深度学习模型的插图。对于计算机来说，解码原始非结构化输入数据的意义通常是一个繁琐的任务，如此图像所示，由不同的像素值组成。理想情况下，将像素组转换为识别图像的映射函数是非常困难的。此外，直接训练计算机进行这种映射几乎是不可逾越的。对于这类任务，深度学习通过创建一系列子映射来解决难题，以达到期望的输出。每个子映射对应于模型的不同层次集合。输入包含可以观察到的变量，并因此在可见层中表示。从给定的输入中，我们可以逐步提取数据的抽象特征。由于这些值在给定数据中不可用或不可见，因此这些层被称为隐藏层。
- en: In the image, from the first layer of data, the edges can easily be identified
    just by a comparative study of the neighboring pixels. The second hidden layer
    can distinguish the corners and contours from the first hidden layer's description
    of the edges. From this second hidden layer, which describes the corners and contours,
    the third hidden layer can identify the different parts of the specific objects.
    Ultimately, the different objects present in the image can be distinctly detected
    from the third layer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像中，从数据的第一层开始，通过对比相邻像素的研究，可以轻松识别边缘。第二个隐藏层可以区分第一个隐藏层描述的边缘中的角和轮廓。从描述角和轮廓的第二个隐藏层开始，第三个隐藏层可以识别特定对象的不同部分。最终，可以从第三层中清晰地检测出图像中存在的不同对象。
- en: Deep learning started its journey exclusively in 2006, **Hinton et al.** in
    2006[2]; also **Bengio et al.** in 2007[3] initially focused on the MNIST digit
    classification problem. In the last few years, deep learning has seen major transitions
    from digits to object recognition in natural images. Apart from this, one of the
    major breakthroughs was achieved by **Krizhevsky et al.** in 2012 [4] using the
    ImageNet dataset.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习起步于2006年，**Hinton等人**在2006年[2]，还有**Bengio等人**在2007年[3]最初集中于MNIST数字分类问题。在过去的几年里，深度学习已经从数字过渡到自然图像中的对象识别。此外，**Krizhevsky等人**在2012年[4]利用ImageNet数据集取得了重大突破。
- en: The scope of this book is mainly limited to deep learning, so before diving
    into it directly, the necessary definitions of deep learning should be discussed.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的范围主要限于深度学习，因此在直接深入探讨之前，应讨论深度学习的必要定义。
- en: 'Many researchers have defined deep learning in many ways, and hence, in the
    last 10 years, it has gone through many definitions too! The following are few
    of the widely accepted definitions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究者以多种方式定义深度学习，因此，在过去的十年中，它也经历了许多定义的变化！以下是几种广泛接受的定义：
- en: 'As noted by GitHub, deep learning is a new area of machine learning research,
    which has been introduced with the objective of moving machine learning closer
    to one of its original goals: Artificial Intelligence. Deep learning is about
    learning multiple levels of representation and abstraction, which help to make
    sense of data such as images, sounds, and texts.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据GitHub的说明，深度学习是机器学习研究的新领域，旨在使机器学习更接近其原始目标之一：人工智能。深度学习涉及学习多层次的表示和抽象，有助于理解诸如图像、声音和文本等数据。
- en: As recently updated by Wikipedia, deep learning is a branch of machine learning
    based on a set of algorithms that attempt to model high-level abstractions in
    the data by using a deep graph with multiple processing layers, composed of multiple
    linear and non-linear transformations.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据维基百科最近的更新，深度学习是机器学习的一个分支，基于一组算法，尝试通过使用深度图（多个处理层，由多个线性和非线性变换组成）对数据中的高级抽象进行建模。
- en: As the definitions suggest, deep learning can also be considered as a special
    type of machine learning. Deep learning has achieved immense popularity in the
    field of data science with its ability to learn complex representation from various
    simple features. To have an in-depth grip on deep learning, we have listed out
    a few terminologies which will be frequently used in the upcoming chapters. The
    next topic of this chapter will help you to lay a foundation for deep learning
    by providing various terminologies and important networks used for deep learning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正如定义所示，深度学习也可以被视为一种特殊的机器学习类型。深度学习在数据科学领域取得了巨大流行，因为它能够从各种简单特征中学习复杂的表示。为了深入理解深度学习，我们列出了一些在接下来章节中将频繁使用的术语。本章的下一个主题将通过提供深度学习中使用的各种术语和重要网络，帮助你为深度学习打下基础。
- en: Getting started with deep learning
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习入门
- en: 'To understand the journey of deep learning in this book, one must know all
    the terminologies and basic concepts of machine learning. However, if you already
    have enough insight into machine learning and related terms, you should feel free
    to ignore this section and jump to the next topic of this chapter. Readers who
    are enthusiastic about data science, and want to learn machine learning thoroughly,
    can follow *Machine Learning* by Tom M. Mitchell (1997) [5] and *Machine Learning:
    a Probabilistic Perspective* (2012) [6].'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解本书中深度学习的发展历程，必须了解所有机器学习的术语和基本概念。然而，如果你已经对机器学习及相关术语有了足够的了解，可以跳过这一部分，直接进入本章的下一个主题。对数据科学充满热情、并且希望深入学习机器学习的读者，可以参考Tom
    M. Mitchell（1997年）的《机器学习》[5]和《机器学习：概率视角》（2012年）[6]。
- en: Note
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Neural networks do not perform miracles. But, used sensibly, they can produce
    some amazing results.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络并不执行奇迹。但如果合理使用，它们可以产生一些惊人的结果。
- en: Deep feed-forward networks
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度前馈网络
- en: Neural networks can be recurrent as well as feed-forward. Feed-forward networks
    do not have any loop associated in their graph, and are arranged in a set of layers.
    A network with many layers is said to be a deep network. In simple words, any
    neural network with two or more layers (hidden) is defined as a **deep feed-forward
    network** or **feed-forward neural** network. *Figure 1.4* shows a generic representation
    of a deep feed-forward neural network.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以是递归的，也可以是前馈的。前馈网络的图中没有任何循环，它们被排列成一组层。一个具有多层的网络被称为深度网络。简单来说，任何具有两层或更多隐藏层的神经网络都被定义为**深度前馈网络**或**前馈神经网络**。*图1.4*展示了深度前馈神经网络的通用表示。
- en: Deep feed-forward networks work on the principle that with an increase in depth,
    the network can also execute more sequential instructions. Instructions in sequence
    can offer great power, as these instructions can point to the earlier instruction.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 深度前馈网络的工作原理是，随着网络深度的增加，它能够执行更多的顺序指令。顺序中的指令可以提供强大的能力，因为这些指令可以指向之前的指令。
- en: The aim of a feed-forward network is to generalize some function *f*. For example,
    classifier *y=f(x)* maps from input *x* to category *y*. A deep feed-forward network
    modified the mapping, *y=f(x; α)*, and learns the value of the parameter *α*,
    which gives the most appropriate value of the function. The following *Figure
    1.4* shows a simple representation of the deep-forward network, to provide the
    architectural difference with the traditional neural network.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈网络的目标是泛化某个函数 *f*。例如，分类器 *y=f(x)* 将输入 *x* 映射到类别 *y*。深度前馈网络修改了映射关系，*y=f(x; α)*，并学习参数
    *α* 的值，该值给出了最合适的函数值。下图*图1.4*展示了深度前馈网络的简单表示，突出了与传统神经网络的架构差异。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A deep neural network is a feed-forward network with many hidden layers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络是具有多个隐藏层的前馈网络。
- en: '![Deep feed-forward networks](img/B05883_01_04-1.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![深度前馈网络](img/B05883_01_04-1.jpg)'
- en: 'Figure 1.4: Figure shows the representation of a shallow and deep feed-forward
    network'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：图中展示了浅层和深层前馈网络的表示
- en: Various learning algorithms
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 各种学习算法
- en: '**Datasets** are considered to be the building blocks of a learning process.
    A dataset can be defined as a collection of interrelated sets of data, which is
    comprised of separate entities, but which can be used as a single entity depending
    on the use-case. The individual data elements of a dataset are called **data points**.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集**被视为学习过程的构建块。数据集可以定义为一组相互关联的数据集合，包含独立实体，但可以根据具体使用场景作为一个整体使用。数据集的单个数据元素称为**数据点**。'
- en: 'The following *Figure 1.5* gives the visual representation of the various data
    points collected from a social network analysis:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的*图 1.5*给出了来自社交网络分析的各种数据点的视觉表示：
- en: '![Various learning algorithms](img/image_01_005.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![各种学习算法](img/image_01_005.jpg)'
- en: 'Figure 1.5: Image shows the scattered data points of social network analysis.
    Image sourced from Wikipedia'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5：图片展示了社交网络分析中的散布数据点。图片来源于维基百科
- en: '**Unlabeled data**: This part of data consists of the human-generated objects,
    which can be easily obtained from the surroundings. Some of the examples are X-rays,
    log file data, news articles, speech, videos, tweets, and so on.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未标记数据**：这部分数据由人类生成的对象组成，通常可以轻松地从周围环境中获得。例子包括X射线、日志文件数据、新闻文章、语音、视频、推文等。'
- en: '**Labelled data**: Labelled data are normalized data from a set of unlabeled
    data. These types of data are usually well formatted, classified, tagged, and
    easily understandable by human beings for further processing.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记数据**：标记数据是从一组未标记数据中归一化得到的数据。这类数据通常格式良好，已经分类、标记，并且容易为人类理解，便于后续处理。'
- en: From the top-level understanding, machine learning techniques can be classified
    as supervised and unsupervised learning, based on how their learning process is
    carried out.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次的理解来看，机器学习技术可以根据其学习过程的执行方式，分为监督学习和无监督学习。
- en: Unsupervised learning
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'In unsupervised learning algorithms, there is no desired output from the given
    input datasets. The system learns meaningful properties and features from its
    experience during the analysis of the dataset. In deep learning, the system generally
    tries to learn from the whole probability distribution of the data points. There
    are various types of unsupervised learning algorithms, which perform clustering.
    To explain in simple words, clustering means separating the data points among
    clusters of similar types of data. However, with this type of learning, there
    is no feedback based on the final output, that is, there won''t be any teacher
    to correct you! *Figure 1.6* shows a basic overview of unsupervised clustering:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习算法中，给定的输入数据集没有期望的输出。系统通过分析数据集的经验学习有意义的属性和特征。在深度学习中，系统通常会尝试从数据点的整体概率分布中学习。有多种类型的无监督学习算法用于聚类。简单来说，聚类就是将数据点按相似类型的数据分组。然而，这种学习方式中没有基于最终输出的反馈，也就是说，**没有教师来纠正你！**
    *图 1.6* 展示了无监督聚类的基本概述：
- en: '![Unsupervised learning](img/image_01_006.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/image_01_006.jpg)'
- en: 'Figure 1.6: Figures shows a simple representation of unsupervised clustering'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6：图中展示了无监督聚类的简单表示
- en: A real life example of an unsupervised clustering algorithm is Google News.
    When we open a topic under Google News, it shows us a number of hyper-links redirecting
    to several pages. Each of these topics can be considered as a cluster of hyper-links
    that point to independent links.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一个实际的无监督聚类算法示例是谷歌新闻。当我们打开谷歌新闻下的某个主题时，它会显示多个超链接，指向不同的页面。这些主题可以看作是指向独立链接的超链接集群。
- en: Supervised learning
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督学习
- en: In supervised learning, unlike unsupervised learning, there is an expected output
    associated with every step of the experience. The system is given a dataset, and
    it already knows how the desired output will look, along with the correct relationship
    between the input and output of every associated layer. This type of learning
    is often used for classification problems.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，与无监督学习不同，每一步经历都有一个期望的输出。系统会得到一个数据集，并且已经知道期望输出的样子，以及每个相关层之间输入和输出的正确关系。这种学习方式通常用于分类问题。
- en: 'The following visual representation is given in *Figure 1.7*:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的视觉表示在*图 1.7*中给出：
- en: '![Supervised learning](img/image_01_007.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习](img/image_01_007.jpg)'
- en: 'Figure 1.7: Figure shows the classification of data based on supervised learning'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7：图中展示了基于监督学习的数据分类
- en: Real-life examples of supervised learning include face detection, face recognition,
    and so on.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的实际应用示例包括人脸检测、人脸识别等。
- en: Although supervised and unsupervised learning look like different identities,
    they are often connected to each other by various means. Hence, the fine line
    between these two learnings is often hazy to the student fraternity.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管监督学习和无监督学习看起来是不同的身份，但它们通常通过各种方式相互连接。因此，这两种学习方式之间的细微界限对于学生群体来说往往是模糊的。
- en: 'The preceding statement can be formulated with the following mathematical expression:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 上述陈述可以通过以下数学表达式来公式化：
- en: 'The general product rule of probability states that for an *n* number of datasets
    n ε ℝ^t,the joint distribution can be fragmented as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 概率的通用乘法规则指出，对于*n*个数据集n ε ℝ^t，联合分布可以分解如下：
- en: '![Supervised learning](img/Capture.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习](img/Capture.jpg)'
- en: The distribution signifies that the appeared unsupervised problem can be resolved
    by *t* number of supervised problems. Apart from this, the conditional probability
    of *p (k | n)*, which is a supervised problem, can be solved using unsupervised
    learning algorithms to experience the joint distribution of *p (n, k)*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 该分布表明，出现的无监督问题可以通过*t*个监督问题来解决。除此之外，*p (k | n)*的条件概率作为一个监督问题，可以通过无监督学习算法来解决，从而体验*p
    (n, k)*的联合分布。
- en: '![Supervised learning](img/B05883_01_18.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![监督学习](img/B05883_01_18.jpg)'
- en: Although these two types are not completely separate identities, they often
    help to classify the machine learning and deep learning algorithms based on the
    operations performed. In generic terms, cluster formation, identifying the density
    of a population based on similarity, and so on are termed as unsupervised learning,
    whereas structured formatted output, regression, classification, and so on are
    recognized as supervised learning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两种类型不是完全独立的身份，但它们通常有助于根据执行的操作对机器学习和深度学习算法进行分类。通俗来说，聚类形成、基于相似性识别群体密度等被称为无监督学习，而结构化输出、回归、分类等则被认为是监督学习。
- en: Semi-supervised learning
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 半监督学习
- en: As the name suggests, in this type of learning both labelled and unlabeled data
    are used during the training. It's a class of supervised learning which uses a
    vast amount of unlabeled data during training.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，在这种学习类型中，训练过程中同时使用有标签和无标签数据。这是一种监督学习，训练时使用大量无标签数据。
- en: For example, semi-supervised learning is used in a Deep belief network (explained
    later), a type of deep network where some layers learn the structure of the data
    (unsupervised), whereas one layer learns how to classify the data (supervised
    learning).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，半监督学习用于深度信念网络（稍后会解释），这是一种深度网络，其中某些层学习数据的结构（无监督），而一层学习如何对数据进行分类（监督学习）。
- en: In semi-supervised learning, unlabeled data from *p (n)* and labelled data from
    *p (n, k)* are used to predict the probability of *k*, given the probability of
    *n*, or *p (k | n).*
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在半监督学习中，使用来自*p (n)*的无标签数据和来自*p (n, k)*的有标签数据来预测给定*n*的条件下*k*的概率，或者*p (k | n)*。
- en: '![Semi-supervised learning](img/image_01_010.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![半监督学习](img/image_01_010.jpg)'
- en: 'Figure 1.8: Figure shows the impact of a large amount of unlabelled data during
    the semi-supervised learning technique. Figure obtained from Wikipedia'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8：图示展示了在半监督学习技术中，大量无标签数据的影响。图源自Wikipedia
- en: In the preceding *Figure 1.8*, at the top it shows the decision boundary that
    the model uses after distinguishing the white and black circles. The figure at
    the bottom displays another decision boundary, which the model embraces. In that
    dataset, in addition to two different categories of circles, a collection of unlabeled
    data (grey circle) is also annexed. This type of training can be viewed as creating
    the cluster, and then marking those with the labelled data, which moves the decision
    boundary away from the high-density data region.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述的*图1.8*中，顶部展示了模型在区分白色和黑色圆圈后所使用的决策边界。底部的图展示了模型采用的另一个决策边界。在该数据集中，除了两种不同类别的圆圈外，还有一组无标签数据（灰色圆圈）。这种训练可以视为首先创建聚类，然后用有标签数据标记这些聚类，从而将决策边界从高密度数据区域移开。
- en: The preceding *Figure 1.8* depicts the illustration of semi-supervised learning.
    You can refer to *Chapelle et al.'s* book [7] to know more about semi-supervised
    learning methods.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 前述的*图1.8*展示了半监督学习的示例。你可以参考*Chapelle等人*的书籍[7]了解更多关于半监督学习方法的信息。
- en: So, as you have already got a foundation in what Artificial Intelligence, machine
    learning, and representation learning are, we can now move our entire focus to
    elaborate on deep learning with further description.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，既然你已经对人工智能、机器学习和表示学习有了基础了解，我们现在可以将全部焦点转移到深入讨论深度学习，并进一步描述其内容。
- en: 'From the previously mentioned definitions of deep learning, two major characteristics
    of deep learning can be pointed out, as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面提到的深度学习定义中，可以提炼出深度学习的两个主要特征，如下所示：
- en: A way of experiencing unsupervised and supervised learning of the feature representation
    through successive knowledge from subsequent abstract layers
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种通过后续抽象层次的连续知识体验无监督和监督学习的特征表示方式
- en: A model comprising of multiple abstract stages of non-linear information processing
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个由多个抽象阶段的非线性信息处理组成的模型
- en: Deep learning terminologies
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习术语
- en: '**Deep Neural Network** (**DNN**): This can be defined as a multilayer perceptron
    with many hidden layers. All the weights of the layers are fully connected to
    each other, and receive connections from the previous layer. The weights are initialized
    with either supervised or unsupervised learning.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度神经网络**（**DNN**）：这可以定义为一个具有多个隐藏层的多层感知器。所有层的权重彼此完全连接，并接收来自前一层的连接。权重通过监督学习或无监督学习初始化。'
- en: '**Recurrent Neural Networks** (**RNN**): RNN is a kind of deep learning network
    that is specially used in learning from time series or sequential data, such as
    speech, video, and so on. The primary concept of RNN is that the observations
    from the previous state need to be retained for the next state. The recent hot
    topic in deep learning with RNN is **Long short-term memory** (**LSTM**).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**递归神经网络**（**RNN**）：RNN是一种专门用于从时间序列或顺序数据（如语音、视频等）中学习的深度学习网络。RNN的基本概念是需要保留来自前一状态的观察信息，以便用于下一状态。当前深度学习中的热门话题之一是**长短时记忆**（**LSTM**）。'
- en: '**Deep belief network** (**DBN**): This type of network [9] [10] [11] can be
    defined as a probabilistic generative model with visible and multiple layers of
    latent variables (hidden). Each hidden layer possesses a statistical relationship
    between units in the lower layer through learning. The more the networks tend
    to move to higher layers, the more complex relationship becomes. This type of
    network can be productively trained using greedy layer-wise training, where all
    the hidden layers are trained one at a time in a bottom-up fashion.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度信念网络**（**DBN**）：这种网络[9][10][11]可以定义为一种概率生成模型，具有可见层和多个潜在变量（隐藏层）。每个隐藏层通过学习在较低层单元之间具有统计关系。网络越是趋向更高层次，关系就越复杂。这种网络可以通过贪心的逐层训练方法进行高效训练，其中所有隐藏层一次一个地以自下而上的方式进行训练。'
- en: '**Boltzmann machine** (**BM**): This can be defined as a network that is a
    symmetrically connected, neuron-like unit, which is capable of taking stochastic
    decisions about whether to remain on or off. BMs generally have a simple learning
    algorithm, which allows them to uncover many interesting features that represent
    complex regularities in the training dataset.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**玻尔兹曼机**（**BM**）：这可以定义为一个网络，它是一个对称连接的类神经元单元，能够对是否保持开/关做出随机决策。玻尔兹曼机通常具有简单的学习算法，可以揭示许多有趣的特征，这些特征代表了训练数据集中的复杂规律。'
- en: '**Restricted Boltzmann machine** (**RBM**): RBM, which is a generative stochastic
    Artificial Neural Network, is a special type of Boltzmann Machine. These types
    of networks have the capability to learn a probability distribution over a collection
    of datasets. An RBM consists of a layer of visible and hidden units, but with
    no visible-visible or hidden-hidden connections.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**限制玻尔兹曼机**（**RBM**）：RBM是一种生成式随机人工神经网络，是玻尔兹曼机的一种特殊类型。这类网络能够学习一组数据集的概率分布。RBM由一层可见单元和隐藏单元组成，但没有可见-可见或隐藏-隐藏连接。'
- en: '**Convolutional neural networks**: Convolutional neural networks are part of
    neural networks; the layers are sparsely connected to each other and to the input
    layer. Each neuron of the subsequent layer is responsible for only a part of the
    input. Deep convolutional neural networks have accomplished some unmatched performance
    in the field of location recognition, image classification, face recognition,
    and so on.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**：卷积神经网络是神经网络的一部分；这些层之间以及与输入层之间的连接是稀疏的。后续层的每个神经元只负责输入的一部分。深度卷积神经网络在定位识别、图像分类、面部识别等领域取得了无与伦比的表现。'
- en: '**Deep auto-encoder**: A deep auto-encoder is a type of auto-encoder that has
    multiple hidden layers. This type of network can be pre-trained as a stack of
    single-layered auto-encoders. The training process is usually difficult: first,
    we need to train the first hidden layer to restructure the input data, which is
    then used to train the next hidden layer to restructure the states of the previous
    hidden layer, and so on.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度自编码器**：深度自编码器是一种具有多个隐含层的自编码器。这种类型的网络可以作为单层自编码器的堆叠进行预训练。训练过程通常较为复杂：首先，我们需要训练第一隐层以重构输入数据，然后用这个数据来训练下一个隐层，重构前一个隐层的状态，依此类推。'
- en: '**Gradient descent** (**GD**): This is an optimization algorithm used widely
    in machine learning to determine the coefficient of a function (*f*), which reduces
    the overall cost function. Gradient descent is mostly used when it is not possible
    to calculate the desired parameter analytically (for example, linear algebra),
    and must be found by some optimization algorithm.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度下降法** (**GD**)：这是一种广泛应用于机器学习的优化算法，用于确定一个函数（*f*）的系数，从而减少整体成本函数。梯度下降法通常在无法通过解析方法（例如线性代数）计算所需参数时使用，必须通过某种优化算法来找到这些参数。'
- en: In gradient descent, weights of the model are incrementally updated with every
    single iteration of the training dataset (epoch).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度下降法中，模型的权重会在每次训练数据集（周期）的迭代中逐步更新。
- en: 'The cost function, *J (w)*, with the sum of the squared errors can be written
    as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数 *J(w)*，带有平方误差的和，可以表示为如下形式：
- en: '![Deep learning terminologies](img/B05883_01_19-1.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习术语](img/B05883_01_19-1.jpg)'
- en: 'The direction of magnitude of the weight update is calculated by taking a step
    in the reverse direction of the cost gradient, as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 权重更新的方向和幅度是通过沿成本梯度的反方向进行一步计算得出的，如下所示：
- en: '![Deep learning terminologies](img/Capture-3.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习术语](img/Capture-3.jpg)'
- en: 'In the preceding equation, *η* is the learning rate of the network. Weights
    are updated incrementally after every epoch with the following rule:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的公式中，*η* 是网络的学习率。每经过一个周期（epoch），权重会根据以下规则逐步更新：
- en: '[PRE0]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Deep learning terminologies](img/Capture-12.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习术语](img/Capture-12.jpg)'
- en: Popular examples that can be optimized using gradient descent are Logistic Regression
    and Linear Regression.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降法优化的流行示例包括逻辑回归和线性回归。
- en: '**Stochastic Gradient Descent** (**SGD**): Various deep learning algorithms,
    which operated on a large amount of datasets, are based on an optimization algorithm
    called stochastic gradient descent. Gradient descent performs well only in the
    case of small datasets. However, in the case of very large-scale datasets, this
    approach becomes extremely costly . In gradient descent, it takes only one single
    step for one pass over the entire training dataset; thus, as the dataset''s size
    tends to increase, the whole algorithm eventually slows down. The weights are
    updated at a very slow rate; hence, the time it takes to converge to the global
    cost minimum becomes protracted.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降法** (**SGD**)：许多基于大量数据集的深度学习算法，都是基于一种叫做随机梯度下降法的优化算法。梯度下降法在小规模数据集上表现良好。然而，在非常大规模的数据集上，这种方法变得非常昂贵。在梯度下降法中，整个训练数据集只需一次遍历就能进行一次权重更新；因此，随着数据集规模的增加，整个算法的运行速度最终会变慢。权重更新的速度非常缓慢，因此，收敛到全局成本最小值所需的时间变得非常长。'
- en: Therefore, to deal with such large-scale datasets, a variation of gradient descent
    called stochastic gradient descent is used. Unlike gradient descent, the weight
    is updated after each iteration of the training dataset, rather than at the end
    of the entire dataset.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了处理如此大规模的数据集，使用了梯度下降法的变种——随机梯度下降法。与梯度下降法不同，随机梯度下降法在每次训练数据集迭代后更新权重，而不是在整个数据集结束后更新。
- en: '[PRE1]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Deep learning terminologies](img/Capture-13.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习术语](img/Capture-13.jpg)'
- en: In the last few years, deep learning has gained tremendous popularity, as it
    has become a junction for research areas of many widely practiced subjects, such
    as pattern recognition, neural networks, graphical modelling, machine learning,
    and signal processing.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习获得了极大的关注，它成为了许多广泛应用学科的交汇点，如模式识别、神经网络、图形建模、机器学习和信号处理等。
- en: 'The other important reasons for this popularity can be summarized by the following
    points:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习广泛应用的其他重要原因可以通过以下几点总结：
- en: In recent years, the ability of **GPU** (**Graphical Processing Units**) has
    increased drastically
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近年来，图形处理单元（GPU）的能力显著提高。
- en: The size of data sizes of the dataset used for training purposes has increased
    significantly
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练目的的数据集的数据规模显著增加。
- en: Recent research in machine learning, data science, and information processing
    has shown some serious advancements
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近年来，机器学习、数据科学和信息处理领域的最新研究显示了一些重大进展。
- en: Detailed descriptions of all these points will be provided in an upcoming topic
    in this chapter.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观点的详细描述将在本章的后续主题中提供。
- en: 'Deep learning: A revolution in Artificial Intelligence'
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习：人工智能的革命
- en: An extensive history of deep learning is beyond the scope of this book. However,
    to get an interest in and cognizance of this subject, some basic context of the
    background is essential.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的详尽历史超出了本书的范围。然而，为了对这一主题产生兴趣和了解，了解一些基本的背景背景是必要的。
- en: In the introduction, we already talked a little about how deep learning occupies
    a space in the perimeter of Artificial Intelligence. This section will detail
    more on how machine learning and deep learning are correlated or different from
    each other. We will also discuss how the trend has varied for these two topics
    in the last decade or so.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍中，我们已经简要讨论了深度学习如何在人工智能的边缘占据一席之地。本节将详细讨论机器学习和深度学习如何相互关联或有何不同。我们还将讨论这两个主题在过去十年左右的趋势变化。
- en: '|   | *"Deep Learning waves have lapped at the shores of computational linguistics
    for several years now, but 2015 seems like the year when the full force of the
    tsunami hit the major Natural Language Processing (NLP) conferences."* |   |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|   | *"深度学习浪潮已经在计算语言学的海岸边拍打了数年，但2015年似乎是这场海啸全面冲击主要自然语言处理（NLP）会议的一年。"* |  
    |'
- en: '|   | --*Dr. Christopher D. Manning, Dec 2015* |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|   | --*克里斯托弗·D·曼宁博士，2015年12月* |'
- en: '![Deep learning: A revolution in Artificial Intelligence](img/B05883_01_17.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习：人工智能的革命](img/B05883_01_17.jpg)'
- en: 'Figure 1.9: Figure depicts that deep learning was in the initial phase approximately
    10 years back. However, machine learning was somewhat a trending topic in the
    researcher''s community.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9：图表明深度学习大约10年前处于初期阶段。然而，机器学习在研究者社区中已成为热门话题。
- en: Deep learning is rapidly expanding its territory in the field of Artificial
    Intelligence, and continuously surprising many researchers with its astonishing
    empirical results. Machine learning and deep learning both represent two different
    schools of thought. Machine learning can be treated as the most fundamental approach
    for AI, where as deep learning can be considered as the new, giant era, with some
    added functionalities of the subject.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习正在迅速扩展其在人工智能领域的地盘，并不断以其惊人的实证结果令许多研究人员惊讶。机器学习和深度学习都代表了两种不同的思想流派。机器学习可以被视为AI的最基本方法，而深度学习则可以被认为是一个新的、巨大的时代，增加了一些该学科的功能。
- en: '![Deep learning: A revolution in Artificial Intelligence](img/B05883_01_18-1.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习：人工智能的革命](img/B05883_01_18-1.jpg)'
- en: 'Figure 1.10: Figure depicts how deep learning is gaining in popularity these
    days, and trying to reach the level of machine learning'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10：图表明深度学习如何在当今日益流行，并试图达到机器学习的水平。
- en: However, machine learning has often failed in completely solving many crucial
    problems of AI, mainly speech recognition, object recognition, and so on.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，机器学习在完全解决AI的许多关键问题上通常失败，主要是语音识别、物体识别等。
- en: The performance of traditional algorithms seems to be more challenging while
    working with high-dimensional data, as the number of random variables keeps on
    increasing. Moreover, the procedures used to attain the generalization in traditional
    machine-learning approaches are not sufficient to learn complicated obligations
    in high-dimensional spaces, which generally impel more computational costs of
    the overall model. The development of deep learning was mostly motivated by the
    collapse of the fundamental algorithms of machine learning on such functions,
    and also to overcome the afore mentioned obstacles.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理高维数据时，传统算法的性能似乎更具挑战性，因为随机变量的数量不断增加。此外，传统机器学习方法中用于实现泛化的程序不足以学习高维空间中的复杂义务，这通常会增加整体模型的计算成本。深度学习的发展主要受到了这些函数中基本算法的崩溃的激励，并且克服了上述障碍。
- en: 'A large proportion of researchers and data scientists believe that, in the
    course of time, deep learning will occupy a major portion of Artificial Intelligence,
    and eventually make machine learning algorithms obsolete. To get a clear idea
    of this, we looked at the current Google trend of these two fields and came to
    the following conclusion:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数研究人员和数据科学家认为，随着时间的推移，深度学习将占据人工智能的主要部分，最终使机器学习算法变得过时。为了清晰了解这一点，我们查看了这两个领域的当前谷歌趋势，并得出了以下结论：
- en: The curve of machine learning has always been the growing stage from the past
    decade. Deep learning is new, but growing faster than machine learning. When trends
    are closely observed, one will find that the growth rate is faster for deep learning
    compared to machine learning.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的发展曲线在过去十年一直处于增长阶段。深度学习是新的，但增长速度超过了机器学习。当仔细观察趋势时，可以发现深度学习的增长速度相比机器学习要更快。
- en: Both of the preceding *Figure 1.9* and *Figure 1.10* depict the visualizations
    of the Google trend.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的*图1.9*和*图1.10*展示了谷歌趋势的可视化。
- en: Motivations for deep learning
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度学习的动机
- en: One of the biggest-known problems that machine learning algorithms face is the
    **curse of dimensionality** [12] [13] [14]. This refers to the fact that certain
    learning algorithms may behave poorly when the number of dimensions in the dataset
    is high. In the next section, we will discuss how deep learning has given sufficient
    hope to this problem by introducing new features. There are many other related
    issues where deep architecture has shown a significant edge over traditional architectures.
    In this part of the chapter, we would like to introduce the more pronounced challenges
    as a separate topic.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法面临的最大已知问题之一是**维度灾难**[12] [13] [14]。这指的是当数据集的维度数很高时，某些学习算法可能表现不佳。在下一节中，我们将讨论深度学习是如何通过引入新特性为这个问题带来足够的希望的。还有许多其他相关问题，深度架构在这些问题上显示出了相较于传统架构的显著优势。在本章的这一部分，我们希望将这些更加突出的挑战作为一个单独的主题介绍。
- en: The curse of dimensionality
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 维度灾难
- en: 'The curse of dimensionality can be defined as the phenomena which arises during
    the analysis and organization of data in high-dimensional spaces (in the range
    of thousands or even higher dimensions). Machine learning problems face extreme
    difficulties when the number of dimensions in the dataset is high. High dimensional
    data are difficult to work with because of the following reasons:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 维度灾难可以定义为在高维空间（通常是成千上万甚至更高维度）中进行数据分析和组织时所出现的现象。当数据集的维度很高时，机器学习问题将面临极大的困难。高维数据难以处理，原因如下：
- en: With the increasing number of dimensions, the number of features will tend to
    increase exponentially, which eventually leads to an increase in noise.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着维度数量的增加，特征的数量将呈指数级增长，最终导致噪音的增加。
- en: In standard practice, we will not get a high enough number of observations to
    generalize the dataset.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在标准实践中，我们无法获得足够多的观察样本来对数据集进行泛化。
- en: A straightforward explanation for the curse of dimensionality could be **combinatorial
    explosion**. As per combinatorial explosion, with the collection of a number of
    variables, an enormous combination could be built. For example, with *n* binary
    variables, the number of possible combinations would be *O (2^n)*. So, in high-dimensional
    spaces, the total number of configurations is going to be almost uncountable,
    much larger than our number of examples available - most of the configurations
    will not have such training examples associated with them. *Figure 1.11* shows
    a pictorial representation of a similar phenomenon for better understanding.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 维度灾难的一个直接解释是**组合爆炸**。根据组合爆炸的理论，当收集了一定数量的变量时，会形成一个庞大的组合。例如，对于*n*个二进制变量，可能的组合数为
    *O (2^n)*。因此，在高维空间中，总的配置数将几乎是无法计数的，远远超过我们可用的样本数——大多数配置将没有与之相关的训练样本。*图1.11*显示了类似现象的图示，便于更好理解。
- en: 'Therefore, this situation is cumbersome for any machine learning model, due
    to the difficulty in the training. **Hughes effect** [15] states the following:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于训练的困难，这种情况对于任何机器学习模型来说都是繁琐的。**休斯效应**[15]指出了以下内容：
- en: '*"With a fixed number of training samples, the predictive power reduces as
    the dimensionality increases."*'
  id: totrans-152
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“在固定数量的训练样本下，随着维度的增加，预测能力会减少。”*'
- en: Hence, the achievable precision of the model almost collapses as the number
    of explanatory variables increases.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随着解释变量数量的增加，模型的可达精度几乎崩溃。
- en: To cope with this scenario, we need to increase the size of the sample dataset
    fed to the system to such an extent that it can compete with the scenario. However,
    as the complexity of data also increases, the number of dimensions almost reaches
    one thousand. For such cases, even a dataset with hundreds of millions of images
    will not be sufficient.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这种情况，我们需要增加输入系统的样本数据集的大小，直到它足以与这种情况相抗衡。然而，随着数据复杂性的增加，维度的数量几乎达到一千。对于这种情况，即使是数亿张图像的数据集也不足以应对。
- en: 'Deep learning, with its deeper network configuration, shows some success in partially
    solving this problem. This contribution is mostly attributed to the following
    reasons:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通过更深的网络配置，在部分解决这个问题上取得了一些成功。这一贡献主要归因于以下几个原因：
- en: Now, the researchers are able to manage the model complexity by redefining the
    network structure before feeding the sample for training
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，研究人员能够通过在将样本输入进行训练前重新定义网络结构来管理模型复杂性。
- en: Deep convolutional networks focus on the higher level features of the data rather
    than the fundamental level information, which extensively further reduces the
    dimension of features
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度卷积网络关注数据的高级特征，而非基础层次的信息，这大大减少了特征的维度。
- en: Although deep learning networks have given some insights to deal with the curse
    of dimensionality, they are not yet able to completely conquer the challenge.
    In Microsoft's recent research on super deep neural networks, they have come up
    with 150 layers; as a result, the parameter space has grown even bigger. The team
    has explored the research with even deep networks almost reaching to 1000 layers;
    however, the result was not up to the mark due to *overfitting* of the model!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习网络为应对维度灾难提供了一些思路，但它们仍未能够完全征服这一挑战。在微软最近的超深神经网络研究中，他们提出了150层的结构；因此，参数空间变得更加庞大。研究团队还探索了更加深层的网络，层数几乎达到1000层；然而，由于模型的*过拟合*，结果并未达到预期。
- en: Note
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注
- en: '**Over-fitting in machine learning**: The phenomenon when a model is over-trained
    to such an extent that it gives a negative impact to its performance is termed
    as over-fitting of the model. This situation occurs when the model learns the
    random fluctuations and unwanted noise of the training datasets. The consequences
    of these phenomena are unsatisfactory--the model is not able to behave well with
    the new dataset, which negatively impacts the model''s ability to generalize.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习中的过拟合**：当模型经过过度训练，导致其性能受到负面影响时，称为模型的过拟合现象。当模型学习到训练数据集中的随机波动和不必要的噪声时，就会发生这种情况。这些现象的后果是不可取的——模型无法在新数据集上表现良好，这会负面影响模型的泛化能力。'
- en: '**Under-fitting in machine learning**: This refers to a situation when the
    model is neither able to perform with the current dataset nor with the new dataset.
    This type of model is not suitable, and shows poor performance with the dataset.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习中的欠拟合**：指的是模型无法在当前数据集上或新数据集上表现良好的情况。这种模型不适用，且在数据集上的表现较差。'
- en: '![The curse of dimensionality](img/B05883_01_11.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![维度灾难](img/B05883_01_11.jpg)'
- en: 'Figure 1.11: Figure shows that with the increase in the number of dimensions
    from one to three, from top to bottom, the number of random variables might increase
    exponentially. Image reproduced with permission from Nicolas Chapados from his
    article DataMining Algorithms for Actuarial Ratemaking.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11：图示显示，从一维到三维，随着维度数量的增加，从上到下，随机变量的数量可能会呈指数增长。此图像经尼古拉斯·查帕多斯（Nicolas Chapados）授权转载，摘自他的文章《数据挖掘算法在精算定价中的应用》。
- en: In the 1D example (top) of the preceding figure, as there are only 10 regions
    of interest, it should not be a tough task for the learning algorithm to generalize
    correctly. However, with the higher dimension 3D example (bottom), the model needs
    to keep track of all the *10*10*10=1000* regions of interest, which is much more
    cumbersome (or almost going to be an impossible task for the model). This can
    be used as the simplest example of the curse of dimensionality.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在前图的 1D 示例（上）中，由于只有 10 个兴趣区域，对于学习算法正确地进行泛化不应是一个困难的任务。然而，在更高维度的 3D 示例（下）中，模型需要跟踪所有
    *10*10*10=1000* 个兴趣区域，这变得更加繁琐（或者几乎是模型无法完成的任务）。这可以作为维度灾难的最简单例子。
- en: The vanishing gradient problem
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 梯度消失问题
- en: The vanishing gradient problem [16] is the obstacle found while training the
    Artificial neural networks, which is associated with some gradient-based method,
    such as Backpropagation. Ideally, this difficulty makes learning and training
    the previous layers really hard. The situation becomes worse when the number of
    layers of a deep neural network increases aggressively.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度消失问题[16]是训练人工神经网络时遇到的障碍，尤其与一些基于梯度的方法，如反向传播，相关。理想情况下，这个问题使得学习和训练前面几层变得非常困难。当深度神经网络的层数急剧增加时，情况变得更加糟糕。
- en: The gradient descent algorithms particularly update the weights by the negative
    of the gradient multiplied by small scaler value (lies between `0` and `1`).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法特别是通过将梯度的负值乘以一个小的标量值（介于`0`和`1`之间）来更新权重。
- en: '![The vanishing gradient problem](img/Capture-6.jpg)![The vanishing gradient
    problem](img/B05883_01_24.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![梯度消失问题](img/Capture-6.jpg)![梯度消失问题](img/B05883_01_24.jpg)'
- en: As shown in the preceding equations, we will repeat the gradient until it reaches
    zero. Ideally, though, we generally set some hyper-parameter for the maximum number
    of iterations. If the number of iterations is too high, the duration of the training
    will also be longer. On the other hand, if the number of iterations becomes imperceptible
    for some deep neural network, we will surely end up with inaccurate results.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的方程所示，我们将重复梯度计算，直到它趋近于零。理想情况下，我们通常会设置一个超参数来限制最大迭代次数。如果迭代次数过多，训练的时间也会相应变长。另一方面，如果某些深度神经网络的迭代次数变得不可察觉，我们最终肯定会得到不准确的结果。
- en: 'In the vanishing gradient problem, the gradients of the network''s output,
    with respect to the parameters of the previous layers, become extremely small.
    As a result, the resultant weight will not show any significant change with each
    iteration. Therefore, even a large change in the value of parameters for the earlier
    layers does not have a significant effect on the overall output. As a result of
    this problem, the training of the deep neural networks becomes infeasible, and
    the prediction of the model becomes unsatisfactory. This phenomenon is known as
    the vanishing gradient problem. This will result in some elongated cost function,
    as shown in next *Figure 1.12*:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在梯度消失问题中，网络输出相对于前一层参数的梯度变得极其微小。因此，权重在每次迭代时不会发生显著变化。即使前几层的参数值发生了较大变化，对整体输出的影响也不明显。由于这个问题，深度神经网络的训练变得不可行，模型的预测效果也不令人满意。这种现象被称为梯度消失问题。这将导致一些拉长的代价函数，如下一个*图1.12*所示：
- en: '![The vanishing gradient problem](img/image_01_020.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![梯度消失问题](img/image_01_020.jpg)'
- en: 'Figure 1.12: Image of a flat gradient and an elongated cost function'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12：平坦梯度和拉长代价函数的图像
- en: 'An example with large gradient is also shown in the following *Figure 1.13*,
    where the gradient descent can converge quickly:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的*图1.13*展示了一个梯度较大的例子，其中梯度下降能够快速收敛：
- en: '![The vanishing gradient problem](img/image_01_021.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![梯度消失问题](img/image_01_021.jpg)'
- en: 'Figure 1.13: Image of a larger gradient cost function; hence the gradient descent
    can converge much more quickly'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.13：较大梯度代价函数的图像；因此梯度下降能够更快地收敛
- en: This is a substantial challenge in the success of deep learning, but now, thanks
    to various different techniques, this problem has been overcome to some extent.
    **Long short-term memory** (**LSTM**) network was one of the major breakthroughs
    which nullified this problem in 1997\. A detailed description is given in [Chapter
    4](ch04.html "Chapter 4. Recurrent Neural Network"), *Recurrent Neural Network*.
    Also, some researchers have tried to resolve the problem with different techniques,
    with feature preparation, activation functions, and so on.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是深度学习成功的一个重大挑战，但现在，得益于各种不同的技术，这个问题在一定程度上得到了克服。**长短期记忆**（**LSTM**）网络是1997年消除这个问题的一个重要突破。详细描述请参见[第4章](ch04.html
    "第4章. 循环神经网络")，*循环神经网络*。此外，一些研究人员也尝试通过不同的技术来解决这个问题，包括特征准备、激活函数等。
- en: Distributed representation
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式表示
- en: All the deep networks are mostly based on the concept of distributed representations,
    which is the heart of theoretical advantage behind the success of deep learning
    algorithms. In the context of deep learning, distributed representations are multiscale
    representations, and are closely related to multiscale modelling of theoretical
    chemistry and physics. The basic idea behind a distributed representation is that
    the perceived feature is the result of multiple factors, which work as a combination
    to produce the desired results. A daily life example could be the human brain,
    which uses distributed representation for disguising the objects in the surroundings.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的深度网络大多基于分布式表示的概念，这也是深度学习算法成功背后的理论优势所在。在深度学习的背景下，分布式表示是多尺度表示，且与理论化学和物理学的多尺度建模密切相关。分布式表示背后的基本思想是，感知到的特征是多个因素的结果，这些因素组合在一起以产生期望的结果。一个日常生活中的例子可以是人脑，它通过分布式表示来伪装周围的物体。
- en: An Artificial neural network, in this kind of representation, will be built
    in such a way that it will have numerous features and layers required to represent
    our necessary model. The model will describe the data, such as speech, video,
    or image, with multiple interdependent layers, where each of the layers will be
    responsible for describing the data at a different level of scale. In this way,
    the representation will be distributed across many layers, involving many scales.
    Hence, this kind of representation is termed as distributed representation.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种表示方式下，人工神经网络将以一种方式构建，使其拥有表示我们所需模型所必需的众多特征和层。该模型将通过多个相互依赖的层来描述数据，例如语音、视频或图像，每一层都负责在不同的尺度层次上描述数据。通过这种方式，表示将在多个层次中分布，涉及多个尺度。因此，这种表示方式被称为分布式表示。
- en: Note
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: A distributed representation is dense in nature. It follows a many-to-many relationship
    between two types of representations. One concept can be represented using more
    than one neuron. On the other hand, one neuron depicts more than one concept.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式表示本质上是稠密的。它遵循两种表示之间的多对多关系。一个概念可以通过多个神经元来表示。另一方面，一个神经元也可以表示多个概念。
- en: The traditional clustering algorithms that use non-distributed representation,
    such as nearest-neighbor algorithms, decision trees, or Gaussian mixtures, all
    require *O(N)* parameters to distinguish *O(N)* input regions. At one point of
    time, one could hardly have believed that any other algorithm could behave better
    than this! However, the deep networks, such as sparse coding, RBM, multi-layer
    neural networks, and so on, can all distinguish as many as *O(2^k)* number of
    input regions with only *O(N)* parameters (where *k* represents the total number
    of non-zero elements in sparse representation, and *k=N* for other non-sparse
    RBMs and dense representations).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的聚类算法，如最近邻算法、决策树或高斯混合模型等，使用的是非分布式表示，这些算法都需要 *O(N)* 个参数来区分 *O(N)* 个输入区域。曾几何时，人们几乎无法相信会有任何其他算法能表现得比这更好！然而，深度网络，如稀疏编码、RBM、多层神经网络等，能够仅用
    *O(N)* 个参数区分多达 *O(2^k)* 个输入区域（其中 *k* 代表稀疏表示中非零元素的总数，*k=N* 适用于其他非稀疏的RBM和密集表示）。
- en: In these kinds of operations, either same clustering is applied on different
    parts of the input, or several clustering takes place in parallel. The generalization
    of clustering to distributed representations is termed as multi-clustering.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些操作中，要么将相同的聚类应用于输入的不同部分，要么进行多个聚类并行操作。聚类对分布式表示的推广被称为多重聚类。
- en: The exponential advantage of using distributed representation is due to the
    reuse of each parameter in multiple examples, which are not necessarily near to
    each other. For example, Restricted Boltzmann machine could be an appropriate
    example in this case. However, with local generalization, non-identical regions
    in the input space are only concerned with their own private set of parameters.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分布式表示的指数级优势源于在多个示例中重用每个参数，而这些示例不一定彼此接近。例如，限制玻尔兹曼机（Restricted Boltzmann Machine）可以作为一个合适的例子。然而，在局部泛化的情况下，输入空间中的非相同区域仅关心其自身的私有参数集。
- en: 'The key advantages are as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 主要优势如下：
- en: The representation of the internal structure of data is robust in terms of damage
    resistance and graceful degradation
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据内部结构的表示在抗损伤性和优雅降级方面具有强大的鲁棒性
- en: They help to generalize the concepts and relations among the data, hence enabling
    the reasoning abilities.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们有助于概括数据之间的概念和关系，从而增强推理能力。
- en: 'The following *Figure 1.14* represents a real-time example of distributed representations:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下*图1.14*展示了分布式表示的一个实时例子：
- en: '![Distributed representation](img/B05883_01_14-2.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![分布式表示](img/B05883_01_14-2.jpg)'
- en: 'Figure 1.14: Figure shows how distributed representation helped the model to
    distinguish among various types of expressions in the images'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14：图示展示了分布式表示如何帮助模型区分图像中各种类型的表达。
- en: Classification of deep learning networks
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习网络的分类
- en: '**Artificial neural networks** in machine learning are often termed as new
    generation neural networks by many researchers. Most of the learning algorithms
    that we hear about were essentially built so as to make the system learn exactly
    the way the biological brain learns. This is how the name **Artificial neural
    networks** came about! Historically, the concept of deep learning emanated from
    **Artificial neural networks** (**ANN**). The practice of deep learning started
    back in the 1960s, or possibly even earlier. With the rise of deep learning, ANN,
    has gained more popularity in the research field.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**在机器学习中常被许多研究者称为新一代神经网络。我们所听说的大多数学习算法，基本上是为了让系统学习和生物大脑一样的方式而构建的。这就是**人工神经网络**这个名称的由来！从历史上看，深度学习的概念源自**人工神经网络**（**ANN**）。深度学习的实践可以追溯到1960年代，甚至可能更早。随着深度学习的兴起，人工神经网络（ANN）在研究领域获得了更大的关注。'
- en: '**Multi-Layer Perceptron** (**MLP**) or feed-forward neural networks with many
    hidden intermediate layers which are referred to as **deep neural networks** (**DNN**),
    are some good examples of the deep architecture model. The first popular deep
    architecture model was published by Ivakhnenko and Lapa in 1965 using supervised
    deep feed-forward multilayer perceptron [17].'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知机**（**MLP**）或具有多个隐藏中间层的前馈神经网络，被称为**深度神经网络**（**DNN**），是深度架构模型的一些典型例子。第一个流行的深度架构模型由Ivakhnenko和Lapa于1965年发表，采用了监督式深度前馈多层感知机[17]。'
- en: '![Classification of deep learning networks](img/B05883_01_15-1.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![深度学习网络的分类](img/B05883_01_15-1.jpg)'
- en: 'Figure 1.15: The GMDH network has four inputs (the component of the input vector
    x), and one output y ,which is an estimate of the true function y= f(x) = y'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.15：GMDH网络有四个输入（输入向量x的组成部分），和一个输出y，该输出是对真实函数y=f(x)=y的估计。
- en: Another paper from Alexey Ivakhnenko, who was working at that time on a better
    prediction of fish population in rivers, used the **group method of data handling
    algorithm** (**GMDH**), which tried to explain a type of deep network with eight
    trained layers, in 1971\. It is still considered as one of most popular papers
    of the current millennium[18]. The preceding *Figure 1.15* shows the GMDH network
    of four inputs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 另一篇来自Alexey Ivakhnenko的论文，他当时正在研究如何更好地预测河流中的鱼群数量，使用了**数据处理组方法**（**GMDH**）算法，该算法尝试解释一种具有八个训练层的深度网络，发表于1971年。至今，这仍被视为本千年最受欢迎的论文之一[18]。前面的*图1.15*展示了具有四个输入的GMDH网络。
- en: Going forward, **Backpropagation** (**BP**), which was a well-known algorithm
    for learning the parameters of similar type of networks, found its popularity
    during the 1980s. However, networks having a number of hidden layers are difficult
    to handle due to many reasons, hence, BP failed to reach the level of expectation
    [8] [19]. Moreover, backpropagation learning uses the gradient descent algorithm,
    which is based on local gradient information, and these operations start from
    some random initial data points. While propagating through the increasing depth
    of networks, these often get collected in some undesired local optima; hence,
    the results generally get stuck in poor solutions.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此后，**反向传播**（**BP**）作为一种广为人知的算法，用于学习类似类型网络的参数，在1980年代得到了广泛应用。然而，由于多种原因，具有多个隐藏层的网络难以处理，因此反向传播未能达到预期的效果[8]
    [19]。此外，反向传播学习使用的是基于局部梯度信息的梯度下降算法，这些操作从一些随机的初始数据点开始。在网络深度逐渐增加的过程中，这些数据往往会集中在一些不期望的局部最优点，因此，结果通常会陷入不良解中。
- en: The *optimization constraints* related to the deep architecture model were pragmatically
    reduced when an efficient, unsupervised learning algorithm was established in
    two papers [8] [20]. The two papers introduced a class of deep generative models
    known as a **Deep belief network** (**DBN**).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度架构模型相关的*优化约束*在两篇论文[8] [20]中通过建立一种高效的无监督学习算法得到了实质性减少。这两篇论文介绍了一类被称为**深度置信网络**（**DBN**）的深度生成模型。
- en: In 2006, two more unsupervised deep models with non-generative, non-probabilistic
    features were published, which became immensely popular with the researcher community.
    One is an energy-based unsupervised model [21], and the other is a variant of
    auto-encoder with subsequent layer training, much like the previous DBN training
    [3]. Both of these algorithms can be efficiently used to train a deep neural network,
    almost exactly like the DBN.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年，另外两种具有非生成性、非概率特征的无监督深度模型被发布，并且在研究人员中获得了极大的关注。一种是基于能量的无监督模型[21]，另一种是具有后续层训练的自编码器变体，类似于先前的DBN训练[3]。这两种算法都可以高效地用于训练深度神经网络，几乎与DBN完全相同。
- en: Since 2006, the world has seen a tremendous explosion in the research of deep
    learning. The subject has seen continuous exponential growth, apart from the traditional
    shallow machine learning techniques.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 自2006年以来，深度学习的研究经历了巨大的爆发。除了传统的浅层机器学习技术外，这一领域也持续呈现出指数级增长。
- en: Based on the learning techniques mentioned in the previous topics of this chapter,
    and depending on the use case of the techniques and architectures used, deep learning
    networks can be broadly classified into two distinct groups.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 基于本章前面提到的学习技术，并根据所使用的技术和架构的应用案例，深度学习网络可以大致分为两大类。
- en: Deep generative or unsupervised models
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度生成或无监督模型
- en: Many deep learning networks fall under this category, such as Restricted Boltzmann
    machine, Deep Belief Networks, Deep Boltzmann machine, De-noising Autoencoders,
    and so on. Most of these networks can be used to engender samples by sampling
    within the networks. However, a few other networks, for example sparse coding
    networks and the like, are difficult to sample, and hence, are, not generative
    in nature.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 许多深度学习网络都属于这一类别，如受限玻尔兹曼机、深度置信网络、深度玻尔兹曼机、去噪自编码器等。大多数这些网络可以通过在网络中进行采样来生成样本。然而，一些其他网络，例如稀疏编码网络等，难以进行采样，因此不是生成式的。
- en: A popular deep unsupervised model is the **Deep Boltzmann machine** (**DBM**)
    [22] [23] [24] [25]. A traditional DBM contains many layers of hidden variables;
    however, the variables within the same layer have no connections between them.
    The traditional **Boltzmann machine** (**BM**), despite having a simpler algorithm,
    is too much complex to study and very slow to train. In a DBM, each layer acquires
    higher-order complicated correlations between the responses of the latent features
    of the previous layers. Many real-life problems, such as object and speech recognition,
    which require learning complex internal representations, are much easier to solve
    with DBMs.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 一种流行的深度无监督模型是**深度玻尔兹曼机**（**DBM**）[22] [23] [24] [25]。传统的DBM包含多层隐藏变量；然而，同一层中的变量之间没有任何连接。尽管传统的**玻尔兹曼机**（**BM**）算法较为简单，但它过于复杂，难以研究，且训练速度非常慢。在DBM中，每一层都能获得前一层潜在特征响应之间的高阶复杂关联。许多现实生活中的问题，如物体和语音识别，需要学习复杂的内部表示，这些问题通过DBM可以更容易地解决。
- en: A DBM with one hidden layer is termed as a **Restricted Boltzmann machine**
    (**RBM**). Similar to a DBM, an RBM does not have any hidden-to-hidden and visible-to-visible
    connections. The crucial property of an RBM is reflected in constituting many
    RBMs. With numerous latent layers formed, the feature activation of a previous
    RBM acts as the input training data for the next. This kind of architecture generates
    a different kind of network named **Deep belief network** (**DBN**). Various applications
    of the Restricted Boltzmann machine and Deep belief network are discussed in detail
    in [Chapter 5](ch05.html "Chapter 5.  Restricted Boltzmann Machines") , *Restricted
    Boltzmann Machines*.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 具有一个隐藏层的DBM被称为**受限玻尔兹曼机**（**RBM**）。与DBM类似，RBM没有任何隐藏层之间或可见层之间的连接。RBM的关键特性体现在其构成多个RBM上。随着多个潜在层的形成，前一个RBM的特征激活作为下一个RBM的输入训练数据。这种架构生成了一种不同类型的网络，称为**深度置信网络**（**DBN**）。关于受限玻尔兹曼机和深度置信网络的各种应用在[第5章](ch05.html
    "第5章 受限玻尔兹曼机")中有详细讨论，*受限玻尔兹曼机*。
- en: 'A primary component of DBN is a set of layers, which reduces its time complexity
    linear of size and depth of the networks. Along with DBN property, which could
    overcome the major drawback of BP by starting the training from some desired initialization
    data points, it has other attractive catching characteristics too. Some of them
    are listed as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: DBN的一个主要组成部分是一组层，它通过减少网络的规模和深度的时间复杂度来提高效率。结合DBN的特性，它能够克服BP的主要缺点，即从某些期望的初始化数据点开始训练，它还具有其他吸引人的特点。以下是其中一些特点：
- en: DBN can be considered as a probabilistic generative model.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBN可以看作是一个概率生成模型。
- en: With hundreds of millions of parameters, DBNs generally undergo the over-fitting
    problem. Also, the deep architecture, due to its voluminous dataset, often experiences
    the under-fitting problem. Both of these problems can be effectively diminished
    in the pre-training step.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBN通常有数亿个参数，因此会出现过拟合问题。此外，由于其庞大的数据集，深度架构通常还会经历欠拟合问题。这两种问题都可以通过预训练步骤有效地减少。
- en: Effective uses of unlabeled data are practiced by DBN.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBN有效地利用了未标记数据。
- en: 'One more deep generative network, which can be used for unsupervised (as well
    as supervised) learning is the **sum-product network** (**SPN**) [26], [27]. SPNs
    are deep networks, which can be viewed as directed acyclic graphs, where the leaves
    of the graph are the observed variables, and the internal nodes are the sum and
    product operations. The ''sum'' nodes represent the mixture models, and the ''product''
    nodes frame the feature hierarchy. SPNs are trained using the expectation-maximization
    algorithm together with Back propagation. The major hindrance in learning SPNs
    is that the gradient rapidly diminishes when moving towards the deep layers. Specifically,
    the standard gradient descent of the regular deep neural networks generated from
    the derivative of the conditional likelihood, goes through the tribulation. A
    solution to reduce this problem is to substitute the marginal inference with the
    most probable state of the latent variables, and then disseminate the gradient
    through this. An exceptional outcome on small-scale image recognition was presented
    by Domingo and Gens in [28]. The following *Figure 1.16* shows a sample SPN network
    for better understanding. It shows a block diagram of the sum-product network:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可以用于无监督（以及监督）学习的深度生成网络是**和积网络**（**SPN**）[26]，[27]。SPN是深度网络，可以视为有向无环图，其中图的叶节点是观察变量，内部节点是求和和乘积操作。‘求和’节点表示混合模型，而‘乘积’节点则构建了特征层次结构。SPN使用期望最大化算法和反向传播一起训练。学习SPN的主要障碍是，当梯度传播到深层时，梯度迅速减小。具体来说，从条件似然的导数生成的常规深度神经网络的标准梯度下降会遇到困难。减少此问题的一种解决方案是用潜在变量的最可能状态代替边际推断，然后通过此状态传播梯度。在[28]中，Domingo和Gens在小规模图像识别任务中展示了出色的结果。以下是*图1.16*，展示了一个示例SPN网络以帮助理解。它显示了和积网络的框图：
- en: '![Deep generative or unsupervised models](img/image_01_024.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![深度生成或无监督模型](img/image_01_024.jpg)'
- en: 'Figure 1.16: Block diagram of sum-product network'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.16：和积网络的框图
- en: Another type of popular deep generative network, which can be used as unsupervised
    (as well as supervised) learning, is the **Recurrent neural network** (**RNN**).
    The depth of this type of network directly depends on the length of the input
    data sequence. In the unsupervised RNN model, experiences from previous data samples
    are used to predict the future data sequence. RNNs have been used as an excellent
    powerful model for data sequencing text or speech, however, their popularity has
    recently decreased due to the rise of vanishing gradient problems [29] [16]. Using
    stochastic curvature estimates, Hessian-free optimization [30] has somewhat overcome
    the limitations. Recently, Bengio et al. [31] and Sutskever [32] have come out
    with different variations to train the generating RNNs, which outperform the Hessian-free
    optimization models. RNN is further elucidated in this book in  [Chapter 4](ch04.html
    "Chapter 4. Recurrent Neural Network") , *Recurrent Neural Network*.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的深度生成网络，可以用作无监督（以及有监督）学习的，是**循环神经网络**（**RNN**）。这种类型的网络的深度直接取决于输入数据序列的长度。在无监督RNN模型中，利用之前数据样本的经验来预测未来的数据序列。RNN已被广泛用于数据序列化文本或语音，然而，由于梯度消失问题的出现，最近它们的流行度有所下降[29]
    [16]。通过使用随机曲率估计，Hessian-free优化[30]在某种程度上克服了这些限制。最近，Bengio等[31]和Sutskever[32]提出了不同的变种来训练生成型RNN，这些变种在性能上超越了Hessian-free优化模型。RNN在本书的[第四章](ch04.html
    "第四章 循环神经网络")中有进一步的阐述，*循环神经网络*。
- en: Among the other subclasses of unsupervised deep networks, the energy-based deep
    models are mostly known architecture [33] [34]. A typical example of the unsupervised
    model category of deep networks is deep autoencoder. Most of the variants of deep
    autoencoder are generative in nature; however, the properties and implementations
    generally vary from each other. Popular examples are predictive sparse coders,
    Transforming Autoencoder, De-noising Autoencoder and their stacked versions, and
    so on. Auto-encoders are explained in detail in [Chapter 6](ch06.html "Chapter 6. 
    Autoencoders") , *Autoencoders*.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他无监督深度网络的子类中，基于能量的深度模型是最为知名的架构之一[33] [34]。无监督模型类别中的一个典型例子是深度自编码器。深度自编码器的大多数变种本质上是生成性的；然而，它们的特性和实现方式通常各不相同。流行的例子包括预测稀疏编码器、变换自编码器、去噪自编码器及其堆叠版本等等。自编码器在[第六章](ch06.html
    "第六章 自编码器")中有详细解释，*自编码器*。
- en: Deep discriminate models
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度判别模型
- en: Most of the discriminative techniques used in supervised learning are shallow
    architectures such as Hidden Marcov models [35], [36], [37], [38], [39], [40],
    [41] or conditional random fields. However, recently, a deep-structured conditional
    random field model has evolved, by passing the output of every lower layer as
    the input of the higher layers. There are multiple versions of deep-structured
    conditional random fields which have been successfully accomplished to for natural
    language processing, phone recognition, language recognition, and so on. Although
    discriminative approaches are successful for deep-architectures, they have not
    been able to reach the expected outcome yet.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数在有监督学习中使用的判别技术是浅层架构，例如隐马尔可夫模型[35]，[36]，[37]，[38]，[39]，[40]，[41]或条件随机场。然而，最近，出现了一种深度结构的条件随机场模型，它通过将每一层的输出作为输入传递到更高层。深度结构的条件随机场模型有多个版本，已经成功应用于自然语言处理、电话识别、语言识别等领域。尽管判别方法在深度架构中取得了一定的成功，但它们仍未达到预期的效果。
- en: As mentioned in the previous section, RNNs have been used for unsupervised learning.
    However, RNNs can also be used as a discriminative model and trained with supervised
    learning. In this case, the output becomes a label sequence related to the input
    data sequence. Speech recognition techniques have already seen such discriminative
    RNNs a long time ago, but with very little success. Paper [42] shows that a Hidden
    Marcov Model was used to mutate the RNN classification outcome into a labelled
    sequence. But unfortunately, the use of Hidden Marcov model for all these reasons
    did not take enough advantage of the full capability of RNNs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所述，RNN（循环神经网络）已经被用于无监督学习。然而，RNN也可以作为判别模型，并通过有监督学习进行训练。在这种情况下，输出成为与输入数据序列相关的标签序列。语音识别技术早在很久以前就已经看到了这种判别RNN的应用，但成功的案例很少。[42]号论文展示了使用隐马尔可夫模型将RNN的分类结果转化为标签序列。然而，遗憾的是，由于种种原因，隐马尔可夫模型的使用未能充分发挥RNN的全部潜力。
- en: A few other methods and models have recently been developed for RNNs, where
    the fundamental idea was to consider the RNN output as some conditional distributions,
    and distribute all over the possible input sequences [43], [44],[45],[46]. This
    helped RNNs to undergo sequence classification while embedding the long-short-term-memory
    to its model. The major benefit was that it neither required the pre-segmentation
    of the training dataset, nor the post-processing of the outputs. Basically, the
    segmentation of the dataset is automatically performed by the algorithm, and one
    differentiable objective function could be derived for optimization of the conditional
    distributions across the label sequence. The effectiveness of this type of algorithm
    is extensively applicable for handwriting recognition operations.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，针对RNN（循环神经网络）开发了几种其他方法和模型，其基本思想是将RNN的输出视为某些条件分布，并将其分布在所有可能的输入序列上[43]，[44]，[45]，[46]。这有助于RNN在嵌入长短期记忆（LSTM）到其模型的同时进行序列分类。其主要优点是，它既不需要对训练数据集进行预先分割，也不需要对输出进行后处理。基本上，数据集的分割是由算法自动执行的，并且可以为条件分布的优化推导出一个可微的目标函数。此类算法的有效性广泛适用于手写识别操作。
- en: One more popular type of discriminative deep architecture is the **convolutional
    neural network** (**CNN**). In CNN, each module comprises of a convolutional layer
    and one pooling layer. To form a deep model, the modules are generally stacked
    one on top of the other, or with a deep neural network on the top of it. The convolutional
    layer helps to share many weights, and the pooling layer segregates the output
    of the convolutional later, minimizing the rate of data from the previous layer.
    CNN has been recognized as a highly efficient model, especially for tasks like
    image recognition, computer vision, and so on. Recently, with specific modifications
    in CNN design, it has also been found equally effective in speech recognition
    too. **Time-delay neural network** (**TDNN**) [47] [48], originated for early
    speech recognition, is a special case for convolutional neural network, and can
    also be considered its predecessor.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的判别式深度架构是**卷积神经网络**（**CNN**）。在CNN中，每个模块包含一个卷积层和一个池化层。为了形成深度模型，这些模块通常是一个接一个地堆叠，或者在其上方叠加一个深度神经网络。卷积层有助于共享许多权重，而池化层则将卷积层的输出进行分隔，最小化来自上一层的数据流量。CNN已被公认为一种高效的模型，尤其在图像识别、计算机视觉等任务中表现出色。最近，通过对CNN设计的特定修改，它在语音识别方面也被证明同样有效。**时延神经网络**（**TDNN**）[47]
    [48]，最初用于早期的语音识别，是卷积神经网络的一个特例，也可以视为其前身。
- en: In this type of model, the weight sharing is limited to only time dimension,
    and no pooling layer is present. [Chapter 3](ch03.html "Chapter 3.  Convolutional
    Neural Network") , *Convolutional Neural Networks* discusses the concept and applications
    of CNNs in depth.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种模型中，权重共享仅限于时间维度，并且没有池化层。[第3章](ch03.html "第3章 卷积神经网络")，*卷积神经网络*深入讨论了CNN的概念和应用。
- en: Deep learning, with its many models, has a wide range of applications too. Many
    of the top technology companies, such as Facebook, Microsoft, Google, Adobe, IBM,
    and so on are extensively using deep learning. Apart from computer science, deep
    learning has also provided valuable contributions to other scientific fields as
    well.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习凭借其多种模型，具有广泛的应用。许多顶尖科技公司，如Facebook、Microsoft、Google、Adobe、IBM等，广泛采用深度学习。除了计算机科学，深度学习还对其他科学领域做出了重要贡献。
- en: Modern CNNs used for object recognition have given a major insight into visual
    processing, which even neuroscientists can explore further. Deep learning also
    provides the necessary functional tools for processing large-scale data, and to
    make predictions in scientific fields. This field is also very successful in predicting
    the behaviors of molecules in order to enhance the pharmaceutical researches.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现代用于物体识别的CNN为视觉处理提供了重要的见解，甚至神经科学家也可以进一步探索。深度学习还提供了处理大规模数据所需的功能工具，并在科学领域中进行预测。该领域在预测分子行为方面也非常成功，旨在促进制药研究。
- en: To summarize, deep learning is a sub-field of machine learning, which has seen
    exceptional growth in usefulness and popularity due to its much wider applicability.
    However, the coming years should be full of challenges and opportunities to ameliorate
    deep learning even further, and explore the subject for new data enthusiasts.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，深度学习是机器学习的一个子领域，因其应用范围更广，已在实用性和流行度方面取得了卓越的增长。然而，未来的几年应该充满挑战与机遇，以进一步改进深度学习，并为新的数据爱好者探索这一主题。
- en: Note
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意事项
- en: 'To help the readers to get more insights into deep learning, here are a few
    other excellent and frequently updated reading lists available online: [http://deeplearning.net/tutorial/](http://deeplearning.net/tutorial/)
    [http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial](http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial)
    [http://deeplearning.net/reading-list/](http://deeplearning.net/reading-list/)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助读者更好地理解深度学习，以下是一些优秀的、经常更新的在线阅读列表：[http://deeplearning.net/tutorial/](http://deeplearning.net/tutorial/)
    [http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial](http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial)
    [http://deeplearning.net/reading-list/](http://deeplearning.net/reading-list/)
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Over the past decade, we have had the privilege of hearing about the greatest
    inventions of deep learning from many of the great scientists and companies working
    in Artificial Intelligence. Deep learning is an approach to machine learning which
    has shown tremendous growth in its usefulness and popularity in the last few years.
    The reason is mostly due to its capability to work with large datasets involving
    high dimensional data, resolving major issues such as vanishing gradient problems,
    and so on, and techniques to train deeper networks. In this chapter, we have explained
    most of these concepts in detail, and have also classified the various algorithms
    of deep learning, which will be elucidated in detail in subsequent chapters.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年里，我们有幸听到许多人工智能领域伟大科学家和公司关于深度学习的重大发明。深度学习是一种机器学习方法，近年来在其实用性和流行度上取得了巨大的增长。其原因主要在于它能够处理涉及高维数据的大型数据集，解决了如梯度消失问题等主要难题，并且有训练更深层网络的技术。在本章中，我们已经详细解释了大部分这些概念，并且还对深度学习的各种算法进行了分类，后续章节将详细阐述这些内容。
- en: The next chapter of this book will introduce the association of big data with
    deep learning. The chapter will mainly focus on how deep learning plays a major
    role in extracting valuable information from large-scale data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的下一章将介绍大数据与深度学习的关联。该章节将主要聚焦于深度学习如何在从大规模数据中提取有价值信息方面发挥重要作用。
