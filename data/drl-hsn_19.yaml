- en: '19'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '19'
- en: Reinforcement Learning with Human Feedback
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€šè¿‡äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ 
- en: In this chapter, weâ€™ll take a look at a relatively recent method that addresses
    situations when the desired behavior is hard to define via the explicit reward
    function â€“ reinforcement learning with human feedback (RLHF) . This is also related
    to exploration (as the method allows humans to push learning in a new direction),
    the problem we discussed in ChapterÂ [18](ch022.xhtml#x1-32800018). Surprisingly,
    the method, initially developed for a very specific subproblem in the RL domain,
    turned out to be enormously successful in the large language models (LLMs). Nowadays,
    RLHF is at the core of modern LLM training pipelines, and without it, the recent
    fascinating progress wouldnâ€™t have been possible.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸€ç§ç›¸å¯¹è¾ƒæ–°çš„æ–¹æ³•ï¼Œè§£å†³äº†å½“æœŸæœ›çš„è¡Œä¸ºå¾ˆéš¾é€šè¿‡æ˜ç¡®çš„å¥–åŠ±å‡½æ•°å®šä¹‰æ—¶çš„æƒ…å†µâ€”â€”é€šè¿‡äººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€‚è¿™ä¹Ÿä¸æ¢ç´¢ç›¸å…³ï¼ˆå› ä¸ºè¯¥æ–¹æ³•å…è®¸äººç±»æ¨åŠ¨å­¦ä¹ æœç€æ–°çš„æ–¹å‘å‘å±•ï¼‰ï¼Œè¿™æ˜¯æˆ‘ä»¬åœ¨ç¬¬[18ç« ](ch022.xhtml#x1-32800018)ä¸­è®¨è®ºè¿‡çš„é—®é¢˜ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œè¿™ç§æ–¹æ³•æœ€åˆæ˜¯ä¸ºå¼ºåŒ–å­¦ä¹ é¢†åŸŸä¸­çš„ä¸€ä¸ªéå¸¸ç‰¹å®šçš„å­é—®é¢˜å¼€å‘çš„ï¼Œç»“æœåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸­å–å¾—äº†å·¨å¤§çš„æˆåŠŸã€‚å¦‚ä»Šï¼ŒRLHFå·²æˆä¸ºç°ä»£LLMè®­ç»ƒæµç¨‹çš„æ ¸å¿ƒï¼Œæ²¡æœ‰å®ƒï¼Œè¿‘æœŸçš„æƒŠäººè¿›å±•æ˜¯ä¸å¯èƒ½å®ç°çš„ã€‚
- en: As this book is not about LLMs and modern chatbots, we will focus purely on
    the original paper from OpenAI and Google by Christiano et al., Deep reinforcement
    learning from human preferences [[Chr+17](#)], which describes the RLHF method
    applied to RL problems and environments. But in the overview of the method, I
    will explain a bit about how this method is used in LLM training.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæœ¬ä¹¦å¹¶ä¸æ¶‰åŠLLMå’Œç°ä»£èŠå¤©æœºå™¨äººï¼Œæˆ‘ä»¬å°†çº¯ç²¹èšç„¦äºOpenAIå’ŒGoogleçš„Christianoç­‰äººæ‰€æå‡ºçš„åŸå§‹è®ºæ–‡ã€Šæ¥è‡ªäººç±»åå¥½çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‹[[Chr+17](#)]ï¼Œè¯¥è®ºæ–‡æè¿°äº†RLHFæ–¹æ³•å¦‚ä½•åº”ç”¨äºå¼ºåŒ–å­¦ä¹ é—®é¢˜å’Œç¯å¢ƒã€‚ä½†åœ¨æ–¹æ³•æ¦‚è¿°ä¸­ï¼Œæˆ‘ä¼šç®€è¦è§£é‡Šè¿™ç§æ–¹æ³•æ˜¯å¦‚ä½•åœ¨LLMè®­ç»ƒä¸­ä½¿ç”¨çš„ã€‚
- en: 'In this chapter, we will:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š
- en: Take a look at human feedback in RL to address problems with unclear reward
    objectives and exploration.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çœ‹çœ‹äººç±»åé¦ˆåœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„åº”ç”¨ï¼Œä»¥è§£å†³å¥–åŠ±ç›®æ ‡ä¸æ˜ç¡®å’Œæ¢ç´¢çš„é—®é¢˜ã€‚
- en: Implement an RLHF pipeline from scratch and check it on the SeaQuest Atari game
    to teach it new behavior.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»é›¶å¼€å§‹å®ç°ä¸€ä¸ªRLHFæµç¨‹ï¼Œå¹¶åœ¨SeaQuest Atariæ¸¸æˆä¸­è¿›è¡Œæµ‹è¯•ï¼Œä»¥æ•™ä¼šå®ƒæ–°çš„è¡Œä¸ºã€‚
- en: Reward functions in complex environments
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤æ‚ç¯å¢ƒä¸­çš„å¥–åŠ±å‡½æ•°
- en: 'Before we go into the details of the RLHF method, letâ€™s start by discussing
    the underlying motivation of the concept. As we discussed in ChapterÂ [1](ch005.xhtml#x1-190001),
    the reward is the core concept in RL. Without a reward, weâ€™re blind â€” all the
    methods weâ€™ve already discussed are heavily dependent on the reward value provided
    by the environment:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ·±å…¥è®¨è®ºRLHFæ–¹æ³•ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆè®¨è®ºä¸€ä¸‹è¿™ä¸€æ¦‚å¿µèƒŒåçš„åŠ¨æœºã€‚æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬[1ç« ](ch005.xhtml#x1-190001)ä¸­è®¨è®ºçš„ï¼Œå¥–åŠ±æ˜¯å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µã€‚æ²¡æœ‰å¥–åŠ±ï¼Œæˆ‘ä»¬å°±åƒçå­â€”â€”æˆ‘ä»¬å·²ç»è®¨è®ºè¿‡çš„æ‰€æœ‰æ–¹æ³•éƒ½ä¸¥é‡ä¾èµ–äºç¯å¢ƒæä¾›çš„å¥–åŠ±å€¼ï¼š
- en: In value-based methods (Part 2 of the book), we used the reward to approximate
    the Q-value to evaluate the actions and choose the most prominent one.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨åŸºäºä»·å€¼çš„æ–¹æ³•ï¼ˆæœ¬ä¹¦ç¬¬2éƒ¨åˆ†ï¼‰ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¥–åŠ±æ¥è¿‘ä¼¼Qå€¼ï¼Œä»¥è¯„ä¼°è¡Œä¸ºå¹¶é€‰æ‹©æœ€ä¼˜çš„è¡ŒåŠ¨ã€‚
- en: In policy-based methods (Part 3), the reward was used even more directly â€” as
    a scale for the Policy Gradient. With all the math removed, we basically optimized
    our policy to prefer actions that bring more accumulated future reward.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨åŸºäºç­–ç•¥çš„æ–¹æ³•ï¼ˆç¬¬3éƒ¨åˆ†ï¼‰ä¸­ï¼Œå¥–åŠ±çš„ä½¿ç”¨æ›´åŠ ç›´æ¥â€”â€”ä½œä¸ºç­–ç•¥æ¢¯åº¦çš„å°ºåº¦ã€‚å»æ‰æ‰€æœ‰æ•°å­¦å†…å®¹åï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šä¼˜åŒ–äº†æˆ‘ä»¬çš„ç­–ç•¥ï¼Œä»¥åå¥½é‚£äº›èƒ½å¤Ÿå¸¦æ¥æ›´å¤šç´¯è®¡æœªæ¥å¥–åŠ±çš„è¡Œä¸ºã€‚
- en: 'In black-box methods (ChapterÂ [17](ch021.xhtml#x1-31100017)), we used the reward
    to make a decision about agent variants: should they be kept for the future or
    discarded?'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨é»‘ç®±æ–¹æ³•ï¼ˆç¬¬[17ç« ](ch021.xhtml#x1-31100017)ï¼‰ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¥–åŠ±æ¥åšå‡ºå…³äºä»£ç†å˜ä½“çš„å†³ç­–ï¼šåº”è¯¥ä¿ç•™å®ƒä»¬ä»¥ä¾›å°†æ¥ä½¿ç”¨ï¼Œè¿˜æ˜¯ä¸¢å¼ƒï¼Ÿ
- en: In almost all the RL environments weâ€™ve experimented with, the reward function
    was predefined for us â€” in Atari games, we had the score; in the FrozenLake environment,
    it was an explicit target position; in simulated robots, it was the distance travelled,
    etc. The only exception was in ChapterÂ [10](ch014.xhtml#x1-16900010), where we
    implemented the environment (stock trading system) ourselves and had to decide
    how the reward was to be shaped. But even in that example, it was fairly obvious
    what should be used as a reward.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å®éªŒè¿‡çš„å‡ ä¹æ‰€æœ‰å¼ºåŒ–å­¦ä¹ ç¯å¢ƒä¸­ï¼Œå¥–åŠ±å‡½æ•°éƒ½æ˜¯é¢„å®šä¹‰çš„â€”â€”åœ¨Atariæ¸¸æˆä¸­ï¼Œæˆ‘ä»¬æœ‰å¾—åˆ†ï¼›åœ¨FrozenLakeç¯å¢ƒä¸­ï¼Œå®ƒæ˜¯ä¸€ä¸ªæ˜ç¡®çš„ç›®æ ‡ä½ç½®ï¼›åœ¨æ¨¡æ‹Ÿæœºå™¨äººä¸­ï¼Œå®ƒæ˜¯è¡Œè¿›çš„è·ç¦»ï¼Œç­‰ç­‰ã€‚å”¯ä¸€çš„ä¾‹å¤–æ˜¯åœ¨ç¬¬[10ç« ](ch014.xhtml#x1-16900010)ï¼Œæˆ‘ä»¬è‡ªå·±å®ç°äº†ç¯å¢ƒï¼ˆè‚¡ç¥¨äº¤æ˜“ç³»ç»Ÿï¼‰ï¼Œå¹¶ä¸”å¿…é¡»å†³å®šå¦‚ä½•è®¾è®¡å¥–åŠ±ã€‚å³ä¾¿åœ¨é‚£ä¸ªä¾‹å­ä¸­ï¼Œåº”è¯¥ä½¿ç”¨ä»€ä¹ˆä½œä¸ºå¥–åŠ±ä¹Ÿç›¸å½“æ˜æ˜¾ã€‚
- en: Unfortunately, in real-life situations, it is not always that easy to formulate
    what should be used as a reward. Letâ€™s look at a couple of examples. If we are
    training the chatbot to solve a set of tasks, itâ€™s important to not only ensure
    the tasks are completed correctly but also consider the style in which they are
    done. What if we ask the system â€œWhatâ€™s the weather forecast for tomorrow?â€ and
    it replies correctly but in a rude manner? Should it be punished for this with
    a negative reward and to what extent? What should we do in the opposite situation
    â€” a very polite answer but the information given is wrong? If we just optimize
    one single criterion (like the correctness of information), we might get a system
    that â€œworksâ€ but is not usable in real life â€“ just because it is so awkward that
    nobody wants to use it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¹¸çš„æ˜¯ï¼Œåœ¨ç°å®ç”Ÿæ´»ä¸­ï¼Œç¡®å®šåº”ä½œä¸ºå¥–åŠ±çš„å†…å®¹å¹¶éæ€»æ˜¯é‚£ä¹ˆç®€å•ã€‚è®©æˆ‘ä»¬æ¥çœ‹å‡ ä¸ªä¾‹å­ã€‚å¦‚æœæˆ‘ä»¬åœ¨è®­ç»ƒèŠå¤©æœºå™¨äººè§£å†³ä¸€ç»„ä»»åŠ¡æ—¶ï¼Œé™¤äº†ç¡®ä¿ä»»åŠ¡æ­£ç¡®å®Œæˆå¤–ï¼Œè¿˜å¿…é¡»è€ƒè™‘å®Œæˆä»»åŠ¡çš„æ–¹å¼ã€‚å¦‚æœæˆ‘ä»¬é—®ç³»ç»Ÿâ€œæ˜å¤©çš„å¤©æ°”é¢„æŠ¥æ˜¯ä»€ä¹ˆï¼Ÿâ€å®ƒå›ç­”æ­£ç¡®ä½†è¯­æ°”ç²—é²ï¼Œåº”è¯¥å› å…¶ä¸ç¤¼è²Œçš„å›ç­”è€Œå—åˆ°è´Ÿé¢å¥–åŠ±å—ï¼Ÿå¦‚æœæ˜¯ç›¸åçš„æƒ…å†µâ€”â€”å›ç­”éå¸¸ç¤¼è²Œï¼Œä½†ä¿¡æ¯é”™è¯¯å‘¢ï¼Ÿå¦‚æœæˆ‘ä»¬åªä¼˜åŒ–ä¸€ä¸ªæ ‡å‡†ï¼ˆæ¯”å¦‚ä¿¡æ¯çš„æ­£ç¡®æ€§ï¼‰ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå¾—åˆ°ä¸€ä¸ªâ€œèƒ½å·¥ä½œâ€çš„ç³»ç»Ÿï¼Œä½†å®ƒåœ¨ç°å®ç”Ÿæ´»ä¸­å´ä¸å¯ç”¨â€”â€”å› ä¸ºå®ƒå¤ªç¬¨æ‹™ï¼Œæ²¡äººæ„¿æ„ä½¿ç”¨ã€‚
- en: 'Another example of a â€œsingle optimization factorâ€ is the transportation of
    goods from point A to point B. Transport companies donâ€™t just try to maximize
    their profits by all means. In addition, they have tons of restrictions and regulations,
    like driving rules, working hours, labour legislation, etc. If we optimize only
    one criterion in our system, we might eventually get â€œDrive through the neighborâ€™s
    fence â€“ this is the fastest way.â€ So, in real life, having a single value we want
    to maximize is an exception rather than the norm. Most of the time, we have several
    parameters that contribute to the final result and we need to find some sort of
    balance between them. Even in the Atari games weâ€™ve already seen, the score might
    be calculated as the sum of different â€œsubgoals.â€ A very good example of this
    is the SeaQuest game we experimented with in the previous chapter. If you havenâ€™t
    played it before, you can do it in your browser to get a better understanding:
    [https://www.retrogames.cz/play_221-Atari2600.php](https://www.retrogames.cz/play_221-Atari2600.php).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªâ€œå•ä¸€ä¼˜åŒ–å› ç´ â€çš„ä¾‹å­æ˜¯ä»Aç‚¹åˆ°Bç‚¹çš„è´§ç‰©è¿è¾“ã€‚è¿è¾“å…¬å¸å¹¶ä¸ä»…ä»…é€šè¿‡ä¸€åˆ‡æ‰‹æ®µæœ€å¤§åŒ–ä»–ä»¬çš„åˆ©æ¶¦ã€‚æ­¤å¤–ï¼Œä»–ä»¬è¿˜é¢ä¸´ç€å¤§é‡çš„é™åˆ¶å’Œè§„å®šï¼Œå¦‚é©¾é©¶è§„åˆ™ã€å·¥ä½œæ—¶é—´ã€åŠ³åŠ¨æ³•è§„ç­‰ã€‚å¦‚æœæˆ‘ä»¬ä»…åœ¨ç³»ç»Ÿä¸­ä¼˜åŒ–ä¸€ä¸ªæ ‡å‡†ï¼Œæœ€ç»ˆå¯èƒ½ä¼šå¾—åˆ°â€œç©¿è¶Šé‚»å±…çš„æ …æ â€”â€”è¿™æ˜¯æœ€å¿«çš„è·¯ã€‚â€å› æ­¤ï¼Œåœ¨ç°å®ç”Ÿæ´»ä¸­ï¼Œè¿½æ±‚å•ä¸€çš„æœ€å¤§åŒ–æ ‡å‡†æ˜¯ä¾‹å¤–è€Œéå¸¸æ€ã€‚å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æœ‰å¤šä¸ªå‚æ•°å…±åŒä½œç”¨äºæœ€ç»ˆç»“æœï¼Œæˆ‘ä»¬éœ€è¦åœ¨å®ƒä»¬ä¹‹é—´æ‰¾åˆ°æŸç§å¹³è¡¡ã€‚å³ä½¿åœ¨æˆ‘ä»¬ä¹‹å‰è§è¿‡çš„é›…è¾¾åˆ©æ¸¸æˆä¸­ï¼Œåˆ†æ•°ä¹Ÿå¯èƒ½æ˜¯ä¸åŒâ€œå­ç›®æ ‡â€ä¹‹å’Œçš„ç»“æœã€‚ä¸€ä¸ªéå¸¸å¥½çš„ä¾‹å­æ˜¯æˆ‘ä»¬åœ¨ä¸Šä¸€ç« å®éªŒè¿‡çš„ã€ŠSeaQuestã€‹æ¸¸æˆã€‚å¦‚æœä½ ä»¥å‰æ²¡ç©è¿‡ï¼Œå¯ä»¥åœ¨æµè§ˆå™¨ä¸­è¿›è¡Œä½“éªŒï¼Œä»¥æ›´å¥½åœ°ç†è§£ï¼š[https://www.retrogames.cz/play_221-Atari2600.php](https://www.retrogames.cz/play_221-Atari2600.php)ã€‚
- en: 'In this game, youâ€™re controlling the submarine and you are scored for the following
    activities:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™æ¬¾æ¸¸æˆä¸­ï¼Œä½ æ§åˆ¶æ½œè‰‡ï¼Œå¹¶æ ¹æ®ä»¥ä¸‹æ´»åŠ¨è·å¾—åˆ†æ•°ï¼š
- en: Shooting evil fish and enemy submarines
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°„å‡»é‚ªæ¶çš„é±¼ç±»å’Œæ•Œæ–¹æ½œè‰‡
- en: Saving divers and bringing them back to the surface
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ•‘æ´æ½œæ°´å‘˜å¹¶å°†ä»–ä»¬å¸¦å›æ°´é¢
- en: Avoiding enemy fire and ships on the surface (they appear in later levels of
    the game)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¿å…æ•Œäººç«åŠ›å’Œæ°´é¢ä¸Šçš„èˆ¹åªï¼ˆå®ƒä»¬å‡ºç°åœ¨æ¸¸æˆçš„åæœŸå…³å¡ï¼‰
- en: As the level of oxygen is limited, your submarine has to go to the surface from
    time to time to refill the reserves. Most of the modern RL methods have no problem
    discovering the reward for shooting fish and submarines â€” starting with trial
    and error, after just a couple of hours of training, the agent learns how to get
    the reward from firing.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ°§æ°”æœ‰é™ï¼Œæ½œè‰‡å¿…é¡»å®šæœŸä¸Šæµ®ä»¥è¡¥å……æ°§æ°”ã€‚å¤§å¤šæ•°ç°ä»£å¼ºåŒ–å­¦ä¹ æ–¹æ³•åœ¨å‘ç°å°„å‡»é±¼ç±»å’Œæ½œè‰‡çš„å¥–åŠ±æ—¶æ²¡æœ‰é—®é¢˜â€”â€”ä»è¯•é”™å¼€å§‹ï¼Œç»è¿‡å‡ å°æ—¶çš„è®­ç»ƒï¼Œæ™ºèƒ½ä½“å°±èƒ½å­¦ä¼šå¦‚ä½•é€šè¿‡å°„å‡»è·å¾—å¥–åŠ±ã€‚
- en: But discovering scoring from saving divers is much trickier, as the reward for
    them is given only after collecting six divers and getting to the surface. Oxygen
    replenishment is also hard to discover by trial and error, as our neural network
    has no prior idea about oxygen, submarines, and how the sudden death of your submarine
    might be related to the gauge at the bottom of the screen. Our RL method with
    ğœ–-greedy exploration could be seen as a newborn baby randomly pushing buttons
    and being rewarded for correct sequences of actions, which might take lots of
    time before the correct lengthy sequence has been executed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å‘ç°é€šè¿‡æ‹¯æ•‘æ½œæ°´å‘˜æ¥å¾—åˆ†è¦å›°éš¾å¾—å¤šï¼Œå› ä¸ºåªæœ‰åœ¨æ”¶é›†äº†å…­ä¸ªæ½œæ°´å‘˜å¹¶æˆåŠŸåˆ°è¾¾æ°´é¢åæ‰ä¼šç»™äºˆå¥–åŠ±ã€‚é€šè¿‡è¯•é”™æ³•å‘ç°æ°§æ°”è¡¥å……ä¹Ÿå¾ˆå›°éš¾ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œå¯¹æ°§æ°”ã€æ½œæ°´è‰‡ä»¥åŠæ½œæ°´è‰‡çªç„¶æ­»äº¡å¦‚ä½•ä¸å±å¹•åº•éƒ¨çš„ä»ªè¡¨ç›¸å…³è”æ²¡æœ‰å…ˆéªŒçŸ¥è¯†ã€‚æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ä¸ğœ–-è´ªå©ªæ¢ç´¢å¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªåˆšå‡ºç”Ÿçš„å©´å„¿éšæœºæŒ‰æŒ‰é’®å¹¶å› æ­£ç¡®çš„åŠ¨ä½œåºåˆ—è€Œè·å¾—å¥–åŠ±ï¼Œè¿™å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´æ‰èƒ½æ‰§è¡Œæ­£ç¡®çš„é•¿åºåˆ—ã€‚
- en: As a result, most of the training episodes in SeaQuest are limited by the average
    score of 300 and 500 game steps. The submarine just dies from a lack of oxygen
    and random surface visits are too rare to discover that the game might be played
    for much longer. At the same time, people who havenâ€™t seen the game before can
    figure out how to refill the oxygen and save divers in just several minutes of
    gameplay.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœæ˜¯ï¼Œåœ¨ã€ŠSeaQuestã€‹ä¸­çš„å¤§å¤šæ•°è®­ç»ƒå›åˆéƒ½å—åˆ°å¹³å‡å¾—åˆ†300å’Œ500æ¸¸æˆæ­¥éª¤çš„é™åˆ¶ã€‚æ½œæ°´è‰‡å› ç¼ºæ°§è€Œæ­»ï¼Œéšæœºçš„è¡¨é¢è®¿é—®è¿‡äºç¨€å°‘ï¼Œä»¥è‡³äºæ— æ³•å‘ç°æ¸¸æˆå¯ä»¥ç©å¾—æ›´ä¹…ã€‚åŒæ—¶ï¼Œä»æœªè§è¿‡è¿™ä¸ªæ¸¸æˆçš„äººèƒ½å¤Ÿåœ¨å‡ åˆ†é’Ÿçš„æ¸¸æˆæ—¶é—´é‡Œæ‰¾å‡ºå¦‚ä½•è¡¥å……æ°§æ°”å¹¶æ‹¯æ•‘æ½œæ°´å‘˜ã€‚
- en: Potentially, we could help our agent and explain somehow why oxygen is important
    by adding it to the reward function (as an extra reward for refilling the oxygen,
    for example), but it might start the vicious circle of tweaking the environment
    here and there â€“ exactly those efforts weâ€™ve tried to avoid by using RL methods.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æ½œåœ¨åœ°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å°†æ°§æ°”çº³å…¥å¥–åŠ±å‡½æ•°ï¼ˆä¾‹å¦‚ä½œä¸ºè¡¥å……æ°§æ°”çš„é¢å¤–å¥–åŠ±ï¼‰æ¥å¸®åŠ©æˆ‘ä»¬çš„æ™ºèƒ½ä½“ï¼Œå¹¶ä»¥æŸç§æ–¹å¼è§£é‡Šæ°§æ°”ä¸ºä½•é‡è¦ï¼Œä½†è¿™å¯èƒ½ä¼šå¼•å‘ç¯å¢ƒè°ƒæ•´çš„æ¶æ€§å¾ªç¯â€”â€”æ­£æ˜¯æˆ‘ä»¬é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ æ–¹æ³•æ‰€è¯•å›¾é¿å…çš„é‚£äº›åŠªåŠ›ã€‚
- en: And, as you might already have guessed, RLHF is exactly the approach that allows
    us to avoid this low-level reward function tweaking, allowing humans to give feedback
    to the agentâ€™s behavior.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€æ–™ï¼ŒRLHFæ­£æ˜¯èƒ½å¤Ÿè®©æˆ‘ä»¬é¿å…è¿™ç§ä½çº§å¥–åŠ±å‡½æ•°è°ƒæ•´çš„æ–¹æ³•ï¼Œä½¿å¾—äººç±»èƒ½å¤Ÿå¯¹æ™ºèƒ½ä½“çš„è¡Œä¸ºæä¾›åé¦ˆã€‚
- en: Theoretical background
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç†è®ºèƒŒæ™¯
- en: Letâ€™s take a look at the original RLHF method published in 2017 by OpenAI and
    Google researchers [[Chr+17](#)]. Since the publication (and especially after
    ChatGPTâ€™s release), this method has been an area of active research. For recent
    developments, you can the check papers at [https://github.com/opendilab/awesome-RLHF](https://github.com/opendilab/awesome-RLHF).
    In addition, weâ€™ll discuss the role of RLHF in the LLM training process.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹OpenAIå’ŒGoogleç ”ç©¶äººå‘˜åœ¨2017å¹´å‘å¸ƒçš„åŸå§‹RLHFæ–¹æ³•[[Chr+17](#)]ã€‚è‡ªä»è¿™ç¯‡è®ºæ–‡å‘å¸ƒåï¼ˆå°¤å…¶æ˜¯åœ¨ChatGPTå‘å¸ƒä¹‹åï¼‰ï¼Œè¯¥æ–¹æ³•æˆä¸ºäº†ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸã€‚æœ‰å…³æœ€æ–°çš„è¿›å±•ï¼Œä½ å¯ä»¥æŸ¥çœ‹[https://github.com/opendilab/awesome-RLHF](https://github.com/opendilab/awesome-RLHF)ä¸Šçš„è®ºæ–‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å°†è®¨è®ºRLHFåœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚
- en: Method overview
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ–¹æ³•æ¦‚è¿°
- en: 'The authors of the paper experimented with two classes of problems: several
    environments from MuJoCo simulated robotics (similar to the continuous control
    problems we discussed in ChapterÂ [15](ch019.xhtml#x1-27200015) and ChapterÂ [16](ch020.xhtml#x1-29000016))
    and several Atari games.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è®ºæ–‡çš„ä½œè€…å®éªŒäº†ä¸¤ç±»é—®é¢˜ï¼šå‡ ç§æ¥è‡ªMuJoCoæ¨¡æ‹Ÿæœºå™¨äººç¯å¢ƒï¼ˆç±»ä¼¼äºæˆ‘ä»¬åœ¨ç¬¬[15](ch019.xhtml#x1-27200015)ç« å’Œç¬¬[16](ch020.xhtml#x1-29000016)ç« è®¨è®ºçš„è¿ç»­æ§åˆ¶é—®é¢˜ï¼‰å’Œå‡ ç§Atariæ¸¸æˆã€‚
- en: The core idea is to keep the original RL model, but replace the reward from
    the environment with a neural network called reward predictor, which is trained
    on data gathered by humans. This network (represented as rÌ‚ (o,a) in the paper
    ) takes the observation and the action and returns the float value of immediate
    reward for the action.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¸å¿ƒæ€æƒ³æ˜¯ä¿æŒåŸå§‹çš„å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œä½†ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ›¿ä»£æ¥è‡ªç¯å¢ƒçš„å¥–åŠ±ï¼Œè¿™ä¸ªç¥ç»ç½‘ç»œå«åšå¥–åŠ±é¢„æµ‹å™¨ï¼Œå®ƒæ˜¯é€šè¿‡äººç±»æ”¶é›†çš„æ•°æ®è¿›è¡Œè®­ç»ƒçš„ã€‚è¿™ä¸ªç½‘ç»œï¼ˆåœ¨è®ºæ–‡ä¸­è¡¨ç¤ºä¸ºrÌ‚(o,a)ï¼‰æ¥å—è§‚å¯Ÿå’ŒåŠ¨ä½œï¼Œå¹¶è¿”å›è¯¥åŠ¨ä½œçš„å³æ—¶å¥–åŠ±æµ®åŠ¨å€¼ã€‚
- en: 'The training data for this reward predictor is not provided directly by humans,
    but deducted from human preferences: people are shown two short video clips with
    examples of the agentâ€™s behavior and asked the question â€œWhich one is better?â€.
    In other words, the training data for the reward predictor is two episode segments
    ÏƒÂ¹ and ÏƒÂ² (fixed-length sequences of (o[t],a[t]) with observations and actions)
    and label Î¼ from the human indicating which of the two is preferred. The given
    answer options are â€œfirst,â€ â€œsecond,â€ â€œboth are good,â€ and â€œcannot judge.â€'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å¥–åŠ±é¢„æµ‹å™¨çš„è®­ç»ƒæ•°æ®å¹¶éç›´æ¥ç”±äººç±»æä¾›ï¼Œè€Œæ˜¯ä»äººç±»åå¥½ä¸­æ¨æ–­å‡ºæ¥ï¼šäººä»¬ä¼šçœ‹åˆ°ä¸¤ä¸ªçŸ­è§†é¢‘ç‰‡æ®µï¼Œå…¶ä¸­å±•ç¤ºäº†æ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œå¹¶è¢«é—®åˆ°â€œå“ªä¸€ä¸ªæ›´å¥½ï¼Ÿâ€æ¢å¥è¯è¯´ï¼Œå¥–åŠ±é¢„æµ‹å™¨çš„è®­ç»ƒæ•°æ®æ˜¯ä¸¤ä¸ªæƒ…èŠ‚ç‰‡æ®µ
    ÏƒÂ¹ å’Œ ÏƒÂ²ï¼ˆåŒ…å«è§‚å¯Ÿå’ŒåŠ¨ä½œçš„å›ºå®šé•¿åº¦åºåˆ— (o[t],a[t])) å’Œæ¥è‡ªäººç±»çš„æ ‡ç­¾ Î¼ï¼Œè¡¨ç¤ºå“ªä¸ªç‰‡æ®µæ›´å—åå¥½ã€‚ç»™å®šçš„ç­”æ¡ˆé€‰é¡¹æœ‰â€œç¬¬ä¸€ä¸ªâ€ï¼Œâ€œç¬¬äºŒä¸ªâ€ï¼Œâ€œä¸¤ä¸ªéƒ½å¥½â€å’Œâ€œæ— æ³•åˆ¤æ–­â€ã€‚
- en: 'The network rÌ‚ (o,a) is trained from this data using cross-entropy loss between
    labels and the function pÌ‚[ÏƒÂ¹ â‰»ÏƒÂ²], which is an estimation of the probability
    of the human preferring segment ÏƒÂ¹ over ÏƒÂ²:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œ rÌ‚ (o,a) æ˜¯é€šè¿‡ä½¿ç”¨æ ‡ç­¾ä¸å‡½æ•° pÌ‚[ÏƒÂ¹ â‰»ÏƒÂ²] ä¹‹é—´çš„äº¤å‰ç†µæŸå¤±æ¥è®­ç»ƒçš„ï¼ŒpÌ‚[ÏƒÂ¹ â‰»ÏƒÂ²] æ˜¯å¯¹äººç±»åå¥½ ÏƒÂ¹ ç›¸è¾ƒäº ÏƒÂ² çš„æ¦‚ç‡çš„ä¼°è®¡ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq71.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq71.png)'
- en: 'In other words, we sum the rewards predicted for every step in the segment,
    exponentiate every reward, and normalize the sum. The cross-entropy loss is calculated
    using the standard formula for the binary classification:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¯¹ç‰‡æ®µä¸­çš„æ¯ä¸€æ­¥é¢„æµ‹å¥–åŠ±è¿›è¡Œæ±‚å’Œï¼Œå–æ¯ä¸ªå¥–åŠ±çš„æŒ‡æ•°ï¼Œç„¶åå¯¹æ€»å’Œè¿›è¡Œå½’ä¸€åŒ–ã€‚äº¤å‰ç†µæŸå¤±æ˜¯ä½¿ç”¨äºŒåˆ†ç±»çš„æ ‡å‡†å…¬å¼è®¡ç®—çš„ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq72.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq72.png)'
- en: 'Values for Î¼[1] and Î¼[2] are assigned based on the humanâ€™s judgement. If the
    first segment was preferred over the second, then Î¼[1] = 1 and Î¼[2] = 0\. If the
    second segment was better, then Î¼[2] = 1 and Î¼[1] = 0\. If the human decided that
    both segments are good, then both Î¼ are set to 0.5\. Such a reward model has several
    benefits in comparison to different approaches:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Î¼[1] å’Œ Î¼[2] çš„å€¼æ˜¯æ ¹æ®äººç±»çš„åˆ¤æ–­åˆ†é…çš„ã€‚å¦‚æœç¬¬ä¸€ä¸ªç‰‡æ®µæ¯”ç¬¬äºŒä¸ªç‰‡æ®µæ›´å—åå¥½ï¼Œåˆ™ Î¼[1] = 1ï¼ŒÎ¼[2] = 0ã€‚è‹¥ç¬¬äºŒä¸ªç‰‡æ®µæ›´å¥½ï¼Œåˆ™ Î¼[2]
    = 1ï¼ŒÎ¼[1] = 0ã€‚å¦‚æœäººç±»è®¤ä¸ºä¸¤ä¸ªç‰‡æ®µéƒ½å¾ˆå¥½ï¼Œåˆ™ä¸¤ä¸ª Î¼ éƒ½è®¾ç½®ä¸º 0.5ã€‚ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”ï¼Œè¿™ç§å¥–åŠ±æ¨¡å‹æœ‰å‡ ä¸ªä¼˜ç‚¹ï¼š
- en: By using a neural network for reward prediction, we can significantly reduce
    the required number of labels. An extreme case would be to ask humans to label
    every action of the policy, but this is prohibitively expensive in the case of
    RL, where millions of interactions take place within the environment. In the case
    of high-level goals, this might be an almost impossible thing to do.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå¥–åŠ±é¢„æµ‹ï¼Œæˆ‘ä»¬å¯ä»¥æ˜¾è‘—å‡å°‘æ‰€éœ€çš„æ ‡ç­¾æ•°é‡ã€‚æç«¯æƒ…å†µä¸‹ï¼Œå¯èƒ½è¦æ±‚äººç±»æ ‡æ³¨ç­–ç•¥çš„æ¯ä¸ªåŠ¨ä½œï¼Œä½†åœ¨å¼ºåŒ–å­¦ä¹ çš„æƒ…å†µä¸‹ï¼Œè¿™æ˜¯ä¸å¯è¡Œçš„ï¼Œå› ä¸ºåœ¨ç¯å¢ƒä¸­ä¼šæœ‰æ•°ç™¾ä¸‡æ¬¡äº¤äº’å‘ç”Ÿã€‚åœ¨é«˜å±‚ç›®æ ‡çš„æƒ…å†µä¸‹ï¼Œè¿™å‡ ä¹æ˜¯ä¸å¯èƒ½å®Œæˆçš„ä»»åŠ¡ã€‚
- en: We give the network feedback not only about good behavior but also about behavior
    that we donâ€™t like. If you remember, in ChapterÂ [14](ch018.xhtml#x1-24700014),
    we used the recorded human demonstrations to train the web automation agent. But
    human demonstrations only show positive examples (â€œdo thisâ€) and have no way of
    including negative examples (â€œdonâ€™t do thatâ€). In addition, human demonstrations
    are harder to collect and might contain more errors.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä»…ç»™ç½‘ç»œåé¦ˆå¥½çš„è¡Œä¸ºï¼Œè¿˜ç»™å®ƒåé¦ˆæˆ‘ä»¬ä¸å–œæ¬¢çš„è¡Œä¸ºã€‚å¦‚æœä½ è®°å¾—ï¼Œåœ¨ç¬¬[14](ch018.xhtml#x1-24700014)ç« ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è®°å½•ä¸‹æ¥çš„äººå·¥ç¤ºèŒƒæ¥è®­ç»ƒç½‘ç»œè‡ªåŠ¨åŒ–ä»£ç†ã€‚ä½†äººå·¥ç¤ºèŒƒåªå±•ç¤ºäº†æ­£é¢ä¾‹å­ï¼ˆâ€œåšè¿™ä¸ªâ€ï¼‰ï¼Œæ²¡æœ‰åŠæ³•åŒ…å«è´Ÿé¢ä¾‹å­ï¼ˆâ€œä¸è¦åšé‚£ä¸ªâ€ï¼‰ã€‚æ­¤å¤–ï¼Œäººå·¥ç¤ºèŒƒæ›´éš¾æ”¶é›†ï¼Œä¸”å¯èƒ½åŒ…å«æ›´å¤šçš„é”™è¯¯ã€‚
- en: By asking for human preferences, we can handle problems where humans can recognize
    the behavior we want, but not necessarily reproduce it. For example, controlling
    the four-legged Ant robot from ChapterÂ [16](ch020.xhtml#x1-29000016) might be
    very challenging for humans. At the same time, we donâ€™t have problems detecting
    when the robot is behaving normally or the policy is wrong.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡è¯¢é—®äººç±»åå¥½ï¼Œæˆ‘ä»¬å¯ä»¥å¤„ç†é‚£äº›äººç±»èƒ½å¤Ÿè¯†åˆ«æˆ‘ä»¬æƒ³è¦çš„è¡Œä¸ºï¼Œä½†ä¸ä¸€å®šèƒ½å¤åˆ¶çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼Œæ§åˆ¶ç¬¬[16](ch020.xhtml#x1-29000016)ç« ä¸­çš„å››è¶³èš‚èšæœºå™¨äººå¯¹äººç±»æ¥è¯´å¯èƒ½éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿæ²¡æœ‰æ£€æµ‹å‡ºæœºå™¨äººè¡Œä¸ºæ­£å¸¸æˆ–ç­–ç•¥é”™è¯¯æ—¶çš„å›°éš¾ã€‚
- en: 'In the RLHF paper, the authors experimented with different approaches to the
    reward model training and its usage in the RL training process. In their setup,
    three different processes were running in parallel:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ RLHF è®ºæ–‡ä¸­ï¼Œä½œè€…å®éªŒäº†ä¸åŒçš„å¥–åŠ±æ¨¡å‹è®­ç»ƒæ–¹æ³•åŠå…¶åœ¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä½¿ç”¨ã€‚åœ¨ä»–ä»¬çš„è®¾ç½®ä¸­ï¼Œä¸‰ç§ä¸åŒçš„è¿‡ç¨‹åŒæ—¶è¿è¡Œï¼š
- en: The RL training method (A2C) used the current rÌ‚ (o,a) network for reward prediction.
    Random trajectory segments Ïƒ = (o[i],a[i]) were stored in the labeling database.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨çš„ RL è®­ç»ƒæ–¹æ³•ï¼ˆA2Cï¼‰ä½¿ç”¨å½“å‰çš„ rÌ‚ (o, a) ç½‘ç»œè¿›è¡Œå¥–åŠ±é¢„æµ‹ã€‚éšæœºè½¨è¿¹æ®µ Ïƒ = (o[i], a[i]) è¢«å­˜å‚¨åœ¨æ ‡æ³¨æ•°æ®åº“ä¸­ã€‚
- en: Human labelers sampled pairs of segments (ÏƒÂ¹,ÏƒÂ²) and assigned their labels Î¼,
    which were stored in the labeling database.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: äººç±»æ ‡æ³¨è€…é‡‡æ ·äº†ä¸€å¯¹æ®µè½ï¼ˆÏƒÂ¹, ÏƒÂ²ï¼‰ï¼Œå¹¶ä¸ºå…¶åˆ†é…æ ‡ç­¾ Î¼ï¼Œæ ‡ç­¾è¢«å­˜å‚¨åœ¨æ ‡æ³¨æ•°æ®åº“ä¸­ã€‚
- en: The reward model rÌ‚ (o,a) was periodically trained on labeled pairs from the
    database and sent to the RL training process.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¥–åŠ±æ¨¡å‹ rÌ‚ (o, a) ä¼šå®šæœŸåœ¨æ¥è‡ªæ•°æ®åº“çš„æ ‡æ³¨å¯¹ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶å‘é€åˆ° RL è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚
- en: This process is shown in FigureÂ [19.1](#x1-352009r1).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥è¿‡ç¨‹å¦‚å›¾[19.1](#x1-352009r1)æ‰€ç¤ºã€‚
- en: '![ DB: TTserrgaaminiennts 11 22 rË†rÏƒÏƒÏƒÎ¼Rewla(Labo,,re,aÏƒÏƒdls),Î¼ ](img/B22150_19_01.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![ DB: TTserrgaaminiennts 11 22 rË†rÏƒÏƒÏƒÎ¼Rewla(Labo,,re,aÏƒÏƒdls),Î¼ ](img/B22150_19_01.png)'
- en: 'FigureÂ 19.1: RLHF structure'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 19.1: RLHF ç»“æ„'
- en: 'As discussed earlier, the paper addressed two classes of problems: Atari games
    and continuous control. On both classes, the results were not especially spectacular
    â€” sometimes traditional RL was better than RLHF, sometimes not. But where RLHF
    really stood out was the LLM training pipeline. Letâ€™s briefly discuss why it happened
    before we start our RLHF experiments.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæœ¬æ–‡è®¨è®ºäº†ä¸¤ç±»é—®é¢˜ï¼šAtari æ¸¸æˆå’Œè¿ç»­æ§åˆ¶ã€‚åœ¨è¿™ä¸¤ç±»é—®é¢˜ä¸Šï¼Œç»“æœå¹¶ä¸ç‰¹åˆ«æ˜¾è‘—â€”â€”æœ‰æ—¶ä¼ ç»Ÿçš„ RL æ¯” RLHF æ›´å¥½ï¼Œæœ‰æ—¶åˆ™ç›¸åã€‚ä½† RLHF
    çœŸæ­£çªå‡ºçš„åœ°æ–¹æ˜¯åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è®­ç»ƒæµç¨‹ä¸­ã€‚æˆ‘ä»¬åœ¨å¼€å§‹ RLHF å®éªŒä¹‹å‰ï¼Œç®€è¦è®¨è®ºä¸€ä¸‹ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚
- en: RLHF and LLMs
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RLHF å’Œ LLMs
- en: 'ChatGPT, released at the end of 2022, has quickly become a really big thing.
    For the general audience, it was even more influential than AlexNet in 2012, as
    AlexNet was â€œtechy stuffâ€â€”it pushed the boundaries but it was much harder to explain
    what was so special about it. ChatGPT was different: just a month after release,
    it had surpassed a user base of 100M users and almost everybody was talking about
    it.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT äº 2022 å¹´åº•å‘å¸ƒï¼Œå¾ˆå¿«æˆä¸ºäº†ä¸€ä¸ªå¤§çƒ­è¯é¢˜ã€‚å¯¹äºæ™®é€šç”¨æˆ·æ¥è¯´ï¼Œå®ƒç”šè‡³æ¯” 2012 å¹´çš„ AlexNet è¿˜è¦æœ‰å½±å“åŠ›ï¼Œå› ä¸º AlexNet
    æ˜¯â€œæŠ€æœ¯æ€§çš„ä¸œè¥¿â€â€”â€”å®ƒæ¨åŠ¨äº†è¾¹ç•Œï¼Œä½†å¾ˆéš¾è§£é‡Šå®ƒåˆ°åº•æœ‰å¤šç‰¹åˆ«ã€‚ChatGPT ä¸ä¸€æ ·ï¼šå‘å¸ƒä»…ä¸€ä¸ªæœˆåï¼Œå®ƒçš„ç”¨æˆ·æ•°é‡å°±çªç ´äº† 1 äº¿ï¼Œè€Œå‡ ä¹æ¯ä¸ªäººéƒ½åœ¨è°ˆè®ºå®ƒã€‚
- en: At the heart of ChatGPT (and any modern LLM) training pipeline is RLHF. So,
    very quickly, this method of fine-tuning large models has become popular and has
    grown in terms of research interest. As this is not a book about LLMs, I will
    just give a quick description of the pipeline and how RLHF is incorporated there,
    as, from my perspective, this is an interesting use case.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPTï¼ˆä»¥åŠä»»ä½•ç°ä»£ LLMï¼‰è®­ç»ƒæµç¨‹çš„æ ¸å¿ƒæ˜¯ RLHFã€‚å› æ­¤ï¼Œè¿™ç§å¾®è°ƒå¤§æ¨¡å‹çš„æ–¹æ³•è¿…é€Ÿæµè¡Œå¼€æ¥ï¼Œå¹¶ä¸”åœ¨ç ”ç©¶å…´è¶£ä¸Šä¹Ÿæœ‰æ‰€å¢é•¿ã€‚ç”±äºè¿™ä¸æ˜¯ä¸€æœ¬å…³äº
    LLM çš„ä¹¦ï¼Œæˆ‘å°†ç®€è¦æè¿°è¯¥æµç¨‹ä»¥åŠ RLHF æ˜¯å¦‚ä½•èå…¥å…¶ä¸­çš„ï¼Œå› ä¸ºä»æˆ‘çš„è§’åº¦æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„åº”ç”¨æ¡ˆä¾‹ã€‚
- en: 'From a high level, LLM training consists of three stages:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼ŒLLM è®­ç»ƒç”±ä¸‰ä¸ªé˜¶æ®µç»„æˆï¼š
- en: 'Pretraining: Here, we perform the initial training of the language model on
    a huge corpus of texts. Basically, we take all the information we can possibly
    get and do unsupervised training of the language model. The volume (and costs)
    are enormous â€” the RedPajama dataset used for LLaMA training contains 1.2 trillion
    tokens (which is approximately 15 million books).'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒï¼šåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åœ¨ä¸€ä¸ªåºå¤§çš„æ–‡æœ¬è¯­æ–™åº“ä¸Šå¯¹è¯­è¨€æ¨¡å‹è¿›è¡Œåˆæ­¥è®­ç»ƒã€‚åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬ä¼šå°½å¯èƒ½è·å–æ‰€æœ‰çš„ä¿¡æ¯å¹¶è¿›è¡Œæ— ç›‘ç£çš„è¯­è¨€æ¨¡å‹è®­ç»ƒã€‚æ•°æ®é‡ï¼ˆåŠå…¶æˆæœ¬ï¼‰æ˜¯å·¨å¤§çš„â€”â€”ç”¨äº
    LLaMA è®­ç»ƒçš„ RedPajama æ•°æ®é›†åŒ…å« 1.2 ä¸‡äº¿ä¸ªæ ‡è®°ï¼ˆå¤§çº¦ç›¸å½“äº 1500 ä¸‡æœ¬ä¹¦ï¼‰ã€‚
- en: At this stage, our randomly-initialized model learns regularities and deep connections
    of the language. But because the data volume is huge, we cannot just curate this
    data â€” it could be fake news, hate speech posts, and other weird stuff you can
    easily find on the internet.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬çš„éšæœºåˆå§‹åŒ–æ¨¡å‹å­¦ä¹ è¯­è¨€çš„è§„å¾‹æ€§å’Œæ·±å±‚æ¬¡çš„è”ç³»ã€‚ä½†ç”±äºæ•°æ®é‡åºå¤§ï¼Œæˆ‘ä»¬ä¸èƒ½ä»…ä»…æŒ‘é€‰è¿™äº›æ•°æ®â€”â€”å®ƒä»¬å¯èƒ½æ˜¯å‡æ–°é—»ã€ä»‡æ¨è¨€è®ºå¸–å­ï¼Œæˆ–æ˜¯ä½ åœ¨äº’è”ç½‘ä¸Šéšä¾¿å¯ä»¥æ‰¾åˆ°çš„å…¶ä»–æ€ªå¼‚å†…å®¹ã€‚
- en: 'Supervised fine-tuning: In this step, we fine-tune the model on predefined
    curated example dialogues. The dataset used here is manually created and validated
    for correctness and the volume is significantly lower â€” around 10Kâ€“100K example
    dialogues.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç›‘ç£å¾®è°ƒï¼šåœ¨è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬ä¼šåœ¨é¢„å®šä¹‰çš„ç²¾é€‰ç¤ºä¾‹å¯¹è¯ä¸Šå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚æ­¤å¤„ä½¿ç”¨çš„æ•°æ®é›†æ˜¯æ‰‹åŠ¨åˆ›å»ºå¹¶éªŒè¯æ­£ç¡®æ€§çš„ï¼Œæ•°æ®é‡æ˜¾è‘—è¾ƒå°â€”â€”å¤§çº¦ä¸º 10K-100K
    ä¸ªç¤ºä¾‹å¯¹è¯ã€‚
- en: This data is normally created by experts in the field and requires lots of effort
    to make and double-check it.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™äº›æ•°æ®é€šå¸¸ç”±è¯¥é¢†åŸŸçš„ä¸“å®¶åˆ›å»ºï¼Œéœ€è¦å¤§é‡çš„ç²¾åŠ›æ¥åˆ¶ä½œå¹¶è¿›è¡Œå¤æ ¸ã€‚
- en: 'RLHF fine-tuning (also known as â€model alignmentâ€): This step uses the same
    process weâ€™ve already described: pairs of generated dialogues are presented to
    users for labeling, the reward model is trained on those labels, and this reward
    model is used in the RL algorithm to fine-tune the LLM model to follow the humanâ€™s
    preferences. The number of labeled samples is larger than on the supervised fine-tuning
    step (around 1M pairs), but because comparing two dialogues is a much simpler
    task than creating a proper dialogue from scratch, this is not a problem.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RLHF å¾®è°ƒï¼ˆä¹Ÿç§°ä¸ºâ€œæ¨¡å‹å¯¹é½â€ï¼‰ï¼šè¿™ä¸€æ­¥ä½¿ç”¨äº†æˆ‘ä»¬å·²ç»æè¿°è¿‡çš„ç›¸åŒè¿‡ç¨‹ï¼šç”Ÿæˆçš„å¯¹è¯å¯¹å‘ˆç°ç»™ç”¨æˆ·è¿›è¡Œæ ‡æ³¨ï¼Œå¥–åŠ±æ¨¡å‹åŸºäºè¿™äº›æ ‡ç­¾è¿›è¡Œè®­ç»ƒï¼Œå¹¶åœ¨ RL ç®—æ³•ä¸­ä½¿ç”¨è¿™ä¸ªå¥–åŠ±æ¨¡å‹æ¥å¾®è°ƒ
    LLM æ¨¡å‹ï¼Œä½¿å…¶éµå¾ªäººç±»çš„åå¥½ã€‚æ ‡æ³¨æ ·æœ¬çš„æ•°é‡æ¯”ç›‘ç£å¾®è°ƒæ­¥éª¤è¦å¤šï¼ˆå¤§çº¦ 1M å¯¹ï¼‰ï¼Œä½†å› ä¸ºæ¯”è¾ƒä¸¤ä¸ªå¯¹è¯è¦æ¯”ä»å¤´å¼€å§‹åˆ›å»ºä¸€ä¸ªåˆé€‚çš„å¯¹è¯ç®€å•å¾—å¤šï¼Œæ‰€ä»¥è¿™ä¸æ˜¯é—®é¢˜ã€‚
- en: 'As you might guess, the first step is the most expensive and lengthy: you have
    to crunch terabytes of texts and feed them through transformers. But at the same
    time, the importance of the steps is totally different. In the last step, the
    system not only learns what the best solution to the presented problem is but
    also has feedback about generating it in a socially acceptable way.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ å¯èƒ½çŒœåˆ°çš„ï¼Œç¬¬ä¸€æ­¥æ˜¯æœ€è€—è´¹èµ„æºå’Œæ—¶é—´çš„ï¼šä½ å¿…é¡»å¤„ç†å¤§é‡çš„æ–‡æœ¬å¹¶é€šè¿‡å˜æ¢å™¨è¿›è¡Œå¤„ç†ã€‚ä½†åŒæ—¶ï¼Œè¿™äº›æ­¥éª¤çš„é‡è¦æ€§æ˜¯å®Œå…¨ä¸åŒçš„ã€‚åœ¨æœ€åä¸€æ­¥ï¼Œç³»ç»Ÿä¸ä»…å­¦ä¹ å¦‚ä½•è§£å†³å‘ˆç°çš„é—®é¢˜ï¼Œè¿˜ä¼šå¾—åˆ°ç”Ÿæˆé—®é¢˜ç­”æ¡ˆæ—¶æ˜¯å¦ç¬¦åˆç¤¾ä¼šæ¥å—æ–¹å¼çš„åé¦ˆã€‚
- en: The RLHF method is very suitable for this task â€” with just pairs of dialogues,
    it can learn the reward model that represents the labelersâ€™ implicit â€œpreference
    modelâ€ for such a complicated thing as the chatbot. Doing this explicitly (via
    the reward function, for example) might be a very challenging problem with lots
    of uncertainty.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF æ–¹æ³•éå¸¸é€‚åˆè¿™ä¸ªä»»åŠ¡â€”â€”åªéœ€è¦ä¸€å¯¹å¯¹è¯ï¼Œå®ƒå°±èƒ½å­¦ä¹ ä»£è¡¨æ ‡æ³¨è€…éšå¼â€œåå¥½æ¨¡å‹â€çš„å¥–åŠ±æ¨¡å‹ï¼Œåº”ç”¨äºåƒèŠå¤©æœºå™¨äººè¿™æ ·å¤æ‚çš„äº‹ç‰©ã€‚æ˜¾å¼åœ°åšè¿™ä»¶äº‹ï¼ˆä¾‹å¦‚é€šè¿‡å¥–åŠ±å‡½æ•°ï¼‰å¯èƒ½æ˜¯ä¸€ä¸ªå…·æœ‰å¾ˆå¤§ä¸ç¡®å®šæ€§çš„æŒ‘æˆ˜æ€§é—®é¢˜ã€‚
- en: RLHF experiments
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RLHF å®éªŒ
- en: To get a better understanding of the pipeline weâ€™ve just discussed, letâ€™s implement
    it ourselves (as â€œdoing is the best way to learn somethingâ€). In the previous
    chapter, we tried the Atari SeaQuest environment, which is tricky from the exploration
    point of view, so it is logical to take this environment and check what we can
    achieve with human feedback.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°ç†è§£æˆ‘ä»¬åˆšæ‰è®¨è®ºçš„æµç¨‹ï¼Œè®©æˆ‘ä»¬è‡ªå·±åŠ¨æ‰‹å®ç°å®ƒï¼ˆå› ä¸ºâ€œåšæ˜¯æœ€å¥½çš„å­¦ä¹ æ–¹æ³•â€ï¼‰ã€‚åœ¨ä¸Šä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°è¯•äº† Atari SeaQuest ç¯å¢ƒï¼Œä»æ¢ç´¢è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªç¯å¢ƒæœ‰ä¸€å®šéš¾åº¦ï¼Œå› æ­¤åˆ©ç”¨è¿™ä¸ªç¯å¢ƒå¹¶æ£€æŸ¥æˆ‘ä»¬èƒ½é€šè¿‡äººç±»åé¦ˆå–å¾—ä»€ä¹ˆæˆå°±æ˜¯åˆä¹é€»è¾‘çš„ã€‚
- en: 'To limit the scope of the chapter and make the example more reproducible, I
    made the following modifications to the experiments described in the RLHF paper
    [[Chr+17](#)]:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é™åˆ¶æœ¬ç« çš„èŒƒå›´å¹¶ä½¿ä¾‹å­æ›´å…·å¯å¤ç°æ€§ï¼Œæˆ‘å¯¹ RLHF è®ºæ–‡ [[Chr+17](#)] ä¸­æè¿°çš„å®éªŒè¿›è¡Œäº†ä»¥ä¸‹ä¿®æ”¹ï¼š
- en: I focused on a single SeaQuest environment. The goal was to improve the agentâ€™s
    gameplay in comparison to the A2C results we got in ChapterÂ [18](ch022.xhtml#x1-32800018)
    â€” an average score of 400 and episodes of 500 steps (due to the lack of oxygen).
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä¸“æ³¨äºå•ä¸€çš„ SeaQuest ç¯å¢ƒã€‚ç›®æ ‡æ˜¯æé«˜ä»£ç†åœ¨ä¸ç¬¬ [18](ch022.xhtml#x1-32800018) ç« ä¸­ A2C ç»“æœçš„å¯¹æ¯”ä¸­çš„æ¸¸æˆè¡¨ç°â€”â€”å¹³å‡å¾—åˆ†ä¸º
    400ï¼Œå›åˆæ­¥æ•°ä¸º 500 æ­¥ï¼ˆç”±äºç¼ºæ°§ï¼‰ã€‚
- en: 'Instead of asynchronous labeling and reward model training, I split them into
    separate steps:'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘å°†å…¶ä»å¼‚æ­¥æ ‡æ³¨å’Œå¥–åŠ±æ¨¡å‹è®­ç»ƒçš„è¿‡ç¨‹ï¼Œåˆ†æˆäº†å•ç‹¬çš„æ­¥éª¤ï¼š
- en: A2C training was performed, storing trajectory segments in local files. This
    training might optionally load and use a reward model network, which would allow
    us to iterate on reward models, labeling more samples after the training.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰§è¡Œäº† A2C è®­ç»ƒï¼Œå°†è½¨è¿¹æ®µå­˜å‚¨åœ¨æœ¬åœ°æ–‡ä»¶ä¸­ã€‚æ­¤è®­ç»ƒå¯é€‰æ‹©æ€§åœ°åŠ è½½å¹¶ä½¿ç”¨å¥–åŠ±æ¨¡å‹ç½‘ç»œï¼Œè¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒåè¿­ä»£å¥–åŠ±æ¨¡å‹ï¼Œæ ‡è®°æ›´å¤šçš„æ ·æœ¬ã€‚
- en: The web UI allowed me to label random pairs of trajectory segments, storing
    the labels in a JSON file.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Web UI è®©æˆ‘å¯ä»¥ä¸ºéšæœºçš„è½¨è¿¹æ®µå¯¹æ‰“ä¸Šæ ‡ç­¾ï¼Œå¹¶å°†æ ‡ç­¾å­˜å‚¨åœ¨ä¸€ä¸ª JSON æ–‡ä»¶ä¸­ã€‚
- en: The reward model was trained on those segments and labels. The result of the
    training was stored on disk.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¥–åŠ±æ¨¡å‹åœ¨è¿™äº›æ®µè½å’Œæ ‡ç­¾ä¸Šè¿›è¡Œäº†è®­ç»ƒã€‚è®­ç»ƒç»“æœè¢«å­˜å‚¨åœ¨ç£ç›˜ä¸Šã€‚
- en: 'I avoided all the variations with the reward model training: no L2 regularization,
    no ensemble, etc.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘é¿å…äº†æ‰€æœ‰ä¸å¥–åŠ±æ¨¡å‹è®­ç»ƒç›¸å…³çš„å˜ä½“ï¼šæ²¡æœ‰ L2 æ­£åˆ™åŒ–ï¼Œæ²¡æœ‰é›†æˆæ–¹æ³•ç­‰ã€‚
- en: 'The number of labels was significantly smaller: in every experiment, I labeling
    an extra 100 pairs of episode segments and retrained the models.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ‡ç­¾çš„æ•°é‡æ˜¾è‘—å‡å°‘ï¼šåœ¨æ¯æ¬¡å®éªŒä¸­ï¼Œæˆ‘æ ‡è®°äº†é¢å¤–çš„ 100 å¯¹å›åˆæ®µï¼Œå¹¶é‡æ–°è®­ç»ƒäº†æ¨¡å‹ã€‚
- en: Actions were explicitly added to the reward model. For the details, check the
    section Reward model.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŠ¨ä½œæ˜ç¡®åœ°åŠ å…¥äº†å¥–åŠ±æ¨¡å‹ä¸­ã€‚è¯¦æƒ…è¯·å‚é˜…â€œå¥–åŠ±æ¨¡å‹â€ä¸€èŠ‚ã€‚
- en: The reward model was used in A2C training for the fine-tuning of the best mode
    saved. For context, in the paper, the model was trained from scratch and improved
    with parallel RLHF labeling and reward model retraining.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¥–åŠ±æ¨¡å‹åœ¨ A2C è®­ç»ƒä¸­ç”¨äºå¯¹ä¿å­˜çš„æœ€ä½³æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ä¸ºäº†è¯´æ˜èƒŒæ™¯ï¼Œåœ¨è®ºæ–‡ä¸­ï¼Œæ¨¡å‹æ˜¯ä»é›¶å¼€å§‹è®­ç»ƒçš„ï¼Œå¹¶é€šè¿‡å¹¶è¡Œçš„ RLHF æ ‡æ³¨å’Œå¥–åŠ±æ¨¡å‹é‡è®­ç»ƒå¾—åˆ°äº†æ”¹å–„ã€‚
- en: Initial training using A2C
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ A2C è¿›è¡Œåˆå§‹è®­ç»ƒ
- en: To get the first model (letâ€™s call it â€œversion 0â€ or v0 for short), I used standard
    A2C code with the same Atari wrappers weâ€™ve already discussed several times in
    this book so far.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·å¾—ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œç‰ˆæœ¬ 0â€æˆ–ç®€ç§° v0ï¼‰ï¼Œæˆ‘ä½¿ç”¨äº†æ ‡å‡†çš„ A2C ä»£ç ï¼Œå¹¶é…åˆæœ¬ä¹¦å‰é¢å·²ç»å¤šæ¬¡è®¨è®ºè¿‡çš„ Atari åŒ…è£…å™¨ã€‚
- en: To start the training, you need to run the Chapter19/01_a2c.py module, and besides
    basic A2C training, it contains a command-line option that enables the usage of
    the reward model (which we covered in earlier chapters), but we donâ€™t need it
    in this step.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹è®­ç»ƒï¼Œæ‚¨éœ€è¦è¿è¡Œ Chapter19/01_a2c.py æ¨¡å—ï¼Œé™¤äº†åŸºæœ¬çš„ A2C è®­ç»ƒå¤–ï¼Œå®ƒè¿˜åŒ…å«ä¸€ä¸ªå‘½ä»¤è¡Œé€‰é¡¹ï¼Œç”¨äºå¯ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆæˆ‘ä»¬åœ¨å‰é¢çš„ç« èŠ‚ä¸­ä»‹ç»è¿‡ï¼‰ï¼Œä½†åœ¨æ­¤æ­¥éª¤ä¸­æˆ‘ä»¬ä¸éœ€è¦å®ƒã€‚
- en: 'For now, to start the training of the basic model, use the following command
    line:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰ï¼Œè¦å¼€å§‹åŸºæœ¬æ¨¡å‹çš„è®­ç»ƒï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¡Œï¼š
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is the description of the command-line options:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯å‘½ä»¤è¡Œé€‰é¡¹çš„æè¿°ï¼š
- en: '--dev: The name of the device used for computation.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '--dev: ç”¨äºè®¡ç®—çš„è®¾å¤‡åç§°ã€‚'
- en: '-n: The name of the run, used in TensorBoard.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-n: è¿è¡Œçš„åç§°ï¼Œç”¨äº TensorBoardã€‚'
- en: '--save: The directory name where the best models after the testing will be
    stored. Every 100 batches, we perform 10 test episodes of the current model on
    SeaQuest with disabled reward clipping (to get the original score range) and if
    the best reward or the count of steps for any of those 10 rounds is better than
    our previous record, we save the model into the file. Those files will be used
    later for fine-tuning.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '--save: åœ¨æµ‹è¯•åå°†å­˜å‚¨æœ€ä½³æ¨¡å‹çš„ç›®å½•åç§°ã€‚æ¯è®­ç»ƒ 100 æ‰¹æ¬¡ï¼Œæˆ‘ä»¬ä¼šå¯¹å½“å‰æ¨¡å‹åœ¨ SeaQuest ä¸Šè¿›è¡Œ 10 æ¬¡æµ‹è¯•å‰§é›†ï¼Œç¦ç”¨å¥–åŠ±å‰ªåˆ‡ï¼ˆä»¥è·å–åŸå§‹åˆ†æ•°èŒƒå›´ï¼‰ï¼Œå¦‚æœè¿™
    10 è½®ä¸­çš„æœ€ä½³å¥–åŠ±æˆ–æ­¥éª¤æ•°è¶…è¿‡æˆ‘ä»¬ä¹‹å‰çš„è®°å½•ï¼Œæˆ‘ä»¬ä¼šå°†æ¨¡å‹ä¿å­˜åˆ°æ–‡ä»¶ä¸­ã€‚è¿™äº›æ–‡ä»¶ç¨åå°†ç”¨äºå¾®è°ƒã€‚'
- en: '--db-path: The directory name where random episode segments will be stored
    during the training. This data will be used for the labeling and training of the
    reward model later.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '--db-path: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†å­˜å‚¨éšæœºå‰§é›†ç‰‡æ®µçš„ç›®å½•åç§°ã€‚è¿™äº›æ•°æ®ç¨åå°†ç”¨äºå¥–åŠ±æ¨¡å‹çš„æ ‡æ³¨å’Œè®­ç»ƒã€‚'
- en: 'Letâ€™s discuss the episode segments database (DB for short). Its structure is
    very simple: every environment used for training (in total, we have 16 of them)
    has an identifier from 0 to 15, which is used as a subdirectory under the directory
    given in the --db-path command-line argument. So, every environment stores random
    segments independently in its own directory. The storage logic is implemented
    in a Gym API Wrapper subclass, which is called EpisodeRecorderWrapper and is in
    the lib/rlhf.py module.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è®¨è®ºä¸€ä¸‹å‰§é›†ç‰‡æ®µæ•°æ®åº“ï¼ˆç®€ç§° DBï¼‰ã€‚å…¶ç»“æ„éå¸¸ç®€å•ï¼šæ¯ä¸ªç”¨äºè®­ç»ƒçš„ç¯å¢ƒï¼ˆæ€»å…±æœ‰ 16 ä¸ªï¼‰éƒ½æœ‰ä¸€ä¸ªä» 0 åˆ° 15 çš„æ ‡è¯†ç¬¦ï¼Œè¿™ä¸ªæ ‡è¯†ç¬¦ç”¨ä½œ --db-path
    å‘½ä»¤è¡Œå‚æ•°æ‰€ç»™å®šç›®å½•ä¸‹çš„å­ç›®å½•ã€‚å› æ­¤ï¼Œæ¯ä¸ªç¯å¢ƒéƒ½ä¼šåœ¨è‡ªå·±çš„ç›®å½•ä¸­ç‹¬ç«‹å­˜å‚¨éšæœºç‰‡æ®µã€‚å­˜å‚¨é€»è¾‘æ˜¯é€šè¿‡ Gym API Wrapper å­ç±»å®ç°çš„ï¼Œè¿™ä¸ªå­ç±»å«åš
    `EpisodeRecorderWrapper`ï¼Œä½äº lib/rlhf.py æ¨¡å—ä¸­ã€‚
- en: 'Letâ€™s take a look at the source code of the wrapper. Initially, we declare
    two hyperparameters, EPISODE_STEPS, which defines the length of segments, and
    START_PROB, which is the probability of starting the episode recording:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹åŒ…è£…å™¨çš„æºä»£ç ã€‚æœ€åˆï¼Œæˆ‘ä»¬å£°æ˜äº†ä¸¤ä¸ªè¶…å‚æ•°ï¼ŒEPISODE_STEPSï¼Œå®ƒå®šä¹‰äº†ç‰‡æ®µçš„é•¿åº¦ï¼Œä»¥åŠ START_PROBï¼Œå®ƒè¡¨ç¤ºå¼€å§‹å‰§é›†è®°å½•çš„æ¦‚ç‡ï¼š
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We store the episode segment as a list of EpisodeStep objects, which is just
    an observation and the action weâ€™re taking at this step. The method that resets
    the environment is very simple â€” it updates the wrapperâ€™s _step_idx field(which
    is a counter of the steps weâ€™ve done in this environment) and stores the observation
    in the _prev_obs field, depending on the _is_store field. This field is True if
    weâ€™re in the middle of segment recording.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å‰§é›†ç‰‡æ®µå­˜å‚¨ä¸ºä¸€ç³»åˆ— `EpisodeStep` å¯¹è±¡ï¼Œè¿™äº›å¯¹è±¡åªæ˜¯æˆ‘ä»¬åœ¨è¯¥æ­¥éª¤ä¸­æ‰€é‡‡å–çš„è§‚å¯Ÿå’ŒåŠ¨ä½œã€‚é‡ç½®ç¯å¢ƒçš„æ–¹æ³•éå¸¸ç®€å•â€”â€”å®ƒä¼šæ›´æ–°åŒ…è£…å™¨çš„
    _step_idx å­—æ®µï¼ˆè¿™æ˜¯æˆ‘ä»¬åœ¨è¯¥ç¯å¢ƒä¸­å·²æ‰§è¡Œæ­¥éª¤çš„è®¡æ•°å™¨ï¼‰ï¼Œå¹¶æ ¹æ® _is_store å­—æ®µå°†è§‚å¯Ÿå€¼å­˜å‚¨åœ¨ _prev_obs å­—æ®µä¸­ã€‚å¦‚æœ _is_store
    å­—æ®µä¸º Trueï¼Œåˆ™è¡¨ç¤ºæˆ‘ä»¬æ­£åœ¨è¿›è¡Œç‰‡æ®µè®°å½•ã€‚
- en: 'Our segments have a fixed number of environment steps (50 by default) and they
    are recorded independent of episode boundaries (in other words, if we started
    the segment recording shortly before the submarineâ€™s death, weâ€™ll record the beginning
    of the next episode after the reset() method):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç‰‡æ®µæœ‰å›ºå®šæ•°é‡çš„ç¯å¢ƒæ­¥éª¤ï¼ˆé»˜è®¤ä¸º 50 æ­¥ï¼‰ï¼Œå®ƒä»¬ç‹¬ç«‹äºå‰§é›†è¾¹ç•Œè¿›è¡Œè®°å½•ï¼ˆæ¢å¥è¯è¯´ï¼Œå¦‚æœæˆ‘ä»¬åœ¨æ½œè‰‡æ­»äº¡å‰ä¸ä¹…å¼€å§‹ç‰‡æ®µè®°å½•ï¼Œé‚£ä¹ˆåœ¨è°ƒç”¨ reset()
    æ–¹æ³•åï¼Œæˆ‘ä»¬ä¼šè®°å½•ä¸‹ä¸€å‰§é›†çš„å¼€å§‹ï¼‰ï¼š
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you want, you can experiment with this logic as, in principle, observations
    after the end of the episode are independent from observations and actions before
    the end of the episode. But it will make the handling of episode segment data
    more complicated, as the length will become variable.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æ„¿æ„ï¼Œä½ å¯ä»¥å°è¯•è¿™ç§é€»è¾‘ï¼Œå› ä¸ºåŸåˆ™ä¸Šï¼Œå‰§é›†ç»“æŸåçš„è§‚å¯Ÿæ•°æ®ä¸å‰§é›†ç»“æŸå‰çš„è§‚å¯Ÿå’ŒåŠ¨ä½œæ˜¯ç‹¬ç«‹çš„ã€‚ä½†è¿™æ ·ä¼šä½¿å‰§é›†ç‰‡æ®µæ•°æ®çš„å¤„ç†æ›´å¤æ‚ï¼Œå› ä¸ºæ•°æ®é•¿åº¦å°†å˜å¾—å¯å˜ã€‚
- en: 'The main logic of the wrapper is in the step() method and it is also not very
    complicated. On every action, we store the step if weâ€™re in the middle of recording;
    otherwise, we generate a random number to make the decision to start the recording:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ…è£…å™¨çš„ä¸»è¦é€»è¾‘åœ¨step()æ–¹æ³•ä¸­ï¼Œä¹Ÿä¸æ˜¯å¾ˆå¤æ‚ã€‚æ¯æ¬¡åŠ¨ä½œæ—¶ï¼Œå¦‚æœæˆ‘ä»¬æ­£åœ¨å½•åˆ¶ï¼Œå°±å­˜å‚¨è¯¥æ­¥éª¤ï¼›å¦åˆ™ï¼Œæˆ‘ä»¬ä¼šç”Ÿæˆä¸€ä¸ªéšæœºæ•°æ¥å†³å®šæ˜¯å¦å¼€å§‹å½•åˆ¶ï¼š
- en: '[PRE3]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: By default, the probability of starting the recording is small (START_PROB =
    0.00005, which is a 0.005% chance), but because of the large number of steps weâ€™re
    doing during the training, we still have plenty of segments to label. For example,
    after 12M environment steps (about 5 hours of training), the database contains
    2,500 recorded segments, which occupy 12GB of disk.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œå¼€å§‹å½•åˆ¶çš„æ¦‚ç‡å¾ˆå°ï¼ˆSTART_PROB = 0.00005ï¼Œå³0.005%çš„å‡ ç‡ï¼‰ï¼Œä½†ç”±äºè®­ç»ƒè¿‡ç¨‹ä¸­æˆ‘ä»¬è¿›è¡Œçš„å¤§é‡æ­¥éª¤ï¼Œæˆ‘ä»¬ä»ç„¶æœ‰è¶³å¤Ÿçš„ç‰‡æ®µå¯ä»¥æ ‡æ³¨ã€‚ä¾‹å¦‚ï¼Œåœ¨1200ä¸‡ç¯å¢ƒæ­¥éª¤ï¼ˆçº¦5å°æ—¶çš„è®­ç»ƒï¼‰ä¹‹åï¼Œæ•°æ®åº“ä¸­åŒ…å«äº†2,500ä¸ªå½•åˆ¶çš„ç‰‡æ®µï¼Œå ç”¨äº†12GBçš„ç£ç›˜ç©ºé—´ã€‚
- en: 'The method step() uses the function store_segment() to store the list of EpisodeStep
    objects, and it is just the pickle.dumps() call for the list of steps:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹æ³•step()ä½¿ç”¨å‡½æ•°store_segment()å­˜å‚¨EpisodeStepå¯¹è±¡çš„åˆ—è¡¨ï¼Œè¿™å®é™…ä¸Šæ˜¯å¯¹æ­¥éª¤åˆ—è¡¨çš„pickle.dumps()è°ƒç”¨ï¼š
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Before we get to the training results, I need to mention one small but important
    detail about the wrapperâ€™s usage. To make the labeling easier, the observations
    we store in the DB are taken before the standard Atari wrappers. This increases
    the size of the data we have to store, but human labelers will see the original
    colorful Atari screen in the original resolution (160 Ã— 192) instead of a downscaled
    picture in shades of gray.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¨è®ºè®­ç»ƒç»“æœä¹‹å‰ï¼Œæˆ‘éœ€è¦æåˆ°ä¸€ä¸ªå…³äºåŒ…è£…å™¨ä½¿ç”¨çš„å°ç»†èŠ‚ï¼Œè™½ç„¶å®ƒä¸å¤§ï¼Œä½†å¾ˆé‡è¦ã€‚ä¸ºäº†è®©æ ‡æ³¨æ›´å®¹æ˜“ï¼Œæˆ‘ä»¬å­˜å‚¨åœ¨æ•°æ®åº“ä¸­çš„è§‚å¯Ÿæ•°æ®æ˜¯æ¥è‡ªæ ‡å‡†AtariåŒ…è£…å™¨ä¹‹å‰çš„ã€‚è¿™è™½ç„¶å¢åŠ äº†æˆ‘ä»¬éœ€è¦å­˜å‚¨çš„æ•°æ®é‡ï¼Œä½†äººå·¥æ ‡æ³¨è€…å°†çœ‹åˆ°åŸå§‹çš„ã€è‰²å½©ä¸°å¯Œçš„Atariå±å¹•ï¼Œåˆ†è¾¨ç‡ä¸ºåŸå§‹çš„160
    Ã— 192ï¼Œè€Œä¸æ˜¯é™çº§åçš„ç°åº¦å›¾åƒã€‚
- en: 'To achieve that, the wrapper is applied right after the original Gymnasium
    environment before the Atari wrappers. The following is the relevant piece of
    code in the 01_a2c.py module:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼ŒåŒ…è£…å™¨åœ¨åŸå§‹Gymnasiumç¯å¢ƒä¹‹åã€AtariåŒ…è£…å™¨ä¹‹å‰åº”ç”¨ã€‚ä»¥ä¸‹æ˜¯01_a2c.pyæ¨¡å—ä¸­çš„ç›¸å…³ä»£ç ç‰‡æ®µï¼š
- en: '[PRE5]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The training process hyperparameters were taken from the paper (LR decrease
    schedule, network architecture, count of environments, etc). I let it train for
    5 hours and 12M observations. The charts with testing results are shown in FigureÂ [19.2](#x1-355105r2).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¿‡ç¨‹çš„è¶…å‚æ•°æ¥è‡ªè®ºæ–‡ï¼ˆå­¦ä¹ ç‡ä¸‹é™è®¡åˆ’ã€ç½‘ç»œæ¶æ„ã€ç¯å¢ƒæ•°é‡ç­‰ï¼‰ã€‚æˆ‘è®©å®ƒè®­ç»ƒäº†5å°æ—¶ï¼Œè¿›è¡Œäº†1200ä¸‡æ¬¡è§‚å¯Ÿã€‚æµ‹è¯•ç»“æœçš„å›¾è¡¨æ˜¾ç¤ºåœ¨å›¾[19.2](#x1-355105r2)ä¸­ã€‚
- en: '![PIC](img/B22150_19_02.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_19_02.png)'
- en: 'FigureÂ 19.2: The reward (left) and steps (right) during the A2C training'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾19.2ï¼šA2Cè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæ­¥éª¤ï¼ˆå³ï¼‰
- en: The best model was able to reach the reward level of 460 (without reward clipping
    in the environment), which is good but is much worse than the results that could
    be achieved if you refill the oxygen from time to time.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä½³æ¨¡å‹èƒ½å¤Ÿè¾¾åˆ°460çš„å¥–åŠ±æ°´å¹³ï¼ˆç¯å¢ƒä¸­æ²¡æœ‰å¥–åŠ±è£å‰ªï¼‰ï¼Œè™½ç„¶å¾ˆä¸é”™ï¼Œä½†ä¸æ—¶ä¸æ—¶è¡¥å……æ°§æ°”æ‰€èƒ½è¾¾åˆ°çš„ç»“æœç›¸æ¯”è¦å·®å¾—å¤šã€‚
- en: The video of this modelâ€™s gameplay is available at [https://youtu.be/R_H3pXu-7cw](https://youtu.be/R_H3pXu-7cw).
    As you can see from the video, our agent mastered shooting the fish almost perfectly,
    but got stuck on the local optima of floating at the bottom (maybe because it
    is safer, as enemy submarines are not present there) and has no idea about the
    oxygen refilling.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹çš„æ¸¸æˆè§†é¢‘å¯ä»¥åœ¨[https://youtu.be/R_H3pXu-7cw](https://youtu.be/R_H3pXu-7cw)è§‚çœ‹ã€‚æ­£å¦‚ä½ ä»è§†é¢‘ä¸­çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“å‡ ä¹å®Œç¾åœ°æŒæ¡äº†å°„å‡»é±¼ç±»çš„æŠ€å·§ï¼Œä½†å®ƒåœ¨æµ®åœ¨åº•éƒ¨çš„å±€éƒ¨æœ€ä¼˜è§£ä¸Šå¡ä½äº†ï¼ˆå¯èƒ½å› ä¸ºåœ¨é‚£é‡Œæ›´å®‰å…¨ï¼Œæ•Œæ–¹æ½œè‰‡ä¸åœ¨é‚£é‡Œï¼‰ï¼Œå¹¶ä¸”å¯¹æ°§æ°”è¡¥å……ä¸€æ— æ‰€çŸ¥ã€‚
- en: You can record your own video from the model file using the tool 01_play.py,
    which takes the model filename.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ä½¿ç”¨å·¥å…·01_play.pyä»æ¨¡å‹æ–‡ä»¶å½•åˆ¶è‡ªå·±çš„è§†é¢‘ï¼Œè¾“å…¥æ¨¡å‹æ–‡ä»¶åå³å¯ã€‚
- en: Labeling process
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ ‡æ³¨è¿‡ç¨‹
- en: During the A2C training, we got 12GB of 2,500 random episode segments. Each
    segment contains 50 steps with screen observations and actions the agent took
    on every step. Now we are ready for the labeling process of the RLHF pipeline.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨A2Cè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è·å¾—äº†12GBçš„2,500ä¸ªéšæœºå‰§é›†ç‰‡æ®µã€‚æ¯ä¸ªç‰‡æ®µåŒ…å«50ä¸ªæ­¥éª¤ï¼ŒåŒ…å«å±å¹•è§‚å¯Ÿå’Œæ™ºèƒ½ä½“åœ¨æ¯ä¸€æ­¥é‡‡å–çš„åŠ¨ä½œã€‚ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½è¿›è¡ŒRLHFç®¡é“çš„æ ‡æ³¨è¿‡ç¨‹ã€‚
- en: During the labeling, we need to randomly sample pairs of episode segments and
    show them to the human, asking the question â€œWhich one is better?â€. The answer
    should be stored for reward model training. Exactly this logic is implemented
    in 02_label_ui.py.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ ‡æ³¨è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦éšæœºæŠ½å–å‰§é›†ç‰‡æ®µå¯¹å¹¶å±•ç¤ºç»™ç”¨æˆ·ï¼Œè¯¢é—®â€œå“ªä¸ªæ›´å¥½ï¼Ÿâ€ã€‚ç­”æ¡ˆåº”å­˜å‚¨ç”¨äºå¥–åŠ±æ¨¡å‹çš„è®­ç»ƒã€‚æ­£æ˜¯è¿™ä¸ªé€»è¾‘åœ¨ 02_label_ui.py ä¸­å®ç°ã€‚
- en: 'The UI of the labeling process is implemented as a web application that uses
    the NiceGUI library ([https://nicegui.io/](https://nicegui.io/)). NiceGUI allows
    a modern web application UI to be implemented in Python and provides a rich set
    of interactive UI widgets, like buttons, lists, pop-up dialogs, etc. In principle,
    you donâ€™t need to know JavaScript and CSS (but it wonâ€™t harm if youâ€™re familiar
    with them). If you have never used NiceGUI before, thatâ€™s not a problem; you just
    need to install it with the following command in your Python environment:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡æ³¨è¿‡ç¨‹çš„ UI ä½œä¸ºä¸€ä¸ª web åº”ç”¨å®ç°ï¼Œä½¿ç”¨äº† NiceGUI åº“ï¼ˆ[https://nicegui.io/](https://nicegui.io/)ï¼‰ã€‚NiceGUI
    å…è®¸ç”¨ Python å®ç°ç°ä»£ web åº”ç”¨ UIï¼Œå¹¶æä¾›äº†ä¸€å¥—ä¸°å¯Œçš„äº¤äº’å¼ UI æ§ä»¶ï¼Œå¦‚æŒ‰é’®ã€åˆ—è¡¨ã€å¼¹å‡ºå¯¹è¯æ¡†ç­‰ã€‚åŸåˆ™ä¸Šï¼Œä½ ä¸éœ€è¦äº†è§£ JavaScript
    å’Œ CSSï¼ˆä½†å¦‚æœä½ ç†Ÿæ‚‰å®ƒä»¬ä¹Ÿæ— å¦¨ï¼‰ã€‚å¦‚æœä½ ä»¥å‰ä»æœªä½¿ç”¨è¿‡ NiceGUIï¼Œä¹Ÿæ²¡é—®é¢˜ï¼›ä½ åªéœ€åœ¨ Python ç¯å¢ƒä¸­é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…å®ƒï¼š
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To start the labeling UI (after installing the NiceGUI package), you need to
    specify the path to the DB with stored episode segments:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¯åŠ¨æ ‡æ³¨ UIï¼ˆåœ¨å®‰è£… NiceGUI åŒ…ä¹‹åï¼‰ï¼Œä½ éœ€è¦æŒ‡å®šå­˜å‚¨å‰§é›†ç‰‡æ®µçš„æ•°æ®åº“è·¯å¾„ï¼š
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The interface is available via HTTP (so, open it in your browser) and listens
    on port 8080 on all machine interfaces, which is convenient if you start it on
    a remote server (but you need to be aware of the possible risk of external access,
    as the labeling UI has no authentification and authorization at all). If you want
    to change the port or limit the scope to the specific network interface, just
    tweak 02_label_ui.py. Letâ€™s look at a screenshot of the labelling interface:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ç•Œé¢é€šè¿‡ HTTP æä¾›æœåŠ¡ï¼ˆæ‰€ä»¥ï¼Œå¯ä»¥åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ï¼‰ï¼Œå¹¶ç›‘å¬æ‰€æœ‰æœºå™¨æ¥å£ä¸Šçš„ 8080 ç«¯å£ï¼Œè¿™åœ¨ä½ å°†å…¶éƒ¨ç½²åˆ°è¿œç¨‹æœåŠ¡å™¨æ—¶éå¸¸æ–¹ä¾¿ï¼ˆä½†ä½ éœ€è¦æ„è¯†åˆ°å¯èƒ½çš„å¤–éƒ¨è®¿é—®é£é™©ï¼Œå› ä¸ºæ ‡æ³¨
    UI å®Œå…¨æ²¡æœ‰èº«ä»½éªŒè¯å’Œæˆæƒï¼‰ã€‚å¦‚æœä½ æƒ³æ›´æ”¹ç«¯å£æˆ–å°†èŒƒå›´é™åˆ¶åˆ°ç‰¹å®šçš„ç½‘ç»œæ¥å£ï¼Œåªéœ€ä¿®æ”¹ 02_label_ui.pyã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æ ‡æ³¨ç•Œé¢çš„æˆªå›¾ï¼š
- en: '![PIC](img/file290.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file290.png)'
- en: 'FigureÂ 19.3: The labeling UI section with DB information'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 19.3ï¼šå¸¦æœ‰æ•°æ®åº“ä¿¡æ¯çš„æ ‡æ³¨ UI éƒ¨åˆ†
- en: 'This interface is very basic: on the left, there are three links to different
    sections of the UI functionality:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç•Œé¢éå¸¸åŸºç¡€ï¼šå·¦ä¾§æœ‰ä¸‰ä¸ªé“¾æ¥ï¼ŒæŒ‡å‘ UI åŠŸèƒ½çš„ä¸åŒéƒ¨åˆ†ï¼š
- en: Overview shows the path to the database, the total count of segments it contains,
    and the amount of labels already created.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¦‚è§ˆæ˜¾ç¤ºæ•°æ®åº“è·¯å¾„ã€å…¶ä¸­åŒ…å«çš„ç‰‡æ®µæ€»æ•°ä»¥åŠå·²åˆ›å»ºçš„æ ‡ç­¾æ•°é‡ã€‚
- en: Label new data samples random pairs of segments and allows you to label them.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ ‡æ³¨æ–°æ•°æ®æ ·æœ¬éšæœºé…å¯¹ç‰‡æ®µå¹¶å…è®¸ä½ ä¸ºå…¶æ·»åŠ æ ‡ç­¾ã€‚
- en: Existing labels shows all the labels and allows you to modify the labels if
    needed.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: â€œç°æœ‰æ ‡ç­¾â€æ˜¾ç¤ºæ‰€æœ‰æ ‡ç­¾ï¼Œå¹¶å…è®¸åœ¨éœ€è¦æ—¶ä¿®æ”¹æ ‡ç­¾ã€‚
- en: 'If needed, the list with links could be hidden or shown by clicking on the
    top-left button (with three horizontal lines). The most time has been spent on
    the Label new data section, shown in FigureÂ ??:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ‰éœ€è¦ï¼Œå¯ä»¥é€šè¿‡ç‚¹å‡»å·¦ä¸Šè§’çš„æŒ‰é’®ï¼ˆå¸¦æœ‰ä¸‰ä¸ªæ¨ªçº¿ï¼‰éšè—æˆ–æ˜¾ç¤ºåŒ…å«é“¾æ¥çš„åˆ—è¡¨ã€‚æœ€å¤šçš„æ—¶é—´èŠ±è´¹åœ¨â€œæ ‡æ³¨æ–°æ•°æ®â€éƒ¨åˆ†ï¼Œè§å›¾ ??ï¼š
- en: '![PIC](img/file291.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file291.png)'
- en: 'FigureÂ 19.4: The interface for adding new labels (for better visualization,
    refer to https://packt.link/gbp/9781835882702 )'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 19.4ï¼šæ·»åŠ æ–°æ ‡ç­¾çš„ç•Œé¢ï¼ˆä¸ºäº†æ›´å¥½åœ°å¯è§†åŒ–ï¼Œå‚è€ƒ https://packt.link/gbp/9781835882702ï¼‰
- en: 'Here, we have a list of 20 randomly sampled pairs of episode segments we can
    label. When the entry in the list is selected, the interface shows both segments
    (as animated GIFs generated by the code on the fly). The user can click one of
    three buttons to add the label:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å« 20 å¯¹éšæœºæŠ½å–çš„å‰§é›†ç‰‡æ®µçš„åˆ—è¡¨ï¼Œå¯ä»¥è¿›è¡Œæ ‡æ³¨ã€‚å½“åˆ—è¡¨ä¸­çš„æ¡ç›®è¢«é€‰æ‹©æ—¶ï¼Œç•Œé¢ä¼šæ˜¾ç¤ºè¿™ä¸¤æ®µç‰‡æ®µï¼ˆä½œä¸ºä»£ç å®æ—¶ç”Ÿæˆçš„åŠ¨ç”» GIFï¼‰ã€‚ç”¨æˆ·å¯ä»¥ç‚¹å‡»ä¸‰ä¸ªæŒ‰é’®ä¸­çš„ä¸€ä¸ªæ¥æ·»åŠ æ ‡ç­¾ï¼š
- en: '#1 IS BETTER (1): Marks the first segment as preferred. Such entries will have
    Î¼[1] = 1.0 and Î¼[2] = 0.0 during the reward model training.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '#1 æ›´å¥½ï¼ˆ1ï¼‰ï¼šå°†ç¬¬ä¸€ä¸ªç‰‡æ®µæ ‡è®°ä¸ºé¦–é€‰ã€‚åœ¨å¥–åŠ±æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¿™æ ·çš„æ¡ç›®ä¼šæœ‰ Î¼[1] = 1.0 å’Œ Î¼[2] = 0.0ã€‚'
- en: 'BOTH ARE GOOD (0): Marks both segments as equally good (or bad), assigning
    Î¼[1] = 0.5 and Î¼[2] = 0.5.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸¤è€…éƒ½å¥½ï¼ˆ0ï¼‰ï¼šå°†ä¸¤ä¸ªç‰‡æ®µæ ‡è®°ä¸ºåŒæ ·å¥½ï¼ˆæˆ–å·®ï¼‰ï¼Œèµ‹å€¼ Î¼[1] = 0.5 å’Œ Î¼[2] = 0.5ã€‚
- en: '#2 IS BETTER (2): Marks the second segment as preferred (Î¼[1] = 0.0 and Î¼[2]
    = 1.0).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '#2 æ›´å¥½ï¼ˆ2ï¼‰ï¼šå°†ç¬¬äºŒä¸ªç‰‡æ®µæ ‡è®°ä¸ºé¦–é€‰ï¼ˆÎ¼[1] = 0.0 å’Œ Î¼[2] = 1.0ï¼‰ã€‚'
- en: Instead of clicking the UI buttons, you can use the keyboard keys 0 (â€œboth are
    goodâ€), 1 (â€œthe first is betterâ€), or 2 (â€œthe second is betterâ€) to assign the
    label. Once the label is assigned, the UI automatically selects the next unlabeled
    entry in the list, so the labeling process could be done with the keyboard only.
    When youâ€™re done with all the labels in the list, you can click the RESAMPLE LIST
    button to load 20 new samples for labeling.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡ä½¿ç”¨é”®ç›˜ä¸Šçš„0ï¼ˆâ€œä¸¤è€…éƒ½å¥½â€ï¼‰ã€1ï¼ˆâ€œç¬¬ä¸€ä¸ªæ›´å¥½â€ï¼‰æˆ–2ï¼ˆâ€œç¬¬äºŒä¸ªæ›´å¥½â€ï¼‰æ¥åˆ†é…æ ‡ç­¾ï¼Œè€Œæ— éœ€ç‚¹å‡»UIæŒ‰é’®ã€‚æ ‡ç­¾åˆ†é…å®Œæˆåï¼ŒUIä¼šè‡ªåŠ¨é€‰æ‹©åˆ—è¡¨ä¸­çš„ä¸‹ä¸€ä¸ªæœªæ ‡è®°æ¡ç›®ï¼Œè¿™æ ·æ•´ä¸ªæ ‡è®°è¿‡ç¨‹ä»…ä½¿ç”¨é”®ç›˜å°±èƒ½å®Œæˆã€‚å½“ä½ å®Œæˆåˆ—è¡¨ä¸­çš„æ‰€æœ‰æ ‡ç­¾åï¼Œå¯ä»¥ç‚¹å‡»RESAMPLE
    LISTæŒ‰é’®åŠ è½½20ä¸ªæ–°çš„æ ·æœ¬è¿›è¡Œæ ‡è®°ã€‚
- en: 'After every label has been assigned (with UI button clicks or key presses),
    the labels are stored in the JSON file labels.json in the root of the DB directory.
    The file has a trivial JSON-line format where every line is an entry containing
    paths to both segments (relative to the DB root) and assigned labels:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªæ ‡ç­¾è¢«åˆ†é…åï¼ˆé€šè¿‡ç‚¹å‡»UIæŒ‰é’®æˆ–æŒ‰ä¸‹é”®ç›˜é”®ï¼‰ï¼Œè¿™äº›æ ‡ç­¾ä¼šå­˜å‚¨åœ¨DBç›®å½•æ ¹ç›®å½•ä¸‹çš„JSONæ–‡ä»¶labels.jsonä¸­ã€‚è¯¥æ–‡ä»¶é‡‡ç”¨ç®€å•çš„JSONè¡Œæ ¼å¼ï¼Œæ¯è¡Œéƒ½æ˜¯ä¸€ä¸ªåŒ…å«æ®µè½è·¯å¾„ï¼ˆç›¸å¯¹äºDBæ ¹ç›®å½•ï¼‰å’Œå·²åˆ†é…æ ‡ç­¾çš„æ¡ç›®ï¼š
- en: '[PRE8]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If needed, existing labels could be reviewed using the Existing labels link
    (shown in FigureÂ [19.5](#x1-356021r5)), which shows almost the same interface
    as Label new data, but instead of sampling 20 fresh pairs, it shows already labeled
    pairs. Those pairs could be changed by clicking the buttons or using the keyboard
    shortcuts described earlier.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœéœ€è¦ï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨â€œç°æœ‰æ ‡ç­¾â€é“¾æ¥ï¼ˆå¦‚å›¾[19.5](#x1-356021r5)æ‰€ç¤ºï¼‰æ¥æŸ¥çœ‹ç°æœ‰æ ‡ç­¾ï¼Œè¯¥ç•Œé¢å‡ ä¹ä¸â€œæ ‡è®°æ–°æ•°æ®â€ç›¸åŒï¼Œä¸åŒä¹‹å¤„åœ¨äºå®ƒæ˜¾ç¤ºçš„ä¸æ˜¯20ä¸ªæ–°é‡‡æ ·çš„å¯¹ï¼Œè€Œæ˜¯å·²ç»æ ‡è®°çš„å¯¹ã€‚è¿™äº›å¯¹å¯ä»¥é€šè¿‡ç‚¹å‡»æŒ‰é’®æˆ–ä½¿ç”¨å‰é¢æè¿°çš„é”®ç›˜å¿«æ·é”®è¿›è¡Œæ›´æ”¹ã€‚
- en: '![PIC](img/file292.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file292.png)'
- en: 'FigureÂ 19.5: The interface for reviewing and editing old labels (for better
    visualization, refer to https://packt.link/gbp/9781835882702 )'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾19.5ï¼šæŸ¥çœ‹å’Œç¼–è¾‘æ—§æ ‡ç­¾çš„ç•Œé¢ï¼ˆä¸ºäº†æ›´å¥½çš„å¯è§†åŒ–ï¼Œå‚è§https://packt.link/gbp/9781835882702 ï¼‰
- en: 'During my experiments, I did the first round of labeling 100 pairs paying most
    attention to the rare cases when the submarine was on the surface (marking them
    as good) and more frequent situations when oxygen was low (marking them as bad).
    In other situations, I prefer the segments where fish were properly hit. With
    some labels at hand, weâ€™re ready to go on to the next step: reward model training.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘çš„å®éªŒä¸­ï¼Œæˆ‘è¿›è¡Œäº†ç¬¬ä¸€è½®æ ‡è®°ï¼Œå…±æ ‡è®°äº†100å¯¹æ ·æœ¬ï¼Œä¸»è¦å…³æ³¨æ½œæ°´è‰‡å‡ºç°åœ¨æ°´é¢ä¸Šçš„ç½•è§æƒ…å†µï¼ˆæ ‡è®°ä¸ºå¥½ï¼‰å’Œæ°§æ°”ä¸è¶³æ—¶æ›´ä¸ºå¸¸è§çš„æƒ…å†µï¼ˆæ ‡è®°ä¸ºåï¼‰ã€‚åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæˆ‘æ›´å€¾å‘äºé€‰æ‹©é‚£äº›é±¼ç¾¤è¢«æ­£ç¡®å‡»ä¸­çš„æ®µè½ã€‚æœ‰äº†è¿™äº›æ ‡ç­¾ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›å…¥ä¸‹ä¸€æ­¥ï¼šå¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚
- en: Reward model training
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¥–åŠ±æ¨¡å‹è®­ç»ƒ
- en: 'The reward model network has most of the structure taken from the paper, with
    the only difference in handling actions. In the paper, the authors do not specify
    how actions are taken into account besides stating â€œFor the reward predictor,
    we use 84 Ã— 84 images as inputs (the same as the inputs to the policy), and stack
    4 frames for a total 84 Ã— 84 Ã— 4 input tensor.â€ From that, I made an assumption
    that the reward model deducts actions â€œimplicitlyâ€ from the dynamics between the
    frames. I havenâ€™t tried this approach in my experiment and instead decided to
    show the actions to the network explicitly by concatenating one-hot encoding to
    the vectors obtained from the convolution layers. As an exercise, you can change
    my code to use the approach from the paper and compare the results. The rest of
    the architecture and training parameters are the same as in the paper. Letâ€™s take
    a look at the reward model network code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±æ¨¡å‹ç½‘ç»œå¤§å¤šæ•°ç»“æ„æ¥è‡ªè®ºæ–‡ï¼Œå”¯ä¸€çš„ä¸åŒåœ¨äºå¦‚ä½•å¤„ç†åŠ¨ä½œã€‚åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…æ²¡æœ‰æ˜ç¡®è¯´æ˜å¦‚ä½•è€ƒè™‘åŠ¨ä½œï¼Œåªæ˜¯æåˆ°â€œå¯¹äºå¥–åŠ±é¢„æµ‹å™¨ï¼Œæˆ‘ä»¬ä½¿ç”¨84 Ã— 84çš„å›¾åƒä½œä¸ºè¾“å…¥ï¼ˆä¸ç­–ç•¥çš„è¾“å…¥ç›¸åŒï¼‰ï¼Œå¹¶å°†4å¸§å›¾åƒå †å åœ¨ä¸€èµ·ï¼Œå½¢æˆæ€»å…±84
    Ã— 84 Ã— 4çš„è¾“å…¥å¼ é‡ã€‚â€æ ¹æ®è¿™ä¸€ç‚¹ï¼Œæˆ‘å‡è®¾å¥–åŠ±æ¨¡å‹é€šè¿‡å¸§ä¹‹é—´çš„åŠ¨æ€â€œéšå¼â€åœ°æ‰£é™¤åŠ¨ä½œã€‚æˆ‘åœ¨å®éªŒä¸­æ²¡æœ‰å°è¯•è¿™ç§æ–¹æ³•ï¼Œè€Œæ˜¯å†³å®šé€šè¿‡å°†one-hotç¼–ç ä¸ä»å·ç§¯å±‚è·å¾—çš„å‘é‡æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œæ˜¾å¼åœ°å‘ç½‘ç»œå±•ç¤ºåŠ¨ä½œã€‚ä½œä¸ºä¸€ä¸ªç»ƒä¹ ï¼Œä½ å¯ä»¥ä¿®æ”¹æˆ‘çš„ä»£ç ï¼Œä½¿ç”¨è®ºæ–‡ä¸­çš„æ–¹æ³•å¹¶æ¯”è¾ƒç»“æœã€‚å…¶ä½™çš„æ¶æ„å’Œè®­ç»ƒå‚æ•°ä¸è®ºæ–‡ä¸­çš„ç›¸åŒã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å¥–åŠ±æ¨¡å‹ç½‘ç»œçš„ä»£ç ï¼š
- en: '[PRE9]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see, convolution layers are combined with batch normalization, dropout,
    and the leaky ReLU activation function.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå·ç§¯å±‚ä¸æ‰¹é‡å½’ä¸€åŒ–ã€ä¸¢å¼ƒå±‚å’Œleaky ReLUæ¿€æ´»å‡½æ•°ç»“åˆä½¿ç”¨ã€‚
- en: 'The training of the reward model is implemented in 03_reward_train.py and has
    nothing complicated. We load labeled data from JSON files (you can pass several
    databases in the command line to use for the training), use 20% of the data for
    the testing, and compute the binary cross entropy objective, which is implemented
    in the calc_loss() function:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±æ¨¡å‹çš„è®­ç»ƒåœ¨ 03_reward_train.py ä¸­å®ç°ï¼Œè¿‡ç¨‹æ²¡æœ‰ä»€ä¹ˆå¤æ‚çš„ã€‚æˆ‘ä»¬ä» JSON æ–‡ä»¶ä¸­åŠ è½½æ ‡æ³¨æ•°æ®ï¼ˆä½ å¯ä»¥åœ¨å‘½ä»¤è¡Œä¸­ä¼ é€’å¤šä¸ªæ•°æ®åº“æ¥ç”¨äºè®­ç»ƒï¼‰ï¼Œä½¿ç”¨
    20% çš„æ•°æ®è¿›è¡Œæµ‹è¯•ï¼Œå¹¶è®¡ç®—äºŒå…ƒäº¤å‰ç†µç›®æ ‡ï¼Œè¿™åœ¨ calc_loss() å‡½æ•°ä¸­å®ç°ï¼š
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Initially, our observations and actions tensors had the following structure:
    (batch,time,colors,height,width) for observations and (batch,time,actions) for
    actions, where time is the sequenceâ€™s time dimension. More concretely, observation
    tensors had the size 64 Ã— 50 Ã— 3 Ã— 210 Ã— 160 and actions had the size 64 Ã— 50
    Ã— 18.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åˆï¼Œæˆ‘ä»¬çš„è§‚å¯Ÿå’ŒåŠ¨ä½œå¼ é‡å…·æœ‰ä»¥ä¸‹ç»“æ„ï¼šè§‚å¯Ÿä¸º(batch,time,colors,height,width)ï¼ŒåŠ¨ä½œä¸º(batch,time,actions)ï¼Œå…¶ä¸­
    time æ˜¯åºåˆ—çš„æ—¶é—´ç»´åº¦ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œè§‚å¯Ÿå¼ é‡çš„å¤§å°ä¸º 64 Ã— 50 Ã— 3 Ã— 210 Ã— 160ï¼ŒåŠ¨ä½œçš„å¤§å°ä¸º 64 Ã— 50 Ã— 18ã€‚
- en: As the first step in loss calculation, we flatten the first two dimensions,
    getting rid of the time dimension and applying the model to compute the reward
    value rÌ‚(o,a). After that, we return the time dimension and sum along it according
    to the paperâ€™s formula weâ€™ve already discussed. Then our computation of loss is
    the application of the torch function to compute the binary cross entropy.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºæŸå¤±è®¡ç®—çš„ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬å±•å¹³å‰ä¸¤ä¸ªç»´åº¦ï¼Œå»é™¤æ—¶é—´ç»´åº¦ï¼Œå¹¶åº”ç”¨æ¨¡å‹è®¡ç®—å¥–åŠ±å€¼ rÌ‚(o,a)ã€‚ä¹‹åï¼Œæˆ‘ä»¬æ¢å¤æ—¶é—´ç»´åº¦ï¼Œå¹¶æ ¹æ®æˆ‘ä»¬å·²ç»è®¨è®ºè¿‡çš„è®ºæ–‡å…¬å¼æ²¿æ—¶é—´ç»´åº¦æ±‚å’Œã€‚ç„¶åï¼Œæˆ‘ä»¬çš„æŸå¤±è®¡ç®—æ˜¯åº”ç”¨
    torch å‡½æ•°æ¥è®¡ç®—äºŒå…ƒäº¤å‰ç†µã€‚
- en: On every epoch of the training, we compute the test loss (on 20% of the data)
    and save the reward model if the new loss is lower than the previous minimum of
    the test loss. If the train loss grows for four epochs in a row, we stop the training.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸä¸­ï¼Œæˆ‘ä»¬è®¡ç®—æµ‹è¯•æŸå¤±ï¼ˆåŸºäº 20% çš„æ•°æ®ï¼‰ï¼Œå¹¶åœ¨æ–°æŸå¤±ä½äºå…ˆå‰æµ‹è¯•æŸå¤±çš„æœ€å°å€¼æ—¶ä¿å­˜å¥–åŠ±æ¨¡å‹ã€‚å¦‚æœè®­ç»ƒæŸå¤±è¿ç»­å››ä¸ªå‘¨æœŸå¢é•¿ï¼Œæˆ‘ä»¬å°†åœæ­¢è®­ç»ƒã€‚
- en: 'With the number of labels set in the previous section (a couple of hundred),
    the training is very quick â€” it takes about a dozen epochs and several minutes.
    The following is the example training process. The command-line argument -o specifies
    the directory name where the best model will be saved:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰ä¸€èŠ‚ä¸­è®¾ç½®çš„æ ‡ç­¾æ•°é‡ï¼ˆå‡ ç™¾ä¸ªï¼‰ä¸‹ï¼Œè®­ç»ƒéå¸¸å¿«é€Ÿâ€”â€”å¤§çº¦åå‡ ä¸ªå‘¨æœŸå’Œå‡ åˆ†é’Ÿæ—¶é—´ã€‚ä»¥ä¸‹æ˜¯ç¤ºä¾‹è®­ç»ƒè¿‡ç¨‹ã€‚å‘½ä»¤è¡Œå‚æ•° -o æŒ‡å®šä¿å­˜æœ€ä½³æ¨¡å‹çš„ç›®å½•åç§°ï¼š
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Combining A2C with the reward model
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å°† A2C ä¸å¥–åŠ±æ¨¡å‹ç›¸ç»“åˆ
- en: 'Once the reward model is trained, we can finally try it for use in RL training.
    To do that, we use the same tool 01_a2c.py but give it a couple of extra arguments:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å¥–åŠ±æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæˆ‘ä»¬æœ€ç»ˆå¯ä»¥å°è¯•å°†å…¶ç”¨äº RL è®­ç»ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„å·¥å…· 01_a2c.pyï¼Œä½†æä¾›å‡ ä¸ªé¢å¤–çš„å‚æ•°ï¼š
- en: '-r or --reward: This gives the path to the reward model to be loaded and used.
    With this option, we donâ€™t use the environment reward but, instead, use the model
    to get the reward from the observation and action we decided to take. This is
    implemented as an additional environment wrapper; weâ€™ll take a look shortly.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -r æˆ– --rewardï¼šè¿™æ˜¯å¥–åŠ±æ¨¡å‹çš„è·¯å¾„ï¼Œç”¨äºåŠ è½½å’Œä½¿ç”¨ã€‚é€šè¿‡æ­¤é€‰é¡¹ï¼Œæˆ‘ä»¬ä¸ä½¿ç”¨ç¯å¢ƒå¥–åŠ±ï¼Œè€Œæ˜¯ä½¿ç”¨æ¨¡å‹ä»æˆ‘ä»¬å†³å®šé‡‡å–çš„è§‚å¯Ÿå’ŒåŠ¨ä½œä¸­è·å¾—å¥–åŠ±ã€‚è¿™ä½œä¸ºé¢å¤–çš„ç¯å¢ƒåŒ…è£…å™¨å®ç°ï¼›æˆ‘ä»¬ç¨åä¼šè¯¦ç»†ä»‹ç»ã€‚
- en: '-m or --model: This is the path to the actor model (stored on the previous
    A2C round of training) to be loaded. As Iâ€™m doing fine-tuning with RLHF instead
    of training with the reward model from scratch, the actor model is needed. In
    principle, you can try to use the reward model to train from scratch, but my experiments
    were not very successful.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -m æˆ– --modelï¼šè¿™æ˜¯è¦åŠ è½½çš„æ¼”å‘˜æ¨¡å‹çš„è·¯å¾„ï¼ˆå­˜å‚¨åœ¨å…ˆå‰ A2C è®­ç»ƒè½®æ¬¡ä¸­ï¼‰ã€‚ç”±äºæˆ‘æ­£åœ¨ä½¿ç”¨ RLHF è¿›è¡Œå¾®è°ƒï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹ä½¿ç”¨å¥–åŠ±æ¨¡å‹è®­ç»ƒï¼Œå› æ­¤éœ€è¦æ¼”å‘˜æ¨¡å‹ã€‚åŸåˆ™ä¸Šï¼Œä½ å¯ä»¥å°è¯•ä½¿ç”¨å¥–åŠ±æ¨¡å‹ä»é›¶å¼€å§‹è®­ç»ƒï¼Œä½†æˆ‘çš„å®éªŒç»“æœå¹¶ä¸ååˆ†æˆåŠŸã€‚
- en: '--finetune: This enables the fine-tuning mode: convolution layers are frozen
    and LR is decreased 10 times. Without those modifications, the actor very quickly
    unlearns all the prior knowledge and the reward drops to almost zero.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: --finetuneï¼šå¯ç”¨å¾®è°ƒæ¨¡å¼ï¼šå·ç§¯å±‚è¢«å†»ç»“ï¼Œå­¦ä¹ ç‡é™ä½ 10 å€ã€‚æ²¡æœ‰è¿™äº›ä¿®æ”¹ï¼Œæ¼”å‘˜å¾ˆå¿«å°±ä¼šå¿˜è®°æ‰€æœ‰å…ˆå‰çš„çŸ¥è¯†ï¼Œå¥–åŠ±å‡ ä¹é™åˆ°é›¶ã€‚
- en: 'So, to use the reward model weâ€™ve just trained, the command line will look
    like this:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¦ä½¿ç”¨æˆ‘ä»¬åˆšåˆšè®­ç»ƒçš„å¥–åŠ±æ¨¡å‹ï¼Œå‘½ä»¤è¡Œçœ‹èµ·æ¥åƒè¿™æ ·ï¼š
- en: ./01_a2c.py --dev cuda -n v1 -r rw/reward-v0.dat --save save/v1 -m save/v0/model_rw=460-steps=580.dat
    --finetune
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ./01_a2c.py --dev cuda -n v1 -r rw/reward-v0.dat --save save/v1 -m save/v0/model_rw=460-steps=580.dat
    --finetune
- en: Before checking the experiment results, letâ€™s take a look at how the reward
    model is used in the RL training process. To minimize the changes needed, I implemented
    an environment wrapper, which is added between the original environment and Atari
    wrappers, because the reward model needs an unscaled full-color game image.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ£€æŸ¥å®éªŒç»“æœä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¥–åŠ±æ¨¡å‹å¦‚ä½•åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ã€‚ä¸ºäº†æœ€å°åŒ–æ‰€éœ€çš„æ”¹åŠ¨ï¼Œæˆ‘å®ç°äº†ä¸€ä¸ªç¯å¢ƒåŒ…è£…å™¨ï¼Œå®ƒè¢«æ·»åŠ åœ¨åŸå§‹ç¯å¢ƒå’ŒAtariåŒ…è£…å™¨ä¹‹é—´ï¼Œå› ä¸ºå¥–åŠ±æ¨¡å‹éœ€è¦ä¸€ä¸ªæœªç»ç¼©æ”¾çš„å…¨å½©æ¸¸æˆå›¾åƒã€‚
- en: 'The code of the wrapper is in lib/rlhf.py and is called RewardModelWrapper.
    The constructor of the wrapper loads the model from the data file and assigns
    a couple of fields. According to the paper, the reward predicted by the reward
    model is normalized to have zero mean and unit variance, so to do the normalization,
    the wrapper maintains the last 100 rewards in collections.deque. Besides normalization,
    the wrapper can have a queue for metrics to be sent. The metrics contain information
    about normalization values and the real sum from the underlying environment:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ…è£…å™¨çš„ä»£ç åœ¨lib/rlhf.pyä¸­ï¼Œåä¸ºRewardModelWrapperã€‚åŒ…è£…å™¨çš„æ„é€ å‡½æ•°ä»æ•°æ®æ–‡ä»¶ä¸­åŠ è½½æ¨¡å‹å¹¶åˆ†é…ä¸€äº›å­—æ®µã€‚æ ¹æ®è®ºæ–‡ï¼Œå¥–åŠ±æ¨¡å‹é¢„æµ‹çš„å¥–åŠ±ç»è¿‡æ ‡å‡†åŒ–ï¼Œä½¿å…¶å‡å€¼ä¸ºé›¶ï¼Œæ–¹å·®ä¸ºä¸€ã€‚å› æ­¤ï¼Œä¸ºäº†è¿›è¡Œæ ‡å‡†åŒ–ï¼ŒåŒ…è£…å™¨ç»´æŠ¤äº†æœ€å100ä¸ªå¥–åŠ±å€¼ï¼Œä½¿ç”¨collections.dequeã€‚æ­¤å¤–ï¼ŒåŒ…è£…å™¨è¿˜å¯ä»¥æœ‰ä¸€ä¸ªé˜Ÿåˆ—ï¼Œç”¨äºå‘é€æŒ‡æ ‡ã€‚è¯¥æŒ‡æ ‡åŒ…å«å…³äºæ ‡å‡†åŒ–å€¼å’Œæ¥è‡ªåº•å±‚ç¯å¢ƒçš„çœŸå®æ€»å’Œçš„ä¿¡æ¯ï¼š
- en: '[PRE12]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the reset() method, we just remember the observation and reset the reward
    counter:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨reset()æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦è®°ä½è§‚å¯Ÿå¹¶é‡ç½®å¥–åŠ±è®¡æ•°å™¨ï¼š
- en: '[PRE13]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The main logic of the wrapper is in the step() function, but it is not very
    complicated: we apply the model to the observation and the action, normalize the
    reward, and return it instead of the real one. The model application is not very
    efficient from a performance perspective and could be optimized (as we have several
    environments working in parallel), but I decided to implement the simple version
    first, leaving optimizations as an excercise for you:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ…è£…å™¨çš„ä¸»è¦é€»è¾‘åœ¨step()å‡½æ•°ä¸­ï¼Œä½†å¹¶ä¸å¤æ‚ï¼šæˆ‘ä»¬å°†æ¨¡å‹åº”ç”¨äºè§‚å¯Ÿå’ŒåŠ¨ä½œï¼Œæ ‡å‡†åŒ–å¥–åŠ±ï¼Œå¹¶è¿”å›å®ƒï¼Œè€Œä¸æ˜¯è¿”å›çœŸå®çš„å¥–åŠ±ã€‚ä»æ€§èƒ½è§’åº¦æ¥çœ‹ï¼Œæ¨¡å‹åº”ç”¨æ•ˆç‡ä¸æ˜¯å¾ˆé«˜ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–ï¼ˆå› ä¸ºæˆ‘ä»¬æœ‰å¤šä¸ªç¯å¢ƒå¹¶è¡Œè¿è¡Œï¼‰ï¼Œä½†æˆ‘å†³å®šå…ˆå®ç°ç®€å•ç‰ˆæœ¬ï¼ŒæŠŠä¼˜åŒ–ç•™ç»™ä½ ä½œä¸ºç»ƒä¹ ï¼š
- en: '[PRE14]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The rest of the training is the same. We just inject the new wrapper into the
    environment-creating function if the reward model file is given in the command
    line:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: å‰©ä¸‹çš„è®­ç»ƒéƒ¨åˆ†ç›¸åŒã€‚æˆ‘ä»¬åªéœ€åœ¨ç¯å¢ƒåˆ›å»ºå‡½æ•°ä¸­æ³¨å…¥æ–°çš„åŒ…è£…å™¨ï¼ˆå¦‚æœå‘½ä»¤è¡Œä¸­ç»™å®šäº†å¥–åŠ±æ¨¡å‹æ–‡ä»¶ï¼‰ï¼š
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With this code, we can now combine the previous model with the labels we made
    before.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™æ®µä»£ç ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å°†ä¹‹å‰çš„æ¨¡å‹ä¸ä¹‹å‰åˆ¶ä½œçš„æ ‡ç­¾ç»“åˆèµ·æ¥ã€‚
- en: Fine-tuning with 100 labels
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨100ä¸ªæ ‡ç­¾è¿›è¡Œå¾®è°ƒ
- en: 'I ran the training with the best model from the basic A2C training, which,
    on testing, achieved a reward of 460 in 580 steps. In addition, I enabled sampling
    of episode segments into the new DB directory (v1 in this case), so the full command
    line was the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä½¿ç”¨ä»åŸºæœ¬A2Cè®­ç»ƒä¸­å¾—åˆ°çš„æœ€ä½³æ¨¡å‹è¿›è¡Œäº†è®­ç»ƒï¼Œåœ¨æµ‹è¯•ä¸­ï¼Œè¯¥æ¨¡å‹åœ¨580æ­¥å†…è·å¾—äº†460çš„å¥–åŠ±ã€‚æ­¤å¤–ï¼Œæˆ‘å¯ç”¨äº†å°†å›åˆç‰‡æ®µé‡‡æ ·åˆ°æ–°DBç›®å½•ï¼ˆæ­¤å¤„ä¸ºv1ï¼‰çš„åŠŸèƒ½ï¼Œå› æ­¤å®Œæ•´çš„å‘½ä»¤è¡Œå¦‚ä¸‹ï¼š
- en: './01_a2c.py --dev cuda -n v1 -r rw/reward-v0.dat --save save/v1 -m save/v0/model_rw=460-steps=580.dat
    --finetune --db-path v1 This model started to overfit quite quickly and after
    2M steps (3 hours), I stopped the training. FigureÂ [19.6](#x1-359003r6) shows
    the test results (reward and count of steps):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ./01_a2c.py --dev cuda -n v1 -r rw/reward-v0.dat --save save/v1 -m save/v0/model_rw=460-steps=580.dat
    --finetune --db-path v1 è¯¥æ¨¡å‹å¾ˆå¿«å°±å¼€å§‹è¿‡æ‹Ÿåˆï¼Œåœ¨2Mæ­¥ï¼ˆ3å°æ—¶ï¼‰åï¼Œæˆ‘åœæ­¢äº†è®­ç»ƒã€‚å›¾[19.6](#x1-359003r6)æ˜¾ç¤ºäº†æµ‹è¯•ç»“æœï¼ˆå¥–åŠ±å’Œæ­¥éª¤æ•°ï¼‰ï¼š
- en: '![PIC](img/B22150_19_06.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_19_06.png)'
- en: 'FigureÂ 19.6: Test reward (left) and steps (right) during the fine-tuning'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾19.6ï¼šå¾®è°ƒè¿‡ç¨‹ä¸­æµ‹è¯•å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæ­¥éª¤ï¼ˆå³ï¼‰
- en: 'FigureÂ [19.7](#x1-359004r7) shows the training reward (predicted by the model)
    and the total loss:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[19.7](#x1-359004r7)æ˜¾ç¤ºäº†è®­ç»ƒå¥–åŠ±ï¼ˆç”±æ¨¡å‹é¢„æµ‹ï¼‰å’Œæ€»æŸå¤±ï¼š
- en: '![PIC](img/B22150_19_07.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_19_07.png)'
- en: 'FigureÂ 19.7: Training reward (left) and total loss (right) during the fine-tuning'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾19.7ï¼šå¾®è°ƒè¿‡ç¨‹ä¸­è®­ç»ƒå¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæ€»æŸå¤±ï¼ˆå³ï¼‰
- en: The best model was stored at the 500K training step and it was able to get a
    reward of 900 in 1,120 steps. In comparison to the original model, this is quite
    an improvement.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨500Kè®­ç»ƒæ­¥æ—¶ï¼Œå®ƒèƒ½å¤Ÿåœ¨1,120æ­¥å†…è·å¾—900çš„å¥–åŠ±ã€‚ä¸åŸå§‹æ¨¡å‹ç›¸æ¯”ï¼Œè¿™æ˜¯ä¸€ä¸ªç›¸å½“å¤§çš„æ”¹è¿›ã€‚
- en: 'A video recording of this model is available here: [https://youtu.be/LnPwuyVrj9g](https://youtu.be/LnPwuyVrj9g).
    From the gameplay, we see that the agent learned how to refill the oxygen and
    is now spending some time in the middle of the screen. I also had the impression
    that it picked divers more intentionally (but I havenâ€™t done specific labeling
    for this behavior). So, overall, the method works and it is quite impressive that
    we can teach the agent something new with just 100 labels.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹çš„è§†é¢‘è®°å½•å¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹ï¼š[https://youtu.be/LnPwuyVrj9g](https://youtu.be/LnPwuyVrj9g)ã€‚ä»æ¸¸æˆç©æ³•æ¥çœ‹ï¼Œæˆ‘ä»¬çœ‹åˆ°ä»£ç†å­¦ä¼šäº†å¦‚ä½•è¡¥å……æ°§æ°”ï¼Œå¹¶ä¸”ç°åœ¨åœ¨å±å¹•ä¸­å¤®åœç•™äº†ä¸€æ®µæ—¶é—´ã€‚æˆ‘ä¹Ÿæœ‰å°è±¡å®ƒæ›´æœ‰æ„åœ°é€‰æ‹©äº†æ½œæ°´å‘˜ï¼ˆä½†æˆ‘å¹¶æ²¡æœ‰ä¸ºè¿™ç§è¡Œä¸ºåšå…·ä½“æ ‡æ³¨ï¼‰ã€‚æ€»ä½“æ¥è¯´ï¼Œè¿™ä¸ªæ–¹æ³•æœ‰æ•ˆï¼Œå¹¶ä¸”ä»…å‡­100ä¸ªæ ‡ç­¾å°±èƒ½æ•™ä¼šä»£ç†ä¸€äº›æ–°ä¸œè¥¿ï¼ŒçœŸçš„å¾ˆä»¤äººå°è±¡æ·±åˆ»ã€‚
- en: Letâ€™s try to improve the model further with more labeling.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡æ›´å¤šçš„æ ‡æ³¨è¿›ä¸€æ­¥æ”¹è¿›æ¨¡å‹ã€‚
- en: The second round of the experiment
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬äºŒè½®å®éªŒ
- en: 'On the second round, I did more labeling: 50 pairs from the v0 DB and 50 pairs
    from segments stored during the fine-tuning (v1 DB). The database generated during
    the fine-tuning (v1) contains many more segments with the submarine floating on
    the surface, which confirms that our pipeline is working as expected. During the
    labeling, I also put more emphasis on oxygen refill segments.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬äºŒè½®å®éªŒä¸­ï¼Œæˆ‘åšäº†æ›´å¤šçš„æ ‡æ³¨ï¼š50å¯¹æ¥è‡ªv0æ•°æ®åº“ï¼Œ50å¯¹æ¥è‡ªå¾®è°ƒè¿‡ç¨‹ä¸­å­˜å‚¨çš„ç‰‡æ®µï¼ˆv1æ•°æ®åº“ï¼‰ã€‚åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ•°æ®åº“ï¼ˆv1ï¼‰åŒ…å«äº†æ›´å¤šçš„æ½œè‰‡æ¼‚æµ®åœ¨æ°´é¢çš„ç‰‡æ®µï¼Œè¿™è¯æ˜æˆ‘ä»¬çš„ç®¡é“è¿è¡Œæ­£å¸¸ã€‚åœ¨æ ‡æ³¨æ—¶ï¼Œæˆ‘ä¹Ÿæ›´åŠ é‡è§†æ°§æ°”è¡¥å……çš„ç‰‡æ®µã€‚
- en: After labeling, I retrained the reward model, which only took several minutes.
    Then, fine-tuning of the best v1 model (with a reward of 900 and 1,120 steps)
    was performed using the reward model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‡æ³¨åï¼Œæˆ‘é‡æ–°è®­ç»ƒäº†å¥–åŠ±æ¨¡å‹ï¼Œè¿™åªç”¨äº†å‡ åˆ†é’Ÿã€‚ç„¶åï¼Œä½¿ç”¨å¥–åŠ±æ¨¡å‹å¯¹æœ€ä½³v1æ¨¡å‹ï¼ˆå¥–åŠ±ä¸º900ï¼Œæ­¥æ•°ä¸º1,120ï¼‰è¿›è¡Œäº†å¾®è°ƒã€‚
- en: 'FigureÂ [19.8](#x1-360002r8) and FigureÂ [19.9](#x1-360003r9) contain charts
    with test results, training the reward, and the loss:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾[19.8](#x1-360002r8)å’Œå›¾[19.9](#x1-360003r9)åŒ…å«äº†æµ‹è¯•ç»“æœçš„å›¾è¡¨ã€å¥–åŠ±è®­ç»ƒå’ŒæŸå¤±ï¼š
- en: '![PIC](img/B22150_19_08.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_19_08.png)'
- en: 'FigureÂ 19.8: Test reward (left) and steps (right) during the fine-tuning'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾19.8ï¼šå¾®è°ƒè¿‡ç¨‹ä¸­çš„æµ‹è¯•å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæ­¥æ•°ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_19_09.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_19_09.png)'
- en: 'FigureÂ 19.9: Training reward (left) and total loss (right) during the fine-tuning'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾19.9ï¼šå¾®è°ƒè¿‡ç¨‹ä¸­çš„è®­ç»ƒå¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæ€»æŸå¤±ï¼ˆå³ï¼‰
- en: 'After 1.5M steps (2 hours), the training got stuck, but the best model wasnâ€™t
    better than the best model of v1: the best model got a reward of 860 in 1,084
    steps.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨1.5Mæ­¥ï¼ˆ2å°æ—¶ï¼‰ä¹‹åï¼Œè®­ç»ƒåœæ»äº†ï¼Œä½†æœ€ä½³æ¨¡å‹å¹¶ä¸æ¯”v1çš„æœ€ä½³æ¨¡å‹æ›´å¥½ï¼šæœ€ä½³æ¨¡å‹åœ¨1,084æ­¥ä¸­è·å¾—äº†860çš„å¥–åŠ±ã€‚
- en: The third round of the experiment
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰è½®å®éªŒ
- en: Here, I paid more attention during the labeling, trying to prioritize not just
    oxygen refill, but also better fish shooting and diver pickup. Unfortunately,
    100 pairs gave just a couple of examples of divers, so more labeling is needed
    to teach the agent this behavior.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘åœ¨æ ‡æ³¨æ—¶æ›´åŠ æ³¨æ„ï¼Œä¸ä»…ä¼˜å…ˆè€ƒè™‘æ°§æ°”è¡¥å……ï¼Œè¿˜è€ƒè™‘äº†æ›´å¥½çš„é±¼ç±»å°„å‡»å’Œæ½œæ°´å‘˜æ¥å–ã€‚ä¸å¹¸çš„æ˜¯ï¼Œ100å¯¹æ ‡ç­¾ä¸­åªå‡ºç°äº†å‡ ä¸ªæ½œæ°´å‘˜çš„ä¾‹å­ï¼Œå› æ­¤éœ€è¦æ›´å¤šçš„æ ‡æ³¨æ¥æ•™ä¼šä»£ç†è¿™ç§è¡Œä¸ºã€‚
- en: Regarding the divers, it might be that the agent doesnâ€™t pick them up just because
    they are very hard to distinguish from the background, so on a grayscale image,
    they are invisible. To fix that, we can tweak the contrast in our Atari wrappers.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºæ½œæ°´å‘˜ï¼Œä»£ç†å¯èƒ½æ²¡æœ‰æ¥å–ä»–ä»¬ï¼Œå› ä¸ºæ½œæ°´å‘˜ä¸èƒŒæ™¯éå¸¸éš¾ä»¥åŒºåˆ†ï¼Œåœ¨ç°åº¦å›¾åƒä¸­æ˜¯ä¸å¯è§çš„ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒæ•´AtariåŒ…è£…å™¨ä¸­çš„å¯¹æ¯”åº¦ã€‚
- en: After the reward model retraining, A2C fine-tuning was started. I also ran it
    for almost 2M steps for 3 hours and the results were interesting. At the end of
    the training (check FigureÂ [19.10](#x1-361003r10) and FigureÂ [19.11](#x1-361004r11)),
    the boat during the testing reached 5,000 steps (which is the limit I set in the
    environment), but the score was fairly low. Most likely, the submarine just stayed
    on the surface, which is very safe, but not what we want â€“ this could be because
    of labeled samples. Strangely, when I tried to record the video of those later
    models, their behavior was different and the number of steps was much lower, which
    could be an indication of some testing bug.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¥–åŠ±æ¨¡å‹é‡æ–°è®­ç»ƒåï¼Œå¼€å§‹äº†A2Cçš„å¾®è°ƒã€‚æˆ‘ä¹Ÿè¿è¡Œäº†å¤§çº¦2Mæ­¥ï¼ŒæŒç»­äº†3å°æ—¶ï¼Œç»“æœå¾ˆæœ‰è¶£ã€‚åœ¨è®­ç»ƒç»“æŸæ—¶ï¼ˆæŸ¥çœ‹å›¾[19.10](#x1-361003r10)å’Œå›¾[19.11](#x1-361004r11)ï¼‰ï¼Œæµ‹è¯•ä¸­çš„èˆ¹åªè¾¾åˆ°äº†5,000æ­¥ï¼ˆè¿™æ˜¯æˆ‘åœ¨ç¯å¢ƒä¸­è®¾å®šçš„é™åˆ¶ï¼‰ï¼Œä½†å¾—åˆ†ç›¸å¯¹è¾ƒä½ã€‚å¾ˆå¯èƒ½ï¼Œæ½œè‰‡åªæ˜¯åœç•™åœ¨æ°´é¢ä¸Šï¼Œè¿™æ˜¯éå¸¸å®‰å…¨çš„ï¼Œä½†è¿™ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„â€”â€”è¿™å¯èƒ½æ˜¯ç”±äºæ ‡æ³¨æ ·æœ¬çš„åŸå› ã€‚å¥‡æ€ªçš„æ˜¯ï¼Œå½“æˆ‘å°è¯•å½•åˆ¶è¿™äº›åæœŸæ¨¡å‹çš„è§†é¢‘æ—¶ï¼Œå®ƒä»¬çš„è¡Œä¸ºå‘ç”Ÿäº†å˜åŒ–ï¼Œæ­¥æ•°ä¹Ÿæ˜æ˜¾è¾ƒä½ï¼Œè¿™å¯èƒ½æ˜¯æµ‹è¯•ä¸­çš„æŸä¸ªbugã€‚
- en: '![PIC](img/B22150_19_10.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_19_10.png)'
- en: 'FigureÂ 19.10: Test reward (left) and steps (right) during the fine-tuning'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾19.10ï¼šå¾®è°ƒè¿‡ç¨‹ä¸­çš„æµ‹è¯•å¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæ­¥éª¤æ•°ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_19_11.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_19_11.png)'
- en: 'FigureÂ 19.11: Training reward (left) and total loss (right) during the fine-tuning'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾19.11ï¼šå¾®è°ƒè¿‡ç¨‹ä¸­è®­ç»ƒå¥–åŠ±ï¼ˆå·¦ï¼‰å’Œæ€»æŸå¤±ï¼ˆå³ï¼‰
- en: 'Before the overfitting, the training generated several policies that were better
    than the v2 models. For example, in this recording, the agent refilled the oxygen
    twice and got a score of 1,820 during the 1,613 steps: [https://youtu.be/DVe_9b3gdxU](https://youtu.be/DVe_9b3gdxU).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿‡æ‹Ÿåˆä¹‹å‰ï¼Œè®­ç»ƒç”Ÿæˆäº†å‡ ç§æ¯”v2æ¨¡å‹æ›´å¥½çš„ç­–ç•¥ã€‚ä¾‹å¦‚ï¼Œåœ¨è¿™ä¸ªå½•éŸ³ä¸­ï¼Œä»£ç†è¿›è¡Œäº†ä¸¤æ¬¡æ°§æ°”è¡¥å……ï¼Œå¹¶åœ¨1,613æ­¥ä¸­è·å¾—äº†1,820åˆ†ï¼š[https://youtu.be/DVe_9b3gdxU](https://youtu.be/DVe_9b3gdxU)ã€‚
- en: Overall results
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ€»ä½“ç»“æœ
- en: In the following table, I have summarized the information about the experiment
    rounds and the results we got.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹è¡¨ä¸­ï¼Œæˆ‘æ€»ç»“äº†å®éªŒå›åˆçš„ç›¸å…³ä¿¡æ¯å’Œæˆ‘ä»¬å¾—åˆ°çš„ç»“æœã€‚
- en: '| Step | Labels | Reward | Steps | Video |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| æ­¥éª¤ | æ ‡ç­¾ | å¥–åŠ± | æ­¥éª¤æ•° | è§†é¢‘ |'
- en: '| Initial | None | 460 | 580 | [https://youtu.be/R_H3pXu-7cw](https://youtu.be/R_H3pXu-7cw)
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| åˆå§‹ | æ—  | 460 | 580 | [https://youtu.be/R_H3pXu-7cw](https://youtu.be/R_H3pXu-7cw)
    |'
- en: '| v1 | 100 | 900 | 1120 | [https://youtu.be/LnPwuyVrj9g](https://youtu.be/LnPwuyVrj9g)
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| v1 | 100 | 900 | 1120 | [https://youtu.be/LnPwuyVrj9g](https://youtu.be/LnPwuyVrj9g)
    |'
- en: '| v2 | 200 | 860 | 1083 |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| v2 | 200 | 860 | 1083 |  |'
- en: '| v3 | 300 | 1820 | 1613 | [https://youtu.be/DVe_9b3gdxU](https://youtu.be/DVe_9b3gdxU)
    |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| v3 | 300 | 1820 | 1613 | [https://youtu.be/DVe_9b3gdxU](https://youtu.be/DVe_9b3gdxU)
    |'
- en: 'TableÂ 19.1: Summary of experiment rounds'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨19.1ï¼šå®éªŒå›åˆæ€»ç»“
- en: As you can see, with just 300 labels, we were able to increase the scoring by
    almost 4 times. As an exercise, you can try to teach the agent to pick up divers,
    which might result in a much better score if done properly.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œå‡­å€Ÿä»…ä»…300ä¸ªæ ‡ç­¾ï¼Œæˆ‘ä»¬æˆåŠŸå°†åˆ†æ•°æé«˜äº†è¿‘4å€ã€‚ä½œä¸ºä¸€ä¸ªç»ƒä¹ ï¼Œä½ å¯ä»¥å°è¯•æ•™ä»£ç†æ¡èµ·æ½œæ°´å‘˜ï¼Œå¦‚æœåšå¾—å¥½ï¼Œå¯èƒ½ä¼šå¾—åˆ°æ›´å¥½çš„æˆç»©ã€‚
- en: Another experiment that might be worth doing is to fine-tune the original v0
    model, instead of the best models from the previous step. It might lead to better
    results, as the training has more time before overfitting.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªå¯èƒ½å€¼å¾—å°è¯•çš„å®éªŒæ˜¯å¾®è°ƒåŸå§‹v0æ¨¡å‹ï¼Œè€Œä¸æ˜¯å‰ä¸€æ­¥ä¸­çš„æœ€ä½³æ¨¡å‹ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´æ›´å¥½çš„ç»“æœï¼Œå› ä¸ºè®­ç»ƒåœ¨è¿‡æ‹Ÿåˆä¹‹å‰æœ‰æ›´å¤šæ—¶é—´ã€‚
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: In this chapter, weâ€™ve taken a look at the recent addition of RLHF to the RL
    toolbox. This method, at the core of the LLM training pipeline, allows you to
    increase the quality of models. In the chapter, we implemented RLHF and applied
    it to the SeaQuest Atari game, which should have illustrated to you how this method
    could be used in RL pipelines for model improvement.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬äº†è§£äº†RLHFåœ¨RLå·¥å…·ç®±ä¸­çš„æ–°åŠ å…¥ã€‚è¿™ç§æ–¹æ³•æ˜¯LLMè®­ç»ƒæµç¨‹çš„æ ¸å¿ƒï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„è´¨é‡ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†RLHFï¼Œå¹¶å°†å…¶åº”ç”¨äºSeaQuest
    Atariæ¸¸æˆï¼Œè¿™åº”è¯¥å‘ä½ å±•ç¤ºäº†è¿™ç§æ–¹æ³•å¦‚ä½•åœ¨RLæµæ°´çº¿ä¸­ç”¨äºæ¨¡å‹æ”¹è¿›ã€‚
- en: 'In the next chapter, weâ€™ll discuss a different family of RL methods: AlphaGo,
    AlphaZero, and MuZero.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¦ä¸€ç±»RLæ–¹æ³•ï¼šAlphaGoã€AlphaZeroå’ŒMuZeroã€‚
