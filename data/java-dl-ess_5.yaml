- en: Chapter 5. Exploring Java Deep Learning Libraries – DL4J, ND4J, and More
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 探索Java深度学习库——DL4J、ND4J及更多
- en: In the previous chapters, you learned the core theories of deep learning algorithms
    and implemented them from scratch. While we can now say that implementations of
    deep learning are not so difficult, we can't deny the fact that it still takes
    some time to implement models. To mitigate this situation, you'll learn how to
    write code with the Java library of deep learning in this chapter so that we can
    focus more on the critical part of data analysis rather than the trivial part.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，你学习了深度学习算法的核心理论，并从零开始实现了它们。虽然我们现在可以说，深度学习的实现并不那么困难，但我们不能否认，实施模型仍然需要一些时间。为了解决这个问题，在本章中，你将学习如何使用Java深度学习库编写代码，这样我们就可以更多地专注于数据分析的关键部分，而不是琐碎的细节。
- en: 'The topics you''ll learn about in this chapter are:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中你将学习的主题包括：
- en: An introduction to the deep learning library of Java
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java深度学习库简介
- en: Example code and how to write your own code with the library
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例代码以及如何使用该库编写你自己的代码
- en: Some additional ways to optimize the model to get a higher precision rate
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些优化模型以提高精度率的附加方法
- en: Implementing from scratch versus a library/framework
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头实现与使用库/框架
- en: 'We implemented the machine learning algorithms of neural networks in [Chapter
    2](ch02.html "Chapter 2. Algorithms for Machine Learning – Preparing for Deep
    Learning"), *Algorithms for Machine Learning – Preparing for Deep Learning*, and
    many deep learning algorithms from scratch in [Chapter 3](ch03.html "Chapter 3. Deep
    Belief Nets and Stacked Denoising Autoencoders"), *Deep Belief Nets and Stacked
    Denoising Autoencoders* and [Chapter 4](ch04.html "Chapter 4. Dropout and Convolutional
    Neural Networks"), *Dropout and Convolutional Neural Networks*. Of course, we
    can apply our own code to practical applications with some customizations, but
    we have to be careful when we want to utilize them because we can''t deny the
    possibility that they might cause several problems in the future. What could they
    be? Here are the possible situations:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第2章](ch02.html "第2章 机器学习算法——为深度学习做准备")中实现了神经网络的机器学习算法，*机器学习算法——为深度学习做准备*，并在[第3章](ch03.html
    "第3章 深度信念网络与堆叠去噪自编码器")，*深度信念网络与堆叠去噪自编码器*和[第4章](ch04.html "第4章 Dropout和卷积神经网络")，*Dropout和卷积神经网络*中从头实现了许多深度学习算法。当然，我们可以通过一些定制将自己的代码应用于实际应用，但在我们想要利用它们时必须小心，因为我们不能否认它们未来可能引发的问题。可能是什么问题呢？以下是可能的情况：
- en: The code we wrote has some missing parameters for better optimization because
    we implemented just the essence of the algorithms for simplicity and so you better
    understand the concepts. While you can still train and optimize the model with
    them, you could get higher precision rates by adding another parameter of your
    own implementation.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们编写的代码缺少一些更好的优化参数，因为我们只是为了简化问题并帮助你更好地理解概念而实现了算法的核心部分。虽然你仍然可以使用这些代码来训练和优化模型，但通过添加你自己实现的额外参数，你可以获得更高的精度率。
- en: As mentioned in the previous chapter, there are still many useful deep learning
    algorithms not explained in this book. While you now have the core components
    of the deep learning algorithms, you might need to implement additional classes
    or methods to get the desired results in your fields and applications.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前章所述，本书中仍有许多有用的深度学习算法未被解释。虽然你现在已经掌握了深度学习算法的核心组件，但你可能需要实现额外的类或方法，以在你的领域和应用中获得期望的结果。
- en: The assumed time consumption will be very critical to the application, especially
    when you think of analyzing huge amounts of data. It is true that Java has a better
    performance in terms of speed compared to other popular languages such as Python
    and R, but you may still need to consider the time cost. One plausible approach
    to solve the problem is using GPU instead of CPU, but this requires complex implementations
    to adjust the code for GPU computing.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假定的时间消耗对应用非常关键，尤其是当你考虑分析大量数据时。确实，相较于Python和R等其他流行语言，Java在速度方面有更好的性能，但你仍然需要考虑时间成本。一个可行的解决方案是使用GPU代替CPU，但这需要复杂的实现来调整代码以适应GPU计算。
- en: These are the main causal issues, and you might also need to take into consideration
    that we don't handle exceptions in the code.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是主要的因果问题，你可能还需要考虑到我们没有在代码中处理异常。
- en: This does not mean that implementing from scratch would have fatal errors. The
    code we wrote can be used substantially as an application for certain scaled data;
    however, you need to take into consideration that you require further coding for
    the fundamental parts you have implemented if you use large-scale data mining,
    where, generally, deep learning is required. This means you need to bear in mind
    that implementation from scratch has more flexibility as you can change the code
    if required, but at the same time it has a negative side in that the algorithm's
    tuning and maintenance also has to be done independently.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着从头实现会有致命的错误。我们编写的代码可以作为处理某些规模数据的应用程序；然而，如果你使用大规模数据挖掘，通常需要深度学习，那么你需要考虑，你所实现的基础部分需要进一步编码。这意味着，你需要记住，从头实现具有更大的灵活性，因为在必要时你可以更改代码，但同时它也有负面影响，即算法的调优和维护必须独立完成。
- en: 'So, how can we solve the problems just mentioned? This is where a library (or
    framework) comes in. Thanks to active research into deep learning globally, there
    are many libraries developed and published using various programming languages
    all over the world. Of course, each library has its respective features but the
    features that every library commonly has can be summarized as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何解决刚才提到的问题呢？这就是库（或框架）派上用场的地方。得益于全球对深度学习的积极研究，世界各地有许多使用各种编程语言开发并发布的库。当然，每个库都有各自的特点，但每个库共同具备的特点可以总结如下：
- en: A model's training can be done just by defining a layer structure of deep learning.
    You can focus on parameter setting and tuning, and you don't need to think about
    the algorithms.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只需定义深度学习的层结构，就可以完成模型的训练。你可以专注于参数设置和调优，而无需考虑算法。
- en: Most of the libraries are open to the public as open source projects and are
    actively updated daily. Therefore, if there are bugs, there's a high possibility
    that these bugs will be fixed quickly (and, of course, committing to a project
    by fixing it yourself should be welcomed).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数库作为开源项目向公众开放，并且每天都在积极更新。因此，如果出现bug，很有可能这些bug会被迅速修复（当然，如果你自己修复它并提交到项目中，这也是受欢迎的）。
- en: It's easy to switch between running the program on CPU or on GPU. As a library
    supplements the cumbersome coding element of GPU computing, you can just focus
    on the implementation without considering CPU or GPU, if a machine supports GPU.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在CPU和GPU之间切换程序运行非常容易。由于库补充了GPU计算中的繁琐编码部分，你可以专注于实现，而无需考虑CPU或GPU，只要机器支持GPU。
- en: Long story short, you can leave out all the parts that could be brutal when
    you implement to a library from scratch. Thanks to this, you can take more time
    on the essential data mining section, hence if you want to utilize practical applications,
    there's a high possibility that you can perform data analysis more efficiently
    using a library.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，你可以省去所有在从头实现库时可能会遇到的麻烦部分。得益于此，你可以将更多时间花在核心数据挖掘部分，因此，如果你希望利用实际应用，使用库进行数据分析时效率更高的可能性也会大大增加。
- en: 'However, depending too much on a library isn''t good. Using a library is convenient,
    but on the flip side, it has some demerits, as listed here:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，过度依赖库并不好。使用库虽然方便，但也有一些缺点，如下所列：
- en: Since you can build various deep learning models easily, you can implement without
    having a concrete understanding of what theory the model is supported by. This
    might not be a problem if we only consider implementations related to a specific
    model, but there will be a risk you can't deal with when you want to combine other
    methods or consider other methods when applying the model.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于你可以轻松构建各种深度学习模型，你可以在没有具体理解模型所依赖的理论的情况下进行实现。如果我们只考虑与特定模型相关的实现，这可能不是问题，但当你想结合其他方法或在应用模型时考虑其他方法时，可能会遇到无法处理的风险。
- en: You can't use algorithms not supported by a library, hence there might be a
    case where you can't choose a model you would like to use. This can be solved
    by a version upgrade, but on the other hand, there's a possibility that some part
    of a past implementation might be deprecated due to a change of specification
    by the upgrade. Moreover, we can't deny the possibility that the development of
    a library is suddenly terminated or utilization turns out to be chargeable due
    to a sudden change in its license. In these cases, there's a risk that the code
    you have developed up to this point cannot be used.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不能使用库中不支持的算法，因此可能会遇到无法选择自己想用的模型的情况。这个问题可以通过版本升级来解决，但另一方面，过去某些实现的部分可能由于规范更改而被弃用。此外，我们不能排除库的开发突然终止或由于许可证的突然变更，使用该库变为收费的可能性。在这些情况下，你之前开发的代码可能无法再使用。
- en: The precision rate you can get from experimentation depends on how a library
    is implemented. For example, if we conduct an experiment with the same neural
    network model in two different libraries, the results we obtain can be hugely
    changed. This is because neural network algorithms include a stochastic operation,
    and the calculation accuracy of a machine is limited, that is, calculated values
    during the process could have fluctuations based on the method of implementation.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你从实验中获得的精度取决于库的实现方式。例如，如果我们在两个不同的库中使用相同的神经网络模型进行实验，得到的结果可能会有很大的不同。这是因为神经网络算法包括随机操作，而且机器的计算精度是有限的，即在计算过程中，基于实现方法的不同，计算值可能会有波动。
- en: Because you well understand the fundamental concepts and theories of deep learning
    algorithms thanks to the previous chapters, we don't need to worry about the first
    point. However, we need to be careful about the remaining two points. From the
    next section on, implementation using a library is introduced and we'll be more
    conscious of the merits and demerits we just discussed.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你在前几章中已经很好地理解了深度学习算法的基本概念和理论，因此我们不需要担心第一个问题。然而，我们需要小心剩下的两个问题。从下一节开始，将介绍如何使用库进行实现，并且我们将更加关注刚刚讨论的优缺点。
- en: Introducing DL4J and ND4J
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍DL4J和ND4J
- en: A lot of the libraries of deep learning have been developed all over the world.
    In November 2015, **TensorFlow** ([http://www.tensorflow.org/](http://www.tensorflow.org/)),
    a machine learning/deep learning library developed by Google, became open to the
    public and attracted great attention.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 全球范围内已经开发了很多深度学习库。2015年11月，**TensorFlow**（[http://www.tensorflow.org/](http://www.tensorflow.org/)），由Google开发的机器学习/深度学习库，公开发布并引起了广泛关注。
- en: When we look at the programming language with which libraries are being developed,
    most of them open to the public are developed by Python or use the Python API.
    TensorFlow is developed with C++ on the backend but it's also possible to write
    code with Python. This book focuses on Java to learn deep learning, hence the
    libraries developed by other languages will be briefly introduced in [Chapter
    7](ch07.html "Chapter 7. Other Important Deep Learning Libraries"), *Other Important
    Deep Learning Libraries*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们看一下开发库所使用的编程语言时，大多数公开的库都是用Python开发的或使用Python API。TensorFlow的后端是用C++开发的，但也可以用Python编写代码。本书重点讲解使用Java学习深度学习，因此其他语言开发的库将在[第7章](ch07.html
    "第7章. 其他重要的深度学习库")中简要介绍，*其他重要的深度学习库*。
- en: 'So, what Java-based libraries do we have? Actually, there are a few cases that
    are actively developed (perhaps there are also some projects not open to public).
    However, there is only one library we can use practically: **Deeplearning4j**
    (**DL4J**). The official project page URL is [http://deeplearning4j.org/](http://deeplearning4j.org/).
    This library is also open source and the source code is all published on GitHub.
    The URL is [https://github.com/deeplearning4j/deeplearning4j](https://github.com/deeplearning4j/deeplearning4j).
    This library was developed by Skymind ([http://www.skymind.io/](http://www.skymind.io/)).
    What kind of library is this? If you look at the project page, it''s introduced
    as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们有哪些基于Java的库可以使用呢？实际上，积极开发的库并不多（也许还有一些未公开的项目）。然而，我们实际上可以使用的库只有一个：**Deeplearning4j**（**DL4J**）。官方项目页面的URL是[http://deeplearning4j.org/](http://deeplearning4j.org/)。这个库也是开源的，源代码全部发布在GitHub上，网址是[https://github.com/deeplearning4j/deeplearning4j](https://github.com/deeplearning4j/deeplearning4j)。该库由Skymind开发（[http://www.skymind.io/](http://www.skymind.io/)）。这个库是什么样的库呢？如果你查看项目页面，它是这样介绍的：
- en: '*"Deeplearning4j is the first commercial-grade, open-source, distributed deep-learning
    library written for Java and Scala. Integrated with Hadoop and Spark, DL4J is
    designed to be used in business environments, rather than as a research tool.
    Skymind is its commercial support arm.*'
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*"Deeplearning4j是第一个为Java和Scala编写的商业级开源分布式深度学习库。与Hadoop和Spark集成，DL4J旨在用于商业环境，而非作为研究工具。Skymind是它的商业支持部门。"*'
- en: ''
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Deeplearning4j aims to be cutting-edge plug and play, more convention than
    configuration, which allows for fast prototyping for non-researchers. DL4J is
    customizable at scale. Released under the Apache 2.0 license, all derivatives
    of DL4J belong to their authors."*'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*Deeplearning4j旨在成为尖端的即插即用，注重约定而非配置，这使得非研究人员能够快速进行原型开发。DL4J具有可扩展的自定义功能。它在Apache
    2.0许可证下发布，DL4J的所有衍生作品归其作者所有。*'
- en: When you read this, you will see that the biggest feature of DL4J is that it
    was designed on the premise of being integrated with Hadoop. This indicates that
    DL4J suits the processing of large-scale data and is more scalable than other
    libraries. Moreover, DL4J supports GPU computing, so it's possible to process
    data even faster.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当你阅读到这里时，你会发现DL4J的最大特点是它是以与Hadoop集成为前提设计的。这表明，DL4J非常适合处理大规模数据，且比其他库更具可扩展性。此外，DL4J支持GPU计算，因此能够更快速地处理数据。
- en: 'Also, DL4J uses a library called **N-Dimensional Arrays for Java** (**ND4J**)
    internally. The project page is [http://nd4j.org/](http://nd4j.org/). The same
    as DL4J, this library is also published on GitHub as an open source project: [https://github.com/deeplearning4j/nd4j](https://github.com/deeplearning4j/nd4j).
    The developer of the library is the same as DL4J, Skymind. As you can see from
    the name of the library, this is a scientific computing library that enables us
    to handle versatile *n*-dimensional array objects. If you are a Python developer,
    it might be easier for you to understand this if you imagine NumPy, as ND4J is
    a library inspired by NumPy. ND4J also supports GPU computing and the reason why
    DL4J is able to do GPU integration is because it uses ND4J internally.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DL4J内部使用一个名为**Java的N维数组**（**ND4J**）的库。该项目页面为[http://nd4j.org/](http://nd4j.org/)。与DL4J相同，这个库也作为开源项目发布在GitHub上：[https://github.com/deeplearning4j/nd4j](https://github.com/deeplearning4j/nd4j)。该库的开发者与DL4J相同，都是Skymind。正如库名所示，这是一个科学计算库，使我们能够处理多功能的*n*维数组对象。如果你是Python开发者，可以通过类比NumPy来更容易理解它，因为ND4J是一个受NumPy启发的库。ND4J还支持GPU计算，DL4J能够进行GPU集成的原因就是它在内部使用了ND4J。
- en: What good can come from working with them on GPUs? Let's briefly look at this
    point. The biggest difference between CPU and GPU is the difference in the number
    of cores. GPU is, as represented in its name, a graphical processing unit, originally
    an integrated circuit for image processing. This is why GPU is well optimized
    to handle the same commands simultaneously. Parallel processing is its forte.
    On the other hand, as CPU needs to process various commands, these tasks are basically
    made to be processed in order. Compared to CPU, GPU is good at processing huge
    numbers of simple tasks, therefore calculations such as training iterations of
    deep learning is its field of expertise.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上与它们一起工作能带来什么好处？让我们简要看一下这一点。CPU和GPU之间最大的区别在于核心数量的差异。GPU，顾名思义，是一个图形处理单元，最初是一个用于图像处理的集成电路。这也是GPU能够优化同时处理相同命令的原因。并行处理是它的强项。另一方面，CPU需要处理各种命令，这些任务通常是按顺序处理的。与CPU相比，GPU擅长处理大量简单的任务，因此像深度学习训练迭代这样的计算是它的专长。
- en: Both ND4J and DL4J are very useful for research and data mining with deep learning.
    From the next section on, we'll see how these are used for deep learning in simple
    examples. You can easily understand the contents because you should already understand
    the core theories of deep learning by now. Hopefully, you can make use of this
    for your fields of study or business.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ND4J和DL4J对于深度学习中的研究和数据挖掘非常有用。从下一节开始，我们将通过简单的例子来看它们是如何用于深度学习的。因为你现在应该已经理解了深度学习的核心理论，所以你可以很容易理解这些内容。希望你能将其应用于你的研究领域或业务中。
- en: Implementations with ND4J
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用ND4J的实现
- en: 'As there are many cases where ND4J alone can be used conveniently, let''s briefly
    grasp how to use ND4J before looking into the explanation of DL4J. If you would
    like to use ND4J alone, once you create a new Maven project, then you can use
    ND4J by adding the following code to `pom.xml`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有很多情况下ND4J本身就能方便地使用，因此在深入DL4J的解释之前，让我们简要了解一下如何使用ND4J。如果你只打算使用ND4J，一旦创建了一个新的Maven项目，你可以通过在`pom.xml`中添加以下代码来使用ND4J：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, `<nd4j.version>` describes the latest version of ND4J, but please check
    whether it is updated when you actually implement the code. Also, switching from
    CPU to GPU is easy while working with ND4J. If you have CUDA installed with version
    7.0, then what you do is just define `artifactId` as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`<nd4j.version>`描述了ND4J的最新版本，但在实际实现代码时，请检查它是否已经更新。另外，使用ND4J时从CPU切换到GPU非常简单。如果你已经安装了CUDA
    7.0版本，那么你只需按照以下方式定义`artifactId`：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can replace the version of `<artifactId>` depending on your configuration.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以根据你的配置替换`<artifactId>`的版本。
- en: 'Let''s look at a simple example of what calculations are possible with ND4J.
    The type we utilize with ND4J is `INDArray`, that is, an extended type of `Array`.
    We begin by importing the following dependencies:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个使用ND4J进行计算的简单例子。我们在ND4J中使用的类型是`INDArray`，即`Array`的扩展类型。我们首先导入以下依赖：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we define `INDArray` as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义`INDArray`如下：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Nd4j.create` takes two arguments. The former defines the actual values within
    `INDArray`, and the latter defines the shape of the vector (matrix). By running
    this code, you get the following result:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`Nd4j.create`接受两个参数。前者定义了`INDArray`中的实际值，后者定义了向量（矩阵）的形状。通过运行这段代码，你将得到以下结果：'
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Since `INDArray` can output its values with `System.out.print`, it''s easy
    to debug. Calculation with scalar can also be done with ease. Add 1 to `x` as
    shown here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`INDArray`可以通过`System.out.print`输出其值，因此调试非常简单。标量计算也可以轻松完成。像下面这样将1加到`x`：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, you will get the following output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将得到以下输出：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Also, the calculation within `INDArray` can be done easily, as shown in the
    following example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`INDArray`中的计算可以轻松完成，以下例子展示了这一点：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, basic arithmetic operations can be represented as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，基本的算术运算可以表示如下：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'These will return the following result:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些将返回以下结果：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Also, ND4J has destructive arithmetic operators. When you write the `x.addi(y)`
    command, `x` changes its own values so that `System.out.println(x);` will return
    the following output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ND4J具有破坏性算术运算符。当你写下`x.addi(y)`命令时，`x`会改变自己的值，因此`System.out.println(x);`将返回以下输出：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Likewise, `subi`, `muli`, and `divi` are also destructive operators. There are
    also many other methods that can conveniently perform calculations between vectors
    or matrices. For more information, you can refer to [http://nd4j.org/documentation.html](http://nd4j.org/documentation.html),
    [http://nd4j.org/doc/](http://nd4j.org/doc/) and `http://nd4j.org/apidocs/`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，`subi`、`muli` 和 `divi` 也是破坏性操作符。还有许多其他方法可以方便地进行向量或矩阵之间的计算。更多信息可以参考 [http://nd4j.org/documentation.html](http://nd4j.org/documentation.html)，[http://nd4j.org/doc/](http://nd4j.org/doc/)
    和 `http://nd4j.org/apidocs/`。
- en: Let's look at one more example to see how machine learning algorithms can be
    written with ND4J. We'll implement the easiest example, perceptrons, based on
    the source code written in [Chapter 2](ch02.html "Chapter 2. Algorithms for Machine
    Learning – Preparing for Deep Learning"), *Algorithms for Machine Learning – Preparing
    for Deep Learning*. We set the package name `DLWJ.examples.ND4J` and the file
    (class) name `Perceptrons.java`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一个例子，看看如何使用 ND4J 编写机器学习算法。我们将实现最简单的例子——感知机，基于 [第2章](ch02.html "第2章：机器学习算法——为深度学习做准备")
    中编写的源代码，*机器学习算法——为深度学习做准备*。我们设置包名 `DLWJ.examples.ND4J`，文件（类）名为 `Perceptrons.java`。
- en: 'First, let''s add these two lines to import from ND4J:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们添加以下两行从 ND4J 导入：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The model has two parameters: `num` of the input layer and the weight. The
    former doesn''t change from the previous code; however, the latter isn''t `Array`
    but `INDArray`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有两个参数：输入层的 `num` 和权重。前者与之前的代码相同；然而，后者不是 `Array`，而是 `INDArray`：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You can see from the constructor that since the weight of the perceptrons is
    represented as a vector, the number of rows is set to the number of units in the
    input layer and the number of columns to 1\. This definition is written here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从构造函数中可以看到，由于感知机的权重表示为一个向量，因此行数设置为输入层中单元的数量，列数设置为 1。这个定义在这里写出：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, because we define the model parameter as `INDArray`, we also define the
    demo data, training data, and test data as `INDArray`. You can see these definitions
    at the beginning of the main method:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，由于我们将模型参数定义为 `INDArray`，我们还将演示数据、训练数据和测试数据定义为 `INDArray`。你可以在主方法的开头看到这些定义：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When we substitute a value into `INDArray`, we use `put`. Please be careful
    that any value we can set with `put` is only the values of the `scalar` type:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将一个值替换到 `INDArray` 中时，我们使用 `put`。请注意，使用 `put` 设置的任何值只能是 `scalar` 类型的值：
- en: '[PRE15]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The flow from a model building and training is the same as the previous code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建和训练的流程与之前的代码相同：
- en: '[PRE16]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Each piece of training data is given to the `train` method by `getRow()`. First,
    let''s see the entire content of the `train` method:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 每一条训练数据都通过 `getRow()` 被传递给 `train` 方法。首先，我们来看一下 `train` 方法的完整内容：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We first focus our attention on the following code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将注意力集中在以下代码上：
- en: '[PRE18]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This is the part that checks whether the data is classified correctly by perceptions,
    as shown in the following equation:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是检查感知机是否正确分类数据的部分，如下所示的方程：
- en: '![Implementations with ND4J](img/B04779_05_06.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![使用ND4J的实现](img/B04779_05_06.jpg)'
- en: 'You can see from the code that `.mmul()` is for the multiplication between
    vectors or matrices. We wrote this part of the calculation in [Chapter 2](ch02.html
    "Chapter 2. Algorithms for Machine Learning – Preparing for Deep Learning"), *Algorithms
    for Machine Learning – Preparing for Deep Learning*, as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码中可以看到，`.mmul()` 是用于向量或矩阵之间的乘法。我们在 [第2章](ch02.html "第2章：机器学习算法——为深度学习做准备")，*机器学习算法——为深度学习做准备*
    中写了这部分计算，如下所示：
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: By comparing both codes, you can see that multiplication between vectors or
    matrices can be written easily with `INDArray`, and so you can implement the algorithm
    intuitively just by following the equations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对比两段代码，可以看到向量或矩阵之间的乘法可以轻松通过 `INDArray` 来写，因此你可以直观地实现算法，只需要跟随方程即可。
- en: 'The equation to update the model parameters is as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 更新模型参数的方程如下：
- en: '[PRE20]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here, again, you can implement the code like you write a math equation. The
    equation is represented as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，同样，你可以像编写数学方程一样实现代码。方程表示如下：
- en: '![Implementations with ND4J](img/B04779_05_13.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![使用ND4J的实现](img/B04779_05_13.jpg)'
- en: 'The last time we implemented this part, we wrote it with a `for` loop:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 上次我们实现这一部分时，是通过 `for` 循环来编写的：
- en: '[PRE21]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Furthermore, the prediction after the training is also the standard forward
    activation, shown as the following equation:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，训练后的预测也是标准的前向激活，表示为以下方程：
- en: '![Implementations with ND4J](img/B04779_05_15.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![使用 ND4J 实现](img/B04779_05_15.jpg)'
- en: 'Here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里：
- en: '![Implementations with ND4J](img/B04779_05_16.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![使用 ND4J 实现](img/B04779_05_16.jpg)'
- en: 'We can simply define the `predict` method with just a single line inside, as
    follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以仅用一行代码简单地定义 `predict` 方法，如下所示：
- en: '[PRE22]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: When you run the program, you can see its precision and accuracy, and the recall
    is the same as we get with the previous code.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行程序时，可以看到其精度和准确性，召回率与我们使用之前代码得到的结果相同。
- en: Thus, it'll greatly help that you implement the algorithms analogous to mathematical
    equations. We only implement perceptrons here, but please try other algorithms
    by yourself.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过将算法实现类比于数学方程，它将极大地帮助你。我们这里仅实现感知器，但请尝试自行实现其他算法。
- en: Implementations with DL4J
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DL4J 的实现
- en: ND4J is the library that helps you to implement deep learning easily and conveniently.
    However, you have to implement the algorithms yourself, which is not too different
    from its implementation in the previous chapters. In other words, ND4J is just
    a library that makes calculating numerical values easier and is not a library
    that is optimized for deep learning algorithms. One library that makes deep learning
    easier to handle is DL4J. Fortunately, as for DL4J, some example code with typical
    methods is published on GitHub ([https://github.com/deeplearning4j/dl4j-0.4-examples](https://github.com/deeplearning4j/dl4j-0.4-examples)).
    These examples are used on the premise that you are using DL4J's version 0.4-*.
    When you actually clone this repository, please check the latest version again.
    In this section, we'll extract the fundamental part from these sample programs
    and take a look at it. We'll reference the forked repository on [https://github.com/yusugomori/dl4j-0.4-examples](https://github.com/yusugomori/dl4j-0.4-examples)
    as a screenshot in this section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ND4J 是一个帮助你轻松方便实现深度学习的库。然而，你仍然需要自己实现算法，这与前几章的实现方式没有太大区别。换句话说，ND4J 只是一个使得计算数值变得更容易的库，并不是专门为深度学习算法优化的库。使得深度学习更容易处理的库是
    DL4J。幸运的是，关于 DL4J，有一些包含典型方法的示例代码已经发布在 GitHub 上（[https://github.com/deeplearning4j/dl4j-0.4-examples](https://github.com/deeplearning4j/dl4j-0.4-examples)）。这些示例代码的前提是你使用的是
    DL4J 的版本 0.4-*。当你实际克隆该仓库时，请再次检查最新版本。在这一节中，我们将从这些示例程序中提取出基本部分并进行查看。我们将在此节中引用 [https://github.com/yusugomori/dl4j-0.4-examples](https://github.com/yusugomori/dl4j-0.4-examples)
    上的分支仓库作为截图示例。
- en: Setup
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: 'Let''s first set up the environments from our cloned repository. If you''re
    using IntelliJ, you can import the project from **File** | **New** | **Project**
    from existing sources and select the path of the repository. Then, choose **Import
    project from external model** and select **Maven** as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从我们克隆的仓库中设置环境。如果你使用 IntelliJ，可以从 **文件** | **新建** | **从现有源导入项目** 中导入该项目，选择仓库的路径。然后，选择
    **从外部模型导入项目**，并选择 **Maven**，如下所示：
- en: '![Setup](img/B04779_05_01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![设置](img/B04779_05_01.jpg)'
- en: 'You don''t have to do anything special for the other steps except click **Next**.
    Please be careful that the supported versions of JDK are 1.7 or above. This may
    not be a problem because we needed version 1.8 or above in the previous chapters.
    Once you have set it up without a problem, you can confirm the structure of the
    directories as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 除了点击**下一步**，其他步骤不需要做任何特殊操作。请注意，支持的 JDK 版本为 1.7 或更高版本。由于在前几章中我们已经需要版本 1.8 或更高版本，这应该不是问题。一旦顺利完成设置，你可以确认目录结构如下：
- en: '![Setup](img/B04779_05_02.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![设置](img/B04779_05_02.jpg)'
- en: 'Once you have set up the project, let''s first look at `pom.xml`. You can see
    that the description of the packages related to DL4J is written as:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你设置好了项目，我们首先来看一下 `pom.xml`。你可以看到与 DL4J 相关的包的描述如下所示：
- en: '[PRE23]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Also, you can see from the following lines that DL4J depends on ND4J:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可以从以下内容看到 DL4J 依赖于 ND4J：
- en: '[PRE24]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If you would like to run a program on GPU, what you have to do is just change
    this written section. As mentioned in the previous section, this can be written
    as follows if you have CUDA installed:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在 GPU 上运行程序，所需要做的就是修改这一部分代码。如前所述，如果你安装了 CUDA，可以按如下方式编写：
- en: '[PRE25]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, `XXX` is the version of CUDA and depends on your machine's preference.
    It's great to adopt GPU computing only using this. We don't have to do anything
    special and we can focus on implementations of deep learning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`XXX` 是 CUDA 的版本号，取决于你机器的偏好。仅使用这个进行 GPU 计算是非常棒的。我们无需做任何特殊的操作，可以专注于深度学习的实现。
- en: 'The other characteristic library that DL4J develops and uses is **Canova**.
    The part that corresponds to `pom.xml` is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J 开发和使用的另一个特征库是 **Canova**。对应于 `pom.xml` 的部分如下：
- en: '[PRE26]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Canova is also, of course, an open source library and its source code can be
    seen on GitHub at [https://github.com/deeplearning4j/Canova](https://github.com/deeplearning4j/Canova).
    As explained on that page, Canova is the library used to vectorize raw data into
    usable vector formats across the machine learning tools. This also helps us focus
    on the more important part of data mining because data formatting is indispensable
    in whatever research or experiment we're performing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Canova 也是一个开源库，其源代码可以在 GitHub 上查看，地址是 [https://github.com/deeplearning4j/Canova](https://github.com/deeplearning4j/Canova)。正如该页面所述，Canova
    是一个用于将原始数据向量化为机器学习工具可以使用的向量格式的库。这也有助于我们专注于数据挖掘中的更重要部分，因为数据格式化在任何研究或实验中都是不可或缺的。
- en: Build
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建
- en: Let's look at the source code in the examples and see how to build a deep learning
    model. During the process, the terms of deep learning that you haven't yet learned
    are also briefly explained. The examples are implemented with various models such
    as MLP, DBN, and CNN, but there is one problem here. As you can see when looking
    at `README.md`, some methods don't generate good precision. This is because, as
    explained in the previous section, the calculation precision a machine has is
    limited and fluctuation occurring with calculated values during the process depends
    completely on the difference of implementation. Hence, practically, learning can't
    be done properly, although theoretically it should be done well. You can get better
    results by, for example, changing the seed values or adjusting the parameters,
    but as we would like to focus on how to use a library, we'll use a model that
    gets higher precision as an example.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看示例中的源代码，了解如何构建一个深度学习模型。在这个过程中，你还会看到一些你还未学习的深度学习术语，并得到简要解释。这些示例实现了各种模型，如
    MLP、DBN 和 CNN，但这里有一个问题。正如在 `README.md` 中看到的那样，一些方法没有生成良好的精度。这是因为，正如前一部分所解释的那样，机器的计算精度是有限的，在过程中的计算值波动完全依赖于实现的差异。因此，从实际情况来看，学习无法顺利进行，尽管理论上应该是可行的。你可以通过改变种子值或调整参数来获得更好的结果，但由于我们希望专注于如何使用这个库，我们将以一个精度更高的模型为例。
- en: DBNIrisExample.java
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBNIrisExample.java
- en: Let's first look at `DBNIrisExample.java` in the package of `deepbelief`. Iris,
    contained in the filename, is one of the benchmark datasets often used when measuring
    the precision or accuracy of a machine learning method. The dataset contains 150
    pieces of data out of 3 classes of 50 instances each, and each class refers to
    a type of Iris plant. The number of inputs is 4 and the number of outputs is therefore
    3\. One class is linearly separable from the other two; the latter are not linearly
    separable from each other.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先来看看位于 `deepbelief` 包中的 `DBNIrisExample.java`。文件名中的 "Iris" 是一个常用于衡量机器学习方法精度或准确性的基准数据集之一。该数据集包含来自
    3 个类别的 150 条数据，每个类别包含 50 个实例，每个类别对应一种鸢尾花的类型。输入特征数为 4，因此输出的类别数为 3。一个类别可以与另外两个类别线性可分；而后两个类别之间则不是线性可分的。
- en: 'The implementation begins by setting up the configuration. Here are the variables
    that need setting:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 实现从设置配置开始。以下是需要设置的变量：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In DL4J, input data can be up to two-dimensional data, hence you need to assign
    the number of rows and columns of the data. As Iris is one-dimensional data, `numColumns`
    is set as `1`. Here `numSamples` is the total data and `batchSize` is the amount
    of data in each mini-batch. Since the total data is 150 and it is relatively small,
    `batchSize` is set at the same number. This means that learning is done without
    splitting the data into mini-batches. `splitTrainNum` is the variable that decides
    the allocation between the training data and test data. Here, 80% of all the dataset
    is training data and 20% is the test data. In the previous section, `listenerFreq`
    decides how often we see loss function's value for logging is seen in the process.
    This value is set to 1 here, which means the value is logged after each epoch.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在DL4J中，输入数据最多可以是二维数据，因此你需要指定数据的行数和列数。由于Iris数据是一维的，`numColumns`被设置为`1`。这里的`numSamples`是总数据量，`batchSize`是每个小批量的数据量。由于总数据为150且相对较小，`batchSize`被设置为相同的数量。这意味着学习是在不将数据拆分为小批量的情况下进行的。`splitTrainNum`是决定训练数据和测试数据分配的变量。这里，80%的数据用于训练，20%用于测试。在前一节中，`listenerFreq`决定了我们在过程中多频繁地查看损失函数的值并记录。这里将其设置为1，意味着在每个epoch后记录一次值。
- en: 'Subsequently, we need to fetch the dataset. In DL4J, a class that can easily
    fetch data with respect to a typical dataset, such as Iris, MINST, and LFW, is
    prepared. Therefore, you can just write the following line if you would like to
    fetch the Iris dataset:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们需要获取数据集。在DL4J中，已经准备了一个可以轻松获取典型数据集（如Iris、MINST和LFW）的类。因此，如果你想获取Iris数据集，只需要写以下这行代码：
- en: '[PRE28]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following two lines are to format data:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下两行代码用于格式化数据：
- en: '[PRE29]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This code splits the data into training data and test data and stores them
    respectively:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将数据拆分为训练数据和测试数据，并分别存储：
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As you can see, it makes data handling easier by treating all the data DL4J
    prepares with the `DataSet` class.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，它通过将DL4J准备的所有数据处理成`DataSet`类，简化了数据处理。
- en: 'Now, let''s actually build a model. The basic structure is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实际构建一个模型。基本结构如下：
- en: '[PRE31]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The code begins by defining the model configuration and then builds and initializes
    the actual model with the definition. Let''s take a look at the configuration
    details. At the beginning, the whole network is set up:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先定义了模型配置，然后根据定义构建和初始化实际模型。让我们来看看配置的细节。开始时，整个网络被设置好：
- en: '[PRE32]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The configuration setup is self-explanatory. However, since you haven't learned
    about regularization before now, let's briefly check it out.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 配置设置是自解释的。然而，由于你之前没有学习过正则化，我们简要地了解一下它。
- en: 'Regularization prevents the neural networks model from overfitting and makes
    the model more generalized. To achieve this, the evaluation function ![DBNIrisExample.java](img/B04779_05_07.jpg)
    is rewritten with the penalty term as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化防止神经网络模型过拟合，并使模型更加通用。为此，评估函数![DBNIrisExample.java](img/B04779_05_07.jpg)被带有惩罚项地重写如下：
- en: '![DBNIrisExample.java](img/B04779_05_08.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![DBNIrisExample.java](img/B04779_05_08.jpg)'
- en: Here, ![DBNIrisExample.java](img/B04779_05_09.jpg) denotes the vector norm.
    The regularization is called L1 regularization when ![DBNIrisExample.java](img/B04779_05_10.jpg)
    and L2 regularization when ![DBNIrisExample.java](img/B04779_05_11.jpg). The norm
    is called L1 norm and L2 norm, respectively. That's why we have `.l1()` and `.l2()`
    in the code. ![DBNIrisExample.java](img/B04779_05_12.jpg) is the hyper parameter.
    These regularization terms make the model more sparse. L2 regularization is also
    called weight decay and is used to prevent the vanishing gradient problem.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![DBNIrisExample.java](img/B04779_05_09.jpg)表示向量范数。当![DBNIrisExample.java](img/B04779_05_10.jpg)时，正则化被称为L1正则化；当![DBNIrisExample.java](img/B04779_05_11.jpg)时，则为L2正则化。分别称为L1范数和L2范数。因此，我们在代码中有`.l1()`和`.l2()`。![DBNIrisExample.java](img/B04779_05_12.jpg)是超参数。这些正则化项使得模型更加稀疏。L2正则化也称为权重衰减，旨在防止梯度消失问题。
- en: The `.useDropConnect()` command is used to enable dropout and `.list()` to define
    the number of layers, excluding the input layer.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`.useDropConnect()`命令用于启用dropout，而`.list()`用于定义层数，排除输入层。'
- en: 'When you set up a whole model, then the next step is to configure each layer.
    In this sample code, the model is not defined as deep neural networks. One single
    RBM layer is defined as a hidden layer:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当你设置完整个模型后，下一步是配置每一层。在这段示例代码中，模型没有定义为深度神经网络。定义了一个单一的RBM层作为隐藏层：
- en: '[PRE33]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here, the value of `0` in the first line is the layer's index and `.k()` is
    for contrastive divergence. Since Iris' data is of float values, we can't use
    binary RBM. That's why we have `RBM.VisibleUnit.GAUSSIAN` here, enabling the model
    to handle continuous values. Also, as for the definition of this layer, what should
    be especially mentioned is the role of `Updater.ADAGRAD`. This is used to optimize
    the learning rate. For now, we go on to the model structure, and a detailed explanation
    of the optimizer will be introduced at the end of this chapter.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，第一行中的`0`是层的索引，`.k()`用于对比散度。由于Iris数据是浮动值，我们不能使用二进制RBM。因此，我们这里使用`RBM.VisibleUnit.GAUSSIAN`，使得模型能够处理连续值。另外，关于该层的定义，特别需要提到的是`Updater.ADAGRAD`的作用。它用于优化学习率。现在，我们继续进行模型结构的设置，优化器的详细解释将在本章末尾介绍。
- en: 'The subsequent output layer is very simple and self-explanatory:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 后续的输出层非常简单，易于理解：
- en: '[PRE34]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Thus, the neural networks have been built with three layers : input layer,
    hidden layer, and output layer. The graphical model of this example can be illustrated
    as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络已经通过三层结构构建：输入层、隐藏层和输出层。这个示例的图形模型可以如下表示：
- en: '![DBNIrisExample.java](img/B04779_05_03.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![DBNIrisExample.java](img/B04779_05_03.jpg)'
- en: 'After the model building, we need to train the networks. Here, again, the code
    is super simple:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型后，我们需要训练网络。这里，代码同样非常简单：
- en: '[PRE35]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Because the first line is to log the process, what we need to do to train the
    model is just to write `model.fit()`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第一行是记录过程，我们需要做的就是写下`model.fit()`来训练模型。
- en: 'Testing or evaluating the model is also easy with DL4J. First, the variables
    for evaluation are set up as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DL4J测试或评估模型也很容易。首先，设置评估的变量如下：
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we can get the values of the feature matrix using:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下代码获取特征矩阵的值：
- en: '[PRE37]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'By running the code, we will have the result as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们将得到如下结果：
- en: '[PRE38]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`F1 Score`, also called `F-Score` or `F-measure`, is the harmonic means of
    precision and recall, and is represented as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`F1 Score`，也叫做`F-Score`或`F-measure`，是精准度和召回率的调和均值，其表示方式如下：'
- en: '![DBNIrisExample.java](img/B04779_05_14.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![DBNIrisExample.java](img/B04779_05_14.jpg)'
- en: 'This value is often calculated to measure the model''s performance as well.
    Also, as written in the example, you can see the actual values and predicted values
    by writing the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值通常也用来衡量模型的性能。此外，正如示例中所写的，你可以通过以下代码查看实际值和预测值：
- en: '[PRE39]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'That''s it for the whole training and test process. The neural networks in
    the preceding code are not deep, but you can easily build deep neural networks
    just by changing the configuration as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，整个训练和测试过程就完成了。前面代码中的神经网络并不深，但你只需通过改变配置来轻松构建深度神经网络，如下所示：
- en: '[PRE40]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As you can see, building deep neural networks requires just simple implementations
    with DL4J. Once you set up the model, what you need to do is adjust the parameters.
    For example, increasing the iterations value or changing the seed value would
    return a better result.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用DL4J构建深度神经网络只需要简单的实现。一旦设置好模型，你需要做的就是调整参数。例如，增加迭代次数或更改种子值会返回更好的结果。
- en: CSVExample.java
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CSVExample.java
- en: 'In the previous example, we train the model with the dataset used as a benchmark
    indicator. When you would like to train and test the model with your own prepared
    data, you can easily import it from CSV. Let''s look at `CSVExample.java` in the
    CSV package. The first step is to initialize the CSV reader as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们使用作为基准指标的数据集来训练模型。当你想要使用自己准备的数据进行训练和测试时，可以很方便地从CSV文件中导入数据。让我们来看一下`CSVExample.java`，它在CSV包中。第一步是初始化CSV读取器，代码如下：
- en: '[PRE41]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In DL4J, a class called `CSVRecordReader` is prepared and you can easily import
    data from a CSV file. The value of the first argument in the `CSVRecordReader`
    class represents how many lines should be skipped in the file. This is convenient
    when the file contains header rows. The second argument is the delimiter. To actually
    read a file and import data, the code can be written as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在DL4J中，有一个叫做`CSVRecordReader`的类，你可以轻松地从CSV文件中导入数据。`CSVRecordReader`类中的第一个参数表示应该跳过文件中的多少行。当文件包含头部行时，这非常方便。第二个参数是分隔符。要实际读取文件并导入数据，代码可以如下编写：
- en: '[PRE42]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'With this code, the file in `resources/iris.txt` will be imported to the model.
    The values in the file here are the same as ones as in the Iris dataset. To use
    this initialized data for model training, we define the iterator as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这段代码，`resources/iris.txt` 文件将被导入到模型中。文件中的值与 Iris 数据集中的值相同。为了使用这些初始化的数据进行模型训练，我们定义了如下的迭代器：
- en: '[PRE43]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In the previous example, we used the `IrisDataSetIterator` class, but here the
    `RecordReaderDataSetIterator` class is used because we use our own prepared data.
    The values `4` and `3` are the number of features and labels, respectively.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们使用了 `IrisDataSetIterator` 类，但在这里使用了 `RecordReaderDataSetIterator`
    类，因为我们使用了自己准备的数据。值 `4` 和 `3` 分别是特征和标签的数量。
- en: 'Building and training a model can be done in almost the same way as the process
    explained in the previous example. In this example, we build deep neural networks
    of two hidden layers with the dropout and the rectifier, that is, we have an input
    layer - hidden layer - hidden layer - output layer, as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 构建和训练模型几乎可以与前面的示例中描述的过程相同。在这个例子中，我们构建了一个具有两个隐藏层的深度神经网络，使用了丢弃法和修正线性单元（ReLU），即我们有一个输入层
    - 隐藏层 - 隐藏层 - 输出层，如下所示：
- en: '[PRE44]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can run the model using the following lines of code:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下几行代码来运行模型：
- en: '[PRE45]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The graphical model is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图形化的模型如下：
- en: '![CSVExample.java](img/B04779_05_04.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![CSVExample.java](img/B04779_05_04.jpg)'
- en: 'This time, however, the way to code for training is slightly different from
    the previous example. Before, we split the data into training data and test data
    using the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这次的训练代码方式与前一个示例略有不同。之前，我们通过以下代码将数据分为训练数据和测试数据：
- en: '[PRE46]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This shows that we shuffle the data within the `.splitTestAndTrain()` method.
    In this example, we set up training data with the following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明我们在 `.splitTestAndTrain()` 方法中对数据进行了洗牌。在这个示例中，我们使用以下代码设置训练数据：
- en: '[PRE47]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'As you can see, here the data is first shuffled and then split into training
    data and test data. Be careful that the types of the arguments in `.splitTestAndTrain()`
    are different from each other. This will be beneficial because we don''t have
    to count the exact amount of data or training data. The actual training is done
    using:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这里首先对数据进行了洗牌，然后将其分为训练数据和测试数据。请注意，`.splitTestAndTrain()` 方法中的参数类型是不同的。这是有益的，因为我们不必精确计算数据或训练数据的数量。实际训练是通过以下代码完成的：
- en: '[PRE48]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The way to evaluate the model is just the same as the previous example:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型的方式与之前的示例相同：
- en: '[PRE49]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'With the preceding code, we get the following result:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，我们得到了以下结果：
- en: '[PRE50]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In addition to the dataset of a benchmark indicator, you can now analyze whatever
    data you have.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基准指标的数据集之外，你现在可以分析任何你拥有的数据。
- en: 'To make the model even deeper, you just need to add another layer as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使模型更加深度，只需像下面这样添加一层：
- en: '[PRE51]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: CNNMnistExample.java/LenetMnistExample.java
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNNMnistExample.java/LenetMnistExample.java
- en: CNN is rather complicated compared to other models because of its structure,
    but we don't need to worry about these complications because we can easily implement
    CNN with DL4J. Let's take a look at `CNNMnistExample.java` in the package of convolution.
    In this example, we train the model with the MNIST dataset ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)),
    one of the most famous benchmark indicators. As mentioned in [Chapter 1](ch01.html
    "Chapter 1. Deep Learning Overview"), *Deep Learning Overview*, this dataset contains
    70,000 handwritten numbers data from 0 to 9, with both a height and width of 28
    pixels each.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于其他模型，CNN 因其结构比较复杂，但我们无需担心这些复杂性，因为我们可以通过 DL4J 轻松实现 CNN。让我们来看一下卷积包中的 `CNNMnistExample.java`。在这个例子中，我们使用
    MNIST 数据集（[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)）训练模型，它是最著名的基准指标之一。正如在[第一章](ch01.html
    "第1章 深度学习概述")，*深度学习概述*中提到的，该数据集包含了从 0 到 9 的 70,000 个手写数字数据，每个数字的高和宽均为 28 像素。
- en: 'First, we define the values necessary for the model:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义了模型所需的值：
- en: '[PRE52]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Since images in MNIST are all grayscale data, the number of channels is set
    to `1`. In this example, we use `2,000` data of `70,000` and split it into training
    data and test data. The size of the mini-batch is `500` here, so the training
    data is divided into 4 mini-batches. Furthermore, the data in each mini-batch
    is split into training data and test data, and each piece of test data is stored
    in `ArrayList`:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于MNIST中的图像都是灰度数据，因此通道数被设置为`1`。在本示例中，我们使用`70,000`个数据中的`2,000`个，并将其拆分为训练数据和测试数据。这里的小批量大小为`500`，因此训练数据被分成了4个小批量。进一步地，每个小批量中的数据被拆分为训练数据和测试数据，并将每一份测试数据存储在`ArrayList`中：
- en: '[PRE53]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We didn''t have to set `ArrayList` in the previous examples because we had
    just one batch. For the `MnistDataSetIterator` class, we can set the MNIST data
    just by using:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们不需要设置`ArrayList`，因为只有一个批次。对于`MnistDataSetIterator`类，我们只需使用以下方法就能设置MNIST数据：
- en: '[PRE54]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then, we build the model with a convolutional layer and subsampling layer.
    Here, we have one convolutional layer and one max-pooling layer, directly followed
    by an output layer. The structure of the configurations for CNN is slightly different
    from the other algorithms:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们构建了一个包含卷积层和子采样层的模型。在这里，我们有一个卷积层和一个最大池化层，后面紧跟着一个输出层。CNN的配置结构与其他算法略有不同：
- en: '[PRE55]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The difference is that we can''t build a model directly from the configuration
    because we need to tell the builder to set up a convolutional layer using `ConvolutionLayerSetup()`
    in advance. Each `.layer()` requires just the same method of coding. The convolutional
    layer is defined as:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 不同之处在于，我们不能直接从配置构建模型，因为我们需要提前告诉构建器通过`ConvolutionLayerSetup()`设置卷积层。每个`.layer()`都需要使用相同的编码方法。卷积层定义如下：
- en: '[PRE56]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Here, the value of `10` in `ConvolutionLayer.Builder()` is the size of the kernels,
    and the value of `6` in `.nOut()` is the number of kernels. Also, `.stride()`
    defines the size of the strides of the kernels. The code we implemented from scratch
    in [Chapter 4](ch04.html "Chapter 4. Dropout and Convolutional Neural Networks"),
    *Dropout and Convolutional Neural Networks* has a functionality equivalent only
    to `.stride(1, 1)`. The larger the number is, the less time it takes because it
    decreases the number of calculations necessary for convolutions, but we have to
    be careful at the same time that it might also decrease the model's precision.
    Anyway, we can implement convolutions with more flexibility now.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`ConvolutionLayer.Builder()`中的`10`值表示卷积核的大小，而`.nOut()`中的`6`值表示卷积核的数量。同时，`.stride()`定义了卷积核的步幅大小。我们从头实现的[第4章](ch04.html
    "第4章：Dropout和卷积神经网络")，*Dropout和卷积神经网络*，仅具有等同于`.stride(1, 1)`的功能。数字越大，所需时间越少，因为它减少了卷积所需的计算量，但我们同时也必须小心，这可能会降低模型的精度。无论如何，现在我们可以更灵活地实现卷积。
- en: 'The `subsampling` layer is described as:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`subsampling`层的描述如下：'
- en: '[PRE57]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Here, `{2, 2}` is the size of the pooling windows. You may have noticed that
    we don't have to set the size of the inputs for each layer, including the output
    layer. These values are automatically set once you set up the model.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`{2, 2}`是池化窗口的大小。你可能已经注意到，我们不需要为每一层（包括输出层）设置输入的大小。一旦设置了模型，这些值会自动设置。
- en: 'The output layer can be written just the same as in the other models:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层的写法与其他模型相同：
- en: '[PRE58]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The graphical model of this example is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的图形模型如下所示：
- en: '![CNNMnistExample.java/LenetMnistExample.java](img/B04779_05_05.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![CNNMnistExample.java/LenetMnistExample.java](img/B04779_05_05.jpg)'
- en: 'After the building comes the training. Since we have multiple mini-batches,
    we need to iterate training through all the batches. This can be achieved easily
    using `.hasNext()` on `DataSetIterator` and `mnistIter` in this case. The whole
    training process can be written as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建完模型后，就进入训练环节。由于我们有多个小批量数据，因此我们需要在所有批次中迭代训练。通过在这种情况下使用`DataSetIterator`和`mnistIter`的`.hasNext()`，这一点可以轻松实现。整个训练过程可以写成如下形式：
- en: '[PRE59]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Here, the test data and test labels are stocked for further use.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，测试数据和测试标签已被存储以供后续使用。
- en: 'During the test, again, we need to iterate the evaluation process of the test
    data because we have more than one mini-batch:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试过程中，我们需要再次迭代测试数据的评估过程，因为我们有多个小批量数据：
- en: '[PRE60]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Then, we use the same as in the other examples:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用与其他示例中相同的方法：
- en: '[PRE61]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This will return the result as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回如下结果：
- en: '[PRE62]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The example just given is the model with one convolutional layer and one subsampling
    layer, but you have deep convolutional neural networks with `LenetMnistExample.java`.
    In this example, there are two convolutional layers and subsampling layers, followed
    by fully connected multi-layer perceptrons:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 刚才举的例子是具有一个卷积层和一个子采样层的模型，但你在`LenetMnistExample.java`中有深度卷积神经网络。在这个例子中，有两个卷积层和子采样层，之后是完全连接的多层感知机：
- en: '[PRE63]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: As you can see in the first convolutional layer, dropout can easily be applied
    to CNN with DL4J.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在第一个卷积层中看到的，dropout可以轻松地应用于使用DL4J的CNN。
- en: 'With this model, we get the following result:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个模型，我们得到以下结果：
- en: '[PRE64]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: You can see from the MNIST dataset page ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/))
    that the state-of-the-art result is much better than the one above. Here, again,
    you would realize how important the combination of parameters, activation functions,
    and optimization algorithms are.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从MNIST数据集页面([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/))看到，最先进的结果远优于上述结果。在这里，你会再次意识到，参数、激活函数和优化算法的组合是多么重要。
- en: Learning rate optimization
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率优化
- en: 'We have learned various deep learning algorithms so far; you may have noticed
    that they have one parameter in common: the learning rate. The learning rate is
    defined in the equations to update the model parameters. So, why not think of
    algorithms to optimize the learning rate? Originally, these equations were described
    as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了各种深度学习算法；你可能已经注意到，它们有一个共同的参数：学习率。学习率在方程中定义，用于更新模型参数。那么，为什么不考虑一些算法来优化学习率呢？最初，这些方程描述如下：
- en: '![Learning rate optimization](img/B04779_05_17.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![学习率优化](img/B04779_05_17.jpg)'
- en: 'Here:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '![Learning rate optimization](img/B04779_05_18.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![学习率优化](img/B04779_05_18.jpg)'
- en: Here, ![Learning rate optimization](img/B04779_05_19.jpg) is the number of steps
    and ![Learning rate optimization](img/B04779_05_20.jpg) is the learning rate.
    It is well known that decreasing the value of the learning rate with each iteration
    lets the model have better precision rates, but we should determine the decline
    carefully because a sudden drop in the value would collapse the model. The learning
    rate is one of the model parameters, so why not optimize it? To do so, we need
    to know what the best rate could be.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![学习率优化](img/B04779_05_19.jpg)是步数，![学习率优化](img/B04779_05_20.jpg)是学习率。众所周知，随着每次迭代减少学习率的值，模型的精度会更高，但我们应该谨慎确定衰减，因为学习率的突然下降会导致模型崩溃。学习率是模型的一个参数，那么为什么不优化它呢？为了做到这一点，我们需要知道最佳的学习率应该是多少。
- en: 'The simplest way of setting the rate is using the momentum, represented as
    follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 设置学习率的最简单方法是使用动量，表示如下：
- en: '![Learning rate optimization](img/B04779_05_21.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![学习率优化](img/B04779_05_21.jpg)'
- en: Here, ![Learning rate optimization](img/B04779_05_22.jpg), called the **momentum
    coefficient**. This hyper parameter is often set to be 0.5 or 0.9 first and then
    fine-tuned.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![学习率优化](img/B04779_05_22.jpg)，称为**动量系数**。这个超参数通常初始设置为0.5或0.9，然后进行微调。
- en: 'Momentum is actually a simple but effective way of adjusting the learning rate
    but **ADAGRAD**, proposed by Duchi et al. ([http://www.magicbroom.info/Papers/DuchiHaSi10.pdf](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf)),
    is known to be a better way. The equation is described as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 动量实际上是一种简单但有效的调整学习率的方法，但**ADAGRAD**（由Duchi等人提出，见[http://www.magicbroom.info/Papers/DuchiHaSi10.pdf](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf)）被认为是一种更好的方法。该方程描述如下：
- en: '![Learning rate optimization](img/B04779_05_23.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![学习率优化](img/B04779_05_23.jpg)'
- en: 'Here:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '![Learning rate optimization](img/B04779_05_24.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![学习率优化](img/B04779_05_24.jpg)'
- en: 'Theoretically, this works well, but practically, we often use the following
    equations to prevent divergence:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，这种方法效果很好，但在实践中，我们通常使用以下方程来防止发散：
- en: '![Learning rate optimization](img/B04779_05_25.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![学习率优化](img/B04779_05_25.jpg)'
- en: 'Or we use:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们使用：
- en: '![Learning rate optimization](img/B04779_05_26.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![学习率优化](img/B04779_05_26.jpg)'
- en: ADAGRAD is easier to use than momentum because the value is set automatically
    and we don't have to set additional hyper parameters.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ADAGRAD比动量更容易使用，因为其值会自动设置，我们无需设置额外的超参数。
- en: 'ADADELTA, suggested by Zeiler ([http://arxiv.org/pdf/1212.5701.pdf](http://arxiv.org/pdf/1212.5701.pdf)),
    is known to be an even better optimizer. This is an algorithm-based optimizer
    and cannot be written in a single equation. Here is a description of ADADELTA:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Zeiler 提出的 ADADELTA（[http://arxiv.org/pdf/1212.5701.pdf](http://arxiv.org/pdf/1212.5701.pdf)）被认为是一个更好的优化器。这是一种基于算法的优化器，不能用一个简单的公式表示。以下是
    ADADELTA 的描述：
- en: 'Initialization:'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化：
- en: Initialize accumulation variables:![Learning rate optimization](img/B04779_05_27.jpg)
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始化累积变量：![学习率优化](img/B04779_05_27.jpg)
- en: 'And:'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并且：
- en: '![Learning rate optimization](img/B04779_05_28.jpg)'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![学习率优化](img/B04779_05_28.jpg)'
- en: 'Iteration ![Learning rate optimization](img/B04779_05_29.jpg):'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代 ![学习率优化](img/B04779_05_29.jpg)：
- en: Compute:![Learning rate optimization](img/B04779_05_30.jpg)
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算：![学习率优化](img/B04779_05_30.jpg)
- en: Accumulate gradient:![Learning rate optimization](img/B04779_05_31.jpg)
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累积梯度：![学习率优化](img/B04779_05_31.jpg)
- en: Compute update:![Learning rate optimization](img/B04779_05_32.jpg)
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算更新：![学习率优化](img/B04779_05_32.jpg)
- en: Accumulate updates:![Learning rate optimization](img/B04779_05_33.jpg)
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 累积更新：![学习率优化](img/B04779_05_33.jpg)
- en: Apply update:![Learning rate optimization](img/B04779_05_17.jpg)
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用更新：![学习率优化](img/B04779_05_17.jpg)
- en: Here, ![Learning rate optimization](img/B04779_05_34.jpg) and ![Learning rate
    optimization](img/B04779_05_35.jpg) are the hyper parameters. You may think ADADELTA
    is rather complicated but you don't need to worry about this complexity when implementing
    with DL4J.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![学习率优化](img/B04779_05_34.jpg) 和 ![学习率优化](img/B04779_05_35.jpg) 是超参数。你可能认为
    ADADELTA 比较复杂，但在使用 DL4J 实现时，你不需要担心这种复杂性。
- en: There are still other optimizers supported in DL4J such as **RMSProp**, **RMSProp**
    + momentum, and **Nesterov's Accelerated Gradient** **Descent**. However, we won't
    dig into them because, practically, momentum, ADAGRAD, and ADADELTA are enough
    to optimize the learning rate.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J 还支持其他优化方法，比如 **RMSProp**、**RMSProp** + 动量法，以及 **Nesterov 加速梯度下降** **法**。然而，我们不会深入讨论这些方法，因为实际上，动量法、ADAGRAD
    和 ADADELTA 已经足够优化学习率了。
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how to implement deep learning models with the
    libraries ND4J and DL4J. Both support GPU computing and both give us the ability
    to implement them without any difficulties. ND4J is a library for scientific computing
    and enables vectorization, which makes it easier to implement a calculation among
    arrays because we don't need to write iterations within them. Since machine learning
    and deep learning algorithms have many equations with vector calculations, such
    as inner products and element-wise multiplication, ND4J also helps implement them.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用 ND4J 和 DL4J 库实现深度学习模型。这两个库都支持 GPU 计算，且使我们能够轻松地实现这些模型。ND4J 是一个用于科学计算的库，支持向量化，使得在数组中实现计算更加简便，因为我们不需要在其中编写迭代。由于机器学习和深度学习算法中有许多涉及向量计算的公式，比如内积和元素级的乘法，ND4J
    也有助于实现这些操作。
- en: 'DL4J is a library for deep learning, and by following some examples with the
    library, you saw that we can easily build, train, and evaluate various types of
    deep learning models. Additionally, while building the model, you learned why
    regularization is necessary to get better results. You also got to know some optimizers
    of the learning rate: momentum, ADAGRAD, and ADADELTA. All of these can be implemented
    easily with DL4J.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J 是一个用于深度学习的库，通过跟随库中的一些示例，你看到我们可以轻松地构建、训练和评估各种类型的深度学习模型。此外，在构建模型时，你还学习了为什么正则化对于获得更好的结果是必要的。你还了解了学习率的一些优化方法：动量法、ADAGRAD
    和 ADADELTA。这些都可以通过 DL4J 轻松实现。
- en: You gained knowledge of the core theories and implementations of deep learning
    algorithms and you now know how to implement them with little difficulty. We can
    say that we've completed the theoretical part of this book. Therefore, in the
    next chapter, we'll look at how deep learning algorithms are adapted to practical
    applications first and then look into other possible fields and ideas to apply
    the algorithms.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 你掌握了深度学习算法的核心理论和实现方法，并且现在能够轻松实现它们。我们可以说，我们已经完成了本书的理论部分。因此，在下一章中，我们将首先探讨深度学习算法如何适应实际应用，然后再深入其他可能的领域和应用想法。
