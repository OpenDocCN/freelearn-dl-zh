- en: Double DQN, Dueling Architectures, and Rainbow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Double DQN、对抗架构和彩虹网络
- en: We discussed the **Deep Q-Network** (**DQN**) algorithm in the previous chapter,
    coded it in Python and TensorFlow, and trained it to play Atari Breakout. In DQN,
    the same Q-network was used to select and evaluate an action. This, unfortunately,
    is known to overestimate the Q values, which results in over-optimistic estimates
    for the values. To mitigate this, DeepMind released another paper where it proposed
    the decoupling of the action selection and action evaluation. This is the crux
    of the **Double DQN** (**DDQN**) architectures, which we will investigate in this
    chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了**深度 Q 网络**（**DQN**）算法，使用 Python 和 TensorFlow 编写了它，并训练其玩 Atari Breakout。在
    DQN 中，使用相同的 Q 网络来选择和评估动作。不幸的是，已知这种方法会高估 Q 值，从而导致值的过度乐观估计。为了缓解这个问题，DeepMind 发布了另一篇论文，提出了将动作选择与动作评估解耦。这就是
    **Double DQN**（**DDQN**）架构的核心，我们将在本章中探讨这一点。
- en: Even later, DeepMind released another paper where they proposed the Q-network
    architecture with two output values, one representing the value, *V(s)*, and the
    other the advantage of taking an action at the given state, *A(s,a)*. DeepMind
    then combined these two to compute the *Q(s,a)* action-value, instead of directly
    determining it as done in DQN and DDQN. These Q-network architectures are referred
    to as the **dueling** network architectures, as the neural network now has dual
    output values, *V(s)* and *A(s,a)*, which are later combined to obtain *Q(s,a)*.
    We will also see these dueling networks in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 后来，DeepMind 发布了另一篇论文，提出了一种 Q 网络架构，具有两个输出值，一个表示值 *V(s)*，另一个表示在给定状态下采取某动作的优势 *A(s,a)*。DeepMind
    然后将这两个值结合，计算 *Q(s,a)* 动作值，而不是像 DQN 和 DDQN 中那样直接确定。这些 Q 网络架构被称为 **对抗** 网络架构，因为神经网络现在有两个输出值
    *V(s)* 和 *A(s,a)*，这些值随后结合得到 *Q(s,a)*。我们将在本章中进一步了解这些对抗网络。
- en: Another extension we will also consider in this chapter are **Rainbow networks**,
    which are a blend of several different ideas fused into one algorithm.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们将在本章考虑的扩展是 **彩虹网络**，它是几种不同思想融合成一个算法的产物。
- en: 'The topics that will be covered in this chapter are the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: Learning the theory behind DDQN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习 DDQN 背后的理论
- en: Coding DDQN and training it to play Atari Breakout
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写 DDQN 并训练其玩 Atari Breakout
- en: Evaluating the performance of DDQN on Atari Breakout
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估 DDQN 在 Atari Breakout 上的表现
- en: Understanding dueling network architectures
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解对抗网络架构
- en: Coding dueling network architecture and training it to play Atari Breakout
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写对抗网络架构并训练其玩 Atari Breakout
- en: Evaluating the performance of dueling architectures on Atari Breakout
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估对抗架构在 Atari Breakout 上的表现
- en: Understanding Rainbow networks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解彩虹网络
- en: Running a Rainbow network on Dopamine
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Dopamine 上运行彩虹网络
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To successfully complete this chapter, knowledge of the following will help
    significantly:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功完成本章，以下知识将大有帮助：
- en: Python (2 or 3)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python（版本 2 或 3）
- en: NumPy
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: Matplotlib
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matplotlib
- en: TensorFlow (version 1.4 or higher)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow（版本 1.4 或更高）
- en: Dopamine (we will discuss this in more detail later)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dopamine（我们稍后会详细讨论）
- en: Understanding Double DQN
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Double DQN
- en: 'DDQN is an extension to DQN, where we use the target network in the Bellman
    update. Specifically, in DDQN, we evaluate the target network''s Q function using
    the action that would be greedy maximization of the primary network''s Q function.
    First, we will use the vanilla DQN target for the Bellman equation update step,
    then, we will extend to DDQN for the same Bellman equation update step; this is
    the crux of the DDQN algorithm. We will then code DDQN in TensorFlow to play Atari
    Breakout. Finally, we will compare and contrast the two algorithms: DQN and DDQN.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DDQN 是 DQN 的扩展，在贝尔曼更新中我们使用目标网络。具体来说，在 DDQN 中，我们使用主网络的 Q 函数的贪心最大化动作来评估目标网络的 Q
    函数。首先，我们将使用普通 DQN 目标进行贝尔曼方程更新步骤，然后，我们将扩展到 DDQN 进行相同的贝尔曼方程更新步骤；这就是 DDQN 算法的核心。接下来，我们将用
    TensorFlow 编写 DDQN 来玩 Atari Breakout。最后，我们将比较 DQN 和 DDQN 两种算法。
- en: Updating the Bellman equation
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更新贝尔曼方程
- en: 'In vanilla DQN, the target for the Bellman update is this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通 DQN 中，贝尔曼更新的目标是：
- en: '![](img/8c173c74-a2f2-4790-aec1-5e9aaca83a53.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c173c74-a2f2-4790-aec1-5e9aaca83a53.png)'
- en: '*θ[t]* represents the model parameters of the target network. This is known
    to over-predict *Q,* and so the change made in DDQN is to replace this target
    value, *y[t]*, with this:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*θ[t]*表示目标网络的模型参数。已知它会过度预测*Q*，因此在DDQN中的改动是将这个目标值*y[t]*替换为：'
- en: '![](img/b33dc1c6-5089-4349-b253-78997a943795.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b33dc1c6-5089-4349-b253-78997a943795.png)'
- en: We must distinguish between the Q-network parameters, *θ*, and the target network
    model parameters, *θ[t]*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须区分Q网络参数*θ*和目标网络模型参数*θ[t]*。
- en: Coding DDQN and training to play Atari Breakout
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写DDQN并训练玩Atari Breakout
- en: 'We will now code DDQN in TensorFlow to play Atari Breakout. As before, we have
    three Python files:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将在TensorFlow中编写DDQN来玩Atari Breakout。和之前一样，我们有三个Python文件：
- en: '`funcs.py`'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`funcs.py`'
- en: '`model.py`'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.py`'
- en: '`ddqn.py`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ddqn.py`'
- en: '`funcs.py` and `model.py` are the same as used before for DQN in [Chapter 3](1bf60e32-a842-4f5c-b7be-c1dc50a43c1e.xhtml),
    *Deep Q-Network (DQN)*. The `ddqn.py` file is the only code where we need to make
    changes to implement DDQN. We will use the same `dqn.py` file from the previous
    chapter and make changes to it to code DDQN. So, let''s first copy the `dqn.py` file
    from before and rename it `ddqn.py`.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '`funcs.py`和`model.py`与之前在[第3章](1bf60e32-a842-4f5c-b7be-c1dc50a43c1e.xhtml)的DQN中使用的相同，*深度Q网络（DQN）*。`ddqn.py`文件是唯一需要修改的代码文件，以实现DDQN。我们将使用上一章中的`dqn.py`文件，并对其进行修改以编写DDQN。所以，首先将之前的`dqn.py`文件复制一份并将其重命名为`ddqn.py`。'
- en: We will summarize the changes we will make to `ddqn.py`, which are actually
    quite minimal. We will still not delete the DQN-related lines of code in the file,
    and instead, use `if` loops to choose between the two algorithms. This helps to
    use one code for both algorithms, which is a better way to code.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将总结对`ddqn.py`所做的改动，这些改动其实非常小。我们仍然不会删除文件中与DQN相关的代码行，而是使用`if`语句在两种算法之间进行选择。这样有助于用一份代码同时处理两种算法，这是更好的编码方式。
- en: 'First, we create a variable called `ALGO`, which will store one of two strings:
    `DQN` or `DDQN`, which is where we specify which of the two algorithms to use:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个名为`ALGO`的变量，它将存储两个字符串之一：`DQN`或`DDQN`，这决定了我们要使用哪一个算法：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, in the lines of code where we evaluate the targets for the mini-batch,
    we use `if` loops to decide whether the algorithm to use is DQN or DDQN and accordingly
    compute the targets as follows. Note that, in DQN, the `greedy_q` variable stores
    the Q value corresponding to the greedy action taking, that is, the largest Q
    value in the target network, which is computed using `np.amax()` and then used
    to compute the target variable, `targets_batch`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在我们评估mini-batch的目标值的代码行中，我们使用`if`语句来决定使用DQN还是DDQN算法，并相应地计算目标，如下所示。请注意，在DQN中，`greedy_q`变量存储对应于贪婪动作的Q值，即目标网络中最大的Q值，这是通过`np.amax()`计算的，然后用于计算目标变量`targets_batch`。
- en: 'In DDQN, on the other hand, we compute the action corresponding to the maximum
    Q in the primary Q-network, which we store in `greedy_q` and evaluate using `np.argmax()`.
    Then, we use `greedy_q` (which represents an action now) in the target network
    Q values. Note that, for Terminal time steps, that is, `done = True`, we should
    not consider the next state and likewise, for non-Terminal steps, `done = False`,
    and here we consider the next step. This is easily accomplished using `np.invert().astype(np.float32)`
    on `done_batch`. The following lines of code show DDQN:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 而在DDQN中，我们计算与主Q网络中最大Q值对应的动作，这个值存储在`greedy_q`中，并通过`np.argmax()`进行评估。然后，我们在目标网络Q值中使用`greedy_q`（它现在表示一个动作）。注意，对于终止时间步骤，即`done
    = True`，我们不应考虑下一个状态；而对于非终止步骤，`done = False`，我们考虑下一个步骤。可以通过对`done_batch`使用`np.invert().astype(np.float32)`轻松实现。以下代码行展示了DDQN：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: That's it for `ddqn.py`. We will now evaluate it on Atari Breakout.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是`ddqn.py`的内容。我们现在将对其进行Atari Breakout的评估。
- en: Evaluating the performance of DDQN on Atari Breakout
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估DDQN在Atari Breakout上的表现
- en: We will now evaluate the performance of DDQN on Atari Breakout. Here, we will
    plot the performance of our DDQN algorithm on Atari Breakout using the `performance.txt`
    file that we wrote in the code. We will use `matplotlib` to plot two graphs as
    explained in the following.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将评估DDQN在Atari Breakout上的表现。在这里，我们将使用在代码中编写的`performance.txt`文件，绘制DDQN算法在Atari
    Breakout上的表现。我们将使用`matplotlib`绘制两张图，如下所述。
- en: 'In the following screenshot, we present the number of time steps per episode
    on Atari Breakout using DDQN and its exponentially weighted moving average. As
    evident, the peak number of time steps is ~2,000 for many episodes toward the
    end of the training, with one episode where it exceeded even 3,000 time steps!
    The moving average is approximately 1,500 time steps toward the end of the training:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，我们展示了在Atari Breakout上使用DDQN进行训练时，每集的时间步数及其指数加权移动平均值。如图所示，许多集的时间步数在训练结束时达到了大约2,000次，并且有一集的时间步数甚至超过了3,000次！移动平均值在训练结束时大约是1,500次：
- en: '![](img/29cdcfd8-86bb-4826-9dfe-3da46f587773.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29cdcfd8-86bb-4826-9dfe-3da46f587773.png)'
- en: 'Figure 1: Number of time steps per episode for Atari Breakout using DDQN'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：使用DDQN在Atari Breakout中，每集的时间步数
- en: 'In the following screenshot, we show the total rewards received per episode
    versus the time number of the global time step. The peak episode reward is over
    350, with the moving average near 150\. Interestingly, the moving average (in
    orange) is still increasing toward the end, which means you can run the training
    even longer to see further gains. This is left to the interested reader:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们展示了每集获得的总奖励与全球时间步数的关系。峰值奖励超过350，移动平均值接近150。有趣的是，移动平均值（橙色曲线）在训练结束时仍在上升，这意味着你可以继续训练以获得更大的收益。对此有兴趣的读者可以进一步探究：
- en: '![](img/ad3e8213-a62c-4ff0-a258-57ad53d8cbb3.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad3e8213-a62c-4ff0-a258-57ad53d8cbb3.png)'
- en: 'Figure 2: Total episode reward versus time step for Atari Breakout using DDQN'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：使用DDQN在Atari Breakout中，累计奖励与时间步长的关系
- en: Note that, due to RAM constraints (16 GB), we used a replay buffer size of 300,000
    only. If the user has access to more RAM power, a bigger replay buffer size can
    be used—for example, 500,000 to 1,000,000, which can result in even better scores.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于RAM限制（16 GB），我们仅使用了300,000的回放缓冲区大小。如果用户拥有更多的RAM资源，可以使用更大的回放缓冲区大小——例如500,000到1,000,000，这可以获得更好的成绩。
- en: As we can see, the DDQN agent is learning to play Atari Breakout well. The moving
    average of the episode rewards is constantly going up, which means you can train
    longer to obtain even higher rewards. This upward trend in the episode reward
    demonstrates the efficacy of the DDQN algorithm for such problems.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，DDQN智能体正在学习如何玩Atari Breakout。每集奖励的移动平均值在不断上升，这意味着你可以训练更长时间，以获得更高的奖励。这种奖励的上升趋势证明了DDQN算法在此类问题中的有效性。
- en: Understanding dueling network architectures
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解对抗网络架构
- en: We will now understand the use of dueling network architectures. In DQN and
    DDQN, and other DQN variants in the literature, the focus was primarily on algorithms,
    that is, how to efficiently and stably update the value function neural networks.
    While this is crucial for developing robust RL algorithms, a parallel but complementary
    direction to advance the field is to also innovate and develop novel neural network
    architectures that are well suited for model-free RL. This is precisely the concept
    behind dueling network architectures, another contribution from DeepMind.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来理解对抗网络架构的应用。在DQN、DDQN及其他文献中的DQN变体中，焦点主要集中在算法上，即如何高效稳定地更新价值函数神经网络。虽然这对于开发强健的强化学习算法至关重要，但推动该领域发展的另一个平行但互补的方向是创新并开发适合无模型强化学习的全新神经网络架构。这正是对抗网络架构的概念，另一个来自DeepMind的贡献。
- en: 'The steps involved in dueling architectures are as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗架构涉及的步骤如下：
- en: Dueling network architecture figure; compare with standard DQN
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对抗网络架构图；与标准DQN进行比较
- en: Computing *Q(s,a)*
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算 *Q(s,a)*
- en: Subtracting the average of the advantage from the `advantage` function
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`优势`函数中减去优势的平均值
- en: 'As we saw in the previous chapter, the output of the Q-network in DQN is *Q(s,a)*,
    the action-value function. In dueling networks, the Q-network instead has two
    output values: the `state value` function, *V(s)*, and the `advantage` function, *A(s,a)*.
    You can then combine them to compute the `state-action value` function, *Q(s,a)*.
    This has the advantage that the network need not learn the `value` function for
    every action at every state. This is particularly useful in states where the actions
    do not affect the environment.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章看到的，在DQN中，Q网络的输出是*Q(s,a)*，即动作价值函数。在对抗网络中，Q网络的输出改为两个值：`状态值`函数 *V(s)*
    和`优势`函数 *A(s,a)*。然后，你可以将它们结合起来计算`状态-动作价值`函数 *Q(s,a)*。这样做的好处在于，网络不需要为每个状态下的每个动作学习`价值`函数。对于那些动作不影响环境的状态，这尤其有用。
- en: For instance, if the agent is a car driving on a straight road with no traffic,
    no action is necessary and so *V(s)* alone will suffice in these states. On the
    other hand, if the road suddenly curves or other cars come into the vicinity of
    the agent, then the agent needs to take actions and so, in these states, the `advantage`
    function comes into play to find the incremental returns a given action can provide
    over the `state value` function. This is the intuition behind separating the estimation
    of *V(s)* and *A(s,a)* in the same network by using two different branches, and
    later combining them.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果智能体是一辆在没有交通的直路上行驶的汽车，则无需采取任何行动，因此在这些状态下，仅*V(s)*就足够了。另一方面，如果道路突然弯曲或有其他车辆接近智能体，则智能体需要采取行动，因此在这些状态下，`advantage`
    函数开始发挥作用，用于找出给定行动相对于`state value`函数能提供的增量回报。这就是通过使用两个不同的分支在同一网络中分离估计 *V(s)* 和
    *A(s,a)* 的直觉，随后将它们合并。
- en: 'Refer to the following diagram for a schematic showing a comparison of the
    standard DQN network and the dueling network architectures:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下图表，展示标准 DQN 网络和对抗网络架构的对比：
- en: '![](img/e364fe22-6475-4485-981c-8b25a530f2ec.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e364fe22-6475-4485-981c-8b25a530f2ec.png)'
- en: 'Figure 3: Schematic of the standard DQN network (top) and the dueling network
    architecture (bottom)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：标准 DQN 网络（上）和对抗网络架构（下）的示意图
- en: 'You can compute the `action-value` function *Q(s,a)* as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按如下方式计算 `action-value` 函数 *Q(s,a)*：
- en: '![](img/8c8ad6f8-0b44-43a8-a37f-8a2edbd99f2d.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c8ad6f8-0b44-43a8-a37f-8a2edbd99f2d.png)'
- en: 'However, this is not unique in that you can have an amount, *δ*, over-predicted
    in *V(s)* and the same amount, *δ*, under-predicted in *A(s,a)*. This makes the
    neural network predictions unidentifiable. To circumvent this problem, the authors
    of the dueling network paper recommend the following way to combine *V(s)* and
    *A(s,a)*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是唯一的，因为你可以在 *V(s)* 中过预测一个数量 *δ*，而在 *A(s,a)* 中预测同样数量的 *δ*。这会导致神经网络的预测不可辨识。为了绕过这个问题，对抗网络论文的作者建议以下方式来组合
    *V(s)* 和 *A(s,a)*：
- en: '![](img/773c5473-e05f-4c28-ab5f-412c4e10a57a.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/773c5473-e05f-4c28-ab5f-412c4e10a57a.png)'
- en: '*|A|* represents the number of actions and *θ* is the neural network parameters
    that are shared between the *V(s)* and *A(s,a)* streams; in addition, *α* and *β*
    are used to denote the neural network parameters in the two different streams,
    that is, in the *A(s,a)* and *V(s)* streams, respectively. Essentially, in the
    preceding equation, we subtract the average `advantage` function from the `advantage`
    function and sum it to the `state value` function to obtain *Q(s,a)*.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*|A|* 代表动作数量，*θ* 是在 *V(s)* 和 *A(s,a)* 流中共享的神经网络参数；此外，*α* 和 *β* 用于表示两个不同流中的神经网络参数，即分别在
    *A(s,a)* 和 *V(s)* 流中。实际上，在上述方程中，我们从 `advantage` 函数中减去其平均值，并将其加到 `state value`
    函数中，从而得到 *Q(s,a)*。'
- en: 'This is the link to the dueling network architectures paper: [https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对抗网络架构论文的链接：[https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581)。
- en: Coding dueling network architecture and training it to play Atari Breakout
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写对抗网络架构并训练它来玩 Atari Breakout
- en: 'We will now code the dueling network architecture and train it to learn to
    play Atari Breakout. For the dueling network architecture, we require the following
    codes:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将编写对抗网络架构，并训练它学习玩 Atari Breakout。对于对抗网络架构，我们需要以下代码：
- en: '`model.py`'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model.py`'
- en: '`funcs.py`'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`funcs.py`'
- en: '`dueling.py`'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dueling.py`'
- en: We will use `funcs.py`, which was used earlier for DDQN, so we reuse it. The
    `dueling.py` code is also identical to `ddqn.py` (which was used earlier, so we
    just rename and reuse it). The only changes to be made are in `model.py`. We copy
    the same `model.py` file from DDQN and summarize here the changes to be made for
    the dueling network architecture. The steps involved are the following.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前用于 DDQN 的 `funcs.py`，因此我们重用它。`dueling.py` 代码与 `ddqn.py` 完全相同（它之前已使用过，所以我们只是重命名并重用）。需要进行的唯一更改是在
    `model.py` 中。我们从 DDQN 中复制相同的 `model.py` 文件，并在此总结对对抗网络架构所做的更改。涉及的步骤如下：
- en: 'We first create a Boolean variable called `DUELING` in `model.py` and assign
    it to `True` if using dueling network architecture; otherwise, it is assigned
    to `False`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在 `model.py` 中创建一个名为 `DUELING` 的布尔变量，并在使用对抗网络架构时将其赋值为 `True`，否则赋值为 `False`：
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will write the code with an `if` loop so that the `DUELING` variable, if
    `False`, will use the earlier code we used in DDQN, and if `True`, we will use
    the dueling network. We will use the `flattened` object that is the flattened
    version of the output of the convolutional layers to create two sub-neural network
    streams. We send `flattened` separately into two different fully connected layers
    with `512` neurons, using the `relu` activation function and the `winit` weights
    initializer defined earlier; the output values of these fully connected layers
    are called `valuestream` and `advantagestream`, respectively:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编写带有`if`循环的代码，以便在`DUELING`变量为`False`时使用我们在DDQN中使用的早期代码，而当其为`True`时使用对战网络。我们将使用`flattened`对象，它是卷积层输出的扁平化版本，用来创建两个子神经网络流。我们将`flattened`分别传入两个不同的完全连接层，每个层有`512`个神经元，使用`relu`激活函数和先前定义的`winit`权重初始化器；这些完全连接层的输出值分别称为`valuestream`和`advantagestream`：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Combining V and A to obtain Q
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将V和A结合得到Q
- en: 'The `advantagestream` object is passed into a fully connected layer with a
    number of neurons equal to the number of actions, that is, `len(self.VALID_ACTIONS)`.
    Likewise, the `valuestream` object is passed into a fully connected layer with
    one neuron. Note that we do not use an activation function for computing the `advantage`
    and `state value` functions, as they can be positive or negative (`relu` will
    set all negative values to zero!). Finally, we combine the advantage and value
    streams using `tf.subtract()` to subtract the advantage and the mean of the `advantage`
    function. The mean is computed using `tf.reduce_mean()` on the `advantage` function:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`advantagestream`对象被传入一个完全连接的层，该层的神经元数量等于动作数量，即`len(self.VALID_ACTIONS)`。同样，`valuestream`对象被传入一个只有一个神经元的完全连接层。请注意，我们在计算`advantage`和`state
    value`函数时不使用激活函数，因为它们可以是正数也可以是负数（`relu`会将所有负值设为零！）。最后，我们使用`tf.subtract()`将`advantage`和`advantage`函数的均值相减，从而将优势流和价值流结合起来。均值通过对`advantage`函数使用`tf.reduce_mean()`来计算：'
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That's it for coding dueling network architectures. We will train an agent with
    the dueling network architecture and evaluate its performance on Atari Breakout.
    Note that we can use the dueling architecture in conjunction with either DQN or
    DDQN. That is to say that we only changed the neural network architecture, not
    the actual Bellman update, and so the dueling architecture works with both DQN
    and DDQN.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是编码对战网络架构的全部内容。我们将使用对战网络架构训练一个智能体，并评估其在Atari Breakout上的表现。请注意，我们可以将对战架构与DQN或DDQN结合使用。也就是说，我们仅改变了神经网络架构，而没有更改实际的Bellman更新，因此对战架构适用于DQN和DDQN。
- en: Evaluating the performance of dueling architectures on Atari Breakout
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估对战架构在Atari Breakout上的表现
- en: We will now evaluate the performance of dueling architectures on Atari Breakout.
    Here, we will plot the performance of our dueling network architecture with DDQN
    on Atari Breakout using the `performance.txt` file that we wrote during the training
    of the agent. We will use `matplotlib` to plot two graphs as explained in the
    following.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将评估对战架构在Atari Breakout上的表现。在这里，我们将使用在训练智能体时写入的`performance.txt`文件，绘制我们在Atari
    Breakout上使用DDQN的对战网络架构的表现。我们将使用`matplotlib`绘制以下两张图：
- en: 'In the following screenshot, we present the number of time steps per episode
    on Atari Breakout using DDQN (in blue) and its exponentially weighted moving average
    (in orange). As evident, the peak number of time steps is ~2,000 for many episodes
    toward the end of the training, with a few episodes even exceeding 4,000 time
    steps! The moving average is approximately 1,500 time steps toward the end of
    the training:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们展示了在使用DDQN的Atari Breakout中每一回合的时间步数（蓝色）以及其指数加权移动平均（橙色）。如图所示，训练结束时许多回合的时间步数峰值约为2,000，甚至有少数回合超过了4,000时间步！而在训练结束时，移动平均约为1,500时间步：
- en: '![](img/21bc1175-c1e7-4a36-a128-aed5bff0c893.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21bc1175-c1e7-4a36-a128-aed5bff0c893.png)'
- en: 'Figure 4: Number of time steps per episode on Atari Breakout using dueling
    network architecture and DDQN'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用对战网络架构和DDQN在Atari Breakout中每回合的时间步数
- en: 'In the following screenshot, we show the total rewards received per episode
    versus the time number of the global time step. The peak episode reward is over
    400, with the moving average near 220\. We also note that the moving average (in
    orange) is still increasing toward the end, which means you can run the training
    even longer to obtain further gains. Overall, the average rewards are higher with
    the dueling network architecture vis-a-vis the non-dueling counterparts, and so
    it is strongly recommended to use these dueling architectures:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图中，我们展示了每回合收到的总奖励与全局时间步长的时间关系。峰值回合奖励超过 400，移动平均值接近 220。我们还注意到，移动平均值（橙色线）在接近尾部时仍然在增加，这意味着你可以继续运行训练，以获得进一步的提升。总体而言，使用对抗网络架构的平均奖励高于非对抗架构，因此强烈推荐使用这些对抗网络架构：
- en: '![](img/447de67c-abb0-4983-8c94-fac145c6a658.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/447de67c-abb0-4983-8c94-fac145c6a658.png)'
- en: 'Figure 5: Total episode reward received versus global time step number for
    Atari Breakout using dueling network architecture and DDQN'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：使用对抗网络架构和 DDQN 进行 Atari Breakout 时，总回合奖励与全局时间步长的关系
- en: Note that, due to RAM constraints (16 GB), we used a replay buffer size of only
    300,000\. If the user has access to more RAM power, a bigger replay buffer size
    can be used—for example, 500,000 to 1,000,000, which can result in even better
    scores.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于 RAM 限制（16 GB），我们只使用了 300,000 的回放缓冲区大小。如果用户拥有更多的 RAM，可以使用更大的回放缓冲区大小——例如
    500,000 到 1,000,000，这样可能会得到更好的得分。
- en: Understanding Rainbow networks
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Rainbow 网络
- en: 'We will now move on to **Rainbow networks**, which is a confluence of several
    different DQN improvements. Since the original DQN paper, several different improvements
    were proposed with notable success. This motivated DeepMind to combine several
    different improvements into an integrated agent, which they refer to as the **Rainbow
    DQN**. Specifically, six different DQN improvements are combined into one integrated
    Rainbow DQN agent. These six improvements are summarized as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论**Rainbow 网络**，它是多个不同 DQN 改进的融合。从原始 DQN 论文开始，提出了几个不同的改进并取得了显著成功。这促使 DeepMind
    将多个不同的改进结合到一个集成的智能体中，并称之为**Rainbow DQN**。具体来说，六个不同的 DQN 改进被集成到一个 Rainbow DQN 智能体中。这六个改进总结如下：
- en: DDQN
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDQN
- en: Dueling network architecture
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗网络架构
- en: Prioritized experience replay
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优先经验回放
- en: Multi-step learning
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多步学习
- en: Distributional RL
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式强化学习
- en: Noisy nets
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声网络
- en: DQN improvements
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DQN 改进
- en: We have already seen DDQN and dueling network architectures and have coded them
    in TensorFlow. The rest of the improvements are described in the following sections.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过了 DDQN 和对抗网络架构，并在 TensorFlow 中实现了它们。其余的改进将在接下来的章节中描述。
- en: Prioritized experience replay
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优先经验回放
- en: 'We used a replay buffer where all of the samples have an equal probability
    of being sampled. This, however, is not very efficient, as some samples are more
    important than others. This is the motivation behind prioritized experience replay,
    where samples that have a higher **Temporal Difference** (**TD**) error are sampled
    with a higher probability than others. The first time a sample is added to the
    replay buffer, it is set a maximum priority value so as to ensure that all samples
    in the buffer are sampled at least once. Thereafter, the TD error is used to determine
    the probability of the experience to be sampled, which we compute as this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个回放缓冲区，其中所有样本都有相等的被采样概率。然而，这种方法并不是很高效，因为某些样本比其他样本更为重要。这就是优先经验回放的动机，优先经验回放中，具有更高**时序差分**（**TD**）误差的样本将以更高的概率被采样。第一次将样本添加到回放缓冲区时，给它设置一个最大优先级值，以确保缓冲区中的所有样本至少会被采样一次。之后，TD
    误差用于确定经验被采样的概率，我们的计算方式如下：
- en: '![](img/180e299e-e33a-4a27-b859-2b8afcd62782.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/180e299e-e33a-4a27-b859-2b8afcd62782.png)'
- en: Whereas the previous *r* is the reward, *θ* is the primary Q-network model parameters,
    and *θ^t* is the target network parameters. *ω* is a positive hyper-parameter
    that determines the shape of the distribution.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，前一个 *r* 是奖励，*θ* 是主 Q 网络模型参数，*θ^t* 是目标网络参数。*ω* 是一个正的超参数，用于确定分布的形状。
- en: Multi-step learning
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多步学习
- en: 'In Q-learning, we accumulate a single reward and use the greedy action at the
    next step. Alternatively, you can also use multi-step targets and compute an *n*-step
    return from a single state:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Q 学习中，我们会累积单一奖励，并在下一个步骤使用贪婪动作。或者，你也可以使用多步目标，并从一个状态计算 *n* 步回报：
- en: '![](img/91f444c4-3846-43ab-b7ac-bf3bf367131e.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91f444c4-3846-43ab-b7ac-bf3bf367131e.png)'
- en: Then, the *n*-step return, *r[t]^((n))* , is used in the Bellman update and
    is known to lead to faster learning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，*n*步回报，*r[t]^((n))*，在贝尔曼更新中使用，并且已知能够加速学习。
- en: Distributional RL
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式强化学习
- en: In **distributional RL**, we learn to approximate the distribution of returns
    instead of the expected return. This is mathematically complicated, is beyond
    the scope of this book, and is not discussed further.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在**分布式强化学习**中，我们学习逼近回报的分布，而不是期望回报。这在数学上是复杂的，超出了本书的范围，因此不再进一步讨论。
- en: Noisy nets
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嘈杂网络
- en: 'In some games (such as Montezuma''s revenge), ε-greedy does not work well,
    as many actions need to be executed before the first reward is received. Under
    this setting, the use of a noisy linear layer that combined a deterministic and
    a noisy stream is recommended, shown as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些游戏中（如《蒙特祖马的复仇》），ε-贪婪策略表现不佳，因为需要执行许多动作才能获得第一个奖励。在这种设置下，推荐使用结合了确定性流和嘈杂流的嘈杂线性层，如下所示：
- en: '![](img/041b0d30-df4e-44e6-9b32-a81183b3f968.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/041b0d30-df4e-44e6-9b32-a81183b3f968.png)'
- en: Here, *x* is the input, *y* is the output, and *b* and *W* are the biases and
    weights in the deterministic stream; *b^(noisy)* and *W^(noisy)* are the biases
    and weights, respectively, in the noisy stream; and *ε^b* and *ε^W* are random
    variables and are applied as element-wise product to the biases and weights, respectively,
    in the noisy stream. The network may choose to ignore the noisy stream in some
    regions of the state space and may use them otherwise, as required. This allows
    for a state-determined exploration strategy.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*x*是输入，*y*是输出，*b*和*W*分别是确定性流中的偏置和权重；*b^(noisy)*和*W^(noisy)*分别是嘈杂流中的偏置和权重；*ε^b*和*ε^W*是随机变量，并作为元素级的乘积应用于嘈杂流中的偏置和权重。网络可以选择在状态空间的某些区域忽略嘈杂流，也可以根据需要使用它们。这允许根据状态确定的探索策略。
- en: We will not be coding the full Rainbow DQN, as it is exhaustive. Instead, we
    will use an open source framework called Dopamine to train a Rainbow DQN agent,
    which will be discussed in the next section.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会编写完整的Rainbow DQN代码，因为它非常复杂。相反，我们将使用一个开源框架叫做Dopamine来训练Rainbow DQN智能体，具体内容将在下一节讨论。
- en: Running a Rainbow network on Dopamine
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在多巴胺上运行Rainbow网络
- en: In 2018, some engineers at Google released an open source, lightweight, TensorFlow-based
    framework for training RL agents, called **Dopamine**. Dopamine, as you may already
    know, is the name of an organic chemical that plays an important role in the brain.
    We will use Dopamine to run Rainbow.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，谷歌的一些工程师发布了一个开源、轻量级、基于TensorFlow的强化学习训练框架，名为**Dopamine**。正如你可能已经知道的，多巴胺是大脑中一种重要的有机化学物质。我们将使用Dopamine来运行Rainbow。
- en: 'The Dopamine framework is based on four design principles:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Dopamine框架基于四个设计原则：
- en: Easy experimentation
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的实验
- en: Flexible development
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵活的开发
- en: Compact and reliable
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧凑且可靠
- en: Reproducible
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可重现性
- en: 'To download Dopamine from GitHub, type the following command in a Terminal:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要从GitHub下载Dopamine，请在终端中输入以下命令：
- en: '[PRE5]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can test whether Dopamine was successfully installed by typing the following
    commands into a Terminal:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在终端中输入以下命令来测试Dopamine是否成功安装：
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of this will look something like the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其输出将类似于以下内容：
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You should see `OK` at the end to confirm that everything went well with the
    download.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在最后看到`OK`，以确认下载一切正常。
- en: Rainbow using Dopamine
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多巴胺的Rainbow
- en: 'To run Rainbow DQN, type the following command into a Terminal:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行Rainbow DQN，请在终端中输入以下命令：
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'That''s it. Dopamine will start training Rainbow DQN and print out training
    statistics on the screen, as well as save checkpoint files. The configuration
    file is stored in the following path:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。多巴胺将开始训练Rainbow DQN，并在屏幕上打印训练统计信息，同时保存检查点文件。配置文件存储在以下路径：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It looks like the following code. `game_name` is set to `Pong` as default; feel
    free to try other Atari games. The number of agent steps for training is set in
    `training_steps`, and for evaluation in `evaluation_steps`. In addition, it introduces
    stochasticity to the training by using the concept of sticky actions, where the
    most recent action is repeated multiple times with a probability of 0.25\. That
    is, if a uniform random number (computed using NumPy's `np.random.rand()`) is
    < 0.25, the most recent action is repeated; otherwise, a new action is taken from
    the policy.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 代码如下所示。`game_name` 默认设置为 `Pong`；也可以尝试其他Atari游戏。训练的代理步骤数设置在`training_steps`中，评估步骤数设置在`evaluation_steps`中。此外，它通过使用粘性动作的概念引入了训练中的随机性，其中最近的动作以0.25的概率被重复多次。也就是说，如果均匀随机数（使用NumPy的`np.random.rand()`计算）<
    0.25，则重复最近的动作；否则，从策略中采取新的动作。
- en: 'The sticky action is a new method of introducing stochasticity to the learning:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 粘性动作是一种引入学习随机性的新方法：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Feel free to experiment with the hyperparameters and see how the learning is
    affected. This is a very nice way to ascertain the sensitivity of the different
    hyperparameters on the learning of the RL agent.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 随意尝试不同的超参数，看看它们如何影响学习。这是检验不同超参数对强化学习代理学习敏感度的一个非常好的方法。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we were introduced to DDQN, dueling network architectures,
    and the Rainbow DQN. We extended our previous DQN code to DDQN and dueling architectures
    and tried it out on Atari Breakout. We can clearly see that the average episode
    rewards are higher with these improvements, and so these improvements are a natural
    choice to use. Next, we also saw Google's Dopamine and used it to train a Rainbow
    DQN agent. Dopamine has several other RL algorithms, and the user is encouraged
    to dig deeper and try out these other RL algorithms as well.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了DDQN、对战网络架构和彩虹DQN。我们将之前的DQN代码扩展到DDQN和对战架构，并在Atari Breakout上进行了测试。我们可以清楚地看到，随着这些改进，平均每集奖励更高，因此这些改进自然成为使用的选择。接下来，我们还看到了谷歌的Dopamine，并使用它来训练一个彩虹DQN代理。Dopamine有几个其他强化学习算法，鼓励用户深入挖掘并尝试这些其他的强化学习算法。
- en: This chapter was a good deep dive into the DQN variants, and we really covered
    a lot of mileage as far as coding of RL algorithms is involved. In the next chapter,
    we will learn about our next RL algorithm called **Deep Deterministic Policy Gradient**
    (**DDPG**), which is our first Actor-Critic RL algorithm and our first continuous
    action space RL algorithm.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了DQN的变体，我们在强化学习算法的编码方面确实取得了很多进展。在下一章中，我们将学习下一种强化学习算法，叫做**深度确定性策略梯度**（**DDPG**），这是我们第一个演员-评论员（Actor-Critic）强化学习算法，也是我们第一个连续动作空间的强化学习算法。
- en: Questions
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Why does DDQN perform better than DQN?
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么DDQN比DQN表现更好？
- en: How does the dueling network architecture help in the training?
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对战斗网络架构如何帮助训练？
- en: Why does prioritized experience replay speed up the training?
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么优先经验回放能加速训练？
- en: How do sticky actions help in the training?
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 粘性动作如何帮助训练？
- en: Further reading
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: The DDQN paper, *Deep Reinforcement Learning with Double Q-learning*, by Hado
    van Hasselt, Arthur Guez*,* and David Silver can be obtained from the following
    link, and the interested reader is recommended to read it: [https://arxiv.org/abs/1509.06461](https://arxiv.org/abs/1509.06461)
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDQN论文，*使用双重Q学习的深度强化学习*，由Hado van Hasselt、Arthur Guez和David Silver撰写，可以通过以下链接获取，建议有兴趣的读者阅读：[https://arxiv.org/abs/1509.06461](https://arxiv.org/abs/1509.06461)
- en: '*Rainbow: Combining Improvements in Deep Reinforcement Learning*, Matteo Hessel,
    Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
    Horgan, Bilal Piot, Mohammad Azar, and David Silver, arXiv:1710.02298 (the Rainbow
    DQN): [https://arxiv.org/abs/1710.02298](https://arxiv.org/abs/1710.02298)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*彩虹：结合深度强化学习中的改进*，Matteo Hessel，Joseph Modayil，Hado van Hasselt，Tom Schaul，Georg
    Ostrovski，Will Dabney，Dan Horgan，Bilal Piot，Mohammad Azar，David Silver，arXiv:1710.02298（彩虹DQN）：[https://arxiv.org/abs/1710.02298](https://arxiv.org/abs/1710.02298)'
- en: '*Prioritized Experience Replay*, Tom Schaul, John Quan, Ioannis Antonoglou,
    David Silver, arXiv:1511.05952: [https://arxiv.org/abs/1511.05952](https://arxiv.org/abs/1511.05952)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优先经验回放*，Tom Schaul，John Quan，Ioannis Antonoglou，David Silver，arXiv:1511.05952：[https://arxiv.org/abs/1511.05952](https://arxiv.org/abs/1511.05952)'
- en: '*Multi-Step Reinforcement Learning: A Unifying Algorithm*, Kristopher de Asis,
    J Fernando Hernandez-Garcia, G Zacharias Holland, Richard S Sutton: [https://arxiv.org/pdf/1703.01327.pdf](https://arxiv.org/pdf/1703.01327.pdf)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多步强化学习：一个统一的算法*，作者：Kristopher de Asis、J Fernando Hernandez-Garcia、G Zacharias
    Holland、Richard S Sutton: [https://arxiv.org/pdf/1703.01327.pdf](https://arxiv.org/pdf/1703.01327.pdf)'
- en: '*Noisy Networks for Exploration,* by Meire Fortunato, Mohammad Gheshlaghi Azar,
    Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis
    Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg, arXiv:1706.10295:
    [https://arxiv.org/abs/1706.10295](https://arxiv.org/abs/1706.10295)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*噪声网络用于探索*，作者：Meire Fortunato、Mohammad Gheshlaghi Azar、Bilal Piot、Jacob Menick、Ian
    Osband、Alex Graves、Vlad Mnih、Remi Munos、Demis Hassabis、Olivier Pietquin、Charles
    Blundell 和 Shane Legg，arXiv:1706.10295: [https://arxiv.org/abs/1706.10295](https://arxiv.org/abs/1706.10295)'
