- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Diarizing Speech with WhisperX and NVIDIA’s NeMo
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用WhisperX和NVIDIA的NeMo进行语音分段
- en: Welcome to [*Chapter 8*](B21020_08.xhtml#_idTextAnchor186), where we will explore
    the world of **speech diarization**. While Whisper has proven to be a powerful
    tool for transcribing speech, there’s another crucial aspect of speech analysis
    that can significantly enhance its utility – speaker diarization. By augmenting
    Whisper with the ability to identify and attribute speech segments to different
    speakers, we open a new realm of possibilities for analyzing multispeaker conversations.
    This chapter will explore how Whisper can be integrated with cutting-edge diarization
    techniques to unlock these capabilities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到[*第8章*](B21020_08.xhtml#_idTextAnchor186)，在这里我们将探索**语音分段**的世界。尽管Whisper已被证明是一个强大的语音转录工具，但语音分析中还有另一个至关重要的方面——说话人分段，这可以显著增强其效用。通过增强Whisper的能力，识别并将语音片段归属于不同的说话人，我们为分析多说话人对话开辟了新的可能性。本章将探讨如何将Whisper与最先进的分段技术结合，以解锁这些能力。
- en: We will start by exploring the evolution of speaker diarization systems, from
    the limitations of early approaches to the transformative impact of transformer
    models. Through practical, hands-on examples, we’ll preprocess audio data, transcribe
    speech with Whisper, and fine-tune the alignment between transcriptions and the
    original audio.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从探索说话人分段系统的演变开始，了解早期方法的局限性，以及变压器模型带来的变革性影响。通过实际的操作示例，我们将预处理音频数据，使用Whisper转录语音，并微调转录与原始音频之间的对齐。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Augmenting Whisper with speaker diarization
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强Whisper的说话人分段能力
- en: Performing hands-on speech diarization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行实际的语音分段
- en: By the end of this chapter, you’ll know how to integrate Whisper with advanced
    techniques such as voice activity detection, speaker embedding extraction, and
    clustering, enabling you to augment its capabilities and achieve state-of-the-art
    diarization performance. You’ll also learn how to leverage NVIDIA’s powerful **multiscale
    diarization decoder** (**MSDD**) model, which considers multiple temporal resolutions
    of speaker embeddings to deliver exceptional accuracy. By mastering the techniques
    presented in this chapter, you’ll be well-equipped to tackle complex multispeaker
    audio scenarios and push the boundaries of what’s possible with OpenAI Whisper.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，您将了解如何将Whisper与先进的技术（如语音活动检测、说话人嵌入提取和聚类）集成，从而增强其功能，并实现最先进的分段性能。您还将学习如何利用NVIDIA强大的**多尺度分段解码器**（**MSDD**）模型，该模型考虑了说话人嵌入的多个时间分辨率，以提供卓越的准确性。通过掌握本章中介绍的技术，您将能够应对复杂的多说话人音频场景，推动OpenAI
    Whisper的可能性极限。
- en: Get ready to dive into the exciting world of speaker diarization and unlock
    new insights from multispeaker conversations. Let’s begin this transformative
    journey together!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好深入探索说话人分段的激动人心的世界，并从多说话人对话中获得新的见解吧！让我们一起开始这段变革之旅！
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To harness the capabilities of OpenAI’s Whisper for advanced applications, this
    chapter leverages Python and Google Colab for ease of use and accessibility. The
    Python environment setup includes the Whisper library for transcription tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用OpenAI的Whisper实现高级应用，本章使用Python和Google Colab，便于使用和访问。Python环境设置包括用于转录任务的Whisper库。
- en: '**Key requirements**:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要求**：'
- en: '**Google Colab notebooks**: The notebooks are set to run our Python code with
    the minimum required memory and capacity. If the **T4 GPU** runtime type is available,
    select it for better performance.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Colab笔记本**：笔记本设置为使用最低要求的内存和容量运行我们的Python代码。如果可用，请选择**T4 GPU**运行类型以获得更好的性能。'
- en: '**Google Colab notebooks**: The notebooks are set to run our Python code with
    the minimum required memory and capacity. If that option is available, change
    the runtime type to **GPU** for better performance.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Google Colab笔记本**：笔记本设置为使用最低要求的内存和容量运行我们的Python代码。如果可用，请将运行时类型更改为**GPU**以获得更好的性能。'
- en: '**Python environment**: Each notebook contains directives to load the required
    Python libraries, including Whisper and Gradio.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python环境**：每个笔记本包含指令，用于加载所需的Python库，包括Whisper和Gradio。'
- en: '**Hugging Face account**: Some notebooks require a Hugging Face account and
    login API key. The Colab notebooks include information about this topic.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face帐户**：某些笔记本需要Hugging Face帐户和登录API密钥。Colab笔记本中包含有关此主题的信息。'
- en: '**Microphone and speakers**: Some notebooks implement a Gradio app with voice
    recording and audio playback. A microphone and speakers connected to your computer
    might help you experience the interactive voice features. Another option is to
    open the URL link that Gradio provides at runtime on your mobile phone; from there,
    you can use the phone’s microphone to record your voice.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**麦克风和扬声器**：一些笔记本实现了一个带有语音录制和音频播放功能的Gradio应用程序。连接到计算机的麦克风和扬声器可以帮助您体验互动语音功能。另一种选择是在运行时打开Gradio提供的URL链接，在您的手机上使用手机麦克风录制您的声音。'
- en: '**GitHub repository access**: All Python code, including examples, is available
    in the chapter’s GitHub repository ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter08](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter08)).
    These Colab notebooks are ready to run, providing a practical and hands-on approach
    to learning.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitHub 仓库访问**：所有Python代码，包括示例，都可以在本章的GitHub仓库中找到（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter08](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/tree/main/Chapter08)）。这些Colab笔记本已准备好运行，提供了一种实用且动手的学习方法。'
- en: By meeting these technical requirements, you will be prepared to explore Whisper
    in different contexts while enjoying the streamlined experience of Google Colab
    and the comprehensive resources available on GitHub.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过满足这些技术要求，您将为在不同情境中探索Whisper做好准备，同时享受Google Colab带来的流畅体验以及GitHub上提供的全面资源。
- en: Augmenting Whisper with speaker diarization
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用说话人分离增强Whisper
- en: Speaker diarization, partitioning an audio stream into segments according to
    the speaker’s identity, is a powerful feature in multispeaker speech processing.
    It addresses the question of *who spoke when?* In a given audio clip, it is crucial
    to enhance the functionality and usability of ASR systems. The origins of speaker
    diarization can be traced back to the 1990s when the foundational work for clustering-based
    diarization paradigms was laid down. These early studies focused on radio broadcast
    news and communications applications, primarily aiming to improve ASR performance.
    The features used in these early studies were handcrafted mainly, with **Mel-frequency
    cepstral coefficients** (**MFCCs**) being a common choice.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 说话人分离是将音频流按说话人的身份划分为不同的段落，是多说话人语音处理中的一个强大功能。它解决了*谁在什么时候说话？*的问题。在给定的音频片段中，增强ASR系统的功能性和可用性至关重要。说话人分离的起源可以追溯到1990年代，当时为基于聚类的分离范式奠定了基础。这些早期的研究主要集中在广播新闻和通信应用，旨在提高ASR性能。早期研究中使用的特征大多是手工设计的，其中**Mel频率倒谱系数**（**MFCCs**）是常见的选择。
- en: Over time, the field of speaker diarization has seen significant advancements,
    particularly with the emergence of deep learning technology. Modern diarization
    systems often leverage neural networks and large-scale GPU computing to improve
    accuracy and efficiency. The progression of diarization techniques has included
    the use of **Gaussian mixture models** (**GMMs**) and **hidden Markov models**
    (**HMMs**) in earlier approaches, followed by the adoption of neural embeddings
    (such as *x*-vectors and *d*-vectors, which we will cover in more detail in the
    *An introduction to speaker embeddings* section later in this chapter) and clustering
    methods in more recent times.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，说话人分离领域取得了显著的进展，特别是在深度学习技术的出现之后。现代分离系统通常利用神经网络和大规模GPU计算来提高准确性和效率。分离技术的发展包括早期方法中使用**高斯混合模型**（**GMMs**）和**隐马尔可夫模型**（**HMMs**），以及近年来采用神经嵌入（如*x*-向量和*d*-向量，我们将在本章稍后的*说话人嵌入介绍*部分中详细介绍）和聚类方法。
- en: One of the most significant contributions to the field has been the development
    of end-to-end neural diarization approaches, which aim to simplify the diarization
    process by merging distinct steps in the diarization pipeline. These approaches
    have been designed to handle the challenges of multispeaker labeling and diarization,
    such as dealing with noisy acoustic environments, a range of vocal tenors, and
    accent nuances.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对该领域最重要的贡献之一是端到端神经分离方法的发展，这些方法通过将分离流程中的不同步骤合并，简化了分离过程。这些方法旨在处理多说话人标注和分离中的挑战，例如处理嘈杂的声学环境、不同的音色和口音差异。
- en: Open source initiatives have also contributed to the evolution of diarization
    capabilities, with tools such as ALIZE, pyannote.audio, pyAudioAnalysis, SHoUT,
    and LIUM SpkDiarization providing resources for researchers and developers to
    implement and experiment with diarization in their applications. Most earlier
    tools are now either inactive or abandoned, except for pyannote.audio (Pyannote).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 开源项目也为说话人分离能力的演变做出了贡献，工具如ALIZE、pyannote.audio、pyAudioAnalysis、SHoUT和LIUM SpkDiarization为研究人员和开发人员提供了资源，以便在他们的应用程序中实现和实验说话人分离。大多数早期工具现在已经不再活跃或被遗弃，只有pyannote.audio（Pyannote）仍在使用。
- en: The early speaker diarization systems, while pioneering in their approach to
    solving the *who spoke when* problem in audio recordings, faced several limitations
    that impacted their accuracy and efficiency. In the next section, we will examine
    the fundamental hurdles of early diarization solutions in more detail.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的说话人分离系统虽然在解决音频录音中*谁在何时说话*的问题上具有开创性，但也面临着若干局限性，这些局限性影响了它们的准确性和效率。在下一节中，我们将更详细地探讨早期说话人分离解决方案的根本性障碍。
- en: Understanding the limitations and constraints of diarization
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解说话人分离的局限性和约束
- en: 'Many of the deficiencies and inaccuracies in early diarization efforts were
    rooted in the technological constraints of the time, the complexity of human speech,
    and the nascent state of machine learning techniques applied to audio processing.
    Understanding these limitations provides valuable insights into the evolution
    of diarization capabilities and the significant advancements made over time:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 早期说话人分离技术中的许多不足和不准确性源于当时的技术约束、人类语音的复杂性以及应用于音频处理的机器学习技术尚处于初步阶段。理解这些局限性为我们提供了对说话人分离能力演变的宝贵见解，并帮助我们认识到随着时间推移所取得的重大进展：
- en: '**Computing limitations**: Early diarization systems were limited by the computational
    power available at the time. Processing large audio datasets required significant
    computational resources, which were not as readily available or as powerful as
    today’s standards. This limitation affected the complexity of the algorithms that
    could be run in a reasonable amount of time, thereby constraining the accuracy
    of early diarization systems.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算限制**：早期的说话人分离系统受限于当时可用的计算能力。处理大规模音频数据集需要大量的计算资源，这些资源在当时并不像今天这样普遍和强大。这一限制影响了可以在合理时间内运行的算法的复杂度，从而限制了早期说话人分离系统的准确性。'
- en: '**Feature extraction and modeling limitations**: The feature extraction techniques
    used in early diarization systems, such as MFCCs, were relatively simplistic compared
    to the sophisticated embeddings used in modern systems. These early features might
    not effectively capture the nuances of different speakers’ voices, leading to
    less accurate speaker differentiation.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取和建模的局限性**：早期的说话人分离系统中使用的特征提取技术，如MFCCs，相较于现代系统中使用的复杂嵌入技术，要简单得多。这些早期的特征可能无法有效捕捉不同说话人声音的细微差别，从而导致说话人区分不够准确。'
- en: '**Reliance on GMMs and HMMs for speaker modeling**: While these models provided
    a foundation for speaker diarization, they were limited in handling the variability
    and complexity of human speech across different speakers and environments.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**依赖GMM和HMM进行说话人建模**：尽管这些模型为说话人分离提供了基础，但它们在处理不同说话人和环境下人类语音的变化性和复杂性时存在局限性。'
- en: '**Handling of speaker change points**: One of the significant challenges for
    early diarization systems was accurately detecting speaker change points. These
    systems struggled particularly with short speech segments and segments close to
    speaker change points. The performance of these systems degraded both as the segment
    duration decreased and the proximity to the speaker change point increased. For
    example, over 33% and 40% of the errors in **single-distant microphone** (**SDM**)
    and **multiple-distant microphone** (**MDM**) conditions occurred within 0.5 seconds
    of a change point for all evaluated systems. SDM refers to a scenario where a
    single microphone is placed at a distance from the speakers, capturing audio from
    all participants. On the other hand, MDM involves multiple microphones placed
    at different locations in the recording environment, providing additional spatial
    information that can be leveraged for improved diarization performance. The percentage
    of errors in the context of these setups highlights early diarization systems’
    challenges in accurately detecting speaker changes, especially near change points.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理说话者变化点**：早期分离系统面临的一个重大挑战是准确检测说话者变化点。这些系统尤其在处理短语音段和接近说话者变化点的语音段时遇到了困难。随着语音段时长的减少以及与变化点的接近，系统的表现会有所下降。例如，在**单一远程麦克风**（**SDM**）和**多重远程麦克风**（**MDM**）的条件下，所有评估的系统中超过33%和40%的错误发生在变化点前后0.5秒内。SDM指的是在离说话者一定距离处放置一个麦克风，用来捕捉所有参与者的音频。而MDM则是在录音环境中不同位置放置多个麦克风，提供额外的空间信息，从而可以提升分离性能。这些设置下的错误百分比突显了早期分离系统在准确检测说话者变化时，尤其是在变化点附近，所面临的挑战。'
- en: '**Scalability and flexibility**: Early diarization systems were often designed
    with specific applications in mind, such as radio broadcast news or meeting recordings,
    and might not quickly adapt to other types of audio content. This lack of flexibility
    limited the broader application of diarization technology. Moreover, the scalability
    of these systems to handle large-scale or real-time diarization tasks was a significant
    challenge.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展性和灵活性**：早期的分离系统通常是针对特定应用设计的，如广播新闻或会议录音，可能无法快速适应其他类型的音频内容。这种缺乏灵活性限制了分离技术的广泛应用。此外，这些系统在处理大规模或实时分离任务时的可扩展性也是一个重大挑战。'
- en: '**Error analysis and improvement directions**: In-depth error analysis of early
    diarization systems revealed that improvements near speaker change points could
    significantly impact overall performance. Modifications such as alternative minimum
    duration constraints and leveraging the difference between the most prominent
    and second-largest log-likelihood scores for unsupervised clustering were explored
    to address these limitations.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误分析和改进方向**：对早期分离系统的深入错误分析表明，改善说话者变化点附近的处理可以显著提升整体性能。为解决这些限制，探索了替代最小持续时间约束以及利用最显著和第二大对数似然得分之间的差异进行无监督聚类等改进措施。'
- en: Despite their groundbreaking efforts, early approaches to speaker diarization
    encountered various limitations that could have improved their accuracy and efficiency.
    These limitations stemmed from technological constraints, the intricacies of human
    speech, and the nascent state of machine-learning techniques. However, introducing
    transformer-based models has revolutionized the field, addressing many of these
    challenges and paving the way for more accurate and efficient solutions.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管早期的说话者分离方法取得了开创性进展，但它们仍面临各种限制，这些限制本可以提高它们的准确性和效率。这些限制来源于技术约束、人类语音的复杂性以及机器学习技术的初期阶段。然而，引入基于变压器的模型彻底改变了这一领域，解决了许多挑战，为更准确高效的解决方案铺平了道路。
- en: Bringing transformers into speech diarization
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将变压器引入语音分离
- en: Transformers have been instrumental in advancing state-of-the-art speech diarization.
    They are adept at handling speech’s sequential and contextual nature, which is
    essential for differentiating between speakers in an audio stream. The self-attention
    mechanism within transformers allows a model to weigh the importance of each part
    of the input data, which is crucial for identifying speaker change points and
    attributing speech segments to the correct speaker.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器在推动最先进的语音分离技术方面起到了关键作用。它们擅长处理语音的序列性和上下文性，这对于在音频流中区分说话人至关重要。变换器中的自注意力机制使得模型能够权衡输入数据中每个部分的重要性，这对于识别说话人切换点并将语音段归属到正确的说话人至关重要。
- en: As mentioned earlier, traditional diarization methods often relied on GMMs and
    HMMs to model speaker characteristics. These methods need to be improved to handle
    the variability and complexity of human speech. In contrast, transformer-based
    diarization systems can process entire data sequences simultaneously, allowing
    them to capture the context and relationships between speech segments more effectively.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，传统的语音分离方法通常依赖于高斯混合模型（GMMs）和隐马尔可夫模型（HMMs）来建模说话人的特征。这些方法需要改进，以应对人类语音的变化性和复杂性。相比之下，基于变换器的语音分离系统可以同时处理整个数据序列，从而更有效地捕捉语音段之间的上下文和关系。
- en: Transformers also enable embeddings, such as *x*-vectors and *d*-vectors, which
    provide a more nuanced representation of speaker characteristics. This leads to
    improved diarization performance, especially in challenging acoustic environments
    or scenarios with overlapping speech.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器还启用了嵌入表示，如*x*-向量和*d*-向量，它们提供了更细致的说话人特征表示。这有助于提高语音分离的性能，特别是在具有挑战性的声学环境或有重叠语音的场景中。
- en: Moving beyond the limitations of earlier diarization attempts, we must introduce
    a game-changing framework that brings transformers into speech diarization – NVIDIA’s
    **Neural Modules** (**NeMo**). NeMo is an open source toolkit for building, training,
    and fine-tuning GPU-accelerated speech and NLP models. It provides a collection
    of pre-built modules and models that can be quickly composed to create complex
    AI applications, such as ASR, natural language understanding, and text-to-speech
    synthesis. NeMo offers a more direct approach to diarization with its transformer-based
    pipeline, opening new possibilities for speaker identification and separation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 超越早期语音分离方法的局限性，我们必须引入一种颠覆性的框架，将变换器（transformers）引入语音分离——NVIDIA的**神经模块**（**NeMo**）。NeMo是一个开源工具包，用于构建、训练和微调GPU加速的语音和自然语言处理（NLP）模型。它提供了一组预构建的模块和模型，可以快速组合以创建复杂的AI应用程序，如自动语音识别（ASR）、自然语言理解和文本到语音合成。NeMo通过其基于变换器的管道为语音分离提供了更直接的方法，开启了说话人识别和分离的新可能性。
- en: Introducing NVIDIA’s NeMo framework
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入NVIDIA的NeMo框架
- en: Compared to traditional methods, transformer-based diarization systems provide
    superior performance and are better suited to the complexities of natural speech.
    NVIDIA’s NeMo toolkit supports training and fine-tuning speaker diarization models.
    NeMo leverages transformer-based models for various speech tasks, including diarization.
    The toolkit provides a pipeline that includes **voice activity detection** (**VAD**),
    **speaker embedding extraction**, and **clustering** modules, which are essential
    components of a diarization system. NeMo’s approach to diarization involves training
    models that can capture the characteristics of unseen speakers and assign audio
    segments to the correct speaker index.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统方法相比，基于变换器的语音分离系统提供了更优的性能，并更适应自然语音的复杂性。NVIDIA的NeMo工具包支持训练和微调说话人语音分离模型。NeMo利用基于变换器的模型处理各种语音任务，包括语音分离。该工具包提供了一个管道，其中包括**语音活动检测**（**VAD**）、**说话人嵌入提取**和**聚类**模块，这些模块是语音分离系统的关键组成部分。NeMo的语音分离方法涉及训练能够捕捉未见过说话人特征的模型，并将音频段分配到正确的说话人索引。
- en: From a more comprehensive point of view, NVIDIA NeMo offers much more than transformer-based
    diarization. NeMo is an end-to-end, cloud-native framework for building, customizing,
    and deploying generative AI models across various platforms, including LLMs. It
    provides a comprehensive solution for the entire generative AI model development
    life cycle, from data processing and model training to inference. NeMo is particularly
    noted for its capabilities in conversational AI, encompassing ASR, NLP, and text-to-speech
    synthesis.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从更全面的角度来看，NVIDIA的NeMo提供的功能远超基于变换器的说话人分离。NeMo是一个端到端的、云原生的框架，用于在各种平台上构建、定制和部署生成性AI模型，包括大型语言模型（LLMs）。它为整个生成性AI模型开发生命周期提供了全面的解决方案，从数据处理、模型训练到推理。NeMo特别以其在对话AI方面的能力而著称，涵盖了自动语音识别（ASR）、自然语言处理（NLP）和文本转语音合成。
- en: NeMo stands out for its ability to handle large-scale models, supporting the
    training of models with up to trillions of parameters. Advanced parallelization
    techniques such as tensor parallelism, pipeline parallelism, and sequence parallelism
    facilitate this, enabling efficient scaling of models across thousands of GPUs.
    The framework is built on top of PyTorch and PyTorch Lightning, offering a familiar
    environment for researchers and developers to innovate within the conversational
    AI space.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: NeMo以其处理大规模模型的能力脱颖而出，支持训练具有数万亿参数的模型。诸如张量并行、流水线并行和序列并行等先进的并行化技术为此提供了支持，使得模型可以在成千上万的GPU上高效扩展。该框架建立在PyTorch和PyTorch
    Lightning之上，为研究人员和开发人员提供了一个熟悉的环境，在对话AI领域进行创新。
- en: One of the critical features of NeMo is its modular architecture, where models
    are composed of neural modules with strongly typed input and output. This design
    promotes reusability and simplifies the creation of new conversational AI models
    by allowing researchers to leverage pre-existing code and pre-trained models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: NeMo的一个关键特点是其模块化架构，在这种架构中，模型由具有强类型输入和输出的神经模块组成。这种设计促进了重用性，并简化了新对话AI模型的创建，允许研究人员利用现有代码和预训练模型。
- en: NeMo is available as open source software, encouraging contributions from the
    community and facilitating widespread adoption and customization. It also integrates
    with NVIDIA’s AI platform, including the NVIDIA Triton Inference Server, to deploy
    models in production environments. NVIDIA NeMo provides a powerful and flexible
    framework to develop state-of-the-art conversational AI models, offering tools
    and resources that streamline bringing generative AI applications from concept
    to deployment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: NeMo作为开源软件可供使用，鼓励社区的贡献，并促进了广泛的采用和定制。它还与NVIDIA的AI平台集成，包括NVIDIA Triton推理服务器，以便在生产环境中部署模型。NVIDIA
    NeMo提供了一个强大而灵活的框架，用于开发最先进的对话AI模型，提供的工具和资源可以简化将生成性AI应用从概念到部署的全过程。
- en: Now that we’ve explored Whisper and NeMo’s capabilities separately, let’s consider
    the potential of integrating these two powerful tools. Combining Whisper’s transcription
    prowess with NeMo’s advanced diarization features can unlock even greater insights
    from audio data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经分别探讨了Whisper和NeMo的能力，接下来我们来考虑将这两种强大工具集成的潜力。将Whisper的转录能力与NeMo的先进说话人分离功能结合，可以从音频数据中解锁更多的洞见。
- en: Integrating Whisper and NeMo
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成Whisper与NeMo
- en: While Whisper is primarily known for its transcription capabilities, it can
    also be adapted for diarization tasks. However, Whisper does not natively support
    speaker diarization. To achieve diarization with Whisper, additional tools such
    as Pyannote, a speaker diarization toolkit, can be used in conjunction with Whisper’s
    transcriptions to identify speakers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Whisper主要以其转录能力而闻名，但它也可以适应说话人分离任务。然而，Whisper并不原生支持说话人分离。为了实现Whisper的说话人分离，需要借助如Pyannote这样的说话人分离工具包，结合Whisper的转录结果来识别说话人。
- en: Integrating NVIDIA’s NeMo with OpenAI’s Whisper for speaker diarization involves
    a novel pipeline that leverages the strengths of both systems to enhance diarization
    outcomes. This integration is particularly notable in the context of inference
    and result interpretation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 将NVIDIA的NeMo与OpenAI的Whisper结合进行说话人分离，涉及到一个创新的流程，利用两个系统的优势来增强分离效果。这种集成在推理和结果解读方面尤其值得注意。
- en: The pipeline begins with Whisper processing audio to generate highly accurate
    transcriptions. Whisper’s role is primarily to transcribe the audio, providing
    detailed textual output of the spoken content. However, Whisper does not natively
    support speaker diarization—identifying *who spoke when* within the audio.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 该管道首先由 Whisper 处理音频以生成高精度的转录。Whisper 的主要角色是转录音频，提供详细的口语内容文本输出。然而，Whisper 本身不支持说话人分离（diarization）——识别音频中*谁在什么时候说话*。
- en: To introduce diarization, the pipeline incorporates NVIDIA’s NeMo, specifically
    its speaker diarization module. NeMo’s diarization system is designed to process
    audio recordings, segmenting them by speaker labels. It achieves this through
    several steps, including VAD, speaker embedding extraction, and clustering. The
    speaker embeddings capture unique voice characteristics, which are then clustered
    to differentiate between speakers in the audio.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了引入分离，管道集成了 NVIDIA 的 NeMo，特别是它的说话人分离模块。NeMo 的分离系统旨在处理音频录音，通过说话人标签进行分段。它通过多个步骤实现这一目标，包括语音活动检测（VAD）、说话人嵌入提取和聚类。说话人嵌入捕捉独特的声音特征，然后通过聚类区分音频中的不同说话人。
- en: The integration of Whisper and NeMo for diarization allows you to align Whisper’s
    transcriptions with speaker labels identified by NeMo. This means that the output
    includes what was said (from Whisper’s transcriptions) and identifies which speaker
    said each part (from NeMo’s diarization). The result is a more comprehensive understanding
    of the audio content, providing both the textual transcription and the speaker
    attribution.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 和 NeMo 在分离中的集成使你能够将 Whisper 的转录与 NeMo 识别的说话人标签对齐。这意味着输出不仅包括说了什么（来自 Whisper
    的转录），还识别了每一部分由哪位说话人说（来自 NeMo 的分离）。结果是对音频内容有更全面的理解，既提供文本转录，又提供说话人的归属。
- en: This integration is beneficial in scenarios where understanding conversation
    dynamics is crucial, such as meetings, interviews, and legal proceedings. It enhances
    the utility of transcriptions by adding a layer of speaker-specific context, making
    it easier to follow conversations and attribute statements accurately.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种集成在理解对话动态至关重要的场景中非常有用，例如会议、访谈和法律程序。通过为转录添加一层特定于说话人的上下文，它增强了转录的实用性，使得更容易跟随对话并准确归属发言。
- en: The integration between Whisper and NeMo for speaker diarization combines Whisper’s
    advanced transcription capabilities with NeMo’s robust diarization framework.
    This synergy enhances the interpretability of audio content by providing detailed
    transcriptions alongside accurate speaker labels, thereby offering a richer analysis
    of spoken interactions.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Whisper 和 NeMo 的说话人分离集成将 Whisper 的先进转录能力与 NeMo 强大的分离框架结合在一起。这种协同作用通过提供详细的转录和准确的说话人标签，增强了音频内容的可解释性，从而为口语互动提供更丰富的分析。
- en: Before we delve deeper into the integration of Whisper and NeMo, it’s crucial
    to understand a fundamental concept in modern speech processing systems – **speaker
    embeddings**. These vectorial representations of speaker characteristics are vital
    in enabling accurate speaker diarization.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨 Whisper 和 NeMo 的集成之前，了解现代语音处理系统中的一个基本概念——**说话人嵌入**至关重要。这些说话人特征的向量表示对于实现准确的说话人分离至关重要。
- en: An introduction to speaker embeddings
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 说话人嵌入简介
- en: Speaker embeddings are vectorial representations extracted from a speech signal
    that encapsulate the characteristics of a speaker’s voice in a compact form. These
    embeddings are designed to be discriminative, meaning they can effectively differentiate
    between speakers while being robust to variations in speech content, channel,
    and environmental noise. The goal is to obtain a fixed-length vector from variable-length
    speech utterances that capture the unique traits of a speaker’s voice.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 说话人嵌入（Speaker embeddings）是从语音信号中提取的向量表示，能够以紧凑的形式 encapsulate 说话人声音的特征。这些嵌入被设计为具有辨别性，意味着它们能够有效地区分说话人，同时对语音内容、通道和环境噪音的变化具有鲁棒性。目标是从可变长度的语音话语中获取固定长度的向量，捕捉说话人声音的独特特征。
- en: Speaker embeddings are a fundamental component in modern speech processing systems,
    enabling various applications from speaker verification to diarization. Their
    ability to condense the rich information of a speaker’s voice into a fixed-length
    vector makes them invaluable for systems that need to recognize, differentiate,
    or track speakers across audio recordings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 说话人嵌入是现代语音处理系统的基础组件，支持从说话人验证到分离等多种应用。它们能够将说话人声音的丰富信息压缩成固定长度的向量，这使得它们在需要识别、区分或追踪不同说话人的音频记录系统中具有不可替代的价值。
- en: 'From a more technical perspective, there are several types of speaker embeddings,
    each with its method of extraction and characteristics:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从更技术的角度来看，有几种类型的说话人嵌入，每种都有其提取方法和特征：
- en: '*i***-vectors**: These embeddings capture speaker and channel variabilities
    in a low-dimensional space. They are derived from a GMM framework and represent
    the differences between a given speaker’s pronunciation and the average pronunciation
    across a set of phonetic classes.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*i*-vectors：这些嵌入在低维空间中捕捉说话人和通道的变异性。它们源自GMM框架，并表示给定说话人的发音与一组语音类别之间的平均发音的差异。'
- en: '*d***-vectors**: These are obtained by training a speaker-discriminative **deep
    neural network** (**DNN**) and extracting frame-level vectors from the last hidden
    layer. These vectors are then averaged over the entire utterance to produce the
    *d*-vector, representing the speaker’s identity.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d*-vectors：这些向量通过训练一个说话人区分的**深度神经网络**（**DNN**）并从最后的隐藏层提取帧级向量得到。这些向量随后在整个话语中进行平均，产生*d*-vector，代表说话人的身份。'
- en: '*x***-vectors**: This type of embedding involves frame- and segment-level feature
    (utterance) processing. *X*-vectors are extracted using a DNN that processes a
    sequence of acoustic features and aggregates them, using a statistics pooling
    layer to produce a fixed-length vector.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*-vectors：这种类型的嵌入涉及帧级和段级特征（话语）处理。*X*-vectors通过一个DNN提取，该DNN处理一系列声学特征并对它们进行聚合，使用统计池化层生成固定长度的向量。'
- en: '*s***-vectors**: Also known as sequence or summary vectors, *s*-vectors are
    derived from recurrent neural network architectures such as RNNs or LSTMs. They
    are designed to capture sequential information and can encode spoken terms and
    word orders to a notable extent.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s*-vectors：也称为序列向量或摘要向量，*s*-vectors来源于递归神经网络架构，如RNN或LSTM。它们旨在捕捉顺序信息，可以在相当程度上编码口语词汇和单词顺序。'
- en: Extracting speaker embeddings typically involves training a neural network model
    to optimize the encoder using loss functions that encourage discriminative learning.
    After training, the pre-activation of a hidden layer at the segment-level network
    is extracted as the speaker embedding. The network is trained on a large dataset
    of speakers to ensure that the embeddings generalize well to unseen speakers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 提取说话人嵌入通常涉及训练一个神经网络模型，通过优化编码器并使用鼓励区分学习的损失函数来实现。训练完成后，提取段级网络中隐藏层的前激活值作为说话人嵌入。该网络在一个包含大量说话人的数据集上进行训练，以确保嵌入能够很好地泛化到未见过的说话人。
- en: In the context of speaker diarization, speaker embeddings cluster speech segments
    according to the speaker’s identity. The embeddings provide a way to measure the
    similarity between segments and groups of those likely to be from the same speaker.
    This is a crucial step in the diarization process, as it allows you to accurately
    attribute speech to the correct speaker within an audio stream.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在说话人分离的背景下，说话人嵌入根据说话人的身份将语音段进行聚类。嵌入提供了一种测量段之间相似性的方法，并将这些段与可能来自同一说话人的段群体进行分组。这是分离过程中的关键步骤，因为它允许你在音频流中准确地将语音归属到正确的说话人。
- en: As we’ve seen, both Whisper augmented with Pyannote and NVIDIA’s NeMo offer
    powerful diarization capabilities. However, it’s essential to understand the critical
    differences between these approaches to make informed decisions when choosing
    a diarization solution.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，增强了Pyannote的Whisper和NVIDIA的NeMo都提供了强大的分离能力。然而，理解这些方法之间的关键区别至关重要，这样才能在选择分离解决方案时做出明智的决定。
- en: Differentiating NVIDIA’s NeMo capabilities
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分NVIDIA的NeMo能力
- en: 'The integration of diarization capabilities into ASR systems has been significantly
    influenced by the advent of transformer models, particularly in the context of
    OpenAI’s Whisper and NVIDIA’s NeMo frameworks. These advancements have improved
    the accuracy of ASR systems and introduced new methodologies to handle speaker
    diarization tasks. Let’s delve into the similarities and differences between Whisper
    diarization using Pyannote and diarization using NVIDIA’s NeMo, focusing on speech
    activity detection, speaker change detection, and overlapped speech detection.
    Understanding the differences between these two approaches to speaker diarization
    is crucial for making informed decisions when choosing a solution for your specific
    use case. By examining how each system handles critical aspects of the diarization
    process, such as speech activity detection, speaker change detection, and overlapped
    speech detection, you can better assess which approach aligns with your accuracy,
    efficiency, and ease of integration requirements:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 分话功能的集成对 ASR 系统的影响受到变换器模型（transformer models）出现的显著推动，特别是在 OpenAI 的 Whisper 和
    NVIDIA 的 NeMo 框架的背景下。这些进展提升了 ASR 系统的准确性，并引入了处理分话任务的新方法。我们将深入探讨使用 Pyannote 的 Whisper
    分话与使用 NVIDIA NeMo 的分话之间的相似性与差异，重点关注语音活动检测、说话人变化检测和重叠语音检测。了解这两种分话方法之间的差异，对于在选择适合自己特定使用案例的解决方案时做出明智决策至关重要。通过研究每个系统如何处理分话过程中的关键环节，如语音活动检测、说话人变化检测和重叠语音检测，您可以更好地评估哪种方法与您的准确性、效率和集成需求最为契合：
- en: '| **Diarization feature** | **Whisper** **with Pyannote** | **NVIDIA NeMo**
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| **分话功能** | **Whisper** **与 Pyannote** | **NVIDIA NeMo** |'
- en: '| --- | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Detecting** **speech activity** | Whisper does not inherently perform VAD
    as part of its diarization process. However, when combined with Pyannote, an external
    VAD model from the Pyannote toolkit can segment the audio into speech and non-speech
    intervals before applying diarization. This approach requires integrating Whisper’s
    ASR capabilities with Pyannote’s VAD models, based on deep learning techniques
    and fine-tuning, for accurate speech/non-speech segmentation. | NeMo’s speaker
    diarization pipeline includes a dedicated VAD module that is trainable and optimized
    as part of the diarization system. This VAD model is designed to detect the presence
    or absence of speech and generate timestamps for speech activity within an audio
    recording. Integrating VAD within NeMo’s diarization pipeline allows for a more
    streamlined process, directly feeding the VAD results into subsequent diarization
    steps. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **检测** **语音活动** | Whisper 本身并不执行语音活动检测（VAD）作为其分话任务的一部分。然而，当与 Pyannote 结合使用时，来自
    Pyannote 工具包的外部 VAD 模型可以在应用分话之前将音频分割为语音和非语音区间。这种方法需要将 Whisper 的语音识别（ASR）能力与 Pyannote
    的 VAD 模型相结合，基于深度学习技术和微调，实现精确的语音/非语音分割。 | NeMo 的分话流程包括一个专门的 VAD 模块，该模块是可训练和优化的，作为分话系统的一部分。此
    VAD 模型旨在检测语音的存在或缺失，并生成语音活动的时间戳。将 VAD 集成到 NeMo 的分话流程中，可以实现更加简化的过程，直接将 VAD 结果传递到后续的分话步骤中。
    |'
- en: '| **Detecting** **speaker change** | The integration of Whisper with Pyannote
    for diarization purposes relies on Pyannote’s speaker change detection capabilities.
    Pyannote employs neural network models to identify points in audio where a speaker
    change occurs. This process is crucial for segmenting the audio into homogeneous
    segments attributed to individual speakers. Speaker change detection in Pyannote
    is a separate module that works with its diarization pipeline. | NeMo’s approach
    to speaker change detection is implicitly handled within its diarization pipeline,
    including modules for extracting and clustering speaker embeddings. While NeMo
    does not explicitly mention a standalone speaker change detection module, identifying
    speaker changes is integrated into the overall diarization workflow, mainly through
    analyzing speaker embeddings and their temporal distribution across audio. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **检测** **说话人变化** | Whisper 与 Pyannote 集成以执行分话任务时，依赖于 Pyannote 的说话人变化检测能力。Pyannote
    使用神经网络模型来识别音频中发生说话人变化的点。这个过程对于将音频分割成归属于各个说话人的同质段落至关重要。Pyannote 中的说话人变化检测是一个独立模块，与其分话流程一起工作。
    | NeMo 的说话人变化检测方法隐式地在其分话流程中处理，包括用于提取和聚类说话人嵌入的模块。虽然 NeMo 没有明确提到独立的说话人变化检测模块，但通过分析说话人嵌入及其在音频中的时间分布，识别说话人变化已集成到整体分话工作流中。
    |'
- en: '| **Detecting** **overlapped speech** | Overlapped speech detection is another
    area where Pyannote complements Whisper’s capabilities. Pyannote’s toolkit includes
    models designed to detect and handle overlapping speech, a challenging aspect
    of speaker diarization. This functionality is crucial for accurately diarizing
    conversations where multiple speakers simultaneously talk. | Like speaker change
    detection, NeMo’s treatment of overlapped speech is integrated into its diarization
    pipeline rather than being addressed by a separate module. The system’s ability
    to handle overlapped speech results from its sophisticated speaker embedding and
    clustering techniques, which can identify and separate speakers even in challenging
    overlapping scenarios. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| **检测** **重叠语音** | 重叠语音检测是Pyannote补充Whisper功能的另一个领域。Pyannote的工具包包括设计用于检测和处理重叠语音的模型，这是说话人分离中一个具有挑战性的方面。这个功能对于准确地分离多个说话人同时发言的对话至关重要。|
    与说话人变化检测类似，NeMo对重叠语音的处理被集成到其分离管道中，而不是通过单独的模块来解决。该系统处理重叠语音的能力来源于其复杂的说话人嵌入和聚类技术，即使在挑战性的重叠情境下，也能识别和分离说话人。|'
- en: '| **Integrating speaker embeddings in the** **diarization pipeline** | Whisper’s
    combination with Pyannote relies on external modules for these tasks, offering
    flexibility and modularity. In contrast, NeMo’s diarization pipeline directly
    integrates these functionalities, providing a streamlined and cohesive workflow.
    These advancements underscore the transformative impact of transformer models
    on speech processing, paving the way for more accurate and efficient diarization
    systems. | NVIDIA’s NeMo toolkit includes a more integrated approach to speaker
    diarization. It provides a complete diarization pipeline that includes VAD, speaker
    embedding extraction, and clustering. NeMo’s speaker embeddings are extracted
    using models explicitly trained for this purpose, and these embeddings are then
    used within the same framework to perform the clustering necessary for diarization.
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **将说话人嵌入集成到** **分离管道** | Whisper与Pyannote的结合依赖于外部模块来完成这些任务，提供了灵活性和模块化。而NeMo的分离管道直接集成了这些功能，提供了一个简化而连贯的工作流程。这些进展凸显了变换器模型对语音处理的变革性影响，为更精确高效的分离系统铺平了道路。|
    NVIDIA的NeMo工具包提供了一种更集成的说话人分离方法。它提供了一个完整的分离管道，包含VAD、说话人嵌入提取和聚类。NeMo的说话人嵌入是通过专门训练的模型提取的，这些嵌入随后在同一框架中用于执行分离所需的聚类。|'
- en: '| **Clustering and assigning** **speaker embeddings** | After extracting speaker
    embeddings, Pyannote uses various clustering algorithms, such as hierarchical
    clustering, to group and assign the embeddings to the respective speakers. This
    clustering process is crucial for determining which audio segments belong to which
    speaker. | NeMo also uses clustering algorithms to group speaker embeddings. However,
    NeMo employs a multiscale, auto-tuning, spectral clustering approach, reportedly
    more resilient than the Pyannote version. This approach involves segmenting the
    audio file with different window lengths and calculating embeddings for multiple
    scales, which are then clustered to label each segment with a speaker. |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| **聚类和分配** **说话人嵌入** | 在提取了说话人嵌入后，Pyannote使用各种聚类算法，如层次聚类，来对嵌入进行分组并将其分配给相应的说话人。这个聚类过程对于确定哪些音频段属于哪个说话人至关重要。|
    NeMo也使用聚类算法对说话人嵌入进行分组。然而，NeMo采用了一种多尺度、自调节的光谱聚类方法，据称比Pyannote的版本更具抗干扰性。这种方法包括使用不同窗口长度对音频文件进行分段，并计算多个尺度的嵌入，随后将这些嵌入进行聚类，以标记每个段落的说话人。|'
- en: Table 8.1 – How different diarization approaches handle critical diarization
    features
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 – 不同的分离方法如何处理关键的分离特征
- en: While both Whisper augmented with Pyannote and NVIDIA’s NeMo use speaker embeddings
    as a core part of their diarization pipelines, their approaches have notable differences.
    Whisper requires an external toolkit (`pyannote.audio`) to perform diarization,
    whereas NeMo offers an all-in-one solution with its speaker embedding extraction
    and clustering modules. NeMo’s multiscale clustering approach is a distinctive
    feature, differentiating it from the Pyannote implementation used with Whisper.
    These differences reflect the diverse methodologies and innovations present in
    the field of speaker diarization research.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Whisper结合Pyannote和NVIDIA的NeMo都使用说话人嵌入作为其分离流程的核心部分，但它们的处理方法有显著的不同。Whisper需要一个外部工具包（`pyannote.audio`）来执行说话人分离，而NeMo则提供了一个集成的解决方案，包括说话人嵌入提取和聚类模块。NeMo的多尺度聚类方法是一个独特的特点，使其与与Whisper结合使用的Pyannote实现有所不同。这些差异反映了语音分离研究领域中多样化的方法和创新。
- en: Blending Whisper and PyAnnote – WhisperX
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 混合Whisper和PyAnnote – WhisperX
- en: WhisperX ([https://replicate.com/dinozoiddev/whisperx](https://replicate.com/dinozoiddev/whisperx))
    provides fast ASR (70x faster than OpenAI’s `Whisper large-v2`) with word-level
    timestamps and speaker diarization, a feature not natively supported by Whisper.
    WhisperX builds upon the foundational strengths of Whisper by addressing some
    of its limitations, particularly in timestamp accuracy and speaker diarization.
    While Whisper provides utterance-level timestamps, WhisperX advances this by offering
    word-level timestamps, crucial for applications requiring precise synchronization
    between text and audio, such as subtitling and detailed audio analysis. This is
    achieved through combining techniques, including VAD, pre-segmentation of audio
    into manageable chunks, and forced alignment with an external phoneme model to
    provide accurate word-level timestamps.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: WhisperX ([https://replicate.com/dinozoiddev/whisperx](https://replicate.com/dinozoiddev/whisperx))
    提供了快速的自动语音识别（比OpenAI的`Whisper large-v2`快70倍），并且具备词级时间戳和说话人分离功能，这些是Whisper本身不原生支持的特性。WhisperX在Whisper的基础优势上进行扩展，解决了其一些局限性，特别是在时间戳精度和说话人分离方面。尽管Whisper提供的是发言级别的时间戳，WhisperX通过提供词级时间戳来推动这一进步，这对于需要精确文本和音频同步的应用（如字幕和详细的音频分析）至关重要。这一功能通过结合多种技术实现，包括VAD（语音活动检测）、将音频预分段为可管理的块，以及使用外部音素模型进行强制对齐，从而提供准确的词级时间戳。
- en: The implementation of WhisperX supports transcription in all languages supported
    by Whisper, with alignment currently available for English audio. It has been
    upgraded to incorporate the latest Whisper models and diarization technologies
    powered by Pyannote to enhance its performance further. At the time of writing,
    WhisperX incorporates `whisper-large-v3` along with diarization upgrades to speaker-diarization-3.1
    and segmentation-3.0, powered by Pyannote. WhisperX demonstrates significant improvements
    over Whisper in word segmentation precision and recall, as well as reductions
    in WER and increases in transcription speed, especially when employing batched
    transcription with VAD preprocessing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: WhisperX的实现支持Whisper支持的所有语言的转录，目前英语音频的对齐功能已经可用。它已升级为融合最新的Whisper模型和由Pyannote支持的分离技术，以进一步提升其性能。在撰写本文时，WhisperX集成了`whisper-large-v3`，并且通过Pyannote增强了说话人分离（更新至speaker-diarization-3.1）和分段技术（更新至segmentation-3.0）。WhisperX在词汇分割的精度和召回率方面展现出了显著改进，同时在词错误率（WER）上有所减少，并且在采用批量转录和VAD预处理时，转录速度有了显著提升。
- en: In summary, WhisperX is a significant evolution of OpenAI’s Whisper, offering
    enhanced functionality through word-level timestamps and speaker diarization.
    These advancements make WhisperX a powerful tool for applications requiring detailed
    and accurate speech transcription and analysis.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，WhisperX是OpenAI的Whisper的重要进化，提供了通过词级时间戳和说话人分离增强的功能。这些进展使WhisperX成为一个强大的工具，适用于需要详细且准确的语音转录和分析的应用。
- en: With this solid theoretical foundation, it’s time to put our knowledge into
    practice. The next hands-on section will explore a practical implementation that
    combines WhisperX, NeMo, and other supporting Python libraries to perform speech
    diarization on real-world audio data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个坚实的理论基础，是时候将我们的知识付诸实践了。接下来的实践部分将探索一个实际的实现，结合WhisperX、NeMo和其他支持的Python库，对现实世界的音频数据进行语音分离。
- en: Performing hands-on speech diarization
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行实践中的语音分离
- en: Transitioning from the theoretical context of speech diarization, let’s immerse
    ourselves in the practical implementation that combines WhisperX, NeMo, and other
    supporting Python libraries, all from the comfort of our trusty Google Colaboratory.
    I encourage you to visit the book’s GitHub repository, find the `LOAIW_ch08_diarizing_speech_with_WhisperX_and_NVIDIA_NeMo.ipynb`
    notebook ([https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter08/LOAIW_ch08_diarizing_speech_with_WhisperX_and_NVIDIA_NeMo.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter08/LOAIW_ch08_diarizing_speech_with_WhisperX_and_NVIDIA_NeMo.ipynb)),
    and run the Python code yourself; feel free to experiment by modifying parameters
    and observe the results. The notebook provides a detailed walk-through to integrate
    Whisper’s transcription capabilities with NeMo’s diarization framework, offering
    a robust solution to analyze speech in audio recordings.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从语音分离的理论背景过渡到实际实现，让我们深入了解结合WhisperX、NeMo以及其他支持Python库的实践应用，所有这些都可以在我们信赖的Google
    Colaboratory中完成。我鼓励你访问本书的GitHub仓库，找到`LOAIW_ch08_diarizing_speech_with_WhisperX_and_NVIDIA_NeMo.ipynb`笔记本（[https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter08/LOAIW_ch08_diarizing_speech_with_WhisperX_and_NVIDIA_NeMo.ipynb](https://github.com/PacktPublishing/Learn-OpenAI-Whisper/blob/main/Chapter08/LOAIW_ch08_diarizing_speech_with_WhisperX_and_NVIDIA_NeMo.ipynb)），并自己运行Python代码；可以随意修改参数并观察结果。该笔记本详细介绍了如何将Whisper的转录功能与NeMo的语音分离框架集成，提供了一个强大的解决方案来分析音频记录中的语音。
- en: The notebook is structured into several key sections, each focusing on a specific
    aspect of the diarization process.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本被结构化为多个关键部分，每个部分专注于语音分离过程中的特定方面。
- en: Setting up the environment
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置环境
- en: 'The first section of the notebook outlines the installation of several Python
    libraries and tools essential for the diarization process:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本的第一部分介绍了几个Python库和工具的安装，这些工具对于语音分离过程至关重要：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s review each to understand their role in diarization:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下每个工具，以理解它们在语音分离中的作用：
- en: '`whisperX`: An extension of OpenAI’s Whisper model, tailored for enhanced functionality.
    Notably, WhisperX installs faster-whisper ([https://github.com/SYSTRAN/faster-whisper](https://github.com/SYSTRAN/faster-whisper)),
    a reimplementation of OpenAI’s Whisper model using CTranslate2 ([https://github.com/OpenNMT/CTranslate2/](https://github.com/OpenNMT/CTranslate2/)).
    This implementation is up to four times faster than OpenAI’s Whisper with the
    same accuracy, while using less memory. The efficiency can be improved with 8-bit
    quantization on both the CPU and GPU.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`whisperX`：OpenAI Whisper模型的扩展，旨在增强功能。特别地，WhisperX安装了faster-whisper（[https://github.com/SYSTRAN/faster-whisper](https://github.com/SYSTRAN/faster-whisper)），这是一个使用CTranslate2（[https://github.com/OpenNMT/CTranslate2/](https://github.com/OpenNMT/CTranslate2/)）重新实现的OpenAI
    Whisper模型。该实现比OpenAI的Whisper快最多四倍，且在保持相同精度的同时，内存占用更少。通过在CPU和GPU上使用8位量化，可以进一步提高效率。'
- en: '`nemo_toolkit[asr]`: NVIDIA’s NeMo toolkit for ASR, providing the foundation
    for speaker diarization.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nemo_toolkit[asr]`：NVIDIA的NeMo工具包，用于自动语音识别（ASR），为说话人分离提供基础。'
- en: '`demucs`: A library for music source separation, functional for preprocessing
    audio files by isolating speech from background music.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`demucs`：一个用于音乐源分离的库，能够通过将语音与背景音乐隔离来进行音频文件的预处理。'
- en: '`dora-search`, `lameenc, and openunmix`: Tools and libraries for audio processing,
    enhancing the quality and compatibility of audio data for diarization tasks.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dora-search`、`lameenc` 和 `openunmix`：用于音频处理的工具和库，提升音频数据的质量和兼容性，以便于语音分离任务。'
- en: '`deepmultilingualpunctuation`: A library for adding punctuation to transcriptions,
    improving the readability and structure of the generated text.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deepmultilingualpunctuation`：一个用于为转录文本添加标点符号的库，改善了生成文本的可读性和结构。'
- en: '`wget and pydub`: Utilities for downloading and manipulating audio files, facilitating
    audio data handling within the Python environment.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wget和pydub`：用于下载和操作音频文件的工具，简化了在Python环境中处理音频数据的过程。'
- en: These libraries collectively form the foundation for processing audio files,
    transcribing speech, and performing speaker diarization. Each tool plays a specific
    role, from preparing the audio data to generating accurate transcriptions and
    identifying distinct speakers within the audio.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库共同构成了处理音频文件、转录语音以及执行说话人分离的基础。每个工具都发挥着特定的作用，从准备音频数据到生成准确的转录内容，再到识别音频中的不同说话人。
- en: Streamlining the diarization workflow with helper functions
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简化语音分离工作流的辅助函数
- en: 'The notebook defines several supporting functions to simplify the process of
    diarizing speech with Whisper and NeMo. These functions are instrumental in managing
    audio data, aligning transcriptions with speaker identities, and enhancing the
    workflow. The following is a concise description of each function:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本定义了几个辅助函数，以简化使用Whisper和NeMo进行话者分离的过程。这些函数在管理音频数据、将转录与说话者身份对齐以及优化工作流程方面起着关键作用。以下是每个函数的简要描述：
- en: '`create_config()`: Initializes and returns a configuration object, setting
    up essential parameters for the diarization process:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_config()`: 初始化并返回配置对象，设置话者分离过程所需的基本参数：'
- en: '[PRE1]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`get_word_ts_anchor()`: Determines the anchor timestamp for words, facilitating
    accurate alignment between spoken words and their timestamps in the audio:'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_word_ts_anchor()`: 确定单词的时间戳锚点，确保说话单词与其音频时间戳之间的准确对齐：'
- en: '[PRE2]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`get_words_speaker_mapping()`: Maps each word in the transcription to the corresponding
    speaker based on the diarization results, ensuring that every word is attributed
    to the correct speaker:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_words_speaker_mapping()`: 根据话者分离结果，将转录中的每个单词映射到相应的说话者，确保每个单词归属于正确的说话者：'
- en: '[PRE3]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`get_first_word_idx_of_sentence()`: Identifies the index of the first word
    in a sentence, crucial for processing sentences in the context of speaker attribution
    and alignment:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_first_word_idx_of_sentence()`: 找到句子中第一个单词的索引，对于在说话者归属和对齐的上下文中处理句子至关重要：'
- en: '[PRE4]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`get_last_word_idx_of_sentence()`: Finds the index of the last word in a sentence,
    aiding in delineating sentence boundaries within the transcribed text:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_last_word_idx_of_sentence()`: 找到句子中最后一个单词的索引，有助于在转录文本中划定句子边界：'
- en: '[PRE5]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`get_realigned_ws_mapping_with_punctuation()`: Adjusts the word-to-speaker
    mapping by considering punctuation, enhancing the accuracy of speaker attribution,
    especially in complex conversational scenarios:'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_realigned_ws_mapping_with_punctuation()`: 考虑标点符号调整单词到说话者的映射，提高在复杂对话场景中的说话者归属准确性：'
- en: '[PRE6]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`get_sentences_speaker_mapping()`: Generates a mapping of entire sentences
    to speakers, providing a higher-level view of speaker contributions throughout
    the audio:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_sentences_speaker_mapping()`: 生成整个句子与说话者的映射，提供说话者在音频中贡献的高层次视图：'
- en: '[PRE7]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`get_speaker_aware_transcript()`: Produces a transcript aware of speaker identities,
    integrating both the textual content and the speaker information into a cohesive
    format:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_speaker_aware_transcript()`: 生成考虑到说话者身份的转录，将文本内容和说话者信息整合为一致的格式：'
- en: '[PRE8]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`format_timestamp()`: Converts timestamps into a human-readable format, essential
    for annotating the transcript with precise timing information:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`format_timestamp()`: 将时间戳转换为人类可读的格式，便于为转录注释准确的时间信息：'
- en: '[PRE9]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`write_srt()`: Outputs the diarization results in the **SubRip Text** (**SRT**)
    format, suitable for subtitles or detailed analysis, including speaker labels
    and timestamps:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write_srt()`: 以**SubRip Text**（**SRT**）格式输出话者分离结果，适用于字幕或详细分析，包括说话者标签和时间戳：'
- en: '[PRE10]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`find_numeral_symbol_tokens()`: Identifies tokens within the transcription
    that represent numeral symbols, aiding in processing numerical data within text:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`find_numeral_symbol_tokens()`: 在转录文本中识别表示数字符号的标记，帮助处理文本中的数字数据：'
- en: '[PRE11]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`_get_next_start_timestamp()`: Calculates the start timestamp for the next
    word, ensuring continuity in the sequence of timestamps across a transcription:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`_get_next_start_timestamp()`: 计算下一个单词的开始时间戳，确保转录中时间戳序列的连续性：'
- en: '[PRE12]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`filter_missing_timestamps()`: Filters and corrects any missing or incomplete
    timestamps in transcription data, maintaining the integrity of temporal information:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_missing_timestamps()`: 过滤并修正转录数据中缺失或不完整的时间戳，保持时间信息的完整性：'
- en: '[PRE13]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`cleanup()`: Cleans up temporary files or directories created during diarization,
    ensuring a tidy working environment:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cleanup()`: 清理在话者分离过程中创建的临时文件或目录，确保工作环境整洁：'
- en: '[PRE14]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`process_language_arg()`: Processes the language argument to ensure compatibility
    with models, facilitating accurate transcription across different languages:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`process_language_arg()`: 处理语言参数，确保与模型兼容，促进不同语言间的准确转录：'
- en: '[PRE15]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`transcribe()`: Utilizes Whisper to transcribe audio into text, providing foundational
    textual data for the diarization process:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transcribe()`: 利用Whisper将音频转录为文本，为话者分离过程提供基础文本数据：'
- en: '[PRE16]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`transcribe_batched()`: Offers a batch processing capability to transcribe
    audio files, optimizing the transcription process for efficiency and scalability:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transcribe_batched()`：提供批处理能力以转录音频文件，从而优化转录过程的效率和可扩展性：'
- en: '[PRE17]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: These functions collectively form the notebook foundation of the diarization
    workflow, enabling seamless integration of Whisper’s transcription capabilities
    with NeMo’s advanced diarization features.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这些功能共同构成了笔记本的说话人分离工作流程基础，能够无缝集成Whisper的转录能力与NeMo的高级说话人分离功能。
- en: Separating music from speech using Demucs
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Demucs将音乐与语音分离
- en: As we explore the notebook, let’s focus on the preprocessing step, which is
    crucial for enhancing speech clarity before diarization. This section introduces
    **Demucs**, a deep-learning model for separating music source vocals from complex
    audio tracks.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索笔记本时，让我们关注预处理步骤，这对于在进行说话人分离之前提升语音清晰度至关重要。本节介绍了**Demucs**，一个用于将音乐源人声与复杂音轨分离的深度学习模型。
- en: 'Separating music from speech is essential, mainly when dealing with recordings
    containing background music or other non-speech elements. By extracting the vocal
    component, the diarization system can more effectively analyze and attribute speech
    to the correct speakers, as the spectral and temporal characteristics of their
    speech signals become more pronounced and less obscured by music:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将音乐与语音分离非常关键，尤其是在处理包含背景音乐或其他非语音元素的录音时。通过提取人声成分，分离系统可以更有效地分析并将语音归属给正确的说话人，因为其语音信号的频谱和时间特征变得更加明显，不会被音乐干扰：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Demucs operates by leveraging a neural network trained to distinguish between
    different audio sources within a mixture. When applied to an audio file, it can
    separate the vocal track from the instrumental, allowing subsequent tools such
    as Whisper and NeMo to process the speech without the interference of background
    music.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Demucs通过利用一个神经网络来区分混合音频中的不同音源来工作。当应用于音频文件时，它可以将人声轨道与伴奏乐器分开，从而使得后续工具（如Whisper和NeMo）能够在没有背景音乐干扰的情况下处理语音。
- en: This separation step is beneficial for the accuracy of speaker diarization and
    any downstream tasks that require clean speech input, such as transcription and
    speech recognition. By using Demucs as part of the preprocessing pipeline, the
    notebook ensures that the input to the diarization system is optimized for the
    best possible performance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分离步骤对说话人分离的准确性以及任何需要清晰语音输入的后续任务（如转录和语音识别）非常有帮助。通过将Demucs作为预处理管道的一部分，笔记本确保输入到分离系统的音频得到了优化，从而实现最佳性能。
- en: Transcribing audio using WhisperX
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用WhisperX进行音频转录
- en: 'The next step is to leverage WhisperX to transcribe the audio content. The
    transcription process involves processing the audio file through Whisper to generate
    a set of text segments, each accompanied by timestamps indicating when the segment
    was spoken:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是利用WhisperX来转录音频内容。转录过程包括将音频文件通过Whisper处理，生成一组文本段落，每个段落都有时间戳，指示该段落被说出的时间：
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This foundational step provides the textual content necessary for speaker diarization
    and further analysis. I hope you’ve noticed that both functions, `transcribe()`
    and `transcribe_batch()`, were previously defined in the notebook.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这一基础步骤提供了进行说话人分离和进一步分析所需的文本内容。我希望你注意到，`transcribe()`和`transcribe_batch()`这两个函数在笔记本中已经定义过了。
- en: Aligning the transcription with the original audio using Wav2Vec2
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Wav2Vec2对转录文本与原始音频进行对齐
- en: 'Following transcription, the notebook introduces the use of **Wav2Vec2** for
    forced alignment, a process that refines the alignment between the transcribed
    text and the original audio. Wav2Vec2, a large-scale neural network model, excels
    at learning representations of speech that are beneficial for speech recognition
    and alignment tasks. By employing Wav2Vec2, we demonstrate how to fine-tune the
    alignment of transcription segments with the audio signal, ensuring that the text
    is accurately synchronized with the spoken words:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在转录之后，笔记本介绍了如何使用**Wav2Vec2**进行强制对齐，这是一个将转录文本与原始音频进行对齐的过程。Wav2Vec2是一个大型神经网络模型，擅长学习有助于语音识别和对齐任务的语音表示。通过使用Wav2Vec2，我们演示了如何微调转录段落与音频信号的对齐，确保文本与所说的话语准确同步：
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This alignment is essential for diarization, as it allows for a more precise
    segmentation of audio based on speaker changes. The combined output of Whisper
    and Wav2Vec2 offers a fully aligned transcription, which is instrumental for tasks
    such as speaker diarization, sentiment analysis, and language identification.
    This section in the notebook emphasizes that if a Wav2Vec2 model is not available
    for a specific language, the word timestamps generated by Whisper will be utilized,
    showcasing the flexibility of the approach.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这一对齐对于分离至关重要，因为它允许根据说话人的变化进行更精确的音频分割。Whisper 和 Wav2Vec2 的结合输出提供了一个完全对齐的转录，这对于说话人分离、情感分析和语言识别等任务非常有帮助。本节强调，如果某个特定语言没有可用的
    Wav2Vec2 模型，则将使用 Whisper 生成的单词时间戳，展示了该方法的灵活性。
- en: By integrating Whisper’s transcription capabilities with Wav2Vec2’s alignment
    precision, we set the stage for accurate speaker diarization, enhancing the overall
    quality and reliability of the diarization process.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 Whisper 的转录能力与 Wav2Vec2 的对齐精度结合，我们为准确的说话人分离奠定了基础，提高了分离过程的整体质量和可靠性。
- en: Using NeMo’s MSDD model for speaker diarization
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 NeMo 的 MSDD 模型进行说话人分离
- en: 'At the core of the notebook, the focus shifts toward the intricate process
    of speaker diarization, leveraging the advanced capabilities of NVIDIA’s NeMo
    MSDD. This section in the notebook is pivotal, as it addresses distinguishing
    between different speakers within an audio signal, a task essential for accurately
    attributing speech segments to individual speakers:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本的核心部分，重点转向了复杂的说话人分离过程，利用了 NVIDIA NeMo MSDD 的先进能力。该部分非常关键，因为它解决了在音频信号中区分不同说话人的问题，这对于将语音片段准确归属到个人说话人至关重要：
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The NeMo MSDD model stands at the forefront of this process, employing a sophisticated
    approach to diarization that considers multiple temporal resolutions of speaker
    embeddings. This multiscale strategy enhances the model’s ability to discern between
    speakers, even in challenging audio environments with overlapping speech or background
    noise.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: NeMo MSDD 模型处于这一过程的最前沿，采用了一种复杂的分离方法，考虑了说话人嵌入的多时间分辨率。这种多尺度策略提高了模型在挑战性音频环境中（如重叠语音或背景噪音）区分说话人的能力。
- en: Mapping speakers to sentences according to timestamps
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根据时间戳将说话人映射到句子
- en: 'After successfully separating speech from music, transcribing the audio using
    Whisper, and performing speaker diarization with the NeMo MSDD model, the next
    challenge is to accurately map each sentence in the transcription to its corresponding
    speaker. This involves analyzing the timestamps associated with each word or segment
    in the transcription and the speaker labels assigned during the diarization process:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功地将语音与音乐分离、使用 Whisper 转录音频并通过 NeMo MSDD 模型进行说话人分离后，下一个挑战是将转录中的每个句子准确地映射到相应的说话人。这涉及分析转录中每个单词或片段的时间戳以及在分离过程中分配的说话人标签：
- en: '[PRE22]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The preceding code ensures that each sentence in the transcription is correctly
    attributed to a speaker, considering the start and end times of spoken segments.
    This meticulous mapping is crucial for applications where understanding conversation
    dynamics, such as who said what and when, is essential. It enables a more granular
    analysis of dialogues, meetings, interviews, and audio content involving multiple
    speakers.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码确保了转录中的每个句子都正确地归属于某个说话人，考虑了口语片段的开始和结束时间。这一细致的映射对于理解对话动态（例如谁在何时说了什么）至关重要。它使得对多说话人对话、会议、访谈和音频内容的分析更加细致。
- en: Enhancing speaker attribution with punctuation-based realignment
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过基于标点符号的重新对齐提升说话人归属
- en: 'The following code snippet demonstrates how punctuation determines the predominant
    speaker for each sentence in a transcription. It employs a pre-trained punctuation
    model, `kredor/punctuate-all`, to predict punctuation marks for the transcribed
    words. The code then processes the words and their predicted punctuation, handling
    exceptional cases such as acronyms (e.g., USA) to avoid incorrect punctuation.
    This approach ensures that the speaker attribution remains consistent within each
    sentence, even in the presence of background comments or brief interjections from
    other speakers. This is particularly useful in scenarios where the transcription
    may not indicate speaker changes, such as when a speaker’s utterance is interrupted
    or overlapped by another’s. By analyzing the distribution of speaker labels for
    each word in a sentence, the code can assign a consistent speaker label to the
    entire sentence, enhancing the coherence of the diarization output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段演示了标点符号如何决定每个句子的主要说话人。它使用一个预训练的标点符号模型`kredor/punctuate-all`，为转录的单词预测标点符号。然后，代码处理这些单词及其预测的标点符号，处理一些特殊情况，例如首字母缩略词（如USA），以避免错误的标点符号。这种方法确保即使在其他说话人的背景评论或简短插话的情况下，每个句子的说话人归属也能保持一致。这在转录未指示说话人变化的情况下特别有用，例如当一个说话人的发言被另一个人的话语打断或重叠时。通过分析每个句子中每个单词的说话人标签分布，代码能够为整个句子分配一个一致的说话人标签，从而增强对话分段输出的连贯性：
- en: '[PRE23]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This approach also addresses instances where background comments or brief interjections
    occur while a primary speaker delivers a monologue. The code effectively attributes
    the main body of speech to the dominant speaker, disregarding sporadic remarks
    from others. This results in a more accurate and reliable mapping of speech segments
    to the appropriate speakers, ensuring that the diarization process reflects the
    actual structure of the conversation.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法还解决了在主要说话人进行独白时，背景评论或简短插话发生的情况。代码有效地将主要的讲话内容归属于主说话人，而忽略其他人的零星评论。这使得语音片段与相应说话人的映射更加准确可靠，确保分段过程反映了对话的实际结构。
- en: Finalizing the diarization process
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完成分段过程
- en: 'In this final section, the code performs essential cleanup tasks, exports the
    diarization results for further use, and replaces speaker IDs with their corresponding
    names. The main steps include the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一部分，代码执行必要的清理任务，导出分段结果以供进一步使用，并将说话人ID替换为对应的名字。主要步骤包括以下内容：
- en: '`get_speaker_aware_transcript` function generates a transcript incorporating
    textual content and speaker information. This transcript is then saved as a file
    with the same name as the input audio file but with a `.``txt` extension:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`get_speaker_aware_transcript` 函数生成一个包含文本内容和说话人信息的转录本。该转录本随后被保存为与输入音频文件同名的文件，但扩展名为`.txt`：'
- en: '[PRE24]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`write_srt function` is employed to export the diarization results in the SRT
    format. This format is commonly used for subtitles and includes speaker labels
    and precise timestamps for each utterance. The SRT file is saved with the same
    name as the input audio file but with a `.``srt` extension:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`write_srt function` 用于将分段结果导出为SRT格式。此格式通常用于字幕，并包括每个发言的说话人标签和精确的时间戳。SRT文件将以与输入音频文件相同的名称保存，但扩展名为`.srt`：'
- en: '[PRE25]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '**Cleaning up temporary files**: The cleanup function removes any temporary
    files or directories created during the diarization process. This step ensures
    a clean and organized working environment, freeing storage space and maintaining
    system efficiency:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**清理临时文件**：清理功能删除在分段过程中创建的任何临时文件或目录。这一步确保了一个干净、井然有序的工作环境，释放了存储空间并保持系统效率：'
- en: '[PRE26]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`Speaker 0`, `Speaker 1`, and `Speaker 2`) with the actual names of the speakers:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Speaker 0`、`Speaker 1`和`Speaker 2`替换为实际说话人的名字：
- en: '[PRE27]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: By completing these final steps, the diarization process is concluded, and the
    results are made available for further analysis, post-processing, or integration
    with other tools and workflows. The exported speaker-aware transcript, SRT file,
    and transcript with mapped speaker names provide valuable insights into the content
    and structure of the audio recording, enabling a wide range of applications, such
    as content analysis, speaker identification, and subtitle generation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成这些最终步骤，语音分离过程得以完成，结果可以用于进一步分析、后处理或与其他工具和工作流程的集成。导出的包含说话者信息的转录、SRT文件和映射了说话者名称的转录提供了对音频录音内容和结构的宝贵洞见，为广泛应用提供了可能，如内容分析、说话者识别和字幕生成。
- en: After diving into the notebook, we uncovered a treasure trove of insights into
    the nuanced world of speech diarization using cutting-edge AI tools. The notebook
    was a hands-on guide, meticulously walking us through separating and transcribing
    speech from complex audio files.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究该笔记本后，我们发现了一个关于使用前沿AI工具进行语音分离的宝贵宝藏。这本笔记本是一本实践指南，详细指导我们如何从复杂的音频文件中分离和转录语音。
- en: One of the first lessons was setting up the right environment. The notebook
    emphasized the need to install specific dependencies, such as Whisper and NeMo,
    which were pivotal for the tasks. This step was crucial, laying the groundwork
    for all subsequent operations.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个教训是如何设置正确的环境。该笔记本强调了安装特定依赖项的必要性，如Whisper和NeMo，它们对于任务的执行至关重要。这一步是基础，为所有后续操作奠定了基础。
- en: As we delved deeper, we learned about the utility of helper functions. These
    functions were the unsung heroes that streamlined the workflow, from processing
    audio files to handling timestamps and cleaning up resources. They exemplified
    the principle of writing clean, reusable code that significantly reduced the project’s
    complexity.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深入学习，我们了解了辅助函数的实用性。这些函数是默默奉献的英雄，它们简化了工作流程，从处理音频文件到处理时间戳和清理资源。它们体现了编写简洁、可重用代码的原则，大大降低了项目的复杂性。
- en: The notebook also introduced us to separating music from speech using Demucs.
    This step was a testament to the power of preprocessing in enhancing the accuracy
    of diarization. By isolating vocals, we focused on speech’s spectral and temporal
    characteristics, which are essential for identifying different speakers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本还介绍了使用Demucs将音乐从语音中分离的技术。这个步骤展示了预处理在提高分离准确性方面的强大作用。通过隔离人声，我们专注于语音的频谱和时间特征，这对于识别不同的说话者至关重要。
- en: Another key takeaway was the integration of multiple models to achieve better
    results. The notebook showcased how Whisper was used for transcription and Wav2Vec2
    for aligning the transcription with the original audio. This synergy between models
    was a brilliant example of how combining different AI tools leads to a more robust
    solution.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键收获是集成多个模型以获得更好的结果。该笔记本展示了如何使用Whisper进行转录，使用Wav2Vec2将转录与原始音频对齐。模型之间的协同作用是一个出色的例子，展示了如何结合不同的AI工具来实现更强大的解决方案。
- en: Mapping speakers into sentences and realigning speech segments using punctuation
    was particularly enlightening. It demonstrated the intricacies of diarization
    and the need for attention to detail to ensure that each speaker was accurately
    represented in the transcript.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 将说话者映射到句子并通过标点符号重新对齐语音片段的过程尤其令人启发。它展示了语音分离的复杂性以及对细节的关注，确保每个说话者在转录中都得到准确表达。
- en: In essence, the notebook was a masterclass in the practical application of AI
    for speech diarization. It not only taught us the technical steps involved but
    also imparted broader lessons on the importance of preprocessing, the power of
    combining different AI models, and the need for meticulous post-processing to
    ensure the integrity of the final output.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，这本笔记本是一个关于AI在语音分离应用中的实用技巧的高级教程。它不仅教会了我们涉及的技术步骤，还传授了关于预处理重要性、结合不同AI模型的力量，以及细致后处理以确保最终输出完整性的更广泛教训。
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we embarked on an exciting exploration of the advanced voice
    capabilities of OpenAI’s Whisper. We delved into powerful techniques that enhance
    Whisper’s performance, such as quantization, and uncovered its potential for speaker
    diarization and real-time speech recognition.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们开始了对OpenAI Whisper先进语音能力的激动人心的探索。我们深入研究了增强Whisper性能的强大技术，例如量化，并发现了它在说话者分离和实时语音识别中的潜力。
- en: We augmented Whisper with speaker diarization capabilities, allowing it to identify
    and attribute speech segments to different speakers within an audio recording.
    By integrating Whisper with the NVIDIA NeMo framework, we discovered how to perform
    accurate speaker diarization, opening new possibilities for analyzing multispeaker
    conversations. Our hands-on experience with WhisperX and NVIDIA NeMo showcased
    the power of combining Whisper’s transcription capabilities with advanced diarization
    techniques.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为Whisper增添了说话人分离功能，使其能够识别并将音频录音中的语音片段归属到不同的说话人。通过将Whisper与NVIDIA NeMo框架整合，我们学会了如何执行精准的说话人分离，为分析多说话人对话开辟了新天地。我们与WhisperX和NVIDIA
    NeMo的实操经验展示了将Whisper的转录能力与先进的说话人分离技术结合的强大潜力。
- en: Throughout the chapter, we acquired a solid understanding of advanced techniques
    to optimize Whisper’s performance and expand its capabilities with speaker diarization.
    The hands-on coding examples and practical insights equipped us with the knowledge
    and skills to apply these techniques in our projects, pushing the boundaries of
    what is possible with Whisper.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入理解了优化Whisper性能和通过说话人分离扩展其功能的高级技巧。通过动手编码示例和实用的见解，我们掌握了应用这些技巧的知识和技能，推动了Whisper的可能性边界。
- en: As we conclude this chapter, we will look ahead to [*Chapter 9*](B21020_09.xhtml#_idTextAnchor207),
    *Harnessing Whisper for Personalized Voice Synthesis*. In that chapter, we will
    gain the knowledge and skills to preprocess audio data, fine-tune voice models,
    and generate realistic speech using a personal voice synthesis model. The hands-on
    coding examples and practical insights will empower you to apply these techniques
    in your projects, pushing the boundaries of what is possible with personalized
    voice synthesis.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们结束本章时，我们将展望[*第9章*](B21020_09.xhtml#_idTextAnchor207)，*利用Whisper进行个性化语音合成*。在那一章中，我们将获得预处理音频数据、微调语音模型以及使用个人语音合成模型生成逼真语音的知识和技能。动手编码示例和实用的见解将使你能够将这些技巧应用到你的项目中，推动个性化语音合成技术的边界。
- en: Join me as we continue our journey with Whisper, ready to embrace the exciting
    possibilities in the rapidly evolving world of voice-synthesis technology.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随我继续与Whisper同行，准备迎接语音合成技术这一迅速发展的领域中的激动人心的可能性。
