- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: How to Use Decision Trees to Enhance K-Means Clustering
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何使用决策树增强K均值聚类
- en: This chapter addresses two critical issues. First, we will explore how to implement
    k-means clustering with dataset volumes that exceed the capacity of the given
    algorithm. Second, we will implement decision trees that verify the results of
    an ML algorithm that surpasses human analytic capacity. We will also explore the
    use of random forests.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了两个关键问题。首先，我们将探索如何在数据集超出给定算法处理能力时实现k-means聚类。其次，我们将实现决策树，验证超越人类分析能力的机器学习算法的结果。我们还将探索随机森林的使用。
- en: When facing such difficult problems, choosing the right model for the task often
    proves to be the most difficult task in ML. When we are given an unfamiliar set
    of features to represent, it can be a somewhat puzzling prospect. Then we have
    to get our hands dirty and try different models. An efficient estimator requires
    good datasets, which might change the course of the project.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在面对这样复杂的问题时，选择适合任务的模型往往是机器学习中最具挑战性的任务。当我们面对一组陌生的特征来表示时，这可能会让人感到困惑。然后我们必须亲自动手，尝试不同的模型。一个高效的估算器需要良好的数据集，这可能会改变项目的方向。
- en: This chapter builds on the k-means clustering (or KMC) program developed in
    *Chapter 4*, *Optimizing Your Solutions with K-Means Clustering*. The issue of
    large datasets is addressed. This exploration will lead us into the world of the
    law of large numbers (LLN), the central limit theorem (CLT), the Monte Carlo estimator,
    decision trees, and random forests.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章基于*第4章*《使用K均值聚类优化你的解决方案》中开发的k-means聚类（或KMC）程序。我们将解决大数据集的问题。这次探索将带领我们进入大数法则（LLN）、中心极限定理（CLT）、蒙特卡罗估计器、决策树和随机森林的世界。
- en: Human intervention in a process such as the one described in this chapter is
    not only unnecessary, but impossible. Not only does machine intelligence surpass
    humans in many cases, but the complexity of a given problem itself often surpasses
    human ability, due to the complex and ever-changing nature of real-life systems.
    Thanks to machine intelligence, humans can deal with increasing amounts of data
    that would otherwise be impossible to manage.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章所描述的过程中，人工干预不仅是多余的，而且是不可能的。机器智能在许多情况下超越了人类，而且由于现实生活系统的复杂性和不断变化的特性，给定问题的复杂性往往超出了人类的能力。得益于机器智能，人类可以处理越来越多的数据，否则这些数据将是无法管理的。
- en: With our toolkit, we will build a solution to analyze the results of an algorithm
    without human intervention.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的工具包，我们将构建一个无需人工干预的算法结果分析解决方案。
- en: 'This chapter covers the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涉及以下主题：
- en: Unsupervised learning with KMC
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用KMC进行无监督学习
- en: Determining if AI must or must not be used
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定是否必须使用AI
- en: Data volume issues
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据量问题
- en: Defining the NP-hard characteristic of KMC
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义KMC的NP难度特性
- en: Random sampling concerning LLN, CLT, and the Monte Carlo estimator
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于大数法则（LLN）、中心极限定理（CLT）和蒙特卡罗估计器的随机抽样
- en: Shuffling a training dataset
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打乱训练数据集
- en: Supervised learning with decision trees and random forests
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树和随机森林进行有监督学习
- en: Chaining KMC to decision trees
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将KMC与决策树连接起来
- en: This chapter begins with unsupervised learning with KMC. We will explore methods
    to avoid running large datasets through random sampling. The output of the KMC
    algorithm will provide the labels for the supervised decision tree algorithm.
    The decision tree will verify the results of the KMC process, a task no human
    could do with large volumes of data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章从无监督学习与KMC开始。我们将探索避免通过随机抽样运行大数据集的方法。KMC算法的输出将为有监督的决策树算法提供标签。决策树将验证KMC过程的结果，这是人类在处理大量数据时无法做到的任务。
- en: All the Python programs and files in this chapter are available at [https://github.com/PacktPublishing/Artificial-Intelligence-By-Example-Second-Edition/tree/master/CH05](https://github.com/PacktPublishing/Artificial-Intelligence-By-Example-Second-Edition/tree/master/CH0).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有Python程序和文件都可以在[https://github.com/PacktPublishing/Artificial-Intelligence-By-Example-Second-Edition/tree/master/CH05](https://github.com/PacktPublishing/Artificial-Intelligence-By-Example-Second-Edition/tree/master/CH05)中找到。
- en: There is also a Jupyter notebook named `COLAB_CH05.ipynb` that contains all
    of the Python programs in one run. You can upload it directly to Google Colaboratory
    ([https://colab.research.google.com/](https://colab.research.google.com/)) using
    your Google Account.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个名为`COLAB_CH05.ipynb`的Jupyter notebook，其中包含所有Python程序，可以一次性运行。你可以直接将其上传到Google
    Colaboratory（[https://colab.research.google.com/](https://colab.research.google.com/)），使用你的Google帐户进行操作。
- en: Unsupervised learning with KMC with large datasets
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用KMC进行无监督学习和大数据集处理
- en: KMC takes unlabeled data and forms clusters of data points. The names (integers)
    of these clusters provide a basis to then run a supervised learning algorithm
    such as a decision tree.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: KMC使用未标注数据并形成数据点的聚类。这些聚类的名称（整数）为接下来运行监督学习算法（如决策树）提供了基础。
- en: In this section, we will see how to use KMC with large datasets.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将学习如何在大数据集上使用KMC。
- en: When facing a project with large unlabeled datasets, the first step consists
    of evaluating if machine learning will be feasible or not. Trying to avoid AI
    in a book on AI may seem paradoxical. However, in AI, as in real life, you should
    use the right tools at the right time. If AI is not necessary to solve a problem,
    do not use it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当面对一个包含大量未标注数据集的项目时，第一步是评估机器学习是否可行。在一本关于人工智能的书中避免使用人工智能可能看起来有些自相矛盾。然而，在人工智能领域，就像在现实生活中一样，你应该在合适的时机使用合适的工具。如果人工智能不是解决问题的必要手段，就不要使用它。
- en: Use a **proof of concept** (**POC**) approach to see if a given AI project is
    possible or not. A POC costs much less than the project itself and helps to build
    a team that believes in the outcome. Or, the POC might show that it is too risky
    to go forward with an ML solution. Intractable problems exist. It's best to avoid
    spending months on something that will not work.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用**概念验证**（**POC**）方法来验证某个AI项目是否可行。POC的成本远低于整个项目，且有助于建立一个相信最终成果的团队。或者，POC可能会显示继续推进机器学习解决方案太过冒险。不可解的问题是存在的，最好避免花费数月的时间在一个无法成功的方案上。
- en: The first step is exploring the data volume and the ML estimator model that
    will be used.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是探索数据量和将要使用的机器学习估算模型。
- en: If the POC proves that a particular ML algorithm will solve the problem at hand,
    the next thing to do is to address data volume issues. The POC shows that the
    model works on a sample dataset. Now, the implementation process can begin.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果POC证明某个特定的机器学习算法能够解决眼前的问题，接下来的步骤就是解决数据量问题。POC展示了模型在样本数据集上的有效性。现在，实施过程可以开始了。
- en: Anybody who has run a machine learning algorithm with a large dataset on a laptop
    knows that it takes some time for a machine learning program to train and test
    these samples. A machine learning program or a deep learning convolutional neural
    network consumes a large amount of machine power. Even if you run an ANN using
    a **GPU** (short for **graphics processing unit**) hoping to get better performance
    than with CPUs, it still takes a lot of time for the training process to run through
    all the learning epochs. An epoch means that we have tried one set of weights,
    for example, to measure the accuracy of the result. If the accuracy is not sufficient,
    we run another epoch, trying other weights until the accuracy is sufficient.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 任何在笔记本电脑上运行过大数据集机器学习算法的人都知道，机器学习程序需要一定的时间来训练和测试这些样本。一个机器学习程序或深度学习卷积神经网络需要消耗大量的计算机算力。即便你使用**GPU**（即**图形处理单元**）运行人工神经网络，期望能比CPU获得更好的性能，它仍然需要大量时间来完成所有学习周期。一个周期（epoch）意味着我们已经尝试了一组权重，例如，用来衡量结果的准确度。如果准确度不够，我们就会运行另一个周期，尝试其他权重，直到准确度足够。
- en: If you go on and you want to train your program on datasets exceeding 1,000,000
    data points, for example, you will consume significant local machine power.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你继续进行并希望在超过1,000,000个数据点的数据集上训练程序，例如，你将消耗大量本地计算机资源。
- en: Suppose you need to use a KMC algorithm in a corporation with hundreds of millions
    to billions of records of data coming from multiple SQL Server instances, Oracle
    databases, and a big data source. For example, suppose that you are working for
    the phone operating activity of a leading phone company. You must apply a KMC
    program to the duration of phone calls in locations around the world over a year.
    That represents millions of records per day, adding up to billions of records
    in a year.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要在一个拥有数亿到数十亿条记录的公司中使用KMC算法，这些数据来自多个SQL Server实例、Oracle数据库和大数据源。例如，假设你正在为一家领先的手机公司做电话运营活动。你必须应用KMC程序来分析一年中全球各地的电话通话时长。这代表着每天有数百万条记录，一年累计达到数十亿条记录。
- en: A machine learning KMC training program running billions of records will consume
    too much CPU/GPU and take too much time even if it works. On top of that, a billion
    records might only represent an insufficient amount of features. Adding more features
    will dramatically increase the size of such a dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 KMC 训练程序能够运行数十亿条记录，消耗的 CPU/GPU 也会非常大，并且即使成功运行也会耗费大量时间。更重要的是，十亿条记录可能只代表了不足够的特征。添加更多的特征将大大增加数据集的大小。
- en: The question now is to find out if KMC will even work with a dataset this size.
    A KMC problem is **NP-hard**. The **P** stands for **polynomial** and the **N**
    for **non-deterministic**.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，KMC 是否能够处理如此大的数据集。KMC 问题是 **NP-hard**。**P** 代表 **多项式**，**N** 代表 **非确定性**。
- en: The solution to our volume problem requires some theoretical considerations.
    We need to identify the difficulty of the problems we are facing.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 解决我们体积问题需要一些理论上的考虑。我们需要识别我们所面临问题的难度。
- en: Identifying the difficulty of the problem
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 识别问题的难度
- en: We first need to understand what level of difficulty we are dealing with. One
    of the concepts that come in handy is NP-hard.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要理解我们正在处理的难度级别。一个有用的概念是 NP-hard。
- en: NP-hard – the meaning of P
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NP-hard——P 的含义
- en: The P in NP-hard means that the time to solve or verify the solution of a P
    problem is polynomial (poly=many, nomial=terms). For example, *x*³ is a polynomial.
    The N means that the problem is non-deterministic.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: NP-hard 中的 P 意味着解决或验证 P 问题的时间是多项式的（poly=多，nomial=项）。例如，*x*³ 是一个多项式。N 表示问题是非确定性的。
- en: 'Once *x* is known, then *x*³ will be computed. For *x* = 3,000,000,000 and
    only 3 elementary calculations, this adds up to:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 *x* 已知，则 *x*³ 会被计算出来。对于 *x* = 3,000,000,000，只需要 3 次基本计算，结果是：
- en: log *x*³ = 28.43
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: log *x*³ = 28.43
- en: It will take 10^(28.43) calculations to compute this particular problem.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 计算这个特定问题需要 10^(28.43) 次计算。
- en: 'It seems scary, but it isn''t that scary for two reasons:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来很可怕，但有两个原因它并不那么可怕：
- en: In the world of big data, the number can be subject to large-scale randomized
    sampling.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大数据的世界里，这个数字可以通过大规模的随机抽样来处理。
- en: KMC can be trained in mini-batches (subsets of the dataset) to speed up computations.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KMC 可以通过小批量（数据集的子集）训练来加速计算。
- en: Polynomial time means that this time will be more or less proportional to the
    size of the input. Even if the time it takes to train the KMC algorithm remains
    a bit fuzzy, as long as the time it will take to verify the solution remains proportional
    thanks to the batch size of the input, the problem remains a polynomial.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式时间意味着此时间大致与输入的大小成正比。即使训练 KMC 算法所需的时间仍然有些模糊，只要验证解决方案所需的时间因输入的批量大小而保持成正比，问题仍然是多项式的。
- en: An exponential algorithm increases with the amount of data, not the number of
    calculations. An exponential function of this example would be *f*(*x*) = 3^x
    = 3^(3,000,000,000) calculations. Such functions can often be broken down into
    multiple classical algorithms. Functions of this type exist in the corporate world,
    but they are out of the scope of this book.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 指数算法随着数据量增加，而不是计算次数增加。例如，这个例子的指数函数是 *f*(*x*) = 3^x = 3^(3,000,000,000) 次计算。这样的函数通常可以拆分为多个经典算法。这类函数在企业界存在，但它们超出了本书的范围。
- en: NP-hard – the meaning of non-deterministic
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NP-hard——非确定性含义
- en: Non-deterministic problems require a heuristic approach, which implies some
    form of heuristics, such as a trial and error approach. We try a set of weights,
    for example, evaluate the result, and then go on until we find a satisfactory
    solution.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 非确定性问题需要启发式方法，这意味着某种形式的启发式，例如试错法。我们试一组权重，例如，评估结果，然后继续，直到找到令人满意的解决方案。
- en: The meaning of hard
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: “困难”的含义
- en: NP-hard can be transposed into an NP problem with some optimization. This means
    that NP-hard is as hard or harder than an NP problem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: NP-hard 可以通过一些优化转化为 NP 问题。这意味着 NP-hard 与 NP 问题一样难，甚至更难。
- en: For example, we can use batches to control the size of the input, the calculation
    time, and the size of the outputs. That way, we can bring an NP-hard problem down
    to an NP problem.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用批量来控制输入的大小、计算时间和输出的大小。这样，我们可以将 NP-hard 问题降为 NP 问题。
- en: One way of creating batches to avoid running an algorithm on a dataset that
    will prove too large for it is to use random sampling.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 创建批量以避免在数据集过大时运行算法的一种方法是使用随机抽样。
- en: Implementing random sampling with mini-batches
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现带有小批量的随机抽样
- en: A large portion of machine learning and deep learning contains random sampling
    in various forms. In this case, a training set of billions of elements will prove
    difficult, if not impossible, to implement without random sampling.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和深度学习的一个重要部分包含了各种形式的随机抽样。在这种情况下，训练集如果包含数十亿个元素，使用随机抽样将是不可或缺的，否则实现起来将极为困难，甚至不可能。
- en: 'Random sampling is used in many methods: Monte Carlo, stochastic gradient descent,
    random forests, and many algorithms. No matter what name the sampling takes, they
    share common concepts to various degrees, depending on the size of the dataset.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 随机抽样在许多方法中都有应用：蒙特卡洛法、随机梯度下降、随机森林以及许多算法。无论采样方法的名称是什么，它们都有共同的概念，具体程度取决于数据集的大小。
- en: Random sampling on large datasets can produce good results, but it requires
    relying on the LLN, which we will explore in the next section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据集上进行随机抽样可以产生良好的结果，但它需要依赖大数法则（LLN），我们将在下一部分进行探讨。
- en: Using the LLN
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用大数法则（LLN）
- en: In probability, the LLN states that when dealing with very large volumes of
    data, significant samples can be effective enough to represent the whole dataset.
    For example, we are all familiar with polls that use small datasets.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率论中，大数法则（LLN）指出，当处理大量数据时，重要的样本足够有效，可以代表整个数据集。例如，我们都熟悉使用小数据集的民意调查。
- en: This principle, like all principles, has its merits and limits. But whatever
    the limitations, this law applies to everyday machine learning algorithms.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这一原则，和所有原则一样，有其优点和局限性。但无论局限性如何，这一法则适用于日常的机器学习算法。
- en: In machine learning, sampling resembles polling. A smaller number of individuals
    represent a larger overall dataset.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，抽样类似于民意调查。较少的个体代表了更大的总体数据集。
- en: 'Sampling mini-batches and averaging them can prove as efficient as calculating
    the whole dataset as long as a scientific method is applied:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对小批量进行抽样并求其平均值，可以与计算整个数据集的效率相当，只要应用了科学方法：
- en: Training with mini-batches or subsets of data
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用小批量或数据子集进行训练
- en: Using an estimator in one form or another to measure the progression of the training
    session until a goal has been reached
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用某种形式的估计器来衡量训练过程的进展，直到达成目标为止
- en: You may be surprised to read "until a goal has been reached" and not "until
    the optimal solution has been reached."
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会惊讶地看到“直到达成目标”为止，而不是“直到达到最优解”为止。
- en: The optimal solution may not represent the best solution. All the features and
    all the parameters are often not expressed. Finding a good solution will often
    be enough to classify or predict efficiently.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最优解可能并不代表最佳解。所有特征和所有参数通常并未完全表达。找到一个好的解决方案通常就足以有效地进行分类或预测。
- en: The LLN explains why random functions are widely used in machine learning and
    deep learning. Random samples provide efficient results if they respect the CLT.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 大数法则（LLN）解释了为什么随机函数在机器学习和深度学习中被广泛使用。如果随机样本遵循中心极限定理（CLT），它们将提供高效的结果。
- en: The CLT
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 中心极限定理（CLT）
- en: The LLN applied to the example of the KMC project must provide a reasonable
    set of centroids using random sampling. If the centroids are correct, then the
    random sample is reliable.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 大数法则（LLN）应用于KMC项目示例时，必须通过随机抽样提供一个合理的重心集合。如果重心正确，那么随机样本就是可靠的。
- en: A centroid is the geometrical center of a set of datasets, as explained in *Chapter
    4*, *Optimizing Your Solutions with K-Means Clustering*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 重心是数据集的几何中心，正如*第4章*中所解释的，*使用K-Means聚类优化你的解决方案*。
- en: 'This approach can now be extended to the CLT, which states that when training
    a large dataset, a subset of mini-batch samples can be sufficient. The following
    two conditions define the main properties of the CLT:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法现在可以扩展到中心极限定理（CLT），该定理指出，当训练大型数据集时，使用小批量样本的子集就足够了。以下两个条件定义了中心极限定理（CLT）的主要特性：
- en: The variance between the data points of the subset (mini-batch) remains reasonable.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子集（小批量）数据点之间的方差保持在合理范围内。
- en: The normal distribution pattern with mini-batch variances remains close to the
    variance of the whole dataset.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有小批量方差的正态分布模式接近整个数据集的方差。
- en: A Monte Carlo estimator, for example, can provide a good basis to see if the
    samples respect the CLT.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，蒙特卡洛估计器可以提供一个良好的基础，判断样本是否符合中心极限定理（CLT）。
- en: Using a Monte Carlo estimator
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用蒙特卡洛估计器
- en: The name Monte Carlo comes from the casinos in Monte Carlo and gambling. Gambling
    represents an excellent memoryless random example. No matter what happens before
    a gambler plays, prior knowledge provides no insight. For example, the gambler
    plays 10 games, losing some and winning some, creating a distribution of probabilities.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: “蒙特卡罗”这个名字来源于蒙特卡罗的赌场和赌博。赌博是一个很好的无记忆随机例子。无论赌徒在玩之前发生什么，先前的知识都不会提供任何洞察。例如，赌徒玩了10局，输了几局，赢了几局，形成了一个概率分布。
- en: The sum of the distribution of *f*(*x*) is computed. Then random samples are
    extracted from a dataset, for example, *x*[1], *x*[2], *x*[3],..., *x*[n].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 计算*F*（*x*）的分布总和。然后从数据集中提取随机样本，例如，*x*[1]、*x*[2]、*x*[3]，...，*x*[n]。
- en: '*f*(*x*) can be estimated through the following equation:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*（*x*）可以通过以下方程进行估算：'
- en: '![](img/B15438_05_001.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_05_001.png)'
- en: The estimator ![](img/B15438_05_002.png) represents the average of the result
    of the predictions of a KMC algorithm or any implemented model.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 估计量 ![](img/B15438_05_002.png) 代表了KMC算法或任何已实现模型的预测结果的平均值。
- en: We have seen that a sample of a dataset can represent a full dataset, just as
    a group of people can represent a population when polling for elections, for example.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，数据集的一个样本可以代表完整的数据集，就像选举时，一群人可以代表整个选民群体一样。
- en: Knowing that we can safely use random samples, just like in polling a population
    for elections, we can now process a full large dataset directly, or preferably
    with random samples.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们可以放心地使用随机样本，就像在选举时对选民群体进行民意调查一样，现在我们可以直接处理整个大数据集，或者最好是使用随机样本。
- en: Trying to train the full training dataset
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尝试训练完整的训练数据集
- en: 'In *Chapter 4*, *Optimizing Your Solutions with K-Means Clustering*, a KMC
    algorithm with a six-cluster configuration produced six centroids (geometric centers),
    as shown here:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章*，《通过K均值聚类优化解决方案》中，采用六个聚类配置的KMC算法生成了六个质心（几何中心），如下所示：
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/02/KMeansClusters.jpg](img/B15438_05_01.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/02/KMeansClusters.jpg](img/B15438_05_01.png)'
- en: 'Figure 5.1: Six centroids'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：六个质心
- en: The problem is now how to avoid costly machine resources to train this KMC dataset
    when dealing with large datasets. The solution is to take random samples from
    the dataset in the same way polling is done on a population for elections, for
    example.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，如何避免使用昂贵的机器资源来训练这个KMC数据集，特别是在处理大数据集时。解决方案是像选举时对选民群体进行民意调查一样，从数据集中提取随机样本。
- en: Training a random sample of the training dataset
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练训练数据集的一个随机样本
- en: The `sampling/k-means_clustering_minibatch.py` program provides a way to verify
    the mini-batch solution.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`sampling/k-means_clustering_minibatch.py` 程序提供了一种验证迷你批次解决方案的方法。'
- en: 'The program begins by loading the data in the following lines of code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 程序首先通过以下代码加载数据：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Loading the dataset might create two problems:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集可能会出现两个问题：
- en: '**The dataset is too large to be loaded in the first place.** In this case,
    load the datasets batch by batch. Using this method, you can test the model on
    many batches to fine-tune your solution.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集太大，无法一次性加载。** 在这种情况下，按批次加载数据集。使用这种方法，你可以在多个批次上测试模型，以微调解决方案。'
- en: '**The dataset can be loaded, but the KMC algorithm chosen cannot absorb the
    volume of data.** A good choice for the size of the mini-batch will solve this
    problem.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集可以加载，但所选择的KMC算法无法处理这么大的数据量。** 选择合适的迷你批次大小将解决这个问题。'
- en: Once a dataset has been loaded, the program will start the training process.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集被加载，程序将开始训练过程。
- en: 'A mini-batch dataset called `dataset1` will be randomly created using Monte
    Carlo''s large data volume principle with a mini-batch size of 1,000\. Many variations
    of the Monte Carlo method apply to machine learning. For our example, it will
    be enough to use a random function to create the mini-batch:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用蒙特卡罗大数据量原理随机创建一个迷你批次数据集，迷你批次大小为1,000。蒙特卡罗方法在机器学习中有许多变体。对于我们的示例，使用随机函数来创建迷你批次就足够了：
- en: '[PRE1]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, the KMC algorithm runs on a standard basis, as shown in the following
    snippet:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，KMC算法按标准方式运行，如下所示：
- en: '[PRE2]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following screenshot, displaying the result produced, resembles the full
    dataset trained by KMC in *Chapter 4*, *Optimizing Your Solutions with K-Means
    Clustering*:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了生成的结果，类似于在*第4章*，《通过K均值聚类优化解决方案》中由KMC训练的完整数据集：
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/02/MiniBatchOutput.jpg](img/B15438_05_02.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/02/MiniBatchOutput.jpg](img/B15438_05_02.png)'
- en: 'Figure 5.2: Output (KMC)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：输出（KMC）
- en: 'The centroids obtained are consistent, as shown here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的质心是一致的，如下所示：
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The output will vary slightly at each run since this is a stochastic process.
    In this section, we broke the dataset down into random samples to optimize the
    training process. Another way to perform random sampling is to shuffle the dataset
    before training it.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个随机过程，输出在每次运行时会略有不同。在本节中，我们将数据集分解为随机样本以优化训练过程。另一种执行随机采样的方法是在训练前对数据集进行洗牌。
- en: Shuffling as another way to perform random sampling
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 洗牌作为另一种执行随机采样的方法
- en: The `sampling/k-means_clustering_minibatch_shuffling.py` program provides another
    way to solve a random sampling approach.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`sampling/k-means_clustering_minibatch_shuffling.py`程序提供了另一种解决随机采样方法的方式。'
- en: KMC is an unsupervised training algorithm. As such, it trains *unlabeled* data.
    A single random computation does not consume a large amount of machine resources,
    but several random selections in a row can.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: KMC是一种无监督训练算法。因此，它训练的是*无标签*数据。单次随机计算不会消耗大量的机器资源，但连续多个随机选择则会。
- en: 'Shuffling can reduce machine consumption costs. Proper shuffling of the data
    before starting training, just like shuffling cards before a poker game, will
    avoid repetitive and random mini-batch computations. In this model, the loading
    data phase and training phase do not change. However, instead of one or several
    random choices for `dataset1`, the mini-batch dataset, we shuffle the complete
    dataset once before starting the training. The following code shows how to shuffle
    datasets:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 洗牌可以减少机器消耗的成本。像在扑克游戏开始前洗牌一样，在开始训练之前对数据进行适当的洗牌，可以避免重复和随机的小批量计算。在此模型中，加载数据阶段和训练阶段不变。然而，我们并不是对`dataset1`这个小批量数据集做一次或几次随机选择，而是在开始训练之前对整个数据集进行一次洗牌。以下代码展示了如何洗牌数据集：
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then we select the first 1,000 shuffled records for training, as shown in the
    following code snippet:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们选择前1000个洗牌后的记录进行训练，如以下代码片段所示：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The result in the following screenshot corresponds to the one with the full
    dataset and the random mini-batch dataset sample:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图中的结果对应于完整数据集和随机小批量数据集样本的结果：
- en: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/02/Shuffled_mini_batch_training.jpg](img/B15438_05_03.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/02/Shuffled_mini_batch_training.jpg](img/B15438_05_03.png)'
- en: 'Figure 5.3: Full and random mini-batch dataset sample'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：完整与随机小批量数据集样本
- en: The centroids produced can provide first-level results to confirm the model,
    as shown in the following output.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 产生的质心可以提供第一层结果，以确认模型，如以下输出所示。
- en: 'The geometric centers or centroids:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 几何中心或质心：
- en: '[PRE6]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Using shuffling instead of random mini-batches has two advantages: limiting
    the number of mini-batch calculations and preventing training the same samples
    twice. If your shuffling algorithm works, you will only need to shuffle the dataset
    once. If not, you might have to go back and use random sampling, as explained
    in the previous section.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用洗牌代替随机小批量有两个优点：限制小批量计算的次数并防止训练相同的样本两次。如果你的洗牌算法有效，你只需要对数据集进行一次洗牌。如果无效，你可能需要返回并使用随机采样，正如前一节所解释的那样。
- en: Random sampling and shuffling helped to solve one part of the dataset volume
    problem.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 随机采样和洗牌帮助解决了数据集体积问题的一个方面。
- en: 'However, now we must explore the other aspect of the implementation of a large
    dataset ML algorithm: verifying the results.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在我们必须探讨实现大数据集机器学习算法的另一个方面：验证结果。
- en: Chaining supervised learning to verify unsupervised learning
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链接监督学习以验证无监督学习
- en: 'This section explores how to verify the output of an unsupervised KMC algorithm
    with a supervised algorithm: a decision tree.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨如何使用监督算法验证无监督KMC算法的输出：决策树。
- en: KMC takes an input with no labels and produces an output with labels. The unsupervised
    process makes sense out of the chaos of incoming data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: KMC接收没有标签的输入并输出带标签的结果。无监督过程使杂乱的输入数据变得有意义。
- en: 'The example in this chapter focuses on two related features: location and distance.
    The clusters produced are location-distance subsets of data within the dataset.
    The input file contains two columns: distance and location. The output file contains
    three columns: distance, location, and a label (cluster number).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的示例集中在两个相关特征上：位置和距离。生成的聚类是数据集中位置-距离的子集。输入文件包含两列：距离和位置。输出文件包含三列：距离、位置和标签（聚类编号）。
- en: The output file can thus be chained to a supervised learning algorithm, such
    as a decision tree. The decision tree will use the labeled data to produce a visual,
    white-box, machine thought process. Also, a decision tree can be trained to verify
    the results of the KMC algorithm. The process starts with preprocessing raw data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，输出文件可以连接到监督学习算法，例如决策树。决策树将使用标记的数据生成可视化的白盒机器思维过程。此外，决策树还可以被训练来验证KMC算法的结果。该过程从预处理原始数据开始。
- en: Preprocessing raw data
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理原始数据
- en: Earlier, it was shown that with large datasets, mini-batches will be necessary.
    Loading billions of records of data in memory is not an option. A random selection
    was applied in `sampling/k-means_clustering_minibatch.py` as part of the KMC algorithm.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，对于大数据集，迷你批次是必要的。将数十亿条记录加载到内存中是不现实的。在`sampling/k-means_clustering_minibatch.py`中应用了随机选择作为KMC算法的一部分。
- en: 'However, since we are chaining our algorithms in a pipeline and since we are
    not training the model, we could take the random sampling function from `sampling/k-means_clustering_minibatch.py`
    and isolate it:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们将算法串联在一个管道中，并且我们并未训练模型，因此我们可以从`sampling/k-means_clustering_minibatch.py`中提取随机采样函数并将其隔离：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The code could be applied to datasets extracted in a preprocessing phase from
    packs of data retrieved from a big data environment, for example. The preprocessing
    phase would be repeated in cycles. We will now explore the pipeline that goes
    from raw data to the output of the chained ML algorithms.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 代码可以应用于从大数据环境中提取的数据集，这些数据集经过预处理阶段提取。例如，预处理阶段将循环执行。现在，我们将探索从原始数据到串联的ML算法输出的管道。
- en: A pipeline of scripts and ML algorithms
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 脚本和ML算法的管道
- en: An ML **pipeline** will take raw data and perform dimension reduction or other
    preprocessing tasks that are not in the scope of this book. Preprocessing the
    data sometimes requires more than ML algorithms such as SQL scripts. Our exploration
    starts right after when ML algorithms such as KMC take over. However, a pipeline
    can run from raw data to ML using classical non-AI scripting as well.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一个ML **管道**将接受原始数据，并执行降维或其他预处理任务，这些任务不在本书的范围内。预处理数据有时需要不仅仅是ML算法，比如SQL脚本。我们的探索从ML算法（如KMC）接管后开始。然而，管道也可以通过经典的非AI脚本来运行，从原始数据到ML。
- en: 'The pipeline described in the following sections can be broken down into three
    major steps, preceded by classical preprocessing scripting:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节描述的管道可以分解为三个主要步骤，前面是经典的预处理脚本：
- en: '**Step 0**: A standard process performs a random sampling of a **training dataset**
    with classical preprocessing scripts before running the KMC program. This aspect
    is out of the scope of the ML process and this book. By doing this, we avoid overloading
    the ML Python programs. The training data will first be processed by a KMC algorithm
    and sent to the decision tree program.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 0**：一个标准过程在运行KMC程序之前，使用经典的预处理脚本对**训练数据集**进行随机采样。这个过程超出了ML过程和本书的范围。这样做的目的是避免过载ML
    Python程序。训练数据将首先通过KMC算法处理，并发送给决策树程序。'
- en: '**Step 1**: A KMC multi-ML program, `kmc2dt_chaining.py`, reads the training
    dataset produced by step 0 using a **saved model** from *Chapter 4*, *Optimizing
    Your Solutions with K-Means Clustering*. The KMC program takes the unlabeled data,
    makes predictions, and produces a labeled output in a file called `ckmc.csv`.
    The output label is the cluster number of a line of the dataset containing a distance
    and location.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 1**：KMC多ML程序`kmc2dt_chaining.py`使用**保存的模型**，读取步骤0产生的训练数据集，该模型来自*第4章*，*通过K-Means聚类优化解决方案*。KMC程序接收未标记的数据，进行预测，并生成标记输出，保存在名为`ckmc.csv`的文件中。输出标签是数据集中包含距离和位置的一行的聚类编号。'
- en: '**Step 2**: The decision tree program, `decision_tree.py`, reads the output
    of the KMC predictions, `ckmc.csv`. The decision tree algorithm trains its model
    and saves the trained model in a file called `dt.sav`.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步骤 2**：决策树程序`decision_tree.py`读取KMC预测的输出文件`ckmc.csv`。决策树算法训练其模型，并将训练好的模型保存在名为`dt.sav`的文件中。'
- en: '**Step 2.1**: The training phase is over. The pipeline now takes raw data retrieved
    by successive equal-sized datasets. This **batch** process will provide a fixed
    amount of data. Calculation time can be planned and mastered. This step is out
    of the scope of the ML process and this book.'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第2.1步**：训练阶段已经结束。现在管道接收由连续相等大小的数据集检索的原始数据。这个**批处理**过程将提供固定数量的数据。可以计划和掌控计算时间。这一步骤超出了机器学习过程和本书的范围。'
- en: '**Step 2.2**: A random sampling script processes the batch and produces a prediction
    dataset for the **predictions**.'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第2.2步**：一个随机抽样脚本处理批处理，并为**预测**生成预测数据集。'
- en: '**Step 3**: `kmc2dt_chaining.py` will now run a KMC algorithm that is chained
    to a decision tree that will verify the results of the KMC. The KMC algorithm
    produces predictions. The decision tree makes predictions on those predictions.
    The decision tree will also provide a visual graph in a PNG for users and administrators.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三步**：`kmc2dt_chaining.py`现在将运行一个KMC算法，该算法与决策树链接，将验证KMC的结果。KMC算法生成预测。决策树对这些预测进行进一步的预测。决策树还将为用户和管理员提供一个PNG格式的可视化图表。'
- en: Steps 2.1 to 3 can run on a twenty-four seven basis in a continuous process.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤2.1到3可以在二十四小时，七天不间断地运行。
- en: It is important to note that random forests are an interesting alternative to
    the decision tree component. It can replace the decision tree algorithm in `kmc2dt_chaining.py`.
    In the next section, we will explore random forests in this context with `random_forest.py`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，随机森林是决策树组件的一种有趣的替代方案。它可以取代`kmc2dt_chaining.py`中的决策树算法。在接下来的部分，我们将探讨随机森林在这个背景下的应用，使用`random_forest.py`。
- en: Step 1 – training and exporting data from an unsupervised ML algorithm
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第1步 - 从无监督机器学习算法中训练并导出数据
- en: '`kmc2dt_chaining.py` can be considered as the **chaining** of a KMC program
    to a decision tree program that will verify the results. Each program forms a link
    of the chain.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`kmc2dt_chaining.py`可以被视为将KMC程序与将验证结果的决策树程序链接起来的**链条**。每个程序形成链条的一个链接。'
- en: From a decision tree project's perspective, `kmc2dt_chaining.py` can be considered
    as a **pipeline** taking unlabeled data and labeling it for the supervised decision
    tree program. A pipeline takes raw data and transforms it using more than one
    ML program.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 从决策树项目的角度来看，`kmc2dt_chaining.py`可以被视为一个**管道**，用于接收未标记的数据并为监督决策树程序进行标记。管道接收原始数据，并使用多个机器学习程序进行转换。
- en: During the training phase of the chained model, `kmc2dt_chaining.py` runs to
    provide datasets for the training of the decision tree. The parameter `adt=0`
    limits the process to the KMC function, which is the first link of the chain.
    The decision tree in this program will thus not be activated in this phase.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在链接模型的训练阶段，`kmc2dt_chaining.py`运行以为决策树的训练提供数据集。参数`adt=0`限制了链条中第一个链接的KMC功能。因此，在这个阶段，该程序中的决策树不会被激活。
- en: '`kmc2dt_chaining.py` will load the dataset, load the saved KMC mode, make predictions,
    and export the labeled result:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`kmc2dt_chaining.py`将加载数据集，加载保存的KMC模型，进行预测，并导出标记结果：'
- en: '**Load the dataset**: `data.csv`, the dataset file, is the same one used in
    *Chapter 4*, *Optimizing Your Solutions with K-Means Clustering*. The two features,
    `location` and `distance`, are loaded:'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加载数据集**：`data.csv`，即章节4中使用的数据集文件，加载两个特征`location`和`distance`：'
- en: '[PRE8]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Load the KMC model**: The k-means cluster model, `kmc_model.sav`, was saved
    by `k-means_clustering_2.py` in *Chapter 4*, *Optimizing Your Solutions with K-Means
    Clustering*. It is now loaded using the `pickle` module to save it:'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加载KMC模型**：k均值聚类模型`kmc_model.sav`是由章节4中的`k-means_clustering_2.py`保存的。现在使用`pickle`模块加载它：'
- en: '[PRE9]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Make predictions**: No further training of the KMC model is required at this
    point. The model can run predictions on the mini-batches it receives. We can use
    an **incremental** process to verify the results on a large scale.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进行预测**：此时不需要对KMC模型进行进一步的训练。模型可以对接收到的小批量数据进行预测。我们可以使用增量过程来在大规模上验证结果。'
- en: If the data is not sufficiently scaled, other algorithms could be applied. In
    this case, the dataset does not require additional scaling. The KMC algorithm
    will make predictions on the sample and produce an output file for the decision
    tree.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果数据没有经过足够的缩放，可以应用其他算法。在这种情况下，数据集不需要额外的缩放。KMC算法将对样本进行预测，并为决策树生成一个输出文件。
- en: 'For this example, the predictions will be generated line by line:'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于这个示例，预测将逐行生成：
- en: '[PRE10]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The results are stored in a NumPy array:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果存储在NumPy数组中：
- en: '[PRE11]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Export the labeled data**: The predictions are saved in a file:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**导出标记数据**：预测结果保存在一个文件中：'
- en: '[PRE12]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This output file is special; it is now labeled:'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个输出文件很特别；它现在已被标记：
- en: '[PRE13]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output file contains three columns:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出文件包含三列：
- en: Column 1 = feature 1 = location; `80`, for example, on the first line
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第1列 = 特征1 = 位置；例如，第一行的`80`
- en: Column 2 = feature 2 = distance; `53`, for example, on the first line
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第2列 = 特征2 = 距离；例如，第一行的`53`
- en: Column 3 = label = cluster calculated; `5`, for example, on the first line
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第3列 = 标签 = 计算的聚类；例如，第一行的`5`
- en: In our chained ML algorithms, this output data will become the input data of
    the next ML algorithm, the decision tree.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的链式机器学习算法中，这个输出数据将成为下一个机器学习算法——决策树——的输入数据。
- en: Step 2 – training a decision tree
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第2步——训练决策树
- en: 'In *Chapter 3*, *Machine Intelligence – Evaluation Functions and Numerical
    Convergence*, a decision tree was described and used to visualize a priority process.
    `Decision_Tree_Priority.py` produced the following graph:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第3章*，*机器智能——评估函数与数值收敛*中，描述了决策树并用它来可视化优先级处理。`Decision_Tree_Priority.py`生成了如下图表：
- en: '![Une image contenant signe, texte  Description générée automatiquement](img/B15438_05_04.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![Une image contenant signe, texte  Description générée automatiquement](img/B15438_05_04.png)'
- en: 'Figure 5.4: Decision tree priorities'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：决策树优先级
- en: The tree starts with a node with a high Gini value. The node is split into two,
    and each node below is a "leaf" in this case because Gini=0.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 树从一个高基尼值的节点开始。该节点被分成两个，每个节点下的部分是“叶子”，因为基尼=0。
- en: The decision tree algorithm implemented in this book uses Gini impurity.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中实现的决策树算法使用基尼不纯度。
- en: Gini impurity represents the probability of a data point being incorrectly classified.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度表示数据点被错误分类的概率。
- en: A decision tree will start with the highest impurities. It will split the probability
    into two branches after having calculated a threshold.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树将从最高的不纯度开始。它将在计算出一个阈值后，将概率分成两个分支。
- en: When a branch reaches a Gini impurity of 0, it reaches its **leaf**.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个分支达到基尼不纯度为0时，它达到了**叶子**。
- en: Let's state that *k* is the probability of a data point being incorrectly classified.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 假设*k*是数据点被错误分类的概率。
- en: 'The dataset *X* from *Chapter 3*, *Machine Intelligence – Evaluation Functions
    and Numerical Convergence*, contains six data points. Four data points are low,
    and two data points are high:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 来自*第3章*的*X*数据集，*机器智能——评估函数与数值收敛*，包含六个数据点。四个数据点是低值，两个数据点是高值：
- en: '*X* = {Low, Low, High, High, Low, Low}'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*X* = {低，低，高，高，低，低}'
- en: 'The equation of Gini impurity calculates the probability of each feature occurring
    and multiplies the result by 1—the probability of each feature occurring on the
    remaining values—as shown in the following equation:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度的公式计算了每个特征出现的概率，并将结果乘以1——每个特征在剩余值中的出现概率，如下所示的公式：
- en: '![](img/B15438_05_003.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_05_003.png)'
- en: 'Applied to the *X* dataset with four lows out of six and two highs out of six,
    the result will be:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于*X*数据集，其中六个数据点中有四个是低值，两个是高值，结果将是：
- en: '*G*(*k*) = (4/6) * (1 – 4/6) + (2/6) * (1 – 2/6)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*G*(*k*) = (4/6) * (1 – 4/6) + (2/6) * (1 – 2/6)'
- en: '*G*(*k*)=(0.66 * 0.33) + (0.33 * 0.66)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*G*(*k*)=(0.66 * 0.33) + (0.33 * 0.66)'
- en: '*G*(*k*)=0.222 + 0.222=0.444'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*G*(*k*)=0.222 + 0.222=0.444'
- en: The probability that a data point will be incorrectly predicted is 0.444, as
    shown in the graph.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点被错误预测的概率为0.444，如图所示。
- en: The decision train is built on **the gain of information** on the features that
    contain the highest Gini impurity value.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树基于**特征的信息增益**构建，选择包含最高基尼不纯度值的特征。
- en: We will now explore the Python implementation of a decision tree to prepare
    it to be chained to the KMC program.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探索决策树的Python实现，为将其链入KMC程序做准备。
- en: Training the decision tree
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练决策树
- en: 'To train the decision tree, `decision_tree.py` will load the dataset, train
    the model, make predictions, and save the model:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练决策树，`decision_tree.py`将加载数据集，训练模型，做出预测，并保存模型：
- en: '**Load the dataset**: Before loading the dataset, you will need to import the
    following modules:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加载数据集**：在加载数据集之前，你需要导入以下模块：'
- en: '[PRE14]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The versions of the modules will vary as the editors produce them and also depend
    on how often you update the versions and the code. For example, you might get
    a warning when you try to unpickle a KMC model from version 0.20.3 when using
    version 0.21.2\. As long as it works, it is fine for educational purposes. However,
    in production, an administrator should have a database with the list of packages
    used and their versions.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 模块的版本会随着编辑者的发布而变化，并且还取决于您更新版本和代码的频率。例如，当您尝试从0.20.3版本解压KMC模型时，如果使用的是0.21.2版本，可能会收到警告。只要它能够正常工作，在教育用途上是没问题的。然而，在生产环境中，管理员应该有一个数据库，其中列出所使用的包及其版本。
- en: 'The dataset is loaded, labeled, and split into training and test datasets:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集被加载、标注，并分为训练数据集和测试数据集：
- en: '[PRE15]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The data in this example will contain two features (location and distance)
    and a label (cluster number) provided by the output of the KMC algorithm. The
    following header shows how the dataset is structured:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 本例中的数据将包含两个特征（位置和距离）以及一个标签（聚类编号），该标签由KMC算法的输出提供。以下标题展示了数据集的结构：
- en: '[PRE16]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Training the model**: Once the datasets are ready, the decision tree classifier
    is created and the model is trained:'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练模型**：一旦数据集准备好，决策树分类器就会被创建，模型将进行训练：'
- en: '[PRE17]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Making predictions**: Once the model is trained, predictions are made on
    the test dataset:'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进行预测**：一旦模型训练完成，就在测试数据集上进行预测：'
- en: '[PRE18]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The predictions use the features to predict a cluster number in the test dataset,
    as shown in the following output:'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测使用特征来预测测试数据集中的聚类编号，如以下输出所示：
- en: '[PRE19]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Measuring the results with metrics**: A key part of the process is to measure
    the results with metrics. If the accuracy approaches 1, then the KMC output chained
    to the decision tree algorithm is reliable:'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用度量衡量结果**：过程的关键部分是使用度量来衡量结果。如果准确度接近1，那么KMC输出链式到决策树算法是可靠的：'
- en: '[PRE20]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In this example, the accuracy was 0.97\. The model can predict the cluster of
    a distance and location with a 0.97 probability, which proves its efficiency.
    The training of the chained ML solution is over, and a prediction cycle begins.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个例子中，准确度为0.97。该模型可以以0.97的概率预测一个距离和位置的聚类，证明了其高效性。链式机器学习解决方案的训练已结束，预测周期开始。
- en: 'You can generate a PNG file of the decision tree with `decision_tree.py`. Uncomment
    the last paragraph of the program, which contains the export function:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`decision_tree.py`生成决策树的PNG文件。取消注释程序的最后一段，其中包含导出功能：
- en: '[PRE21]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note that once you have implemented this function, you can activate or deactivate
    it with the `graph` parameter.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一旦实现了这个功能，您可以通过`graph`参数来激活或停用它。
- en: The following image produced for this example can help you understand the thought
    process of the whole chained solution (KMC and the decision tree).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像是为这个例子生成的，帮助您理解整个链式解决方案（KMC和决策树）的思路。
- en: '![](img/B15438_05_05.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15438_05_05.png)'
- en: 'Figure 5.5: Image output from the code example'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：代码示例的图像输出
- en: The image file, `dt_kmc.png`, is available on GitHub in `CH05`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图像文件`dt_kmc.png`可以在GitHub的`CH05`目录中找到。
- en: Step 3 – a continuous cycle of KMC chained to a decision tree
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第3步 – KMC链式到决策树的持续循环
- en: The training of the chained KMC algorithm chained to a decision tree algorithm
    is over.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 链式KMC算法与决策树算法的训练已完成。
- en: The preprocessing phase using classical big data batch retrieval methods will
    continuously provide randomly sampled datasets with a script.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用经典的大数据批量检索方法的预处理阶段将持续提供带有脚本的随机采样数据集。
- en: '`kmc2dt_chaining.py` can focus on running KMC predictions and passing them
    on to decision tree predictions for white-box checking. The continuous process
    imports a dataset, loads saved models, predicts, and measures the results at the
    decision tree level.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`kmc2dt_chaining.py`可以专注于运行KMC预测并将其传递给决策树预测进行白盒检查。这个持续过程导入数据集，加载保存的模型，进行预测并在决策树级别衡量结果。'
- en: The chained process can run on a twenty-four seven basis if necessary.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 链式过程如果需要可以24小时七天运行。
- en: 'The modules used are required for both the KMC and the decision trees implemented
    in the training programs:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 训练程序中使用的模块是KMC和决策树所必需的：
- en: '[PRE22]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let''s run through the process:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起回顾这个过程：
- en: '**Loading the KMC dataset and model**: The KMC dataset has been prepared in
    the preprocessing phase. The trained model has been previously saved. The dataset
    is loaded with pandas and the model is loaded with `pickle`:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加载KMC数据集和模型**：KMC数据集已在预处理阶段准备好。经过训练的模型已被保存。数据集通过pandas加载，模型通过`pickle`加载：'
- en: '[PRE23]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Predicting and saving**: The goal is to predict the batch dataset line by
    line and save the result in an array in a white-box approach that can be verified
    by the administrator of the system:'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测和保存**：目标是逐行预测批量数据集，并以白盒方式将结果保存到数组中，管理员可以验证该方式：'
- en: '[PRE24]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `ckmc.csv` file generated is the entry point of the next link of the chain:
    the decision tree.'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成的`ckmc.csv`文件是下一个链环的入口：决策树。
- en: The two lines containing the `print` instruction are commented for standard
    runs. However, you may wish to explore the outputs in detail if your code requires
    maintenance. That is why I recommend adding maintenance lines in the code.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含`print`指令的两行代码已注释，供标准运行使用。然而，如果代码需要维护，你可能希望详细查看输出。这就是为什么我建议在代码中添加维护行的原因。
- en: '**Loading the dataset for the decision tree**: The dataset for the decision
    tree is loaded in the same way as in `decision_tree.py`. A parameter activates
    the decision tree part of the code: `adt=1`. The white-box quality control approach
    can thus be activated or deactivated.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加载决策树的数据集**：决策树的数据集与`decision_tree.py`中的方法相同加载。一个参数激活决策树部分的代码：`adt=1`。因此，可以激活或停用白盒质量控制方法。'
- en: 'The program loads the dataset, loads the model, and splits the data:'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 程序加载数据集，加载模型，并进行数据分割：
- en: '[PRE25]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Although the dataset has been split, only the test data is used to verify the
    predictions of the decision tree and the quality of the KMC outputs.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管数据集已经分割，只有测试数据被用来验证决策树的预测和KMC输出的质量。
- en: '**Predicting and measuring the results**: The decision tree predictions are
    made and the accuracy of the results is measured:'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测和衡量结果**：做出决策树预测，并衡量结果的准确性：'
- en: '[PRE26]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Once again, in this example, as in `decision_tree.py`, the accuracy is 0.97.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 再次说明，在这个例子中，和`decision_tree.py`一样，准确率是0.97。
- en: '**Double-checking the accuracy of the predictions**: In the early days of a
    project or for maintenance purposes, double-checking is recommended. The function
    is activated or deactivated with a parameter named `doublecheck`.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**再次检查预测准确性**：在项目的初期阶段或进行维护时，建议进行再次检查。此功能通过名为`doublecheck`的参数激活或停用。'
- en: 'The prediction results are checked line by line against the original labels
    and measured:'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预测结果逐行与原始标签进行对比，并进行衡量：
- en: '[PRE27]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The program will print out the predictions line by line, stating if they are
    `True` or `False`:'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 程序将逐行打印出预测结果，并指明其是`True`还是`False`：
- en: '[PRE28]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this example, the accuracy is 0.99, which is high. Only 9 out of 1,000 predictions
    were `False`. This result is not the same as the metrics function because it is
    a simple calculation that does not take mean errors or other factors into account.
    However, it shows that the KMC produced good results for this problem.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，准确率为0.99，较高。1,000次预测中仅有9次是`False`。这个结果不同于指标函数，因为它是一个简单的计算，没有考虑均值误差或其他因素。然而，它展示了KMC在这个问题上的良好表现。
- en: Decision trees provide a good approach for the KMC. However, random forests
    take machine learning to another level and provide an interesting alternative,
    if necessary, to the use of decision trees.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树为KMC提供了一个良好的方法。然而，随机森林将机器学习提升到了一个新的水平，提供了一个有趣的替代方案，若需要，可以替代决策树的使用。
- en: Random forests as an alternative to decision trees
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林作为决策树的替代方案
- en: Random forests open mind-blowing ML horizons. They are ensemble meta-algorithms.
    As an ensemble, they contain several decision trees. As meta-algorithms, they
    go beyond having one decision tree making predictions.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林开辟了令人震惊的机器学习新天地。它们是集成元算法。作为集成算法，它们包含了多个决策树。作为元算法，它们超越了一个决策树做出预测的方式。
- en: 'To run an ensemble (several decision trees) as a meta-algorithm (several algorithms
    training and predicting the same data), the following module is required:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行一个集成（多个决策树）作为一个元算法（多个算法训练并预测相同数据），需要以下模块：
- en: '[PRE29]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To understand how a random forest works as a meta-algorithm, let''s focus on
    three key parameters in the following classifier:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解随机森林作为元算法如何工作，让我们专注于以下分类器中的三个关键参数：
- en: '[PRE30]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`n_estimators=25`: This parameter states the number of **trees** in the **forest**.
    Each tree will run its prediction. The main method to reach the final prediction
    is obtained by averaging the predictions of each tree.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators=25`：此参数表示 **森林** 中的 **树木** 数量。每棵树都会运行其预测。最终预测结果是通过平均每棵树的预测值获得的。'
- en: At each split of each tree, features are randomly permuted. The trees use different
    feature approaches.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在每棵树的每次分割时，特征会被随机排列。每棵树使用不同的特征方法。
- en: '`bootstrap=True`: When bootstrap is activated, a smaller sample is bootstrapped
    from the sample provided. Each tree thus bootstraps its own samples, adding more
    variety.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bootstrap=True`：当启用 bootstrap 时，从提供的样本中生成一个较小的样本。每棵树都将生成自己的样本，从而增加多样性。'
- en: '`random_state=None`: When `random_state=None` is activated, the random function
    uses `np.random`.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_state=None`：当 `random_state=None` 被激活时，随机函数使用 `np.random`。'
- en: You can also use the other methods by consulting the scikit-learn documentation
    (see *Further reading* at the end of the chapter). My preference is to use `np.random`.
    Note that to split the training state, I use scikit-learn's default random generator
    example with `random_state=0`.
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还可以参考 scikit-learn 文档使用其他方法（见本章末尾的 *进一步阅读*）。我个人更倾向于使用 `np.random`。请注意，拆分训练集时，我使用了
    scikit-learn 默认的随机生成器示例，`random_state=0`。
- en: This shows the importance of these small changes in parameters. After many tests,
    this is what I preferred. But maybe, in other cases, other `random_state` values
    are preferable.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这显示了参数小变化的重要性。经过多次测试后，这是我偏好的配置。但在其他情况下，可能其他 `random_state` 值更为适用。
- en: Beyond these key concepts and parameters, `random_forest.py` can be built in
    a clear, straightforward way.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些关键概念和参数外，`random_forest.py` 可以以清晰、直接的方式构建。
- en: '`random_forest.py` is built with the same structure as the KMC or decision
    tree program. It loads a dataset, prepares the features and target variables,
    splits the dataset into training and testing sub-datasets, predicts, and measures.
    `random_forest.py` also contains a custom double-check function that will display
    each prediction, its status (`True` or `False`), and provide an independent accuracy rate.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`random_forest.py` 的结构与 KMC 或决策树程序相同。它加载数据集，准备特征和目标变量，将数据集拆分为训练集和测试集，进行预测并进行评估。`random_forest.py`
    还包含一个自定义的双重检查函数，该函数将显示每个预测结果、其状态（`True` 或 `False`），并提供独立的准确率。'
- en: '**Loading the dataset**: `ckmc.csv` was generated by the preceding KMC program.
    This time, it will be read by `random_forest.py` instead of `decision_tree.py`.
    The dataset is loaded, and the features and target variables are identified. Note
    the `pp` variable, which will trigger the `print` function or not. This is useful
    for switching from production mode to maintenance mode with a single variable
    change. Change `pp=0` to `pp=1` if you wish to switch to maintenance mode. In
    this case `pp` is activated:'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加载数据集**：`ckmc.csv` 是由前述的 KMC 程序生成的。这次，它将由 `random_forest.py` 读取，而不是 `decision_tree.py`。数据集被加载，并且特征和目标变量被识别。请注意
    `pp` 变量，它将决定是否触发 `print` 函数。这在通过一个变量的更改从生产模式切换到维护模式时非常有用。如果你希望切换到维护模式，只需将 `pp=0`
    更改为 `pp=1`。在这种情况下，`pp` 被激活：'
- en: '[PRE31]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The program prints the labeled data:'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 程序打印标记数据：
- en: '[PRE32]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The program prints the target cluster numbers to predict:'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 程序打印目标簇编号以进行预测：
- en: '[PRE33]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '**Splitting the data, creating the classifier, and training the model**: The
    dataset is split into training and testing data. The random forest classifier
    is created with 25 estimators. Then the model is trained:'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分割、创建分类器和训练模型**：数据集被分为训练数据和测试数据。随机森林分类器使用 25 个估算器进行创建。然后，模型被训练：'
- en: '[PRE34]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The program is ready to predict.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 程序已准备好进行预测。
- en: '**Predicting and measuring the accuracy of the trained model**:'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测和测量训练模型的准确性**：'
- en: '[PRE35]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output results and metrics are satisfactory:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果和度量是令人满意的：
- en: '[PRE36]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: A mean error approaching 0 is an efficient result.
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平均误差接近 0 是一个高效的结果。
- en: '**Double-checking**: A double-checking function is recommended in the early
    stages of an ML project and for maintenance purposes. The function is activated
    by the `doublecheck` parameter as in `kmc2dt_chaining.py`. Set `doublecheck=1`
    if you wish to activate the maintenance mode. It is in fact, the same template:'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双重检查**：在 ML 项目的早期阶段和维护过程中，建议使用双重检查函数。通过 `doublecheck` 参数激活该函数，如同在 `kmc2dt_chaining.py`
    中一样。如果你希望激活维护模式，请设置 `doublecheck=1`。它实际上是相同的模板：'
- en: '[PRE37]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The accuracy of the random forest is efficient. The output of the measurement
    is:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的准确性很高。测量结果输出如下：
- en: '[PRE38]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The absolute error is a simple arithmetic approach that does not take mean errors
    or other factors into account. The score will vary from one run to another because
    of the random nature of the algorithm. However, in this case, 6 errors out of
    1,000 predictions is a good result.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对误差是一种简单的算术方法，不考虑均值误差或其他因素。由于算法的随机性质，分数在不同运行之间会有所不同。然而，在这种情况下，每千次预测中有6个错误是一个不错的结果。
- en: Ensemble meta-algorithms such as random forests can replace the decision tree
    in `kmc2dt_chaining.py` with just a few lines of code, as we just saw in this
    section, tremendously boosting the whole chained ML process.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 像随机森林这样的集成元算法可以用几行代码替换`kmc2dt_chaining.py`中的决策树，就像我们在这一节中看到的那样，极大地提升了整个链式机器学习过程。
- en: Chained ML algorithms using ensemble meta-algorithms are extremely powerful
    and efficient. In this chapter, we used a chained ML solution to deal with large
    datasets and perform automatic quality control on machine intelligence predictions.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 使用集成元算法的链式机器学习算法非常强大且高效。在本章中，我们使用链式机器学习解决方案处理大数据集，并对机器智能预测进行自动质量控制。
- en: Summary
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Although it may seem paradoxical, try to avoid AI before jumping into a project
    that involves millions to billions of records of data (such as SQL, Oracle, and
    big data). Try simpler classical solutions like big data methods. If the AI project
    goes through, LLN will lead to random sampling over the datasets, thanks to CLT.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管看起来可能有些矛盾，但在开始涉及数百万到数十亿条数据记录的项目（如SQL、Oracle和大数据）之前，尝试避免使用AI。可以尝试更简单的经典解决方案，比如大数据方法。如果AI项目通过，LLN将引导通过中央极限定理进行数据集上的随机抽样。
- en: A pipeline of classical and ML processes will solve the volume problem, as well
    as the human analytic limit problem. The random sampling function does not need
    to run a mini-batch function included in the KMC program. Batches can be generated
    as a preprocessing phase using classical programs. These programs will produce
    random batches of equal size to the KMC NP-hard problem, transposing it into an
    NP problem.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 经典和机器学习流程的管道将解决数据量问题，以及人工分析限制问题。随机抽样功能无需在KMC程序中运行迷你批处理功能。批次可以作为预处理阶段，通过经典程序生成。这些程序将生成与KMC
    NP难问题等大小的随机批次，将其转化为一个NP问题。
- en: KMC, an unsupervised training algorithm, will transform unlabeled data into
    a labeled data output containing a cluster number as a label.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: KMC，一种无监督训练算法，将把未标记数据转化为包含聚类编号作为标签的标记数据输出。
- en: In turn, a decision tree, chained to the KMC program, will train its model using
    the output of the KMC. The model will be saved just as the KMC model was saved.
    The random forests algorithm can replace the decision tree algorithm if it provides
    better results during the training phase of the pipeline.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，决策树链入KMC程序，将使用KMC的输出训练其模型。该模型将像保存KMC模型一样被保存。如果在管道的训练阶段，随机森林算法提供了更好的结果，它可以替代决策树算法。
- en: In production mode, a chained ML program containing the KMC trained model and
    the decision tree trained model can make classification predictions on fixed random
    sampled batches. Real-time metrics will monitor the quality of the process. The
    chained program, being continuous, can run twenty-four seven, providing reliable
    real-time results without human intervention.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产模式下，包含KMC训练模型和决策树训练模型的链式机器学习程序可以对固定随机抽样批次进行分类预测。实时指标将监控过程的质量。由于链式程序是连续的，它可以全天候运行，提供可靠的实时结果，而无需人工干预。
- en: 'The next chapter explores yet another ML challenge: the increasing amount of
    language translations generated by global business and social communication.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将探讨另一个机器学习挑战：全球商业和社会沟通中生成的语言翻译数量日益增加。
- en: Questions
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: The number of k clusters is not that important. (Yes | No)
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚类的数量k并不是那么重要。(是 | 否)
- en: Mini-batches and batches contain the same amount of data. (Yes | No)
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迷你批次和批次包含相同数量的数据。(是 | 否)
- en: K-means can run without mini-batches. (Yes | No)
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K均值可以在没有迷你批次的情况下运行。(是 | 否)
- en: Must centroids be optimized for result acceptance? (Yes | No)
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 必须优化质心以便结果被接受吗？(是 | 否)
- en: It does not take long to optimize hyperparameters. (Yes | No)
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化超参数并不需要很长时间。(是 | 否)
- en: It sometimes takes weeks to train a large dataset. (Yes | No)
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练大数据集有时需要几周时间。(是 | 否)
- en: Decision trees and random forests are unsupervised algorithms. (Yes | No)
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决策树和随机森林是无监督算法。(是 | 否)
- en: Further reading
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Decision trees: [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '决策树: [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html)'
- en: 'Random forests: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '随机森林: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)'
