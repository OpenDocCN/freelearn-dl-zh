- en: 5\. Deep Learning for Sequences
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 深度学习在序列中的应用
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will implement deep learning-based approaches to sequence
    modeling, after understanding the considerations of dealing with sequences. We
    will begin with **Recurrent Neural Networks** (**RNNs**), an intuitive approach
    to sequence processing that has provided state-of-the-art results. We will then
    discuss and implement 1D convolutions as another approach and see how it compares
    with RNNs. We will also combine RNNs with 1D convolutions in a hybrid model. We
    will employ all of these models on a classic sequence processing task – stock
    price prediction. By the end of this chapter, you will become adept at implementing
    deep learning approaches for sequences, particularly plain RNNs and 1D convolutions,
    and you will have laid the foundations for more advanced RNN-based models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将实现基于深度学习的序列建模方法，在了解处理序列时需要注意的事项后开始。我们将从**递归神经网络**（**RNNs**）开始，这是一个直观的序列处理方法，已提供了最先进的结果。接着我们将讨论并实现1D卷积作为另一种方法，并对比其与RNN的效果。我们还将把RNN与1D卷积结合在一个混合模型中。我们将在一个经典的序列处理任务——股票价格预测上使用这些模型。到本章结束时，你将能够熟练地实现深度学习序列方法，特别是普通RNN和1D卷积，并为更先进的基于RNN的模型奠定基础。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: 'Let''s say you''re working with text data and your objective is to build a
    model that checks whether a sentence is grammatically correct. Consider the following
    sentence: *"words? while sequence be this solved of can the ignoring".* The question
    didn''t make sense, right? Well, how about the following? *"Can this be solved
    while ignoring the sequence of words?"*'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在处理文本数据，目标是构建一个模型，检查一个句子是否语法正确。考虑以下句子：*“words? while sequence be this solved
    of can the ignoring”。* 这个问题没有意义，对吧？那么，下面的句子怎么样？*“Can this be solved while ignoring
    the sequence of words?”*
- en: Suddenly, the text makes complete sense. What do we acknowledge, then, about
    working with text data? That sequence matters.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 突然间，文本变得完全有意义。那么，关于处理文本数据我们得出什么结论呢？序列很重要。
- en: In the task of assessing whether a given sentence is grammatically correct,
    the sequence is important. Sequence-agnostic models would fail terribly at the
    task. The nature of the task requires you to analyze the sequence of the terms.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估一个句子是否语法正确的任务中，序列是非常重要的。忽略序列的模型会在任务中惨败。这个任务的性质要求你分析术语的顺序。
- en: 'In the previous chapter, we worked with text data, discussing ideas around
    representation and creating our own word vectors. Text and natural language data
    have another important characteristic – they have a sequence to them. While text
    data is one example of sequence data, sequences are everywhere: from speech to
    stock prices, from music to global temperatures. In this chapter, we''ll start
    working with sequential data in a way that considers the order of the elements.
    We will begin with RNNs, a deep learning approach that exploits the sequence of
    data to provide insightful results of tasks such as machine translation, sentiment
    analysis, recommender systems, and time series prediction, to name a few. We will
    then look at using convolutions for sequence data. Finally, we will see how these
    approaches can be combined in a single, powerful deep learning architecture. Along
    the way, we will also build an RNN-based model for stock price prediction.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们处理了文本数据，讨论了表示的相关思想，并创建了我们自己的词向量。文本和自然语言数据有一个重要的特性——它们具有顺序性。文本数据是序列数据的一种例子，但序列无处不在：从语音到股价，从音乐到全球气温。在本章中，我们将开始以一种考虑元素顺序的方式处理顺序数据。我们将从RNN开始，这是一种深度学习方法，利用数据的序列性为诸如机器翻译、情感分析、推荐系统和时间序列预测等任务提供有见地的结果。然后，我们将研究如何使用卷积处理序列数据。最后，我们将看到如何将这些方法结合在一个强大的深度学习架构中。过程中，我们还将构建一个基于RNN的股票价格预测模型。
- en: Working with Sequences
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与序列的工作
- en: Let's look at another example to make the importance of sequence modeling clearer.
    The task is to predict the stock price for a company for the next 30 days. The
    data provided to you is the stock price for today. You can see this in the following
    plot, where the *y-axis* represents the stock price and the *x-axis* denotes the
    date. Is this data sufficient?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个例子，以更清楚地说明序列建模的重要性。任务是预测某公司未来30天的股票价格。提供给你的数据是今天的股票价格。你可以在以下图表中看到，*y轴*表示股票价格，*x轴*表示日期。这样的数据足够吗？
- en: '![Figure 5.1: Stock price with just 1 day’s data'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1：仅使用一天数据的股票价格'
- en: '](img/B15385_05_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_01.jpg)'
- en: 'Figure 5.1: Stock price with just 1 day''s data'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：仅用1天数据的股票价格
- en: 'Surely, one data point, that is, the price on a given day, is not sufficient
    to predict the price for the next 30 days. We need more information. Particularly,
    we need information about the past – how the stock price has been moving for the
    past few days/months/years. So, we ask for, and get, data for three years:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，单一数据点，即某一天的价格，无法预测接下来30天的价格。我们需要更多的信息。特别是，我们需要关于过去的信息——股票价格在过去几天/月/年的走势。因此，我们请求并获得了三年的数据：
- en: '![Figure 5.2: Stock price prediction using historical data'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.2：使用历史数据的股票价格预测'
- en: '](img/B15385_05_02.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_02.jpg)'
- en: 'Figure 5.2: Stock price prediction using historical data'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：使用历史数据的股票价格预测
- en: This seems much more useful, right? Looking at the past trend and some patterns
    in the data, we can make predictions on the future stock prices. Thus, by looking
    at the past trend, we get a rough idea of how the stock will move over the next
    few days. We can't do this without a sequence. Again, sequence matters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这样看起来更有用，对吧？通过观察过去的趋势和数据中的一些模式，我们可以对未来的股票价格进行预测。因此，通过观察过去的趋势，我们可以大致了解股票在接下来几天的走势。如果没有序列，这是无法做到的。再次强调，序列很重要。
- en: 'In real-world use cases, say, machine translation, you need to consider the
    sequence in the data. Sequence-agnostic models can only get you so far in some
    tasks; you need an approach that truly exploits the information contained in the
    sequence. But before talking about the workings of those architectures, we need
    to answer an important question: *what are sequences, anyway*?'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，比如机器翻译，你需要考虑数据中的序列。忽略序列的模型只能在某些任务中起到有限作用；你需要一种真正能够充分利用序列中信息的方法。但在讨论这些架构的工作原理之前，我们需要回答一个重要问题：*序列到底是什么*？
- en: 'While the definition of a "*sequence*" from the dictionary is rather self-explanatory,
    we need to be able to identify sequences for ourselves and decide whether we need
    to consider the sequence. To understand this idea, let''s go back to the first
    example we saw: "*words? while sequence be this solved of can the ignoring*" versus
    "*can this be solved while ignoring the sequence of words?*"'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然词典中的“*序列*”定义是相当直白的，但我们需要能够自己识别序列，并决定是否需要考虑序列。为了理解这个概念，让我们回到我们看到的第一个例子：“*words?
    while sequence be this solved of can the ignoring*” 和 “*can this be solved while
    ignoring the sequence of words?*”
- en: 'When you jumbled the terms of the meaningful sentence text, it stopped making
    sense and lost all/most of the information. This can be a simple and effective
    test for a sequence: If you jumbled the elements, does it still make sense? If
    the answer is "no," then you have a sequence at hand. While sequences are everywhere,
    here are some examples of sequence data: language, music, movie scripts, music
    videos, time-series data (stock prices, commodity prices, and more), and the survival
    probability of a patient.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将有意义的句子中的词语顺序打乱时，它就变得没有意义，丧失了所有或大部分信息。这可以是测试序列的一种简单有效的方法：如果你打乱了元素，它还合理吗？如果答案是否定的，那你手中就有一个序列。虽然序列无处不在，这里有一些序列数据的例子：语言、音乐、电影剧本、音乐视频、时间序列数据（股票价格、商品价格等），以及患者的生存概率。
- en: Time Series Data – Stock Price Prediction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列数据——股票价格预测
- en: We will start working on our own model for predicting stock prices. The objective
    of the stock price prediction task is to build a model that can predict the next
    day's stock price based on historical prices. As we saw in the previous section,
    the task requires us to consider the sequence in the data. We will predict the
    stock price for Apple Inc.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始构建自己的股票价格预测模型。股票价格预测任务的目标是构建一个基于历史价格能够预测第二天股票价格的模型。正如我们在前一节中看到的，这个任务要求我们考虑数据中的顺序。我们将预测苹果公司（Apple
    Inc.）的股票价格。
- en: Note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'We will use a cleaned-up version of Apple''s historical stock data that''s
    been sourced from the Nasdaq website: [https://www.nasdaq.com/market-activity/stocks/aapl/historical](https://www.nasdaq.com/market-activity/stocks/aapl/historical).
    The dataset can be downloaded from the following link: [https://packt.live/325WSKR](https://packt.live/325WSKR).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用从纳斯达克网站获取的经过清理的苹果历史股票数据：[https://www.nasdaq.com/market-activity/stocks/aapl/historical](https://www.nasdaq.com/market-activity/stocks/aapl/historical)。数据集可以从以下链接下载：[https://packt.live/325WSKR](https://packt.live/325WSKR)。
- en: Make sure to place the file (`AAPL.csv`) in your working directory and start
    a new Jupyter Notebook for the code. It is important that you run all the code
    in the exercises and the topic sections in a single Jupyter Notebook.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 确保将文件（`AAPL.csv`）放在你的工作目录中，并开始一个新的 Jupyter Notebook 来编写代码。你必须在同一个 Jupyter Notebook
    中运行所有练习和主题部分的代码。
- en: 'Let''s begin by understanding the data. We will load the required libraries
    and then load and plot the data. You can use the following commands to load the
    necessary libraries and use the cell magic command (`%matplotlib inline`) to plot
    the images inline:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从理解数据开始。我们将加载所需的库，然后加载并绘制数据。你可以使用以下命令加载必要的库，并使用单元格魔法命令（`%matplotlib inline`）将图像内联显示：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we''ll load the `.csv` file, using the `read_csv()` method from Pandas,
    into a DataFrame (`inp0`) and have a look at a few records using the `head` method
    of the pandas DataFrame:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 Pandas 的 `read_csv()` 方法加载 `.csv` 文件到一个 DataFrame（`inp0`），并通过 pandas
    DataFrame 的 `head` 方法查看一些记录：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You should get the following output:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到以下输出：
- en: '![Figure 5.3: The first five records of the AAPL dataset'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3：AAPL 数据集的前五条记录'
- en: '](img/B15385_05_03.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_03.jpg)'
- en: 'Figure 5.3: The first five records of the AAPL dataset'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：AAPL 数据集的前五条记录
- en: We can see that the first record is for January 17, 2020 and is the most recent
    date in the data (the latest data at the time of writing this book). As is the
    convention for pandas DataFrames, the first record has an index of 0 (the index
    is simply the identifier for the row, and each row has an index value). `Open`
    refers to the value of a particular stock at the opening of the trade, `High`
    refers to the highest value of the stock during the day, while `Low` and `Close`
    represent the lowest price and closing price, respectively. We also have the volume
    traded on the day.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，第一条记录是2020年1月17日，是数据中最接近的日期（截至本书写作时的最新数据）。按照 pandas DataFrame 的惯例，第一条记录的索引为
    0（索引是行的标识符，每一行都有一个索引值）。`Open` 表示股票开盘时的价格，`High` 表示当天股票的最高价格，而 `Low` 和 `Close`
    分别代表最低价格和收盘价。我们还记录了当天的交易量。
- en: 'Let''s also look at the last few records of the dataset using the following
    command:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们还通过以下命令查看数据集的最后几条记录：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The records look as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 记录如下所示：
- en: '![Figure 5.4: Bottom five records of the AAPL dataset'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.4：AAPL 数据集的底部五条记录'
- en: '](img/B15385_05_04.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_04.jpg)'
- en: 'Figure 5.4: Bottom five records of the AAPL dataset'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4：AAPL 数据集的底部五条记录
- en: From the preceding tables, we can see that we have daily opening, high, low,
    and closing prices, and volumes, from January 25, 2010 to January 17, 2020\. For
    our purpose, we are concerned with the closing price.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的表格中我们可以看到，我们有从2010年1月25日到2020年1月17日的每日开盘价、最高价、最低价和收盘价，以及成交量。对于我们的目的，我们关心的是收盘价。
- en: 'Exercise 5.01: Visualizing Our Time-Series Data'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.01：可视化我们的时间序列数据
- en: 'In this exercise, we will extract the closing price from the data, perform
    the necessary formatting, and plot the time series to gain a better understanding
    of the data. Make sure that you have read through the preceding section and loaded
    the data, as well as imported the relevant libraries. Perform the following steps
    to complete this exercise:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将从数据中提取收盘价，进行必要的格式化，并绘制时间序列图，以更好地理解数据。确保你已经阅读了前面的章节并加载了数据，同时导入了相关库。按照以下步骤完成本练习：
- en: 'Use the following command to import the necessary libraries if you haven''t already:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你还没有导入所需的库，可以使用以下命令：
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Download the file titled `AAPL.csv` from GitHub ([https://packt.live/325WSKR](https://packt.live/325WSKR))
    and load it into a DataFrame:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 GitHub 下载名为`AAPL.csv`的文件 ([https://packt.live/325WSKR](https://packt.live/325WSKR))
    并将其加载到一个 DataFrame 中：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Plot the `Close` column as a line plot to see the pattern using the `plot`
    method of the DataFrame, specifying the `Date` column as the *X-axis*:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `plot` 方法将 `Close` 列绘制成折线图，观察其模式，并指定 `Date` 列为 *X 轴*：
- en: '[PRE5]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The plot for this will be as follows, with the *X-axis* showing the closing
    price and the *Y-axis* representing the dates:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个图的绘制如下，*X 轴*显示的是收盘价，*Y 轴*表示日期：
- en: '![Figure 5.5: Plot of the closing price'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.5：收盘价图'
- en: '](img/B15385_05_05.jpg)'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_05_05.jpg)'
- en: 'Figure 5.5: Plot of the closing price'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.5：收盘价图
- en: From the plot, we can see that the latest values are getting plotted first (on
    the left). We'll reverse the data for convenience of plotting and handling. We'll
    achieve this by sorting the DataFrame by the index (remember that the index was
    0 for the latest record) in descending order.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从图中可以看到，最新的值首先被绘制（在左侧）。为了便于绘制和处理，我们将反转数据。我们将通过按索引（记住索引为最新记录的0）降序排列DataFrame来实现这一点。
- en: 'Reverse the data by sorting the DataFrame on the index. Plot the closing price
    again and supply `Date` as the *X-axis*:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过按索引排序DataFrame反转数据。重新绘制收盘价，并将`Date`作为*X轴*：
- en: '[PRE6]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The closing price will be plotted as follows:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 收盘价将按以下方式绘制：
- en: '![Figure 5.6: The trend after reversing the data'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图5.6：反转数据后的趋势'
- en: '](img/B15385_05_06.jpg)'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_05_06.jpg)'
- en: 'Figure 5.6: The trend after reversing the data'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.6：反转数据后的趋势
- en: That worked as expected. We can see that the latest values are plotted to the right.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这按预期工作。我们可以看到最新的值已被绘制到右侧。
- en: 'Extract the values for `Close` from the DataFrame as a `numpy` array, reshaped
    to specify one column using `array.reshape(-1,1)`:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从DataFrame中提取`Close`列的值，作为`numpy`数组，并通过`array.reshape(-1,1)`调整为一列：
- en: '[PRE7]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Plot the values as a line plot using matplotlib. Don''t worry about marking
    the dates; the order of the data is clear (matplotlib will use an index instead,
    beginning with 0 for the first point):'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用matplotlib绘制值的线形图。无需担心标记日期；数据的顺序是清楚的（matplotlib会使用索引，起始点为0）：
- en: '[PRE8]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The resulting trend is as follows, with the *X-axis* representing the index
    and the *Y-axis* showing the closing price:'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得到的趋势如下，*X轴*表示索引，*Y轴*显示收盘价：
- en: '![Figure 5.7: The daily stock price trend'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图5.7：每日股价趋势'
- en: '](img/B15385_05_07.jpg)'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_05_07.jpg)'
- en: 'Figure 5.7: The daily stock price trend'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：每日股价趋势
- en: That's what our sequence data looks like. There is no continuous clear trend;
    the prices rose for a period, after which the stock waxed and waned. The pattern
    isn't straightforward. We can see that there is some seasonality at a small duration
    (maybe monthly). Overall, the pattern is rather complex and there are no obvious
    and easy-to-identify cyclicities in the data that we can exploit. This complex
    sequence is what we will work with – predicting the stock price for a day using
    historical values.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的序列数据的样子。数据没有明显的连续趋势；价格曾在一段时间内上涨，之后股价波动不定。模式不简单。我们可以看到在短期内（可能是按月）有一定的季节性变化。总体来说，模式相当复杂，数据中没有明显且易于识别的周期性变化可以利用。这个复杂的序列就是我们要处理的——使用历史数据预测某一天的股价。
- en: Note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZctArW](https://packt.live/2ZctArW).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2ZctArW](https://packt.live/2ZctArW)。
- en: You can also run this example online at [https://packt.live/38EDOEA](https://packt.live/38EDOEA).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在[https://packt.live/38EDOEA](https://packt.live/38EDOEA)上在线运行此示例。必须执行整个Notebook才能获得期望的结果。
- en: In this exercise, we loaded the stock price data. After reversing the data for
    ease of handling, we extracted the closing price (the `Close` column). We plotted
    the data to visually examine the trend and patterns in the data, acknowledging
    that there aren't any obvious patterns in the data for us to exploit.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们加载了股价数据。为了便于处理，我们反转了数据，并提取了收盘价（`Close`列）。我们绘制了数据图，以直观地检查数据中的趋势和模式，意识到数据中并没有明显的模式可以利用。
- en: Note
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Whether you treat the data as a sequence also depends on the task at hand. If
    the task doesn't need the information in the sequence, then maybe you don't need
    to treat it as such.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 是否将数据视为序列也取决于当前的任务。如果任务不需要序列中的信息，那么可能就不需要将其视为序列。
- en: In this chapter, we'll be focusing on tasks that require/greatly benefit from
    exploiting the sequence in the data. How is that done? We'll find out in the following
    sections, where we'll discuss the intuition and the approach behind RNNs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将专注于那些需要或能从数据序列中获益的任务。这是如何实现的？我们将在接下来的章节中找出答案，讨论RNN背后的直觉和方法。
- en: Recurrent Neural Networks
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: 'How does our brain process a sentence? Let''s try to understand how our brain
    processes a sentence as we read it. You see some terms in a sentence, and you
    need to identify the sentiment contained in the sentence (positive, negative,
    neutral). Let''s look at the first term – "`I`":'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑如何处理一个句子？让我们试着理解一下我们的大脑在阅读句子时如何处理它。你看到句子中的一些词汇，然后需要识别句子中包含的情感（正面、负面、中性）。让我们来看第一个词汇——
    "`I`"：
- en: '![Figure 5.8 Sentiment analysis for the first term'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.8：第一个词汇的情感分析'
- en: '](img/B15385_05_08.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_08.jpg)'
- en: Figure 5.8 Sentiment analysis for the first term
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8：第一个词汇的情感分析
- en: '"`I`" is neutral, so our classification (neutral) is appropriate. Let''s look
    at another term:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '"`I`" 是中性的，因此我们的分类（中性）是合适的。让我们看一下下一个词汇：'
- en: '![Figure 5.9: Sentiment analysis with two terms'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.9：包含两个词汇的情感分析'
- en: '](img/B15385_05_09.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_09.jpg)'
- en: 'Figure 5.9: Sentiment analysis with two terms'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：包含两个词汇的情感分析
- en: 'With the term "`can''t`," we need to update our assessment of the sentiment.
    "`I`" and "`can''t`" together typically have a negative connotation, so our current
    assessment is updated as "negative" and is marked with a cross. Let''s look at
    the next couple of words:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 加入了 "`can't`" 这个词后，我们需要更新对情感的评估。"`I`" 和 "`can't`" 一起通常具有负面含义，因此我们当前的评估被更新为“负面”，并标记为一个叉号。让我们看一下接下来的几个词汇：
- en: '![Figure 5.10: Sentiment analysis with four terms'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.10：包含四个词汇的情感分析'
- en: '](img/B15385_05_10.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_10.jpg)'
- en: 'Figure 5.10: Sentiment analysis with four terms'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10：包含四个词汇的情感分析
- en: 'After the two additional terms, we maintain our prediction that the sentence
    has a negative sentiment. With all the information so far, "`I can''t find any`,"
    is a good assessment. Let''s look at the final term:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在增加两个词汇之后，我们仍然预测句子具有负面情感。根据目前的信息，"`I can't find any`" 是一个合理的判断。让我们看一下最后一个词汇：
- en: '![Figure 5.11: Sentiment analysis with the final term added'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11：添加最终词汇后的情感分析'
- en: '](img/B15385_05_11.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_11.jpg)'
- en: 'Figure 5.11: Sentiment analysis with the final term added'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11：添加最终词汇后的情感分析
- en: With that last term coming in, our prediction is completely overturned. Suddenly,
    we now agree that this is a positive expression. Your assessment is updated with
    each new term coming in, is it not? Your brain gathers/collects all the information
    it has at hand and makes an assessment. On arrival of the new term, the assessment
    so far is updated. This process is exactly what an RNN mimics.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 随着最后一个词汇的加入，我们的预测完全被推翻了。突然间，我们现在认为这是一个积极的表达。每加入一个新词汇，你的评估都会更新，是不是？你的大脑收集所有现有的信息，并做出评估。当新词汇到达时，当前的评估会被更新。这个过程正是
    RNN 模拟的过程。
- en: So, what makes a network "recurrent"? The key idea is to *not only process new
    information but also retain the information received so far*. This is achieved
    in RNNs by making the output depend not only on the new input value but also on
    the current "state" (information captured so far). To understand this better,
    let's see how a standard feedforward neural network would process a simple sentence
    and compare it with how an RNN would process it.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是什么使网络“具有递归性”？关键思想是*不仅处理新信息，还保留迄今为止接收到的信息*。在 RNN 中，通过使输出不仅依赖于新的输入值，还依赖于当前的“状态”（即迄今为止捕获的信息），从而实现这一点。为了更好地理解这一点，让我们看看标准的前馈神经网络如何处理一个简单的句子，并与
    RNN 的处理方式进行比较。
- en: 'Consider the task of sentiment classification (positive or negative) for an
    input sentence, "*life is good*." In a standard feedforward network, the inputs
    corresponding to all the terms in the sentence are passed to the network together.
    As depicted in the following diagram, the input data is the combined representation
    of all the terms in the sentence that have been passed to the hidden layers of
    the network. All the terms are considered together to classify the sentiment in
    the sentence as positive:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑情感分类任务（正面或负面），例如输入句子“*life is good*”。在标准前馈网络中，句子中所有词汇对应的输入会一起传递给网络。如下面的图示所示，输入数据是句子中所有词汇的组合表示，这些词汇已经传递到网络的隐藏层中。所有的词汇一起被考虑，用于将句子的情感分类为正面：
- en: '![Figure 5.12: Standard feedforward network for sentiment classification'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.12：用于情感分类的标准前馈网络'
- en: '](img/B15385_05_12.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_12.jpg)'
- en: 'Figure 5.12: Standard feedforward network for sentiment classification'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12：用于情感分类的标准前馈网络
- en: 'In contrast, an RNN would process the sentence word by word. As shown in the
    following diagram, the first input for the term "*life*" is passed to the hidden
    layers at time *t=0*. The hidden layers provide some output values, but this isn''t
    the final classification of the sentence and is rather an intermediate value of
    the hidden layers. No classification is done yet:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，RNN 会逐字处理句子。如以下图所示，词项 "*life*" 的第一个输入在 *t=0* 时传递给隐藏层。隐藏层提供了一些输出值，但这还不是句子的最终分类，而只是隐藏层的中间值。此时尚未进行分类：
- en: '![Figure 5.13: RNN processing the first term at time t=0'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.13: RNN 在 t=0 时处理第一个词项'
- en: '](img/B15385_05_13.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_13.jpg)'
- en: 'Figure 5.13: RNN processing the first term at time t=0'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.13: RNN 在 t=0 时处理第一个词项'
- en: 'The next term "`is`"), along with its corresponding input, is processed at
    time *t=1* and then fed to the hidden layers. As shown in the following diagram,
    this time, the hidden layer also considers the intermediate output from the hidden
    layer at time *t=0*, which is essentially the output corresponding to the term
    "`life`." The output from the hidden layers will now effectively consider the
    new input ("`is`") and the input at the previous time step ("`life`"):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下一项 "`is`" 及其对应的输入在 *t=1* 时被处理，并传递给隐藏层。如以下图所示，此时，隐藏层还会考虑 *t=0* 时隐藏层的中间输出，这实际上是对应于
    "`life`" 这一项的输出。此时，隐藏层的输出将有效地考虑新的输入（"`is`"）和前一时间步的输入（"`life`"）：
- en: '![Figure 5.14: The network at t=1'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.14: t=1 时的网络'
- en: '](img/B15385_05_14.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_14.jpg)'
- en: 'Figure 5.14: The network at t=1'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.14: t=1 时的网络'
- en: 'After time step *t=1*, the output of the hidden layers effectively contains
    information from the terms "`life`" and "`is`,", effectively holding information
    corresponding to the inputs so far. At time *t=2*, the data corresponding to the
    next term, that is, "`good`," is fed into the hidden layers. The following diagram
    shows that the hidden layers use this new input data, along with the output from
    hidden layers from time *t=1*, to provide an output. This output effectively considers
    all the inputs so far, in the order in which they appear in the input text. It
    is when the entire sentence is processed that the final classification of the
    sentence is made ("positive", in this case):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *t=1* 时步之后，隐藏层的输出有效地包含了来自 "`life`" 和 "`is`" 的信息，实际上保持了迄今为止的输入信息。在 *t=2* 时，下一项数据，即
    "`good`"，被传递到隐藏层。以下图所示，隐藏层将使用新的输入数据，以及来自 *t=1* 时隐藏层的输出，生成一个输出。这个输出有效地考虑了到目前为止的所有输入，并按照它们在输入文本中的出现顺序进行处理。直到整个句子被处理完毕，才会做出最终的分类（在此为“积极”）：
- en: '![Figure 5.15: Output at t=2 when the entire sentence is processed'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.15: 在 t=2 时处理整个句子的输出'
- en: '](img/B15385_05_15.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_15.jpg)'
- en: 'Figure 5.15: Output at t=2 when the entire sentence is processed'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.15: 在 t=2 时处理整个句子的输出'
- en: Loops – An Integral Part of RNNs
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环 – RNN 的核心部分
- en: 'A common part of RNNs is using "loops," as shown in the following diagram.
    By loops, we mean a mechanism of retaining the "state" value containing the information
    so far and using it along with the new input:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的一个常见部分是使用“循环”，如以下图所示。所谓循环，是指一种保留“状态”值的机制，包含迄今为止的信息，并将其与新的输入一起使用：
- en: '![Figure 5.16: RNNs depicted with a loop'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.16: 带循环的 RNN'
- en: '](img/B15385_05_16.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_16.jpg)'
- en: 'Figure 5.16: RNNs depicted with a loop'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.16: 带循环的 RNN'
- en: 'As shown in the following diagram, this is done by simply making a virtual
    copy of the hidden layer and using it at the next time step, that is, when processing
    the next input. If processing a sentence term by term, this would mean, for each
    term, saving the hidden layer output (time *t-1*), and when the new term comes
    in at time *t*, processing the hidden layer output (time *t*) along with its previous
    state (time *t-1*). That''s all there really is to it:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下图所示，这是通过简单地虚拟复制隐藏层并在下一个时间步使用它来完成的，即在处理下一个输入时。如果逐词处理句子，这意味着对于每个词项，保存隐藏层的输出（时间
    *t-1*），当新词项在时间 *t* 到来时，将处理隐藏层的输出（时间 *t*）及其前一状态（时间 *t-1*）。实际上，过程就是如此简单：
- en: '![Figure 5.17: Copying the hidden layer state'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.17: 复制隐藏层状态'
- en: '](img/B15385_05_17.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_17.jpg)'
- en: 'Figure 5.17: Copying the hidden layer state'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.17: 复制隐藏层状态'
- en: To make the workings of RNNs even more clear, let's expand the view from *Figure
    5.15*, where we saw how the input sentence is processed term by term. We'll understand
    how different an RNN is from a standard feedforward network.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地了解 RNN 的工作原理，让我们扩展视图，从*图 5.15*开始，我们可以看到输入句子是如何按词处理的。我们将理解 RNN 与标准前馈网络有何不同。
- en: 'The part highlighted by the dotted box should be familiar to you – it represents
    the standard feedforward network with hidden layers (rectangles with dotted lines).
    The data for an input flows from left to right across the depth of the network,
    using feedforward weights, WF, to provide an output -- exactly as in a standard
    feedforward network. The recurrent part is the flow of data from bottom to top,
    across the time steps:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 被虚线框突出显示的部分应该对你很熟悉——它代表了带有隐藏层（虚线矩形）的标准前馈网络。输入数据从左到右流经网络的深度，使用前馈权重 WF 来提供输出——这与标准的前馈网络完全相同。递归部分是数据从底部到顶部的流动，跨越时间步：
- en: '![Figure. 5.18: RNN architecture'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.18：RNN 架构'
- en: '](img/B15385_05_18.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_18.jpg)'
- en: 'Figure. 5.18: RNN architecture'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18：RNN 架构
- en: 'For all the hidden layers, the output propagates along the time dimension too,
    to the next time step. Alternately, for a hidden layer at time step *t* and depth
    *l*, the inputs are as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有隐藏层，输出也沿着时间维度传播到下一个时间步。或者，对于时间步 *t* 和深度 *l* 的隐藏层，输入如下：
- en: Data from the previous hidden layer at the same time step
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自同一时间步的前一个隐藏层的数据
- en: Data from the same hidden layer at the previous time step
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自前一个时间步的相同隐藏层的数据
- en: 'Have a good look at the preceding diagram to understand the workings of an
    RNN. The output from the hidden layer can be derived as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细查看前面的图示，以理解 RNN 的工作原理。隐藏层的输出可以通过以下方式推导出来：
- en: '![Figure 5.19: Calculating activations in an RNN'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.19：在 RNN 中计算激活值'
- en: '](img/B15385_05_19.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_19.jpg)'
- en: 'Figure 5.19: Calculating activations in an RNN'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19：在 RNN 中计算激活值
- en: The first part of the formula, *W*F(l)at(l-1), corresponds to the result of
    the feedforward calculation, that is, applying feedforward weights (*W*F) to the
    output (*a*t(l-1)) from the previous layer. The second part corresponds to the
    recurrent calculation, that is, applying recurrent weights (*W*R(l)) to the output
    from the same layer from the previous time step (*a*t-1(l)). Additionally, as
    with all neural network layers, there is a bias term as well. This result, on
    applying the activation function, becomes the output from the layer at time *t*
    and depth *l* (*a*t(l)).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 公式的第一部分，*W*F(l)at(l-1)，对应于前馈计算的结果，也就是将前馈权重 (*W*F) 应用于来自前一层的输出 (*a*t(l-1))。第二部分对应于递归计算，即将递归权重
    (*W*R(l)) 应用于来自前一个时间步的同一层的输出 (*a*t-1(l))。此外，与所有神经网络层一样，还有一个偏置项。应用激活函数后，该结果成为时间
    *t* 和深度 *l* 层的输出 (*a*t(l))。
- en: To make the idea more concrete, let's implement the feedforward steps of a simple
    RNN using TensorFlow.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个概念更加具体，我们将使用 TensorFlow 实现一个简单 RNN 的前馈步骤。
- en: 'Exercise 5.02: Implementing the Forward Pass of a Simple RNN Using TensorFlow'
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.02：使用 TensorFlow 实现简单 RNN 的前馈传递
- en: 'In this exercise, we will use TensorFlow to perform one pass of the operations
    in a simple RNN with one hidden layer and two time steps. By performing one pass,
    we mean calculating the activation of the hidden layer at time step *t=0*, then
    using this output along with the new input at *t=1* (applying the appropriate
    recurrent and feedforward weights) to obtain the output at time *t=1*. Initiate
    a new Jupyter Notebook for this exercise and perform the following steps:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用 TensorFlow 执行一个简单 RNN 的操作，其中有一个隐藏层和两个时间步。通过执行一次传递，我们的意思是计算时间步 *t=0*
    时隐藏层的激活值，然后使用该输出以及 *t=1* 时的新输入（应用适当的递归和前馈权重）来获得 *t=1* 时的输出。启动一个新的 Jupyter Notebook
    进行此练习并执行以下步骤：
- en: 'Import TensorFlow and NumPy. Set a random seed of `0` using `numpy` to make
    the results reproducible:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 TensorFlow 和 NumPy。使用 `numpy` 设置随机种子为 `0` 以使结果具有可复现性：
- en: '[PRE9]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the `num_inputs` and `num_neurons` constants that will be holding the
    number of inputs (2) and the number of neurons in the hidden layer (3), respectively:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `num_inputs` 和 `num_neurons` 常量，分别表示输入的数量（2）和隐藏层中神经元的数量（3）：
- en: '[PRE10]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We will have two inputs at each time step. Let's call them `xt0` and `xt1`.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在每个时间步，我们将有两个输入。我们将它们称为 `xt0` 和 `xt1`。
- en: 'Define the variables for the weight matrices. We need two of them – one for
    the feedforward weights and another for the recurrent weights. Initialize them randomly:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义权重矩阵的变量。我们需要两个——一个用于前馈权重，另一个用于递归权重。随机初始化它们：
- en: '[PRE11]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Notice the dimensions for the recurrent weights – it is a square matrix, with
    as many rows/columns as the number of neurons in the hidden layer.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意递归权重的维度——它是一个方阵，行/列的数量等于隐藏层神经元的数量。
- en: 'Add the bias variable (to make the activations fit the data better), with as
    many zeros as the number of neurons in the hidden layer:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加偏置变量（以使激活函数更好地拟合数据），其值为隐藏层神经元数量的零：
- en: '[PRE12]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create the data – three examples for `xt0` (two inputs, three examples) as
    `[[0,1], [2,3], [4,5]]` and `xt1` as `[[100,101], [102,103], [104,105]]` – as
    `numpy` arrays of the `float32` type (consistent with `dtype` for TensorFlow''s
    default float representation):'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建数据——`xt0` 的三个示例（两个输入，三个示例），为 `[[0,1], [2,3], [4,5]]`，`xt1` 为 `[[100,101],
    [102,103], [104,105]]`——作为 `numpy` 数组，类型为 `float32`（与 TensorFlow 默认浮动表示的 `dtype`
    一致）：
- en: '[PRE13]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Define a function named `forward_pass` to apply a forward pass to the given
    data, that is, `xt0`, `xt1`. Use `tanh` as the activation function. The output
    at *t=0* should be derived from `Wf` and `xt0` alone. The output at *t=1* must
    use `yt0` with the recurrent weights, `Wf`, and use the new input, `xt1`. The
    function should return outputs at the two time steps:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个名为 `forward_pass` 的函数，用于对给定数据（即 `xt0`，`xt1`）应用前向传播。使用 `tanh` 作为激活函数。时间 *t=0*
    时的输出应该仅由 `Wf` 和 `xt0` 得到。时间 *t=1* 时的输出必须使用 `yt0` 和递归权重 `Wf`，并且使用新的输入 `xt1`。该函数应返回两个时间步的输出：
- en: '[PRE14]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that there is no recurrent weight here at time step 0; it comes into play
    only after the first time step.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在时间步 0 时这里没有递归权重；它只会在第一个时间步之后才会起作用。
- en: 'Perform the forward pass by calling the `forward_pass` function with the created
    data (`xt0_batch`, `xt1_batch`) and put the output into variables, `yt0_output`
    and `yt1_output`:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `forward_pass` 函数并传入创建的数据（`xt0_batch`，`xt1_batch`），执行前向传播，并将输出存储在变量 `yt0_output`
    和 `yt1_output` 中：
- en: '[PRE15]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Print the output values, `yt0_output` and `yt1_output`, using the `print` function
    from TensorFlow:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `print` 函数打印输出值 `yt0_output` 和 `yt1_output`：
- en: '[PRE16]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output at *t=0* gets printed out like so. Note that this result could be
    slightly different for you because of the random initialization that''s done by
    TensorFlow:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 时间 *t=0* 时的输出如下所示。注意，由于 TensorFlow 执行随机初始化，您看到的结果可能会略有不同：
- en: '[PRE17]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, print the values of yt1_output:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，打印 `yt1_output` 的值：
- en: '[PRE18]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output at *t=1* gets is printed as follows. Again, this could be slightly
    different for you because of the random initial values, but all the values should
    be close to 1 or -1:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 时间 *t=1* 时的输出如下所示。再次说明，由于随机初始化值的不同，您看到的结果可能会略有不同，但所有值应接近 1 或 -1：
- en: '[PRE19]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We can see that the final output at time *t=1* is a 3x3 matrix – representing
    the outputs for the three neurons in the hidden layer for the three instances
    of data.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，时间 *t=1* 时的最终输出是一个 3x3 的矩阵——表示三种数据实例的隐藏层三个神经元的输出。
- en: Note
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZctArW](https://packt.live/2ZctArW).
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参阅 [https://packt.live/2ZctArW](https://packt.live/2ZctArW)。
- en: You can also run this example online at [https://packt.live/38EDOEA](https://packt.live/38EDOEA).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您也可以在 [https://packt.live/38EDOEA](https://packt.live/38EDOEA) 在线运行此示例。您必须执行整个
    Notebook 才能获得期望的结果。
- en: Note
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Despite having set the seeds for `numpy` as well as `tensorflow` to achieve
    reproducible results, there are a lot more causes for the variation in results.
    While the values you see may be different, the output you see should largely agree
    with ours.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管我们已为 `numpy` 和 `tensorflow` 设置了种子以实现可复现的结果，但仍然有许多因素会导致结果的变化。尽管您看到的值可能不同，但您看到的输出应该与我们的结果大致一致。
- en: In this exercise, we manually performed the forward pass for two time steps
    in a simple RNN. We saw that it's merely using the hidden layer output from the
    previous time step as an input to the next. Now, you don't really need to perform
    any of this manually – Keras makes making RNNs very simple. We will use Keras
    for our stock price prediction model.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们手动执行了一个简单 RNN 的两个时间步的前向传播。我们看到，它仅仅是将前一个时间步的隐藏层输出作为输入传递给下一个时间步。现在，您实际上不需要手动执行这些步骤——Keras
    使得构建 RNN 非常简单。我们将使用 Keras 来进行股票价格预测模型。
- en: The Flexibility and Versatility of RNNs
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN 的灵活性和多样性
- en: In *Exercise 5.2*, *Implementing the Forward Pass of a Simple RNN Using TensorFlow*,
    we used two inputs at each time step, and we had an output at each time step.
    But it doesn't always have to be that way. RNNs have a lot of flexibility to offer.
    For starters, you can have single/multiple inputs as well as outputs. Additionally,
    you needn't have inputs and outputs at each time step.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习 5.2*，*使用 TensorFlow 实现简单 RNN 的前向传递*中，我们在每个时间步使用了两个输入，并且每个时间步都有一个输出。但这并不总是必须的。RNN
    提供了很多灵活性。首先，你可以有单个/多个输入以及输出。此外，你不必在每个时间步都有输入和输出。
- en: 'You could have the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以有以下情况：
- en: Inputs at different time steps with the output only at the final step
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同时间步的输入，输出仅在最后一步得到
- en: A single input with outputs at multiple time steps
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个输入，多个时间步的输出
- en: Both inputs and outputs (equal or unequal lengths) at multiple time steps
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个时间步，输入和输出（长度相等或不等）
- en: 'There is enormous flexibility in RNN architectures, and this flexibility makes
    them very versatile. Let''s take a look at some possible architectures and what
    some potential applications can be:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: RNN架构具有巨大的灵活性，这种灵活性使其非常多才多艺。让我们看看一些可能的架构以及它们的一些潜在应用：
- en: '![Figure 5.20: Inputs at multiple steps with the output at the final step'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.20：多个时间步的输入，输出仅在最后一步得到'
- en: '](img/B15385_05_20.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_20.jpg)'
- en: 'Figure 5.20: Inputs at multiple steps with the output at the final step'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.20：多个时间步的输入，输出仅在最后一步得到
- en: You can have inputs at multiple time steps, such as in a sequence (or single
    or more inputs) with the output only at the final time step, when the prediction
    is made, as shown in the preceding diagram. At each time step, the hidden layers
    operate on the feedforward output from the previous layer and the recurrent output
    from its copy from the previous time step. But there is no prediction for the
    intermediate time steps. Prediction is made only after processing the entire input
    sequence – the same process we saw in *Figure. 5.15* (the "*life is good*" example).
    Text classification applications extensively use this architecture – sentiment
    classification into positive/negative, classifying an email into spam/ham, identifying
    hate speech in comments, automatically moderating product reviews on a shopping
    platform, and many more.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在多个时间步输入数据，例如在一个序列中（或一个或多个输入），并且输出仅在最后一个时间步生成预测，如前面的图所示。在每个时间步，隐藏层基于上一层的前馈输出和上一时间步副本的循环输出进行操作。但是在中间时间步并不会进行预测。预测只会在处理完整个输入序列后进行——这与我们在*图
    5.15*（"*生活是美好*"的例子）中看到的过程相同。文本分类应用广泛使用这种架构——情感分类（正面/负面）、将邮件分类为垃圾邮件/正常邮件、识别评论中的仇恨言论、自动审核购物平台上的产品评论等。
- en: 'Time series prediction (for example, stock prices) also utilizes this architecture,
    where the past few values are processed to predict a single future value:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列预测（例如，股票价格）也利用这种架构，其中处理过去的几个值来预测一个未来的值：
- en: '![Figure 5.21: Input in a single step, output in multiple steps'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.21：单步输入，多步输出'
- en: '](img/B15385_05_21.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_21.jpg)'
- en: 'Figure 5.21: Input in a single step, output in multiple steps'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.21：单步输入，多步输出
- en: The preceding diagram illustrates another architecture in which the input is
    received in a single step, but the output is obtained at multiple time steps.
    Applications around generation – generating images for a given keyword, generating
    music for a given keyword (composer), or generating a paragraph of text for a
    given keyword – are based on this architecture.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图示例了另一种架构，其中输入在一个时间步接收，但输出是在多个时间步得到的。围绕生成的应用——根据给定关键词生成图像、根据给定关键词生成音乐（作曲家）或根据给定关键词生成一段文本——都基于这种架构。
- en: You could also have an output at each time step corresponding to the input,
    as depicted in the following diagram. Essentially, this model will help you make
    a prediction for each incoming element of the sequence. An example of such a task
    would be the Parts-of-Speech tagging of terms – for each term in a sentence, we
    identify whether the term is a noun, verb, adjective, or another part of speech.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在每个时间步产生与输入相对应的输出，如下图所示。从本质上讲，这种模型将帮助你对序列中的每个输入元素进行预测。这样任务的一个例子是词性标注——对于句子中的每个词，我们识别该词是名词、动词、形容词还是其他词性。
- en: 'Another example from natural language processing would be **Named Entity Recognition**
    (**NER**) where, for each term in the text, the objective is to detect whether
    it represents a named entity and then classify it as an organization, person,
    place, or another category if it does:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 来自自然语言处理的另一个例子是**命名实体识别**（**NER**），在这里，对于文本中的每个词语，目标是检测它是否代表一个命名实体，然后将其分类为组织、人物、地点或其他类别（如果是的话）：
- en: '![Figure 5.22: Multiple outputs at each time step'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.22：每个时间步的多个输出'
- en: '](img/B15385_05_22.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_22.jpg)'
- en: 'Figure 5.22: Multiple outputs at each time step'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.22：每个时间步的多个输出
- en: In the previous architecture, we had an output for each incoming element. In
    many situations, this doesn't work, and we need an architecture that has different
    lengths for input and output, as shown in the following diagram. Think of translation
    between languages. Does a sentence in English necessarily have the same number
    of terms in its German translation? More often than not, the answer is no. For
    such cases, the architecture in the following diagram provides the notion of an
    "encoder" and a "decoder." The information corresponding to the input sequence
    is stored in the final hidden layer of the encoder network, which in itself has
    recurrent layers.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的架构中，我们为每个输入元素都提供了一个输出。在许多情况下，这种方法并不适用，我们需要一种架构，能够处理输入和输出长度不同的情况，如下图所示。想想语言之间的翻译。一句英语在德语中是否一定有相同数量的词汇？通常答案是否定的。对于这种情况，下面的架构提供了“编码器”和“解码器”的概念。输入序列对应的信息存储在编码器网络的最终隐藏层中，该层本身包含循环层。
- en: 'This representation/information is processed by the decoder network (again,
    this is recurrent), which outputs the translated sequence:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表示/信息由解码器网络处理（同样，这也是递归的），它输出翻译后的序列：
- en: '![Figure 5.23: Architecture with different lengths for input and output'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.23：输入和输出长度不同的架构'
- en: '](img/B15385_05_23.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_23.jpg)'
- en: 'Figure 5.23: Architecture with different lengths for input and output'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.23：输入和输出长度不同的架构
- en: For all of these architectures, you could also have multiple inputs, making
    RNN models even more versatile. For example, when making stock price predictions,
    you could provide multiple inputs (previous stock prices of company, the stock
    exchange index, crude oil price, and whatever you think is relevant) over multiple
    time steps, and the RNN will be able to accommodate and utilize all of these.
    This is one of the reasons RNNs are very popular and have changed the way we work
    with sequences today. Of course, you also have all the predictive power of deep
    learning to add.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有这些架构，你还可以有多个输入，使得RNN模型更加多功能。例如，在进行股票价格预测时，你可以在多个时间步长内提供多个输入（公司过去的股价、股市指数、原油价格以及你认为相关的任何其他信息），RNN将能够处理并利用所有这些输入。这也是RNN广受欢迎的原因之一，它们改变了我们今天处理序列数据的方式。当然，你还可以利用深度学习的所有预测能力。
- en: Preparing the Data for Stock Price Prediction
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 股票价格预测数据的准备
- en: For our stock price prediction task, we will predict the value of a given stock
    on any day by using the past few days' data and feeding it to an RNN. Here, we
    have a single input (single feature), over multiple time steps, and a single output.
    We will employ the RNN architecture from *Figure 5.20*.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的股票价格预测任务，我们将使用过去几天的数据，通过输入到RNN中来预测某一天给定股票的价值。在这里，我们有一个单一的输入（单一特征），跨越多个时间步，并且只有一个输出。我们将采用*图5.20*中的RNN架构。
- en: Note
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: Continue in the same Jupyter Notebook that we plotted our time-series data in
    throughout this chapter (unless specified otherwise).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续使用同一个Jupyter Notebook来绘制我们的时间序列数据（除非另有说明）。
- en: 'So far, we''ve looked at the data and understood what we''re dealing with.
    Next, we need to prepare the data for the model. The first step is to create a
    train-test split of the data. Since this is time-series data, we can''t just randomly
    pick points to assign to our train and test sets. We need to maintain the sequence.
    For time-series data, we typically reserve the first portion of the data to train
    on and utilize the last part of the data for our test set. In our case, we will
    take the first 75% records as our training data and the last 25% as our test data.
    The following command will help us get the size of the train set needed:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经查看了数据并理解了我们处理的内容。接下来，我们需要为模型准备数据。第一步是对数据进行训练集和测试集的划分。由于这是时间序列数据，我们不能随意选择点来分配训练集和测试集。我们需要保持顺序。对于时间序列数据，我们通常保留数据的前一部分用于训练，最后一部分用于测试。在我们的案例中，我们将前75%的记录作为训练数据，最后25%的记录作为测试数据。以下命令将帮助我们获得所需的训练集大小：
- en: '[PRE20]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This is the number of records we''ll have in the train set. We can separate
    the sets as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在训练集中的记录数量。我们可以按以下方式分割数据集：
- en: '[PRE21]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The lengths of the train and test sets will be as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和测试集的长度如下所示：
- en: '[PRE22]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we need to scale the stock data. For that, we can employ the min-max
    scaler from `sklearn`. The `MinMaxScaler` scales the data so that it''s in a range
    between 0 and 1 (inclusive) – the highest value in the data being mapped to 1\.
    We''ll fit and transform the scaler on the train data and only transform the test
    data:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要对股票数据进行缩放。为此，我们可以使用`sklearn`中的min-max缩放器。`MinMaxScaler`将数据缩放到0到1（包括0和1）之间——数据中的最高值映射为1。我们将在训练数据上拟合并转换缩放器，然后只对测试数据进行转换：
- en: '[PRE23]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The next important step is to format the data to get the "features" for each
    instance. We need to define a "lookback period" – the number of days from the
    history that we want to use to predict the next value. The following code will
    help us define a function that returns the target value of `y` (stock price for
    a day) and `X` (values for each day in the lookback period):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个重要步骤是格式化数据，以获取每个实例的“特征”。我们需要定义一个“回溯期”——即从历史数据中用来预测下一个值的天数。以下代码将帮助我们定义一个返回目标值`y`（某一天的股票价格）和`X`（回溯期内每天的值）的函数：
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The function takes in a dataset (a series of numbers, rather) and, for the
    provided lookback, adds as many values from the history. It does so by shifting
    the series, each time concatenating it to the result. The function returns the
    stock price for the day as *y* and the values in lookback period (shifted values)
    as our features. Now, we can define a lookback period and see the result of applying
    the function to our data:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接收一个数据集（实际上是一个数字序列），并根据提供的回溯期，添加历史中的相应值。它通过每次移动序列并将其与结果连接来完成此操作。该函数返回当天的股票价格作为*y*，以及回溯期内的值（已偏移的值）作为我们的特征。现在，我们可以定义一个回溯期，并查看将该函数应用于我们的数据后的结果：
- en: '[PRE25]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Try the following command to examine the shape of the outcome datasets:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试以下命令来检查结果数据集的形状：
- en: '[PRE26]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE27]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As expected, there are 10 features for each example, corresponding to the past
    10 days. We have this history for the train data as well as the test data. With
    that, data preparation is complete. Before we move on to building our first RNN
    on this data, let's understand RNNs a little more.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，每个样本有10个特征，分别对应过去10天的数据。我们拥有训练数据和测试数据的历史数据。有了这些，数据准备工作就完成了。在开始构建第一个RNN模型之前，让我们进一步了解一下RNN。
- en: Note
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `trainX` and `trainY` variables we created here will be used throughout
    the exercises that follow. So, make sure you are running this chapter's code in
    the same Jupyter Notebook.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里创建的`trainX`和`trainY`变量将在接下来的练习中使用。所以，请确保你在同一个Jupyter Notebook中运行本章节的代码。
- en: Parameters in an RNN
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN中的参数
- en: 'To calculate the number of parameters in an RNN layer, let''s take a look at
    a generic hidden layer:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算RNN层中的参数数量，我们来看看一个通用的隐藏层：
- en: '![Figure 5.24: Parameters of the recurrent layer'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.24：递归层的参数'
- en: '](img/B15385_05_24.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_24.jpg)'
- en: 'Figure 5.24: Parameters of the recurrent layer'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.24：递归层的参数
- en: The hidden layer takes inputs from the previous hidden layer at the same time
    step, and also from itself from a previous time step. If the input layer (the
    previous hidden layer) to the RNN layer is m-dimensional, we would need *n×m*
    weights/parameters, where *n* is the number of neurons in the RNN layer. For the
    output layer, the dimensionality if the weights would be *n×k*, if *k* is the
    dimensionality of the output. The recurrent weight is always a square matrix of
    dimensionality *n×n* – since the dimensionality of the input is the same as the
    layer itself.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层从前一个隐藏层的同一时间步获取输入，同时也从自身的先前时间步获取输入。如果输入层（前一个隐藏层）是m维的，那么我们需要*n×m*个权重/参数，其中*n*是RNN层中神经元的数量。对于输出层，如果输出的维度是*k*，则权重的维度为*n×k*。递归权重始终是一个维度为*n×n*的方阵——因为输入的维度与该层本身相同。
- en: 'The number of parameters for any RNN layer would therefore be `n`2 `+ nk +
    nm`, where we have the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，任何RNN层的参数数量为`n`² `+ nk + nm`，其中我们有以下内容：
- en: '`n`: Dimension of the hidden (current) layer'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n`：隐藏（当前）层的维度'
- en: '`m`: Dimension of the input layer'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`m`：输入层的维度'
- en: '`k`: Dimension of the output layer'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k`：输出层的维度'
- en: Training RNNs
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练RNN
- en: 'How to forward propagate information in an RNN should be clear by now. If not,
    please refer to *Figure 5.19* with the equations. The new information propagates
    along the depth of the network as well along the time steps, using the previous
    hidden states at each step. The additional two key aspects of training RNNs are
    as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在RNN中前向传播信息现在应该已经很清楚。如果不清楚，请参考*图5.19*中的方程式。新的信息沿着网络的深度以及时间步传播，使用每个步骤的前一个隐藏状态。训练RNN的另外两个关键方面如下：
- en: '**Defining loss**: We know how loss is defined for a standard neural network;
    that is, it has a single output. With RNNs, in the case that there is a single
    time step at the output (for example, text classification), the loss is calculated
    the same way as in standard neural networks. But we know that RNNs could have
    outputs over multiple time steps (for example, in Part-of-Speech tagging or machine
    translation). How is loss defined across multiple time steps? A very simple and
    popular approach is summing up the loss at all the steps. The loss for the entire
    sequence is calculated as the sum of the loss at all time steps.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义损失**：我们知道如何为标准神经网络定义损失；也就是说，它只有一个输出。对于RNN，如果输出是单一时间步（例如，文本分类），则损失的计算方式与标准神经网络相同。但我们知道，RNN可以在多个时间步上有输出（例如，在词性标注或机器翻译中）。那么，如何在多个时间步上定义损失呢？一种非常简单且流行的方法是将所有步骤的损失加总。整个序列的损失计算为所有时间步损失的总和。'
- en: '**Backpropagation**: Backpropagation of the errors now needs us to work across
    time steps, since there is a time dimension as well. We have already seen that
    loss is defined as the sum of loss at each time step. The usual chain rule application
    helps us out; we also need to sum the gradients at each time step over time. This
    has a very catchy name: **Backpropagation Through Time** (**BPTT**).'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播**：反向传播误差现在需要我们跨越时间步进行，因为还有时间维度。我们已经看到，损失被定义为每个时间步的损失之和。通常的链式法则应用有助于我们解决这个问题；我们还需要将每个时间步的梯度在时间上进行求和。这个过程有一个非常吸引人的名字：**通过时间的反向传播**（**BPTT**）。'
- en: Note
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: A detailed treatment of the training process and the involved math is beyond
    the scope of this book. The basic concept is all we need to understand the considerations
    involved.
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练过程的详细处理和涉及的数学超出了本书的范围。我们需要理解的基本概念就足够了。
- en: Now, let's continue building our first RNN model using Keras. We will introduce
    two new layers that are available in Keras in this chapter and understand their
    function and utility. The first layer we need is the `SimpleRNN` layer.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续使用Keras构建我们的第一个RNN模型。在本章中，我们将介绍Keras中可用的两个新层，并理解它们的功能和用途。我们需要的第一个层是`SimpleRNN`层。
- en: 'To import all the necessary utilities from Keras, you can use the following
    code:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 要从Keras导入所有必要的工具，您可以使用以下代码：
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The SimpleRNN layer is the simplest plain vanilla RNN layer. It takes in a sequence,
    and the output of the neuron is fed back as input. Additionally, if we want to
    follow this RNN layer with another RNN layer, we have the option of returning
    sequences as output. Let's have a look at some of the options.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: SimpleRNN层是最简单的基本RNN层。它接受一个序列，并将神经元的输出反馈作为输入。此外，如果我们希望在这个RNN层后跟随另一个RNN层，我们可以选择将序列作为输出。让我们来看一下其中的一些选项。
- en: '`?SimpleRNN`: The signature for the SimpleRNN layer is as follows:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`?SimpleRNN`：SimpleRNN 层的签名如下：'
- en: '![Figure 5.25: Signature of the SimpleRNN layer'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.25：SimpleRNN 层的签名](img/B15385_05_25.jpg)'
- en: '](img/B15385_05_25.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_25.jpg)'
- en: 'Figure 5.25: Signature of the SimpleRNN layer'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.25：SimpleRNN 层的签名
- en: We can see that the layer also has all the usual options of regular/standard
    layers in Keras that let you specify the activations, initialization, dropout,
    and more.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，Keras 的层也具有所有常规/标准层的选项，可以让你指定激活函数、初始化、丢弃等。
- en: The RNN layers expect the input data to be in a certain format. Since we can
    have input data as multiple time steps for multiple features, the input format
    is expected to make that specification unambiguous. The expected input shape is
    (look_back, number of features). It expects a matrix with the same lookback history
    for each feature.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 层期望输入数据具有特定格式。由于我们可能有多个特征的多时间步输入数据，输入格式应该能够明确表达这一要求。期望的输入形状是 (look_back,
    特征数量)。它期望每个特征都有相同的回溯历史。
- en: In our case, we have one feature, and the lookback period is 10\. So, the expected
    input shape is (10, 1). Note that we currently have each input as a list of 10
    values, so we need to make sure it is understood as (10,1). We'll use the reshape
    layer for this purpose. The reshape layer needs the input shape and the target
    shape. Let's start building our model by instantiating and adding a reshape layer.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们只有一个特征，回溯期为 10。因此，预期的输入形状是 (10, 1)。请注意，我们当前每个输入都是一个包含 10 个值的列表，所以我们需要确保它被理解为
    (10, 1)。我们将使用重塑层来完成此操作。重塑层需要输入形状和目标形状。让我们通过实例化并添加重塑层来开始构建模型。
- en: Note
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Even though we have set the seeds for `numpy` as well as `tensorflow` to achieve
    reproducible results, there are a lot more causes for variation owing to which
    you may get a result that's different from ours. This applies to all the models
    we'll use here. While the values you see may be different, the output you see
    should largely agree with ours. If the model performance is very different, you
    may want to tweak the number of epochs – the reason for this being that the weights
    in neural networks are initialized randomly, so the starting point for you and
    us could be slightly different, and we may reach a similar position when training
    a different number of epochs.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们已经设置了 `numpy` 和 `tensorflow` 的种子以实现可重复的结果，但仍然有许多变化的因素，因此你可能会得到与我们不同的结果。这适用于我们将在此处使用的所有模型。虽然你看到的值可能不同，但你看到的输出应该与我们大致相符。如果模型表现差异很大，你可能需要调整训练轮数
    —— 原因在于神经网络中的权重是随机初始化的，因此你和我们可能从不同的起点开始，经过不同的轮次训练后可能会达到相似的状态。
- en: 'Exercise 5.03: Building Our First Plain RNN Model'
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.03：构建我们的第一个普通 RNN 模型
- en: 'In this exercise, we will build our first plain RNN model. We will have a reshape
    layer, followed by a `SimpleRNN` layer, followed by a dense layer for the prediction.
    We will use the formatted data for `trainX` and `trainY` that we created earlier,
    along with the initialized layers from Keras. Perform the following steps to complete
    this exercise:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将构建我们的第一个普通 RNN 模型。我们将有一个重塑层，接着是一个 `SimpleRNN` 层，最后是一个用于预测的全连接层。我们将使用之前创建的格式化数据
    `trainX` 和 `trainY`，以及从 Keras 初始化的层。请执行以下步骤来完成此练习：
- en: 'Let''s gather the necessary utilities from Keras. Use the following code to
    do so:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Keras 中收集必要的工具。使用以下代码进行操作：
- en: '[PRE29]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Instantiate the `Sequential` model:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化 `Sequential` 模型：
- en: '[PRE30]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Add a `Reshape` layer to get the data in the format (`look_back`, `1`):'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个 `Reshape` 层，以使数据符合 (`look_back`, `1`) 的格式：
- en: '[PRE31]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note the arguments to the `Reshape` layer. The target shape is (`lookback, 1`),
    as we discussed.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意 `Reshape` 层的参数。目标形状是（`lookback, 1`），正如我们所讨论的。
- en: 'Add a `SimpleRNN` layer with 32 neurons and specify the input shape. Note that
    we took an arbitrary number of neurons, so you''re welcome to experiment with
    this number:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个具有 32 个神经元的 `SimpleRNN` 层，并指定输入形状。请注意，我们选择了一个任意数量的神经元，因此你可以尝试不同的数量：
- en: '[PRE32]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Add a `Dense` layer of size 1:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个大小为 1 的 `Dense` 层：
- en: '[PRE33]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Add an `Activation` layer with a linear activation:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个 `Activation` 层，并使用线性激活函数：
- en: '[PRE34]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Compile the model with the `adam` optimizer and `mean_squared_error` (since
    we''re predicting a real-values quantity):'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `adam` 优化器和 `mean_squared_error`（因为我们预测的是一个实数值量）来编译模型：
- en: '[PRE35]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Print a summary of the model:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印模型的摘要：
- en: '[PRE36]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The summary will be printed as follows:'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摘要将如下所示：
- en: '![Figure 5.26: Summary of the SimpleRNN model'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.26：简单RNN模型的总结](img/B15385_05_25.jpg)'
- en: '](img/B15385_05_26.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_26.jpg)'
- en: 'Figure 5.26: Summary of the SimpleRNN model'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.26：SimpleRNN模型总结
- en: Pay attention to the number of parameters in the `SimpleRNN` layer. It works
    out to be as we expected.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意`SimpleRNN`层中的参数数量。结果正如我们预期的那样。
- en: Note
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZctArW](https://packt.live/2ZctArW).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2ZctArW](https://packt.live/2ZctArW)。
- en: You can also run this example online at [https://packt.live/38EDOEA](https://packt.live/38EDOEA).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在线运行这个示例，网址为[https://packt.live/38EDOEA](https://packt.live/38EDOEA)。您必须执行整个Notebook才能获得预期结果。
- en: In this exercise, we defined our model architecture using a single-layer plain
    RNN architecture. This is indeed a very simple model, in comparison to the kinds
    of models we built earlier for image data. Next, let's see how this model performs
    on the task at hand.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们使用单层普通RNN架构定义了我们的模型结构。与我们之前为图像数据构建的模型相比，这确实是一个非常简单的模型。接下来，让我们看看这个模型在当前任务中的表现如何。
- en: Model Training and Performance Evaluation
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练和性能评估
- en: 'We have defined and compiled the model. The next step is to learn the model
    parameters by fitting the model on the train data. We can do this by using a batch
    size of 1 and a validation split of 10%, and by training for only three epochs.
    We tried different values of epochs and found that the model gave the best result
    at three epochs. The following code will help us train the model using the `fit()`
    method:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义并编译了模型。下一步是通过在训练数据上拟合模型来学习模型参数。我们可以使用批量大小为1，验证集比例为10%，并且只训练三轮（epoch）。我们尝试了不同的轮次值，发现三轮时模型效果最佳。以下代码将帮助我们使用`fit()`方法训练模型：
- en: '[PRE37]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.27: Training output'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.27：训练输出'
- en: '](img/B15385_05_27.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_27.jpg)'
- en: 'Figure 5.27: Training output'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.27：训练输出
- en: We can see that the loss is already pretty low. We trained the model here without
    doing any careful hyperparameter tuning. You can see that for this dataset, three
    epochs was sufficient, and we're trying to keep it simple here. With the model
    training done, we now need to assess the performance on the train and test sets.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，损失值已经相当低了。我们在这里训练了模型，并没有做任何仔细的超参数调优。可以看到，对于这个数据集，三轮就足够了，而且我们这里尽量保持简单。模型训练完成后，我们现在需要评估在训练集和测试集上的表现。
- en: 'To make our code a little more modular, we''ll define two functions – one to
    print the RMS error on the train and test sets and the other function to plot
    the predictions for the test data along with the original values in the data.
    Let''s begin by defining our first function, using the `sqrt` function from `math`
    to get the root of the `mean_squared_error` provided to us by the model''s `evaluate`
    method. The function definition is as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的代码更模块化，我们将定义两个函数——一个用于打印训练集和测试集上的RMS误差，另一个函数用于绘制测试数据的预测值以及数据中的原始值。我们首先定义第一个函数，使用`math`中的`sqrt`函数来获取模型`evaluate`方法提供的`mean_squared_error`的平方根。函数定义如下：
- en: '[PRE38]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'To see how our model did, we need to supply our `model` object to this method.
    This can be done as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看我们的模型表现如何，我们需要将`model`对象提供给此方法。可以按如下方式进行：
- en: '[PRE39]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output will be as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE40]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The values seem rather low (admittedly, we don''t really have a benchmark here,
    but these values do seem to be good considering that our outcome values are ranging
    from 0 to 1). But this is a summary statistic, and we already know that the values
    in the data change considerably. A better idea would be to visually assess the
    performance, comparing the actual values to the predicted for the test period.
    The following code will help us define a function that plots the predictions for
    a given model object:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值看起来相当低（坦率地说，我们这里没有真正的基准，但考虑到我们的输出值范围从0到1，这些值似乎是不错的）。但这是一个汇总统计值，我们已经知道数据中的值变化相当大。更好的做法是通过视觉评估模型的表现，将实际值与测试期的预测值进行比较。以下代码将帮助我们定义一个函数，用来绘制给定模型对象的预测值：
- en: '[PRE41]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'First, the function makes predictions on the test data. Since this data is
    scaled, we apply the inverse transform to get the data back to its original scale
    before plotting it. The function plots the actual values as a solid line and the
    predicted values as dotted lines. Let''s use this function to visually assess
    how our model performs. We need to simply pass the model object to the `plot_pred`
    function, as demonstrated in the following code:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，函数对测试数据进行预测。由于这些数据已经进行了缩放，因此在绘制图表之前，我们需要应用逆变换将数据恢复到原始尺度。该函数以实线绘制实际值，虚线绘制预测值。让我们使用这个函数来直观评估模型的表现。我们只需将模型对象传递给`plot_pred`函数，如以下代码所示：
- en: '[PRE42]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The plot that''s displayed is as follows:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 显示的图表如下所示：
- en: '![Figure 5.28: Predictions versus actuals'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.28：预测值与实际值的对比'
- en: '](img/B15385_05_28.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_28.jpg)'
- en: 'Figure 5.28: Predictions versus actuals'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.28：预测值与实际值的对比
- en: The preceding diagram visualizes the predictions (dotted lines) from the model
    juxtaposed with the actual values (solid lines). That looks pretty good, doesn't
    it? At this scale, it looks like overlap between the predicted and the actual
    is very high – the prediction curve fits the actual values almost perfectly. Prima
    facie, it does seem that the model has done a great job.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了模型的预测值（虚线）与实际值（实线）的对比。看起来不错，对吧？在这个尺度下，预测值和实际值几乎完全重叠——预测曲线几乎完美地拟合了实际值。乍一看，似乎模型表现得非常好。
- en: 'But before congratulating ourselves, let''s recall the granularity at which
    we worked – we''re working with 10 points to predict the next day''s stock price.
    Of course, at this scale, even if we took simple averages, the plot would look
    impressive. We need to zoom in, a lot, to understand this better. Let''s zoom
    in so that the individual points are visible. We''ll use the `%matplotlib notebook`
    cell magic command for interactivity in the chart and zoom in on the values corresponding
    to indices `2400` – `2500` in the plot:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们自我庆祝之前，让我们回顾一下我们工作的粒度——我们正在使用10个数据点来预测第二天的股价。当然，在这个尺度下，即使我们仅仅取简单的平均值，图表看起来也会很惊人。我们需要大大放大，以便更好地理解。让我们放大一下，使得各个数据点能够清晰可见。我们将使用`%matplotlib
    notebook`单元魔法命令来使图表具有互动性，并放大图表中对应于索引`2400`至`2500`的数值：
- en: '[PRE43]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If the graph presented below is not displayed properly for some reason, run
    the cell containing `%matplotlib notebook` for a couple of times. Alternatively,
    you can also use `%matplotlib inline` instead of `%matplotlib notebook`.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 如果下面呈现的图表因某种原因未正确显示，请多运行几次包含`%matplotlib notebook`的单元。或者，你也可以使用`%matplotlib
    inline`来代替`%matplotlib notebook`。
- en: 'The output is as follows, with the dotted lines showing the predictions and
    the solid line depicting the actual values:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下，虚线表示预测值，实线表示实际值：
- en: '![Figure 5.29: Zoomed-in view of predictions'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.29：预测的放大视图'
- en: '](img/B15385_05_29.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_29.jpg)'
- en: 'Figure 5.29: Zoomed-in view of predictions'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.29：预测的放大视图
- en: Even after zooming in, the result is pretty good. All the variations have been
    captured well. A single RNN layer with just 32 neurons giving us this kind of
    result is great. Those who have worked with time series prediction using classical
    methods would be elated (as we were) to see the efficacy of RNNs for this task.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在放大后，结果依然相当不错。所有的变化都被很好地捕捉到。仅使用一个含32个神经元的RNN层就能获得这样的结果，非常不错。那些使用传统方法进行时间序列预测的人会对RNN在这个任务中的效果感到兴奋（就像我们一样）。
- en: We saw what RNNs are and, through our stock price prediction model, also saw
    the predictive power of even a very simple model for a sequence prediction task.
    We mentioned earlier that using an RNN is one approach to sequence processing.
    There is another noteworthy approach to handling sequences that employs convolutions.
    We'll explore it in the next section.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了什么是RNN，并通过我们的股价预测模型，看到即便是一个非常简单的模型也能在序列预测任务中展现出强大的预测能力。我们之前提到，使用RNN是一种处理序列的方式。还有另一种值得注意的处理序列的方法，那就是使用卷积。我们将在下一节中进行探讨。
- en: 1D Convolutions for Sequence Processing
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一维卷积用于序列处理
- en: 'In the previous chapters, you saw how deep neural networks benefit from convolutions
    – you saw convnets and how they can be used for working with images, and how they
    help with the following:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你已经看到深度神经网络是如何通过卷积受益的——你了解了卷积神经网络（ConvNets）以及它们是如何用于图像处理的，并且你看到了它们如何帮助解决以下问题：
- en: Reducing the number of parameters
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少参数的数量
- en: Learning the "local features" for the image
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习图像的“局部特征”
- en: 'Interestingly, and this is something that is not very obvious, convnets can
    also be very helpful for sequence processing tasks. Instead of 2D, we could use
    1D convolutions for sequence data. How does 1D convolution work? Let''s take a
    look:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这一点可能不太明显，卷积神经网络在序列处理任务中也能发挥很大的作用。与 2D 卷积不同，我们可以对序列数据使用 1D 卷积。1D 卷积是如何工作的呢？让我们来看一下：
- en: '![Figure 5.30: Feature generation using 1D convolutions'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.30：使用 1D 卷积生成特征'
- en: '](img/B15385_05_30.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_30.jpg)'
- en: 'Figure 5.30: Feature generation using 1D convolutions'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.30：使用 1D 卷积生成特征
- en: In *Chapter 3*, *Image Classification with Convolutional Networks*, we saw how
    a filter works for the case of images, extracting "patches" from the input image
    to provide us with output "features." In the case of 1D, a filter extracts subsequences
    from the input sequence and multiplies them by the weights to give us a value
    for the output features. As shown in the preceding diagram, the filter moves from
    the beginning to the end of the sequence (top to bottom). This way, the 1D convnet
    extracts local patches. As in the 2D case, the patches/features learned here can
    be recognized later in a different position in the sequence. Of course, as with
    2D convolutions, you can choose the filter size and the stride for 1D convolutions
    as well. If used with a stride more than 1, the 1D convnet can also significantly
    reduce the number of features.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 3 章*，*使用卷积神经网络进行图像分类*中，我们看到过滤器在图像的情况下如何工作，从输入图像中提取“补丁”，并为我们提供输出“特征”。在 1D
    的情况下，过滤器从输入序列中提取子序列，并将它们与权重相乘，以给出输出特征的值。如前图所示，过滤器从序列的开始到结束（从上到下）移动。这样，1D 卷积神经网络就提取了局部补丁。与
    2D 情况一样，在这里学到的补丁/特征也可以在序列中的不同位置识别出来。当然，和 2D 卷积一样，你也可以选择 1D 卷积的过滤器大小和步幅。如果使用步幅大于
    1，1D 卷积神经网络也可以显著减少特征的数量。
- en: Note
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When employed on text data as the first layer, the "local features" that 1D
    convolutions extract are features for groups of words. A filter size of 2 would
    help extract two-word combos (called bi-grams), 3 would extract three-word combos
    (tri-grams), and so on. Larger filter sizes would learn larger groups of terms.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 当作为第一层应用于文本数据时，1D 卷积提取的“局部特征”是针对词组的特征。过滤器大小为 2 时，可以提取二元词组（bi-grams），为 3 时提取三元词组（tri-grams），依此类推。较大的过滤器大小会学习更大的词组。
- en: You could also apply pooling to 1D – max or average pooling to further subsample
    the features. So, you could greatly reduce the effective length of sequence that
    you're dealing with. A long input sequence can be brought down to a much smaller,
    more manageable length. This should certainly help with speed.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以对 1D 数据应用池化操作——最大池化或平均池化，以进一步对特征进行下采样。这样，你可以显著减少处理序列时的有效长度。长输入序列可以被缩减到更小、更易管理的长度。这无疑有助于提高速度。
- en: We understand that we benefit in speed. But do 1D convnets perform well for
    sequences? 1D convnets have shown very good results in tasks around translation
    and text classification. They have also shown great results for audio generation
    and other tasks regarding predicting from sequences.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道这在速度上带来了好处。但 1D 卷积神经网络（convnets）在处理序列时表现如何呢？1D 卷积神经网络在翻译和文本分类等任务中取得了非常好的结果。它们在音频生成和其他基于序列预测的任务中也表现出色。
- en: Will 1D convnets perform well for our task of stock price prediction? Ponder
    it – think about what kind of features we get and how we're handling the sequence.
    If you aren't sure, then don't worry – we're going to employ a 1D convnet-based
    model for our task and see for ourselves in the next exercise.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 1D 卷积神经网络在我们进行股票价格预测的任务中表现如何？想一想——考虑一下我们能获得什么样的特征，以及我们是如何处理序列的。如果你不确定，那也没关系——我们将使用基于
    1D 卷积神经网络的模型来完成任务，并在接下来的练习中亲自验证。
- en: 'Exercise 5.04: Building a 1D Convolution-Based Model'
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.04：构建基于 1D 卷积的模型
- en: 'In this exercise, we will build our first model using 1D convnets and evaluate
    its performance. We''ll employ a single `Conv1D` layer, followed by `MaxPooling1D`.
    We''ll continue using the same dataset and notebook we''ve been using so far.
    Perform the following steps to complete this exercise:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将构建第一个基于 1D 卷积神经网络的模型，并评估其性能。我们将使用一个 `Conv1D` 层，后跟 `MaxPooling1D`。我们将继续使用之前的同一数据集和笔记本。请执行以下步骤以完成这个练习：
- en: 'Import the 1D convolution-related layers from Keras:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Keras 中导入与 1D 卷积相关的层：
- en: '[PRE44]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Initialize a `Sequential` model and add a `Reshape` layer to reshape each instance
    as a vector (`look_back, 1`):'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个 `Sequential` 模型，并添加一个 `Reshape` 层，将每个实例重塑为一个向量（`look_back, 1`）：
- en: '[PRE45]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Add a Conv1D layer with five filters of size 5 and `relu` as the activation
    function:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个包含五个滤波器（大小为 5）并使用`relu`作为激活函数的 Conv1D 层：
- en: '[PRE46]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Note that we're using fewer filters than the sequence length. In many other
    applications, the sequence can be much longer than in our example. The filters
    are generally much lower in number than the input sequence.
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们使用的滤波器比序列长度少。在许多其他应用中，序列可能远长于我们的示例。滤波器的数量通常远低于输入序列的长度。
- en: 'Add a Maxpooling1D layer with a pool size of 5:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个池化大小为 5 的 Maxpooling1D 层：
- en: '[PRE47]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Flatten the output with a `Flatten` layer:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Flatten`层将输出展平：
- en: '[PRE48]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Add a `Dense` layer with a single neuron and add a linear activation layer:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个包含单个神经元的`Dense`层，并添加一个线性激活层：
- en: '[PRE49]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Print out the summary of the model:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出模型的摘要：
- en: '[PRE50]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The model''s summary is as follows:'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型的摘要如下：
- en: '![Figure 5.31: Summary of the model'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.31: 模型摘要'
- en: '](img/B15385_05_31.jpg)'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_05_31.jpg)'
- en: 'Figure 5.31: Summary of the model'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 5.31: 模型摘要'
- en: Notice the dimensions of the output from the Conv1D layer – 6 x 5\. This is
    expected – for a filter size of 5, we get 6 features. Also, take a look at the
    overall number of parameters. It's just 36, which is indeed a very small number.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意 Conv1D 层输出的维度——6 x 5。这是预期的——对于大小为 5 的滤波器，我们得到 6 个特征。另外，看看参数的总数。只有 36，确实是一个非常小的数字。
- en: 'Compile the model with the loss as `mean_squared_error` and `adam` as the `optimizer`,
    and then fit it on the train data for 5 epochs:'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型编译，损失函数为`mean_squared_error`，优化器为`adam`，然后在训练数据上训练 5 个 epoch：
- en: '[PRE51]'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You should see the following output:'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您应该看到以下输出：
- en: '![Figure 5.32: Training and validation loss'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.32: 训练和验证损失'
- en: '](img/B15385_05_32.jpg)'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_05_32.jpg)'
- en: 'Figure 5.32: Training and validation loss'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 5.32: 训练和验证损失'
- en: From the preceding screenshot, we can see that the validation loss is pretty
    low for the 1D convolution model too. We need to see whether this performance
    is comparable to that of the plain RNN. Let's evaluate the performance of the
    model and see whether it aligns with our expectations.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从前面的截图中，我们可以看到 1D 卷积模型的验证损失也相当低。我们需要查看这种性能是否与普通 RNN 的性能相当。让我们评估一下模型的性能，看看是否符合我们的预期。
- en: 'Use the `get_model_perf` function to get the RMSE for the train and test sets:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`get_model_perf`函数获取训练集和测试集的 RMSE：
- en: '[PRE52]'
  id: totrans-354
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output is as follows:'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE53]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: This is marginally higher than that of the plain RNN model. Let's visualize
    the predictions next.
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这略高于普通 RNN 模型的性能。接下来，让我们可视化一下预测结果。
- en: 'Using the `plot_pred` function, plot the predictions and the actual values:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plot_pred`函数，绘制预测值和实际值：
- en: '[PRE54]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The model output would be as follows, with the dotted lines showing the predictions
    and solid lines depicting the actual values:'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型的输出如下，虚线表示预测值，实线表示实际值：
- en: '![Figure 5.33: Plotting the predictions and actual values'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.33: 绘制预测值和实际值'
- en: '](img/B15385_05_33.jpg)'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_05_33.jpg)'
- en: 'Figure 5.33: Plotting the predictions and actual values'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 5.33: 绘制预测值和实际值'
- en: This is very similar to the plot from the predictions from the RNN model (*Figure
    5.29*). But we now acknowledge that a better assessment would need interactive
    visualization and zooming in to a scale where the individual points are visible.
    Let's zoom in using the interactive plotting features of matplotlib using the
    notebook backend by using the `%matplotlib` cell magic command.
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这与 RNN 模型的预测图非常相似（*图 5.29*）。但我们现在认识到，更好的评估需要交互式可视化，并放大到每个点都可见的尺度。让我们使用 matplotlib
    的交互式绘图功能，通过 notebook 后端并使用`%matplotlib`单元格魔法命令来进行放大。
- en: 'Plot again with interactivity and zoom into the last 100 data points:'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次绘制，并进行交互式操作，放大最后 100 个数据点：
- en: '[PRE55]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output will be as follows:'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.34: Zoomed-in view of predictions'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.34: 放大显示预测值'
- en: '](img/B15385_05_34.jpg)'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_05_34.jpg)'
- en: 'Figure 5.34: Zoomed-in view of predictions'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.34: 放大显示预测值'
- en: Note
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If the preceding graph is not displayed properly for some reason, run the cell
    containing `%matplotlib notebook` for a couple of times. Alternatively, you can
    also use `%matplotlib inline` instead of `%matplotlib notebook`.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 如果前面的图表由于某些原因没有正确显示，请运行包含`%matplotlib notebook`的单元格几次。或者，您也可以使用`%matplotlib
    inline`来代替`%matplotlib notebook`。
- en: The preceding diagram shows a closer view of the predictions (dotted lines)
    and the actual values (solid lines). Things aren't looking too good at this scale.
    The output is very smooth, and almost looks like some kind of averaging is going
    on. What happened? Is this in line with your expectations? Can you explain this
    output?
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显示了预测（虚线）和实际值（实线）的更详细视图。在这个尺度上看，情况并不太好。输出非常平滑，几乎看起来像某种平均化。发生了什么？这符合您的预期吗？能解释一下这个输出吗？
- en: Note
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZctArW](https://packt.live/2ZctArW).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/2ZctArW](https://packt.live/2ZctArW)。
- en: You can also run this example online at [https://packt.live/38EDOEA](https://packt.live/38EDOEA).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在[https://packt.live/38EDOEA](https://packt.live/38EDOEA)在线运行此示例。为了获得预期的结果，您必须执行整个笔记本。
- en: In this exercise, we built and trained our 1D convolution-based model for stock
    price prediction. We saw that the number of parameters is very low, and that the
    training time was much lower.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们为股价预测构建并训练了基于1D卷积的模型。我们发现参数数量非常低，训练时间也大大缩短。
- en: Performance of 1D Convnets
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1D卷积网络的性能
- en: To explain the result in the previous exercise, we need to understand what is
    happening after we extract the subsequences using the Conv1D layer. The sequence
    in the data is being captured, that is, in the individual filters. But is the
    sequence being retained after that, and are we really exploiting the sequence
    in the data? No, we are not. Once the patches have been extracted, they are being
    treated independently. It is for this reason that the performance is not great.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释上一个练习的结果，我们需要理解在使用Conv1D层提取子序列后发生了什么。数据中的序列正在被捕获，也就是说，在各个过滤器中。但是在此之后，序列是否被保留，我们是否真正利用了数据中的序列？不，我们没有。一旦提取了补丁，它们就被独立处理了。因此，性能并不是很好的原因就在于此。
- en: So, why did we state that 1D convnets do great on sequence tasks previously?
    How do you make them perform well for our task? 1D convnets do very well on tasks
    regarding text, especially classification, where the short, local sequence has
    very high importance and following the order in the entire sequence (say, 200
    terms) doesn't provide a huge benefit. For time series tasks, we need order in
    the entire sequence. There are ways to induce order consideration for tasks such
    as time series tasks, but they aren't great.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我们之前说1D卷积网络在序列任务上表现出色？如何使它们在我们的任务中表现良好？1D卷积网络在文本等任务中表现非常好，特别是分类任务，其中短而局部的序列非常重要，而在整个序列（比如200个项）的顺序中遵循并不会带来巨大的好处。对于时间序列任务，我们需要整个序列的顺序。有方法可以诱导序列考虑，例如对时间序列任务，但效果不是很好。
- en: Using 1D Convnets with RNNs
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用1D卷积网络与RNN
- en: We saw the benefits of 1D convnets – speed, feature reduction, lower number
    of parameters, learning local features, and much more. We also saw that RNNs provide
    very powerful and flexible architectures for handling sequences but have a lot
    of parameters and are expensive to train. One possible approach can be to combine
    both – the benefit of the representation and feature reduction from 1D convnets
    in the initial layers, and the benefit of the sequence processing power of RNNs
    in the following layers. Let's try it out for our task.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了1D卷积网络的优势 – 速度快、特征减少、参数数量少、学习局部特征等等。我们也看到了RNN提供了非常强大和灵活的序列处理架构，但参数很多，训练成本高昂。一种可能的方法是结合两者
    – 在初始层中利用1D卷积网络的表征和特征减少的优势，在后续层中利用RNN的序列处理能力。让我们尝试一下适用于我们的任务。
- en: 'Exercise 5.05: Building a Hybrid (1D Convolution and RNN) Model'
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'Exercise 5.05: 构建混合模型（1D卷积和RNN）'
- en: 'In this exercise, we will build a model that will employ both 1D convolutions
    and RNNs and assess the change in performance. Making a hybrid model is straightforward
    – we''ll begin with the convolution layer, the output of which is features in
    a sequence. The sequence can be fed straight into the RNN layer. Therefore, combining
    the 1D convolutions with RNNs is as simple as following the Conv1D layer with
    an RNN layer. We''ll continue this exercise in the same Jupyter Notebook. Perform
    the following steps to complete this exercise:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将构建一个模型，结合使用 1D 卷积和 RNN，并评估性能变化。创建一个混合模型非常简单——我们从卷积层开始，卷积层的输出是一个序列的特征。这个序列可以直接输入到
    RNN 层中。因此，结合 1D 卷积和 RNN 就像在 Conv1D 层后面接一个 RNN 层一样简单。我们将在同一个 Jupyter Notebook 中继续这个练习。按照以下步骤完成本次练习：
- en: 'Initialize a sequential model, add a `Reshape` layer (as in the preceding exercise),
    and add a `Conv1D` layer with five filters and a filter size 3:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个顺序模型，添加一个 `Reshape` 层（如前面的练习所示），然后添加一个包含五个过滤器和过滤器大小为 3 的 `Conv1D` 层：
- en: '[PRE56]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Next, add a `SimpleRNN` layer with 32 neurons, followed by a `Dense` layer
    and an `Activation` layer:'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，添加一个包含 32 个神经元的 `SimpleRNN` 层，后跟一个 `Dense` 层和一个 `Activation` 层：
- en: '[PRE57]'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Print out the model summary:'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出模型摘要：
- en: '[PRE58]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output will be as follows:'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 5.35: Summary of the hybrid (1D convolution and RNN) model'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.35：混合（1D 卷积和 RNN）模型的摘要'
- en: '](img/B15385_05_35.jpg)'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_05_35.jpg)'
- en: 'Figure 5.35: Summary of the hybrid (1D convolution and RNN) model'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.35：混合（1D 卷积和 RNN）模型的摘要
- en: The output from the Conv1D layer is 8 × 5 – 8 features from 5 filters. The overall
    number of parameters is slightly higher than the plain RNN model. This is because
    the sequence size we're dealing with is very low. If we were dealing with larger
    sequences, we would have seen a reduction in the parameters. Let's compile and
    fit the model.
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Conv1D 层的输出是 8 × 5——来自 5 个过滤器的 8 个特征。总体参数数量略高于纯 RNN 模型。这是因为我们处理的序列大小非常小。如果处理更大的序列，我们将看到参数的减少。现在，让我们编译并训练模型。
- en: 'Compile and fit the model on the training data for three epochs:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编译并在训练数据上训练模型，训练 3 个周期：
- en: '[PRE59]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The model training output is as follows:'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型训练输出如下：
- en: '![Figure 5.36: Training and validation loss'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.36：训练和验证损失'
- en: '](img/B15385_05_36.jpg)'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_05_36.jpg)'
- en: 'Figure 5.36: Training and validation loss'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.36：训练和验证损失
- en: Let's assess the performance first by looking at RMSE. We don't expect this
    to be very useful for our example, but let's print it out as good practice.
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们首先通过查看 RMSE 来评估性能。我们不指望这对我们的例子有太大帮助，但作为一种良好的实践，还是将其打印出来。
- en: 'Print the RMSE for the train and test set using the `get_model_perf` function:'
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `get_model_perf` 函数打印训练集和测试集的 RMSE：
- en: '[PRE60]'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'You''ll get the following output:'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将得到以下输出：
- en: '[PRE61]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The values seem lower, but only a very close look will help us assess the performance
    of the model.
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些值看起来较低，但只有仔细观察才能帮助我们评估模型的性能。
- en: 'Plot the prediction versus actual in interactive mode and zoom in on the last
    100 points:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在交互模式下绘制预测与实际值的对比，并放大最后 100 个点：
- en: '[PRE62]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output of the preceding command will be as follows:'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面命令的输出将如下所示：
- en: '![Figure 5.37: Plot of the combined model'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.37：混合模型的绘图'
- en: '](img/B15385_05_37.jpg)'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15385_05_37.jpg)'
- en: 'Figure 5.37: Plot of the combined model'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.37：混合模型的绘图
- en: 'Following is a zoomed-in view of the predictions:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是预测结果的放大视图：
- en: '![Figure 5.38: Zoomed-in view of predictions'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.38：预测结果的放大视图'
- en: '](img/B15385_05_38.jpg)'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_38.jpg)'
- en: 'Figure 5.38: Zoomed-in view of predictions'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.38：预测结果的放大视图
- en: Note
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If the graphs presented below are not displayed properly for some reason, run
    the cell containing `%matplotlib notebook` for a couple of times. Alternatively,
    you can also use `%matplotlib inline` instead of `%matplotlib notebook`.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 如果下面展示的图形由于某些原因没有正确显示，请多次运行包含 `%matplotlib notebook` 的单元。或者，你也可以使用 `%matplotlib
    inline` 代替 `%matplotlib notebook`。
- en: This result is extremely good. The prediction (dotted lines) is extremely close
    to the actual (solid lines) for the test data – capturing not only the level but
    also the minute variations very well. There is also some effective regularization
    going on when the 1D convnet is extracting patches from the sequence. These features
    are being fed in sequence to the RNN, which is using its raw power to provide
    the output we see. There is indeed merit in combining 1D convnets with RNNs.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 该结果非常好。对于测试数据，预测（虚线）与实际值（实线）非常接近——不仅捕捉了水平变化，还很好地捕捉了细微的变化。当1D卷积网络从序列中提取补丁时，也有一些有效的正则化发生。这些特征被按序列输入到RNN中，RNN利用其强大的计算能力提供我们看到的输出。将1D卷积网络与RNN结合确实是有价值的。
- en: Note
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZctArW](https://packt.live/2ZctArW).
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考[https://packt.live/2ZctArW](https://packt.live/2ZctArW)。
- en: You can also run this example online at [https://packt.live/38EDOEA](https://packt.live/38EDOEA).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/38EDOEA](https://packt.live/38EDOEA)上在线运行这个示例。你必须执行整个Notebook才能得到期望的结果。
- en: In this exercise, we saw how we can combine 1D convnets and RNNs to form a hybrid
    model that can provide high performance. We acknowledge that there is merit in
    trying this combination for sequence processing tasks.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们看到如何将1D卷积网络和RNN结合形成一个混合模型，该模型能够提供高性能。我们承认，尝试这种组合在序列处理任务中是有价值的。
- en: 'Activity 5.01: Using a Plain RNN Model to Predict IBM Stock Prices'
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动5.01：使用普通RNN模型预测IBM股票价格
- en: We have seen RNNs in action and can now appreciate the kind of power they bring
    in sequence prediction tasks. We also saw that RNNs in conjunction with 1D convnets
    provide great results. Now, let's employ these ideas in another stock price prediction
    task, this time predicting the stock price for IBM. The dataset can be downloaded
    from [https://packt.live/3fgmqIL](https://packt.live/3fgmqIL). You will visualize
    the data and understand the patterns. From your understanding of the data, choose
    a lookback period and build an RNN-based model for prediction. The model will
    have a 1D convnet as well as a plain RNN layer. You will also employ dropout to
    prevent overfitting.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到RNN的实际应用，现在可以欣赏它们在序列预测任务中所带来的强大能力。我们还看到，RNN与1D卷积网络结合能够提供出色的结果。接下来，让我们将这些思路应用到另一个股票价格预测任务中，这次预测IBM的股票价格。数据集可以从[https://packt.live/3fgmqIL](https://packt.live/3fgmqIL)下载。你将可视化数据并理解其中的模式。根据你对数据的理解，选择一个回溯期并构建一个基于RNN的预测模型。该模型将包含1D卷积网络和普通RNN层。你还将使用dropout来防止过拟合。
- en: 'Perform the following steps to complete this exercise:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以完成此练习：
- en: Load the `.csv` file, reverse the index, and plot the time series (the `Close`
    column) for visual inspection.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`.csv`文件，反转索引，并绘制时间序列（`Close`列）以便可视化检查。
- en: Extract the values for `Close` from the DataFrame as a `numpy` array and plot
    them using `matplotlib`.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从DataFrame中提取`Close`的值作为`numpy`数组，并使用`matplotlib`进行绘制。
- en: Assign the final 25% data as test data and the first 75% as train data.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最后25%的数据指定为测试数据，前75%的数据作为训练数据。
- en: Using `MinMaxScaler` from `sklearn`, scale the train and test data.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`sklearn`中的`MinMaxScaler`对训练数据和测试数据进行缩放。
- en: Using the `get_lookback` function we defined in this chapter, get lookback data
    for the train and test data using a lookback period of 15.
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们在本章定义的`get_lookback`函数，通过15的回溯期获取训练数据和测试数据的回溯数据。
- en: From Keras, import all the necessary layers for employing plain RNNs (`SimpleRNN`,
    `Activation`, `Dropout`, `Dense`, and `Reshape`) and 1D convolutions (Conv1D).
    Also, import `mean_squared_error`.
  id: totrans-433
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Keras导入所有必要的层来使用普通RNN（`SimpleRNN`、`Activation`、`Dropout`、`Dense`和`Reshape`）和1D卷积（Conv1D）。另外，导入`mean_squared_error`。
- en: Build a model with a 1D convolution layer (5 filters of size 3) and an RNN layer
    with 32 neurons. Add 25% dropout after the RNN layer. Print the model's summary.
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个包含1D卷积层（5个大小为3的滤波器）和一个具有32个神经元的RNN层的模型。在RNN层之后添加25%的dropout。打印模型的摘要。
- en: Compile the model with the `mean_squared_error` loss and the `adam` optimizer.
    Fit this on the train data in five epochs with a validation split of 10% and a
    batch size of 1.
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`mean_squared_error`损失函数和`adam`优化器编译模型。在训练数据上进行五个epochs的拟合，验证集划分比例为10%，批大小为1。
- en: Using the `get_model_perf` method, print the RMSE of the model.
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`get_model_perf`方法，打印模型的RMSE。
- en: Plot the predictions – the entire view, as well as a zoomed-in one for a close
    assessment of the performance.
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制预测结果——展示整体视图以及一个放大的视图以便更精细地评估性能。
- en: 'The zoomed-in view of the predictions (dotted lines) versus the actuals (solid
    lines) should be as follows:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 预测结果（虚线）与实际结果（实线）的放大视图应如下所示：
- en: '![Figure 5.39: Zoomed-in view of predictions'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.39：预测结果的放大视图'
- en: '](img/B15385_05_39.jpg)'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15385_05_39.jpg)'
- en: 'Figure 5.39: Zoomed-in view of predictions'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.39：预测结果的放大视图
- en: Note
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 410.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的详细步骤以及解决方案和附加评论，详见第 410 页。
- en: Summary
  id: totrans-444
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at the considerations of working with sequences.
    There are several tasks that require us to exploit information contained in a
    sequence, where sequence-agnostic models would fare poorly. We saw that using
    RNNs is a very powerful approach to sequence modeling – the architecture explicitly
    processes the sequence and considers the information accumulated so far, along
    with the new input, to generate the output. Even very simple RNN architectures
    performed very well on our stock price prediction task. We got the kind of results
    that would take a lot of effort to get using classical approaches.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了处理序列时的注意事项。有几个任务要求我们利用序列中包含的信息，而对于这些任务，序列无关模型的表现会很差。我们看到，使用 RNN 是进行序列建模的一种非常强大的方法——该架构显式地处理序列，并考虑到迄今为止积累的信息和新输入，以生成输出。即使是非常简单的
    RNN 架构，在我们的股价预测任务中也表现得非常好。我们得到了使用经典方法需要付出大量努力才能取得的那种结果。
- en: We also saw that 1D convolutions can be employed in sequence prediction tasks.
    1D convolutions, like their 2D counterparts for images, learn local features in
    a sequence. We built a 1D convolution model that didn't fare too well on our task.
    The final model that we built combined 1D convolutions and RNNs and provided excellent
    results regarding the stock price prediction task.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到，1D 卷积可以应用于序列预测任务。像 2D 卷积用于图像一样，1D 卷积可以学习序列中的局部特征。我们建立了一个 1D 卷积模型，但在我们的任务中表现不太理想。最终我们建立的模型结合了
    1D 卷积和 RNN，并在股价预测任务中取得了优异的结果。
- en: In the next chapter, we will discuss models that are variations of RNNs that
    are even more powerful. We will also discuss architectures that extract the latent
    power of the idea of the RNN. We will apply these "RNNs on steroids" to an important
    task in natural language processing – sentiment classification.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论一些变种的 RNN 模型，它们更加强大。我们还将讨论提取 RNN 概念潜在能力的架构。我们将把这些“强化版 RNN”应用于自然语言处理中的一个重要任务——情感分类。
