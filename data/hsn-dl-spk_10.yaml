- en: Deploying on a Distributed System
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在分布式系统上部署
- en: 'The upcoming chapters of this book will show what we have learned so far in
    order to implement some practical and real-world use cases of CNNs and RNNs. But
    before doing that, let''s consider DL4J in a production environment. This chapter
    is divided into four main sections:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书接下来的章节将展示我们迄今为止学到的内容，以便实现一些实际的、现实世界中的CNN和RNN用例。但在此之前，我们先考虑一下DL4J在生产环境中的应用。本章分为四个主要部分：
- en: Some considerations about the setup for a DL4J environment in production, with
    focus in particular on memory management, CPU, and GPU setup, and job submission
    for training
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于DL4J生产环境设置的一些考虑，特别关注内存管理、CPU和GPU设置以及训练作业的提交
- en: Distributed training architecture details (data parallelism and strategies implemented
    in DL4J)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式训练架构细节（数据并行性和DL4J中实现的策略）
- en: The practical way to import, train, and execute Python (Keras and TensorFlow)
    models in a DL4J (JVM)-based production environment
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在基于DL4J（JVM）的生产环境中导入、训练和执行Python（Keras和TensorFlow）模型的实际方法
- en: A comparison between DL4J and a couple of alternative DL frameworks for the
    Scala programming language (with particular focus on their readiness for production)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J与几种替代的Scala编程语言DL框架的比较（特别关注它们在生产环境中的就绪性）
- en: Setup of a distributed environment with DeepLearning4j
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置一个分布式环境与DeepLearning4j
- en: This section explains some tricks to do when setting up a production environment
    for DL4J neural network model training and execution.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了一些设置DL4J神经网络模型训练和执行的生产环境时的技巧。
- en: Memory management
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存管理
- en: In [Chapter 7](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Training Neural
    Networks with Spark*, in the *Performance considerations *section, we learned
    how DL4J handles memory when training or running a model. Because it relies on
    ND4J, it also utilizes off-heap memory and not only heap memory. Being off-heap,
    it means that it is outside the scope managed by the JVM's **Garbage Collection**
    (**GC**) mechanism (the memory is allocated outside the JVM). At the JVM level,
    there are only pointers to off-heap memory locations; they can be passed to the
    C++ code via the Java Native Interface (JNI, [https://docs.oracle.com/javase/8/docs/technotes/guides/jni/spec/jniTOC.html](https://docs.oracle.com/javase/8/docs/technotes/guides/jni/spec/jniTOC.html))
    for use in ND4J operations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)，*使用Spark训练神经网络*章节中的*性能考虑*部分，我们学习了在训练或运行模型时，DL4J如何处理内存。由于它依赖于ND4J，它不仅使用堆内存，还利用堆外内存。作为堆外内存，它位于JVM的**垃圾回收**（**GC**）机制管理的范围之外（内存分配在JVM外部）。在JVM层面，只有指向堆外内存位置的指针；这些指针可以通过Java本地接口（JNI，
    [https://docs.oracle.com/javase/8/docs/technotes/guides/jni/spec/jniTOC.html](https://docs.oracle.com/javase/8/docs/technotes/guides/jni/spec/jniTOC.html)）传递给C++代码，用于ND4J操作。
- en: 'In DL4J, it is possible to manage memory allocations using two different approaches:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在DL4J中，可以使用两种不同的方法来管理内存分配：
- en: JVM GC and weak reference tracking
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JVM垃圾回收（GC）和弱引用跟踪
- en: Memory workspaces
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存工作空间
- en: 'In this section, both approaches are going to be covered. The idea behind both
    is the same: once an `INDArray` is no longer required, the off-heap memory associated
    with it should be released so that it can be reused. The difference between the
    two approaches is as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将涵盖这两种方法。它们的思想是相同的：一旦`INDArray`不再需要，应该释放与其相关的堆外内存，以便可以重复使用。两种方法之间的区别如下：
- en: '**JVM GC**: When an `INDArray` is collected by the garbage collector, its off-heap
    memory is deallocated, with the assumption that it is not used elsewhere'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**JVM垃圾回收（GC）**：当`INDArray`被垃圾回收器回收时，它的堆外内存会被释放，假设该内存不会在其他地方使用'
- en: '**Memory workspaces**: When an `INDArray` leaves the workspace scope, its off-heap
    memory may be reused, without deallocation and reallocation'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存工作空间**：当`INDArray`离开工作空间范围时，它的堆外内存可以被重用，而无需进行释放和重新分配'
- en: Please refer to [Chapter 7](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Training
    Neural Networks with Spark*, in the *Performance considerations* section, for
    details on how to configure the limits for the heap and off-heap memory.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考[第7章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)，*使用Spark训练神经网络*章节中的*性能考虑*部分，了解如何配置堆内存和堆外内存的限制。
- en: The memory workspaces approach needs more explanation. Compared to the JVM GC
    approach, it gives the best results in terms of performance in cyclic workloads.
    Within a workspace, any operation is possible with `INDArrays`. Then at the end
    of the workspace loop, all `INDArrays` content in memory is invalidated. Whether
    an `INDArray` should be needed outside a workspace (which could be the case when
    moving results out of it), it is possible to use the `detach` method of the `INDArray`
    itself to create an independent copy of it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 内存工作空间的方式需要更多解释。与 JVM 垃圾回收方法相比，它在循环工作负载的性能方面提供了最佳的结果。在工作空间内，任何操作都可以与 `INDArrays`
    一起进行。然后，在工作空间循环结束时，内存中所有 `INDArrays` 的内容都会失效。是否需要在工作空间外部使用 `INDArray`（当需要将结果移出工作空间时，可能会出现这种情况），可以使用
    `INDArray` 本身的 `detach` 方法来创建它的独立副本。
- en: 'Workspaces are enabled by default in DL4J releases from 1.0.0-alpha or later.
    In order to use them, they need to be activated for DL4J release 0.9.1 or older.
    In DL4J 0.9.1, at network configuration time, workspaces can be activated this
    way (for training):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从 DL4J 1.0.0-alpha 版本开始，工作空间默认启用。对于 DL4J 0.9.1 或更早版本，使用工作空间需要先进行激活。在 DL4J 0.9.1
    中，在网络配置时，工作空间可以这样激活（用于训练）：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Or for inference, they can be activated as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在推理时，可以按以下方式激活：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A `SEPARATE` workspace is slower, but it uses less memory, while a `SINGLE`
    workspace is faster, but requires more memory. The choice between `SEPARATE` and
    `SINGLE` depends on the compromise you choose between memory footprint and performance.
    When workspaces are enabled, all the memory used during training is made reusable
    and tracked without interference by the JVM GC. Only the `output` method, which
    uses workspaces internally for the feed-forward loop, is an exception, but then
    it detaches the resulting `INDArray` from the workspaces, so it can then be handled
    by the JVM GC. Starting from release 1.0.0-beta, the `SEPARATE` and `SINGLE` modes
    have been deprecated. The available modes are `ENABLED` (default) and `NONE`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '`SEPARATE` 工作空间较慢，但使用的内存较少，而 `SINGLE` 工作空间较快，但需要更多的内存。选择 `SEPARATE` 和 `SINGLE`
    之间的权衡，取决于你对内存占用和性能的平衡。在启用工作空间时，训练过程中使用的所有内存都会被重用并进行跟踪，且不受 JVM 垃圾回收的干扰。只有 `output`
    方法，内部使用工作空间来进行前向传播循环，才是例外，但它会将生成的 `INDArray` 从工作空间中分离出来，这样它就可以由 JVM 垃圾回收器处理。从
    1.0.0-beta 版本开始，`SEPARATE` 和 `SINGLE` 模式已被弃用，现有的模式是 `ENABLED`（默认）和 `NONE`。'
- en: 'Please remember that, when a training process uses workspaces, in order to
    get the most from this approach, periodic GC calls need to be disabled, as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，当训练过程使用工作空间时，为了最大限度地利用这种方法，需要禁用定期的垃圾回收调用，具体如下：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Or their frequency needs to be reduced, as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 或者它们的频率需要减少，具体如下：
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This setting should be done before invoking the `fit` method for the model in
    training. The workspace modes are available also for `ParallelWrapper` (in the
    case of training demanded to DL4J only, running multiple models on the same server).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 该设置应在调用模型的 `fit` 方法进行训练之前进行。工作空间模式也适用于 `ParallelWrapper`（在仅要求 DL4J 进行训练的情况下，在同一服务器上运行多个模型）。
- en: 'In some cases, to save memory, it would be necessary to release all the workspaces
    created during training or evaluation. This can be done by invoking the following
    method of `WorkspaceManager`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，为了节省内存，可能需要释放在训练或评估期间创建的所有工作空间。这可以通过调用 `WorkspaceManager` 的以下方法来完成：
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It destroys all workspaces that have been created within the calling thread.
    Workspaces created in some external threads that are no longer needed can be destroyed
    using the same method in that thread.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 它会销毁调用线程中创建的所有工作空间。可以使用相同的方法在不再需要的外部线程中销毁创建的工作空间。
- en: 'In DL4J release 1.0.0-alpha and later, when using the `nd4j-native` backend,
    it is also possible to use a memory-mapped file instead of RAM. While it is slower,
    it allows memory allocation in a manner that is impossible to achieve using RAM.
    This option is mostly workable in those cases where `INDArrays` can''t fit into
    RAM. Here''s how this could be done programmatically:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DL4J 1.0.0-alpha 版本及以后版本中，使用 `nd4j-native` 后端时，还可以使用内存映射文件代替 RAM。虽然这比较慢，但它允许以一种使用
    RAM 无法实现的方式进行内存分配。这种选项主要适用于那些 `INDArrays` 无法放入 RAM 的情况。以下是如何以编程方式实现：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this example, a temporary file of 2 GB is created, a workspace is mapped
    there, and the `ndArray` `INDArray` is created in that workspace.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，创建了一个 2 GB 的临时文件，映射了一个工作空间，并在该工作空间中创建了 `ndArray` `INDArray`。
- en: CPU and GPU setup
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CPU 和 GPU 设置
- en: 'As mentioned before in this book, any application implemented through DL4J
    can be executed on CPUs or GPUs. To switch from CPUs to GPUs, a change in the
    application dependencies for ND4J is needed. Here''s an example for CUDA release
    9.2 (or later) and NVIDIA-compatible hardware (the example is for Maven, but the
    same dependency could be set for Gradle or sbt), as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书前面所提到的，任何通过DL4J实现的应用程序都可以在CPU或GPU上执行。要从CPU切换到GPU，需要更改ND4J的应用程序依赖。以下是CUDA
    9.2版本（或更高版本）和支持NVIDIA硬件的示例（该示例适用于Maven，但相同的依赖关系也可以用于Gradle或sbt），如下所示：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This dependency replaces that for `nd4j-native`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个依赖替代了`nd4j-native`的依赖。
- en: 'When you have multiple GPUs in your system, whether it should restrict their
    usage and force to execute on a single one, it is possible to change this programmatically
    through the `CudaEnvironment` helper class ([https://deeplearning4j.org/api/latest/org/nd4j/jita/conf/CudaEnvironment.html](https://deeplearning4j.org/api/latest/org/nd4j/jita/conf/CudaEnvironment.html))
    of the `nd4j-cuda` library. The following line of code needs to be executed as
    the first instruction in a DL4J application entry point:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的系统中有多个GPU时，是否应该限制它们的使用并强制在一个GPU上执行，可以通过`nd4j-cuda`库中的`CudaEnvironment`助手类([https://deeplearning4j.org/api/latest/org/nd4j/jita/conf/CudaEnvironment.html](https://deeplearning4j.org/api/latest/org/nd4j/jita/conf/CudaEnvironment.html))以编程方式进行更改。以下代码行需要作为DL4J应用程序入口点的第一条指令执行：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In section 10.1.1, we have learned how to configure heap and off-heap memory
    in DL4J. Some considerations need to be made when executing on GPUs. It should
    be clear that the settings for the command-line arguments `org.bytedeco.javacpp.maxbytes`
    and `org.bytedeco.javacpp.maxphysicalbytes` define the memory limits for the GPU(s),
    because for `INDArrays`*,* the off-heap memory is mapped to the GPU (`nd4j-cuda`
    is used).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10.1.1节中，我们已经学习了如何在DL4J中配置堆内存和堆外内存。在GPU执行时需要考虑一些问题。需要明确的是，命令行参数`org.bytedeco.javacpp.maxbytes`和`org.bytedeco.javacpp.maxphysicalbytes`定义了GPU的内存限制，因为对于`INDArrays`，堆外内存被映射到GPU上（使用的是`nd4j-cuda`）。
- en: 'Also, when running on GPUs, most probably less RAM would be used in the JVM
    heap, while more would be used in the off-heap, as this is where all of the `INDArrays`
    are stored. Allocating too much to the JVM heap would leave a real risk of having
    not enough memory left off-heap. Anyway, while doing proper settings, in some
    situations, execution could lead to the following exception:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在GPU上运行时，JVM堆内存的使用量通常较少，而堆外内存的使用量较多，因为所有的`INDArrays`都存储在堆外内存中。如果将太多内存分配给JVM堆内存，可能会导致堆外内存不足的风险。在进行适当设置时，在某些情况下，执行可能会导致以下异常：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This means that we have run out of off-heap memory. In situations like this
    (in particular for training), we need to consider `WorkspaceConfiguration` to
    handle the `INDArrays` memory allocation (as learned in the *Memory management*
    section). If not, the `INDArrays` and their off-heap resources would be reclaimed
    through the JVM GC approach, which might severely increase latency and generate
    other potential out of memory issues.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的堆外内存已经用完。在这种情况下（特别是在训练过程中），我们需要考虑使用`WorkspaceConfiguration`来处理`INDArrays`的内存分配（如在*内存管理*部分所学）。如果不这样做，`INDArrays`及其堆外资源将通过JVM
    GC机制回收，这可能会显著增加延迟，并产生其他潜在的内存不足问题。
- en: The command-line arguments to set the memory limits are optional. Not specifying
    anything means that by default 25% of the total system RAM is set as the limit
    for the heap memory, while by default twice the RAM reserved for the heap memory
    would be set for the off-heap memory. It is up to us to find the perfect balance,
    particularly in cases of execution on GPUs, considering the expected amount of
    off-heap memory for the `INDArrays`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 设置内存限制的命令行参数是可选的。如果没有指定，默认情况下，堆内存的限制为总系统内存的25%，而堆外内存的限制是堆内存的两倍。我们需要根据实际情况找到最佳平衡，特别是在GPU执行时，考虑`INDArrays`所需的堆外内存。
- en: Typically, CPU RAM is greater than GPU RAM. For this reason, how much RAM is
    being used off-heap needs to be monitored. DL4J allocates memory on the GPU equivalent
    to the amount of off-heap memory specified through the previously mentioned command-line
    arguments. In order to make the communication between CPU and GPU more efficient,
    DL4J allocates off-heap memory on the CPU RAM too. This way, a CPU can access
    data from an `INDArray` with no need to fetch data from a GPU any time there is
    a call for it.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CPU 内存大于 GPU 内存。因此，需要监控多少内存被用作堆外内存。DL4J 会在 GPU 上分配与通过上述命令行参数指定的堆外内存相等的内存。为了提高
    CPU 和 GPU 之间的通信效率，DL4J 还会在 CPU 内存上分配堆外内存。这样，CPU 就可以直接访问 `INDArray` 中的数据，而无需每次都从
    GPU 获取数据。
- en: 'However there is one caveat: if a GPU has less than 2 GB of RAM, it''s probably
    not suitable for DL production workloads. In that case, a CPU should be used.
    Typically, DL workloads require a minimum of 4 GB of RAM (8 GB of RAM is recommended
    on a GPU).'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个警告：如果 GPU 的内存少于 2 GB，那么它可能不适合用于深度学习（DL）生产工作负载。在这种情况下，应使用 CPU。通常，深度学习工作负载至少需要
    4 GB 的内存（推荐在 GPU 上使用 8 GB 的内存）。
- en: 'Here is a final consideration: with a CUDA backend and using workspaces, it is
    also possible to use `HOST_ONLY` memory. Programmatically, this could be set up
    as in the following example:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的因素是：使用 CUDA 后端并通过工作区，也可以使用 `HOST_ONLY` 内存。从编程角度来看，可以通过以下示例进行设置：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This reduces performance, but it can be useful as in-memory cache pairs when
    using the `unsafeDuplication` method of `INDArray`, which performs efficient (but
    unsafe) `INDArray` duplication.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这样会降低性能，但在使用 `INDArray` 的 `unsafeDuplication` 方法时，它可以作为内存缓存对来使用，`unsafeDuplication`
    方法能够高效地（但不安全地）进行 `INDArray` 复制。
- en: Building a job to be submitted to Spark for training
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个作业并提交给 Spark 进行训练
- en: At this stage, I am assuming you have already started browsing and trying the
    code examples in the GitHub repository ([https://github.com/PacktPublishing/Hands-On-Deep-Learning-with-Apache-Spark](https://github.com/PacktPublishing/Hands-On-Deep-Learning-with-Apache-Spark))
    associated with this book. If so, you should have noticed that all of the Scala
    examples use Apache Maven ([https://maven.apache.org/](https://maven.apache.org/))
    for packaging and dependency management. In this section, I am going to refer
    to this tool in order to build a DL4J job that will then be submitted to Spark
    to train a model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我假设你已经开始浏览并尝试本书相关的 GitHub 仓库中的代码示例（[https://github.com/PacktPublishing/Hands-On-Deep-Learning-with-Apache-Spark](https://github.com/PacktPublishing/Hands-On-Deep-Learning-with-Apache-Spark)）。如果是这样，你应该已经注意到所有
    Scala 示例都使用 Apache Maven（[https://maven.apache.org/](https://maven.apache.org/)）进行打包和依赖管理。在本节中，我将使用这个工具来构建一个
    DL4J 作业，然后将其提交给 Spark 来训练模型。
- en: 'Once you are confident that the job that you have developed is ready for training
    in the destination Spark cluster, the first thing to do is to build the uber-JAR
    file (also called the fat JAR file), which contains the Scala DL4J Spark program
    classes and dependencies. Check that all of the required DL4J dependencies for
    the given project are present in the `<dependencies>` block of the project POM
    file. Check that the correct version of the dl4j-Spark library has been selected;
    all of the examples in this book are meant to be used with Scala 2.11.x and Apache
    Spark 2.2.x. The code should look as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你确认开发的作业已经准备好在目标 Spark 集群中进行训练，首先要做的是构建 uber-JAR 文件（也叫 fat JAR 文件），它包含 Scala
    DL4J Spark 程序类和依赖项。检查项目 POM 文件中的 `<dependencies>` 块，确保所有必需的 DL4J 依赖项都已列出。确保选择了正确版本的
    dl4j-Spark 库；本书中的所有示例都旨在与 Scala 2.11.x 和 Apache Spark 2.2.x 一起使用。代码应如下所示：
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If your project POM file, as well as the other dependencies, contains references
    to Scala and/or any of the Spark libraries, please declare their scope as `provided`,
    as they are already available across the cluster nodes. This way, the uber-JAR
    would be lighter.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的项目 POM 文件以及其他依赖项包含对 Scala 和/或任何 Spark 库的引用，请将它们的作用域声明为 `provided`，因为它们已经在集群节点上可用。这样，uber-JAR
    文件会更轻。
- en: 'Once you have checked for the proper dependencies, you need to instruct the
    POM file on how to build the uber-JAR. There are three techniques to build an
    uber-JAR: unshaded, shaded, and JAR of JARs. The best approach for this case would
    be a shaded uber-JAR. Along with the unshaded approach, it works with the Java
    default class loader (so there is no need to bundle an extra special class loader),
    but brings the advantage of skipping some dependency version conflicts and the
    possibility, when there are files present in multiple JARs with the same path,
    to apply an appending transformation to them. Shading can be achieved in Maven
    through the Shade plugin ([http://maven.apache.org/plugins/maven-shade-plugin/](http://maven.apache.org/plugins/maven-shade-plugin/)).
    The plugin needs to be registered in the `<plugins>` section of the POM file as
    follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦检查了正确的依赖项，你需要在POM文件中指示如何构建uber-JAR。构建uber-JAR有三种技术：unshaded、shaded和JAR of
    JARs。对于本案例，最好的方法是使用shaded uber-JAR。与unshaded方法一样，它适用于Java默认的类加载器（因此无需捆绑额外的特殊类加载器），但它的优点是跳过某些依赖版本冲突，并且在多个JAR中存在相同路径的文件时，可以对其应用追加转换。Shading可以通过Maven的Shade插件实现（[http://maven.apache.org/plugins/maven-shade-plugin/](http://maven.apache.org/plugins/maven-shade-plugin/)）。该插件需要在POM文件的`<plugins>`部分注册，方法如下：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This plugin executes when the following command is issued:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当发出以下命令时，本插件会执行：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: At the end of the packaging process, the latest versions of this plugin replace
    the slim JAR with the uber-JAR, renaming it with the original filename. For a
    project with the following coordinates, the name of the uber-JAR would be `rnnspark-1.0.jar`*:*
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在打包过程结束时，插件的最新版本会用uber-JAR替换精简版JAR，并将其重命名为原始文件名。对于具有以下坐标的项目，uber-JAR的名称将为`rnnspark-1.0.jar`*：*
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The slim JAR is preserved anyway, but it is renamed as `original-rnnspark-1.0.jar`.
    They both can be found inside the `target` sub-directory of the project root directory.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 精简版JAR依然会被保留，但它会被重命名为`original-rnnspark-1.0.jar`。它们都可以在项目根目录的`target`子目录中找到。
- en: 'The JAR can then be submitted to the Spark cluster for training using the `spark-submit`
    script, the same way as for any other Spark job, as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，JAR可以通过`spark-submit`脚本提交到Spark集群进行训练，与提交任何其他Spark作业的方式相同，如下所示：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Spark distributed training architecture details
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark分布式训练架构细节
- en: The *Distributed network training with Spark and DeepLearning4J* section in [Chapter
    7](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Training Neural Networks with
    Spark*, explains why it is important to train MNNs in a distributed way across
    a cluster, and states that DL4J uses a parameter averaging approach to parallel
    training. This section goes through the architecture details of the distributed
    training approaches (parameter averaging and gradient sharing, which replaced
    the parameter averaging approach in DL4J starting from release 1.0.0-beta of the
    framework). The way DL4J approaches distributed training is transparent to developers,
    but it is good to have knowledge of it anyway.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)中的*分布式网络训练与Spark和DeepLearning4J*部分，*使用Spark训练神经网络*，解释了为什么将MNNs（神经网络模型）以分布式方式在集群中进行训练是重要的，并指出DL4J采用了参数平均方法进行并行训练。本节详细介绍了分布式训练方法的架构细节（参数平均和梯度共享，后者从DL4J框架的1.0.0-beta版本开始替代了参数平均方法）。虽然DL4J的分布式训练方式对开发者是透明的，但了解它仍然是有益的。'
- en: Model parallelism and data parallelism
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型并行性和数据并行性
- en: Parallelizing/distributing training computation can happen as **model parallelism**
    or **data parallelism**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 并行化/分布式训练计算可以通过**模型并行性**或**数据并行性**来实现。
- en: 'In model parallelism (see following diagram), different nodes of the cluster
    are responsible for the computation in different parts of a single MNN (an approach
    could be that each layer in the network is assigned to a different node):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型并行性（见下图）中，集群的不同节点负责单个MNN（神经网络模型）中不同部分的计算（例如，每一层网络分配给不同的节点）：
- en: '![](img/7d5a9a36-ede6-4b17-a35a-f71efc2d529f.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d5a9a36-ede6-4b17-a35a-f71efc2d529f.png)'
- en: 'Figure 10.1: Model parallelism'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：模型并行性
- en: 'In data parallelism (see the following diagram), different cluster nodes have
    a complete copy of the network model, but they get a different subset of the training
    data. The results from each node are then combined, as demonstrated in the following
    diagram:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据并行性（见下图）中，不同的集群节点拥有网络模型的完整副本，但它们获取不同子集的训练数据。然后，来自每个节点的结果会被合并，如下图所示：
- en: '![](img/4f2147d0-b25b-4698-ba8c-698dca76d88b.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f2147d0-b25b-4698-ba8c-698dca76d88b.png)'
- en: 'Figure 10.2: Data parallelism'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：数据并行性
- en: These two approaches can also be combined; they aren't mutually exclusive. Model
    parallelism works well in practice, but data parallelism has to be preferred for
    distributed training; matters like implementation, fault tolerance, and optimized
    cluster resource utilization (just to mention a few) are definitely easier for
    data parallelism than for model parallelism.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法也可以结合使用，它们并不相互排斥。模型并行性在实践中效果很好，但对于分布式训练，数据并行性是首选；实施、容错和优化集群资源利用等方面（仅举几例）在数据并行性中比在模型并行性中更容易实现。
- en: The data parallelism approach requires some way of combining the results and
    synchronizing the model parameters across workers. In the next two subsections,
    we are going to explore just the two (parameter averaging and gradient sharing)
    that have been implemented in DL4J.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行性方法需要某种方式来合并结果并同步各工作节点的模型参数。在接下来的两个小节中，我们将探讨 DL4J 中已实现的两种方法（参数平均化和梯度共享）。
- en: Parameter averaging
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数平均化
- en: 'Parameter averaging happens as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 参数平均化按以下方式进行：
- en: The master first initializes the neural network parameters based on the model
    configuration
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主节点首先根据模型配置初始化神经网络参数
- en: Then, it distributes a copy of the current parameters to each worker
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它将当前参数的副本分发给每个工作节点
- en: The training starts on each worker using its own subset of data
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个工作节点使用其自己的数据子集开始训练
- en: The master sets the global parameters to the average parameters for each worker
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主节点将全局参数设置为每个工作节点的平均参数
- en: In those cases where there is more data to process, the flow repeats from *Step
    2*
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在需要处理更多数据的情况下，流程将从*步骤 2*重新开始
- en: 'The following diagram shows a representation from *Step 2* to *Step 4*:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了从*步骤 2*到*步骤 4*的表示：
- en: '![](img/bcfecfcc-1965-401b-b353-82b078608429.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcfecfcc-1965-401b-b353-82b078608429.png)'
- en: 'Figure 10.3: Parameter averaging'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：参数平均化
- en: In this diagram, **W** represents the parameters (weights and biases) in the
    network. In DL4J, this implementation uses Spark's TreeAggregate ([https://umbertogriffo.gitbooks.io/apache-spark-best-practices-and-tuning/content/treereduce_and_treeaggregate_demystified.html](https://umbertogriffo.gitbooks.io/apache-spark-best-practices-and-tuning/content/treereduce_and_treeaggregate_demystified.html)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在此图中，**W**表示网络中的参数（权重和偏置）。在 DL4J 中，这一实现使用了 Spark 的 TreeAggregate（[https://umbertogriffo.gitbooks.io/apache-spark-best-practices-and-tuning/content/treereduce_and_treeaggregate_demystified.html](https://umbertogriffo.gitbooks.io/apache-spark-best-practices-and-tuning/content/treereduce_and_treeaggregate_demystified.html)）。
- en: Parameter averaging is a simple approach, but it comes with some challenges.
    The most intuitive idea for doing averaging is to simply average the parameters
    after each iteration. While this approach can work, the added overhead could be
    extremely high and the network communication and synchronization costs may nullify
    any benefit from scaling the cluster by adding extra nodes. For this reason, parameter
    averaging is typically implemented with an averaging period (number of minibatches
    per worker) greater than one. If the averaging period is too infrequent, the local
    parameters in each worker may significantly diverge, resulting in a poor model.
    Good averaging periods are of the order of once in every 10 to 20 minibatches
    per worker. Another challenge is related to the optimization methods (the updaters
    of DL4J). It has been demonstrated that these methods ([http://ruder.io/optimizing-gradient-descent/](http://ruder.io/optimizing-gradient-descent/))
    improve the convergence properties during neural network training. But they have
    an internal state that could probably be averaged as well. This results in a faster
    convergence in each worker, but at the cost of doubling the size of the network
    transfers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 参数平均化是一种简单的方法，但它带来了一些挑战。最直观的平均化方法是每次迭代后直接对参数进行平均。虽然这种方法是可行的，但增加的开销可能非常高，网络通信和同步成本可能会抵消通过增加额外节点来扩展集群的任何好处。因此，参数平均化通常会在平均周期（每个工作节点的最小批次数量）大于一时实现。如果平均周期过于稀疏，每个工作节点的局部参数可能会显著偏离，导致模型效果不佳。合适的平均周期通常是每个工作节点每
    10 到 20 次最小批次中进行一次。另一个挑战与优化方法（DL4J 的更新方法）相关。已有研究表明，这些方法（[http://ruder.io/optimizing-gradient-descent/](http://ruder.io/optimizing-gradient-descent/)）能够改善神经网络训练过程中的收敛性。但它们有一个内部状态，也可能需要进行平均化。这将导致每个工作节点的收敛速度更快，但代价是网络传输的大小翻倍。
- en: Asynchronous stochastic gradient sharing
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步随机梯度共享
- en: 'Asynchronous stochastic gradient sharing is the approach that has been chosen
    in the latest release of DL4J (and future ones as well). The main difference between
    asynchronous stochastic gradient sharing and parameter averaging is that in asynchronous
    stochastic gradient sharing, updates instead of parameters are transferred from
    the workers to the parameter server. From an architectural perspective, this is
    similar to parameter averaging (see the following diagram):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 异步随机梯度共享是最新版本的DL4J（以及未来版本）所选择的方法。异步随机梯度共享和参数平均的主要区别在于，在异步随机梯度共享中，更新而不是参数被从工作者传递到参数服务器。从架构角度来看，这与参数平均类似（参见下图）：
- en: '![](img/79e28023-42f8-4305-8972-923b5354f0be.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/79e28023-42f8-4305-8972-923b5354f0be.png)'
- en: Figure 10.4: Asynchronous stochastic gradient sharing architecture
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4：异步随机梯度共享架构
- en: 'What is different is the formula through which the parameters are calculated:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 不同之处在于计算参数的公式：
- en: '![](img/159c04a0-1d94-43b1-bc89-5ad05105fc87.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/159c04a0-1d94-43b1-bc89-5ad05105fc87.png)'
- en: Here, ![](img/c91cb8c9-1433-4549-b7aa-381e8950a0a3.png) is the scaling factor.
    The asynchronous stochastic gradient sharing algorithm is obtained by allowing
    the updates ![](img/108002a8-d16d-451e-836f-f5c30eb9e7bf.png) to be applied to
    the parameter vectors as soon as they are computed.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/c91cb8c9-1433-4549-b7aa-381e8950a0a3.png) 是缩放因子。通过允许将更新 ![](img/108002a8-d16d-451e-836f-f5c30eb9e7bf.png)
    在计算完成后立即应用到参数向量，从而得到异步随机梯度共享算法。
- en: 'One of the main benefits of asynchronous stochastic gradient sharing is that
    it is possible to obtain higher throughput in a distributed system, rather than
    waiting for the parameter averaging step to be completed, so the workers can spend
    more time performing useful computations. Another benefit is the following: the
    workers can potentially incorporate parameter updates from other workers sooner,
    when compared to the case of using synchronous updating.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 异步随机梯度共享的主要优点之一是，它可以在分布式系统中获得更高的吞吐量，而无需等待参数平均步骤完成，从而使工作者可以花更多时间进行有用的计算。另一个优点是：与同步更新的情况相比，工作者可以更早地结合来自其他工作者的参数更新。
- en: 'One downside of asynchronous stochastic gradient sharing is the so-called stale
    gradient problem. The calculation of gradients (updates) requires time, and by
    the time a worker has finished his calculations and applied the results to the
    global parameter vector, the parameters may have been updated more than once (a
    problem you can''t see in the parameter averaging, as this has a synchronous nature).
    Several approaches have been proposed in order to mitigate the stale gradient
    problem. Among these, one is by scaling the value ![](img/c91cb8c9-1433-4549-b7aa-381e8950a0a3.png)
    separately for each update, based on the staleness of the gradients. Another way
    is called soft synchronization: rather than updating the global parameter vector
    immediately, the parameter server waits to collect a given number of updates from
    any of the learners. Then, the formula through which the parameters are updated
    becomes this:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 异步随机梯度共享的一个缺点是所谓的陈旧梯度问题。梯度（更新）的计算需要时间，当一个工作者完成计算并将结果应用到全局参数向量时，参数可能已经更新了不止一次（这个问题在参数平均中看不出来，因为参数平均是同步的）。为了解决陈旧梯度问题，已经提出了几种方法。其中一种方法是根据梯度的陈旧程度，针对每次更新单独调整值
    ![](img/c91cb8c9-1433-4549-b7aa-381e8950a0a3.png)。另一种方法称为软同步：与其立即更新全局参数向量，参数服务器会等待收集来自任何学习者的一定数量的更新。然后，通过该公式更新参数：
- en: '![](img/7ed522f5-d93c-42d3-997c-26e0fea27211.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ed522f5-d93c-42d3-997c-26e0fea27211.png)'
- en: Here, *s* is the number of updates that the parameter server waits to collect
    and ![](img/7fbd793e-4160-4bee-bba8-18fec6a7f311.png) is a Scalar staleness-dependent
    scaling factor.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*s* 是参数服务器等待收集的更新数量，![](img/7fbd793e-4160-4bee-bba8-18fec6a7f311.png) 是与陈旧程度相关的标量缩放因子。
- en: In DL4J, while the parameter averaging implementation has always been fault
    tolerant, the gradient sharing implementation has been made fully fault tolerant
    starting from release 1.0.0-beta3.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DL4J 中，尽管参数平均实现一直是容错的，但从 1.0.0-beta3 版本开始，梯度共享实现已完全具备容错能力。
- en: Importing Python models into the JVM with DL4J
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Python 模型导入到 JVM 中使用 DL4J
- en: In the previous chapter, we have learned how powerful and, at the same time,
    how easy the DL4J APIs are when it comes to configuring, building, and training
    multilayer neural network models. The possibilities to implement new models are
    almost innumerable relying on this framework only in Scala or Java.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们已经学习了在配置、构建和训练多层神经网络模型时，DL4J API是如何强大且易于使用的。仅依靠这个框架，在Scala或Java中实现新模型的可能性几乎是无穷无尽的。
- en: 'But, let''s have a look at the following search results from Google; they concern
    TensorFlow neural network models that are available on the web:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，让我们来看一下Google的以下搜索结果；它们是关于网络上可用的TensorFlow神经网络模型：
- en: '![](img/d75b5945-793d-4b08-b910-141e38adc3e8.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d75b5945-793d-4b08-b910-141e38adc3e8.png)'
- en: 'Figure 10.5: The result of a Google search about TensorFlow neural network
    models'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：关于TensorFlow神经网络模型的Google搜索结果
- en: You can see that it is quite an impressive number in terms of results. And this
    is just a raw search. Refining the search to more specific model implementations
    means that the numbers are pretty high. But what's TensorFlow? TensorFlow ([https://www.tensorflow.org/](https://www.tensorflow.org/))
    is a powerful and comprehensive open source framework for ML and DL, developed
    by the Google Brain team. At present, it is the most popularly used framework
    by data scientists. So it has a big community, and lots of shared models and examples
    are available for it. This explains the big numbers. Among those models, the chances
    of finding a pre-trained model that fits your specific use case needs are high.
    So, where's the problem? TensorFlow is mostly Python.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，从结果来看，这是一个相当令人印象深刻的数字。而这只是一个原始搜索。将搜索精炼到更具体的模型实现时，数字会更高。但什么是TensorFlow？TensorFlow
    ([https://www.tensorflow.org/](https://www.tensorflow.org/)) 是一个功能强大且全面的开源框架，专为机器学习（ML）和深度学习（DL）开发，由Google
    Brain团队研发。目前，它是数据科学家最常用的框架。因此，它拥有庞大的社区，许多共享的模型和示例可以使用。这也解释了这些庞大的数字。在这些模型中，找到一个符合你特定使用场景需求的预训练模型的几率是很高的。那么，问题在哪里呢？TensorFlow主要是Python。
- en: 'It provides support for other programming languages, such as Java for the JVM,
    but its Java API is currently experimental and isn''t covered by the TensorFlow
    API stability guarantees. Furthermore, the TensorFlow Python API presents a steep
    learning curve for non-Python developers and software engineers with no or a basic
    data science background. How then can they benefit from this framework? How can
    we reuse an existing valid model in a JVM-based environment? Keras ([https://keras.io/](https://keras.io/))
    comes to the rescue. It is an open source, high-level neural network library written
    in Python that can be used to replace the TensorFlow high-level API (the following
    diagram shows the TensorFlow framework architecture):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 它也支持其他编程语言，比如Java（适用于JVM），但是它的Java API目前还处于实验阶段，并且不包含在TensorFlow的API稳定性保证中。此外，TensorFlow的Python
    API对于没有Python开发经验的开发者和没有或只有基础数据科学背景的软件工程师来说，学习曲线较为陡峭。那么，他们如何能从这个框架中受益呢？我们如何在基于JVM的环境中复用现有的有效模型？Keras
    ([https://keras.io/](https://keras.io/)) 来解救了我们。它是一个开源的高层神经网络库，用Python编写，可以用来替代TensorFlow的高层API（下图展示了TensorFlow框架架构）：
- en: '![](img/c632938e-2c0f-4e23-9ea7-0b8593e06ea9.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c632938e-2c0f-4e23-9ea7-0b8593e06ea9.png)'
- en: 'Figure 10.6: The TensorFlow architecture'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6：TensorFlow架构
- en: Compared to TensorFlow, Keras is lightweight and allows easier prototyping.
    It can run not only on top of TensorFlow, but also on other backend Python engines.
    And last but not least, it can be used to import Python models into DL4J. The
    Keras Model Import DL4J library provides facilities for importing neural network
    models configured and trained through the Keras framework.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于TensorFlow，Keras更加轻量，且更易于原型开发。它不仅可以运行在TensorFlow之上，还可以运行在其他后端Python引擎上。而且，Keras还可以用于将Python模型导入到DL4J。Keras模型导入DL4J库提供了导入通过Keras框架配置和训练的神经网络模型的功能。
- en: 'The following diagram shows that once a model has been imported into DL4J,
    the full production stack is at disposal for using it:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一旦模型被导入到DL4J后，可以使用完整的生产堆栈来使用它：
- en: '![](img/841beb3d-cdf3-4dff-86f9-c55d732296d5.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/841beb3d-cdf3-4dff-86f9-c55d732296d5.png)'
- en: 'Figure 10.7: Importing a Keras model into DL4J'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：将Keras模型导入DL4J
- en: 'Let''s now go into detail to understand how this happens. For the examples
    in this section, I am assuming you have already Python 2.7.x and the `pip` ([https://pypi.org/project/pip/](https://pypi.org/project/pip/))
    package installer for Python on your machine. In order to implement a model in
    Keras, we have to install Keras itself and choose a backend (TensorFlow for the
    examples presented here). TensorFlow has to be installed first, as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们来详细了解这一过程。对于本节中的示例，我假设你已经在机器上安装了 Python 2.7.x 和 `pip`（[https://pypi.org/project/pip/](https://pypi.org/project/pip/)）包管理器。为了在
    Keras 中实现模型，我们必须先安装 Keras 并选择一个后端（此处示例选择 TensorFlow）。必须首先安装 TensorFlow，如下所示：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'That''s for the CPU only. If you need to run it on GPUs, you need to install
    the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅适用于 CPU。如果你需要在 GPU 上运行，需要安装以下内容：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can now install Keras, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以安装 Keras，如下所示：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Keras uses TensorFlow as its default tensor manipulation library, so no extra
    action has to be taken if TensorFlow is our backend of choice.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 使用 TensorFlow 作为默认的张量操作库，因此如果我们选择 TensorFlow 作为后端，就无需采取额外的操作。
- en: 'Let''s start simple, implementing an MLP model using the Keras API. After the
    necessary imports, enter the following lines of code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从简单的开始，使用 Keras API 实现一个 MLP 模型。在进行必要的导入后，输入以下代码：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We create a `Sequential` model, as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个 `Sequential` 模型，如下所示：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we add layers through the `add` method of `Sequential`, as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过 `Sequential` 的 `add` 方法添加层，如下所示：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The configuration of the learning process for this model can be done through
    the `compile` method, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的学习过程配置可以通过 `compile` 方法完成，如下所示：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we serialize the model in HDF5 format, as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将模型序列化为 HDF5 格式，如下所示：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Hierarchical Data Format** (**HDF**) is a set of file formats (with the extensions
    .hdf5 and .h5) to store and manage large amounts of data, in particular multidimensional
    numeric arrays. Keras uses it to save and load models.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**层次数据格式** (**HDF**) 是一组文件格式（扩展名为 .hdf5 和 .h5），用于存储和管理大量数据，特别是多维数字数组。Keras
    使用它来保存和加载模型。'
- en: 'After saving this simple program, `basic_mlp.py`, and running it, as follows
    the model will be serialized and saved in the `basic_mlp.h5` file:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 保存这个简单程序 `basic_mlp.py` 并运行后，模型将被序列化并保存在 `basic_mlp.h5` 文件中：
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we are ready to import this model into DL4J. We need to add to the Scala
    project the usual DataVec API, DL4J core, and ND4J dependencies, plus the DL4J
    model import library, as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好将此模型导入到 DL4J 中。我们需要将通常的 DataVec API、DL4J 核心和 ND4J 依赖项，以及 DL4J 模型导入库添加到
    Scala 项目中，如下所示：
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Copy the `basic_mlp.h5` file in the resource folder of the project, then programmatically
    get its path, as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `basic_mlp.h5` 文件复制到项目的资源文件夹中，然后通过编程方式获取其路径，如下所示：
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Then, load the model as DL4J `MultiLayerNetwork`*,* using the `importKerasSequentialModelAndWeights`
    method of the `KerasModelImport` class ([https://static.javadoc.io/org.deeplearning4j/deeplearning4j-modelimport/1.0.0-alpha/org/deeplearning4j/nn/modelimport/keras/KerasModelImport.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-modelimport/1.0.0-alpha/org/deeplearning4j/nn/modelimport/keras/KerasModelImport.html)),
    as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过 `KerasModelImport` 类的 `importKerasSequentialModelAndWeights` 方法（[https://static.javadoc.io/org.deeplearning4j/deeplearning4j-modelimport/1.0.0-alpha/org/deeplearning4j/nn/modelimport/keras/KerasModelImport.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-modelimport/1.0.0-alpha/org/deeplearning4j/nn/modelimport/keras/KerasModelImport.html)）将模型加载为
    DL4J 的 `MultiLayerNetwork`，如下所示：
- en: '[PRE26]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Generate some mock data, as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 生成一些模拟数据，如下所示：
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, we can train the model the usual way in DL4J, as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以像往常一样在 DL4J 中训练模型，如下所示：
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: All the considerations made in [Chapter 7](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml),
    *Training Neural Networks with Spark*, [Chapter 8](b30120ea-bd42-4cb7-95d9-5ecaa2b7c181.xhtml),
    *Monitoring and Debugging Neural Network Training*, and [Chapter 9](869a9495-e759-4810-8623-d8b76ba61398.xhtml),
    *Interpreting Neural Network Output*, about training, monitoring, and evaluation
    with DL4J, apply here too.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 7 章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)，《使用 Spark 训练神经网络》，[第 8
    章](b30120ea-bd42-4cb7-95d9-5ecaa2b7c181.xhtml)，《监控与调试神经网络训练》，和 [第 9 章](869a9495-e759-4810-8623-d8b76ba61398.xhtml)，《解释神经网络输出》中关于训练、监控和评估
    DL4J 的内容，也适用于这里。
- en: 'It is possible, of course, to train the model in Keras (as in the following
    example):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，也可以在 Keras 中训练模型（如下示例）：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here, `x_train` and `y_train` are NumPy ([http://www.numpy.org/](http://www.numpy.org/))
    arrays) and evaluate it before saving it in serialized form, as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`x_train` 和 `y_train` 是 NumPy ([http://www.numpy.org/](http://www.numpy.org/))
    数组，并在保存为序列化格式之前进行评估，方法如下：
- en: '`loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)`'
- en: You can finally import the pre-trained model in the same way as explained previously,
    and just run it.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像之前所解释的那样，导入预训练模型，然后直接运行它。
- en: The same as for `Sequential` model imports, DL4J allows also the importing of
    Keras `Functional` models.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `Sequential` 模型导入一样，DL4J 也允许导入 Keras 的 `Functional` 模型。
- en: The latest versions of DL4J, also allow the importing of TensorFlow models.
    Imagine you want to import this ([https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py))
    pre-trained model (a CNN estimator for the `MNIST` database). At the end of the
    training, which happens in TensorFlow, you can save the model in a serialized
    form. TensorFlow's file format is based on Protocol Buffers ([https://developers.google.com/protocol-buffers/?hl=en](https://developers.google.com/protocol-buffers/?hl=en)),
    which is a language and platform neutral extensible serialization mechanism for
    structured data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最新版本的 DL4J 还允许导入 TensorFlow 模型。假设你想要导入这个([https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py](https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py))预训练模型（一个用于`MNIST`数据库的
    CNN 估算器）。在 TensorFlow 中进行训练后，你可以将模型保存为序列化格式。TensorFlow 的文件格式基于协议缓冲区（[https://developers.google.com/protocol-buffers/?hl=en](https://developers.google.com/protocol-buffers/?hl=en)），它是一种语言和平台中立的可扩展序列化机制，用于结构化数据。
- en: 'Copy the serialized `mnist.pb` file into the resource folder of the DL4J Scala
    project and then programmatically get it and import the model, as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 将序列化的 `mnist.pb` 文件复制到 DL4J Scala 项目的资源文件夹中，然后通过编程方式获取并导入模型，方法如下：
- en: '[PRE30]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, feed the model with images and start to do predictions, as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，给模型输入图像并开始进行预测，方法如下：
- en: '[PRE31]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Alternatives to DL4J for the Scala programming language
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scala 编程语言的 DL4J 替代方案
- en: DL4J isn't the only framework for deep learning available for the Scala programming
    language. Two open source alternatives exist. In this section, we are going to
    learn more about them and do a comparison with DL4J.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J 并不是唯一为 Scala 编程语言提供的深度学习框架，还有两个开源替代方案。在本节中，我们将详细了解它们，并与 DL4J 进行对比。
- en: BigDL
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: BigDL
- en: BigDL ([https://bigdl-project.github.io/0.6.0/](https://bigdl-project.github.io/0.6.0/))
    is an open source, distributed, deep learning framework for Apache Spark implemented
    by Intel ([https://www.intel.com](https://www.intel.com)). It is licensed with
    the Apache 2.0 license, the same as for DL4J. It has been implemented in Scala
    and exposes APIs for Scala and Python. It doesn't provide support for CUDA. While
    DL4J allows cross-platform execution in standalone mode (including Android mobile
    devices) and distributed mode (with and without Spark), BigDL has been designed
    to execute in a Spark cluster only. The available benchmarks state that training/running
    this framework is faster than the most popular Python frameworks, such as TensorFlow
    or Caffe, because BigDL uses the Intel Math Kernel Library (MKL, [https://software.intel.com/en-us/mkl](https://software.intel.com/en-us/mkl)),
    assuming it is running on Intel processor-based machines.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: BigDL ([https://bigdl-project.github.io/0.6.0/](https://bigdl-project.github.io/0.6.0/))
    是一个开源的分布式深度学习框架，适用于 Apache Spark，由英特尔 ([https://www.intel.com](https://www.intel.com))
    实现。它使用与 DL4J 相同的 Apache 2.0 许可证。它是用 Scala 实现的，并暴露了 Scala 和 Python 的 API。它不支持 CUDA。虽然
    DL4J 允许在独立模式（包括 Android 移动设备）和分布式模式（有或没有 Spark）下跨平台执行，但 BigDL 仅设计为在 Spark 集群中执行。现有的基准测试表明，训练/运行此框架比最流行的
    Python 框架（如 TensorFlow 或 Caffe）要快，因为 BigDL 使用英特尔数学核心库（MKL，[https://software.intel.com/en-us/mkl](https://software.intel.com/en-us/mkl)），前提是它运行在基于英特尔处理器的机器上。
- en: It provides high-level APIs for neural networks and the possibility to import
    Python models from Keras, Caffe, or Torch.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 它为神经网络提供了高级 API，并且支持从 Keras、Caffe 或 Torch 导入 Python 模型。
- en: While it has been implemented in Scala, at the time this chapter was written,
    it supports only Scala 2.10.x.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它是用 Scala 实现的，但在编写本章时，它仅支持 Scala 2.10.x。
- en: Looking at the latest evolution of this framework, it seems that Intel is going
    to provide more support for importing Python models implemented with other frameworks
    (and is starting also to provide support for some TensorFlow operations) and enhancements
    of the Python API, rather than the Scala API.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个框架的最新发展来看，英特尔似乎将提供更多对导入通过其他框架实现的 Python 模型的支持（并且也开始支持一些 TensorFlow 操作）以及
    Python API 的增强，而不是 Scala API。
- en: What about community and contributions? BigDL is supported and driven by Intel,
    which keeps an eye in particular on how this framework is used on hardware based
    on their microprocessors. So, this could be a potential risk in adopting this
    framework in other production hardware contexts. While DL4J is supported by Skymind
    ([https://skymind.ai/](https://skymind.ai/)), the company owned by Adam Gibson,
    who is one of the authors of this framework, the vision, in terms of future evolution,
    isn't restricted to the company business. The goal is to make the framework more
    comprehensive in terms of capabilities and try to further reduce the gap between
    the JVM languages and Python in relation to available numeric computation and
    DL tools/features. Also, the number of contributors, commits, and releases are
    increasing for DL4J.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，社区和贡献方面呢？BigDL 由英特尔支持和驱动，特别关注这个框架在基于其微处理器的硬件上的使用情况。因此，在其他生产硬件环境中采用这个框架可能存在潜在风险。而
    DL4J 由 Skymind（[https://skymind.ai/](https://skymind.ai/)）支持，该公司由 Adam Gibson
    拥有，Adam Gibson 是该框架的作者之一。就未来发展而言，DL4J 的愿景并不局限于公司的业务。目标是使框架在功能上更全面，并尝试进一步缩小 JVM
    语言与 Python 在可用数值计算和深度学习工具/功能方面的差距。同时，DL4J 的贡献者、提交和版本发布数量都在增加。
- en: Compared to the Scala BigDL API, the DL4J API for Scala (and Java) is more high
    level (some sort of DSL), which is in particular of great help for Scala developers
    approaching the DL world for the first time, as it speeds up the the process of
    getting familiar with the framework, and allows programmers to focus more on the
    model being trained and implemented.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Scala BigDL API 相比，DL4J 对 Scala（和 Java）的 API 更高层次（某种程度的领域特定语言），这对于第一次接触深度学习的
    Scala 开发者特别有帮助，因为它加快了熟悉框架的过程，并且让程序员可以更多地关注正在训练和实现的模型。
- en: If your plan is to stay in the JVM world, I definitely believe DL4J is a better
    choice than BigDL.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的计划是留在 JVM 世界，我绝对认为 DL4J 比 BigDL 更合适。
- en: DeepLearning.scala
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DeepLearning.scala
- en: DeepLearning.scala ([https://deeplearning.thoughtworks.school/](https://deeplearning.thoughtworks.school/))
    is a DL framework from ThoughtWorks ([https://www.thoughtworks.com/](https://www.thoughtworks.com/)).
    Implemented in Scala, the goal since the start has been, to get the most from
    the functional programming and object-oriented paradigms for this language. It
    supports GPU-accelerated *N*-dimensional arrays. Neural networks in this framework
    can be built from mathematical formulas, so it can calculate derivatives of the
    weights in the formulas used.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: DeepLearning.scala ([https://deeplearning.thoughtworks.school/](https://deeplearning.thoughtworks.school/))
    是来自 ThoughtWorks ([https://www.thoughtworks.com/](https://www.thoughtworks.com/))
    的深度学习框架。该框架用 Scala 实现，自开始以来，其目标就是最大化地利用函数式编程和面向对象编程范式。它支持 GPU 加速的 *N* 维数组。该框架中的神经网络可以通过数学公式构建，因此可以计算公式中权重的导数。
- en: This framework supports plugins, so it could be extended by writing custom plugins,
    which can then coexist along with the plugin set available out of the box (a significant
    set of plugins is currently available in terms of models, algorithms, hyperparameters,
    calculation features, and so on).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个框架支持插件，因此可以通过编写自定义插件来扩展它，这些插件可以与开箱即用的插件集共存（目前在模型、算法、超参数、计算功能等方面有一套相当丰富的插件）。
- en: DeepLearning.scala applications can run as standalone on the JVM, as Jupyter
    ([http://jupyter.org/](http://jupyter.org/)) notebooks, or as scripts in Ammonite
    ([http://ammonite.io/](http://ammonite.io/)).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: DeepLearning.scala 应用程序可以作为独立程序在 JVM 上运行，作为 Jupyter ([http://jupyter.org/](http://jupyter.org/))
    笔记本运行，或者作为 Ammonite ([http://ammonite.io/](http://ammonite.io/)) 脚本运行。
- en: Numerical computing happens through ND4J, the same as for DL4J.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 数值计算通过 ND4J 进行，与 DL4J 相同。
- en: It doesn't have support for Python, nor facilities to import models implemented
    through Python DL frameworks.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 它不支持 Python，也没有导入通过 Python 深度学习框架实现的模型的功能。
- en: 'One big difference between this framework and others, such as DL4J and BigDL,
    is the following: the structure of the neural networks is dynamically determined
    at runtime. All the Scala language features (functions, expressions, control flows,
    and so on) are available for implementation. Neural networks are Scala Monads,
    so they can be created by composing higher order functions, but that''s not the
    only option in DeepLearning.scala; the framework also provides a type class called
    `Applicative` (through the Scalaz library, [http://eed3si9n.com/learning-scalaz/Applicative.html](http://eed3si9n.com/learning-scalaz/Applicative.html)),
    which allows multiple calculations in parallel.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个框架与其他框架（如 DL4J 和 BigDL）之间的一个大区别如下：神经网络的结构在运行时动态确定。所有的 Scala 语言特性（函数、表达式、控制流等）都可以用于实现。神经网络是
    Scala 单子（Monads），因此可以通过组合高阶函数来创建，但这不是 DeepLearning.scala 中唯一的选项；该框架还提供了一种类型类 `Applicative`（通过
    Scalaz 库，[http://eed3si9n.com/learning-scalaz/Applicative.html](http://eed3si9n.com/learning-scalaz/Applicative.html)），它允许并行计算多个任务。
- en: No native support for Spark or Hadoop was available for this framework at the
    time this chapter was written.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本章撰写时，该框架并未提供对 Spark 或 Hadoop 的原生支持。
- en: DeepLearning.scala can be a good alternative to DL4J in those contexts where
    there's no need for Apache Spark distributed training, and where you want to implement
    things in pure Scala. In terms of APIs for this programming language, it is more
    observant of the principles of pure Scala programming than DL4J, which has targeted
    all of the languages that run on the JVM (starting from Java, then extending to
    Scala, Clojure, and others, including Android).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在不需要 Apache Spark 分布式训练的环境下，DeepLearning.scala 可以是 DL4J 的一个不错替代选择，特别是在你希望使用纯
    Scala 实现的情况下。在该编程语言的 API 方面，它比 DL4J 更遵循纯 Scala 编程原则，而 DL4J 的目标是所有在 JVM 上运行的语言（从
    Java 开始，然后扩展到 Scala、Clojure 等，甚至包括 Android）。
- en: 'The initial visions for these two frameworks are also different: DL4J started
    to target software engineers, while DeepLearning.scala has an approach more oriented
    toward data scientists. Still to be verified is its level of stability and performance
    in production, as it is younger than DL4J and has a smaller number of adopters
    in real use cases. The lack of support to import existing models from Python frameworks
    could also be a limitation, because you would need to build and train your model
    from scratch and can''t rely on existing Python models that may be an excellent
    fit for your specific use case. In terms of community and releases, at present
    it can''t of course compare with DL4J and BigDL (even if there is the chance that
    it could grow in the very near future). Last but not least, the official documentation
    and examples are limited and not yet as mature and comprehensive as they are for
    DL4J.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个框架的最初愿景也不同：DL4J 开始时针对的是软件工程师，而 DeepLearning.scala 的方法则更多地面向数据科学家。它在生产环境中的稳定性和性能仍待验证，因为它比
    DL4J 更年轻，并且在实际使用案例中的采用者较少。缺乏对从 Python 框架导入现有模型的支持也可能是一个限制，因为你需要从头开始构建和训练模型，而无法依赖于现有的
    Python 模型，后者可能非常适合你的特定用例。在社区和发布方面，目前它当然无法与 DL4J 和 BigDL 相提并论（尽管它有可能在不久的将来增长）。最后但同样重要的是，官方文档和示例仍然有限，且尚未像
    DL4J 那样成熟和全面。
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, some concepts to think about when moving DL4J to production
    have been discussed. In particular, we understood how heap and off-heap memory
    management should be set up, looked at extra considerations on GPUs setup, saw
    how to prepare job JARs to be submitted to Spark for training, and also saw how
    it is possible to import and integrate Python models into an existing DL4J JVM
    infrastructure. Finally, a comparison between DL4J and two other DL frameworks
    for Scala (BigDL and DeepLearning.scala) was presented, and the reasons why DL4J
    could be a better choice from a production perspective were detailed.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了将 DL4J 移动到生产环境时需要考虑的一些概念。特别是，我们理解了堆内存和非堆内存管理的设置方式，了解了 GPU 配置的额外考虑因素，学习了如何准备要提交给
    Spark 进行训练的作业 JAR 文件，并看到如何将 Python 模型导入并集成到现有的 DL4J JVM 基础设施中。最后，介绍了 DL4J 与另外两个针对
    Scala 的深度学习框架（BigDL 和 DeepLearning.scala）之间的比较，并详细阐述了为什么从生产角度来看，DL4J 可能是一个更好的选择。
- en: In the next chapter, the core concepts of Natural Language Processing (NLP)
    will be explained, and a complete Scala implementation of NLP using Apache Spark
    and its MLLib (Machine Learning Library) will be detailed. We will go through
    the potential limitations of this approach, before in [Chapter 12](2bff1c2a-d984-49a0-aa22-03bafeb05fbc.xhtml),
    *Textual Analysis and Deep Learning*, presenting the same solution using DL4J
    and/or Keras/TensorFlow.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，将解释自然语言处理（NLP）的核心概念，并详细介绍使用Apache Spark及其MLLib（机器学习库）实现NLP的完整Scala实现。我们将在[第12章](2bff1c2a-d984-49a0-aa22-03bafeb05fbc.xhtml)中，*文本分析与深度学习*，介绍使用DL4J和/或Keras/TensorFlow实现相同的解决方案，并探讨这种方法的潜在局限性。
