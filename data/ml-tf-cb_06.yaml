- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: In this chapter, we will introduce neural networks and how to implement them
    in TensorFlow. Most of the subsequent chapters will be based on neural networks,
    so learning how to use them in TensorFlow is very important.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍神经网络以及如何在TensorFlow中实现它们。后续大部分章节将基于神经网络，因此学习如何在TensorFlow中使用它们是非常重要的。
- en: Neural networks are currently breaking records in tasks such as image and speech
    recognition, reading handwriting, understanding text, image segmentation, dialog
    systems, autonomous car driving, and so much more. While some of these tasks will
    be covered in later chapters, it is important to introduce neural networks as
    a general-purpose, easy-to-implement machine learning algorithm, so that we can
    expand on it later.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络目前在图像和语音识别、阅读手写字、理解文本、图像分割、对话系统、自动驾驶汽车等任务中创下了纪录。虽然这些任务将在后续章节中涵盖，但介绍神经网络作为一种通用、易于实现的机器学习算法是很重要的，这样我们后面可以进一步展开。
- en: The concept of a neural network has been around for decades. However, it only
    recently gained traction because we now have the computational power to train
    large networks because of advances in processing power, algorithm efficiency,
    and data sizes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的概念已经存在了几十年。然而，直到最近，由于处理能力、算法效率和数据规模的进步，我们才有足够的计算能力来训练大型网络，因此神经网络才开始获得广泛关注。
- en: A neural network is, fundamentally, a sequence of operations applied to a matrix
    of input data. These operations are usually collections of additions and multiplications
    followed by the application of non-linear functions. One example that we have
    already seen is logistic regression, which we looked at in *Chapter 4*, *Linear
    Regression*. Logistic regression is the sum of partial slope-feature products
    followed by the application of the sigmoid function, which is non-linear. Neural
    networks generalize this a little more by allowing any combination of operations
    and non-linear functions, which includes the application of absolute values, maximums,
    minimums, and so on.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络本质上是一系列操作应用于输入数据矩阵的过程。这些操作通常是加法和乘法的组合，之后应用非线性函数。我们已经见过的一个例子是逻辑回归，这在*第4章*、*线性回归*中有所讨论。逻辑回归是各个偏斜特征乘积的和，之后应用非线性的sigmoid函数。神经网络通过允许任意组合的操作和非线性函数进行更广泛的泛化，这包括应用绝对值、最大值、最小值等。
- en: The most important trick to neural networks is called **backpropagation**. Backpropagation
    is a procedure that allows us to update model variables based on the learning
    rate and the output of the loss function. We used backpropagation to update our
    model variables in *Chapter 3*, *Keras*, and *Chapter 4*, *Linear Regression*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中最重要的技巧叫做**反向传播**。反向传播是一种允许我们根据学习率和损失函数输出更新模型变量的过程。我们在*第3章*、*Keras*和*第4章*、*线性回归*中都使用了反向传播来更新模型变量。
- en: Another important feature to take note of regarding neural networks is the non-linear
    activation function. Since most neural networks are just combinations of addition
    and multiplication operations, they will not be able to model non-linear datasets.
    To address this issue, we will use non-linear activation functions in our neural
    networks. This will allow the neural network to adapt to most non-linear situations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 关于神经网络，另一个需要注意的重要特性是非线性激活函数。由于大多数神经网络只是加法和乘法操作的组合，它们无法对非线性数据集进行建模。为了解决这个问题，我们将在神经网络中使用非线性激活函数。这将使神经网络能够适应大多数非线性情况。
- en: It is important to remember that, as we have seen in many of the algorithms
    covered, neural networks are sensitive to the hyperparameters we choose. In this
    chapter, we will explore the impact of different learning rates, loss functions,
    and optimization procedures.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，正如我们在许多已介绍的算法中看到的，神经网络对我们选择的超参数非常敏感。在本章中，我们将探讨不同学习率、损失函数和优化过程的影响。
- en: 'There are a few more resources I would recommend to you for learning about
    neural networks that cover the topic in greater depth and more detail:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些我推荐的学习神经网络的资源，它们会更深入、更详细地讲解这个话题：
- en: The seminal paper describing backpropagation is *Efficient Back Prop* by Yann
    LeCun et al. The PDF is located here: [http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述反向传播的开创性论文是Yann LeCun等人所写的*Efficient Back Prop*。PDF文件位于这里：[http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)。
- en: CS231, *Convolutional Neural Networks for Visual Recognition*, by Stanford University.
    Class resources are available here: [http://cs231n.stanford.edu/](http://cs231n.stanford.edu/).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CS231，*卷积神经网络与视觉识别*，由斯坦福大学提供。课程资源可以在这里找到：[http://cs231n.stanford.edu/](http://cs231n.stanford.edu/)。
- en: CS224d, *Deep Learning for Natural Language Processing*, by Stanford University.
    Class resources are available here: [http://cs224d.stanford.edu/](http://cs224d.stanford.edu/).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CS224d，*自然语言处理的深度学习*，由斯坦福大学提供。课程资源可以在这里找到：[http://cs224d.stanford.edu/](http://cs224d.stanford.edu/)。
- en: '*Deep Learning*, a book by the MIT Press. Goodfellow, et al. 2016\. The book
    is located here: [http://www.deeplearningbook.org](http://www.deeplearningbook.org).'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*深度学习*，由MIT出版社出版的书籍。Goodfellow等，2016年。本书可以在这里找到：[http://www.deeplearningbook.org](http://www.deeplearningbook.org)。'
- en: The online book *Neural Networks and Deep Learning* by Michael Nielsen, which
    is located here: [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/).
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线书籍 *神经网络与深度学习* 由Michael Nielsen编写，可以在这里找到：[http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)。
- en: For a more pragmatic approach and introduction to neural networks, Andrej Karpathy
    has written a great summary with JavaScript examples called *A Hacker's Guide
    to Neural Networks*. The write-up is located here: [http://karpathy.github.io/neuralnets/](http://karpathy.github.io/neuralnets/).
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于更务实的方法和神经网络的介绍，Andrej Karpathy写了一个很棒的总结，里面有JavaScript示例，叫做 *黑客的神经网络指南*。该文可以在这里找到：[http://karpathy.github.io/neuralnets/](http://karpathy.github.io/neuralnets/)。
- en: Another site that summarizes deep learning well is called *Deep Learning for
    Beginners* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. The web page
    can be found here: [http://randomekek.github.io/deep/deeplearning.html](http://randomekek.github.io/deep/deeplearning.html).
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个很好的总结深度学习的网站是 *深度学习入门*，由Ian Goodfellow、Yoshua Bengio 和 Aaron Courville编写。该网页可以在这里找到：[http://randomekek.github.io/deep/deeplearning.html](http://randomekek.github.io/deep/deeplearning.html)。
- en: We will start by introducing the basic concepts of neural networking before
    working up to multilayer networks. In the last section, we will create a neural
    network that will learn how to play Tic-Tac-Toe.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从介绍神经网络的基本概念开始，然后逐步深入到多层网络。在最后一节，我们将创建一个神经网络，学习如何玩井字游戏。
- en: 'In this chapter, we''ll cover the following recipes:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下内容：
- en: Implementing operational gates
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现操作门
- en: Working with gates and activation functions
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与门和激活函数的工作
- en: Implementing a one-layer neural network
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现单层神经网络
- en: Implementing different layers
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现不同的层
- en: Using a multilayer neural network
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多层神经网络
- en: Improving the predictions of linear models
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改善线性模型的预测
- en: Learning to play Tic-Tac-Toe
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习玩井字游戏
- en: The reader can find all of the code from this chapter online at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook), and
    on the Packt repository at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以在 [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)
    上找到本章的所有代码，并在Packt的代码库中找到：[https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)。
- en: Implementing operational gates
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现操作门
- en: One of the most fundamental concepts of neural networks is its functioning as
    an operational gate. In this section, we will start with a multiplication operation
    as a gate, before moving on to consider nested gate operations.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络最基本的概念之一是它作为操作门的功能。在本节中，我们将从乘法运算作为门开始，然后再考虑嵌套的门操作。
- en: Getting ready
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The first operational gate we will implement is *f*(*x*) = *a* · *x*:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现的第一个操作门是 *f*(*x*) = *a* · *x*：
- en: '![](img/B16254_06_01.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_06_01.png)'
- en: To optimize this gate, we declare the *a* input as a variable and *x* as the
    input tensor of our model. This means that TensorFlow will try to change the *a* value
    and not the *x* value. We will create the loss function as the difference between
    the output and the target value, which is 50.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化这个门，我们将 *a* 输入声明为变量，并将*x*作为我们模型的输入张量。这意味着TensorFlow会尝试改变 *a* 的值，而不是 *x* 的值。我们将创建损失函数，它是输出值和目标值之间的差异，目标值为50。
- en: 'The second, nested, operational gate will be *f*(*x*) = *a* · *x* + *b*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个嵌套的操作门将是 *f*(*x*) = *a* · *x* + *b*：
- en: '![](img/B16254_06_02.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_06_02.png)'
- en: Again, we will declare *a* and *b* as variables and *x* as the input tensor
    of our model. We optimize the output toward the target value of 50 again. The
    interesting thing to note is that the solution for this second example is not
    unique. There are many combinations of model variables that will allow the output
    to be 50\. With neural networks, we do not care so much about the values of the
    intermediate model variables, but instead place more emphasis on the desired output.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将声明 *a* 和 *b* 为变量，*x* 为我们模型的输入张量。我们再次优化输出，使其趋向目标值 50。需要注意的有趣之处在于，第二个示例的解并不是唯一的。有许多不同的模型变量组合都能使输出为
    50。对于神经网络，我们并不太在意中间模型变量的值，而更关注最终的输出值。
- en: How to do it...
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'To implement the first operational gate, *f*(*x*) = *a* · *x*, in TensorFlow
    and train the output toward the value of 50, follow these steps:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 TensorFlow 中实现第一个运算门 *f*(*x*) = *a* · *x*，并将输出训练到值 50，请按照以下步骤操作：
- en: 'Start off by loading TensorFlow as follows:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，通过以下方式加载 TensorFlow：
- en: '[PRE0]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we will need to declare our model variable and input data. We make our
    input data equal to the value 5, so that the multiplication factor to get 50 will
    be 10 (that is, 5*10=50), as follows:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要声明我们的模型变量和输入数据。我们将输入数据设为 5，这样乘数因子将是 10（即 5*10=50），具体如下：
- en: '[PRE1]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we create a lambda layer that computes the operation, and we create a
    functional Keras model with the following input:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个 Lambda 层来计算操作，并创建一个具有以下输入的功能性 Keras 模型：
- en: '[PRE2]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will now declare our optimizing algorithm as the stochastic gradient descent
    as follows:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将声明我们的优化算法为随机梯度下降，具体如下：
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now optimize our model output toward the desired value of 50\. We will
    use the loss function as the L2 distance between the output and the desired target
    value of 50\. We do this by continually feeding in the input value of 5 and backpropagating
    the loss to update the model variable toward the value of 10, shown as follows:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以优化我们的模型输出，使其趋向目标值 50。我们将使用损失函数作为输出与目标值 50 之间的 L2 距离。我们通过不断输入值 5 并反向传播损失来更新模型变量，使其趋向
    10，具体如下所示：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding step should result in the following output:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的步骤应该会得到以下输出：
- en: '[PRE5]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next, we will do the same with the two-nested operational gate, *f*(*x*) = *a*
    · *x* + *b*.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将对嵌套的两个运算门 *f*(*x*) = *a* · *x* + *b* 进行相同的操作。
- en: 'We will start in exactly the same way as the preceding example, but will initialize
    two model variables, `a` and `b`, as follows:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将以与前一个示例完全相同的方式开始，但会初始化两个模型变量，`a` 和 `b`，具体如下：
- en: '[PRE6]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We now optimize the model variables to train the output toward the target value
    of 50, shown as follows:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在优化模型变量，将输出训练到目标值 50，具体如下所示：
- en: '[PRE7]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding step should result in the following output:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的步骤应该会得到以下输出：
- en: '[PRE8]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It is important to note here that the solution to the second example is not
    unique. This does not matter as much in neural networks, as all parameters are
    adjusted toward reducing the loss. The final solution here will depend on the
    initial values of a and b. If these were randomly initialized, instead of to the
    value of 1, we would see different ending values for the model variables for each
    iteration.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，第二个示例的解并不是唯一的。这在神经网络中并不那么重要，因为所有参数都被调整以减少损失。这里的最终解将取决于 *a* 和 *b* 的初始值。如果这些值是随机初始化的，而不是设为
    1，我们会看到每次迭代后模型变量的最终值不同。
- en: How it works...
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We achieved the optimization of a computational gate via TensorFlow's implicit
    backpropagation. TensorFlow keeps track of our model's operations and variable
    values and makes adjustments in respect of our optimization algorithm specification
    and the output of the loss function.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 TensorFlow 的隐式反向传播实现了计算门的优化。TensorFlow 跟踪我们模型的操作和变量值，并根据我们的优化算法规范和损失函数的输出进行调整。
- en: We can keep expanding the operational gates while keeping track of which inputs
    are variables and which inputs are data. This is important to keep track of, because
    TensorFlow will change all variables to minimize the loss but not the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以继续扩展运算门，同时跟踪哪些输入是变量，哪些输入是数据。这一点非常重要，因为 TensorFlow 会调整所有变量以最小化损失，但不会更改数据。
- en: The implicit ability to keep track of the computational graph and update the
    model variables automatically with every training step is one of the great features
    of TensorFlow and what makes it so powerful.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 能够隐式跟踪计算图并在每次训练步骤中自动更新模型变量，是TensorFlow的一个重要特点，这也使得它非常强大。
- en: Working with gates and activation functions
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与门和激活函数的工作
- en: Now that we can link together operational gates, we want to run the computational
    graph output through an activation function. In this section, we will introduce
    common activation functions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将操作门连接在一起，我们希望通过激活函数运行计算图输出。在本节中，我们将介绍常见的激活函数。
- en: Getting ready
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In this section, we will compare and contrast two different activation functions: **sigmoid** and **rectified
    linear unit** (**ReLU**). Recall that the two functions are given by the following
    equations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将比较并对比两种不同的激活函数：**sigmoid**和**rectified linear unit**（**ReLU**）。回顾一下，这两个函数由以下方程给出：
- en: '![](img/B16254_06_03.png)![](img/B16254_06_04.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_06_03.png)![](img/B16254_06_04.png)'
- en: In this example, we will create two one-layer neural networks with the same
    structure, except that one will feed through the sigmoid activation and one will
    feed through the ReLU activation. The loss function will be governed by the L2
    distance from the value 0.75\. We will randomly pull batch data and then optimize
    the output toward 0.75.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将创建两个单层神经网络，它们具有相同的结构，唯一不同的是一个使用sigmoid激活函数，另一个使用ReLU激活函数。损失函数将由与0.75的L2距离决定。我们将随机提取批处理数据，然后优化输出使其趋向0.75。
- en: How to do it...
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We proceed with the recipe as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下步骤进行操作：
- en: 'We will start by loading the necessary libraries. This is also a good point
    at which we can bring up how to set a random seed with TensorFlow. Since we will
    be using a random number generator from NumPy and TensorFlow, we need to set a
    random seed for both. With the same random seeds set, we should be able to replicate
    the results. We do this with the following input:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从加载必要的库开始。这也是我们可以提到如何在TensorFlow中设置随机种子的好时机。由于我们将使用NumPy和TensorFlow的随机数生成器，因此我们需要为两者设置随机种子。设置相同的随机种子后，我们应该能够复制结果。我们通过以下输入来实现：
- en: '[PRE9]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now we need to declare our batch size, model variables, and data model inputs.
    Our computational graph will consist of feeding in our normally distributed data
    into two similar neural networks that differ only by the activation function at
    the end, shown as follows:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要声明我们的批处理大小、模型变量和数据模型输入。我们的计算图将包括将正态分布数据输入到两个相似的神经网络中，这两个网络仅在末端的激活函数上有所不同，如下所示：
- en: '[PRE10]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we''ll declare our two models, the sigmoid activation model and the ReLU
    activation model, as follows:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将声明我们的两个模型，sigmoid激活模型和ReLU激活模型，如下所示：
- en: '[PRE11]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we need to declare our optimization algorithm and initialize our variables,
    shown as follows:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要声明我们的优化算法并初始化变量，如下所示：
- en: '[PRE12]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we''ll loop through our training for 750 iterations for both models, as
    shown in the following code block. The loss functions will be the average L2 norm
    between the model output and the value of 0.75\. We will also save the loss output
    and the activation output values for plotting later on:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将对两个模型进行750次迭代训练，如下所示的代码块所示。损失函数将是模型输出与0.75的L2范数平均值。我们还将保存损失输出和激活输出值，稍后用于绘图：
- en: '[PRE13]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To plot the loss and the activation outputs, we need to input the following
    code:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要绘制损失和激活输出，我们需要输入以下代码：
- en: '[PRE14]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The activation output needs to be plotted as shown in the following diagram:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 激活输出需要如以下图所示进行绘制：
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/B8A59E6E.tmp](img/B16254_06_05.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/B8A59E6E.tmp](img/B16254_06_05.png)'
- en: 'Figure 6.1: Computational graph outputs from the network with the sigmoid activation
    and a network with the ReLU activation'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：具有sigmoid激活函数的网络与具有ReLU激活函数的网络的计算图输出
- en: 'The two neural networks work with a similar architecture and target (0.75)
    but with two different activation functions, sigmoid and ReLU. It is important
    to notice how much more rapidly the ReLU activation network converges to the desired
    target of 0.75 than the sigmoid activation, as shown in the following diagram:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个神经网络具有相似的架构和目标（0.75），但使用了两种不同的激活函数：sigmoid和ReLU。需要注意的是，ReLU激活网络比sigmoid激活网络更快速地收敛到0.75这一目标，如下图所示：
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/F46E84AC.tmp](img/B16254_06_06.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/F46E84AC.tmp](img/B16254_06_06.png)'
- en: 'Figure 6.2: This figure depicts the loss value of the sigmoid and the ReLU
    activation networks. Notice how extreme the ReLU loss is at the beginning of the
    iterations'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2：该图展示了sigmoid和ReLU激活网络的损失值。注意ReLU损失在迭代初期的极端情况
- en: How it works...
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Because of the form of the ReLU activation function, it returns the value of
    zero much more often than the sigmoid function. We consider this behavior as a
    type of sparsity. This sparsity results in a speeding up of convergence, but a
    loss of controlled gradients. On the other hand, the sigmoid function has very
    well-controlled gradients and does not risk the extreme values that the ReLU activation
    does, as illustrated in the following table:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ReLU激活函数的形式，它比sigmoid函数更频繁地返回零值。我们将这种行为视为一种稀疏性。这种稀疏性加速了收敛速度，但却失去了对梯度的控制。另一方面，sigmoid函数具有非常好的梯度控制，且不像ReLU激活函数那样会产生极端值，以下表格进行了说明：
- en: '| Activation function | Advantages | Disadvantages |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | 优势 | 劣势 |'
- en: '| Sigmoid | Less extreme outputs | Slower convergence |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Sigmoid | 输出较少极端 | 收敛较慢 |'
- en: '| ReLU | Quick convergence | Extreme output values possible |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ReLU | 快速收敛 | 可能产生极端输出值 |'
- en: There's more...
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: In this section, we compared the ReLU activation function and the sigmoid activation
    function for neural networks. There are many other activation functions that are
    commonly used for neural networks, but most fall into either one of two categories;
    the first category contains functions that are shaped like the sigmoid function,
    such as arctan, hypertangent, heaviside step, and so on; the second category contains
    functions that are shaped like the ReLU function, such as softplus, leaky ReLU,
    and so on. Most of what we discussed in this section about comparing the two functions
    will hold true for activations in either category. However, it is important to
    note that the choice of activation function has a big impact on the convergence
    and the output of neural networks.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们比较了ReLU激活函数和sigmoid激活函数在神经网络中的表现。虽然有许多其他激活函数常用于神经网络，但大多数都属于以下两类：第一类包含类似sigmoid函数形状的函数，如arctan、hypertangent、Heaviside阶跃函数等；第二类包含类似ReLU函数形状的函数，如softplus、leaky
    ReLU等。我们在本节中讨论的关于比较这两种函数的大部分内容，适用于这两类激活函数。然而，值得注意的是，激活函数的选择对神经网络的收敛性和输出有很大影响。
- en: Implementing a one-layer neural network
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现一个单层神经网络
- en: We have all of the tools needed to implement a neural network that operates
    on real data, so in this section, we will create a neural network with one layer
    that operates on the `Iris` dataset.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经具备了实施神经网络所需的所有工具，因此在本节中，我们将创建一个在`Iris`数据集上运行的单层神经网络。
- en: Getting ready
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this section, we will implement a neural network with one hidden layer. It
    will be important to understand that a fully connected neural network is based
    mostly on matrix multiplication. As such, it is important that the dimensions
    of the data and matrix are lined up correctly.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个具有一个隐藏层的神经网络。理解全连接神经网络大多数基于矩阵乘法是非常重要的。因此，确保数据和矩阵的维度正确对齐也很重要。
- en: Since this is a regression problem, we will use **mean squared error** (**MSE**)
    as the loss function.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个回归问题，我们将使用**均方误差**（**MSE**）作为损失函数。
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We proceed with the recipe as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按如下步骤继续操作：
- en: 'To create the computational graph, we''ll start by loading the following necessary
    libraries:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了创建计算图，我们将从加载以下必要的库开始：
- en: '[PRE15]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we''ll load the `Iris` data and store the length as the target value with
    the following code:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将加载`Iris`数据集，并通过以下代码将长度存储为目标值：
- en: '[PRE16]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Since the dataset is smaller, we will want to set a seed to make the results
    reproducible, as follows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于数据集较小，我们希望设置一个种子，使得结果可复现，具体如下：
- en: '[PRE17]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To prepare the data, we''ll create a 80-20 train-test split and normalize the
    `x` features to be between `0` and `1` via min-max scaling, shown as follows:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了准备数据，我们将创建一个80-20的训练集和测试集拆分，并通过min-max缩放将`x`特征规范化到`0`和`1`之间，具体如下所示：
- en: '[PRE18]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now we will declare the batch size and the data model input with the following
    code:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将声明批次大小和数据模型输入，具体代码如下：
- en: '[PRE19]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The important part is to declare our model variables with the appropriate shape.
    We can declare the size of our hidden layer to be any size we wish; in the following
    code block, we have set it to have five hidden nodes:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关键部分是用适当的形状声明我们的模型变量。我们可以声明我们隐藏层的大小为任意大小；在下面的代码块中，我们设置它为五个隐藏节点：
- en: '[PRE20]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We''ll now declare our model in two steps. The first step will be creating
    the hidden layer output and the second will be creating the `final_output` of
    the model, as follows:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将分两步声明我们的模型。第一步将创建隐藏层的输出，第二步将创建模型的`final_output`，如下所示：
- en: As a note, our model goes from three input features to five hidden nodes, and
    finally to one output value.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们的模型从三个输入特征到五个隐藏节点，最后到一个输出值。
- en: '[PRE21]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we''ll declare our optimizing algorithm with the following code:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将使用以下代码声明优化算法：
- en: '[PRE22]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we loop through our training iterations. We''ll also initialize two lists
    in which we can store our `train` and `test_loss` functions. In every loop, we
    also want to randomly select a batch from the training data for fitting to the
    model, shown as follows:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们循环执行训练迭代。我们还将初始化两个列表，用于存储我们的`train`和`test_loss`函数。在每个循环中，我们还希望从训练数据中随机选择一个批次，以适应模型，如下所示：
- en: '[PRE23]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can plot the losses with `matplotlib` and the following code:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以用`matplotlib`和以下代码绘制损失：
- en: '[PRE24]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We proceed with the recipe by plotting the following diagram:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续通过绘制以下图表来进行本次实验：
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/EE3BD051.tmp](img/B16254_06_07.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![临时链接](https://example.org)（img/B16254_06_07.png）'
- en: 'Figure 6.3: We plot the loss (MSE) of the train and test set'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3：我们绘制了训练集和测试集的损失（MSE）
- en: 'Note that we can also see that the train set loss is not as smooth as that
    in the test set. This is because of two reasons: the first is that we are using
    a smaller batch size than the test set, although not by much; the second cause
    is the fact that we are training on the train set, and the test set does not impact
    the variables of the model.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们还可以看到训练集的损失不像测试集那样平滑。这是由于两个原因：首先，我们使用的批量大小比测试集小，尽管差距不大；第二个原因是我们在训练集上训练，而测试集不影响模型的变量。
- en: How it works...
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'Our model has now been visualized as a neural network diagram, as shown in
    the following diagram:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型现在已经被可视化为神经网络图，如下所示：
- en: '![](img/B16254_06_08.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![临时链接](https://example.org)（img/B16254_06_08.png）'
- en: 'Figure 6.4: A neural network diagram'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4：神经网络图
- en: 'The preceding figure is a visualization of our neural network that has five
    nodes in the hidden layer. We are feeding in three values: the **sepal length**
    (**S.L.**), the **sepal width** (**S.W.**), and the **petal length** (**P.L.**).
    The target will be the petal width. In total, there will be 26 total variables
    in the model.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图是我们的神经网络的可视化，隐藏层有五个节点。我们输入三个值：**sepal length**（**S.L.**）、**sepal width**（**S.W.**）和**petal
    length**（**P.L.**）。目标将是花瓣宽度。总共，模型中将有 26 个变量。
- en: Implementing different layers
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现不同的层
- en: It is important to know how to implement different layers. In the preceding
    recipe, we implemented fully connected layers. In this recipe, we will further
    expand our knowledge of various layers.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要知道如何实现不同的层。在前面的示例中，我们实现了全连接层。在这个示例中，我们将进一步扩展我们对各种层的了解。
- en: Getting ready
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We have explored how to connect data inputs and a fully connected hidden layer,
    but there are more types of layers available as built-in functions inside TensorFlow.
    The most popular layers that are used are convolutional layers and maxpool layers.
    We will show you how to create and use such layers with input data and with fully
    connected data. First, we will look at how to use these layers on one-dimensional
    data, and then on two-dimensional data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了如何连接数据输入和完全连接的隐藏层，但是在 TensorFlow 中还有更多类型的内置函数可用作层。最常用的层是卷积层和最大池层。我们将展示如何在一维数据和二维数据上创建和使用这些层。首先，我们将看看如何在一维数据上使用这些层，然后是二维数据。
- en: While neural networks can be layered in any fashion, one of the most common
    designs is to use convolutional layers and fully connected layers to first create
    features. If we then have too many features, it is common to use a maxpool layer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络可以以任何方式分层，但最常见的设计之一是首先使用卷积层和全连接层创建特征。如果然后有太多的特征，常见的做法是使用最大池层。
- en: After these layers, non-linear layers are commonly introduced as activation
    functions. **Convolutional neural networks** (**CNNs**), which we will consider
    in *Chapter 8*, *Convolutional Neural Networks, *usually have convolutional, maxpool,
    and activation layers.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些层之后，通常会引入非线性层作为激活函数。**卷积神经网络**（**CNNs**），我们将在*第8章*中讨论，通常具有卷积层、最大池化层和激活层。
- en: How to do it...
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will first look at one-dimensional data. We need to generate a random array
    of data for this task using the following steps:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先查看一维数据。我们需要为这个任务生成一个随机数据数组，操作步骤如下：
- en: 'We''ll start by loading the libraries we need, as follows:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先加载所需的库，如下所示：
- en: '[PRE25]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now we''ll initialize some parameters and we''ll create the input data layer
    with the following code:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将初始化一些参数，并使用以下代码创建输入数据层：
- en: '[PRE26]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we will define a convolutional layer, as follows:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个卷积层，如下所示：
- en: For our example data, we have a batch size of `1`, a width of `1`, a height
    of `25`, and a channel size of `1`. Also note that we can calculate the output
    dimensions of convolutional layers with the `output_size=(W-F+2P)/S+1` formula,
    where `W` is the input size, `F` is the filter size, `P` is the padding size,
    and `S` is the stride size.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于我们的示例数据，批处理大小为`1`，宽度为`1`，高度为`25`，通道大小为`1`。还请注意，我们可以通过`output_size=(W-F+2P)/S+1`公式来计算卷积层的输出尺寸，其中`W`是输入尺寸，`F`是滤波器尺寸，`P`是填充尺寸，`S`是步幅尺寸。
- en: '[PRE27]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we add a ReLU activation layer, as follows:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们添加一个ReLU激活层，如下所示：
- en: '[PRE28]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now we''ll add a maxpool layer. This layer will create a `maxpool` on a moving
    window across our one-dimensional vector. For this example, we will initialize
    it to have a width of 5, shown as follows:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将添加一个最大池化层。这个层将在我们的一维向量上创建一个`maxpool`，并在一个移动窗口中应用。对于这个例子，我们将初始化它，使宽度为5，如下所示：
- en: TensorFlow's `maxpool` arguments are very similar to those of the convolutional
    layer. While a `maxpool` argument does not have a filter, it does have size, stride,
    and padding options. Since we have a window of 5 with valid padding (no zero padding),
    then our output array will have 4 fewer entries.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TensorFlow的`maxpool`参数与卷积层的非常相似。虽然`maxpool`参数没有滤波器，但它有大小、步幅和填充选项。由于我们有一个宽度为5的窗口，并且使用有效填充（没有零填充），所以我们的输出数组将少4个元素。
- en: '[PRE29]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The final layer that we will connect is the fully connected layer. Here, we
    will use a dense layer, as shown in the following code block:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将连接的最后一层是全连接层。在这里，我们将使用一个密集层，如下所示的代码块：
- en: '[PRE30]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now we''ll create the model, and print the output of each of the layers, as
    follows:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将创建模型，并打印每一层的输出，如下所示：
- en: '[PRE31]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding step should result in the following output:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的步骤应该会生成以下输出：
- en: '[PRE32]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: One-dimensional data is very important to consider for neural networks. Time
    series, signal processing, and some text embeddings are considered to be one-dimensional
    and are frequently used in neural networks.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一维数据对神经网络来说非常重要。时间序列、信号处理和一些文本嵌入都被认为是一维数据，并且在神经网络中被频繁使用。
- en: 'We will now consider the same types of layer in an equivalent order but for
    two-dimensional data:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将考虑相同类型的层，但对于二维数据，顺序是等效的：
- en: 'We will start by initializing the variables, as follows:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先初始化变量，如下所示：
- en: '[PRE33]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then we will initialize our input data layer. Since our data has a height and
    width already, we just need to expand it in two dimensions (a batch size of 1,
    and a channel size of 1) as follows:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将初始化输入数据层。由于我们的数据已经有了高度和宽度，我们只需要在两个维度上扩展它（批处理大小为1，通道大小为1），如下所示：
- en: '[PRE34]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Just as in the one-dimensional example, we now need to add a 2D convolutional
    layer. For the filter, we will use a random 2x2 filter, a stride of 2 in both
    directions, and valid padding (in other words, no zero padding). Because our input
    matrix is 10x10, our convolutional output will be 5x5, shown as follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像一维示例中一样，现在我们需要添加一个二维卷积层。对于滤波器，我们将使用一个随机的2x2滤波器，步幅为2，方向上都为2，并使用有效填充（换句话说，不使用零填充）。由于我们的输入矩阵是10x10，因此卷积输出将是5x5，如下所示：
- en: '[PRE35]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we add a ReLU activation layer, as follows:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们添加一个ReLU激活层，如下所示：
- en: '[PRE36]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Our maxpool layer is very similar to the one-dimensional case, except we have
    to declare a width and height for the maxpool window and the stride. In our case,
    we will use the same value for all spatial dimensions so we will set integer values,
    shown as follows:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的最大池化层与一维情况非常相似，唯一不同的是我们需要为最大池化窗口和步幅声明宽度和高度。在我们的例子中，我们将在所有空间维度上使用相同的值，因此我们将设置整数值，如下所示：
- en: '[PRE37]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Our fully connected layer is very similar to the one-dimensional output. We
    use a dense layer, as follows:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的全连接层与一维输出非常相似。我们使用一个稠密层，具体如下：
- en: '[PRE38]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now we''ll create the model, and print the output of each of the layers, as
    follows:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将创建模型，并打印每一层的输出，具体如下：
- en: '[PRE39]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The preceding step should result in the following output:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述步骤应该会得到以下输出：
- en: '[PRE40]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: How it works...
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We should now know how to use the convolutional and maxpool layers in TensorFlow
    with one-dimensional and two-dimensional data. Regardless of the shape of the
    input, we ended up with outputs of the same size. This is important for illustrating
    the flexibility of neural network layers. This section should also impress upon
    us again the importance of shapes and sizes in neural network operations.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在应该知道如何在TensorFlow中使用卷积层和最大池化层，处理一维和二维数据。不管输入的形状如何，我们最终都会得到相同大小的输出。这对于展示神经网络层的灵活性非常重要。本节还应再次提醒我们，形状和大小在神经网络操作中的重要性。
- en: Using a multilayer neural network
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多层神经网络
- en: We will now apply our knowledge of different layers to real data by using a
    multilayer neural network on the low birth weight dataset.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过在低出生体重数据集上使用多层神经网络，应用我们对不同层的知识于实际数据。
- en: Getting ready
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Now that we know how to create neural networks and work with layers, we will
    apply this methodology with the aim of predicting birth weights in the low birth
    weight dataset. We'll create a neural network with three hidden layers. The low
    birth weight dataset includes the actual birth weights and an indicator variable
    for whether the given birth weight is above or below 2,500 grams. In this example,
    we'll make the target the actual birth weight (regression) and then see what the
    accuracy is on the classification at the end. At the end, our model should be
    able to identify whether the birth weight will be <2,500 grams.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道如何创建神经网络并处理层，我们将应用这种方法来预测低出生体重数据集中的出生体重。我们将创建一个有三层隐藏层的神经网络。低出生体重数据集包括实际出生体重和一个指示变量，表明给定的出生体重是否高于或低于2500克。在这个示例中，我们将目标设为实际出生体重（回归），然后查看分类结果的准确性。最终，我们的模型应该能够判断出生体重是否低于2500克。
- en: How to do it...
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We proceed with the recipe as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下步骤继续操作：
- en: 'We will start by loading the libraries as follows:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将开始加载库，具体如下：
- en: '[PRE41]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We''ll now load the data from the website using the `requests` module. After
    this, we will split the data into features of interest and the target value, shown
    as follows:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`requests`模块从网站加载数据。之后，我们将把数据拆分为感兴趣的特征和目标值，具体如下：
- en: '[PRE42]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To help with repeatability, we now need to set the random seed for both NumPy
    and TensorFlow. Then we declare our batch size as follows:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了帮助结果的可重复性，我们现在需要为NumPy和TensorFlow设置随机种子。然后我们声明我们的批处理大小，具体如下：
- en: '[PRE43]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we split the data into an 80-20 train-test split. After this, we need
    to normalize our input features so that they are between 0 and 1 with min-max
    scaling, shown as follows:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将数据分割为80-20的训练集和测试集。之后，我们需要对输入特征进行归一化，使其值在0到1之间，采用最小-最大缩放，具体如下：
- en: '[PRE44]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Normalizing input features is a common feature transformation and is especially
    useful for neural networks. It will help with convergence if our data is centered
    between 0 and 1 for the activation functions.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对输入特征进行归一化是一种常见的特征转换方法，尤其对于神经网络特别有用。如果我们的数据在0到1之间进行中心化，这将有助于激活函数的收敛。
- en: 'Since we have multiple layers that have similar initialized variables, we now
    need to create a function to initialize both the weights and the bias. We do that
    with the following code:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们有多个具有相似初始化变量的层，现在我们需要创建一个函数来初始化权重和偏置。我们使用以下代码完成此任务：
- en: '[PRE45]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We now need to initialize our input data layer. There will be seven input features.
    The output will be the birth weight in grams:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要初始化我们的输入数据层。将有七个输入特征，输出将是出生体重（以克为单位）：
- en: '[PRE46]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The fully connected layer will be used three times for all three hidden layers.
    To prevent repeated code, we will create a layer function for use when we initialize
    our model, shown as follows:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 全连接层将用于所有三个隐藏层，每个层都会使用三次。为了避免重复代码，我们将创建一个层函数，以便在初始化模型时使用，具体如下：
- en: '[PRE47]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now it''s time to create our model. For each layer (and output layer), we will
    initialize a weight matrix, bias matrix, and the fully connected layer. For this
    example, we will use hidden layers of sizes 25, 10, and 3:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候创建我们的模型了。对于每一层（以及输出层），我们将初始化一个权重矩阵、一个偏置矩阵和全连接层。对于这个示例，我们将使用大小分别为25、10和3的隐藏层：
- en: The model that we are using will have 522 variables to fit. To arrive at this
    number, we can see that between the data and the first hidden layer we have 7*25+25=200 variables.
    If we continue in this way and add them up, we'll have 200+260+33+4=497 variables.
    This is significantly larger than the nine variables that we used in the logistic
    regression model on this data.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用的模型将有522个变量需要拟合。为了得到这个数字，我们可以看到在数据和第一个隐藏层之间有7*25+25=200个变量。如果我们继续这样加总，我们会得到200+260+33+4=497个变量。这比我们在逻辑回归模型中使用的九个变量要大得多。
- en: '[PRE48]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We''ll now declare our optimizer (using Adam optimization) and loop through
    our training iterations. We will use the L1 loss function (the absolute value).
    We''ll also initialize two lists in which we can store our `train` and `test_loss` functions.
    In every loop, we also want to randomly select a batch from the training data
    for fitting to the model and print the status every 25 generations, shown as follows:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将声明优化器（使用Adam优化算法），并循环执行训练迭代。我们将使用L1损失函数（绝对值）。我们还将初始化两个列表，用于存储我们的`train`和`test_loss`函数。在每次循环中，我们还希望随机选择一批训练数据进行模型拟合，并在每25代时打印状态，如下所示：
- en: '[PRE49]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The preceding step should result in the following output:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上一步应该会得到如下输出：
- en: '[PRE50]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The following is a snippet of code that plots the train and test loss with `matplotlib`:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是一个代码片段，它使用`matplotlib`绘制训练和测试损失：
- en: '[PRE51]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We proceed with the recipe by plotting the following diagram:'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过绘制以下图表继续进行该步骤：
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/AE276A4A.tmp](img/B16254_06_09.png)'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/AE276A4A.tmp](img/B16254_06_09.png)'
- en: 'Figure 6.5: In the preceding figure, we plot the train and test losses for
    our neural network that we trained to predict birth weight in grams. Notice that
    we have arrived at a good model after approximately 30 generations'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.5：在上图中，我们绘制了为预测出生体重（单位：克）而训练的神经网络的训练和测试损失。注意，大约经过30代后，我们已经得到一个很好的模型。
- en: 'Now, we need to output the train and test regression results and turn them
    into classification results by creating an indicator for if they are above or
    below 2,500 grams. To find out the model''s accuracy, we need to use the following
    code:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要输出训练和测试的回归结果，并通过创建一个指示器来将它们转化为分类结果，判断它们是否高于或低于2,500克。为了找出模型的准确性，我们需要使用以下代码：
- en: '[PRE52]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The preceding step should result in the following output:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上一步应该会得到如下输出：
- en: '[PRE53]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As you can see, both the train set accuracy and the test set accuracy are quite
    good and the models learn without under- or overfitting.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，训练集准确率和测试集准确率都相当不错，且模型没有出现欠拟合或过拟合的情况。
- en: How it works...
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we created a regression neural network with three fully connected
    hidden layers to predict the birth weight of the low birth weight dataset. In
    the next recipe, we will try to improve our logistic regression by making it a
    multiple-layer, logistic-type neural network.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本步骤中，我们创建了一个回归神经网络，具有三个完全连接的隐藏层，用于预测低出生体重数据集的出生体重。在下一个步骤中，我们将尝试通过将逻辑回归模型转变为多层神经网络来改进它。
- en: Improving the predictions of linear models
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进线性模型的预测
- en: In this recipe, we will attempt to improve our logistic model by increasing
    the accuracy of the low birth weight prediction. We will use a neural network.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本步骤中，我们将通过提高低出生体重预测的准确性来改进我们的逻辑回归模型。我们将使用神经网络。
- en: Getting ready
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: For this recipe, we will load the low birth weight data and use a neural network
    with two hidden fully connected layers with sigmoid activations to fit the probability
    of a low birth weight.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本步骤，我们将加载低出生体重数据，并使用具有两个隐藏层的神经网络，这些隐藏层采用sigmoid激活函数来拟合低出生体重的概率。
- en: How to do it...
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 怎么做...
- en: 'We proceed with the recipe as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按以下步骤继续进行：
- en: 'We start by loading the libraries and initializing our computational graph
    as follows:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载库并初始化我们的计算图，如下所示：
- en: '[PRE54]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, we load, extract, and normalize our data as in the preceding recipe,
    except that here we are going to be using the low birth weight indicator variable
    as our target instead of the actual birth weight, shown as follows:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载、提取并规范化数据，方法与前面的步骤相同，不同的是这里我们将使用低出生体重指标变量作为目标，而不是实际出生体重，如下所示：
- en: '[PRE55]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, we need to declare our batch size, our seed in order to have reproductible
    results, and our input data layer as follows:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要声明批量大小、种子（以确保结果可重复）以及输入数据层，如下所示：
- en: '[PRE56]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'As previously, we now need to declare functions that initialize a variable
    and a layer in our model. To create a better logistic function, we need to create
    a function that returns a logistic layer on an input layer. In other words, we
    will just use a fully connected layer and return a sigmoid element for each layer.
    It is important to remember that our loss function will have the final sigmoid
    included, so we want to specify on our last layer that we will not return the
    sigmoid of the output, shown as follows:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，我们现在需要声明初始化变量和模型中各层的函数。为了创建一个更好的逻辑函数，我们需要创建一个函数，它返回输入层上的逻辑层。换句话说，我们将使用一个完全连接的层，并为每一层返回一个sigmoid元素。需要记住的是，我们的损失函数将包含最终的sigmoid，因此我们希望在最后一层中指定不返回输出的sigmoid，如下所示：
- en: '[PRE57]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now we will declare three layers (two hidden layers and an output layer). We
    will start by initializing a weight and bias matrix for each layer and defining
    the layer operations as follows:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将声明三层（两层隐藏层和一层输出层）。我们将从为每一层初始化权重和偏置矩阵开始，并定义各层操作，如下所示：
- en: '[PRE58]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Next, we define a loss function (cross-entropy) and declare the optimization
    algorithm, as follows:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义损失函数（交叉熵）并声明优化算法，如下所示：
- en: '[PRE59]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Cross-entropy is a way of measuring distances between probabilities. Here, we
    want to measure the difference between certainty (0 or 1) and our model probability
    (0 < x < 1). TensorFlow implements cross-entropy with the built-in sigmoid function.
    This is also important as part of the hyperparameter tuning, as we are more likely
    to find the best loss function, learning rate, and optimization algorithm for
    the problem at hand. For brevity in this recipe, we do not include hyperparameter
    tuning.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 交叉熵是一种衡量概率之间距离的方法。在这里，我们希望衡量确定性（0或1）与我们模型的概率（0 < x < 1）之间的差异。TensorFlow通过内置的sigmoid函数实现交叉熵。这一点也很重要，因为它是超参数调优的一部分，我们更有可能找到适合当前问题的最佳损失函数、学习率和优化算法。为了简洁起见，本食谱中不包括超参数调优。
- en: 'In order to evaluate and compare our model to previous models, we need to create
    a prediction and accuracy operation on the graph. This will allow us to feed in
    the whole test set and determine the accuracy, as follows:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估和比较我们的模型与之前的模型，我们需要在图中创建一个预测和准确性操作。这将允许我们输入整个测试集并确定准确性，如下所示：
- en: '[PRE60]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We are now ready to start our training loop. We will train for 1,500 generations
    and save the model loss and train and test accuracies for plotting later. Our
    training loop is started with the following code:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备开始我们的训练循环。我们将训练1,500代，并保存模型损失以及训练和测试集的准确性，以便稍后绘制图表。我们的训练循环通过以下代码启动：
- en: '[PRE61]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The preceding step should result in the following output:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的步骤应产生以下输出：
- en: '[PRE62]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The following code blocks illustrate how to plot the cross-entropy loss and
    train and test set accuracies with `matplotlib`:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码块展示了如何使用`matplotlib`绘制交叉熵损失以及训练集和测试集的准确性：
- en: '[PRE63]'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We get the plot for cross-entropy loss per generation as follows:'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们获得了每代的交叉熵损失图，如下所示：
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/E3D23CF0.tmp](img/B16254_06_10.png)'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/E3D23CF0.tmp](img/B16254_06_10.png)'
- en: 'Figure 6.6: Training loss over 1,500 iterations'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：1,500次迭代的训练损失
- en: 'Within approximately 150 generations, we have reached a good model. As we continue
    to train, we can see that very little is gained over the remaining iterations,
    as shown in the following diagram:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在大约150代之后，我们已经达到了一个不错的模型。随着训练的继续，我们可以看到在剩余的迭代中几乎没有什么提升，如下图所示：
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/D849E07E.tmp](img/B16254_06_11.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/D849E07E.tmp](img/B16254_06_11.png)'
- en: 'Figure 6.7: Accuracy for the train set and test set'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：训练集和测试集的准确性
- en: As you can see in the preceding diagram, we arrived at a good model very quickly.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在前面的图中所见，我们很快得到了一个不错的模型。
- en: How it works...
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: When considering using neural networks to model data, you have to consider the
    advantages and disadvantages. While our model has converged faster than previous
    models, and perhaps with greater accuracy, this comes with a price; we are training
    many more model variables and there is a greater chance of overfitting. To check
    if overfitting is occurring, we look at the accuracy of the test and train sets.
    If the accuracy of the training set continues to increase while the accuracy on
    the test set stays the same or even decreases slightly, we can assume overfitting
    is occurring.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑使用神经网络建模数据时，必须权衡其优缺点。虽然我们的模型比之前的模型收敛得更快，也许准确率更高，但这也有代价；我们训练了更多的模型变量，过拟合的风险也更大。要检查是否发生了过拟合，我们需要查看训练集和测试集的准确率。如果训练集的准确率持续增加，而测试集的准确率保持不变或稍有下降，我们可以假设发生了过拟合。
- en: To combat underfitting, we can increase our model depth or train the model for
    more iterations. To address overfitting, we can add more data or add regularization
    techniques to our model.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对欠拟合，我们可以增加模型的深度或训练更多的迭代次数。为了应对过拟合，我们可以增加数据量或在模型中加入正则化技术。
- en: It is also important to note that our model variables are not as interpretable
    as a linear model. Neural network models have coefficients that are harder to
    interpret than linear models, as they explain the significance of features within
    the model.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，我们的模型变量不像线性模型那样具有可解释性。神经网络模型的系数比线性模型更难解释，因为它们用来解释模型中各特征的重要性。
- en: Learning to play Tic-Tac-Toe
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习玩井字棋
- en: To show how adaptable neural networks can be, we will now attempt to use a neural
    network in order to learn the optimal moves for Tic-Tac-Toe. We will approach
    this knowing that Tic-Tac-Toe is a deterministic game and that the optimal moves
    are already known.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示神经网络的适应性，我们现在尝试使用神经网络来学习井字棋的最优走法。在这一过程中，我们已知井字棋是一个确定性游戏，且最优走法已被确定。
- en: Getting ready
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备开始
- en: To train our model, we will use a list of board positions followed by the optimal
    response for a number of different boards. We can reduce the amount of boards
    to train on by considering only board positions that are different with regard
    to symmetry. The non-identity transformations of a Tic-Tac-Toe board are a rotation
    (in either direction) of 90 degrees, 180 degrees, and 270 degrees, a horizontal
    reflection, and a vertical reflection. Given this idea, we will use a shortlist
    of boards with the optimal move, apply two random transformations, and then feed
    that into our neural network for learning.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的模型，我们将使用一组棋盘位置，并在其后附上多个不同棋盘的最优回应。通过仅考虑在对称性方面不同的棋盘位置，我们可以减少训练的棋盘数量。井字棋棋盘的非恒等变换包括90度、180度和270度的旋转（无论方向）、水平反射和垂直反射。基于这一思想，我们将使用一份包含最优走法的棋盘简短清单，应用两个随机变换，然后将其输入到神经网络进行学习。
- en: Since Tic-Tac-Toe is a deterministic game, it is worth noting that whoever goes
    first should either win or draw. We will hope for a model that can respond to
    our moves optimally and ultimately result in a draw.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 由于井字棋是一个确定性游戏，值得注意的是，先手的一方应该要么获胜，要么与对方平局。我们希望模型能够对我们的走法做出最优回应，并最终导致平局。
- en: 'If we denote Xs using 1, Os using -1, and empty spaces using 0, then the following
    diagram illustrates how we can consider a board position and an optimal move as
    a row of data:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用1表示X，-1表示O，0表示空格，那么下图展示了如何将棋盘位置和最优走法视为数据行：
- en: '![](img/B16254_06_12.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16254_06_12.png)'
- en: 'Figure 6.8: Here, we illustrate how to consider a board and an optimal move
    as a row of data. Note that X = 1, O = -1, empty spaces are 0, and we start indexing
    at 0'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：这里，我们展示了如何将棋盘和最优走法视为数据行。请注意，X = 1，O = -1，空格为0，并且我们从0开始索引。
- en: In addition to the model loss, to check how our model is performing we will
    do two things. The first check we will perform is to remove a position and an
    optimal move row from our training set. This will allow us to see if the neural
    network model can generalize a move it hasn't seen before. The second way to evaluate
    our model is to actually play a game against it at the end.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型损失外，为了检查模型的表现，我们将执行两项操作。我们首先要做的检查是从训练集中移除一个位置和最优走法的数据行。这将帮助我们判断神经网络模型是否能够推广到它未见过的走法。评估模型的第二种方法是最终与它进行一局对弈。
- en: The list of possible boards and optimal moves can be found in the GitHub directory
    for this recipe at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook/tree/master/ch6/08_Learning_Tic_Tac_Toe](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook/tree/master/ch6/08_Lea) and
    in the Packt repository at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在GitHub的目录中找到此食谱的所有可能棋盘和最优走法列表，网址为[https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook/tree/master/ch6/08_Learning_Tic_Tac_Toe](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook/tree/master/ch6/08_Lea)，或者在Packt的仓库中找到[https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook)。
- en: How to do it...
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We proceed with the recipe as follows:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下步骤继续进行：
- en: 'We need to start by loading the necessary libraries for this script, as follows:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要首先加载此脚本所需的库，代码如下：
- en: '[PRE64]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, we declare the following batch size for training our model:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们声明用于训练模型的批次大小，代码如下：
- en: '[PRE65]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'To make visualizing the boards a bit easier, we will create a function that
    outputs Tic-Tac-Toe boards with Xs and Os. This is done with the following code:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使得可视化棋盘变得更加简单，我们将创建一个函数，输出带有X和O的井字棋棋盘。可以通过以下代码实现：
- en: '[PRE66]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now we have to create a function that will return a new board and an optimal
    response position under a transformation. This is done with the following code:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个函数，返回一个新棋盘和一个在变换下的最优响应位置。可以通过以下代码实现：
- en: '[PRE67]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The list of boards and their optimal responses is in a `.csv` file in the directory
    available in the GitHub repository at [https://github.com/nfmcclure/tensorflow_cookbook](https://github.com/nfmcclure/tensorflow_cookbook) or
    the Packt repository at [https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook-Second-Edition](https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook-Second-Edition).
    We will create a function that will load the file with the boards and responses
    and will store it as a list of tuples, as follows:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 棋盘列表及其最优响应存储在一个`.csv`文件中，该文件位于GitHub仓库中的目录，网址为[https://github.com/nfmcclure/tensorflow_cookbook](https://github.com/nfmcclure/tensorflow_cookbook)
    或 Packt 仓库中的[https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook-Second-Edition](https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook-Second-Edition)。我们将创建一个函数，加载包含棋盘和响应的文件，并将其存储为一个元组列表，代码如下：
- en: '[PRE68]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now we need to tie everything together to create a function that will return
    a randomly transformed board and response. This is done with the following code:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要将所有部分结合起来，创建一个函数，返回一个随机变换的棋盘和响应。可以通过以下代码实现：
- en: '[PRE69]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, we load our data and create a training set as follows:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载数据并创建一个训练集，代码如下：
- en: '[PRE70]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Remember that we want to remove one board and an optimal response from our
    training set to see if the model can generalize making the best move. The best
    move for the following board will be to play at index number 6:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请记住，我们要从训练集中删除一个棋盘和最优响应，看看模型是否能够泛化并做出最佳决策。以下棋盘的最佳走法是将棋子放在索引位置6：
- en: '[PRE71]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'We can now initialize the weights and bias and create our models:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以初始化权重和偏置，并创建我们的模型：
- en: '[PRE72]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Now, we create our model. Note that we do not include the `softmax()` activation
    function in the following model because it is included in the loss function:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们创建我们的模型。请注意，我们在以下模型中没有包含`softmax()`激活函数，因为它已包含在损失函数中：
- en: '[PRE73]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Next, we will declare our optimizer, as follows:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将声明优化器，代码如下：
- en: '[PRE74]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We can now loop through the training of our neural network with the following
    code. Note that our `loss` function will be the average softmax of the final output
    logits (unstandardized output):'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下代码进行神经网络的训练循环。请注意，我们的`loss`函数将是最终输出对数（未经标准化）的平均softmax：
- en: '[PRE75]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The following is the code needed to plot the loss over the model training:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是绘制模型训练过程中损失的代码：
- en: '[PRE76]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We should get the following plot for the loss per generation:'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们应该得到以下每代的损失曲线：
- en: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/2B562542.tmp](img/B16254_06_13.png)'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![/var/folders/6z/j6f33tds3v7dv8tryfqnxf5m0000gn/T/com.microsoft.Word/Content.MSO/2B562542.tmp](img/B16254_06_13.png)'
- en: 'Figure 6.9: A Tic-Tac-Toe train set loss over 10,000 iterations'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.9：井字棋训练集在10,000次迭代后的损失
- en: In the preceding diagram, we have plotted the loss over the training steps.
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们绘制了训练步骤中的损失曲线。
- en: 'To test the model, we need to see how it performs on the test board that we
    removed from the training set. We are hoping that the model can generalize and
    predict the optimal index for moving, which will be index number 6\. Most of the
    time the model will succeed, shown as follows:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试模型，我们需要查看它在从训练集移除的测试板上的表现。我们希望模型能够泛化并预测移动的最优索引，即索引编号6。大多数时候，模型会成功，如下所示：
- en: '[PRE77]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'The preceding step should result in the following output:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的步骤应该会得到以下输出：
- en: '[PRE78]'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'In order to evaluate our model, we need to play against our trained model.
    To do this, we have to create a function that will check for a win. This way,
    our program will know when to stop asking for more moves. This is done with the
    following code:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估我们的模型，我们需要与训练好的模型对战。为此，我们必须创建一个函数来检查是否获胜。这样，程序就能知道何时停止请求更多的移动。可以通过以下代码实现：
- en: '[PRE79]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Now we can loop through and play a game with our model. We start with a blank
    board (all zeros), we ask the user to input an index (0-8) of where to play, and
    we then feed that into the model for a prediction. For the model''s move, we take
    the largest available prediction that is also an open space. From this game, we
    can see that our model is not perfect, as follows:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以循环并与模型进行游戏。我们从一个空白的棋盘开始（全为零），然后让用户输入一个索引（0-8）表示要下的位置，然后将其输入模型进行预测。对于模型的移动，我们选择最大的可用预测，并且该位置必须是空的。从这场游戏中，我们可以看到我们的模型并不完美，如下所示：
- en: '[PRE80]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The preceding step should result in the following interactive output:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的步骤应该会得到以下交互式输出：
- en: '[PRE81]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: As you can see, a human player beats the machine very quickly and easily.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，人类玩家很快且轻松地战胜了机器。
- en: How it works...
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this section, we trained a neural network to play Tic-Tac-Toe by feeding
    in board positions and a nine-dimensional vector, and predicted the optimal response.
    We only had to feed in a few possible Tic-Tac-Toe boards and apply random transformations
    to each board to increase the training set size.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们训练了一个神经网络，通过输入棋盘位置和一个九维向量来玩井字游戏，并预测最优反应。我们只需要输入几个可能的井字游戏棋盘，并对每个棋盘应用随机变换来增加训练集的大小。
- en: 'To test our algorithm, we removed all instances of one specific board and saw
    whether our model could generalize to predict the optimal response. Finally, we
    played a sample game against our model. This model isn''t perfect yet. Using more
    data or applying a more complex neural network architecture could be done to improve
    it. But the better thing to do is to change the type of learning: instead of using
    supervised learning, we''re better off using a reinforcement learning-based approach.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的算法，我们移除了一个特定棋盘的所有实例，并查看我们的模型是否能够泛化并预测最优反应。最后，我们与模型进行了一场样本游戏。这个模型还不完美。我们可以通过使用更多数据或应用更复杂的神经网络架构来改进它。但更好的做法是改变学习的类型：与其使用监督学习，我们更应该使用基于强化学习的方法。
