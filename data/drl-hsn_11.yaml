- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Policy Gradients
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦
- en: In this first chapter of Part 3 of the book, we will consider an alternative
    way to handle Markov decision process (MDP) problems, which form a full family
    of methods called policy gradient methods. In some situations, these methods work
    better than value-based methods, so it is really important to be familiar with
    them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¹¦ç¬¬ä¸‰éƒ¨åˆ†çš„ç¬¬ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†è€ƒè™‘ä¸€ç§å¤„ç†é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰é—®é¢˜çš„æ›¿ä»£æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å½¢æˆäº†ä¸€ä¸ªå®Œæ•´çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ç³»åˆ—ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™äº›æ–¹æ³•æ¯”åŸºäºå€¼çš„æ–¹æ³•æ•ˆæœæ›´å¥½ï¼Œå› æ­¤ç†Ÿæ‚‰å®ƒä»¬éå¸¸é‡è¦ã€‚
- en: 'In this chapter, we will:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†ï¼š
- en: Cover an overview of the methods, their motivations, and their strengths and
    weaknesses in comparison to the already familiar Q-learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¦‚è¿°è¿™äº›æ–¹æ³•ã€å®ƒä»¬çš„åŠ¨æœºï¼Œä»¥åŠä¸æˆ‘ä»¬å·²çŸ¥çš„Qå­¦ä¹ æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒä»¬çš„ä¼˜ç¼ºç‚¹ã€‚
- en: Start with a simple policy gradient method called REINFORCE and try to apply
    it to our CartPole environment, comparing it with the deep Q-network (DQN) approach
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä»ä¸€ä¸ªç®€å•çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•â€”â€”REINFORCEå¼€å§‹ï¼Œå°è¯•å°†å…¶åº”ç”¨åˆ°æˆ‘ä»¬çš„CartPoleç¯å¢ƒä¸­ï¼Œå¹¶ä¸æ·±åº¦Qç½‘ç»œï¼ˆDQNï¼‰æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚
- en: Discuss problems with the vanilla REINFORCE method and ways to address them
    with the Policy Gradient (PG) method, which is a step toward a much more advanced
    method, A3C, that weâ€™ll take a look at in the next chapter
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®¨è®ºåŸå§‹REINFORCEæ–¹æ³•çš„é—®é¢˜ä»¥åŠå¦‚ä½•é€šè¿‡ç­–ç•¥æ¢¯åº¦ï¼ˆPGï¼‰æ–¹æ³•æ¥è§£å†³è¿™äº›é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ç§å‘æ›´é«˜çº§æ–¹æ³•A3Cè¿ˆè¿›çš„æ­¥éª¤ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« è¯¦ç»†è®¨è®ºã€‚
- en: Values and policy
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å€¼ä¸ç­–ç•¥
- en: 'Before getting to the main subject of this chapter, policy gradients, letâ€™s
    refresh our minds with the common characteristics of the methods covered in Part
    2 of this book. The central topic in value iteration and Q-learning is the value
    of the state (V [s]) or value of the state and action (Q[s,a]). Value is defined
    as the discounted total reward that we can gather from this state or by issuing
    this particular action from the state. If we know this quantity, our decision
    on every step becomes simple and obvious: we just act greedily in terms of value,
    and that guarantees us a good total reward at the end of the episode. So, the
    values of states (in the case of the value iteration method) or state + action
    (in the case of Q-learning) stand between us and the best reward. To obtain these
    values, we have used the Bellman equation, which expresses the value in the current
    step via the value in the next step.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿›å…¥æœ¬ç« çš„ä¸»è¦å†…å®¹â€”â€”ç­–ç•¥æ¢¯åº¦ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹æœ¬ä¹¦ç¬¬äºŒéƒ¨åˆ†æ¶µç›–çš„å„ç§æ–¹æ³•çš„å…±åŒç‰¹å¾ã€‚å€¼è¿­ä»£å’ŒQå­¦ä¹ çš„æ ¸å¿ƒä¸»é¢˜æ˜¯çŠ¶æ€çš„å€¼ï¼ˆV[s]ï¼‰æˆ–çŠ¶æ€å’ŒåŠ¨ä½œçš„å€¼ï¼ˆQ[s,a]ï¼‰ã€‚å€¼è¢«å®šä¹‰ä¸ºæˆ‘ä»¬ä»è¿™ä¸ªçŠ¶æ€ä¸­è·å¾—çš„æŠ˜æ‰£æ€»å¥–åŠ±ï¼Œæˆ–è€…ä»è¿™ä¸ªçŠ¶æ€å‘å‡ºç‰¹å®šåŠ¨ä½œæ‰€è·å¾—çš„å¥–åŠ±ã€‚å¦‚æœæˆ‘ä»¬çŸ¥é“è¿™ä¸ªé‡ï¼Œæˆ‘ä»¬åœ¨æ¯ä¸€æ­¥çš„å†³ç­–å°±å˜å¾—ç®€å•ä¸”æ˜¾è€Œæ˜“è§ï¼šæˆ‘ä»¬åªéœ€è¦åœ¨å€¼çš„åŸºç¡€ä¸Šè´ªå©ªåœ°è¡ŒåŠ¨ï¼Œè¿™å°±èƒ½ç¡®ä¿æˆ‘ä»¬åœ¨æ•´ä¸ªå›åˆç»“æŸæ—¶è·å¾—ä¸€ä¸ªè¾ƒå¥½çš„æ€»å¥–åŠ±ã€‚å› æ­¤ï¼ŒçŠ¶æ€çš„å€¼ï¼ˆåœ¨å€¼è¿­ä»£æ–¹æ³•ä¸­ï¼‰æˆ–çŠ¶æ€+åŠ¨ä½œçš„å€¼ï¼ˆåœ¨Qå­¦ä¹ ä¸­ï¼‰åœ¨æˆ‘ä»¬ä¸æœ€ä½³å¥–åŠ±ä¹‹é—´æ¶èµ·äº†ä¸€åº§æ¡¥æ¢ã€‚ä¸ºäº†å¾—åˆ°è¿™äº›å€¼ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†è´å°”æ›¼æ–¹ç¨‹ï¼Œå®ƒé€šè¿‡ä¸‹ä¸€ä¸ªæ­¥éª¤çš„å€¼æ¥è¡¨ç¤ºå½“å‰æ­¥éª¤çš„å€¼ã€‚
- en: In ChapterÂ [1](ch005.xhtml#x1-190001), we defined the entity that tells us what
    to do in every state as the policy. As in Q-learning methods, when values are
    dictating to us how to behave, they are actually defining our policy. Formally,
    this can be written as Ï€(s) = arg max[a]Q(s,a), which means that the result of
    our policy Ï€, at every state s, is the action with the largest Q.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬[1](ch005.xhtml#x1-190001)ç« ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹å‘Šè¯‰æˆ‘ä»¬è¯¥åšä»€ä¹ˆçš„å®ä½“ä¸ºç­–ç•¥ã€‚æ­£å¦‚Qå­¦ä¹ æ–¹æ³•ä¸€æ ·ï¼Œå½“å€¼å‘Šè¯‰æˆ‘ä»¬å¦‚ä½•è¡ŒåŠ¨æ—¶ï¼Œå®ƒä»¬å®é™…ä¸Šæ˜¯åœ¨å®šä¹‰æˆ‘ä»¬çš„ç­–ç•¥ã€‚æ­£å¼æ¥è¯´ï¼Œè¿™å¯ä»¥å†™æˆÏ€(s)
    = arg max[a]Q(s,a)ï¼Œè¿™æ„å‘³ç€åœ¨æ¯ä¸ªçŠ¶æ€sä¸‹ï¼Œæˆ‘ä»¬çš„ç­–ç•¥Ï€çš„ç»“æœæ˜¯å…·æœ‰æœ€å¤§Qå€¼çš„åŠ¨ä½œã€‚
- en: This policy-values connection is obvious, so I havenâ€™t placed emphasis on the
    policy as a separate entity, and we have spent most of our time talking about
    values and how to approximate them correctly. Now itâ€™s time to focus on this connection
    and the policy itself.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥ä¸å€¼çš„å…³ç³»æ˜¯æ˜¾è€Œæ˜“è§çš„ï¼Œæ‰€ä»¥æˆ‘æ²¡æœ‰å°†ç­–ç•¥å•ç‹¬ä½œä¸ºä¸€ä¸ªå®ä½“è¿›è¡Œå¼ºè°ƒï¼Œæˆ‘ä»¬çš„å¤§éƒ¨åˆ†æ—¶é—´éƒ½åœ¨è®¨è®ºå€¼ä»¥åŠå¦‚ä½•æ­£ç¡®åœ°è¿‘ä¼¼å®ƒä»¬ã€‚ç°åœ¨æ˜¯æ—¶å€™å…³æ³¨è¿™ä¸ªå…³ç³»ä»¥åŠç­–ç•¥æœ¬èº«äº†ã€‚
- en: Why the policy?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆæ˜¯ç­–ç•¥ï¼Ÿ
- en: There are several reasons why the policy is an interesting topic to explore.
    First of all, the policy is what we are looking for when we are dealing with a
    reinforcement learning problem. When the agent obtains the observation and needs
    to make a decision about what to do next, it needs the policy, not the value of
    the state or particular action. We do care about the total reward, but at every
    state, we may have little interest in the exact value of the state.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ˜¯ä¸€ä¸ªå€¼å¾—æ·±å…¥æ¢è®¨çš„æœ‰è¶£è¯é¢˜ï¼ŒåŸå› æœ‰å¾ˆå¤šã€‚é¦–å…ˆï¼Œå½“æˆ‘ä»¬å¤„ç†å¼ºåŒ–å­¦ä¹ é—®é¢˜æ—¶ï¼Œç­–ç•¥æ­£æ˜¯æˆ‘ä»¬éœ€è¦å¯»æ‰¾çš„å†…å®¹ã€‚å½“æ™ºèƒ½ä½“è·å¾—è§‚å¯Ÿç»“æœå¹¶éœ€è¦å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨æ—¶ï¼Œå®ƒéœ€è¦çš„æ˜¯ç­–ç•¥ï¼Œè€Œä¸æ˜¯çŠ¶æ€æˆ–ç‰¹å®šåŠ¨ä½œçš„å€¼ã€‚æˆ‘ä»¬ç¡®å®å…³å¿ƒæ€»å¥–åŠ±ï¼Œä½†åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½å¯¹çŠ¶æ€çš„ç¡®åˆ‡å€¼å¹¶ä¸æ„Ÿå…´è¶£ã€‚
- en: 'Imagine this situation: youâ€™re walking in the jungle and you suddenly realize
    that there is a hungry tiger hiding in the bushes. You have several alternatives,
    such as running, hiding, or trying to throw your backpack at it, but asking, â€Whatâ€™s
    the exact value of the run action and is it larger than the value of the do nothing
    action?â€ is a bit silly. You donâ€™t care much about the value, because you need
    to make the decision on what to do quickly and thatâ€™s it. Our Q-learning approach
    tried to answer the policy question indirectly by approximating the values of
    the states and trying to choose the best alternative, but if we are not interested
    in values, why do extra work?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è±¡ä¸€ä¸‹è¿™ç§æƒ…å†µï¼šä½ æ­£åœ¨ä¸›æ—ä¸­èµ°è·¯ï¼Œçªç„¶æ„è¯†åˆ°æœ‰ä¸€åªé¥¥é¥¿çš„è€è™è—åœ¨çŒæœ¨ä¸›ä¸­ã€‚ä½ æœ‰å‡ ç§é€‰æ‹©ï¼Œæ¯”å¦‚è·‘æ­¥ã€èº²è—æˆ–è€…è¯•å›¾æŠŠèƒŒåŒ…æ‰”å‘å®ƒï¼Œä½†é—®â€œè·‘æ­¥è¿™ä¸ªåŠ¨ä½œçš„ç¡®åˆ‡å€¼æ˜¯å¤šå°‘ï¼Ÿå®ƒå¤§äºä»€ä¹ˆéƒ½ä¸åšçš„åŠ¨ä½œå€¼å—ï¼Ÿâ€æœ‰ç‚¹å‚»ã€‚ä½ å¹¶ä¸å¤ªå…³å¿ƒè¿™ä¸ªå€¼ï¼Œå› ä¸ºä½ éœ€è¦è¿…é€Ÿåšå‡ºå†³å®šï¼Œä»…æ­¤è€Œå·²ã€‚æˆ‘ä»¬çš„Qå­¦ä¹ æ–¹æ³•é€šè¿‡è¿‘ä¼¼çŠ¶æ€çš„å€¼å¹¶å°è¯•é€‰æ‹©æœ€ä½³å¤‡é€‰æ–¹æ¡ˆæ¥é—´æ¥å›ç­”ç­–ç•¥é—®é¢˜ï¼Œä½†å¦‚æœæˆ‘ä»¬å¯¹å€¼ä¸æ„Ÿå…´è¶£ï¼Œä¸ºä»€ä¹ˆè¦åšå¤šä½™çš„å·¥ä½œå‘¢ï¼Ÿ
- en: 'Another reason why policies may be preferred is related to situations when
    an environment has lots of actions or, in the extreme case, with continuous action
    space problems. To be able to decide on the best action to take with Q(s,a), we
    need to solve a small optimization problem, finding a, which maximizes Q(s,a).
    In the case of an Atari game with several discrete actions, this wasnâ€™t a problem:
    we just approximated the values of all actions and took the action with the largest
    Q. If our action is not a small discrete set but has a scalar value attached to
    it, such as a steering wheel angle or the speed at which we want to run from the
    tiger, this optimization problem becomes hard because Q is usually represented
    by a highly nonlinear neural network (NN), so finding the argument that maximizes
    the functionâ€™s values can be tricky. In such cases, itâ€™s much more feasible to
    avoid values and work with the policy directly.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥å¯èƒ½æ›´å—æ¬¢è¿çš„å¦ä¸€ä¸ªåŸå› æ˜¯å½“ç¯å¢ƒæœ‰å¾ˆå¤šåŠ¨ä½œï¼Œæˆ–è€…åœ¨æç«¯æƒ…å†µä¸‹ï¼Œå…·æœ‰è¿ç»­åŠ¨ä½œç©ºé—´çš„é—®é¢˜ã€‚ä¸ºäº†èƒ½å¤Ÿæ ¹æ®Q(s,a)å†³å®šæœ€ä½³åŠ¨ä½œï¼Œæˆ‘ä»¬éœ€è¦è§£å†³ä¸€ä¸ªå°çš„ä¼˜åŒ–é—®é¢˜ï¼Œå¯»æ‰¾èƒ½å¤Ÿæœ€å¤§åŒ–Q(s,a)çš„aã€‚åœ¨ä¸€ä¸ªå…·æœ‰å¤šä¸ªç¦»æ•£åŠ¨ä½œçš„Atariæ¸¸æˆä¸­ï¼Œè¿™ä¸æ˜¯é—®é¢˜ï¼šæˆ‘ä»¬åªæ˜¯è¿‘ä¼¼æ‰€æœ‰åŠ¨ä½œçš„å€¼ï¼Œå¹¶é€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œã€‚å¦‚æœæˆ‘ä»¬çš„åŠ¨ä½œä¸æ˜¯ä¸€ä¸ªå°çš„ç¦»æ•£é›†åˆï¼Œè€Œæ˜¯é™„æœ‰æ ‡é‡å€¼ï¼Œå¦‚æ–¹å‘ç›˜è§’åº¦æˆ–æˆ‘ä»¬å¸Œæœ›ä»è€è™é‚£é‡Œé€ƒè·‘çš„é€Ÿåº¦ï¼Œé‚£ä¹ˆè¿™ä¸ªä¼˜åŒ–é—®é¢˜å˜å¾—éå¸¸å›°éš¾ï¼Œå› ä¸ºQé€šå¸¸æ˜¯ç”±ä¸€ä¸ªé«˜åº¦éçº¿æ€§çš„ç¥ç»ç½‘ç»œï¼ˆNNï¼‰è¡¨ç¤ºçš„ï¼Œå› æ­¤æ‰¾åˆ°èƒ½æœ€å¤§åŒ–å‡½æ•°å€¼çš„å‚æ•°å¯èƒ½ä¼šå¾ˆæ£˜æ‰‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé¿å…ä½¿ç”¨å€¼å¹¶ç›´æ¥å¤„ç†ç­–ç•¥è¦å¯è¡Œå¾—å¤šã€‚
- en: An extra benefit of policy learning is an environment with stochasticity. As
    you saw in ChapterÂ [8](ch012.xhtml#x1-1240008), in a categorical DQN, our agent
    can benefit a lot from working with the distribution of Q-values, instead of expected
    mean values, as our network can more precisely capture the underlying probability
    distribution. As you will see in the next section, the policy is naturally represented
    as the probability of actions, which is a step in the same direction as the categorical
    DQN method.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥å­¦ä¹ çš„é¢å¤–å¥½å¤„ä¹‹ä¸€æ˜¯å…·æœ‰éšæœºæ€§çš„ç¯å¢ƒã€‚æ­£å¦‚ä½ åœ¨ç¬¬[8](ch012.xhtml#x1-1240008)ç« çœ‹åˆ°çš„ï¼Œåœ¨ä¸€ä¸ªåˆ†ç±»DQNä¸­ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“å¯ä»¥é€šè¿‡å¤„ç†Qå€¼çš„åˆ†å¸ƒè€ŒéæœŸæœ›å‡å€¼ï¼Œè·å¾—å¾ˆå¤§çš„æ”¶ç›Šï¼Œå› ä¸ºæˆ‘ä»¬çš„ç½‘ç»œèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°æ•æ‰åˆ°æ½œåœ¨çš„æ¦‚ç‡åˆ†å¸ƒã€‚æ­£å¦‚ä½ åœ¨ä¸‹ä¸€èŠ‚å°†çœ‹åˆ°çš„ï¼Œç­–ç•¥è‡ªç„¶è¡¨ç¤ºä¸ºè¡ŒåŠ¨çš„æ¦‚ç‡ï¼Œè¿™ä¸€æ­¥ä¸åˆ†ç±»DQNæ–¹æ³•çš„æ–¹å‘ä¸€è‡´ã€‚
- en: Policy representation
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç­–ç•¥è¡¨ç¤º
- en: 'Now that you know the benefits of the policy, letâ€™s give it a try. So, how
    do we represent the policy? In the case of Q-values, they were parametrized by
    the NN that returns values of actions as scalars. If we want our network to parametrize
    the actions, we have several options. The first and the simplest way could be
    just returning the identifier of the action (in the case of a discrete set of
    actions). However, this is not the best way to deal with a discrete set. A much
    more common solution, which is heavily used in classification tasks, is to return
    the probability distribution of our actions. In other words, for N mutually exclusive
    actions, we return N numbers representing the probability of taking each action
    in the given state (which we pass as an input to the network). This representation
    is shown in the following illustration:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å·²ç»äº†è§£äº†ç­–ç•¥çš„å¥½å¤„ï¼Œè®©æˆ‘ä»¬è¯•è¯•çœ‹ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•è¡¨ç¤ºç­–ç•¥å‘¢ï¼Ÿåœ¨Qå€¼çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬æ˜¯ç”±è¿”å›åŠ¨ä½œå€¼çš„æ ‡é‡çš„ç¥ç»ç½‘ç»œï¼ˆNNï¼‰å‚æ•°åŒ–çš„ã€‚å¦‚æœæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç½‘ç»œå¯¹åŠ¨ä½œè¿›è¡Œå‚æ•°åŒ–ï¼Œæˆ‘ä»¬æœ‰å‡ ç§é€‰æ‹©ã€‚æœ€ç®€å•çš„æ–¹å¼å¯èƒ½å°±æ˜¯ç›´æ¥è¿”å›åŠ¨ä½œçš„æ ‡è¯†ç¬¦ï¼ˆåœ¨ç¦»æ•£åŠ¨ä½œé›†åˆçš„æƒ…å†µä¸‹ï¼‰ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ˜¯å¤„ç†ç¦»æ•£é›†åˆçš„æœ€ä½³æ–¹å¼ã€‚ä¸€ä¸ªæ›´å¸¸è§çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­è¢«å¹¿æ³›ä½¿ç”¨ï¼Œæ˜¯è¿”å›æˆ‘ä»¬åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒã€‚æ¢å¥è¯è¯´ï¼Œå¯¹äºNä¸ªäº’æ–¥çš„åŠ¨ä½œï¼Œæˆ‘ä»¬è¿”å›Nä¸ªæ•°å­—ï¼Œè¡¨ç¤ºåœ¨ç»™å®šçŠ¶æ€ä¸‹é‡‡å–æ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼ˆæˆ‘ä»¬å°†çŠ¶æ€ä½œä¸ºè¾“å…¥ä¼ é€’ç»™ç½‘ç»œï¼‰ã€‚è¿™ç§è¡¨ç¤ºæ–¹æ³•åœ¨ä¸‹å›¾ä¸­å±•ç¤ºï¼š
- en: '![PIC](img/B22150_11_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_11_01.png)'
- en: 'FigureÂ 11.1: Policy approximation with an NN for a discrete set of actions'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾11.1ï¼šç¦»æ•£åŠ¨ä½œé›†çš„ç¥ç»ç½‘ç»œç­–ç•¥è¿‘ä¼¼
- en: 'Such a representation of actions as probabilities has the additional advantage
    of smooth representation: if we change our network weights a bit, the output of
    the network will also change slightly. In the case of a discrete numbers output,
    even a small adjustment of the weights can lead to a jump to a different action.
    However, if our output is the probability distribution, a small change of weights
    will usually lead to a small change in output distribution, such as slightly increasing
    the probability of one action versus the others. This is a very nice property
    to have, as gradient optimization methods are all about tweaking the parameters
    of a model a bit to improve the results.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å°†åŠ¨ä½œè¡¨ç¤ºä¸ºæ¦‚ç‡çš„è¿™ç§æ–¹å¼æœ‰ä¸€ä¸ªé¢å¤–çš„ä¼˜åŠ¿ï¼Œå°±æ˜¯å¹³æ»‘çš„è¡¨ç¤ºï¼šå¦‚æœæˆ‘ä»¬ç¨å¾®æ”¹å˜ç½‘ç»œçš„æƒé‡ï¼Œç½‘ç»œçš„è¾“å‡ºä¹Ÿä¼šå‘ç”Ÿè½»å¾®å˜åŒ–ã€‚åœ¨ç¦»æ•£æ•°å€¼è¾“å‡ºçš„æƒ…å†µä¸‹ï¼Œå³ä½¿æƒé‡åšäº†å°å¹…è°ƒæ•´ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´è·³è·ƒåˆ°ä¸åŒçš„åŠ¨ä½œã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬çš„è¾“å‡ºæ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œæƒé‡çš„å¾®å°å˜åŒ–é€šå¸¸ä¼šå¯¼è‡´è¾“å‡ºåˆ†å¸ƒçš„è½»å¾®å˜åŒ–ï¼Œä¾‹å¦‚ç¨å¾®å¢åŠ æŸä¸ªåŠ¨ä½œçš„æ¦‚ç‡ï¼Œè€Œå…¶ä»–åŠ¨ä½œçš„æ¦‚ç‡ç›¸åº”å‡å°‘ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„ç‰¹æ€§ï¼Œå› ä¸ºæ¢¯åº¦ä¼˜åŒ–æ–¹æ³•çš„æ ¸å¿ƒå°±æ˜¯ç¨å¾®è°ƒæ•´æ¨¡å‹çš„å‚æ•°æ¥æ”¹è¿›ç»“æœã€‚
- en: Policy gradients
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦
- en: 'We have decided on our policy representation, but what we havenâ€™t seen so far
    is how we are going to change our networkâ€™s parameters to improve the policy.
    If you remember ChapterÂ [4](ch008.xhtml#x1-740004), we solved a very similar problem
    using the cross-entropy method: our network took observations as inputs and returned
    the probability distribution of the actions. In fact, the cross-entropy method
    is a younger brother of the methods that we will discuss in this part of the book.
    To start, we will get acquainted with the method called REINFORCE, which differs
    only slightly from the cross-entropy method, but first, we need to look at some
    mathematical notation that we will use in this and the following chapters.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»å†³å®šäº†ç­–ç•¥çš„è¡¨ç¤ºæ–¹å¼ï¼Œä½†åˆ°ç›®å‰ä¸ºæ­¢æˆ‘ä»¬è¿˜æ²¡æœ‰çœ‹åˆ°çš„æ˜¯å¦‚ä½•æ”¹å˜ç½‘ç»œçš„å‚æ•°æ¥æ”¹å–„ç­–ç•¥ã€‚å¦‚æœä½ è¿˜è®°å¾—ç¬¬[4](ch008.xhtml#x1-740004)ç« ï¼Œæˆ‘ä»¬é€šè¿‡äº¤å‰ç†µæ–¹æ³•è§£å†³äº†ä¸€ä¸ªéå¸¸ç›¸ä¼¼çš„é—®é¢˜ï¼šæˆ‘ä»¬çš„ç½‘ç»œå°†è§‚å¯Ÿå€¼ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒã€‚å®é™…ä¸Šï¼Œäº¤å‰ç†µæ–¹æ³•æ˜¯æˆ‘ä»¬åœ¨æœ¬ä¹¦è¿™ä¸€éƒ¨åˆ†å°†è®¨è®ºçš„æ–¹æ³•çš„â€œå¼Ÿå¼Ÿâ€ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†äº†è§£ä¸€ç§å«åšREINFORCEçš„æ–¹æ³•ï¼Œå®ƒä¸äº¤å‰ç†µæ–¹æ³•ä»…æœ‰ç»†å¾®çš„åŒºåˆ«ï¼Œä½†åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦äº†è§£ä¸€äº›æˆ‘ä»¬å°†åœ¨æœ¬ç« åŠåç»­ç« èŠ‚ä¸­ä½¿ç”¨çš„æ•°å­¦ç¬¦å·ã€‚
- en: We define the policy gradient as âˆ‡J â‰ˆğ”¼[Q(s,a)âˆ‡log Ï€(a|s)]. Of course, there
    is strong proof of this, but itâ€™s not that important. What interests us much more
    is the meaning of this expression.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç­–ç•¥æ¢¯åº¦å®šä¹‰ä¸ºâˆ‡J â‰ˆğ”¼[Q(s,a)âˆ‡log Ï€(a|s)]ã€‚å½“ç„¶ï¼Œè¿™æœ‰å¼ºæœ‰åŠ›çš„è¯æ˜ï¼Œä½†è¿™å¹¶ä¸æ˜¯æœ€é‡è¦çš„ã€‚æˆ‘ä»¬æ›´æ„Ÿå…´è¶£çš„æ˜¯è¿™ä¸ªè¡¨è¾¾å¼çš„å«ä¹‰ã€‚
- en: The policy gradient defines the direction in which we need to change our networkâ€™s
    parameters to improve the policy in terms of the accumulated total reward. The
    scale of the gradient is proportional to the value of the action taken, which
    is Q(s,a) in the formula, and the gradient is equal to the gradient of the log
    probability of the action taken. This means that we are trying to increase the
    probability of actions that have given us good total rewards and decrease the
    probability of actions with bad final outcomes. Expectation, ğ”¼ in the formula,
    just means that we average the gradient of several steps that we have taken in
    the environment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ¢¯åº¦å®šä¹‰äº†æˆ‘ä»¬éœ€è¦æ”¹å˜ç½‘ç»œå‚æ•°çš„æ–¹å‘ï¼Œä»¥æ ¹æ®ç´¯ç§¯çš„æ€»å¥–åŠ±æ¥æ”¹è¿›ç­–ç•¥ã€‚æ¢¯åº¦çš„å¤§å°ä¸æ‰€é‡‡å–çš„åŠ¨ä½œçš„å€¼æˆæ­£æ¯”ï¼Œè¿™ä¸ªå€¼åœ¨å…¬å¼ä¸­è¡¨ç¤ºä¸ºQ(s,a)ï¼Œæ¢¯åº¦ç­‰äºæ‰€é‡‡å–åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡çš„æ¢¯åº¦ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬è¯•å›¾å¢åŠ é‚£äº›ç»™æˆ‘ä»¬å¸¦æ¥è¾ƒå¥½æ€»å¥–åŠ±çš„åŠ¨ä½œçš„æ¦‚ç‡ï¼Œå¹¶å‡å°‘é‚£äº›å¸¦æ¥è¾ƒå·®æœ€ç»ˆç»“æœçš„åŠ¨ä½œçš„æ¦‚ç‡ã€‚å…¬å¼ä¸­çš„æœŸæœ›ç¬¦å·ğ”¼åªæ˜¯è¡¨ç¤ºæˆ‘ä»¬å¯¹åœ¨ç¯å¢ƒä¸­é‡‡å–çš„å‡ ä¸ªæ­¥éª¤çš„æ¢¯åº¦è¿›è¡Œå¹³å‡ã€‚
- en: 'From a practical point of view, policy gradient methods could be implemented
    by performing optimization of this loss function: â„’ = âˆ’Q(s,a)log Ï€(a|s). The minus
    sign is important, as the loss function is minimized during stochastic gradient
    descent (SGD), but we want to maximize our policy gradient. You will see code
    examples of policy gradient methods later in this and the following chapters.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å®é™…åº”ç”¨çš„è§’åº¦æ¥çœ‹ï¼Œç­–ç•¥æ¢¯åº¦æ–¹æ³•å¯ä»¥é€šè¿‡ä¼˜åŒ–è¿™ä¸ªæŸå¤±å‡½æ•°æ¥å®ç°ï¼šâ„’ = âˆ’Q(s,a)log Ï€(a|s)ã€‚è´Ÿå·å¾ˆé‡è¦ï¼Œå› ä¸ºåœ¨éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰è¿‡ç¨‹ä¸­ï¼ŒæŸå¤±å‡½æ•°æ˜¯è¢«æœ€å°åŒ–çš„ï¼Œä½†æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–æˆ‘ä»¬çš„ç­–ç•¥æ¢¯åº¦ã€‚ä½ å°†åœ¨æœ¬ç« åŠåç»­ç« èŠ‚ä¸­çœ‹åˆ°ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ä»£ç ç¤ºä¾‹ã€‚
- en: The REINFORCE method
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: REINFORCEæ–¹æ³•
- en: The formula of policy gradient that you have just seen is used by most policy-based
    methods, but the details can vary. One very important point is how exactly gradient
    scales, Q(s,a), are calculated. In the cross-entropy method from ChapterÂ [4](ch008.xhtml#x1-740004),
    we played several episodes, calculated the total reward for each of them, and
    trained on transitions from episodes with a better-than-average reward. This training
    procedure is a policy gradient method with Q(s,a) = 1 for state and action pairs
    from good episodes (with a large total reward) and Q(s,a) = 0 for state and action
    pairs from worse episodes.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åˆšåˆšçœ‹åˆ°çš„ç­–ç•¥æ¢¯åº¦å…¬å¼æ˜¯å¤§å¤šæ•°åŸºäºç­–ç•¥çš„æ–¹æ³•æ‰€ä½¿ç”¨çš„ï¼Œä½†å…·ä½“ç»†èŠ‚å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚ä¸€ä¸ªéå¸¸é‡è¦çš„ç‚¹æ˜¯å¦‚ä½•ç²¾ç¡®åœ°è®¡ç®—æ¢¯åº¦å°ºåº¦ï¼ŒQ(s,a)ã€‚åœ¨ç¬¬[4](ch008.xhtml#x1-740004)ç« çš„äº¤å‰ç†µæ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬æ’­æ”¾äº†å‡ ä¸ªå›åˆï¼Œè®¡ç®—äº†æ¯ä¸ªå›åˆçš„æ€»å¥–åŠ±ï¼Œå¹¶è®­ç»ƒæ¥è‡ªå¥–åŠ±ä¼˜äºå¹³å‡æ°´å¹³çš„å›åˆçš„è½¬ç§»ã€‚è¿™ç§è®­ç»ƒè¿‡ç¨‹æ˜¯ä¸€ä¸ªç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œå¯¹äºæ¥è‡ªå¥½å›åˆï¼ˆæœ‰è¾ƒå¤§æ€»å¥–åŠ±ï¼‰çš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼ŒQ(s,a)
    = 1ï¼Œè€Œå¯¹äºæ¥è‡ªå·®å›åˆçš„çŠ¶æ€-åŠ¨ä½œå¯¹ï¼ŒQ(s,a) = 0ã€‚
- en: 'The cross-entropy method worked even with those simple assumptions, but the
    obvious improvement will be to use Q(s,a) for training instead of just 0 and 1\.
    Why should it help? The answer is a more fine-grained separation of episodes.
    For example, transitions from the episode with a total reward of 10 should contribute
    to the gradient more than transitions from the episode with the reward of 1\.
    Another reason to use Q(s,a) instead of just 0 or 1 constants is to increase the
    probabilities of good actions at the beginning of the episode and decrease the
    probability of actions closer to the end of the episode. In the cross-entropy
    method, we take â€eliteâ€ episodes and train on their actions regardless of the
    actionsâ€™ offset in the episode. By using Q(s,a) (which includes discount factor
    Î³), we put more emphasis on good actions in the beginning of the episode than
    on the actions at the end of the episode. Thatâ€™s exactly the idea of the method
    called REINFORCE. Its steps are as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: äº¤å‰ç†µæ–¹æ³•å³ä½¿åœ¨é‚£äº›ç®€å•çš„å‡è®¾ä¸‹ä¹Ÿèƒ½èµ·ä½œç”¨ï¼Œä½†æ˜¾è€Œæ˜“è§çš„æ”¹è¿›æ˜¯ç”¨Q(s,a)æ¥è¿›è¡Œè®­ç»ƒï¼Œè€Œä¸ä»…ä»…æ˜¯0å’Œ1ã€‚ä¸ºä»€ä¹ˆè¿™æ ·ä¼šæœ‰å¸®åŠ©ï¼Ÿç­”æ¡ˆæ˜¯å¯ä»¥æ›´ç²¾ç»†åœ°åŒºåˆ†å›åˆã€‚ä¾‹å¦‚ï¼Œæ¥è‡ªæ€»å¥–åŠ±ä¸º10çš„å›åˆçš„è½¬ç§»åº”è¯¥æ¯”æ¥è‡ªå¥–åŠ±ä¸º1çš„å›åˆçš„è½¬ç§»å¯¹æ¢¯åº¦è´¡çŒ®æ›´å¤§ã€‚å¦ä¸€ä¸ªä½¿ç”¨Q(s,a)è€Œä¸ä»…ä»…æ˜¯0æˆ–1å¸¸æ•°çš„åŸå› æ˜¯ï¼Œåœ¨å›åˆçš„å¼€å§‹å¢åŠ å¥½åŠ¨ä½œçš„æ¦‚ç‡ï¼Œå¹¶å‡å°‘æ¥è¿‘å›åˆç»“æŸæ—¶çš„åŠ¨ä½œæ¦‚ç‡ã€‚åœ¨äº¤å‰ç†µæ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬é€‰å–â€œç²¾è‹±â€å›åˆï¼Œå¹¶è®­ç»ƒå…¶åŠ¨ä½œï¼Œè€Œä¸ç®¡åŠ¨ä½œåœ¨å›åˆä¸­çš„åç§»é‡ã€‚é€šè¿‡ä½¿ç”¨Q(s,a)ï¼ˆåŒ…æ‹¬æŠ˜æ‰£å› å­Î³ï¼‰ï¼Œæˆ‘ä»¬åœ¨å›åˆå¼€å§‹æ—¶å¯¹å¥½åŠ¨ä½œç»™äºˆæ›´å¤šé‡è§†ï¼Œè€Œä¸æ˜¯å›åˆç»“æŸæ—¶çš„åŠ¨ä½œã€‚è¿™æ­£æ˜¯REINFORCEæ–¹æ³•çš„æ€æƒ³ã€‚å…¶æ­¥éª¤å¦‚ä¸‹ï¼š
- en: Initialize the network with random weights.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨éšæœºæƒé‡åˆå§‹åŒ–ç½‘ç»œã€‚
- en: Play N full episodes, saving their (s,a,r,sâ€²) transitions.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ’­æ”¾Nä¸ªå®Œæ•´çš„å›åˆï¼Œä¿å­˜å®ƒä»¬çš„(s,a,r,sâ€²)è½¬ç§»ã€‚
- en: 'For every step, t, of every episode, k, calculate the discounted total reward
    for the subsequent steps:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹æ¯ä¸ªå›åˆkçš„æ¯ä¸€æ­¥tï¼Œè®¡ç®—éšåçš„æ­¥éª¤çš„æŠ˜æ‰£æ€»å¥–åŠ±ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq40.png)'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq40.png)'
- en: 'Calculate the loss function for all transitions:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—æ‰€æœ‰è½¬ç§»çš„æŸå¤±å‡½æ•°ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq41.png)'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq41.png)'
- en: Perform an SGD update of weights, minimizing the loss.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ‰§è¡ŒSGDæ›´æ–°æƒé‡ï¼Œæœ€å°åŒ–æŸå¤±ã€‚
- en: Repeat from step 2 until convergence is achieved.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä»ç¬¬2æ­¥å¼€å§‹é‡å¤ï¼Œç›´åˆ°æ”¶æ•›ä¸ºæ­¢ã€‚
- en: 'This algorithm is different from Q-learning in several important ways:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ç®—æ³•ä¸Qå­¦ä¹ æœ‰å‡ ä¸ªé‡è¦çš„ä¸åŒä¹‹å¤„ï¼š
- en: 'No explicit exploration is needed: In Q-learning, we used an epsilon-greedy
    strategy to explore the environment and prevent our agent from getting stuck with
    a non-optimal policy. Now, with the probabilities returned by the network, the
    exploration is performed automatically. At the beginning, the network is initialized
    with random weights, and it returns a uniform probability distribution. This distribution
    corresponds to random agent behavior.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸éœ€è¦æ˜¾å¼æ¢ç´¢ï¼šåœ¨Qå­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†Îµ-greedyç­–ç•¥æ¥æ¢ç´¢ç¯å¢ƒï¼Œå¹¶é˜²æ­¢æˆ‘ä»¬çš„æ™ºèƒ½ä½“é™·å…¥ä¸€ä¸ªéæœ€ä¼˜ç­–ç•¥ä¸­ã€‚ç°åœ¨ï¼Œä½¿ç”¨ç½‘ç»œè¿”å›çš„æ¦‚ç‡ï¼Œæ¢ç´¢è¿‡ç¨‹ä¼šè‡ªåŠ¨è¿›è¡Œã€‚æœ€å¼€å§‹ï¼Œç½‘ç»œä»¥éšæœºæƒé‡åˆå§‹åŒ–ï¼Œè¿”å›å‡åŒ€æ¦‚ç‡åˆ†å¸ƒã€‚è¿™ç§åˆ†å¸ƒå¯¹åº”äºæ™ºèƒ½ä½“çš„éšæœºè¡Œä¸ºã€‚
- en: 'No replay buffer is used: Policy gradient methods belong to the on-policy methods
    class, which means that we canâ€™t train on data obtained from the old policy. This
    is both good and bad. The good part is that such methods usually converge faster.
    The bad side is that they usually require much more interaction with the environment
    than off-policy methods such as DQN.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸ä½¿ç”¨å›æ”¾ç¼“å†²åŒºï¼šç­–ç•¥æ¢¯åº¦æ–¹æ³•å±äºåœ¨çº¿æ–¹æ³•ç±»ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨æ—§ç­–ç•¥è·å¾—çš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚è¿™æ—¢æœ‰å¥½çš„ä¸€é¢ï¼Œä¹Ÿæœ‰åçš„ä¸€é¢ã€‚å¥½çš„ä¸€é¢æ˜¯ï¼Œè¿™ç±»æ–¹æ³•é€šå¸¸æ”¶æ•›è¾ƒå¿«ã€‚åçš„ä¸€é¢æ˜¯ï¼Œå®ƒä»¬é€šå¸¸æ¯”ç¦»ç­–ç•¥æ–¹æ³•ï¼ˆå¦‚DQNï¼‰éœ€è¦æ›´å¤šçš„ç¯å¢ƒäº¤äº’ã€‚
- en: 'No target network is needed: Here, we use Q-values, but they are obtained from
    our experience in the environment. In DQN, we used the target network to break
    the correlation in Q-value approximation, but we are not approximating anymore.
    In the next chapter, you will see that the target network trick can still be useful
    in policy gradient methods.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸éœ€è¦ç›®æ ‡ç½‘ç»œï¼šåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ Q å€¼ï¼Œä½†å®ƒä»¬æ˜¯ä»æˆ‘ä»¬åœ¨ç¯å¢ƒä¸­çš„ç»éªŒä¸­è·å¾—çš„ã€‚åœ¨ DQN ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›®æ ‡ç½‘ç»œæ¥æ‰“ç ´ Q å€¼é€¼è¿‘ä¸­çš„ç›¸å…³æ€§ï¼Œä½†æˆ‘ä»¬ä¸å†è¿›è¡Œé€¼è¿‘äº†ã€‚åœ¨ä¸‹ä¸€ç« ï¼Œä½ ä¼šçœ‹åˆ°ç›®æ ‡ç½‘ç»œæŠ€å·§åœ¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­ä»ç„¶æ˜¯æœ‰ç”¨çš„ã€‚
- en: The CartPole example
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CartPole ç¤ºä¾‹
- en: To see the method in action, letâ€™s check the implementation of the REINFORCE
    method on the familiar CartPole environment. The full code of the example is in
    Chapter11/02_cartpole_reinforce.py.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†çœ‹åˆ°è¿™ä¸ªæ–¹æ³•çš„å®é™…æ•ˆæœï¼Œæˆ‘ä»¬æ¥æ£€æŸ¥åœ¨ç†Ÿæ‚‰çš„ CartPole ç¯å¢ƒä¸­å®ç° REINFORCE æ–¹æ³•çš„ä»£ç ã€‚è¯¥ç¤ºä¾‹çš„å®Œæ•´ä»£ç ä½äº Chapter11/02_cartpole_reinforce.pyã€‚
- en: 'In the beginning, we define hyperparameters (imports are omitted):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å¼€å§‹ï¼Œæˆ‘ä»¬å®šä¹‰äº†è¶…å‚æ•°ï¼ˆçœç•¥äº†å¯¼å…¥éƒ¨åˆ†ï¼‰ï¼š
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The EPISODES_TO_TRAIN value specifies how many complete episodes we will use
    for training.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: EPISODES_TO_TRAIN å€¼æŒ‡å®šäº†æˆ‘ä»¬å°†ç”¨äºè®­ç»ƒçš„å®Œæ•´å›åˆæ•°ã€‚
- en: 'The following network should also be familiar to you:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ç½‘ç»œä¹Ÿåº”è¯¥å¯¹ä½ æ¥è¯´å¾ˆç†Ÿæ‚‰ï¼š
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that despite the fact our network returns probabilities, we are not applying
    softmax nonlinearity to the output. The reason behind this is that we will use
    the PyTorch log_softmax function to calculate the logarithm of the softmax output
    at once. This method of calculation is much more numerically stable; however,
    we need to remember that output from the network is not probability, but raw scores
    (usually called logits).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå°½ç®¡æˆ‘ä»¬çš„ç½‘ç»œè¿”å›çš„æ˜¯æ¦‚ç‡ï¼Œä½†æˆ‘ä»¬å¹¶æ²¡æœ‰å¯¹è¾“å‡ºåº”ç”¨ softmax éçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚è¿™æ ·åšçš„åŸå› æ˜¯ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ PyTorch çš„ log_softmax
    å‡½æ•°ä¸€æ¬¡æ€§è®¡ç®— softmax è¾“å‡ºçš„å¯¹æ•°ã€‚è¿™æ ·è®¡ç®—æ–¹æ³•åœ¨æ•°å€¼ä¸Šæ›´ç¨³å®šï¼›ä½†æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦è®°ä½ï¼Œç½‘ç»œçš„è¾“å‡ºä¸æ˜¯æ¦‚ç‡ï¼Œè€Œæ˜¯åŸå§‹å¾—åˆ†ï¼ˆé€šå¸¸ç§°ä¸º logitsï¼‰ã€‚
- en: 'This next function is a bit tricky:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ä¸ªå‡½æ•°æœ‰ç‚¹æ£˜æ‰‹ï¼š
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: It accepts a list of rewards for the whole episode and needs to calculate the
    discounted total reward for every step. To do this efficiently, we calculate the
    reward from the end of the local reward list. Indeed, the last step of the episode
    will have a total reward equal to its local reward. The step before the last will
    have the total reward of r[tâˆ’1] + Î³ â‹…r[t] (if t is an index of the last step).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæ¥å—ä¸€ä¸ªåŒ…å«æ•´ä¸ªå›åˆå¥–åŠ±çš„åˆ—è¡¨ï¼Œå¹¶éœ€è¦ä¸ºæ¯ä¸€æ­¥è®¡ç®—æŠ˜æ‰£åçš„æ€»å¥–åŠ±ã€‚ä¸ºäº†é«˜æ•ˆåœ°åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä»å±€éƒ¨å¥–åŠ±åˆ—è¡¨çš„æœ«å°¾è®¡ç®—å¥–åŠ±ã€‚å®é™…ä¸Šï¼Œå›åˆçš„æœ€åä¸€æ­¥å°†å…·æœ‰ä¸å…¶å±€éƒ¨å¥–åŠ±ç›¸ç­‰çš„æ€»å¥–åŠ±ã€‚å€’æ•°ç¬¬äºŒæ­¥çš„æ€»å¥–åŠ±å°†æ˜¯
    r[tâˆ’1] + Î³ â‹…r[t]ï¼ˆå¦‚æœ t æ˜¯æœ€åä¸€æ­¥çš„ç´¢å¼•ï¼‰ã€‚
- en: Our sum_r variable contains the total reward for the previous steps, so to get
    the total reward for the current step, we need to multiply sum_r by Î³ and add
    the local reward from that step.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ sum_r å˜é‡åŒ…å«å‰ä¸€æ­¥çš„æ€»å¥–åŠ±ï¼Œå› æ­¤è¦è·å–å½“å‰æ­¥éª¤çš„æ€»å¥–åŠ±ï¼Œæˆ‘ä»¬éœ€è¦å°† sum_r ä¹˜ä»¥ Î³ å¹¶åŠ ä¸Šè¯¥æ­¥éª¤çš„å±€éƒ¨å¥–åŠ±ã€‚
- en: 'The preparation steps before the training loop should also be familiar to you:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯ä¹‹å‰çš„å‡†å¤‡æ­¥éª¤åº”è¯¥å¯¹ä½ æ¥è¯´ä¹Ÿå¾ˆç†Ÿæ‚‰ï¼š
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The only new element is the agent class from the PTAN library. Here, we are
    using ptan.agent.PolicyAgent, which needs to make a decision about actions for
    every observation. As our network now returns the policy as the probabilities
    of the actions, in order to select the action to take, we need to obtain the probabilities
    from the network and then perform random sampling from this probability distribution.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å”¯ä¸€çš„æ–°å…ƒç´ æ˜¯æ¥è‡ª PTAN åº“çš„ agent ç±»ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ ptan.agent.PolicyAgentï¼Œå®ƒéœ€è¦ä¸ºæ¯ä¸ªè§‚æµ‹åšå‡ºåŠ¨ä½œå†³ç­–ã€‚ç”±äºæˆ‘ä»¬çš„ç½‘ç»œç°åœ¨è¿”å›çš„æ˜¯åŠ¨ä½œçš„æ¦‚ç‡åˆ†å¸ƒï¼Œä¸ºäº†é€‰æ‹©è¦æ‰§è¡Œçš„åŠ¨ä½œï¼Œæˆ‘ä»¬éœ€è¦ä»ç½‘ç»œä¸­è·å–æ¦‚ç‡ï¼Œç„¶åä»è¯¥æ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡Œéšæœºé‡‡æ ·ã€‚
- en: When we worked with DQN, the output of the network was Q-values, so if one action
    had a value of 0.4 and another action had a value of 0.5, the second action was
    preferred 100% of the time. In the case of the probability distribution, if the
    first action has a probability of 0.4 and the second 0.5, our agent should take
    the first action with a 40% chance and the second with a 50% chance. Of course,
    our network can decide to take the second action 100% of the time, and in this
    case, it returns a probability of 0 for the first action and a probability of
    1 for the second action.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬ä½¿ç”¨ DQN æ—¶ï¼Œç½‘ç»œçš„è¾“å‡ºæ˜¯ Q å€¼ï¼Œå› æ­¤å¦‚æœæŸä¸ªåŠ¨ä½œçš„å€¼ä¸º 0.4ï¼Œå¦ä¸€ä¸ªåŠ¨ä½œçš„å€¼ä¸º 0.5ï¼Œé‚£ä¹ˆç¬¬äºŒä¸ªåŠ¨ä½œå°±ä¼šè¢« 100% çš„æ¦‚ç‡ä¼˜å…ˆé€‰æ‹©ã€‚åœ¨æ¦‚ç‡åˆ†å¸ƒçš„æƒ…å†µä¸‹ï¼Œå¦‚æœç¬¬ä¸€ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ä¸º
    0.4ï¼Œç¬¬äºŒä¸ªåŠ¨ä½œçš„æ¦‚ç‡ä¸º 0.5ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“åº”è¯¥ä»¥ 40% çš„æ¦‚ç‡é€‰æ‹©ç¬¬ä¸€ä¸ªåŠ¨ä½œï¼Œä»¥ 50% çš„æ¦‚ç‡é€‰æ‹©ç¬¬äºŒä¸ªåŠ¨ä½œã€‚å½“ç„¶ï¼Œæˆ‘ä»¬çš„ç½‘ç»œä¹Ÿå¯ä»¥å†³å®š 100%
    é€‰æ‹©ç¬¬äºŒä¸ªåŠ¨ä½œï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç¬¬ä¸€ä¸ªåŠ¨ä½œçš„æ¦‚ç‡ä¸º 0ï¼Œç¬¬äºŒä¸ªåŠ¨ä½œçš„æ¦‚ç‡ä¸º 1ã€‚
- en: This difference is important to understand, but the change in the implementation
    is not large. Our PolicyAgent internally calls the NumPy random.choice() function
    with probabilities from the network. The apply_softmax argument instructs it to
    convert the network output to probabilities by calling softmax first. The third
    argument, preprocessor, is a way to get around the fact that the CartPole environment
    in Gymnasium returns the observation as a float64 instead of the float32 required
    by PyTorch.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå·®å¼‚å¾ˆé‡è¦ï¼Œéœ€è¦ç†è§£ï¼Œä½†å®ç°ä¸Šçš„å˜åŒ–ä¸å¤§ã€‚æˆ‘ä»¬çš„PolicyAgentå†…éƒ¨è°ƒç”¨NumPyçš„random.choice()å‡½æ•°ï¼Œå¹¶ä½¿ç”¨ç½‘ç»œçš„æ¦‚ç‡ã€‚apply_softmaxå‚æ•°æŒ‡ç¤ºå®ƒé¦–å…ˆé€šè¿‡è°ƒç”¨softmaxå°†ç½‘ç»œè¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡ã€‚ç¬¬ä¸‰ä¸ªå‚æ•°preprocessoræ˜¯ä¸ºäº†ç»•è¿‡Gymnasiumä¸­çš„CartPoleç¯å¢ƒè¿”å›çš„è§‚å¯Ÿå€¼æ˜¯float64ç±»å‹ï¼Œè€ŒPyTorchéœ€è¦float32ç±»å‹çš„é—®é¢˜ã€‚
- en: 'Before we can start the training loop, we need several variables:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¼€å§‹è®­ç»ƒå¾ªç¯ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›å˜é‡ï¼š
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first two variables, total_rewards and done_episodes, are used for reporting
    and contain the total rewards for the episodes and the count of completed episodes.
    The next few variables are used to gather the training data. The cur_rewards list
    contains local rewards for the episode being currently played. As this episode
    reaches the end, we calculate the discounted total rewards from local rewards
    using the calc_qvals() function and append them to the batch_qvals list. The batch_states
    and batch_actions lists contain states and actions that we saw in the last training.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: å‰ä¸¤ä¸ªå˜é‡total_rewardså’Œdone_episodesç”¨äºæŠ¥å‘Šï¼ŒåŒ…å«å›åˆçš„æ€»å¥–åŠ±å’Œå·²å®Œæˆå›åˆçš„è®¡æ•°ã€‚æ¥ä¸‹æ¥çš„å‡ ä¸ªå˜é‡ç”¨äºæ”¶é›†è®­ç»ƒæ•°æ®ã€‚cur_rewardsåˆ—è¡¨åŒ…å«å½“å‰æ­£åœ¨è¿›è¡Œçš„å›åˆçš„å±€éƒ¨å¥–åŠ±ã€‚å½“è¯¥å›åˆç»“æŸæ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨calc_qvals()å‡½æ•°ä»å±€éƒ¨å¥–åŠ±è®¡ç®—æŠ˜æ‰£åçš„æ€»å¥–åŠ±ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°batch_qvalsåˆ—è¡¨ä¸­ã€‚batch_stateså’Œbatch_actionsåˆ—è¡¨åŒ…å«æˆ‘ä»¬åœ¨ä¸Šæ¬¡è®­ç»ƒä¸­çœ‹åˆ°çš„çŠ¶æ€å’ŒåŠ¨ä½œã€‚
- en: 'The following code snippet is the beginning of the training loop:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç ç‰‡æ®µæ˜¯è®­ç»ƒå¾ªç¯çš„å¼€å§‹ï¼š
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Every experience that we get from the experience source contains the state,
    action, local reward, and next state. If the end of the episode has been reached,
    the next state will be None. For non-terminal experience entries, we just save
    the state, action, and local reward in our lists. At the end of the episode, we
    convert the local rewards into Q-values and increment the episodes counter.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»ç»éªŒæºä¸­è·å¾—çš„æ¯ä¸ªç»éªŒåŒ…å«çŠ¶æ€ã€åŠ¨ä½œã€å±€éƒ¨å¥–åŠ±å’Œä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚å¦‚æœå›åˆå·²ç»“æŸï¼Œä¸‹ä¸€ä¸ªçŠ¶æ€å°†ä¸ºNoneã€‚å¯¹äºéç»ˆæ­¢çš„ç»éªŒæ¡ç›®ï¼Œæˆ‘ä»¬åªéœ€å°†çŠ¶æ€ã€åŠ¨ä½œå’Œå±€éƒ¨å¥–åŠ±ä¿å­˜åœ¨åˆ—è¡¨ä¸­ã€‚åœ¨å›åˆç»“æŸæ—¶ï¼Œæˆ‘ä»¬å°†å±€éƒ¨å¥–åŠ±è½¬æ¢ä¸ºQå€¼ï¼Œå¹¶å¢åŠ å›åˆè®¡æ•°å™¨ã€‚
- en: 'This part of the training loop is performed at the end of the episode and is
    responsible for reporting the current progress and writing metrics to TensorBoard:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå¾ªç¯çš„è¿™ä¸€éƒ¨åˆ†åœ¨å›åˆç»“æŸæ—¶æ‰§è¡Œï¼Œè´Ÿè´£æŠ¥å‘Šå½“å‰è¿›å±•å¹¶å°†æŒ‡æ ‡å†™å…¥TensorBoardï¼š
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When enough episodes have passed since the last training step, we can optimize
    the gathered examples. As a first step, we convert states, actions, and Q-values
    into the appropriate PyTorch form:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å½“è‡ªä¸Šæ¬¡è®­ç»ƒæ­¥éª¤ä»¥æ¥ç»è¿‡è¶³å¤Ÿçš„å›åˆæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä¼˜åŒ–æ”¶é›†åˆ°çš„ç¤ºä¾‹ã€‚ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†çŠ¶æ€ã€åŠ¨ä½œå’ŒQå€¼è½¬æ¢ä¸ºé€‚å½“çš„PyTorchæ ¼å¼ï¼š
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we calculate the loss from the steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬æ ¹æ®æ­¥éª¤è®¡ç®—æŸå¤±ï¼š
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we ask our network to calculate states into logits and calculate the logarithm
    and softmax of them. On the third line, we select log probabilities from the actions
    taken and scale them with Q-values. On the last line, we average those scaled
    values and do negation to obtain the loss to minimize. To reiterate, this minus
    sign is very important because our policy gradient needs to be maximized to improve
    the policy. As the optimizer in PyTorch minimizes the loss function, we need to
    negate the policy gradient.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¦æ±‚ç½‘ç»œå°†çŠ¶æ€è®¡ç®—ä¸ºlogitsï¼Œå¹¶è®¡ç®—å…¶å¯¹æ•°å’Œsoftmaxã€‚åœ¨ç¬¬ä¸‰è¡Œï¼Œæˆ‘ä»¬ä»æ‰€é‡‡å–çš„åŠ¨ä½œä¸­é€‰æ‹©å¯¹æ•°æ¦‚ç‡ï¼Œå¹¶ç”¨Qå€¼è¿›è¡Œç¼©æ”¾ã€‚åœ¨æœ€åä¸€è¡Œï¼Œæˆ‘ä»¬å¹³å‡è¿™äº›ç¼©æ”¾åçš„å€¼å¹¶å–è´Ÿæ•°ï¼Œä»¥è·å¾—éœ€è¦æœ€å°åŒ–çš„æŸå¤±ã€‚å†å¼ºè°ƒä¸€ä¸‹ï¼Œè¿™ä¸ªè´Ÿå·éå¸¸é‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬çš„ç­–ç•¥æ¢¯åº¦éœ€è¦æœ€å¤§åŒ–ï¼Œä»¥æ”¹å–„ç­–ç•¥ã€‚ç”±äºPyTorchä¸­çš„ä¼˜åŒ–å™¨æ˜¯æœ€å°åŒ–æŸå¤±å‡½æ•°çš„ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å–ç­–ç•¥æ¢¯åº¦çš„è´Ÿå€¼ã€‚
- en: 'The rest of the code is clear:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å‰©ä½™çš„ä»£ç å¾ˆæ¸…æ™°ï¼š
- en: '[PRE9]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we perform backpropagation to gather gradients in our variables and ask
    the optimizer to perform an SGD update. At the end of the training loop, we reset
    the episodes counter and clear our lists for fresh data to gather.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ‰§è¡Œåå‘ä¼ æ’­æ¥æ”¶é›†å˜é‡ä¸­çš„æ¢¯åº¦ï¼Œå¹¶è¦æ±‚ä¼˜åŒ–å™¨æ‰§è¡ŒSGDæ›´æ–°ã€‚åœ¨è®­ç»ƒå¾ªç¯ç»“æŸæ—¶ï¼Œæˆ‘ä»¬é‡ç½®å›åˆè®¡æ•°å™¨å¹¶æ¸…ç©ºåˆ—è¡¨ï¼Œä»¥ä¾¿æ”¶é›†æ–°çš„æ•°æ®ã€‚
- en: Results
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'For reference, Iâ€™ve implemented DQN in the CartPole environment with almost
    the same hyperparameters as our REINFORCE example. Youâ€™ll find it in Chapter11/01_cartpole_dqn.py.
    Neither example requires any command-line arguments, and they should converge
    in less than a minute:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºå‚è€ƒï¼Œæˆ‘åœ¨CartPoleç¯å¢ƒä¸­å®ç°äº†DQNï¼Œä½¿ç”¨çš„è¶…å‚æ•°å‡ ä¹ä¸æˆ‘ä»¬çš„REINFORCEç¤ºä¾‹ç›¸åŒã€‚ä½ å¯ä»¥åœ¨Chapter11/01_cartpole_dqn.pyä¸­æ‰¾åˆ°å®ƒã€‚ä¸¤ä¸ªç¤ºä¾‹éƒ½ä¸éœ€è¦ä»»ä½•å‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶ä¸”å®ƒä»¬åº”è¯¥åœ¨ä¸åˆ°ä¸€åˆ†é’Ÿçš„æ—¶é—´å†…æ”¶æ•›ï¼š
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The convergence dynamics for both DQN and REINFORCE are shown in the following
    charts. Your training dynamics may vary due to the randomness of the training.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: DQNå’ŒREINFORCEçš„æ”¶æ•›åŠ¨æ€å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ä½ çš„è®­ç»ƒåŠ¨æ€å¯èƒ½ä¼šå› è®­ç»ƒçš„éšæœºæ€§è€Œæœ‰æ‰€ä¸åŒã€‚
- en: '![PIC](img/B22150_11_02.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_11_02.png)'
- en: 'FigureÂ 11.2: Count of episodes played over time (left) and training steps (right)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾11.2ï¼šéšç€æ—¶é—´æ¨ç§»çš„å›åˆæ•°ï¼ˆå·¦ï¼‰å’Œè®­ç»ƒæ­¥éª¤ï¼ˆå³ï¼‰
- en: These two charts compare the count of episodes played over time and over the
    training steps.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤å¼ å›¾è¡¨æ¯”è¾ƒäº†éšæ—¶é—´æ¨ç§»å’Œè®­ç»ƒæ­¥éª¤çš„å›åˆæ•°ã€‚
- en: 'The next chart compares the smoothed reward for episodes played:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€å¼ å›¾è¡¨æ¯”è¾ƒäº†å¹³æ»‘çš„å›åˆå¥–åŠ±ï¼š
- en: '![PIC](img/B22150_11_03.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_11_03.png)'
- en: 'FigureÂ 11.3: Reward dynamics for two methods'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾11.3ï¼šä¸¤ç§æ–¹æ³•çš„å¥–åŠ±åŠ¨æ€
- en: As you can see, the methods converged almost identically (with REINFORCE being
    slightly faster), but then DQN had problems when the mean reward climbed above
    400 and had to start almost from scratch.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œæ–¹æ³•å‡ ä¹ä»¥ç›¸åŒçš„é€Ÿåº¦æ”¶æ•›ï¼ˆREINFORCEç•¥å¿«ï¼‰ï¼Œä½†å½“å¹³å‡å¥–åŠ±è¶…è¿‡400æ—¶ï¼ŒDQNå‡ºç°äº†é—®é¢˜ï¼Œå¿…é¡»å‡ ä¹ä»å¤´å¼€å§‹ã€‚
- en: If you remember from ChapterÂ [4](ch008.xhtml#x1-740004), the cross-entropy method
    required about 40 batches of 16 episodes each to solve the CartPole environment,
    which is 640 episodes in total. The REINFORCE method was able to do the same in
    fewer than 400 episodes, which is a nice improvement.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ è¿˜è®°å¾—ç¬¬[4](ch008.xhtml#x1-740004)ç« ï¼Œäº¤å‰ç†µæ–¹æ³•éœ€è¦å¤§çº¦40ä¸ªæ‰¹æ¬¡ï¼Œæ¯ä¸ªæ‰¹æ¬¡16ä¸ªå›åˆæ¥è§£å†³CartPoleç¯å¢ƒï¼Œæ€»å…±æ˜¯640ä¸ªå›åˆã€‚REINFORCEæ–¹æ³•èƒ½å¤Ÿåœ¨ä¸åˆ°400ä¸ªå›åˆå†…å®ŒæˆåŒæ ·çš„ä»»åŠ¡ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ”¹è¿›ã€‚
- en: Policy-based versus value-based methods
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸºäºç­–ç•¥ä¸åŸºäºä»·å€¼çš„æ–¹æ³•
- en: 'Letâ€™s now step back from the code that we have just seen and check the differences
    between these families of methods:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æš‚æ—¶å›é¡¾ä¸€ä¸‹æˆ‘ä»¬åˆšæ‰çœ‹åˆ°çš„ä»£ç ï¼Œå¹¶æ£€æŸ¥è¿™äº›æ–¹æ³•å®¶æ—ä¹‹é—´çš„å·®å¼‚ï¼š
- en: 'Policy methods directly optimize what we care about: our behavior. Value methods,
    such as DQN, do the same indirectly, learning the value first and providing us
    with the policy based on this value.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ–¹æ³•ç›´æ¥ä¼˜åŒ–æˆ‘ä»¬å…³å¿ƒçš„å†…å®¹ï¼šæˆ‘ä»¬çš„è¡Œä¸ºã€‚ä»·å€¼æ–¹æ³•ï¼Œå¦‚DQNï¼Œé€šè¿‡é—´æ¥çš„æ–¹å¼æ¥åšåŒæ ·çš„äº‹æƒ…ï¼Œå…ˆå­¦ä¹ ä»·å€¼ï¼Œç„¶åæ ¹æ®è¿™ä¸ªä»·å€¼æä¾›ç­–ç•¥ã€‚
- en: Policy methods are on-policy and require fresh samples from the environment.
    Value methods can benefit from old data, obtained from the old policy, human demonstration,
    and other sources.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ–¹æ³•æ˜¯åœ¨çº¿çš„ï¼Œéœ€è¦æ¥è‡ªç¯å¢ƒçš„æ–°æ ·æœ¬ã€‚ä»·å€¼æ–¹æ³•å¯ä»¥ä»æ—§çš„æ•°æ®ä¸­å—ç›Šï¼Œè¿™äº›æ•°æ®æ¥è‡ªäºæ—§çš„ç­–ç•¥ã€äººç±»ç¤ºèŒƒä»¥åŠå…¶ä»–æ¥æºã€‚
- en: Policy methods are usually less sample-efficient, which means they require more
    interaction with the environment. Value methods can benefit from large replay
    buffers. However, sample efficiency doesnâ€™t mean that value methods are more computationally
    efficient, and very often, itâ€™s the opposite.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç­–ç•¥æ–¹æ³•é€šå¸¸è¾ƒä¸ºä½æ•ˆï¼Œæ„å‘³ç€å®ƒä»¬éœ€è¦æ›´å¤šä¸ç¯å¢ƒçš„äº¤äº’ã€‚ä»·å€¼æ–¹æ³•å¯ä»¥ä»å¤§è§„æ¨¡çš„å›æ”¾ç¼“å†²åŒºä¸­å—ç›Šã€‚ç„¶è€Œï¼Œæ ·æœ¬æ•ˆç‡å¹¶ä¸æ„å‘³ç€ä»·å€¼æ–¹æ³•åœ¨è®¡ç®—æ•ˆç‡ä¸Šæ›´é«˜ï¼Œå®é™…ä¸Šï¼Œå¾€å¾€æ˜¯ç›¸åçš„ã€‚
- en: 'In the preceding example, during the training, we needed to access our NN only
    once, to get the probabilities of actions. In DQN, we need to process two batches
    of states: one for the current state and another for the next state in the Bellman
    update.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å‰é¢çš„ç¤ºä¾‹ä¸­ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦è®¿é—®ä¸€æ¬¡ç¥ç»ç½‘ç»œï¼Œä»¥è·å¾—è¡ŒåŠ¨çš„æ¦‚ç‡ã€‚åœ¨DQNä¸­ï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†ä¸¤æ‰¹çŠ¶æ€ï¼šä¸€æ‰¹æ˜¯å½“å‰çŠ¶æ€ï¼Œå¦ä¸€æ‰¹æ˜¯è´å°”æ›¼æ›´æ–°ä¸­çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚
- en: As you can see, there is no strong preference for one family or another. In
    some situations, policy methods will be the more natural choice, like in continuous
    control problems or cases when access to the environment is cheap and fast. However,
    there are many situations when value methods will shine, for example, the recent
    state-of-the-art results on Atari games achieved by DQN variants. Ideally, you
    should be familiar with both families and understand the strong and weak sides
    of both camps.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ‰€è§ï¼Œä¼¼ä¹æ²¡æœ‰å¼ºçƒˆçš„å€¾å‘åå‘æŸä¸ªæ–¹æ³•å®¶æ—ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç­–ç•¥æ–¹æ³•ä¼šæ˜¯æ›´è‡ªç„¶çš„é€‰æ‹©ï¼Œæ¯”å¦‚åœ¨è¿ç»­æ§åˆ¶é—®é¢˜ä¸­ï¼Œæˆ–è€…åœ¨ç¯å¢ƒè®¿é—®ä¾¿å®œä¸”å¿«é€Ÿçš„æƒ…å†µä¸‹ã€‚ç„¶è€Œï¼Œä¹Ÿæœ‰è®¸å¤šæƒ…å†µæ˜¯ä»·å€¼æ–¹æ³•ä¼šå¤§æ”¾å¼‚å½©ï¼Œæ¯”å¦‚æœ€è¿‘DQNå˜ä½“åœ¨Atariæ¸¸æˆä¸Šçš„æœ€æ–°æˆæœã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œä½ åº”è¯¥ç†Ÿæ‚‰è¿™ä¸¤ç§æ–¹æ³•å®¶æ—ï¼Œå¹¶ç†è§£å®ƒä»¬çš„ä¼˜ç¼ºç‚¹ã€‚
- en: In the next section, we will talk about the REINFORCE methodâ€™s limitations,
    ways to improve it, and how to apply a policy gradient method to our favorite
    Pong game.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®º REINFORCE æ–¹æ³•çš„å±€é™æ€§ã€æ”¹è¿›æ–¹æ³•ï¼Œä»¥åŠå¦‚ä½•å°†æ”¿ç­–æ¢¯åº¦æ–¹æ³•åº”ç”¨åˆ°æˆ‘ä»¬æœ€å–œæ¬¢çš„ Pong æ¸¸æˆä¸­ã€‚
- en: REINFORCE issues
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: REINFORCE é—®é¢˜
- en: In the previous section, we discussed the REINFORCE method, which is a natural
    extension of the cross-entropy method. Unfortunately, both REINFORCE and the cross-entropy
    method still suffer from several problems, which make both of them limited to
    simple environments.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å‰ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº† REINFORCE æ–¹æ³•ï¼Œå®ƒæ˜¯äº¤å‰ç†µæ–¹æ³•çš„è‡ªç„¶æ‰©å±•ã€‚ä¸å¹¸çš„æ˜¯ï¼ŒREINFORCE å’Œäº¤å‰ç†µæ–¹æ³•ä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œè¿™ä½¿å¾—å®ƒä»¬éƒ½ä»…é™äºç®€å•çš„ç¯å¢ƒä¸­ã€‚
- en: Full episodes are required
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éœ€è¦å®Œæ•´çš„å›åˆ
- en: 'First of all, we still need to wait for the full episode to complete before
    we can start training. Even worse, both REINFORCE and the cross-entropy method
    behave better with more episodes used for training (just because more episodes
    means more training data, which means more accurate policy gradients). This situation
    is fine for short episodes in the CartPole, when in the beginning, we can barely
    handle the bar for more than 10 steps; but in Pong, it is completely different:
    every episode can last for hundreds or even thousands of frames. Itâ€™s equally
    bad from the training perspective, as our training batch becomes very large, and
    from the sample efficiency perspective, as we need to communicate with the environment
    a lot just to perform a single training step.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦ç­‰å¾…å®Œæ•´çš„å›åˆç»“æŸæ‰èƒ½å¼€å§‹è®­ç»ƒã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼ŒREINFORCE å’Œäº¤å‰ç†µæ–¹æ³•åœ¨ä½¿ç”¨æ›´å¤šå›åˆè¿›è¡Œè®­ç»ƒæ—¶è¡¨ç°æ›´å¥½ï¼ˆåªæ˜¯å› ä¸ºæ›´å¤šçš„å›åˆæ„å‘³ç€æ›´å¤šçš„è®­ç»ƒæ•°æ®ï¼Œè¿™æ„å‘³ç€æ›´å‡†ç¡®çš„æ”¿ç­–æ¢¯åº¦ï¼‰ã€‚è¿™ç§æƒ…å†µåœ¨
    CartPole çš„çŸ­å›åˆä¸­æ˜¯å¯ä»¥æ¥å—çš„ï¼Œå› ä¸ºä¸€å¼€å§‹æˆ‘ä»¬å‡ ä¹æ— æ³•ä¿æŒæ†å­è¶…è¿‡ 10 æ­¥ï¼›ä½†åœ¨ Pong ä¸­ï¼Œæƒ…å†µå®Œå…¨ä¸åŒï¼šæ¯ä¸ªå›åˆå¯èƒ½æŒç»­æ•°ç™¾ç”šè‡³æ•°åƒå¸§ã€‚ä»è®­ç»ƒçš„è§’åº¦æ¥çœ‹ï¼Œè¿™åŒæ ·ç³Ÿç³•ï¼Œå› ä¸ºæˆ‘ä»¬çš„è®­ç»ƒæ‰¹æ¬¡å˜å¾—éå¸¸å¤§ï¼›ä»æ ·æœ¬æ•ˆç‡çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬éœ€è¦ä¸ç¯å¢ƒè¿›è¡Œå¤§é‡çš„äº¤äº’æ‰èƒ½æ‰§è¡Œä¸€æ¬¡è®­ç»ƒæ­¥éª¤ã€‚
- en: 'The purpose of the complete episode requirement is to get as accurate a Q-estimation
    as possible. When we talked about DQN, you saw that, in practice, itâ€™s fine to
    replace the exact value for a discounted reward with our estimation using the
    one-step Bellman equation: Q(s,a) = r[a] + Î³V (sâ€²). To estimate V (s), we used
    our own Q-estimation, but in the case of the policy gradient, we donâ€™t have V
    (s) or Q(s,a) anymore.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å®Œæ•´å›åˆè¦æ±‚çš„ç›®çš„æ˜¯å°½å¯èƒ½å‡†ç¡®åœ°è·å¾— Q ä¼°è®¡ã€‚å½“æˆ‘ä»¬è®¨è®º DQN æ—¶ï¼Œä½ ä¼šçœ‹åˆ°ï¼Œå®é™…ä¸Šï¼Œç”¨ä¸€é˜¶è´å°”æ›¼æ–¹ç¨‹æ›¿æ¢æŠ˜æ‰£å¥–åŠ±çš„ç²¾ç¡®å€¼ä¸æˆ‘ä»¬çš„ä¼°è®¡æ˜¯å¯ä»¥çš„ï¼šQ(s,a)
    = r[a] + Î³V (sâ€²)ã€‚ä¸ºäº†ä¼°è®¡ V(s)ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†è‡ªå·±çš„ Q ä¼°è®¡ï¼Œä½†åœ¨æ”¿ç­–æ¢¯åº¦çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸å†æœ‰ V(s) æˆ– Q(s,a)ã€‚
- en: 'To overcome this, two approaches exist:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå­˜åœ¨ä¸¤ç§æ–¹æ³•ï¼š
- en: We can ask our network to estimate V (s) and use this estimation to obtain Q.
    This approach will be discussed in the next chapter and is called the actor-critic
    method, which is the most popular method from the policy gradient family.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥è¦æ±‚æˆ‘ä»¬çš„ç½‘ç»œä¼°è®¡ V(s)ï¼Œå¹¶ä½¿ç”¨è¿™ä¸ªä¼°è®¡æ¥è·å¾— Qã€‚è¿™ç§æ–¹æ³•å°†åœ¨ä¸‹ä¸€ç« è®¨è®ºï¼Œå®ƒè¢«ç§°ä¸ºæ¼”å‘˜-è¯„è®ºå‘˜æ–¹æ³•ï¼Œæ˜¯æ”¿ç­–æ¢¯åº¦å®¶æ—ä¸­æœ€æµè¡Œçš„æ–¹æ³•ã€‚
- en: Alternatively, we can do the Bellman equation, unrolling N steps ahead, which
    will effectively exploit the fact that the value contribution decreases when gamma
    is less than 1\. Indeed, with Î³ = 0.9, the value coefficient at the 10th step
    will be 0.9^(10) â‰ˆ 0.35\. At step 50, this coefficient will be 0.9^(50) â‰ˆ 0.00515,
    which is a really small contribution to the total reward. With Î³ = 0.99, the required
    count of steps will become larger, but we can still do this.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¦å¤–ï¼Œæˆ‘ä»¬å¯ä»¥æ‰§è¡Œè´å°”æ›¼æ–¹ç¨‹ï¼Œå±•å¼€ N æ­¥ï¼Œè¿™å°†æœ‰æ•ˆåˆ©ç”¨å½“ Î³ å°äº 1 æ—¶ï¼Œä»·å€¼è´¡çŒ®é€’å‡çš„äº‹å®ã€‚äº‹å®ä¸Šï¼Œå½“ Î³ = 0.9 æ—¶ï¼Œç¬¬ 10 æ­¥çš„ä»·å€¼ç³»æ•°å°†æ˜¯
    0.9^(10) â‰ˆ 0.35ã€‚åœ¨ç¬¬ 50 æ­¥æ—¶ï¼Œè¿™ä¸ªç³»æ•°å°†æ˜¯ 0.9^(50) â‰ˆ 0.00515ï¼Œè¿™å¯¹æ€»å¥–åŠ±çš„è´¡çŒ®éå¸¸å°ã€‚å½“ Î³ = 0.99 æ—¶ï¼Œæ‰€éœ€çš„æ­¥æ•°ä¼šå˜å¤§ï¼Œä½†æˆ‘ä»¬ä»ç„¶å¯ä»¥è¿™æ ·åšã€‚
- en: High gradient variance
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é«˜æ¢¯åº¦æ–¹å·®
- en: In the policy gradient formula, âˆ‡J â‰ˆğ”¼[Q(s,a)âˆ‡log Ï€(a|s)], we have a gradient
    proportional to the discounted reward from the given state. However, the range
    of this reward is heavily environment-dependent. For example, in the CartPole
    environment, we get a reward of 1 for every timestamp that we are holding the
    pole vertically. If we can do this for five steps, we get a total (undiscounted)
    reward of 5\. If our agent is smart and can hold the pole for, say, 100 steps,
    the total reward will be 100\. The difference in value between those two scenarios
    is 20 times, which means that the scale between the gradients of unsuccessful
    samples will be 20 times lower than for more successful ones. Such a large difference
    can seriously affect our training dynamics, as one lucky episode will dominate
    in the final gradient.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç­–ç•¥æ¢¯åº¦å…¬å¼ä¸­ï¼Œâˆ‡J â‰ˆğ”¼[Q(s,a)âˆ‡log Ï€(a|s)]ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸ç»™å®šçŠ¶æ€ä¸‹çš„æŠ˜æ‰£å¥–åŠ±æˆæ­£æ¯”çš„æ¢¯åº¦ã€‚ç„¶è€Œï¼Œè¿™ä¸ªå¥–åŠ±çš„èŒƒå›´å—åˆ°ç¯å¢ƒçš„é«˜åº¦ä¾èµ–ã€‚ä¾‹å¦‚ï¼Œåœ¨CartPoleç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬æ¯å½“ä¿æŒæ†å­å‚ç›´æ—¶ï¼Œå°±ä¼šå¾—åˆ°1çš„å¥–åŠ±ã€‚å¦‚æœæˆ‘ä»¬èƒ½ä¿æŒäº”æ­¥ï¼Œæˆ‘ä»¬å°†è·å¾—æ€»çš„ï¼ˆæœªæŠ˜æ‰£çš„ï¼‰å¥–åŠ±ä¸º5ã€‚å¦‚æœæˆ‘ä»¬çš„æ™ºèƒ½ä½“éå¸¸èªæ˜ï¼Œèƒ½å¤Ÿä¿æŒæ†å­ï¼Œä¾‹å¦‚100æ­¥ï¼Œæ€»å¥–åŠ±å°†æ˜¯100ã€‚ä¸¤è€…ä¹‹é—´çš„ä»·å€¼å·®å¼‚æ˜¯20å€ï¼Œè¿™æ„å‘³ç€å¤±è´¥æ ·æœ¬çš„æ¢¯åº¦å°ºåº¦å°†æ¯”æˆåŠŸæ ·æœ¬ä½20å€ã€‚å¦‚æ­¤å·¨å¤§çš„å·®å¼‚ä¼šä¸¥é‡å½±å“æˆ‘ä»¬çš„è®­ç»ƒåŠ¨æ€ï¼Œå› ä¸ºä¸€æ¬¡å¹¸è¿çš„ç»å†ä¼šåœ¨æœ€ç»ˆæ¢¯åº¦ä¸­å ä¸»å¯¼åœ°ä½ã€‚
- en: 'In mathematical terms, the policy gradient has high variance, and we need to
    do something about this in complex environments; otherwise, the training process
    can become unstable. The usual approach to handling this is subtracting a value
    called the baseline from the Q. The possible choices for the baseline are as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ•°å­¦è§’åº¦æ¥çœ‹ï¼Œç­–ç•¥æ¢¯åº¦å…·æœ‰è¾ƒé«˜çš„æ–¹å·®ï¼Œåœ¨å¤æ‚ç¯å¢ƒä¸­æˆ‘ä»¬éœ€è¦é‡‡å–æªæ–½æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼›å¦åˆ™ï¼Œè®­ç»ƒè¿‡ç¨‹å¯èƒ½ä¼šå˜å¾—ä¸ç¨³å®šã€‚é€šå¸¸å¤„ç†è¿™ç§æƒ…å†µçš„æ–¹æ³•æ˜¯ä»Qä¸­å‡å»ä¸€ä¸ªç§°ä¸ºåŸºå‡†å€¼çš„å€¼ã€‚åŸºå‡†å€¼çš„å¯èƒ½é€‰æ‹©å¦‚ä¸‹ï¼š
- en: Some constant value, which is normally the mean of the discounted rewards
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¸¸æ•°å€¼ï¼Œé€šå¸¸æ˜¯æŠ˜æ‰£å¥–åŠ±çš„å¹³å‡å€¼
- en: The moving average of the discounted rewards
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æŠ˜æ‰£å¥–åŠ±çš„ç§»åŠ¨å¹³å‡å€¼
- en: The value of the state, V (s)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çŠ¶æ€çš„ä»·å€¼ï¼ŒV (s)
- en: To illustrate the baseline effect on the training, in Chapter11/03_cartpole_reinforce_baseline.py
    I implemented the second way of calculating the baseline (the average of rewards).
    The only difference with the version youâ€™ve already seen is in the calc_qvals()
    function. Iâ€™m not going to discuss the results here; you can experiment yourself.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜åŸºå‡†å€¼å¯¹è®­ç»ƒçš„å½±å“ï¼Œåœ¨Chapter11/03_cartpole_reinforce_baseline.pyä¸­ï¼Œæˆ‘å®ç°äº†ç¬¬äºŒç§è®¡ç®—åŸºå‡†å€¼çš„æ–¹æ³•ï¼ˆå¥–åŠ±çš„å¹³å‡å€¼ï¼‰ã€‚ä¸æ‚¨å·²ç»çœ‹åˆ°çš„ç‰ˆæœ¬å”¯ä¸€ä¸åŒçš„æ˜¯åœ¨calc_qvals()å‡½æ•°ä¸­ã€‚æˆ‘è¿™é‡Œä¸æ‰“ç®—è®¨è®ºç»“æœï¼›æ‚¨å¯ä»¥è‡ªå·±è¿›è¡Œå®éªŒã€‚
- en: Exploration problems
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¢ç´¢é—®é¢˜
- en: 'Even with the policy represented as the probability distribution, there is
    a high chance that the agent will converge to some locally optimal policy and
    stop exploring the environment. In DQN, we solved this using epsilon-greedy action
    selection: with the probability epsilon, the agent took a random action instead
    of the action dictated by the current policy. We can use the same approach, of
    course, but policy gradient methods allow us to follow a better path, called the
    entropy bonus.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿ç­–ç•¥è¡¨ç¤ºä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œæ™ºèƒ½ä½“ä»ç„¶æœ‰å¾ˆå¤§æ¦‚ç‡ä¼šæ”¶æ•›åˆ°æŸä¸ªå±€éƒ¨æœ€ä¼˜ç­–ç•¥ï¼Œå¹¶åœæ­¢æ¢ç´¢ç¯å¢ƒã€‚åœ¨DQNä¸­ï¼Œæˆ‘ä»¬é€šè¿‡epsilon-greedyè¡ŒåŠ¨é€‰æ‹©æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼šä»¥epsilonçš„æ¦‚ç‡ï¼Œæ™ºèƒ½ä½“é‡‡å–ä¸€ä¸ªéšæœºè¡ŒåŠ¨ï¼Œè€Œä¸æ˜¯ç”±å½“å‰ç­–ç•¥å†³å®šçš„è¡ŒåŠ¨ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ç›¸åŒçš„æ–¹æ³•ï¼Œä½†ç­–ç•¥æ¢¯åº¦æ–¹æ³•å…è®¸æˆ‘ä»¬èµ°ä¸€æ¡æ›´å¥½çš„è·¯å¾„ï¼Œç§°ä¸ºç†µå¥–åŠ±ã€‚
- en: In information theory, entropy is a measure of uncertainty in a system. Being
    applied to the agentâ€™s policy, entropy shows how uncertain the agent is about
    which action to take. In math notation, the entropy of the policy is defined as
    H(Ï€) = âˆ’âˆ‘ Ï€(a|s)log Ï€(a|s). The value of entropy is always greater than zero and
    has a single maximum when the policy is uniform; in other words, all actions have
    the same probability. Entropy becomes minimal when our policy has 1 for one action
    and 0 for all others, which means that the agent is absolutely sure what to do.
    To prevent our agent from being stuck in the local minimum, we subtract the entropy
    from the loss function, punishing the agent for being too certain about the action
    to take.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¿¡æ¯è®ºä¸­ï¼Œç†µæ˜¯è¡¡é‡ç³»ç»Ÿä¸ç¡®å®šæ€§çš„ä¸€ä¸ªæŒ‡æ ‡ã€‚åº”ç”¨åˆ°æ™ºèƒ½ä½“çš„ç­–ç•¥ä¸­ï¼Œç†µæ˜¾ç¤ºäº†æ™ºèƒ½ä½“åœ¨é€‰æ‹©è¡ŒåŠ¨æ—¶çš„ä¸ç¡®å®šæ€§ã€‚ç”¨æ•°å­¦ç¬¦å·è¡¨ç¤ºï¼Œç­–ç•¥çš„ç†µå®šä¹‰ä¸ºH(Ï€) = âˆ’âˆ‘
    Ï€(a|s)log Ï€(a|s)ã€‚ç†µçš„å€¼æ€»æ˜¯å¤§äºé›¶ï¼Œå½“ç­–ç•¥å‡åŒ€æ—¶ï¼Œç†µæœ‰ä¸€ä¸ªå•ä¸€çš„æœ€å¤§å€¼ï¼›æ¢å¥è¯è¯´ï¼Œæ‰€æœ‰åŠ¨ä½œçš„æ¦‚ç‡ç›¸åŒã€‚å½“æˆ‘ä»¬çš„ç­–ç•¥å¯¹äºæŸä¸€åŠ¨ä½œçš„æ¦‚ç‡ä¸º1ï¼Œå…¶ä»–åŠ¨ä½œçš„æ¦‚ç‡ä¸º0æ—¶ï¼Œç†µæœ€å°ï¼Œè¿™æ„å‘³ç€æ™ºèƒ½ä½“å®Œå…¨ç¡®å®šè¯¥åšä»€ä¹ˆã€‚ä¸ºäº†é˜²æ­¢æ™ºèƒ½ä½“é™·å…¥å±€éƒ¨æœ€å°å€¼ï¼Œæˆ‘ä»¬ä»æŸå¤±å‡½æ•°ä¸­å‡å»ç†µï¼Œæƒ©ç½šæ™ºèƒ½ä½“å¯¹é‡‡å–çš„è¡ŒåŠ¨è¿‡äºç¡®å®šã€‚
- en: High correlation of samples
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ ·æœ¬çš„é«˜åº¦ç›¸å…³æ€§
- en: 'As we discussed in ChapterÂ [6](#), training samples in a single episode are
    usually heavily correlated, which is bad for SGD training. In the case of DQN,
    we solved this issue by having a large replay buffer with a size from 100,000
    to several million observations. This solution is not applicable to the policy
    gradient family anymore because those methods belong to the on-policy class. The
    implication is simple: using old samples generated by the old policy, we will
    get policy gradients for that old policy, not for our current one.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨ç¬¬[6](#)ç« ä¸­è®¨è®ºçš„é‚£æ ·ï¼Œå•ä¸ªå›åˆä¸­çš„è®­ç»ƒæ ·æœ¬é€šå¸¸é«˜åº¦ç›¸å…³ï¼Œè¿™å¯¹äºSGDè®­ç»ƒæ¥è¯´æ˜¯ä¸å¥½çš„ä¸€ç‚¹ã€‚åœ¨DQNçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬é€šè¿‡æ‹¥æœ‰ä¸€ä¸ªå¤§å°ä»100,000åˆ°å‡ ç™¾ä¸‡ä¸ªè§‚å¯Ÿå€¼çš„å¤§å‹é‡æ”¾ç¼“å†²åŒºæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚è¿™ä¸ªè§£å†³æ–¹æ¡ˆå¯¹äºç­–ç•¥æ¢¯åº¦ç±»æ–¹æ³•å°±ä¸é€‚ç”¨äº†ï¼Œå› ä¸ºè¿™äº›æ–¹æ³•å±äºåœ¨ç­–ç•¥ç±»ã€‚å…¶å«ä¹‰å¾ˆç®€å•ï¼šä½¿ç”¨æ—§ç­–ç•¥ç”Ÿæˆçš„æ—§æ ·æœ¬ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°è¯¥æ—§ç­–ç•¥çš„ç­–ç•¥æ¢¯åº¦ï¼Œè€Œä¸æ˜¯å½“å‰ç­–ç•¥çš„ã€‚
- en: 'The obvious, but unfortunately wrong, solution would be to reduce the replay
    buffer size. It might work in some simple cases but, in general, we need fresh
    training data generated by our current policy. To solve this, parallel environments
    are normally used. The idea is simple: instead of communicating with one environment,
    we use several and use their transitions as training data.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¾è€Œæ˜“è§ï¼Œä½†ä¸å¹¸çš„æ˜¯é”™è¯¯çš„è§£å†³æ–¹æ¡ˆæ˜¯å‡å°‘é‡æ”¾ç¼“å†²åŒºçš„å¤§å°ã€‚è¿™åœ¨ä¸€äº›ç®€å•çš„æ¡ˆä¾‹ä¸­å¯èƒ½æœ‰æ•ˆï¼Œä½†ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬éœ€è¦ç”±å½“å‰ç­–ç•¥ç”Ÿæˆçš„æ–°é²œè®­ç»ƒæ•°æ®ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šå¸¸ä½¿ç”¨å¹¶è¡Œç¯å¢ƒã€‚å…¶æƒ³æ³•å¾ˆç®€å•ï¼šæˆ‘ä»¬ä¸ä¸ä¸€ä¸ªç¯å¢ƒè¿›è¡Œé€šä¿¡ï¼Œè€Œæ˜¯ä½¿ç”¨å¤šä¸ªç¯å¢ƒï¼Œå¹¶åˆ©ç”¨å®ƒä»¬çš„è¿‡æ¸¡ä½œä¸ºè®­ç»ƒæ•°æ®ã€‚
- en: Policy gradient methods on CartPole
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CartPoleä¸Šçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•
- en: Nowadays, almost nobody uses the vanilla policy gradient method, as the much
    more stable actor-critic method exists. However, I still want to show the policy
    gradient implementation, as it establishes very important concepts and metrics
    to check the policy gradient methodâ€™s performance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä»Šï¼Œå‡ ä¹æ²¡æœ‰äººå†ä½¿ç”¨åŸå§‹çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œå› ä¸ºæ›´ç¨³å®šçš„æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•å·²ç»å­˜åœ¨ã€‚ç„¶è€Œï¼Œæˆ‘ä»ç„¶æƒ³å±•ç¤ºç­–ç•¥æ¢¯åº¦çš„å®ç°ï¼Œå› ä¸ºå®ƒå»ºç«‹äº†éå¸¸é‡è¦çš„æ¦‚å¿µå’Œè¡¡é‡æ ‡å‡†ï¼Œç”¨äºæ£€æŸ¥ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„æ€§èƒ½ã€‚
- en: Implementation
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: We will start with a much simpler environment of CartPole, and in the next section,
    we will check its performance in our favorite Pong environment. The complete code
    for the following example is available in Chapter11/04_cartpole_pg.py.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä»ä¸€ä¸ªç®€å•å¾—å¤šçš„CartPoleç¯å¢ƒå¼€å§‹ï¼Œåœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†æ£€æŸ¥å…¶åœ¨æˆ‘ä»¬æœ€å–œæ¬¢çš„Pongç¯å¢ƒä¸­çš„è¡¨ç°ã€‚ä»¥ä¸‹ç¤ºä¾‹çš„å®Œæ•´ä»£ç å¯åœ¨Chapter11/04_cartpole_pg.pyä¸­æ‰¾åˆ°ã€‚
- en: 'Besides the already familiar hyperparameters, we have two new ones:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†å·²ç»ç†Ÿæ‚‰çš„è¶…å‚æ•°å¤–ï¼Œæˆ‘ä»¬è¿˜æœ‰ä¸¤ä¸ªæ–°çš„è¶…å‚æ•°ï¼š
- en: '[PRE11]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The ENTROPY_BETA value is the scale of the entropy bonus and the REWARD_STEPS
    value specifies how many steps ahead the Bellman equation is unrolled to estimate
    the discounted total reward of every transition.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ENTROPY_BETAå€¼æ˜¯ç†µå¥–åŠ±çš„è§„æ¨¡ï¼ŒREWARD_STEPSå€¼æŒ‡å®šäº†å±•å¼€Bellmanæ–¹ç¨‹çš„æ­¥æ•°ï¼Œç”¨ä»¥ä¼°ç®—æ¯ä¸ªè¿‡æ¸¡çš„æŠ˜æ‰£æ€»å¥–åŠ±ã€‚
- en: 'The following is the network architecture:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ç½‘ç»œæ¶æ„ï¼š
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This is exactly the same as in the previous examples for CartPole: a two-layer
    network with 128 neurons in the hidden layer. The preparation code is also the
    same as before, except the experience source is asked to unroll the Bellman equation
    for 10 steps.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ä¹‹å‰CartPoleç¤ºä¾‹ä¸­çš„å®Œå…¨ç›¸åŒï¼šä¸€ä¸ªéšè—å±‚æœ‰128ä¸ªç¥ç»å…ƒçš„ä¸¤å±‚ç½‘ç»œã€‚å‡†å¤‡ä»£ç ä¹Ÿä¸ä¹‹å‰ç›¸åŒï¼Œé™¤äº†ç»éªŒæºéœ€è¦å±•å¼€Bellmanæ–¹ç¨‹10æ­¥ã€‚
- en: 'The following is the part that differs from 04_cartpole_pg.py:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä¸04_cartpole_pg.pyä¸åŒçš„éƒ¨åˆ†ï¼š
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the training loop, we maintain the sum of the discounted reward for every
    transition and use it to calculate the baseline for the policy scale:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬ç»´æŠ¤æ¯ä¸ªè¿‡æ¸¡çš„æŠ˜æ‰£å¥–åŠ±ä¹‹å’Œï¼Œå¹¶ç”¨å®ƒæ¥è®¡ç®—ç­–ç•¥å°ºåº¦çš„åŸºå‡†ï¼š
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the loss calculation, we use the same code as before to calculate the policy
    loss (which is the negated policy gradient):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æŸå¤±è®¡ç®—ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„ä»£ç æ¥è®¡ç®—ç­–ç•¥æŸå¤±ï¼ˆå³è´Ÿçš„ç­–ç•¥æ¢¯åº¦ï¼‰ï¼š
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Then we add the entropy bonus to the loss by calculating the entropy of the
    batch and subtracting it from the loss. As entropy has a maximum for uniform probability
    distribution and we want to push the training toward this maximum, we need to
    subtract from the loss.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡è®¡ç®—æ‰¹æ¬¡çš„ç†µå¹¶ä»æŸå¤±ä¸­å‡å»å®ƒï¼Œæ¥å‘æŸå¤±ä¸­æ·»åŠ ç†µå¥–åŠ±ã€‚ç”±äºç†µå¯¹äºå‡åŒ€æ¦‚ç‡åˆ†å¸ƒæœ‰æœ€å¤§å€¼ï¼Œè€Œæˆ‘ä»¬å¸Œæœ›å°†è®­ç»ƒæ¨åŠ¨åˆ°è¿™ä¸ªæœ€å¤§å€¼ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä»æŸå¤±ä¸­å‡å»ç†µã€‚
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, we calculate the Kullback-Leibler (KL) divergence between the new policy
    and the old policy. KL divergence in information theory measures how one probability
    distribution diverges from another expected probability distribution, as we saw
    in ChapterÂ [4](ch008.xhtml#x1-740004). In our example, it is being used to compare
    the policy returned by the model before and after the optimization step:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—æ–°ç­–ç•¥ä¸æ—§ç­–ç•¥ä¹‹é—´çš„ Kullback-Leibler (KL) æ•£åº¦ã€‚KL æ•£åº¦æ˜¯ä¿¡æ¯è®ºä¸­çš„ä¸€ä¸ªæ¦‚å¿µï¼Œç”¨æ¥è¡¡é‡ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸å¦ä¸€ä¸ªé¢„æœŸæ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚ï¼Œå°±åƒæˆ‘ä»¬åœ¨ç¬¬[4](ch008.xhtml#x1-740004)ç« ä¸­æ‰€çœ‹åˆ°çš„é‚£æ ·ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå®ƒè¢«ç”¨æ¥æ¯”è¾ƒä¼˜åŒ–æ­¥éª¤å‰åæ¨¡å‹è¿”å›çš„ç­–ç•¥ï¼š
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: High spikes in KL are usually a bad sign since it means that our policy was
    pushed too far from the previous policy, which is a bad idea most of the time
    (as our NN is a very nonlinear function in a high-dimensional space, such large
    changes in the model weight could have a very strong influence on the policy).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: KL çš„é«˜å³°é€šå¸¸æ˜¯ä¸€ä¸ªä¸å¥½çš„ä¿¡å·ï¼Œå› ä¸ºè¿™æ„å‘³ç€æˆ‘ä»¬çš„ç­–ç•¥ä¸ä¹‹å‰çš„ç­–ç•¥ç›¸å·®å¤ªè¿œï¼Œè¿™åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹éƒ½æ˜¯ä¸å¥½çš„åšæ³•ï¼ˆå› ä¸ºæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ªé«˜ç»´ç©ºé—´ä¸­çš„éå¸¸éçº¿æ€§å‡½æ•°ï¼Œæ¨¡å‹æƒé‡çš„å¦‚æ­¤å¤§å˜åŒ–å¯èƒ½ä¼šå¯¹ç­–ç•¥äº§ç”Ÿéå¸¸å¼ºçš„å½±å“ï¼‰ã€‚
- en: Finally, we calculate the statistics about the gradients on this training step.
    Itâ€™s usually good practice to show the graph of the maximum and L2 norm (which
    is the length of the vector) of gradients to get an idea about the training dynamics.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è®¡ç®—è¿™ä¸€æ­¥è®­ç»ƒä¸­çš„æ¢¯åº¦ç»Ÿè®¡æ•°æ®ã€‚é€šå¸¸ï¼Œå±•ç¤ºæ¢¯åº¦çš„æœ€å¤§å€¼å’Œ L2 èŒƒæ•°ï¼ˆå³å‘é‡çš„é•¿åº¦ï¼‰å›¾è¡¨æ˜¯ä¸€ç§å¥½çš„å®è·µï¼Œè¿™å¯ä»¥å¸®åŠ©æˆ‘ä»¬äº†è§£è®­ç»ƒåŠ¨æ€ã€‚
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'At the end of the training loop, we dump all the values that we want to monitor
    in TensorBoard:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå¾ªç¯ç»“æŸæ—¶ï¼Œæˆ‘ä»¬å°†æ‰€æœ‰å¸Œæœ›åœ¨ TensorBoard ä¸­ç›‘è§†çš„å€¼è¿›è¡Œè½¬å‚¨ï¼š
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Results
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: 'In this example, we will plot a lot of charts in TensorBoard. Letâ€™s start with
    the familiar one: reward. As you can see in the following chart, the dynamics
    and performance are not very different from the REINFORCE method:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨ TensorBoard ä¸­ç»˜åˆ¶è®¸å¤šå›¾è¡¨ã€‚è®©æˆ‘ä»¬ä»ç†Ÿæ‚‰çš„å›¾è¡¨å¼€å§‹ï¼šå¥–åŠ±ã€‚å¦‚ä»¥ä¸‹å›¾æ‰€ç¤ºï¼ŒåŠ¨æ€å’Œè¡¨ç°ä¸ REINFORCE æ–¹æ³•æ²¡æœ‰å¤ªå¤§ä¸åŒï¼š
- en: '![PIC](img/B22150_11_04.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_11_04.png)'
- en: 'FigureÂ 11.4: The reward dynamics of the policy gradient method'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11.4ï¼šç­–ç•¥æ¢¯åº¦æ³•çš„å¥–åŠ±åŠ¨æ€
- en: 'The next two charts are related to our baseline and scales of policy gradients.
    We expect the baseline to converge to 1 + 0.99 + 0.99Â² + â€¦ + 0.99â¹, which is approximately
    9.56\. Scales of policy gradients should oscillate around zero. Thatâ€™s exactly
    what we can see in the following graph:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„ä¸¤ä¸ªå›¾è¡¨ä¸æˆ‘ä»¬çš„åŸºçº¿å’Œç­–ç•¥æ¢¯åº¦çš„å°ºåº¦ç›¸å…³ã€‚æˆ‘ä»¬é¢„è®¡åŸºçº¿å°†æ”¶æ•›åˆ° 1 + 0.99 + 0.99Â² + â€¦ + 0.99â¹ï¼Œå¤§çº¦ä¸º 9.56ã€‚ç­–ç•¥æ¢¯åº¦çš„å°ºåº¦åº”å›´ç»•é›¶æ³¢åŠ¨ã€‚è¿™æ­£æ˜¯æˆ‘ä»¬åœ¨ä¸‹å›¾ä¸­çœ‹åˆ°çš„ï¼š
- en: '![PIC](img/B22150_11_05.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_11_05.png)'
- en: 'FigureÂ 11.5: Baseline value (left) and batch scales (right)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11.5ï¼šåŸºçº¿å€¼ï¼ˆå·¦ï¼‰ä¸æ‰¹æ¬¡å°ºåº¦ï¼ˆå³ï¼‰
- en: 'The entropy is decreasing over time from 0.69 to 0.52 (FigureÂ [11.6](#x1-198005r6)).
    The starting value corresponds to the maximum entropy with two actions, which
    is approximately 0.69:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ç†µéšç€æ—¶é—´çš„æ¨ç§»ä» 0.69 é™åˆ° 0.52ï¼ˆå›¾[11.6](#x1-198005r6)ï¼‰ã€‚èµ·å§‹å€¼å¯¹åº”äºå…·æœ‰ä¸¤ä¸ªåŠ¨ä½œçš„æœ€å¤§ç†µï¼Œå¤§çº¦ä¸º 0.69ï¼š
- en: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq42.png) ![Ï€ (a |s) = P[At = a|St = s]
    ](img/eq43.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![Ï€ (a |s) = P[At = a|St = s] ](img/eq42.png) ![Ï€ (a |s) = P[At = a|St = s]
    ](img/eq43.png)'
- en: 'The fact that the entropy is decreasing during the training, as indicated by
    the following chart, shows that our policy is moving from uniform distribution
    to more deterministic actions:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ç†µåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å‡å°‘ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿™è¡¨æ˜æˆ‘ä»¬çš„ç­–ç•¥æ­£åœ¨ä»å‡åŒ€åˆ†å¸ƒè½¬å‘æ›´ç¡®å®šæ€§çš„åŠ¨ä½œï¼š
- en: '![PIC](img/B22150_11_06.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_11_06.png)'
- en: 'FigureÂ 11.6: Entropy during the training'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11.6ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„ç†µ
- en: The next group of plots (FigureÂ [11.7](#x1-198007r7) and FigureÂ [11.8](#x1-198008r8))
    is related to loss, which includes policy loss, entropy loss, and their sum. The
    entropy loss is scaled and is a mirrored version of the preceding entropy chart.
    The policy loss shows the mean scale and direction of the policy gradient computed
    on the batch. Here, we should check the relative size of both of them to prevent
    entropy loss from dominating too much.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€ç»„å›¾è¡¨ï¼ˆå›¾[11.7](#x1-198007r7)å’Œå›¾[11.8](#x1-198008r8)ï¼‰ä¸æŸå¤±ç›¸å…³ï¼ŒåŒ…æ‹¬ç­–ç•¥æŸå¤±ã€ç†µæŸå¤±åŠå…¶æ€»å’Œã€‚ç†µæŸå¤±ç»è¿‡ç¼©æ”¾ï¼Œæ˜¯å‰é¢ç†µå›¾çš„é•œåƒç‰ˆæœ¬ã€‚ç­–ç•¥æŸå¤±æ˜¾ç¤ºäº†åœ¨æ‰¹æ¬¡ä¸Šè®¡ç®—çš„ç­–ç•¥æ¢¯åº¦çš„å‡å€¼ã€å°ºåº¦å’Œæ–¹å‘ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åº”æ£€æŸ¥ä¸¤è€…çš„ç›¸å¯¹å¤§å°ï¼Œä»¥é˜²ç†µæŸå¤±è¿‡åº¦ä¸»å¯¼ã€‚
- en: '![PIC](img/B22150_11_07.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_11_07.png)'
- en: 'FigureÂ 11.7: Entropy loss (left) and policy loss (right)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11.7ï¼šç†µæŸå¤±ï¼ˆå·¦ï¼‰ä¸ç­–ç•¥æŸå¤±ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_11_08.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B22150_11_08.png)'
- en: 'FigureÂ 11.8: Total loss'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 11.8ï¼šæ€»æŸå¤±
- en: 'The final set of charts (FigureÂ [11.9](#x1-198010r9) and FigureÂ [11.10](#x1-198011r10))
    shows the gradientâ€™s L2 values, the maximum of L2, and KL. Our gradients look
    healthy during the whole training: they are not too large and not too small, and
    there are no huge spikes. The KL charts also look normal as there are some spikes,
    but they are not very large and donâ€™t exceed 10^(âˆ’3):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åä¸€ç»„å›¾è¡¨ï¼ˆå›¾[11.9](#x1-198010r9)å’Œå›¾[11.10](#x1-198011r10)ï¼‰æ˜¾ç¤ºäº†æ¢¯åº¦çš„L2å€¼ã€L2çš„æœ€å¤§å€¼å’ŒKLå€¼ã€‚æˆ‘ä»¬çš„æ¢¯åº¦åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çœ‹èµ·æ¥å¾ˆå¥åº·ï¼šå®ƒä»¬ä¸å¤ªå¤§ä¹Ÿä¸å¤ªå°ï¼Œæ²¡æœ‰å‡ºç°å·¨å¤§çš„æ³¢åŠ¨ã€‚KLå›¾è¡¨ä¹Ÿçœ‹èµ·æ¥æ­£å¸¸ï¼Œè™½ç„¶æœ‰ä¸€äº›æ³¢åŠ¨ï¼Œä½†å®ƒä»¬å¹¶ä¸å¤§ï¼Œå¹¶ä¸”æ²¡æœ‰è¶…è¿‡10^(-3)ï¼š
- en: '![PIC](img/B22150_11_09.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_11_09.png)'
- en: 'FigureÂ 11.9: Gradients L2 (left) and maximum value (right)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾11.9ï¼šæ¢¯åº¦L2ï¼ˆå·¦ï¼‰å’Œæœ€å¤§å€¼ï¼ˆå³ï¼‰
- en: '![PIC](img/B22150_11_10.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/B22150_11_10.png)'
- en: 'FigureÂ 11.10: KL divergence'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾11.10ï¼šKLæ•£åº¦
- en: Policy gradient methods on Pong
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pongä¸Šçš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•
- en: As weâ€™ve seen in the previous section, the vanilla policy gradient method works
    well on a simple CartPole environment, but it works surprisingly badly in more
    complicated environments.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨å‰ä¸€èŠ‚ä¸­çœ‹åˆ°çš„ï¼Œæ ‡å‡†çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•åœ¨ç®€å•çš„CartPoleç¯å¢ƒä¸­è¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨æ›´å¤æ‚çš„ç¯å¢ƒä¸­å´è¡¨ç°å¾—å‡ºå¥‡çš„å·®ã€‚
- en: For the relatively simple Atari game Pong, our DQN was able to completely solve
    it in 1 million frames and showed positive reward dynamics in just 100,000 frames,
    whereas the policy gradient method failed to converge. Due to the instability
    of policy gradient training, it became very hard to find good hyperparameters
    and was still very sensitive to initialization. This doesnâ€™t mean that the policy
    gradient method is bad, because, as you will see in the next chapter, just one
    tweak of the network architecture to get a better baseline in the gradients will
    turn the policy gradient method into one of the best methods (the asynchronous
    advantage actor-critic method). Of course, there is a good chance that my hyperparameters
    are completely wrong or the code has some hidden bugs, or there could be other
    unforeseen problems. Regardless, unsuccessful results still have value, at least
    as a demonstration of bad convergence dynamics.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç›¸å¯¹ç®€å•çš„Atariæ¸¸æˆã€ŠPongã€‹ï¼Œæˆ‘ä»¬çš„DQNèƒ½å¤Ÿåœ¨100ä¸‡å¸§å†…å®Œå…¨è§£å†³å®ƒï¼Œå¹¶ä¸”åœ¨ä»…ä»…10ä¸‡å¸§å†…å°±æ˜¾ç¤ºå‡ºäº†æ­£å‘å¥–åŠ±åŠ¨æ€ï¼Œè€Œç­–ç•¥æ¢¯åº¦æ–¹æ³•åˆ™æœªèƒ½æ”¶æ•›ã€‚ç”±äºç­–ç•¥æ¢¯åº¦è®­ç»ƒçš„ä¸ç¨³å®šæ€§ï¼Œå¾ˆéš¾æ‰¾åˆ°åˆé€‚çš„è¶…å‚æ•°ï¼Œå¹¶ä¸”å¯¹åˆå§‹åŒ–éå¸¸æ•æ„Ÿã€‚è¿™å¹¶ä¸æ„å‘³ç€ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸å¥½ï¼Œå› ä¸ºæ­£å¦‚ä½ åœ¨ä¸‹ä¸€ç« å°†çœ‹åˆ°çš„ï¼Œåªéœ€ç¨å¾®è°ƒæ•´ç½‘ç»œæ¶æ„ä»¥è·å¾—æ›´å¥½çš„åŸºçº¿æ¢¯åº¦ï¼Œç­–ç•¥æ¢¯åº¦æ–¹æ³•å°±ä¼šå˜æˆæœ€å¥½çš„æ–¹æ³•ä¹‹ä¸€ï¼ˆå¼‚æ­¥ä¼˜åŠ¿æ¼”å‘˜è¯„è®ºå‘˜æ–¹æ³•ï¼‰ã€‚å½“ç„¶ï¼Œä¹Ÿæœ‰å¾ˆå¤§çš„å¯èƒ½æ€§æˆ‘çš„è¶…å‚æ•°å®Œå…¨é”™è¯¯ï¼Œæˆ–è€…ä»£ç ä¸­å­˜åœ¨ä¸€äº›éšè—çš„BUGï¼Œæˆ–è€…å¯èƒ½æœ‰å…¶ä»–æœªé¢„è§çš„é—®é¢˜ã€‚æ— è®ºå¦‚ä½•ï¼Œå¤±è´¥çš„ç»“æœä»ç„¶æœ‰ä»·å€¼ï¼Œè‡³å°‘å®ƒèƒ½å±•ç¤ºä¸è‰¯æ”¶æ•›åŠ¨æ€ã€‚
- en: Implementation
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®ç°
- en: You can find the complete code for the example in Chapter11/05_pong_pg.py.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨Chapter11/05_pong_pg.pyä¸­æ‰¾åˆ°å®Œæ•´çš„ç¤ºä¾‹ä»£ç ã€‚
- en: 'The three main differences from the previous exampleâ€™s code are as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å‰ä¸€ä¸ªç¤ºä¾‹ä»£ç ç›¸æ¯”ï¼Œä¸»è¦æœ‰ä¸‰ä¸ªåŒºåˆ«ï¼š
- en: 'The baseline is estimated with a moving average for 1 million past transitions,
    instead of all examples. To make moving average calculations faster, a deque-backed
    buffer is created:'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åŸºçº¿æ˜¯é€šè¿‡å¯¹è¿‡å»100ä¸‡ä¸ªè¿‡æ¸¡è¿›è¡Œç§»åŠ¨å¹³å‡æ¥ä¼°ç®—çš„ï¼Œè€Œä¸æ˜¯å¯¹æ‰€æœ‰ç¤ºä¾‹è¿›è¡Œä¼°ç®—ã€‚ä¸ºäº†åŠ é€Ÿç§»åŠ¨å¹³å‡çš„è®¡ç®—ï¼Œåˆ›å»ºäº†ä¸€ä¸ªç”±dequeæ”¯æŒçš„ç¼“å†²åŒºï¼š
- en: '[PRE20]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Several concurrent environments are used. The second difference in this example
    is working with multiple environments, and this functionality is supported by
    the PTAN library. The only action we have to take is to pass the array of Env
    objects to the ExperienceSource class. All the rest is done automatically. In
    the case of several environments, the experience source asks them for transitions
    in round-robin fashion, providing us with less-correlated training samples.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨äº†å¤šä¸ªå¹¶å‘ç¯å¢ƒã€‚è¿™ä¸ªç¤ºä¾‹ä¸­çš„ç¬¬äºŒä¸ªåŒºåˆ«æ˜¯ä½¿ç”¨å¤šä¸ªç¯å¢ƒï¼Œè¿™ä¸€åŠŸèƒ½ç”±PTANåº“æä¾›æ”¯æŒã€‚æˆ‘ä»¬å”¯ä¸€éœ€è¦åšçš„å°±æ˜¯å°†Envå¯¹è±¡æ•°ç»„ä¼ é€’ç»™ExperienceSourceç±»ï¼Œå…¶ä»–çš„éƒ½ç”±ç³»ç»Ÿè‡ªåŠ¨å®Œæˆã€‚åœ¨å¤šä¸ªç¯å¢ƒçš„æƒ…å†µä¸‹ï¼Œç»éªŒæºä¼šä»¥è½®è¯¢çš„æ–¹å¼è¯·æ±‚å®ƒä»¬çš„è¿‡æ¸¡ï¼Œä»è€Œä¸ºæˆ‘ä»¬æä¾›æ›´å°‘ç›¸å…³çš„è®­ç»ƒæ ·æœ¬ã€‚
- en: Gradients are clipped to improve training stability. The last difference from
    the CartPole example is gradient clipping, which is performed using the PyTorch
    clip_grad_norm function from the torch.nn.utils package.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢¯åº¦è¢«è£å‰ªä»¥æé«˜è®­ç»ƒçš„ç¨³å®šæ€§ã€‚ä¸CartPoleç¤ºä¾‹çš„æœ€åä¸€ä¸ªåŒºåˆ«æ˜¯æ¢¯åº¦è£å‰ªï¼Œå®ƒæ˜¯ä½¿ç”¨PyTorchçš„clip_grad_normå‡½æ•°ï¼ˆæ¥è‡ªtorch.nn.utilsåŒ…ï¼‰è¿›è¡Œçš„ã€‚
- en: 'The hyperparameters for the best variant are the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ä½³å˜ä½“çš„è¶…å‚æ•°å¦‚ä¸‹ï¼š
- en: '[PRE21]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Results
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»“æœ
- en: Despite all my efforts to make the example converge, it wasnâ€™t very successful.
    Even after hyperparameter tuning (â‰ˆ 400 samples of hyperparameters), the best
    result has the average reward around âˆ’19.7 after 1 million training steps.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»˜å‡ºäº†å¾ˆå¤šåŠªåŠ›ä½¿ç¤ºä¾‹æ”¶æ•›ï¼Œä½†ç»“æœå¹¶ä¸ç†æƒ³ã€‚å³ä½¿ç»è¿‡è¶…å‚æ•°è°ƒä¼˜ï¼ˆçº¦400ä¸ªè¶…å‚æ•°æ ·æœ¬ï¼‰ï¼Œæœ€ä½³ç»“æœåœ¨è®­ç»ƒ1ç™¾ä¸‡æ­¥åï¼Œå¹³å‡å¥–åŠ±ä»ç„¶çº¦ä¸ºâˆ’19.7ã€‚
- en: You can try it yourself, the code is in Chapter11/05_pong_pg.py and Chapter11/05_pong_pg_tune.py.
    But I can only conclude that Pong turned out to be too complex for the vanilla
    PG method.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥è‡ªå·±å°è¯•ï¼Œä»£ç ä½äºChapter11/05_pong_pg.pyå’ŒChapter11/05_pong_pg_tune.pyä¸­ã€‚ä½†æˆ‘åªèƒ½å¾—å‡ºç»“è®ºï¼ŒPongå¯¹äºåŸå§‹çš„PGæ–¹æ³•æ¥è¯´è¿‡äºå¤æ‚ã€‚
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: 'In this chapter, you saw an alternative way of solving RL problems: policy
    gradient methods, which are different in many ways from the familiar DQN method.
    We explored a basic method called REINFORCE, which is a generalization of our
    first method in RL-domain cross-entropy. This policy gradient method is simple,
    but when applied to the Pong environment, it didnâ€™t produce good results.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç« ä¸­ï¼Œä½ çœ‹åˆ°äº†å¦ä¸€ç§è§£å†³å¼ºåŒ–å­¦ä¹ é—®é¢˜çš„æ–¹æ³•ï¼šç­–ç•¥æ¢¯åº¦æ–¹æ³•ï¼Œå®ƒä¸æˆ‘ä»¬ç†Ÿæ‚‰çš„DQNæ–¹æ³•æœ‰è®¸å¤šä¸åŒä¹‹å¤„ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸€ç§åä¸ºREINFORCEçš„åŸºæœ¬æ–¹æ³•ï¼Œå®ƒæ˜¯æˆ‘ä»¬åœ¨RLé¢†åŸŸäº¤å‰ç†µæ–¹æ³•çš„ä¸€ä¸ªæ¨å¹¿ã€‚è¿™ä¸ªç­–ç•¥æ¢¯åº¦æ–¹æ³•å¾ˆç®€å•ï¼Œä½†åœ¨åº”ç”¨åˆ°Pongç¯å¢ƒæ—¶ï¼Œæœªèƒ½äº§ç”Ÿè‰¯å¥½çš„ç»“æœã€‚
- en: In the next chapter, we will consider ways to improve the stability of policy
    gradient methods by combining the families of value-based and policy-based methods.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†è€ƒè™‘é€šè¿‡ç»“åˆåŸºäºå€¼çš„æ–¹æ³•å’ŒåŸºäºç­–ç•¥çš„æ–¹æ³•ï¼Œæ¥æé«˜ç­–ç•¥æ¢¯åº¦æ–¹æ³•çš„ç¨³å®šæ€§ã€‚
