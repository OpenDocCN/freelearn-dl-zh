- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Dataset Annotation and Labeling
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集标注和标签化
- en: '**Dataset annotation** is the process of enriching raw data within a dataset
    with informative metadata or tags, making it understandable and usable for supervised
    machine learning models. This metadata varies depending on the data type and the
    intended task. For text data, annotation can involve assigning labels or categories
    to entire documents or specific text spans, identifying and marking entities,
    establishing relationships between entities, highlighting key information, and
    adding semantic interpretations. The goal of annotation is to provide structured
    information that enables the model to learn patterns and make accurate predictions
    or generate relevant outputs.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集标注**是丰富数据集中原始数据的过程，通过添加信息性元数据或标签，使其对监督机器学习模型可理解和使用。这些元数据根据数据类型和预期任务而变化。对于文本数据，标注可能涉及为整个文档或特定的文本片段分配标签或类别，识别和标记实体，建立实体之间的关系，突出关键信息，以及添加语义解释。标注的目的是提供结构化信息，使模型能够学习模式并做出准确的预测或生成相关的输出。'
- en: '**Dataset labeling** is a specific type of dataset annotation focused on assigning
    predefined categorical tags or class labels to individual data points. This is
    commonly used for classification tasks, where the goal is to categorize data into
    distinct groups. In the context of text data, labeling might involve categorizing
    documents by sentiment, topic, or genre.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据集标注**是一种专注于为单个数据点分配预定义的类别标签或类别的特定数据集标注类型。这通常用于分类任务，其目标是将数据分类到不同的组别中。在文本数据的背景下，标注可能涉及根据情感、主题或体裁对文档进行分类。'
- en: While labeling provides crucial supervisory signals for classification models,
    annotation is a broader term encompassing more complex forms of data enrichment
    beyond simple categorization. Effective dataset annotation, including appropriate
    labeling strategies, is fundamental for developing high-performing language models
    capable of tackling diverse and sophisticated language-based tasks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然标注为分类模型提供了至关重要的监督信号，但标注是一个更广泛的术语，它包括比简单分类更复杂的数据丰富形式。有效的数据集标注，包括适当的标注策略，对于开发能够处理各种复杂语言任务的性能优异的语言模型至关重要。
- en: Dataset annotation and labeling are the processes for developing high-performing
    models. In this chapter, we’ll explore advanced techniques for creating well-annotated
    datasets that can significantly impact your LLM’s performance across various tasks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集标注和标签化是开发高性能模型的过程。在本章中，我们将探讨创建良好标注数据集的高级技术，这些技术可以显著影响您的大型语言模型（LLM）在各种任务上的性能。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: The importance of quality annotations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质量标注的重要性
- en: Annotation strategies for different tasks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同任务的标注策略
- en: Tools and platforms for large-scale text annotation
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模文本标注的工具和平台
- en: Managing annotation quality
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理标注质量
- en: Crowdsourcing annotations – benefits and challenges
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 群众外包标注 - 利益与挑战
- en: Semi-automated annotation techniques
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半自动化标注技术
- en: Scaling annotation processes for massive language datasets
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展大规模语言数据集的标注流程
- en: The importance of quality annotations
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 质量标注的重要性
- en: High-quality annotations are fundamental to the success of LLM training. They
    provide the ground truth that guides the model’s learning process, enabling it
    to understand the nuances of language and perform specific tasks accurately. Poor
    annotations can lead to biased or inaccurate models, while high-quality annotations
    can significantly enhance an LLM’s performance and generalization capabilities.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 高质量的标注对于LLM训练的成功至关重要。它们提供了指导模型学习过程的真实信息，使模型能够理解语言的细微差别并准确执行特定任务。低质量的标注可能导致有偏见或不准确的模型，而高质量的标注可以显著提高LLM的性能和泛化能力。
- en: So, what are high-quality annotations?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是高质量的标注？
- en: High-quality annotations are characterized by consistent labeling across similar
    instances, complete coverage of all relevant elements within the dataset without
    omissions, and accurate alignment with ground truth or established standards –
    this means labels must precisely reflect the true nature of the data, follow predetermined
    annotation guidelines rigorously, and maintain reliability even in edge cases
    or ambiguous situations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 高质量注释的特点是相似实例之间标签的一致性，对数据集中所有相关元素的完整覆盖，没有遗漏，并且与事实或既定标准的准确对齐——这意味着标签必须精确反映数据的真实性质，严格遵循预定的注释指南，并在边缘情况或模糊情况下保持可靠性。
- en: Let’s illustrate the impact of annotation quality with a **named-entity recognition**
    (**NER**) task using the spaCy library. NER is a **natural language processing**
    (**NLP**) technique that identifies and classifies key information (entities)
    in text into predefined categories such as names of people, organizations, locations,
    expressions of times, quantities, monetary values, and more. SpaCy is a popular
    open source library for advanced NLP in Python, known for its efficiency and accuracy.
    It provides pre-trained models that can perform various NLP tasks, including NER,
    part-of-speech tagging, dependency parsing, and more, making it easier for developers
    to integrate sophisticated language processing capabilities into their applications.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用 spaCy 库的 **命名实体识别**（**NER**）任务来说明注释质量的影响。NER 是一种 **自然语言处理**（**NLP**）技术，它将文本中的关键信息（实体）识别和分类到预定义的类别中，例如人名、组织、地点、时间、数量、货币价值等。SpaCy
    是一个流行的开源库，用于 Python 中的高级 NLP，以其效率和准确性而闻名。它提供了预训练模型，可以执行各种 NLP 任务，包括 NER、词性标注、依存句法分析等，使开发者更容易将复杂的语言处理能力集成到他们的应用程序中。
- en: 'The following Python code snippet demonstrates how to programmatically create
    training data in the spaCy format for NER tasks:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 Python 代码片段演示了如何以编程方式创建 spaCy 格式的 NER 任务训练数据：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code creates a training dataset for NER using spaCy. Let’s break it down:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用 spaCy 创建 NER 的训练数据集。让我们分解一下：
- en: We import necessary modules from spaCy, including `DocBin` for the efficient
    storage of training data.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从 spaCy 导入必要的模块，包括用于高效存储训练数据的 `DocBin`。
- en: 'The `create_training_data` function converts raw text and annotations into
    spaCy’s training format:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`create_training_data` 函数将原始文本和注释转换为 spaCy 的训练格式：'
- en: It creates a blank English language model as a starting point.
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它创建了一个空白英语语言模型作为起点。
- en: A `DocBin` object is initialized to store the processed documents efficiently.
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个 `DocBin` 对象来高效地存储处理后的文档。
- en: For each text and its annotations, we create a spaCy `Doc` object and add entity
    spans based on the provided annotations.
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个文本及其注释，我们创建一个 spaCy `Doc` 对象，并根据提供的注释添加实体跨度。
- en: We provide two example sentences with their corresponding NER annotations.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们提供了两个带有相应 NER 注释的示例句子。
- en: In this code, `doc.char_span()` creates entity spans by mapping character-level
    `start` and `end` positions from the annotations to the actual token boundaries
    in the spaCy `Doc` object. It converts raw character indices (such as `0` to `9`
    for `Apple Inc.`) into proper spaCy `Span` objects that align with token boundaries,
    ensuring the entity labels are correctly attached to the exact text sequences
    they represent within the document.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此代码中，`doc.char_span()` 通过将注释中的字符级 `start` 和 `end` 位置映射到 spaCy `Doc` 对象的实际标记边界来创建实体跨度。它将原始字符索引（例如
    `Apple Inc.` 的 `0` 到 `9`）转换为与标记边界对齐的适当的 spaCy `Span` 对象，确保实体标签正确地附加到文档中它们所代表的精确文本序列。
- en: The training data is saved to disk in spaCy’s binary format.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练数据以 spaCy 的二进制格式保存到磁盘。
- en: The quality of these annotations directly impacts the model’s ability to identify
    and classify entities correctly. For instance, if `Apple Inc.` were incorrectly
    labeled as a person instead of an organization, the model would learn to misclassify
    company names as people.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些注释的质量直接影响模型正确识别和分类实体的能力。例如，如果 `Apple Inc.` 被错误地标记为个人而不是组织，模型就会学会错误地将公司名称分类为个人。
- en: Annotation strategies for different tasks
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同任务的注释策略
- en: 'Different LLM tasks require specific annotation strategies. Let’s explore a
    few common tasks and their annotation approaches:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的 LLM 任务需要特定的注释策略。让我们探讨一些常见任务及其注释方法：
- en: '`datasets` library:'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`datasets` 库：'
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This code creates a simple dataset for sentiment analysis. Each text is associated
    with a label representing its sentiment.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码创建了一个简单的数据集用于情感分析。每个文本都与一个表示其情感的标签相关联。
- en: '**NER**: For NER, we annotate specific spans of text with entity labels. Here’s
    an approach using the **BIO** **tagging scheme**.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NER**：对于命名实体识别（NER），我们使用实体标签标注特定的文本跨度。这里介绍一种使用**BIO**标签方案的方法。'
- en: BIO tagging scheme
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: BIO标签方案
- en: The `"B-"` to mark the beginning word of an entity, `"I-"` to mark any subsequent
    words that are part of the same entity, and `"O"` to mark words that aren’t part
    of any entity. This approach solves the problem of distinguishing between adjacent
    entities and handling multi-word entities – for instance, helping models understand
    that `New York Times` is a single organization entity, or that in a sentence with
    `Steve Jobs met Steve Wozniak`, there are two distinct person entities rather
    than one or four separate entities. The simplicity and effectiveness of this labeling
    system make it a standard choice for teaching machines to recognize and classify
    named entities in text.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`"B-"`标记实体的起始单词，`"I-"`标记属于同一实体的任何后续单词，以及`"O"`标记不属于任何实体的单词。这种方法解决了区分相邻实体和处理多词实体的难题——例如，帮助模型理解`《纽约时报》`是一个单一的组织实体，或者在句子`Steve
    Jobs met Steve Wozniak`中，存在两个不同的人物实体，而不是一个或四个单独的实体。这种标签系统的简洁性和有效性使其成为机器学习识别和分类文本中命名实体的标准选择。
- en: 'The following code demonstrates how to directly encode the text into a format
    suitable for the transformer model using the tokenizer:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何使用分词器直接将文本编码成适合transformer模型的格式：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This example demonstrates how to create BIO tags for NER tasks. The `B-` prefix
    indicates the beginning of an entity, `I-` indicates the continuation of an entity,
    and `O` represents tokens outside any entity.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例演示了如何为NER任务创建BIO标签。`B-`前缀表示实体的开始，`I-`表示实体的延续，而`O`表示任何实体的外部标记。
- en: '`start` and `end` positions of the answer in the context:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 答案在上下文中的`start`和`end`位置：
- en: '[PRE3]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This code demonstrates how to annotate the answer span for a question-answering
    task.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码演示了如何为问答任务标注答案跨度。
- en: Now, let’s visit some tools and platforms for performing large-scale text annotation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看一些用于执行大规模文本标注的工具和平台。
- en: Tools and platforms for large-scale text annotation
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模文本标注的工具和平台
- en: Data annotation is the backbone of many machine learning projects, providing
    the labeled data needed to train and evaluate models. However, manual annotation,
    especially at scale, is time-consuming, error-prone, and difficult to manage.
    This is where specialized annotation tools become essential. They streamline the
    process, improve data quality, and offer features such as automation, collaboration,
    and integration with machine learning workflows, ultimately making large-scale
    annotation projects feasible and efficient.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标注是许多机器学习项目的基石，为训练和评估模型提供所需的标记数据。然而，手动标注，尤其是在大规模上，既耗时又容易出错，难以管理。这就是专业标注工具变得至关重要的地方。它们简化了流程，提高了数据质量，并提供自动化、协作以及与机器学习工作流程集成的功能，最终使大规模标注项目变得可行且高效。
- en: '**Prodigy**, a powerful commercial tool from the creators of spaCy, stands
    out for its active learning capabilities. It intelligently suggests the most informative
    examples to label next, significantly reducing annotation effort. Prodigy’s strength
    lies in its customizability, allowing users to define annotation workflows with
    Python code and seamlessly integrate them with machine learning models, especially
    within the spaCy ecosystem. It’s an excellent choice for projects that require
    complex annotation tasks, have a budget for a premium tool, and value the efficiency
    gains of active learning.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**Prodigy**，来自spaCy创建者的强大商业工具，因其主动学习功能而脱颖而出。它智能地建议下一个需要标注的最具信息量的示例，显著减少了标注工作量。Prodigy的优势在于其可定制性，允许用户使用Python代码定义标注工作流程，并将其无缝集成到机器学习模型中，尤其是在spaCy生态系统中。对于需要复杂标注任务、有预算购买高级工具且重视主动学习效率提升的项目来说，它是一个极佳的选择。'
- en: '**Label Studio** is a versatile, open source option that caters to a wide array
    of data types, including text, images, audio, and video. Its user-friendly visual
    interface and customizable labeling configurations make it accessible to annotators
    of all levels. Label Studio also supports collaboration and offers various export
    formats, making it compatible with diverse machine learning platforms. It’s a
    strong contender for projects needing a flexible, free solution that supports
    multiple data types and requires a collaborative annotation environment.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Label Studio** 是一个多功能的开源选项，适用于多种数据类型，包括文本、图像、音频和视频。它友好的可视化界面和可定制的标注配置使其对所有级别的标注者都易于使用。Label
    Studio 还支持协作，并提供多种导出格式，使其与各种机器学习平台兼容。对于需要灵活、免费解决方案且支持多种数据类型并需要协作标注环境的项目来说，它是一个强有力的竞争者。'
- en: '**Doccano** is a specialized, open source tool designed explicitly for text
    annotation in machine learning. It excels in tasks such as sequence labeling,
    text classification, and sequence-to-sequence labeling. Doccano features a simple
    and intuitive interface, supports multiple users, and provides an API for integration
    with machine learning pipelines. It’s the go-to choice for projects solely focused
    on text annotation that need a straightforward, free solution and desire seamless
    integration with their existing machine learning workflows.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**Doccano** 是一个专门为机器学习中的文本标注设计的开源工具。它在序列标注、文本分类和序列到序列标注等任务上表现出色。Doccano 具有简单直观的界面，支持多用户，并提供
    API 以与机器学习管道集成。对于仅关注文本标注且需要简单、免费解决方案并希望与现有机器学习工作流程无缝集成的项目来说，它是首选选择。'
- en: 'Here’s an example of how you might integrate annotations from Doccano into
    a Python workflow:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，说明如何将 Doccano 的标注集成到 Python 工作流程中：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This code loads NER annotations from a Doccano export file and processes them
    into a format suitable for training a BERT-based token classification model. The
    tokens and `ner_tags` in the following example show a sample format:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码从 Doccano 导出文件中加载 NER 标注，并将它们处理成适合训练基于 BERT 的标记分类模型的格式。以下示例中的标记和 `ner_tags`
    显示了样本格式：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This example demonstrates NER for identifying and classifying animal names within
    a text. The text contains a sentence about a Bengal tiger and spotted deer in
    the Sundarbans. The `labels` list provides the start and end indices of the animal
    entities (`"Bengal tiger"`, `"spotted deer"`) and their corresponding type (`"ANIMAL"`),
    as well as the geopolitical entity, i.e., `"Sundarbans"` (`"GPE"`). The `tokens`
    list is the word-level segmentation of the text. Finally, the `ner_tags` list
    represents the NER annotations in the BIO (Begin-Inside-Outside) format, where
    `"B-ANIMAL"` marks the beginning of an animal entity, `"I-ANIMAL"` marks subsequent
    words within the same animal entity, `"B-GPE"` marks the beginning of a geopolitical
    entity, and `"O"` signifies tokens that are not part of any named entity.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例演示了在文本中识别和分类动物名称的命名实体识别（NER）。文本包含关于孟加拉虎和斑点鹿在恒河三角洲的句子。`labels` 列表提供了动物实体（`"Bengal
    tiger"`，`"spotted deer"`）的起始和结束索引以及它们对应的类型（`"ANIMAL"`），以及地缘政治实体，即 `"Sundarbans"`（`"GPE"`）。`tokens`
    列表是文本的词级分割。最后，`ner_tags` 列表表示 BIO（Begin-Inside-Outside）格式的 NER 标注，其中 `"B-ANIMAL"`
    标记动物实体的开始，`"I-ANIMAL"` 标记同一动物实体内的后续单词，`"B-GPE"` 标记地缘政治实体的开始，而 `"O"` 表示不属于任何命名实体的标记。
- en: Managing annotation quality
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理标注质量
- en: To ensure high-quality annotations, we need to implement a robust quality assurance
    process.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保高质量的标注，我们需要实施一个强大的质量保证流程。
- en: 'Let’s look at some of the approaches to measure annotation quality:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些衡量标注质量的方法：
- en: '`-1` and `1`, where `1` indicates perfect agreement, `0` indicates agreement
    equivalent to chance, and negative values indicate agreement less than chance.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-1` 和 `1`，其中 `1` 表示完全一致，`0` 表示与机会一致，负值表示低于机会的一致性。'
- en: 'The following code calculates Cohen’s Kappa coefficient to quantify the agreement
    between two sets of categorical ratings:'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码计算 Cohen 的 Kappa 系数，以量化两组分类评级之间的一致性：
- en: '[PRE6]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`calculate_accuracy`, computes the agreement between a set of true labels (the
    `gold_standard`) and a set of predicted or annotated labels (annotations):'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`calculate_accuracy` 函数计算一组真实标签（即 `gold_standard`）与一组预测或标注标签（即标注）之间的协议：'
- en: '[PRE7]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: While Cohen’s Kappa and accuracy against a gold standard are fundamental, other
    metrics provide deeper insights into annotation quality. For instance, Krippendorff’s
    Alpha offers a versatile approach, accommodating various data types and handling
    missing data, making it suitable for complex annotation tasks. In scenarios involving
    multiple annotators, Fleiss’ Kappa extends Cohen’s Kappa, providing an overall
    assessment of agreement across the group.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然Cohen的Kappa和与黄金标准的准确性是基础，但其他指标可以更深入地了解标注质量。例如，Krippendorff的Alpha提供了一种灵活的方法，适应各种数据类型并处理缺失数据，使其适合复杂的标注任务。在涉及多个标注者的场景中，Fleiss的Kappa扩展了Cohen的Kappa，提供了整个群体的一致性总体评估。
- en: For tasks such as object detection or image segmentation, **intersection over
    union** (**IoU**) becomes crucial, quantifying the overlap between predicted and
    ground truth bounding boxes or masks. Furthermore, especially when dealing with
    imbalanced datasets or specific error types that are more costly, precision, recall,
    and the F1-score provide a nuanced evaluation, particularly useful in tasks such
    as NER.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于诸如目标检测或图像分割等任务，**交并比**（**IoU**）变得至关重要，它量化了预测和真实边界框或掩模之间的重叠。此外，特别是在处理不平衡数据集或成本更高的特定错误类型时，精确度、召回率和
    F1 分数提供了细微的评价，特别适用于诸如命名实体识别（NER）等任务。
- en: '**Sensitivity and specificity**: These metrics, often used in medical diagnosis
    or binary classification, are also valuable for annotation quality assessment.
    Sensitivity (also known as recall or true positive rate) measures the proportion
    of actual positives that are correctly identified, while specificity (true negative
    rate) measures the proportion of actual negatives that are correctly identified.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**敏感度和特异性**：这些指标，常用于医学诊断或二元分类，对于标注质量评估也很有价值。敏感性（也称为召回率或真正率）衡量的是实际正例中被正确识别的比例，而特异性（真正负率）衡量的是实际负例中被正确识别的比例。'
- en: '**Root mean square error** (**RMSE**) **and mean absolute error** (**MAE**):
    For tasks involving numerical or continuous annotations (e.g., rating scales,
    bounding box coordinates, etc.), RMSE and MAE can quantify the difference between
    the annotated values and the true values. RMSE gives higher weight to larger errors,
    while MAE treats all errors equally.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方根误差**（**RMSE**）和**平均绝对误差**（**MAE**）：对于涉及数值或连续标注的任务（例如，评分量表、边界框坐标等），RMSE
    和 MAE 可以量化标注值与真实值之间的差异。RMSE 对较大误差赋予更高的权重，而 MAE 对所有误差同等对待。'
- en: '**Time-based metrics**: Besides the quality of labels, the efficiency of the
    annotation process is also important. Tracking the time spent per annotation,
    especially when correlated with accuracy or agreement scores, can reveal areas
    for process improvement or identify annotators who might need additional training.
    Also, analyzing the distribution of annotation times can help identify unusually
    difficult or ambiguous instances.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于时间的指标**：除了标签的质量外，标注过程的效率也很重要。跟踪每条标注花费的时间，尤其是当与准确性或一致性评分相关联时，可以揭示流程改进的领域或识别可能需要额外培训的标注者。此外，分析标注时间的分布可以帮助识别异常困难或模糊的实例。'
- en: Ultimately, a holistic approach to annotation quality involves considering a
    combination of relevant metrics, tailored to the specific task and project goals.
    Regular monitoring, feedback loops, and iterative refinement of guidelines and
    training are essential to maintain high standards throughout the annotation process.
    Remember that the choice of metrics should align with the nature of the data,
    the complexity of the task, and the desired outcomes of the machine learning project.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，对标注质量的全面方法涉及考虑一系列相关指标的组合，这些指标针对特定的任务和项目目标量身定制。定期监控、反馈循环以及指南和培训的迭代改进对于在整个标注过程中保持高标准至关重要。记住，指标的选择应与数据的性质、任务的复杂性和机器学习项目的预期结果相一致。
- en: An effective alternative for scaling annotation efforts is the use of crowdsourcing.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**众包是扩展标注工作的有效替代方案**。'
- en: Crowdsourcing annotations – benefits and challenges
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**众包标注——优势和挑战**'
- en: 'Crowdsourcing can be an effective way to scale annotation efforts. Platforms
    such as Amazon Mechanical Turk or Appen (formerly Figure Eight) provide access
    to a large workforce. However, ensuring quality can be challenging. Here’s an
    example of how you might aggregate crowd-sourced annotations:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**众包可以是一种有效的扩展标注工作的方法**。例如，Amazon Mechanical Turk 或 Appen（前身为 Figure Eight）等平台提供了对大量工作力的访问。然而，确保质量可能具有挑战性。以下是一个如何汇总众包标注的例子：'
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This code uses a simple majority voting scheme to aggregate annotations from
    multiple annotators. While this approach is effective in many cases, tie-breakers
    are needed for situations with equal votes, and additional strategies such as
    assigning weights based on annotator reliability or leveraging machine-learning-based
    reconciliation models can further improve quality.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用简单的多数投票方案来汇总多个标注者的标注。虽然这种方法在许多情况下都有效，但在票数相等的情况下需要平局处理，并且可以采用基于标注者可靠性分配权重或利用基于机器学习的协调模型等额外策略来进一步提高质量。
- en: Next, we’ll delve into semi-automated annotation techniques, where machine learning
    models assist human annotators to accelerate labeling tasks.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入了解半自动化标注技术，其中机器学习模型协助人工标注者加速标注任务。
- en: Semi-automated annotation techniques
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半自动化标注技术
- en: 'Semi-automated annotation combines machine learning with human verification
    to speed up the annotation process. Here’s a simple example using spaCy:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 半自动化标注结合机器学习与人工验证以加快标注过程。以下是一个使用 spaCy 的简单示例：
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This code uses a pre-trained spaCy model to generate initial NER annotations,
    which can then be verified and corrected by human annotators.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用预训练的 spaCy 模型生成初始 NER 标注，然后可以由人工标注者进行验证和纠正。
- en: Next, we explore a couple of strategies for scaling annotation workflows to
    handle large-scale language datasets.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨一些策略，以扩展标注工作流程以处理大规模语言数据集。
- en: Scaling annotation processes for massive language datasets
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展大规模语言数据集的标注过程
- en: 'For massive datasets, consider the following strategies:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大规模数据集，考虑以下策略：
- en: '**Distributed processing**: Use libraries such as **Dask** or **PySpark** for
    distributed annotation processing. Dask and PySpark are powerful libraries that
    can be used for distributed data annotation processing, enabling teams to handle
    large-scale annotation tasks efficiently. These libraries allow you to parallelize
    annotation workflows across multiple cores or even clusters of computers, significantly
    speeding up the process for massive datasets. With Dask, you can scale existing
    Python-based annotation scripts to run on distributed systems, while PySpark offers
    robust data processing capabilities within the Apache Spark ecosystem. Both libraries
    provide familiar APIs that make it easier to transition from local annotation
    pipelines to distributed ones, allowing annotation teams to process and manage
    datasets that are too large for a single machine.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式处理**：使用如 **Dask** 或 **PySpark** 这样的库进行分布式标注处理。Dask 和 PySpark 是强大的库，可用于分布式数据标注处理，使团队能够高效地处理大规模标注任务。这些库允许您在多个核心或甚至计算机集群上并行化标注工作流程，显著加快大规模数据集的处理速度。使用
    Dask，您可以扩展现有的基于 Python 的标注脚本来在分布式系统中运行，而 PySpark 在 Apache Spark 生态系统内提供强大的数据处理能力。这两个库都提供了熟悉的
    API，使得从本地标注管道过渡到分布式管道变得更加容易，允许标注团队处理和管理单个机器无法处理的大型数据集。'
- en: '**Active learning**: This technique involves iteratively selecting the most
    informative samples for human labeling, based on model uncertainty or expected
    impact. Starting with a small, labeled dataset, it trains a model, uses it to
    identify valuable unlabeled samples, has humans annotate these, and then updates
    the model. This cycle repeats, optimizing annotation efforts and improving model
    performance efficiently.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主动学习**：这项技术涉及根据模型不确定性或预期影响，迭代选择最具信息量的样本进行人工标注。从一个小的、已标注的数据集开始，训练一个模型，使用它来识别有价值的未标注样本，然后由人工进行标注，并更新模型。这个周期重复进行，优化标注努力并有效地提高模型性能。'
- en: 'Here’s a simple active learning example:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里有一个简单的主动学习示例：
- en: '[PRE10]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This example demonstrates a basic active learning loop, where the model selects
    the most informative samples for annotation, potentially reducing the total number
    of annotations needed.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个例子演示了一个基本的主动学习循环，其中模型选择最具信息量的样本进行标注，这可能会减少所需的总标注数量。
- en: Now that we’ve visited some annotation techniques, let’s check out some of the
    biases that may occur while performing annotation and how we can avoid them.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了某些标注技术，让我们来看看在执行标注过程中可能出现的偏差以及如何避免它们。
- en: Annotation biases and mitigation strategies
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标注偏差及缓解策略
- en: Annotation biases are systematic errors or prejudices that can creep into labeled
    datasets during the annotation process. These biases can significantly impact
    the performance and fairness of machine learning models trained on this data,
    leading to models that are inaccurate or exhibit discriminatory behavior. Recognizing
    and mitigating these biases is crucial for building robust and ethical AI systems.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 标注偏差是在标注过程中可能渗透到标注数据集中的系统性错误或偏见。这些偏差可能会显著影响基于这些数据训练的机器学习模型的性能和公平性，导致模型不准确或表现出歧视行为。识别和减轻这些偏差对于构建稳健和道德的AI系统至关重要。
- en: 'Types of annotation bias include the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 标注偏差的类型包括以下：
- en: '**Selection bias**: This occurs when the data selected for annotation is not
    representative of the true distribution of data the model will encounter in the
    real world. For instance, if a dataset for facial recognition primarily contains
    images of people with lighter skin tones, the model trained on it will likely
    perform poorly on people with darker skin tones.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择偏差**：当用于标注的数据不能代表模型在现实世界中遇到的真实数据分布时，就会发生这种情况。例如，如果用于面部识别的数据集主要包含肤色较浅的人的图像，那么在它上面训练的模型在处理肤色较深的人时可能会表现不佳。'
- en: '**Labeling bias**: This arises from the subjective interpretations, cultural
    backgrounds, or personal beliefs of the annotators. For example, in sentiment
    analysis, annotators from different cultures might label the same text with different
    sentiment polarities. Similarly, an annotator’s personal biases might lead them
    to label certain groups or individuals more negatively or positively than others.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标注偏差**：这源于标注者的主观解释、文化背景或个人信仰。例如，在情感分析中，来自不同文化的标注者可能对相同的文本进行不同的情感极性标注。同样，标注者的个人偏见可能导致他们对某些群体或个人进行更负面或更正面的标注，而其他人则不然。'
- en: '**Confirmation bias**: Annotators might unconsciously favor labels that confirm
    their pre-existing beliefs or hypotheses about the data.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确认偏差**：标注者可能无意识地倾向于确认他们关于数据的先入为主的信念或假设。'
- en: '**Automation bias**: Over-reliance on suggestions from pre-trained models or
    active learning systems can lead annotators to accept incorrect labels without
    sufficient scrutiny.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化偏差**：过度依赖预训练模型或主动学习系统的建议可能导致标注者在没有足够审查的情况下接受错误的标签。'
- en: '**Ambiguity in guidelines**: If the annotation guidelines are unclear or incomplete,
    it can lead to inconsistent labeling across annotators, introducing noise and
    bias into the dataset.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指南中的模糊性**：如果标注指南不明确或不完整，可能会导致标注者之间标注不一致，将噪声和偏差引入数据集。'
- en: 'Here are some strategies to mitigate bias:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些减轻偏差的策略：
- en: '**Diverse and representative data**: Ensure that the data selected for annotation
    is diverse and representative of the target population and use cases. This may
    involve oversampling underrepresented groups or collecting data from multiple
    sources.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性和代表性数据**：确保用于标注的数据既多样化又能够代表目标人群和使用案例。这可能涉及对代表性不足的群体进行过度采样或从多个来源收集数据。'
- en: '**Clear and comprehensive guidelines**: Develop detailed annotation guidelines
    that clearly define the labeling criteria and provide examples for each label.
    Address potential ambiguities and edge cases in the guidelines. Regularly review
    and update the guidelines based on annotator feedback and emerging issues.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**明确和全面的指南**：制定详细的标注指南，明确定义标注标准，并为每个标签提供示例。在指南中解决潜在的模糊性和边缘情况。根据标注者的反馈和新兴问题定期审查和更新指南。'
- en: '**Annotator training and calibration**: Provide thorough training to annotators
    on the task, guidelines, and potential biases they should be aware of. Conduct
    calibration sessions where annotators label the same data and discuss any discrepancies
    to ensure consistency.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标注者培训和校准**：对标注者进行关于任务、指南以及他们应该意识到的潜在偏差的全面培训。进行校准会议，让标注者对相同的数据进行标注并讨论任何差异，以确保一致性。'
- en: '**Multiple annotators and inter-annotator agreement**: Use multiple annotators
    for each data point and measure **inter-annotator agreement** (**IAA**) using
    metrics such as Cohen’s Kappa or Fleiss’ Kappa. A high IAA indicates good consistency,
    while a low IAA suggests issues with the guidelines, training, or the task itself.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多个标注者和标注者间一致性**：对每个数据点使用多个标注者，并使用如Cohen的Kappa或Fleiss的Kappa等指标来衡量**标注者间一致性**（IAA）。高IAA表明一致性良好，而低IAA则表明指南、培训或任务本身存在问题。'
- en: '**Adjudication process**: Establish a process for resolving disagreements between
    annotators. This might involve having a senior annotator or expert review and
    make the final decision.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**裁决过程**：建立一个解决标注员之间分歧的程序。这可能涉及让资深标注员或专家审查并做出最终决定。'
- en: '**Active learning with bias awareness**: When using active learning, be mindful
    of potential biases in the model’s suggestions. Encourage annotators to critically
    evaluate the suggestions and not blindly accept them.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有偏差意识的主动学习**：在使用主动学习时，要留意模型建议中可能存在的偏差。鼓励标注员批判性地评估建议，而不是盲目接受。'
- en: '**Bias auditing and evaluation**: Regularly audit the labeled data and the
    trained models for potential biases. Evaluate model performance across different
    demographic groups or categories to identify any disparities.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏差审计和评估**：定期审计标记数据和训练模型以识别潜在的偏差。评估模型在不同人口群体或类别中的性能，以识别任何差异。'
- en: '**Diverse annotation teams**: Assemble annotation teams with diverse backgrounds,
    perspectives, and experiences to mitigate the influence of individual biases.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多元化的标注团队**：组建具有不同背景、观点和经验的标注团队，以减轻个人偏差的影响。'
- en: By implementing these mitigation strategies, you can significantly reduce the
    impact of annotation biases, leading to more accurate, fair, and reliable machine
    learning models. It’s important to remember that bias mitigation is an ongoing
    process that requires continuous monitoring, evaluation, and refinement throughout
    the entire machine learning life cycle.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施这些缓解策略，您可以显著减少标注偏差的影响，从而实现更准确、公平和可靠的机器学习模型。重要的是要记住，偏差缓解是一个持续的过程，需要在整个机器学习生命周期中持续监控、评估和改进。
- en: Summary
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: From this design pattern, you learned about advanced techniques for dataset
    annotation and labeling in LLM development. You now understand the crucial importance
    of high-quality annotations in improving model performance and generalization.
    You’ve gained insights into various annotation strategies for different LLM tasks,
    including text classification, NER, and question answering.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个设计模式中，您了解了LLM开发中数据集标注和标记的高级技术。您现在理解了高质量标注在提高模型性能和泛化能力中的关键重要性。您对各种LLM任务的标注策略有了深入了解，包括文本分类、命名实体识别和问答。
- en: In this chapter, we introduced you to tools and platforms for large-scale text
    annotation, methods for managing annotation quality, and the pros and cons of
    crowdsourcing annotations. You also learned about semi-automated annotation techniques
    and strategies for scaling annotation processes for massive language datasets,
    such as distributed processing and active learning. We provided practical examples
    using libraries such as spaCy, transformers, and scikit-learn, which helped you
    grasp key concepts and implementation approaches.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您介绍了用于大规模文本标注的工具和平台、管理标注质量的方法以及众包标注的优缺点。您还了解了半自动化标注技术和用于大规模语言数据集标注过程扩展的策略，例如分布式处理和主动学习。我们通过使用spaCy、transformers和scikit-learn等库提供了实际示例，这有助于您掌握关键概念和实现方法。
- en: In the next chapter, you’ll explore how to build efficient and scalable pipelines
    for training LLMs. This includes exploring best practices for data preprocessing,
    key considerations for designing model architectures, and strategies to optimize
    performance and scalability.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将探索如何构建用于训练LLM的高效和可扩展的管道。这包括探索数据预处理的最佳实践、设计模型架构的关键考虑因素以及优化性能和可扩展性的策略。
- en: 'Part 2: Training and Optimization of Large Language Models'
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：大型语言模型的训练和优化
- en: This part delves into the processes required to train and optimize LLMs effectively.
    We guide you through designing robust training pipelines that balance modularity
    and scalability. You will learn how to tune hyperparameters to maximize performance,
    implement regularization techniques to stabilize training, and integrate efficient
    checkpointing and recovery methods for long-running training sessions. Additionally,
    we explore advanced topics such as pruning and quantization, which enable you
    to reduce model size and computational requirements without sacrificing performance.
    Fine-tuning techniques for adapting pre-trained models to specific tasks or domains
    are also covered in detail. By the end of this part, you will be equipped to build,
    train, and optimize LLMs capable of meeting the challenges of real-world applications.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分深入探讨了有效训练和优化大型语言模型所需的过程。我们将引导您设计既模块化又可扩展的稳健训练流程。您将学习如何调整超参数以最大化性能，实施正则化技术以稳定训练，并集成高效的检查点和恢复方法以支持长时间运行的训练会话。此外，我们还将探讨高级主题，如剪枝和量化，这些技术可以帮助您在不牺牲性能的情况下减小模型大小和计算需求。此外，还将详细介绍微调技术，这些技术用于将预训练模型适应特定任务或领域。到本部分结束时，您将具备构建、训练和优化能够应对现实应用挑战的大型语言模型的能力。
- en: 'This part has the following chapters:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 7*](B31249_07.xhtml#_idTextAnchor108), *Training Pipeline*'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B31249_07.xhtml#_idTextAnchor108), *训练流程*'
- en: '[*Chapter 8*](B31249_08.xhtml#_idTextAnchor120), *Hyperparameter Tuning*'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B31249_08.xhtml#_idTextAnchor120), *超参数调整*'
- en: '[*Chapter 9*](B31249_09.xhtml#_idTextAnchor141), *Regularization*'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B31249_09.xhtml#_idTextAnchor141), *正则化*'
- en: '[*Chapter 10*](B31249_10.xhtml#_idTextAnchor162), *Checkpointing and Recovery*'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B31249_10.xhtml#_idTextAnchor162), *检查点和恢复*'
- en: '[*Chapter 11*](B31249_11.xhtml#_idTextAnchor181), *Fine-Tuning*'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B31249_11.xhtml#_idTextAnchor181), *微调*'
- en: '[*Chapter 12*](B31249_12.xhtml#_idTextAnchor191), *Model Pruning*'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第12章*](B31249_12.xhtml#_idTextAnchor191), *模型剪枝*'
- en: '[*Chapter 13*](B31249_13.xhtml#_idTextAnchor209), *Quantization*'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第13章*](B31249_13.xhtml#_idTextAnchor209), *量化*'
