- en: 3D Worlds
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3D世界
- en: We are almost nearing the end of our journey into what **artificial general
    intelligence** (**AGI**) is and how **deep reinforcement learning** (**DRL**)
    can be used to help us get there. While it is still questionable whether DRL is
    indeed the right path to AGI, it is what appears to be our current best option.
    However, the reason we are questioning DRL is because of its ability or inability
    to master diverse 3D spaces or worlds, the same 3D spaces we humans and all animals
    have mastered but something we find very difficult to train RL agents on. In fact,
    it is the belief of many an AGI researcher that solving the 3D state-space problem
    could go a long way to solving true general artificial intelligence. We will look
    at why that is the case in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎接近了探索什么是**通用人工智能**（**AGI**）以及如何使用**深度强化学习**（**DRL**）帮助我们达到这一目标的旅程的终点。尽管目前对DRL是否确实是通往AGI的正确路径还存在疑问，但它似乎是我们当前最好的选择。然而，我们质疑DRL的原因在于其能否或不能掌握多样化的3D空间或世界，正如人类和所有动物所掌握的3D空间，但我们发现很难在RL代理上训练。事实上，许多AGI研究人员的信念是，解决3D状态空间问题对于解决真正的通用人工智能可能大有裨益。我们将在本章中探讨为什么这是可能的。
- en: For this chapter, we are going to look at why 3D worlds pose such a unique problem
    to DRL agents and the ways we can train them to interpret state. We will look
    at how typical 3D agents use vision to interpret state and we will look to the
    type of deep learning networks derived from that. Then we look to a practical
    example of using 3D vision in an environment and what options we have for processing
    state. Next, sticking with Unity, we look at the Obstacle Tower Challenge, an
    AI challenge with a $100,000 prize, and what implementation was used to win the
    prize. Moving to the end of the chapter, we will look at another 3D environment
    called Habitat and how it can be used for developing agents.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将探讨为什么3D世界对DRL代理构成了如此独特的问题，以及我们可以如何训练它们来解释状态。我们将探讨典型3D代理如何使用视觉来解释状态，并考察由此产生的深度学习网络类型。然后，我们将探讨在环境中使用3D视觉的一个实际例子以及我们处理状态的可选方案。接下来，继续使用Unity，我们将探讨障碍塔挑战，这是一个有10万美元奖金的AI挑战，以及赢得奖金所使用的实现方法。在章节的结尾，我们将探讨另一个名为Habitat的3D环境，以及它如何用于开发代理。
- en: 'Here is a summary of the main points we will discuss this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论的主要要点如下：
- en: Reasoning on 3D worlds
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在3D世界中推理
- en: Training a visual agent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练视觉代理
- en: Generalizing 3D vision
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用化3D视觉
- en: Challenging the Unity Obstacle Tower Challenge
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 挑战Unity障碍塔挑战
- en: Exploring habitat—embodied agents by FAIR
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索FAIR的栖息地——具身代理
- en: The examples in this chapter can take an especially long time to train, so please
    either be patient or perhaps just choose to do one. This not only saves you time
    but reduces energy consumption. In the next chapter, we explore why 3D worlds
    are so special.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的示例可能需要特别长的时间来训练，所以请耐心等待，或者也许您可以选择只做一项。这不仅节省了您的时间，还减少了能源消耗。在下一章中，我们将探讨为什么3D世界如此特别。
- en: Reasoning on 3D worlds
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在3D世界中推理
- en: So, why are 3D worlds so important, or are at least believed to be so? Well,
    it all has to come down to state interpretation, or what we in DRL like to call
    state representation. A lot of work is being done on better representation of
    state for RL and other problems. The theory is that being able to represent just
    key or converged points of state allow us to simplify the problem dramatically.
    We have looked at doing just that using various techniques over several chapters.
    Recall how we discretized the state representation of a continuous observation
    space into a discrete space using a grid mesh. This technique is how we solved
    more difficult continuous space problems with the tools we had at the time. Over
    the course of several chapters since then, we saw how we could input that continuous
    space directly into our deep learning network. That included the ability to directly
    input an image as the game state, a screenshot, using convolutional neural networks.
    However, 3D worlds, ones that represent the real world, pose a unique challenge
    to representing state.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么3D世界如此重要，或者至少人们认为它很重要呢？好吧，这一切都归结于状态解释，或者我们DRL（深度强化学习）领域喜欢称之为状态表示。目前有很多工作致力于为强化学习和其他问题提供更好的状态表示。理论上是这样的，能够仅仅表示状态的关键点或收敛点，可以使我们显著简化问题。我们在多个章节中探讨了使用各种技术来实现这一点。回想一下，我们是如何将连续观察空间的状态表示离散化到网格网格中的。这种技术是我们当时使用现有工具解决更困难连续空间问题的方法。从那时起，在几个章节中，我们看到了如何将连续空间直接输入到我们的深度学习网络中。这包括直接将图像作为游戏状态、截图输入，使用卷积神经网络。然而，3D世界，那些代表现实世界的世界，在表示状态方面提出了独特的挑战。
- en: So what is the difficulty in representing the state space in a 3D environment?
    Could we not just give the agent sensors, as we did in other environments? Well,
    yes and no. The problem is that giving the agent sensors is putting our bias on
    what the agent needs to use in order to interpret the problem. For example, we
    could give the agent a sensor that told it the distance of an object directly
    in front of it, as well as to its left and right. While that would likely be enough
    information for any driving agent, would it work for an agent that needed to climb
    stairs? Not likely. Instead, we would likely need to give the height of the stairs
    as another sensor input, which means our preferred method of introducing state
    to an agent for a 3D world is using vision or an image of the environment. The
    reason for this, of course, is to remove any bias on our part (us humans) and
    we can best do that by just feeding the environment state as an image directly
    to the agent.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在3D环境中表示状态空间有什么困难呢？我们难道不能像在其他环境中那样给智能体传感器吗？好吧，是的，也不完全是。问题是给智能体传感器实际上是我们对智能体需要用来解释问题的需求强加偏见。例如，我们可以给智能体一个传感器，直接告诉它前方、左侧和右侧物体的距离。虽然这可能对任何驾驶智能体来说足够了，但对于需要爬楼梯的智能体来说，可能就不适用了。相反，我们可能需要提供楼梯高度作为另一个传感器输入，这意味着我们为3D世界引入状态给智能体的首选方法是使用视觉或环境图像。当然，这样做的原因是为了消除我们（人类）的任何偏见，我们最好的做法就是直接将环境状态作为图像直接喂给智能体。
- en: We have already seen how we could input game state using an image of the playing
    area when we looked at playing Atari games. However, those game environments were
    all 2D, meaning the state space was essentially flattened or converged. The word
    **converged** works here because this becomes the problem when tackling 3D environments
    and the real world. In 3D space, one vantage point could potentially yield to
    multiple states, and likewise, multiple state spaces can be observed by one single
    vantage point.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，当我们研究玩Atari游戏时，如何使用游戏区域的图像输入游戏状态。然而，那些游戏环境都是2D的，这意味着状态空间本质上被扁平化或收敛了。在这里，“收敛”这个词适用，因为当处理3D环境和现实世界时，这变成了一个问题。在3D空间中，一个视角可能产生多个状态，同样，一个视角也可能观察到多个状态空间。
- en: 'We can see how this works in the following diagram:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下图中看到这是如何工作的：
- en: '![](img/3615c006-7587-4155-9acf-ad804d03949f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3615c006-7587-4155-9acf-ad804d03949f.png)'
- en: Examples of agent state in a 3D world
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 3D世界中的智能体状态示例
- en: In the diagram, we can see the agent, the blue dot in the center of Visual Hallway
    environment in Unity with the ML-Agents toolkit. We will review an example of
    this environment shortly, so don't worry about reviewing it just yet. You can
    see from the diagram how the agent is observing different observations of state
    from the same physical position using an agent camera. An agent camera is the
    vision we give to the agent to observe the world.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，我们可以看到代理，在 Unity 的 Visual Hallway 环境中心的一个蓝色点，使用 ML-Agents 工具包。我们很快将回顾这个环境的示例，所以请不用担心现在就复习它。您可以从图中看到，代理是如何使用代理相机从同一物理位置观察不同状态观察的。代理相机是我们给予代理以观察世界的视觉。
- en: From this camera, the agent ingests the state as a visual observation that is
    fed as an image into a deep learning network. This image is broken up with 2D
    convolutional neural network layers into features, which the agent learns. The
    problem is that we are using 2D filters to try and digest 3D information. In [Chapter
    7](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml), *Going Deeper with DDQN*, we explored
    using CNNs to ingest the image state from Atari games and, as we have seen, this
    works very well.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个相机，代理将状态作为视觉观察摄入，这个观察作为图像输入到深度学习网络中。这个图像被 2D 卷积神经网络层分解成特征，这些特征是代理学习的。问题是，我们正在使用
    2D 滤波器来尝试消化 3D 信息。在 [第 7 章](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml)，*使用 DDQN
    深入学习* 中，我们探讨了使用 CNN 从 Atari 游戏中摄入图像状态，正如我们所看到的，这非常有效。
- en: You will need the ML-Agents toolkit installed and should have opened the **UnitySDK**
    test project. If you need assistance with this, return to [Chapter 11](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml),
    *Exploiting ML-Agents, *and follow some of the exercises there first.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装 ML-Agents 工具包，并且应该已经打开了 **UnitySDK** 测试项目。如果您需要这方面的帮助，请返回到 [第 11 章](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml)，*利用
    ML-Agents*，并首先遵循那里的一些练习。
- en: 'Unity does the same thing for its agent camera setup, and in the next exercise
    we will see how the following looks:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Unity 对其代理相机设置也做同样的事情，在下一个练习中，我们将看到以下内容：
- en: Locate the folder at `ml-agents/ml-agents/mlagents/trainers` located in the
    ML-Agents repository. If you need help pulling the repository, follow the previous
    information tip given.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定位到位于 ML-Agents 仓库中的 `ml-agents/ml-agents/mlagents/trainers` 文件夹。如果您需要帮助拉取仓库，请遵循之前给出的信息提示。
- en: From this folder, locate and open the `models.py` file in a text or Python IDE.
    ML-Agents is written in TensorFlow, which may be intimidating at first, but the
    code follows many of the same principles as PyTorch.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这个文件夹中，找到并打开文本或 Python IDE 中的 `models.py` 文件。ML-Agents 使用 TensorFlow 编写，一开始可能有些令人畏惧，但代码遵循了许多与
    PyTorch 相同的原则。
- en: Around line 250 a `create_visual_observation_encoder` function from the `LearningModel` base
    class is created. This is the base class model that ML-Agents, the PPO, and SAC
    implementations use.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大约在第 250 行，从 `LearningModel` 基类创建了一个 `create_visual_observation_encoder` 函数。这是
    ML-Agents、PPO 和 SAC 实现使用的基类模型。
- en: ML-Agents was originally developed in Keras and then matured to TensorFlow in
    order to improve performance. Since that time, PyTorch has seen a huge surge in
    popularity for academic researchers, as well as builders. At the time of writing,
    PyTorch is the fastest growing DL framework. It remains to be seen if Unity will
    also follow suit and convert the code to PyTorch, or just upgrade it to TensorFlow
    2.0.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents 最初是在 Keras 中开发的，然后成熟到 TensorFlow 以提高性能。从那时起，PyTorch 在学术研究人员和构建者中看到了巨大的流行增长。在撰写本文时，PyTorch
    是增长最快的深度学习框架。目前尚不清楚 Unity 是否也会效仿并将代码转换为 PyTorch，或者只是升级到 TensorFlow 2.0。
- en: 'The `create_visual_observation_encoder` function is the base function for encoding
    state, and the full definition of the function (minus comments) is shown here:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`create_visual_observation_encoder` 函数是编码状态的基函数，函数的完整定义（不包括注释）如下所示：'
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'While the code is in TensorFlow, there are a few obvious indicators of common
    terms, such as layers and conv2d. With that information, you can see that this
    encoder uses two CNN layers: one with a kernel size of 8 x 8, a stride of 4 x
    4, and 16 filters; followed by a second layer that uses a kernel size of 4 x 4,
    a stride of 2 x 2, and 32 filters.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虽然代码在 TensorFlow 中，但有一些明显的常见术语的指标，例如 layers 和 conv2d。有了这些信息，您可以看到这个编码器使用了两个
    CNN 层：一个具有 8 x 8 的内核大小、4 x 4 的步长和 16 个滤波器；接着是一个使用 4 x 4 的内核大小、2 x 2 的步长和 32 个滤波器的第二层。
- en: Notice again the use of no pooling layers. This is because spatial information
    is lost when we use pooling between CNN layers. However, depending on the depth
    of the network, a single pooling layer near the top can be beneficial.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意再次使用了没有池化层的做法。这是因为当我们使用池化在 CNN 层之间时，会丢失空间信息。然而，根据网络的深度，靠近顶部的单个池化层可能是有益的。
- en: 'Notice the return from the function is a hidden flat layer denoted by `hidden_flat`.
    Recall that our CNN layers are being used to learn state that is then fed into
    our learning network as the following diagram shows:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意函数的返回值是一个由 `hidden_flat` 表示的隐藏平坦层。回想一下，我们的 CNN 层被用来学习状态，然后作为以下图所示的那样，将其输入到我们的学习网络中。
- en: '![](img/f158292d-be01-4052-a899-7c7353056078.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f158292d-be01-4052-a899-7c7353056078.png)'
- en: Example network diagram
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 示例网络图
- en: The preceding diagram is a simplified network diagram showing that the CNN layers
    flatten as they feed into the hidden middle layer. Flattening is converting that
    convolutional 2D data into a one-dimensional vector and then feeding that into
    the rest of the network.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述图是一个简化的网络图，显示了 CNN 层在输入到隐藏中间层时会被展平。展平是将卷积 2D 数据转换为一条一维向量，然后将其输入到网络的其余部分。
- en: We can see how the image source is defined by opening up the Unity editor to
    the **ML-Agents UnitySDK** project to the **VisualHallway** scene located in the
    `Assets/ML-Agents/Examples/Hallway/Scenes` folder.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过打开 Unity 编辑器到 `Assets/ML-Agents/Examples/Hallway/Scenes` 文件夹中的 **VisualHallway**
    场景来查看图像源是如何定义的。
- en: 'Expand the first **VisualSymbolFinderArea** and select the **Agent** object
    in the **Hierarchy** window. Then, in the **Inspector** window, locate and double-click
    on the **Brain** to bring it up in the following window:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展开第一个 **VisualSymbolFinderArea**，在 **Hierarchy** 窗口中选择 **Agent** 对象。然后，在 **Inspector**
    窗口中找到并双击 **Brain**，将其在以下窗口中打开：
- en: '![](img/e8ab9385-dd56-41f1-9f1c-f8e16e985c52.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e8ab9385-dd56-41f1-9f1c-f8e16e985c52.png)'
- en: Inspecting the VisualHallwayLearning Brain
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 VisualHallwayLearning 脑
- en: The important thing to note here is that the agent is set up to accept an image
    of size 84 x 84 pixels. That means the agent camera is sampled down to an image
    size matching the same pixel area. A relatively small pixel area for this environment
    works because of the lack of detail in the scene. If the detail increased, we
    would likely also need to increase the resolution of the input image.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的重要一点是，代理被设置为接受 84 x 84 像素大小的图像。这意味着代理摄像头被采样到一个与相同像素面积匹配的图像大小。由于场景中缺乏细节，这个相对较小的像素面积对于这个环境来说是可以工作的。如果细节增加，我们可能还需要增加输入图像的分辨率。
- en: In the next section, we look at training the agent visually using the ML-Agents
    toolkit.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨使用 ML-Agents 工具包通过视觉训练代理。
- en: Training a visual agent
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练视觉代理
- en: Unity develops a 2D and 3D gaming engine/platform that has become the most popular
    platform for building games. Most of these games are the 3D variety, hence the
    specialized interest by Unity in mastering the task of agents that can tackle
    more 3D natural worlds. It naturally follows then that Unity has invested substantially
    into this problem and has/is working with DeepMind to develop this further. How
    this collaboration turns out remains to be seen, but one thing is for certain
    is that Unity will be our go-to platform for exploring 3D agent training.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Unity 开发了一个 2D 和 3D 游戏引擎/平台，它已经成为构建游戏最受欢迎的平台。其中大部分游戏是 3D 类型，因此 Unity 对掌握能够处理更多
    3D 自然世界的代理的任务产生了专门兴趣。因此，Unity 在这个问题上投入了大量资金，并且与 DeepMind 合作进一步开发。这种合作的结果还有待观察，但有一点可以肯定的是，Unity
    将成为我们探索 3D 代理训练的首选平台。
- en: 'In the next exercise, we are going to jump back into Unity and look at how
    we can train an agent in a visual 3D environment. Unity is arguably the best place
    to set up and build these type of environments as we have seen in the earlier
    chapters. Open the Unity editor and follow these steps:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将回到 Unity，看看我们如何在视觉 3D 环境中训练一个代理。Unity 可以说是设置和构建这类环境的最佳场所，正如我们在前面的章节中看到的。打开
    Unity 编辑器，按照以下步骤操作：
- en: Open the **VisualHallway** scene located in the `Assets/ML-Agents/Examples/Hallway/Scenes`
    folder.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于 `Assets/ML-Agents/Examples/Hallway/Scenes` 文件夹中的 **VisualHallway** 场景。
- en: 'Locate the **Academy** object in the scene hierarchy window and set the **Control**
    option to enabled on the **Hallway Academy** component **Brains** section and
    as shown in the following screenshot:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在场景层次结构窗口中找到 **Academy** 对象，并将 **Hallway Academy** 组件的 **Brains** 部分的 **Control**
    选项设置为启用，如图所示：
- en: '![](img/8d787f4a-007b-4d0f-9960-e166f925ad35.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d787f4a-007b-4d0f-9960-e166f925ad35.png)'
- en: Setting the academy to control the learning brain
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 将学院设置为控制学习大脑
- en: This sets the Academy to control the Brain for the agent.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将设置学院以控制智能体的大脑。
- en: 'Next, select all the **VisualSymbolFinderArea** objects from **(1)** to **(7)**
    and then make sure to enable them all by clicking the object''s **Active** option
    in the Inspector window, as shown in the following screenshot:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从**(1)**到**(7)**选择所有**VisualSymbolFinderArea**对象，并确保通过在检查器窗口中点击对象的**Active**选项来启用它们，如下截图所示：
- en: '![](img/e657e8c6-2c23-473a-94c7-2d98d97914e8.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e657e8c6-2c23-473a-94c7-2d98d97914e8.png)'
- en: Enabling all the sub-environments in the scene
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 启用场景中的所有子环境
- en: This enables all the sub environment areas and allows us to run an additional
    seven agents when training. As we have seen when using actor-critic methods, being
    able to sample more efficiently from the environment has many advantages. Almost
    all the example ML-Agents environments provide for multiple sub-training environments.
    These multiple environments are considered separate environments but allow for
    the brain to be trained synchronously with multiple agents.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这使得所有子环境区域都可以运行，并且我们在训练时可以额外运行七个智能体。正如我们使用演员-评论员方法时所看到的，能够更有效地从环境中采样有许多优势。几乎所有示例ML-Agents环境都提供了多个子训练环境。这些多个环境被认为是独立的环境，但允许大脑与多个智能体同步训练。
- en: Save the scene and the project file from the **File** menu.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**文件**菜单保存场景和项目文件。
- en: Open a new Python or Anaconda shell and set the virtual environment to use the
    one you set up earlier for ML-Agents. If you need help, refer to [Chapter 11](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml),
    *Exploiting ML-Agents*.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Python或Anaconda shell，并将虚拟环境设置为使用你之前为ML-Agents设置的虚拟环境。如果你需要帮助，请参阅[第11章](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml)，*利用ML-Agents*。
- en: 'Navigate to the Unity `ml-agents` folder and execute the following command
    to start training:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到Unity `ml-agents` 文件夹并执行以下命令以开始训练：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will start the Python trainer, and after a few seconds, will prompt you
    to click Play in the editor. After you do that, the agents in all the environments
    will begin training and you will be able to visualize this in the editor. An example
    of how this looks in the command shell is shown in the following screenshot:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将启动Python训练器，几秒钟后，会提示你在编辑器中点击播放。完成此操作后，所有环境中的智能体将开始训练，你可以在编辑器中可视化这个过程。以下截图显示了在命令行中看起来是怎样的：
- en: '![](img/48f2e1f1-d14a-4bc4-a4b5-7ec0a648f834.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48f2e1f1-d14a-4bc4-a4b5-7ec0a648f834.png)'
- en: Running the ML-Agents trainer
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 运行ML-Agents训练器
- en: Now that we have reviewed how to train an agent in Unity with ML-Agents, we
    can move on to explore some other undocumented training options in the next section.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何在Unity中使用ML-Agents训练智能体，我们可以继续探索下一节中的一些其他未记录的训练选项。
- en: If you encounter problems training the Hallway environment, you can always try
    one of the other various environments. It is not uncommon for a few of the environments
    to become broken because of releases or version conflicts.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到在走廊环境中训练的问题，你总是可以尝试其他各种环境。由于发布或版本冲突，一些环境变得损坏并不罕见。
- en: Generalizing 3D vision
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推广3D视觉
- en: As previously mentioned in [Chapter 11](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml),
    *Exploiting ML-Agents*, we saw how the team at Unity is one of the leaders in
    training agents for 3D worlds. After all, they do have a strong vested interest
    in providing an AI platform that developers can just plug into and build intelligent
    agents. Except, the very agents that fit this broad type of application are now
    considered the first step to AGI because if Unity can successfully build a universal
    agent to play any game, it will have effectively built a first-level AGI.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文在[第11章](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml)中提到的，*利用ML-Agents*，我们了解到Unity团队在训练3D世界中的智能体方面是领导者之一。毕竟，他们确实有很强的利益驱动，提供开发者可以轻松接入并构建智能体的AI平台。然而，这种适用于广泛应用的智能体现在被认为是通往通用人工智能的第一步，因为如果Unity能够成功构建一个能够玩任何游戏的通用智能体，那么它实际上就构建了一个初级通用人工智能。
- en: The problem with defining AGI is trying to understand how broad or general an
    intelligence has to be as well as how we quantify the agent's understanding of
    that environment and possible ability to transfer knowledge to other tasks. We
    really won't know how best to define what that is until someone has the confidence
    to stand up and claim to have developed an AGI. A big part of that claim will
    depend on how well an agent can generalize environmental state and a big part
    of that will be generalizing 3D vision itself.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 AGI 的问题在于试图理解一种智能有多广泛或有多一般，以及我们如何量化智能体对环境的理解以及将知识转移到其他任务的可能能力。我们真的不知道如何最好地定义它，直到有人有信心站起来并声称已经开发出
    AGI。这个声明的一个很大部分将取决于智能体如何泛化环境状态，而其中很大一部分将是泛化 3D 视觉本身。
- en: Unity has an undocumented way to alter the type of visual encoder you can use
    in training on an environment (at least at time of writing).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Unity 有一种未记录的方法可以改变在环境中训练时可以使用的视觉编码器的类型（至少在撰写本文时是这样）。
- en: 'In the next exercise, we look at how the hyperparameter can be added to the
    configuration and set for different visual encoders by following these steps:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将查看如何通过以下步骤将超参数添加到配置中并设置不同的视觉编码器：
- en: Locate the `trainer_config.yaml` configuration file located in the `mlagents/ml-agents/config`
    folder and open it in an IDE or text editor.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到位于 `mlagents/ml-agents/config` 文件夹中的 `trainer_config.yaml` 配置文件，并在 IDE 或文本编辑器中打开它。
- en: '**YAML** is an acronym that stands for for **YAML ain''t markup language**.
    The format of the ML-Agents configuration markup files is quite similar to Windows
    INI configuration files of old.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**YAML** 是一个缩写，代表 **YAML ain''t markup language**。ML-Agents 配置标记文件的格式与旧版的 Windows
    INI 配置文件非常相似。'
- en: 'This file defines the configuration for the various learning brains. Locate
    the section for the `VisualHallwayLearning` brain as shown here:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此文件定义了各种学习大脑的配置。找到 `VisualHallwayLearning` 大脑的章节，如下所示：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'These hyperparameters are additional to a set of base values set in a default
    brain configuration at the top of the config file and shown as follows:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些超参数是除了在配置文件顶部默认大脑配置中设置的基值之外额外的。如下所示：
- en: '[PRE3]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The hyperparameter of interest for us is the `vis_encode_type` value set to
    simple highlighted in the preceding code example. ML-Agents supports two additional
    types of visual encoding by changing that option like so:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们感兴趣的超参数是设置为简单并突出显示在前面代码示例中的 `vis_encode_type` 值。ML-Agents 通过更改此选项支持两种额外的视觉编码类型：
- en: '`vis_enc_type`: Hyperparameter to set type of visual encoding:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vis_enc_type`：设置视觉编码类型的超参数：'
- en: '`simple`: This is the default and the version we already looked at.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`simple`：这是默认版本，也是我们之前看过的版本。'
- en: '`nature_cnn`: This defines a CNN architecture proposed by a paper in Nature,
    we will look at this closer shortly.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nature_cnn`：这定义了由 Nature 期刊中一篇论文提出的 CNN 架构，我们将在稍后更详细地了解它。'
- en: '`resnet`: ResNet is a published CNN architecture that has been shown to be
    very effective at image classification.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resnet`：ResNet 是一种已发表的 CNN 架构，已被证明在图像分类方面非常有效。'
- en: 'We will change the default value in our `VisualHallwayLearning` brain by adding
    a new line to the end of the brain''s configuration:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过在 `VisualHallwayLearning` 大脑配置的末尾添加新行来更改我们大脑中的默认值：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we know how to set these, let''s see what they look like by opening
    the `models.py` code like we did earlier from the `ml-agents/trainers` folder.
    Scroll down past the `create_visual_observation_encoder` function to the `create_nature_cnn_observation_encoder`
    function shown here:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们知道了如何设置这些，让我们通过打开 `ml-agents/trainers` 文件夹中我们之前打开的 `models.py` 代码来看看它们的样子。滚动到
    `create_visual_observation_encoder` 函数下方，找到如下所示的 `create_nature_cnn_observation_encoder`
    函数：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The main difference with this implementation is the use of a third layer called
    `conv3`. We can see this third layer has a kernel size of 3 x 3, a stride of 1
    x 1 and 64 filters. With a smaller kernel and stride size, we can see this new
    layer is being used to extract finer features. How useful that is depends on the
    environment.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与这种实现的主要区别在于使用了名为 `conv3` 的第三层。我们可以看到这个第三层的核大小为 3 x 3，步长为 1 x 1，有 64 个滤波器。由于核和步长尺寸较小，我们可以看到这个新层被用来提取更精细的特征。这种特征有多有用取决于环境。
- en: 'Next, we want to look at the third visual encoding implementation listed just
    after the last function. The next function is `create_resent_visual_observation_encoder`
    and is shown as follows:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们想查看列在最后一个函数之后的第三个视觉编码实现。下一个函数是 `create_resent_visual_observation_encoder`，如下所示：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can now go back and update the `vis_enc_type` hyperparameter in the config
    file and retrain the visual agent. Note which encoder is more successful if you
    have time to run both versions.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在可以回到配置文件中更新`vis_enc_type`超参数，并重新训练视觉代理。如果你有时间运行两个版本，注意哪个编码器更成功。
- en: We have seen what variations of visual encoders that ML-Agents supports and
    the team at Unity has also included a relatively new variant called ResNet. ResNet
    is an important achievement, and thus far, has shown to be useful for training
    agents in some visual environments. Therefore, in the next section, we will spend
    some extra time looking at ResNet.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了ML-Agents支持的视觉编码器的各种变体，Unity团队也加入了一个相对较新的变体，称为ResNet。ResNet是一个重要的成就，迄今为止，它已被证明在某些视觉环境中训练代理是有用的。因此，在下一节中，我们将花更多的时间来探讨ResNet。
- en: ResNet for visual observation encoding
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于视觉观察编码的ResNet
- en: Convolutional layers have been used in various configurations for performing
    image classification and recognition tasks successfully for some time now. The
    problem we encounter with using straight 2D CNNs is we are essentially flattening
    state representations, but generally not in a good way. This means that we are
    taking a visual observation of a 3D space and flattening it to a 2D image that
    we then try and extract important features from. This results in an agent thinking
    it is in the same state if it recognizes the same visual features from potentially
    different locations in the same 3D environment. This creates confusion in the
    agent and you can visualize this by watching an agent just wander in circles.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层已经被用于各种配置，用于成功执行图像分类和识别任务已有一些时间了。我们使用直接2D CNN时遇到的问题是，我们本质上是在平坦化状态表示，但通常不是以好的方式。这意味着我们正在将3D空间的视觉观察结果平坦化成2D图像，然后尝试从中提取重要特征。这导致代理认为如果它从同一3D环境中的不同位置识别出相同的视觉特征，它就处于相同的状态。这会在代理中造成混淆，你可以通过观察一个代理只是绕着圈子乱转来可视化这一点。
- en: The same type of agent confusion can often be seen happening due to vanishing
    or exploding gradients. We haven't encountered this problem very frequently because
    our networks have been quite shallow. However, in order to improve network performance,
    we often deepen the network by adding additional layers. In fact, in some vision
    classification networks, there could be 100 layers or more of convolution trying
    to extract all manner of features. By adding this many additional layers, we introduce
    the opportunity for vanishing gradients. A vanishing gradient is a term we use
    for a gradient that becomes so small as to appear to vanish, or really have no
    effect on training/learning. Remember that our gradient calculation requires a
    total loss that is then transferred back through the network. The more layers
    the loss needs to push back through the network, the smaller it becomes. This
    is a major issue in the deep CNN networks that we use for image classification
    and interpretation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的代理混淆通常是由于梯度消失或爆炸引起的。我们并没有经常遇到这个问题，因为我们的网络相当浅。然而，为了提高网络性能，我们经常通过添加额外的层来加深网络。实际上，在一些视觉分类网络中，可能有100层或更多的卷积层试图提取各种特征。通过添加这么多额外的层，我们引入了梯度消失的机会。梯度消失是我们用来描述梯度变得如此之小，以至于看起来消失了，或者实际上对训练/学习没有影响的术语。记住，我们的梯度计算需要一个总损失，然后通过网络传递回来。损失需要推回网络的层数越多，它就越小。这是我们用于图像分类和解释的深度CNN网络中的一个主要问题。
- en: 'ResNet or residual CNN networks were introduced as a way of allowing for deeper
    encoding structures without suffering vanishing gradients. Residual networks are
    so named because they carry forth a residual identity called an **identity shortcut
    connection**. The following diagram, sourced from the *Deep Residual Learning
    for Image Recognition* paper, shows the basic components in a residual block:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet或残差CNN网络被引入作为一种允许更深的编码结构而不受梯度消失影响的方法。残差网络之所以被称为残差网络，是因为它们携带一个称为**身份快捷连接**的残差身份。以下图表来自*深度残差学习用于图像识别*论文，展示了残差块中的基本组件：
- en: '![](img/88c43406-c183-43a5-9841-a062dce3403f.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/88c43406-c183-43a5-9841-a062dce3403f.png)'
- en: A residual block
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一个残差块
- en: The intuition from the authors in the paper is that stacked layers shouldn't
    degrade network performance just because they are stacked. Instead, by pushing
    the output of the last layer to the layer ahead, we are effectively able to isolate
    training to individual layers. We refer to this as an **identity** because the
    size of the output from the last layer will likely not match the input of the
    next layer, since we are bypassing the middle layer. Instead, we multiply the
    output of the last layer with an identity input tensor in order to match the output
    to the input.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 论文作者的直觉是，堆叠的层不应该仅仅因为它们是堆叠的而降低网络性能。相反，通过将最后一层的输出推送到下一层，我们实际上能够有效地将训练隔离到各个单独的层。我们称这为**恒等**，因为最后一层输出的尺寸可能不会匹配下一层的输入，因为我们绕过了中间层。相反，我们用恒等输入张量乘以最后一层的输出，以便匹配输出和输入。
- en: 'Let''s jump back to the ResNet encoder implementation back in ML-Agents and
    see how this is done in the next exercise:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到ML-Agents中的ResNet编码器实现，看看在下一个练习中是如何做到这一点的：
- en: Open the `models.py` file located in the `mlagents/ml-agents/trainers` folder.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于 `mlagents/ml-agents/trainers` 文件夹中的 `models.py` 文件。
- en: 'Scroll down to the `create_resnet_visual_observation_encoder` function again.
    Look at the first two lines that define some variables for building up the residual
    network as shown here:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次向下滚动到 `create_resnet_visual_observation_encoder` 函数。查看定义构建残差网络的变量，如下所示的前两行：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, scroll down a little more to where we enumerate the number of channels
    listed to build up each of the input layers. The code is shown as follows:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，再向下滚动一点，到我们列举构建每个输入层所需通道数的地方。代码如下所示：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The `n_channels` variable represents the number of channels or filters used
    in each of the input convolution layers. Thus, we are creating three groups of
    residual layers with an input layer and blocks in between. The blocks are used
    to isolate the training to each of the layers.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n_channels` 变量表示每个输入卷积层使用的通道数或过滤器。因此，我们正在创建包含输入层和中间块的三个残差层组。这些块用于将训练隔离到每一层。'
- en: 'Keep scrolling down, and we can see where the blocks are constructed between
    the layers with the following code:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续向下滚动，我们可以看到以下代码中块是如何在层之间构建的：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This code creates a network structure similar to what is shown in the following
    diagram:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码创建了一个类似于以下图中所示的网络结构：
- en: '![](img/078574c1-5e9c-4d9d-8858-1ef548849664.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/078574c1-5e9c-4d9d-8858-1ef548849664.png)'
- en: Diagram of the ResNet architecture in ML-Agents
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ML-Agents中ResNet架构图
- en: In essence, we still only have three distinct convolutional layers extracting
    features, but each of those layers can now be trained independently. Furthermore,
    we can likely increase the depth of this network several times and expect an increase
    in visual encoding performance.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从本质上讲，我们仍然只有三个不同的卷积层提取特征，但每个这样的层现在可以独立训练。此外，我们很可能将这个网络的深度增加几倍，并预期视觉编码性能会有所提高。
- en: Go back and, if you have not already done so, train a visual agent with residual
    networks for visual observation encoding.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回去，如果你还没有这样做，用残差网络训练一个视觉代理进行视觉观察编码。
- en: What you will likely find if you went back and trained another visual agent
    with residual networks is the agent performs marginally better, but they still
    can get confused. Again, this is more of a problem with the visual encoding system
    than the DRL itself. However, it is believed that once we can tackle visual encoding
    of visual environments, real AGI will certainly be a lot closer.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回去用残差网络训练另一个视觉代理，你可能会发现代理的表现略有提高，但它们仍然可能会感到困惑。再次强调，这更多的是视觉编码系统的问题，而不是DRL本身的问题。然而，人们认为，一旦我们能够解决视觉环境的视觉编码问题，真正的AGI（通用人工智能）将无疑会大大接近。
- en: In the next section, we look at a special environment that the team at Unity
    put together (with the help of Google DeepMind) in order to challenge DRL researchers,
    which is the very problem of visual encoding 3D worlds.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨Unity团队（在Google DeepMind的帮助下）为挑战DRL研究人员而构建的一个特殊环境，这正是3D世界视觉编码的问题。
- en: Challenging the Unity Obstacle Tower Challenge
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战Unity障碍塔挑战
- en: 'In late 2018, Unity, with the help of DeepMind, began development of a challenge
    designed to task researchers in the most challenging areas of DRL. The challenge
    was developed with Unity as a Gym interface environment and featured a game using
    a 3D first-person perspective. The 3D perspective is a type of game interface
    made famous with the likes of games such as Tomb Raider and Resident Evil, to
    name just a couple of examples. An example of the game interface is shown in the
    following screenshot:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 到2018年底，Unity在DeepMind的帮助下开始开发一个挑战，旨在让DRL领域最具挑战性的研究人员承担任务。挑战是以Unity作为Gym接口环境开发的，并使用3D第一人称视角的游戏。3D视角是一种游戏界面，如《古墓丽影》和《生化危机》等游戏使其闻名。以下截图显示了游戏界面的一个示例：
- en: '![](img/9134c7f0-ca58-413f-bc3d-fa8cd1de16be.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9134c7f0-ca58-413f-bc3d-fa8cd1de16be.png)'
- en: Example the obstacle tower challenge
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 举例说明障碍塔挑战
- en: The Obstacle Tower Challenge is not only in 3D, but the patterns and materials
    in the rooms and on the walls change over the levels. This makes vision generalization
    even more difficult. Furthermore, the challenge poses multiple concurrent steps
    to complete tasks. That is, each level requires the character to find a door and
    open it. On more advancing levels, the doors require a special key to be activated
    or acquired, which makes this almost a multi-task RL problem—not a problem we
    have considered solving previously. Fortunately, as we demonstrated using ML-Agents
    Curiosity Learning, multi-step RL can be accomplished, provided the tasks are
    linearly connected. This means there is no branching or tasks that require decisions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 障碍塔挑战不仅是在3D中，而且房间和墙壁上的模式和材料会随着关卡的变化而变化。这使得视觉泛化变得更加困难。此外，挑战提出了多个并发步骤来完成任务。也就是说，每个关卡都需要角色找到一扇门并将其打开。在更高级的关卡中，门需要特殊的钥匙才能激活或获取，这使得这几乎成为一个多任务RL问题——这不是我们之前考虑解决的问题。幸运的是，正如我们使用ML-Agents
    Curiosity Learning所展示的，只要任务线性连接，多步RL是可以实现的。这意味着没有分支或需要决策的任务。
- en: Multi-task reinforcement learning is quickly advancing in research but it is
    still a very complicated topic. The current preferred method to solve MTRL is
    called **meta reinforcement learning**. We will cover Meta Reinforcement Learning
    in [Chapter 14](a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml), *From DRL to AGI*,
    where we will talk about the next evolutions of DRL in the coming months and/or
    years.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务强化学习在研究方面迅速发展，但它仍然是一个非常复杂的话题。解决MTRL的当前首选方法被称为**元强化学习**。我们将在[第14章](a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml)“从DRL到AGI”中介绍元强化学习，我们将讨论未来几个月或几年DRL的下一阶段发展。
- en: For the next exercise, we are going to closely review the work of the winner
    of the Unity Obstacle Tower Challenge, Alex Nichol. Alex won the $100,000 challenge
    by entering a modified PPO agent that was pre-trained on classified images and
    human recorded demonstrations (behavioural cloning). He essentially won by better
    generalizing the agent's observations of state using a number of engineered solutions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将仔细审查Unity障碍塔挑战的获胜者Alex Nichol的工作。Alex通过提交一个在分类图像和人类记录的演示（行为克隆）上预训练的修改后的PPO智能体赢得了10万美元的挑战。他实际上是通过使用一系列工程解决方案更好地泛化智能体的状态观察而获胜的。
- en: 'Open up your Anaconda prompt and follow the next example:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 打开你的Anaconda提示符，并按照以下示例操作：
- en: 'It is recommended that you create a new virtual environment before installing
    any new code and environments. This can easily be done with Anaconda using the
    following commands:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建议在安装任何新代码和环境之前创建一个新的虚拟环境。这可以通过Anaconda使用以下命令轻松完成：
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'First, you will need to download and install the Unity Obstacle Tower Challenge
    from this repository ([https://github.com/Unity-Technologies/obstacle-tower-env](https://github.com/Unity-Technologies/obstacle-tower-env))
    or just use the following commands from a new virtual environment:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您需要从以下存储库下载并安装Unity障碍塔挑战([https://github.com/Unity-Technologies/obstacle-tower-env](https://github.com/Unity-Technologies/obstacle-tower-env))，或者只需从新的虚拟环境使用以下命令：
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Running the OTC environment is quite simple and can be done with this simple
    block of code that just performs random actions in the environment:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行OTC环境相当简单，可以使用以下简单的代码块在环境中执行随机动作：
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The code to run the OTC environment should be quite familiar by now, but does
    have one item to note. The agent cycles through episodes or lives, but the agent
    only has a certain number of lives. This environment simulates a real game, and
    hence, the agent only has a limited number of tries and time to complete the challenge.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行OTC环境的代码现在应该相当熟悉了，但有一个需要注意的项目。代理会循环通过回合或生命，但代理只有一定数量的生命。这个环境模拟了一个真实游戏，因此代理只有有限的尝试次数和时间来完成挑战。
- en: Next, pull down the repository from Alex Nichol (`unixpickle`) here: [https://github.com/unixpickle/obs-tower2.git](https://github.com/unixpickle/obs-tower2.git),
    or check the `Chapter_13/obs-tower2` source folder.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，从Alex Nichol（`unixpickle`）这里拉取仓库：[https://github.com/unixpickle/obs-tower2.git](https://github.com/unixpickle/obs-tower2.git)，或者检查`Chapter_13/obs-tower2`源文件夹。
- en: 'Navigate to the folder and run the following command to install the required
    dependencies:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到文件夹并运行以下命令以安装所需的依赖项：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After that, you need to configure some environment variables to the following:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，你需要配置一些环境变量到以下内容：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'How you set these environment variables will depend on your OS and at what
    level you want them set. For Windows users, you can set the environment variable
    using the **System Environment Variables** setup panel as shown here:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你设置这些环境变量的方式将取决于你的操作系统以及你想要设置它们的位置级别。对于Windows用户，你可以使用**系统环境变量**设置面板来设置环境变量，如下所示：
- en: '![](img/75adffed-d9da-4814-ab62-c582c01ed82c.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/75adffed-d9da-4814-ab62-c582c01ed82c.png)'
- en: Setting the environment variables (Windows)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 设置环境变量（Windows）
- en: Now with everything set up, it is time to move on to pre-training the agent.
    We will cover that training in the next section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切都已经设置好了，是时候开始预训练代理了。我们将在下一节中介绍这一训练过程。
- en: Pre-training the agent
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练代理
- en: We have already covered a number of ways to manage training performance often
    caused by low rewards or rewards sparsity. This covered using a technique called
    behavioural cloning, whereby a human demonstrates a set of actions leading to
    a reward and those actions are then fed back into the agent as a pre-trained policy.
    The winning implementation here used a combination of behavioural cloning with
    pre-trained image classification.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了几种管理训练性能的方法，这些方法通常由低奖励或奖励稀疏性引起。这包括使用一种称为行为克隆的技术，其中人类演示一系列导致奖励的动作，然后这些动作作为预训练策略反馈给代理。在这里，获胜的实现是行为克隆与预训练图像分类的组合。
- en: 'We will continue from where we left off in the last exercise and learn what
    steps we need to perform in order to pre-train a classifier first:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从上一个练习结束的地方继续，学习我们需要执行哪些步骤来首先预训练一个分类器：
- en: Firstly, we need to capture images of the environment in order to pre-train
    a classifier. This requires you to run the `record.py` script located at the `obs_tower2/recorder/record.py`
    folder. Make sure when running this script that your environment variables are
    configured correctly.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要捕获环境中的图像以预训练一个分类器。这需要你运行位于`obs_tower2/recorder/record.py`文件夹中的`record.py`脚本。确保在运行此脚本时，你的环境变量配置正确。
- en: The documentation or `README.md` on the repository is good but is only really
    intended for advanced users who are very interested in replicating results. If
    you do encounter issues in this walkthrough, refer back to that documentation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库中的文档或`README.md`是好的，但它主要面向对复制结果非常感兴趣的先进用户。如果你在这次演练中遇到问题，请参考该文档。
- en: Running the script will launch the Unity OTC and allow you as a player to interact
    with the game. As you play the game, the `record.py` script will record your moves
    as images after every episode. You will need to play several games in order to
    have enough training data. Alternatively, Alex has provided a number of recordings
    online at this location: [http://obstower.aqnichol.com/](http://obstower.aqnichol.com/).
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行脚本将启动Unity OTC，并允许你作为玩家与游戏互动。在你玩游戏的过程中，`record.py`脚本会在每个回合结束后记录你的动作图像。你需要玩几场比赛才能收集到足够的训练数据。或者，Alex已经在这个位置上提供了一些在线记录：[http://obstower.aqnichol.com/](http://obstower.aqnichol.com/)。
- en: '**Note:**'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意：**'
- en: The recordings and labels are both in tar files with the recordings weighing
    in at 25 GB.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 记录和标签都在tar文件中，记录文件大小为25 GB。
- en: Next, we need to label the recorded images in order to assist the agent in classification.
    Locate and run the `main.py` script located in the `obs_tower2/labeler/` folder.
    This will launch a web application. As long as you have your paths set correctly,
    you can now open a browser and go to `http://127.0.0.1:5000` (localhost, port
    `5000`).
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要对记录的图像进行标注，以帮助智能体进行分类。找到并运行位于`obs_tower2/labeler/`文件夹中的`main.py`脚本。这将启动一个网络应用程序。只要你的路径设置正确，你现在就可以打开浏览器并访问`http://127.0.0.1:5000`（本地主机，端口`5000`）。
- en: 'You will now be prompted to label images by using the web interface. For each
    image, classify the state as shown in the following screenshot:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在将通过网页界面被提示对图像进行标注。对于每张图像，按照以下截图所示进行分类：
- en: '![](img/4863d210-ccdd-417c-a78f-d796b60ecab9.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4863d210-ccdd-417c-a78f-d796b60ecab9.png)'
- en: Labeling image data for classification
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为分类标注图像数据
- en: Alex notes in his original documentation that he could label 20-40 images per
    second after some practice. Again, if you want to avoid this step, just download
    the tar files containing his example recordings and labels.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 亚历克斯在他的原始文档中提到，经过一些练习后，他可以每秒标注20-40张图像。再次提醒，如果你想避免这一步骤，只需下载包含他示例录音和标签的tar文件。
- en: 'Next, you will need to run that classifier with the training input images and
    labels you either just generated or downloaded. Run the classified by executing
    the following commands:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你需要运行该分类器，使用你刚刚生成或下载的训练输入图像和标签。通过执行以下命令来运行分类器：
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: After the classification is done, the results will be output to a `save_classifier.pk1` file
    periodically. The whole process may take several hours to train completely.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类完成后，结果将定期输出到`save_classifier.pk1`文件。整个过程可能需要几个小时才能完全训练完成。
- en: 'With the pre-classifier built, we can move to behavioral cloning using the
    human sample playing. This means you will used the saved and pre-labelled sessions
    as inputs for later agent training. You can start the process by running the following:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建了预分类器之后，我们可以使用人类样本进行行为克隆。这意味着你将使用保存并预先标注的会话作为后续智能体训练的输入。你可以通过运行以下命令来启动此过程：
- en: '[PRE16]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Running this script generates periodic output to a `save_clone.pkl` file and
    the whole script can take a day or more to run. When the script is complete, copy
    the output to a `save_prior.pkl` file like so:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行此脚本会定期将输出生成到`save_clone.pkl`文件，整个脚本可能需要一天或更长时间才能运行。当脚本完成后，将输出复制到`save_prior.pkl`文件，如下所示：
- en: '[PRE17]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This creates a prior set of recordings or memories we will use to train the
    agent in the next section.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个先验记录集或记忆集，我们将在下一节中用它来训练智能体。
- en: Prierarchy – implicit hierarchies
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次化 - 隐式层次
- en: 'Alex used the notion of hierarchical reinforcement learning in order to tackle
    the problem of multi-task agent learning that OTC requires you to solve. HRL is
    another method outside Meta-RL that has been used to successfully solve multi-task
    problems. Prierarchy-RL refines this by building a prior hierarchy that allows
    an action or action-state to be defined by entropy or uncertainty. High entropy
    or highly uncertain actions become high level or top-based actions. This is someone
    abstract in concept, so let''s look at a code example to see how this comes together:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 亚历克斯使用分层强化学习的概念来解决OTC要求你解决的多个任务智能体学习问题。HRL是Meta-RL之外另一种用于成功解决多任务问题的方法。Prierarchy-RL通过构建一个先验层次结构来改进这一点，允许通过熵或不确定性定义动作或动作状态。高熵或高度不确定的动作成为高级或基于顶部的动作。这个概念有些抽象，所以让我们通过一个代码示例来看看它是如何结合在一起的：
- en: 'The base agent used to win the challenge was PPO; following is a full source
    listing of that agent and a refresher to PPO:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于赢得挑战的基础智能体是PPO；以下是该智能体的完整源代码列表以及PPO的复习：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Familiarize yourself with the differences between this implementation and what
    we covered for PPO. Our example was simplified for explanation purposes but follows
    the same patterns.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 熟悉这个实现与我们在PPO中讨论的内容之间的区别。我们的示例为了解释目的而简化，但遵循相同的模式。
- en: 'Pay particular attention to the code in `inner_loop` and understand how this
    works:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特别注意`inner_loop`中的代码，并理解其工作原理：
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Open the `prierarchy.py` file located in the root `obs_tower2` folder and as
    shown here:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于根目录`obs_tower2`文件夹中的`prierarchy.py`文件，如下所示：
- en: '[PRE20]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: What we see here is the `Pierarchy` class, an extension to `PPO`, which works
    by extending the `inner_loop` function. Simply, this code refines the KL-Divergence
    calculation that allowed us to secure that spot on the hill without falling off.
    Recall this was our discussion of the clipped objective function.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里看到的是`Hierarchy`类，它是`PPO`的扩展，通过扩展`inner_loop`函数来工作。简单来说，这段代码优化了KL-Divergence计算，使我们能够在山丘上稳固地占据位置而不会掉落。回想一下，这是我们关于裁剪目标函数的讨论。
- en: 'Notice the use of the `prior` policy or the policy that was generated based
    on the pre-training and behavioral cloning done earlier. This prior policy defines
    if actions are high or low in uncertainty. That way, an agent can actually use
    the prior hierarchy or prierarchy to select a series of high and then lower entropy/uncertain
    actions.  The following diagram illustrates how this effectively works:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意到使用了`prior`策略或基于先前预训练和行为克隆生成的策略。这个先验策略定义了动作是高不确定性还是低不确定性。这样，代理实际上可以使用先验层次结构或先验结构来选择一系列高熵/不确定的动作。以下图表说明了这种有效的工作方式：
- en: '![](img/f367f2af-2a1b-45e6-973f-b8dd7cf7a3bc.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f367f2af-2a1b-45e6-973f-b8dd7cf7a3bc.png)'
- en: Agent selecting action based on entropy hierarchy
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 基于熵层次结构选择动作的代理
- en: Thus, instead of deciding when and if to explore, the agent decides random actions
    based on their hierarchy or uncertainty. This means that higher-level actions
    can be reduced in uncertainty quickly because each successive action has less
    uncertainty.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，代理不再决定何时以及是否进行探索，而是根据其层次结构或不确定性来决定随机动作。这意味着高级动作可以快速减少不确定性，因为每个后续动作的不确定性都更小。
- en: A helpful example when trying to understand Priercarchy is the movie *Groundhog
    Day*, starring Bill Murray. In the movie, the character continually cycles through
    the same day, attempting by trial and error to find the optimum path to break
    out of the path. In the movie, we can see the character try thousands, perhaps
    millions, of different combinations, but we see this done in hierarchical steps.
    We first see the character wildly going about his day never accomplishing anything,
    until he learns through past hierarchical actions what are the best possible rewards.
    He learns that by improving on himself, his time in eternity becomes more pleasant.
    In the end, we see the character try to live their best life, only to discover
    they solved the game and can move on to the next day.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当试图理解层级结构时，一个有帮助的例子是电影《 Groundhog Day》，由比尔·默瑞主演。在电影中，这个角色不断地经历同一天，通过尝试和错误来寻找突破路径的最佳途径。在电影中，我们可以看到这个角色尝试了数千次，甚至数百万次不同的组合，但我们看到这是以层次步骤完成的。我们首先看到这个角色在一天中疯狂地四处走动，却什么也没做成，直到他通过过去的层次动作学会了最佳可能的奖励。他意识到，通过自我提升，他在永恒中的时间变得更加愉快。最后，我们看到这个角色试图过上最好的生活，却发现他们解决了游戏，可以进入下一天。
- en: 'You can train the agent by running the following command on the first 10 levels:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以通过在第一层的前10层运行以下命令来训练代理：
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, to train the agent on floors greater than 10, you can use the following:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，为了在超过10层的楼层上训练代理，你可以使用以下方法：
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Every 10 levels in the OTC, the game theme changes. This means the wall color
    and textures will change as well as the tasks that need to get completed. As we
    mentioned earlier, this visual change, combined with 3D, will make the Unity OTC
    one of the most difficult and benchmark challenges to beat when we first get smart/bold
    and/or brave enough to tackle AGI. AGI and the road to more general intelligence
    with DRL will be our focus for [Chapter 14](a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml),
    *From DRL to AGI*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在OTC的每10层，游戏主题都会改变。这意味着墙壁颜色和纹理也会改变，以及需要完成的任务。正如我们之前提到的，这种视觉变化，加上3D，将使Unity OTC成为我们在首次变得足够聪明/大胆和/或勇敢地应对AGI时，最具挑战性和基准挑战之一。AGI和通过DRL走向更普遍智能的道路将是我们在第14章[从DRL到AGI](a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml)的关注点。
- en: In the next section, we look at 3D world Habitat by Facebook, which is more
    difficult but equally fun.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨Facebook的3D世界Habitat，这更具挑战性但同样有趣。
- en: Exploring Habitat – embodied agents by FAIR
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索栖息地 - 由FAIR的具身代理
- en: Habitat is a relatively new entry by Facebook AI Research for a new form of
    embodied agents. This platform represents the ability to represent full 3D worlds
    displayed from real-world complex scenes. The environment is intended for AI research
    of robots and robotic-like applications that DRL will likely power in the coming
    years. To be fair though, pun intended, this environment is implemented to train
    all forms of AI on this type of environment. The current Habitat repository only
    features some simple examples and implementation of PPO.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Habitat是Facebook AI Research为新型具身智能体提出的一个相对较新的平台。这个平台代表了能够从真实世界的复杂场景中展示全3D世界的能力。该环境旨在为机器人及其类似应用提供AI研究，这些应用在未来几年可能会由DRL（深度强化学习）提供动力。公平地说，这个环境是为了训练所有类型的AI在这个环境中而实现的。当前的Habitat仓库只包含一些简单的示例和PPO的实现。
- en: 'The Habitat platform comes in two pieces: the Habitat Sim and Habitat API.
    The simulation environment is a full 3D powered world that can render at thousands
    of frames per second, which is powered by photogrammetry RGBD data. RGBD is essentially
    RGB color data plus depth. Therefore, any image taken will have a color value
    and depth. This allows the data to be mapped in 3D as a hyper-realistic representation
    of the real environment. You can explore what one of these environments look like
    by using Habitat itself in your browser by following the next quick exercise:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Habitat平台由两部分组成：Habitat Sim和Habitat API。模拟环境是一个全3D的强大世界，可以每秒渲染数千帧，由摄影测量RGBD数据驱动。RGBD本质上是由RGB颜色数据和深度数据组成。因此，任何图像都会有一个颜色值和深度值。这使得数据可以以超现实的方式在3D中映射，成为真实环境的超逼真表示。您可以通过在浏览器中使用Habitat本身来探索这些环境的外观，请按照下一个快速练习进行操作：
- en: Navigate your browser to [https://aihabitat.org/demo/.](https://aihabitat.org/demo/)
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的浏览器导航到[https://aihabitat.org/demo/](https://aihabitat.org/demo/)。
- en: Habitat will currently only run in Chrome or on your desktop.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Habitat目前只能在Chrome浏览器或您的桌面上运行。
- en: 'It may take some time to load the app so be patient. When the app is loaded,
    you will see something like the following screenshot:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载应用程序可能需要一些时间，请耐心等待。当应用程序加载完成后，您将看到以下截图类似的内容：
- en: '![](img/ebbec784-c61d-4fa2-aa67-1cca633cf1d8.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ebbec784-c61d-4fa2-aa67-1cca633cf1d8.png)'
- en: Example of Habitat running in the browser
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Habitat在浏览器中运行的示例
- en: Use the WASD keys to move around in the environment.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用WASD键在环境中移动。
- en: Habitat supports importing from the following three vendors: [MatterPort3D](https://niessner.github.io/Matterport/),
    [Gibson](http://gibsonenv.stanford.edu/database/), and [Replica](https://github.com/facebookresearch/Replica-Dataset),
    who produce tools and utilities to capture RGBD data and have libraries of this
    data. Now that we understand what Habitat is, we will set it up in the next section.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Habitat支持从以下三个供应商导入：[MatterPort3D](https://niessner.github.io/Matterport/)、[Gibson](http://gibsonenv.stanford.edu/database/)和[Replica](https://github.com/facebookresearch/Replica-Dataset)，他们提供捕获RGBD数据的工具和实用程序，并拥有这些数据库。现在我们了解了Habitat是什么，我们将在下一节中设置它。
- en: Installing Habitat
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Habitat
- en: 'At the time of writing, Habitat was still a new product, but the documentation
    worked well to painlessly install and run an agent for training. In our next exercise,
    we walk through parts of that documentation to install and run a training agent
    in Habitat:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Habitat仍然是一个新产品，但文档工作得很好，可以轻松安装和运行用于训练的智能体。在我们的下一个练习中，我们将介绍该文档的部分内容，以在Habitat中安装和运行训练智能体：
- en: 'Open an Anaconda command prompt and navigate to a clean folder. Use the following
    commands to download and install the Habitat:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Anaconda命令提示符并导航到一个干净的文件夹。使用以下命令下载和安装Habitat：
- en: '[PRE23]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, create a new virtual environment and install the required dependencies
    with the following:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建一个新的虚拟环境，并使用以下命令安装所需的依赖项：
- en: '[PRE24]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we need to build the Habitat Sim with the following:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用以下命令构建Habitat Sim：
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Download the test scenes from the following link: [http://dl.fbaipublicfiles.com/habitat/habitat-test-scenes.zip](http://dl.fbaipublicfiles.com/habitat/habitat-test-scenes.zip).'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下链接下载测试场景：[http://dl.fbaipublicfiles.com/habitat/habitat-test-scenes.zip](http://dl.fbaipublicfiles.com/habitat/habitat-test-scenes.zip)。
- en: Unzip the scene files into a familiar path, one that you can link to later.
    These files are sets of RGBD data that represent the scenes.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将场景文件解压缩到熟悉的路径，一个您可以稍后链接到的路径。这些文件是代表场景的RGBD数据集。
- en: RGBD image capture is not new, and traditionally, it has been expensive since
    it requires moving a camera equipped with a special sensor around a room. Thankfully,
    most modern cell phones also feature this depth sensor. This depth sensor is often
    used to build augmented reality applications now. Perhaps in a few years, agents
    themselves will be trained to capture these types of images using just a simple
    cell phone.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: RGBD图像捕捉并不新鲜，传统上，它很昂贵，因为它需要移动一个装有特殊传感器的摄像头在房间内移动。幸运的是，大多数现代智能手机也具备这种深度传感器。这个深度传感器现在常用于构建增强现实应用。也许在几年后，智能体本身将被训练，仅使用简单的手机就能捕捉这些类型的图像。
- en: 'After everything is installed, we can test the Habitat installation by running
    the following command:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，我们可以通过运行以下命令来测试Habitat的安装：
- en: '[PRE26]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: That will launch the Sim in non-interactive fashion and play some random moves.
    If you want to see or interact with the environment, you will need to download
    and install the interactive plugin found in the repository documentation.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将以非交互方式启动模拟并执行一些随机移动。如果您想查看或与环境交互，您将需要下载并安装存储库文档中找到的交互式插件。
- en: After the Sim is installed, we can move on to installing the API and training
    an agent in the next section.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟安装完成后，我们可以在下一节中继续安装API并训练智能体。
- en: Training in Habitat
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Habitat中进行训练
- en: 'At the time of writing, Habitat was quite new but showed amazing potential,
    especially for training agents. This means the environment currently only has
    a simple and PPO agent implementation in which you can quickly train agents. Of
    course, since Habitat uses PyTorch, you could probably implement one of the other
    algorithms we have covered. In the next exercise, we finish off by looking at
    the PPO implementation in Habitat and how to run it:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Habitat相当新颖，但显示出巨大的潜力，尤其是在训练智能体方面。这意味着当前环境中只有简单的PPO智能体实现，您可以快速训练智能体。当然，由于Habitat使用PyTorch，您可能能够实现我们之前介绍的其他算法之一。在下一项练习中，我们将通过查看Habitat中的PPO实现及其运行方式来结束：
- en: 'Download and install the Habitat API with the following commands:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令下载并安装Habitat API：
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'At this point, you can use the API in a number of ways. We will first look
    at a basic code example you could write to run the Sim:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，您可以使用多种方式使用API。我们首先将查看一个基本的代码示例，您可以用它来运行模拟：
- en: '[PRE28]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As you can see, the Sim allows us to program an agent using the same familiar
    Gym style interface we are used to.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，模拟允许我们使用我们熟悉的Gym风格界面来编程智能体。
- en: Next, we need to install the Habitat Baselines package. This package is the
    RL portion and currently provides an example of PPO. The package is named Baselines
    after the OpenAI testing package of the same name.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要安装Habitat Baselines包。这个包是强化学习部分，目前提供了一个PPO的示例。这个包以OpenAI同名测试包命名。
- en: 'Install the Habitat Baselines package using the following commands:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装Habitat Baselines包：
- en: '[PRE29]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After the installation, you can run the `run.py` script in order to train an
    agent with the following command:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，您可以通过运行以下命令来运行`run.py`脚本来训练一个智能体：
- en: '[PRE30]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, you can test this agent with the following command:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以使用以下命令测试这个智能体：
- en: '[PRE31]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Habitat is a fairly recent development and opens the door to training agents/robots
    in real-world environments. While Unity and ML-Agents are great platforms for
    training agents in 3D game environments, they still do not compare to the complexity
    of the real world. In the real world, objects are rarely perfect and are often
    very complex, which makes these environments especially difficult to generalize,
    and therefore, train on. In the next section, we finish the chapter with our typical
    exercises.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Habitat是一个相对较新的发展，为在真实世界环境中训练智能体/机器人打开了大门。虽然Unity和ML-Agents是训练3D游戏环境中智能体的优秀平台，但它们仍然无法与真实世界的复杂性相比。在真实世界中，物体很少完美，通常是复杂的，这使得这些环境特别难以泛化，因此难以训练。在下一节中，我们通过典型的练习来结束这一章。
- en: Exercises
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'As we progressed through this book, the exercises have morphed from learning
    exercises to almost research efforts, and that is the case in this chapter. Therefore,
    the exercises in this chapter are meant for the hardcore RL enthusiast and may
    not be for everyone:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们通过这本书的进展，练习已经从学习练习转变为几乎接近研究工作，这一章也是如此。因此，这一章的练习是为那些热衷于强化学习的人准备的，可能并不适合每个人：
- en: Tune the hyperparameters for one of the sample visual environments in the ML-Agents
    toolkit.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整ML-Agents工具包中一个样本视觉环境的超参数。
- en: Modify the visual observation standard encoder found in the ML-Agents toolkit
    to include additional layers or different kernel filter settings.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改ML-Agents工具包中找到的视觉观察标准编码器，以包括额外的层或不同的内核滤波器设置。
- en: Train an agent with `nature_cnn` or `resnet` visual encoder networks and compare
    their performance with earlier examples using the base visual encoder.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nature_cnn`或`resnet`视觉编码器网络训练一个代理，并将它们的性能与使用基础视觉编码器的早期示例进行比较。
- en: Modify the `resnet` visual encoder to accommodate many more layers or other
    variations of filter/kernel size.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改`resnet`视觉编码器，以适应更多层或其他滤波器/内核尺寸的变化。
- en: Download, install, and play the Unity Obstacle Tower Challenge and see how far
    you can get in the game. As you play, think of yourself as an agent and reflect
    on what actions you are taking and how they reflect your current task trajectory.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载、安装并玩Unity Obstacle Tower Challenge，看看你在游戏中能走多远。在玩游戏的同时，将自己视为一个代理，反思你所采取的行动以及它们如何反映你当前的任务轨迹。
- en: Build your own implementation of an algorithm to test against the Unity OTC.
    Completing this challenge will be especially rewarding if you beat the results
    of the previous winner. This challenge is still somewhat open and anyone claiming
    to do higher than level 20 will probably make a big impact on DRL in the future.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建自己的算法实现，以测试Unity OTC。如果你打败了前一名获胜者的结果，完成这个挑战将特别有成就感。这个挑战仍然相对开放，任何声称达到20级以上的人可能会对未来DRL产生重大影响。
- en: Replace the PPO base example in the Habitat Baselines module with an implementation
    of Rainbow DQN. How does the performance compare?
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Habitat Baselines模块中的PPO基础示例替换为Rainbow DQN的实现。性能比较如何？
- en: Implement a different visual encoder for the Habitat Baselines framework. Perhaps
    use the previous examples of `nature_cnn` or `resnet`.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Habitat Baselines框架实现一个不同的视觉编码器。也许可以使用`nature_cnn`或`resnet`的先前示例。
- en: Compete in the Habitat Challenge. This is a challenge that requires an agent
    to complete a navigation task through a series of waypoints. It's certainly not
    as difficult as the OTC, but the visual environment is far more complex.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参加Habitat挑战。这是一个要求代理通过一系列航点完成导航任务的挑战。这当然不像OTC那么困难，但视觉环境要复杂得多。
- en: Habitat is intended more for sensor development instead of visual development.
    See if you are able to combine visual observation encoding with other sensor input
    as a type of combined visual and sensor observation input.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Habitat更适用于传感器开发而不是视觉开发。看看你是否能够将视觉观察编码与其他传感器输入相结合，作为视觉和传感器观察输入的组合。
- en: The exercises in this chapter are intended to be entirely optional; please choose
    to do these only if you have a reason to do so. They likely will require additional
    time as this is a very complex area to develop in.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的练习完全是可选的；请仅在您有理由这样做的情况下选择进行这些练习。由于这是一个非常复杂的领域，因此它们可能需要额外的时间来开发。
- en: Summary
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the concept of 3D worlds for not only games but
    the real world. The real world, and to a greater extent 3D worlds, are the next
    great frontier in DRL research. We looked at why 3D creates nuances for DRL that
    we haven't quite figured out how best to solve. Then, we looked at using 2D visual
    observation encoders but tuned for 3D spaces, with variations in the Nature CNN
    and ResNet or residual networks. After that, we looked at the Unity Obstacle Tower
    Challenge, which challenged developers to build an agent capable of solving the
    3D multi-task environment.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了3D世界的概念，不仅限于游戏，还包括现实世界。现实世界，以及更大程度上是3D世界，是DRL研究的下一个伟大前沿。我们探讨了为什么3D为DRL创造了我们尚未完全找到最佳解决方案的细微差别。然后，我们研究了使用针对3D空间调整的2D视觉观察编码器，包括Nature
    CNN和ResNet或残差网络的变化。之后，我们研究了Unity Obstacle Tower Challenge，这个挑战要求开发者构建一个能够解决3D多任务环境的代理。
- en: From there, we looked at the winning entries use of Prierarchy; a form of HRL
    in order to manage multiple task spaces. We also looked at the code in detail
    to see how this reflected in the winners modified PPO implementation. Lastly,
    we finished the chapter by looking at Habitat; an advanced AI environment that
    uses RGBD and depth based color data, to render real-world environments in 3D.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，我们研究了获胜条目使用Prierarchy；一种用于管理多个任务空间的HRL形式。我们还详细研究了代码，以了解这如何反映在获胜者修改的PPO实现中。最后，我们通过研究Habitat来结束本章；这是一个高级AI环境，使用RGBD和基于颜色的深度数据，以3D形式渲染现实世界环境。
- en: We are almost done with our journey, and in the next and final chapter, we will
    look at how DRL is moving toward artificial general intelligence, or what we refer
    to as AGI.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎完成了这次旅程，在下一章和最后一章中，我们将探讨深度强化学习（DRL）是如何迈向通用人工智能，或者我们称之为AGI的方向。
