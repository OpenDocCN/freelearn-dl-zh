- en: Transfer Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: In the previous chapter, we learned about recognizing the class that an image
    belongs to in a given image. In this chapter, we will learn about one of the drawbacks
    of CNN and also about how we can overcome it using certain pre-trained models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何识别图像属于哪个类别。本章将介绍CNN的一个缺点，以及如何通过使用某些预训练模型来克服这一问题。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Gender classification of a person in an image using CNNs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CNN进行图像中人物的性别分类
- en: Gender classification of a person in image using the VGG16 architecture-based
    model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于VGG16架构的模型进行图像中人物的性别分类
- en: Visualizing the output of the intermediate layers of a neural network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化神经网络中间层的输出
- en: Gender classification of a person in image using the VGG19 architecture-based
    model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于VGG19架构的模型进行图像中人物的性别分类
- en: Gender classification of a using the ResNet architecture-based model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于ResNet架构的模型进行性别分类
- en: Gender classification of a using the inception architecture-based model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于Inception架构的模型进行性别分类
- en: Detecting the key points within image of a face
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测人脸图像中的关键点
- en: Gender classification of the person in an image using CNNs
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络（CNN）进行图像中人物的性别分类
- en: To understand some of the limitations of CNNs, let's go through an example where
    we try to identify whether the given image contains the image of a cat or a dog.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解CNN的一些局限性，让我们通过一个示例来识别给定图像是猫还是狗。
- en: Getting ready
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'We will gain an intuition of how a CNN predicts the class of object present
    in the image through the following steps:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下步骤，我们将直观地了解卷积神经网络如何预测图像中物体的类别：
- en: 'A convolution filter is activated by certain parts of the image:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积滤波器由图像的某些部分激活：
- en: For example, certain filters might activate if the image has a certain pattern—it
    contains a circular structure, for example
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，某些滤波器可能会在图像具有某种模式时激活——例如，图像包含一个圆形结构
- en: 'A pooling layer ensures that image translation is taken care of:'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层确保图像平移问题得以处理：
- en: This ensures that even if an image is big, over an increased number of pooling
    operations, the size of the image becomes small and the object can then be detected
    as the object is now expected to be in the smaller portion of the image (as it
    is pooled multiple times)
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这确保了即使图像较大，通过多次池化操作，图像的大小变小，物体也可以被检测到，因为物体现在应该出现在图像的较小部分（因为它已经多次池化）
- en: The final flatten layer flattens all the patterns that are extracted by various
    convolution and pooling operations
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终的扁平层将所有通过不同卷积和池化操作提取的特征进行扁平化
- en: Let's impose a scenario where the number of images in a training dataset is
    small. In such a case, the model does not have enough data points for it to generalize
    on a test dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设训练数据集中的图像数量很少。在这种情况下，模型没有足够的数据点来对测试数据集进行泛化。
- en: Additionally, given that the convolutions are learning various features from
    scratch, it could potentially take many epochs before the model starts to fit
    on top of the training dataset if the training dataset contains images that have
    a large shape (width and height).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑到卷积从头开始学习各种特征，如果训练数据集中的图像具有较大的形状（宽度和高度），则可能需要多个周期才能让模型开始适应训练数据集。
- en: 'Hence, in the next section, we will code the following scenario of building
    a CNN, where there are a few images (~1,700 images) and test the accuracy on different
    shapes of images:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在下一部分，我们将编写以下构建CNN的情景代码，其中包含一些图像（大约1,700张图像），并测试不同形状图像的准确性：
- en: Accuracy in 10 epochs where the image size is 300 X 300
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在10个周期中，当图像尺寸为300 X 300时的准确性
- en: Accuracy in 10 epochs where the image size is 50 X 50
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在10个周期中，当图像尺寸为50 X 50时的准确性
- en: How to do it...
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到……
- en: In this section, we will fetch a dataset and perform classification analysis
    where the image size in one scenario is 300 x 300, while in the other scenario,
    it is 50 x 50. (Please refer to `Transfer_learning.ipynb` file in GitHub while
    implementing the code.)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分中，我们将获取一个数据集并进行分类分析，其中一个情景的图像尺寸为300 x 300，而另一个情景的图像尺寸为50 x 50。（在实现代码时，请参考GitHub上的`Transfer_learning.ipynb`文件。）
- en: Scenario 1 – big images
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情景1——大图像
- en: 'Fetch the dataset. For this analysis, we will continue with the male versus
    female classification dataset that we have downloaded in the Gender classification
    case study in [Chapter 4](750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml), *Building
    a Deep Convolutional Neural Network*:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取数据集。对于此分析，我们将继续使用在[第 4 章](750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml)的性别分类案例研究中下载的男性与女性分类数据集，*构建深度卷积神经网络*：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Extract the image paths and then prepare the input and output data:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取图像路径，然后准备输入和输出数据：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A sample of the images is as follows:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是图像的示例：
- en: '![](img/45f228ee-b2bc-43e1-af97-beb22587efb2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45f228ee-b2bc-43e1-af97-beb22587efb2.png)'
- en: Note that all the images are 300 x 300 x 3 in size.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有图像的大小为 300 x 300 x 3。
- en: 'Create the input and output dataset arrays:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数据集数组：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding step, we are looping through all the images (one at a time),
    reading the image into an array (we could have gotten away without this step in
    this iteration. However, in the next scenario of resizing the image, we will resize
    images in this step). Additionally, we are storing the labels of each image.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步中，我们遍历了所有图像（逐一进行），将图像读取到一个数组中（在这次迭代中其实可以跳过此步骤。然而，在下一个调整图像大小的场景中，我们将在此步骤调整图像大小）。此外，我们还存储了每个图像的标签。
- en: 'Prepare the input array so that it can be passed to a CNN. Additionally, prepare
    the output array:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备输入数组，以便可以传递给 CNN。此外，准备输出数组：
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we are converting the list of arrays into a numpy array so that it can
    then be passed to the neural network.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将数组列表转换为 numpy 数组，以便将其传递给神经网络。
- en: 'Scale the input array and create input and output arrays:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放输入数组并创建输入和输出数组：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create train and test datasets:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练和测试数据集：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define the model and compile it:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型并编译它：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding code, we are building a model that has multiple layers of convolution,
    pooling, and dropout. Furthermore, we are passing the output of final dropout
    through a flattening layer and then connecting the flattened output to a 512 node
    hidden layer before connecting the hidden layer to the output layer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们构建了一个模型，其中包含多个卷积层、池化层和 dropout 层。此外，我们将最终 dropout 层的输出传递到一个展平层，然后将展平后的输出连接到一个
    512 节点的隐藏层，最后将隐藏层连接到输出层。
- en: 'A summary of the model is as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要如下：
- en: '![](img/d240df6a-7253-411a-bfd8-ee5b457bb19b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d240df6a-7253-411a-bfd8-ee5b457bb19b.png)'
- en: 'In the following code, we are compiling the model to reduce binary cross entropy
    loss, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们编译模型以减少二进制交叉熵损失，如下所示：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Fit the model:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the preceding step, you can see that the model does not train over increasing
    epochs, as shown in the following graph (the code for this diagram is the same
    as we saw in the *Scaling input data* section in [Chapter 2](2846fd6b-a2c2-4ebe-b36a-17f061b4cfa4.xhtml), *Building
    a Deep Feedforward Neural Network*, and it can be found in the GitHub repository
    of this chapter):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步中，你可以看到，随着训练轮次的增加，模型并没有继续训练，如下图所示（此图的代码与我们在[第 2 章](2846fd6b-a2c2-4ebe-b36a-17f061b4cfa4.xhtml)的*缩放输入数据*部分看到的代码相同，并且可以在本章的
    GitHub 仓库中找到）：
- en: '![](img/d79e7aad-4074-4f56-a06d-feaf5337835d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d79e7aad-4074-4f56-a06d-feaf5337835d.png)'
- en: In the preceding graph, you can see that the model hardly learned anything,
    as the loss did not vary much. Also, the accuracy was stuck near the 51% mark
    (which is roughly the distribution of male versus female images in the original
    dataset).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，你可以看到模型几乎没有学习到任何东西，因为损失几乎没有变化。而且，准确率卡在了 51% 左右（这大约是原始数据集中男性与女性图像的分布比例）。
- en: Scenario 2 – smaller images
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景 2 – 较小的图像
- en: 'In this scenario, we will modify the following in the model:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，我们将在模型中做如下修改：
- en: 'Input image size:'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入图像大小：
- en: We will reduce the size from 300 X 300 to 50 X 50
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将把大小从 300 x 300 缩小到 50 x 50
- en: 'Model architecture:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型架构：
- en: The structure of the architecture remains the same as what we saw in **Scenario
    1 – big images**
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构的结构与我们在**场景 1 – 大图像**中看到的相同
- en: 'Create a dataset with the input of the reduced image size (50 X 50 X 3) and
    output labels. For this, we will continue from *step 4* of Scenario 1:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个数据集，输入为调整后图像大小（50 x 50 x 3），输出为标签。为此，我们将继续从场景 1 的*第 4 步*开始：
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Create the input and output arrays for the train, test datasets:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练和测试数据集的输入和输出数组：
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Build and compile the model:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建和编译模型：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'A summary of the model is as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要如下：
- en: '![](img/55212a46-0138-4e44-9c56-04382a0bad47.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55212a46-0138-4e44-9c56-04382a0bad47.png)'
- en: 'Fit the model:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The accuracy and loss of the model training across train and test datasets
    over increasing epochs is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在训练和测试数据集上随着训练轮次的增加，准确率和损失情况如下：
- en: '![](img/46cf59b8-b7ce-4522-8730-c564a321597a.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46cf59b8-b7ce-4522-8730-c564a321597a.png)'
- en: Note that, while the accuracy increased and the loss decreased steadily in both
    the training and test datasets initially, over increasing epochs, the model started
    to overfit (specialize) on training data and had an accuracy of ~76% on the test
    dataset.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然在最初的训练和测试数据集上，准确率有所提高且损失逐渐下降，但随着训练轮次的增加，模型开始在训练数据上过拟合（专注），并且在测试数据集上的准确率为约76%。
- en: From this, we can see that the CNN works when the input size is small and thus
    the filters had to learn from a smaller portion of the image. However, as, the
    image size increased, the CNN had a tough time learning.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从中我们可以看到，当输入大小较小且卷积核必须从图像的较小部分学习时，CNN能正常工作。然而，随着图像大小的增加，CNN在学习上遇到了困难。
- en: Given that we have discovered that the image size has an impact on model accuracy,
    in the new scenario, let's use aggressive pooling to ensure that the bigger image
    (300 x 300 shape) reduces to a smaller one quickly.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们已经发现图像大小对模型准确率有影响，在新的场景中，我们将使用激进的池化，以确保较大的图像（300 x 300形状）能迅速缩小。
- en: Scenario 3 – aggressive pooling on big images
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 场景3 – 对大图像进行激进的池化
- en: In the following code, we will retain the analysis we have done until step 6
    in Scenario 1\. However, the only change will be the model architecture; in the
    following model architecture, we have more aggressive pooling than what we used
    in Scenario 1.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们将保留在场景1中执行到第6步的分析。然而，唯一的变化将是模型架构；在接下来的模型架构中，我们使用了比场景1中更激进的池化方法。
- en: 'In the following architecture, having a bigger window of pooling in each layer
    ensures that we capture the activations in a larger area compared to the scenario
    of having lower pool sizes. The architecture of the model is as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下架构中，每一层具有更大的池化窗口，确保我们能够捕捉到比使用较小池化大小时更大的区域的激活。模型架构如下：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Note that in this architecture, the pool size is 3 x 3 and not 2 x 2, as we
    had in the previous scenario:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个架构中，池化大小是3 x 3，而不是我们在先前场景中使用的2 x 2：
- en: '![](img/754dcf89-b23c-4d37-adec-f09749e89b81.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/754dcf89-b23c-4d37-adec-f09749e89b81.png)'
- en: 'Once we fit a model on the input and output arrays, the variation of accuracy
    and loss on the train and test datasets is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在输入和输出数组上拟合模型，训练和测试数据集上准确率和损失的变化如下：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following is the output of the preceding code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码的输出结果：
- en: '![](img/97fe1ba4-eb37-476a-9328-3b3f225d9952.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97fe1ba4-eb37-476a-9328-3b3f225d9952.png)'
- en: We can see that the test data has ~70% accuracy in correctly classifying gender
    in images.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，测试数据在正确分类图像中的性别时，准确率约为70%。
- en: However, you can see that there is a considerable amount of overfitting on top
    of the training dataset (as the loss decreases steadily on the training dataset,
    while not on the test dataset).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你可以看到，在训练数据集上存在相当大的过拟合现象（因为训练数据集上的损失持续下降，而测试数据集上并未出现类似下降）。
- en: Gender classification of the person in image using the VGG16 architecture-based
    model
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于VGG16架构的模型对图像中人的性别进行分类
- en: 'In the previous section on gender classification using CNN, we saw that when
    we build a CNN model from scratch, we could encounter some of the following scenarios:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分的基于CNN的性别分类中，我们看到，当我们从头开始构建CNN模型时，可能会遇到以下一些情况：
- en: The number of images that were passed is not sufficient for the model to learn
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传递的图像数量不足以让模型学习
- en: Convolutions might not be learning all the features in our images when the images
    are big in size
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当图像尺寸较大时，卷积层可能无法学习到我们图像中的所有特征
- en: The first problem could be tackled by performing our analysis on a large dataset.
    The second one could be tackled by training a larger network on the larger dataset
    for a longer number of epochs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题可以通过在大数据集上执行我们的分析来解决。第二个问题可以通过在较大数据集上训练更大的网络，并进行更长时间的训练来解决。
- en: However, while we are able to perform all of this, more often than not, we do
    not have the amount of data that is needed to perform such an analysis. Transfer
    learning using pre-trained models comes to the rescue in such scenarios.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，虽然我们能够执行所有这些操作，但往往缺乏进行此类分析所需的数据量。使用预训练模型进行迁移学习可以在这种情况下提供帮助。
- en: ImageNet is a popular competition where participants are asked to predict the
    various classes of an image, where the images are of various sizes and also contain
    multiple classes of objects.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet是一个流行的竞赛，参与者需要预测图像的不同类别，图像的大小各异，并且包含多个类别的对象。
- en: There were multiple research teams that competed in this competition to come
    up with a model that is able to predict images of multiple classes where there
    are millions of images in a dataset. Given that there were millions of images,
    the first problem of a limited dataset is resolved. Additionally, given the huge
    networks the research teams have built, the problem of coming up with convolutions
    that learn a variety of features is also resolved.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个研究团队参与了这场竞争，目标是提出一个能够预测包含数百万图像的数据集中的多类图像的模型。由于数据集中有数百万图像，数据集有限性的问题得以解决。此外，鉴于研究团队们建立了巨大的网络，卷积网络学习多种特征的问题也得到了解决。
- en: Hence, we are in a position to reuse the convolutions that were built on a different
    dataset, where the convolutions are learning to predict the various features in
    an image and then pass them through a hidden layer so that we can predict the
    class of an image for our specific dataset. There are multiple pre-trained models
    that were developed by different groups. We will go through VGG16 here.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以重用在不同数据集上构建的卷积层，在这些卷积层中，网络学习预测图像中的各种特征，然后将这些特征传递通过隐藏层，以便我们能够预测针对特定数据集的图像类别。不同的研究小组开发了多个预训练模型，本文将介绍VGG16。
- en: Getting ready
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this section, let's try to understand how we can leverage the VGG16 pre-trained
    network for our gender classification exercise.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将尝试理解如何利用VGG16的预训练网络来进行性别分类练习。
- en: 'The VGG16 model''s architecture is as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: VGG16模型的架构如下：
- en: '![](img/c55e5a9b-e05e-4c31-9943-aeb2a6ae4c83.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c55e5a9b-e05e-4c31-9943-aeb2a6ae4c83.png)'
- en: Notice that the model's architecture is very similar to the model that we trained
    in the Gender classification using CNNs section. The major difference is that
    this model is deeper (more hidden layers). Additionally, the weights of the VGG16
    network are obtained by training on millions of images.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型的架构与我们在“使用CNN进行性别分类”一节中训练的模型非常相似。主要的区别在于，这个模型更深（更多的隐藏层）。此外，VGG16网络的权重是通过在数百万图像上训练得到的。
- en: We'll ensure that the VGG16 weights are frozen from updating while training
    our model to classify gender in an image. The output of passing an image in the
    gender classification exercise (which is of 300 x 300 x 3 in shape) is 9 x 9 x
    512 in shape.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将确保在训练我们的模型以分类图像中的性别时，VGG16的权重不会被更新。通过性别分类练习（形状为300 x 300 x 3的图像）的输出形状是9 x
    9 x 512。
- en: We shall keep the weights as they were in the original network, extract the
    9 x 9 x 512 output, pass it through another convolution pooling operation, flatten
    it, connect it to a hidden layer, and then pass it through the sigmoid activation
    to determine whether the image is of a male or a female.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将保留原网络中的权重，提取9 x 9 x 512的输出，通过另一个卷积池化操作，进行平坦化，连接到隐藏层，并通过sigmoid激活函数来判断图像是男性还是女性。
- en: Essentially, by using the convolution and pooling layers of the VGG16 model,
    we are using the filters that were trained on a much bigger dataset. Ultimately,
    we will be fine-tuning the output of these convolution and pooling layers for
    the objects that we are trying to predict.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，通过使用VGG16模型的卷积层和池化层，我们是在使用在更大数据集上训练得到的滤波器。最终，我们将对这些卷积层和池化层的输出进行微调，以适应我们要预测的对象。
- en: How to do it...
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'With this strategy in place, let''s code up our solution as follows (Please
    refer to `Transfer_learning.ipynb` file in GitHub while implementing the code):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个策略，让我们按照以下方式编写代码（在实现代码时，请参考GitHub中的`Transfer_learning.ipynb`文件）：
- en: 'Import the pre-trained model:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入预训练模型：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that we are excluding the last layer in the VGG16 model. This is to ensure
    that we fine-tune the VGG16 model for the problem that we are trying solve. Additionally,
    given that our input image shape is 300 X 300 X 3, we are specifying the same
    while downloading the VGG16 model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在VGG16模型中排除了最后一层。这是为了确保我们根据我们要解决的问题对VGG16模型进行微调。此外，鉴于我们的输入图像尺寸为300 X 300
    X 3，我们在下载VGG16模型时也指定了相同的尺寸。
- en: 'Preprocess the set of images. This preprocessing step ensures that the images
    are processed in a manner that the pre-trained model can take as input. For example,
    in the following code, we are performing preprocessing for one of the images,
    named `img`:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理图像集。这个预处理步骤确保图像的处理方式能够被预训练的模型接受作为输入。例如，在下面的代码中，我们对其中一张名为`img`的图像进行预处理：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We are preprocessing the image as per the preprocessing requirement in VGG16
    using the `preprocess_input` method.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`preprocess_input`方法按照VGG16的预处理要求来预处理图像。
- en: Create the input and output datasets. For this exercise, we will continue from
    the end of step 3 in Scenario 1 of Gender classification using CNN. Here, the
    process of creating input and output datasets remains the same as what we have
    already done, with a minor modification of extracting features using the VGG16
    model.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数据集。在本练习中，我们将从“使用CNN进行性别分类”场景1的第3步结束继续。在这里，创建输入和输出数据集的过程与我们之前做的一样，唯一的变化是使用VGG16模型提取特征。
- en: 'We will pass each image through `vgg16_model` so that we take the output of `vgg16_model` as
    the processed input. Additionally, we will be performing the preprocessing on
    top of the input as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过`vgg16_model`传递每张图像，以便将`vgg16_model`的输出作为处理后的输入。此外，我们还将对输入进行如下预处理：
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, we pass the pre-processed input to the VGG16 model to extract features,
    as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将预处理后的输入传递给VGG16模型以提取特征，如下所示：
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: In the preceding code, in addition to passing the image through VGG16 model,
    we have also stored the input values in a list.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，除了将图像传递给VGG16模型外，我们还将输入值存储在一个列表中。
- en: 'Convert the input and output to NumPy arrays and create training and test datasets:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入和输出转换为NumPy数组，并创建训练和测试数据集：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Build and compile the model:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译模型：
- en: '[PRE22]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The summary of the model is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要如下：
- en: '![](img/05928ce6-c892-492b-87f5-bdb7ba98730c.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/05928ce6-c892-492b-87f5-bdb7ba98730c.png)'
- en: 'Compile the model:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 编译模型：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Fit the model while scaling the input data:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在缩放输入数据时训练模型：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once we fit the model, we should see that we are able to attain an accuracy
    of ~89% on the test dataset in the first few epochs:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练模型，我们应该能看到，在前几轮训练中，模型能够在测试数据集上达到约89%的准确率：
- en: '![](img/1d5465bb-b9da-4905-a985-3d0a7c1d6b23.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d5465bb-b9da-4905-a985-3d0a7c1d6b23.png)'
- en: Contrast this with the models we built in the Gender classification using CNN
    section, where in any of the scenarios, we were not able to reach 80% accuracy
    in classification in 10 epochs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与我们在“使用CNN进行性别分类”部分中构建的模型进行对比，在那些场景中，我们无法在10轮训练内实现80%的分类准确率。
- en: 'A sample of some of the images where the model mis-classified is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些模型误分类的图像示例：
- en: '![](img/fcb824ca-4791-478c-9d65-7d2f63cec26c.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fcb824ca-4791-478c-9d65-7d2f63cec26c.png)'
- en: Note that, in the preceding picture, the model potentially mis-classified when
    the input image is either a part of a face or if the object in the image occupies
    a much smaller portion of the total image or potentially, if the label was provided
    incorrectly.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的图片中，当输入图像是面部的一部分，或者图像中的物体占据了图像总面积的较小部分，或者标签可能不正确时，模型可能会误分类。
- en: Visualizing the output of the intermediate layers of a neural network
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化神经网络中间层的输出
- en: In the previous section, we built a model that learns to classify gender from
    images with an accuracy of 89%. However, as of now, it is a black box for us in
    terms of what the filters are learning.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分，我们构建了一个可以从图像中学习性别分类的模型，准确率为89%。然而，到目前为止，在滤波器学到了什么方面，它对我们来说仍然是一个黑箱。
- en: In this section, we will learn how to extract what the various filters in a
    model are learning. Additionally, we will contrast the scenario of what the filters
    in the initial layers are learning with what the features in the last few layers
    are learning.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将学习如何提取模型中各个滤波器学到了什么。此外，我们还将对比初始层中的滤波器学到的内容与最后几层中的特征学到的内容。
- en: Getting ready
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'To understand how to extract what the various filters are learning, let''s
    adopt the following strategy:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解如何提取各个滤波器学到了什么，让我们采用以下策略：
- en: We will select an image on which to perform analysis.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将选择一张图像进行分析。
- en: We will select the first convolution to understand what the various filters
    in the first convolution are learning.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将选择第一个卷积层，以理解第一个卷积中的各个滤波器学到了什么。
- en: 'Calculate the output of the convolution of weights in the first layer and the
    input image:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算第一层权重与输入图像卷积的输出：
- en: 'In this step, we will extract the intermediate output of our model:'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此步骤中，我们将提取模型的中间输出：
- en: We will be extracting the output of the first layer of the model.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将提取模型的第一层输出。
- en: 'To extract the output of first layer, we will use the functional API:'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了提取第一层的输出，我们将使用功能性API：
- en: The input to the functional API is the input image, and the output will be the
    output of the first layer.
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能性API的输入是输入图像，输出将是第一层的输出。
- en: This returns the output of the intermediate layer across all the channels (filters).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将返回所有通道（滤波器）中间层的输出。
- en: We will perform these steps on both the first layer and the last layer of the
    convolution.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将对卷积的第一层和最后一层执行这些步骤。
- en: Finally, we will visualize the output of the convolution operations across all
    channels.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将可视化所有通道的卷积操作输出。
- en: We will also visualize the output of a given channel across all images.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还将可视化给定通道在所有图像上的输出。
- en: How to do it...
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: In this section, we will code up the process of visualizing what the filters
    are learning across the convolution filters of the initial layers as well as the
    final layers.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将编写代码，展示初始层和最终层的卷积滤波器所学习的内容。
- en: 'We''ll reuse the data that we prepared in the *Gender classification using
    CNN* recipe''s Scenario 1 from *step 1* to *step 4* (please refer to `Transfer_learning.ipynb`
    file in GitHub while implementing the code):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重用在*使用CNN进行性别分类*食谱的情景1中*步骤1*到*步骤4*准备的数据（在实现代码时，请参考GitHub中的`Transfer_learning.ipynb`文件）：
- en: 'Identify an image for which you want to visualize the intermediate output:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一张图像来可视化其中间输出：
- en: '[PRE25]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](img/009c041f-b3c5-40f0-973f-726d4f649b44.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/009c041f-b3c5-40f0-973f-726d4f649b44.png)'
- en: 'Define the functional API that takes the image as an input, and the first convolution
    layer''s output as output:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义功能性API，输入为图像，输出为第一卷积层的输出：
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We have defined an intermediate model named `activation_model`, where we are
    passing the image of interest as input and extracting the first layer's output
    as the model's output.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个名为`activation_model`的中间模型，在该模型中，我们将感兴趣的图像作为输入，并提取第一层的输出作为模型的输出。
- en: Once we have defined the model, we will extract the activations of the first
    layer by passing the input image through the model. Note that we will have to
    reshape the input image so that it is the shape the model expects.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了模型，我们将通过将输入图像传递给模型来提取第一层的激活。请注意，我们必须调整输入图像的形状，以便它符合模型的要求。
- en: 'Let''s visualize the first 36 filters in the output, as follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们按如下方式可视化输出中的前36个滤波器：
- en: '[PRE27]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the preceding code, we created a 6 x 6 frame on which we can plot 36 images.
    Furthermore, we are looping through all the channels in `first_layer_activation`
    and plotting the output of the first layer, as follows:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们创建了一个6x6的框架，用于绘制36张图像。此外，我们正在循环遍历`first_layer_activation`中的所有通道，并绘制第一层的输出，具体如下：
- en: '![](img/ae741c21-6feb-427e-abbf-692edf9c8774.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae741c21-6feb-427e-abbf-692edf9c8774.png)'
- en: Here, we can see that certain filters extract the contours of the original image
    (filter 0, 4, 7, 10, for example). Additionally, certain filters have learned
    to recognize only a few aspects, such as ears, eyes, and nose (filter 30, for
    example).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到某些滤波器提取了原始图像的轮廓（例如滤波器0、4、7、10）。此外，某些滤波器已经学会了仅识别几个特征，例如耳朵、眼睛和鼻子（例如滤波器30）。
- en: 'Let''s validate our understanding that certain filters are able to extract
    contours of the original image by going through the output of filter 7 for 36
    images, as follows:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过检查36张图像中滤波器7的输出来验证我们对某些滤波器能够提取原始图像轮廓的理解，具体如下：
- en: '[PRE28]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the preceding code, we are looping through the first 36 images and plotting
    the output of the first convolution layer for all 36 images:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们正在循环遍历前36张图像，并绘制所有36张图像的第一卷积层输出：
- en: '![](img/1d4d36d6-33c2-45aa-9a20-ace2c48fa515.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d4d36d6-33c2-45aa-9a20-ace2c48fa515.png)'
- en: Note that, across all the images, the seventh filter is learning the contours
    within an image.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在所有图像中，第七个滤波器正在学习图像中的轮廓。
- en: 'Let''s try to understand what the filters in the last convolution layer are
    learning. To understand where the last convolution layer is located in our model,
    let''s extract the various layers in our model:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试理解最后一个卷积层中的滤波器学到了什么。为了理解我们模型中最后一个卷积层的位置，我们将提取模型中的各种层：
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The following layers name will be displayed by executing the preceding code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码后将显示以下层名称：
- en: '![](img/2eec4bdc-3084-42c3-bb48-c6718498c6d0.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2eec4bdc-3084-42c3-bb48-c6718498c6d0.png)'
- en: 'Note that the last convolution layer is the ninth output of our model and can
    be extracted as follows:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，最后一个卷积层是我们模型中的第九个输出，可以通过以下方式提取：
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The size of the image has now shrunk considerably (to 1, 9,9,512), due to the
    multiple pooling operations that were performed on top of the image. A visualization
    of what the various filters in the last convolution layer are learning is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在图像上进行了多次池化操作，图像的大小现在已经大幅缩小（到1, 9,9,512）。以下是最后一个卷积层中各种滤波器学习的可视化效果：
- en: '![](img/a9768459-b4fe-4a48-9f37-be7dcaf450fb.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9768459-b4fe-4a48-9f37-be7dcaf450fb.png)'
- en: Note that, in this iteration, it is not as clear to understand what the last
    convolution layer's filters are learning (as the contours are not easy to attribute
    to one of the parts of original image), as these are more granular than the contours
    that were learned in the first convolution layer.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在此迭代中，不太容易理解最后一个卷积层的滤波器学到了什么（因为轮廓不容易归属于原图的某一部分），这些比第一个卷积层学到的轮廓更为细致。
- en: Gender classification of the person in image using the VGG19 architecture-based
    model
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于VGG19架构的模型对图像中的人物进行性别分类
- en: In the previous section, we learned about how VGG16 works. VGG19 is an improved
    version of VGG16, with a greater number of convolution and pooling operations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解了VGG16的工作原理。VGG19是VGG16的改进版本，具有更多的卷积和池化操作。
- en: Getting ready
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'The architecture of the VGG19 model is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: VGG19模型的架构如下：
- en: '![](img/6d17d7c6-4df1-4ad0-8e47-16e93e90cadd.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d17d7c6-4df1-4ad0-8e47-16e93e90cadd.png)'
- en: Note that the preceding architecture has more layers, as well as more parameters.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前述架构具有更多的层和更多的参数。
- en: Note that the 16 and 19 in the VGG16 and VGG19 architectures stand for the number
    of layers in each of these networks. Once we extract the 9 x 9 x 512 output after
    we pass each image through the VGG19 network, that output will be the input for
    our model.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，VGG16和VGG19架构中的16和19分别表示这些网络中的层数。我们通过VGG19网络传递每个图像后提取的9 x 9 x 512输出将作为我们的模型的输入。
- en: Additionally, the process of creating input and output datasets and then building,
    compiling, and fitting a model will remain the same as what we saw in the Gender
    classification using a VGG16 model-based architecture recipe.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，创建输入和输出数据集，然后构建、编译和拟合模型的过程与我们在使用基于VGG16模型架构进行性别分类的食谱中看到的过程相同。
- en: How to do it...
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In this section, we will code up the VGG19 pre-trained model, as follows (Please
    refer to `Transfer_learning.ipynb` file in GitHub while implementing the code):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将编码VGG19的预训练模型，代码如下（在实现代码时，请参考GitHub中的`Transfer_learning.ipynb`文件）：
- en: 'Prepare the input and output data (we''ll continue from *step 3* in Scenario
    1 of the *Gender classification using CNN* recipe):'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备输入和输出数据（我们将继续从*性别分类使用CNN*食谱中的*步骤3*开始）：
- en: '[PRE31]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Convert the input and output into their corresponding arrays and create the
    training and test datasets:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入和输出转换为相应的数组，并创建训练集和测试集：
- en: '[PRE32]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Build and compile the model:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译模型：
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'A visualization of the model is as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是模型的可视化：
- en: '![](img/62ec64d6-36d0-41c4-9855-8692eb6c3594.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/62ec64d6-36d0-41c4-9855-8692eb6c3594.png)'
- en: '[PRE34]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Fit the model while scaling the input data:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在对输入数据进行缩放的同时拟合模型：
- en: '[PRE35]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let''s plot the training and test datasets'' loss and accuracy measures:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制训练集和测试集的损失和准确率度量：
- en: '![](img/42a19290-8e44-4746-92c9-acc35015a8f5.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42a19290-8e44-4746-92c9-acc35015a8f5.png)'
- en: We should note that we were able to achieve ~89% accuracy on the test dataset
    when we used the VGG19 architecture, which is very similar to that of the VGG16
    architecture.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，当使用VGG19架构时，我们能够在测试数据集上实现约89%的准确率，这与VGG16架构非常相似。
- en: 'A sample of mis-classified images is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是错误分类图像的示例：
- en: '![](img/73d219f5-a4bd-412c-b609-880880dfe98e.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/73d219f5-a4bd-412c-b609-880880dfe98e.png)'
- en: Note that, VGG19 seems to mis-classify based on the space occupied by a person
    in an image. Additionally, it seems to give higher weightage to predict that a
    male with long hair is a female.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，VGG19 似乎根据图像中人物所占空间来错误分类。此外，它似乎更倾向于预测长发男性为女性。
- en: Gender classification using the Inception v3 architecture-based model
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Inception v3 架构的性别分类模型
- en: In the previous recipes, we implemented gender classification based on the VGG16
    and VGG19 architectures. In this section, we'll implement the classification using
    the Inception architecture.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的食谱中，我们基于 VGG16 和 VGG19 架构实现了性别分类。在本节中，我们将使用 Inception 架构来实现分类。
- en: An intuition of how inception model comes in handy, is as follows.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: inception 模型如何派上用场的直观理解如下：
- en: There will be images where the object occupies the majority of the image. Similarly,
    there will be images where the object occupies a small portion of the total image.
    If we have the same size of kernels in both scenario, we are making it difficult
    for the model to learn – some images might have objects that are small and others
    might have objects that are larger.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 会有一些图像中，物体占据了图像的大部分。同样，也会有一些图像中，物体只占据了图像的一小部分。如果在这两种情况下我们使用相同大小的卷积核，那么就会使模型的学习变得困难——一些图像可能包含较小的物体，而其他图像可能包含较大的物体。
- en: To address this problem, we will have filters of multiple sizes that operate
    at the same layer.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将在同一层中使用多个大小的卷积核。
- en: 'In such a scenario, the network essentially gets wide rather than getting deep,
    as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，网络本质上会变得更宽，而不是更深，如下所示：
- en: '![](img/54cce575-4a32-480d-bedf-29f96bb1f82e.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54cce575-4a32-480d-bedf-29f96bb1f82e.png)'
- en: 'In the preceding diagram, note that we are performing convolutions of multiple
    filters in a given layer. The inception v1 module has nine such modules stacked
    linearly, as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述图示中，注意我们在给定层中执行多个卷积操作。inception v1 模块有九个这样的模块线性堆叠，如下所示：
- en: '![](img/80aeb9f6-c85e-4033-83a1-15593f4127c7.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/80aeb9f6-c85e-4033-83a1-15593f4127c7.png)'
- en: Source: http://joelouismarino.github.io/images/blog_images/blog_googlenet_keras/googlenet_diagram.png
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 来源: http://joelouismarino.github.io/images/blog_images/blog_googlenet_keras/googlenet_diagram.png
- en: Note that this architecture is fairly deep as well as wide. This is likely to
    result in a vanishing gradient problem (as we saw in the case for batch normalization
    in [Chapter 2](2846fd6b-a2c2-4ebe-b36a-17f061b4cfa4.xhtml), *Building a Deep Feedforward
    Neural Network*).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种架构既深又宽。可能会导致梯度消失问题（正如我们在[第 2 章](2846fd6b-a2c2-4ebe-b36a-17f061b4cfa4.xhtml)中关于批量归一化的案例中所见，*构建深度前馈神经网络*）。
- en: 'To get around the problem of a vanishing gradient, inception v1 has two auxiliary
    classifiers that stem out of the inception modules. The overall loss of inception based
    network tries to minimize is as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免梯度消失的问题，inception v1 在 inception 模块中添加了两个辅助分类器。inception 基础网络的总体损失函数如下所示：
- en: '[PRE36]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Note that auxiliary losses are used only during training and are ignored during
    the prediction process.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，辅助损失仅在训练期间使用，在预测过程中会被忽略。
- en: Inception v2 and v3 are improvements on top of the inception v1 architecture
    where in v2, the authors have performed optimizations on top of convolution operations
    to process images faster and in v3, the authors have added 7 x 7 convolutions
    on top of the existing convolutions so that they can be concatenated together.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v2 和 v3 是对 inception v1 架构的改进，在 v2 中，作者在卷积操作的基础上进行了优化，以加快图像处理速度，而在
    v3 中，作者在现有卷积上添加了 7 x 7 的卷积，以便将它们连接起来。
- en: How to do it...
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The process in which we code up inception v3 is very similar to the way in
    which we built the VGG19 model-based classifier (Please refer to `Transfer_learning.ipynb`
    file in GitHub while implementing the code):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现 inception v3 的过程与构建基于 VGG19 模型的分类器非常相似（在实现代码时，请参考 GitHub 中的`Transfer_learning.ipynb`文件）：
- en: 'Download the pre-trained Inception model:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载预训练的 Inception 模型：
- en: '[PRE37]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Note that we would need an input image that is at least 300 x 300 in shape for
    the inception v3 pre-trained model to work.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要一个至少为 300 x 300 大小的输入图像，才能使 inception v3 预训练模型正常工作。
- en: 'Create the input and output datasets (we''ll continue from step 3 in Scenario
    1 of the *Gender classification using CNNs* recipe):'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数据集（我们将从*性别分类使用 CNNs*食谱中的场景 1 第 3 步继续）：
- en: '[PRE38]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Create the input and output arrays, along with the training and test datasets:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数组，以及训练和测试数据集：
- en: '[PRE39]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Build and compile the model:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译模型：
- en: '[PRE40]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The preceding model can be visualized as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的模型可以如下可视化：
- en: '![](img/e6af6840-948a-4ae5-81f1-54292efc2e7e.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e6af6840-948a-4ae5-81f1-54292efc2e7e.png)'
- en: 'Fit the model while scaling the input data:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在缩放输入数据的同时拟合模型：
- en: '[PRE41]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The variation of accuracy and loss values is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度和损失值的变化如下：
- en: '![](img/24d5c932-1897-4540-af2c-c99ea772fbd1.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](img/24d5c932-1897-4540-af2c-c99ea772fbd1.png)'
- en: You should notice that the accuracy in this scenario too is also ~90%.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意到，在这种情况下，准确度也是大约 ~90%。
- en: Gender classification of the person in image using the ResNet 50 architecture-based
    model
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于ResNet 50架构的模型对图像中的人物进行性别分类
- en: From VGG16 to VGG19, we have increased the number of layers and generally, the
    deeper the neural network, the better its accuracy. However, if merely increasing
    the number of layers is the trick, then we could keep on adding more layers (while
    taking care to avoid over-fitting) to the model to get a more accurate results.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 从VGG16到VGG19，我们增加了层数，通常来说，神经网络越深，准确度越高。然而，如果仅仅增加层数是技巧，那么我们可以继续增加更多层（同时注意避免过拟合）来获得更准确的结果。
- en: Unfortunately, that does not turn out to be true and the issue of the vanishing
    gradient comes into the picture. As the number of layers increases, the gradient
    becomes so small as it traverses the network that it becomes hard to adjust the
    weights, and the network performance deteriorates.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这并不完全正确，梯度消失的问题出现了。随着层数的增加，梯度在网络中传递时变得非常小，以至于很难调整权重，网络性能会下降。
- en: ResNet comes into the picture to address this specific scenario.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet的出现是为了应对这一特定情况。
- en: Imagine a scenario where a convolution layer does nothing but pass the output
    of the previous layer to the next layer if the model has nothing to learn. However,
    if the model has to learn a few other features, the convolution layer takes the
    previous layer's output as input and learns the additional features that need
    to be learnt to perform classification.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一种情况，如果模型没有任何需要学习的内容，卷积层仅仅将前一层的输出传递给下一层。然而，如果模型需要学习一些其他特征，卷积层会将前一层的输出作为输入，并学习需要学习的附加特征来执行分类。
- en: The term residual is the additional feature that the model is expected to learn
    from one layer to the next layers.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 残差是模型期望从一层到下一层学习的附加特征。
- en: 'A typical ResNet architecture looks as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的ResNet架构如下所示：
- en: '![](img/4e8186c6-17ab-4d77-9351-bca12d65e3f5.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4e8186c6-17ab-4d77-9351-bca12d65e3f5.jpg)'
- en: Source: https://arxiv.org/pdf/1512.03385.pdf
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/pdf/1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)
- en: Note that we have skip connections that are connecting a previous layer to a
    layer down the line, along with the traditional convolution layers in this network.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们有跳跃连接，这些连接将前一层与后续层连接起来，并且网络中还有传统的卷积层。
- en: Furthermore, the 50 in ResNet50 comes from the fact that we have a total of
    50 layers in the network.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ResNet50中的50表示该网络总共有50层。
- en: How to do it...
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行...
- en: 'The ResNet50 architecture is built as follows (please refer to `Transfer_learning.ipynb`
    file in GitHub while implementing the code):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet50架构的构建如下（在实现代码时，请参考GitHub上的`Transfer_learning.ipynb`文件）：
- en: 'Download the pre-trained inception model:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载预训练的Inception模型：
- en: '[PRE42]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note that we need an input image that is at least 224 x 224 in shape for the
    ResNet50 pre-trained model to work.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要一个至少为224 x 224形状的输入图像，才能使ResNet50预训练模型正常工作。
- en: 'Create the input and output datasets (we''ll continue from *step 3* in Scenario
    1 of the *Gender classification using CNNs* recipe):'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数据集（我们将从*性别分类使用CNNs*教程中的*步骤3*继续）：
- en: '[PRE43]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Create the input and output arrays, along with the training and test datasets:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数组，并准备训练和测试数据集：
- en: '[PRE44]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Build and compile the model:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译模型：
- en: '[PRE45]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'A summary of the model is as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结如下：
- en: '![](img/a10c5097-edd5-4d36-ac78-482437730971.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a10c5097-edd5-4d36-ac78-482437730971.png)'
- en: 'Fit the model while scaling the input data:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在缩放输入数据的同时拟合模型：
- en: '[PRE46]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The variation of the accuracy and loss values is as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度和损失值的变化如下：
- en: '![](img/69d8c163-2be3-4ace-90ef-5e3bcb0fb8b6.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69d8c163-2be3-4ace-90ef-5e3bcb0fb8b6.png)'
- en: Note that the preceding model gives an accuracy of 92%.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的模型给出的准确率为92%。
- en: There is no considerable difference in the accuracy levels of multiple pre-trained
    models on gender classification, as potentially they were trained to extract the
    general features, but not necessarily the features to classify gender.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在性别分类的多个预训练模型中，准确度没有显著差异，因为它们可能训练出来的是提取一般特征的模型，而不一定是用来分类性别的特征。
- en: Detecting the key points within image of a face
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测图像中的面部关键点
- en: In this recipe, we will learn about detecting the key points of a human face,
    which are the boundaries of the left and right eyes, the nose, and the four coordinates
    of the mouth.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将学习如何检测人脸的关键点，这些关键点包括左右眼的边界、鼻子以及嘴巴的四个坐标。
- en: 'Here are two sample pictures with the key points:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两张带有关键点的示例图片：
- en: '![](img/a1d5652b-4dc0-4a50-81d8-212f94cd0d87.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1d5652b-4dc0-4a50-81d8-212f94cd0d87.png)'
- en: Note that the key points that we are expected to detect are plotted as dots
    in this picture. A total of 68 key points are detected on the image of face, where
    the key points of the face include - Mouth, right eyebrow, left eyebrow, right
    eye, left eye, nose, jaw.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们预计要检测的关键点在这张图片中作为点绘制。图像中共检测到68个关键点，其中包括面部的关键点——嘴巴、右眉毛、左眉毛、右眼、左眼、鼻子、下巴。
- en: In this case study, we will leverage the VGG16 transfer learning technique that
    we learned in the *Gender classification in image using the VGG16 architecture-based
    model* section to detect the key points on the face.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例中，我们将利用在 *使用基于VGG16架构的模型进行图像性别分类* 部分中学到的VGG16迁移学习技术来检测面部的关键点。
- en: Getting ready
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: For the key-point detection task, we will work on a dataset where we annotate
    the points that we want to detect. For this exercise, the input will be the image
    on which we want to detect the key points and the output will be the *x* and *y*
    coordinates of the key points. The dataset can be downloaded from here: [https://github.com/udacity/P1_Facial_Keypoints](https://github.com/udacity/P1_Facial_Keypoints).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关键点检测任务，我们将使用一个数据集，在该数据集上我们标注了要检测的点。对于这个练习，输入将是我们要检测关键点的图像，输出将是关键点的 *x* 和
    *y* 坐标。数据集可以从这里下载：[https://github.com/udacity/P1_Facial_Keypoints](https://github.com/udacity/P1_Facial_Keypoints)。
- en: 'The steps we''ll follow are as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循以下步骤：
- en: Download the dataset
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载数据集
- en: Resize the images to a standard shape
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像调整为标准形状
- en: While resizing the images, ensure that the key points are modified so that they
    represent the modified (resized) image
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在调整图像大小时，确保关键点已修改，以便它们代表修改后的（调整大小的）图像
- en: Pass the resized images through VGG16 model
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将调整大小的图像传递给VGG16模型
- en: Create input and output arrays, where the input array is the output of passing
    image through VGG16 model and the output array is the modified facial key point
    locations
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建输入和输出数组，其中输入数组是通过VGG16模型传递图像的输出，输出数组是修改后的面部关键点位置
- en: Fit a model that minimizes the absolute error value of the difference between
    predicted and actual facial key points
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 适配一个模型，最小化预测的面部关键点与实际面部关键点之间的绝对误差值
- en: How to do it...
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The strategy that we discussed is coded as follows (Please refer to `Facial_keypoints.ipynb`
    file in GitHub while implementing the code):'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的策略的代码如下（在实现代码时，请参考 GitHub 中的 `Facial_keypoints.ipynb` 文件）：
- en: 'Download and import the dataset:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并导入数据集：
- en: '[PRE47]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Inspect this dataset.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 检查这个数据集。
- en: '![](img/fa5e1b6c-254b-4440-88d1-6829589e3257.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa5e1b6c-254b-4440-88d1-6829589e3257.png)'
- en: There are a total of 137 columns of which the first column is the name of image
    and the rest of 136 columns represent the x and y co-ordinate values of the 68
    key points of face of the corresponding image.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 总共有137列，其中第一列是图像的名称，剩余的136列代表对应图像中68个面部关键点的 *x* 和 *y* 坐标值。
- en: 'Preprocess the dataset to extract the image, resized image, VGG16 features
    of image, modified key point locations as the output:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据集，提取图像、调整大小后的图像、VGG16特征以及修改后的关键点位置作为输出：
- en: 'Initialize lists that will be appended to create input and output arrays:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化将被附加以创建输入和输出数组的列表：
- en: '[PRE48]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Loop through the images and read them:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 循环读取图像：
- en: '[PRE49]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Capture the key point values and store them
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获关键点值并存储
- en: '[PRE50]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Resize the images
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 调整图像大小
- en: '[PRE51]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Preprocess the image so that it can be passed through VGG16 model and extract
    features:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理图像，以便可以通过VGG16模型传递并提取特征：
- en: '[PRE52]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Append the input and output values to corresponding lists:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入和输出值附加到相应的列表中：
- en: '[PRE53]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Create input and output arrays:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 创建输入和输出数组：
- en: '[PRE54]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Build and compile a model
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建并编译模型
- en: '[PRE55]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](img/e415b00a-b498-468b-b3ff-023188db2dd0.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e415b00a-b498-468b-b3ff-023188db2dd0.png)'
- en: 'Compile the model:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 编译模型：
- en: '[PRE56]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Fit the model
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合模型
- en: '[PRE57]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Note that, we are dividing the input array with maximum value of input array
    so that we scale the input dataset. The variation of training and test loss over
    increasing epochs is as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们通过输入数组的最大值来对输入数组进行除法运算，以便对输入数据集进行缩放。随着训练轮数增加，训练损失和测试损失的变化如下：
- en: '![](img/07513580-4052-4792-9a44-5b2731f5769b.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/07513580-4052-4792-9a44-5b2731f5769b.png)'
- en: 'Predict on a test image. In the following code, we are predicting on the second
    image from last in input array (note that, as `validation_split` is `0.1`, second
    image from last was not supplied to model while training). We are ensuring that
    we are passing our image through `preprocess_input` method and then through `VGG16_model`
    and finally, the scaled version of `VGG16_model` output to the `model_vgg16` that
    we built:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试图像进行预测。在以下代码中，我们对输入数组中的倒数第二张图像进行预测（注意，`validation_split` 为 `0.1`，因此倒数第二张图像在训练过程中并未提供给模型）。我们确保将图像传入
    `preprocess_input` 方法，然后通过 `VGG16_model`，最后将 `VGG16_model` 输出的缩放版本传递给我们构建的 `model_vgg16`：
- en: '[PRE58]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The preceding prediction on test image can be visualized as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 对测试图像的前述预测可以通过以下方式可视化：
- en: '![](img/cee845d9-96bf-44a9-a7fa-5fa46fbeecda.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cee845d9-96bf-44a9-a7fa-5fa46fbeecda.png)'
- en: We can see that the key points are detected very accurately on the test image.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，关键点在测试图像上被非常准确地检测出来。
