- en: Machine Learning and Neural Networks 101
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与神经网络入门
- en: '** Artificial intelligence** (**AI**) has captured much of our attention in
    recent years. From face recognition security systems in our smartphones to booking
    an Uber ride through Alexa, AI has become ubiquitous in our everyday lives. Still,
    we are constantly being reminded that the full potential of AI has not yet been
    realized, and that AI will become an even bigger transformative factor in our
    lives.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）近年来吸引了大量关注。从智能手机中的人脸识别安全系统到通过 Alexa 预定 Uber 出行，AI 已经无处不在地渗透到我们的日常生活中。然而，我们不断被提醒，AI
    的全部潜力尚未被完全实现，AI 将在未来变得更加具有变革性，深刻影响我们的生活。'
- en: When we look at the horizon, we can see the relentless progression of AI with
    its promise to better our everyday lives. Powered by AI, self-driving cars are
    becoming less science fiction, and more of a reality. Self-driving cars aim to
    reduce traffic accidents by eliminating human error, ultimately improving our
    lives. Similarly, the usage of AI in healthcare promises to improve outcomes.
    Notably, the UK's National Health Service has announced an ambitious AI project
    to diagnose early-stage cancer, which can potentially save thousands of lives.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们展望未来时，可以看到 AI 不断进步，并承诺改善我们的日常生活。得益于 AI，自动驾驶汽车正在从科幻走向现实。自动驾驶汽车旨在通过消除人为错误来减少交通事故，从而最终改善我们的生活。类似地，AI
    在医疗领域的应用也有望改善治疗效果。特别是，英国的国家卫生服务系统（NHS）已宣布一项雄心勃勃的 AI 项目，用于早期癌症诊断，这可能拯救数千条生命。
- en: The transformative nature of AI has led experts to call it the fourth industrial
    revolution. AI is the catalyst that will shape modern industries, and having knowledge
    of AI is essential in this new world. By the end of this book, you will have a
    better understanding of the algorithms that power AI, and will have developed
    real-life projects using these cutting-edge algorithms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: AI 的变革性特质使得专家们称其为第四次工业革命。AI 是塑造现代产业的催化剂，拥有 AI 知识在这个新时代至关重要。在本书结束时，你将对驱动 AI 的算法有更深入的理解，并且通过这些前沿算法开发了实际项目。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: A primer on machine learning and neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习和神经网络入门
- en: Setting up your computer for machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习配置计算机环境
- en: Executing your machine learning projects from start to finish using the machine
    learning workflow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头到尾执行你的机器学习项目，使用机器学习工作流
- en: Creating your own neural network from scratch in Python without using a machine
    learning library
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中从零开始创建自己的神经网络，而不使用任何机器学习库
- en: Using pandas for data analysis in Python
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pandas 在 Python 中进行数据分析
- en: Leveraging machine learning libraries such as Keras to build powerful neural
    networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用像 Keras 这样的机器学习库来构建强大的神经网络
- en: What is machine learning?
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Although machine learning and AI are often used interchangeably, there are subtle
    differences that set them apart. The term AI was first coined in the 1950s, and
    it refers to the capability of a machine to imitate intelligent human behavior.
    To that end, researchers and computer scientists have pursued several approaches.
    Early efforts in AI were centered around an approach known as symbolic AI. Symbolic
    AI attempts to express human knowledge in a declarative form that computers could
    process. The height of symbolic AI resulted in the expert system, a computer system
    that emulated human decision making.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习和 AI 经常被交替使用，但它们之间存在微妙的区别。AI 这一术语最早出现在 1950 年代，它指的是机器模仿智能人类行为的能力。为了这一目标，研究人员和计算机科学家们尝试了多种方法。AI
    的早期工作集中于一种被称为符号 AI 的方法。符号 AI 尝试以声明性形式表达人类知识，使计算机可以处理。符号 AI 的巅峰成果是专家系统，这是一种模拟人类决策过程的计算机系统。
- en: However, one major drawback of symbolic AI is that it relied on the domain knowledge
    of human experts, and required those rules and knowledge to be hardcoded for problem-solving.
    AI as a scientific field went through a period of drought (known as the AI winter),
    when scientists became increasingly disillusioned by the limitations of AI.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，符号 AI 的一个主要缺点是它依赖于人类专家的领域知识，并且需要将这些规则和知识硬编码到问题解决中。AI 作为一个科学领域经历了一段干旱期（被称为
    AI 冬天），当时科学家们越来越失望于 AI 的局限性。
- en: While symbolic AI took center stage in the 1950s, a subfield of AI known as
    machine learning was quietly bubbling in the background.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然符号 AI 在 1950 年代曾占据主导地位，但被称为机器学习的 AI 子领域在背后悄然发展。
- en: Machine learning refers to algorithms that computers use to learn from data,
    allowing it to make predictions on future, unseen data.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是指计算机使用的算法，通过从数据中学习，使其能够对未来未见过的数据做出预测。
- en: However, early AI researchers did not pay much attention to machine learning,
    as computers back then were neither powerful enough nor had the capability to
    store the huge amount of data that machine learning algorithms require. As it
    turns out, machine learning would not be left in the cold for long. In the late
    2000s, AI enjoyed a resurgence, with machine learning largely propelling its growth.
    The key reason for this resurgence was the maturation of computer systems that
    could collect and store a massive amount of data (big data), along with processors
    that are fast enough to run the machine learning algorithms. Thus, the AI summer
    began.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，早期的AI研究人员并没有太关注机器学习，因为当时的计算机既不够强大，也没有足够的存储能力来存储机器学习算法所需要的海量数据。事实证明，机器学习并没有被冷落太久。在2000年代末，AI经历了复兴，机器学习在其中起到了推动作用。这次复兴的关键原因是计算机系统的成熟，这些系统可以收集并存储大量数据（大数据），并且处理器足够快，可以运行机器学习算法。因此，AI的“夏天”开始了。
- en: Machine learning algorithms
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法
- en: 'Now that we have talked about what machine learning is, we need to understand
    how machine learning algorithms work. Machine learning algorithms can be broadly
    classified into two categories:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了什么是机器学习，我们需要理解机器学习算法是如何工作的。机器学习算法大致可以分为两类：
- en: '**Supervised learning**: Using labeled training data, the algorithm learns
    the rule for mapping the input variables into the target variable. For example,
    a supervised learning algorithm learns to predict whether there will be rain (the
    target variable) from input variables such as the temperature, time, season, atmospheric
    pressure, and so on.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：使用带标签的训练数据，算法学习将输入变量映射到目标变量的规则。例如，一个监督学习算法可以学习如何从温度、时间、季节、气压等输入变量中预测是否会下雨（目标变量）。'
- en: '**Unsupervised learning**: Using unlabeled training data, the algorithm learns
    associative rules for the data. The most common use case for unsupervised learning
    algorithms is in clustering analysis, where the algorithm learns hidden patterns
    and groups in data that are not explicitly labeled.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：使用无标签的训练数据，算法学习数据的关联规则。无监督学习算法最常见的应用是聚类分析，算法可以学习数据中那些未明确标记的隐藏模式和群体。'
- en: 'In this book, we will focus on supervised learning algorithms. As a concrete
    example of a supervised learning algorithm, let''s consider the following problem.
    You are an animal lover and a machine learning enthusiast and you wish to build
    a machine learning algorithm using supervised learning to predict whether an animal
    is a friend (a friendly puppy) or a foe (a dangerous bear). For simplicity, let''s
    assume that you have collected two measurements from different breeds of dogs
    and bears—their **Weight** and their **Speed**. After collecting the data (known
    as the training dataset), you plot them out on a graph, along with their labels
    (**Friend or Foe**):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将重点讨论监督学习算法。作为一个监督学习算法的具体例子，假设我们遇到以下问题。你是一个动物爱好者和机器学习爱好者，你希望使用监督学习建立一个机器学习算法，预测一个动物是朋友（友好的小狗）还是敌人（危险的熊）。为了简化问题，假设你已经收集了不同品种的狗和熊的两个测量值——它们的**体重**和**速度**。收集完数据（称为训练数据集）后，你将它们绘制在图表上，并标注它们的标签（**朋友或敌人**）：
- en: '![](img/66e4607f-a0dc-445c-8b1c-3b039eb7ac0c.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66e4607f-a0dc-445c-8b1c-3b039eb7ac0c.png)'
- en: Immediately, we can see that dogs tend to weigh less, and are generally faster,
    while bears are heavier and generally slower. If we draw a line (known as a decision
    boundary) between the dogs and the bears, we can use that line to make future
    predictions. Whenever we receive the measurements for a new animal, we can just
    see if it falls to the left or to the right of the line. Friends are to the left,
    and foes are to the right.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 立即，我们可以看到狗的体重往往较轻，通常更快，而熊则较重，通常更慢。如果我们在狗和熊之间画一条线（称为决策边界），我们可以用这条线来进行未来的预测。每当我们接收到一个新动物的测量值时，只需判断它是否位于线的左侧或右侧。朋友在左边，敌人则在右边。
- en: But this is a trivial dataset. What if we collect hundreds of different measurements?
    Then the graph would be more than 100-dimensional, and it would be impossible
    for a human being to draw a dividing line. However, such a task is not a problem
    for machine learning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是一个简单的数据集。如果我们收集了数百个不同的测量数据呢？那时图表将超过100维，人类将无法绘制分隔线。然而，对于机器学习来说，这样的任务并不成问题。
- en: 'In this example, the task of the machine learning algorithm is to learn the
    optimal decision boundary separating the datasets. Ideally, we want the algorithm
    to produce a **Decision Boundary** that completely separates the two classes of
    data (although this is not always possible, depending on the dataset):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，机器学习算法的任务是学习最佳的决策边界，区分数据集。理想情况下，我们希望算法生成一个完全将两类数据分隔开的**决策边界**（尽管根据数据集的不同，这并不总是可能的）：
- en: '![](img/bede3862-ca7e-488b-b900-3bce50f65ae2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bede3862-ca7e-488b-b900-3bce50f65ae2.png)'
- en: With this **Decision Boundary**, we can then make predictions on future, unseen
    data. If the **New Instance** lies to the left of the **Decision Boundary**, then
    we classify it as a friend. Vice versa, if the new instance lies to the right
    of the **Decision Boundary**, then we classify it as a foe.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个**决策边界**，我们就可以对未来的未见数据进行预测。如果**新实例**位于**决策边界**的左侧，则我们将其分类为朋友。反之，如果新实例位于**决策边界**的右侧，则我们将其分类为敌人。
- en: In this trivial example, we have used only two input variables and two classes.
    However, we can generalize the problem to include multiple input variables with
    multiple classes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单的例子中，我们只使用了两个输入变量和两个类别。然而，我们可以将问题推广到包含多个输入变量和多个类别的情况。
- en: 'Naturally, our choice of machine learning algorithm affects the kind of decision
    boundary produced. Some of the more popular supervised machine learning algorithms are
    as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，我们选择的机器学习算法会影响产生的决策边界类型。一些更常见的监督式机器学习算法如下：
- en: Neural networks
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: Linear regression
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归
- en: Logistic regression
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: '**Support vector machines** (**SVMs**)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVMs**）'
- en: Decision trees
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: The nature of the dataset (such as an image dataset or a numerical dataset)
    and the underlying problem that we are trying to solve should dictate the machine
    learning algorithm used. In this book, we will focus on neural networks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的性质（如图像数据集或数值数据集）以及我们要解决的潜在问题应该决定所使用的机器学习算法。在本书中，我们将重点讨论神经网络。
- en: The machine learning workflow
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习工作流
- en: 'We have discussed what machine learning is. But how exactly do you *do* machine
    learning? At a high level, machine learning projects are all about taking in raw
    data as input and churning out **Predictions** as **Output**. To do that, there
    are several important intermediate steps that must be accomplished. This machine
    learning workflow can be summarized by the following diagram:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了什么是机器学习。那么，如何*进行*机器学习呢？从高层次来看，机器学习项目的核心是将原始数据作为输入，输出**预测**作为**结果**。为了实现这一点，有几个重要的中间步骤必须完成。这个机器学习工作流可以通过以下图示来概括：
- en: '![](img/14e0c1b0-4330-4fa7-88c3-80001e91d063.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/14e0c1b0-4330-4fa7-88c3-80001e91d063.png)'
- en: The **Input** to our machine learning workflow will always be data. Data can
    come from different sources, with different data formats. For example, if we are
    working on a computer vision-based project, then our data will likely be images.
    For most other machine learning projects, the data will be presented in a tabular
    form, similar to spreadsheets. In some machine learning projects, data collection
    will be a significant first step. In this book, we will assume that the data will
    be provided to us, allowing us to focus on the machine learning aspect.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们机器学习工作流的**输入**始终是数据。数据可以来自不同的来源，具有不同的数据格式。例如，如果我们正在进行一个基于计算机视觉的项目，那么我们的数据很可能是图像。对于大多数其他机器学习项目，数据将以表格形式呈现，类似于电子表格。在一些机器学习项目中，数据收集将是一个重要的第一步。在本书中，我们假设数据将由我们提供，使我们可以专注于机器学习方面。
- en: The next step is to preprocess the data. Raw data is often messy, error-prone,
    and unsuitable for machine learning algorithms. Hence, we need to preprocess the
    data before we feed it to our models. In cases where data is provided from multiple
    sources, we need to merge the data into a single dataset. Machine learning models
    also require a numeric dataset for training purposes. If there are any categorical
    variables in the raw dataset (that is, gender, country, day of week, and so on),
    we need to encode those variables as numeric variables. We will see how we can
    do so later on in the chapter. Data scaling and normalization is also required
    for certain machine learning algorithms. The intuition behind this is that if
    the magnitude of certain variables is much greater than other variables, then
    certain machine learning algorithms will mistakenly place more emphasis on those
    dominating variables.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是预处理数据。原始数据通常杂乱、易出错，并不适合用于机器学习算法。因此，在将数据输入模型之前，我们需要先对其进行预处理。如果数据来自多个来源，我们需要将这些数据合并成一个数据集。机器学习模型也需要数值型的数据集来进行训练。如果原始数据集中有任何类别变量（如性别、国家、星期几等），我们需要将这些变量编码为数值型变量。我们将在本章稍后部分展示如何实现这一点。对于某些机器学习算法，还需要进行数据缩放和归一化。其背后的直觉是，如果某些变量的数值范围远大于其他变量，那么某些机器学习算法可能会错误地将更多的重视放在这些占主导地位的变量上。
- en: Real-world datasets are often messy. You will find that the data is incomplete
    and contains missing data in several rows and columns. There are several ways
    to deal with missing data, each with its own advantages and disadvantages. The
    easiest way is to simply discard rows and columns with missing data. However,
    this may not be practical, as we may end up discarding a significant percentage
    of our data. We can also replace the missing variables with the mean of the variables
    (if the variables happen to be numeric). This approach is more ideal than discarding
    data, as it preserves our dataset. However, replacing missing values with the
    mean tends to affect the distribution of the data, which may negatively impact
    our machine learning models. One other method is to predict what the missing values
    are, based on other values that are present. However, we have to be careful as
    doing this may introduce significant bias into our dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中的数据集通常是杂乱无章的。你会发现数据不完整，并且在多行多列中缺失数据。处理缺失数据有多种方法，每种方法都有其优缺点。最简单的方法是直接丢弃含有缺失数据的行和列。然而，这可能不切实际，因为我们可能会丢弃大量的数据。我们也可以用变量的均值来替代缺失的数据（如果这些变量是数字型的）。这种方法比丢弃数据更理想，因为它能保留我们的数据集。然而，用均值替代缺失值往往会影响数据的分布，这可能会对我们的机器学习模型产生负面影响。另一种方法是根据其他存在的值预测缺失值。然而，我们必须小心，因为这样做可能会引入显著的偏差。
- en: Lastly, in **Data Preprocessing**, we need to split the dataset into a training
    and testing dataset. Our machine learning models will be trained and fitted only
    on the training set. Once we are satisfied with the performance of our model,
    we will then evaluate our model using the testing dataset. Note that our model
    should never be trained on the testing set. This ensures that the evaluation of
    model performance is unbiased, and will reflect its real-world performance.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在**数据预处理**中，我们需要将数据集拆分为训练集和测试集。我们的机器学习模型只会在训练集上进行训练和拟合。一旦我们对模型的表现满意，我们将使用测试集对模型进行评估。需要注意的是，模型绝不应在测试集上进行训练。这确保了模型性能的评估是公正的，并能反映其在现实世界中的表现。
- en: Once **Data Preprocessing** has been completed, we will move on to **Exploratory
    Data Analysis** (**EDA**). EDA is the process of uncovering insights from your
    data using data visualization. EDA allows us to construct new features (known
    as feature engineering) and inject domain knowledge into our machine learning
    models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦**数据预处理**完成，我们将进入**探索性数据分析**（**EDA**）。EDA是通过数据可视化从数据中发掘洞察的过程。EDA使我们能够构造新的特征（称为特征工程），并将领域知识注入到我们的机器学习模型中。
- en: Finally, we get to the heart of machine learning. After **Data Preprocessing**
    and EDA have been completed, we move on to **Model Building**. As mentioned in
    the earlier section, there are several machine learning algorithms at our disposal,
    and the nature of the problem should dictate the type of machine learning algorithm
    used. In this book, we will focus on neural networks. In **Model Building**, **Hyperparameter
    Tuning** is an essential step, and the right hyperparameters can drastically improve
    the performance of our model. In a later section, we will look at some of the hyperparameters
    in a neural network. Once the model has been trained, we are finally ready to
    evaluate our model using the testing set.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们进入机器学习的核心部分。在完成**数据预处理**和 EDA 之后，我们进入**模型构建**阶段。如前文所述，我们可以使用多种机器学习算法，具体选择哪种算法应由问题的性质决定。在本书中，我们将专注于神经网络。在**模型构建**中，**超参数调优**是一个至关重要的步骤，正确的超参数能够大幅提升模型的性能。在后续章节中，我们将探讨神经网络中的一些超参数。训练完模型后，我们终于可以使用测试集来评估我们的模型了。
- en: As we can see, the machine learning workflow consists of many intermediate steps,
    each of which are crucial to the overall performance of our model. The major advantage
    of using Python for machine learning is that the entire machine learning workflow
    can be executed end-to-end entirely in Python, using just a handful of open source
    libraries. In this book, you will gain experience using Python in each step of
    the machine learning workflow, as you create sophisticated neural network projects
    from scratch.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，机器学习的工作流程包含许多中间步骤，每个步骤都对我们模型的整体表现至关重要。使用 Python 进行机器学习的主要优势是，整个机器学习工作流程可以完全在
    Python 中端到端地执行，并且只需要使用少数开源库。在本书中，你将体验到如何在机器学习工作流程的每个步骤中使用 Python，并从零开始创建复杂的神经网络项目。
- en: Setting up your computer for machine learning
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为机器学习配置你的计算机
- en: Before we dive deeper into neural networks and machine learning, let's make
    sure that you have set up your computer properly, so that you can run the code
    in this book smoothly.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究神经网络和机器学习之前，让我们确保你的计算机已经正确设置，这样你就可以顺利运行本书中的代码了。
- en: In this book, we will use the Python programming language for each neural network
    project. Along with Python itself, we also require several Python libraries, such
    as Keras, pandas, NumPy, and many more. There are several ways to install Python
    and the required libraries, but the easiest way by far is to use Anaconda.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用 Python 编程语言进行每个神经网络项目的开发。除了 Python 本身，我们还需要一些 Python 库，如 Keras、pandas、NumPy
    等等。安装 Python 和所需库有多种方式，但最简单的方式无疑是使用 Anaconda。
- en: Anaconda is a free and open source distribution of Python and its libraries.
    Anaconda provides a handy package manager that allows us to easily install Python
    and all other libraries that we require. To install Anaconda, simply head to the
    website at [https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/) and
    download the Anaconda installer (select the Python 3.x installer).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Anaconda 是一个免费的开源 Python 及其库的发行版。Anaconda 提供了一个方便的包管理器，让我们能够轻松安装 Python 以及所有其他所需的库。要安装
    Anaconda，只需访问网站：[https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/)，然后下载
    Anaconda 安装程序（选择 Python 3.x 版本的安装程序）。
- en: Besides Anaconda, we also require Git. Git is essential for machine learning
    and software engineering in general. Git allows us to easily download code from
    GitHub, which is probably the most widely used software hosting service. To install
    Git, head to the Git website at [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).
    You can simply download and run the appropriate installer for your OS.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Anaconda，我们还需要 Git。Git 对于机器学习和软件工程至关重要。Git 使我们能够轻松地从 GitHub 下载代码，而 GitHub
    可能是最广泛使用的软件托管服务。要安装 Git，请访问 Git 网站：[https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)。你可以直接下载并运行适合你操作系统的安装程序。
- en: Once Anaconda and Git are installed, we are ready to download the code for this
    book. The code that you see in this book can be found in our accompanying GitHub
    repository.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了 Anaconda 和 Git，我们就可以开始下载本书的代码了。本书中的代码可以在我们的 GitHub 仓库中找到。
- en: 'To download the code, simply run the following command from a command line
    (use Terminal if you''re using macOS/Linux, and if you''re using Windows, use
    the Anaconda Command Prompt):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载代码，只需在命令行中运行以下命令（如果你使用 macOS/Linux，请使用 Terminal；如果你使用 Windows，请使用 Anaconda
    命令提示符）：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `git clone` command will download all the Python code in this book to your
    computer.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`git clone` 命令将会下载本书中所有的 Python 代码到你的计算机上。'
- en: 'Once that''s done, run the following command to move into the folder that you
    just downloaded:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，运行以下命令来进入刚刚下载的文件夹：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Within the folder, you will find a file titled `environment.yml`. With this
    file, we can install Python and all the required libraries into a virtual environment.
    You can think of a virtual environment as an isolated, sandboxed environment where
    we can install a fresh copy of Python and all the required libraries. The `environment.yml`
    file contains instructions for Anaconda to install a specific version of each
    library into a virtual environment. This ensures that the Python code will be
    executed in a standardized environment that we have designed.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件夹中，你会找到一个名为 `environment.yml` 的文件。有了这个文件，我们可以将 Python 和所有必需的库安装到一个虚拟环境中。你可以把虚拟环境想象成一个隔离的、沙盒式的环境，在这里我们可以安装一个全新的
    Python 和所有必需的库。`environment.yml` 文件包含了 Anaconda 安装每个库特定版本的指令，确保 Python 代码将在我们设计的标准化环境中执行。
- en: 'To install the required dependencies using Anaconda and the `environment.yml`
    file, simply execute the following command from a command line:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Anaconda 和 `environment.yml` 文件安装所需的依赖项，只需从命令行执行以下命令：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Just like that, Anaconda will install all required packages into a `neural-network-projects-python`
    virtual environment. To enter this virtual environment, we execute this next command:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 就像这样，Anaconda 将会在 `neural-network-projects-python` 虚拟环境中安装所有必需的包。要进入这个虚拟环境，我们执行以下命令：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'That''s it! We are now in a virtual environment with all dependencies installed.
    To execute a Python file in this virtual environment, we can run something like
    this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们现在处于一个已安装所有依赖项的虚拟环境中。要在这个虚拟环境中执行 Python 文件，我们可以运行类似以下的命令：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To leave the virtual environment, we can run the following command:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要离开虚拟环境，我们可以运行以下命令：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Just note that you should be within the virtual environment (by running `conda
    activate neural-network-projects-python` first) whenever you run any Python code
    provided by us.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，无论何时运行我们提供的任何 Python 代码，你都应该在虚拟环境中（首先运行 `conda activate neural-network-projects-python`）。
- en: Now that we've set up our computer, let's return back to neural networks. We'll
    look at the theory behind neural networks, and how to program one from scratch
    in Python.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经配置好了我们的计算机，让我们回到神经网络。我们将看看神经网络背后的理论，以及如何在 Python 中从头开始编程一个。
- en: Neural networks
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: Neural networks are a class of machine learning algorithms that are loosely
    inspired by neurons in the human brain. However, without delving too much into
    brain analogies, I find it easier to simply describe neural networks as a mathematical
    function that maps a given input to the desired output. To understand what that
    means, let's take a look at a single layer neural network (known as a perceptron).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一类机器学习算法，它们受到人脑中神经元的松散启发。然而，不深入讨论大脑类比，我觉得更容易简单地将神经网络描述为一个将给定输入映射到所需输出的数学函数。为了理解这意味着什么，让我们来看看单层神经网络（即感知器）。
- en: 'A **Perceptron** can be illustrated with the following diagram:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知器**可以用以下图示来说明：'
- en: '![](img/c2e5d068-6510-410a-8d7d-4d5f4759683e.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2e5d068-6510-410a-8d7d-4d5f4759683e.png)'
- en: 'At its core, the **Perceptron** is simply a mathematical function that takes
    in a set of inputs, performs some mathematical computation, and outputs the result
    of the computation. In this case, that mathematical function is simply this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知器**的核心是一个简单的数学函数，它接受一组输入，执行一些数学计算，并输出计算结果。在这种情况下，这个数学函数就是这样的：'
- en: '![](img/bdb4a2b7-f8d8-4120-bdeb-3f745715d1ca.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bdb4a2b7-f8d8-4120-bdeb-3f745715d1ca.png)'
- en: '![](img/50c8a031-2d2f-42a6-965d-378cae83b9ce.png) refers to the weights of
    the **Perceptron**. We will explain what the weights in a neural network refers
    to in the next few sections. For now, we just need to keep in mind that neural
    networks are simply mathematical functions that map a given input to a desired
    output.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/50c8a031-2d2f-42a6-965d-378cae83b9ce.png) 表示**感知器**的权重。我们将在接下来的几节中解释神经网络中权重的含义。现在，我们只需记住神经网络简单地是一个将给定输入映射到期望输出的数学函数。'
- en: Why neural networks?
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择神经网络？
- en: Before we dive into creating our own neural network, it is worth understanding
    why neural networks have gained such an important foothold in machine learning
    and AI.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入创建自己的神经网络之前，了解神经网络为何在机器学习和人工智能中占据如此重要的位置是值得的。
- en: The first reason is that neural networks are universal function approximators.
    What that means is that given any arbitrary function that we are trying to model,
    no matter how complex, neural networks are alwaysable to represent that function.
    This has a profound implication on neural networks and AI in general. Assuming
    that any problem in the world can be described by a mathematical function (no
    matter how complex), we can use neural networks to represent that function, effectively
    modeling anything in the world. A caveat to this is that while scientists have
    proved the universality of neural networks, a large and complex neural network
    may never be trained and generalized correctly.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原因是神经网络是通用的函数逼近器。这意味着，对于我们要建模的任何任意函数，无论多么复杂，神经网络总是能够表示这个函数。这对于神经网络和人工智能来说具有深远的意义。假设世界上任何问题都可以通过一个数学函数来描述（无论多么复杂），我们可以利用神经网络来表示这个函数，从而有效地建模世界上的任何事物。需要注意的是，尽管科学家们已经证明了神经网络的通用性，但一个庞大且复杂的神经网络可能永远无法正确地训练和泛化。
- en: The second reason is that the architecture of neural networks are highly scalable
    and flexible. As we will see in the next section, we can easily stack layers in
    each neural network, increasing the complexity of the neural network. Perhaps
    more interestingly, the capabilities of neural networks are only limited by our
    own imagination. Through creative neural network architecture design, machine
    learning engineers have learned how to use neural networks to predict time series
    data (known as **recurrent neural networks** (**RNNs**)), which are used in areas
    such as speech recognition. In recent years, scientists have also shown that by
    pitting two neural networks against each other in a contest (known as a **generative
    adversarial network** (**GAN**)), we can generate photorealistic images that are
    indistinguishable to the human eye.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是神经网络的架构具有高度的可扩展性和灵活性。正如我们将在下一节中看到的那样，我们可以轻松地在每个神经网络中堆叠层次，从而增加神经网络的复杂性。或许更有趣的是，神经网络的能力仅受限于我们自己的想象力。通过创造性的神经网络架构设计，机器学习工程师已经学会如何利用神经网络来预测时间序列数据（称为**递归神经网络**（**RNNs**）），这些网络被广泛应用于语音识别等领域。近年来，科学家们还证明，通过让两个神经网络相互对抗（称为**生成对抗网络**（**GAN**）），我们可以生成与人眼难以区分的逼真图像。
- en: The basic architecture of neural networks
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络的基本架构
- en: In this section, we will look at the basic architecture of neural networks,
    the building blocks on which all complex neural networks are based. We will also
    code up our own basic neural network from scratch in Python, without any machine
    learning libraries. This exercise will help you gain an intuitive understanding
    of the inner workings of neural networks.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将了解神经网络的基本架构，这是所有复杂神经网络的基础构建模块。我们还将用 Python 从零开始编写自己的基本神经网络，且不依赖任何机器学习库。这个练习将帮助你直观地理解神经网络的内部工作原理。
- en: 'Neural networks consist of the following components:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由以下几个组件构成：
- en: An input layer, *x*
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层，*x*
- en: An arbitrary amount of hidden layers
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任意数量的隐藏层
- en: An output layer, *ŷ*
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层，*ŷ*
- en: A set of weights and biases between each layer, *W* and *b*
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每层之间的一组权重和偏置，*W* 和 *b*
- en: A choice of activation function for each hidden layer, ***σ***
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个隐藏层的激活函数选择，***σ***
- en: 'The following diagram shows the architecture of a two-layer neural network
    (note that the input layer is typically excluded when counting the number of layers
    in a neural network):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个两层神经网络的架构（请注意，输入层通常在计算神经网络层数时是被排除的）：
- en: '![](img/979cbfd4-9395-480d-9f30-6eb520569a9d.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/979cbfd4-9395-480d-9f30-6eb520569a9d.png)'
- en: Training a neural network from scratch in Python
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Python 中从零开始训练神经网络
- en: Now that we understand the basic architecture of a neural network, let's create
    our own neural network from scratch in Python.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经理解了神经网络的基本架构，那么让我们用 Python 从头开始创建自己的神经网络。
- en: 'First, let''s create a `NeuralNetwork` class in Python:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在 Python 中创建一个 `NeuralNetwork` 类：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice that in the preceding code, we initialize the weights (`self.weights1`
    and `self.weights2`) as a NumPy array with random values. NumPy arrays are used
    to represent multidimensional arrays in Python. The exact dimensions of our weights
    are specified in the parameters of the `np.random.rand()` function. For the dimensions
    of the first weight array, we use a variable (`self.input.shape[1]`) to create
    an array of variable dimensions, depending on the size of our input.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的代码中，我们初始化权重（`self.weights1`和`self.weights2`）为一个具有随机值的NumPy数组。NumPy数组用于表示Python中的多维数组。我们的权重的确切维度在`np.random.rand()`函数的参数中指定。对于第一个权重数组的维度，我们使用一个变量（`self.input.shape[1]`）来创建一个具有可变维度的数组，具体取决于我们输入的大小。
- en: 'The output, *ŷ*,of a simple two-layer neural network is as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的两层神经网络的输出*ŷ*如下所示：
- en: '![](img/c2f686f1-a94d-4f30-be9e-d29480326b24.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2f686f1-a94d-4f30-be9e-d29480326b24.png)'
- en: You might notice that in the preceding equation, the weights, *W*,and the biases, *b*,are
    the only variables that affects the output, *ŷ*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会注意到，在前述方程中，权重*W*和偏置*b*是影响输出*ŷ*的唯一变量。
- en: Naturally, the right values for the weights and biases determine the strength
    of the predictions. The process of fine-tuning the weights and biases from the
    input data is known as training the neural network.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，权重和偏置的正确值决定了预测的强度。从输入数据中微调权重和偏置的过程称为神经网络的训练。
- en: 'Each iteration of the training process consists of the following steps:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程的每一次迭代包括以下步骤：
- en: Calculating the predicted output **ŷ**,known as **Feedforward**
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算预测输出**ŷ**，即**前向传播**
- en: Updating the weights and biases, known as **Backpropagation**
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重和偏置，被称为**反向传播**
- en: 'The following sequential graph illustrates the process:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的顺序图说明了这个过程：
- en: '![](img/e05188ed-5f65-4c56-9934-cb58361d5f3e.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e05188ed-5f65-4c56-9934-cb58361d5f3e.png)'
- en: Feedforward
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前向传播
- en: 'As we''ve seen in the preceding sequential graph, feedforward is just simple
    calculus, and for a basic two-layer neural network, the output of the neural network
    is as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的顺序图中所看到的，前向传播只是简单的微积分，对于一个基本的两层神经网络，神经网络的输出如下：
- en: '![](img/5a5455e6-935d-4c34-82a5-102f26b78c2f.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a5455e6-935d-4c34-82a5-102f26b78c2f.png)'
- en: 'Let''s add a `feedforward` function in our Python code to do exactly that.
    Note that for simplicity, we have assumed the biases to be `0`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的Python代码中添加一个`feedforward`函数来完成这个任务。请注意，为了简单起见，我们假设偏置为`0`：
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: However, we still need a way to evaluate the accuracy of our predictions (that
    is, how far off our predictions are). The `loss` function allows us to do exactly
    that.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍然需要一种方法来评估我们预测的准确性（即我们的预测有多大的偏差）。`loss`函数正是允许我们做到这一点的方法。
- en: The loss function
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`loss`函数'
- en: 'There are many available `loss` functions, and the nature of our problem should
    dictate our choice of `loss` function. For now, we''ll use a simple *S**um-of-Squares
    Error* as our `loss` function:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多可用的`loss`函数，我们问题的性质应该决定我们选择的`loss`函数。目前，我们将使用简单的*平方误差和*作为我们的`loss`函数：
- en: '![](img/7e86faa7-73f3-4d34-9bf9-3c47dad08b5d.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e86faa7-73f3-4d34-9bf9-3c47dad08b5d.png)'
- en: The *sum-of-squares error* is simply the sum of the difference between each
    predicted value and the actual value. The difference is squared so that we measure
    the absolute value of the difference.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*平方误差和*就是每个预测值与实际值之间的差异的平方和。差异被平方以便我们可以测量差异的绝对值。'
- en: Our goal in training is to find the best set of weights and biases that minimizes
    the `loss` function.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练中的目标是找到最佳的权重和偏置集，以最小化`loss`函数。
- en: Backpropagation
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: Now that we've measured the error of our prediction (loss), we need to find
    a way to propagate the error back, and to update our weights and biases.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经测量了我们预测的误差（损失），我们需要找到一种方法来将误差向后传播，并更新我们的权重和偏置。
- en: In order to know the appropriate amount to adjust the weights and biases by,
    we need to know the derivative of the `loss` function with respect to the weights
    and biases.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了知道我们应该调整权重和偏置的适当数量，我们需要知道`loss`函数对于权重和偏置的导数。
- en: 'Recall from calculus that the derivative of a function is simply the slope
    of the function:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从微积分中可以知道，函数的导数就是函数的斜率：
- en: '![](img/603d354a-7d0f-414f-8436-0f6b0f800c06.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/603d354a-7d0f-414f-8436-0f6b0f800c06.png)'
- en: If we have the derivative, we can simply update the weights and biases by increasing/reducing
    with it (refer to the preceding diagram). This is known as **gradient descent**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有导数，就可以通过增大/减小权重和偏置来更新它们（参见前面的图示）。这就是**梯度下降**。
- en: However, we can't directly calculate the derivative of the `loss` function with
    respect to the weights and biases because the equation of the `loss` function
    does not contain the weights and biases. We need the chain ruleto help us calculate
    it. At this point, we are not going to delve into the chain rule because the math
    behind it can be rather complicated. Furthermore, machine learning libraries such
    as Keras takes care of gradient descent for us without requiring us to work out
    the chain rule from scratch. The key idea that we need to know is that once we
    have the derivative (slope) of the `loss` function with respect to the weights,
    we can adjust the weights accordingly.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不能直接计算 `loss` 函数相对于权重和偏置的导数，因为 `loss` 函数的方程中并不包含权重和偏置。我们需要链式法则来帮助我们计算它。在这一点上，我们不打算深入探讨链式法则，因为它背后的数学相当复杂。此外，像Keras这样的机器学习库已经为我们处理了梯度下降，不需要我们从头开始推导链式法则。我们需要了解的关键思想是，一旦我们得到
    `loss` 函数关于权重的导数（斜率），我们就可以相应地调整权重。
- en: 'Now let''s add the `backprop` function into our Python code:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们把 `backprop` 函数添加到我们的Python代码中：
- en: '[PRE8]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice that in the preceding code, we used a `sigmoid` function in the feedforward
    function. The `sigmoid` function is an activation function to *squash* the values
    between `0` and `1`. This is important because we need our predictions to be between
    `0` and `1` for this binary prediction problem. We will go through the `sigmoid`
    activation function in greater detail in the next chapter, [Chapter 2](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml),
    *Predicting Diabetes with Multilayer Perceptrons*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在之前的代码中，我们在前馈函数中使用了 `sigmoid` 函数。`sigmoid` 函数是一种激活函数，用来*压缩*值到 `0` 和 `1`
    之间。这很重要，因为我们需要将预测值限制在 `0` 和 `1` 之间，以解决这个二元预测问题。我们将在下一章[第2章](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml)《用多层感知机预测糖尿病》中更详细地讲解
    `sigmoid` 激活函数。
- en: Putting it all together
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 综合起来看
- en: Now that we have our complete Python code for doing feedforward and backpropagation,
    let's apply our neural network on an example and see how well it does.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了完整的Python代码来进行前馈和反向传播，接下来让我们在一个例子上应用我们的神经网络，看看它的表现如何。
- en: 'The following table contains four data points, each with three input variables
    ( *x[1]*, *x[2]*, and *x[3]*) and a target variable (*Y*):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下表包含了四个数据点，每个数据点有三个输入变量（*x[1]*，*x[2]*，*x[3]*）和一个目标变量（*Y*）：
- en: '| **x[1]** | **x[2]** | **x[3]** | **Y** |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **x[1]** | **x[2]** | **x[3]** | **Y** |'
- en: '| 0 | 0 | 1 | 0 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 1 | 0 |'
- en: '| 0 | 1 | 1 | 1 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 1 | 1 |'
- en: '| 1 | 0 | 1 | 1 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 | 1 |'
- en: '| 1 | 1 | 1 | 0 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 | 0 |'
- en: Our neural network should learn the ideal set of weights to represent this function.
    Note that it isn't exactly trivial for us to work out the weights just by inspection
    alone.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络应该能学到表示这个函数的理想权重集合。需要注意的是，仅仅通过观察我们并不容易算出这些权重。
- en: 'Let''s train the neural network for 1,500 iterations and see what happens.
    Looking at the following loss-per-iteration graph, we can clearly see the loss monotonically
    decreasing toward a minimum. This is consistent with the gradient descent algorithm
    that we discussed earlier:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练神经网络1500次迭代，看看会发生什么。从以下每次迭代的损失图表中，我们可以清晰地看到损失单调递减，趋向最小值。这与我们之前讨论的梯度下降算法是一致的：
- en: '![](img/2237ea9f-d19e-4b91-a4cc-8315e58c47d0.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2237ea9f-d19e-4b91-a4cc-8315e58c47d0.png)'
- en: 'Let''s look at the final prediction (output) from the neural network after
    1,500 iterations:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看神经网络在1500次迭代后的最终预测（输出）：
- en: '| **Prediction** | **Y (Actual)** |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| **预测** | **Y（实际）** |'
- en: '| 0.023 | 0 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 0.023 | 0 |'
- en: '| 0.979 | 1 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 0.979 | 1 |'
- en: '| 0.975 | 1 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 0.975 | 1 |'
- en: '| 0.025 | 0 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 0.025 | 0 |'
- en: We did it! Our feedforward and backpropagation algorithm trained the neural
    network successfully and the predictions converged on the true values.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做到了！我们的前馈和反向传播算法成功地训练了神经网络，预测结果已经收敛到真实值。
- en: Note that there's a slight difference between the predictions and the actual
    values. This is desirable, as it prevents overfitting and allows the neural network
    to generalizebetter to unseen data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，预测值与实际值之间有些许差异。这是理想的，因为它能防止过拟合，并且使得神经网络能更好地**泛化**到未见过的数据。
- en: Now that we understand the inner workings of a neural network, we will introduce
    the machine learning libraries in Python that we will use for the rest of the
    book. Don't worry if you find it difficult to create your own neural network from
    scratch at this point. For the rest of the book, we'll be using libraries that
    will greatly simplify the process of building and training a neural network.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了神经网络的内部工作原理，接下来将介绍我们在本书中将使用的Python机器学习库。如果你此时觉得从零开始创建自己的神经网络有些困难，不用担心。在本书接下来的部分中，我们将使用那些能够大大简化构建和训练神经网络过程的库。
- en: Deep learning and neural networks
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习与神经网络
- en: What about deep learning? How is it different from neural networks? To put it
    simply, deep learning is a machine learning algorithm that uses multiple layers
    in a neural network for learning (also known as deep nets). While we can think
    of a single-layer perceptron as the simplest neural network, deep nets are simply
    neural networks on the opposite end of the complexity spectrum.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 那深度学习呢？它与神经网络有何不同？简单来说，深度学习是一种使用神经网络中多个层次进行学习的机器学习算法（也称为深度网络）。虽然我们可以将单层感知机看作是最简单的神经网络，但深度网络则是神经网络在复杂性谱系的另一端。
- en: In a **deep neural network** (**DNN**), each layer learns information of increasing
    complexity, before passing it to successive layers. For example, when a DNN is
    trained for the purpose of facial recognition, the first few layers learn to identify
    edges in faces, followed by contours such as eyes and eventually complete facial
    features.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在**深度神经网络**（**DNN**）中，每一层都会学习越来越复杂的信息，然后将其传递给后续的层。例如，当一个DNN被训练用于人脸识别时，前几层学习识别人脸的边缘，接着是眼睛等轮廓，最终学习完整的面部特征。
- en: Although perceptrons were introduced back in the 1950s, deep learning did not
    take off until a few years ago. A key reason for the relatively slow progress
    of deep learning in the past few centuries is largely due to a lack of data and
    a lack of computation power. In the past few years, however, we have witnessed
    deep learning driving key innovations in machine learning and AI. Today, deep
    learning is the algorithm of choice when it comes to image recognition, autonomous
    vehicles, speech recognition, and game playing. So, what changed over the last
    few years?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知机早在1950年代就被提出，深度学习直到几年前才真正兴起。过去几个世纪深度学习进展相对缓慢的一个关键原因，主要是由于缺乏数据和计算能力。然而，在过去几年中，我们已经见证了深度学习推动机器学习和人工智能的关键创新。如今，深度学习已成为图像识别、自动驾驶、语音识别和游戏对战的首选算法。那么，过去几年发生了什么变化呢？
- en: In recent years, computer storage has become affordable enough to collect and
    store the massive amount of data that deep learning requires. It is becoming increasingly
    affordable to keep massive amount of data in the cloud, where it can be accessed
    by a cluster of computers from anywhere on earth. With the affordability of data
    storage, data is also becoming democratized. For example, websites such as ImageNet
    provides 14 million different images for deep learning researchers. Data is no
    longer a commodity that is owned by a privileged few.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，计算机存储变得足够经济实惠，可以收集和存储深度学习所需的大量数据。如今，存储海量数据在云端变得愈加可负担，并且可以通过一组计算机在地球上的任何地方进行访问。随着数据存储的可负担性提升，数据也变得越来越民主化。例如，像ImageNet这样的网页为深度学习研究者提供了1400万张不同的图像。数据不再是少数特权人士所拥有的商品。
- en: The computational power that deep learning requires is also becoming more affordable
    and powerful. Most of deep learning today is powered by **graphics processing
    units** (**GPUs**), which excel in the computation required by DNNs. Keeping with
    the theme of democratization, many websites also provides free GPU processing
    power for deep learning enthusiasts. For example, Google Colab provides a free
    Tesla K80 GPU in the cloud for deep learning, available for anyone to use.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习所需的计算能力也变得更加经济实惠和强大。如今，大多数深度学习工作依赖于**图形处理单元**（**GPU**），它们在执行深度神经网络（DNN）所需的计算任务时表现出色。为了进一步推动数据民主化，许多网站也为深度学习爱好者提供免费的GPU计算能力。例如，Google
    Colab为深度学习提供了免费的Tesla K80 GPU，任何人都可以使用。
- en: With these recent advancements, deep learning is becoming available to everyone.
    In the next few sections, we will introduce the Python libraries that we will
    use for deep learning.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 随着这些最新进展，深度学习正在变得对每个人都可用。在接下来的几个部分中，我们将介绍我们将在深度学习中使用的Python库。
- en: pandas – a powerful data analysis toolkit in Python
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas – Python中的强大数据分析工具包
- en: pandas is perhaps the most ubiquitous library in Python for data analysis. Built
    upon the powerful NumPy library, pandas provides a fast and flexible data structure
    in Python for handling real-world datasets. Raw data is often presented in tabular
    form, shared using the `.csv` file format. pandas provides a simple interface
    for importing these `.csv` files into a data structure known as DataFrames that
    makes it extremely easy to manipulate data in Python.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 可能是 Python 中用于数据分析的最普及库。pandas 基于强大的 NumPy 库，提供了一个快速且灵活的数据结构，用于处理现实世界的数据集。原始数据通常以表格形式呈现，使用
    `.csv` 文件格式共享。pandas 提供了一个简单的接口，用于将这些 `.csv` 文件导入到一个被称为 DataFrame 的数据结构中，这使得在
    Python 中操作数据变得极其简单。
- en: pandas DataFrames
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas DataFrame
- en: 'pandas DataFrames are two-dimensional data structures, which you can think
    of as spreadsheets in Excel. DataFrames allow us to easily import the `.csv` files
    using a simple command. For example, the following sample code allows us to import
    the `raw_data.csv` file:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: pandas DataFrame 是二维数据结构，可以将其看作 Excel 中的电子表格。DataFrame 允许我们使用简单的命令轻松导入 `.csv`
    文件。例如，下面的示例代码允许我们导入 `raw_data.csv` 文件：
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once the data is imported as a DataFrame, we can easily perform data preprocessing
    on it. Let''s work through it using the Iris flower dataset. The Iris flower dataset
    is a commonly used dataset that contains data on the measurements (sepal length
    and width, petal length and width) of several classes of flowers. First, let''s
    import the dataset as provided for free by **University of California Irvine**
    (**UCI**). Notice that pandas is able to import a dataset directly from a URL:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据作为 DataFrame 被导入，我们就可以轻松地对其进行数据预处理。让我们使用鸢尾花数据集来演示。鸢尾花数据集是一个常用的数据集，包含了几种花卉的测量数据（萼片的长度和宽度，花瓣的长度和宽度）。首先，让我们导入由**加利福尼亚大学欧文分校**（**UCI**）免费提供的数据集。请注意，pandas
    可以直接从网址导入数据集：
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that it''s in a DataFrame, we can easily manipulate the data. First, let''s
    get a summary of the data as it is always important to know what kind of data
    we''re working with:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经进入 DataFrame，我们可以轻松地操作数据。首先，让我们获取数据的概述，因为了解我们正在处理什么样的数据总是很重要的：
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output will be as shown in the following screenshot:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下截图所示：
- en: '![](img/8594b08a-140e-4ab0-b189-fe08c6d0007f.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8594b08a-140e-4ab0-b189-fe08c6d0007f.png)'
- en: It looks like there are 150 rows in the dataset, with four numeric columns containing
    information regarding the `sepal_length` and `sepal_width`, along with the `petal_length`
    and `petal_width`. There is also one non-numeric column containing information
    regarding the class (that is, species) of the flowers.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来数据集中有 150 行，包含四个数值列，包含关于 `sepal_length` 和 `sepal_width` 的信息，以及关于 `petal_length`
    和 `petal_width` 的信息。还有一个非数值列，包含关于花卉类别（即物种）的信息。
- en: 'We can get a quick statistical summary of the four numeric columns by calling
    the `describe()` function:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用 `describe()` 函数来快速获取四个数值列的统计概要：
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下截图所示：
- en: '![](img/d55aa987-bbc2-43bd-8c6c-ab9420cee49a.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d55aa987-bbc2-43bd-8c6c-ab9420cee49a.png)'
- en: 'Next, let''s take a look at the first 10 rows of the data:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下数据的前 10 行：
- en: '[PRE13]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下截图所示：
- en: '![](img/21b44887-cd45-48e2-9a46-ddc688f77d40.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21b44887-cd45-48e2-9a46-ddc688f77d40.png)'
- en: 'Simple, isn''t it? pandas also allows us to perform data wrangling easily.
    For example, we can do the following to filter and select rows with `sepal_length`
    greater than `5.0`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 简单吧？pandas 还允许我们轻松地进行数据清洗。例如，我们可以通过以下方法来筛选并选择 `sepal_length` 大于 `5.0` 的行：
- en: '[PRE14]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下截图所示：
- en: '![](img/c819132e-6363-45c8-853e-e1e32062e3a9.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c819132e-6363-45c8-853e-e1e32062e3a9.png)'
- en: The `loc` command allows us to access a group of rows and columns.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`loc` 命令允许我们访问一组行和列。'
- en: Data visualization in pandas
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas 中的数据可视化
- en: EDA is perhaps one of the most important steps in the machine learning workflow,
    and pandas makes it extremely easy to visualize data in Python. pandas provides
    a high-level API for the popular `matplotlib` library, which makes it easy to
    construct plots directly from DataFrames.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: EDA 可能是机器学习工作流程中最重要的步骤之一，而 pandas 使得在 Python 中可视化数据变得非常容易。pandas 为流行的 `matplotlib`
    库提供了一个高级 API，使得直接从 DataFrame 构建图表变得轻而易举。
- en: 'As an example, let''s visualize the Iris dataset using pandas to uncover important
    insights. Let''s plot a scatterplot to visualize how `sepal_width` is related
    to `sepal_length`. We can construct a scatterplot easily using the `DataFrame.plot.scatter()`
    method, which is built into all DataFrames:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个例子，让我们使用pandas可视化Iris数据集，以揭示重要的见解。让我们绘制一个散点图来可视化`sepal_width`与`sepal_length`的关系。我们可以通过`DataFrame.plot.scatter()`方法轻松地构建一个散点图，该方法内置于所有DataFrame中：
- en: '[PRE15]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We''ll get a scatterplot, as shown in the following screenshot:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到一个散点图，如下图所示：
- en: '![](img/b6c4ba47-98da-48da-8c21-85d90e461773.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6c4ba47-98da-48da-8c21-85d90e461773.png)'
- en: From the scatterplot, we can notice some interesting insights. First, the relationship
    between `sepal_width` and `sepal_length` is dependent on the species. Setosa (dots)
    has a fairly linear relationship between `sepal_width` and `sepal_length`, while
    versicolor (triangle) and virginica (star) tends to have much greater `sepal_length`
    than Setosa. If we're designing a machine learning algorithm to predict the type
    of species of flower, we know that the `sepal_width` and `sepal_length` are important
    features to include in our model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 从散点图中，我们可以发现一些有趣的见解。首先，`sepal_width`和`sepal_length`之间的关系取决于物种。Setosa（圆点）在`sepal_width`和`sepal_length`之间有相当线性的关系，而versicolor（三角形）和virginica（星形）通常具有比Setosa更大的`sepal_length`。如果我们正在设计一个机器学习算法来预测花卉的物种类型，我们知道`sepal_width`和`sepal_length`是我们模型中需要包括的重要特征。
- en: 'Next, let''s plot a histogram to investigate the distribution. Consistent with
    scatterplots, pandas DataFrames provides a built in method to plot histograms
    using the `DataFrame.plot.hist()` function:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们绘制一个直方图来研究分布情况。与散点图一致，pandas DataFrame 提供了一个内置方法来使用`DataFrame.plot.hist()`函数绘制直方图：
- en: '[PRE16]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And we can see the output in the following screenshot:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下截图中看到输出结果：
- en: '![](img/bd8c1d3b-b859-485a-902f-59b5379a5ff3.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd8c1d3b-b859-485a-902f-59b5379a5ff3.png)'
- en: 'We can see that the distribution of petal lengths is essentially bimodal. It
    appears that certain species of flowers have shorter petals than the rest. We
    can also plot a boxplot of the data. The boxplot is an important data visualization
    tool used by data scientists to understand the distribution of the data based
    on the first quartile, median, and the third quartile:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到花瓣长度的分布本质上是双峰的。似乎某些物种的花瓣比其他物种更短。我们还可以绘制数据的箱型图。箱型图是数据科学家用来理解数据分布的重要可视化工具，基于第一四分位数、中位数和第三四分位数：
- en: '[PRE17]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is given in the following screenshot:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下图所示：
- en: '![](img/4f91a419-3b73-47d9-bf0b-9f988805dbea.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f91a419-3b73-47d9-bf0b-9f988805dbea.png)'
- en: From the boxplot, we can see that the variance of `sepal_width` is much smaller
    than the other numeric variables, with `petal_length` having the greatest variance.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 从箱型图中，我们可以看到`sepal_width`的方差远小于其他数值变量，而`petal_length`具有最大的方差。
- en: We have now seen how convenient and easy it is to visualize data using pandas
    directly. Keep in mind that EDA is a crucial step in the machine learning pipeline,
    and it is something that we will continue to do in every project for the rest
    of the book.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到，使用pandas直接可视化数据是多么方便和容易。请记住，EDA（探索性数据分析）是机器学习管道中的一个关键步骤，我们将在本书的每个项目中继续执行此操作。
- en: Data preprocessing in pandas
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: pandas中的数据预处理
- en: Lastly, let's take a look at how we can use pandas for data preprocessing, specifically
    to encode categorical variables and to impute missing values.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看如何使用pandas进行数据预处理，特别是编码分类变量和填补缺失值。
- en: Encoding categorical variables
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码分类变量
- en: 'In machine learning projects, it is common to receive datasets with categorical
    variables. Here are some examples of categorical variables in datasets:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目中，通常会收到包含分类变量的数据集。以下是数据集中分类变量的一些示例：
- en: '**Gender**: Male, female'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性别**：男，女'
- en: '**Day**:Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**星期**：星期一，星期二，星期三，星期四，星期五，星期六，星期天'
- en: '**Country**:USA, UK, China, Japan'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**国家**：美国，英国，中国，日本'
- en: Machine learning algorithms such as neural networks are unable to work with
    such categorical variables as they expect numerical variables. Therefore, we need
    to perform preprocessing on these variables before feeding them into a machine
    learning algorithm.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 像神经网络这样的机器学习算法无法处理分类变量，因为它们期望的是数值变量。因此，在将这些变量输入到机器学习算法之前，我们需要对其进行预处理。
- en: 'One common way to convert these categorical variables into numerical variables
    is a technique known as one-hot encoding, implemented by the `get_dummies()` function
    in pandas. One-hot encoding is a process that converts a categorical variable
    with `n` categories into `n` distinct binary features. An example is provided
    in the following table:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些类别变量转换为数值变量的一种常见方法是使用一种叫做独热编码（one-hot encoding）的技术，这在pandas中的`get_dummies()`函数中得到了实现。独热编码是一种将具有`n`个类别的类别变量转换为`n`个独特二进制特征的过程。以下表格展示了一个例子：
- en: '![](img/493047e9-352a-4f6d-8ad7-310528d2a7d5.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/493047e9-352a-4f6d-8ad7-310528d2a7d5.png)'
- en: 'Essentially, the transformed features are binary features with a **1** value
    if it represents the original feature, and **0** otherwise. As you can imagine,
    it would be a hassle to write the code for this manually. Fortunately, pandas
    has a handy function that does exactly that. First, let''s create a DataFrame
    in pandas using the data in the preceding table:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，转换后的特征是二进制特征，若它代表原始特征则为**1**，否则为**0**。正如你所想的，手动编写这段代码会很麻烦。幸运的是，pandas提供了一个非常方便的函数来完成这个工作。首先，让我们使用前面的表格数据在pandas中创建一个数据框：
- en: '[PRE18]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can see the output in the following screenshot:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在下面的截图中看到输出：
- en: '![](img/c4e56c5d-278d-4beb-afeb-6572cbe9486a.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4e56c5d-278d-4beb-afeb-6572cbe9486a.png)'
- en: 'To one-hot encode the preceding categorical feature using pandas, it is as
    simple as calling the following function:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pandas进行前述类别特征的独热编码非常简单，只需调用以下函数：
- en: '[PRE19]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here''s the output:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![](img/8b5a3920-12aa-4c02-8cde-0d330f4927f7.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b5a3920-12aa-4c02-8cde-0d330f4927f7.png)'
- en: Imputing missing values
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充缺失值
- en: As discussed earlier, imputing missing values is an essential part of the machine
    learning workflow. Real-world datasets are messy and usually contain missing values.
    Most machine learning models such as neural networks are unable to work with missing
    data, and hence we have to preprocess the data before we feed the data into our
    models. pandas makes it easy to handle missing values.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，填充缺失值是机器学习工作流中的一个关键步骤。现实世界中的数据集通常是杂乱的，并且包含缺失值。大多数机器学习模型，如神经网络，无法处理缺失数据，因此在将数据输入模型之前，我们必须进行数据预处理。pandas
    使得处理缺失值变得更加容易。
- en: 'Let''s use the Iris dataset from earlier. The Iris dataset does not have any
    missing values by default. Therefore, we have to delete some values on purpose
    for the sake of this exercise. The following code randomly selects `10` rows in
    the dataset, and deletes the `sepal_length` values in these `10` rows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前的鸢尾花数据集。默认情况下，鸢尾花数据集没有缺失值。因此，为了这个练习，我们必须故意删除一些值。以下代码随机选择数据集中的`10`行，并删除这`10`行中的`sepal_length`值：
- en: '[PRE20]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s use this modified dataset to see how we can deal with missing values. First,
    let''s check where our missing values are:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个修改后的数据集来看看如何处理缺失值。首先，让我们检查一下缺失值所在的位置：
- en: '[PRE21]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The preceding `print` function gives the following output:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的`print`函数给出的输出如下：
- en: '![](img/ab3a96aa-9d4c-43e5-a26a-1c9a4981616e.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab3a96aa-9d4c-43e5-a26a-1c9a4981616e.png)'
- en: Unsurprisingly, pandas tells us that there are missing (that is, null) values
    in the `sepal_length` column. This command is useful to find out which columns
    in our dataset contains missing values.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 不出所料，pandas告诉我们在`sepal_length`列中存在缺失值（即空值）。这个命令对于找出数据集中哪些列包含缺失值非常有用。
- en: 'One way to deal with missing values is to simply remove any rows with missing
    values. pandas provides a handy `dropna` function for us to do that:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失值的一种方法是简单地删除所有包含缺失值的行。pandas提供了一个非常方便的`dropna`函数来实现这一点：
- en: '[PRE22]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![](img/03ec86b0-1d20-45a2-a573-e1db0a2b9737.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03ec86b0-1d20-45a2-a573-e1db0a2b9737.png)'
- en: 'Another way is to replace the missing `sepal_length` values with the mean of
    the non-missing `sepal_length` values:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是用非缺失的`sepal_length`值的均值来替换缺失的`sepal_length`值：
- en: '[PRE23]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: pandas will automatically exclude the missing values when calculating the mean
    using `df.mean()`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`df.mean()`计算均值时，pandas会自动排除缺失值。
- en: 'Now let''s confirm that there are no missing values:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们确认没有缺失值：
- en: '![](img/6da172fd-6933-44e2-90af-951b5589e9db.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6da172fd-6933-44e2-90af-951b5589e9db.png)'
- en: With the missing values handled, we can then pass the DataFrame to machine learning
    models.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 处理完缺失值后，我们可以将数据框传递给机器学习模型。
- en: Using pandas in neural network projects
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在神经网络项目中使用pandas
- en: We have seen how pandas can be used to import tabular data in `.csv` format,
    and perform data preprocessing and data visualization directly using built-in
    functions in pandas. For the rest of the book, we will use pandas when the dataset
    is of a tabular nature. pandas plays a crucial role in data preprocessing and
    EDA, as we shall see in future chapters.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何使用 pandas 导入 `.csv` 格式的表格数据，并直接使用 pandas 中的内置函数进行数据预处理和数据可视化。在本书的其余部分，当数据集是表格形式时，我们将使用
    pandas。正如我们将在未来章节中看到的，pandas 在数据预处理和 EDA 中扮演着至关重要的角色。
- en: TensorFlow and Keras – open source deep learning libraries
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 和 Keras – 开源深度学习库
- en: TensorFlow is an open source library for neural networks and deep learning developed
    by the Google Brain team. Designed for scalability, TensorFlow runs across a variety
    of platforms, from desktops to mobile devices and even to clusters of computers.
    Today, TensorFlow is one of the most popular machine learning libraries and is
    used extensively in a wide variety of real-world applications. For example, TensorFlow
    powers the AI behind many online services that we use today, including image search,
    voice recognition, recommendation engines. TensorFlow has become the silent workhorse
    powering many AI applications, even though we might not even notice it.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是一个由 Google Brain 团队开发的开源神经网络和深度学习库。TensorFlow 旨在具备可扩展性，可以在各种平台上运行，从桌面到移动设备，甚至是计算机集群。如今，TensorFlow
    是最流行的机器学习库之一，并广泛应用于各种现实世界的应用中。例如，TensorFlow 驱动着我们今天使用的许多在线服务的 AI，包括图像搜索、语音识别和推荐引擎。TensorFlow
    已经成为许多 AI 应用程序背后的“默默工作者”，即使我们可能都没有注意到它的存在。
- en: Keras is a high-level API that runs on top of TensorFlow. So, why Keras? Why
    do we need another library to act as an API for TensorFlow? To put it simply,
    Keras removes the complexities in building neural networks, and enables rapid
    experimentation and testing without concerning the user with low-level implementation
    details. Keras provides a simple and intuitive API for building neural networks
    using TensorFlow. Its guiding principles are modularity and extensibility. As
    we shall see later, it is extremely easy to build neural networks by stacking
    Keras API calls on top of one another, which you can think of like stacking Lego
    blocks in order to create bigger structures. This beginner-friendly approach has
    led to the popularity of Keras as one of the top machine learning libraries in
    Python. In this book, we will use Keras as the primary machine learning library
    for building our neural network projects.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个运行在 TensorFlow 之上的高级 API。那么，为什么是 Keras？为什么我们需要另一个库作为 TensorFlow 的 API？简单来说，Keras
    去除了构建神经网络的复杂性，并使快速实验和测试变得容易，而无需让用户关注低级实现细节。Keras 为使用 TensorFlow 构建神经网络提供了一个简单直观的
    API。它的指导原则是模块化和可扩展性。正如我们稍后将看到的，通过将 Keras API 调用按顺序堆叠起来，构建神经网络非常简单，可以像堆积乐高积木一样，创建更大的结构。这种对初学者友好的方法使得
    Keras 成为了 Python 中最流行的机器学习库之一。在本书中，我们将使用 Keras 作为构建神经网络项目的主要机器学习库。
- en: The fundamental building blocks in Keras
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Keras 中的基本构建模块
- en: The fundamental building blocks in Keras are layers, and we can stack layers
    linearly to create a model. The **Loss Function **that we choose will provide
    the metrics for which we will use to train our model using an **Optimizer. **Recall
    that while building our neural network from scratch earlier, we had to define
    and write the code for those terms. We call these the fundamental building blocks
    in Keras because we can build any neural network using these basic structures.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 中的基本构建模块是层，我们可以将这些层按线性方式堆叠起来以创建一个模型。我们选择的**损失函数**将提供训练模型时使用的度量标准，而训练时使用的将是**优化器**。回想一下，在之前从头构建神经网络时，我们必须定义并编写这些术语的代码。我们称这些为
    Keras 中的基本构建模块，因为我们可以使用这些基本结构来构建任何神经网络。
- en: 'The following diagram illustrates the relationship between these building blocks
    in Keras:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了 Keras 中这些构建模块之间的关系：
- en: '![](img/081fd69c-7a42-4394-a198-a533a7e2892d.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/081fd69c-7a42-4394-a198-a533a7e2892d.png)'
- en: Layers – the atom of neural networks in Keras
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层 – Keras 中神经网络的基本单元
- en: You can think of layers in Keras as an atom, because they are the smallest unit
    of our neural network. Each layer takes in an input performs a mathematical function,
    then outputs that for the next layer. The core layers in Keras includes dense
    layers, activation layers, and dropout layers. There are other layers that are
    more complex, including convolutional layers and pooling layers. In this book,
    you will be exposed to projects that uses all these layers.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将Keras中的层看作原子，因为它们是我们神经网络中最小的单位。每一层接收输入，执行数学函数，然后将结果输出给下一层。Keras中的核心层包括全连接层、激活层和丢弃层。还有一些更复杂的层，包括卷积层和池化层。在本书中，你将接触到使用所有这些层的项目。
- en: For now, let's take a closer look at dense layers, which are by far the most
    common type of layer used in Keras. A dense layer is also known as a fully-connected
    layer. It is fully-connected because it uses all of its input (as opposed to a
    subset of the input) for the mathematical function that it implements.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地了解一下全连接层，这是Keras中最常用的层。全连接层也叫做密集层。之所以称为全连接，是因为它使用了所有的输入（而不是部分输入）来执行它实现的数学函数。
- en: 'A dense layer implements the following function:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接层实现以下功能：
- en: '![](img/1899d70e-e335-4861-a5b2-afdbc619f1d0.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1899d70e-e335-4861-a5b2-afdbc619f1d0.png)'
- en: '![](img/b4d15384-5e35-4805-9005-2c8d0eb02210.png) is the output, ![](img/cb669ce2-0c83-4127-ae60-d0105f4b99ed.png) is
    the activation function, ![](img/81a91630-b8d9-443e-ac60-657e76f6808c.png) is
    the input, and ![](img/5f5bd2fa-6f2c-408b-b07c-834d7b3f2462.png) and ![](img/825fd59c-a153-4f9e-97f5-41c17f287c57.png) are
    the weights and biases respectively.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/b4d15384-5e35-4805-9005-2c8d0eb02210.png)是输出，![](img/cb669ce2-0c83-4127-ae60-d0105f4b99ed.png)是激活函数，![](img/81a91630-b8d9-443e-ac60-657e76f6808c.png)是输入，![](img/5f5bd2fa-6f2c-408b-b07c-834d7b3f2462.png)和![](img/825fd59c-a153-4f9e-97f5-41c17f287c57.png)分别是权重和偏置。'
- en: This equation should look familiar to you. We used the fully-connected layer
    when we were building our neural network from scratch earlier.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程式对你来说应该很熟悉。我们在之前从头构建神经网络时，使用了全连接层。
- en: Models – a collection of layers
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型 – 层的集合
- en: If layers can be thought of as atoms, then models can be thought of as molecules
    in Keras. A model is simply a collection of layers, and the most commonly used
    model in Keras is the `Sequential` model. A `Sequential` model allows us to linearly
    stack layers on one another, where a single layer is connected to one other layer
    only. This allows us to easily design model architectures without worrying about
    the underlying math. As we will see in later chapters, there is a significant
    amount of thought needed to ensure that consecutive layer dimensions are compatible with
    one another, something that Keras takes care for us under the hood!
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将层看作是原子，那么在Keras中，模型可以看作是分子。模型只是层的集合，Keras中最常用的模型是`Sequential`模型。`Sequential`模型允许我们将层线性堆叠，其中每一层只与上一层连接。这使得我们能够轻松设计模型架构，而无需担心底层的数学原理。正如我们将在后面的章节中看到的，确保连续层维度兼容是需要大量思考的事情，而Keras已经在幕后为我们解决了这个问题！
- en: Once we have defined our model architecture, we need to define our training
    process, which is done using the `compile` method in Keras. The `compile`  method
    takes in several arguments, but the most important arguments we need to define
    is the optimizer and the loss function.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了我们的模型架构，就需要定义训练过程，使用Keras中的`compile`方法来完成。`compile`方法接受多个参数，但我们需要定义的最重要参数是优化器和损失函数。
- en: Loss function – error metric for neural network training
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失函数 – 神经网络训练的误差度量
- en: In an earlier section, we defined the loss function as a way to evaluate the
    goodness of our predictions (that is, how far off our predictions are). The nature
    of our problem should dictate the loss function used. There are several loss functions
    implemented in Keras, but the most commonly used loss functions are `mean_squared_error`, `categorical_crossentropy`,
    and `binary_crossentropy`.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们定义了损失函数作为评估我们预测结果好坏（即预测结果与实际结果的差距）的方式。我们使用的损失函数应该由问题的性质决定。Keras中实现了多种损失函数，但最常用的损失函数是`mean_squared_error`、`categorical_crossentropy`和`binary_crossentropy`。
- en: 'As a general rule of thumb, this is how you should choose which loss function
    to use:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，选择损失函数的原则如下：
- en: '`mean_squared_error` if the problem is a regression problem'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果问题是回归问题，使用`mean_squared_error`
- en: '`categorical_crossentropy` if the problem is a multiclass classification problem'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果问题是多分类问题，使用`categorical_crossentropy`
- en: '`binary_crossentropy` if the problem is a binary classification problem'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果问题是二分类问题，则使用 `binary_crossentropy`
- en: In certain cases, you might find that the default loss functions in Keras are
    unsuitable for your problem. In that case, you can define your own loss function
    by defining a custom function in Python, then passing that custom function to
    the `compile` method in Keras.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能会发现 Keras 中的默认损失函数不适合您的问题。在这种情况下，您可以通过在 Python 中定义一个自定义函数来定义自己的损失函数，然后将该自定义函数传递给
    Keras 中的 `compile` 方法。
- en: Optimizers – training algorithm for neural networks
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化器 – 神经网络的训练算法
- en: An optimizer is an algorithm for updating the weights of the neural network
    in the training process. Optimizers in Keras are based on the gradient descent
    algorithm, which we have covered in an earlier section.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器是用于在训练过程中更新神经网络权重的算法。Keras 中的优化器基于梯度下降算法，我们在前面的章节中已做过讲解。
- en: While we won't cover in detail the differences between each optimizer, it is
    important to note that our choice of optimizer should depend on the nature of
    the problem. In general, researchers have found that the `Adam` optimizer works
    best for DNNs, while the `sgd` optimizer works best for shallow neural networks.
    The `Adagrad` optimizer is also a popular choice, and it adapts the learning rate
    of the algorithm based on how frequent a particular set of weights are updated.
    The main advantage of this approach is that it eliminates the need to manually
    tune the learning rate hyperparameter, which is a time-consuming process in the
    machine learning workflow.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们不会详细讲解每个优化器之间的差异，但需要注意的是，我们选择优化器的依据应取决于问题的性质。一般而言，研究人员发现 `Adam` 优化器最适合深度神经网络（DNN），而
    `sgd` 优化器则最适合浅层神经网络。`Adagrad` 优化器也是一个流行的选择，它根据特定权重更新的频率来调整算法的学习率。这个方法的主要优点是，它消除了手动调整学习率超参数的需要，这是机器学习流程中的一项耗时工作。
- en: Creating neural networks in Keras
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Keras 中创建神经网络
- en: 'Let''s take a look at how we can use Keras to build the two-layer neural network
    that we introduced earlier. To build a linear collection of layers, first declare
    a `Sequential` model in Keras:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何使用 Keras 构建我们之前介绍的两层神经网络。为了构建一个线性堆叠的层，首先在 Keras 中声明一个 `Sequential`
    模型：
- en: '[PRE24]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This creates an empty  `Sequential` model that we can now add layers to. Adding
    layers in Keras is simple and similar to stacking Lego blocks on top of one another.
    We start by adding layers from the left (the layer closest to the input):'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个空的 `Sequential` 模型，我们现在可以向其中添加层。在 Keras 中添加层非常简单，类似于将乐高积木一个个叠加起来。我们从左边开始添加层（即离输入最近的层）：
- en: '[PRE25]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Stacking layers in Keras is as simple as calling the `model.add()` command.
    Notice that we had to define the number of units in each layer. Generally, increasing
    the number of units increases the complexity of the model, as it means that there
    are more weights to be trained. For the first layer, we had to define `input_dim`.
    This informs Keras the number of features (that is, columns) in the dataset. Also,
    note that we have used a `Dense` layer. A `Dense` layer is simply a fully connected
    layer. In later chapters, we will introduce other kinds of layers, specific to
    different types of problems.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中堆叠层就像调用 `model.add()` 命令一样简单。注意，我们必须定义每层的单元数量。通常，增加单元的数量会增加模型的复杂性，因为这意味着需要训练更多的权重。对于第一层，我们必须定义
    `input_dim`。这告诉 Keras 数据集中的特征数量（即列数）。此外，请注意，我们使用了 `Dense` 层。`Dense` 层仅仅是一个全连接层。在后续章节中，我们将介绍其他类型的层，专门用于不同类型的问题。
- en: 'We can verify the structure of our model by calling the `model.summary()` function:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用 `model.summary()` 函数来验证模型的结构：
- en: '[PRE26]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is shown in the following screenshot:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '![](img/9289d890-2b97-4c18-9e1b-9c1886620e33.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9289d890-2b97-4c18-9e1b-9c1886620e33.png)'
- en: The number of params is the number of weights and biases we need to train for
    the model that we have just defined.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 参数的数量是我们需要为刚定义的模型训练的权重和偏置的数量。
- en: 'Once we are satisfied with our model''s architecture, let''s compile it and
    start the training process:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对模型的架构感到满意，就可以编译它并开始训练过程：
- en: '[PRE27]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note that we have defined the learning rate of the `sgd` optimizer to be 1.0
    (`lr=1`). In general, the learning rate is a hyperparameter of the neural network
    that needs to be tuned carefully depending on the problem. We will take a closer
    look at tuning hyperparameters in later chapters.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已将 `sgd` 优化器的学习率设置为 1.0（`lr=1`）。一般来说，学习率是神经网络的一个超参数，需要根据具体问题仔细调整。在后续章节中，我们将详细探讨如何调整超参数。
- en: The `mean_squared_error` loss function in Keras is similar to the sum-of-squares
    error that we have defined earlier. We are using the SGDoptimizer to train our
    model. Recall that gradient descent is the method of updating the weights and
    biases by moving it toward the derivative of the loss function with respect to
    the weights and biases.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 中的 `mean_squared_error` 损失函数类似于我们之前定义的平方和误差。我们正在使用 SGD 优化器来训练模型。回顾一下，梯度下降法是通过将权重和偏差朝着损失函数相对于权重和偏差的导数方向更新的方式来优化模型。
- en: Let's use the same data that we used earlier to train our neural network. This
    will allow us to compare the predictions obtained using Keras versus the predictions
    obtained when we created our neural network from scratch earlier.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用之前使用过的数据来训练我们的神经网络。这将使我们能够比较使用 Keras 获得的预测结果与我们之前从零开始创建神经网络时获得的预测结果。
- en: 'Let''s define an `X` and `Y` NumPy array, corresponding to the features and
    the target variables respectively:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个 `X` 和 `Y` 的 NumPy 数组，分别对应特征和目标变量：
- en: '[PRE28]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, let''s train the model for `1500` iterations:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们训练模型 `1500` 次迭代：
- en: '[PRE29]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To get the predictions, run the `model.predict()` command on our data:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取预测结果，请在我们的数据上运行 `model.predict()` 命令：
- en: '[PRE30]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The preceding code gives the following output:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/fbc61878-0d50-4431-a6d6-913d66f5196c.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fbc61878-0d50-4431-a6d6-913d66f5196c.png)'
- en: Comparing this to the predictions that we obtained earlier, we can see that
    the results are extremely similar. The major advantage of using Keras is that
    we did not have to worry about the low-level implementation details and mathematics
    while building our neural network, unlike what we did earlier. In fact, we did
    no math at all. All we did in Keras was to call a series of APIs to build our
    neural network. This allows us to focus on high-level details, enabling rapid
    experimentation.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 将此与我们之前获得的预测结果进行比较，可以看到结果非常相似。使用 Keras 的主要优势是，我们在构建神经网络时无需担心底层实现细节和数学问题，这与我们之前的做法不同。实际上，我们完全没有进行任何数学运算。在
    Keras 中，我们所做的只是调用一系列 API 来构建神经网络。这使我们能够专注于高层次的细节，从而实现快速实验。
- en: Other Python libraries
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他 Python 库
- en: Besides pandas and Keras, we will also be using other Python libraries, such
    as scikit-learn and seaborn. scikit-learn is an open source machine learning library
    that is widely used in machine learning projects. The main functionality that
    we use in scikit-learn is to separate our data into a training and testing set
    during data preprocessing. seaborn is an alternative data visualization in Python
    that has been gaining traction recently. In the later chapters, we'll see how
    we can use seaborn to make data visualizations.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 pandas 和 Keras，我们还将使用其他 Python 库，如 scikit-learn 和 seaborn。scikit-learn 是一个开源的机器学习库，在机器学习项目中被广泛使用。我们在
    scikit-learn 中使用的主要功能是在数据预处理时将数据分为训练集和测试集。seaborn 是 Python 中的另一种数据可视化工具，近年来越来越受到关注。在后续章节中，我们将看到如何使用
    seaborn 来进行数据可视化。
- en: Summary
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen what machine learning is, and looked at the complete
    end-to-end workflow for every machine learning project. We have also seen what
    neural networks and deep learning is, and coded up our own neural network from
    scratch and in Keras.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解了什么是机器学习，并查看了每个机器学习项目的完整端到端工作流程。我们还了解了什么是神经网络和深度学习，并从零开始在 Keras 中编写了我们自己的神经网络。
- en: For the rest of the book, we will create our own real-world neural network projects.
    Each chapter will cover one project, and the projects are listed in order of increasing
    complexity. By the end of the book, you will have created your own neural network
    projects in medical diagnosis, taxi fare predictions, image classification, sentiment
    analysis, and much more. In the next chapter, [Chapter 2](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml),
    *Predicting Diabetes with Multilayer Perceptrons* we will cover diabetes prediction
    with **multilayer perceptrons** (**MLPs**). Let's get started!
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的剩余部分，我们将创建自己的实际神经网络项目。每一章将涵盖一个项目，项目的复杂性按顺序递增。到书的最后，你将完成自己的神经网络项目，包括医学诊断、出租车费用预测、图像分类、情感分析等更多内容。在下一章，[第2章](81c9304f-2e96-4a6d-8ece-d972006f3180.xhtml)，*使用多层感知器预测糖尿病*，我们将介绍使用**多层感知器**（**MLP**）进行糖尿病预测。让我们开始吧！
