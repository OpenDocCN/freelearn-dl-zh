- en: '*Chapter 13*: Other Advanced Topics'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第13章*：其他高级主题'
- en: In this chapter, we'll cover several advanced topics of reinforcement learning
    (RL). First of all, we'll go deeper into distributed RL, in addition to what we
    covered in the previous chapters. This area is key to dealing with excessive data
    needs to train agents for sophisticated tasks. Curiosity-driven RL handles hard-exploration
    problems that are not solvable by traditional exploration techniques. Offline
    RL leverages offline data to obtain good policies. All of these are hot research
    areas that you will hear more about in the next few years.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖强化学习（RL）的几个高级主题。首先，我们将深入探讨分布式强化学习，除了之前章节中涉及的内容之外。这个领域对于处理训练智能体进行复杂任务所需的过多数据至关重要。好奇驱动的强化学习处理传统探索技术无法解决的困难探索问题。离线强化学习利用离线数据来获得良好的策略。所有这些都是热门的研究领域，您将在未来几年听到更多相关内容。
- en: 'So, in this chapter, you will learn about the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在本章中，您将学习以下内容：
- en: Distributed RL
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式强化学习
- en: Curiosity-driven RL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好奇驱动的强化学习
- en: Offline RL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离线强化学习
- en: Let's get started!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Distributed reinforcement learning
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式强化学习
- en: As we've already mentioned in earlier chapters, training sophisticated RL agents
    requires massive amounts of data. While one critical area of research is to increase
    the sample efficiency in RL; the other, complementary direction is how to best
    utilize the compute power and parallelization and reduce the wall-clock time and
    cost of training. We've already covered, implemented, and used distributed RL
    algorithms and libraries in the earlier chapters. So, this section will be an
    extension of the previous discussions due to the importance of this topic. Here,
    we present additional material on state-of-the-art distributed RL architectures,
    algorithms, and libraries. With that, let's get started with SEED RL, an architecture
    designed for massive and efficient parallelization.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在之前的章节中提到的，训练复杂的强化学习智能体需要大量的数据。虽然研究的一个关键领域是提高强化学习中的样本效率；另一个互补方向则是如何最好地利用计算能力和并行化，减少训练的实际时间和成本。我们已经在前面的章节中讨论、实现并使用了分布式强化学习算法和库。因此，本节将是对之前讨论的扩展，因为这一话题非常重要。在这里，我们将介绍更多关于最先进的分布式强化学习架构、算法和库的内容。那么，让我们从SEED
    RL开始，这是一种为大规模和高效并行化设计的架构。
- en: Scalable, efficient deep reinforcement learning – SEED RL
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展、高效的深度强化学习 – SEED RL
- en: 'Let''s begin the discussion by revisiting the Ape-X architecture, which is
    a milestone in scalable RL. The key contribution of Ape-X is to decouple learning
    from acting: The actors generate experiences, at their own pace, the learner learns
    from the experiences at its own pace, and the actors update their local copies
    of the neural network policy periodically. An illustration of this flow for Ape-X
    DQN is given in *Figure 13.1*:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从重新审视Ape-X架构开始讨论，它是可扩展强化学习的一个里程碑。Ape-X的关键贡献是将学习和行动解耦：演员们按自己的节奏生成经验，学习器按自己的节奏从这些经验中学习，而演员们则定期更新他们本地的神经网络策略副本。Ape-X
    DQN的流程示意图见*图 13.1*：
- en: '![Figure 13.1 – Ape-X DQN architecture, revisited'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.1 – Ape-X DQN 架构，重新审视'
- en: '](img/B14160_13_1.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_13_1.jpg)'
- en: Figure 13.1 – Ape-X DQN architecture, revisited
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 – Ape-X DQN 架构，重新审视
- en: 'Now, let''s unpack this architecture from a computational and data communication
    point of view:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从计算和数据通信的角度来分析这个架构：
- en: Actors, potentially hundreds of them, periodically pull ![](img/Formula_13_001.png)
    parameters, the neural network policy, from a central learner. Depending on the
    size of the policy network, hundreds of thousands of numbers are pushed from the
    learner to the remote actors. This creates a big communication load between the
    learner and the actors, two orders of magnitude larger than what it would take
    to transfer actions and observations.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 演员们，可能有数百个，会定期从中央学习器拉取 ![](img/Formula_13_001.png) 参数，即神经网络策略。根据策略网络的大小，成千上万的数字会从学习器推送到远程演员。这会在学习器和演员之间产生很大的通信负载，远远超过传输动作和观察所需的两倍数量级。
- en: Once an actor receives the policy parameters, it uses it to infer actions for
    each step of the environment. In most settings, only the learner uses a GPU and
    the actors work on CPU nodes. So, in this architecture, a lot of inference has
    to be done on CPUs, which is much less efficient for this purpose compared to
    GPU inference.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦一个演员接收到策略参数，它便使用这些参数推断每个环境步骤的动作。在大多数设置中，只有学习者使用GPU，演员则在CPU节点上工作。因此，在这种架构中，大量的推断必须在CPU上进行，相较于GPU推断，这种方式效率要低得多。
- en: Actors switch between environment and inference steps, which have different
    compute requirements. Carrying out both steps on the same node either leads to
    computational bottlenecks (when it is a CPU node that has to do inference) or
    underutilization of resources (when it is a GPU node, the GPU capacity is wasted).
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 演员在环境和推断步骤之间切换，这两者具有不同的计算需求。将这两个步骤执行在同一节点上，要么会导致计算瓶颈（当需要推断的节点是CPU节点时），要么会导致资源的低效利用（当节点是GPU节点时，GPU的计算能力被浪费）。
- en: 'To overcome these inefficiencies, the SEED RL architecture makes the following
    key proposal: *Moving the action inference to the learner.* So, an actor sends
    its observation to the central learner, where the policy parameters are, and receives
    an action back. This way, the inference time is reduced as it is done on a GPU
    rather than a CPU.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些低效问题，SEED RL架构提出了以下关键方案：*将动作推断移至学习者端*。因此，演员将观察结果发送给中央学习者（那里存储着策略参数），并接收到回传的动作。通过这种方式，推断时间被缩短，因为推断在GPU上进行，而不是CPU上。
- en: 'Of course, the story does not end here. What we have described so far leads
    to a different set of challenges:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，故事并未结束。我们迄今所描述的情况带来了另一组挑战：
- en: Since the actor needs to send the observation in each environment step to a
    remote learner to receive an action, this creates a **latency** issue that did
    not exist before.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于演员需要在每个环境步骤中将观察结果发送到远程学习者以接收动作，因此出现了**延迟**问题，这是之前所没有的。
- en: While an actor waits for an action, it remains idle, causing *underutilization
    of the compute resources on the actor node*.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当演员等待动作时，它保持空闲状态，导致*演员节点计算资源的低效利用*。
- en: Passing individual observations to the learner GPU increases the total *communication
    overhead with the GPU*.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将单个观察结果传递给学习者GPU会增加总的*与GPU的通信开销*。
- en: The GPU resources need to be tuned to handle both inference and learning.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU资源需要调整，以同时处理推断和学习。
- en: 'To overcome these challenges, SEED RL has the following structure:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些挑战，SEED RL具有以下结构：
- en: A very fast communication protocol, called **gRPC**, to transfer the observations
    and actions between the actors and the learner.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种非常快速的通信协议，称为**gRPC**，用于在演员和学习者之间传输观察结果和动作。
- en: Multiple environments are placed on a single actor to maximize utilization.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个环境被放置在同一个演员上，以最大化利用率。
- en: Observations are batched before being passed to the GPU to reduce the overhead.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将观察结果传递给GPU之前，会对其进行批处理以减少开销。
- en: 'There is a fourth challenge of tuning the resource allocation, but it is a
    tuning problem rather than being a fundamental architecture problem. As a result,
    SEED RL proposes an architecture that can do the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 资源分配调整是第四个挑战，但这只是一个调优问题，而非根本性的架构问题。因此，SEED RL提出了一种架构，可以做到以下几点：
- en: Process millions of observations per second.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每秒处理数百万条观察数据。
- en: Reduce the cost of experiments, by up to 80%.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将实验成本降低高达80%。
- en: Decrease the wall-clock time by increasing the training speed by up to three
    times.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将训练速度提高三倍，减少墙钟时间。
- en: 'The SEED RL architecture is illustrated in *Figure 13.2*, taken from the SEED
    RL paper, which compares it to IMPALA, which suffers from similar downsides as
    Ape-X:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SEED RL架构如*图13.2*所示，取自SEED RL论文，并将其与IMPALA进行了比较，IMPALA也面临与Ape-X类似的缺点：
- en: '![Figure 13.2 – A comparison of IMPALA and SEED architectures (source: Espeholt
    et al, 2020)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.2 – IMPALA与SEED架构的比较（来源：Espeholt等人，2020）'
- en: '](img/B14160_13_2.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_13_2.jpg)'
- en: 'Figure 13.2 – A comparison of IMPALA and SEED architectures (source: Espeholt
    et al, 2020)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 – IMPALA与SEED架构的比较（来源：Espeholt等人，2020）
- en: So far, so good. For the implementation details, we refer you to *Espeholt et
    al, 2020* and the code repository associated with the paper.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。有关实现细节，我们推荐参考*Espeholt等人，2020*以及与论文相关的代码库。
- en: Info
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: The authors have open-sourced SEED RL on [https://github.com/google-research/seed_rl](https://github.com/google-research/seed_rl).
    The repo has implementations of the IMPALA, SAC, and the R2D2 agents.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作者已将 SEED RL 开源，地址为 [https://github.com/google-research/seed_rl](https://github.com/google-research/seed_rl)。该仓库包含了
    IMPALA、SAC 和 R2D2 代理的实现。
- en: We will cover the R2D2 agent momentarily and then run some experiments. But
    before we close this section, let's also provide you with one more resource.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将很快介绍 R2D2 代理，并进行一些实验。但在结束本节之前，我们还会为你提供另一个资源。
- en: Info
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: If you are interested in diving deeper into the engineering aspects of the architecture,
    gRPC is a great tool to have under your belt. It is a fast communication protocol
    that is used to connect microservices in many tech companies. Check it out at
    https://grpc.io.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣深入了解架构的工程方面，gRPC 是一个非常有用的工具。它是一个快速的通信协议，广泛应用于许多科技公司的微服务之间的连接。可以在 [https://grpc.io](https://grpc.io)
    查看。
- en: Awesome job! You are now up to date with the state of the art in distributed
    RL. Next, we'll cover a state-of-the-art model that is used in distributed RL
    architectures, R2D2\.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！你现在已经掌握了分布式强化学习的最新技术。接下来，我们将介绍一种在分布式 RL 架构中使用的最先进的模型，R2D2。
- en: Recurrent experience replay in distributed reinforcement learning
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分布式强化学习中的递归经验回放
- en: One of the most influential contributions to the recent RL literature, which
    set the state of the art in the classical benchmarks at the time, is the **Recurrent
    Replay Distributed DQN** (**R2D2**) agent. The main contribution of the R2D2 work
    is actually related to the effective use of **recurrent neural networks** (**RNNs**)
    in an RL agent, which is also implemented in a distributed setting. The paper
    uses **long-short term memory** (**LSTM**) as the choice of RNN, which we'll also
    adapt here in our discussion. So, let's start with what the challenge is with
    training RNNs when it comes to initializing the recurrent state, and then talk
    about how the R2D2 agent addresses it.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最近强化学习文献中最具影响力的贡献之一，是 **递归回放分布式 DQN**（**R2D2**）代理，它在当时设定了经典基准的最新技术水平。R2D2 研究的主要贡献实际上是与
    **递归神经网络**（**RNNs**）在强化学习代理中的有效应用有关，且这一方法也在分布式环境中实现。论文中使用了 **长短期记忆**（**LSTM**）作为
    RNN 的选择，我们在接下来的讨论中也会采用这种方法。那么，首先让我们从训练 RNN 时初始化递归状态的挑战谈起，再讨论 R2D2 代理如何解决这个问题。
- en: The initial recurrent state mismatch problem in recurrent neural networks
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 递归神经网络中的初始递归状态不匹配问题
- en: In the previous chapters, we discussed the importance of carrying a memory of
    observations to uncover partially observable states. For example, rather than
    using a single frame in an Atari game, which will not convey information such
    as the speeds of the objects, basing the action on a sequence of past frames,
    from which the speed and so on can be derived, will lead to higher rewards. An
    effective way of processing sequence data, as we also mentioned, is using RNNs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们讨论了携带观察记忆的重要性，以便揭示部分可观察的状态。例如，单独使用一帧 Atari 游戏画面将无法传达物体速度等信息，而基于一系列过去的画面，推算出物体速度等信息来做决策，会带来更高的奖励。如我们前面提到的，处理序列数据的有效方法是使用
    RNNs。
- en: 'The idea behind an RNN is to pass the inputs of a sequence to the same neural
    network one by one, but then also pass information, a memory, and a summary of
    the past steps, from one step to the next, which is illustrated in *Figure 13.3*:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 的基本思想是将序列的输入逐一传递给同一个神经网络，同时将过去步骤的信息、记忆和摘要从一个步骤传递到下一个步骤，这一点在 *图 13.3* 中有所说明：
- en: '![Figure 13.3 – A depiction of RNNs with a) compact, and b) unrolled representations'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.3 – RNN 的示意图，其中 a) 为紧凑表示，b) 为展开表示'
- en: '](img/B14160_13_3.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_13_3.jpg)'
- en: Figure 13.3 – A depiction of RNNs with a) compact, and b) unrolled representations
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 – RNN 的示意图，其中 a) 为紧凑表示，b) 为展开表示
- en: A key question here is what to use for the initial recurrent state, ![](img/Formula_13_002.png).
    Most commonly and conveniently, the recurrent state is initialized as all zeros.
    This is not a big problem when an actor steps through the environment and this
    initial recurrent state corresponds to the start of an episode. However, while
    training from stored samples that correspond to small sections of longer trajectories,
    such an initialization becomes a problem. Let's see why.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个关键问题是，如何为初始递归状态 ![](img/Formula_13_002.png) 进行初始化。最常见和便捷的方式是将递归状态初始化为全零。对于环境中每一步的演员来说，这并不是一个大问题，因为这个初始递归状态对应于一个回合的开始。然而，在从对应于较长轨迹小段的存储样本进行训练时，这种初始化就成了一个问题。我们来看看为什么。
- en: 'Consider the scenario illustrated in *Figure 13.4*. We are trying to train
    the RNN on a stored sample ![](img/Formula_13_003.png), so the observation is
    a sequence of four frames that are passed to the policy network. So,![](img/Formula_13_004.png)
    is the first frame and ![](img/Formula_13_005.png) is the last and the most recent
    frame in the sampled ![](img/Formula_13_006.png) sequence (and the argument is
    similar for ![](img/Formula_13_007.png)). As we feed the inputs, the ![](img/Formula_13_008.png)''s
    will be obtained and passed to the subsequent steps and we use zeros for ![](img/Formula_13_009.png):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑*图 13.4*所示的场景。我们正在尝试训练一个 RNN 来处理一个存储的样本 ![](img/Formula_13_003.png)，因此观测值是四帧组成的序列，这些帧被传递到策略网络中。所以，![](img/Formula_13_004.png)
    是第一帧，![](img/Formula_13_005.png) 是采样的 ![](img/Formula_13_006.png) 序列中的最后一帧，也是最新的帧（同样的情况适用于
    ![](img/Formula_13_007.png)）。当我们输入这些数据时，![](img/Formula_13_008.png) 将被获取并传递到后续步骤中，而我们对
    ![](img/Formula_13_009.png) 使用零值：
- en: '![Figure 13.4 – Using a sequence of frames to obtain an action from an RNN'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.4 – 使用一系列帧从 RNN 获取动作'
- en: '](img/B14160_13_4.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_13_4.jpg)'
- en: Figure 13.4 – Using a sequence of frames to obtain an action from an RNN
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 – 使用一系列帧从 RNN 获取动作
- en: 'Now, remember that the role of a recurrent state ![](img/Formula_13_010.png)
    is to summarize what happened up to step ![](img/Formula_13_011.png). When we
    use a vector of zeros for ![](img/Formula_13_012.png) during training, to generate
    value function predictions and target values for the Q function, for example,
    it creates several problems, which are related but slightly different from each
    other:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，记住递归状态 ![](img/Formula_13_010.png) 的作用是总结直到第 ![](img/Formula_13_011.png)
    步所发生的事情。当我们在训练期间使用零向量作为 ![](img/Formula_13_012.png) 时，例如在生成价值函数预测和 Q 函数的目标值时，会产生一些问题，这些问题虽然相关，但有所不同：
- en: It does not any convey meaningful information about what happened before that
    timestep.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不会传达任何关于该时间步之前发生了什么的有意义信息。
- en: We use that same vector (of zeros) regardless of what happened before the sampled
    sequence, which leads to an overloaded representation.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用相同的向量（零向量），无论采样序列之前发生了什么，这会导致过载的表示。
- en: A vector of zeros, since it is not an output of the RNN, is not a meaningful
    representation of the RNN anyway.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于零向量不是 RNN 的输出，它本身并不是 RNN 的有意义表示。
- en: As a result, the RNN gets "confused" about what to make of the hidden states
    in general and reduces its reliance on memory, which defeats the very purpose
    of using them.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，RNN 对隐状态的处理变得“混乱”，并减少了对记忆的依赖，这违背了使用递归神经网络的初衷。
- en: One solution to this is to record the whole trajectory and process/replay it
    during training to calculate the recurrent states for each step. This is also
    problematic because replaying all sample trajectories of arbitrary lengths during
    training is a lot of overhead.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是记录整个轨迹，并在训练时处理/重放它，以计算每一步的递归状态。这也存在问题，因为在训练时重放所有任意长度的样本轨迹会带来大量开销。
- en: Next, let's see how the R2D2 agent addresses this issue.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一下 R2D2 智能体是如何解决这个问题的。
- en: R2D2 solution to the initial recurrent state mismatch
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: R2D2 对初始递归状态不匹配的解决方案
- en: 'The solution of the R2D2 agent is two-fold:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: R2D2 智能体的解决方案是双管齐下：
- en: Store the recurrent states from the rollouts.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储回合中的递归状态。
- en: Use a burn-in period.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用烧入期。
- en: Let's look into these in more detail in the following sections.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将更详细地探讨这些解决方案。
- en: Storing the recurrent states from rollouts
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 存储回合中的递归状态
- en: While an agent steps through the environment, at the beginning of the episode,
    it initializes the recurrent state. Then it uses the recurrent policy network
    to take its actions at each step, and the recurrent states corresponding to each
    of those observations are also generated. The R2D2 agent sends these recurrent
    states along with the sampled experience to the replay buffer to later use them
    to initialize the network at training time instead of vectors of zeros.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当智能体在环境中执行时，在每一集的开始，它初始化递归状态。然后，它使用递归策略网络在每个步骤采取行动，并且每个观察对应的递归状态也会被生成。R2D2智能体将这些递归状态与采样的经验一起发送到重放缓冲区，以便稍后在训练时用它们初始化网络，而不是用零向量。
- en: 'In general, this significantly remedies the negative impact of using zero initialization.
    However, it is still not a perfect solution: The recurrent states stored in the
    replay buffer would be stale by the time they were used in training. This is because
    the network is constantly updated, whereas these states would carry a representation
    that was generated by an older version of the network, such as what was used at
    the rollout time. This is called **representational drift**.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这显著弥补了使用零初始化的负面影响。然而，这仍然不是一个完美的解决方案：存储在重放缓冲区中的递归状态在训练时使用时会变得过时。因为网络是不断更新的，而这些状态会携带由旧版本网络生成的表示，例如在回放时使用的网络。这被称为**表示漂移**。
- en: To mitigate representational drift, R2D2 proposes an additional mechanism, which
    is to use a burn-in period at the beginning of the sequence.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解表示漂移，R2D2提出了一种额外的机制，即在序列开始时使用预热期。
- en: Using a burn-in period
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用预热期
- en: 'Using a burn-in period works as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预热期的工作方式如下：
- en: Store a sequence that is longer than what we normally would.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 存储一个比我们通常存储的序列更长的序列。
- en: Use the extra portion at the beginning of the sequence to unroll the RNN with
    the current parameters.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用序列开头的额外部分，用当前参数展开RNN。
- en: With that, produce an initial state that is not stale for after the burn-in
    portion.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样，生成一个不会过时的初始状态，适用于预热部分之后。
- en: Don't use the burn-in portion during the backpropagation.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在反向传播时不要使用预热部分。
- en: 'This is depicted in *Figure 13.5*:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这在*图13.5*中有所描述：
- en: '![Figure 13.5 – Representation of R2D2''s use of stored recurrent states with
    a two-step burn-in'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.5 – 表示R2D2使用存储的递归状态并进行两步预热的示意图'
- en: '](img/B14160_13_5.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_13_5.jpg)'
- en: Figure 13.5 – Representation of R2D2's use of stored recurrent states with a
    two-step burn-in
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5 – 表示R2D2使用存储的递归状态并进行两步预热的示意图
- en: 'So, for the example in the figure, the idea is that rather than using ![](img/Formula_13_013.png),
    which is generated under some old policy ![](img/Formula_13_014.png), do the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，图中的例子是说，与其使用 ![](img/Formula_13_013.png)，它是由某些旧策略 ![](img/Formula_13_014.png)
    生成的，不如做如下操作：
- en: Use ![](img/Formula_13_015.png) to initialize the recurrent state at training
    time.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ![](img/Formula_13_015.png) 来在训练时初始化递归状态。
- en: Unroll the RNN with the current parameters ![](img/Formula_13_016.png) over
    the burn-in portion to generate an ![](img/Formula_13_017.png).
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前参数 ![](img/Formula_13_016.png) 展开RNN的递归状态，通过预热部分生成 ![](img/Formula_13_017.png)。
- en: This hopefully recovers from the stale representation of ![](img/Formula_13_018.png)
    and leads to a more accurate initialization than ![](img/Formula_13_019.png) would.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这有望从过时的表示 ![](img/Formula_13_018.png) 中恢复，并且比 ![](img/Formula_13_019.png) 更准确地初始化。
- en: This is more accurate in the sense that it is closer to what we would have obtained
    if we stored and unrolled the entire trajectory from the beginning till ![](img/Formula_13_020.png)
    using ![](img/Formula_13_021.png).
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这在精确度上更好，因为它更接近我们如果从一开始存储并展开整个轨迹，直到 ![](img/Formula_13_020.png)，使用 ![](img/Formula_13_021.png)
    所得到的结果。
- en: So, this was the R2D2 agent. Before we wrap up this section, let's discuss what
    the R2D2 agent has achieved.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是R2D2智能体。在我们结束这一部分之前，先来讨论一下R2D2智能体的成就。
- en: Key results from the R2D2 paper
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: R2D2论文的关键结果
- en: 'The R2D2 work has really interesting insights, for which I highly recommend
    you read the full paper. However, for the completeness of our discussion, here
    is a summary:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: R2D2的工作提供了非常有趣的见解，我强烈推荐你阅读完整的论文。然而，为了我们讨论的完整性，以下是一个总结：
- en: R2D2 quadruples the previous state of the art on Atari benchmarks that were
    set by Ape-X DQN, being the first agent to achieve a superhuman level of performance
    on 52 out of 57 games, and with a higher sample efficiency.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R2D2 在 Atari 基准测试中将 Ape-X DQN 创下的先前最高纪录提高了四倍，是第一个在 57 款游戏中有 52 款取得超人类水平表现的智能体，并且具有更高的样本效率。
- en: It achieves this using a single set of hyperparameters across all environments,
    which speaks to the robustness of the agent.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过在所有环境中使用一组超参数来实现这一点，显示出智能体的强大鲁棒性。
- en: Interestingly, R2D2 improves the performance even in environments that are considered
    fully observable, which you would not expect using a memory to help. The authors
    explain this with the high representation power of LSTM.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有趣的是，即便在被认为是完全可观察的环境中，R2D2 也能提高性能，而在这些环境中，通常不指望使用记忆来帮助。作者通过 LSTM 的高表示能力来解释这一点。
- en: Storing the recurrent states and using a burn-in period are both greatly beneficial,
    while the impact of the former is greater. These approaches can be used together,
    which is the most effective, or individually.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储递归状态并使用预热期都非常有益，其中前者的影响更大。可以将这两种方法结合使用，这是最有效的，或者单独使用。
- en: Using zero start states decreases an agent's capability to rely on the memory.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用零初始状态会降低智能体对记忆的依赖能力。
- en: For your information, in three out of the five environments in which the R2D2
    agent could not exceed human-level performance, it can actually achieve it by
    modifying the parameters. The remaining two environments, Montezuma's Revenge
    and Pitfall, are notorious hard-exploration problems, to which we will return
    in the latter sections of the chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 供您参考，在五个环境中，R2D2 智能体未能超过人类级别的表现，但通过修改参数，它实际上可以实现超越。剩下的两个环境，Montezuma's Revenge
    和 Pitfall，是著名的难探索问题，后续章节将进一步讨论这两个环境。
- en: With that, let's wrap up our discussion here and go into some hands-on work.
    In the next section, we are going to use the SEED RL architecture with an R2D2
    agent.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这样一来，让我们在这里总结讨论，并进入一些实践工作。下一节中，我们将使用 SEED RL 架构与 R2D2 智能体。
- en: Experimenting with SEED RL and R2D2
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验 SEED RL 和 R2D2
- en: In this section, we'll give a short demo of the SEED RL repo and how to use
    it to train agents. Let's start with setting up the environment.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要演示 SEED RL 仓库及其如何用于训练智能体。让我们从设置环境开始。
- en: Setting up the environment
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置环境
- en: The SEED RL architecture uses multiple libraries, such as TensorFlow and gRPC,
    that interact in rather sophisticated ways. To save us from most of the setup,
    the maintainers of SEED RL use Docker containers to train RL agents.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: SEED RL 架构使用多个库，如 TensorFlow 和 gRPC，它们之间以相当复杂的方式进行交互。为了简化大部分配置工作，SEED RL 的维护者使用
    Docker 容器来训练 RL 智能体。
- en: Info
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: Docker and container technology are among the fundamental tools behind today's
    internet services. If you are into machine learning engineering and/or are interested
    in serving your models in a production environment, it is a must to know. A quick
    bootcamp on Docker by Mumshad Mannambeth is available at [https://youtu.be/fqMOX6JJhGo](https://youtu.be/fqMOX6JJhGo).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 和容器技术是当今互联网服务背后的基础工具。如果你从事机器学习工程，或有兴趣在生产环境中提供你的模型，了解 Docker 是必不可少的。Mumshad
    Mannambeth 的快速 Docker 启蒙课程可在 [https://youtu.be/fqMOX6JJhGo](https://youtu.be/fqMOX6JJhGo)
    上找到。
- en: 'The setup instructions are available on the SEED RL GitHub page. In a nutshell,
    they are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 设置说明可以在 SEED RL GitHub 页面找到。简而言之，设置步骤如下：
- en: Install Docker on your machine.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的机器上安装 Docker。
- en: Enable running Docker as a non-root user.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用以非 root 用户身份运行 Docker。
- en: Install `git`.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 `git`。
- en: Clone the SEED repository.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆 SEED 仓库。
- en: 'Start your training for the environments defined in the repo using the `run_local.sh`
    script, as follows:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `run_local.sh` 脚本启动仓库中定义的环境训练，如下所示：
- en: '[PRE0]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A few additions to this setup may be needed if your NVIDIA GPU is not recognized
    by the SEED container:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的 NVIDIA GPU 没有被 SEED 容器识别，可能需要对该设置进行一些附加配置：
- en: Install the NVIDIA Container Toolkit at [https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html)
    安装 NVIDIA Container Toolkit。
- en: Install NVIDIA Modprobe, for example for Ubuntu, using `sudo apt-get install
    nvidia-modprobe`.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 NVIDIA Modprobe，例如在 Ubuntu 上使用 `sudo apt-get install nvidia-modprobe`。
- en: Reboot your workstation.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重启你的工作站。
- en: 'Once your setup is successful, you should see that your agent starts training
    on a tmux terminal, as shown in *Figure 13.6*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的设置成功，你应该看到代理开始在tmux终端上训练，如*图13.6*所示：
- en: '![Figure 13.6 – SEED RL training on a tmux terminal'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.6 – SEED RL在tmux终端上的训练'
- en: '](img/B14160_13_6.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_13_6.jpg)'
- en: Figure 13.6 – SEED RL training on a tmux terminal
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6 – SEED RL在tmux终端上的训练
- en: Info
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: Tmux is a terminal multiplexer, basically a window manager within the terminal.
    For a quick demo on how to use tmux, check out [https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/](https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Tmux是一个终端复用器，基本上是终端内的窗口管理器。要快速了解如何使用tmux，请查看[https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/](https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/)。
- en: Now you have SEED, a state-of-the-art RL framework, running on your machine!
    You can plug in your custom environments for training by following the Atari,
    Football, or DMLab example folders.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你的机器上已经运行了SEED，这是一款最先进的强化学习框架！你可以通过按照Atari、Football或DMLab示例文件夹中的说明，插入自定义环境进行训练。
- en: Info
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'The R2D2 agent is also available at DeepMind''s ACME library, along with many
    other agents: [https://github.com/deepmind/acme](https://github.com/deepmind/acme).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: R2D2代理也可以在DeepMind的ACME库中找到，里面还有许多其他代理：[https://github.com/deepmind/acme](https://github.com/deepmind/acme)。
- en: Next, we'll discuss curiosity-driven RL.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论好奇心驱动的强化学习。
- en: Curiosity-driven reinforcement learning
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 好奇心驱动的强化学习
- en: When we discussed the R2D2 agent, we mentioned that there were only a few Atari
    games left in the benchmark set that the agent could not exceed the human performance
    in. The remaining challenge for the agent was to solve **hard-exploration** problems,
    which have very sparse and/or misleading rewards. Later work that came out of
    Google DeepMind addressed those challenges as well, with agents called **Never
    Give Up** (**NGU**) and **Agent57** reaching superhuman-level performance in all
    of the 57 games used in the benchmarks. In this section, we are going to discuss
    these agents and the methods they used for effective exploration.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论R2D2代理时，我们提到在基准测试集中只剩下少数几个Atari游戏，代理在这些游戏中无法超过人类表现。代理面临的剩余挑战是解决**困难探索**问题，这些问题有非常稀疏和/或误导性的奖励。后续的工作来自Google
    DeepMind，也解决了这些挑战，使用了名为**Never Give Up**（**NGU**）和**Agent57**的代理，在基准测试中使用的57个游戏中都达到了超人类水平的表现。在本节中，我们将讨论这些代理以及它们用于有效探索的方法。
- en: Let's dive in by describing the concepts of hard-exploration and **curiosity-driven
    learning**.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从描述**困难探索**和**好奇心驱动学习**的概念开始。
- en: Curiosity-driven learning for hard-exploration problems
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 针对困难探索问题的好奇心驱动学习
- en: 'Let''s consider the simple grid world illustrated in *Figure 13.7*:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看*图13.7*所示的简单网格世界：
- en: '![Figure 13.7 – A hard-exploration grid-world problem'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.7 – 一个困难探索的网格世界问题'
- en: '](img/B14160_13_7.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_13_7.jpg)'
- en: Figure 13.7 – A hard-exploration grid-world problem
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7 – 一个困难探索的网格世界问题
- en: 'Assume the following setting in this grid world:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在这个网格世界中有以下设置：
- en: There are 102 total states, 101 for the grid world and 1 for the cliff surrounding
    it.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总共有102个状态，101个用于网格世界，1个用于环绕它的悬崖。
- en: The agent starts in the far left of the world and its goal is to reach the trophy
    on the far right.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理从世界的最左端开始，目标是到达最右端的奖杯。
- en: Reaching the trophy has a reward of 1,000, falling off the cliff has a reward
    of -100, and there's a -1 reward for each time step that passes to encourage quick
    exploration.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到达奖杯的奖励为1,000，掉入悬崖的奖励为-100，每个时间步的奖励为-1，以鼓励快速探索。
- en: An episode terminates when the trophy is reached, the agent falls off the cliff,
    or after 1,000 time steps.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个回合结束的条件是：代理到达奖杯，掉进悬崖，或经过1,000个时间步。
- en: 'The agent has five actions available to it at every time step: to stay still,
    or to go up, down, left, or right.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个时间步，代理有五种可用的动作：保持静止，或向上、向下、向左或向右移动。
- en: 'If you train an agent in the current setting, even with the most powerful algorithms
    we have covered, such as PPO, R2D2, and so on, the resulting policy will likely
    be suicidal:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在当前设置下训练一个代理，即使使用我们已覆盖的最强算法，如PPO、R2D2等，最终得到的策略可能也是自杀式的：
- en: It is very difficult to stumble upon the trophy through random actions, so the
    agent may never discover that there is a trophy with a high reward in this grid
    world.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过随机动作很难偶然找到奖杯，因此代理可能永远不会发现网格世界中有一个高奖励的奖杯。
- en: Waiting until the end of the episode results in a total reward of -1000.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等到回合结束会导致总奖励为-1000。
- en: In this dark world, the agent may decide to commit suicide as early as possible
    to avoid prolonged suffering.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个黑暗的世界里，智能体可能决定尽早自杀，以避免长时间的痛苦。
- en: Even with the most powerful algorithms, the weak link in this approach is the
    strategy of exploration through random actions. The probability of stumbling upon
    the optimal set of moves is ![](img/Formula_13_022.png).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是最强大的算法，这种方法中的薄弱环节也在于通过随机动作进行探索的策略。偶然碰到最优动作集的概率是！[](img/Formula_13_022.png)。
- en: Tip
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'To find the expected number of steps it will take for the agent to reach the
    trophy through random actions, we can use the following equation:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算智能体通过随机动作到达奖杯所需的预期步数，我们可以使用以下方程：
- en: '![](img/Formula_13_023.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_13_023.jpg)'
- en: where ![](img/Formula_13_024.png) is the expected number of steps it will take
    for the agent to reach the trophy when in state ![](img/Formula_13_025.png). We
    need to generate these equations for all states (it will be slightly different
    for ![](img/Formula_13_026.png)) and solve the resulting system of equations.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/Formula_13_024.png) 是智能体在状态 ![](img/Formula_13_025.png) 时到达奖杯所需的预期步数。我们需要为所有状态生成这些方程（对于
    ![](img/Formula_13_026.png) 会有所不同），并求解结果的方程组。
- en: When we discussed the Machine Teaching approach previously, we mentioned that
    the human teacher can craft the reward function to encourage the agent to go right
    in the world. The downside of this approach is that it may not be feasible to
    manually craft the reward function in more complex environments. In fact, the
    winning strategy may not even be known by the teacher to guide the agent.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前讨论机器教学方法时，我们提到过，教师可以设计奖励函数，鼓励智能体在世界中走对路。这种方法的缺点是，在更复杂的环境中，手动设计奖励函数可能不可行。事实上，教师可能连最优策略都不知道，无法引导智能体。
- en: Then the question becomes how can we encourage the agent to explore the environment
    efficiently? One good answer is to reward the agent for the states it visited
    for the first time, for example, with a reward of +1 in our grid world. Enjoying
    discovering the world could make a good motivation for the agent to avoid suicide,
    which will also lead to winning the trophy eventually.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 那么问题就变成了，如何鼓励智能体高效地探索环境呢？一个好的答案是对智能体首次访问的状态给予奖励，例如，在我们的网格世界中，给它+1的奖励。享受发现世界的乐趣可能会成为智能体避免自杀的动力，这最终也会导致赢得奖杯。
- en: 'This approach is called **curiosity-driven learning**, which involves giving
    an **intrinsic reward** to the agent based on the *novelty* of its observations.
    The reward takes the following form:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为**好奇心驱动学习**，它通过基于观察的新奇性给智能体提供**内在奖励**。奖励的形式如下：
- en: '![](img/Formula_13_027.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_13_027.jpg)'
- en: where ![](img/Formula_13_028.png) is the extrinsic reward assigned by the environment
    at time ![](img/Formula_13_029.png), ![](img/Formula_13_030.png) is the intrinsic
    reward for the novelty of the observation at time ![](img/Formula_08_016.png),
    and ![](img/Formula_13_032.png) is a hyperparameter to tune the relative importance
    of exploration.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/Formula_13_028.png) 是环境在时间 ![](img/Formula_13_029.png) 给予的外在奖励，![](img/Formula_13_030.png)
    是时间 ![](img/Formula_08_016.png) 对观察的新奇性给予的内在奖励，而 ![](img/Formula_13_032.png) 是调节探索相对重要性的超参数。
- en: Before we discuss the NGU and Agent57 agents, let's look into some practical
    challenges in curiosity-driven RL.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论NGU和Agent57智能体之前，让我们深入探讨一下好奇心驱动的强化学习中的一些实际挑战。
- en: Challenges in curiosity-driven reinforcement learning
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 好奇心驱动的强化学习中的挑战
- en: The grid world example we provided above has one of the simplest possible settings.
    On the other hand, our expectation of RL agents is to solve many sophisticated
    exploration problems. That, of course, comes with challenges. Let's discuss a
    few of them here.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 上面我们提供的网格世界示例是最简单的设置之一。另一方面，我们对强化学习智能体的期望是它们能够解决许多复杂的探索问题。当然，这也带来了挑战。我们来讨论其中的几个挑战。
- en: Assessing novelty when observations are in continuous space and/or are high-dimensional
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在观察处于连续空间和/或高维空间时评估新奇性
- en: 'When we have discrete observations, it is simple to assess whether an observation
    is novel or not: We can simply count how many times the agent has seen each observation.
    When the observation is in continuous space, such as images, however, it gets
    complicated as it is not possible to simply count them. A similar challenge is
    when the number of dimensions of the observation space is too big, as it is in
    an image.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有离散的观察时，评估一个观察是否新颖很简单：我们只需要计算智能体已经看到每个观察的次数。然而，当观察处于连续空间时，例如图像，就变得复杂，因为无法简单地进行计数。类似的挑战是，当观察空间的维度过大时，就像在图像中一样。
- en: Noisy TV problem
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 噪声电视问题
- en: An interesting failure state for curiosity-driven exploration is to have a source
    of noise in the environment, such as a noisy TV that displays random frames in
    a maze.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于好奇心驱动的探索，一个有趣的失败状态是环境中有噪声源，比如在迷宫中播放随机画面的嘈杂电视。
- en: '![Figure 13.8 – Noisy TV problem illustrated in OpenAI''s experiments (source
    OpenAI et al. 2018)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.8 – OpenAI 实验中展示的噪声电视问题（来源：OpenAI 等，2018）'
- en: '](img/B14160_13_8.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_13_8.jpg)'
- en: Figure 13.8 – Noisy TV problem illustrated in OpenAI's experiments (source OpenAI
    et al. 2018)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8 – OpenAI 实验中展示的噪声电视问题（来源：OpenAI 等，2018）
- en: The agent then gets stuck in front of the noisy TV (like a lot of people do)
    to do meaningless exploration rather than actually discovering the maze.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，智能体会像很多人一样，困在嘈杂的电视前，进行无意义的探索，而不是实际地发现迷宫。
- en: Life-long novelty
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 终身新颖性
- en: The intrinsic reward, as we described above, is given based on the novelty of
    the observations within an episode. However, we want our agent to avoid making
    the same discoveries again and again in different episodes. In other words, we
    need a mechanism to assess *life-long novelty* for effective exploration.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，内在奖励是基于一个回合内观察到的新颖性给予的。然而，我们希望智能体能够避免在不同回合中一再做出相同的发现。换句话说，我们需要一个机制来评估*终身新颖性*，以实现有效的探索。
- en: There are different ways of addressing these challenges. Next, we will review
    how the NGU and the Agent57 agents address them, leading to their state-of-the-art
    performance in the classic RL benchmarks.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些挑战有多种方法。接下来，我们将回顾 NGU 和 Agent57 智能体是如何应对这些挑战的，并探讨它们如何在经典强化学习基准测试中实现最先进的性能。
- en: Never Give Up
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 永不放弃
- en: The NGU agent effectively brings together some key exploration strategies. Let's
    take a look at this in the following sections.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: NGU 智能体有效地将一些关键的探索策略结合在一起。接下来我们将在以下章节中详细探讨这一点。
- en: Obtaining embeddings for observations
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取观察的嵌入
- en: 'The NGU agent obtains embeddings from observations in such a way that it handles
    the two challenges together regarding a) high-dimensional observation space, and
    b) noise in observations. Here is how: Given an ![](img/Formula_13_033.png) triplet
    sampled from the environment, where ![](img/Formula_13_034.png) is the observation
    and ![](img/Formula_13_035.png) is the action at time ![](img/Formula_13_036.png),
    it trains the neural network to predict action from the two consecutive observations.
    This is illustrated in *Figure 13.9*:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: NGU 智能体通过从观察中获得嵌入方式，处理了关于 a) 高维观察空间和 b) 观察中的噪声这两个挑战。具体来说：给定从环境中采样的一个三元组 ![](img/Formula_13_033.png)，其中
    ![](img/Formula_13_034.png) 是观察，![](img/Formula_13_035.png) 是时间 ![](img/Formula_13_036.png)
    时的动作，它通过训练神经网络从两个连续观察中预测动作。这个过程如*图 13.9*所示：
- en: '![Figure 13.9 – NGU agent embedding network'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.9 – NGU 智能体嵌入网络'
- en: '](img/B14160_13_9.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_13_9.jpg)'
- en: Figure 13.9 – NGU agent embedding network
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9 – NGU 智能体嵌入网络
- en: The embeddings, the ![](img/Formula_13_037.png)-dimensional representations
    of the images coming out of the ![](img/Formula_13_038.png) embedding network,
    denoted as ![](img/Formula_13_039.png), is what the agent will use to assess the
    novelty of the observations later.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入，即来自 ![](img/Formula_13_038.png) 嵌入网络的图像的 ![](img/Formula_13_037.png) 维度表示，记作
    ![](img/Formula_13_039.png)，是智能体稍后用于评估观察新颖性的依据。
- en: If you are wondering why there is this fancy setup to obtain some lower-dimensional
    representations of image observations, it is to address the noisy TV problem.
    Noise in the observations is not useful information while predicting the action
    that led the environment from emitting observation ![](img/Formula_13_040.png)
    to ![](img/Formula_13_041.png) in the next step. In other words, actions taken
    by the agent would not explain the noise in the observations. Therefore, we don't
    expect a network that predicts the action from observations to learn representations
    carrying the noise, at least not dominantly. So, this is a clever way of denoising
    the observation representations.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道为什么有这样一个复杂的设置来获取图像观察的低维表示，这是为了应对噪声电视问题。观察中的噪声在预测导致环境从发出观察![](img/Formula_13_040.png)到![](img/Formula_13_041.png)的动作时并没有提供有用的信息。换句话说，代理执行的动作不会解释观察中的噪声。因此，我们不希望一个从观察中预测动作的网络学习到包含噪声的表示，至少不会是主导的。所以，这是一个巧妙的去噪方式，处理观察表示。
- en: Let's next see how these representations are used.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看这些表示是如何使用的。
- en: Episodic novelty module
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回合新颖性模块
- en: 'In order to assess how novel an observation ![](img/Formula_13_042.png) is
    compared to the previous observations in the episode and calculate an episodic
    intrinsic reward ![](img/Formula_13_043.png), the NGU agent does the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估观察结果![](img/Formula_13_042.png)与回合中先前观察结果的相对新颖性，并计算一个回合内的内在奖励![](img/Formula_13_043.png)，NGU代理执行以下操作：
- en: Stores the embeddings from the observations encountered in an episode in a memory
    ![](img/Formula_08_077.png)
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将在一个回合中遇到的观察结果的嵌入存储在一个记忆中！[](img/Formula_08_077.png)
- en: Compares ![](img/Formula_13_045.png) to ![](img/Formula_13_046.png)-nearest
    embeddings in ![](img/Formula_13_047.png)
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将![](img/Formula_13_045.png)与![](img/Formula_13_046.png)-最近的嵌入在![](img/Formula_13_047.png)中进行比较
- en: Calculates an intrinsic reward that is inversely proportional to the sum of
    the similarities between ![](img/Formula_13_048.png) and its ![](img/Formula_13_049.png)
    neighbors
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算一个内在奖励，该奖励与![](img/Formula_13_048.png)和其![](img/Formula_13_049.png)邻居之间相似度之和成反比
- en: 'This idea is illustrated in *Figure 13.10*:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法在*图13.10*中得到了说明：
- en: '![Figure 13.10 – NGU episodic novelty module'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.10 – NGU回合新颖性模块'
- en: '](img/B14160_13_10.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_13_10.jpg)'
- en: Figure 13.10 – NGU episodic novelty module
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.10 – NGU回合新颖性模块
- en: To avoid the somewhat crowded notation, we'll leave the details of the calculation
    to the paper, but this should give you the idea.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免有些拥挤的符号表示，我们将计算的细节留给论文，但这应该能让你大致了解。
- en: Finally, let's discuss how the NGU agent assesses life-long novelty.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们讨论NGU代理如何评估终身新颖性。
- en: Life-long novelty module with random distillation networks
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 终身新颖性模块，带有随机蒸馏网络
- en: During training, RL agents collect experiences across many parallel processes
    and over many episodes, leading to billions of observations in some applications.
    Therefore, it is not quite straightforward to tell whether an observation is a
    novel one among all.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，强化学习代理会在许多并行进程和回合中收集经验，这在某些应用中会导致数十亿次观察。因此，判断一个观察是否在所有观察中是新颖的并不完全直接。
- en: 'A clever way to address that is to use **Random Network Distillation** (**RND**),
    which the NGU agent does. RND involves two networks: a random network and a predictor
    network. Here is how they work:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这一问题的巧妙方法是使用**随机网络蒸馏**（**RND**），这正是NGU代理所做的。RND涉及两个网络：一个随机网络和一个预测器网络。它们的工作方式如下：
- en: The random network is randomly initialized at the beginning of the training.
    Naturally, it leads to an arbitrary mapping from observations to outputs.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机网络在训练开始时是随机初始化的。自然，它导致了从观察到输出的任意映射。
- en: The predictor network tries to learn this mapping, which is what the random
    network does, throughout the training.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测器网络试图学习这个映射，这是随机网络在整个训练过程中所做的。
- en: The predictor network's error will be low on previously encountered observations
    and high on novel ones.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测器网络的误差会在先前遇到的观察上较低，而在新颖的观察上较高。
- en: The higher the prediction error is, the larger the intrinsic reward will be.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测误差越大，内在奖励就会越大。
- en: 'The RND architecture is illustrated in *Figure 13.11*:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: RND架构在*图13.11*中得到了说明：
- en: '![Figure 13.11 – RND architecture in the NGU agent'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 13.11 – NGU代理中的RND架构'
- en: '](img/B14160_13_11.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_13_11.jpg)'
- en: Figure 13.11 – RND architecture in the NGU agent
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.11 – NGU代理中的RND架构
- en: The NGU agent uses this error to obtain a multiplier, ![](img/Formula_13_050.png),
    to scale ![](img/Formula_13_051.png). More specifically,
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: NGU 智能体利用此误差来获得乘数，![](img/Formula_13_050.png)，以调整 ![](img/Formula_13_051.png)。更具体地说，
- en: '![](img/Formula_13_052.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_13_052.jpg)'
- en: where ![](img/Formula_13_053.png) and ![](img/Formula_13_054.png) are the mean
    and standard deviation of the prediction network errors. So, to obtain a multiplier
    greater than 1 for an observation, the error, the "surprise," of the predictor
    network should be greater than the average error it makes.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/Formula_13_053.png) 和 ![](img/Formula_13_054.png) 是预测网络误差的均值和标准差。因此，要获得大于
    1 的乘数，预测网络的误差，即“惊讶”，应该大于它所做的平均误差。
- en: Now, let's put everything together.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将一切整合起来。
- en: Combining the intrinsic and extrinsic rewards
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合内在奖励和外在奖励
- en: 'After obtaining an episodic intrinsic reward and a multiplier based on life-long
    novelty for an observation, the combined intrinsic reward at time ![](img/Formula_13_055.png)
    is calculated as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得基于长期新奇性的观察的季节性内在奖励和乘数后，结合的内在奖励在时间 ![](img/Formula_13_055.png) 时的计算方式如下：
- en: '![](img/Formula_13_056.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_13_056.jpg)'
- en: 'where ![](img/Formula_13_057.png) is a hyperparameter to cap the multiplier.
    Then the episode reward is a weighted sum of the intrinsic and extrinsic rewards:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/Formula_13_057.png) 是一个超参数，用于限制乘数的上限。然后，回合奖励是内在奖励和外在奖励的加权和：
- en: '![](img/Formula_13_058.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_13_058.jpg)'
- en: This is it! We have covered some of the key ideas behind the NGU agent. There
    are more details to it, such as how to set the ![](img/Formula_13_059.png) values
    across parallelized actors and then use it to parametrize the value function network.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们已经涵盖了 NGU 智能体的一些关键思想。它还有更多的细节，例如如何在并行化的行为者中设置 ![](img/Formula_13_059.png)
    值，然后利用它来参数化价值函数网络。
- en: Before we wrap up our discussion on curiosity-driven learning, let's briefly
    talk about an extension to the NGU agent, Agent57\.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束关于好奇心驱动学习的讨论之前，让我们简要谈谈 NGU 智能体的扩展，Agent57。
- en: Agent57 improvements
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Agent57 改进
- en: 'Agent57 extends the NGU agent to set the new state of the art. The main improvements
    are as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Agent57 扩展了 NGU 智能体，设定了新的技术前沿。主要改进如下：
- en: It trains separate value function networks for intrinsic and extrinsic rewards
    and then combines them.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它为内在奖励和外在奖励分别训练价值函数网络，然后将它们结合起来。
- en: It trains a population of policies, for which the sliding-window **upper confidence
    bound** (**UCB**) method is used to the pick ![](img/Formula_13_060.png) and discount
    factor ![](img/Formula_13_061.png) while prioritizing one policy over the other.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它训练一组策略，并使用滑动窗口**上置信界限** (**UCB**) 方法来选择 ![](img/Formula_13_060.png) 和折扣因子 ![](img/Formula_13_061.png)，同时优先考虑一个策略而非另一个。
- en: With that, we conclude our discussion on curiosity-driven RL, which is key to
    solving hard-exploration problems in RL. Having said that, exploration strategies
    in RL is a broad topic. For a more comprehensive review of the topic, I suggest
    you read Lilian Weng's blog post (*Weng*, *2020*) on this and then dive into the
    papers referred to in the blog.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就结束了关于好奇心驱动的强化学习的讨论，这是解决强化学习中难以探索问题的关键。话虽如此，强化学习中的探索策略是一个广泛的话题。为了更全面地了解这一主题，我建议你阅读
    Lilian Weng 关于此话题的博客文章（*Weng*，*2020*），然后深入研究博客中提到的论文。
- en: 'Next, we''ll discuss another important area: offline RL.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论另一个重要领域：离线强化学习。
- en: Offline reinforcement learning
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离线强化学习
- en: '**Offline RL** is about training agents using data recorded during some prior
    interactions of an agent (likely non-RL, such as a human agent) with the environment,
    as opposed to directly interacting with it. It is also called **batch RL**. In
    this section, we look into some of the key components of offline RL. Let''s get
    started with an overview of how it works.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**离线强化学习** 是通过使用智能体与环境进行的一些先前交互（可能是非强化学习的，如人类智能体）录制的数据来训练智能体，而不是直接与环境互动。这也被称为**批量强化学习**。在这一部分，我们将研究离线强化学习的一些关键组成部分。让我们从一个概述开始，了解它是如何工作的。'
- en: An overview of how offline reinforcement learning works
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离线强化学习工作原理概述
- en: 'In offline RL, the agent does not directly interact with the environment to
    explore and learn a policy. *Figure 13.12* contrasts this to on-policy and off-policy
    settings:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在离线强化学习中，智能体不会直接与环境互动来探索和学习策略。*图13.12* 将其与在线策略和离策略设置进行了对比：
- en: '![Figure 13.12 – Comparison of on-policy, off-policy, and offline deep RL (adapted
    from Levine, 2020)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![图13.12 – 在线策略、离策略与离线深度强化学习的对比（改编自 Levine, 2020）'
- en: '](img/B14160_13_12.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_13_12.jpg)'
- en: Figure 13.12 – Comparison of on-policy, off-policy, and offline deep RL (adapted
    from *Levine, 2020*)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.12 – 在线策略、离策略和离线深度RL的比较（改编自*Levine, 2020*）
- en: 'Let''s unpack what this figure illustrates:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解读一下这张图所展示的内容：
- en: In on-policy RL, the agent collects a batch of experiences with each policy.
    Then, it uses this batch to update the policy. This cycle repeats until a satisfactory
    policy is obtained.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在在线策略RL中，代理会使用每个策略收集一批经验。然后，使用这批经验来更新策略。这个循环会一直重复，直到获得令人满意的策略。
- en: In off-policy RL, the agent samples experiences from a replay buffer to periodically
    improve the policy. The updated policy is then used in the rollouts to generate
    new experience, which gradually replaces the old experience in the replay buffer.
    This cycle repeats until a satisfactory policy is obtained.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在离策略RL中，代理从重放缓冲区采样经验，以周期性地改进策略。更新后的策略会在回合中使用，生成新的经验，并逐渐替换重放缓冲区中的旧经验。这个循环会一直重复，直到获得令人满意的策略。
- en: In offline RL, there is some behavior policy ![](img/Formula_13_062.png) interacting
    with the environment and collecting experience. This behavior policy does not
    have to belong to an RL agent. In fact, in most cases, it is either human behavior,
    a rule-based decision mechanism, a classical controller, and so on. The experience
    recorded from these interactions is what the RL agent will use to learn a policy,
    hopefully improving the behavior policy. So, in offline RL, the RL agent does
    not interact with the environment.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在离线RL中，存在一些行为策略 ![](img/Formula_13_062.png) 与环境交互并收集经验。这个行为策略不一定属于RL代理。事实上，在大多数情况下，它可能是人类行为、基于规则的决策机制、经典控制器等。从这些交互中记录的经验将是RL代理用来学习策略的依据，希望能够改进行为策略。因此，在离线RL中，RL代理并不与环境进行交互。
- en: One of the obvious questions in your mind could be why we cannot just put the
    offline data into something like a replay buffer and use a DQN agent or similar.
    This is an important point, so let's discuss it.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问的一个显而易见的问题是，为什么我们不能将离线数据放入类似重放缓冲区的东西，并使用DQN代理或类似的方法呢？这是一个重要的问题，我们来讨论一下。
- en: Why we need special algorithms for offline learning
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们需要为离线学习设计特殊的算法
- en: 'Interacting with the environment for an RL agent is necessary to observe the
    consequences of its actions in different states. Offline RL, on the other hand,
    does not let the agent interact and explore, which is a serious limitation. Here
    are some examples to illustrate this point:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 对于强化学习（RL）代理来说，必须与环境进行交互，以便观察其在不同状态下行为的后果。另一方面，离线RL不允许代理进行交互和探索，这是一个严重的限制。以下是一些示例来说明这一点：
- en: Let's say we have data from a human driving a car in town. The maximum speed
    the driver reached as per the logs is 50 mph. The RL agent might infer from the
    logs that increasing the speed reduces the travel time and may come up with a
    policy that suggests driving at 150 mph in town. Since the agent never observed
    its possible consequences, it does not have a lot of chance to correct its approach.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们有来自一个人类在城市中开车的数据。根据日志，驾驶员达到的最大速度是50英里每小时。RL代理可能会从日志中推断出，增加速度会减少旅行时间，并可能提出一个策略，建议在城市中以150英里每小时的速度行驶。由于代理从未观察到这种做法可能带来的后果，它没有太多机会纠正这种做法。
- en: While using a value-based method, such as DQN, the Q network is initialized
    randomly. As a result, some ![](img/Formula_13_063.png) values will be very high
    just by chance, suggesting a policy driving the agent to ![](img/Formula_13_064.png)
    and then taking action ![](img/Formula_13_065.png). When the agent is able to
    explore, it can evaluate the policy and correct such bad estimates. In offline
    RL, it cannot.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用基于值的方法，如DQN时，Q网络是随机初始化的。因此，某些 ![](img/Formula_13_063.png) 值可能仅凭运气就非常高，从而暗示一个策略，推动代理执行
    ![](img/Formula_13_064.png) 并采取行动 ![](img/Formula_13_065.png)。当代理能够进行探索时，它可以评估该策略并纠正这些不好的估计。但在离线RL中，它无法做到这一点。
- en: So, the core of the problem here is the **distributional shift**, that is, the
    discrepancy between the behavior policy and the resulting RL policy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这里问题的核心是**分布变化**，即行为策略与RL策略之间的差异。
- en: So, hopefully, you are convinced that offline RL requires some special algorithms.
    Then the next question is, is it worth it? Why should we bother when we can happily
    obtain superhuman-level performance with all the clever approaches and models
    we've discussed so far? Let's see why.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，希望你已经信服了离线RL需要一些特殊的算法。那么，下一个问题是，这样做值得吗？当我们可以使用我们迄今为止讨论的所有聪明方法和模型获得超人类级别的表现时，为什么我们还要为此费心呢？让我们看看原因。
- en: Why offline reinforcement learning is crucial
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么离线强化学习至关重要
- en: The very reason that video games are the most common testbed for RL is we can
    collect the amount of data needed for training. When it comes to training RL policies
    for real-world applications, such as robotics, autonomous driving, supply chain,
    finance, and so on, we need simulations of these processes to be able to collect
    the necessary amounts of data and wildly explore various policies. *This is arguably
    the single most important challenge in real-world RL*.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 视频游戏之所以是强化学习（RL）最常见的测试平台，是因为我们可以收集到用于训练所需的大量数据。当涉及到为实际应用（如机器人技术、自动驾驶、供应链、金融等）训练RL策略时，我们需要这些过程的模拟，以便能够收集必要的数据量并广泛探索各种策略。*这无疑是现实世界RL中最重要的挑战之一*。
- en: 'Here are some reasons why:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些原因：
- en: Building a high-fidelity simulation of a real-world process is often very costly
    and could take years.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个高保真度的现实世界过程模拟通常非常昂贵，可能需要数年时间。
- en: High-fidelity simulations are likely to require a lot of compute resources to
    run, making it hard to scale them for RL training.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高保真度的模拟可能需要大量的计算资源来运行，这使得它们很难在RL训练中进行规模化。
- en: Simulations could quickly become stale if the environment dynamics change in
    a way that is not parametrized in the simulation.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果环境动态发生变化且未在模拟中进行参数化，模拟可能会很快变得过时。
- en: Even when the fidelity is very high, it may not be high enough for RL. RL is
    prone to overfitting to errors, quirks, and assumptions of the (simulation) environment
    it interacts with. So, this creates a sim-to-real gap.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使保真度非常高，也可能不足以满足RL的要求。RL容易对它所交互的（模拟）环境中的错误、怪癖和假设过拟合。因此，这就产生了模拟到现实的差距。
- en: It could be costly or unsafe to deploy RL agents that might have overfit to
    the simulation.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署可能已对模拟过拟合的RL智能体可能会很昂贵或不安全。
- en: 'As a result, a simulation is a rare beast to run into in businesses and organizations.
    Do you know what we have in abundance? Data. We have processes that generate a
    lot of data:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，模拟在企业和组织中是一种稀有的存在。你知道我们拥有的是什么吗？数据。我们有许多生成大量数据的过程：
- en: Manufacturing environments have machine logs.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制造环境有机器日志。
- en: Retailers have data on their past pricing strategies and their results.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零售商有关于他们过去定价策略及其结果的数据。
- en: Trading firms have logs of their buy and sell decisions.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交易公司有他们的买卖决策日志。
- en: We have, and can obtain, a lot of car-driving videos.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有很多汽车驾驶视频，并且能够获得它们。
- en: Offline RL has the potential to drive automation for all those processes and
    create huge real-world value.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 离线RL有潜力为所有这些过程推动自动化，并创造巨大的现实世界价值。
- en: After this long but necessary motivation, it is finally time to go into a specific
    offline RL algorithm.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这段冗长但必要的动机说明后，终于到了介绍具体的离线RL算法的时刻。
- en: Advantage weighted actor-critic
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势加权演员评论家
- en: 'Offline RL is a hot area of research and there are many algorithms that have
    been proposed. One common theme is to make sure that the learned policy stays
    close to the behavior policy. A common measure to assess the discrepancy is KL
    divergence:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 离线RL是一个热门的研究领域，已经提出了许多算法。一个共同的主题是确保学习到的策略保持接近行为策略。评估差异的常用衡量标准是KL散度：
- en: '![](img/Formula_13_066.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_13_066.jpg)'
- en: 'On the other hand, different from the other approaches, **advantage weighted
    actor-critic** (**AWAC**) exhibits the following traits:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，与其他方法不同，**优势加权演员评论家**（**AWAC**）表现出以下特征：
- en: It does not try to fit a model to explicitly learn ![](img/Formula_13_067.png).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它并不试图拟合一个模型来显式地学习 ![](img/Formula_13_067.png)。
- en: It implicitly punishes the distributional shift.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过惩罚分布的偏移来隐式地进行调整。
- en: It uses dynamic programming a train to ![](img/Formula_13_068.png) function
    for data efficiency.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用动态规划来训练数据效率高的 ![](img/Formula_13_068.png) 函数。
- en: 'To this end, AWAC optimizes the following objective function:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，AWAC优化以下目标函数：
- en: '![](img/Formula_13_069.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_13_069.jpg)'
- en: 'which leads to the following policy update step:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下的策略更新步骤：
- en: '![](img/Formula_13_070.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_13_070.jpg)'
- en: where ![](img/Formula_13_071.png) is a hyperparameter and ![](img/Formula_13_072.png)
    is a normalization quantity. The key idea here is to encourage actions with a
    higher advantage.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/Formula_13_071.png) 是超参数， ![](img/Formula_13_072.png) 是归一化量。这里的关键思想是鼓励具有较高优势的动作。
- en: Info
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: One of the key contributions of AWAC is that the policy that is trained from
    offline data can then later be fine-tuned effectively by interacting with the
    environment if that opportunity exists.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: AWAC的一个关键贡献是，基于离线数据训练的策略在有机会的情况下，可以通过与环境互动进行有效的微调。
- en: We defer the details of the algorithm to the paper (by *Nair et al, 2020*),
    and the implementation to the RLkit repo at [https://github.com/vitchyr/rlkit](https://github.com/vitchyr/rlkit).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将算法的详细信息推迟到论文中（由*Nair等人，2020*），实现则可以在RLkit代码库中找到，地址为[https://github.com/vitchyr/rlkit](https://github.com/vitchyr/rlkit)。
- en: Let's wrap up our discussion on offline RL with benchmark datasets and the corresponding
    repos.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下关于离线强化学习的讨论，包含基准数据集及相应的代码库。
- en: Offline reinforcement learning benchmarks
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离线强化学习基准
- en: 'As offline RL is taking off, researchers from DeepMind and UC Berkeley have
    created benchmark datasets and repos so that offline RL algorithms can be compared
    to each other in a standardized way. These will serve as the "Gym" for offline
    RL, if you will:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 随着离线强化学习（Offline RL）逐渐兴起，来自DeepMind和加利福尼亚大学伯克利分校的研究人员创建了基准数据集和代码库，以便离线强化学习算法可以通过标准化的方式进行相互比较。这些将成为离线强化学习的“Gym”，可以这么理解：
- en: '*RL Unplugged* by DeepMind includes datasets from Atari, Locomotion, DeepMind
    Control Suite environments, as well as real-world datasets. It is available at
    [https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged](https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged).'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RL Unplugged* 由DeepMind推出，包含来自Atari、Locomotion、DeepMind Control Suite环境的数据集，以及真实世界的数据集。它可以在[https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged](https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged)上获得。'
- en: '*D4RL* by UC Berkeley''s **Robotics and AI Lab** (**RAIL**) includes datasets
    from various environments such as Maze2D, Adroit, Flow, and CARLA. It is available
    at [https://github.com/rail-berkeley/d4rl](https://github.com/rail-berkeley/d4rl).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D4RL* 由加利福尼亚大学伯克利分校的**机器人与人工智能实验室**（**RAIL**）推出，包含来自不同环境的数据集，如Maze2D、Adroit、Flow和CARLA。它可以在[https://github.com/rail-berkeley/d4rl](https://github.com/rail-berkeley/d4rl)上获得。'
- en: Great work! You are now up to speed with one of the key emerging fields – offline
    RL.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！你现在已经跟上了这一新兴领域的步伐——离线强化学习。
- en: Summary
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered several advanced topics that are very hot areas
    of research. Distributed RL is key to be able to scale RL experiments efficiently.
    Curiosity-driven RL makes solving hard-exploration problems possible through effective
    exploration strategies. And finally, offline RL has the potential to transform
    how RL is used for real-world problems by leveraging the data logs already available
    for many processes.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了几个非常热门的研究领域的高级主题。分布式强化学习是高效扩展强化学习实验的关键。基于好奇心驱动的强化学习通过有效的探索策略使解决困难的探索问题成为可能。最后，离线强化学习有潜力通过利用已经可用的许多过程的数据日志，彻底改变强化学习在现实世界问题中的应用。
- en: With this chapter, we conclude the part of our book on algorithmic and theoretical
    discussions. The remaining chapters will be more applied, starting with robotics
    applications in the next chapter.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们结束了关于算法和理论讨论的部分。接下来的章节将更侧重于应用，从下一章的机器人学应用开始。
- en: References
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[https://arxiv.org/abs/1910.06591](https://arxiv.org/abs/1910.06591)'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/1910.06591](https://arxiv.org/abs/1910.06591)'
- en: '[https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html](https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html](https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html)'
- en: '[https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark)'
- en: '[https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/](https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/](https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/)'
- en: '[https://youtu.be/C3yKgCzvE_E](https://youtu.be/C3yKgCzvE_E)'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://youtu.be/C3yKgCzvE_E](https://youtu.be/C3yKgCzvE_E)'
- en: '[https://medium.com/@sergey.levine/decisions-from-data-how-offline-reinforcement-learning-will-change-how-we-use-ml-24d98cb069b0](mailto:https://medium.com/@sergey.levine/decisions-from-data-how-offline-reinforcement-learning-will-change-how-we-use-ml-24d98cb069b0)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://medium.com/@sergey.levine/decisions-from-data-how-offline-reinforcement-learning-will-change-how-we-use-ml-24d98cb069b0](mailto:https://medium.com/@sergey.levine/decisions-from-data-how-offline-reinforcement-learning-will-change-how-we-use-ml-24d98cb069b0)'
- en: '[https://offline-rl-neurips.github.io/](https://offline-rl-neurips.github.io/)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://offline-rl-neurips.github.io/](https://offline-rl-neurips.github.io/)'
- en: '[https://github.com/vitchyr/rlkit/tree/master/rlkit](https://github.com/vitchyr/rlkit/tree/master/rlkit)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/vitchyr/rlkit/tree/master/rlkit](https://github.com/vitchyr/rlkit/tree/master/rlkit)'
- en: '[https://arxiv.org/pdf/2005.01643.pdf](https://arxiv.org/pdf/2005.01643.pdf)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/pdf/2005.01643.pdf](https://arxiv.org/pdf/2005.01643.pdf)'
- en: '[https://arxiv.org/abs/2006.09359](https://arxiv.org/abs/2006.09359)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/2006.09359](https://arxiv.org/abs/2006.09359)'
- en: '[https://offline-rl.github.io/](https://offline-rl.github.io/)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://offline-rl.github.io/](https://offline-rl.github.io/)'
- en: '[https://bair.berkeley.edu/blog/2020/09/10/awac/](https://bair.berkeley.edu/blog/2020/09/10/awac/)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://bair.berkeley.edu/blog/2020/09/10/awac/](https://bair.berkeley.edu/blog/2020/09/10/awac/)'
