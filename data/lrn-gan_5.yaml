- en: Chapter 5. Using Various Generative Models to Generate Images
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章 使用各种生成模型生成图像
- en: Deep learning shines with big data and deeper models. It has millions of parameters
    that can take even weeks to train. Some real-life scenarios may not have sufficient
    data, hardware, or resources to train bigger networks in order to achieve the
    desired accuracy. Is there any alternative approach or do we need to reinvent
    the training wheel from scratch all the time?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在大数据和更深层模型中展现出巨大的优势。它拥有数百万个参数，训练可能需要数周的时间。一些现实场景可能没有足够的数据、硬件或资源来训练更大的网络，以达到所需的准确性。是否存在其他的解决方案，还是我们每次都需要从头开始重新发明训练的方法？
- en: In this chapter, we will first look at the powerful and widely used training
    approach in modern deep learning based applications named **Transfer Learning**
    through hands-on examples with real datasets (`MNIST`, `cars vs cats vs dogs vs
    flower`, `LFW`). Also, you will build deep learning-based network over large distributed
    clusters using Apache Spark and BigDL. Then you will combine both Transfer Learning
    and GAN to generate high resolution realistic images with facial datasets. Finally,
    you will also understand how to create artistic hallucination on images beyond
    GAN.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先通过使用真实数据集（`MNIST`，`cars vs cats vs dogs vs flower`，`LFW`）的实际操作示例，了解现代深度学习应用中一种强大且广泛使用的训练方法——**迁移学习**。此外，您还将使用Apache
    Spark和BigDL在大型分布式集群上构建基于深度学习的网络。然后，您将结合迁移学习和GAN，利用面部数据集生成高分辨率的真实图像。最后，您还将了解如何在图像上创造艺术性的幻觉，超越GAN的范畴。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将覆盖以下主题：
- en: What is Transfer Learning?—its benefits and applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是迁移学习？——其优势和应用
- en: Classifying `cars vs dog vs flower` with pre-trained VGG model using Keras
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练VGG模型和Keras对`cars vs dog vs flower`进行分类
- en: Training and deploying a deep network over large distributed clusters with Apache
    Spark—deep learning pipeline
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Apache Spark在大型分布式集群上训练和部署深度网络——深度学习流水线
- en: Identifying handwritten digits through feature extraction and fine tuning using
    BigDL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征提取和使用BigDL微调来识别手写数字
- en: High resolution image generation using pre-trained model and SRGAN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练模型和SRGAN生成高分辨率图像
- en: Generating artistic hallucinated images with DeepDream and image generation
    with VAE
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DeepDream生成艺术性幻觉图像，并通过VAE进行图像生成
- en: Building a deep learning model from scratch requires sophisticated resources
    and also it is very time consuming. And hence you don't always want to build such
    deep models from scratch to solve your problem at hand. Instead of reinventing
    the same wheel, you will reuse an already existing model built for similar problems
    to satisfy your use case.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始构建一个深度学习模型需要复杂的资源，并且非常耗时。因此，您不一定要从零开始构建这样的深度模型来解决手头的问题。您可以通过重用已经为类似问题构建的现有模型来满足您的需求，而不是重新发明同样的轮子。
- en: Let's say you want to build a self-driving car. You can either to spend years
    building a decent image recognition algorithm from scratch or you can simply take
    the pre-trained inception model built by Google from a huge dataset of ImageNet.
    A pre-trained model may not reach the desired accuracy level for your application,
    but it saves huge effort required to reinvent the wheel. And with some fine tuning
    and tricks your accuracy level will definitely improve.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想要构建一辆自动驾驶汽车。您可以选择花费数年时间从零开始构建一个合适的图像识别算法，或者您可以直接使用Google基于ImageNet大数据集构建的预训练Inception模型。预训练模型可能无法达到您应用所需的准确度，但它能节省大量重新发明轮子的工作。通过一些微调和技巧，您的准确度水平肯定会提高。
- en: Introduction to Transfer Learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习简介
- en: Pre-trained models are not optimized for tackling user specific datasets, but
    they are extremely useful for the task at hand that has similarity with the trained
    model task.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型并不是针对特定用户数据集进行优化的，但对于与训练模型任务相似的当前任务，它们非常有用。
- en: For example, a popular model, InceptionV3, is optimized for classifying images
    on a broad set of 1000 categories, but our domain might be to classify some dog
    breeds. A well-known technique used in deep learning that adapts an existing trained
    model for a similar task to the task at hand is known as Transfer Learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个流行的模型InceptionV3经过优化用于对1000个类别的图像进行分类，但我们的领域可能是分类某些狗的品种。深度学习中常用的技术之一就是迁移学习，它将一个已经训练好的模型适应于类似任务的当前任务。
- en: And this is why Transfer Learning has gained a lot of popularity among deep
    learning practitioners and in recent years has become the go-to technique in many
    real-life use cases. It is all about transferring knowledge (or features) among
    related domains.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么迁移学习在深度学习从业者中获得了广泛的关注，并且近年来在许多实际应用中成为了首选技术的原因。它的核心是将知识（或特征）在相关领域之间进行迁移。
- en: The purpose of Transfer Learning
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习的目的
- en: Let's say you have trained a deep neural network to differentiate between fresh
    mango and rotten mango. During training the network will have required thousands
    of rotten and fresh mango images and hours of training to learn knowledge such
    as if any fruit is rotten, liquid will come out of it and it will produce a bad
    smell. Now with this training experience the network can be used for different
    tasks/use-cases to differentiate between rotten apples and fresh apples using
    the knowledge of the rotten features learned during training of mango images.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你已经训练了一个深度神经网络来区分新鲜芒果和烂芒果。在训练过程中，网络需要成千上万张新鲜和烂芒果的图片以及数小时的训练，以学习一些知识，例如，如果任何水果腐烂，液体会流出来并产生难闻的气味。现在，凭借这一训练经验，网络可以用于其他任务/用例，利用在芒果图片训练过程中学到的腐烂特征知识来区分烂苹果和新鲜苹果。
- en: The general approach of Transfer Learning is to train a base network and then
    copy its first n layers to the first n layers of a target network. The remaining
    layers of the target network are initialized randomly and trained toward the targeted
    use case.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的一般方法是训练一个基础网络，然后将其前n层复制到目标网络的前n层。目标网络的剩余层随机初始化并朝着目标用例进行训练。
- en: 'The main scenarios for using Transfer Learning in your deep learning workflow
    are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迁移学习的主要场景如下：
- en: '**Smaller datasets**: When you have a smaller dataset, building a deep learning
    model from scratch won''t work well. Transfer Learning provides the way to apply
    a pre-trained model to new classes of data. Let''s say a pre-trained model built
    from one million images of ImageNet data will converge to a decent solution (after
    training on just a fraction of the available smaller training data, for example,
    CIFAR-10) compared to a deep learning model built with a smaller dataset from
    scratch.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**较小的数据集**：当你拥有较小的数据集时，从头开始构建深度学习模型效果不好。迁移学习提供了一种方法，能够将预训练的模型应用于新类别的数据。例如，一个从ImageNet数据的百万张图片中训练出来的预训练模型，相较于一个从小数据集从头开始训练的深度学习模型，即使只用一小部分较小训练数据（例如CIFAR-10），也能收敛到一个不错的解决方案。'
- en: '**Less resources**: Deep learning processes (such as convolution) require a
    significant amount of resource and time. Deep learning processes are well suited
    to run on high grade GPU-based machines. But with pre-trained models, you can
    easily train across a full training set (let''s say 50,000 images) in less than
    a minute using your laptop/notebook without GPU, since the majority of time a
    model is modified in the final layer with a simple update of just a classifier
    or regressor.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更少的资源**：深度学习过程（如卷积）需要大量的资源和时间。深度学习过程非常适合在高性能基于GPU的机器上运行。但是，通过使用预训练模型，你可以轻松地在不到一分钟的时间内，利用没有GPU的笔记本电脑/笔记本对完整的训练集（比如说50,000张图片）进行训练，因为大多数时候，模型仅在最终层进行修改，通过简单地更新一个分类器或回归器来完成。'
- en: Various approaches of using pre-trained models
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预训练模型的各种方法
- en: 'We will discuss how pre-trained model could be used in different ways:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论如何以不同的方式使用预训练模型：
- en: '**Using pre-trained architecture**: Instead of transferring weights of the
    trained model, we can only use the architecture and initialize our own random
    weights to our new dataset.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用预训练架构**：我们可以仅使用架构并为新的数据集初始化随机权重，而不是转移训练好的模型的权重。'
- en: '**Feature extractor**: A pre-trained model can be used as a feature extraction
    mechanism just by simply removing the output layer of the network (that gives
    the probabilities for being in each of the n classes) and then freezing all the
    previous layers of the network as a fixed feature extractor for the new dataset.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取器**：预训练模型可以作为特征提取机制使用，只需简单地去除网络的输出层（即提供每个类别的概率），然后冻结网络的所有前层，将其作为新数据集的固定特征提取器。'
- en: '**Partially freezing the network**: Instead of replacing only the final layer
    and extracting features from all previous layers, sometimes we might train our
    new model partially (that is, to keep the weights of initial layers of the network
    frozen while retraining only the higher layers). The choice of the number of frozen
    layers can be considered as one more hyper-parameter.![Various approaches of using
    pre-trained models](img/B08086_05_01.png.jpg)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部分冻结网络**：有时我们会训练我们的新模型的部分网络（即保持网络初始层的权重被冻结，只重新训练较高层的权重），而不是仅替换最后一层并从所有前一层提取特征。冻结的层数可以看作是另一个超参数。![使用预训练模型的各种方法](img/B08086_05_01.png.jpg)'
- en: 'Figure-1: Transfer Learning with a pre-trained model'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图-1：使用预训练模型进行迁移学习
- en: 'Depending mainly on data size and dataset similarity, you might have to decide
    on how to proceed with Transfer Learning. The following table discusses these
    scenarios:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 主要取决于数据大小和数据集的相似性，您可能需要决定如何进行迁移学习。以下表格讨论了这些场景：
- en: '|   | High data similarity | Low data similarity |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|   | 高数据相似性 | 低数据相似性 |'
- en: '| --- | --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Data size small** | In the scenario of small data size but high data similarity,
    we will modify only the output layers of the pre-trained model and use it as a
    feature extractor. | When both data size as well as data similarity is low, we
    can freeze initial *k* layers of the pre-trained network and train only the *(n-k)*
    remaining layers again. This will help the top layers to customize to the new
    dataset and the small data size will also get compensated by frozen initial *k*
    layers of the network. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **数据集较小** | 在数据量小但数据相似性高的情况下，我们只会修改预训练模型的输出层，并将其作为特征提取器使用。 | 当数据量和数据相似性都较低时，我们可以冻结预训练网络的前*k*层，只重新训练剩余的*(n-k)*层。这样，顶层可以根据新数据集进行定制，而小数据量也可以通过冻结的初始*k*层得到补偿。
    |'
- en: '| **Data size large** | In this scenario, we can use the architecture and initial
    weights of the pre-trained model. | Although we have a large dataset, the data
    is very different compared to the one used for training the pre-trained model,
    so using it in this scenario would not be effective. Instead it is better to train
    the deep network from scratch. |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| **数据集较大** | 在这种情况下，我们可以使用预训练模型的架构和初始权重。 | 尽管我们有一个较大的数据集，但与用于训练预训练模型的数据相比，数据差异很大，因此在这种情况下使用预训练模型效果不佳。相反，最好从头开始训练深度网络。
    |'
- en: In case of image recognition Transfer Learning utilize the pre-trained convolutional
    layers to extract features about the new input images, that means only a small
    part of the original model (mainly the dense layers) are retrained. The rest of
    the network remains frozen. In this way, it saves a lot of time and resource by
    passing the raw images through the frozen part of the network only once, and then
    never goes through that part of the network again.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像识别中，迁移学习利用预训练的卷积层提取新输入图像的特征，这意味着只有原始模型的一小部分（主要是全连接层）需要重新训练。其余的网络保持冻结。通过这种方式，它通过仅将原始图像传递给冻结部分的网络一次，并且以后再也不会经过这部分网络，节省了大量时间和资源。
- en: Classifying car vs cat vs dog vs flower using Keras
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras进行汽车、猫、狗和花的分类
- en: Let us implement the concept of Transfer Learning and fine-tuning to identify
    customizable object categories using a customized dataset consisting of 150 training
    images and 50 validation images for each category of car, cat, dog, and flower.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现迁移学习和微调的概念，使用定制数据集识别可定制的物体类别。每个类别（汽车、猫、狗和花）有150张训练图像和50张验证图像。
- en: Note that the dataset is prepared by taking images from the *Kaggle Dogs vs.Cats*
    ([https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats)),
    Stanford cars ([http://ai.stanford.edu/~jkrause/cars/car_dataset.html](http://ai.stanford.edu/~jkrause/cars/car_dataset.html)),
    and `Oxford flower` dataset ([http://www.robots.ox.ac.uk/~vgg/data/flowers](http://www.robots.ox.ac.uk/~vgg/data/flowers)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，数据集是通过采集来自*Kaggle Dogs vs.Cats*（[https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats)）、斯坦福汽车数据集（[http://ai.stanford.edu/~jkrause/cars/car_dataset.html](http://ai.stanford.edu/~jkrause/cars/car_dataset.html)）和`Oxford
    flower`数据集（[http://www.robots.ox.ac.uk/~vgg/data/flowers](http://www.robots.ox.ac.uk/~vgg/data/flowers)）的图像准备的。
- en: '![Classifying car vs cat vs dog vs flower using Keras](img/B08086_05_02.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras进行汽车、猫、狗和花的分类](img/B08086_05_02.jpg)'
- en: 'Figure-2: Car vs Cat vs Dog vs Flower dataset structure'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图-2：汽车 vs 猫 vs 狗 vs 花的数据集结构
- en: 'We need to perform some preprocessing using the `preprocessing` function and
    apply various data augmentation transformation through `rotation`, `shift`, `shear`,
    `zoom`, and `flip` parameters:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用`preprocessing`函数进行一些预处理，并通过`rotation`、`shift`、`shear`、`zoom`和`flip`等参数应用各种数据增强变换：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we need to load the InceptionV3 model from the `keras.applications` module.
    The flag `include_top=False` is used to leave out the weights of the last fully
    connected layer:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要从`keras.applications`模块加载InceptionV3模型。`include_top=False`标志用于省略最后一层全连接层的权重：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Initialize a new last layer by adding fully-connected `Dense` layer of size
    1024, followed by a `softmax` function on the output to squeeze the values between
    `[0,1]`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加大小为1024的全连接`Dense`层并在输出上应用`softmax`函数来初始化一个新的最后一层，以将值压缩到`[0,1]`之间：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once the last layer of the network is stabilized (Transfer Learning), we can
    move onto retraining more layers (fine-tuning).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络的最后一层稳定下来（迁移学习），我们就可以开始重新训练更多的层（微调）。
- en: 'Use a utility method to freeze all layers and compile the model:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个实用方法冻结所有层并编译模型：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is another utility method to freeze the bottom of the top two inception
    blocks in the InceptionV3 architecture and retrain the remaining top:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个实用方法，用于冻结InceptionV3架构中前两个Inception模块的底部并重新训练剩余的顶部：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we''re all ready for training using the `fit_generator` method and finally
    save our model:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已准备好使用`fit_generator`方法进行训练，并最终保存我们的模型：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Run the following command for training and fine tuning:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下命令进行训练和微调：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Classifying car vs cat vs dog vs flower using Keras](img/B08086_05_03.png.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras对汽车、猫、狗和花进行分类](img/B08086_05_03.png.jpg)'
- en: 'Even with such a small dataset size, we are able to achieve an accuracy of
    98.5 percent on the validation set by utilizing the power of the pre-trained model
    and Transfer Learning:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 即使数据集如此小，我们仍然能够利用预训练模型和迁移学习的优势，在验证集上实现98.5%的准确率：
- en: '![Classifying car vs cat vs dog vs flower using Keras](img/B08086_05_04.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras对汽车、猫、狗和花进行分类](img/B08086_05_04.jpg)'
- en: 'Voila, we can now use the saved model to predict images (either from the local
    filesystem or from the URL) with test data:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！现在我们可以使用保存的模型来预测图像（无论是来自本地文件系统还是URL）并进行测试：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Classifying car vs cat vs dog vs flower using Keras](img/B08086_05_05.png.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![使用Keras对汽车、猫、狗和花进行分类](img/B08086_05_05.png.jpg)'
- en: Large scale deep learning with Apache Spark
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行大规模深度学习
- en: Deep learning is a resource hungry and computationally intensive process and
    you get better result with more data and bigger network, but its speed gets impacted
    by the size of the datasets as well. And in practice, deep learning requires experimenting
    with different values for training parameters known as hyper-parameter tuning,
    where you have to run your deep networks on a large dataset iteratively or many
    times and speed does matter. Some common ways to tackle this problem is to use
    faster hardware (usually GPUs), optimized code (with a proper production-ready
    framework), and scaling out over distributed clusters to achieve some form of
    parallelism.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一个资源密集且计算密集的过程，拥有更多数据和更大的网络可以获得更好的结果，但其速度也会受到数据集大小的影响。在实践中，深度学习需要通过调整训练参数（即超参数调整）来进行实验，在这个过程中，你需要多次迭代或运行深度网络并且速度至关重要。常见的解决方法是使用更快的硬件（通常是GPU）、优化的代码（使用适合生产环境的框架）和通过分布式集群扩展来实现某种形式的并行性。
- en: Data parallelism is a concept of sharding large datasets into multiple chunks
    of data and then processing chunks over neural networks running on separate nodes
    of distributed clusters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行性是将大型数据集分割成多个数据块，然后在分布式集群的独立节点上运行神经网络处理这些数据块的概念。
- en: Apache Spark is a fast, general-purpose, fault-tolerant framework for interactive
    and iterative computations on large, distributed datasets by doing in-memory processing
    of RDDs or DataFrames instead of saving data to hard disks. It supports a wide
    variety of data sources as well as storage layers. It provides unified data access
    to combine different data formats, streaming data, and defining complex operations
    using high-level, composable operators.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个快速、通用、容错的框架，用于在大规模分布式数据集上进行交互式和迭代计算，它通过对RDD或DataFrame进行内存处理，而不是将数据保存到硬盘中，来实现大数据集的处理。它支持多种数据源以及存储层，并提供统一的数据访问，能够结合不同的数据格式、流数据，并使用高级可组合操作符定义复杂的操作。
- en: Today Spark is the superpower of big data processing and makes big data accessible
    to everyone. But Spark or its core modules alone are not capable of training or
    running deep networks over the clusters. In the next few sections, we will develop
    deep learning applications over Apache Spark cluster with optimized libraries.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，Spark是大数据处理的超能力，使每个人都能访问大数据。但仅凭Spark或其核心模块无法在集群上训练或运行深度网络。在接下来的几个部分，我们将利用优化后的库在Apache
    Spark集群上开发深度学习应用程序。
- en: Note
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For coding purposes, we will not cover the distributed Spark cluster setup,
    instead use Apache Spark standalone mode. More information about Spark cluster
    mode can be found at: [https://spark.apache.org/docs/latest/cluster-overview.html](https://spark.apache.org/docs/latest/cluster-overview.html).'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 出于编码目的，我们将不涵盖分布式Spark集群的设置，而是使用Apache Spark独立模式。有关Spark集群模式的更多信息，请参见：[https://spark.apache.org/docs/latest/cluster-overview.html](https://spark.apache.org/docs/latest/cluster-overview.html)。
- en: Running pre-trained models using Spark deep learning
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Spark深度学习运行预训练模型
- en: Deep learning pipelines is an open-source library that leverages the power of
    Apache Spark cluster to easily integrate scalable deep learning into machine learning
    workflows. It is built on top of Apache Spark's ML Pipelines for training, and
    uses Spark DataFrames and SQL for deploying models. It provides high-level APIs
    for running Transfer Learning in a distributed manner by integrating pre-trained
    model as transformer in Spark ML Pipeline.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习管道是一个开源库，利用Apache Spark集群的强大功能，轻松将可扩展的深度学习集成到机器学习工作流中。它建立在Apache Spark的ML管道之上进行训练，并使用Spark
    DataFrame和SQL部署模型。它提供高级API，通过将预训练模型作为转换器集成到Spark ML管道中，以分布式方式运行迁移学习。
- en: Deep learning pipelines make Transfer Learning easier with the concept of a
    featurizer. The featurizer (or `DeepImageFeaturizer` in case of image operation)
    automatically removes the last layer of a pre-trained neural network model and
    uses all the previous layers output as features for the classification algorithm
    (for example, logistic regression) specific to the new problem domain.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习管道通过特征提取器的概念简化了迁移学习。特征提取器（在图像操作中是`DeepImageFeaturizer`）自动去除预训练神经网络模型的最后一层，并将所有前面的层输出作为特征，供特定于新问题领域的分类算法（例如逻辑回归）使用。
- en: 'Let us implement deep learning pipelines for predicting sample images of the
    `flower` dataset ([http://download.tensorflow.org/example_images/flower_photos.tgz](http://download.tensorflow.org/example_images/flower_photos.tgz))
    with a pre-trained model over the Spark cluster:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Spark集群上实现深度学习管道，预测`flower`数据集（[http://download.tensorflow.org/example_images/flower_photos.tgz](http://download.tensorflow.org/example_images/flower_photos.tgz)）的示例图像，使用预训练模型：
- en: 'First start the PySpark with the deep learning pipeline package:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，启动带有深度学习管道包的PySpark：
- en: '[PRE8]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Tip
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: 'Note: If you get the error **No module named sparkdl** while starting the PySpark
    with deep learning, please check the GitHub page for workaround:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意：如果在启动PySpark时遇到**No module named sparkdl**的错误，请查看GitHub页面以获取解决方法：
- en: '[https://github.com/databricks/spark-deep-learning/issues/18](https://github.com/databricks/spark-deep-learning/issues/18)'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://github.com/databricks/spark-deep-learning/issues/18](https://github.com/databricks/spark-deep-learning/issues/18)'
- en: First read the images and randomly split it into `train`, `test` set.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，读取图像并将其随机拆分为`train`和`test`集。
- en: '[PRE9]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then create a pipeline with `DeepImageFeaturizer` using the `InceptionV3` model:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用`InceptionV3`模型通过`DeepImageFeaturizer`创建一个管道：
- en: '[PRE10]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now fit the images with an existing pre-trained model, where `train_images_df`
    is a dataset of images and labels:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用现有的预训练模型来拟合图像，其中`train_images_df`是图像和标签的数据集：
- en: '[PRE11]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we will evaluate the accuracy:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将评估准确性：
- en: '[PRE12]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Running pre-trained models using Spark deep learning](img/B08086_05_06.png.jpg)'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![使用Spark深度学习运行预训练模型](img/B08086_05_06.png.jpg)'
- en: 'In addition to `DeepImageFeaturizer`, we can also utilize the pre-existing
    model just to do prediction, without any retraining or fine tuning using `DeepImagePredictor`:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`DeepImageFeaturizer`，我们还可以仅使用预先存在的模型进行预测，无需任何重新训练或微调，使用`DeepImagePredictor`：
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The input image and its top five predictions are shown as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像及其前五个预测结果如下所示：
- en: '![Running pre-trained models using Spark deep learning](img/B08086_05_07.png.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark深度学习运行预训练模型](img/B08086_05_07.png.jpg)'
- en: In addition to using the built-in pre-trained models, deep learning pipeline
    allows users to plug in Keras models or TensorFlow graphs in a Spark prediction
    pipeline. This really turns any single-node deep models running on a single-node
    machine into one that can be trained and deployed in a distributed fashion, on
    a large amount of data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用内置的预训练模型外，深度学习管道还允许用户在Spark预测管道中插入Keras模型或TensorFlow图。这实际上将任何在单节点机器上运行的单节点深度模型转化为可以在分布式环境下进行训练和部署的模型，并能够处理大量数据。
- en: 'First, we will load the Keras built-in InceptionV3 model and save it in the
    file:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将加载Keras内置的InceptionV3模型，并将其保存到文件中：
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'During the prediction phase, we will simply load the model and pass images
    through it to get the desired prediction:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测阶段，我们只需加载模型并将图像传递给它，以获得期望的预测结果：
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Deep learning pipeline is a really fast way of doing Transfer Learning over
    distributed Spark clusters. But you must have noticed that featurizer only allow
    us to change the final layer of the pre-trained model. But in some scenarios,
    you might have to modify more than one layer of the pre-trained network to get
    the desired result and deep learning pipeline doesn't provide this full capability.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习管道是一种在分布式Spark集群上进行迁移学习的非常快速的方法。但你一定注意到，特征提取器仅允许我们更改预训练模型的最终层。但是在某些场景下，你可能需要修改预训练网络的多个层以获得期望的结果，而深度学习管道并未提供这种完整的能力。
- en: Handwritten digit recognition at a large scale using BigDL
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用BigDL进行大规模手写数字识别
- en: 'BigDL is an open-source distributed high performance deep learning library
    that can run directly on top of Apache Spark clusters. Its high performance is
    achieved by combing **Intel® Math Kernel Library** (**MKL**) along with multithreaded
    programming in each Spark task. BigDL provides Keras style (both sequential and
    function) high-level APIs to build deep learning application and scale out to
    perform analytics at a large scale. The main purposes of using BigDL are:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: BigDL是一个开源的分布式高性能深度学习库，可以直接运行在Apache Spark集群上。其高性能通过结合**Intel® 数学核心库**（**MKL**）和每个Spark任务中的多线程编程来实现。BigDL提供类似Keras的高级API（包括顺序和功能模型），用于构建深度学习应用并扩展以执行大规模的分析。使用BigDL的主要目的包括：
- en: Running deep learning model at a large scale and analyzing massive amount of
    data residing in a Spark or Hadoop cluster (in say Hive, HDFS, or HBase)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark或Hadoop集群中运行深度学习模型并分析大量数据（比如在Hive、HDFS或HBase中）
- en: Adding deep learning functionality (both training and prediction) to your big
    data workflow![Handwritten digit recognition at a large scale using BigDL](img/B08086_05_08.png.jpg)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将深度学习功能（包括训练和预测）添加到你的大数据工作流中！[使用BigDL进行大规模手写数字识别](img/B08086_05_08.png.jpg)
- en: 'Figure-3: BigDL execution over Spark cluster'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3：BigDL在Spark集群上的执行
- en: As you can see from the figure, the BigDL driver program is first launched in
    the Spark master node of the cluster. Then with the help of **cluster manager**
    and the driver program, Spark tasks are distributed across the Spark executors
    on the worker nodes. And BigDL interacts with Intel MKL to enable faster execution
    of those tasks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，BigDL驱动程序首先在集群的Spark主节点上启动。然后，在**集群管理器**和驱动程序的帮助下，Spark任务会分配到工作节点上的Spark执行器。BigDL与Intel
    MKL交互，以加快这些任务的执行速度。
- en: 'Let us implement a deep neural network at a large scale for identifying hand-written
    digits with the `mnist` dataset. First we will prepare training and validation
    samples:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`mnist`数据集实现一个大规模的深度神经网络，用于识别手写数字。首先，我们将准备训练和验证样本：
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then we will create the LeNet architecture consisting of two sets of convolutional,
    activation, and pooling layers, followed by a fully-connected layer, activation,
    another fully-connected, and finally a `SoftMax` classifier. LeNet is small, yet
    powerful enough to provide interesting results:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将创建LeNet架构，包含两组卷积层、激活层和池化层，接着是一个全连接层、激活层、另一个全连接层，最后是一个`SoftMax`分类器。LeNet小巧但足够强大，能够提供有趣的结果：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we will configure an `Optimizer` and set the validation logic:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将配置一个`优化器`并设置验证逻辑：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then we will take a few test samples, and make prediction by checking both
    the predicted labels and the ground truth labels:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将取几个测试样本，并通过检查预测标签和真实标签来进行预测：
- en: '[PRE19]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, we will train the LeNet model in the Spark cluster using the `spark-submit`
    command. Download the BigDL distribution ([https://bigdl-project.github.io/master/#release-download/](https://bigdl-project.github.io/master/#release-download/))
    based on your Apache Spark version and then execute the file (`run.sh`) provided
    with the code to submit the job in the Spark cluster:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在Spark集群中使用`spark-submit`命令训练LeNet模型。根据你的Apache Spark版本下载BigDL发行版（[https://bigdl-project.github.io/master/#release-download/](https://bigdl-project.github.io/master/#release-download/)），然后执行代码中提供的文件（`run.sh`），以在Spark集群中提交任务：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'More information regarding `spark-submit` can be found at: [https://spark.apache.org/docs/latest/submitting-applications.html](https://spark.apache.org/docs/latest/submitting-applications.html).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有关`spark-submit`的更多信息，请访问：[https://spark.apache.org/docs/latest/submitting-applications.html](https://spark.apache.org/docs/latest/submitting-applications.html)。
- en: 'Once you have submitted the job, you can track the progress on the **Spark
    Master** application page as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你提交了任务，你可以通过**Spark Master**应用页面跟踪任务进度，如下所示：
- en: '![Handwritten digit recognition at a large scale using BigDL](img/B08086_05_09.png.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![使用BigDL进行大规模手写数字识别](img/B08086_05_09.png.jpg)'
- en: 'Figure-4: BigDL job of LeNet5 model running on Apache Spark cluster'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：在Apache Spark集群上运行的BigDL LeNet5模型任务
- en: 'After the job has successfully finished, you can search the logs of the Spark
    workers to verify the accuracy of your model similar to the one shown as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 任务成功完成后，你可以查看Spark工作节点的日志，以验证你的模型的准确性，类似于下图所示：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: High resolution image generation using SRGAN
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SRGAN生成高分辨率图像
- en: '**Super Resolution Generative Network** (**SRGAN)** excels in generating high
    resolution images from its low-resolution counterpart. During the training phase,
    a high resolution image is transformed to a low resolution image by applying the
    Gaussian filter to a high resolution image followed by the down-sampling operation.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**超分辨率生成网络**（**SRGAN**）在从低分辨率图像生成高分辨率图像方面表现出色。在训练阶段，通过对高分辨率图像应用高斯滤波器后进行下采样操作，将高分辨率图像转化为低分辨率图像。'
- en: 'Let us define some notation before diving into the network architecture:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解网络架构之前，让我们定义一些符号：
- en: '*I*^(*LR*): Low resolution image having the size width(*W*) x height(*H*) x
    color channels(*C*)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*I*^(*LR*): 低分辨率图像，大小为宽度(*W*) × 高度(*H*) × 颜色通道(*C*)'
- en: '*I*^(*HR*): High resolution image having the size *rW × rH × C*'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*I*^(*HR*): 高分辨率图像，大小为*rW × rH × C*'
- en: '*I*^(*SR*): Super resolution image having the size *rW × rH × C*'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*I*^(*SR*): 超分辨率图像，大小为*rW × rH × C*'
- en: '*r*: down sampling factor'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*r*: 下采样因子'
- en: '*G*[*θG*]: Generator network'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*G*[*θG*]: 生成器网络'
- en: '*D*[*θD*]: Discriminator network'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D*[*θD*]: 判别器网络'
- en: 'To achieve the goal of estimating a high resolution input image from its corresponding
    low resolution counterpart, the generator network is trained as a feed-forward
    convolution neural network *G*[*θG*] parametrized by *θG* where *θG* is represented
    by weights (*W1:L*) and biases (*b1:L*) of the L-layer of the deep network and
    is obtained by optimizing super resolution specific `loss` function. For training
    images having high resolution ![High resolution image generation using SRGAN](img/B08086_05_23.jpg),
    *n=1*; N along with its corresponding low resolution ![High resolution image generation
    using SRGAN](img/B08086_05_24.jpg), *n=1*, N, we can solve for *θG*, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现从低分辨率图像估计高分辨率输入图像的目标，生成器网络作为一个前馈卷积神经网络进行训练，*G*[*θG*]由*θG*参数化，其中*θG*由深度网络L层的权重（*W1:L*）和偏置（*b1:L*）表示，并通过优化超分辨率特定的`loss`函数得到。对于具有高分辨率的训练图像！[使用SRGAN生成高分辨率图像](img/B08086_05_23.jpg)，*n=1*；N及其对应的低分辨率图像！[使用SRGAN生成高分辨率图像](img/B08086_05_24.jpg)，*n=1*，N，我们可以按如下方式求解*θG*：
- en: '![High resolution image generation using SRGAN](img/B08086_05_10.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![使用SRGAN生成高分辨率图像](img/B08086_05_10.jpg)'
- en: Note
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The formulation of the perceptual loss function *lSR* is critical for the performance
    of the generator network. In general, perceptual loss is commonly modeled based
    on **Mean Square Error** (**MSE**), but to avoid unsatisfying solutions with overly
    smooth textures, a new content loss based on the ReLU activation layers of the
    pre-trained 19 layer VGG network is formulated.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 感知损失函数*lSR*的公式对生成器网络的性能至关重要。通常，感知损失基于**均方误差**（**MSE**）建模，但为了避免过于平滑的纹理带来的不理想结果，提出了一种新的基于预训练19层VGG网络的ReLU激活层的内容损失函数。
- en: 'The perceptual loss is the weighted combination of several `loss` functions
    that map important characteristics of the super resolution image as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 感知损失是几种`损失`函数的加权组合，这些损失函数映射超分辨率图像的重要特征，如下所示：
- en: '![High resolution image generation using SRGAN](img/B08086_05_11.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![使用SRGAN生成高分辨率图像](img/B08086_05_11.jpg)'
- en: '**Content loss**: The VGG-based content loss is defined as the Euclidean distance
    between the feature representations of a reconstructed image *G**[θG]* (*I*^(*LR*))
    and the corresponding high resolution image *I**HR*. Here ![High resolution image
    generation using SRGAN](img/B08086_05_25.jpg)[*i,j*] indicate the feature map
    obtained by the jth convolution (after activation) before the ith max-pooling
    layer within the VGG19 network. And *Wi,j*, *Hi,j* describe the dimensions of
    the respective feature maps within the *VGG* network:![High resolution image generation
    using SRGAN](img/B08086_05_12.jpg)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容损失**：基于VGG的内容损失定义为重建图像*G**[θG]* (*I*^(*LR*))与对应高分辨率图像*I**HR*之间的欧氏距离。这里![使用SRGAN生成高分辨率图像](img/B08086_05_25.jpg)[*i,j*]表示在VGG19网络中第i个最大池化层前的第j个卷积层（激活后）获得的特征图。而*Wi,j*、*Hi,j*描述了*VGG*网络中各个特征图的维度：![使用SRGAN生成高分辨率图像](img/B08086_05_12.jpg)'
- en: '**Adversarial loss**: The generative loss is based on the probabilities of
    the discriminator *D*[*θD*]*(G*[*θG*]*(I*[*LR*]*))* over all training images and
    encourages the network to favor solutions residing on the manifold of natural
    images, in order to fool the discriminator network:![High resolution image generation
    using SRGAN](img/B08086_05_13.jpg)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗损失**：生成损失基于判别器*D*[*θD*]*(G*[*θG*]*(I*[*LR*]*))对所有训练图像的概率，鼓励网络选择位于自然图像流形上的解决方案，以此来欺骗判别器网络：![使用SRGAN生成高分辨率图像](img/B08086_05_13.jpg)'
- en: 'Similar to the concept of adversarial network, the general idea behind the
    SRGAN approach is to train a generator *G* with the goal of fooling a discriminator
    *D* that is trained to distinguish super-resolution images from real images:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于对抗网络的概念，SRGAN方法背后的总体思想是训练一个生成器*G*，其目标是欺骗一个判别器*D*，该判别器经过训练，能够区分超分辨率图像和真实图像：
- en: '![High resolution image generation using SRGAN](img/B08086_05_14.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![使用SRGAN生成高分辨率图像](img/B08086_05_14.jpg)'
- en: Based on this approach the generator learns to create solutions that are highly
    similar to real images and thus hard to classify by discriminator *D*. And this
    encourages perceptually superior solutions residing in the subspace, the manifold,
    of natural images.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一方法，生成器学习创造出与真实图像高度相似的解决方案，从而使判别器*D*很难进行分类。这也鼓励感知上优越的解决方案位于自然图像的子空间流形上。
- en: Architecture of the SRGAN
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SRGAN架构
- en: As illustrated in the following figure, the generator network **G** consists
    of **B** **residual blocks** with identical layout. Each block has two convolutional
    layers with small 3×3 kernels and 64 feature maps followed by batch-normalization
    layers [32] and ParametricReLU [28] as the `activation` function. The resolution
    of the input image is increased by two trained sub-pixel convolution layers.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，生成器网络**G**由**B**个**残差块**组成，布局相同。每个块包含两个卷积层，使用小的3×3核和64个特征图，随后是批归一化层[32]和ParametricReLU[28]作为`激活`函数。输入图像的分辨率通过两个训练过的子像素卷积层增加。
- en: The discriminator network uses Leaky ReLU activation (with *α* = 0.2) and consists
    of eight convolutional layers with an increasing number of 3×3 filter kernels,
    increasing by a factor of 2 from 64 to 512 kernels. Each time the number of features
    is doubled, strided convolutions are used to reduce the image resolution.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络使用Leaky ReLU激活函数（*α* = 0.2），并由八个卷积层组成，卷积核的数量逐渐增加，从64个3×3卷积核增加到512个，每次特征数量翻倍，使用步幅卷积来减少图像分辨率。
- en: 'The resulting 512 feature maps go through two dense layers followed by a final
    sigmoid activation layer to obtain a classification probability for a generated
    image sample:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的512个特征图通过两个全连接层，随后通过最终的sigmoid激活层，获得生成图像样本的分类概率：
- en: '![Architecture of the SRGAN](img/B08086_05_15.png.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![SRGAN架构](img/B08086_05_15.png.jpg)'
- en: 'Figure-5: Architecture of generator and discriminator network with corresponding
    kernel size (k), number of feature maps (n) and stride (s) indicated for each
    convolutional layer.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：生成器和判别器网络的架构，标注了每个卷积层的对应核大小（k）、特征图数量（n）和步幅（s）。
- en: 'Source: *arXiv, 1609.04802, 2017*'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：*arXiv, 1609.04802, 2017*
- en: Now it's time to deep dive into the code with TensorFlow and generate high resolution
    images using an LFW facial dataset.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候深入了解TensorFlow代码，并使用LFW人脸数据集生成高分辨率图像了。
- en: 'The generator network is first built as a single deconvolution layer with 3×3
    kernels and 64 feature maps followed by ReLU as an `activation` function. Then
    there are five residual blocks with identical layout of each block having two
    convolutional layers followed by batch-normalization and ReLU. Finally, the resolution
    of the input image is increased by two trained pixel-shuffle layers:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络首先构建为一个单独的反卷积层，具有3×3的内核和64个特征图，后接ReLU作为`activation`函数。接下来是五个残差块，每个块具有两个卷积层，后接批量归一化和ReLU。最后，通过两个训练过的像素洗牌层增加输入图像的分辨率：
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `deconvolution layer` function is defined with a TensorFlow `conv2d_transpose`
    method with Xavier initialization as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`deconvolution layer`函数使用TensorFlow的`conv2d_transpose`方法定义，并采用Xavier初始化，具体如下：'
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The discriminator network consists of eight convolutional layers having 3×3
    filter kernels that get increased by a factor of 2 from 64 to 512 kernels. The
    resulting 512 feature maps are flattened and go through two dense fully connected
    layers followed by a final softmax layer to obtain a classification probability
    for a generated image sample:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器网络由八个卷积层组成，使用3×3的滤波器内核，内核数量从64逐步增加至512。最终的512个特征图被拉平，并通过两层密集的全连接层，最后通过一个softmax层以获得生成图像样本的分类概率：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The network uses LeakyReLU (with *α* = 0.2) as an `activation` function with
    the convolutional layer:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 网络使用LeakyReLU（*α* = 0.2）作为卷积层的`activation`函数：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `convolution layer` function is defined with a TensorFlow `conv2d` method
    with Xavier initialization as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`convolution layer`函数使用TensorFlow的`conv2d`方法定义，并采用Xavier初始化，具体如下：'
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Please note that the code implementation uses the least squares `loss` function
    (to avoid the vanishing gradient problem) for the discriminator, instead of the
    sigmoid cross entropy `loss` function as proposed in the original paper of SRGAN
    (*arXiv, 1609.04802, 2017*):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，代码实现使用了最小二乘`loss`函数（以避免梯度消失问题）来代替SRGAN原论文中提出的sigmoid交叉熵`loss`函数（*arXiv,
    1609.04802, 2017*）：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'More information about **Least Square GAN** can be found at: [https://arxiv.org/abs/1611.04076](https://arxiv.org/abs/1611.04076)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有关**最小二乘GAN**的更多信息，请访问：[https://arxiv.org/abs/1611.04076](https://arxiv.org/abs/1611.04076)
- en: 'The `code` directory structure for running the SRGAN is shown as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 运行SRGAN的`code`目录结构如下：
- en: '![Architecture of the SRGAN](img/B08086_05_16.png.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![SRGAN架构](img/B08086_05_16.png.jpg)'
- en: 'First let us download an `LFW facial` dataset and do some preprocessing (frontal
    face detection and splitting the dataset as train and test) and store the dataset
    under the `data/` directory:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们下载一个`LFW人脸`数据集并进行一些预处理（正面人脸检测以及将数据集拆分为训练集和测试集），并将数据集存储在`data/`目录下：
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Architecture of the SRGAN](img/B08086_05_17.png.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![SRGAN架构](img/B08086_05_17.png.jpg)'
- en: 'Next download the pre-trained VGG19 model from the following link, extract
    it, and save it under the `backup/` directory:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来从以下链接下载预训练的VGG19模型，解压后保存在`backup/`目录下：
- en: '[https://drive.google.com/open?id=0B-s6ok7B0V9vcXNfSzdjZ0lCc0k](https://drive.google.com/open?id=0B-s6ok7B0V9vcXNfSzdjZ0lCc0k)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://drive.google.com/open?id=0B-s6ok7B0V9vcXNfSzdjZ0lCc0k](https://drive.google.com/open?id=0B-s6ok7B0V9vcXNfSzdjZ0lCc0k)'
- en: 'Next execute the `trainSrgan.py` file to start the SRGAN operation using a
    VGG19 model:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来执行`trainSrgan.py`文件，使用VGG19模型开始SRGAN操作：
- en: '[PRE29]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Once the training is going on the generator network will start generating super
    resolution images in the `result/` directory. Some of the sample images from the
    `result/` directory are shown as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练开始，生成器网络将开始在`result/`目录中生成超分辨率图像。以下是从`result/`目录中展示的一些示例图像：
- en: '![Architecture of the SRGAN](img/B08086_05_18.png.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![SRGAN架构](img/B08086_05_18.png.jpg)'
- en: Generating artistic hallucinated images using DeepDream
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DeepDream生成艺术化的幻觉图像
- en: The DeepDream algorithm is a modified neural network that has capability of
    producing impressive surrealistic, dream-like hallucinogenic appearances by changing
    the image in the direction of training data. It uses backpropagation to change
    the image instead of changing weights through the network.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: DeepDream算法是一种改进的神经网络，通过改变图像以适应训练数据的方向，从而能够产生令人印象深刻的超现实、梦幻般的幻觉效果。它使用反向传播改变图像，而不是通过网络改变权重。
- en: 'Broadly, the algorithm can be summarized in the following steps:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 大致上，算法可以概括为以下步骤：
- en: Select a layer of the network and a filter that you feel is interesting.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个你认为有趣的网络层和滤波器。
- en: Then compute the activations of the image up to that layer.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后计算图像在该层的激活值。
- en: Back-propagate the activations of the filter back to the input image.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将滤波器的激活反向传播回输入图像。
- en: Multiply the gradients with learning rate and add them to the input image.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将梯度与学习率相乘，并将其添加到输入图像中。
- en: Repeat steps 2 to 4 until you are satisfied with the result.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2到4，直到对结果满意为止。
- en: Applying the algorithm iteratively on the output and applying some zooming after
    each iteration helps the network to generate an endless stream of new impressions
    by exploring the set of things that it knows about.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出上迭代地应用算法，并在每次迭代后进行一些缩放，帮助网络通过探索它已知的事物集合，生成源源不断的新印象。
- en: 'Let''s deep dive into the code to generate a hallucinogenic dreamy image. We
    will apply the following settings to the various layers of the pre-trained VGG16
    model available in Keras. Note that instead of using pre-trained we can apply
    this setting to a fresh neural network architecture of your choice as well:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入代码，生成一个致幻的梦幻图像。我们将对Keras中预训练的VGG16模型的各个层应用以下设置。请注意，除了使用预训练模型外，我们也可以将这个设置应用于你选择的全新神经网络架构：
- en: '[PRE30]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This utility function basically loads, resizes, and formats images to an appropriate
    tensors format:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实用函数基本上加载、调整大小并将图像格式化为适当的张量格式：
- en: '[PRE31]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then we calculate the continuity loss to give the image a local coherence and
    avoid messy blurs that look like a variant of the total variation loss discussed
    in the paper ([http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf](http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf)):'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算连续性损失，以使图像具有局部连贯性，并避免出现像论文中讨论的全变差损失变体那样的模糊混乱（[http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf](http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf)）：
- en: '[PRE32]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we will load the VGG16 model with pretrained weights:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将加载VGG16模型，并使用预训练的权重：
- en: '[PRE33]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'After that we will add the `l2` norm of the features of a layer to the `loss`
    and then add continuity loss to give the image local coherence followed by adding
    again `l2` norm to loss in order to prevent pixels from taking very high values
    and then compute the gradients of the dream with respect to the loss:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将添加层特征的`l2`范数到`loss`中，然后再添加连续性损失，以提供图像的局部连贯性，接着再次将`l2`范数添加到损失中，以防止像素值过高，然后计算与损失相关的梦境梯度：
- en: '[PRE34]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, we will add a `random_jitter` to the input image and run the `L-BFGS`
    optimizer over the pixels of the generated image to minimize the loss:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在输入图像上添加`random_jitter`，并对生成图像的像素运行`L-BFGS`优化器，以最小化损失：
- en: '[PRE35]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'At the end, we will decode the dream and save it in an output image file:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将解码梦境并将其保存在输出图像文件中：
- en: '[PRE36]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The dreamy output generated just after five iterations is shown as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 仅经过五次迭代生成的梦幻输出如下所示：
- en: '![Generating artistic hallucinated images using DeepDream](img/B08086_05_19.png.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![使用DeepDream生成艺术性幻觉图像](img/B08086_05_19.png.jpg)'
- en: 'Figure-6: Left showing the original input image and right showing the dreamy
    artistic image created by deep dream'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图-6：左侧显示原始输入图像，右侧显示由DeepDream创建的梦幻艺术图像
- en: Generating handwritten digits with VAE using TensorFlow
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow生成手写数字与VAE
- en: The **Variational Autoencoder** (**VAE**) nicely synthesizes unsupervised learning
    with variational Bayesian methods into a sleek package. It applies a probabilistic
    turn on the basic autoencoder approach by treating inputs, hidden representations,
    and reconstructed outputs as probabilistic random variables within a directed
    graphical model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**变分自编码器**（**VAE**）将无监督学习与变分贝叶斯方法巧妙地结合在一起，形成一个精简的包。它通过将输入、隐藏表示和重建输出视为有向图模型中的概率随机变量，在基本的自编码器方法上加入了概率化的转折。'
- en: From Bayesian perspective, the encoder becomes a variational inference network,
    that maps the observed inputs to posterior distributions over latent space, and
    the decoder becomes a generative network that maps the arbitrary latent coordinates
    back to distributions over the original data space.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从贝叶斯的角度来看，编码器变成了一个变分推理网络，它将观察到的输入映射到潜在空间的后验分布，而解码器则变成了一个生成网络，将任意的潜在坐标映射回原始数据空间的分布。
- en: 'VAE is all about adding a constraint on the encoding network that generates
    latent vectors that roughly follow a unit Gaussian distribution (this constraint
    that separates a VAE from a standard autoencoder) and then reconstructs the image
    back by passing the latent vector through the decoder network:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的核心在于对编码网络添加约束，使其生成的大致遵循单位高斯分布的潜在向量（这个约束使得VAE与标准自编码器有所区别），然后通过解码器网络将潜在向量重新构建成图像：
- en: '![Generating handwritten digits with VAE using TensorFlow](img/B08086_05_22.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![使用TensorFlow生成手写数字的VAE](img/B08086_05_22.jpg)'
- en: A real world analogy of VAE
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VAE的现实世界类比
- en: Let's say we want to generate data (an animal) and a good way of doing it is
    to first decide what kind of data we want to generate, before actually generating
    the data. So, we must imagine some criteria about representing the animal, like
    it should have four legs and be able to swim. Once we have those criteria, we
    can then generate the animal by sampling from the animal kingdom. Our imagination
    criteria are analogous to latent variables. Deciding the latent variable in the
    first place helps to describe the data well, otherwise it is like generating data
    blindly.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想生成数据（一个动物），一种有效的方式是先决定我们想生成什么样的数据，再实际生成数据。因此，我们必须想象一些关于如何描述动物的标准，比如它应该有四条腿并能够游泳。一旦我们有了这些标准，我们就可以通过从动物王国中采样来生成动物。我们的想象标准类似于潜在变量。一开始决定潜在变量有助于很好地描述数据，否则就像是盲目地生成数据。
- en: 'The basic idea of VAE is to infer *p(z)* using *p(z|x)*. Let''s now expand
    with some mathematical notation:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的基本思想是通过 *p(z|x)* 推断 *p(z)*。现在让我们使用一些数学符号来展开：
- en: '*x*: Represents the data (that is, animal) we want to generate'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*：表示我们想生成的数据（即动物）'
- en: '*z*: Represents the latent variable (that is, our imagination)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*z*：表示潜在变量（即我们的想象力）'
- en: '*p(x)*: Represents the distribution of data (that is, animal kingdom)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(x)*：表示数据的分布（即动物王国）'
- en: '*p(z)*: Represents the normal probability distribution of the latent variable
    (that is, the source of imagination—our brain)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(z)*：表示潜在变量的正态概率分布（即想象力的来源——我们的脑袋）'
- en: '*p(x|z)*: Probability distribution of generating data given the latent variable
    (that is, turning imagination into a realistic animal)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(x|z)*：给定潜在变量生成数据的概率分布（即将想象力转化为现实的动物）'
- en: As per the analogous example, we want to restrict our imagination only on the
    animal kingdom domain, so that we shouldn't imagine about things such as root,
    leaf, money, glass, GPU, refrigerator, carpet as it's very unlikely that those
    things have anything in common with the animal kingdom.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 根据类比示例，我们希望将想象力仅局限于动物王国领域，因此我们不应该去想诸如根、叶、金钱、玻璃、GPU、冰箱、地毯等事物，因为这些东西与动物王国几乎没有任何共同点。
- en: 'The `loss` function of the VAE is basically the negative log-likelihood with
    a regularizer as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: VAE的`loss`函数基本上是负对数似然函数，并带有一个正则项，如下所示：
- en: '![A real world analogy of VAE](img/B08086_05_20.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![VAE的现实世界类比](img/B08086_05_20.jpg)'
- en: The first term is the expected negative log-likelihood or reconstruction loss
    of the ith data point where the expectation is calculated with respect to the
    encoder's distribution over the representations. This term helps the decoder to
    reconstruct the data well and incur large costs if it fails to do so. The second
    term represents the Kullback-Leibler divergence between encoder distribution *q(z|x)*
    and *p(z)* and acts as a regularizer that adds a penalty to the loss when an encoder's
    output representation *z* differs from normal distribution.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项是第i个数据点的期望负对数似然或重建损失，其中期望是根据编码器对表示的分布进行计算的。这个项帮助解码器很好地重建数据，如果解码失败则会产生很大的损失。第二个项表示编码器分布
    *q(z|x)* 和 *p(z)* 之间的Kullback-Leibler散度，并充当正则项，当编码器的输出表示 *z* 偏离正态分布时会对损失加上惩罚。
- en: 'Now let''s dive deep into the code for generating handwritten digits from the
    MNIST dataset with VAE using TensorFlow. First let us create the encoder network
    *Q(z|X)* with a single hidden layer that will take *X* as input and output *μ(X)*
    and *Σ(X)* as part of Gaussian distribution:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入研究如何使用TensorFlow从MNIST数据集生成手写数字的代码。首先，我们创建一个具有单隐藏层的编码器网络 *Q(z|X)*，它将
    *X* 作为输入，并输出 *μ(X)* 和 *Σ(X)* 作为高斯分布的一部分：
- en: '[PRE37]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now we create the decoder network *P(X|z)*:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建解码器网络 *P(X|z)*：
- en: '[PRE38]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then we calculate the reconstruction loss and Kullback-Leibler divergence loss
    and sum them up to get the total loss of the VAE network:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算重构损失和 Kullback-Leibler 散度损失，并将它们相加得到 VAE 网络的总损失：
- en: '[PRE39]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'And then use an `AdamOptimizer` to minimize the loss:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用 `AdamOptimizer` 来最小化损失：
- en: '[PRE40]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Run the file (`VAE.py` or `VAE.ipynb`) to start VAE operations on an `MNIST`
    dataset and the images will be generated in the output folder. The sample handwritten
    digit images are generated after 10,000 iterations:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 运行文件（`VAE.py` 或 `VAE.ipynb`）以启动 VAE 操作，并使用 `MNIST` 数据集，图像将生成在输出文件夹中。样本手写数字图像在经过
    10,000 次迭代后生成：
- en: '![A real world analogy of VAE](img/B08086_05_21.png.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![VAE的现实世界类比](img/B08086_05_21.png.jpg)'
- en: A comparison of two generative models—GAN and VAE
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 两种生成模型——GAN 和 VAE 的比较
- en: Although both are very exciting approaches of the generative model and have
    helped researchers to make inroads into the unsupervised domain along with generating
    capability, these two models differ in how they are trained. GAN is rooted in
    game theory, with an objective to find the nash equilibrium between the discriminator
    network and generator network. Whereas VAE is basically a probabilistic graphical
    model rooted in Bayesian inference, whose goal is latent modeling, that is, it
    tries to model the probability distribution of underlying data, in order to sample
    new data from that distribution.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管生成模型的这两种方法都非常激动人心，并且帮助研究人员在无监督领域取得了进展，同时具备生成能力，但这两种模型在训练方式上有所不同。GAN 根植于博弈论，目标是找到判别网络和生成网络之间的纳什均衡。而
    VAE 基本上是一个基于贝叶斯推断的概率图模型，其目标是潜在建模，也就是说，它尝试对底层数据的概率分布进行建模，从而从该分布中采样生成新数据。
- en: VAE has a clear known way of evaluating the quality of the model (such as log-likelihood,
    either estimated by importance sampling or lower-bounded) compared to GAN, but
    the problem with VAE is that it uses direct mean squared error in the calculation
    of latent loss, instead of an adversarial network, so they over simplify the objective
    task as they are bound to work in a latent space and as a result they often generate
    blured images compared to GAN.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GAN 相比，VAE 有一种明确已知的评估模型质量的方式（例如对数似然，通常通过重要性采样或下界估算），但 VAE 的问题在于它在计算潜在损失时使用直接的均方误差，而不是使用对抗网络，因此它们过度简化了目标任务，因为它们只能在潜在空间中工作，结果生成的图像通常比
    GAN 更模糊。
- en: Summary
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Transfer Learning addresses the problem of dealing with small data effectively
    without re-inventing the training wheel from scratch. You have learned to extract
    and transfer features from a pre-trained model and apply it to your own problem
    domain. Also, you have mastered training and running deeper models over a large
    scale distributed system using Spark and its related components. Then, you have
    generated a realistic high resolution image leveraging the power of Transfer Learning
    within SRGAN. Also, you have mastered the concepts of other generative model approaches
    such as VAE and DeepDream for artistic image generation. In the last chapter,
    we will shift our focus from training deep models or generative models and learn
    various approaches of deploying your deep learning-based applications in production.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习解决了如何有效处理小数据的问题，而无需从头开始重新发明训练轮子。你已经学会了从预训练模型中提取并迁移特征，并将其应用于自己的问题领域。此外，你还掌握了在大规模分布式系统中使用
    Spark 及其相关组件训练和运行更深层的模型。然后，你利用迁移学习的力量，在 SRGAN 中生成了一个逼真的高分辨率图像。此外，你还掌握了其他生成模型方法的概念，如
    VAE 和 DeepDream，用于艺术图像生成。在最后一章中，我们将把焦点从训练深度模型或生成模型转移到学习在生产环境中部署基于深度学习的应用程序的各种方法。
