- en: Summary
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Train and Set up a Chatbot, Able to Discuss Like a Human
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练并设置一个能够像人类一样对话的聊天机器人
- en: 'This chapter will show you how to train an automatic chatbot that will be able
    to answer simple and generic questions, and how to create an endpoint over HTTP
    for providing the answers via an API. More specifically, we will show:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向你展示如何训练一个自动聊天机器人，使其能够回答简单且通用的问题，以及如何通过HTTP创建一个端点，通过API提供答案。更具体地，我们将展示：
- en: What's the corpus and how to preprocess the corpus
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是语料库以及如何预处理语料库
- en: How to train a chatbot and how to test it
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何训练一个聊天机器人以及如何测试它
- en: How to create an HTTP endpoint to expose the API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何创建一个HTTP端点来暴露API
- en: Introduction to the project
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目介绍
- en: 'Chatbots are becoming increasingly used as a way to provide assistance to users.
    Many companies, including banks, mobile/landline companies and large e-sellers
    now use chatbots for customer assistance and for helping users in pre-sales. The
    Q&A page is not enough anymore: each customer is nowadays expecting an answer
    to his very own question which maybe is not covered or only partially covered
    in the Q&A. Also, chatbots are a great tool for companies which don''t need to
    provide additional customer service capacity for trivial questions: they really
    look like a win-win situation!'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人正变得越来越普及，成为为用户提供帮助的一种方式。许多公司，包括银行、移动/固话公司以及大型电子商务公司，现在都在使用聊天机器人为客户提供帮助，并在售前阶段帮助用户。如今，单纯的Q&A页面已经不够了：每个客户现在都期待得到针对自己问题的答案，而这些问题可能在Q&A中没有覆盖或只是部分覆盖。此外，对于那些不需要为琐碎问题提供额外客户服务容量的公司来说，聊天机器人是一项很棒的工具：这真的是一种双赢的局面！
- en: Chatbots have become very popular tools ever since deep learning became popular.
    Thanks to deep learning, we're now able to train the bot to provide better and
    personalized questions, and, in the last implementation, to retain a per-user
    context.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自从深度学习流行以来，聊天机器人已经成为非常流行的工具。得益于深度学习，我们现在能够训练机器人提供更好的个性化问题，并且在最新的实现中，还能够保持每个用户的上下文。
- en: 'Cutting it short, there are mainly two types of chatbot: the first is a simple
    one, which tries to understand the topic, always providing the same answer for
    all questions about the same topic. For example, on a train website, the questions
    *Where can I find the timetable of the City_A to City_B service?* and *What''s
    the next train departing from City_A?* will likely get the same answer, that could
    read *Hi! The timetable on our network is available on this page: <link>*. Basically,
    behind the scene, this types of chatbots use classification algorithms to understand
    the topic (in the example, both questions are about the timetable topic). Given
    the topic, they always provide the same answer. Usually, they have a list of N
    topics and N answers; also, if the probability of the classified topic is low
    (the question is too vague, or it''s on a topic not included in the list), they
    usually ask the user to be more specific and repeat the question, eventually pointing
    out other ways to do the question (send an email or call the customer service
    number, for example).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，主要有两种类型的聊天机器人：第一种是简单的聊天机器人，它尝试理解话题，总是为所有关于同一话题的问题提供相同的答案。例如，在火车网站上，问题*“从City_A到City_B的时刻表在哪里？”*和*“从City_A出发的下一班火车是什么？”*可能会得到相同的答案，可能是*“你好！我们网络上的时刻表可以在这个页面找到：<link>”*。基本上，这种类型的聊天机器人通过分类算法来理解话题（在这个例子中，两个问题都是关于时刻表的话题）。在确定话题后，它们总是提供相同的答案。通常，它们有一个包含N个话题和N个答案的列表；此外，如果分类出来的话题概率较低（问题太模糊，或是涉及到不在列表中的话题），它们通常会要求用户更具体地说明并重复问题，最终可能会提供其他提问方式（例如发送电子邮件或拨打客服热线）。
- en: 'The second type of chatbots is more advanced, smarter, but also more complex.
    For those, the answers are built using an RNN, in the same way that machine translation
    is performed (see the previous chapter). Those chatbots are able to provide more
    personalized answers, and they may provide a more specific reply. In fact, they
    don''t just guess the topic, but with an RNN engine they''re able to understand
    more about the user''s questions and provide the best possible answer: in fact,
    it''s very unlikely you''ll get the same answers with two different questions
    using these types if chatbots.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种类型的聊天机器人更为先进、更智能，但也更复杂。对于这种类型的聊天机器人，答案是通过RNN（循环神经网络）构建的，方式类似于机器翻译的实现（见前一章）。这些聊天机器人能够提供更个性化的答案，并且它们可能提供更具体的回复。事实上，它们不仅仅是猜测话题，而是通过RNN引擎，能够更好地理解用户的问题并提供最佳的答案：实际上，使用这种类型的聊天机器人，两个不同问题得到相同答案的可能性非常小。
- en: In this chapter, we will try to build a chatbot of the second type using an
    RNN similarly to what we've done in the previous chapter with the machine translation
    system. Also, we will show how to put the chatbot behind an HTTP endpoint, in
    order to use the chatbot as a service from your website, or, more simply, from
    your command line.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将尝试使用RNN构建第二种类型的聊天机器人，类似于我们在上一章中使用机器翻译系统所做的。同时，我们将展示如何将聊天机器人放在HTTP端点后面，以便将聊天机器人作为服务从您的网站或更简单地从命令行中使用。
- en: The input corpus
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入语料库
- en: Unfortunately, we haven't found any consumer-oriented dataset that is open source
    and freely available on the Internet. Therefore, we will train the chatbot with
    a more generic dataset, not really focused on customer service. Specifically,
    we will use the Cornell Movie Dialogs Corpus, from the Cornell University. The
    corpus contains the collection of conversations extracted from raw movie scripts,
    therefore the chatbot will be able to give answer more to fictional questions
    than real ones. The Cornell corpus contains more than 200,000 conversational exchanges
    between 10+ thousands of movie characters, extracted from 617 movies.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们没有找到任何面向消费者的、开放源代码并且可以自由使用的网络数据集。因此，我们将使用一个更通用的数据集来训练聊天机器人，而不是专注于客户服务的聊天数据集。具体来说，我们将使用康奈尔电影对话语料库（Cornell
    Movie Dialogs Corpus），该语料库来自康奈尔大学。该语料库包含从原始电影剧本中提取的对话集，因此聊天机器人能够更多地回答虚构性问题而非现实性问题。康奈尔语料库包含来自617部电影中的10,000多个电影角色之间的200,000多个对话交换。
- en: 'The dataset is available here: [https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以在这里获取：[https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)。
- en: 'We would like to thank the authors for having released the corpus: that makes
    experimentation, reproducibility and knowledge sharing easier.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢作者发布了这个语料库：这使得实验、可重复性和知识共享变得更加容易。
- en: 'The dataset comes as a `.zip` archive file. After decompressing it, you''ll
    find several files in it:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集以`.zip`归档文件的形式提供。解压后，您将找到其中的几个文件：
- en: '`README.txt` contains the description of the dataset, the format of the corpora
    files, the details on the collection procedure and the author''s contact.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`README.txt`包含了数据集的描述、语料库文件的格式、收集过程的细节以及作者的联系方式。'
- en: '`Chameleons.pdf` is the original paper for which the corpus has been released.
    Although the goal of the paper is strictly not around chatbots, it studies the
    language used in dialogues, and it''s a good source of information to understanding
    more'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Chameleons.pdf`是发布该语料库的原始论文。虽然论文的主要目标并不直接围绕聊天机器人，但它研究了对话中使用的语言，是理解更多内容的好信息来源。'
- en: '`movie_conversations.txt` contains all the dialogues structure. For each conversation,
    it includes the ID of the two characters involved in the discussion, the ID of
    the movie and the list of sentences IDs (or utterances, to be more precise) in
    chronological order. For example, the first line of the file is:'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movie_conversations.txt`包含了所有的对话结构。对于每个对话，它包括参与讨论的两个人物ID、电影ID以及按时间顺序排列的句子ID（或者更准确地说是发言ID）列表。例如，文件的第一行是：'
- en: '*u0 +++$+++ u2 +++$+++ m0 +++$+++ [''L194'', ''L195'', ''L196'', ''L197'']*'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*u0 +++$+++ u2 +++$+++ m0 +++$+++ [''L194'', ''L195'', ''L196'', ''L197'']*'
- en: 'That means that user `u0` had a conversation with user `u2` in the movie `m0` and
    the conversation had 4 utterances: `''L194''`, `''L195''`, `''L196''` and `''L197''`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着用户`u0`在电影`m0`中与用户`u2`进行了对话，该对话包含了4个发言：`'L194'`、`'L195'`、`'L196'`和`'L197'`。
- en: '`movie_lines.txt` contains the actual text of each utterance ID and the person
    who produced it. For example, the utterance `L195` is listed here as:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movie_lines.txt`包含了每个发言ID的实际文本及其发言者。例如，发言`L195`在此列出为：'
- en: '*L195 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Well, I thought we''d start
    with pronunciation, if that''s okay with you.*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*L195 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Well, I thought we''d start
    with pronunciation, if that''s okay with you.*'
- en: So, the text of the utterance `L195` is *Well, I thought we'd start with pronunciation,
    if that's okay with you.* And it was pronounced by the character `u2` whose name
    is `CAMERON` in the movie `m0`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，发言`L195`的文本是*Well, I thought we'd start with pronunciation, if that's okay
    with you.*，并且是由电影`m0`中的角色`u2`（名为`CAMERON`）所发出的。
- en: '`movie_titles_metadata.txt` contains information about the movies, including
    the title, year, IMDB rating, the number of votes in IMDB and the genres. For
    example, the movie `m0` here is described as:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movie_titles_metadata.txt` 包含有关电影的信息，包括标题、年份、IMDB评分、IMDB的投票数和流派。例如，这里描述的电影
    `m0` 是这样的：'
- en: '*m0 +++$+++ 10 things i hate about you +++$+++ 1999 +++$+++ 6.90 +++$+++ 62847
    +++$+++ [''comedy'', ''romance'']*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*m0 +++$+++ 10 things i hate about you +++$+++ 1999 +++$+++ 6.90 +++$+++ 62847
    +++$+++ [''喜剧'', ''爱情'']*'
- en: So, the title of the movie whose ID is `m0` is *10 things i hate about you*,
    it's from 1999, it's a comedy with romance and it received almost 63 thousand
    votes on IMDB with an average score of 6.9 (over 10.0)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，电影ID为 `m0` 的电影标题是 *10 things i hate about you*，出自1999年，是一部喜剧爱情片，IMDB上获得了近63,000票，平均评分为6.9（满分10分）。
- en: '`movie_characters_metadata.txt` contains information about the movie characters,
    including the name the title of the movie where he/she appears, the gender (if
    known) and the position in the credits (if known). For example, the character
    “u2” appears in this file with this description:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`movie_characters_metadata.txt` 包含有关电影角色的信息，包括角色名、出现的电影标题、性别（如果已知）和在演职员表中的位置（如果已知）。例如，角色“u2”在这个文件中以此描述：'
- en: '*u2 +++$+++ CAMERON +++$+++ m0 +++$+++ 10 things i hate about you +++$+++ m
    +++$+++ 3*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*u2 +++$+++ CAMERON +++$+++ m0 +++$+++ 10 things i hate about you +++$+++ m
    +++$+++ 3*'
- en: The character `u2` is named *CAMERON*, it appears in the movie `m0` whose title
    is *10 things i hate about you*, his gender is male and he's the third person
    appearing in the credits.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 角色 `u2` 的名字是 *CAMERON*，出现在电影 `m0` 中，标题是 *10 things i hate about you*，他是男性，排名第三。
- en: '`raw_script_urls.txt` contains the source URL where the dialogues of each movie
    can be retrieved. For example, for the movie `m0` that''s it:'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`raw_script_urls.txt` 包含可以检索每部电影对话的源URL。例如，对于电影 `m0`，它是：'
- en: '*m0 +++$+++ 10 things i hate about you +++$+++ http://www.dailyscript.com/scripts/10Things.html*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*m0 +++$+++ 10 things i hate about you +++$+++ http://www.dailyscript.com/scripts/10Things.html*'
- en: 'As you will have noticed, most files use the token  *+++$+++*  to separate
    the fields. Beyond that, the format looks pretty straightforward to parse. Please
    take particular care while parsing the files: their format is not UTF-8 but *ISO-8859-1*.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您注意到的那样，大多数文件使用标记 *+++$+++* 分隔字段。除此之外，该格式看起来相当容易解析。请特别注意解析文件时的格式：它们不是 UTF-8，而是
    *ISO-8859-1*。
- en: Creating the training dataset
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建训练数据集
- en: 'Let''s now create the training set for the chatbot. We''d need all the conversations
    between the characters in the correct order: fortunately, the corpora contains
    more than what we actually need. For creating the dataset, we will start by downloading
    the zip archive, if it''s not already on disk. We''ll then decompress the archive
    in a temporary folder (if you''re using Windows, that should be `C:\Temp`), and
    we will read just the `movie_lines.txt` and the `movie_conversations.txt` files,
    the ones we really need to create a dataset of consecutive utterances.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们为聊天机器人创建训练集。我们需要所有角色之间按正确顺序的对话：幸运的是，语料库包含了我们实际需要的以上内容。为了创建数据集，我们将从下载zip存档开始（如果尚未在磁盘上）。然后，我们将在临时文件夹解压缩存档（如果您使用Windows，应该是
    `C:\Temp`），并且我们将仅读取 `movie_lines.txt` 和 `movie_conversations.txt` 文件，这些是我们真正需要创建连续话语数据集的文件。
- en: Let's now go step by step, creating multiple functions, one for each step, in
    the file `corpora_downloader.py`. The first function we need is to retrieve the
    file from the Internet, if not available on disk.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们一步一步地进行，创建多个函数，每个步骤一个函数，在文件 `corpora_downloader.py` 中。我们需要的第一个函数是，如果磁盘上没有可用，从互联网上检索文件。
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This function does exactly that: it checks whether the “`README.txt`” file
    is available locally; if not, it downloads the file (thanks for the urlretrieve
    function in the urllib.request module) and it decompresses the zip (using the
    zipfile module).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数正是这样做的：它检查本地是否有 “`README.txt`” 文件；如果没有，它将下载文件（感谢 `urllib.request` 模块中的 `urlretrieve`
    函数），然后解压缩zip（使用 `zipfile` 模块）。
- en: 'The next step is read the conversation file and extract the list of utterance
    IDS. As a reminder, its format is: *u0 +++$+++ u2 +++$+++ m0 +++$+++ [''L194'',
    ''L195'', ''L196'', ''L197'']*, therefore what we''re looking for is the fourth
    element of the list after we split it on the token  *+++$+++* . Also, we''d need
    to clean up the square brackets and the apostrophes to have a clean list of IDs.
    For doing that, we shall import the re module, and the function will look like
    this.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是读取对话文件并提取话语ID列表。提醒一下，它的格式是：*u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195',
    'L196', 'L197']*，因此我们需要关注的是通过用*+++$+++*分割后的列表中的第四个元素。此外，我们还需要清除方括号和撇号，以获得一个干净的ID列表。为此，我们将导入re模块，函数将如下所示。
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As previously said, remember to read the file with the right encoding, otherwise,
    you''ll get an error. The output of this function is a list of lists, each of
    them containing the sequence of utterance IDS in a conversation between characters.
    Next step is to read and parse the `movie_lines.txt` file, to extract the actual
    utterances texts. As a reminder, the file looks like this line:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，记得以正确的编码读取文件，否则会出现错误。此函数的输出是一个包含对话中角色话语ID序列的列表的列表。下一步是读取并解析`movie_lines.txt`文件，以提取实际的对话文本。提醒一下，文件的格式如下：
- en: '*L195 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Well, I thought we''d start
    with pronunciation, if that''s okay with you.*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*L195 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ 好吧，我想我们从发音开始，如果你没问题的话。*'
- en: Here, what we're looking for are the first and the last chunks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要关注的是第一个和最后一个块。
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The very last bit is about tokenization and alignment. We''d like to have a
    set whose observations have two sequential utterances. In this way, we will train
    the chatbot, given the first utterance, to provide the next one. Hopefully, this
    will lead to a smart chatbot, able to reply to multiple questions. Here''s the
    function:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后部分涉及到标记化和对齐。我们希望拥有一组观察结果，其中包含两个连续的话语。通过这种方式，我们可以训练聊天机器人，在给定第一个话语的情况下，生成下一个话语。希望这能促使聊天机器人变得智能，能够回答多个问题。以下是这个函数：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Its output is a generator containing a tuple of the two utterances (the one
    on the right follows temporally the one on the left). Also, utterances are tokenized
    on the space character.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 它的输出是一个生成器，包含两个话语的元组（右边的那个时间上紧跟在左边的后面）。此外，话语是在空格字符上进行标记化的。
- en: 'Finally, we can wrap up everything into a function, which downloads the file
    and unzip it (if not cached), parse the conversations and the lines, and format
    the dataset as a generator. As a default, we will store the files in the `/tmp` directory:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将所有内容封装到一个函数中，该函数下载文件并解压（如果未缓存），解析对话和行，并将数据集格式化为生成器。默认情况下，我们将文件存储在`/tmp`目录中：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: At this point, our training set looks very similar to the training set used
    in the translation project, in the previous chapter. Actually, it's not just similar,
    it's the same format with the same goal. We can, therefore, use some pieces of
    code we've developed in the previous chapter. For example, the `corpora_tools.py`
    file can be used here without any change (also, it requires the `data_utils.py`).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们的训练集与上一章翻译项目中使用的训练集非常相似。实际上，它不仅相似，它是相同的格式和相同的目标。因此，我们可以使用在上一章中开发的一些代码片段。例如，`corpora_tools.py`文件可以在这里直接使用而不需要任何更改（此外，它还依赖于`data_utils.py`）。
- en: Given that file, we can dig more into the corpora, with a script to check the
    chatbot input.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 给定该文件，我们可以进一步深入分析语料库，使用一个脚本检查聊天机器人的输入。
- en: 'To inspect the corpora, we can use the `corpora_tools.py` we made in the previous
    chapter, and the file we''ve previously created. Let''s retrieve the Cornell Movie
    Dialog Corpus, format the corpora and print an example and its length:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查语料库，我们可以使用在上一章中编写的`corpora_tools.py`，以及我们之前创建的文件。让我们获取Cornell电影对话语料库，格式化语料库并打印一个示例及其长度：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This code prints an example of two tokenized consecutive utterances, and the
    number of examples in the dataset, that is more than 220,000:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码打印了两个标记化的连续话语示例，以及数据集中示例的数量，超过了220,000个：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s now clean the punctuation in the sentences, lowercase them and limits
    their size to 20 words maximum (that is examples where at least one of the sentences
    is longer than 20 words are discarded). This is needed to standardize the tokens:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们清理句子中的标点符号，将其转为小写，并将其长度限制为最多20个单词（也就是那些至少有一个句子长度超过20个单词的示例会被丢弃）。这是为了标准化标记：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This leads us to almost 140,000 examples:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们得到近140,000个示例：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, let''s create the dictionaries for the two sets of sentences. Practically,
    they should look the same (since the same sentence appears once on the left side,
    and once in the right side) except there might be some changes introduced by the
    first and last sentences of a conversation (they appear only once). To make the
    best out of our corpora, let''s build two dictionaries of words and then encode
    all the words in the corpora with their dictionary indexes:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 'Then, let''s create the dictionaries for the two sets of sentences. Practically,
    they should look the same (since the same sentence appears once on the left side,
    and once in the right side) except there might be some changes introduced by the
    first and last sentences of a conversation (they appear only once). To make the
    best out of our corpora, let''s build two dictionaries of words and then encode
    all the words in the corpora with their dictionary indexes:'
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'That prints the following output. We also notice that a dictionary of 15 thousand
    entries doesn''t contain all the words and more than 16 thousand (less popular)
    of them don''t fit into it:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 'That prints the following output. We also notice that a dictionary of 15 thousand
    entries doesn''t contain all the words and more than 16 thousand (less popular)
    of them don''t fit into it:'
- en: '[PRE10]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As the final step, let''s add paddings and markings to the sentences:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'As the final step, let''s add paddings and markings to the sentences:'
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And that, as expected, prints:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'And that, as expected, prints:'
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Training the chatbot
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Training the chatbot
- en: 'After we''re done with the corpora, it''s now time to work on the model. This
    project requires again a sequence to sequence model, therefore we can use an RNN.
    Even more, we can reuse part of the code from the previous project: we''d just
    need to change how the dataset is built, and the parameters of the model. We can
    then copy the training script built in the previous chapter, and modify the `build_dataset`
    function, to use the Cornell dataset.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 'After we''re done with the corpora, it''s now time to work on the model. This
    project requires again a sequence to sequence model, therefore we can use an RNN.
    Even more, we can reuse part of the code from the previous project: we''d just
    need to change how the dataset is built, and the parameters of the model. We can
    then copy the training script built in the previous chapter, and modify the `build_dataset`
    function, to use the Cornell dataset.'
- en: Mind that the dataset used in this chapter is bigger than the one used in the
    previous, therefore you may need to limit the corpora to a few dozen thousand
    lines. On a 4 years old laptop with 8GB RAM, we had to select only the first 30
    thousand lines, otherwise, the program ran out of memory and kept swapping. As
    a side effect of having fewer examples, even the dictionaries are smaller, resulting
    in less than 10 thousands words each.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Mind that the dataset used in this chapter is bigger than the one used in the
    previous, therefore you may need to limit the corpora to a few dozen thousand
    lines. On a 4 years old laptop with 8GB RAM, we had to select only the first 30
    thousand lines, otherwise, the program ran out of memory and kept swapping. As
    a side effect of having fewer examples, even the dictionaries are smaller, resulting
    in less than 10 thousands words each.
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: By inserting this function into the `train_translator.py` file (from the previous
    chapter) and rename the file as `train_chatbot.py`, we can run the training of
    the chatbot.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: By inserting this function into the `train_translator.py` file (from the previous
    chapter) and rename the file as `train_chatbot.py`, we can run the training of
    the chatbot.
- en: 'After a few iterations, you can stop the program and you''ll see something
    similar to this output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 'After a few iterations, you can stop the program and you''ll see something
    similar to this output:'
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Again, if you change the settings, you may end up with a different perplexity.
    To obtain these results, we set the RNN size to 256 and 2 layers, the batch size
    of 128 samples, and the learning rate to 1.0.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Again, if you change the settings, you may end up with a different perplexity.
    To obtain these results, we set the RNN size to 256 and 2 layers, the batch size
    of 128 samples, and the learning rate to 1.0.
- en: At this point, the chatbot is ready to be tested. Although you can test the
    chatbot with the same code as in the `test_translator.py` of the previous chapter,
    here we would like to do a more elaborate solution, which allows exposing the
    chatbot as a service with APIs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: At this point, the chatbot is ready to be tested. Although you can test the
    chatbot with the same code as in the `test_translator.py` of the previous chapter,
    here we would like to do a more elaborate solution, which allows exposing the
    chatbot as a service with APIs.
- en: Chatbox API
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Chatbox API
- en: First of all, we need a web framework to expose the API. In this project, we've
    chosen Bottle, a lightweight simple framework very easy to use.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: First of all, we need a web framework to expose the API. In this project, we've
    chosen Bottle, a lightweight simple framework very easy to use.
- en: To install the package, run `pip install bottle` from the command line. To gather
    further information and dig into the code, take a look at the project webpage,
    [https://bottlepy.org](https://bottlepy.org).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: To install the package, run `pip install bottle` from the command line. To gather
    further information and dig into the code, take a look at the project webpage,
    [https://bottlepy.org](https://bottlepy.org).
- en: 'Let''s now create a function to parse an arbitrary sentence provided by the
    user as an argument. All the following code should live in the `test_chatbot_aas.py`
    file. Let''s start with some imports and the function to clean, tokenize and prepare
    the sentence using the dictionary:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个函数，用来解析用户作为参数提供的任意句子。所有接下来的代码应该都写在`test_chatbot_aas.py`文件中。我们从一些导入和使用字典来清理、分词并准备句子的函数开始：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The function `prepare_sentence` does the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`prepare_sentence`函数执行以下操作：'
- en: Tokenizes the input sentence
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对输入句子进行分词
- en: Cleans it (lowercase and punctuation cleanup)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理它（转换为小写并清理标点符号）
- en: Converts tokens to dictionary IDs
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将词元转换为字典ID
- en: Add markers and paddings to reach the default length
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加标记和填充以达到默认长度
- en: 'Next, we will need a function to convert the predicted sequence of numbers
    to an actual sentence composed of words. This is done by the function `decode`,
    which runs the prediction given the input sentence and with softmax predicts the
    most likely output. Finally, it returns the sentence without paddings and markers
    (a more exhaustive description of the function is provided in the previous chapter):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个函数，将预测的数字序列转换为由单词组成的实际句子。这是通过`decode`函数完成的，该函数根据输入句子运行预测，并使用softmax预测最可能的输出。最后，它返回没有填充和标记的句子（函数的更详细描述见上一章）：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, the main function, that is, the function to run in the script:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是主函数，也就是在脚本中运行的函数：
- en: '[PRE17]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Initially, it loads the dictionary and prepares the inverse dictionary. Then,
    it uses the Bottle API to create an HTTP GET endpoint (under the /api URL). The
    route decorator sets and enriches the function to run when the endpoint is contacted
    via HTTP GET. In this case, the `api()` function is run, which first reads the
    sentence passed as HTTP parameter, then calls the `prepare_sentence` function,
    described above, and finally runs the decoding step. What's returned is a dictionary
    containing both the input sentence provided by the user and the reply of the chatbot.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，它加载字典并准备反向字典。接着，它使用Bottle API创建一个HTTP GET端点（在/api URL下）。路由装饰器设置并增强了当通过HTTP
    GET访问该端点时运行的函数。在这种情况下，运行的是`api()`函数，它首先读取作为HTTP参数传递的句子，然后调用上述的`prepare_sentence`函数，最后执行解码步骤。返回的是一个字典，其中包含用户提供的输入句子和聊天机器人的回复。
- en: Finally, the webserver is turned on, on the localhost at port 8080\. Isn't very
    easy to have a chatbot as a service with Bottle?
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，网页服务器已启动，运行在localhost的8080端口上。使用Bottle实现聊天机器人作为服务是不是非常简单？
- en: 'It''s now time to run it and check the outputs. To run it, run from the command
    line:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候运行它并检查输出了。要运行它，请从命令行执行：
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Then, let's start querying the chatbot with some generic questions, to do so
    we can use CURL, a simple command line; also all the browsers are ok, just remember
    that the URL should be encoded, for example, the space character should be replaced
    with its encoding, that is, `%20`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，让我们开始用一些通用问题查询聊天机器人，为此我们可以使用CURL，这是一个简单的命令行工具；此外，所有浏览器都可以使用，只需记住URL应当编码，例如，空格字符应该用它的编码替代，即`%20`。
- en: 'Curl makes things easier, having a simple way to encode the URL request. Here
    are a couple of examples:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Curl让事情变得更容易，它提供了一种简单的方式来编码URL请求。以下是几个示例：
- en: '[PRE19]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If the system doesn''t work with your browser, try encoding the URL, for example:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统在你的浏览器中无法正常工作，请尝试对URL进行编码，例如：
- en: '`$> curl -X GET http://127.0.0.1:8080/api?sentence=how%20are%20you?` `{"data":
    [{"out": "that '' s okay .", "in": "how are you?"}]}`.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`$> curl -X GET http://127.0.0.1:8080/api?sentence=how%20are%20you?` `{"data":
    [{"out": "that '' s okay .", "in": "how are you?"}]}`'
- en: Replies are quite funny; always remember that we trained the chatbox on movies,
    therefore the type of replies follow that style.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 回复相当有趣；始终记得我们训练聊天机器人的数据集是电影，因此回复的风格跟电影有关。
- en: To turn off the webserver, use *Ctrl* + *C*.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要关闭网页服务器，请使用*Ctrl* + *C*。
- en: Home assignments
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 家庭作业
- en: 'Following are the home assignments:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是家庭作业：
- en: Can you create a simple webpage which queries the chatbot via JS?
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能创建一个简单的网页，通过JS查询聊天机器人吗？
- en: Many other training sets are available on the Internet; try to see the differences
    of answers between the models. Which one is the best for a customer service bot?
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 互联网上有许多其他训练集可供选择；尝试查看不同模型之间的回答差异。哪种最适合客户服务机器人？
- en: Can you modify the model, to be trained as a service, that is, by passing the
    sentences via HTTP GET/POST?
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能否修改模型，使其作为服务进行训练，即通过HTTP GET/POST传递句子？
- en: Summary
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we''ve implemented a chatbot, able to respond to questions
    through an HTTP endpoint and a GET API. It''s another great example of what we
    can do with RNN. In the next chapter, we''re moving to a different topic: how
    to create a recommender system using Tensorflow.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们实现了一个聊天机器人，能够通过 HTTP 端点和 GET API 回答问题。这是我们使用 RNN 能做的又一个精彩示例。在下一章，我们将转向另一个话题：如何使用
    Tensorflow 创建推荐系统。
