- en: '*Chapter 12*: Meta-Reinforcement Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第12章*：元强化学习'
- en: 'Humans learn new skills from much less data compared to a **reinforcement learning**
    (**RL**) agent. This is because first, we come with priors in our brains at birth;
    and second, we are able to transfer our knowledge from one skill to another quite
    efficiently. Meta-RL aims to achieve a similar capability. In this chapter, we
    will describe what meta-RL is, what approaches we use, and what the challenges
    are under the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 与**强化学习**（**RL**）智能体相比，人类从较少的数据中学习新技能。这是因为首先，我们出生时大脑中就带有先验知识；其次，我们能够高效地将一种技能的知识迁移到另一种技能上。元强化学习（Meta-RL）旨在实现类似的能力。在本章中，我们将描述什么是元强化学习，我们使用了哪些方法，以及在以下主题下面临的挑战：
- en: Introduction to meta-RL
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元强化学习简介
- en: Meta-RL with recurrent policies
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有递归策略的元强化学习
- en: Gradient-based meta-RL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于梯度的元强化学习
- en: Meta-RL as partially observed RL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元强化学习作为部分观测强化学习
- en: Challenges in meta-RL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元强化学习中的挑战
- en: Introduction to meta-RL
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元强化学习简介
- en: In this chapter, we will introduce meta-RL, which is actually a very intuitive
    concept but could be hard to wrap your mind around at first. To make things even
    clearer for you, we will also discuss the connection between meta-RL and other
    concepts we covered in the earlier chapters.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍元强化学习，这实际上是一个非常直观的概念，但一开始可能比较难以理解。为了让你更加清楚，我们还将讨论元强化学习与前几章中涉及的其他概念之间的联系。
- en: Learning to learn
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学会学习
- en: 'Let''s say you are trying to convince a friend to go on a trip that you really
    want to go on together. There are several arguments that come to your mind. You
    could talk about the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在说服一个朋友一起去你非常想去的旅行。你脑海中浮现出了几个论点。你可以谈论以下几点：
- en: The beauty of the nature at your destination.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你目的地自然风光的美丽。
- en: How you are so burned out and really need this time away.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已经精疲力尽，真的很需要这段时间休息。
- en: This could be the last chance for a trip together for a while because you will
    be busy at work.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可能是你们一起旅行的最后机会，因为你将会很忙于工作。
- en: Well, you've known your friend for years now and know how they love nature,
    so you recognize that the first argument will be the most enticing one, because
    they love nature! If it was your mom, perhaps you could use the second one because
    she cares about you a lot and wants to support you. In either of these situations,
    you know how to achieve what you want because you have a common past with these
    individuals.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，你已经认识你的朋友很多年了，知道他们有多喜欢大自然，所以你意识到第一个论点将是最具吸引力的，因为他们喜欢大自然！如果是你的妈妈，也许你可以用第二个论点，因为她非常关心你，并且希望支持你。在这些情况下，你知道如何达成你想要的目标，因为你和这些人有共同的过去。
- en: 'Have you ever left a store, say a car dealership, buying something more expensive
    than you had originally planned? How were you convinced? Perhaps the salesperson
    figured out how much you care about the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你曾经在一家商店离开时，比如去车行，买了比原计划更贵的东西吗？你是怎么被说服的？也许销售员猜出了你在乎以下几点：
- en: Your family, and convinced you to buy an SUV to make them more comfortable
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的家人，并说服你购买一辆让他们更舒适的SUV
- en: Your look, and convinced you to buy a sports car that will attract a lot of
    eyes
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的外表，并说服你购买一辆能吸引众人目光的跑车
- en: The environment, and convinced you to buy an electric vehicle with no emissions
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境，并说服你购买一辆没有排放的电动汽车
- en: The salesperson does not know you, but through years of experience and training,
    they know how to learn about you very quickly and effectively. They ask you questions,
    understand your background, discover your interests, and figure out your budget.
    Then, you are presented with some options, and based on what you like and don't
    like, you end up with an offer package with a make and model, upgrades and options,
    and a payment plan, all customized for you.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 销售员不了解你，但通过多年的经验和培训，他们知道如何迅速有效地了解你。他们问你问题，了解你的背景，发现你的兴趣，弄清楚你的预算。然后，他们会给你提供一些选项，根据你的喜好和不喜欢，最终会给你一个定制的报价套餐，包括品牌、型号、升级选项和支付计划。
- en: Here, the former of these examples correspond to RL, where an agent learns a
    good policy for a particular environment and a task to maximize its reward. The
    latter example corresponds to meta-RL, where an agent learns a good **procedure**
    to quickly adapt to a new environment and task to maximize its reward.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，前面的例子对应强化学习（RL），其中智能体为特定环境和任务学习一个好的策略，以最大化其奖励。后面的例子则对应元强化学习（Meta-RL），其中智能体学习一个好的**过程**，以快速适应新的环境和任务，从而最大化奖励。
- en: After this example, let's formally define meta-RL.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子之后，让我们正式定义元强化学习（meta-RL）。
- en: Defining meta-RL
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义元强化学习（meta-RL）。
- en: 'In meta-RL, in each episode, the agent faces a task, ![](img/Formula_12_001.png),
    that comes from a distribution, ![](img/Formula_12_002.png), ![](img/Formula_12_003.png).
    A task, ![](img/Formula_12_004.png), is a **Markov** **decision** **process**
    (**MDP**) with possibly different transition and reward dynamics, described as
    ![](img/Formula_12_005.png), where we have the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在元强化学习中，每一轮，智能体面临一个任务 ![](img/Formula_12_001.png)，该任务来自一个分布 ![](img/Formula_12_002.png)，
    ![](img/Formula_12_003.png)。任务 ![](img/Formula_12_004.png) 是一个**马尔可夫** **决策**
    **过程**（**MDP**），可能具有不同的转移和奖励动态，描述为 ![](img/Formula_12_005.png)，其中包括以下内容：
- en: '![](img/Formula_12_006.png) is the state space.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_12_006.png) 是状态空间。'
- en: '![](img/Formula_12_007.png) is the action space.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_12_007.png) 是动作空间。'
- en: '![](img/Formula_12_008.png) is the transition distribution for task ![](img/Formula_12_009.png).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_12_008.png) 是任务 ![](img/Formula_12_009.png) 的转移分布。'
- en: '![](img/Formula_12_010.png) is the reward function for task ![](img/Formula_12_011.png).'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/Formula_12_010.png) 是任务 ![](img/Formula_12_011.png) 的奖励函数。'
- en: 'So, during the training and test time, we expect the tasks to come from the
    same distribution, but we don''t expect them to be the same, which is the setup
    in a typical machine learning problem. In meta-RL, at the test time, we expect
    the agent to do the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在训练和测试期间，我们期望任务来自相同的分布，但我们并不期望它们完全相同，这正是典型机器学习问题中的设定。在元强化学习中，在测试时，我们期望智能体做以下事情：
- en: Effectively explore to understand the task.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有效地探索以理解任务。
- en: Adapt to the task.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 适应任务。
- en: Meta-learning is embedded in animal learning. Let's explore this connection
    next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习（meta-learning）是嵌入在动物学习中的。接下来让我们探索这种联系。
- en: Relation to animal learning and the Harlow experiment
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与动物学习及哈洛实验的关系
- en: 'Artificial neural networks notoriously require a lot of data to be trained.
    On the other hand, our brains learn much more efficiently from small data. There
    are two major factors contributing to this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络 notoriously 需要大量数据来训练。另一方面，我们的大脑能够从少量数据中更高效地学习。这主要有两个因素：
- en: Unlike an untrained artificial neural network, our brains are pre-trained and
    come with **priors** embedded in for vision, audio, and motor skill tasks. Some
    especially impressive examples of pre-trained beings are **precocial** animals,
    such as ducks, whose ducklings take to water within 2 hours of hatching.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与未经训练的人工神经网络不同，我们的大脑是预训练的，且内嵌了视觉、听觉和运动技能任务的**先验**。一些尤其令人印象深刻的预训练生物是**初生性**动物，例如鸭子，它们的小鸭子在孵化后的两小时内便能下水。
- en: 'When we learn new tasks, we learn at two time scales: in a **fast loop**, we
    learn about the specific task we are dealing with and, as we will see more examples,
    in a **slow loop**, we learn **abstractions** that help us generalize our knowledge
    to new examples very fast. Let''s say you learn about a particular cat breed,
    such as the American Curl, and all the examples you have seen are in white and
    yellow tones. When you see a black cat of this breed, it won''t be difficult for
    you to recognize it. That is because the abstraction you developed will help you
    recognize this breed from its peculiar ears that curl back toward the back of
    the skull, not from its color.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们学习新任务时，我们在两个时间尺度上学习：在**快速循环**中，我们学习关于当前任务的具体内容；而如我们将看到的更多例子，在**慢速循环**中，我们学习**抽象**，这有助于我们将知识快速泛化到新的例子中。假设你学习某一特定的猫品种，例如美式卷耳猫，且你见到的所有例子都呈现白色和黄色。当你看到一只黑色的这种品种猫时，你不会觉得难以辨认。这是因为你已经发展出一种抽象的知识，帮助你通过这种猫独特的耳朵（向后卷曲）识别它，而非通过它的颜色。
- en: One of the big challenges in machine learning is to enable learning from small
    data similar to previously. To mimic *step 1*, we fine-tune trained models for
    new tasks. For example, a language model that is trained on generic corpora (Wikipedia
    pages, news articles, and so on) can be fine-tuned on a specialized corpus (maritime
    law) where the amount of data available is limited. *Step 2* is what meta-learning
    is about.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的一个大挑战是使得能够从类似先前情况的少量数据中学习。为了模仿*步骤 1*，我们会对训练好的模型进行微调，适应新任务。例如，一个在通用语料库（如维基百科页面、新闻文章等）上训练的语言模型，可以在一个专业语料库（如海事法）上进行微调，尽管可用的数据量有限。*步骤
    2* 就是元学习的核心内容。
- en: Tip
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Empirically, fine-tuning trained models for new tasks doesn't work in RL as
    well as in image or language tasks. It turns out that the neural network representations
    of RL policies are not as hierarchical as, for example, in image recognition,
    where the first layers detect edges and the last layers detect complete objects.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 经验上，对于新任务进行微调训练的模型，在强化学习（RL）中并不像在图像或语言任务中那样有效。事实证明，强化学习策略的神经网络表示不像图像识别中的那样层次化，例如，在图像识别中，第一层检测边缘，最后一层检测完整的物体。
- en: 'To better understand meta-learning capabilities in animals, let''s take a look
    at a canonical example: the Harlow experiment.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解动物中的元学习能力，我们来看一个典型的例子：哈罗实验。
- en: The Harlow experiment
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 哈罗实验
- en: 'The Harlow experiment explored meta-learning in animals, and involved a monkey
    that was shown two objects at a time:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 哈罗实验探讨了动物中的元学习，涉及一只猴子，它一次被展示两个物体：
- en: One of these objects was associated with a food reward, which the monkey had
    to discover.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些物体中的一个与食物奖励相关联，猴子需要发现这一点。
- en: In each step (in a total of six), the objects were randomly placed in left and
    right positions.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一步（共六步）中，物体被随机放置在左侧或右侧位置。
- en: The monkey had to learn which object gave it a reward independent of its position.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猴子必须学会根据物体本身，而不是物体的位置，来判断哪个物体给它带来奖励。
- en: After the six steps were over, the objects were replaced with new ones that
    were unfamiliar to the monkey and with an unknown reward association.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在六个步骤结束后，物体被替换为猴子不熟悉的新物体，并且这些物体与未知的奖励关联。
- en: The monkey learned a strategy to randomly pick an object in the first step,
    understand which object gave the reward, and choose that object in the remaining
    steps regardless of the position of the object.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猴子在第一步中学会了随机挑选一个物体，理解哪个物体给它带来奖励，并在剩下的步骤中根据物体是否带来奖励而选择这个物体，而不考虑物体的位置。
- en: 'This experiment nicely expresses the meta-learning capabilities in animals
    as it involves the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验很好地表达了动物中的元学习能力，因为它涉及到以下内容：
- en: An unfamiliar environment/task for the agent
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于智能体来说，这是一个不熟悉的环境/任务
- en: The agent's effective adaptation to the unfamiliar environment/task through
    a strategy that involves necessary exploration, developing a task-specific policy
    on the fly (making a choice based on the object associated with the reward but
    not its position), and then exploitation
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体通过一种策略有效适应不熟悉的环境/任务，这种策略包括必要的探索，实时制定特定任务的策略（根据与奖励相关联的物体做出选择，而非其位置），然后是利用阶段。
- en: The goal in meta-RL is along the same lines, as we will see shortly. For now,
    let's continue to explore meta-RL's relation to some other concepts we have covered.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 元强化学习的目标与此相似，正如我们稍后会看到的那样。现在，让我们继续探索元强化学习与我们已经涵盖的其他概念的关系。
- en: Relation to partial observability and domain randomization
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与部分可观测性和领域随机化的关系
- en: One of the main goals in a meta-RL procedure is to uncover the underlying environment/task
    at the test time. By definition, this means the environment is partially observable,
    and meta-RL is a specific approach to deal with it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 元强化学习程序的主要目标之一是在测试时揭示潜在的环境/任务。根据定义，这意味着环境是部分可观测的，而元强化学习是一种专门应对这一问题的方法。
- en: 'In the previous chapter, [*Chapter 11*](B14160_11_Final_SK_ePub.xhtml#_idTextAnchor239),
    *Generalization and Partial Observability*, we discussed that we need memory and
    domain randomization to deal with partial observability. So, how is meta-RL different?
    Well, memory is still one of the key tools leveraged in meta-RL. We also randomize
    the environments/tasks while training meta-RL agents, similar to domain randomization.
    At this point, they may seem indistinguishable to you. However, there is a key
    difference:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，[*第11章*](B14160_11_Final_SK_ePub.xhtml#_idTextAnchor239)，*泛化与部分可观测性*中，我们讨论了处理部分可观测性时需要使用记忆和领域随机化。那么，元强化学习（meta-RL）有什么不同呢？嗯，记忆依然是元强化学习中一个关键的工具。我们还在训练元强化学习智能体时对环境/任务进行随机化，这类似于领域随机化。此时，它们可能对你来说似乎没有区别。然而，存在一个关键的区别：
- en: In domain randomization, the goal of training the agent is to develop a robust
    policy for all variations of an environment over a set of parameter ranges. For
    example, a robot could be trained under a range of friction and torque values.
    At test time, based on a sequence of observations that carry information and torque,
    the agent takes actions using the trained policy.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在领域随机化中，训练智能体的目标是为环境的所有变化开发一个健壮的策略，涵盖一系列参数范围。例如，一个机器人可以在一系列摩擦力和扭矩值下进行训练。在测试时，基于一系列携带信息和扭矩的观察，智能体使用训练好的策略采取行动。
- en: In meta-RL, the goal of training the agent is to develop an adaptation procedure
    for new environments/tasks, which will potentially lead to different policies
    at the test time after an exploration period.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在元强化学习中，训练智能体的目标是为新的环境/任务开发一个适应程序，这可能会导致在探索阶段后测试时使用不同的策略。
- en: 'The difference can still be subtle when it comes to memory-based meta-RL methods,
    and the training procedure may be identical in some cases. To better understand
    the difference, remember the Harlow experiment: the idea of domain randomization
    does not suit the experiment, since the objects shown to the agent are completely
    new in every episode. Therefore, the agent does not learn how to act over a range
    of objects in meta-RL. Instead, it learns how to discover the task and act accordingly
    when it is shown completely new objects.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于记忆的元强化学习方法中，差异仍然可能很微妙，并且在某些情况下，训练过程可能是相同的。为了更好地理解这种差异，请记住哈洛实验：领域随机化的想法不适用于该实验，因为每一集展示给智能体的对象在每次都是全新的。因此，智能体在元强化学习中并不是学习如何在一系列对象上采取行动。而是它学习如何发现任务，并在展示完全新对象时相应地采取行动。
- en: With that, now it is finally time to discuss several meta-RL approaches.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，终于是时候讨论几种元强化学习方法了。
- en: Info
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: A pioneer in meta-learning is Professor Chelsea Finn of Stanford University,
    who worked with Professor Sergey Levine of UC Berkeley as her PhD advisor. Profesor
    Finn has an entire course on meta-learning, available at [https://cs330.stanford.edu/](https://cs330.stanford.edu/).
    In this chapter, we mostly follow the terminology and classification of meta-RL
    approaches used by Professor Finn and Professor Levine.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习的先驱之一是斯坦福大学的切尔西·芬教授，她在博士阶段曾与加州大学伯克利分校的谢尔盖·莱文教授合作。芬教授开设了一门关于元学习的课程，课程链接在[https://cs330.stanford.edu/](https://cs330.stanford.edu/)。在本章中，我们主要遵循芬教授和莱文教授使用的元强化学习方法的术语和分类。
- en: Next, let's start with meta-RL with recurrent policies.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们从使用递归策略的元强化学习开始。
- en: Meta-RL with recurrent policies
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用递归策略的元强化学习
- en: In this section, we will cover one of the more intuitive approaches in meta-RL
    that uses **recurrent** **neural** **networks** (**RNNs**) to keep a memory, also
    known as the RL­2 algorithm. Let's start with an example to understand this approach.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍元强化学习中一种更直观的方法，该方法使用**递归** **神经** **网络**（**RNNs**）来保持记忆，也被称为RL-2算法。让我们从一个示例开始，理解这种方法。
- en: Grid world example
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网格世界示例
- en: 'Consider a grid world where the agent''s task is to reach a goal state, G,
    from a start state, S. These states are randomly placed for different tasks, so
    the agent has to learn exploring the world to discover where the goal state is,
    and is then given a big reward. When the same task is repeated, the agent is expected
    to quickly reach the goal state, which is to adapt to the environment, since there
    is a penalty incurred for each time step. This is illustrated in *Figure 12.1*:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个网格世界，智能体的任务是从起始状态S到达目标状态G。这些状态在不同任务中是随机放置的，因此智能体必须学会探索世界，以发现目标状态在哪里，并在此后获得丰厚奖励。当同一任务被重复时，期望智能体能够快速到达目标状态，这也就是适应环境，因为每经过一步就会遭受惩罚。这个过程在*图
    12.1*中有所示意：
- en: '![Figure 12.1 – Grid world example for meta-RL. (a) a task, (b) the agent''s
    exploration of the task, (c) the agent''s exploitation of what it learned'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.1 – 元强化学习的网格世界示例。（a）任务，（b）智能体对任务的探索，（c）智能体利用其所学的知识'
- en: '](img/B14160_12_1.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_12_1.jpg)'
- en: Figure 12.1 – Grid world example for meta-RL. (a) a task, (b) the agent's exploration
    of the task, (c) the agent's exploitation of what it learned
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – 元强化学习的网格世界示例。（a）任务，（b）智能体对任务的探索，（c）智能体利用其所学的知识
- en: 'In order to excel at the task, the agent must do the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在任务中表现出色，智能体必须执行以下操作：
- en: Explore the environment (at test time).
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索环境（在测试时）。
- en: Remember and exploit what it learned earlier.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记住并利用其先前学到的知识。
- en: 'Now, since we would like the agent to remember its previous experiences, we
    need to introduce a memory mechanism, implying using an RNN to represent the policy.
    There are several points we need to pay attention to:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，由于我们希望智能体记住其先前的经验，我们需要引入一个记忆机制，意味着使用递归神经网络（RNN）来表示策略。有几个要点需要注意：
- en: Just remembering the past observations is not enough as the goal state changes
    from task to task. The agent also needs to remember the past actions and rewards
    so that it can associate which actions in which states led to which reward to
    be able to uncover the task.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅仅记住过去的观察值是不够的，因为目标状态在不同任务之间是变化的。智能体还需要记住过去的动作和奖励，以便能够关联在特定状态下采取的哪些动作导致了哪些奖励，从而揭示任务的规律。
- en: Just remembering the history within the current episode is not enough. Notice
    that once the agent reaches the goal state, the episode ends. If we don't carry
    the memory over to the next episode, the agent has no way of benefiting from the
    experience gained in the previous episode. Again, note that there is no training
    or updates to the weights of the policy network taking place here. This is all
    happening at the test time, in an unseen task.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅仅记住当前回合中的历史是不够的。请注意，一旦智能体达到目标状态，回合就结束了。如果我们不将记忆传递到下一个回合，智能体将无法从前一个回合中获得的经验中受益。再者，请注意，在这里没有进行任何训练或更新策略网络的权重。这一切都发生在测试时，在一个未知的任务中。
- en: 'Handling the former is easy: we just need to feed actions and rewards along
    with the observations to the RNN. To handle the former, we make sure that we *do
    not reset the recurrent state between episodes* unless the task changes so that
    the memory is not discontinued.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 处理前者很简单：我们只需要将动作和奖励与观察值一起喂给 RNN。为了处理后者，我们确保*除非任务发生变化，否则在回合之间不会重置循环状态*，以确保记忆不会中断。
- en: 'Now, during training, why would the agent learn a policy that explicitly starts
    a new task with an exploration phase? That is because the exploration phase helps
    the agent discover the task and collect higher rewards later. If we reward the
    agent during training based on individual episodes, the agent would not learn
    this behavior. That is because exploration has some immediate costs, which are
    recouped only in future episodes and when that memory is carried across episodes
    for the same task. To this end, we form **meta-episodes**, or **trials**, which
    are ![](img/Formula_12_012.png) episodes of the same task that are concatenated
    together. Again, the recurrent state is not reset within each episode, and the
    reward is calculated over the meta-episode. This is illustrated in *Figure 12.2*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在训练过程中，为什么智能体会学习一个明确地以探索阶段开始新任务的策略呢？那是因为探索阶段帮助智能体发现任务，并在后续收获更高的奖励。如果我们在训练过程中仅基于单个回合奖励智能体，智能体将不会学到这种行为。因为探索是有即时成本的，这些成本只有在后续回合中才能回收，且当相同任务的记忆跨回合传递时才会得到补偿。为此，我们形成了**元回合**或**试验**，即！[](img/Formula_12_012.png)是将多个同一任务的回合串联起来的过程。再次强调，在每个回合内，循环状态不会被重置，奖励是根据元回合来计算的。如*图
    12.2*所示：
- en: '![](img/B14160_12_2.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14160_12_2.jpg)'
- en: 'Figure 12.2 – Procedure of agent-environment interaction (source: Duan et al.,
    2017)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 – 智能体与环境交互的过程（来源：Duan et al., 2017）
- en: Next, let's see how this can be implemented in RLlib.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何在 RLlib 中实现这一点。
- en: RLlib implementation
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RLlib 实现
- en: 'Concerning what we mentioned previously, meta-episodes can be formed by modifying
    the environment, so that is not quite related to RLlib. For the rest, we modify
    the model dictionary inside the agent config:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们之前提到的，元回合可以通过修改环境来形成，因此与 RLlib 并没有直接关系。至于其他部分，我们在智能体配置中的模型字典里进行修改：
- en: 'First, we enable the **long short-term memory** (**LSTM**) model:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们启用**长短期记忆**（**LSTM**）模型：
- en: '[PRE0]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We pass actions and rewards in addition to observations to the LSTM:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将动作和奖励与观察值一起传递给 LSTM：
- en: '[PRE1]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We make sure that the LSTM input sequence is long enough to cover the multiple
    episodes within a meta-episode. The default sequence length is `20`:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们确保 LSTM 输入序列足够长，能够覆盖一个元任务中的多个回合。默认的序列长度为 `20`：
- en: '[PRE2]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That is all you need! You can train your meta-RL agents with a few lines of
    code change!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这些！你只需要通过几行代码更改，就可以训练你的元强化学习智能体！
- en: Info
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: This procedure may not always converge, or when it does, it may converge to
    bad policies. Try training multiple times (with different seeds) and with different
    hyperparameter settings, and this may help you obtain a good policy, but this
    is not guaranteed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程可能并不总是收敛，或者即使收敛，也可能收敛到不好的策略。可以尝试多次训练（使用不同的种子）并调整超参数设置，这可能会帮助你获得一个好的策略，但这并不能保证成功。
- en: With this, we can proceed on to gradient-based approaches.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以继续使用基于梯度的方法。
- en: Gradient-based meta-RL
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于梯度的元强化学习
- en: Gradient-based meta-RL methods propose improving the policy by continuing the
    training at test time so that the policy adapts to the environment it is applied
    in. The key is that the policy parameters right before adaptation, ![](img/Formula_06_036.png),
    are set in such a way that the adaptation takes place in just a few shots.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的元强化学习方法提出，通过在测试时继续训练来改进策略，使策略能够适应所应用的环境。关键在于，策略参数在适应之前的状态，![](img/Formula_06_036.png)，应该设置成一种能够在少数几步内完成适应的方式。
- en: Tip
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Gradient-based meta-RL is based on the idea that some initializations of policy
    parameters enable learning from very little data during adaptation. The meta-training
    procedure aims to find this initialization.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的元强化学习（meta-RL）基于一个思想：某些策略参数的初始化可以使得在适应过程中，从非常少的数据中进行学习。元训练过程旨在找到这种初始化。
- en: A specific approach in this branch is called **model-agnostic meta-learning**
    (**MAML**), which is a general meta-learning method that can also be applied to
    RL. MAML trains the agent for a variety of tasks to figure out a good ![](img/Formula_12_014.png)
    value that facilitates adaptation and learning from a few shots.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分支中的一个特定方法叫做**无模型元学习**（**MAML**），它是一种通用的元学习方法，也可以应用于强化学习。MAML 训练智能体完成各种任务，以找到一个有助于适应和从少量样本中学习的良好
    ![](img/Formula_12_014.png) 值。
- en: Let's see how you can use RLlib for this.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何使用 RLlib 来实现这一点。
- en: RLlib implementation
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RLlib 实现
- en: 'MAML is one of the agents implemented in RLlib and can be easily used with
    Ray''s Tune:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: MAML 是 RLlib 中实现的一个智能体，并且可以很容易地与 Ray 的 Tune 一起使用：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Using MAML requires implementing a few additional methods within the environment.
    These are, namely, the `sample_tasks`, `set_task`, and `get_task` methods, which
    help with training over various tasks. An example implementation could be in a
    pendulum environment, which is implemented in RLlib as follows ([https://github.com/ray-project/ray/blob/releases/1.0.0/rllib/examples/env/pendulum_mass.py](https://github.com/ray-project/ray/blob/releases/1.0.0/rllib/examples/env/pendulum_mass.py)):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 MAML 需要在环境中实现一些额外的方法。这些方法分别是 `sample_tasks`、`set_task` 和 `get_task`，它们有助于在各种任务之间进行训练。一个示例实现可以是一个摆环境，RLlib
    中的实现如下 ([https://github.com/ray-project/ray/blob/releases/1.0.0/rllib/examples/env/pendulum_mass.py](https://github.com/ray-project/ray/blob/releases/1.0.0/rllib/examples/env/pendulum_mass.py))：
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'While training MAML, RLlib measures the agent''s performance before any adaptation
    to the environment it is in via `episode_reward_mean`. The performance after *N*
    gradient steps of adaptation is shown in `episode_reward_mean_adapt_N`. The number
    of these inner adaptation steps is a config of the agent that can be modified:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练 MAML 时，RLlib 会通过 `episode_reward_mean` 测量智能体在任何适应之前的环境表现。经过 *N* 次梯度适应步骤后的表现会显示在
    `episode_reward_mean_adapt_N` 中。这些内部适应步骤的次数是智能体的一个配置项，可以修改：
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'During training, you can see these metrics displayed on TensorBoard:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，您可以在 TensorBoard 上看到这些指标：
- en: '![Figure 12.3 – TensorBoard statistics'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.3 – TensorBoard 统计数据'
- en: '](img/B14160_12_3.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_12_3.jpg)'
- en: Figure 12.3 – TensorBoard statistics
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3 – TensorBoard 统计数据
- en: That's it! Now, let's introduce the last approach of this chapter.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在，让我们介绍本章的最后一种方法。
- en: Meta-RL as partially observed RL
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元强化学习作为部分观测强化学习
- en: 'Another approach in meta-RL is to focus on the partially observable nature
    of the tasks and explicitly estimate the state from the observations until that
    point in time:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 元强化学习中的另一种方法是专注于任务的部分可观测特性，并明确地从直到此时为止的观测中估计状态：
- en: '![](img/Formula_12_015.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_12_015.jpg)'
- en: 'Then, form a probability distribution over possible tasks based on the likelihood
    of them being active in that episode, or more precisely, some vector that contains
    the task information:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，基于任务在该回合中处于活动状态的可能性，或者更准确地说，基于包含任务信息的向量，形成一个可能任务的概率分布：
- en: '![](img/Formula_12_016.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_12_016.jpg)'
- en: 'Then, iteratively sample a task vector from this probability distribution and
    pass that to the policy, in addition to the state:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从这个概率分布中迭代地抽取任务向量，并将其与状态一起传递给策略：
- en: Sample ![](img/Formula_12_017.png).
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抽样 ![](img/Formula_12_017.png)。
- en: Take actions from a policy that receives the state and task vector as input,
    ![](img/Formula_12_018.png).
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从接收状态和任务向量作为输入的策略中采取行动，![](img/Formula_12_018.png)。
- en: With that, we conclude our discussion on the three main meta-RL methods. Before
    we wrap up the chapter, let's discuss some of the challenges in meta-RL.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些内容，我们就结束了对三种主要元强化学习方法的讨论。在总结本章之前，让我们讨论一些元强化学习中的挑战。
- en: Challenges in meta-RL
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 元强化学习中的挑战
- en: 'The main challenges regarding meta-RL, following *Rakelly*, *2019*, are as
    follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关于元强化学习的主要挑战，参见 *Rakelly*，*2019*，如下所示：
- en: Meta-RL requires a meta-training step over various tasks, which are usually
    hand-crafted. A challenge is to create an automated procedure to generate these
    tasks.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元强化学习需要在多个任务上进行元训练，这些任务通常是手工设计的。一个挑战是创建一个自动化过程来生成这些任务。
- en: The exploration phase that is supposed to be learned during meta-training is
    in practice not efficiently learned.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在元训练过程中，应该学习的探索阶段实际上并未有效学习。
- en: Meta-training involves sampling from an independent and identical distribution
    of tasks, which is not a realistic assumption. So, one goal is to make meta-RL
    more "online" by making it learn from a stream of tasks.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元训练涉及从独立同分布的任务中进行采样，而这一假设并不现实。因此，一个目标是通过让元强化学习从任务流中学习，使其变得更加“在线”。
- en: Well, congratulations on coming this far! We have just covered meta-RL, a concept
    that could be hard to absorb. Hopefully, this introduction gives you the courage
    to dive into the literature on this topic and further explore it for yourself.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你走到这一步！我们刚刚讨论了元强化学习，这是一个可能很难吸收的概念。希望这次介绍能给你勇气深入阅读相关文献，并进一步探索这个话题。
- en: Summary
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we covered meta-RL, one of the most important research directions
    in RL as its promise is to train agents that can adapt to new environments very
    quickly. To this end, we covered three methods: recurrent policies, gradient-based,
    and partial observability-based. Currently, meta-RL is in its infancy and is not
    performing as well as the more established approaches, so we covered the challenges
    in this area.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讨论了元强化学习（meta-RL），它是强化学习领域最重要的研究方向之一，因为它的潜力是训练能够快速适应新环境的智能体。为此，我们介绍了三种方法：递归策略、基于梯度的方法和基于部分可观察性的策略。目前，元强化学习还处于起步阶段，其表现尚不如更成熟的方法，因此我们讨论了该领域面临的挑战。
- en: In the next chapter, we will cover several advanced topics in a single chapter.
    So, stay tuned to further deepen your RL expertise.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将涵盖多个高级主题，并将其集中在一章中。所以，请继续关注，以进一步加深你的强化学习（RL）专业知识。
- en: References
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Prefrontal cortex as a meta-reinforcement learning system*, Wang, JX., Kurth-Nelson,
    Z., et al: [https://www.nature.com/articles/s41593-018-0147-8](https://www.nature.com/articles/s41593-018-0147-8)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*前额叶皮层作为元强化学习系统*，Wang, JX., Kurth-Nelson, Z., 等人： [https://www.nature.com/articles/s41593-018-0147-8](https://www.nature.com/articles/s41593-018-0147-8)'
- en: '*Meta-Reinforcement Learning*: [https://blog.floydhub.com/meta-rl/](https://blog.floydhub.com/meta-rl/)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*元强化学习*： [https://blog.floydhub.com/meta-rl/](https://blog.floydhub.com/meta-rl/)'
- en: '*Prefrontal cortex as a meta-reinforcement learning system*, blog: [https://deepmind.com/blog/article/prefrontal-cortex-meta-reinforcement-learning-system](https://deepmind.com/blog/article/prefrontal-cortex-meta-reinforcement-learning-system)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*前额叶皮层作为元强化学习系统*，博客： [https://deepmind.com/blog/article/prefrontal-cortex-meta-reinforcement-learning-system](https://deepmind.com/blog/article/prefrontal-cortex-meta-reinforcement-learning-system)'
- en: '*RL2, Fast Reinforcement Learning via Slow Reinforcement Learning*, Duan, Y.,
    Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., & Abbeel P.: [https://arxiv.org/abs/1611.02779](https://arxiv.org/abs/1611.02779)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*RL2，快速强化学习通过慢速强化学习*，Duan, Y., Schulman, J., Chen, X., Bartlett, P. L., Sutskever,
    I., & Abbeel P.： [https://arxiv.org/abs/1611.02779](https://arxiv.org/abs/1611.02779)'
- en: '*Learning to reinforcement learn*, Wang, JX., Kurth-Nelson, Z., Tirumala, D.,
    Soyer, H., Leibo, J. Z., Munos, R., Blundell, C., Kumaran, D., & Botvinick, M.:
    [https://arxiv.org/abs/1611.05763](https://arxiv.org/abs/1611.05763)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习强化学习*，Wang, JX., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J. Z.,
    Munos, R., Blundell, C., Kumaran, D., & Botvinick, M.： [https://arxiv.org/abs/1611.05763](https://arxiv.org/abs/1611.05763)'
- en: '*Open-sourcing Psychlab*: [https://deepmind.com/blog/article/open-sourcing-psychlab](https://deepmind.com/blog/article/open-sourcing-psychlab)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*开源Psychlab*： [https://deepmind.com/blog/article/open-sourcing-psychlab](https://deepmind.com/blog/article/open-sourcing-psychlab)'
- en: '*Meta-Reinforcement Learning of Structured Exploration Strategies*: [https://papers.nips.cc/paper/2018/file/4de754248c196c85ee4fbdcee89179bd-Paper.pdf](https://papers.nips.cc/paper/2018/file/4de754248c196c85ee4fbdcee89179bd-Paper.pdf)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*结构化探索策略的元强化学习*： [https://papers.nips.cc/paper/2018/file/4de754248c196c85ee4fbdcee89179bd-Paper.pdf](https://papers.nips.cc/paper/2018/file/4de754248c196c85ee4fbdcee89179bd-Paper.pdf)'
- en: '*Precociality*: [https://en.wikipedia.org/wiki/Precociality](https://en.wikipedia.org/wiki/Precociality)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*早熟性*： [https://en.wikipedia.org/wiki/Precociality](https://en.wikipedia.org/wiki/Precociality)'
- en: '*Meta-Learning: from Few-Shot Learning to Rapid Reinforcement Learning*: [https://icml.cc/media/Slides/icml/2019/halla(10-09-15)-10-13-00-4340-meta-learning.pdf](https://icml.cc/media/Slides/icml/2019/halla(10-09-15)-10-13-00-4340-meta-learning.pdf)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*元学习：从少样本学习到快速强化学习*：[https://icml.cc/media/Slides/icml/2019/halla(10-09-15)-10-13-00-4340-meta-learning.pdf](https://icml.cc/media/Slides/icml/2019/halla(10-09-15)-10-13-00-4340-meta-learning.pdf)'
- en: 'Workshop on Meta-Learning (MetaLearn 2020): [https://meta-learn.github.io/2020/](https://meta-learn.github.io/2020/)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元学习研讨会（MetaLearn 2020）：[https://meta-learn.github.io/2020/](https://meta-learn.github.io/2020/)
