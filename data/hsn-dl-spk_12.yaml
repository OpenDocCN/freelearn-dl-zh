- en: Textual Analysis and Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分析与深度学习
- en: 'In the previous chapter, we became familiar with the core concepts of **Natural
    Language Processing** (**NLP**) and then we saw some implementation examples in
    Scala with Apache Spark, and two open source libraries for this framework. We
    also understood the pros and cons of those solutions. This chapter walks through
    hands-on examples of NLP use case implementations using DL (Scala and Spark).
    The following four cases will be covered:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了**自然语言处理**（**NLP**）的核心概念，然后我们通过Scala和Apache Spark中的一些实现示例，学习了两个开源库的应用，并了解了这些解决方案的优缺点。本章将通过实际案例展示使用DL进行NLP的实现（使用Scala和Spark）。以下四个案例将被覆盖：
- en: DL4J
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J
- en: TensorFlow
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: Keras and TensorFlow backend
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras与TensorFlow后端
- en: DL4J and Keras model import
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J和Keras模型导入
- en: The chapter covers some considerations regarding the pros and cons for each
    of those DL approaches in order, so that readers should then be ready to understand
    in which cases one framework is preferred over the others.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了关于每种DL方法的优缺点的考虑，以便读者可以了解在何种情况下某个框架比其他框架更受青睐。
- en: Hands-on NLP with DL4J
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动手实践 NLP 与 DL4J
- en: The first example we are going to examine is a sentiment analysis case for movie
    reviews, the same as for the last example shown in the previous chapter (the *Hands-on
    NLP with Spark-NLP *section). The difference is that here, we are going to combine
    Word2Vec ([https://en.wikipedia.org/wiki/Word2vec](https://en.wikipedia.org/wiki/Word2vec))
    and an RNN model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要检查的第一个示例是电影评论的情感分析案例，与上一章中展示的最后一个示例（*动手实践 NLP with Spark-NLP*部分）相同。不同之处在于，这里我们将结合Word2Vec（[https://en.wikipedia.org/wiki/Word2vec](https://en.wikipedia.org/wiki/Word2vec)）和RNN模型。
- en: Word2Vec can be seen as a neural network with two layers only, which expects
    as input some text content and then returns vectors. It isn't a deep neural network,
    but it is used to turn text into a numerical format that deep neural networks
    can understand. Word2Vec is useful because it can group the vectors of similar
    words together in a vector space. It does this mathematically. It creates, without
    human intervention, distributed numerical representations of word features. The
    vectors that represent words are called **neural word embeddings***. W*ord2vec
    trains words against others that neighbor them in the input text. The way it does
    it is using context to predict a target word (**Continuous Bag Of Words** (**CBOW**))
    or using a word to predict a target context (skip-gram). It has been demonstrated
    that the second approach produces more accurate results when dealing with large
    datasets. If the feature vector assigned to a word can't be used to accurately
    predict its context, an adjustment happens to the vector components. Each word's
    context in the input text becomes the teacher by sending errors back. This way
    the word vectors that have been estimated similar by the context where they are,
    are moved closer together.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec可以看作是一个只有两层的神经网络，它接受一些文本内容作为输入，然后返回向量。它不是一个深度神经网络，但它用于将文本转换为深度神经网络能够理解的数字格式。Word2Vec非常有用，因为它可以在向量空间中将相似词汇的向量聚集在一起。它通过数学方式实现这一点。它在没有人工干预的情况下，创建了分布式的单词特征的数值表示。表示单词的向量被称为**神经词嵌入**。Word2Vec训练词汇与输入文本中的邻近词汇之间的关系。它通过上下文来预测目标词汇（**连续词袋模型**（**CBOW**））或使用一个词汇来预测目标上下文（跳字模型）。研究表明，当处理大型数据集时，第二种方法能够产生更精确的结果。如果分配给某个单词的特征向量不能准确预测它的上下文，那么该向量的组成部分就会发生调整。每个单词在输入文本中的上下文变成了“教师”，通过反馈错误进行调整。这样，通过上下文被认为相似的单词向量就会被推得更近。
- en: The dataset used for training and testing is the *Large Movie Review Dataset,*
    which is available for download at [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)
    and is free to use. It contains 25,000 highly popular movie reviews for training
    and another 25,000 for testing.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练和测试的数据集是*大型电影评论数据集*，可以在[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)下载，且免费使用。该数据集包含25,000条热门电影评论用于训练，另外还有25,000条用于测试。
- en: The dependencies for this example are DL4J NN, DL4J NLP, and ND4J.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的依赖项包括DL4J NN、DL4J NLP和ND4J。
- en: 'Set up the RNN configuration using, as usual, the DL4J `NeuralNetConfiguration.Builder`
    class, as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，使用DL4J的`NeuralNetConfiguration.Builder`类来设置RNN配置，如下所示：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This network is made by a Graves LSTM RNN (please go back to [Chapter 6](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Recurrent
    Neural Networks*, for more details on it) plus the DL4J—specific RNN output layer
    `RnnOutputLayer`. The activation function for this output layer is SoftMax.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络由一个Graves LSTM RNN构成（更多细节请参见[第6章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)，《*递归神经网络*》），加上DL4J特定的RNN输出层`RnnOutputLayer`。该输出层的激活函数是SoftMax。
- en: 'We can now create the network using the preceding configuration set, as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用前面设置的配置来创建网络，如下所示：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Before starting the training, we need to prepare the training set to make it
    ready to be used. For this purpose, we are going to use the dataset iterator by
    Alex Black that can be found among the GitHub examples for DL4J ([https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment/SentimentExampleIterator.java](https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment/SentimentExampleIterator.java)).
    It is in Java, so it has been adapted to Scala and added to the source code examples
    of this book. It implements the `DataSetIterator` interface ([https://static.javadoc.io/org.nd4j/nd4j-api/1.0.0-alpha/org/nd4j/linalg/dataset/api/iterator/DataSetIterator.html](https://static.javadoc.io/org.nd4j/nd4j-api/1.0.0-alpha/org/nd4j/linalg/dataset/api/iterator/DataSetIterator.html))
    and it is specialized for the IMDB review datasets. It expects as input a raw
    IMDB dataset (it could be a training or testing dataset), plus a `wordVectors`
    object, and then generates the dataset ready to be used for training/test purposes.
    This particular implementation uses the Google News 300 pre-trained vectors as
    `wordVectors` objects; it can be freely downloaded in GZIP format from the [https://github.com/mmihaltz/word2vec-GoogleNews-vectors/](https://github.com/mmihaltz/word2vec-GoogleNews-vectors/) GitHub
    repo. It needs to be unzipped before it can be used. Once extracted, the model
    can be loaded though the `loadStaticModel` of the `WordVectorSerializer` class ([https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/embeddings/loader/WordVectorSerializer.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/embeddings/loader/WordVectorSerializer.html))
    as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始训练之前，我们需要准备训练集，以使其准备好供使用。为此，我们将使用Alex Black的`dataset iterator`，该迭代器可以在DL4J的GitHub示例中找到（[https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment/SentimentExampleIterator.java](https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/recurrent/word2vecsentiment/SentimentExampleIterator.java)）。它是用Java编写的，因此已经被改编为Scala并添加到本书的源代码示例中。它实现了`DataSetIterator`接口（[https://static.javadoc.io/org.nd4j/nd4j-api/1.0.0-alpha/org/nd4j/linalg/dataset/api/iterator/DataSetIterator.html](https://static.javadoc.io/org.nd4j/nd4j-api/1.0.0-alpha/org/nd4j/linalg/dataset/api/iterator/DataSetIterator.html)），并且专门针对IMDB评论数据集。它的输入是原始的IMDB数据集（可以是训练集或测试集），以及一个`wordVectors`对象，然后生成准备好用于训练/测试的数据集。这个特定的实现使用了Google
    News 300预训练向量作为`wordVectors`对象；可以从[https://github.com/mmihaltz/word2vec-GoogleNews-vectors/](https://github.com/mmihaltz/word2vec-GoogleNews-vectors/)
    GitHub库中免费下载GZIP格式的文件。需要解压缩后才能使用。一旦提取，模型可以通过`WordVectorSerializer`类的`loadStaticModel`方法加载（[https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/embeddings/loader/WordVectorSerializer.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/embeddings/loader/WordVectorSerializer.html)），如下所示：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Training and test data can be now prepared through the custom dataset iterator
    `SentimentExampleIterator`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以通过自定义数据集迭代器`SentimentExampleIterator`准备训练和测试数据：
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then, we can test and evaluate the model in DL4J and Spark as explained in [Chapter
    6](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Recurrent Neural Networks*, [Chapter
    7](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Training Neural Networks with
    Spark*, and [Chapter 8](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Monitoring
    and Debugging Neural Network Training*. Please be aware that the Google model
    used here is pretty big (about 3.5 GB), so take this into account when training
    the model in this example, in terms of resources needed (memory in particular).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以在DL4J和Spark中测试和评估模型，具体内容参见[第6章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)，《*递归神经网络*》、[第7章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)，《*使用Spark训练神经网络*》，以及[第8章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)，《*监控和调试神经网络训练*》。请注意，本文中使用的Google模型非常大（约3.5
    GB），因此在训练该示例中的模型时，需考虑所需的资源（特别是内存）。
- en: In this first code example, we have used the common API of the DL4J main modules
    that are typically used for different MNNs in different use case scenarios. We
    have also explicitly used Word2Vec there. Anyway, the DL4J API also provides some
    basic facilities specific for NLP built on top of ClearTK ([https://cleartk.github.io/cleartk/](https://cleartk.github.io/cleartk/)),
    an open source framework for ML, and NLP for Apache UIMA ([http://uima.apache.org/](http://uima.apache.org/)).
    In the second example that is going to be presented in this section, we are going
    to use those facilities.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一个代码示例中，我们使用了DL4J主模块的常用API，这些API通常用于不同用例场景中的不同MNN。我们还在其中明确使用了Word2Vec。无论如何，DL4J
    API还提供了一些针对NLP的基础设施，这些设施是基于ClearTK（[https://cleartk.github.io/cleartk/](https://cleartk.github.io/cleartk/)）构建的，ClearTK是一个开源的机器学习（ML）和自然语言处理（NLP）框架，适用于Apache
    UIMA（[http://uima.apache.org/](http://uima.apache.org/)）。在本节接下来展示的第二个示例中，我们将使用这些设施。
- en: 'The dependencies for this second example are DataVec, DL4J NLP, and ND4J. While
    they are properly loaded as transitive dependencies by Maven or Gradle, the following
    two libraries. Need to be explicitly declared among the project dependencies to
    skip `NoClassDefFoundError` at runtime:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 该第二个示例的依赖项是DataVec、DL4J NLP和ND4J。尽管它们已通过Maven或Gradle正确加载为传递性依赖项，但以下两个库需要明确声明在项目依赖项中，以避免在运行时发生`NoClassDefFoundError`：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A file containing about 100,000 generic sentences has been used as input for
    this example. We need to load it in our application, as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含大约100,000个通用句子的文件已作为此示例的输入。我们需要将其加载到我们的应用程序中，操作如下：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The DL4J NLP library provides the `SentenceIterator` interface ([https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/sentenceiterator/SentenceIterator.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/sentenceiterator/SentenceIterator.html))
    and several implementations for it. In this specific example, we are going to
    use the `BasicLineIterator` implementation ([https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/sentenceiterator/BasicLineIterator.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/sentenceiterator/BasicLineIterator.html))
    in order to remove white spaces at the beginning and the end of each sentence
    in the input text, as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J NLP库提供了`SentenceIterator`接口（[https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/sentenceiterator/SentenceIterator.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/sentenceiterator/SentenceIterator.html)）以及多个实现。在这个特定的示例中，我们将使用`BasicLineIterator`实现（[https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/sentenceiterator/BasicLineIterator.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/sentenceiterator/BasicLineIterator.html)），以便去除输入文本中每个句子开头和结尾的空格，具体操作如下：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We need to do the tokenization now in order to segment the input text into
    single words. For this, we use the `DefaultTokenizerFactory` implementation ([https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/tokenization/tokenizerfactory/DefaultTokenizerFactory.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/tokenization/tokenizerfactory/DefaultTokenizerFactory.html))
    and set as tokenizer a `CommomPreprocessor` ([https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/tokenization/tokenizer/preprocessor/CommonPreprocessor.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/tokenization/tokenizer/preprocessor/CommonPreprocessor.html))
    to remove punctuation marks, numbers, and special characters, and then force lowercase
    for all the generated tokens, as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要进行分词操作，将输入文本切分成单个词语。为此，我们使用`DefaultTokenizerFactory`实现（[https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/tokenization/tokenizerfactory/DefaultTokenizerFactory.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/tokenization/tokenizerfactory/DefaultTokenizerFactory.html)），并设置`CommomPreprocessor`（[https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/tokenization/tokenizer/preprocessor/CommonPreprocessor.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/text/tokenization/tokenizer/preprocessor/CommonPreprocessor.html)）作为分词器，去除标点符号、数字和特殊字符，并将所有生成的词元强制转换为小写，具体操作如下：
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The model can now be built, as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 模型现在可以构建，如下所示：
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As mentioned earlier, we are using Word2Vec, so the model is built through the
    `Word2Vec.Builder` class ([https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/word2vec/Word2Vec.Builder.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/word2vec/Word2Vec.Builder.html)),
    setting as tokenizer factory for the one created previously.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用的是 Word2Vec，因此模型是通过 `Word2Vec.Builder` 类 ([https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/word2vec/Word2Vec.Builder.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/word2vec/Word2Vec.Builder.html))
    构建的，设置为先前创建的分词器工厂。
- en: 'Let''s start the model fitting:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始模型拟合：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And save the word vectors in a file when finished, as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，可以将词向量保存在文件中，具体如下：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `WordVectorSerializer` utility class ([https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/embeddings/loader/WordVectorSerializer.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/embeddings/loader/WordVectorSerializer.html))
    handles word vector serialization and persistence.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`WordVectorSerializer` 工具类 ([https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/embeddings/loader/WordVectorSerializer.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/embeddings/loader/WordVectorSerializer.html))
    处理词向量的序列化和持久化。'
- en: 'The model can be tested this way:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下方式测试模型：
- en: '[PRE11]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The produced output is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出如下：
- en: '![](img/e3123b10-3450-4244-9fa8-e00c7c0f457e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3123b10-3450-4244-9fa8-e00c7c0f457e.png)'
- en: '**GloVe** ([https://en.wikipedia.org/wiki/GloVe_(machine_learning)](https://en.wikipedia.org/wiki/GloVe_(machine_learning))),
    like Wor2Vec, is a model for distributed word representation, but it uses a different
    approach. While Word2Vec extracts the embeddings from a neural network that is
    designed to predict neighboring words, in GloVe the embeddings are optimized directly.
    This way the product of two-word vectors is equal to the logarithm of the number
    of times the two words occur near each other. For example, if the words *cat*
    and *mouse* occur near each other 20 times in a text, then *(vec(cat) * vec(mouse))
    = log(20)*. The DL4J NLP library also provides a GloVe model implementation, `GloVe.Builder`
    ([https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/glove/Glove.Builder.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/glove/Glove.Builder.html)).
    So, this example could be adapted for the GloVe model. The same file containing
    about 100,000 generic sentences used for the Word2Vec example is the input for
    this new one. The `SentenceIterator` and tokenization don''t change (the same
    as for the Word2Vec example). What''s different is the model to build, as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**GloVe** ([https://en.wikipedia.org/wiki/GloVe_(machine_learning)](https://en.wikipedia.org/wiki/GloVe_(machine_learning)))，与
    Word2Vec 类似，是一种分布式词表示模型，但采用了不同的方法。Word2Vec 从一个旨在预测相邻词语的神经网络中提取嵌入，而 GloVe 则直接优化嵌入。这样，两个词向量的乘积等于这两个词在一起出现次数的对数。例如，如果词语
    *cat* 和 *mouse* 在文本中一共出现了 20 次，那么 *(vec(cat) * vec(mouse)) = log(20)*。DL4J NLP
    库也提供了 GloVe 模型的实现，`GloVe.Builder` ([https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/glove/Glove.Builder.html](https://static.javadoc.io/org.deeplearning4j/deeplearning4j-nlp/1.0.0-alpha/org/deeplearning4j/models/glove/Glove.Builder.html))。因此，这个示例可以适配到
    GloVe 模型。与 Word2Vec 示例相同的包含约 100,000 个通用句子的文件作为新的输入。`SentenceIterator` 和分词方法没有变化（与
    Word2Vec 示例相同）。不同之处在于构建的模型，如下所示：'
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can fit the model by invoking its `fit` method, as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用其 `fit` 方法来拟合模型，具体如下：
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After the fitting process completes, we can use model to do several things,
    such as find the similarity between two words, as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合过程完成后，我们可以使用模型执行多项操作，例如查找两个词之间的相似度，具体如下：
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Or, find the *n* nearest words to a given one:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，找到与给定词语最相似的 *n* 个词：
- en: '[PRE15]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output produced will look like this:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 产生的输出将如下所示：
- en: '![](img/c8400fd5-a95a-4df1-af5f-ab92a0e621f5.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8400fd5-a95a-4df1-af5f-ab92a0e621f5.png)'
- en: After seeing these last two examples, you are probably wondering which model,
    Word2Vec or GloVe, is better. There is no winner; it all depends on the data.
    It is possible to pick up one model and train it in a way that the encoded vectors
    at the end become specific for the domain of the use case scenario in which the
    model is working.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到这最后两个例子后，你可能会想知道到底哪个模型更好，是Word2Vec还是GloVe。其实没有绝对的赢家，这完全取决于数据。你可以选择一个模型并以某种方式训练它，使得最终编码的向量变得特定于模型工作所在的用例场景的领域。
- en: Hands-on NLP with TensorFlow
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用TensorFlow进行实践中的NLP
- en: 'In this section, we are going to use TensorFlow (Python) to do DL sentiment
    analysis using the same *Large Movie Review Dataset* as for the first example
    in the previous section. Prerequisites for this example are Python 2.7.x, the
    PIP package manager, and Tensorflow. The *Importing Python Models in the JVM with
    DL4J* section in [Chapter 10](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Deploying
    on a Distributed System*, covers the details of setting up the required tools.
    We are also going to use the TensorFlow hub library ([https://www.tensorflow.org/hub/](https://www.tensorflow.org/hub/)),
    which has been created for reusable ML modules. It needs to be installed through
    `pip`, as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用TensorFlow（Python）进行深度学习情感分析，使用与上一节第一个示例相同的*大型电影评论数据集*。本示例的前提是Python
    2.7.x、PIP包管理器和TensorFlow。*在JVM中导入Python模型与DL4J*一节位于[第10章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)的*部署到分布式系统*部分，涵盖了设置所需工具的详细信息。我们还将使用TensorFlow
    Hub库（[https://www.tensorflow.org/hub/](https://www.tensorflow.org/hub/)），这是为可重用的机器学习模块而创建的。需要通过`pip`安装，如下所示：
- en: '[PRE16]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The example also requires the `pandas` ([https://pandas.pydata.org/](https://pandas.pydata.org/))
    data analysis library, as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例还需要`pandas`（[https://pandas.pydata.org/](https://pandas.pydata.org/)）数据分析库，如下所示：
- en: '[PRE17]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Import the necessary modules:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 导入必要的模块：
- en: '[PRE18]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we define a function to load all of the files from an input directory
    into a pandas DataFrame, as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数，将所有文件从输入目录加载到pandas DataFrame中，如下所示：
- en: '[PRE19]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we define another function to merge the positive and negative reviews,
    add a column called `polarity`*,* and do some shuffling, as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义另一个函数来合并正面和负面评论，添加一个名为`polarity`*的列*，并进行一些随机打乱，如下所示：
- en: '[PRE20]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Implement a third function to download the movie review dataset and use the `load_dataset`
    function to create the following training and test DataFrames:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 实现第三个函数来下载电影评论数据集，并使用`load_dataset`函数创建以下训练集和测试集DataFrame：
- en: '[PRE21]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This function downloads the datasets the first time the code is executed. Then,
    unless you delete them, the following executions get them from the local disk.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数在第一次执行代码时会下载数据集。然后，除非你删除它们，否则后续执行将从本地磁盘获取它们。
- en: 'The two DataFrames are then created this way:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个DataFrame是通过这种方式创建的：
- en: '[PRE22]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can also pretty-print the training DataFrame head to the console to check
    that everything went fine, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将训练数据集的前几行漂亮地打印到控制台，以检查一切是否正常，如下所示：
- en: '[PRE23]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The example output is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 示例输出如下：
- en: '![](img/0892372b-9b6d-40d6-a465-3292fffac720.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0892372b-9b6d-40d6-a465-3292fffac720.png)'
- en: 'Now that we have the data, we can define the model. We are going to use the
    **Estimator** API ([https://www.tensorflow.org/guide/estimators](https://www.tensorflow.org/guide/estimators)),
    a high-level TensorFlow API that has been introduced in the framework to simplify
    ML programming. *Estimator* provides some input functions that form the wrapper
    of the pandas DataFrames. So, we define the following function: `train_input_fn`
    to train on the whole training set with no limit on training epochs:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了数据，可以定义模型了。我们将使用**Estimator** API（[https://www.tensorflow.org/guide/estimators](https://www.tensorflow.org/guide/estimators)），这是TensorFlow中的一个高级API，旨在简化机器学习编程。*Estimator*提供了一些输入函数，作为pandas
    DataFrame的封装。所以，我们定义如下函数：`train_input_fn`，以在整个训练集上进行训练，并且不限制训练轮次：
- en: '[PRE24]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To do prediction on the whole training set execute the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对整个训练集进行预测，执行以下操作：
- en: '[PRE25]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And we use `predict_test_input_fn` to do predictions on the test set:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用`predict_test_input_fn`对测试集进行预测：
- en: '[PRE26]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The TensorFlow hub library provides a feature column that applies a module
    on a given input text feature whose values are strings, and then passes the outputs
    of the module downstream. In this example, we are going to use the `nnlm-en-dim128`
    module ([https://tfhub.dev/google/nnlm-en-dim128/1](https://tfhub.dev/google/nnlm-en-dim128/1)),
    which has been trained on the English Google News 200B corpus. The way we embed
    and use this module in our code is as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow hub 库提供了一个特征列，它会对给定的输入文本特征应用一个模块，该特征的值是字符串，然后将模块的输出传递到下游。在这个示例中，我们将使用`nnlm-en-dim128`模块（[https://tfhub.dev/google/nnlm-en-dim128/1](https://tfhub.dev/google/nnlm-en-dim128/1)），该模块已经在英文
    Google News 200B 语料库上进行了训练。我们在代码中嵌入和使用该模块的方式如下：
- en: '[PRE27]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'For classification purposes, we use a `DNNClassifier` ([https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier))
    provided by the TensorFlow hub library. It extends `Estimator` ([https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator))
    and is a classifier for TensorFlow DNN models. So the `Estimator` in our example
    is created this way:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 出于分类目的，我们使用 TensorFlow hub 库提供的`DNNClassifier`（[https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier)）。它扩展了`Estimator`（[https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)），并且是
    TensorFlow DNN 模型的分类器。所以，在我们的示例中，`Estimator`是这样创建的：
- en: '[PRE28]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that we are specifying `embedded_text_feature_column` as a feature column.
    The two hidden layers have `500` and `100` nodes respectively. `AdagradOptimizer`
    is the default optimizer for `DNNClassifier`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将`embedded_text_feature_column`指定为特征列。两个隐藏层分别具有`500`和`100`个节点。`AdagradOptimizer`是`DNNClassifier`的默认优化器。
- en: 'The training of the model can be implemented with a single line of code, by
    invoking the `train` method of our `Estimator`*,* as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的训练可以通过一行代码实现，方法是调用我们的`Estimator`*的`train`方法，如下所示：
- en: '[PRE29]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Given the size of the training dataset used for this example (25 KB), 1,000
    steps is equivalent to five epochs (using the default batch size).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个示例使用的训练数据集大小为 25 KB，1,000 步相当于五个 epoch（使用默认的批量大小）。
- en: 'After the training has completed, we can then do predictions for the training
    dataset, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以对训练数据集进行预测，具体如下：
- en: '[PRE30]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And the test dataset as well, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 测试数据集如下：
- en: '[PRE31]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Here''s the output of the application, showing the accuracy for both predictions:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这是应用程序的输出，显示了两种预测的准确性：
- en: '![](img/19709820-20cd-46cc-81e6-039c059c3b41.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19709820-20cd-46cc-81e6-039c059c3b41.png)'
- en: 'We can also do evaluation of the model and, as explained in [Chapter 9](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml),
    *Interpreting Neural Network Output*, in the *Evaluation for Classification* section,
    calculate the confusion matrix in order to understand the distribution of wrong
    classifications. Let''s define a function to get the predictions first, as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对模型进行评估，正如在[第9章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)中所解释的，*解释神经网络输出*，在*分类评估*部分中，计算混淆矩阵以了解错误分类的分布。首先让我们定义一个函数来获取预测值，具体如下：
- en: '[PRE32]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, create the confusion matrix starting on the training dataset, as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从训练数据集开始创建混淆矩阵，具体如下：
- en: '[PRE33]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'And, normalize it to have each row sum equals to `1`, as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将其归一化，使每行的总和等于`1`，具体如下：
- en: '[PRE34]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output of the confusion matrix on screen will look like this:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕上显示的混淆矩阵输出将如下所示：
- en: '![](img/1a61cf30-383f-406d-aafe-31d1bff21dd3.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a61cf30-383f-406d-aafe-31d1bff21dd3.png)'
- en: However, you can also render it in a more elegant way using some chart library
    available in Python of your choice.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你也可以使用你选择的 Python 图表库以更优雅的方式呈现它。
- en: You have noticed that, while this code is compact and doesn't require advanced
    Python knowledge, it isn't an easy entry point for a starter in ML and DL, as
    TensorFlow implicitly requires a good knowledge of ML concepts in order to understand
    its API. Making a comparison with the DL4J API, you can tangibly feel this difference.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，尽管这段代码很简洁且不需要高级的 Python 知识，但它并不是机器学习（ML）和深度学习（DL）初学者的易入门点，因为 TensorFlow
    隐式要求对 ML 概念有一定了解，才能理解其 API。与 DL4J API 比较时，你可以明显感觉到这种差异。
- en: Hand-on NLP with Keras and a TensorFlow backend
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 和 TensorFlow 后端进行实践 NLP
- en: As mentioned in [Chapter 10](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Deploying
    on a Distributed System*, in the  *Importing Python Models in the JVM with DL4J*
    section, when doing DL in Python, an alternative to TensorFlow is Keras. It can
    be used as a high-level API on top of a TensorFlow backed. In this section, we
    are going to learn how to do sentiment analysis in Keras, and finally we will
    make a comparison between this implementation and the previous one in TensorFlow.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第10章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)《在分布式系统上部署》中所述，在*使用DL4J在JVM中导入Python模型*部分，当在Python中进行深度学习时，TensorFlow的替代方案是Keras。它可以作为一个高层API，在TensorFlow的支持下使用。在本节中，我们将学习如何在Keras中进行情感分析，最后，我们将比较此实现与之前TensorFlow中的实现。
- en: 'We are going to use the exact same IMDB dataset (25,000 samples for training
    and 25,000 for test) as for the previous implementations through DL4J and TensorFlow.
    The prerequisites for this example are the same as for the TensorFlow example
    (Python 2.7.x, the PIP package manager, and Tensorflow), plus of course Keras.
    The Keras code module has that dataset built in:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与前面通过DL4J和TensorFlow实现相同的IMDB数据集（25,000个训练样本和25,000个测试样本）。此示例的先决条件与TensorFlow示例相同（Python
    2.7.x，PIP包管理器和TensorFlow），当然还需要Keras。Keras代码模块内置了该数据集：
- en: '[PRE35]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'So, we just need to set the vocabulary size and load the data from there, and
    not from any other external location, as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们只需要设置词汇表的大小并从那里加载数据，而不是从其他外部位置加载，如下所示：
- en: '[PRE36]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'At the end of the download, you can print a sample of the downloaded reviews
    for inspection purposes, as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后，您可以打印下载评论的样本以供检查，如下所示：
- en: '[PRE37]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is shown as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '![](img/d6c6c8d4-2558-41b2-bd8c-f261f020c74e.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d6c6c8d4-2558-41b2-bd8c-f261f020c74e.png)'
- en: 'You can see that at this stage the reviews are stored as a sequence of integers,
    IDs that have been preassigned to single words. Also the label is an integer (0
    means negative, 1 means positive). It is possible anyway to map the downloaded
    reviews back to their original words by using the dictionary returned by the `imdb.get_word_index()`
    method, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，在这个阶段，评论已作为整数序列存储，这些整数是预先分配给单个单词的ID。另外，标签是一个整数（0表示负面，1表示正面）。不过，您仍然可以通过使用`imdb.get_word_index()`方法返回的字典，将下载的评论映射回它们原始的单词，如下所示：
- en: '[PRE38]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](img/f6aedc98-82c7-44d2-addf-c3a8c3f67bcb.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6aedc98-82c7-44d2-addf-c3a8c3f67bcb.png)'
- en: 'In the preceding screenshot, you can see the returned dictionary of the words
    used in the input reviews. We are going to use an RNN model for this example.
    In order to feed data to it, all the inputs should have the same length. Looking
    at the maximum and minimum lengths of the downloaded reviews (following is the
    code to get this info and its output):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，您可以看到输入评论中使用的单词的返回字典。我们将使用RNN模型进行此示例。为了向模型输入数据，所有输入数据的长度必须相同。通过查看下载评论的最大和最小长度（以下是获取此信息的代码及其输出）：
- en: '[PRE39]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is shown as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '![](img/3517618f-f769-4a64-a8e6-e54fb4178429.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3517618f-f769-4a64-a8e6-e54fb4178429.png)'
- en: 'We can see that they don''t have all the same length. So, we need to limit
    the maximum review length to, let''s say, 500 words by truncating the longer reviews
    and padding the shorter ones with zeros. This can be done through the `sequence.pad_sequences`
    Keras function, as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到它们的长度不完全相同。因此，我们需要将最大评论长度限制为500个单词，比如通过截断较长的评论，并用零填充较短的评论。这可以通过`sequence.pad_sequences`
    Keras函数实现，如下所示：
- en: '[PRE40]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let''s design the RNN model, as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设计RNN模型，如下所示：
- en: '[PRE41]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'It is a simple RNN model, with three layers, embedding, LSTM, and dense, as
    follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的RNN模型，包含三层：嵌入层、LSTM层和全连接层，如下所示：
- en: '![](img/d5d01892-b8c9-4725-8cc3-0d848814f30e.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5d01892-b8c9-4725-8cc3-0d848814f30e.png)'
- en: The input for this model is a sequence of integer word IDs with a maximum length
    of `500`, and its output is a binary label (`0` or `1`).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型的输入是一个最大长度为`500`的整数单词ID序列，输出是一个二进制标签（`0`或`1`）。
- en: 'The configuration of the learning process for this model can be done through
    its `compile` method, as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型的学习过程配置可以通过其`compile`方法来完成，如下所示：
- en: '[PRE42]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'After setting up the batch size and number of training epochs, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好批量大小和训练周期数之后，如下所示：
- en: '[PRE43]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can start the training, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以开始训练，如下所示：
- en: '[PRE44]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![](img/478ba6b3-4b25-4568-84a7-955ff745d274.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/478ba6b3-4b25-4568-84a7-955ff745d274.png)'
- en: 'When the training completes, we can evaluate the model to assess its level
    of accuracy using the test dataset, as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练完成后，我们可以使用测试数据集评估模型的准确性，方法如下：
- en: '[PRE45]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![](img/0cf6ea34-cc97-4000-b328-39e87834b7ee.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0cf6ea34-cc97-4000-b328-39e87834b7ee.png)'
- en: Looking at the code of this example, you should have noticed that it is more
    high-level if than the previous example with TensorFlow, and that the focus at
    development time is mostly on the specific problem model implementation details
    rather than the ML/DL mechanisms behind it.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这个示例的代码，你应该已经注意到，相较于之前的TensorFlow示例，这个示例更高层次，开发时的重点主要放在特定问题模型的实现细节上，而不是其背后的机器学习/深度学习机制。
- en: Hands-on NLP with Keras model import into DL4J
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras模型导入到DL4J的实战NLP
- en: In [Chapter 10](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml), *Deploying on a
    Distributed System*, *Importing Python Models in the JVM with DL4J* section, we
    learned how to import existing Keras models into DL4J and use them to make predictions
    or re-train them in a JVM-based environment.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](3b6f47c0-6e17-484b-ad30-b6f92eb0473c.xhtml)，*在分布式系统上部署*、*在JVM中使用DL4J导入Python模型*部分，我们学习了如何将现有的Keras模型导入到DL4J中，并在JVM环境中使用它们进行预测或重新训练。
- en: 'This applies to the model we implemented and trained in the *Hand-on NLP with
    Keras and TensorFlow backend* section in Python, using Keras with a TensorFlow
    backed. We need to modify the code for that example to serialize the model in
    HDF5 format by doing the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这适用于我们在*使用Keras和TensorFlow后端的实战NLP*部分中实现并训练的模型，我们使用Keras和TensorFlow作为后端。我们需要修改该示例的代码，通过以下方式将模型序列化为HDF5格式：
- en: '[PRE46]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The `sa_rnn.h5` file produced needs to be copied into the resource folder for
    the Scala project to be implemented. The dependencies for the project are the
    DataVec API, the DL4J core, ND4J, and the DL4J model import library.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的`sa_rnn.h5`文件需要被复制到Scala项目的资源文件夹中。项目的依赖项包括DataVec API、DL4J核心、ND4J以及DL4J模型导入库。
- en: 'We need to import and transform the Large Movie Review database as explained
    in section 12.1, in case we want to retrain the model through DL4J. Then, we need
    to import the Keras model programmatically, as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要按照第12.1节中解释的方式导入并转换大型电影评论数据库，如果我们希望通过DL4J重新训练模型。然后，我们需要按如下方式编程导入Keras模型：
- en: '[PRE47]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Finally, we can start to do predictions by invoking the `predict` method of
    `model` (which is an instance of `MultiLayerNetwork`, as usual in DL4J), passing
    the input data as an ND4J DataSet ([https://static.javadoc.io/org.nd4j/nd4j-api/1.0.0-alpha/org/nd4j/linalg/dataset/api/DataSet.html](https://static.javadoc.io/org.nd4j/nd4j-api/1.0.0-alpha/org/nd4j/linalg/dataset/api/DataSet.html)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过调用`model`（这是`MultiLayerNetwork`的实例，和在DL4J中的常见做法一样）的`predict`方法，传入输入数据作为ND4J
    DataSet（[https://static.javadoc.io/org.nd4j/nd4j-api/1.0.0-alpha/org/nd4j/linalg/dataset/api/DataSet.html](https://static.javadoc.io/org.nd4j/nd4j-api/1.0.0-alpha/org/nd4j/linalg/dataset/api/DataSet.html)）来进行预测。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter closes the explanation of the NLP implementation process with Scala.
    In this chapter and the previous one, we evaluated different frameworks for this
    programming language, and the pros and cons of each have been detailed. In this
    chapter, the focus has been mostly on a DL approach to NLP. For that, some Python
    alternatives have been presented, and the potential integration of those Python
    models in a JVM context with the DL4J framework has been highlighted. At this
    stage, a reader should be able to accurately evaluate what will be the best fit
    for his/her particular NLP use case.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了对Scala实现过程的NLP解释。在本章和前一章中，我们评估了这种编程语言的不同框架，并详细列出了每种框架的优缺点。本章的重点主要放在了深度学习方法（DL）在NLP中的应用。为此，我们介绍了一些Python的替代方案，并强调了这些Python模型在JVM环境中与DL4J框架的潜在集成。此时，读者应该能够准确评估出哪些方案最适合他/她的特定NLP应用案例。
- en: Starting from the next chapter, we will learn more about convolution and how
    CNNs apply to image recognition problems. Image recognition will be explained
    by presenting different implementations using different frameworks, including
    DL4J, Keras, and TensorFlow.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 从下一章开始，我们将深入学习卷积和卷积神经网络（CNN）如何应用于图像识别问题。通过展示不同框架（包括DL4J、Keras和TensorFlow）的不同实现，将解释图像识别。
