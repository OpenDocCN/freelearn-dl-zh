- en: Understanding Black-Box Optimization Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解黑盒优化算法
- en: In the previous chapters, we looked at reinforcement learning algorithms, ranging
    from value-based to policy-based methods and from model-free to model-based methods. In
    this chapter, we'll provide another solution for solving sequential tasks, that
    is, with a class of black-box algorithms **evolutionary algorithms** (**EA**). EAs
    are driven by evolutionary mechanisms and are sometimes preferred to **reinforcement
    learning** (**RL**) as they don't require backpropagation. They also offer other
    complementary benefits to RL. We'll start this chapter by giving you a brief recap
    of RL algorithms so that you'll better understand how EA fits into these sets
    of problems. Then, you'll learn about the basic building blocks of EA and how
    those algorithms work. We'll also take advantage of this introduction and look
    at one of the most well-known EAs, namely **evolution strategies** (**ES**), in
    more depth.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们研究了强化学习（RL）算法，从基于价值的方法到基于策略的方法，以及从无模型方法到基于模型的方法。在本章中，我们将提供另一种解决序列任务的方法，那就是使用一类黑盒算法——**进化算法**（**EA**）。EAs由进化机制驱动，有时比**强化学习**（**RL**）更受青睐，因为它们不需要反向传播。它们还为RL提供了其他互补的好处。本章将从简要回顾强化学习（RL）算法开始，帮助你更好地理解EA如何融入这些问题解决方案中。接着，你将了解EA的基本构建模块及其工作原理。我们还将利用这个介绍，深入研究其中一种最著名的进化算法——**进化策略**（**ES**）。
- en: A recent algorithm that was developed by OpenAI caused a great boost in the
    adoption of ES for solving sequential tasks. They showed how ES algorithms can
    be massively parallelized and scaled linearly on a number of CPUs while achieving
    high performance. After an explanation of evolution strategies, we'll take a deeper
    look at this algorithm and develop it in TensorFlow so that you'll be able to
    apply it to the tasks you care about.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由OpenAI开发的一种近期算法大大推动了进化策略（ES）在解决序列任务中的应用。它们展示了ES算法如何能够在多个CPU上进行大规模并行化并线性扩展，同时实现高性能。在解释了进化策略后，我们将更深入地探讨这个算法，并在TensorFlow中进行开发，这样你就可以将它应用于你关心的任务。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Beyond RL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超越强化学习（RL）
- en: The core of EAs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EAs的核心
- en: Scalable evolution strategies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展的进化策略
- en: Scalable ES applied to LunarLander
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展的进化策略应用于LunarLander
- en: Beyond RL
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越强化学习（RL）
- en: RL algorithms are the usual choice when we're faced with sequential decision
    problems. Usually, it's difficult to find other ways to solve these tasks other
    than using RL. Despite the hundreds of different optimization methods that are
    out there, so far, only RL has worked well on problems for sequential decision-making.
    But this doesn't mean it's the only option.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）算法通常是我们面对序列决策问题时的首选。通常，除了使用RL，很难找到其他方法来解决这些任务。尽管有数百种不同的优化方法，但到目前为止，只有RL在序列决策问题上取得了良好的效果。但这并不意味着它是唯一的选择。
- en: We'll start this chapter by recapping on the inner workings of RL algorithms
    and questioning the usefulness of their components for solving sequential tasks.
    This brief summary will help us introduce a new type of algorithm that offers
    many advantages (as well as some disadvantages) that could be used as a replacement
    for RL.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将从回顾强化学习（RL）算法的内在工作原理开始，并质疑其组件在解决序列任务中的有效性。这个简要总结将帮助我们介绍一种新的算法类型，该算法提供了许多优点（以及一些缺点），可以作为RL的替代方案。
- en: A brief recap of RL
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习（RL）简要回顾
- en: In the beginning, a policy is initialized randomly and used to interact with
    the environment for either a given number of steps, or entire trajectories, to
    collect data. On each interaction, the state visited, the action taken, and the
    reward obtained are recorded. This information provides a full description of
    the influence of the agent in the environment. Then, in order to improve the policy,
    the backpropagation algorithm (based on the loss function, in order to move the
    predictions to a better estimate) computes the gradient of each weight of the
    network. These gradients are then applied with a stochastic gradient descent optimizer.
    This process (gathering data from the environment and optimizing the neural network
    with **stochastic gradient descent** (**SGD**)) is repeated until a convergence
    criterion is met.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时，策略被随机初始化，并用来与环境交互，收集数据，交互可以是给定的步数或整个轨迹。在每次交互中，访问的状态、采取的行动和获得的奖励都会被记录下来。这些信息提供了代理在环境中影响的完整描述。然后，为了改进策略，基于损失函数的反向传播算法（为了将预测值移向更好的估计）计算网络每个权重的梯度。接着，这些梯度会通过随机梯度下降优化器进行应用。这个过程（从环境中收集数据并使用**随机梯度下降**（**SGD**）优化神经网络）会一直重复，直到满足收敛标准为止。
- en: 'There are two important things to note here that will be useful in the following
    discussion:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两件重要的事情需要注意，它们在接下来的讨论中会非常有用：
- en: '**Temporal credit assignment**: Because RL algorithms optimize the policy on
    each step, allocating the quality of each action and state is required. This is
    done by assigning a value to each state-action pair. Moreover, a discount factor
    is used to minimize the influence of distant actions and to give more weight to
    the last actions. This will help us solve the problem of assigning the credit
    to the actions, but will also introduce inaccuracies in the system.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时序信用分配**：因为强化学习算法在每一步优化策略，所以需要对每个行动和状态分配质量。这是通过为每个状态-行动对分配一个值来完成的。此外，使用折扣因子来最小化远距离行动的影响，并给予最后行动更多的权重。这将帮助我们解决将信用分配给行动的问题，但也会引入系统中的不准确性。'
- en: '**Exploration**: In order to maintain a degree of exploration in the actions, additional
    noise is injected into the policy of RL algorithms. The way in which the noise
    is injected depends on the algorithm, but usually, the actions are sampled from
    a stochastic distribution. By doing so, if the agent is in the same situation
    twice, it may take different actions that would lead to two different paths. This
    strategy also encourages exploration in deterministic environments. By deviating
    the path each time, the agent may discover different – and potentially better –
    solutions. With this additional noise that asymptotically tends to 0, the agent
    is then able to converge to a better and final deterministic policy.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**探索**：为了保持一定程度的探索，额外的噪声会被注入到强化学习算法的策略中。噪声注入的方式依赖于算法，通常情况下，行动是从一个随机分布中采样的。这样做的目的是，如果代理处于相同的情境两次，它可能会采取不同的行动，进而导致两条不同的路径。这种策略也能在确定性环境中鼓励探索。通过每次偏离路径，代理可能会发现不同的——甚至是更好的——解决方案。通过这种额外的噪声，且噪声渐进趋近于0，代理最终能够收敛到一个更好且最终的确定性策略。'
- en: But are backpropagation, temporal credit assignment, and stochastic actions
    actually a prerequisite for learning and building complex policies?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，反向传播、时序信用分配和随机行动，真的算是学习和构建复杂策略的前提条件吗？
- en: The alternative
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替代方法
- en: The answer to this question is no.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案是否定的。
- en: As we learned in [Chapter 10](e61cf178-d8e8-47bf-ab79-ef546f23e309.xhtml), *Imitation
    Learning with the DAgger Algorithm*, by reducing policy learning to an imitation
    problem using backpropagation and SGD, we can learn about a discriminative model from
    an expert in order to predict which actions to take next. Still, this involves
    backpropagation and requires an expert that may not always be available.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第10章](e61cf178-d8e8-47bf-ab79-ef546f23e309.xhtml)《使用DAgger算法的模仿学习》中所学到的那样，通过使用反向传播和随机梯度下降（SGD）将策略学习转化为模仿问题，我们可以通过专家学习一个**判别模型**，以预测下一步应采取的行动。不过，这仍然涉及反向传播，并且需要一个可能并不总是能获得的专家。
- en: Another general subset of algorithms for global optimization does exist. They
    are called EAs, and they aren't based on backpropagation and don't require any
    of the other two principles, namely temporal credit assignment and noisy actions.
    Furthermore, as we said in the introduction to this chapter, these evolutionary
    algorithms are very general and can be used in a large variety of problems, including
    sequential decision tasks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用于全局优化的常见算法子集确实存在。它们被称为进化算法（EAs），并且不基于反向传播，也不需要另外两个原理，即时间信用分配和噪声行为。此外，正如我们在本章开头所说的，这些进化算法非常通用，可以应用于各种问题，包括顺序决策任务。
- en: EAs
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进化算法（EAs）
- en: As you may have guessed, EAs differ in many aspects from RL algorithms and are
    principally inspired by biological evolution. EAs include many similar methods
    such as, genetic algorithms, evolution strategies, and genetic programming, which
    vary in their implementation details and in the nature of their representation.
    However, they are all mainly based on four basic mechanisms – reproductions, mutation,
    crossover, and selection – that are cycled in a guess-and-check process. We'll
    see what this means as we progress through this chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能猜到的，进化算法与强化学习算法在许多方面不同，并且主要受到生物进化的启发。进化算法包括许多类似的方法，如遗传算法、进化策略和遗传编程，它们在实现细节和表示的方式上有所不同。然而，它们都主要基于四个基本机制——繁殖、变异、交叉和选择——并通过一种猜测与检验的过程循环进行。随着我们本章的进展，我们将看到这些机制具体意味着什么。
- en: Evolutionary algorithms are defined as black-box algorithms. These are algorithms
    that optimize a function, [![](img/7bf9bc1b-7d93-4527-8b6c-db935d6b9f12.png)],
    with respect to [![](img/78d5816f-a32e-4420-83f3-a85a85de01f8.png)] without making
    any assumption about [![](img/0da66c3f-a9e7-4314-9b1c-8029fbd2092e.png)]. Hence, [![](img/469deea3-9131-4342-8c89-058bb702d379.png)] can
    be anything you want. We only care about the output of [![](img/7d4de719-2d7d-4b35-8071-af7c8167991a.png)].
    This has many advantages, as well as some disadvantages. The primary advantage
    is that we don't have to care about the structure of [![](img/434d6333-a4e5-45a8-9b64-e420e9184a95.png)] and
    we are free to use what is best for us and for the problem at hand. On the other
    hand, the main disadvantage is that these optimization methods cannot be explained
    and thus their mechanism cannot be interpreted. In problems where interpretability
    is of great importance, these methods are not appealing.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算法被定义为黑箱算法。这些算法优化一个函数，[![](img/7bf9bc1b-7d93-4527-8b6c-db935d6b9f12.png)]，相对于[![](img/78d5816f-a32e-4420-83f3-a85a85de01f8.png)]，而不对[![](img/0da66c3f-a9e7-4314-9b1c-8029fbd2092e.png)]做任何假设。因此，[![](img/469deea3-9131-4342-8c89-058bb702d379.png)]可以是你想要的任何东西。我们只关心[![](img/7d4de719-2d7d-4b35-8071-af7c8167991a.png)]的输出。这有许多优点，也有一些缺点。主要的优点是我们不需要关心[![](img/434d6333-a4e5-45a8-9b64-e420e9184a95.png)]的结构，我们可以自由地使用对我们和当前问题最合适的方法。另一方面，主要的缺点是这些优化方法无法解释，因此其机制无法被理解。在需要可解释性的情况下，这些方法就不太有吸引力了。
- en: Reinforcement learning has almost always been preferred for solving sequential
    tasks, especially for medium to difficult tasks. However, a recent paper from
    OpenAI highlights that the evolution strategy, which is an evolutionary algorithm,
    can be used as an alternative to RL. This statement is mainly due to the performance
    that's reached asymptotically by the algorithm and its incredible ability to be
    scaled across thousands of CPUs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习几乎一直被优先用于解决顺序任务，尤其是对于中等到困难的任务。然而，OpenAI 最近的一篇论文强调，进化策略（Evolutionary Strategy），一种进化算法，可以作为强化学习的替代方法。这一观点主要源于该算法所达到的渐近性能以及其在数千个
    CPU 上扩展的惊人能力。
- en: Before we look at how this algorithm is able to scale so well while learning
    good policies on difficult tasks, let's take a more in-depth look at EAs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们了解这个算法如何能够在学习困难任务的优秀策略时良好扩展之前，让我们更深入地了解进化算法。
- en: The core of EAs
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进化算法的核心
- en: EAs are inspired by biological evolution and implement techniques and mechanisms
    that simulate biological evolution. This means that EAs go through many trials
    to create a population of new candidate solutions. These solutions are also called
    **individuals** (in RL problems, a candidate solution is a policy) that are better
    than the previous generation, in a similar way to the process within nature wherein
    only the strongest survive and have the possibility to procreate.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算法（EAs）受到生物进化的启发，采用模拟生物进化的技术和机制。这意味着进化算法通过多次试验来创建新的候选解种群。这些解也被称为**个体**（在强化学习问题中，候选解就是策略），它们比上一代更优，类似于自然界中的过程，只有最强者生存下来并有可能繁衍后代。
- en: 'One of the advantages of EAs is that they are derivative-free methods, meaning
    that they don''t use the derivative to find the solution. This allows EAs to work
    very well with all sorts of differentiable and non-differentiable functions, including deep
    neural networks. This combination is schematized in the following diagram. Note
    that each individual is a separate deep neural network, and so we''ll have as
    many neural networks as the number of individuals at any given moment. In the
    following diagram, the population is composed of five individuals:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算法的一个优势是它们是无导数的方法，意味着它们不使用导数来寻找解。这使得进化算法能够很好地处理各种可微和不可微的函数，包括深度神经网络。这种结合在下图中进行了示意。请注意，每个个体都是一个独立的深度神经网络，因此在任何时刻，神经网络的数量与个体数量相同。在下图中，种群由五个个体组成：
- en: '![](img/185767dc-22b5-4956-a2fe-f07e80085a75.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/185767dc-22b5-4956-a2fe-f07e80085a75.png)'
- en: Figure 11.1\. Optimization of deep neural networks through evolutionary algorithms
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1\. 通过进化算法优化深度神经网络
- en: 'The specificity of each type of evolutionary algorithm differs from the others,
    but the underlying cycle is common to all the EAs and works as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 每种进化算法的具体性有所不同，但它们的基本循环在所有进化算法中是共同的，其工作原理如下：
- en: A population of individuals (also called **candidate solutions** or **phenotypes**)
    is created so that each of them has a set of different properties (called **chromosomes**
    or **genotypes**). The initial population is initialized randomly.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个由个体组成的种群（也称为**候选解**或**表型**），每个个体都有一组不同的属性（称为**染色体**或**基因型**）。初始种群是随机初始化的。
- en: Each candidate solution is evaluated independently by a fitness function that
    determines its quality. The fitness function is usually related to the objective
    function and, using the terminology we've used so far, the fitness function could
    be the total reward accumulated by the agent (that is, the candidate solution)
    throughout its life.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个候选解都通过一个适应度函数独立评估，该函数决定解的质量。适应度函数通常与目标函数相关，按照我们到目前为止使用的术语，适应度函数可以是智能体（即候选解）在其生命周期内累积的总奖励。
- en: Then, the fitter individuals of the population are selected, and their genome
    is modified in order to produce the new generation. In some cases, the less fit
    candidate solution can be used as a negative example to generate the next generation.
    This whole step varies largely, depending on the algorithm. Some algorithms, such
    as genetic algorithms, breed new individuals through two processes called **crossover**
    and **mutation**, which give birth to new individuals (called **offspring**).
    Others, such as evolution strategies, breed new individuals through mutation only.
    We'll explain crossover and mutation in more depth later in this chapter, but
    generally speaking, crossover is the process that combines genetic information
    from two parents, while mutation only alters some gene values in the offspring.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，从种群中选出适应性更强的个体，并修改它们的基因组，以便生成新一代个体。在某些情况下，适应性较差的候选解可以作为负面示例，帮助生成下一代。这个过程的具体步骤在不同算法中差异较大。一些算法，例如遗传算法，通过**交叉**和**变异**这两个过程来繁殖新个体，这些新个体被称为**后代**。其他算法，如进化策略，仅通过变异来繁殖新个体。我们将在本章稍后更深入地解释交叉和变异，但一般来说，交叉是将两个父代的基因信息结合的过程，而变异则仅改变后代中某些基因值。
- en: Repeat the whole process, going through steps 1-3 until a terminal condition
    is met. On each iteration, the population that's created is also called a **generation**.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复整个过程，经过步骤 1-3，直到满足终止条件。在每次迭代中，创建的种群也称为**代**。
- en: 'This iterative process, as shown in the following diagram, terminates when
    a given fitness level has been reached or a maximum number of generations have
    been produced. As we can see, the population is created by crossover and mutation,
    but as we habe already explained, these processes may vary, depending on the specific
    algorithm:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，这一迭代过程在达到给定的适应度水平或生成了最大数量的世代后终止。如我们所见，种群是通过交叉和变异生成的，但正如我们已经解释过的，这些过程可能会有所不同，具体取决于使用的算法：
- en: '![](img/f73e8a1d-c237-49b8-9e85-d6a9565eb061.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f73e8a1d-c237-49b8-9e85-d6a9565eb061.png)'
- en: Figure 11.2\. The main cycle of evolutionary algorithms
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2. 进化算法的主要循环
- en: 'The main body of a general EA is very simple and can be written in just a few
    lines of code, as shown here. To summarize this code, on each iteration, and until
    a fitted generation has been produced, new candidates are generated and evaluated.
    The candidates are created from the best-fitted individuals of the previous generation:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一般演化算法的主体非常简单，可以用几行代码来实现，如这里所示。概括来说，在每次迭代中，直到生成适应度合格的个体为止，会生成新的候选个体并进行评估。这些候选个体是由上一代中适应度最好的个体生成的：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that the implementation details of the solver are dependent on the algorithm
    that's used.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，求解器的实现细节取决于所使用的算法。
- en: The applications of EAs are actually spread across many fields and problems,
    from economy to biology, and from computer program optimization to ant colony
    optimization.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算法的应用实际上广泛分布于多个领域和问题，从经济学到生物学，从计算机程序优化到蚁群优化。
- en: Since we are mostly interested in the application of evolutionary algorithms
    for solving sequential decision-making tasks, we will explain the two most common
    EAs that are used to solve these kinds of jobs. They are known as **genetic algorithms**
    (**GAs**) and **evolution strategies** (**ESes**). Later, we'll take a step further
    with ES by developing a highly scalable version of it.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们主要关注的是进化算法在解决顺序决策任务中的应用，我们将解释两种最常见的进化算法，它们被用来解决这些类型的任务。它们分别被称为**遗传算法**（**GAs**）和**进化策略**（**ESes**）。随后，我们将进一步发展进化策略，开发其高度可扩展的版本。
- en: Genetic algorithms
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遗传算法
- en: 'The idea of GAs is very straightforward—evaluate the current generations, use
    only the top-performing individuals to generate the next candidate solutions,
    and discard the other individuals. This is shown in the preceding diagram. The
    survivors will generate the next population by crossover and mutation. These two
    processes are represented in the following diagram. Crossover is done by selecting
    two solutions among the survivors and combining their parameters. Mutation, on
    the other hand, involves changing a few random parameters on the offspring''s
    genotype:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算法的思想非常直接——评估当前的代数，只使用表现最好的个体生成下一代候选解，并丢弃其他个体。如前面所示的图表所示，存活下来的个体通过交叉和变异生成下一代种群。这两个过程在以下图表中表示。交叉是通过在存活个体中选择两个解，并将它们的参数结合在一起进行的。变异则是通过改变后代基因型中的一些随机参数来实现的：
- en: '![](img/40983c68-bd09-46a1-8955-2f6bb61edb4d.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40983c68-bd09-46a1-8955-2f6bb61edb4d.png)'
- en: Figure 11.3\. Visual illustration of mutation and crossover
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3. 变异和交叉的视觉示意图
- en: Crossover and mutation can be approached in many different ways. In the simpler
    version, crossover is done by choosing parts from the two parents randomly, and
    mutation is done by mutating the solution that's obtained by adding Gaussian noise
    with a fixed standard deviation. By only keeping the best individuals and injecting
    their genes into the newly born individuals, the solutions will improve over time
    until a condition is met. However, on complex problems, this simple solution is
    prone to be stuck in a local optimum (meaning that the solution is only within
    a small set of candidate solutions). In this case, a more advanced genetic algorithm
    such as **NeroEvolution of Augmenting Topologies** (**NEAT**) is preferred. NEAT
    not only alters the weights of the network but also its structure.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉和变异可以采用多种不同的方式。在更简单的版本中，交叉是通过随机选择两个父母的部分进行的，变异则是通过向获得的解添加高斯噪声来实现的，噪声的标准差是固定的。通过只保留最优秀的个体并将它们的基因注入新生个体中，解会随着时间的推移不断改进，直到满足某一条件。然而，在复杂问题中，这种简单的解决方案容易陷入局部最优解（意味着解仅限于一个小的候选解集合）。在这种情况下，通常会选择更先进的遗传算法，如**增强拓扑进化网络**（**NEAT**）。NEAT不仅改变网络的权重，还会改变其结构。
- en: Evolution strategies
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进化策略
- en: '**Evolution strategies** (**ESes**) are even easier than GAs as they are primarily
    based on mutation to create a new population.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**进化策略**（**ESes**）比遗传算法（GAs）更简单，因为它们主要基于突变来创建新种群。'
- en: Mutation is performed by adding values that have been sampled from a normal
    distribution to the genotype. A very simple version of ES is obtained by just
    selecting the most performant individual across the whole population and sampling
    the next generation from a normal distribution with a fixed standard deviation
    and a mean equal to that of the best-performing individual.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 突变通过向基因型添加从正态分布中采样的值来执行。进化策略的一个非常简单的版本是通过在整个种群中选择表现最好的个体，并从具有固定标准差且均值等于最佳表现个体的正态分布中采样下一代。
- en: Outside of the sphere of small problems, using this algorithm is not recommended.
    This is because following only a single leader and using a fixed standard deviation
    could prevent potential solutions from exploring a more diverse search space.
    As a consequence, the solution to this method would probably end in a narrow local
    minimum. An immediate and better strategy would be to generate the offspring by
    combining the ![](img/09f6e608-2765-4ccf-96fa-006c8cfe4bd8.png) top performing
    candidate solutions and weighing them by their fitness rank. Ranking the individuals
    according to their fitness values is called fitness ranking. This strategy is
    preferred to using the actual fitness values as it is invariant to the transformation
    of the objective function and it prevents the new generation from moving too much
    toward a possible outlier.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在小规模问题之外，不推荐使用该算法。这是因为仅跟随一个领导者并使用固定的标准差可能会阻止潜在解探索更多样化的搜索空间。因此，该方法的解可能会陷入狭窄的局部最小值。一个直接且更好的策略是通过结合表现最好的候选解并按其适应度排名加权来生成后代。根据适应度值对个体进行排名称为适应度排名。该策略优于使用实际适应度值，因为它对目标函数的变换不变，并且能防止新一代过度偏向潜在的异常值。
- en: CMA-ES
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CMA-ES
- en: The **Covariance Matrix Adaptation Evolution Strategy**, or **CMA-ES** for short,
    is an evolutionary strategy algorithm. Unlike the simpler version of the evolution
    strategy, it samples the new candidate solution according to a multivariate normal
    distribution. The name CMA comes from the fact that the dependencies between the
    variables are kept in a covariance matrix that has been adapted to increase or
    decrease the search space on the next generation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**协方差矩阵适应进化策略**，简称**CMA-ES**，是一种进化策略算法。与更简单的进化策略版本不同，它根据多元正态分布来采样新的候选解。CMA
    这个名字来源于这样一个事实：变量之间的依赖关系被保存在协方差矩阵中，并通过适应该矩阵来增加或减少下一代的搜索空间。'
- en: Put simply, CMA-ES shrinks the search space by incrementally decreasing the
    covariance matrix in a given direction when it's confident of the space around
    it. Instead, CMA-ES increases the covariance matrix and thus enlarges the possible
    search space when it's less confident.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，当CMA-ES对周围空间有较高的信心时，它通过逐步减少协方差矩阵的值来缩小搜索空间。相反，当CMA-ES对空间的信心较低时，它增加协方差矩阵，从而扩大可能的搜索空间。
- en: ES versus RL
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进化策略与强化学习
- en: 'ESes are an interesting alternative to RL. Nonetheless, the pros and cons must
    be evaluated so that we can pick the correct approach. Let''s briefly look at
    the main advantages of ES:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 进化策略（ESes）是强化学习（RL）的一种有趣替代方法。然而，必须评估其优缺点，以便我们选择正确的方案。让我们简要了解一下进化策略的主要优势：
- en: '**Derivative-free methods**: There''s no need for backpropagation. Only the
    forward pass is performed for estimating the fitness function (or equivalently,
    the cumulative reward). This opens the door to all the non-differentiable functions,
    for example; hard attention mechanisms. Moreover, by avoiding backpropagation,
    the code gains efficiency and speed.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无导数方法**：无需反向传播。只需执行前向传递来估算适应度函数（或等效地，累计奖励）。这为所有不可微分的函数打开了大门，例如：硬注意力机制。此外，通过避免反向传播，代码的效率和速度得到了提高。'
- en: '**Very general**: The generality of ES is mainly due to its property of being
    a black-box optimization method. Because we don''t care about the agent, the actions
    that it performs, or the states visited, we can abstract these and concentrate
    only on its evaluation. Furthermore, ES allows learning without explicit targets
    and also with extremely sparse feedback. Additionally, ESes are more general in
    the sense that they can optimize a much larger set of functions.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非常通用**：进化策略的通用性主要来自于它作为黑盒优化方法的特性。由于我们不关心智能体、其执行的动作或访问的状态，我们可以将这些内容抽象化，只专注于其评估。此外，进化策略可以在没有明确目标的情况下进行学习，并且能在极度稀疏的反馈下工作。进化策略也更为通用，因为它们能够优化更多样化的函数集合。'
- en: '**Highly parallelizable and robust**: As we''ll soon see, ES is much easier
    to parallelize than RL, and the computations can be spread across thousands of
    workers. The robustness of evolution strategies is due to the few hyperparameters
    that are required to make the algorithms work. For example, in comparison to RL,
    there''s no need to specify the length of the trajectories, the lambda value,
    the discount factor, the number of frames to skip, and so on. Also, the ES is
    very attractive for tasks with a very long horizon.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高度可并行且具有鲁棒性**：正如我们即将看到的，进化策略比强化学习更容易并行化，计算可以分布到成千上万的工作节点上。进化策略的鲁棒性归功于其所需的少量超参数。例如，与强化学习相比，不需要指定轨迹的长度、lambda值、折扣因子、跳过的帧数等。另外，进化策略对于具有非常长时间跨度的任务也非常具有吸引力。'
- en: 'On the other hand, reinforcement learning is preferred for the following key
    aspects:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，强化学习在以下几个关键方面更受偏爱：
- en: '**Sample efficiency**: RL algorithms make better use of the information that''s
    acquired from the environment and as a consequence, they require less data and
    fewer steps to learn the tasks.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本效率**：强化学习算法更有效地利用从环境中获得的信息，因此它们需要更少的数据和步骤来学习任务。'
- en: '**Excellent performance**: Overall, reinforcement learning algorithms outperform
    performance evolution strategies.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卓越的性能**：总体而言，强化学习算法在性能上优于进化策略。'
- en: Scalable evolution strategies
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可扩展的进化策略
- en: Now that we've introduced black-box evolutionary algorithms and evolution strategies
    in particular, we are ready to put what we have just learned into practice. The
    paper called *Evolution Strategies as a Scalable Alternative to Reinforcement
    Learning* by OpenAI made a major contribution to the adoption of evolution strategies
    as an alternative to reinforcement learning algorithms.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了黑盒进化算法，特别是进化策略，我们准备将刚刚学到的内容付诸实践。OpenAI的论文《*进化策略作为强化学习的可扩展替代方案*》对进化策略作为强化学习算法的替代方案的采用做出了重大贡献。
- en: The main contribution of this paper is in the approach that scales ES extremely
    well with a number of CPUs. In particular, the new approach uses a novel communication
    strategy across CPUs that involves only scalars, and so it is able to scale across
    thousands of parallel workers.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的主要贡献在于一种方法，该方法能够在多个CPU上极其有效地扩展进化策略。特别地，这种新方法使用了一种跨CPU的新型通信策略，仅涉及标量，因此能够在成千上万的并行工作节点之间进行扩展。
- en: Generally, ES requires more experience and thus is less efficient than RL. However,
    by spreading the computation across so many workers (thanks to the adoption of
    this new strategy), the task can be solved in less wall clock time. As an example,
    in the paper, the authors solve the 3D Humanoid Walking pattern in just 10 minutes
    with 1,440 CPUs, with a linear speedup in the number of CPU cores. Because usual
    RL algorithms cannot reach this level of scalability, they take hours to solve
    the same task.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，进化策略（ES）需要更多的经验，因此效率低于强化学习（RL）。然而，通过将计算分布到如此多的工作节点（得益于采用这种新策略），任务可以在更少的墙钟时间内解决。例如，在论文中，作者使用1,440个CPU在仅10分钟内解决了3D人形行走模式，且CPU核心数呈线性加速。由于常规强化学习算法无法达到这种扩展性水平，它们需要几个小时才能解决相同的任务。
- en: Let's look at how they are able to scale so well.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它们是如何能够如此高效地扩展的。
- en: The core
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心
- en: 'In the paper, a version of ES is used that maximizes the average objective
    value, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，使用了一种版本的进化策略，旨在最大化平均目标值，具体如下：
- en: '![](img/ba38e6a1-fdc3-4592-afec-231c17fd721c.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba38e6a1-fdc3-4592-afec-231c17fd721c.png)'
- en: It does this by searching over a population, ![](img/539e6611-0c4e-4075-9e74-35cce86c0178.png),
    that's parameterized by ![](img/3a36c710-4434-455b-8b98-03c3151da93e.png) with
    stochastic gradient ascent. ![](img/051c0918-d493-43e6-a1d4-9de7f947e548.png) is
    the objective function (or fitness function) while ![](img/4d706f8e-4f4f-49a4-975c-6f02832c082f.png) is the
    parameters of the actor. In our problems, ![](img/3bc547ab-803d-49b4-ac20-452762b6b5e6.png) is
    simply the stochastic return that's obtained by the agent with ![](img/0f404f12-ce92-410d-9d06-404c260e2f6a.png) in
    the environment.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过在种群中搜索，![](img/539e6611-0c4e-4075-9e74-35cce86c0178.png)，并通过![](img/3a36c710-4434-455b-8b98-03c3151da93e.png)进行随机梯度上升。![](img/051c0918-d493-43e6-a1d4-9de7f947e548.png)是目标函数（或适应度函数），而![](img/4d706f8e-4f4f-49a4-975c-6f02832c082f.png)是演员的参数。在我们的问题中，![](img/3bc547ab-803d-49b4-ac20-452762b6b5e6.png)仅仅是代理通过![](img/0f404f12-ce92-410d-9d06-404c260e2f6a.png)在环境中获得的随机回报。
- en: 'The population distribution, ![](img/add68569-90f7-4ffd-bf1c-f2310e74ac9f.png),is
    a multivariate Gaussian with a mean, ![](img/c7584f6b-9665-483c-9396-b5f63d319342.png), and
    fixed standard deviation, ![](img/9585cbc6-5654-4b6b-bb0c-14c85f7ae8c2.png), as
    follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 种群分布，![](img/add68569-90f7-4ffd-bf1c-f2310e74ac9f.png)，是一个多元高斯分布，均值为！[](img/c7584f6b-9665-483c-9396-b5f63d319342.png)，标准差为！[](img/9585cbc6-5654-4b6b-bb0c-14c85f7ae8c2.png)，如图所示：
- en: '![](img/6ac4b3a4-7bcc-4497-be9a-739357c40167.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ac4b3a4-7bcc-4497-be9a-739357c40167.png)'
- en: 'From here, we can define the step update by using the stochastic gradient estimate,
    as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们可以通过使用随机梯度估算来定义步长更新，如下所示：
- en: '![](img/f4bc3a83-dad8-43d6-bc80-3e5846b68fd5.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4bc3a83-dad8-43d6-bc80-3e5846b68fd5.png)'
- en: With this update, we can estimate the stochastic gradient (without performing
    backpropagation) using the results of the episodes from the population. We can
    update the parameters using one of the well-known update methods, such as Adam
    or RMSProp as well.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个更新，我们可以使用来自种群的样本结果估算随机梯度（无需执行反向传播）。我们也可以使用一些著名的更新方法，比如Adam或RMSProp，来更新参数。
- en: Parallelizing ES
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行化ES
- en: 'It''s easy to see how ES can be scaled across multiple CPUs: each worker is
    assigned to a separate candidate solution of the population. The evaluation can
    be done in complete autonomy, and as described in the paper, optimization can
    be done in parallel on each worker, with only a few scalars shared between each
    CPU unit.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 很容易看出，如何将ES扩展到多个CPU：每个工作者被分配到种群的一个独立候选解上。评估可以完全自主地进行，正如论文中所描述的，优化可以在每个工作者上并行进行，且每个CPU单元之间只共享少量标量。
- en: Specifically, the only information that's shared between workers is the scalar
    return, ![](img/42692633-6bcf-401b-bded-d2cb8a1dabb8.png), of an episode and the
    random seed that has been used to sample ![](img/f805a18c-a321-40fd-926a-c72d7f12f46e.png).
    The amount of data can be further shrunk by sending only the return, but in this
    case, the random seed of each worker has to be synchronized with all the others.
    We decided to adopt the first technique, while the paper used the second one.
    In our simple implementation, the difference is negligible and both techniques
    require extremely low bandwidth.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，工作者之间唯一共享的信息是一个回合的标量回报，![](img/42692633-6bcf-401b-bded-d2cb8a1dabb8.png)，以及用于采样的随机种子！[](img/f805a18c-a321-40fd-926a-c72d7f12f46e.png)。通过仅传输回报，我们可以进一步缩减数据量，但在这种情况下，每个工作者的随机种子必须与其他工作者同步。我们决定采用第一种技术，而论文使用的是第二种。在我们简单的实现中，差异可以忽略不计，且两种技术都需要极低的带宽。
- en: Other tricks
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他技巧
- en: 'Two more techniques are used to improve the performance of the algorithm:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另外有两种技术被用来提高算法的性能：
- en: '**Fitness shaping** – **objective ranking**: We discussed this technique previously.
    It''s very simple. Instead of using the raw returns to compute the update, a rank
    transformation is used. The rank is invariant to the transformation of the objective
    function and thus performs better with spread returns. Additionally, it removes
    the noise of the outliers.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**健身塑形** – **目标排名**：我们之前讨论过这个技巧。它非常简单。与其直接使用原始回报来计算更新，不如使用排名变换。排名对于目标函数的变换是不可变的，因此在回报差异较大的情况下表现更好。此外，它还去除了异常值的噪声。'
- en: '**Mirror noise**: This trick reduces the variance and involves the evaluation
    of the network with both noise ![](img/c238b7ca-715c-4fe0-add9-7c75cc76cbd5.png) and ![](img/5fd782b2-9cad-48cc-959d-0ccafd86ee28.png); that
    is, for each individual, we''ll have two mutations: ![](img/8374e454-8f2f-42b4-8157-0055e8b86858.png) and ![](img/9acbcfa7-181f-4d62-82a0-3399abb61d07.png).'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**镜像噪声**：这个技巧减少了方差，并且涉及到同时评估带有噪声的网络 ![](img/c238b7ca-715c-4fe0-add9-7c75cc76cbd5.png) 和 ![](img/5fd782b2-9cad-48cc-959d-0ccafd86ee28.png)；也就是说，对于每个个体，我们会有两种变异：![](img/8374e454-8f2f-42b4-8157-0055e8b86858.png) 和 ![](img/9acbcfa7-181f-4d62-82a0-3399abb61d07.png)。'
- en: Pseudocode
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伪代码
- en: 'The parallelized evolution strategy that combines all of these features is
    summarized in the following pseudocode:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 结合所有这些特性的并行化进化策略总结如下伪代码：
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, all that remains is to implement this algorithm.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，剩下的就是实现这个算法了。
- en: Scalable implementation
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可扩展的实现
- en: To simplify the implementation and to make the parallelized version of ES work
    well with a limited number of workers (and CPUs), we will develop a structure
    similar to the one that's shown in the following diagram. The main process creates
    one worker for each CPU core and executes the main cycle. On each iteration, it
    waits until a given number of new candidates are evaluated by the workers. Different
    from the implementation provided in the paper, each worker evaluates more than
    one agent on each iteration. So, if we have four CPUs, four workers will be created.
    Then, if we want a total batch size bigger than the number of workers on each
    iteration of the main process, let's say, 40, each worker will create and evaluate
    10 individuals each time. The return values and seeds are returned to the main
    application, which waits for results from all 40 individuals, before continuing
    with the following lines of code.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化实现，并使得并行化版本的ES能够在有限的工作者（和CPU）数量下运行良好，我们将开发一个类似于以下图示的结构。主进程为每个CPU核心创建一个工作者，并执行主循环。在每次迭代时，它会等待直到指定数量的新候选个体被工作者评估。与论文中提供的实现不同，每个工作者在每次迭代中会评估多个代理。因此，如果我们有四个CPU，则会创建四个工作者。然后，如果我们希望在每次主进程迭代时总批量大小大于工作者的数量，比如40，则每个工作者每次会创建并评估10个个体。返回值和种子会被返回给主应用程序，主应用程序会等待所有40个个体的结果，然后继续执行后续代码行。
- en: 'Then, these results are propagated in a batch to all the workers, which optimize
    the neural network seperately, following the update provided in the formula (11.2):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这些结果会以批量的形式传播给所有工作者，工作者分别优化神经网络，按照公式 (11.2) 中提供的更新进行操作：
- en: '![](img/fde043f4-e707-4c42-98ec-7bb6cc7d4855.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fde043f4-e707-4c42-98ec-7bb6cc7d4855.png)'
- en: Figure 11.4\. Diagram showing the main components involved in the parallel version
    of ES
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4\. 展示ES并行版本主要组件的示意图
- en: 'Following what we just described, the code is divided into three main buckets:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们刚才描述的，代码被分为三个主要部分：
- en: The main process that creates and manages the queues and the workers.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建并管理队列和工作者的主要过程。
- en: A function that defines the task of the workers.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义工作者任务的函数。
- en: Additionally, there are some functions that perform simple tasks, such as ranking
    the returns and evaluating the agent.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，还有一些执行简单任务的函数，例如对回报进行排序和评估代理。
- en: Let's explain the code of the main process so that you have a broad view of
    the algorithm before going into detail about the workers.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释一下主进程的代码，以便在深入了解工作者之前，您能够对整个算法有一个大致的了解。
- en: The main function
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主函数
- en: 'This is defined in a function called `ES` that has the following arguments:
    the name of the Gym environment, the size of the neural network''s hidden layers,
    the total number of generations, the number of workers, the Adam learning rate,
    the batch size, and the standard deviation noise:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在名为`ES`的函数中定义的，该函数具有以下参数：Gym环境的名称，神经网络隐藏层的大小，代数总数，工作者数量，Adam学习率，批量大小和标准差噪声：
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we set an initial seed that is shared among the workers to initialize
    the parameters with the same weights. Moreover, we calculate the number of individuals
    that a worker has to generate and evaluate on each iteration and create two `multiprocessing.Queue`
    queues. These queues are the entry and exit points for the variables that are
    passed to and from the workers:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们设置一个初始种子，在所有工作者之间共享，以初始化参数并使权重保持一致。此外，我们计算每个工作者在每次迭代中需要生成并评估的个体数量，并创建两个
    `multiprocessing.Queue` 队列。这些队列是传递给工作者以及从工作者传回的变量的进出口：
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, the multiprocessing processes, `multiprocessing.Process`, are instantiated.
    These will run the `worker` function, which is given as the first argument to
    the `Process` constructor in an asynchronous way. All the other variables that
    are passed to the `worker` function are assigned to `args` and are pretty much
    the same as the parameters taken by ES, with the addition of the two queues. The
    processes start running when the `start()` method is called:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，实例化多进程`multiprocessing.Process`。这些进程将以异步方式运行`worker`函数，该函数作为第一个参数传递给`Process`构造函数。传递给`worker`函数的所有其他变量被分配到`args`，这些变量与ES所接受的参数非常相似，唯一不同的是额外的两个队列。进程在调用`start()`方法时开始运行：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Once the parallel workers have started, we can iterate across the generations
    and wait until all the individuals have been generated and evaluated separately
    in each worker. Remember that the total number of individuals that are created
    on every generation is the number of workers, `num_workers`, multiplied by the
    individuals generated on each worker, `indiv_per_worker`. This architecture is
    unique to our implementation as we have only four CPU cores available, compared
    to the implementation in the paper, which benefits from thousands of CPUs. Generally,
    the population that''s created on every generation is usually between 20 and 1,000:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦并行工作者启动，我们就可以跨代进行迭代，直到所有个体在每个工作者中被生成并单独评估。请记住，每一代生成的个体总数是工作者数量`num_workers`与每个工作者生成的个体数`indiv_per_worker`的乘积。这种架构是我们实现的独特之处，因为我们只有四个CPU核心，而论文中的实现则利用了成千上万的CPU。通常，每一代生成的人口数量通常在20到1000之间：
- en: '[PRE5]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the previous snippet, `output_queue.get()` gets an element from `output_queue`,
    which is populated by the workers. In our implementation, `output_queue.get()`
    returns two elements. The first element, `p_rews`, is the fitness value (the return
    value) of the agent that's generated using `p_seed`, which is given as the second
    element.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，`output_queue.get()`从`output_queue`获取一个元素，该队列由工作者填充。在我们的实现中，`output_queue.get()`返回两个元素。第一个元素，`p_rews`，是使用`p_seed`生成的智能体的适应度值（即返回值），它作为第二个元素给出。
- en: 'When the `for` cycle terminates, we rank the returns and put the batch returns
    and seeds on the `params_queue` queue, which will be read by all the workers to
    optimize the agent. The code for this is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当`for`循环终止时，我们对返回值进行排序，并将批量返回值和种子放入`params_queue`队列中，所有的工作者将读取该队列来优化智能体。代码如下所示：
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, when all the training iterations have been executed, we can terminate
    the workers:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当所有训练迭代执行完毕时，我们可以终止工作者：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This concludes the main function. Now, all we need to do is implement the workers.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了主函数。现在，我们需要做的就是实现工作者。
- en: Workers
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作者
- en: The workers' functionalities are defined in the `worker` function, which was
    previously passed as an argument to `mp.Process`. We cannot go through all the
    code because it'd take too much time and space to explain, but we'll explain the
    core components here. As always, the full implementation is available in this
    book's repository on GitHub. So, if you are interested in looking at it in more
    depth, take the time to examine the code on GitHub.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 工作者的功能在`worker`函数中定义，之前已作为参数传递给`mp.Process`。我们不能逐行讲解所有代码，因为那会占用太多时间和篇幅，但我们会在这里解释核心部分。和往常一样，完整的实现代码可以在本书的GitHub仓库中找到。所以，如果你有兴趣深入了解，可以抽时间查看GitHub上的代码。
- en: In the first few lines of `worker`, the computational graph is created to run
    the policy and optimize it. Specifically, the policy is a multi-layer perceptron
    with `tanh` nonlinearities as the activation function. In this case, Adam is used
    to apply the expected gradient that's computed following the second term of (11.2).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在`worker`的前几行中，创建了计算图以运行策略并优化它。具体来说，策略是一个多层感知机，`tanh`非线性激活函数用于激活层。在这种情况下，Adam被用来应用根据(11.2)公式第二项计算出的预期梯度。
- en: Then, `agent_op(o)` and `evaluation_on_noise(noise)` are defined. The former
    runs the policy (or candidate solution) to obtain the action for a given state
    or observation, `o`, and the latter evaluates the new candidate solution that
    is obtained by adding the perturbation `noise` (that has the same shape as the
    policy) to the current policy's parameters.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`agent_op(o)`和`evaluation_on_noise(noise)`被定义。前者运行策略（或候选解）以获得给定状态或观测`o`的行动，后者评估通过将扰动`noise`（与策略形状相同）添加到当前策略的参数中获得的新候选解。
- en: 'Jumping directly to the most interesting part, we create a new session by specifying
    that it can rely on, at most, 4 CPUs and initialize the global variables. Don''t
    worry if you don''t have 4 CPUs available. Setting `allow_soft_placement` to `True`
    tells TensorFlow to use only the supported devices:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 直接跳到最有趣的部分，我们通过指定最多可以依赖4个CPU来创建一个新的会话，并初始化全局变量。如果你没有4个CPU可用，也不用担心。将`allow_soft_placement`设置为`True`，会告诉TensorFlow只使用受支持的设备：
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Despite using all 4 CPUs, we allocate only one to each worker. In the definition
    of the computational graph, we set the device on which the computation will be
    performed. For example, to specify that the worker has to use only CPU 0, you
    can put the graph inside a `with` statement, which defines the device to use:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用了所有4个CPU，但我们仅为每个工作线程分配一个。在计算图的定义中，我们设置了计算将在哪个设备上执行。例如，要指定工作线程仅使用CPU 0，可以将图形放在`with`语句中，从而定义要使用的设备：
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Going back to our implementation, we can loop forever, or at least until the
    worker has something to do. This condition is checked later, inside the `while`
    cycle.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的实现，我们可以无限循环，或者至少直到工作线程有任务可以处理。这个条件稍后会在`while`循环内进行检查。
- en: An important thing to note is that because we perform many calculations on the
    weights of the neural network, it is much easier to deal with flattened weights.
    So, for example, instead of dealing with a list of the form [8,32,32,4], we'll
    perform computations on a one-dimensional array of length 8*32*32*4\. The functions
    that perform the conversion from the former to the latter, and vice versa, are
    defined in TensorFlow (take a look at the full implementation on GitHub if you
    are interested in knowing how this is done).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的注意事项是，因为我们在神经网络的权重上进行了很多计算，所以处理*扁平化*的权重要容易得多。因此，例如，我们不会处理形如[8,32,32,4]的列表，而是对一个长度为8*32*32*4的一维数组进行计算。执行从前者到后者的转换以及反向转换的函数在TensorFlow中已经定义（如果你有兴趣了解是如何实现的，可以查看GitHub上的完整实现）。
- en: 'Also, before starting the `while` loop, we retrieve the shape of the flattened agent:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在开始`while`循环之前，我们检索*扁平化*代理的形状：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the first part of the `while` loop, the candidates are generated and evaluated. The
    candidate solutions are built by adding a normal perturbation to the weights;
    that is, ![](img/35368897-b2dd-4f34-9776-f797818d1a56.png). This is done by choosing
    a new random seed every time, which will uniquely sample the perturbation (or
    noise), ![](img/acc67400-71b8-4b04-8fef-43469154d93a.png), from a normal distribution.
    This is a key part of the algorithm because, later, the other workers will have
    to regenerate the same perturbation from the same seed. After that, the two new
    offspring (there are two because we are using mirror sampling) are evaluated and
    the results are put in the `output_queue` queue:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在`while`循环的第一部分，生成并评估候选解。候选解是通过向权重添加正态扰动构建的；也就是说，![](img/35368897-b2dd-4f34-9776-f797818d1a56.png)。这是通过每次选择一个新的随机种子来完成的，该种子将唯一地从正态分布中采样扰动（或噪声），![](img/acc67400-71b8-4b04-8fef-43469154d93a.png)。这是算法的关键部分，因为稍后，其他工作线程将必须从相同的种子重新生成相同的扰动。之后，两个新后代（由于我们使用的是镜像采样，所以有两个）将被评估，并将结果放入`output_queue`队列中：
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Note that the following snippet (which we used previously), is just a way to
    set the NumPy random seed, `seed`, locally:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，以下代码片段（我们之前使用过的）只是用于在本地设置NumPy随机种子`seed`的一种方式：
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Outside the `with` statement, the seed that's used to generate random values
    will not be `seed` anymore.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在`with`语句外，生成随机值所用的种子将不再是`seed`。
- en: 'The second part of the `while` loop involves the acquisition of all the returns
    and seeds, the reconstruction of the perturbations from those seeds, the computation
    of the stochastic gradient estimate following the formula (11.2), and the policy''s
    optimization. The `params_queue` queue is populated by the main process, which
    we saw earlier. It does this by sending the normalized ranks and seeds of the
    population that were generated by the workers in the first phase. The code is
    as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`while`循环的第二部分涉及获取所有返回值和种子，从这些种子中重建扰动，按照公式（11.2）计算随机梯度估计，并优化策略。`params_queue`队列由主进程填充，正如我们之前看到的那样。它通过发送在第一阶段由工作线程生成的种群的归一化排名和种子来完成此操作。代码如下：'
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The last few lines in the preceding code compute the gradient estimate; that
    is, they calculate the second term of formula (11.2):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中的最后几行计算了梯度估计；也就是说，它们计算了公式（11.2）中的第二项：
- en: '![](img/abe6a754-4493-48b2-a6a4-789975a1a0c0.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/abe6a754-4493-48b2-a6a4-789975a1a0c0.png)'
- en: Here, ![](img/6fed24d8-5e14-41bd-809e-e74e3ba4eee5.png) is the normalized rank
    of ![](img/5307ef79-2210-4c5d-96a4-4cbc4d08cb52.png) and ![](img/ae2228ad-b5e8-4148-8a27-d20fd14fd478.png) candidates
    their perturbation.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/6fed24d8-5e14-41bd-809e-e74e3ba4eee5.png) 是![](img/5307ef79-2210-4c5d-96a4-4cbc4d08cb52.png) 和![](img/ae2228ad-b5e8-4148-8a27-d20fd14fd478.png) 候选项的归一化排名以及它们的扰动。
- en: '`apply_g` is the operation that applies the `vars_grads` gradient (11.3) using
    Adam. Note that we pass `-var_grads` as we want to perform gradient ascent and
    not gradient descent.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply_g` 是应用 `vars_grads` 梯度（11.3）使用 Adam 的操作。请注意，我们传递 `-var_grads`，因为我们希望进行梯度上升而不是梯度下降。'
- en: That's all for the implementation. Now, we have to apply it to an environment
    and test it to see how it performs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 实施已完成。现在，我们必须将其应用于环境并进行测试，以查看其表现如何。
- en: Applying scalable ES to LunarLander
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用可扩展 ES 到 LunarLander
- en: How well will the scalable version of evolution strategies perform in the LunarLander
    environment? Let's find out!
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展演化策略在 LunarLander 环境中的表现如何？让我们来看看吧！
- en: As you may recall, we already used LunarLander against A2C and REINFORCE in
    [Chapter 6](6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml), *Learning Stochastic
    and PG optimization*. This task consists of landing a lander on the moon through
    continuous actions. We decided to use this environment for its medium difficulty
    and to compare the ES results to those that were obtained with A2C.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还记得，我们已经在[第6章](6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml)中对比了 LunarLander
    与 A2C 和 REINFORCE，*学习随机和PG优化*。这个任务包括通过连续动作在月球上着陆。我们决定使用这个中等难度的环境来比较 ES 的结果与使用
    A2C 获得的结果。
- en: 'The hyperparameters that performed the best in this environment are as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中表现最佳的超参数如下：
- en: '| **Hyperparameter** | **Variable name** | **Value** |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| **超参数** | **变量名** | **值** |'
- en: '| Neural network size | `hidden_sizes` | [32, 32] |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 神经网络大小 | `hidden_sizes` | [32, 32] |'
- en: '| Training iterations (or generations) | `number_iter` | 200 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 训练迭代次数（或代数） | `number_iter` | 200 |'
- en: '| Worker''s number | `num_workers` | 4 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 工作人数 | `num_workers` | 4 |'
- en: '| Adam learning rate | `lr` | 0.02 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Adam 学习率 | `lr` | 0.02 |'
- en: '| Individuals per worker | `indiv_per_worker` | 12 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 每个工作人员的个体数 | `indiv_per_worker` | 12 |'
- en: '| Standard deviation | `std_noise` | 0.05 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 标准差 | `std_noise` | 0.05 |'
- en: The results are shown in the following graph. What immediately catches your
    eye is that the curve is very stable and smooth. Furthermore, notice that it reaches
    an average score of about 200 after 2.5-3 million steps. Comparing the results
    with those obtained with A2C (in Figure 6.7), you can see that the evolution strategy
    took almost 2-3 times more steps than A2C and REINFORCE.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在以下图表中。立即引人注目的是曲线非常稳定平滑。此外，请注意，在约250-300万步后，它达到了平均分数约为200。与在图6.7中使用A2C获得的结果进行比较，您会发现演化策略比A2C和REINFORCE多花了近2-3倍的步骤。
- en: 'As demonstrated in the paper, by using massive parallelization (using at least
    hundreds of CPUs), you should be able to obtain very good policies in just minutes.
    Unfortunately, we don''t have such computational power. However, if you do, you
    may want to try it for yourself:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如论文所示，通过大规模并行化（至少使用数百个CPU），您应该能够在短短几分钟内获得非常好的策略。不幸的是，我们没有这样的计算能力。但是，如果您有的话，可以自行尝试：
- en: '![](img/9ee4c3a1-b67b-4d59-918d-73e3d587e9f6.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ee4c3a1-b67b-4d59-918d-73e3d587e9f6.png)'
- en: Figure 11.5 The performance of scalable evolution strategies
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 可扩展演化策略的性能
- en: Overall, the results are great and show that ES is a viable solution for very
    long horizon problems and tasks with very sparse rewards.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，结果非常好，显示出 ES 对于非常长的时间跨度问题和具有非常稀疏奖励的任务是一个可行的解决方案。
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about EAs, a new class of black-box algorithms
    inspired by biological evolution that can be applied to RL tasks. EAs solve these
    problems from a different perspective compared to reinforcement learning. You
    saw that many characteristics that we have to deal with when we design RL algorithms
    are not valid in evolutionary methods. The differences are in both the intrinsic
    optimization method and the underlying assumptions. For example, because EAs are
    black-box algorithms, we can optimize whatever function we want as we are no longer
    constrained to using differentiable functions, like we were with RL. EAs have
    many other advantages, as we saw throughout this chapter, but they also have numerous
    downsides.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了进化算法（EAs），这是一类受生物进化启发的新型黑箱算法，可以应用于强化学习任务。与强化学习相比，进化算法从不同的角度解决这些问题。你看到，在设计强化学习算法时需要处理的许多特性，在进化方法中并不适用。它们的不同之处既体现在内在的优化方法上，也体现在基本假设上。例如，由于进化算法是黑箱算法，我们可以优化任何我们想要的函数，因为我们不再受限于使用可微分函数，这一点与强化学习时有所不同。正如我们在本章中看到的，进化算法有许多其他优点，但它们也有许多缺点。
- en: 'Next, we looked at two evolutionary algorithms: genetic algorithms and evolution
    strategies. Genetic algorithms are more complex as they create offspring from
    two parents through crossover and mutation. Evolution strategies select the best-performing
    individuals from a population that has been created only by mutation from the
    previous generation. The simplicity of ES is one of the key elements that enables
    the immense scalability of the algorithm across thousands of parallel workers.
    This scalability has been demonstrated in the paper by OpenAI, showing the ability
    of ES to perform at the levels of RL algorithms in complex environments.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们研究了两种进化算法：遗传算法和进化策略。遗传算法更复杂，因为它通过交叉和变异从两个父母创建后代。进化策略从一个仅通过变异生成的种群中选择表现最好的个体。ES的简单性是算法能够在成千上万个并行工作者之间实现巨大可扩展性的关键因素之一。OpenAI的论文中展示了这种可扩展性，证明了ES能够在复杂环境中达到与RL算法相当的表现。
- en: 'To get hands-on with evolutionary algorithms, we implemented the scalable evolution
    strategy from the paper we cited throughout this chapter. Furthermore, we tested
    it on LunarLander and saw that ES is able to solve the environment with high performance.
    Though the results are great, ES used two to three times more steps than AC and
    REINFORCE to learn the task. This is the main drawback of ESes: they need a lot
    of experience. Despite this, thanks to their capacity to scale linearly to the
    number of workers, with enough computational power, you might be able to solve
    this task in a fraction of the time compared to reinforcement learning algorithms.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实践进化算法，我们实现了本章引用的论文中的可扩展进化策略。此外，我们还在LunarLander上进行了测试，发现ES能够以高性能解决该环境。尽管结果很棒，ES的学习任务比AC和REINFORCE多用了两到三倍的步骤。这是ES的主要缺点：它们需要大量的经验。尽管如此，凭借其线性扩展到工作者数量的能力，在足够的计算能力支持下，你可能能够在比强化学习算法短得多的时间内解决这个任务。
- en: In the next chapter, we'll go back to reinforcement learning and talk about
    a problem known as the exploration-exploitation dilemma. We'll see what it is
    and why it's crucial in online settings. Then, we'll use a potential solution
    to the problem to develop a meta-algorithm called ESBAS, which chooses the most
    appropriate algorithm for each situation.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将回到强化学习，讨论一个被称为探索-利用困境的问题。我们将了解它是什么以及为什么它在在线设置中至关重要。接着，我们将使用一个潜在的解决方案来开发一个元算法，称为ESBAS，它为每种情况选择最合适的算法。
- en: Questions
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are two alternative algorithms to reinforcement learning for solving sequential
    decision-making problems?
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪两种替代强化学习的算法可以解决序列决策问题？
- en: What are the processes that give birth to new individuals in evolutionary algorithms?
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进化算法中产生新个体的过程是什么？
- en: What is the source of inspiration for evolutionary algorithms such as genetic
    algorithms?
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进化算法（如遗传算法）的灵感来源是什么？
- en: How does CMA-ES evolve evolution strategies?
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CMA-ES如何演化进化策略？
- en: What's one advantage and one disadvantage of evolution strategies?
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进化策略的一个优点和一个缺点是什么？
- en: What's the trick that's used in the *Evolution Strategies as a Scalable Alternative
    to Reinforcement Learning* paper to reduce the variance?
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在论文《*进化策略作为强化学习的可扩展替代方法*》中，使用了什么技巧来减少方差？
- en: Further reading
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: To read the original paper of OpenAI that proposed the scalable version of ES,
    that is, the *Evolution Strategies as a Scalable Alternative to Reinforcement
    Learning* paper, go to [https://arxiv.org/pdf/1703.03864.pdf](https://arxiv.org/pdf/1703.03864.pdf).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要阅读OpenAI提出的可扩展版本ES的原始论文，即*Evolution Strategies as a Scalable Alternative to
    Reinforcement Learning*论文，请访问[https://arxiv.org/pdf/1703.03864.pdf](https://arxiv.org/pdf/1703.03864.pdf)。
- en: To read the paper that presented NEAT, that is, *Evolving Neural Networks through*
    *Augmenting Topologies*, go to [http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要阅读提出NEAT的论文，即*通过增强拓扑演化神经网络*，请访问[http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)。
