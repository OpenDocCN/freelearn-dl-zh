- en: PyTorch Experiments on NLP and RNN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 在 NLP 和 RNN 上的实验
- en: In this chapter, we are going to deep dive into the PyTorch library on **natural
    language processing** (**NLP**) and other experiments. Then, we will convert the
    developed model into a format that can be used in an Android or iOS application
    using TensorFlow and CoreML.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入研究 PyTorch 库在 **自然语言处理**（**NLP**）和其他实验中的应用。然后，我们将把开发的模型转换为可以在 Android
    或 iOS 应用中使用的格式，使用 TensorFlow 和 CoreML。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主题：
- en: Introduction to PyTorch features and installation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 特性和安装简介
- en: Using variables in PyTorch
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 PyTorch 中使用变量
- en: Building our own model network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建我们自己的模型网络
- en: Classifying **recurrent neural networks** (**RNN**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类 **递归神经网络**（**RNN**）
- en: Natural language processing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: PyTorch
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch
- en: 'PyTorch is a Python-based library that''s used to perform scientific computing
    operations with GPUs. It helps by performing faster experimentation to run production-grade
    ecosystems and distribute the training of libraries. It also provides two high-level
    features: tensor computations and building neural networks on tape-based autograd
    systems.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是一个基于 Python 的库，用于执行与 GPU 相关的科学计算操作。它通过加速实验来帮助运行生产级生态系统并分布式训练库。它还提供了两个高级特性：张量计算和基于磁带的自动求导系统构建神经网络。
- en: The features of PyTorch
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 的特性
- en: 'PyTorch provides an end-to-end deep learning system. Its features are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 提供了一个端到端的深度学习系统。它的特点如下：
- en: '**Python utilization**: PyTorch is not simply a Python binding to a C++ framework.
    It is deeply integrated in Python so that it can be used with other popular libraries
    and frameworks.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python 使用**：PyTorch 不仅仅是 C++ 框架的 Python 绑定。它深度集成于 Python，因此可以与其他流行的库和框架一起使用。'
- en: '**Tools and libraries**: It has an active community of researchers and developers
    in the areas of computer vision and reinforcement learning.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具和库**：它在计算机视觉和强化学习领域拥有一个活跃的研究人员和开发者社区。'
- en: '**Flexible frontend**: This includes ease of use and hybrid in eager mode,
    accelerate speeds and seamless transitions to graph mode, and functionality and
    optimization in C++ runtime.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活的前端**：包括易于使用的混合模式支持，在急切模式下加速速度并实现无缝切换到图模式，以及在 C++ 运行时的功能性和优化。'
- en: '**Cloud support**: This is supported on all of the major cloud platforms, allowing
    for seamless development and scaling with prebuilt images, so that it is able
    to run as a production-grade application.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云支持**：支持所有主要的云平台，允许使用预构建的镜像进行无缝开发和扩展，以便能够作为生产级应用运行。'
- en: '**Distributed training**: This includes performance optimization with the advantage
    of native support for the asynchronous execution of operations and peer-to-peer
    (p2p) communications, so that we can access both C++ and Python.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式训练**：包括通过原生支持异步执行操作和点对点（p2p）通信来优化性能，这样我们可以同时访问 C++ 和 Python。'
- en: '**Native support for ONNX**: We can export models into the standard **Open
    Neural Network Exchange** (**ONNX**) format for access to other platforms, runtimes,
    and visualizers.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原生支持 ONNX**：我们可以将模型导出为标准的 **Open Neural Network Exchange** (**ONNX**) 格式，以便在其他平台、运行时和可视化工具中访问。'
- en: Installing PyTorch
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 PyTorch
- en: 'There is a stable version of PyTorch available at the time of writing this
    book, that is, 1.0\. There is also a nightly preview build available if you want
    to have a hands-on look at the latest code repository. You need to have the dependencies
    installed based on your package manager. **Anaconda** is the recommended package
    manager, and it installs all the dependencies automatically. **LibTorch** is only
    available for C++. Here is a grid showing the installation options that are available
    for installing PyTorch:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写本书时，有一个稳定版本的 PyTorch 可用，即 1.0。如果你想亲自体验最新的代码库，还可以选择使用每日预览构建版。你需要根据你的包管理器安装相应的依赖项。**Anaconda**
    是推荐的包管理器，它会自动安装所有依赖项。**LibTorch** 仅适用于 C++。以下是安装 PyTorch 时可用的安装选项网格：
- en: '![](img/17ee8f45-5596-405d-9255-31d5f2fc0fd4.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17ee8f45-5596-405d-9255-31d5f2fc0fd4.png)'
- en: The preceding screenshot specifies the package grid that was used while this
    book was being written. You can pick any package grid as per your hardware configuration
    availability.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的截图指定了在编写本书时使用的包网格。你可以根据硬件配置的可用性选择任何一个包网格。
- en: 'To install PyTorch, and to start Jupyter Notebook, run the following command:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 PyTorch 并启动 Jupyter Notebook，请运行以下命令：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The installation of PyTorch is shown in the following screenshot:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 的安装过程如下图所示：
- en: '![](img/c6d1e025-b6a4-4a84-b77c-fc04da37914e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6d1e025-b6a4-4a84-b77c-fc04da37914e.png)'
- en: 'When you initiate the Jupyter Notebook, a new browser session opens up with
    an empty notebook, as shown here:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当你启动 Jupyter Notebook 时，一个新的浏览器会话会打开，显示一个空白的笔记本，如下所示：
- en: '![](img/63b602b7-9b98-4f82-b22f-ed7fe087acb3.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63b602b7-9b98-4f82-b22f-ed7fe087acb3.png)'
- en: Let's look at the basics of PyTorch.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解一下 PyTorch 的基础。
- en: PyTorch basics
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PyTorch 基础
- en: Now that PyTorch has been installed, we can start experimenting with it. We
    will start with `torch` and `numpy`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 PyTorch 已经安装完成，我们可以开始实验了。我们将从 `torch` 和 `numpy` 开始。
- en: 'From the top menu, create a new notebook and include the following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从顶部菜单创建一个新的笔记本，并包含以下代码：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let''s do some mathematical operations:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行一些数学运算：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s calculate the mean method and print the results:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算均值方法并打印结果：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following code shows the output of the mathematical operations:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了数学运算的输出：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, let's look at how to use different variables in PyTorch.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看如何在 PyTorch 中使用不同的变量。
- en: Using variables in PyTorch
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 PyTorch 中使用变量
- en: 'Variables in `torch` are used to build a computational graph. Whenever a variable
    is calculated, it builds a computational graph. This computational graph is used
    to connect all the calculation steps (nodes), and finally, when the error is reversed,
    the modification range (gradient) in all the variables is calculated at once.
    In comparison, `tensor` does not have this ability. We will look into this difference
    with a simple example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch` 中的变量用于构建计算图。每当一个变量被计算时，它都会构建一个计算图。这个计算图用于连接所有的计算步骤（节点），最终当误差反向传播时，会同时计算所有变量的修改范围（梯度）。相比之下，`tensor`
    并不具备这种能力。我们将通过一个简单的例子来探讨这种差异：'
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we will be printing the results for all parameters:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将打印所有参数的结果：
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the output of the preceding code block:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码块的输出：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, let's try plotting data on a graph using `matplotlib`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试使用 `matplotlib` 在图表上绘制数据。
- en: Plotting values on a graph
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在图表上绘制值
- en: 'Let''s work on one simple program to plot values on a graph. To do this, use
    the following code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个简单的程序，将值绘制在图表上。为此，使用以下代码：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Following code block lists down a few of the activation methods:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块列出了一些激活方法：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Using `matplotlib` to activate the functions:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `matplotlib` 激活函数：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s plot the values on the graph, as shown in the following screenshot:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在图表上绘制这些值，如下所示：
- en: '![](img/8b0e9e83-cf7f-4146-b058-5e9f304ce5e4.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b0e9e83-cf7f-4146-b058-5e9f304ce5e4.png)'
- en: Note that the first line in the preceding code is required to draw the graph
    inside Jupyter Notebook. If you are running the Python file directly from the
    Terminal, you can omit the first line of the code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面代码的第一行是必须的，用于在 Jupyter Notebook 中绘制图表。如果你是直接从终端运行 Python 文件，可以省略代码的第一行。
- en: Building our own model network
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建我们自己的模型网络
- en: In this section, we will work on building our own network using PyTorch with
    a step-by-step example.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将通过一步步的示例使用 PyTorch 构建我们自己的网络。
- en: Let's begin by looking at linear regression as a starting point.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从线性回归开始，作为起点。
- en: Linear regression
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: Linear regression is probably the first method that anyone will learn in terms
    of machine learning. The objective of linear regression is to find a relationship
    between one or more features (independent variables) and a continuous target variable
    (the dependent variable), which can be seen in the following code.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归可能是任何人在学习机器学习时接触的第一个方法。线性回归的目标是找到一个或多个特征（自变量）与一个连续的目标变量（因变量）之间的关系，这可以在以下代码中看到。
- en: 'Import all the necessary libraries and declare all the necessary variables:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所有必要的库并声明所有必要的变量：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will define the linear regression class and run a simple `nn` to explain
    regression:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义线性回归类，并运行一个简单的 `nn` 来解释回归：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we will see how to plot the graphs and display the process of learning:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到如何绘制图表并展示学习过程：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s plot the output of this code on the graph, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这段代码的输出绘制到图表上，如下所示：
- en: '![](img/61351240-2e18-4920-986a-c70d07c5c39a.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61351240-2e18-4920-986a-c70d07c5c39a.png)'
- en: 'The final plot looks as follows, with the loss (meaning the deviation between
    the predicted output and the actual output) equaling 0.01:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的图表如下所示，其中损失（即预测输出与实际输出之间的偏差）为 0.01：
- en: '![](img/2f453783-0ea5-4706-b70b-d9108fe28426.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f453783-0ea5-4706-b70b-d9108fe28426.png)'
- en: Now, we will start working toward deeper use cases using PyTorch.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将开始使用PyTorch进行更深入的应用案例。
- en: Classification
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: A classification problem runs a neural network model to classify the inputs.
    For example, it classifies images of clothing into trousers, tops, and shirts. When
    we provide more inputs to the classification model, it will predict the value
    of the outcomes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题运行神经网络模型以对输入进行分类。例如，它将衣物的图像分类为裤子、上衣和衬衫。当我们向分类模型提供更多输入时，它将预测输出的结果值。
- en: A simple example would be filtering an email as *spam* or *not spam*. Classification
    either predicts categorical class labels based on the training set or the values
    (class labels) when classifying attributes that are used in classifying new data.
    There are many classification models, such as Naive Bayes, random forests, decision
    tress, and logistic regression.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的示例是将电子邮件过滤为*垃圾邮件*或*非垃圾邮件*。分类要么根据训练集预测分类标签，要么在分类新数据时使用的分类属性来预测分类标签（类别标签）。有许多分类模型，如朴素贝叶斯、随机森林、决策树和逻辑回归。
- en: 'Here, we will work on a simple classification problem. To do this, use the
    following this code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将处理一个简单的分类问题。为此，使用以下代码：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let''s plot the graphs and display the learning processes:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制图表并显示学习过程：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will pick only a few plots from the output, as shown in the following screenshot:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只从输出中选取几个图形，如以下截图所示：
- en: '![](img/521a242d-fe44-4f20-ab09-3e8314c22e47.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/521a242d-fe44-4f20-ab09-3e8314c22e47.png)'
- en: 'You can see that the accuracy levels have increased with the increased number
    of steps in the iteration:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到随着迭代步骤数的增加，准确度水平也有所提升：
- en: '![](img/6356bef0-a6b8-4200-9586-b482aedd91a3.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6356bef0-a6b8-4200-9586-b482aedd91a3.png)'
- en: 'We can reach an accuracy level of 1.00 in the final step of our execution:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在执行的最后一步达到1.00的准确度水平：
- en: '![](img/eab78e53-d218-4ff3-b0bf-898dc7358898.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eab78e53-d218-4ff3-b0bf-898dc7358898.png)'
- en: Simple neural networks with torch
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用torch构建简单神经网络
- en: 'Neural networks are necessary when a heuristic approach is required to solve
    a problem. Let''s explore a basic neural network using the following example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要启发式方法来解决问题时，神经网络是必不可少的。让我们通过以下示例来探索一个基本的神经网络：
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Following is the easiest and fastest way to build your network:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是构建网络的最简单且最快的方法：
- en: '[PRE18]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '[PRE19]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Saving and reloading data on the network
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在网络上保存和重新加载数据
- en: 'Let''s look at one example of how to save data on the network and then restore
    the data:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个保存网络数据然后恢复数据的示例：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Two ways to save the net:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 保存网络的两种方式：
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Plotting the results:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制结果：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output of the code will look similar to the graphs that are shown in the
    following diagram:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的输出将类似于以下图表所示的图形：
- en: '![](img/221a7acc-e365-426a-ab76-8f37e4ffff1f.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/221a7acc-e365-426a-ab76-8f37e4ffff1f.png)'
- en: Running with batches
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量运行
- en: Torch helps you organize your data through `DataLoader`. We can use it to package
    the data through batch training. We can have our own data format (NumPy array,
    for example, or any other) loaded into Tensor, along with a wrapper.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Torch帮助你通过`DataLoader`来组织数据。我们可以使用它通过批量训练来打包数据。我们可以将自己的数据格式（例如NumPy数组或其他格式）加载到Tensor中，并进行包装。
- en: 'The following is an example of a dataset where random numbers are taken into
    the dataset in batches and trained:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个数据集的示例，其中随机数以批量的形式被引入数据集并进行训练：
- en: '[PRE23]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output of the code is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的输出如下：
- en: '[PRE24]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Optimization algorithms
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化算法
- en: There is always doubt about which optimization algorithm should be used in our
    implementation of the neural network for a better output. This is done by modifying
    the key parameters, such as the **weights **and **bias **values.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现神经网络时，总是存在关于应该使用哪种优化算法以获得更好输出的疑问。这是通过修改关键参数，如**权重**和**偏差**值来完成的。
- en: These algorithms are used to minimize (or maximize) error (*E*(*x*)), which
    is dependent on the internal parameters. They are used for computing the target
    results (*Y*) from the set of predictors (*x*) that are used in the model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法用于最小化（或最大化）误差（*E*(*x*)），它依赖于内部参数。它们用于计算从模型中使用的预测变量（*x*）集得出的目标结果（*Y*）。
- en: 'Now, let''s look at the different types of algorithms by using the following
    example:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过以下示例来看看不同类型的算法：
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Putting dateset into torch dataset:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集放入torch数据集：
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Training the model for various epochs:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型并进行多个周期：
- en: '[PRE27]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output of executing the preceding code block is displayed in the following
    plot:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码块的输出显示在以下图表中：
- en: '![](img/94766868-218f-469f-bf52-4d762dae6b47.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94766868-218f-469f-bf52-4d762dae6b47.png)'
- en: 'The output of the Epoch count will look like this:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 计数的输出将如下所示：
- en: '[PRE28]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will plot all the optimizers and represent them in the graph, as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将绘制所有优化器，并将它们表示在图表中，如下所示：
- en: '![](img/747f9f2a-2f1a-4180-bc46-f6f3d32b9554.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/747f9f2a-2f1a-4180-bc46-f6f3d32b9554.png)'
- en: In the next section, we will look at RNNs.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将讨论 RNN。
- en: Recurrent neural networks
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: With RNNs, unlike feedforward neural networks, we can use the internal memory
    to process inputs in a sequential manner. In RNN, the connection between nodes
    forms a directed graph along a temporal sequence. This helps in tasking the RNN
    with largely unsegmented and connected speech or character recognition.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RNN 时，与前馈神经网络不同，我们可以利用内部记忆按顺序处理输入。在 RNN 中，节点之间的连接沿时间序列形成一个有向图。这有助于将任务分配给
    RNN，处理大量未分割且互相关联的语音或字符识别。
- en: The MNIST database
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MNIST 数据库
- en: The **MNIST** database consists of 60,000 handwritten digits. It also consists
    of a test dataset that's made up of 10,000 digits. While it is a subset of the
    NIST dataset, all the digits in this dataset are size-normalized and have been
    centered on a 28 x 28 pixels-sized image. Here, every pixel contains a value of
    0-255 with its grayscale value.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**MNIST** 数据库包含 60,000 个手写数字。此外，还有一个由 10,000 个数字组成的测试数据集。虽然它是 NIST 数据集的一个子集，但该数据集中的所有数字都进行了大小标准化，并且已经居中在一个
    28 x 28 像素的图像中。这里，每个像素的值为 0-255，表示其灰度值。'
- en: The MNIST dataset can be found at [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST 数据集可以在 [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)
    找到。
- en: The NIST dataset can be found a [https://www.nist.gov/srd/nist-special-database-19](https://www.nist.gov/srd/nist-special-database-19).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: NIST 数据集可以在 [https://www.nist.gov/srd/nist-special-database-19](https://www.nist.gov/srd/nist-special-database-19)
    找到。
- en: RNN classification
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 分类
- en: 'Here, we will look at an example of how to build an RNN to identify handwritten
    numbers from the MNIST database:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将看一个例子，展示如何构建一个 RNN 来识别 MNIST 数据库中的手写数字：
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Plotting one example:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制一个示例：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Converting test data into Variable, pick 2000 samples to speed up testing:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将测试数据转换为变量，选择 2000 个样本加速测试：
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Training and testing the epochs:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试不同的 Epoch：
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following files need to be downloaded and extracted to train the images:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 需要下载并解压以下文件以训练图像：
- en: '[PRE33]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出结果如下：
- en: '![](img/b9e4d3d4-eb13-4231-9257-661e0a37dcf2.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9e4d3d4-eb13-4231-9257-661e0a37dcf2.png)'
- en: 'Let''s take the processing further with this code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下代码进一步处理：
- en: '[PRE34]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output of epochs is as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Epoch 输出结果如下：
- en: '[PRE35]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: RNN cyclic neural network – regression
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 循环神经网络 – 回归
- en: Now, we will deal with a regression problem under RNN. The cyclic neural network
    provides memory to the neural network. For the serial data, the cyclic neural
    network can achieve better results. We will use RNN here in this example to predict
    time series data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将处理一个基于 RNN 的回归问题。循环神经网络为神经网络提供了记忆功能。对于序列数据，循环神经网络可以实现更好的效果。在这个例子中，我们将使用
    RNN 来预测时间序列数据。
- en: To find out more about circular neural networks, go to [https://iopscience.iop.org/article/10.1209/0295-5075/18/3/003/meta](https://iopscience.iop.org/article/10.1209/0295-5075/18/3/003/meta).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于循环神经网络的信息，请访问 [https://iopscience.iop.org/article/10.1209/0295-5075/18/3/003/meta](https://iopscience.iop.org/article/10.1209/0295-5075/18/3/003/meta)。
- en: 'The following code is for the logistic regression:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于逻辑回归：
- en: '[PRE36]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `RNN` class is defined in the following code. We will use `r_out` in a
    linear way to calculated the predicted output. We can also use a `for` loop to
    calculate the predicted output with `torch.stack`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`RNN` 类在以下代码中定义。我们将以线性方式使用 `r_out` 计算预测输出。我们也可以使用 `for` 循环与 `torch.stack` 来计算预测输出：'
- en: '[PRE37]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE38]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We need to optimize RNN parameters now, as shown in the following code, before
    running the `for` loop to give the prediction:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要优化 RNN 参数，如下代码所示，在运行 `for` 循环以进行预测之前：
- en: '[PRE39]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The following block of code will look like a motion picture effect when it''s
    run, which we can''t represent here in this book. We have added a few screenshots
    to help you visualize this. We are using `x` as an input `sin` value and `y` as
    an output fitting `cos` value. Because a relationship exists between the two curves,
    we will use `sin` to predict `cos`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块运行时会呈现动态效果，但在本书中无法展示。我们添加了一些截图帮助你理解这一效果。我们使用 `x` 作为输入的 `sin` 值，`y` 作为输出的拟合
    `cos` 值。由于这两条曲线之间存在关系，我们将使用 `sin` 来预测 `cos`：
- en: '[PRE40]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Plotting the results:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制结果：
- en: '[PRE41]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出如下：
- en: '![](img/30e1a235-cfa4-4e32-8513-e9ac707ebcf2.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30e1a235-cfa4-4e32-8513-e9ac707ebcf2.png)'
- en: 'The following is a plot graph that will be generated after iteration 10:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是第10次迭代后生成的图形：
- en: '![](img/1e9db5f3-5b02-4c09-8269-9eb57193c681.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1e9db5f3-5b02-4c09-8269-9eb57193c681.png)'
- en: 'The following is a plot graph that will be generated after iteration 25:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是第25次迭代后生成的图形：
- en: '![](img/8aa151a8-b47a-41ba-9b09-faa0635f0a87.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8aa151a8-b47a-41ba-9b09-faa0635f0a87.png)'
- en: 'We are not showing all 100 iteration output images here, but we will skip to
    the final output, iteration 100, as shown in the following screenshot:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在这里展示所有100次迭代的输出图像，而是直接跳到最终的输出，即第100次迭代，如下截图所示：
- en: '![](img/93c8dff1-5c2e-4c1d-8efb-e6f16c330095.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93c8dff1-5c2e-4c1d-8efb-e6f16c330095.png)'
- en: In the next section, we will look at NLP.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探讨自然语言处理（NLP）。
- en: Natural language processing
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Now is the time to experiment with a few NLP techniques with the help of PyTorch.
    This will be more useful for those of you who haven't written code in any deep
    learning framework before, but who may have better understanding of NLP core problems
    and algorithms.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候利用 PyTorch 尝试一些自然语言处理技术了。这对那些之前没有在任何深度学习框架中编写代码的人特别有用，尤其是那些对 NLP 核心问题和算法有更好理解的人。
- en: In this chapter, we will look into simple examples with small dimensions, so
    that we can see how the weight of the layers changes as the network is training.
    You can try out your own model once you understand the network and how it works.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将通过简单的小维度示例来观察神经网络训练过程中层权重的变化。一旦你理解了网络的工作原理，就可以尝试自己的模型。
- en: Before working on any NLP-based problems, we need to understand the basic building
    blocks on deep learning, including affine maps, non-linearities, and objective
    functions.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理任何基于 NLP 的问题之前，我们需要理解深度学习的基本构件，包括仿射映射、非线性和目标函数。
- en: Affine maps
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仿射映射
- en: '**Affine maps** are one of the basic building components of deep learning,
    and are represented as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**仿射映射** 是深度学习的基本构件之一，如下所示：'
- en: '![](img/9ac3ea96-07ab-425b-9c2b-2487fec88f51.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9ac3ea96-07ab-425b-9c2b-2487fec88f51.png)'
- en: In this case, the matrix is represented by A and the vectors are represented by *x* and *b*.
    *A* and *b* are the parameters that need to be learned, while *b* is the bias.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，矩阵由 A 表示，向量由 *x* 和 *b* 表示。*A* 和 *b* 是需要学习的参数，而 *b* 是偏置。
- en: 'A simple example to explain this is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 解释这个的简单示例如下：
- en: '[PRE42]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'After this, run the program with the following command:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，使用以下命令运行程序：
- en: '[PRE43]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output will be as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE44]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Non-linearities
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性
- en: 'First, we need to identify why we need non-linearities. Consider, we have two
    affine maps: `f(x)=Ax+b` and `g(x)=Cx+d`. `f(g(x))` is shown in the following
    equation:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要明确为什么我们需要非线性。考虑我们有两个仿射映射：`f(x)=Ax+b` 和 `g(x)=Cx+d`。`f(g(x))` 如下所示：
- en: '![](img/3c3d54a3-b248-4007-8b84-6f120f64839a.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c3d54a3-b248-4007-8b84-6f120f64839a.png)'
- en: Here, we can see that when affine maps are composed together, the resultant
    is an affine map, where *Ad+b* is a vector and *AC* is a matrix.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，当仿射映射组合在一起时，结果仍然是一个仿射映射，其中 *Ad+b* 是一个向量，*AC* 是一个矩阵。
- en: We can identify neural networks as long chains of affine compositions. Previously,
    it was possible that non-linearities were introduced in-between the affine layers.
    But thankfully, it isn't the case any longer, and hence that helps in building
    more powerful and efficient models.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将神经网络视为一系列仿射组合。以前，仿射层之间可能引入了非线性。但是幸运的是，现在已经不是这样了，这有助于构建更强大且高效的模型。
- en: 'While working with the most common functions such as tanh (x), σ(x) and ReLU
    (x), we see that there are a few core non-linearities, as shown in the following
    code block:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用最常见的函数（如 tanh (x)、σ(x) 和 ReLU (x)）时，我们看到有一些核心的非线性，如下所示的代码块所示：
- en: '[PRE45]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '[PRE46]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Objective functions
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标函数
- en: The objective function (also called the loss function or cost function) will
    help your network to minimize. It works by selecting a training instance, running
    it through your neural network, then computing the loss of the output.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数（也称为损失函数或代价函数）将帮助您的网络进行最小化。它通过选择一个训练实例，将其传递通过神经网络，然后计算输出的损失来工作。
- en: The derivative of the loss function is updated for finding the parameters of
    the model. Like, if your model predicts an answer confidently, and the answer
    turns out to be wrong, the the computed loss will be high. If the predicted answer
    is correct, then the loss is low.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数的导数会被更新，以找到模型的参数。例如，如果模型自信地预测了一个答案，而答案结果是错误的，那么计算出的损失将会很高。如果预测答案正确，那么损失较低。
- en: How is the network minimized?
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 网络如何最小化？
- en: First, the function will select a training instance
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，函数将选择一个训练实例
- en: Then, it is passed through our neural network to get the output
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它通过我们的神经网络传递以获得输出
- en: Finally, the loss of the output is calculated
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，输出的损失被计算出来
- en: In our training examples we need to minimize the loss function to minimize the
    probability of wrong results with the actual dataset.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的训练示例中，我们需要最小化损失函数，以减少使用实际数据集时错误结果的概率。
- en: Building network components in PyTorch
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中构建网络组件
- en: Before shifting our focus to NLP, in this section we will use non-linearities
    and affine maps to build a network in PyTorch. In this example, we will learn
    to compute a loss function using the built in negative log likelihood in PyTorch
    and using backpropagation for updating the parmeters.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在将注意力转向NLP之前，在本节中我们将使用非线性激活函数和仿射映射在PyTorch中构建一个网络。在这个例子中，我们将学习如何使用PyTorch内置的负对数似然（negative
    log likelihood）来计算损失函数，并使用反向传播更新参数。
- en: Please note that all the components of the network need to be inherited from
    `nn.Module` and also override the `forward()` method. Considering boilerplate,
    these are the details we should remember. The network components are provided
    functionality when we inherit those components from `nn.Module`
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，网络的所有组件需要继承自`nn.Module`，并且还需要重写`forward()`方法。考虑到样板代码，这些是我们应该记住的细节。当我们从`nn.Module`继承这些组件时，网络组件会提供相应的功能。
- en: Now, as mentioned previously, we will look at an example, in which the network
    takes a scattered bag-of-words (BoW) representation and and the output is a probability
    distribution into two labels, that is, English and Spanish. Also, this model is
    an example of logistic regression.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如前所述，我们将看一个例子，其中网络接收一个稀疏的词袋（BoW）表示，输出是一个概率分布到两个标签，即英语和西班牙语。同时，这个模型是逻辑回归的一个例子。
- en: BoW classifier using logistic regression
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归的BoW分类器
- en: Probabilities will be logged onto our two labels English and Spanish on which
    our generated model will map a sparse BoW representation. In the vocabulary, we
    will assign each word as an index. Let's say for example, we have two words in
    our vocabulary, that is hello and world, which have indices as zero and one, respectively.
    For example, for the sentence *hello hello hello hello hello,* the BoW vector
    is *[5,0]*. Similarly the BoW vector for *hello world world hello world* is *[2,3]*,
    and so on.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 概率将被记录在我们的两个标签“英语”和“西班牙语”上，我们生成的模型将映射一个稀疏的BoW表示。在词汇表中，我们会为每个词分配一个索引。假设在我们的词汇表中有两个词，即hello和world，它们的索引分别是零和一。例如，对于句子
    *hello hello hello hello hello,* 其BoW向量是 *[5,0]*。类似地，*hello world world hello
    world* 的BoW向量是 *[2,3]*，以此类推。
- en: Generally, it is *[Count(hello),Count(world)].*
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，它是 *[Count(hello), Count(world)]*。
- en: Let us denote is BOW vector as *x.*
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将BOW向量表示为 *x.*
- en: 'The network output is as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的输出如下：
- en: '![](img/8905bb53-f8c9-4a9a-9d76-58c0c236c43e.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8905bb53-f8c9-4a9a-9d76-58c0c236c43e.png)'
- en: 'Next, we need to pass the input through an affine map and then use log softmax:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要通过仿射映射传递输入，然后使用log softmax：
- en: '[PRE47]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, we will define the parameters that are needed. Here, those parameters
    are `A` and `B`, and the following code block explains the further implementations
    are required:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义所需的参数。在这里，这些参数是`A`和`B`，以下代码块解释了进一步所需的实现：
- en: '[PRE48]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, the model knows its own parameters. The first output is `A`, while the
    second is `B`, as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，模型知道了自己的参数。第一个输出是`A`，第二个是`B`，如下所示：
- en: '[PRE49]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '[PRE50]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We got the tensor output values. But, as we can see from the preceding code,
    these values aren't in correspondence to the log probability whether which is
    `English` and which corresponds to word `Spanish`. We need to train the model,
    and for that it's important to define these values to the log probabilities.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了张量输出值。但是，正如我们从前面的代码中看到的，这些值与对数概率并不对应，无论哪个是`English`，哪个对应的是单词`Spanish`。我们需要训练模型，为此将这些值映射到对数概率是很重要的。
- en: '[PRE51]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Let's start training our model then. We start with passing instances through
    the model to the those log probabilities. Then, the loss function is computed,
    and once the loss function is computer we calculate the gradient of this loss
    function. Finally, the parameters are updated with a gradient step. The `nn` package
    in PyTorch provides the loss functions. We want nn.NLLLoss() as the negative log
    likelihood loss. Optimization functions are also defined is `torch.optim`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们开始训练我们的模型吧。我们首先通过模型传递实例，得到这些对数概率。然后计算损失函数，损失函数计算完成后，我们计算该损失函数的梯度。最后，使用梯度更新参数。PyTorch中的`nn`包提供了损失函数。我们需要使用nn.NLLLoss()作为负对数似然损失。优化函数也在`torch.optim`中定义。
- en: 'Here, we will just use **Stochastic Gradient Descent** (**SGD**):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用**随机梯度下降法**（**SGD**）：
- en: '[PRE52]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: We don't want to pass the training data again and again for no reason. Real
    datasets have multiple instances and not just 2\. It is reasonable to train the
    model for epochs between 5 to 30.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不想毫无理由地一次次传递训练数据。实际数据集有多个实例，而不仅仅是2个。合理的做法是将模型训练在5到30个epoch之间。
- en: 'The following code shows the range for our example:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了我们示例的范围：
- en: '[PRE53]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Here, we will compute the various factors such as loss, gradient, and updating
    the parameters by calling the function optimizer.step():'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将通过调用函数optimizer.step()来计算各种因素，如损失、梯度和更新参数：
- en: '[PRE54]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output is as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE55]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Summary
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Now, we have the basic understanding of performing text-based processing using
    PyTorch. We also have a better understanding on how RNN works and how can we approach
    NLP-related problems using PyTorch.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经基本了解了如何使用PyTorch进行基于文本的处理。我们也更清楚了RNN是如何工作的，以及如何使用PyTorch解决与NLP相关的问题。
- en: In the upcoming chapters, we will build applications using what we have learned
    about neural networks and NLP. Happy coding!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将利用所学的神经网络和NLP知识构建应用程序。祝编码愉快！
