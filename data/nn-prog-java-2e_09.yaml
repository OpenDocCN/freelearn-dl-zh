- en: Chapter 9. Optimizing and Adapting Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章：优化和调整神经网络
- en: 'In this chapter, the reader will be presented with techniques that help to
    optimize neural networks, in order to get the best performance. Tasks such as
    input selection, dataset separation and filtering, choosing the number of hidden
    neurons, and cross-validation strategies are examples of what can be adjusted
    to improve a neural network''s performance. Furthermore, this chapter focuses
    on methods for adapting neural networks to real-time data. Two implementations
    of these techniques are presented here. Application problems will be selected
    for exercises. This chapter deals with the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，读者将了解帮助优化神经网络的技巧，以便获得最佳性能。例如，输入选择、数据集分离和过滤、选择隐藏神经元数量和交叉验证策略等任务都是可以调整以改善神经网络性能的例子。此外，本章重点介绍将神经网络适应实时数据的方法。这里将展示这些技术的两种实现。练习将选择应用问题。本章涉及以下主题：
- en: Input selection
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入选择
- en: Dimensionality reduction
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度降低
- en: Data filtering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据过滤
- en: Structure selection
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构选择
- en: Pruning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剪枝
- en: Validation strategies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证策略
- en: Cross-validation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Online retraining
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在线重新训练
- en: Stochastic online learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机在线学习
- en: Adaptive neural networks
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应神经网络
- en: Adaptive resonance theory
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应共振理论
- en: Common issues in neural network implementations
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络实现中的常见问题
- en: 'When developing a neural network application, it is quite common to face problems
    regarding how accurate the results are. The source of these problems can be various:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发神经网络应用时，遇到关于结果准确性的问题是很常见的。这些问题可能来自多个方面：
- en: Bad input selection
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入选择不当
- en: Noisy data
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 噪声数据
- en: Too big a dataset
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集太大
- en: Unsuitable structure
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不合适的结构
- en: Inadequate number of hidden neurons
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏神经元数量不足
- en: Inadequate learning rate
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率不足
- en: Insufficient stop condition
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏停止条件
- en: Bad dataset segmentation
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集分割不当
- en: Bad validation strategy
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 差的验证策略
- en: The design of a neural network application sometimes requires a lot of patience
    and the use of trial and error methods. There is no methodology stating specifically
    which number of hidden units and/or architecture should be used, but there are
    recommendations on how to choose these parameters properly. Another issue programmers
    may face is a long training time, which often causes the neural network to not
    learn the data. No matter how long the training runs, the neural network won't
    converge.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络应用的设计有时需要很多耐心和试错方法。没有具体说明应该使用多少隐藏单元和/或架构的方法论，但有一些关于如何正确选择这些参数的建议。程序员可能面临的另一个问题是训练时间过长，这通常会导致神经网络无法学习数据。无论训练运行多长时间，神经网络都不会收敛。
- en: Tip
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Designing a neural network requires the programmer or designer to test and redesign
    the neural structure as many times as needed, until an acceptable result is obtained.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 设计神经网络需要程序员或设计师根据需要多次测试和重新设计神经网络结构，直到获得可接受的结果。
- en: On the other hand, the neural network solution designer may wish to improve
    the results. Because a neural network can learn until the learning algorithm reaches
    the stop condition, the number of epochs or the mean squared error, the results
    are not accurate enough or not generalized. This will require a redesign of the
    neural structure, or a new dataset selection.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，神经网络解决方案的设计师可能希望改进结果。因为神经网络可以学习直到学习算法达到停止条件，比如迭代次数或均方误差，结果可能不够准确或不够泛化。这需要重新设计神经网络结构或选择新的数据集。
- en: Input selection
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入选择
- en: One of the key tasks in designing a neural network application is to select
    appropriate inputs. For the unsupervised case, one wishes to use only relevant
    variables on which the neural network will find the patterns. And for the supervised
    case, there is a need to map the outputs to the inputs, so one needs to choose
    only the input variables which somewhat have influence on the output.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 设计神经网络应用的一个关键任务是选择合适的输入。对于无监督情况，希望只使用神经网络能找到模式的变量。对于监督情况，需要将输出映射到输入，因此需要选择对输出有一定影响的输入变量。
- en: Data correlation
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据相关性
- en: One strategy that helps in selecting good inputs in the supervised case is the
    correlation between data series, which is implemented in [Chapter 5](ch05.xhtml
    "Chapter 5. Forecasting Weather"), *Forecasting Weather*. A correlation between
    data series is a measure of how one data sequence reacts or influences the other.
    Suppose we have one dataset containing a number of data series, from which we
    choose one to be an output. Now we need to select the inputs from the remaining
    variables.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习案例中，选择良好输入的一种策略是数据系列之间的相关性，这在[第5章](ch05.xhtml "第5章. 预测天气") *预测天气* 中实现。数据系列之间的相关性是衡量一个数据序列如何反应或影响另一个数据序列的度量。假设我们有一个包含多个数据系列的数据集，从中选择一个作为输出。现在我们需要从剩余的变量中选择输入。
- en: The correlation takes values from *-1* to 1, where values near to *+1* indicate
    a positive correlation, values near -1 indicate a negative correlation, and values
    near *0* indicate no correlation at all.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 相关系数的取值范围从 *-1* 到 1，其中接近 *+1* 的值表示正相关，接近 -1 的值表示负相关，接近 *0* 的值表示没有相关性。
- en: 'As an example, let''s see three charts of two variables *X* and *Y*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看两个变量 *X* 和 *Y* 的三个图表：
- en: '![Data correlation](img/B5964_09_01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![数据相关性](img/B5964_09_01.jpg)'
- en: In the first chart, to the left, visually one can see that as one variable decreases,
    the other increases its value (corr. -0.8). The middle chart shows the case when
    the two variables vary in the same direction, therefore positive correlation (corr.
    +0.7). The third chart, to the right, shows a case where there is no correlation
    between the variables (corr. *-0.1*).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个图表中，左侧可以看到，当一个变量减少时，另一个变量的值增加（相关系数 -0.8）。中间的图表显示了两个变量在同一方向变化的情形，因此是正相关（相关系数
    +0.7）。右侧的第三个图表显示了变量之间没有相关性的情形（相关系数 *-0.1*）。
- en: There is no threshold rule as to which correlation should be taken into account
    as a limit; it depends on the application. While absolute correlation values greater
    than 0.5 may be suitable for one application, in others, values near 0.2 may add
    a significant contribution.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 没有关于应该考虑哪个相关性作为限制阈值的规则；这取决于应用。虽然绝对相关性值大于 0.5 可能适合一个应用，但在其他情况下，接近 0.2 的值可能贡献显著。
- en: Transforming data
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换
- en: Linear correlation is very good in detecting behaviors between data series when
    they are presumably linear. However, if two data series form a parable when plotted
    together, linear correlation won't be able to identify any relation. That's why
    sometimes we need to transform data into a view that exhibits a linear correlation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据系列在假设上是线性的时，线性相关性在检测数据系列之间的行为方面非常好。然而，如果两个数据系列在绘制在一起时形成一个抛物线，线性相关性将无法识别任何关系。这就是为什么有时我们需要将数据转换为一个显示线性相关性的视角。
- en: Data transformation depends on the problem that is being faced. It consists
    of inserting an additional data series with processed data from one or more data
    series. One example is an equation (possibly nonlinear) that includes one or more
    parameters. Some behaviors are more detectable under a transformed view of the
    data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换取决于面临的问题。它包括插入一个或多个数据系列处理后的附加数据系列。一个例子是包含一个或多个参数的方程（可能是非线性的）。在数据转换的视角下，一些行为更容易被检测到。
- en: Tip
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Data transformation also involves a bit of knowledge about the problem. However,
    by seeing the scatter plot of two data series, it becomes easier to choose which
    transformation to apply.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换还涉及一些关于问题的知识。然而，通过查看两个数据系列的散点图，选择要应用哪种转换变得更容易。
- en: Dimensionality reduction
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度降低
- en: 'Another interesting point is regarding removing redundant data. Sometimes this
    is desired when there is a lot of available data in both unsupervised and supervised
    learning. As an example, let''s see a chart of two variables:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的观点是关于去除冗余数据。在无监督学习和监督学习中，当有大量可用数据时，有时这可能是所需的。作为一个例子，让我们看看两个变量的图表：
- en: '![Dimensionality reduction](img/B5964_09_02.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![维度降低](img/B5964_09_02.jpg)'
- en: It can be seen that both X and *Y* variables share the same shape, so this can
    be interpreted as a redundancy, as both variables are carrying almost the same
    information due the high positive correlation. Thus, one can consider a technique
    called **Principal Component Analysis** (**PCA**) which gives a good approach
    for dealing with these cases.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到 X 和 *Y* 变量具有相同的形状，这可以解释为冗余，因为这两个变量由于高度正相关而携带几乎相同的信息。因此，可以考虑一种称为 **主成分分析**
    (**PCA**) 的技术，它为处理这些情况提供了一个良好的方法。
- en: 'The result of PCA will be a new variable summarizing the previous two (or more).
    Basically, the original data series are subtracted by the mean and then multiplied
    by the transposed eigenvectors of the covariance matrix:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的结果将是一个新的变量，它总结了前面的两个（或更多）变量。基本上，原始数据系列减去平均值，然后乘以协方差矩阵的转置特征向量：
- en: '![Dimensionality reduction](img/B05964_09_02_01.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![降维](img/B05964_09_02_01.jpg)'
- en: Here, *SXY* is the covariance between the variables *X* and *Y*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*SXY*是变量*X*和*Y*之间的协方差。
- en: 'The derived new data will be then:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 导出的新数据将是：
- en: '![Dimensionality reduction](img/B05964_09_02_02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![降维](img/B05964_09_02_02.jpg)'
- en: 'Let''s see now what a new variable would look like in a chart, compared to
    the original ones:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看新的变量在图表中看起来会是什么样子，与原始变量相比：
- en: '![Dimensionality reduction](img/B5964_09_03.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![降维](img/B05964_09_03.jpg)'
- en: 'In our framework, we are going to add the class `PCA` that will perform this
    transformation and preprocessing before applying the data into a neural network:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的框架中，我们将添加一个名为`PCA`的类，该类将在将数据应用于神经网络之前执行这种转换和预处理：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Data filtering
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据过滤
- en: Noisy data and bad data are also sources of problems in neural network applications;
    that's why we need to filter data. One of the common data filtering techniques
    can be performed by excluding the records that exceed the usual range. For example,
    temperature values are between -40 and 40, so a value such as 50 would be considered
    an outlier and could be taken out.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声数据和不良数据也是神经网络应用中问题的来源；这就是为什么我们需要过滤数据。一种常见的数据过滤技术可以通过排除超出常规范围的记录来实现。例如，温度值在-40到40之间，因此像50这样的值将被视为异常值并可能被移除。
- en: 'The 3-sigma rule is a good and effective measure for filtering. It consists
    in filtering the values that are beyond three times the standard deviation from
    the mean:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 3-σ规则是一种良好且有效的过滤措施。它包括过滤那些超出平均值三倍标准差之外的值：
- en: '![Data filtering](img/B05964_09_03_01.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![数据过滤](img/B05964_09_03_01.jpg)'
- en: 'Let''s add a class to deal with data filtering:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加一个类来处理数据过滤：
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'These classes can be called in `DataSet` by the following methods, which are
    then called elsewhere for filtering and reducing dimensionality:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类可以通过以下方法在`DataSet`中调用，然后在其他地方用于过滤和降维：
- en: '[PRE2]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Cross-validation
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: 'Among a number of strategies for validating a neural network, one very important
    one is cross-validation. This strategy ensures that all data has been presented
    to the neural network as training and test data. The dataset is partitioned into
    *K* groups, of which one is separated for testing while the others are for training:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多验证神经网络的策略中，交叉验证是非常重要的一种。这种策略确保所有数据都作为训练数据和测试数据呈现给神经网络。数据集被划分为*K*组，其中一组用于测试，其余用于训练：
- en: '![Cross-validation](img/B5964_09_04.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![交叉验证](img/B5964_09_04.jpg)'
- en: 'In our code, let''s create a class called `CrossValidation` to manage cross-validation:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码中，让我们创建一个名为`CrossValidation`的类来管理交叉验证：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Structure selection
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构选择
- en: To choose an adequate structure for a neural network is also a very important
    step. However, this is often done empirically, since there is no rule on how many
    hidden units a neural network should have. The only measure of how many units
    are adequate is the neural network performance. One assumes that if the general
    error is low enough, then the structure is suitable. Nevertheless, there might
    be a smaller structure that could yield the same result.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个适合神经网络的适当结构也是一个非常重要的步骤。然而，这通常是通过经验来做的，因为没有关于神经网络应该有多少隐藏单元的规则。衡量单元数量的唯一标准是神经网络的性能。人们假设如果总体误差足够低，那么结构就是合适的。尽管如此，可能存在一个更小的结构可以产生相同的结果。
- en: 'In this context, there are basically two methodologies: constructive and pruning.
    The constructive consists in starting with only the input and output layers, then
    adding new neurons at a hidden layer, until a good result can be obtained. The
    destructive approach, also known as pruning, works on a bigger structure on which
    the neurons having few contributions to the output are taken out.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，基本上有两种方法：构造性和剪枝。构造性方法是从只有输入和输出层开始，然后在隐藏层中添加新的神经元，直到获得良好的结果。破坏性方法，也称为剪枝，是在一个更大的结构上进行的，其中移除了对输出贡献很少的神经元。
- en: 'The constructive approach is depicted in the following figure:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 构造性方法在以下图中表示：
- en: '![Structure selection](img/B5964_09_05.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![结构选择](img/B5964_09_05.jpg)'
- en: 'Pruning is the way back: when given a high number of neurons, one wishes to
    *prune* those whose sensitivity is very low, that is, whose contribution to the
    error is minimal:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝是回归之路：当给定大量神经元时，人们希望剪除那些灵敏度非常低的神经元，即那些对误差贡献最小的神经元：
- en: '![Structure selection](img/B5964_09_06.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![结构选择](img/B5964_09_06.jpg)'
- en: 'To implement pruning, we`ve added the following properties in the class `NeuralNet`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现剪枝，我们在`NeuralNet`类中添加了以下属性：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A method called `removeNeuron` in the class `NeuralLayer`, which actually sets
    all the connections of the neuron to zero, disables weight updating and fires
    only zero at the neuron`s output. This method is called if the property pruning
    of the `NeuralNet` object is set to true. The sensitivity calculation is according
    to the chain rule, as shown in [Chapter 3](ch03.xhtml "Chapter 3. Perceptrons
    and Supervised Learning"), *Perceptrons and Supervised Learning* and implemented
    in the `calcNewWeigth` method:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在`NeuralLayer`类中有一个名为`removeNeuron`的方法，实际上是将神经元的所有连接设置为0，禁用权重更新，并在神经元输出处仅产生0。如果将`NeuralNet`对象的剪枝属性设置为true，则调用此方法。灵敏度计算是根据链式法则进行的，如[第3章](ch03.xhtml
    "第3章。感知器和监督学习")中所示，*感知器和监督学习*，并在`calcNewWeigth`方法中实现：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Online retraining
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线重新训练
- en: During the learning process, it is important to design how the training should
    be performed. Two basic approaches are batch and incremental learning.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习过程中，设计如何进行训练非常重要。两种基本方法是批量学习和增量学习。
- en: 'In batch learning, all the records are fed to the network, so it can evaluate
    the error and then update the weights:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在批量学习中，所有记录都被输入到网络中，因此它可以评估误差，然后更新权重：
- en: '![Online retraining](img/B5964_09_07.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![在线重新训练](img/B5964_09_07.jpg)'
- en: 'In incremental learning, the update is performed after each record has been
    sent to the network:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在增量学习中，更新是在将每条记录发送到网络后进行的：
- en: '![Online retraining](img/B5964_09_08.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![在线重新训练](img/B5964_09_08.jpg)'
- en: Both approaches work well and have advantages and disadvantages. While batch
    learning can used for a less frequent, though more directed, weight update, incremental
    learning provides a method for fine-tuned weight adjustment. In that context,
    it is possible to design a mode of learning that enables the network to learn
    continually.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都工作得很好，并且各有优缺点。虽然批量学习可以用于较少但更直接的权重更新，但增量学习提供了一种微调权重调整的方法。在这种情况下，可以设计一种学习模式，使网络能够持续学习。
- en: Tip
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: As a suggested exercise, the reader may pick one of the datasets available in
    the code and design a training using part of the records, and then train using
    another part in both modes, online and batch. See the `IncrementalLearning.java`
    file for details.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项建议的练习，读者可以从代码中可用的数据集中选择一个，并使用部分记录设计训练，然后在两种模式下（在线和批量）使用另一部分进行训练。有关详细信息，请参阅`IncrementalLearning.java`文件。
- en: Stochastic online learning
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机在线学习
- en: Offline learning means that the neural network learns while not in *operation*.
    Every neural network application is supposed to work in an environment, and in
    order to be at production, it should be properly trained. Offline training is
    suitable for putting the network into operation, since its outputs may be varied
    over large ranges of values, which would certainly compromise the system, if it
    is in operation. But when it comes to online learning, there are restrictions.
    While in offline learning, it's possible to use cross-validation and bootstrapping
    to predict errors, in online learning, this can't be done since there's no "training
    dataset" anymore. However, one would need online training when some improvement
    in the neural network's performance is desired.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 离线学习意味着神经网络在不在*操作*状态下进行学习。每个神经网络应用都应在某个环境中工作，为了进入生产状态，它应该得到适当的训练。离线训练适合将网络投入运行，因为其输出可能在大范围内变化，如果它在运行中，这肯定会损害系统。但是，当涉及到在线学习时，存在限制。在离线学习中，可以使用交叉验证和自助法来预测误差，而在在线学习中，由于没有“训练数据集”，这不可能做到。然而，当需要提高神经网络性能时，就需要进行在线训练。
- en: 'A stochastic method is used when online learning is performed. This algorithm
    to improve neural network training is composed of two main features: random choice
    of samples for training and variation of learning rate in runtime (online). This
    training method has been used when noise is found in the objective function. It
    helps to escape the local minimum (one of the best solutions) and to reach the
    global minimum (the best solution):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行在线学习时使用随机方法。这个用于改进神经网络训练的算法由两个主要特性组成：随机选择训练样本和在运行时（在线）变动学习率。当在目标函数中发现噪声时，使用这种方法进行训练。它有助于逃离局部最小值（最佳解决方案之一）并达到全局最小值（最佳解决方案）：
- en: 'The pseudo-algorithm is displayed below (source: [ftp://ftp.sas.com/pub/neural/FAQ2.html#A_styles](ftp://ftp.sas.com/pub/neural/FAQ2.html#A_styles)):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 伪算法如下（来源：[ftp://ftp.sas.com/pub/neural/FAQ2.html#A_styles](ftp://ftp.sas.com/pub/neural/FAQ2.html#A_styles))：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Implementation
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现方式
- en: 'The Java project has created the class `BackpropagtionOnline` inside the `learn`
    package. The differences between this algorithm and classic Backpropagation was
    programmed by changing the `train()` method, by adding two new methods: `generateIndexRandomList()`
    and `reduceLearningRate()`. The first one generates a random list of indexes to
    be used in the training step and the second one executes the learning rate online
    variation according to the following heuristic:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Java项目在`learn`包内创建了名为`BackpropagtionOnline`的类。这个算法与经典反向传播算法的不同之处在于通过修改`train()`方法，添加了两个新方法：`generateIndexRandomList()`和`reduceLearningRate()`。第一个方法生成一个随机索引列表，用于训练步骤，第二个方法根据以下启发式方法在线执行学习率的变动：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This method will be called at the end of the `train()` method.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将在`train()`方法结束时被调用。
- en: Application
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用
- en: 'It has used data from previous chapters to test this new way to train neural
    nets. The same neural net topology defined in each chapter ([Chapter 5](ch05.xhtml
    "Chapter 5. Forecasting Weather"), *Forecasting Weather* and [Chapter 8](ch08.xhtml
    "Chapter 8. Text Recognition"), *Text Recognition*) has been used to train the
    nets of this chapter. The first one is the weather forecasting problem and the
    second one is the OCR. The following table shows the comparison of results:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 已经使用前几章的数据来测试这种新的训练神经网络的方法。每个章节中定义的相同神经网络拓扑（[第5章](ch05.xhtml "第5章. 预测天气"), *预测天气*
    和 [第8章](ch08.xhtml "第8章. 文本识别"), *文本识别*）已经用于训练本章的神经网络。第一个是天气预报问题，第二个是OCR。下表显示了结果的比较：
- en: '![Application](img/B5964_09_08_01.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![应用](img/B5964_09_08_01.jpg)'
- en: 'In addition, charts of the MSE evolution have been plotted and are shown here:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，已经绘制了MSE演变的图表，并在此处展示：
- en: '![Application](img/B5964_09_09.jpg)![Application](img/B5964_09_10.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![应用](img/B5964_09_09.jpg)![应用](img/B5964_09_10.jpg)'
- en: The curve showed in the first chart (Weather Forecast) has a saw shape, because
    of the variation of learning rate. Besides, it's very similar to the curve, as
    shown in [Chapter 5](ch05.xhtml "Chapter 5. Forecasting Weather"), *Forecasting
    Weather* On the other hand, the second chart (OCR) shows that the training process
    was faster and stops near the 900th epoch because it reached a very small MSE
    error.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张图表（天气预报）中显示的曲线呈锯齿状，这是因为学习率的变动。此外，它非常类似于[第5章](ch05.xhtml "第5章. 预测天气")中显示的曲线，*预测天气*。另一方面，第二张图表（OCR）显示训练过程更快，并在大约第900个epoch时停止，因为它达到了一个非常小的均方误差（MSE）错误。
- en: 'Other experiments were made: training neural nets with a backpropagation algorithm,
    and considering the learning rate found by the online approach. The MSE values
    reduced in both problems:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 还进行了其他实验：使用反向传播算法训练神经网络，并考虑在线方法找到的学习率。两个问题中的MSE值都降低了：
- en: '![Application](img/B5964_09_11.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![应用](img/B5964_09_11.jpg)'
- en: Another important observation consists in the fact that training process demonstrated
    by the training terminated almost in the 3,000th epoch. Therefore, it's faster
    and better than the training process seen in [Chapter 8](ch08.xhtml "Chapter 8. Text
    Recognition"), *Text Recognition* using the same algorithm.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的观察结果是，训练过程在约第3,000个epoch时几乎终止。因此，它比使用相同算法在[第8章](ch08.xhtml "第8章. 文本识别"),
    *文本识别*中看到的训练过程更快、更好。
- en: Adaptive neural networks
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自适应神经网络
- en: Analogous to human learning, neural networks may also work in order not to forget
    previous knowledge. Using the traditional approaches for neural learning, this
    is nearly impossible, due to the fact that every training implies replacing all
    the connections already made by new ones, thereby *forgetting* the previous knowledge.
    Thus a need arises to make the neural networks adapt to new knowledge by incrementing
    instead of replacing their current knowledge. To address that issue, we are going
    to explore one method called **adaptive resonance theory** (**ART**).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类学习类似，神经网络也可能工作以避免忘记以前的知识。使用传统的神经网络学习方法，由于每次训练都意味着用新的连接替换已经建立的连接，因此几乎不可能忘记以前的知识。因此，需要使神经网络通过增加而不是替换当前知识来适应新知识。为了解决这个问题，我们将探讨一种称为**自适应共振理论**（**ART**）的方法。
- en: Adaptive resonance theory
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应共振理论
- en: 'The question that drove the development of this theory was: *How can an adaptive
    system remain plastic to a significant input and yet keep stability for irrelevant
    inputs?* In other words: *How can it retain previously learned information while
    learning new information?*'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动这一理论发展的疑问是：*一个自适应系统如何对重要输入保持可塑性，同时对无关输入保持稳定性？*换句话说：*它如何在学习新信息的同时保留以前学习的信息？*
- en: We've seen that competitive learning in unsupervised learning deals with pattern
    recognition, whereby similar inputs yield similar outputs or fire the same neurons.
    In an ART topology, the resonance comes in when the information is being retrieved
    from the network, by providing a feedback from the competitive layer and the input
    layer. So, while the network receives data to learn, there is an oscillation resulting
    from the feedback between the competitive and input layers. This oscillation stabilizes
    when the pattern is fully developed inside the neural network. This resonance
    then reinforces the stored pattern.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，在无监督学习中，竞争性学习处理模式识别，其中相似的输入产生相似的输出或激活相同的神经元。在ART拓扑结构中，当从网络中检索信息时，通过提供竞争层和输入层的反馈，共振出现。因此，当网络接收数据以进行学习时，竞争层和输入层之间的反馈会产生振荡。当模式在神经网络内部完全发展时，这种振荡会稳定下来。然后，这种共振会加强存储的模式。
- en: Implementation
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现
- en: 'A new class called `ART` has created into the some package, inheriting from
    `CompetitiveLearning`. Besides other small contributions, its great change is
    the vigilance test:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个包中创建了一个名为`ART`的新类，它继承自`CompetitiveLearning`。除了其他小的贡献外，它的重大变化是警觉性测试：
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The training method is shown below. It''s possible to notice that, firstly,
    global variables and the neural net are initialized; after that, the number of
    training sets and the training patterns are stored; then the training process
    begins. The first step of this process is to calculate the index of the winner
    neuron; the second is make attribution of the neural net output. The next step
    consists of verifying whether the neural net has learned or not, whether it has
    learned that weights are fixed; if not, another training sample is presented to
    the net:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 训练方法如下所示。可以注意到，首先，全局变量和神经网络被初始化；之后，存储了训练集的数量和训练模式；然后开始训练过程。这个过程的第一步是计算获胜神经元的索引；第二步是对神经网络输出进行归因。接下来的步骤包括验证神经网络是否已经学习，是否已经学习到权重是固定的；如果没有，则向网络展示另一个训练样本：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Summary
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we''ve seen a few topics that make a neural network work better,
    either by improving its accuracy or by extending its knowledge. These techniques
    help a lot in designing solutions with artificial neural networks. The reader
    is welcome to apply this framework in any desired task that neural networks can
    be used on, in order to explore the enhanced power that these structures can have.
    Even simple details such as selecting input data may influence the entire learning
    process, as well as filtering bad data or eliminating redundant variables. We
    demonstrated two implementations, two strategies that help to improve the performance
    of a neural network: stochastic online learning and adaptive resonance theory.
    These methodologies enable the network to extend its knowledge and therefore adapt
    to new, changing environments.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了一些使神经网络工作得更好的主题，无论是通过提高其准确性还是通过扩展其知识。这些技术在设计使用人工神经网络的解决方案方面非常有帮助。读者可以自由地将这个框架应用于任何可以使用神经网络的期望任务，以探索这些结构可能具有的增强能力。甚至像选择输入数据这样的简单细节也可能影响整个学习过程，以及过滤不良数据或消除冗余变量。我们展示了两种实现，两种有助于提高神经网络性能的策略：随机在线学习和自适应共振理论。这些方法使网络能够扩展其知识，因此能够适应新的、不断变化的环境。
