- en: Building a Deep Feedforward Neural Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度前馈神经网络
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Training a vanilla neural network
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个基础神经网络
- en: Scaling the input dataset
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对输入数据集进行缩放
- en: Impact of training when the majority of inputs are greater than zero
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当大多数输入值大于零时的训练影响
- en: Impact of batch size on model accuracy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小对模型准确性的影响
- en: Building a deep neural network to improve network accuracy
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建深度神经网络以提高网络准确性
- en: Varying the learning rate to improve network accuracy
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变学习率以提高网络准确性
- en: Varying the loss optimizer to improve network accuracy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变损失优化器以提高网络准确性
- en: Understanding the scenario of overfitting
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解过拟合的情境
- en: Speeding up the training process using batch normalization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用批量归一化加速训练过程
- en: In the previous chapter, we looked at the basics of the function of a neural
    network. We also learned that there are various hyperparameters that impact the
    accuracy of a neural network. In this chapter, we will get into the details of
    the functions of the various hyperparameters within a neural network.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解了神经网络的基本功能。我们还学习了有许多超参数会影响神经网络的准确性。在本章中，我们将详细探讨神经网络中各种超参数的功能。
- en: All the codes for this chapter are available at https://github.com/kishore-ayyadevara/Neural-Networks-with-Keras-Cookbook/blob/master/Neural_network_hyper_parameters.ipynb
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码可以在https://github.com/kishore-ayyadevara/Neural-Networks-with-Keras-Cookbook/blob/master/Neural_network_hyper_parameters.ipynb找到
- en: Training a vanilla neural network
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个基础神经网络
- en: To understand how to train a vanilla neural network, we will go through the
    task of predicting the label of a digit in the MNIST dataset, which is a popular
    dataset of images of digits (one digit per image) and the corresponding label
    of the digit that is contained in the image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解如何训练一个基础神经网络，我们将通过预测MNIST数据集中数字标签的任务，来实现这一点。MNIST是一个流行的数字图像数据集（每张图片包含一个数字），并且每个图像都有对应的标签。
- en: Getting ready
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 做好准备
- en: 'Training a neural network is done in the following steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的步骤如下：
- en: Import the relevant packages and datasets
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包和数据集
- en: 'Preprocess the targets (convert them into one-hot encoded vectors) so that
    we can perform optimization on top of them:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对目标进行预处理（将它们转换为独热编码向量），以便我们能够在其上进行优化：
- en: We shall be minimizing categorical cross entropy loss
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将最小化类别交叉熵损失
- en: 'Create train and test datasets:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建训练集和测试集：
- en: We have the train dataset so that we create a model based on it
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有训练数据集，因此我们可以基于它创建一个模型
- en: 'The test dataset is not seen by the model:'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据集对模型是不可见的：
- en: Hence, the accuracy on the test dataset is an indicator of how well the model
    is likely to work on data when the model is productionalized, as data in the production
    scenario (which might occur a few days/weeks after building the model) cannot
    be seen by the model
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，测试数据集上的准确性是衡量模型在实际应用中可能表现如何的一个指标，因为生产环境中的数据（可能是在构建模型几天/几周后出现的数据）是模型无法看到的
- en: Initialize a model
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个模型
- en: 'Define the model architecture:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型架构：
- en: Specify the number of units in a hidden layer
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定隐藏层中的单元数
- en: Specify the activation function that is to be performed in a hidden layer
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定在隐藏层中执行的激活函数
- en: Specify the number of hidden layers
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定隐藏层的数量
- en: Specify the loss function that we want to minimize
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定我们想要最小化的损失函数
- en: Provide the optimizer that will minimize the loss function
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供优化器，以最小化损失函数
- en: 'Fit the model:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型：
- en: Mention the batch size to update weights
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提到批量大小以更新权重
- en: Mention the total number of epochs
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提到总的训练轮数
- en: 'Test the model:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试模型：
- en: Mention the validation data, otherwise, mention the validation split, which
    will consider the last x% of total data as test data
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提到验证数据，否则提到验证分割，这将把总数据的最后x%作为测试数据
- en: Calculate the accuracy and loss values on top of the test dataset
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算测试数据集上的准确率和损失值
- en: Check for anything interesting in the way in which loss value and accuracy values
    changed over an increasing number of epochs
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查随着训练轮次增加，损失值和准确性值变化中是否有任何有趣的现象
- en: Using this strategy, let's go ahead and build a neural network model in Keras,
    in the following section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种策略，让我们在接下来的部分中构建一个Keras神经网络模型。
- en: How to do it...
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Import the relevant packages and dataset, and visualize the input dataset:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包和数据集，并可视化输入数据集：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code, we are importing the relevant Keras files and are also
    importing the MNIST dataset (which is provided as a built-in dataset in Keras).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们导入了相关的 Keras 文件，并且还导入了 MNIST 数据集（该数据集作为 Keras 中的内置数据集提供）。
- en: 'The MNIST dataset contains images of digits where the images are of 28 x 28
    in shape. Let''s plot a few images to see what they will look like in the code
    here:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MNIST 数据集包含数字图像，每张图像的形状为 28 x 28。我们来绘制一些图像，看看它们在代码中的样子：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following screenshot shows the output of the previous code block:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了前面代码块的输出：
- en: '![](img/97a5b714-f5d0-493b-a367-5565bf21d112.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/97a5b714-f5d0-493b-a367-5565bf21d112.png)'
- en: 'Flatten the 28 x 28 images so that the input is all the 784 pixel values. Additionally,
    one-hot encode the outputs. This step is key in the dataset preparation process:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 28 x 28 的图像展平，使得输入为所有 784 个像素值。此外，对输出进行 one-hot 编码。这一步骤在数据集准备过程中非常关键：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the preceding step, we are reshaping the input dataset using the reshape
    method that converts an array of a given shape into a different shape. In this
    specific case, we are converting an array that has an `X_train.shape[0]` number
    of data points (images) where there are `X_train.shape[1]` rows and `X_train.shape[2]`
    columns in each image, into an array of an `X_train.shape[0]` number of data points
    (images) and `X_train.shape[1] * X_train.shape[2]` values per image. Similarly,
    we perform the same exercise on the test dataset:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们使用 reshape 方法对输入数据集进行重新调整形状，该方法将给定形状的数组转换为不同的形状。在这个特定的例子中，我们将具有 `X_train.shape[0]`
    个数据点（图像）的数组，其中每个图像有 `X_train.shape[1]` 行和 `X_train.shape[2]` 列，转换为一个包含 `X_train.shape[0]`
    个数据点（图像）和每个图像 `X_train.shape[1] * X_train.shape[2]` 个值的数组。类似地，我们对测试数据集执行相同的操作：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s try to understand how one-hot encoding works. If the unique possible
    labels are *{0, 1, 2, 3}*, they will be one-hot encoded, as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解 one-hot 编码是如何工作的。如果唯一可能的标签是 *{0, 1, 2, 3}*，它们将被 one-hot 编码，如下所示：
- en: '| **Label** | **0** | **1** | **2** | **3** |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| **Label** | **0** | **1** | **2** | **3** |'
- en: '| **0** | 1 | 0 | 0 | 0 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 1 | 0 | 0 | 0 |'
- en: '| **1** | 0 | 1 | 0 | 0 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0 | 1 | 0 | 0 |'
- en: '| **2** | 0 | 0 | 1 | 0 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 0 | 0 | 1 | 0 |'
- en: '| **3** | 0 | 0 | 0 | 1 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 0 | 0 | 0 | 1 |'
- en: Essentially, each label will occupy a unique column in the dataset, and if the
    label is present, the column value will be one, and every other column value will
    be zero.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，每个标签将占据数据集中的一个唯一列，如果标签存在，该列的值将为 1，其他所有列的值将为 0。
- en: In Keras, the one-hot encoding approach on top of labels is performed using
    the `to_categorical` method, which figures out the number of unique labels in
    the target data, and then converts them into a one-hot encoded vector.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，基于标签的 one-hot 编码是通过 `to_categorical` 方法实现的，该方法会计算目标数据中的唯一标签数，并将其转换为
    one-hot 编码向量。
- en: 'Build a neural network with a hidden layer with 1,000 units:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个具有 1,000 个单元的隐藏层神经网络：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In the preceding step, we mention that the input has 784 values that are connected
    to 1,000 values in a hidden layer. Additionally, we are also specifying that the
    activation, which is to be performed in the hidden layer after the matrix multiplication
    of the input and the weights connecting the input and hidden layer, is the ReLu
    activation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们提到输入有 784 个值，这些值连接到隐藏层中的 1,000 个值。此外，我们还指定了在输入与连接输入和隐藏层的权重矩阵相乘后，要在隐藏层执行的激活函数是
    ReLu 激活函数。
- en: Finally, the hidden layer is connected to an output that has 10 values (as there
    are 10 columns in the vector created by the `to_categorical` method), and we perform
    softmax on top of the output so that we obtain the probability of an image belonging
    to a certain class.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，隐藏层与一个具有 10 个值的输出连接（因为 `to_categorical` 方法创建的向量有 10 列），并且我们在输出上执行 softmax，以便获得图像属于某一类别的概率。
- en: 'The preceding model architecture can be visualized as follows:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前述的模型架构可以如下可视化：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A summary of model is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的总结如下：
- en: '![](img/02ffb258-590f-499a-a9b7-6d7f2bd3b7ee.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02ffb258-590f-499a-a9b7-6d7f2bd3b7ee.png)'
- en: In the preceding architecture, the number of parameters in the first layer is
    785,000, as the 784 input units are connected to 1,000 hidden units, resulting
    in 784 * 1,000 weight values, and 1,000 bias values, for the 1,000 hidden units,
    resulting in a total of 785,000 parameters.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的架构中，第一层的参数数量为 785,000，因为 784 个输入单元连接到 1,000 个隐藏单元，导致 784 * 1,000 个权重值，以及
    1,000 个偏置值，最终得到 785,000 个参数。
- en: Similarly, the output layer has 10 outputs, which are connected to each of the
    1,000 hidden layers, resulting in 1,000 * 10 parameters and 10 biases—a total
    of 10,010 parameters.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，输出层有10个输出，它们与每个1,000个隐藏层连接，从而产生1,000 * 10个参数和10个偏置——总共10,010个参数。
- en: The output layer has 10 units as there are 10 possible labels in the output.
    The output layer now gives us a probability value for each class for a given input
    image.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层有10个单元，因为输出中有10个可能的标签。输出层现在为每个类提供给定输入图像的概率值。
- en: 'Compile the model as follows:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示编译模型：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that because the target variable is a one-hot encoded vector with multiple
    classes in it, the loss function will be a categorical cross-entropy loss.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于目标变量是一个包含多个类别的独热编码向量，因此损失函数将是分类交叉熵损失。
- en: Additionally, we are using the Adam optimizer to minimize the cost function
    (more on different optimizers in the *Varying the loss optimizer to improve network
    accuracy* recipe).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使用Adam优化器来最小化代价函数（关于不同优化器的更多信息请参考*通过改变损失优化器来提升网络准确性*配方）。
- en: We are also noting that we will need to look at the accuracy metric while the
    model is getting trained.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到，在模型训练过程中，我们需要查看准确率指标。
- en: 'Fit the model as follows:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示拟合模型：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding code, we have specified the input (`X_train`) and the output
    (`y_train`) that the model will fit. Additionally, we also specify the input and
    output of the test dataset, which the model will not use to train weights; however
    it, will give us an idea of how different the loss value and accuracy values are
    between the training and the test datasets.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们已指定模型将拟合的输入（`X_train`）和输出（`y_train`）。此外，我们还指定了测试数据集的输入和输出，模型不会使用这些数据来训练权重；然而，它将帮助我们了解训练数据集和测试数据集之间损失值和准确率值的差异。
- en: 'Extract the training and test loss and accuracy metrics over different epochs:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取不同训练轮次下的训练和测试损失以及准确率指标：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: While fitting a model, the history variable will have stored the accuracy and
    loss values corresponding to the model in each epoch for both the training and
    the test datasets. In the preceding steps, we are storing those values in a list
    so that we can plot the variation of accuracy and loss in both the training and
    test datasets over an increasing number of epochs.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合模型时，history变量会存储每个训练轮次中训练数据集和测试数据集的准确率和损失值。在前面的步骤中，我们将这些值存储在一个列表中，以便绘制随着轮次增加，训练和测试数据集的准确率和损失的变化。
- en: 'Visualize the training and test loss and the accuracy over a different number
    of epochs:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化不同训练轮次下的训练和测试损失以及准确率：
- en: '[PRE9]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code produces the following diagram, where the first plot shows
    the training and test loss values over increasing epochs, and the second plot
    shows the training and test accuracy over increasing epochs:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了以下图表，其中第一个图显示了随着训练轮次增加的训练和测试损失值，第二个图显示了随着训练轮次增加的训练和测试准确率：
- en: '![](img/3d56bf42-6d83-4844-a9b7-06ea6ad79d71.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3d56bf42-6d83-4844-a9b7-06ea6ad79d71.png)'
- en: Note that the previous network resulted in an accuracy of 97%. Also, note that
    loss values (and thereby, accuracy) have a step change over a different number
    of epochs. We will contrast this change in loss with the scenario when the input
    dataset is scaled in the next section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的网络达到了97%的准确率。还要注意，损失值（从而准确率）在不同的训练轮次中发生了阶跃变化。我们将在下一部分对比输入数据集缩放前后的损失变化。
- en: 'Let''s calculate the accuracy of the model manually:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 手动计算模型的准确性：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the preceding step, we are using the `predict` method to calculate the expected
    output values for a given input (`X_test` in this case) to the model. Note that
    we are specifying it as `model.predict`, as we have initialized a sequential model
    named `model`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们使用`predict`方法来计算给定输入（在此案例中为`X_test`）的预期输出值。请注意，我们将其指定为`model.predict`，因为我们已初始化了一个名为`model`的顺序模型：
- en: '[PRE11]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code, we are looping all of the test predictions one at a time.
    For each test prediction, we are perming `argmax` to obtain the index that has
    the highest probability value.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在逐一遍历所有测试预测。对于每个测试预测，我们执行`argmax`来获取具有最高概率值的索引。
- en: Similarly, we perform the same exercise for the actual values of the test dataset.
    The prediction of the index of the highest value is the same in both the prediction
    and the actual values of the test dataset.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们对测试数据集的实际值执行相同的操作。预测最高值的索引在预测值和测试数据集的实际值中是相同的。
- en: Finally, the number of correct predictions over the total number of data points
    in the test dataset is the accuracy of the model on the test dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，模型在测试数据集上的准确率是正确预测的数量除以测试数据集中的总数据点数。
- en: How it works...
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The key steps that we have performed in the preceding code are as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的代码中执行的关键步骤如下：
- en: We flattened the input dataset so that each pixel is considered a variable using
    the `reshape` method
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`reshape`方法将输入数据集展平，使每个像素都被视为一个变量。
- en: We performed one-hot encoding on the output values so that we can distinguish
    between different labels using the `to_categorical` method in the `np_utils` package
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对输出值进行了独热编码，以便使用`np_utils`包中的`to_categorical`方法区分不同的标签。
- en: We built a neural network with a hidden layer using the sequential addition
    of layers
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过逐层添加的方式构建了一个带有隐藏层的神经网络。
- en: We compiled the neural network to minimize the categorical cross entropy loss
    (as the output has 10 different categories) using the `model.compile` method
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`model.compile`方法编译了神经网络，以最小化分类交叉熵损失（因为输出有10个不同的类别）。
- en: We fitted the model with training data using the `model.fit` method
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`model.fit`方法通过训练数据拟合模型。
- en: We extracted the training and test loss accuracies across all the epochs that
    were stored in the history
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们提取了存储在历史中的所有纪元的训练和测试损失准确率。
- en: We predicted the probability of each class in the test dataset using the `model.predict`
    method
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`model.predict`方法预测了测试数据集中每个类别的概率。
- en: We looped through all the images in the test dataset and identified the class
    that has the highest probability
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们遍历了测试数据集中的所有图像，并识别出具有最高概率的类别。
- en: Finally, we calculated the accuracy (the number of instances in which a predicted
    class matches the actual class of the image out of the total number of instances)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终，我们计算了准确率（预测类别与图像实际类别匹配的实例数量占总实例数的比例）。
- en: In the next section, we will look at the reasons for the step change in the
    loss and accuracy values, and move toward making the change more smooth.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨损失和准确率值发生阶跃变化的原因，并努力使变化更加平滑。
- en: Scaling the input dataset
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缩放输入数据集
- en: Scaling a dataset is a process where we limit the variables within a dataset
    to ensure they do not have a very wide range of different values. One way to achieve
    this is to divide each variable in the dataset by the maximum value of the variable.
    Typically, neural networks perform well when we scale the input datasets.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放数据集是一个过程，其中我们限制数据集中变量的范围，确保它们没有非常宽泛的不同值。一种实现方法是将数据集中的每个变量除以该变量的最大值。通常情况下，神经网络在我们缩放输入数据集时表现较好。
- en: In this section, let's understand the reason neural networks perform better
    when the dataset is scaled.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，让我们了解为什么当数据集被缩放时，神经网络表现得更好。
- en: Getting ready
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: To understand the impact of the scaling input on the output, let's contrast
    the scenario where we check the output when the input dataset is not scaled, with
    the output when the input dataset is scaled.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解输入缩放对输出的影响，我们将检查输入数据集未缩放时的输出与输入数据集缩放时的输出进行对比。
- en: 'Input data is not scaled:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据未缩放：
- en: '![](img/4c549749-a8f8-45e8-8f9c-1ab0f2077506.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4c549749-a8f8-45e8-8f9c-1ab0f2077506.jpg)'
- en: 'In the preceding table, note that the output (sigmoid) did not vary a lot,
    even though the weight value varied from 0.01 to 0.9\. The sigmoid function is
    calculated as the sigmoid value of the multiplication of the input with the weight,
    and then adding a bias to it:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表格中，注意到即使权重值从0.01变化到0.9，输出（sigmoid）变化不大。sigmoid函数是通过输入与权重的乘积计算sigmoid值，然后加上偏置：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Where `w` is the weight, `x` is the input, and `b` is the bias value.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`w`是权重，`x`是输入，`b`是偏置值。
- en: The reason for no change in the sigmoid output is due to the fact that the multiplication
    of  `w*x` is a large number (as x is a large number) resulting in the sigmoid
    value always falling in the saturated portion of the sigmoid curve (saturated
    value on the top-right or bottom-left of the sigmoid curve).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid 输出没有变化的原因是，因为 `w*x` 的乘积是一个大数（因为 x 是一个大数），导致 sigmoid 值总是落在 sigmoid 曲线的饱和部分（位于
    sigmoid 曲线的右上角或左下角的饱和值）。
- en: 'In this scenario, let''s multiply different weight values by a small input
    number, as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，让我们将不同的权重值与一个小的输入数值相乘，如下所示：
- en: '![](img/7de39d87-5f55-4a9c-ad30-771958d1d0ce.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7de39d87-5f55-4a9c-ad30-771958d1d0ce.png)'
- en: The sigmoid output in the preceding table varies, as the input and weight values
    are small, resulting in a smaller value when the input and the weight are multiplied,
    further resulting in the sigmoid value having variation in output.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表格中，sigmoid 输出有所不同，因为输入和权重值较小，导致输入与权重相乘时得到较小的结果，进而导致 sigmoid 输出出现变化。
- en: From this exercise, we learned about the importance of scaling the input dataset
    so that it results in a smaller value when the weights (provided the weights do
    not have a high range) are multiplied by the input values. This phenomenon results
    in the weight value not getting updated quickly enough.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这次练习，我们了解了缩放输入数据集的重要性，这样在权重（前提是权重范围不大）的值与输入值相乘时会得到一个较小的结果。这种现象导致权重值更新不够迅速。
- en: Thus, to achieve the optimal weight value, we should scale our input dataset
    while initializing the weights to not have a huge range (typically, weights have
    a random value between -1 and +1 during initialization).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了达到最佳的权重值，我们应该缩放输入数据集，同时初始化权重值时不应有过大的范围（通常，在初始化时，权重值是一个介于 -1 和 +1 之间的随机值）。
- en: These issues hold true when the weight value is also a very big number. Hence,
    we are better off initializing the weight values as a small value that is closer
    to zero.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当权重值也是一个非常大的数字时，这些问题仍然存在。因此，我们最好将权重值初始化为一个接近零的小值。
- en: How to do it...
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s go through the set up of scaling the dataset that we have used in the
    previous section, and compare the results with and without scaling:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下上节中使用的数据集缩放设置，并比较有无缩放的结果：
- en: 'Import the relevant packages and datasets:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入相关的包和数据集：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: There are multiple ways to scale a dataset. One way is to convert all the data
    points to a value between zero and one (by dividing each data point with the maximum
    value in the total dataset, which is what we are doing in the following code).
    Another popular method, among the multiple other ways, is to normalize the dataset
    so that the values are between -1 and +1 by subtracting each data point with the
    overall dataset mean, and then dividing each resulting data point by the standard
    deviation of values in the original dataset.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 缩放数据集有多种方法。一种方法是将所有数据点转换为介于零和一之间的值（通过将每个数据点除以整个数据集的最大值，这就是我们在以下代码中做的）。另一种流行的方式（在多种方法中）是对数据集进行标准化，使得值介于
    -1 和 +1 之间，方法是用每个数据点减去整个数据集的均值，然后将每个结果数据点除以原始数据集中的标准差。
- en: 'Now we will be flattening the input dataset and scaling it, as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将对输入数据集进行平展并缩放，如下所示：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding step, we have scaled the training and test inputs to a value
    between zero and one by dividing each value by the maximum possible value in the
    dataset, which is 255\. Additionally, we convert the output dataset into a one-hot
    encoded format:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步中，我们通过将每个值除以数据集中的最大值（255），将训练和测试输入缩放到一个介于零和一之间的值。此外，我们将输出数据集转换为 one-hot
    编码格式：
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Build the model and compile it using the following code:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码构建模型并进行编译：
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that the preceding model is exactly the same as the one we built in the
    previous section. However, the only difference is that it will be executed on
    the training dataset that is scaled, whereas the previous one was not scaled.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前述模型与我们在上一节中构建的模型完全相同。唯一的区别是，它将在经过缩放的训练数据集上执行，而之前的模型没有经过缩放。
- en: 'Fit the model as follows:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式拟合模型：
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You will notice that the accuracy of the preceding model is ~98.25%.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，前述模型的准确率大约是 98.25%。
- en: 'Plot the training and test accuracy and the loss values over different epochs
    (the code to generate the following plots remains the same as the one we used
    in step 8 of the *Training a vanilla neural network* recipe):'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制训练和测试准确度以及不同时期的损失值（生成以下图表的代码与我们在*训练普通神经网络*食谱的第8步中使用的代码相同）：
- en: '![](img/4affc74a-14b2-4085-9407-5da31fb07f4b.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4affc74a-14b2-4085-9407-5da31fb07f4b.png)'
- en: From the preceding diagram, you should notice that training and test losses
    decreased smoothly over increasing epochs when compared to the non-scaled dataset
    that we saw in the previous section.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，你应该注意到，与上一部分我们看到的未缩放数据集相比，训练和测试损失在增加的时期中平滑下降。
- en: While the preceding network gave us good results in terms of a smoothly decreasing
    loss value, we noticed that there is a gap between the training and test accuracy/loss
    values, indicating that there is potential overfitting on top of the training
    dataset. **Overfitting** is the phenomenon where the model specializes on the
    training data that it might not work as well on the test dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前述网络在平滑下降的损失值方面给出了良好的结果，但我们注意到训练准确度和测试准确度/损失值之间存在差距，表明在训练数据集上可能存在过拟合现象。**过拟合**是指模型过度专注于训练数据，导致在测试数据集上表现不佳的现象。
- en: How it works...
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The key steps that we have performed in the preceding code are as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的代码中执行的关键步骤如下：
- en: We flattened the input dataset so that each pixel is considered a variable using
    the reshape method
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 reshape 方法将输入数据集展平，以便每个像素被视为一个变量
- en: Additionally, we scaled the dataset so that each variable now has a value between
    zero and one
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，我们对数据集进行了缩放，使得每个变量的值现在都在零和一之间
- en: We achieved the preceding by dividing the values of a variable with the maximum
    value of that variable
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过将变量的值除以该变量的最大值来实现前述操作
- en: We performed one-hot encoding on the output values so that we can distinguish
    between different labels using the `to_categorical` method in the `np_utils` package
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对输出值进行了独热编码，以便使用 `np_utils` 包中的 `to_categorical` 方法区分不同的标签
- en: We built a neural network with a hidden layer using the sequential addition
    of layers
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过顺序添加层的方式构建了一个具有隐藏层的神经网络
- en: We compiled the neural network to minimize categorical cross entropy loss (as
    the output has 10 different categories) using the `model.compile` method
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 `model.compile` 方法将神经网络编译为最小化类别交叉熵损失（因为输出有10个不同的类别）
- en: We fitted the model with training data using the `model.fit` method
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 `model.fit` 方法通过训练数据来拟合模型
- en: We extracted the training and test losses accuracies across all the epochs that
    were stored in the history
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们提取了在所有时期中存储在历史记录中的训练和测试损失准确度
- en: We also identified a scenario that we consider overfitting
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还识别了一个我们认为是过拟合的场景
- en: There's more...
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'In addition to scaling a variable''s values by dividing the values by the maximum
    among the values in a variable, the other commonly used scaling methods are as
    follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过将变量的值除以该变量的最大值来缩放变量值外，其他常用的缩放方法如下：
- en: Min-max normalization
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小-最大归一化
- en: Mean normalization
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值归一化
- en: Standardization
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化
- en: More information about these scaling methods can be found on Wikipedia here: [https://en.wikipedia.org/wiki/Feature_scaling](https://en.wikipedia.org/wiki/Feature_scaling).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有关这些缩放方法的更多信息，请访问维基百科：[https://en.wikipedia.org/wiki/Feature_scaling](https://en.wikipedia.org/wiki/Feature_scaling)。
- en: Impact on training when the majority of inputs are greater than zero
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当大多数输入值大于零时对训练的影响
- en: So far, in the dataset that we have considered, we have not looked at the distribution
    of values in the input dataset. Certain values of the input result in faster training.
    In this section, we will understand a scenario where weights are trained faster
    when the training time depends on the input values.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在我们考虑的数据集中，我们并未查看输入数据集中的值的分布。输入的某些值导致训练速度更快。在本节中，我们将了解在训练时间依赖于输入值时，权重训练速度更快的情况。
- en: Getting ready
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: In this section, we will follow the model-building process in exactly the same
    way as we did in the previous section.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将完全按照与上一节相同的方式进行模型构建。
- en: 'However, we will adopt a small change to our strategy:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们将对我们的策略做出一些小的调整：
- en: We will invert the background color, and also the foreground color. Essentially,
    the background will be colored white in this scenario, and the label will be written
    in black.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将反转背景颜色，以及前景颜色。本质上，在这种情况下，背景将是白色的，标签将用黑色书写。
- en: The intuition for this change impacting the model accuracy is as follows.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这种变化影响模型准确性的直觉如下。
- en: The pixels in the corner of images do not contribute toward predicting the label
    of an image. Given that a black pixel (original scenario) has a pixel value of
    zero, it is automatically taken care of, as when this input is multiplied by any
    weight value, the output is zero. This will result in the network learning that
    any change in the weight value connecting this corner pixel to a hidden layer
    will not have an impact on changing the loss value.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图像角落的像素对预测图像标签没有贡献。考虑到黑色像素（原始场景）的像素值为零，它会自动被处理，因为当这个输入与任何权重值相乘时，输出就是零。这将导致网络学习到，连接此角落像素与隐藏层的权重值的任何变化都不会对损失值的变化产生影响。
- en: However, if we have a white pixel in the corner (where we already know that
    the corner pixels do not contribute toward predicting the label of an image),
    it will contribute toward certain hidden unit values, and thus the weights need
    to be fine-tuned until the impact of the corner pixels on the predicted label
    is minimal.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果角落的像素是白色的（我们已经知道角落的像素对预测图像标签没有贡献），它将对某些隐藏单元的值产生影响，因此权重需要微调，直到角落像素对预测标签的影响最小。
- en: How to do it...
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Load and scale the input dataset:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载并缩放输入数据集：
- en: '[PRE18]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let''s look at the distribution of the input values:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看输入值的分布：
- en: '[PRE19]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code flattens all the inputs into a single list, and hence, is
    of the shape (47,040,000), which is the same as the `28 x 28 x X_train.shape[0]`.
    Let''s plot the distribution of all the input values:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将所有输入值展平为一个单一的列表，因此其形状为 (47,040,000)，与 `28 x 28 x X_train.shape[0]` 相同。接下来，让我们绘制所有输入值的分布：
- en: '[PRE20]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/aeb37d82-2cee-490f-b933-4cfe855c3975.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aeb37d82-2cee-490f-b933-4cfe855c3975.png)'
- en: We notice that the majority of the inputs are zero (you should note that all
    the input images have a background that is black hence, a majority of the values
    are zero, which is the pixel value of the color black).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到大多数输入值为零（你应该注意到所有输入图像的背景都是黑色的，因此，大多数值为零，这是黑色的像素值）。
- en: 'In this section, let''s explore a scenario where we invert the colors, in which
    the background is white and the letters are written in black, using the following
    code:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本节中，让我们探索一个场景，其中我们反转颜色，背景为白色，字母为黑色，使用以下代码：
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s plot the images:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这些图像：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'They will look as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 它们将如下所示：
- en: '![](img/41d3b675-5acf-4a2a-830c-d911857cb189.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41d3b675-5acf-4a2a-830c-d911857cb189.png)'
- en: 'The histogram of the resulting images now looks as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图像的直方图现在如下所示：
- en: '![](img/c8f2c774-2d0e-47dd-9324-7e199534c934.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8f2c774-2d0e-47dd-9324-7e199534c934.png)'
- en: You should notice that the majority of the input values now have a value of
    one.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意到，大多数输入值现在的值为一。
- en: 'Let''s go ahead and build our model using the same model architecture that
    we built in the S*caling input dataset* section:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用在*S*caling 输入数据集*部分中构建的相同模型架构来构建我们的模型：
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Plot the training and test accuracy and loss values over different epochs (the
    code to generate the following plots remains the same as the one we used in step
    8 of the *Training a vanilla neural network* recipe):'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制不同训练周期（epochs）下的训练和测试准确率及损失值（生成以下图表的代码与我们在*训练普通神经网络*步骤 8 中使用的代码相同）：
- en: '![](img/bf81c069-33e3-44f5-905f-76c8bcb34578.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf81c069-33e3-44f5-905f-76c8bcb34578.png)'
- en: We should note that model accuracy has now fallen to ~97%, compared to ~98%
    when using the same model for the same number of epochs and batch size, but on
    a dataset that has a majority of zeros (and not a majority of ones). Additionally,
    the model achieved an accuracy of 97%, considerably more slowly than in the scenario
    where the majority of the input pixels are zero.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到，模型准确率现在已下降到约 97%，而使用相同的模型进行相同数量的周期和批量大小时，在一个大多数值为零（而不是大多数为一）的数据集上，准确率为约
    98%。此外，模型的准确率为 97%，比输入像素大多数为零的场景要慢得多。
- en: The intuition for the decrease in accuracy, when the majority of the data points
    are non-zero is that, when the majority of pixels are zero, the model's task was
    easier (less weights had to be fine-tuned), as it had to make predictions based
    on a few pixel values (the minority that had a pixel value greater than zero).
    However, a higher number of weights need to be fine-tuned to make predictions
    when a majority of the data points are non-zero.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当大多数数据点为非零时，准确度下降的直觉是：当大多数像素为零时，模型的任务较简单（需要调整的权重较少），因为它只需根据少数像素值（那些像素值大于零的）进行预测。然而，当大多数数据点为非零时，模型需要调整更多的权重以进行预测。
- en: Impact of batch size on model accuracy
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量大小对模型准确度的影响
- en: In the previous sections, for all the models that we have built, we considered
    a batch size of 32\. In this section, we will try to understand the impact of
    varying the batch size on accuracy.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的各个部分中，我们为所有构建的模型都考虑了批量大小为32。在这一部分中，我们将尝试理解批量大小对准确度的影响。
- en: Getting ready
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: 'To understand the reason batch size has an impact on model accuracy, let''s
    contrast two scenarios where the total dataset size is 60,000:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解批量大小对模型准确度的影响，让我们对比两个情境，其中数据集总大小为60,000：
- en: Batch size is 30,000
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小为30,000
- en: Batch size is 32
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大小为32
- en: When the batch size is large, the number of times of weight update per epoch
    is small, when compared to the scenario when the batch size is small.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当批量大小较大时，每个训练轮次中的权重更新次数较少，相较于批量大小较小的情境。
- en: The reason for a high number of weight updates per epoch when the batch size
    is small is that less data points are considered to calculate the loss value.
    This results in more batches per epoch, as, loosely, in an epoch, you would have
    to go through all the training data points in a dataset.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小较小时，每个训练轮次中权重更新次数较多的原因是，计算损失值时考虑的数据点较少。这导致每个训练轮次中批量的数量增多，因为大致而言，在一个训练轮次中，您需要遍历数据集中的所有训练数据点。
- en: Thus, the lower the batch size, the better the accuracy for the same number
    of epochs. However, while deciding the number of data points to be considered
    for a batch size, you should also ensure that the batch size is not too small
    so that it might overfit on top of a small batch of data.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，批量大小越小，在相同的训练轮次下准确度越高。然而，在决定用于批量大小的数据点数量时，您还应确保批量大小不宜过小，以免在小批量数据上发生过拟合。
- en: How to do it...
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'In the previous recipe, we built a model with a batch size of 32\. In this
    recipe, we will go ahead and implement the model to contrast the scenario between
    a low batch size and a high batch size for the same number of epochs:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的步骤中，我们建立了一个批量大小为32的模型。在这个步骤中，我们将继续实现模型，并对比低批量大小和高批量大小在相同训练轮次下的情境：
- en: 'Preprocess the dataset and fit the model as follows:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下步骤预处理数据集并拟合模型：
- en: '[PRE24]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that the only change in code is the `batch_size` parameter in the model
    fit process.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，代码中的唯一变化是在模型拟合过程中使用的`batch_size`参数。
- en: 'Plot the training and test accuracy and loss values over different epochs (the
    code to generate the following plots remains the same as the code we used in step
    8 of the *Training a vanilla neural network* recipe):'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制不同训练轮次下的训练准确度、测试准确度和损失值（生成以下图表的代码与我们在*训练基本神经网络*步骤8中使用的代码相同）：
- en: '![](img/8045caed-04a4-41b0-a58d-5aaafba99a70.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8045caed-04a4-41b0-a58d-5aaafba99a70.png)'
- en: In the preceding scenario, you should notice that the model accuracy reached
    ~98% at a much later epoch, when compared to the model accuracy it reached when
    the batch size was smaller.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的情境中，您应该注意到，与批量大小较小的模型相比，模型准确度在较晚的训练轮次才达到约98%。
- en: How it works...
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: You should notice that the accuracy is much lower initially and that it catches
    up only after a considerable number of epochs are run. The reason for a low accuracy
    during initial epochs is that the number of times of weight update is much lower
    in this scenario when compared to the previous scenario (where the batch size
    was smaller).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该注意到，最初准确度较低，只有在经过相当数量的训练轮次后才会赶上。初期准确度较低的原因是，在这种情况下，权重更新的次数远低于之前的情境（批量大小较小的情境）。
- en: In this scenario, when the batch size is 30,000, and the total dataset size
    is 60,000, when we run the model for 500 epochs, the weight updates happens at
    epochs * (dataset size/ batch size) = 500 * (60,000/30,000) = 1,000 times.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，当批次大小为30,000，总数据集大小为60,000时，当我们运行500个周期时，权重更新会发生在周期 * (数据集大小 / 批次大小)
    = 500 * (60,000 / 30,000) = 1,000次。
- en: In the previous scenario, the weight updates happens at 500 * (60,000/32) =
    937,500 times.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的场景中，权重更新发生在500 * (60,000 / 32) = 937,500次。
- en: Hence, the lower the batch size, the more times the weights get updated and,
    generally, the better the accuracy is for the same number of epochs.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，批次大小越小，权重更新的次数就越多，一般来说，对于相同数量的周期，准确率会更好。
- en: At the same time, you should be careful not to have too few examples in the
    batch size, which might result in not only having a very long training time, but
    also a potential overfitting scenario.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，你应该小心不要让批次大小过小，这可能导致不仅训练时间过长，还有可能出现过拟合的情况。
- en: Building a deep neural network to improve network accuracy
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度神经网络以提高网络准确率
- en: Until now, we have looked at model architectures where the neural network has
    only one hidden layer between the input and the output layers. In this section,
    we will look at the neural network where there are multiple hidden layers (and
    hence a deep neural network), while reusing the same MNIST training and test dataset
    that were scaled.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过了神经网络模型，其中神经网络只有一个隐藏层，位于输入层和输出层之间。在这一节中，我们将讨论具有多个隐藏层（因此是深度神经网络）的神经网络，同时重用已缩放的相同MNIST训练和测试数据集。
- en: Getting ready
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: A deep neural network means that there are multiple hidden layers connecting
    the input to the output layer. Multiple hidden layers ensure that the neural network
    learns a complex non-linear relation between the input and output, which a simple
    neural network cannot learn (due to a limited number of hidden layers).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络意味着有多个隐藏层将输入层和输出层连接起来。多个隐藏层确保神经网络学习输入和输出之间复杂的非线性关系，这是简单神经网络无法学习的（因为隐藏层数量有限）。
- en: 'A typical deep feedforward neural network looks as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的深度前馈神经网络如下所示：
- en: '![](img/5b116810-f567-4e06-9f94-efdbe6f013e0.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b116810-f567-4e06-9f94-efdbe6f013e0.png)'
- en: How to do it...
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'A deep neural network architecture is built by adding multiple hidden layers
    between input and output layers, as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络架构是通过在输入层和输出层之间添加多个隐藏层来构建的，如下所示：
- en: 'Load the dataset and scale it:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集并进行缩放：
- en: '[PRE25]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Build a model with multiple hidden layers connecting the input and output layers:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个具有多个隐藏层将输入层和输出层连接起来的模型：
- en: '[PRE26]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding model architecture results in a model summary, as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 前述模型架构的结果如下所示：
- en: '![](img/f36020ac-67c3-494d-a646-f863b71527dc.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f36020ac-67c3-494d-a646-f863b71527dc.png)'
- en: Note that the preceding model results in a higher number of parameters, as a
    result of deep architectures (as there are multiple hidden layers in the model).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前述模型会导致更多的参数数量，这是由于深度架构的结果（因为模型中有多个隐藏层）。
- en: 'Now that the model is set up, let''s compile and fit the model:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在模型已经设置好，我们来编译并拟合模型：
- en: '[PRE27]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding results in a model with an accuracy of 98.6%, which is slightly
    better than the accuracies we observed with the model architectures that we saw
    earlier. The training and test loss and accuracy are as follows (the code to generate
    the plots in the following diagram remains the same as the code we used in step
    8 of the *Training a vanilla neural network* recipe):'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 前述结果得到一个98.6%的准确率，这略高于我们之前看到的模型架构的准确率。训练和测试的损失及准确率如下所示（生成以下图表的代码与我们在*训练基础神经网络*食谱第8步中使用的代码相同）：
- en: '![](img/f11ab003-caee-4343-8b8b-eadce1391e26.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f11ab003-caee-4343-8b8b-eadce1391e26.png)'
- en: Note that, in this scenario, there is a considerable gap between training and
    test loss, indicating that the deep feedforward neural network specialized on
    training data. Again, in the sections on overfitting, we will learn about ways
    to avoid overfitting on training data.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种情况下，训练损失和测试损失之间存在相当大的差距，表明深度前馈神经网络在训练数据上过拟合了。我们将在过拟合的章节中学习如何避免对训练数据的过拟合。
- en: Varying the learning rate to improve network accuracy
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整学习率以提高网络准确率
- en: So far, in the previous recipes, we used the default learning rate of the Adam
    optimizer, which is 0.0001.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在之前的食谱中，我们使用的是Adam优化器的默认学习率0.0001。
- en: In this section, we will manually set the learning rate to a higher number and
    see the impact of changing the learning rate on model accuracy, while reusing
    the same MNIST training and test dataset that were scaled in the previous recipes.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将手动将学习率设置为更大的数值，并观察改变学习率对模型准确性的影响，同时复用之前食谱中已缩放的相同MNIST训练和测试数据集。
- en: Getting ready
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: In the previous chapter on building feedforward neural networks, we learned
    that the learning rate is used in updating weights and the change in weight is
    proportional to the amount of loss reduction.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章节关于构建前馈神经网络的内容中，我们学习了学习率用于更新权重，且权重的变化与损失减少的量成正比。
- en: Additionally, a change in a weight's value is equal to the decrease in loss
    multiplied by the learning rate. Hence, the lower the learning rate, the lower
    the change in the weight value, and vice versa.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，权重值的变化等于损失减少量与学习率的乘积。因此，学习率越低，权重值的变化越小，反之亦然。
- en: You can essentially think of the weight values as a continuous spectrum where
    the weights are initialized randomly. When the change in the weight values is
    great, there is a good possibility that the various weight values in the spectrum
    are not considered. However, when the change in the weight value is slight, the
    weights might achieve a global minima, as more possible weight values could be
    considered.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以基本上将权重值视为一个连续的光谱，其中权重是随机初始化的。当权重值变化较大时，有很大可能在该光谱中并未考虑到所有可能的权重值。然而，当权重值变化较小，可能会实现全局最小值，因为可能会考虑更多的权重值。
- en: 'To understand this further, let''s consider the toy example of fitting the
    *y = 2x* line where the initial weight value is 1.477 and the initial bias value
    is zero. The feedforward and back propagation functions will remain the same as
    we saw in the previous chapter:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更进一步理解，我们来考虑拟合*y = 2x*直线的玩具示例，其中初始权重值为1.477，初始偏置值为零。前馈和反向传播函数将保持与前一章相同：
- en: '[PRE28]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Note that the only change from the backward propagation function that we saw
    in the previous chapter is that we are passing the learning rate as a parameter
    in the preceding function. The value of weight when the learning rate is 0.01
    over a different number of epochs is as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，从前一章看到的反向传播函数中唯一的变化是我们将学习率作为参数传递给了前述函数。当学习率为0.01时，在不同训练轮次下的权重值如下：
- en: '[PRE29]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The plot of the change in weight over different epochs can be obtained using
    the following code:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 不同训练轮次下权重变化的图形可以通过以下代码获得：
- en: '[PRE30]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出结果如下：
- en: '![](img/ff23c179-fb57-4c71-905c-512353516f30.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff23c179-fb57-4c71-905c-512353516f30.png)'
- en: 'In a similar manner, the value of the weight over a different number of epochs
    when the learning rate is 0.1 is as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，当学习率为0.1时，在不同训练轮次下的权重值如下：
- en: '![](img/226c0088-34ef-46bc-a399-46ce411fd70e.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](img/226c0088-34ef-46bc-a399-46ce411fd70e.png)'
- en: 'This screenshot shows the value of the weight over a different number of epochs when
    the learning rate is 0.5:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 该截图显示了当学习率为0.5时，在不同训练轮次下权重的变化值：
- en: '![](img/c6883c3e-8044-4ba1-97cc-34943ece045f.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c6883c3e-8044-4ba1-97cc-34943ece045f.png)'
- en: Note that, in the preceding scenario, there was a drastic change in the weight
    values initially, and the 0.1 learning rate converged, while the 0.5 learning
    rate did not converge to an optimal solution, and thus became stuck in a local
    minima.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前述场景中，最初权重值发生了剧烈变化，而学习率为0.1时收敛了，而学习率为0.5时未能收敛到最优解，因此陷入了局部最小值。
- en: In the case when the learning rate was 0.5, given the weight value was stuck
    in a local minima, it could not reach the optimal value of two.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习率为0.5的情况下，由于权重值停滞在局部最小值，它无法达到最优值2。
- en: How to do it...
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行...
- en: Now that we understand how learning rate influences the output values, let's
    see the impact of the learning rate in action on the MNIST dataset we saw earlier,
    where we keep the same model architecture but will only be changing the learning
    rate parameter.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了学习率如何影响输出值，让我们看看学习率对之前看到的MNIST数据集的实际影响，我们将保持相同的模型架构，但只改变学习率参数。
- en: Note that we will be using the same data-preprocessing steps as those of step
    1 and step 2 in the *Scaling input dataset* recipe.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将使用与*缩放输入数据集*食谱中第1步和第2步相同的数据预处理步骤。
- en: 'Once we have the dataset preprocessed, we vary the learning rate of the model
    by specifying the optimizer in the next step:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集预处理完成，我们通过在下一步中指定优化器来调整模型的学习率：
- en: 'We change the learning rate as follows:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按如下方式调整学习率：
- en: '[PRE31]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With the preceding code, we have initialized the Adam optimizer with a specified
    learning rate of 0.01.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前述代码，我们已使用指定的学习率0.01初始化了Adam优化器。
- en: 'We build, compile, and fit the model as follows:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按照如下方式构建、编译并训练模型：
- en: '[PRE32]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The accuracy of the preceding network is ~90% at the end of 500 epochs. Let''s
    have a look at how loss function and accuracy vary over a different number of
    epochs (the code to generate the plots in the following diagram remains the same
    as the code we used in step 8 of the *Training a vanilla neural network* recipe):'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 前述网络的准确率在500个训练轮次结束时约为90%。让我们看看损失函数和准确率在不同训练轮次下的变化（生成下图的代码与我们在*训练基础神经网络*方法第8步中使用的代码相同）：
- en: '![](img/1b584542-f5c3-48b0-9af0-1855dea2f4c7.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1b584542-f5c3-48b0-9af0-1855dea2f4c7.png)'
- en: Note that when the learning rate was high (0.01 in the current scenario) compared
    to 0.0001 (in the scenario considered in the *Scaling input dataset* recipe),
    the loss decreased less smoothly when compared to the low-learning-rate model.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当学习率较高（在当前情况下为0.01）时，与0.0001（在*缩放输入数据集*方法中考虑的情况）相比，损失函数下降得不如低学习率模型平稳。
- en: The low-learning-rate model updates the weights slowly, thereby resulting in
    a smoothly reducing loss function, as well as a high accuracy, which was achieved
    slowly over a higher number of epochs.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 低学习率的模型更新权重较慢，因此导致损失函数平稳下降，并且准确率较高，这种准确率是在更多的训练轮次中逐渐获得的。
- en: Alternatively, the step changes in loss values when the learning rate is higher
    are due to the loss values getting stuck in a local minima until the weight values
    change to optimal values. A lower learning rate gives a better possibility of
    arriving at the optimal weight values faster, as the weights are changed slowly,
    but steadily, in the right direction.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，当学习率较高时，损失值的步进变化是由于损失值陷入局部最小值，直到权重值改变为最优值。较低的学习率可以更快地到达最优权重值，因为权重变化较慢，但在正确的方向上稳步前进。
- en: 'In a similar manner, let''s explore the network accuracy when the learning
    rate is as high as 0.1:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，让我们探讨当学习率为0.1时网络的准确性：
- en: '[PRE33]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'It is to be noted that the loss values could not decrease much further, as
    the learning rate was high; that is, potentially the weights got stuck in a local
    minima:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，由于学习率较高，损失值无法进一步显著下降；也就是说，可能权重已经陷入局部最小值：
- en: '![](img/2c8659c9-e4e3-4524-9dfd-5f6661fd3b96.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c8659c9-e4e3-4524-9dfd-5f6661fd3b96.png)'
- en: Thus, it is, in general, a good idea to set the learning rate to a low value
    and let the network learn over a high number of epochs.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一般来说，设置较低的学习率并让网络在较多的训练轮次中进行学习是一个好主意。
- en: Varying the loss optimizer to improve network accuracy
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改变损失优化器以提高网络准确性
- en: So far, in the previous recipes, we considered the loss optimizer to be the
    Adam optimizer. However, there are multiple other variants of optimizers, and
    a change in the optimizer is likely to impact the speed with which the model learns
    to fit the input and the output.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在前面的几种方法中，我们认为损失优化器是Adam优化器。然而，优化器有多种变体，优化器的更改可能会影响模型学习拟合输入与输出的速度。
- en: In this recipe, we will understand the impact of changing the optimizer on model
    accuracy.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本方法中，我们将理解改变优化器对模型准确性的影响。
- en: Getting ready
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'To understand the impact of varying the optimizer on network accuracy, let''s
    contrast the scenario laid out in previous sections (which was the Adam optimizer)
    with using a **stochastic gradient descent optimizer** in this section, while reusing
    the same MNIST training and test datasets that were scaled (the same data-preprocessing
    steps as those of step 1 and step 2 in the *Scaling the dataset* recipe):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解优化器变化对网络准确性的影响，下面我们将对比前面章节中的情况（使用的是Adam优化器）和这一节使用**随机梯度下降优化器**的情况，同时重新使用已缩放的相同MNIST训练和测试数据集（数据预处理步骤与*缩放数据集*方法中的步骤1和步骤2相同）：
- en: '[PRE34]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Note that when we used the stochastic gradient descent optimizer in the preceding
    code, the final accuracy after 100 epochs is ~98% (the code to generate the plots
    in the following diagram remains the same as the code we used in step 8 of the *Training
    a vanilla neural network* recipe):'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们在前面的代码中使用随机梯度下降优化器时，经过100轮训练后的最终准确率大约为98%（以下图表的生成代码与我们在*训练一个基础神经网络*食谱的第8步中使用的代码相同）：
- en: '![](img/38cdae47-275b-46d5-b662-fc29d5f85df3.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/38cdae47-275b-46d5-b662-fc29d5f85df3.png)'
- en: However, we should also note that the model achieved the high accuracy levels
    much more slowly when compared to the model that used Adam optimization.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们也应该注意，与使用Adam优化器的模型相比，该模型在达到高准确率的过程中变得更加缓慢。
- en: There's more...
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Some of the other loss optimizers available are as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些可用的损失优化器如下：
- en: RMSprop
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSprop
- en: Adagrad
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adagrad
- en: Adadelta
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adadelta
- en: Adamax
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adamax
- en: Nadam
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nadam
- en: You can learn more about the various optimizers here: [https://keras.io/optimizers/](https://keras.io/optimizers/).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里了解更多关于不同优化器的信息：[https://keras.io/optimizers/](https://keras.io/optimizers/)。
- en: Additionally, you can find the source code of each optimizer here: [https://github.com/keras-team/keras/blob/master/keras/optimizers.py](https://github.com/keras-team/keras/blob/master/keras/optimizers.py).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还可以在这里找到每个优化器的源代码：[https://github.com/keras-team/keras/blob/master/keras/optimizers.py](https://github.com/keras-team/keras/blob/master/keras/optimizers.py)。
- en: Understanding the scenario of overfitting
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解过拟合的情形
- en: In some of the previous recipes, we have noticed that the training accuracy
    is ~100%, while test accuracy is ~98%, which is a case of overfitting on top of
    a training dataset. Let's gain an intuition of the delta between the training
    and the test accuracies.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的一些食谱中，我们注意到训练准确率接近100%，而测试准确率约为98%，这就是在训练数据集上发生过拟合的情况。让我们对训练和测试准确率之间的差异有一个直观的理解。
- en: 'To understand the phenomenon resulting in overfitting, let''s contrast two
    scenarios where we compare the training and test accuracies along with a histogram
    of the weights:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解导致过拟合的现象，让我们对比两个情形，分别比较训练和测试的准确率，并且展示权重的直方图：
- en: Model is run for five epochs
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型运行五轮训练
- en: Model is run for 100 epochs
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型运行100轮训练
- en: 'The comparison-of-accuracy metric between training and test datasets between
    the two scenarios is as follows:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 两个情形之间的训练集和测试集的准确率对比如下：
- en: '| **Scenario** | **Training dataset** | **Test dataset** |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| **情形** | **训练数据集** | **测试数据集** |'
- en: '| 5 epochs | 97.59% | 97.1% |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 5轮训练 | 97.59% | 97.1% |'
- en: '| 100 epochs | 100% | 98.28% |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 100轮训练 | 100% | 98.28% |'
- en: 'Once we plot the histogram of weights that are connecting the hidden layer
    to the output layer, we will notice that the 100-epochs scenario has a higher
    spread of weights when compared to the five-epochs scenario:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们绘制出连接隐藏层和输出层的权重的直方图，我们会注意到，与五轮训练的情况相比，100轮训练的权重分布范围更大：
- en: '![](img/a6f6b04b-293f-4fb2-a76f-211d40b81f0c.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6f6b04b-293f-4fb2-a76f-211d40b81f0c.png)'
- en: '![](img/7c5f76b7-f483-4666-9fc5-ea98573fc588.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c5f76b7-f483-4666-9fc5-ea98573fc588.png)'
- en: From the preceding pictures, you should note that the 100 epochs scenario had
    a higher dispersion of weight values when compared to the five-epochs scenario.
    This is because of the higher amount of opportunity that the model had to overfit
    on top of the training dataset when the model is run for 100-epochs, when compared
    to when the model is run for five epochs, as the number of weight updates in the
    100-epochs scenario is higher than the number of weight updates in the five-epochs
    scenario.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图片中你应该注意到，100轮训练的情况相比于五轮训练的情况，权重值的分布更加广泛。这是因为在100轮训练中，模型有更多的机会在训练集上过拟合，而相比之下，五轮训练的更新次数较少，因此过拟合的机会也较少。
- en: A high value of weight (along with a difference in the training and test dataset)
    is a good indication of a potential over-fitting of the model and/or a potential
    opportunity to scale input/weights to increase the accuracy of the model.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 权重值过高（以及训练集和测试集之间的差异）是模型可能发生过拟合和/或可能有机会缩放输入/权重以提高模型准确率的良好指示。
- en: Additionally, also note that a neural network can have hundreds of thousands
    of weights (and millions in certain architectures) that need to be adjusted, and
    thus, there is always a chance that one or the other weight can get updated to
    a very high number to fine-tune for one outlier row of the dataset.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，神经网络中可能包含数十万甚至百万的权重（某些架构中为数百万），这些权重都需要进行调整，因此，某些权重有可能会被更新为非常大的值，以便针对数据集中某一特异值行进行微调。
- en: Overcoming over-fitting using regularization
  id: totrans-307
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则化克服过拟合
- en: In the previous section, we established that a high weight magnitude is one
    of the reasons for over-fitting. In this section, we will look into ways to get
    around the problem of over-fitting, such as penalizing for high weight magnitude
    values.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们已确定高权重幅度是过拟合的原因之一。在本节中，我们将探讨如何避免过拟合问题，例如对高权重幅度值进行惩罚。
- en: '**Regularization** gives a penalty for having a high magnitude of weights in
    model. L1 and L2 regularizations are among the most commonly used regularization
    techniques and work as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '**正则化**通过对模型中的高权重幅度进行惩罚来进行调节。L1 和 L2 正则化是最常用的正则化技术，其工作原理如下：'
- en: 'L2 regularization minimizes the weighted sum of squares of weights at the specified
    layer of the neural network, in addition to minimizing the loss function (which
    is the sum of squared loss in the following formula):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: L2 正则化在最小化损失函数的同时（即以下公式中的平方损失和），还会最小化神经网络指定层的权重平方和：
- en: '![](img/64a98b14-388a-4cbb-b604-30c56e739681.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64a98b14-388a-4cbb-b604-30c56e739681.png)'
- en: Where ![](img/f9fef716-fe17-45b4-8abf-6a669c5d3cc6.png) is the weightage associated
    with the regularization term and is a hyperparameter that needs to be tuned, *y*
    is the predicted value of ![](img/28abf306-b171-4bd7-b8de-1bdffe82639c.png), and ![](img/f552cdd0-2311-4b72-8ec6-29e8c99fcf69.png) is
    the weight values across all the layers of the model.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/f9fef716-fe17-45b4-8abf-6a669c5d3cc6.png) 是与正则化项相关的权重值，这是一个需要调整的超参数，*y*
    是预测值，![](img/28abf306-b171-4bd7-b8de-1bdffe82639c.png) 是目标值， ![](img/f552cdd0-2311-4b72-8ec6-29e8c99fcf69.png) 是模型所有层的权重值。
- en: 'L1 regularization minimizes the weighted sum of absolute values of weights
    at the specified layer of the neural network in addition to minimizing the loss
    function (which is the sum of the squared loss in the following formula):'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: L1 正则化在最小化损失函数的同时（即以下公式中的平方损失和），还会最小化神经网络指定层的权重绝对值之和：
- en: '![](img/9e52d2e2-aca1-4e06-a3af-007deacb2468.png).'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/9e52d2e2-aca1-4e06-a3af-007deacb2468.png)。'
- en: This way, we ensure that weights do not get customized for extreme cases in
    the training dataset only (and thus, not generalizing on the test data).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以确保权重不会仅针对训练集中的极端案例进行调整（从而避免在测试数据上无法泛化）。
- en: How to do it
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点
- en: 'L1/L2 regularization is implemented in Keras, as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: L1/L2 正则化在 Keras 中的实现方式如下：
- en: '[PRE35]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note that the preceding involves invoking an additional hyperparameter—`kernel_regularizer`—and
    then specifying whether it is an L1/L2 regularization. Furthermore, we also specify
    the lambda value that gives the weight to regularization.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面的操作涉及调用一个额外的超参数——`kernel_regularizer`，然后指定它是 L1 还是 L2 正则化。此外，我们还会指定 lambda
    值，这个值决定了正则化的权重。
- en: We notice that, post regularization, the training dataset accuracy does not
    happen to be at ~100%, while the test data accuracy is at 98%. The histogram of
    weights post-L2 regularization is visualized in the next graph.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，在正则化之后，训练集的准确率并未达到 ~100%，而测试集的准确率为 98%。L2 正则化后权重的直方图将在下一个图中显示。
- en: 'The weights of connecting the hidden layer to the output layer are extracted
    as follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 连接隐藏层和输出层的权重提取方式如下：
- en: '[PRE36]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Once the weights are extracted, they are plotted as follows:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦提取了权重，它们会按以下方式绘制：
- en: '[PRE37]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![](img/e672e37d-2059-4520-9564-12ee59698144.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e672e37d-2059-4520-9564-12ee59698144.png)'
- en: We notice that the majority of weights are now much closer to zero when compared
    to the previous scenario, thus presenting a case to avoid the overfitting issue.
    We would see a similar trend in the case of L1 regularization.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，与之前的情况相比，绝大多数权重现在都更接近于零，这就避免了过拟合问题。在 L1 正则化的情况下，我们也能看到类似的趋势。
- en: Notice that the weight values when regularization exists are much lower when
    compared to the weight values when regularization is performed.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在存在正则化的情况下，权重值比没有进行正则化时的权重值要低得多。
- en: Thus, the L1 and L2 regularizations help us to avoid the overfitting issue on
    top of the training dataset.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，L1和L2正则化帮助我们避免了训练数据集上的过拟合问题。
- en: Overcoming overfitting using dropout
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用丢弃法克服过拟合
- en: In the previous section of overcoming overfitting using regularization, we used
    L1/ L2 regularization as a means to avoid overfitting. In this section, we will
    use another tool that is helpful to achieve the same—**dropout**.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用L1/L2正则化来克服过拟合问题。在这一节中，我们将使用另一种有助于实现相同目标的工具——**丢弃法**。
- en: 'Dropout can be considered a way in which only a certain percentage of weights
    get updated, while the others do not get updated in a given iteration of weight
    updates. This way, we are in a position where not all weights get updated in a
    weight update process, thus avoiding certain weights to achieve a very high magnitude
    when compared to others:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃法可以看作是一种方法，其中只有某个百分比的权重会被更新，而其他的权重则不会在某次权重更新迭代中被更新。这样，我们就处于一个位置，即并非所有权重都在权重更新过程中被更新，从而避免了某些权重相较于其他权重达到非常高的幅度：
- en: '[PRE38]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the preceding code, we have given a dropout of 0.75; that is, randomly, 75%
    of weights do not get updated in a certain weight update iteration.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们设置了0.75的丢弃率；也就是说，在某次权重更新迭代中，75%的权重不会被更新。
- en: The preceding would result in the gap between the training and test accuracy
    being not as high as it is when the model was built without dropout in the previous
    scenario, where the spread of weights was higher.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 上述情况会导致训练准确度和测试准确度之间的差距没有像在没有丢弃法的情况下构建模型时那么大，因为在没有丢弃法的情况下，权重的分布较广。
- en: 'Note the histogram of weights of the first layer now:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，现在第一层权重的直方图：
- en: '[PRE39]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](img/64d666de-bbb5-4c96-94d6-79946b4e26a6.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64d666de-bbb5-4c96-94d6-79946b4e26a6.png)'
- en: Note that in the preceding scenario, the frequency count of weights that are
    beyond 0.2 or -0.2 is less when compared to the 100-epochs scenario.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前述场景中，超出0.2或-0.2的权重的频次要比100个epoch的场景少。
- en: Speeding up the training process using batch normalization
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用批量归一化加速训练过程
- en: In the previous section on the scaling dataset, we learned that optimization
    is slow when the input data is not scaled (that is, it is not between zero and
    one).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节关于数据集缩放的内容中，我们学习到，当输入数据没有被缩放（即它没有在零到一之间）时，优化过程会很慢。
- en: 'The hidden layer value could be high in the following scenarios:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下情境中，隐藏层的值可能会很高：
- en: Input data values are high
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入数据值很高
- en: Weight values are high
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重值很高
- en: The multiplication of weight and input are high
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重和输入的乘积很高
- en: Any of these scenarios can result in a large output value on the hidden layer.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这些情境中的任何一种都可能导致隐藏层的输出值很大。
- en: Note that the hidden layer is the input layer to output layer. Hence, the phenomenon
    of high input values resulting in a slow optimization holds true when hidden layer
    values are large as well.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，隐藏层是输入层到输出层的过渡。因此，当隐藏层值很大时，高输入值导致优化缓慢的现象同样适用。
- en: '**Batch normalization** comes to the rescue in this scenario. We have already
    learned that, when input values are high, we perform scaling to reduce the input
    values. Additionally, we have learned that scaling can also be performed using
    a different method, which is to subtract the mean of the input and divide it by
    the standard deviation of the input. Batch normalization performs this method
    of scaling.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**批量归一化**在这种情况下发挥了重要作用。我们已经学到，当输入值很高时，我们会进行缩放以减少输入值。此外，我们还学到，可以使用另一种方法进行缩放，即减去输入的均值，并除以输入的标准差。批量归一化正是采用这种方法进行缩放。'
- en: 'Typically, all values are scaled using the following formula:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，所有值都使用以下公式进行缩放：
- en: '![](img/bc09c4fd-85ae-46d7-97ac-2dc608a9c27f.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc09c4fd-85ae-46d7-97ac-2dc608a9c27f.png)'
- en: '![](img/e4844bdd-acf2-4e1d-b5f7-f8288413842a.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e4844bdd-acf2-4e1d-b5f7-f8288413842a.png)'
- en: '![](img/110c9b11-033b-4bfc-9b4f-445cc803a93e.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![](img/110c9b11-033b-4bfc-9b4f-445cc803a93e.png)'
- en: '![](img/aa3c6535-7868-42c7-bd2f-4056453df216.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa3c6535-7868-42c7-bd2f-4056453df216.png)'
- en: Notice that *γ* and *β* are learned during training, along with the original
    parameters of the network.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，*γ* 和 *β* 在训练过程中会学习到，连同网络的原始参数一起。
- en: How to do it...
  id: totrans-354
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'In code, batch normalization is applied as follows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，批量归一化是这样应用的：
- en: Note that we will be using the same data-preprocessing steps as those we used
    in step 1 and step 2 in the *Scaling the input dataset* recipe.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将使用与步骤1和步骤2中在*缩放输入数据集*步骤中相同的数据预处理方法。
- en: 'Import the `BatchNormalization` method as follows:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式导入`BatchNormalization`方法：
- en: '[PRE40]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Instantiate a model and build the same architecture as we built when using
    the regularization technique. The only addition is that we perform batch normalization
    in a hidden layer:'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个模型，并构建与我们使用正则化技术时相同的架构。唯一的不同之处是我们在一个隐藏层中执行批量归一化：
- en: '[PRE41]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Build, compile, and fit the model as follows:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式构建、编译和拟合模型：
- en: '[PRE42]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The preceding results in training that is much faster than when there is no
    batch normalization, as follows:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 前述结果显示，当没有批量归一化时，训练速度明显较慢，如下所示：
- en: '![](img/2f5f1e2c-8a0d-45df-8c1a-d1a7918628b9.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f5f1e2c-8a0d-45df-8c1a-d1a7918628b9.png)'
- en: 'The previous graphs show the training and test loss and accuracy when there
    is no batch normalization, but only regularization. The following graphs show
    the training and test loss and accuracy with both regularization and batch normalization:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了没有批量归一化，仅使用正则化时的训练和测试损失与准确度。接下来的图表显示了同时使用正则化和批量归一化时的训练和测试损失与准确度：
- en: '![](img/32d51718-2639-441a-8155-bd78778bed39.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32d51718-2639-441a-8155-bd78778bed39.png)'
- en: Note that, in the preceding two scenarios, we see much faster training when
    we perform batch normalization (test dataset accuracy of ~97%) than compared to
    when we don't (test dataset accuracy of ~91%).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前述的两种场景中，当我们执行批量归一化时，训练速度明显更快（测试数据集准确率约为97%），相比之下，当我们不执行时（测试数据集准确率约为91%）：
- en: Thus, batch normalization results in much quicker training.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，批量归一化使得训练速度大大加快。
