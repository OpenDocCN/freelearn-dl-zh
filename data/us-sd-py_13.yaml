- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Generating Images with ControlNet
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ControlNet 生成图像
- en: Stable Diffusion’s ControlNet is a neural network plugin that allows you to
    control diffusion models by adding extra conditions. It was first introduced in
    a paper called Adding Conditional Control to Text-to-Image Diffusion Models [1]
    by Lvmin Zhang and Maneesh Agrawala, published in 2023.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion 的 ControlNet 是一个神经网络插件，允许你通过添加额外条件来控制扩散模型。它首次在 2023 年由 Zhang
    Lvmin 和 Maneesh Agrawala 发表的论文《Adding Conditional Control to Text-to-Image Diffusion
    Models [1]》中介绍。
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What is ControlNet and how is it different?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 ControlNet 以及它与其他方法有何不同？
- en: Usage of ControlNet
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ControlNet 的使用方法
- en: Using multiple ControlNets in one pipeline
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个管道中使用多个 ControlNet
- en: How ControlNet works
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ControlNet 的工作原理
- en: More ControlNet usage
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多 ControlNet 使用方法
- en: By the end of this chapter, you will understand how ControlNet works and how
    to use Stable Diffusion V1.5 and Stable Diffusion XL ControlNet models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解 ControlNet 的工作原理以及如何使用 Stable Diffusion V1.5 和 Stable Diffusion
    XL ControlNet 模型。
- en: What is ControlNet and how is it different?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 ControlNet 以及它与其他方法有何不同？
- en: In terms of “control,” you may recall textual embedding, LoRA, and the image-to-image
    diffusion pipeline. But what makes ControlNet different and useful?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在“控制”方面，你可能还记得文本嵌入、LoRA 和图像到图像的扩散管道。但是什么让 ControlNet 不同且有用？
- en: 'Unlike other solutions, ControlNet is a model that works on the UNet diffusion
    process directly. We compare these solutions in *Table 13.1*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他解决方案不同，ControlNet 是一个直接在 UNet 扩散过程中工作的模型。我们在 *表 13.1* 中比较了这些解决方案：
- en: '| **Control Method** | **Functioning Stage** | **Usage Scenario** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **控制方法** | **工作阶段** | **使用场景** |'
- en: '| --- | --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Textual Embedding | Text encoder | Add a new style, a new concept, or a new
    face |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 文本嵌入 | 文本编码器 | 添加新的风格、新的概念或新的面孔 |'
- en: '| LoRA | Merge LoRA weights to the UNet model (and the CLIP text encoder, optional)
    | Add a set of styles, concepts, and generate content |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 将 LoRA 权重合并到 UNet 模型（以及可选的 CLIP 文本编码器） | 添加一组风格、概念并生成内容 |'
- en: '| Image-to-Image | Provide the initial latent image | Fix images, or add styles
    and concepts to images |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 图像到图像 | 提供初始潜在图像 | 修复图像，或向图像添加风格和概念 |'
- en: '| ControlNet | ControlNet participant denoising together with a checkpoint
    model UNet | Control shape, pose, content detail |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| ControlNet | ControlNet 参与者与检查点模型 UNet 一起去噪 | 控制形状、姿势、内容细节 |'
- en: 'Table 13.1: A comparison of textual embedding, LoRA, image-to-image, and ControlNet'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.1：文本嵌入、LoRA、图像到图像和 ControlNet 的比较
- en: In many ways, ControlNet is similar to the image-to-image pipeline, as we discussed
    in [*Chapter 11*](B21263_11.xhtml#_idTextAnchor214). Both image-to-image and ControlNet
    can be used to enhance images.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，ControlNet 与我们讨论过的图像到图像管道类似，如 [*第 11 章*](B21263_11.xhtml#_idTextAnchor214)。图像到图像和
    ControlNet 都可以用来增强图像。
- en: However, ControlNet can “control” the image in a more precise way. Imagine you
    want to generate an image that uses a specific pose from another image or perfectly
    align objects within the scene to a specific reference point. This kind of precision
    is impossible with the out-of-the-box Stable Diffusion model. ControlNet is the
    tool that can help you achieve these goals.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ControlNet 可以以更精确的方式“控制”图像。想象一下，你想要生成一个使用另一张图像中的特定姿势或完美对齐场景中的物体到特定参考点的图像。这种精度是使用现成的
    Stable Diffusion 模型无法实现的。ControlNet 是帮助你实现这些目标的工具。
- en: 'Besides, ControlNet models work with all other open source checkpoint models,
    unlike some other solutions, which work only with one base model provided by their
    author. The team that created ControlNet not only open-sourced the model but also
    open-sourced the code to train a new model. In other words, we can train a ControlNet
    model and make it work with any other model. This is what the original paper says
    [1]:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ControlNet 模型与其他所有开源检查点模型兼容，而一些其他解决方案仅与作者提供的单个基础模型兼容。创建 ControlNet 的团队不仅开源了模型，还开源了训练新模型的代码。换句话说，我们可以训练一个
    ControlNet 模型，使其与其他任何模型一起工作。这正是原始论文 [1] 中所说的：
- en: Since Stable Diffusion is a typical UNet structure, this ControlNet architecture
    is likely to be applicable with other models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Stable Diffusion 是一个典型的 UNet 结构，这种 ControlNet 架构可能适用于其他模型。
- en: Note that ControlNet models will only work with models using the same base model.
    A **Stable Diffusion** (**SD**) v1.5 ControlNet model works with all other SD
    v1.5 models. For **Stable Diffusion XL** (**SDXL**) models, we will need a ControlNet
    model that is trained with SDXL. This is because SDXL models use a different architecture,
    a larger UNet than the SD v1.5\. Without additional work, a ControlNet model is
    trained with one architecture and only works with this type of model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，ControlNet 模型只能与使用相同基础模型的模型一起工作。一个 **Stable Diffusion** (**SD**) v1.5 ControlNet
    模型可以与所有其他 SD v1.5 模型一起工作。对于 **Stable Diffusion XL** (**SDXL**) 模型，我们需要一个经过 SDXL
    训练的 ControlNet 模型。这是因为 SDXL 模型使用不同的架构，比 SD v1.5 的 UNet 更大。没有额外的工作，一个 ControlNet
    模型只能与这种类型的模型一起工作。
- en: 'I used “*without additional work*” because in December 2023, to bridge this
    gap, a paper from Lingmin Ran et al, called *X-Adapter: Adding Universal Compatibility
    of Plugins for Upgraded Diffusion Model* was published [8]. This paper details
    an adapter that enables us to use SD V1.5 LoRA and ControlNet in a new SDXL model.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '我使用“*没有额外工作*”是因为在 2023 年 12 月，为了弥合这一差距，Lingmin Ran 等人发表了一篇名为 *X-Adapter: Adding
    Universal Compatibility of Plugins for Upgraded Diffusion Model* 的论文 [8]。这篇论文详细介绍了适配器，使我们能够在新
    SDXL 模型中使用 SD V1.5 LoRA 和 ControlNet。'
- en: Next, let’s start using ControlNet with SD models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们开始使用 SD 模型结合 ControlNet。
- en: Usage of ControlNet
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ControlNet 的方法
- en: Before diving into the backend of ControlNet, in this section, we will start
    using ControlNet to help control image generation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨 ControlNet 的后端之前，在本节中，我们将开始使用 ControlNet 来帮助控制图像生成。
- en: In the following example, we will first generate an image using SD, take the
    Canny shape of the object, and then use the Canny shape to generate a new image
    with the help of ControlNet.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将首先使用 SD 生成一个图像，获取对象的 Canny 形状，然后使用 Canny 形状在 ControlNet 的帮助下生成一个新的图像。
- en: Note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A Canny image refers to an image that has undergone Canny edge detection, which
    is a popular edge detection algorithm. It was developed by John F. Canny in 1986\.
    [7]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Canny 图像指的是经过 Canny 边缘检测的图像，这是一种流行的边缘检测算法。它由 John F. Canny 在 1986 年开发。[7]
- en: 'Let’s use SD to generate an image using the following code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码使用 SD 生成一个图像：
- en: 'Generate a sample image using SD:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 SD 生成样本图像：
- en: '[PRE0]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will see an image of a cat, as shown in *Figure 13**.1*:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将看到一只猫的图像，如图 *13*.1* 所示：
- en: '![Figure 13.1: A cat, generated by SD](img/B21263_13_01.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.1：由 SD 生成的猫](img/B21263_13_01.jpg)'
- en: 'Figure 13.1: A cat, generated by SD'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1：由 SD 生成的猫
- en: Then we will get the Canny shape of the sample image.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将得到样本图像的 Canny 形状。
- en: 'We will need another package, `controlnet_aux`, to create a Canny image from
    an image. Simply execute the following two lines of `pip` commands to install
    `controlnet_aux`:'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们需要另一个包，`controlnet_aux`，从图像创建 Canny 图像。只需执行以下两行 `pip` 命令来安装 `controlnet_aux`：
- en: '[PRE22]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can generate the image Canny edge shape with three lines of code:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以用三行代码生成图像的 Canny 边缘形状：
- en: '[PRE24]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here’s a breakdown of the code:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是代码的分解：
- en: '`from controlnet_aux import CannyDetector`: This line imports the `CannyDetector`
    class from the `controlnet_aux` module. There are many other detectors.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`from controlnet_aux import CannyDetector`：这一行从 `controlnet_aux` 模块导入 `CannyDetector`
    类。有许多其他检测器。'
- en: '`image_canny = canny(image, 30, 100)`: This line calls the `__call__` method
    of the `CannyDetector` class (which is implemented as a callable object) with
    the following arguments:'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image_canny = canny(image, 30, 100)`：这一行调用 `CannyDetector` 类（实现为一个可调用对象）的
    `__call__` 方法，并以下列参数：'
- en: '`image`: This is the input image to which the Canny edge detection algorithm
    will be applied.'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`image`：这是将要应用 Canny 边缘检测算法的输入图像。'
- en: '`30`: This is the lower threshold value for the edges. Any edges with an intensity
    gradient below this value will be discarded.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`30`：这是边缘的下限阈值值。任何强度梯度低于此值的边缘将被丢弃。'
- en: '`100`: This is the upper threshold value for the edges. Any edges with an intensity
    gradient above this value will be considered strong edges.'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`100`：这是边缘的上限阈值值。任何强度梯度高于此值的边缘将被视为强边缘。'
- en: 'The preceding code will generate the Canny image shown in *Figure 13**.2*:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码将生成如图 *13*.2* 所示的 Canny 图像：
- en: '![Figure 13.2: Canny image of a cat](img/B21263_13_02.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.2：猫的 Canny 图像](img/B21263_13_02.jpg)'
- en: 'Figure 13.2: Canny image of a cat'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2：猫的 Canny 图像
- en: 'We will now use the ControlNet model to generate a new image based on this
    Canny image. First, let’s load up the ControlNet model:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用 ControlNet 模型根据这个 Canny 图像生成一个新的图像。首先，让我们加载 ControlNet 模型：
- en: '[PRE27]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The code will download the ControlNet model from Hugging Face automatically
    at your first run. If you have the ControlNet `safetensors` model in your storage
    and want to use your own model, you can convert the file to the diffuser format
    first. You can find the conversion code in [*Chapter 6*](B21263_06.xhtml#_idTextAnchor117).
    Then, replace `takuma104/control_v11` with the path to the ControlNet model.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首次运行时，代码将自动从 Hugging Face 下载 ControlNet 模型。如果您存储中有 ControlNet `safetensors`
    模型并想使用自己的模型，您首先需要将文件转换为 diffuser 格式。您可以在[*第 6 章*](B21263_06.xhtml#_idTextAnchor117)中找到转换代码。然后，将
    `takuma104/control_v11` 替换为 ControlNet 模型的路径。
- en: 'Initialize a ControlNet pipeline:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个 ControlNet 管道：
- en: '[PRE33]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note that you can freely swap `stablediffusionapi/deliberate-v2` with any other
    SD v1.5 models from the community.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，您可以自由地将 `stablediffusionapi/deliberate-v2` 与社区中的任何其他 SD v1.5 模型交换。
- en: 'Generate the new image using the ControlNet pipeline. In the following example,
    we will replace the cat with a dog:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ControlNet 管道生成新的图像。在以下示例中，我们将用狗替换猫：
- en: '[PRE40]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'These lines of code will generate a new image following the Canny edge, but
    the little cat is now a dog, as shown in *Figure 13**.3*:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些代码行将生成一个遵循 Canny 边缘的新图像，但现在小猫变成了一只狗，如图 *图 13.3* 所示：
- en: '![Figure 13.3: A dog, generated using the cat Canny image with ControlNet](img/B21263_13_03.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.3：使用猫的 Canny 图像和 ControlNet 生成的狗](img/B21263_13_03.jpg)'
- en: 'Figure 13.3: A dog, generated using the cat Canny image with ControlNet'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3：使用猫的 Canny 图像和 ControlNet 生成的狗
- en: The cat’s body structure and shape are preserved. Feel free to change the prompt
    and settings to explore the amazing capabilities of the model. One thing to note
    is that if you don’t provide a prompt to the ControlNet pipeline, the pipeline
    will still output a meaningful image, maybe another style of cat, which means
    the ControlNet model learned the underlying meaning of a certain Canny edge.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 猫的身体结构和形状得到了保留。您可以随意更改提示和设置，以探索模型的惊人能力。需要注意的是，如果您不向 ControlNet 管道提供提示，管道仍然会输出有意义的图像，可能是另一种风格的猫，这意味着
    ControlNet 模型学会了某种 Canny 边缘的潜在含义。
- en: In this example, we used only one ControlNet model, but we can also provide
    multiple ControlNet models to one pipeline.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们只使用了一个 ControlNet 模型，但我们也可以向一个管道提供多个 ControlNet 模型。
- en: Using multiple ControlNets in one pipeline
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在一个管道中使用多个 ControlNets
- en: In this section, we will initialize one more ControlNet, NormalBAE, and then
    feed the Canny and NormalBAE ControlNet models together to form a pipeline.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将初始化另一个 ControlNet，NormalBAE，然后将 Canny 和 NormalBAE ControlNet 模型一起输入以形成一个管道。
- en: 'Let’s generate a Normal BAE as one additional control image. Normal BAE is
    a model that’s used to estimate a normal map using the normal uncertainty method
    [4] proposed by Bae et al:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一个作为额外控制图像的正常 BAE。Normal BAE 是一个使用 Bae 等人提出的正常不确定性方法 [4] 估计正常图的模型：
- en: '[PRE57]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This code will generate the original image’s Normal BAE map, as shown in *Figure
    13**.4*:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将生成原始图像的正常 BAE 映射，如图 *图 13.4* 所示：
- en: '![Figure 13.4: Normal BAE image of the generated cat](img/B21263_13_04.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.4：生成的猫的正常 BAE 图像](img/B21263_13_04.jpg)'
- en: 'Figure 13.4: Normal BAE image of the generated cat'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4：生成的猫的正常 BAE 图像
- en: 'Now, let’s initialize two ControlNet models for one pipeline: one Canny ControlNet
    model, and another NormalBae ControlNet model:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为单个管道初始化两个 ControlNet 模型：一个 Canny ControlNet 模型，另一个 NormalBae ControlNet
    模型：
- en: '[PRE58]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'From the code, we can easily see that all ControlNet models share the same
    architecture. To load different ControlNet models, we only need to change the
    model name. Also, note that the two ControlNet models are in a Python `controlnets`
    `list`. We can provide these ControlNet models to the pipeline directly, as shown
    here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码中，我们可以轻松地看出所有 ControlNet 模型都共享相同的架构。要加载不同的 ControlNet 模型，我们只需更改模型名称。此外，请注意，两个
    ControlNet 模型位于 Python `controlnets` `列表` 中。我们可以直接将这些 ControlNet 模型提供给管道，如下所示：
- en: '[PRE59]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'In the inference stage, use one additional parameter, `controlnet_conditioning_scale`,
    to control the influence scale of each ControlNet:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理阶段，使用一个额外的参数，`controlnet_conditioning_scale`，来控制每个 ControlNet 的影响范围：
- en: '[PRE60]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This code will give us another image, as shown in *Figure 13**.5*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将给我们另一张图像，如图 *图 13.5* 所示：
- en: '![Figure 13.5: A dog generated from Canny ControlNet and a Normal BAE ControlNet](img/B21263_13_05.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.5：由 Canny ControlNet 和正常 BAE ControlNet 生成的狗](img/B21263_13_05.jpg)'
- en: 'Figure 13.5: A dog generated from Canny ControlNet and a Normal BAE ControlNet'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5：由 Canny ControlNet 和正常 BAE ControlNet 生成的狗
- en: In `controlnet_conditioning_scale = [0.5,0.5]`, I give each ControlNet model
    a `0.5` scale value. The two scale values add up to `1.0`. We should give weights
    that add up to no more than `2`. Values that are too will lead to undesired images.
    For instance, if you give weights of `1.2` and `1.3` to each ControlNet model,
    like `controlnet_conditioning_scale = [1.2,1.3]`, you may get an undesired image.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `controlnet_conditioning_scale = [0.5,0.5]` 中，我为每个 ControlNet 模型赋予一个 `0.5`
    的缩放值。这两个缩放值加起来为 `1.0`。我们应该赋予的总权重不超过 `2`。过高的值会导致不期望的图像。例如，如果你给每个 ControlNet 模型赋予
    `1.2` 和 `1.3` 的权重，如 `controlnet_conditioning_scale = [1.2,1.3]`，你可能会得到一个不期望的图像。
- en: If we have successfully generated images using the ControlNet models, we have
    together witnessed the power of ControlNet. In the next section, we will discuss
    how ControlNet works.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们成功使用 ControlNet 模型生成图像，我们就共同见证了 ControlNet 的力量。在下一节中，我们将讨论 ControlNet 的工作原理。
- en: How ControlNet works
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ControlNet 的工作原理
- en: In this section, we will drill down into the ControlNet structure and see how
    ControlNet works internally.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨 ControlNet 的结构，并了解 ControlNet 内部是如何工作的。
- en: 'ControlNet works by injecting additional conditions into the blocks of a neural
    network. As shown in *Figure 13**.6*, the trainable copy is the ControlNet block
    that adds additional guidance to the original SD UNet block:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet 通过向神经网络块注入额外的条件来工作。如图 *图 13**.6* 所示，可训练的副本是添加额外指导到原始 SD UNet 块的 ControlNet
    块：
- en: '![Figure 13.6: Adding ControlNet components](img/B21263_13_06.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.6：添加 ControlNet 组件](img/B21263_13_06.jpg)'
- en: 'Figure 13.6: Adding ControlNet components'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6：添加 ControlNet 组件
- en: During the training stage, we take a copy of the target layer block as the ControlNet
    block. In *Figure 13**.6*, it is denoted as a **trainable copy**. Unlike typical
    neural network initialization with Gaussian distributions for all parameters,
    ControlNet utilizes pre-trained weights from the Stable Diffusion base model.
    Most of these base model parameters are frozen (with the option to unfreeze them
    later) and only the additional ControlNet components are trained from scratch.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，我们取目标层块的一个副本作为 ControlNet 块。在 *图 13**.6* 中，它被标记为 **可训练的副本**。与所有参数使用高斯分布进行典型神经网络初始化不同，ControlNet
    使用来自稳定扩散基础模型的预训练权重。这些基础模型参数中的大多数都是冻结的（有选项在以后解冻它们），只有额外的 ControlNet 组件是从头开始训练的。
- en: During training and inference, the input x is usually a 3D dimensional vector,
    x ∈ ℝ h×w×c, with h, w, c as the height, width, and number of channels. c is a
    conditioning vector that we will pass into the SD UNet and also to the ControlNet
    model network.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和推理过程中，输入 x 通常是一个三维向量，x ∈ ℝ h×w×c，其中 h、w、c 分别是高度、宽度和通道数。c 是一个条件向量，我们将将其传递到
    SD UNet 和 ControlNet 模型网络中。
- en: The **zero convolution** plays a pivotal role in this process. **Zero convolutions**
    are 1D convolutions with weights and biases initialized to zero. The advantage
    of zero convolution is that, even without a single training step, the value injected
    from ControlNet will have no effect on image generation. This ensures that the
    side network does not negatively impact image generation at any stage.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**零卷积**在这个过程中起着关键作用。**零卷积**是权重和偏差初始化为零的 1D 卷积。零卷积的优势在于，即使没有进行单个训练步骤，从 ControlNet
    注入的值也不会对图像生成产生影响。这确保了辅助网络在任何阶段都不会对图像生成产生负面影响。'
- en: 'You might be thinking: if the weight of a convolution layer is zero, wouldn''t
    the gradient also be zero, rendering the network unable to learn? However, as
    the paper''s authors explain [5], the reality is more nuanced.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：如果卷积层的权重为零，梯度不也会为零吗？这难道不会使网络无法学习吗？然而，正如论文的作者解释[5]的那样，实际情况更为复杂。
- en: 'Let’s consider one simple case:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的例子：
- en: y = wx + b
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: y = wx + b
- en: 'Then we have the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们还有以下情况：
- en: ∂ y / ∂ w = x, ∂ y / ∂ x = w, ∂ y / ∂ b = 1
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ∂ y / ∂ w = x, ∂ y / ∂ x = w, ∂ y / ∂ b = 1
- en: 'And if w = 0 and x ≠ 0, then we have this:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 w = 0 且 x ≠ 0，那么我们就有以下情况：
- en: ∂ y / ∂ w ≠ 0, ∂ y / ∂ x = 0, ∂ y / ∂ b ≠ 0
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ∂ y / ∂ w ≠ 0, ∂ y / ∂ x = 0, ∂ y / ∂ b ≠ 0
- en: 'This means as long as x ≠ 0, one gradient descent iteration will make w non-zero.
    Then, we have:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着只要 x ≠ 0，一次梯度下降迭代就会使 w 非零。然后，我们有：
- en: ∂ y / ∂ x ≠ 0
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ∂ y / ∂ x ≠ 0
- en: So, the zero convolutions will progressively become a common convolution layer
    with non-zero weights. What a genius design!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，零卷积将逐渐变成具有非零权重的普通卷积层。多么天才的设计啊！
- en: The SD UNet is only connected with a ControlNet at the encoder blocks and the
    middle blocks. The trainable blue blocks and the white zero convolution layers
    are added to build a ControlNet. It’s simple and effective.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: SD UNet仅在编码器块和中间块与ControlNet连接。可训练的蓝色块和白色零卷积层被添加来构建ControlNet。它简单而有效。
- en: In the original paper— Adding Conditional Control to Text-to-Image Diffusion
    Models [1] by Lvmin Zhang et al — its authors also provided an ablative study
    and discussed lots of different cases, such as swapping the **zero convolution**
    layer with a **traditional convolution** layer and comparing the differences.
    It is a great paper and fun to read.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始论文——张吕民等人撰写的《向文本到图像扩散模型添加条件控制》[1]中——其作者还提供了一种消融研究，并讨论了许多不同的情况，例如将**零卷积**层替换为**传统卷积**层，并比较了差异。这是一篇优秀的论文，值得一读。
- en: Further usage
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步使用
- en: In this section, we will introduce more usage of ControlNet, covering SD V1.5
    and also SDXL.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍更多关于ControlNet的使用，涵盖SD V1.5和SDXL。
- en: More ControlNets with SD
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多带有SD的ControlNets
- en: 'The author of the ControlNet-v1-1-nightly repository [3] lists all the currently
    available V1.1 ControlNet models for SD. As of the time I am writing this chapter,
    the list is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet-v1-1-nightly存储库[3]的作者列出了目前可用的所有V1.1 ControlNet模型。在我撰写这一章的时候，列表如下：
- en: '[PRE61]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: You can simply swap the ControlNet model’s name with one from this list to start
    using it. Generate the control image using one of the annotators from the open
    source ControlNet auxiliary models[6].
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以简单地用列表中的一个模型名称替换ControlNet模型名称，然后开始使用它。使用开源ControlNet辅助模型[6]中的一个注释器生成控制图像。
- en: Considering the speed of development in the field of AI, when you are reading
    this, the version may have increased to v1.1+. However, the underlying mechanism
    should be the same.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到人工智能领域的发展速度，当你阅读这段内容时，版本可能已增加到v1.1+。然而，底层机制应该是相同的。
- en: SDXL ControlNets
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SDXL ControlNets
- en: As I am writing this chapter, SDXL has just been released, and this new model
    generates excellent images with shorter prompts than before. The Hugging Face
    Diffusers team trained and provided several ControlNet models for the XL models.
    Its usage is almost the same as the previous version. Here, let’s use the `controlnet-openpose-sdxl-1.0`
    open pose ControlNet for SDXL.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当我撰写这一章时，SDXL刚刚发布，这个新模型生成的图像质量优秀，且比之前需要更短的提示词。Hugging Face Diffusers团队为XL模型训练并提供了几个ControlNet模型。其使用方法几乎与之前版本相同。在这里，我们将使用`controlnet-openpose-sdxl-1.0`开放姿态ControlNet为SDXL。
- en: Note that you will need a dedicated GPU with more than 15 GB of VRAM to run
    the following example.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您将需要一个具有超过15 GB VRAM的专用GPU来运行以下示例。
- en: 'Let’s initialize an SDXL pipeline using the following code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码初始化一个SDXL管道：
- en: '[PRE62]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Then, generate an image with a man in it:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，生成一个包含男士的图像：
- en: '[PRE63]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The code will generate an image, as shown in *Figure 13**.7*:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将生成一个图像，如图*图13.7*所示：
- en: '![Figure 13.7: A man in a suit, generated by SDXL](img/B21263_13_07.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图13.7：由SDXL生成的穿着西装的男士](img/B21263_13_07.jpg)'
- en: 'Figure 13.7: A man in a suit, generated by SDXL'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7：由SDXL生成的穿着西装的男士
- en: 'We can use `OpenposeDetector` from `controlnet_aux` [6] to extract the pose:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`controlnet_aux`中的`OpenposeDetector`[6]来提取姿态：
- en: '[PRE64]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We will get the pose image shown in *Figure 13**.8*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将获得如图*图13.8*所示的姿态图像：
- en: '![Figure 13.8: Pose image of the man in a suit](img/B21263_13_08.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图13.8：穿着西装的男士姿态图像](img/B21263_13_08.jpg)'
- en: 'Figure 13.8: Pose image of the man in a suit'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8：穿着西装的男士姿态图像
- en: 'Now, let’s start an SDXL pipeline with the SDXL ControlNet open pose model:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用SDXL ControlNet开放姿态模型启动SDXL管道：
- en: '[PRE65]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now we can use the new ControlNet pipeline to generate a new image from the
    pose image with the same style. We will reuse the prompt but replace **man** with
    **woman**. We are aiming to generate a new image of a woman in a suit but in the
    same pose as the previous image of a man:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用新的ControlNet管道从姿态图像生成具有相同风格的新的图像。我们将重用提示词，但将**man**替换为**woman**。我们的目标是生成一个穿着西装的女士的新图像，但与之前男士的图像具有相同的姿态：
- en: '[PRE66]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The code generates a new image with the same pose, exactly matching the expectations,
    as shown in *Figure 13**.9*:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成一个具有相同姿态的新图像，与预期完全匹配，如图*图13.9*所示：
- en: '![Figure 13.9: A woman in a suit, generated using an SDXL ControlNet](img/B21263_13_09.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图13.9：使用SDXL ControlNet生成的穿着西装的女士](img/B21263_13_09.jpg)'
- en: 'Figure 13.9: A woman in a suit, generated using an SDXL ControlNet'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9：使用SDXL ControlNet生成的穿着西装的女士
- en: We will further discuss Stable Diffusion XL in [*Chapter 16*](B21263_16.xhtml#_idTextAnchor309).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第16章*](B21263_16.xhtml#_idTextAnchor309)中进一步讨论Stable Diffusion XL。
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced a way to precisely control image generation using
    SD ControlNets. From the detailed samples we have provided, you can start using
    one or multiple ControlNet models with SD v1.5 and also SDXL.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一种使用SD ControlNets精确控制图像生成的方法。从我们提供的详细示例中，你可以开始使用一个或多个ControlNet模型与SD
    v1.5以及SDXL一起使用。
- en: We also drilled down into the internals of ControlNet, explaining how it works
    in a nutshell.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还深入探讨了ControlNet的内部机制，简要解释了它是如何工作的。
- en: We can use ControlNet in lots of applications, including applying a style to
    an image, applying a shape to an image, merging two images into one, and generating
    a human body using a posed image. It is powerful and amazingly useful in many
    ways. Our imagination is the only limitation.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在许多应用中使用ControlNet，包括将风格应用于图像、将形状应用于图像、将两个图像合并为一个，以及使用摆姿势的图像生成人体。它在许多方面都非常强大且非常有用。我们的想象力是唯一的限制。
- en: 'However, there is one other limitation: it is hard to align the background
    and overall context between two generations (with different seeds). You may want
    to use ControlNet to generate a video from the extracted frames from a source
    video, but the results are still not ideal.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有一个限制：很难在两个生成（具有不同的种子）之间对齐背景和整体上下文。你可能想使用ControlNet从源视频提取的帧生成视频，但结果仍然不理想。
- en: In the next chapter, we will cover a solution to generate video and animation
    using SD.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍使用SD生成视频和动画的解决方案。
- en: References
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Adding conditional control to text-to-image diffusion models: [https://arxiv.org/abs/2302.05543](https://arxiv.org/abs/2302.05543)'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '向文本到图像扩散模型添加条件控制: [https://arxiv.org/abs/2302.05543](https://arxiv.org/abs/2302.05543)'
- en: 'ControlNet v1.0 GitHub repository: [https://github.com/lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet)'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ControlNet v1.0 GitHub仓库: [https://github.com/lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet)'
- en: 'ControlNet v1.1 GitHub repository: [https://github.com/lllyasviel/ControlNet-v1-1-nightly](https://github.com/lllyasviel/ControlNet-v1-1-nightly)'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ControlNet v1.1 GitHub仓库: [https://github.com/lllyasviel/ControlNet-v1-1-nightly](https://github.com/lllyasviel/ControlNet-v1-1-nightly)'
- en: '`surface_normal_uncertainty`: [https://github.com/baegwangbin/surface_normal_uncertainty](https://github.com/baegwangbin/surface_normal_uncertainty)'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`surface_normal_uncertainty`: [https://github.com/baegwangbin/surface_normal_uncertainty](https://github.com/baegwangbin/surface_normal_uncertainty)'
- en: 'Zero convolution FAQ: [https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md](https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md)'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '零卷积常见问题解答: [https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md](https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md)'
- en: 'ControlNet AUX: [https://github.com/patrickvonplaten/controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux)'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ControlNet AUX: [https://github.com/patrickvonplaten/controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux)'
- en: 'Canny edge detector: [https://en.wikipedia.org/wiki/Canny_edge_detector](https://en.wikipedia.org/wiki/Canny_edge_detector)'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Canny边缘检测器: [https://en.wikipedia.org/wiki/Canny_edge_detector](https://en.wikipedia.org/wiki/Canny_edge_detector)'
- en: 'X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion
    Model: [https://showlab.github.io/X-Adapter/](https://showlab.github.io/X-Adapter/)'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'X-Adapter：为升级的扩散模型添加插件通用兼容性: [https://showlab.github.io/X-Adapter/](https://showlab.github.io/X-Adapter/)'
