- en: Training Your GANs to Break Different Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练你的GAN模型以突破不同的模型
- en: There has been a clear trend that people enjoy using deep learning methods to
    solve problems in the computer vision field. Has one of your classmates or colleagues
    ever shown off their latest image classifier to you? Now, with GANs, you may actually
    get the chance to show them what you can do by generating adversarial examples
    to break their previous models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 人们逐渐倾向于使用深度学习方法来解决计算机视觉领域的问题。你有没有见过你的同学或同事向你炫耀他们最新的图像分类器？现在，借助GANs，你可能真的有机会通过生成对抗性样本来打破他们之前的模型，向他们展示你能做些什么。
- en: We will be looking into the fundamentals of adversarial examples and how to
    attack and confuse a CNN model with **FGSM** (**Fast Gradient Sign Method**).
    We will also learn how to train an ensemble classifier with several pre-trained
    CNN models via transfer learning on Kaggle's Cats vs. Dogs dataset, following
    which, we will learn how to use an accimage library to speed up your image loading
    even more and train a GAN model to generate adversarial examples and fool the
    image classifier.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨对抗性样本的基本原理，并学习如何使用**FGSM**（**快速梯度符号方法**）攻击并迷惑CNN模型。我们还将学习如何通过迁移学习，在Kaggle的猫狗数据集上训练一个集成分类器，之后，我们将学习如何使用accimage库进一步加速图像加载，并训练一个GAN模型生成对抗性样本，从而愚弄图像分类器。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Adversarial examples – attacking deep learning models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗性样本 - 攻击深度学习模型
- en: Generative adversarial examples
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗性样本
- en: Adversarial examples – attacking deep learning models
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗性样本 - 攻击深度学习模型
- en: It is known that with deep learning methods that have huge numbers of parameters,
    sometimes more than tens of millions, it becomes more difficult for humans to comprehend
    what exactly they have learned, except the fact that they perform unexpectedly
    well in CV and NLP fields. If someone around you feels exceptionally comfortable
    using deep learning to solve each and every practical problem without a second
    thought, what we are about to learn in this chapter will help them to realize
    the potential risks their models are exposed to.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，使用具有大量参数的深度学习方法（有时超过千万个参数）时，人们往往难以理解它们到底学到了什么，除了它们在计算机视觉和自然语言处理领域表现异常出色这一事实。如果你身边的人感觉非常轻松地使用深度学习来解决每一个实际问题，并且毫不犹豫，那么我们即将在本章学习的内容将帮助他们意识到他们的模型可能面临的潜在风险。
- en: What are adversarial examples and how are they created?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是对抗性样本，它是如何创建的？
- en: Adversarial examples are a kind of sample (often modified based on real data)
    that are easily mistakenly classified by a machine learning system (and sometimes
    look normal to the human eye). Modifications to image data could be a small amount
    of added noise ([https://openai.com/blog/adversarial-example-research](https://openai.com/blog/adversarial-example-research))
    or a small image patch (Tom B. Brown, et al, 2017). Sometimes, printing them on
    paper and taking pictures of adversarial examples also fools neural networks.
    It is even possible to 3D-print an object that fools neural networks from almost
    all perspectives (Anish Athalye, et al, 2018). Although you can create some random
    samples that look like nothing natural and still cause neural networks to make
    mistakes, it is far more interesting to study the adversarial examples that look
    normal to humans but are misclassified by neural networks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性样本是一种样本（通常是基于真实数据修改的），它们很容易被机器学习系统错误分类（有时看起来对人眼是正常的）。对图像数据的修改可以是少量的噪声（[https://openai.com/blog/adversarial-example-research](https://openai.com/blog/adversarial-example-research)）或一个小的图像补丁（Tom
    B. Brown 等人，2017年）。有时，将它们打印在纸上并拍照，甚至能愚弄神经网络。从几乎所有角度来看，甚至可以通过3D打印一个物体来愚弄神经网络（Anish
    Athalye 等人，2018年）。尽管你可以创建一些看似毫无意义的随机样本，仍然能导致神经网络出错，但更有趣的是研究那些看起来对人类来说是正常的，但却被神经网络误分类的对抗性样本。
- en: Be assured that we are not going off-topic discussing adversarial examples here.
    For starters, Ian Goodfellow, known as the father of GANs, has spent a decent
    amount of time studying adversarial examples. Adversarial examples and GANs might
    be siblings! Joking aside, GANs are good for generating convincing and realistic
    samples, as well as generating samples that fool other classifier models. In this
    chapter, we will first walk through how to construct adversarial examples and
    attack a small model in PyTorch. Then, we will show you how to use GANs to generate
    adversarial examples to attack a large model.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 请放心，我们并没有偏离主题，讨论对抗样本。在此之前，Ian Goodfellow（被称为GAN之父）曾花了相当多的时间研究对抗样本。对抗样本和GAN可能是兄弟关系！开个玩笑，GAN擅长生成逼真且具有说服力的样本，也能生成误导其他分类器的样本。在本章中，我们首先将讲解如何构造对抗样本，并用它来攻击一个小型模型。接着，我们将展示如何使用GAN生成对抗样本以攻击大型模型。
- en: Adversarial attacking with PyTorch
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch进行对抗攻击
- en: There is an excellent toolbox for adversarial attacks, defense, and benchmarks
    for TensorFlow called CleverHans ([https://github.com/tensorflow/cleverhans](https://github.com/tensorflow/cleverhans)).
    Currently, the developers are making plans to support PyTorch ([https://github.com/tensorflow/cleverhans/blob/master/tutorials/future/torch/cifar10_tutorial.py](https://github.com/tensorflow/cleverhans/blob/master/tutorials/future/torch/cifar10_tutorial.py)).
    In this section, we will need to implement an adversarial example in PyTorch.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个非常优秀的对抗攻击、对抗防御和基准测试工具箱，叫做CleverHans，适用于TensorFlow，地址是[https://github.com/tensorflow/cleverhans](https://github.com/tensorflow/cleverhans)。目前，开发者正在计划支持PyTorch，详情请见[https://github.com/tensorflow/cleverhans/blob/master/tutorials/future/torch/cifar10_tutorial.py](https://github.com/tensorflow/cleverhans/blob/master/tutorials/future/torch/cifar10_tutorial.py)。在本节中，我们需要在PyTorch中实现一个对抗样本。
- en: 'The following code snippet is based on the official tutorial by PyTorch: [https://pytorch.org/tutorials/beginner/fgsm_tutorial.html](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html).
    We will slightly modify the model and the creation of adversarial examples will
    be performed in batchs. Start with a blank file named `advAttackGAN.py`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段基于PyTorch的官方教程：[https://pytorch.org/tutorials/beginner/fgsm_tutorial.html](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html)。我们将稍作修改模型，并且对抗样本的创建将以批次形式进行。以一个名为`advAttackGAN.py`的空文件开始：
- en: 'Import the modules:'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入模块：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define the device and the perturbation factors:'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义设备和扰动因子：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the CNN model, which is known as the LeNet-5 model:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义CNN模型，这个模型被称为LeNet-5模型：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Define the data loader for both training and testing. Here, we''ll use the
    MNIST dataset:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义训练和测试的数据加载器。在这里，我们将使用MNIST数据集：
- en: '[PRE3]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that, to make the defined perturbation factors work for our model, we are
    not normalizing (whitening) the data by subtracting the mean value and dividing
    by the standard deviation value.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了让定义的扰动因子能够适用于我们的模型，我们没有对数据进行标准化（去均值并除以标准差）。
- en: 'Create the `model`, `optimizer`, and `loss` functions:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`model`、`optimizer`和`loss`函数：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the `train` and `test` functions:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`train`和`test`函数：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s train the model and see what this small model is capable of:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们训练这个模型，看看这个小型模型能做什么：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output messages may look like these:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 输出信息可能如下所示：
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can see that our small CNN model achieves 99.31% test accuracy after only
    5 epochs of training.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，经过仅仅5个训练周期后，我们的小型CNN模型在测试集上的准确率达到了99.31%。
- en: 'Now, implement FGSM to create an adversarial example from the read sample and
    its derivatives:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，实施FGSM以创建一个来自读取样本及其导数的对抗样本：
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Use `fgsm_attack` to perturbate test images and see what happens:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fgsm_attack`对测试图像进行扰动，并观察会发生什么：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, we save the first five test images to `adv_examples` to show the predicted
    labels before and after perturbation. You can always replace the `indices = torch.arange(5)` line with
    the following line to only show adversarial examples that cause the model to fail:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将前五个测试图像保存到`adv_examples`中，以展示扰动前后预测标签。你可以随时将`indices = torch.arange(5)`这一行替换成以下代码，以仅显示那些导致模型失败的对抗样本：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output messages in the Terminal may look like these:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 终端中的输出信息可能如下所示：
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We can see that, as `epsilon` increases, more samples are mistakenly classified
    by the model. The test accuracy of the model drops to 16.7% at worst.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，随着`epsilon`的增大，模型误分类的样本数量增加。在最糟糕的情况下，模型的测试准确率降至16.7%。
- en: 'Finally, illustrate the perturbed images with `matplotlib`:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`matplotlib`展示扰动后的图像：
- en: '[PRE12]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here are the first five test images and their predicted labels before and after
    perturbation with different factor values:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是前五个测试图像及其在不同因子值扰动前后的预测标签：
- en: '![](img/c40dba63-d0ee-4476-9f3b-b8d4a4f906f7.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c40dba63-d0ee-4476-9f3b-b8d4a4f906f7.png)'
- en: Adversarial examples created from MNIST
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从MNIST创建的对抗样本
- en: Generative adversarial examples
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗样本
- en: We have been using GANs to generate various types of images in the previous
    chapters. Now, it's time to try generating adversarial examples with GANs and
    break some models!
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们一直使用GAN生成各种类型的图像。现在，是时候尝试用GAN生成对抗样本并破解一些模型了！
- en: Preparing an ensemble classifier for Kaggle's Cats vs. Dogs
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为Kaggle的猫狗数据集准备一个集成分类器
- en: To make our demonstration more similar to practical scenarios, we will train
    a decent model on Kaggle's Cats vs. Dogs dataset ([https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats)),
    then break the model with adversarial examples generated by GAN. This dataset
    contains 25,000 training images and 12,500 testing images of either dogs or cats.
    Here, we will only use the 25,000 training images in our experiment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的演示更接近实际场景，我们将在Kaggle的猫狗数据集([https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats))上训练一个合理的模型，然后用GAN生成的对抗样本来破解模型。该数据集包含25,000张训练图像和12,500张测试图像，图像内容为狗或猫。在本实验中，我们只使用25,000张训练图像。
- en: 'For convenience, after downloading the dataset, put images of cats and dogs
    in separate folders, so that the file structure looks like this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，在下载数据集后，将猫和狗的图像分别放入不同的文件夹中，使文件结构如下所示：
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The model we are training on this dataset is formed of several pre-trained
    models provided by PyTorch Hub ([https://github.com/pytorch/hub](https://github.com/pytorch/hub)).
    We will also need to perform transfer training on the pre-trained models to fit
    our dataset:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个数据集上训练的模型由几个由PyTorch Hub提供的预训练模型组成([https://github.com/pytorch/hub](https://github.com/pytorch/hub))。我们还需要对这些预训练模型进行迁移训练，以适应我们的数据集：
- en: '![](img/76921976-ed7d-47c0-8a37-cbf386539d27.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76921976-ed7d-47c0-8a37-cbf386539d27.png)'
- en: Ensemble model for Kaggle's Cats vs. Dogs
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle猫狗数据集的集成模型
- en: 'Now, we need to load and preprocess the data, create an ensemble classifier,
    and train this model. Here are the detailed steps:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要加载和预处理数据，创建一个集成分类器，并训练这个模型。下面是详细步骤：
- en: 'Create a Python file named `cats_dogs.py` and import the Python modules:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`cats_dogs.py`的Python文件，并导入相关Python模块：
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, the custom module files, `advGAN`, `data_utils`,and `model_ensemble`,
    will be created later.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`advGAN`、`data_utils`和`model_ensemble`等自定义模块文件稍后将会创建。
- en: 'Define the main entry point in `cats_dogs.py`, in which argument values are
    parsed and the image decoding backend is defined. Here, only some of the lines
    are shown due to the length. The full source code is available in the `cats_dogs`
    folder, under the code repository for this chapter:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`cats_dogs.py`中定义主入口点，在其中解析参数值并定义图像解码后端。这里仅显示部分代码行，由于篇幅原因，完整的源代码可以在本章节的代码库中的`cats_dogs`文件夹中找到：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we use `accimage` as an image decoding backend for `torchvision`. **Accimage**
    ([https://github.com/pytorch/accimage](https://github.com/pytorch/accimage)) is
    an image decoding and preprocessing library designed for `torchvision`, which
    uses Intel IPP ([https://software.intel.com/en-us/intel-ipp](https://software.intel.com/en-us/intel-ipp))
    to improve processing speed.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`accimage`作为`torchvision`的图像解码后端。**Accimage** ([https://github.com/pytorch/accimage](https://github.com/pytorch/accimage))是一个为`torchvision`设计的图像解码和预处理库，它使用英特尔IPP
    ([https://software.intel.com/en-us/intel-ipp](https://software.intel.com/en-us/intel-ipp))来提高处理速度。
- en: 'Above the main entry point, define the `main` function, in which we first load
    and split the training images into a training set and a validation set:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在主入口点上方，定义`main`函数，在其中我们首先加载并将训练图像拆分为训练集和验证集：
- en: '[PRE16]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We split the 25,000 training images into 2 collections, in which 80% of the
    images are randomly selected to form the training set and the remaining 20% form
    the validation set. Here, `_transforms_catsdogs` is defined in `data_utils.py`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将25,000张训练图像拆分成两个集合，其中80%的图像随机选择形成训练集，其余20%形成验证集。在这里，`_transforms_catsdogs`在`data_utils.py`中定义：
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Again, we are not whitening the images. However, if you are interested in how
    to efficiently calculate the mean and standard deviation values of a dataset,
    the code snippet is provided in the `mean_std.py` file.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们没有对图像进行白化处理。然而，如果你有兴趣了解如何高效地计算数据集的均值和标准差，可以查看`mean_std.py`文件中的代码片段。
- en: Be comfortable with using `multiprocessing.Pool` to process your data, which
    is demonstrated in `mean_std.py`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 熟练使用 `multiprocessing.Pool` 来处理数据，这在 `mean_std.py` 中有演示。
- en: 'Get the pre-trained model files from PyTorch Hub and start transfer learning:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 PyTorch Hub 获取预训练模型文件并开始迁移学习：
- en: '[PRE18]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The code for evaluation is omitted due to the length. Here, `transfer_init`
    is defined in `model_ensemble.py` and is responsible for replacing the second
    to the last layer in each model so that we can train it for any number of classes:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代码过长，评估代码已省略。这里，`transfer_init` 在 `model_ensemble.py` 中定义，负责替换每个模型倒数第二层，以便我们可以根据需要训练任意数量的类别：
- en: '[PRE19]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here's why we can transfer the knowledge learned from one domain (trained on
    ImageNet) to another domain (Cats vs. Dogs) by simply replacing the last layer
    (usually a fully connected layer). All of the convolution layers in a CNN are
    responsible for extracting features from the image and intermediate feature maps.
    The fully connected layer can be seen as recombining the highest-level features
    to form the final abstraction of the raw data. It is obvious that good models
    trained on ImageNet are good at extracting features. Therefore, recombining those
    features differently is highly likely to be capable for an easier dataset such
    as Cats vs. Dogs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为什么我们可以通过简单地替换最后一层（通常是全连接层），将从一个领域（在 ImageNet 上训练）学到的知识迁移到另一个领域（猫狗分类）。CNN
    中的所有卷积层负责从图像及中间特征图中提取特征。全连接层可以看作是重新组合最高层次的特征，形成原始数据的最终抽象。显然，在 ImageNet 上训练的好模型非常擅长提取特征。因此，以不同方式重新组合这些特征，很可能能够处理像猫狗分类这样较简单的数据集。
- en: 'Also, `data_prefetcher` is used to speed up the training process. It''s defined
    in `data_utils.py`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`data_prefetcher` 用于加速训练过程。它在 `data_utils.py` 中定义：
- en: '[PRE20]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The training of these individual models can be really fast. Here is the GPU
    memory consumption and validation accuracy after 25 epochs of transfer learning:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些单独模型的训练速度可以非常快。以下是经过 25 个 epoch 的迁移学习后的 GPU 内存消耗和验证准确率：
- en: '| **Model** | **Memory** | **Accuracy** |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **内存** | **准确率** |'
- en: '| MobileNet V2 | 1665MB | 98.14% |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet V2 | 1665MB | 98.14% |'
- en: '| ResNet-18 | 1185MB | 98.24% |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 | 1185MB | 98.24% |'
- en: '| DenseNet | 1943MB | 98.76% |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet | 1943MB | 98.76% |'
- en: '| GoogleNet | 1447MB | 98.06% |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| GoogleNet | 1447MB | 98.06% |'
- en: '| ResNeXt-50 | 1621MB | 98.98% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ResNeXt-50 | 1621MB | 98.98% |'
- en: ResNet-34, ShuffleNet V2, SqueezeNet, and VGG-11 are not selected due to either
    low performance or high memory consumption (over 2 GB).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet-34、ShuffleNet V2、SqueezeNet 和 VGG-11 未被选择，原因是它们的性能较低或内存消耗过高（超过 2 GB）。
- en: Saving your model to the hard drive with `torch.save(model.state_dict(), PATH)`
    will only export the parameter values and you need to explicitly define `model`
    before loading it in another script. However, `torch.save(model, PATH)` will save
    everything, including the model definition, to the file.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `torch.save(model.state_dict(), PATH)` 将模型保存到硬盘时，只会导出参数值，你需要在加载到另一个脚本时明确地定义
    `model`。然而，`torch.save(model, PATH)` 会保存所有内容，包括模型定义。
- en: 'Put together the ensemble classifier in `model_ensemble.py`:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `model_ensemble.py` 中组合集成分类器：
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, the prediction results from all models are combined together to give the
    final prediction in `vote_layer`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，将所有模型的预测结果组合在一起，在 `vote_layer` 中给出最终预测。
- en: Alternatively, you can always directly put together the feature maps from the
    last convolution layers in the pretrained models and train one single fully connected
    layer to predict the image label.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你也可以直接将预训练模型中最后一层卷积层的特征图组合起来，训练一个单独的全连接层来预测图像标签。
- en: 'Get back to the `cats_dogs.py` file and start training the ensemble classifier:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到 `cats_dogs.py` 文件并开始训练集成分类器：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Again, the code for evaluation is omitted due to the length. Validation accuracy
    of the ensemble classifier reaches 99.32% after only 2 epochs of training. The
    training of the ensemble classifier only takes 2775 MB of the GPU memory and the
    exported model file size is no more than 200 MB.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 再次由于代码过长，评估代码已省略。集成分类器经过仅仅 2 个 epoch 的训练后，验证准确率达到了 99.32%。集成分类器的训练仅占用 2775 MB
    的 GPU 内存，导出的模型文件大小不超过 200 MB。
- en: Breaking the classifier with advGAN
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 advGAN 打破分类器
- en: 'The GAN model we''ll use for generating adversarial examples is largely borrowed
    from [https://github.com/mathcbc/advGAN_pytorch](https://github.com/mathcbc/advGAN_pytorch).
    Let''s create two files named `advGAN.py` and `models.py` and put the following
    code in these files:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于生成对抗样本的GAN模型主要借鉴自[https://github.com/mathcbc/advGAN_pytorch](https://github.com/mathcbc/advGAN_pytorch)。让我们创建两个文件，分别命名为`advGAN.py`和`models.py`，并将以下代码放入这些文件中：
- en: '`advGAN.py`: Within this file, you will see the following:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`advGAN.py`：在这个文件中，你将看到以下内容：'
- en: '[PRE23]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Part of the code is omitted due to the length. We can see that this GAN model
    is only responsible for generating the noise part in the adversarial example,
    which is clamped to [-0.3, 0.3] before being added to the original image. During
    training, MSE loss is used to measure the discriminator loss. L1-loss is used
    to calculate the adversarial loss for the generator. The L2-norm of the generated
    perturbation noise is also included in the generator loss. However, the performance
    of the GAN is highly related to the classifier (`self.model`) we are aiming to
    break, which means that the GAN model needs to be retrained each time a new classifier
    is introduced.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅原因，部分代码被省略。我们可以看到，这个GAN模型只负责生成对抗示例中的噪声部分，生成的噪声会被限制在[-0.3, 0.3]范围内，然后加到原始图像中。在训练过程中，使用MSE损失来衡量判别器损失，使用L1损失来计算生成器的对抗损失。生成的扰动噪声的L2范数也包含在生成器损失中。然而，GAN的性能与我们试图突破的分类器（`self.model`）密切相关，这意味着每次引入新的分类器时，都需要重新训练GAN模型。
- en: The code in `models.py` is omitted here but is available in the code repository
    for this chapter, since you can basically design the discriminator and generator
    any way you like. Here, we use a 4-layer CNN as the discriminator network and
    a 14-layer ResNet-like CNN as the generator network.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`models.py`中的代码在这里省略，但可以在本章的代码库中找到，因为你基本上可以根据自己的需求设计判别器和生成器。这里，我们使用一个4层的CNN作为判别器网络，以及一个14层类似ResNet的CNN作为生成器网络。'
- en: Back to `cats_dogs.py`, we need to train the GAN model to learn how to break
    the ensemble classifier.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 返回`cats_dogs.py`，我们需要训练GAN模型来学习如何突破集成分类器。
- en: 'Redefine the data loader because we need a smaller batch size to fit in the
    11 GB GPU memory:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新定义数据加载器，因为我们需要一个更小的批量大小来适应11 GB的GPU内存：
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Start training the GAN model:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始训练GAN模型：
- en: '[PRE25]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Attack the ensemble classifier with the GAN:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用GAN攻击集成分类器：
- en: '[PRE26]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'It takes about 6 hours to finish 60 epochs of training on the GAN model. The
    attack result may look like this:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练GAN模型60个epoch大约需要6个小时。攻击结果可能如下所示：
- en: '[PRE27]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We can see that the validation accuracy drops from 99.32% to 10.3% as a result
    of the adversarial attack by the GAN.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，验证准确率从99.32%降到10.3%，这是由于GAN对抗攻击的结果。
- en: 'Display some of the misclassified images with `matplotlib`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`matplotlib`显示一些被错误分类的图像：
- en: '[PRE28]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now that everything in the code is finished, it's time to finally actually run
    our program.  We need to do this multiple times, once for each model. Create an
    empty folder named models in your code folder to hold the saved models.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，代码中的一切都已经完成，最终我们可以真正运行程序了。我们需要多次运行程序，每次针对不同的模型。请在你的代码文件夹中创建一个名为models的空文件夹，用于保存模型。
- en: 'We''ll start the program from the command line:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从命令行启动程序：
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Once all the models have run, we can finally test our ensemble code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有模型都运行完毕，我们就可以最终测试我们的集成代码：
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Here are some of the pertubated images generated by the GAN that fooled our
    ensemble classifier:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些由GAN生成的扰动图像，这些图像成功欺骗了我们的集成分类器：
- en: '![](img/0156142b-1ebb-4e7d-b7b9-67837d9e8eab.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0156142b-1ebb-4e7d-b7b9-67837d9e8eab.png)'
- en: Adversarial examples generated by the GAN
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: GAN生成的对抗样本
- en: Summary
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: A lot that has gone on in this chapter. You've learned the basics of Fast Gradient
    Sign Methods, how to train a  classifier with pre-trained models, how to deal
    with transfer learning, and much more.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容涵盖了很多内容。你学到了快速梯度符号法（Fast Gradient Sign Methods）的基础知识，如何使用预训练模型训练分类器，如何处理迁移学习，等等。
- en: In the next chapter, we will show how to combine **NLP** (**Natural Language
    Processing**) with GANs and generate images from the description text.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将展示如何将**NLP**（**自然语言处理**）与GAN结合，并从描述文本中生成图像。
- en: References and further reading list
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献和进一步阅读列表
- en: Goodfellow I, Papernot N, Huang S, et. al. (Feb 24, 2017). *Attacking machine
    learning with adversarial examples*. Retrieved from [https://openai.com/blog/adversarial-example-research](https://openai.com/blog/adversarial-example-research).
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Goodfellow I, Papernot N, Huang S, 等（2017年2月24日）。*通过对抗性样本攻击机器学习*。来源：[https://openai.com/blog/adversarial-example-research](https://openai.com/blog/adversarial-example-research)。
- en: Brown T, Mané D, Roy A, et al (2017). *Adversarial Patch*. NIPS.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Brown T, Mané D, Roy A, 等（2017）。*对抗性补丁*。NIPS。
- en: Athalye A, Engstrom L, Ilyas A. (2018). *Synthesizing Robust Adversarial Examples*.
    ICML.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Athalye A, Engstrom L, Ilyas A.（2018）。*合成稳健的对抗性样本*。ICML。
