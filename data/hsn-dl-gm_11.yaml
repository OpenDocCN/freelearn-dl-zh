- en: Rewards and Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励和强化学习
- en: Rewards are a fundamental aspect of reinforcement learning, and the concept
    is easy to grasp. After all, we partly teach and train others—dogs and children,
    for instance—with reinforcement through rewards. The concept of implementing rewards
    or a `reward` function in a simulation can be somewhat difficult, and prone to
    a lot of trial and error. This is the reason for waiting until a later and more
    advanced chapter to talk about rewards, building `reward` functions, and reward
    assistance methods such as Curriculum Learning, Backplay, Curiosity Learning,
    and Imitation Learning / Behavioral Cloning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励是强化学习的一个基本概念，且易于理解。毕竟，我们在一定程度上是通过奖励来训练和教导他人——比如训练狗或小孩。将奖励或`reward`函数实现到仿真中可能会有些困难，并且容易出现很多试错过程。这也是为什么我们要等到更后面、更高级的章节再讨论奖励、构建`reward`函数以及奖励辅助方法，如课程学习、反向播放、好奇心学习和模仿学习/行为克隆。
- en: 'Here is a quick summary of the concepts we will cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是本章将要讲解概念的简要总结：
- en: Rewards and `reward` functions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励和`reward`函数
- en: Sparsity of rewards
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励的稀疏性
- en: Curriculum Learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程学习
- en: Understanding Backplay
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解反向播放
- en: Curiosity Learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好奇心学习
- en: While this is an advanced chapter, it is also an essential one and not something
    you want to skip over. Likewise, many of the top-performing RL demos, such as
    AlphaStar from DeepMind, use the advanced algorithms in this chapter to teach
    agents to do tasks that were previously not thought possible.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章内容较为高级，但它也是至关重要的一章，不容错过。同样，许多顶级的强化学习示范，如DeepMind的AlphaStar，都是利用本章中的高级算法来教导代理执行以前认为不可能完成的任务。
- en: Rewards and reward functions
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励和奖励函数
- en: We often face this preconceived notion of rewards-based learning or training
    as comprising of an action being completed, followed by a reward, be it good or
    bad. While this notion of RL works completely fine for a single action-based task,
    such as the old multi-arm bandit problem we looked at earlier, or teaching a dog
    a trick, recall that reinforcement learning is really about an agent learning
    the value of actions by anticipating future rewards through a series of actions.
    At each action step, when the agent is not exploring, the agent will determine
    its next course of action based on what it perceives as having the best reward.
    What is not always so clear is what those rewards should represent numerically,
    and to what extent that matters. Therefore, it is often helpful to map out a simple
    set of `reward` functions that describe the learning behavior we want our agent
    to train on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们常常面对奖励学习或训练的先入为主的观念，即任务完成后会有奖励，可能是好的也可能是坏的。虽然这种基于奖励的强化学习概念对于单一动作的任务完全适用，比如之前提到的经典多臂赌博机问题，或是教狗做一个动作，但请记住，强化学习实际上是关于代理通过一系列动作预测未来奖励，进而学习动作的价值。在每个动作步骤中，当代理不再进行探索时，它会根据所感知到的最佳奖励来决定下一步的行动。并不是总能清楚地知道这些奖励应该在数值上表示什么，以及这一点有多重要。因此，通常需要绘制出一组简单的`reward`函数，描述我们希望代理训练的学习行为。
- en: 'Let''s open up the Unity editor to the GridWorld example and learn how to create
    a set of `reward` functions and mappings that describe that training, as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开Unity编辑器中的GridWorld示例，并学习如何创建一组`reward`函数和映射，描述该训练过程，如下所示：
- en: Open up the `GridWorld` example from the Assets | ML-Agents | Examples | GridWorld
    | Scenes folder.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Assets | ML-Agents | Examples | GridWorld | Scenes文件夹中打开`GridWorld`示例。
- en: Select the trueAgent object in the Hierarchy and then switch the agent's brain,
    at Grid Agent | Brain, to GridWorldLearning.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级视图中选择trueAgent对象，然后将代理的脑部（Grid Agent | Brain）切换为GridWorldLearning。
- en: Select the GridAcademy and set the Grid Academy | Brains | Control option to
    enabled.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择GridAcademy并将Grid Academy | Brains | Control选项设置为启用。
- en: Select and disable the Main Camera in the scene. This will make the agent's
    camera the primary camera, and the one we can view the scene with.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择并禁用场景中的主摄像机。这样代理的摄像机将成为主要摄像机，且我们可以通过它来查看场景。
- en: Open up and prepare a Python or Anaconda window for training. Check previous
    chapters or the Unity documentation if you need to remember how to do this.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开并准备一个Python或Anaconda窗口用于训练。如果需要记起如何操作，请查看前几章或Unity文档。
- en: Save the scene and project.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存场景和项目。
- en: 'Launch the sample into training using the following command at the Python/Anaconda
    window:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令在Python/Anaconda窗口中启动示例进行训练：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'One of the first things you will appreciate about this sample is how quickly
    it trains. Remember that the primary reason the agent trains so quickly is because
    the state space is so small; 5x5 in this example. An example of the simulation
    running is shown in the following screenshot:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会很快感受到这个示例的训练速度。记住，代理训练如此快速的主要原因是状态空间非常小；在这个例子中是 5x5。以下截图展示了模拟运行的一个例子：
- en: '![](img/9c4ababe-df7d-4469-8711-394029344d77.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c4ababe-df7d-4469-8711-394029344d77.png)'
- en: GridWorld example running on 5x5 grid
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GridWorld 示例运行在 5x5 网格上
- en: Run the sample until completion. It does not take long to run, even on older
    systems.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行示例直到完成。即使在较老的系统上运行，也不会花费太长时间。
- en: Notice how the agent quickly goes from a negative reward to a positive reward
    as it learns to place the cube over the green +. However, did you notice that
    the agent starts training from a negative mean reward? The agent starts with a
    zero reward value, so let's examine where the negative reward is coming from.
    In the next section, we look at how to build the `reward` functions by looking
    at the code.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当代理学习将立方体放置到绿色+号上时，代理迅速从负奖励转变为正奖励。然而，你是否注意到代理从一个负的平均奖励开始训练？代理初始时的奖励值为零，所以让我们来看看负奖励是从哪里来的。在接下来的章节中，我们将通过查看代码来了解如何构建
    `reward` 函数。
- en: Building reward functions
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建奖励函数
- en: Building `reward` functions can be quite simple, as this one will be, or extremely
    complex, as you may well imagine. While this step is optional for training these
    examples, it is almost mandatory when you go to build your own environments. It
    can also identify problems in your training, and ways of enhancing or easing training
    as well.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 `reward` 函数可以非常简单，像这个例子一样，或者非常复杂，正如你可能想象的那样。虽然在训练这些示例时这一步是可选的，但在你构建自己的环境时几乎是必需的。它还可以帮助你发现训练中的问题，以及提高或简化训练的方法。
- en: 'Open up the Unity editor and follow this exercise to build these sample `reward`
    functions:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 Unity 编辑器，并按照本练习构建这些示例 `reward` 函数：
- en: Select the trueAgent object in the Hierarchy window and then click the target
    icon beside the Grid Agent component.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级窗口中选择 trueAgent 对象，然后点击网格代理组件旁边的目标图标。
- en: Select Edit Script from the Contact menu.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从联系菜单中选择编辑脚本。
- en: 'After the script opens in your editor, scroll down to the `AgentAction` method
    as follows:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本在编辑器中打开后，向下滚动到 `AgentAction` 方法，如下所示：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We want to focus on the highlighted lines, `AddReward` and `SetReward`:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们要关注高亮显示的行，`AddReward` 和 `SetReward`：
- en: '`AddReward(-.1f)`: This first line denotes a step reward. Every step the agent
    takes will cost the agent a negative reward. This is the reason we see the agent
    show negative rewards until it finds the positive reward.'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AddReward(-.1f)`：这一行表示步骤奖励。代理每走一步都会受到负奖励。这也是我们看到代理展示负奖励，直到它找到正奖励的原因。'
- en: '`SetReward(1f)`: This the final positive reward the agent receives, and it
    is set to the maximum value of `1`. In these types of training scenarios, we prefer
    to use a range of rewards from -1 to +1.'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SetReward(1f)`：这是代理收到的最终正奖励，并且设置为最大值 `1`。在这类训练场景中，我们通常使用从 -1 到 +1 的奖励范围。'
- en: '`SetReward(-1f)`: This is the pit of death reward, and a final negative reward.'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SetReward(-1f)`：这是死亡深渊奖励，也是最终的负奖励。'
- en: 'Using each of the previous statements, we can map these to `reward` functions
    as follows:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用之前的每个语句，我们可以将这些映射到 `reward` 函数，如下所示：
- en: '`AddReward(-.1f)` = ![](img/7a5780bb-6d40-4c81-9281-60fa27a826b0.png)'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AddReward(-.1f)` = ![](img/7a5780bb-6d40-4c81-9281-60fa27a826b0.png)'
- en: '`SetReward(1f)` = ![](img/46cf177b-acec-48c4-895b-52e4affbe006.png)'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SetReward(1f)` = ![](img/46cf177b-acec-48c4-895b-52e4affbe006.png)'
- en: '`SetReward(-1f)` = ![](img/93703d4c-bc45-428f-8af7-cabf822b5d37.png)'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SetReward(-1f)` = ![](img/93703d4c-bc45-428f-8af7-cabf822b5d37.png)'
- en: One thing to notice here is that `AddReward` is an incremental reward, while
    `SetReward` sets the final value. So, the agent only ever sees a positive reward
    by reaching the final goal.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里需要注意的一点是，`AddReward` 是增量奖励，而 `SetReward` 设置最终值。所以，代理只有通过达到最终目标才能看到正奖励。
- en: By mapping these `reward` functions, we can see that the only way an agent can
    learn a positive reward is by finding its way to a goal. This is the reason the
    agent begins with a negative reward, it essentially only first learns to avoid
    wasting time or moves until it randomly encounters the goal. From there, the agent
    can quickly assign value to states based on previous positive rewards received.
    The issue is that the agent first needs to encounter a positive reward before
    we begin with the actual training. We discuss this particular problem in the next
    section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过映射这些`reward`函数，我们可以看到，代理要想学习到正奖励，唯一的办法就是找到目标。这就是为什么代理一开始会收到负奖励的原因，代理本质上是先学会避免浪费时间或行动，直到它随机遇到目标。从那时起，代理可以根据之前获得的正奖励快速给状态赋值。问题在于，代理首先需要遇到正奖励，然后我们才能开始实际的训练。我们将在下一节讨论这个问题。
- en: Sparsity of rewards
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励稀疏
- en: 'We call the situation where an agent does not get enough, or any, positive
    rewards, a sparsity of rewards. The simplest way to show how a sparsity of rewards
    can happen is by example, and fortunately, the GridWorld example can easily demonstrate
    this for us. Open the editor to the GridWorld example and follow this exercise:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称代理没有得到足够的正奖励，或者根本没有正奖励的情况为奖励稀疏。展示奖励稀疏如何发生的最简单方法是通过示例，幸运的是，GridWorld示例能够轻松地为我们演示这一点。打开编辑器中的GridWorld示例，并按照本练习操作：
- en: Open the GridWorld sample scene from where we left it in the last exercise.
    For the purposes of this exercise, it is also helpful to have trained the original
    sample to completion. GridWorld is one of those nice compact examples that train
    quickly and is an excellent place to test basic concepts, or even hyperparameters.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开上一个练习中我们离开的GridWorld示例场景。为了本次练习的目的，最好已经将原始示例训练完成。GridWorld是一个紧凑的小示例，训练速度快，是测试基本概念甚至超参数的绝佳场所。
- en: 'Select the GridAcademy and change the Grid Academy | Reset Parameters | gridSize
    to `25`, as shown in the following screen excerpt:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择GridAcademy并将Grid Academy | Reset Parameters | gridSize更改为`25`，如下图所示：
- en: '![](img/2fe193fd-2c8a-4336-ac0b-1d21aa78cf42.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fe193fd-2c8a-4336-ac0b-1d21aa78cf42.png)'
- en: Setting the GridAcademy gridSize parameter
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 设置GridAcademy的gridSize参数
- en: Save the scene and the project.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存场景和项目。
- en: 'Launch the sample into training with the following command from your Python/Anaconda
    window:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从你的Python/Anaconda窗口使用以下命令启动示例进行训练：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will launch the sample and, assuming you still have the agentCam as the
    main camera, you should see the following in the Game window:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将启动示例，并且假设你仍然将agentCam作为主摄像机，你应该在游戏窗口中看到以下内容：
- en: '![](img/a0dfe420-200c-4c5c-bd83-ccb2c2ce722f.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0dfe420-200c-4c5c-bd83-ccb2c2ce722f.png)'
- en: The GridWorld with a grid size of 25x25
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 具有25x25网格大小的GridWorld
- en: We have extended the game play space from a 5x5 grid to a 25x25 grid, making
    the goal (+) symbol much more difficult for the agent to randomly find.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经将游戏的空间从5x5的网格扩展到25x25的网格，使得代理随机找到目标（+）符号变得更加困难。
- en: What you will quickly notice after a few reported iterations is how poorly the
    agent is performing in some cases even, reporting less than a -1 mean reward.
    What's more, the agent could continue training like this for a long time. In fact,
    it is possible the agent could never discover a reward within 100, 200, 1,000,
    or more iterations. Now, this may appear to be a problem of state, and, in some
    ways, you may think of it that way. However, remember that the input state into
    our agent is the same camera view, a state of 84x84 pixels image, and we have
    not changed that. So, for the purposes of this example, think of state in the
    policy RL algorithm as remaining fixed. Therefore, our best course of action in
    order to fix the problem is to increase the rewards.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会很快注意到，在几轮训练后，代理在某些情况下的表现非常差，甚至报告的平均奖励小于-1。更糟糕的是，代理可能会继续这样训练很长时间。事实上，代理可能在100、200、1,000次甚至更多的迭代中都无法发现奖励。现在，这看起来像是状态的问题，从某种角度来看，你可能会这么认为。然而，记住，我们给代理的输入状态始终是相同的摄像机视图，84x84像素的状态图像，我们并没有改变这一点。因此，在这个示例中，可以认为策略RL算法中的状态保持不变。因此，解决问题的最佳方法是增加奖励。
- en: Stop the training example from the Python/Anaconda window by typing *Ctrl* +
    *C*. In order to be fair, we will increase the number of rewards for goals and
    deaths equally.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Python/Anaconda窗口中通过输入*Ctrl* + *C*停止训练示例。为了公平起见，我们将平等地增加目标和死亡的奖励数量。
- en: 'Back in the editor, select the GridAcademy and increase the numObstacles and
    numGoals on the Grid Academy | Reset Parameters component properties, as shown
    in the following excerpt:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回编辑器，选择GridAcademy并增加Grid Academy | Reset Parameters组件属性中的numObstacles和numGoals，如下所示：
- en: '![](img/a0980bb1-9142-4054-8d0d-873996c266bb.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0980bb1-9142-4054-8d0d-873996c266bb.png)'
- en: Updating the number of Obstacles and Goals
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 更新障碍物和目标的数量
- en: Save the scene and the project.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存场景和项目。
- en: 'Launch the training session with the following code:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码启动训练会话：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is to denote that we are running the sample with five times the number
    of obstacles and goals.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这表示我们正在用五倍数量的障碍物和目标运行示例。
- en: Let the agent train for 25,000 iterations and notice the performance increase.
    Let the agent train to completion and compare the results to our first run.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让智能体训练25,000次迭代，并观察性能的提高。让智能体训练直到完成，并将结果与我们的第一次训练进行比较。
- en: The problem of sparsity of rewards is generally encountered more frequently
    in discrete action tasks, such as GridWorld/Hallway and so on. because the `reward`
    function is often absolute. In continuous learning tasks, the `reward` function
    is often more gradual and is typically measured by some progress to a goal, and
    not just the goal itself.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励稀疏性问题通常更常见于离散动作任务中，例如GridWorld/Hallway等，因为`reward`函数通常是绝对的。在连续学习任务中，`reward`函数往往更为渐进，通常通过达到某个目标的进展来衡量，而不仅仅是目标本身。
- en: By increasing the number of obstacles and goals—the negative and positive rewards—we
    are able to train the agent much more quickly, although it is likely you will
    see very erratic cycles of training, and the agent never truly gets as good as
    the original. In fact, the training actually may diverge at some point later on.
    The reason for this is partly because of its limited vision, and we have only
    partially corrected the sparse rewards problem. We can, of course, fix the issue
    of sparse rewards in this example by simply increasing the number of goals and
    obstacles. You can go back and try a value of 25 for the number of obstacles and
    rewards and see much more stable, long-term results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增加障碍物和目标的数量——即负奖励和正奖励——我们能够更快速地训练智能体，尽管你可能会看到非常不稳定的训练周期，而且智能体的表现永远不会真正达到最初的水平。实际上，训练在某个点可能会发生偏离。造成这种情况的原因部分是由于其有限的视野，并且我们仅部分解决了稀疏奖励问题。当然，我们可以通过简单地增加目标和障碍物的数量来解决这个稀疏奖励问题。你可以返回并尝试将障碍物和奖励的数量设置为25，看看能否得到更稳定的长期结果。
- en: Of course, in many RL problems, an increasing number of rewards is not an option,
    and we need to look at cleverer methods, as we will see in the next section. Fortunately,
    a number of methods have arisen, in very brief time, looking to address the problem
    of sparse or difficult rewards. Unity, being at the top, quickly jumped on and
    implemented a number of methods, the first of which we will look at is called
    Curriculum Learning, which we will discuss in the next section.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在许多强化学习问题中，增加奖励的数量并不是一个选项，我们需要寻找更巧妙的方法，正如我们在下一节中将看到的那样。幸运的是，很多方法在非常短的时间内相继出现，旨在解决稀疏或困难奖励的问题。Unity作为领先者，迅速采纳并实施了多种方法，其中我们将讨论的第一个方法叫做课程学习（Curriculum
    Learning），我们将在下一节中详细讨论。
- en: Curriculum Learning
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 课程学习
- en: Curriculum Learning allows for an agent to progressively learn a difficult task
    by stepping up the `reward` function. While the reward remains absolute, the agent
    finds or achieves the goal in a simpler manner, and so learns the purpose of the
    reward. Then, as the training progresses and as the agent learns, the difficulty
    of receiving a reward increases, which, in turn, forces the agent to learn.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习允许智能体通过逐步提升`reward`函数来逐步学习一个困难的任务。当奖励保持绝对时，智能体以更简单的方式找到或实现目标，从而学习到奖励的目的。然后，随着训练的进行和智能体的学习，获得奖励的难度增加，这反过来迫使智能体进行学习。
- en: 'Unity, of course, has a few samples of this, and we will look at the `WallJump`
    example of how a Curriculum Learning sample is set up in the following exercise:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Unity有一些此类示例，我们将在接下来的练习中查看`WallJump`示例，了解如何设置一个课程学习样本：
- en: Open the WallJump scene from the Assets | ML-Agents | Examples | WallJump |
    Scenes folder.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开WallJump场景，路径为Assets | ML-Agents | Examples | WallJump | Scenes文件夹。
- en: Select the Academy object in the Hierarchy window.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层次窗口中选择Academy对象。
- en: 'Click both Control options on Wall Jump Academy | Brains | Control parameter
    as shown in the following excerpt:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，点击Wall Jump Academy | Brains | Control参数中的两个控制选项：
- en: '![](img/561b1d1e-5aca-4ddf-8e80-565983988cad.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/561b1d1e-5aca-4ddf-8e80-565983988cad.png)'
- en: Setting the multiple brains to learning
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 设置多个大脑进行学习
- en: This sample uses multiple brains in order to better separate the learning by
    task. In fact, all the brains will be trained in tandem.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个示例使用多个大脑来更好地按任务分离学习。实际上，所有大脑将同时进行训练。
- en: Curriculum Learning uses a second configuration file to describe the curriculum
    or steps of learning the agent will undergo.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 课程学习使用第二个配置文件来描述代理将经历的课程或学习步骤。
- en: Open the `ML-Agents/ml-agents/config/curricul/wall-jump` folder.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`ML-Agents/ml-agents/config/curricul/wall-jump`文件夹。
- en: 'Open the `SmallWallJumpLearning.json` file in a text editor. The file is shown
    for reference as follows:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文本编辑器中打开`SmallWallJumpLearning.json`文件。文件如下所示：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This JSON file defines the configuration the SmallWallJumpLearning brain will
    take as part of its curriculum or steps to learning. The definition for all these
    parameters are well documented in the Unity documentation, but we will take a
    look at parameters from the documentation as follows:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个JSON文件定义了SmallWallJumpLearning大脑作为其课程或学习步骤的一部分所采取的配置。这些参数的定义在Unity文档中有详细说明，但我们将按照文档中的参数进行查看：
- en: '`measure` *–* What to measure learning progress, and advancement in lessons
    by:'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`measure` *–* 衡量学习进展和课程进度的指标：'
- en: reward – Uses a measure received reward.
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: reward – 使用衡量标准接收到的奖励。
- en: progress – Uses ratio of steps/max_steps.
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: progress – 使用步骤与最大步骤数的比率。
- en: '`thresholds` (float array) –Points in value of measure where the lesson should
    be increased.'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thresholds`（浮动数组）– 衡量标准值的点，在这些点上课程应当提升。'
- en: '`min_lesson_length` (int) *–* The minimum number of episodes that should be
    completed before the lesson can change. If a measure is set to reward, the average
    cumulative reward of the last `min_lesson_length` episodes will be used to determine
    if the lesson should change. Must be non-negative.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_lesson_length`（整数）*–* 在课程可以更改之前应完成的最小回合数。如果设置了奖励衡量标准，则将使用最近`min_lesson_length`回合的平均累积奖励来决定是否应更改课程。必须是非负数。'
- en: What we can see by reading this file is that there are three lessons set by
    a `measure` of `progress` defined by the number of episodes. The episode boundaries
    are defined at `.1` or 10%, `.3` or 30%, and `.5` or 50% of the total episodes.
    With each lesson, we set parameters defined by boundaries, and in this example
    the parameter is `small_wall_height` with a first lesson boundary of `1.5` to
    `2.0`, a second lesson boundary of `2.0` to `2.5`, and a third lesson at `2.5`
    to `4.0`.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过阅读此文件我们可以看到，设定了三个课程，通过`progress`的`measure`来定义，`progress`由回合数来表示。回合边界定义为`.1`或10%，`.3`或30%，和`.5`或50%总回合数。每个课程我们都通过边界定义参数，在这个例子中，参数是`small_wall_height`，第一个课程的边界是`1.5`到`2.0`，第二个课程的边界是`2.0`到`2.5`，第三个课程的边界是`2.5`到`4.0`。
- en: Open up a Python/Anaconda window and prepare it for training.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个Python/Anaconda窗口并准备好进行训练。
- en: 'Launch the training session with the following command:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令启动训练会话：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The extra bit that is highlighted adds the folder to the secondary curriculum
    configuration.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高亮的额外部分将文件夹添加到辅助课程配置中。
- en: You will need to wait for at least half of the full training steps to run in
    order to see all three levels of training.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要等待至少一半的完整训练步骤才能看到所有三个训练阶段。
- en: This example introduced one technique we can use to solve the problem of sparse
    or difficult to achieve rewards. In the next section, we look at a specialized
    form of Curriculum Training called Backplay.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子介绍了一种我们可以用来解决稀疏或难以获得奖励问题的技术。在接下来的部分中，我们将讨论一种专门的课程训练形式，叫做Backplay。
- en: Understanding Backplay
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Backplay
- en: 'In late 2018, Cinjon Resnick released an innovative paper, titled *Backplay:* *Man
    muss immer umkehren*, ([https://arxiv.org/abs/1807.06919](https://arxiv.org/abs/1807.06919))
    that introduced a refined form of Curriculum Learning called Backplay. The basic
    premise is that you start the agent more or less at the goal, and then progressively
    move the agent back during training. This method may not work for all situations,
    but we will use this method with Curriculum Training to see how we can improve
    the VisualHallway example in the following exercise:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年末，Cinjon Resnick发布了一篇创新论文，题为*Backplay:* *Man muss immer umkehren*，([https://arxiv.org/abs/1807.06919](https://arxiv.org/abs/1807.06919))，其中介绍了一种叫做Backplay的课程学习改进方法。基本前提是，训练时将代理从目标开始，然后逐步将其移回。这种方法可能并不适用于所有情况，但我们将使用这种方法结合课程训练，看看如何在以下练习中改进VisualHallway示例：
- en: Open the VisualHallway scene from the Assets |ML-Agents | Examples | Hallway
    | Scenes folder.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Assets | ML-Agents | Examples | Hallway | Scenes文件夹中打开VisualHallway场景。
- en: Make sure the scene is reset to the default starting point. If you need to,
    pull down the source from ML-Agents again.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保场景重置为默认的起始点。如果需要，可以再次从ML-Agents拉取源代码。
- en: Set the scene for learning using the VisualHallwayLearning brain, and make sure
    that the agent is just using the default visual observations of 84x84.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用VisualHallwayLearning大脑设置学习场景，并确保智能体仅使用默认的84x84视觉观察。
- en: 'Select the Academy object and in the Inspector window add a new Hallway Academy | Reset
    Parameter called `distance`, as shown in the following excerpt:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择Academy对象，在检查器窗口中添加一个新的Hallway Academy | Reset Parameter，命名为`distance`，如下所示：
- en: '![](img/cf28b7dd-01eb-4697-bab2-15e476686232.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf28b7dd-01eb-4697-bab2-15e476686232.png)'
- en: Setting a new Reset Parameter on the Academy
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在Academy上设置新的重置参数
- en: You can use Reset Parameters for more than just Curriculum Learning, as they
    can help you easily configure training parameters within the editor. The parameter
    we are defining here is going to set the distance, the agent is away from the
    back goal region. This sample is intended to show the concept of Backplay, and
    in order to properly implement it we would need to move the agent right in front
    of the proper goal—we will defer from doing this for now.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以将重置参数用于不仅仅是课程学习，因为它们可以帮助你轻松配置编辑器中的训练参数。我们在这里定义的参数将设置智能体距离目标区域的距离。此示例旨在展示Backplay的概念，为了正确实现它，我们需要将智能体移动到目标的正前方——但我们暂时不进行这一操作。
- en: Select the VisualHallwayArea | Agent and open the Hallway Academy script in
    your code editor of choice.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择VisualHallwayArea | Agent，并在你喜欢的代码编辑器中打开Hallway Academy脚本。
- en: 'Scroll down to the `AgentReset` method and adjust the top line to that shown
    as follows:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向下滚动到`AgentReset`方法，并将顶部代码行调整为如下所示：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This single line of code will adjust the starting offset of the agent to the
    now preset Reset Parameters of the Academy. Likewise, as the Academy updates those
    parameters during training, the agent will also see updated values.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这一行代码将调整智能体的起始偏移量，以适应现在预设的Academy重置参数。同样，当Academy在训练过程中更新这些参数时，智能体也会看到更新的值。
- en: Save the file and return to the editor. The editor will recompile your code
    changes and let you know if everything is okay. A red error in the console will
    typically mean you have a compiler error, likely caused by incorrect syntax.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存文件并返回编辑器。编辑器会重新编译你的代码更改，并告知你一切是否正常。如果控制台出现红色错误，通常意味着有编译错误，可能是由语法错误引起的。
- en: 'Open a prepared Python/Anaconda window and run the training session with the
    following command:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个准备好的Python/Anaconda窗口，并运行以下命令来开始训练：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This will run the session in regular mode, without Curriculum Learning, but
    it will adjust the starting position of the agent to be closer to the goals. Let
    this sample run and see how well the agent performs now that it starts so close
    to the goals.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将以常规模式运行会话，不使用课程学习，但它会将智能体的起始位置调整得更接近目标区域。让这个示例运行，并观察智能体在如此接近目标时的表现如何。
- en: Let the training run for a while and observe the difference in training from
    the original. One thing you will notice is that the agent can't help but run into
    the reward now, which is what we are after. The next piece we need to implement
    is the Curriculum Learning part, where we will move the agent back as it learns
    to find the reward in the next section.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让训练运行一段时间，并观察与原始训练的区别。你会注意到的一点是，智能体现在不自觉地跑向奖励区域，这正是我们所期望的。接下来我们需要实现的是课程学习部分，即智能体在学习如何在下一部分找到奖励的过程中逐渐向后移动。
- en: Implementing Backplay through Curriculum Learning
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过课程学习实现Backplay
- en: 'In the last section, we implemented the first part of Backplay, which is having
    the agent start next to, or very close to the goal. The next part we need to accomplish
    is progressively moving the agent back to its intended starting point using Curriculum
    Learning. Open up the Unity editor to the VisualHallway scene again and follow
    these steps:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们实现了Backplay的第一部分，即让智能体起始于目标附近或非常接近目标的位置。接下来我们需要完成的部分是使用课程学习（Curriculum
    Learning）将智能体逐步移回到它的预定起始点。请再次打开Unity编辑器并进入VisualHallway场景，按照以下步骤操作：
- en: Open the `ML-Agents/ml-agents/config` folder with a file explorer or command
    shell.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用文件浏览器或命令行打开`ML-Agents/ml-agents/config`文件夹。
- en: Create a new folder called `hallway` and navigate to the new folder.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`hallway`的新文件夹并进入该文件夹。
- en: Open a text editor or create a new JSON text file called `VisualHallwayLearning.json`
    in the new directory. **JavaScript Object Notation** (**JSON**)is intended to
    describe objects in JavaScript, it has become a standard for configuration settings
    as well.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开文本编辑器或在新目录下创建一个名为 `VisualHallwayLearning.json` 的新 JSON 文本文件。**JavaScript 对象表示法**（**JSON**）用于描述
    JavaScript 中的对象，它也成为了一种配置设置的标准。
- en: 'Enter the following JSON text in the new file:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新文件中输入以下 JSON 文本：
- en: '[PRE8]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This configuration file defines a curriculum that we will use to train an agent
    on Backplay. The file defines a `measure` of `rewards` and `thresholds` that define
    when the agent will advance to the next level of training. When a reward threshold
    is hit for a minimum episode length of `100` steps, than the training will advance
    to the next `distance` parameter. Notice how we define the distance parameter
    with `12`, representing a distance close to the goals, and then decreasing. You
    could, of course, create a function that maps different range values, but we will
    leave that up to you.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个配置文件定义了我们将在 Backplay 上训练代理的课程。该文件定义了一个 `measure`，包括 `rewards` 和 `thresholds`，用于确定代理何时会晋级到下一个训练阶段。当奖励阈值达到最低
    `100` 步长时，训练将进入下一个 `distance` 参数。注意，我们如何将距离参数定义为 `12`，表示距离目标较近，然后逐渐减少。当然，你也可以创建一个函数来映射不同的范围值，但这部分留给你自己完成。
- en: Save the file after you are done editing.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑完成后保存文件。
- en: 'Launch a training session from a Python/Anaconda window with the following
    command:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Python/Anaconda 窗口启动训练会话，使用以下命令：
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After the training starts, notice how the curriculum is getting set in the
    Python/Anaconda window, as shown in the following screenshot:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练开始后，注意 Python/Anaconda 窗口中课程如何被设置，如下图所示：
- en: '![](img/6fe16960-1238-46f1-ba9e-737ee9c2831b.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6fe16960-1238-46f1-ba9e-737ee9c2831b.png)'
- en: Watching the curriculum parameters getting set in training
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 观看训练中课程参数的设置
- en: Wait for the agent to train, and see how many levels of training it can accomplish
    before the end of the session.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待代理训练完成，看看它能在会话结束前完成多少个训练级别。
- en: Now, one thing we need to come clean about is that this sample is more an innovative
    example than a true example of Backplay. Actual Backplay is described as putting
    the agent at the goal and working backward. In this example, we are putting the
    agent almost at the goal and working backward. The difference is subtle, but,
    by now, hopefully you can appreciate that, in terms of training, it could be significant.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要澄清的一点是，这个示例更像是一个创新示例，而不是 Backplay 的真实示例。实际的 Backplay 是将代理放在目标位置，然后向后工作。在这个示例中，我们几乎将代理放置在目标位置并向后工作。这个差异是微妙的，但到现在为止，希望你能理解，从训练的角度来看，这可能是有意义的。
- en: Curiosity Learning
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 好奇心学习
- en: Up until now, we have considered just the extrinsic or external rewards an agent
    may receive in an environment. The Hallway example, for instance, gives a +1 external
    reward when the agent reaches the goal, and a -1 external reward if it gets the
    wrong goal. However, real animals like us can actually learn based on internal
    motivations, or by using an internal `reward` function. A great example of this
    is a baby (a cat, a human, or whatever) that has an obvious natural motivation
    to be curious through play. The curiosity of playing provides the baby with an
    internal or intrinsic reward, but the actual act itself gives it a negative external
    or extrinsic reward. After all, the baby is expending energy, a negative external
    reward, yet it plays on and on in order to learn more general information about
    its environment. This, in turn, allows it to explore more of the environment and
    ultimately attain some very difficult goal, such as hunting, or going to work.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了代理在环境中可能获得的外部奖励。例如，走廊示例当代理到达目标时会给出 +1 的外部奖励，而当它达到错误的目标时会给予 -1 的外部奖励。然而，像我们这样的真实动物其实可以基于内在动机进行学习，或者使用内在的
    `reward` 函数。一个很好的例子是一个婴儿（猫、人类，或者其他任何东西），它通过玩耍有明显的自然动机去保持好奇心。玩耍的好奇心为婴儿提供了一个内在或固有的奖励，但实际上，这一行为本身会带来负面的外部奖励。毕竟，婴儿在消耗能量，这是一个负面的外部奖励，但它依然会继续玩耍，从而学习更多关于环境的一般信息。这反过来使它能够探索更多的环境，并最终达到一些非常困难的目标，比如打猎或上班。
- en: This form of internal or intrinsic reward modeling falls into a subclass of
    RL, called Motivated Reinforcement Learning. As you may well imagine, this whole
    arc of learning could have huge applications in gaming, from creating NPCs to
    more believable opponents that actually get motivated by some personality trait
    or emotion. Imagine having a computer opponent that can get angry, or even, compassionate?
    Of course, we are a long way from getting there, but in the interim, Unity has
    added an intrinsic reward system in order to model agent curiosity, and this is
    called Curiosity Learning.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这种内部或内在奖励建模属于强化学习（RL）的一类子类别，称为动机强化学习（Motivated Reinforcement Learning）。正如你可以想象的，这种学习模式在游戏中有着巨大的应用前景，从创建NPC到构建更具可信度的对手，这些对手实际上会受到某种性格特征或情绪的驱动。试想一下，拥有一个能生气甚至表现出同情心的电脑对手？当然，我们离这个目标还很远，但在此期间，Unity已经添加了一个内在奖励系统，用于模拟智能体的好奇心，这就是所谓的好奇心学习。
- en: '**Curiosity Learning** (**CL**) was first developed by researchers at the University
    of California, Berkley, in a paper called *Curiosity-Driven Exploration by **Self-Supervised
    Prediction,* which you can find at [https://pathak22.github.io/noreward-rl/.](https://pathak22.github.io/noreward-rl/) The
    paper goes on to describe a system of solving sparse rewards problems using forward
    and inverse neural networks. They called the system an **Intrinsic Curiosity Module**
    (**ICM**), with the intent for it to be used as a layer or module on top of other
    RL systems. This is exactly what Unity did, and they have added this as a module
    to ML-Agents.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**好奇心学习**（**CL**）最初由加利福尼亚大学伯克利分校的研究人员在一篇名为*由**自监督预测驱动的好奇心探索*的论文中提出，您可以在[https://pathak22.github.io/noreward-rl/](https://pathak22.github.io/noreward-rl/)中找到该论文。论文进一步描述了一个使用正向和反向神经网络解决稀疏奖励问题的系统。研究人员将该系统称为**内在好奇心模块**（**ICM**），旨在将其作为其他强化学习（RL）系统之上的一层或模块使用。这正是Unity所做的，他们将其作为模块添加到ML-Agents中。'
- en: The Lead Researcher at Unity, Dr. Arthur Juliani, has an excellent blog post
    on their implementation that can be found at [https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Unity的首席研究员Arthur Juliani博士在他们的实现上有一篇很棒的博客文章，您可以在[https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/](https://blogs.unity3d.com/2018/06/26/solving-sparse-reward-tasks-with-curiosity/)找到。
- en: 'ICM works by using an inverse neural network that is trained using the current
    and next observation of the agent. It uses an encoder to encode a prediction on
    what the action was between the two states, current and next. Then, the forward
    network is trained on the current observation and action in which it encodes to
    the next observation. The difference is then taken between the real and predicted
    encodings from the inverse and forward models. In this case, the bigger the difference,
    the bigger the surprise, and the more intrinsic the rewards. A diagram extracted
    from Dr. Juliani''s blog is shown as follows, describing how this works:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ICM通过使用反向神经网络工作，该网络通过当前和下一个观察到的智能体状态进行训练。它使用编码器对两种状态之间的动作进行预测编码，当前状态和下一个状态。然后，正向网络在当前观察和动作的基础上进行训练，将其编码为下一个观察状态。接着，从反向和正向模型中分别计算实际和预测编码之间的差异。在这种情况下，差异越大，惊讶感越强，内在奖励也就越多。以下是从Arthur
    Juliani博士的博客中提取的图示，描述了这一过程如何工作：
- en: '![](img/7c0c6c7f-8ad5-4fa4-a258-c0327d8e0ddf.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c0c6c7f-8ad5-4fa4-a258-c0327d8e0ddf.png)'
- en: Inner workings of the Curiosity Learning Module
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 好奇心学习模块的内部工作原理
- en: The diagram shows the depiction of the two models and layers in blue, forward
    and inverse, with the blue lines depicting network flow, the green box representing
    the intrinsic model calculation, and the reward output in the form of the green
    dotted lines.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表显示了两个模型和层的表示，分别是蓝色的正向和反向，蓝色线条表示网络流动，绿色框表示内在模型计算，而奖励输出则以绿色虚线的形式呈现。
- en: 'Well, that''s enough theory, its time to see how this CL works in practice.
    Fortunately, Unity has a very well developed environment that features this new
    module that is called Pyramids. Let''s open Unity and follow the next exercise
    to see this environment in action:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，理论部分讲得差不多了，接下来是时候看看这个CL如何在实践中运作了。幸运的是，Unity有一个非常完善的环境，展示了这个名为Pyramids的新模块。让我们打开Unity并按照接下来的练习查看这个环境的实际效果：
- en: Open the Pyramid scene from the Assets | ML-Agents | Examples | Pyramids | Scenes
    folder.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Assets | ML-Agents | Examples | Pyramids | Scenes文件夹中的Pyramid场景。
- en: Select the AreaPB(1) to AreaPB(15) in the Hierarchy window and then deactivate
    these objects in the Inspector window.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层级视图窗口中选择AreaPB(1)到AreaPB(15)，然后在检查器窗口中禁用这些对象。
- en: Leave the scene in player mode. For the first time, we want you to play the
    scene on your own and figure out the goal. Even if you read the blog or played
    the scene, try again, but this time, think what reward functions would need to
    be in place.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 离开玩家模式下的场景。第一次，我们希望你自己来体验这个场景并弄清楚目标。即使你已经阅读过博客或玩过这个场景，也请再试一次，不过这次需要思考应该设置哪些奖励函数。
- en: Press Play in the editor and start playing the game in Player mode. If you have
    not played the game before or understand the premise, don't be surprised if it
    takes you a while to solve the puzzle.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编辑器中按下播放按钮，并开始以玩家模式玩游戏。如果你之前没有玩过游戏或者不了解前提，不要感到惊讶，如果你花了一段时间才能解开这个谜题。
- en: Now, for those of you that didn't read or play ahead, here is the premise. The
    scene starts where the agent is randomly placed into an area of rooms with pyramids
    of stone in which one has a switch. The goal of the agent is to activate the switch
    that then spawns a pyramid of sand boxes with a large gold box on top. The switch
    turns from red to green after it is activated. After the pyramid appears, the
    agent then needs to knock the pyramid over and retrieve the gold box. It certainly
    is not the most complex of puzzles, but one that does require a bit of exploration
    and curiosity.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些没有提前阅读或玩过的同学，下面是前提。场景开始时，代理被随机放置在一个有多个房间的区域，里面有石质金字塔，其中一个有开关。代理的目标是激活开关，然后生成一个沙箱金字塔，上面放着一个大金盒子。开关在被激活后会从红色变为绿色。金字塔出现后，代理需要把金字塔推倒并取回金盒子。这并不是最复杂的谜题，但确实需要一些探索和好奇心。
- en: Imagine if we tried to model this form of curiosity, or need to explore, with
    a set of `reward` functions. We would need a `reward` function for activating
    the button, moving to rooms, knocking over blocks, and, of course, getting the
    gold box. Then we would have to determine the value of each of those objectives,
    perhaps using some form of **Inverse Reinforcement Learning** (**IRL**). However,
    with Curiosity Learning, we can create the reward function for just the end goal
    of getting the box (+1), and perhaps a small negative step goal (.0001), then
    use intrinsic curiosity rewards to let the agent learn the remaining steps. Quite
    a clever trick, and we will see how this works in the next section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们尝试用一组`reward`函数来模拟这种好奇心或探索的需求。我们将需要一个用于激活按钮的`reward`函数，一个用于进入房间的函数，一个用于推倒积木的函数，当然还有一个用于获得金盒子的函数。然后我们需要确定每个目标的价值，或许可以使用某种形式的**逆向强化学习**（**IRL**）。然而，使用好奇心学习，我们可以只为获取盒子的最终目标创建奖励函数（+1），并可能设置一个小的负步长目标（.0001），然后利用内在的好奇心奖励让代理学习其余步骤。这个技巧相当巧妙，我们将在下一部分看到它是如何工作的。
- en: The Curiosity Intrinsic module in action
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 好奇心内在模块正在运行
- en: 'With our appreciation of the difficulty of the Pyramids task, we can move on
    to training the agent with curiosity in the following exercise:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们理解金字塔任务的难度后，我们可以继续在接下来的练习中训练具有好奇心的代理：
- en: Open the Pyramids scene in the editor.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编辑器中打开金字塔场景。
- en: Select the AreaRB | Agent object in the Hierarchy window.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层次视图中选择AreaRB | 代理对象。
- en: Switch the Pyramid Agent | Brain for the PyramidsLearning brain.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将金字塔代理 | 大脑切换为PyramidsLearning大脑。
- en: Select the Academy object in the Hierarchy window.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在层次视图中选择Academy对象。
- en: 'Enable the Control option on the Academy | Pyramid Academy | Brains | Control
    property, as shown in the following screenshot:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用Academy | Pyramid Academy | 大脑 | 控制属性中的控制选项，如下截图所示：
- en: '![](img/31a355e2-ae0d-4f6f-8635-5f620ace0450.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31a355e2-ae0d-4f6f-8635-5f620ace0450.png)'
- en: Setting the Academy to Control
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 设置Academy为控制模式
- en: Open a Python or Anaconda console and prepare it for training.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个Python或Anaconda控制台，并为训练做准备。
- en: Open the `trainer_config.yaml` file located in the `ML-Agents/ml-agents/config`
    folder.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开位于`ML-Agents/ml-agents/config`文件夹中的`trainer_config.yaml`文件。
- en: 'Scroll down to the `PyramidsLearning` configuration section, as follows:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向下滚动到`PyramidsLearning`配置部分，如下所示：
- en: '[PRE10]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There are three new configuration parameters highlighted in bold:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里有三个新的配置参数，已用粗体标出：
- en: '`use_curiosity`: Set this to `true` to use the module, but it is generally
    `false` by default.'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_curiosity`：将其设置为`true`以使用该模块，但默认情况下通常为`false`。'
- en: '`curiosity_strength`: This is how strongly the agent values the intrinsic reward
    of curiosity over the extrinsic ones.'
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`curiosity_strength`：这是代理对好奇心内在奖励与外在奖励的重视程度。'
- en: '`curiosity_enc_size`: This is the size of the encoded layer we compress the
    network to. If you think back to autoencoders, you can see the size of 256 is
    quite large, but also consider the size of the state space or observation space
    you may be encoding.'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`curiosity_enc_size`：这是我们将网络压缩到的编码层的大小。如果你回想一下自编码器，你会发现256的大小相当大，但也要考虑你可能要编码的状态空间或观察空间的大小。'
- en: Leave the parameters at the values they are set.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 保持参数为已设置的值。
- en: 'Launch the training session with the following command:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令启动训练会话：
- en: '[PRE11]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: While this training session may take a while, it can be entertaining to watch
    how the agent explores. Even with the current settings, using only one training
    area, you may be able to see the agent solve the puzzle on a few iterations.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这次训练会话可能需要一些时间，但观察代理如何探索也许会很有趣。即使在当前的设置下，使用仅一个训练区域，你也许能够看到代理在几次迭代后解决这个难题。
- en: Since ICM is a module, it can quickly be activated for any other example we
    want to see the effects on, which is what we will do in the next section.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于ICM是一个模块，它可以快速激活到任何我们想要查看效果的其他示例中，这也是我们在下一部分将要做的。
- en: Trying ICM on Hallway/VisualHallway
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Hallway/VisualHallway上尝试ICM
- en: 'Not unlike the agents we train, we learn quite well from trial and error. This
    is the reason we practice, practice, and practice more of those very difficult
    tasks such as dancing, singing, or playing an instrument. RL is no different and
    requires the practitioner to learn the ins and outs training through the rigors
    of trial, error, and further exploration. Therefore, in this next exercise, we
    are going to combine Backplay (Curriculum Learning) and Curiosity Learning together
    into our old friend, the Hallway, and see what effect it has, as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们训练的代理类似，我们通过试错学习得很好。这也是我们为何要不断练习那些非常困难的任务，比如跳舞、唱歌或演奏乐器。强化学习（RL）也不例外，要求从业者通过试验、错误和进一步探索的严峻考验中学习其中的技巧。因此，在接下来的练习中，我们将把Backplay（课程学习）和好奇心学习结合在一起，应用到我们熟悉的Hallway场景中，看看它会带来什么效果，如下所示：
- en: Open the Hallway or VisualHallway scene (your preference) as we last left it,
    with Curriculum Learning enabled and set to simulate Backplay.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Hallway或VisualHallway场景（你可以选择其一），如我们上次所留，启用了课程学习并设置为模拟Backplay。
- en: Open the `trainer_config.yaml` configuration file location in the `ML-Agents/ml-agents/config`
    folder.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`trainer_config.yaml`配置文件，位置在`ML-Agents/ml-agents/config`文件夹中。
- en: 'Scroll down to the `HallwayLearning `or `VisualHallwayLearning` brain configuration
    parameters and add the following additional configuration lines:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向下滚动至`HallwayLearning`或`VisualHallwayLearning`脑网络配置参数，并添加以下附加配置行：
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This will enable the curiosity module for this example. We use the same settings
    for curiosity as we used for the last Pyrmarids example.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将启用本示例的好奇心模块。我们使用与上一个金字塔示例相同的好奇心设置。
- en: Make sure this sample is prepared for curriculum Backplay as we configured it
    in that section. If you need to, go back and review that section and add the capability
    to this example before continuing.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保这个示例已为我们在该部分中配置的课程Backplay做好准备。如果需要，回去复习那部分内容，并在继续之前将其能力添加到此示例中。
- en: This may require you to create a new curricula file that uses the same parameters
    as we did previously. Remember that the curricula file needs to have the same
    name as the brain it is being used against.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要你创建一个新的课程文件，使用与我们之前相同的参数。记住，课程文件需要与其所使用的脑网络名称相同。
- en: 'Open a Python/Anaconda window prepared for training and start training with
    the following command:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个准备好进行训练的Python/Anaconda窗口，并使用以下命令开始训练：
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let the training run until completion, as the results can be interesting and
    show some of the powerful possibilities of layering learning enhancements for
    extrinsic and intrinsic rewards.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让训练运行直到完成，因为结果可能会很有趣，并展示叠加学习增强对外部和内部奖励的一些强大可能性。
- en: This exercise showed how to run an agent with both Curriculum Learning simulating
    Backplay, and Curiosity Learning adding an aspect of agent motivation to the learning.
    As you may well imagine, intrinsic reward learning and the whole field of Motivated
    Reinforcement Learning may lead to some interesting advances and enhancements
    to our DRL.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习展示了如何通过课程学习模拟Backplay以及好奇心学习为学习过程添加代理动机来运行一个代理。正如你可以想象的那样，内在奖励学习和整个动机强化学习领域可能会对我们的深度强化学习（DRL）带来一些有趣的进展和改进。
- en: In the next section, we will review a number of helpful exercises that should
    help you learn more about these concepts.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾一些有助于你学习这些概念的练习。
- en: Exercises
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'While your motivation may vary as to why you are reading this book, hopefully
    by now you can appreciate the value of just doing things on your own. As always,
    we present these exercises for your enjoyment and learning, and hope you have
    fun completing them:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你阅读本书的动机可能各不相同，但希望到目前为止你已经能够意识到亲自实践的价值。正如以往，我们呈现这些练习供你享受与学习，并希望你在完成它们时玩得开心：
- en: Select another sample scene that uses discrete actions and write the reward
    functions that go with it. Yes, that means you will need to open up and look at
    the code.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择另一个使用离散动作的示例场景，并编写相应的奖励函数。是的，这意味着你需要打开代码并查看它。
- en: Select a continuous action scene and try writing the reward functions for it.
    While this one may be difficult, it is essential if you want to build your own
    control training agent.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个连续动作场景，并尝试为其编写奖励函数。虽然这可能有点困难，但如果你想要构建自己的控制训练智能体，这是必不可少的。
- en: Add Curriculum Learning to one of the other discrete action samples we have
    explored. Decide on how you can break the training into levels of difficulty and
    create parameters for controlling the evolution of the training.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们探索的另一个离散动作示例中添加课程学习。决定如何将训练分解成不同的难度等级，并创建控制训练进程的参数。
- en: Add Curriculum Learning to a continuous action sample. This is more difficult,
    and you likely want to perform exercise number two first.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个连续动作示例中添加课程学习。这更具挑战性，你可能希望先完成第二个练习。
- en: Implement actual Backplay on the Hallway environment by placing the agent starting
    at the goal and then, as the agent trains, move it back to the desired start with
    Curriculum Learning.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Hallway环境中实际实现Backplay，通过将智能体从目标位置开始，并在智能体训练过程中，使用课程学习将其移动回期望的起始位置。
- en: Implement Backplay on another discrete action example you have run and see the
    effect it has on training.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个离散动作示例中实现Backplay，并查看它对训练的影响。
- en: Implement Curiosity Learning on the VisualPyramids example and notice the difference
    in training.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在VisualPyramids示例中实施好奇心学习，并注意训练中的差异。
- en: Implement Curiosity Learning on a continuous action example and notice the effect
    it has on training. Is it what you expected?
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个连续动作示例中实施好奇心学习，并观察它对训练的效果。这是你预期的吗？
- en: Disable Curiosity Learning on the Pyramids example and see what effect this
    has on agent training.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 禁用Pyramids示例中的好奇心学习，观察这对智能体训练的影响。
- en: Think of a way in which you could add Backplay to the VisualPyramids example.
    You'll get bonus points if you actually build it.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 想一想，如何将Backplay添加到VisualPyramids示例中。如果你真的实现了它，你将获得额外的积分。
- en: As you can see, the exercises are getting more demanding as we progress through
    the book. Remember, even completing one or two of these exercises will make a
    difference in your take-away knowledge.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，随着我们逐步深入本书，练习的难度也在增加。记住，即使只完成一两个练习，也会对你所掌握的知识产生重要影响。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at a fundamental component of RL, and that is rewards.
    We learned that, when building training environments, it was best that we defined
    a set of `reward` functions our agent will live by. By understanding these equations,
    we get a better sense of how frequent or sparse rewards can negatively affect
    training. We then looked at a few methods, the first of which is called Curriculum
    Learning, that could be used to ease or step the agent's extrinsic rewards. After
    that, we explored another technique, called Backplay, that used a reverse play
    technique and Curriculum Training to enhance an agent's training. Finally, we
    looked at internal or intrinsic rewards, and the concept of Motivated Reinforcement
    Learning. We then learned that the first intrinsic reward system developed into
    ML-Agents was to give an agent a motivation for curiosity. We looked at how to
    use Curiosity Learning on a few examples, and even incorporated it with Backplay
    via Curriculum Learning.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章，我们探讨了强化学习（RL）中的一个基础组成部分，那就是奖励。我们了解到，在构建训练环境时，最好定义一套`奖励`函数，让智能体遵循这些规则。通过理解这些方程式，我们能够更好地理解频繁或稀疏的奖励如何对训练产生负面影响。接着，我们介绍了几种方法，其中第一种叫做课程学习（Curriculum
    Learning），可以用来缓解或逐步推进智能体的外部奖励。之后，我们探讨了另一种技术，叫做反向播放（Backplay），它使用反向播放技术和课程训练来增强智能体的训练。最后，我们介绍了内在奖励或内在强化学习的概念。我们还了解到，最初开发的内在奖励系统是为了赋予智能体好奇心的动机。我们查看了如何在一些例子中应用好奇心学习，并通过课程学习将其与反向播放结合使用。
- en: In the next chapter, we look to more reward helper solutions in the form of
    Imitation and Transfer Learning, where we will learn how a human's gameplay experience
    can be mapped to a form of learning called Imitation Learning or Behavioral Cloning.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探索更多奖励辅助解决方案，形式为模仿学习和迁移学习，在这一章中，我们将学习如何将人类的游戏体验映射到一种叫做模仿学习或行为克隆的学习形式。
