- en: Developing Applications in a Distributed Environment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在分布式环境中开发应用程序
- en: As the demand increases regarding the quantity of data and resource requirements
    for parallel computations, legacy approaches may not perform well. So far, we
    have seen how big data development has become famous and is the most followed
    approach by enterprises due to the same reasons. DL4J supports neural network
    training, evaluation, and inference on distributed clusters.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据量和并行计算资源需求的增加，传统方法可能表现不佳。到目前为止，我们已经看到大数据开发因这些原因而变得流行，并成为企业最常采用的方法。DL4J 支持在分布式集群上进行神经网络训练、评估和推理。
- en: 'Modern approaches to heavy training, or output generation tasks, distribute
    training effort across multiple machines. This also brings additional challenges.
    We need to ensure that we have the following constraints checked before we use
    Spark to perform distributed training/evaluation/inference:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现代方法将繁重的训练或输出生成任务分配到多台机器上进行训练。这也带来了额外的挑战。在使用 Spark 执行分布式训练/评估/推理之前，我们需要确保满足以下约束条件：
- en: Our data should be significantly large enough to justify the need for distributed
    clusters. Small network/data on Spark doesn't really gain any performance improvements
    and local machine execution may have much better results in such scenarios.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的数据应该足够大，以至于能证明使用分布式集群的必要性。在 Spark 上的小型网络/数据并不会真正带来性能上的提升，在这种情况下，本地机器执行可能会有更好的效果。
- en: We have more than a single machine to perform training/evaluation or inference.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有多个机器来执行训练/评估或推理。
- en: Let's say we have a single machine with multiple GPU processors. We could simply
    use a parallel wrapper rather than Spark in this case. A parallel wrapper enables
    parallel training on a single machine with multiple cores. Parallel wrappers will
    be discussed in [Chapter 12](0db31248-e40b-4479-9939-0baccb0e11d1.xhtml), *Benchmarking
    and Neural Network Optimization*, where you will find out how to configure them.
    Also, if the neural network takes more than 100 ms for one single iteration, it
    may be worth considering distributed training.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一台配备多个 GPU 处理器的机器。在这种情况下，我们可以简单地使用并行包装器，而不是使用 Spark。并行包装器允许在单台机器上使用多个核心进行并行训练。并行包装器将在[第
    12 章](0db31248-e40b-4479-9939-0baccb0e11d1.xhtml)《基准测试和神经网络优化》中讨论，你将在那里了解如何配置它们。此外，如果神经网络每次迭代超过
    100 毫秒，可能值得考虑使用分布式训练。
- en: 'In this chapter, we will discuss how to configure DL4J for distributed training,
    evaluation, and inference. We will develop a distributed neural network for the `TinyImageNet`
    classifier. In this chapter, we will cover the following recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何配置 DL4J 进行分布式训练、评估和推理。我们将为 `TinyImageNet` 分类器开发一个分布式神经网络。在本章中，我们将覆盖以下内容：
- en: Setting up DL4J and the required dependencies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 DL4J 和所需的依赖项
- en: Creating an uber-JAR for training
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为训练创建一个 uber-JAR
- en: CPU/GPU-specific configuration for training
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU/GPU 特定的训练配置
- en: Memory settings and garbage collection for Spark
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 的内存设置和垃圾回收
- en: Configuring encoding thresholds
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置编码阈值
- en: Performing a distributed test set evaluation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行分布式测试集评估
- en: Saving and loading trained neural network models
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存和加载训练好的神经网络模型
- en: Performing distributed inference
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行分布式推理
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The source code for this chapter can be found at [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的源代码可以在 [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/tree/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app)
    找到。
- en: After cloning our GitHub repository, navigate to the `Java-Deep-Learning-Cookbook/10_Developing_applications_in_distributed_environment/sourceCode` directory.
    Then, import the `cookbookapp` project as a Maven project by importing the `pom.xml` file.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 克隆我们的 GitHub 仓库后，进入 `Java-Deep-Learning-Cookbook/10_Developing_applications_in_distributed_environment/sourceCode`
    目录。然后，通过导入 `pom.xml` 文件将 `cookbookapp` 项目作为 Maven 项目导入。
- en: 'You need to run either of the following preprocessor scripts (`PreProcessLocal.java`
    or `PreProcessSpark.java`) before running the actual source code:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行实际源代码之前，您需要运行以下预处理脚本之一（`PreProcessLocal.java`或`PreProcessSpark.java`）：
- en: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app/PreProcessLocal.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app/PreProcessLocal.java)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app/PreProcessLocal.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app/PreProcessLocal.java)'
- en: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app/PreprocessSpark.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app/PreprocessSpark.java)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app/PreprocessSpark.java](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing_applications_in_distributed_environment/sourceCode/cookbookapp/src/main/java/com/javacookbook/app/PreprocessSpark.java)'
- en: These scripts can be found in the `cookbookapp` project.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些脚本可以在`cookbookapp`项目中找到。
- en: You will also need the `TinyImageNet` dataset, which can be found at [http://cs231n.stanford.edu/tiny-imagenet-200.zip](http://cs231n.stanford.edu/tiny-imagenet-200.zip).
    The home page can be found at [https://tiny-imagenet.herokuapp.com/](https://tiny-imagenet.herokuapp.com/).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要`TinyImageNet`数据集，可以在[http://cs231n.stanford.edu/tiny-imagenet-200.zip](http://cs231n.stanford.edu/tiny-imagenet-200.zip)找到。主页地址为[https://tiny-imagenet.herokuapp.com/](https://tiny-imagenet.herokuapp.com/)。
- en: It is desirable if you have some prior knowledge of working with Apache Spark
    and Hadoop so that you get the most out of this chapter. Also, this chapter assumes
    that Java is already installed on your machine and has been added to your environment
    variables. We recommend Java version 1.8.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一些关于使用Apache Spark和Hadoop的先验知识，那将是非常有益的，这样您能从本章中获得最大的收益。此外，本章假设您的机器已经安装了Java并将其添加到环境变量中。我们推荐使用Java
    1.8版本。
- en: Note that the source code requires good hardware in terms of memory/processing
    power. We recommend that you have at least 16 GB of RAM on your host machine in
    case you're running the source on a laptop/desktop.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，源代码对硬件（特别是内存/处理能力）有较高要求。我们建议您的主机机器至少拥有16 GB的RAM，特别是在您将源代码运行在笔记本/台式机上时。
- en: Setting up DL4J and the required dependencies
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置DL4J及其所需依赖项
- en: We are discussing setting up DL4J again because we are now dealing with a distributed
    environment. For demonstration purposes, we will use Spark's local mode. Due to
    this, we can focus on DL4J rather than setting up clusters, worker nodes, and
    so on. In this recipe, we will set up a single node Spark cluster (Spark local),
    as well as configure DL4J-specific dependencies.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次讨论如何设置DL4J，因为我们现在涉及的是一个分布式环境。为了演示目的，我们将使用Spark的本地模式。由于此原因，我们可以专注于DL4J，而不是设置集群、工作节点等。在本示例中，我们将设置一个单节点Spark集群（Spark本地模式），并配置DL4J特定的依赖项。
- en: Getting ready
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In order to demonstrate the use of a distributed neural network, you will need
    the following:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示分布式神经网络的使用，您需要以下内容：
- en: A distributed filesystem (Hadoop) for file management
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式文件系统（Hadoop）用于文件管理
- en: Distributed computing (Spark) in order to process big data
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式计算（Spark）以处理大数据
- en: How to do it...
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Add the following Maven dependency for Apache Spark:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下Maven依赖项以支持Apache Spark：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Add the following Maven dependency for `DataVec` for Spark:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下Maven依赖项以支持Spark中的`DataVec`：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Add the following Maven dependency for parameter averaging:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下Maven依赖项以支持参数平均：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Add the following Maven dependency for gradient sharing:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下Maven依赖项以支持梯度共享：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Add the following Maven dependency for the ND4J backend:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下Maven依赖项以支持ND4J后端：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Add the following Maven dependency for CUDA:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下Maven依赖项以支持CUDA：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Add the following Maven dependency for JCommander:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下Maven依赖项以支持JCommander：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Download Hadoop from the official website at [https://hadoop.apache.org/releases.html](https://hadoop.apache.org/releases.html) and
    add the required environment variables.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从官方网站[https://hadoop.apache.org/releases.html](https://hadoop.apache.org/releases.html)下载Hadoop并添加所需的环境变量。
- en: 'Extract the downloaded Hadoop package and create the following environment
    variables:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 解压下载的Hadoop包并创建以下环境变量：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Add the following entry to the `PATH` environment variable:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下条目添加到`PATH`环境变量中：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Create name/data node directories for Hadoop. Navigate to the Hadoop home directory
    (which is set in the `HADOOP_HOME` environment variable) and create a directory
    named `data`. Then, create two subdirectories named `datanode` and `namenode`
    underneath it. Make sure that access for read/write/delete has been provided for
    these directories.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为Hadoop创建name/data节点目录。导航到Hadoop主目录（在`HADOOP_HOME`环境变量中设置），并创建一个名为`data`的目录。然后，在其下创建名为`datanode`和`namenode`的两个子目录。确保已为这些目录提供读/写/删除权限。
- en: 'Navigate to `hadoop-x.x/etc/hadoop` and open `hdfs-site.xml`. Then, add the
    following configuration:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`hadoop-x.x/etc/hadoop`并打开`hdfs-site.xml`。然后，添加以下配置：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Navigate to `hadoop-x.x/etc/hadoop` and open `mapred-site.xml`. Then, add the
    following configuration:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`hadoop-x.x/etc/hadoop`并打开`mapred-site.xml`。然后，添加以下配置：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Navigate to `hadoop-x.x/etc/hadoop` and open `yarn-site.xml`. Then, add the
    following configuration:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`hadoop-x.x/etc/hadoop`并打开`yarn-site.xml`。然后，添加以下配置：
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Navigate to `hadoop-x.x/etc/hadoop` and open `core-site.xml`. Then, add the
    following configuration:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`hadoop-x.x/etc/hadoop`并打开`core-site.xml`。然后，添加以下配置：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Navigate to `hadoop-x.x/etc/hadoop` and open `hadoop-env.cmd`. Then, replace
    `set JAVA_HOME=%JAVA_HOME%` with `set JAVA_HOME={JavaHomeAbsolutePath}`.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`hadoop-x.x/etc/hadoop`并打开`hadoop-env.cmd`。然后，将`set JAVA_HOME=%JAVA_HOME%`替换为`set
    JAVA_HOME={JavaHomeAbsolutePath}`。
- en: Add the `winutils` Hadoop fix (only applicable for Windows). You can download
    this from [http://tiny.cc/hadoop-config-windows](http://tiny.cc/hadoop-config-windows). Alternatively,
    you can navigate to the respective GitHub repository, [https://github.com/steveloughran/winutils](https://github.com/steveloughran/winutils),
    and get the fix that matches your installed Hadoop version. Replace the `bin`
    folder at `${HADOOP_HOME}` with the `bin` folder in the fix.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 添加`winutils` Hadoop修复（仅适用于Windows）。你可以从[http://tiny.cc/hadoop-config-windows](http://tiny.cc/hadoop-config-windows)下载此修复程序。或者，你也可以导航到相关的GitHub库[https://github.com/steveloughran/winutils](https://github.com/steveloughran/winutils)，获取与你安装的Hadoop版本匹配的修复程序。将`${HADOOP_HOME}`中的`bin`文件夹替换为修复程序中的`bin`文件夹。
- en: 'Run the following Hadoop command to format `namenode`:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下Hadoop命令来格式化`namenode`：
- en: '[PRE13]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You should see the following output:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![](img/41f46736-3d0a-40a0-82e8-27e2fdf9a69b.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41f46736-3d0a-40a0-82e8-27e2fdf9a69b.png)'
- en: 'Navigate to `${HADOOP_HOME}\sbin` and start the Hadoop services:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到`${HADOOP_HOME}\sbin`并启动Hadoop服务：
- en: For Windows, run `start-all.cmd`.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Windows，运行`start-all.cmd`。
- en: For Linux or any other OS, run `start-all.sh` from Terminal.
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于Linux或任何其他操作系统，从终端运行`start-all.sh`。
- en: 'You should see the following output:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![](img/9727f210-5dd4-40c4-b67b-f32a47ebc8f4.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9727f210-5dd4-40c4-b67b-f32a47ebc8f4.png)'
- en: 'Hit `http://localhost:50070/` in your browser and verify whether Hadoop is
    up and running:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中访问`http://localhost:50070/`并验证Hadoop是否正常运行：
- en: '![](img/3632ac4d-fbcf-4421-bf28-9534434e821f.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3632ac4d-fbcf-4421-bf28-9534434e821f.png)'
- en: 'Install Spark from [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html) and
    add the required environment variables. Extract the package and add the following
    environment variables:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)下载Spark并添加所需的环境变量。解压包并添加以下环境变量：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Configure Spark''s properties. Navigate to the directory location at `SPARK_CONF_DIR`
    and open the `spark-env.sh` file. Then, add the following configuration:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置Spark的属性。导航到`SPARK_CONF_DIR`所在的目录，并打开`spark-env.sh`文件。然后，添加以下配置：
- en: '[PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Run the Spark master by running the following command:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令启动Spark主节点：
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You should see the following output:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![](img/4f00b4cd-c9ea-432c-ad46-ae77b4414f33.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f00b4cd-c9ea-432c-ad46-ae77b4414f33.png)'
- en: 'Hit `http://localhost:8080/` in your browser and verify whether Hadoop is up
    and running:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中访问`http://localhost:8080/`并验证Hadoop是否正常运行：
- en: '![](img/65bfa3c8-838f-4d6e-bdc9-1d1f95d3f690.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/65bfa3c8-838f-4d6e-bdc9-1d1f95d3f690.png)'
- en: How it works...
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 2, dependencies were added for `DataVec`. We need to use data transformation
    functions in Spark just like in regular training. Transformation is a data requirement
    for neural networks and is not Spark-specific.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，为`DataVec`添加了依赖项。我们需要在Spark中使用数据转换函数，就像在常规训练中一样。转换是神经网络的一个数据需求，并非Spark特有。
- en: For example, we talked about `LocalTransformExecutor` in [Chapter 2](6ac5dff5-cc98-4d52-bc59-1da01b2aeded.xhtml),
    *Data Extraction, Transformation, and Loading*. `LocalTransformExecutor` is used
    for `DataVec` transformation in non-distributed environments. `SparkTransformExecutor`
    will be used for the `DataVec` transformation process in Spark.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们在[第2章](6ac5dff5-cc98-4d52-bc59-1da01b2aeded.xhtml)中讨论了`LocalTransformExecutor`，*数据提取、转换和加载*。`LocalTransformExecutor`用于非分布式环境中的`DataVec`转换。`SparkTransformExecutor`将在Spark中用于`DataVec`转换过程。
- en: In step 4, we added dependencies for gradient sharing. Training times are faster
    for gradient sharing and it is designed to be scalable and fault-tolerant. Therefore,
    gradient sharing is preferred over parameter averaging. In gradient sharing, instead
    of relaying all the parameter updates/gradients across the network, it only updates
    those that are above the specified threshold. Let's say we have an update vector
    at the beginning that we want to communicate across the network. Due to this,
    we will be creating a sparse binary vector for the large values (as specified
    by a threshold) in the update vector. We will use this sparse binary vector for
    further communication. The main idea is to decrease the communication effort.
    Note that the rest of the updates will not be discarded and are added in a residual
    vector for processing later. Residual vectors will be kept for future updates
    (delayed communication) and not lost. Gradient sharing in DL4J is an asynchronous
    SGD implementation. You can read more about this in detail at [http://nikkostrom.com/publications/interspeech2015/strom_interspeech2015.pdf](http://nikkostrom.com/publications/interspeech2015/strom_interspeech2015.pdf).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤4中，我们添加了梯度共享的依赖项。梯度共享使得训练时间更快，它被设计为可扩展和容错的。因此，梯度共享优于参数平均。在梯度共享中，不是将所有参数更新/梯度通过网络传递，而是仅更新那些超过指定阈值的部分。假设我们在开始时有一个更新向量，我们希望将其通过网络传递。为了实现这一点，我们将为更新向量中的大值（由阈值指定）创建一个稀疏二进制向量。我们将使用这个稀疏二进制向量进行进一步的通信。主要思路是减少通信工作量。请注意，其余的更新不会被丢弃，而是会被添加到一个残差向量中，稍后处理。残差向量将被保留用于未来的更新（延迟通信），而不会丢失。DL4J中的梯度共享是异步SGD实现。您可以在这里详细阅读：[http://nikkostrom.com/publications/interspeech2015/strom_interspeech2015.pdf](http://nikkostrom.com/publications/interspeech2015/strom_interspeech2015.pdf)。
- en: In step 5, we added CUDA dependencies for the Spark distributed training application.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤5中，我们为Spark分布式训练应用程序添加了CUDA依赖项。
- en: 'Here are the uber-JAR requirements for this:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是关于此项的uber-JAR要求：
- en: If the OS that's building the uber-JAR is the same as that of the cluster OS
    (for example, run it on Linux and then execute it on a Spark Linux cluster), include
    the `nd4j-cuda-x.x` dependency in the `pom.xml` file.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果构建uber-JAR的操作系统与集群操作系统相同（例如，在Linux上运行并在Spark Linux集群上执行），请在`pom.xml`文件中包含`nd4j-cuda-x.x`依赖项。
- en: If the OS that's building the uber-JAR is not the same as that of the cluster
    OS (for example, run it on Windows and then execute it on a Spark Linux cluster),
    include the `nd4j-cuda-x.x-platform` dependency in the `pom.xml` file.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果构建uber-JAR的操作系统与集群操作系统不同（例如，在Windows上运行并在Spark Linux集群上执行），请在`pom.xml`文件中包含`nd4j-cuda-x.x-platform`依赖项。
- en: Just replace `x.x` with the CUDA version you have installed (for example, `nd4j-cuda-9.2` for CUDA 9.2*).*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 只需将`x.x`替换为您安装的CUDA版本（例如，`nd4j-cuda-9.2`代表CUDA 9.2）。
- en: In cases where the clusters don't have CUDA/cuDNN set up, we can include `redist javacpp-` presets
    for the cluster OS. You can refer to the respective dependencies here: [https://deeplearning4j.org/docs/latest/deeplearning4j-config-cuDNN](https://deeplearning4j.org/docs/latest/deeplearning4j-config-cudnn).
    That way, we don't have to install CUDA or cuDNN in each and every cluster machine.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果集群没有设置CUDA/cuDNN，可以为集群操作系统包含`redist javacpp-`预设。您可以参考这里的相应依赖项：[https://deeplearning4j.org/docs/latest/deeplearning4j-config-cuDNN](https://deeplearning4j.org/docs/latest/deeplearning4j-config-cudnn)。这样，我们就不需要在每台集群机器上安装CUDA或cuDNN。
- en: In step 6, we added a Maven dependency for JCommander. JCommander is used to
    parse command-line arguments that are supplied with `spark-submit`. We need this
    because we will be passing directory locations (HDFS/local) of the train/test
    data as command-line arguments in `spark-submit`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 6 步中，我们为 JCommander 添加了 Maven 依赖。JCommander 用于解析通过 `spark-submit` 提供的命令行参数。我们需要这个，因为我们将在
    `spark-submit` 中传递训练/测试数据的目录位置（HDFS/本地）作为命令行参数。
- en: From steps 7 to 16, we downloaded and configured Hadoop. Remember to replace `{PathDownloaded}`
    with the actual location of the extracted Hadoop package. Also, replace `x.x`
    with the Hadoop version you've downloaded. We need to specify the disk location
    where we will store the metadata and the data represented in HDFS. Due to this,
    we created name/data directories in step 8/step 9. To make changes, in step 10,
    we configured `mapred-site.xml`. If you can't locate the file in the directory,
    just create an XML file by copying all the content from the `mapred-site.xml.template` file,
    and then make the changes that were mentioned in step 10.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从第 7 步到第 16 步，我们下载并配置了 Hadoop。记得将`{PathDownloaded}`替换为实际的 Hadoop 包提取位置。同时，将
    `x.x` 替换为你下载的 Hadoop 版本。我们需要指定将存储元数据和数据的磁盘位置，这就是为什么我们在第 8 步/第 9 步创建了 name/data
    目录。为了进行修改，在第 10 步中，我们配置了`mapred-site.xml`。如果你无法在目录中找到该文件，只需通过复制`mapred-site.xml.template`文件中的所有内容创建一个新的
    XML 文件，然后进行第 10 步中提到的修改。
- en: In step 13, we replaced the `JAVA_HOME` path variable with the actual Java home
    directory location. This was done to avoid certain `ClassNotFound` exceptions
    from being encountered at runtime.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 13 步中，我们将 `JAVA_HOME` 路径变量替换为实际的 Java 主目录位置。这样做是为了避免在运行时遇到某些 `ClassNotFound`
    异常。
- en: In step 18, make sure that you are downloading the Spark version that matches
    your Hadoop version. For example, if you have Hadoop 2.7.3, then get the Spark
    version that looks like `spark-x.x-bin-hadoop2.7`. When we made changes in step
    19, if the `spark-env.sh` file isn't present, then just create a new file named
    `spark-env.sh` by copying the content from the `spark-env.sh.template` file. Then,
    make the changes that were mentioned in step 19\. After completing all the steps
    in this recipe, you should be able to perform distributed neural network training
    via the `spark-submit` command.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 18 步中，确保你下载的是与 Hadoop 版本匹配的 Spark 版本。例如，如果你使用 Hadoop 2.7.3，那么下载的 Spark 版本应该是
    `spark-x.x-bin-hadoop2.7`。当我们在第 19 步做出修改时，如果 `spark-env.sh` 文件不存在，则只需通过复制 `spark-env.sh.template`
    文件中的内容创建一个新文件 `spark-env.sh`。然后，进行第 19 步中提到的修改。完成此教程中的所有步骤后，你应该能够通过 `spark-submit`
    命令执行分布式神经网络训练。
- en: Creating an uber-JAR for training
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为训练创建 uber-JAR
- en: The training job that's executed by `spark-submit` will need to resolve all
    the required dependencies at runtime. In order to manage this task, we will create
    an uber-JAR that has the application runtime and its required dependencies. We
    will use the Maven configurations in `pom.xml` to create an uber-JAR so that we
    can perform distributed training. Effectively, we will create an uber-JAR and
    submit it to `spark-submit` to perform the training job in Spark.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `spark-submit` 执行的训练作业需要在运行时解析所有必需的依赖项。为了管理这个任务，我们将创建一个包含应用程序运行时和其所需依赖项的
    uber-JAR。我们将使用 `pom.xml` 中的 Maven 配置来创建 uber-JAR，这样我们就可以进行分布式训练。实际上，我们将创建一个 uber-JAR，并将其提交到
    `spark-submit` 来执行 Spark 中的训练作业。
- en: In this recipe, we will create an uber-JAR using the Maven shade plugin for
    Spark training.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将使用 Maven shade 插件为 Spark 训练创建 uber-JAR。
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Create an uber-JAR (shaded JAR) by adding the Maven shade plugin to the `pom.xml` file,
    as shown here:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将 Maven shade 插件添加到`pom.xml`文件中来创建 uber-JAR（阴影 JAR），如下面所示：
- en: '![](img/4d8158cc-6169-4fd5-960e-8c6d1fa051d9.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d8158cc-6169-4fd5-960e-8c6d1fa051d9.png)'
- en: 'Refer to the `pom.xml` file in this book''s GitHub repository for more information: [https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing%20applications%20in%20distributed%20environment/sourceCode/cookbookapp/pom.xml](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing%20applications%20in%20distributed%20environment/sourceCode/cookbookapp/pom.xml).
    Add the following filter to the Maven configuration:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请参考本书 GitHub 仓库中的`pom.xml`文件：[https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing%20applications%20in%20distributed%20environment/sourceCode/cookbookapp/pom.xml](https://github.com/PacktPublishing/Java-Deep-Learning-Cookbook/blob/master/10_Developing%20applications%20in%20distributed%20environment/sourceCode/cookbookapp/pom.xml)。将以下过滤器添加到
    Maven 配置中：
- en: '[PRE17]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Hit the Maven command to build an uber-JAR for the project:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 Maven 命令以构建项目的 Uber-JAR：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'In step 1, you need to specify the main class that should run while executing
    the JAR file. In the preceding demonstration, `SparkExample` is our main class
    that invokes a training session. You may come across exceptions that look as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 1 中，您需要指定在执行 JAR 文件时应该运行的主类。在前面的示例中，`SparkExample` 是我们的主类，用于启动训练会话。您可能会遇到如下异常：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Some of the dependencies that were added to the Maven configuration may have
    a signed JAR, which may cause issues like these.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一些添加到 Maven 配置中的依赖项可能包含签名的 JAR，这可能会导致如下问题。
- en: In step 2, we added the filters to prevent the addition of signed `.jars` during
    the Maven build.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 2 中，我们添加了过滤器以防止在 Maven 构建过程中添加签名的 `.jars`。
- en: 'In step 3, we generated an executable `.jar` file with all the required dependencies.
    We can submit this `.jar` file to `spark-submit` to train our networks on Spark. The
    `.jar` file is created in the `target` directory of the project:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 3 中，我们生成了一个包含所有必需依赖项的可执行 `.jar` 文件。我们可以将此 `.jar` 文件提交给 `spark-submit`，在
    Spark 上训练我们的网络。该 `.jar` 文件位于项目的 `target` 目录中：
- en: '![](img/58b2d8d3-043e-43ac-b3ba-104fb70c7fdc.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58b2d8d3-043e-43ac-b3ba-104fb70c7fdc.png)'
- en: The Maven shade plugin is not the only way to build an uber-JAR file. However,
    the Maven shade plugin is recommended over other alternatives. Other alternatives
    may not be able to include the required files from source `.jars`. Some of those
    files act as dependencies for the Java service loader's functionality. ND4J makes
    use of Java's service loader functionality. Therefore, other alternative plugins
    can cause issues.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Maven Shade 插件不是构建 Uber-JAR 文件的唯一方法。然而，推荐使用 Maven Shade 插件而非其他替代方案。其他替代方案可能无法包含来自源
    `.jars` 的所需文件。其中一些文件作为 Java 服务加载器功能的依赖项。ND4J 利用 Java 的服务加载器功能。因此，其他替代插件可能会导致问题。
- en: CPU/GPU-specific configuration for training
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练的 CPU/GPU 特定配置
- en: Hardware-specific changes are generic configurations that can't be ignored in
    a distributed environment. DL4J supports GPU-accelerated training in NVIDIA GPUs
    with CUDA/cuDNN enabled. We can also perform Spark distributed training using
    GPUs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件特定的更改是分布式环境中无法忽视的通用配置。DL4J 支持启用 CUDA/cuDNN 的 NVIDIA GPU 加速训练。我们还可以使用 GPU 执行
    Spark 分布式训练。
- en: In this recipe, we will configure CPU/GPU-specific changes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将配置 CPU/GPU 特定的更改。
- en: How to do it...
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Download, install, and set up the CUDA toolkit from [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads).
    OS-specific setup instructions are available at the NVIDIA CUDA official website.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 [https://developer.nvidia.com/cuda-downloads](https://developer.nvidia.com/cuda-downloads)
    下载、安装并设置 CUDA 工具包。操作系统特定的设置说明可以在 NVIDIA CUDA 官方网站找到。
- en: 'Configure the GPU for Spark distributed training by adding a Maven dependency
    for ND4J''s CUDA backend:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过为 ND4J 的 CUDA 后端添加 Maven 依赖项来配置 Spark 分布式训练的 GPU：
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Configure the CPU for Spark distributed training by adding an ND4J-native dependency:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加 ND4J 本地依赖项来配置 CPU 用于 Spark 分布式训练：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: How it works...
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We need to enable a proper ND4J backend so that we can utilize GPU resources,
    as we mentioned in step 1. Enable the `nd4j-cuda-x.x` dependency in your `pom.xml` file
    for GPU training, where `x.x` refers to the CUDA version that you have installed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要启用一个适当的 ND4J 后端，以便能够利用 GPU 资源，正如我们在步骤 1 中提到的那样。在 `pom.xml` 文件中启用 `nd4j-cuda-x.x`
    依赖项以进行 GPU 训练，其中 `x.x` 指您安装的 CUDA 版本。
- en: We may include both ND4J backends (CUDA/native dependencies) if the master node
    is running on the CPU and the worker nodes are running on the GPU, as we mentioned
    in the previous recipe. If both backends are present in the classpath, the CUDA
    backend will be tried out first. If it doesn't load for some reason, then the
    CPU backend (native) will be loaded. The priority can also be changed by changing
    the `BACKEND_PRIORITY_CPU` and `BACKEND_PRIORITY_GPU` environment variables in
    the master node. The backend will be picked depending on which one of these environment
    variables has the highest value.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果主节点在 CPU 上运行，而工作节点在 GPU 上运行，如前面食谱中所述，我们可以包含两个 ND4J 后端（CUDA / 本地依赖）。如果两个后端都在类路径中，CUDA
    后端将首先被尝试。如果因某种原因没有加载，那么将加载 CPU 后端（本地）。通过在主节点中更改 `BACKEND_PRIORITY_CPU` 和 `BACKEND_PRIORITY_GPU`
    环境变量，也可以更改优先级。后端将根据这些环境变量中的最大值来选择。
- en: In step 3, we added CPU-specific configuration that targets CPU-only hardware.
    We don't have to keep this configuration if both the master/worker nodes have
    GPU hardware in place.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 3 中，我们添加了针对仅有 CPU 硬件的配置。如果主节点/工作节点都配备了 GPU 硬件，那么我们不需要保留此配置。
- en: There's more...
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'We can further optimize the training throughput by configuring cuDNN into CUDA
    devices. We can run a training instance in Spark without CUDA/cuDNN installed
    on every node. To gain optimal performance with cuDNN support, we can add the
    DL4J CUDA dependency. For that, the following components must be added and made
    available:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将 cuDNN 配置到 CUDA 设备中来进一步优化训练吞吐量。我们可以在没有安装 CUDA/cuDNN 的情况下，在 Spark 上运行训练实例。为了获得最佳的性能支持，我们可以添加
    DL4J CUDA 依赖项。为此，必须添加并使以下组件可用：
- en: 'The DL4J CUDA Maven dependency:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J CUDA Maven 依赖项：
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The cuDNN library files at [https://developer.nvidia.com/cuDNN](https://developer.nvidia.com/cudnn).
    Note that you need to sign up to the NVIDIA website to download cuDNN libraries.
    Signup is free. Refer to the installation guide here: [https://docs.nvidia.com/deeplearning/sdk/cuDNN-install/index.html](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cuDNN 库文件在 [https://developer.nvidia.com/cuDNN](https://developer.nvidia.com/cudnn)。请注意，你需要注册
    NVIDIA 网站账户才能下载 cuDNN 库。注册是免费的。请参阅安装指南：[https://docs.nvidia.com/deeplearning/sdk/cuDNN-install/index.html](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html)。
- en: Memory settings and garbage collection for Spark
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 的内存设置和垃圾回收
- en: Memory management is very crucial for distributed training with large datasets
    in production. It directly influences the resource consumption and performance
    of the neural network. Memory management involves configuring off-heap and on-heap
    memory spaces. DL4J/ND4J-specific memory configuration will be discussed in detail
    in [Chapter 12](0db31248-e40b-4479-9939-0baccb0e11d1.xhtml), *Benchmarking and
    Neural Network Optimization*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 内存管理对于大数据集的分布式训练至关重要，尤其是在生产环境中。它直接影响神经网络的资源消耗和性能。内存管理涉及堆内存和堆外内存空间的配置。DL4J/ND4J
    特定的内存配置将在 [第 12 章](0db31248-e40b-4479-9939-0baccb0e11d1.xhtml) 中详细讨论，*基准测试与神经网络优化*。
- en: In this recipe, we will focus on memory configuration in the context of Spark.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将专注于 Spark 环境下的内存配置。
- en: How to do it...
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Add the `--executor-memory` command-line argument while submitting a job to
    `spark-submit` to set on-heap memory for the worker node. For example, we could
    use `--executor-memory 4g` to allocate 4 GB of memory.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在提交作业到 `spark-submit` 时，添加 `--executor-memory` 命令行参数来设置工作节点的堆内存。例如，我们可以使用 `--executor-memory
    4g` 来分配 4 GB 的内存。
- en: 'Add the `--conf` command-line argument to set the off-heap memory for the worker
    node:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加 `--conf` 命令行参数来设置工作节点的堆外内存：
- en: '[PRE23]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Add the `--conf` command-line argument to set the off-heap memory for the master
    node. For example, we could use `--conf "spark.driver.memoryOverhead=-Dorg.bytedeco.javacpp.maxbytes=8G"`
    to allocate 8 GB of memory.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加 `--conf` 命令行参数来设置主节点的堆外内存。例如，我们可以使用 `--conf "spark.driver.memoryOverhead=-Dorg.bytedeco.javacpp.maxbytes=8G"`
    来分配 8 GB 的内存。
- en: Add the `--driver-memory` command-line argument to specify the on-heap memory
    for the master node. For example, we could use `--driver-memory 4g` to allocate
    4 GB of memory.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加 `--driver-memory` 命令行参数来指定主节点的堆内存。例如，我们可以使用 `--driver-memory 4g` 来分配 4 GB
    的内存。
- en: 'Configure garbage collection for the worker nodes by calling `workerTogglePeriodicGC()`
    and `workerPeriodicGCFrequency()` while you set up the distributed neural network
    using `SharedTrainingMaster`:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `workerTogglePeriodicGC()` 和 `workerPeriodicGCFrequency()` 来为工作节点配置垃圾回收，同时使用
    `SharedTrainingMaster` 设置分布式神经网络：
- en: '[PRE24]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Enable Kryo optimization in DL4J by adding the following dependency to the `pom.xml` file:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将以下依赖项添加到 `pom.xml` 文件中来启用 DL4J 中的 Kryo 优化：
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Configure `KryoSerializer` with `SparkConf`:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `SparkConf` 配置 `KryoSerializer`：
- en: '[PRE26]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Add locality configuration to `spark-submit`, as shown here:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，添加本地性配置到 `spark-submit`：
- en: '[PRE27]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: How it works...
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, we discussed Spark-specific memory configurations. We mentioned that
    this can be configured for master/worker nodes. Also, these memory configurations
    can be dependent on the cluster resource manager.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 1 中，我们讨论了 Spark 特定的内存配置。我们提到过，这可以为主节点/工作节点进行配置。此外，这些内存配置可能依赖于集群资源管理器。
- en: 'Note that the `--executor-memory 4g` command-line argument is for YARN. Please
    refer to the respective cluster resource manager documentation to find out the
    respective command-line argument for the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`--executor-memory 4g`命令行参数是针对YARN的。请参考相应的集群资源管理器文档，查找以下参数的相应命令行参数：
- en: Spark Standalone: [https://spark.apache.org/docs/latest/spark-standalone.html](https://spark.apache.org/docs/latest/spark-standalone.html)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Standalone: [https://spark.apache.org/docs/latest/spark-standalone.html](https://spark.apache.org/docs/latest/spark-standalone.html)
- en: Mesos: [https://spark.apache.org/docs/latest/running-on-mesos.html](https://spark.apache.org/docs/latest/running-on-mesos.html)
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesos: [https://spark.apache.org/docs/latest/running-on-mesos.html](https://spark.apache.org/docs/latest/running-on-mesos.html)
- en: YARN: [https://spark.apache.org/docs/latest/running-on-yarn.html](https://spark.apache.org/docs/latest/running-on-yarn.html)
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YARN: [https://spark.apache.org/docs/latest/running-on-yarn.html](https://spark.apache.org/docs/latest/running-on-yarn.html)
- en: 'For Spark Standalone, use the following command-line options to configure the
    memory space:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Spark Standalone，请使用以下命令行选项配置内存空间：
- en: 'The on-heap memory for the driver can be configured like so (`8G` -> 8 GB of
    memory):'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动节点的堆内存可以按如下方式配置（`8G` -> 8GB内存）：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The off-heap memory for the driver can be configured like so:'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动节点的非堆内存可以按如下方式配置：
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The on-heap memory for the worker can be configured like so:'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点的堆内存可以按如下方式配置：
- en: '[PRE30]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The off-heap memory for the worker can be configured like so:'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点的非堆内存可以按如下方式配置：
- en: '[PRE31]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In step 5, we discussed garbage collection for worker nodes. Generally speaking,
    there are two ways in which we can control the frequency of garbage collection.
    The following is the first approach:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5步中，我们讨论了工作节点的垃圾回收。一般来说，我们可以通过两种方式控制垃圾回收的频率。以下是第一种方法：
- en: '[PRE32]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This will limit the frequency of garbage collector calls to the specified time
    interval, that is, `frequencyIntervalInMs`. The second approach is as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这将限制垃圾收集器调用的频率为指定的时间间隔，即`frequencyIntervalInMs`。第二种方法如下：
- en: '[PRE33]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This will totally disable the garbage collector's calls. However, the these
    approaches will not alter the worker node's memory configuration. We can configure
    the worker node's memory using the builder methods that are available in `SharedTrainingMaster`.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这将完全禁用垃圾收集器的调用。然而，这些方法不会改变工作节点的内存配置。我们可以使用`SharedTrainingMaster`中可用的构建器方法来配置工作节点的内存。
- en: We call `workerTogglePeriodicGC()` to disable/enable periodic **garbage collector**
    (**GC**) calls and `workerPeriodicGCFrequency()` to set the frequency in which
    GC needs to be called.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`workerTogglePeriodicGC()`来禁用/启用周期性**垃圾收集器**（**GC**）调用，并且调用`workerPeriodicGCFrequency()`来设置垃圾回收的频率。
- en: In step 6, we added support for Kryo serialization in ND4J. The Kryo serializer
    is a Java serialization framework that helps to increase the speed/efficiency
    during training in Spark.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6步中，我们为ND4J添加了对Kryo序列化的支持。Kryo序列化器是一个Java序列化框架，有助于提高Spark训练过程中的速度和效率。
- en: For more information, refer to [https://spark.apache.org/docs/latest/tuning.html](https://spark.apache.org/docs/latest/tuning.html).
    In step 8, locality configuration is an optional configuration that can be used
    to improve training performance. Data locality can have a major impact on the
    performance of Spark jobs. The idea is to ship the data and code together so that
    the computation can be performed really quickly. For more information, please
    refer to [https://spark.apache.org/docs/latest/tuning.html#data-locality](https://spark.apache.org/docs/latest/tuning.html#data-locality).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多信息，请参考 [https://spark.apache.org/docs/latest/tuning.html](https://spark.apache.org/docs/latest/tuning.html)。在第8步中，本地性配置是一个可选配置，可以用于提高训练性能。数据本地性对Spark作业的性能有重大影响。其思路是将数据和代码一起传输，以便计算能够快速执行。欲了解更多信息，请参考 [https://spark.apache.org/docs/latest/tuning.html#data-locality](https://spark.apache.org/docs/latest/tuning.html#data-locality)。
- en: There's more...
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: 'Memory configurations are often applied to master/worker nodes separately.
    Therefore, memory configuration on worker nodes alone may not bring the required
    results. The approach we take can vary, depending on the cluster resource manager
    we use. Therefore, it is important to refer to the respective documentation on
    the different approaches for a specific cluster resource manager. Also, note that
    the default memory settings in the cluster resource managers are not appropriate
    (too low) for libraries (ND4J/DL4J) that heavily rely on off-heap memory space.
    `spark-submit` can load the configurations in two different ways. One way is to
    use the *command line*, as we discussed previously, while another one is to specify
    the configuration in the `spark-defaults.conf` file, like so:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 内存配置通常分别应用于主节点/工作节点。因此，仅在工作节点上进行内存配置可能无法达到所需的效果。我们采取的方法可以根据所使用的集群资源管理器而有所不同。因此，参考关于特定集群资源管理器的不同方法的相关文档非常重要。此外，请注意，集群资源管理器中的默认内存设置不适合（过低）那些高度依赖堆外内存空间的库（ND4J/DL4J）。`spark-submit`
    可以通过两种方式加载配置。一种方法是使用 *命令行*，如前所述，另一种方法是将配置指定在 `spark-defaults.conf` 文件中，如下所示：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Spark can accept any Spark properties using the `--conf` flag. We used it to
    specify off-heap memory space in this recipe. You can read more about Spark configuration here: [http://spark.apache.org/docs/latest/configuration.html](http://spark.apache.org/docs/latest/configuration.html):'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 可以使用 `--conf` 标志接受任何 Spark 属性。我们在本教程中使用它来指定堆外内存空间。你可以在此处阅读有关 Spark 配置的更多信息：[http://spark.apache.org/docs/latest/configuration.html](http://spark.apache.org/docs/latest/configuration.html)：
- en: The dataset should justify the memory allocation in the driver/executor. For
    10 MB of data, we don't have to assign too much of the memory to the executor/driver.
    In this case, 2 GB to 4 GB of memory would be enough. Allotting too much memory
    won't make any difference and it can actually reduce the performance.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集应合理分配驱动程序/执行程序的内存。对于 10 MB 的数据，我们不需要为执行程序/驱动程序分配过多内存。在这种情况下，2 GB 至 4 GB 的内存就足够了。分配过多内存不会带来任何区别，反而可能降低性能。
- en: The *driver* is the process where the main Spark job runs. *Executors* are worker
    node tasks that have individual tasks allotted to run. If the application runs
    in local mode, the driver memory is not necessarily allotted. The driver memory
    is connected to the master node and it is relevant while the application is running
    in *cluster* mode. In *cluster* mode, the Spark job will not run on the local
    machine it was submitted from. The Spark driver component will launch inside the
    cluster.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*驱动程序*是主 Spark 作业运行的进程。 *执行器*是分配给工作节点的任务，每个任务有其单独的任务。如果应用程序以本地模式运行，则不一定分配驱动程序内存。驱动程序内存连接到主节点，并且与应用程序在
    *集群* 模式下运行时相关。在 *集群* 模式下，Spark 作业不会在提交的本地机器上运行。Spark 驱动组件将在集群内部启动。'
- en: Kryo is a fast and efficient serialization framework for Java. Kryo can also
    perform automatic deep/shallow copying of objects in order to attain a high speed,
    low size, and easy-to-use API. The DL4J API can make use of Kryo serialization
    to optimize the performance a bit further. However, note that since INDArrays consume
    off-heap memory space, Kryo may not result in much performance gain. Check the
    respective logs to ensure your Kryo configuration is correct while using it with
    the `SparkDl4jMultiLayer` or `SparkComputationGraph` classes.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kryo 是一个快速高效的 Java 序列化框架。Kryo 还可以执行对象的自动深拷贝/浅拷贝，以获得高速度、低体积和易于使用的 API。DL4J API
    可以利用 Kryo 序列化进一步优化性能。然而，请注意，由于 INDArrays 消耗堆外内存空间，Kryo 可能不会带来太多的性能提升。在使用 Kryo
    与 `SparkDl4jMultiLayer` 或 `SparkComputationGraph` 类时，请检查相应的日志，以确保 Kryo 配置正确。
- en: Just like in regular training, we need to add the proper ND4J backend for DL4J
    Spark to function. For newer versions of YARN, some additional configurations
    may be required. Refer to the YARN documentation for more details: [https://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-site/UsingGpus.html](https://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-site/UsingGpus.html).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就像在常规训练中一样，我们需要为 DL4J Spark 添加合适的 ND4J 后端才能正常运行。对于较新版本的 YARN，可能需要一些额外的配置。有关详细信息，请参考 YARN
    文档：[https://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-site/UsingGpus.html](https://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-site/UsingGpus.html)。
- en: Also, note that older versions (2.7.x or earlier) will not support GPUs natively
    (GPU and CPU). For these versions, we need to use node labels to ensure that jobs
    are running in GPU-only machines.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，旧版本（2.7.x 或更早版本）不原生支持 GPU（GPU 和 CPU）。对于这些版本，我们需要使用节点标签来确保作业运行在仅 GPU 的机器上。
- en: If you perform Spark training, you need to be aware of data locality in order
    to optimize the throughput. Data locality ensures that the data and the code that
    operates on the Spark job are together and not separate. Data locality ships the
    serialized code from place to place (instead of chunks of data) where the data
    operates. It will speed up its performance and won't introduce further issues
    since the size of the code will be significantly smaller than the data. Spark
    provides a configuration property named `spark.locality.wait` to specify the timeout
    before moving the data to a free CPU. If you set it to zero, then data will be
    immediately moved to a free executor rather than wait for a specific executor
    to become free. If the freely available executor is distant from the executor
    where the current task is executed, then it is an additional effort. However,
    we are saving time by waiting for a nearby executor to become free. So, the computation
    time can still be reduced. You can read more about data locality on Spark here: [https://spark.apache.org/docs/latest/tuning.html#data-locality](https://spark.apache.org/docs/latest/tuning.html#data-locality).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你进行 Spark 训练，需要注意数据本地性以优化吞吐量。数据本地性确保数据和操作 Spark 作业的代码是一起的，而不是分开的。数据本地性将序列化的代码从一个地方传输到另一个地方（而不是数据块），在那里数据进行操作。它将加速性能，并且不会引入进一步的问题，因为代码的大小远小于数据。Spark
    提供了一个名为 `spark.locality.wait` 的配置属性，用于指定在将数据移至空闲 CPU 之前的超时。如果你将其设置为零，则数据将立即被移动到一个空闲的执行器，而不是等待某个特定的执行器变为空闲。如果空闲执行器距离当前任务执行的执行器较远，那么这将增加额外的工作量。然而，我们通过等待附近的执行器变空闲来节省时间。因此，计算时间仍然可以减少。你可以在
    Spark 官方文档中阅读更多关于数据本地性的信息：[https://spark.apache.org/docs/latest/tuning.html#data-locality](https://spark.apache.org/docs/latest/tuning.html#data-locality)。
- en: Configuring encoding thresholds
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置编码阈值
- en: The DL4J Spark implementation makes use of a threshold encoding scheme to perform
    parameter updates across nodes in order to reduce the commuted message size across
    the network and thereby reduce the cost of traffic. The threshold encoding scheme
    introduces a new distributed training-specific hyperparameter called **encoding
    threshold**.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J Spark 实现利用阈值编码方案跨节点执行参数更新，以减少网络中传输的消息大小，从而降低流量成本。阈值编码方案引入了一个新的分布式训练专用超参数，称为
    **编码阈值**。
- en: In this recipe, we will configure the threshold algorithm in a distributed training
    implementation.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方案中，我们将在分布式训练实现中配置阈值算法。
- en: How to do it...
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Configure the threshold algorithm in `SharedTrainingMaster`:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `SharedTrainingMaster` 中配置阈值算法：
- en: '[PRE35]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Configure the residual vectors by calling `residualPostProcessor()`:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `residualPostProcessor()` 配置残差向量：
- en: '[PRE36]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works...
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In step 1, we configured the threshold algorithm in `SharedTrainingMaster`,
    where the default algorithm is `AdaptiveThresholdAlgorithm`. Threshold algorithms
    will determine the encoding threshold for distributed training, which is a hyperparameter
    that's specific to distributed training. Also, note that we are not discarding
    the rest of the parameter updates. As we mentioned earlier, we put them into separate
    residual vectors and process them later. We do this to reduce the network traffic/load
    during training. **`AdaptiveThresholdAlgorithm` **is preferred in most cases for
    better performance.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 1 步中，我们在 `SharedTrainingMaster` 中配置了阈值算法，默认算法为 `AdaptiveThresholdAlgorithm`。阈值算法将决定分布式训练的编码阈值，这是一个特定于分布式训练的超参数。此外，值得注意的是，我们并没有丢弃其余的参数更新。如前所述，我们将它们放入单独的残差向量，并在后续处理。这是为了减少训练过程中网络流量/负载。**`AdaptiveThresholdAlgorithm`**
    在大多数情况下更为优选，以获得更好的性能。
- en: In step 2, we used `ResidualPostProcessor` to post process the residual vector.
    The residual vector was created internally by the gradient sharing implementation
    to collect parameter updates that were not marked by the specified bound. Most
    implementations of `ResidualPostProcessor` will clip/decay the residual vector
    so that the values in them will not become too large compared to the threshold
    value. `ResidualClippingPostProcessor` is one such implementation. `ResidualPostProcessor`
    will prevent the residual vector from becoming too large in size as it can take
    too much time to communicate and may lead to stale gradient issues.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤2中，我们使用了`ResidualPostProcessor`来处理残差向量。残差向量是由梯度共享实现内部创建的，用于收集未被指定边界标记的参数更新。大多数`ResidualPostProcessor`的实现都会裁剪/衰减残差向量，以确保其中的值不会相对于阈值过大。`ResidualClippingPostProcessor`就是一种这样的实现。`ResidualPostProcessor`将防止残差向量变得过大，因为它可能需要太长时间来传输，且可能导致过时的梯度问题。
- en: 'In step 1, we called `thresholdAlgorithm()` to set the threshold algorithm.
    In step 2, we called `residualPostProcessor()` to post process the residual vector
    for the gradient sharing implementation in DL4J. `ResidualClippingPostProcessor`
    accepts two attributes: `clipValue` and `frequency`. `clipValue` is the multiple
    of the current threshold that we use for clipping. For example, if threshold is
    `t` and `clipValue` is `c`, then the residual vectors will be clipped to the range
    **`[-c*t , c*t]`**.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1中，我们调用了`thresholdAlgorithm()`来设置阈值算法。在步骤2中，我们调用了`residualPostProcessor()`，以处理用于DL4J中的梯度共享实现的残差向量。`ResidualClippingPostProcessor`接受两个属性：`clipValue`和`frequency`。`clipValue`是我们用于裁剪的当前阈值的倍数。例如，如果阈值为`t`，而`clipValue`为`c`，那么残差向量将被裁剪到范围**`[-c*t
    , c*t]`**。
- en: There's more...
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容…
- en: The idea behind the threshold (the encoding threshold, in our context) is that
    the parameter updates will happen across clusters, but only for the values that
    come under the user-defined limit (threshold). This threshold value is what we
    refer to as the encoding threshold. Parameter updates refer to the changes in
    gradient values during the training process. High/low encoding threshold values
    are not good for optimal results. So, it is reasonable to come up with a range
    of acceptable values for the encoding threshold. This is also termed as the sparsity
    ratio, in which the parameter updates happen across clusters.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值背后的理念（在我们的上下文中是编码阈值）是，参数更新将在集群间发生，但仅限于那些符合用户定义的限制（阈值）的值。这个阈值值就是我们所说的编码阈值。参数更新指的是在训练过程中梯度值的变化。过高或过低的编码阈值对于获得最佳结果并不理想。因此，提出一个可接受的编码阈值范围是合理的。这也被称为稀疏度比率，其中参数更新发生在集群之间。
- en: In this recipe, we also discussed how to configure threshold algorithms for
    distributed training. The default choice would be to use `AdaptiveThresholdAlgorithm` if `AdaptiveThresholdAlgorithm` provides
    undesired results.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇教程中，我们还讨论了如何为分布式训练配置阈值算法。如果`AdaptiveThresholdAlgorithm`的效果不理想，默认选择是使用它。
- en: 'The following are the various threshold algorithms that are available in DL4J:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是DL4J中可用的各种阈值算法：
- en: '`AdaptiveThresholdAlgorithm`: This is the default threshold algorithm that
    works well in most scenarios.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AdaptiveThresholdAlgorithm`：这是默认的阈值算法，在大多数场景下都能很好地工作。'
- en: '`FixedThresholdAlgorithm`: This is a fixed and non-adaptive threshold strategy.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FixedThresholdAlgorithm`：这是一种固定且非自适应的阈值策略。'
- en: '`TargetSparsityThresholdAlgorithm`: This is an adaptive threshold strategy
    with a specific target. It decreases or increases the threshold to try and match
    the target.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TargetSparsityThresholdAlgorithm`：这是一种具有特定目标的自适应阈值策略。它通过降低或提高阈值来尝试匹配目标。'
- en: Performing a distributed test set evaluation
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行分布式测试集评估
- en: 'There are challenges involved in distributed neural network training. Some
    of these challenges include managing different hardware dependencies across master
    and worker nodes, configuring distributed training to produce good performance,
    memory benchmarks across the distributed clusters, and more. We discussed some
    of those concerns in the previous recipes. While keeping such configurations in
    place, we''ll move on to the actual distributed training/evaluation. In this recipe,
    we will perform the following tasks:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式神经网络训练中存在一些挑战。这些挑战包括管理主节点和工作节点之间的不同硬件依赖关系，配置分布式训练以提高性能，跨分布式集群的内存基准测试等。我们在之前的教程中讨论了一些这些问题。在保持这些配置的同时，我们将继续进行实际的分布式训练/评估。在本教程中，我们将执行以下任务：
- en: ETL for DL4J Spark training
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DL4J Spark训练的ETL过程
- en: Create a neural network for Spark training
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为Spark训练创建神经网络
- en: Perform a test set evaluation
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行测试集评估
- en: How to do it...
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何执行...
- en: 'Download, extract, and copy the contents of the `TinyImageNet` dataset to the
    following directory location:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载、解压并将`TinyImageNet`数据集的内容复制到以下目录位置：
- en: '[PRE37]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Create batches of images for training using the `TinyImageNet` dataset:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`TinyImageNet`数据集创建训练图像批次：
- en: '[PRE38]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Create batches of images for testing using the `TinyImageNet` dataset:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`TinyImageNet`数据集创建测试图像批次：
- en: '[PRE39]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Create an `ImageRecordReader` that holds a reference of the dataset:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`ImageRecordReader`，它保存数据集的引用：
- en: '[PRE40]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create `RecordReaderFileBatchLoader` from `ImageRecordReader` to load the batch
    data:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`ImageRecordReader`创建`RecordReaderFileBatchLoader`以加载批数据：
- en: '[PRE41]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Use JCommander at the beginning of your source code to parse command-line arguments:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在源代码开始时使用JCommander解析命令行参数：
- en: '[PRE42]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Create a parameter server configuration (gradient sharing) for Spark training
    using `VoidConfiguration`, as shown in the following code:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`VoidConfiguration`为Spark训练创建参数服务器配置（梯度共享），如下代码所示：
- en: '[PRE43]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Configure a distributed training network using `SharedTrainingMaster`, as shown
    in the following code:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`SharedTrainingMaster`配置分布式训练网络，如下代码所示：
- en: '[PRE44]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Create a `GraphBuilder` for `ComputationGraphConfguration`, as shown in the
    following code:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为`ComputationGraphConfguration`创建`GraphBuilder`，如下代码所示：
- en: '[PRE45]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Use `DarknetHelper` from the DL4J Model Zoo to power up our CNN architecture,
    as shown in the following code:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用DL4J模型库中的`DarknetHelper`来增强我们的CNN架构，如下代码所示：
- en: '[PRE46]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Configure the output layers while considering the number of labels and loss
    functions, as shown in the following code:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置输出层时，考虑标签的数量和损失函数，如下代码所示：
- en: '[PRE47]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Create `ComputationGraphConfguration` from the `GraphBuilder`:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`GraphBuilder`创建`ComputationGraphConfguration`：
- en: '[PRE48]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Create the `SparkComputationGraph` model from the defined configuration and
    set training listeners to it:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从定义的配置创建`SparkComputationGraph`模型，并为其设置训练监听器：
- en: '[PRE49]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Create `JavaRDD` objects that represent the HDFS paths of the batch files that
    we created earlier for training:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建代表我们之前为训练创建的批文件的HDFS路径的`JavaRDD`对象：
- en: '[PRE50]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Invoke the training instance by calling `fitPaths()`:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`fitPaths()`来启动训练实例：
- en: '[PRE51]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Create `JavaRDD` objects that represent the HDFS paths to batch files that
    we created earlier for testing:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建代表我们之前创建的用于测试的批文件的HDFS路径的`JavaRDD`对象：
- en: '[PRE52]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Evaluate the distributed neural network by calling `doEvaluation()`:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`doEvaluation()`评估分布式神经网络：
- en: '[PRE53]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Run the distributed training instance on `spark-submit` in the following format:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下格式中通过`spark-submit`运行分布式训练实例：
- en: '[PRE54]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: How it works....
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的....
- en: 'Step 1 can be automated using `TinyImageNetFetcher`, as shown here:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步可以通过`TinyImageNetFetcher`来自动化，如下所示：
- en: '[PRE55]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'For any OS, the data needs to be copied to the user''s home directory. Once
    it is executed, we can get a reference to the train/test dataset directory, as
    shown here:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何操作系统，数据需要复制到用户的主目录。一旦执行完毕，我们可以获取训练/测试数据集目录的引用，如下所示：
- en: '[PRE56]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: You can also mention your own input directory location from your local disk
    or HDFS. You will need to mention that in place of `dirPathDataSet` in step 2.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以提到自己本地磁盘或HDFS的输入目录位置。你需要在第二步中将其作为`dirPathDataSet`替换。
- en: In step 2 and step 3, we created batches of images so that we could optimize
    the distributed training. We used `createFileBatchesLocal()` to create these batches,
    where the source of the data is a local disk. If you want to create batches from
    the HDFS source, then use `createFileBatchesSpark()` instead. These compressed
    batch files will save space and reduce bottlenecks in computation. Suppose we
    loaded 64 images in a compressed batch – we don't require 64 different disk reads
    to process the batch file. These batches contain the contents of raw files from
    multiple files.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步和第三步中，我们创建了图像批次，以便优化分布式训练。我们使用了`createFileBatchesLocal()`来创建这些批次，其中数据来源于本地磁盘。如果你想从HDFS源创建批次，则可以使用`createFileBatchesSpark()`。这些压缩的批文件将节省空间并减少计算瓶颈。假设我们在一个压缩批次中加载了64张图像—我们不需要64次不同的磁盘读取来处理该批次文件。这些批次包含了多个文件的原始文件内容。
- en: In step 5, we used `RecordReaderFileBatchLoader` to process file batch objects
    that were created using either `createFileBatchesLocal()` or `createFileBatchesSpark()`.
    As we mentioned in step 6, you can use JCommander to process the command-line
    arguments from `spark-submit` or write your own logic to handle them.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5步，我们使用`RecordReaderFileBatchLoader`处理了通过`createFileBatchesLocal()`或`createFileBatchesSpark()`创建的文件批处理对象。如第6步所提到的，你可以使用JCommander处理来自`spark-submit`的命令行参数，或者编写自己的逻辑来处理这些参数。
- en: In step 7, we configured the parameter server using the `VoidConfiguration`
    class. This is a basic configuration POJO class for the parameter server. We can
    mention the port number, network mask, and so on for the parameter server. The
    network mask is a very important configuration in a shared network environment
    and YARN.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7步，我们使用`VoidConfiguration`类配置了参数服务器。这是一个用于参数服务器的基本配置POJO类。我们可以为参数服务器指定端口号、网络掩码等配置。网络掩码在共享网络环境和YARN中是非常重要的配置。
- en: In step 8, we started configuring the distributed network for training using
    `SharedTrainingMaster`. We added important configurations such as threshold algorithms,
    worker node count, minibatch size, and so on.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8步，我们开始使用`SharedTrainingMaster`配置分布式网络进行训练。我们添加了重要的配置，例如阈值算法、工作节点数、最小批量大小等。
- en: Starting from steps 9 and 10, we focused on distributed neural network layer
    configuration. We used `DarknetHelper` from the DL4J Model Zoo to borrow functionalities
    from DarkNet, TinyYOLO and YOLO2.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 从第9步和第10步开始，我们专注于分布式神经网络层配置。我们使用来自DL4J模型库的`DarknetHelper`，借用了DarkNet、TinyYOLO和YOLO2的功能。
- en: In step 11, we added the output layer configuration for our tiny `ImageNet`
    classifier. There are 200 labels in which the image classifier makes a prediction.
    In step 13, we created a Spark-based `ComputationGraph` using `SparkComputationGraph`.
    If the underlying network structure is `MultiLayerNetwork`, then you could use `SparkDl4jMultiLayer`
    instead.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在第11步，我们为我们的微型`ImageNet`分类器添加了输出层配置。该分类器有200个标签，用于进行图像分类预测。在第13步，我们使用`SparkComputationGraph`创建了一个基于Spark的`ComputationGraph`。如果底层网络结构是`MultiLayerNetwork`，你可以改用`SparkDl4jMultiLayer`。
- en: 'In step 17, we created an evaluation instance, as shown here:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在第17步，我们创建了一个评估实例，如下所示：
- en: '[PRE57]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: The second attribute (`5`, in the preceding code) represents the value `N`,
    which is used to measure the top `N` accuracy metrics. For example, evaluation
    on a sample will be correct if the probability for the `true` class is one of
    the highest `N` values.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个属性（前述代码中的`5`）表示值`N`，用于衡量前`N`的准确性指标。例如，如果`true`类别的概率是前`N`个最高的值之一，那么对样本的评估就是正确的。
- en: Saving and loading trained neural network models
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存和加载训练好的神经网络模型
- en: Training the neural network over and over to perform an evaluation is not a
    good idea since training is a very costly operation. This is why model persistence
    is important in distributed systems as well.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 反复训练神经网络以进行评估并不是一个好主意，因为训练是一项非常耗费资源的操作。这也是为什么模型持久化在分布式系统中同样重要的原因。
- en: In this recipe, we will persist the distributed neural network models to disk
    and load them for further use.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个教程中，我们将把分布式神经网络模型持久化到磁盘，并在之后加载以供进一步使用。
- en: How to do it...
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Save the distributed neural network model using `ModelSerializer`:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ModelSerializer`保存分布式神经网络模型：
- en: '[PRE58]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Save the distributed neural network model using `save()`:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`save()`保存分布式神经网络模型：
- en: '[PRE59]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Load the distributed neural network model using `ModelSerializer`:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ModelSerializer`加载分布式神经网络模型：
- en: '[PRE60]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Load the distributed neural network model using `load()`:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`load()`加载分布式神经网络模型：
- en: '[PRE61]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: How it works...
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Although we used `save()` or `load()` for the model's persistence in a local
    machine, it is not an ideal practice in production. For a distributed cluster
    environment, we can make use of `BufferedInputStream`/`BufferedOutputStream` in
    steps 1 and 2 to save/load models to/from clusters. We can use `ModelSerializer`
    or `save()`/`load()` just like we demonstrated earlier. We just need to be aware
    of the cluster resource manager and model persistence, which can be performed
    across clusters.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在本地机器上使用`save()`或`load()`进行模型持久化，但在生产环境中这并不是最佳实践。对于分布式集群环境，我们可以在第1步和第2步使用`BufferedInputStream`/`BufferedOutputStream`将模型保存到集群或从集群加载模型。我们可以像之前展示的那样使用`ModelSerializer`或`save()`/`load()`。我们只需注意集群资源管理器和模型持久化，这可以跨集群进行。
- en: There's more...
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: '`SparkDl4jMultiLayer` and `SparkComputationGraph` internally make use of the
    standard implementations of `MultiLayerNetwork` and `ComputationGraph`, respectively.
    Thus, their internal structure can be accessed by calling the `getNetwork()` method.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkDl4jMultiLayer`和`SparkComputationGraph`内部分别使用了`MultiLayerNetwork`和`ComputationGraph`的标准实现。因此，可以通过调用`getNetwork()`方法访问它们的内部结构。'
- en: Performing distributed inference
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行分布式推理
- en: In this chapter, we have discussed how to perform distributed training using
    DL4J. We have also performed distributed evaluation to evaluate the trained distributed
    model. Now, let's discuss how to utilize the distributed model to solve use cases
    such as predictions. This is referred to as inference. Let's go over how we can
    perform *distributed* inference in a Spark environment.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何使用DL4J进行分布式训练。我们还进行了分布式评估，以评估训练好的分布式模型。现在，让我们讨论如何利用分布式模型来解决预测等用例。这被称为推理。接下来，我们将介绍如何在Spark环境中进行*分布式*推理。
- en: In this recipe, we will perform distributed inference on Spark using DL4J.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用DL4J在Spark上执行分布式推理。
- en: How to do it...
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform distributed inference for `SparkDl4jMultiLayer` by calling `feedForwardWithKey()`,
    as shown here:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`feedForwardWithKey()`执行`SparkDl4jMultiLayer`的分布式推理，如下所示：
- en: '[PRE62]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Perform distributed inference for `SparkComputationGraph` by calling `feedForwardWithKey()`:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用`feedForwardWithKey()`执行`SparkComputationGraph`的分布式推理：
- en: '[PRE63]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: How it works...
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The intent of the `feedForwardWithKey()` method in step 1 and 2 is to generate
    output/predictions for the given input dataset. A map is returned from this method.
    The input data is represented by the keys in the map and the results (output)
    are represented by values (`INDArray`).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步和第2步中`feedForwardWithKey()`方法的目的是为给定的输入数据集生成输出/预测。该方法返回一个映射。输入数据通过映射中的键表示，结果（输出）通过值（`INDArray`）表示。
- en: '`feedForwardWithKey()` accepts two attributes: input data and the minibatch
    size for feed-forward operations. The input data (features) is in the format of `JavaPairRDD<K`,
    `INDArray>`.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '`feedForwardWithKey()`接受两个参数：输入数据和用于前馈操作的小批量大小。输入数据（特征）采用`JavaPairRDD<K`, `INDArray>`的格式。'
- en: Note that RDD data is unordered. We need a way to map each input to the respective
    results (output). Hence, we need to have a key-value pair that maps each input
    to its respective output. That's the main reason why we use key values here. It
    has nothing to do with the inference process. Values for the minibatch size are
    used for the trade-off between memory versus computational efficiency.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，RDD数据是无序的。我们需要一种方法将每个输入映射到相应的结果（输出）。因此，我们需要一个键值对，将每个输入映射到其相应的输出。这就是为什么我们在这里使用键值的主要原因。这与推理过程本身无关。*小批量大小*的值用于在内存与计算效率之间进行权衡。
