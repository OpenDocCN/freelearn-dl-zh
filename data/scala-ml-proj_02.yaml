- en: Analyzing and Predicting Telecommunication Churn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析与预测电信行业流失
- en: In this chapter, we will develop a **machine learning** (**ML**) project to
    analyze and predict whether a customer is likely to cancel the subscription to
    his telecommunication contract or not. In addition, we'll do some preliminary
    analysis of the data and take a closer look at what types of customer features
    are typically responsible for such a churn.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将开发一个**机器学习**（**ML**）项目，用于分析和预测客户是否可能取消其电信合同的订阅。此外，我们还将对数据进行一些初步分析，仔细查看哪些客户特征通常与这种流失相关。
- en: Widely used classification algorithms, such as decision trees, random forest,
    logistic regression, and **Support Vector Machines** (**SVMs**) will be used for
    analyzing and making the prediction. By the end, readers will be able to choose
    the best model to use for a production-ready environment.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛使用的分类算法，如决策树、随机森林、逻辑回归和**支持向量机**（**SVM**），将用于分析和做出预测。最终，读者将能够选择最适合生产环境的最佳模型。
- en: 'In a nutshell, we will learn the following topics throughout this end-to-end
    project:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在这个端到端的项目中，我们将学习以下主题：
- en: Why, and how, do we do churn prediction?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么以及如何进行流失预测？
- en: Logistic regression-based churn prediction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于逻辑回归的流失预测
- en: SVM-based churn prediction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 SVM 的流失预测
- en: Decision tree-based churn prediction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于决策树的流失预测
- en: Random forest-based churn prediction
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于随机森林的流失预测
- en: Selecting the best model for deployment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最佳模型进行部署
- en: Why do we perform churn analysis, and how do we do it?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们要进行流失分析，如何进行流失分析？
- en: '**Customer churn** is the loss of clients or customers (also known as **customer
    attrition**, customer turnover, or customer defection). This concept was initially
    used within the telecommunications industry when many subscribers switched to
    other service providers. However, it has become a very important issue in other
    areas of business, such as banks, internet service providers, insurance companies,
    and so on. Well, two of the primary reasons for churn are customer dissatisfaction
    and cheaper and/or better offers from the competition.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**客户流失**是指客户或顾客的流失（也称为**客户流失率**、客户流动率或客户弃用）。这一概念最初用于电信行业，当时许多用户转向其他服务提供商。然而，这已成为其他业务领域的重要问题，如银行、互联网服务提供商、保险公司等。嗯，流失的主要原因之一是客户不满，以及竞争对手提供更便宜或更好的优惠。'
- en: 'As you can see in *Figure 1*, there are four possible contracts with the customer
    in a business industry: contractual, non-contractual, voluntary, and involuntary.
    The full cost of customer churn includes both the lost revenue and the (tele-)
    marketing costs involved with replacing those customers with new ones. However,
    this type of loss can cause a huge loss to a business. Think back to a decade
    ago, when Nokia was the dominator of the cell phone market. All of a sudden, Apple
    announced iPhone 3G, and that was a revolution in the smartphone era. Then, around
    10 to 12% of customers stopped using Nokia and switched to iPhone. Although later
    on, Nokia also tried to release a smartphone, eventually, they could not compete
    with Apple:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在*图 1*中所见，商业行业中与客户可能签订的合同有四种类型：契约性合同、非契约性合同、自愿性合同和非自愿性合同。客户流失的全部成本包括失去的收入以及与用新客户替代这些流失客户所涉及的（电）营销成本。然而，这种类型的损失可能会给企业带来巨大的损失。想想十年前，当诺基亚是手机市场的霸主时，突然，苹果发布了
    iPhone 3G，这标志着智能手机时代的革命。接着，大约10%到12%的客户停止使用诺基亚，转而选择了 iPhone。虽然后来诺基亚也尝试推出智能手机，但最终，它们无法与苹果竞争：
- en: '![](img/cc26a83d-61b4-4cf0-b673-1e91438ec329.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cc26a83d-61b4-4cf0-b673-1e91438ec329.png)'
- en: 'Figure 1: Four types of possible contracts with the customers'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：与客户可能签订的四种合同类型
- en: Churn prediction is fundamental to businesses, as it allows them to detect customers
    who are likely to cancel a subscription, product, or service. It can also minimize
    customer defection. It does so by predicting which customers are likely to cancel
    a subscription to a service. Then, the respective business can have a special
    offer or plan for those customers (who might cancel the subscription). This way,
    a business can reduce the churn ratio. This should be a key business goal of every
    online business.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 流失预测对企业至关重要，因为它能帮助企业检测出可能取消订阅、产品或服务的客户。它还可以最大限度地减少客户流失。通过预测哪些客户可能取消订阅服务，企业可以为这些客户（可能取消订阅的客户）提供特别优惠或计划。这样，企业就可以减少流失率。这应该是每个在线业务的关键目标。
- en: 'When it comes to employee churn prediction, the typical task is to determine
    what factors predict an employee leaving his/her job. These types of prediction
    processes are heavily data-driven and are often required to utilize advanced ML
    techniques. In this chapter, however, we will mainly focus on customer churn prediction
    and analysis. For this, a number of factors should be analyzed in order to understand
    the customer''s behavior, including but not limited to:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在员工流失预测方面，典型的任务是确定哪些因素预测员工离职。这类预测过程依赖于大量数据，通常需要利用先进的机器学习技术。然而，在本章中，我们将主要关注客户流失的预测和分析。为此，应该分析多个因素，以便理解客户行为，包括但不限于：
- en: Customer's demographic data, such as age, marital status, and so on
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户的基本信息数据，如年龄、婚姻状况等
- en: Customer's sentiment analysis of social media
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户的社交媒体情感分析
- en: Browsing behavior from clickstream logs
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从点击流日志中获取的浏览行为
- en: Historical data that shows patterns of behavior that suggest churn
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示行为模式的历史数据，提示可能的客户流失
- en: Customer's usage patterns and geographical usage trends
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户的使用模式和地理位置使用趋势
- en: Calling-circle data and support call center statistics
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通话圈数据和支持呼叫中心统计信息
- en: Developing a churn analytics pipeline
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发一个流失分析管道
- en: 'In ML, we observe an algorithm''s performance in two stages: learning and inference.
    The ultimate target of the learning stage is to prepare and describe the available
    data, also called the **feature vector**, which is used to train the model.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们将算法的表现分为两个阶段：学习和推理。学习阶段的最终目标是准备和描述可用数据，也称为**特征向量**，它用于训练模型。
- en: The learning stage is one of the most important stages, but it is also truly
    time-consuming. It involves preparing a list of vectors, also called **feature
    vectors** (vectors of numbers representing the value of each feature), from the
    training data after transformation so that we can feed them to the learning algorithms.
    On the other hand, training data also sometimes contains impure information that
    needs some pre-processing, such as cleaning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 学习阶段是最重要的阶段之一，但也是极其耗时的。它包括从经过转换的训练数据中准备特征向量（也称为特征向量，表示每个特征值的数字向量），以便我们可以将其输入到学习算法中。另一方面，训练数据有时也包含一些不纯净的信息，需要一些预处理，例如清理。
- en: 'Once we have the feature vectors, the next step in this stage is preparing
    (or writing/reusing) the learning algorithm. The next important step is training
    the algorithm to prepare the predictive model. Typically, (and of course based
    on data size), running an algorithm may take hours (or even days) so that the
    features converge into a useful model, as shown in the following figure:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有特征向量，接下来的步骤是准备（或编写/重用）学习算法。下一个重要步骤是训练算法，以准备预测模型。通常，（当然根据数据大小），运行一个算法可能需要几个小时（甚至几天），以使特征收敛为有用的模型，如下图所示：
- en: '![](img/f458d39c-885d-4161-b766-251ea56c1efb.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f458d39c-885d-4161-b766-251ea56c1efb.png)'
- en: 'Figure 2: Learning and training a predictive model - it shows how to generate
    the feature vectors from the training data to train the learning algorithm that
    produces a predictive model'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：学习和训练预测模型 - 展示了如何从训练数据中生成特征向量，进而训练学习算法，最终产生预测模型
- en: 'The second most important stage is the inference that is used for making an
    intelligent use of the model, such as predicting from the never-before-seen data,
    making recommendations, deducing future rules, and so on. Typically, it takes
    less time compared to the learning stage, and is sometimes even in real time.
    Thus, inferencing is all about testing the model against new (that is, unobserved)
    data and evaluating the performance of the model itself, as shown in the following
    figure:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个最重要的阶段是推理，它用于智能地利用模型，例如对从未见过的数据进行预测、提供推荐、推断未来规则等。通常，与学习阶段相比，推理所需的时间较短，有时甚至是实时的。因此，推理的核心是通过新的（即未观察过的）数据测试模型，并评估模型本身的表现，如下图所示：
- en: '![](img/7e917a27-ffcf-43d6-8794-a16d48bd0fed.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e917a27-ffcf-43d6-8794-a16d48bd0fed.png)'
- en: 'Figure 3: Inferencing from an existing model towards predictive analytics (feature
    vectors are generated from unknown data for making predictions)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：从现有模型进行推理以进行预测分析（特征向量由未知数据生成，用于做出预测）
- en: 'However, during the whole process and for making the predictive model a successful
    one, data acts as the first-class citizen in all ML tasks. Keeping all this in
    mind, the following figure shows an analytics pipeline that can be used by telecommunication
    companies:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在整个过程中，为了使预测模型成功，数据在所有机器学习任务中都是至关重要的。考虑到这一点，下面的图表展示了电信公司可以使用的分析管道：
- en: '![](img/9abacbdc-9ed4-4ab4-938f-54ee24efcc24.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9abacbdc-9ed4-4ab4-938f-54ee24efcc24.png)'
- en: 'Figure 4: Churn analytics pipeline'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：流失分析管道
- en: With this kind of analysis, telecom companies can discern how to predict and
    enhance the customer experience, which can, in turn, prevent churn and tailor
    marketing campaigns. In practice, often these business assessments are used in
    order to retain the customers most likely to leave, as opposed to those who are
    likely to stay.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种分析，电信公司可以辨别如何预测并改善客户体验，从而防止客户流失并量身定制营销活动。在实际操作中，这类商业评估通常用于留住最可能流失的客户，而非那些可能留下的客户。
- en: Thus, we need to develop a predictive model so that it ensures that our model
    is sensitive to the `Churn = True` samples—that is, a binary classification problem.
    We will see more details in upcoming sections.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要开发一个预测模型，确保我们的模型对 `Churn = True` 样本具有敏感性——这是一个二分类问题。我们将在接下来的章节中详细探讨。
- en: Description of the dataset
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集描述
- en: 'The **Orange Telecom''s Churn Dataset**, which consists of cleaned customer
    activity data (features), along with a churn label specifying whether a customer
    canceled the subscription, will be used to develop our predictive model. The churn-80
    and churn-20 datasets can be downloaded from the following links, respectively:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**Orange Telecom 的客户流失数据集**，包含了清理后的客户活动数据（特征），以及一个流失标签，指示客户是否取消了订阅。我们将使用该数据集来开发我们的预测模型。可以通过以下链接分别下载
    churn-80 和 churn-20 数据集：'
- en: '[https://bml-data.s3.amazonaws.com/churn-bigml-80.csv](https://bml-data.s3.amazonaws.com/churn-bigml-80.csv)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://bml-data.s3.amazonaws.com/churn-bigml-80.csv](https://bml-data.s3.amazonaws.com/churn-bigml-80.csv)'
- en: '[https://bml-data.s3.amazonaws.com/churn-bigml-20.csv](https://bml-data.s3.amazonaws.com/churn-bigml-20.csv)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://bml-data.s3.amazonaws.com/churn-bigml-20.csv](https://bml-data.s3.amazonaws.com/churn-bigml-20.csv)'
- en: However, as more data is often desirable for developing ML models, let's use
    the larger set (that is, churn-80) for training and cross-validation purposes,
    and the smaller set (that is, churn-20) for final testing and model performance
    evaluation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于更多的数据对于开发机器学习模型通常是有利的，因此我们将使用较大的数据集（即 churn-80）进行训练和交叉验证，使用较小的数据集（即 churn-20）进行最终测试和模型性能评估。
- en: 'Note that the latter set is only used to evaluate the model (that is for demonstration
    purposes). For a production ready environment, telecommunication companies can
    use their own dataset with necessary preprocessing and feature engineering. The
    dataset has the following schema:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，后者数据集仅用于评估模型（即用于演示目的）。在生产环境中，电信公司可以使用自己的数据集，经过必要的预处理和特征工程。该数据集的结构如下：
- en: '**State**: `String`'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**州**: `String`'
- en: '**Account length**: `Integer`'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**账户时长**: `Integer`'
- en: '**Area code**: `Integer`'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**区号**: `Integer`'
- en: '**International plan**: `String`'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**国际计划**: `String`'
- en: '**Voicemail plan**: `String`'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音邮件计划**: `String`'
- en: '**Number email messages**: `Integer`'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**电子邮件消息数量**: `Integer`'
- en: '**Total day minutes**: `Double`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总白天分钟数**: `Double`'
- en: '**Total day calls**: `Integer`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总白天电话数量**: `Integer`'
- en: '**Total day charge**: `Double`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总白天费用**: `Double`'
- en: '**Total eve minutes**: `Double`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总傍晚分钟数**: `Double`'
- en: '**Total eve calls**: `Integer`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总傍晚电话数量**: `Integer`'
- en: '**Total eve charge**: `Double`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总傍晚费用**: `Double`'
- en: '**Total night minutes**: `Double`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总夜间通话分钟数**: `Double`'
- en: '**Total night calls**: `Integer`'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总夜间电话数量**: `Integer`'
- en: '**Total night charge**: `Double`'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总夜间费用**: `Double`'
- en: '**Total intl minutes**: `Double`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总国际分钟数**: `Double`'
- en: '**Total intl calls**: `Integer`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总国际电话数量**: `Integer`'
- en: '**Total intl charge**: `Double`'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总国际费用**: `Double`'
- en: '**Customer service calls**: `Integer`'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户服务电话**: `Integer`'
- en: Exploratory analysis and feature engineering
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性分析与特征工程
- en: 'In this sub-section, we will see some EDA of the dataset before we start preprocessing
    and feature engineering. Only then creation of an analytics pipeline makes sense.
    At first, let''s import necessary packages and libraries as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们将在开始预处理和特征工程之前，对数据集进行一些探索性数据分析（EDA）。只有在此之后，创建分析管道才有意义。首先，让我们导入必要的软件包和库，代码如下：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Then, let's specify the data source and schema for the dataset to be processed.
    When loading the data into a DataFrame, we can specify the schema. This specification
    provides optimized performance compared to the pre-Spark 2.x schema inference.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们指定数据集的来源和模式。当将数据加载到DataFrame时，我们可以指定模式。这一指定相比于Spark 2.x之前的模式推断提供了优化的性能。
- en: 'At first, let''s create a Scala case class with all the fields specified. The
    variable names are self-explanatory:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个包含所有字段的Scala案例类。变量名一目了然：
- en: '[PRE1]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let''s create a custom schema having a structure similar to our already
    created data source, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个自定义模式，结构与我们已创建的数据源相似，如下所示：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s create a Spark session and import the `implicit._` that enables us to
    specify a DataFrame operation, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个Spark会话并导入`implicit._`，以便我们指定DataFrame操作，如下所示：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now let''s create the training set. We read the CSV file with Spark''s recommended
    format, `com.databricks.spark.csv`. We don''t need any explicit schema inference,
    making the infer Schema false, but instead, we need our own schema we just created
    previously. Then, we load the data file from our desired location, and finally,
    specify our data source so that our DataFrame looks exactly the same as we specified:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建训练集。我们使用Spark推荐的格式`com.databricks.spark.csv`读取CSV文件。我们不需要显式的模式推断，因此将推断模式设置为false，而是需要我们之前创建的自定义模式。接着，我们从所需位置加载数据文件，最后指定数据源，确保我们的DataFrame与我们指定的结构完全一致：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let''s see what the schema looks like:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看模式是什么样的：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![](img/489ed265-308b-4a22-82c2-127c473bd023.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/489ed265-308b-4a22-82c2-127c473bd023.png)'
- en: 'Excellent! It looks exactly the same as the data structure. Now let''s see
    some sample data using the `show()` method, as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！它看起来与数据结构完全相同。现在让我们使用`show()`方法查看一些示例数据，如下所示：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the following figure, column names are made shorter for visibility on the
    picture:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，列名已被缩短，以便在图中显示：
- en: '![](img/601a7523-4f1b-45d2-9481-d0f868bca26d.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/601a7523-4f1b-45d2-9481-d0f868bca26d.png)'
- en: 'We can also see related statistics of the training set using the `describe()`
    method from Spark:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用Spark的`describe()`方法查看训练集的相关统计数据：
- en: The `describe()` method is a Spark DataFrame's built-in method for statistical
    processing. It applies summary statistics calculations on all numeric columns.
    Finally, it returns the computed values as a single DataFrame.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe()`方法是Spark DataFrame的内置方法，用于统计处理。它对所有数值列应用汇总统计计算，最后将计算值作为单个DataFrame返回。'
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/fe307ce5-7904-41fd-b30b-63a24fee94e5.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe307ce5-7904-41fd-b30b-63a24fee94e5.png)'
- en: 'If this dataset can be fit into RAM, we can cache it for quick and repeated
    access using the `cache()` method from Spark:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个数据集可以装入内存，我们可以使用Spark的`cache()`方法将其缓存，以便快速和重复地访问：
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let''s see some useful properties, such as variable correlation with churn.
    For example, let''s see how the churn is related to the total number of international
    calls:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一些有用的属性，比如与流失（churn）的变量相关性。例如，看看流失与国际通话总数之间的关系：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s see how the churn is related to the total international call charges:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看流失与国际通话费用总额之间的关系：
- en: '[PRE10]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that we also need to have the test set prepared to evaluate the model,
    let''s prepare the same set, similar to the train set, as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们还需要准备测试集来评估模型，让我们准备与训练集相似的测试集，如下所示：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now let''s cache them for faster access for further manipulation:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将它们缓存，以便更快地进行进一步操作：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, let''s see some related properties of the training set to understand its
    suitableness for our purposes. At first, let''s create a temp view for persistence
    for this session. We can create a catalog as an interface that can be used to
    create, drop, alter, or query underlying databases, tables, functions, and many
    more:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看一些训练集的相关属性，以了解它是否适合我们的目的。首先，我们为当前会话创建一个临时视图以用于持久化。我们可以创建一个目录作为接口，用于创建、删除、修改或查询底层数据库、表、函数等：
- en: '[PRE13]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Grouping the data by the churn label and calculating the number of instances
    in each group demonstrates that there are around six times more false churn samples
    as true churn samples. Let''s verify this statement with the following line of
    code:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 按照流失标签对数据进行分组并计算每个组中的实例数量，显示出假流失样本大约是实际流失样本的六倍。让我们使用以下代码验证这一说法：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can also see the previous statement, verified using Apache Zeppelin (see
    more details on how to configure and getting started in [Chapter 8](3e09dbd3-a9bb-4451-97f1-1a961d28b4a0.xhtml),
    *Using Deep Belief Networks in Bank Marketing*), as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到前面的语句，使用 Apache Zeppelin 验证过的（有关如何配置和入门的更多细节，请参见[第 8 章](3e09dbd3-a9bb-4451-97f1-1a961d28b4a0.xhtml)，*在银行营销中使用深度信念网络*），如下所示：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/b933288d-c96b-4772-888f-b1930c1c0593.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b933288d-c96b-4772-888f-b1930c1c0593.png)'
- en: As we have already stated, in most cases the target is to retain the customers
    who are most likely to leave, as opposed to those who are likely to stay or are
    staying. This also signifies that we should prepare our training set such that
    it ensures that our ML model is sensitive to the true churn samples—that is, having
    churn label true.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所述，在大多数情况下，目标是保留那些最有可能流失的客户，而不是那些可能会留下或已经留下的客户。这也意味着我们应该准备我们的训练集，确保我们的 ML
    模型能够敏感地识别真正的流失样本——即，标记为流失（True）的样本。
- en: We can also observe that the preceding training set is highly unbalanced. Therefore,
    it would be feasible to put two sample types on the same footing using stratified
    sampling. The `sampleBy()` method can be used to do so when provided with fractions
    of each sample type to be returned.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以观察到，前面的训练集高度不平衡。因此，使用分层抽样将两种样本类型放在同等基础上是可行的。当提供每种样本类型的返回比例时，可以使用 `sampleBy()`
    方法来实现。
- en: 'Here, we''re keeping all instances of the `True` churn class, but downsampling
    the `False` churn class to a fraction of *388/2278*, which is about `0.1675`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们保留了所有 `True` 流失类的实例，但将 `False` 流失类下采样至 *388/2278*，约为 `0.1675`：
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This way, we are also mapping only `True` churn samples. Now, let''s create
    a new DataFrame for the training set containing only downsampled ones:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们也仅映射了 `True` 流失样本。现在，让我们为仅包含下采样样本的训练集创建一个新的 DataFrame：
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The third parameter is the seed used for the reproducibility purpose. Now let''s
    see:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个参数是用于可重复性目的的种子值。现在让我们来看一下：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now let''s see how the variables are related to each other. Let''s see how
    the day, night, evening, and international voice calls contribute to the `churn`
    class. Just execute the following line:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看变量之间的关系。让我们查看白天、夜晚、傍晚和国际语音通话如何影响 `churn` 类别。只需执行以下代码：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/5a0af826-28e0-463e-b042-50b3498f37b0.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5a0af826-28e0-463e-b042-50b3498f37b0.png)'
- en: 'On Apache Zeppelin, the preceding result can be seen as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Apache Zeppelin 上，可以看到如下的前置结果：
- en: '![](img/a764a0a5-e3c5-4867-b495-2d5952c8407d.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a764a0a5-e3c5-4867-b495-2d5952c8407d.png)'
- en: 'Now, let''s see how many minutes of day, night, evening, and international
    voice calls have contributed to the preceding total charge to the `churn` class.
    Just execute the following line:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看白天、夜晚、傍晚和国际语音通话分别对 `churn` 类别的前置总费用贡献了多少。只需执行以下代码：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/548bee1c-eb4e-40dc-b02c-e58b1729a683.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/548bee1c-eb4e-40dc-b02c-e58b1729a683.png)'
- en: 'On Apache Zeppelin, the preceding result can be seen as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Apache Zeppelin 上，可以看到如下的前置结果：
- en: '![](img/aa00e9d2-c314-4459-8e51-135748fb243d.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aa00e9d2-c314-4459-8e51-135748fb243d.png)'
- en: 'From the preceding two graphs and tables, it is clear that total day minutes
    and total day charge are a highly correlated feature in this training set, which
    is not beneficial for our ML model training. Therefore, it would be better to
    remove them altogether. Moreover, the following graph shows all possible correlations
    (plotted in PySpark, though):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的两张图表和表格可以清楚地看出，总白天通话分钟数和总白天费用是这个训练集中的高度相关特征，这对我们的 ML 模型训练并不有利。因此，最好将它们完全去除。此外，以下图表展示了所有可能的相关性（虽然是用
    PySpark 绘制的）：
- en: '![](img/7aea33d2-92d3-4c01-8b74-aba1b5cb883e.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7aea33d2-92d3-4c01-8b74-aba1b5cb883e.jpg)'
- en: 'Figure 5: Correlation matrix, including all the features'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：包含所有特征的相关矩阵
- en: 'Let''s drop one column of each pair of correlated fields, along with the **State**
    and **Area code** columns, too, since those will not be used either:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们丢弃每对相关字段中的一列，同时也丢弃**State**和**Area code**列，因为这些列也不会使用：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Excellent. Finally, we have our training DataFrame that can be used for better
    predictive modeling. Let''s take a look at some columns of the resulting DataFrame:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。最后，我们得到了可以用于更好的预测建模的训练 DataFrame。让我们看一下结果 DataFrame 的一些列：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/a90814fb-81d6-440c-bc2a-226ee24a1bd0.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a90814fb-81d6-440c-bc2a-226ee24a1bd0.png)'
- en: However, we are not done yet; the current DataFrame cannot be fed to the model
    as an estimator. As we described, the Spark ML API needs our data to be converted
    in a Spark DataFrame format, consisting of a label (in Double) and features (in
    Vector).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还没有完成；当前的DataFrame不能作为估算器输入给模型。如我们所描述的，Spark ML API要求我们的数据必须转换为Spark DataFrame格式，包含标签（Double类型）和特征（Vector类型）。
- en: Now, we need to create a pipeline to pass the data through and chain several
    transformers and estimators. The pipeline then works as a feature extractor. More
    specifically, we have prepared two `StringIndexer` transformers and a `VectorAssembler`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要创建一个管道来传递数据，并将多个变换器和估算器连接起来。这个管道随后作为特征提取器工作。更具体地说，我们已经准备好了两个`StringIndexer`变换器和一个`VectorAssembler`。
- en: '`StringIndexer` encodes a categorical column of labels to a column of label
    indices (that is, numerical). If the input column is numeric, we have to cast
    it into a string and index the string values. Other Spark pipeline components,
    such as Estimator or Transformer, make use of this string-indexed label. In order
    to do this, the input column of the component must be set to this string-indexed
    column name. In many cases, you can set the input column with `setInputCol`. Interested
    readers should refer to this [https://spark.apache.org/docs/latest/ml-features.html](https://spark.apache.org/docs/latest/ml-features.html) for
    more details.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`StringIndexer`将一个分类标签列编码为标签索引列（即数字）。如果输入列是数字类型，我们必须将其转换为字符串并对字符串值进行索引。其他Spark管道组件，如估算器或变换器，都会利用这个字符串索引标签。为了做到这一点，组件的输入列必须设置为这个字符串索引列的名称。在许多情况下，你可以使用`setInputCol`来设置输入列。有兴趣的读者可以参考这个[https://spark.apache.org/docs/latest/ml-features.html](https://spark.apache.org/docs/latest/ml-features.html)以获取更多详情。'
- en: The first `StringIndexer` converts the String categorical feature `international_plan`
    and labels into number indices. The second `StringIndexer` converts the categorical
    label (that is, `churn`) to numeric. This way, indexing categorical features enables
    decision trees and random forest-like classifiers to treat categorical features
    appropriately, hence improving performance.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`StringIndexer`将分类特征`international_plan`和标签转换为数字索引。第二个`StringIndexer`将分类标签（即`churn`）转换为数字。通过这种方式，索引化的分类特征使得决策树和随机森林等分类器能够适当处理分类特征，从而提高性能。
- en: 'Now, add the following lines of code, index labels, and metadata to the label
    column. Fit on the whole dataset to include all labels in the index:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，添加以下代码行，对标签列进行索引标签和元数据处理。在整个数据集上进行拟合，以确保所有标签都包括在索引中：
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we need to extract the most important features that contribute to the classification.
    Since we have dropped some columns already, the resulting column set consists
    of the following fields:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要提取对分类最有贡献的最重要特征。由于我们已经删除了一些列，结果列集包含以下字段：
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As we have already converted categorical labels into numeric using `StringIndexer`,
    the next task is to extract the features:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经使用`StringIndexer`将分类标签转换为数字，接下来的任务是提取特征：
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, let''s transform the features into feature vectors, which are vectors
    of numbers representing the value for each feature. In our case, we will use `VectorAssembler`.
    It takes all the `featureCols` and combines/transforms them into a single column
    called **features**:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将特征转换为特征向量，特征向量是表示每个特征值的数字向量。在我们的例子中，我们将使用`VectorAssembler`。它将所有的`featureCols`合并/转换成一个名为**features**的单列：
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now that we have the real training set consisting of labels and feature vectors
    ready, the next task is to create an estimator—the third element of a pipeline.
    We start with a very simple but powerful Logistic Regressionclassifier.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了包含标签和特征向量的真实训练集，接下来的任务是创建一个估算器——管道的第三个元素。我们从一个非常简单但强大的逻辑回归分类器开始。
- en: LR for churn prediction
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于流失预测的LR
- en: 'LR is one of the most widely used classifiers to predict a binary response.
    It is a linear ML method, as described in [Chapter 1](4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml),
    *Analyzing Insurance Severity Claim*. The `loss` function is the formulation given
    by the logistic loss:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: LR是预测二元响应最常用的分类器之一。它是一种线性机器学习方法，正如在[第1章](4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml)中所描述的，*分析保险严重性理赔*。`loss`函数是由逻辑损失给出的公式：
- en: '![](img/58d1cdb4-4e46-48ce-8051-bf44e2fbf31c.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/58d1cdb4-4e46-48ce-8051-bf44e2fbf31c.png)'
- en: 'For the LR model, the `loss` function is the logistic loss. For a binary classification
    problem, the algorithm outputs a binary LR model such that, for a given new data
    point, denoted by *x*, the model makes predictions by applying the logistic function:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LR 模型，`loss` 函数是逻辑损失函数。对于二分类问题，该算法输出一个二元 LR 模型，对于给定的新数据点，记为 *x*，该模型通过应用逻辑函数进行预测：
- en: '![](img/a34028da-b03e-4cb1-a247-c08dd7e7d7ce.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a34028da-b03e-4cb1-a247-c08dd7e7d7ce.png)'
- en: In the preceding equation, *z = W^TX* and if *f(W^TX)>0.5*, the outcome is positive;
    otherwise, it is negative.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述方程中，*z = W^TX*，如果 *f(W^TX)>0.5*，则结果为正；否则为负。
- en: Note that the raw output of the LR model, *f(z)*, has a probabilistic interpretation.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，LR 模型的原始输出 *f(z)* 具有概率解释。
- en: Note that compared to linear regression, logistic regression provides you with
    a higher classification accuracy. Moreover, it is a flexible way to regularize
    a model for custom adjustment, and overall, the model responses are measures of
    probability.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与线性回归相比，逻辑回归为你提供了更高的分类精度。此外，它是一种灵活的方式来对模型进行正则化，以进行自定义调整，总体而言，模型的响应是概率的度量。
- en: 'Most importantly, whereas linear regression can predict only continuous values,
    linear regression can still be generalized enough to make it predict discrete
    values:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，尽管线性回归只能预测连续值，线性回归仍然足够通用，可以预测离散值：
- en: '[PRE27]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Now that we already know linear regression's working principle, let's start
    using the Spark-based implementation of linear regression. Let's start by importing
    the required packages and libraries.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道线性回归的工作原理，让我们开始使用基于 Spark 的线性回归实现。首先，让我们导入所需的包和库。
- en: 'Now, let''s create a Spark session and import implicit:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个 Spark 会话并导入隐式转换：
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We now need to define some hyperparameters to train an linear regression-based
    pipeline:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要定义一些超参数来训练基于线性回归的管道：
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `RegParam` is a scalar that helps adjust the strength of the constraints:
    a small value implies a soft margin, so naturally, a large value implies a hard
    margin, and being an infinity is the hardest margin.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`RegParam` 是一个标量，用于调整约束的强度：较小的值表示软边界，因此，自然地，较大的值表示硬边界，而无限大则是最硬的边界。'
- en: By default, LR performs an L2 regularization with the regularization parameter
    set to 1.0\. The same model performs an L1 regularized variant of LR with the
    regularization parameter (that is, `RegParam`) set to 0.10\. Elastic Net is a
    combination of L1 and L2 regularization.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，LR 执行 L2 正则化，正则化参数设置为 1.0。相同的模型执行 L1 正则化变种的 LR，正则化参数（即 `RegParam`）设置为
    0.10。弹性网络是 L1 和 L2 正则化的组合。
- en: 'On the other hand, the `Tol` parameter is used for the convergence tolerance
    for iterative algorithms such as logistic regression or linear SVM. Now, once
    we have the hyperparameters defined and initialized, the next task is to instantiate
    an linear regression estimator, as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`Tol` 参数用于迭代算法（如逻辑回归或线性支持向量机）的收敛容忍度。现在，一旦我们定义并初始化了超参数，接下来的任务是实例化一个线性回归估算器，如下所示：
- en: '[PRE30]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now that we have three transformers and an estimator ready, the next task is
    to chain in a single pipeline—that is, each of them acts as a stage:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经有了三个变换器和一个估算器，接下来的任务是将它们串联成一个单一的管道——即，它们每一个都作为一个阶段：
- en: '[PRE31]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In order to perform such a grid search over the hyperparameter space, we need
    to define it first. Here, the functional programming properties of Scala are quite
    handy, because we just add function pointers and the respective parameters to
    be evaluated to the parameter grid, where you set up the parameters to test, and
    a cross-validation evaluator, to construct a model selection workflow. This searches
    through linear regression''s max iteration, regularization param, tolerance, and
    Elastic Net for the best model:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在超参数空间上执行这样的网格搜索，我们需要先定义它。在这里，Scala 的函数式编程特性非常方便，因为我们只需将函数指针和要评估的相应参数添加到参数网格中，在该网格中你设置要测试的参数，并使用交叉验证评估器来构建一个模型选择工作流。这将搜索线性回归的最大迭代次数、正则化参数、容忍度和弹性网络，以找到最佳模型：
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that the hyperparameters form an n-dimensional space where *n* is the number
    of hyperparameters. Every point in this space is one particular hyperparameter
    configuration, which is a hyperparameter vector. Of course, we can't explore every
    point in this space, so what we basically do is a grid search over a (hopefully
    evenly distributed) subset in that space.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，超参数形成了一个n维空间，其中*n*是超参数的数量。这个空间中的每个点是一个特定的超参数配置，即超参数向量。当然，我们无法探索该空间中的每个点，因此我们基本上做的是在该空间中对（希望均匀分布的）子集进行网格搜索。
- en: 'We then need to define a `BinaryClassificationEvaluator` evaluator, since this
    is a binary classification problem. Using this evaluator, the model will be evaluated
    according to a precision metric by comparing the test label column with the test
    prediction column. The default metrics are an area under the precision-recall
    curve and an area under the **receiver operating characteristic** (**ROC**) curve:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要定义一个`BinaryClassificationEvaluator`评估器，因为这是一个二分类问题。使用该评估器，模型将通过比较测试标签列与测试预测列，根据精度指标进行评估。默认的度量标准是精度-召回曲线下面积和**接收者操作特征**（**ROC**）曲线下面积：
- en: '[PRE33]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We use a `CrossValidator` for best model selection. The `CrossValidator` uses
    the Estimator Pipeline, the Parameter Grid, and the Classification Evaluator.
    The `CrossValidator` uses the `ParamGridBuilder` to iterate through the max iteration,
    regression param, and tolerance and Elastic Net parameters of linear regression,
    and then evaluates the models, repeating 10 times per parameter value for reliable
    results—that is, 10-fold cross-validation:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`CrossValidator`进行最佳模型选择。`CrossValidator`使用估算器管道、参数网格和分类评估器。`CrossValidator`使用`ParamGridBuilder`来遍历线性回归的最大迭代次数、回归参数、容差和弹性网参数，然后评估模型，对于每个参数值重复10次以获得可靠结果——即进行10折交叉验证：
- en: '[PRE34]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The preceding code is meant to perform cross-validation. The validator itself
    uses the `BinaryClassificationEvaluator` estimator for evaluating the training
    in the progressive grid space on each fold and makes sure that there's no overfitting.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码旨在执行交叉验证。验证器本身使用`BinaryClassificationEvaluator`评估器来评估每一折中的训练，确保没有过拟合发生。
- en: 'Although there is so much stuff going on behind the scenes, the interface to
    our `CrossValidator` object stays slim and well-known, as `CrossValidator` also
    extends from Estimator and supports the fit method. This means that, after calling
    fit, the complete predefined pipeline, including all feature preprocessing and
    the LR classifier, is executed multiple times—each time with a different hyperparameter
    vector:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管后台有很多复杂操作，`CrossValidator`对象的接口依然简洁且熟悉，因为`CrossValidator`也继承自估算器，并支持fit方法。这意味着，在调用fit后，完整的预定义管道，包括所有特征预处理和LR分类器，将被多次执行——每次使用不同的超参数向量：
- en: '[PRE35]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now it''s time to evaluate the predictive power of the LR model we created
    using the test dataset, which has not been used for any training or cross-validation
    so far—that is, unseen data to the model. As a first step, we need to transform
    the test set to the model pipeline, which will map the features according to the
    same mechanism we described in the preceding feature engineering step:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候使用测试数据集评估我们创建的LR模型的预测能力了，该测试数据集此前未用于任何训练或交叉验证——也就是说，对模型来说是未见过的数据。第一步，我们需要将测试集转换为模型管道，这将根据我们在前述特征工程步骤中描述的相同机制映射特征：
- en: '[PRE36]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](img/b2101733-4336-453c-9d7e-2bf2fb7c7f1a.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2101733-4336-453c-9d7e-2bf2fb7c7f1a.png)'
- en: The prediction probabilities can also be very useful in ranking customers according
    to their likeliness to imperfection. This way, a limited number of resources can
    be utilized in a telecommunication business for withholding but can be focused
    to the most valuable customers.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 预测概率在根据客户的缺陷可能性进行排名时也非常有用。通过这种方式，电信业务可以利用有限的资源进行保留，并集中于最有价值的客户。
- en: 'However, seeing the previous prediction DataFrame, it is really difficult to
    guess the classification accuracy. In the second step, the evaluator evaluates
    itself using `BinaryClassificationEvaluator`, as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，看到之前的预测数据框，实际上很难猜测分类准确率。在第二步中，评估器通过`BinaryClassificationEvaluator`自行进行评估，如下所示：
- en: '[PRE37]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: So, we get about 77% of classification accuracy from our binary classification
    model. Now using the accuracy for the binary classifier does not make enough sense.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们的二分类模型的分类准确率大约为77%。现在，使用准确率来评估二分类器并没有太大意义。
- en: 'Hence, researchers often recommend other performance metrics, such as area
    under the precision-recall curve and area under the ROC curve. However, for this
    we need to construct an RDD containing the raw scores on the test set:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，研究人员经常推荐其他性能指标，如精确度-召回率曲线下的面积和 ROC 曲线下的面积。然而，为此我们需要构建一个包含测试集原始得分的 RDD：
- en: '[PRE38]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, the preceding RDD can be used to compute the two previously-mentioned
    performance metrics:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以使用前述 RDD 来计算之前提到的两个性能指标：
- en: '[PRE39]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'In this case, the evaluation returns 77% accuracy, but only 58% precision.
    In the following, we calculate some more metrics; for example, false and true
    positive and negative predictions are also useful to evaluate the model''s performance:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，评估结果为 77% 的准确率，但只有 58% 的精确度。接下来，我们计算一些其他的性能指标；例如，假阳性、真阳性和假阴性预测对评估模型的性能也非常有用：
- en: '**True positive**: How often the model correctly predicted subscription canceling'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性**：模型正确预测订阅取消的频率'
- en: '**False positive**: How often the model incorrectly predicted subscription
    canceling'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性**：模型错误预测订阅取消的频率'
- en: '**True negative**: How often the model correctly predicted no canceling at
    all'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性**：模型正确预测没有取消的频率'
- en: '**False negative**: How often the model incorrectly predicted no canceling'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性**：模型错误预测没有取消的频率'
- en: '[PRE40]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](img/16371f9d-cf39-4676-98fc-4407d4fba749.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16371f9d-cf39-4676-98fc-4407d4fba749.png)'
- en: Yet, we have not received good accuracy, so let's continue trying other classifiers,
    such as SMV. This time, we will use the linear SVM implementation from the Apache
    Spark ML package.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还没有得到良好的准确性，因此让我们继续尝试其他分类器，例如 SMV。这次，我们将使用来自 Apache Spark ML 包的线性 SVM 实现。
- en: SVM for churn prediction
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVM 用于流失预测
- en: 'SVM is also used widely for large-scale classification (that is, binary as
    well as multinomial) tasks. Besides, it is also a linear ML method, as described
    in [Chapter 1](4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml), *Analyzing Insurance
    Severity Claim*. The linear SVM algorithm outputs an SVM model, where the loss
    function used by SVM can be defined using the hinge loss, as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 也广泛用于大规模分类任务（即二分类以及多项分类）。此外，它也是一种线性机器学习方法，如[第 1 章](4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml)《分析保险赔偿严重性》中所描述。线性
    SVM 算法输出一个 SVM 模型，其中 SVM 使用的损失函数可以通过铰链损失来定义，如下所示：
- en: '*L(**w**;**x**,y):=max{0,1−y**w**^T**x**}*'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*L(**w**;**x**,y):=max{0,1−y**w**^T**x**}*'
- en: The linear SVMs in Spark are trained with an L2 regularization, by default.
    However, it also supports L1 regularization, by which the problem itself becomes
    a linear program.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 中的线性 SVM 默认使用 L2 正则化进行训练。然而，它也支持 L1 正则化，通过这种方式，问题本身变成了一个线性规划问题。
- en: Now, suppose we have a set of new data points *x*; the model makes predictions
    based on the value of ***w**^T**x***. By default, if ***w****^T****x**≥0*, then
    the outcome is positive, and negative otherwise.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们有一组新的数据点 *x*；模型根据 ***w**^T**x*** 的值做出预测。默认情况下，如果 ***w****^T****x**≥0*，则结果为正，否则为负。
- en: 'Now that we already know the SVMs working principle, let''s start using the
    Spark-based implementation of SVM. Let''s start by importing the required packages
    and libraries:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 SVM 的工作原理，让我们开始使用基于 Spark 的 SVM 实现。我们从导入所需的包和库开始：
- en: '[PRE41]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now let''s create a Spark session and import implicit:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个 Spark 会话并导入隐式转换：
- en: '[PRE42]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We now need to define some hyperparameters to train an LR-based pipeline:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要定义一些超参数来训练基于 LR 的管道：
- en: '[PRE43]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, once we have the hyperparameters defined and initialized, the next task
    is to instantiate an LR estimator, as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦我们定义并初始化了超参数，下一步是实例化一个 LR 估计器，如下所示：
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now that we have three transformers and an estimator ready, the next task is
    to chain in a single pipeline—that is, each of them acts as a stage:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好三个转换器和一个估计器，下一步是将它们串联成一个管道——也就是说，每个都充当一个阶段：
- en: '[PRE45]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let''s define the `paramGrid` to perform such a grid search over the hyperparameter
    space. This searches through SVM''s max iteration, regularization param, tolerance,
    and Elastic Net for the best model:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义 `paramGrid`，以便在超参数空间上执行网格搜索。这个搜索将遍历 SVM 的最大迭代次数、正则化参数、容差和弹性网，以寻找最佳模型：
- en: '[PRE46]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s define a `BinaryClassificationEvaluator` evaluator to evaluate the model:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个 `BinaryClassificationEvaluator` 评估器来评估模型：
- en: '[PRE47]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We use a `CrossValidator` for performing 10-fold cross-validation for best
    model selection:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `CrossValidator` 执行 10 次交叉验证，以选择最佳模型：
- en: '[PRE48]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let''s now call the `fit` method so that the complete predefined pipeline,
    including all feature preprocessing and the LR classifier, is executed multiple
    times—each time with a different hyperparameter vector:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们调用`fit`方法，以便完整的预定义流水线，包括所有特征预处理和LR分类器，将被执行多次——每次使用不同的超参数向量：
- en: '[PRE49]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now it''s time to evaluate the predictive power of the SVM model on the test
    dataset. As a first step, we need to transform the test set with the model pipeline,
    which will map the features according to the same mechanism we described in the
    preceding feature engineering step:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候评估SVM模型在测试数据集上的预测能力了。第一步，我们需要使用模型流水线转换测试集，这将根据我们在前面特征工程步骤中描述的机制来映射特征：
- en: '[PRE50]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![](img/ad85dba1-6541-44bf-bf6c-560e0d867804.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad85dba1-6541-44bf-bf6c-560e0d867804.png)'
- en: 'However, seeing the previous prediction DataFrame, it is really difficult to
    guess the classification accuracy. In the second step, the evaluator evaluates
    itself using `BinaryClassificationEvaluator`, as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从之前的预测数据框中，确实很难猜测分类准确率。在第二步中，评估器使用`BinaryClassificationEvaluator`进行自我评估，如下所示：
- en: '[PRE51]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: So we get about 75% of classification accuracy from our binary classification
    model. Now, using the accuracy for the binary classifier does not make enough
    sense.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们从我们的二分类模型中得到了大约75%的分类准确率。现在，单单使用二分类器的准确率并没有太大意义。
- en: 'Hence, researchers often recommend other performance metrics, such as area
    under the precision-recall curve and area under the ROC curve. However, for this
    we need to construct an RDD containing the raw scores on the test set:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，研究人员通常推荐其他性能指标，比如精确度-召回曲线下面积和ROC曲线下面积。然而，为此我们需要构建一个包含测试集原始分数的RDD：
- en: '[PRE52]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now the preceding RDD can be used to compute the two previously-mentioned performance
    metrics:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以使用前面的RDD来计算两个之前提到的性能指标：
- en: '[PRE53]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'In this case, the evaluation returns 75% accuracy but only 55% precision. In
    the following, we again calculate some more metrics; for example, false and true
    positive and negative predictions are also useful to evaluate the model''s performance:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，评估返回了75%的准确率，但仅有55%的精确度。接下来，我们再次计算一些其他指标；例如，假阳性、真阳性、假阴性和真阴性预测也有助于评估模型的性能：
- en: '[PRE54]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '![](img/6d30c141-8fcc-4bda-8273-b5fc3c4f9a3d.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d30c141-8fcc-4bda-8273-b5fc3c4f9a3d.png)'
- en: Yet, we have not received good accuracy using SVM. Moreover, there is no option
    to select the most suitable features, which would help us train our model with
    the most appropriate features. This time , we will again use a more robust classifier,
    such as the **decision trees** (**DTs**) implementation from the Apache Spark
    ML package.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们使用SVM时并没有获得好的准确率。而且，无法选择最合适的特征，这会帮助我们用最合适的特征训练模型。这一次，我们将再次使用一个更强大的分类器，比如Apache
    Spark ML包中的**决策树**（**DTs**）实现。
- en: DTs for churn prediction
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于流失预测的决策树
- en: DTs are commonly considered a supervised learning technique used for solving
    classification and regression tasks.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通常被认为是一种监督学习技术，用于解决分类和回归任务。
- en: More technically, each branch in a DT represents a possible decision, occurrence,
    or reaction, in terms of statistical probability. Compared to naive Bayes, DTs
    are a far more robust classification technique. The reason is that at first, the
    DT splits the features into training and test sets. Then, it produces a good generalization
    to infer the predicted labels or classes. Most interestingly, a DT algorithm can
    handle both binary and multiclass classification problems.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 更技术性地讲，决策树中的每个分支代表一个可能的决策、事件或反应，基于统计概率。与朴素贝叶斯相比，决策树是一种更强健的分类技术。原因在于，首先，决策树将特征分为训练集和测试集。然后，它通过良好的泛化能力来推断预测标签或类别。最有趣的是，决策树算法可以处理二分类和多分类问题。
- en: 'For instance, in the following example figure, DTs learn from the admission
    data to approximate a sine curve with a set of **if...else** decision rules. The
    dataset contains the record of each student who applied for admission, say, to
    an American university. Each record contains the graduate record exam score, CGPA
    score, and the rank of the column. Now we will have to predict who is competent
    based on these three features (variables). DTs can be used to solve this kind
    of problem after training the DT model and pruning unwanted branches of the tree.
    In general, a deeper tree signifies more complex decision rules and a better-fitted
    model:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在下面的示例图中，DT从入学数据中学习，通过一组**if...else**决策规则来近似正弦曲线。数据集包含每个申请入学学生的记录，例如，申请进入美国大学的学生。每条记录包含研究生入学考试成绩、CGPA成绩和排名。现在，我们需要根据这三个特征（变量）预测谁是合格的。DTs可以在训练DT模型并剪枝不需要的树枝后，用于解决这种问题。通常来说，更深的树表示更复杂的决策规则和更好的拟合模型：
- en: '![](img/fa48fbd9-4b1f-4e97-abd1-fe85ca6be6d4.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fa48fbd9-4b1f-4e97-abd1-fe85ca6be6d4.png)'
- en: 'Figure 6: Decision tree for university admission data'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：大学入学数据的决策树
- en: 'Therefore, the deeper the tree, the more complex the decision rules and the
    more fitted the model is. Now let''s see some pros and cons of DTs:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，树越深，决策规则越复杂，模型拟合度越高。现在，让我们看一下DT的优缺点：
- en: '|  | **Pros** | **Cons** | **Better at** |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | **优点** | **缺点** | **更擅长** |'
- en: '| **Decision trees (DTs)** | -Simple to implement, train, and interpret-Trees
    can be visualized-Requires little data preparation-Less model building and prediction
    time-Can handle both numeric and categorical data-Possibility of validating the
    model using the statistical tests-Robust against noise and missing values-High
    accuracy | -Interpretation is hard with large and complex trees-Duplication may
    occur within the same sub-tree-Possible issues with diagonal decision boundaries-Decision
    tree learners can create over-complex trees that do not generalize the data well-Sometimes
    DTs can be unstable because of small variants in the data-Learning the DT itself
    is an NP-complete problem-DT learners create biased trees if some classes dominate
    | -Targeting highly accurate classification-Medical diagnosis and prognosis-Credit
    risk analytics |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| **决策树 (DTs)** | -简单实现、训练和解释-可以可视化树-数据准备要求少-模型构建和预测时间较短-可以处理数值和分类数据-通过统计测试验证模型的可能性-对噪声和缺失值具有鲁棒性-高精度
    | -大而复杂的树难以解释-同一子树中可能出现重复-可能存在对角决策边界的问题-决策树学习者可能会创建过于复杂的树，无法很好地泛化数据-有时DTs可能因为数据中的微小变化而不稳定-学习DT本身是一个NP完全问题-如果某些类占主导地位，DT学习者会创建偏倚的树
    | -目标是实现高准确度的分类-医学诊断和预后-信用风险分析 |'
- en: 'Now, that we already know the working principle of DTs, let''s start using
    the Spark-based implementation of DTs. Let''s start by importing required packages
    and libraries:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经了解了DT的工作原理，接下来让我们开始使用基于Spark的DT实现。首先，导入所需的包和库：
- en: '[PRE55]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now let''s create a Spark session and import implicit:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个Spark会话并导入隐式转换：
- en: '[PRE56]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now, once we have the hyperparameters defined and initialized, the next task
    is to instantiate a `DecisionTreeClassifier` estimator, as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦我们定义并初始化了超参数，下一步是实例化一个`DecisionTreeClassifier`估算器，如下所示：
- en: '[PRE57]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now that we have three transformers and an estimator ready, the next task is
    to chain in a single pipeline—that is, each of them acts as a stage:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有三个转换器和一个估算器准备好，接下来的任务是将它们串联成一个单一的管道——即它们每个都作为一个阶段：
- en: '[PRE58]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Let''s define the paramgrid to perform such a grid search over the hyperparameter
    space. This search is through DT''s impurity, max bins, and max depth for the
    best model. Maximum depth of the tree: depth 0 means 1 leaf node; depth 1 means
    1 internal node + 2 leaf nodes.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义参数网格，在超参数空间上执行这样的网格搜索。这个搜索通过DT的杂质、最大分箱数和最大深度来寻找最佳模型。树的最大深度：深度0表示1个叶节点；深度1表示1个内部节点+2个叶节点。
- en: 'On the other hand, the maximum number of bins is used for separate continuous
    features and for choosing how to split on features at each node. More bins give
    higher granularity. In short, we search through decision tree''s `maxDepth` and
    `maxBins` parameters for the best model:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，最大分箱数用于分离连续特征并选择每个节点上如何分裂特征。更多的分箱提供更高的粒度。简而言之，我们通过决策树的`maxDepth`和`maxBins`参数来搜索最佳模型：
- en: '[PRE59]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: In the preceding code segment, we're creating a progressive paramgrid through
    sequence format. That means we are creating the grid space with different hyperparameter
    combinations. This will help us provide the best model that consists of the most
    optimal hyperparameters.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码段中，我们通过序列格式创建了一个逐步的参数网格。这意味着我们正在创建一个包含不同超参数组合的网格空间。这将帮助我们提供由最优超参数组成的最佳模型。
- en: 'Let''s define a `BinaryClassificationEvaluator` evaluator to evaluate the model:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个`BinaryClassificationEvaluator`评估器来评估模型：
- en: '[PRE60]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We use a `CrossValidator` for performing 10-fold cross-validation for best
    model selection:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`CrossValidator`进行10折交叉验证，以选择最佳模型：
- en: '[PRE61]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Let''s now call the `fit` method so that the complete predefined pipeline,
    including all feature preprocessing and the DT classifier, is executed multiple
    times—each time with a different hyperparameter vector:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们调用`fit`方法，这样完整的预定义管道，包括所有特征预处理和决策树分类器，将被多次执行——每次使用不同的超参数向量：
- en: '[PRE62]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now it''s time to evaluate the predictive power of the DT model on the test
    dataset. As a first step, we need to transform the test set with the model pipeline,
    which will map the features according to the same mechanism we described in the
    previous feature engineering step:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候评估决策树模型在测试数据集上的预测能力了。第一步，我们需要使用模型管道转换测试集，这将按照我们在前面的特征工程步骤中描述的相同机制映射特征：
- en: '[PRE63]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '![](img/7ecce996-69f6-4412-b021-7588816d89d6.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7ecce996-69f6-4412-b021-7588816d89d6.png)'
- en: 'However, seeing the preceding prediction DataFrame, it is really difficult
    to guess the classification accuracy. In the second step, in the evaluation is
    the evaluate itself using `BinaryClassificationEvaluator`, as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，看到前面的预测DataFrame，真的很难猜测分类准确率。在第二步中，评估是通过使用`BinaryClassificationEvaluator`进行评估，如下所示：
- en: '[PRE64]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'So, we get about 87% of classification accuracy from our binary classification
    model. Now, similar to SVM and LR, we will observe the area under the precision-recall
    curve and the area under the ROC curve based on the following RDD containing the
    raw scores on the test set:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们从我们的二元分类模型中得到了大约87%的分类准确率。现在，类似于SVM和LR，我们将基于以下包含测试集原始分数的RDD，观察精确度-召回曲线下的面积和ROC曲线下的面积：
- en: '[PRE65]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now the preceding RDD can be used to compute the two previously-mentioned performance
    metrics:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，前面的RDD可以用于计算之前提到的两个性能指标：
- en: '[PRE66]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'In this case, the evaluation returns 87% accuracy but only 73% precision, which
    is much better than that of SVM and LR. In the following, we again calculate some
    more metrics; for example, false and true positive and negative predictions are
    also useful to evaluate the model''s performance:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，评估结果返回87%的准确率，但只有73%的精确度，这比SVM和LR要好得多。接下来，我们将再次计算一些其他指标；例如，假阳性和真阳性及假阴性预测也有助于评估模型的性能：
- en: '[PRE67]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![](img/e919b818-4fc3-4bd9-8bde-c1e4974f3518.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e919b818-4fc3-4bd9-8bde-c1e4974f3518.png)'
- en: 'Fantastic; we achieved 87% accuracy, but for what factors? Well, it can be
    debugged to get the decision tree constructed during the classification. But first,
    let''s see at what level we achieved the best model after the cross-validation:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了；我们达到了87%的准确率，但是什么因素导致的呢？嗯，可以通过调试来获得分类过程中构建的决策树。但首先，让我们看看在交叉验证后我们在什么层次上达到了最佳模型：
- en: '[PRE68]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The Best Model and Parameters:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳模型和参数：
- en: '[PRE69]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'That means we achieved the best tree model at depth 5 having 53 nodes. Now
    let''s extract those moves (that is, decisions) taken during tree construction
    by showing the tree. This tree helps us to find the most valuable features in
    our dataset:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们在深度为5，节点数为53的决策树模型上达到了最佳效果。现在，让我们通过展示树来提取树构建过程中做出的决策。这棵树帮助我们找出数据集中最有价值的特征：
- en: '[PRE70]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Learned classification tree model:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 学到的分类树模型：
- en: '[PRE71]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'In the preceding output, the `toDebugString()` function prints the tree''s
    decision nodes and the final prediction comes out at the end leaves. It is also
    clearly seen that features 11 and 3 are used for decision making; they are the
    two most important reasons why a customer is likely to churn. But what are those
    two features? Let''s see them:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，`toDebugString()`函数打印了决策树的决策节点，最终预测结果出现在叶子节点上。我们也可以清楚地看到特征11和3被用来做决策；它们是客户可能流失的两个最重要因素。那么这两个特征是什么呢？我们来看一下：
- en: '[PRE72]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: So the customer service calls and total day minutes are selected by the decision
    trees, since it provides an automated mechanism for determining the most important
    features.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，客户服务电话和总通话时长被决策树选中，因为它提供了一种自动化机制来确定最重要的特征。
- en: Wait! We are not finished yet. Last but not least, we will use an ensemble technique,
    RF, which is considered a more robust classifier than DTs. Again, let's use the
    Random Forest implementation from the Apache Spark ML package.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 等等！我们还没完成。最后但同样重要的是，我们将使用一种集成技术——随机森林（RF），它被认为比决策树（DTs）更强大的分类器。同样，让我们使用Apache
    Spark ML包中的随机森林实现。
- en: Random Forest for churn prediction
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林用于流失预测
- en: As described in [Chapter 1](4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml), *Analyzing
    Insurance Severity Claim*, Random Forest is an ensemble technique that takes a
    subset of observations and a subset of variables to build decision trees—that
    is, an ensemble of DTs. More technically, it builds several decision trees and
    integrates them together to get a more accurate and stable prediction.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在[第1章](4b0be2d2-f313-471f-83fe-830931fc8af9.xhtml)中所述，*分析保险严重性索赔*，随机森林是一种集成技术，它通过构建决策树集成来进行预测——即，多个决策树的集成。更技术地讲，它构建多个决策树，并将它们集成在一起，以获得更准确和更稳定的预测。
- en: '![](img/5cc25b68-25d8-404b-8419-01ce7bd71ce4.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5cc25b68-25d8-404b-8419-01ce7bd71ce4.png)'
- en: 'Figure 7: Random forest and its assembling technique explained'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：随机森林及其集成技术的解释
- en: 'This is a direct consequence, since by maximum voting from a panel of independent
    juries, we get the final prediction better than the best jury (see the preceding
    figure). Now that we already know the working principle of RF, let''s start using
    the Spark-based implementation of RF. Let''s start by importing the required packages
    and libraries:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个直接的结果，因为通过独立评审团的最大投票，我们得到了比最佳评审团更好的最终预测（见前图）。现在我们已经知道了随机森林（RF）的工作原理，让我们开始使用基于Spark的RF实现。首先，导入所需的包和库：
- en: '[PRE73]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Now let''s create Spark session and import implicit:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建Spark会话并导入隐式库：
- en: '[PRE74]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Now, once we have the hyperparameters defined and initialized, the next task
    is to instantiate a `DecisionTreeClassifier` estimator, as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦我们定义并初始化了超参数，接下来的任务是实例化一个`DecisionTreeClassifier`估计器，如下所示：
- en: '[PRE75]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Now that we have three transformers and an estimator ready, the next task is
    to chain in a single pipeline—that is, each of them acts as a stage:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了三个变换器和一个估计器，接下来的任务是将它们串联成一个单一的管道——也就是说，每个变换器作为一个阶段：
- en: '[PRE76]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Let''s define the paramgrid to perform such a grid search over the hyperparameter
    space:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义paramgrid，以便在超参数空间上执行网格搜索：
- en: '[PRE77]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let''s define a `BinaryClassificationEvaluator` evaluator to evaluate the model:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个`BinaryClassificationEvaluator`评估器来评估模型：
- en: '[PRE78]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We use a `CrossValidator` for performing 10-fold cross-validation for best
    model selection:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`CrossValidator`执行10折交叉验证，以选择最佳模型：
- en: '[PRE79]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Let''s now call the `fit` method so that the complete, predefined pipeline,
    including all feature preprocessing and the DT classifier, is executed multiple
    times—each time with a different hyperparameter vector:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们调用`fit`方法，以便执行完整的预定义管道，其中包括所有的特征预处理和决策树分类器，每次都会用不同的超参数向量执行：
- en: '[PRE80]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now it''s time to evaluate the predictive power of the DT model on the test
    dataset. As a first step, we need to transform the test set to the model pipeline,
    which will map the features according to the same mechanism we described in the
    previous feature engineering step:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候评估决策树模型在测试数据集上的预测能力了。第一步，我们需要将测试集转换为模型管道，这将按照我们在之前的特征工程步骤中描述的相同机制映射特征：
- en: '[PRE81]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '![](img/3c3565fc-44f6-4bf0-a838-72fae88c09e3.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c3565fc-44f6-4bf0-a838-72fae88c09e3.png)'
- en: 'However, seeing the preceding prediction DataFrame, it is really difficult
    to guess the classification accuracy. In the second step, in the evaluation is
    the evaluate itself using `BinaryClassificationEvaluator`, as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通过查看前述的预测数据框，确实很难猜测分类准确性。在第二步中，评估是通过使用`BinaryClassificationEvaluator`来进行的，如下所示：
- en: '[PRE82]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'So, we get about 87% of classification accuracy from our binary classification
    model. Now, similar to SVM and LR, we will observe the area under the precision-recall
    curve and the area under the ROC curve based on the following RDD containing the
    raw scores on the test set:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的二分类模型得到了约87%的分类准确率。现在，类似于SVM和LR，我们将根据以下包含测试集原始分数的RDD，观察精度-召回曲线下的面积以及ROC曲线下的面积：
- en: '[PRE83]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now the preceding RDD can be used to compute the two previously-mentioned performance
    metrics:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，前述的RDD可以用来计算之前提到的两个性能指标：
- en: '[PRE84]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'In this case, the evaluation returns 87% accuracy but only 73% precision, which
    is much better than that of SVM and LR. In the following, we again calculate some
    more metrics; for example, false and true positive and negative predictions are
    also useful to evaluate the model''s performance:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，评估返回了87%的准确性，但仅有73%的精度，这比SVM和LR要好得多。接下来，我们将再次计算一些更多的指标；例如，假阳性和真阳性、假阴性和真阴性预测也有助于评估模型的性能：
- en: '[PRE85]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'We will get the following result:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到以下结果：
- en: '![](img/9c1a146a-2390-4b0e-b71e-af399d8e61b3.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c1a146a-2390-4b0e-b71e-af399d8e61b3.png)'
- en: Fantastic; we achieved 91% accuracy, but for what factors? Well, similar to
    DT, Random Forest can be debugged to get the decision tree that was constructed
    during the classification. For the tree to be printed and the most important features
    selected, try the last few lines of code in the DT, and you're done.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了；我们达到了91%的准确率，但是什么因素导致的呢？嗯，类似于决策树，随机森林也可以调试，获取分类过程中构建的决策树。为了打印树并选择最重要的特征，尝试DT的最后几行代码，您就完成了。
- en: 'Can you now guess how many different models were trained? Well, we have 10-folds
    on CrossValidation and five-dimensional hyperparameter space cardinalities between
    2 and 7\. Now let''s do some simple math: 10 * 7 * 5 * 2 * 3 * 6 = 12600 models!'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你能猜到训练了多少个不同的模型吗？嗯，我们在交叉验证上有10折，超参数空间的基数为2到7的五个维度。现在来做一些简单的数学计算：10 * 7 *
    5 * 2 * 3 * 6 = 12600个模型！
- en: Note that we still make the hyperparameter space confined, with `numTrees`,
    `maxBins`, and `maxDepth` limited to 7\. Also, remember that bigger trees will
    most likely perform better. Therefore, feel free to play around with this code
    and add features, and also use a bigger hyperparameter space, say, bigger trees.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们仍然将超参数空间限制在`numTrees`、`maxBins`和`maxDepth`的范围内，最大为7。此外，请记住，较大的树通常会表现得更好。因此，欢迎在此代码上进行尝试，添加更多特性，并使用更大的超参数空间，例如，更大的树。
- en: Selecting the best model for deployment
  id: totrans-315
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择最佳模型进行部署
- en: 'From the preceding results, it can be seen that LR and SVM models have the
    same but higher false positive rate compared to Random Forest and DT. So we can
    say that DT and Random Forest have better accuracy overall in terms of true positive
    counts. Let''s see the validity of the preceding statement with prediction distributions
    on pie charts for each model:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 从前述结果可以看出，LR和SVM模型的假阳性率与随机森林和决策树相同，但较高。因此，我们可以说，在真正的阳性计数方面，决策树和随机森林的准确性整体上更好。让我们通过每个模型的饼图预测分布来验证前述说法的有效性：
- en: '![](img/e0108be2-f86e-40ab-9f3f-00e341dda8df.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0108be2-f86e-40ab-9f3f-00e341dda8df.png)'
- en: Now, it's worth mentioning that using random forest, we are actually getting
    high accuracy, but it's a very resource, as well as time-consuming job; the training,
    especially, takes a considerably longer time as compared to LR and SVM.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，值得一提的是，使用随机森林时，我们实际上能获得较高的准确性，但这是一个非常耗费资源和时间的工作；特别是训练，与LR和SVM相比，训练时间显著较长。
- en: Therefore, if you don't have higher memory or computing power, it is recommended
    to increase the Java heap space prior to running this code to avoid OOM errors.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您的内存或计算能力较低，建议在运行此代码之前增加Java堆空间，以避免OOM错误。
- en: 'Finally, if you want to deploy the best model (that is, Random Forest in our
    case), it is recommended to save the cross-validated model immediately after the
    `fit()` method invocation:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您想部署最佳模型（在我们的案例中是随机森林），建议在`fit()`方法调用后立即保存交叉验证模型：
- en: '[PRE86]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Your trained model will be saved to that location. The directory will include:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的模型将保存在该位置。该目录将包含：
- en: The best model
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳模型
- en: Estimator
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计器
- en: Evaluator
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估器
- en: The metadata of the training itself
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练本身的元数据
- en: 'Now the next task will be restoring the same model, as follows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，接下来的任务是恢复相同的模型，如下所示：
- en: '[PRE87]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Finally, we need to transform the test set to the model pipeline that maps
    the features according to the same mechanism we described in the preceding feature
    engineering step:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将测试集转换为模型管道，以便根据我们在前述特征工程步骤中描述的相同机制映射特征：
- en: '[PRE88]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Finally, we evaluate the restored model:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估恢复的模型：
- en: '[PRE89]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'You will receive the following output:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 您将收到以下输出：
- en: '![](img/5cb2c2c5-1984-4919-a0c1-8d433a22c386.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5cb2c2c5-1984-4919-a0c1-8d433a22c386.png)'
- en: Well, done! We have managed to reuse the model and do the same prediction. But,
    probably due to the randomness of data, we observed slightly different predictions.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，完成了！我们成功地重用了模型并进行了相同的预测。但是，由于数据的随机性，我们观察到略微不同的预测结果。
- en: Summary
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we have seen how to develop an ML project to predict whether
    a customer is likely to cancel their subscription or not, and then used it to
    develop a real-life predictive model. We have developed predictive models using
    LR, SVMs, DTs, and Random Forest. We have also analyzed what types of customer
    data are typically used to do preliminary analysis of the data. Finally, we have
    seen how to choose which model to use for a production-ready environment.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经学习了如何开发一个机器学习项目来预测客户是否有可能取消订阅，并通过此方法开发了一个真实的预测模型。我们使用了逻辑回归（LR）、支持向量机（SVM）、决策树（DT）和随机森林（Random
    Forest）来构建预测模型。我们还分析了通常用来进行数据初步分析的客户数据类型。最后，我们了解了如何选择适合生产环境的模型。
- en: In the next chapter, we will see how to develop a real-life project that collects
    historical and live **Bitcoin** data and predicts the price for an upcoming week,
    month, and so on. In addition to this, we will see how to generate a simple signal
    for online cryptocurrency trading.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何开发一个真实项目，该项目收集历史和实时的**比特币**数据，并预测未来一周、一个月等的价格。此外，我们还将学习如何为在线加密货币交易生成一个简单的信号。
