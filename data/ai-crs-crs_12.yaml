- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Deep Convolutional Q-Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度卷积Q学习
- en: Now that you understand how **Artificial Neural Networks** (**ANNs**) work,
    you're ready to tackle an incredibly useful tool, mostly used when dealing with
    images—**Convolutional Neural Networks** (**CNNs**). To put it simply, CNNs allow
    your AI to see images in real time as if it had eyes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了**人工神经网络**（**ANNs**）是如何工作的，你可以开始学习一个非常有用的工具，它主要用于处理图像——**卷积神经网络**（**CNNs**）。简单来说，CNN使得你的AI可以像有眼睛一样实时“看到”图像。
- en: 'We will tackle them in the following steps:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过以下步骤来处理它们：
- en: What are CNNs used for?
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CNN的应用是什么？
- en: How do CNNs work?
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CNN是如何工作的？
- en: Convolution
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积
- en: Max pooling
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最大池化
- en: Flattening
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扁平化
- en: Full connection
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完全连接
- en: Once you've understood those steps, you'll understand CNNs, and how they can
    be used in deep convolutional Q-learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了这些步骤，你就能理解CNN，以及它们如何在深度卷积Q学习中发挥作用。
- en: What are CNNs used for?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN的应用是什么？
- en: CNNs are mostly used with images or videos, and sometimes with text to tackle
    **Natural Language Processing** (**NLP**) problems. They are often used in object
    recognition, for example, predicting whether there is a cat or a dog in a picture
    or video. They are also often used with deep Q-learning (which we will discuss
    later on), when the environment returns 2D states of itself, for example, when
    we are trying to build a self-driving car that reads outputs from cameras around
    it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CNN主要用于图像或视频，有时也用于处理文本以解决**自然语言处理**（**NLP**）问题。它们通常用于物体识别，例如预测一张图片或视频中是猫还是狗。它们还经常与深度Q学习一起使用（我们稍后将讨论），当环境返回自身的二维状态时，例如当我们尝试构建一个可以读取周围摄像头输出的自动驾驶汽车时。
- en: Remember the example in *Chapter 9*, *Going Pro with Artificial Brains - Deep
    Q-Learning*, where we were predicting houses' prices. As inputs, we had all of
    the values that define a house (area, age, number of bedrooms, and so on), and
    as output, we had the price of a house. In the case of CNNs, things are very similar.
    For example, if we wanted to solve the same problem using CNNs, we would have
    images of houses as inputs and the price of a house as output.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在*第9章*，“使用人工大脑走向专业——深度Q学习”中，我们预测了房价。作为输入，我们有定义房子的所有值（面积、年龄、卧室数量等），作为输出，我们有房子的价格。在CNN的情况下，事情非常相似。例如，如果我们想用CNN解决同样的问题，我们将以房子的图像作为输入，房价作为输出。
- en: 'This diagram should illustrate what I mean:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图应该能说明我的意思：
- en: '![](img/B14110_12_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_01.png)'
- en: 'Figure 1: Input Image – CNN – Output Label'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：输入图像 – CNN – 输出标签
- en: As you can see, the input is an image that flows through a CNN and comes out
    as an output. In the case of this diagram, the output is a class to which the
    image corresponds. What do I mean by a class? For example, if we wanted to predict
    whether the inputted image is a smiling face or a sad face, then one class would
    be *smiling face*, and the other would be *sad face*. Our output should then correctly
    decide to which class the input image corresponds.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，输入的是一张图像，图像流经CNN并作为输出产生结果。在这个图中，输出是与图像对应的类别。什么是类别？例如，如果我们想预测输入的图像是笑脸还是悲伤的面孔，那么一个类别就是*笑脸*，另一个类别就是*悲伤面孔*。我们的输出应该能够正确判断输入图像对应的类别。
- en: 'Speaking of happy and sad faces, here''s a diagram that represents it in more
    detail:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 说到快乐和悲伤的面孔，这里有一个更详细的图示：
- en: '![](img/B14110_12_02.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_02.png)'
- en: 'Figure 2: Two different classes to predict (Happy or Sad)'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：两个不同的类别预测（快乐或悲伤）
- en: In the preceding example, we've run two images through a CNN. The first one
    is a smiling face and the other one is a sad face. As I mentioned before, our
    network predicts whether the image is a happy or a sad face.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们通过CNN处理了两张图像。第一张是笑脸，另一张是悲伤的面孔。正如我之前提到的，我们的网络预测图像是快乐面孔还是悲伤面孔。
- en: 'I can imagine what you''re thinking right now: how does it all work? What''s
    inside this black box we call a CNN? I''ll answer these questions in the following
    sections.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我能想象你现在在想什么：这一切是怎么运作的？我们所说的这个黑盒子——CNN，里面到底是什么？我将在接下来的章节中回答这些问题。
- en: How do CNNs work?
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN是如何工作的？
- en: 'Before we can go deep into the structure of CNNs, we need to understand a couple
    of points. I will introduce you to the first point with a question: how many dimensions
    does a colored RGB image have?'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨CNN的结构之前，我们需要理解几个要点。我将通过一个问题引导你了解第一个要点：一个彩色RGB图像有多少个维度？
- en: 'The answer may surprise you: it''s 3!'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 答案可能会让你吃惊：是3！
- en: 'Why? Because every RGB image is, in fact, represented by three 2D images, each
    one corresponding to a color in RGB architecture. So, there is one image corresponding
    to red, one corresponding to green, and one to blue. Grayscale images are only
    2D, because they are represented by only one scale as there are no colors. The
    following diagram should make it clearer:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么？因为每个 RGB 图像实际上由三张 2D 图像表示，每张图像对应 RGB 结构中的一个颜色。因此，红色对应一张图像，绿色对应一张图像，蓝色对应一张图像。灰度图像只有
    2D，因为它们只由一个灰度值表示，没有颜色。下面的图示应该能让这一点更加清晰：
- en: '![](img/B14110_12_03.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_03.png)'
- en: 'Figure 3: RGB versus black and white images'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：RGB 图像与黑白图像的对比
- en: As you can see, a colored image is represented by a 3D array. Each color has
    its own layer in the picture, and this layer is called a **channel**. A grayscale
    (black and white) image only has one channel and is, therefore, a 2D array.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，一张彩色图像由 3D 数组表示。每种颜色在图像中有自己的层，这层被称为**通道**。而灰度（黑白）图像只有一个通道，因此它是一个 2D
    数组。
- en: As you probably know, images are made out of pixels. Each of these is represented
    by a value that ranges from 0 to 255, where 0 is a pixel turned off and 255 is
    a fully bright pixel. It's important to understand that when we say that a pixel
    has the value (255, 255, 0), then that means this pixel is fully bright on the
    red and green channel and turned off on the blue channel.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所知，图像是由像素组成的。每个像素都由一个介于 0 到 255 之间的数值表示，其中 0 表示关闭的像素，255 表示完全亮起的像素。理解这一点很重要：当我们说一个像素的值是（255，255，0）时，这意味着这个像素在红色和绿色通道上完全亮起，而在蓝色通道上是关闭的。
- en: From now on, to understand everything better, we'll be dealing with very simple
    images. In fact, our images will be grayscale (1 channel, 2D) and the pixels will
    either be fully bright or turned off. In order to make pictures easier to read,
    we'll assign 1 to a turned off pixel (black) and 0 to a fully bright one (white).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在起，为了更好地理解一切，我们将处理非常简单的图像。实际上，我们的图像将是灰度图像（1 个通道，2D），像素将是完全亮起的或关闭的。为了让图像更容易读取，我们将关闭的像素（黑色）赋值为
    1，完全亮起的像素（白色）赋值为 0。
- en: 'Going back to the case of happy and sad faces, this is what our 2D array representing
    a happy face would look like:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 回到悲伤和快乐面孔的例子，这就是我们用 2D 数组表示的快乐面孔的样子：
- en: '![](img/B14110_12_04.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_04.png)'
- en: 'Figure 4: The pixel representation'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：像素表示
- en: As you can see, we have an array where **0** corresponds to a white pixel and
    **1** corresponds to a black pixel. The picture on the right is our smiling face
    represented by an array.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，我们有一个数组，其中**0**表示白色像素，**1**表示黑色像素。右边的图片是我们用数组表示的笑脸。
- en: 'Now that we understand the foundations and that we''ve simplified the problem,
    we''re ready to tackle CNNs. In order to fully understand them, we need to split
    our learning into the four steps that make up a CNN:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了基础知识，并且简化了问题，我们准备好迎接卷积神经网络（CNN）的挑战了。为了完全理解它们，我们需要将学习内容分为组成 CNN 的四个步骤：
- en: Convolution
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积
- en: Max pooling
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最大池化
- en: Flattening
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扁平化
- en: Full connection
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 全连接
- en: Now we'll get to know each of these four steps one by one.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将逐一了解这四个步骤。
- en: Step 1 – Convolution
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 1 – 卷积
- en: 'This is the first crucial step of every CNN. In convolution, we apply something
    called **feature detectors** to the inputted image. Why do we have to do so? This
    is because all images contain certain features that define what is in the picture.
    For example, to recognize which face is sad and which one is happy, we need to
    understand the meaning of the shape of the mouth, which is a feature of this image.
    It''s easier to understand this from a diagram:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是每个卷积神经网络（CNN）中的第一个关键步骤。在卷积操作中，我们将一种叫做**特征检测器**的东西应用于输入的图像。为什么我们要这么做呢？因为所有图像都包含一些特定的特征，这些特征定义了图像中的内容。例如，要识别哪个面部表情是悲伤的，哪个是快乐的，我们需要理解嘴巴的形状，这就是图像中的一个特征。从图示中理解这个更为清晰：
- en: '![](img/B14110_12_05.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_05.png)'
- en: 'Figure 5: Step 1 – Convolution (1/5)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：步骤 1 – 卷积（1/5）
- en: In the preceding diagram, we applied a feature detector, also known as a filter,
    to the smiling face we had as input. As you can see, a filter is a 2D array with
    some values inside. When we apply this feature detector to the image it covers
    (in this case it is a 3 x 3 grid), we check how many pixels from this part of
    the image match the filter's pixels. Then we put this number into a new 2D array
    called **feature map**. In other words, the more a part of the picture matches
    the picture detector, the higher the number we put into the feature map.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图中，我们应用了一个特征检测器，也就是一个滤波器，作用于我们输入的笑脸图像。如你所见，滤波器是一个包含一些值的二维数组。当我们将这个特征检测器应用到图像上时，它覆盖了（在这个例子中是
    3 x 3 网格）。我们检查图像的这一部分有多少像素与滤波器的像素匹配。然后，我们把这个数字放入一个新的二维数组，称为**特征图**。换句话说，图像的某一部分与特征检测器匹配得越多，我们就将越高的数字放入特征图中。
- en: 'Next, we *slide* the feature detector across the entire image. In the next
    iteration, this is what will happen:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们*滑动*特征检测器遍历整张图像。在下一次迭代中，会发生这样的情况：
- en: '![](img/B14110_12_06.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_06.png)'
- en: 'Figure 6: Step 1 – Convolution (2/5)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：步骤 1 – 卷积（2/5）
- en: As you can see, we slide the filter one place to the right. This time, one pixel
    matches in both the filter and in this part of the image. That's why we put **1**
    in the feature map.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们将滤波器向右滑动了一格。这次，滤波器和图像中的这一部分有一个像素匹配。这就是为什么我们在特征图中放置**1**的原因。
- en: 'What do you think happens when we hit the boundary of this image? What would
    you do? I''ll show you what happens with these two diagrams:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你觉得当我们碰到这张图像的边界时会发生什么？你会怎么做？我将通过这两个图示给你展示：
- en: '![](img/B14110_12_07.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_07.png)'
- en: 'Figure 7: Step 1 – Convolution (3/5)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：步骤 1 – 卷积（3/5）
- en: '![](img/B14110_12_08.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_08.png)'
- en: 'Figure 8: Step 1 – Convolution (4/5)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：步骤 1 – 卷积（4/5）
- en: 'Here, we had this exact situation: in the first image, our filter hits the
    boundary. It turns out that our feature detector simply *jumps* to the next line.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们正好遇到了这种情况：在第一张图像中，我们的滤波器碰到了边界。结果我们的特征检测器会直接*跳跃*到下一行。
- en: 'The whole magic of the convolution wouldn''t work if we had only one filter.
    In reality, we use many filters, which produce many different feature maps. This
    set of feature maps is called a **convolution layer**, or **convolutional layer**.
    Here''s a diagram to recap:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只有一个滤波器，卷积的所有魔法都无法奏效。实际上，我们使用多个滤波器，产生多个不同的特征图。这组特征图被称为**卷积层**，或者**卷积神经层**。下面是一个总结图：
- en: '![](img/B14110_12_09.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_09.png)'
- en: 'Figure 9: Step 1 – Convolution (5/5)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：步骤 1 – 卷积（5/5）
- en: Here, we can see an input image to which many filters were applied. All together,
    they create a convolutional layer from many feature maps. This is the first step
    when building a CNN.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们可以看到一张输入图像，已经应用了多个滤波器。所有这些滤波器一起创建了一个卷积层，由多个特征图组成。这是构建卷积神经网络（CNN）的第一步。
- en: Now that we understand convolution, we can proceed to another important step—max
    pooling.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了卷积操作，我们可以继续进行另一个重要的步骤——最大池化。
- en: Step 2 – Max pooling
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤 2 – 最大池化
- en: This step in CNNs is responsible for lowering the size of each feature map.
    When dealing with neural networks, we don't want to have too many inputs, otherwise
    our network wouldn't be able to learn properly because of the high complexity.
    Therefore, a method of reducing the size called **max pooling** needs to be introduced.
    It lets us reduce the size without losing any important features, and it makes
    features partially invariant to shifts (translations and rotations).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络（CNN）中，这一步骤负责降低每个特征图的大小。在处理神经网络时，我们不希望输入数据过多，否则我们的网络由于复杂性过高，无法正常学习。因此，需要引入一种叫做**最大池化**的尺寸缩减方法。它让我们在不丢失重要特征的情况下减少大小，并且使特征对位移（平移和旋转）具有部分不变性。
- en: Technically, a max pooling algorithm is also based on an array sliding across
    the entire feature map. In this case, we are not searching for any features but,
    rather, for the maximum value in a specific area of a feature map.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，最大池化算法也基于一个数组滑动整个特征图。在这种情况下，我们不是在寻找任何特征，而是寻找特征图中特定区域的最大值。
- en: 'Let me show you what I mean with this graphic:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我通过这个图形给你展示我是什么意思：
- en: '![](img/B14110_12_10.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_10.png)'
- en: 'Figure 10: Step 2 – Max pooling (1/5)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：步骤 2 – 最大池化（1/5）
- en: In this example, we're taking the feature map, obtained after the convolution
    step we had before, and then we are running it through max pooling. As you can
    see, we have a window of size 2 x 2 looking for the highest values in the part
    of feature map it covers. In this case, it's 1.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们正在使用之前卷积步骤得到的特征图，然后通过最大池化进行处理。正如你所看到的，我们有一个大小为2 x 2的窗口，寻找它覆盖部分特征图中的最大值。在这个例子中，最大值是1。
- en: Can you tell what will happen in the next iteration?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你能预测下一次迭代会发生什么吗？
- en: 'As you may have suspected, this window will slide to the right, although in
    a slightly different way than before. It moves like this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经猜到的，这个窗口将滑动到右边，尽管方式与之前稍有不同。它的移动方式如下：
- en: '![](img/B14110_12_11.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_11.png)'
- en: 'Figure 11: Step 2 – Max Pooling (2/5)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：步骤2 – 最大池化（2/5）
- en: This window *jumps* its size to the right, which I hope you remember is different
    from the convolution step, where the feature detector slid one cell at a time.
    In this case, the highest value is 1 as well, and therefore we write **1** in
    the **pooled feature map**.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个窗口*跳跃*到右边，我希望你记得，这与卷积步骤不同，在卷积步骤中，特征检测器一次滑动一个单元格。在这个例子中，最大值也是1，因此我们在**池化特征图**中写下**1**。
- en: 'What happens this time when we hit the boundary of the feature map? Things
    look slightly different from before once again. This is what happens:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这次当我们碰到特征图的边界时，会发生什么呢？事情再次看起来与之前有所不同。发生的情况是：
- en: '![](img/B14110_12_12.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_12.png)'
- en: 'Figure 12: Step 2 – Max pooling (3/5)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：步骤2 – 最大池化（3/5）
- en: The window crosses the boundary and searches for the highest value in the part
    of the feature map that is still inside the max pooling window. Yet again, the
    highest value is 1.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口穿越边界，寻找特征图中仍然在最大池化窗口内的部分的最大值。再次地，最大值是1。
- en: 'But what happens now? After all, there''s no space left to go to the right.
    There''s also only one row at the bottom left for max pooling. This is what the
    algorithm does:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 那现在会发生什么呢？毕竟，右边已经没有空间可走了。而且底部左侧只有一行可以进行最大池化。算法会这样做：
- en: '![](img/B14110_12_13.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_13.png)'
- en: 'Figure 13: Step 2 – Max pooling (4/5)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：步骤2 – 最大池化（4/5）
- en: 'As we can see, it once again crosses the boundary and searches for the highest
    value in what is inside the window. In this case, it is 0\. This process is repeated
    until the window hits the bottom right corner of the feature map. To recap what
    our CNN looks like for now, have a look at the following diagram:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，它再次穿越边界，寻找窗口内部的最大值。在这个例子中，最大值是0。这个过程会一直重复，直到窗口碰到特征图的右下角。为了回顾一下我们目前的卷积神经网络长什么样，可以看看以下的图示：
- en: '![](img/B14110_12_14.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_14.png)'
- en: 'Figure 14: Step 2 – Max pooling (5/5)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：步骤2 – 最大池化（5/5）
- en: We had a smiling face as input, then we ran it through convolution to obtain
    many feature maps, called the convolutional layer. Now we've run all the feature
    maps through max pooling and obtained many pooled feature maps, all together called
    the **pooling layer**.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们输入了一个笑脸，然后通过卷积获得了许多特征图，这些图被称为卷积层。现在我们已经将所有特征图通过最大池化处理，并获得了许多池化特征图，这些图合起来被称为**池化层**。
- en: Now we can continue to the next step, which will let us input the pooling layer
    into a neural network. This step is called **flattening**.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续进行下一步，这将让我们将池化层输入神经网络。这个步骤叫做**扁平化**。
- en: Step 3 – Flattening
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤3 – 扁平化
- en: 'This is a very short step. As the name may suggest, we change all the pooled
    feature maps from 2D arrays to 1D ones. As I mentioned before, this will let us
    input the image into a neural network with ease. So, how exactly will we achieve
    this? The following diagram should help you understand:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简短的步骤。正如名字所示，我们将所有池化后的特征图从二维数组转换为一维数组。正如我之前提到的，这样做能让我们轻松地将图像输入神经网络。那么，我们到底是如何实现这一点的呢？以下图示应该能帮助你理解：
- en: '![](img/B14110_12_15.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_15.png)'
- en: 'Figure 15: Step 3 – Flattening (1/3)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：步骤3 – 扁平化（1/3）
- en: Here we go back to the pooled feature map we obtained before. To flatten it,
    we take pixel values starting from the top left, finishing at bottom right. An
    operation like this returns a 1D array, containing the same values as the 2D array
    we started with.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们回到之前得到的池化特征图。为了扁平化它，我们从左上角开始获取像素值，一直到右下角。像这样的操作会返回一个一维数组，包含与我们最初的二维数组相同的值。
- en: But remember, we don't have one pooled feature map, we have an entire layer
    of them. What do you think we should do with that?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 但记住，我们并不是只有一个池化特征图，而是有一整层池化特征图。你认为我们应该怎么处理这些呢？
- en: 'The answer is simple: we put this entire layer into a single 1D flattened array,
    one pooled feature map after another. Why does it have to be 1D? This is because
    ANNs only accept 1D arrays as their inputs. All the layers in a traditional neural
    network are 1D, which means that the input has to be 1D as well. Therefore, we
    flatten all the pooled feature maps, like so:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 答案很简单：我们将整个层压缩成一个单一的1D扁平数组，一个接一个地放入池化特征图。为什么必须是1D？因为人工神经网络（ANNs）只接受1D数组作为输入。传统神经网络的所有层都是1D的，这意味着输入也必须是1D。因此，我们将所有池化特征图扁平化，像这样：
- en: '![](img/B14110_12_16.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_16.png)'
- en: 'Figure 16: Step 3 – Flattening (2/3)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：步骤3 – 扁平化 (2/3)
- en: We've taken the entire layer and transformed it into a single flattened 1D array.
    We'll soon use this array as the input of a traditional neural network.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将整个层转换为一个单一的扁平化1D数组。我们将很快把这个数组用作传统神经网络的输入。
- en: 'First, let''s remind ourselves of what our model looks like now:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们回顾一下当前模型的结构：
- en: '![](img/B14110_12_17.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_17.png)'
- en: 'Figure 17: Step 3 – Flattening (3/3)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：步骤3 – 扁平化 (3/3)
- en: So, we have a Convolutional Layer, Pooling Layer, and a freshly added, flattened
    1D layer. Now we can go back to a classic ANN, that is, a fully connected neural
    network, and treat this last layer as an input for this network. This leads us
    to the final step, **full connection**.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们有一个卷积层、池化层，以及一个新添加的扁平化1D层。现在我们可以回到经典的人工神经网络（ANN），即全连接神经网络，并将这个最后的层视为该网络的输入。这引领我们进入最后一步，即**全连接**。
- en: Step 4 – Full connection
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 步骤4 – 全连接
- en: The final step of creating a CNN is to connect it to a classic fully-connected
    neural network. Remember that we already have a 1D array telling us in a compressed
    way what the image looks like, so why not just use it as an input to a fully-connected
    neural network? After all, it's the latter that's able to make predictions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 创建CNN的最后一步是将其连接到经典的全连接神经网络。记住，我们已经有了一个1D数组，简明地告诉我们图像的外观，那么为什么不直接将它作为输入传递给全连接神经网络呢？毕竟，是后者能够进行预测。
- en: 'That''s exactly what we do next, just like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们接下来要做的，就像这样：
- en: '![](img/B14110_12_18.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_18.png)'
- en: 'Figure 18: Step 4 – Full connection'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：步骤4 – 全连接
- en: After flattening, we input those returned values straight into the fully-connected
    neural network, which then yields the prediction—the output value.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 扁平化后，我们将这些返回的值直接输入到全连接神经网络中，然后得到预测值——输出值。
- en: You might be wondering how the back-propagation phase works now. In a CNN, back-propagation
    not only updates the weights in the fully-connected neural network, but also the
    filters used in the convolution step. The max pooling and flattening steps will
    remain the same, as there is nothing to update there.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，现在反向传播阶段是如何工作的。在CNN中，反向传播不仅更新全连接神经网络中的权重，还更新卷积步骤中使用的滤波器。最大池化和扁平化步骤保持不变，因为那里没有需要更新的内容。
- en: 'In conclusion, CNNs look for some specific features. This is why they''re mostly
    used when we are dealing with images, where searching for features is crucial.
    For example, when trying to recognize a sad and a happy face, a CNN needs to understand
    which mouth''s shape means a sad face and which means a happy face. In order to
    obtain an output, a CNN has to run these steps:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，CNN会寻找一些特定的特征。这也是它们主要在处理图像时使用的原因，因为在图像处理中，特征的搜索至关重要。例如，当尝试识别一个悲伤和一个开心的面孔时，CNN需要理解哪个嘴巴的形状表示悲伤面孔，哪个表示开心面孔。为了得到输出，CNN必须执行以下步骤：
- en: '**Convolution** – Applying filters to the input image. This operation will
    find the features our CNN is looking for and save them in a feature map.'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**卷积** – 将滤波器应用于输入图像。这个操作将会找到我们CNN所需的特征，并将它们保存在特征图中。'
- en: '**Max pooling** – Lowering the feature map size, by taking a maximum value
    in a given area and saving these values in a new array called pooled feature map.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**最大池化** – 通过在给定区域内取最大值，来降低特征图的大小，并将这些值保存在一个新的数组中，称为池化特征图。'
- en: '**Flattening** – Changing the entire pooling layer (all pooled feature maps)
    to a 1D vector. This will allow us to input this vector into a neural network.'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**扁平化** – 将整个池化层（所有池化特征图）转换为1D向量。这将允许我们将这个向量输入到神经网络中。'
- en: '**Full connection** – Creating a neural network, which takes as input a flattened
    pooling layer and returns a value that we would like to predict. This last step
    lets us make predictions.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**全连接** – 创建一个神经网络，它将扁平化的池化层作为输入，并返回我们想要预测的值。这个最后的步骤使我们能够进行预测。'
- en: Deep convolutional Q-learning
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度卷积Q学习
- en: In the chapter on deep Q-learning (*Chapter 9*, *Going Pro with Artificial Brains
    – Deep Q-Learning*), our inputs were vectors of encoded values defining the states
    of the environment. When working with images or videos, encoded vectors aren't
    the best inputs to describe a state (the input frame), simply because an encoded
    vector doesn't preserve the spatial structure of an image. The spatial structure
    is important because it gives us more information to help predict the next state,
    and predicting the next state is essential for our AI to learn the correct next
    move.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度Q学习（*第9章*，*人工大脑进阶——深度Q学习*）的章节中，我们的输入是定义环境状态的编码值的向量。当处理图像或视频时，编码向量并不是描述状态（输入帧）的最佳输入方式，因为编码向量不能保留图像的空间结构。空间结构非常重要，因为它为我们提供更多信息，帮助预测下一个状态，而预测下一个状态对于AI学习正确的下一步至关重要。
- en: Therefore, we need to preserve the spatial structure. To do that, our inputs
    must be 3D images (2D for the array of pixels plus one additional dimension for
    the colors, as illustrated at the beginning of this chapter). For example, if
    we train an AI to play a video game, the inputs are simply the images of the screen
    itself, exactly what a human sees when playing the game.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要保留空间结构。为了做到这一点，我们的输入必须是3D图像（对于像素数组来说是2D，再加上一个额外的维度来表示颜色，正如本章开头所示）。例如，如果我们训练一个AI来玩电子游戏，那么输入就是游戏屏幕本身的图像，完全就是人类玩游戏时所看到的内容。
- en: Following this analogy, the AI acts like it has human eyes; it observes the
    input images on the screen when playing the game. Those input images go into a
    CNN (the eyes for a human), which detects the state in each image. Then they're
    forward-propagated through the pooling layers where max pooling is applied. Then
    the pooling layers are flattened into a 1D vector, which becomes the input of
    our deep Q-learning network (the exact same one as in *Chapter 9*, *Going Pro
    with Artificial Brains – Deep Q-Learning*). In the end, the same deep Q-learning
    process is run.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 按照这个类比，AI就像是拥有了人类的眼睛；它在玩游戏时观察屏幕上的输入图像。这些输入图像进入一个CNN（人类的眼睛），它检测每一幅图像的状态。然后，它们通过池化层进行前向传播，在池化层中应用最大池化。接着，池化层会被展平为1D向量，作为我们深度Q学习网络的输入（与*第9章*，*人工大脑进阶——深度Q学习*中的网络完全相同）。最终，执行相同的深度Q学习过程。
- en: 'The following graph illustrates deep convolutional Q-learning applied to the
    famous game of Doom:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了深度卷积Q学习在著名游戏《毁灭战士》中的应用：
- en: '![](img/B14110_12_19.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14110_12_19.png)'
- en: 'Figure 19: Deep convolutional Q-learning for Doom'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：深度卷积Q学习用于《毁灭战士》
- en: In summary, deep convolutional Q-learning is the same as deep Q-learning, with
    the only differences being that the inputs are now images, and a CNN is added
    at the beginning of the fully-connected deep Q-learning network to detect the
    states of those images.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，深度卷积Q学习与深度Q学习相同，唯一的不同是输入现在是图像，并且在完全连接的深度Q学习网络的开始部分加入了CNN，用于检测这些图像的状态。
- en: Summary
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: You've learned about another type of neural network—a Convolutional Neural Network.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了另一种类型的神经网络——卷积神经网络。
- en: 'We established that this network is used mostly with images and searches for
    certain features in these pictures. It uses three additional steps that ANNs don''t
    have: convolution, where we search for features; max pooling, where we shrink
    the image in size; and flattening, where we flatten 2D images to a 1D vector so
    that we can input it into a neural network.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定，这个网络主要用于图像，并在这些图像中搜索特定的特征。它使用了三步是ANNs没有的额外步骤：卷积，用于搜索特征；最大池化，用于缩小图像大小；以及展平，将2D图像展平为1D向量，以便将其输入到神经网络中。
- en: 'In the next chapter, you’ll build a deep convolutional Q-learning model to
    solve a classic gaming problem: Snake.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将构建一个深度卷积Q学习模型，来解决一个经典的游戏问题：贪吃蛇。
