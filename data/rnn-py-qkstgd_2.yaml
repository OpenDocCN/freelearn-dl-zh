- en: Building Your First RNN with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow构建你的第一个RNN
- en: In this chapter, you will gain a hands-on experience of building a **recurrent
    neural network** (**RNN**). First, you will be introduced to the most widely used
    machine learning library—TensorFlow. From learning the basics to advancing into
    some fundamental techniques, you will obtain a reasonable understanding of how
    to apply this powerful library to your applications. Then, you will take on a
    fairly simple task of building an actual model. The process will show you how
    to prepare your data, train the network, and make predictions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将获得构建**循环神经网络**（**RNN**）的实践经验。首先，你将学习最广泛使用的机器学习库——TensorFlow。从学习基础知识到掌握一些基本技术，你将合理理解如何将这个强大的库应用于你的应用中。然后，你将开始一个相对简单的任务，构建一个实际的模型。这个过程将向你展示如何准备数据、训练网络并进行预测。
- en: 'In summary, the topics of this chapter include the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，本章的主题包括以下内容：
- en: '**What are you going to build?**: Introduction of your task'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**你将要构建什么？**：你的任务介绍'
- en: '**Introduction to TensorFlow**: Taking first steps into learning the TensorFlow
    framework'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow简介**：开始学习TensorFlow框架的第一步'
- en: '**Coding the RNN**: You will go through the process of writing your first neural
    network using TensorFlow. This includes all steps required for a finished solution'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编写RNN代码**：你将经历编写第一个神经网络的过程。包括完成解决方案所需的所有步骤'
- en: The prerequisites for this chapter are basic Python programming knowledge and
    decent understanding of recurrent neural networks captured in the [Chapter 1](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml),
    *Introducing Recurrent Neural Networks*. After reading this chapter, you should
    have a full understanding of how to use TensorFlow with Python and how easy and
    straightforward it is to build a neural network.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的前提条件是基本的Python编程知识，以及对循环神经网络的基本理解，参见[第1章](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml)，*介绍循环神经网络*。阅读本章后，你应该能够全面了解如何使用Python和TensorFlow，并明白构建神经网络是多么简单直接。
- en: What are you going to build?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你将要构建什么？
- en: 'Your first steps into the practical world of recurrent neural networks will
    be to build a simple model which determines the parity ([http://mathworld.wolfram.com/Parity.html](http://mathworld.wolfram.com/Parity.html))
    of a bit sequence . This is a warm-up exercise released by OpenAI in January 2018
    ([https://blog.openai.com/requests-for-research-2/](https://blog.openai.com/requests-for-research-2/)).
    The task can be explained as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你进入实际应用领域的第一步将是构建一个简单的模型，该模型用于确定一个比特序列的奇偶性 ([http://mathworld.wolfram.com/Parity.html](http://mathworld.wolfram.com/Parity.html))。这是OpenAI在2018年1月发布的一个热身练习
    ([https://blog.openai.com/requests-for-research-2/](https://blog.openai.com/requests-for-research-2/))。这个任务可以这样解释：
- en: Given a binary string of a length of `50`, determine whether there is an even
    or odd number of ones. If that number is even, output `0`, otherwise `1`.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个长度为`50`的二进制字符串，确定其中是否包含偶数个或奇数个1。如果该数字是偶数，则输出`0`，否则输出`1`。
- en: Later in this chapter, we will give a detailed explanation of the solution,
    together with addressing the difficult parts and how to tackle them.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章稍后将详细解释解决方案，并讨论一些难点及其应对方法。
- en: Introduction to TensorFlow
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow简介
- en: TensorFlow is an open source library built by Google, which aims to assist developers
    in creating machine learning models of any kind. The recent improvements in the
    deep learning space created the need for an easy and fast way of building neural
    networks. TensorFlow addresses this problem in an excellent fashion, by providing
    a wide range of APIs and tools to help developers focus on their specific problem,
    rather than dealing with mathematical equations and scalability issues.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是Google构建的一个开源库，旨在帮助开发人员创建各种类型的机器学习模型。深度学习领域的最新进展促使了对一种易于快速构建神经网络的方法的需求。TensorFlow通过提供丰富的API和工具来解决这个问题，帮助开发者专注于他们的具体问题，而不必处理数学方程和可扩展性问题。
- en: 'TensorFlow offers two main ways of programming a model:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了两种主要的模型编程方式：
- en: Graph-based execution
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于图的执行
- en: Eager execution
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 急切执行
- en: Graph-based execution
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于图的执行
- en: 'Graph-based execution is an alternative way of representing mathematical equations
    and functions. Considering the expression *a = (b*c) + (d*e),* we can use a graph
    representation as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的执行是一种表示数学方程和函数的替代方式。考虑表达式 *a = (b*c) + (d*e)*，我们可以用图形表示如下：
- en: 'Separate the expression into the following:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将表达式分解为以下部分：
- en: '*x = b*c*'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x = b*c*'
- en: '*y = d*e*'
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y = d*e*'
- en: '*a = x+y*'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a = x+y*'
- en: 'Build the following graph:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建以下图形：
- en: '![](img/ac973fd4-5726-459a-8dbd-10d0aa1924a4.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac973fd4-5726-459a-8dbd-10d0aa1924a4.png)'
- en: As you can see from the previous example, using graphs lets compute two equations
    in parallel. This way, the code can be distributed among multiple CPUs/GPUs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的示例可以看出，使用图形可以并行计算两个方程。这样，代码可以分布到多个 CPU/GPU 上。
- en: More complex variants of that example are used in TensorFlow for training heavy
    models. Following that technique, TensorFlow graph-based execution requires a
    two-step approach when building your neural network. One should first construct
    the graph architecture and then execute it to receive results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例的更复杂变体被用于 TensorFlow 中训练大型模型。按照这种方法，基于图的 TensorFlow 执行在构建神经网络时需要一个两步法。首先应构建图架构，然后执行它以获取结果。
- en: This approach makes your application run faster and it will be distributed across
    multiple CPUs, GPUs, and so on. Unfortunately, some complexity comes along with
    it. Understanding how this way of programming work, and the inability to debug
    your code in the already familiar way (for example, printing values at any point
    in your program) makes the graph-based execution (see for more details [http://smahesh.com/blog/2017/07/10/understanding-tensorflow-graph/](http://smahesh.com/blog/2017/07/10/understanding-tensorflow-graph/))
    a bit challenging for beginners.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使你的应用程序运行得更快，并且能够分布到多个 CPU、GPU 等设备上。不幸的是，它也带来了一些复杂性。理解这种编程方式是如何工作的，以及无法像以前那样调试代码（例如，在程序的任何点打印值），使得基于图的执行（更多细节请见
    [http://smahesh.com/blog/2017/07/10/understanding-tensorflow-graph/](http://smahesh.com/blog/2017/07/10/understanding-tensorflow-graph/)）对于初学者来说有些挑战。
- en: Even though this technique may introduce a new way of programming, our examples
    will be based upon it. The reason behind this decision lies in the fact that there
    are many more resources out there and almost every TensorFlow example you come
    across is graph-based. In addition, I believe it is of vital importance to understand
    the fundamentals, even if they introduce unfamiliar techniques.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种技术可能会引入一种新的编程方式，我们的示例将基于它。做出这个决定的原因在于，外面有更多的资源，而且几乎你遇到的每一个 TensorFlow 示例都是基于图的。此外，我认为理解基础知识至关重要，即使它们引入了不熟悉的技术。
- en: Eager execution
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 急切执行
- en: 'Eager execution is an approach, recently introduced by Google, which, as stated
    in the documentation ([https://www.tensorflow.org/guide/eager](https://www.tensorflow.org/guide/eager)),
    uses the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 急切执行是一种由 Google 最近推出的方法，正如文档中所述 ([https://www.tensorflow.org/guide/eager](https://www.tensorflow.org/guide/eager))，它使用以下内容：
- en: 'An imperative programming environment that evaluates operations immediately,
    without building graphs: operations return concrete values instead of constructing
    a computational graph to run later. This makes it easy to get started with TensorFlow
    and debug models, and it reduces boilerplate as well.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一种命令式编程环境，立即评估操作，而不是构建图形：操作返回具体的值，而不是构建一个计算图以便稍后运行。这使得开始使用 TensorFlow 和调试模型变得更加容易，同时也减少了模板代码。
- en: As you can see, there is no overhead to learning the new programming technique
    and debugging is seamless. For a better understanding, I recommend checking this
    tutorial from the TensorFlow Conference 2018 ([https://www.youtube.com/watch?v=T8AW0fKP0Hs](https://www.youtube.com/watch?v=T8AW0fKP0Hs)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，学习这种新的编程技术没有额外负担，调试也很顺畅。为了更好地理解，我建议查看 TensorFlow 2018 年大会的这篇教程 ([https://www.youtube.com/watch?v=T8AW0fKP0Hs](https://www.youtube.com/watch?v=T8AW0fKP0Hs))。
- en: I must state that, once you learn how to manipulate the TF API, building models
    becomes really easy on both graph-based and eager execution. Don't panic if the
    former seems complicated at first—I can assure you that it is worth investing
    the time to understand it properly.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我必须声明，一旦你学会如何操作 TF API，在图计算和急切执行（eager execution）上构建模型会变得非常容易。如果一开始前者看起来很复杂，不用慌张——我可以向你保证，花时间理解它是值得的。
- en: Coding the recurrent neural network
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写递归神经网络代码
- en: As mentioned before, the aim of our task is to build a recurrent neural network
    that predicts the parity of a bit sequence. We will approach this problem in a
    slightly different way. Since the parity of a sequence depends on the number of
    ones, we will sum up the elements of the sequence and find whether the result
    is even or not. If it is even, we will output `0`, otherwise, `1`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的任务目标是构建一个递归神经网络，用于预测比特序列的奇偶性。我们将以略有不同的方式来处理这个问题。由于序列的奇偶性取决于1的数量，我们将对序列的元素进行求和，找出结果是否为偶数。如果是偶数，我们将输出`0`，否则输出`1`。
- en: 'This section of the chapter includes code samples and goes through the following
    steps:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的这一部分包括代码示例，并执行以下步骤：
- en: Generating data to train the model
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成用于训练模型的数据
- en: Building the TensorFlow graph (using TensorFlow's built-in functions for recurrent
    neural networks)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建TensorFlow图（使用TensorFlow内置的递归神经网络函数）
- en: Training the neural network with the generated data
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用生成的数据训练神经网络
- en: Evaluating the model and determining its accuracy
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型并确定其准确性
- en: Generating data
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成数据
- en: Let's revisit the OpenAI's task ([https://blog.openai.com/requests-for-research-2/](https://blog.openai.com/requests-for-research-2/)).
    As stated there, we need to generate a dataset of random 100,000 binary strings
    of length 50. In other words, our training set will be formed of 100,000 examples
    and the recurrent neural network will accept 50 time steps. The result of the
    last time step would be counted as the model prediction.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视OpenAI的任务([https://blog.openai.com/requests-for-research-2/](https://blog.openai.com/requests-for-research-2/))。如文中所述，我们需要生成一个包含100,000个长度为50的随机二进制字符串的数据集。换句话说，我们的训练集将由100,000个示例组成，递归神经网络将接受50个时间步长。最后一个时间步的结果将被视为模型的预测值。
- en: The task of determining the sum of a sequence can be viewed as a classification
    problem where the result can be any of the classes from `0` to `50`. A standard
    practice in machine learning is to encode the data into an easily decodable numeric
    way. But why is that? Most machine learning algorithms cannot accept anything
    apart from numeric data, so we need to always encode our input/output. This means
    that, our predictions will also come out in an encoding format. Thus, it is vital
    to understand the actual value behind these predictions. This means that we need
    to be able to easily decode them into a human understandable format.  A popular
    way of encoding data for classification problems is one-hot encoding.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 确定序列和求和任务可以被视为一个分类问题，其中结果可以是`0`到`50`之间的任何类别。机器学习中的一种标准做法是将数据编码为易于解码的数字格式。那为什么要这么做呢？大多数机器学习算法只能接受数字数据，因此我们总是需要对输入/输出进行编码。这意味着我们的预测也将以编码格式输出。因此，理解这些预测背后的实际值是至关重要的。这意味着我们需要能够轻松地将它们解码为人类可理解的格式。一种常见的分类问题数据编码方式是独热编码（one-hot
    encoding）。
- en: Here is an example of that technique.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是该技术的一个示例。
- en: Imagine the predicted output for a specific sequence is `30`. We can encode
    this number by introducing a `1x50` array where all numbers, except the one in
    the 30th position, are `0s - [0, 0,..., 0, 1, 0, ..., 0, 0, 0]`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设某个特定序列的预测输出为`30`。我们可以通过引入一个`1x50`的数组来编码这个数字，其中除了第30个位置上的数字，其余位置都是`0`——`[0,
    0,..., 0, 1, 0, ..., 0, 0, 0]`。
- en: 'Before preparing the actual data, we need to import all of the necessary libraries.
    To do that, follow this link ([https://www.python.org/downloads/](https://www.python.org/downloads/))to install
    Python on your machine. In your command-line/Terminal window install the following
    packages:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备实际数据之前，我们需要导入所有必需的库。为此，请访问此链接([https://www.python.org/downloads/](https://www.python.org/downloads/))来在你的计算机上安装Python。在命令行/终端窗口中安装以下软件包：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After you have done that, create a new file called `ch2_task.py` and import
    the following libraries:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这一步后，创建一个名为`ch2_task.py`的新文件，并导入以下库：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Preparing the data requires an input and output value. The input value is a
    three-dimensional array of a size of `[100000, 50, 1]`, with `100000` items, each
    one containing `50` one-element arrays (either `0` or `1`), and is shown in the
    following example:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 准备数据需要输入和输出值。输入值是一个三维数组，大小为`[100000, 50, 1]`，包含`100000`个项目，每个项目包含`50`个一元素数组（值为`0`或`1`），示例如下：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following example shows the implementation:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了实现过程：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, `num_classes` is the number of time steps in our RNN (`50`, in this example).
    The preceding code returns a list with the `100000` binary sequences. The style
    is not very Pythonic but, written in this way, it makes it easy to follow and understand.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`num_classes`是我们RNN中的时间步数（在这个例子中是`50`）。前面的代码返回一个包含`100000`个二进制序列的列表。虽然这种写法不是特别符合Python的风格，但这样写使得跟踪和理解变得更加容易。
- en: First, we start with initializing the `multiple_values` variable. It contains
    a binary representation of the first `2^(20 )= 1,048,576` numbers, where each
    binary number is padded with zeros to accommodate the length of `50`. Obtaining
    so many examples minimizes the chance of similarity between any two of them. We
    use the `map` function together with `int` in order to convert the produced string
    into a number.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从初始化`multiple_values`变量开始。它包含了前`2^(20)=1,048,576`个数字的二进制表示，其中每个二进制数都用零填充以适应`50`的长度。获得如此多的示例可以最小化任何两个之间的相似性。我们使用`map`函数与`int`结合，目的是将生成的字符串转换为数字。
- en: Here is a quick example of how this works. We want to represent the number `2`
    inside the `multiple_values` array. The binary version of `2` is `'10'`, so the
    string produced after `'{0:050b}'.format(i)` where `i = 2`, is `'00000000000000000000000000000000000000000000000010'`
    (48 zeros at the front to accommodate a length of `50`). Finally, the `map` function
    makes the previous string into a number without removing the zeros at the front.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简短的示例，说明它是如何工作的。我们想要在`multiple_values`数组中表示数字`2`。`2`的二进制版本是`'10'`，所以在`'{0:050b}'.format(i)`中，`i
    = 2`生成的字符串是`'00000000000000000000000000000000000000000000000010'`（前面有48个零以适应长度为`50`）。最后，`map`函数将这个字符串转换为数字，并且不会去掉前面的零。
- en: Then, we shuffle the `multiple_values` array, assuring difference between neighboring
    elements. This is important during backpropagation when the network is trained,
    because we are iteratively looping throughout the array and training the network
    at each step using a single example. Having similar values next to each other
    inside the array may produce biased results and incorrect future predictions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们打乱`multiple_values`数组，确保相邻元素之间有所不同。这在反向传播过程中非常重要，因为我们在训练网络时是逐步遍历数组，并在每一步使用单个示例来训练网络。如果数组中相似的值彼此紧挨着，可能会导致偏倚结果和不正确的未来预测。
- en: Finally, we enter a loop, which traverses over all of the binary elements and
    builds an array similar to the one we saw previously. An important thing to note
    is the usage of `num_examples`, which slices the array, so we pick only the first
    `100,000` values.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们进入一个循环，遍历所有的二进制元素，并构建一个与之前看到的类似的数组。需要注意的是`num_examples`的使用，它会对数组进行切片，因此我们只选择前`100,000`个值。
- en: 'The second part of this section shows how to generate the expected output (the
    sum of all the elements in each list from the input set). These outputs are used
    to evaluate the model and tune the `weight`/`biases` during backpropagation. The
    following example shows the implementation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分的第二部分展示了如何生成预期的输出（输入集中每个列表中所有元素的总和）。这些输出用于评估模型并在反向传播过程中调整`weight`/`biases`。以下示例展示了实现方式：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `inputs` parameter is a result of `input_values()` that we declared earlier.
    The `output_values()` function returns a list of one-hot encoded representations
    of each member in `inputs`. If the sum of all of the elements in the `[[0], [1],
    [1], [1], [0], ..., [0], [1]]` sequence is `48`, then its corresponding value
    inside `output_values` is `[0, 0, 0, ..., 1, 0, 0]` where `1` is at position `48`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`inputs`参数是我们之前声明的`input_values()`的结果。`output_values()`函数返回`inputs`中每个成员的独热编码表示列表。如果`[[0],
    [1], [1], [1], [0], ..., [0], [1]]`序列中所有元素的总和为`48`，那么它在`output_values`中的对应值就是`[0,
    0, 0, ..., 1, 0, 0]`，其中`1`位于第`48`的位置。'
- en: 'Finally, we use the `generate_data()` function to obtain the final values for
    the network''s input and output, as shown in the following example:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`generate_data()`函数获取网络输入和输出的最终值，如下例所示：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We use the previous function to create these two new variables: `input_values`,
    and `output_values = generate_data()`. One thing to pay attention to is the dimensions
    of these lists:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用之前的函数来创建这两个新变量：`input_values`和`output_values = generate_data()`。需要注意的是这些列表的维度：
- en: '`input_values` is of a size of `[num_examples, num_classes, 1]`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_values`的大小为`[num_examples, num_classes, 1]`'
- en: '`output_values` is of a size of `[num_examples, num_classes]`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`output_values`的大小为`[num_examples, num_classes]`'
- en: Where `num_examples = 100000` and `num_classes = 50` .
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，`num_examples = 100000` 和 `num_classes = 50`。
- en: Building the TensorFlow graph
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建TensorFlow图
- en: Constructing the TensorFlow graph is probably the most complex part of building
    a neural network. We will precisely examine all of the steps so you can obtain
    a full understanding.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 TensorFlow 图可能是构建神经网络中最复杂的部分。我们将仔细检查所有步骤，确保你能够全面理解。
- en: The TensorFlow graph can be viewed as a direct implementation of the recurrent
    neural network model, including all equations and algorithms introduced in [Chapter
    1](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml), *Introducing Recurrent Neural
    Networks*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 图可以看作是递归神经网络模型的直接实现，包括在[第 1 章](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml)中介绍的所有方程和算法，*引入递归神经网络*。
- en: 'First, we start with setting the parameters of the model, as shown in the following
    example:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从设置模型的参数开始，如下例所示：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`X` and `Y` are declared as `tf.placeholder`, which inserts a placeholder (inside
    the graph) for a tensor that will be always fed. Placeholders are used for variables
    that expect data when training the network. They often hold values for the training
    input and expected output of the network. You might be surprised why one of the
    dimensions is `None`. The reason is that we have trained the network using batches.
    These are collections of several elements from our training data stacked together.
    When specifying the dimension as None, we let the tensor decide this dimension,
    calculating it using the other two values.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`X` 和 `Y` 被声明为 `tf.placeholder`，这会在图中插入一个占位符（用于一个始终会被馈送的张量）。占位符用于在训练网络时预期接收数据的变量。它们通常保存网络训练输入和预期输出的值。你可能会对其中一个维度是
    `None` 感到惊讶。原因是我们在训练网络时使用了批量。批量是由多个来自训练数据的元素堆叠在一起形成的集合。当指定维度为 None 时，我们让张量决定这个维度，并通过其他两个值来计算它。'
- en: According to the TensorFlow documentation: A tensor is a generalization of vectors
    and matrices to potentially higher dimensions. Internally, TensorFlow represents
    tensors as n-dimensional arrays of base datatypes.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 TensorFlow 文档：张量是对向量和矩阵的一个推广，允许更高维度的表示。在 TensorFlow 内部，张量被表示为 n 维数组，元素类型是基本数据类型。
- en: When performing training using batches, we split the training data into several
    smaller arrays of size—`batch_size`. Then, instead of training the network with
    all examples at once, we use one batch at a time.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用批量进行训练时，我们将训练数据拆分成若干个小的数组，每个数组的大小为 `batch_size`。然后，我们不再一次性使用所有示例来训练网络，而是一次使用一个批量。
- en: The advantages of this are less memory is required and faster learning is achieved.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的优点是，所需的内存更少，学习速度更快。
- en: The `weight` and `biases` are declared as `tf.Variable`, which holds a certain
    value during training. This value can be modified. When a variable is first introduced,
    one should specify an initial value, type, and shape. The type and shape remain
    constant and cannot be changed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`weight` 和 `biases` 被声明为 `tf.Variable`，它在训练过程中持有某个值，这个值是可以修改的。当一个变量首次引入时，应该指定其初始值、类型和形状。类型和形状保持不变，不能更改。'
- en: Next, let's build the RNN cell. If you recall from [Chapter 1](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml),
    *Introducing Recurrent Neural Networks*, an input at time step, *t,* is plugged
    into an RNN cell to produce an output, ![](img/f8bef6b8-45e0-40b0-bec8-5c67c9bffec7.png),
    and a hidden state, ![](img/81fbe1cb-da8c-49f8-885c-0de64e94c48a.png). Then, the
    hidden state and the new input at time step (`*t*+1`) are plugged into a new RNN
    cell (which shares the same weights and biases as the previous). It produces its
    own output,  ![](img/ad4e2cd0-a77e-469c-8581-ff28b0f6d3c0.png), and hidden state, ![](img/76cb085f-239b-41b8-bd98-786096161a07.png).
    This pattern is repeated for every time step.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们构建 RNN 单元。如果你回忆起[第 1 章](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml)，*引入递归神经网络*，在时间步长
    *t* 时，输入被传入 RNN 单元，以产生一个输出，![](img/f8bef6b8-45e0-40b0-bec8-5c67c9bffec7.png)，以及一个隐藏状态，![](img/81fbe1cb-da8c-49f8-885c-0de64e94c48a.png)。然后，隐藏状态和时间步长
    `*t*+1` 时的新输入被传入一个新的 RNN 单元（该单元与前一个共享相同的权重和偏差）。它会产生自己的输出，![](img/ad4e2cd0-a77e-469c-8581-ff28b0f6d3c0.png)，以及隐藏状态，![](img/76cb085f-239b-41b8-bd98-786096161a07.png)。这个模式会在每个时间步长中重复。
- en: 'With TensorFlow, the previous operation is just a single line:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow，之前的操作仅需要一行代码：
- en: '`rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units=num_hidden_units)`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`rnn_cell = tf.contrib.rnn.BasicRNNCell(num_units=num_hidden_units)`'
- en: As you already know, each cell requires an activation function that is applied
    to the hidden state. By default, TensorFlow chooses **tanh** (perfect for our
    use case) but you can specify any that you wish. Just add an additional parameter
    called `activation` .
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经知道的，每个单元都需要一个应用于隐状态的激活函数。默认情况下，TensorFlow选择**tanh**（非常适合我们的用例），但你可以指定任何你想要的函数。只需添加一个名为`activation`的额外参数。
- en: Both in `weights` and in `rnn_cell`, you can see a parameter called  `num_hidden_units` . 
    As stated here ([https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell](https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell)),
    the `num_hidden_units` is a direct representation of the learning capacity of
    a neural network. It determines the dimensionality of both the memory state, ![](img/eaa3fc86-2047-4889-a480-911a7fced27d.png),
    and the output, ![](img/28afa699-8307-4672-bdb3-518a6fa25397.png) .
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在`weights`和`rnn_cell`中，你可以看到一个名为`num_hidden_units`的参数。正如这里所述（[https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell](https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell)），`num_hidden_units`是神经网络学习能力的直接表现。它决定了记忆状态的维度，![](img/eaa3fc86-2047-4889-a480-911a7fced27d.png)，以及输出的维度，![](img/28afa699-8307-4672-bdb3-518a6fa25397.png)。
- en: 'The next step is to produce the output of the network. This can also be implemented
    with a single line:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是生成网络的输出。这也可以通过一行代码实现：
- en: '`outputs, state = tf.nn.dynamic_rnn(rnn_cell, inputs=X, dtype=tf.float32)`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`outputs, state = tf.nn.dynamic_rnn(rnn_cell, inputs=X, dtype=tf.float32)`'
- en: 'Since `X` is a batch of input sequences, then `outputs` represents a batch
    of outputs at every time step in all sequences. To evaluate the prediction, we
    need the value of the last time step for every output in the batch. This happens
    in three steps, explained in the following bulleted examples:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`X`是一个输入序列的批次，因此`outputs`表示每个时间步在所有序列中的输出批次。为了评估预测，我们需要批次中每个输出的最后一个时间步的值。这可以通过以下三步来实现，如下所述：
- en: 'We get the values from the last time step: `outputs = tf.transpose(outputs,
    [1, 0, 2])`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从最后一个时间步获得值：`outputs = tf.transpose(outputs, [1, 0, 2])`
- en: This would reshape the output's tensor from (`1000, 50, 24`) to (`50, 1,000,
    24`) so that the outputs from the last time step in every sequence are accessible
    to be gathered using the following: `last_output = tf.gather(outputs, int(outputs.get_shape()[0])
    - 1)`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把输出的张量从（`1000, 50, 24`）重塑为（`50, 1,000, 24`），以便可以使用以下方法获取每个序列中最后一个时间步的输出：`last_output
    = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)`。
- en: 'Let''s review the following diagram to understand how this `last_output` is
    obtained:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾以下图表，以理解如何获得这个`last_output`：
- en: The previous diagram shows how one input example of `50` steps is plugged into
    the network. This operation should be done `1,000` times for each individual example
    having `50` steps but, for the sake of simplicity, we are showing only one example.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表展示了如何将一个输入示例的`50`个时间步输入到网络中。这个操作应该对每个具有`50`个时间步的独立示例执行`1,000`次，但为了简便起见，我们这里只展示了一个示例。
- en: After iteratively going through each time step, we produce `50` outputs, each
    one having the dimensions (`24, 1`). So, for one example of `50` input time steps,
    we produce `50` output steps. Presenting all of the outputs mathematically results
    in a (`1,000, 50, 24`) matrix. The height of the matrix is `1,000`—the number
    of individual examples. The width of the matrix is `50`—the number of time steps
    for each example. The depth of the matrix is `24`—the dimension of each element.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在迭代地遍历每个时间步后，我们产生`50`个输出，每个输出的维度为（`24, 1`）。因此，对于一个具有`50`个输入时间步的示例，我们会产生`50`个输出时间步。将所有输出以数学形式表示时，得到一个（`1,000,
    50, 24`）的矩阵。矩阵的高度为`1,000`——即单独的示例数量。矩阵的宽度为`50`——即每个示例的时间步数量。矩阵的深度为`24`——即每个元素的维度。
- en: To make a prediction, we only care about `output_last` at each example, and
    since the number of examples is `1,000`, we only need 1,000 output values. As
    seen in the previous example, we transpose the matrix (`1000, 50, 24`) into (`50,
    1000, 24`), which will make it easier to get `output_last` from each example.
    Then, we use `tf.gather` to obtain the `last_output` tensor which has size of
    (`1000, 24, 1`).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做出预测，我们只关心每个示例中的`output_last`，由于示例的数量为`1,000`，我们只需要`1,000`个输出值。正如在前面的示例中所见，我们将矩阵（`1000,
    50, 24`）转置为（`50, 1000, 24`），这样可以更容易地从每个示例中获得`output_last`。然后，我们使用`tf.gather`来获取`last_output`张量，其大小为（`1000,
    24, 1`）。
- en: 'Final lines of building our graph include:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 构建图的最后几行包括：
- en: 'We predict the output of the particular sequence:'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们预测特定序列的输出：
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Using the newly obtained tensor, `last_output` , we can calculate a prediction
    using the weights and biases.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新获得的张量`last_output`，我们可以利用权重和偏置来计算预测值。
- en: 'We evaluate the output based on the expected value:'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们根据期望值来评估输出：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can use the popular cross entropy loss function in a combination with `softmax`.
    If you recall from [Chapter 1](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml), *Introducing
    Recurrent Neural Networks*, the `softmax` function transforms a tensor to emphasize
    the largest values and suppress values that are significantly below the maximum
    value. This is done by normalizing the values from the initial array to ones that
    add up to `1`. For example, the input `[0.1, 0.2, 0.3, 0.4, 0.1, 0.2, 0.3]` becomes
    [`0.125, 0.138, 0.153, 0.169, 0.125, 0.138, 0.153`]. The cross entropy is a loss
    function that computes the difference between the `label` (expected values) and
    `logits` (predicted values).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将流行的交叉熵损失函数与`softmax`结合使用。如果你还记得[第一章](d6266376-9b8b-4d69-925b-a4e56307951b.xhtml)《介绍递归神经网络》中的内容，`softmax`函数会将张量转换为强调最大值并抑制显著低于最大值的值。这是通过将初始数组中的值归一化为总和为`1`的数值来实现的。例如，输入`[0.1,
    0.2, 0.3, 0.4, 0.1, 0.2, 0.3]`变为[`0.125, 0.138, 0.153, 0.169, 0.125, 0.138, 0.153`]。交叉熵是一个损失函数，用于计算`label`（期望值）与`logits`（预测值）之间的差异。
- en: Since `tf.nn.softmax_cross_entropy_with_logits_v2` returns a 1-D tensor of a
    length of `batch_size` (declared below), we use `tf.reduce_mean` to compute the
    mean of all elements in that tensor.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`tf.nn.softmax_cross_entropy_with_logits_v2`返回一个长度为`batch_size`（在下面声明）的1维张量，我们使用`tf.reduce_mean`来计算该张量中所有元素的平均值。
- en: 'As a final step, we will see how TensorFlow makes it easy for us to optimize
    the weights and biases. Once we have obtained the loss function, we need to perform
    a backpropagation algorithm, adjusting the weights and biases to minimize the
    loss. This can be done in the following way:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步，我们将看到TensorFlow如何简化我们优化权重和偏置的过程。一旦我们获得了损失函数，就需要执行反向传播算法，调整权重和偏置以最小化损失。这可以通过以下方式完成：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`learning_rate` is one of the model''s hyperparameters and is used when optimizing
    the loss function. Tuning this value is essential for better performance, so feel
    free to adjust it and evaluate the results.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate`是模型的超参数之一，在优化损失函数时使用。调节这个值对提高性能至关重要，因此可以随意调整并评估结果。'
- en: Minimizing the error of the loss function is done using an Adam optimizer. Here
    ([https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow](https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow))
    is a good explanation of why it is preferred over the Gradient descent.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化损失函数的误差是通过使用Adam优化器完成的。这里的([https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow](https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow))提供了一个很好的解释，说明了为什么它优于梯度下降法。
- en: 'We have just built the architecture of our recurrent neural network. Let''s
    put everything together, as shown in the following example:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚构建了递归神经网络的架构。现在，让我们将所有内容整合起来，如下例所示：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The next task is to train the neural network using the TensorFlow graph in combination
    with the previously generated data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步任务是使用TensorFlow图与之前生成的数据结合训练神经网络。
- en: Training the RNN
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练RNN
- en: In this section, we will go through the second part of a TensorFlow program—executing
    the graph with a predefined data. For this to happen, we will use the `Session`
    object, which encapsulates an environment in which the tensor objects are executed.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讲解TensorFlow程序的第二部分——执行带有预定义数据的计算图。为了实现这一点，我们将使用`Session`对象，它封装了一个执行张量对象的环境。
- en: 'The code for our training is shown in the following example:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练的代码如下所示：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: First, we initialize the batch size. At each training step, the network is tuned,
    based on examples from the chosen batch. Then, we compute the number of batches
    as well as the number of epochs—this determines how many times our model should
    loop through the training set. `tf.Session()` encapsulates the code in a TensorFlow
    `Session` and `sess.run(tf.global_variables_initializer())` ([https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer](https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer))
    makes sure all variables hold their values.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化批次大小。在每次训练步骤中，网络会根据所选批次中的示例进行调优。然后，我们计算批次数量以及迭代次数——这决定了我们的模型应该遍历训练集多少次。`tf.Session()`将代码封装在TensorFlow的`Session`中，而`sess.run(tf.global_variables_initializer())`（[https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer](https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer)）确保所有变量都保持其值。
- en: Then, we store an individual batch from the training set in `training_x` and `training_y` .
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将训练集中的一个独立批次存储在`training_x`和`training_y`中。
- en: The last, and most, important part of training the network comes with the usage
    of  `sess.run()` . By calling this function, you can compute the value of any
    tensor. In addition, one can specify as many arguments as you want by ordering
    them in a list—in our case, we have specified the optimizer and loss function.
    Remember how, while building the graph, we created placeholders for holding the
    values of the current batch? These values should be mentioned in the `feed_dict`
    parameter when running `Session`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 训练网络的最后一个也是最重要的部分是使用`sess.run()`。通过调用这个函数，你可以计算任何张量的值。此外，你可以按照顺序在列表中指定任意数量的参数——在我们的例子中，我们指定了优化器和损失函数。还记得在构建图时，我们为当前批次的值创建了占位符吗？这些值应当在运行`Session`时通过`feed_dict`参数传入。
- en: Training this network can take around four or five hours. You can verify that
    it is learning by examining the value of the loss function. If its value decreases,
    then the network is successfully modifying the weights and biases. If the value
    is not decreasing, you most likely need to make some additional changes to optimize
    the performance. These will be explained in Chapter 6, *Improving Your RNN Performance*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个网络大约需要四到五个小时。你可以通过检查损失函数的值来验证它是否在学习。如果值在减小，那么网络正在成功地调整权重和偏置。如果值没有减小，你可能需要做一些额外的调整来优化性能。这些将在第六章中讲解，*提升你的RNN性能*。
- en: Evaluating the predictions
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估预测结果
- en: 'Testing the model using a fresh new example can be accomplished in the following
    way:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一个全新的示例来测试模型可以通过以下方式完成：
- en: '[PRE12]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is where `test_example` is an array of a size of  (`1 x num_classes x 1`).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`test_example`是一个大小为（`1 x num_classes x 1`）的数组。
- en: 'Let `test_example` be as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 设`test_example`为以下内容：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The sum of all elements in the above array is equal to `30`. With the last line, `prediction_result[0].argsort()[-1:][::-1``]`, we
    can find the index of the largest number. The index would tell us the sum of the
    sequence. As a last step, we need to find the remainder when this number is divided
    by `2`. This will give us the parity of the sequence.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上述数组中所有元素的总和等于`30`。通过最后一行`prediction_result[0].argsort()[-1:][::-1]`，我们可以找到最大数字的索引。这个索引将告诉我们序列的和。最后一步，我们需要找到这个数字除以`2`后的余数，这将给我们序列的奇偶性。
- en: Both training and evaluation are done together after you run `python3 ch2_task.py`.
    If you want to only do evaluation, comment out the lines between `70` and `91`
    from the program and run it again.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和评估是在你运行`python3 ch2_task.py`后一起进行的。如果你只想做评估，可以将程序中第70行到第91行的代码注释掉，再重新运行。
- en: Summary
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you explored how to build a simple recurrent neural network
    to solve the problem of identifying sequence parity. You obtained a brief understanding
    of the TensorFlow library and how it can be utilized for building deep learning
    models. I hope the study of this chapter leaves you more confident in your deep
    learning knowledge, as well as excited to learn and grow more in this field.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你探索了如何构建一个简单的循环神经网络来解决识别序列奇偶性的问题。你对TensorFlow库及其在构建深度学习模型中的应用有了简要的了解。希望本章的学习能够让你对深度学习的知识更加自信，并激励你在这个领域继续学习和成长。
- en: In the next chapter, you will go a step further by implementing a more sophisticated
    neural network for the task of generating text. You will gain both theoretical
    and practical experience. This will result in you learning about a new type of
    network, GRU, and understanding how to implement it in TensorFlow. In addition,
    you will face the challenge of formatting your input text correctly as well as
    using it for training the TensorFlow graph.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将通过实现一个更复杂的神经网络来生成文本。你将获得理论和实践经验。这将使你学习到一种新的网络类型——GRU，并理解如何在 TensorFlow
    中实现它。此外，你还将面临正确格式化输入文本的挑战，并将其用于训练 TensorFlow 图。
- en: I can assure you that an exciting learning experience is coming, so I cannot
    wait for you to be part of it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以向你保证，激动人心的学习经历即将到来，我迫不及待希望你成为其中的一部分。
- en: External links
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 外部链接
- en: Parity: [http://mathworld.wolfram.com/Parity.html](http://mathworld.wolfram.com/Parity.html)
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奇偶性：[http://mathworld.wolfram.com/Parity.html](http://mathworld.wolfram.com/Parity.html)
- en: Request for Research 2.0 by OpenAI: [https://blog.openai.com/requests-for-research-2/ ](https://blog.openai.com/requests-for-research-2/)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 研究请求 2.0：[https://blog.openai.com/requests-for-research-2/](https://blog.openai.com/requests-for-research-2/)
- en: Eager execution documentation:[ https://www.tensorflow.org/guide/eager](https://www.tensorflow.org/guide/eager)
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迫切执行文档：[https://www.tensorflow.org/guide/eager](https://www.tensorflow.org/guide/eager)
- en: Eager execution (TensorFlow Conference 2018): [https://www.youtube.com/watch?v=T8AW0fKP0Hs](https://www.youtube.com/watch?v=T8AW0fKP0Hs)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迫切执行（TensorFlow 会议 2018）：[https://www.youtube.com/watch?v=T8AW0fKP0Hs](https://www.youtube.com/watch?v=T8AW0fKP0Hs)
- en: Python installation: [https://www.python.org/downloads/](https://www.python.org/downloads/)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 安装：[https://www.python.org/downloads/](https://www.python.org/downloads/)
- en: Understanding `num_hidden_units` : [https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell](https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解`num_hidden_units`：[https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell](https://stackoverflow.com/questions/37901047/what-is-num-units-in-tensorflow-basiclstmcell)
- en: Adam versus Gradient descent optimizer: [https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow](https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam 与梯度下降优化器：[https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow](https://stats.stackexchange.com/questions/184448/difference-between-gradientdescentoptimizer-and-adamoptimizer-tensorflow)
- en: Understanding `sess.run(tf.global_variables_initializer()`: [https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer](https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解`sess.run(tf.global_variables_initializer())`：[https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer](https://stackoverflow.com/questions/44433438/understanding-tf-global-variables-initializer)
