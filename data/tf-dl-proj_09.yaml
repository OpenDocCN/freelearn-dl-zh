- en: Building a TensorFlow Recommender System
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个 TensorFlow 推荐系统
- en: A recommender system is an algorithm that makes personalized suggestions to
    users based on their past interactions with the software. The most famous example
    is the "customers who bought X also bought Y" type of recommendation on Amazon
    and other e-commerce websites.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是一种基于用户与软件的过去交互，向用户提供个性化建议的算法。最著名的例子就是亚马逊和其他电子商务网站上那种“购买了 X 的用户也购买了 Y”的推荐。
- en: 'In the past few years, recommender systems have gained a lot of importance:
    it has become clear for the online businesses that the better the recommendations
    they give on their websites, the more money they make. This is why today almost
    every website has a block with personalized recommendations.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，推荐系统变得越来越重要：在线企业已明确认识到，网站上提供的推荐越好，赚的钱就越多。这也是为什么今天几乎每个网站都有一个个性化推荐的模块。
- en: In this chapter, we will see how we can use TensorFlow to build our own recommender
    system.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到如何使用 TensorFlow 来构建自己的推荐系统。
- en: 'In particular, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将特别涵盖以下主题：
- en: Basics of recommender systems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统基础
- en: Matrix Factorization for recommender systems
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统中的矩阵分解
- en: Bayesian Personalized Ranking
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯个性化排序
- en: Advanced recommender systems based on Recurrent Neural Nets
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于循环神经网络的高级推荐系统
- en: By the end of this chapter, you will know how to prepare data for training a
    recommender system, how to build your own models with TensorFlow, and how to perform
    a simple evaluation of the quality of these models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将知道如何准备数据以训练推荐系统，如何使用 TensorFlow 构建自己的模型，并如何对这些模型的质量进行简单评估。
- en: Recommender systems
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统
- en: The task of a **recommender system** is to take a list of all possible items
    and rank them according to preferences of particular users. This ranked list is
    referred to as a personalized ranking, or, more often, as a **recommendation**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**推荐系统**的任务是根据特定用户的偏好，从所有可能的项目中排名，并生成个性化的排名列表，通常被称为**推荐**。'
- en: 'For example, a shopping website may have a section with recommendations where
    users can see items that they may find relevant and could decide to buy. Websites
    selling tickets to concerts may recommend interesting shows, and an online music
    player may suggest songs that the user is likely to enjoy. Or a website with online
    courses, such as [Coursera.org](http://Coursera.org), may recommend a course similar
    to ones the user has already finished:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个购物网站可能会有一个推荐部分，用户可以看到他们可能感兴趣的商品，并可能决定购买。销售演唱会门票的网站可能会推荐有趣的演出，在线音乐播放器可能会推荐用户可能喜欢的歌曲。或者，像[Coursera.org](http://Coursera.org)这样的在线课程网站，可能会推荐与用户已经完成的课程类似的课程：
- en: '![](img/02fa716f-a5ef-4dc2-850d-b3d5c2f17f61.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02fa716f-a5ef-4dc2-850d-b3d5c2f17f61.png)'
- en: Course recommendation on website
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 网站上的课程推荐
- en: 'The recommendations are typically based on historical data: the past transaction
    history, visits, and clicks that the users have made. So, a recommender system
    is a system that takes this historical data and uses machine learning to extract
    patterns in the behavior of the users and based on that comes up with the best
    recommendations.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐通常基于历史数据：用户的过去交易历史、访问记录和点击行为。因此，推荐系统是一个利用历史数据，通过机器学习提取用户行为模式，并根据这些模式提供最佳推荐的系统。
- en: 'Companies are quite interested in making the recommendations as good as possible:
    this usually makes users engaged by improving their experience. Hence, it brings
    the revenue up. When we recommend an item that the user otherwise would not notice,
    and the user buys it, not only do we make the user satisfied, but we also sell
    an item that we would not otherwise have sold.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 公司非常关注如何使推荐尽可能好：这通常通过改善用户体验来增加用户参与度，从而带动收入增长。当我们推荐一个用户本来不会注意到的商品，而用户最终购买了它时，不仅使用户感到满意，而且我们还卖出了本来不会卖出去的商品。
- en: This chapter project is about implementing multiple recommender system algorithms
    using TensorFlow. We will start with classical time-proven algorithms and then
    go deeper and try a more complex model based on RNN and LSTM.  For each model
    in this chapter, we will first give a short introduction and then we implement
    this model in TensorFlow.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章项目将实现多个推荐系统算法，使用 TensorFlow。我们将从经典的经过时间考验的算法开始，然后深入探讨并尝试基于 RNN 和 LSTM 的更复杂模型。对于本章中的每个模型，我们将首先简要介绍，然后在
    TensorFlow 中实现该模型。
- en: To illustrate these ideas, we use the Online Retail Dataset from the UCI Machine
    Learning repository. This dataset can be downloaded from [http://archive.ics.uci.edu/ml/datasets/online+retail](http://archive.ics.uci.edu/ml/datasets/online+retail).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这些概念，我们使用 UCI 机器学习库中的在线零售数据集。这个数据集可以从 [http://archive.ics.uci.edu/ml/datasets/online+retail](http://archive.ics.uci.edu/ml/datasets/online+retail)
    下载。
- en: 'The dataset itself is an Excel file with the following features:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集本身是一个包含以下特征的 Excel 文件：
- en: '`InvoiceNo`: The invoice number, which is used to uniquely identify each transaction'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InvoiceNo`：发票号，用于唯一标识每一笔交易'
- en: '`StockCode`: The code of the purchased item'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StockCode`：购买商品的代码'
- en: '`Description`: The name of the product'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Description`：产品名称'
- en: '`Quantity`: The number of times the item is purchased in the transaction'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Quantity`：交易中购买商品的数量'
- en: '`UnitPrice`: Price per item'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UnitPrice`：每件商品的价格'
- en: '`CustomerID`: The ID of the customer'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CustomerID`：客户的 ID'
- en: '`Country`: The name of the customer''s country of the customer'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Country`：客户所在国家的名称'
- en: It consists of 25,900 transactions, with each transaction containing around
    20 items. This makes approximately 540,000 items in total. The recorded transactions
    were made by 4,300 users starting from December 2010 up until December 2011.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 它包含 25,900 个交易，每个交易大约有 20 件商品，总计约 540,000 件商品。记录的交易来自 4,300 名用户，交易时间从 2010 年
    12 月开始，到 2011 年 12 月结束。
- en: 'To download the dataset, we can either use the browser and save the file or
    use `wget`:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载数据集，我们可以使用浏览器保存文件，或者使用`wget`：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For this project, we will use the following Python packages:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们将使用以下 Python 包：
- en: '`pandas` for reading the data'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` 用于读取数据'
- en: '`numpy` and `scipy` for numerical data manipulations'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy` 和 `scipy` 用于数值数据操作'
- en: '`tensorflow` for creating the models'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow` 用于创建模型'
- en: '`implicit` for the baseline solution'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`implicit` 用于基准解决方案'
- en: '[optional] `tqdm` for monitoring the progress'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可选] `tqdm` 用于监控进度'
- en: '[optional] `numba` for speeding up the computations'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[可选] `numba` 用于加速计算'
- en: 'If you use Anaconda, then you should already have `numba` installed, but if
    not, a simple `pip install numba` will get this package for you. To install `implicit`,
    we again use `pip`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Anaconda，那么你应该已经安装了`numba`，但如果没有，简单地运行`pip install numba`就可以安装这个包。要安装`implicit`，我们再次使用`pip`：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once the dataset is downloaded and the packages are installed, we are ready
    to start. In the next section, we will review the Matrix Factorization techniques,
    then prepare the dataset, and finally implement some of them in TensorFlow.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集下载完成并且包安装好，我们就可以开始了。在接下来的部分，我们将回顾矩阵分解技术，然后准备数据集，最后在 TensorFlow 中实现其中的一些。
- en: Matrix factorization for recommender systems
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统中的矩阵分解
- en: In this section, we will go over traditional techniques for recommending systems.
    As we will see, these techniques are really easy to implement in TensorFlow, and
    the resulting code is very flexible and easily allows modifications and improvements.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍传统的推荐系统技术。正如我们将看到的，这些技术在 TensorFlow 中实现非常简单，生成的代码非常灵活，容易进行修改和改进。
- en: For this section, we will use the Online Retail Dataset. We first define the
    problem we want to solve and establish a few baselines. Then we implement the
    classical Matrix factorization algorithm as well as its modification based on
    Bayesian Personalized Ranking.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一部分，我们将使用在线零售数据集。我们首先定义我们想要解决的问题，并建立一些基准。然后我们实现经典的矩阵分解算法，以及基于贝叶斯个性化排序的修改版。
- en: Dataset preparation and baseline
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集准备和基准
- en: Now we are ready to start building a recommender system.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备开始构建推荐系统了。
- en: 'First, declare the imports:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，声明导入：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let us read the dataset:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取数据集：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Reading `xlsx` files may take a while. To save time when we next want to read
    the file, we can save the loaded copy into a `pickle` file:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 读取 `xlsx` 文件可能需要一些时间。为了节省时间，在下次读取该文件时，我们可以将加载的副本保存到 `pickle` 文件中：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This file is a lot faster to read, so for loading, we should use the pickled
    version:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件读取起来更快，因此在加载时我们应该使用 pickle 版本：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once the data is loaded, we can have a look at the data. We can do this by
    invoking the `head` function:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据加载完成，我们可以查看数据。我们可以通过调用 `head` 函数来做到这一点：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We then see the following table:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到以下表格：
- en: '![](img/52e3542d-e4a4-4604-9dc4-0a0ea1fdae2f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/52e3542d-e4a4-4604-9dc4-0a0ea1fdae2f.png)'
- en: 'If we take a closer look at the data, we can notice the following problems:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们仔细查看数据，可以发现以下问题：
- en: The column names are in capital letters. This is a bit unusual, so we may lowercase
    them.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列名是大写的，这有点不常见，所以我们可以将其转换为小写。
- en: 'Some of the transactions are returns: they are not of interest to us, so we
    should filter them out.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些交易是退货：它们对我们不重要，因此我们应该将其过滤掉。
- en: Finally, some of the transactions belong to unknown users. We can assign some
    common ID for these users, for example, `-1`. Also, unknown users are encoded
    as `NaN`s, this is why the `CustomerID` column is encoded as float—so we need
    to convert it to an integer.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，一些交易属于未知用户。我们可以为这些用户分配一个通用ID，例如`-1`。同时，未知用户被编码为`NaN`，这就是为什么`CustomerID`列被编码为浮动类型——因此我们需要将其转换为整数。
- en: 'These problems can be fixed with the following code:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题可以通过以下代码修复：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we should encode all item IDs (`stockcode`) with integers. One of the
    ways to do it is to build a mapping from each code to some unique index number:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应该用整数编码所有商品ID（`stockcode`）。一种方法是为每个代码构建一个与唯一索引号的映射：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now after we have encoded the items, we can split the dataset into train, validation,
    and test parts. Since we have e-commerce transactions data, the most sensible
    way to do the split is based on time. So we will use:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对商品进行编码，可以将数据集划分为训练集、验证集和测试集。由于我们有电商交易数据，最合理的划分方式是按时间划分。所以我们将使用：
- en: '**Training set**: before 2011.10.09 (around 10 months of data, approximately
    378,500 rows)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：2011年10月9日之前（大约10个月的数据，大约378,500行）'
- en: '**Validation set**: between 2011.10.09 and 2011.11.09 (one month of data, approximately
    64,500 rows)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**：2011年10月9日到2011年11月9日之间（一个月的数据，大约64,500行）'
- en: '**Test set**: after 2011.11.09 (also one month, approximately 89,000 rows)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**：2011年11月9日之后（同样是一个月的数据，大约89,000行）'
- en: 'For that we just filter the dataframes:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们只需过滤数据框：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this section, we will consider the following (very simplified) recommendation
    scenario:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考虑以下（非常简化的）推荐场景：
- en: The user enters the website.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户访问网站。
- en: We present five recommendations.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们提供五个推荐。
- en: The user assesses the lists, maybe buys some things from there, and then continues
    shopping as usual.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户查看列表，可能从中购买一些商品，然后像往常一样继续购物。
- en: So we need to build a model for the second step. To do so, we use the training
    data and then simulate the second and third steps using the validation set. To
    evaluate whether our recommendation was good or not, we count the number of recommended
    items that the user has actually bought.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要为第二步构建一个模型。为此，我们使用训练数据，然后利用验证集模拟第二步和第三步。为了评估我们的推荐是否有效，我们计算用户实际购买的推荐商品数量。
- en: Our evaluation measure is then the number of successful recommendations (the
    items the user has actually bought) divided by the number of total recommendations
    we made. This is called **precision—**a common measure of evaluating the performance
    of machine learning models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估指标是成功推荐的数量（即用户实际购买的商品）与我们做出的总推荐数量的比值。这叫做**精准度**——这是评估机器学习模型性能的常用指标。
- en: For this project we use precision. Of course, it is a rather simplistic way
    of evaluating the performance, and there are different ways of doing this. Other
    metrics you may want to use include **MAP** (**Mean Average Precision**), **NDCG**
    (**Normalized Discounted Cumulative Gain**), and so on. For simplicity, however,
    we do not use them in this chapter.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们使用精准度。 当然，这是一个相当简化的评估性能的方法，实际上有不同的评估方式。你可能想使用的其他指标包括**MAP**（**平均精准度**），**NDCG**（**标准化折扣累积增益**）等。不过为了简化，我们在本章中并未使用它们。
- en: Before we jump into using machine learning algorithm for this task, let us first
    establish a basic baseline. For example, we can calculate how many times each
    item was bought, then take the most frequent five items, and recommend these items
    to all the users.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始使用机器学习算法处理此任务之前，先建立一个基本的基准。比如，我们可以计算每个商品的购买次数，然后选出购买次数最多的五个商品，推荐给所有用户。
- en: 'With pandas it is easy to do:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pandas很容易做到：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This gives us an array of integers—`stockcode` codes:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们一个整数数组——`stockcode`代码：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now we use this array to recommend it to all the users. So we repeat the `top`
    array as many times as there are transactions in the validation dataset, and then
    we use this as the recommendations and calculate the precision metric to evaluate
    the quality.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用这个数组推荐给所有用户。所以我们将`top`数组重复与验证数据集中交易的数量相同的次数，然后使用这些作为推荐，并计算精准度指标来评估质量。
- en: 'For repeating we use the `tile` function from numpy:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于重复项，我们使用 numpy 的`tile`函数：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `tile` function takes in an array and repeats it `num_group` times. After
    reshaping, it gives us the following array:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`tile`函数接受一个数组并将其重复`num_group`次。经过重塑后，它会给我们以下数组：'
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now we are ready to calculate the precision of this recommendation.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好计算这个推荐系统的精度了。
- en: 'However, there is a complication: the way the items are stored makes it difficult
    to calculate the number of correctly classified elements per group. Using `groupby`
    from pandas is one way of solving the problem:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个复杂的问题：由于项目存储的方式，使得计算每个组内正确分类元素的数量变得困难。使用 pandas 的`groupby`是解决这个问题的一种方法：
- en: Group by `invoiceno` (this is our transaction ID)
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照`invoiceno`（即我们的交易 ID）分组
- en: For each transaction make a recommendation
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每个交易做出推荐
- en: Record the number of correct predictions per each group
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录每个组的正确预测数量
- en: Calculate the overall precision
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算总体精度
- en: However, this way is often very slow and inefficient. It may work fine for this
    particular project, but for slightly larger datasets it becomes a problem.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方式通常非常慢且低效。对于这个特定项目可能没问题，但对于稍微大的数据集，它就成为了一个问题。
- en: 'The reason it is slow is the way `groupby` is implemented in pandas: it internally
    performs sorting, which we do not need. However, we can improve the speed by exploiting
    the way the data is stored: we know that the elements of our dataframe are always
    ordered. That is, if a transaction starts at a certain row number `i`, then it
    ends at the number `i + k`, where `k` is the number of items in this transaction.
    In other words, all the rows between `i` and `i + k` belong to the same `invoiceid`.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 它慢的原因在于 pandas 中`groupby`的实现方式：它在内部执行排序，而我们并不需要这样做。然而，我们可以通过利用数据存储的方式来提高速度：我们知道数据框中的元素总是有序的。也就是说，如果一个交易从某个行号`i`开始，那么它将在行号`i
    + k`结束，其中`k`是该交易中的项目数。换句话说，`i`和`i + k`之间的所有行都属于同一个`invoiceid`。
- en: So we need to know where each transaction starts and where it ends. For this
    purpose, we keep a special array of length `n + 1`, where `n` is the number of
    groups (transactions) we have in our dataset.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要知道每个交易的开始和结束位置。为此，我们保持一个长度为`n + 1`的特殊数组，其中`n`是数据集中组（交易）的数量。
- en: 'Let us call this array `indptr`. For each transaction `t`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这个数组为`indptr`。对于每个交易`t`：
- en: '`indptr[t]` returns the number of the row in the dataframe where the transaction
    starts'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`indptr[t]`返回交易开始的行号'
- en: '`indptr[t + 1]` returns the row where it ends'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`indptr[t + 1]`返回交易结束的行号'
- en: 'This way of representing the groups of various length is inspired by the CSR
    algorithm—Compressed Row Storage (sometimes Compressed Sparse Row). It is used
    to represent sparse matrices in memory. You can read about it more in the Netlib
    documentation—[http://netlib.org/linalg/html_templates/node91.html](http://netlib.org/linalg/html_templates/node91.html).
    You may also recognize this name from scipy—it is one of the possible ways of
    representing matrices in the `scipy.sparse` package: [https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示不同长度组的方式灵感来自于 CSR 算法——压缩行存储（有时称为压缩稀疏行）。它用于在内存中表示稀疏矩阵。你可以在 Netlib 文档中阅读更多内容——[http://netlib.org/linalg/html_templates/node91.html](http://netlib.org/linalg/html_templates/node91.html)。你也许在
    scipy 中也会认出这个名字——它是`scipy.sparse`包中表示矩阵的几种方式之一：[https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html)。
- en: 'Creating such arrays is not difficult in Python: we just need to see where
    the current transaction finishes and the next one starts. So at each row index,
    we can compare the current index with the previous one, and if it is different,
    record the index. This can be done efficiently using the `shift` method from pandas:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中创建这样的数组并不困难：我们只需要查看当前交易的结束位置和下一个交易的开始位置。所以，在每一行索引处，我们可以将当前索引与前一个索引进行比较，如果不同，就记录该索引。这可以通过使用
    pandas 的`shift`方法高效完成：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This way we get the pointers array for the validation set:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们就得到了验证集的指针数组：
- en: '[PRE15]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we can use it for the `precision` function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在`precision`函数中使用它：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here the logic is straightforward: for each transaction we check how many items
    we predicted correctly. The total amount of correctly predicted items is stored
    in `tp`. At the end we divide `tp` by the total number of predictions, which is
    the size of the prediction matrix, that is, number of transactions times five
    in our case.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的逻辑很简单：对于每一个交易，我们检查我们预测正确的物品数量。所有预测正确的物品总数存储在`tp`中。最后，我们将`tp`除以预测的总数，即预测矩阵的大小，也就是在我们的例子中，交易次数乘以五。
- en: Note the `@njit` decorator from numba. This decorator tells numba that the code
    should be optimized. When we invoke this function for the first time, numba analyzes
    the code and uses the **just-in-time** (**JIT**) compiler to translate the function
    to native code. When the function is compiled, it runs multiple orders of magnitude
    faster—comparable to native code written in C.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 numba 的 `@njit` 装饰器。这个装饰器告诉 numba 代码应该被优化。当我们首次调用这个函数时，numba 会分析代码并使用 **即时编译（JIT）**
    编译器将函数转换为本地代码。当函数被编译后，它的执行速度比原来快了几个数量级——接近于用 C 编写的本地代码。
- en: Numba's `@jit` and `@njit` decorators give a very easy way to improve the speed
    of the code. Often it is enough just to put the `@jit` decorator on a function
    to see a significant speed-up. If a function takes time to compute, numba is a
    good way to improve the performance.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Numba 的 `@jit` 和 `@njit` 装饰器提供了一种非常简便的方式来提高代码的运行速度。通常，只需将 `@jit` 装饰器应用于函数，就能显著加速代码。如果一个函数计算耗时，numba
    是提高性能的一个好方法。
- en: 'Now we can check what is the precision of this baseline:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查这个基准的精度：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Executing this code should produce 0.064\. That is, in 6.4% of the cases we
    made the correct recommendation. This means that the user ended up buying the
    recommended item only in 6.4% cases.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码应得到 0.064。也就是说，在 6.4% 的情况下，我们做出了正确的推荐。这意味着用户仅在 6.4% 的情况下购买了推荐的物品。
- en: Now when we take a first look at the data and establish a simple baseline, we
    can proceed to more complex techniques such as matrix factorization.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在当我们初步查看数据并建立一个简单的基准后，我们可以继续使用更复杂的技术，如矩阵分解。
- en: Matrix factorization
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵分解
- en: In 2006 Netflix, a DVD rental company, organized the famous Netflix competition.
    The goal of this competition was to improve their recommender system. For this
    purpose, the company released a large dataset of movie ratings. This competition
    was notable in a few ways. First, the prize pool was one million dollars, and
    that was one of the main reasons it became famous. Second, because of the prize,
    and because of the dataset itself, many researchers invested their time into this
    problem and that significantly advanced the state of the art in recommender systems.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年，DVD租赁公司Netflix举办了著名的Netflix竞赛。竞赛的目标是改善他们的推荐系统。为此，公司发布了一个大规模的电影评分数据集。这个竞赛有几个显著特点。首先，奖金池为一百万美元，这是它成名的主要原因之一。其次，由于奖金以及数据集本身，许多研究人员投入了大量时间来解决这个问题，这大大推动了推荐系统领域的技术进步。
- en: It was the Netflix competition that showed that recommenders based on matrix
    factorization are very powerful, can scale to a large number of training examples,
    and yet are not very difficult to implement and deploy.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正是 Netflix 竞赛展示了基于矩阵分解的推荐系统非常强大，能够扩展到大量的训练样本，而且实现和部署并不难。
- en: The paper Matrix factorization techniques for recommender systems by Koren and
    others (2009) nicely summarizes the key findings, which we will also present in
    this chapter.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Koren 等人（2009）在论文《矩阵分解技术用于推荐系统》中很好地总结了关键发现，我们将在本章中呈现这些内容。
- en: 'Imagine we have the rating ![](img/98fd5f39-d35d-49b0-ad2c-aa30ed07d363.png)
    of a movie ![](img/66daca4b-c73b-447d-9ceb-bc300cb3586b.png) rated by user ![](img/8b07ace5-618f-41d5-91a5-e0506f1a457f.png).
    We can model this rating by:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有电影 ![](img/66daca4b-c73b-447d-9ceb-bc300cb3586b.png) 的评分 ![](img/98fd5f39-d35d-49b0-ad2c-aa30ed07d363.png)，由用户
    ![](img/8b07ace5-618f-41d5-91a5-e0506f1a457f.png) 评分。我们可以通过以下方式来建模这个评分：
- en: '![](img/7e8ade47-11e9-4810-84eb-fec7ff9d39d8.png).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/7e8ade47-11e9-4810-84eb-fec7ff9d39d8.png)。'
- en: 'Here we decompose the rating into four factors:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将评分分解为四个因素：
- en: '![](img/81c7ae03-43ea-4206-93ed-0b04b5824b18.png) is the global bias'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/81c7ae03-43ea-4206-93ed-0b04b5824b18.png) 是全局偏差'
- en: '![](img/9c6b6df6-205c-463c-9810-66c0dee4c15a.png) is the bias of the item ![](img/c9dd36a1-3eda-4010-83a0-1d46d6994654.png) 
    (in case of Netflix—movie)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/9c6b6df6-205c-463c-9810-66c0dee4c15a.png) 是物品 ![](img/c9dd36a1-3eda-4010-83a0-1d46d6994654.png)
    的偏差（在 Netflix 的情况下——电影）'
- en: '![](img/bc21c33d-09f1-4cef-9c53-b4e2defe35b5.png) is the bias of the user ![](img/885e8c87-0cac-43e4-9a28-b18d8be8041d.png)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/bc21c33d-09f1-4cef-9c53-b4e2defe35b5.png) 是用户的偏差 ![](img/885e8c87-0cac-43e4-9a28-b18d8be8041d.png)'
- en: '![](img/01c64fa6-c21c-4639-9d3e-0c2c329254e0.png) is the inner product between
    the user vector ![](img/4591a13d-fbfb-49ea-9011-b9e6bddaa573.png)and the item
    vector ![](img/71d94c57-a930-4e78-98d9-6e650a9621ee.png)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/01c64fa6-c21c-4639-9d3e-0c2c329254e0.png) 是用户向量 ![](img/4591a13d-fbfb-49ea-9011-b9e6bddaa573.png)和物品向量
    ![](img/71d94c57-a930-4e78-98d9-6e650a9621ee.png)之间的内积'
- en: The last factor—the inner product between the user and the item vectors—is the
    reason this technique is called **Matrix Factorization**.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的因子——用户和物品向量之间的内积——就是为什么这种技术被称为**矩阵因式分解**的原因。
- en: Let us take all the user vectors ![](img/5a5207bd-929a-4867-b5ea-5392fa16a78b.png),
    and put them into a matrix ![](img/0f196557-27ab-41c1-a800-ca0c164e82c9.png) as
    rows. We then will have an ![](img/a41ee6c4-0b66-4a39-8ae6-1d2c237311ac.png) matrix,
    where ![](img/571408bb-509f-49dd-8140-f48cca5813fa.png) is the number of users
    and ![](img/71640dff-fda0-4eec-9f3c-9be795501ceb.png) is the dimensionality of
    the vectors. Likewise, we can take the item vectors ![](img/79215541-8a5b-47d8-802b-eab5f5b9ea32.png)
    and put them into a matrix ![](img/1ad3a204-4a48-46d0-bb4a-fb25b210cf06.png) as
    rows. This matrix has the size ![](img/61057add-7c74-414c-ab15-6b95d56a7c36.png),
    where ![](img/1add554b-907f-4a93-8619-30e75057a150.png) is the number of items,
    and ![](img/8803cc3c-1035-450b-bebe-0b58c7efdbf2.png) is again the dimensionality
    of the vectors. The dimensionality ![](img/71640dff-fda0-4eec-9f3c-9be795501ceb.png)
    is a parameter of the model, which allows us to control how much we want to compress
    the information. The smaller ![](img/71640dff-fda0-4eec-9f3c-9be795501ceb.png)
    is, the less information is preserved from the original rating matrix.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有的用户向量 ![](img/5a5207bd-929a-4867-b5ea-5392fa16a78b.png) 放入一个矩阵 ![](img/0f196557-27ab-41c1-a800-ca0c164e82c9.png)
    中作为行。然后我们将得到一个 ![](img/a41ee6c4-0b66-4a39-8ae6-1d2c237311ac.png) 矩阵，其中 ![](img/571408bb-509f-49dd-8140-f48cca5813fa.png)
    是用户的数量， ![](img/71640dff-fda0-4eec-9f3c-9be795501ceb.png) 是向量的维度。同样地，我们可以将物品向量 ![](img/79215541-8a5b-47d8-802b-eab5f5b9ea32.png)
    放入一个矩阵 ![](img/1ad3a204-4a48-46d0-bb4a-fb25b210cf06.png) 中作为行。这个矩阵的大小为 ![](img/61057add-7c74-414c-ab15-6b95d56a7c36.png)，其中 ![](img/1add554b-907f-4a93-8619-30e75057a150.png)
    是物品的数量， ![](img/8803cc3c-1035-450b-bebe-0b58c7efdbf2.png) 再次是向量的维度。维度 ![](img/71640dff-fda0-4eec-9f3c-9be795501ceb.png)
    是模型的一个参数，它允许我们控制压缩信息的程度。维度 ![](img/71640dff-fda0-4eec-9f3c-9be795501ceb.png) 越小，从原始评分矩阵中保留的信息就越少。
- en: Lastly, we take all the known ratings and put them into a matrix ![](img/d1a8b96c-66df-4286-9ba2-e75a4f37bbb2.png)—this
    matrix is of ![](img/9874a860-bb0a-4b89-b635-3832b2bab47e.png) size. Then this
    matrix can be factorized as
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将所有已知的评分放入一个矩阵 ![](img/d1a8b96c-66df-4286-9ba2-e75a4f37bbb2.png)——这个矩阵的大小为 ![](img/9874a860-bb0a-4b89-b635-3832b2bab47e.png)。然后，这个矩阵可以被因式分解为
- en: '![](img/61a9bcd5-0cad-408b-98cb-329f37342bf4.png).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/61a9bcd5-0cad-408b-98cb-329f37342bf4.png)。'
- en: Without the biases part, this is exactly what we have when we compute ![](img/fbcab995-4049-4711-92e5-238f3137cf46.png)
    in the preceding formula.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 没有偏差部分时，这正是我们在前面公式中计算 ![](img/fbcab995-4049-4711-92e5-238f3137cf46.png)时得到的。
- en: 'To make the predicted rating ![](img/fbcab995-4049-4711-92e5-238f3137cf46.png)
    as close as possible to the observed rating rating ![](img/c9449ff1-83d3-4909-9747-53b0c8842a35.png),
    we minimize the squared error between them. That is, our training objective is
    the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使预测的评分 ![](img/fbcab995-4049-4711-92e5-238f3137cf46.png) 尽可能接近观测到的评分 ![](img/c9449ff1-83d3-4909-9747-53b0c8842a35.png)，我们最小化它们之间的平方误差。也就是说，我们的训练目标是以下内容：
- en: '![](img/6bed0f23-a85e-4821-a255-f3fe5260c9fc.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6bed0f23-a85e-4821-a255-f3fe5260c9fc.png)'
- en: This way of factorizing the rating matrix is sometimes called **SVD** because
    it is inspired by the classical Singular Value Decomposition method—it also optimizes
    the sum of squared errors. However, the classical SVD often tends to overfit to
    the training data, which is why here we include the regularization term in the
    objective.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对评分矩阵的因式分解有时被称为**SVD**，因为它的灵感来自经典的奇异值分解方法——它同样优化了平方误差的和。然而，经典的SVD往往容易对训练数据进行过拟合，这就是为什么在这里我们在目标函数中加入了正则化项。
- en: 'After defining the optimization problem, the paper then talks about two ways
    of solving it:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了优化问题之后，论文接着讨论了两种解决方法：
- en: '**Stochastic Gradient Descent** (**SGD**)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降** (**SGD**)'
- en: '**Alternating Least Squares** (**ALS**)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交替最小二乘法** (**ALS**)'
- en: Later in this chapter, we will use TensorFlow to implement the SGD method ourselves
    and compare it to the results of the ALS method from the `implicit` library.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面，我们将使用 TensorFlow 来实现 SGD 方法，并将其与 `implicit` 库中的 ALS 方法的结果进行比较。
- en: However, the dataset we use for this project is different from the Netflix competition
    dataset in a very important way—we do not know what the users do not like. We
    only observe what they like. That is why next we will talk about ways to handle
    such cases.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们在这个项目中使用的数据集与 Netflix 竞赛数据集有一个非常重要的区别——我们并不知道用户不喜欢什么。我们只能观察到他们喜欢什么。这也是为什么接下来我们将讨论如何处理这种情况的方法。
- en: Implicit feedback datasets
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐式反馈数据集
- en: In case of the Netflix competition, the data there relies on the explicit feedback
    given by the users. The users went to the website and explicitly told them how
    much they like the movie by giving it a rating from 1 to 5.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Netflix 竞赛中，那里使用的数据依赖于用户提供的显式反馈。用户会访问网站并明确告诉他们自己有多喜欢某部电影，评分范围从 1 到 5。
- en: Typically it is quite difficult to make users do that. However, just by visiting
    the website and interacting with it, the users already generated a lot of useful
    information, which can be used to infer their interests. All the clicks, page
    visits, and past purchases tell us about the preferences of the user. This kind
    of data is called **implicit** - the users do not explicitly tell us what they
    like, but instead, they indirectly convey this information to us by using the
    system. By collecting this interaction information we get implicit feedback datasets.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通常来说，让用户做这件事是相当困难的。然而，仅仅通过访问网站并与其互动，用户已经生成了大量有用的信息，这些信息可以用来推断他们的兴趣。所有的点击、页面访问和过去的购买行为都能告诉我们用户的偏好。这种数据被称为**隐式反馈**——用户并没有明确告诉我们他们喜欢什么，而是通过使用系统间接地传达了这一信息。通过收集这些互动信息，我们得到了隐式反馈数据集。
- en: The Online Retail Dataset we use for this project is exactly that. It tells
    us what the users previously bought, but does not tell us what the users do not
    like. We do not know if the users did not buy an item because they did not like
    it, or just because they did not know the item existed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个项目中使用的在线零售数据集正是这种数据集。它告诉我们用户之前购买了什么，但并没有告诉我们用户不喜欢什么。我们无法得知用户未购买某个商品，是因为他们不喜欢它，还是因为他们根本不知道这个商品的存在。
- en: Luckily for us, with minor modification, we still can apply the Matrix Factorization
    techniques to implicit datasets. Instead of the explicit ratings, the matrix takes
    values of 1 and 0—depending on whether there was an interaction with the item
    or not. Additionally, it is possible to express the confidence that the value
    1 or 0 is indeed correct, and this is typically done by counting how many times
    the users have interacted with the item. The more times they interact with it,
    the larger our confidence becomes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，经过少许修改，我们仍然可以将矩阵分解技术应用于隐式数据集。与显式评分不同，矩阵中的值取 1 或 0——取决于是否与某个商品发生了互动。此外，还可以表示值为
    1 或 0 的正确性置信度，这通常通过统计用户与商品的互动次数来实现。用户与商品互动的次数越多，我们的置信度就越大。
- en: So, in our case all values that the user has bought get the value 1 in the matrix,
    and all the rest are 0's. Thus we can see this is a binary classification problem
    and implement an SGD-based model in TensorFlow for learning the user and item
    matrices.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的案例中，用户购买过的所有值在矩阵中都赋值为 1，其他所有值为 0。由此我们可以看到这是一个二元分类问题，并在 TensorFlow 中实现基于
    SGD 的模型来学习用户和商品矩阵。
- en: But before we do that, we will establish another baseline have stronger than
    the previous one. We will use the `implicit` library, which uses ALS.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 但在此之前，我们将建立另一个比之前更强的基准。我们将使用 `implicit` 库，该库使用的是 ALS 方法。
- en: '*Collaborative Filtering for Implicit Feedback Datasets* by Hu et al (2008)
    gives a good introduction to the ALS method for implicit feedback datasets. We
    do not focus on ALS in this chapter, but if you want to learn how ALS is implemented
    in libraries such as `implicit`, this paper is definitely a great source. At the
    time of writing, the paper was accessible via [http://yifanhu.net/PUB/cf.pdf](http://yifanhu.net/PUB/cf.pdf).'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*隐式反馈数据集的协同过滤*（Collaborative Filtering for Implicit Feedback Datasets）由 Hu
    等人（2008年）撰写，提供了隐式反馈数据集 ALS 方法的良好介绍。本章并不专注于 ALS，但如果你想了解如何在诸如 `implicit` 这样的库中实现
    ALS，这篇论文无疑是一个很好的资源。在撰写本文时，该论文可以通过 [http://yifanhu.net/PUB/cf.pdf](http://yifanhu.net/PUB/cf.pdf)
    访问。'
- en: First, we need to prepare the data in the format `implicit` expects—and for
    that we need to construct the user-item matrix *X*. For that we need to translate
    both users and items to IDs, so we can map each user to a row of *X*, and each
    item—to the column of *X*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将数据准备为 `implicit` 所期望的格式——为此，我们需要构建用户-物品矩阵 *X*。为了做到这一点，我们需要将用户和物品都转换为
    ID，这样我们就可以将每个用户映射到 *X* 的一行，将每个物品映射到 *X* 的一列。
- en: 'We have already converted items (the column `stockcode`) to integers. How we
    need to perform the same on the user IDs (the column `customerid`):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将物品（列 `stockcode`）转换为整数。现在我们需要对用户 ID（列 `customerid`）执行相同的操作：
- en: '[PRE18]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Note that in the first line we perform the filtering and keep only known users
    there—these are the users we will use for training the model afterward. Then we
    apply the same procedure to the users in the validation set:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在第一行我们执行了过滤操作，只保留了已知用户——这些用户将用于后续的模型训练。然后我们对验证集中的用户应用相同的程序：
- en: '[PRE19]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next we use these integer codes to construct the matrix *X*:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用这些整数代码来构建矩阵 *X*：
- en: '[PRE20]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `sp.csr_matrix` is a function from the `scipy.sparse` package. It takes
    in the rows and column indicies plus the corresponding value for each index pair,
    and constructs a matrix in the Compressed Storage Row format.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`sp.csr_matrix` 是 `scipy.sparse` 包中的一个函数。它接受行和列的索引以及每对索引的对应值，并以压缩行存储格式构建矩阵。'
- en: Using sparse matrices is a great way to reduce the space consumption of data
    matrices. In recommender systems there are many users and many items. When we
    construct a matrix, we put zeros for all the items the user has not interacted
    with. Keeping all these zeros is wasteful, so sparse matrices give a way to store
    only non-zero entries. You can read more about them in the `scipy.sparse` package
    documentation at [https://docs.scipy.org/doc/scipy/reference/sparse.html](https://docs.scipy.org/doc/scipy/reference/sparse.html).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用稀疏矩阵是减少数据矩阵空间消耗的好方法。在推荐系统中，用户和物品的数量都很多。当我们构建矩阵时，我们会将所有用户未互动的物品填充为零。保留所有这些零是浪费，因此稀疏矩阵提供了一种只存储非零条目的方法。你可以在
    `scipy.sparse` 包的文档中阅读更多内容：[https://docs.scipy.org/doc/scipy/reference/sparse.html](https://docs.scipy.org/doc/scipy/reference/sparse.html)。
- en: 'Now let us use `implicit` to factorize the matrix *X* and learn the user and
    item vectors:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用 `implicit` 来对矩阵 *X* 进行分解，并学习用户和物品向量：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To use ALS we use the `AlternatingLeastSquares` class. It takes two parameters:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ALS 时，我们使用 `AlternatingLeastSquares` 类。它需要两个参数：
- en: '`factors`: this is the dimensionality of the user and item vectors, which we
    called previously k'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`factors`：这是用户和物品向量的维度，之前我们称之为 k'
- en: '`regularization`: the L2 regularization parameter to avoid overfitting'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`regularization`：L2 正则化参数，用于避免过拟合'
- en: 'Then we invoke the `fit` function to learn the vectors. Once the training is
    done, these vectors are easy to get:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们调用 `fit` 函数来学习向量。一旦训练完成，这些向量就可以轻松获取：
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: After getting the *U* and *I* matrices, we can use them to make recommendations
    to the user, and for that, we simply calculate the inner product between the rows
    of each matrix. We will see soon how to do it.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在得到 *U* 和 *I* 矩阵后，我们可以用它们向用户推荐内容，方法是计算每个矩阵行之间的内积。我们很快会看到如何做到这一点。
- en: 'Matrix factorization methods have a problem: they cannot deal with new users.
    To overcome this problem, we can simply combine it with the baseline method: use
    the baseline to make a recommendation to new and unknown users, but apply Matrix
    Factorization to known users.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解方法有一个问题：它们无法处理新用户。为了解决这个问题，我们可以简单地将它与基准方法结合：使用基准方法向新用户和未知用户推荐内容，但对已知用户应用矩阵分解。
- en: 'So, first we select the IDs of known users in the validation set:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，首先我们在验证集里选择已知用户的 ID：
- en: '[PRE23]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We will make recommendations only to these users. Then we copy the baseline
    solution, and replace the prediction for the known users by values from ALS:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只向这些用户推荐内容。然后，我们复制基准解决方案，并通过 ALS 的值替换已知用户的预测：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here we get the vectors for each user ID in the validation set and multiply
    them with all the item vectors. Next, for each user we select top five items according
    to the score.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们获取验证集中每个用户 ID 的向量，并将其与所有物品向量相乘。接下来，对于每个用户，我们根据得分选择排名前五的物品。
- en: This outputs 13.9%. This is a lot stronger baseline than our previous baseline
    of 6%. This should be a lot more difficult to outperform, but next, we nonetheless
    try to do it.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出 13.9%。这个基准比我们之前的 6% 强很多。这个基准应该更难超越，但接下来，我们还是尝试去做。
- en: SGD-based matrix factorization
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于 SGD 的矩阵分解
- en: 'Now we are finally ready to implement the matrix factorization model in TensorFlow.
    Let us do this and see if we can improve the baseline by `implicit`. Implementing
    ALS in TensorFlow is not an easy task: it is better suited for gradient-based
    methods such as SGD. This is why we will do exactly that, and leave ALS to specialized
    implementations.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于准备好在 TensorFlow 中实现矩阵分解模型了。让我们来实现它，看看我们是否能通过 `implicit` 来改善基准模型。用 TensorFlow
    实现 ALS 并非易事：它更适合基于梯度的方法，如 SGD。这就是为什么我们要采用这种方法，并将 ALS 留给专业的实现。
- en: 'Here we implement the formula from the previous sections:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们实现了前面章节中的公式：
- en: '![](img/19735a8b-b706-4cc3-8af0-0126f8ec73fc.png).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/19735a8b-b706-4cc3-8af0-0126f8ec73fc.png)。'
- en: 'Recall that the objective there was the following:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，之前的目标函数是如下的：
- en: '![](img/88241b76-b696-4d88-920e-581017e5050a.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88241b76-b696-4d88-920e-581017e5050a.png)'
- en: Note that in this objective we still have the squared error, which is no longer
    the case for us since we model this as a binary classification problem. With TensorFlow
    it does not really matter, and the optimization loss can easily be changed.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个目标函数中我们仍然使用了平方误差，但对我们来说，情况已经不同，因为我们将其建模为一个二分类问题。对于 TensorFlow 来说，实际上并不重要，优化损失可以很容易地更改。
- en: In our model we will use the log loss instead—it is better suited for binary
    classification problems than squared error.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的模型中，我们将使用对数损失，它比平方误差更适合于二分类问题。
- en: The *p* and *q* vectors make up the *U* and *I* matrices, respectively. What
    we need to do is to learn these *U* and *I* matrices. We can store the full matrices
    *U* and *I* as a TensorFlow `Variable`'s and then use the embedding layer to look
    up the appropriate *p* and *q* vectors.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*p* 和 *q* 向量分别构成 *U* 和 *I* 矩阵。我们需要做的是学习这些 *U* 和 *I* 矩阵。我们可以将完整的 *U* 和 *I* 矩阵存储为
    TensorFlow 的 `Variable`，然后使用嵌入层来查找适当的 *p* 和 *q* 向量。'
- en: 'Let us define a helper function for declaring embedding layers:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个辅助函数来声明嵌入层：
- en: '[PRE25]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This function creates a matrix of the specified dimension, initializes it with
    random values, and finally uses the lookup layer to convert user or item indexes
    into vectors.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数创建一个指定维度的矩阵，用随机值初始化，并最终使用查找层将用户或物品的索引转换为向量。
- en: 'We use this function as a part of the model graph:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个函数作为模型图的一部分来使用：
- en: '[PRE26]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The model gets three inputs:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型接收三个输入：
- en: '`place_user`: The user IDs'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`place_user`：用户 ID'
- en: '`place_item`: The item IDs'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`place_item`：物品的 ID'
- en: '`place_y`: The labels of each (user, item) pair'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`place_y`：每个（用户，物品）对的标签'
- en: 'Then we define:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义：
- en: '`user_factors`: The user matrix ![](img/54af3d80-6cd8-4d76-850c-8e967bce940d.png)'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user_factors`：用户矩阵 ![](img/54af3d80-6cd8-4d76-850c-8e967bce940d.png)'
- en: '`user_bias`: The bias of each user ![](img/a861c52e-2c8f-41ad-a15d-4550abcd98ef.png)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`user_bias`：每个用户的偏置项 ![](img/a861c52e-2c8f-41ad-a15d-4550abcd98ef.png)'
- en: '`item_factors`: The item matrix ![](img/21db3c48-dad6-4ec3-9ed3-98098a095cd0.png)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`item_factors`：物品矩阵 ![](img/21db3c48-dad6-4ec3-9ed3-98098a095cd0.png)'
- en: '`item_bias`: The bias of each item ![](img/e60773db-edab-42e6-ab33-ef2ac6cdf229.png)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`item_bias`：每个物品的偏置项 ![](img/e60773db-edab-42e6-ab33-ef2ac6cdf229.png)'
- en: '`global_bias`: The global bias ![](img/4a98f9f9-4a5b-4c22-b932-01ec62b04826.png)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`global_bias`：全局偏置项 ![](img/4a98f9f9-4a5b-4c22-b932-01ec62b04826.png)'
- en: Then, we put together all the biases and take the dot product between the user
    and item factors. This is our prediction, which we then pass through the sigmoid
    function to get probabilities.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将所有的偏置项组合在一起，并计算用户和物品因子之间的点积。这就是我们的预测结果，然后我们将其传递通过 sigmoid 函数以获得概率。
- en: Finally, we define our objective function as a sum of the data loss and regularization
    loss and use Adam for minimizing this objective.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将目标函数定义为数据损失和正则化损失的和，并使用 Adam 来最小化该目标。
- en: 'The model has the following parameters:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 模型具有以下参数：
- en: '`num_users` and `num_items`: The number of users (items). They specify the
    number of rows in *U* and *I* matrices, respectively.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_users` 和 `num_items`：用户（物品）的数量。它们分别指定 *U* 和 *I* 矩阵中的行数。'
- en: '`num_factors`: The number of latent features for users and items. This specifies
    the number of columns in both *U* and *I*.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_factors`：用户和物品的潜在特征数量。它指定了 *U* 和 *I* 中的列数。'
- en: '`lambda_user` and `lambda_item`: The regularization parameters.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lambda_user` 和 `lambda_item`：正则化参数。'
- en: '`lr`: Learning rate for the optimizer.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lr`：优化器的学习率。'
- en: '`K`: The number of negative examples to sample for each positive case (see
    the explanation in the following section).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`K`：每个正样本要采样的负样本数量（见下一节的解释）。'
- en: 'Now let us train the model. For that, we need to cut the input into small batches.
    Let us use a helper function for that:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来训练模型。为此，我们需要将输入切分成小批次。我们可以使用一个辅助函数来实现：
- en: '[PRE27]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This will turn one array into a list of arrays of specified size.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把一个数组转换为指定大小的数组列表。
- en: 'Recall that our dataset is based on implicit feedback, and the number positive
    instances—interactions that did occur—is very small compared to the number of
    negative instances—the interactions that did not occur. What do we do with it?
    The solution is simple: we use **negative sampling**. The idea behind it is to
    sample only a small fraction of negative examples. Typically, for each positive
    example, we sample `K` negative examples, and `K` is a tunable parameter. And
    this is exactly what we do here.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们的数据集是基于隐式反馈的，正例的数量——即发生的交互——与负例的数量——即未发生的交互——相比非常少。我们该如何处理它呢？解决方案很简单：我们使用**负采样**。其背后的思想是仅采样一小部分负例。通常，对于每个正例，我们会采样`K`个负例，而`K`是一个可调的参数。这正是我们在这里所做的。
- en: 'So let us train the model:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们来训练这个模型：
- en: '[PRE28]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We run the model for 10 epochs, then for each epoch we shuffle the data randomly
    and cut it into batches of 5000 positive examples. Then for each batch, we generate
    *K* * 5000 negative examples (*K* = 5 in our case) and put positive and negative
    examples together in one array. Finally, we run the model, and at each update
    step, we monitor the training loss using `tqdm`. The tqdm library provides a very
    nice way to monitor the training progress.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行模型10个周期（epoch），然后在每个周期内，我们随机打乱数据并将其切分为5000个正例的批次。接着，对于每个批次，我们生成*K* * 5000个负例（在我们这里，*K*
    = 5）并将正负例放在同一个数组中。最后，我们运行模型，并在每次更新步骤时，使用`tqdm`监控训练损失。tqdm库提供了一种非常好的方式来监控训练进度。
- en: 'This is the output we produce when we use the tqdm jupyter notebook widgets:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们使用tqdm jupyter notebook小部件时生成的输出：
- en: '![](img/644abd40-c3a3-4467-b9e9-e2acaa128e09.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/644abd40-c3a3-4467-b9e9-e2acaa128e09.png)'
- en: 'At the end of each epoch, we calculate precision—to monitor how our model is
    performing for our defined recommendation scenario. The `calculate_validation_precision`
    function is used for that. It is implemented in a similar way to what we did previously
    with implicit:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个周期结束时，我们计算精度—以监控模型在我们定义的推荐场景中的表现。`calculate_validation_precision`函数就是用来完成这一工作的。它的实现方式与我们之前在隐式反馈中做的类似：
- en: We first extract the matrices and the biases
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先提取矩阵和偏差。
- en: Then put them together to get the score for each (user, item) pair
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后将它们组合在一起，得到每个（用户，项目）对的评分。
- en: Finally, we sort these pairs and keep the top five ones
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们对这些对进行排序，保留前五个。
- en: 'For this particular case we do not need the global bias as well as the user
    bias: adding them will not change the order of items per user. This is how this
    function can be implemented:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的情况，我们不需要全局偏差以及用户偏差：加入它们不会改变每个用户的项目排序。这个函数可以这样实现：
- en: '[PRE29]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This is the output we get:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们得到的输出：
- en: '[PRE30]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: By the sixth epoch it beats the previous baseline, and by the tenth, it reaches
    15.2%.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 到第六个周期时，它超过了之前的基线，而到了第十个周期时，达到了15.2%。
- en: Matrix factorization techniques usually give a very strong baseline solution
    for recommender systems. But with a small adjustment, the same technique can produce
    even better results. Instead of optimizing a loss for binary classification, we
    can use a different loss designed specifically for ranking problems. In the next
    section, we will learn about this kind of loss and see how to make this adjustment.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解技术通常会为推荐系统提供一个非常强的基线解决方案。但只需做一些小的调整，相同的技术可以产生更好的结果。我们可以不优化二分类的损失，而是使用一种专门为排序问题设计的损失。在下一部分，我们将了解这种损失，并看到如何进行这种调整。
- en: Bayesian personalized ranking
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯个性化排序
- en: We use Matrix factorization methods for making a personalized ranking of items
    for each user. However, to solve this problem we use a binary classification optimization
    criterion—the log loss. This loss works fine and optimizing it often produces
    good ranking models. What if instead we could use a loss specifically designed
    for training a ranking function?
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用矩阵分解方法为每个用户制作个性化的项目排序。然而，为了解决这个问题，我们使用了一个二分类优化标准——对数损失。这种损失表现良好，优化它通常会产生很好的排序模型。如果我们能使用一个专门为训练排序函数设计的损失会怎样呢？
- en: 'Of course, it is possible to use an objective that directly optimizes for ranking.
    In the paper *BPR: Bayesian Personalized Ranking from Implicit Feedback* by Rendle
    et al (2012), the authors propose an optimization criterion, which they call **BPR-Opt**.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '当然，可以使用直接优化排名的目标函数。在Rendle等人（2012年）的论文《BPR: 基于隐式反馈的贝叶斯个性化排名》中，作者提出了一种优化准则，称为
    **BPR-Opt**。'
- en: 'Previously, we looked at individual items in separation from the other items.
    That is, we tried to predict the rating of an item, or the probability that the
    item *i* will be interesting to the user *u*. These kinds of ranking models are
    usually called "point-wise": they use traditional supervised learning methods
    such as regression or classification to learn the score, and then rank the items
    according to this score. This is exactly what we did in the previous section.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们把每个物品独立来看待，即我们尝试预测某个物品的评分，或者预测物品 *i* 对用户 *u* 是否感兴趣的概率。这类排名模型通常被称为“点对点”（point-wise）：它们使用传统的监督学习方法，如回归或分类，来学习评分，然后根据该评分对物品进行排名。这正是我们在上一节中所做的。
- en: BPR-Opt is different. Instead, it looks at the pairs of items. If we know that
    user *u* has bought item *i*, but never bought item *j*, then most likely *u*
    is more interested in *i* than in *j*. Thus, when we train a model, the score ![](img/43327e8f-4905-4604-971c-45c3de06a735.png)
    it produces for *i* should be higher than the score ![](img/0ca2d573-7670-4188-8ddd-627c36542b63.png)
    for *j*. In other words, for the scoring model we want ![](img/fad30377-0328-4a81-b9cb-0c0a77824f44.png).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: BPR-Opt 不同。相反，它关注的是物品对。如果我们知道用户 *u* 已经购买了物品 *i*，但从未购买过物品 *j*，那么很可能 *u* 对 *i*
    的兴趣大于对 *j* 的兴趣。因此，当我们训练一个模型时，它为 *i* 产生的评分 ![](img/43327e8f-4905-4604-971c-45c3de06a735.png)
    应该高于为 *j* 产生的评分 ![](img/0ca2d573-7670-4188-8ddd-627c36542b63.png)。换句话说，对于评分模型，我们希望 ![](img/fad30377-0328-4a81-b9cb-0c0a77824f44.png)。
- en: 'Therefore, for training this algorithm we need triples (user, positive item,
    negative item). For such triple *(u, i, j)* we define the pair-wise difference
    in scores as:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，训练该算法时，我们需要三元组（用户，正向物品，负向物品）。对于这样的三元组 *(u, i, j)*，我们定义评分的成对差异为：
- en: '![](img/5851cdb3-a518-41ab-be76-c642e8807b76.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5851cdb3-a518-41ab-be76-c642e8807b76.png)'
- en: where ![](img/88e3b9c2-64cf-4b7b-a232-ad4de51eebbc.png) and ![](img/aa2877f6-ad11-4320-8576-2fd7a5eeb8a2.png)
    is scores for *(u, i)* and *(u, j)*, respectively.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/88e3b9c2-64cf-4b7b-a232-ad4de51eebbc.png) 和 ![](img/aa2877f6-ad11-4320-8576-2fd7a5eeb8a2.png)
    分别是 *(u, i)* 和 *(u, j)* 的评分。
- en: 'When training, we adjust parameters of our model in such a way that at the
    end item *i* does rank higher than item *j*. We do this by optimizing the following
    objective:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们调整模型的参数，使得最终物品 *i* 的排名高于物品 *j*。我们通过优化以下目标来实现这一点：
- en: '![](img/e916bafa-cb69-45c5-a673-53c1c81812a4.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e916bafa-cb69-45c5-a673-53c1c81812a4.png)'
- en: Where ![](img/dc64fa1f-d229-4969-b574-2ce3bc7b1f29.png) are the differences, ![](img/4f026a47-c6e1-4384-a9db-d7b36e6fd6a6.png)
    is the sigmoid function, and ![](img/ca1a0231-2db7-493a-ba7f-64ac9d5f4edf.png)
    is all the parameters of the model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/dc64fa1f-d229-4969-b574-2ce3bc7b1f29.png) 是差异， ![](img/4f026a47-c6e1-4384-a9db-d7b36e6fd6a6.png)
    是sigmoid函数， ![](img/ca1a0231-2db7-493a-ba7f-64ac9d5f4edf.png) 是模型的所有参数。
- en: 'It is straightforward to change our previous code to optimize this loss. The
    way we compute the score for (*u, i*) and (*u, j*) is the same: we use the biases
    and the inner product between the user and item vectors. Then we compute the difference
    between the scores and feed the difference into the new objective.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很简单地修改之前的代码来优化这个损失函数。我们计算 (*u, i*) 和 (*u, j*) 的评分方式不变：我们使用偏置和用户与物品向量的内积。然后，我们计算评分之间的差异，并将差异输入到新的目标函数中。
- en: 'The difference in the implementation is also not large:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 实现中的差异也不大：
- en: For BPR-Opt we do not have `place_y`, but instead, we will have `place_item_pos`
    and `place_item_neg` for the positive and the negative items, respectively.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 BPR-Opt，我们没有 `place_y`，而是会分别为正向物品和负向物品使用 `place_item_pos` 和 `place_item_neg`。
- en: 'We no longer need the user bias and the global bias: when we compute the difference,
    these biases cancel each other out. What is more, they are not really important
    for ranking—we have noted that previously when computing the predictions for the
    validation data.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不再需要用户偏置和全局偏置：当我们计算差异时，这些偏置会相互抵消。而且，它们对排名来说并不重要——我们在先前计算验证数据的预测时就注意到了这一点。
- en: 'Another slight difference in implementation is that because we now have two
    inputs items, and these items have to share the embeddings, we need to define
    and create the embeddings slightly differently. For that we modify the `embed`
    helper function, and separate the variable creation and the lookup layer:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个实现上的小差异是，由于我们现在有两个输入项，并且这些项必须共享嵌入，我们需要稍微不同地定义和创建嵌入。为此，我们修改了`embed`辅助函数，并且将变量创建和查找层分开。
- en: '[PRE31]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, let us see how it looks in the code:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看代码中的实现：
- en: '[PRE32]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The way to train this model is also slightly different. The authors of the BPR-Opt
    paper suggest using the bootstrap sampling instead of the usual full-pass over
    all the data, that is, at each training step we uniformly sample the triples (user,
    positive item, negative item) from the training dataset.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个模型的方法也略有不同。BPR-Opt论文的作者建议使用自助采样（bootstrap sampling），而不是通常的全数据遍历，也就是说，在每个训练步骤中，我们从训练数据集中均匀地采样三元组（用户、正向项、负向项）。
- en: 'Luckily, this is even easier to implement than the full-pass:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这比全数据遍历实现起来要简单得多：
- en: '[PRE33]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: After around 70 iterations it reaches the precision of around 15.4%. While it
    is not significantly different from the previous model (it reached 15.2%), it
    does open a lot of possibilities for optimizing directly for ranking. More importantly,
    we show how easy it is to adjust the existent method such that instead of optimizing
    the point-wise loss it optimizes a pair-wise objective.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 大约经过70次迭代后，它的精度达到了约15.4%。尽管与之前的模型（精度为15.2%）差别不大，但它为直接优化排名提供了很多可能性。更重要的是，我们展示了调整现有方法的难易程度，使其不再优化逐点损失，而是优化成对目标。
- en: In the next section, we will go deeper and see how recurrent neural networks
    can model user actions as sequences and how we can use them as recommender systems.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将更深入地探讨循环神经网络如何将用户行为建模为序列，并且我们将看看如何将它们用作推荐系统。
- en: RNN for recommender systems
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于推荐系统的RNN
- en: A **recurrent neural networks** (**RNN**) is a special kind of neural network
    for modeling sequences, and it is quite successful in a number applications. One
    such application is sequence generation. In the article *The Unreasonable Effectiveness
    of Recurrent Neural Networks*, Andrej Karpathy writes about multiple examples
    where RNNs show very impressive results, including generation of Shakespeare,
    Wikipedia articles, XML, Latex, and even C code!
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）是一种特殊的神经网络，用于建模序列，并且在多个应用中都取得了相当成功。其中一个应用是序列生成。在《*循环神经网络的非理性有效性*》一文中，Andrej
    Karpathy写到多个RNN取得非常令人印象深刻的结果的例子，包括生成莎士比亚作品、维基百科文章、XML、Latex，甚至是C代码！'
- en: Since they have proven useful in a few applications already, the natural question
    to ask is whether we can apply RNNs to some other domains. What about recommender
    systems, for example? This is the question the authors of the recurrent neural
    networks *Based Subreddit Recommender System* report have asked themselves (see
    [https://cole-maclean.github.io/blog/RNN-Based-Subreddit-Recommender-System/](https://cole-maclean.github.io/blog/RNN-Based-Subreddit-Recommender-System/)).
    The answer is yes, we can use RNNs for that too!
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 既然RNN已经在一些应用中证明了其有效性，那么一个自然的问题是：我们能否将RNN应用到其他领域呢？比如推荐系统？这是《基于循环神经网络的子版块推荐系统》报告的作者们所提出的问题（请见[https://cole-maclean.github.io/blog/RNN-Based-Subreddit-Recommender-System/](https://cole-maclean.github.io/blog/RNN-Based-Subreddit-Recommender-System/)）。答案是肯定的，我们也可以将RNN应用于这个领域！
- en: 'In this section, we will try to answer this question as well. For this part
    we consider a slightly different recommendation scenario than previously:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们也将尝试回答这个问题。对于这一部分，我们考虑一个与之前略有不同的推荐场景：
- en: The user enters the website.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户进入网站。
- en: We present five recommendations.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们提供了五个推荐。
- en: After each purchase, we update the recommendations.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次购买后，我们更新推荐列表。
- en: This scenario needs a different way of evaluating the results. Each time the
    user buys something, we can check whether this item was among the suggested ones
    or not. If it was, then our recommendation is considered successful. So we can
    calculate how many successful recommendations we have made. This way of evaluating
    performance is called Top-5 accuracy and it is often used for evaluating classification
    models with a large number of target classes.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这个场景需要一种不同的结果评估方式。每当用户购买某个商品时，我们可以检查该商品是否在推荐列表中。如果在，则我们的推荐被视为成功。因此，我们可以计算出我们做了多少次成功的推荐。这种评估性能的方式叫做Top-5准确度，它通常用于评估具有大量目标类别的分类模型。
- en: Historically RNNs are used for language models, that is, for predicting what
    will be the most likely next word given in the sentence so far. And, of course,
    there is already an implementation of such a language model in the TensorFlow
    model repository located at [https://github.com/tensorflow/models](https://github.com/tensorflow/models)
    (in the [`tutorials/rnn/ptb/`](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb)
    folder). Some of the code samples in the remaining of this chapter are heavily
    inspired by this example.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，RNN（循环神经网络）用于语言模型，即预测给定句子中下一个最可能出现的单词。当然，TensorFlow模型库中已经有一个实现了这样的语言模型，位于[https://github.com/tensorflow/models](https://github.com/tensorflow/models)（在[`tutorials/rnn/ptb/`](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb)文件夹中）。本章剩下的一些代码示例深受这个例子的启发。
- en: So let us get started.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。
- en: Data preparation and baseline
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备和基准
- en: Like previously, we need to represent the items and users as integers. This
    time, however, we need to have a special placeholder value for unknown users.
    Additionally, we need a special placeholder for items to represent "no item" at
    the beginning of each transaction. We will talk more about it later in this section,
    but for now, we need to implement the encoding such that the `0` index is reserved
    for special purposes.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前一样，我们需要将项目和用户表示为整数。然而，这次我们需要为未知用户设置一个特殊的占位符值。此外，我们还需要为项目设置一个特殊的占位符，表示每个交易开始时的“无项目”。我们稍后会详细讨论这一点，但目前，我们需要实现编码，以便`0`索引保留用于特殊用途。
- en: 'Previously we were using a dictionary, but this time let us implement a special
    class, `LabelEncoder`, for this purpose:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们使用的是字典，但这次我们为此目的实现了一个特殊的类`LabelEncoder`：
- en: '[PRE34]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The implementation is straightforward and it largely repeats the code we used
    previously, but this time it is wrapped in a class, and also reserves `0` for
    special needs—for example, for elements that are missing in the training data.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 实现非常简单，基本上重复了我们之前使用的代码，但这次它被封装在一个类中，并且保留了`0`索引用于特殊需求——例如，用于训练数据中缺失的元素。
- en: 'Let us use this encoder to convert the items to integers:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个编码器将项目转换为整数：
- en: '[PRE35]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then we perform the same train-validation-test split: first 10 months we use
    for training, one for validation and the last one—for testing.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们执行相同的训练-验证-测试划分：前10个月用于训练，一个月用于验证，最后一个月用于测试。
- en: 'Next, we encode the user ids:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对用户ID进行编码：
- en: '[PRE36]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Like previously, we use the most frequently bought items for the baseline. However,
    this time the scenario is different, which is why we also adjust the baseline
    slightly. In particular, if one of the recommended items is bought by the user,
    we remove it from the future recommendations.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前一样，我们使用最常购买的项目作为基准。然而，这次情况有所不同，因此我们稍微调整了基准。具体而言，如果用户购买了某个推荐项目，我们会将其从未来的推荐中移除。
- en: 'Here is how we can implement it:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们如何实现它的：
- en: '[PRE37]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In the preceding code, `indptr` is the array of pointers—the same one that we
    used for implementing the `precision` function previously.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`indptr`是指针数组——它与我们之前用于实现`precision`函数的数组相同。
- en: 'So now we apply this to the validation data and produce the results:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将其应用到验证数据并生成结果：
- en: '[PRE38]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The baseline looks as follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 基准模型如下所示：
- en: '[PRE39]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now let us implement the top-k accuracy metric. We again use the `@njit` decorator
    from numba to speed this function up:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来实现top-k准确率指标。我们再次使用来自numba的`@njit`装饰器来加速这个函数：
- en: '[PRE40]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To evaluate the performance of the baseline, just invoke with the true labels
    and the predictions:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 要评估基准模型的性能，只需调用真实标签和预测结果：
- en: '[PRE41]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: It prints `0.012`, that is, only in 1.2% cases we make a successful recommendation.
    Looks like there is a lot of room for improvement!
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 它打印出`0.012`，即我们只有在1.2%的情况下能够成功推荐。这看起来有很大的改进空间！
- en: 'The next step is breaking the long array of items into separate transactions.
    We again can reuse the pointer array, which tells us where each transaction starts
    and where it ends:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将长数组拆分为单独的交易。我们可以再次重用指针数组，它告诉我们每个交易的开始和结束位置：
- en: '[PRE42]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now we can unwrap the transactions and put them into a separate dataframe:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以解包交易并将它们放入一个单独的数据框中：
- en: '[PRE43]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To have a look at what we have at the end, use the `head` function:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看最终结果，使用`head`函数：
- en: '[PRE44]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This shows the following:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了以下内容：
- en: '![](img/fc729f2f-9cbd-4907-acfb-fa4ebb9aa61e.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc729f2f-9cbd-4907-acfb-fa4ebb9aa61e.png)'
- en: These sequences have varying lengths, and this is a problem for RNNs. So, we
    need to convert them into fixed-length sequences, which we can easily feed to
    the model later.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这些序列的长度各不相同，这对 RNN 来说是一个问题。因此，我们需要将它们转换为固定长度的序列，这样我们以后就可以轻松地将其输入到模型中。
- en: In case the original sequence is too short, we need to pad it with zeros. If
    the sequence is too long, we need to cut it or split it into multiple sequences.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果原始序列太短，我们需要用零填充它。如果序列太长，我们需要将其切割或分割成多个序列。
- en: Lastly, we also need to represent the state when the user has entered the website
    but has not bought anything yet. We can do this by inserting the dummy zero item—an
    item with index `0`, which we reserved for special purposes, just like this one.
    In addition to that, we can also use this dummy item to pad the sequences that
    are too small.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还需要表示用户已进入网站但尚未购买任何商品的状态。我们可以通过插入虚拟零项来实现——这是一项具有索引 `0` 的商品，专门为特殊用途保留，就像这个一样。此外，我们还可以利用这个虚拟项来填充那些过小的序列。
- en: 'We also need to prepare the labels for the RNN. Suppose we have the following
    sequence:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要为 RNN 准备标签。假设我们有以下序列：
- en: '![](img/aace2f95-38d0-4b47-b07a-271275634b92.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aace2f95-38d0-4b47-b07a-271275634b92.png)'
- en: 'We want to produce a sequence of fixed length 5\. With padding in the beginning,
    the sequence we use for training will look as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望生成一个固定长度为 5 的序列。通过在开头填充，训练用的序列将如下所示：
- en: '![](img/eca12751-9e8d-4cbb-9b98-fa46a06d8349.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eca12751-9e8d-4cbb-9b98-fa46a06d8349.png)'
- en: 'Here we pad the original sequence with zero at the beginning and do not include
    the last element—the last element will only be included in the target sequence.
    So the target sequence—the output we want to predict—should look as follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们在原始序列的开始处用零填充，并且不包括最后一个元素——最后一个元素只会包含在目标序列中。因此，目标序列——我们要预测的输出——应该如下所示：
- en: '![](img/9162b799-a679-469a-8cc6-0503ddc9f8d2.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9162b799-a679-469a-8cc6-0503ddc9f8d2.png)'
- en: 'It may look confusing at the beginning, but the idea is simple. We want to
    construct the sequences in such a way that for the position *i* in *X*, the position
    *i* in *Y* contains the element we want to predict. For the preceding example
    we want to learn the following rules:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始可能看起来有些困惑，但这个想法很简单。我们希望构造序列，使得对于 *X* 中的第 *i* 个位置，*Y* 中的第 *i* 个位置包含我们想要预测的元素。对于前面的例子，我们想要学习以下规则：
- en: '![](img/24dceee9-e8c7-4c70-9892-42ca4fac0eb7.png)- both are at the position
    `0` in *X* and *Y*'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/24dceee9-e8c7-4c70-9892-42ca4fac0eb7.png) - 都位于 *X* 和 *Y* 的位置 `0`。'
- en: '![](img/0dbaad9b-860b-4f49-b4e2-01c567e47fb2.png) —both are at the position
    `1` in *X* and *Y*'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/0dbaad9b-860b-4f49-b4e2-01c567e47fb2.png) — 都位于 *X* 和 *Y* 的位置 `1`。'
- en: and so on
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以此类推
- en: 'Now imagine we have a smaller sequence of length 2, which we need to pad to
    a sequence of length 5:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们有一个较小的长度为 2 的序列，我们需要将其填充到长度为 5 的序列：
- en: '![](img/82115137-b47a-4a31-95ca-9b29c1f1bfa2.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82115137-b47a-4a31-95ca-9b29c1f1bfa2.png)'
- en: 'In this case, we again pad the input sequence with `0` in the beginning, and
    also with some zeros at the end:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们再次在输入序列的开头填充 `0`，并且在末尾也加上了一些零：
- en: '![](img/3cc62b3d-1ed9-4aec-a82b-4985df4598b5.png).'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/3cc62b3d-1ed9-4aec-a82b-4985df4598b5.png)。'
- en: 'We transform the target sequence Y similarly:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们类似地转换目标序列 Y：
- en: '![](img/eb8811f0-4462-409b-9bd7-3d17f9fcaaab.png).'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/eb8811f0-4462-409b-9bd7-3d17f9fcaaab.png)。'
- en: 'If the input is too long, for example ![](img/76c862e9-e9a1-4c19-a73e-bea827b054fa.png),
    we can cut it into multiple sequences:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入太长，例如 ![](img/76c862e9-e9a1-4c19-a73e-bea827b054fa.png)，我们可以将其切分为多个序列：
- en: '![](img/e0c3b30d-b045-488b-830b-4f587f3b2c46.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0c3b30d-b045-488b-830b-4f587f3b2c46.png)'
- en: 'To perform such a transformation, we write a function `pad_seq`.  It adds the
    needed amount of zeros at the beginning and at the end of the sequence. Then we
    `pad_seq` in another function - `prepare_training_data`—the function that creates
    the matrices *X* and *Y* for each sequence:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行这样的转换，我们编写了一个函数 `pad_seq`。它会在序列的开始和结束处添加所需数量的零。然后，我们在另一个函数中调用 `pad_seq`
    —— `prepare_training_data` —— 该函数会为每个序列创建 *X* 和 *Y* 的矩阵：
- en: '[PRE45]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'What is left to do is invoking the `prepare_training_data` function for each
    sequence in the training history, and then put the results together in `X_train`
    and `Y_train` matrices:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的就是为每个训练历史中的序列调用 `prepare_training_data` 函数，然后将结果合并到 `X_train` 和 `Y_train`
    矩阵中：
- en: '[PRE46]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: At this point, we have finished data preparation. Now we are ready to finally
    create an RNN model that can process this data.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经完成了数据准备。现在，我们准备好创建一个可以处理这些数据的 RNN 模型。
- en: RNN recommender system in TensorFlow
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 中的 RNN 推荐系统
- en: The data preparation is done and now we take the produced matrices `X_train`
    and `Y_train` and use them for training a model. But of course, we need to create
    the model first. In this chapter, we will use a recurrent neural network with
    LSTM cells (Long Short-Term Memory). LSTM cells are better than plain RNN cells
    because they can capture long-term dependencies better.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备工作已经完成，现在我们使用生成的矩阵 `X_train` 和 `Y_train` 来训练模型。但当然，我们需要先创建模型。在本章中，我们将使用带有
    LSTM 单元（长短期记忆）的循环神经网络。LSTM 单元比普通 RNN 单元更好，因为它们能够更好地捕捉长期依赖关系。
- en: A great resource to learn more about LSTMs is the blog post "Understanding LSTM
    Networks" by Christopher Olah, which is available at [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).
    In this chapter, we do not go into theoretical details about how LSTM and RNN
    work and only look at using them in TensorFow.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多关于 LSTM 的知识，可以参考 Christopher Olah 的博客文章《理解 LSTM 网络》，可以在 [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    找到。在本章中，我们不会深入探讨 LSTM 和 RNN 的理论细节，只关注如何在 TensorFlow 中使用它们。
- en: 'Let us start with defining a special configuration class that holds all the
    important training parameters:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义一个特殊的配置类开始，它包含所有重要的训练参数：
- en: '[PRE47]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Here the `Config` class defines the following parameters:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `Config` 类定义了以下参数：
- en: '`num_steps`—This is the size of the fixed-length sequences'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_steps`—这是固定长度序列的大小'
- en: '`num_items`—The number of items in our training data (+1 for the dummy `0`
    item)'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_items`—我们训练数据中项目的数量（+1 代表虚拟的 `0` 项）'
- en: '`num_users`—The number of users (again +1 for the dummy `0` user)'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_users`—用户数量（同样 +1 代表虚拟的 `0` 用户）'
- en: '`init_scale`—Scale of the weights parameters, needed for the initialization'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`init_scale`—权重参数的缩放因子，初始化时需要'
- en: '`learning_rate`—The rate at which we update the weights'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`—我们更新权重的速率'
- en: '`max_grad_norm`—The maximally allowed norm of the gradient, if the gradient
    exceeds this value, we clip it'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_grad_norm`—梯度的最大允许范数，如果梯度超过此值，我们将进行裁剪'
- en: '`num_layers`—The number of LSTM layers in the network'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_layers`—网络中 LSTM 层的数量'
- en: '`hidden_size`—The size of the hidden dense layer that converts the output of
    LSTM to output probabilities'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hidden_size`—将 LSTM 输出转换为输出概率的隐藏密集层的大小'
- en: '`embedding_size`—The dimensionality of the item embeddings'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`embedding_size`—项目嵌入的维度'
- en: '`batch_size`—The number of sequences we feed into the net in a single training
    step'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`—我们在单次训练步骤中输入到网络的序列数'
- en: 'Now we finally implement the model. We start off by defining two useful helper
    functions—we will use them for adding the RNN part to our model:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们终于实现模型。我们首先定义了两个有用的辅助函数——我们将用它们来将 RNN 部分添加到我们的模型中：
- en: '[PRE48]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now we can use the `rnn_model` function to create our model:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 `rnn_model` 函数来创建我们的模型：
- en: '[PRE49]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'In this model there are multiple parts, which is described as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中有多个部分，具体如下：
- en: First, we specify the inputs. Like previously, these are IDs, which later we
    convert to vectors by using the embeddings layer.
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们指定输入。和之前一样，这些是 ID，稍后我们通过嵌入层将它们转换为向量。
- en: Second, we add the RNN layer followed by a dense layer. The LSTM layer learns
    the temporary patters in purchase behavior, and the dense layer converts this
    information into a probability distribution over all possible items.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二，我们添加 RNN 层，接着是一个密集层。LSTM 层学习购买行为中的时间模式，密集层将这些信息转换为所有可能项目的概率分布。
- en: Third, since our model is multi-class classification model, we optimize the
    categorical cross-entropy loss.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三，由于我们的模型是多类分类模型，我们优化分类交叉熵损失。
- en: Finally, LSTMs are known to have problems with exploding gradients, which is
    why we perform gradient clipping when performing the optimization.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，LSTM 被认为有梯度爆炸的问题，这就是为什么我们在进行优化时会执行梯度裁剪。
- en: The function returns all the important variables in a dictionary—so, later on,
    we will be able to use them when training and validating the results.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数返回一个包含所有重要变量的字典——所以稍后我们将能够在训练和验证结果时使用它们。
- en: 'The reason this time we create a function, and not just global variables like
    previously, is to be able to change the parameters between training and testing
    phases. During training, the `batch_size` and `num_steps` variables could take
    any value, and, in fact, they are tunable parameters of the model. On the contrary,
    during testing, these parameters could take only one possible value: `1`. The
    reason is that when the user buys something, it is always one item at a time,
    and not several, so `num_steps` is one. The `batch_size` is also one for the same
    reason.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们创建一个函数，而不是像之前那样仅使用全局变量，原因是我们希望在训练和测试阶段之间能够改变参数。在训练过程中，`batch_size` 和 `num_steps`
    变量可以取任何值，实际上它们是模型的可调参数。相反，在测试过程中，这些参数只能取一个值：`1`。原因是当用户购买物品时，总是一次购买一个项目，而不是多个，所以
    `num_steps` 为 1。`batch_size` 也因为同样的原因为 1。
- en: 'For this reason, we create two configs: one for training, and one for validation:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们创建了两个配置：一个用于训练，一个用于验证：
- en: '[PRE50]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now let us define the computational graph for the model. Since we want to learn
    the parameters during training, but then use them in a separate model with different
    parameters during testing, we need to make the learned parameters shareable. These
    parameters include embeddings, LSTM, and the weights of the dense layer. To make
    both models share the parameters, we use a variable scope with `reuse=True`:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义模型的计算图。由于我们希望在训练过程中学习参数，但在测试过程中使用具有不同参数的独立模型，因此我们需要使学习到的参数可共享。这些参数包括嵌入、LSTM
    和密集层的权重。为了使两个模型共享这些参数，我们使用一个变量作用域并设置`reuse=True`：
- en: '[PRE51]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The graph is ready. Now we can train the model, and for this purpose, we create
    a `run_epoch` helper function:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图准备好了。现在我们可以训练模型，为此我们创建一个 `run_epoch` 辅助函数：
- en: '[PRE52]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The initial part of the function should already be familiar to us: it first
    creates a dictionary of variables that we are interested to get from the model
    and also shuffle the dataset.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的初始部分应该对我们来说已经很熟悉：它首先创建一个我们感兴趣的变量字典，并且打乱数据集。
- en: 'The next part is different though: since this time we have an RNN model (LSTM
    cell, to be exact), we need to keep its state across runs. To do it we first get
    the initial state—which should be all zeros—and then make sure the model gets
    exactly these values. After each step, we record the final step of the LSTM and
    re-enter it to the model. This way the model can learn typical behavior patterns.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，下一部分有所不同：由于这次我们使用的是 RNN 模型（准确来说是 LSTM 单元），我们需要在多次运行中保持其状态。为此，我们首先获取初始状态——它应该全为零——然后确保模型确切地获得这些值。每完成一步，我们记录
    LSTM 的最终状态并将其重新输入到模型中。通过这种方式，模型可以学习典型的行为模式。
- en: Again, like previously, we use `tqdm` to monitor progress, and we display both
    how many steps we have already taken during the epoch and the current training
    loss.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 再次像之前一样，我们使用 `tqdm` 来监控进度，并展示我们在一个周期中已经进行的步骤数量和当前的训练损失。
- en: 'Let us train this model for one epoch:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练这个模型一个周期：
- en: '[PRE53]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'One epoch is enough for the model to learn some patterns, so now we can see
    whether it was actually able to do it. For that we first write another helper
    function, which will emulate our recommendation scenario:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 一个周期已经足够让模型学习一些模式，所以现在我们可以查看它是否真的能够做到这一点。为此，我们首先编写另一个辅助函数，模拟我们的推荐场景：
- en: '[PRE54]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'What we do here is the following:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做的事情是：
- en: First, we initialize the prediction matrix, its size like in the baseline, is
    the number of items in the validation set times the number of recommendations.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们初始化预测矩阵，其大小与基准模型相同，为验证集中的项目数量与推荐数量的乘积。
- en: Then we run the model for each transaction in the dataset.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们对数据集中的每个事务运行模型。
- en: Each time we start with the dummy zero item and the empty zero LSTM state.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每次我们从虚拟的零项和空的零 LSTM 状态开始。
- en: Then one by one we predict the next possible item and put the actual item the
    user bought as the previous item—which we will feed into the model on the next
    step.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们逐个预测下一个可能的项目，并将用户实际购买的项目作为前一个项目——我们将在下一步将其输入到模型中。
- en: Finally, we take the output of the dense layer and get top-k most likely predictions
    as our recommendation for this particular step.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们取密集层的输出并获取前 k 个最可能的预测，作为我们在这一特定步骤的推荐。
- en: 'Let us execute this function and look at its performance:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行这个函数并观察它的性能：
- en: '[PRE55]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We see the output 7.1%, which is seven times better than the baseline.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到输出为 7.1%，是基准模型的七倍。
- en: 'This is a very basic model, and there is definitely a lot of room for improvement:
    we can tune the learning rate and train for a few more epochs with gradually decreasing
    learning rate. We can change the `batch_size`, `num_steps`, as well as all other
    parameters. We also do not use any regularization—neither weight decay nor dropout.
    Adding it should be helpful.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常基础的模型，肯定还有很大的改进空间：我们可以调整学习率，并且逐渐减少学习率的训练几个时期。我们可以改变`batch_size`，`num_steps`，以及所有其他参数。我们也没有使用任何正则化——既不是权重衰减也不是dropout。添加它应该会有所帮助。
- en: 'But most importantly, we did not use any user information here: the recommendations
    were based solely on the patterns of items. We should be able to get additional
    improvement by including the user context. After all, the recommender systems
    should be personalized, that is, tailored for a particular user.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 但最重要的是，我们在这里没有使用任何用户信息：推荐仅基于物品的模式。通过包含用户上下文，我们应该能够获得额外的改进。毕竟，推荐系统应该是个性化的，即针对特定用户量身定制。
- en: 'Right now our `X_train` matrix contains only items. We should include another
    input, for example `U_train`, which contains the user IDs:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的`X_train`矩阵只包含物品。我们应该包括另一个输入，例如`U_train`，其中包含用户ID：
- en: '[PRE56]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let us change the model now. The easiest way to incorporate user features is
    to stack together user vectors with item vectors and put the stacked matrix to
    LSTM. It is quite easy to implement, we just need to modify a few lines of the
    code:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们改变模型。将用户特征合并到物品向量中并将堆叠矩阵放入LSTM是最简单的方法。实现起来非常容易，我们只需要修改几行代码：
- en: '[PRE57]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The changes between the new implementation and the previous model are shown
    in bold. In particular, the differences are the following:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 新实现与之前模型之间的变化用**粗体**显示。特别是，差异如下：
- en: We add `place_u`—The placeholder that takes the user ID as input
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们添加`place_u`——作为输入接受用户ID的占位符
- en: Rename `embeddings` to `item_embeddings`—not to confuse them with `user_embeddings`,
    which we added a few lines after that
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`embeddings`重命名为`item_embeddings`——以免与我们在其后添加的`user_embeddings`混淆几行
- en: Finally, we concatenate user features with item features
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将用户特征与物品特征串联起来。
- en: The rest of the model code stays unchanged!
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 模型其余的代码保持不变！
- en: 'Initialization is similar to the previous model:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化类似于前一个模型：
- en: '[PRE58]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The only difference here is that we invoke a different function when creating
    the model. The code for training one epoch of the model is very similar to the
    previous one. The only things that we change are the extra parameters of the function,
    which we add into the `feed_dict` inside:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别是，我们在创建模型时调用不同的函数。模型训练一个时期的代码与以前非常相似。我们改变的唯一事物是函数的额外参数，我们将它们添加到`feed_dict`中：
- en: '[PRE59]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let us train this new model for one epoch:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们为这个新模型训练一个时期：
- en: '[PRE60]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The way we use the model is also almost the same as previous:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用模型的方式与之前几乎相同：
- en: '[PRE61]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Finally, we run this function to generate the predictions for the validation
    set, and calculate the accuracy of these recommendations:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们运行此函数为验证集生成预测，并计算这些推荐的准确性：
- en: '[PRE62]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output we see is 0.252, which is 25%. We naturally expect it to be better,
    but the improvement was quite drastic: almost four times better than the previous
    model, and 25 better than the naive baseline. Here we skip the model check on
    the hold-out test set, but you can (and generally should) do it yourself to make
    sure the model does not overfit.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到的输出是0.252，即25%。我们自然期望它更好，但改进非常显著：几乎比上一个模型好了四倍，并且比朴素基线好了25个百分点。在保留测试集上跳过模型检查，但您可以（并且通常应该）自行执行以确保模型不会过拟合。
- en: Summary
  id: totrans-383
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered recommender systems. We first looked at some background
    theory, implemented simple methods with TensorFlow, and then discussed some improvements
    such as the application of BPR-Opt to recommendations. These models are important
    to know and very useful to have when implementing the actual recommender systems.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了推荐系统。我们首先介绍了一些背景理论，用TensorFlow实现了简单的方法，然后讨论了一些改进，例如应用BPR-Opt到推荐中。了解这些模型并在实际推荐系统中实现它们非常重要和有用。
- en: In the second section, we tried to apply the novel techniques for building recommender
    systems based on Recurrent Neural Nets and LSTMs. We looked at the user's purchase
    history as a sequence and were able to use sequence models to make successful
    recommendations.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们尝试应用基于递归神经网络（RNN）和长短期记忆网络（LSTM）构建推荐系统的创新技术。我们将用户的购买历史视为一个序列，并能够利用序列模型进行成功的推荐。
- en: 'In the next chapter, we will cover Reinforcement Learning. This is one of the
    areas where the recent advances of Deep Learning have significantly changed the
    state-of-the-art: the models now are able to beat humans in many games. We will
    look at the advanced models that caused the change and we will also learn how
    to use TensorFlow to implement real AI.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论强化学习。这是深度学习最近的进展显著改变了技术前沿的领域之一：现在的模型在许多游戏中能够击败人类。我们将研究导致这一变化的先进模型，并学习如何使用TensorFlow实现真正的人工智能。
