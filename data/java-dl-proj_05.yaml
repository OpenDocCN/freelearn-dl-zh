- en: Transfer Learning for Image Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分类的迁移学习
- en: In [Chapter 3](5e051ef6-a3bf-4fe3-b888-8cebdb4240e6.xhtml), *Multi-Label* *Image
    Classification using Convolutional Neural Networks*, we saw how to develop an
    end-to-end project for handling multi-label image classification problems using
    CNN based on Java and the **Deeplearning4J** (**DL4J**) framework on real Yelp
    image datasets. For that purpose, we developed a CNN model from scratch.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](5e051ef6-a3bf-4fe3-b888-8cebdb4240e6.xhtml)，*多标签* *图像分类使用卷积神经网络*，我们展示了如何使用基于Java的卷积神经网络（CNN）和**Deeplearning4J**（**DL4J**）框架，在实际的Yelp图像数据集上开发一个端到端的项目来处理多标签图像分类问题。为此，我们从头开始开发了一个CNN模型。
- en: Unfortunately, developing such a model from scratch is very time consuming and
    requires a significant amount of computational resources. Secondly, sometimes,
    we may not even have enough data to train such deep networks. For example, ImageNet
    is one of the largest image datasets at the moment and has millions of labeled
    images.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，从零开始开发这样的模型是非常耗时的，并且需要大量的计算资源。其次，有时我们甚至可能没有足够的数据来训练如此深的网络。例如，ImageNet是目前最大的图像数据集之一，拥有数百万张带标签的图像。
- en: 'Therefore, we will develop an end-to-end project to solve dog versus cat image
    classification using a pretrained VGG-16 model, which is already trained with
    ImageNet. In the end, we will wrap up everything in a Java JFrame and JPanel application
    to make the overall pipeline understandable. Concisely, we will learn the following
    topics throughout an end-to-end project:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将开发一个端到端的项目，使用已通过ImageNet训练的预训练VGG-16模型来解决狗与猫的图像分类问题。最后，我们将把所有内容打包成一个Java
    JFrame和JPanel应用程序，以便让整体流程更加易懂。简而言之，整个端到端项目将帮助我们学习以下内容：
- en: Transfer learning for image classification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类的迁移学习
- en: Developing an image classifier using transfer learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用迁移学习开发图像分类器
- en: Dataset collection and description
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的收集与描述
- en: Developing a dog versus cat detector UI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发一个狗与猫检测器用户界面
- en: '**Frequently Asked Questions** (**FAQs**)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**常见问题解答** (**FAQs**)'
- en: Image classification with pretrained VGG16
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预训练的VGG16进行图像分类
- en: One of the most useful and emerging applications in the ML domain nowadays is
    using the transfer learning technique; it provides high portability between different
    frameworks and platforms.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 目前机器学习领域最有用且新兴的应用之一是使用迁移学习技术；它提供了不同框架和平台之间的高可移植性。
- en: Once you've trained a neural network, what you get is a set of trained hyperparameters'
    values. For example, **LeNet-5** has 60k parameter values, **AlexNet** has 60
    million, and **VGG- 16** has about 138 million parameters. These architectures
    are trained using anything from 1,000 to millions of images and typically have
    very deep architectures, having hundreds of layers that contribute toward so many
    hyperparameters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你训练好一个神经网络，你得到的就是一组训练好的超参数值。例如，**LeNet-5**有60k个参数值，**AlexNet**有6000万个，**VGG-16**有大约1.38亿个参数。这些架构的训练使用了从1000张到数百万张图像，通常这些架构非常深，拥有数百层，这些层都贡献了大量的超参数。
- en: There are many open source community guys or even tech giants who have made
    those pretrained models publicly available for research (and also industry) so
    that they can be restored and reused to solve similar problems. For example, suppose
    we want to classify new images into one of 1,000 classes in the case of AlexNet
    and 10 for LeNet-5\. We typically do not need to deal with so many parameters
    but only a few selected ones (we will see an example soon).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有很多开源社区成员甚至是科技巨头，他们已经公开了这些预训练模型，供研究（以及行业）使用，大家可以恢复并重用这些模型来解决类似问题。例如，假设我们想要将新图像分类到AlexNet的1000个类别之一，或者LeNet-5的10个类别中。我们通常不需要处理这么多参数，而只需关注一些选择出来的参数（我们很快就会看到一个例子）。
- en: In short, we do not need to train such a deep network from scratch, but we reuse
    the existing pre-trained model; still, we manage to achieve acceptable classification
    accuracy. More technically, we can use the weights of that pre-trained model as
    a feature extractor, or we can just initialize our architecture with it and then
    fine-tune them to our new task.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们不需要从头开始训练如此深的网络，而是重用现有的预训练模型；我们仍然能够实现可接受的分类准确率。从技术上讲，我们可以使用该预训练模型的权重作为特征提取器，或者直接用它初始化我们的架构，然后进行微调以适应我们的新任务。
- en: 'In this regard, while using the TL technique to solve your own problem, there
    might be three options available:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，使用迁移学习技术来解决自己的问题时，可能有三种选择：
- en: '**Use a Deep CNN as a fixed feature extractor**: We can reuse a pre-trained
    ImageNet having a fully connected layer by removing the output layer if we are
    no longer interested in the 1,000 categories it has. This way, we can treat all
    other layers, as a feature extractor. Even once you have extracted the features
    using the pre-trained model, you can feed these features to any linear classifier,
    such as the softmax classifier, or even linear SVM!'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将深度CNN用作固定特征提取器**：如果我们不再关心ImageNet中它的1,000个类别，我们可以通过移除输出层来重用预训练的ImageNet，它具有一个完全连接的层。这样，我们可以将其他所有层视为特征提取器。即使在使用预训练模型提取了特征之后，你也可以将这些特征输入到任何线性分类器中，例如softmax分类器，甚至是线性SVM！'
- en: '**Fine-tune the Deep CNN**: Trying to fine-tune the whole network, or even
    most of the layers, may result in overfitting. Therefore, with some extra effort
    to fine-tune the pre-trained weights on your new task using backpropagation.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调深度CNN**：尝试微调整个网络，甚至大多数层，可能会导致过拟合。因此，需要额外的努力，使用反向传播在新任务上微调预训练的权重。'
- en: '**Reuse pre-trained models with checkpointing**: The third widely used scenario
    is to download checkpoints that people have made available on the internet. You
    may go for this scenario if you do not have big computational power to train the
    model from scratch, so you just initialize the model with the released checkpoints
    and then do a little fine-tuning.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重用带检查点的预训练模型**：第三种广泛使用的场景是下载互联网上公开的检查点。如果你没有足够的计算能力从头训练模型，你可以选择这个场景，只需使用已发布的检查点初始化模型，然后进行少量微调。'
- en: 'Now at this point, you may have an interesting question come to mind: what
    is the difference between traditional ML and ML using transfer learning? Well,
    in traditional ML, you do not transfer any knowledge or representations to any
    other task, which is not the case in transfer learning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能会产生一个有趣的问题：传统的机器学习和使用迁移学习的机器学习有什么区别？嗯，在传统的机器学习中，你不会将任何知识或表示转移到其他任务中，而迁移学习则不同。
- en: Unlike traditional machine learning, the source and target task or domains do
    not have to come from the same distribution, but they have to be similar. Moreover,
    you can use transfer learning in case of fewer training samples or if you do not
    have the necessary computational power.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统机器学习不同，源任务和目标任务或领域不必来自相同的分布，但它们必须是相似的。此外，你可以在训练样本较少或没有足够计算能力的情况下使用迁移学习。
- en: '![](img/792d1f27-123a-4af5-b400-7bd0cb749722.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/792d1f27-123a-4af5-b400-7bd0cb749722.png)'
- en: Traditional machine learning versus transfer learning
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 传统机器学习与迁移学习
- en: DL4J and transfer learning
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DL4J 和迁移学习
- en: 'Now, let''s take a look how the DL4J provides us with these functionalities
    through the transfer learning API it has. The DL4J transfer learning API enables
    users to (see more at [https://deeplearning4j.org/transfer-learning](https://deeplearning4j.org/transfer-learning)):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看DL4J是如何通过其迁移学习API为我们提供这些功能的。DL4J的迁移学习API使用户能够（更多信息请见[https://deeplearning4j.org/transfer-learning](https://deeplearning4j.org/transfer-learning)）：
- en: Modify the architecture of an existing model
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改现有模型的架构
- en: Fine-tune learning configurations of an existing model
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调现有模型的学习配置
- en: Hold parameters of a specified layer (also called a **frozen layer**) constant
    during training
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中保持指定层的参数（也叫**冻结层**）不变
- en: 'These functionalities are depicted in the following diagram, where we solve
    task B (similar to task A) using the transfer learning technique:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这些功能在下图中有所体现，我们通过迁移学习技术解决任务B（与任务A相似）：
- en: '![](img/74b69371-b7bf-47e7-8f0a-9ec9bde897ac.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74b69371-b7bf-47e7-8f0a-9ec9bde897ac.png)'
- en: Working principle of transfer learning
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的工作原理
- en: In the next section, we will provide more insights into how to use such a pretrained
    model with DL4J to help us in transfer learning.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将深入探讨如何使用DL4J与预训练模型来帮助我们进行迁移学习。
- en: Developing an image classifier using transfer learning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迁移学习开发图像分类器
- en: In the next section, we will see how to distinguish between dogs and cats based
    on their raw images. We will also see how to implement our first CNN model to
    deal with the raw and color image having three channels.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将展示如何根据狗和猫的原始图像进行区分。我们还将看到如何实现我们的第一个CNN模型来处理具有三通道的原始彩色图像。
- en: This project is highly inspired (but extended significantly) by the "Java Image
    Cat&Dog Recognition with Deep Neural Networks" article by Klevis Ramo ([http://ramok.tech/](http://ramok.tech/)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目深受（但进行了大量扩展）Klevis Ramo的文章《Java图像猫与狗识别与深度神经网络》启发（[http://ramok.tech/](http://ramok.tech/)）。
- en: 'The `code` folder has three packages with a few Java files in each. Their functionalities
    are outlined as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`code`文件夹包含三个包，每个包中有一些Java文件。它们的功能如下：'
- en: '`com.packt.JavaDL.DogvCatClassification.Train`:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`com.packt.JavaDL.DogvCatClassification.Train`：'
- en: '`TrainCatvsDogVG16.java`: It is used to train the network and the trained model
    is saved to a user specific location. Finally, it prints the results.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TrainCatvsDogVG16.java`：用于训练网络，并将训练好的模型保存到用户指定的位置。最后，它输出结果。'
- en: '`PetType.java`: Contains an `enum` type that specifies pet types (that is,
    cat, dog, and unknown).'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PetType.java`：包含一个`enum`类型，指定宠物类型（即，猫、狗和未知）。'
- en: '`VG16CatvDogEvaluator.java`: Restores the trained model saved in a specified
    location by the `TrainCatvsDogVG16.java` class. Then it evaluates on both test
    and validation sets. Finally, it prints the results.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VG16CatvDogEvaluator.java`：恢复由`TrainCatvsDogVG16.java`类保存到指定位置的训练模型。然后它在测试集和验证集上进行评估。最后，输出结果。'
- en: '`com.packt.JavaDL.DogvCatClassification.Classifier`:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`com.packt.JavaDL.DogvCatClassification.Classifier`：'
- en: '`PetClassfier.java`: Gives the user the opportunity to upload a sample image
    (that is, either dog or cat). Then, the user can make the detection from a high-level
    UI.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PetClassfier.java`：为用户提供上传样本图像的机会（即，狗或猫）。然后，用户可以通过高级用户界面进行检测。'
- en: '`com.packt.JavaDL.DogvCatClassification.UI`:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`com.packt.JavaDL.DogvCatClassification.UI`：'
- en: '`ImagePanel.java`: Acts as the image panel by extending the Java JPanel'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ImagePanel.java`：通过扩展Java的JPanel类，作为图像面板使用'
- en: '`UI.java`: Creates the user interface for uploading the image and shows the
    result'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UI.java`：创建上传图像的用户界面并显示结果'
- en: '`ProgressBar.java`: Shows the progress bar'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ProgressBar.java`：显示进度条'
- en: We will explore them step by step. First, let us look at the dataset description.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一步步进行探讨。首先，让我们看看数据集的描述。
- en: Dataset collection and description
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集收集与描述
- en: For this end-to-end project, we will use the dog versus cat dataset from Microsoft
    that was provided for the infamous dogs versus cats classification problem as
    a playground competition. The dataset can be downloaded from [https://www.microsoft.com/en-us/download/details.aspx?id=54765.](https://www.microsoft.com/en-us/download/details.aspx?id=54765.)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个端到端项目，我们将使用微软提供的狗与猫数据集，该数据集曾作为臭名昭著的“狗与猫分类问题”的竞赛平台。数据集可以从[https://www.microsoft.com/en-us/download/details.aspx?id=54765.](https://www.microsoft.com/en-us/download/details.aspx?id=54765.)下载。
- en: 'The train folder contains 25k images of both dogs and cats, where the labels
    are part of the filename. However, the test folder contains 12.5k images named
    according to numeric IDs. Now let''s take a look at some sample snaps from the
    25k images:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 训练文件夹包含25k张狗和猫的图像，其中标签是文件名的一部分。然而，测试文件夹包含12.5k张根据数字ID命名的图像。现在让我们看看从这25k张图像中随机选取的一些样本：
- en: '![](img/56ec1ba5-7445-4ac4-82ae-dc4d575a03ae.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56ec1ba5-7445-4ac4-82ae-dc4d575a03ae.png)'
- en: Showing the true labels of images that are randomly selected
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 显示随机选择的图像的真实标签
- en: For each image in the test set, we have to predict whether an image contains
    a dog (*1 = dog, 0 = cat*). In short, this is a binary classification problem.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试集中的每一张图像，我们必须预测该图像是否包含一只狗（*1 = 狗，0 = 猫*）。简而言之，这是一个二分类问题。
- en: Architecture choice and adoption
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构选择与采纳
- en: 'As mentioned earlier, we will be reusing the VGG-16 pretrained model, which
    is already trained with different images of cat and dog breeds from ImageNet (see
    the list here at [http://www.image-net.org/challenges/LSVRC/2014/results#clsloc](http://www.image-net.org/challenges/LSVRC/2014/results#clsloc)).
    The original VGG-16 model had 1,000 classes of images to be predicted as outlined
    in the following diagram:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将重用VGG-16的预训练模型，该模型已使用来自ImageNet的不同猫狗品种图像进行了训练（请参阅[这里](http://www.image-net.org/challenges/LSVRC/2014/results#clsloc)的列表）。原始的VGG-16模型有1,000个要预测的图像类别，如下图所示：
- en: '![](img/8de28bf5-65da-4908-9b80-d3edef23304d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8de28bf5-65da-4908-9b80-d3edef23304d.png)'
- en: Original VGG-16 model architecture
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 原始VGG-16模型架构
- en: Fortunately, the trained model and network weights are already available on
    the DL4J website (see [http://blob.deeplearning4j.org/models/vgg16_dl4j_inference.zip](http://blob.deeplearning4j.org/models/vgg16_dl4j_inference.zip))
    and the size is about 500 MB.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，训练好的模型和网络权重已经可以在 DL4J 网站上找到（参见 [http://blob.deeplearning4j.org/models/vgg16_dl4j_inference.zip](http://blob.deeplearning4j.org/models/vgg16_dl4j_inference.zip)），大小约为
    500 MB。
- en: You can manually download and restore, or a better way is to do it the DL4J
    way, where you just need to specify the pretrained type (up to DL4J 1.0.0 alpha,
    there were only four pretrained types available, such as ImageNet, CIFAR, MNIST,
    and VGG-Face).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以手动下载和恢复，或者更好的方式是采用 DL4J 的方式，只需指定预训练类型（直到 DL4J 1.0.0 alpha 版本时，只有四种预训练类型可用，如
    ImageNet、CIFAR、MNIST 和 VGG-Face）。
- en: 'The latter is very straightforward; just use the following lines of code and
    the trained model will be downloaded automatically (it will take a while depending
    on Internet speed though):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 后者非常简单；只需使用以下几行代码，训练好的模型将自动下载（但这需要根据网络速度花费一些时间）：
- en: '[PRE0]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the preceding code snippet, the `ComputationGraph` class is used to instantiate
    a computation graph, which is a neural network with an arbitrary (that is, a directed,
    acyclic graph) connection structure. This graph structure may also have an arbitrary
    number of inputs and outputs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，`ComputationGraph` 类被用来实例化一个计算图，它是一个具有任意（即有向无环）连接结构的神经网络。这个图结构也可以有任意数量的输入和输出。
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let''s take a look at the network architecture including the number of
    neurons in/out, the parameter shape, and the number of parameters:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下网络架构，包括进出神经元的数量、参数形状和参数数量：
- en: '![](img/913bd9a8-7cd0-4b01-8494-d4f46594a1ea.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/913bd9a8-7cd0-4b01-8494-d4f46594a1ea.png)'
- en: VGG-16 model architecture as a computational graph
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: VGG-16 模型架构作为计算图
- en: 'Now that we have the pretrained model, using this, we will predict as many
    as 1,000 classes. And the trainable parameters are equal to total parameters:
    138 million. It is a difficult job to train so many parameters.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了预训练的模型，利用它，我们可以预测最多1,000个类别。而可训练的参数数量等于总参数数量：1.38亿。训练这么多参数是件很困难的事。
- en: 'Nevertheless, since we need only two classes to be predicted, we need to modify
    the model architecture slightly such that it outputs only two classes instead
    of 1,000\. So we leave everything unchanged. The modified VGG-16 network will
    then look like this:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们只需要预测两个类别，因此我们需要稍微修改模型架构，使其仅输出两个类别，而不是1,000个。所以我们保持其他部分不变。修改后的 VGG-16
    网络将如下所示：
- en: '![](img/7a85deba-fab7-4f1c-91ec-adacd11c161a.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a85deba-fab7-4f1c-91ec-adacd11c161a.png)'
- en: From the input to the last fully connected layer (that is, fc2) is freeze
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入层到最后一个全连接层（即 fc2）被冻结
- en: In the preceding diagram, we freeze until the last pooling layer and use initial
    weights. The green part is the topic of interest that we want to train, so we
    are going to train only the last layer for the two classes. In other words, in
    our case, we are going to freeze from the input to the last fully connected layer,
    which is `fc2`. That is, the `featurizeExtractionLayer` variable value would be
    `fc2`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们冻结了直到最后一个池化层并使用初始权重。绿色部分是我们希望训练的主题，因此我们只训练最后一层，针对两个类别。换句话说，在我们的案例中，我们将从输入层到最后一个全连接层（即`fc2`）冻结。也就是说，`featurizeExtractionLayer`
    变量的值将是 `fc2`。
- en: 'However, before that, let us define some properties such as seed, the number
    of classes, and up to which layer we want to freeze:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在此之前，让我们定义一些属性，比如种子、类别数量以及我们想冻结到哪一层：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we instantiate the configuration for fine-tuning, which will override
    the values for all non-frozen layers with the values set here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们实例化微调的配置，这将覆盖所有非冻结层的值，并使用此处设置的值：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**FineTuneConfiguration** is the configuration for fine-tuning. Values set
    in this configuration will override the values in each non-frozen layer. Interested
    readers can take a look at [https://deeplearning4j.org/doc/org/deeplearning4j/nn/transferlearning/FineTuneConfiguration.html](https://deeplearning4j.org/doc/org/deeplearning4j/nn/transferlearning/FineTuneConfiguration.html).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**FineTuneConfiguration** 是微调的配置。在此配置中设置的值将覆盖每个非冻结层中的值。有兴趣的读者可以查看 [https://deeplearning4j.org/doc/org/deeplearning4j/nn/transferlearning/FineTuneConfiguration.html](https://deeplearning4j.org/doc/org/deeplearning4j/nn/transferlearning/FineTuneConfiguration.html)。'
- en: 'Then we create a configuration graph that will do the trick: it will work as
    the transfer learner using pretrained VGG-16 model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个配置图，它将完成这项工作：它将作为转移学习器，使用预训练的 VGG-16 模型：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following screenshot shows the output of the previous code snippet:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了前一个代码片段的输出：
- en: '![](img/55a7cae8-7caf-4ffe-9693-5f9134fd91f5.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55a7cae8-7caf-4ffe-9693-5f9134fd91f5.png)'
- en: The frozen network has only 8,194 trainable parameters
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 冻结网络仅有 8,194 个可训练参数
- en: In the preceding code, we removed previously computed predictions and instead
    used our way so that the modified network predicts only two classes by re-adding
    a new predictions layer.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们移除了之前计算的预测，改用了我们的方法，使得修改后的网络仅通过重新添加一个新的预测层来预测两个类别。
- en: In addition, the `setFeatureExtractor` method freezes the weights by specifying
    a layer vertex to set as a feature extractor. Then, the specified layer vertex
    and the layers on the path from an input vertex to it will be frozen, with the
    parameters staying constant.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`setFeatureExtractor` 方法通过指定一个层顶点作为特征提取器来冻结权重。然后，指定的层顶点及其路径上的层（从输入顶点到该层的路径）将被冻结，参数保持不变。
- en: Thus, we are going to train only 8,192 parameters (out of 138 million parameters)
    from the last layer to the two outputs; two extra parameters are for the biases
    for two classes. In short, by freezing until the fc2 layer, now the trainable
    parameters are drastically reduced from 138 million to 8,194 (that is *8,192 network
    params + 2 bias params*).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将只训练 8,192 个参数（在 1.38 亿个参数中），从最后一层到两个输出；另外两个参数是两个类别的偏置。简而言之，通过冻结至 fc2 层，现在可训练参数从
    1.38 亿减少至 8,194（即 *8,192 个网络参数 + 2 个偏置参数*）。
- en: Train and test set preparation
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练集和测试集准备
- en: 'Now that we have created a **ComputationGraph**, we need to prepare the training
    and test sets for the fine-tuning stage. But even before that, we define some
    parameters, such as allowable format and data paths:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个 **ComputationGraph**，接下来需要为微调阶段准备训练集和测试集。但在此之前，我们需要定义一些参数，例如允许的格式和数据路径：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s briefly discuss the difference between MultiLayerNetwork and ComputationGraph.
    In DL4J, there are two types of network composed of multiple layers:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 简要讨论一下 **MultiLayerNetwork** 和 **ComputationGraph** 之间的区别。在 DL4J 中，有两种类型的网络由多个层组成：
- en: '**MultiLayerNetwork**: A stack of neural network layers we''ve used so far.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MultiLayerNetwork**：我们至今使用的神经网络层堆栈。'
- en: '**ComputationGraph**: This allows networks to be built with the following features:
    multiple network input arrays and multiple network outputs (for both classification
    and regression). In this network type, layers connected with each other using
    a directed acyclic graph connection structure.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ComputationGraph**：允许构建具有以下特性的网络：多个网络输入数组和多个网络输出（适用于分类和回归）。在这种网络类型中，层通过有向无环图连接结构相互连接。'
- en: 'Anyway, let''s come to the point. Once the params are set, the next task is
    defining the file paths. Readers should follow this path or show an accurate path
    during training:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，进入正题。设置完参数后，接下来的任务是定义文件路径。读者应该在训练时遵循此路径或提供准确的路径：
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then we will use the `NativeImageLoader` class based on the `JavaCV` library
    for loading images, where the allowed formats are `.bmp`, `.gif`, `.jpg`, `.jpeg`,
    `.jp2`, `.pbm`, `.pgm`, `.ppm`, `.pnm`, `.png`, `.tif`, `.tiff`, `.exr`, and `.webp`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们将使用基于 **JavaCV** 库的 `NativeImageLoader` 类来加载图像，允许的格式包括 `.bmp`、`.gif`、`.jpg`、`.jpeg`、`.jp2`、`.pbm`、`.pgm`、`.ppm`、`.pnm`、`.png`、`.tif`、`.tiff`、`.exr`
    和 `.webp`：
- en: '**JavaCV** uses wrappers from the JavaCPP presets of several libraries for
    computer vision (for example, OpenCV and FFmpeg). More details can be found at
    [https://github.com/bytedeco/javacv](https://github.com/bytedeco/javacv).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**JavaCV** 使用来自 JavaCPP 预设的多个计算机视觉库的封装（例如 OpenCV 和 FFmpeg）。更多详细信息请访问 [https://github.com/bytedeco/javacv](https://github.com/bytedeco/javacv)。'
- en: '[PRE7]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Once the features are extracted from images, we randomly split the features
    space into 80% for training and the remaining 20% for validating the training
    itself to prevent overfitting:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从图像中提取特征，我们将特征空间随机划分为80%用于训练，剩余20%用于验证训练过程，以防止过拟合：
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In addition, our DL4J network will not be able to consume the data in this
    format, but we need to convert it to `DataSetIterator` format:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的 DL4J 网络无法直接处理这种格式的数据，但我们需要将其转换为 `DataSetIterator` 格式：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the preceding lines, we converted both training and validation sets into
    `DataSetIterator` through the `getDataSetIterator()` method. The signature of
    this method can be seen as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码行中，我们通过`getDataSetIterator()`方法将训练集和验证集都转换为`DataSetIterator`。该方法的签名如下：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Fantastic! Up to this point, we have managed to prepare the training sets. Nevertheless,
    remember that this will take a while since it has to process 12,500 images.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！到目前为止，我们已经成功地准备好了训练集。不过，请记住，这个过程可能需要一些时间，因为需要处理12,500张图像。
- en: Now we can start the training. However, you might be wondering why we did not
    talk about the test set. Well, yes! Definitely we will need the test set, too.
    However, let's discuss this in the network evaluation step.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始训练了。不过，你可能会好奇为什么我们没有提到测试集。嗯，没错！我们肯定也需要使用测试集。不过，让我们在网络评估步骤中再讨论这个问题。
- en: Network training and evaluation
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络训练与评估
- en: 'Now that training and test sets are prepared, we can start the training. However,
    before that, we define some hyperparameters for the dataset preparation:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 既然训练集和测试集已经准备好，我们就可以开始训练了。不过，在此之前，我们需要定义一些数据集准备的超参数：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Additionally, we specify the path where the trained model will be saved for
    future reuse:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还指定了训练好的模型将保存的路径，以便未来重复使用：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we can start training the network. We will do the training combined such
    that training is carried out with the training set and validation is effected
    by using the validation set. Finally, the network will evaluate the network performance
    using the test set. Therefore, for this, we need to prepare the test set too:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始训练网络了。我们将进行综合训练，使得训练使用训练集，而验证则使用验证集进行。最后，网络将使用测试集评估网络性能。因此，我们还需要准备测试集：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then we start the training; we used a batch size of 128 and 100 epochs. Therefore,
    the first while loop will be executed 100 times. Then, the second inner `while`
    loop will be executed 196 times (25,000 cat and dog images/128):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们开始训练；我们使用了128的批量大小和100个epoch。因此，第一次`while`循环将执行100次。接着，第二个内部`while`循环将执行196次（25,000张猫狗图像/128）：
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This way, we've already tried to make the training faster, but still it might
    take several hours or even days depending on a number of an epoch that is set.
    And, if the training is carried out on a CPU rather than GPU, it might take several
    days. For me, it took 48 hours for 100 epochs. By the way, my machine has a Core
    i7 processor, 32 GB of RAM, and GeForce GTX 1050 GPU.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们已经尝试使训练变得更快，但仍然可能需要几个小时甚至几天，具体取决于设置的epoch数量。而且，如果训练是在CPU上进行而不是GPU，那么可能需要几天时间。对我来说，100个epoch花了48小时。顺便提一下，我的机器配备的是Core
    i7处理器、32GB内存和GeForce GTX 1050 GPU。
- en: Epoch versus iteration
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 时代与迭代
- en: An epoch is a full traversal through the data, and one iteration is one forward
    and one back propagation on the batch size specified.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一个epoch是对数据的完全遍历，而一个迭代是对指定批量大小的一次前向传播和一次反向传播。
- en: 'Anyway, once the training is complete, the trained model will be saved in the
    location specified previously. Now let us take a look at how the training went.
    For this, we will see the performance on the validation set (as stated earlier,
    we used 15% of the total training set as a validation set, that is, 5,000 images):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，一旦训练完成，训练好的模型将保存在之前指定的位置。现在让我们看看训练的结果如何。为此，我们将查看验证集上的表现（如前所述，我们使用了总训练集的15%作为验证集，也就是5,000张图像）：
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, when we evaluated our model on a full test set (that is, 12,500 images),
    I experienced the following performance metrics:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当我们在完整测试集（即12,500张图像）上评估模型时，我得到了以下的性能指标：
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Restoring the trained model and inferencing
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 恢复训练好的模型并进行推理
- en: 'Now that we have seen how our model performed, it would be worth exploring
    the feasibility of restoring the already trained model. In other words, we will
    restore the trained model and evaluate the network performance on both validation
    and test sets:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看过了模型的表现，值得探索一下恢复已训练模型的可行性。换句话说，我们将恢复训练好的模型，并在验证集和测试集上评估网络性能：
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In the previous line, of code, first, we restored the trained model from the
    disk; then we performed the evaluation on both the test set (full test set) and
    validation set (on 20% of the training set).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面一行代码中，首先，我们从磁盘恢复了训练好的模型；然后，我们在测试集（完整测试集）和验证集（训练集的20%）上进行了评估。
- en: 'Now, let''s take a look at the signature of the `runOnTestSet()` method, which
    is straightforward, in the sense that we already described a similar workflow
    in the previous subsection:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下`runOnTestSet()`方法的签名，它很简单，因为我们在前面的子节中已经描述了类似的工作流程：
- en: '[PRE18]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, let''s take a look at the signature of the `runOnValidationSet` method:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下`runOnValidationSet`方法的签名：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Making simple inferencing
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行简单推理
- en: Now we have seen that our trained model shows outstanding accuracy on both test
    and validation sets. So why don't we develop a UI that would help us make the
    thing easier? As outlined previously, we will develop a simple UI that will allow
    us to unload a sample image, and then we should be able to detect it through a
    simple button press. This part is pure Java, so I'm not going to discuss the details
    here.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到，我们训练的模型在测试集和验证集上都表现出色。那么，为什么不开发一个UI来帮助我们简化操作呢？如前所述，我们将开发一个简单的UI，它将允许我们上传一张样本图片，然后我们应该能够通过按下一个按钮来检测它。这部分是纯Java实现的，所以我在这里不讨论细节。
- en: 'If we run the `PetClassifier.java` class, it first loads our trained model
    and acts as the backend deployed the model. Then it calls the `UI.java` class
    to load the user interface, which looks as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行`PetClassifier.java`类，它首先加载我们训练的模型，并作为后台部署该模型。然后它调用`UI.java`类来加载用户界面，界面如下所示：
- en: '![](img/dd775f02-53c6-4827-b0b2-5b3bdc1593f6.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dd775f02-53c6-4827-b0b2-5b3bdc1593f6.png)'
- en: UI for the cat versus dog recognizer
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 猫狗识别器的UI
- en: 'In the console, you should experience the following logs/messages:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制台中，你应该看到以下日志/消息：
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, let''s upload a few photos from the test set (it makes more sense since
    we are reusing the trained model, which is trained to recognize only the training
    set, so the test set is still unseen):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们上传一些来自测试集的照片（这更有意义，因为我们正在重新使用训练好的模型，而该模型只训练了训练集，因此测试集中的图片仍然是未见过的）：
- en: '![](img/78c5ac06-bae8-46c2-85f3-8f7c5afb8c4b.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/78c5ac06-bae8-46c2-85f3-8f7c5afb8c4b.png)'
- en: Our cat versus dog recognizer recognizes dogs from the images having dogs of
    different shapes and colors
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的猫狗识别器能够识别具有不同形状和颜色的狗的图片
- en: 'Therefore, our trained model has been able to recognize dogs having a different
    shape, size, and color in terms of images. Now, let us try to upload a few cat
    images and see if it works:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们训练好的模型能够识别不同形状、尺寸和颜色的狗的图片。现在，让我们尝试上传几张猫的图片，看看它是否能正常工作：
- en: '![](img/399ca5c8-189f-4585-b8be-d13a23744b0f.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/399ca5c8-189f-4585-b8be-d13a23744b0f.png)'
- en: Our cat versus dog recognizer recognizes cats from the images having cats of
    different shape and colors
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的猫狗识别器能够识别具有不同形状和颜色的猫的图片
- en: Frequently asked questions (FAQs)
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见问题解答（FAQ）
- en: Now that we have solved the dog versus cat classification problem with outstanding
    accuracy, there are other practical aspects of transfer learning and overall deep
    learning phenomena that need to be considered too. In this section, we will see
    some frequently asked questions that might already be on your mind. Answers to
    these questions can be found in Appendix A.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经通过卓越的准确性解决了猫狗分类问题，但转移学习和深度学习现象的其他实际方面也需要考虑。在本节中，我们将看到一些你可能已经在脑海中的常见问题，答案可以在附录A找到。
- en: Can I train the model with my own animal images?
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我可以用自己的动物图片来训练模型吗？
- en: Training using all the images is taking too long. What can I do?
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所有图片进行训练太慢了。我该怎么做？
- en: Can I wrap up this application as a web app?
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我可以将这个应用程序打包成一个Web应用吗？
- en: Can I use VGG-19 for this task?
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我可以使用VGG-19来完成这个任务吗？
- en: How many hyperparameters do we have? I also want to see for each layer.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们有多少个超参数？我还想查看每一层的超参数。
- en: Summary
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we solved an interesting dog versus cat classification problem
    using the transfer learning technique. We used a pre-trained VGG16 model and its
    weights, and subsequently we fine-tuned the training with a real-life cat versus
    dog dataset from Kaggle.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用转移学习技术解决了一个有趣的猫狗分类问题。我们使用了一个预训练的VGG16模型及其权重，然后通过使用来自Kaggle的现实生活猫狗数据集进行微调训练。
- en: Once the training was complete, we saved the trained model for model persistence
    and subsequent reuse. We saw that the trained model can successfully detect and
    differentiate both cat and dog images having very different sizes, qualities,
    and shapes.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们保存了训练好的模型，以便于模型的持久化和后续复用。我们看到，训练好的模型能够成功地检测并区分具有不同尺寸、质量和形状的猫狗图片。
- en: Even the trained model/classifier can be used in solving a real-life cat versus
    dog problem. The takeaway is that this technique with some minimal effort can
    be extended and used for solving similar image classification problems, which
    applies to both binary and multiclass classification problems.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是经过训练的模型/分类器，也可以用于解决现实生活中的猫狗问题。总结来说，这种技术通过一些最小的努力可以扩展，并用于解决类似的图像分类问题，适用于二分类和多分类问题。
- en: In the next chapter, we will see how to develop an end-to-end project that will
    detect objects from video frames when a video clip plays continuously. We will
    also see how to utilize a pre-trained `TinyYOLO` model, which is a smaller variant
    of the original YOLOv2 model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将展示如何开发一个端到端的项目，在视频片段持续播放时从视频帧中检测物体。我们还将学习如何利用预训练的 `TinyYOLO` 模型，它是原始
    YOLOv2 模型的一个小型变体。
- en: Furthermore, some typical challenges in object detection from both still images
    and videos will be discussed. Then we will demonstrate how to solve them using
    bounding box and non-max suppression techniques. Nevertheless, we will see how
    to process a video clip using the JavaCV library on top of DL4J. Finally, we will
    see some frequently asked questions that should be useful for adopting and extending
    this project.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将讨论一些典型的图像和视频中的物体检测挑战。然后，我们将展示如何使用边界框和非最大抑制技术来解决这些问题。最后，我们将展示如何使用 JavaCV
    库和 DL4J 库处理视频片段。最后，我们还将解答一些常见问题，这些问题对于采纳和扩展这个项目非常有帮助。
- en: Answers to questions
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题解答
- en: '**Answer** **to question 1**: Yes, of course, you can. However, please note
    that you have to provide a sufficient number of images, preferably at least a
    few thousand images for each animal type. Otherwise, the model will not be trained
    well.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 1 的回答**：是的，当然可以。不过，请注意，你必须提供足够数量的图像，最好每种动物类型至少提供几千张图像。否则，模型将无法训练得很好。'
- en: '**Answer** **to question 2**: A possible reason could be you are trying to
    feed all the images at once or you are training on CPU (and your machine does
    not have a good configuration). The former can be addressed easily; we can undertake
    the training in batch mode, which is recommended for the era of deep learning.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 2 的回答**：一个可能的原因是你尝试一次性喂入所有图像，或者你在使用 CPU 训练（而你的机器配置不佳）。前者可以通过简单的方式解决；我们可以采用批量模式进行训练，这也是深度学习时代推荐的方式。'
- en: The latter case can be addressed by migrating your training from CPU to GPU.
    However, if your machine does not have a GPU, you can try migrating to Amazon
    GPU instance to get the support for a single (p2.xlarge) or multiple GPUs (for
    example, p2.8xlarge).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 后者的情况可以通过将训练从 CPU 迁移到 GPU 来解决。不过，如果你的机器没有 GPU，你可以尝试迁移到 Amazon GPU 实例，支持单个（p2.xlarge）或多个
    GPU（例如，p2.8xlarge）。
- en: '**Answer** **to question 3**: The application provided should be enough to
    understand the effectiveness of the application. However, this application can
    still be wrapped up as a web application where the trained model can be served
    at the backend.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 3 的回答**：提供的应用程序应该足够帮助你理解应用的有效性。不过，这个应用程序仍然可以包装成一个 Web 应用程序，在后台提供训练好的模型。'
- en: I often use Spring Boot Framework (see more at [https://projects.spring.io/spring-boot/](https://projects.spring.io/spring-boot/))
    for this purpose. Apart from this, Java CUBA studio can be used too (see [https://www.cuba-platform.com/](https://www.cuba-platform.com/)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常使用 Spring Boot 框架（更多信息请参见 [https://projects.spring.io/spring-boot/](https://projects.spring.io/spring-boot/)）来完成这项工作。除此之外，Java
    CUBA Studio 也可以使用（请参见 [https://www.cuba-platform.com/](https://www.cuba-platform.com/)）。
- en: As mentioned earlier in this chapter, VGG-16 is a small variant of VGG-19\.
    Unfortunately, there is no way to use VGG-19 directly. However, readers can try
    to load VGG-19 can be imported with Keras import.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面提到的，VGG-16 是 VGG-19 的一个小型变体。不幸的是，无法直接使用 VGG-19。不过，读者可以尝试使用 Keras 导入 VGG-19。
- en: '**Answer to question 6:** Just use the following code immediately after the
    network initialization:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题 6 的回答**：只需在网络初始化后立即使用以下代码：'
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
