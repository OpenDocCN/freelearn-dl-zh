- en: Appendix 1. References
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 1. 参考文献
- en: '[1] Hsu, F.-H. (2002). Behind Deep Blue: Building the Computer That Defeated
    the World Chess Champion . Princeton University Press, Princeton, NJ, USA. [2]
    Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. 2006\. A fast learning algorithm
    for deep belief nets. Neural Comput. 18, 7 (July 2006), 1527-1554. [3] Bengio,
    Yoshua, et al. "Greedy layer-wise training of deep networks." Advances in neural
    information processing systems 19 (2007): 153. [4] Krizhevsky, Alex, Ilya Sutskever,
    and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural
    networks." Advances in neural information processing systems. 2012. [5] Machine
    Learning, Tom Mitchell, McGraw Hill, 1997. [6] Machine Learning: A Probabilistic
    Perspective (Adaptive Computation and Machine Learning series), Kevin P. Murphy
    [7] O. Chapelle, B. Scholkopf and A. Zien Eds., "Semi-Supervised Learning (Chapelle,
    O. et al., Eds.; 2006) [Book reviews]," in IEEE Transactions on Neural Networks,
    vol. 20, no. 3, pp. 542-542, March 2009. [8] Y. Bengio. Learning deep architectures
    for AI. in Foundations and Trends in Machine Learning, 2(1):1–127, 2009. [9] G.
    Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent DBNHMMs in large vocabulary
    continuous speech recognition. In Proceedings of International Conference on Acoustics
    Speech and Signal Processing (ICASSP). 2011. [10] A. Mohamed, G. Dahl, and G.
    Hinton. Acoustic modeling using deep belief networks. IEEE Transactions on Audio,
    Speech, & Language Processing, 20(1), January 2012. [11] A. Mohamed, D. Yu, and
    L. Deng. Investigation of full-sequence training of deep belief networks for speech
    recognition. In Proceedings of Inter speech. 2010. [12] Indyk, Piotr, and Rajeev
    Motwani. "Approximate nearest neighbors: towards removing the curse of dimensionality."
    Proceedings of the thirtieth annual ACM symposium on Theory of computing. ACM,
    1998. [13] Friedman, Jerome H. "On bias, variance, 0/1—loss, and the curse-of-dimensionality."
    Data mining and knowledge discovery 1.1 (1997): 55-77. [14] Keogh, Eamonn, and
    Abdullah Mueen. "Curse of dimensionality." Encyclopedia of Machine Learning. Springer
    US, 2011\. 257-258. [15] Hughes, G.F. (January 1968). "On the mean accuracy of
    statistical pattern recognizers". IEEE Transactions on Information Theory. 14
    (1): 55–63. [16] Bengio, Yoshua, Patrice Simard, and Paolo Frasconi. "Learning
    long-term dependencies with gradient descent is difficult." IEEE transactions
    on neural networks 5.2 (1994): 157-166.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Hsu, F.-H. (2002). 《深蓝背后：打造击败世界象棋冠军的计算机》. 普林斯顿大学出版社，普林斯顿，新泽西州，美国。 [2] Geoffrey
    E. Hinton, Simon Osindero, 和 Yee-Whye Teh. 2006年. 《深度信念网络的快速学习算法》。神经计算，18(7)（2006年7月），1527-1554。
    [3] Bengio, Yoshua，等人. "深度网络的贪心层训练"。《神经信息处理系统进展》 19 (2007): 153。 [4] Krizhevsky,
    Alex, Ilya Sutskever 和 Geoffrey E. Hinton. "使用深度卷积神经网络进行ImageNet分类。"《神经信息处理系统进展》，2012年。
    [5] 《机器学习》，Tom Mitchell，McGraw Hill，1997年。 [6] 《机器学习：一种概率视角》（自适应计算与机器学习系列），Kevin
    P. Murphy [7] O. Chapelle，B. Scholkopf 和 A. Zien 主编，"半监督学习（Chapelle，O. 等人，主编；2006年）[书评]"，《IEEE神经网络学报》，第20卷，第3期，542-542页，2009年3月。
    [8] Y. Bengio. 《为人工智能学习深度架构》。载于《机器学习基础与趋势》，2(1):1–127，2009年。 [9] G. Dahl，D. Yu，L.
    Deng 和 A. Acero. "基于上下文的DBNHMMs在大词汇量连续语音识别中的应用"。国际声学、语音与信号处理会议（ICASSP）论文集，2011年。
    [10] A. Mohamed，G. Dahl 和 G. Hinton. "使用深度信念网络进行声学建模"。《IEEE音频、语音与语言处理学报》，20(1)，2012年1月。
    [11] A. Mohamed，D. Yu 和 L. Deng. "深度信念网络全序列训练在语音识别中的应用研究"。国际语音学会会议论文集，2010年。 [12]
    Indyk, Piotr 和 Rajeev Motwani. "近似最近邻：消除维度灾难的尝试"。第30届ACM计算理论年会论文集，ACM，1998年。 [13]
    Friedman, Jerome H. "关于偏差、方差、0/1损失和维度灾难"。《数据挖掘与知识发现》 1.1 (1997): 55-77。 [14] Keogh,
    Eamonn 和 Abdullah Mueen. "维度灾难"。《机器学习百科全书》，Springer US，2011年，257-258页。 [15] Hughes,
    G.F. (1968年1月). "关于统计模式识别器的平均准确率"。《IEEE信息理论学报》，14(1): 55–63。 [16] Bengio, Yoshua,
    Patrice Simard 和 Paolo Frasconi. "使用梯度下降学习长期依赖关系是困难的"。《IEEE神经网络学报》5.2 (1994):
    157-166。'
- en: '[17] Ivakhnenko, Alexey (1965). Cybernetic Predicting Devices. Kiev: Naukova
    Dumka. [18] Ivakhnenko, Alexey (1971). "Polynomial theory of complex systems".
    IEEE Transactions on Systems, Man and Cybernetics (4): 364–378. [19] X. Glorot
    and Y. Bengio. Understanding the difficulty of training deep feed-forward neural
    networks. In Proceedings of Artificial Intelligence and Statistics (AISTATS).
    2010. [20] G. Hinton and R. Salakhutdinov. Reducing the dimensionality of data
    with neural networks. Science, 313(5786):504–507, July 2006 [21] M. Ranzato, C.
    Poultney, S. Chopra, and Y. LeCun. Efficient learning of sparse representations
    with an energy-based model. In Proceedings of Neural Information Processing Systems
    (NIPS). 2006. [22] I. Goodfellow, M. Mirza, A. Courville, and Y. Bengio. Multi-prediction
    deep boltzmann machines. In Proceedings of Neural Information Processing Systems
    (NIPS). 2013. [23] R. Salakhutdinov and G. Hinton. Deep boltzmann machines. In
    Proceedings of Artificial Intelligence and Statistics (AISTATS). 2009. [24] R.
    Salakhutdinov and G. Hinton. A better way to pretrain deep boltzmann machines.
    In Proceedings of Neural Information Processing Systems (NIPS). 2012. [25] N.
    Srivastava and R. Salakhutdinov. Multimodal learning with deep boltzmann machines.
    In Proceedings of Neural Information Processing Systems (NIPS). 2012. [26] H.
    Poon and P. Domingos. Sum-product networks: A new deep architecture. In Proceedings
    of Uncertainty in Artificial Intelligence. 2011. [27] R. Gens and P. Domingo.
    Discriminative learning of sum-product networks. Neural Information Processing
    Systems (NIPS), 2012. [28] R. Gens and P. Domingo. Discriminative learning of
    sum-product networks. Neural Information Processing Systems (NIPS), 2012. [29]
    S. Hochreiter. Untersuchungen zu dynamischen neuronalen netzen. Diploma thesis,
    Institut fur Informatik, Technische Universitat Munchen, 1991. [30] J.Martens.
    Deep learning with hessian-free optimization. In Proceedings of international
    Conference on Machine Learning (ICML). 2010. [31] Y. Bengio. Deep learning of
    representations: Looking forward. In Statistical Language and Speech Processing,
    pages 1–37\. Springer, 2013. [32] I. Sutskever. Training recurrent neural networks.
    Ph.D. Thesis, University of Toronto, 2013. [33] J. Ngiam, Z. Chen, P. Koh, and
    A. Ng. Learning deep energy models. In Proceedings of International Conference
    on Machine Learning (ICML). 2011. [34] Y. LeCun, S. Chopra, M. Ranzato, and F.
    Huang. Energy-based models in document recognition and computer vision. In Proceedings
    of International Conference on Document Analysis and Recognition (ICDAR). 2007.
    [35] R. Chengalvarayan and L. Deng. Speech trajectory discrimination using the
    minimum classification error learning. IEEE Transactions on Speech and Audio Processing,
    6(6):505–515, 1998.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[17] Ivakhnenko, Alexey (1965). Cybernetic Predicting Devices. Kiev: Naukova
    Dumka. [18] Ivakhnenko, Alexey (1971). "Polynomial theory of complex systems".
    IEEE Transactions on Systems, Man and Cybernetics (4): 364–378. [19] X. Glorot
    和 Y. Bengio。理解深度前馈神经网络训练的困难。在人工智能和统计学会议论文集（AISTATS）中的论文。2010年。[20] G. Hinton 和
    R. Salakhutdinov。利用神经网络减少数据的维度。科学，313(5786)：504–507，2006年7月[21] M. Ranzato, C.
    Poultney, S. Chopra 和 Y. LeCun。利用基于能量的模型高效学习稀疏表示。在神经信息处理系统（NIPS）会议论文集中。2006年。[22]
    I. Goodfellow, M. Mirza, A. Courville 和 Y. Bengio。多预测深度玻尔兹曼机。在神经信息处理系统（NIPS）会议论文集中。2013年。[23]
    R. Salakhutdinov 和 G. Hinton。深度玻尔兹曼机。在人工智能和统计学会议（AISTATS）论文集中。2009年。[24] R. Salakhutdinov
    和 G. Hinton。预训练深度玻尔兹曼机的更好方法。在神经信息处理系统（NIPS）会议论文集中。2012年。[25] N. Srivastava 和 R.
    Salakhutdinov。多模态学习与深度玻尔兹曼机。在神经信息处理系统（NIPS）会议论文集中。2012年。[26] H. Poon 和 P. Domingos。总-产品网络：一种新的深度架构。在不确定性人工智能会议论文集中。2011年。[27]
    R. Gens 和 P. Domingo。总-产品网络的判别学习。神经信息处理系统（NIPS），2012年。[28] R. Gens 和 P. Domingo。总-产品网络的判别学习。神经信息处理系统（NIPS），2012年。[29]
    S. Hochreiter。动态神经网络研究。技术大学慕尼黑，计算机学院，1991年。毕业论文。[30] J.Martens。使用无Hessian优化的深度学习。在国际机器学习大会（ICML）论文集中。2010年。[31]
    Y. Bengio。代表性的深度学习：展望。在统计语言和语音处理中，页1–37。斯普林格出版社，2013年。[32] I. Sutskever。训练递归神经网络。多伦多大学，博士论文，2013年。[33]
    J. Ngiam, Z. Chen, P. Koh 和 A. Ng。学习深度能量模型。在国际机器学习大会（ICML）论文集中。2011年。[34] Y. LeCun,
    S. Chopra, M. Ranzato 和 F. Huang。文件识别和计算机视觉中的基于能量的模型。在国际文档分析和识别大会（ICDAR）论文集中。2007年。[35]
    R. Chengalvarayan 和 L. Deng。利用最小分类错误学习进行语音轨迹判别。IEEE语音和音频处理交易，6(6)：505–515，1998年。'
- en: '[36] M. Gibson and T. Hain. Error approximation and minimum phone error acoustic
    model estimation. IEEE Transactions on Audio, Speech, and Language Processing,
    18(6):1269–1279, August 2010 [37] X. He, L. Deng, andW. Chou. Discriminative learning
    in sequential pattern recognition — a unifying review for optimization-oriented
    speech recognition. IEEE Signal Processing Magazine, 25:14–36, 2008. [38] H. Jiang
    and X. Li. Parameter estimation of statistical models using convex optimization:
    An advanced method of discriminative training for speech and language processing.
    IEEE Signal Processing Magazine, 27(3):115–127, 2010. [39] B.-H. Juang, W. Chou,
    and C.-H. Lee. Minimum classification error rate methods for speech recognition.
    IEEE Transactions On Speech and Audio Processing, 5:257–265, 1997. [40] D. Povey
    and P. Woodland. Minimum phone error and I-smoothing for improved discriminative
    training. In Proceedings of International Conference on Acoustics Speech and Signal
    Processing (ICASSP). 2002 [41] D. Yu, L. Deng, X. He, and X. Acero. Large-margin
    minimum classification error training for large-scale speech recognition tasks.
    In Proceedings of International Conference on Acoustics Speech and Signal Processing
    (ICASSP). 2007. [42] A. Robinson. An application of recurrent nets to phone probability
    estimation. IEEE Transactions on Neural Networks, 5:298–305, 1994 [43] A. Graves.
    Sequence transduction with recurrent neural networks. Representation Learning
    Workshop, International Conference on Machine Learning (ICML), 2012. [44] A. Graves,
    S. Fernandez, F. Gomez, and J. Schmidhuber. Connectionist temporal classification:
    Labeling unsegmented sequence data with recurrent neural networks. In Proceedings
    of International Conference on Machine Learning (ICML). 2006. [45] A. Graves,
    N. Jaitly, and A. Mohamed. Hybrid speech recognition with deep bidirectional LSTM.
    In Proceedings of the Automatic Speech Recognition and Understanding Workshop
    (ASRU). 2013. [46] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with
    deep recurrent neural networks. In Proceedings of International Conference on
    Acoustics Speech and Signal Processing (ICASSP). 2013 [47] K. Lang, A. Waibel,
    and G. Hinton. A time-delay neural network architecture for isolated word recognition.
    Neural Networks, 3(1):23–43, 1990. [48] A.Waibel, T. Hanazawa, G. Hinton, K. Shikano,
    and K. Lang. Phoneme recognition using time-delay neural networks. IEEE Transactions
    on Acoustical Speech, and Signal Processing, 37:328–339, 1989. [50] Moore, Gordon
    E. (1965-04-19). "Cramming more components onto integrated circuits". Electronics.
    Retrieved 2016-07-01. [51] [http://www.emc.com/collateral/analyst-reports/idc-the-digital-universe-in-2020.pdf](http://www.emc.com/collateral/analyst-reports/idc-the-digital-universe-in-2020.pdf)
    [52] D. Beaver, S. Kumar, H. C. Li, J. Sobel, and P. Vajgel, \Finding a needle
    in haystack: Facebooks photo storage," in OSDI, 2010, pp. 4760. [53] Michele Banko
    and Eric Brill. 2001\. Scaling to very very large corpora for natural language
    disambiguation. In Proceedings of the 39th Annual Meeting on Association for Computational
    Linguistics (ACL ''01). Association for Computational Linguistics, Stroudsburg,
    PA, USA, 26-33. [54] [http://www.huffingtonpost.in/entry/big-data-and-deep-learnin_b_3325352](http://www.huffingtonpost.in/entry/big-data-and-deep-learnin_b_3325352)
    [55] X. W. Chen and X. Lin, "Big Data Deep Learning: Challenges and Perspectives,"
    in IEEE Access, vol. 2, no. , pp. 514-525, 2014. [56] Bengio Y, LeCun Y (2007)
    Scaling learning algorithms towards, AI. In: Bottou L, Chapelle O, DeCoste D,
    Weston J (eds). Large Scale Kernel Machines. MIT Press, Cambridge, MA Vol. 34\.
    pp 321–360\. [http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf](http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf)
    [57] A. Coats, B. Huval, T. Wng, D. Wu, and A. Wu, ``Deep Learning with COTS HPS
    systems,'''' J. Mach. Learn. Res., vol. 28, no. 3, pp. 1337-1345, 2013. [58] J.Wang
    and X. Shen, ``Large margin semi-supervised learning,'''' J. Mach. Learn. Res.,
    vol. 8, no. 8, pp. 1867-1891, 2007 [59] R. Fergus, Y. Weiss, and A. Torralba,
    ``Semi-supervised learning in gigantic image collections,'''' in Proc. Adv. NIPS,
    2009, pp. 522-530. [60] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Ng,
    ``Multimodal deep learning,'''' in Proc. 28th Int. Conf. Mach. Learn., Bellevue,
    WA, USA, 2011 [61] N. Srivastava and R. Salakhutdinov, ``Multimodal learning with
    deep Boltzmann machines,'''' in Proc. Adv. NIPS, 2012 [62] L. Bottou, ``Online
    algorithms and stochastic approximations,'''' in On-Line Learning in Neural Networks,
    D. Saad, Ed. Cambridge, U.K.: Cambridge Univ. Press, 1998. [63] A. Blum and C.
    Burch, ``On-line learning and the metrical task system problem,'''' in Proc. 10th
    Annu. Conf. Comput. Learn. Theory, 1997, pp. 45-53. [64] N. Cesa-Bianchi, Y. Freund,
    D. Helmbold, and M. Warmuth, ``On-line prediction and conversation strategies,''''
    in Proc. Conf. Comput. Learn. Theory Eurocolt, vol. 53\. Oxford, U.K., 1994, pp.
    205-216. [65] Y. Freund and R. Schapire, ``Game theory, on-line prediction and
    boosting,'''' in Proc. 9th Annu. Conf. Comput. Learn. Theory, 1996, pp. 325-332.
    [66] Q. Le et al., ‘‘Building high-level features using large scale unsupervised
    learning,’’ in Proc. Int. Conf. Mach. Learn., 2012. [67] C. P. Lim and R. F. Harrison,
    ``Online pattern classifcation with multiple neural network systems: An experimental
    study,'''' IEEE Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 33, no. 2, pp.
    235-247, May 2003. [68] P. Riegler and M. Biehl, ``On-line backpropagation in
    two-layered neural networks,'''' J. Phys. A, vol. 28, no. 20, pp. L507-L513, 1995
    [69] M. Rattray and D. Saad, ``Globally optimal on-line learning rules for multi-layer
    neural networks,'''' J. Phys. A, Math. General, vol. 30, no. 22, pp. L771-776,
    1997.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[36] M. Gibson 和 T. Hain. 错误逼近与最小电话错误声学模型估计。IEEE 音频、语音与语言处理学报，18(6)：1269–1279，2010年8月。[37]
    X. He, L. Deng 和 W. Chou. 序列模式识别中的判别学习—面向优化的语音识别统一综述。IEEE 信号处理杂志，25：14–36，2008年。[38]
    H. Jiang 和 X. Li. 使用凸优化的统计模型参数估计：一种面向语音和语言处理的先进判别训练方法。IEEE 信号处理杂志，27(3)：115–127，2010年。[39]
    B.-H. Juang, W. Chou 和 C.-H. Lee. 语音识别的最小分类错误率方法。IEEE 语音与音频处理学报，5：257–265，1997年。[40]
    D. Povey 和 P. Woodland. 最小电话错误与 I 平滑用于改进判别训练。在国际声学、语音与信号处理会议（ICASSP）上发表，2002年。[41]
    D. Yu, L. Deng, X. He 和 X. Acero. 面向大规模语音识别任务的“大间隔最小分类错误训练”。在国际声学、语音与信号处理会议（ICASSP）上发表，2007年。[42]
    A. Robinson. 循环神经网络在电话概率估计中的应用。IEEE 神经网络学报，5：298–305，1994年。[43] A. Graves. 使用循环神经网络的序列转导。国际机器学习会议（ICML）表征学习研讨会，2012年。[44]
    A. Graves, S. Fernandez, F. Gomez 和 J. Schmidhuber. 连接主义时序分类：使用循环神经网络标注未分段序列数据。在国际机器学习会议（ICML）上发表，2006年。[45]
    A. Graves, N. Jaitly 和 A. Mohamed. 使用深度双向 LSTM 的混合语音识别。在自动语音识别与理解研讨会（ASRU）上发表，2013年。[46]
    A. Graves, A. Mohamed 和 G. Hinton. 使用深度循环神经网络的语音识别。在国际声学、语音与信号处理会议（ICASSP）上发表，2013年。[47]
    K. Lang, A. Waibel 和 G. Hinton. 用于孤立词识别的时延神经网络架构。神经网络，3(1)：23–43，1990年。[48] A.
    Waibel, T. Hanazawa, G. Hinton, K. Shikano 和 K. Lang. 使用时延神经网络的音素识别。IEEE 声学、语音与信号处理学报，37：328–339，1989年。[50]
    Moore, Gordon E. (1965-04-19). "将更多组件集成到集成电路中"。电子学。2016-07-01 检索。[51] [http://www.emc.com/collateral/analyst-reports/idc-the-digital-universe-in-2020.pdf](http://www.emc.com/collateral/analyst-reports/idc-the-digital-universe-in-2020.pdf)
    [52] D. Beaver, S. Kumar, H. C. Li, J. Sobel 和 P. Vajgel, “在干草堆中找针：Facebook 的照片存储”，在
    OSDI，2010年，第47-60页。[53] Michele Banko 和 Eric Brill. 2001年。大规模语料库在自然语言歧义消解中的扩展。在第39届计算语言学年会（ACL
    ''01）上发表。计算语言学协会，美国宾夕法尼亚州斯特劳兹堡，26-33页。[54] [http://www.huffingtonpost.in/entry/big-data-and-deep-learnin_b_3325352](http://www.huffingtonpost.in/entry/big-data-and-deep-learnin_b_3325352)
    [55] X. W. Chen 和 X. Lin, "大数据深度学习：挑战与展望," 在 IEEE Access，第2卷，第X期，514-525页，2014年。[56]
    Bengio Y, LeCun Y (2007) 将学习算法扩展至人工智能。In: Bottou L, Chapelle O, DeCoste D, Weston
    J (编). 大规模核机器。麻省理工学院出版社，美国剑桥，第34卷。第321–360页。[http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf](http://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf)
    [57] A. Coats, B. Huval, T. Wng, D. Wu 和 A. Wu, “使用COTS HPS系统的深度学习，”J. Mach. Learn.
    Res., 第28卷，第3期，第1337-1345页，2013年。[58] J.Wang 和 X. Shen, "大间隔半监督学习," J. Mach. Learn.
    Res., 第8卷，第8期，第1867-1891页，2007年。[59] R. Fergus, Y. Weiss 和 A. Torralba, “在庞大的图像集合中进行半监督学习”，在NIPS会议论文集（2009年）中，第522-530页。[60]
    J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee 和 A. Ng, “多模态深度学习”，在第28届国际机器学习大会（2011年）上发表，美国华盛顿州贝尔维尤。[61]
    N. Srivastava 和 R. Salakhutdinov, “使用深度玻尔兹曼机的多模态学习”，在NIPS会议论文集（2012年）中发表。[62]
    L. Bottou, “在线算法与随机逼近”，在《神经网络中的在线学习》，D. Saad（编）。剑桥大学出版社，英国剑桥，1998年。[63] A. Blum
    和 C. Burch, “在线学习与度量任务系统问题”，在第10届计算学习理论年会（1997年）上发表，第45-53页。[64] N. Cesa-Bianchi,
    Y. Freund, D. Helmbold 和 M. Warmuth, “在线预测与对话策略”，在欧罗科尔特计算学习理论会议论文集，第53卷。英国牛津，1994年，第205-216页。[65]
    Y. Freund 和 R. Schapire, “博弈论、在线预测与提升”，在第9届计算学习理论年会（1996年）上发表，第325-332页。[66] Q.
    Le 等，“使用大规模无监督学习构建高级特征”，在国际机器学习会议论文集，2012年。[67] C. P. Lim 和 R. F. Harrison, “使用多个神经网络系统的在线模式分类：一项实验研究”，IEEE
    系统、人工智能与控制学报，C卷，33(2)：235-247，2003年5月。[68] P. Riegler 和 M. Biehl, “两层神经网络中的在线反向传播”，J.
    Phys. A，第28卷，第20期，第L507-L513页，1995年。[69] M. Rattray 和 D. Saad, “多层神经网络的全局最优在线学习规则”，J.
    Phys. A，数学一般，第30卷，第22期，第L771-776页，1997年。'
- en: '[70] P. Campolucci, A. Uncini, F. Piazza, and B. Rao, ``On-line learning algorithms
    for locally recurrent neural networks,'''' IEEE Trans. Neural Netw., vol. 10,
    no. 2, pp. 253-271, Mar. 1999 [71] N. Liang, G. Huang, P. Saratchandran, and N.
    Sundararajan, ``A fast and accurate online sequential learning algorithm for feedforward
    networks,'''' IEEE Trans. Neural Netw., vol. 17, no. 6, pp. 1411-1423, Nov. 2006.
    [72] L. Bottou and O. Bousequet, ``Stochastic gradient learning in neural networks,''''
    in Proc. Neuro-Nimes, 1991. [73] S. Shalev-Shwartz, Y. Singer, and N. Srebro,
    ``Pegasos: Primal estimated sub-gradient solver for SVM,'''' in Proc. Int. Conf.
    Mach. Learn., 2007. [74] D. Scherer, A. Müller, and S. Behnke, ``Evaluation of
    pooling operations in convolutional architectures for object recognition,''''
    in Proc. Int. Conf. Artif. Neural Netw., 2010, pp. 92-101. [75] J. Chien and H.
    Hsieh, ``Nonstationary source separation using sequential and variational Bayesian
    learning,'''' IEEE Trans. Neural Netw. Learn. Syst., vol. 24, no. 5, pp. 681-694,
    May 2013. [76] W. de Oliveira, ``The Rosenblatt Bayesian algorithm learning in
    a nonstationary environment,'''' IEEE Trans. Neural Netw., vol. 18, no. 2, pp.
    584-588, Mar. 2007. [77] Hadoop Distributed File System,[http://hadoop.apache.org/2012](http://hadoop.apache.org).
    [78] T. White. 2009\. Hadoop: The Definitive Guide. OReilly Media, Inc. June 2009
    [79] Shvachko, K.; Hairong Kuang; Radia, S.; Chansler, R., May 2010\. The Hadoop
    Distributed File System,"2010 IEEE 26th Symposium on Mass Storage Systems and
    Technologies (MSST). vol., no., pp.1,10 [80] Hadoop Distributed File System,[https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/).
    [81] Dev, Dipayan, and Ripon Patgiri. "Dr. Hadoop: an infinite scalable metadata
    management for Hadoop—How the baby elephant becomes immortal." Frontiers of Information
    Technology & Electronic Engineering 17 (2016): 15-31. [82] [http://deeplearning4j.org/](http://deeplearning4j.org/)
    [83] Dean, Jeffrey, and Sanjay Ghemawat. "MapReduce: simplified data processing
    on large clusters." Communications of the ACM 51.1 (2008): 107-113. [84] [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)
    [85] [http://torch.ch/](http://torch.ch/) [86] Borthakur, Dhruba. "The hadoop
    distributed file system: Architecture and design." Hadoop Project Website 11.2007
    (2007): 21. [87] Borthakur, Dhruba. "HDFS architecture guide." HADOOP APACHE PROJECT
    [https://hadoop.apache.org/docs/r1.2.1/hdfs_design.pdf](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.pdf)
    (2008): 39. [88] [http://deeplearning4j.org/quickstart](http://deeplearning4j.org/quickstart)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[70] P. Campolucci, A. Uncini, F. Piazza 和 B. Rao，``针对局部递归神经网络的在线学习算法，''''IEEE
    Trans. Neural Netw.，第10卷，第2期，页253-271，1999年3月。[71] N. Liang, G. Huang, P. Saratchandran
    和 N. Sundararajan，``一种快速且准确的前馈网络在线顺序学习算法，''''IEEE Trans. Neural Netw.，第17卷，第6期，页1411-1423，2006年11月。[72]
    L. Bottou 和 O. Bousequet，``神经网络中的随机梯度学习，''''在Neuro-Nimes会议论文集，1991年。[73] S. Shalev-Shwartz,
    Y. Singer 和 N. Srebro，``Pegasos：SVM的原始估计子梯度求解器，''''在国际机器学习会议论文集，2007年。[74] D.
    Scherer, A. Müller 和 S. Behnke，``卷积架构中池化操作的评估用于物体识别，''''在国际人工神经网络会议论文集，2010年，页92-101。[75]
    J. Chien 和 H. Hsieh，``使用顺序和变分贝叶斯学习进行非平稳源分离，''''IEEE Trans. Neural Netw. Learn.
    Syst.，第24卷，第5期，页681-694，2013年5月。[76] W. de Oliveira，``Rosenblatt贝叶斯算法在非平稳环境中的学习，''''IEEE
    Trans. Neural Netw.，第18卷，第2期，页584-588，2007年3月。[77] Hadoop分布式文件系统，[http://hadoop.apache.org/2012](http://hadoop.apache.org)。[78]
    T. White. 2009年\. 《Hadoop：权威指南》。O''Reilly Media, Inc. 2009年6月。[79] Shvachko, K.;
    Hairong Kuang; Radia, S.; Chansler, R.，2010年5月\. 《Hadoop分布式文件系统》，2010年IEEE第26届大容量存储系统与技术研讨会（MSST）。卷，无，页1，10。[80]
    Hadoop分布式文件系统，[https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/)。[81]
    Dev, Dipayan 和 Ripon Patgiri. "Dr. Hadoop: 一种无限可扩展的Hadoop元数据管理——小象如何变得永生。" 《信息技术与电子工程前沿》17（2016年）：15-31。[82]
    [http://deeplearning4j.org/](http://deeplearning4j.org/) [83] Dean, Jeffrey 和
    Sanjay Ghemawat. "MapReduce: 简化大规模集群数据处理。" 《ACM通讯》51.1（2008年）：107-113。[84] [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)
    [85] [http://torch.ch/](http://torch.ch/) [86] Borthakur, Dhruba. "Hadoop分布式文件系统：架构与设计。"
    Hadoop项目网站 11.2007（2007年）：21。[87] Borthakur, Dhruba. "HDFS架构指南。" HADOOP APACHE
    PROJECT [https://hadoop.apache.org/docs/r1.2.1/hdfs_design.pdf](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.pdf)（2008年）：39。[88]
    [http://deeplearning4j.org/quickstart](http://deeplearning4j.org/quickstart)'
- en: '[89] LeCun, Yann, and Yoshua Bengio. "Convolutional networks for images, speech,
    and time series." The handbook of brain theory and neural networks 3361.10 (1995):
    1995. [90] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based
    learning applied to document recognition. Proc. IEEE 86, 2278–2324\. doi:10.1109/5.726791
    [91] Gao, H., Mao, J., Zhou, J., Huang, Z., Wang, L., and Xu, W. (2015). Are you
    talking to a machine? Dataset and methods for multilingual image question answering.
    arXiv preprint arXiv:1505.05612. [92] Srinivas, Suraj, et al. "A Taxonomy of Deep
    Convolutional Neural Nets for Computer Vision." arXiv preprint arXiv:1601.06615
    (2016). [93] Zhou, Y-T., et al. "Image restoration using a neural network." IEEE
    Transactions on Acoustics, Speech, and Signal Processing 36.7 (1988): 1141-1151.
    [94] Maas, Andrew L., Awni Y. Hannun, and Andrew Y. Ng. "Rectifier nonlinearities
    improve neural network acoustic models." Proc. ICML. Vol. 30\. No. 1\. 2013. [95]
    He, Kaiming, et al. "Delving deep into rectifiers: Surpassing human-level performance
    on imagenet classification." Proceedings of the IEEE International Conference
    on Computer Vision. 2015. [96] [http://web.engr.illinois.edu/~slazebni/spring14/lec24_cnn.pdf](http://web.engr.illinois.edu/~slazebni/spring14/lec24_cnn.pdf)
    [97] Zeiler, Matthew D., and Rob Fergus. "Visualizing and understanding convolutional
    networks." European Conference on Computer Vision. Springer International Publishing,
    2014. [98] Simonyan, Karen, and Andrew Zisserman. "Very deep convolutional networks
    for large-scale image recognition." arXiv preprint arXiv:1409.1556 (2014). [99]
    Szegedy, Christian, et al. "Going deeper with convolutions." Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition. 2015. [100] He, Kaiming,
    et al. "Deep residual learning for image recognition." arXiv preprint arXiv:1512.03385
    (2015). [101] Krizhevsky, Alex. "One weird trick for parallelizing convolutional
    neural networks." arXiv preprint arXiv:1404.5997 (2014). [102] S. Hochreiter and
    J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.
    [103] Mikolov, Tomas, et al. "Recurrent neural network based language model."
    Interspeech. Vol. 2\. 2010. [104] Rumelhart, D. E., Hinton, G. E., and Williams,
    R. J. (1986). Learning representations by backpropagating errors. Nature, 323,
    533–536. [105] Mikolov, T., Sutskever, I., Chen, K., Corrado, G., and Dean, J.
    (2013a). Distributed representations of words and phrases and their compositionality.
    In Advances in Neural Information Processing Systems 26, pages 3111–3119. [106]Graves,
    A. (2013). Generating sequences with recurrent neural networks. arXiv:1308.0850 [cs.NE].
    [107] Pascanu, R., Mikolov, T., and Bengio, Y. (2013a). On the difficulty of training
    recurrent neural networks. In ICML’2013.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '[89] LeCun, Yann和Yoshua Bengio。"用于图像、语音和时间序列的卷积网络。"大脑理论和神经网络手册3361.10（1995年）：1995。
    [90] LeCun, Y.、Bottou, L.、Bengio, Y.和Haffner, P.（1998）。基于梯度的文档识别应用学习。IEEE 86,
    2278-2324。doi:10.1109/5.726791 [91] Gao, H.、Mao, J.、Zhou, J.、Huang, Z.、Wang, L.和Xu,
    W.（2015）。您在与机器交谈吗？多语言图像问答的数据集和方法。arXiv预印本arXiv:1505.05612。 [92] Srinivas, Suraj等人。"计算机视觉深度卷积神经网络的分类学。"arXiv预印本arXiv:1601.06615（2016）。
    [93] Zhou, Y-T.等人。"使用神经网络进行图像恢复。"IEEE声学、语音和信号处理期刊36.7（1988）：1141-1151。 [94] Maas,
    Andrew L.、Awni Y. Hannun和Andrew Y. Ng。"整流器非线性改进神经网络声学模型。"Proc. ICML。Vol. 30。No.
    1。2013。 [95] He, Kaiming等人。"深入研究整流器：超越图像分类上的人类水平性能。"IEEE国际计算机视觉会议论文集。2015。 [96]
    [http://web.engr.illinois.edu/~slazebni/spring14/lec24_cnn.pdf](http://web.engr.illinois.edu/~slazebni/spring14/lec24_cnn.pdf)
    [97] Zeiler, Matthew D.和Rob Fergus。"可视化和理解卷积网络。"欧洲计算机视觉会议。斯普林格国际出版社，2014年。 [98]
    Simonyan, Karen和Andrew Zisserman。"用于大规模图像识别的非常深的卷积网络。"arXiv预印本arXiv:1409.1556（2014）。
    [99] Szegedy, Christian等人。"通过卷积更深入地挖掘。"IEEE计算机视觉与模式识别会议论文集。2015。 [100] He, Kaiming等人。"用于图像识别的深度残余学习。"arXiv预印本arXiv:1512.03385（2015）。
    [101] Krizhevsky, Alex。"并行化卷积神经网络的一个奇怪的技巧。"arXiv预印本arXiv:1404.5997（2014）。 [102]
    S. Hochreiter和J. Schmidhuber。长短期记忆。神经计算，9（8）：1735-1780，1997。 [103] Mikolov, Tomas等人。"基于递归神经网络的语言模型。"Interspeech。Vol.
    2。2010。 [104] Rumelhart, D. E.、Hinton, G. E.和Williams, R. J.（1986）。通过反向传播学习表示。自然，323，533-536。
    [105] Mikolov, T.、Sutskever, I.、Chen, K.、Corrado, G.和Dean, J.（2013a）。单词和短语的分布式表示及其组合性。在神经信息处理系统26的进展中，页面3111-3119。
    [106] Graves, A.（2013）。使用递归神经网络生成序列。arXiv:1308.0850 [cs.NE]。 [107] Pascanu, R.、Mikolov,
    T.和Bengio, Y.（2013a）。训练递归神经网络的困难。在ICML’2013。'
- en: '[108] Mikolov, T., Sutskever, I., Deoras, A., Le, H., Kombrink, S., and Cernocky,
    J. (2012a). Subword language modeling with neural networks. unpublished [109]
    Graves, A., Mohamed, A., and Hinton, G. (2013). Speech recognition with deep recurrent
    neural networks. ICASSP [110] Graves, A., Liwicki, M., Fernandez, S., Bertolami,
    R., Bunke, H., and Schmidhuber, J. (2009). A novel connectionist system for improved
    unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis and
    Machine Intelligence. [111] [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    [112] [https://web.stanford.edu/group/pdplab/pdphandbook/handbookch8.html](https://web.stanford.edu/group/pdplab/pdphandbook/handbookch8.html)
    [113] Schuster, Mike, and Kuldip K. Paliwal. "Bidirectional recurrent neural networks."
    IEEE Transactions on Signal Processing 45.11 (1997): 2673-2681. [114] Graves,
    Alan, Navdeep Jaitly, and Abdel-rahman Mohamed. "Hybrid speech recognition with
    deep bidirectional LSTM." Automatic Speech Recognition and Understanding (ASRU),
    2013 IEEE Workshop on. IEEE, 2013 [115] Baldi, Pierre, et al. "Exploiting the
    past and the future in protein secondary structure prediction." Bioinformatics
    15.11 (1999): 937-946 [116] Hochreiter, Sepp, and Jürgen Schmidhuber. "Long short-term
    memory." Neural computation 9.8 (1997): 1735-1780. [117] A. Graves, M. Liwicki,
    S. Fernandez, R. Bertolami, H. Bunke, J. Schmidhuber. A Novel Connectionist System
    for Improved Unconstrained Handwriting Recognition. IEEE Transactions on Pattern
    Analysis and Machine Intelligence, vol. 31, no. 5, 2009. [118] With QuickType,
    Apple wants to do more than guess your next text. It wants to give you an AI.".
    WIRED. Retrieved 2016-06-16 [119] Sak, Hasim, Andrew W. Senior, and Françoise
    Beaufays. "Long short-term memory recurrent neural network architectures for large
    scale acoustic modeling." INTERSPEECH. 2014. [120] Poultney, Christopher, Sumit
    Chopra, and Yann L. Cun. "Efficient learning of sparse representations with an
    energy-based model." Advances in neural information processing systems. 2006.
    [121] LeCun, Yann, et al. "A tutorial on energy-based learning." Predicting structured
    data 1 (2006): 0. [122] Ackley, David H., Geoffrey E. Hinton, and Terrence J.
    Sejnowski. "A learning algorithm for Boltzmann machines." Cognitive science 9.1
    (1985): 147-169. [123] Desjardins, G. and Bengio, Y. (2008). Empirical evaluation
    of convolutional RBMs for vision. Technical Report 1327, Département d’Informatique
    et de Recherche Opérationnelle, Université de Montréal. [124] Hinton, G. E., Osindero,
    S., and Teh, Y. (2006). A fast learning algorithm for deep belief nets. Neural
    Computation, 18, 1527–1554. [125] Hinton, G. E. (2007b). Learning multiple layers
    of representation. Trends in cognitive sciences , 11(10), 428–434.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[108] Mikolov, T., Sutskever, I., Deoras, A., Le, H., Kombrink, S., 和 Cernocky,
    J. (2012a)。使用神经网络的子词语言建模。未发表的[109] Graves, A., Mohamed, A., 和 Hinton, G. (2013)。深度递归神经网络的语音识别。ICASSP[110]
    Graves, A., Liwicki, M., Fernandez, S., Bertolami, R., Bunke, H., 和 Schmidhuber,
    J. (2009)。一种新型的连接主义系统，用于改善无约束手写识别。IEEE模式分析与机器智能汇刊。[111] [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    [112] [https://web.stanford.edu/group/pdplab/pdphandbook/handbookch8.html](https://web.stanford.edu/group/pdplab/pdphandbook/handbookch8.html)
    [113] Schuster, Mike, 和 Kuldip K. Paliwal。"双向递归神经网络"。IEEE信号处理汇刊 45.11 (1997):
    2673-2681。[114] Graves, Alan, Navdeep Jaitly, 和 Abdel-rahman Mohamed。"基于深度双向LSTM的混合语音识别"。自动语音识别与理解（ASRU），2013
    IEEE研讨会。IEEE，2013[115] Baldi, Pierre, 等。"在蛋白质二级结构预测中利用过去和未来"。生物信息学 15.11 (1999):
    937-946[116] Hochreiter, Sepp, 和 Jürgen Schmidhuber。"长短时记忆"。神经计算 9.8 (1997): 1735-1780。[117]
    A. Graves, M. Liwicki, S. Fernandez, R. Bertolami, H. Bunke, J. Schmidhuber。改进的无约束手写识别的新型连接主义系统。IEEE模式分析与机器智能汇刊，31卷，第5期，2009年。[118]
    使用QuickType，苹果公司希望做的不仅仅是猜测你的下一条文本。它还想给你一个AI。"WIRED"。2016年6月16日检索。[119] Sak, Hasim,
    Andrew W. Senior, 和 Françoise Beaufays。"用于大规模声学建模的长短时记忆递归神经网络架构"。INTERSPEECH。2014年。[120]
    Poultney, Christopher, Sumit Chopra, 和 Yann L. Cun。"利用基于能量的模型进行稀疏表示的高效学习"。神经信息处理系统进展。2006年。[121]
    LeCun, Yann, 等。"关于基于能量的学习的教程"。结构化数据预测 1 (2006): 0。[122] Ackley, David H., Geoffrey
    E. Hinton, 和 Terrence J. Sejnowski。"Boltzmann机的学习算法"。认知科学 9.1 (1985): 147-169。[123]
    Desjardins, G. 和 Bengio, Y. (2008)。卷积RBM在视觉中的经验评估。技术报告1327，蒙特利尔大学计算机科学与运筹学系。[124]
    Hinton, G. E., Osindero, S., 和 Teh, Y. (2006)。一种快速学习算法，用于深度置信网络。神经计算，18，1527–1554。[125]
    Hinton, G. E. (2007b)。学习多层表示。认知科学趋势，11(10)，428–434。'
- en: '[126] Bengio, Yoshua, et al. "Greedy layer-wise training of deep networks."
    Advances in neural information processing systems 19 (2007): 153. [127] A.-R.
    Mohamed, T. N. Sainath, G. Dahl, B. Ramabhadran, G. E. Hinton, and M. A. Picheny,
    ``Deep belief networks using discriminative features for phone recognition,''''
    in Proc. IEEE ICASSP, May 2011, pp. 5060-5063. [128] R. Salakhutdinov and G. Hinton,
    ``Semantic hashing,'''' Int. J. Approx. Reasoning, vol. 50, no. 7, pp. 969-978,
    2009. [129] G. W. Taylor, G. E. Hinton, and S. T. Roweis, ``Modeling human motion
    using binary latent variables,'''' in Advances in Neural Information Processing
    Systems. Cambridge, MA, USA: MIT Press, 2006,pp. 1345-1352. [130] Zhang, Kunlei,
    and Xue-Wen Chen. "Large-scale deep belief nets with mapreduce." IEEE Access 2
    (2014): 395-403.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[126] Bengio, Yoshua 等人. "深度网络的贪婪分层训练." 《神经信息处理系统进展》19 (2007): 153. [127] A.-R.
    Mohamed, T. N. Sainath, G. Dahl, B. Ramabhadran, G. E. Hinton 和 M. A. Picheny,
    ``使用判别特征进行电话识别的深度信念网络,'''' 见《IEEE ICASSP会议论文集》，2011年5月，pp. 5060-5063. [128] R.
    Salakhutdinov 和 G. Hinton, ``语义哈希,'''' 《近似推理国际期刊》，第50卷，第7期，pp. 969-978，2009年.
    [129] G. W. Taylor, G. E. Hinton 和 S. T. Roweis, ``使用二进制潜变量建模人类运动,'''' 见《神经信息处理系统进展》.
    美国马萨诸塞州剑桥: MIT出版社, 2006年，pp. 1345-1352. [130] Zhang, Kunlei 和 Xue-Wen Chen. "大规模深度信念网络与MapReduce."
    《IEEE Access》2 (2014): 395-403.'
- en: '[131] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning:
    A review and new perspectives. Technical report, arXiv:1206.5538, 2012b. [132]
    Makhzani, Alireza, and Brendan Frey. "k-Sparse Autoencoders." arXiv preprint arXiv:1312.5663
    (2013). [133] Hinton, Geoffrey E., and Ruslan R. Salakhutdinov. "Reducing the
    dimensionality of data with neural networks." Science 313.5786 (2006): 504-507.
    [134] Vincent, Pascal, et al. "Stacked denoising autoencoders: Learning useful
    representations in a deep network with a local denoising criterion." Journal of
    Machine Learning Research 11.Dec (2010): 3371-3408. [135] Salakhutdinov, Ruslan,
    and Geoffrey Hinton. "Semantic hashing." RBM 500.3 (2007): 500. [136] Nesi, Paolo,
    Gianni Pantaleo, and Gianmarco Sanesi. "A hadoop based platform for natural language
    processing of web pages and documents." Journal of Visual Languages & Computing
    31 (2015): 130-138.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[131] Yoshua Bengio, Aaron Courville 和 Pascal Vincent. 表示学习：回顾与新视角. 技术报告，arXiv:1206.5538，2012b.
    [132] Makhzani, Alireza 和 Brendan Frey. "k-稀疏自编码器." arXiv预印本 arXiv:1312.5663 (2013).
    [133] Hinton, Geoffrey E. 和 Ruslan R. Salakhutdinov. "使用神经网络减少数据的维度." 《科学》313.5786
    (2006): 504-507. [134] Vincent, Pascal 等人. "堆叠去噪自编码器：在具有局部去噪准则的深度网络中学习有用的表示."
    《机器学习研究期刊》11.12 (2010): 3371-3408. [135] Salakhutdinov, Ruslan 和 Geoffrey Hinton.
    "语义哈希." RBM 500.3 (2007): 500. [136] Nesi, Paolo, Gianni Pantaleo 和 Gianmarco
    Sanesi. "基于Hadoop的平台用于网页和文档的自然语言处理." 《视觉语言与计算期刊》31 (2015): 130-138.'
