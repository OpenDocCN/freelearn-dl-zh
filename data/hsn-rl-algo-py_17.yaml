- en: Assessments
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: Chapter 3
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章
- en: What's a stochastic policy?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是随机策略？
- en: It's a policy defined in terms of a probability distribution
  id: totrans-3
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一个基于概率分布定义的策略。
- en: How can a return be defined in terms of the return at the next time step?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何用下一个时间步的回报来定义回报？
- en: '![](img/a8f2ea82-7dd4-4a28-9c02-a139773735e5.png)'
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/a8f2ea82-7dd4-4a28-9c02-a139773735e5.png)'
- en: Why is the Bellman equation so important?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么贝尔曼方程如此重要？
- en: Because it provides a general formula to compute the value of a state using
    the current reward and the value of the subsequent state.
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为它提供了一个通用公式，用于计算一个状态的价值，利用当前的奖励和后续状态的价值。
- en: Which are the limiting factors of DP algorithms?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DP算法的限制因素是什么？
- en: Due to a complexity explosion with the number of states, they have to be limited.
    The other constraint is that the dynamics of the system have to be fully known.
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于状态数量的复杂性爆炸，它们必须受到限制。另一个限制是系统的动态必须完全已知。
- en: What's policy evaluation?
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是策略评估？
- en: Is an iterative method to compute the value function for a given policy using
    the Bellman equations.
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是一种使用贝尔曼方程计算给定策略的价值函数的迭代方法。
- en: How does policy iteration and value iteration differs?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略迭代和价值迭代有什么区别？
- en: Policy iteration alternate between policy evaluation and policy improvement,
    value iteration instead, combine the two in a single update using the max function.
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略迭代在策略评估和策略改进之间交替进行，而价值迭代则将二者结合在一个单一的更新中，使用最大函数。
- en: Chapter 4
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 第4章
- en: What's the main property of the MC method used in RL?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RL中使用的MC方法的主要特点是什么？
- en: The estimation of the value function as the average return from a state.
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将状态的平均回报作为价值函数的估计。
- en: Why are MC methods offline?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么蒙特卡洛（MC）方法是离线的？
- en: Because they update the state value only when the complete trajectory is available.
    Thus they have to wait until the end of the episode.
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为它们只有在完整的轨迹可用时才会更新状态值。因此，它们必须等到本轮结束。
- en: What are the two main ideas of TD learning?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD学习的两大主要思想是什么？
- en: They combine the ideas of sampling and bootstrapping
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们结合了采样和自举的思想。
- en: What are the differences between MC and TD?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MC和TD有什么不同？
- en: MC learn from the full trajectory, whereas TD learn at every step acquiring
    knowledge also from an incomplete trajectory.
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: MC从完整的轨迹中学习，而TD则在每一步都学习，也能从不完整的轨迹中获取知识。
- en: Why is exploration important in TD learning?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么探索在TD学习中如此重要？
- en: Because the TD update is done only on the action-state visited, so if some of
    them has not been discovered, in the absence of an exploration strategy they will
    never be visited. Thus, some good policy may not be discovered.
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为TD更新仅在访问的状态-动作对上进行，所以如果某些状态-动作对未被发现，在没有探索策略的情况下，它们将永远不会被访问到。因此，一些好的策略可能永远不会被发现。
- en: Why Q-learning is off-policy?
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么Q学习是脱离策略的？
- en: Because the Q-learning update is done independently of the behavior policy.
    It uses the greedy policy of the max operation.
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为Q学习的更新独立于行为策略。它使用最大操作的贪婪策略。
- en: Chapter 5
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章
- en: What arise of the deadly triad problem?
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 死亡三元组问题的产生原因是什么？
- en: When off-policy learning are combined with function approximation and boostrapping.
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当脱离策略学习与函数逼近和自举（bootstrapping）结合时。
- en: How DQN overcome the instabilities?
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN如何克服不稳定性？
- en: Using a replay buffer and a separate online and target network.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用重放缓冲区和独立的在线网络与目标网络。
- en: What's the moving target problem?
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是移动目标问题？
- en: It's a problem that arises when the target values aren't fixed and they change
    as the network is optimized.
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个问题，当目标值不是固定的，它们会随着网络的优化而变化。
- en: How the moving target problem is mitigated in DQN?
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN是如何减轻移动目标问题的？
- en: Introducing a target network that is updated less frequently than the online
    network.
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入一个目标网络，其更新频率低于在线网络。
- en: What's the optimization procedure used in DQN?
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN中使用的优化过程是什么？
- en: A mean square error loss function is optimized through stochastic gradient descent,
    an iterative method that performs gradient descent on a batch.
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过**随机梯度下降**优化均方误差损失函数，这是一种对批量进行梯度下降的迭代方法。
- en: What's the definition of a state-action advantage value function?
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 状态-动作优势值函数的定义是什么？
- en: '![](img/f4a26ee4-c9ad-415b-91a1-0edc7a8c48f8.png)'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/f4a26ee4-c9ad-415b-91a1-0edc7a8c48f8.png)'
- en: Chapter 6
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第6章
- en: How PG algorithms maximize the objective function?
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PG算法如何最大化目标函数？
- en: They do it by taking a step in the opposite direction of the objective function's
    derivative. The step is proportional to the return.
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们通过沿着目标函数的导数的相反方向迈出一步来进行。该步长与回报成正比。
- en: What's the main intuition behind PG algorithms?
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PG算法背后的主要直觉是什么？
- en: Encourage good actions and dissuade the agent from the bad ones.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鼓励好的行为，并劝阻代理执行不好的行为。
- en: Why introducing a baseline in REINFORCE it remains unbiased?
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么在REINFORCE中引入基准仍然保持无偏？
- en: Because in expectation ![](img/d61043ed-4794-4c60-8ea9-3bda3f910e9b.png)
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为在期望下 ![](img/d61043ed-4794-4c60-8ea9-3bda3f910e9b.png)
- en: To which broader class of algorithms belong to REINFORCE?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: REINFORCE属于哪个更广泛的算法类别？
- en: It is a Monte Carlo method as it relies on full trajectories like MC methods
    do.
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一种蒙特卡罗方法，因为它像MC方法一样依赖完整的轨迹。
- en: How the critic in AC methods differs from a value function used as a baseline
    in REINFORCE?
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AC方法中的评论员（critic）与REINFORCE中作为基准使用的价值函数有何不同？
- en: Besides the learned function is the same, the critic uses the approximated value
    function for bootstrap the action-state value instead in REINFORCE (but also in
    AC) it is used as a baseline to reduce the variance.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了学习的函数相同，评论员使用的是近似的价值函数来为动作-状态值进行自举，而在REINFORCE（但也在AC中），它被用作基准来减少方差。
- en: If you had to develop an algorithm for an agent that has to learn to move, would
    you prefer REINFORCE or AC?
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你需要为一个必须学会移动的代理开发算法，你会选择REINFORCE还是AC？
- en: You should first try an actor-critic algorithm as the agent has to learn a continuous
    task.
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该首先尝试一个演员-评论员算法，因为代理需要学习一个连续任务。
- en: Could you use an n-step Actor-Critic algorithm as a REINFORCE algorithm?
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用n步演员-评论员算法作为REINFORCE算法吗？
- en: Yes, you could as far as ![](img/07bc1b22-035f-494f-b830-2a334312721a.png) is
    greater than the maximum possible number of steps in the environment.
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是的，只要 ![](img/07bc1b22-035f-494f-b830-2a334312721a.png) 大于环境中可能的最大步骤数，你就可以这么做。
- en: Chapter 7
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第七章
- en: How can a policy neural network control a continuous agent?
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略神经网络如何控制一个连续的代理？
- en: One way to do it is to predict the mean and the standard deviation that describe
    a Gaussian distribution. The standard deviation could either be conditioned on
    a state (the input of the neural network) or be a standalone parameter.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种方法是预测描述高斯分布的均值和标准差。标准差可以根据状态（神经网络的输入）进行条件化，或者是一个独立的参数。
- en: What's the KL divergence?
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是KL散度？
- en: Is a measure of proximity of two probability distributions.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是两个概率分布接近程度的度量。
- en: What's the main idea behind TRPO?
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TRPO背后的主要思想是什么？
- en: To optimize the new objective function in a region near the old probability
    distribution.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在接近旧概率分布的区域内优化新的目标函数。
- en: How is the KL divergence used in TRPO?
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KL散度在TRPO中是如何使用的？
- en: It is used as a hard constraint to limit the digression between an old and a
    new policy.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它作为硬约束用于限制旧策略和新策略之间的偏差。
- en: What's the main benefit of PPO?
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PPO的主要优点是什么？
- en: It uses only a first-order optimization that increase the simplicity of the
    algorithm and has a better sample efficiency and performance.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它只使用一阶优化，这提高了算法的简洁性，并且具有更好的样本效率和性能。
- en: How does PPO achieve good sample efficiency?
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PPO是如何实现良好的样本效率的？
- en: It run minibatch updates several times exploiting better the data.
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它运行小批量更新，充分利用数据。
- en: Chapter 8
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 第八章
- en: Which is the primary limitation of Q-learning algorithms?
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习算法的主要限制是什么？
- en: Ther action space has to be discrete and small in order to compute the global
    maximum.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作空间必须是离散的且较小，以便计算全局最大值。
- en: Why are stochastic gradient algorithms sample inefficient?
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么随机梯度算法样本效率低？
- en: Because the are on-policy and need new data every time the policy changes.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为它们是策略梯度方法，需要每次策略变化时都获取新的数据。
- en: How does deterministic policy gradient overcome the maximization problem?
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定性策略梯度是如何克服最大化问题的？
- en: DPG model the policy as a deterministic function predicting only a deterministic
    action and the deterministic policy gradient theorem gives a way to compute the
    gradient used to update the policy.
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: DPG将策略建模为一个确定性函数，只预测一个确定性的动作，确定性策略梯度定理提供了一种计算梯度的方法，用于更新策略。
- en: How does DPG guarantee enough exploration?
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DPG是如何保证足够的探索的？
- en: By adding noise into the deterministic policy or by learning a different behavior
    policy.
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过向确定性策略中加入噪声，或通过学习不同的行为策略。
- en: What DDPG stands for? And what is its main contribution?
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDPG代表什么？它的主要贡献是什么？
- en: DDPG stands for Deep Deterministic Policy Gradient and is an algorithm that
    adapts the deterministic policy gradient to work with deep neural networks. They
    use new strategies to stabilize and speed up learning.
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: DDPG代表深度确定性策略梯度，是一种将确定性策略梯度适应深度神经网络的算法。它们使用新策略来稳定和加速学习。
- en: Which problems does TD3 propose to minimize?
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD3提出了哪些问题以进行最小化？
- en: Overestimation bias common in Q-learning and high variance estimates.
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习中常见的高估偏差和高方差估计。
- en: What new mechanisms does TD3 employ?
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD3采用了哪些新机制？
- en: To reduce the overestimation bias, they use a Clipped Double Q-learning while
    they address the variance problem with a delayed policy update and a smoothing
    regularization technique.
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了减少高估偏差，他们使用了剪切双Q学习（Clipped Double Q-learning），同时通过延迟策略更新和平滑正则化技术来解决方差问题。
- en: Chapter 9
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第9章
- en: Would you use a model-based or a model-free algorithm if you had only 10 games
    to train your agent to play checkers?
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你只有10局棋的时间来训练你的智能体玩跳棋，你会选择基于模型的算法还是无模型的算法？
- en: I would use a model-based algorithm. The model of checkers is known and plan
    on is a feasible task.
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我会使用基于模型的算法。跳棋的模型是已知的，并且规划是可行的任务。
- en: What are the disadvantages of model-based algorithms?
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于模型的算法有哪些缺点？
- en: Overall, they require more computational power and achieve lower asymptotical
    performance with respect to model-free algorithms.
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总的来说，它们需要更多的计算能力，并且相较于无模型算法，获得的渐近性能较低。
- en: If a model of the environment is unknown, how can it be learned?
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果环境的模型未知，如何学习该模型？
- en: Once a dataset is collected through interactions with the real environment,
    the dynamics model can be learned in a usual supervised way.
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦通过与真实环境的交互收集了数据集，动态模型可以通过常规的监督学习方式进行学习。
- en: Why data-aggregation methods are used?
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要使用数据聚合方法？
- en: Because usually the first interactions with the environment are done with a
    naive policy that doesn't explore all of it. Further interactions with a more
    defined policy are required to affine the model of the environment.
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为通常与环境的首次交互是通过一个天真的策略完成的，该策略没有探索环境的全部。后续的交互需要更明确的策略来优化环境模型。
- en: How does ME-TRPO stabilize training?
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ME-TRPO如何稳定训练过程？
- en: 'ME-TRPO employs two main features: an ensemble of models and early stopping
    techniques.'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ME-TRPO采用了两个主要特性：一组模型和早停技术。
- en: Why an ensemble of models improve policy learning?
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么一组模型可以改善策略学习？
- en: Because predictions that are done by an ensemble of models take into account
    any uncertainty of the single model.
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为由一组模型做出的预测考虑了单一模型的任何不确定性。
- en: Chapter 10
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第10章
- en: Is imitation learning considered a reinforcement learning technique?
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模仿学习是否被视为强化学习技术？
- en: No, because the underlying frameworks are different. The objective of IL isn't
    to maximize the reward as in RL.
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不，因为它们的底层框架不同。IL的目标不是像RL那样最大化奖励。
- en: Would you use imitation learning to build a unbitable agent in Go?
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你会使用模仿学习来构建一个不可战胜的围棋智能体吗？
- en: Probably not, because it requires an expert from which to learn. And if the
    agent has to be the best player in the world means that there's no worthy expert.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能不是，因为它需要一个专家来学习。如果智能体必须成为世界上最强的玩家，那么就没有值得学习的专家了。
- en: What's the full name of DAgger?
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAgger的全名是什么？
- en: Dataset aggregations
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集聚合
- en: What's the main strength of DAgger?
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAgger的主要优势是什么？
- en: It overcomes the problem of distribution mismatch by employing the expert to
    teach actively the learner to recover from errors.
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过让专家主动教导学习者从错误中恢复，克服了分布不匹配的问题。
- en: Where would you use IRL instead of IL?
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在什么情况下你会使用IRL而不是IL？
- en: In problems where the reward function is easier to learn and where there's the
    necessity to learn a policy better than that of the expert.
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在奖励函数更容易学习并且需要学习比专家更好的策略的情况下。
- en: Chapter 11
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 第11章
- en: What are two alternative algorithms to reinforcement learning for solving sequential
    decision problems?
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有哪两种强化学习的替代算法可以解决顺序决策问题？
- en: evolution strategies and genetic algorithms
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化策略和遗传算法
- en: What are the processes that give birth to new individuals in evolutionary algorithms?
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进化算法中，哪些过程会产生新的个体？
- en: The mutation that mutates the gene of a parent and crossover that combines genetic
    information from two parents.
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 突变是改变父代基因，而交叉是结合来自两个父代的遗传信息。
- en: What is the source of inspiration of evolutionary algorithms like genetic algorithms?
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启发式算法，如遗传算法，的灵感来源于什么？
- en: Evolutionary algorithms are principally inspired by biological evolution.
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化算法主要受到生物进化的启发。
- en: How does CMA-ES evolve evolution strategies?
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMA-ES如何进化进化策略？
- en: CMA-ES samples new candidate from a multivariate normal distribution with the
    covariance matrix that is adapted to the population.
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CMA-ES 从一个多元正态分布中采样新候选，协方差矩阵会根据种群进行调整。
- en: What's one advantage and one disadvantage of evolution strategies?
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化策略的一个优点和一个缺点是什么？
- en: One advantage is that they are derivative-free methods while a disadvantage
    is that of being sample inefficient.
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个优点是它们是无导数的方法，而缺点是样本效率较低。
- en: What's the trick used in the "Evolution Strategies as Scalable Alternative to
    Reinforcement Learning" paper to reduce the variance?
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在“进化策略作为可扩展的强化学习替代方案”论文中，采用了什么技巧来减少方差？
- en: They propose to use mirroring noise and generate an additional mutation with
    a perturbation with the opposite sign.
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们提出使用镜像噪声，并生成一个带有相反符号扰动的附加突变。
- en: Chapter 12
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第12章
- en: What's the exploration-exploitation dilemma?
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是探索-开发困境？
- en: Is a decision problem of whether it's better to explore in order to make better
    decisions in the future or exploit the best current option.
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个决策问题，是否最好进行探索，以便在未来做出更好的决策，还是利用当前最好的选项。
- en: What are two exploration strategies that we already used in previous RL algorithms?
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在之前的 RL 算法中使用过哪两种探索策略？
- en: '![](img/618091c9-6f25-492e-a561-fd2e229c3a57.png)-greedy and a strategy that
    introduces some additional noise into the policy.'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/618091c9-6f25-492e-a561-fd2e229c3a57.png)- 贪心策略和一个向策略中引入额外噪声的策略。'
- en: What's UCB?
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 UCB？
- en: Upper Confidence Bound is an optimistic exploration algorithm that estimates
    an upper confidence bound for each value and selects the action that maximizes
    (12.3)
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上置信界（UCB）是一种乐观的探索算法，它为每个值估计一个上置信界，并选择最大化该值的动作（12.3）。
- en: Is Montezuma's Revenge or Multi-armed bandit problem more difficult to solve?
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是 Montezuma's Revenge 还是多臂老虎机问题更难解决？
- en: Montezuma's Revenge is much more difficult than the multi-armed bandit problem
    just for the fact that the latter is stateless while the former has an astronomical
    number of possible states. Montezuma's Revenge has also more complexity intrinsic
    in the game.
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Montezuma's Revenge 比多臂老虎机问题要难得多，仅仅因为后者是无状态的，而前者有着天文数字般的可能状态。Montezuma's Revenge
    游戏本身也具有更多的内在复杂性。
- en: How ESBAS tackle the problem of online RL algorithm selection?
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ESBAS 如何解决在线 RL 算法选择的问题？
- en: By employing a meta-algorithm that learns which algorithm among a fixed portfolio
    performs better in a given circumstance.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用一个元算法，学习在特定情况下哪个固定算法组合表现更好。
- en: Chapter 13
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第13章
- en: How would you rank DQN, A2C, and ES based on their sample efficiency?
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何根据样本效率对 DQN、A2C 和 ES 进行排名？
- en: DQN is the most sample-efficiency followed by A2C and ES.
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: DQN 是最具样本效率的，其次是 A2C 和 ES。
- en: What would their rank be if rated on the training time and 100 CPUs are available?
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果按训练时间和可用的 100 个 CPU 排名，它们的排名会是多少？
- en: ES probably would be the faster to train, then A2C and DQN.
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ES 可能是训练最快的，然后是 A2C 和 DQN。
- en: Would you start debugging an RL algorithm on CartPole or  MontezumaRevenge?
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你会在 CartPole 还是 MontezumaRevenge 上开始调试一个 RL 算法？
- en: CartPole. You should start the debug of an algorithm with an easy task.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CartPole。你应该从一个简单的任务开始调试算法。
- en: Why is it better to use multiple seeds when comparing multiple deep RL algorithms?
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么在比较多个深度 RL 算法时，使用多个种子会更好？
- en: The results from a single trial can be highly volatile due to the stochasticity
    of the neural network and environment. By averaging multiple random seeds the
    results would approximate the average case.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单次试验的结果可能因为神经网络和环境的随机性而高度波动。通过对多个随机种子进行平均，结果会接近平均情况。
- en: Does the intrinsic reward help the exploration of an environment?
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内在奖励是否有助于环境的探索？
- en: Yes, this's because the intrinsic reward is a sort of exploration bonus that
    would increase the curiosity of the agent to visit novel states.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是的，这是因为内在奖励是一种探索奖励，它会增加智能体对访问新状态的好奇心。
- en: What's transfer learning?
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是迁移学习？
- en: Is the task of efficiently transfer knowledge between two environments.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务是有效地在两个环境之间转移知识。
