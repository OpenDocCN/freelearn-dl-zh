- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introduction to Large Language Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型简介
- en: Dear reader, welcome to *Building Large Language Model Applications*! In this
    book, we will explore the fascinating world of a new era of application developments,
    where **large language models** (**LLMs**) are the main protagonists.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 亲爱的读者，欢迎来到*构建大型语言模型应用*！在这本书中，我们将探索一个新时代应用开发的迷人世界，其中**大型语言模型**（**LLMs**）是主要角色。
- en: 'During the last year, we all learned the power of generative **artificial intelligence**
    (**AI**) tools such as ChatGPT, Bing Chat, Bard, and Dall-E. What impressed us
    the most was their stunning capabilities of generating human-like content based
    on user requests made in natural language. It is, in fact, their conversational
    capabilities that made them so easily consumable and, therefore, popular as soon
    as they entered the market. Thanks to this phase, we learned to acknowledge the
    power of generative AI and its core models: LLMs. However, LLMs are more than
    language generators. They can be also seen as reasoning engines that can become
    the brains of our intelligent applications.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的一年里，我们都了解了ChatGPT、Bing Chat、Bard和Dall-E等生成式**人工智能**（**AI**）工具的力量。最让我们印象深刻的是它们基于用户用自然语言提出的请求生成类似人类内容的能力。实际上，正是它们的对话能力使它们易于消费，因此一旦进入市场就变得流行。多亏了这个阶段，我们学会了承认生成式AI及其核心模型：LLM的力量。然而，LLM不仅仅是语言生成器。它们也可以被视为推理引擎，可以成为我们智能应用的头脑。
- en: In this book, we will see the theory and practice of how to build LLM-powered
    applications, addressing a variety of scenarios and showing new components and
    frameworks that are entering the domain of software development in this new era
    of AI. The book will start with *Part 1*, where we will introduce the theory behind
    LLMs, the most promising LLMs in the market right now, and the emerging frameworks
    for LLMs-powered applications. Afterward, we will move to a hands-on part where
    we will implement many applications using various LLMs, addressing different scenarios
    and real-world problems. Finally, we will conclude the book with a third part,
    covering the emerging trends in the field of LLMs, alongside the risk of AI tools
    and how to mitigate them with responsible AI practices.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将探讨如何构建由LLM驱动的应用的理论和实践，涵盖各种场景，并展示进入这个AI新时代软件开发领域的新组件和框架。本书将从*第一部分*开始，介绍LLM背后的理论，目前市场上最有前途的LLM，以及为LLM驱动的应用而兴起的新框架。之后，我们将转向实践部分，使用各种LLM实现许多应用，解决不同的场景和现实世界问题。最后，我们将以第三部分结束本书，涵盖LLM领域的最新趋势，以及AI工具的风险以及如何通过负责任的AI实践来减轻这些风险。
- en: So, let’s dive in and start with some definitions of the context we are moving
    in. This chapter provides an introduction and deep dive into LLMs, a powerful
    set of deep learning neural networks that feature the domain of generative AI.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们深入探讨，并从我们所处环境的定义开始。本章提供了对LLM的介绍和深入研究，LLM是一组强大的深度学习神经网络，它以生成AI领域为特征。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding LLMs, their differentiators from classical machine learning models,
    and their relevant jargon
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解LLM，它们与经典机器学习模型的区别，以及相关的术语
- en: Overview of the most popular LLM architectures
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最受欢迎的LLM架构概述
- en: How LLMs are trained and consumed
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的训练和消费方式
- en: Base LLMs versus fine-tuned LLMs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础型LLM与微调型LLM
- en: By the end of this chapter, you will have the fundamental knowledge of what
    LLMs are, how they work, and how you can make them more tailored to your applications.
    This will also pave the way for the concrete usage of LLMs in the hands-on part
    of this book, where we will see in practice how to embed LLMs within your applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将掌握LLM的基本知识，了解它们是如何工作的，以及如何使它们更符合你的应用需求。这将为本书实践部分中LLM的具体应用铺平道路，在那里我们将看到如何在实践中将LLM嵌入到你的应用中。
- en: What are large foundation models and LLMs?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是大型基础模型和LLM？
- en: LLMs are deep-learning-based models that use many parameters to learn from vast
    amounts of unlabeled texts. They can perform various natural language processing
    tasks such as recognizing, summarizing, translating, predicting, and generating
    text.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLM是基于深度学习的模型，使用大量参数从大量未标记的文本中学习。它们可以执行各种自然语言处理任务，如识别、总结、翻译、预测和生成文本。
- en: '**Definition**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: '**Deep learning** is a branch of machine learning that is characterized by
    neural networks with multiple layers, hence the term “deep.” These deep neural
    networks can automatically learn hierarchical data representations, with each
    layer extracting increasingly abstract features from the input data. The depth
    of these networks refers to the number of layers they possess, enabling them to
    effectively model intricate relationships and patterns in complex datasets.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**是机器学习的一个分支，其特征是具有多层神经网络的神经网络，因此得名“深度”。这些深度神经网络可以自动学习层次化的数据表示，每一层从输入数据中提取越来越抽象的特征。这些网络的深度指的是它们所拥有的层数，这使得它们能够有效地模拟复杂数据集中的复杂关系和模式。'
- en: 'LLMs belong to a wider set of models that feature the AI subfield of generative
    AI: **large foundation models** (**LFMs**). Hence, in the following sections,
    we will explore the rise and development of LFMs and LLMs, as well as their technical
    architecture, which is a crucial task to understand their functioning and properly
    adopt those technologies within your applications.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs属于一个更广泛的模型集合，这些模型具有人工智能子领域生成式人工智能的特征：**大型基础模型**（LFMs）。因此，在接下来的章节中，我们将探讨LFMs和LLMs的兴起和发展，以及它们的技术架构，这是理解它们的工作原理并在您的应用程序中正确采用这些技术的关键任务。
- en: We will start by understanding why LFMs and LLMs differ from traditional AI
    models and how they represent a paradigm shift in this field. We will then explore
    the technical functioning of LLMs, how they work, and the mechanisms behind their
    outcomes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先了解为什么低秩因子分解（LFMs）和大型语言模型（LLMs）与传统人工智能模型不同，以及它们如何代表这一领域的范式转变。然后，我们将探讨LLMs的技术运作方式，它们是如何工作的，以及它们结果背后的机制。
- en: AI paradigm shift – an introduction to foundation models
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工智能范式转变——基础模型简介
- en: A foundation model refers to a type of pre-trained generative AI model that
    offers immense versatility by being adaptable for various specific tasks. These
    models undergo extensive training on vast and diverse datasets, enabling them
    to grasp general patterns and relationships within the data – not just limited
    to textual but also covering other data formats such as images, audio, and video.
    This initial pre-training phase equips the models with a strong foundational understanding
    across different domains, laying the groundwork for further fine-tuning. This
    cross-domain capability differentiates generative AI models from standard **natural
    language understanding** (**NLU**) algorithms.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型是指一种预训练的生成式人工智能模型，通过适应各种特定任务而提供极大的灵活性。这些模型在大量且多样化的数据集上进行广泛的训练，使它们能够掌握数据中的通用模式和关系——不仅限于文本，还包括其他数据格式，如图像、音频和视频。这个初始的预训练阶段使模型在各个领域拥有强大的基础理解，为后续的微调奠定了基础。这种跨领域的功能将生成式人工智能模型与标准的**自然语言理解**（NLU）算法区分开来。
- en: '**Note**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Generative AI and NLU algorithms are both related to **natural language processing**
    (**NLP**), which is a branch of AI that deals with human language. However, they
    have different goals and applications.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能和NLU算法都与**自然语言处理**（NLP）相关，NLP是处理人类语言的AI分支。然而，它们有不同的目标和应用。
- en: The difference between generative AI and NLU algorithms is that generative AI
    aims to create new natural language content, while NLU algorithms aim to understand
    existing natural language content. Generative AI can be used for tasks such as
    text summarization, text generation, image captioning, or style transfer. NLU
    algorithms can be used for tasks such as chatbots, question answering, sentiment
    analysis, or machine translation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能（Generative AI）与自然语言理解（NLU）算法之间的区别在于，生成式人工智能旨在创建新的自然语言内容，而NLU算法旨在理解现有的自然语言内容。生成式人工智能可用于文本摘要、文本生成、图像标题或风格迁移等任务。NLU算法可用于聊天机器人、问答、情感分析或机器翻译等任务。
- en: Foundation models are designed with transfer learning in mind, meaning they
    can effectively apply the knowledge acquired during pre-training to new, related
    tasks. This transfer of knowledge enhances their adaptability, making them efficient
    at quickly mastering new tasks with relatively little additional training.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的设计考虑了迁移学习，这意味着它们可以有效地将预训练期间获得的知识应用于新的相关任务。这种知识迁移增强了它们的适应性，使它们能够以相对较少的额外训练快速掌握新任务。
- en: One notable characteristic of foundation models is their large architecture,
    containing millions or even billions of parameters. This extensive scale enables
    them to capture complex patterns and relationships within the data, contributing
    to their impressive performance across various tasks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型的一个显著特点是它们的大型架构，包含数百万甚至数十亿个参数。这种广泛的规模使它们能够捕捉数据中的复杂模式和关系，从而有助于它们在各个任务上表现出令人印象深刻的表现。
- en: Due to their comprehensive pre-training and transfer learning capabilities,
    foundation models exhibit strong generalization skills. This means they can perform
    well across a range of tasks and efficiently adapt to new, unseen data, eliminating
    the need for training separate models for individual tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们具有全面的预训练和迁移学习能力，基础模型表现出强大的泛化能力。这意味着它们可以在各种任务上表现良好，并能够高效地适应新的、未见过的数据，从而消除了为单个任务训练单独模型的需要。
- en: This paradigm shift in artificial neural network design offers considerable
    advantages, as foundation models, with their diverse training datasets, can adapt
    to different tasks based on users’ intent without compromising performance or
    efficiency. In the past, creating and training distinct neural networks for each
    task, such as named entity recognition or sentiment analysis, would have been
    necessary, but now, foundation models provide a unified and powerful solution
    for multiple applications.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络设计中的这种范式转变提供了相当多的优势，因为基础模型，凭借其多样化的训练数据集，可以根据用户的意图适应不同的任务，而不会损害性能或效率。在过去，为每个任务，如命名实体识别或情感分析，创建和训练不同的神经网络是必要的，但现在，基础模型为多个应用提供了一个统一且强大的解决方案。
- en: '![](img/B21714_01_01.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_01_01.png)'
- en: 'Figure 1.1: From task-specific models to general models'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：从特定任务模型到通用模型
- en: Now, we said that LFMs are trained on a huge amount of heterogeneous data in
    different formats. Whenever that data is unstructured, natural language data,
    we refer to the output LFM as an LLM, due to its focus on text understanding and
    generation.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们说LFMs是在不同格式的海量异构数据上训练的。每当这些数据是无结构化的自然语言数据时，我们将其输出的LFM称为LLM，因为它专注于文本理解和生成。
- en: '![A close-up of a white card  Description automatically generated](img/B21714_01_02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![一张白色卡的特写  自动生成的描述](img/B21714_01_02.png)'
- en: 'Figure 1.2: Features of LLMs'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：LLMs的特点
- en: We can then say that an LLM is a type of foundation model specifically designed
    for NLP tasks. These models, such as ChatGPT, BERT, Llama, and many others, are
    trained on vast amounts of text data and can generate human-like text, answer
    questions, perform translations, and more.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以认为LLM是一种专门为NLP任务设计的基础模型。这些模型，如ChatGPT、BERT、Llama以及许多其他模型，都是在大量文本数据上训练的，能够生成类似人类的文本，回答问题，执行翻译等。
- en: Nevertheless, LLMs aren’t limited to performing text-related tasks. As we will
    see throughout the book, those unique models can be seen as reasoning engines,
    extremely good in common sense reasoning. This means that they can assist us in
    complex tasks, analytical problem-solving, enhanced connections, and insights
    among pieces of information.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，LLMs并不仅限于执行与文本相关的任务。正如我们将在整本书中看到的那样，这些独特的模型可以被视为推理引擎，在常识推理方面非常出色。这意味着它们可以帮助我们进行复杂任务、分析问题解决、增强信息片段之间的联系和洞察力。
- en: In fact, as LLMs mimic the way our brains are made (as we will see in the next
    section), their architectures are featured by connected neurons. Now, human brains
    have about 100 trillion connections, way more than those within an LLM. Nevertheless,
    LLMs have proven to be much better at packing a lot of knowledge into those fewer
    connections than we are.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，正如我们将在下一节中看到的那样，LLMs模仿我们大脑的构成方式，它们的架构以连接的神经元为特征。现在，人脑大约有100万亿个连接，远多于LLM内部的连接。尽管如此，LLMs已被证明在将大量知识打包到那些较少的连接中方面比我们做得更好。
- en: Under the hood of an LLM
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM的内部结构
- en: 'LLMs are a particular type of **artificial neural networks** (**ANNs**): computational
    models inspired by the structure and functioning of the human brain. They have
    proven to be highly effective in solving complex problems, particularly in areas
    like pattern recognition, classification, regression, and decision-making tasks.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: LLM是一种特定类型的**人工神经网络**（**ANNs**）：受人类大脑结构和功能启发的计算模型。它们已被证明在解决复杂问题方面非常有效，尤其是在模式识别、分类、回归和决策任务等领域。
- en: The basic building block of an ANN is the artificial neuron, also known as a
    node or unit. These neurons are organized into layers, and the connections between
    neurons are weighted to represent the strength of the relationship between them.
    Those weights represent the **parameters** of the model that will be optimized
    during the training process.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ANN的基本构建块是人工神经元，也称为节点或单元。这些神经元被组织成层，神经元之间的连接被加权以表示它们之间关系的强度。这些权重代表了模型在训练过程中将要优化的**参数**。
- en: 'ANNs are, by definition, mathematical models that work with numerical data.
    Hence, when it comes to unstructured, textual data as in the context of LLMs,
    there are two fundamental activities that are required to prepare data as model
    input:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ANN按定义是处理数值数据的数学模型。因此，当涉及到LLMs上下文中的非结构化文本数据时，有两个基本活动是必需的，以准备数据作为模型输入：
- en: '**Tokenization**: This is the process of breaking down a piece of text (a sentence,
    paragraph, or document) into smaller units called tokens. These tokens can be
    words, subwords, or even characters, depending on the chosen tokenization scheme
    or algorithm. The goal of tokenization is to create a structured representation
    of the text that can be easily processed by machine learning models.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**：这是将文本片段（句子、段落或文档）分解成称为标记的更小单元的过程。这些标记可以是单词、子词，甚至字符，具体取决于选择的分词方案或算法。分词的目的是创建一个结构化的文本表示，以便机器学习模型能够轻松处理。'
- en: '![A black rectangle with a black arrow pointing to a black rectangle  Description
    automatically generated](img/B21714_01_03.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![黑色矩形，黑色箭头指向黑色矩形，自动生成描述](img/B21714_01_03.png)'
- en: 'Figure 1.3: Example of tokenization'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：分词的示例
- en: '**Embedding**: Once the text has been tokenized, each token is converted into
    a dense numerical vector called an embedding. Embeddings are a way to represent
    words, subwords, or characters in a continuous vector space. These embeddings
    are learned during the training of the language model and capture semantic relationships
    between tokens. The numerical representation allows the model to perform mathematical
    operations on the tokens and understand the context in which they appear.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入**：一旦文本被分词，每个标记就被转换成一个称为嵌入的密集数值向量。嵌入是在语言模型训练期间学习到的，它捕捉了标记之间的语义关系。这种数值表示允许模型在标记上执行数学运算，并理解它们出现的上下文。'
- en: '![A black and white diagram  Description automatically generated](img/B21714_01_04.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![黑白图，自动生成描述](img/B21714_01_04.png)'
- en: 'Figure 1.4: Example of embedding'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：嵌入的示例
- en: In summary, tokenization breaks down text into smaller units called tokens,
    and embeddings convert these tokens into dense numerical vectors. This relationship
    allows LLMs to process and understand textual data in a meaningful and context-aware
    manner, enabling them to perform a wide range of NLP tasks with impressive accuracy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，分词将文本分解成称为标记的更小单元，而嵌入将这些标记转换成密集的数值向量。这种关系使得LLMs能够以有意义和上下文感知的方式处理和理解文本数据，从而能够以令人印象深刻的准确性执行广泛的NLP任务。
- en: 'For example, let’s consider a two-dimensional embedding space where we want
    to vectorize the words Man, King, Woman, and Queen. The idea is that the mathematical
    distance between each pair of those words should be representative of their semantic
    similarity. This is illustrated by the following graph:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑一个二维嵌入空间，我们想要将单词Man、King、Woman和Queen向量化。想法是这些单词之间的数学距离应该代表它们的语义相似性。以下图表展示了这一点：
- en: '![](img/B21714_01_05.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_01_05.png)'
- en: 'Figure 1.5: Example of words embedding in a 2D space'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5：2D空间中词语嵌入的示例
- en: As a result, if we properly embed the words, the relationship **King** – **Man**
    + **Woman** ≈ **Queen** should hold.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们正确地嵌入词语，则关系**King** – **Man** + **Woman** ≈ **Queen**应该成立。
- en: 'Once we have the vectorized input, we can pass it into the multi-layered neural
    network. There are three main types of layers:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了向量化的输入，我们就可以将其传递到多层神经网络中。主要有三种类型的层：
- en: '**Input layer**: The first layer of the neural network receives the input data.
    Each neuron in this layer corresponds to a feature or attribute of the input data.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：神经网络的第一层接收输入数据。这一层的每个神经元对应于输入数据的一个特征或属性。'
- en: '**Hidden layers**: Between the input and output layers, there can be one or
    more hidden layers. These layers process the input data through a series of mathematical
    transformations and extract relevant patterns and representations from the data.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：在输入层和输出层之间，可以有一个或多个隐藏层。这些层通过一系列数学变换处理输入数据，并从数据中提取相关模式和表示。'
- en: '**Output layer**: The final layer of the neural network produces the desired
    output, which could be predictions, classifications, or other relevant results
    depending on the task the neural network is designed for.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：神经网络的最外层产生所需的输出，这可能是预测、分类或其他相关结果，具体取决于神经网络设计用于的任务。'
- en: '![](img/B21714_01_06.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_01_06.png)'
- en: 'Figure 1.6: High-level architecture of a generic ANN'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6：通用人工神经网络的总体架构
- en: The process of training an ANN involves the process of **backpropagation** by
    iteratively adjusting the weights of the connections between neurons based on
    the training data and the desired outputs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 训练人工神经网络的过程涉及根据训练数据和期望输出，通过迭代调整神经元之间连接的权重来进行的**反向传播**过程。
- en: '**Definition**'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: '**Backpropagation** is an algorithm used in deep learning to train neural networks.
    It involves two phases: the forward pass, where data is passed through the network
    to compute the output, and the backward pass, where errors are propagated backward
    to update the network’s parameters and improve its performance. This iterative
    process helps the network learn from data and make accurate predictions.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播** 是一种在深度学习中用于训练神经网络的算法。它涉及两个阶段：正向传播，其中数据通过网络传递以计算输出，以及反向传播，其中错误被反向传播以更新网络的参数并提高其性能。这个迭代过程帮助网络从数据中学习并做出准确的预测。'
- en: During backpropagation, the network learns by comparing its predictions with
    the ground truth and minimizing the error or loss between them. The objective
    of training is to find the optimal set of weights that enables the neural network
    to make accurate predictions on new, unseen data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中，网络通过比较其预测与真实值以及最小化它们之间的错误或损失来学习。训练的目的是找到一组最优的权重，使神经网络能够对新、未见过的数据做出准确的预测。
- en: ANNs can vary in architecture, including the number of layers, the number of
    neurons in each layer, and the connections between them.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络的架构可以有所不同，包括层数、每层的神经元数量以及它们之间的连接。
- en: When it comes to generative AI and LLMs, their remarkable capability of generating
    text based on our prompts is based on the statistical concept of Bayes’ theorem.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到生成式 AI 和大型语言模型时，它们根据我们的提示生成文本的非凡能力基于贝叶斯定理的统计概念。
- en: '**Definition**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: 'Bayes’ theorem, named after the Reverend Thomas Bayes, is a fundamental concept
    in probability theory and statistics. It describes how to update the probability
    of a hypothesis based on new evidence. Bayes’ theorem is particularly useful when
    we want to make inferences about unknown parameters or events in the presence
    of uncertainty. According to Bayes’ theorem, given two events, A and B, we can
    define the conditional probability of A given B as:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理，以托马斯·贝叶斯牧师的名字命名，是概率论和统计学中的一个基本概念。它描述了如何根据新证据更新假设的概率。贝叶斯定理在我们想要在不确定性存在的情况下对未知参数或事件进行推理时特别有用。根据贝叶斯定理，给定两个事件
    A 和 B，我们可以定义在给定 B 的条件下 A 的条件概率：
- en: '![](img/B21714_01_001.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21714_01_001.png)'
- en: 'Where:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*P*(*B*|*A*) = probability of *B* occurring given *A*, also known as the likelihood
    of *A* given a fixed *B*.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*B*|*A*) = 在给定 *A* 的情况下发生 *B* 的概率，也称为在固定的 *B* 下 *A* 的似然性。'
- en: '*P*(*A*|*B*) = probability of *A* occurring, given *B*; also known as the posterior
    probability of *A*, given *B*.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*A*|*B*) = 在给定 *B* 的情况下发生 *A* 的概率；也称为在给定 *B* 的 *A* 的后验概率。'
- en: '*P*(*A*) and *P*(*B*) = probability of observing *A* or *B* without any conditions.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*A*) 和 *P*(*B*) = 在没有任何条件的情况下观察 *A* 或 *B* 的概率。'
- en: Bayes’ theorem relates the conditional probability of an event based on new
    evidence with the a priori probability of the event. Translated into the context
    of LLMs, we are saying that such a model functions by predicting the next most
    likely word, given the previous words prompted by the user.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理将基于新证据的事件的条件概率与事件的事先概率相关联。在大型语言模型的背景下，我们说的是这样的模型通过预测用户提示的前一个单词后最有可能的下一个单词来工作。
- en: But how can LLMs know which is the next most likely word? Well, thanks to the
    enormous amount of data on which LLMs have been trained (we will dive deeper into
    the process of training an LLM in the next sections). Based on the training text
    corpus, the model will be able to identify, given a user’s prompt, the next most
    likely word or, more generally, text completion.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 但LLM如何知道下一个最有可能的词是什么呢？好吧，多亏了LLM在训练过程中所训练的巨大数据量（我们将在下一节中深入探讨LLM的训练过程）。基于训练文本语料库，模型将能够根据用户的提示识别出下一个最有可能的词，或者更普遍地说，是文本补全。
- en: 'For example, let’s consider the following prompt: “*The cat is on the….*” and
    we want our LLM to complete this sentence. However, the LLM may generate multiple
    candidate words, so we need a method to evaluate which of the candidates is the
    most likely one. To do so, we can use Bayes’ theorem to select the most likely
    word given the context. Let’s see the required steps:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑以下提示：“*猫在……上.*”并且我们希望我们的LLM来完成这个句子。然而，LLM可能会生成多个候选词，因此我们需要一种方法来评估哪个候选词最有可能。为此，我们可以使用贝叶斯定理来选择给定上下文中最可能的词。让我们看看所需的步骤：
- en: '**Prior probability** **P**(**A**): The prior probability represents the probability
    of each candidate word being the next word in the context, based on the language
    model’s knowledge learned during training. Let’s assume the LLM has three candidate
    words: “table,” “chair,” and “roof.”'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**先验概率** **P**(**A**): 先验概率表示每个候选词在上下文中作为下一个词的概率，这是基于语言模型在训练期间学习到的知识。假设LLM有三个候选词：“table”，“chair”，和“roof”。'
- en: P(“table”), P(“chain”), and P(“roof”) are the prior probabilities for each candidate
    word, based on the language model’s knowledge of the frequency of these words
    in the training data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: P(“table”), P(“chain”), 和 P(“roof”) 是每个候选词的先验概率，这些概率基于语言模型对训练数据中这些词频率的了解。
- en: '**Likelihood** (**P**(**B**|**A**)): The likelihood represents how well each
    candidate word fits the context “The cat is on the....” This is the probability
    of observing the context given each candidate word. The LLM calculates this based
    on the training data and how often each word appears in similar contexts.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**似然** (**P**(**B**|**A**)): 似然表示每个候选词与上下文“猫在……上”的匹配程度。这是在给定每个候选词的情况下观察上下文的概率。LLM根据训练数据和每个词在类似上下文中的出现频率来计算这个概率。'
- en: For example, if the LLM has seen many instances of “The cat is on the table,”
    it would assign a high likelihood to “table” as the next word in the given context.
    Similarly, if it has seen many instances of “The cat is on the chair,” it would
    assign a high likelihood to “chair” as the next word.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果LLM看到许多“猫在桌子上”的实例，它会给“table”作为给定上下文中的下一个词赋予高似然。同样，如果它看到许多“猫在椅子上”的实例，它会给“chair”作为下一个词赋予高似然。
- en: P(“The cat is on the table”), P(“The cat is on the chair”), and P(“The cat is
    on the roof”) are the likelihoods for each candidate word given the context.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: P(“The cat is on the table”), P(“The cat is on the chair”), 和 P(“The cat is
    on the roof”) 是给定上下文中每个候选词的似然。
- en: '**Posterior probability** (**P**(**A**|**B**)): Using Bayes’ theorem, we can
    calculate the posterior probability for each candidate word based on the prior
    probability and the likelihood:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后验概率** (**P**(**A**|**B**)): 使用贝叶斯定理，我们可以根据先验概率和似然计算每个候选词的后验概率：'
- en: '![](img/B21714_01_002.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_01_002.png)'
- en: '![](img/B21714_01_003.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_01_003.png)'
- en: '![](img/B21714_01_004.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_01_004.png)'
- en: '**Selecting the most likely word**. After calculating the posterior probabilities
    for each candidate word, we choose the word with the highest posterior probability
    as the most likely next word to complete the sentence.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择最有可能的词**。在计算每个候选词的后验概率后，我们选择后验概率最高的词作为最有可能的下一个词来完成句子。'
- en: The LLM uses Bayes’ theorem and the probabilities learned during training to
    generate text that is contextually relevant and meaningful, capturing patterns
    and associations from the training data to complete sentences in a coherent manner.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: LLM使用贝叶斯定理和训练期间学习的概率来生成与上下文相关且有意义的内容，通过从训练数据中捕获模式和关联，以连贯的方式完成句子。
- en: 'The following figure illustrates how it translates into the architectural framework
    of a neural network:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示说明了它如何转化为神经网络架构框架：
- en: '![](img/B21714_01_07.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_01_07.png)'
- en: 'Figure 1.7: Predicting the next most likely word in an LLM'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7：预测LLM中的下一个最有可能的词
- en: '**Note**'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: 'The last layer of the ANN is typically a non-linear activation function. In
    the above illustration, the function is Softmax, a mathematical function that
    converts a vector of real numbers into a probability distribution. It is often
    used in machine learning to normalize the output of a neural network or a classifier.
    The Softmax function is defined as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ANN的最后一层通常是一个非线性激活函数。在上面的图中，该函数是Softmax，这是一个将实数向量转换为概率分布的数学函数。它在机器学习中常用于归一化神经网络或分类器的输出。Softmax函数的定义如下：
- en: '![](img/B21714_01_005.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_01_005.png)'
- en: where z[i] is the *i*-th element of the input vector, and K is the number of
    elements in the vector. The Softmax function ensures that each element of the
    output vector is between 0 and 1 and that the sum of all elements is 1\. This
    makes the output vector suitable for representing probabilities of different classes
    or outcomes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，z[i]是输入向量的第*i*个元素，K是向量中的元素数量。Softmax函数确保输出向量的每个元素都在0到1之间，并且所有元素的总和为1。这使得输出向量适合表示不同类别或结果的概率。
- en: 'Overall, ANNs are the core pillars of the development of generative AI models:
    thanks to their mechanisms of tokenization, embedding, and multiple hidden layers,
    they can capture complex patterns even in the most unstructured data, such as
    natural language.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，ANNs是生成式AI模型发展的核心支柱：得益于它们的标记化、嵌入和多层隐藏机制，它们能够在最无结构的数据中捕捉复杂的模式，例如自然语言。
- en: However, what we are observing today is a set of models that demonstrates incredible
    capabilities that have never been seen before, and this is due to a particular
    ANNs’ architectural framework, introduced in recent years and the main protagonist
    of LLM development. This framework is called the transformer, and we are going
    to cover it in the following section.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们今天所观察到的，是一系列展现出前所未有的惊人能力的模型，这要归功于近年来引入的特定的人工神经网络（ANNs）架构框架，它是LLM发展的主要推动者。这个框架被称为“转换器”，我们将在下一节中对其进行介绍。
- en: Most popular LLM transformers-based architectures
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最流行的基于LLM的转换器架构
- en: ANNs, as we saw in the preceding sections, are at the heart of LLMs. Nevertheless,
    in order to be *generative*, those ANNs need to be endowed with some peculiar
    capabilities, such as parallel processing of textual sentences or keeping the
    memory of the previous context.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中看到的，ANNs是LLMs的核心。然而，为了成为*生成式*的，这些ANNs需要具备一些特殊的能力，例如并行处理文本句子或保持先前上下文的记忆。
- en: These particular capabilities were at the core of generative AI research in
    the last decades, starting from the 80s and 90s. However, it is only in recent
    years that the main drawbacks of these early models – such as the capability of
    text parallel processing or memory management – have been bypassed by modern generative
    AI frameworks. Those frameworks are the so-called **transformers**.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特殊能力是过去几十年生成式AI研究的核心，从20世纪80年代和90年代开始。然而，直到最近几年，这些早期模型的主要缺点——如文本并行处理能力或内存管理——才被现代生成式AI框架所克服。这些框架就是所谓的**转换器**。
- en: In the following sections, we will explore the evolution of generative AI model
    architecture, from early developments to state-of-the-art transformers. We will
    start by covering the first generative AI models that paved the way for further
    research, highlighting their limitations and the approaches to overcome them.
    We will then explore the introduction of transformer-based architectures, covering
    their main components and explaining why they represent the state of the art for
    LLMs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将探讨生成式AI模型架构的演变，从早期的发展到最先进的转换器。我们将从介绍为后续研究铺平道路的第一批生成式AI模型开始，突出它们的局限性以及克服这些局限性的方法。然后，我们将探讨基于转换器的架构的引入，涵盖它们的主要组件，并解释为什么它们代表了LLMs的当前最佳水平。
- en: Early experiments
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 早期实验
- en: 'The very first popular generative AI ANN architectures trace back to the 80s
    and 90s, including:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个流行的生成式AI ANN架构可以追溯到20世纪80年代和90年代，包括：
- en: '**Recurrent neural networks** (**RNNs**): RNNs are a type of ANN designed to
    handle sequential data. They have recurrent connections that allow information
    to persist across time steps, making them suitable for tasks like language modeling,
    machine translation, and text generation. However, RNNs have limitations in capturing
    long-range dependencies due to the vanishing or exploding gradient problem.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）：RNNs 是一种用于处理序列数据的 ANN 类型。它们具有循环连接，允许信息在时间步之间持续存在，这使得它们适合语言建模、机器翻译和文本生成等任务。然而，由于梯度消失或爆炸问题，RNNs
    在捕捉长距离依赖关系方面存在局限性。'
- en: '**Definition**'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**定义**'
- en: In ANNs, the gradient is a measure of how much the model’s performance would
    improve if we slightly adjusted its internal parameters (weights). During training,
    RNNs try to minimize the difference between their predictions and the actual targets
    by adjusting their weights based on the gradient of the loss function. The problem
    of vanishing or exploding gradient arises in RNNs during training when the gradients
    become extremely small or large, respectively. The vanishing gradient problem
    occurs when the gradient becomes extremely small during training. As a result,
    the RNN learns very slowly and struggles to capture long-term patterns in the
    data. Conversely, the exploding gradient problem happens when the gradient becomes
    extremely large. This leads to unstable training and prevents the RNN from converging
    to a good solution.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 ANNs 中，梯度是衡量如果我们稍微调整模型的内部参数（权重），模型性能将提高多少的度量。在训练过程中，RNNs 通过根据损失函数的梯度调整其权重来最小化其预测与实际目标之间的差异。在训练过程中，当梯度变得极其小或大时，RNNs
    会出现梯度消失或爆炸的问题。梯度消失问题发生在训练过程中梯度变得极其小的时候。结果，RNN 学习非常缓慢，难以捕捉数据中的长期模式。相反，梯度爆炸问题发生在梯度变得极其大的时候。这会导致训练不稳定，并阻止
    RNN 收敛到良好的解决方案。
- en: '**Long short-term memory** (**LSTM**): LSTMs are a variant of RNNs that address
    the vanishing gradient problem. They introduce gating mechanisms that enable better
    preservation of important information across longer sequences. LSTMs became popular
    for various sequential tasks, including text generation, speech recognition, and
    sentiment analysis.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）：LSTMs 是一种解决梯度消失问题的 RNN 变体。它们引入了门控机制，使得在更长的序列中更好地保留重要信息。LSTMs
    因其在文本生成、语音识别和情感分析等各种序列任务中的流行而变得流行。'
- en: These architectures were popular and effective for various generative tasks,
    but they had limitations in handling long-range dependencies, scalability, and
    overall efficiency, especially when dealing with large-scale NLP tasks that would
    need massive parallel processing. The transformer framework was introduced to
    overcome these limitations. In the next section, we are going to see how a transformers-based
    architecture overcomes the above limitations and is at the core of modern generative
    AI LLMs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构在处理各种生成任务时都很受欢迎且有效，但在处理长距离依赖、可扩展性和整体效率方面存在局限性，尤其是在处理需要大量并行处理的大规模自然语言处理任务时。Transformer
    框架的引入是为了克服这些局限性。在下一节中，我们将看到基于 Transformer 的架构如何克服上述局限性，并成为现代生成式 AI LLMs 的核心。
- en: Introducing the transformer architecture
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 Transformer 架构
- en: The transformer architecture is a deep learning model introduced in the paper
    “Attention Is All You Need” by Vaswani et al. (2017). It revolutionized NLP and
    other sequence-to-sequence tasks.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构是由 Vaswani 等人在 2017 年发表的论文“Attention Is All You Need”中引入的深度学习模型。它彻底改变了自然语言处理和其他序列到序列任务。
- en: The transformer dispenses with recurrence and convolutions entirely and relies
    solely on **attention mechanisms** to encode and decode sequences.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 完全摒弃了循环和卷积，完全依赖于**注意力机制**来编码和解码序列。
- en: '**Definition**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: In the transformer architecture, “attention” is a mechanism that enables the
    model to focus on relevant parts of the input sequence while generating the output.
    It calculates attention scores between input and output positions, applies Softmax
    to get weights, and takes a weighted sum of the input sequence to obtain context
    vectors. Attention is crucial for capturing long-range dependencies and relationships
    between words in the data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformer 架构中，“注意力”是一种机制，它使模型能够在生成输出时关注输入序列的相关部分。它计算输入和输出位置之间的注意力分数，应用 Softmax
    获取权重，并对输入序列进行加权求和以获得上下文向量。注意力对于捕捉数据中单词之间的长距离依赖关系和关系至关重要。
- en: 'Since transformers use attention on the same sequence that is currently being
    encoded, we refer to it as **self-attention**. Self-attention layers are responsible
    for determining the importance of each input token in generating the output. Those
    answer the question: “*Which part of the input should I focus on?*”'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于transformer在当前正在编码的相同序列上使用注意力，我们将其称为**自注意力**。自注意力层负责确定每个输入标记在生成输出中的重要性。它们回答了这样的问题：“*我应该关注输入的哪个部分？*”
- en: 'In order to obtain the self-attention vector for a sentence, the elements we
    need are “value”, “query”, and “key.” These matrices are used to calculate attention
    scores between the elements in the input sequence and are the three weight matrices
    that are learned during the training process (typically initialized with random
    values). More specifically, their purpose is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得句子的自注意力向量，我们需要的是“值”、“查询”和“键”。这些矩阵用于计算输入序列中元素之间的注意力分数，并在训练过程中学习（通常用随机值初始化）。更具体地说，它们的作用如下：
- en: Query (*Q*) is used to represent the current focus of the attention mechanism
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询（*Q*）用于表示注意力机制的当前焦点
- en: Key (*K*) is used to determine which parts of the input should be given attention
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键（K）**用于确定输入的哪些部分应该给予注意力'
- en: Value (*V*) is used to compute the context vectors
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值（V）**用于计算上下文向量'
- en: 'They can be represented as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可以表示如下：
- en: '![](img/B21714_01_08.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_01_08.png)'
- en: 'Figure 1.8: Decomposition of the Input matrix into Q, K, and V vectors'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8：输入矩阵分解为Q、K和V向量
- en: Those matrices are then multiplied and passed through a non-linear transformation
    (thanks to a Softmax function). The output of the self-attention layer represents
    the input values in a transformed, context-aware manner, which allows the transformer
    to attend to different parts of the input depending on the task at hand.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这些矩阵随后相乘并通过非线性变换（归功于Softmax函数）。自注意力层的输出以转换后的、上下文感知的方式表示输入值，这使得transformer能够根据任务关注输入的不同部分。
- en: '![Chart, box and whisker chart  Description automatically generated](img/B21714_01_09.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图表，箱线图  描述自动生成](img/B21714_01_09.png)'
- en: 'Figure 1.9: Representation of Q, K, and V matrices multiplication to obtain
    the context vector'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9：Q、K和V矩阵乘积以获得上下文向量
- en: 'The mathematical formula is the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 数学公式如下：
- en: '![](img/B21714_01_006.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B21714_01_006.png)'
- en: 'From an architectural point of view, the transformer consists of two main components,
    an encoder and a decoder:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构角度来看，transformer由两个主要组件组成，一个编码器和一个解码器：
- en: The **encoder** takes the input sequence and produces a sequence of hidden states,
    each of which is a weighted sum of all the input embeddings.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**接收输入序列并生成一系列隐藏状态，每个隐藏状态都是所有输入嵌入的加权求和。'
- en: The **decoder** takes the output sequence (shifted right by one position) and
    produces a sequence of predictions, each of which is a weighted sum of all the
    encoder’s hidden states and the previous decoder’s hidden states.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**接收输出序列（向右移动一个位置）并生成一系列预测，每个预测都是所有编码器隐藏状态和前一个解码器隐藏状态的加权求和。'
- en: '**Note**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: The reason for shifting the output sequence right by one position in the decoder
    layer is to prevent the model from seeing the current token when predicting the
    next token. This is because the model is trained to generate the output sequence
    given the input sequence, and the output sequence should not depend on itself.
    By shifting the output sequence right, the model only sees the previous tokens
    as input and learns to predict the next token based on the input sequence and
    the previous output tokens. This way, the model can learn to generate coherent
    and meaningful sentences without cheating.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在解码器层中将输出序列向右移动一个位置的原因是为了防止模型在预测下一个标记时看到当前的标记。这是因为模型被训练为根据输入序列生成输出序列，输出序列不应依赖于自身。通过将输出序列向右移动，模型只能看到前一个标记作为输入，并学习根据输入序列和前一个输出标记来预测下一个标记。这样，模型可以学习生成连贯且有意义的话语，而不会作弊。
- en: 'The following illustration from the original paper shows the transformer architecture:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的插图来自原始论文，展示了transformer架构：
- en: '![Diagram  Description automatically generated](img/B21714_01_10.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图  描述自动生成](img/B21714_01_10.png)'
- en: 'Figure 1.10: Simplified transformer architecture'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10：简化的transformer架构
- en: 'Let’s examine each building block, starting from the encoding part:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一检查每个构建块，从编码部分开始：
- en: '**Input embedding**: These are the vector representations of tokenized input
    text.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入嵌入**：这些是分词输入文本的向量表示。'
- en: '**Positional encoding**: As the transformer does not have an inherent sense
    of word order (unlike RNNs with their sequential nature), positional encodings
    are added to the input embeddings. These encodings provide information about the
    positions of words in the input sequence, allowing the model to understand the
    order of tokens.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位置编码**：由于变换器没有固有的词序感（与具有序列性质的RNN不同），因此将位置编码添加到输入嵌入中。这些编码提供了关于输入序列中单词位置的信息，使模型能够理解标记的顺序。'
- en: '**Multi-head attention layer**: This is a mechanism in which multiple self-attention
    mechanisms operate in parallel on different parts of the input data, producing
    multiple representations. This allows the transformer model to attend to different
    parts of the input data in parallel and aggregate information from multiple perspectives.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多头注意力层**：这是一种机制，其中多个自注意力机制并行地在输入数据的不同部分上操作，产生多个表示。这使得变换器模型能够并行地关注输入数据的不同部分，并从多个角度汇总信息。'
- en: '**Add and norm layer**: This combines element-wise addition and layer normalization.
    It adds the output of a layer to the original input and then applies layer normalization
    to stabilize and accelerate training. This technique helps mitigate gradient-related
    issues and improves the model’s performance on sequential data.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加和归一化层**：这结合了元素级加法和层归一化。它将层的输出添加到原始输入中，然后应用层归一化以稳定和加速训练。这项技术有助于减轻梯度相关的问题，并提高模型在序列数据上的性能。'
- en: '**Feed-forward layer**: This is responsible for transforming the normalized
    output of attention layers into a suitable representation for the final output,
    using a non-linear activation function, such as the previously mentioned Softmax.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈层**：这负责使用非线性激活函数（如之前提到的Softmax）将注意力层的归一化输出转换为最终输出的合适表示。'
- en: 'The decoding part of the transformer starts with a similar process as the encoding
    part, where the target sequence (output sequence) undergoes input embedding and
    positional encoding. Let’s understand these blocks:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器的解码部分从与编码部分类似的过程开始，其中目标序列（输出序列）经历输入嵌入和位置编码。让我们了解这些块：
- en: '**Output embedding** (**shifted right**): For the decoder, the target sequence
    is “shifted right” by one position. This means that at each position, the model
    tries to predict the token that comes after the analyzed token in the original
    target sequence. This is achieved by removing the last token from the target sequence
    and padding it with a special start-of-sequence token (start symbol). This way,
    the decoder learns to generate the correct token based on the preceding context
    during **autoregressive decoding**.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出嵌入**（**右移**）：对于解码器，目标序列被“右移”一个位置。这意味着在每个位置，模型试图预测原始目标序列中分析标记之后的标记。这是通过从目标序列中移除最后一个标记并用特殊的序列开始标记（开始符号）填充来实现的。这样，解码器通过在**自回归解码**期间根据先前的上下文生成正确的标记来学习。'
- en: '**Definition**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: Autoregressive decoding is a technique for generating output sequences from
    a model that predicts each output token based on the previous output tokens. It
    is often used in NLP tasks such as machine translation, text summarization, and
    text generation.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归解码是一种从预测每个输出标记的模型生成输出序列的技术，该模型基于先前输出的标记。它常用于自然语言处理任务，如机器翻译、文本摘要和文本生成。
- en: Autoregressive decoding works by feeding the model an initial token, such as
    a start-of-sequence symbol, and then using the model’s prediction as the next
    input token. This process is repeated until the model generates an end-of-sequence
    symbol or reaches a maximum length. The output sequence is then the concatenation
    of all the predicted tokens.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归解码通过向模型提供一个初始标记（如序列开始符号）并使用模型的预测作为下一个输入标记来工作。这个过程重复进行，直到模型生成一个序列结束符号或达到最大长度。输出序列然后是所有预测标记的连接。
- en: '**Decoder layers**: Similarly to the encoder block, here, we also have Positional
    Encoding, Multi-Head Attention, Add and Norm, and Feed Forward layers, whose role
    is the same as for the encoding part.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器层**：与编码器块类似，这里我们也有位置编码、多头注意力、添加和归一化以及前馈层，它们的作用与编码部分相同。'
- en: '**Linear and Softmax**: These layers apply, respectively, a linear and non-linear
    transformation to the output vector. The non-linear transformation (Softmax) conveys
    the output vector into a probability distribution, corresponding to a set of candidate
    words. The word corresponding to the greatest element of the probability vector
    will be the output of the whole process.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性与Softmax**：这些层分别对输出向量应用线性和非线性变换。非线性变换（Softmax）将输出向量转换为概率分布，对应于一组候选词。概率向量中最大元素对应的词将是整个过程的输出。'
- en: The transformer architecture paved the way for modern LLMs, and it also saw
    many variations with respect to its original framework.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器架构为现代LLM铺平了道路，并且在其原始框架的基础上也出现了许多变化。
- en: Some models use only the encoder part, such as **BERT** (**Bidirectional Encoder
    Representations from Transformers**), which is designed for NLU tasks such as
    text classification, question answering, and sentiment analysis.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型仅使用编码器部分，例如**BERT**（**双向编码器表示来自转换器**），它被设计用于NLU（自然语言理解）任务，如文本分类、问答和情感分析。
- en: Other models use only the decoder part, such as **GPT-3** (**Generative Pre-trained
    Transformer 3**), which is designed for natural language generation tasks such
    as text completion, summarization, and dialogue.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 其他模型仅使用解码器部分，例如**GPT-3**（**生成预训练转换器3**），它被设计用于自然语言生成任务，如文本补全、摘要和对话。
- en: Finally, there are models that use both the encoder and the decoder parts, such
    as **T5** (**Text-to-Text Transfer Transformer**), which is designed for various
    NLP tasks that can be framed as text-to-text transformations, such as translation,
    paraphrasing, and text simplification.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有一些模型同时使用编码器和解码器部分，例如**T5**（**文本到文本转换转换器**），它被设计用于各种可以表述为文本到文本转换的NLP（自然语言处理）任务，如翻译、改写和文本简化。
- en: Regardless of the variant, the core component of a transformer – the attention
    mechanism – remains a constant within LLM architecture, and it also represents
    the reason why those frameworks gained so much popularity within the context of
    generative AI and NLP.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 不论是哪种变体，转换器的核心组件——注意力机制——在LLM架构中保持不变，这也代表了为什么这些框架在生成AI和NLP的背景下获得了如此多的关注。
- en: However, the architectural variant of an LLM is not the only element that features
    the functioning of that model. This functioning is indeed characterized also by
    *what the model knows*, depending on its training dataset, and *how well it applies
    its knowledge upon the user’s request*, depending on its evaluation metrics.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLM的架构变体并不是唯一影响该模型功能特性的元素。实际上，该模型的功能特性也由其训练数据集决定的“模型知道什么”以及根据其评估指标“模型如何根据用户请求应用其知识”来表征。
- en: In the next section, we are going to cover both the processes of training and
    evaluating LLMs, also providing those metrics needed to differentiate among different
    LLMs and understand which one to use for specific use cases within your applications.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍训练和评估LLM的过程，并提供区分不同LLM以及了解在您的应用程序中针对特定用例使用哪个LLM所需的指标。
- en: Training and evaluating LLMs
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和评估LLM
- en: 'In the preceding sections, we saw how choosing an LLM architecture is a pivotal
    step in determining its functioning. However, the quality and diversity of the
    output text depend largely on two factors: the training dataset and the evaluation
    metric.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了选择LLM架构是确定其功能的关键步骤。然而，输出文本的质量和多样性在很大程度上取决于两个因素：训练数据集和评估指标。
- en: The training dataset determines what kind of data the LLM learns from and how
    well it can generalize to new domains and languages. The evaluation metric measures
    how well the LLM performs on specific tasks and benchmarks, and how it compares
    to other models and human writers. Therefore, choosing an appropriate training
    dataset and evaluation metric is crucial for developing and assessing LLMs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集决定了LLM（大型语言模型）学习的数据类型以及它如何能够泛化到新的领域和语言。评估指标衡量LLM在特定任务和基准测试上的表现，以及它与其他模型和人类作者的比较。因此，选择合适的训练数据集和评估指标对于开发和评估LLM至关重要。
- en: In this section, we will discuss some of the challenges and trade-offs involved
    in selecting and using different training datasets and evaluation metrics for
    LLMs, as well as some of the recent developments and future directions in this
    area.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论在选择和使用不同训练数据集和评估指标时涉及的一些挑战和权衡，以及该领域的一些最新发展和未来方向。
- en: Training an LLM
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练LLM
- en: 'By definition, LLMs are *huge*, from a double point of view:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，LLM从两个方面来看都是**巨大**的：
- en: '**Number of parameters**: This is a measure of the complexity of the LLM architecture
    and represents the number of connections among neurons. Complex architectures
    have thousands of layers, each one having multiple neurons, meaning that among
    layers, we will have several connections with associated parameters (or weights).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数数量**：这是衡量LLM架构复杂度的指标，代表了神经元之间的连接数量。复杂的架构有数千层，每一层都有多个神经元，这意味着在层之间，我们将有多个带有相关参数（或权重）的连接。'
- en: '**Training set**: This refers to the unlabeled text corpus on which the LLM
    learns and trains its parameters. To give an idea of how big such a text corpus
    for an LLM can be, let’s consider OpenAI’s GPT-3 training set:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：这指的是LLM学习和训练其参数的无标签文本语料库。为了说明LLM的文本语料库可能有多大，让我们考虑OpenAI的GPT-3训练集：'
- en: '![Table  Description automatically generated](img/B21714_01_11.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B21714_01_11.png)'
- en: 'Figure 1.11: GPT-3 knowledge base'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.11：GPT-3知识库
- en: 'Considering the assumption:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下假设：
- en: 1 token ~= 4 characters in English
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1个token约等于4个英语字符
- en: 1 token ~= ¾ words
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1个token约等于3/4个单词
- en: We can conclude that GPT-3 has been trained on around **374 billion words**.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以得出结论，GPT-3大约训练了**3740亿个单词**。
- en: So generally speaking, LLMs are trained using unsupervised learning on massive
    datasets, which often consist of billions of sentences collected from diverse
    sources on the internet. The transformer architecture, with its self-attention
    mechanism, allows the model to efficiently process long sequences of text and
    capture intricate dependencies between words. Training such models necessitates
    vast computational resources, typically employing distributed systems with multiple
    **graphics processing units** (**GPUs**) or **tensor processing units** (**TPUs**).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 所以一般来说，LLM是通过在大量数据集上使用无监督学习进行训练的，这些数据集通常包含从互联网上不同来源收集的数十亿个句子。Transformer架构，凭借其自注意力机制，使得模型能够高效地处理长序列的文本并捕捉词语之间的复杂依赖关系。训练此类模型需要大量的计算资源，通常采用具有多个**图形处理单元（GPU**）或**张量处理单元（TPU**）的分布式系统。
- en: '**Definition**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: A tensor is a multi-dimensional array used in mathematics and computer science.
    It holds numerical data and is fundamental in fields like machine learning.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 张量是多维数组，在数学和计算机科学中使用。它包含数值数据，是机器学习等领域的基本元素。
- en: A TPU is a specialized hardware accelerator created by Google for deep learning
    tasks. TPUs are optimized for tensor operations, making them highly efficient
    for training and running neural networks. They offer fast processing while consuming
    less power, enabling faster model training and inference in data centers.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: TPU是谷歌为深度学习任务创建的专用硬件加速器。TPU针对张量操作进行了优化，使得它们在训练和运行神经网络时非常高效。它们提供快速处理的同时消耗更少的电力，使得数据中心中的模型训练和推理更快。
- en: The training process involves numerous iterations over the dataset, fine-tuning
    the model’s parameters using optimization algorithms backpropagation. Through
    this process, transformer-based language models acquire a deep understanding of
    language patterns, semantics, and context, enabling them to excel in a wide range
    of NLP tasks, from text generation to sentiment analysis and machine translation.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程涉及对数据集的多次迭代，使用优化算法和反向传播微调模型的参数。通过这个过程，基于Transformer的语言模型获得了对语言模式、语义和上下文的深入理解，使它们能够在从文本生成到情感分析和机器翻译的广泛NLP任务中表现出色。
- en: 'The following are the main steps involved in the training process of an LLM:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在LLM训练过程中的主要步骤：
- en: '**Data collection**: This is the process of gathering a large amount of text
    data from various sources, such as the open web, books, news articles, social
    media, etc. The data should be diverse, high-quality, and representative of the
    natural language that the LLM will encounter.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据收集**：这是一个从各种来源收集大量文本数据的过程，例如公开网络、书籍、新闻文章、社交媒体等。数据应该是多样化的、高质量的，并且能够代表LLM将遇到的自然语言。'
- en: '**Data preprocessing**: This is the process of cleaning, filtering, and formatting
    the data for training. This may include removing duplicates, noise, or sensitive
    information, splitting the data into sentences or paragraphs, tokenizing the text
    into subwords or characters, etc.'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据预处理**：这是清理、过滤和格式化数据以进行训练的过程。这可能包括删除重复项、噪声或敏感信息，将数据分割成句子或段落，将文本标记为子词或字符等。'
- en: '**Model architecture**: This is the process of designing the structure and
    parameters of the LLM. This may include choosing the type of neural network (such
    as transformer) and its structure (such as decoder only, encoder only, or encoder-decoder),
    the number and size of layers, the attention mechanism, the activation function,
    etc.'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型架构**：这是设计LLM（大型语言模型）的结构和参数的过程。这可能包括选择神经网络的类型（例如transformer）及其结构（例如仅解码器、仅编码器或编码器-解码器），层数和大小，注意力机制，激活函数等。'
- en: '**Model initialization**: This is the process of assigning initial values to
    the weights and biases of the LLM. This may be done randomly or by using pre-trained
    weights from another model.'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型初始化**：这是为LLM的权重和偏差分配初始值的过程。这可能通过随机分配或使用来自另一个模型的预训练权重来完成。'
- en: '**Model pre-training**: This is the process of updating the weights and biases
    of the LLM by feeding it batches of data and computing the loss function. The
    loss function measures how well the LLM predicts the next token given the previous
    tokens. The LLM tries to minimize the loss by using an **optimization algorithm**
    (such as gradient descent) that adjusts the weights and biases in the direction
    that reduces the loss with the backpropagation mechanism. The model training may
    take several epochs (iterations over the entire dataset) until it converges to
    a low loss value.'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型预训练**：这是通过向LLM提供数据批次并计算损失函数来更新LLM的权重和偏差的过程。损失函数衡量LLM在给定前一个标记的情况下预测下一个标记的能力。LLM通过使用**优化算法**（例如梯度下降）来调整权重和偏差，以减少损失并利用反向传播机制来最小化损失。模型训练可能需要多个epoch（在整个数据集上的迭代）才能收敛到低损失值。'
- en: '**Definition**'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: In the context of neural networks, the optimization algorithm during training
    is the method used to find the best set of weights for the model that minimizes
    the prediction error or maximizes the accuracy of the training data. The most
    common optimization algorithm for neural networks is **stochastic gradient descent**
    (**SGD**), which updates the weights in small steps based on the gradient of the
    error function and the current input-output pair. SGD is often combined with backpropagation,
    which we defined earlier in this chapter.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络背景下，训练过程中的优化算法是用于找到最小化预测误差或最大化训练数据准确性的模型最佳权重集的方法。神经网络中最常见的优化算法是**随机梯度下降**（**SGD**），它根据误差函数的梯度和当前输入-输出对以小步骤更新权重。SGD通常与我们在本章中先前定义的**反向传播**相结合。
- en: The output of the pre-training phase is the so-called base model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练阶段的输出被称为所谓的基模型。
- en: '**Fine-tuning**: The base model is trained in a supervised way with a dataset
    made of tuples of (prompt, ideal response). This step is necessary to make the
    base model more in line with AI assistants, such as ChatGPT. The output of this
    phase is called the **supervised fine-tuned** (**SFT**) model.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调**：基模型通过使用由（提示，理想响应）元组组成的监督数据集进行监督训练。这一步骤是使基模型更符合AI助手（如ChatGPT）所必需的。这一阶段的输出被称为**监督微调**（**SFT**）模型。'
- en: '**Reinforcement learning from human feedback** (**RLHF**): This step consists
    of iteratively optimizing the SFT model (by updating some of its parameters) with
    respect to the reward model (typically another LLM trained incorporating human
    preferences).'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**基于人类反馈的强化学习**（**RLHF**）：这一步骤包括通过更新其部分参数来迭代优化SFT模型（相对于奖励模型，通常是另一个结合人类偏好的LLM训练的模型）。'
- en: '**Definition**'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: '**Reinforcement learning** (**RL**) is a branch of machine learning that focuses
    on training computers to make optimal decisions by interacting with their environment.
    Instead of being given explicit instructions, the computer learns through trial
    and error: by exploring the environment and receiving rewards or penalties for
    its actions. The goal of reinforcement learning is to find the optimal behavior
    or policy that maximizes the expected reward or value of a given model. To do
    so, the RL process involves a **reward model** (**RM**) that is able to provide
    a “preferability score” to the computer. In the context of RLHF, the RM is trained
    to incorporate human preferences.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）是机器学习的一个分支，专注于通过与环境交互来训练计算机做出最佳决策。计算机不是被给予明确的指令，而是通过试错来学习：通过探索环境和为其动作获得奖励或惩罚。强化学习的目标是找到最优行为或策略，以最大化给定模型的预期奖励或价值。为此，RL过程涉及一个**奖励模型**（**RM**），能够为计算机提供“偏好分数”。在RLHF的背景下，RM被训练以结合人类偏好。'
- en: Note that RLHF is a pivotal milestone in achieving human alignment with AI systems.
    Due to the rapid achievements in the field of generative AI, it is pivotal to
    keep endowing those powerful LLMs and, more generally, LFMs with those preferences
    and values that are typical of human beings.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，RLHF（人类反馈强化学习）是实现人工智能系统与人类对齐的关键里程碑。由于生成人工智能领域的快速进展，持续赋予那些强大的LLM（大型语言模型）以及更广泛的LFM（语言模型）以典型的人类偏好和价值至关重要。
- en: Once we have a trained model, the next and final step is evaluating its performance.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有一个训练好的模型，下一步和最后一步就是评估其性能。
- en: Model evaluation
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型评估
- en: Evaluating traditional AI models was, in some ways, pretty intuitive. For example,
    let’s think about an image classification model that has to determine whether
    the input image represents a dog or a cat. So we train our model on a training
    dataset with a set of labeled images and, once the model is trained, we test it
    on unlabeled images. The evaluation metric is simply the percentage of correctly
    classified images over the total number of images within the test set.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 评估传统的AI模型在某些方面是相当直观的。例如，让我们考虑一个图像分类模型，它必须确定输入图像代表的是狗还是猫。因此，我们在一组标记图像的训练数据集上训练我们的模型，一旦模型训练完成，我们就在未标记的图像上对其进行测试。评估指标仅仅是测试集中正确分类的图像占总图像数量的百分比。
- en: When it comes to LLMs, the story is a bit different. As those models are trained
    on unlabeled text and are not task-specific, but rather generic and adaptable
    given a user’s prompt, traditional evaluation metrics were not suitable anymore.
    Evaluating an LLM means, among other things, measuring its language fluency, coherence,
    and ability to emulate different styles depending on the user’s request.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到LLM时，情况略有不同。由于这些模型是在未标记的文本上训练的，并且不是针对特定任务的，而是根据用户的提示具有通用性和适应性，因此传统的评估指标不再适用。评估一个LLM意味着，在众多方面中，测量其语言流畅性、连贯性以及根据用户请求模仿不同风格的能力。
- en: 'Hence, a new set of evaluation frameworks needed to be introduced. The following
    are the most popular frameworks used to evaluate LLMs:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，需要引入一套新的评估框架。以下是最常用的用于评估LLM的框架：
- en: '**General Language Understanding Evaluation** (**GLUE**) and **SuperGLUE**:
    This benchmark is used to measure the performance of LLMs on various NLU tasks,
    such as sentiment analysis, natural language inference, question answering, etc.
    The higher the score on the GLUE benchmark, the better the LLM is at generalizing
    across different tasks and domains.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用语言理解评估**（**GLUE**）和**SuperGLUE**：这个基准用于衡量LLM在多种自然语言理解（NLU）任务上的性能，例如情感分析、自然语言推理、问答等。在GLUE基准上的分数越高，LLM在不同任务和领域中的泛化能力就越强。'
- en: It recently evolved into a new benchmark styled after GLUE and called **SuperGLUE**,
    which comes with more difficult tasks. It consists of eight challenging tasks
    that require more advanced reasoning skills than GLUE, such as natural language
    inference, question answering, coreference resolution, etc., a broad coverage
    diagnostic set that tests models on various linguistic capabilities and failure
    modes, and a leaderboard that ranks models based on their average score across
    all tasks.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 它最近演变成一个以GLUE为风格的新的基准，称为**SuperGLUE**，它包含更困难的任务。它由八个需要比GLUE更高级推理技能的挑战性任务组成，例如自然语言推理、问答、指代消解等，一个广泛的覆盖诊断集，用于测试模型在多种语言能力和失败模式上的表现，以及一个根据模型在所有任务上的平均分数进行排名的排行榜。
- en: The difference between the GLUE and the SuperGLUE benchmark is that the SuperGLUE
    benchmark is more challenging and realistic than the GLUE benchmark, as it covers
    more complex tasks and phenomena, requires models to handle multiple domains and
    formats, and has higher human performance baselines. The SuperGLUE benchmark is
    designed to drive research in the development of more general and robust NLU systems.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: GLUE基准和SuperGLUE基准之间的区别在于，SuperGLUE基准比GLUE基准更具挑战性和现实性，因为它涵盖了更复杂的任务和现象，要求模型处理多个领域和格式，并且具有更高的基准人类性能。SuperGLUE基准旨在推动更通用和鲁棒的NLU系统开发的研究。
- en: '**Massive Multitask Language Understanding** (**MMLU**): This benchmark measures
    the knowledge of an LLM using zero-shot and few-shot settings.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大规模多任务语言理解**（**MMLU**）：这个基准使用零样本和少样本设置来衡量LLM的知识。'
- en: '**Definition**'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: The concept of zero-shot evaluation is a method of evaluating a language model
    without any labeled data or fine-tuning. It measures how well the language model
    can perform a new task by using natural language instructions or examples as prompts
    and computing the likelihood of the correct output given the input. It is the
    probability that a trained model will produce a particular set of tokens without
    needing any labeled training data.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本评估的概念是一种在没有任何标记数据或微调的情况下评估语言模型的方法。它通过使用自然语言指令或示例作为提示，并计算给定输入的正确输出的可能性，来衡量语言模型执行新任务的能力。这是训练模型在没有需要标记的训练数据的情况下产生特定一组标记的概率。
- en: This design adds complexity to the benchmark and aligns it more closely with
    the way we assess human performance. The benchmark comprises 14,000 multiple-choice
    questions categorized into 57 groups, spanning STEM, humanities, social sciences,
    and other fields. It covers a spectrum of difficulty levels, ranging from basic
    to advanced professional, assessing both general knowledge and problem-solving
    skills. The subjects encompass various areas, including traditional ones like
    mathematics and history, as well as specialized domains like law and ethics. The
    extensive range of subjects and depth of coverage make this benchmark valuable
    for uncovering any gaps in a model’s knowledge. Scoring is based on subject-specific
    accuracy and the average accuracy across all subjects.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计增加了基准的复杂性，使其更接近于我们评估人类表现的方式。基准包括14,000个多项选择题，分为57组，涵盖STEM、人文学科、社会科学和其他领域。它涵盖了从基础到高级专业水平的各种难度级别，评估了一般知识和解决问题的技能。主题包括传统领域，如数学和历史，以及法律和伦理等专门领域。广泛的主题范围和深度覆盖使这个基准对于揭示模型知识中的任何差距非常有价值。评分基于特定主题的准确性和所有主题的平均准确率。
- en: '**HellaSwag**: The HellaSwag evaluation framework is a method of evaluating
    LLMs on their ability to generate plausible and common sense continuations for
    given contexts. It is based on the HellaSwag dataset, which is a collection of
    70,000 multiple-choice questions that cover diverse domains and genres, such as
    books, movies, recipes, etc. Each question consists of a context (a few sentences
    that describe a situation or an event) and four possible endings (one correct
    and three incorrect). The endings are designed to be hard to distinguish for LLMs,
    as they require world knowledge, common sense reasoning, and linguistic understanding.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HellaSwag**：HellaSwag评估框架是一种评估LLM在给定上下文中生成合理和常识性延续的能力的方法。它基于HellaSwag数据集，这是一个包含70,000个多项选择题的集合，涵盖了多样化的领域和类型，如书籍、电影、食谱等。每个问题由一个上下文（描述情况或事件的几个句子）和四个可能的结尾（一个正确和三个错误）组成。结尾被设计成对LLM来说难以区分，因为它们需要世界知识、常识推理和语言理解。'
- en: '**TruthfulQA**: This benchmark evaluates a language model’s accuracy in generating
    responses to questions. It includes 817 questions across 38 categories like health,
    law, finance, and politics. The questions are designed to mimic those that humans
    might answer incorrectly due to false beliefs or misunderstandings.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TruthfulQA**：这个基准评估语言模型在生成问题回答的准确性。它包括38个类别（如健康、法律、金融和政治）中的817个问题。这些问题被设计成模仿人类可能由于错误信念或误解而回答错误的问题。'
- en: '**AI2 Reasoning Challenge** (**ARC**): This benchmark is used to measure LLMs’
    reasoning capabilities and to stimulate the development of models that can perform
    complex NLU tasks. It consists of a dataset of 7,787 multiple-choice science questions,
    assembled to encourage research in advanced question answering. The dataset is
    divided into an Easy set and a Challenge set, where the latter contains only questions
    that require complex reasoning or additional knowledge to answer correctly. The
    benchmark also provides a corpus of over 14 million science sentences that can
    be used as supporting evidence for the questions.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI2 Reasoning Challenge**（**ARC**）：这个基准用于衡量LLM的推理能力，并刺激能够执行复杂NLU任务的模型的发展。它由7,787个多项选择题的数据集组成，旨在鼓励高级问答研究。数据集分为简单集和挑战集，后者只包含需要复杂推理或额外知识才能正确回答的问题。基准还提供了一个超过1400万个科学句子的语料库，可以用作问题的支持证据。'
- en: It is important to note that each evaluation framework has a focus on a specific
    feature. Namely, the GLUE benchmark focuses on grammar, paraphrasing, and text
    similarity, while MMLU focuses on generalized language understanding among various
    domains and tasks. Hence, while evaluating an LLM, it is important to have a clear
    understanding of the final goal, so that the most relevant evaluation framework
    can be used. Alternatively, if the goal is that of having the best of the breed
    in any task, it is key not to use only one evaluation framework, but rather an
    average of multiple frameworks.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，每个评估框架都专注于特定的功能。具体来说，GLUE基准关注语法、释义和文本相似性，而MMLU关注各个领域和任务中的泛化语言理解。因此，在评估LLM时，重要的是要清楚了解最终目标，以便使用最相关的评估框架。或者，如果目标是任何任务中的最佳实践，关键不是只使用一个评估框架，而是多个框架的平均值。
- en: In addition to that, in case no existing LLM is able to tackle your specific
    use cases, you still have a margin to customize those models and make them more
    tailored toward your application scenarios. In the next section, we are indeed
    going to cover the existing techniques of LLM customization, from the lightest
    ones (such as prompt engineering) up to the whole training of an LLM from scratch.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果没有任何现有的LLM能够处理你的特定用例，你仍然有空间自定义这些模型，使它们更符合你的应用场景。在下一节中，我们将确实介绍LLM定制的现有技术，从最轻的（如提示工程）到从头开始训练整个LLM。
- en: Base models versus customized models
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基础模型与定制模型
- en: The nice thing about LLMs is that they have been trained and ready to use. As
    we saw in the previous section, training an LLM requires great investment in hardware
    (GPUs or TPUs) and it might last for months, and these two factors might mean
    it is not feasible for individuals and small businesses.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的好处在于它们已经训练好并准备好使用。正如我们在上一节中看到的，训练LLM需要大量硬件投资（GPU或TPU），可能需要数月时间，这两个因素可能意味着对个人和小企业来说不可行。
- en: Luckily, pre-trained LLMs are generalized enough to be applicable to various
    tasks, so they can be consumed without further tuning directly via their REST
    API (we will dive deeper into model consumption in the next chapters).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，预训练的LLM足够通用，可以应用于各种任务，因此可以直接通过它们的REST API进行消费，无需进一步调整（我们将在下一章深入探讨模型消费）。
- en: Nevertheless, there might be scenarios where a general-purpose LLM is not enough,
    since it lacks domain-specific knowledge or doesn’t conform to a particular style
    and taxonomy of communication. If this is the case, you might want to customize
    your model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，可能存在一些场景，通用LLM不足以应对，因为它缺乏特定领域的知识或不符合特定的沟通风格和分类法。如果是这种情况，你可能想定制你的模型。
- en: How to customize your model
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何定制你的模型
- en: 'There are three main ways to customize your model:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 定制你的模型主要有三种方法：
- en: '**Extending non-parametric knowledge**: This allows the model to access external
    sources of information to integrate its parametric knowledge while responding
    to the user’s query.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展非参数化知识**：这允许模型在响应用户查询时访问外部信息源，以整合其参数化知识。'
- en: '**Definition**'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: 'LLMs exhibit two types of knowledge: parametric and non-parametric. The parametric
    knowledge is the one embedded in the LLM’s parameters, deriving from the unlabeled
    text corpora during the training phase. On the other hand, non-parametric knowledge
    is the one we can “attach” to the model via embedded documentation. Non-parametric
    knowledge doesn’t change the structure of the model, but rather, allows it to
    navigate through external documentation to be used as relevant context to answer
    the user’s query.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: LLM表现出两种类型的知识：参数化和非参数化。参数化知识是嵌入在LLM参数中的知识，来源于训练阶段的无标签文本语料库。另一方面，非参数化知识是我们可以通过嵌入文档“附加”到模型上的知识。非参数化知识不会改变模型的结构，而是允许它通过外部文档导航，用作回答用户查询的相关上下文。
- en: This might involve connecting the model to web sources (like Wikipedia) or internal
    documentation with domain-specific knowledge. The connection of the LLM to external
    sources is called a plug-in, and we will be discussing it more deeply in the hands-on
    section of this book.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能涉及将模型连接到网络资源（如维基百科）或具有特定领域知识的内部文档。将LLM连接到外部源的过程称为插件，我们将在本书的实践部分更深入地讨论它。
- en: '**Few-shot learning**: In this type of model customization, the LLM is given
    a **metaprompt** with a small number of examples (typically between 3 and 5) of
    each new task it is asked to perform. The model must use its prior knowledge to
    generalize from these examples to perform the task.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少样本学习**：在这种类型的模型定制中，LLM被提供了一个包含少量示例（通常在3到5个之间）的**元提示**，这些示例是它被要求执行的新任务。模型必须使用其先验知识从这些示例中泛化以执行任务。'
- en: '**Definition**'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义**'
- en: A metaprompt is a message or instruction that can be used to improve the performance
    of LLMs on new tasks with a few examples.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 元提示是一种可以用于通过几个示例提高LLMs在新任务上性能的消息或指令。
- en: '**Fine tuning**: The fine-tuning process involves using smaller, task-specific
    datasets to customize the foundation models for particular applications.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微调**：微调过程涉及使用较小、特定于任务的数据集来定制特定应用的基座模型。'
- en: This approach differs from the first ones because, with fine-tuning, the parameters
    of the pre-trained model are altered and optimized toward the specific task. This
    is done by training the model on a smaller labeled dataset that is specific to
    the new task. The key idea behind fine-tuning is to leverage the knowledge learned
    from the pre-trained model and fine-tune it to the new task, rather than training
    a model from scratch.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与最初的方法不同，因为通过微调，预训练模型的参数被调整并优化以适应特定任务。这是通过在针对新任务的小型标记数据集上训练模型来完成的。微调背后的关键思想是利用从预训练模型中学到的知识，并将其微调以适应新任务，而不是从头开始训练一个模型。
- en: '![A diagram of a data processing process  Description automatically generated](img/B21714_01_12.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![数据处理流程图，描述自动生成](img/B21714_01_12.png)'
- en: 'Figure 1.12: Illustration of the process of fine-tuning'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.12：微调过程示意图
- en: 'In the preceding figure, you can see a schema on how fine-tuning works on OpenAI
    pre-built models. The idea is that you have available a pre-trained model with
    general-purpose weights or parameters. Then, you feed your model with custom data,
    typically in the form of “key-value” prompts and completions:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，你可以看到一个关于在OpenAI预建模型上如何进行微调的架构。其思想是，你有一个带有通用权重或参数的预训练模型可用。然后，你用自定义数据来喂养你的模型，通常是“键值”提示和完成的表单：
- en: '[PRE0]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once the training is done, you will have a customized model that is particularly
    performant for a given task, for example, the classification of your company’s
    documentation.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成训练，你将有一个针对特定任务特别高效的定制模型，例如，你公司文档的分类。
- en: The nice thing about fine-tuning is that you can make pre-built models tailored
    to your use cases, without the need to retrain them from scratch, yet leveraging
    smaller training datasets and hence less training time and compute. At the same
    time, the model keeps its generative power and accuracy learned via the original
    training, the one that occurred to the massive dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的好处在于，你可以根据你的用例定制预建模型，而无需从头开始重新训练它们，同时利用较小的训练数据集，从而减少训练时间和计算。同时，模型保留了通过原始训练学到的生成能力和准确性，这是在大量数据集上发生的。
- en: In *Chapter 11*, *Fine-Tuning Large Language Models*, we will focus on fine-tuning
    your model in Python so that you can test it for your own task.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第11章*，“微调大型语言模型”中，我们将专注于在Python中微调你的模型，以便你可以测试它对你的任务。
- en: On top of the above techniques (which you can also combine among each other),
    there is a fourth one, which is the most “drastic.” It consists of training an
    LLM from scratch, which you might want to either build on your own or initialize
    from a pre-built architecture. We will see how to approach this technique in the
    final chapters.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述技术（你还可以将它们相互结合）之上，还有一个第四个技术，这是最“激进”的。它包括从头开始训练一个LLM，你可能想自己构建或者从一个预建架构中初始化。我们将在最后一章中看到如何接近这种技术。
- en: Summary
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the field of LLMs, with a technical deep dive into
    their architecture, functioning, and training process. We saw the most prominent
    architectures, such as the transformer-based frameworks, how the training process
    works, and different ways to customize your own LLM.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了大型语言模型（LLMs）的领域，对其架构、功能和训练过程进行了技术深入探讨。我们看到了最突出的架构，例如基于transformer的框架，训练过程是如何工作的，以及自定义你自己的LLM的不同方法。
- en: We now have the foundation to understand what LLMs are. In the next chapter,
    we will see *how* to use them and, more specifically, how to build intelligent
    applications with them.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了理解什么是大型语言模型（LLMs）的基础。在下一章中，我们将看到*如何*使用它们，以及更具体地，如何使用它们构建智能应用。
- en: References
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Attention is all you need: [1706.03762.pdf (arxiv.org)](https://arxiv.org)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意力即是所需：[1706.03762.pdf (arxiv.org)](https://arxiv.org)
- en: 'Possible End of Humanity from AI? Geoffrey Hinton at MIT Technology Review’s
    EmTech Digital: [https://www.youtube.com/watch?v=sitHS6UDMJc&t=594s&ab_channel=JosephRaczynski](https://www.youtube.com/watch?v=sitHS6UDMJc&t=594s&ab_channel=JosephRaczynski)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工智能可能导致人类终结？杰弗里·辛顿在麻省理工学院科技评论的EmTech Digital上的讨论：[https://www.youtube.com/watch?v=sitHS6UDMJc&t=594s&ab_channel=JosephRaczynski](https://www.youtube.com/watch?v=sitHS6UDMJc&t=594s&ab_channel=JosephRaczynski)
- en: 'The Glue Benchmark: [https://gluebenchmark.com/](https://gluebenchmark.com/)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glue基准测试：[https://gluebenchmark.com/](https://gluebenchmark.com/)
- en: 'TruthfulQA: [https://paperswithcode.com/dataset/truthfulqa](https://paperswithcode.com/dataset/truthfulqa)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TruthfulQA：[https://paperswithcode.com/dataset/truthfulqa](https://paperswithcode.com/dataset/truthfulqa)
- en: 'Hugging Face Open LLM Leaderboard: [https://huggingface.co/spaces/optimum/llm-perf-leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face Open LLM排行榜：[https://huggingface.co/spaces/optimum/llm-perf-leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)
- en: 'Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge:
    [https://arxiv.org/abs/1803.05457](https://arxiv.org/abs/1803.05457 )'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你认为自己已经解决了问答问题？试试ARC，AI2推理挑战：[https://arxiv.org/abs/1803.05457](https://arxiv.org/abs/1803.05457
    )
- en: Join our community on Discord
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llm]( https://packt.link/llm )'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llm]( https://packt.link/llm )'
- en: '![](img/QR_Code214329708533108046.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code214329708533108046.png)'
