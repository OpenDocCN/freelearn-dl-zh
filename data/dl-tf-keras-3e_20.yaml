- en: '20'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '20'
- en: Advanced Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级卷积神经网络
- en: 'In this chapter, we will see some more advanced uses for **Convolutional Neural
    Networks** (**CNNs**). We will explore:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将看到 CNN 的一些更高级的应用。我们将探索：
- en: How CNNs can be applied within the areas of computer vision, video, textual
    documents, audio, and music
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN 如何应用于计算机视觉、视频、文本文件、音频和音乐等领域
- en: How to use CNNs for text processing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 CNN 进行文本处理
- en: What capsule networks are
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胶囊网络是什么
- en: Computer vision
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp20](https://packt.link/dltfchp20).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码文件可以在 [https://packt.link/dltfchp20](https://packt.link/dltfchp20) 找到。
- en: Let’s start by using CNNs for complex tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从使用 CNN 进行复杂任务开始。
- en: Composing CNNs for complex tasks
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合 CNN 进行复杂任务
- en: We have discussed CNNs quite extensively in *Chapter 3*, *Convolutional Neural
    Networks*, and at this point, you are probably convinced about the effectiveness
    of the CNN architecture for image classification tasks. What you may find surprising,
    however, is that the basic CNN architecture can be composed and extended in various
    ways to solve a variety of more complex tasks. In this section, we will look at
    the computer vision tasks mentioned in *Figure 20.1* and show how they can be
    solved by turning CNNs into larger and more complex architectures.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第 3 章*《卷积神经网络》中已经详细讨论了 CNN，现在你可能已经深信 CNN 架构对于图像分类任务的有效性。然而，你可能会惊讶地发现，基本的
    CNN 架构可以通过组合和扩展的方式，解决各种更复杂的任务。在本节中，我们将查看*图 20.1*中提到的计算机视觉任务，并展示如何通过将 CNN 转变为更大、更复杂的架构来解决它们。
- en: '![](img/B18331_20_01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_01.png)'
- en: 'Figure 20.1: Different Computer Vision Tasks – source: Introduction to Artificial
    Intelligence and Computer Vision Revolution (https://www.slideshare.net/darian_f/introduction-to-the-artificial-intelligence-and-computer-vision-revolution)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.1：不同的计算机视觉任务 – 来源：人工智能与计算机视觉革命介绍 (https://www.slideshare.net/darian_f/introduction-to-the-artificial-intelligence-and-computer-vision-revolution)
- en: Classification and localization
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类与定位
- en: In the classification and localization task, not only do you have to report
    the class of object found in the image, but also the coordinates of the bounding
    box where the object appears in the image. This type of task assumes that there
    is only one instance of the object in an image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类和定位任务中，除了需要报告图像中物体的类别，还需要给出物体在图像中出现的边界框坐标。这类任务假设图像中只有一个物体实例。
- en: This can be achieved by attaching a “regression head” in addition to the “classification
    head” in a typical classification network. Recall that in a classification network,
    the final output of convolution and pooling operations, called the feature map,
    is fed into a fully connected network that produces a vector of class probabilities.
    This fully connected network is called the classification head, and it is tuned
    using a categorical loss function (*L*[c]) such as categorical cross-entropy.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过在典型的分类网络中，除了“分类头”外再附加一个“回归头”来实现。回想一下，在一个分类网络中，卷积和池化操作的最终输出，称为特征图，将被送入一个全连接网络，生成一个类别概率向量。这个全连接网络被称为分类头，并通过使用分类损失函数
    (*L*[c])，如类别交叉熵，进行调优。
- en: Similarly, a regression head is another fully connected network that takes the
    feature map and produces a vector (*x*, *y*, *w*, *h*) representing the top left
    *x* and *y* coordinates, and the width and height of the bounding box. It is tuned
    using a continuous loss function (*L*[R]) such as mean squared error. The entire
    network is tuned using a linear combination of the two losses, i.e.,
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，回归头是另一个全连接网络，它接收特征图并生成一个向量 (*x*, *y*, *w*, *h*)，表示左上角的 *x* 和 *y* 坐标，以及边界框的宽度和高度。它通过使用连续损失函数
    (*L*[R])，如均方误差，进行调优。整个网络通过这两种损失的线性组合进行调优，即：
- en: '![](img/B18331_20_001.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_001.png)'
- en: Here, ![](img/B18331_11_021.png) is a hyperparameter and can take a value between
    0 and 1\. Unless the value is determined by some domain knowledge about the problem,
    it can be set to 0.5.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B18331_11_021.png) 是一个超参数，可以取值在 0 到 1 之间。除非该值由一些关于问题的领域知识决定，否则可以设置为
    0.5。
- en: '*Figure 20.2* shows a typical classification and localization network architecture:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 20.2* 显示了一个典型的分类与定位网络架构：'
- en: '![](img/B18331_20_02.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_02.png)'
- en: 'Figure 20.2: Network architecture for image classification and localization'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.2：图像分类与定位的网络架构
- en: As you can see, the only difference with respect to a typical CNN classification
    network is the additional regression head at the top right-hand side.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，与典型的 CNN 分类网络的唯一区别是右上角额外的回归头。
- en: Semantic segmentation
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义分割
- en: Another class of problem that builds on the basic classification idea is “semantic
    segmentation.” Here the aim is to classify every single pixel on the image as
    belonging to a single class.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基于基本分类思想的另一类问题是“语义分割”。其目标是将图像中的每一个像素分类为属于某一类。
- en: An initial method of implementation could be to build a classifier network for
    each pixel, where the input is a small neighborhood around each pixel. In practice,
    this approach is not very performant, so an improvement over this implementation
    might be to run the image through convolutions that will increase the feature
    depth, while keeping the image width and height constant. Each pixel then has
    a feature map that can be sent through a fully connected network that predicts
    the class of the pixel. However, in practice, this is also quite expensive and
    is not normally used.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一种初步的实现方法可能是为每个像素构建一个分类网络，其中输入是每个像素周围的小邻域。实际上，这种方法的性能并不好，因此对该实现的改进可能是通过卷积操作处理图像，增加特征深度，同时保持图像的宽度和高度不变。每个像素会有一个特征图，可以通过一个全连接网络来预测该像素的类别。然而，实际上，这也相当昂贵，通常不被使用。
- en: A third approach is to use a CNN encoder-decoder network, where the encoder
    decreases the width and height of the image but increases its depth (number of
    features), while the decoder uses transposed convolution operations to increase
    its size and decrease its depth. Transposed convolution (or upsampling) is the
    process of going in the opposite direction of a normal convolution. Input to this
    network is the image and the output is the segmentation map. A popular implementation
    of this encoder-decoder architecture is the U-Net (a good implementation is available
    at [https://github.com/jakeret/tf_unet](https://github.com/jakeret/tf_unet)),
    originally developed for biomedical image segmentation, which has additional skip
    connections between corresponding layers of the encoder and decoder.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法是使用 CNN 编码器-解码器网络，其中编码器减小图像的宽度和高度，但增加其深度（特征数量），而解码器使用反卷积操作增加图像的尺寸并减少其深度。反卷积（或上采样）是进行与普通卷积相反的过程。该网络的输入是图像，输出是分割图。该编码器-解码器架构的一个流行实现是
    U-Net（一个很好的实现可以在[https://github.com/jakeret/tf_unet](https://github.com/jakeret/tf_unet)找到），最初为生物医学图像分割开发，具有编码器和解码器之间额外的跳跃连接。
- en: '*Figure 20.3* shows the U-Net architecture:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 20.3* 展示了 U-Net 架构：'
- en: '![Chart  Description automatically generated](img/B18331_20_03.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![Chart  Description automatically generated](img/B18331_20_03.png)'
- en: 'Figure 20.3: U-Net architecture'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.3：U-Net 架构
- en: Object detection
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标检测
- en: The object detection task is similar to the classification and localization
    task. The big difference is that now there are multiple objects in the image,
    and for each one of them, we need to find the class and the bounding box coordinates.
    In addition, neither the number of objects nor their size is known in advance.
    As you can imagine, this is a difficult problem, and a fair amount of research
    has gone into it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测任务与分类和定位任务类似。最大的区别在于现在图像中有多个对象，并且对于每个对象，我们需要找到其类别和边界框坐标。此外，事先无法知道对象的数量或大小。正如你所想，这个问题非常复杂，且已经有大量的研究投入其中。
- en: A first approach to the problem might be to create many random crops of the
    input image and, for each crop, apply the classification and localization network
    we described earlier. However, such an approach is very wasteful in terms of computing
    and unlikely to be very successful.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的第一种方法可能是创建许多输入图像的随机裁剪，对于每个裁剪，应用我们之前描述的分类和定位网络。然而，这种方法在计算上非常浪费，且不太可能非常成功。
- en: 'A more practical approach would be to use a tool such as Selective Search (*Selective
    Search for Object Recognition*, by Uijlings et al., [http://www.huppelen.nl/publications/selectiveSearchDraft.pdf](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)),
    which uses traditional computer vision techniques to find areas in the image that
    might contain objects. These regions are called “region proposals,” and the network
    to detect them is called **Region-based CNN**, or **R-CNN**. In the original R-CNN,
    the regions were resized and fed into a network to yield image vectors. These
    vectors were then classified with an SVM-based classifier (see [https://en.wikipedia.org/wiki/Support-vector_machine](https://en.wikipedia.org/wiki/Support-vector_machine)),
    and the bounding boxes proposed by the external tool were corrected using a linear
    regression network over the image vectors. An R-CNN network can be represented
    conceptually as shown in *Figure 20.4*:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 更实用的方法是使用如选择性搜索（*Selective Search for Object Recognition*，由 Uijlings 等人编写，[http://www.huppelen.nl/publications/selectiveSearchDraft.pdf](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf)）等工具，利用传统的计算机视觉技术来查找图像中可能包含物体的区域。这些区域被称为“区域建议”，用于检测这些区域的网络称为**基于区域的
    CNN**，或称**R-CNN**。在原始的 R-CNN 中，区域被调整大小后输入网络，产生图像向量。然后，使用基于 SVM 的分类器对这些向量进行分类（见[https://en.wikipedia.org/wiki/Support-vector_machine](https://en.wikipedia.org/wiki/Support-vector_machine)），外部工具提出的边界框通过图像向量上的线性回归网络进行修正。R-CNN
    网络的概念表示如 *图 20.4* 所示：
- en: '![Chart, diagram  Description automatically generated](img/B18331_20_04.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图表，描述自动生成](img/B18331_20_04.png)'
- en: 'Figure 20.4: R-CNN network'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.4：R-CNN 网络
- en: The next iteration of the R-CNN network is called the Fast R-CNN. The Fast R-CNN
    still gets its region proposals from an external tool, but instead of feeding
    each region proposal through the CNN, the entire image is fed through the CNN
    and the region proposals are projected onto the resulting feature map. Each region
    of interest is fed through a **Region Of Interest** (**ROI**) pooling layer and
    then to a fully connected network, which produces a feature vector for the ROI.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN 网络的下一个迭代版本叫做 Fast R-CNN。Fast R-CNN 仍然从外部工具获取区域建议，但不再将每个区域建议分别输入 CNN，而是将整个图像输入
    CNN，并将区域建议投影到生成的特征图上。每个感兴趣的区域会通过**感兴趣区域**（**ROI**）池化层，然后传递到一个全连接网络，生成该 ROI 的特征向量。
- en: 'ROI pooling is a widely used operation in object detection tasks using CNNs.
    The ROI pooling layer uses max pooling to convert the features inside any valid
    region of interest into a small feature map with a fixed spatial extent of *H*
    x *W* (where *H* and *W* are two hyperparameters). The feature vector is then
    fed into two fully connected networks, one to predict the class of the ROI and
    the other to correct the bounding box coordinates for the proposal. This is illustrated
    in *Figure 20.5*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ROI 池化是使用 CNN 进行物体检测任务时广泛应用的一种操作。ROI 池化层使用最大池化将任何有效感兴趣区域内的特征转换为一个具有固定空间大小 *H*
    x *W*（其中 *H* 和 *W* 是两个超参数）的较小特征图。然后，将该特征向量输入两个全连接网络，一个用于预测 ROI 的类别，另一个用于修正区域建议的边界框坐标。如
    *图 20.5* 所示：
- en: '![Diagram  Description automatically generated](img/B18331_20_05.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图表，描述自动生成](img/B18331_20_05.png)'
- en: 'Figure 20.5: Fast R-CNN network architecture'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.5：Fast R-CNN 网络架构
- en: The Fast R-CNN is about 25x faster than the R-CNN. The next improvement, called
    the Faster R-CNN (an implementation is at [https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN)),
    removes the external region proposal mechanism and replaces it with a trainable
    component, called the **Region Proposal Network** (**RPN**), within the network
    itself. The output of this network is combined with the feature map and passed
    in through a similar pipeline to the Fast R-CNN network, as shown in *Figure 20.6*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN 比 R-CNN 快约 25 倍。下一步的改进，称为 Faster R-CNN（实现代码可见 [https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN](https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN)），移除了外部区域建议机制，并用一个可训练组件
    —— **区域建议网络**（**RPN**） —— 替代，嵌入到网络本身。该网络的输出与特征图结合，并通过与 Fast R-CNN 网络类似的管道传递，如
    *图 20.6* 所示。
- en: 'The Faster R-CNN network is about 10x faster than the Fast R-CNN network, making
    it approximately 250x faster than an R-CNN network:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN 网络的速度是 Fast R-CNN 网络的约 10 倍，使其比 R-CNN 网络快约 250 倍：
- en: '![Diagram  Description automatically generated](img/B18331_20_06.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图表，描述自动生成](img/B18331_20_06.png)'
- en: 'Figure 20.6: Faster R-CNN network architecture'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.6：Faster R-CNN 网络架构
- en: Another somewhat different class of object detection networks are **Single Shot
    Detectors** (**SSD**) such as **YOLO** (**You Only Look Once**). In these cases,
    each image is split into a predefined number of parts using a grid. In the case
    of YOLO, a 7 x 7 grid is used, resulting in 49 sub-images. A predetermined set
    of crops with different aspect ratios are applied to each sub-image. Given *B*
    bounding boxes and *C* object classes, the output for each image is a vector of
    size ![](img/B18331_20_003.png). Each bounding box has a confidence and coordinates
    (*x*, *y*, *w*, *h*), and each grid has prediction probabilities for the different
    objects detected within them.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类稍微不同的物体检测网络是 **单次检测器**（**SSD**），例如 **YOLO**（**You Only Look Once**）。在这些情况下，每张图片都会被使用网格分割成预定义数量的部分。对于
    YOLO 来说，使用的是一个 7 x 7 的网格，结果是 49 个子图像。每个子图像都会应用一组预定的不同纵横比的裁剪。给定 *B* 个边界框和 *C* 个物体类别，每张图片的输出是一个大小为
    ![](img/B18331_20_003.png) 的向量。每个边界框都有一个置信度和坐标（*x*，*y*，*w*，*h*），每个网格会有一个预测概率，表示在其中检测到的不同物体。
- en: The YOLO network is a CNN, which does this transformation. The final predictions
    and bounding boxes are found by aggregating the findings from this vector. In
    YOLO, a single convolutional network predicts the bounding boxes and the related
    class probabilities. YOLO is the faster solution for object detection. An implementation
    is at [https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow](https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO 网络是一个 CNN，它执行这种转换。最终的预测和边界框通过聚合此向量中的结果来获得。在 YOLO 中，单个卷积网络预测边界框及相关类别概率。YOLO
    是物体检测的更快解决方案。实现可以在 [https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow](https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow)
    找到。
- en: Instance segmentation
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实例分割
- en: Instance segmentation is similar to semantic segmentation – the process of associating
    each pixel of an image with a class label – with a few important distinctions.
    First, it needs to distinguish between different instances of the same class in
    an image. Second, it is not required to label every single pixel in the image.
    In some respects, instance segmentation is also similar to object detection, except
    that instead of bounding boxes, we want to find a binary mask that covers each
    object.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 实例分割与语义分割相似——即将图像的每个像素与一个类别标签关联——但有一些重要的区别。首先，它需要区分图像中同一类别的不同实例。其次，它不要求标记图像中的每一个像素。在某些方面，实例分割也类似于物体检测，不同之处在于我们不使用边界框，而是需要找到覆盖每个物体的二进制掩码。
- en: 'The second definition leads to the intuition behind the Mask R-CNN network.
    The Mask R-CNN is a Faster R-CNN with an additional CNN in front of its regression
    head, which takes as input the bounding box coordinates reported for each ROI
    and converts it to a binary mask [11]:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个定义揭示了 Mask R-CNN 网络背后的直觉。Mask R-CNN 是一个带有额外 CNN 的 Faster R-CNN，该 CNN 位于回归头部之前，输入为为每个
    ROI 报告的边界框坐标，并将其转换为二进制掩码 [11]：
- en: '![Diagram  Description automatically generated](img/B18331_20_07.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](img/B18331_20_07.png)'
- en: 'Figure 20.7: Mask R-CNN architecture'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.7：Mask R-CNN 架构
- en: In April 2019, Google released Mask R-CNN in open source, pretrained with TPUs.
    This is available at
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2019 年 4 月，Google 开源发布了 Mask R-CNN，并且用 TPUs 进行了预训练。你可以在以下链接找到该模型：
- en: '[https://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/mask_rcnn/mask_rcnn_demo.ipynb](https://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/mask_rcnn/mask_rcnn_demo.ipynb).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/mask_rcnn/mask_rcnn_demo.ipynb](https://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/mask_rcnn/mask_rcnn_demo.ipynb)。'
- en: 'I suggest playing with the Colab notebook to see what the results are. In *Figure
    20.8*, we see an example of image segmentation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议你尝试一下 Colab 笔记本，看看结果如何。在 *图 20.8* 中，我们看到了一个图像分割的示例：
- en: '![](img/B18331_20_08.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_08.png)'
- en: 'Figure 20.8: An example of image segmentation'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.8：图像分割的一个示例
- en: Google also released another model trained on TPUs called DeepLab, and you can
    see an image (*Figure 20.9*) from the demo. This is available at
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Google 还发布了另一个基于 TPUs 训练的模型，名为 DeepLab，你可以从演示中看到一张图片（*图 20.9*）。这个模型可以在以下链接找到：
- en: '[https://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb#scrollTo=edGukUHXyymr](https://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb#scrollTo=edGukUHXyymr):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb#scrollTo=edGukUHXyymr](https://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb#scrollTo=edGukUHXyymr)：'
- en: '![](img/B18331_20_09.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_09.png)'
- en: 'Figure 20.9: An example of image segmentation'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.9：图像分割的示例
- en: In this section, we have covered, at a somewhat high level, various network
    architectures that are popular in computer vision. Note that all of them are composed
    by the same basic CNN and fully connected architectures. This composability is
    one of the most powerful features of deep learning. Hopefully, this has given
    you some ideas for networks that could be adapted for your own computer vision
    use cases.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们大致介绍了几种在计算机视觉领域流行的网络架构。请注意，所有这些架构都由相同的基本 CNN 和全连接架构组成。这种可组合性是深度学习最强大的特性之一。希望这能给你一些启示，帮助你设计适合自己计算机视觉应用的网络。
- en: Application zoos with tf.Keras and TensorFlow Hub
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 tf.Keras 和 TensorFlow Hub 的应用程序
- en: One of the nice things about transfer learning is that it is possible to reuse
    pretrained networks to save time and resources. There are many collections of
    ready-to-use networks out there, but the following two are the most used.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习的一个好处是可以重用预训练网络，从而节省时间和资源。市面上有许多现成的网络集合，但以下两个是最常用的。
- en: Keras Applications
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keras 应用程序
- en: Keras Applications (Keras Applications are available at [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications))
    includes models for image classification with weights trained on ImageNet (Xception,
    VGG16, VGG19, ResNet, ResNetV2, ResNeXt, InceptionV3, InceptionResNetV2, MobileNet,
    MobileNetV2, DenseNet, and NASNet). In addition, there are a few other reference
    implementations from the community for object detection and segmentation, sequence
    learning, reinforcement learning (see *Chapter 11*), and GANs (see *Chapter 9*).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 应用程序（Keras 应用程序可以在 [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)
    找到）包括了用于图像分类的模型，这些模型在 ImageNet 上训练过（Xception、VGG16、VGG19、ResNet、ResNetV2、ResNeXt、InceptionV3、InceptionResNetV2、MobileNet、MobileNetV2、DenseNet
    和 NASNet）。此外，还有一些来自社区的其他参考实现，涉及目标检测和分割、序列学习、强化学习（见 *第 11 章*）以及 GANs（见 *第 9 章*）。
- en: TensorFlow Hub
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Hub
- en: TensorFlow Hub (available at [https://www.tensorflow.org/hub](https://www.tensorflow.org/hub))
    is an alternative collection of pretrained models. TensorFlow Hub includes modules
    for text classification, sentence encoding (see *Chapter 4*), image classification,
    feature extraction, image generation with GANs, and video classification. Currently,
    both Google and DeepMind contribute to TensorFlow Hub.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Hub（可访问 [https://www.tensorflow.org/hub](https://www.tensorflow.org/hub)）是一个预训练模型的替代集合。TensorFlow
    Hub 包含了文本分类、句子编码（见 *第 4 章*）、图像分类、特征提取、使用 GAN 生成图像以及视频分类的模块。目前，Google 和 DeepMind
    都在为 TensorFlow Hub 做贡献。
- en: 'Let’s look at an example of using `TF.Hub`. In this case, we have a simple
    image classifier using MobileNetv2:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个使用 `TF.Hub` 的示例。在这个例子中，我们有一个使用 MobileNetv2 的简单图像分类器：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Pretty simple indeed. Just remember to use `hub.KerasLayer()` for wrapping any
    Hub layer. In this section, we have discussed how to use TensorFlow Hub.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 确实很简单。只需记得使用 `hub.KerasLayer()` 来包装任何 Hub 层。在本节中，我们讨论了如何使用 TensorFlow Hub。
- en: Next, we will focus on other CNN architectures.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将重点介绍其他 CNN 架构。
- en: Answering questions about images (visual Q&A)
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回答关于图像的问题（视觉问答）
- en: One of the nice things about neural networks is that different media types can
    be combined together to provide a unified interpretation. For instance, **Visual
    Question Answering** (**VQA**) combines image recognition and text natural language
    processing. Training can use VQA (VQA is available at [https://visualqa.org/](https://visualqa.org/)),
    a dataset containing open-ended questions about images. These questions require
    an understanding of vision, language, and common knowledge to be answered. The
    following images are taken from a demo available at [https://visualqa.org/](https://visualqa.org/).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个优点是可以将不同类型的媒体结合在一起，以提供统一的解释。例如，**视觉问答**（**VQA**）结合了图像识别和文本自然语言处理。训练可以使用VQA（VQA
    数据集可以在[https://visualqa.org/](https://visualqa.org/)获取），它包含有关图像的开放式问题。这些问题需要理解视觉、语言和常识才能回答。以下图像来自于[https://visualqa.org/](https://visualqa.org/)上的一个演示。
- en: 'Note the question at the top of the image, and the subsequent answers:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意图像顶部的问题，以及随后的答案：
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_20_10.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B18331_20_10.png)'
- en: 'Figure 20.10: Examples of visual question and answers'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.10：视觉问答示例
- en: 'If you want to start playing with VQA, the first thing is to get appropriate
    training datasets such as the VQA dataset, the CLEVR dataset (available at [https://cs.stanford.edu/people/jcjohns/clevr/](https://cs.stanford.edu/people/jcjohns/clevr/)),
    or the FigureQA dataset (available at [https://datasets.maluuba.com/FigureQA](https://datasets.maluuba.com/FigureQA));
    alternatively, you can participate in a Kaggle VQA challenge (available at [https://www.kaggle.com/c/visual-question-answering](https://www.kaggle.com/c/visual-question-answering)).
    Then you can build a model that is the combination of a CNN and an RNN and start
    experimenting. For instance, a CNN can be something like this code fragment, which
    takes an image with three channels (224 x 224) as input and produces a feature
    vector for the image:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想开始玩VQA，首先需要获取适当的训练数据集，如VQA数据集、CLEVR数据集（可在[https://cs.stanford.edu/people/jcjohns/clevr/](https://cs.stanford.edu/people/jcjohns/clevr/)获取）或FigureQA数据集（可在[https://datasets.maluuba.com/FigureQA](https://datasets.maluuba.com/FigureQA)获取）；或者，你可以参与Kaggle的VQA挑战（可在[https://www.kaggle.com/c/visual-question-answering](https://www.kaggle.com/c/visual-question-answering)参与）。然后，你可以构建一个结合CNN和RNN的模型并开始实验。例如，CNN可以是这样的代码片段，它接受一个具有三个通道（224
    x 224）的图像作为输入，并为图像生成一个特征向量：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Text can be encoded with an RNN; for now, think of it as a black box taking
    a text fragment (the question) in input and producing a feature vector for the
    text:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 文本可以通过RNN进行编码；目前，可以将其视为一个黑盒，它接受一个文本片段（问题）作为输入，并为文本生成一个特征向量：
- en: '[PRE2]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then the two feature vectors (one for the image, and one for the text) are
    combined into one joint vector, which is provided as input to a dense network
    to produce the combined network:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将两个特征向量（一个是图像的，另一个是文本的）合并为一个联合向量，该向量作为输入提供给密集网络，以生成组合网络：
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For instance, if we have a set of labeled images, then we can learn what the
    best questions and answers are for describing an image. The number of options
    is enormous! If you want to know more, I suggest that you investigate Maluuba,
    a start-up providing the FigureQA dataset with 100,000 figure images and 1,327,368
    question-answer pairs in the training set. Maluuba has been recently acquired
    by Microsoft, and the lab is advised by Yoshua Bengio, one of the fathers of deep
    learning.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一组标记的图像，那么我们可以学习描述图像的最佳问题和答案。选择的数量非常庞大！如果你想了解更多，我建议你调查Maluuba，一家提供FigureQA数据集的初创公司，该数据集包含100,000个图像和1,327,368对问答。Maluuba最近被微软收购，实验室由深度学习的奠基人之一Yoshua
    Bengio担任顾问。
- en: In this section, we have discussed how to implement visual Q&A. The next section
    is about style transfer, a deep learning technique used for training neural networks
    to create art.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了如何实现视觉问答。下一节将介绍风格迁移，这是一种用于训练神经网络创作艺术的深度学习技术。
- en: Creating a DeepDream network
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个DeepDream网络
- en: Another interesting application of CNNs is DeepDream, a computer vision program
    created by Google [8] that uses a CNN to find and enhance patterns in images.
    The result is a dream-like hallucinogenic effect. Similar to the previous example,
    we are going to use a pretrained network to extract features. However, in this
    case, we want to “enhance” patterns in images, meaning that we need to maximize
    some functions. This tells us that we need to use a gradient ascent and not a
    descent. First, let’s see an example from Google gallery (available at [https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb))
    where the classic Seattle landscape is “incepted” with hallucinogenic dreams such
    as birds, cards, and strange flying objects.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的另一个有趣应用是DeepDream，一个由Google [8] 创建的计算机视觉程序，它利用CNN在图像中寻找并增强模式。结果是梦幻般的迷幻效果。与之前的示例类似，我们将使用一个预训练的网络来提取特征。然而，在这种情况下，我们希望“增强”图像中的模式，这意味着我们需要最大化一些函数。这告诉我们需要使用梯度上升，而不是梯度下降。首先，让我们看一个来自Google画廊的示例（可在[https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb)获取），其中经典的西雅图景观被“接纳”了梦幻般的幻觉效果，如鸟类、卡片和奇怪的飞行物体。
- en: 'Google released the DeepDream code as open source (available at [https://github.com/google/deepdream](https://github.com/google/deepdream)),
    but we will use a simplified example made by a random forest (available at [https://www.tensorflow.org/tutorials/generative/deepdream](https://www.tensorflow.org/tutorials/generative/deepdream)):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Google发布了DeepDream的开源代码（可在[https://github.com/google/deepdream](https://github.com/google/deepdream)获取），但我们将使用一个由随机森林生成的简化示例（可在[https://www.tensorflow.org/tutorials/generative/deepdream](https://www.tensorflow.org/tutorials/generative/deepdream)获取）：
- en: '![](img/B18331_20_11.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_11.png)'
- en: 'Figure 20.11: DeepDreaming Seattle'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.11：深度梦境：西雅图
- en: 'Let’s start with some image preprocessing:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一些图像预处理开始：
- en: '[PRE4]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now let’s use the Inception pretrained network to extract features. We use
    several layers, and the goal is to maximize their activations. The `tf.keras`
    functional API is our friend here:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用预训练的Inception网络来提取特征。我们使用多个层，目标是最大化它们的激活值。`tf.keras`函数式API在这里对我们非常有用：
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The loss function is the mean of all the activation layers considered, normalized
    by the number of units in the layer itself:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数是所有激活层的平均值，通过该层自身单元的数量进行归一化：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let’s run the gradient ascent:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们运行梯度上升：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This transforms the image on the left into the psychedelic image on the right:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把左侧的图像转换成右侧的迷幻图像：
- en: '![](img/B18331_20_12.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_12.png)'
- en: 'Figure 20.12: DeepDreaming of a green field with clouds'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.12：深度梦境：绿地与云彩
- en: Inspecting what a network has learned
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查网络学到了什么
- en: 'A particularly interesting research effort is being devoted to understand what
    neural networks are actually learning in order to be able to recognize images
    so well. This is called neural network “interpretability.” Activation atlases
    is a promising recent technique that aims to show the feature visualizations of
    averaged activation functions. In this way, activation atlases produce a global
    map seen through the eyes of the network. Let’s look at a demo available at [https://distill.pub/2019/activation-atlas/](https://distill.pub/2019/activation-atlas/):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特别有趣的研究方向是了解神经网络到底在学习什么，从而能够如此精准地识别图像。这被称为神经网络的“可解释性”。激活图谱是一种有前景的近期技术，旨在展示平均激活函数的特征可视化。通过这种方式，激活图谱生成了通过网络“眼睛”看到的全球地图。让我们来看一个可用的演示：[https://distill.pub/2019/activation-atlas/](https://distill.pub/2019/activation-atlas/)：
- en: '![](img/B18331_20_13.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_13.png)'
- en: 'Figure 20.13: Examples of inspections'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.13：检查示例
- en: In this image, an InceptionV1 network used for vision classification reveals
    many fully realized features, such as electronics, screens, a Polaroid camera,
    buildings, food, animal ears, plants, and watery backgrounds. Note that grid cells
    are labeled with the classification they give the most support for. Grid cells
    are also sized according to the number of activations that are averaged within.
    This representation is very powerful because it allows us to inspect the different
    layers of a network and how the activation functions fire in response to the input.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图片中，使用用于视觉分类的InceptionV1网络展示了许多完全实现的特征，例如电子产品、屏幕、宝丽来相机、建筑物、食物、动物耳朵、植物和水域背景。请注意，网格单元标注了它们给出最多支持的分类。网格单元的大小也根据其中平均激活的次数进行调整。这种表示方法非常强大，因为它允许我们检查网络的不同层以及激活函数如何响应输入进行激活。
- en: In this section, we have seen many techniques to process images with CNNs. Next,
    we’ll move on to video processing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经看到了许多用CNN处理图像的技术。接下来，我们将转向视频处理。
- en: Video
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频
- en: In this section, we are going to discuss how to use CNNs with videos and the
    different techniques that we can use.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何将CNN与视频结合使用，以及我们可以使用的不同技术。
- en: Classifying videos with pretrained nets in six different ways
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用预训练网络以六种不同方式分类视频
- en: 'Classifying videos is an area of active research because of the large amount
    of data needed for processing this type of media. Memory requirements are frequently
    reaching the limits of modern GPUs and a distributed form of training on multiple
    machines might be required. Researchers are currently exploring different directions
    of investigation, with increasing levels of complexity from the first approach
    to the sixth, as described below. Let’s review them:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 视频分类是一个活跃的研究领域，因为处理这种类型的媒体需要大量的数据。内存需求通常会达到现代GPU的极限，可能需要在多台机器上进行分布式训练。目前，研究人员正在探索不同的研究方向，从第一种方法到第六种方法的复杂度逐步增加，具体如下所述。让我们来回顾一下：
- en: The **first approach** consists of classifying one video frame at a time by
    considering each one of them as a separate image processed with a 2D CNN. This
    approach simply reduces the video classification problem to an image classification
    problem. Each video frame “emits” a classification output, and the video is classified
    by taking into account the more frequently chosen category for each frame.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第一种方法**是逐帧对视频进行分类，将每一帧视为一个单独的图像，并用2D卷积神经网络（CNN）处理。这种方法简单地将视频分类问题简化为图像分类问题。每一帧视频“发出”一个分类输出，视频的分类通过考虑每一帧最常选择的类别来确定。'
- en: The **second approach** consists of creating one single network where a 2D CNN
    is combined with an RNN (see *Chapter 9*, *Generative Models*). The idea is that
    the CNN will take into account the image components and the RNN will take into
    account the sequence information for each video. This type of network can be very
    difficult to train because of the very high number of parameters to optimize.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二种方法**是创建一个单一的网络，将2D卷积神经网络（CNN）与循环神经网络（RNN）结合起来（参见*第9章*，*生成模型*）。其思想是，CNN将考虑图像的组成部分，而RNN则考虑每个视频的序列信息。这种类型的网络可能非常难以训练，因为它有大量需要优化的参数。'
- en: The **third approach** is to use a 3D ConvNet, where 3D ConvNets are an extension
    of 2D ConvNets operating on a 3D tensor (time, image width, and image height).
    This approach is another natural extension of image classification. Again, 3D
    ConvNets can be hard to train.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第三种方法**是使用3D卷积网络（3D ConvNet），其中3D卷积网络是2D卷积网络的扩展，操作于3D张量（时间、图像宽度和图像高度）。这种方法是图像分类的另一种自然扩展。同样，3D卷积网络也可能很难训练。'
- en: 'The **fourth approach** is based on a clever idea: instead of using CNNs directly
    for classification, they can be used for storing offline features for each frame
    in the video. The idea is that feature extraction can be made very efficient with
    transfer learning, as shown in a previous recipe. After all the features are extracted,
    they can be passed as a set of inputs into an RNN, which will learn sequences
    across multiple frames and emit the final classification.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第四种方法**基于一个巧妙的想法：不是直接使用CNN进行分类，而是将它们用于存储每一帧视频的离线特征。其思想是，特征提取可以通过迁移学习变得非常高效，如之前的食谱所示。在提取所有特征后，它们可以作为输入集传递给RNN，RNN将学习跨多个帧的序列并输出最终分类。'
- en: The **fifth approach** is a simple variant of the fourth, where the final layer
    is an MLP instead of an RNN. In certain situations, this approach can be simpler
    and less expensive in terms of computational requirements.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第五种方法**是第四种方法的一种简单变体，其中最后一层是MLP，而不是RNN。在某些情况下，这种方法可能更简单，且在计算需求上更低。'
- en: The **sixth approach** is a variant of the fourth, where the phase of feature
    extraction is realized with a 3D CNN that extracts spatial and visual features.
    These features are then passed into either an RNN or an MLP.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第六种方法**是第四种方法的一种变体，其中特征提取阶段是通过一个3D卷积神经网络（CNN）来实现的，该网络提取空间和视觉特征。这些特征随后传递给一个RNN或MLP。'
- en: Deciding upon the best approach is strictly dependent on your specific application,
    and there is no definitive answer. The first three approaches are generally more
    computationally expensive and less clever, while the last three approaches are
    less expensive, and they frequently achieve better performance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 确定最佳方法完全取决于你的具体应用，并没有明确的答案。前三种方法通常计算开销较大且较为笨重，而后三种方法则开销较小，且经常能够取得更好的性能。
- en: So far, we have explored how CNNs can be used for image and video applications.
    In the next section, we will apply these ideas within a text-based context.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了CNN如何用于图像和视频应用。接下来的部分，我们将把这些思想应用于基于文本的上下文中。
- en: Text documents
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本文档
- en: What do text and images have in common? At first glance, very little. However,
    if we represent a sentence or a document as a matrix, then this matrix is not
    much different from an image matrix where each cell is a pixel. So, the next question
    is, how can we represent a piece of text as a matrix?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 文本和图像有什么共同点？乍一看，似乎没有什么共同点。然而，如果我们将一个句子或文档表示为一个矩阵，那么这个矩阵与图像矩阵没有太大区别，因为每个单元格就像图像中的一个像素。那么，下一个问题是，我们如何将一段文本表示为一个矩阵呢？
- en: 'Well, it is pretty simple: each row of a matrix is a vector that represents
    a basic unit for the text. Of course, now we need to define what a basic unit
    is. A simple choice could be to say that the basic unit is a character. Another
    choice would be to say that a basic unit is a word; yet another choice is to aggregate
    similar words together and then denote each aggregation (sometimes called cluster
    or embedding) with a representative symbol.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 其实很简单：矩阵的每一行是一个表示文本基本单元的向量。当然，现在我们需要定义什么是基本单元。一个简单的选择是将基本单元定义为一个字符。另一个选择是将基本单元定义为一个单词；还有一种选择是将相似的单词聚合在一起，然后用一个代表性的符号来表示每个聚合（有时称为簇或嵌入）。
- en: Note that regardless of the specific choice adopted for our basic units, we
    need to have a 1:1 mapping from basic units into integer IDs so that the text
    can be seen as a matrix. For instance, if we have a document with 10 lines of
    text and each line is a 100-dimensional embedding, then we will represent our
    text with a matrix of 10 x 100\. In this very particular “image,” a “pixel” is
    turned on if that sentence, *X*, contains the embedding, represented by position
    *Y*. You might also notice that a text is not really a matrix but more a vector
    because two words located in adjacent rows of text have very little in common.
    Indeed, this is a major difference when compared with images, where two pixels
    located in adjacent columns are likely to have some degree of correlation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，无论我们为基本单元选择什么，必须保证从基本单元到整数ID的1:1映射，以便将文本视为矩阵。例如，如果我们有一个包含10行文本的文档，每行是一个100维的嵌入，那么我们将用一个10
    x 100的矩阵来表示文本。在这个非常特殊的“图像”中，只有当某个句子*X*包含位置*Y*表示的嵌入时，那个“像素”才会被点亮。你可能还会注意到，文本并不是真正的矩阵，更像是一个向量，因为位于相邻行的两个单词几乎没有什么关联。实际上，这与图像有很大的区别，因为图像中位于相邻列的两个像素可能会有某种程度的相关性。
- en: 'Now you might wonder: *I understand that we represent the text as a vector
    but, in doing so, we lose the position of the words. This position should be important,
    shouldn’t it?* Well, it turns out that in many real applications, knowing whether
    a sentence contains a particular basic unit (a char, a word, or an aggregate)
    or not is pretty useful information even if we don’t keep track of where exactly
    in the sentence this basic unit is located.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能会想：*我理解我们将文本表示为一个向量，但这样做的话，我们失去了单词的位置。这个位置应该很重要，不是吗？* 结果证明，在许多实际应用中，知道一个句子是否包含某个特定的基本单元（字符、单词或聚合）是非常有用的信息，即使我们没有追踪这个基本单元在句子中的确切位置。
- en: For instance, CNNs achieve pretty good results for **sentiment analysis**, where
    we need to understand if a piece of text has a positive or a negative sentiment;
    for **spam detection**, where we need to understand if a piece of text is useful
    information or spam; and for **topic categorization**, where we need to understand
    what a piece of text is all about. However, CNNs are not well suited for a **Part
    of Speech** (**POS**) analysis, where the goal is to understand what the logical
    role of every single word is (for example, a verb, an adverb, a subject, and so
    on). CNNs are also not well suited for **entity extraction**, where we need to
    understand where relevant entities are located in sentences.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，CNN在**情感分析**中取得了不错的结果，在情感分析中，我们需要理解一段文本是积极的还是消极的；在**垃圾邮件检测**中，我们需要判断一段文本是有用的信息还是垃圾邮件；在**主题分类**中，我们需要了解一段文本的主题是什么。然而，CNN并不适合**词性分析**（**POS**），在词性分析中，目标是理解每个单词的逻辑角色是什么（例如，动词、副词、主语等）。CNN也不太适合**实体提取**，在实体提取中，我们需要理解句子中相关实体的位置。
- en: Indeed, it turns out that a position is pretty useful information for the last
    two use cases. 1D ConvNets are very similar to 2D ConvNets. However, the former
    operates on a single vector, while the latter operates on matrices.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，事实证明，位置对于最后两个使用案例非常有用。1D卷积神经网络（ConvNets）与2D卷积神经网络非常相似。然而，前者操作的是单一向量，而后者操作的是矩阵。
- en: Using a CNN for sentiment analysis
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用CNN进行情感分析
- en: 'Let’s have a look at the code. First of all, we load the dataset with `tensorflow_datasets`.
    In this case we use IMDB, a collection of movie reviews:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下代码。首先，我们使用`tensorflow_datasets`加载数据集。在这个例子中，我们使用IMDB，它是一个电影评论的集合：
- en: '[PRE8]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then we build a suitable CNN model. We use embeddings (see *Chapter 4*, *Word
    Embeddings*) to map the sparse vocabulary typically observed in documents into
    a dense feature space of dimensions `dim_embedding`. Then we use `Conv1D`, followed
    by a `GlobalMaxPooling1D` for averaging, and two `Dense` layers – the last one
    has only one neuron firing binary choices (positive or negative reviews):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们构建一个合适的CNN模型。我们使用词嵌入（参见*第4章*，*词嵌入*）将文档中通常观察到的稀疏词汇映射到一个密集的特征空间，维度为`dim_embedding`。然后，我们使用`Conv1D`，接着是`GlobalMaxPooling1D`进行平均，再加上两个`Dense`层——最后一个只有一个神经元，用于输出二元选择（正面或负面评论）：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The model has more than 2,700,000 parameters, and it is summarized as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有超过2,700,000个参数，概述如下：
- en: '[PRE10]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we compile and fit the model with the Adam optimizer and binary cross-entropy
    loss:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用Adam优化器和二元交叉熵损失函数来编译并拟合模型：
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The final accuracy is 88.21%, showing that it is possible to successfully use
    CNNs for textual processing:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最终准确率为88.21%，这表明成功使用卷积神经网络（CNN）进行文本处理是可能的：
- en: '[PRE12]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that many other non-image applications can also be converted to an image
    and classified using CNNs (see, for instance, [https://becominghuman.ai/sound-classification-using-images-68d4770df426](https://becominghuman.ai/sound-classification-using-images-68d4770df426)).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，许多其他非图像应用也可以转换为图像并使用CNN进行分类（例如，参见[https://becominghuman.ai/sound-classification-using-images-68d4770df426](https://becominghuman.ai/sound-classification-using-images-68d4770df426)）。
- en: Audio and music
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 音频与音乐
- en: We have used CNNs for images, videos, and texts. Now let’s have a look at how
    variants of CNNs can be used for audio.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将CNN应用于图像、视频和文本。现在，让我们来看看CNN的变种如何应用于音频。
- en: So, you might wonder why learning to synthesize audio is so difficult. Well,
    each digital sound we hear is based on 16,000 samples per second (sometimes 48K
    or more), and building a predictive model where we learn to reproduce a sample
    based on all the previous ones is a very difficult challenge.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你可能会想，为什么学习合成音频这么困难。嗯，我们听到的每个数字声音都基于每秒16,000个样本（有时是48K或更多），并且构建一个预测模型，通过所有之前的样本来学习重现一个样本是一个非常困难的挑战。
- en: Dilated ConvNets, WaveNet, and NSynth
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩张卷积神经网络（Dilated ConvNets）、WaveNet和NSynth
- en: WaveNet is a deep generative model for producing raw audio waveforms. This breakthrough
    technology was introduced (available at [https://deepmind.com/blog/wavenet-a-generative-model-for-raw-audio/](https://deepmind.com/blog/wavenet-a-generative-model-for-raw-audio/))
    by Google DeepMind for teaching computers how to speak. The results are truly
    impressive, and online you can find examples of synthetic voices where the computer
    learns how to talk with the voice of celebrities such as Matt Damon. There are
    experiments showing that WaveNet improved the current state-of-the-art **Text-to-Speech**
    (**TTS**) systems, reducing the difference with respect to human voices by 50%
    for both US English and Mandarin Chinese. The metric used for comparison is called
    **Mean Opinion Score** (**MOS**), a subjective paired comparison test. In the
    MOS tests, after listening to each sound stimulus, the subjects were asked to
    rate the naturalness of the stimulus on a five-point scale from “Bad” (1) to “Excellent”
    (5).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: WaveNet是一种用于生成原始音频波形的深度生成模型。这项突破性的技术由Google DeepMind提出（可在[https://deepmind.com/blog/wavenet-a-generative-model-for-raw-audio/](https://deepmind.com/blog/wavenet-a-generative-model-for-raw-audio/)查看），用于教计算机如何说话。其结果非常令人印象深刻，网上可以找到一些合成语音的示例，其中计算机学会了用像马特·达蒙等名人的声音进行对话。有实验表明，WaveNet提高了现有的**文本转语音**（**TTS**）系统，将与人类语音的差异减少了50%，适用于美国英语和普通话。这一比较使用的度量标准叫做**平均意见分数**（**MOS**），是一种主观的配对比较测试。在MOS测试中，听完每个声音刺激后，受试者被要求对刺激的自然度进行五分制评分，从“差”（1分）到“优秀”（5分）。
- en: What is even cooler is that DeepMind demonstrated that WaveNet can be also used
    to teach computers how to generate the sound of musical instruments such as piano
    music.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 更酷的是，DeepMind展示了WaveNet还可以用于教计算机如何生成像钢琴音乐这样的乐器声音。
- en: 'Now some definitions. TTS systems are typically divided into two different
    classes: concatenative and parametric.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一些定义。TTS系统通常分为两类：拼接式和参数化式。
- en: Concatenative TTS is where single speech voice fragments are first memorized
    and then recombined when the voice has to be reproduced. However, this approach
    does not scale because it is possible to reproduce only the memorized voice fragments,
    and it is not possible to reproduce new speakers or different types of audio without
    memorizing the fragments from the beginning.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 拼接式TTS是将单个语音片段先存储，然后在需要复现语音时重新组合。然而，这种方法无法扩展，因为只能重现已存储的语音片段，无法在没有重新记忆片段的情况下复现新的说话者或不同类型的音频。
- en: Parametric TTS is where a model is created to store all the characteristic features
    of the audio to be synthesized. Before WaveNet, the audio generated with parametric
    TTS was less natural than concatenative TTS. WaveNet enabled significant improvement
    by modeling directly the production of audio sounds, instead of using intermediate
    signal processing algorithms as in the past.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化TTS是指创建一个模型，用来存储所有要合成音频的特征。在WaveNet之前，使用参数化TTS生成的音频不如拼接式TTS自然。WaveNet通过直接建模音频声音的生成，显著改善了这一点，而不是像过去那样使用中间信号处理算法。
- en: In principle, WaveNet can be seen as a stack of 1D convolutional layers with
    a constant stride of one and with no pooling layers. Note that the input and the
    output have by construction the same dimension, so ConvNets are well suited to
    modeling sequential data such as audio sounds. However, it has been shown that
    in order to reach a large size for the receptive field in the output neuron, it
    is necessary to either use a massive number of large filters or increase the network
    depth prohibitively. Remember that the receptive field of a neuron in a layer
    is the cross-section of the previous layer from which neurons provide inputs.
    For this reason, pure ConvNets are not so effective in learning how to synthesize
    audio.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，WaveNet可以视为一堆具有恒定步幅为1且没有池化层的1D卷积层。注意，输入和输出在结构上具有相同的维度，因此卷积神经网络（ConvNets）非常适合建模像音频声音这样的序列数据。然而，研究表明，为了在输出神经元中达到较大的感受野，需要使用大量的大型滤波器或以不可接受的方式增加网络深度。记住，层中神经元的感受野是前一层的交叉部分，神经元从中接收输入。因此，纯粹的卷积神经网络在学习如何合成音频方面并不那么有效。
- en: 'The key intuition behind WaveNet is the so-called **Dilated Causal Convolutions**
    [5] (sometimes called **atrous convolution**), which simply means that some input
    values are skipped when the filter of a convolutional layer is applied. “Atrous”
    is a “bastardization” of the French expression “à trous,” meaning “with holes.”
    So an atrous convolution is a convolution with holes. As an example, in one dimension,
    a filter *w* of size 3 with a dilation of 1 would compute the following sum: *w*[0]
    *x*[0] + *w*[1] *x*[2] + *w*[3] *x*[4].'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: WaveNet的关键直觉是所谓的**扩张因果卷积**[5]（有时也称为**空洞卷积**），这意味着在应用卷积层的滤波器时，一些输入值被跳过。“Atrous”是法语表达式“à
    trous”的变形，意思是“带孔的”。因此，空洞卷积就是带孔的卷积。例如，在一维情况下，一个大小为3、扩张为1的滤波器*w*将计算以下和：*w*[0] *x*[0]
    + *w*[1] *x*[2] + *w*[2] *x*[4]。
- en: 'In short, in D-dilated convolution, usually the stride is 1, but nothing prevents
    you from using other strides. An example is given in *Figure 20.14* with increased
    dilatation (hole) sizes = 0, 1, 2:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在D-扩张卷积中，通常步幅为1，但没有什么可以阻止你使用其他步幅。一个例子见*图 20.14*，展示了扩张（空洞）大小分别为0、1、2时的情况：
- en: '![](img/B18331_20_14.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_14.png)'
- en: 'Figure 20.14: Dilatation with increased sizes'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.14：扩张与增大尺寸
- en: Thanks to this simple idea of introducing *holes*, it is possible to stack multiple
    dilated convolutional layers with exponentially increasing filters and learn long-range
    input dependencies without having an excessively deep network.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于引入*空洞*的这个简单思路，能够堆叠多个扩张卷积层，通过指数增长的滤波器学习长程输入依赖关系，而不需要一个过于深的网络。
- en: A WaveNet is therefore a ConvNet where the convolutional layers have various
    dilation factors, allowing the receptive field to grow exponentially with depth
    and therefore efficiently cover thousands of audio timesteps.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，WaveNet是一个卷积神经网络，其中卷积层具有不同的扩张因子，从而使感受野随着深度的增加而指数增长，进而有效地覆盖成千上万个音频时间步长。
- en: When we train, the inputs are sounds recorded from human speakers. The waveforms
    are quantized to a fixed integer range. A WaveNet defines an initial convolutional
    layer accessing only the current and previous input. Then, there is a stack of
    dilated ConvNet layers, still accessing only current and previous inputs. At the
    end, there is a series of dense layers combining previous results, followed by
    a softmax activation function for categorical outputs.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时，输入是来自人类发音者的录音。波形被量化为固定的整数范围。WaveNet定义了一个初始卷积层，仅访问当前和先前的输入。然后是一个扩张卷积神经网络层的堆叠，依旧只访问当前和先前的输入。最后是一个密集层序列，结合先前的结果，接着是一个用于分类输出的softmax激活函数。
- en: 'At each step, a value is predicted from the network and fed back into the input.
    At the same time, a new prediction for the next step is computed. The loss function
    is the cross-entropy between the output for the current step and the input at
    the next step. *Figure 20.15* shows the visualization of a WaveNet stack and its
    receptive field as introduced in Aaron van den Oord [9]. Note that generation
    can be slow because the waveform has to be synthesized in a sequential fashion,
    as *x*[t] must be sampled first in order to obtain ![](img/B18331_20_004.png)
    where *x* is the input:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤中，从网络中预测一个值并将其反馈到输入中。同时，计算下一步的预测值。损失函数是当前步骤的输出和下一步输入之间的交叉熵。*图 20.15* 展示了Aaron
    van den Oord [9]介绍的WaveNet堆叠及其感受野的可视化。请注意，生成过程可能较慢，因为波形需要按顺序合成，因为* x*[t]必须首先被采样，以便获得![](img/B18331_20_004.png)，其中*x*是输入：
- en: '![A picture containing diagram  Description automatically generated](img/B18331_20_15.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing diagram  Description automatically generated](img/B18331_20_15.png)'
- en: 'Figure 20.15: WaveNet internal connections'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.15：WaveNet内部连接
- en: 'A method for performing a sampling in parallel has been proposed in Parallel
    WaveNet [10], which achieves a three orders-of-magnitude speedup. This uses two
    networks as a WaveNet teacher network, which is slow but ensures a correct result,
    and a WaveNet student network, which tries to mimic the behavior of the teacher;
    this can prove to be less accurate but is faster. This approach is similar to
    the one used for GANs (see *Chapter 9*, *Generative Models*) but the student does
    not try to fool the teacher, as typically happens in GANs. In fact, the model
    is not just quicker but also of higher fidelity, capable of creating waveforms
    with 24,000 samples per second:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在《Parallel WaveNet [10]》中，提出了一种并行采样的方法，实现了三个数量级的加速。该方法使用两个网络作为 WaveNet 教师网络，一个较慢但能确保正确结果的教师网络和一个试图模仿教师行为的
    WaveNet 学生网络；学生网络可能精度较低，但速度更快。这种方法类似于 GAN（见*第九章*，*生成模型*）中使用的方法，但学生并不试图欺骗教师，正如通常在
    GAN 中发生的那样。实际上，该模型不仅速度更快，而且精度更高，能够生成每秒 24,000 个采样的波形：
- en: '![Diagram, shape  Description automatically generated](img/B18331_20_16.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图示，形状 描述自动生成](img/B18331_20_16.png)'
- en: 'Figure 20.16: Examples of WaveNet Student and Teacher'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.16：WaveNet 学生与教师的示例
- en: This model has been deployed in production at Google, and is currently being
    used to serve Google Assistant queries in real time to millions of users. At the
    annual I/O developer conference in May 2018, it was announced that new Google
    Assistant voices were available thanks to WaveNet.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型已经在 Google 的生产环境中部署，并且目前正在实时为数百万用户提供 Google Assistant 查询。在 2018 年 5 月的年度
    I/O 开发者大会上，宣布由于 WaveNet，新版的 Google Assistant 语音已经上线。
- en: Two implementations of WaveNet models for TensorFlow are currently available.
    One is the original implementation of DeepMind’s WaveNet, and the other is called
    Magenta NSynth. The original WaveNet version is available at [https://github.com/ibab/tensorflow-wavenet](https://github.com/ibab/tensorflow-wavenet).
    NSynth is an evolution of WaveNet recently released by the Google Brain group,
    which, instead of being causal, aims at seeing the entire context of the input
    chunk. Magenta is available at [https://magenta.tensorflow.org/nsynth](https://magenta.tensorflow.org/nsynth).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有两个 TensorFlow 中的 WaveNet 模型实现。一个是 DeepMind 的原始 WaveNet 实现，另一个叫做 Magenta NSynth。原始
    WaveNet 版本可以在 [https://github.com/ibab/tensorflow-wavenet](https://github.com/ibab/tensorflow-wavenet)
    上获取。NSynth 是 WaveNet 的一种进化版，最近由 Google Brain 团队发布，它不同于传统的因果模型，而是旨在看到输入块的整个上下文。Magenta
    可在 [https://magenta.tensorflow.org/nsynth](https://magenta.tensorflow.org/nsynth)
    上获取。
- en: 'The neural network is truly complex, as depicted in the image below, but for
    the sake of this introductory discussion, it is sufficient to know that the network
    learns how to reproduce its input by using an approach based on reducing the error
    during the encoding/decoding phases:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络确实很复杂，如下图所示，但为了本次入门讨论的方便，只需知道该网络通过减少编码/解码阶段的误差来学习如何再现其输入：
- en: '![NSynth_blog_figs_WaveNetAE_diagram.png](img/B18331_20_17.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![NSynth_blog_figs_WaveNetAE_diagram.png](img/B18331_20_17.png)'
- en: 'Figure 20.17: Magenta internal architecture'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.17：Magenta 内部架构
- en: If you are interested in understanding more, I would suggest having a look at
    the online Colab notebook where you can play with models generated with NSynth.
    NSynth Colab is available at [https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb](https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有兴趣了解更多内容，建议查看在线 Colab 笔记本，你可以在其中使用 NSynth 生成的模型。NSynth Colab 可在 [https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb](https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb)
    上访问。
- en: MuseNet is a very recent and impressive cool audio generation tool developed
    by OpenAI. MuseNet uses a sparse transformer to train a 72-layer network with
    24 attention heads. MuseNet is available at [https://openai.com/blog/musenet/](https://openai.com/blog/musenet/).
    Transformers, discussed in *Chapter 6*, are very good at predicting what comes
    next in a sequence – whether text, images, or sound.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: MuseNet 是 OpenAI 开发的一个非常新的、令人印象深刻的音频生成工具。MuseNet 使用稀疏变换器训练一个具有 72 层和 24 个注意力头的网络。MuseNet
    可在 [https://openai.com/blog/musenet/](https://openai.com/blog/musenet/) 上访问。*第六章*讨论的变换器非常擅长预测序列中的下一个元素——无论是文本、图像还是声音。
- en: 'In transformers, every output element is connected to every input element,
    and the weightings between them are dynamically calculated according to a process
    called attention. MuseNet can produce up to 4-minute musical compositions with
    10 different instruments, and can combine styles from country, to Mozart, to the
    Beatles. For instance, I generated a remake of Beethoven’s “Für Elise” in the
    style of Lady Gaga with piano, drums, guitar, and bass. You can try this for yourself
    at the link provided under the section **Try MuseNet**:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在变压器中，每个输出元素都与每个输入元素连接，它们之间的权重是根据一个叫做注意力的过程动态计算的。MuseNet可以生成最多4分钟的音乐作品，包含10种不同的乐器，并能将乡村、莫扎特、披头士等风格相结合。例如，我生成了一首贝多芬《致爱丽丝》的翻版，以Lady
    Gaga风格演绎，使用了钢琴、鼓、吉他和贝斯。你可以通过**试用MuseNet**部分下提供的链接亲自尝试：
- en: '![Graphical user interface  Description automatically generated](img/B18331_20_18.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面  描述自动生成](img/B18331_20_18.png)'
- en: 'Figure 20.18: An example of using MuseNet'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.18：使用MuseNet的示例
- en: A summary of convolution operations
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积操作总结
- en: In this section, we present a summary of different convolution operations. A
    convolutional layer has *I* input channels and produces *O* output channels. *I*
    x *O* x *K* parameters are used, where *K* is the number of values in the kernel.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们总结了不同卷积操作。卷积层有*I*个输入通道，并产生*O*个输出通道。使用了*I* x *O* x *K*个参数，其中*K*是核中值的数量。
- en: Basic CNNs
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本CNN
- en: Let’s remind ourselves briefly what a CNN is. CNNs take in an input image (two
    dimensions), text (two dimensions), or video (three dimensions) and apply multiple
    filters to the input. Each filter is like a flashlight sliding across the areas
    of the input, and the areas that it is shining over are called the receptive field.
    Each filter is a tensor of the same depth of the input (for instance, if the image
    has a depth of three, then the filter must also have a depth of three).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要回顾一下什么是CNN。CNN输入的是图像（二维）、文本（二维）或视频（三维），并对输入应用多个滤波器。每个滤波器就像一盏手电筒，滑过输入的区域，它照射到的区域叫做感受野。每个滤波器是与输入深度相同的张量（例如，如果图像深度为三，则滤波器的深度也必须为三）。
- en: When the filter is sliding, or convolving, around the input image, the values
    in the filter are multiplied by the values of the input. The multiplications are
    then summarized into one single value. This process is repeated for each location,
    producing an activation map (a.k.a. a feature map). Of course, it is possible
    to use multiple filters where each filter will act as a feature identifier. For
    instance, for images, the filter can identify edges, colors, lines, and curves.
    The key intuition is to treat the filter values as weights and fine-tune them
    during training via backpropagation.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当滤波器滑动或卷积输入图像时，滤波器中的值会与输入的值相乘。然后，将乘积汇总成一个单一的值。这个过程会对每个位置重复，产生一个激活图（也叫特征图）。当然，也可以使用多个滤波器，每个滤波器作为特征识别器。例如，对于图像，滤波器可以识别边缘、颜色、线条和曲线。关键的直觉是将滤波器的值视为权重，并在训练过程中通过反向传播进行微调。
- en: 'A convolution layer can be configured by using the following config parameters:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层可以通过以下配置参数进行配置：
- en: '**Kernel size**: This is the field of view of the convolution.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核大小**：这是卷积的视野。'
- en: '**Stride**: This is the step size of the kernel when we traverse the image.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步幅**：这是核在遍历图像时的步长。'
- en: '**Padding**: Defines how the border of our sample is handled.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**填充**：定义了我们如何处理样本的边界。'
- en: Dilated convolution
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 空洞卷积
- en: 'Dilated convolutions (or atrous convolutions) introduce another config parameter:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 空洞卷积（或称为Atrous卷积）引入了另一个配置参数：
- en: '**Dilation rate**: This is the spacing between the values in a kernel.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**空洞率**：这是核中值之间的间隔。'
- en: Dilated convolutions are used in many contexts including audio processing with
    WaveNet.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 空洞卷积被广泛应用于多个场景，包括使用WaveNet进行音频处理。
- en: Transposed convolution
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转置卷积
- en: Transposed convolution is a transformation going in the opposite direction of
    a normal convolution. For instance, this can be useful to project feature maps
    into a higher dimensional space or for building convolutional autoencoders (see
    *Chapter 8*, *Autoencoders*). One way to think about transposed convolution is
    to compute the output shape of a normal CNN for a given input shape first. Then
    we invert input and output shapes with the transposed convolution. TensorFlow
    2.0 supports transposed convolutions with Conv2DTranspose layers, which can be
    used, for instance, in GANs (see *Chapter 9*, *Generative Models*) for generating
    images.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积是一种与正常卷积方向相反的变换。例如，这在将特征图投射到更高维空间时非常有用，或者用于构建卷积自编码器（见*第8章*，*自编码器*）。理解转置卷积的一种方式是，首先计算给定输入形状的正常CNN输出形状。然后，我们用转置卷积反转输入和输出形状。TensorFlow
    2.0支持转置卷积，通过Conv2DTranspose层可以使用它，例如，在生成对抗网络（GANs）（见*第9章*，*生成模型*）中生成图像。
- en: Separable convolution
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可分离卷积
- en: Separable convolution aims at separating the kernel in multiple steps. Let the
    convolution be *y* = *conv*(*x*, *k*) where *y* is the output, *x* is the input,
    and *k* is the kernel. Let’s assume the kernel is separable, *k* = *k*1.*k*2 where
    . is the dot product – in this case, instead of doing a 2-dimension convolution
    with *k*, we can get to the same result by doing two 1-dimension convolutions
    with *k*1 and *k*2\. Separable convolutions are frequently used to save on computation
    resources.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 可分离卷积旨在将卷积核分成多个步骤。设卷积为 *y* = *conv*(*x*, *k*)，其中 *y* 是输出，*x* 是输入，*k* 是卷积核。假设卷积核是可分离的，*k*
    = *k*1.*k*2，其中“.”表示点积—在这种情况下，我们可以通过分别使用 *k*1 和 *k*2 做两个一维卷积来得到与 *k* 做二维卷积相同的结果。可分离卷积通常用于节省计算资源。
- en: Depthwise convolution
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度卷积
- en: Let’s consider an image with multiple channels. In the normal 2D convolution,
    the filter is as deep as the input, and it allows us to mix channels for generating
    each element of the output. In depthwise convolutions, each channel is kept separate,
    the filter is split into channels, each convolution is applied separately, and
    the results are stacked back together into one tensor.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个包含多个通道的图像。在正常的二维卷积中，滤波器的深度与输入相同，并且它允许我们混合通道来生成输出的每个元素。在深度卷积中，每个通道是分开处理的，滤波器被分割为多个通道，每个卷积分别应用，结果再重新堆叠成一个张量。
- en: Depthwise separable convolution
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度可分离卷积
- en: 'This convolution should not be confused with the separable convolution. After
    completing the depthwise convolution, an additional step is performed: a 1x1 convolution
    across channels. Depthwise separable convolutions are used in Xception. They are
    also used in MobileNet, a model particularly useful for mobile and embedded vision
    applications because of its reduced model size and complexity.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这个卷积不应与可分离卷积混淆。在完成深度卷积后，会执行一个额外的步骤：跨通道进行1x1卷积。深度可分离卷积在Xception中得到了应用。它们也用于MobileNet，这是一种特别适用于移动和嵌入式视觉应用的模型，因为它的模型尺寸和复杂度都较小。
- en: In this section, we have discussed all the major forms of convolution. The next
    section will discuss capsule networks, a new form of learning introduced in 2017.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经讨论了所有主要的卷积形式。下一节将讨论胶囊网络，这是一种在2017年提出的新型学习方法。
- en: Capsule networks
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 胶囊网络
- en: Capsule networks (or CapsNets) are a very recent and innovative type of deep
    learning network. This technique was introduced at the end of October 2017 in
    a seminal paper titled *Dynamic Routing Between Capsules* by Sara Sabour, Nicholas
    Frost, and Geoffrey Hinton ([https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829))
    [14]. Hinton is the father of deep learning and, therefore, the whole deep learning
    community is excited to see the progress made with Capsules. Indeed, CapsNets
    are already beating the best CNN on MNIST classification, which is... well, impressive!!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 胶囊网络（或CapsNets）是近年来一种非常创新的深度学习网络类型。该技术在2017年10月底由Sara Sabour、Nicholas Frost和Geoffrey
    Hinton提出，并发布在题为《胶囊之间的动态路由》（*Dynamic Routing Between Capsules*）的开创性论文中（[https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829)）[14]。Hinton是深度学习的奠基人，因此整个深度学习社区都为胶囊网络的进展感到兴奋。事实上，CapsNets已经在MNIST分类任务中超越了最好的CNN，这真是……令人印象深刻！！
- en: What is the problem with CNNs?
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）有什么问题？
- en: In CNNs, each layer “understands” an image at a progressive level of granularity.
    As we discussed in multiple sections, the first layer will most likely recognize
    straight lines or simple curves and edges, while subsequent layers will start
    to understand more complex shapes such as rectangles up to complex forms such
    as human faces.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在CNN中，每一层“理解”图像的粒度逐渐增大。如我们在多个章节中讨论的那样，第一层最有可能识别直线、简单的曲线和边缘，而后续的层会开始理解更复杂的形状，如矩形，直到像人脸这样复杂的形式。
- en: 'Now, one critical operation used for CNNs is pooling. Pooling aims at creating
    positional invariance and it is used after each CNN layer to make any problem
    computationally tractable. However, pooling introduces a significant problem because
    it forces us to lose all the positional data. This is not good. Think about a
    face: it consists of two eyes, a mouth, and a nose, and what is important is that
    there is a spatial relationship between these parts (for example, the mouth is
    below the nose, which is typically below the eyes). Indeed, Hinton said: *The
    pooling operation used in convolutional neural networks is a big mistake and the
    fact that it works so well is a disaster*. Technically, we do not need positional
    invariance but instead we need equivariance. Equivariance is a fancy term for
    indicating that we want to understand the rotation or proportion change in an
    image, and we want to adapt the network accordingly. In this way, the spatial
    positioning among the different components in an image is not lost.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，卷积神经网络（CNN）中的一个关键操作是池化（pooling）。池化的目的是实现位置不变性，并且通常在每个CNN层之后使用，以便让任何问题在计算上变得可处理。然而，池化带来了一个显著的问题，因为它迫使我们丢失所有的位置信息。这是不可取的。想象一下一个面孔：它由两只眼睛、一张嘴和一个鼻子组成，重要的是这些部分之间有空间关系（例如，嘴巴在鼻子下方，而鼻子通常位于眼睛下方）。事实上，Hinton曾说过：*卷积神经网络中使用的池化操作是一个重大错误，而它之所以能如此有效，简直是一场灾难*。从技术上讲，我们并不需要位置不变性，而是需要等变性（equivariance）。等变性是一个专业术语，表示我们希望理解图像中的旋转或比例变化，并希望网络能够做出相应的调整。这样，图像中不同组件之间的空间关系就不会丢失。
- en: What is new with capsule networks?
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 胶囊网络有什么新特点？
- en: According to Hinton et al., our brain has modules called “capsules,” and each
    capsule is specialized in handling a particular type of information. In particular,
    there are capsules that work well for “understanding” the concept of position,
    the concept of size, the concept of orientation, the concept of deformation, textures,
    and so on. In addition to that, the authors suggest that our brain has particularly
    efficient mechanisms for dynamically routing each piece of information to the
    capsule that is considered best suited for handling a particular type of information.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Hinton等人的说法，我们的大脑有一些叫做“胶囊”的模块，每个胶囊都专门处理某种特定类型的信息。特别地，有些胶囊在“理解”位置概念、大小概念、方向概念、变形概念、纹理等方面表现得非常好。除此之外，作者还建议，我们的大脑拥有特别高效的机制，能够动态地将每个信息片段传递给最适合处理该类型信息的胶囊。
- en: So, the main difference between CNN and CapsNets is that with a CNN, we keep
    adding layers for creating a deep network, while with CapsNet, we nest a neural
    layer inside another. A capsule is a group of neurons that introduces more structure
    to a network, and it produces a vector to signal the existence of an entity in
    an image. In particular, Hinton uses the length of the activity vector to represent
    the probability that the entity exists and its orientation to represent the instantiation
    parameters. When multiple predictions agree, a higher-level capsule becomes active.
    For each possible parent, the capsule produces an additional prediction vector.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，CNN和胶囊网络（CapsNets）之间的主要区别在于，CNN通过不断添加层来创建深度网络，而胶囊网络则是在每一层内部嵌套神经层。一个胶囊是一个神经元群体，它为网络引入了更多结构，并生成一个向量来表示图像中某个实体的存在。具体来说，Hinton使用活动向量的长度来表示该实体存在的概率，而使用方向来表示实例化参数。当多个预测结果一致时，高层胶囊会被激活。对于每个可能的父胶囊，子胶囊会生成一个额外的预测向量。
- en: 'Now a second innovation comes in place: we will use dynamic routing across
    capsules and will no longer use the raw idea of pooling. A lower-level capsule
    prefers to send its output to higher-level capsules for which the activity vectors
    have a big scalar product, with the prediction coming from the lower-level capsule.
    The parent with the largest scalar prediction vector product increases the capsule
    bond. All the other parents decrease their bond. In other words, the idea is that
    if a higher-level capsule agrees with a lower-level one, then it will ask to send
    more information of that type. If there is no agreement, it will ask to send fewer
    of them. This dynamic routing by the agreement method is superior to the current
    mechanism like max pooling and, according to Hinton, routing is ultimately a way
    to parse the image. Indeed, max pooling is ignoring anything but the largest value,
    while dynamic routing selectively propagates information according to the agreement
    between lower layers and upper layers.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在出现了第二个创新：我们将在胶囊之间使用动态路由，不再使用传统的池化方法。低层胶囊更倾向于将其输出发送到与其活动向量有较大标量积的高层胶囊，预测来自低层胶囊。具有最大标量预测向量积的父胶囊将增加其胶囊联系。所有其他父胶囊将减少它们的联系。换句话说，这个想法是，如果高层胶囊同意低层胶囊的观点，它将请求发送更多这种类型的信息。如果没有达成一致，它将请求发送更少的信息。这种通过一致性方法进行的动态路由优于当前的机制，如最大池化，并且根据
    Hinton 的说法，路由最终是一种解析图像的方法。实际上，最大池化忽略了除最大值以外的所有信息，而动态路由则根据低层和高层之间的一致性选择性地传播信息。
- en: 'A third difference is that a new nonlinear activation function has been introduced.
    Instead of adding a squashing function to each layer as in CNN, CapsNet adds a
    squashing function to a nested set of layers. The nonlinear activation function
    is represented in *Equation 1*, and it is called the squashing function:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个不同之处是引入了新的非线性激活函数。与 CNN 中在每一层添加压缩函数不同，CapsNet 在一组嵌套的层次中添加了压缩函数。非线性激活函数在 *公式
    1* 中表示，称为压缩函数：
- en: '![](img/B18331_20_005.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18331_20_005.png)'
- en: where v[j] is the vector output of capsule *j* and s[j] is its total input.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 v[j] 是胶囊 *j* 的向量输出，s[j] 是其总输入。
- en: Moreover, Hinton and others show that a discriminatively trained, multi-layer
    capsule system achieves state-of-the-art performances on MNIST and is considerably
    better than a convolutional net at recognizing highly overlapping digits.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Hinton 等人表明，一个经过辨别训练的多层胶囊系统在 MNIST 上实现了最先进的性能，并且在识别高度重叠的数字方面显著优于卷积神经网络。
- en: 'Based on the paper *Dynamic Routing Between Capsules*, a simple CapsNet architecture
    looks as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 基于论文《胶囊间的动态路由》，一个简单的 CapsNet 架构如下所示：
- en: '![Screen Shot 2017-11-03 at 7.22.09 PM.png](img/B18331_20_19_new.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![Screen Shot 2017-11-03 at 7.22.09 PM.png](img/B18331_20_19_new.png)'
- en: 'Figure 20.19: An example of CapsNet'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.19：CapsNet 示例
- en: The architecture is shallow with only two convolutional layers and one fully
    connected layer. Conv1 has 256 9 x 9 convolution kernels with a stride of 1 and
    ReLU activation. The role of this layer is to convert pixel intensities to the
    activities of local feature detectors that are then used as inputs to the PrimaryCapsules
    layer. PrimaryCapsules is a convolutional capsule layer with 32 channels; each
    primary capsule contains 8 convolutional units with a 9 x 9 kernel and a stride
    of 2\. In total, PrimaryCapsules has [32, 6, 6] capsule outputs (each output is
    an 8D vector) and each capsule in the [6, 6] grid shares its weights with each
    other. The final layer (DigitCaps) has one 16D capsule per digit class and each
    one of these capsules receives input from all the other capsules in the layer
    below. Routing happens only between two consecutive capsule layers (for example,
    PrimaryCapsules and DigitCaps).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构较为浅层，仅包含两层卷积层和一层全连接层。Conv1 具有 256 个 9 x 9 的卷积核，步长为 1，并采用 ReLU 激活函数。该层的作用是将像素强度转换为局部特征检测器的活动，然后将这些活动作为输入传递到
    PrimaryCapsules 层。PrimaryCapsules 是一个具有 32 个通道的卷积胶囊层；每个主胶囊包含 8 个 9 x 9 的卷积单元，步长为
    2。总的来说，PrimaryCapsules 具有 [32, 6, 6] 的胶囊输出（每个输出为 8 维向量），且 [6, 6] 网格中的每个胶囊与其他胶囊共享权重。最终层（DigitCaps）为每个数字类别提供一个
    16 维的胶囊，每个胶囊接收来自下层所有其他胶囊的输入。路由仅发生在两个连续的胶囊层之间（例如，PrimaryCapsules 和 DigitCaps）。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have seen many applications of CNNs across very different
    domains, from traditional image processing and computer vision to close-enough
    video processing, not-so-close audio processing, and text processing. In just
    a few years, CNNs have taken machine learning by storm.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经看到CNN在不同领域中的许多应用，从传统的图像处理和计算机视觉，到接近的视频处理、相对较远的音频处理以及文本处理。在短短几年内，CNN已经席卷了机器学习领域。
- en: Nowadays, it is not uncommon to see multimodal processing, where text, images,
    audio, and videos are considered together to achieve better performance, frequently
    by means of combining CNNs together with a bunch of other techniques such as RNNs
    and reinforcement learning. Of course, there is much more to consider, and CNNs
    have recently been applied to many other domains such as genetic inference [13],
    which are, at least at first glance, far away from the original scope of their
    design.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，看到多模态处理已经不再罕见，其中文本、图像、音频和视频会被一同考虑，以实现更好的性能，通常通过将CNN与其他技术（如RNN和强化学习）结合来完成。当然，还有很多需要考虑的方面，CNN最近已被应用到许多其他领域，如基因推理[13]，这些领域至少从表面上看，与CNN的原始设计目标相距甚远。
- en: References
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Yosinski, J. and Clune, Y. B. J. *How transferable are features in deep neural
    networks*. Advances in Neural Information Processing Systems 27, pp. 3320–3328.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yosinski, J. 和 Clune, Y. B. J. *深度神经网络中的特征迁移性*。神经信息处理系统进展 27, 第3320–3328页。
- en: Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). *Rethinking
    the Inception Architecture for Computer Vision*. 2016 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 2818–2826.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., 和 Wojna, Z. (2016). *重新思考计算机视觉中的Inception架构*。2016年IEEE计算机视觉与模式识别会议（CVPR），第2818–2826页。
- en: 'Sandler, M., Howard, A., Zhu, M., Zhmonginov, A., and Chen, L. C. (2019). *MobileNetV2:
    Inverted Residuals and Linear Bottlenecks*. Google Inc.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Sandler, M., Howard, A., Zhu, M., Zhmonginov, A., 和 Chen, L. C. (2019). *MobileNetV2:
    反向残差和线性瓶颈*。Google Inc.'
- en: Krizhevsky, A., Sutskever, I., Hinton, G. E., (2012). *ImageNet classification
    with deep convolutional neural networks*.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Krizhevsky, A., Sutskever, I., Hinton, G. E., (2012). *使用深度卷积神经网络进行ImageNet分类*。
- en: Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (28 Jan 2018).
    *Densely Connected Convolutional Networks*. [http://arxiv.org/abs/1608.06993](http://arxiv.org/abs/1608.06993)
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Huang, G., Liu, Z., van der Maaten, L., 和 Weinberger, K. Q. (2018年1月28日). *密集连接卷积网络*。
    [http://arxiv.org/abs/1608.06993](http://arxiv.org/abs/1608.06993)
- en: 'Chollet, F. (2017). *Xception: Deep Learning with Depthwise Separable Convolutions*.
    [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Chollet, F. (2017). *Xception: 深度学习与深度可分离卷积*。 [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)'
- en: Gatys, L. A., Ecker, A. S., and Bethge, M. (2016). *A Neural Algorithm of Artistic
    Style*. [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gatys, L. A., Ecker, A. S., 和 Bethge, M. (2016). *艺术风格的神经算法*。 [https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576)
- en: Mordvintsev, A., Olah, C., and Tyka, M. ( 2015). *DeepDream - a code example
    for visualizing Neural Networks*. Google Research.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mordvintsev, A., Olah, C., 和 Tyka, M. (2015). *DeepDream - 可视化神经网络的代码示例*。Google研究。
- en: 'van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves,
    A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. (2016). *WaveNet: A generative
    model for raw audio*. arXiv preprint.'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves,
    A., Kalchbrenner, N., Senior, A., 和 Kavukcuoglu, K. (2016). *WaveNet: 一种原始音频的生成模型*。arXiv预印本。'
- en: 'van den Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu,
    K., van den Driessche, G., Lockhart, E., Cobo, L. C., Stimberg, F., Casagrande,
    N., Grewe, D., Noury, S., Dieleman, S., Elsen, E., Kalchbrenner, N., Zen, H.,
    Graves, A., King, H., Walters, T., Belov, D., and Hassabis, D. (2017). *Parallel
    WaveNet: Fast High-Fidelity Speech Synthesis*.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'van den Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu,
    K., van den Driessche, G., Lockhart, E., Cobo, L. C., Stimberg, F., Casagrande,
    N., Grewe, D., Noury, S., Dieleman, S., Elsen, E., Kalchbrenner, N., Zen, H.,
    Graves, A., King, H., Walters, T., Belov, D., 和 Hassabis, D. (2017). *Parallel
    WaveNet: 快速高保真语音合成*。'
- en: He, K., Gkioxari, G., Dollár, P., and Girshick, R. (2018). *Mask R-CNN*.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: He, K., Gkioxari, G., Dollár, P., 和 Girshick, R. (2018). *Mask R-CNN*。
- en: Chen, L-C., Zhu, Y., Papandreou, G., Schroff, F., and Adam, H. (2018). *Encoder-Decoder
    with Atrous Separable Convolution for Semantic Image Segmentation*.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Chen, L-C., Zhu, Y., Papandreou, G., Schroff, F., 和 Adam, H. (2018). *基于空洞可分离卷积的编码器-解码器结构用于语义图像分割*。
- en: Flagel, L., Brandvain, Y., and Schrider, D.R. (2018). *The Unreasonable Effectiveness
    of Convolutional Neural Networks in Population Genetic Inference*.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Flagel, L., Brandvain, Y., 和 Schrider, D.R. (2018). *卷积神经网络在群体遗传推断中的非凡有效性*。
- en: Sabour, S., Frosst, N., and Hinton, G. E. (2017). *Dynamic Routing Between Capsules*
    [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829)
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sabour, S., Frosst, N., 和 Hinton, G. E. (2017). *胶囊网络中的动态路由* [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829)
- en: Join our book’s Discord space
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，结识志同道合的人，与超过 2000 名成员一起学习，网址：[https://packt.link/keras](https://packt.link/keras)
- en: '![](img/QR_Code1831217224278819687.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1831217224278819687.png)'
