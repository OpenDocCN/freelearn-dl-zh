- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: Starting from AlexNet in 2012, which won the large-scale ImageNet competition,
    to the BERT pre-trained language model in 2018, which topped many **natural language
    processing** (**NLP**) leaderboards, the revolution of modern **deep learning**
    (**DL**) in the broader **artificial intelligence** (**AI**) and **machine learning**
    (**ML**) community continues. Yet, the challenges of moving these DL models from
    offline experimentation to a production environment remain. This is largely due
    to the complexity and lack of a unified open source framework for supporting the
    full life cycle development of DL. This book will help you understand the big
    picture of DL full life cycle development, and implement DL pipelines that can
    scale from a local offline experiment to a distributed environment and online
    production clouds, with an emphasis on hands-on project-based learning to support
    the end-to-end DL process using the popular open source MLflow framework.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从2012年AlexNet赢得大规模ImageNet竞赛，到2018年BERT预训练语言模型在多个**自然语言处理**（**NLP**）排行榜上名列前茅，现代**深度学习**（**DL**）在广泛的**人工智能**（**AI**）和**机器学习**（**ML**）社区中的革命持续进行。然而，将这些DL模型从离线实验转移到生产环境的挑战依然存在。这主要是因为缺乏一个统一的开源框架来支持DL完整生命周期的开发。本书将帮助你理解DL完整生命周期开发的全貌，并实现可从本地离线实验扩展到分布式环境和在线生产云的DL管道，重点是通过实践项目式学习，使用流行的开源MLflow框架支持DL过程的端到端实现。
- en: The book starts with an overview of the DL full life cycle and the emerging
    **machine learning operations** (**MLOps**) field, providing a clear picture of
    the four pillars of DL (data, model, code, and explainability) and the role of
    MLflow in these areas. A basic transfer learning-based NLP sentiment model using
    PyTorch Lightning Flash is built in the first chapter, which is further developed,
    tuned, and deployed to production throughout the rest of the book. From there
    onward, it guides you step-by-step to understand the concept of MLflow experiments
    and usage patterns, using MLflow as a unified framework to track DL data, code
    and pipeline, model, parameters, and metrics at scale. We'll run DL pipelines
    in a distributed execution environment with reproducibility and provenance tracking,
    and tune DL models through **hyperparameter optimization** (**HPO**) with Ray
    Tune, Optuna and HyperBand. We'll also build a multi-step DL inference pipeline
    with preprocessing and postprocessing steps, deploy a DL inference pipeline for
    production using Ray Serve and AWS SageMaker, and finally, provide a DL Explanation-as-a-Service
    using **SHapley Additive exPlanations** (**SHAP**) and MLflow integration.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书从DL完整生命周期的概述和新兴的**机器学习运维**（**MLOps**）领域开始，提供了DL四大支柱（数据、模型、代码和可解释性）以及MLflow在这些领域中的作用的清晰图景。在第一章中，基于转移学习的基本NLP情感分析模型使用PyTorch
    Lightning Flash构建，并在接下来的章节中进一步开发、调优并部署到生产环境。从此开始，本书将一步步引导你理解MLflow实验的概念和使用模式，使用MLflow作为统一的框架来跟踪DL数据、代码与管道、模型、参数和指标的规模化管理。我们将在分布式执行环境中运行DL管道，确保可重现性和溯源追踪，并通过Ray
    Tune、Optuna和HyperBand对DL模型进行**超参数优化**（**HPO**）。我们还将构建一个多步骤DL推理管道，包括预处理和后处理步骤，使用Ray
    Serve和AWS SageMaker部署DL推理管道到生产环境，最后，提供一个使用**SHapley加法解释**（**SHAP**）和MLflow集成的DL解释服务。
- en: By the end of this book, you'll have the foundation and hands-on experience
    to build a DL pipeline from initial offline experimentation to final deployment
    and production, all within a reproducible and open source framework. Along the
    way, you will also learn the unique challenges with DL pipelines and how we overcome
    them with practical and scalable solutions such as using multi-core CPUs, **graphical
    processing units** (**GPUs**), distributed and parallel computing frameworks,
    and the cloud.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的结尾，你将拥有从初始离线实验到最终部署和生产的深度学习（DL）管道构建的基础和实践经验，所有内容都在一个可重现和开源的框架中完成。在此过程中，你还将学习到与DL管道相关的独特挑战，以及我们如何通过实际且可扩展的解决方案克服这些挑战，例如使用多核CPU、**图形处理单元**（**GPU**）、分布式和并行计算框架，以及云计算。
- en: Who this book is for
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书适合谁阅读
- en: This book is written for data scientists, ML engineers, and AI practitioners
    who want to master the full life cycle of DL development from inception to production
    using the open source MLflow framework and related tools such as Ray Tune, SHAP,
    and Ray Serve. The scalable, reproducible, and provenance-aware implementations
    presented in this book ensure you build an enterprise-grade DL pipeline successfully.
    This book will support anyone building powerful DL cloud applications.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本书是为数据科学家、机器学习工程师和人工智能从业者编写的，旨在帮助他们掌握深度学习从构想到生产的完整生命周期，使用开源MLflow框架及相关工具，如Ray
    Tune、SHAP和Ray Serve。本书中展示的可扩展、可复现且关注溯源的实现，确保您能够成功构建企业级深度学习管道。本书将为构建强大深度学习云应用程序的人员提供支持。
- en: What this book covers
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书内容概述
- en: '[*Chapter 1*](B18120_01_ePub.xhtml#_idTextAnchor015), *Deep Learning Life Cycle
    and MLOps Challenges*, covers the five stages of the full life cycle of DL and
    the first DL model in this book using the transfer learning approach for text
    sentiment classification. It also defines the concept of MLOps along with the
    three foundation layers and four pillars, and the roles of MLflow in these areas.
    An overview of the challenges in DL data, model, code, and explainability are
    also presented. This chapter is designed to bring everyone to the same foundational
    level and provides clarity and guidelines on the scope of the rest of the book.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第一章*](B18120_01_ePub.xhtml#_idTextAnchor015)，*深度学习生命周期与MLOps挑战*，涵盖了深度学习完整生命周期的五个阶段，并使用迁移学习方法进行文本情感分类的第一个深度学习模型。它还定义了MLOps的概念，并介绍了其三个基础层次和四大支柱，以及MLflow在这些领域的作用。此外，还概述了深度学习在数据、模型、代码和可解释性方面面临的挑战。本章旨在将每个人带入相同的基础水平，并为本书其余部分的范围提供清晰的指导。'
- en: '[*Chapter 2*](B18120_02_ePub.xhtml#_idTextAnchor027), *Getting Started with
    MLflow for Deep Learning*, serves as an MLflow primer and a first hands-on learning
    module to quickly set up a local filesystem-based MLflow tracking server or interact
    with a remote managed MLflow tracking server in Databricks, and perform a first
    DL experiment using MLflow auto logging. It also explains some foundational MLflow
    concepts through concrete examples such as experiments, runs, metadata about and
    the relationship between experiments and runs, code tracking, model logging, and
    model flavor. Specifically, we underline that experiments should be first-class
    entities that can be used to bridge the gap between the offline and online production
    life cycle of DL models. This chapter builds the foundational knowledge of MLflow.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第二章*](B18120_02_ePub.xhtml#_idTextAnchor027)，*使用MLflow开始深度学习之旅*，作为MLflow的入门教程和首个实践模块，快速设置基于本地文件系统的MLflow追踪服务器，或在Databricks上与远程管理的MLflow追踪服务器互动，并使用MLflow自动日志记录进行首个深度学习实验。本章还通过具体示例讲解了一些基础的MLflow概念，如实验、运行、实验和运行之间的元数据及其关系、代码追踪、模型日志记录和模型类型。特别地，我们强调实验应作为一等公民实体，能够弥合深度学习模型的离线与在线生产生命周期之间的差距。本章为MLflow的基础知识奠定了基础。'
- en: '[*Chapter 3*](B18120_03_ePub.xhtml#_idTextAnchor040), *Tracking Models, Parameters,
    and Metrics*, covers the first in-depth learning module on tracking using a fully-fledged
    local MLflow tracking server. It starts with setting up a local fully-fledged
    MLflow tracking server that runs in Docker Desktop, with a MySQL backend store
    and a MinIO artifact store. Before implementing tracking, this chapter provides
    an open provenance tracking framework based on the open provenance model vocabulary
    specification, and presents six types of provenance questions that could be implemented
    by using MLflow. It then provides hands-on implementation examples on how to use
    MLflow model-logging APIs and registry APIs to track model provenance, model metrics,
    and parameters, with or without auto logging. Unlike other typical MLflow API
    tutorials, which only provide guidance on using the APIs, this chapter instead
    focuses on how successfully we can use MLflow to answer the provenance questions.
    By the end of this chapter, we could answer four out of six provenance questions,
    and the remaining two questions can only be answered when we have a multi-step
    pipeline or deployment to production, which are covered in the later chapters.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第3章*](B18120_03_ePub.xhtml#_idTextAnchor040)，*跟踪模型、参数和指标*，涵盖了使用完整本地MLflow跟踪服务器的第一次深入学习模块。它从设置在Docker
    Desktop中运行的本地完整MLflow跟踪服务器开始，后端存储使用MySQL，工件存储使用MinIO。在实施跟踪之前，本章提供了基于开放源追踪模型词汇规范的开放源追踪框架，并提出了六种可以通过使用MLflow实现的溯源问题。接着，它提供了如何使用MLflow模型日志API和注册API来跟踪模型溯源、模型指标和参数的实践示例，无论是否启用自动日志。与其他典型的MLflow
    API教程不同，本章不仅仅提供使用API的指导，而是专注于我们如何成功地使用MLflow来回答溯源问题。在本章结束时，我们可以回答六个溯源问题中的四个，剩下的两个问题只能在拥有多步骤管道或部署到生产环境时回答，这些内容将在后续章节中涵盖。'
- en: '[*Chapter 4*](B18120_04_ePub.xhtml#_idTextAnchor050), *Tracking Code and Data
    Versioning*, covers the second in-depth learning module on MLflow tracking. It
    analyzes the current practices on the usage of notebooks and pipelines in the
    ML/DL projects. It recommends using VS Code notebooks and shows a concrete DL
    notebook example that can be run either interactively or non-interactively with
    MLflow tracking enabled. It also recommends using MLflow''s **MLproject** to implement
    a multi-step DL pipeline using MLflow''s entry points and pipeline chaining. A
    three-step DL pipeline is created for DL model training and registration. In addition,
    it also shows the pipeline level tracking and individual step tracking through
    the parent-child nested run in MLflow. Finally, it shows how to track public and
    privately built Python libraries and data versioning in **Delta Lake** using MLflow.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第4章*](B18120_04_ePub.xhtml#_idTextAnchor050)，*跟踪代码和数据版本管理*，涵盖了关于MLflow跟踪的第二个深入学习模块。它分析了在ML/DL项目中使用笔记本和管道的现有实践。它推荐使用VS
    Code笔记本，并展示了一个具体的深度学习笔记本示例，该示例可以在启用MLflow跟踪的情况下交互式或非交互式运行。它还建议使用MLflow的**MLproject**，通过MLflow的入口点和管道链实现多步骤的深度学习管道。为深度学习模型的训练和注册创建了一个三步深度学习管道。此外，它还展示了通过MLflow中的父子嵌套运行进行的管道级跟踪和单个步骤的跟踪。最后，它展示了如何使用MLflow跟踪公共和私有构建的Python库以及在**Delta
    Lake**中进行数据版本管理。'
- en: '[*Chapter 5*](B18120_05_ePub.xhtml#_idTextAnchor060), *Running DL Pipelines
    in Different Environments*, covers how to run a DL pipeline in different environments.
    It starts with the scenarios and requirements for executing DL pipelines in different
    environments. It then shows how to use MLflow''s **command-line interface** (**CLI**)
    to submit runs in four scenarios: running locally with local code, running locally
    with remote code in GitHub, running remotely in the cloud with local code, and
    running remotely in the cloud with remote code in GitHub. The flexibility and
    reproducibility supported by MLflow to execute a DL pipeline also provide building
    blocks for **continuous integration/continuous deployment** (**CI/CD**) automation
    when needed.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第5章*](B18120_05_ePub.xhtml#_idTextAnchor060)，*在不同环境中运行深度学习管道*，涵盖了如何在不同环境中运行深度学习管道。首先介绍了在不同环境中执行深度学习管道的场景和要求。接着展示了如何使用MLflow的**命令行界面**（**CLI**）在四种场景中提交运行：本地运行本地代码、在GitHub上运行本地代码、在云端远程运行本地代码、以及在云端远程运行GitHub上的代码。MLflow所支持的灵活性和可重现性在执行深度学习管道时，也为需要时的**持续集成/持续部署**（**CI/CD**）自动化提供了构建块。'
- en: '[*Chapter 6*](B18120_06_ePub.xhtml#_idTextAnchor069), *Running Hyperparameter
    Tuning at Scale*, covers using MLflow to support HPO at scale using state-of-the-art
    HPO frameworks such as Ray Tune. It starts with a review of the types and challenges
    of DL pipeline hyperparameters. Then, it compares three HPO frameworks Ray Tune,
    Optuna, and HyperOpt, and provides a detailed analysis of the pros and cons and
    their integration maturity with MLflow. It then recommends and shows how to use
    Ray Tune with MLflow to do HPO tuning for the DL model we have been working on
    in this book so far. Furthermore, it covers how to switch to other HPO search
    and scheduler algorithms such as Optuna and HyperBand. This enables us to produce
    high-performance DL models that meet the business requirements in a cost-effective
    and scalable way.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第六章*](B18120_06_ePub.xhtml#_idTextAnchor069)，*大规模超参数调优运行*，介绍了如何使用 MLflow
    支持大规模的超参数优化（HPO），并利用最先进的 HPO 框架如 Ray Tune。首先回顾了深度学习流水线超参数的类型和挑战。然后，比对了三个 HPO 框架：Ray
    Tune、Optuna 和 HyperOpt，并对它们与 MLflow 的集成成熟度及优缺点进行了详细分析。接着，推荐并展示了如何使用 Ray Tune 与
    MLflow 结合，对本书中迄今为止所讨论的深度学习模型进行超参数调优。此外，还介绍了如何切换到其他 HPO 搜索和调度算法，如 Optuna 和 HyperBand。这使得我们能够以一种具有成本效益且可扩展的方式，生产符合业务需求的高性能深度学习模型。'
- en: '[*Chapter 7*](B18120_07_ePub.xhtml#_idTextAnchor083), *Multi-Step Deep Learning
    Inference Pipeline*, covers creating a multi-step inference pipeline using MLflow''s
    custom Python model approach. It starts with an overview of four patterns of inference
    workflows in production where a single trained model is usually not enough to
    meet the business application requirements. Additional preprocessing and postprocessing
    steps are needed. It then presents a step-by-step guide to implementing a multi-step
    inference pipeline that wraps the previously fine-tuned DL sentiment model with
    language detection, caching, and additional model metadata. This inference pipeline
    is then logged as a generic MLflow **PyFunc** model that can be loaded using the
    common MLflow PyFunc load API. Having an inference pipeline wrapped as an MLflow
    model opens doors for automation and consistent management of the model pipeline
    within the same MLflow framework.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第七章*](B18120_07_ePub.xhtml#_idTextAnchor083)，*多步骤深度学习推理流水线*，介绍了使用 MLflow
    的自定义 Python 模型方法创建多步骤推理流水线的过程。首先概述了生产环境中四种推理工作流模式，在这些模式下，单一的训练模型通常不足以满足业务应用的需求，需要额外的预处理和后处理步骤。接着，提供了一个逐步指南，讲解如何实现一个多步骤推理流水线，该流水线将先前微调过的深度学习情感模型与语言检测、缓存以及额外的模型元数据结合起来。该推理流水线随后会作为一个通用的
    MLflow **PyFunc** 模型进行日志记录，可以通过通用的 MLflow PyFunc 加载 API 加载。将推理流水线包装成 MLflow 模型为自动化和在同一
    MLflow 框架内一致地管理模型流水线开辟了新天地。'
- en: '[*Chapter 8*](B18120_08_ePub.xhtml#_idTextAnchor095), *Deploying a DL Inference
    Pipeline at Scale*, covers deploying a DL inference pipeline into different host
    environments for production usage. It starts with an overview of the landscape
    of deployment and hosting environments including batch inference and streaming
    inference at scale. It then describes the different deployment mechanisms such
    as MLflow built-in model serving tools, custom deployment plugins, and generic
    model serving frameworks such as Ray Serve. It shows examples of how to deploy
    a batch inference pipeline using MLflow''s Spark `mlflow-ray-serve`. It then describes
    a complete step-by-step guide to deploying a DL inference pipeline to a managed
    AWS SageMaker instance for production usage.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第八章*](B18120_08_ePub.xhtml#_idTextAnchor095)，*大规模部署深度学习推理流水线*，介绍了如何将深度学习推理流水线部署到不同的主机环境中以进行生产使用。首先概述了部署和托管环境的全景，包括大规模的批量推理和流式推理。接着，描述了不同的部署机制，如
    MLflow 内置的模型服务工具、自定义部署插件以及像 Ray Serve 这样的通用模型服务框架。示例展示了如何使用 MLflow 的 Spark `mlflow-ray-serve`
    部署批量推理流水线。接下来，提供了一个完整的逐步指南，讲解如何将深度学习推理流水线部署到托管的 AWS SageMaker 实例中用于生产环境。'
- en: '[*Chapter 9*](B18120_09_ePub.xhtml#_idTextAnchor112), *Fundamentals of Deep
    Learning Explainability*, covers the foundational concepts of explainability and
    exploration of using two popular explainability tools. It starts with an overview
    of the eight dimensions of explainability and **explainable AI** (**XAI**), then
    provides concrete learning examples to explore the usage of SHAP and Transformers-interpret
    toolboxes for an NLP sentiment pipeline. It emphasizes that explainability should
    be lifted to be the first-class artifact when developing a DL application since
    there are increasing demands and expectations for model and data explanation in
    various business applications and domains.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第9章*](B18120_09_ePub.xhtml#_idTextAnchor112)，*深度学习可解释性基础*，介绍了可解释性的基础概念，并探索了使用两个流行的可解释性工具。本章从概述可解释性的八个维度和**可解释的人工智能**（**XAI**）开始，然后提供了具体的学习示例，探索如何使用SHAP和Transformers-interpret工具箱进行NLP情感分析管道的应用。它强调，在开发深度学习应用时，可解释性应该被提升为一类重要的工件，因为在各类商业应用和领域中，对模型和数据解释的需求和期望正在不断增加。'
- en: '[*Chapter 10*](B18120_10_ePub.xhtml#_idTextAnchor127), *Implementing DL Explainability
    with MLflow*, covers how to implement DL explainability using MLflow to provide
    **Explanation-as-a-Service** (**EaaS**). It starts with an overview of MLflow''s
    current capability to support explainers and explanations. Specifically, the existing
    integration with SHAP in MLflow APIs does not support DL explainability at scale.
    Therefore, it provides two generic ways of using MLflow''s artifact logging APIs
    and **PyFunc** APIs for the implementation. Examples are provided for implementing
    SHAP explanation, which logs the SHAP value in a bar chart in an MLflow tracking
    server''s artifact store. A SHAP explainer can be logged as an MLflow Python model,
    and then loaded as either a Spark UDF for batch explanation or as a web service
    for online EaaS. This provides maximal flexibility within a unified MLflow framework
    for implementing explainability.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[*第10章*](B18120_10_ePub.xhtml#_idTextAnchor127)，*使用MLflow实现深度学习可解释性*，介绍了如何使用MLflow实现深度学习可解释性，并提供**解释即服务**（**EaaS**）。本章从概述MLflow当前支持的解释器和解释功能开始。具体来说，MLflow
    API与SHAP的现有集成不支持大规模的深度学习可解释性。因此，本章提供了使用MLflow的工件日志记录API和**PyFunc** API来实现的两种通用方法。文中提供了实现SHAP解释的示例，该解释将SHAP值以条形图的形式记录到MLflow跟踪服务器的工件存储中。SHAP解释器可以作为MLflow
    Python模型进行日志记录，然后作为Spark UDF批处理解释或作为Web服务进行在线EaaS加载。这为在统一的MLflow框架内实现可解释性提供了最大的灵活性。'
- en: To get the most out of this book
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何充分利用本书
- en: 'The majority of the code in this book can be implemented and executed using
    the open source MLflow tool, with a few exceptions where a 14-day full Databricks
    trial is needed (sign up at [https://databricks.com/try-databricks](https://databricks.com/try-databricks))
    along with an AWS Free Tier account (sign up at [https://aws.amazon.com/free/](https://aws.amazon.com/free/)).
    The following lists some major software packages covered in this book:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的大多数代码可以使用开源的MLflow工具进行实现和执行，少数情况需要14天的完整Databricks试用（可以在[https://databricks.com/try-databricks](https://databricks.com/try-databricks)注册）和一个AWS免费账户（可以在[https://aws.amazon.com/free/](https://aws.amazon.com/free/)注册）。以下列出了本书中涵盖的一些主要软件包：
- en: MLflow 1.20.2 and above
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLflow 1.20.2及以上版本
- en: Python 3.8.10
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3.8.10
- en: Lightning-flash 0.5.0
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lightning-flash 0.5.0
- en: Transformers 4.9.2
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers 4.9.2
- en: SHAP 0.40.0
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHAP 0.40.0
- en: PySpark 3.2.1
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 3.2.1
- en: Ray[tune] 1.9.2
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ray[tune] 1.9.2
- en: Optuna 2.10.0
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Optuna 2.10.0
- en: 'The complete package dependencies are listed in each chapter''s `requirements.txt`
    file or the `conda.yaml` file in this book''s GitHub repository. All code has
    been tested to run successfully in a macOS or Linux environment. If you are a
    Microsoft Windows user, it is recommended to install **WSL2** to run the bash
    scripts provided in this book: [https://www.windowscentral.com/how-install-wsl2-windows-10](https://www.windowscentral.com/how-install-wsl2-windows-10).
    It is a known issue that the MLflow CLI does not work properly in the Microsoft
    Windows command line.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中每一章的完整包依赖列在`requirements.txt`文件或本书GitHub仓库中的`conda.yaml`文件中。所有代码已在macOS或Linux环境下成功测试。如果你是Microsoft
    Windows用户，建议安装**WSL2**以运行本书中提供的bash脚本：[https://www.windowscentral.com/how-install-wsl2-windows-10](https://www.windowscentral.com/how-install-wsl2-windows-10)。已知问题是MLflow
    CLI在Microsoft Windows命令行中无法正常工作。
- en: Starting from [*Chapter 3*](B18120_03_ePub.xhtml#_idTextAnchor040)*,* *Tracking
    Models, Parameters, and Metrics* of this book, you will also need to have Docker
    Desktop ([https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/))
    installed to set up a fully-fledged local MLflow tracking server for executing
    the code in this book. AWS SageMaker is needed in [*Chapter 8*](B18120_08_ePub.xhtml#_idTextAnchor095)*,
    Deploying a DL Inference Pipeline at Scale,* for the cloud deployment example.
    VS Code version 1.60 or above ([https://code.visualstudio.com/updates/v1_60](https://code.visualstudio.com/updates/v1_60))
    is used as the **integrated development environment** (**IDE**) in this book.
    Miniconda version 4.10.3 or above ([https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html))
    is used throughout this book for creating and activating virtual environments.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从本书的 [*第 3 章*](B18120_03_ePub.xhtml#_idTextAnchor040)*，* *模型、参数和指标的追踪* 开始，你还需要安装
    Docker Desktop（[https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)）来设置一个完整的本地
    MLflow 跟踪服务器，以便执行本书中的代码。[第 8 章](B18120_08_ePub.xhtml#_idTextAnchor095)*，* *在大规模上部署深度学习推理管道*
    需要 AWS SageMaker 进行云部署示例。本书中使用的是 VS Code 版本 1.60 或更高版本 ([https://code.visualstudio.com/updates/v1_60](https://code.visualstudio.com/updates/v1_60))，作为
    **集成开发环境** (**IDE**)。本书中的虚拟环境创建和激活使用的是 Miniconda 版本 4.10.3 或更高版本 ([https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html))。
- en: '**If you are using the digital version of this book, we advise you to type
    the code yourself or access the code from the book''s GitHub repository (a link
    is available in the next section). Doing so will help you avoid any potential
    errors related to the copying and pasting of code.**'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你使用的是本书的电子版本，建议你自己输入代码或从本书的 GitHub 仓库中获取代码（下一个部分会提供链接）。这样可以帮助你避免因复制粘贴代码而可能出现的错误。**'
- en: Finally, to get the most out of this book, you should have experience in programming
    in Python and have a basic understanding of popular ML and data manipulation libraries
    such as pandas and PySpark.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了充分利用本书的内容，你应该具有 Python 编程经验，并且对常用的机器学习和数据处理库（如 pandas 和 PySpark）有基本了解。
- en: Download the example code files
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载示例代码文件
- en: You can download the example code files for this book from GitHub at [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow).
    If there's an update to the code, it will be updated in the GitHub repository.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 GitHub 下载本书的示例代码文件，链接为 [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow)。如果代码有更新，它将在
    GitHub 仓库中同步更新。
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了来自我们丰富图书和视频目录的其他代码包，详情请访问 [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)。赶紧去看看吧！
- en: Download the color images
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载彩色图像
- en: 'We also provide a PDF file that has color images of the screenshots and diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781803241333_ColorImages.pdf](_ColorImages.pdf).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了 PDF 文件，包含本书中使用的屏幕截图和图表的彩色图像。你可以在这里下载：[https://static.packt-cdn.com/downloads/9781803241333_ColorImages.pdf](_ColorImages.pdf)。
- en: Conventions used
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用的约定
- en: There are a number of text conventions used throughout this book.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用了许多文本约定。
- en: '`Code in text`: Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: "For learning purposes, we have provided two example
    `mlruns` artifacts and the `huggingface` cache folder in the GitHub repository
    under the `chapter08` folder."'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`Code in text`：表示文本中的代码词汇、数据库表名、文件夹名称、文件名、文件扩展名、路径名、虚拟网址、用户输入和 Twitter 账号。示例如下：“为了学习目的，我们在
    GitHub 仓库的 `chapter08` 文件夹下提供了两个示例 `mlruns` 工件和 `huggingface` 缓存文件夹。”'
- en: 'A block of code is set as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块按如下方式书写：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望引起你对代码块中特定部分的注意时，相关行或项目将加粗显示：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Any command-line input or output is written as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 任何命令行输入或输出都按如下方式书写：
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For instance, words in menus or dialog boxes appear in **bold**. Here is an example:
    "To execute the code in this cell, you can just click on **Run Cell** in the top-right
    drop-down menu."'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗体**：表示一个新术语、重要词汇或屏幕上看到的词语。例如，菜单或对话框中的词语以 **粗体** 显示。举个例子：“要执行这个单元格中的代码，你只需点击右上角下拉菜单中的
    **Run Cell**。”'
- en: Tips or Important Notes
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 提示或重要注意事项
- en: Appear like this.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种形式出现。
- en: Get in touch
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联系我们
- en: Feedback from our readers is always welcome.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们始终欢迎读者的反馈。
- en: '**General feedback**: If you have questions about any aspect of this book,
    email us at [customercare@packtpub.com](mailto:customercare@packtpub.com) and
    mention the book title in the subject of your message.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**常见反馈**：如果你对本书的任何内容有疑问，请通过电子邮件联系我们：[customercare@packtpub.com](mailto:customercare@packtpub.com)，并在邮件主题中注明书名。'
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit www.packtpub.com/support/errata
    and fill in the form.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**勘误**：虽然我们已尽力确保内容的准确性，但错误难免发生。如果你在本书中发现错误，我们将不胜感激，如果你能将其报告给我们。请访问 www.packtpub.com/support/errata
    并填写表单。'
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at [copyright@packt.com](mailto:copyright@packt.com)
    with a link to the material.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**盗版**：如果你在互联网上发现我们的作品以任何形式的非法复制，我们将非常感激你能提供该材料的地址或网站名称。请通过 [copyright@packt.com](mailto:copyright@packt.com)
    与我们联系，并提供链接。'
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你有兴趣成为作者**：如果你在某个领域有专业知识，并且有兴趣写书或为书籍做贡献，请访问 [authors.packtpub.com](http://authors.packtpub.com)。'
- en: Share Your Thoughts
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分享你的想法
- en: Once you've read *Practical Deep Learning at Scale with MLflow*, we'd love to
    hear your thoughts! Please click here to go straight to the Amazon review page
    for this book and share your feedback.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你阅读了 *Practical Deep Learning at Scale with MLflow*，我们希望听到你的反馈！请点击这里，直接访问本书的亚马逊评论页面，并分享你的看法。
- en: Your review is important to us and the tech community and will help us make
    sure we're delivering excellent quality content.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 你的评论对我们和技术社区都非常重要，它将帮助我们确保提供优质的内容。
