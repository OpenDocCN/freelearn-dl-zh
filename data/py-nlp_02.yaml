- en: Practical Understanding of a Corpus and Dataset
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语料库和数据集的实际理解
- en: 'In this chapter, we''ll explore the first building block of natural language
    processing. We are going to cover the following topics to get a practical understanding
    of a corpus or dataset:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将探索自然语言处理的第一个构建模块。我们将涵盖以下主题，以便对语料库或数据集有一个实际的理解：
- en: What is corpus?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是语料库？
- en: Why do we need corpus?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为什么需要语料库？
- en: Understanding corpus analysis
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解语料库分析
- en: Understanding types of data attributes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据属性的类型
- en: Exploring different file formats of datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据集的不同文件格式
- en: Resources for access free corpus
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取免费语料库的资源
- en: Preparing datasets for NLP applications
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为NLP应用准备数据集
- en: Developing the web scrapping application
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发网页抓取应用
- en: What is a corpus?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是语料库？
- en: 'Natural language processing related applications are built using a huge amount
    of data. In layman''s terms, you can say that a large collection of data is called
    **corpus**. So, more formally and technically, corpus can be defined as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 与自然语言处理相关的应用是基于大量数据构建的。用通俗的话来说，你可以将大量数据集合称为**语料库**。因此，更正式和技术上讲，语料库可以定义如下：
- en: Corpus is a collection of written or spoken natural language material, stored
    on computer, and used to find out how language is used. So more precisely, a corpus
    is a systematic computerized collection of authentic language that is used for
    linguistic analysis as well as corpus analysis. If you have more than one corpus,
    it is called **corpora**.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库是存储在计算机上的书面或口语自然语言材料的集合，用于研究语言的使用方式。更准确地说，语料库是一个系统化的计算机化真实语言集合，用于语言学分析和语料库分析。如果你有多个语料库，这些集合称为**语料库**。
- en: In order to develop NLP applications, we need corpus that is written or spoken
    natural language material. We use this material or data as input data and try
    to find out the facts that can help us develop NLP applications. Sometimes, NLP
    applications use a single corpus as the input, and at other times, they use multiple
    corpora as input.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发NLP应用，我们需要书面或口语的自然语言材料。我们使用这些材料或数据作为输入数据，并尝试找出能够帮助我们开发NLP应用的事实。有时，NLP应用使用单一语料库作为输入，有时则使用多个语料库作为输入。
- en: 'There are many reasons of using corpus for developing NLP applications, some
    of which are as follows:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用语料库开发NLP应用的原因有很多，以下是其中一些：
- en: With the help of corpus, we can perform some statistical analysis such as frequency
    distribution, co-occurrences of words, and so on. Don't worry, we will see some
    basic statistical analysis for corpus later in this chapter.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 借助语料库，我们可以进行一些统计分析，如词频分布、词汇共现等。别担心，我们将在本章后面看到一些语料库的基本统计分析。
- en: We can define and validate linguistics rules for various NLP applications. If
    you are building a grammar correction system, you will use the text corpus and
    try to find out the grammatically incorrect instances, and then you will define
    the grammar rules that help us to correct those instances.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以为各种自然语言处理（NLP）应用定义和验证语言学规则。如果你正在构建一个语法纠错系统，你将使用文本语料库，尝试找出语法上不正确的实例，然后定义帮助我们纠正这些实例的语法规则。
- en: We can define some specific linguistic rules that depend on the usage of the
    language. With the help of the rule-based system, you can define the linguistic
    rules and validate the rules using corpus.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以定义一些特定的语言学规则，这些规则依赖于语言的使用方式。借助基于规则的系统，你可以定义语言学规则，并使用语料库验证这些规则。
- en: 'In a corpus, the large collection of data can be in the following formats:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在语料库中，大量数据可以有以下几种格式：
- en: Text data, meaning written material
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本数据，指书面材料
- en: Speech data, meaning spoken material
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音数据，指口语材料
- en: 'Let''s see what exactly text data is and how can we collect the text data.
    Text data is a collection of written information. There are several resources
    that can be used for getting written information such as news articles, books,
    digital libraries, email messages, web pages, blogs, and so on. Right now, we
    all are living in a digital world, so the amount of text information is growing
    rapidly. So, we can use all the given resources to get the text data and then
    make our own corpus. Let''s take an example: if you want to build a system that
    summarizes news articles, you will first gather various news articles present
    on the web and generate a collection of new articles so that the collection is
    your corpus for news articles and has text data. You can use web scraping tools
    to get information from raw HTML pages. In this chapter, we will develop one.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看文本数据到底是什么以及如何收集文本数据。文本数据是书面信息的集合。有多种资源可以用来获取书面信息，例如新闻文章、书籍、数字图书馆、电子邮件信息、网页、博客等等。现在，我们都生活在一个数字化的世界中，因此文本信息的数量正在迅速增长。因此，我们可以使用所有这些资源来获取文本数据，并且可以创建自己的语料库。举个例子：如果你想构建一个系统来总结新闻文章，你首先需要收集互联网上的各种新闻文章，然后生成一个新闻文章的集合，这个集合就是你的新闻文章语料库，包含文本数据。你可以使用网页抓取工具从原始HTML页面中获取信息。在本章中，我们将开发一个工具。
- en: 'Now we will see how speech data is collected. A speech data corpus generally
    has two things: one is an audio file, and the other one is its text transcription.
    Generally, we can obtain speech data from audio recordings. This audio recording
    may have dialogues or conversations of people. Let me give you an example: in
    India, when you call a bank customer care department, if you pay attention, you
    get to know that each and every call is recorded. This is the way you can generate
    speech data or speech corpus. For this book, we are concentrating just on text
    data and not on speech data.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下语音数据是如何收集的。语音数据语料库通常有两部分：一部分是音频文件，另一部分是其文本转录。通常，我们可以通过音频录音来获得语音数据。这些音频录音可能包含人们的对话或交流。举个例子：在印度，当你拨打银行客服部门电话时，如果你留心，会发现每个电话都被录音。这就是你可以生成语音数据或语音语料库的方式。对于本书，我们专注于文本数据，而非语音数据。
- en: A corpus is also referred to as a dataset in some cases.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，语料库也被称为数据集。
- en: 'There are three types of corpus:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库有三种类型：
- en: '**Monolingual corpus:** This type of corpus has one language'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单语语料库：** 这种类型的语料库只有一种语言。'
- en: '**Bilingual corpus:** This type of corpus has two languages'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双语语料库：** 这种类型的语料库有两种语言。'
- en: '**Multilingual corpus:** This type of corpus has more than one language'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多语种语料库：** 这种类型的语料库有多于一种语言。'
- en: 'A few examples of the available corpora are given as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些可用语料库的例子：
- en: Google Books Ngram corpus
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Books Ngram 语料库
- en: Brown corpus
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布朗语料库
- en: American National corpus
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国国家语料库
- en: Why do we need a corpus?
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们为什么需要语料库？
- en: 'In any NLP application, we need data or corpus to building NLP tools and applications.
    A corpus is the most critical and basic building block of any NLP-related application.
    It provides us with quantitative data that is used to build NLP applications.
    We can also use some part of the data to test and challenge our ideas and intuitions
    about the language. Corpus plays a very big role in NLP applications. Challenges
    regarding creating a corpus for NLP applications are as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何自然语言处理（NLP）应用中，我们都需要数据或语料库来构建NLP工具和应用程序。语料库是任何与NLP相关的应用程序中最关键和最基本的构建块。它为我们提供了定量数据，用于构建NLP应用程序。我们还可以使用部分数据来测试和挑战我们对语言的想法和直觉。语料库在NLP应用程序中起着非常重要的作用。关于为NLP应用程序创建语料库的挑战如下：
- en: Deciding the type of data we need in order to solve the problem statement
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定我们需要哪种类型的数据来解决问题陈述
- en: Availability of data
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的可用性
- en: Quality of the data
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的质量
- en: Adequacy of the data in terms of amount
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据量的充足性
- en: Now you may want to know the details of all the preceding questions; for that,
    I will take an example that can help you to understand all the previous points
    easily. Consider that you want to make an NLP tool that understands the medical
    state of a particular patient and can help generate a diagnosis after proper medical
    analysis.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能想了解前面所有问题的详细信息；为了帮助你轻松理解所有前述内容，我将举一个例子。假设你想制作一个NLP工具，用于了解特定患者的医疗状况，并能在经过适当医疗分析后生成诊断。
- en: 'Here, our aspect is more biased toward the corpus level and generalized. If
    you look at the preceding example as an NLP learner, you should process the problem
    statement as stated here:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们的角度更偏向语料库层面，并且是概括性的。如果你作为NLP学习者看待前面的例子，你应该按以下方式处理问题陈述：
- en: What kind of data do I need if I want to solve the problem statement?
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我要解决问题陈述，我需要什么样的数据？
- en: Clinical notes or patient history
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 临床笔记或病史记录
- en: Audio recording of the conversation between doctor and patient
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医生与病人之间的对话录音
- en: Do you have this kind of corpus or data with you?
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否拥有这种语料库或数据？
- en: If yes, great! You are in a good position, so you can proceed to the next question.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是，那就太好了！你处于一个很好的位置，可以继续回答下一个问题。
- en: If not, OK! No worries. You need to process one more question, which is probably
    a difficult but interesting one.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不是，没关系！不用担心。你需要再处理一个问题，可能是一个困难但有趣的问题。
- en: Is there an open source corpus available?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否有开源语料库可用？
- en: If yes, download it, and continue to the next question.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是，下载它，并继续到下一个问题。
- en: If not, think of how you can access the data and build the corpus. Think of
    web scraping tools and techniques. But you have to explore the ethical as well
    as legal aspects of your web scraping tool.
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不是，想一想你如何访问数据并构建语料库。可以考虑使用网页抓取工具和技术。但你需要探索网页抓取工具的伦理和法律方面的问题。
- en: What is the quality level of the corpus?
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语料库的质量水平如何？
- en: 'Go through the corpus, and try to figure out the following things:'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浏览语料库，并尝试找出以下几点：
- en: If you can't understand the dataset at all, then what to do?
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你根本无法理解数据集，那么该怎么办？
- en: Spend more time with your dataset.
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多花些时间与数据集互动。
- en: Think like a machine, and try to think of all the things you would process if
    you were fed with this kind of a dataset. Don't think that you will throw an error!
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像机器一样思考，试着想象如果你被喂入这种数据集，你会处理哪些事情。不要认为你会抛出错误！
- en: Find one thing that you feel you can begin with.
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出你觉得可以开始的事情。
- en: Suppose your NLP tool has diagnosed a human disease, think of what you would
    ask the patient if you were the doctor's machine. Now you can start understanding
    your dataset and then think about the preprocessing part. Do not rush to the it.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设你的NLP工具已经诊断出一种人类疾病，想象一下如果你是医生的机器，你会问病人什么问题。现在你可以开始理解你的数据集，并思考预处理部分。不要急于处理它。
- en: If you can understand the dataset, then what to do?
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你能理解数据集，那么接下来该做什么？
- en: Do you need each and every thing that is in the corpus to build an NLP system?
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建NLP系统时，是否需要语料库中的每一个内容？
- en: If yes, then proceed to the next level, which we will look at in Chapter 5,
    *Feature Engineering and NLP Algorithms*.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是，那么请继续到下一个阶段，我们将在第五章中讨论，*特征工程与NLP算法*。
- en: If not, then proceed to the next level, which we will look at in Chapter 4,
    *Preprocessing*.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不是，那么继续到下一个阶段，我们将在第四章中讨论，*预处理*。
- en: Will the amount of data be sufficient for solving the problem statement on at
    least a **proof of concept** (**POC**) basis?
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据量是否足够，至少能在**概念验证**（**POC**）基础上解决问题？
- en: According to my experience, I would prefer to have at least 500 MB to 1 GB of
    data for a small POC.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据我的经验，我更倾向于至少拥有500MB到1GB的数据用于小型POC。
- en: 'For startups, to collect 500 MB to 1 GB data is also a challenge for the following
    reasons:'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于初创企业来说，收集500MB到1GB的数据也面临挑战，原因如下：
- en: Startups are new in business.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初创企业是商业中新手。
- en: Sometimes they are very innovative, and there is no ready-made dataset available.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时它们非常创新，并且没有现成的可用数据集。
- en: Even if they manage to build a POC, to validate their product in real life is
    also challenging.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使他们成功构建了POC，在现实生活中验证他们的产品也是一项挑战。
- en: 'Refer to *Figure 2.1* for a description of the preceding process:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*图2.1*，以了解前述过程的描述：
- en: '![](img/e79a2f33-2530-47d0-95fe-98d93f406f93.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e79a2f33-2530-47d0-95fe-98d93f406f93.png)'
- en: 'Figure 2.1: Description of the process defined under why do we need corpus?'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1：为什么我们需要语料库？过程描述。
- en: Understanding corpus analysis
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解语料库分析
- en: In this section, we will first understand what corpus analysis is. After this,
    we will briefly touch upon speech analysis. We will also understand how we can
    analyze text corpus for different NLP applications. At the end, we will do some
    practical corpus analysis for text corpus. Let's begin!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先将了解什么是语料库分析。之后，我们将简要探讨语音分析。我们还将了解如何为不同的自然语言处理（NLP）应用分析文本语料库。最后，我们将进行一些实际的文本语料库分析。让我们开始吧！
- en: Corpus analysis can be defined as a methodology for pursuing in-depth investigations
    of linguistic concepts as grounded in the context of authentic and communicative
    situations. Here, we are talking about the digitally stored language corpora,
    which is made available for access, retrieval, and analysis via computer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库分析可以定义为一种方法论，用于在真实和交际情境的背景下深入调查语言概念。在这里，我们讨论的是数字存储的语言语料库，这些语料库可以通过计算机进行访问、检索和分析。
- en: Corpus analysis for speech data needs the analysis of phonetic understanding
    of each of the data instances. Apart from phonetic analysis, we also need to do
    conversation analysis, which gives us an idea of how social interaction happens
    in day-to-day life in a specific language. Suppose in real life, if you are doing
    conversational analysis for casual English language, maybe you find a sentence
    such as *What's up, dude?* more frequently used in conversations compared to *How
    are you, sir (or madam)?*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 语音数据的语料库分析需要对每个数据实例的语音理解进行分析。除了语音分析外，我们还需要进行对话分析，这有助于我们了解特定语言中日常生活中社会互动的发生方式。例如，在现实生活中，如果你正在进行英语口语对话分析，你可能会发现像*What's
    up, dude?*这样的句子在对话中比*How are you, sir (or madam)?*使用得更频繁。
- en: Corpus analysis for text data consists in statistically probing, manipulating,
    and generalizing the dataset. So for a text dataset, we generally perform analysis
    of how many different words are present in the corpus and what the frequency of
    certain words in the corpus is. If the corpus contains any noise, we try to remove
    that noise. In almost every NLP application, we need to do some basic corpus analysis
    so we can understand our corpus well. `nltk` provides us with some inbuilt corpus.
    So, we perform corpus analysis using this inbuilt corpus. Before jumping to the
    practical part, it is very important to know what type of corpora is present in
    `nltk`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据的语料库分析包括统计性地探查、操纵和概括数据集。因此，对于文本数据集，我们通常会分析语料库中有多少不同的单词，以及某些单词在语料库中的频率。如果语料库中有任何噪声，我们会尝试去除这些噪声。在几乎所有的自然语言处理（NLP）应用中，我们都需要进行一些基本的语料库分析，以便更好地理解我们的语料库。`nltk`为我们提供了一些内建的语料库，因此我们可以使用这些内建的语料库进行语料库分析。在开始实际操作之前，了解`nltk`中有哪些类型的语料库是非常重要的。
- en: '`nltk` has four types of corpora. Let''s look at each of them:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk`有四种类型的语料库。让我们来看一下它们：'
- en: '**Isolate corpus**: This type of corpus is a collection of text or natural
    language. Examples of this kind of corpus are `gutenberg`, `webtext`, and so on.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**孤立语料库**：这种类型的语料库是一个由文本或自然语言组成的集合。此类语料库的例子包括`gutenberg`、`webtext`等。'
- en: '**Categorized corpus**: This type of corpus is a collection of texts that are
    grouped into different types of categories.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类语料库**：这种类型的语料库是由分为不同类别的文本组成的集合。'
- en: An example of this kind of corpus is the `brown` corpus, which contains data
    for different categories such as news, hobbies, humor, and so on.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种语料库的一个例子是`brown`语料库，它包含了不同类别的数据，如新闻、爱好、幽默等。
- en: '**Overlapping corpus**: This type of corpus is a collection of texts that are
    categorized, but the categories overlap with each other. An example of this kind
    of corpus is the `reuters` corpus, which contains data that is categorized, but
    the defined categories overlap with each other.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重叠语料库**：这种类型的语料库是由分类文本组成，但这些分类之间是相互重叠的。此类语料库的一个例子是`reuters`语料库，它包含了被分类的数据，但定义的分类之间是重叠的。'
- en: More explicitly, I want to define the example of the `reuters` corpus. For example,
    if you consider different types of coconuts as one category, you can see subcategories
    of coconut-oil, and you also have cotton oil. So, in the `reuters` corpus, the
    various data categories are overlapped.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更明确地说，我想定义`reuters`语料库的例子。例如，如果你把不同类型的椰子视为一个类别，你会看到椰子油和棉花油是其子类别。因此，在`reuters`语料库中，各种数据类别是重叠的。
- en: '**Temporal corpus**: This type of corpus is a collection of the usages of natural
    language over a period of time.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间语料库**：这种类型的语料库是一个涵盖一定时间段内自然语言使用情况的集合。'
- en: An example of this kind of corpus is the `inaugural address` corpus.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种语料库的一个例子是`inaugural address`语料库。
- en: Suppose you recode the usage of a language in any city of India in 1950\. Then
    you repeat the same activity to see the usage of the language in that particular
    city in 1980 and then again in 2017\. You will have recorded the various data
    attributes regarding how people used the language and what the changes over a
    period of time were.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 假设你在1950年记录了印度某城市的语言使用情况。然后你重复相同的活动，分别记录该城市在1980年和2017年的语言使用情况。你将记录关于人们如何使用语言以及随时间变化的各项数据属性。
- en: 'Now enough of theory, let''s jump to the practical stuff. You can access the
    following links to see the codes:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，理论讲解足够了，我们来看看实际操作。你可以通过以下链接查看代码：
- en: This chapter code is on the GitHub directory URL at [https://github.com/jalajthanaki/NLPython/tree/master/ch2](https://github.com/jalajthanaki/NLPython/tree/master/ch2).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 该章节的代码位于GitHub目录URL：[https://github.com/jalajthanaki/NLPython/tree/master/ch2](https://github.com/jalajthanaki/NLPython/tree/master/ch2)。
- en: 'Follow the Python code on this URL: [https://nbviewer.jupyter.org/github/jalajthanaki/NLPython/blob/master/ch2/2_1_Basic_corpus_analysis.html](https://nbviewer.jupyter.org/github/jalajthanaki/NLPython/blob/master/ch2/2_1_Basic_corpus_analysis.html)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 请在此URL查看Python代码：[https://nbviewer.jupyter.org/github/jalajthanaki/NLPython/blob/master/ch2/2_1_Basic_corpus_analysis.html](https://nbviewer.jupyter.org/github/jalajthanaki/NLPython/blob/master/ch2/2_1_Basic_corpus_analysis.html)
- en: The Python code has basic commands of how to access corpus using the `nltk`
    API. We are using the `brown` and `gutenberg` corpora. We touch upon some of the
    basic corpus-related APIs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这段Python代码包含了如何通过`nltk` API访问语料库的基本命令。我们使用的是`brown`和`gutenberg`语料库，并简要介绍了一些与语料库相关的基本API。
- en: 'A description of the basic API attributes is given in the following table:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格描述了基本API属性：
- en: '| **API Attributes** | **Description** |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| **API属性** | **描述** |'
- en: '| `fileids()` | This results in files of the corpus |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| `fileids()` | 显示语料库中的文件 |'
- en: '| `fileids([categories])` | This results in files of the corpus corresponding
    to these categories |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| `fileids([categories])` | 显示与这些类别对应的语料库文件 |'
- en: '| `categories()` | This lists categories of the corpus |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `categories()` | 列出语料库的类别 |'
- en: '| `categories([fileids])` | This shows categories of the corpus corresponding
    to these files |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `categories([fileids])` | 显示与这些文件对应的语料库类别 |'
- en: '| `raw()` | This shows the raw content of the corpus |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `raw()` | 显示语料库的原始内容 |'
- en: '| `raw(fileids=[f1,f2,f3])` | This shows the raw content of the specified files
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| `raw(fileids=[f1,f2,f3])` | 显示指定文件的原始内容 |'
- en: '| `raw(categories=[c1,c2])` | This shows the raw content of the specified categories
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| `raw(categories=[c1,c2])` | 显示指定类别的原始内容 |'
- en: '| `words()` | This shows the words of the whole corpus |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| `words()` | 显示整个语料库的单词 |'
- en: '| `words(fileids=[f1,f2,f3])` | This shows the words of specified `fileids`
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| `words(fileids=[f1,f2,f3])` | 显示指定`fileids`的单词 |'
- en: '| `words(categories=[c1,c2])` | This shows the words of the specified categories
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| `words(categories=[c1,c2])` | 显示指定类别的单词 |'
- en: '| `sents()` | This shows the sentences of the whole corpus |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| `sents()` | 显示整个语料库的句子 |'
- en: '| `sents(fileids=[f1,f2,f3])` | This shows the sentences of specified `fileids`
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| `sents(fileids=[f1,f2,f3])` | 显示指定`fileids`的句子 |'
- en: '| `sents(categories=[c1,c2])` | This shows the sentences of the specified categories
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| `sents(categories=[c1,c2])` | 显示指定类别的句子 |'
- en: '| `abspath(fileid)` | This shows the location of the given file on disk |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| `abspath(fileid)` | 显示给定文件在磁盘上的位置 |'
- en: '| `encoding(fileid)` | This shows the encoding of the file (if known) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| `encoding(fileid)` | 显示文件的编码方式（如果已知） |'
- en: '| `open(fileid)` | This basically opens a stream for reading the given corpus
    file |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `open(fileid)` | 基本上是打开一个流以读取给定的语料库文件 |'
- en: '| `root` | This shows a path, if it is the path to the root of the locally
    installed corpus |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `root` | 显示路径，如果它是本地安装的语料库的根路径 |'
- en: '| `readme()` | This shows the contents of the `README` file of the corpus |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `readme()` | 显示语料库的`README`文件内容 |'
- en: We have seen the code for loading your customized corpus using `nltk` as well
    as done the frequency distribution for the available corpus and our custom corpus.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过了如何使用`nltk`加载自定义语料库的代码，并且对可用语料库及我们自定义语料库进行了频率分布分析。
- en: The `FreqDist` class is used to encode frequency distributions, which count
    the number of times each word occurs in a corpus.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`FreqDist`类用于编码频率分布，用于统计每个单词在语料库中出现的次数。'
- en: All `nltk` corpora are not that noisy. A basic kind of preprocessing is required
    for them to generate features out of them. Using a basic corpus-loading API of
    `nltk` helps you identify the extreme level of junk data. Suppose you have a bio-chemistry
    corpus, then you may have a lot of equations and other complex names of chemicals
    that cannot be parsed accurately using the existing parsers. You can then, according
    to your problem statement, make a decision as to whether you should remove them
    in the preprocessing stage or keep them and do some customization on parsing in
    the **part-of-speech tagging** (**POS**) level.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所有`nltk`语料库并非都很嘈杂。它们需要进行基本的预处理，才能从中生成特征。使用`nltk`的基础语料库加载API，可以帮助你识别极端的垃圾数据。假设你有一个生物化学语料库，那么可能会包含大量无法准确解析的方程式和复杂的化学名称。在这种情况下，你可以根据问题陈述决定是否在预处理阶段删除它们，或保留它们并在**词性标注**（**POS**）级别进行解析的定制。
- en: In real-life applications, corpora are very dirty. Using `FreqDist`,you can
    take a look at how words are distributed and what we should and shouldn't consider.
    At the time of preprocessing, you need to check many complex attributes such as
    whether the results of parsing, POS tagging, and sentence splitting are appropriate
    or not. We will look at all these in a detailed manner in Chapter 4, *Preprocessing*,
    and Chapter 5, *Feature Engineering and NLP Algorithms*.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，语料库通常非常杂乱。使用`FreqDist`，你可以查看单词的分布情况，了解我们应该考虑和不应该考虑的内容。在预处理阶段，你需要检查许多复杂的属性，如解析结果、词性标注和句子拆分是否恰当。我们将在第4章《预处理》和第5章《特征工程与NLP算法》中详细讲解这些内容。
- en: Note here that the corpus analysis is in terms of the technical aspect. We are
    not focusing on corpus linguistics analysis, so guys, do not confuse the two.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里语料库的分析是从技术角度进行的。我们并不专注于语料库语言学分析，因此大家不要混淆两者。
- en: 'If you want to read more on corpus linguistics analysis, refer to this URL:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进一步阅读语料库语言学分析的内容，请参考此网址：
- en: '[https://en.wikipedia.org/wiki/Corpus_linguistics](https://en.wikipedia.org/wiki/Corpus_linguistics)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Corpus_linguistics](https://en.wikipedia.org/wiki/Corpus_linguistics)'
- en: If you want to explore the `nltk` API more, the URL is [http://www.nltk.org/](http://www.nltk.org/).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进一步探索`nltk` API，网址是 [http://www.nltk.org/](http://www.nltk.org/)。
- en: Exercise
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'Calculate the number of words in the `brown` corpus with `fileID: fileidcc12`.'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '计算`brown`语料库中`fileID: fileidcc12`的单词数。'
- en: Create your own corpus file, load it using `nltk`, and then check the frequency
    distribution of that corpus.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建你自己的语料库文件，使用`nltk`加载它，然后检查该语料库的频率分布。
- en: Understanding types of data attributes
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据属性的类型
- en: 'Now let''s focus on what kind of data attributes can appear in the corpus.
    *Figure 2.3* provides you with details about the different types of data attributes:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们专注于语料库中可能出现的各种数据属性。*图 2.3*为你提供了不同类型数据属性的详细信息：
- en: '![](img/ef5c1724-9a60-484c-a823-b2318f0368db.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef5c1724-9a60-484c-a823-b2318f0368db.png)'
- en: 'Figure 2.3: Types of data attributes'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：数据属性的类型
- en: I want to give some examples of the different types of corpora. The examples
    are generalized, so you guys can understand the different type of data attributes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我想给出一些不同类型语料库的例子。这些例子是概括性的，目的是让大家理解不同类型的数据属性。
- en: Categorical or qualitative data attributes
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类或定性数据属性
- en: 'Categorical or qualitative data attributes are as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 分类或定性数据属性如下：
- en: These kinds of data attributes are more descriptive
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些类型的数据属性更具描述性
- en: Examples are our written notes, corpora provided by `nltk`, a corpus that has
    recorded different types of breeds of dogs, such as collie, shepherd, and terrier
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例包括我们的书面笔记，`nltk`提供的语料库，以及记录了不同种类狗的品种（如柯利犬、牧羊犬和梗犬）的语料库。
- en: 'There are two sub-types of categorical data attributes:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 分类数据属性有两个子类型：
- en: '**Ordinal data**:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**序数数据**：'
- en: This type of data attribute is used to measure non-numeric concepts such as
    satisfaction level, happiness level, discomfort level, and so on.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种数据属性用于衡量非数值的概念，如满意度、幸福感、不适感等。
- en: 'Consider the following questions, for example, which you''re to answer from
    the options given:'
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请考虑以下问题，作为例子，你需要从给定的选项中选择答案：
- en: 'Question 1: How do you feel today?'
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题 1：你今天感觉如何？
- en: 'Options for Question 1:'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题 1 的选项：
- en: Very bad
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常不好
- en: Bad
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不好
- en: Good
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好
- en: Happy
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开心
- en: Very happy
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常开心
- en: Now you will choose any of the given options. Suppose you choose Good, nobody
    can convert how good you feel to a numeric value.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，您将选择任何给定的选项。假设您选择了“好”，没有人能将您感受到的“好”转换成数值。
- en: All the preceding options are non-numeric concepts. Hence, they lie under the
    category of ordinal data.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有前面的选项都是非数值概念。因此，它们属于有序数据类别。
- en: 'Question 2: How would you rate our hotel service?'
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题 2：您如何评价我们的酒店服务？
- en: 'Options for Question 2:'
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题 2 的选项：
- en: Bad
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坏
- en: Average
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均
- en: Above average
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高于平均水平
- en: Good
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 好
- en: Great
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很好
- en: Now suppose you choose any of the given options. All the aforementioned options
    will measure your satisfaction level, and it is difficult to convert your answer
    to a numeric value because answers will vary from person to person.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设您选择了任意给定选项。所有上述选项都会衡量您的满意度，而由于每个人的回答不同，很难将您的答案转换为数值，因为不同人的答案会有所不同。
- en: Because one person says Good and another person says Above average, there may
    be a chance that they both feel the same about the hotel service but give different
    responses. In simple words, you can say that the difference between one option
    and the other is unknown. So you can't precisely decide the numerical values for
    these kinds of data.
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为一个人说“好”，另一个人说“高于平均水平”，可能他们对酒店服务的感受是一样的，但给出的回答却不同。简单来说，您可以说，一个选项与另一个选项之间的差异是未知的。所以，您不能精确地为这些数据决定数值。
- en: '**Nominal data**:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**名义数据**：'
- en: This type of data attribute is used to record data that doesn't overlap.
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种类型的数据属性用于记录不重叠的数据。
- en: 'Example: What is your gender? The answer is either male or female, and the
    answers are not overlapping.'
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：您的性别是什么？答案是男性或女性，且答案不重叠。
- en: 'Take another example: What is the color of your eyes? The answer is either
    black, brown, blue, or gray. (By the way, we are not considering the color lenses
    available in the market!)'
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再举一个例子：您的眼睛是什么颜色？答案是黑色、棕色、蓝色或灰色。（顺便说一下，我们不考虑市场上销售的彩色隐形眼镜！）
- en: In NLP-related applications, we will mainly deal with categorical data attributes.
    So, to derive appropriate data points from a corpus that has categorical data
    attributes is part of feature engineering. We will see more on this in Chapter
    5, *Feature Engineering and NLP Algorithms*.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在与自然语言处理相关的应用中，我们主要处理的是类别数据属性。因此，从具有类别数据属性的语料库中提取适当的数据点是特征工程的一部分。我们将在第五章*特征工程与自然语言处理算法*中详细讨论。
- en: Some corpora contain both sub-types of categorical data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一些语料库包含了这两种类别数据的子类型。
- en: Numeric or quantitative data attributes
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数值型或定量数据属性
- en: 'The following are numeric or quantitative data attributes:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数值型或定量数据属性：
- en: These kinds of data attributes are numeric and represent a measurable quantity
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这类数据属性是数值型的，表示一个可衡量的量
- en: 'Examples: Financial data, population of a city, weight of people, and so on'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：财务数据、城市人口、人体体重等
- en: 'There are two sub-types of numeric data attributes:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 数值数据属性有两个子类型：
- en: '**Continuous data**:'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续数据**：'
- en: These kinds of data attributes are continuous
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这类数据属性是连续的
- en: 'Examples: If you are recording the weight of a student, from 10 to 12 years
    of age, whatever data you collect about the student''s weight is continuous data;
    Iris flower corpus'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：如果您记录一个学生的体重，从 10 到 12 岁，您收集到的关于该学生体重的数据是连续数据；鸢尾花语料库
- en: '**Discrete data**:'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**离散数据**：'
- en: Discrete data can only take certain values
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散数据只能取某些特定的值
- en: 'Examples: If you are rolling two dice, you can only have the resultant values
    of 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, and 12; you never get 1 or 1.5 as a result
    if you are rolling two dice'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：如果您掷两个骰子，您只能得到 2、3、4、5、6、7、8、9、10、11 和 12 这些结果；如果您掷两个骰子，结果永远不会是 1 或 1.5
- en: 'Take another example: If you toss a coin, you will get either heads or tails'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再举一个例子：如果您掷硬币，您将得到正面或反面。
- en: These kinds of data attributes are a major part of analytics applications.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这类数据属性是分析应用中的主要部分。
- en: Exploring different file formats for corpora
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索语料库的不同文件格式
- en: 'Corpora can be in many different formats. In practice, we can use the following
    file formats. All these file formats are generally used to store features, which
    we will feed into our machine learning algorithms later. Practical stuff regarding
    dealing with the following file formats will be incorporated from Chapter 4, *Preprocessing*
    onward. Following are the aforementioned file formats:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库可以有许多不同的格式。实际上，我们可以使用以下文件格式。这些文件格式通常用于存储特征，我们稍后将把这些特征输入到机器学习算法中。关于如何处理以下文件格式的实际内容将在第
    4 章 *预处理* 及之后的章节中进行介绍。以下是上述文件格式：
- en: '`.txt`: This format is basically given to us as a raw dataset. The `gutenberg`
    corpus is one of the example corpora. Some of the real-life applications have
    parallel corpora. Suppose you want to make Grammarly a kind of grammar correction
    software, then you will need a parallel corpus.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.txt`：这种格式通常是作为原始数据集提供给我们的。`gutenberg` 语料库就是一个示例语料库。一些实际应用中有平行语料库。假设你想制作一个类似
    Grammarly 的语法修正软件，那么你将需要一个平行语料库。'
- en: '`.csv`: This kind of file format is generally given to us if we are participating
    in some hackathons or on Kaggle. We use this file format to save our features,
    which we will derive from raw text, and the feature `.csv` file will be used to
    train our machines for NLP applications.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.csv`：这种文件格式通常会在我们参加一些黑客马拉松或者在 Kaggle 上时提供给我们。我们使用这种格式保存从原始文本中提取的特征，而特征 `.csv`
    文件将用于训练我们的机器进行 NLP 应用。'
- en: '`.tsv`: To understand this kind of file format usage, we will take an example.
    Suppose you want to make an NLP system that suggests where we should put a comma
    in a sentence. In this case, we cannot use the `.csv` file format to store our
    features because some of our feature attributes contain commas, and this will
    affect the performance when we start processing our feature file. You can also
    use any customized delimiter as well. You can put \t, ||, and so on for ease of
    further processing.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.tsv`：为了理解这种文件格式的用法，我们将举一个例子。假设你想制作一个 NLP 系统，建议我们在句子中应该放置逗号的位置。在这种情况下，我们不能使用
    `.csv` 文件格式来存储特征，因为我们的一些特征属性包含逗号，这会在开始处理特征文件时影响性能。你也可以使用任何自定义的分隔符。你可以使用 \t、||
    等，方便后续处理。'
- en: '`.xml`: Some well-known NLP parsers and tools provide results in the `.xml`
    format. For example, the Stanford CoreNLP toolkit provides parser results in the
    `.xml` format. This kind of file format is mainly used to store the results of
    NLP applications.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.xml`：一些知名的 NLP 解析器和工具会以 `.xml` 格式提供结果。例如，斯坦福 CoreNLP 工具包会以 `.xml` 格式提供解析结果。这种文件格式主要用于存储
    NLP 应用的结果。'
- en: '`.json`: The Stanford CoreNLP toolkit provides its results in the `.json` format.
    This kind of file format is mainly used to store results of NLP applications,
    and it is easy to display and integrate with web applications.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.json`：斯坦福 CoreNLP 工具包以 `.json` 格式提供其结果。这种文件格式主要用于存储 NLP 应用的结果，并且易于展示和与网页应用集成。'
- en: '`LibSVM`: This is one of the special file formats. Refer to the following *Figure
    2.4***:**'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LibSVM`：这是其中一种特殊的文件格式。参考下面的 *图 2.4*：'
- en: '![](img/a2f37adf-81c8-4133-95ed-33fb9385ec83.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2f37adf-81c8-4133-95ed-33fb9385ec83.png)'
- en: 'Figure 2.4: LibSVM file format example'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：LibSVM 文件格式示例
- en: '`LibSVM` allows for sparse training data. The non-zero values are the only
    ones that are included in the training dataset. Hence, the index specifies the
    column of the instance data (feature index). To convert from a conventional dataset,
    just iterate over the data, and if the value of `X(i,j)` is non-zero, print `j
    + 1: X(i,j)`.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LibSVM` 支持稀疏训练数据。只有非零值会被包含在训练数据集中。因此，索引指定了实例数据的列（特征索引）。要从传统数据集转换，只需遍历数据，如果
    `X(i,j)` 的值非零，则输出 `j + 1: X(i,j)`。'
- en: '`X(i,j)`: This is a sparse matrix:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X(i,j)`：这是一个稀疏矩阵：'
- en: If the value of `X(i,j)` is equal to non-zero, include it in the `LibSVM` format
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `X(i,j)` 的值不等于零，则将其包含在 `LibSVM` 格式中
- en: '`j+1`: This is the value of `X(i,j)`, where `j` is the column index of the
    matrix starting with `0`, so we add `1`'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`j+1`：这是 `X(i,j)` 的值，其中 `j` 是矩阵的列索引，从 `0` 开始，因此我们加上 `1`'
- en: Otherwise, do not include it in the `LibSVM` format
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，请不要将其包含在 `LibSVM` 格式中
- en: 'Let''s take the following example:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们来看下面的例子：
- en: 'Example: 1 5:1 7:1 14:1 19:1'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例：1 5:1 7:1 14:1 19:1
- en: Here, *1* is the class or label
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里，*1* 是类别或标签
- en: 'In the preceding example, let''s focus on *5:1*, where *5* is the key, and
    *1* is the value; *5:1* is the key : value pair'
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上述例子中，让我们关注 *5:1*，其中 *5* 是键，*1* 是值；*5:1* 是键值对
- en: '*5* is the column number or data attribute number, which is the key and is
    in the `LibSVM` format; we are considering only those data columns that contain
    non-zero values, so here, *1* is the value'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*5* 是列号或数据属性号，是关键字段，并采用 `LibSVM` 格式；我们仅考虑包含非零值的数据列，因此在这里，*1* 是该值。'
- en: The values of parameters with indexes 1, 2, 3, 4, 6, and others unmentioned
    are 0s, so we are not including these in our example
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数的值中，索引为 1、2、3、4、6 以及其他未提及的值均为 0，因此我们在示例中未包含这些。
- en: This kind of data format is used in Apache Spark to train your data, and you
    will learn how to convert text data to the `LibSVM` format from Chapter 5, *Feature
    Engineering and NLP Algorithms* onwards.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种数据格式用于 Apache Spark 训练数据，您将从第 5 章 *特征工程与 NLP 算法* 开始学习如何将文本数据转换为 `LibSVM` 格式。
- en: '**Customized format**: You can make your feature file using the customized
    file format. (Refer to the `CoNLL` dataset.) It is kind of a customized file format.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义格式**：您可以使用自定义文件格式创建特征文件。（参考 `CoNLL` 数据集。）这是一种自定义的文件格式。'
- en: 'There are many different `CoNLL` formats since `CoNLL` is a different shared
    task each year. *Figure 2.5* shows a data sample in the `CoNLL` format:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于 `CoNLL` 每年都有不同的共享任务，所以有许多不同的 `CoNLL` 格式。*图 2.5* 显示了一个 `CoNLL` 格式的数据示例：
- en: '![](img/0a1793c5-fe04-49a4-8f14-eee22a26f6a9.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a1793c5-fe04-49a4-8f14-eee22a26f6a9.png)'
- en: 'Figure 2.5: Data sample in CoNLL format'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：CoNLL 格式的数据示例
- en: Resources for accessing free corpora
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问免费语料库的资源
- en: Getting the corpus is a challenging task, but in this section, I will provide
    you with some of the links from which you can download a free corpus and use it
    to build NLP applications.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 获取语料库是一项具有挑战性的任务，但在本节中，我将提供一些链接，您可以通过这些链接下载免费语料库，并将其用于构建 NLP 应用程序。
- en: 'The `nltk` library provides some inbuilt `corpus`. To list down all the corpus
    names, execute the following commands:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk` 库提供了一些内置的 `corpus`。要列出所有语料库名称，请执行以下命令：'
- en: '[PRE0]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In *Figure 2.2*, you can see the output of the preceding code; the highlighted
    part indicates the name of the corpora that are already installed:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 2.2* 中，您可以看到前面代码的输出；高亮部分表示已经安装的语料库的名称：
- en: '![](img/3da1717d-bc7b-4cdf-853f-920898bfb117.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3da1717d-bc7b-4cdf-853f-920898bfb117.png)'
- en: 'Figure 2.2: List of all available corpora in nltk'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：nltk 中所有可用语料库的列表
- en: 'If you guys want to use IDE to develop an NLP application using Python, you
    can use the PyCharm community version. You can follow its installation steps by
    clicking on the following URL: [https://github.com/jalajthanaki/NLPython/blob/master/ch2/Pycharm_installation_guide.md](https://github.com/jalajthanaki/NLPython/blob/master/ch2/Pycharm_installation_guide.md)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你们想使用 IDE 开发一个 Python NLP 应用程序，可以使用 PyCharm 社区版。点击以下 URL 可以查看安装步骤：[https://github.com/jalajthanaki/NLPython/blob/master/ch2/Pycharm_installation_guide.md](https://github.com/jalajthanaki/NLPython/blob/master/ch2/Pycharm_installation_guide.md)
- en: 'If you want to explore more corpus resources, take a look at *Big Data: 33
    Brilliant and Free Data Sources for 2016*, Bernard Marr ([https://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/#53369cd5b54d](https://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/#53369cd5b54d)).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您想探索更多语料库资源，可以查看 *Big Data: 33 Brilliant and Free Data Sources for 2016*，Bernard
    Marr （[https://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/#53369cd5b54d](https://www.forbes.com/sites/bernardmarr/2016/02/12/big-data-35-brilliant-and-free-data-sources-for-2016/#53369cd5b54d)）。'
- en: Until now, we have looked at a lot of basic stuff. Now let me give you an idea
    of how we can prepare a dataset for a natural language processing applications,
    which will be developed with the help of machine learning.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了很多基础内容。现在让我为您展示如何为自然语言处理应用程序准备数据集，该应用程序将借助机器学习进行开发。
- en: Preparing a dataset for NLP applications
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 NLP 应用程序准备数据集
- en: 'In this section, we will look at the basic steps that can help you prepare
    a dataset for NLP or any data science applications. There are basically three
    steps for preparing your dataset, given as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些基本步骤，帮助您为 NLP 或任何数据科学应用程序准备数据集。准备数据集的基本步骤如下：
- en: Selecting data
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择数据
- en: Preprocessing data
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Transforming data
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换数据
- en: Selecting data
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择数据
- en: Suppose you are working with world tech giants such as Google, Apple, Facebook,
    and so on. Then you could easily get a large amount of data, but if you are not
    working with giants and instead doing independent research or learning some NLP
    concepts, then how and from where can you get a dataset? First, decide what kind
    of dataset you need as per the NLP application that you want to develop. Also,
    consider the end result of the NLP application that you are trying to build. If
    you want to make a chatbot for the healthcare domain, you should not use a dialog
    dataset of banking customer care. So, understand your application or problem statement
    thoroughly.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在与全球科技巨头如Google、Apple、Facebook等公司合作，那么你可以轻松获得大量数据。但是，如果你不是与这些巨头合作，而是在进行独立研究或学习一些NLP概念，那么你该如何以及从哪里获取数据集呢？首先，根据你希望开发的NLP应用，确定你需要什么样的数据集。同时，考虑你所要构建的NLP应用的最终结果。如果你想为医疗健康领域制作一个聊天机器人，你就不应该使用银行客服对话数据集。因此，彻底理解你的应用或问题陈述是非常重要的。
- en: 'You can use the following links to download free datasets:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下链接下载免费数据集：
- en: '[https://github.com/caesar0301/awesome-public-datasets](https://github.com/caesar0301/awesome-public-datasets).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/caesar0301/awesome-public-datasets](https://github.com/caesar0301/awesome-public-datasets)。'
- en: '[https://www.kaggle.com/datasets](https://www.kaggle.com/datasets).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)。'
- en: '[https://www.reddit.com/r/datasets/](https://www.reddit.com/r/datasets/).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.reddit.com/r/datasets/](https://www.reddit.com/r/datasets/)。'
- en: You can also use the Google Advanced Search feature, or you can use Python web
    scraping libraries such as `beautifulsoup` or `scrapy`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用Google高级搜索功能，或者使用Python的网页抓取库，例如`beautifulsoup`或`scrapy`。
- en: After selecting the dataset as per the application, you can move on to the next
    step.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在根据应用选择好数据集后，你可以继续进行下一步。
- en: Preprocessing the dataset
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理数据集
- en: In this step, we will do some basic data analysis, such as which attributes
    are available in the dataset. This stage has three sub-stages, and we will look
    at each of them. You will find more details about the preprocessing stage in Chapter
    4, *Preprocessing*. Here, I'll give you just the basic information.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们将进行一些基本的数据分析，例如数据集包含了哪些属性。这个阶段有三个子阶段，我们将逐一探讨。你将在第4章中找到更多关于预处理阶段的细节，*预处理*。在这里，我会给你提供一些基本信息。
- en: Formatting
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 格式化
- en: In this step, generate the dataset format that you feel most comfortable working
    with. If you have a dataset in the JSON format and you feel that you are most
    comfortable working with CSV, then convert the dataset from JSON to CSV.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，生成你最习惯使用的数据集格式。如果你有一个JSON格式的数据集，而你觉得使用CSV格式更为方便，那么就将数据集从JSON格式转换为CSV格式。
- en: Cleaning
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理
- en: In this step, we clean the data. If the dataset has missing values, either delete
    that data record or replace it with the most appropriate nearest value. If you
    find any unnecessary data attributes, you can remove them as well. Suppose you
    are making a grammar correction system, then you can remove the mathematical equations
    from your dataset because your grammar correction application doesn't use equations.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们将清理数据。如果数据集中有缺失值，可以删除该数据记录，或者用最合适的邻近值替换它。如果发现有不必要的数据属性，也可以将其删除。假设你正在做一个语法修正系统，那么你可以删除数据集中的数学方程式，因为你的语法修正应用不需要方程式。
- en: Sampling
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抽样
- en: In this stage, you can actually try to figure out which of the available data
    attributes our present dataset has and which of the data attributes can be derived
    by us. We are also trying to figure out what the most important data attributes
    are as per our application. Suppose we are building a chatbot. We will then try
    to break down sentences into words so as to identify the keywords of the sentence.
    So, the word-level information can be derived from the sentence, and both word-level
    and sentence level information are important for the chatbot application. As such,
    we do not remove sentences, apart from junk sentences. Using sampling, we try
    to extract the best data attributes that represent the overall dataset very well.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你实际上可以尝试弄清楚我们当前数据集中有哪些可用的数据属性，以及哪些数据属性可以由我们推导出来。我们还在试图找出根据我们的应用最重要的数据属性。假设我们正在构建一个聊天机器人，我们将尝试将句子拆解成单词，以识别句子的关键词。因此，单词级别的信息可以从句子中推导出来，单词级别和句子级别的信息对聊天机器人应用来说都非常重要。因此，除去无用的句子外，我们不会删除任何句子。通过抽样，我们尝试提取那些能够很好地代表整体数据集的最佳数据属性。
- en: Now we can look at the last stage, which is the transformation stage.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看看最后一个阶段，即转换阶段。
- en: Transforming data
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换
- en: In this stage, we will apply some feature engineering techniques that help us
    convert the text data into numeric data so the machine can understand our dataset
    and try to find out the pattern in the dataset. So, this stage is basically a
    data manipulation stage. In the NLP domain, for the transformation stage, we can
    use some encoding and vectorization techniques. Don't get scared by the terminology.
    We will look at all the data manipulation techniques and feature extraction techniques
    in Chapter 5, *Feature Engineering and NLP Algorithms* and Chapter 6, *Advance
    Feature Engineering and NLP Algorithms*.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段，我们将应用一些特征工程技术，帮助我们将文本数据转换为数值数据，使机器能够理解我们的数据集，并尝试找出数据集中的模式。所以，这一阶段基本上是数据处理阶段。在NLP领域，转化阶段，我们可以使用一些编码和向量化技术。别被术语吓到，我们将在第五章*特征工程与NLP算法*和第六章*高级特征工程与NLP算法*中详细讨论所有的数据处理技术和特征提取技术。
- en: All the preceding stages are basic steps to prepare the dataset for any NLP
    or data science related applications. Now, let's see how you can generate data
    using web scraping.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 所有前面的步骤是为任何NLP或数据科学相关应用准备数据集的基本步骤。现在，让我们看看如何通过网页抓取生成数据。
- en: Web scraping
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网页抓取
- en: To develop a web scraping tool, we can use libraries such as `beautifulsoup`
    and `scrapy`. Here, I'm giving some of the basic code for web scraping.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要开发一个网页抓取工具，我们可以使用`beautifulsoup`和`scrapy`等库。这里，我给出一些网页抓取的基础代码。
- en: 'Take a look at the code snippet in *Figure 2.6,* which is used to develop a
    basic web scraper using `beautifulsoup`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 查看*图2.6*中的代码片段，该片段用于开发一个使用`beautifulsoup`的基本网页抓取工具：
- en: '![](img/b319fd5a-5d4e-4fc7-a6e0-0f461633119e.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b319fd5a-5d4e-4fc7-a6e0-0f461633119e.png)'
- en: 'Figure 2.6: Basic web scraper tool using beautifulsoup'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：使用beautifulsoup的基本网页抓取工具
- en: 'The following *Figure 2.7* demonstrates the output:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下*图2.7*展示了输出结果：
- en: '![](img/f53b1140-5385-448d-a2f1-3e45601e6d69.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f53b1140-5385-448d-a2f1-3e45601e6d69.png)'
- en: 'Figure 2.7: Output of basic web scraper using beautifulsoup'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：使用beautifulsoup的基本网页抓取工具的输出
- en: 'You can find the installation guide for `beautifulsoup` and `scrapy` at this
    link:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个链接中找到`beautifulsoup`和`scrapy`的安装指南：
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch2/Chapter_2_Installation_Commands.txt](https://github.com/jalajthanaki/NLPython/blob/master/ch2/Chapter_2_Installation_Commands.txt).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/blob/master/ch2/Chapter_2_Installation_Commands.txt](https://github.com/jalajthanaki/NLPython/blob/master/ch2/Chapter_2_Installation_Commands.txt)。'
- en: 'You can find the code at this link:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过这个链接找到代码：
- en: '[https://github.com/jalajthanaki/NLPython/blob/master/ch2/2_2_Basic_webscraping_byusing_beautifulsuop.py](https://github.com/jalajthanaki/NLPython/blob/master/ch2/2_2_Basic_webscraping_byusing_beautifulsuop.py).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/blob/master/ch2/2_2_Basic_webscraping_byusing_beautifulsuop.py](https://github.com/jalajthanaki/NLPython/blob/master/ch2/2_2_Basic_webscraping_byusing_beautifulsuop.py)。'
- en: If you get any warning while running the script, it will be fine; don't worry
    about warnings.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在运行脚本时收到任何警告，没关系，不用担心警告。
- en: Now, let's do some web scraping using `scrapy`. For that, we need to create
    a new scrapy project.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`scrapy`进行一些网页抓取。为此，我们需要创建一个新的scrapy项目。
- en: 'Follow the command to create the scrapy project. Execute the following command
    on your terminal:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 按照命令创建scrapy项目。请在终端中执行以下命令：
- en: '[PRE1]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'I''m creating a scrapy project with the `web_scraping_test` name; the command
    is as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在创建一个名为`web_scraping_test`的scrapy项目，命令如下：
- en: '[PRE2]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once you execute the preceding command, you can see the output as shown in
    *Figure 2.8*:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦执行前面的命令，你将看到如*图2.8*所示的输出：
- en: '![](img/772c2f47-79c0-4e47-a8ac-41d87f2cfaf3.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/772c2f47-79c0-4e47-a8ac-41d87f2cfaf3.png)'
- en: 'Figure 2.8: Output when you create a new scrapy project'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：创建新scrapy项目时的输出
- en: 'After creating a project, perform the following steps:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 创建项目后，执行以下步骤：
- en: Edit your `items.py` file, which has been created already.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编辑你已经创建的`items.py`文件。
- en: Create the `WebScrapingTestspider` file inside the `spiders` directory.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`spiders`目录下创建`WebScrapingTestspider`文件。
- en: 'Go to the website page that you want to scrape, and select `xpath` of the element.
    You can read more on the `xpath` selector by clicking at this link:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入你想抓取的网页，选择该元素的`xpath`。你可以通过点击此链接阅读更多关于`xpath`选择器的信息：
- en: '[https://doc.scrapy.org/en/1.0/topics/selectors.html](https://doc.scrapy.org/en/1.0/topics/selectors.html)[](https://doc.scrapy.org/en/1.0/topics/selectors.html)'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://doc.scrapy.org/en/1.0/topics/selectors.html](https://doc.scrapy.org/en/1.0/topics/selectors.html)[](https://doc.scrapy.org/en/1.0/topics/selectors.html)'
- en: 'Take a look at the code snippet in *Figure 2.9.* Its code is available at the
    GitHub URL:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看*图 2.9*中的代码片段。其代码可在GitHub网址获取：
- en: '[https://github.com/jalajthanaki/NLPython/tree/master/web_scraping_test](https://github.com/jalajthanaki/NLPython/tree/master/web_scraping_test)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/jalajthanaki/NLPython/tree/master/web_scraping_test](https://github.com/jalajthanaki/NLPython/tree/master/web_scraping_test)'
- en: '![](img/d1164bb1-7d61-4f42-8ecd-4e4b24036caf.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1164bb1-7d61-4f42-8ecd-4e4b24036caf.png)'
- en: 'Figure 2.9: The items.py file where we have defined items we need to scrape'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9：我们定义了需要抓取的项目的items.py文件
- en: '*Figure 2.10* is used to develop a basic web scraper using `scrapy`:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.10*用于使用`scrapy`开发一个基础网页抓取工具：'
- en: '![](img/087e8c20-48b8-4769-848c-6669c2aa842a.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![](img/087e8c20-48b8-4769-848c-6669c2aa842a.png)'
- en: 'Figure 2.10: Spider file containing actual code'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10：包含实际代码的蜘蛛文件
- en: '*Figure 2.11* demonstrates the output, which is in the form of a CSV file:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.11*演示了输出，格式为CSV文件：'
- en: '![](img/10379c97-edd6-44dd-a3b8-614e61d26f2e.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![](img/10379c97-edd6-44dd-a3b8-614e61d26f2e.png)'
- en: 'Figure 2.11: Output of scraper is redirected to a CSV file'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11：抓取器的输出被重定向到CSV文件
- en: 'If you get any SSL-related warnings, refer to the answer at this link:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你收到任何与SSL相关的警告，请参考此链接中的答案：
- en: '[https://stackoverflow.com/questions/29134512/insecureplatformwarning-a-true-sslcontext-object-is-not-available-this-prevent](https://stackoverflow.com/questions/29134512/insecureplatformwarning-a-true-sslcontext-object-is-not-available-this-prevent)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://stackoverflow.com/questions/29134512/insecureplatformwarning-a-true-sslcontext-object-is-not-available-this-prevent](https://stackoverflow.com/questions/29134512/insecureplatformwarning-a-true-sslcontext-object-is-not-available-this-prevent)'
- en: You can develop a web scraper that bypasses AJAX and scripts, but you need to
    be very careful when you do this because you need to keep in mind that you are
    not doing anything unethical. So, here, we are not going to cover the part on
    bypassing AJAX and scripts and scraping data. Out of curiosity, you can search
    on the web how people actually do this. You can use the `Selenium` library to
    do automatic clicking to perform web events.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以开发一个绕过AJAX和脚本的网页抓取工具，但你在做这个时需要非常小心，因为你必须确保自己没有做任何不道德的事情。因此，我们在这里不会涉及如何绕过AJAX和脚本来抓取数据的部分。如果你感到好奇，可以在网上查找别人是如何做到这一点的。你可以使用`Selenium`库来自动点击执行网页事件。
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw that a corpus is the basic building block for NLP applications.
    We also got an idea about the different types of corpora and their data attributes.
    We touched upon the practical analysis aspects of a corpus. We used the `nltk`
    API to make corpus analysis easy.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们了解到语料库是自然语言处理应用的基本构建块。我们还了解了不同类型的语料库及其数据属性。我们简要探讨了语料库的实际分析方面。我们使用`nltk`
    API来简化语料库分析。
- en: In the next chapter, we will address the basic and effective aspects of natural
    language using linguistic concepts such as parts of speech, lexical items, and
    tokenization, which will further help us in preprocessing and feature engineering.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过使用词性、词汇项和分词等语言学概念，探讨自然语言的基本有效方面，这将进一步帮助我们在数据预处理和特征工程中。
