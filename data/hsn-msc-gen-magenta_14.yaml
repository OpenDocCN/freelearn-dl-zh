- en: Assessments
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: 'Chapter 1: Introduction to Magenta and Generative Art'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章：Magenta与生成艺术简介
- en: Randomness.
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机性。
- en: Markov chain.
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 马尔可夫链。
- en: Algorave.
  id: totrans-4
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Algorave。
- en: '**Long short-term memory** (**LSTM**).'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）。'
- en: Autonomous systems generate music without operator input; assisting music systems
    will complement an artist while working.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自主系统生成音乐，无需操作员输入；辅助音乐系统将在创作时补充艺术家的工作。
- en: 'Symbolic: sheet music, MIDI, MusicXML, AbcNotation. Sub-symbolic: raw audio
    (waveform), spectrogram.'
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 符号化：乐谱、MIDI、MusicXML、AbcNotation。子符号化：原始音频（波形）、频谱图。
- en: '"Note On" and "Note Off" timing, pitch between 1 and 127 kHz, velocity, and
    channel.'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '"音符开启"与"音符关闭"的时序、音高范围为1到127 kHz、速度和通道。'
- en: At a sample rate of 96 kHz, the Nyquist frequency is 96 kHz/2 = 48 kHz and the
    frequency range is 0 to 48 kHz. This is worse for listening to audio since 28
    kHz of audio is lost on the ear (remember anything over 20 khz cannot be heard),
    and that sampling rate is not properly supported by much audio equipment. It is
    useful in recording and audio editing though.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在96 kHz的采样率下，奈奎斯特频率为96 kHz/2 = 48 kHz，频率范围为0到48 kHz。这对于听音频来说效果较差，因为28 kHz的音频在耳朵上是听不见的（记住，超过20
    kHz的声音是无法听到的），而且这种采样率并不被很多音频设备正确支持。不过，在录音和音频编辑中它还是有用的。
- en: A single musical note, A4, is played for 1 second loudly.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 单个音乐音符A4被响亮地演奏1秒钟。
- en: Drums, voice (melody), harmony (polyphony), and interpolation and manipulation.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鼓、声部（旋律）、和声（复音）、插值和操作。
- en: 'Chapter 2: Generating Drum Sequences with the Drums RNN'
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章：使用鼓RNN生成鼓序列
- en: Given a current sequence, predict the score for the next note, then do a prediction
    for each step you want to generate.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个当前序列，预测下一个音符的乐谱，然后对每一个你希望生成的步骤进行预测。
- en: (1) RNNs operate on sequences of vectors, for the input and output, which is
    good for sequential data such as a music score, and (2) keep an internal state
    composed of the previous output steps, which is good for doing a prediction based
    on past inputs, not only the current input.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1) RNN（递归神经网络）对向量序列进行操作，用于输入和输出，这对于像乐谱这样的序列数据非常适用；(2) 保持一个由前一输出步骤组成的内部状态，这对于基于过去输入做出预测非常有用，而不仅仅是基于当前输入。
- en: (1) First, the hidden layer will get *h(t + 1)*, which is the output of the
    previous hidden layer, and (2) it will also receive *x(t + 2)*, which is the input
    of the current step.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1) 首先，隐藏层将得到 *h(t + 1)*，即前一个隐藏层的输出；(2) 它还将接收 *x(t + 2)*，即当前步骤的输入。
- en: 'The number of bars generated will be 2 bars, or 32 steps, since we have 16
    steps per bar. At 80 QPM, each step takes 0.1875 seconds, because you take the
    number of seconds in a minute, divide by the QPM, and divide by the number of
    steps per quarter: 60 / 80 / 4 = 0.1875\. Finally, you have 32 steps at 0.1875
    seconds each, so the total time is 32 * 0.1875 = 6 seconds.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成的音符小节数将是2小节，或者32个步骤，因为每小节有16个步骤。在80 QPM下，每个步骤的时长为0.1875秒，因为你将一分钟的秒数除以QPM，再除以每小节的步骤数：60
    / 80 / 4 = 0.1875。最后，你有32个步骤，每个步骤时长0.1875秒，所以总时间为32 * 0.1875 = 6秒。
- en: Increasing the branch factor reduces the randomness, since you have more branches
    to choose from when selecting the best branch, but increasing the temperature
    will increase the randomness. Doing both at the same time will cancel out each
    other, we just don't know in what proportions.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加分支因子会减少随机性，因为你有更多的分支可以选择最佳分支，但增加温度会增加随机性。两者同时进行会互相抵消，只是我们不知道这种抵消的比例。
- en: At each step, the algorithm will generate four branches and keep two. At the
    last iteration, the beam search will search for the best branch in the graph by
    checking the remaining two nodes at each level multiplied by the number of steps
    of the generated graph (also the height of the tree), which is three. So we go
    through 2 * 3 nodes = 6 nodes.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个步骤中，算法将生成四个分支并保留两个。在最后一次迭代中，束搜索将通过检查每一层剩余的两个节点与生成图的步骤数（即树的高度）相乘来搜索图中最佳的分支，这个数是三。因此，我们会遍历2
    * 3个节点 = 6个节点。
- en: '`NoteSequence`.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`NoteSequence`。'
- en: 'The MIDI notes maps to the following classes: 36 maps to 0 (kick drum), 40
    maps to 1 (snare drum), 42 maps to 2 (closed hi-hat). The resulting index is calculated
    with 2⁰ + 2¹ + 2² = 7, so the resulting vector will be *v = [0, 0, 0, 0, 0, 0,
    1, 0, ... ]*.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MIDI音符映射到以下类别：36映射到0（踢鼓），40映射到1（军鼓），42映射到2（闭合高帽）。计算得到的索引为2⁰ + 2¹ + 2² = 7，因此得到的向量为
    *v = [0, 0, 0, 0, 0, 0, 1, 0, ... ]*。
- en: 'The bit representation of index 131 is `10000011` (in Python, you can use `"{0:b}".format(131)`
    to get that). This is represented as 2⁰ + 2¹ + 2⁷, which gives us the following
    classes: 0 (drum kit), 1 (snare drum) and 7 (crash cymbal). We then arbitrarily
    take the first element of each class: *{36, 38, 49}*.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 索引131的位表示是`10000011`（在Python中，你可以使用`"{0:b}".format(131)`来获取）。这是 2⁰ + 2¹ + 2⁷，得到以下类别：0（鼓组）、1（小军鼓）和7（撞击钹）。然后，我们任意选择每个类别的第一个元素：
    *{36, 38, 49}*。
- en: 'Chapter 3: Generating Polyphonic Melodies'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章：生成复调旋律
- en: Vanishing gradients (values get multiplied by small values in each RNN step)
    and exploding gradients are common RNN problems that occur when training during
    the backpropagation step. LSTM provides a dedicated cell state that is modified
    by forget, input, and output gates to alleviate those problems.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 消失梯度（在每个RNN步骤中，值被小值相乘）和梯度爆炸是常见的RNN问题，通常发生在反向传播步骤的训练过程中。LSTM提供了一个专用的细胞状态，通过遗忘门、输入门和输出门进行修改，以缓解这些问题。
- en: '**Gated** **recurrent** **units** (**GRUs**) are simpler but less expressive
    memory cells, where the forget and input gates are combined into a single **update
    gate**.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**门控** **递归** **单元**（**GRUs**）是更简单但表达力较弱的记忆单元，其中遗忘门和输入门合并为一个单一的**更新门**。'
- en: For a 3/4 time signature, you need 3 steps per quarter note, times 4 steps per
    quarter note, which equals 12 steps per bar. For a binary step counter to count
    to 12, you need 5 bits (like for 4/4 time) that will only count to 12\. For 3
    lookbacks, you'll need to look at the past 3 bars, with each bar being 12 steps,
    so you have *[36, 24, 12]*.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于3/4拍的节奏标记，你需要每四分之一音符3步，每四分之一音符4步，总共每小节12步。对于一个二进制步数计数器，要计数到12，你需要5个比特位（像4/4拍那样），它们只能计数到12。对于3个回顾，你需要查看过去3小节，每小节12步，所以你得到
    *[36, 24, 12]*。
- en: The resulting vector is the sum of the previous step vectors, each applied with
    the attention mask, so we have 0.1 applied to *[1, 0, 0, 0]*, plus 0.5 applied
    to *[0, 1, 0, x]* giving *[0.10, 0.50, 0.00, 0.25]*. The value of x is 0.5, because
    0.5 times 0.5 equals 0.25.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果向量是前一步向量的和，每个向量都应用了注意力掩码，因此我们对 *[1, 0, 0, 0]* 应用了 0.1，对 *[0, 1, 0, x]* 应用了
    0.5，得到了 *[0.10, 0.50, 0.00, 0.25]*。x 的值是 0.5，因为 0.5 乘以 0.5 等于 0.25。
- en: A C major chord of one quarter note.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个C大调和弦，时值为一个四分音符。
- en: In Polyphony RNN, there are no note end events. If no `CONTINUED_NOTE` is used
    for a pitch in a step, it stops the note. In Perfomance RNN, a `NOTE_END 56` event
    would be used.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Polyphony RNN中，没有音符结束事件。如果在一个步骤中没有使用`CONTINUED_NOTE`来表示某个音高，则该音符会停止。在Performance
    RNN中，则会使用`NOTE_END 56`事件。
- en: (1) Expressive timing using `TIME_SHIFT` events, present in all Performance
    RNN models, for example in the `performance` configuration, and (2) dynamic play
    using `VELOCITY` events, present in the `performance_with_dynamics` configuration.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1) 使用`TIME_SHIFT`事件表达的时序性，这些事件在所有的Performance RNN模型中都有，例如在`performance`配置中，以及(2)
    使用`VELOCITY`事件的动态播放，这些事件出现在`performance_with_dynamics`配置中。
- en: It will change the number of iterations during the RNN steps call. A bigger
    number of notes per seconds will ask for more RNN steps during the generation.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将在RNN步骤调用期间改变迭代次数。每秒钟的音符数量越大，在生成过程中所需的RNN步骤就越多。
- en: 'Chapter 4: Latent Space Interpolation with MusicVAE'
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章：使用MusicVAE进行潜在空间插值
- en: The main use is dimensionality reduction, to force the network to learn important
    features, making it possible to reconstruct the original input. The downside of
    AE is that the latent space represented by the hidden layer is not continuous,
    making it hard to sample since the decoder won't be able to make sense of some
    of the points.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主要用途是降维，迫使网络学习重要特征，从而使得重构原始输入成为可能。AE的缺点是隐藏层表示的潜在空间不是连续的，难以进行采样，因为解码器无法理解某些点。
- en: The reconstruction loss penalizes the network when it creates outputs that are
    different from the input.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重建损失在网络生成与输入不同的输出时进行惩罚。
- en: In VAE, the latent space is continuous and smooth, making it possible to sample
    any point of the space and interpolate between two points. It is achieved by having
    the latent variables follow a probability distribution of P(z), often a Gaussian
    distribution.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在VAE中，潜在空间是连续和平滑的，使得可以对空间中的任何点进行采样，并在两点之间进行插值。它是通过让潜在变量遵循 P(z) 的概率分布来实现的，通常是高斯分布。
- en: The KL divergence measures how much two probability distributions diverge from
    each other. When combined with the reconstruction loss, it centers the clusters
    around 0 and makes them more or less close to one another.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KL散度衡量两个概率分布之间的差异。当与重构损失结合时，它会将聚类集中在0附近，并使它们更接近或更远离彼此。
- en: We sample the normal distribution using `np.random.randn(4, 512)`.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`np.random.randn(4, 512)`来采样正态分布。
- en: Calculate the direction between two points in the latent space.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算潜在空间中两点之间的方向。
- en: 'Chapter 5: Audio Generation with NSynth and GANSynth'
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章：使用NSynth和GANSynth进行音频生成
- en: You have to handle 16,000 samples per second (at least) and keep track of the
    general structure at a bigger time scale.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要处理每秒16,000个样本（至少），并在更大的时间尺度上跟踪整体结构。
- en: NSynth is a WaveNet-style autoencoder that learns its own temporal embedding,
    making it possible to capture long term structure, and providing access to a useful
    hidden space.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NSynth是一个WaveNet风格的自编码器，能够学习自己的时间嵌入，使得捕捉长期结构成为可能，并提供访问有用的隐藏空间。
- en: The colors in the rainbowgram are the 16 dimensions of the temporal embedding.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 彩虹图中的颜色代表了时间嵌入的16个维度。
- en: Check the `timestretch` method in the `audio_utils.py` file in the chapter's
    code.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请查看章节代码中`audio_utils.py`文件中的`timestretch`方法。
- en: GANSynth uses upsampling convolutions, making the training and generation processing
    in parallel possible for the entire audio sample.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GANSynth使用上采样卷积，使得整个音频样本的训练和生成处理可以并行进行。
- en: You need to sample the random normal distribution using `np.random.normal(size=[10,
    256])`, where 10 is the number of sampled instruments, and 256 is the size of
    the latent vector (given by the `latent_vector_size` configuration).
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要使用`np.random.normal(size=[10, 256])`来采样随机正态分布，其中10是采样的乐器数量，256是潜在向量的大小（由`latent_vector_size`配置给出）。
- en: 'Chapter 6: Data Preparation for Training'
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章：训练数据准备
- en: MIDI is not a text format, so it is harder to use and modify, but it is extremely
    common. MusicXML is rather rare and cumbersome but has the advantage of being
    in text format. ABCNotation is also rather rare, but has the advantage of being
    in text format and closer to sheet music.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MIDI不是文本格式，因此更难使用和修改，但它非常常见。MusicXML相对较少见且笨重，但它的优势在于采用文本格式。ABCNotation也相对罕见，但其优势在于是文本格式，并且更接近乐谱。
- en: Use the code from `chapter_06_example_08.py`, and change the `program=43` in
    the extraction.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`chapter_06_example_08.py`中的代码，并在提取过程中更改`program=43`。
- en: There are 1,116 rock songs in LMD and 3,138 songs for jazz, blues, and country.
    Refer to `chapter_06_example_02.py` and `chapter_06_example_03.py` to see how
    to make statistics with genre information.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LMD中有1,116首摇滚歌曲，3,138首爵士、蓝调和乡村歌曲。请参考`chapter_06_example_02.py`和`chapter_06_example_03.py`，了解如何根据音乐风格信息进行统计。
- en: Use the `RepeatSequence` class in `melody_rnn_pipeline_example.py`.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`melody_rnn_pipeline_example.py`中使用`RepeatSequence`类。
- en: Use the code from `chapter_06_example_09.py`. Yes, we can train a quantized
    model with it since the data preparation pipeline quantizes the input.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`chapter_06_example_09.py`中的代码。是的，我们可以用它训练一个量化模型，因为数据准备管道会量化输入。
- en: For small datasets, data augmentation plays an essential role in creating more
    data, because sometimes you just don't have more. For bigger datasets, it also
    plays a role by creating more relevant data and variations on existing data, which
    is good for the network training phase.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于小型数据集，数据增强在创造更多数据方面起着至关重要的作用，因为有时你根本没有更多数据。对于较大的数据集，数据增强也起到作用，通过创建更多相关数据和现有数据的变种，帮助网络的训练阶段。
- en: 'Chapter 7: Training Magenta Models'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章：训练Magenta模型
- en: See `chapter_07_example_03.py`.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请参见`chapter_07_example_03.py`。
- en: A network that underfits is a network that hasn't reached its optimum, meaning
    it won't predict well with the evaluation data, because it fits poorly the training
    data (for now). It can be fixed by letting it train long enough, by adding more
    network capacity, and more data.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 欠拟合的网络是指尚未达到最佳状态的网络，这意味着它在评估数据上无法做出良好预测，因为它对训练数据的拟合不佳（目前为止）。可以通过让其训练足够长时间、增加网络容量和更多数据来修正。
- en: A network that overfits is a network that has learned to predict the input but
    cannot generalize to values outside of its training set. It can be fixed by adding
    more data, by reducing the network capacity, or by using regularization techniques
    such as dropout.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 过拟合的网络是指已学会预测输入数据，但无法推广到训练集之外的值的网络。可以通过增加更多数据、减少网络容量，或使用正则化技术（如dropout）来修正。
- en: Early stopping.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提前停止。
- en: 'Read *On Large-Batch Training for Deep Learning: Generalization Gap and Sharp
    Minima*, which explains that a larger batch size leads to sharp minimizers, which
    in turn leads to poorer generalization. Therefore it is worse in terms of efficiency,
    but might be better in terms of training time, since more data is processed at
    the same time.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读《关于深度学习大批量训练的问题：泛化差距与锐化极小值》，其中解释了更大的批量大小会导致锐化极小值，从而导致泛化能力较差。因此在效率方面更差，但在训练时间方面可能更好，因为可以同时处理更多数据。
- en: A bigger network is a network that will be more precise in its prediction, so
    maximizing that should be important. The network size should also grow with the
    size (and quality) of the data. For example, a network that's too big for its
    data will likely overfit.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更大的网络会在预测中更加精确，因此最大化这一点很重要。网络的规模也应随数据的大小（和质量）增长。例如，对于数据而言过大的网络可能会导致过拟合。
- en: It helps with the exploding gradients problem because the weights will be multiplied
    by smaller values, limiting the possibility of having big gradients. Another way
    of doing this is by reducing the learning rate.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它有助于解决梯度爆炸问题，因为权重将乘以较小的值，限制大梯度的可能性。另一种方法是降低学习率。
- en: It can be used to launch training on more powerful machines and to launch multiple
    training sessions at the same time. Unfortunately, using cloud providers have
    a cost, meaning the more training time and power we use, the more costly our training
    will get.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可用于在更强大的机器上启动训练，也可以同时启动多个训练会话。不幸的是，使用云提供商会产生成本，这意味着我们使用的训练时间和功率越多，成本就会越高。
- en: 'Chapter 8: Magenta in the Browser with Magenta.js'
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第八章：在浏览器中使用 Magenta.js
- en: We can train models using TensorFlow.js, but we cannot train models using Magenta.js.
    We need to train the models in Magenta using Python and import the resulting models
    in Magenta.js.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用 TensorFlow.js 训练模型，但无法使用 Magenta.js 训练模型。我们需要使用 Python 在 Magenta 中训练模型，然后在
    Magenta.js 中导入生成的模型。
- en: The Web Audio API enables audio synthesis in the browser using audio nodes for
    generation, transformation, and routing. The easiest way to use it is to use an
    audio framework such as Tone.js.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Web Audio API 允许在浏览器中使用音频节点进行音频合成、转换和路由。最简单的使用方法是使用像 Tone.js 这样的音频框架。
- en: The method is `randomSample` and the argument is the pitch of the generated
    note. As an example, using 60 will result in a single note at MIDI pitch 60, or
    C4 in letter notation. This is also useful as a reference for pitching the note
    up or down using Tone.js.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法是 `randomSample`，参数是生成音符的音高。例如，使用 60 将生成 MIDI 音高为 60 的单音符，或者在字母符号中是 C4。这也是使用
    Tone.js 调高或调低音符音高的参考。
- en: The method is `sample` and the number of instruments depends on the model that
    is being used. In our example, we've used the `trio` model, which generates three
    instruments. Using a `melody` model will generate only one lead instrument.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 方法是 `sample`，乐器数量取决于正在使用的模型。在我们的示例中，我们使用了 `trio` 模型，它生成三种乐器。使用 `melody` 模型将只生成一个主导乐器。
- en: Since JavaScript is single threaded, long synchronous computations that are
    launched in the UI thread will block its execution. Using a web worker makes it
    possible to execute code in another thread.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为 JavaScript 是单线程的，如果在 UI 线程中启动长时间的同步计算，将会阻塞其执行。使用 Web Worker 可以在另一个线程中执行代码。
- en: Using the Web MIDI API in the browser, which is not well supported at the moment,
    or using Magenta.js in a Node.js process on the server side, making it easier
    to send MIDI to other processes.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在浏览器中使用 Web MIDI API，目前支持不够完善，或者在服务器端使用 Magenta.js 进程，这样可以更容易地向其他进程发送 MIDI。
- en: 'Chapter 9: Making Magenta Interact with Music Applications'
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第九章：使 Magenta 与音乐应用程序互动
- en: A DAW will have more functions geared towards music production such as recording,
    audio, MIDI editing, effects and mastering, and song composition. A software synthesizer
    like FluidSynth will have less functionalities, but have the advantage of being
    lightweight and easy to use.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DAW 将具有更多面向音乐制作的功能，如录制、音频、MIDI 编辑、效果和母带处理以及歌曲作曲。像 FluidSynth 这样的软件合成器功能较少，但优点是轻量且易于使用。
- en: Most music software won't open MIDI ports by themselves, so to send sequences
    back and forth between them we have to manually open ports.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 大多数音乐软件不会自动打开 MIDI 端口，因此要在它们之间发送序列，我们必须手动打开端口。
- en: See the code in `chapter_09_example_05.py` in this chapter's code.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参见本章的代码 `chapter_09_example_05.py`。
- en: Because syncing two pieces of software that have desynced requires restarting
    them. A MIDI clock enables syncing once per beat.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为将两个失去同步的软件重新同步需要重启它们。MIDI 时钟可以在每个节拍时启用同步。
- en: Because Magenta Studio integrates with existing music production tools such
    as DAWs and doesn't require any technical knowledge, it makes AI-generated music
    available to a greater audience, which is ultimately the goal of Magenta.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因为 Magenta Studio 与现有的音乐制作工具（如 DAW）集成，并且不需要任何技术知识，它使得 AI 生成的音乐能够面向更广泛的受众，这也是
    Magenta 的最终目标。
- en: Magenta Studio plugins and Magenta Studio standalone are both based on Magenta.js,
    packaged using Electron. Magenta Studio Plugins uses the Max MSP integration in
    Ableton Live to execute inside it.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Magenta Studio 插件和 Magenta Studio 独立版都基于 Magenta.js，并通过 Electron 打包。Magenta
    Studio 插件使用 Ableton Live 中的 Max MSP 集成来执行。
