- en: '*Chapter 14*: Autonomous Systems'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第14章*：自主系统'
- en: So far in the book, we have covered many state-of-the-art algorithms and approaches
    in reinforcement learning. Now, starting with this chapter, we will see them in
    action to take on real-world problems! We'll start with robot learning, an important
    application area for reinforcement learning. To this end, we will train a KUKA
    robot to grasp objects on a tray using PyBullet physics simulation. We will discuss
    several ways of solving this hard-exploration problem and solve it both using
    a manually crafted curriculum as well as using the ALP-GMM algorithm. At the end
    of the chapter, we will present other simulation libraries for robotics and autonomous
    driving, which are commonly used to train reinforcement learning agents.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书已经涵盖了许多强化学习中的最前沿算法和方法。从本章开始，我们将看到它们如何在实际应用中应对现实问题！我们将从机器人学习开始，这是强化学习的重要应用领域。为此，我们将使用
    PyBullet 物理仿真训练 KUKA 机器人抓取托盘上的物体。我们将讨论几种解决这个困难探索问题的方法，并且会通过手动构建课程学习法和 ALP-GMM
    算法来解决它。在本章结束时，我们还将介绍其他用于机器人学和自动驾驶的仿真库，这些库通常用于训练强化学习代理。
- en: 'So, this chapter covers the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，本章内容包括以下几点：
- en: Introducing PyBullet
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 PyBullet
- en: Getting familiar with the KUKA environment
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉 KUKA 环境
- en: Developing strategies to solve the KUKA environment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发解决 KUKA 环境的策略
- en: Using curriculum learning to train the KUKA robot
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用课程学习法训练 KUKA 机器人
- en: Going beyond PyBullet, into autonomous driving
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超越 PyBullet，进入自动驾驶领域
- en: This is one of the most challenging and fun areas for reinforcement learning.
    Let's dive right in!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这是强化学习中最具挑战性且有趣的领域之一。让我们深入了解吧！
- en: Introducing PyBullet
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 PyBullet
- en: 'PyBullet is a popular high-fidelity physics simulation module for robotics,
    machine learning, games, and more. It is one of the most commonly used libraries
    for robot learning using RL, especially in sim-to-real transfer research and applications:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PyBullet 是一个流行的高保真物理仿真模块，广泛应用于机器人学、机器学习、游戏等领域。它是使用强化学习进行机器人学习时最常用的库之一，特别是在从仿真到现实的迁移研究和应用中：
- en: '![Figure 14.1 – PyBullet environments and visualizations (source: PyBullet
    GitHub repo)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.1 – PyBullet 环境与可视化（来源：PyBullet GitHub 仓库）'
- en: '](img/B14160_14_1.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_14_1.jpg)'
- en: 'Figure 14.1 – PyBullet environments and visualizations (source: PyBullet GitHub
    repo)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1 – PyBullet 环境与可视化（来源：PyBullet GitHub 仓库）
- en: PyBullet allows developers to create their own physics simulations. In addition,
    it has prebuilt environments using the OpenAI Gym interface. Some of those environments
    are shown in *Figure 14.1*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: PyBullet 允许开发者创建自己的物理仿真。此外，它还提供了使用 OpenAI Gym 接口的预构建环境。部分这些环境如*图 14.1*所示。
- en: In the next section, we will set up a virtual environment for PyBullet.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将为 PyBullet 设置一个虚拟环境。
- en: Setting up PyBullet
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 PyBullet
- en: 'It is almost always a good idea to work in virtual environments for Python
    projects, which is also what we will do for our robot learning experiments in
    this chapter. So, let''s go ahead and execute the following commands to install
    the libraries we will use:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 项目中使用虚拟环境几乎总是一个好主意，这也是我们将在本章中的机器人学习实验中所做的。所以，让我们继续执行以下命令来安装我们将使用的库：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can test whether your installation is working by running this:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行以下命令来测试你的安装是否正常：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And if everything is working fine, you will see a cool Ant robot wandering
    around as in *Figure 14.2*:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切正常，你会看到一个很酷的蚂蚁机器人四处游走，正如在*图 14.2*中所示：
- en: '![Figure 14.2 – Ant robot walking in PyBullet'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.2 – 蚂蚁机器人在 PyBullet 中行走'
- en: '](img/B14160_14_2.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_14_2.jpg)'
- en: Figure 14.2 – Ant robot walking in PyBullet
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2 – 蚂蚁机器人在 PyBullet 中行走
- en: Great! We are now ready to proceed to the KUKA environment that we will use.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们现在可以继续进行我们将要使用的 KUKA 环境了。
- en: Getting familiar with the KUKA environment
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熟悉 KUKA 环境
- en: 'KUKA is a company that offers industrial robotics solutions, which are widely
    used in manufacturing and assembly environments. PyBullet includes a simulation
    of a KUKA robot, used for object-grasping simulations (*Figure 14.3*):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: KUKA 是一家提供工业机器人解决方案的公司，这些解决方案广泛应用于制造和组装环境。PyBullet 包含了 KUKA 机器人的仿真，用于物体抓取仿真（*图
    14.3*）：
- en: '![Figure 14.3 – KUKA robots are widely used in industry. (a) A real KUKA robot'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.3 – KUKA 机器人在工业中被广泛使用。（a）一台真实的 KUKA 机器人'
- en: (image source CNC Robotics website), (b) a PyBullet simulation
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: （图片来源 CNC Robotics 网站），（b）一个 PyBullet 仿真
- en: '](img/B14160_14_3.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_14_3.jpg)'
- en: Figure 14.3 – KUKA robots are widely used in industry. (a) A real KUKA robot
    (image source CNC Robotics website), (b) a PyBullet simulation
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3 – KUKA 机器人广泛应用于工业中。(a) 一台真实的 KUKA 机器人（图片来源：CNC Robotics 网站），(b) 一种 PyBullet
    仿真
- en: 'There are multiple KUKA environments in PyBullet, for the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: PyBullet 中有多个 KUKA 环境，具体如下：
- en: Grasping a rectangular block using robot and object positions and angles
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器人和物体的位置及角度抓取矩形块
- en: Grasping a rectangular block using camera inputs
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用摄像头输入抓取矩形块
- en: Grasping random objects using camera/position inputs
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用摄像头/位置输入抓取随机物体
- en: In this chapter, we'll focus on the first one, which we'll look into next in
    more detail.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注第一个动作，接下来会更详细地介绍它。
- en: Grasping a rectangular block using a KUKA robot
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 KUKA 机器人抓取矩形块
- en: 'In this environment, the goal of the robot is to reach a rectangular object,
    grasp it, and raise it up to a certain height. An example scene from the environment,
    along with the robot coordinate system, is shown in *Figure 14.4*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中，机器人的目标是到达一个矩形物体，抓取它并将其抬升到一定高度。环境中的一个示例场景以及机器人坐标系如*图 14.4*所示：
- en: '![Figure 14.4 – Object-grasping scene and the robot coordinate system'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.4 – 物体抓取场景和机器人坐标系'
- en: '](img/B14160_14_4.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_14_4.jpg)'
- en: Figure 14.4 – Object-grasping scene and the robot coordinate system
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4 – 物体抓取场景和机器人坐标系
- en: The dynamics and initial position of the robot joints are defined in the `Kuka`
    class of the `pybullet_envs` package. We will talk about these details only as
    much as we need to, but you should feel free to dive into the class definition
    to better understand the dynamics.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人关节的动力学和初始位置在 `pybullet_envs` 包中的 `Kuka` 类中定义。我们将根据需要讨论这些细节，但你可以自由深入了解类定义，以更好地理解动力学。
- en: Info
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: To better understand the PyBullet environment and how the `Kuka` class is constructed,
    you can check out the *PyBullet Quickstart Guide* at [https://bit.ly/323PjmO](https://bit.ly/323PjmO).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 PyBullet 环境以及 `Kuka` 类的构建，你可以查看 *PyBullet 快速入门指南*，链接：[https://bit.ly/323PjmO](https://bit.ly/323PjmO)。
- en: Let's now dive into the Gym environment created to control this robot inside
    PyBullet.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解为控制此机器人在 PyBullet 内部创建的 Gym 环境。
- en: The KUKA Gym environment
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KUKA Gym 环境
- en: '`KukaGymEnv` wraps the `Kuka` robot class and turns it into a Gym environment.
    The action, observation, reward, and terminal conditions are defined as follows.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`KukaGymEnv` 封装了 `Kuka` 机器人类，并将其转化为一个 Gym 环境。动作、观察、奖励和终止条件定义如下。'
- en: Actions
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作
- en: 'There are three actions the agent takes in the environment, which are all about
    moving the gripper. These actions are as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个环境中，代理执行的三种动作都与移动夹持器有关。这些动作如下：
- en: Velocity along the ![](img/Formula_14_001.png) axis
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿![](img/Formula_14_001.png)轴的速度
- en: Velocity along the ![](img/Formula_14_002.png) axis
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿![](img/Formula_14_002.png)轴的速度
- en: Angular velocity to rotate the gripper (yaw)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于旋转夹持器的角速度（偏航）
- en: The environment itself moves the gripper along the ![](img/Formula_14_003.png)
    axis towards the tray, where the object is located. When it gets sufficiently
    close to the tray, it closes the fingers of the gripper to try grasping the object.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 环境本身将夹持器沿![](img/Formula_14_003.png)轴移动到托盘上，物体位于托盘中。当夹持器足够接近托盘时，它会闭合夹持器的手指以尝试抓取物体。
- en: The environment can be configured to accept discrete or continuous actions.
    We will use the latter in our case.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 环境可以配置为接受离散或连续的动作。我们将在本案例中使用后者。
- en: Observations
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 观察值
- en: 'The agent receives nine observations from the environment:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 代理从环境中接收九个观察值：
- en: Three observations for the ![](img/Formula_14_004.png), ![](img/Formula_14_005.png),
    and ![](img/Formula_14_006.png) positions of the gripper
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个观察值用于夹持器的![](img/Formula_14_004.png)、![](img/Formula_14_005.png)和![](img/Formula_14_006.png)位置
- en: Three observations for the Euler angles of the gripper with respect to the ![](img/Formula_14_007.png),
    ![](img/Formula_14_008.png), and ![](img/Formula_14_009.png) axes
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个观察值用于夹持器相对于![](img/Formula_14_007.png)、![](img/Formula_14_008.png)和![](img/Formula_14_009.png)轴的欧拉角
- en: Two observations for the ![](img/Formula_14_010.png) and ![](img/Formula_14_011.png)
    positions of the object **relative** to the gripper
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个观察值用于物体相对于夹持器的![](img/Formula_14_010.png)和![](img/Formula_14_011.png)位置，**相对**于夹持器。
- en: One observation for the Euler angle of the object **relative** to the gripper's
    Euler angle along the ![](img/Formula_14_012.png) axis
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个观察值用于物体相对于夹持器的欧拉角，沿![](img/Formula_14_012.png)轴
- en: Reward
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 奖励
- en: The reward for grasping the object successfully and lifting it up to a certain
    height is 10,000 points. Other than that, there is a slight cost that penalizes
    the distance between the gripper and the object. Additionally, there is also some
    energy cost from rotating the gripper.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 成功抓取对象并将其提升到一定高度的奖励是 10,000 分。 此外，还有一个轻微的成本，用于惩罚夹爪与对象之间的距离。 另外，旋转夹爪也会消耗一些能量。
- en: Terminal conditions
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 终止条件
- en: An episode terminates after 1,000 steps or after the gripper closes, whichever
    occurs first.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 episode 在 1,000 步之后或夹爪关闭之后结束，以先发生者为准。
- en: The best way to wrap your mind around how the environment works is to actually
    experiment with it, which is what you will do next.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解环境如何运作的最佳方式是实际进行实验，这也是接下来要做的事情。
- en: 'This can be done with the following code file: `Chapter14/manual_control_kuka.py.`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下代码文件完成：`Chapter14/manual_control_kuka.py.`
- en: This script allows you to control the robot manually. You can use the "gym-like"
    control mode, where the vertical speed and the gripper finger angles are controlled
    by the environment. Alternatively, you can choose the non-gym-like mode to exert
    more control.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本允许您手动控制机器人。 您可以使用 "类似 gym 的" 控制模式，在此模式下，环境控制垂直速度和夹爪指角度。 或者，您可以选择非类似 gym 的模式来更多地控制。
- en: One thing you will notice is that even if you keep the speeds along the ![](img/Formula_14_013.png)
    and ![](img/Formula_14_014.png) axes at zero, in the gym-like control mode, the
    robot will change its ![](img/Formula_14_015.png) and ![](img/Formula_14_016.png)
    positions while going down. This is because the default speed of the gripper along
    the ![](img/Formula_14_017.png) axis is too high. You can actually verify that,
    in the non-gym-like mode, values below ![](img/Formula_14_018.png) for ![](img/Formula_14_019.png)
    alter the positions on the other axes too much. We will reduce the speed when
    we customize the environment to alleviate that.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到的一件事是，即使在类似 gym 的控制模式下将速度沿着 ![](img/Formula_14_013.png) 和 ![](img/Formula_14_014.png)
    轴保持为零，机器人在下降时会改变其 ![](img/Formula_14_015.png) 和 ![](img/Formula_14_016.png) 位置。
    这是因为夹爪沿 ![](img/Formula_14_017.png) 轴的默认速度太高。 您可以验证，在非类似 gym 的模式下，为 ![](img/Formula_14_018.png)
    以下的值对 ![](img/Formula_14_019.png) 会使其他轴的位置发生较大改变。 在我们定制环境时，我们将减少速度以减轻这种情况。
- en: Now that you are familiar with the KUKA environment, let's discuss some alternative
    strategies to solve it.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经熟悉 KUKA 环境，让我们讨论一些解决它的替代策略。
- en: Developing strategies to solve the KUKA environment
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发解决 KUKA 环境的策略
- en: 'The object-grasping problem in the environment is a **hard-exploration** problem,
    meaning that it is unlikely to stumble upon the sparse reward that the agent receives
    at the end upon grasping the object. Reducing the vertical speed as we will do
    will make it a bit easier. Still, let''s refresh our minds about what strategies
    we have covered to address these kinds of problems:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 环境中的抓取物体问题是一个 **难探索** 问题，这意味着在抓取对象后代理程序接收的稀疏奖励不太可能被发现。 像我们即将做的减少垂直速度将使它稍微容易一些。
    不过，让我们回顾一下我们已经涵盖的解决这类问题的策略：
- en: '**Reward shaping** is one of the most common **machine teaching** strategies
    that we discussed earlier. In some problems, incentivizing the agent towards the
    goal is very straightforward. In many problems, though, it can be quite painful.
    So, unless there is an obvious way of doing so, crafting the reward function may
    just take too much time (and expertise about the problem). Also notice that the
    original reward function has a component to penalize the distance between the
    gripper and the object, so the reward is already shaped to some extent. We will
    not go beyond that in our solution.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励塑形** 是我们之前讨论过的最常见的 **机器教学** 策略之一。 在某些问题中，激励代理朝向目标非常直接。 虽然在许多问题中，这样做可能会很痛苦。
    因此，除非有明显的方法，否则制定奖励函数可能需要太多时间（以及对问题的专业知识）。 还请注意，原始奖励函数有一个成分来惩罚夹爪与对象之间的距离，因此奖励已经在某种程度上被塑造。
    在我们的解决方案中，我们将不会超越此范围。'
- en: '**Curiosity-driven learning** incentivizes the agent to discover new parts
    of the state space. For this problem, though, we don''t need the agent to randomly
    explore the state space too much as we already have some idea about what it should
    do. So, we will skip this technique as well.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以好奇心驱动的学习** 激励代理程序发现状态空间的新部分。 对于这个问题，我们不需要代理程序过多地随机探索状态空间，因为我们已经对它应该做什么有一些想法。
    因此，我们也将跳过这个技术。'
- en: '`"entropy_coeff"` config inside the PPO trainer of RLlib, which is what we
    will use. However, our hyperparameter search (we will come to it soon) ended up
    picking this value as zero.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"entropy_coeff"` 配置位于 RLlib 的 PPO 训练器中，这是我们将使用的配置。然而，我们的超参数搜索（稍后会详细介绍）最终将这个值选为零。'
- en: '**Curriculum learning** is perhaps the most suitable approach here. We can
    identify what makes the problem challenging for the agent, start training it at
    easy levels, and gradually increase the difficulty.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**课程学习** 可能是这里最合适的方法。我们可以识别出使问题变得具有挑战性的因素，从简单的水平开始训练智能体，并逐渐增加难度。'
- en: So, curriculum learning is what we will leverage to solve this problem. But
    first, let's identify the dimensions to parametrize the environment to create
    a curriculum.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，课程学习是我们将用来解决这个问题的方法。但首先，让我们识别出参数化环境的维度，以便创建课程。
- en: Parametrizing the difficulty of the problem
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数化问题的难度
- en: 'When you experimented with the environment, you may have noticed the factors
    that make the problem difficult:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当你实验环境时，你可能已经注意到一些让问题变得困难的因素：
- en: The gripper starts too high to discover the correct sequences of actions to
    grasp the object. So, the robot joint that adjusts the height will be one dimension
    we will parameterize. It turns out that this is set in the second element of the
    `jointPositions` array of the `Kuka` class.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 夹爪的起始位置过高，无法发现正确的抓取顺序。因此，调整高度的机器人关节将是我们需要参数化的一个维度。事实证明，它是在 `Kuka` 类的 `jointPositions`
    数组的第二个元素中设置的。
- en: When the gripper is not at its original height, it may get misaligned with the
    location of the object along the ![](img/Formula_14_020.png) axis. We will also
    parametrize the position of the joint that controls this, which is the fourth
    element of the `jointPositions` array of the `Kuka` class.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当夹爪不在原始高度时，它可能会与物体沿 ![](img/Formula_14_020.png) 轴的位置发生错位。我们还将对控制此位置的关节进行参数化，该关节是
    `Kuka` 类的 `jointPositions` 数组的第四个元素。
- en: Randomizing the object position is another source of difficulty for the agent,
    which takes place for the ![](img/Formula_14_021.png) and ![](img/Formula_14_022.png)
    positions as well as the object angle. We will parametrize the degree of randomization
    between 0 and 100% for each of these components.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机化物体位置是另一个给智能体带来困难的因素，它影响到 ![](img/Formula_14_021.png) 和 ![](img/Formula_14_022.png)
    的位置以及物体的角度。我们将对这些组件的随机化程度进行参数化，范围从 0% 到 100%。
- en: Even when the object is not randomly positioned, its center is not aligned with
    the default position of the robot on the ![](img/Formula_14_023.png) axis. We
    will add some bias to the ![](img/Formula_14_024.png) position of the object,
    again parametrized.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使物体没有被随机放置，它的中心也未必与机器人在 ![](img/Formula_14_023.png) 轴上的默认位置对齐。我们将对物体在 ![](img/Formula_14_024.png)
    位置添加一些偏差，同样进行参数化。
- en: This is great! We know what to do, which is a big first step. Now, we can go
    into curriculum learning!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这太棒了！我们知道该怎么做，这是一个重要的第一步。现在，我们可以开始课程学习！
- en: Using curriculum learning to train the KUKA robot
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用课程学习训练 KUKA 机器人
- en: The first step before actually kicking off some training is to customize the
    `Kuka` class as well as `KukaGymEnv` to make them work with the curriculum learning
    parameters we described above. So, let's do that next.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际启动训练之前，第一步是定制 `Kuka` 类和 `KukaGymEnv`，使其能够与我们上面描述的课程学习参数一起工作。接下来我们就来做这个。
- en: Customizing the environment for curriculum learning
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为课程学习定制环境
- en: 'First, we start by creating a `CustomKuka` class that inherits the original
    `Kuka` class of PyBullet. Here is how we do it:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过创建一个继承自 PyBullet 原始 `Kuka` 类的 `CustomKuka` 类来开始。以下是我们的实现方式：
- en: Chapter14/custom_kuka.py
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Chapter14/custom_kuka.py
- en: 'We first need to create the new class, and accept an additional argument, the
    `jp_override` dictionary, which stands for **joint position override**:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先需要创建一个新的类，并接受一个额外的参数，`jp_override` 字典，它代表 **关节位置覆盖**：
- en: '[PRE2]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We need this to change the `jointPositions` array set in the `reset` method,
    which we override:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要这个来改变在我们重写的 `reset` 方法中设置的 `jointPositions` 数组：
- en: '[PRE3]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, it's time to create `CustomKukaEnv`.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，是时候创建 `CustomKukaEnv` 了。
- en: 'Create the custom environment that accepts all these parametrization inputs
    for curriculum learning:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个自定义环境，接受所有这些课程学习的参数化输入：
- en: '[PRE4]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that we are also making it RLlib compatible by accepting `env_config`.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们还通过接受 `env_config` 使其兼容 RLlib。
- en: 'We use the randomization parameters in the `reset` method to override the default
    amount of randomization in the object position:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`reset`方法中的随机化参数来覆盖物体位置的默认随机化程度：
- en: '[PRE5]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Also, we should now replace the old `Kuka` class with `CustomKuka` and pass
    the joint position override input to it:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们现在应该用`CustomKuka`替换旧的`Kuka`类，并将关节位置重写的输入传递给它：
- en: '[PRE6]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we override the `step` method of the environment to decrease the default
    speed on the ![](img/Formula_14_025.png) axis:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们重写了环境的`step`方法，以降低在![](img/Formula_14_025.png)轴上的默认速度：
- en: '[PRE7]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Also notice that we rescaled the reward (it will end up between -10 and 10)
    to make the training easy.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 还要注意，我们重新缩放了奖励（它将在-10到10之间），以便训练更容易。
- en: Great job! Next, let's discuss what kind of curriculum to use.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！接下来，让我们讨论使用什么样的课程。
- en: Designing the lessons in the curriculum
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计课程中的课程
- en: 'It is one thing to determine the dimensions to parametrize the difficulty of
    the problem, and another thing to decide how to expose this parametrization to
    the agent. We know that the agent should start with easy lessons and move to more
    difficult ones gradually. This, though, raises some important questions:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 确定参数化问题难度的维度是一回事，而决定如何将这种参数化暴露给代理则是另一回事。我们知道，代理应该从简单的课程开始，逐渐过渡到更难的课程。然而，这也引发了一些重要问题：
- en: Which parts of the parametrized space are easy?
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数化空间的哪些部分容易？
- en: What should the step sizes be to change the parameters between lesson transitions?
    In other words, how should we slice the space into lessons?
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课与课之间，调整参数的步长应该是多少？换句话说，我们应该如何将空间划分成不同的课程？
- en: What are the criteria of success for the agent to transition to the next lesson?
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理转到下一课的成功标准是什么？
- en: What if the agent fails in a lesson, meaning that its performance is unexpectedly
    bad? Should it go back to the previous lesson? What is the bar for failure?
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果代理在某一课上失败，意味着它的表现出乎意料地差，该怎么办？是否应该回到上一节课？失败的标准是什么？
- en: What if the agent cannot transition into the next lesson for a long time? Does
    it mean that we set the bar for success for the lesson too high? Should we divide
    that lesson into sub-lessons?
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果代理长时间无法进入下一课，该怎么办？这是否意味着我们设定的成功标准太高？是否应该将这一课划分为子课？
- en: As you can see, these are non-trivial questions to answer when we are designing
    a curriculum manually. But also remember that in [*Chapter 11*](B14160_11_Final_SK_ePub.xhtml#_idTextAnchor239),
    *Generalization and Partial Observability*, we introduced the **Absolute Learning
    Progress with Gaussian Mixture Models (ALP-GMM)** method, which handles all these
    decisions for us. Here, we will implement both, starting with a manual curriculum
    first.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在手动设计课程时，这些问题并非易事。但也请记得，在[*第11章*](B14160_11_Final_SK_ePub.xhtml#_idTextAnchor239)《泛化与部分可观察性》中，我们介绍了**使用高斯混合模型的绝对学习进度（ALP-GMM）**方法，它会为我们处理所有这些决策。在这里，我们将实现两者，首先从手动课程开始。
- en: Training the agent using a manually designed curriculum
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用手动设计的课程对代理进行训练
- en: 'We will design a rather simple curriculum for this problem. It will transition
    the agent to subsequent lessons when it meets the success criteria, and fall back
    to a previous lesson in the event of low performance. The curriculum will be implemented
    as a method inside the `CustomKukaEnv` class, with the `increase_difficulty` method:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为这个问题设计一个相对简单的课程。当代理达到成功标准时，课程会将它推进到下一课；当表现不佳时，会回退到前一课。该课程将在`CustomKukaEnv`类内部实现，并包含`increase_difficulty`方法：
- en: 'We start by defining the delta changes in the parameter values during lesson
    transitions. For the joint values, we will decrease the joint positions from what
    is entered by the user (easy) to the original values in the environment (difficult):'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先定义在课程过渡期间参数值的增量变化。对于关节值，我们将关节位置从用户输入的值（简单）降低到环境中的原始值（困难）：
- en: '[PRE8]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'During each lesson transition, we also make sure to increase the randomization
    of the object position:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每节课的过渡过程中，我们还确保增加物体位置的随机化程度：
- en: '[PRE9]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we remember to set the biases to zeros when the object position becomes
    fully randomized:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，当物体位置完全随机化时，我们记得将偏差设置为零：
- en: '[PRE10]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So far, so good. We have almost everything ready to train our agent. One last
    thing before doing so: let''s discuss how to pick the hyperparameters.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利。我们几乎准备好训练代理了。在此之前，还有一件事：让我们讨论如何选择超参数。
- en: Hyperparameter selection
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超参数选择
- en: In order to tune the hyperparameters in RLlib, we can use the Ray Tune library.
    In [*Chapter 15*](B14160_15_Final_SK_ePub.xhtml#_idTextAnchor329), *Supply Chain
    Management*, we will provide you with an example of how it is done. For now, you
    can just use the hyperparameters we have picked in `Chapter14/configs.py`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在RLlib中调节超参数，我们可以使用Ray Tune库。在[*第15章*](B14160_15_Final_SK_ePub.xhtml#_idTextAnchor329)，《供应链管理》中，我们将提供一个示例，说明如何进行调节。现在，您可以直接使用我们在`Chapter14/configs.py`中选择的超参数。
- en: Tip
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: In hard-exploration problems, it may make more sense to tune the hyperparameters
    for a simple version of the problem. This is because without observing some reasonable
    rewards, the tuning may not pick a good set of hyperparameter values. After we
    do an initial tuning in an easy environment setting, the chosen values can be
    adjusted later in the process if the learning stalls.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在困难的探索问题中，可能更合理的是先为问题的简化版本调节超参数。这是因为如果没有观察到一些合理的奖励，调节过程可能不会选择一个好的超参数集。我们在一个简单的环境设置中进行初步调节后，如果学习停滞，选择的值可以在后续过程中进行调整。
- en: Finally, let's see how we can use the environment we have just created during
    training with the curriculum we have defined.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看如何在训练过程中使用我们刚刚创建的环境，以及我们定义的课程。
- en: Training the agent on the curriculum using RLlib
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用RLlib在课程中训练智能体
- en: 'To proceed with the training, we need the following ingredients:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续进行训练，我们需要以下要素：
- en: Initial parameters for the curriculum
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程的初始参数
- en: Some criteria to define the success (and failure, if needed)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些定义成功（以及失败，如果需要）的标准
- en: A callback function that will execute the lesson transitions
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个回调函数，将执行课程过渡
- en: 'In the following code snippet, we use the PPO algorithm in RLlib, set the initial
    parameters, and set the reward threshold (empirically) to *5.5* in the callback
    function that executes the lesson transitions:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，我们使用RLlib中的PPO算法，设置初始参数，并在执行课程过渡的回调函数中将奖励阈值（经验值）设置为*5.5*：
- en: Chapter14/train_ppo_manual_curriculum.py
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Chapter14/train_ppo_manual_curriculum.py
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This should kick off the training and you will see the curriculum learning in
    action! You will notice that as the agent transitions to the next lesson, its
    performance will usually drop as the environment gets more difficult.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会启动训练，您将看到课程学习的实际效果！您会注意到，当智能体过渡到下一课时，随着环境变得更困难，其表现通常会下降。
- en: We will look into the results of this training later. Let's now also implement
    the ALP-GMM algorithm.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会查看此训练的结果。现在让我们继续实现ALP-GMM算法。
- en: Curriculum learning using absolute learning progress
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用绝对学习进展的课程学习
- en: 'The ALP-GMM method focuses on where the biggest performance change (absolute
    learning progress) in the parameter space is and generates parameters around that
    gap. This idea is illustrated in *Figure 14.5*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ALP-GMM方法专注于在参数空间中性能变化最大（绝对学习进展）的位置，并在该空隙周围生成参数。这个想法在*图14.5*中得到了说明：
- en: '![Figure 14.5 – ALP-GMM generates parameters (tasks) around points, between
    which the biggest episode reward change is observed'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.5 – ALP-GMM 在点周围生成参数（任务），在这些点之间，观察到最大的回合奖励变化'
- en: '](img/B14160_14_5.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_14_5.jpg)'
- en: Figure 14.5 – ALP-GMM generates parameters (tasks) around points, between which
    the biggest episode reward change is observed
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5 – ALP-GMM 在点周围生成参数（任务），在这些点之间，观察到最大的回合奖励变化
- en: This way, the learning budget is not spent on the parts of the state space that
    have already been learned, or on the parts that are too difficult to learn for
    the current agent.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，学习预算就不会花费在已经学习过的状态空间部分，或者花费在当前智能体无法学习的部分上。
- en: After this recap, let's go ahead and implement it. We start by creating a custom
    environment in which the ALP-GMM algorithm will run.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次回顾之后，让我们继续实现它。我们首先创建一个自定义环境，在这个环境中将运行ALP-GMM算法。
- en: Chapter14/custom_kuka.py
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Chapter14/custom_kuka.py
- en: 'We get the ALP-GMM implementation directly from the source repo accompanying
    the paper (Portelas et al. 2019) and put it under `Chapter14/alp`. We can then
    plug that into the new environment we create, `ALPKukaEnv`, the key pieces of
    which are here:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直接从与论文（Portelas 等，2019）附带的源代码库中获取ALP-GMM的实现，并将其放置在`Chapter14/alp`目录下。然后，我们可以将其插入到我们创建的新环境`ALPKukaEnv`中，关键部分如下：
- en: 'We create the class and define all the minimum and maximum values of the parameter
    space we are trying to teach the agent:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建类并定义我们尝试教授给智能体的参数空间的所有最小值和最大值：
- en: '[PRE12]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, the task is the latest sample from the parameter space generated by the
    ALP-GMM algorithm to configure the environment.
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里的任务是由ALP-GMM算法生成的参数空间中的最新样本，用于配置环境。
- en: 'A task is sampled at the beginning of each episode. Once an episode finishes,
    the task (environment parameters used in the episode) and the episode reward are
    used to update the GMM model:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个回合开始时都会采样一个任务。一旦回合结束，任务（在该回合中使用的环境参数）和回合奖励将被用来更新GMM模型：
- en: '[PRE13]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And finally, we make sure to keep track of the episode reward:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们确保跟踪每一回合的奖励：
- en: '[PRE14]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'One thing to note here is that ALP-GMM is normally implemented in a centralized
    fashion: a central process generates all the tasks for the rollout workers and
    collects the episode rewards associated with those tasks to process. Here, since
    we are working in RLlib, it is easier to implement it inside the environment instances.
    In order to account for the reduced amount of data collected in a single rollout,
    we used `"fit_rate": 20`, lower than the original level of 250, so that a rollout
    worker doesn''t wait too long before it fits a GMM to the task-reward data it
    collects.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '这里需要注意的一点是，ALP-GMM通常以集中式方式实现：一个中心进程为所有回合工作者生成任务，并收集与这些任务相关的回合奖励进行处理。在这里，由于我们在RLlib中工作，所以更容易在环境实例中实现它。为了考虑单次回合中收集的数据量减少，我们使用了`"fit_rate":
    20`，低于原始的250水平，这样回合工作者在拟合GMM到它收集的任务-奖励数据之前不会等待太久。'
- en: After creating `ALPKukaEnv`, the rest is just a simple call of the Ray `tune.run()`
    function, which is available in `Chapter14/train_ppo_alp.py`. Note that, unlike
    in a manual curriculum, we don't specify the initial values of the parameters.
    Instead, we have passed the bounds of the ALP-GMM processes, and they guide the
    curriculum within those bounds.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`ALPKukaEnv`后，其余的工作只是简单地调用Ray的`tune.run()`函数，位于`Chapter14/train_ppo_alp.py`中。请注意，与手动课程不同，我们没有指定参数的初始值。而是传递了ALP-GMM进程的边界，这些边界引导了课程的设计。
- en: Now, we are ready to do a curriculum learning bake-off!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备进行一个课程学习的比拼！
- en: Comparing the experiment results
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较实验结果
- en: 'We kick off three training sessions using the manual curriculum we described,
    the ALP-GMM one, and one without any curriculum implemented. The TensorBoard view
    of the training progress is shown in *Figure 14.6*:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们启动了三次训练：一次使用我们描述的手动课程，一次使用ALP-GMM，另一次则没有实现任何课程。训练进度的TensorBoard视图显示在*图14.6*中：
- en: '![Figure 14.6 – Training progress on TensorBoard'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.6 – TensorBoard上的训练进度'
- en: '](img/B14160_14_6.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_14_6.jpg)'
- en: Figure 14.6 – Training progress on TensorBoard
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.6 – TensorBoard上的训练进度
- en: 'A first glance might tell you that the manual curriculum and ALP-GMM are close,
    while not using a curriculum is a distant third. Actually, this is not the case.
    Let''s unpack this plot:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始你可能认为手动课程和ALP-GMM相近，而不使用课程的则排在第三。实际上情况并非如此。让我们来解析这个图：
- en: The manual curriculum goes from easy to difficult. That is why it is at the
    top most of the time. In our run, it could not even get to the latest lesson within
    the time budget. Therefore, the performance shown in the figure is inflated for
    the manual curriculum.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动课程从简单到困难。这就是为什么它大部分时间都排在顶部的原因。在我们的运行中，它甚至在时间预算内无法完成最新的课程。因此，图中显示的手动课程表现被夸大了。
- en: 'The no-curriculum training is always competing at the most difficult level.
    That is why it is at the bottom most of the time: The other agents are not running
    against the hardest parameter configurations so they slowly get there.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无课程训练始终在最困难的级别进行竞争。这就是它大部分时间处于最底部的原因：其他代理并没有运行在最难的参数配置上，因此它们会慢慢到达那里。
- en: ALP-GMM is in the middle for the most part, because it is experimenting with
    difficult and hard configurations at the same time while focusing on somewhere
    in between.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALP-GMM大部分时间处于中间位置，因为它在同时尝试困难和极难的配置，同时关注中间的一些地方。
- en: 'Since this plot is inconclusive, we evaluate the agents on the original (most
    difficult) configuration. The results are the following after 100 test episodes
    for each:'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于这个图没有给出明确结论，我们在原始（最困难）配置上评估了代理的表现。每个代理经过100个测试回合后的结果如下：
- en: '[PRE15]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The manual curriculum performed the worst as it could not get to the latest
    lesson as of the end of the training.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动课程表现最差，因为在训练结束时无法完成最新的课程。
- en: The no-curriculum training had some success, but starting with the most difficult
    setting seems to have set it back. Also, the evaluation performance is in line
    with what is shown on TensorBoard, since the evaluation settings are no different
    from the training settings in this case.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无课程训练取得了一些成功，但从最困难的设置开始似乎让其退步。此外，评估性能与TensorBoard上显示的结果一致，因为在这种情况下，评估设置与训练设置没有不同。
- en: ALP-GMM seems to have benefited from gradually increasing the difficulty and
    performs the best.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ALP-GMM似乎从逐渐增加难度中受益，并且表现最佳。
- en: The no-curriculum training's peak point on the TensorBoard graph is similar
    to ALP-GMM's latest performance. So, our modification with respect to the vertical
    speed of the robot diminished the difference between the two. Not using a curriculum,
    however, causes the agent to not learn at all in many hard-exploration scenarios.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无课程训练在TensorBoard图表上的峰值与ALP-GMM的最新表现相似。因此，我们在垂直速度方面的修改减少了两者之间的差异。然而，不使用课程训练会导致代理在许多困难探索场景中完全无法学习。
- en: You can find the code for the evaluation in `Chapter14/evaluate_ppo.py`. Also,
    you can use the script `Chapter14/visualize_policy.py` to watch your trained agents
    in action, see where they fall short, and come up with ideas to improve the performance!
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在`Chapter14/evaluate_ppo.py`中找到评估的代码。此外，您还可以使用脚本`Chapter14/visualize_policy.py`来观察您训练的代理如何表现，看到它们的不足之处，并提出改进性能的想法！
- en: This concludes our discussion on the KUKA example of robot learning. In the
    next section, we will wrap up this chapter with a list of some popular simulation
    environments used to train autonomous robots and vehicles.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分结束了我们对KUKA机器人学习示例的讨论。在下一部分，我们将总结本章，并列出一些常用的仿真环境，用于训练自动化机器人和车辆。
- en: Going beyond PyBullet into autonomous driving
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越PyBullet，进入自动驾驶领域
- en: 'PyBullet is a great environment to test the capabilities of reinforcement learning
    algorithms in a high-fidelity physics simulation. Some of the other libraries
    you will come across at the intersection of robotics and reinforcement learning
    are as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: PyBullet是一个极好的环境，可以在高保真物理仿真中测试强化学习算法的能力。在机器人技术和强化学习交叉的领域，您会遇到以下一些库：
- en: 'Gazebo: [http://gazebosim.org/](http://gazebosim.org/).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gazebo: [http://gazebosim.org/](http://gazebosim.org/)。'
- en: 'MuJoCo (requires a license): [http://www.mujoco.org/](http://www.mujoco.org/).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MuJoCo（需要许可证）：[http://www.mujoco.org/](http://www.mujoco.org/)。
- en: 'Adroit: [https://github.com/vikashplus/Adroit](https://github.com/vikashplus/Adroit).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Adroit: [https://github.com/vikashplus/Adroit](https://github.com/vikashplus/Adroit)。'
- en: In addition, you will see Unity and Unreal Engine-based environments used to
    train reinforcement learning agents.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您将看到基于Unity和Unreal Engine的环境被用于训练强化学习代理。
- en: 'The next and more popular level of autonomy is of course autonomous vehicles.
    RL is increasingly experimented with in realistic autonomous vehicle simulations
    as well. The most popular libraries in this area are these:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个更常见的自主性层级当然是自动驾驶车辆。RL在现实的自动驾驶车辆仿真中也得到了越来越多的实验。这方面最流行的库有：
- en: 'CARLA: [https://github.com/carla-simulator/carla](https://github.com/carla-simulator/carla).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CARLA: [https://github.com/carla-simulator/carla](https://github.com/carla-simulator/carla)。'
- en: 'AirSim: [https://github.com/microsoft/AirSim](https://github.com/microsoft/AirSim).
    (Disclaimer: The author is a Microsoft employee at the time of authoring this
    book and part of the organization developing AirSim.)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AirSim: [https://github.com/microsoft/AirSim](https://github.com/microsoft/AirSim)。（免责声明：本书作者在写作时是微软员工，并且是开发AirSim的团队成员之一。）'
- en: With this, we conclude this chapter on robot learning. This is a very hot application
    area in RL, and there are many environments I hope you enjoyed and are inspired
    to start tinkering with robots.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 到此，我们结束了这一章关于机器人学习的内容。这是RL中的一个非常热门的应用领域，里面有很多环境，我希望您享受这部分内容并从中获得灵感，开始动手尝试机器人。
- en: Summary
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Autonomous robots and vehicles are going to play a huge role in our world in
    the future, and reinforcement learning is one of the primary approaches to create
    such autonomous systems. In this chapter, we have taken a peek at what it looks
    like to train a robot to accomplish an object-grasping task, a major challenge
    in robotics with many applications in manufacturing and material handling in warehouses.
    We used the PyBullet physics simulator to train a KUKA robot in a hard-exploration
    setting, for which we used both manual and ALP-GMM-based curriculum learning.
    Now that you have a fairly good grasp of how to utilize these techniques, you
    can take on other similar problems.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 自主机器人和车辆将在未来在我们的世界中发挥巨大作用，而强化学习是创建这些自主系统的主要方法之一。在本章中，我们简单了解了训练机器人完成物体抓取任务的过程，这是机器人学中的一大挑战，在制造业和仓库物料搬运中有着广泛应用。我们使用PyBullet物理仿真器，在一个困难的探索环境中训练了一个KUKA机器人，并使用了手动和基于ALP-GMM的课程学习方法。现在，既然你已经较为熟练地掌握了如何利用这些技术，你可以尝试解决其他类似的问题。
- en: 'In the next chapter, we will look into another major area for reinforcement
    learning applications: supply chain management. Stay tuned for another exciting
    journey!'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨强化学习应用的另一个主要领域：供应链管理。敬请期待另一段激动人心的旅程！
- en: References
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Coumans, E., Bai, Y. (2016-2019). PyBullet, a Python module for physics simulation
    for games, robotics, and machine learning. URL: [http://pybullet.org](http://pybullet.org).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coumans, E., Bai, Y. (2016-2019). PyBullet，一个用于游戏、机器人和机器学习的物理仿真Python模块. 网址：[http://pybullet.org](http://pybullet.org)。
- en: 'Bulletphysics/Bullet3\. (2020). Bullet Physics SDK, GitHub. URL: [https://github.com/bulletphysics/bullet3](https://github.com/bulletphysics/bullet3).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bulletphysics/Bullet3\. (2020). Bullet Physics SDK, GitHub. 网址：[https://github.com/bulletphysics/bullet3](https://github.com/bulletphysics/bullet3)。
- en: 'CNC Robotics. (2018). KUKA Industrial Robots, Robotic Specialists. URL: [https://www.cncrobotics.co.uk/news/kuka-robots/](https://www.cncrobotics.co.uk/news/kuka-robots/).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNC Robotics. (2018). KUKA工业机器人，机器人专家. 网址：[https://www.cncrobotics.co.uk/news/kuka-robots/](https://www.cncrobotics.co.uk/news/kuka-robots/)。
- en: 'KUKA AG. (2020). URL: [https://www.kuka.com/en-us](https://www.kuka.com/en-us).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KUKA AG. (2020). 网址：[https://www.kuka.com/en-us](https://www.kuka.com/en-us)。
- en: Portelas, Rémy, et al. (2019). Teacher Algorithms for Curriculum Learning of
    Deep RL in Continuously Parameterized Environments. ArXiv:1910.07224 [Cs, Stat],
    Oct. 2019\. arXiv.org, [http://arxiv.org/abs/1910.07224](http://arxiv.org/abs/1910.07224).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Portelas, Rémy, 等人. (2019). 深度强化学习在连续参数化环境中的教学算法. ArXiv:1910.07224 [Cs, Stat],
    2019年10月\. arXiv.org, [http://arxiv.org/abs/1910.07224](http://arxiv.org/abs/1910.07224)。
