- en: The Apache Spark Ecosystem
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark生态系统
- en: 'Apache Spark ([http://spark.apache.org/](http://spark.apache.org/)) is an open
    source, fast cluster-computing platform. It was originally created by AMPLab at
    the University of California, Berkeley. Its source code was later donated to the
    Apache Software Foundation ([https://www.apache.org/](https://www.apache.org/)).
    Spark comes with a very fast computation speed because data is loaded into distributed
    memory (RAM) across a cluster of machines. Not only can data be quickly transformed,
    but also cached on demand for a variety of use cases. Compared to Hadoop MapReduce,
    it runs programs up to 100 times faster when the data fits in memory, or 10 times
    faster on disk. Spark provides support for four programming languages: Java, Scala,
    Python, and R. This book covers the Spark APIs (and deep learning frameworks)
    for Scala ([https://www.scala-lang.org/](https://www.scala-lang.org/)) and Python
    ([https://www.python.org/](https://www.python.org/)) only.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark ([http://spark.apache.org/](http://spark.apache.org/)) 是一个开源的快速集群计算平台。最初由加利福尼亚大学伯克利分校的AMPLab创建，后来其源代码被捐赠给了Apache软件基金会
    ([https://www.apache.org/](https://www.apache.org/))。Spark因其计算速度非常快而广受欢迎，因为数据被加载到分布式内存（RAM）中，分布在集群中的各台机器上。数据不仅可以快速转换，还可以根据需要进行缓存，适用于多种用例。与Hadoop
    MapReduce相比，当数据能够放入内存时，Spark的程序运行速度快达100倍，或者在磁盘上快10倍。Spark支持四种编程语言：Java、Scala、Python和R。本书仅涵盖Scala
    ([https://www.scala-lang.org/](https://www.scala-lang.org/)) 和Python ([https://www.python.org/](https://www.python.org/))
    的Spark API（以及深度学习框架）。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Apache Spark fundamentals
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark基础知识
- en: Getting Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取Spark
- en: '**Resilient Distributed Dataset** (**RDD**) programming'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Resilient Distributed Dataset**（**RDD**）编程'
- en: Spark SQL, Datasets, and DataFrames
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL、Datasets和DataFrames
- en: Spark Streaming
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: Cluster mode using a different manager
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同管理器的集群模式
- en: Apache Spark fundamentals
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark基础知识
- en: This section covers the Apache Spark fundamentals. It is important to become
    very familiar with the concepts that are presented here before moving on to the
    next chapters, where we'll be exploring the available APIs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍Apache Spark的基础知识。在进入下一个章节之前，熟悉这里呈现的概念非常重要，因为后续我们将探索可用的API。
- en: 'As mentioned in the introduction to this chapter, the Spark engine processes
    data in distributed memory across the nodes of a cluster. The following diagram
    shows the logical structure of how a typical Spark job processes information:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章导言所述，Spark引擎在集群节点的分布式内存中处理数据。以下图表显示了一个典型Spark作业处理信息的逻辑结构：
- en: '![](img/eeaba019-7780-4cff-b4ae-4826b460da54.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eeaba019-7780-4cff-b4ae-4826b460da54.png)'
- en: Figure 1.1
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1
- en: 'Spark executes a job in the following way:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Spark以以下方式执行作业：
- en: '![](img/443b4632-a826-4125-8f2e-f94068acc6c5.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/443b4632-a826-4125-8f2e-f94068acc6c5.png)'
- en: Figure 1.2
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2
- en: The **Master** controls how data is partitioned and takes advantage of data
    locality while keeping track of all the distributed data computation on the **Slave**
    machines. If a certain Slave machine becomes unavailable, the data on that machine
    is reconstructed on another available machine(s). In standalone mode, the Master
    is a single point of failure. This chapter's *Cluster mode using different managers* section
    covers the possible running modes and explains fault tolerance in Spark.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**Master**控制数据的分区方式，并利用数据局部性，同时跟踪所有在**Slave**机器上的分布式数据计算。如果某台Slave机器不可用，该机器上的数据会在其他可用机器上重新构建。在独立模式下，Master是单点故障。章节中关于*使用不同管理器的集群模式*部分涵盖了可能的运行模式，并解释了Spark中的容错机制。'
- en: 'Spark comes with five major components:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Spark包含五个主要组件：
- en: '![](img/a81796d6-ff55-4a8f-9210-a8fd4335a2b6.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a81796d6-ff55-4a8f-9210-a8fd4335a2b6.png)'
- en: Figure 1.3
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3
- en: 'These components are as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组件如下：
- en: The core engine.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心引擎。
- en: '**Spark SQL**: A module for structured data processing.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**：结构化数据处理模块。'
- en: '**Spark Streaming**: This extends the core Spark API. It allows live data stream
    processing. Its strengths include scalability, high throughput, and fault tolerance.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming**：这是对核心Spark API的扩展。它允许实时数据流处理。其优势包括可扩展性、高吞吐量和容错性。'
- en: '**MLib**: The Spark machine learning library.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLib**：Spark机器学习库。'
- en: '**GraphX**: Graphs and graph-parallel computation algorithms.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX**：图形和图并行计算算法。'
- en: Spark can access data that's stored in different systems, such as HDFS, Cassandra,
    MongoDB, relational databases, and also cloud storage services such as Amazon
    S3 and Azure Data Lake Storage.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 可以访问存储在不同系统中的数据，如 HDFS、Cassandra、MongoDB、关系型数据库，还可以访问如 Amazon S3 和 Azure
    Data Lake Storage 等云存储服务。
- en: Getting Spark
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取 Spark
- en: Now, let's get hands-on with Spark so that we can go deeper into the core APIs
    and libraries. In all of the chapters of this book, I will be referring to the
    2.2.1 release of Spark, however, several examples that are presented here should
    work with the 2.0 release or later. I will put a note when an example is specifically
    for 2.2+ releases only.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们动手实践 Spark，以便深入了解核心 API 和库。在本书的所有章节中，我将引用 Spark 2.2.1 版本，然而，这里展示的多个示例应适用于
    2.0 版本及更高版本。我会在示例仅适用于 2.2+ 版本时做出说明。
- en: 'First of all, you need to download Spark from its official website ([https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)).
    The download page should look like this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要从官方网站下载 Spark（[https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)）。下载页面应该是这样的：
- en: '![](img/f94983ad-59e1-4a1b-9846-824a1483a3b6.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f94983ad-59e1-4a1b-9846-824a1483a3b6.png)'
- en: Figure 1.4
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4
- en: You need to have JDK 1.8+ and Python 2.7+ or 3.4+ (only if you need to develop
    using this language). Spark 2.2.1 supports Scala 2.11\. The JDK needs to be present
    on your user path system variable, though, alternatively, you could have your
    user `JAVA_HOME` environment variable pointing to a JDK installation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要安装 JDK 1.8+ 和 Python 2.7+ 或 3.4+（如果你需要使用这些语言进行开发）。Spark 2.2.1 支持 Scala 2.11。JDK
    需要在你的用户路径系统变量中存在，或者你可以设置用户的 `JAVA_HOME` 环境变量指向 JDK 安装目录。
- en: Extract the content of the downloaded archive to any local directory. Move to
    the `$SPARK_HOME/bin` directory. There, among the other executables, you will
    find the interactive Spark shells for Scala and Python. They are the best way
    to get familiar with this framework. In this chapter, I am going to present examples
    that you can run through these shells.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 将下载的压缩包内容解压到任何本地目录。然后，进入 `$SPARK_HOME/bin` 目录。在那里，你会找到 Scala 和 Python 的交互式 Spark
    shell。它们是熟悉这个框架的最佳方式。在本章中，我将展示你可以通过这些 shell 运行的示例。
- en: 'You can run a Scala shell using the following command:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下命令运行一个 Scala shell：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If you don''t specify an argument, Spark assumes that you''re running locally
    in standalone mode. Here''s the expected output to the console:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有指定参数，Spark 会假设你在本地以独立模式运行。以下是控制台的预期输出：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The web UI is available at the following URL: `http://<host>:4040`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Web 用户界面可以通过以下 URL 访问：`http://<host>:4040`。
- en: 'It will give you the following output:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 它将给你以下输出：
- en: '![](img/8c9ffd02-bce0-4053-9993-0e7524190af5.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c9ffd02-bce0-4053-9993-0e7524190af5.png)'
- en: Figure 1.5
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5
- en: From there, you can check the status of your jobs and executors.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以查看作业和执行器的状态。
- en: From the output of the console startup, you will notice that two built-in variables,
    `sc` and `spark`, are available. `sc` represents the `SparkContext` ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)),
    which in Spark < 2.0 was the entry point for each application. Through the Spark
    context (and its specializations), you can get input data from data sources, create
    and manipulate RDDs ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)),
    and attain the Spark primary abstraction before 2.0\. The *RDD programming* section
    will cover this topic and other operations in more detail. Starting from release
    2.0, a new entry point, `SparkSession` ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)),
    and a new main data abstraction, the Dataset ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)),
    were introduced. More details on them are presented in the following sections.
    The `SparkContext` is still part of the Spark API so that compatibility with existing
    frameworks not supporting Spark sessions is ensured, but the direction the project
    has taken is to move development to use the `SparkSession`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 从控制台启动的输出中，你会注意到有两个内建变量，`sc`和`spark`是可用的。`sc`表示`SparkContext`（[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)），在Spark
    < 2.0中，这是每个应用的入口点。通过Spark上下文（及其专用版本），你可以从数据源获取输入数据，创建和操作RDD（[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)），并获取2.0之前Spark的主要抽象。*RDD编程*部分将详细介绍这一主题和其他操作。从2.0版本开始，引入了一个新的入口点`SparkSession`（[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)），以及一个新的主数据抽象——Dataset（[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)）。后续章节将介绍更多细节。`SparkContext`仍然是Spark
    API的一部分，确保与不支持Spark会话的现有框架兼容，但项目的方向是将开发重点转向使用`SparkSession`。
- en: 'Here''s an example of how to read and manipulate a text file and put it into
    a Dataset using the Spark shell (the file used in this example is part of the
    resources for the examples that are bundled with the Spark distribution):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个如何读取和操作文本文件，并使用Spark shell将其放入Dataset的例子（该例子使用的文件是Spark分发包中示例资源的一部分）：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The result is a Dataset instance that contains the file lines. You can then
    make several operations on this Dataset, such as counting the number of lines:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个包含文件行的Dataset实例。你可以对这个Dataset进行多种操作，比如统计行数：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can also get the first line of the Dataset:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以获取Dataset的第一行：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this example, we used a path on the local filesystem. In these cases, the
    file should be accessible from the same path by all of the workers, so you will
    need to copy the file across all workers or use a network-mounted shared filesystem.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了本地文件系统中的路径。在这种情况下，所有工作节点都应该能从相同路径访问该文件，因此你需要将文件复制到所有工作节点，或者使用一个网络挂载的共享文件系统。
- en: 'To close a shell, you can type the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要关闭一个shell，你可以输入以下命令：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To see the list of all of the available shell commands, type the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看所有可用的shell命令列表，输入以下命令：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: All commands can be abbreviated, for example, `:he` instead of `:help`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所有命令都可以缩写，例如，使用`:he`代替`:help`。
- en: 'The following is the list of commands:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是命令列表：
- en: '| **Commands** |  **Purpose** |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| **命令** |  **目的** |'
- en: '| `:edit <id>&#124;<line>`  | Edit history |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `:edit <id>&#124;<line>`  | 编辑历史记录 |'
- en: '| `:help [command]`  | Prints summary or command-specific help |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `:help [command]`  | 打印总结或命令特定的帮助 |'
- en: '| `:history [num]`   | Shows history (optional `num` is commands to show) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `:history [num]`   | 显示历史记录（可选的`num`是要显示的命令数量） |'
- en: '| `:h? <string>`     | Search history |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `:h? <string>`     | 搜索历史记录 |'
- en: '| `:imports [name name ...]`  | Show import history, identifying the sources
    of names |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `:imports [name name ...]`  | 显示导入历史，标识名称的来源 |'
- en: '| `:implicits [-v]`           | Show the `implicits` in scope |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `:implicits [-v]`           | 显示作用域中的`implicits` |'
- en: '| `:javap <path&#124;class>`  | Disassemble a file or class name |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `:javap <path&#124;class>`  | 反汇编文件或类名 |'
- en: '| `:line <id>&#124;<line>`   | Place line(s) at the end of history |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `:line <id>&#124;<line>`   | 将行放置在历史记录的末尾 |'
- en: '| `:load <path>`  | Interpret lines in a file |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `:load <path>`  | 解释文件中的行 |'
- en: '| `:paste [-raw] [path]`    | Enter paste mode or paste a file |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `:paste [-raw] [path]`    | 进入粘贴模式或粘贴文件 |'
- en: '| `:power`              | Enable power user mode |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `:power`              | 启用高级用户模式 |'
- en: '| `:quit`        | Exit the interpreter |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `:quit`        | 退出解释器 |'
- en: '| `:replay [options]`      | Reset the `repl` and `replay` on all previous
    commands |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `:replay [options]`      | 重置`repl`和所有先前命令上的`replay` |'
- en: '| `:require <path>`   | Add a `jar` to the classpath |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `:require <path>`   | 将`jar`添加到类路径 |'
- en: '| `:reset [options]`    | Reset the `repl` to its initial state, forgetting
    all session entries |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| `:reset [options]`    | 将`repl`重置为初始状态，忘记所有会话条目 |'
- en: '| `:save <path>` | Save the replayable session to a file |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `:save <path>` | 将可重放会话保存到文件中 |'
- en: '| `:sh <command line>`      | Run a shell command (the result is `implicitly
    => List[String]`) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `:sh <command line>`      | 运行Shell命令（结果为`implicitly => List[String]`） |'
- en: '| `:settings <options>`  | Update compiler options, if possible; see `reset`
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `:settings <options>`  | 更新编译器选项，如果可能的话；参见`reset` |'
- en: '| `:silent`           | Disable or enable the automatic printing of results
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `:silent`           | 禁用或启用自动打印结果 |'
- en: '| `:type [-v] <expr>`     | Display the type of expression without evaluating it
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `:type [-v] <expr>`     | 显示表达式的类型而不评估它 |'
- en: '| `:kind [-v] <expr>` | Display the kind of expression |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `:kind [-v] <expr>` | 显示表达式的种类 |'
- en: '| `:warnings`       | Show the suppressed warnings from the most recent line
    that had any |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `:warnings`       | 显示最近一行的抑制警告 |'
- en: 'Like Scala, an interactive shell is available for Python. You can run it using
    the following command:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与Scala一样，Python也有交互式shell。您可以使用以下命令运行它：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'A built-in variable named `spark` representing the `SparkSession` is available.
    You can do the same things as for the Scala shell:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 内置变量名为`spark`，代表`SparkSession`可用。您可以像Scala shell一样做同样的事情：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Unlike Java and Scala, Python is more dynamic and is not strongly typed. Therefore, a
    `DataSet` in Python is a `DataSet[Row]`, but you can call it a DataFrame so that
    it's consistent with the DataFrame concept of the Pandas framework ([https://pandas.pydata.org/](https://pandas.pydata.org/)).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 与Java和Scala不同，Python更加动态，且类型不是强制的。因此，在Python中，`DataSet`是`DataSet[Row]`，但您可以称之为DataFrame，以便与Pandas框架的DataFrame概念保持一致（[https://pandas.pydata.org/](https://pandas.pydata.org/)）。
- en: 'To close a Python shell, you can type the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要关闭Python shell，您可以输入以下内容：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Interactive shells aren''t the only choice for running code in Spark. It is
    also possible to implement self-contained applications. Here''s an example of
    reading and manipulating a file in Scala:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中运行代码的选择不仅限于交互式shell。也可以实现独立的应用程序。以下是在Scala中读取和操作文件的示例：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Applications should define a `main()` method instead of extending `scala.App`.
    Note the code to create `SparkSession`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序应该定义一个`main()`方法，而不是扩展`scala.App`。注意创建`SparkSession`的代码：
- en: '[PRE11]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It follows the builder factory design pattern.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 它遵循生成器工厂设计模式。
- en: 'Always explicitly close the session before ending the program execution:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束程序执行之前，始终显式关闭会话：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To build the application, you can use a build tool of your choice (`Maven`,
    `sbt`, or `Gradle`), adding the dependencies from Spark 2.2.1 and Scala 2.11\.
    Once a JAR file has been generated, you can use the `$SPARK_HOME/bin/spark-submit`
    command to execute it, specifying the JAR filename, the Spark master URL, and
    a list of optional parameters, including the job name, the main class, the maximum
    memory to be used by each executor, and many others.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建应用程序，您可以使用您选择的构建工具（`Maven`、`sbt`或`Gradle`），添加来自Spark 2.2.1和Scala 2.11的依赖项。生成JAR文件后，您可以使用`$SPARK_HOME/bin/spark-submit`命令执行它，指定JAR文件名、Spark主URL和一系列可选参数，包括作业名称、主类、每个执行器使用的最大内存等。
- en: 'The same self-contained application could have been implemented in Python as
    well:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的独立应用也可以在Python中实现：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This can be saved in a `.py` file and submitted through the same `$SPARK_HOME/bin/spark-submit`
    command for execution.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以保存在`.py`文件中，并通过相同的`$SPARK_HOME/bin/spark-submit`命令提交以执行。
- en: RDD programming
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RDD编程
- en: In general, every Spark application is a driver program that runs the logic
    that has been implemented for it and executes parallel operations on a cluster.
    In accordance with the previous definition, the main abstraction provided by the
    core Spark framework is the RDD. It is an immutable distributed collection of
    data that is partitioned across machines in a cluster. Operations on RDDs can
    happen in parallel.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，每个 Spark 应用程序是一个驱动程序，运行为其实现的逻辑并在集群上执行并行操作。根据前面的定义，核心 Spark 框架提供的主要抽象是 RDD。它是一个不可变的分布式数据集合，数据在集群中的机器上进行分区。对
    RDD 的操作可以并行执行。
- en: 'Two types of operations are available on an RDD:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对 RDD 有两种类型的操作：
- en: Transformations
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换
- en: Actions
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行动
- en: A **transformation** is an operation on an RDD that produces another RDD, while
    an **action** is an operation that triggers some computation and then returns
    a value to the master or can be persisted to a storage system. Transformations
    are lazy—they aren't executed until an action is invoked. Here's the strength
    point of Spark—Spark masters and their drivers both remember the transformations
    that have been applied to an RDD, so if a partition is lost (for example, a slave
    goes down), it can be easily rebuilt on some other node of the cluster.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**转换**（**transformation**）是对 RDD 的操作，产生另一个 RDD，而 **行动**（**action**）则是触发某些计算的操作，并将结果返回给主节点，或将结果持久化到存储系统中。转换是惰性执行的——直到调用行动才会执行。这里就是
    Spark 的强大之处——Spark 的主节点和驱动程序都记住了已经应用于 RDD 的转换操作，因此如果一个分区丢失（例如，一个从节点宕机），它可以很容易地在集群的其他节点上重新构建。
    |'
- en: 'The following table lists some of the common transformations supported by Spark:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了 Spark 支持的常见转换：
- en: '| **Transformation** | **Purpose** |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| **转换** | **目的** |'
- en: '| `map(func)` | Returns a new RDD by applying the `func` function on each data
    element of the source RDD. |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `map(func)` | 通过对源 RDD 的每个数据元素应用 `func` 函数，返回一个新的 RDD。 |'
- en: '| `filter(func)` | Returns a new RDD by selecting those data elements for which
    the applied `func` function returns `true`. |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| `filter(func)` | 通过选择那些 `func` 函数返回 `true` 的数据元素，返回一个新的 RDD。 |'
- en: '| `flatMap(func)` | This transformation is similar to `map`: the difference
    is that each input item can be mapped to zero or multiple output items (the applied `func` function
    should return a `Seq`). |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `flatMap(func)` | 这个转换类似于 `map`：不同之处在于，每个输入项可以映射到零个或多个输出项（应用的 `func` 函数应该返回一个
    `Seq`）。 |'
- en: '| `union(otherRdd)` | Returns a new RDD that contains the union of the elements
    in the source RDD and the `otherRdd` argument. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| `union(otherRdd)` | 返回一个新的 RDD，包含源 RDD 和 `otherRdd` 参数中元素的并集。 |'
- en: '| `distinct([numPartitions])` | Returns a new RDD that contains only the distinct
    elements of the source RDD. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `distinct([numPartitions])` | 返回一个新的 RDD，仅包含源 RDD 中的唯一元素。 |'
- en: '| `groupByKey([numPartiotions])` | When called on an RDD of (*K*, *V*) pairs,
    it returns an RDD of (*K*, *Iterable<V>*) pairs. By default, the level of parallelism
    in the output RDD depends on the number of partitions of the source RDD. You can
    pass an optional `numPartitions` argument to set a different number of partitions.
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `groupByKey([numPartitions])` | 当对一个包含 (*K*, *V*) 对的 RDD 调用时，它返回一个 (*K*,
    *Iterable<V>*) 对的 RDD。默认情况下，输出 RDD 的并行度取决于源 RDD 的分区数。你可以传递一个可选的 `numPartitions`
    参数来设置不同的分区数。 |'
- en: '| `reduceByKey(func, [numPartitions])` | When called on an RDD of (*K*, *V*)
    pairs, it returns an RDD of (*K*, *V*) pairs, where the values for each key are
    aggregated using the given reduce `func` function, which must be of type *(V*,*V)
    => V*. The same as for the `groupByKey` transformation, the number of reduce partitions
    is configurable through an optional `numPartitions` second argument.  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKey(func, [numPartitions])` | 当对一个包含 (*K*, *V*) 对的 RDD 调用时，它返回一个
    (*K*, *V*) 对的 RDD，其中每个键的值通过给定的 `func` 函数进行聚合，该函数的类型必须为 *(V*, *V) => V*。与 `groupByKey`
    转换相同，reduce 操作的分区数可以通过可选的 `numPartitions` 第二个参数进行配置。 |'
- en: '| `sortByKey([ascending], [numPartitions])` | When called on an RDD of (*K*,
    *V*) pairs, it returns an RDD of (*K*, *V*) pairs sorted by keys in ascending
    or descending order, as specified in the Boolean `ascending` argument. The number
    of partitions for the output RDD is configurable through an optional `numPartitions`
    second argument. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `sortByKey([ascending], [numPartitions])` | 当对一个包含 (*K*, *V*) 对的 RDD 调用时，它返回一个按键排序的
    (*K*, *V*) 对的 RDD，排序顺序根据布尔值 `ascending` 参数来指定（升序或降序）。输出 RDD 的分区数量可以通过可选的 `numPartitions`
    第二个参数进行配置。 |'
- en: '| `join(otherRdd, [numPartitions])` | When called on RDDs of type (*K*, *V*)
    and (*K*, *W*), it returns an RDD of (*K*, (*V*, *W*)) pairs with all pairs of
    elements for each key. It supports left outer join, right outer join, and full
    outer join. The number of partitions for the output RDD is configurable through
    an optional `numPartitions` second argument. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| `join(otherRdd, [numPartitions])` | 当应用于类型为 (*K*, *V*) 和 (*K*, *W*) 的 RDD
    时，它返回一个 (*K*, (*V*, *W*)) 对的 RDD，为每个键提供所有元素对。它支持左外连接、右外连接和全外连接。输出 RDD 的分区数量可以通过可选的
    `numPartitions` 参数进行配置。 |'
- en: 'The following table lists some of the common actions supported by Spark:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了 Spark 支持的一些常见操作：
- en: '| **Action** | **Purpose** |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| **操作** | **目的** |'
- en: '| `reduce(func)` | Aggregates the elements of an RDD using a given function, `func`
    (this takes two arguments and returns one). To ensure the correct parallelism
    at compute time, the reduce function, `func`, has to be commutative and associative.
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| `reduce(func)` | 使用给定函数 `func` 聚合 RDD 的元素（此函数接受两个参数并返回一个结果）。为了确保计算时的正确并行性，reduce
    函数 `func` 必须是交换律和结合律成立的。 |'
- en: '| `collect()` | Returns all the elements of an RDD as an array to the driver.
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| `collect()` | 返回 RDD 中所有元素作为一个数组传递给驱动程序。 |'
- en: '| `count()` | Returns the total number of elements in an RDD. |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| `count()` | 返回 RDD 中元素的总数。 |'
- en: '| `first()` | Returns the first element of an RDD. |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| `first()` | 返回 RDD 中的第一个元素。 |'
- en: '| `take(n)` | Returns an array containing the first *n* elements of an RDD.
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| `take(n)` | 返回一个包含 RDD 中前 *n* 个元素的数组。 |'
- en: '| `foreach(func)` | Executes the `func` function on each element of an RDD.
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| `foreach(func)` | 对 RDD 中的每个元素执行 `func` 函数。 |'
- en: '| `saveAsTextFile(path)` | Writes the elements of an RDD as a text file in
    a given directory (with the absolute location specified through the `path` argument)
    in the local filesystem, HDFS, or any other Hadoop-supported filesystem. This
    is available for Scala and Java only. |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| `saveAsTextFile(path)` | 将 RDD 的元素以文本文件的形式写入指定目录（通过 `path` 参数指定绝对路径），支持本地文件系统、HDFS
    或其他 Hadoop 支持的文件系统。此功能仅适用于 Scala 和 Java。 |'
- en: '| `countByKey()` | This action is only available on RDDs of type (*K*, *V*)
    – it returns a hashmap of (*K*, *Int*) pairs, where *K* is a key of the source
    RDD and its value is the count for that given key, *K*. |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| `countByKey()` | 这个操作仅适用于类型为 (*K*, *V*) 的 RDD——它返回一个 (*K*, *Int*) 对的哈希映射，其中
    *K* 是源 RDD 的键，值是该键 *K* 的计数。 |'
- en: 'Now, let''s understand the concepts of transformation and action through an
    example that could be executed in the Scala shell—this finds the *N* most commonly
    used words in an input text file. The following diagram depicts a potential implementation
    for this problem:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个示例理解转换和操作的概念，该示例可以在 Scala shell 中执行——它找到输入文本文件中最常用的 *N* 个单词。以下图示展示了这一问题的潜在实现：
- en: '![](img/d5572d5c-5e5e-42db-8f01-0c624d1dc699.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5572d5c-5e5e-42db-8f01-0c624d1dc699.png)'
- en: Figure 1.6
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6
- en: Let's translate this into code.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其转换为代码。
- en: 'First of all, let''s load the content of a text file into an RDD of strings:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将文本文件的内容加载到一个字符串类型的 RDD 中：
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we will apply the necessary transformations and actions:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将应用必要的转换和操作：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, we have the following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们有以下内容：
- en: '`flatMap(str=>str.split(" "))`: Splits each line into single words'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flatMap(str=>str.split(" "))`: 将每一行拆分为单个单词'
- en: '`filter(!_.isEmpty)`: Removes empty strings'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter(!_.isEmpty)`: 移除空字符串'
- en: '`map(word=>(word,1))`: Maps each word into a key-value pair'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map(word=>(word,1))`: 将每个单词映射为键值对'
- en: '`reduceByKey(_+_)`: Aggregates the count'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reduceByKey(_+_)`: 聚合计数'
- en: '`map{case(word, count) => (count, word)}`: Reverses the `(word, count)` pairs
    to `(count, word)`'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map{case(word, count) => (count, word)}`: 反转 `(word, count)` 对为 `(count, word)`'
- en: '`sortByKey(false)`: Sorts by descending order'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sortByKey(false)`: 按降序排序'
- en: 'Finally, print the five most used words in the input content to the console:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将输入内容中使用频率最高的五个单词打印到控制台：
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The same could be achieved in Python in the following way:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的功能也可以在 Python 中通过以下方式实现：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The result, of course, is the same as for the Scala example:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 结果当然与 Scala 示例相同：
- en: '[PRE18]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Spark can persist RDDs (and Datasets as well) in memory while executing operations
    on them. Persisting and caching are synonyms in Spark. When persisting an RDD,
    each node of the cluster stores the RDD partitions that it needs to compute in
    memory and reuses them in further actions on the same RDD (or RDDs that have been
    derived from it through some transformations). This is the reason why future actions
    execute much faster. It is possible to mark an RDD to be persisted using its `persist()`
    method. The first time an action is executed on it, it will be kept in memory
    on the cluster''s nodes. The Spark cache is fault-tolerant—this means that, if
    for any reason all of the partitions of an RDD are lost, it will be automatically
    recalculated using the transformations that created it. A persisted RDD can be
    stored using different storage levels. Levels can be set by passing a `StorageLevel`
    object to the `persist()` method of the RDD. The following table lists all of
    the available storage levels and their meanings:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 可以在执行操作时将 RDD（以及数据集）持久化到内存中。在 Spark 中，持久化和缓存是同义词。持久化 RDD 时，集群中的每个节点会将需要计算的
    RDD 分区存储在内存中，并在对相同的 RDD（或通过一些转换从其派生的 RDD）进行进一步操作时重用这些分区。这就是为什么后续操作执行得更快的原因。可以通过调用
    RDD 的 `persist()` 方法来标记一个 RDD 进行持久化。当第一次对它执行操作时，它会被保存在集群节点的内存中。Spark 缓存是容错的——这意味着，如果由于某种原因丢失了
    RDD 的所有分区，它将通过创建它的转换重新计算这些分区。持久化的 RDD 可以使用不同的存储级别来存储。可以通过将一个 `StorageLevel` 对象传递给
    RDD 的 `persist()` 方法来设置级别。以下表格列出了所有可用的存储级别及其含义：
- en: '| **Storage Level** | **Purpose** |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| **存储级别** | **用途** |'
- en: '| `MEMORY_ONLY` | This is the default storage level. It stores RDDs as deserialized
    Java objects in memory. In those cases where an RDD shouldn''t fit in memory,
    some of its partitions won''t be cached and will be recalculated on the fly when
    needed. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_ONLY` | 这是默认的存储级别。它将 RDD 存储为反序列化的 Java 对象在内存中。在 RDD 无法完全放入内存的情况下，一些分区不会被缓存，需要时会动态重新计算。
    |'
- en: '| `MEMORY_AND_DISK` | It stores RDDs as deserialized Java objects in memory
    first, but, in those cases where an RDD shouldn''t fit in memory, it stores some
    partitions on disk (this is the main difference between `MEMORY_ONLY`), and reads
    them from there when needed. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_AND_DISK` | 它首先将 RDD 存储为反序列化的 Java 对象在内存中，但当 RDD 无法完全放入内存时，它会将部分分区存储在磁盘上（这是与
    `MEMORY_ONLY` 之间的主要区别），并在需要时从磁盘中读取。 |'
- en: '| `MEMORY_ONLY_SER` | It stores RDDs as serialized Java objects. Compared to
    `MEMORY_ONLY`, this is more space-efficient, but more CPU-intensive in read operations.
    This is available for JVM languages only. |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_ONLY_SER` | 它将 RDD 存储为序列化的 Java 对象。与 `MEMORY_ONLY` 相比，这种方式更节省空间，但在读取操作时
    CPU 占用更高。仅适用于 JVM 语言。 |'
- en: '| `MEMORY_AND_DISK_SER` | Is similar to `MEMORY_ONLY_SER` (it stores RDDs as
    serialized Java objects), with the main difference being that it stores partitions
    that don''t fit in memory to disk. This is available only for JVM languages. |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_AND_DISK_SER` | 类似于 `MEMORY_ONLY_SER`（将 RDD 存储为序列化的 Java 对象），主要区别在于，对于无法完全放入内存的分区，它将其存储到磁盘中。仅适用于
    JVM 语言。 |'
- en: '| `DISK_ONLY` | It stores the RDD partitions on disk only. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `DISK_ONLY` | 仅将 RDD 分区存储在磁盘中。 |'
- en: '| `MEMORY_ONLY_2`, `MEMORY_AND_DISK_2`, and so on | The same as the two preceding
    levels (`MEMORY_ONLY` and `MEMORY_AND_DISK`), but each partition is replicated
    on two cluster nodes. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `MEMORY_ONLY_2`、`MEMORY_AND_DISK_2` 等 | 与前两种级别（`MEMORY_ONLY` 和 `MEMORY_AND_DISK`）相同，但每个分区会在两个集群节点上复制。
    |'
- en: '| `OFF_HEAP` | Similar to `MEMORY_ONLY_SER`, but it stores data in off-heap
    memory (assuming off-heap memory is enabled). Please be careful when using this
    storage level as it is still experimental. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `OFF_HEAP` | 类似于 `MEMORY_ONLY_SER`，但它将数据存储在堆外内存中（假设启用了堆外内存）。使用此存储级别时需要小心，因为它仍处于实验阶段。
    |'
- en: When a function is passed to a Spark operation, it is executed on a remote cluster
    node that will work on separate copies of all the variables that are used in the
    function. Once done, the variables will be copied to each machine. There will
    be no updates to the variables on the remote machine when propagated back to the
    driver program. It would be inefficient to support general, read-write shared
    variables across tasks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个函数被传递给 Spark 操作时，它会在远程集群节点上执行，该节点将处理函数中使用的所有变量的单独副本。执行完成后，这些变量将被复制到每台机器上。当变量传回驱动程序时，远程机器上的变量不会被更新。支持跨任务的一般读写共享变量是低效的。
- en: However, there are two limited types of shared variables that are available
    in Spark for two common usage patterns – broadcast variables and accumulators.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在 Spark 中有两种有限类型的共享变量可供使用，适用于两种常见的使用模式——广播变量和累加器。
- en: 'One of the most common operations in Spark programming is to perform joins
    on RDDs to consolidate data by a given key. In these cases, it is quite possible
    to have large Datasets sent around to slave nodes that host the partitions to
    be joined. You can easily understand that this situation presents a huge performance
    bottleneck, as network I/O is 100 times slower than RAM access. To mitigate this
    issue, Spark provides broadcast variables, which are broadcast to slave nodes.
    RDD operations on the nodes can quickly access the broadcast variable value. Spark
    also attempts to distribute broadcast variables using efficient broadcast algorithms
    to reduce communication costs. Broadcast variables are created from a variable, *v*,
    by calling the `SparkContext.broadcast(v)` method. The broadcast variable is a
    wrapper around *v*, and its value can be obtained by calling the `value` method.
    Here''s an example in Scala that you can run through the Spark shell:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 编程中最常见的操作之一是对 RDD 执行连接操作，根据给定的键整合数据。在这些情况下，可能会有大量的数据集被发送到执行分区的从属节点进行连接。可以很容易地理解，这种情况会导致巨大的性能瓶颈，因为网络
    I/O 的速度比内存访问慢 100 倍。为了解决这个问题，Spark 提供了广播变量，可以将其广播到从属节点。节点上的 RDD 操作可以快速访问广播变量的值。Spark
    还尝试使用高效的广播算法来分发广播变量，以减少通信开销。广播变量是通过调用`SparkContext.broadcast(v)`方法从变量*v*创建的。广播变量是*v*的一个封装，其值可以通过调用`value`方法获取。以下是一个可以通过
    Spark shell 运行的 Scala 示例：
- en: '[PRE19]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: After its creation, the broadcast variable, `broadcastVar`, can be used in any
    function that's executed on the cluster, but not the initial value, *v,* as this
    prevents *v* being shipped to all the nodes more than once. To ensure that all
    the nodes get the same value of the broadcast variable, *v* must not be modified
    after `broadcastVar` has been broadcast.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建后，广播变量`broadcastVar`可以在集群中执行的任何函数中使用，但初始值*v*除外，因为这可以防止*v*被多次发送到所有节点。为了确保所有节点都能获得相同的广播变量值，*v*在`broadcastVar`广播后不能被修改。
- en: 'Here''s the code for the same example in Python:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是相同示例的 Python 代码：
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: To aggregate information across executors in a Spark cluster, `accumulator`
    variables should be used. The fact that they are added through an associative
    and commutative operation ensures their efficient support in parallel computation.
    Spark natively provides support for the accumulators of numeric types—they can
    be created by calling `SparkContext.longAccumulator()` (to accumulate values of
    type `Long`) or `SparkContext.doubleAccumulator()` (to accumulate values of type
    `Double`) methods.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 Spark 集群的执行器之间聚合信息，应该使用`accumulator`变量。由于它们通过一个结合性和交换性的操作进行添加，因此可以有效支持并行计算。Spark
    本地支持数值类型的累加器——可以通过调用`SparkContext.longAccumulator()`（用于累加`Long`类型的值）或`SparkContext.doubleAccumulator()`（用于累加`Double`类型的值）方法来创建它们。
- en: 'However, it is possible to programmatically add support for other types. Any
    task running on a cluster can add to an accumulator using the `add` method, but
    they cannot read its value – this operation is only allowed for the driver program,
    which uses its `value` method. Here''s a code example in Scala:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也可以通过编程方式为其他类型提供支持。任何在集群上运行的任务都可以使用`add`方法向累加器添加值，但它们无法读取累加器的值——这个操作只允许驱动程序执行，它可以使用累加器的`value`方法。以下是
    Scala 中的代码示例：
- en: '[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In this case, an accumulator has been created, and has assigned a name to it.
    It is possible to create unnamed accumulators, but a named accumulator will display
    in the web UI for the stage that modifies that accumulator:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，已经创建了一个累加器，并给它分配了一个名称。可以创建没有名称的累加器，但具有名称的累加器将在修改该累加器的阶段的 Web UI 中显示：
- en: '![](img/54fa8bc2-1eb4-4f0a-bed6-8f5ce959ba84.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54fa8bc2-1eb4-4f0a-bed6-8f5ce959ba84.png)'
- en: Figure 1.7
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7
- en: This can be helpful for understanding the progress of running stages.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于理解运行阶段的进度很有帮助。
- en: 'The same example in Python is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的 Python 示例如下：
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Tracking accumulators in the web UI isn't supported for Python.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Python 不支持在 Web UI 中跟踪累加器。
- en: Please be aware that Spark guarantees to update accumulators *inside actions
    only*. When restarting a task, the accumulators will be updated only once. The
    same isn't true for transformations.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Spark 仅在**action**内部更新累加器。当重新启动任务时，累加器只会更新一次。对于**transformation**，情况则不同。
- en: Spark SQL, Datasets, and DataFrames
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL、Datasets和DataFrames
- en: Spark SQL is the Spark module for structured data processing. The main difference
    between this API and the RDD API is that the provided Spark SQL interfaces give
    more information about the structure of both the data and the performed computation.
    This extra information is used by Spark internally to add extra optimizations
    through the Catalyst optimization engine, which is the same execution engine that's
    used regardless of whatever API or programming language is involved.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是用于结构化数据处理的Spark模块。这个API和RDD API的主要区别在于，提供的Spark SQL接口可以更好地了解数据和执行计算的结构。这些额外的信息被Spark内部利用，通过Catalyst优化引擎进行额外的优化，这个执行引擎无论使用哪种API或编程语言，都是相同的。
- en: Spark SQL is commonly used to execute SQL queries (even if this isn't the only
    way to use it). Whatever programming language supported by Spark encapsulates
    the SQL code to be executed, the results of a query are returned as a **Dataset**.
    A Dataset is a distributed collection of data, and was added as an interface in
    Spark 1.6\. It combines the benefits of RDDs (such as strong typing and the ability
    to apply useful lambda functions) with the benefits of Spark SQL's optimized execution
    engine (Catalyst, [https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)).
    You can construct a Dataset by starting with Java/Scala objects and then manipulating
    it through the usual functional transformations. The Dataset API is available
    in Scala and Java, while Python doesn't have support for it. However, due to the
    dynamic nature of this programming language, many of the benefits of the Dataset
    API are already available for it.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL通常用于执行SQL查询（即使这不是唯一的使用方式）。无论使用Spark支持的哪种编程语言来封装SQL代码，查询的结果都会以**Dataset**的形式返回。Dataset是一个分布式数据集合，它作为接口在Spark
    1.6中被引入。它结合了RDD的优势（如强类型和应用有用的lambda函数的能力）与Spark SQL优化执行引擎（Catalyst，[https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)）的优势。你可以通过从Java/Scala对象开始并通过常规的函数式转换来操作Dataset。Dataset
    API在Scala和Java中可用，而Python不支持它。然而，由于Python语言的动态特性，很多Dataset API的优势在Python中已经可以使用。
- en: Starting from Spark 2.0, the DataFrame and Dataset APIs have been merged into
    the Dataset API, so a **DataFrame** is just a Dataset that's been organized into
    named columns and is conceptually equivalent to a table in an RDBMS, but with
    better optimizations under the hood (being part of the Dataset API, the Catalyst
    optimization engine works behind the scenes for DataFrames, too). You can construct
    a DataFrame from diverse sources, such as structured data files, Hive tables,
    database tables, and RDDs, to name a few. Unlike the Dataset API, the DataFrame
    API is available in any of the programming languages that are supported by Spark.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 从Spark 2.0开始，DataFrame和Dataset API已合并为Dataset API，因此**DataFrame**只是一个已经被组织成命名列的Dataset，在概念上等同于RDBMS中的表，但其底层优化更佳（作为Dataset
    API的一部分，Catalyst优化引擎也在幕后为DataFrame工作）。你可以从不同的数据源构建DataFrame，例如结构化数据文件、Hive表、数据库表和RDD等。与Dataset
    API不同，DataFrame API可以在任何Spark支持的编程语言中使用。
- en: Let's start and get hands-on so that we can better understand the concepts behind
    Spark SQL. The first full example I am going to show is Scala-based. Start a Scala
    Spark shell to run the following code interactively.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始动手实践，以便更好地理解Spark SQL背后的概念。我将展示的第一个完整示例是基于Scala的。启动一个Scala Spark shell，以交互方式运行以下代码。
- en: 'Let''s use `people.json` as a data source. One of the files that''s available
    as a resource for this example has been shipped along with the Spark distribution
    and can be used to create a DataFrame that''s a Dataset of Rows ([http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row)):'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`people.json`作为数据源。作为此示例的资源之一，包含在Spark分发包中的文件可用于创建一个DataFrame，这是一个行的Dataset（[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row)）：
- en: '[PRE23]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can print the content of the DataFrame to the console to check that it
    is what you expected:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将DataFrame的内容打印到控制台，检查它是否符合你的预期：
- en: '[PRE24]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Before you perform DataFrame operations, you need to import the implicit conversions
    (such as converting RDDs to DataFrames) and use the `$` notation:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行DataFrame操作之前，你需要导入隐式转换（例如将RDD转换为DataFrame），并使用`$`符号：
- en: '[PRE25]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, you can print the DataFrame schema in a tree format:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以以树状格式打印DataFrame模式：
- en: '[PRE26]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Select a single column (let''s say `name`):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 选择单一列（例如`name`）：
- en: '[PRE27]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Filter the data:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤数据：
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then add a  `groupBy` clause:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 然后添加一个`groupBy`子句：
- en: '[PRE29]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Select all rows and increment a numeric field:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 选择所有行并递增一个数字字段：
- en: '[PRE30]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'It is possible to run SQL queries programmatically through the `sql` function
    of `SparkSession`. This function returns the results of the query in a DataFrame,
    which, for Scala, is a `Dataset[Row]`. Let''s consider the same DataFrame as for
    the previous example:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过`SparkSession`的`sql`函数以编程方式运行SQL查询。该函数返回查询结果的DataFrame，在Scala中是`Dataset[Row]`。让我们考虑与前面示例相同的DataFrame：
- en: '[PRE31]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You can register it as an SQL temporary view:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将其注册为SQL临时视图：
- en: '[PRE32]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, you can execute an SQL query there:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以在此处执行SQL查询：
- en: '[PRE33]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The same things can be done in Python as well:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中也可以执行相同的操作：
- en: '[PRE34]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Resulting in the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE35]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Other features of Spark SQL and Datasets (data sources, aggregations, self-contained
    applications, and so on) will be covered in [Chapter 3](44fab060-12c9-4eec-9e15-103da589a510.xhtml), *Extract,
    Transform, Load*.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL和Datasets的其他功能（数据源、聚合、自包含应用等）将在[第3章](44fab060-12c9-4eec-9e15-103da589a510.xhtml)中介绍，*提取、转换、加载*。
- en: Spark Streaming
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: 'Spark Streaming is another Spark module that extends the core Spark API and
    provides a scalable, fault-tolerant, and efficient way of processing live streaming
    data. By converting streaming data into *micro* batches, Spark''s simple batch
    programming model can be applied in streaming use cases too. This unified programming
    model makes it easy to combine batch and interactive data processing with streaming.
    Diverse sources that ingest data are supported (Kafka, Kinesis, TCP sockets, S3,
    or HDFS, just to mention a few of the popular ones), as well as data coming from
    them, and can be processed using any of the high-level functions available in
    Spark. Finally, the processed data can be persisted to RDBMS, NoSQL databases,
    HDFS, object storage systems, and so on, or consumed through live dashboards.
    Nothing prevents other advanced Spark components, such as MLlib or GraphX, being
    applied to data streams:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming是另一个Spark模块，扩展了核心Spark API，并提供了一种可扩展、容错和高效的方式来处理实时流数据。通过将流数据转换为*微*批次，Spark的简单批处理编程模型也可以应用于流式用例中。这个统一的编程模型使得将批处理和交互式数据处理与流式处理相结合变得容易。支持多种数据摄取源（例如Kafka、Kinesis、TCP套接字、S3或HDFS等流行源），并且可以使用Spark中任何高级函数来处理从这些源获取的数据。最终，处理过的数据可以持久化到关系数据库、NoSQL数据库、HDFS、对象存储系统等，或通过实时仪表板进行消费。没有什么可以阻止其他高级Spark组件（如MLlib或GraphX）应用于数据流：
- en: '![](img/7c308eea-7aa6-4d4c-a97a-8493b28bdaef.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c308eea-7aa6-4d4c-a97a-8493b28bdaef.png)'
- en: Figure 1.8
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.8
- en: 'The following diagram shows how Spark Streaming works internally—it receives
    live input data streams and divides them into batches; these are processed by
    the Spark engine to generate the final batches of results:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了Spark Streaming的内部工作原理——它接收实时输入数据流并将其划分为批次；这些批次由Spark引擎处理，生成最终的批量结果：
- en: '![](img/5bef871b-ee77-48d0-bb68-d403cd2acc13.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5bef871b-ee77-48d0-bb68-d403cd2acc13.png)'
- en: Figure 1.9
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.9
- en: The higher-level abstraction of Spark Streaming is the **DStream** (short for
    **Discretized Stream**), which is a wrapper around a continuous flow of data.
    Internally, a DStream is represented as a sequence of RDDs. A DStream contains
    a list of other DStreams that it depends on, a function to convert its input RDDs
    into output ones, and a time interval at which to invoke the function. DStreams
    are created by either manipulating existing ones, for example, applying a map
    or filter function (which internally creates `MappedDStreams` and `FilteredDStreams`,
    respectively), or by reading from an external source (the base class in these
    cases is `InputDStream`).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming的高级抽象是**DStream**（**Discretized Stream**的简称），它是对连续数据流的封装。从内部来看，DStream是作为一系列RDD的序列来表示的。DStream包含它依赖的其他DStream的列表、将输入RDD转换为输出RDD的函数，以及调用该函数的时间间隔。DStream可以通过操作现有的DStream来创建，例如应用map或filter函数（这分别内部创建了`MappedDStreams`和`FilteredDStreams`），或者通过从外部源读取数据（在这些情况下，基类是`InputDStream`）。
- en: Let's implement a simple Scala example—a streaming word count self-contained
    application. The code used for this class can be found among the examples that
    are bundled with the Spark distribution. To compile and package it, you need to
    add the dependency to Spark Streaming to your `Maven`, `Gradle`, or `sbt` project
    descriptor, along with the dependencies from Spark Core and Scala.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个简单的Scala示例——一个流式单词计数自包含应用程序。此类所用的代码可以在与Spark发行版一起捆绑的示例中找到。要编译和打包它，你需要将Spark
    Streaming的依赖项添加到你的`Maven`、`Gradle`或`sbt`项目描述文件中，还需要添加来自Spark Core和Scala的依赖项。
- en: 'First, we have to create the `SparkConf` and a `StreamingContext` (which is
    the main entry point for any streaming functionality) from it:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从`SparkConf`和`StreamingContext`（它是任何流式功能的主要入口点）开始创建：
- en: '[PRE36]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The batch interval has been set to 1 second. A DStream representing streaming
    data from a TCP source can be created using the `ssc` streaming context; we need
    just to specify the source hostname and port, as well as the desired storage level:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理间隔已设置为1秒。表示来自TCP源的流数据的DStream可以通过`ssc`流上下文创建；我们只需要指定源的主机名和端口，以及所需的存储级别：
- en: '[PRE37]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The returned `lines` DStream is the stream of data that is going to be received
    from the server. Each record will be a single line of text that we want to split
    into single words, thus specifying the space character as a separator:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的`lines` DStream是将从服务器接收到的数据流。每条记录将是我们希望分割为单个单词的单行文本，从而指定空格字符作为分隔符：
- en: '[PRE38]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then, we will count those words:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将对这些单词进行计数：
- en: '[PRE39]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The `words` DStream is mapped (a one-to-one transformation) to a DStream of
    (*word*, *1*) pairs, which is then reduced to get the frequency of words in each
    batch of data. The last command will print a few of the counts that are generated
    every second. Each RDD in a DStream contains data from a certain interval – any
    operation applied on a DStream translates to operations on the underlying RDDs:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`words` DStream被映射（一个一对一的转换）为一个(*word*, *1*)对的DStream，随后通过减少操作得到每批数据中单词的频率。最后的命令会每秒打印生成的计数。DStream中的每个RDD包含来自某个时间间隔的数据——对DStream应用的任何操作都转换为对底层RDD的操作：'
- en: '![](img/284d70b8-6143-4b6d-bce0-41c7bea81814.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/284d70b8-6143-4b6d-bce0-41c7bea81814.png)'
- en: Figure 1.10
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.10
- en: 'To start the processing after all the transformations have been set up, use
    the following code:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好所有转换后，使用以下代码开始处理：
- en: '[PRE40]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Before running this example, first you will need to run `netcat` (a small utility
    found in most Unix-like systems) as a data server:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此示例之前，首先需要运行`netcat`（一个在大多数类Unix系统中找到的小工具）作为数据服务器：
- en: '[PRE41]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then, in a different Terminal, you can start the example by passing the following
    as arguments:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在另一个终端中，你可以通过传递以下参数来启动示例：
- en: '[PRE42]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Any line that's typed into the Terminal and run with the `netcat` server will
    be counted and printed on the application screen every second.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 任何在终端中输入并通过`netcat`服务器运行的行都会被计数，并且每秒在应用程序屏幕上打印一次。
- en: 'Regardless of whether `nc` shouldn''t be available in the system where you
    run this example, you can implement your own data server in Scala:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 无论`nc`是否在运行此示例的系统中不可用，你都可以在Scala中实现自己的数据服务器：
- en: '[PRE43]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The same self-contained application in Python could be as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的自包含应用程序在Python中可能如下所示：
- en: '[PRE44]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'DStreams support most parts of the transformations that are available for RDDs.
    This means that data from input DStreams can be modified in the same way as the
    data in RDDs. The following table lists some of the common transformations supported
    by Spark DStreams:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: DStreams支持RDD大部分可用的转换功能。这意味着输入DStream中的数据可以像RDD中的数据一样被修改。下表列出了Spark DStreams支持的一些常见转换：
- en: '| **Transformation** | **Purpose** |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| **转换** | **用途** |'
- en: '| `map(func)` | Returns a new DStream. The `func` map function is applied to
    each element of the source DStream. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| `map(func)` | 返回一个新的DStream。`func`映射函数应用于源DStream的每个元素。 |'
- en: '| `flatMap(func)` | The same as for `map`. The only difference is that each
    input item in the new DStream can be mapped to 0 or more output items. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `flatMap(func)` | 与`map`相同。唯一的区别是新DStream中的每个输入项可以映射到0个或多个输出项。 |'
- en: '| `filter(func)` | Returns a new DStream containing only the elements of the
    source DStream for which the `func` filter function returned true. |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| `filter(func)` | 返回一个新的DStream，只包含源DStream中`func`过滤函数返回true的元素。 |'
- en: '| `repartition(numPartitions)` | This is used to set the level of parallelism
    by creating a different number of partitions. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| `repartition(numPartitions)` | 用于通过创建不同数量的分区来设置并行度。 |'
- en: '| `union(otherStream)` | Returns a new DStream. It contains the union of the
    elements in the source DStream and the input `otherDStream` DStream. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| `union(otherStream)` | 返回一个新的 DStream。它包含源 DStream 和输入的 `otherDStream` DStream
    中元素的并集。 |'
- en: '| `count()` | Returns a new DStream. It contains single element RDDs that are
    obtained by counting the number of elements contained in each RDD arriving from
    the source. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `count()` | 返回一个新的 DStream。它包含通过计算每个 RDD 中元素的数量而得到的单一元素 RDD。 |'
- en: '| `reduce(func)` | Returns a new DStream. It contains single element RDDs that
    are obtained by aggregating those in each RDD of the source by applying the `func` function
    (which should be associative and commutative to allow for correct parallel computation).
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| `reduce(func)` | 返回一个新的 DStream。它包含通过应用 `func` 函数（该函数应该是结合性和交换性的，以便支持正确的并行计算）在源中每个
    RDD 上聚合得到的单一元素 RDD。 |'
- en: '| `countByValue()` | Returns a new DStream of (*K*, *Long*) pairs, where *K*
    is the type of the elements of the source. The value of each key represents its
    frequency in each RDD of the source. |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| `countByValue()` | 返回一个新的 DStream，包含 (*K*, *Long*) 对，其中 *K* 是源元素的类型。每个键的值表示其在源中每个
    RDD 中的频率。 |'
- en: '| `reduceByKey(func, [numTasks])` | Returns a new DStream of (*K*, *V*) pairs
    (for a source DStream of (*K*, *V*) pairs). The values for each key are aggregated
    by applying the reduce `func` function. To do the grouping, this transformation
    uses Spark''s default number of parallel tasks (which is two in local mode, while
    it is determined by the `config` property `spark.default.parallelism` in cluster
    mode), but this can be changed by passing an optional `numTasks` argument. |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKey(func, [numTasks])` | 返回一个新的 DStream，包含 (*K*, *V*) 对（对于源 DStream
    中的 (*K*, *V*) 对）。每个键的值通过应用 `func` 函数来进行聚合。为了进行分组，此转换使用 Spark 默认的并行任务数（在本地模式下是
    2，而在集群模式下由 `config` 属性 `spark.default.parallelism` 确定），但可以通过传递可选的 `numTasks` 参数来更改此数值。
    |'
- en: '| `join(otherStream, [numTasks])` | Returns a new DStream of (*K*, (*V*, *W*))
    pairs when called on two DStreams of (*K*, *V*) and (*K*, *W*) pairs, respectively.
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| `join(otherStream, [numTasks])` | 返回一个新的 DStream，包含 (*K*, (*V*, *W*)) 对，当它分别应用于两个
    DStream，其中一个包含 (*K*, *V*) 对，另一个包含 (*K*, *W*) 对时。 |'
- en: '| `cogroup(otherStream, [numTasks])` | Returns a new DStream of (*K*, *Seq[V]*,
    *Seq[W]*) tuples when called on two DStreams of (*K*, *V*) and (*K*, *W*) pairs,
    respectively. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| `cogroup(otherStream, [numTasks])` | 返回一个新的 DStream，包含 (*K*, *Seq[V]*, *Seq[W]*)
    元组，当它分别应用于两个 DStream，其中一个包含 (*K*, *V*) 对，另一个包含 (*K*, *W*) 对时。 |'
- en: '| `transform(func)` | Returns a new DStream. It applies an RDD-to-RDD `func` function
    to every RDD of the source. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `transform(func)` | 返回一个新的 DStream。它将 RDD 到 RDD 的 `func` 函数应用于源中的每个 RDD。
    |'
- en: '| `updateStateByKey(func)` | Returns a new state DStream. The state for each
    key in the new DStream is updated by applying the `func` input function to the
    previous state and the new values for the key. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| `updateStateByKey(func)` | 返回一个新的状态 DStream。新 DStream 中每个键的状态通过将输入函数 `func`
    应用于先前的状态和该键的新值来更新。 |'
- en: 'Windowed computations are provided by Spark Streaming. As shown in the following
    diagram, they allow you to apply transformations over sliding windows of data:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口计算由 Spark Streaming 提供。如以下图所示，它们允许你在滑动数据窗口上应用转换：
- en: '![](img/8ce148aa-91a4-4ed9-b146-ebcf9cc70cff.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ce148aa-91a4-4ed9-b146-ebcf9cc70cff.png)'
- en: Figure 1.11
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.11
- en: 'When a window slides over a source DStream, all its RDDs that fall within that
    window are taken into account and transformed to produce the RDDs of the returned
    windowed DStream. Looking at the specific example that''s shown in the preceding
    diagram, the window-based operation is applied over three time units of data and
    it slides by two. Two parameters need to be specified by any window operation
    that''s used:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个窗口在源 DStream 上滑动时，所有在该窗口内的 RDD 都会被考虑并转换，生成返回的窗口化 DStream 的 RDD。看一下上面图示的具体例子，基于窗口的操作应用于三个时间单位的数据，并且它以两为滑动步长。任何使用的窗口操作都需要指定两个参数：
- en: '**Window length**: The duration of the window'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**窗口长度**：窗口的持续时间'
- en: '**Sliding interval**: The interval at which the window operation is performed'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滑动间隔**：窗口操作执行的间隔时间'
- en: These two parameters must be multiples of the batch interval of the source DStream.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个参数必须是源 DStream 批次间隔的倍数。
- en: 'Let''s see how this could be applied to the application that was presented
    at the beginning of this section. Suppose you want to generate a word count every
    10 seconds over the last 60 seconds of data. The `reduceByKey` operation needs
    to be applied on the (*word*, *1*) pairs of the DStream over the last 60 seconds
    of data. This can be achieved with the `reduceByKeyAndWindow` operation. When
    translated into Scala code, this is as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个方法如何应用于本节开始时介绍的应用场景。假设你想要每隔 10 秒钟生成一次过去 60 秒数据的字数统计。需要在过去 60 秒的 DStream
    中对（*word*, *1*）对应用 `reduceByKey` 操作。这可以通过 `reduceByKeyAndWindow` 操作来实现。转换为 Scala
    代码如下：
- en: '[PRE45]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'For Python, it is as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Python，代码如下：
- en: '[PRE46]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The following table lists some of the common window operations supported by
    Spark for DStreams:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表列出了 Spark 为 DStreams 支持的一些常见窗口操作：
- en: '| **Transformation** | **Purpose** |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| **转换** | **目的** |'
- en: '| `window(windowLength, slideInterval)` | Returns a new DStream. It is based
    on windowed batches of the source. |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| `window(windowLength, slideInterval)` | 返回一个新的 DStream。它基于源数据的窗口化批次。 |'
- en: '| `countByWindow(windowLength, slideInterval)` | Returns a sliding window count
    (based on the `windowLength` and `slideInterval` parameters) of elements in the
    source DStream. |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| `countByWindow(windowLength, slideInterval)` | 返回源 DStream 中元素的滑动窗口计数（基于
    `windowLength` 和 `slideInterval` 参数）。 |'
- en: '| `reduceByWindow(func, windowLength, slideInterval)` | Returns a new single
    element DStream. It is created by aggregating elements in the source DStream over
    a sliding interval by applying the `func` reduce function (which, to allow for
    correct parallel computation, is associative and commutative). |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByWindow(func, windowLength, slideInterval)` | 返回一个新的单元素 DStream。它是通过在滑动时间间隔内聚合源
    DStream 中的元素，并应用 `func` 减少函数来创建的（为了支持正确的并行计算，`func` 必须是结合性和交换性的）。 |'
- en: '| `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])` | Returns
    a new DStream of (*K*, *V*) pairs (the same *K* and *V* as for the source DStream).
    The values for each key are aggregated using the `func` input function over batches
    (defined by the `windowLength` and `slideInterval` arguments) in a sliding window.
    The number of parallel tasks to do the grouping is two (default) in local mode,
    while in cluster mode this is given by the Spark configuration property `spark.default.parallelism.
    numTask`, which is an optional argument to specify a custom number of tasks. |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])` | 返回一个新的由（*K*,
    *V*）对组成的 DStream（与源 DStream 相同的 *K* 和 *V*）。每个键的值通过在滑动窗口中对批次（由 `windowLength` 和
    `slideInterval` 参数定义）应用 `func` 输入函数来聚合。并行任务的数量在本地模式下为 2（默认），而在集群模式下由 Spark 配置属性
    `spark.default.parallelism.numTask` 给出，这是一个可选参数，用于指定自定义任务数量。 |'
- en: '| `reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])`
    | This is a more efficient version of the `reduceByKeyAndWindow` transformation.
    This time, the reduce value of the current window is calculated incrementally
    using the reduce values of the previous one. This happens by reducing the new
    data that enters a window while inverse reducing the old data that leaves the
    same one. Please note that this mechanism only works if the `func` reduce function
    has a corresponding inverse reduce function, `invFunc`. |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])`
    | 这是 `reduceByKeyAndWindow` 转换的一个更高效版本。这次，当前窗口的减少值是通过使用前一个窗口的减少值逐步计算出来的。通过减少进入窗口的新数据，同时对离开窗口的旧数据进行逆减少，来实现这一点。请注意，这种机制只有在
    `func` 函数有相应的逆减少函数 `invFunc` 时才能工作。 |'
- en: '| `countByValueAndWindow(windowLength, slideInterval, [numTasks])` | Returns
    a DStream of (*K*, *Long*) pairs (whatever (*K*, *V*) pairs the source DStream
    is made of). The value of each key in the returned DStream is its frequency within
    a given sliding window (defined by the `windowLength` and `slideInterval` arguments).
    `numTask` is an optional argument to specify a custom number of tasks. |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| `countByValueAndWindow(windowLength, slideInterval, [numTasks])` | 返回一个由（*K*,
    *Long*）对组成的 DStream（与源 DStream 组成的（*K*, *V*）对相同）。返回的 DStream 中每个键的值是其在给定滑动窗口内的频率（由
    `windowLength` 和 `slideInterval` 参数定义）。`numTask` 是一个可选参数，用于指定自定义任务数量。 |'
- en: Cluster mode using different managers
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用不同管理器的集群模式
- en: The following diagram shows how Spark applications run on a cluster. They are
    independent sets of processes that are coordinated by the `SparkContext` object
    in the **Driver Program**. `SparkContext` connects to a **Cluster Manager**, which
    is responsible for allocating resources across applications. Once the **SparkContext**
    is connected, Spark gets executors across cluster nodes.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了Spark应用程序在集群上如何运行。它们是由`SparkContext`对象在**Driver Program**中协调的独立进程集。`SparkContext`连接到**Cluster
    Manager**，后者负责在各个应用程序之间分配资源。一旦**SparkContext**连接，Spark将在集群节点上获取执行器。
- en: 'Executors are processes that execute computations and store data for a given
    Spark application. **SparkContext** sends the application code (which could be
    a JAR file for Scala or .py files for Python) to the executors. Finally, it sends
    the tasks to run to the executors:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器是执行计算并存储给定Spark应用程序数据的进程。**SparkContext**将应用程序代码（Scala的JAR文件或Python的.py文件）发送到执行器。最后，它将运行任务发送到执行器：
- en: '![](img/fb1e7d7d-9bfd-4eb2-88e2-c35d41606cb9.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb1e7d7d-9bfd-4eb2-88e2-c35d41606cb9.png)'
- en: Figure 1.12
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.12
- en: To isolate applications from each other, every Spark application receives its
    own executor processes. They stay alive for the duration of the whole application
    and run tasks in multithreading mode. The downside to this is that it isn't possible
    to share data across different Spark applications – to share it, data needs to
    be persisted to an external storage system.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将不同的应用程序彼此隔离，每个Spark应用程序都会获得自己的执行进程。这些进程会在整个应用程序的运行期间保持活跃，并以多线程模式运行任务。缺点是无法在不同的Spark应用程序之间共享数据——为了共享数据，数据需要持久化到外部存储系统。
- en: Spark supports different cluster managers, but it is agnostic to the underlying
    type.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持不同的集群管理器，但它与底层类型无关。
- en: The driver program, at execution time, must be network addressable from the
    worker nodes because it has to listen for and accept incoming connections from
    its executors. Because it schedules tasks on the cluster, it should be executed
    close to the worker nodes, on the same local area network (if possible).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 执行时，驱动程序必须能从工作节点的网络地址访问，因为它必须监听并接受来自执行器的连接请求。由于它负责在集群上调度任务，因此应尽量将其执行在接近工作节点的地方，即在同一个局域网中（如果可能）。
- en: 'The following are the cluster managers that are currently supported in Spark:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是当前在Spark中支持的集群管理器：
- en: '**Standalone**: A simple cluster manager that makes it easy to set up a cluster.
    It is included with Spark.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Standalone**：一种简单的集群管理器，便于设置集群。它包含在Spark中。'
- en: '**Apache Mesos**: An open source project that''s used to manage computer clusters,
    and was developed at the University of California, Berkeley.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache Mesos**：一个开源项目，用于管理计算机集群，开发于加利福尼亚大学伯克利分校。'
- en: '**Hadoop YARN**: The resource manager available in Hadoop starting from release
    2.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop YARN**：从Hadoop 2版本开始提供的资源管理器。'
- en: '**Kubernetes**: An open source platform for providing a container-centric infrastructure.
    Kubernetes support in Spark is still experimental, so it''s probably not ready
    for production yet.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes**：一个开源平台，提供面向容器的基础设施。Spark中的Kubernetes支持仍处于实验阶段，因此可能尚未准备好用于生产环境。'
- en: Standalone mode
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立模式
- en: 'For standalone mode, you only need to place a compiled version of Spark on
    each node of the cluster. All the cluster nodes need to be able to resolve the
    hostnames of the other cluster members and are routable to one another. The Spark
    master URL can be configured in the `$SPARK_HOME/conf/spark-defaults.conf` file
    on all of the nodes:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对于独立模式，您只需要将编译版本的Spark放置在集群的每个节点上。所有集群节点需要能够解析其他集群成员的主机名，并且能够相互路由。Spark主节点的URL可以在所有节点的`$SPARK_HOME/conf/spark-defaults.conf`文件中进行配置：
- en: '[PRE47]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then, the hostname or IP address of the Spark master node needs to be specified
    in the `$SPARK_HOME/conf/spark-env.sh` file on all of the nodes, as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，需要在所有节点的`$SPARK_HOME/conf/spark-env.sh`文件中指定Spark主节点的主机名或IP地址，如下所示：
- en: '[PRE48]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'It is now possible to start a standalone master server by executing the following
    script:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，可以通过执行以下脚本启动一个独立的主服务器：
- en: '[PRE49]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Once the master has completed, a web UI will be available at the `http://<master_hostname_or_IP>:8080`
    URL. From there, it is possible to obtain the master URL that''s to be used when
    starting the workers. One or more workers can now be started by executing the
    following script:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦主节点完成，Web UI将可以通过`http://<master_hostname_or_IP>:8080` URL访问。从这里可以获得用于启动工作节点的主节点URL。现在可以通过执行以下脚本启动一个或多个工作节点：
- en: '[PRE50]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Each worker, after the start, comes with its own web UI, whose URL is `http://<worker_hostname_or_IP>:8081`.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 每个工作节点启动后都会有自己的 Web UI，其 URL 为 `http://<worker_hostname_or_IP>:8081`。
- en: The list of workers, along with other information about their number of CPUs
    and memory, can be found in the master's web UI.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点的列表，以及它们的 CPU 数量和内存等信息，可以在主节点的 Web UI 中找到。
- en: 'The way to do this is to run a standalone cluster manually. It is also possible
    to use the provided launch scripts. A `$SPARK_HOME/conf/slaves` file needs to
    be created as a preliminary step. It must contain the hostnames – one per line –
    of all of the machines where the Spark workers should start. Passwordless **SSH**
    (short for **Secure Shell**) for the Spark master to the Spark slaves needs to
    be enabled to allow remote login for the slave daemon startup and shutdown actions.
    A cluster can then be launched or stopped using the following shell scripts, which
    are available in the `$SPARK_HOME/sbin` directory:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的方法是手动运行独立集群。也可以使用提供的启动脚本。需要创建一个 `$SPARK_HOME/conf/slaves` 文件作为初步步骤。该文件必须包含所有要启动
    Spark 工作节点的机器的主机名，每行一个。在 Spark 主节点与 Spark 从节点之间需要启用无密码**SSH**（即**安全外壳**）以允许远程登录，从而启动和停止从节点守护进程。然后，可以使用以下
    shell 脚本启动或停止集群，这些脚本位于 `$SPARK_HOME/sbin` 目录中：
- en: '`start-master.sh`: Starts a master instance'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start-master.sh`：启动一个主节点实例'
- en: '`start-slaves.sh`: Starts a slave instance on each machine specified in the
    `conf/slaves` file'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start-slaves.sh`：在 `conf/slaves` 文件中指定的每台机器上启动一个从节点实例'
- en: '`start-slave.sh`: Starts a single slave instance'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start-slave.sh`：启动单个从节点实例'
- en: '`start-all.sh`: Starts both a master and a number of slaves'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start-all.sh`：同时启动主节点和多个从节点'
- en: '`stop-master.sh`: Stops a master that has been started via the `sbin/start-master.sh`
    script'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stop-master.sh`：停止通过 `sbin/start-master.sh` 脚本启动的主节点'
- en: '`stop-slaves.sh`: Stops all slave instances on the nodes specified in the `conf/slaves`
    file'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stop-slaves.sh`：停止 `conf/slaves` 文件中指定节点上的所有从节点实例'
- en: '`stop-all.sh`: Stops both a master and its slaves'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stop-all.sh`：停止主节点及其从节点'
- en: These scripts must be executed on the machine the Spark master will run on.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这些脚本必须在将运行 Spark 主节点的机器上执行。
- en: 'It is possible to run an interactive Spark shell against a cluster in the following
    way:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下方式运行一个交互式 Spark shell 对集群进行操作：
- en: '[PRE51]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The `$SPARK_HOME/bin/spark-submit` script can be used to submit a compiled
    Spark application to the cluster. Spark currently supports two deploy modes for
    standalone clusters: client and cluster. In client mode, the driver and the client
    that submits the application are launched in the same process, while in cluster
    mode, the driver is launched from one of the worker processes and the client process
    exits as soon as it completes submitting the application (it doesn''t have to
    wait for the application to finish).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `$SPARK_HOME/bin/spark-submit` 脚本将已编译的 Spark 应用程序提交到集群。Spark 当前支持独立集群的两种部署模式：客户端模式和集群模式。在客户端模式下，驱动程序和提交应用程序的客户端在同一进程中启动，而在集群模式下，驱动程序从其中一个工作进程启动，客户端进程在提交应用程序后立即退出（无需等待应用程序完成）。
- en: When an application is launched through `spark-submit`, then its JAR file is
    automatically distributed to all the worker nodes. Any additional JAR that an
    application depends on should be specified through the `jars` flag using a comma
    as a delimiter (for example, `jars`, `jar1`, `jar2`).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 当通过 `spark-submit` 启动应用程序时，其 JAR 文件会自动分发到所有工作节点。应用程序依赖的任何附加 JAR 文件应通过 `jars`
    标志指定，并使用逗号作为分隔符（例如，`jars`, `jar1`, `jar2`）。
- en: As mentioned in the *Apache Spark fundamentals* section, in standalone mode,
    the Spark master is a single point of failure. This means that if the Spark master
    node should go down, the Spark cluster would stop functioning and all currently
    submitted or running applications would fail, and it wouldn't be possible to submit
    new applications.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*Apache Spark 基础知识*一节中提到的，独立模式下，Spark 主节点是单点故障。这意味着如果 Spark 主节点宕机，Spark 集群将停止工作，所有当前提交或正在运行的应用程序将失败，并且无法提交新的应用程序。
- en: High availability can be configured using Apache ZooKeeper ([https://zookeeper.apache.org/](https://zookeeper.apache.org/)),
    an open source and highly reliable distributed coordination service, or can be
    deployed as a cluster through Mesos or YARN, which we will talk about in the following
    two sections.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 Apache ZooKeeper ([https://zookeeper.apache.org/](https://zookeeper.apache.org/))
    来配置高可用性，ZooKeeper 是一个开源且高度可靠的分布式协调服务，或者可以通过 Mesos 或 YARN 部署为集群，这部分将在接下来的两节中讨论。
- en: Mesos cluster mode
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Mesos 集群模式
- en: 'Spark can run on clusters that are managed by Apache Mesos ([http://mesos.apache.org/](http://mesos.apache.org/)).
    Mesos is a cross-platform, cloud provider-agnostic, centralized, and fault-tolerant
    cluster manager, designed for distributed computing environments. Among its main
    features, it provides resource management and isolation, and the scheduling of
    CPU and memory across the cluster. It can join multiple physical resources into
    a single virtual one, and in doing so is different from classic virtualization,
    where a single physical resource is split into multiple virtual resources. With
    Mesos, it is possible to build or schedule cluster frameworks such as Apache Spark
    (though it is not restricted to just this). The following diagram shows the Mesos
    architecture:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以在由Apache Mesos（[http://mesos.apache.org/](http://mesos.apache.org/)）管理的集群上运行。Mesos是一个跨平台、云提供商无关、集中式且容错的集群管理器，专为分布式计算环境设计。其主要特性包括资源管理和隔离，以及跨集群的CPU和内存调度。它可以将多个物理资源合并为单个虚拟资源，这与传统的虚拟化不同，传统虚拟化将单个物理资源分割为多个虚拟资源。使用Mesos，可以构建或调度诸如Apache
    Spark之类的集群框架（尽管不仅限于此）。下图展示了Mesos的架构：
- en: '![](img/799fa1d4-15de-4229-abc1-907ef787edb9.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![](img/799fa1d4-15de-4229-abc1-907ef787edb9.png)'
- en: Figure 1.13
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.13
- en: Mesos consists of a master daemon and frameworks. The master daemon manages
    agent daemons running on each cluster node, while the Mesos frameworks run tasks
    on the agents. The master empowers fine-grained sharing of resources (including
    CPU and RAM) across frameworks by making them resource offers. It decides how
    much of the available resources to offer to each framework, depending on given
    organizational policies. To support diverse sets of policies, the master uses
    a modular architecture that makes it easy to add new allocation modules through
    a plugin mechanism. A Mesos framework consists of two components – a scheduler,
    which registers itself with the master to be offered resources, and an executor,
    a process that is launched on agent nodes to execute the framework's tasks. While
    it is the master that determines how many resources are offered to each framework,
    the frameworks' schedulers are responsible for selecting which of the offered
    resources to use. The moment a framework accepts offered resources, it passes
    a description of the tasks it wants to execute on them to Mesos. Mesos, in turn,
    launches the tasks on the corresponding agents.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos由主守护程序和框架组成。主守护程序管理每个集群节点上运行的代理守护程序，而Mesos框架在代理上运行任务。主守护程序通过提供资源来实现对框架的细粒度资源共享（包括CPU和RAM）。它根据给定的组织策略决定向每个框架提供多少可用资源。为了支持各种策略集，主使用模块化架构通过插件机制轻松添加新的分配模块。一个Mesos框架由两个组件组成
    — 调度程序注册自己以接收主提供的资源，执行程序在代理节点上启动以执行框架的任务。尽管主决定向每个框架提供多少资源，但框架的调度程序负责选择要使用的提供的资源。框架一旦接受提供的资源，就会向Mesos传递它想要在这些资源上执行的任务的描述。Mesos随后在相应的代理上启动这些任务。
- en: 'The advantages of deploying a Spark cluster using Mesos to replace the Spark
    Master Manager include the following:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Mesos部署Spark集群以取代Spark Master Manager的优势包括以下几点：
- en: Dynamic partitioning between Spark and other frameworks
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark和其他框架之间进行动态分区
- en: Scalable partitioning between multiple instances of Spark
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个Spark实例之间进行可扩展分区
- en: 'Spark 2.2.1 is designed to be used with Mesos 1.0.0+. In this section, I won''t
    describe the steps to deploy a Mesos cluster – I am assuming that a Mesos cluster
    is already available and running. No particular procedure or patch is required
    in terms of Mesos installation to run Spark on it. To verify that the Mesos cluster
    is ready for Spark, navigate to the Mesos master web UI at port `5050`:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 2.2.1设计用于与Mesos 1.0.0+配合使用。在本节中，我不会描述部署Mesos集群的步骤 — 我假设Mesos集群已经可用并正在运行。在Mesos主节点的Web
    UI上，端口为`5050`，验证Mesos集群准备好运行Spark：
- en: '![](img/b907d35e-657d-4d72-a920-01dbf429c2ca.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b907d35e-657d-4d72-a920-01dbf429c2ca.png)'
- en: Figure 1.14
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.14
- en: Check that all of the expected machines are present in the Agents tab.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 检查Agents标签中是否存在所有预期的机器。
- en: To use Mesos from Spark, a Spark binary package needs to be available in a place
    that's accessible by Mesos itself, and a Spark driver program needs to be configured
    to connect to Mesos. Alternatively, it is possible to install Spark in the same
    location across all the Mesos slaves and then configure the `spark.mesos.executor.home`
    property (the default value is `$SPARK_HOME`) to point to that location.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 Spark 使用 Mesos，需要在 Mesos 本身可以访问的位置提供 Spark 二进制包，并且需要配置 Spark 驱动程序程序以连接到 Mesos。或者，也可以在所有
    Mesos 从节点上安装 Spark，然后配置 `spark.mesos.executor.home` 属性（默认值为 `$SPARK_HOME`）以指向该位置。
- en: The Mesos master URLs have the form `mesos://host:5050` for a single-master
    Mesos cluster, or `mesos://zk://host1:2181,host2:2181,host3:2181/mesos` for a
    multi-master Mesos cluster when using Zookeeper.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos 主节点的 URL 形式为 `mesos://host:5050`，对于单主节点的 Mesos 集群，或者对于使用 Zookeeper 的多主节点
    Mesos 集群，形式为 `mesos://zk://host1:2181,host2:2181,host3:2181/mesos`。
- en: 'The following is an example of how to start a Spark shell on a Mesos cluster:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何在 Mesos 集群上启动 Spark shell 的示例：
- en: '[PRE52]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'A Spark application can be submitted to a Mesos managed Spark cluster as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Spark 应用程序可以按以下方式提交到 Mesos 管理的 Spark 集群：
- en: '[PRE53]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: YARN cluster mode
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: YARN 集群模式
- en: YARN ([http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html](http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html)),
    which was introduced in Apache Hadoop 2.0, brought significant improvements in
    terms of scalability, high availability, and support for different paradigms.
    In the Hadoop version 1 **MapReduce** framework, job execution was controlled
    by types of processes—a single master process called `JobTracker` coordinates
    all the jobs running on the cluster and assigns `map` and `reduce` tasks to run
    on the `TaskTrackers`, which are a number of subordinate processes running assigned
    tasks and periodically reporting the progress to the `JobTracker`. Having a single
    `JobTracker` was a scalability bottleneck. The maximum cluster size was a little
    more than 4,000 nodes, with the number of concurrent tasks limited to 40,000\.
    Furthermore, the `JobTracker` was a single point of failure and the only available
    programming model was **MapReduce**.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: YARN ([http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html](http://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html))，它是在
    Apache Hadoop 2.0 中引入的，带来了在可扩展性、高可用性和对不同范式的支持方面的显著改进。在 Hadoop 版本 1 的 **MapReduce**
    框架中，作业执行由几种类型的进程控制——一个名为 `JobTracker` 的单一主进程协调集群中运行的所有作业，并将 `map` 和 `reduce` 任务分配给
    `TaskTrackers`，这些是运行分配任务的从属进程，并定期将进度报告给 `JobTracker`。拥有一个单一的 `JobTracker` 成为可扩展性的瓶颈。最大集群规模略超过
    4000 个节点，并且并发任务数限制为 40,000。此外，`JobTracker` 是单点故障，并且唯一可用的编程模型是 **MapReduce**。
- en: 'The fundamental idea of YARN is to split up the functionalities of resource
    management and job scheduling or monitoring into separate daemons. The idea is
    to have a global **ResourceManager** and per-application **ApplicationMaster**
    (**App Mstr**). An application is either a single job or a DAG of jobs. The following
    is a diagram of YARN''s architecture:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: YARN 的基本思想是将资源管理和作业调度或监控的功能拆分为独立的守护进程。其思路是拥有一个全局的**ResourceManager**和每个应用程序的**ApplicationMaster**（**App
    Mstr**）。一个应用程序可以是一个单独的作业，也可以是作业的有向无环图（DAG）。以下是 YARN 架构的示意图：
- en: '![](img/43bba525-0732-49a6-a9fc-52110b582088.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43bba525-0732-49a6-a9fc-52110b582088.png)'
- en: Figure 1.15
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.15
- en: The **ResourceManager** and the **NodeManager** form the YARN framework. The
    **ResourceManager** decides on resource usage across all the running applications,
    while the **NodeManager** is an agent running on any machine in the cluster and
    is responsible for the containers by monitoring their resource usage (including
    CPU and memory) and reporting to the **ResourceManager**. The **ResourceManager**
    consists of two components – the scheduler and the ApplicationsManager. The scheduler
    is the component that's responsible for allocating resources to the various applications
    running, and it doesn't perform any monitoring of applications' statuses, nor
    offer guarantees about restarting any failed tasks. It performs scheduling based
    on an application's resource requirements.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '**ResourceManager** 和 **NodeManager** 组成了 YARN 框架。**ResourceManager** 决定所有运行应用程序的资源使用，而
    **NodeManager** 是运行在集群中任何机器上的代理，负责通过监控容器的资源使用（包括 CPU 和内存）并向 **ResourceManager**
    报告。**ResourceManager** 由两个组件组成——调度器和 ApplicationsManager。调度器是负责分配资源给各种正在运行的应用程序的组件，但它不对应用程序状态进行监控，也不提供重启失败任务的保证。它是根据应用程序的资源需求来进行调度的。'
- en: The ApplicationsManager accepts job submissions and provides a service to restart
    the **App Mstr** container on any failure. The per-application **App Mstr** is
    responsible for negotiating the appropriate resource containers from the scheduler
    and monitoring their status and progress. YARN, by its nature, is a general scheduler,
    so support for non-MapReduce jobs (such as Spark jobs) is available for Hadoop
    clusters.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ApplicationsManager 接受作业提交，并提供在任何故障时重新启动 **App Mstr** 容器的服务。每个应用程序的 **App Mstr**
    负责与调度程序协商适当的资源容器，并监视其状态和进度。YARN 作为通用调度程序，支持用于 Hadoop 集群的非 MapReduce 作业（如 Spark
    作业）。
- en: Submitting Spark applications on YARN
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 YARN 上提交 Spark 应用程序
- en: 'To launch Spark applications on YARN, the `HADOOP_CONF_DIR or YARN_CONF_DIR
    env` variable needs to be set and pointing to the directory that contains the
    client-side configuration files for the Hadoop cluster. These configurations are
    needed to connect to the YARN ResourceManager and to write to HDFS. This configuration
    is distributed to the YARN cluster so that all the containers used by the Spark
    application have the same configuration. To launch Spark applications on YARN,
    two deployment modes are available:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 YARN 上启动 Spark 应用程序，需要设置 `HADOOP_CONF_DIR` 或 `YARN_CONF_DIR` 环境变量，并指向包含 Hadoop
    集群客户端配置文件的目录。这些配置用于连接到 YARN ResourceManager 和写入 HDFS。此配置分发到 YARN 集群，以便 Spark 应用程序使用的所有容器具有相同的配置。在
    YARN 上启动 Spark 应用程序时，有两种部署模式可用：
- en: '** Cluster mode**: In this case, the Spark driver runs inside an application
    master process that''s managed by YARN on the cluster. The client can finish its
    execution after initiating the application.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cluster mode**：在此情况下，Spark Driver 在由 YARN 在集群上管理的应用程序主进程内运行。客户端在启动应用程序后可以完成其执行。'
- en: '**Client mode**: In this case, the driver runs and the client runs in the same
    process. The application master is used for the sole purpose of requesting resources
    from YARN.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Client mode**：在此情况下，Driver 和客户端在同一个进程中运行。应用程序主进程仅用于从 YARN 请求资源的目的。'
- en: Unlike the other modes, in which the master's address is specified in the `master`
    parameter, in YARN mode, the ResourceManager's address is retrieved from the Hadoop
    configuration. Therefore, the `master` parameter value is always `yarn`.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他模式不同，在 YARN 模式中，Master 的地址是从 Hadoop 配置中检索的 ResourceManager 的地址。因此，`master`
    参数的值始终为 `yarn`。
- en: 'You can use the following command to launch a Spark application in cluster
    mode:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令在集群模式下启动 Spark 应用程序：
- en: '[PRE54]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: In cluster mode, since the driver runs on a different machine than the client,
    the `SparkContext.addJar` method doesn't work with the files that are local to
    the client. The only choice is to include them using the `jars` option in the
    `launch` command.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群模式下，由于 Driver 运行在与客户端不同的机器上，`SparkContext.addJar` 方法无法使用客户端本地的文件。唯一的选择是使用
    `launch` 命令中的 `jars` 选项包含它们。
- en: Launching a Spark application in client mode happens the same way—the `deploy-mode`
    option value needs to change from cluster to client.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端模式下启动 Spark 应用程序的方法相同——`deploy-mode` 选项值需要从 cluster 更改为 client。
- en: Kubernetes cluster mode
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 集群模式
- en: '**Kubernetes** ([https://kubernetes.io/](https://kubernetes.io/)) is an open
    source system that''s used automate the deployment, scaling, and management of
    containerized applications. It was originally implemented at Google and then open
    sourced in 2014\. The following are the main concepts of Kubernetes:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubernetes** ([https://kubernetes.io/](https://kubernetes.io/)) 是一个开源系统，用于自动化部署、扩展和管理容器化应用程序。它最初由
    Google 实施，于 2014 年开源。以下是 Kubernetes 的主要概念：'
- en: '**Pod**: This is the smallest deployable unit of computing that can be created
    and managed. A pod can be seen as a group of one or more containers that share
    network and storage space, which also contains a specification for how to run
    those containers.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod**：这是可以创建和管理的最小计算可部署单元。Pod 可以看作是一个或多个共享网络和存储空间的容器组，还包含如何运行这些容器的规范。'
- en: '**Deployment**: This is a layer of abstraction whose primary purpose is to
    declare how many replicas of a pod should be running at a time.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Deployment**：这是一个抽象层，其主要目的是声明应该同时运行多少个 Pod 的副本。'
- en: '**Ingress**: This is an open channel for communication with a service running
    in a pod.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ingress**：这是与在 Pod 中运行的服务通信的开放通道。'
- en: '**Node**: This is a representation of a single machine in a cluster.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Node**：这是集群中单个机器的表示。'
- en: '**Persistent volume**: This provides a filesystem that can be mounted to a
    cluster, not to be associated with any particular node. This is the way Kubernetes
    persists information (data, files, and so on).'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持久卷**：它提供一个文件系统，可以挂载到集群，而不与任何特定节点关联。这是 Kubernetes 持久化信息（数据、文件等）的方法。'
- en: 'The following diagram (source: [https://d33wubrfki0l68.cloudfront.net/518e18713c865fe67a5f23fc64260806d72b38f5/61d75/images/docs/post-ccm-arch.png](https://d33wubrfki0l68.cloudfront.net/518e18713c865fe67a5f23fc64260806d72b38f5/61d75/images/docs/post-ccm-arch.png))
    shows the Kubernetes architecture:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图（来源：[https://d33wubrfki0l68.cloudfront.net/518e18713c865fe67a5f23fc64260806d72b38f5/61d75/images/docs/post-ccm-arch.png](https://d33wubrfki0l68.cloudfront.net/518e18713c865fe67a5f23fc64260806d72b38f5/61d75/images/docs/post-ccm-arch.png)）展示了
    Kubernetes 架构：
- en: '![](img/8f5c0874-a4ff-436e-ab31-90d0af1728bd.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8f5c0874-a4ff-436e-ab31-90d0af1728bd.png)'
- en: Figure 1.16
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.16
- en: 'The main components of the Kubernetes architecture are as follows:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 架构的主要组件如下：
- en: '**Cloud controller manager**: It runs the Kubernetes controllers'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云控制器管理器**：它运行 Kubernetes 控制器'
- en: '**Controllers**: There are four of them—node, route, service, and PersistenceVolumeLabels'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器**：共有四个——节点、路由、服务和 PersistentVolumeLabels'
- en: '**Kubelets**: The primary agents that run on nodes'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubelets**：运行在节点上的主要代理'
- en: 'The submission of Spark jobs to a Kubernetes cluster can be done directly through
    `spark-submit`. Kubernetes requires that we supply Docker ([https://www.docker.com/](https://www.docker.com/))
    images that can be deployed into containers within pods. Starting from the 2.3
    release, Spark provides a Dockerfile (`$SPARK_HOME/kubernetes/dockerfiles/Dockerfile`,
    which can also be customized to match specific applications'' needs) and a script
    (`$SPARK_HOME/bin/docker-image-tool.sh`) that can be used to build and publish
    Docker images that are to be used within a Kubernetes backend. The following is
    the syntax that''s used to build a Docker image through the provided script:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 提交 Spark 作业到 Kubernetes 集群可以通过 `spark-submit` 直接完成。Kubernetes 要求我们提供可以部署到 pod
    中容器的 Docker ([https://www.docker.com/](https://www.docker.com/)) 镜像。从 2.3 版本开始，Spark
    提供了一个 Dockerfile (`$SPARK_HOME/kubernetes/dockerfiles/Dockerfile`，也可以根据特定应用需求进行定制)
    和一个脚本 (`$SPARK_HOME/bin/docker-image-tool.sh`)，用于构建和发布将用于 Kubernetes 后端的 Docker
    镜像。以下是通过提供的脚本构建 Docker 镜像的语法：
- en: '[PRE55]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This following is the syntax to push an image to a Docker repository while
    using the same script:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用相同脚本将镜像推送到 Docker 仓库的语法：
- en: '[PRE56]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'A job can be submitted in the following way:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 作业可以通过以下方式提交：
- en: '[PRE57]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Kubernetes requires application names to contain only lowercase alphanumeric
    characters, hyphens, and dots, and to start and end with an alphanumeric character.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 要求应用程序名称仅包含小写字母数字字符、连字符和点，并且必须以字母数字字符开头和结尾。
- en: 'The following diagram shows the way the submission mechanism works:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了提交机制的工作方式：
- en: '![](img/5b892e4b-7efe-4945-865d-479e6a496840.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5b892e4b-7efe-4945-865d-479e6a496840.png)'
- en: Figure 1.17
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.17
- en: 'Here''s what happens:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是发生的事情：
- en: Spark creates a driver that's running within a Kubernetes pod
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 创建了一个在 Kubernetes pod 中运行的驱动程序
- en: The driver creates the executors, which also run within Kubernetes pods, and
    then connects to them and executes application code
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动程序创建执行器，执行器也运行在 Kubernetes pod 中，然后连接到它们并执行应用程序代码
- en: At the end of the execution, the executor pods terminate and are cleaned up,
    while the driver pod still persists logs and remains in a completed state (which
    means that it doesn't use cluster computation or memory resources) in the Kubernetes
    API (until it's eventually garbage collected or manually deleted)
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行结束时，执行器 pod 会终止并被清理，而驱动程序 pod 会继续保留日志，并保持完成状态（意味着它不再使用集群的计算或内存资源），在 Kubernetes
    API 中（直到最终被垃圾回收或手动删除）
- en: Summary
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we became familiar with Apache Spark and most of its main modules.
    We started to use the available Spark shells and wrote our first self-contained
    application using the Scala and Python programming languages. Finally, we explored
    different ways of deploying and running Spark in cluster mode. Everything we have
    learned about so far is necessary for understanding the topics that are presented
    from [Chapter 3](44fab060-12c9-4eec-9e15-103da589a510.xhtml), *Extract, Transform,
    Load*, onward. If you have any doubts about any of the presented topics, I suggest
    that you go back and read this chapter again before moving on.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们熟悉了Apache Spark及其主要模块。我们开始使用可用的Spark shell，并使用Scala和Python编程语言编写了第一个自包含的应用程序。最后，我们探索了在集群模式下部署和运行Spark的不同方法。到目前为止，我们学到的所有内容都是理解从[第3章](44fab060-12c9-4eec-9e15-103da589a510.xhtml)
    *提取、转换、加载* 及之后主题所必需的。如果你对所呈现的任何主题有疑问，我建议你在继续之前回过头再阅读一遍本章。
- en: In the next chapter, we are going to explore the basics of DL, with an emphasis
    on some particular implementations of multi-layer neural networks.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探索深度学习（DL）的基础知识，重点介绍多层神经网络的某些具体实现。
