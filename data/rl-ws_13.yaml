- en: Appendix
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录
- en: 1\. Introduction to Reinforcement Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1\. 强化学习简介
- en: 'Activity 1.01: Measuring the Performance of a Random Agent'
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动1.01：测量随机智能体的表现
- en: 'Import the required libraries – `abc`, `numpy`, and `gym`:'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库—`abc`、`numpy`和`gym`：
- en: '[PRE0]'
  id: totrans-4
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define the abstract class representing the agent:'
  id: totrans-5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义表示智能体的抽象类：
- en: '[PRE1]'
  id: totrans-6
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: An agent is represented by only a constructor and an abstract method, `pi`.
    This method is the actual policy; it takes as input the environment state and
    returns the selected action.
  id: totrans-7
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 智能体仅通过构造函数和一个抽象方法`pi`表示。该方法是实际的策略；它以环境状态为输入，返回选定的动作。
- en: 'Define a continuous agent. A continuous agent has to initialize the probability
    distribution according to the action space passed as an input to the constructor:'
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义连续智能体。连续智能体必须根据传递给构造函数的动作空间初始化概率分布：
- en: '[PRE2]'
  id: totrans-9
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If the upper and lower bounds are infinite, the probability distribution is
    simply a normal distribution centered at 0, with a scale that is equal to 1:'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果上下界是无限的，则概率分布只是一个以0为中心的正态分布，尺度等于1：
- en: '[PRE3]'
  id: totrans-11
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If the upper and lower bounds are both finite, the distribution is a uniform
    distribution defined in that range:'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果上下界都是有限的，则分布是定义在该范围内的均匀分布：
- en: '[PRE4]'
  id: totrans-13
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If the lower bound is ![1](img/B16182_01_a.png), the probability distribution
    is a shifted negative exponential distribution:'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果下界是![1](img/B16182_01_a.png)，则概率分布是一个平移的负指数分布：
- en: '[PRE5]'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If the upper bound is ![2](img/B16182_01_b.png), the probability distribution
    is a shifted exponential distribution:'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果上界是![2](img/B16182_01_b.png)，则概率分布是一个平移的指数分布：
- en: '[PRE6]'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the `pi` method, which is simply a call to the distribution defined
    in the constructor:'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`pi`方法，这只是对构造函数中定义的分布的调用：
- en: '[PRE7]'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We are ready to define the discrete agent. As before, the agent has to correctly
    initialize the action distribution according to the action space that is passed
    as a parameter:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们准备定义离散智能体。与之前一样，智能体必须根据传递的动作空间正确初始化动作分布：
- en: '[PRE8]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now it is useful to define a utility function to create the correct agent type
    based on the action space:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，定义一个实用函数来根据动作空间创建正确的智能体类型是有用的：
- en: '[PRE9]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The last step is to define the RL loop in which the agent interacts with the
    environment and collects rewards.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一步是定义强化学习循环，其中智能体与环境互动并收集奖励。
- en: 'Define the parameters, and then create the environment and the agent:'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定义参数，然后创建环境和智能体：
- en: '[PRE10]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We have to track the returns for each episode; to do this, we can use a simple
    list:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要跟踪每个回合的回报；为此，我们可以使用一个简单的列表：
- en: '[PRE11]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Start a loop for each episode:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个回合开始一个循环：
- en: '[PRE12]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Initialize the variables for the calculation of the cumulated discount factor
    and the current episode return:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化计算累计折扣因子和当前回合回报的变量：
- en: '[PRE13]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Reset the environment and get the first observation:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境并获取第一个观察：
- en: '[PRE14]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Loop for the number of timesteps:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环执行若干时间步：
- en: '[PRE15]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Render the environment, select the action, and then apply it:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 渲染环境，选择动作，然后应用它：
- en: '[PRE16]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Increment the return, and calculate the cumulated discount factor:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加回报，并计算累计折扣因子：
- en: '[PRE17]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If the episode is terminated, break from the timestep''s loop:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果回合已结束，则跳出时间步循环：
- en: '[PRE18]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After the timestep loop, we have to record the current return by appending
    it to the list of returns for each episode:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在时间步循环结束后，我们必须通过将当前回报添加到每个回合的回报列表中来记录当前回报：
- en: '[PRE19]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After the episode loop, close the environment and calculate `statistics`:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在回合循环结束后，关闭环境并计算`statistics`：
- en: '[PRE20]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You will get the following results:'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下结果：
- en: '[PRE21]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In this activity, we implemented two different types of agents: a discrete
    agent, working with discrete environments, and a continuous agent, working with
    continuous environments.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们实现了两种不同类型的智能体：离散智能体，适用于离散环境，和连续智能体，适用于连续环境。
- en: 'Additionally, you can render the episodes inside a notebook using the following
    code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以使用以下代码在笔记本中渲染回合：
- en: '[PRE22]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You can see the episode duration is not too long. This is because the actions
    are taken at random, so the pole falls after some timesteps.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到回合持续时间不长。这是因为动作是随机执行的，因此在若干时间步后，杆子会倒下。
- en: Note
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fbxR3Y](https://packt.live/3fbxR3Y).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/3fbxR3Y](https://packt.live/3fbxR3Y)。
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本节当前没有在线交互式示例，需要在本地运行。
- en: Discrete and continuous agents are two different possibilities when facing a
    new RL problem.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 离散代理和连续代理是在面对新强化学习问题时的两种不同可能性。
- en: We have designed our agents in a very flexible way so that they can be applied
    to almost all environments without having to change the code.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计的代理非常灵活，可以应用于几乎所有环境，而无需更改代码。
- en: We also implemented a simple RL loop and measured the performance of our agent
    on a classical RL problem.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实现了一个简单的强化学习循环，并衡量了代理在经典强化学习问题中的表现。
- en: 2\. Markov Decision Processes and Bellman Equations
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 马尔可夫决策过程与贝尔曼方程
- en: 'Activity 2.01: Solving Gridworld'
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 2.01：求解 Gridworld
- en: 'Import the required libraries:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE23]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Define the `visualization` function:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `visualization` 函数：
- en: '[PRE24]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Define the possible actions:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义可能的动作：
- en: '[PRE25]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Define the `Policy` class, representing the random policy:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `Policy` 类，表示随机策略：
- en: '[PRE26]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define the `Environment` class and the `step` function:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `Environment` 类和 `step` 函数：
- en: '[PRE27]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Loop for all states and actions and build the transition and reward matrices:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 针对所有状态和动作进行循环，构建转移矩阵和奖励矩阵：
- en: '[PRE28]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Check the correctness of the matrix:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查矩阵的正确性：
- en: '[PRE29]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Calculate the expected reward for each state:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个状态的预期奖励：
- en: '[PRE30]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Use the function to visualize the expected reward:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用该函数可视化预期奖励：
- en: '[PRE31]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The function visualizes the matrix using Matplotlib. You should see something
    similar to this:'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数使用 Matplotlib 可视化矩阵。你应该看到类似于以下内容：
- en: '![Figure 2.62: The expected reward for each state'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.62：每个状态的预期奖励](img/B16182_02_62.jpg)'
- en: '](img/B16182_02_62.jpg)'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_02_62.jpg)'
- en: 'Figure 2.62: The expected reward for each state'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 2.62：每个状态的预期奖励
- en: The previous figure is a color representation of the expected reward associated
    with each state considering the current policy. Notice that the expected reward
    of bad states is exactly equal to `-1`. The expected reward of good states is
    exactly equal to `10` and `5`, respectively.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的图像是一个颜色表示，展示了在当前策略下与每个状态相关联的预期奖励。注意，坏状态的预期奖励恰好等于 `-1`。好状态的预期奖励分别恰好等于 `10`
    和 `5`。
- en: 'Now set up the matrix form of the Bellman expectation equation:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在设置贝尔曼期望方程的矩阵形式：
- en: '[PRE32]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Solve the Bellman equation:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 求解贝尔曼方程：
- en: '[PRE33]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Visualize the result:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化结果：
- en: '[PRE34]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![Figure 2.63: State values of Gridworld'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.63：Gridworld 的状态值](img/B16182_02_63.jpg)'
- en: '](img/B16182_02_63.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_02_63.jpg)'
- en: 'Figure 2.63: State values of Gridworld'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.63：Gridworld 的状态值
- en: Note that the value of good states is less than the expected reward from those
    states. This is because landing states have an expected reward that is negative
    or because landing states are close to states for which the reward is negative.
    You can see that the state with the higher value is state ![a](img/B16182_02_63a.png),
    followed by state ![b](img/B16182_02_63b.png). It is also interesting to note
    the high value of the state in position (`0, 2`), which is close to the good states.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，好状态的值小于这些状态的预期奖励。这是因为着陆状态的预期奖励为负，或者着陆状态接近奖励为负的状态。你可以看到，值较高的状态是状态 ![a](img/B16182_02_63a.png)，接下来是状态
    ![b](img/B16182_02_63b.png)。另一个有趣的现象是位置 (`0, 2`) 上的状态值较高，且靠近好状态。
- en: Note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Al9xOB](https://packt.live/2Al9xOB).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此部分的源代码，请参考 [https://packt.live/2Al9xOB](https://packt.live/2Al9xOB)。
- en: You can also run this example online at [https://packt.live/2UChxBy](https://packt.live/2UChxBy).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，网址为 [https://packt.live/2UChxBy](https://packt.live/2UChxBy)。
- en: In this activity, we experimented with the Gridworld environment, one of the
    most common toy RL environments. We defined a random policy, and we solved the
    Bellman expectation equation using `scipy.linalg.solve` to find the state values
    of the policy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们尝试了 Gridworld 环境，这是最常见的强化学习玩具环境之一。我们定义了一个随机策略，并使用 `scipy.linalg.solve`
    求解贝尔曼期望方程，以找到该策略的状态值。
- en: It is important to visualize the results, when possible, to get a better understanding
    and to spot any errors.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是在可能的情况下可视化结果，以便更好地理解并发现任何错误。
- en: 3\. Deep Learning in Practice with TensorFlow 2
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3\. 使用 TensorFlow 2 进行深度学习实践
- en: 'Activity 3.01: Classifying Fashion Clothes Using a TensorFlow Dataset and TensorFlow
    2'
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 3.01：使用 TensorFlow 数据集和 TensorFlow 2 分类时尚服装
- en: 'Import all the required modules:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有所需的模块：
- en: '[PRE35]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Import the Fashion MNIST dataset using TensorFlow datasets and split it into
    train and test splits. Then, create a list of classes:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 数据集导入 Fashion MNIST 数据集，并将其拆分为训练集和测试集。然后，创建一个类别列表：
- en: '[PRE36]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Explore the dataset to get familiar with the input features, that is, shapes,
    labels, and classes:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索数据集，熟悉输入特征，即形状、标签和类别：
- en: '[PRE37]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output will be as follows:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE38]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Visualize some instances of the training set.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化一些训练集实例。
- en: 'It is also useful to take a look at how the images will appear. The following
    code snippet shows the first training set instance:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 观察图像的显示效果也很有用。以下代码片段展示了第一个训练集实例：
- en: '[PRE39]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output image will be as follows:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出图像将如下所示：
- en: '![Figure 3.30: First training image plot'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.30：第一个训练图像图'
- en: '](img/B16182_03_30.jpg)'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_30.jpg)'
- en: 'Figure 3.30: First training image plot'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.30：第一个训练图像图
- en: 'Perform feature normalization:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行特征归一化：
- en: '[PRE40]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, let''s take a look at some instances of our training set by plotting `25`
    of them with their corresponding labels:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们通过绘制`25`个样本及其对应的标签来查看一些训练集实例：
- en: '[PRE41]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output image will be as follows:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出图像将如下所示：
- en: '![Figure 3.31: A set of 25 training samples and their corresponding labels'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.31：一组 25 个训练样本及其对应的标签'
- en: '](img/B16182_03_31.jpg)'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_31.jpg)'
- en: 'Figure 3.31: A set of 25 training samples and their corresponding labels'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.31：一组 25 个训练样本及其对应的标签
- en: 'Build the classification model. First, create a model using a layers'' sequence:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建分类模型。首先，使用一系列层创建一个模型：
- en: '[PRE42]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Then, associate the model with an `optimizer`, a `loss` function, and a `metrics`:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将模型与`优化器`、`损失`函数和`指标`关联起来：
- en: '[PRE43]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Train the deep neural network:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练深度神经网络：
- en: '[PRE44]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The last output lines will be as follows:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后的输出行将如下所示：
- en: '[PRE45]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Test the model's accuracy. The accuracy should be in excess of 88%.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试模型的准确性。准确率应超过 88%。
- en: 'Evaluate the model on the test set and print the accuracy score:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估模型，并打印准确率得分：
- en: '[PRE46]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output will be as follows:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE47]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Note
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The accuracy may show slightly different values due to random sampling with
    a variable random seed.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于随机抽样和可变的随机种子，准确率可能会有所不同。
- en: Perform inference and check the predictions against the ground truth.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行推理并检查预测结果与真实值的对比。
- en: 'As a first step, add a `softmax` layer to the model so that it outputs probabilities
    instead of logits. Then, print out the probabilities of the first test instance
    with the following code:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，向模型中添加一个`softmax`层，使其输出概率而不是对数几率。然后，使用以下代码打印出第一个测试实例的概率：
- en: '[PRE48]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output will be as follows:'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE49]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, compare one model prediction (that is, the class with the highest predicted
    probability), the one on the first test instance, with its ground truth:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将一个模型的预测（即最高预测概率的类别）与第一个测试实例的真实标签进行对比：
- en: '[PRE50]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output will be as follows:'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE51]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'In order to perform a comparison that''s even clearer, create the following
    two functions. The first one plots the `i`-th test set instance image with a caption
    showing the predicted class with the highest probability, its probability in percent,
    and the ground truth between round brackets. This caption will be `blue` for correct
    predictions, and `red` for incorrect ones:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了进行更清晰的对比，创建以下两个函数。第一个函数绘制第`i`个测试集实例图像，并显示最高预测概率的类别、其百分比概率，以及括号中的真实标签。对于正确预测，标题将显示为`蓝色`，对于错误预测，则显示为`红色`：
- en: '[PRE52]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The second function creates a second image showing a bar plot of all classes''
    predicted probabilities. It will color the highest probable one in `blue` if the
    prediction is correct, or in `red` if it is incorrect. In this second case, the
    bar corresponding to the correct label is colored in `blue`:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个函数创建第二个图像，显示所有类别的预测概率的条形图。如果预测正确，它将以`蓝色`标记概率最高的类别，若预测错误，则用`红色`标记。如果是后者，与正确标签对应的条形图将以`蓝色`标记：
- en: '[PRE53]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Using these two functions, we can examine every instance of the test set. In
    the following snippet, the first test instance is being plotted:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这两个函数，我们可以检查测试集中的每个实例。在下面的代码片段中，正在绘制第一个测试实例：
- en: '[PRE54]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output will be as follows:'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 3.32: First test instance, correctly predicted'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.32：第一个测试实例，正确预测'
- en: '](img/B16182_03_32.jpg)'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_03_32.jpg)'
- en: 'Figure 3.32: First test instance, correctly predicted'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3.32：第一个测试实例，正确预测
- en: 'The very same approach can be used to plot a user-defined number of test instances,
    arranging the output in subplots, as follows:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样的方法也可以用来绘制用户自定义数量的测试实例，并将输出安排在子图中，如下所示：
- en: '[PRE55]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output will be as follows:'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 3.33: First 25 test instances with their predicted classes and ground
    truth comparison'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.33：前 25 个测试实例及其预测类别与真实标签的对比](img/B16182_03_33.jpg)'
- en: '](img/B16182_03_33.jpg)'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_04_14.jpg)'
- en: 'Figure 3.33: First 25 test instances with their predicted classes and ground
    truth comparison'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.33：前 25 个测试实例及其预测类别与真实标签的对比
- en: Note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3dXv3am](https://packt.live/3dXv3am).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/3dXv3am](https://packt.live/3dXv3am)。
- en: You can also run this example online at [https://packt.live/2Ux5JR5](https://packt.live/2Ux5JR5).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在线运行这个示例，地址是 [https://packt.live/2Ux5JR5](https://packt.live/2Ux5JR5)。
- en: In this activity, we faced a problem that is quite similar to a real-world one.
    We had to deal with complex high dimensional inputs – in our case, grayscale images
    – and we wanted to build a model capable of autonomously grouping them into 10
    different categories. Thanks to the power of deep learning and state-of-the-art
    machine learning frameworks, we were able to build a fully connected neural network
    that achieves a classification accuracy in excess of 88%.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们面临了一个与现实世界问题非常相似的挑战。我们必须处理复杂的高维输入——在我们的案例中是灰度图像——并且我们希望构建一个能够自动将它们分成
    10 个不同类别的模型。得益于深度学习的强大能力和最先进的机器学习框架，我们成功构建了一个全连接神经网络，达到了超过 88% 的分类准确率。
- en: 4\. Getting started with OpenAI and TensorFlow for Reinforcement Learning
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4\. 使用 OpenAI 和 TensorFlow 开始强化学习
- en: 'Activity 4.01: Training a Reinforcement Learning Agent to Play a Classic Video
    Game'
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 4.01：训练强化学习智能体玩经典视频游戏
- en: 'Import all the required modules from OpenAI Baselines and TensorFlow in order
    to use the `PPO` algorithm:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的模块，包括 OpenAI Baselines 和 TensorFlow，以便使用 `PPO` 算法：
- en: '[PRE56]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Define and register a custom convolutional neural network for the policy network:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为策略网络定义并注册一个自定义卷积神经网络：
- en: '[PRE57]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Create a function to build the environment in the format required by OpenAI
    Baselines:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，以 OpenAI Baselines 所需的格式构建环境：
- en: '[PRE58]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Build the `PongNoFrameskip-v4` environment, choose the required policy network
    parameters, and train it:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建 `PongNoFrameskip-v4` 环境，选择所需的策略网络参数并进行训练：
- en: '[PRE59]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'While training, the model produces an output similar to the following (only
    a few lines have been reported here):'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练过程中，模型会输出类似以下内容（这里只报告了部分行）：
- en: '[PRE60]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Run the trained agent in the environment and print the cumulative reward:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在环境中运行训练好的智能体并打印累积奖励：
- en: '[PRE61]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The following lines show the last part of the output:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下几行显示了输出的最后部分：
- en: '[PRE62]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'It also renders the environment, showing what happens in the environment in
    real time:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它还呈现了环境，实时显示环境中的变化：
- en: '![Figure 4.14: One frame of the real-time environment, after rendering'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.14：渲染后的实时环境中的一帧](img/B16182_03_33.jpg)'
- en: '](img/B16182_04_14.jpg)'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_04_14.jpg)'
- en: 'Figure 4.14: One frame of the real-time environment, after rendering'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.14：渲染后的实时环境中的一帧
- en: 'Use the built-in OpenAI Baselines run script to train PPO on the `PongNoFrameskip-v0`
    environment:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用内置的 OpenAI Baselines 运行脚本，在 `PongNoFrameskip-v0` 环境中训练 PPO：
- en: '[PRE63]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The last few lines of the output will be similar to the following:'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的最后几行将类似于以下内容：
- en: '[PRE64]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Use the built-in OpenAI Baselines run script to run the trained model on the
    `PongNoFrameskip-v0` environment:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用内置的 OpenAI Baselines 运行脚本，在 `PongNoFrameskip-v0` 环境中运行训练好的模型：
- en: '[PRE65]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The output will be similar to the following:'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE66]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Use the pretrained weights provided to see the trained agent in action:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用提供的预训练权重查看训练好的智能体的表现：
- en: '[PRE67]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output will be as follows:'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE68]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'You can read the `.tar` file by using the following command:'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用以下命令读取 `.tar` 文件：
- en: '[PRE69]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The output will be as follows:'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE70]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Use the built-in OpenAI Baselines run script to train PPO on `PongNoFrameskip-v0`:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用内置的 OpenAI Baselines 运行脚本，在 `PongNoFrameskip-v0` 上训练 PPO：
- en: '[PRE71]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Note
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/30yFmOi](https://packt.live/30yFmOi).
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/30yFmOi](https://packt.live/30yFmOi)。
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本部分目前没有在线互动示例，需在本地运行。
- en: In this activity, we learned how to train a state-of-the-art reinforcement learning
    agent that, by only looking at screen pixels, is able to achieve better-than-human
    performance when playing a classic Atari video game. We made use of a convolutional
    neural network to encode environment observations and leveraged the state-of-the-art
    OpenAI tool to successfully train a PPO algorithm.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们学习了如何训练一个最先进的强化学习智能体，该智能体通过仅查看屏幕像素，就能在玩经典的Atari视频游戏时实现超越人类的表现。我们利用卷积神经网络对环境观察进行编码，并利用最先进的OpenAI工具成功训练了PPO算法。
- en: 5\. Dynamic Programming
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5\. 动态规划
- en: 'Activity 5.01: Implementing Policy and Value Iteration on the FrozenLake-v0
    Environment'
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 5.01：在FrozenLake-v0环境中实现策略和价值迭代
- en: 'Import the required libraries:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE72]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Initialize the environment and reset the current one. Set `is_slippery=False`
    in the initializer. Show the size of the action space and the number of possible
    states:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化环境并重置当前环境。在初始化器中将`is_slippery=False`。显示动作空间的大小和可能的状态数量：
- en: '[PRE73]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Perform policy evaluation iterations until the smallest change is less than
    `smallest_change`:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行策略评估迭代，直到最小变化小于`smallest_change`：
- en: '[PRE74]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Take the action according to the current policy:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据当前策略执行动作：
- en: '[PRE75]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Use the Bellman optimality equation to update ![6](img/B16182_05_26a.png):'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '使用贝尔曼最优方程更新 ![6](img/B16182_05_26a.png):'
- en: '[PRE76]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Perform policy improvement using the Bellman optimality equation:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用贝尔曼最优方程进行策略改进：
- en: '[PRE77]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Calculate the future reward by taking this action. Note that we are using the
    simplified equation because we don''t have non-one transition probabilities:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行此动作计算未来的奖励。请注意，我们使用的是简化方程，因为我们没有非一的转移概率：
- en: '[PRE78]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Using `assert` statements, we can avoid getting into unwanted situations:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`assert`语句，我们可以避免进入不期望的情况：
- en: '[PRE79]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Update the best action for this current state:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新当前状态下的最佳动作：
- en: '[PRE80]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Find the most optimal policy for the FrozenLake-v0 environment using policy
    iteration:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用策略迭代为FrozenLake-v0环境找到最优策略：
- en: '[PRE81]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Perform a test pass on the FrozenLake-v0 environment:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在FrozenLake-v0环境中执行测试：
- en: '[PRE82]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Define the maximum number of steps the agent is allowed to take. If it doesn''t
    reach a solution in this time, then we call it an episode and proceed ahead:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义智能体允许采取的最大步数。如果在此时间内未找到解决方案，则称之为一个回合，并继续：
- en: '[PRE83]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Take the action that has the highest Q value in the current state:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在当前状态下选择具有最高Q值的动作：
- en: '[PRE84]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Step through the `FrozenLake-v0` environment randomly:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机在`FrozenLake-v0`环境中执行操作：
- en: '[PRE85]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Perform value iteration to find the most optimal policy for the FrozenLake-v0
    environment:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行价值迭代，找到FrozenLake-v0环境的最优策略：
- en: '[PRE86]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Initialize the value table randomly and initialize the policy randomly:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化价值表，并随机初始化策略：
- en: '[PRE87]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Check the reward obtained if we were to perform this action:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查执行此动作时获得的奖励：
- en: '[PRE88]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Run the code and make sure the output matches the expectation by running it
    in the `main` block:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行代码并确保输出与期望一致，可以在`main`块中运行代码进行验证：
- en: '[PRE89]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'After running this, you should be able to see the following output:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行此代码后，你应该能看到如下输出：
- en: '![Figure 5.27: FrozenLake-v0 environment output'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.27: FrozenLake-v0 环境输出'
- en: '](img/B16182_05_27.jpg)'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_05_27.jpg)'
- en: 'Figure 5.27: FrozenLake-v0 environment output'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5.27: FrozenLake-v0 环境输出'
- en: As can be seen from the output, we have successfully achieved the goal of retrieving
    the frisbee.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，我们成功达成了获取飞盘的目标。
- en: Note
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fxtZuq](https://packt.live/3fxtZuq).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 若要获取此部分的源代码，请参考[https://packt.live/3fxtZuq](https://packt.live/3fxtZuq)。
- en: You can also run this example online at [https://packt.live/2ChI1Ss](https://packt.live/2ChI1Ss).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2ChI1Ss](https://packt.live/2ChI1Ss) 在线运行此示例。
- en: 6\. Monte Carlo Methods
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 蒙特卡洛方法
- en: 'Activity 6.01: Exploring the Frozen Lake Problem – the Reward Function'
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 6.01：探索冰湖问题——奖励函数
- en: 'Import the necessary libraries:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE90]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Select the environment as `FrozenLake`. `is_slippery` is set to `False`. The
    environment is reset with the line `env.reset()` and rendered with the line `env.render()`:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择环境为`FrozenLake`。`is_slippery`设置为`False`。通过`env.reset()`重置环境，并通过`env.render()`渲染环境：
- en: '[PRE91]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'You will get the following output:'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将看到如下输出：
- en: '![Figure 6.15: Frozen Lake state rendered'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.15: 冰湖状态渲染'
- en: '](img/B16182_06_15.jpg)'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_06_15.jpg)'
- en: 'Figure 6.15: Frozen Lake state rendered'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 6.15: 冰湖状态渲染'
- en: This is a text grid with the letters `S`, `F`, `G`, and `H` used to represent
    the current environment of `FrozenLake`. The highlighted cell `S` is the current
    state of the agent.
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个文本网格，字母`S`、`F`、`G`和`H`用于表示`FrozenLake`的当前环境。突出显示的单元格`S`是代理的当前状态。
- en: 'Print the possible values in the observation space and the number of action
    values using the `print(env.observation_space)` and `print(env.action_space)`
    functions respectively:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分别使用`print(env.observation_space)`和`print(env.action_space)`函数打印观察空间中的可能值和动作值的数量：
- en: '[PRE92]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'You will get the following output:'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '[PRE93]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '`16` is the number of cells in the grid, so `print(env.observation_space)`
    prints `16`. `4` is the number of possible actions, so `print(env.action_space)`
    prints `4`. `Discrete` shows the observation space and action space take only
    discrete values and do not take continuous values.'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`16`是网格中的单元格数，因此`print(env.observation_space)`打印出`16`。`4`是可能的动作数，因此`print(env.action_space)`打印出`4`。`Discrete`表示观察空间和动作空间仅取离散值，不取连续值。'
- en: 'The next step is to define a function to generate a frozen lake episode. We
    initialize `episodes` and the environment:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是定义一个函数来生成冻结湖泊episode。我们初始化`episodes`和环境：
- en: '[PRE94]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Navigate step by step and store `episode` and return `reward`:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐步导航并存储`episode`并返回`reward`：
- en: '[PRE95]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: The action is obtained with `env.action_space.sample()`. `next_state`, `action`,
    and `reward` are obtained by calling the `env_step(action)` function. They are
    then appended to an episode. The `episode` is now a list of states, actions, and
    rewards.
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 动作通过`env.action_space.sample()`获得。`next_state`、`action`和`reward`是通过调用`env_step(action)`函数获得的，然后它们被添加到一个episode中。现在，`episode`是一个由状态、动作和奖励组成的列表。
- en: The key is now to calculate the success rate, which is the likelihood of success
    for a batch of episodes. The way we do this is by calculating the total number
    of attempts in a batch of episodes. We calculate how many of them successfully
    reached the goal. The ratio of the agent successfully reaching the goal to the
    number of attempts made by the agent is the success ratio.
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关键在于计算成功率，即一批episode的成功概率。我们通过计算一批episode中的总尝试次数来实现这一点，接着计算其中有多少次成功到达目标。代理成功到达目标的比例与代理尝试次数之比就是成功率。
- en: 'First, we initialize the total reward:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们初始化总奖励：
- en: '[PRE96]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Generate the episode and reward for every iteration and calculate the total
    reward:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每次迭代生成episode和reward，并计算总奖励：
- en: '[PRE97]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'The success ratio is calculated by dividing `total_reward` by `100` and is
    printed:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将`total_reward`除以`100`来计算成功率，并打印出来：
- en: '[PRE98]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The frozen lake prediction is calculated using the `frozen_lake_prediction`
    function:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`frozen_lake_prediction`函数计算冻结湖泊预测：
- en: '[PRE99]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'You will get the following output:'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你将获得以下输出：
- en: '![Figure 6.16: Output of Frozen Lake without learning'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.16：没有学习的冻结湖泊输出'
- en: '](img/B16182_06_16.jpg)'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_06_16.jpg)'
- en: 'Figure 6.16: Output of Frozen Lake without learning'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16：没有学习的冻结湖泊输出
- en: The output prints the policy win ratio for the various episodes in batches of
    100\. The ratios are quite low as this is the simulation of an agent following
    a random policy. We will see in the next exercise how this can be improved by
    learning to a higher level by using a combination of a greedy policy and an epsilon
    soft policy.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 输出打印了每100个episode的策略胜率。由于这是模拟代理遵循随机策略的情况，因此这些比率相当低。我们将在下一个练习中看到，通过结合贪心策略和epsilon软策略，如何将其提高到更高水平。
- en: Note
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Akh8Nm](https://packt.live/2Akh8Nm).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参考[https://packt.live/2Akh8Nm](https://packt.live/2Akh8Nm)。
- en: You can also run this example online at [https://packt.live/2zruU07](https://packt.live/2zruU07).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2zruU07](https://packt.live/2zruU07)在线运行此示例。
- en: Activity 6.02 Solving Frozen Lake Using Monte Carlo Control Every Visit Epsilon
    Soft
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动6.02 使用蒙特卡洛控制法每次访问解决冻结湖泊问题 epsilon软策略
- en: 'Import the necessary libraries:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE100]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Select the environment as `FrozenLake`. `is_slippery` is set to `False`:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择环境为`FrozenLake`，并将`is_slippery`设置为`False`：
- en: '[PRE101]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Initialize the `Q` value and `num_state_action` to zeros:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Q`值和`num_state_action`初始化为零：
- en: '[PRE102]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Set the value of `num_episodes` to `100000` and create `rewardsList`. We set
    `epsilon` to `0.30`:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`num_episodes`的值设为`100000`，并创建`rewardsList`。我们将`epsilon`设置为`0.30`：
- en: '[PRE103]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Setting epsilon to `0.30` means we will explore with a likelihood of 0.30 and
    be greedy with a likelihood of 1-0.30 or 0.70.
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将epsilon设置为`0.30`意味着我们以0.30的概率进行探索，以1-0.30或0.70的概率进行贪婪选择。
- en: 'Run the loop till `num_episodes`. We initialize the environment, `results_List`,
    and `result_sum` to zero. Also, reset the environment:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行循环直到`num_episodes`。我们将环境、`results_List`和`result_sum`初始化为零。同时，重置环境：
- en: '[PRE104]'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Start a `while` loop, and check whether you need to pick a random action with
    a probability epsilon or greedy policy with a probability of 1-epsilon:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开始一个`while`循环，检查是否需要以`ε`的概率选择一个随机动作，或以1-ε的概率选择贪婪策略：
- en: '[PRE105]'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Now step through the `action` and get `new_state` and `reward`:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在进行`action`，并获取`new_state`和`reward`：
- en: '[PRE106]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'The result list is appended with the `state` and `action` pair. `result_sum`
    is incremented by the value of the result:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果列表附加`state`和`action`对。`result_sum`按结果的值递增：
- en: '[PRE107]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '`new_state` is assigned to `state` and `result_sum` is appended to `rewardsList`:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`new_state`赋值给`state`，并将`result_sum`附加到`rewardsList`：
- en: '[PRE108]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Calculate `Q[s,a]` using the incremental method, as `Q[s,a] + (result_sum –
    Q[s,a]) / N(s,a)`:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用增量方法计算`Q[s,a]`，即`Q[s,a] + (result_sum – Q[s,a]) / N(s,a)`：
- en: '[PRE109]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Print the value of the success rates in batches of `1000`:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每`1000`次批量打印成功率的值：
- en: '[PRE110]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: 'Print the final success rate:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印最终的成功率：
- en: '[PRE111]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'You will get the following output initially:'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你最初将得到以下输出：
- en: '![Figure 6.17: Initial output of the Frozen Lake success rate'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 6.17：冰湖成功率的初始输出](img/B16182_06_17.jpg)'
- en: '](img/B16182_06_17.jpg)'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_06_17.jpg)'
- en: 'Figure 6.17: Initial output of the Frozen Lake success rate'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17：冰湖成功率的初始输出
- en: 'You will get the following output finally:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 最终你将得到以下输出：
- en: '![Figure 6.18: Final output of the Frozen Lake success rate'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.18：冰湖成功率的最终输出](img/B16182_06_17.jpg)'
- en: '](img/B16182_06_18.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16182_06_18.jpg)'
- en: 'Figure 6.18: Final output of the Frozen Lake success rate'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18：冰湖成功率的最终输出
- en: The success rate starts with a very low value close to 0% but with reinforcement
    learning, it learns, and the success rate increases incrementally going up to
    60%.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 成功率从接近0%的非常低值开始，但通过强化学习，它逐渐学习，成功率逐步增加，最终达到60%。
- en: Note
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Ync9Dq](https://packt.live/2Ync9Dq).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2Ync9Dq](https://packt.live/2Ync9Dq)。
- en: You can also run this example online at [https://packt.live/3cUJLxQ](https://packt.live/3cUJLxQ).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例：[https://packt.live/3cUJLxQ](https://packt.live/3cUJLxQ)。
- en: 7\. Temporal Difference Learning
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7. 时间差分学习
- en: 'Activity 7.01: Using TD(0) Q-Learning to Solve FrozenLake-v0 Stochastic Transitions'
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 7.01：使用TD(0) Q-Learning解决FrozenLake-v0随机转移
- en: 'Import the required modules:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的模块：
- en: '[PRE112]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'Instantiate the `gym` environment called `FrozenLake-v0` using the `is_slippery`
    flag set to `True` in order to enable stochasticity:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`is_slippery`标志设置为`True`来实例化名为`FrozenLake-v0`的`gym`环境，以启用随机性：
- en: '[PRE113]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Take a look at the action and observation spaces:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下动作和观察空间：
- en: '[PRE114]'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'This will print out the following:'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出以下内容：
- en: '[PRE115]'
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Create two dictionaries to easily translate the `actions` numbers into moves:'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个字典以轻松地将`actions`的数字转换为动作：
- en: '[PRE116]'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Reset the environment and render it to take a look at the grid problem:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境并渲染它，以查看网格问题：
- en: '[PRE117]'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Its initial state is as follows:'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它的初始状态如下：
- en: '![Figure 7.39: Environment''s initial state'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.39：环境的初始状态](img/B16182_07_39.jpg)'
- en: '](img/B16182_07_39.jpg)'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_39.jpg)'
- en: 'Figure 7.39: Environment''s initial state'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.39：环境的初始状态
- en: 'Visualize the optimal policy for this environment:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化此环境的最佳策略：
- en: '[PRE118]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: 'This prints out the following output:'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出以下输出：
- en: '[PRE119]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'Define the functions that will take ε-greedy actions:'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义执行ε-贪婪动作的函数：
- en: '[PRE120]'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: 'Define a function that will take greedy actions:'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个执行贪婪策略的函数：
- en: '[PRE121]'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'Define a function that will calculate the agent''s average performance:'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数来计算代理的平均表现：
- en: '[PRE122]'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'Initialize the Q-table so that all the values are equal to `1`, except for
    the values at the terminal states:'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化Q表，使得所有值都等于`1`，终止状态的值除外：
- en: '[PRE123]'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: 'Set the number of total episodes, the number of steps representing the interval
    by which we''re evaluating the agent''s average performance, the learning rate,
    the discounting factor, the `ε` value for the exploration policy, and an array
    to collect all the agent''s performance evaluations during training:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置总回合数、表示我们评估代理平均表现的步骤间隔数、学习率、折扣因子、探索策略的`ε`值，以及一个数组来收集训练期间代理的所有表现评估：
- en: '[PRE124]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'Train the Q-learning algorithm. Loop among all episodes:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练Q学习算法。循环所有的回合：
- en: '[PRE125]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'Reset the environment and start the in-episode loop:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重置环境并开始回合内循环：
- en: '[PRE126]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'Select the exploration action with an ε-greedy policy:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 ε-贪婪策略选择探索动作：
- en: '[PRE127]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'Step the environment with the selected exploration action and retrieval of
    the new state, reward, and done conditions:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步进环境，选择探索动作，并获取新的状态、奖励和完成条件：
- en: '[PRE128]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'Select a new action with the greedy policy:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用贪婪策略选择一个新的动作：
- en: '[PRE129]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Update the Q-table with the Q-learning TD(0) rule:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Q-learning TD(0) 规则更新 Q 表：
- en: '[PRE130]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: 'Update the state with a new value:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新值更新状态：
- en: '[PRE131]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'Evaluate the agent''s average performance for every step:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估代理在每一步的平均表现：
- en: '[PRE132]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: 'Plot the Q-learning agent''s mean reward history during training:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制 Q-learning 代理在训练过程中的平均奖励历史：
- en: '[PRE133]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'This generates the following output, showing the learning progress for the
    Q-learning algorithm:'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下输出，展示 Q-learning 算法的学习进展：
- en: '[PRE134]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'The plot for this can be visualized as follows:'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个图表可以按如下方式可视化：
- en: '![Figure 7.40: Average reward of an epoch trend over training epochs'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.40：训练周期中每个周期的平均奖励趋势'
- en: '](img/B16182_07_40.jpg)'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_07_40.jpg)'
- en: 'Figure 7.40: Average reward of an epoch trend over training epochs'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.40：训练周期中每个周期的平均奖励趋势
- en: In this case, as in the case of Q-learning applied to the deterministic environment,
    the plot shows how quickly Q-learning performance grows over epochs as the agent
    collects more and more experience. It also demonstrates that the algorithm is
    not capable of reaching 100% success after learning due to the limitations of
    stochasticity. When compared with using the SARSA method on a stochastic environment,
    as seen in *Figure 7.15*, the algorithm's performance grows faster and more steadily.
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，和应用于确定性环境的 Q-learning 一样，图表显示了随着代理收集越来越多的经验，Q-learning 性能在周期中增长的速度有多快。它还证明了，由于随机性的限制，该算法在学习后无法达到
    100% 的成功率。与在随机环境中使用 SARSA 方法的表现相比，如 *图 7.15* 所示，该算法的性能增长速度更快且更稳步。
- en: 'Evaluate the greedy policy''s performance for the trained agent (Q-table):'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估训练后的代理（Q 表）的贪婪策略表现：
- en: '[PRE135]'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'This prints out the following:'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将打印出以下内容：
- en: '[PRE136]'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'Display the Q-table values:'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示 Q 表的值：
- en: '[PRE137]'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'This generates the following output:'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE138]'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: 'Print out the greedy policy that was found and compare it with the optimal
    policy:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出找到的贪婪策略，并与最优策略进行比较：
- en: '[PRE139]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'This generates the following output:'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成以下输出：
- en: '[PRE140]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE140]'
- en: This output shows that, as for all the exercises in this chapter, the off-policy,
    one-step Q-learning algorithm is able to find the optimal policy by simply exploring
    the environment, even in the context of stochastic environment transitions. As
    anticipated, for this setting, it is not possible to achieve the maximum reward
    100% of the time.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出显示了，正如本章中所有练习的情况一样，离策略、一阶段的 Q-learning 算法能够通过简单地探索环境找到最优策略，即使在随机环境转移的背景下也是如此。如预期的那样，在这个设置下，不可能
    100% 的时间内实现最大奖励。
- en: As we can see, for every state of the grid world that the greedy policy obtained
    with the Q-table that was calculated by our algorithm, it prescribes an action
    that is in accordance with the optimal policy that was defined by analyzing the
    environment problem. As we already saw, there are two states in which many different
    actions are equally optimal, and the agent correctly implements one of them.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，对于网格世界的每个状态，通过我们算法计算出的 Q 表所得到的贪婪策略都会指定一个符合最优策略的动作，而最优策略是通过分析环境问题定义的。正如我们之前所看到的，有两个状态，在这些状态下，许多不同的动作都是最优的，代理正确地执行了其中之一。
- en: Note
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3elMxxu](https://packt.live/3elMxxu).
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/3elMxxu](https://packt.live/3elMxxu)。
- en: You can also run this example online at [https://packt.live/37HSDWx](https://packt.live/37HSDWx).
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问 [https://packt.live/37HSDWx](https://packt.live/37HSDWx)。
- en: 8\. The Multi-Armed Bandit Problem
  id: totrans-406
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8\. 多臂赌博机问题
- en: 'Activity 8.01: Queueing Bandits'
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 8.01：排队赌博机
- en: 'Import the necessary libraries and tools, as follows:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库和工具，如下所示：
- en: '[PRE141]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'Declare the bandit object, as follows:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明 bandit 对象，如下所示：
- en: '[PRE142]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE142]'
- en: The `N_CLASSES` variable will be used by our subsequent code.
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`N_CLASSES` 变量将在我们后续的代码中使用。'
- en: 'Implement the Greedy algorithm, as follows:'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现贪婪算法，如下所示：
- en: '[PRE143]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE143]'
- en: Notice that we are taking care to avoid choosing a class that does not have
    any customers left in it by checking if `queue_lengths[class_]` is greater than
    0 or not. The remaining code is analogous to what we had in our earlier discussion
    of Greedy.
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我们通过检查 `queue_lengths[class_]` 是否大于 0 来小心避免选择一个没有剩余客户的类别。剩余的代码与我们早先讨论的贪婪算法类似。
- en: 'Subsequently, apply the algorithm to the bandit object, as follows:'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随后，将算法应用于赌博机对象，如下所示：
- en: '[PRE144]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'This will generate the following graph:'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将生成如下图表：
- en: '![Figure 8.24: Distribution of cumulative waiting time from Greedy'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.24：来自贪婪算法（Greedy）的累积等待时间分布'
- en: '](img/B16182_08_24.jpg)'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_24.jpg)'
- en: '[PRE145]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE145]'
- en: While these values might appear large compared to our earlier discussions, this
    is because the reward/cost distributions we are working with here take on higher
    values. We will use these values from Greedy as a frame of reference to analyze
    the performance of later algorithms.
  id: totrans-422
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然这些值与我们之前的讨论相比看起来较大，但这是因为我们这里所使用的奖励/成本分布的数值较高。我们将使用贪婪算法的这些值作为参考框架，分析后续算法的表现。
- en: 'Implement the Explore-then-commit algorithm using the following code:'
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码实现探索后再承诺（Explore-then-commit）算法：
- en: '[PRE146]'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE146]'
- en: 'Apply the algorithm to the bandit object, as follows:'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将算法应用于赌博机对象，如下所示：
- en: '[PRE147]'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE147]'
- en: 'This will produce the following graph:'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将产生以下图表：
- en: '![Figure 8.25: Distribution of cumulative waiting time from Explore-then-commit'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.25：来自探索后再承诺（Explore-then-commit）算法的累积等待时间分布'
- en: '](img/B16182_08_25.jpg)'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_25.jpg)'
- en: 'Figure 8.25: Distribution of cumulative waiting time from Explore-then-commit'
  id: totrans-430
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.25：来自探索后再承诺（Explore-then-commit）算法的累积等待时间分布
- en: 'This will also produce the max and average cumulative waiting times: `(1238591.3208636027,
    45909.77140562623)`. Compared to Greedy `(1218887.7924350922, 45155.236786598274)`,
    Explore-then-commit did relatively worse on this queueing bandit problem.'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这也将产生最大值和平均累积等待时间：（`(1238591.3208636027, 45909.77140562623)`）。与贪婪算法的结果（`(1218887.7924350922,
    45155.236786598274)`）相比，探索后再承诺算法在这个排队赌博机问题上表现较差。
- en: 'Implement Thompson Sampling, as follows:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现汤普森采样，如下所示：
- en: '[PRE148]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE148]'
- en: Recall that in our initial discussion of Thompson Sampling, we draw random samples
    to estimate the reward expectation for each arm. Here, we drew random samples
    from the corresponding Gamma distributions (which are being used to model service
    rates) to estimate the rates (or the inverse job lengths) and choose the largest
    drawn sample.
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回顾我们之前讨论汤普森采样时，我们从每个臂的对应伽玛分布中抽取随机样本（这些分布用于建模服务率），以估算各个臂的奖励期望值。在这里，我们从相应的伽玛分布中抽取随机样本（这些伽玛分布用于建模服务率），以估算服务率（或工作时长的倒数），并选择最大值作为被抽取的样本。
- en: 'This can be applied to solve the bandit problem using the following code:'
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这可以通过以下代码应用于解决赌博机问题：
- en: '[PRE149]'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE149]'
- en: 'The following plot will be produced:'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将产生以下图表：
- en: '![Figure 8.26: Distribution of cumulative waiting time from Thompson Sampling'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 8.26：来自汤普森采样（Thompson Sampling）的累积等待时间分布'
- en: '](img/B16182_08_26.jpg)'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_26.jpg)'
- en: 'Figure 8.26: Distribution of cumulative waiting time from Thompson Sampling'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 8.26：来自汤普森采样（Thompson Sampling）的累积等待时间分布
- en: From the max and mean waiting time `(1218887.7924350922, 45129.343871806814)`,
    we can see that Thompson Sampling is able to improve on Greedy.
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从最大值和平均等待时间（`(1218887.7924350922, 45129.343871806814)`）来看，我们可以看到汤普森采样在贪婪算法上有所改进。
- en: 'The modified version of Thompson Sampling can be implemented as follows:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 汤普森采样的修改版本可以实现如下：
- en: '[PRE150]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE150]'
- en: The initialization method of this class implementation has an additional attribute,
    `r`, which we will use to implement the exploitation logic.
  id: totrans-444
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该类实现的初始化方法增加了一个额外的属性 `r`，我们将用它来实现利用逻辑。
- en: In the `decide()` method, right before we draw samples to estimate the rates,
    we check to see if the current time (`t`) is greater than the current queue length
    (the sum of `queue_lengths`). This Boolean indicates whether we have processed
    more than half of the customers or not. If so, we simply implement the logic of
    the Greedy algorithm and return the arm with the optimal average rate. Otherwise,
    we have our actual Thompson Sampling logic.
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在 `decide()` 方法中，在我们抽取样本来估算服务率之前，我们检查当前时间（`t`）是否大于当前队列长度（`queue_lengths` 的总和）。该布尔值指示我们是否已经处理了超过一半的客户。如果是，我们就直接实现贪婪算法的逻辑，并返回具有最佳平均速率的臂。如果不是，我们就执行实际的汤普森采样逻辑。
- en: 'The `update()` method should be the same as the actual Thompson Sampling algorithm
    from the previous step, as follows:'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`update()` 方法应与上一阶段实际的汤普森采样算法相同，如下所示：'
- en: '[PRE151]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE151]'
- en: 'Finally, apply the algorithm to the bandit problem:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将算法应用于赌博机问题：
- en: '[PRE152]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'We will obtain the following graph:'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将得到如下图表：
- en: '![Figure 8.27: Distribution of cumulative waiting time from modified Thompson
    Sampling'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图8.27：修改版汤普森采样的累积等待时间分布'
- en: '](img/B16182_08_27.jpg)'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_08_27.jpg)'
- en: 'Figure 8.27: Distribution of cumulative waiting time from modified Thompson
    Sampling'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.27：修改版汤普森采样的累积等待时间分布
- en: Together with the max and mean waiting time `(1218887.7924350922, 45093.244027644556)`,
    we can see that this modified version of Thompson Sampling is more effective than
    the original at minimizing the cumulative waiting time across the experiments.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 结合最大和平均等待时间`(1218887.7924350922, 45093.244027644556)`，我们可以看到，修改版的汤普森采样在减少实验中的累积等待时间方面比原版更有效。
- en: This speaks to the potential benefit of designing algorithms that are tailored
    to the contextual bandit problem that they are trying to solve.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明设计专门针对上下文赌博机问题的算法可能带来的潜在益处。
- en: Note
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2Yuw2IQ](https://packt.live/2Yuw2IQ).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本节的源代码，请参考[https://packt.live/2Yuw2IQ](https://packt.live/2Yuw2IQ)。
- en: You can also run this example online at [https://packt.live/3hnK5Z5](https://packt.live/3hnK5Z5).
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/3hnK5Z5](https://packt.live/3hnK5Z5)在线运行此示例。
- en: Throughout this activity, we have learned how to apply the approaches discussed
    in this chapter to a queueing bandit problem, that is, exploring an example of
    a potential contextual bandit process. Most notably, we have considered a variant
    of Thompson Sampling that has been modified to fit the context of the queueing
    problem, thus successfully lowering our cumulative regret compared to other algorithms.
    This activity also marks the end of this chapter.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个活动过程中，我们学习了如何将本章讨论的方法应用于排队赌博机问题，即探索一个潜在的上下文赌博机过程。最重要的是，我们考虑了一个修改版的汤普森采样，使其适应排队问题的上下文，从而成功地降低了我们的累积遗憾，相较于其他算法。本活动也标志着本章的结束。
- en: 9\. What Is Deep Q-Learning?
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9\. 什么是深度Q学习？
- en: 'Activity 9.01: Implementing a Double Deep Q Network in PyTorch for the CartPole
    Environment'
  id: totrans-461
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动9.01：在PyTorch中实现双深度Q网络（DDQN）以应对CartPole环境
- en: 'Open a new Jupyter notebook and import all of the required libraries:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter笔记本，并导入所有必需的库：
- en: '[PRE153]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE153]'
- en: 'Write code that will create a device based on the availability of a GPU environment:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码，基于GPU环境的可用性来创建设备：
- en: '[PRE154]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE154]'
- en: 'Create a `gym` environment using the `''CartPole-v0''` environment:'
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`'CartPole-v0'`环境创建一个`gym`环境：
- en: '[PRE155]'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE155]'
- en: 'Set the `seed` for torch and the environment for reproducibility:'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置`seed`以确保torch和环境的可复现性：
- en: '[PRE156]'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE156]'
- en: 'Fetch the number of states and actions from the environment:'
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从环境中获取状态数和动作数：
- en: '[PRE157]'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE157]'
- en: 'The output is as follows:'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE158]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE158]'
- en: 'Set all of the hyperparameter values required for the DDQN process:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置DDQN过程所需的所有超参数值：
- en: '[PRE159]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE159]'
- en: 'Implement the `calculate_epsilon` function, as described in the previous exercises:'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`calculate_epsilon`函数，如前面练习中所描述的：
- en: '[PRE160]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE160]'
- en: 'Create a class, called `DQN`, that accepts the number of states as inputs and
    outputs Q values for the number of actions present in the environment, with the
    network that has a hidden layer of size `64`:'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`DQN`的类，接受状态数作为输入，并输出环境中存在的动作数的Q值，网络的隐藏层大小为`64`：
- en: '[PRE161]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE161]'
- en: 'Implement the `ExperienceReplay` class, as described in the previous exercises:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现`ExperienceReplay`类，如前面练习中所描述的：
- en: '[PRE162]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE162]'
- en: 'Instantiate the `ExperienceReplay` class by passing the buffer size as input:'
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过传递缓冲区大小作为输入来实例化`ExperienceReplay`类：
- en: '[PRE163]'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE163]'
- en: 'Implement the DQN agent class with the changes discussed for the `optimize`
    function (from the code example given in the *Double Deep Q Network (DDQN)* section):'
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现DQN代理类，并修改`optimize`函数（参考*双深度Q网络（DDQN）*部分给出的代码示例）：
- en: '[PRE164]'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE164]'
- en: 'Write the training process loop with the help of the following steps. First,
    instantiate the DQN agent using the class created earlier. Create a `steps_total`
    empty list to collect the total number of steps for each episode. Initialize `steps_counter`
    with zero and use it to calculate the decayed epsilon value for each step:'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下步骤编写训练过程的循环。首先，使用之前创建的类实例化DQN代理。创建一个`steps_total`空列表，用于收集每个回合的总步数。将`steps_counter`初始化为零，并用它来计算每个步骤的衰减epsilon值：
- en: '[PRE165]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE165]'
- en: Use two loops during the training process; the first one is to play the game
    for a certain number of steps. The second loop ensures that each episode goes
    on for a fixed number of steps. Inside the second `for` loop, the first step is
    to calculate the epsilon value for the current step.
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在训练过程中使用两个循环；第一个循环用于执行游戏若干步，第二个循环确保每个回合持续固定的步数。在第二个`for`循环中，第一步是计算当前步骤的 epsilon
    值。
- en: Using the present state and epsilon value, you can select the action to perform.
    The next step is to take the action. Once you take the action, the environment
    returns the `new_state`, `reward`, and `done` flags.
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用当前状态和 epsilon 值，你可以选择要执行的动作。下一步是执行动作。一旦你执行了动作，环境将返回`new_state`、`reward`和`done`标志。
- en: 'Using the `optimize` function, perform one step of gradient descent to optimize
    the DQN. Now make the new state the present state for the next iteration. Finally,
    check whether the episode is over. If the episode is over, then you can collect
    and record the reward for the current episode:'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`optimize`函数执行一次梯度下降步骤来优化 DQN。然后将新状态设置为下一次迭代的当前状态。最后，检查回合是否结束。如果回合结束，那么你可以收集并记录当前回合的奖励：
- en: '[PRE166]'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'Now observe the reward. As the reward is scalar feedback and gives an indication
    of how well the agent is performing, you should look at the average reward and
    the average reward for the last 100 episodes. Also, perform the graphical representation
    of rewards. Check how the agent is performing while playing more episodes and
    what the reward average is for the last 100 episodes:'
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在观察奖励。由于奖励是标量反馈，能够指示代理的表现，你应查看平均奖励和最后 100 个回合的平均奖励。同时，绘制奖励的图形表示。检查代理在进行更多回合时的表现，以及最后
    100 个回合的奖励平均值：
- en: '[PRE167]'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE167]'
- en: 'The output will be as follows:'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE168]'
  id: totrans-495
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE168]'
- en: 'Plot the rewards collected in the y axis and the number of episodes in the
    x axis to visualize how the rewards have been collected with the increasing number
    of episodes:'
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 y 轴绘制收集到的奖励，在 x 轴绘制游戏的回合数，以可视化奖励随回合数的变化：
- en: '[PRE169]'
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE169]'
- en: 'The output will be as follows:'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 9.37: Plot for the rewards collected by the agent'
  id: totrans-499
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.37：代理收集的奖励图'
- en: '](img/B16182_09_37.jpg)'
  id: totrans-500
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_09_37.jpg)'
- en: 'Figure 9.37: Plot for the rewards collected by the agent'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.37：代理收集的奖励图
- en: Note
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3hnLDTd](https://packt.live/3hnLDTd).
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/3hnLDTd](https://packt.live/3hnLDTd)。
- en: You can also run this example online at [https://packt.live/37ol5MK](https://packt.live/37ol5MK).
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问 [https://packt.live/37ol5MK](https://packt.live/37ol5MK)。
- en: 'The following is a comparison between different DQN techniques and DDQN:'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是不同 DQN 技术与 DDQN 的比较：
- en: '**Vanilla DQN Outputs:**'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '**基础 DQN 输出：**'
- en: '[PRE170]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '**DQN with Experience Replay and Target Network Outputs:**'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '**带有经验回放和目标网络的 DQN 输出：**'
- en: '[PRE171]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '**DDQN Outputs:**'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '**DDQN 输出：**'
- en: '[PRE172]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: As you can see from the preceding figure, along with the comparison of the results
    shown earlier, DDQN has the highest average reward, compared to other DQN implementations,
    and the average reward for the last 100 episodes is also higher. We can say that
    DDQN improves performance significantly in comparison to the other two DQN techniques.
    After completing this whole activity, we have learned how to combine a DDQN network
    with experience replay to overcome the issues of a vanilla DQN and achieve more
    stable rewards.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前图所示，通过前面的结果对比，DDQN 相较于其他 DQN 实现具有最高的平均奖励，并且最后 100 个回合的平均奖励也更高。我们可以说，DDQN
    相较于其他两种 DQN 技术，显著提高了性能。完成整个活动后，我们学会了如何将 DDQN 网络与经验回放结合，克服基础 DQN 的问题，并实现更稳定的奖励。
- en: 10\. Playing an Atari Game with Deep Recurrent Q-Networks
  id: totrans-513
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10\. 使用深度递归 Q 网络（DRQN）玩 Atari 游戏
- en: 'Activity 10.01: Training a DQN with CNNs to Play Breakout'
  id: totrans-514
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 10.01：使用 CNN 训练 DQN 玩 Breakout
- en: '**Solution**'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**'
- en: 'Open a new Jupyter Notebook and import the relevant packages: `gym`, `random`,
    `tensorflow`, `numpy`, and `collections`:'
  id: totrans-516
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter Notebook，并导入相关的包：`gym`、`random`、`tensorflow`、`numpy` 和 `collections`：
- en: '[PRE173]'
  id: totrans-517
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE173]'
- en: 'Set the seed for NumPy and TensorFlow to `168`:'
  id: totrans-518
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为 NumPy 和 TensorFlow 设置种子为`168`：
- en: '[PRE174]'
  id: totrans-519
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE174]'
- en: 'Create the `DQN` class with the following methods: the `build_model()` method
    to instantiate a CNN, the `get_action()` method to apply the epsilon-greedy algorithm
    to choose the action to be played, the `add_experience()` method to store in memory
    the experience acquired by playing the game, the `replay()` method, which will
    perform experience replay by sampling experiences from the memory and train the
    DQN model with a callback to save the model every two episodes, and the `update_epsilon()`
    method to gradually decrease the epsilon value for epsilon-greedy:'
  id: totrans-520
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`DQN`类，其中包含以下方法：`build_model()`方法，用于实例化一个CNN；`get_action()`方法，应用epsilon-greedy算法来选择要执行的动作；`add_experience()`方法，将玩游戏过程中获得的经验存储到内存中；`replay()`方法，通过从内存中采样经验并训练DQN模型，每两回合保存一次模型；`update_epsilon()`方法，用于逐渐减少epsilon-greedy的epsilon值：
- en: '[PRE175]'
  id: totrans-521
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE175]'
- en: 'Create the `initialize_env()` function, which will initialize the Breakout
    environment:'
  id: totrans-522
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`initialize_env()`函数，它将初始化Breakout环境：
- en: '[PRE176]'
  id: totrans-523
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE176]'
- en: 'Create the `preprocess_state()` function to preprocess the input images:'
  id: totrans-524
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`preprocess_state()`函数来预处理输入图像：
- en: '[PRE177]'
  id: totrans-525
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE177]'
- en: 'Create the `play_game()` function, which will play an entire game of Breakout:'
  id: totrans-526
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`play_game()`函数，它将玩一个完整的Breakout游戏：
- en: '[PRE178]'
  id: totrans-527
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE178]'
- en: 'Create the `train_agent()` function, which will iterate through a number of
    episodes where the agent will play a game and perform experience replay:'
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`train_agent()`函数，它将通过多个回合进行迭代，代理将在每一轮中玩游戏并执行经验重放：
- en: '[PRE179]'
  id: totrans-529
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE179]'
- en: 'Instantiate a Breakout environment called `env` with the `gym.make()` function:'
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`gym.make()`函数实例化一个名为`env`的Breakout环境：
- en: '[PRE180]'
  id: totrans-531
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE180]'
- en: 'Create two variables, `IMG_SIZE` and `SEQUENCE`, that will take the values
    `84` and `4`, respectively:'
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个变量，`IMG_SIZE`和`SEQUENCE`，分别设置为`84`和`4`：
- en: '[PRE181]'
  id: totrans-533
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE181]'
- en: 'Instantiate a `DQN` object called `agent`:'
  id: totrans-534
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个名为`agent`的`DQN`对象：
- en: '[PRE182]'
  id: totrans-535
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE182]'
- en: 'Create a variable called `episodes` that will take the value `50`:'
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`episodes`的变量，值设为`50`：
- en: '[PRE183]'
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE183]'
- en: 'Call the `train_agent` function by providing `env`, `episodes`, and `agent`:'
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过提供`env`、`episodes`和`agent`来调用`train_agent`函数：
- en: '[PRE184]'
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE184]'
- en: 'The following is the output of the code:'
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是代码的输出：
- en: '[PRE185]'
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE185]'
- en: Note
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3hoZXdV](https://packt.live/3hoZXdV).
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参见[https://packt.live/3hoZXdV](https://packt.live/3hoZXdV)。
- en: You can also run this example online at [https://packt.live/3dWLwfa](https://packt.live/3dWLwfa).
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，网址为[https://packt.live/3dWLwfa](https://packt.live/3dWLwfa)。
- en: You just completed the first activity of this chapter. You successfully built
    and trained a DQN agent combined with CNNs to play the game Breakout. The performance
    of this model is very similar to the random agent (average score of 0.6). However,
    if you train it for longer (by increasing the number of episodes), it may achieve
    a better score.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚完成了本章的第一个活动。你成功地构建并训练了一个结合了CNN的DQN代理来玩Breakout游戏。该模型的表现与随机代理非常相似（平均得分为0.6）。然而，如果你训练更长时间（通过增加训练集数目），它可能会取得更好的成绩。
- en: 'Activity 10.02: Training a DRQN to Play Breakout'
  id: totrans-546
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动10.02：训练DRQN玩Breakout
- en: '**Solution**'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**'
- en: 'Open a new Jupyter Notebook and import the relevant packages: `gym`, `random`,
    `tensorflow`, `numpy`, and `collections`:'
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook并导入相关的包：`gym`、`random`、`tensorflow`、`numpy`和`collections`：
- en: '[PRE186]'
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE186]'
- en: 'Set the seed for NumPy and TensorFlow to `168`:'
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将NumPy和TensorFlow的随机种子设置为`168`：
- en: '[PRE187]'
  id: totrans-551
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE187]'
- en: 'Create the `DRQN` class with the following methods: the `build_model()` method
    to instantiate a CNN combined with a RNN model, the `get_action()` method to apply
    the epsilon-greedy algorithm to choose the action to be played, the `add_experience()`
    method to store in memory the experience acquired by playing the game, the `replay()`
    method, which will perform experience replay by sampling experiences from the
    memory and train the DRQN model with a callback to save the model every two episodes,
    and the `update_epsilon()` method to gradually decrease the epsilon value for
    epsilon-greedy:'
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`DRQN`类，其中包含以下方法：`build_model()`方法，用于实例化一个结合了CNN和RNN的模型；`get_action()`方法，应用epsilon-greedy算法来选择要执行的动作；`add_experience()`方法，用于将玩游戏过程中获得的经验存储到内存中；`replay()`方法，通过从内存中采样经验并训练DRQN模型，每两回合保存一次模型；`update_epsilon()`方法，用于逐渐减少epsilon-greedy的epsilon值：
- en: '[PRE188]'
  id: totrans-553
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE188]'
- en: 'Create the `initialize_env()` function, which will initialize the Breakout
    environment:'
  id: totrans-554
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`initialize_env()`函数，它将初始化Breakout环境：
- en: '[PRE189]'
  id: totrans-555
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE189]'
- en: 'Create the `preprocess_state()` function to preprocess the input images:'
  id: totrans-556
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`preprocess_state()`函数来预处理输入图像：
- en: '[PRE190]'
  id: totrans-557
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE190]'
- en: 'Create the `combine_images()` function to stack the previous four screenshots:'
  id: totrans-558
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`combine_images()`函数，用于将之前的四个截图堆叠在一起：
- en: '[PRE191]'
  id: totrans-559
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE191]'
- en: 'Create the `play_game()` function, which will play an entire game of Breakout:'
  id: totrans-560
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`play_game()`函数，该函数将进行一整局Breakout游戏：
- en: '[PRE192]'
  id: totrans-561
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE192]'
- en: 'Create the `train_agent()` function, which will iterate through a number of
    episodes where the agent will play a game and perform experience replay:'
  id: totrans-562
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`train_agent()`函数，该函数将在多个回合中反复进行，代理将玩游戏并进行经验回放：
- en: '[PRE193]'
  id: totrans-563
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE193]'
- en: 'Instantiate a Breakout environment called `env` with `gym.make()`:'
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`gym.make()`实例化一个名为`env`的Breakout环境：
- en: '[PRE194]'
  id: totrans-565
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE194]'
- en: 'Create two variables, `IMG_SIZE` and `SEQUENCE`, that will take the values
    `84` and `4`, respectively:'
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个变量，`IMG_SIZE`和`SEQUENCE`，分别赋值为`84`和`4`：
- en: '[PRE195]'
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE195]'
- en: 'Instantiate a `DRQN` object called `agent`:'
  id: totrans-568
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个名为`agent`的`DRQN`对象：
- en: '[PRE196]'
  id: totrans-569
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE196]'
- en: 'Create a variable called `episodes` that will take the value `200`:'
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`episodes`的变量，赋值为`200`：
- en: '[PRE197]'
  id: totrans-571
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE197]'
- en: 'Call the `train_agent` function by providing `env`, `episodes`, and `agent`:'
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`train_agent`函数，传入`env`，`episodes`和`agent`：
- en: '[PRE198]'
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE198]'
- en: 'The following is the output of the code:'
  id: totrans-574
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是代码的输出：
- en: '[PRE199]'
  id: totrans-575
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE199]'
- en: Note
  id: totrans-576
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意：
- en: To access the source code for this specific section, please refer to [https://packt.live/2AjdgMx](https://packt.live/2AjdgMx).
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2AjdgMx](https://packt.live/2AjdgMx)。
- en: You can also run this example online at [https://packt.live/37mhlLM](https://packt.live/37mhlLM).
  id: totrans-578
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还可以在[https://packt.live/37mhlLM](https://packt.live/37mhlLM)在线运行此示例。
- en: In this activity, we added an LSTM layer and built a DRQN agent. It learned
    how to play the Breakout game, but didn't achieve satisfactory results even after
    200 episodes. It seems this is still at the exploratory stage. You may try to
    train it for more episodes.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，我们添加了一个LSTM层并构建了一个DRQN代理。它学会了如何玩Breakout游戏，但即使经过200回合，也没有取得令人满意的结果。看来它仍处于探索阶段。你可以尝试训练更多回合。
- en: 'Activity 10.03: Training a DARQN to Play Breakout'
  id: totrans-580
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动10.03：训练DARQN玩Breakout
- en: '**Solution**'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决方案**'
- en: 'Open a new Jupyter Notebook and import the relevant packages: `gym`, `random`,
    `tensorflow`, `numpy`, and `collections`:'
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的Jupyter Notebook并导入相关的包：`gym`，`random`，`tensorflow`，`numpy`和`collections`：
- en: '[PRE200]'
  id: totrans-583
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE200]'
- en: 'Set the seed for NumPy and TensorFlow to `168`:'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将NumPy和TensorFlow的种子设置为`168`：
- en: '[PRE201]'
  id: totrans-585
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE201]'
- en: 'Create the `DARQN` class and create the following methods: the `build_model()`
    method to instantiate a CNN combined with an RNN model, the `get_action()` method
    to apply the epsilon-greedy algorithm to choose the action to be played, the `add_experience()`
    method to store in memory the experience acquired by playing the game, the `replay()`
    method, which will perform experience replay by sampling experiences from the
    memory and train the DARQN model with a callback to save the model every two episodes,
    and the `update_epsilon()` method to gradually decrease the epsilon value for
    epsilon-greedy:'
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`DARQN`类并创建以下方法：`build_model()`方法，用于实例化一个结合了CNN和RNN模型的网络，`get_action()`方法，用于应用epsilon-greedy算法选择要执行的动作，`add_experience()`方法，用于将通过玩游戏获得的经验存储到内存中，`replay()`方法，将通过从内存中采样经验并进行经验回放来训练DARQN模型，并设置回调每两个回合保存一次模型，`update_epsilon()`方法，用于逐步减少epsilon-greedy中的epsilon值：
- en: '[PRE202]'
  id: totrans-587
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE202]'
- en: 'Create the `initialize_env()` function, which will initialize the Breakout
    environment:'
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`initialize_env()`函数，该函数将初始化Breakout环境：
- en: '[PRE203]'
  id: totrans-589
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE203]'
- en: 'Create the `preprocess_state()` function to preprocess the input images:'
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`preprocess_state()`函数，用于预处理输入图像：
- en: '[PRE204]'
  id: totrans-591
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE204]'
- en: 'Create the `combine_images()` function to stack the previous four screenshots:'
  id: totrans-592
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`combine_images()`函数，用于将之前的四个截图堆叠在一起：
- en: '[PRE205]'
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE205]'
- en: 'Create the `preprocess_state()` function to preprocess the input images:'
  id: totrans-594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`preprocess_state()`函数，用于预处理输入图像：
- en: '[PRE206]'
  id: totrans-595
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE206]'
- en: 'Create the `train_agent()` function, which will iterate through a number of
    episodes where the agent will play a game and perform experience replay:'
  id: totrans-596
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建`train_agent()`函数，该函数将在多个回合中反复进行，代理将玩游戏并进行经验回放：
- en: '[PRE207]'
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE207]'
- en: 'Instantiate a Breakout environment called `env` with `gym.make()`:'
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`gym.make()`实例化一个名为`env`的Breakout环境：
- en: '[PRE208]'
  id: totrans-599
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE208]'
- en: 'Create two variables, `IMG_SIZE` and `SEQUENCE`, that will take the values
    `84` and `4`, respectively:'
  id: totrans-600
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建两个变量，`IMG_SIZE`和`SEQUENCE`，分别赋值为`84`和`4`：
- en: '[PRE209]'
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE209]'
- en: 'Instantiate a `DRQN` object called `agent`:'
  id: totrans-602
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个名为`agent`的`DRQN`对象：
- en: '[PRE210]'
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE210]'
- en: 'Create a variable called `episodes` that will take the value `400`:'
  id: totrans-604
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`episodes`的变量，赋值为`400`：
- en: '[PRE211]'
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE211]'
- en: 'Call the `train_agent` function by providing `env`, `episodes`, and `agent`:'
  id: totrans-606
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用`train_agent`函数，传入`env`，`episodes`和`agent`：
- en: '[PRE212]'
  id: totrans-607
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE212]'
- en: 'The following is the output of the code:'
  id: totrans-608
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是代码的输出：
- en: '[PRE213]'
  id: totrans-609
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE213]'
- en: In this activity, we built and trained a `DARQN` agent. It successfully learned
    how to play the Breakout game. It started with a score of `1.0` and achieved a
    final score of over `10` after `400` episodes, as shown in the preceding results.
    This is quite remarkable performance.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们构建并训练了一个`DARQN`代理。它成功学会了如何玩 Breakout 游戏。它的初始分数是 `1.0`，经过 `400` 回合后，最终得分超过
    `10`，如前面的结果所示。这是相当出色的表现。
- en: Note
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2XUDZrH](https://packt.live/2XUDZrH).
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/2XUDZrH](https://packt.live/2XUDZrH)。
- en: You can also run this example online at [https://packt.live/2UDCsUP](https://packt.live/2UDCsUP).
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/2UDCsUP](https://packt.live/2UDCsUP) 上在线运行此示例。
- en: 11\. Policy-Based Methods for Reinforcement Learning
  id: totrans-614
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11. 强化学习的基于策略方法
- en: 'Activity 11.01: Creating an Agent That Learns a Model Using DDPG'
  id: totrans-615
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 11.01：创建一个使用 DDPG 学习模型的代理
- en: 'Import the necessary libraries (`os`, `gym`, and `ddpg`):'
  id: totrans-616
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库（`os`、`gym` 和 `ddpg`）：
- en: '[PRE214]'
  id: totrans-617
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE214]'
- en: 'First, we create our Gym environment (`LunarLanderContinuous-v2`), as we did
    previously:'
  id: totrans-618
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建我们的 Gym 环境（`LunarLanderContinuous-v2`），就像之前一样：
- en: '[PRE215]'
  id: totrans-619
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE215]'
- en: 'Initialize the agent with some sensible hyperparameters, as in *Exercise 11.02*,
    *Creating a Learning Agent*:'
  id: totrans-620
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一些合理的超参数初始化代理，如*练习 11.02*，*创建学习代理*：
- en: '[PRE216]'
  id: totrans-621
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE216]'
- en: Set up a random seed so that our experiments are reproducible.
  id: totrans-622
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个随机种子，以便我们的实验可以复现。
- en: '[PRE217]'
  id: totrans-623
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE217]'
- en: 'Create a blank array to story the scores; you can name it `history`. Iterate
    for at least `1000` episodes and in each episode, set a running score variable
    to `0` and the `done` flag to `False`, then reset the environment. Then, when
    the `done` flag is not `True`, carry out the following step:'
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个空数组来存储分数，你可以将其命名为`history`。循环至少进行`1000`个回合，在每个回合中，将运行分数变量设置为`0`，并将`done`标志设置为`False`，然后重置环境。然后，当`done`标志不为`True`时，执行以下步骤：
- en: '[PRE218]'
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE218]'
- en: 'Select the observations and get the new `state`, `reward`, and `done` flags.
    Save the `observation`, `action`, `reward`, `state_new`, and `done` flags. Call
    the `learn` function of the agent and add the current reward to the running score.
    Set the new state as the observation and finally, when the done flag is `True`,
    append `score` to `history`:'
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择观察结果并获取新的`state`、`reward`和`done`标志。保存`observation`、`action`、`reward`、`state_new`和`done`标志。调用代理的`learn`函数并将当前奖励添加到运行分数中。将新状态设置为观察，并最终，当`done`标志为`True`时，将`score`添加到`history`中：
- en: '[PRE219]'
  id: totrans-627
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE219]'
- en: You can print out `score` and mean `score_history` results to see how the agent
    is learning over time.
  id: totrans-628
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以打印出`score`和平均`score_history`结果，查看代理在一段时间内如何学习。
- en: Note
  id: totrans-629
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To observe the rewards, we can simply add the `print` statement. The rewards
    will be similar to those in the previous exercise.
  id: totrans-630
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要观察奖励，我们可以简单地添加 `print` 语句。奖励将与前面练习中的类似。
- en: Run the code for at least 1,000 iterations and watch your lander attempt to
    land on the lunar surface.
  id: totrans-631
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行代码至少 1,000 次，并观看你的着陆器尝试着陆在月球表面。
- en: Note
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To see the Lunar Lander simulation once the policy is learned, we just need
    to uncomment the `env.render()` code from the preceding code block. As seen in
    the previous exercise, this will open another window, where we will be able to
    see the game simulation.
  id: totrans-633
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要查看学习到策略后的月球着陆仿真，我们只需要取消注释前面代码块中的`env.render()`代码。如前面的练习所示，这将打开另一个窗口，我们可以在其中看到游戏仿真。
- en: 'Here''s a glimpse of how your lunar lander might behave once it has learned
    the policy:'
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是你创建的月球着陆器在学习策略后可能的行为表现：
- en: '![Figure 11.16: Screenshots from the environment after 1,000 rounds of training'
  id: totrans-635
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 11.16：经过 1,000 轮训练后的环境截图'
- en: '](img/B16182_11_16.jpg)'
  id: totrans-636
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_11_16.jpg)'
- en: 'Figure 11.16: Screenshots from the environment after 1,000 rounds of training'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.16：经过 1,000 轮训练后的环境截图
- en: Note
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/30X03Ul](https://packt.live/30X03Ul).
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/30X03Ul](https://packt.live/30X03Ul)。
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线互动示例，需要在本地运行。
- en: 'Activity 11.02: Loading the Saved Policy to Run the Lunar Lander Simulation'
  id: totrans-641
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 11.02：加载保存的策略以运行月球着陆仿真
- en: 'Import the essential Python libraries:'
  id: totrans-642
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的 Python 库：
- en: '[PRE220]'
  id: totrans-643
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE220]'
- en: 'Set your device using the `device` parameter:'
  id: totrans-644
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`device`参数设置你的设备：
- en: '[PRE221]'
  id: totrans-645
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE221]'
- en: 'Define the `ReplayBuffer` class, as we did in the previous exercise:'
  id: totrans-646
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `ReplayBuffer` 类，如我们在上一练习中所做的：
- en: '[PRE222]'
  id: totrans-647
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE222]'
- en: 'Define the `ActorCritic` class, as we did in the previous exercise:'
  id: totrans-648
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义 `ActorCritic` 类，如我们在前一个练习中所做的：
- en: '[PRE223]'
  id: totrans-649
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE223]'
- en: 'Define the `Agent` class, as we did in the previous exercise:'
  id: totrans-650
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`Agent`类，如我们在前面的练习中所做的：
- en: '[PRE224]'
  id: totrans-651
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE224]'
- en: 'Create the Lunar Lander environment. Initialize the random seed:'
  id: totrans-652
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建月球着陆器环境。初始化随机种子：
- en: '[PRE225]'
  id: totrans-653
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE225]'
- en: 'Create the memory buffer and initialize the agent with hyperparameters, as
    in the previous exercise:'
  id: totrans-654
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建内存缓冲区，并按照前面的练习初始化代理和超参数：
- en: '[PRE226]'
  id: totrans-655
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE226]'
- en: 'Load the saved policy as an old policy from the `Exercise11.03` folder:'
  id: totrans-656
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`Exercise11.03`文件夹加载保存的策略作为旧策略：
- en: '[PRE227]'
  id: totrans-657
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE227]'
- en: 'Finally, loop through your desired number of episodes. In every iteration,
    start by initializing the episode reward as `0`. Do not forget to reset the state.
    Run another loop, specifying the `max` timestamp. Get the `state`, `reward`, and
    `done` flags for each action taken and add the reward to the episode reward. Render
    the environment to see how your Lunar Lander is doing:'
  id: totrans-658
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，循环遍历你期望的回合数。在每次迭代中，首先将回合奖励初始化为`0`。不要忘记重置状态。再运行一个循环，指定`max`时间戳。获取每个动作的`state`、`reward`和`done`标志，并将奖励加到回合奖励中。渲染环境，查看你的月球着陆器的表现：
- en: '[PRE228]'
  id: totrans-659
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE228]'
- en: 'The following is the output of the code:'
  id: totrans-660
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是代码的输出：
- en: '[PRE229]'
  id: totrans-661
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE229]'
- en: 'You''ll see the reward oscillate in the positive zone as our Lunar Lander now
    has some idea of what a good policy can be. The reward may oscillate as there
    is more scope for learning. You might iterate over a few thousand more iterations
    to make your agent learn a better policy. Do not hesitate to tinker with the parameters
    specified in the code. The following screenshot shows the simulation output of
    some of the stages:'
  id: totrans-662
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你会看到奖励在正区间波动，因为我们的月球着陆器现在对什么是好的策略有了一些了解。由于学习的空间较大，奖励可能会波动。你可能需要再迭代几千次，以便让代理学习出更好的策略。不要犹豫，随时调整代码中指定的参数。以下截图展示了某些阶段的仿真输出：
- en: '![Figure 11.17: The environment showing the simulation of the Lunar Lander'
  id: totrans-663
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 11.17：显示月球着陆器仿真的环境'
- en: '](img/B16182_11_17.jpg)'
  id: totrans-664
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_11_17.jpg)'
- en: 'Figure 11.17: The environment showing the simulation of the Lunar Lander'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.17：显示月球着陆器仿真的环境
- en: Before this activity, we explained some necessary concepts, such as creating
    a learning agent, training a policy, saving and loading the learned policies,
    and so on, in isolation. Through carrying out this activity, you learned how to
    build a complete RL project or a working prototype on your own by combining all
    that you have learned in this chapter.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项活动之前，我们单独解释了一些必要的概念，例如创建学习代理、训练策略、保存和加载已学习的策略等等。通过进行这项活动，你学会了如何通过结合本章所学的内容，自己构建一个完整的RL项目或工作原型。
- en: Note
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The complete simulation output can be found in the form of images at [https://packt.live/3ehPaAj](https://packt.live/3ehPaAj).
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的仿真输出可以通过[https://packt.live/3ehPaAj](https://packt.live/3ehPaAj)以图像形式查看。
- en: To access the source code for this specific section, please refer to [https://packt.live/2YhzrvD](https://packt.live/2YhzrvD).
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此部分的源代码，请参考[https://packt.live/2YhzrvD](https://packt.live/2YhzrvD)。
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 本节当前没有在线互动示例，需要在本地运行。
- en: 12\. Evolutionary Strategies for RL
  id: totrans-671
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12. 强化学习的进化策略
- en: 'Activity 12.01: Cart-Pole Activity'
  id: totrans-672
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 12.01：倒立摆活动
- en: 'Import the required packages as follows:'
  id: totrans-673
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的包，代码如下：
- en: '[PRE230]'
  id: totrans-674
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE230]'
- en: 'Initialize the environment and the state and action space shapes:'
  id: totrans-675
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化环境、状态和动作空间形状：
- en: '[PRE231]'
  id: totrans-676
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE231]'
- en: 'Create a function to generate randomly selected initial network parameters:'
  id: totrans-677
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，用于生成随机选择的初始网络参数：
- en: '[PRE232]'
  id: totrans-678
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE232]'
- en: 'Create a function to generate the neural network using the set of parameters:'
  id: totrans-679
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，使用参数集生成神经网络：
- en: '[PRE233]'
  id: totrans-680
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE233]'
- en: 'Create a function to get the total reward for `300` steps when using the neural
    network:'
  id: totrans-681
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，在使用神经网络时获取`300`步的总奖励：
- en: '[PRE234]'
  id: totrans-682
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE234]'
- en: 'Create a function to get the fitness scores for each element of the population
    when running the initial random selection:'
  id: totrans-683
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，在执行初始随机选择时获取种群中每个元素的适应度评分：
- en: '[PRE235]'
  id: totrans-684
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE235]'
- en: 'Create a mutation function:'
  id: totrans-685
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个变异函数：
- en: '[PRE236]'
  id: totrans-686
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE236]'
- en: 'Create a single-point crossover function:'
  id: totrans-687
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个单点交叉函数：
- en: '[PRE237]'
  id: totrans-688
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE237]'
- en: 'Create a function for creating the next generation by selecting the pair with
    the highest rewards:'
  id: totrans-689
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，通过选择奖励最高的一对来创建下一代：
- en: '[PRE238]'
  id: totrans-690
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE238]'
- en: 'Get the current parameters for the weights and bias using a `for` loop to go
    through the indices:'
  id: totrans-691
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`for`循环遍历索引，获取权重和偏置的当前参数：
- en: '[PRE239]'
  id: totrans-692
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE239]'
- en: 'Build the neural network using the identified parameters and obtain a new reward
    based on the constructed neural network:'
  id: totrans-693
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用已识别的参数构建神经网络，并基于构建的神经网络获得新的奖励：
- en: '[PRE240]'
  id: totrans-694
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE240]'
- en: 'Create a function to output the convergence graph:'
  id: totrans-695
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数来输出收敛图：
- en: '[PRE241]'
  id: totrans-696
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE241]'
- en: 'Create a function for the genetic algorithm that outputs the parameters of
    the neural network based on the highest average reward:'
  id: totrans-697
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个用于遗传算法的函数，根据最高平均奖励输出神经网络的参数：
- en: '[PRE242]'
  id: totrans-698
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE242]'
- en: 'Create a function that decodes the array of parameters to each neural network
    parameter:'
  id: totrans-699
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，用于解码参数数组到每个神经网络参数：
- en: '[PRE243]'
  id: totrans-700
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE243]'
- en: 'Set the generations to `50`, the number of trial tests to `15`, and the number
    of steps and trials to `500`:'
  id: totrans-701
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将代数设置为`50`，试验次数设置为`15`，步数和试验数设置为`500`：
- en: '[PRE244]'
  id: totrans-702
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE244]'
- en: 'The output (just the first few lines are shown here) will be similar to the
    following:'
  id: totrans-703
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出（这里只显示了前几行）将类似于以下内容：
- en: '[PRE245]'
  id: totrans-704
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE245]'
- en: 'The output can be visualized in a plot as follows:'
  id: totrans-705
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出可以通过如下图形进行可视化：
- en: '![Figure 12.15: Rewards obtained over the generations'
  id: totrans-706
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图12.15：在各代中获得的奖励'
- en: '](img/B16182_12_15.jpg)'
  id: totrans-707
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16182_12_15.jpg)'
- en: 'Figure 12.15: Rewards obtained over the generations'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.15：在各代中获得的奖励
- en: 'The average of the rewards output (just the last few lines are shown here)
    will be similar to the following:'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的奖励平均值（这里只显示了最后几行）将类似于以下内容：
- en: '[PRE246]'
  id: totrans-710
  prefs: []
  type: TYPE_PRE
  zh: '[PRE246]'
- en: You will notice that depending on the start state, the convergence of the GA
    algorithm to the highest score will vary; also, the neural network model will
    not always achieve the optimal solution. The purpose of this activity was for
    you to implement the genetic algorithm techniques studied in this chapter and
    to see how you can combine evolutionary methods of neural network parameter tuning
    for action selection.
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，根据初始状态的不同，GA算法收敛到最高分数的速度会有所不同；另外，神经网络模型并不总是能够达到最优解。这个活动的目的是让你实现本章学习的遗传算法技术，并观察如何结合神经网络参数调优的进化方法来进行动作选择。
- en: Note
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2AmKR8m](https://packt.live/2AmKR8m).
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问该特定部分的源代码，请参考[https://packt.live/2AmKR8m](https://packt.live/2AmKR8m)。
- en: This section does not currently have an online interactive example and will
    need to be run locally.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 本节目前没有在线交互示例，需要在本地运行。
