- en: Classifying Clothing Images using Capsule Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用胶囊网络分类服装图像
- en: In this chapter, we will learn how to implement capsule networks on the Fashion
    MNIST dataset. This chapter will cover the inner workings of capsule networks
    and explain how to implement them in TensorFlow. You will also learn how to evaluate
    and optimize the model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何在Fashion MNIST数据集上实现胶囊网络。本章将涵盖胶囊网络的内部工作原理，并解释如何在TensorFlow中实现它们。你还将学习如何评估和优化模型。
- en: We have chosen capsule networks because they have the ability to preserve the
    spatial relationships of images. Capsule networks were introduced by Geoff Hinton,
    et al. They published a paper in 2017 that can be found at [https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829).
    Capsule networks gained immense popularity within the deep learning community
    as a new type of neural network.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择胶囊网络是因为它们具有保留图像空间关系的能力。胶囊网络由Geoff Hinton等人提出。他们在2017年发表了一篇论文，可以在[https://arxiv.org/abs/1710.09829](https://arxiv.org/abs/1710.09829)找到。胶囊网络作为一种新的神经网络类型，在深度学习社区中获得了极大的关注。
- en: 'By the end of this chapter, we will be able to classify clothing using capsule
    networks after going through the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将能够通过以下内容使用胶囊网络进行服装分类：
- en: Understanding the importance of capsule networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解胶囊网络的重要性
- en: A brief understanding of capsules
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对胶囊的简要理解
- en: The routing by agreement algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过协议路由算法
- en: The implementation of the CapsNet architecture for classifying Fashion-MNIST
    images
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现CapsNet架构来分类Fashion-MNIST图像
- en: The limitations of capsule networks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胶囊网络的局限性
- en: Understanding the importance of capsule networks
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解胶囊网络的重要性
- en: '**Convolutional neural networks** (**CNNs**) form the backbone of all the major
    breakthroughs in image detection today. CNNs work by detecting the basic features
    that are present in the lower layers of the network and then proceed to detect
    the higher level features present in the higher layers of the network. This setup
    does not contain a pose (translational and rotational) relationship between the
    lower-level features that make up any complex object.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）构成了当前所有图像检测重大突破的基础。CNN通过检测网络低层中存在的基本特征，然后继续检测网络高层中存在的更高级特征来工作。这种设置没有包含低级特征之间的姿势（平移和旋转）关系，而这些低级特征构成了任何复杂物体。'
- en: Imagine trying to identify a face. In this case, just having eyes, nose, and
    ears in an image can lead a CNN to conclude that it's a face without caring about
    the relative orientation of the concerned objects. To explain this further, if
    an image has a nose above the eyes, CNNs still can detect that it's an image.
    CNNs take care of this problem by using *max pooling*, which helps increase the
    *field of view* for the higher layers. However, this operation is not a perfect
    solution as we tend to lose valuable information in the image by using it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下试图识别一张面孔。在这种情况下，仅仅有眼睛、鼻子和耳朵在图像中，可能会导致CNN认为它是面孔，而不关心相关物体的相对方向。为了进一步解释这一点，如果图像中鼻子在眼睛上方，CNN仍然可以检测出它是图像。CNN通过使用*最大池化*来解决这个问题，帮助提高更高层的*视野范围*。然而，这个操作并不是完美的解决方案，因为我们使用它时，往往会丢失图像中的有价值信息。
- en: 'As a matter of fact, Hinton himself states the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，Hinton本人这样说：
- en: '"The pooling operation used in convolutional neural networks is a big mistake
    and the fact that it works so well is a disaster."'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: “卷积神经网络中使用的池化操作是一个大错误，然而它之所以能如此有效，正是一个灾难。”
- en: In the paper, Hinton tries to provide an intuition on solving this problem using
    the inverse graphics approach. For graphics in computers, an image is constructed
    by using an internal representation of the objects present in the image. This
    is done using arrays and matrices. This internal representation helps preserve
    the shape, the orientation, and the object's relative position when compared to
    all other objects in the image. The software takes this internal representation
    and publishes the image on the screen using a process known as **rendering**.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中，Hinton试图通过逆图形方法提供解决该问题的直觉。对于计算机图形学，图像是通过使用图像中物体的内部表示来构建的。这是通过数组和矩阵来完成的。这种内部表示有助于保留形状、方向和物体之间的相对位置，与图像中的所有其他物体进行比较。软件采用这种内部表示，并通过称为**渲染**的过程将图像发布到屏幕上。
- en: Hinton specifies that the human brain does some sort of inverse graphics. We
    see an image through our eyes, and then brain our dissects the image and constructs
    a hierarchical representation of different objects in the image before trying
    to match them to the existing patterns that we have seen. An interesting observation
    to note is that humans can identify objects in an image, irrespective of their
    viewing angle.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 休·辛顿指出，人脑进行了一种逆向图形处理。我们通过眼睛看到图像，然后大脑会分析这些图像，并构建出图像中不同对象的层次化表示，随后将其与我们曾经见过的模式进行匹配。一个有趣的观察是，人类可以识别图像中的对象，而不论它们的视角如何。
- en: He then proceeds to argue that in order to perform classification, it is necessary
    to preserve the relative orientation and position of different objects in the
    image (this helps mimic the human capability, as we discussed previously). It's
    quite intuitive that once we have these relationships built into the representation
    using the model, it is very easy for a model to detect an object when it views
    it from various angles. Let's imagine the case of viewing the Taj Mahal (the famous
    monument in India). Our eyes can identify the Taj Mahal from various angles. However,
    if you present the same images to a CNN, it might fail to detect the Taj Mahal
    from different viewpoints. This is because CNNs don't have an understanding of
    3D space like our brains do. This is the reason capsule network theory is quite
    important.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 他接着提出，为了进行分类，必须保留图像中不同对象的相对方向和位置（这有助于模拟我们之前讨论过的人类能力）。直观地讲，一旦我们在模型中构建了这些关系，模型就能非常容易地检测到从不同角度看到的对象。让我们设想一下观看泰姬陵（印度的著名建筑）的情况。我们的眼睛可以从不同角度识别泰姬陵。然而，如果你把相同的图像输入到CNN中，它可能无法从不同视角识别泰姬陵。这是因为CNN不像我们的大脑那样理解3D空间。这也是胶囊网络理论如此重要的原因。
- en: One of the major concerns here is this: *How do we incorporate these hierarchical
    relationships into the deep neural networks?* The relationship between different
    objects in an image is modeled by something called a **pose**, which is basically
    rotation and translation. This is specific to the graphics of a computer. We will
    look at how these relationships are modeled in capsule networks in the subsequent
    sections.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个主要问题是：*我们如何将这些层级关系纳入深度神经网络中？* 图像中不同对象之间的关系通过一种叫做**姿势（pose）**的方式来建模，基本上是旋转和平移的结合。这是计算机图形学中的特定概念。在接下来的章节中，我们将讨论这些关系如何在胶囊网络中被建模。
- en: Understanding capsules
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解胶囊网络
- en: In traditional CNNs, we define different filters that run over the entire image.
    The 2D matrices produced by each filter are stacked on top of one another to constitute
    the output of a convolutional layer. Subsequently, we perform the max pooling
    operation to find the invariance in activities. Invariance here implies that the
    output is robust to small changes in the input as the max pooling operation always
    picks up the max activity. As mentioned previously, max pooling results in the
    valuable loss of information and is unable to represent the relative orientation
    of different objects to others in the image.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的卷积神经网络（CNN）中，我们定义了不同的滤波器，它们会遍历整个图像。每个滤波器生成的2D矩阵会堆叠在一起，形成卷积层的输出。接着，我们执行最大池化操作，以找到活动的变换不变性。这里的变换不变性意味着输出对输入的小变化具有鲁棒性，因为最大池化操作总是选择最大活动。正如前面所提到的，最大池化会导致有价值的信息丢失，并且无法表示图像中不同对象之间的相对方向。
- en: 'Capsules, on the other hand, encode all of the information of the objects they
    are detecting in a vector form as opposed to a scalar output by a neuron. These
    vectors have the following properties:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，胶囊网络以向量形式编码它们检测到的对象的所有信息，而不是通过神经元的标量输出。这些向量具有以下属性：
- en: The length of the vector indicates the probability of an object in the image.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量的长度表示图像中对象的概率。
- en: Different elements of the vector encode different properties of the object.
    These properties include various kinds of instantiation parameters such as pose
    (position, size, orientation), hue, thickness, and so on.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向量的不同元素编码了对象的不同属性。这些属性包括各种实例化参数，如姿势（位置、大小、方向）、色调、厚度等。
- en: With this representation, if the detected object moves in the image, the length
    of the vector remains the same. However, the orientation or values of different
    elements in the vector representation will change. Let's take the previous example
    of viewing the Taj Mahal again. Even if we were to move (or change the orientation)
    of the Taj Mahal in the image, the capsule representation should be able to detect
    the object in the image.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种表示方式，如果图像中的物体发生了移动，向量的长度将保持不变。然而，向量表示中不同元素的方向或数值将发生变化。我们再来看之前的泰姬陵的例子。即使我们在图像中移动（或改变方向）泰姬陵，胶囊表示应该仍能在图像中检测到这个物体。
- en: How do capsules work?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 胶囊是如何工作的？
- en: Before looking at how capsules work, let's try to revisit how neurons function.
    A neuron receives scalar inputs from the previous layer's neurons, multiplies
    them by the corresponding weights, and sums the outputs. This summed output is
    passed through some non-linearity (such as ReLU) that outputs a new scalar, which
    is passed on to next layer's neurons.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解胶囊如何工作的之前，让我们先回顾一下神经元是如何工作的。一个神经元从前一层的神经元接收标量输入，将它们与相应的权重相乘，并将输出求和。这个求和后的输出通过某种非线性函数（如ReLU）进行处理，输出一个新的标量，然后传递给下一层的神经元。
- en: 'In contrast to this, capsules take a vector as an input, as well as output
    a vector. The following diagram illustrates the process of computing the output
    of a capsule:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相反，胶囊以向量作为输入，同时也输出一个向量。以下图示展示了计算胶囊输出的过程：
- en: '![](img/efb3f69b-0e10-4527-a848-796048ff6eba.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/efb3f69b-0e10-4527-a848-796048ff6eba.png)'
- en: 'Let''s look at each step in detail:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看每一步：
- en: '**Capsule j** (at the higher levels) receives the vector inputs from the lower
    layers as **u**[**1**, ]**u**[**2**, ]**u**[**3**, ]and so on. As discussed earlier,
    each input vector encodes both the probability of an object detected at the lower
    layer and also its orientation parameters. These input vectors are multiplied
    by weight matrices, **W[ij]**, which try to model the relationship between lower
    layer objects and higher layer objects. In the case of detecting the Taj Mahal,
    you can think of this as a relationship between edges that are detected at the
    lower layers and the pillars of the Taj Mahal at the higher layers. The output
    of this multiplication is the predicted vector of the higher level object (in
    this case, the pillar) based on the detected objects in the lower layer. Therefore, ![](img/f9addba5-b122-4fd1-ab7c-8335d149797e.png) denotes
    the position of a pillar of the Taj Mahal based on the detected vertical edge,
    ![](img/7b4dd575-4305-455f-93a0-7795e596a253.png) can denote the position of the
    pillar based on the detected horizontal edge, and so on. Intuitively, if all of
    the predicted vectors point to the same object with a similar orientation, then
    that object must be present in the image:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**胶囊 j**（在更高层）接收来自下层的向量输入，如**u**[**1**]、**u**[**2**]、**u**[**3**]，依此类推。如前所述，每个输入向量编码了在下层检测到的物体的概率以及其方向参数。这些输入向量与权重矩阵**W[ij]**相乘，后者试图建模下层物体和高层物体之间的关系。在检测泰姬陵的情况下，可以将其视为下层检测到的边缘与上层泰姬陵柱子之间的关系。这个乘法运算的结果是基于下层检测到的物体来预测高层物体（在此为柱子）的向量。因此，![](img/f9addba5-b122-4fd1-ab7c-8335d149797e.png)
    表示基于检测到的垂直边缘，泰姬陵柱子的位置，![](img/7b4dd575-4305-455f-93a0-7795e596a253.png) 则可以表示基于检测到的水平边缘的柱子位置，依此类推。直观地说，如果所有预测的向量都指向同一个物体且具有相似的方向，那么该物体必定存在于图像中：'
- en: '![](img/76ec11e0-dab3-4c7f-8f7e-94b3ab7853b7.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76ec11e0-dab3-4c7f-8f7e-94b3ab7853b7.png)'
- en: 'Next, these predicted vectors are multiplied by scalar weights (**c[i]**s),
    which help in routing the predicted vectors to the right capsules in the higher
    layer. We then sum the weighted vectors that are obtained through this multiplication.
    This step will feel familiar to traditional neural networks, which multiply the
    scalar inputs by weights before providing them to the input of higher-level neurons.
    In such cases, the weights are determined by a back propagation algorithm. However,
    in the case of capsule networks, they are determined by the dynamic routing algorithm,
    which we will discuss in detail in the next section. The formula is given as follows:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，这些预测向量将与标量权重（**c[i]**s）相乘，这有助于将预测向量路由到上层正确的胶囊。然后，我们将通过此乘法得到的加权向量求和。这一步与传统神经网络中的操作很相似，后者会将标量输入与权重相乘，再将它们提供给更高层次的神经元。在这些情况下，权重是通过反向传播算法来确定的。然而，在胶囊网络中，权重是通过动态路由算法来确定的，我们将在下一节中详细讨论。公式如下：
- en: '![](img/3ef7fa87-24ea-4fb5-9316-0bb69f379088.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ef7fa87-24ea-4fb5-9316-0bb69f379088.png)'
- en: 'We mentioned a new word in the previous formula, known as **squashing**. This
    is the non-linearity that is used in capsule networks. You can think of this as
    a counterpart to the non-linearity we use in traditional neural networks. Essentially,
    squashing tries to reduce the vector to less than a unit norm to facilitate the
    interpretation of the length of the vector as probability:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前面的公式中，我们提到了一个新词，叫做**压缩**。这是胶囊网络中使用的非线性函数。你可以把它看作是传统神经网络中使用的非线性的对应物。从本质上讲，压缩试图将向量缩小到小于单位范数的值，以便将向量的长度解释为概率：
- en: '![](img/9270c789-f218-4f63-a002-18f28ea3935b.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9270c789-f218-4f63-a002-18f28ea3935b.png)'
- en: Here, **v[j]**is the output of the **j** layer capsule.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，**v[j]**是**j**层胶囊的输出。
- en: 'The implementation of the squashing function in the code is done as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩函数在代码中的实现如下：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The dynamic routing algorithm
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 动态路由算法
- en: As mentioned earlier, it is necessary for the capsule in the lower layer to
    decide how to send its output to the higher-level capsules. This is achieved through
    the novel concept of the dynamic routing algorithm, which was introduced in the paper ([https://arxiv.org/pdf/1710.09829.pdf](https://arxiv.org/pdf/1710.09829.pdf)).
    The key idea behind this algorithm is that the lower layer capsule will send their
    output to the higher-level capsules that *match* the input.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，下层胶囊需要决定如何将其输出发送到上层胶囊。这是通过动态路由算法这一新颖的概念来实现的，该算法在论文中被提出（[https://arxiv.org/pdf/1710.09829.pdf](https://arxiv.org/pdf/1710.09829.pdf)）。该算法的关键思想是，下层胶囊将其输出发送到与输入*匹配*的上层胶囊。
- en: 'This is achieved through the weights (**c[ij]**) mentioned in the last section.
    These weights multiply the outputs from the lower layer capsule **i** before pushing
    them as the input to the higher level capsule **j**. Some of the properties of
    these weights are as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过上一部分提到的权重（**c[ij]**）实现的。这些权重在将来自下层胶囊**i**的输出作为输入传递给上层胶囊**j**之前，会先将它们相乘。以下是这些权重的一些特性：
- en: '**c[ij]**s are non-negative in nature and are determined by the dynamic-routing
    algorithm'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**c[ij]**是非负的，并且由动态路由算法决定'
- en: The number of weights in the lower layer capsule is equal to the number of higher-level
    capsules
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下层胶囊中的权重数量等于上层胶囊的数量
- en: The sum of the weights of each lower layer capsule **i** amounts to 1
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个下层胶囊**i**的权重之和为1
- en: 'Implement the iterative routing algorithm using the following code:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码实现迭代路由算法：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that, in the previous code, we are dividing the actual routing function
    in the code into two parts so that we can focus on the dynamic routing algorithm part. The
    first part of the function takes vector **u **as input from the lower layer capsule(s).
    First, it generates the vector ![](img/e99506b5-2cc6-4303-97e8-a8458ab3a538.png) using
    the weight vector **W**. Also, observe that we define a temporary variable called ![](img/b8ffc378-2e7a-43e7-8fe4-366e59603847.png),
    which is initialized to zero at the start of training. The values of ![](img/789003d1-8295-41a1-873d-997492b18576.png) will
    be updated in the algorithm and will be stored in **c[ij ]**at the end of the
    algorithm. The second part of the function implements the actual iterative routing
    algorithm, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的代码中，我们将实际的路由函数分成了两部分，以便我们能够专注于动态路由算法部分。该函数的第一部分接收来自下层胶囊的向量**u**作为输入。首先，它使用权重向量**W**生成向量 ![](img/e99506b5-2cc6-4303-97e8-a8458ab3a538.png)。此外，注意我们定义了一个名为 ![](img/b8ffc378-2e7a-43e7-8fe4-366e59603847.png)的临时变量，初始值为零，直到训练开始时。 ![](img/789003d1-8295-41a1-873d-997492b18576.png)的值将在算法中更新，并最终存储在**c[ij]**中。该函数的第二部分实现了实际的迭代路由算法，具体如下：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: First, we define a loop over `ROUTING_ITERATIONS`. This is the parameter that
    is defined by the user. Hinton mentions in his paper that the typical values of
    `3` should suffice for this.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一个`ROUTING_ITERATIONS`的循环。这个参数由用户定义。Hinton在他的论文中提到，典型的值`3`应该足够了。
- en: Next, we perform a softmax on ![](img/21f4dcdf-7490-4e0f-b9a0-0b272bdcb52d.png) to
    compute the initial values of the **c[ij]**s. Note that **c[ij]**s are not to
    be included in the back propagation since these can only be obtained through the
    iterative algorithm. For this reason, all of the routing iterations before the
    last one are performed on ![](img/def7cb9b-d181-4391-b933-e6f0086112a2.png) (which
    helps to stop gradients, as defined earlier).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对 ![](img/21f4dcdf-7490-4e0f-b9a0-0b272bdcb52d.png) 进行 softmax 操作，以计算**c[ij]**的初始值。请注意，**c[ij]**不包含在反向传播中，因为这些值只能通过迭代算法获得。因此，在最后一次迭代之前的所有路由迭代都将在 ![](img/def7cb9b-d181-4391-b933-e6f0086112a2.png) 上进行（这有助于停止梯度，正如之前定义的那样）。
- en: 'For each routing iteration, we use the following operations for each higher-level
    capsule **j**:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次路由迭代，我们对每个上层胶囊**j**执行以下操作：
- en: '![](img/e2a65403-0c31-4567-904f-d450bce8425b.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2a65403-0c31-4567-904f-d450bce8425b.png)'
- en: '![](img/facf7cb8-585c-40b8-a5ad-b69d902e31ba.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/facf7cb8-585c-40b8-a5ad-b69d902e31ba.png)'
- en: '![](img/59a19cee-4b0a-4be8-abaa-18cfa02582aa.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59a19cee-4b0a-4be8-abaa-18cfa02582aa.png)'
- en: We have explained the first two equations already. Now let's try to understand
    the third equation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经解释了前两个方程。现在让我们试着理解第三个方程。
- en: 'The third equation is the essence of the iterative routing algorithm. It updates
    the weights ![](img/531663b5-de20-45e0-bc3a-98af928f53d9.png). The formula states
    that the new weight value is the sum of the old weights: the predicted vector
    from lower layer capsules and the output of the higher layer capsule. The dot
    product is essentially trying to capture the notion of similarity between the
    input vector and the output vector of the capsule. This way, the output from the
    lower capsule **i **is only sent to the higher-level capsule **j**, which agrees
    to its input. The dot product achieves the agreement. This algorithm is repeated
    a number of times equal to the `ROUTING_ITERATIONS` parameter in the code.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个方程是迭代路由算法的核心。它更新权重 ![](img/531663b5-de20-45e0-bc3a-98af928f53d9.png)。公式表示新权重值是旧权重的和：来自下层胶囊的预测向量和来自上层胶囊的输出。点积本质上是尝试捕捉输入向量和胶囊输出向量之间的相似性。通过这种方式，来自下层胶囊**i**的输出只会发送给与其输入一致的上层胶囊**j**。点积实现了这种一致性。这个算法会重复执行与代码中的`ROUTING_ITERATIONS`参数相等的次数。
- en: This concludes our discussion on the innovative routing algorithm and its applications.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们对创新性路由算法及其应用的讨论的结束。
- en: CapsNet for classifying Fashion MNIST images
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CapsNet 用于分类 Fashion MNIST 图像
- en: 'Now let''s take a look at the implementation of CapsNet for classifying Fashion
    MNIST images. **Zalando**, the e-commerce company, recently released a new replacement
    for the MNIST dataset, known as **Fashion MNIST** ([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)).
    The Fashion MNIST dataset includes 28 x 28 grayscale images under 10 categories:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下CapsNet在Fashion MNIST图像分类中的实现。**Zalando**，这家电子商务公司，最近发布了一个新的MNIST数据集替代品，名为**Fashion
    MNIST**（[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)）。Fashion
    MNIST数据集包含10类28 x 28灰度图像：
- en: '| **Category name** | **Label (in dataset)** |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| **类别名称** | **标签（数据集中的值）** |'
- en: '| T-shirt/top | 0 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| T-shirt/top | 0 |'
- en: '| Trouser | 1 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Trouser | 1 |'
- en: '| Pullover | 2 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Pullover | 2 |'
- en: '| Dress | 3 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Dress | 3 |'
- en: '| Coat | 4 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Coat | 4 |'
- en: '| Sandal | 5 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Sandal | 5 |'
- en: '| Shirt | 6 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Shirt | 6 |'
- en: '| Sneaker | 7 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Sneaker | 7 |'
- en: '| Bag | 8 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Bag | 8 |'
- en: '| Ankle boot | 9 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Ankle boot | 9 |'
- en: 'The following are some sample images from the dataset:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是数据集中的一些示例图片：
- en: '![](img/9d0d1cb9-d86b-4668-8d17-8f00a05bbff0.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d0d1cb9-d86b-4668-8d17-8f00a05bbff0.png)'
- en: The training set contains 60K examples, and the test set contains 10K examples.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集包含60K个示例，测试集包含10K个示例。
- en: CapsNet implementation
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CapsNet实现
- en: 'The CapsNet architecture consists of two parts, each consisting of three layers.
    The first three layers are encoders, while the next three layers are decoders:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: CapsNet架构由两部分组成，每部分包含三层。前三层是编码器，接下来的三层是解码器：
- en: '| **Layer Num** | **Layer Name ** | **Layer Type** |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| **层编号** | **层名称** | **层类型** |'
- en: '| 1 | Convolutional Layer | Encoder |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 卷积层 | 编码器 |'
- en: '| 2 | PrimaryCaps Layer | Encoder |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 初级胶囊层 | 编码器 |'
- en: '| 3 | DigitCaps Layer | Encoder |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 数字胶囊层 | 编码器 |'
- en: '| 4 | Fully Connected Layer 1 | Decoder |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 全连接层1 | 解码器 |'
- en: '| 5 | Fully Connected Layer 2 | Decoder |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 全连接层2 | 解码器 |'
- en: '| 6 | Fully Connecter Layer 3 | Decoder |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 全连接层3 | 解码器 |'
- en: Let's try to understand these layers in detail.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试详细理解这些层。
- en: Understanding the encoder
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解编码器
- en: 'The following diagram illustrates the structure of the encoder used for modeling.
    Note that it shows the MNIST digit image as an input, but we are using the Fashion-MNIST
    data as an input to the model:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了用于建模的编码器结构。请注意，它显示的是MNIST数字图像作为输入，但我们使用的是Fashion-MNIST数据集作为模型的输入：
- en: '![](img/d86e413b-dce9-47e9-b9a0-f8feca24cfff.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d86e413b-dce9-47e9-b9a0-f8feca24cfff.png)'
- en: The encoder essentially takes an input of a 28x28 image and produces a 16-dimensional
    representation of that image. As mentioned previously, the length of the 16D vector
    denotes the probability that an object is present in the image. The components
    of the vector represent various instantiation parameters.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器本质上接受一个28x28的图像输入，并生成该图像的16维表示。如前所述，16维向量的长度表示图像中是否存在某个物体的概率。向量的各个组成部分表示不同的实例化参数。
- en: 'The three layers dedicated to the encoder are as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 专门用于编码器的三层如下：
- en: '**Layer 1-convolutional layer**: Layer 1 is a standard convolutional layer.
    The input to this layer is a 28x28 grayscale image and the output is a 20x20x256
    tensor. The other parameters of this layer are as follows:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层1-卷积层**：层1是一个标准的卷积层。该层的输入是一个28x28的灰度图像，输出是一个20x20x256的张量。该层的其他参数如下：'
- en: '| Parameter name | Value |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 参数名称 | 值 |'
- en: '| Filters | 256 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 过滤器 | 256 |'
- en: '| Kernel Size  | 9 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 卷积核大小 | 9 |'
- en: '| Activation | ReLU |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Activation | ReLU |'
- en: '| Strides | 1 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Strides | 1 |'
- en: '**Layer 2-primary caps layer**: Layer 2 is the first layer with capsules. The
    main purpose of this layer is to use the output of the first convolutional layer
    to produce higher level features. It has 32 primary capsules. It also takes an
    input of a 20 x 20 x 256 tensor. Every capsule present in this layer applies the
    convolutional kernels to the input to produce an output of a 6 x 6 x 8 tensor.
    With 32 capsules, this output is now a 6 x 6 x 8 x 32 tensor.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层2-初级胶囊层**：层2是第一个包含胶囊的层。该层的主要目的是利用第一层卷积层的输出生成更高层次的特征。它有32个初级胶囊。它也接受一个20 x
    20 x 256的张量作为输入。该层中的每个胶囊都将卷积核应用于输入，生成一个6 x 6 x 8的张量输出。通过32个胶囊，输出现在是一个6 x 6 x 8
    x 32的张量。'
- en: 'The convolutional parameters that are common for all capsules in the layer
    are mentioned as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本层中所有胶囊共享的卷积参数如下所示：
- en: '| **Parameter Name** | **Value** |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **参数名称** | **值** |'
- en: '| Filters | 256 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 过滤器 | 256 |'
- en: '| Kernel Size | 9 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Kernel Size | 9 |'
- en: '| Activation | ReLU |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 激活函数 | ReLU |'
- en: '| Strides | 2 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Strides | 2 |'
- en: Note that we also `squash`the output of this layer.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还会对该层的输出进行`squash`处理。
- en: '**Layer 3-DigitCaps layer**: This layer has 10 capsules – one for each class
    label. Each capsule is a 16D vector. The input to this layer are 6x6x32 8D vectors
    (**u**, as we defined previously). Each of these vectors have their own weight
    matrix, ![](img/bb7634a1-9cfe-4935-975a-c356f2a634f4.png), which produces ![](img/dc96a72e-28a1-4c91-a972-aac9c5e4142c.png).
    These ![](img/dedba9da-b209-4ec0-b93b-9eb5d7672693.png) are then used in the routing
    by the agreement algorithm that we described previously.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层 3-DigitCaps 层**：此层有10个胶囊——每个类标签对应一个胶囊。每个胶囊是一个16维的向量。此层的输入是6x6x32的8维向量（**u**，如前所述）。每个向量都有自己的权重矩阵，！[](img/bb7634a1-9cfe-4935-975a-c356f2a634f4.png)，它产生！[](img/dc96a72e-28a1-4c91-a972-aac9c5e4142c.png)。这些！[](img/dedba9da-b209-4ec0-b93b-9eb5d7672693.png)然后在我们之前描述的路由协议中由一致性算法使用。'
- en: Note that the original paper names this layer as the DigitCaps layer because
    it uses the MNIST dataset. We are continuing to use the same name for the Fashion
    MNIST dataset, as it is easier to relate to the original paper.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，原始论文将此层命名为DigitCaps层，因为它使用了MNIST数据集。我们继续使用相同的名称来适应Fashion MNIST数据集，因为这与原始论文更容易对应。
- en: Understanding the decoder
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解解码器
- en: 'The structure of the decoder is shown in the following diagram:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的结构如下面的图所示：
- en: '![](img/b0cc93a8-ad5a-44fb-89a0-8c77c2de32b1.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0cc93a8-ad5a-44fb-89a0-8c77c2de32b1.png)'
- en: The decoder essentially tries to reconstruct the image from the correct DigitCaps
    capsule for each image. You can view this as a regularization step, with *loss*
    being the Euclidean distance between the predicted output and the original label.
    You could argue that you don't require reconstruction in this application as you
    are just carrying out classification. However, Hinton specifically shows in his
    original paper that adding reconstruction loss does improve the accuracy of the
    model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器本质上尝试从每个图像的正确DigitCaps胶囊中重建图像。你可以将其视为一个正则化步骤，*损失*是预测输出与原始标签之间的欧几里得距离。你可以争辩说，在这个应用中不需要重建，因为你只是进行分类。然而，Hinton在他的原始论文中明确指出，添加重建损失确实提高了模型的准确性。
- en: 'The decoder''s structure is pretty simple, and consists of only three fully
    connected layers. The input and the output shapes of all three layers are as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的结构非常简单，只由三个全连接层组成。三个层的输入和输出形状如下：
- en: '| **Layer** | **Input Shape** | **Output Shape** |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **层** | **输入形状** | **输出形状** |'
- en: '| Fully Connected Layer 4  | 16 x 10 | 512 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 全连接层 4 | 16 x 10 | 512 |'
- en: '| Fully Connected Layer 5  | 512 | 1,024 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 全连接层 5 | 512 | 1,024 |'
- en: '| Fully Connected Layer 6 | 1,024 | 784 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 全连接层 6 | 1,024 | 784 |'
- en: However, before passing the input to the three fully connected layers, during
    training, we mask all but the activity vector of the correct digit capsule. Since
    we don't have the correct labels during testing, we pass the activity vector with
    the highest norm to the fully connected layers.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在将输入传递给三个全连接层之前，在训练过程中，我们会掩蔽除正确数字胶囊的活动向量外的所有向量。由于在测试过程中我们没有正确的标签，我们会将活动向量的最大范数传递给全连接层。
- en: Defining the loss function
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义损失函数
- en: 'The loss function for capsule networks is composed of two parts:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 胶囊网络的损失函数由两部分组成：
- en: '**Margin loss**: Margin loss is exactly the same as what''s used in **Support
    Vector Machines** (**SVM**). Effectively, we want the digit capsule to have an
    instantiation vector for class *k*, but only if the label is class *k*. For all
    other classes, we don''t require any instantiation parameters. For each digit
    capsule k, we define separate loss as ![](img/b555a2b5-ab4e-461a-822d-582948acdf34.png):'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边距损失**：边距损失与**支持向量机**（**SVM**）中使用的完全相同。实际上，我们希望数字胶囊对类*k*有一个实例化向量，但前提是标签是类*k*。对于所有其他类别，我们不需要任何实例化参数。对于每个数字胶囊k，我们定义单独的损失为！[](img/b555a2b5-ab4e-461a-822d-582948acdf34.png)：'
- en: '![](img/525e47e1-a86f-4a4c-8688-be37e9595e7c.png)'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/525e47e1-a86f-4a4c-8688-be37e9595e7c.png)'
- en: If an image belongs to class k, then ![](img/4bc5e47a-3971-4d9d-9e54-6ba9e79ea6a4.png) else
    0. ![](img/4ffb1d5c-d0ef-4d06-8d7a-10b3a6e0d1a7.png) are the other two parameters. ![](img/cf48ef4f-6a62-46d3-9e1b-614c7f7d1cf3.png) is
    used for stability when initial learning the model. The total margin loss is the
    sum of losses of all digit capsules.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图像属于类k，则！[](img/4bc5e47a-3971-4d9d-9e54-6ba9e79ea6a4.png)，否则为0。！[](img/4ffb1d5c-d0ef-4d06-8d7a-10b3a6e0d1a7.png)是其他两个参数。！[](img/cf48ef4f-6a62-46d3-9e1b-614c7f7d1cf3.png)在模型初始学习时用于稳定性。总的边距损失是所有数字胶囊损失的总和。
- en: To explain this simply, for digit caps *k* (which is the true label), the loss
    is zero if we predict a correct label with a probability of > 0.9; otherwise it
    is non-zero. For all other digit caps, the loss is zero if we predict the probability
    of all those classes to be less than 0.1; otherwise, it is non-zero.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，对于数字胶囊*k*（即真实标签），如果我们以大于0.9的概率预测出正确的标签，则损失为零；否则，损失为非零。对于所有其他数字胶囊，如果我们预测所有这些类别的概率都小于0.1，则损失为零；否则，损失为非零。
- en: '**Reconstruction loss**: Reconstruction loss is mainly used as a regularizer
    for the model so that we can focus on learning the representations to reproduce
    the image. Intuitively, this can also result in easing the learning of the instantiation
    parameters of the model. This is generated by taking the Euclidean distance between
    the pixels of the reconstructed image and the input image. The total loss for
    the model is given as follows:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重建损失**：重建损失主要用作模型的正则化器，以便我们能集中精力学习图像的表示，并重现图像。直观地说，这也有助于简化模型实例化参数的学习。它通过计算重建图像和输入图像像素之间的欧几里得距离来生成。模型的总损失如下所示：'
- en: '*Total loss = Margin loss + 0.0005 Reconstruction loss*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*总损失 = 边距损失 + 0.0005 重建损失*'
- en: Note that reconstruction loss is weighted down heavily to ensure that it doesn't
    dominate the margin loss during training.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，重建损失被大大加权，以确保它在训练过程中不会主导边距损失。
- en: Training and testing the model
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和测试模型
- en: 'The following are the steps for training and testing the model:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是训练和测试模型的步骤：
- en: 'The first step is to read the training and testing datasets. Here are steps
    we must implement for reading the data:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是读取训练和测试数据集。以下是我们必须执行的数据读取步骤：
- en: First, we load the training/testing images and label data from the files we
    downloaded for the **Fashion MNIST **data ([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)).
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们从下载的**Fashion MNIST**数据集中加载训练/测试图像和标签数据（[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)）。
- en: Then, we reshape the image data to a shape of 28 x 28 x 1 for our model and
    normalize it by 255 to keep the input of the model between 0 and 1.
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将图像数据重塑为28 x 28 x 1的形状，以供我们的模型使用，并通过255进行归一化，保持模型输入在0和1之间。
- en: We split the training data into train and validation datasets, each with 55,000 and
    5000 images respectively.
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将训练数据分割为训练集和验证集，每个集分别包含55,000张和5000张图像。
- en: We convert our target array **y** for both training and testing datasets so
    that we have a one-hot representation of the 10 classes in the dataset that we
    are going to feed into the model.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将训练和测试数据集中的目标数组**y**进行转换，以便我们能获得数据集中10个类别的独热表示，这些数据将被输入到模型中。
- en: Make sure to choose around 10% of data for out validation. In this project,
    we choose 5000 random images (8% of the total images) for the validation data
    set.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 确保选择大约10%的数据作为验证集。在这个项目中，我们选择了5000张随机图像（占总图像的8%）作为验证数据集。
- en: 'The code for the preceding steps is as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 前述步骤的代码如下：
- en: '[PRE3]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that we normalize the image pixels by `255` after loading the dataset for
    training stability and faster convergence.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在加载数据集后，我们通过`255`对图像像素进行归一化，以确保训练的稳定性和更快的收敛。
- en: 'Implement the encoder by creating the three neural network layers that have
    been defined in the *U**nderstanding the encoder* section:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过创建在*理解编码器*部分中定义的三个神经网络层，来实现编码器：
- en: '[PRE4]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, implement the decoder layers to reconstruct the images, as described
    in the *Understanding the decoder*section. Here are the important steps once again
    for reference:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，实现解码器层以重建图像，如*理解解码器*部分所述。以下是参考的重要步骤：
- en: First, we calculate the norm of each activity vector in a digit caps output
    for masking purposes. We also add an epsilon to the norm for stability purposes.
  id: totrans-137
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们计算数字胶囊输出中每个活动向量的范数，用于遮盖目的。我们还会为稳定性添加一个epsilon到范数中。
- en: For training, we mask all of the activity vectors in digit caps output, except
    the one with the correct label. On the other hand, for testing, we mask all the
    activity vectors in digit caps output, except the one with the highest norm (or
    predicted label). We implement this branching mechanism in the decoder with **tf.cond**, which
    defines a control flow operation in the TensorFlow graph.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练时，我们会遮盖掉数字胶囊输出中的所有活动向量，除了具有正确标签的那个。另一方面，在测试时，我们会遮盖掉数字胶囊输出中的所有活动向量，除了具有最高范数（或预测标签）的那个。我们在解码器中使用**tf.cond**实现这一分支机制，它在TensorFlow图中定义了一个控制流操作。
- en: Finally, we flatten the masked output from the digit caps and flatten it as
    a one-dimensional vector that can be fed to the fully connected layers.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将来自数字胶囊的掩蔽输出展平，并将其展平成一个一维向量，以便可以输入到全连接层。
- en: To read up on `tf.cond`, refer to [https://www.tensorflow.org/api_docs/python/tf/cond](https://www.tensorflow.org/api_docs/python/tf/cond).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解`tf.cond`，请参阅[https://www.tensorflow.org/api_docs/python/tf/cond](https://www.tensorflow.org/api_docs/python/tf/cond)。
- en: 'The code for the preceding steps is as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 上述步骤的代码如下：
- en: '[PRE5]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Implement margin loss using the formula mentioned in the *Defining the loss
    function* section as follows:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*定义损失函数*部分中提到的公式实现边缘损失，如下所示：
- en: '[PRE6]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Implement reconstruction loss using the formula mentioned in the *Defining
    the loss function* section:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*定义损失函数*部分中提到的公式实现重建损失：
- en: '[PRE7]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define the optimizer as an Adam optimizer, using the default parameters and
    an accuracy metric as the usual classification accuracy. These need to be implemented
    in the CapsNet class itself using the following code:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义优化器为Adam优化器，使用默认参数和准确性度量作为常规分类准确率。这些需要在CapsNet类中使用以下代码实现：
- en: '[PRE8]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To learn more about the Adam Optimizer, refer to[ https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于Adam优化器的信息，请参阅[https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)。
- en: 'Implement the support for checkpointing and restoring the model. Select the
    best model based on validation set accuracy; we checkpoint the model only for
    the epoch, where we observe a decrease in the validation set accuracy and finally,
    log the summary output for TensorBoard visualization. We train our model for 10
    epochs each having batch size 128\. Remember, you can vary these parameters to
    improve the accuracy of your model:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现对检查点和恢复模型的支持。根据验证集准确率选择最佳模型；我们只在验证集准确率下降的时期进行模型检查点操作，最后，将摘要输出记录到TensorBoard进行可视化。我们将模型训练10个周期，每个周期的批次大小为128。记住，你可以调整这些参数来提高模型的准确性：
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This model achieved almost `99%` accuracy with `10` epochs on the validation
    and test sets, which is quite good.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在验证集和测试集上经过`10`个周期后达到了几乎`99%`的准确率，表现相当不错。
- en: Reconstructing sample images
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重建样本图像
- en: 'We will also reconstruct some sample images to see how the model is performing.
    We will use the following images as the input:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将重建一些样本图像，以查看模型的表现。我们将使用以下图像作为输入：
- en: '![](img/d1e7025d-af36-486e-bbd0-88ccce351187.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1e7025d-af36-486e-bbd0-88ccce351187.png)'
- en: 'The code for reconstructing the preceding images is as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 重建上述图像的代码如下：
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The reconstruction function for plotting the images and saving them is given
    as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 用于绘制图像并保存的重建函数如下所示：
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The reconstructed images now look like this:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，重建的图像如下所示：
- en: '![](img/401f42ba-c0c8-42d5-988c-8f6457e32506.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/401f42ba-c0c8-42d5-988c-8f6457e32506.png)'
- en: As we can see, the labels are perfect, while the reconstructed images aren't
    as perfect but very similar. With more hyper parameter tuning, we can generate
    much better reconstructed images.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，标签是完美的，而重建图像虽然不完美，但非常相似。通过更多的超参数调优，我们可以生成更好的重建图像。
- en: Limitations of capsule networks
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 胶囊网络的局限性
- en: 'While capsule networks are great and they address the core issues of convolutional
    neural networks, they still have a long way to go. Some of the limitations of
    capsule networks are as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然胶囊网络很棒，并且解决了卷积神经网络的核心问题，但它们仍然有很长的路要走。胶囊网络的一些局限性如下：
- en: The network has not been tested on large datasets like ImageNet. This puts a
    question mark on their ability to perform well on large datasets.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该网络尚未在像ImageNet这样的超大数据集上进行测试。这对其在大数据集上的表现提出了疑问。
- en: The algorithm is slow, mainly due to the inner loop of the dynamic routing algorithm.
    The number of iterations can be fairly large for large datasets.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法运行较慢，主要由于动态路由算法的内部循环。对于大数据集，迭代次数可能相当大。
- en: Capsule networks definitely have higher complexity in implementation compared
    to CNNs.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与CNN相比，胶囊网络在实现上确实具有更高的复杂性。
- en: It would be interesting to see how the deep learning community addresses the
    limitations of capsule networks.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 看到深度学习社区如何解决胶囊网络的局限性将会很有趣。
- en: Summary
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at the very popular neural network architecture CapsNet,
    by Geoff Hinton (presumably the father of deep learning).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了由Geoff Hinton（深度学习的奠基人之一）提出的非常流行的神经网络架构CapsNet。
- en: We started off by understanding the limitations of CNNs in their current form.
    They use max pooling as a crutch to achieve invariance in activities. Max pooling
    has a tendency to lose information, and it can't model the relationships between
    different objects in the image. We then touched upon how the human brain detects
    objects and are viewpoint invariant. We drew an analogy to computer graphics and
    understood how we can probably incorporate pose information in neural networks.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先理解了卷积神经网络（CNN）在当前形式下的局限性。它们使用最大池化作为一种依赖工具来实现活动的不变性。最大池化有信息丢失的倾向，而且它无法建模图像中不同物体之间的关系。接着，我们讨论了人脑如何检测物体并且具有视角不变性。我们通过类比计算机图形学，理解了如何可能在神经网络中加入姿态信息。
- en: Subsequently, we learned about the basic building blocks of capsule networks,
    that is, capsules. We understood how they differ from the traditional neuron in
    that they take a vector as the input and produce a vector output. We also learned
    about a special kind of non-linearity in capsules, namely the `squash`function.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们了解了胶囊网络的基本构建块——胶囊。我们理解了胶囊与传统神经元的区别，它们接受向量作为输入，并输出一个向量。我们还了解了胶囊中的一种特殊非线性函数——`squash`函数。
- en: In the next section, we learned about the novel **dynamic routing algorithm**,
    which helps route the output from lower layer capsules to higher layer capsules.
    The coefficients ![](img/71dd376b-2924-4d16-855c-5fbe21567127.png) are learned
    through several iterations of the routing algorithm. The crux of the algorithm
    was the step in which we update the coefficients ![](img/91b45590-99bc-4394-afde-cde1358ade36.png) 
    by using the dot product of the predicted vector ![](img/1df88cb6-46c6-4d83-b51f-ade7ad980ed4.png) and
    the output vector of the higher-layer capsule ![](img/89497384-2dc1-46c8-aa49-7a2f0f64d41d.png).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们了解了一种新颖的**动态路由算法**，它有助于将低层胶囊的输出路由到高层胶囊。系数 ![](img/71dd376b-2924-4d16-855c-5fbe21567127.png) 通过多次迭代的路由算法进行学习。算法的关键步骤是通过使用预测向量 ![](img/1df88cb6-46c6-4d83-b51f-ade7ad980ed4.png) 和高层胶囊输出向量 ![](img/89497384-2dc1-46c8-aa49-7a2f0f64d41d.png) 的点积来更新系数 ![](img/91b45590-99bc-4394-afde-cde1358ade36.png)。
- en: Furthermore, we implemented CapsNet for the Fashion MNIST dataset. We used a
    convolutional layer, followed by a PrimaryCaps layer and a DigitCaps layer. We
    learned about the encoder architecture and how we can get a vector representation
    of the images. This was followed by an understanding of the decoder architecture
    to reconstruct the image from the learned representations. The loss function in
    this architecture was a combination of margin loss (like in SVMs) and weighted-down
    reconstruction loss. The reconstruction loss was weighted down so that the model
    could focus more on margin loss during training.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们为Fashion MNIST数据集实现了CapsNet。我们使用了卷积层，接着是PrimaryCaps层和DigitCaps层。我们了解了编码器架构，以及如何获取图像的向量表示。然后我们学习了解码器架构，以便从学习到的表示中重建图像。该架构中的损失函数是边际损失（如同支持向量机中的损失）和加权重构损失的组合。为了让模型在训练时能更多地关注边际损失，重构损失被加权降低。
- en: We then trained the model on 10 epochs with a batch size of 128 and achieved
    over 99% accuracy on the validation and test sets. We reconstructed some sample
    images to visualize the output and found the reconstruction to be fairly accurate.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将模型训练了10个周期，批量大小为128，并在验证集和测试集上达到了超过99%的准确率。我们重建了一些示例图像以可视化输出，并发现重建结果相当准确。
- en: In summary, throughout this chapter, we were able to understand and implement
    capsule networks from scratch using TensorFlow and trained them on the Fashion
    MNIST dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在本章中，我们能够理解并从头开始实现胶囊网络，使用TensorFlow并在Fashion MNIST数据集上进行训练。
- en: Now that you have built the basic capsule network, you can try to extend this
    model by incorporating multiple capsule layers and see how it performs, use on
    other image datasets and see whether this algorithm is scalable, run it without reconstruction
    loss, and see whether you can still reconstruct the input image. By doing this,
    you will be able to develop a good intuition toward this algorithm.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经构建了基本的胶囊网络，您可以尝试通过加入多个胶囊层来扩展这个模型，并查看它的表现；您还可以在其他图像数据集上使用这个模型，并观察该算法是否具有可扩展性；或者在没有重构损失的情况下运行，看看是否还能重建输入图像。通过这样做，您将能够对该算法形成较好的直觉。
- en: In the next chapter, we will look at the face-detection project using TensorFlow.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍使用TensorFlow进行人脸检测的项目。
