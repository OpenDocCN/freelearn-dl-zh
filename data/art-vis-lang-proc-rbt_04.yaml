- en: '*Chapter 4*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*'
- en: Neural Networks with NLP
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有自然语言处理的神经网络
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Explain what a Recurrent Neural Network is
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释什么是循环神经网络
- en: Design and build a Recurrent Neural Network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和构建循环神经网络
- en: Evaluate non-numeric data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估非数值数据
- en: Evaluate the different state-of-the-art language models with RNNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估使用 RNN 的不同最先进的语言模型
- en: Predict a value with a temporal sequence of data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用时间序列数据预测一个值
- en: This chapter covers various aspects of RNNs. it deals with explaining, designing,
    and building the various RNN models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了循环神经网络（RNN）的各个方面，主要讲解、设计和构建不同的 RNN 模型。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: As mentioned in the previous chapter, Natural Language Processing (NLP) is an
    area of Artificial Intelligence (AI) that covers how computers can understand
    and manipulate human language in order to perform useful tasks. Now, with the
    growth of deep learning techniques, deep NLP has become a new area of research.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，自然语言处理（NLP）是人工智能（AI）中的一个领域，涵盖了计算机如何理解和操作人类语言以执行有用的任务。如今，随着深度学习技术的增长，深度自然语言处理（深度
    NLP）已成为一个新的研究领域。
- en: 'So, what is deep NLP? It is a combination of NLP techniques and deep learning.
    The result of the combination of these techniques are advances in the following
    areas:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是深度自然语言处理？它是自然语言处理技术和深度学习的结合。结合这些技术的结果在以下领域取得了进展：
- en: 'Linguistics: Speech to text'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语言学：语音转文本
- en: 'Tools: POS tagging, entity recognition, and sentence parsing'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具：词性标注、实体识别和句法分析
- en: 'Applications: Sentiment analysis, question answering, dialogue agents, and
    machine translation'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用：情感分析、问答、对话代理和机器翻译
- en: One of the most important approaches of deep NLP is the representation of words
    and sentences. Words can be represented as a vector located in a plane full of
    other words. Depending on the similarity of each word to another word, its distance
    in the plane would be accordingly set as greater or smaller.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 NLP 最重要的方法之一是对单词和句子的表示。单词可以表示为一个向量，位于充满其他单词的平面中。根据每个单词与另一个单词的相似度，其在平面中的距离会相应地设置为更大或更小。
- en: '![Figure 4.1: Representation of words in multiple dimensions](img/C13550_04_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1：多维度中的词语表示](img/C13550_04_01.jpg)'
- en: 'Figure 4.1: Representation of words in multiple dimensions'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.1：多维度中的词语表示
- en: 'The previous figure shows an example of word embedding. **Word embedding**
    is a collection of techniques and methods that map words and sentences from a
    corpus into vectors or real numbers. It generates a representation of each word
    in terms of the context in which a word appears. Then, word embedding can find
    the similarities between words. For example, the nearest words to dog are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 上一张图展示了词嵌入的示例。**词嵌入**是一组技术和方法，将词语和句子从语料库映射到向量或实数。它生成了每个词语的表示，基于词语出现的上下文。然后，词嵌入可以找到词语之间的相似性。例如，离“dog”最近的词语如下：
- en: Dogs
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 狗
- en: Cat
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 猫
- en: Cow
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 牛
- en: Rat
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 老鼠
- en: Bird
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鸟
- en: There are different ways to generate embeddings, such as Word2Vec, which will
    be covered in *Chapter 7*, *Build a Conversational Agent to Manage the Robot*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方式可以生成词嵌入，例如 Word2Vec，它将在*第7章*中讲解，*构建一个对话代理来管理机器人*。
- en: This is not the only big change deep learning brings to NLP on a morphological
    level. With deep learning, a word can be represented as a combination of vectors.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是深度学习对 NLP 在形态学层面带来的唯一巨大变化。通过深度学习，一个词可以表示为多个向量的组合。
- en: Each morpheme is a vector, and a word is the result of combining several morpheme
    vectors.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 每个语素都是一个向量，而一个词是通过组合多个语素向量得到的结果。
- en: This technique of combining vectors is also used on a semantic level, but for
    the creation of words and for the creation of a sentence. Each phrase is formed
    by a combination of many word vectors, so a sentence can be represented as one
    vector.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这种向量组合的技术也在语义层面上得到应用，但它用于单词和句子的创建。每个短语都是通过许多单词向量的组合形成的，因此一个句子可以表示为一个向量。
- en: Another improvement is in parsing sentences. This task is hard because it is
    ambiguous. Neural networks can accurately determine the grammatical structure
    of a sentence.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个改进是在句法分析方面。这项任务很困难，因为它具有歧义性。神经网络可以准确地确定句子的语法结构。
- en: 'In full application terms, the areas are as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用的全面术语中，相关领域如下：
- en: '**Sentiment analysis**: Traditionally, this consists of a bag of words labeled
    with positive or negative sentiments. Then, combining these words returns the
    sentiment of the whole sentence. Now, using deep learning and word representation
    models, the results are better.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：传统方法是将词汇包标记为正面或负面情感。然后，通过组合这些词汇来返回整个句子的情感。如今，利用深度学习和词表示模型，结果更为优秀。'
- en: '**Question answering**: To find the answer to a question, vector representations
    can match a document, a paragraph, or a sentence with an input question.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题回答**：为了找到问题的答案，向量表示可以将文档、段落或句子与输入问题匹配。'
- en: '**Dialogue agents**: With neural language models, a model can understand a
    query and create a response.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对话代理**：利用神经语言模型，模型可以理解查询并生成回应。'
- en: '**Machine translation**: Machine translation is one of the hardest tasks in
    NLP. A lot of approaches and models have been tried. Traditional models are very
    large and complex, but deep learning neural machine translation has solved that
    problem. Sentences are encoded with vectors, and the output is decoded.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器翻译**：机器翻译是NLP中最难的任务之一。已经尝试了许多方法和模型。传统模型非常庞大和复杂，但深度学习神经机器翻译解决了这个问题。句子通过向量进行编码，输出则进行解码。'
- en: The vector representation of words is fundamental to deep NLP. Creating a plane,
    many tasks can be completed. Before analyzing deep NLP techniques, we are going
    to review what a recurrent neural network (RNN) is, what its applications are
    within deep learning, and how to create our first RNN.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 词的向量表示是深度自然语言处理（NLP）的基础。创建一个平面，可以完成许多任务。在分析深度NLP技术之前，我们将回顾什么是循环神经网络（RNN），它在深度学习中的应用是什么，以及如何创建我们的第一个RNN。
- en: Our future conversational agent will detect the intention of a conversation
    and respond with a predefined answer. But with a good dataset of conversations,
    we could create a Recurrent Neural Network to train a language model (LM) capable
    of generating a response to a given topic in a conversation. This task can be
    performed by other neural network architectures, such as seq2seq models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们未来的对话代理将能够检测到对话的意图并作出预定义的回答。但是通过一个良好的对话数据集，我们可以创建一个循环神经网络（RNN）来训练一个能够根据对话主题生成回应的语言模型（LM）。这一任务也可以通过其他神经网络架构来实现，例如seq2seq模型。
- en: Recurrent Neural Networks
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: In this section, we are going to review **Recurrent Neural Networks** (**RNNs**).
    This topic will first look at the theory of RNNs. It will review many architectures
    within this model and help you to work out which model to use to solve a certain
    problem, and it will also look at several types of RNN and their pros and cons.
    Also, we will look at how to create a simple RNN, train it, and make predictions.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾**循环神经网络**（**RNNs**）。本话题将首先介绍RNN的理论。它将回顾该模型中的多种架构，帮助你确定使用哪种模型来解决特定问题，还会探讨几种类型的RNN及其优缺点。此外，我们将了解如何创建一个简单的RNN，训练它并进行预测。
- en: Introduction to Recurrent Neural Networks (RNN)
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 循环神经网络（RNN）介绍
- en: Human behavior shows a variety of serially ordered action sequences. A human
    is capable of learning dynamic paths based on a set of previous actions or sequences.
    This means that people do not start learning from scratch; we have some previous
    knowledge, which helps us. For example you could not understand a word if you
    did not understand the previous word in a sentence!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 人类行为展示了各种有序的动作序列。人类能够基于一组先前的动作或序列来学习动态路径。这意味着人们并不是从零开始学习；我们有一些先前的知识，这有助于我们。例如，如果你不理解句子中的前一个词，你就无法理解下一个词！
- en: Traditionally, neural networks cannot solve these types of problem because they
    cannot learn previous information. But what happens with problems that cannot
    be solved with just current information?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，神经网络无法解决这些类型的问题，因为它们不能学习先前的信息。那么，当问题无法仅通过当前信息解决时，应该怎么办呢？
- en: In 1986, Michael I. Jordan proposed a model that deals with the classical problem
    of temporal organization. This model is capable of learning the trajectories of
    a dynamic object by studying its previous movements. Jordan created the first
    RNN.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 1986年，Michael I. Jordan 提出了一个处理时间组织经典问题的模型。该模型能够通过研究动态物体的先前运动来学习其轨迹。Jordan 创造了第一个RNN。
- en: '![Figure 4.2: Example of non-previous information versus temporal sequences](img/C13550_04_02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.2：非前置信息与时间序列的示例](img/C13550_04_02.jpg)'
- en: 'Figure 4.2: Example of non-previous information versus temporal sequences'
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.2：非前置信息与时间序列的示例
- en: In the previous figure, the image on the left shows us that, without any information,
    we cannot know what the next action of the black point will be, but if we suppose
    its previous movements are recorded as the red line on the right-hand side of
    the graph we can predict what its next action will be.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一图中，左侧的图像告诉我们，如果没有任何信息，我们无法知道黑点的下一个动作会是什么。但如果我们假设它的前一个动作被记录为右侧图表中的红线，我们就能预测它的下一个动作。
- en: Inside Recurrent Neural Networks
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 递归神经网络内部
- en: So far, we have seen that RNNs are different to neural networks (NNs). RNN neurons
    are like normal neurons, but with loops within them, allowing them to store a
    time state. Storing the state of a certain moment in time, they can make predictions
    based on previous state of time.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到RNN与神经网络（NN）不同。RNN的神经元就像普通神经元，但它们内部有循环，这使它们能够存储时间状态。通过存储某一时刻的状态，它们可以根据前一个时间状态进行预测。
- en: '![Figure 4.3: Traditional neuron](img/C13550_04_03.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3：传统神经元](img/C13550_04_03.jpg)'
- en: 'Figure 4.3: Traditional neuron'
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.3：传统神经元
- en: 'The preceding figure shows a traditional neuron, used in an NN. *X**n* are
    the inputs of the neuron, and after the activation function, it generates a response.
    The schema of an RNN neuron is different:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了一个传统神经元，通常用于神经网络（NN）中。*X**n*是神经元的输入，在激活函数后，生成响应。RNN神经元的架构则有所不同：
- en: '![Figure 4.4: Recurrent neuron](img/C13550_04_04_(1).jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4：递归神经元](img/C13550_04_04_(1).jpg)'
- en: 'Figure 4.4: Recurrent neuron'
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.4：递归神经元
- en: The loop in the previous figure allows the neuron to store the time state. *h**n*
    is the output of the input, *X**n*, and the previous state. The neuron changes
    and evolves over time.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 前图中的循环允许神经元存储时间状态。*h**n*是输入 *X**n* 和前一个状态的输出。神经元随着时间的推移而变化和演化。
- en: 'If the input of the neuron is a sequence, an unrolled RNN would be like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果神经元的输入是一个序列，那么展开的RNN将是这样的：
- en: '![Figure 4.5: Unrolled recurrent neuron](img/C13550_04_05.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5：展开的递归神经元](img/C13550_04_05.jpg)'
- en: 'Figure 4.5: Unrolled recurrent neuron'
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.5：展开的递归神经元
- en: The chain-like schema in figure 4.5 shows that RNNs are closely related to sequences
    and lists. So, we have as many neurons as inputs, and each neuron passes its state
    to the next.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5中的链状架构展示了RNN与序列和列表的紧密关系。因此，我们有与输入数量相同的神经元，每个神经元将其状态传递给下一个神经元。
- en: RNN architectures
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNN架构
- en: 'Depending on the quantity of inputs and outputs in the RNN, there are many
    architectures with different numbers of neurons. Each architecture is specialized
    for a certain task. So far, there are many types of network:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 根据RNN中输入和输出的数量，有许多具有不同神经元数量的架构。每种架构都专门用于某个特定任务。到目前为止，已经有许多种网络类型：
- en: '![Figure 4.6: Structures of RNNs](img/C13550_04_06.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6：RNN的结构](img/C13550_04_06.jpg)'
- en: 'Figure 4.6: Structures of RNNs'
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.6：RNN的结构
- en: The previous figure shows the various classifications of RNNs. Earlier in this
    book, we reviewed the one-to-one architecture. In this chapter, we will learn
    about the many-to-one architecture.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 前图展示了RNN的各种分类。书中早些时候，我们回顾了“一对一”架构。在本章中，我们将学习“多对一”架构。
- en: '**One-to-one**: Classification or regression tasks from one input (image classification).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对一**：来自一个输入的分类或回归任务（图像分类）。'
- en: '**One-to-many**: Image captioning tasks. These are hard tasks in deep learning.
    For example, a model that passes an image as an input could describe the elements
    that are in the picture.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对多**：图像描述任务。这些是深度学习中的难题。例如，传递图像作为输入的模型可以描述图像中的元素。'
- en: '**Many-to-one**: Temporal series, sentiment analysis… every task with just
    one output but based in a sequence of different inputs.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对一**：时间序列，情感分析……每个任务只有一个输出，但基于一系列不同的输入。'
- en: '**Many-to-many**: Machine automated translation systems.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对多**：机器自动翻译系统。'
- en: '**Synchronized many-to-many**: Video classification.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同步多对多**：视频分类。'
- en: Long-Dependency Problem
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长期依赖问题
- en: 'In some tasks, it is only necessary to use the most recent information to predict
    the next step of a model. With a temporal series, it is necessary to check older
    elements to learn or predict the next element or word in a sentence. For example,
    take a look at this sentence:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些任务中，预测模型的下一步只需要使用最新的信息。对于时间序列任务，需要检查更早的元素来学习或预测句子中的下一个元素或词语。例如，看看这句话：
- en: The clouds are in the sky.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'The clouds are in the sky.  '
- en: 'Now imagine this sentence:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象这个句子：
- en: The clouds are in the [?]
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: The clouds are in the [?]
- en: 'You would assume that the required word would be sky, and you know this because
    of the previous information:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会假设需要的词是 sky，你知道这个是因为之前的信息：
- en: The clouds are in the
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云层位于
- en: 'But there are other tasks in which the model would need previous information
    to obtain a better prediction. For example, have a look at this sentence:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 但也有一些任务，模型需要使用先前的信息来获得更好的预测。例如，看看这句话：
- en: I was born in Italy, but when I was 3, I moved to France… that's the reason
    why I speak [?]
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我出生在意大利，但3岁时搬到了法国……这就是我能说[?]的原因
- en: 'To predict the word, the model needs to take the information from the beginning
    of the sentence, and that could be a problem. This is a problem with RNNs: when
    the distance to the information is large, it is more difficult to learn. This
    problem is called the **vanishing gradient**.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测词汇，模型需要从句子的开头获取信息，而这可能是一个问题。这是 RNN 的一个问题：当信息之间的距离较大时，学习变得更加困难。这个问题被称为 **梯度消失**。
- en: '**The vanishing gradient problem**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度消失问题**'
- en: Information travels through time in an RNN so that information from previous
    steps is used as input in the next step. At each step, the model calculates the
    cost function, so each time, the model may obtain an error measure. While propagating
    the error calculated through the network, and trying to minimize that error when
    updating the weights, the result of that operation is a number closer to zero
    (if you multiply two small numbers, the result is a smaller number). This means
    the gradient of the model becomes less and less with each multiplication. The
    problem here is that the network will not train properly. A solution to this problem
    with RNNs is to use Long Short-Term Memory (LSTM).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RNN 中，信息是随着时间流动的，因此前一步的信息会作为输入传递到下一步。每一步，模型都会计算代价函数，所以每次模型可能都会获得一个误差值。在通过网络传播误差，并在更新权重时尽量减小误差的过程中，操作结果会逐渐接近零（如果将两个小数相乘，结果会是一个更小的数）。这意味着模型的梯度在每次乘法操作后会变得越来越小。这里的问题是网络无法正确训练。解决
    RNN 问题的方法之一是使用长短期记忆（LSTM）。
- en: 'Exercise 14: Predict House Prices with an RNN'
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 14：使用 RNN 预测房价
- en: We are going to create our first RNN using Keras. This exercise is not a time-series
    problem. We are going to use a regression dataset to introduce RNNs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Keras 创建我们的第一个 RNN。这个练习不是一个时间序列问题。我们将使用回归数据集来介绍 RNN。
- en: 'We can use several methods included in the Keras library as a model or a type
    of layer:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Keras 库中包含的多种方法来作为模型或层的类型：
- en: 'Keras models: These let us use the different available models in Keras. We
    are going to use the Sequential model.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 模型：这些让我们可以使用 Keras 中的不同模型。我们将使用 Sequential 模型。
- en: 'Keras layers: We can add different types of layers to our neural network. In
    this exercise, we are going to use LSTM and a Dense layer. A dense layer is a
    regular layer of neurons in a neural network. Each neuron receives input from
    all the neurons in the previous layer, but they are densely connected.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 层：我们可以向神经网络中添加不同类型的层。在本次练习中，我们将使用 LSTM 和 Dense 层。Dense 层是神经网络中的常规神经元层，每个神经元都接收来自前一层所有神经元的输入，且这些神经元之间是密集连接的。
- en: 'The main objective of this exercise is to predict the value of a house in Boston,
    so our dataset will contain information on each house, such as the total area
    of the property or the number of rooms:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本次练习的主要目标是预测波士顿一所房子的价值，因此我们的数据集将包含每所房子的相关信息，比如房产的总面积或房间数量：
- en: 'Import the dataset of Boston house prices from `sklearn` and take a look at
    the data:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`sklearn`导入波士顿房价数据集，并查看数据：
- en: '[PRE0]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Figure 4.7: Boston house-prices data](img/C13550_04_07.jpg)'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.7：波士顿房价数据](img/C13550_04_07.jpg)'
- en: 'Figure 4.7: Boston house prices data'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.7：波士顿房价数据
- en: 'You can see the data has high values, so the best thing to do is to normalize
    the data. With the `MinMaxScaler` function of `sklearn`, we are going to transform
    our data into values between 0 and 1:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以看到数据的数值很高，所以最好的做法是对数据进行归一化。使用 `sklearn` 的 `MinMaxScaler` 函数，我们将把数据转换为 0 到
    1 之间的数值：
- en: '[PRE1]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Divide the data into train and test sets. A good percentage for the test set
    is 20% of the data:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集。测试集的合理比例是数据的 20%：
- en: '[PRE2]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Figure 4.8: Shape of the train and test data](img/C13550_04_08.jpg)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.8：训练数据和测试数据的形状](img/C13550_04_08.jpg)'
- en: 'Figure 4.8: Shape of the train and test data'
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.8：训练数据和测试数据的形状
- en: 'Import the Keras libraries and set a seed to initialize the weights:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 Keras 库并设置随机种子以初始化权重：
- en: '[PRE3]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create a simple model. The dense layer is just a set of neurons. The last dense
    layer has only one neuron to return the output:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个简单的模型。密集层只是一个神经元集合。最后一层密集层只有一个神经元用于返回输出：
- en: '[PRE4]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Train the network:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练网络：
- en: '[PRE5]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Figure 4.9: Training the network](img/C13550_04_09.jpg)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.9：训练网络](img/C13550_04_09.jpg)'
- en: 'Figure 4.9: Training the network'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.9：训练网络
- en: 'Compute the error of the model:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型的误差：
- en: '[PRE6]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Figure 4.10: Computing the error in the model](img/C13550_04_10.jpg)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.10：计算模型中的误差](img/C13550_04_10.jpg)'
- en: 'Figure 4.10: Computing the error of the model'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.10：计算模型的误差
- en: 'Plot the predictions:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制预测结果：
- en: '[PRE7]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 4.11: Predictions of our model](img/C13550_04_11.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11：我们模型的预测](img/C13550_04_11.jpg)'
- en: 'Figure 4.11: Predictions of our model'
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.11：我们模型的预测
- en: Now you have an RNN for a regression problem! You can try to modify the parameters,
    add more layers, or change the number of neurons to see what happens. In the next
    exercise, we will solve time-series problems with LSTM layers.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了一个用于回归问题的RNN！你可以尝试修改参数、添加更多层，或者改变神经元的数量来观察会发生什么。在下一个练习中，我们将使用LSTM层解决时间序列问题。
- en: Long Short-Term Memory
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长短期记忆
- en: '**LSTM** is a type of RNN that''s designed to solve the long-dependency problem.
    It can remember values for long or short time periods. The principal way it differs
    from traditional RNNs is that they include a cell or a loop to store the memory
    internally.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM** 是一种RNN，旨在解决长期依赖问题。它可以记住长时间或短时间的数值。它与传统的RNN的主要区别在于，它们包含一个单元或循环来内部存储记忆。'
- en: 'This type of neural network was created in 1997 by Hochreiter and Schmidhuber.
    This is the basic schema of an LSTM neuron:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的神经网络是由Hochreiter和Schmidhuber于1997年创建的。这是一个LSTM神经元的基本结构：
- en: '![Figure 4.12: LSTM neuron structure](img/C13550_04_12.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12：LSTM神经元结构](img/C13550_04_12.jpg)'
- en: 'Figure 4.12: LSTM neuron structure'
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.12：LSTM神经元结构
- en: 'As you can see in the previous figure, the schema of an LSTM neuron is complex.
    It has three types of gate:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前一张图中看到的，LSTM神经元的结构是复杂的。它有三种类型的门控：
- en: 'Input gate: Allows us to control the input values to update the state of the
    memory cell.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门：允许我们控制输入值以更新记忆单元的状态。
- en: 'Forget gate: Allows us to erase the content of the memory cell.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忘记门：允许我们擦除记忆单元中的内容。
- en: 'Output gate: Allows us to control the returned values of the input and cell
    memory content.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出门：允许我们控制输入和记忆单元内容返回的值。
- en: 'An LSTM model in Keras has a three-dimensional input:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中，LSTM模型具有三维输入：
- en: 'Sample: Is the amount of data you have (quantity of sequences).'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本：是你拥有的数据量（序列的数量）。
- en: 'Time step: Is the memory of your network. In other words, it stores previous
    information in order to make better predictions.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间步：是你网络的记忆。换句话说，它存储之前的信息，以便做出更好的预测。
- en: 'Features: Is the number of features in every time step. For example, if you
    are processing pictures, the features are the number of pixels.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征：是每个时间步中的特征数量。例如，如果你处理的是图像，特征就是像素的数量。
- en: Note
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注：
- en: This complex design causes another type of network to be formed. This new type
    of neural network is a **Gated Recurrent Unit (GRU)**, and it solves the vanishing
    gradient problem.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种复杂的设计会导致另一种类型的网络的形成。这种新类型的神经网络是**门控循环单元（GRU）**，它解决了消失梯度问题。
- en: 'Exercise 15: Predict the Next Solution of a Mathematical Function'
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 15：预测数学函数的下一个解
- en: 'In this exercise, we are going to build an LSTM to predict the values of a
    sine function. In this exercise, you will learn how to train and predict a model
    with Keras, using the LSTM model. Also, this exercise will cover data generation
    and how to split data into training samples and test samples:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将构建一个LSTM来预测正弦函数的值。在这个练习中，你将学习如何使用Keras训练和预测一个LSTM模型。此外，这个练习还将介绍数据生成以及如何将数据划分为训练样本和测试样本：
- en: 'With Keras, we can create an RNN using the Sequential class, and we can create
    an LSTM to add new recurrent neurons. Import the Keras libraries for LSTM models,
    NumPy for setting up the data, and matplotlib to print the graphs:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Keras，我们可以通过Sequential类创建一个RNN，并且可以创建一个LSTM来添加新的循环神经元。导入Keras库来构建LSTM模型，导入NumPy来设置数据，导入matplotlib来绘制图表：
- en: '[PRE8]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Create the dataset to train and evaluate the model. We are going to generate
    an array of 1,000 values as a result of the sine function:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建用于训练和评估模型的数据集。我们将生成一个包含1000个值的数组，作为正弦函数的结果：
- en: '[PRE9]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To see if the data is good, let''s plot it:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了查看数据是否合适，让我们绘制它：
- en: '[PRE10]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![Figure 4.13: Output with the plotted data](img/C13550_04_13.jpg)'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.13：带有绘制数据的输出](img/C13550_04_13.jpg)'
- en: 'Figure 4.13: Output with the plotted data'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.13：带有绘制数据的输出
- en: As this chapter explains, RNN works with sequences of data, so we need to split
    our data into sequences. In our case, the maximum length of the sequences will
    be 5\. This is necessary because the RNNs need sequences as input.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如本章所述，RNN 使用数据序列，因此我们需要将数据拆分为序列。在我们的例子中，序列的最大长度将为 5。这是必要的，因为 RNN 需要序列作为输入。
- en: 'This model will be **many-to-one** because the input is a sequence and the
    output is just a value. To see why we are going to create an RNN using the many-to-one
    structure, we just need to know the dimensions of our input and output data:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该模型将是**多对一**的，因为输入是一个序列，而输出只是一个值。要理解为什么我们要使用多对一结构来创建 RNN，只需要了解输入和输出数据的维度：
- en: '[PRE11]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Prepare the data to introduce it to the LSTM model. Pay attention to the shape
    of the `x` and `y` variables. RNNs need a three-dimensional vector as input and
    a two-dimensional vector as output. That''s why we will reshape the variables:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备数据以将其输入到 LSTM 模型中。注意 `x` 和 `y` 变量的形状。RNN 需要一个三维向量作为输入，一个二维向量作为输出。因此，我们将调整变量的形状：
- en: '[PRE12]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Figure 4.14: Reshaping the variables](img/C13550_04_14.jpg)'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.14：调整变量的形状](img/C13550_04_14.jpg)'
- en: 'Figure 4.14: Reshaping the variables'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.14：调整变量的形状
- en: Note
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The input dimension of an LSTM is 3.
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LSTM 的输入维度为 3。
- en: 'Split the data into train and test sets:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集：
- en: '[PRE13]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Figure 4.15: Splitting data as train and test](img/C13550_04_15.jpg)'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.15：将数据拆分为训练集和测试集](img/C13550_04_15.jpg)'
- en: 'Figure 4.15: Splitting data into train and test sets'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.15：将数据拆分为训练集和测试集
- en: 'Build a simple model with one LSTM unit and one dense layer with one neuron
    and linear activation. The dense layer is just a regular layer of neurons receiving
    the input from the previous layer and generating many neurons as output. Because
    of that, our dense layer has only one neuron because we need a scalar value as
    the output:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个简单的模型，其中包含一个 LSTM 单元和一个带有一个神经元的全连接层，使用线性激活函数。全连接层只是接收来自上一层输入并生成多个神经元输出的常规神经层。因此，我们的全连接层只有一个神经元，因为我们需要一个标量值作为输出：
- en: '[PRE14]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Train the model for 5 epochs (one epoch is when the entire dataset is processed
    by the neural network) and a batch size of 32 and evaluate it:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型 5 个周期（一个周期是神经网络处理整个数据集的过程），批次大小为 32，并进行评估：
- en: '[PRE15]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Figure 4.16: Training with 5 epochs with batch size 32](img/C13550_04_16.jpg)'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.16：使用 5 个周期和批次大小为 32 进行训练](img/C13550_04_16.jpg)'
- en: 'Figure 4.16: Training with 5 epochs with a batch size of 32'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.16：使用 5 个周期和批次大小为 32 进行训练
- en: 'Plot the test predictions to see if it works well:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制测试预测结果，看看它是否表现良好：
- en: '[PRE16]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Figure 4.17: Plotting the predicted shape](img/C13550_04_17.jpg)'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.17：绘制预测结果的形状](img/C13550_04_17.jpg)'
- en: 'Figure 4.17: Plotting the predicted shape'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.17：绘制预测结果的形状
- en: 'Let''s improve our model. Create a new one with four units in the LSTM layer
    and one dense layer with one neuron, but with the sigmoid activation:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们改进我们的模型。创建一个新的模型，LSTM 层中有四个单元，一个带有一个神经元的全连接层，并使用 sigmoid 激活函数：
- en: '[PRE17]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Train and evaluate it for 25 epochs and with a batch size of 8:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练并评估模型 25 个周期，批次大小为 8：
- en: '[PRE18]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Figure 4.18: Training for 25 epochs with batch size 8](img/C13550_04_18.jpg)'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.18：使用 25 个周期和批次大小为 8 进行训练](img/C13550_04_18.jpg)'
- en: 'Figure 4.18: Training for 25 epochs with a batch size of 8'
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.18：使用 25 个周期和批次大小为 8 进行训练
- en: 'Plot the predictions of the model:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制模型的预测结果：
- en: '[PRE19]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Figure 4.19: Prediction of our neural network](img/C13550_04_19.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.19：我们的神经网络的预测结果](img/C13550_04_19.jpg)'
- en: 'Figure 4.19: Predictions of our neural network'
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.19：我们的神经网络的预测结果
- en: You can now compare the plots of each model, and we can see that the second
    model is better. With this exercise, you have learned the basics of LSTM, how
    to train and evaluate the model you have created, and also how to determine whether
    it is good or not.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以比较每个模型的图表，我们可以看到第二个模型更好。通过这个练习，你已经掌握了 LSTM 的基础知识，学会了如何训练和评估你创建的模型，以及如何判断它是否好。
- en: Neural Language Models
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经语言模型
- en: '*Chapter 3*, *Fundamentals of Natural Language Processing* introduced us to
    statistical language models (LMs), which are the probability distribution for
    a sequence of words. We know LMs can be used to predict the next word in a sentence,
    or to compute the probability distribution of the next word.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*第 3 章*，*自然语言处理基础* 向我们介绍了统计语言模型（LMs），即一个单词序列的概率分布。我们知道语言模型可以用来预测句子中的下一个单词，或者计算下一个单词的概率分布。'
- en: '![Figure 4.20: LM formula to compute the probability distribution of an upcoming
    word](img/C13550_04_20.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图4.20：用于计算下一个单词概率分布的语言模型公式](img/C13550_04_20.jpg)'
- en: 'Figure 4.20: LM formula to compute the probability distribution of an upcoming
    word'
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.20：用于计算下一个单词概率分布的语言模型公式
- en: The sequence of words is *x1* , *x2* … and the next word is *x**t+1*. *w**j*
    is a word in the vocabulary. *V* is the vocabulary and *j* is a position of a
    word in that vocabulary. *w**j* is the word located in position *j* within *V*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 单词序列是*x1*，*x2* … 下一个单词是*x**t+1*。*w**j*是词汇表中的一个词。*V*是词汇表，*j*是词汇表中词的位置信息。*w**j*是位于位置*j*的词。
- en: You use LMs every day. The keyboards on cell phones use this technology to predict
    the next word of a sentence, and search engines such as Google use it to predict
    what you want to search in their search for engine.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你每天都在使用语言模型（LM）。手机上的键盘使用此技术来预测句子的下一个单词，像谷歌这样的搜索引擎也使用它来预测你想要搜索的内容。
- en: We talked about the n-gram model and bigrams counting the words in a corpus,
    but that solution has some limitations, such as long dependencies. Deep NLP and
    neural LMs will help to get around these limitations.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了n-gram模型和通过计数语料库中的词语来计算bigram，但该解决方案有一些局限性，如长依赖性。深度NLP和神经语言模型将有助于绕过这些局限性。
- en: Introduction to Neural Language Models
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经语言模型简介
- en: Neural LMs follow the same structure as statistical LMs. They aim to predict
    the next word in a sentence, but in a different way. A neural LM is motivated
    by an RNN because of the use of sequences as inputs.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 神经语言模型遵循与统计语言模型相同的结构。它们的目标是预测句子中的下一个单词，但方式不同。神经语言模型的灵感来源于RNN，因为使用了序列作为输入。
- en: '*Exercise 15*, *Predict the Next Solution of a Mathematical Function* predicts
    the next result of the sine function from a sequence of five previous steps. In
    this case, instead of sequences of sine function results, the data is words, and
    the model will predict the next word.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '*练习15*，*预测数学函数的下一个解*，通过前五个步骤的序列来预测正弦函数的下一个结果。在这种情况下，数据不是正弦函数结果的序列，而是单词，模型将预测下一个单词。'
- en: These neural LMs emerged from the necessity to improve the statistical approach.
    Newer models can work around some of the limitations and problems of traditional
    LMs.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这些神经语言模型（LMs）源于改善统计方法的需求。新的模型可以绕过一些传统语言模型的局限性和问题。
- en: '**Problems of statistical LMs**'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**统计语言模型的问题**'
- en: In the previous chapter, we reviewed LMs and the concepts of N-grams, bigrams,
    and the Markov model. These methods are executed by counting occurrences in the
    text. That's why these methods are called statistical LMs.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们回顾了语言模型以及n-gram、bigram和马尔可夫模型的概念。这些方法通过计数文本中的词频来执行。这就是为什么这些方法被称为统计语言模型的原因。
- en: The main problem with LMs is data limitation. What can we do if the probability
    distribution of the sentence we want to compute does not exist in the data? A
    partial solution here is the smoothing method, but that is insufficient.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的主要问题是数据限制。如果我们要计算的句子的概率分布在数据中不存在，该怎么办？一个部分解决方案是平滑方法，但它不足够。
- en: Another solution is to use the Markov Assumption (each probability only depends
    on the previous step, simplifying the Chain Rule) to simplify the sentence, but
    that will not give a good prediction. What this means is, we could simplify our
    model using 3-grams.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解决方案是使用马尔可夫假设（每个概率仅依赖于前一个步骤，从而简化链式法则）来简化句子，但这不会给出好的预测。这意味着我们可以使用3-grams来简化我们的模型。
- en: A solution to this problem is to increase the size of the corpus, but the corpus
    will end up being to large. These limitations in n-gram models are called **sparsity
    problems**.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是增加语料库的大小，但语料库最终会变得过大。这种n-gram模型的局限性被称为**稀疏性问题**。
- en: '**Window-Based Neural Model**'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于窗口的神经模型**'
- en: A first approximation of this new model was the use of a sliding window to compute
    the probabilities of the next word. The concept of this solution comes from window
    classification.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新模型的第一次近似是使用滑动窗口来计算下一个单词的概率。这个解决方案的概念来自于窗口分类。
- en: In terms of words, it is hard to understand the meaning of a single word without
    any context. There are many problems if that word is not in a sentence or in a
    paragraph, for example, ambiguity between two similar words or auto-antonyms.
    Auto-antonyms are words with multiple meanings. The word handicap, depending on
    its context, can mean an advantage (for example, in sport) or a disadvantage (sometimes
    offensive, a physical problem).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 就单词而言，没有上下文，很难理解一个单词的含义。如果这个单词没有出现在句子或段落中，会出现很多问题，例如两个相似单词之间的歧义或自反义词问题。自反义词是具有多重含义的词。比如单词
    "handicap" ，根据上下文，它可以表示优势（例如，在运动中）或劣势（有时具有冒犯意味，是一种身体问题）。
- en: 'Window classification classifies a word in the context (created by the window)
    of its neighboring words. The approach of a sliding window can be used to generate
    an LM. Here is a graphical example:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口分类法通过其邻近单词的上下文（由窗口创建）对单词进行分类。可以使用滑动窗口的方法生成语言模型。以下是一个图形示例：
- en: '![Figure 4.21: Window-based neural LM](img/C13550_04_21.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.21：基于窗口的神经语言模型](img/C13550_04_21.jpg)'
- en: 'Figure 4.21: Window-based neural LM'
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.21：基于窗口的神经语言模型
- en: 'In the previous figure, there is an example of how a window-based neural model
    works. The window size is 5 (word1 to word5). It creates a vector joining the
    embedding vector of each word, and computes this in a hidden layer:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，展示了基于窗口的神经模型是如何工作的。窗口大小为5（word1 到 word5）。它创建一个向量，将每个单词的嵌入向量连接起来，并在隐藏层中进行计算：
- en: '![Figure 4.22: Hidden layer formula](img/C13550_04_22.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.22：隐藏层公式](img/C13550_04_22.jpg)'
- en: 'Figure 4.22: Hidden layer formula'
  id: totrans-196
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.22：隐藏层公式
- en: 'And finally, to predict a word, the model returns a value that can be used
    to classify the probability of the word:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了预测一个单词，模型返回一个可以用来分类该单词概率的值：
- en: '![Figure 4.23: Softmax function](img/C13550_04_23.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.23：Softmax 函数](img/C13550_04_23.jpg)'
- en: 'Figure 4.23: Softmax function'
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.23：Softmax 函数
- en: Then, the word with the highest value will be the predicted word.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，具有最高值的单词将是预测的单词。
- en: Note
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注
- en: We are not going to go deeper into these terms because we will use an LSTM to
    create the LM.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不打算深入探讨这些术语，因为我们将使用 LSTM 来创建语言模型。
- en: 'The benefits of this approach over the traditional one are as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 相比传统方法，这种方法的优势如下：
- en: Less computational work. Window-based neural models need less computational
    resources because they don't need to iterate through the corpus computing probabilities.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更少的计算工作。基于窗口的神经模型需要更少的计算资源，因为它们不需要遍历语料库计算概率。
- en: It avoids the problem of changing the dimension of the N-gram to find a good
    probability distribution.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它避免了通过改变 N-gram 的维度来寻找好的概率分布的问题。
- en: The generated text will have more sense in terms of meaning because this approach
    solves the sparsity problem.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的文本在意义上会更有逻辑，因为这种方法解决了稀疏性问题。
- en: 'But there are some problems:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 但是存在一些问题：
- en: 'Window limitations: The size of the window cannot be large, so the meaning
    of some words could be wrong.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口的限制：窗口的大小不能太大，否则一些单词的含义可能会出错。
- en: Each window has its own weight value, so it can cause ambiguity.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个窗口都有自己的权重值，因此可能会导致歧义。
- en: If the window grows in size, the model grows too.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果窗口的大小增加，模型也会随之增长。
- en: Analyzing the problems with the window model, an RNN can improve its performance.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 分析窗口模型的问题时，RNN 可以提高性能。
- en: RNN Language Model
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNN 语言模型
- en: An RNN is able to compute the probabilities of an upcoming word in a sequence
    of previous steps. The core idea of this approach is to apply the same weights
    repeatedly throughout the process of training.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 能够计算序列中下一个单词的概率，基于之前步骤的信息。该方法的核心思想是，在整个训练过程中反复应用相同的权重。
- en: 'There are some advantages of using an RNN LM over a window-based model:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 RNN 语言模型相对于基于窗口的模型有一些优势：
- en: This architecture can process any length sentence; it does not have a fixed
    size, unlike the window-based approach.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种架构可以处理任意长度的句子；它没有固定的大小，不像基于窗口的方法。
- en: The model is the same for every input size. It will not grow if the input is
    larger.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个输入大小，模型是相同的。如果输入更大，它不会增长。
- en: Depending on the NN architecture, it can use information from the previous steps
    and from the steps ahead.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据神经网络架构，它可以使用来自前一步和后一步的信息。
- en: The weights are shared across the timesteps.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重在各个时间步之间是共享的。
- en: So far, we have talked about different ways to improve the statistical LM and
    the pros and cons of each one. Before developing an RNN LM, we need to know how
    to introduce a sentence as input in the NN.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了改进统计语言模型的不同方法以及每种方法的优缺点。在开发 RNN 语言模型之前，我们需要了解如何将句子作为输入引入神经网络。
- en: '**One-hot encoding**'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '**独热编码**'
- en: Neural networks and machine learning are about numbers. As we have seen throughout
    this book, input elements are numbers and outputs are codified labels. But if
    a neural network has a sentence or a set of characters as input, how can it transform
    this into numerical values?
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和机器学习都是关于数字的。正如我们在本书中所看到的，输入元素是数字，输出是编码标签。但如果神经网络的输入是一个句子或一组字符，如何将其转化为数值呢？
- en: One-hot encoding is a numerical representation of discrete variables. It assumes
    a feature vector with the same size for different values within a discrete set
    of variables. This means that if there is a corpus of size 10, each word will
    be codified as a vector of length 10\. So, each dimension corresponds to a unique
    element of the set.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码是离散变量的数值表示。它假设对于离散变量集中的不同值，特征向量的大小相同。这意味着如果语料库的大小为 10，每个单词将被编码为一个长度为 10
    的向量。因此，每个维度对应集合中的一个唯一元素。
- en: '![Figure 4.24: RNN pre-processing data flow](img/C13550_04_24.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.24：RNN 数据预处理流](img/C13550_04_24.jpg)'
- en: 'Figure 4.24: RNN pre-processing data flow'
  id: totrans-224
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.24：RNN 数据预处理流
- en: The previous figure shows how one-hot encoding works. It is important to understand
    the shapes of each vector because the neural network needs to understand what
    input data we have and what output we want to obtain. Next, *Exercise 16, Encoding
    a small Corpus* will help you examine the fundamentals of one-hot encoding in
    more detail.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 前一张图展示了独热编码的工作原理。理解每个向量的形状非常重要，因为神经网络需要了解我们拥有的输入数据以及我们希望获得的输出。接下来，*练习 16，编码一个小语料库*
    将帮助你更详细地学习独热编码的基础。
- en: 'Exercise 16: Encoding a Small Corpus'
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 16：编码一个小语料库
- en: In this exercise, we are going to learn how to encode a set of words using one-hot
    encoding. It is the most basic encoding method, and it gives us a representation
    of discrete variables.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将学习如何使用独热编码对一组单词进行编码。这是最基本的编码方法，它为我们提供了离散变量的表示。
- en: 'This exercise will cover different ways of performing this task. One way is
    to manually perform encoding, while another way is to use libraries. After finishing
    the exercise, we will obtain a vector representation of each word, ready to use
    as the input for a neural network:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习将涵盖执行此任务的不同方法。一种方法是手动执行编码，另一种方法是使用库。在完成练习后，我们将获得每个单词的向量表示，准备作为神经网络的输入：
- en: 'Define a corpus. This corpus is the same one that we used in *Chapter 3*, *Fundamentals
    of Natural Language Processing*:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个语料库。这个语料库与我们在*第三章*，*自然语言处理基础*中使用的语料库相同：
- en: '[PRE20]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Tokenize it using `spaCy`. We are not going to use the stop-words (erasing
    useless words, such as articles) method because we have a small corpus. We want
    all the tokens:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `spaCy` 对其进行分词。我们不会使用停用词（去除无用的词，如冠词）方法，因为我们有一个小语料库。我们希望保留所有的标记：
- en: '[PRE21]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Create a list with every unique token in the corpus:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含语料库中每个唯一标记的列表：
- en: '[PRE22]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![Figure 4.25: List with each unique token in the corpus](img/C13550_04_25.jpg)'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.25：语料库中每个唯一标记的列表](img/C13550_04_25.jpg)'
- en: 'Figure 4.25: List with each unique token in the corpus'
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.25：语料库中每个唯一标记的列表
- en: 'Create a dictionary with each word in the corpus as the key and a unique number
    as the value. This dictionary will look like {word:value}, and this value will
    have the index of 1 in the one-hot encoded vector:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个字典，其中每个单词在语料库中作为键，唯一的数字作为值。这个字典将类似于 {word:value}，并且该值将在独热编码向量中具有索引 1：
- en: '[PRE23]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Figure 4.26: Each word as a key and a unique number as value](img/C13550_04_26.jpg)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.26：每个单词作为键，唯一的数字作为值](img/C13550_04_26.jpg)'
- en: 'Figure 4.26: Each word as a key and a unique number as a value'
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.26：每个单词作为键，唯一的数字作为值
- en: 'Encode a sentence. This way of performing encoding is manual. There are some
    libraries, such as sklearn, that provide automatic encoding methods:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对句子进行编码。这种编码方式是手动的。有一些库，如 sklearn，提供自动编码方法：
- en: '[PRE24]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](img/C13550_04_27.jpg)'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C13550_04_27.jpg)'
- en: 'Figure 4.27: Manual one-hot encoded vectors.'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.27：手动独热编码向量
- en: '[PRE25]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Import the `sklearn` methods. sklearn first encodes each unique token in the
    corpus with `LabelEncoder`, and then uses `OneHotEncoder` to create the vectors:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `sklearn` 方法。sklearn 首先使用 `LabelEncoder` 对语料库中的每个唯一标记进行编码，然后使用 `OneHotEncoder`
    创建向量：
- en: '[PRE26]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](img/C13550_04_28.jpg)'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C13550_04_28.jpg)'
- en: 'Figure 4.28: Vectors created with OneHotEncoder'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.28：使用 OneHotEncoder 创建的向量
- en: 'Now, take the same sentence that we encoded before and apply the `LabelEncoder`
    transform method we created:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用之前编码的相同句子，并应用我们创建的 `LabelEncoder` 转换方法：
- en: '[PRE27]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Figure 4.29: LabelEncoder transform applied](img/C13550_04_29.jpg)'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.29：应用 LabelEncoder 转换](img/C13550_04_29.jpg)'
- en: 'Figure 4.29: LabelEncoder transform applied'
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.29：应用 LabelEncoder 转换
- en: 'We can decode `LabelEncoder` in the initial sentence:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以解码 `LabelEncoder` 中的初始句子：
- en: '[PRE28]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Figure 4.30: Decoded LabelEncoder](img/C13550_04_30.jpg)'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 4.30：解码后的 LabelEncoder](img/C13550_04_30.jpg)'
- en: 'Figure 4.30: Decoded LabelEncoder'
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.30：解码后的 LabelEncoder
- en: 'Declare `OneHotEncoder` with `sparse=False` (if you do not specify this, it
    will return a sparse matrix):'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `sparse=False` 声明 `OneHotEncoder`（如果不指定此项，它将返回一个稀疏矩阵）：
- en: '[PRE29]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To encode our sentence with the label encoder that we have created, we need
    to reshape our labeled corpus to fit it into the `onehot_encoder` method:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使用我们创建的标签编码器编码句子，我们需要将带标签的语料库重塑以适应 `onehot_encoder` 方法：
- en: '[PRE30]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we can transform our sentence (encoded with LabelEncoder) into a one-hot
    vector. The results of this way of encoding and manual encoding will not be the
    same, but they will have the same shape:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以将我们的句子（使用 LabelEncoder 编码）转换成一个 one-hot 向量。这种编码方式与手动编码的结果不会完全相同，但它们会具有相同的形状：
- en: '[PRE31]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![Figure 4.31: One-hot encoded vectors using Sklearn methods](img/C13550_04_31.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.31：使用 Sklearn 方法进行的 One-hot 编码向量](img/C13550_04_31.jpg)'
- en: 'Figure 4.31: One-hot encoded vectors using Sklearn methods'
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.31：使用 Sklearn 方法进行的 One-hot 编码向量
- en: Note
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This exercise is really important. If you do not understand the shapes of the
    matrices, it will be very hard to understand the inputs of RNNs.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习非常重要。如果你不理解矩阵的形状，理解 RNN 的输入将会非常困难。
- en: Good job! You have finished *Exercise 16*. Now you can encode discrete variables
    into vectors. This is part of pre-processing data to train and evaluate a neural
    network. Next, we have the activity of the chapter, the objective of which is
    to create an LM using RNNs and one-hot encoding.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！你完成了*练习 16*。现在你可以将离散变量编码成向量了。这是训练和评估神经网络的预处理数据的一部分。接下来，我们将进行本章的活动，目标是使用
    RNN 和 one-hot 编码创建一个语言模型（LM）。
- en: Note
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: For larger corpuses, one-hot encoding is not very useful because it would create
    huge vectors for the words. Instead, it is normal to use an embedding vector.
    This concept will be covered later in this chapter.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较大的语料库，one-hot 编码不是很有用，因为它会为每个词创建巨大的向量。因此，通常使用嵌入向量。这个概念将在本章稍后介绍。
- en: The Input Dimensions of RNNs
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RNN 的输入维度
- en: Before getting started with the RNN activity, you may not understand input dimensions.
    In this section, we will focus on understanding the shape of the n-dimensional
    arrays, and how we can add a new dimension or erase one.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始 RNN 活动之前，你可能不了解输入维度。在本节中，我们将重点理解 n 维数组的形状，以及如何添加或删除一个维度。
- en: '**Sequence data format**'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**序列数据格式**'
- en: 'We''ve mentioned the many-to-one architecture, where each sample consists of
    a fixed sequence and a label. That label corresponds with the upcoming value in
    the sequence. It is something like this:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到了多对一架构，其中每个样本由一个固定的序列和一个标签组成。这个标签对应序列中的下一个值。就像这样：
- en: '![Figure 4.32: Format of sequence data](img/C13550_04_32.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.32：序列数据的格式](img/C13550_04_32.jpg)'
- en: 'Figure 4.32: Format of sequence data'
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.32：序列数据的格式
- en: 'In this example, we have two sequences in matrix X, and the two output labels
    in Y. So, the shapes are as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们在矩阵 X 中有两个序列，Y 中有两个输出标签。因此，形状如下：
- en: X = (2, 4)
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: X = (2, 4)
- en: Y = (2)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Y = (2)
- en: But if you tried to insert this data into an RNN, it wouldn't work because it
    does not have the correct dimensions.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你尝试将这些数据输入到 RNN 中，它将无法正常工作，因为它没有正确的维度。
- en: '**RNN data format**'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**RNN 数据格式**'
- en: To implement an RNN with temporal sequences in Keras, the model will need an
    input vector with three dimensions and, as output, one vector with two dimensions.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 Keras 中实现带有时间序列的 RNN，模型将需要一个具有三维的输入向量，并且输出一个二维的向量。
- en: 'So, for the X matrix, we will have the following:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，对于 X 矩阵，我们将得到以下形状：
- en: Number of samples
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本数量
- en: Sequence length
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 序列长度
- en: Value length
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值的长度
- en: '![Figure 4.33: RNN data format](img/C13550_04_33.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.33：RNN 数据格式](img/C13550_04_33.jpg)'
- en: 'Figure 4.33: RNN data format'
  id: totrans-288
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4.33：RNN 数据格式
- en: 'The shapes here are as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的形状如下：
- en: X = (2, 4, 1)
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: X = (2, 4, 1)
- en: Y = (2, 1)
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: Y = (2, 1)
- en: '**One-hot format**'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '**One-hot 格式**'
- en: 'With one-hot encoding, we have the same dimensions as input, but the value
    length changes. In the preceding figure, we can see the values ([1], [2], …) with
    one-dimensionality. But with one-hot encoding, these values will change to vectors,
    so the shape would be as follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 使用one-hot编码后，我们的输入维度相同，但值的长度发生了变化。在前面的图中，我们可以看到值（[1]，[2]，…）是单维的。但使用one-hot编码后，这些值将变成向量，因此形状将如下所示：
- en: '![Figure 4.34: One-hot format](img/C13550_04_34.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图4.34：One-hot格式](img/C13550_04_34.jpg)'
- en: 'Figure 4.34: One-hot format'
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4.34：One-hot格式
- en: X = (2, 4, 3)
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: X = (2, 4, 3)
- en: Y = (2, 3)
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: Y = (2, 3)
- en: To perform all these changes to the dimensions, the **reshape** method from
    the NumPy library will be used.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行所有这些维度变化，将使用NumPy库中的**reshape**方法。
- en: Note
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: With this knowledge of dimensions, you can start the activity, and remember,
    the input dimension of an LSTM is three and the output dimension is two. So, if
    you create two LSTM layers continuously, how can you add the third dimension to
    the output of the first layer? Change the return state to True.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解维度的知识，你可以开始进行活动，记住，LSTM的输入维度是三，而输出维度是二。那么，如果你连续创建两层LSTM，如何将第三个维度添加到第一层的输出中呢？将返回状态设置为True。
- en: 'Activity 4: Predict the Next Character in a Sequence'
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 4：预测序列中的下一个字符
- en: In this activity, we will predict the upcoming character in a long sequence.
    The activity has to be performed using one-hot encoding to create the input and
    output vectors. The architecture of the model will be an LSTM, as we saw in *Exercise
    14*, *Predict Houses Prices with an RNN*.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将预测长序列中的下一个字符。这个活动必须使用one-hot编码来创建输入和输出向量。模型的架构将是LSTM，就像我们在*练习14*中看到的那样，*使用RNN预测房价*。
- en: 'Scenario: You work in a global company as the security manager. One morning,
    you notice a hacker has discovered and changed all the passwords for the company''s
    databases. You and your team of engineers start trying to decode the hacker''s
    passwords to enter the system and fix everything. After analyzing all the new
    passwords, you see a common structure.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 场景：你在一家全球公司担任安全经理。某天早晨，你发现黑客已经发现并更改了公司数据库的所有密码。你和你的工程师团队开始尝试解码黑客的密码，以进入系统并修复所有问题。分析所有新密码后，你发现了一个共同的结构。
- en: You only need to decode one more character in the password, but you don't know
    what the character is and you only have one more opportunity to get the correct
    password.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需要解码密码中的一个字符，但你不知道那个字符是什么，而且你只有一次机会输入正确的密码。
- en: Then, you decide to create a program that analyzes long sequences of data and
    the five characters of the password you already know. With this information, it
    can predict the last character of the password.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你决定创建一个程序，分析长序列的数据以及你已经知道的五个密码字符。通过这些信息，它可以预测密码的最后一个字符。
- en: 'The first five characters of the password are: tyuio. What will the last character
    be?'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 密码的前五个字符是：tyuio。最后一个字符会是什么？
- en: Note
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You have to use one-hot encoding and LSTM. You will train your model with one-hot
    encoded vectors.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须使用one-hot编码和LSTM。你将使用one-hot编码向量训练模型。
- en: 'This is the sequence of data: qwertyuiopasdfghjklñzxcvbnm'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是数据序列：qwertyuiopasdfghjklñzxcvbnm
- en: Note
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'This sequence is repeated 100 times, so do this: sequence = ''qwertyuiopasdfghjklñzxcvbnm''
    * 100.'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个序列重复了100次，所以这样做：sequence = 'qwertyuiopasdfghjklñzxcvbnm' * 100。
- en: Divide the data into sequences of five characters and prepare the output data.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据划分为五个字符一组，并准备输出数据。
- en: Encode the input and the output sequences as one-hot encoded vectors.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入和输出序列编码为one-hot编码向量。
- en: Set the train and test data.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置训练数据和测试数据。
- en: Design the model.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设计模型。
- en: Note
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The output has many zeroes, so it is hard to achieve an exact result. Use the
    LeakyRelu activation function with an alpha of 0.01, and when you do the prediction,
    round off the value of that vector.
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出包含许多零，因此很难达到精确的结果。使用LeakyRelu激活函数，alpha值为0.01，当进行预测时，将该向量的值四舍五入。
- en: Train and evaluate it.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练并评估它。
- en: Create a function that, when given five characters, predicts the next one in
    order to work out the last character of the password.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，当给定五个字符时，预测下一个字符，以便找出密码的最后一个字符。
- en: Note
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 308.
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第308页找到。
- en: Summary
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: AI and deep learning are making huge advances in terms of images and artificial
    vision thanks to convolutional networks. But RNNs also have a lot of power.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: AI和深度学习在图像和人工视觉方面正在取得巨大进展，这要归功于卷积网络。但RNN也拥有强大的能力。
- en: In this chapter, we reviewed how a neural network would can to predict the values
    of a sine function using temporal sequences. If you change the training data,
    this architecture can learn about stock movements for each distribution. Also,
    there are many architectures for RNNs, each of which is optimized for a certain
    task. But RNNs have a problem with vanishing gradients. A solution to this problem
    is a new model, called LSTM, which changes the structure of a neuron to memorize
    timesteps.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们回顾了神经网络如何使用时间序列来预测正弦函数的值。如果你改变训练数据，这种架构可以学习每个分布的股票走势。此外，RNN有许多不同的架构，每种架构都针对特定任务进行了优化。但是，RNN存在梯度消失的问题。解决这个问题的一种方法是新模型——长短期记忆网络（LSTM），它通过改变神经元的结构来记住时间步长。
- en: Focusing on linguistics, statistical LMs have many problems related with computational
    load and distribution probabilities. To solve the sparsity problem, the size of
    the n-gram model was lowered to 4 or 3 grams, but that was an insufficient number
    of steps back to predict an upcoming word. If we use this approach, the sparsity
    problem appears. A neural LM with a fixed window size can prevent the sparsity
    problem, but there are still problems with the limited size of the window and
    the weights. With RNNs, these problems do not arise, and depending on the architecture,
    it can obtain better results, looking many steps back and forward. But deep learning
    is about vectors and numbers. When you want to predict words, you need to encode
    the data to train the model. There are various different methods, such as the
    one-hot encoder or the label encoder. You can now generate text from a trained
    corpus and an RNN.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 聚焦于语言学，统计语言模型（LM）存在许多与计算负载和分布概率相关的问题。为了解决稀疏性问题，n-gram模型的大小被降低到4或3个gram，但这个步长不足以预测下一个词。如果我们使用这种方法，稀疏性问题仍然会出现。一个具有固定窗口大小的神经语言模型（LM）可以避免稀疏性问题，但仍然存在窗口大小有限和权重的限制问题。使用RNN时，这些问题不会出现，并且根据架构的不同，它可以获得更好的结果，能够向前和向后看很多步。但是深度学习是关于向量和数字的。当你想要预测单词时，你需要对数据进行编码以训练模型。有多种不同的方法，例如独热编码（one-hot
    encoder）或标签编码（label encoder）。现在，你可以从已训练的语料库和RNN中生成文本。
- en: In the next chapter, we will talk about Convolutional Neural Networks (CNNs).
    We will review the fundamental techniques and architectures of CNNs, and also
    look at more complex implementations, such as transfer learning.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论卷积神经网络（CNN）。我们将回顾CNN的基本技术和架构，并进一步探讨更复杂的实现方法，例如迁移学习。
