- en: Text Classification Using Recurrent Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用循环神经网络进行文本分类
- en: Recurrent neural networks are useful for solving problems where data involves
    sequences. Some examples of applications involving sequences are seen in text
    classification, time series prediction, the sequence of frames in videos, DNA
    sequences, and speech recognition.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络对于解决涉及序列的数据问题非常有用。一些涉及序列的应用实例包括文本分类、时间序列预测、视频帧序列、DNA序列以及语音识别。
- en: In this chapter, we will develop a sentiment (positive or negative) classification
    model using a recurrent neural network. We will begin by preparing the data for
    developing the text classification model, followed by developing a sequential
    model, compiling the model, fitting the model, evaluating the model, prediction,
    and model performance assessment using a confusion matrix. We will also review
    some tips for sentiment classification performance optimization.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用循环神经网络开发一个情感（正面或负面）分类模型。我们将从准备数据开始，开发文本分类模型，然后是构建顺序模型、编译模型、拟合模型、评估模型、预测以及使用混淆矩阵评估模型性能。我们还将回顾一些优化情感分类性能的小贴士。
- en: 'More specifically, in this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，本章将涵盖以下主题：
- en: Preparing data for model building
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为模型构建准备数据
- en: Developing a recurrent neural network model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发一个循环神经网络模型
- en: Fitting the model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拟合模型
- en: Model evaluation and prediction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估与预测
- en: Performance optimization tips and best practices
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能优化建议与最佳实践
- en: Preparing data for model building
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为模型构建准备数据
- en: In this chapter, we'll be using the **Internet Movie Database** (**IMDb**) movie
    reviews text data that's available in the Keras package. Note that there is no
    need to download this data from anywhere as it can be easily accessed from the
    Keras library using code that we will discuss soon. In addition, this dataset
    is preprocessed so that text data is converted into a sequence of integers. We
    cannot use text data directly for model building, and such preprocessing of text
    data into a sequence of integers is necessary before the data can be used as input
    for developing deep learning networks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用**互联网电影数据库**（**IMDb**）的电影评论文本数据，该数据可通过Keras包获取。请注意，您不需要从任何地方下载此数据，因为它可以通过我们稍后将讨论的代码轻松地从Keras库中获取。此外，这个数据集已经过预处理，文本数据已被转换为整数序列。我们不能直接使用文本数据来构建模型，因此，文本数据转化为整数序列的预处理是开发深度学习网络前的必要步骤。
- en: 'We will start by loading the `imdb` data using the `dataset_imdb` function,
    where we will also specify the number of most frequent words as 500 using `num_words`.
    Then, we''ll split the `imdb` data into `train` and `test` datasets. Let''s take
    a look at the following code to understand this data:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过使用`dataset_imdb`函数加载`imdb`数据，并使用`num_words`参数指定最频繁的单词数量为500。然后，我们将把`imdb`数据拆分为`train`和`test`数据集。让我们看看以下代码来理解这些数据：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s take a look at the preceding code:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看前面的代码：
- en: '`train_x` and `test_x` contain integers representing reviews in the train and
    test data, respectively.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_x`和`test_x`包含分别表示训练数据和测试数据中评论的整数。'
- en: Similarly, `train_y` and `test_y` contain `0` and `1` labels, representing negative
    and positive sentiments, respectively.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，`train_y`和`test_y`包含`0`和`1`标签，分别表示负面和正面情感。
- en: Using the `length` function, we can see that both `train_x` and `test_x` are
    based on 25,000 movie reviews each.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`length`函数，我们可以看到`train_x`和`test_x`都基于各自的25,000条电影评论。
- en: The tables for `train_y` and `test_y` show that there is an equal number of
    positive (12,500) and negative (12,500) reviews in the train and test data.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_y`和`test_y`的表格显示训练和测试数据中正面评论（12,500条）和负面评论（12,500条）的数量相等。'
- en: Having such a balanced dataset is useful in avoiding any bias due to class imbalance
    issues.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有如此平衡的数据集有助于避免由于类别不平衡问题而导致的偏差。
- en: The words in the movie review are represented by unique integers and each integer
    that is assigned to a word is based on its overall frequency in the dataset. For
    example, integer 1 represents the most frequent word, while integer 2 represents
    the second most frequent word, and so on. In addition, integer 0 is not used for
    any specific word but it indicates an unknown word.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 电影评论中的单词通过唯一的整数来表示，每个分配给单词的整数是基于其在数据集中的总体频率。例如，整数1表示最频繁的单词，而整数2表示第二频繁的单词，依此类推。此外，整数0并不代表任何特定的单词，而是表示一个未知的单词。
- en: 'Let''s take a look at the third and sixth sequences in the `train_x` data using
    the following code:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用以下代码查看`train_x`数据中的第三个和第六个序列：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'From the preceding code and output, we can observe the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码和输出中，我们可以观察到以下情况：
- en: From the output of the third movie review-related sequence of integers, we can
    observe that the third review contains 141 integers between 1 (1st integer) and
    369 (16th integer).
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从第三个电影评论相关的整数序列的输出中，我们可以观察到第三个评论包含141个整数，范围从1（第一个整数）到369（第16个整数）。
- en: Since we restricted the use of the most frequent words to 500, for the third
    review, there is no integer larger than 500.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们将最常见的单词限制在500个以内，因此对于第三个评论，不存在大于500的整数。
- en: Similarly, from the output of the sixth review's related sequence of integers,
    we can observe that the sixth review contains 43 integers between 1 (1st integer)
    and 226 (35th integer).
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样地，从第六个评论相关的整数序列的输出中，我们可以观察到第六个评论包含43个整数，范围从1（第一个整数）到226（第35个整数）。
- en: Looking at the length of the first six sequences in the `train_x` data, we can
    observe that the length of the movie review varies between 43 (6th review in train
    data) and 550 (4th review in train data). Such variation in the length of the
    movie reviews is normal and is as expected.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看`train_x`数据中前六个序列的长度，我们可以观察到电影评论的长度在43（train数据中的第六个评论）到550（train数据中的第四个评论）之间变化。电影评论长度的这种变化是正常的，并且是预期的。
- en: Before we can develop a movie review sentiment classification model, we need
    to find a way to make the length of a sequence of integers the same for all the
    movie reviews. We can achieve this by padding sequences.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开发电影评论情感分类模型之前，我们需要找到一种方法，使所有电影评论的整数序列长度相同。我们可以通过填充序列来实现这一点。
- en: Padding sequences
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充序列
- en: 'Padding the text sequences is carried out to ensure that all the sequences
    have the same length. Let''s take a look at the following code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 填充文本序列是为了确保所有序列具有相同的长度。让我们看看以下代码：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'From the preceding code, we can observe the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以观察到以下情况：
- en: We can achieve equal length for all the sequences of integers with the help
    of the `pad_sequences` function and by specifying a value for `maxlen`.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过`pad_sequences`函数并指定`maxlen`的值来实现所有整数序列的等长。
- en: In this example, we have restricted the length of each movie review sequence
    in the train and test data to 100\. Note that before padding of sequences, the
    structure of `train_x` and `test_x` is a list of 25,000 reviews.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将每个训练和测试数据中的每个电影评论序列的长度限制为100。请注意，在填充序列之前，`train_x`和`test_x`的结构是一个包含25,000条评论的列表。
- en: However, after padding the sequences, the structure for both changes to a matrix
    that's 25,000 x 100\. This can be easily verified by running `str(train_x)` before
    and after padding.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，在填充序列之后，两者的结构都变成了一个25,000 x 100的矩阵。通过在填充之前和之后运行`str(train_x)`可以轻松验证这一点。
- en: 'To observe the impact of padding on a sequence of integers, let''s take a look
    at the following code, along with its output:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察填充对整数序列的影响，让我们看看以下代码及其输出：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output of the third sequence of integers after padding of the `train_x` can
    be seen in the preceding code. Here, we can observe the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在`train_x`填充后，可以在前述代码中看到第三个整数序列的输出。在这里，我们可以观察到以下情况：
- en: The third sequence now has a length of 100\. The third sequence originally had
    141 integers and we can observe that 41 integers that were located at the beginning
    of the sequence have been truncated.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个序列现在的长度是100。第三个序列原本有141个整数，我们可以观察到序列开头的41个整数被截断了。
- en: On the other hand, the output of the sixth sequence shows a different pattern.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，第六个序列的输出显示出不同的模式。
- en: The sixth sequence originally had a length of 43, but now 57 zeros have been
    added to the beginning of the sequence to artificially extended the length to
    100.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第六个序列原本长度为43，但现在在序列开头添加了57个零，将长度人为地扩展为100。
- en: All 25,000 sequences of integers related to movie reviews in each of the train
    and test data are impacted in a similar way.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有25,000个与电影评论相关的整数序列，在每个训练和测试数据中都以类似的方式受到影响。
- en: In the next section, we will develop an architecture for a recurrent neural
    network that will be used for developing a movie review sentiment classification
    model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将为递归神经网络开发一个架构，用于开发电影评论情感分类模型。
- en: Developing a recurrent neural network model
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发递归神经网络模型
- en: 'In this section, we will develop the architecture for the recurrent neural
    network and compile it. Let''s look at the following code:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将开发递归神经网络的架构并对其进行编译。让我们来看一下下面的代码：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We start by initializing the model using the `keras_model_sequential` function.
    Then, we add embedding and simple **recurrent neural network** (**RNN**) layers.
    For the embedding layer, we specify `input_dim` to be 500, which is the same as
    the number of most frequent words that we had specified earlier. The next layer
    is a simple RNN layer, with the number of hidden units specified as 8.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过`keras_model_sequential`函数初始化模型。然后，我们添加嵌入层和简单的**递归神经网络**（**RNN**）层。对于嵌入层，我们指定`input_dim`为500，这与我们之前指定的最常见单词数量相同。接下来是一个简单的RNN层，隐藏单元的数量指定为8。
- en: Note that the default activation function for the `layer_simple_rnn` layer is
    a hyperbolic tangent (tanh), which is an S-shaped curve where the output ranges
    from -1 to +1.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`layer_simple_rnn`层的默认激活函数是双曲正切（tanh），它是一个S形曲线，输出范围是-1到+1。
- en: The last dense layer has one unit to capture movie review sentiment (positive
    or negative) with the activation function sigmoid. When an output lies between
    0 and 1, as in this case, it is convenient for interpretation as it can be thought
    of as a probability.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层密集层有一个单元，用来捕捉电影评论的情感（正面或负面），其激活函数为sigmoid。当输出在0和1之间时，如本例所示，它便于解释，可以视为一个概率。
- en: Note that the sigmoid activation function is an S-shaped curve where the output
    ranges between 0 and 1.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，sigmoid激活函数是一个S形曲线，其中输出的范围在0到1之间。
- en: Now, let's look at the model summary and understand how we can calculate on
    the number of parameters that are required.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下模型摘要，理解如何计算所需的参数数量。
- en: Calculation of parameters
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数计算
- en: 'The summary of the RNN model is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: RNN模型的摘要如下：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The number of parameters for the embedding layer is arrived at by multiplying
    500 (number of most frequent words) and 32 (output dimension) to obtain 16,000\.
    To arrive at the number of parameters for the simple RNN layer, we use *(h(h+i)
    + h)*, where *h* represents the number of hidden units and *i* represents the
    input dimension for this layer. In this case, this is 32.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的参数数量是通过将500（最常见的单词数量）与32（输出维度）相乘得到的16,000。为了计算简单RNN层的参数数量，我们使用*(h(h+i) +
    h)*，其中*h*代表隐藏单元的数量，*i*代表该层的输入维度。在这种情况下，这个值是32。
- en: Thus, we have (8(8 + 32)+8) = 328 parameters.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有（8（8 + 32）+8）= 328个参数。
- en: Note that if we consider a fully connected dense layer here, we would have obtained
    (8 x 32 + 8) = 264\. However, the additional 64 parameters are due to the fact
    that we use recurrent layers to capture sequences in the text data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们在这里考虑一个全连接的密集层，我们将得到（8 x 32 + 8）= 264个参数。然而，额外的64个参数是由于我们使用递归层来捕捉文本数据中的序列信息。
- en: In recurrent layers, information from the previous input is also used, which
    leads to these extra parameters that we can see here. This is the reason why RNNs
    are better suited for handling sequence data compared to a regular densely connected
    neural network layer. For the last layer, which is a dense layer, we have (1 x
    8 + 1) = 9 parameters. Overall, this architecture has 16,337 parameters.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归层中，除了当前输入信息外，前一个输入的信息也被使用，这导致了这些额外的参数，这就是为什么RNN比普通的全连接神经网络层更适合处理序列数据的原因。对于最后一层，即密集层，我们有（1
    x 8 + 1）= 9个参数。总的来说，这个架构有16,337个参数。
- en: In recurrent layers, the use of information from the previous input helps to
    provide a better representation of a sequence that is present in text or similar
    data that contains some kind of sequence.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在递归层中，利用来自前一个输入的信息有助于更好地表示文本或类似数据中的序列。
- en: Compiling the model
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译模型
- en: 'The code for compiling the model is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 编译模型的代码如下：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We compile the model with the `rmsprop` optimizer, which is recommended for
    recurrent neural networks. We make use of `binary_crossentropy` as the loss function
    due to a binary type of response since movie reviews are either positive or negative.
    Finally, for metrics, we have specified accuracy.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`rmsprop`优化器来编译模型，这对于递归神经网络（RNN）是推荐的。由于电影评论的反馈是二元的（正面或负面），我们使用`binary_crossentropy`作为损失函数。最后，对于评估指标，我们指定了准确率。
- en: In the next section, we will use this architecture to develop a movie review
    sentiment classification model that uses recurrent neural networks.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将使用此架构来开发一个使用递归神经网络的电影评论情感分类模型。
- en: Fitting the model
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'The code for fitting the model is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合模型的代码如下：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For fitting the model,  we will make use of a 20% validation split, which uses
    20,000 movie review data from training data for building the model. The remaining
    5,000 movie review training data is used for assessing validation in the form
    of loss and accuracy. We run 10 epochs with a batch size of 128.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了拟合模型，我们将使用20%的验证数据划分，其中20,000条电影评论数据来自训练数据，用于构建模型。剩余的5,000条电影评论训练数据则用于评估验证，以损失和准确率的形式进行评估。我们将运行10个epoch，每个batch大小为128。
- en: When using a validation split, it is important to note that, with 20%, it uses
    the first 80% of the training data for training and the last 20% of the training
    data for validation. Thus, if the first 50% of the review data was negative and
    the last 50% was positive, the 20% validation split will cause model validation
    to be based only on positive reviews. Therefore, before using a validation split,
    we must verify that this is not the case; otherwise, it will introduce significant
    bias.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用验证数据划分时，需要注意的是，20%的划分使用训练数据的前80%进行训练，最后20%的训练数据用于验证。因此，如果前50%的评论数据是负面的，后50%是正面的，那么20%的验证数据划分将使得模型验证仅基于正面评论。因此，在使用验证数据划分之前，我们必须验证这一点，避免出现这种情况，否则会引入显著的偏差。
- en: Accuracy and loss
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准确率和损失
- en: 'The accuracy and loss values after 10 epochs for training and validation data
    using `plot(model_one)` can be seen in the following graph:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`plot(model_one)`显示的训练和验证数据的准确率和损失值（在10个epoch后）可以在以下图表中看到：
- en: '![](img/296b64bb-74b6-4079-b07d-d50070d75845.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/296b64bb-74b6-4079-b07d-d50070d75845.png)'
- en: 'From the preceding graph, the following observations can be made:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，可以得出以下结论：
- en: The training loss continues to decrease from epoch 1 to 10.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练损失从第1个epoch到第10个epoch持续下降。
- en: Validation loss reduces initially, but it starts to look flat after 3 epochs.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证损失最初减少，但在第3个epoch后开始趋于平稳。
- en: A similar pattern is also observed for accuracy in the opposite direction.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在相反的方向上，准确率也观察到了类似的模式。
- en: In the next section, we will evaluate the classification model and assess model
    prediction performance with the help of train and test data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将评估分类模型，并通过训练和测试数据评估模型的预测性能。
- en: Model evaluation and prediction
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估与预测
- en: First, we will evaluate the model based on the train data for loss and accuracy.
    We will also obtain a confusion matrix based on the train data. The same process
    shall be repeated with the test data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将基于训练数据评估模型的损失和准确率。同时，我们还将基于训练数据获取混淆矩阵。对于测试数据，也会重复相同的过程。
- en: Training the data
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据
- en: 'We will use the `evaluate` function to obtain the loss and accuracy values,
    as shown in the following code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`evaluate`函数来获取损失和准确率值，如以下代码所示：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As seen from the preceding output, the loss and accuracy values based on the
    training data are 0.406 and 0.821, respectively.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的输出可以看出，基于训练数据的损失和准确率分别为0.406和0.821。
- en: 'Predictions using training data are used for developing a confusion matrix,
    as shown in the following code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用训练数据进行的预测将用于生成混淆矩阵，如以下代码所示：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following observations can be made by looking at the preceding confusion
    matrix:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察前面的混淆矩阵，可以得出以下结论：
- en: There are 9,778 movie reviews that are correctly classified as negative and
    there are 10,738 movie reviews that are correctly classified as positive. We can
    observe that the model does a decent job of classifying the reviews as positive
    or negative.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有9,778条电影评论被正确分类为负面，10,738条电影评论被正确分类为正面。我们可以观察到，模型在将评论分类为正面或负面方面表现得相当不错。
- en: Looking at the misclassifications, we can also observe that, on 2,722 occasions,
    negative movie reviews are misclassified as positive movie reviews. This is relatively
    higher compared to the misclassification of positive reviews as negative (1,762
    times) by the classification model.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过查看错误分类，我们还可以观察到，负面电影评论被错误地分类为正面评论的次数为2,722次。与将正面评论错误分类为负面评论（1,762次）相比，这一错误分类的频率较高。
- en: Next, let's do a similar assessment based on test data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们基于测试数据做一个类似的评估。
- en: Testing the data
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试数据
- en: 'The code to obtain the loss and accuracy values is as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 获取损失和准确率值的代码如下：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we can see that the loss and accuracy based on the test data are 0.467
    and 0.778, respectively. These results are slightly inferior to what we observed
    for the train data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到基于测试数据的损失和准确率分别为0.467和0.778。这些结果略逊色于我们在训练数据中观察到的结果。
- en: 'Next, we''ll predict the classes for the test data and use the results to obtain
    a confusion matrix, as shown in the following code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将预测测试数据的类别，并使用结果生成混淆矩阵，代码如下所示：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Apart from the overall results being slightly inferior to the ones that we obtained
    from the train data, we can't see any major differences between the train and
    test data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 除了整体结果略逊色于我们从训练数据中获得的结果外，我们没有看到训练数据和测试数据之间的重大差异。
- en: In the next section, we will explore a few strategies to improve model performance.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探索一些改进模型性能的策略。
- en: Performance optimization tips and best practices
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能优化技巧和最佳实践
- en: When developing a recurrent neural network model, we come across situations
    where we need to make several decisions related to the network. These decisions
    could include trying a different activation function rather than the default one
    that we had used. Let's make such changes and see what impact they have on the
    movie review sentiment classification performance of the model.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发循环神经网络模型时，我们会遇到需要做出多个与网络相关的决策的情况。这些决策可能包括尝试不同的激活函数，而不是我们之前使用的默认函数。让我们做出这样的更改，并观察它们对电影评论情感分类模型的性能产生什么影响。
- en: 'In this section, we will experiment with the following four factors:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实验以下四个因素：
- en: Number of units in the simple RNN layer
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单RNN层中的单元数量
- en: Using different activation functions in the simple RNN layer
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在简单RNN层中使用不同的激活函数
- en: Adding more recurrent layers
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加更多的循环层
- en: Changes in the maximum length for padding sequences
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充序列的最大长度变化
- en: Number of units in the simple RNN layer
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单RNN层中的单元数量
- en: 'The code for incorporating this change and then compiling/fitting the model
    is as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 集成此更改并编译/拟合模型的代码如下：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, we change the architecture by increasing the number of units in the simple
    RNN layer from 8 to 32\. Everything else is kept the same. Then, we compile and
    fit the model, as shown in the preceding code.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过将简单RNN层中的单元数量从8增加到32来改变架构，其他设置保持不变。然后，我们编译并拟合模型，如前面的代码所示。
- en: 'The accuracy and loss values after 10 epochs can be seen in the following graph:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 10个周期后的准确率和损失值可以在以下图表中看到：
- en: '![](img/a5f3db7e-27c4-4c12-a305-528783ec3d78.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5f3db7e-27c4-4c12-a305-528783ec3d78.png)'
- en: 'The preceding plot indicates the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图表显示了以下内容：
- en: A significantly bigger gap between training and validation data on epoch 3 onward.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从第3个周期开始，训练数据和验证数据之间的差距显著增大。
- en: This clearly suggests an increased level of overfitting compared to the preceding
    plot, where the number of units in the simple RNN was 8.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这明显表明，与前面的图表相比，过拟合程度有所增加，而当时简单RNN中的单元数量为8。
- en: This is also reflected in the higher loss value of 0.585 and the lower accuracy
    value of 0.757 that we obtained for the test data based on this new model.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这也反映在我们基于这个新模型获得的测试数据上，损失值为0.585，准确率为0.757。
- en: Now, let's experiment with a different activation function in the simple RNN
    layer and see whether this overfitting issue can be resolved.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试在简单RNN层中使用不同的激活函数，看看是否能够解决这个过拟合问题。
- en: Using different activation functions in the simple RNN layer
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在简单RNN层中使用不同的激活函数
- en: 'This change can be seen in the following code:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个变化可以在以下代码中看到：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In the preceding code, we are changing the default activation function in the
    simple RNN layer to a ReLU activation function. We keep everything else the same
    as what we had in the previous experiment.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们将简单RNN层中的默认激活函数更改为ReLU激活函数。我们保持与上一个实验中相同的其他设置。
- en: 'The accuracy and loss values after 10 epochs can be seen in the following graph:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 10个周期后的准确率和损失值可以在以下图表中看到：
- en: '![](img/381e8b5e-2d09-42d2-bdce-ff8d0b3feb7c.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/381e8b5e-2d09-42d2-bdce-ff8d0b3feb7c.png)'
- en: 'From the preceding plot, we can observe the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 从上面的图表中，我们可以观察到以下几点：
- en: The loss and accuracy values look much better now.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，损失和准确率的值看起来好多了。
- en: Both the loss and accuracy curves based on training and validation are now closer
    to each other.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在，基于训练和验证的损失与准确率曲线更加接近。
- en: We used the model to find the loss and accuracy values based on the test data
    that we obtained, that is, 0.423 and 0.803, respectively. This shows better results
    compared to the results we've obtained so far.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用模型根据获得的测试数据计算损失和准确度值，分别为0.423和0.803。与我们迄今为止获得的结果相比，这显示了更好的效果。
- en: Next, we will experiment further by adding more recurrent layers. This will
    help us build a deeper recurrent neural network model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过添加更多的循环层进一步实验。这将帮助我们构建一个更深的循环神经网络模型。
- en: Adding more recurrent layers
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 添加更多的循环层
- en: 'Now, we will experiment by adding two additional recurrent layers to the current
    network. The code that''s incorporating this change is as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过向当前网络添加两个额外的循环层来进行实验。包含此更改的代码如下：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When we add these additional recurrent layers, we also set `return_sequences`
    to `TRUE`. We keep everything else the same and compile/fit the model. The plot
    for the loss and accuracy values based on the training and validation data is
    as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们添加这些额外的循环层时，我们还将`return_sequences`设置为`TRUE`。我们保持其他所有设置不变，然后编译并拟合模型。基于训练和验证数据的损失和准确度图表如下：
- en: '![](img/82787f84-dbc6-4bba-a200-7fe4294bfbf3.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/82787f84-dbc6-4bba-a200-7fe4294bfbf3.png)'
- en: 'From the preceding plot, we can observe the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以观察到以下内容：
- en: After 10 epochs, the loss and accuracy values for training and validation show
    a reasonable level of closeness, indicating the absence of overfitting.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在10个epoch之后，训练和验证的损失与准确度值表现出合理的接近度，表明没有发生过拟合。
- en: The loss and accuracy based on the test data we calculated show a decent improvement
    in the results with 0.403 and 0.816, respectively.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于测试数据计算出的损失和准确度值分别为0.403和0.816，结果显示出明显的改进。
- en: This shows that deeper recurrent layers did help capture sequences of words
    in the movie reviews in a much better way. This, in turn, enabled improved classification
    of the sentiment in movie reviews as positive or negative.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这表明，增加更深的循环层确实有助于更好地捕捉电影评论中的词序列。反过来，这也促使我们能够更好地将电影评论的情感分类为积极或消极。
- en: The maximum length for padding sequences
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充序列的最大长度
- en: 'So far, we have used a maximum length of 100 for padding sequences of movie
    reviews in the train and test data. Let''s look at the summary of the length of
    movie reviews in the `train` and `test` data using the following code:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已使用最大长度为100来填充电影评论序列的训练和测试数据。我们可以通过以下代码查看`train`和`test`数据中电影评论长度的总结：
- en: '[PRE15]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'From the preceding code, we can make the following observations:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以得出以下观察结果：
- en: From the summary of the length of movie reviews in the train data, we can see
    that the minimum length is 11, the maximum length is 2,494, and that the median
    length is 178.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从训练数据中电影评论长度的总结中，我们可以看到最短长度为11，最长长度为2,494，中位数长度为178。
- en: Similarly, the test data has a minimum review length of 7, a maximum length
    of 2,315, and a median length of 174.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，测试数据的最短评论长度为7，最长评论长度为2,315，中位数长度为174。
- en: Note that when the maximum padding length is below the median (which is the
    case with a maximum length of 100), we tend to truncate more movie reviews by
    removing words beyond 100\. At the same time, when we choose a maximum length
    for padding to be significantly above the median, we will have a situation where
    a higher number of movie reviews will need to contain zeros and fewer number of
    reviews will be truncated.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当最大填充长度低于中位数（例如最大长度为100时），我们通常会通过去除超过100的单词来截断更多的电影评论。同时，当我们选择的填充最大长度远高于中位数时，将会出现更多的电影评论需要包含零，而较少的评论会被截断。
- en: 'In this section, we are going to explore the impact of keeping the maximum
    length of the sequence of words in the movie reviews near the median value. The
    code for incorporating this change is as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将探讨将电影评论中词序列的最大长度保持在接近中位数值的影响。包含此更改的代码如下：
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: From the preceding code, we can see that we run the model after specifying `maxlen`
    as 200\. We keep everything else the same as what we had for `model_four`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码可以看到，我们在将`maxlen`指定为200后运行模型。我们将其他所有设置保持与`model_four`相同。
- en: 'The plot for the loss and accuracy for the training and validation data is
    as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据和验证数据的损失与准确度图表如下：
- en: '![](img/43039c1f-594a-45eb-861a-3800e19aab7a.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/43039c1f-594a-45eb-861a-3800e19aab7a.png)'
- en: 'From the preceding plot, we can make the following observations:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以得出以下观察结果：
- en: There's the absence of an overfitting issue since the training and validation
    data points are very close to each other.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于训练和验证数据点非常接近，因此没有过拟合问题。
- en: The loss and accuracy based on the test data were calculated as 0.383 and 0.830,
    respectively.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于测试数据的损失和准确度分别计算为0.383和0.830。
- en: The loss and accuracy values are at their best level at this stage.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这一阶段，损失和准确度值达到了最佳水平。
- en: 'The confusion matrix based on the test data is as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 基于测试数据的混淆矩阵如下所示：
- en: '[PRE17]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'From the confusion matrix, we can make the following observations:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从混淆矩阵中，我们可以做出以下观察：
- en: This classification model seems to performs slightly better when correctly classifying
    the movie review as positive (10,681) compared to when classifying a negative
    (10,066) review correctly.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该分类模型在正确分类电影评论为正面（10,681）时表现略好于正确分类负面（10,066）评论的情况。
- en: As far as reviews that are classified incorrectly are concerned, the trend that
    we had observed earlier, where negative movie reviews were mistakenly classified
    by the model as positive being on the higher side, exists in this case too.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就被错误分类的评论而言，我们之前观察到的趋势仍然存在，即负面电影评论被模型错误分类为正面评论的情况较多，这在此案例中也是如此。
- en: In this section, we experimented with a number of units, activation functions,
    the number of recurrent layers in the network, and the amount of padding in order
    to improve the movie review sentiment classification model. Some other factors
    that you could explore further include the number of most frequent words to include
    and changing the maximum length at the time of padding sequences.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们尝试了多个单元、激活函数、网络中的递归层数以及填充量，以改进电影评论情感分类模型。您还可以进一步探索的其他因素包括要包含的最常见单词数量和在填充序列时更改最大长度。
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we illustrated the use of the recurrent neural network model
    for text sentiment classification using IMDb movie review data. Compared to a
    regular densely connected network, recurrent neural networks are better suited
    to deal with data that has sequences in it. Text data is one such example that
    we worked with in this chapter.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了使用递归神经网络模型进行文本情感分类，数据来自IMDb电影评论。与常规的全连接网络相比，递归神经网络更适合处理包含序列的数据。文本数据就是我们在本章中使用的一个例子。
- en: In general, deep networks involve many factors or variables, and this calls
    for some amount of experimentation involving making changes to the levels for
    such factors before arriving at a useful model. In this chapter, we also developed
    five different movie review sentiment classification models.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，深度网络涉及许多因素或变量，这需要一定的实验，涉及在得出有用模型之前，对这些因素的级别进行调整。本章中，我们还开发了五种不同的电影评论情感分类模型。
- en: A variant of recurrent neural networks that has become popular is **Long Short-Term
    Memory** (**LSTM**) networks. LSTM networks are capable of learning long-term
    dependencies and help recurrent networks remember inputs for a longer time.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一种变体的递归神经网络是**长短期记忆**（**LSTM**）网络。LSTM网络能够学习长期依赖关系，并帮助递归网络记住更长时间的输入。
- en: In the next chapter, we will go over an application example of using an LSTM
    network, where we will continue to use IMDb movie review data and explore further
    improvements that can be made to the sentiment classification model's performance.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍使用LSTM网络的应用示例，我们将继续使用IMDb电影评论数据，并进一步探索可以提高情感分类模型性能的改进。
