- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Model Preparation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型准备
- en: In this chapter, you’ll learn how to decide which model will be most useful
    to serve as a basis for your pretraining regime. You’ll learn how to think about
    the size of the model in parameters, along with the key loss functions and how
    they determine performance in production. Finally, you’ll combine the scaling
    laws with the expected size of your dataset to select ceiling and floor model
    sizes that you’ll use to guide your experiments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何决定哪个模型最适合作为你预训练方案的基础。你将学会如何考虑模型的参数大小，以及关键的损失函数和它们如何在生产中决定性能。最后，你将结合缩放法则和数据集的预期大小，选择模型的上限和下限大小，以指导你的实验。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Finding your best base model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找最佳基础模型
- en: Finding your pretraining loss function
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找你的预训练损失函数
- en: Solving for your model size
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决模型大小问题
- en: Planning future experiments
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规划未来的实验
- en: Finding your best base model
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找最佳基础模型
- en: 'At this point in the book, you should have learned how to pick your use case,
    how to find a dataset, and how to compare that with research datasets. You should
    have particularly learned how to compare that dataset with those available in
    the open source community. Now comes the fun part: picking your model!'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到了本书的这一部分，你应该已经学会了如何选择使用场景，如何找到数据集，以及如何将其与研究数据集进行对比。你特别应该已经学会了如何将这个数据集与开源社区中可用的数据集进行对比。接下来是有趣的部分：选择你的模型！
- en: Most likely, you already have a few candidates in mind. If you’re working with
    natural language, you’re probably thinking about something in the family of **Generative
    Pretrained Transformers** (**GPT**) for a generative use case, BERT for classification,
    or T5 for something akin to translation. For vision, you may be looking at CoCa
    *(1)*, CLIP *(2)*, or a jointly masked vision and language model *(3)*. For multimodal
    datasets, you might pick one straight from the vision examples or something much
    more unique based on your specific use case.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，你已经心中有几个候选模型。如果你在处理自然语言，可能会考虑**生成预训练变换器**（**GPT**）系列中的某个模型用于生成任务，BERT用于分类任务，或者T5用于类似翻译的任务。如果是视觉任务，你可能会选择CoCa
    *(1)*、CLIP *(2)*，或者联合掩码的视觉和语言模型 *(3)*。对于多模态数据集，你可以选择直接来自视觉领域的模型，或者根据具体的使用场景选择一个更独特的模型。
- en: 'In [*Chapter 1*](B18942_01.xhtml#_idTextAnchor016), *An Introduction to Pretraining
    Foundation Models*, we briefly introduced some of these state-of-the-art models
    and dove into the core transformer-based neural network architecture that makes
    them tick. Let’s briefly recap each of these models and reinforce why they matter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B18942_01.xhtml#_idTextAnchor016)《预训练基础模型介绍》中，我们简要介绍了这些最先进的模型，并深入探讨了它们背后的核心基于变换器的神经网络架构。让我们简要回顾一下这些模型，并强调它们为何重要：
- en: '**Encoders**. Broadly speaking, an encoder architecture takes a lengthy input,
    such as a long sentence or a long embedding, and compresses it into something
    denser. An encoder might take an input of length 500 and, through a series of
    neural networks, compress this into an output of length 50\. Encoder-only models
    were popularized by the BERT model and all of its subsequent relatives, including
    DeBERTa, RoBERTa, XLM, AlBERT, and so on. If you’re curious, DeBERTa is included
    here because, despite its updated attention mechanism, which uses a disentangling
    objective function with a novel enhanced mask decoder, it is still generally appropriate
    for classification tasks.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**。广义上讲，编码器架构接受较长的输入，比如一长句子或长嵌入，并将其压缩成更紧凑的表示。一个编码器可能会接收一个长度为500的输入，通过一系列神经网络，将其压缩成一个长度为50的输出。编码器模型通过BERT及其后续系列（包括DeBERTa、RoBERTa、XLM、AlBERT等）得到了广泛应用。如果你感兴趣，DeBERTa在这里被提到，因为尽管它采用了更新的注意力机制，使用了一个解耦目标函数并结合了新颖的增强掩码解码器，但它仍然普遍适用于分类任务。'
- en: Important note
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Pick an encoder-only model architecture if you want to keep your model smaller
    and you’re confident you won’t need any generative abilities. This means you should
    plan on not using this model for text generation, zero-shot performance, summarization,
    and the final step in question answering.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望保持模型较小，并且确信不需要任何生成能力，那么可以选择仅包含编码器的模型架构。这意味着你应该计划不使用此模型进行文本生成、零样本性能、摘要生成以及问答的最后步骤。
- en: '**Decoders**: A decoder model architecture does exactly the reverse of an encoder.
    It takes dense input, say, of length 50, and uses learnable feedforward networks
    to recompose that back into a larger space, for example, of length 250\. We’ll
    dive into the mechanics of how that happens later in this chapter (hint: it’s
    all about the *pretraining loss function*). Decoder-only models came to the global
    stage with the GPT-4 model *(4)*, and open source options such as OPT and BLOOM
    are now available.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解码器**：解码器模型架构正好与编码器相反。它接收密集输入，例如长度为50的输入，并利用可学习的前馈网络将其重新组合成更大的空间，例如长度为250的输出。我们将在本章稍后深入探讨这一过程的细节（提示：这一切都与*预训练损失函数*有关）。解码器单一模型首次进入全球舞台是通过GPT-4模型*(4)*，如今像OPT和BLOOM这样的开源选项也已问世。'
- en: Pick a decoder-only model (i.e., diffusion) if you want to focus on your model’s
    generative ability. If you need strong summarization, generation, or the ability
    to generate quality images, decoder-only models are the way to go.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想专注于模型的生成能力，可以选择解码器单一模型（即扩散模型）。如果你需要强大的总结能力、生成能力或生成高质量图像的能力，解码器单一模型是最佳选择。
- en: '**Diffusion models**: If you want to pretrain an image generation model, such
    as DALL-E 2 *(5)*, Imagen *(6)*, Flamingo *(7)*, and Stable Diffusion, then you’re
    looking at using a diffusion model. Diffusion models, which we’ll explore later
    in the book, are really interesting training systems that use multiple pretrained
    models to embed joint vision and language pairs. This ultimately joins them through
    a U-Net, which progressively adds, then removes, noise during the training process.
    The models learn to generate images by comparing the generated image from the
    provided caption and updating the model weights based on how far the image is
    from the caption provided.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩散模型**：如果你想预训练一个图像生成模型，例如DALL-E 2*(5)*、Imagen*(6)*、Flamingo*(7)*和Stable Diffusion，那么你需要使用扩散模型。扩散模型是一些非常有趣的训练系统，它们通过使用多个预训练模型来嵌入联合的视觉和语言对。最终，通过U-Net将它们连接起来，U-Net在训练过程中逐步加入噪声，然后再去除噪声。模型通过将生成的图像与给定的描述进行对比，学习如何生成图像，并根据图像与描述的差距来更新模型权重。'
- en: '**Combination encoder-decoder models**: The most common use case for a mixture
    of encoders and decoders in the same neural network today is overwhelmingly translation.
    Models in this category came to fame with T5 *(8)*, which was known to be able
    to take strings in one language paired with their translations in another at very
    large scales. T5 has since evolved into BART, FLAN-T5, M2M, MBart, BigBird, and
    others.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组合编码器-解码器模型**：当前最常见的编码器和解码器混合使用的神经网络应用场景是翻译。这个类别的模型因T5*(8)*而闻名，它能够在大规模的情况下处理一种语言的字符串与另一种语言的翻译对。T5之后发展成了BART、FLAN-T5、M2M、MBart、BigBird等模型。'
- en: Pick a combination encoder-decoder model when you are certain that translation
    is core to your use case. This might be the case for writing code from prompts,
    summarizing documents, or transferring styles.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当你确信翻译是你使用场景的核心时，选择一个组合编码器-解码器模型。这可能适用于从提示中写代码、总结文档或转移风格等场景。
- en: Now, think to yourself, which of these do I need? Hopefully, at this point,
    you can settle on one of these major categories.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想一想，你需要哪一种模型？希望此时你已经能够确定其中一个主要类别。
- en: Starting with the smallest base model you can
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从最小的基础模型开始。
- en: Throughout this chapter, we’ll learn how to solve for the size of our model
    using scaling laws. However, at this point, it’s helpful to introduce the concept
    of a **base model**. A base model is usually the absolute smallest version of
    a model available. You can find this on the Hugging Face Hub, for example, or
    on the GitHub site associated with the paper. Base models are usually in the order
    of a few hundred million parameters in size, so they tend to fit on a single GPU.
    They don’t take up a lot of GPU memory, and they fit nicely when stored on disk
    in most environments. Base models are fast in production because the literal size
    of the neural network is smaller, computations can happen faster, and the data
    has fewer layers to pass through until the final output. All these benefits mean
    that putting a base model into production and working with it across your entire
    pipeline is going to be a lot easier than working with something larger. For this
    reason, when working with customers, I strongly recommend beginning experiments
    with the smallest model you can and increasing in size only when that stops giving
    you the mileage you need.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用扩展法则来解决模型的大小问题。然而，在此时，引入**基础模型**的概念是有帮助的。基础模型通常是可用模型中最小的版本。例如，你可以在
    Hugging Face Hub 或与论文相关的 GitHub 网站上找到这些模型。基础模型通常包含几亿个参数，因此它们适合放在单个 GPU 上。它们不占用太多
    GPU 内存，在大多数环境中也可以很好地存储在磁盘上。基础模型在生产中运行非常快，因为神经网络的实际大小较小，计算可以更快地完成，数据也需要经过较少的层才能输出最终结果。所有这些好处意味着，将基础模型投入生产并在整个管道中使用它要比使用更大的模型容易得多。因此，在与客户合作时，我强烈建议从你能找到的最小模型开始实验，只有在该模型不再提供你需要的效果时，再逐步增大模型的规模。
- en: Later in this chapter, we’ll talk about when, where, and how to design experiments
    that incorporate a larger model. In [*Chapter 14*](B18942_14.xhtml#_idTextAnchor217),
    we’ll learn how to operationalize these with MLOps pipelines!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，我们将讨论何时、何地以及如何设计包含更大模型的实验。在[*第14章*](B18942_14.xhtml#_idTextAnchor217)中，我们将学习如何通过
    MLOps 管道将这些实验付诸实践！
- en: Trade-off – simplicity versus complexity
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权衡 —— 简单与复杂
- en: 'One aspect that is helpful to consider when applying machine learning is this
    simple dimension: simplicity versus complexity. A simple model may be smaller.
    It might have fewer novel operations. Its writeup on the Hugging Face Hub may
    literally be shorter. It may have fewer GitHub issues but more associated papers.
    Starting with a simple artifact is a good way to give your teams a healthy beginning.
    You want to start projects with early success rather than failing to get off the
    ground. A simple project with a simple model may be fine-tuning BERT on just a
    few GB of data. After you’ve tested this on a real use case, and when you have
    more unsupervised data, you may simply try continuing to pretrain on this unsupervised
    set.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用机器学习时，一个有帮助的方面是考虑这个简单的维度：简单与复杂。一个简单的模型可能更小，可能包含更少的创新操作。它在 Hugging Face Hub
    上的描述可能字面上更短。它可能有更少的 GitHub 问题，但可能有更多相关的论文。从一个简单的工件开始是为你的团队提供健康开端的好方法。你希望在项目初期获得成功，而不是在启动时就遇到困难。一个简单的项目，例如在几GB数据上对
    BERT 进行微调，可能是一个不错的选择。当你在实际用例上测试完后，并且有更多的无监督数据时，你可以简单地尝试继续在这个无监督数据集上进行预训练。
- en: Important note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Starting with a simple artifact is a good way to give your teams a healthy beginning.
    You want to start projects with early success rather than failing to get off the
    ground.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个简单的工件开始是为你的团队提供健康开端的好方法。你希望在项目初期获得成功，而不是在启动时就遇到困难。
- en: On the other hand, complexity may be able to boost your model performance beyond
    what is possible with simpler models. This includes scaling the models, datasets,
    and computation sizes, in addition to incorporating multiple models throughout
    your preprocessing, training, and deployment pipelines.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，复杂性可能会提升模型性能，超越简单模型所能达到的水平。这包括模型、数据集和计算规模的扩展，以及在预处理、训练和部署管道中融入多个模型。
- en: As we’ve seen throughout the book, scale alone is a promising tactic for many
    use cases. In the chapter on fine-tuning, however, we’ll explore how techniques
    such as instruction fine-tuning, chain-of-thought tuning, and reinforcement learning
    with human feedback can all boost model performance without necessarily scaling
    parameter size. These are promising trends!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中所看到的，仅仅依靠规模对许多用例来说是一种有前景的策略。然而，在微调章节中，我们将探讨如何通过指令微调、思维链微调以及带有人工反馈的强化学习等技术，在不必扩大参数规模的情况下提高模型性能。这些都是有前景的趋势！
- en: Trade-off – applying to many use cases versus being narrow to only one
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 权衡——应用于许多用例与仅限于单一用例
- en: 'Another key aspect to consider in the design of your overall solution, or product,
    is your ability to extend to as many use cases as possible. This follows the basic
    economics of maximizing a return on your investment. In this project, you will
    have two big investments:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计整体解决方案或产品时，另一个关键方面是你扩展到尽可能多用例的能力。这符合最大化投资回报的基本经济学原则。在这个项目中，你将有两个重要的投资：
- en: Your time
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的时间
- en: Your compute costs, which we’ll dive into in the upcoming chapter on GPUs
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的计算成本，我们将在接下来的关于GPU的章节中深入探讨。
- en: Both of these are allocations from your organization in order to produce some
    output, or, in this case, a model. Every use case this model is able to solve
    *is a potential path to value for your project*. Every time you fine-tune this
    model, deploy it into production for an application, use it for downstream analysis,
    or integrate it into a demonstration or report, you create a way for your organization
    to get value from its investment in your project. You set yourself up for success
    when your project is positioned to be able to solve as many use cases as you can.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者都是你所在组织的资源分配，用于产生某些输出，或者在这种情况下，生成一个模型。这个模型能够解决的每一个用例*都是你项目创造价值的潜在路径*。每次你对这个模型进行微调，将其部署到生产中用于应用，或用它进行下游分析，或者将其集成到演示或报告中时，你就为你的组织创造了从对项目投资中获取价值的途径。当你的项目能够解决尽可能多的用例时，你就为成功打下了基础。
- en: In the case of pretraining and fine-tuning, this is an easy problem to solve.
    First, look at how many models are already deployed in your organization. If these
    are transformer-based models, then odds are they are fine-tuned artifacts from
    some set of open source models. Look at those open source models as your target
    range. Do your teams use BERT? RoBERTa? GPT-2? Pick the model that covers as many
    downstream tasks as you can.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练和微调的情况下，这是一个容易解决的问题。首先，看看你组织中已经部署了多少模型。如果这些是基于变换器的模型，那么它们很可能是从某些开源模型中微调出来的产物。将这些开源模型视为你的目标范围。你的团队使用BERT吗？RoBERTa？GPT-2？选择覆盖尽可能多下游任务的模型。
- en: Alternatively, you may consider solving a much smaller number of use cases if
    these are extremely high value. Search is a great example of this. From e-commerce
    to hospitality, customer service to product delivery, when a search engine gets
    a lot of traffic, it is probably a high-value business. Search is a top application
    for large, pretrained models, especially for new projects looking to leverage
    cutting-edge technologies for the highest impact.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，如果这些用例具有极高的价值，你可以考虑解决数量更少的用例。搜索就是一个很好的例子。从电子商务到酒店业，从客户服务到产品交付，当搜索引擎获得大量流量时，它很可能是一个高价值的业务。搜索是大型预训练模型的顶级应用，特别适用于那些希望利用前沿技术产生最大影响的新项目。
- en: Tactical approaches to finding your best base model
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 寻找最佳基础模型的战术方法
- en: Practically speaking, here’s how I think about finding your best base model.
    First, as you did in [*Chapter 2*](B18942_02.xhtml#_idTextAnchor034), list out
    the key use cases you want your model to address. Look at model leader boards,
    such as those we discussed in [*Chapter 1*](B18942_01.xhtml#_idTextAnchor016),
    to see which ones seem to consistently hit the top. Consider the base architecture
    of this model and compare it to your top use cases. If you find a recent one with
    open source code samples and model weights, and it seems to map reasonably well
    to the use cases you’re exploring, then I’d use that to start with.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这是我思考如何找到最佳基础模型的方式。首先，像在[*第二章*](B18942_02.xhtml#_idTextAnchor034)中做的那样，列出你希望模型解决的关键用例。查看模型排行榜，比如我们在[*第一章*](B18942_01.xhtml#_idTextAnchor016)中讨论过的排行榜，看看哪些模型似乎始终排在前列。考虑这些模型的基础架构，并将其与您的顶级用例进行比较。如果你找到一个最近的模型，且它有开源代码样本和模型权重，并且看起来与您正在探索的用例匹配良好，那么我建议从这个模型开始。
- en: If you want to push yourself, try to reimagine lower-level aspects of that model
    as areas for improvement. This might be updates to the neural networks, combinations
    of these, or even custom operators that might improve your overall goal. Remember
    that you want to keep both accuracy and efficiency as key indicators!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想挑战自己，试着重新设想模型的低层次方面作为改进的方向。这可能是对神经网络的更新，或者是这些网络的组合，甚至是可能改善整体目标的自定义操作符。记住，你要把准确性和效率作为关键指标！
- en: 'Once you’ve settled on your best base model or set of top models you want to
    consider, it’s time to dig into a key element of this model that determines its
    ability to learn: the pretraining loss function.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你确定了最适合的基础模型或想要考虑的一组顶级模型，就该深入探讨这个模型的一个关键要素，它决定了模型的学习能力：预训练损失函数。
- en: Finding your pretraining loss function
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 找到你的预训练损失函数
- en: 'We introduced this topic in [*Chapter 1*](B18942_01.xhtml#_idTextAnchor016)
    as a *pretraining objective*, or in vision as a *pretext task*. Remember that
    these are essentially different words for the same thing: the mathematical quantity
    your model will optimize for while performing **self-supervised learning**. This
    is valuable because it opens you up to a plethora of unsupervised data, which
    is, on average, more available than supervised data. Usually, this pretraining
    function injects some type of noise and then tries to learn what the real data
    patterns look like from the false ones (**causal language modeling as with GPT**).
    Some functions inject masks and learn how to predict which words have been masked
    (**masked language modeling as with BERT**). Others substitute some words with
    reasonable alternatives that reduce the overall size of the needed dataset (**token
    detection as** **with DeBERTa**).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第1章*](B18942_01.xhtml#_idTextAnchor016)中介绍了这个话题，作为*预训练目标*，或者在视觉领域中作为*前置任务*。记住，这些本质上是不同的词语，但指的是同一件事：你的模型在执行**自监督学习**时会优化的数学量。这很有价值，因为它使你能够接触到大量的无监督数据，而这些数据通常比有监督数据更容易获得。通常，这个预训练函数会注入某种噪声，然后尝试从虚假数据中学习真实数据的模式（**如GPT的因果语言建模**）。一些函数会注入掩码并学习如何预测被掩盖的词（**如BERT的掩码语言建模**）。还有一些会用合理的替代词替换某些词，从而减少所需数据集的总体大小（**如DeBERTa的标记检测**）。
- en: Important note
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When we pretrain our models, we use a pretraining loss function to create the
    ability for the model to recognize aspects of the dataset, which eventually predicts
    truth from falsehoods.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们预训练我们的模型时，我们使用一个预训练损失函数来赋予模型识别数据集各个方面的能力，最终从虚假中预测出真实。
- en: But what gives? Why do we care about pretraining objectives so much? How will
    it impact your project? The reason you should care about a pretraining loss function
    is that it is the primary factor in determining where your model can be used and
    how well it will perform. In the previous section, we mapped certain types of
    model architectures (encoders, decoders, and mixtures) to different machine learning
    use cases (classification, generation, and translation). The real reason why this
    mapping exists is because of the pretraining loss function!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么这么重要呢？为什么我们如此关心预训练目标？它将如何影响你的项目？你应该关心预训练损失函数的原因是，它是决定你的模型可以在哪里使用以及其表现如何的主要因素。在前一节中，我们将某些类型的模型架构（编码器、解码器和混合架构）映射到不同的机器学习用例（分类、生成和翻译）。这种映射存在的真正原因就是因为预训练损失函数！
- en: Consider decoder-only models, notably GPT-3 and similar candidates. The pretraining
    function here is called *causal* because it works from left to right. The pretraining
    function picks up some base text string, say, half a sentence, and then uses the
    decoder to try to generate the rest of the sentence. Commonly, you’ll pair this
    with an objective metric, such as perplexity, to consider how close to the root
    text your generated strings were. As the training run continues, the neural network
    optimizes this perplexity metric to change the weights such that the overall loss
    decreases, pushing up the performance of your model step by step.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑仅解码器模型，尤其是GPT-3和类似的候选模型。这里的预训练函数被称为*因果*，因为它是从左到右工作的。预训练函数会从一个基础文本字符串开始，比如半句话，然后使用解码器尝试生成剩余的句子。通常，你会将这个与目标度量（比如困惑度）配对，来考虑你生成的字符串与根文本的接近程度。随着训练的继续，神经网络会优化困惑度度量，通过调整权重使得总体损失减少，一步步提高模型的性能。
- en: Interestingly, given the recent performance of GPT-based models, we’re starting
    to see these being applied across a variety of use cases with zero-shot performance.
    This means that once you train your GPT-based model at scale, you can then use
    it for prompting scenarios without providing previous examples, offering classification,
    entity extraction, sentiment analysis, question answering, and more. While you’re
    still performing text generation, strictly speaking, the way this is used can
    solve more than just open-ended content generation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，鉴于基于GPT的模型近期表现出色，我们开始看到它们在各种使用场景中得到应用，并且具备零样本性能。这意味着一旦你大规模训练了GPT模型，你就可以将其用于无须提供前置示例的提示场景，完成分类、实体抽取、情感分析、问答等任务。虽然从严格意义上说，你仍然是在进行文本生成，但这种方式的使用可以解决的不仅仅是开放性内容生成问题。
- en: Next, let’s examine the pretraining loss functions across different models in
    vision, language, and multimodal scenarios.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看在视觉、语言和多模态场景中，不同模型的预训练损失函数。
- en: Pretraining loss functions in vision – ViT and CoCa
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉中的预训练损失函数——ViT与CoCa
- en: We’ve learned a lot about the core transformer model architecture, and now you
    should feel somewhat comfortable about how this works in natural language processing.
    But what about computer vision? The Vision Transformer *(9)* took a step in this
    direction, bridging the gap from advancements in NLP and making these available
    to the vision community. Notably, the **Vision Transformer, or ViT,** showed that
    convolution could be removed entirely from the model. A pretrained ViT reduced
    the overall amount of computational resources necessary to train downstream models
    that achieved results comparable with top convolution-based approaches of its
    time.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了很多关于核心的变换器模型架构的知识，现在你应该对这种模型在自然语言处理中的工作方式有所了解。但计算机视觉呢？视觉变换器*(9)*朝这个方向迈出了重要一步，弥合了自然语言处理（NLP）中的进展，并将这些成果带给了视觉社区。特别地，**视觉变换器（ViT）**展示了完全可以从模型中去除卷积操作。一个预训练的ViT减少了训练下游模型所需的整体计算资源，并且这些模型的表现与当时顶级的基于卷积的方法相媲美。
- en: To be sure, CNNs are absolutely still in use today, and there are many cases
    where they clearly outperfom ViTs. At its core, a CNN maintains the visual structure
    of an image well. The core process of convolution is a left-to-right, top-to-bottom
    rendering of all the pixels of the image into dense representations. One benefit
    of this approach is **inductive bias**, a learned preference the model develops
    for pixels in relation to each other while training. This is a core part of why
    CNNs learn vision well. ViTs lack this ability, instead working with the pixels
    as tokens. ViTs offer some benefits of scaling, due to their core self-attention
    operations, but CNNs may be more common in smaller datasets and models. Recent
    work *(5)* has begun bridging this gap for ViTs, bringing inductive bias to them.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，卷积神经网络（CNN）今天仍然在使用，且在许多场景下，它们的表现明显优于ViT。CNN的核心优势在于它能够很好地保持图像的视觉结构。卷积操作的核心过程是将图像的所有像素从左到右、从上到下地渲染为密集的表示。这种方法的一个好处是**归纳偏差**，即模型在训练过程中对像素之间的相互关系产生的学习偏好。这是CNN能够学习视觉的核心原因。ViT没有这种能力，而是将像素当作令牌进行处理。由于其核心的自注意力机制，ViT在扩展性方面提供了一些优势，但在较小的数据集和模型中，CNN可能更为常见。近期的研究*(5)*已经开始弥补ViT的这一缺陷，为它们引入了归纳偏差。
- en: But how does it work? Through an encoder! The solution starts with a basic data
    processing technique that flattens the input. A 2D image is taken and simply reshaped
    into a sequence of *flattened 2D patches*. Then, the encoder applies a *linear
    projection* process to merge all of the parts of the image into a single row,
    including positional terms, so the location of the content is still known to the
    model. This single row is fed into the transformer encoder, which itself uses
    the self-attention process. The encoder reduces the size of the rows until it
    hits the final row, the labels. The model then selects one of the available classes.
    Finally, the loss function, in connection with the ground truth, provides a learnable
    signal for the model to update its weights and improve accuracy on the next epoch.
    Let’s take a look at this process.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 那么它是如何工作的呢？通过一个编码器！这个解决方案从一种基本的数据处理技术开始，它将输入展平。将二维图像取出，简单地重塑成一个*展平的二维补丁*序列。接着，编码器应用*线性投影*过程，将图像的所有部分合并成一个单独的行，并包括位置信息，这样内容的位置仍然能为模型所知。这个单独的行被送入变换器编码器，而变换器编码器本身使用自注意力过程。编码器不断缩小行的大小，直到最终得到标签行。然后，模型从可用的类别中选择一个。最后，损失函数与实际标签相结合，提供了一个可学习的信号，供模型更新其权重并在下一轮训练中提高准确性。让我们来看一下这个过程。
- en: '![Figure 3.1 – The ViT](img/B18942_03_01.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1 – ViT](img/B18942_03_01.jpg)'
- en: Figure 3.1 – The ViT
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 – ViT
- en: Notably, the ViT *is still effectively a supervised learning process*. Clearly,
    the learning method here relies on labels known ahead of time. This is a big difference
    from the pretraining regimes in language where the labels are unknown. But this
    base vision transfer still serves as an accuracy boost to enhance downstream models,
    so it’s worth evaluating. While there are some projects *(10)* that attempt truly
    unsupervised approaches in vision, personally, I haven’t yet seen a case in vision
    where this strictly outperforms a supervised approach. Perhaps this is a core
    difference between the two domains. Perhaps I’ll be proved wrong next month.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，ViT *仍然是一个有效的监督学习过程*。显然，这里的学习方法依赖于事先已知的标签。这与语言领域中的预训练方式有很大不同，因为语言中的标签通常是未知的。但这种基础的视觉迁移仍然能提升下游模型的准确性，因此值得评估。虽然有一些项目*(10)*尝试在视觉中实现真正的无监督方法，但个人而言，我尚未见过在视觉领域中严格优于监督方法的情况。也许这就是两个领域之间的核心区别。也许下个月我会被证明是错的。
- en: 'Another pretraining loss function that is key to the vision domain is contrastive.
    We introduced this in the first chapter, but now I’d like to take you deeper.
    We’ll spotlight one model using this: **CoCA**. Interestingly, the authors attempt
    to unify all three model architectures we’ve mentioned so far: encoder-only, decoder-only,
    and mixed encoder-decoder. Their trained model is able to solve use cases such
    as visual recognition, vision-language alignment, image captioning, end-to-end
    fine-tuning, frozen feature evaluation, and multimodal understanding with zero-shot
    transfer (more on zero-shot later in the book). In fact, CoCa uses two pretraining
    objectives: one to handle the images and the other to handle the text. Let’s break
    it down!'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在视觉领域至关重要的预训练损失函数是对比损失。我们在第一章中提到过这个概念，但现在我想带你深入了解。我们将重点介绍一个使用此方法的模型：**CoCA**。有趣的是，作者尝试将我们迄今为止提到的三种模型架构统一起来：仅编码器、仅解码器和混合编码器-解码器。经过训练的模型能够解决视觉识别、视觉-语言对齐、图像描述、端到端微调、冻结特征评估以及零-shot迁移等用例（稍后书中会详细讨论零-shot）。事实上，CoCA使用了两种预训练目标：一个用于处理图像，另一个用于处理文本。让我们来详细了解一下！
- en: 'The image part of the workflow looks similar to ViT; it takes an image, applies
    a flattening process, and feeds this into an encoder. In fact, the base implementation
    of CoCa uses ViT by default! However, instead of directly producing classification
    at the end of the encoder, these dense arrays are used in two ways:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程中的图像部分与ViT类似；它先取一张图像，应用展平过程，然后将其输入到编码器中。事实上，CoCA的基础实现默认使用ViT！然而，它并不是直接在编码器的输出端产生分类结果，而是将这些密集数组以两种方式使用：
- en: First, as an input to the final decoder
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，作为最终解码器的输入
- en: Second, as an input to an *intermediary* *loss function*
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，作为*中间* *损失函数*的输入
- en: 'This is called **contrastive loss** because it effectively contrasts the visual
    content with the textual. The final output then applies a **captioning loss**,
    increasing the accuracy of the text produced at the final stage of the model to
    label, or caption, the provided image. This is why the model is named CoCa: Contrastive
    Captioners.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**对比损失**，因为它有效地将视觉内容与文本内容进行对比。最终的输出随后应用**标注损失**，提高模型在最后阶段为提供的图像进行标签或标注时生成文本的准确性。这就是为什么这个模型被命名为
    CoCa：对比标注生成器。
- en: '![Figure 3.2 – Contrastive and captioning loss](img/B18942_03_02.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 对比损失与标注损失](img/B18942_03_02.jpg)'
- en: Figure 3.2 – Contrastive and captioning loss
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 – 对比损失与标注损失
- en: On the language side of the workflow, we see a decoder. The decoder takes a
    provided textual input, such as the original caption of the image. Then, it applies
    another flattening process to tokenize and embed the words, preparing them for
    the decoder. The decoder then reduces the dimensionality of the words, outputting
    a denser representation of the caption. This is then provided as an input to the
    contrastive loss function, better enabling the joint comparison between image
    and text.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作流的语言部分，我们看到一个解码器。解码器接收提供的文本输入，例如图像的原始标注。然后，它应用另一个扁平化过程，将单词进行分词和嵌入，为解码器做准备。接着，解码器将单词的维度降到更低，输出一个更密集的标注表示。然后，这个表示被作为输入提供给对比损失函数，从而更好地实现图像与文本之间的联合比较。
- en: '![](img/B18942_03_F01.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18942_03_F01.png)'
- en: 'This is the weighted loss function for CoCa:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 CoCa 的加权损失函数：
- en: '![](img/B18942_03_F02.png) = Overall loss for CoCa'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18942_03_F02.png) = CoCa 的整体损失'
- en: '![](img/B18942_03_F03.png) = Contrastive loss'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18942_03_F03.png) = 对比损失'
- en: '![](img/B18942_03_F04.png) = Captioning loss'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18942_03_F04.png) = 标注损失'
- en: '![](img/B18942_03_F05.png) = Hyperparameter to weight the contrastive loss'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18942_03_F05.png) = 用于加权对比损失的超参数'
- en: '![](img/B18942_03_F06.png) = Hyperparameter to weight the captioning loss'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B18942_03_F06.png) = 用于加权标注损失的超参数'
- en: Finally, the entire model uses a weighted combination of both loss functions
    to serve as the global loss function. How do they determine the weights, you ask?
    Through experimentation! This experimentation is almost certainly dependent on
    the dataset and task at hand. If you were using CoCa to solve a use case with
    extremely rich visual data but very weak language data, you might consider starting
    with a higher weight on contrastive loss. However, if your language data was excellent
    from the start and your visual data was only mildly informative, you might start
    with a higher weight on captioning.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，整个模型使用这两种损失函数的加权组合作为全局损失函数。你可能会问，他们是如何确定这些权重的？通过实验！这些实验几乎肯定依赖于数据集和任务。如果你使用
    CoCa 来解决一个视觉数据极其丰富但语言数据非常薄弱的使用场景，你可能会考虑从较高的对比损失权重开始。然而，如果你的语言数据一开始就非常优秀，而视觉数据只有轻微的信息量，你可能会从较高的标注损失权重开始。
- en: In this way, you can start to understand how we pick hyperparameters for our
    models. This is the subject of a different chapter later on, so for now, I want
    you to develop the intuition that *the precise implementation of each of these
    models can be highly personalized to your datasets and use cases*. Reading these
    papers and learning about how these state-of-the-art models work is a helpful
    step to provide more depth to your own analysis and understanding. You can and
    should apply this knowledge for gain back at work!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，你可以开始理解我们是如何为模型选择超参数的。这是后续章节的内容，所以现在，我希望你能培养一种直觉，即*每个模型的具体实现可以根据你的数据集和使用场景高度个性化*。阅读这些论文并了解这些最先进的模型如何工作是一个有助于你加深自己分析和理解的步骤。你可以并且应该把这些知识应用到你的工作中去获取更多收益！
- en: Pretraining loss functions in language – Alexa Teacher Model
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言中的预训练损失函数 – Alexa 教师模型
- en: At this point, you should feel pretty comfortable with the **masked language
    modeling** we discussed, especially how it makes encoder-only models happen in
    language, such as BERT. You should also know about **causal language modeling**,
    which enables decoder-only models such as GPT. Now let’s find out what happens
    when we mix the two!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你应该已经对我们讨论过的**掩码语言建模**感到相当熟悉，尤其是它如何在语言中实现仅编码器的模型，如 BERT。你还应该了解**因果语言建模**，它使得像
    GPT 这样的仅解码器模型成为可能。现在，让我们看看当我们将两者结合时会发生什么！
- en: 'The Alexa Teacher Model (11) came out just a few weeks ago as of writing this,
    and as an Amazonian, I can tell you it feels great to see a large language model
    come out of your own organization! But that’s not the only reason I’m mentioning
    it here. There are two reasons I think you should know about the **Alexa Teacher**
    **Model** (**AlexaTM**):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Alexa 教师模型（11）就在我写这段文字的几周前发布，作为一名亚马逊员工，我可以告诉你，看到自己公司推出一个大型语言模型的感觉真好！但这并不是我在这里提到它的唯一原因。我认为你应该了解**Alexa
    教师** **模型**（**AlexaTM**）有两个原因：
- en: First, it uses a concept called *few-shot learning* to easily transfer its knowledge
    about human verbal communication from one language to another. As we’ll learn
    in [*Chapter 13*](B18942_13.xhtml#_idTextAnchor198) on prompt engineering, few-shot
    learning means you pass a few examples to the model on *inference*. These few
    examples work wonders in prompting the model to respond more accurately. This
    is especially useful for language, because it allows language researchers to develop
    solutions for *low-resource languages*, enabling some digital technology in relevant
    communities.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，它使用一个叫做*少量学习（few-shot learning）*的概念，轻松地将关于人类语言交流的知识从一种语言转移到另一种语言。正如我们在[*第
    13 章*](B18942_13.xhtml#_idTextAnchor198)的提示工程中将要学习的那样，少量学习意味着你给模型提供一些*推理*的示例。这些少量示例能帮助模型更准确地回应。这对于语言来说尤其有用，因为它允许语言研究人员为*低资源语言*开发解决方案，从而为相关社区引入一些数字技术。
- en: Second, using this few-shot learning approach, a 20-billion parameter version
    of AlexaTM was able to outperform a model 27 times its size, PaLM, at 540B on
    the same problem *(12)*. This is a critical juncture to internalize and a trend
    that I hope we’ll continue to see more of over the years. While bigger can sometimes
    be better in machine learning models, it isn’t always, and sometimes it’s worse.
    Remember, smaller models are faster to train, easier to work with, and faster
    for inference, so if the accuracy is the same or better, always move to a small
    model.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，使用这种少量学习方法，一个拥有 200 亿参数的 AlexaTM 模型在同样的问题上，能够超越一个 540 亿参数的 27 倍大模型 PaLM *(12)*。这是一个至关重要的时刻，我希望我们在未来几年能看到这种趋势。虽然在机器学习模型中，规模更大有时意味着更好，但并不总是如此，有时甚至可能更差。记住，较小的模型训练速度更快，使用起来更简便，推理速度也更快，所以如果准确性相同或更好，总是选择小模型。
- en: 'Now, how does it work? Similar to the CoCa example, there are now two loss
    functions we’re tracking. The first you’re already familiar with: **causal language
    modeling**, or **CLM** for short. Here, the CLM process tries to predict the end
    of a sentence. Now, AlexaTM 20B combines that with a **denoising loss function**.
    The denoising process was introduced in BART *(13)* as a way to jointly learn
    on the combination of encoders and decoders, notably *through introducing noise*.
    A given document is intentionally corrupted by introducing masks via the encoder.
    The decoder is then asked to predict the likelihood of the document being original.
    Actually, the process of adding noise and then trying to discriminate noise from
    truth is a bit similar to adversarial learning in general.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，它是如何工作的呢？类似于 CoCa 示例，我们现在追踪了两个损失函数。第一个你已经熟悉：**因果语言建模**，简称**CLM**。在这里，CLM
    过程尝试预测一句话的结尾。现在，AlexaTM 20B 将其与**去噪损失函数**结合在一起。去噪过程在 BART *(13)* 中被引入，作为一种在编码器和解码器的组合上进行联合学习的方法，特别是*通过引入噪声*。给定的文档通过编码器故意被破坏，加入掩码。然后，解码器被要求预测文档是否是原始的。实际上，加入噪声然后尝试区分噪声与真实信息的过程，和对抗学习有些相似。
- en: '![Figure 3.3 – Alexa Teacher Model](img/B18942_03_03.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3 – Alexa 教师模型](img/B18942_03_03.jpg)'
- en: Figure 3.3 – Alexa Teacher Model
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 – Alexa 教师模型
- en: Interestingly, the AlexaTM 20B uses the CLM pretraining objective *only 20%
    of the time*. This is to introduce the ability of the model to work well in few-shot
    cases. During this time, the model does not produce noise at all; it simply tries
    to complete sentences. This is denoted via a signal provided to the beginning
    of 20% of the sentences [CLM]. The authors also randomly fed the model 20% and
    80% of the document to ensure it performs well in both short and long cases. To
    start the training quickly, they also began with a 10B pretrained encoder, which
    they unfroze after hitting 100,000 steps. This overall process took 120 days on
    128 A100 GPUs (NVIDIA hardware), which translates to only 16 `ml.p4d.24xlarge`
    on Amazon SageMaker distributed training. More on SageMaker training to come!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，AlexaTM 20B只在**20%的时间**内使用CLM预训练目标。这是为了引入模型在少量样本情况下表现良好的能力。在这段时间里，模型完全不产生噪声；它只是尝试完成句子。这通过提供一个信号来表示，信号出现在20%的句子的开头[CLM]。作者还随机向模型输入了20%和80%的文档，以确保它在短文本和长文本的情况下都能表现良好。为了快速启动训练，他们还从一个10B预训练编码器开始，并在达到100,000步后解冻。整个过程在128个A100
    GPU（NVIDIA硬件）上进行了120天，这相当于在Amazon SageMaker分布式训练中使用16个`ml.p4d.24xlarge`实例。更多关于SageMaker训练的内容将会介绍！
- en: For your reference, the “Teacher” in AlexaTM refers to a process called **distillation**.
    **Distillation** is another way to transfer knowledge, comparable to fine-tuning.
    In fine-tuning, we attach extra layers to a larger base model, then we usually
    run it on a smaller set of supervised data. In distillation, we pair one larger
    “teacher” model with a much smaller “student” model. The student model is then
    trained to generate the same probability distribution as the teacher model but
    at a much smaller computational cost. All of [*Chapter 10*](B18942_10.xhtml#_idTextAnchor152)
    is dedicated to methods for fine-tuning your model and comes with coding examples.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 供你参考，AlexaTM中的“教师”指的是一种称为**蒸馏**的过程。**蒸馏**是另一种传递知识的方式，可以与微调相媲美。在微调中，我们将额外的层附加到较大的基础模型上，然后通常在一个较小的有监督数据集上运行它。在蒸馏中，我们将一个较大的“教师”模型与一个较小的“学生”模型配对。然后，学生模型被训练以生成与教师模型相同的概率分布，但计算成本要低得多。[*第10章*](B18942_10.xhtml#_idTextAnchor152)全部讲述了微调模型的方法，并附有代码示例。
- en: Changing your pretraining loss function
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 改变你的预训练损失函数
- en: 'Now that we’ve surveyed some of the top pretraining loss functions across vision,
    language, and multimodal scenarios, you might be wondering: is that it? What do
    I do with this information?'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经调查了在视觉、语言和多模态场景中一些最顶尖的预训练损失函数，你可能会想：就这些吗？我该如何使用这些信息呢？
- en: The answer to this question largely depends on your years of experience in machine
    learning. If you are just getting started, then you certainly don’t need to be
    overly fixated on this aspect of your project. Simply pick your top model, understand
    how it learns, and get on with your project. However, as your level of experience
    increases, you may start to want to experiment with the pretraining loss regime
    itself, which is excellent! As you grow in machine learning, particularly as a
    developer or a scientist, it becomes extremely valuable to contribute your own
    novel ideas back to the community. Inventing new pretraining loss functions or,
    for that matter, any new optimization you come across in the entire modeling journey,
    is both incredibly valuable and deeply fulfilling. This is where you can truly
    establish a new state of the art in a given domain, possibly even a new domain
    that you yourself invent!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案很大程度上取决于你在机器学习领域的经验年限。如果你刚开始入门，那么你当然不需要过分关注项目中的这一方面。只需选择最适合的模型，理解它是如何学习的，然后继续推进你的项目。然而，随着经验的增加，你可能开始想要尝试调整预训练损失函数，这很好！随着你在机器学习方面的成长，特别是作为开发者或科学家，回馈社区自己独特的创意是非常有价值的。在整个建模过程中，发明新的预训练损失函数或其他任何新的优化方法，不仅极具价值，而且令人满足。在这一过程中，你可以真正为某个领域设立新的技术标准，甚至可能是你自己发明的新领域！
- en: Solving for your model size
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决你的模型大小问题
- en: Now that you’ve picked your best base model(s), you understand its pretraining
    regime, and you identified your dataset and its overall size in the last chapter,
    let’s start to understand the sizes of models you can target!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经选择了最佳的基础模型，并理解了它的预训练机制，同时在上一章中识别了你的数据集及其整体大小，接下来让我们开始了解你可以瞄准的模型大小！
- en: You may remember that in [*Chapter 1*](B18942_01.xhtml#_idTextAnchor016), we
    introduced a core concept called *the scaling laws*. Introduced by Kaplan et al.
    in 2020, this bold idea suggests a formal relationship between the overall sizes
    of your compute training cluster, your dataset, and your model. Prior to Kaplan,
    most machine learning practitioners had understood there to be a general relationship
    between these three, but his team took the bold task of proving this empirically
    via power laws.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，在[*第1章*](B18942_01.xhtml#_idTextAnchor016)中，我们介绍了一个核心概念——*规模法则*。这个大胆的观点是由Kaplan等人在2020年提出的，建议了计算训练集群的整体规模、数据集和模型之间的正式关系。在Kaplan之前，大多数机器学习从业者已经理解到这三者之间存在某种一般关系，但他的团队勇敢地通过幂律的实证研究来证明了这一点。
- en: The basic thing you need to understand can be demonstrated with a simple graphic.
    To train your model well, both in terms of producing the highest accuracy you
    can and in getting the most value out of your overall compute budget, it’s helpful
    to think about the following key items as a fundamental relationship.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要理解的基本内容可以通过一个简单的图示来展示。为了更好地训练你的模型，无论是在实现最高准确度方面，还是在充分利用你整体计算预算方面，考虑以下关键项作为一个基本关系是很有帮助的。
- en: Personally, I find it helpful to consider this visually. The fundamental way
    your model is going to learn anything about the real world is through the dataset
    itself. Naturally, you can see that as the size of the model increases, you’ll
    want the dataset to also increase in some capacity. As the dataset increases,
    the model should also increase to some degree.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 就个人而言，我发现以视觉化的方式来考虑这一点是非常有帮助的。你的模型学习任何关于真实世界的知识，根本的方式是通过数据集本身。自然地，你可以看到，随着模型的规模增加，你希望数据集在某种程度上也要增大。随着数据集的增加，模型也应该在一定程度上增大。
- en: '![Figure 3.4 – The inter-relationship of machine learning](img/B18942_03_04.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4 – 机器学习的相互关系](img/B18942_03_04.jpg)'
- en: Figure 3.4 – The inter-relationship of machine learning
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 – 机器学习的相互关系
- en: You can even think about it in terms of human learning. As we gain more experience,
    more knowledge, and more understanding, our brain literally builds new pathways
    to interpret, store, and learn from these experiences. The more new experiences
    you have and the more problems you solve, the more your brain evolves to store
    the necessary information. On the flip side, as our experiences and new challenges
    decrease, our brain loses some of its elasticity to respond in kind. This is an
    example of biological optimization at work!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以从人类学习的角度来思考这一点。当我们获得更多经验、更多知识、更多理解时，我们的大脑实际上会构建新的路径，以解读、存储和从这些经验中学习。你拥有的新的经历越多，解决的问题越多，你的大脑就会越进化，存储所需的信息。反之，当我们的经历和新挑战减少时，我们的大脑会失去一定的弹性，无法作出相应的反应。这就是生物优化的一个例子！
- en: To complete the analogy, our lived human experiences are like a dataset. Every
    new challenge, new relationship, new experience, and new problem is like adding
    extra records and aspects to the dataset. Similarly, our brains are like a model.
    Organically, our bodies handle building and releasing pathways in the brain dynamically
    with our most recent experiences. What we are trying to do as computer scientists
    is replicate this process with code through a process called *training*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个类比，我们的生活经验就像是一个数据集。每一个新的挑战、新的关系、新的经历和新的问题就像是往数据集中添加额外的记录和方面。类似地，我们的大脑就像是一个模型。我们的身体自然地处理在大脑中建立和释放路径的过程，这个过程是动态的，依据我们最新的经历。作为计算机科学家，我们所要做的就是通过一种叫做*训练*的过程，用代码来复制这一过程。
- en: In terms of pretraining across vision, language, and multimodal scenarios, know
    that this relationship still applies. If you pair a large, complex model with
    a small dataset, it’s likely that *your model will overfit.* Overfitting means
    you may get an extremely high level of accuracy on your training sets but completely
    fail to generalize well and provide meaningful results outside of the training
    procedure. On the other hand, if you pair a tiny model with an extremely large
    dataset, *you may well be underfitting.* This means you may not even perform well
    on the training set, let alone elsewhere.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨视觉、语言和多模态场景的预训练方面，请知道这种关系仍然适用。如果你将一个大型、复杂的模型与一个小数据集配对，很可能会出现*过拟合*的情况。过拟合意味着你可能在训练集上获得极高的准确率，但完全无法进行良好的泛化，也无法在训练过程之外提供有意义的结果。另一方面，如果你将一个小型模型与一个极大的数据集配对，*你很可能会出现欠拟合*。这意味着你可能在训练集上表现不佳，更不用说在其他地方了。
- en: Pairing the compute with the model and dataset sizes is all about cost optimization.
    There are inherent trade-offs in *scaling horizontally*, which means adding more
    compute to your cluster. This is different from *scaling vertically*, which means
    upgrading your instances to larger and more recent versions. Most teams find a
    natural balance between throwing as many machines as they can at a model to get
    the runtime low versus using as few machines as possible but taking many days,
    weeks, or even months to complete the training run. You want to find a natural
    middle ground between these two. We’ll dive into these topics, including core
    distribution methods such as model and data parallelism, in [*Chapter 5*](B18942_05.xhtml#_idTextAnchor085),
    *Distribution Fundamentals*.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 将计算与模型和数据集大小相匹配完全是关于成本优化的问题。*横向扩展*存在固有的权衡，这意味着向你的集群中添加更多计算资源。这与*纵向扩展*不同，后者意味着将实例升级到更大和更新的版本。大多数团队会在尽量用尽可能多的机器来降低运行时间与尽量少用机器但需要多天、数周甚至数月才能完成训练之间找到一个自然的平衡。你需要在这两者之间找到一个合适的中间地带。我们将在[*第五章*](B18942_05.xhtml#_idTextAnchor085)中深入探讨这些话题，包括核心的分布式方法，如模型并行和数据并行，*分布式基础*。
- en: Case study – Amazon Search speeds up runtime seven-fold with distributed training
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 案例研究——亚马逊搜索通过分布式训练将运行时间缩短七倍
- en: One great example of finding the natural balance between speed and per-hour
    cost is *Amazon Search*! Search is, as you might expect, the team responsible
    for helping you find the products you are most interested in on [amazon.com](http://amazon.com).
    Every time you try to find something on Amazon, the query runs through our search
    engine to find exactly what you’re looking for.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找速度与每小时成本之间自然平衡的一个很好的例子是*亚马逊搜索*！搜索团队，正如你所料，负责帮助你在[amazon.com](http://amazon.com)上找到你最感兴趣的产品。每次你在亚马逊上寻找某样东西时，查询都会通过我们的搜索引擎，精确找到你正在寻找的内容。
- en: Scientists and developers value the ability to iterate quickly. They love testing
    out ideas as fast as possible, getting feedback on them, and quickly jumping into
    the next version. This allows them to *optimize*, or simply improve their ideas
    rapidly. Staying agile on experimentation helps you keep the overall cost of research
    and development down, because it reduces the time it takes to move from initial
    product ideation to full release.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 科学家和开发者重视快速迭代的能力。他们喜欢尽快测试想法，快速获得反馈，然后迅速进入下一个版本。这使他们能够*优化*，或者简单地快速改进他们的想法。保持实验的灵活性有助于降低研发的整体成本，因为它减少了从初步产品构思到全面发布所需的时间。
- en: At Amazon, SageMaker partnered with Search to release native support for PyTorch
    Lightning with an optimized inter-node communication project, Distributed Data
    Parallel. As a result, Search was able to move from training on 1 node up to 8
    nodes, reducing the overall time to train from 99 minutes down to 13.5!
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在亚马逊，SageMaker与搜索团队合作，推出了对PyTorch Lightning的原生支持，并优化了节点间通信项目——分布式数据并行。因此，搜索团队能够将训练从1个节点扩展到8个节点，将整体训练时间从99分钟缩短到13.5分钟！
- en: They didn’t change the model size or dataset. They kept both constant and simply
    added a data parallel strategy to make copies of the model and shared the data
    out to all accelerators (GPUs). This allowed them to scale horizontally, adding
    extra nodes to their cluster and reducing the overall job time.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 他们并没有改变模型大小或数据集，保持这两者不变，只是加入了数据并行策略，复制模型并将数据分发到所有加速器（GPU）。这使他们能够进行横向扩展，向集群中添加额外节点，从而减少了整体作业时间。
- en: We’ll dive into these distributed concepts in later chapters, but for now, simply
    know that *you can decrease the amount of time it takes to train your model when
    you use a distribution strategy with extra nodes in* *your cluster*.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后续章节中深入探讨这些分布式概念，但现在，只需知道，*当你使用分布式策略并在集群中增加额外节点时，你可以缩短训练模型所需的时间*。
- en: Practical approaches to solving for your model size
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决模型大小问题的实际方法
- en: Now that you have a good understanding of the relationship between data, model,
    and compute sizes, let’s get down to the nuts and bolts of figuring out which
    ones are right for you!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对数据、模型和计算资源之间的关系有了较好的理解，我们来深入探讨如何确定适合你的配置！
- en: Most teams will consider the compute budget as fixed. Think of this number as
    what you should plan on asking your senior leadership to approve for your project.
    As we learned in [*Chapter 2*](B18942_02.xhtml#_idTextAnchor034), you should think
    of this number as some fraction of the overall value that increased accuracy will
    have on your business.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数团队会把计算预算视为固定的。把这个数字看作你应该计划向高级领导层申请批准的项目预算。正如我们在[*第二章*](B18942_02.xhtml#_idTextAnchor034)中学到的，你应该将这个数字视为你业务中增加的准确度所带来的整体价值的某一部分。
- en: The second true bottleneck is your dataset size. Figure out how large your candidate
    dataset is. In vision, you might be counting images. In language, you might count
    tokens. I still like to baseline at GB, because this is easily understood and
    translatable across domains. Generally speaking, a good way to get started is
    just to find the models and ppaers you are inspired by, dive into them, understand
    how large their datasets were, and use that as your baseline. This will range
    from 10’s of GB’s to a few PB. For those of you who are new to machine learning,
    that’s a great place to start. Following the standard paths of proven expertise
    is a great way to get yourself started toward successful projects. However, for
    those of you who aren’t new to machine learning, let’s take a quick look at using
    the scaling laws to solve for your optimal model size.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个真正的瓶颈是你的数据集大小。弄清楚你的候选数据集有多大。在视觉任务中，你可能是在计算图像数量；在语言任务中，你可能是在计算标记数量。我仍然喜欢以GB为基准，因为它容易理解且可以跨领域转化。一般来说，一个好的起点是找到你受到启发的模型和论文，深入研究它们，了解它们的数据集有多大，并以此作为你的基准。这些大小范围通常从几十GB到几个PB不等。对于那些刚接触机器学习的人来说，这是一个很好的起点。遵循已经验证过的专家经验路径是开始成功项目的好方法。然而，对于那些不新手的人，我们快速看一下如何利用缩放法则来计算最佳模型大小。
- en: Not all scaling laws are created equal
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不是所有的缩放法则都一样
- en: First, it’s helpful to know that while the general relationship between your
    model, data, and compute size is intuitively understandable, the precise mathematical
    formulas actually can vary quite a bit. As we learned in [*Chapter 2*](B18942_02.xhtml#_idTextAnchor034),
    Kaplan used the mathematical term, ![](img/B18942_03_F07.png) which indicates
    that two quantities are “proportional” to each other. In other words, when two
    terms are equated by the proportional sign, we know that the two quantities are
    certainly linked, but we don’t know precisely which constant terms govern that
    relationship.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，了解一下，虽然模型、数据和计算大小之间的关系在直觉上是可以理解的，但精确的数学公式实际上可能会有很大差异。正如我们在[*第二章*](B18942_02.xhtml#_idTextAnchor034)中学到的，Kaplan使用了数学术语！[](img/B18942_03_F07.png)，这表示两个量是“成比例”的。换句话说，当两个项被比例符号等号连接时，我们知道这两个量是相关的，但我们并不知道确切的常数项来支配这个关系。
- en: This is the case in large deep learning models. Different papers and research
    terms have preferences for certain aspects of this, such as Kaplan preferring
    to keep model sizes large but datasets somewhat smaller and Hoffman suggesting
    to increase both equally. Kaplan originally presented autoregressive models, or
    decoder-based models, as the most *sample efficient.* However, the AlexaTM project
    indicated that joint encoders and decoders could actually be *more* efficient.
    All this is to say that while the scaling laws can suggest optimal model settings,
    results will vary.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是大型深度学习模型的情况。不同的论文和研究团队在这方面有各自的偏好，例如Kaplan倾向于保持模型规模较大，而数据集相对较小，而Hoffman则建议两者都要增加。Kaplan最初提出自回归模型，或者基于解码器的模型，作为最*样本高效*的模型。然而，AlexaTM项目表明，联合编码器和解码器实际上可能更高效。所有这些都说明，尽管缩放法则可以建议最佳的模型设置，结果会有所不同。
- en: Next, let’s try to define the lower and upper model sizes you want to build
    up toward in your experiments.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们尝试定义你想在实验中构建的模型大小的下限和上限。
- en: Planning future experiments
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划未来的实验
- en: Now that you have an idea of what model size you’d like to target given your
    compute budget and data constraints, let’s learn how to think about each run of
    your job *as an experiment*. Fundamentally, each stage in the machine learning
    process is ultimately a unique experiment. Some of your inputs to each stage in
    the project stay the same; you could call these your **dependent variables**.
    Some of your inputs to the project change; these are your *independent variables*.
    it takes time to build up skills on your project Simply put, change something
    and see what happens. Just make sure you’re only changing one thing, so it’s empirically
    clear what the result is!
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经对在计算预算和数据限制下想要针对的模型大小有所了解，让我们来学习如何将每一次作业的执行*视为一次实验*。从根本上讲，机器学习过程中的每个阶段都可以看作是一次独特的实验。你在项目中某些阶段的输入保持不变；这些可以称为你的**因变量**。而有些输入会发生变化；这些则是你的*自变量*。在项目中积累技能需要时间，简单来说，改变某些东西并观察结果。只需确保你只改变一项内容，这样结果才会有实证依据！
- en: It’s critical to understand that the whole scope of your project is *not* going
    to happen all at once. A lot of this is because it takes time to build up skills
    on your time. Even if you are starting with a completely experienced team, which
    frankly happens very rarely, the ecosystem of machine learning itself changes
    so rapidly that every few months, you’ll be learning about something new. So,
    plan on adding extra time to learn about all the newest releases.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是要明白，你整个项目的范围*不会*一蹴而就。很多时候，这是因为在项目中积累技能需要时间。即使你从一支经验丰富的团队开始，坦率来说，这种情况非常少见，机器学习的生态系统本身变化如此之快，以至于每隔几个月，你就会学习到一些新的内容。因此，计划为学习所有最新发布的内容预留额外的时间。
- en: At the beginning, start with the smallest possible experiment you can. Get the
    smallest version of your model up and running on your local IDE. Depending on
    the size of the model and the corresponding hardware you’ll need, you can do this
    on a variety of compute options, from Jupyter notebooks and more robust options
    to your laptop, free compute experimental environments, and more. On AWS, we provide
    a wide variety of these options through our fully managed machine learning service,
    Amazon SageMaker.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一开始，尽量从最小的实验开始。让你的模型的最小版本在本地 IDE 上运行。根据模型的大小和你需要的硬件，你可以选择多种计算选项，从 Jupyter notebooks
    和更强大的选项，到你的笔记本电脑、免费的计算实验环境等。在 AWS 上，我们通过我们的完全托管机器学习服务 Amazon SageMaker 提供了各种这些选项。
- en: 'After demonstrating interesting results on a tiny fraction of the dataset,
    I like to move directly into a remote SageMaker training job. You’ll learn more
    about training on SageMaker in the next chapter, but for now, I’d simply like
    you to know that *you can easily and seamlessly scale up and down your training
    needs on Amazon SageMaker*. All of the practical guidance in this book will focus
    on how to do this efficiently. In terms of your project, you might consider working
    on the SageMaker Training API until you have a successful job run. I’d keep at
    this until you are running on a single instance with multiple GPUs: my go-to is
    the `g` family with four GPUs. This can be either `ml.g4dn.12xlarge`, or `ml.g5.12xlarge`.
    More on what this means in the chapters to come! Using multiple GPUs *will require
    a data and/or model parallelization strategy*, which is the entire content of
    [*Chapter 5*](B18942_05.xhtml#_idTextAnchor085).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据集的一小部分展示出有趣的结果之后，我喜欢直接进入远程 SageMaker 训练任务。你将在下一章中了解更多关于 SageMaker 上的训练，但现在我只想让你知道，*你可以轻松且无缝地根据需求在
    Amazon SageMaker 上扩展和缩减训练工作量*。本书中所有的实践指南将聚焦于如何高效地实现这一点。在你的项目中，你可以考虑在 SageMaker
    Training API 上工作，直到成功运行作业。我会坚持做这项工作，直到你在一台拥有多个 GPU 的单实例上运行：我常用的是 `g` 系列，配备四个 GPU。这个可以是
    `ml.g4dn.12xlarge`，也可以是 `ml.g5.12xlarge`。接下来的章节会详细说明这些配置！使用多个 GPU *将需要数据和/或模型并行化策略*，这一部分内容会在
    [*第 5 章*](B18942_05.xhtml#_idTextAnchor085)中讲解。
- en: Once you are running successfully both on SageMaker remote training and across
    multiple GPUs, then it’s time to increase everything. Bump up the size of your
    data. Remember that model, data, compute, and key hyperparameters such as learning
    rate and batch size, in addition to the model architecture itself, are all interrelated.
    Finding the right settings for this is the entire content of [*Chapter 7*](B18942_07.xhtml#_idTextAnchor116).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你成功地在SageMaker远程训练和多个GPU上运行，那么就该提高一切了。增加数据的规模。记住，模型、数据、计算能力以及关键超参数（如学习率和批量大小）和模型架构本身是相互关联的。找到合适的设置是[*第7章*](B18942_07.xhtml#_idTextAnchor116)的全部内容。
- en: As soon as you start increasing, extra complications arise. You want to make
    sure your loss is decreasing sufficiently, but you also want to keep GPU utilization
    high. You want to debug and improve the operators and communications in your job,
    but you also want to evaluate training throughput. When your job breaks, which
    it will with almost perfect certainty, you want to get it back online as quickly
    as possible, but accurately as well.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你开始增加，额外的复杂性就会出现。你需要确保你的损失足够减少，但同时也要保持GPU的高利用率。你想调试和改进工作中的运算符和通信，但也需要评估训练吞吐量。当你的工作出现故障时，几乎可以确定它会发生，你希望尽快将其恢复，但也要确保准确性。
- en: Diving into these nuances, unpacking them, and helping you get your project
    back on track to avoid as many of the roadblocks and capitalize on as many known
    issues is the entire focus of *Part 3*.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 深入这些细节，解构它们，帮助你让项目重新回到正轨，避免尽可能多的障碍并利用已知问题的机会，正是*第三部分*的重点。
- en: In short, there are many different discrete phases of pretraining a large model.
    This entire book aims to help you navigate them safely. In the next chapter, we’ll
    dive into the GPU itself and uncover how to best utilize these efficient processors,
    also known as **accelerators**.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，预训练大模型有许多不同的离散阶段。整本书的目标是帮助你安全地应对这些阶段。在下一章，我们将深入探讨GPU本身，揭示如何最佳利用这些高效的处理器，也被称为**加速器**。
- en: Summary
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how to find your best base model, including the
    basics of architecture and the most common use cases and modalities, and you were
    given general guidance to start with the smallest model you can. You learned about
    key trade-offs, such as simplicity versus complexity, and applying many use cases
    versus applying only one. You received tactical guidance on how to find a base
    model with good support. You learned how to find your pretraining loss function,
    including masked language modeling, causal language modeling, and those common
    in vision models such as ViT and CoCa. We looked at the Alexa Teacher Model, and
    we learned how to use the scaling laws to solve for our model size, with help
    from a case study from Amazon Search.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何找到最佳的基础模型，包括架构基础、最常见的使用案例和模态，同时你获得了从最小模型开始的一般指导。你了解了关键的权衡，比如简单与复杂的平衡，以及多种使用案例与单一使用案例的应用。你收到了关于如何找到支持良好的基础模型的策略性指导。你学习了如何找到预训练损失函数，包括掩蔽语言建模、因果语言建模，以及在视觉模型中常见的如ViT和CoCa等模型。我们探讨了Alexa教师模型，并了解了如何通过扩展法则计算我们的模型规模，结合亚马逊搜索的案例研究。
- en: 'Next up: working with accelerators!'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步：与加速器一起工作！
- en: References
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Please go through the following content for more information on a few topics
    covered in the chapter:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请阅读以下内容以获取更多本章涉及的部分主题的信息：
- en: '*CoCa: Contrastive Captioners are Image-Text Foundation Models:* [https://arxiv.org/abs/2205.01917](https://arxiv.org/abs/2205.01917)'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*CoCa: 对比性字幕生成器是图像-文本基础模型：* [https://arxiv.org/abs/2205.01917](https://arxiv.org/abs/2205.01917)'
- en: '*CLIP: Connecting text and* *images:* [https://openai.com/blog/clip/](https://openai.com/blog/clip/)'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*CLIP: 连接文本和* *图像：* [https://openai.com/blog/clip/](https://openai.com/blog/clip/)'
- en: '*MASKED VISION AND LANGUAGE MODELING FOR MULTI-MODAL REPRESENTATION* *LEARNING:*
    [https://arxiv.org/pdf/2208.02131.pdf](https://arxiv.org/pdf/2208.02131.pdf)'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*面向多模态表示学习的遮罩视觉与语言建模：* [https://arxiv.org/pdf/2208.02131.pdf](https://arxiv.org/pdf/2208.02131.pdf)'
- en: '*Language Models are Few-Shot* *Learners:* [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*语言模型是少量样本* *学习者：* [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)'
- en: '*Hierarchical Text-Conditional Image Generation with CLIP* *Latents:* [https://cdn.openai.com/papers/dall-e-2.pdf](https://cdn.openai.com/papers/dall-e-2.pdf)'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*基于CLIP潜变量的层次化文本条件图像生成：* [https://cdn.openai.com/papers/dall-e-2.pdf](https://cdn.openai.com/papers/dall-e-2.pdf)'
- en: '*Photorealistic Text-to-Image Diffusion Models with Deep Language* *Understanding:*
    [https://arxiv.org/abs/2205.11487](https://arxiv.org/abs/2205.11487)'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*逼真的文本到图像扩散模型与深度语言理解：* [https://arxiv.org/abs/2205.11487](https://arxiv.org/abs/2205.11487)'
- en: '*Flamingo: a Visual Language Model for Few-Shot* *Learning:* [https://arxiv.org/pdf/2204.14198.pdf](https://arxiv.org/pdf/2204.14198.pdf)'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Flamingo：一种少样本学习的视觉语言模型：* [https://arxiv.org/pdf/2204.14198.pdf](https://arxiv.org/pdf/2204.14198.pdf)'
- en: '*Exploring the Limits of Transfer Learning with a Unified Text-to-Text* *Transformer:*
    [https://arxiv.org/pdf/1910.10683.pdf](https://arxiv.org/pdf/1910.10683.pdf)'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*探索统一文本到文本变换器的迁移学习极限：* [https://arxiv.org/pdf/1910.10683.pdf](https://arxiv.org/pdf/1910.10683.pdf)'
- en: '*AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT* *SCALE:*
    [https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*一张图片值16X16个词：用于大规模图像识别的变换器：* [https://arxiv.org/pdf/2010.11929.pdf](https://arxiv.org/pdf/2010.11929.pdf)'
- en: '*Unsupervised Pre-Training of Image Features on Non-Curated Data:* [https://arxiv.org/pdf/1905.01278.pdf](https://arxiv.org/pdf/1905.01278.pdf)'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*在非策划数据上无监督图像特征预训练：* [https://arxiv.org/pdf/1905.01278.pdf](https://arxiv.org/pdf/1905.01278.pdf)'
- en: '*Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders
    for Natural Language Understanding* *Systems:* [https://arxiv.org/pdf/2206.07808.pdf](https://arxiv.org/pdf/2206.07808.pdf)'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Alexa教师模型：预训练和提炼多十亿参数编码器用于自然语言理解系统：* [https://arxiv.org/pdf/2206.07808.pdf](https://arxiv.org/pdf/2206.07808.pdf)'
- en: '*Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders
    for Natural Language Understanding Systems:* [https://arxiv.org/pdf/2208.01448.pdf](https://arxiv.org/pdf/2208.01448.pdf)'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Alexa教师模型：预训练和提炼多十亿参数编码器用于自然语言理解系统：* [https://arxiv.org/pdf/2208.01448.pdf](https://arxiv.org/pdf/2208.01448.pdf)'
- en: '*BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,
    Translation, and* *Comprehension:* [https://arxiv.org/pdf/1910.13461.pdf](https://arxiv.org/pdf/1910.13461.pdf)'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*BART：用于自然语言生成、翻译和理解的去噪序列到序列预训练：* [https://arxiv.org/pdf/1910.13461.pdf](https://arxiv.org/pdf/1910.13461.pdf)'
- en: 'Part 2: Configure Your Environment'
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：配置你的环境
- en: In part 2, you’ll learn how to configure your environment for large-scale pretraining.
    We’ll dive into **graphics processing units** (**GPUs**), parallelization basics,
    and the second part of dataset preparation.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，你将学习如何为大规模预训练配置你的环境。我们将深入探讨**图形处理单元**（**GPU**）、并行化基础知识以及数据集准备的第二部分。
- en: 'This section has the following chapters:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含以下章节：
- en: '[*Chapter 4*](B18942_04.xhtml#_idTextAnchor066), *Containers and Accelerators
    on the Cloud*'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B18942_04.xhtml#_idTextAnchor066)，*云中的容器和加速器*'
- en: '[*Chapter 5*](B18942_05.xhtml#_idTextAnchor085), *Distribution Fundamentals*'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B18942_05.xhtml#_idTextAnchor085)，*分布基础知识*'
- en: '[*Chapter 6*](B18942_06.xhtml#_idTextAnchor106), *Dataset Preparation: Part
    Two, the Data Loader*'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B18942_06.xhtml#_idTextAnchor106)，*数据集准备：第二部分，数据加载器*'
