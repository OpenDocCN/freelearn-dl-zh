- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Performing End-to-End View Synthesis with SynSin
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SynSin进行端到端视图合成
- en: This chapter is dedicated to the latest state-of-the-art view synthesis model
    called SynSin. View synthesis is one of the main directions in 3D deep learning,
    which can be used in multiple different domains such as AR, VR, gaming, and more.
    The goal is to create a model for the given image as an input to reconstruct a
    new image from another view.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章专注于名为SynSin的最新视图合成模型。视图合成是3D深度学习的主要方向之一，可以在多个不同领域（如AR、VR、游戏等）中使用。其目标是为给定的图像作为输入，从另一个视角重建一个新的图像。
- en: In this chapter, first, we will explore view synthesis and the existing approaches
    to solving this problem. We will discuss all advantages and disadvantages of these
    techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先探讨视图合成及其现有的解决方法。我们将讨论这些技术的所有优缺点。
- en: Second, we are going to dive deeper into the architecture of the SynSin model.
    This is an end-to-end model that consists of three main modules. We will discuss
    each of them and understand how these modules help to solve view synthesis without
    any 3D data.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们将深入探讨SynSin模型的架构。这是一个端到端模型，由三个主要模块组成。我们将讨论每个模块，并了解这些模块如何帮助解决视图合成而无需任何3D数据。
- en: After understanding the whole structure of the model, we will move on to hands-on
    practice, where we will set up and work with the model to better understand the
    whole view synthesis process. We will train and test the model, and also use pre-trained
    models for inference.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解模型的整体结构之后，我们将进入实际操作阶段，设置和操作模型以更好地理解整个视图合成过程。我们将训练和测试模型，并使用预训练模型进行推断。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要主题：
- en: Overview of view synthesis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视图合成概述
- en: SynSin network architecture
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SynSin网络架构
- en: Hands-on model training and testing
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练和测试实践
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To run the example code snippets in this book, ideally, readers need to have
    a computer with a GPU. However, running the code snippets with only CPUs is not
    impossible.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本书中的示例代码片段，理想情况下读者需要一台配备GPU的计算机。然而，仅使用CPU运行代码片段并非不可能。
- en: 'The recommended computer configuration includes the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的计算机配置包括以下内容：
- en: A GPU, for example, the Nvidia GTX series or the RTX series with at least 8
    GB of memory
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，具有至少8GB内存的Nvidia GTX系列或RTX系列GPU
- en: Python 3
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3
- en: The PyTorch and PyTorch3D libraries
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch和PyTorch3D库
- en: The code snippets for this chapter can be found at [https:github.com/PacktPublishing/3D-Deep-Learning-with-Python](https://github.com/PacktPublishing/3D-Deep-Learning-with-Python).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码片段可以在[https:github.com/PacktPublishing/3D-Deep-Learning-with-Python](https://github.com/PacktPublishing/3D-Deep-Learning-with-Python)找到。
- en: Overview of view synthesis
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视图合成概述
- en: One of the most popular research directions in 3D computer vision is view synthesis.
    Given the data and the viewpoint, the idea of this research direction is to generate
    a new image that renders the object from another viewpoint.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在3D计算机视觉中最流行的研究方向之一是视图合成。在这个研究方向中，给定数据和视点，其核心思想是生成一个新的图像，从另一个视角渲染对象。
- en: 'View synthesis comes with two challenges. The model should understand the 3D
    structure and semantic information of the image. By 3D structure, we mean that
    when changing the viewpoint, we get closer to some objects and far away from others.
    A good model should handle this by rendering images where some objects are bigger
    and some are smaller to view - change. By semantic information, we mean that the
    model should differentiate the objects and should understand what objects are
    presented in the image. This is important because some objects can be partially
    included in the image; therefore, during the reconstruction, the model should
    understand the semantics of the object to know how to reconstruct the continuation
    of that object. For example, given an image of a car on one side where we only
    see two wheels, we know that there are two more wheels on the other side of the
    car. The model must contain these semantics during reconstruction:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 视图合成面临两个挑战。模型应该理解图像的3D结构和语义信息。所谓3D结构，是指当视角发生变化时，我们会靠近一些物体而远离其他物体。一个好的模型应该通过渲染图像来处理这种变化，其中一些物体变得更大，另一些则变得更小。所谓语义信息，是指模型应区分图像中的物体，并理解图像中展示的物体。这一点非常重要，因为某些物体可能只部分出现在图像中；因此，在重建过程中，模型应理解物体的语义，以便知道如何重建该物体的延续部分。例如，给定一张车子的一侧图像，我们只能看到两个车轮，我们知道车子的另一侧还有两个轮子。模型在重建时必须包含这些语义信息：
- en: '![Figure 9.1: The red-framed photographs show the original image, and the blue-framed
    photographs show the newly generated images; this is an example of view synthesis
    using the SynSin methodology ](img/B18217_09_1.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1：红框的照片展示了原始图像，蓝框的照片展示了新生成的图像；这是使用SynSin方法进行视图合成的一个示例](img/B18217_09_1.jpg)'
- en: 'Figure 9.1: The red-framed photographs show the original image, and the blue-framed
    photographs show the newly generated images; this is an example of view synthesis
    using the SynSin methodology'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：红框的照片展示了原始图像，蓝框的照片展示了新生成的图像；这是使用SynSin方法进行视图合成的一个示例。
- en: 'Many challenges need to be addressed. For the models, it’s hard to understand
    the 3D scene from an image. There are several methods for view synthesis:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 许多挑战需要解决。对于模型来说，从一张图像中理解3D场景是很困难的。视图合成有几种方法：
- en: '**View synthesis from multiple images**: Deep neural networks can be used to
    learn the depth of multiple images, and then reconstruct new images from another
    view. However, as mentioned earlier, this implies that we have multiple images
    from slightly different views, and sometimes, it’s hard to obtain such data.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多幅图像的视图合成**：深度神经网络可以用来学习多幅图像的深度，然后从另一个视角重建新图像。然而，如前所述，这意味着我们有来自稍微不同视角的多幅图像，有时很难获得这种数据。'
- en: '**View synthesis using ground-truth depth**: This involves a group of techniques
    where a ground-truth mask is used beside the image, which represents the depth
    of the image and semantics. Although in some cases, these types of models can
    achieve good results, it’s hard and expensive to gather data on a large scale,
    especially when it comes to outdoor scenes. Also, it’s expensive and time-consuming
    to annotate such data on a large scale, too.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用真实深度进行视图合成**：这涉及一组技术，其中使用真实深度掩码与图像一同表示图像的深度和语义。尽管在某些情况下，这些类型的模型可以取得不错的结果，但收集大规模数据是困难且昂贵的，尤其是在户外场景中。此外，对这类数据进行大规模标注也既昂贵又耗时。'
- en: '**View synthesis from a single image**: This is a more realistic setting when
    we have only one image and we aim to reconstruct an image from the new view. It’s
    harder to get more accurate results by only using one image. SynSin belongs to
    a group of methods that can achieve a state-of-the-art view synthesis.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单幅图像的视图合成**：当我们只有一张图像并且旨在从新视角重建图像时，这是一种更为现实的设置。仅使用一张图像很难获得更准确的结果。SynSin属于一类能够实现最先进视图合成方法的技术。'
- en: So, we have covered a brief overview of view synthesis. Now, we will explore
    SynSin, dive into the network architecture, and examine the model training and
    testing processes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们已经简要概述了视图合成的内容。接下来，我们将深入探讨SynSin，研究其网络架构，并检查模型的训练和测试过程。
- en: SynSin network architecture
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SynSin网络架构
- en: 'The idea of SynSin is to solve the view synthesis problem with an end-to-end
    model using only one image at test time. This is a model that doesn’t need 3D
    data annotations and acheives very good accuracy compared to its baseline:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: SynSin的核心思想是通过一个端到端模型来解决视图合成问题，并且在测试时只需使用一张图像。这个模型不需要3D数据注释，并且与基准模型相比，能实现非常好的准确度：
- en: '![Figure 9.2: The structure of the end-to-end SynSin method ](img/B18217_09_2.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2：端到端SynSin方法的结构](img/B18217_09_2.jpg)'
- en: 'Figure 9.2: The structure of the end-to-end SynSin method'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：端到端SynSin方法的结构
- en: 'The model is trained end-to-end, and it consists of three different modules:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是端到端训练的，包含三个不同的模块：
- en: Spatial feature and depth networks
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空间特征和深度网络
- en: Neural point cloud renderer
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经点云渲染器
- en: Refinement module and discriminator
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精炼模块和判别器
- en: Let’s dive deeper into each one to better understand the architecture.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解每个网络，以便更好地理解架构。
- en: Spatial feature and depth networks
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 空间特征和深度网络
- en: 'If we zoom into the first part of *Figure 9.2*, we can see two different networks
    that are fed by the same image. These are the spatial feature network (**f**)
    and the depth network (**d**) (*Figure 9.3*):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们放大看一下*图9.2*的第一部分，我们可以看到两种不同的网络，它们都输入相同的图像。这些分别是空间特征网络（**f**）和深度网络（**d**）（*图9.3*）：
- en: '![ Figure 9.3: Input and outputs of the spatial feature and depth networks
    ](img/B18217_09_3.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3：空间特征和深度网络的输入与输出](img/B18217_09_3.jpg)'
- en: 'Figure 9.3: Input and outputs of the spatial feature and depth networks'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3：空间特征和深度网络的输入与输出
- en: Given a reference image and the desired change in pose (**T**), we wish to generate
    an image as if that change in the pose were applied to the reference image. For
    the first part, we only use a reference image and feed it to two networks. A spatial
    feature network aims to learn feature maps, which are higher-resolution representations
    of the image. This part of the model is responsible for learning semantic information
    about the image. This model consists of eight ResNet blocks and outputs 64-dimensional
    feature maps for each pixel of the image. The output has the same resolution as
    the original image.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一张参考图像和期望的姿态变化（**T**），我们希望生成一张图像，好像该姿态变化已经应用到参考图像中。对于第一部分，我们只使用参考图像，并将其输入到两个网络中。空间特征网络的目标是学习特征图，这些特征图是图像的高分辨率表示。模型的这一部分负责学习图像的语义信息。该模型由八个ResNet块组成，并为图像的每个像素输出64维特征图。输出的分辨率与原始图像相同。
- en: Next, the depth network aims to learn the 3D structure of the image. It won’t
    be an accurate 3D structure, as we don’t use exact 3D annotations. However, the
    model will further improve it. UNet with eight downsampling and upsampling layers
    are used for this network, followed by the sigmoid layer. Again, the output has
    the same resolution as the original image.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，深度网络的目标是学习图像的3D结构。由于我们没有使用精确的3D注释，它不会是一个准确的3D结构。但该模型会进一步改善这一点。此网络使用了带有八个下采样和上采样层的UNet，并跟随一个sigmoid层。同样，输出具有与原始图像相同的分辨率。
- en: As you might have noticed, both models keep a high resolution for the output
    channels. This will further help to reconstruct more accurate and higher-quality
    images.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能注意到的，两个模型都保持了输出通道的高分辨率。这将进一步帮助重建更准确、更高质量的图像。
- en: Neural point cloud renderer
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经点云渲染器
- en: The next step is to create a 3D point cloud that can then be used with a view
    transform point to render a new image from the new viewpoint. For that, we use
    the combined output of the spatial feature and depth networks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个3D点云，然后可以使用视图变换点从新的视角渲染出新图像。为此，我们使用空间特征和深度网络的组合输出。
- en: 'The next step should be rendering the image from another point. In most scenarios,
    a naïve renderer would be used. This projects 3D points to one pixel or a small
    region in the new view. A naïve renderer uses a z-buffer, which keeps all the
    distances from the point to the camera. The problem with the naïve renderer is
    that it’s not differentiable, which means we can’t use gradients to update our
    depth and spatial feature networks. Moreover, we want to render features instead
    of RGB images. This means the naïve renderer won’t work for this technique:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步应该是从另一个视角渲染图像。在大多数情况下，会使用一个简单的渲染器。该渲染器将3D点投影到新视图中的一个像素或小区域。简单渲染器使用z-buffer来保存从点到相机的所有距离。简单渲染器的问题在于它不可微分，这意味着我们无法使用梯度来更新我们的深度和空间特征网络。此外，我们希望渲染的是特征，而不是RGB图像。这意味着简单渲染器不适用于这一技术：
- en: '![Figure 9.4: Pose transformation in the neural point cloud renderer ](img/B18217_09_4.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4：神经点云渲染器中的姿态变换](img/B18217_09_4.jpg)'
- en: 'Figure 9.4: Pose transformation in the neural point cloud renderer'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：神经点云渲染器中的姿态变换
- en: 'Why not just differentiate naïve renderers? Here, we face two problems:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不直接对朴素渲染器进行微分？在这里，我们面临两个问题：
- en: '**Small neighborhoods**: As mentioned earlier, each point only appears on one
    or a few pixels of the rendered image. Therefore, there are only a few gradients
    for each point. This is a drawback of local gradients, which degrades the performance
    of the network relying on gradient updates.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**小邻域**：如前所述，每个点仅出现在渲染图像的一个或几个像素上。因此，每个点只有少量的梯度。这是局部梯度的一个缺点，降低了依赖梯度更新的网络性能。'
- en: '**The hard z-buffer**: The z-buffer only keeps the nearest point for rendering
    the image. If new points appear closer, suddenly the output will change drastically.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬 z 缓冲区**：z 缓冲区只保留最近的点用于渲染图像。如果新的点出现得更近，输出将会突然发生剧烈变化。'
- en: 'To overcome the issues presented here, the model tries to soften the hard decision.
    This technique is called a **neural point cloud renderer**. To do that, the renderer,
    instead of assigning a pixel for a point, splats with varying influence. This
    solves a small neighborhood problem. For the hard z-buffer issue, we then accumulate
    the effect of the nearest points, not just the nearest point:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这里提出的问题，该模型试图软化硬决策。这项技术被称为 **神经点云渲染器**。为此，渲染器不是为每个点分配一个像素，而是使用不同的影响力进行撒布。这样可以解决小邻域问题。对于硬
    z 缓冲区问题，我们则通过积累最近点的影响，而不仅仅是最近的点：
- en: '![Figure 9.5: Projecting the point with the splatting technique ](img/B18217_09_5.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5：使用撒布技术投影点](img/B18217_09_5.jpg)'
- en: 'Figure 9.5: Projecting the point with the splatting technique'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5：使用撒布技术投影点
- en: 'A 3D point is projected and splatted with radius **r** (*Figure 9.5*). Then,
    the influence of the 3D point on that pixel is measured by the Euclidean distance
    between the center of the splatted point to that point:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 3D 点通过半径 **r**（*图 9.5*）进行投影并进行点撒布。然后，测量该 3D 点对该像素的影响，方法是计算撒布点的中心到该像素的欧几里得距离：
- en: '![Figure 9.6:  The effect of forward and backward propagation with a neural
    point cloud renderer on an example of naïve (b) and SynSin (c) rendering ](img/B18217_09_6.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6：神经点云渲染器在朴素渲染（b）和 SynSin 渲染（c）示例上的前向和反向传播效果](img/B18217_09_6.jpg)'
- en: 'Figure 9.6: The effect of forward and backward propagation with a neural point
    cloud renderer on an example of naïve (b) and SynSin (c) rendering'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：神经点云渲染器在朴素渲染（b）和 SynSin 渲染（c）示例上的前向和反向传播效果
- en: As you can see in the preceding figure, each point is splatted, which helps
    us to not lose too much information and helps in solving tricky problems.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的图所示，每个点都会进行撒布，这有助于我们不会丢失太多信息，并有助于解决棘手的问题。
- en: 'The advantage of this approach is that it allows you to gather more gradients
    for one 3D point, which improves the network learning process for both spatial
    features and depth networks:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优势在于，它允许为一个 3D 点收集更多梯度，从而改善网络对空间特征和深度网络的学习过程：
- en: '![Figure 9.7: Backpropagation for the naïve renderer and the neural point cloud
    renderer ](img/B18217_09_7.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7：朴素渲染器和神经点云渲染器的反向传播](img/B18217_09_7.jpg)'
- en: 'Figure 9.7: Backpropagation for the naïve renderer and the neural point cloud
    renderer'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：朴素渲染器和神经点云渲染器的反向传播
- en: 'Lastly, we need to gather and accumulate points in the z-buffer. First, we
    sort points according to their distance from the new camera, and then K-nearest
    neighbors with alpha compositing are used to accumulate points:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要在 z 缓冲区中收集并积累点。首先，我们根据与新相机的距离对点进行排序，然后使用 K 近邻算法和 alpha 合成技术来积累点：
- en: '![Figure 9.8: 3D point cloud output ](img/B18217_09_8.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8：3D 点云输出](img/B18217_09_8.jpg)'
- en: 'Figure 9.8: 3D point cloud output'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8：3D 点云输出
- en: As you can see in *Figure 9.8*, the 3D point cloud outputs an unrefined new
    view. The output should then become the input of the refiner module.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 9.8*所示，3D 点云输出的是一个未经精细化的新视图。该输出应该作为精细化模块的输入。
- en: Refinement module and discriminator
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精细化模块和判别器
- en: 'Last but not least, the model consists of a refinement module. This module
    has two missions: first to improve the accuracy of the projected feature and,
    second, to fill the nonvisible part of the image from the new view. It should
    output semantically meaningful and geometrically accurate images. For example,
    if only one part of the table is visible in the image and in the new view, the
    image should contain a larger part of it, this module should understand semantically
    that this is a table, and during the reconstruction, it should keep the lines
    of the new part geometrically correct (for instance, the straight lines should
    remain straight). The model learns these properties from a dataset of real-world
    images:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，该模型包括一个精炼模块。该模块有两个任务：首先是提高投影特征的准确性，其次是从新视角填充图像中不可见的部分。它应该输出语义上有意义且几何上准确的图像。例如，如果图像中只显示了桌子的某一部分，在新视角下，图像应该包含桌子的一部分，该模块应该从语义上理解这是桌子，并在重建过程中保持新部分的几何线条正确（例如，直线应该保持直线）。该模型从真实世界图像的数据集中学习这些特性：
- en: '![Figure 9.9: The refinement module ](img/B18217_09_9.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9：精炼模块](img/B18217_09_9.jpg)'
- en: 'Figure 9.9: The refinement module'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9：精炼模块
- en: The refinement module (**g**) gets inputs from the neural point cloud renderer
    and then outputs the final reconstructed image. Then, it is used in loss objectives
    to improve the training process.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 精炼模块（**g**）从神经点云渲染器获取输入，然后输出最终重建的图像。之后，它被用于损失目标中，以改善训练过程。
- en: This task is solved with generative models. ResNet with eight blocks is used,
    and to keep the resolution of the image good, downsampling and upsampling blocks
    were used, too. We use GAN with two multilayer discriminators and feature matching
    loss on the discriminator.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 该任务通过生成模型解决。使用了包含八个块的ResNet，为了保持图像的良好分辨率，还使用了下采样和上采样模块。我们使用了具有两个多层判别器和特征匹配损失的GAN。
- en: 'The final loss of the model consists of the L1 loss, content loss, and discriminator
    loss between the generated and target images:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的最终损失由L1损失、内容损失和生成图像与目标图像之间的判别器损失组成：
- en: '![](img/Formula_09_001.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_09_001.jpg)'
- en: The loss function is then used for model optimization as usual.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用损失函数进行模型优化，如往常一样。
- en: This is how SynSin combines various modules to create an end-to-end process
    of synthesizing views from just one image. Next, we will explore the practical
    implementation of the model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是SynSin如何结合各种模块，创建一个从单一图像合成视图的端到端过程。接下来，我们将探索模型的实际实现。
- en: Hands-on model training and testing
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的模型训练与测试
- en: 'Facebook Research released the GitHub repository of the SynSin model, which
    allows us to train the model and use an already pre-trained model for inference.
    In this section, we will discuss both the training process and inference with
    pre-trained models:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook Research发布了SynSin模型的GitHub仓库，使我们能够训练模型并使用已经预训练的模型进行推理。在这一部分，我们将讨论训练过程和使用预训练模型进行推理：
- en: 'But first, we need to set up the model. We need to clone the GitHub repository,
    create an environment, and install all the requirements:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 但首先，我们需要设置模型。我们需要克隆GitHub仓库，创建环境，并安装所有要求：
- en: '[PRE0]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If requirements can’t be installed with the preceding command, it’s always possible
    to install them manually. For manual installation, follow the `synsin/INSTALL.md`
    file instructions.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无法通过前面的命令安装要求，始终可以手动安装。手动安装时，请按照`synsin/INSTALL.md`文件中的说明操作。
- en: 'The model was trained on three different datasets:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型在三个不同的数据集上进行训练：
- en: '`RealEstate10K`'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`RealEstate10K`'
- en: '`MP3D`'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MP3D`'
- en: '`KITTI`'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`KITTI`'
- en: For the training, data can be downloaded from the dataset websites. For this
    book, we are going to use the `KITTI` dataset; however, feel free to try other
    datasets, too.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，数据可以从数据集网站下载。对于本书，我们将使用`KITTI`数据集；但也可以尝试其他数据集。
- en: Instructions on how to download the KITTI dataset can be found in the SynSin
    repository at [https://github.com/facebookresearch/synsin/blob/main/KITTI.md](https://github.com/facebookresearch/synsin/blob/main/KITTI.md).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如何下载KITTI数据集的说明可以在SynSin仓库的[https://github.com/facebookresearch/synsin/blob/main/KITTI.md](https://github.com/facebookresearch/synsin/blob/main/KITTI.md)找到。
- en: First, we need to download the dataset from the website and store the files
    in `${KITTI_HOME}/dataset_kitti`, where `KITTI_HOME` is the path where the dataset
    will be located.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从网站下载数据集，并将文件存储在`${KITTI_HOME}/dataset_kitti`，其中`KITTI_HOME`是数据集所在的路径。
- en: 'Next, we need to update the `./options/options.py` file, where we need to add
    the path to the KITTI dataset on our local machine:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要更新 `./options/options.py` 文件，在其中添加本地计算机上 KITTI 数据集的路径：
- en: '[PRE1]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you are going to use another dataset, you should find the `DataLoader` for
    other datasets and add the path to that dataset.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算使用其他数据集，应该找到其他数据集的 `DataLoader` 并添加该数据集的路径。
- en: 'Before training, we have to download pre-trained models by running the following
    command:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练之前，我们必须通过运行以下命令下载预训练模型：
- en: '[PRE2]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we open and look inside the file, we can see that it includes all the pre-trained
    models for all three datasets. Therefore, when running the command, it will create
    three different folders per dataset and download all the pre-trained models for
    that dataset. We can use them both for training and inference. If you don’t want
    them all downloaded, you can always download them manually by just running the
    following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打开文件并查看内部内容，会看到它包含了所有三个数据集的预训练模型。因此，在运行命令时，它会为每个数据集创建三个不同的文件夹，并下载该数据集的所有预训练模型。我们可以将它们用于训练和推理。如果你不想下载所有模型，可以通过运行以下命令手动下载：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This command will run the SynSin pre-trained model for the real estate dataset.
    For more information about pre-trained models, the `readme.txt` file can be downloaded
    as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将运行 SynSin 预训练模型，用于房地产数据集。如需了解更多关于预训练模型的信息，可以下载 `readme.txt` 文件：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For training, you need to run the `train.py` file. You can run it from the
    shell using `./train.sh`. If we open the `train.sh` file, we can find commands
    for the three different datasets. The default example for KITTI is as follows:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于训练，你需要运行 `train.py` 文件。你可以通过 `./train.sh` 从命令行运行它。如果我们打开 `train.sh` 文件，可以找到针对三个不同数据集的命令。KITTI
    的默认示例如下：
- en: '[PRE5]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can play with parameters and datasets and try to simulate the results of
    the original model. When the training process is complete, you can use your new
    model for evaluation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以调整参数和数据集，尝试模拟原始模型的结果。当训练过程完成后，你可以使用新的模型进行评估。
- en: 'For the evaluation, first, we need to have generated ground-truth images. To
    get that, we need to run the following code:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于评估，首先，我们需要生成的真实图像。为了得到这个，我们需要运行以下代码：
- en: '[PRE6]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We need to set the path where the results will be saved instead of `TEST_FOLDER`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置保存结果的路径，而不是 `TEST_FOLDER`。
- en: 'The first line exports a new variable, named `KITTI`, with the path of the
    images to the dataset. The next script creates the generated and ground-truth
    pairs for each image:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行导出一个名为 `KITTI` 的新变量，包含数据集图像的路径。接下来的脚本为每张图像创建生成的图像和真实图像配对：
- en: '![Figure 9.10: An example of the output of eval_kitti.py ](img/B18217_09_10.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.10：eval_kitti.py 输出示例](img/B18217_09_10.png)'
- en: 'Figure 9.10: An example of the output of eval_kitti.py'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10：eval_kitti.py 输出示例
- en: The first image is the input image, and the second image is the ground truth.
    The third image is the network output. As you might have noticed, the camera was
    moved forward slightly, and for this specific case, the model output seems very
    well generated.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 第一张图是输入图像，第二张是地面真相。第三张是网络输出。如你所见，相机略微向前移动，对于这种特定情况，模型输出似乎生成得非常好。
- en: 'However, we need some numerical representation to understand how well the network
    works. That is why we need to run the `evaluation/evaluate_perceptualsim.py` file,
    which will calculate the accuracy:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，我们需要一些数值表示来理解网络的表现。因此，我们需要运行 `evaluation/evaluate_perceptualsim.py` 文件，它将计算准确度：
- en: '[PRE7]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The preceding command will help to evaluate the model given the path to test
    images, where one of them is the predicted image and the other one is the target
    image.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将帮助评估模型，给定测试图像的路径，其中一张是预测图像，另一张是目标图像。
- en: 'The output from my test is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我的测试输出如下：
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: One of the metrics used in the evaluation is perceptual similarity, which measures
    distance in the VGG feature space. The closer to zero, the higher the similarity
    between images. PSNR is the next metric to measure image reconstruction. It calculates
    the ratio between the maximum signal power and the power of distorting noise,
    which, in our case, is the reconstructed image. Finally, the **Structural Similarity
    Index** (**SSIM**) is a metric that quantifies the deterioration in image quality.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 用于评估的指标之一是感知相似度，它衡量VGG特征空间中的距离。越接近零，图像之间的相似度越高。PSNR是另一个衡量图像重建的指标。它计算最大信号功率与失真噪声功率之间的比率，在我们这种情况下，失真噪声就是重建图像。最后，**结构相似性指数**（**SSIM**）是一个量化图像质量退化的指标。
- en: 'Next, we can use a pre-trained model for inference. We need an input image
    that we will use for inference:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用预训练模型进行推理。我们需要一张输入图像用于推理：
- en: '![Figure 9.11: The input image for inference ](img/B18217_09_11.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11：推理的输入图像](img/B18217_09_11.jpg)'
- en: 'Figure 9.11: The input image for inference'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11：推理的输入图像
- en: Next, we will use the `realestate` model to generate a new image. First, we
    need to set up the model.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`realestate`模型生成一张新图像。首先，我们需要设置模型。
- en: The codes for setting up the model can be found in the GitHub repository in
    a file called `set_up_model_for_inference.py`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 设置模型的代码可以在GitHub仓库中的`set_up_model_for_inference.py`文件中找到。
- en: 'To set up the model, first, we need to import all the necessary packages:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置模型，首先，我们需要导入所有必要的包：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we are going to create a function that takes the model path as an input
    and outputs the model ready for inference. We will break the whole function into
    smaller chunks to understand the code better. However, the complete function can
    be found on GitHub:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个函数，它以模型路径为输入并输出准备好的推理模型。为了更好地理解代码，我们将把整个函数分解成更小的部分。然而，完整的函数可以在GitHub上找到：
- en: '[PRE10]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we enable the `cudnn` package and define the device on which the model
    will be working. Also, the second line imports the model, allowing it to gain
    access to all the options set for the training, which can be modified if needed.
    Note that `render_ids` refers to the GPU ID, which, in some cases, might be different
    for users with different hardware setups.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们启用`cudnn`包并定义模型将要运行的设备。此外，第二行导入模型，使其可以访问为训练设置的所有选项，如果需要可以进行修改。请注意，`render_ids`指的是GPU
    ID，在某些情况下，可能由于用户硬件配置不同而有所不同。
- en: 'Next, we define the model:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义模型：
- en: '[PRE11]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `get_model` function is imported from the `options.py` file, which loads
    the weights and returns the final model. Then, from `options`, we check whether
    we have a synchronized model, which means we are running the model on different
    machines. If we have it, we run the `convert_model` function, which takes the
    model and replaces all the `BatchNorm` modules with the `SunchronizedBatchNorm`
    modules.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_model`函数从`options.py`文件导入，它加载权重并返回最终模型。然后，从`options`中，我们检查是否有同步模型，这意味着我们在不同的机器上运行该模型。如果有，我们运行`convert_model`函数，它会将模型中的所有`BatchNorm`模块替换为`SynchronizedBatchNorm`模块。'
- en: 'Finally, we load the model:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们加载模型：
- en: '[PRE12]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `BaseModel` function sets up the final mode. Depending on the train or test
    mode, it can set the optimizer and initialize the weights. In our case, it will
    set up the model for test mode.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaseModel`函数设置了最终的模式。根据训练模式或测试模式，它可以设置优化器并初始化权重。在我们的例子中，它将为测试模式设置模型。'
- en: All this code is summed up in one function called `synsin_model`, which we will
    import for inference.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些代码被总结在一个名为`synsin_model`的函数中，我们将导入该函数用于推理。
- en: The following code is from the `inference_unseen_image.py` file. We will write
    a function that takes the model path, the test image, and the new view transformation
    parameters and will output the new image from the new view. If we specify the
    `save_path` parameter, it will automatically save the output image.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码来自`inference_unseen_image.py`文件。我们将编写一个函数，它接收模型路径、测试图像和新的视图转换参数，并输出来自新视图的图像。如果我们指定了`save_path`参数，它将自动保存输出图像。
- en: 'Again, we will first import all the modules needed for inference:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，我们将首先导入所有需要用于推理的模块：
- en: '[PRE13]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we set up the model and create the data transformation for preprocessing:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们设置模型并创建用于预处理的数据转换：
- en: '[PRE14]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we need to specify the view transformation parameters:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要指定视图转换参数：
- en: '[PRE15]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we need to specify parameters for rotation and translation. Note that
    `theta` and `phi` are responsible for rotation, while `tx`, `ty`, and `tz` are
    used for translation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要指定旋转和平移的参数。注意，`theta`和`phi`负责旋转，而`tx`、`ty`和`tz`用于平移。
- en: 'Next, we are going to use the uploaded image and new transformation to get
    output from the network:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用上传的图像和新的变换来获取网络的输出：
- en: '[PRE16]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, `pred_imgs` is the model output that is the new image, and depth is the
    3D depth predicted by the model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`pred_imgs`是模型输出的新图像，depth是模型预测的3D深度。
- en: 'Finally, we will use the output of the network to visualize the original image,
    the new predicted image, and the output image:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将使用网络的输出可视化原始图像、新预测图像和输出图像：
- en: '[PRE17]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We use `matplotlib` to visualize the output. Here is the result of the following
    code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`matplotlib`来可视化输出。以下是以下代码的结果：
- en: '![Figure 9.12: Result of the inference ](img/B18217_09_12.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.12: 推理结果](img/B18217_09_12.jpg)'
- en: 'Figure 9.12: Result of the inference'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.12: 推理结果'
- en: As we can see, we have a new view, and the model reconstructs the new angle
    very well. Now we can play with transformation parameters, to generate images
    from another view.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，我们获得了一个新的视角，模型很好地重建了新角度。现在我们可以调整变换参数，从另一个视角生成图像。
- en: 'If we slightly change `theta` and `phi`, we get another view transformation.
    Now we will reconstruct the right part of the image:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们稍微改变`theta`和`phi`，就会得到另一个视角的变换。现在我们将重建图像的右侧部分：
- en: '[PRE18]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output looks like this:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下所示：
- en: '![Figure 9.13: The result of the inference ](img/B18217_09_13.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.13: 推理结果](img/B18217_09_13.jpg)'
- en: 'Figure 9.13: The result of the inference'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.13: 推理结果'
- en: Changing the transformation parameters all at once or changing them in bigger
    steps can result in worse accuracy.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性大幅度改变变换参数或步长较大可能会导致较差的精度。
- en: 'Now we know how to create an image from the new view. Next, we will write some
    brief code to sequentially create images and make a small video:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们知道如何从新视角创建图像。接下来，我们将编写一些简短的代码，顺序地创建图像并制作一个小视频：
- en: '[PRE19]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This chunk of code takes an image as input and, for the given number of frames,
    generates sequential images. By sequential, we mean that each output of the model
    becomes the input for the next image generation:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将图像作为输入，并为给定数量的帧生成顺序图像。所谓顺序，是指模型的每个输出成为下一次图像生成的输入：
- en: '![Figure 9.14: Sequential view synthesis ](img/B18217_09_14.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.14: 顺序视图合成](img/B18217_09_14.jpg)'
- en: 'Figure 9.14: Sequential view synthesis'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.14: 顺序视图合成'
- en: In the preceding figure, there are four consecutive frames. As you can see,
    it’s harder and harder for the model to generate good images when we try bigger
    steps. This is a good time to start playing with the model’s hyperparameters,
    different camera settings, and step sizes to see how it can improve or reduce
    the accuracy of the model’s output.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，有四个连续的帧。正如你所看到的，当我们尝试较大步长时，模型生成的图像越来越差。此时正是开始调整模型超参数、不同相机设置和步长大小的好时机，以查看它如何改善或减少模型输出的精度。
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: At the beginning of the chapter, we looked at the SynSin model structure, and
    we gained a deep understanding of the end-to-end process of the model. As mentioned
    earlier, one interesting approach during the model creation was a differentiable
    renderer as a part of the training. Also, we saw that the model helps to solve
    the problem of not having a huge, annotated dataset, or if you don’t have multiple
    images for test time. That is why this is a state-of-the-art model, which would
    be easier to use in real-life scenarios. We looked at the pros and cons of the
    model. Also, we looked at how to initialize the model, train, test, and use new
    images for inference.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开始时，我们看到了SynSin模型的结构，并深入了解了该模型的端到端过程。如前所述，模型创建过程中的一个有趣方法是使用可微渲染器作为训练的一部分。此外，我们还看到该模型有助于解决没有大量标注数据集，或者在测试时没有多张图像的问题。这就是为什么这是一个最先进的模型，它更容易在实际场景中使用。我们查看了该模型的优缺点，也看了如何初始化模型、训练、测试并使用新图像进行推理。
- en: In the next chapter, we will look at the Mesh R-CNN model, which combines two
    different tasks (object detection and 3D model construction) into one model. We
    will explore the architecture of the model and test the model performance on a
    random image.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将探讨Mesh R-CNN模型，它将两种不同的任务（目标检测和3D模型构建）结合成一个模型。我们将深入了解该模型的架构，并在一张随机图像上测试模型性能。
