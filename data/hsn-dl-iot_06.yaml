- en: Audio/Speech/Voice Recognition in IoT
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IoT中的音频/语音/语音识别
- en: Automatic audio/speech/voice recognition is becoming a common, convenient way
    for people to interact with their devices, including smartphones, wearables, and
    other smart devices. Machine learning and DL algorithms are useful for audio/speech/voice
    recognition and decision making. Consequently, they are very promising for IoT
    applications, which rely on audio/speech/voice recognition for their activity
    and decisions. This chapter will present DL-based speech/voice data analysis and
    recognition in IoT applications in general.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自动音频/语音/语音识别正成为人们与设备（包括智能手机、可穿戴设备和其他智能设备）交互的常见、便捷方式。机器学习和深度学习算法对于音频/语音/语音识别和决策制定非常有用。因此，它们对依赖音频/语音/语音识别进行活动和决策的IoT应用非常有前景。本章将总体介绍基于深度学习的IoT应用中的语音/语音数据分析和识别。
- en: 'The first part of this chapter will briefly describe different IoT applications
    and their speech/voice recognition-based decision making. In addition, it will
    briefly discuss two IoT applications and their speech/voice recognition-based
    implementations in a real-world scenario. In the second part of the chapter, we
    will present a hands-on speech/voice detection implementation of the applications
    using DL algorithms. We will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分将简要描述不同的IoT应用及其基于语音/语音识别的决策制定。此外，还将简要讨论两个IoT应用及其在实际场景中的基于语音/语音识别的实现。第二部分将介绍使用深度学习算法的应用的语音/语音检测实现。我们将涵盖以下主题：
- en: IoT applications and audio/speech recognition
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IoT应用与音频/语音识别
- en: Use case one – voice-controlled smart light
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例一 – 语音控制的智能灯
- en: Implementing a voice-controlled smart light
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现语音控制的智能灯
- en: Use case two – voice-controlled home access
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用例二 – 语音控制的家庭访问
- en: Implementing voice-controlled home access
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现语音控制的家庭访问
- en: DL for audio/speech recognition in IoT
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IoT中的音频/语音识别的深度学习（DL）
- en: DL algorithms for audio/speech recognition in IoT applications
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IoT应用中的音频/语音识别的深度学习算法
- en: Different deployment options for DL-based audio/speech recognition in IoT
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于深度学习的音频/语音识别在IoT中的不同部署选项
- en: Data collection and preprocessing
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集与预处理
- en: Model training
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练
- en: Evaluating models
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型评估
- en: Speech/voice recognition for IoT
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IoT中的语音/语音识别
- en: 'Like image recognition, the speech/voice recognition landscape in IoT applications
    is rapidly changing. In recent years, consumers have become depending on voice
    command features and this has been fueled by Amazon, Google, Xiomi, and other
    companies'' voice-enabled search and/or devices. This technology is becoming an
    extremely useful technology for users. Statistics show that around 50% of households
    ([https://techcrunch.com/2017/11/08/voice-enabled-smart-speakers-to-reach-55-of-u-s-households-by-2022-says-report/](https://techcrunch.com/2017/11/08/voice-enabled-smart-speakers-to-reach-55-of-u-s-households-by-2022-says-report/))
    in the United States use voice-activated commands for accessing online content.
    Thus, IoT, machine learning, and DL-supported speech/voice recognition has revolutionized
    the focus of businesses and consumer expectations. Many industries—including home
    automation, healthcare, automobiles, and entertainment—are adopting voice-enabled
    IoT applications. As shown in the following diagram, these applications use one
    or more of the following speech/voice recognition services:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 像图像识别一样，IoT应用中的语音/语音识别领域正在迅速变化。近年来，消费者已经开始依赖语音命令功能，这一趋势得到了亚马逊、谷歌、小米等公司语音启用搜索和/或设备的推动。这项技术正变得对用户极为有用。统计数据显示，约50%的美国家庭（[https://techcrunch.com/2017/11/08/voice-enabled-smart-speakers-to-reach-55-of-u-s-households-by-2022-says-report/](https://techcrunch.com/2017/11/08/voice-enabled-smart-speakers-to-reach-55-of-u-s-households-by-2022-says-report/)）使用语音激活命令来访问在线内容。因此，IoT、机器学习和深度学习支持的语音/语音识别已经彻底改变了商业和消费者期望的焦点。许多行业——包括家庭自动化、医疗保健、汽车和娱乐——正在采用语音启用的IoT应用。如下图所示，这些应用使用了以下一种或多种语音/语音识别服务：
- en: '![](img/8cdd501c-ab06-4b16-b800-936f0e920c28.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8cdd501c-ab06-4b16-b800-936f0e920c28.png)'
- en: '**Speech/command Recognition:** Voice-controlled IoT applications are gaining
    popularity in many application domains, such as smart home/office, smart hospital,
    and smart cars, because of their convenience. For example, a mobility disabled
    person may find difficulty in switching on their TV or light. A voice-controlled/commanded
    TV/light can ease this difficulty by turning on the TV/light simply by listening
    to a voice. This will offer independent living to many disabled individuals and/or
    people with special needs. Voice-activated smart microwave ovens can revolutionize
    cooking. Moreover, a voice enabled smart speaker can assist with and answer many
    common questions in many public service areas, such as hospitals, airports, and
    train stations. For example, a smart voice-enabled speaker can answer patients''
    common questions in hospital, such as when the visiting time is and who the ward
    doctor is.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音/命令识别：** 语音控制的物联网应用在许多应用领域中越来越受欢迎，如智能家居/办公室、智能医院和智能汽车，因为它们的便利性。例如，行动不便的人可能会发现开关电视或灯光困难。语音控制/命令的电视/灯光可以通过简单地听取声音来打开电视/灯光，从而缓解这一困难。这将为许多残疾人士和/或有特殊需求的人提供独立的生活方式。语音激活的智能微波炉可以彻底改变烹饪方式。此外，语音启用的智能音响可以帮助并回答许多常见问题，服务于医院、机场、火车站等公共服务领域。例如，智能语音音响可以回答医院中患者的常见问题，比如探视时间是什么时候，病房医生是谁。'
- en: '**Person/Speaker Identification:** Speaker/person recognition is the second
    important service provided by IoT applications that has received the spotlight
    in recent years. The key applications that are utilizing DL/machine learning-based
    speaker recognition services include personalized voice-controlled assistants,
    smart home appliances, biometric authentication in security services, criminal
    investigations, and smart cars [1,2]. Voice-controlled home/office access is an
    example of biometric authentication.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人/说话人识别：** 说话人/人物识别是物联网应用提供的第二个重要服务，近年来备受关注。利用基于深度学习/机器学习的说话人识别服务的关键应用包括个性化语音助手、智能家电、安全服务中的生物认证、刑事调查和智能汽车[1,2]。语音控制的家庭/办公室访问是生物认证的一个例子。'
- en: '**Sentiment Analysis/Emotion Detection:** User emotion detection or sentiment
    analysis can be useful in providing personalized and effective services to the
    user. IoT applications, such as smart healthcare [3], smart education, and security
    and safety, can improve their services through DL-based emotion detection or sentiment
    analysis. For example, in a smart classroom, a teacher can analyze the students''
    sentiments in real time or quasi real time to offer personalized and/or group-wise
    teaching. This will improve their learning experience.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析/情绪检测：** 用户情绪检测或情感分析对于提供个性化和高效的服务非常有用。物联网应用，如智能医疗[3]、智能教育以及安全和保障服务，可以通过基于深度学习的情绪检测或情感分析来提升其服务。例如，在智能教室中，教师可以实时或准实时分析学生的情绪，以提供个性化和/或小组教学。这将改善学生的学习体验。'
- en: '**Language Translation:** There are 6,500 ([https://www.infoplease.com/askeds/how-many-spoken-languages](https://www.infoplease.com/askeds/how-many-spoken-languages))
    active spoken languages worldwide, and this is a challenge to effective communication
    and interoperability. Many public services, such as the immigration office, can
    use a translator instead of a paid interpreter. Tourists can use smart devices,
    such as **ILI** ([https://iamili.com/us/](https://iamili.com/us/)), to effectively
    communicate with others.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言翻译：** 全球有6500种活跃的口语语言（[https://www.infoplease.com/askeds/how-many-spoken-languages](https://www.infoplease.com/askeds/how-many-spoken-languages)），这给有效沟通和互操作性带来了挑战。许多公共服务，如移民办公室，可以使用翻译器代替付费口译员。游客可以使用智能设备，如**ILI**（[https://iamili.com/us/](https://iamili.com/us/)），与他人有效沟通。'
- en: Use case one – voice-controlled smart light
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例一 – 语音控制智能灯
- en: According to the **World Health Organisation** (**WHO**), more than one billion
    people in the world live with some form of disability. Almost 20% of them are
    experiencing considerable difficulties in functioning and living independently.
    In the future, disability will be an even bigger concern because of its increasing
    prevalence. IoT applications, such as smart home, with the support of machine
    learning/DL, can offer support to this community and improve their quality of
    life through independence. One of these applications is a voice-activated smart
    light/fan control.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 根据**世界卫生组织**（**WHO**）的统计，全球有超过十亿人生活在某种形式的残疾中。 其中约20%的人在功能和独立生活方面面临显著困难。 未来，由于残疾患病率的不断增加，残疾问题将变得更加严重。
    得到机器学习/深度学习支持的物联网应用，如智能家居，能够为这一群体提供支持，并通过提高独立性改善他们的生活质量。 其中一个应用就是语音激活的智能灯/风扇控制。
- en: An individual facing a disability such as mobility impairment faces various
    difficulties in living their day-to-day life. One of these difficulties is switching
    on/off home or office lights/fans/other devices. Voice-activated smart control
    of home/office lights/fans/other devices is an IoT application. However, voice
    recognition and the correct detection of a given command is not an easy job. A
    person's accent, pronunciation, and ambient noises can make the person's voice
    recognition difficult. An appropriate DL algorithm trained on a significantly
    large voice dataset can be useful in addressing these issues and can make a working
    voice-controlled smart light application.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 面对残疾（如行动障碍）的个体，在日常生活中会面临各种困难。 其中一个困难是开关家里或办公室的灯/风扇/其他设备。 语音激活的家居/办公室灯/风扇/其他设备的智能控制是一种物联网应用。
    然而，语音识别和准确检测给定命令并不是一件容易的事情。 一个人的口音、发音和周围噪音可能会使得语音识别变得困难。 适当的深度学习（DL）算法，经过在大规模语音数据集上训练，能够有效解决这些问题，并可以实现一个功能完备的语音控制智能灯应用。
- en: Implementing use case one
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现用例一
- en: 'The following diagram presents the key components needed for the implementation
    of a voice-activated light (in a room):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了实现语音激活灯光（在房间内）的关键组件：
- en: '![](img/98aa3b44-3195-4593-a2c6-56f5c37c2678.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/98aa3b44-3195-4593-a2c6-56f5c37c2678.png)'
- en: 'As shown in the preceding diagram, the implementation of the use case will
    need the following components:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，实施该用例将需要以下组件：
- en: '**Sensors and a Computing Platform**: For this use case, we are considering
    two omnidirectional microphones that are installed on the walls of the room. These
    microphones are wirelessly connected to a computing platform. In this use case,
    we are using a Raspberry Pi 3 as the computing platform, and this can work as
    the smart home’s edge-computing device to control the IoT devices deployed in
    the home. We need two more devices: a 433 MHz wireless transmitter, connected
    to the Raspberry Pi, to transmit the processed commands to the switch, and a 433
    MHz remote control or wirelessly controlled mains socket to control the light
    or target device.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传感器和计算平台**：对于这个用例，我们考虑在房间墙上安装两个全向麦克风。 这些麦克风通过无线方式与计算平台连接。 在这个用例中，我们使用树莓派3作为计算平台，它可以作为智能家居的边缘计算设备，控制家居中部署的物联网设备。
    我们还需要另外两种设备：一台433 MHz无线发射器，通过树莓派连接，用于将处理后的命令传输到开关；以及一台433 MHz遥控器或无线控制的电源插座，用于控制灯光或目标设备。'
- en: '**Voice-Activated Command Detection and Control**: In this phase, the edge-computing
    device will be installed with one app. The installed app on the Raspberry Pi will
    be loaded with a pre-trained voice command detection and classification model.
    Once one of the microphones receives a “switch off the light” command or similar,
    it sends the received commands to the Raspberry Pi for processing and detection
    using the DL model. Finally, the Raspberry Pi transmits detected commands to the
    wirelessly controlled mains socket for the necessary action to be taken on the
    light.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音激活命令检测与控制**：在此阶段，边缘计算设备将安装一个应用程序。 安装在树莓派上的应用程序将加载一个预训练的语音命令检测与分类模型。 一旦某个麦克风接收到“关灯”命令或类似的指令，它将把收到的命令发送到树莓派进行处理和检测，使用深度学习模型进行分析。
    最终，树莓派将检测到的命令传输给无线控制的电源插座，从而对灯光进行必要的操作。'
- en: '**Desktop or Server for Model Learning**: We also need a desktop/server or
    access to a cloud computing platform in order to learn the model for voice detection
    and classification using reference datasets. This learned model will be preinstalled
    in the Raspberry Pi.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**桌面或服务器用于模型学习**：我们还需要一台桌面计算机/服务器或访问云计算平台，用于通过参考数据集学习语音检测和分类的模型。该学习模型将预先安装在树莓派上。'
- en: The second part (in the sections starting from *DL for Sound/Audio Recognition
    in IoT*) of the chapter will describe the implementation of the DL-based anomaly
    detection of the preceding use case. All the necessary code is available in the
    chapter's code folder.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本章第二部分（从*DL用于物联网中的声音/音频识别*部分开始）将描述基于深度学习的前述用例的异常检测实现。所有必要的代码可以在本章的代码文件夹中找到。
- en: Use case two – voice-controlled home access
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例二 – 语音控制家居访问
- en: Creating secure and friendly access to homes, offices, and any other premises
    is a challenging task, as it may need keys or an access card (such as a hotel
    room access card) that a user may not always remember to carry with them. The
    use of smart devices, including IoT solutions, can offer secure and friendly access
    to many premises. A potential approach to smart and secure access to homes/offices
    is image recognition-based identification of people and the opening of a door/gate
    accordingly. However, one problem with this approach is that any intruder can
    collect a photograph of one or more permitted persons and present the photo to
    the installed camera to access the office/home. One solution to this problem is
    to use a combination of image recognition and voice recognition or only voice
    recognition to allow access to the home/office.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 创建安全且友好的家居、办公室及其他场所的访问方式是一个具有挑战性的任务，因为它可能需要钥匙或访问卡（如酒店房间卡），而用户不一定总是记得随身携带。智能设备的使用，包括物联网解决方案，可以为许多场所提供安全且友好的访问方式。智能安全访问的一个潜在方法是基于图像识别对人员进行身份验证，并据此打开门/门禁。然而，这种方法的问题在于，任何入侵者都可以收集一个或多个被允许进入人员的照片，并将照片展示给已安装的摄像头，从而获得访问权限。解决这一问题的一个方法是结合使用图像识别和语音识别，或仅使用语音识别来允许进入家门/办公室。
- en: A voice biometric (or voiceprint) is unique to every individual, and mimicking
    this is a challenging task. However, detection of this unique property is not
    an easy job. DL-based speech recognition can identify unique properties and the
    corresponding person, and allow access only to that person.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 声音生物识别（或声音指纹）对每个人来说都是独一无二的，模拟这一特征是一个具有挑战性的任务。然而，检测这一独特属性并非易事。基于深度学习的语音识别可以识别独特的属性和相应的人，并且仅允许该人访问。
- en: Implementing use case two
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现用例二
- en: 'As shown in the following diagram, the implementation of the voice-activated
    light (in a room) use case consists of three main elements:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，语音激活灯光（在房间内）使用案例的实现包含三个主要元素：
- en: '![](img/a239f740-81e4-43d7-ac66-b7eefd4c15b8.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a239f740-81e4-43d7-ac66-b7eefd4c15b8.png)'
- en: '**Sensors and computing platform**: For this use case, we are considering one
    omnidirectional microphone installed in the entrance of the home and connected
    to the computing platform wirelessly or concealed in the walls. For the computing
    platform, we are using a Raspberry Pi , and this will work as the smart home''s
    edge-computing device to control the IoT devices deployed in the home. Also, the
    door is installed with a digital lock system that can be controlled through a
    computer.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传感器和计算平台**：对于此用例，我们考虑在家门口安装一台全向麦克风，且通过无线方式与计算平台连接，或将其隐藏在墙内。对于计算平台，我们使用树莓派，它将作为智能家居的边缘计算设备，控制家中部署的物联网设备。同时，门上安装了一个数字锁系统，可以通过计算机进行控制。'
- en: '**V****oice-activated command detection and control**: In this phase, the edge-computing
    device will be installed with one app. The installed app on the Raspberry Pi will
    be loaded with a pre-trained speaker or person detection and classification model.
    Once an authentic user talks to the door microphone, it gathers the audio signals
    and sends the received speech signal to the Raspberry Pi for processing and person
    detection using the DL model. If the detected person is on the **white list**
    (the list of occupants of the home) of the smart home controller (Raspberry Pi,
    in this case), the controller will command the door to be unlocked, otherwise
    it won''t.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语音激活命令检测与控制**：在这一阶段，边缘计算设备将安装一个应用程序。安装在树莓派上的应用程序将加载一个预训练的扬声器或人声检测与分类模型。一旦真实用户对门口的麦克风说话，设备便会收集音频信号，并将接收到的语音信号发送到树莓派进行处理和人脸检测，使用深度学习模型。如果检测到的人在智能家居控制器（此处为树莓派）的**白名单**（家中住户名单）中，控制器将命令门解锁，否则不会解锁。'
- en: '**Desktop or server for model learning:** We also need a desktop/server or
    access to a cloud computing platform in order to learn the model for voice detection
    and classification using reference datasets. This learned model will be preinstalled
    in the Raspberry Pi.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**桌面或服务器用于模型学习**：我们还需要一个桌面/服务器或访问云计算平台，以便使用参考数据集学习语音检测和分类模型。该学习的模型将预安装在树莓派中。'
- en: All the following sections describe the implementation of the DL-based command/speaker
    recognition needed for the aforementioned use cases. All the necessary code is
    available in the chapter's code folder.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下所有章节描述了实现基于深度学习的命令/扬声器识别的过程，这些是上述应用场景所需的。所有必要的代码可以在章节的代码文件夹中找到。
- en: DL for sound/audio recognition in IoT
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物联网中的声音/音频识别的深度学习
- en: It is important to understand the working principle of an **Automatic Speech
    Recognition** (**ASR**) system before discussing the useful DL models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论有用的深度学习模型之前，理解**自动语音识别**（**ASR**）系统的工作原理是非常重要的。
- en: ASR system model
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ASR系统模型
- en: An **Automatic Speech Recognition** (**ASR**) system needs three main sources
    of knowledge. These sources are known as an **acoustic model**, a **phonetic lexicon**,
    and a **language model** [4]. Generally, an acoustic model deals with the sounds
    of language, including the phonemes and extra sounds (such as pauses, breathing,
    background noise, and so on). On the other hand, a phonetic lexicon model or dictionary
    includes the words that can be understood by the system, with their possible pronunciations.
    Finally, a language model includes knowledge about the potential word sequences
    of a language. In recent years, DL approaches have been extensively used in acoustic
    and language models of ASR.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**自动语音识别**（**ASR**）系统需要三个主要的知识来源。这些来源分别是**声学模型**、**语音词典**和**语言模型**[4]。通常，声学模型处理语言的声音，包括音素和额外的声音（如停顿、呼吸、背景噪音等）。另一方面，语音词典模型或字典包括系统可以理解的单词及其可能的发音。最后，语言模型包含有关语言潜在单词序列的知识。近年来，深度学习（DL）方法已广泛应用于ASR的声学和语言模型中。'
- en: 'The following diagram presents a system model for **automatic speech recognition**
    (**ASR**). The model consists of three main stages:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了一个**自动语音识别**（**ASR**）的系统模型。该模型由三个主要阶段组成：
- en: Data gathering
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集
- en: Signal analysis and feature extraction (also known as **preprocessing**)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号分析与特征提取（也称为**预处理**）
- en: 'Decoding/identification/classification. As shown in the following diagram,
    DL will be used in the identification stage:'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码/识别/分类。如以下图所示，深度学习将用于识别阶段：
- en: '![](img/eff5f6b3-cac2-47d8-9b2c-897050ff357f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eff5f6b3-cac2-47d8-9b2c-897050ff357f.png)'
- en: Features extraction in ASR
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ASR中的特征提取
- en: '**Features extraction** is an important preprocessing stage in a DL pipeline
    of ASR. This stage consists of an analyzer and the extraction of audio fingerprints
    or features. This stage also mainly computes a sequence of feature vectors, which
    provides a compact representation of a gathered speech signal. Generally, this
    task can be performed in three key steps. The first step is known as speech analysis.
    This step carries out a spectra-temporal analysis of the speech signal and generates
    raw features describing the envelope of the power spectrum of short speech intervals.
    The second step extracts an extended feature vector that consists of static and
    dynamic features. The final step converts these extended feature vectors into
    more compact and robust vectors. Importantly, these vectors are the input for
    a DL-based command/speaker/language recognizer.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征提取**是自动语音识别（ASR）深度学习（DL）流程中的一个重要预处理阶段。这个阶段包括一个分析器和音频指纹或特征的提取。这个阶段主要计算一系列特征向量，提供了对采集到的语音信号的紧凑表示。通常，这个任务可以分为三个关键步骤。第一步称为语音分析。这一步对语音信号进行频谱时域分析，生成描述短语音间隔功率谱包络的原始特征。第二步提取一个扩展的特征向量，包含静态和动态特征。最后一步将这些扩展的特征向量转换为更紧凑、更鲁棒的向量。重要的是，这些向量是基于深度学习的命令/说话人/语言识别器的输入。'
- en: 'A number of feature extraction methods are available for ASR, and **Linear
    Predictive Codes** (**LPC**), **Perceptual Linear Prediction** (**PLP**), and
    **Mel Frequency Cepstral Coefficients** (**MFCC**) are widely used ones. MFCC
    is the most widely used method for feature extraction. The following diagram presents
    the key components of MFCC:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种特征提取方法可用于ASR，**线性预测编码**（**LPC**）、**感知线性预测**（**PLP**）和**梅尔频率倒谱系数**（**MFCC**）是其中广泛使用的方法。MFCC是最广泛使用的特征提取方法。下图展示了MFCC的关键组成部分：
- en: '![](img/e1502bfa-3932-49b9-a2b3-bffe7a5d0e4d.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1502bfa-3932-49b9-a2b3-bffe7a5d0e4d.png)'
- en: 'The key steps of the MFCC are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: MFCC的关键步骤如下：
- en: Inputting sound files and converting them to original sound data (a time domain
    signal).
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入音频文件并将其转换为原始声音数据（时域信号）。
- en: Converting time domain signals into frequency domain signals through short-time
    Fourier transforms, windowing, and framing.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过短时傅里叶变换、加窗和帧分割将时域信号转换为频域信号。
- en: Turning frequency into a linear relationship that humans can perceive through
    Mel spectrum transformation.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过梅尔谱变换将频率转换为人类可感知的线性关系。
- en: Separating the DC component from the sine component by adopting DCT Transform
    through Mel cepstrum analysis.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过梅尔倒谱分析采用离散余弦变换（DCT）将直流成分与正弦成分分离。
- en: Extracting sound spectrum feature vectors and converting them into images.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取声音频谱特征向量并将其转换为图像。
- en: DL models for ASR
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ASR的深度学习模型
- en: A number of DL algorithms or models have been used in ASR. A **Deep Belief Network**
    (**DBN**) is one of the early implementations of DL in ASR. Generally, it has
    been used as a pre-training layer with a single supervised layer of a **Deep Neural
    Network** (**DNN**). **Long Short-Term Memory** (**LSTM**) has been used for large-scale
    acoustic modeling. **Time Delay Neural Network** (**TDNN**) architectures have
    been used for audio signal processing. CNN, which has popularized DL, is also
    used as DL architecture for ASR. Use of DL architectures has significantly improved
    the speech recognition accuracy of ASRs. However, not all DL architectures have
    shown improvements, especially in different types of audio signals and environments,
    such as noisy and reverberant environments. CNNs can be used to reduce spectral
    variations and model the spectral correlation that exists in a speech signal.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 许多深度学习算法或模型已被应用于自动语音识别（ASR）。**深度信念网络**（**DBN**）是深度学习在ASR中的早期实现之一。通常，它作为预训练层与一个单独的监督层的**深度神经网络**（**DNN**）一起使用。**长短期记忆**（**LSTM**）已被用于大规模声学建模。**时延神经网络**（**TDNN**）架构已被用于音频信号处理。CNN（卷积神经网络），它普及了深度学习，也被用作ASR的深度学习架构。深度学习架构的使用显著提高了ASR的语音识别准确性。然而，并非所有深度学习架构都显示出改进，特别是在不同类型的音频信号和环境中，如嘈杂和混响环境。CNN可以用于减少频谱变化并建模语音信号中存在的频谱相关性。
- en: '**Recurrent Neural Networks** (**RNNs**) and LSTM are widely used in continuous
    and/or natural language processing because of the capability to incorporate temporal
    features of input during evolution. On the contrary, CNNs are good for short and
    non-continuous audio signals because of their translation invariance, such as
    the skill of discovering structure patterns, regardless of the position. In addition,
    CNNs show the best performance for speech recognition in noisy and reverberant
    environments, and LSTMs are better in clean conditions. The reason for this could
    be CNNs'' emphasis on local correlations as opposed to global ones. In this context,
    we will use CNNs for the implementation of use cases, as voices used for light
    control and speech used for door access are short and non-continuous. In addition,
    their environments can be noisy and reflective.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）和 LSTM 因其能够在演变过程中结合输入的时间特征，广泛应用于连续和/或自然语言处理。相反，CNN 更适合处理短小且非连续的音频信号，因为其具有平移不变性，比如发现结构模式的能力，不受位置的影响。此外，CNN
    在噪声和回响环境中的语音识别表现最佳，而 LSTM 在干净条件下表现更好。这是因为 CNN 强调局部相关性，而非全局相关性。在这种背景下，我们将使用 CNN
    来实现用例，因为用于灯光控制的语音和用于门禁的语音都很短且非连续。此外，它们的环境可能会有噪声和回响。'
- en: CNNs and transfer learning for speech recognition in IoT applications
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN 和转移学习在物联网应用中的语音识别
- en: A CNN is a very widely used DL algorithm for image recognition. Recently, this
    has become popular in audio/speech/speaker recognition, as these signals can be
    converted into images. A CNN has different implementations, including two versions
    of Mobilenets, and Incentive V3\. An overview of Mobilenets and Incentive V3 are
    presented in [Chapter 3](b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml), *Image Recognition
    in IoT*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 是一种广泛应用的深度学习（DL）算法，主要用于图像识别。近年来，这种算法在音频/语音/说话人识别中变得越来越流行，因为这些信号可以转换为图像。CNN
    有多种实现方式，包括两个版本的 Mobilenets 和 Incentive V3。Mobilenets 和 Incentive V3 的概述可以参见 [第
    3 章](b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml)，*物联网中的图像识别*。
- en: Collecting data
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集
- en: Data collection for ASR is a challenging task for many reasons, including privacy.
    Consequently, open source datasets are limited in number. Importantly, these datasets
    may not be easy to access, may have insufficient data/speakers, or may be noisy.
    In this context, we decided to use two different datasets for the two use cases.
    For the voice-driven controlled smart light, we are using Google’s speech command
    datasets, and for use case two, we can scrap data from one of three popular open
    data sources, LibriVox, LibriSpeech ASR, corpus, voxceleb, and YouTube.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 自动语音识别（ASR）数据收集由于多种原因具有挑战性，包括隐私问题。因此，开源数据集的数量有限。更重要的是，这些数据集可能难以访问，可能缺乏足够的数据/说话人，或者可能包含噪声。在这种情况下，我们决定为两个用例使用两个不同的数据集。对于语音驱动的智能灯控制，我们使用
    Google 的语音命令数据集；对于第二个用例，我们可以从三个流行的开放数据源中抓取数据，分别是 LibriVox、LibriSpeech ASR 语料库、VoxCeleb
    和 YouTube。
- en: 'Google''s speech command dataset includes 65,000 one-second long utterances
    of 30 short words, contributed to by thousands of different members of the public
    through the AIY website. The dataset offers basic audio data on common words such
    as `On`, `Off`, `Yes`, digits, and directions, but this can be useful in testing
    the first use case. For example, the `switch on the light` command can be represented
    by `On` while `switch off the light` can be represented by `Off` data in the dataset.
    Similarly, data gathered on an individual''s speech through scrapping can represent
    the occupants of a home. The second use case will consider a typical home with
    three to five occupants. These occupants will be the white list for the home and
    will be granted access if they are identified. Any people other than the listed
    ones will not be granted automated access to the home. We tested CNN on Google’s
    speech commands dataset and a smaller version of it. The following screenshots
    show a hierarchical view of the smaller dataset used for use case one:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Google 的语音命令数据集包含 65,000 条一秒钟长的 30 个简短单词的发音，由数千名来自公众的不同成员通过 AIY 网站贡献。该数据集提供了常见单词的基础音频数据，如
    `On`、`Off`、`Yes`、数字和方向，这些数据对于测试第一个用例非常有用。例如，`switch on the light` 命令可以通过数据集中的
    `On` 来表示，而 `switch off the light` 命令则可以通过 `Off` 数据来表示。类似地，通过抓取个人语音获得的数据可以代表家庭的住户。第二个用例将考虑一个典型的家庭，通常有三到五个住户。这些住户将是家庭的白名单，若被识别出来，将被授予访问权限。任何非白名单中的人员将无法自动进入家庭。我们在
    Google 的语音命令数据集及其较小版本上测试了 CNN。以下截图展示了用于第一个用例的较小数据集的层次结构视图：
- en: '![](img/5af7802d-765a-4872-b10b-a641843da7b0.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5af7802d-765a-4872-b10b-a641843da7b0.png)'
- en: 'For use case two, we scrapped data from LibriVox and also downloaded audio
    files from the LibriSpeech ASR corpus. We wrote a web scrapper using BeautifulSoup
    and Selenium for the scrapping. You can write a similar scrapper using other Python
    modules or even other languages, such as Node.js, C, C++, and PHP. The scrapper
    will parse the LibriVox website or any other given link and download the listed
    audio books/files we want. In the following code, we briefly present the scrapper''s
    script, which consists of three main parts:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于用例二，我们从 LibriVox 抓取了数据，并且还下载了 LibriSpeech ASR 语音识别语料库的音频文件。我们使用 BeautifulSoup
    和 Selenium 编写了一个网页抓取器。你也可以使用其他 Python 模块或甚至其他语言（如 Node.js、C、C++ 和 PHP）编写类似的抓取器。该抓取器将解析
    LibriVox 网站或任何其他给定的链接，并下载我们需要的音频书籍/文件。在以下代码中，我们简要展示了抓取器的脚本，它包含三个主要部分：
- en: '**Part 1**: Import the necessary Python modules for audio file scrapping:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 1 部分**：导入所需的 Python 模块以进行音频文件抓取：'
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Part 2**: Prepare the links for the audio books to be downloaded. Please
    note that the links may include repeated readers, which will be cleaned up to
    produce a non-repeated reader/speaker/home occupants dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 2 部分**：准备要下载的音频书籍链接。请注意，这些链接可能包含重复的读者，将对其进行清理，以生成一个非重复的读者/发言人/家庭成员数据集：'
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Part 3**: Download the audio files from the listed books and form a dataset
    of non-repeatable readers/speakers:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**第 3 部分**：从列出的书籍下载音频文件，并形成一个非重复读者/发言人的数据集：'
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After downloading the desired number of reader's/speaker's audio files or `.mp3`
    files (such as five speakers or home occupants), we process the `.mp3` files and
    convert them into fixed-size five-second audio files (`.wav`). We can do this
    through a shell script using tools such as ffmpeg, sox, and mp3splt, or we can
    do it manually (if there are not many readers/occupants and files).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下载所需数量的读者/发言人的音频文件或`.mp3`文件（例如五个发言人或家庭成员）后，我们处理这些`.mp3`文件并将它们转换为固定大小的五秒钟音频文件（`.wav`）。我们可以通过使用像
    ffmpeg、sox 和 mp3splt 这样的工具，通过 shell 脚本来完成，也可以手动进行（如果读者/家庭成员和文件不多的话）。
- en: 'As the implementations are based on CNNs, we need to convert the WAV audio
    files into images. The process of converting audio files into images varies according
    to the input data format. We can use `convert_wav2spect.sh` (available in the [Chapter
    4](ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml), *Audio/Speech/Voice Recognition
    in IoT* code folder) to convert the WAV files into fixed-size (503 x 800) spectrogram
    color images:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于实现基于卷积神经网络（CNN），我们需要将 WAV 音频文件转换为图像。音频文件转换为图像的过程根据输入数据格式的不同而有所变化。我们可以使用`convert_wav2spect.sh`（可在[第
    4 章](ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml)，*音频/语音/物联网中的语音识别*代码文件夹中找到）将 WAV
    文件转换为固定大小（503 x 800）的频谱图颜色图像：
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Generally, sox, the tool in the preceding script, supports the `.png` format,
    and if we need to convert the images, we can do this through the batch renaming
    of the files from Windows or Command Prompt. The following screenshot shows a
    hierarchical view of the dataset used for use case 2:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，前面脚本中使用的工具 sox 支持`.png`格式，如果需要转换图像，可以通过 Windows 或命令提示符批量重命名文件来实现。以下截图展示了用于案例
    2 的数据集的层次视图：
- en: '![](img/c3b3cb1e-f0da-434b-a61a-65db268db505.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3b3cb1e-f0da-434b-a61a-65db268db505.png)'
- en: Exploring data
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: 'It is essential to explore a dataset before applying DL algorithms on the data.
    To explore, firstly, we can run the audio signal (`.wav`) to the image converter, `wav2image.py`
    (available in [Chapter 4](ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml), *Audio/Speech/Voice
    Recognition in IoT* code directory), to see how the spectrum image looks. This
    will produce images, as shown. The following screenshot shows converted images
    for an `on` command:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在对数据应用深度学习算法之前，探索数据集是非常重要的。要进行探索，首先，我们可以将音频信号（`.wav`）传递给图像转换器`wav2image.py`（可在[第
    4 章](ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml)，*音频/语音/物联网中的语音识别*代码目录中找到），以查看频谱图像的效果。这将生成图像，如下所示。以下截图展示了一个`on`命令的转换图像：
- en: '![](img/c3981384-dab4-49e0-ba8b-649b84db6b39.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3981384-dab4-49e0-ba8b-649b84db6b39.png)'
- en: 'The following screenshot shows converted images for an `off` command. As we
    can see from the screenshots, their color distributions are different, which will
    be exploited by the DL algorithms in order to recognize them:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了一个`off`命令的转换图像。从截图中可以看出，它们的颜色分布不同，这将被深度学习算法利用来进行识别：
- en: '![](img/1ba84f18-1ca5-4fb4-b862-2fd84139a358.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ba84f18-1ca5-4fb4-b862-2fd84139a358.png)'
- en: 'We can also carry out group-wise exploration of data, and for this we can run
    `image_explorer.py` on the dataset we want to explore, as shown:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以对数据进行分组探索，为此我们可以在想要探索的数据集上运行`image_explorer.py`，如下所示：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following screenshot presents a snapshot of the data exploration process
    of the spectrum image data in the speech commands dataset. Interestingly, the
    colors of the images are different than the individual images presented earlier.
    This could be because of the tools we used for them. For the group ones, we used
    the sox tool; while we used `ffmpegf` for the individual ones:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图展示了在语音命令数据集中探索谱图像数据的过程。有趣的是，这些图像的颜色与之前展示的单独图像不同。这可能是由于我们用于它们的工具不同。对于组合图像，我们使用了sox工具；而对于单独的图像，我们使用了`ffmpegf`：
- en: '![](img/bfcc2321-4038-4716-a4c9-5466e3f98088.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfcc2321-4038-4716-a4c9-5466e3f98088.png)'
- en: As shown in the preceding screenshot of data exploration, the differences between
    four different speech commands in spectrum images may not always be significant.
    This is a challenge in audio signal recognition.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在前面的数据探索截图中所示，谱图像中四种不同语音命令之间的差异并不总是显著。这是音频信号识别中的一个挑战。
- en: 'The following screenshot presents a snapshot of the data exploration process
    of the spectrum image data based on a speaker’s/occupant’s speech (5-second) dataset:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图展示了基于说话者/居住者语音（5秒）数据集的谱图像数据探索过程的快照：
- en: '![](img/b7842bb0-a9af-45ce-8275-ac00b08b795a.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7842bb0-a9af-45ce-8275-ac00b08b795a.png)'
- en: As shown in the preceding screenshot, each occupant’s short speech spectrum
    images present a pattern that will help to classify the occupants and grant access
    accordingly.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面的截图所示，每个居住者的短语音谱图像呈现出一种模式，这将有助于对居住者进行分类，并相应地授予访问权限。
- en: Data preprocessing
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: '**Data preprocessing** is an essential step for a DL pipeline. The speech commands
    dataset consists of 1-second `.wav` files for each short speech command, and these
    files only need to be converted into a spectrum image. However, the downloaded
    audio files for the second use case are not uniform in length; hence, they require
    two-step preprocessing:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据预处理** 是深度学习流水线中的一个关键步骤。语音命令数据集由每个短语音命令的1秒`.wav`文件组成，这些文件只需转换为谱图像。然而，第二使用案例的下载音频文件长度不统一，因此需要进行两步预处理：'
- en: '`.mp3` to uniform length (such as a 5-second length) WAV file conversion'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`.mp3`转换为统一长度（如5秒）的WAV文件
- en: '`.wav` file to spectrum image conversion.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.wav`文件转换为谱图像。'
- en: 'The preprocessing of the datasets is discussed in the data collection section.
    A few issues to be noted during the training image set preparation are as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的预处理在数据收集部分有所讨论。在训练图像集准备过程中需要注意以下几个问题：
- en: '**Data Size**: We need to collect at least a hundred images for each class
    in order to train a model that works well. The more we can gather, the better
    the accuracy of the trained model is likely to be. Each of the categories in the
    use case one dataset has more than 3,000 sample images. However, one-shot learning
    (learning with fewer samples) works well with fewer than 100 training samples.
    We also made sure that the images are a good representation of what our application
    will actually face in a real implementation.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据规模**：我们需要为每个类别收集至少一百张图像，以便训练一个效果良好的模型。我们收集的图像数量越多，训练模型的准确性就可能越高。在这个使用案例中的每个数据集类别中，有超过3,000张样本图像。然而，少样本学习（使用较少样本进行学习）在少于100个训练样本时也能发挥良好效果。我们还确保这些图像能够很好地代表我们应用程序在实际实施中可能面临的情况。'
- en: '**Data heterogeneity**: Data collected for training should be heterogeneous.
    For example, audio or speech signals about a speaker need to be taken in as wide
    a variety of situations as possible, at different conditions of their voice, and
    with different devices.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据异质性**：用于训练的数据应该是异质的。例如，关于说话者的音频或语音信号应该在尽可能多的情况下采集，包括他们的不同语音状态和使用不同设备的情况。'
- en: Models training
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练
- en: As mentioned earlier, we are using transfer learning for both use cases, which
    does not require training from scratch; retraining the models with a new dataset
    will sufficiently work in many cases. In addition, in [Chapter 3](b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml),
    *Image Recognition in IoT* , we found that Mobilenet V1 is a lightweight (low-memory
    footprint and lower training time) CNN architecture. Consequently, we are implementing
    both uses using the Mobilenet V1 network. Importantly, we will use TensorFlow's
    `retrain.py` module as it is specially designed for CNNs (such as Mobilenet V1)
    based transfer learning).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们对两个用例都采用迁移学习，无需从头开始训练；使用新数据集重新训练模型在很多情况下已经足够。另外，在[第3章](b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml)，*物联网中的图像识别*中，我们发现Mobilenet
    V1是一种轻量级（占用内存少且训练时间较短）的CNN架构。因此，我们在实现两个用例时都采用Mobilenet V1网络。重要的是，我们将使用TensorFlow的`retrain.py`模块，因为它专门为CNN（如Mobilenet
    V1）基于迁移学习设计。
- en: 'We need to understand the list of key arguments of `retrain.py` before retraining
    Mobilenet V1 on the datasets. For the retraining, if we type in our Terminal (in
    Linux or macOS) or Command Prompt (Windows) `python retrain.py -h`, we will see
    a window like the following screenshot with additional information (such as an
    overview of each argument):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新训练Mobilenet V1模型之前，我们需要了解`retrain.py`的关键参数列表。对于重新训练，如果我们在Linux或macOS的终端或Windows的命令提示符中键入`python
    retrain.py -h`，将会看到一个窗口，类似以下截屏，并包含额外的信息（如每个参数的概述）：
- en: '![](img/ac1fd4ce-9554-46c5-93dd-f976765ca767.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac1fd4ce-9554-46c5-93dd-f976765ca767.png)'
- en: As shown in the preceding screenshot, the compulsory argument is the `-–image
    directory`, and it needs to be a dataset directory in which we want to train or
    retrain the models. In the case of Mobilenet V1, we must explicitly mention the
    CNN architecture, such as `--architecture mobilenet_1.0_224`. For the rest of
    the arguments, including data split ratio among training, validation, and test,
    we used the default values. The default split of data is to put 80% of the images
    into the main training set, keep 10% aside to run frequently as validation during
    training, and the final 10% of the data is for testing the real-world performance
    of the classifier.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的截屏所示，强制参数是`-–image directory`，它需要是一个数据集目录，其中我们希望训练或重新训练模型。对于Mobilenet V1，我们必须明确提到CNN架构，例如`--architecture
    mobilenet_1.0_224`。对于其余的参数，包括在训练、验证和测试数据之间的数据分割比例，我们使用了默认值。数据的默认分割是将80%的图像放入主训练集中，保留10%作为验证集频繁用于训练，最后10%的数据用于测试分类器在现实世界中的性能。
- en: 'The following is the command for running the retraining model for the Mobilenet
    v1 model:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是运行Mobilnet V1模型重新训练的命令：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once we run the preceding commands, they will generate the retrain models (`retrained_graph.pb`)
    and labels text (`retrained_labels.txt`) in the given directory and the summary
    directory consists of training and validation summary information for the models.
    The summary information (`--summaries_dir argument with default value retrain_logs)`)
    can be used by TensorBoard to visualize different aspects of the models, including
    the networks and their performance graphs. If we type the following command in
    the Terminal or Command Prompt, it will run `tensorboard`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行上述命令，它们将在给定目录中生成重新训练模型（`retrained_graph.pb`）和标签文本（`retrained_labels.txt`），而摘要目录包含了模型的训练和验证摘要信息。摘要信息（`--summaries_dir`参数默认值为`retrain_logs`）可以被TensorBoard用来可视化模型的不同方面，包括网络和性能图表。如果我们在终端或命令提示符中输入以下命令，将会运行`tensorboard`：
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once TensorBoard is running, navigate your web browser to `localhost:6006` to
    view TensorBoard and view the network of the corresponding model. The following
    diagram presents the network of the Mobilnet V1 used:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦TensorBoard运行起来，请在浏览器中输入`localhost:6006`，可以查看TensorBoard并浏览相应模型的网络。以下图表展示了所使用的Mobilnet
    V1网络的结构。
- en: '![](img/9529247b-18f8-40bb-83c5-6ef78a80144d.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9529247b-18f8-40bb-83c5-6ef78a80144d.png)'
- en: Evaluating models
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'We can evaluate the models from three different aspects:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从三个不同的角度评估模型：
- en: Learning/(re)training time
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习/(重新)训练时间
- en: Storage requirement
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储需求
- en: Performance (accuracy)
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能（准确性）
- en: The Mobilnet V1's retraining and validation process using the `retrain.py` module
    took less than an hour on a desktop (Intel Xenon CPU E5-1650 v3@3.5GHz and 32
    GB RAM) with GPU support.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`retrain.py`模块进行的Mobilnet V1的重新训练和验证过程，在台式机（Intel Xenon CPU E5-1650 v3@3.5GHz
    和 32 GB RAM）上，配备GPU支持，仅需不到一小时。
- en: 'The storage/memory requirement of a model is an essential consideration for
    resource-constrained IoT devices. To evaluate the storage/memory footprint of
    the Mobilenet V1, we compared its storage requirement to another two similar networks''
    (the Incentive V3 and CIFAR-10 CNN) storage requirements. The following screenshot
    presents the storage requirements for the three models. As shown, Mobilenet V1
    requires only 17.1 MB, less than one-fifth of the Incentive V3 (87.5 MB) and CIFAR-10
    CNN (91.1 MB). In terms of storage requirements, Mobilenet V1 is a better choice
    for many resource-constrained IoT devices, including the Raspberry Pi and smartphones:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的存储/内存需求是资源受限的物联网设备中的一个重要考虑因素。为了评估Mobilenet V1的存储/内存占用，我们将其存储需求与另外两个类似网络（Incentive
    V3和CIFAR-10 CNN）的存储需求进行了比较。以下截图展示了三个模型的存储需求。如图所示，Mobilenet V1仅需要17.1 MB，少于Incentive
    V3（87.5 MB）和CIFAR-10 CNN（91.1 MB）的五分之一。在存储需求方面，Mobilenet V1是许多资源受限物联网设备（如Raspberry
    Pi和智能手机）的更好选择：
- en: '![](img/df8c7fd0-c88c-4399-9848-540d8e283d4c.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df8c7fd0-c88c-4399-9848-540d8e283d4c.png)'
- en: 'Finally, we have evaluated the performance of the models. Two levels of performance
    evaluation have been carried out for the use cases:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们对模型的性能进行了评估。两个层次的性能评估已经在不同的用例中完成：
- en: Dataset-wide evaluation or testing has been done during the retraining phase
    on the desktop PC platform/server side
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集级别的评估或测试在桌面PC平台/服务器端的重训练阶段完成
- en: Individual audio and a group of home occupants, samples were tested or evaluated
    in the Raspberry Pi 3 environment. All the evaluation performances are presented
    in the following figures.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 个别音频和一组家庭成员样本在Raspberry Pi 3环境中进行测试或评估。所有的评估结果展示在以下图表中。
- en: Model performance (use case 1)
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能（用例 1）
- en: 'The following screenshots present the evaluation results of Mobilenet V1 on
    a speech command dataset (customized to only five commands, including `on`, `no`,
    `off`, `yes`, and `stop`). Note that `on` is considered to be *switch on the light*
    for use case one due to the lack of a real dataset:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了Mobilenet V1在语音命令数据集（自定义为仅包含五个命令，包括`on`，`no`，`off`，`yes`和`stop`）上的评估结果。请注意，由于缺乏真实数据集，在用例一中，`on`被认为是*打开灯光*：
- en: '![](img/95f50be3-73bc-4066-b89a-f6e152d10def.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95f50be3-73bc-4066-b89a-f6e152d10def.png)'
- en: 'The following screenshot was generated from the TensorBoard log files. The
    orange line represents the training and the blue one represents the validation
    accuracy of the Mobilenet V1 on the command dataset:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图来自TensorBoard日志文件。橙色线表示训练准确度，蓝色线表示Mobilenet V1在指令数据集上的验证准确度：
- en: '![](img/b9e9d8c2-8965-43e4-b3ee-f0fbce31e3b2.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9e9d8c2-8965-43e4-b3ee-f0fbce31e3b2.png)'
- en: As we can see from the preceding two screenshots, the performance of the Mobilenet
    V1 is not great, but it will be sufficient for detecting commands by adding more
    information to the commands, such as *switch on the main light* instead of only
    *on*. Furthermore, we can use a better audio file to image converter to improve
    the image quality and recognition accuracy.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的两张截图中可以看出，Mobilenet V1的性能并不算很好，但通过增加更多的指令信息，如*打开主灯*而不仅仅是*开*，就足以用于指令检测。此外，我们可以使用更好的音频文件转图像转换器来提高图像质量和识别准确性。
- en: Model performance (use case 2)
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型性能（用例 2）
- en: 'The following screenshots represents the evaluation results of Mobilenet V1
    on a `three occupants` dataset. As we can see, the performance of the dataset
    is reasonably good. It can successfully detect occupants more than 90% of the
    time:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了Mobilenet V1在`三名成员`数据集上的评估结果。可以看出，数据集的性能相当不错，能够成功检测到占用者的概率超过90%：
- en: '![](img/d5dbedef-8761-444e-9534-31fbcfefaf1d.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d5dbedef-8761-444e-9534-31fbcfefaf1d.png)'
- en: 'The following screenshot was generated from the TensorBoard log files. The
    orange line represents the training and the blue one represents the validation
    accuracy of the Mobilenet V1 on the `three occupants` dataset:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图来自TensorBoard日志文件。橙色线表示训练准确度，蓝色线表示Mobilenet V1在`三名成员`数据集上的验证准确度：
- en: '![](img/bbed2591-7daf-4640-9ed3-0653a6c3d26e.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbed2591-7daf-4640-9ed3-0653a6c3d26e.png)'
- en: We also tested the Mobilenet V1 on a `five occupants` dataset, and this consistently
    showed accuracy in the range of 85-94%. Finally, we can export the trained model
    detail (such as `retrained_mobilenet_graph.pb` and `retrained_labels.txt`) to
    an IoT device, including a smartphone or Raspberry Pi, and we can test the model
    on new data from both use cases using the provided `label_image.py` code or something
    similar.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在一个`五名居住者`数据集上测试了Mobilenet V1，并且它在85-94%的准确率范围内稳定表现。最后，我们可以将训练好的模型详细信息（如`retrained_mobilenet_graph.pb`和`retrained_labels.txt`）导出到物联网设备，包括智能手机或树莓派，然后使用提供的`label_image.py`代码或类似的工具，测试模型在新数据上的表现。
- en: Summary
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Automatic audio/speech/voice recognition is becoming a popular means for people
    to interact with their devices, including smartphones, wearables, and other smart
    devices. Machine learning and DL algorithms are essential in audio/speech/voice-based
    decision making.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 自动音频/语音/声音识别正成为人们与设备交互的流行方式，包括智能手机、可穿戴设备及其他智能设备。机器学习和深度学习算法在基于音频/语音/声音的决策中至关重要。
- en: In the first part of this chapter, we briefly described different IoT applications
    and their audio/speech/voice detection-based decision making. We also briefly
    discussed two potential use cases of IoT where DL algorithms can be useful in
    speech/command-based decision making. The first use case considered an IoT application
    to make a home smart using voice-controlled lighting. The second use case also
    made a home or office smart, where a DL-based IoT solution offered automated access
    control to the smart home or office. In the second part of the chapter, we briefly
    discussed the data collection process for the use cases, and discussed the rationale
    behind selecting a CNN, especially the Mobilenet V1\. The rest of the sections
    of the chapter describe all the necessary components of the DL pipeline for these
    models and their results.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分，我们简要描述了不同的物联网应用及其基于音频/语音/声音检测的决策过程。我们还简要讨论了物联网中深度学习算法在基于语音/命令的决策中的潜在应用。第一个用例考虑了一个通过语音控制照明来使家庭智能化的物联网应用。第二个用例同样使家庭或办公室智能化，其中基于深度学习的物联网解决方案提供了自动化的智能家居或办公室访问控制。在本章的第二部分，我们简要讨论了这些用例的数据收集过程，并讨论了选择CNN，特别是Mobilenet
    V1的理由。章中的其余部分描述了这些模型的深度学习管道的所有必要组件及其结果。
- en: Many IoT devices and/or users are mobile. Localization of the devices and users
    is essential for offering them services when they are on the move. GPS can support
    outdoor localization, but it does not work in indoor environments. Consequently,
    alternative technologies are necessary for indoor localization. Different indoor
    technologies, including WiFi-fingerprinting, are available, and generally they
    work based on a device's communication signal analysis. In the next chapter ([Chapter
    5](3426d6f2-8913-4585-b04b-f0b3a8bd235d.xhtml), *Indoor Localization in IoT*),
    we will discuss and demonstrate how DL models can be used for indoor localization
    in IoT applications.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 许多物联网设备和/或用户是移动的。在他们移动时，为其提供服务的前提是设备和用户的定位。GPS可以支持户外定位，但在室内环境中无法使用。因此，室内定位需要其他替代技术。目前有多种室内定位技术，包括WiFi指纹识别，它们通常基于设备通信信号的分析。在下一章（[第5章](3426d6f2-8913-4585-b04b-f0b3a8bd235d.xhtml)，*物联网中的室内定位*）中，我们将讨论并演示如何使用深度学习模型在物联网应用中进行室内定位。
- en: References
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Assistive technology: [http://www.who.int/en/news-room/fact-sheets/detail/assistive-technology](http://www.who.int/en/news-room/fact-sheets/detail/assistive-technology)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '辅助技术: [http://www.who.int/en/news-room/fact-sheets/detail/assistive-technology](http://www.who.int/en/news-room/fact-sheets/detail/assistive-technology)'
- en: '*Smart and Robust Speaker Recognition for Context-Aware In-Vehicle Applications*,
    I Bisio, C Garibotto, A Grattarola, F Lavagetto, and A Sciarrone, in IEEE Transactions
    on Vehicular Technology, vol. 67, no. 9, pp. 8,808-8,821, September, 2018.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于上下文感知车载应用的智能且强健的说话人识别*，I Bisio，C Garibotto，A Grattarola，F Lavagetto，A Sciarrone，发表于《IEEE
    车辆技术学报》，第67卷，第9期，第8808-8821页，2018年9月。'
- en: '*Emotion-Aware Connected Healthcare Big Data Towards 5G*, M S Hossain and G
    Muhammad, in IEEE Internet of Things Journal, vol. 5, no. 4, pp. 2,399-2,406,
    August, 2018.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*面向5G的情感感知连接健康大数据*，M S Hossain 和 G Muhammad，发表于《IEEE 物联网学报》，第5卷，第4期，第2399-2406页，2018年8月。'
- en: '*Machine Learning Paradigms for Speech Recognition*, L Deng, X Li (2013). IEEE
    Transactions on Audio, Speech, and Language Processing, vol. 2, # 5.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语音识别的机器学习范式*，L Deng，X Li（2013）。《IEEE 音频、语音与语言处理学报》，第2卷，第5期。'
- en: '*On Comparison of Deep Learning Architectures for Distant Speech Recognition*,
    R Sustika, A R Yuliani, E Zaenudin, and H F Pardede, *2017 Second International
    Conferences on Information Technology, Information Systems and Electrical Engineering
    (ICITISEE)*, Yogyakarta, 2017, pp. 17-21.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于深度学习架构在远距离语音识别中的比较*，R Sustika，A R Yuliani，E Zaenudin 和 H F Pardede，*2017第二届信息技术、信息系统与电气工程国际会议（ICITISEE）*，日惹，2017年，第17-21页。'
- en: '*Deep Neural Networks for Acoustic Modeling in Speech Recognition*, G Hinton,
    L Deng, D Yu, G E Dahl, A R Mohamed, N Jaitly, A Senior, V Vanhoucke, P Nguyen,
    T N Sainath, and B Kingsbury, IEEE Signal Processing Magazine, vol. 29, # 6, pp.
    82–97, 2012.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于语音识别的声学建模的深度神经网络*，G Hinton，L Deng，D Yu，G E Dahl，A R Mohamed，N Jaitly，A Senior，V
    Vanhoucke，P Nguyen，T N Sainath 和 B Kingsbury，《IEEE信号处理杂志》，卷29，第6期，第82–97页，2012年。'
- en: '*Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale
    Acoustic Modeling*, H Sak, A Senior, and F Beaufays, in Fifteenth Annual Conference
    of the International Speech Communication Association, 2014.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*长短期记忆递归神经网络架构用于大规模声学建模*，H Sak，A Senior 和 F Beaufays，发表于国际语音通信协会第十五届年会，2014年。'
- en: '*Phoneme recognition using time delay neural network*, IEEE Transaction on
    Acoustics, Speech, and Signal Processing, G. H. K. S. K. J. L. Alexander Waibel,
    Toshiyuki Hanazawa, vol. 37, # 3, 1989.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用时延神经网络的音素识别*，《IEEE声学、语音与信号处理学报》，G. H. K. S. K. J. L. Alexander Waibel，Toshiyuki
    Hanazawa，卷37，第3期，1989年。'
- en: '*A Time Delay Neural Network Architecture for Efficient Modeling of Long Temporal
    Contexts*, V Peddinti, D Povey, and S Khudanpur, in Proceedings of Interspeech.
    ISCA, 2005.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于高效建模长时间上下文的时延神经网络架构*，V Peddinti，D Povey 和 S Khudanpur，发表于《Interspeech 论文集》，ISCA，2005年。'
- en: '*Deep Convolutional Neural Network for lvcsr*, B. K. B. R. Tara N Sainath and
    Abdel Rahman Mohamed, in International Conference on Acoustics, Speech and Signal
    Processing. IEEE, 2013, pp. 8614–8618.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于大词汇连续语音识别的深度卷积神经网络*，B. K. B. R. Tara N Sainath 和 Abdel Rahman Mohamed，发表于国际声学、语音与信号处理会议。IEEE，2013年，第8614–8618页。'
- en: '*Mel Frequency Cepstral Coefficients for Music Modeling*, Logan, Beth and others,
    ISMIR,vol. 270, 2000.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于音乐建模的梅尔频率倒谱系数*，Logan，Beth 等，ISMIR，卷270，2000年。'
- en: '*Launching the Speech Commands Dataset*, Pete Warden: [https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*发布语音命令数据集*，Pete Warden: [https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html)。'
