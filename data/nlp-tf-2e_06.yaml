- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Recurrent Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: '**Recurrent Neural Networks** (**RNNs**) are a special family of neural networks
    that are designed to cope with sequential data (that is, time-series data), such
    as stock market prices or a sequence of texts (for example, variable-length sentences).
    RNNs maintain a state variable that captures the various patterns present in sequential
    data; therefore, they are able to model sequential data. In comparison, conventional
    feed-forward neural networks do not have this ability unless the data is represented
    with a feature representation that captures the important patterns present in
    the sequence. However, coming up with such feature representations is extremely
    difficult. Another alternative for feed-forward models to model sequential data
    is to have a separate set of parameters for each position in time/sequence so
    that the set of parameters assigned to a certain position learns about the patterns
    that occur at that position. This will greatly increase the memory requirement
    for your model.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**循环神经网络** (**RNNs**) 是一类特殊的神经网络，旨在处理序列数据（即时间序列数据），如股票市场价格或文本序列（例如，可变长度的句子）。RNN
    维持一个状态变量，用于捕捉序列数据中存在的各种模式；因此，它们能够建模序列数据。相比之下，传统的前馈神经网络没有这种能力，除非数据被表示为捕捉序列中重要模式的特征表示。然而，提出这样的特征表示是非常困难的。前馈模型用于建模序列数据的另一个替代方案是为每个时间/序列位置设置一组独立的参数，以便为特定位置分配的参数可以学习该位置发生的模式。这会大大增加模型的内存需求。'
- en: However, as opposed to having a separate set of parameters for each position
    like feed-forward networks, RNNs share the same set of parameters over time. Sharing
    parameters over time is an important part of RNNs and in fact is one of the main
    enablers for learning temporal patterns. Then the state variable is updated over
    time for each input we observe in the sequence. These parameters shared over time,
    combined with the state vector, are able to predict the next value of a sequence,
    given the previously observed values of the sequence. Furthermore, since we process
    a single element of a sequence at a time (for example, one word in a document
    at a time), RNNs can process data of arbitrary lengths without padding data with
    special tokens.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与前馈网络为每个位置拥有一组独立参数不同，RNN 在时间上共享相同的参数。时间上的参数共享是 RNN 的一个重要部分，实际上是学习时间序列模式的主要推动力之一。然后，状态变量会随着我们在序列中观察到的每个输入而随时间更新。随着时间共享的这些参数，结合状态向量，能够根据序列中先前观察到的值预测序列的下一个值。此外，由于我们每次只处理序列中的一个元素（例如，每次处理文档中的一个单词），RNN
    可以处理任意长度的数据，而无需使用特殊标记对数据进行填充。
- en: In this chapter, we will dive into the details of RNNs. First, we will discuss
    how an RNN can be formed by starting with a simple feed-forward model.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将深入探讨 RNN 的细节。首先，我们将讨论如何通过从一个简单的前馈模型开始来形成一个 RNN。
- en: 'After this we will discuss the basic functionality of an RNN. We will also
    delve into the underlying equations, such as output calculation and parameter
    update rules of RNNs, and discuss several variants of applications of RNNs: one-to-one,
    one-to-many, and many-to-many RNNs. We will walk through an example of using RNNs
    to identify named entities (e.g. person names, organization, etc.), which has
    valuable downstream use cases like building knowledge bases. We will discuss a
    more complex RNN model that can read text both forward and backward, and uses
    convolutional layers to increase the model accuracy. This chapter will cover this
    through the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，我们将讨论 RNN 的基本功能。我们还将深入探讨 RNN 的基础方程式，例如输出计算和参数更新规则，并讨论几种 RNN 的应用变体：一对一、一对多和多对多的
    RNN。我们将通过一个例子，展示如何使用 RNN 来识别命名实体（例如人名、组织名等），这对于构建知识库等下游应用具有重要价值。我们还将讨论一个更复杂的 RNN
    模型，该模型能够同时正向和反向读取文本，并使用卷积层提高模型的准确性。本章将通过以下几个主要主题进行讲解：
- en: Understanding RNNs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 RNN
- en: Backpropagation Through Time
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过时间的反向传播
- en: Applications of RNNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 的应用
- en: Named Entity Recognition (NER) with RNNs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 RNN 进行命名实体识别（NER）
- en: NER with character and token embeddings
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用字符和标记嵌入进行命名实体识别（NER）
- en: Understanding RNNs
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 RNN
- en: In this section, we will discuss what an RNN is by starting with a gentle introduction,
    and then move on to more in-depth technical details. We mentioned earlier that
    RNNs maintain a state variable that evolves over time as the RNN sees more data,
    thus giving it the power to model sequential data. In particular, this state variable
    is updated over time by a set of recurrent connections. The existence of recurrent
    connections is the main structural difference between an RNN and a feed-forward
    network. The recurrent connections can be understood as links between a series
    of memories that the RNN learned in the past, connecting to the current state
    variable of the RNN. In other words, the recurrent connections update the current
    state variable with respect to the past memory the RNN has, enabling the RNN to
    make a prediction based on the current input as well as the previous inputs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过温和的介绍来讨论 RNN 的定义，然后深入探讨更具体的技术细节。我们之前提到过，RNN 通过维护一个随着时间推移而变化的状态变量来处理更多的数据，从而使其具备建模顺序数据的能力。特别是，这个状态变量通过一组循环连接在时间上不断更新。循环连接的存在是
    RNN 和前馈网络之间的主要结构性差异。循环连接可以理解为 RNN 在过去学到的一系列记忆之间的联系，这些记忆与 RNN 当前的状态变量相连接。换句话说，循环连接根据
    RNN 所拥有的过去记忆来更新当前的状态变量，使得 RNN 能够基于当前输入以及之前的输入进行预测。
- en: The term RNN is sometimes used to refer to the family of recurrent models, which
    has many different models. In other words, it is sometimes used as a generalization
    of a specific RNN variant. Here, we are using the term RNN to refer to one of
    the earliest implementations of an RNN model known as the Elman network.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 RNN 有时用来指代循环模型家族，它包含许多不同的模型。换句话说，有时它被用作某个特定 RNN 变体的泛化。在这里，我们使用 RNN 这个术语来指代一种最早实现的
    RNN 模型，称为 Elman 网络。
- en: In the upcoming section, we will discuss the following topics. First, we will
    discuss how we can start by representing a feed-forward network as a computational
    graph.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将讨论以下主题。首先，我们将讨论如何通过将前馈网络表示为计算图来开始。
- en: Then we will see through an example why a feed-forward network might fail at
    a sequential task. Then we will adapt that feed-forward graph to model sequential
    data, which will give us the basic computational graph of an RNN. We will also
    discuss the technical details (for example, update rules) of an RNN. Finally,
    we will discuss the details of how we can train RNN models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将通过一个例子来说明前馈网络为什么可能在顺序任务中失败。接着，我们将调整该前馈图来建模顺序数据，这将给我们一个 RNN 的基本计算图。我们还将讨论
    RNN 的技术细节（例如，更新规则）。最后，我们将讨论如何训练 RNN 模型的具体细节。
- en: The problem with feed-forward neural networks
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前馈神经网络的问题
- en: 'To understand the limits of feed-forward neural networks and how RNNs address
    them, let’s imagine a sequence of data:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解前馈神经网络的局限性以及 RNN 如何解决这些问题，让我们想象一个数据序列：
- en: '![](img/B14070_06_001.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_001.png)'
- en: 'Next, let’s assume that, in the real world, *x* and *y* are linked in the following
    relationship:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，假设在现实世界中，*x* 和 *y* 之间存在以下关系：
- en: '![](img/B14070_06_002.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_002.png)'
- en: '![](img/B14070_06_003.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_003.png)'
- en: Here, *g*[1] and *g*[2] are transformations (e.g. multiplying with a weight
    matrix followed by a non-linear transformation). This means that the current output
    *y*[t] depends on the current state *h*[t,] where *h*[t] is calculated with the
    current input *x*[t] and previous state *h*[t-1]. The state encodes information
    about previous inputs observed historically by the model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*g*[1] 和 *g*[2] 是转换（例如，乘以权重矩阵后进行非线性转换）。这意味着当前输出 *y*[t] 依赖于当前状态 *h*[t]，其中
    *h*[t] 是通过当前输入 *x*[t] 和前一个状态 *h*[t-1] 计算得出的。这个状态编码了模型历史上观察到的关于先前输入的信息。
- en: 'Now, let’s imagine a simple feed-forward neural network, which we will represent
    with the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们想象一个简单的前馈神经网络，我们将通过以下方式表示它：
- en: '![](img/B14070_06_004.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_004.png)'
- en: Here, *y*[t] is the predicted output for some input *x*[t].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y*[t] 是某个输入 *x*[t] 的预测输出。
- en: If we use a feed-forward neural network to solve this task, the network will
    have to produce ![](img/B14070_06_005.png) one at a time, by taking ![](img/B14070_06_006.png)
    as inputs, one at a time. Now, let’s consider the problem we face in this solution
    for a time-series problem.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用前馈神经网络来解决这个任务，网络将不得不一次处理一个 ![](img/B14070_06_005.png)，每次将 ![](img/B14070_06_006.png)
    作为输入。现在，让我们考虑一下这种解决方案在时间序列问题中可能面临的问题。
- en: The predicted output *y*[t] at time *t* of a feed-forward neural network depends
    only on the current input *x*[t]. In other words, it does not have any knowledge
    about the inputs that led to *x*[t] (that is, ![](img/B14070_06_007.png)). For
    this reason, a feed-forward neural network will fail at a task where the current
    output not only depends on the current input but also on the previous inputs.
    Let’s understand this through an example.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一个前馈神经网络在时间 *t* 预测的输出 *y*[t] 仅依赖于当前输入 *x*[t]。换句话说，它并不知道导致 *x*[t] 的输入（即 ![](img/B14070_06_007.png)）。因此，前馈神经网络无法完成这样一个任务：当前的输出不仅依赖于当前输入，还依赖于先前的输入。让我们通过一个例子来理解这一点。
- en: 'Say we need to train a neural network to fill in missing words. We have the
    following phrase, and we would like to predict the next word:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要训练一个神经网络来填补缺失的单词。我们有如下短语，并希望预测下一个单词：
- en: '*James has a cat and it likes to drink ____.*'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*詹姆斯有一只猫，它喜欢喝____。*'
- en: If we are to process one word at a time and use a feed-forward neural network,
    we will only have the input *drink* and this is not enough at all to understand
    the phrase or even to understand the context (the word *drink* can appear in many
    different contexts). One can argue that we can achieve good results by processing
    the full sentence in a single go. Even though this is true, such an approach has
    limitations such as processing very long sentences. However, there is a new family
    of models known as Transformers that are processing the full sequences of data
    with fully-connected layers, and have been surpassing the performance of sequential
    models. We will have a separate chapter on these models later.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们每次处理一个单词并使用前馈神经网络，那么我们只会得到输入 *drink*，这远远不足以理解这个短语，甚至无法理解上下文（单词 *drink* 可以出现在许多不同的语境中）。有些人可能会认为，通过一次性处理完整句子，我们可以得到较好的结果。尽管这是对的，但这种方法也有局限性，比如在处理非常长的句子时。然而，现在有一种新的模型家族，称为Transformer，它们使用完全连接的层来处理完整的数据序列，并且在性能上超越了顺序模型。我们稍后会单独讲解这些模型。
- en: Modeling with RNNs
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用RNN建模
- en: 'On the other hand, we can use an RNN to find a solution to this problem. We
    will start with the data we have:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们可以使用RNN来解决这个问题。我们将从已有的数据开始：
- en: '![](img/B14070_06_008.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_008.png)'
- en: 'Assume that we have the following relationship:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下关系：
- en: '![](img/B14070_06_009.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_009.png)'
- en: '![](img/B14070_06_010.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_010.png)'
- en: 'Now, let’s replace *g*[1] with a function approximator ![](img/B14070_06_011.png)
    parametrized by ![](img/B14070_06_012.png) that takes the current input *x*[t]
    and the previous state of the system *h*[t-1] as the input and produces the current
    state *h*[t]. Then, we will replace *g*[2] with ![](img/B14070_06_013.png), which
    takes the current state of the system *h*[t] to produce *y*[t]. This gives us
    the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将 *g*[1] 替换为一个函数逼近器 ![](img/B14070_06_011.png)，该函数由 ![](img/B14070_06_012.png)
    参数化，它接受当前输入 *x*[t] 和系统的先前状态 *h*[t-1] 作为输入，并生成当前状态 *h*[t]。然后，我们将 *g*[2] 替换为 ![](img/B14070_06_013.png)，它接受系统的当前状态
    *h*[t] 并生成 *y*[t]。这就给出了如下结果：
- en: '![](img/B14070_06_014.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_014.png)'
- en: '![](img/B14070_06_015.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_015.png)'
- en: 'We can think of ![](img/B14070_06_016.png) as an approximation of the true
    model that generates *x* and *y*. To understand this more clearly, let’s now expand
    the equation as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 ![](img/B14070_06_016.png) 看作是生成 *x* 和 *y* 的真实模型的近似。为了更清楚地理解这一点，让我们将方程展开如下：
- en: '![](img/B14070_06_017.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_017.png)'
- en: 'For example, we can represent *y*[4] as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将 *y*[4] 表示为如下形式：
- en: '![](img/B14070_06_018.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_018.png)'
- en: 'Also, by expansion we get the following (omitting ![](img/B14070_06_012.png)
    and ![](img/B14070_06_020.png) for clarity):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，通过展开，我们得到以下结果（为了清晰起见，省略了 ![](img/B14070_06_012.png) 和 ![](img/B14070_06_020.png)）：
- en: '![](img/B14070_06_021.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_021.png)'
- en: 'This can be illustrated in a graph, as shown in *Figure 6.1*:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过图形来表示，如 *图6.1* 所示：
- en: '![Modeling with Recurrent Neural Networks](img/B14070_06_01.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![使用递归神经网络建模](img/B14070_06_01.png)'
- en: 'Figure 6.1: The relationship between x[t] and y[t] expanded'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：x[t] 和 y[t] 的关系展开
- en: 'We can generally summarize the diagram, for any given time step *t*, as shown
    in *Figure 6.2*:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将该图进行概括，对于任何给定的时间步 *t*，如 *图6.2* 所示：
- en: '![Modeling with Recurrent Neural Networks](img/B14070_06_02.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![使用递归神经网络建模](img/B14070_06_02.png)'
- en: 'Figure 6.2: A single-step calculation of an RNN structure'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：RNN结构的单步计算
- en: However, it should be understood that *h*[t-1] in fact is what *h*[t] was before
    receiving *x*[t]. In other words, *h*[t-1] is *h*[t] before one time step.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要理解的是，*h*[t-1]实际上是在接收到*x*[t]之前的*h*[t]。换句话说，*h*[t-1]是*h*[t]在一个时间步之前的值。
- en: 'Therefore, we can represent the calculation of *h*[t] with a recurrent connection,
    as shown in *Figure 6.3*:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用循环连接表示*h*[t]的计算，如*图6.3*所示：
- en: '![Modeling with Recurrent Neural Networks](img/B14070_06_03.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![使用循环神经网络建模](img/B14070_06_03.png)'
- en: 'Figure 6.3: A single-step calculation of an RNN with the recurrent connection'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：带有循环连接的RNN单步计算
- en: The ability to summarize a chain of equations mapping ![](img/B14070_06_022.png)
    to ![](img/B14070_06_023.png) as in *Figure 6.3* allows us to write any *y*[t]
    in terms of *x*[t], *h*[t-1], and *h*[t]. This is the key idea behind an RNN.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 将一系列方程映射从![](img/B14070_06_022.png)到![](img/B14070_06_023.png)，如*图6.3*所示，这使我们能够将任何*y*[t]表示为*x*[t]、*h*[t-1]和*h*[t]的函数。这是RNN的核心思想。
- en: Technical description of an RNN
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环神经网络的技术描述
- en: 'Let’s now have an even closer look at what makes an RNN and define the mathematical
    equations for the calculations taking place within an RNN. Let’s start with the
    two functions we derived as function approximators for learning *y*[t] from *x*[t]:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们更深入地了解RNN的构成，并定义在RNN内部发生的计算的数学方程式。我们从我们推导出的两个函数开始，作为学习从*x*[t]到*y*[t]的函数逼近器：
- en: '![](img/B14070_06_024.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_024.png)'
- en: '![](img/B14070_06_025.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_025.png)'
- en: 'As we have seen, a neural network is composed of a set of weights and biases
    and some nonlinear activation function. Therefore, we can write the preceding
    relation as shown here:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，神经网络由一组权重、偏置和一些非线性激活函数组成。因此，我们可以将之前的关系写成如下形式：
- en: '![](img/B14070_06_026.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_026.png)'
- en: 'Here, tanh is the tanh activation function, and *U* is a weight matrix of size
    ![](img/B14070_06_027.png), where *m* is the number of hidden units and *d* is
    the dimensionality of the input. Also, *W* is a weight matrix of size ![](img/B14070_06_028.png)
    that creates the recurrent link from *h*[t-1] to *h*[t]. The *y*[t] relation is
    given by the following equation:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，tanh是tanh激活函数，*U*是大小为![](img/B14070_06_027.png)的权重矩阵，其中*m*是隐藏单元的数量，*d*是输入的维度。此外，*W*是大小为![](img/B14070_06_028.png)的权重矩阵，用于从*h*[t-1]到*h*[t]创建循环连接。*y*[t]的关系由以下方程给出：
- en: '![](img/B14070_06_029.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_029.png)'
- en: 'Here, *V* is a weight matrix of size ![](img/B14070_06_030.png) and *c* is
    the dimensionality of the output (this can be the number of output classes). In
    *Figure 6.4*, we illustrate how these weights form an RNN. The arrows represent
    the direction that the data flows in the network:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*V*是大小为![](img/B14070_06_030.png)的权重矩阵，*c*是输出的维度（这可以是输出类别的数量）。在*图6.4*中，我们展示了这些权重如何形成一个RNN。箭头表示数据在网络中的流动方向：
- en: '![Technical description of a Recurrent Neural Network](img/B14070_06_04.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![循环神经网络的技术描述](img/B14070_06_04.png)'
- en: 'Figure 6.4: The structure of an RNN'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：RNN的结构
- en: So far, we have seen how we can represent an RNN with a graph of computational
    nodes, with edges denoting computations. Also, we looked at the actual mathematics
    behind an RNN. Let’s now look at how to optimize (or train) the weights of an
    RNN to learn from sequential data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到如何用计算节点的图表示RNN，其中边表示计算过程。此外，我们还探讨了RNN背后的实际数学原理。现在让我们来看一下如何优化（或训练）RNN的权重，以便从序列数据中学习。
- en: Backpropagation Through Time
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间反向传播
- en: For training RNNs, a special form of **backpropagation**, known as **Backpropagation
    Through Time** (**BPTT**), is used. To understand BPTT, however, first we need
    to understand how **BP** works. Then we will discuss why BP cannot be directly
    applied to RNNs, but how BP can be adapted for RNNs, resulting in BPTT. Finally,
    we will discuss two major problems present in BPTT.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练RNN时，使用一种特殊形式的**反向传播**，称为**时间反向传播**（**BPTT**）。然而，要理解BPTT，首先我们需要了解**BP**是如何工作的。然后我们将讨论为什么BP不能直接应用于RNN，但如何将BP适应RNN，从而得出BPTT。最后，我们将讨论BPTT中存在的两个主要问题。
- en: How backpropagation works
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播是如何工作的
- en: 'Backpropagation is the technique that is used to train a feed-forward neural
    network. In backpropagation, you do the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是用来训练前馈神经网络的技术。在反向传播过程中，你会执行以下操作：
- en: Calculate a prediction for a given input
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算给定输入的预测
- en: Calculate an error, *E*, of the prediction by comparing it to the actual label
    of the input (for example, mean squared error and cross-entropy loss)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将预测与输入的实际标签进行比较，计算预测的误差 *E*（例如，均方误差和交叉熵损失）
- en: Update the weights of the feed-forward network to minimize the loss calculated
    in *step 2*, by taking a small step in the opposite direction of the gradient
    ![](img/B14070_06_031.png) for all *w*[ij], where *w*[ij] is the *j*^(th) weight
    of the *i*^(th) layer
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在梯度的相反方向上迈出小步，更新前馈网络的权重，以最小化在*步骤 2*中计算的损失，针对所有 *w*[ij]，其中 *w*[ij] 是 *i*^(th)
    层的 *j*^(th) 权重
- en: 'To understand the above computations more clearly, consider the feed-forward
    network depicted in *Figure 6.5*. This has two single weights, *w*[1] and *w*[2],
    and calculates two outputs, *h* and *y*, as shown in the following figure. We
    assume no nonlinearities in the model for simplicity:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地理解上述计算，考虑 *图 6.5* 中描绘的前馈网络。该网络有两个单一的权重 *w*[1] 和 *w*[2]，并计算两个输出 *h* 和 *y*，如下面的图所示。为简化模型，我们假设没有非线性：
- en: '![How backpropagation works](img/B14070_06_05.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播如何工作](img/B14070_06_05.png)'
- en: 'Figure 6.5: Computations of a feed-forward network'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5：前馈网络的计算
- en: 'We can calculate ![](img/B14070_06_032.png) using the chain rule as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用链式法则如下计算 ![](img/B14070_06_032.png)：
- en: '![](img/B14070_06_033.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_033.png)'
- en: 'This simplifies to the following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这简化为以下内容：
- en: '![](img/B14070_06_034.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_034.png)'
- en: Here, *l* is the correct label for the data point *x*. Also, we are assuming
    the mean squared error as the loss function. Everything here is defined, and it
    is quite straightforward to calculate ![](img/B14070_06_035.png).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*l* 是数据点 *x* 的正确标签。此外，我们假设均方误差作为损失函数。这里的一切都是定义明确的，计算 ![](img/B14070_06_035.png)
    也相当直接。
- en: Why we cannot use BP directly for RNNs
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们不能直接对 RNN 使用 BP
- en: 'Now, let’s try the same for the RNN in *Figure 6.6*. Now we have an additional
    recurrent weight *w*[3]. We have omitted the time components of inputs and outputs
    for the clarity of the problem we are trying to emphasize:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对 *图 6.6* 中的 RNN 做同样的尝试。现在我们有了一个额外的递归权重 *w*[3]。为了清晰地突出我们要强调的问题，我们省略了输入和输出的时间成分：
- en: '![Why we cannot use BP directly for RNNs](img/B14070_06_06.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![为什么我们不能直接对 RNN 使用 BP](img/B14070_06_06.png)'
- en: 'Figure 6.6: Computations of an RNN'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6：RNN 的计算
- en: 'Let’s see what happens if we apply the chain rule to calculate ![](img/B14070_06_036.png):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如果我们应用链式法则来计算 ![](img/B14070_06_036.png) 会发生什么：
- en: '![](img/B14070_06_037.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_037.png)'
- en: 'This becomes the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这变为以下内容：
- en: '![](img/B14070_06_038.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_038.png)'
- en: The term ![](img/B14070_06_039.png) here creates problems because it is a recursive
    term. You end up with an infinite number of derivative terms, as *h* is recursive
    (that is, calculating *h* includes *h* itself) and *h* is not a constant and dependent
    on *w*[3]. This is solved by unrolling the input sequence *x* over time, creating
    a copy of the RNN for each input *x*[t] and calculating derivatives for each copy
    separately, and collapsing those updates into one, by summing up the gradients,
    to calculate the weight update. We will discuss the details of this process next.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的项 ![](img/B14070_06_039.png) 会产生问题，因为它是一个递归项。你最终会得到无限多个导数项，因为 *h* 是递归的（也就是说，计算
    *h* 包含了 *h* 本身），并且 *h* 不是常量，而是依赖于 *w*[3]。这通过随着时间展开输入序列 *x* 来解决，为每个输入 *x*[t] 创建一个
    RNN 的副本，分别计算每个副本的导数，然后通过将这些更新求和来合并这些更新，进而计算权重更新。我们将在接下来讨论这个过程的细节。
- en: Backpropagation Through Time – training RNNs
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播通过时间——训练 RNN
- en: 'The trick to calculating backpropagation for RNNs is to consider not a single
    input, but the full input sequence. Then, if we calculate ![](img/B14070_06_040.png)
    at time step 4, we will get the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 RNN 的反向传播技巧是考虑完整的输入序列，而不仅仅是单个输入。然后，如果我们在时间步 4 计算 ![](img/B14070_06_040.png)，我们将得到以下结果：
- en: '![](img/B14070_06_041.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_041.png)'
- en: This means that we need to calculate the sum of gradients for all the time steps
    up to the fourth time step. In other words, we will first unroll the sequence
    so that we can calculate ![](img/B14070_06_042.png) and ![](img/B14070_06_043.png)
    for each time step *j*. This is done by creating four copies of the RNN. So, to
    calculate ![](img/B14070_06_044.png), we need *t-j+1* copies of the RNN. Then
    we will roll up the copies to a single RNN by summing up gradients with respect
    to all previous time steps to get the gradient, and update the RNN with the gradient
    ![](img/B14070_06_045.png).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们需要计算所有时间步长（直到第四个时间步）的梯度和。换句话说，我们将首先展开序列，以便我们可以为每个时间步 *j* 计算 ![](img/B14070_06_042.png)
    和 ![](img/B14070_06_043.png)。这是通过创建四个 RNN 的副本来完成的。因此，要计算 ![](img/B14070_06_044.png)，我们需要
    *t-j+1* 个 RNN 副本。然后，我们将通过将所有前一个时间步的梯度相加，滚动这些副本为一个单一的 RNN，得到梯度，并使用梯度 ![](img/B14070_06_045.png)
    来更新 RNN。
- en: However, this becomes costly as the number of time steps increases. For more
    computational efficiency, we can use **Truncated Backpropagation Through Time**
    (**TBPTT**) to optimize recurrent models, which is an approximation of BPTT.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着时间步数的增加，这变得非常昂贵。为了更高效的计算，我们可以使用 **截断反向传播通过时间**（**TBPTT**）来优化递归模型，这是 BPTT
    的一种近似方法。
- en: Truncated BPTT – training RNNs efficiently
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 截断 BPTT——高效训练 RNN
- en: 'In TBPTT, we only calculate the gradients for a fixed number of *T* time steps
    (in contrast to calculating it up to the very beginning of the sequence as in
    BPTT). More specifically, when calculating ![](img/B14070_06_040.png), for time
    step *t*, we only calculate derivatives down to *t-T* (that is, we do not compute
    derivatives up to the very beginning):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TBPTT 中，我们只计算固定数量 *T* 时间步的梯度（与 BPTT 中计算到序列开始不同）。更具体地说，在计算 ![](img/B14070_06_040.png)
    时，对于时间步 *t*，我们只计算到 *t-T* 的导数（也就是说，我们不计算到序列的开始）：
- en: '![](img/B14070_06_047.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_047.png)'
- en: This is much more computationally efficient than standard BPTT. In standard
    BPTT, for each time step *t*, we calculate derivatives up to the very beginning
    of the sequence. But this gets computationally infeasible as the sequence length
    becomes larger and larger (for example, this could occur when processing a long
    text document word by word). However, in truncated BPTT, we only calculate the
    derivatives for a fixed number of steps backward, and as you can imagine, the
    computational cost does not change as the sequence becomes larger.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这比标准的 BPTT 在计算上更为高效。在标准 BPTT 中，对于每个时间步 *t*，我们需要计算从序列开始到当前时间步的导数。但随着序列长度越来越大，这变得计算上不可行（例如，当逐字处理一篇长文本时）。然而，在截断
    BPTT 中，我们只计算固定步数的反向导数，正如你可以想象的那样，随着序列变长，计算成本不会发生变化。
- en: Limitations of BPTT – vanishing and exploding gradients
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BPTT 的局限性——消失梯度和爆炸梯度
- en: Having a way to calculate gradients for recurrent weights and having a computationally
    efficient approximation such as TBPTT does not enable us to train RNNs without
    trouble. Something else can go wrong with the calculations.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有了计算递归权重梯度的方法，并且拥有像 TBPTT 这样的计算高效近似方法，我们仍然无法毫无问题地训练 RNN。计算中可能还会出现其他问题。
- en: 'To see why, let’s expand a single term in ![](img/B14070_06_048.png), which
    is as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么如此，让我们展开 ![](img/B14070_06_048.png) 中的一个单项，公式如下：
- en: '![](img/B14070_06_049.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_049.png)'
- en: 'Since we know that the issues of backpropagation arise from the recurrent connections,
    let’s ignore the *w*[1]*x* terms and consider the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道反向传播的问题来自于递归连接，因此让我们忽略 *w*[1]*x* 项，考虑以下内容：
- en: '![](img/B14070_06_050.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_050.png)'
- en: 'By simply expanding *h*[3] and doing simple arithmetic operations we can show
    this:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单地展开 *h*[3] 并进行简单的算术运算，我们可以证明这一点：
- en: '![](img/B14070_06_051.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_051.png)'
- en: We see that for just four time steps we have a term ![](img/B14070_06_052.png).
    So at the *n*^(th) time step, it would become ![](img/B14070_06_053.png). Say
    we initialized *w*[3] to be very small (say 0.00001) at *n*=*100* time step; the
    gradient would be infinitesimally small (of scale 10^(-500)). Also, since computers
    have limited precision in representing a number, this update would be ignored
    (that is, arithmetic underflow). This is called the **vanishing gradient**.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，对于仅四个时间步，我们有一个项 ![](img/B14070_06_052.png)。因此，在 *n*^(th) 时间步，它将变成 ![](img/B14070_06_053.png)。假设我们在
    *n*=*100* 时间步将 *w*[3] 初始化为非常小的值（例如 0.00001）；那么梯度将变得极其微小（约为 10^(-500) 的量级）。此外，由于计算机在表示数字时精度有限，这个更新将被忽略（即，算术下溢）。这被称为
    **消失梯度**。
- en: Solving the vanishing gradient is not very straightforward. There are no easy
    ways of rescaling the gradients so that they will properly propagate through time.
    A few techniques used in practice to solve the problem of vanishing gradients
    are to use careful initialization of weights (for example, the Xavier initialization),
    or to use momentum-based optimization methods (that is, in addition to the current
    gradient update, we add an additional term, which is the accumulation of all the
    past gradients known as the **velocity term**). However, more principled approaches
    to solving the vanishing gradient problem, such as different structural modifications
    to the standard RNN, have been introduced, as we will see in *Chapter 7, Understanding
    Long* *Short-Term Memory Networks*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 解决梯度消失问题并不是非常直接。没有简单的方法可以重新缩放梯度，以便它们能够正确地在时间上传播。实践中解决梯度消失问题的一些技术包括：仔细初始化权重（例如，Xavier
    初始化），或使用基于动量的优化方法（即，除了当前的梯度更新外，我们还会添加一个额外的项，这是所有过去梯度的累积，称为**速度项**）。然而，解决梯度消失问题的更有原则的方法，例如对标准
    RNN 的不同结构修改，已经被提出，正如我们将在*第7章，理解长短期记忆网络*中看到的那样。
- en: On the other hand, say that we initialized *w*[3] to be very large (say 1000.00).
    Then at the *n*=*100* time step, the gradients would be massive (of scale 10^(300)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，假设我们将 *w*[3] 初始化为非常大（比如 1000.00）。那么在 *n*=*100* 的时间步长下，梯度将变得巨大（规模为 10^(300)）。
- en: This leads to numerical instabilities and you will get values such as `Inf`
    or `NaN` (that is, not a number) in Python. This is called the **exploding gradient**.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致数值不稳定，您将在 Python 中得到类似 `Inf` 或 `NaN`（即非数字）的值。这被称为**梯度爆炸**。
- en: Gradient explosion can also take place due to the complexity of the loss surface
    of a problem. Complex nonconvex loss surfaces are very common in deep neural networks
    due to both the dimensionality of inputs as well as the large number of parameters
    (weights) present in the models.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度爆炸也可能由于问题损失面（loss surface）的复杂性而发生。由于输入的维度和模型中存在大量参数（权重），复杂的非凸损失面在深度神经网络中非常常见。
- en: '*Figure* *6.7* illustrates the loss surface of an RNN and highlights the presence
    of walls with very high curvature. If the optimization method comes in contact
    with such a wall, then the gradients will explode or overshoot, as shown by the
    solid line in the image. This can either lead to very poor loss minimization,
    numerical instabilities, or both. A simple solution to avoid gradient explosion
    in such situations is to clip the gradients to a reasonably small value when it
    is larger than some threshold. The dashed line in the figure shows what happens
    when we clip the gradient at some small value. (Gradient clipping is covered in
    the paper *On the difficulty of training recurrent neural networks*, *Pascanu*,
    *Mikolov*, *and* *Bengio*, *International Conference on Machine Learning (2013):
    1310-1318*.)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.7* 展示了 RNN 的损失面，并突出了具有非常高曲率的墙壁。如果优化方法接触到这样的墙壁，梯度将会爆炸或超调，如图中实线所示。这可能导致非常差的损失最小化、数值不稳定，或者两者都有。避免在这种情况下梯度爆炸的一个简单方法是将梯度裁剪到一个合理的小值，当其大于某个阈值时。图中的虚线显示了当我们在某个小值处裁剪梯度时会发生什么。（梯度裁剪在论文《训练循环神经网络的难题》中有介绍，*Pascanu*,
    *Mikolov*, *and* *Bengio*, *国际机器学习大会 (2013): 1310-1318*。）'
- en: '![Limitations of BPTT – vanishing and exploding gradients](img/B14070_06_07.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![BPTT的限制 - 梯度消失与梯度爆炸](img/B14070_06_07.png)'
- en: 'Figure 6.7: The gradient explosion phenomenon. Source: This figure is from
    the paper ‘On the difficulty of training recurrent neural networks’ by Pascanu,
    Mikolov, and Bengio'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：梯度爆炸现象。来源：此图来自Pascanu、Mikolov和Bengio的论文《训练循环神经网络的难题》。
- en: Here we conclude our discussion about BPTT, which adapts backpropagation for
    RNNs. Next we will discuss various ways that RNNs can be used to solve applications.
    These applications include sentence classification, image captioning, and machine
    translation. We will categorize the RNNs into several different categories such
    as one-to-one, one-to-many, many-to-one, and many-to-many.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们结束了关于 BPTT 的讨论，BPTT 是为 RNN 适配的反向传播算法。接下来，我们将讨论 RNN 如何用于解决各种应用。这些应用包括句子分类、图像描述和机器翻译。我们将把
    RNN 分类为不同的类别，如一对一、一对多、多对一和多对多。
- en: Applications of RNNs
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RNN 的应用
- en: So far, we have only talked about one-to-one-mapped RNNs, where the current
    output depends on the current input as well as the previously observed history
    of inputs. This means that there exists an output for the sequence of previously
    observed inputs and the current input. However, in the real word, there can be
    situations where there is only one output for a sequence of inputs, a sequence
    of outputs for a single input, and a sequence of outputs for a sequence of inputs
    where the sequence sizes are different. In this section, we will look at several
    different settings of RNN models and the applications they would be used in.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了一对一映射的 RNN，其中当前输出依赖于当前输入以及先前观察到的输入历史。这意味着，对于先前观察到的输入序列和当前输入，存在一个输出。然而，在实际应用中，可能会出现只有一个输出对应于输入序列、一个输出对应于单一输入、以及一个输出序列对应于输入序列但序列大小不同的情况。在本节中，我们将讨论几种不同的
    RNN 模型设置及其应用。
- en: One-to-one RNNs
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一对一 RNN
- en: 'In one-to-one RNNs, the current input depends on the previously observed inputs
    (see *Figure 6.8*). Such RNNs are appropriate for problems where each input has
    an output, but the output depends both on the current input and the history of
    inputs that led to the current input. An example of such a task is stock market
    prediction, where we output a value for the current input, and this output also
    depends on how the previous inputs have behaved. Another example would be scene
    classification, where each pixel in an image is labeled (for example, labels such
    as car, road, and person). Sometimes *x*[t+1] can be the same as *y*[t] for some
    problems. For example, in text generation problems, the previously predicted word
    becomes an input to predict the next word. The following figure depicts a one-to-one
    RNN:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在一对一 RNN 中，当前输入依赖于先前观察到的输入（见 *图 6.8*）。这种 RNN 适用于每个输入都有输出的问题，但输出既依赖于当前输入，也依赖于导致当前输入的输入历史。一个这样的任务示例是股市预测，其中我们为当前输入输出一个值，而这个输出还依赖于之前输入的表现。另一个例子是场景分类，其中图像中的每个像素都有标签（例如，标签如车、道路和人）。有时，*x*[t+1]
    可能与 *y*[t] 相同，这对于某些问题是成立的。例如，在文本生成问题中，之前预测的单词成为预测下一个单词的输入。下图展示了一对一 RNN：
- en: '![Diagram  Description automatically generated](img/B14070_06_08.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图示 描述自动生成](img/B14070_06_08.png)'
- en: 'Figure 6.8: One-to-one RNNs having temporal dependencies'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8：具有时间依赖关系的一对一 RNN
- en: One-to-many RNNs
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一对多 RNN
- en: A one-to-many RNN would take a single input and output a sequence (see *Figure
    6.9*). Here, we assume the inputs to be independent of each other.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一对多 RNN 将接受单一输入并输出一个序列（见 *图 6.9*）。在这里，我们假设输入之间是相互独立的。
- en: 'That is, we do not need information about previous inputs to make a prediction
    about the current input. However, the recurrent connections are needed because,
    although we process a single input, the output is a sequence of values that depends
    on the previous output values. An example task where such an RNN would be used
    is an image captioning task. For example, for a given input image, the text caption
    can consist of five or ten words. In other words, the RNN will keep predicting
    words until it outputs a meaningful phrase describing the image. The following
    figure depicts a one-to-many RNN:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们不需要关于先前输入的信息就能对当前输入进行预测。然而，循环连接是必要的，因为尽管我们处理的是单一输入，但输出是一个依赖于先前输出值的值序列。一个可以使用这种
    RNN 的示例任务是图像字幕生成任务。例如，对于给定的输入图像，文本字幕可能由五个或十个单词组成。换句话说，RNN 会不断预测单词，直到输出一个描述图像的有意义短语。下图展示了一对多
    RNN：
- en: '![One-to-many RNNs](img/B14070_06_09.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![一对多 RNN](img/B14070_06_09.png)'
- en: 'Figure 6.9: A one-to-many RNN'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9：一对多 RNN
- en: Many-to-one RNNs
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多对一 RNN
- en: 'Many-to-one RNNs take an input of arbitrary length and produce a single output
    for the sequence of inputs (see *Figure 6.10*). Sentence classification is one
    such task that can benefit from a many-to-one RNN. A sentence is represented to
    the model as a sequence of words of arbitrary length. The model takes it as the
    input and produces an output, classifying the sentence into one of a set of predefined
    classes. Some specific examples of sentence classification are as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 多对一 RNN 接受任意长度的输入，并为输入序列产生一个单一的输出（见 *图 6.10*）。句子分类就是一个可以从多对一 RNN 中受益的任务。句子被模型表示为任意长度的单词序列。模型将其作为输入，并产生一个输出，将句子分类为预定义类中的一种。以下是句子分类的一些具体示例：
- en: Classifying movie reviews as positive or negative statements (that is, sentiment
    analysis)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将电影评论分类为正面或负面（即情感分析）
- en: Classifying a sentence depending on what the sentence describes (for example,
    person, object, or location)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据句子的描述对其进行分类（例如，人物、物体或位置）
- en: Another application of many-to-one RNNs is classifying large-scale images by
    processing only a patch of images at a time and moving the window over the whole
    image.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 多对一 RNN 的另一个应用是通过一次处理图像的一个补丁并将窗口在整个图像上移动，来对大规模图像进行分类。
- en: 'The following figure depicts a many-to-one RNN:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个多对一 RNN：
- en: '![Many-to-one RNNs](img/B14070_06_10.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![多对一 RNN](img/B14070_06_10.png)'
- en: 'Figure 6.10: A many-to-one RNN'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10：一个多对一 RNN
- en: Many-to-many RNNs
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多对多 RNN
- en: 'Many-to-many RNNs (or Sequences-to-Sequence, seq2seq for short) often produce
    arbitrary-length outputs from arbitrary-length inputs (see *Figure 6.11*). In
    other words, inputs and outputs do not have to be of the same length. This is
    particularly useful in machine translation, where we translate a sentence from
    one language to another. As you can imagine, one sentence in a certain language
    does not always align with a sentence from another language. Another such example
    is chatbots, where the chatbot reads a sequence of words (that is, a user request)
    and outputs a sequence of words (that is, the answer). The following figure depicts
    a many-to-many RNN:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 多对多 RNN（或称序列到序列，简写为 seq2seq）通常会从任意长度的输入中生成任意长度的输出（见 *图 6.11*）。换句话说，输入和输出不需要是相同的长度。这在机器翻译中尤其有用，因为我们将一个语言的句子翻译成另一种语言。如你所想，一个语言中的一个句子不一定与另一个语言中的句子对齐。另一个例子是聊天机器人，其中聊天机器人读取一串单词（即用户请求），并输出一串单词（即回答）。以下图示展示了一个多对多
    RNN：
- en: '![Many-to-many RNNs](img/B14070_06_11.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![多对多 RNN](img/B14070_06_11.png)'
- en: 'Figure 6.11: A many-to-many RNN'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11：一个多对多 RNN
- en: 'We can summarize the different types of applications of feed-forward networks
    and RNNs as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以总结前馈网络和 RNN 的不同应用类型如下：
- en: '| **Algorithm** | **Description** | **Applications** |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| **算法** | **描述** | **应用** |'
- en: '| One-to-one RNNs | These take a single input and give a single output. Current
    input depends on the previously observed input(s). | Stock market prediction,
    scene classification, and text generation |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 一对一 RNN | 这些网络接受单一输入并生成单一输出。当前输入依赖于之前观察到的输入。 | 股票市场预测，场景分类和文本生成 |'
- en: '| One-to-many RNNs | These take a single input and give an output consisting
    of an arbitrary number of elements | Image captioning |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 一对多 RNN | 这些网络接受单一输入，并生成一个包含任意数量元素的输出 | 图像描述 |'
- en: '| Many-to-one RNNs | These take a sequence of inputs and give a single output.
    | Sentence classification (considering a single word as a single input) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 多对一 RNN | 这些网络接受一个输入序列，并生成单一输出。 | 句子分类（将单一单词视为单一输入） |'
- en: '| Many-to-many RNNs | These take a sequence of arbitrary length as inputs and
    output a sequence of arbitrary length. | Machine translation, chatbots |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 多对多 RNN | 这些网络接受任意长度的序列作为输入，并输出任意长度的序列。 | 机器翻译，聊天机器人 |'
- en: Next, we will learn how to use RNNs to identify various entities mentioned in
    a text corpus.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何使用 RNN 来识别文本语料库中提到的各种实体。
- en: Named Entity Recognition with RNNs
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 RNN 进行命名实体识别
- en: 'Now let’s look at our first task: using an RNN to identify named entities in
    a text corpus. This task is known as **Named Entity Recognition** (**NER**). We
    will be using a modified version of the well-known **CoNLL 2003** (which stands
    for **Conference on Computational Natural Language Learning - 2003**) dataset
    for NER.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下我们的第一个任务：使用 RNN 识别文本语料库中的命名实体。这个任务被称为 **命名实体识别**（**NER**）。我们将使用经过修改的著名
    **CoNLL 2003**（即 **计算自然语言学习会议 - 2003**）数据集来进行命名实体识别。
- en: CoNLL 2003 is available for multiple languages, and the English data was generated
    from a Reuters Corpus that contains news stories published between August 1996
    and August 1997\. The database we’ll be using is found at [https://github.com/ZihanWangKi/CrossWeigh](https://github.com/ZihanWangKi/CrossWeigh)
    and is called **CoNLLPP**. It is a more closely curated version than the original
    CoNLL, which contains errors in the dataset induced by incorrectly understanding
    the context of a word. For example, in the phrase *“Chicago won …”* Chicago was
    identified as a location, whereas it is in fact an organization. This exercise
    is available in `ch06_rnns_for_named_entity_recognition.ipynb` in the `Ch06-Recurrent-Neural-Networks`
    folder.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: CoNLL 2003 数据集支持多种语言，英文数据来源于路透社语料库，该语料库包含了 1996 年 8 月到 1997 年 8 月之间发布的新闻报道。我们将使用的数据库位于
    [https://github.com/ZihanWangKi/CrossWeigh](https://github.com/ZihanWangKi/CrossWeigh)，名为
    **CoNLLPP**。与原始的 CoNLL 数据集相比，它是一个经过更加精细筛选的版本，避免了由于错误理解单词上下文而引起的数据集错误。例如，在短语 *“Chicago
    won …”* 中，Chicago 被识别为一个地点，而实际上它是一个组织。这个练习可以在 `Ch06-Recurrent-Neural-Networks`
    文件夹下的 `ch06_rnns_for_named_entity_recognition.ipynb` 中找到。
- en: Understanding the data
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据
- en: 'We have defined a function called `download_data()`, which can be used to download
    the data. We will not go into the details of it as it simply downloads several
    files and places them in a data folder. Once the download finishes, you’ll have
    three files:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个名为 `download_data()` 的函数，可以用来下载数据。我们不会深入探讨它的细节，因为它只是下载几个文件并将它们放入一个数据文件夹。一旦下载完成，您将拥有三个文件：
- en: '`data\conllpp_train.txt` – Training set, contains 14041 sentences'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data\conllpp_train.txt` – 训练集，包含 14041 个句子'
- en: '`data\conllpp_dev.txt` – Validation set, contains 3250 sentences'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data\conllpp_dev.txt` – 验证集，包含 3250 个句子'
- en: '`data\conllpp_test.txt` – Test set, contains 3452 sentences'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data\conllpp_test.txt` – 测试集，包含 3452 个句子'
- en: 'Next up, we will read the data and convert it into a specific format that suits
    our model. But before that, we need to see what our data looks like originally:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将读取数据并将其转换为适合我们模型的特定格式。但在此之前，我们需要看看原始数据的样子：
- en: '[PRE0]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you can see, the document has a single word in each line along with the
    associated tags of that word. These tags are in the following order:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，文档中每行包含一个单词，并带有该单词的相关标签。这些标签的顺序如下：
- en: The Part-of-speech (POS) tag (e.g. noun - `NN`, verb - `VB`, determinant - `DT`,
    etc.)
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词性标签（POS标签）（例如，名词 - `NN`，动词 - `VB`，限定词 - `DT` 等）
- en: Chunk tag – A chunk is a segment of text made of one or more tokens (for example,
    `NP` represents a noun phrase such as “The European Commission”)
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 短语块标签 – 短语块是由一个或多个标记组成的文本段落（例如，`NP` 代表名词短语，如 “The European Commission”）
- en: Named entity tag (e.g. Location, Organization, Person, etc.)
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 命名实体标签（例如，位置、组织、人物等）
- en: Both chunk tags and named entity tags have a `B-` and `I-` prefix (e.g. `B-ORG`
    or `I-ORG`). These prefixes are there to differentiate the starting token of an
    entity/chunk from the continuing token of an entity/chunk.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是短语块标签还是命名实体标签，都有 `B-` 和 `I-` 前缀（例如，`B-ORG` 或 `I-ORG`）。这些前缀用于区分实体/短语块的起始标记与后续标记。
- en: 'There are also five types of entities in the dataset:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中还有五种类型的实体：
- en: Location-based entities (`LOC`)
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于位置的实体（`LOC`）
- en: Person-based entities (`PER`)
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于人物的实体（`PER`）
- en: Organization-based entities (`ORG`)
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于组织的实体（`ORG`）
- en: Miscellaneous entities (`MISC`)
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杂项实体（`MISC`）
- en: Non-entities (`O`)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非实体（`O`）
- en: Finally, there’s an empty line between separate sentences.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，每个句子之间有一个空行。
- en: 'Now let’s look at the code that loads the data we downloaded into memory, so
    that we can start using it:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下加载我们下载的数据到内存中的代码，这样我们就可以开始使用它了：
- en: '[PRE1]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we will store all the sentences (as a list of strings in `sentences`)
    and all the labels associated with each token in the sentences (as a list of lists
    in `ner_labels`). We will read the file line by line. We will maintain a Boolean
    called `is_sos` that indicates whether we are at the start of a sentence. We will
    also have two temporary lists (`sentence_tokens` and `sentence_labels`) that will
    accumulate the tokens and the NER labels of the current sentence. When we are
    at the start of a sentence, we reset these temporary lists. Otherwise, we keep
    writing each token and NER label we see in the file to these temporary lists.
    We can now run this function on the train, validation, and test corpora we have:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将存储所有句子（作为`sentences`中的字符串列表）和与每个标记相关的所有标签（作为`ner_labels`中的列表列表）。我们将逐行读取文件。我们会维护一个布尔值`is_sos`，用来表示我们是否在句子的开头。我们还会有两个临时列表（`sentence_tokens`和`sentence_labels`），用来累积当前句子的标记和NER标签。当我们处于句子的开始时，我们会重置这些临时列表。否则，我们会将每个在文件中看到的标记和NER标签写入这些临时列表。现在，我们可以在训练集、验证集和测试集上运行这个函数：
- en: '[PRE2]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will print a few samples and see what we have with:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将打印几个样本，看看我们得到了什么：
- en: '[PRE3]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This produces:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了：
- en: '[PRE4]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'One of the unique characteristics of NER tasks is the class imbalance. That
    is, not all classes will have a roughly equal number of samples. As you can probably
    guess, in a corpus, there are more non-named entities than named entities. This
    leads to a significant class imbalance among labels. Therefore, let’s have a look
    at the distribution of samples among different classes:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: NER 任务的一个独特特点是类别不平衡。也就是说，并非所有类别的样本数量大致相等。正如你可能猜到的，在语料库中，非命名实体的数量要多于命名实体。这导致标签之间出现显著的类别不平衡。因此，让我们来看看不同类别之间样本的分布：
- en: '[PRE5]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To analyze the data, we will first convert the NER labels into a pandas `Series`
    object. This can be done by simply calling the `pd.Series()` construct on `train_labels`,
    `valid_labels`, and `test_labels`. But remember that these were lists of lists,
    where each inner list represents the NER tags for all the tokens in a sentence.
    To create a flat list, we can use the `chain()` function from the built-in Python
    library `itertools`. It will chain several lists together to form a single list.
    After that, we call the `value_counts()` function on that pandas `Series`. This
    will return a new list, where the indices are unique labels found in the original
    `Series` and the values are the counts of occurrences of each label. This gives
    us:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析数据，我们将首先把 NER 标签转换为 pandas 的`Series`对象。可以通过简单地在`train_labels`、`valid_labels`和`test_labels`上调用`pd.Series()`构造函数来完成。但请记住，这些是列表的列表，其中每个内部列表代表句子中所有标记的
    NER 标签。为了创建一个扁平化的列表，我们可以使用内置的 Python 库`itertools`中的`chain()`函数。它会将多个列表连接在一起，形成一个单一的列表。之后，我们在这个
    pandas `Series`上调用`value_counts()`函数。这将返回一个新列表，其中索引是原始`Series`中找到的唯一标签，而值是每个标签出现的次数。这样我们就得到了：
- en: '[PRE6]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, O labels are several magnitudes higher than the volume of other
    labels. We need to keep this in mind when training the model. Subsequently, we
    will analyze the sequence length (i.e. number of tokens) of each sentence. We
    need this information later to pad our sentences to a fixed length.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，O 标签的数量远远超过其他标签的数量。在训练模型时，我们需要记住这一点。接下来，我们将分析每个句子的序列长度（即标记的数量）。我们稍后需要这些信息来将句子填充到固定长度。
- en: '[PRE7]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we create a pandas `Series`, where each item has the length of a sentence
    after splitting each sentence into a list of tokens.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建一个 pandas `Series`，其中每个项目都是在将每个句子拆分为标记列表后，句子的长度。
- en: 'Then we will look at the 5% and 95% percentiles of those lengths. This produces:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将查看这些长度的 5% 和 95% 分位数。这将产生：
- en: '[PRE8]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We can see that 95% of our sentences have 37 tokens or less.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，95%的句子长度为 37 个标记或更少。
- en: Processing data
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理数据
- en: 'Now it’s time to process the data. We will keep the sentences in the same format,
    i.e. a list of strings where each string represents a sentence. This is because
    we will integrate text processing right into our model (as opposed to doing it
    externally). For labels, we have to do several changes. Remember labels are a
    list of lists, where the inner lists represent labels for all the tokens in each
    sentence. Specifically we will do the following:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候处理数据了。我们将保持句子的原始格式，即一个字符串列表，每个字符串代表一个句子。因为我们将把文本处理直接集成到模型中（而不是在外部进行处理）。对于标签，我们需要做一些改变。记住，标签是一个列表的列表，其中每个内部列表表示每个句子中所有标记的标签。具体来说，我们将执行以下操作：
- en: Convert the class labels to class IDs
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将类别标签转换为类别 ID
- en: Pad the sequences of labels to a specified maximum length
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标签序列填充至指定的最大长度
- en: Generate a mask that indicates the padded labels, so that we can use this information
    to disregard the padded labels during model training
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个掩码，指示填充标签，以便我们可以在模型训练过程中忽略填充的标签
- en: First let’s write a function to get a class label to class ID mapping. This
    function leverages pandas’ `unique()` function to get the unique labels in the
    training set and generate a mapping of integers to unique labels found.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们编写一个函数来获取类标签到类 ID 的映射。这个函数利用 pandas 的`unique()`函数获取训练集中的唯一标签，并生成一个整数到唯一标签的映射。
- en: '[PRE9]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If you run this with:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行以下代码：
- en: '[PRE10]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then you will get:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你将得到：
- en: '[PRE11]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We write a function called `get_padded_int_labels()` that will take sequences
    of class labels and return sequences of padded class IDs, with the option to return
    a mask indicating padded labels. This function takes the following arguments:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们编写了一个名为`get_padded_int_labels()`的函数，该函数接受类标签的序列并返回填充后的类 ID 序列，并可选择返回一个表示填充标签的掩码。该函数接受以下参数：
- en: '`labels` (`List[List[str]]`) – A list of lists of strings, where each string
    is a class label of the string type'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels` (`List[List[str]]`) – 一个字符串列表的列表，其中每个字符串是类标签'
- en: '`labels_map` (`Dict[str, int]`) – A dictionary mapping a string label to a
    class ID of type integer'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels_map` (`Dict[str, int]`) – 一个字典，将字符串标签映射到整数类型的类 ID'
- en: '`max_seq_length` (`int`) – A maximum length to be padded to (longer sequences
    will be truncated at this length)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_seq_length` (`int`) – 要填充的最大长度（较长的序列将在此长度处被截断）'
- en: '`return_mask` (`bool`) – Whether to return the mask showing padded labels or
    not'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_mask` (`bool`) – 是否返回显示填充标签的掩码'
- en: 'Let’s now look at the code that performs the aforementioned operations:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下执行上述操作的代码：
- en: '[PRE12]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can see the first step in the function converts all the string labels in
    `labels` to integer labels using the `labels_map`. Next we get the padded sequences
    with the `tf.keras.preprocessing.sequence.pad_sequences()` function. We discussed
    this function in detail in the previous chapter. Essentially, it will pad (with
    a specified value) and truncate arbitrary-length sequences, to return fixed-length
    sequences. We are instructing the function to do both padding and truncating at
    the end of sequences, and to pad with a special value of `-1`. Then we can simply
    generate the mask as a boolean filter where `padded_labels` is not equal to `-1`.
    Thus, the positions where original labels exist will have a value of `1` and the
    rest will have `0`. However, we have to convert the `-1` values to a class ID
    found in the `labels_map`. We will give them the class ID of the label `O` (i.e.
    others).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到函数的第一步将`labels`中的所有字符串标签通过`labels_map`转换为整数标签。接下来，我们使用`tf.keras.preprocessing.sequence.pad_sequences()`函数获得填充后的序列。我们在上一章中详细讨论了这个函数。本质上，它将对任意长度的序列进行填充（使用指定的值）和截断，返回固定长度的序列。我们指示该函数在序列的末尾进行填充和截断，填充值为特殊值`-1`。然后我们可以简单地生成一个布尔值掩码，其中`padded_labels`不等于`-1`。因此，原始标签所在的位置将标记为`1`，其余位置为`0`。但是，我们必须将`-1`的值转换为`labels_map`中找到的类
    ID。我们将其分配给标签`O`（即其他）。
- en: 'From our findings in the previous chapter, we will set the maximum sequence
    length to `40`. Remember that the 95% percentile fell at the length of 37 words:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们在上一章中的发现，我们将最大序列长度设置为`40`。记住，95%的分位数落在37个词的长度上：
- en: '[PRE13]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'And now we will generate processed labels and masks for all of the training,
    validation, and testing data:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将为所有训练、验证和测试数据生成处理后的标签和掩码：
- en: '[PRE14]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Finally, we will print the processed labels and masks of the first two sequences:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将打印前两个序列的处理后的标签和掩码：
- en: '[PRE15]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Which returns:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回：
- en: '[PRE16]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can see that the mask is indicating the true labels and padded ones clearly.
    Next, we will define some hyperparameters of the model.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到掩码清楚地指示了真实标签和填充标签。接下来，我们将定义模型的一些超参数。
- en: Defining hyperparameters
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义超参数
- en: 'Now let’s define several hyperparameters needed for our RNN, as shown here:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义我们 RNN 所需的几个超参数，如下所示：
- en: '`max_seq_length` – Denotes the maximum length for a sequence. We infer this
    from our training data during data exploration. It is important to have a reasonable
    length for sequences, as otherwise, memory can explode, due to the unrolling of
    the RNN.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_seq_length` – 表示序列的最大长度。我们在数据探索过程中从训练数据中推断出这一点。为序列设置合理的长度非常重要，否则，由于 RNN
    的展开，内存可能会爆炸。'
- en: '`emedding_size` – The dimensionality of token embeddings. Since we have a small
    corpus, a value < 100 will suffice.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emedding_size` – 词向量的维度。由于我们拥有一个小型语料库，值小于100即可。'
- en: '`rnn_hidden_size` – The dimensionality of hidden layers in the RNN. Increasing
    dimensionality of the hidden layer usually leads to better performance. However,
    note that increasing the size of the hidden layer causes all three sets of internal
    weights (that is, *U*, *W*, and *V*) to increase as well, thus resulting in a
    high computational footprint.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rnn_hidden_size` – RNN中隐藏层的维度。增加隐藏层的维度通常能提高性能。然而，请注意，增加隐藏层的大小会导致所有三组内部权重（即*U*、*W*和*V*）的增加，从而导致较高的计算负担。'
- en: '`n_classes` – Number of unique output classes present.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_classes` – 唯一输出类的数量。'
- en: '`batch_size` – The batch size for training data, validation data, and test
    data. A higher batch size often leads to better results as we are seeing more
    data during each optimization step, but just like unrolling, this causes a higher
    memory requirement.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size` – 训练数据、验证数据和测试数据的批量大小。较高的批量大小通常会带来更好的结果，因为在每次优化步骤中，我们会看到更多的数据，但就像展开一样，这也会导致更高的内存需求。'
- en: '`epochs` – The number of epochs to train the model for.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs` – 训练模型的轮数。'
- en: 'These are defined below:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是定义的内容：
- en: '[PRE17]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now we will define the model.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义模型。
- en: Defining the model
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义模型
- en: 'We will define the model here. Our model will have an embedding layer, followed
    by a simple RNN layer, and finally a dense prediction layer. One thing to note
    in the work we have done so far is that, unlike in previous chapters, we haven’t
    yet defined a `Tokenizer` object. Although the `Tokenizer` has been an important
    part of our NLP pipeline to convert each token (or word) into an ID, there’s a
    big downside to using an external tokenizer. After training the model, if you
    forget to save the tokenizer along with the model, your machine learning model
    becomes useless: to combat this, during inference, you would need to map each
    word to the exact ID it was mapped to during training.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里定义模型。我们的模型将包含一个嵌入层，接着是一个简单的RNN层，最后是一个密集预测层。需要注意的是，在我们迄今为止的工作中，与前几章不同，我们尚未定义`Tokenizer`对象。虽然`Tokenizer`是我们自然语言处理（NLP）管道中的重要部分，用来将每个token（或单词）转换为ID，但使用外部分词器有一个大缺点。训练模型后，如果你忘记将分词器与模型一起保存，那么你的机器学习模型就会变得毫无用处：为了应对这一点，在推理时，你需要将每个单词映射到它在训练期间所对应的ID。
- en: 'This is a significant risk the tokenizer poses. In this chapter, we will seek
    an alternative, where we will integrate the tokenization mechanism right into
    our model, so that we don’t need to worry about it later. *Figure 6.12* depicts
    the overall architecture of the model:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这是分词器所带来的重大风险。在本章中，我们将寻求一种替代方法，在模型中集成分词机制，这样我们以后就不需要再担心这个问题了。*图6.12*展示了模型的整体架构：
- en: '![](img/B14070_06_12.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_12.png)'
- en: 'Figure 6.12: Overall architecture of the model. The text vectorization layer
    tokenizes the text and converts it into word IDs. Next, each token is fed as an
    input at each timestep of the RNN. Finally, the RNN predicts a label for each
    token at every time step'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12：模型的整体架构。文本向量化层将文本分词并转换为词ID。接下来，每个token作为RNN的每个时间步的输入。最后，RNN在每个时间步预测每个token的标签。
- en: Introduction to the TextVectorization layer
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本向量化层介绍
- en: 'The `TextVectorization` layer can be thought of as a modernized tokenizer that
    can be plugged into the model. Here, we will play around just with the `TextVectorization`
    layer, without the overhead of the complexity from the rest of the model. First,
    we will import the `TextVectorization` layer:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`TextVectorization`层可以看作是一个现代化的分词器，可以插入到模型中。在这里，我们将仅操作`TextVectorization`层，而不涉及模型其他部分的复杂性。首先，我们将导入`TextVectorization`层：'
- en: '[PRE18]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now we will define a simple text corpus:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义一个简单的文本语料库：
- en: '[PRE19]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can instantiate a text vectorization layer as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按如下方式实例化文本向量化层：
- en: '[PRE20]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After instantiating, you need to fit this layer on some data. This way, just
    like the tokenizer we used previously, it can learn a word-to-numerical ID mapping.
    For this, we invoke the `adapt()` method of the layer, by passing the corpus of
    text as an input:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化后，您需要在一些数据上拟合该层。这样，像我们之前使用的分词器一样，它可以学习单词到数字ID的映射。为此，我们通过传递文本语料库作为输入，调用该层的`adapt()`方法：
- en: '[PRE21]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can generate the tokenized output as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按如下方式生成分词输出：
- en: '[PRE22]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Which will have:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 它将包含：
- en: '[PRE23]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can also see the vocabulary the layer has learned:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看该层所学到的词汇：
- en: '[PRE24]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can see that the layer has done some pre-processing (e.g. turned words to
    lowercase and removed punctuation). Next let’s see how we can limit the size of
    the vocabulary. We can do this with the `max_tokens` argument:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到该层已经完成了一些预处理（例如将单词转换为小写并去除了标点符号）。接下来让我们看看如何限制词汇表的大小。我们可以通过`max_tokens`参数来实现：
- en: '[PRE25]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If you convert the `toy_corpus` to word IDs, you will see:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将`toy_corpus`转换为单词ID，你将看到：
- en: '[PRE26]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The vocabulary will be as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表将如下所示：
- en: '[PRE27]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can now see that there are only five elements in the vocabulary, just like
    we specified. Now if you need to skip the text pre-processing that happens within
    the layer, you can do so by setting the `standardize` argument to `None` in the
    layer:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到，词汇表中只有五个元素，就像我们指定的那样。现在，如果你需要跳过层内部发生的文本预处理，你可以通过将层中的`standardize`参数设置为`None`来实现：
- en: '[PRE28]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This will produce:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生：
- en: '[PRE29]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The vocabulary will look as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表将如下所示：
- en: '[PRE30]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Finally, we can also control the padding/truncation of sequences with the `output_sequence_length`
    command. For example, the following command will pad/truncate sequences at length
    `4:`
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还可以通过`output_sequence_length`命令控制序列的填充/截断。例如，以下命令将在长度为`4`的位置进行填充/截断：
- en: '[PRE31]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This will produce:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生：
- en: '[PRE32]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here the vocabulary is:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的词汇表是：
- en: '[PRE33]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now you have a good understanding of the arguments and what they do in the `TextVectorization`
    layer. Let’s now discuss the model.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经很好地理解了`TextVectorization`层中的参数及其作用。接下来让我们讨论模型。
- en: Defining the rest of the model
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义模型的其余部分
- en: 'First we will import the necessary modules:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入必要的模块：
- en: '[PRE34]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We will define an input layer that has a single column (i.e. each sentence
    represented as a single unit) and has `dtype=tf.string`:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个输入层，该层有一个单列（即每个句子表示为一个单元），并且`dtype=tf.string`：
- en: '[PRE35]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we will define a function that takes a corpus, a maximum sequence length,
    and a vocabulary size, and returns the trained `TextVectorization` layer and the
    vocabulary size:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个函数，该函数接收一个语料库、最大序列长度和词汇表大小，并返回训练好的`TextVectorization`层和词汇表大小：
- en: '[PRE36]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The function does what we have already described. However, pay attention to
    the various arguments we have set for the vectorization layer. We are passing
    the vocabulary size as `max_tokens`; we are setting the `standardize` to `None`.
    This is an important setting. When performing NER, keeping the case of characters
    is very important. Typically, an entity starts with an uppercase letter (e.g.
    the name of a person or organization). Therefore, we should preserve the case
    in the text.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数做的就是我们已经描述过的内容。然而，注意我们为向量化层设置的各种参数。我们将词汇表大小作为`max_tokens`传递；我们将`standardize`设置为`None`。这是一个重要的设置。在进行命名实体识别（NER）时，保持字符的大小写非常重要。通常，一个实体以大写字母开头（例如人的名字或组织名称）。因此，我们应该保留文本中的大小写。
- en: 'Finally, we also set the `output_sequence_length` to the sequence length we
    found during the analysis. With that, we create the text vectorization layer as
    follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还将`output_sequence_length`设置为我们在分析过程中找到的序列长度。这样，我们就可以按如下方式创建文本向量化层：
- en: '[PRE37]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then pass the `word_input` to the `vectorize_layer` and get the output:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将`word_input`传递给`vectorize_layer`并获取输出：
- en: '[PRE38]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output from the `vectorize_layer` (i.e `vectorized_out`) will be sent to
    an embedding layer. This embedding layer is a randomly initialized embedding layer,
    which will have an output dimensionality of `embedding_size`:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 来自`vectorize_layer`的输出（即`vectorized_out`）将传递到一个嵌入层。这个嵌入层是一个随机初始化的嵌入层，输出的维度为`embedding_size`：
- en: '[PRE39]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Until now, we dealt with feed-forward networks. Outputs of feed-forward networks
    did not have a time dimension. But if you look at the output from the `TextVectorization`
    layer, it will be a `[batch size, sequence length]` - sized output. When this
    output goes through an embedding layer, the output would be a `[batch size, sequence
    length, embedding size]`-shaped tensor. In other words, there is an additional
    time dimension included in the output of the embedding layer.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们处理的是前馈网络。前馈网络的输出没有时间维度。但是，如果你查看`TextVectorization`层的输出，它将是一个`[batch
    size, sequence length]`形状的输出。当这个输出经过嵌入层时，输出将是一个`[batch size, sequence length,
    embedding size]`形状的张量。换句话说，嵌入层的输出中包含了一个额外的时间维度。
- en: Another difference is the introduction of the `mask_true` argument. Masking
    is used to mask uninformative words added to sequences (e.g. the padding token
    added to make sentences a fixed length), as they do not contribute to the final
    outcome. Masking is a commonly used technique in sequence learning. To learn more
    about masking, please read the information box below.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个区别是引入了`mask_true`参数。遮蔽（masking）用于掩盖添加到序列中的无效词（例如，为了使句子长度固定而添加的填充符号），因为它们对最终结果没有贡献。遮蔽是序列学习中常用的技术。要了解更多关于遮蔽的内容，请阅读下方的信息框。
- en: '**Masking in sequence learning**'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '**序列学习中的遮蔽**'
- en: Naturally, text has arbitrary lengths. For example, sentences in a corpus would
    have a wide variety of token lengths. But deep networks process tensors with fixed
    dimensions. To bring arbitrary-length sentences to constant length, we pad these
    sequences with some special value (e.g. 0). However, these padded values are synthetic,
    and only serve as a way to ensure the correct input shape. They should not contribute
    to the final loss or evaluation metrics. To ignore them during loss calculation
    and evaluation, “masking” is used. The idea is to multiply the loss resulting
    from padded timesteps with a zero, essentially cutting them off from the final
    loss.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，文本的长度是任意的。例如，语料库中的句子可能有不同的标记长度。而深度网络处理的是固定维度的张量。为了将任意长度的句子转换为常数长度，我们会用一些特殊的值（例如
    0）对这些序列进行填充。然而，这些填充值是人工的，只是为了确保正确的输入形状。它们不应该对最终的损失或评估指标产生影响。为了在损失计算和评估时忽略它们，使用了“遮蔽”技术。其原理是将来自填充时间步长的损失乘以零，实质上将其从最终损失中切断。
- en: It would be cumbersome to manually perform masking when training a model. But
    in TensorFlow, most layers support masking. For example, in the embedding layer,
    to ignore padded values (which will be zeros), all you need to do is set `mask_true=True`.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时手动执行遮蔽操作会非常繁琐。但在 TensorFlow 中，大多数层都支持遮蔽。例如，在嵌入层中，为了忽略填充的值（通常是零），你只需要设置`mask_true=True`。
- en: When you enable masking in a layer, it will propagate the mask to the downstream
    layers, flowing down until the loss computations. In other words, you only need
    to enable masking at the start of the model (as we have done at the embedding
    layer) and the rest is taken care of by TensorFlow.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在某个层中启用遮蔽时，它会将遮蔽传播到下游层，直到损失计算为止。换句话说，你只需要在模型开始时启用遮蔽（就像我们在嵌入层中所做的那样），剩下的部分由
    TensorFlow 自动处理。
- en: 'Following this, we will define the core layer of our model, the RNN:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义模型的核心层——RNN：
- en: '[PRE40]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You can implement a vanilla RNN by simply calling `tf.keras.layers.SimpleRNN`.
    Here we pass two important arguments. There are other useful arguments besides
    the two discussed here, however, they will be covered in later chapters with more
    complex variants of RNNs:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过简单地调用`tf.keras.layers.SimpleRNN`来实现一个基础的 RNN。在这里，我们传递了两个重要的参数。除了这两个参数，还有其他有用的参数，但它们将在后续章节中与更复杂的
    RNN 变体一起讲解：
- en: '`units` (`int`) – This defines the hidden output size of the RNN model. The
    larger this is, the more representational power the model will have.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`units`（`int`） – 这定义了 RNN 模型的隐藏输出大小。这个值越大，模型的表示能力就越强。'
- en: '`return_sequences` (`bool`) – Whether to return outputs from all the timesteps,
    or to return only the last output. For NER tasks, we need to label every single
    token. Therefore we need to return outputs for all the time steps.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_sequences`（`bool`） – 是否返回所有时间步的输出，还是仅返回最后一个输出。对于命名实体识别（NER）任务，我们需要标注每个单独的标记。因此，我们需要返回所有时间步的输出。'
- en: 'The `rnn_layer` takes a `[batch size, sequence length, embedding size]`-sized
    tensor and returns a `[batch size, sequence length, rnn hidden size]`-sized tensor.
    Finally, the time-distributed output from the RNN will go to a Dense layer with
    `n_classes` output nodes and a `softmax` activation:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '`rnn_layer` 接受一个形状为 `[batch size, sequence length, embedding size]` 的张量，并返回一个形状为
    `[batch size, sequence length, rnn hidden size]` 的张量。最后，来自 RNN 的时间分布输出将传递给一个具有
    `n_classes` 输出节点和 `softmax` 激活函数的全连接层：'
- en: '[PRE41]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, we can define the final model as follows. It takes a batch of string
    sentences as the input, and returns a batch of sequences of labels as the output:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以按如下方式定义最终模型。它接收一批字符串句子作为输入，并返回一批标签序列作为输出：
- en: '[PRE42]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: We have now finished building the model. Next, we will discuss the loss function
    and the evaluation metrics.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了模型的构建。接下来，我们将讨论损失函数和评估指标。
- en: Evaluation metrics and the loss function
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估指标和损失函数
- en: 'During our previous discussion, we alluded to the fact that NER tasks carry
    a high class imbalance. It is quite normal for text to have more non-entity-related
    tokens than entity-related tokens. This leads to large amounts of other (`O`)
    type labels and fewer of the remaining types. We need to take this into consideration
    when training the model and evaluating the model. We will address the class imbalance
    in two ways:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的讨论中，我们提到过命名实体识别（NER）任务通常存在较大的类别不平衡问题。文本中非实体相关的标记通常比实体相关的标记更多。这导致出现大量的其他（`O`）类型标签，而其他类型的标签较少。在训练模型和评估模型时，我们需要考虑这一点。我们将通过两种方式来解决类别不平衡问题：
- en: We will create a new evaluation metric that is resilient to class imbalance
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将创建一个新的评估指标，能够抵抗类别不平衡
- en: We will use sample weights to penalize more frequent classes and boost the importance
    of rare classes
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用样本权重来惩罚频繁出现的类别，并提升稀有类别的重要性
- en: In this section, we will only address the former. The latter will be addressed
    in the next section. We will define a modified version of the accuracy. This is
    called a macro-averaged accuracy. In macro averaging, we compute accuracies for
    each class separately, and then average it. Therefore, the class imbalance is
    ignored when computing the accuracy. When computing standard metrics like accuracy
    precision or recall, there are different types of averaging available. To learn
    more about these, read the information box below.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们仅解决前者问题。后者将在下一节中讨论。我们将定义一个修改版的准确率。这被称为宏观平均准确率。在宏观平均中，我们分别计算每个类别的准确率，然后求平均。因此，在计算准确率时，类别不平衡问题被忽略。当计算标准指标（如准确率、精确率或召回率）时，有多种不同的平均方式可供选择。欲了解更多信息，请参阅下方的信息框。
- en: '**Different types of metric averaging**'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '**不同类型的指标平均方式**'
- en: 'There are different types of averaging available for metrics. You can read
    one such example of these averaging available in scikit-learn explained at [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html
    ). Consider a simple binary classification example with the following confusion
    matrix results:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 指标有多种可用的平均方式。你可以在 scikit-learn 文档中阅读其中一种平均方式，详细信息请见 [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html)。考虑一个简单的二分类示例，混淆矩阵的结果如下：
- en: '![](img/B14070_06_13.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_13.png)'
- en: 'Figure 6.13: Example confusion matrix results'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13：示例混淆矩阵结果
- en: '**micro** – Computes a global metric, ignoring the differences in class distribution.
    e.g. 35/65 = ~54%'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**微观** – 计算全局指标，忽略类别分布的差异。例如 35/65 = ~54%'
- en: '**macro** – Computes the metric for each class separately and computes the
    mean. e.g. (35/40 + 0/25)/2 = ~43.7%'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**宏观** – 分别计算每个类别的指标并求平均。例如 (35/40 + 0/25)/2 = ~43.7%'
- en: '**weighted** – Computes the metric for each class separately and weighs it
    by support (i.e. number of true labels for each class). e.g. (35/40)* 40 + (0/25)
    * 25 / 65 = ~54%'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加权** – 分别计算每个类别的指标并按支持度加权（即每个类别的真实标签数量）。例如 (35/40)* 40 + (0/25) * 25 / 65
    = ~54%'
- en: Here you can see the micro and weighted return the same result. This is because
    the denominator of the accuracy computation is the same as the support. Therefore,
    they cancel out in the weighted averaging. However for other metrics such as precision
    and recall you will get different values.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到微观和加权返回相同的结果。这是因为准确率计算的分母与支持度相同。因此，在加权平均时它们会相互抵消。然而，对于精确率和召回率等其他指标，你将获得不同的值。
- en: 'Below we define the function to compute macro accuracy using a batch of true
    targets (`y_true`) and predictions (`y_pred`). `y_true` will have the shape `[batch_size,
    sequence length]` and `y_pred` will have the shape `[batch size, sequence length,
    n_classes]`:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在下文中，我们定义了一个函数来计算宏观准确率，输入为一批真实目标（`y_true`）和预测值（`y_pred`）。`y_true`的形状为`[batch_size,
    sequence length]`，`y_pred`的形状为`[batch size, sequence length, n_classes]`：
- en: '[PRE43]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: It is important to note that we have to write this function using TensorFlow
    operations, so that they are executed as a graph. Even though TensorFlow 2 has
    migrated toward more imperative style execution operations, there still are remnants
    of the declarative style introduced by TensorFlow 1.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们必须使用 TensorFlow 操作来编写此函数，以确保它们作为图执行。尽管 TensorFlow 2 已转向更具命令式风格的执行操作，但
    TensorFlow 1 中引入的声明式风格仍然有所残留。
- en: First we flatten `y_true` so that it’s a vector. Next we get the predicted label
    from `y_pred` using the `tf.argmax()` function and flatten the predicted labels
    to a vector. The two flattened structures will have the same number of elements.
    Then we sort `y_true`, so that same-labeled elements are close together.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 首先我们将`y_true`展平，使其成为一个向量。接着，我们使用`tf.argmax()`函数从`y_pred`中获取预测标签，并将预测标签展平为一个向量。这两个展平后的结构将具有相同的元素数量。然后，我们对`y_true`进行排序，使得相同标签的元素紧密排列在一起。
- en: We take the indices of the original data after sorting and then use the `tf.gather()`
    function to order `y_pred` in the same order as `y_true`. In other words, `sorted_y_true`
    and `sorted_y_pred` still have the same correspondence with each other. The `tf.gather()`
    function takes a tensor and a set of indices and orders the passed tensor in the
    order of the indices. For more information about `tf.gather()` refer to [https://www.tensorflow.org/api_docs/python/tf/gather](https://www.tensorflow.org/api_docs/python/tf/gather).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在排序后的原始数据中取索引，然后使用`tf.gather()`函数将`y_pred`按与`y_true`相同的顺序排列。换句话说，`sorted_y_true`和`sorted_y_pred`之间仍然保持相同的对应关系。`tf.gather()`函数接收一个张量和一组索引，并根据这些索引对传入的张量进行排序。关于`tf.gather()`的更多信息，请参考[https://www.tensorflow.org/api_docs/python/tf/gather](https://www.tensorflow.org/api_docs/python/tf/gather)。
- en: 'Then we compute `sorted_correct`, which is a simple indicator function that
    switches on if the corresponding element in `sorted_y_true` and `sorted_y_pred`
    are the same, and if not stays off. Then we use the `tf.math.segment_sum()` function
    to compute a segmented sum of correctly predicted samples. Samples belonging to
    each class are considered a single segment (`correct_for_each_label`). The `segment_sum()`
    function takes two arguments: `data` and `segment_ids`. For example, if the `data`
    is `[0, 1, 2, 3, 4, 5, 6, 7]` and `segment_ids` are `[0, 0, 0, 1, 1, 2, 3, 3]`,
    then the segment sum would be `[0+1+2, 3+4, 5, 6+7] = [3, 7, 5, 13]`.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算`sorted_correct`，这是一个简单的指示函数，当`sorted_y_true`和`sorted_y_pred`中的对应元素相同时，它会启动，如果不同则保持关闭。接着我们使用`tf.math.segment_sum()`函数来计算正确预测样本的分段和。每个类别的样本被视为一个单独的段（`correct_for_each_label`）。`segment_sum()`函数有两个参数：`data`和`segment_ids`。例如，如果`data`是`[0,
    1, 2, 3, 4, 5, 6, 7]`，`segment_ids`是`[0, 0, 0, 1, 1, 2, 3, 3]`，则分段和为`[0+1+2, 3+4,
    5, 6+7] = [3, 7, 5, 13]`。
- en: Then we do the same for a vector of 1s. In this case, we get the number of true
    samples present for each class in the batch of data (`all_for_each_label`). Note
    that we are adding a 1 at the end. This is to avoid division by 0 in the next
    step. Finally, we divide `correct_for_each_label` by `all_for_each_label`, which
    gives us a vector containing the accuracy of each class. With that we compute
    the mean accuracy, which is the macro-averaged accuracy.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对一个由1组成的向量做同样的操作。在这种情况下，我们得到了每个类别在数据批次中存在的真实样本数量（`all_for_each_label`）。请注意，我们在末尾添加了一个1。这是为了避免在下一步中出现除以0的情况。最后，我们将`correct_for_each_label`除以`all_for_each_label`，得到一个包含每个类别准确率的向量。然后我们计算平均准确率，即宏平均准确率。
- en: 'Finally we wrap this function in a `MeanMetricWrapper` that will produce a
    `tf.keras.metrics.Metric` object that we can pass to the `model.compile()` function:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将这个函数封装在一个`MeanMetricWrapper`中，这将产生一个`tf.keras.metrics.Metric`对象，我们可以将其传递给`model.compile()`函数：
- en: '[PRE44]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Compile the model by calling:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用以下方式来编译模型：
- en: '[PRE45]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Next, we will train the model with the data prepared.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用准备好的数据训练模型。
- en: Training and evaluating RNN on NER task
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在NER任务上训练和评估RNN
- en: 'Let’s train our model on the data we have prepared. But first, we need to define
    a function to tackle the class imbalance in our dataset. We will pass sample weights
    to the `model.fit()` function. To compute sample weights, we will first define
    a function called `get_class_weights()` that computes `class_weights` for each
    class. Next we will pass the class weights to another function, `get_sample_weights_from_class_weights()`,
    which will generate sample weights:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在准备好的数据上训练模型。但首先，我们需要定义一个函数来处理数据集中的类别不平衡问题。我们将把样本权重传递给`model.fit()`函数。为了计算样本权重，我们首先定义一个名为`get_class_weights()`的函数，用来计算每个类别的`class_weights`。接下来，我们将把类别权重传递给另一个函数`get_sample_weights_from_class_weights()`，该函数将生成样本权重：
- en: '[PRE46]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The first function, `get_class_weights()`, takes a `train_labels` (a list of
    list of class IDs). Then we create a pandas `Series` object with `train_labels`.
    Note that we are using a function called `chain` from the built-in `itertools`
    library, which will flatten `train_labels` to a list of class IDs. The `Series`
    object contains frequency counts of each class label that appears in the train
    dataset. Next to compute weights, we divide the minimum frequency element-wise
    from other frequencies. In other words, if the frequency for class label ![](img/B14070_06_054.png)
    is denoted by ![](img/B14070_06_055.png), and the total label set is denoted by
    ![](img/B14070_06_056.png), the weight for class ![](img/B14070_06_054.png) is
    computed as:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数`get_class_weights()`接受`train_labels`（一个包含类别ID列表的列表）。然后我们使用`train_labels`创建一个pandas的`Series`对象。注意，我们使用了内置的`itertools`库中的`chain`函数，它会将`train_labels`展平为类别ID的列表。这个`Series`对象包含了在训练数据集中每个类别标签的频次。接下来，为了计算权重，我们将最小频次按元素逐一从其他频次中进行除法运算。换句话说，如果类别标签![](img/B14070_06_054.png)的频率用![](img/B14070_06_055.png)表示，总标签集用![](img/B14070_06_056.png)表示，则类别![](img/B14070_06_054.png)的权重计算公式为：
- en: '![](img/B14070_06_058.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_058.png)'
- en: 'Finally, the output is converted into a dictionary that has class IDs as keys
    and class weights as values. Next we need to convert the `class_weights` to `sample_weights`.
    We simply perform a dictionary lookup element-wise on each label to generate a
    sample weight from `class_weights`. The `sample_weights` will be the same shape
    as the `train_labels` as there’s one weight for each sample:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出被转换为一个字典，其中类别ID作为键，类别权重作为值。接下来，我们需要将`class_weights`转换为`sample_weights`。我们只需对每个标签执行字典查找操作，按元素逐一生成样本权重，基于`class_weights`。`sample_weights`的形状将与`train_labels`相同，因为每个样本都有一个权重：
- en: '[PRE47]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can use NumPy’s `np.vectorize()` function to achieve this. `np.vectorize()`
    takes in a function (e.g. `class_weights.get()` is the key lookup function provided
    by Python) and applies that on all elements, which gives us the sample weights.
    Call the functions we defined above to generate the actual weights:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用NumPy的`np.vectorize()`函数来实现这一点。`np.vectorize()`接受一个函数（例如，`class_weights.get()`是Python提供的键查找函数），并将其应用于所有元素，从而得到样本权重。调用我们之前定义的函数来生成实际的权重：
- en: '[PRE48]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'After we have the sample weights at our disposal, we can train our model. You
    can view the `class_weights` by printing them out. This will give:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们拥有了样本权重后，我们可以训练模型。你可以通过打印`class_weights`来查看它们。这将给出：
- en: '[PRE49]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You can see the class `Other` has the lowest weight (because it’s the most
    frequent), and the class `I-MISC` has the highest as it’s the least frequent.
    Now we will train our model using the prepared data:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到类别`Other`的权重最低（因为它是最频繁的类别），而类别`I-MISC`的权重最高，因为它是最不频繁的类别。现在我们将使用准备好的数据训练我们的模型：
- en: '[PRE50]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You should get an accuracy of around 78-79% without any special performance
    optimization tricks. Next you can evaluate the model on test data with:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能得到大约78-79%的准确率，没有进行任何特殊的性能优化技巧。接下来，你可以使用以下命令在测试数据上评估模型：
- en: '[PRE51]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This will give a test accuracy of around 77%. Since the validation accuracy
    and test accuracy are on par, we can say that the model has generalized well.
    But to make sure, let’s visually inspect a few samples from the test set.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出大约77%的测试准确率。由于验证准确率和测试准确率相当，我们可以说模型的泛化表现良好。但为了确保这一点，让我们视觉检查一下测试集中的一些样本。
- en: Visually analyzing outputs
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化分析输出
- en: 'To analyze the output, we will use the first five sentences in the test set:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析输出，我们将使用测试集中的前五个句子：
- en: '[PRE52]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next predict using the model and convert those predictions to predicted class
    IDs:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来使用模型进行预测，并将这些预测转换为预测的类别ID：
- en: '[PRE53]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We will create a reversed `labels_map` that has a mapping from label ID to
    label string:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个反转的`labels_map`，它将标签ID映射到标签字符串：
- en: '[PRE54]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Finally, we will print out the results:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将打印出结果：
- en: '[PRE55]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'This will print out:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出：
- en: '[PRE56]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: It can be seen that our model is doing a decent job. It is good at identifying
    locations but is struggling at identifying the names of people. Here we end our
    discussion about the basic RNN solution that performs NER. In the next section,
    we will make the model more complex, giving it the ability to understand text
    better by providing more fine-grained details. Let’s understand how we can improve
    our model.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到我们的模型表现不错。它擅长识别位置，但在识别人物名称上存在困难。在这里，我们结束了关于执行命名实体识别（NER）的基本RNN解决方案的讨论。在接下来的部分，我们将使模型更加复杂，赋予它通过提供更细粒度的细节来更好理解文本的能力。让我们了解一下如何改进我们的模型。
- en: NER with character and token embeddings
  id: totrans-350
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用字符和标记嵌入进行命名实体识别（NER）
- en: Nowadays, recurrent models used to solve the NER task are much more sophisticated
    than having just a single embedding layer and an RNN model. They involve using
    more advanced recurrent models like **Long Short-Term Memory** (**LSTM**), **Gated
    Recurrent Units** (**GRUs**), etc. We will set aside the discussion about these
    advanced models for several upcoming chapters. Here we will focus our discussion
    on a technique that provides the model embeddings at multiple scales, enabling
    it to understand language better. That is, instead of relying only on token embeddings,
    also use character embeddings. Then a token embedding is generated with the character
    embeddings by shifting a convolutional window over the characters in the token.
    Don’t worry if you don’t understand the details yet. The following sections will
    go into specific details of the solution. This exercise is available in `ch06_rnns_for_named_entity_recognition.ipynb`
    in the `Ch06-Recurrent-Neural-Networks` folder.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，用于解决命名实体识别（NER）任务的递归模型比仅使用单一嵌入层和RNN模型要复杂得多。它们涉及使用更高级的递归模型，如**长短期记忆（LSTM）**、**门控递归单元（GRU）**等。我们将在接下来的几章中暂时不讨论这些高级模型。这里，我们将重点讨论一种能够提供多尺度模型嵌入的技术，从而使其更好地理解语言。也就是说，除了依赖标记嵌入外，还要使用字符嵌入。然后，通过在标记的字符上滑动卷积窗口，利用字符嵌入生成标记嵌入。如果你现在还不理解细节，别担心，接下来的章节将详细介绍解决方案。这个练习可以在`Ch06-Recurrent-Neural-Networks`文件夹中的`ch06_rnns_for_named_entity_recognition.ipynb`找到。
- en: Using convolution to generate token embeddings
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用卷积生成标记嵌入
- en: 'A combination of character embeddings and a convolutional kernel can be used
    to generate token embeddings (*Figure 6.14*). The method will be as follows:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 组合字符嵌入和卷积核可以用来生成标记嵌入（*图6.14*）。该方法如下：
- en: Pad each token (e.g. word) to a predefined length
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个标记（例如单词）填充到预定的长度
- en: Look up the character embeddings for the characters in the token from an embedding
    layer
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找标记中字符的字符嵌入，来自嵌入层
- en: Shift a convolutional kernel over the sequence of character embeddings to generate
    a token embedding
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将卷积核滑过字符嵌入序列，生成标记嵌入
- en: '![](img/B14070_06_14.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_06_14.png)'
- en: 'Figure 6.14: How token embeddings are generated using character embeddings
    and the convolution operation'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14：如何使用字符嵌入和卷积操作生成标记嵌入
- en: 'The very first thing we need to do is analyze the statistics around how many
    characters there are for a token in our corpus. Similar to how we did it previously,
    we can do this with pandas:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是分析语料库中每个标记的字符统计信息。类似于之前的方法，我们可以使用pandas来完成：
- en: '[PRE57]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In computing `vocab_ser`, the first part (i.e. `pd.Series(train_sentences).str.split()`)
    will result in a pandas `Series`, whose elements are a list of tokens (each token
    in the sentence is an item of that list). Next, `explode()` will convert the `Series`
    of a list of tokens into a `Series` of tokens, by converting each token into a
    separate item in the `Series`. Finally we take only the unique tokens in that
    `Series`. Here we end up with a pandas `Series` where each item is a unique token.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算`vocab_ser`时，第一部分（即`pd.Series(train_sentences).str.split()`）将产生一个pandas `Series`，其元素是标记列表（句子中的每个标记都是该列表的一个元素）。接下来，`explode()`将把包含标记列表的`Series`转换成单独的标记`Series`，即将每个标记转换为`Series`中的一个独立元素。最后，我们只取该`Series`中的唯一标记。最终我们会得到一个pandas
    `Series`，其中每一项是一个唯一的标记。
- en: 'We will now use the `str.len()` function to get the length of each token (i.e.
    the number of characters) and look at the 95% percentile in that. We will get
    the following:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用`str.len()`函数获取每个标记的长度（即字符数），并查看其中的95%分位数。我们将得到以下结果：
- en: '[PRE58]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We can see around 95% of our words have less than or equal to 12 characters.
    Next, we will write a function to pad shorter tokens:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到大约95%的单词字符数小于或等于12个。接下来，我们将编写一个函数来填充较短的标记：
- en: '[PRE59]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The function takes a set of tokenized sentences (i.e. each sentence as a list
    of tokens, not a string) and a maximum sequence length. Note that this is the
    maximum sequence length we used previously, not the new token length we discussed.
    This function would then do the following:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数接受一组标记化的句子（即每个句子作为一个标记列表，而不是字符串）和一个最大序列长度。请注意，这是我们之前使用的最大序列长度，而不是我们讨论过的新标记长度。该函数将执行以下操作：
- en: For longer sentences, only return the `max_seq_length` tokens
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于较长的句子，只返回`max_seq_length`个标记
- en: For shorter sentences, append ‘‘ as a token until `max_seq_length` is reached
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于较短的句子，追加`‘’`作为标记，直到达到`max_seq_length`
- en: 'Let’s run this function on a small toy dataset:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个小型的玩具数据集上运行这个函数：
- en: '[PRE60]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This will return:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回：
- en: '[PRE61]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We will now define a new `TextVectorization` layer that can cope with the changes
    we introduced to the data. Instead of tokenizing on the token level, the new `TextVectorization`
    layer must tokenize on the character level. For this we need to make a few changes.
    We will again write a function to contain this vectorization layer:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义一个新的`TextVectorization`层来应对我们对数据所做的变化。新的`TextVectorization`层必须在字符级进行标记化，而不是在标记级进行。为此，我们需要做一些更改。我们将再次编写一个函数来包含这个向量化层：
- en: '[PRE62]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We first define a function called `_split_char()` that takes a token (as a
    `tf.Tensor`) and returns a char-tokenized tensor. For example, `_split_char(tf.constant([''abcd'']))`
    would return `<tf.RaggedTensor [[b''a'', b''b'', b''c'', b''d'']]>`. Then we define
    a `TextVectorization` layer that will use this newly defined function as the way
    to split the data it gets. We will also define `output_sequence_length` as `max_token_length`.
    Then we create `tokenized_sentences`, a list of list of strings, and pad it using
    the `prepare_corpus_for_char_embeddings()` function we defined earlier. Finally
    we use the `TextVectorization` layer’s `adapt()` function to fit it with the data
    we prepared. Two key differences between the previous token-based text vectorizer
    and this char-based text vectorizer are in the input dimensions and the final
    output dimensions:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个名为`_split_char()`的函数，它接收一个标记（作为`tf.Tensor`）并返回一个字符标记化的张量。例如，`_split_char(tf.constant(['abcd']))`将返回`<tf.RaggedTensor
    [[b'a', b'b', b'c', b'd']]>`。然后，我们定义一个`TextVectorization`层，使用这个新定义的函数作为分割数据的方式。我们还会将`output_sequence_length`定义为`max_token_length`。接着，我们创建`tokenized_sentences`，这是一个包含字符串列表的列表，并使用之前定义的`prepare_corpus_for_char_embeddings()`函数对其进行填充。最后，我们使用`TextVectorization`层的`adapt()`函数来调整其适配我们准备的数据。之前基于标记的文本向量化器和这个基于字符的文本向量化器之间的两个关键区别在于输入维度和最终输出维度：
- en: Token-based vectorizer – Takes in a `[batch size, 1]`-sized input and produces
    a `[batch size, sequence length]`-sized output
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于标记的向量化器 – 接收一个`[batch size, 1]`大小的输入并生成一个`[batch size, sequence length]`大小的输出
- en: Char-based vectorizer – Takes in a `[batch size, sequence length, 1]`-sized
    input and produces a `[batch size, sequence length, token length]`-sized output
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于字符的向量化器 – 接收一个`[batch size, sequence length, 1]`大小的输入并生成一个`[batch size, sequence
    length, token length]`大小的输出
- en: Now we are equipped with the ingredients to implement our new and improved NER
    classifier.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经具备了实现新改进的NER分类器所需的所有要素。
- en: Implementing the new NER model
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现新的NER模型
- en: With a good conceptual understanding of the model, let’s implement the new NER
    model. We will first define some hyperparameters, followed by defining a text
    vectorizer as before. However, our `TextVectorization` will be more complex in
    this section, as we have several different levels of tokenization taking place
    (e.g. char-level and token-level). Finally we define the RNN-based model that
    produces the output.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在对模型有了良好的概念理解后，让我们实现新的NER模型。我们将首先定义一些超参数，接着像之前一样定义文本向量化器。然而，在这一部分中，我们的`TextVectorization`将变得更为复杂，因为我们将进行多层次的标记化（例如，字符级和标记级）。最后，我们定义一个基于RNN的模型来生成输出。
- en: Defining hyperparameters
  id: totrans-381
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义超参数
- en: 'First, we will define the two hyperparameters as follows:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义如下两个超参数：
- en: '[PRE63]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Defining the input layer
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义输入层
- en: 'We then define an input layer with the data type `tf.strings` as before:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们定义一个与之前相同的数据类型为`tf.strings`的输入层：
- en: '[PRE64]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The inputs to this layer would be a batch of sentences, where each sentence
    is a string.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 该层的输入将是一批句子，其中每个句子都是一个字符串。
- en: Defining the token-based TextVectorization layer
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义基于标记的TextVectorization层
- en: 'Then we define the token-level `TextVectorization` layer just like we did above:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们像上面一样定义标记级别的`TextVectorization`层：
- en: '[PRE65]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Defining the character-based TextVectorization layer
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义基于字符的TextVectorization层
- en: 'For the character-level vectorization layer we will employ the `get_fitted_char_vectorization_layer()`
    function we defined above:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 对于字符级别的向量化层，我们将使用上面定义的`get_fitted_char_vectorization_layer()`函数：
- en: '[PRE66]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Next, we will discuss the inputs for this layer.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论该层的输入。
- en: Processing the inputs for the char_vectorize_layer
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理char_vectorize_layer的输入
- en: We will use the same `word_input` for this new vectorization layer as well.
    However, using the same input means we need to introduce some interim pre-processing
    to get the input to the correct format intended for this layer. Remember that
    the input to this layer needs to be a `[batch size, sequence length, 1]`-sized
    tensor.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对这个新的向量化层使用相同的`word_input`。然而，使用相同的输入意味着我们需要引入一些中间预处理步骤，以将输入转换为适合此层的正确格式。请记住，传入此层的输入需要是一个形状为`[batch
    size, sequence length, 1]`的张量。
- en: 'This means the sentences need to be tokenized to a list of tokens. For that
    we will use the `tf.keras.layers.Lambda()` layer and the `tf.strings.split()`
    function:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着句子需要被标记化为一系列令牌。为此，我们将使用`tf.keras.layers.Lambda()`层和`tf.strings.split()`函数：
- en: '[PRE67]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The `Lambda` layer is used as a way to create a layer from a custom TensorFlow/Keras
    function, which may not be available as a standard layer in Keras. Here we are
    using a `Lambda` layer to define a layer that will tokenize a passed input to
    a list of tokens. Furthermore, the `tf.strings.split()` function returns a ragged
    tensor. In a typical tensor, all the dimensions need to have a constant size.
    A ragged tensor is a special tensor whose dimensions are not fixed. For example,
    since a list of sentences is highly unlikely to have the same number of tokens,
    this results in a ragged tensor. But TensorFlow will complain if you try to go
    forward with a `tf.RaggedTensor` as most layers do not support these tensors.
    Therefore, we need to convert this to a standard tensor using the `to_tensor()`
    function. We can pass a shape to this function and it will make sure the shape
    of the resulting tensor will be the defined shape (by means of padding and truncations).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '`Lambda`层用于从自定义的TensorFlow/Keras函数创建一个层，这个函数可能在Keras中没有作为标准层提供。在这里，我们使用`Lambda`层来定义一个层，将传入的输入标记化为一系列令牌。此外，`tf.strings.split()`函数返回一个稀疏张量。在典型的张量中，所有维度需要具有固定大小。而稀疏张量是一种特殊的张量，其维度不是固定的。例如，由于句子列表不太可能有相同数量的令牌，因此这会导致一个稀疏张量。但是，TensorFlow会抱怨，如果你尝试继续使用`tf.RaggedTensor`，因为大多数层不支持这些张量。因此，我们需要使用`to_tensor()`函数将其转换为标准张量。我们可以向该函数传递一个形状，它会确保结果张量的形状为定义的形状（通过填充和截断）。'
- en: A key thing to pay attention to is how the shapes of the input-output tensors
    are transformed at each layer. For example, we started off with a `[batch size,
    1]`-sized tensor that went into the `Lambda` layer to be transformed to a `[batch
    size, sequence length, 1]`-sized layer. Finally, the `char_vectorize_layer` transforms
    this into a `[batch size, sequence length, token length]`-sized tensor.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 需要特别注意的一点是每个层如何转换输入输出张量的形状。例如，我们一开始使用的是一个形状为`[batch size, 1]`的张量，进入`Lambda`层后转变为形状为`[batch
    size, sequence length, 1]`的层。最后，`char_vectorize_layer`将其转换为形状为`[batch size, sequence
    length, token length]`的张量。
- en: 'We will then define an embedding layer, with which we will look up embeddings
    for the resulting char IDs coming from the `char_vectorize_layer`:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将定义一个嵌入层，通过它我们可以查找来自`char_vectorize_layer`的字符 ID 对应的嵌入向量：
- en: '[PRE68]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This layer produces a `[batch size, sequence length, token length, 32]`-sized
    tensor, with a char embedding vector for each character in the tensor. Now it’s
    time to perform convolution on top of this output.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层生成一个形状为`[batch size, sequence length, token length, 32]`的张量，每个字符在张量中都有一个字符嵌入向量。现在是时候对这个输出进行卷积操作了。
- en: Performing convolution on the character embeddings
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对字符嵌入进行卷积操作
- en: 'We will define a 1D convolution layer with a kernel size of 5 (i.e. convolutional
    window size), a stride of 1, `''same''` padding, and a ReLU activation. We then
    feed the output from the previous section to this:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将定义一个1D卷积层，卷积核大小为5（即卷积窗口大小），步幅为1，`'same'`填充，并使用ReLU激活函数。然后我们将前一部分的输出传递给这个层：
- en: '[PRE69]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This layer typically takes a `[batch size, width, in channels]`-sized tensor.
    However, in our case, we have a four-dimensional input. This means, our Conv1D
    layer is going to behave in a time-distributed fashion. Put in another way, it
    will take an input with a temporal dimension (i.e. sequence length dimension)
    and produce an output with that dimension intact. In other words, it takes our
    input of shape `[batch size, sequence length, token length, 32 (in channels)]`
    and produces a `[batch size, sequence length, token length, 1 (out channels)]`-sized
    output. You can see that the convolution only operates on the last two dimensions,
    while keeping the first two as they are.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层通常接受一个大小为`[批次大小, 宽度, 输入通道]`的张量。然而，在我们的案例中，我们有一个四维输入。这意味着，我们的 Conv1D 层将以时间分布的方式进行运算。换句话说，它将处理一个具有时间维度（即序列长度维度）的输入，并生成一个保持该维度不变的输出。换句话说，它会接受形状为`[批次大小,
    序列长度, 标记长度, 32 (输入通道)]`的输入，并生成一个形状为`[批次大小, 序列长度, 标记长度, 1 (输出通道)]`的输出。你可以看到，卷积只在最后两个维度上进行运算，而保持前两个维度不变。
- en: Another way to think about this is, ignore the batch and sequence dimensions
    and visualize how convolution would work on the width and in channel dimensions.
    Then apply the same operation element-wise to other dimensions, while considering
    the operation on 2D `[width, in channel]` tensors as a single unit of computation.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考方式是，忽略批次和序列维度，直观地理解卷积如何在宽度和输入通道维度上进行运算。然后，将相同的操作逐元素应用到其他维度，同时将二维的`[宽度,
    输入通道]`张量视为一个单独的计算单元。
- en: 'Remember that we have a `[batch size, sequence length, token length, 1]`-sized
    output. This has an extra dimension of 1 at the end. We will write a simple `Lambda`
    layer to get rid of this dimension:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们有一个大小为`[批次大小, 序列长度, 标记长度, 1]`的输出。它在最后有一个额外的维度 1。我们将写一个简单的`Lambda`层来去掉这个维度：
- en: '[PRE70]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'To get the final output embedding (i.e. a combination of token- and character-based
    embeddings), we concatenate the two embeddings on the last axis. This would result
    in a 48 element-long vector (i.e. 32 element-long token embedding + 12 element-long
    char-based token embedding):'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到最终的输出嵌入（即标记嵌入和基于字符的嵌入的结合），我们在最后一个维度上连接这两种嵌入。这样会得到一个长度为 48 的向量（即 32 长度的标记嵌入
    + 12 长度的基于字符的标记嵌入）：
- en: '[PRE71]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The rest of the model, we will keep it the same. First define an RNN layer
    and pass the `concat_embedding_out` as an input:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的模型部分，我们保持不变。首先定义一个 RNN 层，并将`concat_embedding_out`作为输入：
- en: '[PRE72]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Remember that we have set `return_sequences=True`, which means it will produce
    an output at each time step, as opposed to only at the last time step. Next, we
    define the final Dense layer, which has `n_classes` output nodes (i.e. 9) and
    a `softmax` activation:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们已将`return_sequences=True`，这意味着它会在每个时间步产生一个输出，而不是仅在最后一个时间步产生输出。接下来，我们定义最终的
    Dense 层，它有`n_classes`个输出节点（即 9 个），并使用`softmax`激活函数：
- en: '[PRE73]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We define the model and compile it like before:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像以前一样定义并编译模型：
- en: '[PRE74]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: This is our final model. The key difference in this model compared to the previous
    solution is that it used two different embedding types. A standard token-based
    embedding layer and a complex, char-based embedding that was leveraged to generate
    token embeddings using the convolution operation. Now let’s train the model.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的最终模型。与之前的解决方案相比，这个模型的关键区别在于它使用了两种不同的嵌入类型。一种是标准的基于标记的嵌入层，另一种是复杂的基于字符的嵌入，用于生成通过卷积操作得到的标记嵌入。现在，让我们来训练这个模型。
- en: Model training and evaluation
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练与评估
- en: 'Model training is identical to the training we did for the standard RNN model,
    so we will not discuss it further:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练与我们为标准 RNN 模型所做的训练相同，因此我们将不再进一步讨论。
- en: '[PRE75]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: You should get around a ~2% validation accuracy and a ~1% test accuracy boost
    after these modifications.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些修改后，你应该能够获得大约 ~2% 的验证准确率提升和 ~1% 的测试准确率提升。
- en: Other improvements you can make
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你可以做的其他改进
- en: Here we will discuss several improvements you can make to uplift the model performance
    even further.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论一些可以进一步提升模型性能的改进。
- en: '**More RNN layers** – Adding more stacked RNN layers. By adding more hidden
    RNN layers, we can allow the model to learn more refined latent representations,
    leading to better performance. An example usage is shown below:'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更多 RNN 层** — 添加更多堆叠的 RNN 层。通过增加更多的隐藏 RNN 层，我们可以使模型学习到更精细的潜在表示，从而提高性能。以下是一个示例用法：'
- en: '[PRE76]'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '**Make the RNN layer bidirectional** – The RNN models we discussed so far are
    uni-directional, i.e. looks at the sequence of text from forward to backward.
    However a different variant known as bi-directional RNNs looks at the sequence
    in both directions, i.e. forward to backward and backward to forward. This leads
    to better language understanding in models and inevitably better performance.
    We will discuss this variant in more detail in the upcoming chapters. An example
    usage is shown below:'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使 RNN 层具有双向性** – 到目前为止，我们讨论的 RNN 模型都是单向的，即从前向后看文本序列。然而，另一种变体称为双向 RNN，会从两个方向查看序列，即从前向后和从后向前。这有助于模型更好地理解语言，并不可避免地提高性能。我们将在接下来的章节中更详细地讨论这一变体。下面是一个示例用法：'
- en: '[PRE77]'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '**Incorporate regularization techniques** – You can leverage L2 regularization
    and dropout techniques to avoid overfitting and improve generalization of the
    model.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**融入正则化技术** – 你可以利用 L2 正则化和丢弃法（dropout）技术来避免过拟合，并提高模型的泛化能力。'
- en: '**Use early stopping and learning rate reduction to reduce overfitting** –
    During model training, use early stopping (i.e. training the model only until
    the validation accuracy is improving) and learning rate reduction (i.e. gradually
    reducing the learning rate over the epochs).'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用早停和学习率衰减来减少过拟合** – 在模型训练过程中，使用早停（即仅在验证准确率提升时继续训练模型）和学习率衰减（即在训练过程中逐步降低学习率）。'
- en: We recommend experimenting with some of these techniques yourself to see how
    they can maximize the performance of your RNNs.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议你自己尝试一些这些技术，看看它们如何最大化 RNN 的性能。
- en: Summary
  id: totrans-433
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at RNNs, which are different from conventional feed-forward
    neural networks and more powerful in terms of solving temporal tasks.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了与传统的前馈神经网络不同的 RNN，它在解决时间序列任务时更为强大。
- en: Specifically, we discussed how to arrive at an RNN from a feed-forward neural
    network type structure.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们讨论了如何从前馈神经网络结构得出 RNN。
- en: We assumed a sequence of inputs and outputs, and designed a computational graph
    that can represent the sequence of inputs and outputs.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设有一个输入输出序列，并设计了一个能够表示输入输出序列的计算图。
- en: This computational graph resulted in a series of copies of functions that we
    applied to each individual input-output tuple in the sequence. Then, by generalizing
    this model to any given single time step *t* in the sequence, we were able to
    arrive at the basic computational graph of an RNN. We discussed the exact equations
    and update rules used to calculate the hidden state and the output.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算图结果是将函数复制应用于序列中的每个输入输出元组。然后，通过将这个模型推广到序列中的任意单个时间步 *t*，我们能够得出 RNN 的基本计算图。我们讨论了计算隐藏状态和输出的精确方程和更新规则。
- en: Next we discussed how RNNs are trained with data using BPTT. We examined how
    we can arrive at BPTT with standard backpropagation as well as why we can’t use
    standard backpropagation for RNNs. We also discussed two important practical issues
    that arise with BPTT—vanishing gradient and exploding gradient—and how these can
    be solved on the surface level.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了如何使用 BPTT 训练 RNN。我们分析了如何通过标准反向传播方法得到 BPTT，以及为什么不能使用标准的反向传播来训练 RNN。我们还讨论了使用
    BPTT 时出现的两个重要实际问题——梯度消失和梯度爆炸——以及如何在表面层面解决这些问题。
- en: Then we moved on to the practical applications of RNNs. We discussed four main
    categories of RNNs. One-to-one architectures are used for tasks such as text generation,
    scene classification, and video frame labeling. Many-to-one architectures are
    used for sentiment analysis, where we process the sentences/phrases word by word
    (compared to processing a full sentence in a single go, as we saw in the previous
    chapter). One-to-many architectures are common in image captioning tasks, where
    we map a single image to an arbitrarily long sentence phrase describing the image.
    Many-to-many architectures are leveraged for machine translation tasks.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续探讨了 RNN 的实际应用。我们讨论了四种主要的 RNN 架构。单一对单一架构用于文本生成、场景分类和视频帧标注等任务。多对单一架构用于情感分析，在这里我们逐词处理句子/短语（与上一章中一次性处理完整句子不同）。单对多架构在图像字幕生成任务中常见，其中我们将单张图像映射为一个任意长的句子短语来描述该图像。多对多架构用于机器翻译任务。
- en: We solved the task of NER with RNNs. In NER, the problem is to, given a sequence
    of tokens, predict a label for each token. The label represents an entity (e.g.
    organization, location, person, etc.). For this we used embeddings as well as
    an RNN to process each token while considering the sequence of tokens as a time-series
    input. We also used a text vectorization layer to convert tokens into word IDs.
    A key benefit of the text vectorization layer is that it is built as a part of
    the model, unlike the tokenizer we used before.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 RNN 解决了命名实体识别（NER）任务。在 NER 中，问题是根据给定的标记序列，为每个标记预测一个标签。该标签表示一个实体（例如组织、位置、人物等）。为此，我们使用了嵌入以及
    RNN 来处理每个标记，同时将标记序列视为时间序列输入。我们还使用了一个文本向量化层将标记转换为词 ID。文本向量化层的一个关键优势是它是模型的一部分，而不像我们之前使用的分词器那样单独存在。
- en: Finally, we looked at how we can adopt character embeddings and the convolution
    operation to generate token embeddings. We used these new token embeddings along
    with standard word embeddings to improve model accuracy.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了如何采用字符嵌入和卷积操作来生成标记嵌入。我们将这些新生成的标记嵌入与标准的词嵌入结合使用，以提高模型的准确性。
- en: In the next chapter, we will discuss a more powerful RNN model known as **Long
    Short-Term Memory** (**LSTM**) networks that further reduces the adverse effect
    of the vanishing gradient, and thus produces much better results.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论一种更强大的 RNN 模型——**长短时记忆**（**LSTM**）网络，它进一步减少了消失梯度的负面影响，从而产生更好的结果。
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本书的代码文件，请访问我们的 GitHub 页面：[https://packt.link/nlpgithub](https://packt.link/nlpgithub)
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，结识志同道合的人，并与超过 1000 名成员一起学习：[https://packt.link/nlp](https://packt.link/nlp)
- en: '![](img/QR_Code5143653472357468031.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code5143653472357468031.png)'
