- en: Chapter 4. Tagging Words and Tokens
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章. 词语和标记的标注
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下方法：
- en: Interesting phrase detection
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有趣短语检测
- en: Foreground- or background-driven interesting phrase detection
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前景或背景驱动的有趣短语检测
- en: Hidden Markov Models (HMM) – part-of-speech
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型（HMM）——词性标注
- en: N-best word tagging
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N最佳词标注
- en: Confidence-based tagging
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于置信度的标注
- en: Training word tagging
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练词标注
- en: Word-tagging evaluation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词语标注评估
- en: Conditional random fields (CRF) for word/token tagging
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件随机场（CRF）用于词/标记标注
- en: Modifying CRFs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改CRF
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: Words and tokens are the focus of this chapter. The more common extraction technologies,
    such as named entity recognition, are actually encoded into the concepts presented
    here, but this will have to wait until [Chapter 5](ch05.html "Chapter 5. Finding
    Spans in Text – Chunking"), *Finding Spans in Text – Chunking*. We will start
    easy with finding interesting sets of tokens. Then, we will move on to HMM and
    finish with one of the most complex components of LingPipe—CRF. As usual, we show
    you how to evaluate tagging and train your own taggers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是词语和标记。像命名实体识别这样的常见提取技术，实际上已经编码成了这里呈现的概念，但这需要等到[第5章](ch05.html "第5章. 在文本中找到跨度
    - Chunking")，*在文本中找到跨度 - Chunking*时才能讲解。我们将从简单的寻找有趣的标记集开始，然后转向隐马尔可夫模型（HMM），最后介绍LingPipe中最复杂的组件之一——条件随机场（CRF）。和往常一样，我们会向你展示如何评估标注并训练你自己的标注器。
- en: Interesting phrase detection
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有趣短语检测
- en: 'Imagine that a program can take a bunch of text data and automatically find
    the interesting parts, where "interesting" means that the word or phrase occurs
    more often than expected. It has a very nice property—no training data is needed,
    and it works for any language that we have tokens for. You have seen this most
    often in tag clouds such as the one in the following figure:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个程序能够自动从一堆文本数据中找到有趣的部分，其中“有趣”意味着某个词或短语出现的频率高于预期。它有一个非常好的特性——不需要训练数据，而且适用于我们有标记的任何语言。你最常在标签云中看到这种情况，如下图所示：
- en: '![Interesting phrase detection](img/4672OS_04_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![有趣短语检测](img/4672OS_04_01.jpg)'
- en: The preceding figure shows a tag cloud generated for the [lingpipe.com](http://lingpipe.com)
    home page. However, be aware that tag clouds are considered to be the "mullets
    of the Internet" as noted by Jeffery Zeldman in [http://www.zeldman.com/daily/0405d.shtml](http://www.zeldman.com/daily/0405d.shtml),
    so you will be on shaky ground if you deploy such a feature on a website.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了为[lingpipe.com](http://lingpipe.com)主页生成的标签云。然而，正如Jeffery Zeldman在[http://www.zeldman.com/daily/0405d.shtml](http://www.zeldman.com/daily/0405d.shtml)中指出的那样，标签云被认为是“互联网的穆雷发型”，因此如果你在网站上部署这样的功能，可能会站不住脚。
- en: How to do it...
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点……
- en: 'To get the interesting phrases from a small dataset with tweets about Disney,
    perform the following steps:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要从一个包含迪士尼推文的小数据集中提取有趣短语，请执行以下步骤：
- en: 'Fire up the command line and type:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动命令行并输入：
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The program should respond with something like:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 程序应该返回类似如下的结果：
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can also supply a `.csv` file in our standard format as an argument to see
    different data.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以提供一个`.csv`文件，按照我们的标准格式作为参数，以查看不同的数据。
- en: The output tends to be tantalizingly useless. Tantalizingly useless means that
    some useful phrases show up, but with a bunch of less interesting phrases that
    you will never want in your summary of what is interesting in the data. On the
    interesting side, we can see `Crayola Color`, `Lindsey Lohan`, `Episode VII`,
    and so on. On the junk side, we can see `ncipes azules`, `pictures releases`,
    and so on. There are lots of ways to address the junk output—the obvious first
    step will be to use a language ID classifier to throw out non-English.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 输出往往是令人既期待又无用的。所谓“既期待又无用”是指一些有用的短语出现了，但同时也有许多无趣的短语，这些短语你在总结数据中有趣的部分时根本不想要。在有趣的那一侧，我们能看到`Crayola
    Color`、`Lindsey Lohan`、`Episode VII`等。在垃圾短语的那一侧，我们看到`ncipes azules`、`pictures
    releases`等。解决垃圾输出有很多方法——最直接的一步是使用语言识别分类器将非英语的内容过滤掉。
- en: How it works...
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Here, we will go through the source in its entirety, broken by explanatory
    text:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将完整地浏览源代码，并通过解释性文字进行拆解：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, we see the path, imports, and the `main()` method. Our ternary operator
    that supplies a default file name or reads from the command line is the last line:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到路径、导入语句和`main()`方法。我们提供默认文件名或从命令行读取的三元运算符是最后一行：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After collecting the input data, the first interesting code constructs a tokenized
    language model which differs in significant ways from the character language models
    used in [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*.
    A tokenized language model operates over tokens created by `TokenizerFactory`,
    and the `ngram` parameter dictates the number of tokens used instead of the number
    of characters. A subtlety of `TokenizedLM` is that it can also use character language
    models to make predictions for tokens it has not seen before. See the *Foreground-
    or background-driven interesting phrase detection* recipe to understand how this
    works in practice; don''t use the preceding constructor unless there are no unknown
    tokens when estimating. Also, the relevant Javadoc provides more details on this.
    In the following code snippet, the language model is trained:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集输入数据后，第一个有趣的代码构建了一个标记化的语言模型，这与[第1章](ch01.html "第1章. 简单分类器")中使用的字符语言模型有显著不同，*简单分类器*。标记化语言模型操作的是由`TokenizerFactory`创建的标记，而`ngram`参数决定了使用的标记数，而不是字符数。`TokenizedLM`的一个微妙之处在于，它还可以使用字符语言模型来为它之前未见过的标记做出预测。请参见*前景或背景驱动的有趣短语检测*食谱，了解这一过程是如何在实践中运作的；除非在估算时没有未知标记，否则不要使用之前的构造器。此外，相关的
    Javadoc 提供了更多的细节。在以下代码片段中，语言模型被训练：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The next relevant step is the creation of collocations:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的相关步骤是创建搭配词：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The parameterization controls the phrase length in tokens; it also sets a minimum
    count of how often the phrase can be seen and how many phrases to return. We can
    look at phrases of length 3 as we have a language model that stores 3 grams. Next,
    we will visit the results:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化控制短语的长度（以标记为单位）；它还设置了短语出现的最小次数以及返回多少个短语。我们可以查看长度为 3 的短语，因为我们有一个存储 3-gram
    的语言模型。接下来，我们将查看结果：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `SortedSet<ScoredObject<String[]>>` collocation is sorted from a high score
    to a low score. The intuition behind the score is that a higher score is given
    when the tokens are seen together more than one would expect, given their singleton
    frequency in the training data. In other words, phrases are scored depending on
    how much they vary from the independence assumption based on the tokens. See the
    Javadoc at [http://alias-i.com/lingpipe/docs/api/com/aliasi/lm/TokenizedLM.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/lm/TokenizedLM.html)
    for the exact definitions—an interesting exercise will be to create your own score
    and compare it with what is done in LingPipe.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`SortedSet<ScoredObject<String[]>>` 搭配词按得分从高到低排序。得分的直观理解是，当标记的共现次数超过其在训练数据中单独出现的频率时，给予更高的得分。换句话说，短语的得分取决于它们如何偏离基于标记的独立假设。请参阅
    Javadoc [http://alias-i.com/lingpipe/docs/api/com/aliasi/lm/TokenizedLM.html](http://alias-i.com/lingpipe/docs/api/com/aliasi/lm/TokenizedLM.html)
    获取准确的定义——一个有趣的练习是创建你自己的得分系统，并与 LingPipe 中的做法进行比较。'
- en: There's more...
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'Given that this code is close to being usable on a website, it is worth discussing
    tuning. Tuning is the process of looking at system output and making changes based
    on the mistakes the system makes. Some changes that we would immediately consider
    include:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于此代码接近可在网站上使用，因此值得讨论调优。调优是查看系统输出并根据系统的错误做出修改的过程。一些我们会立即考虑的修改包括：
- en: A language ID classifier that will be handy to filter out non-English texts
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个语言 ID 分类器，方便用来过滤非英语文本
- en: Some thought around how to better tokenize the data
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思考如何更好地标记化数据
- en: Varying token lengths to include 3 grams and unigrams in the summary
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变标记长度，以便在摘要中包含 3-gram 和 unigram
- en: Using named entity recognition to highlight proper nouns
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用命名实体识别来突出专有名词
- en: Foreground- or background-driven interesting phrase detection
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前景或背景驱动的有趣短语检测
- en: 'Like the previous recipe, this recipe finds interesting phrases, but it uses
    another language model to determine what is interesting. Amazon''s statistically
    improbable phrases (**SIP**) work this way. You can get a clear view from their
    website at [http://www.amazon.com/gp/search-inside/sipshelp.html](http://www.amazon.com/gp/search-inside/sipshelp.html):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前的食谱一样，这个食谱也会找到有趣的短语，但它使用了另一种语言模型来判断什么是有趣的。亚马逊的统计不可能短语（**SIP**）就是这样运作的。你可以通过他们的官网[http://www.amazon.com/gp/search-inside/sipshelp.html](http://www.amazon.com/gp/search-inside/sipshelp.html)清晰了解：
- en: '*"Amazon.com''s Statistically Improbable Phrases, or "SIPs", are the most distinctive
    phrases in the text of books in the Search Inside!™ program. To identify SIPs,
    our computers scan the text of all books in the Search Inside! program. If they
    find a phrase that occurs a large number of times in a particular book relative
    to all Search Inside! books, that phrase is a SIP in that book.*'
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“亚马逊的统计学上不太可能出现的短语，或称为“SIPs”，是《搜索内容！™》项目中书籍文本中最具辨识度的短语。为了识别SIPs，我们的计算机扫描所有《搜索内容！》项目中的书籍文本。如果它们发现某个短语在某本书中相对于所有《搜索内容！》书籍出现的频率很高，那么该短语就是该书中的SIP。”*'
- en: ''
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: SIPs are not necessarily improbable within a particular book, but they are improbable
    relative to all books in Search Inside!."
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: SIPs在某本书中不一定是不太可能的，但相对于《搜索内容！》中的所有书籍，它们是不太可能的。
- en: The foreground model will be the book being processed, and the background model
    will be all the other books in Amazon's Search Inside!™ program. While Amazon
    has probably introduced tweaks that differ, it is the same basic idea.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 前景模型将是正在处理的书籍，而背景模型将是亚马逊《搜索内容！™》项目中的所有其他书籍。虽然亚马逊可能已经引入了一些不同的调整，但基本理念是相同的。
- en: Getting ready
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'There are a few sources of data worth looking at to get interesting phrases
    with two separate language models. The key is you want the background model to
    function as the source of expected word/phrase distributions that will help highlight
    interesting phrases in the foreground model. Some examples include:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个数据源值得查看，以便通过两个独立的语言模型得到有趣的短语。关键在于，你希望背景模型作为预期单词/短语分布的来源，帮助突出前景模型中的有趣短语。一些示例包括：
- en: '**Time-separated Twitter data**: The examples of time-separated Twitter data
    are as follows:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间分隔的推特数据**：时间分隔的推特数据示例如下：'
- en: '**Background model**: This refers to a year worth of tweets about Disney World
    up to yesterday.'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**背景模型**：这指的是直到昨天关于迪士尼世界的一整年的推文。'
- en: '**Foreground model**: Tweets for today.'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前景模型**：今天的推文。'
- en: '**Interesting phrases**: What''s new in Disney World today on Twitter.'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有趣的短语**：今天关于迪士尼世界在推特上的新内容。'
- en: '**Topic-separated Twitter data**: The examples of topic-separated Twitter data
    are as follows:'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**话题分隔的推特数据**：话题分隔的推特数据示例如下：'
- en: '**Background model**: Tweets about Disneyland'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**背景模型**：关于迪士尼乐园的推文'
- en: '**Foreground model**: Tweets about Disney World'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前景模型**：关于迪士尼世界的推文'
- en: '**Interesting phrases**: What is said about Disney World that is not said about
    Disneyland'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有趣的短语**：关于迪士尼世界说的而不是关于迪士尼乐园说的'
- en: '**Books on very similar topics**: The examples of books on similar topics are
    as follows:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似主题的书籍**：关于相似主题的书籍示例如下：'
- en: '**Background model**: A pile of early sci-fi novels'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**背景模型**：一堆早期的科幻小说'
- en: '**Foreground model**: Jules Verne''s *War of the Worlds*'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前景模型**：儒勒·凡尔纳的*世界大战*'
- en: '**Interesting phrases**: The unique phrases and concepts of "War of the Worlds"'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有趣的短语**：《世界大战》的独特短语和概念'
- en: How to do it...
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'Here are the steps to run a foreground or background model on tweets about
    Disneyland versus tweets about Disney World:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是运行一个前景或背景模型来处理关于迪士尼乐园与迪士尼世界推文的步骤：
- en: 'In the command line, type:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行中输入：
- en: '[PRE7]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output will look something like:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将类似于：
- en: '[PRE8]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The foreground model consists of tweets for the search term, `disneyland`, and
    the background model consists of tweets for the search term, `disneyworld`.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前景模型包括关于搜索词`disneyland`的推文，背景模型包括关于搜索词`disneyworld`的推文。
- en: The top tied results are for unique features of California-based Disneyland,
    namely, the name of the castle, Sleeping Beauty's Castle, and a theme park built
    in the parking lot of Disneyland, California Adventure.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 排名前列的结果是关于加利福尼亚州迪士尼乐园独特的特征，特别是城堡的名字——睡美人城堡，以及在迪士尼乐园停车场建造的主题公园——加州冒险乐园。
- en: The next bigram is for *Winter Dreams*, which refers to a premier for a film.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个二元组是关于*冬季梦想*，它指的是一部电影的首映。
- en: Overall, not a bad output to distinguish between the tweets of the two resorts.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 总体而言，输出效果不错，可以区分这两家度假村的推文。
- en: How it works...
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'The code is in `src/com/lingpipe/cookbook/chapter4/InterestingPhrasesForegroundBackground.java`.
    The exposition starts after we loaded the raw `.csv` data for the foreground and
    background models:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 代码位于`src/com/lingpipe/cookbook/chapter4/InterestingPhrasesForegroundBackground.java`。当我们加载前景和背景模型的原始`.csv`数据后，展示内容开始：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'One can be excused for wondering why we dedicated all of [Chapter 2](ch02.html
    "Chapter 2. Finding and Working with Words"), *Finding and Working with Words*,
    to tokenization, but it turns out that most NLP systems are very sensitive to
    how the character stream is broken up into words or tokens. In the preceding code
    snippet, we saw a stack of three tokenizer factories doing useful violence to
    the character sequence. The first two are covered adequately in [Chapter 2](ch02.html
    "Chapter 2. Finding and Working with Words"), *Finding and Working with Words*,
    but the third one is a customized factory that bears some examination. The intent
    behind the `LengthFilterTokenizerFactoryPreserveToken` class is to filter short
    tokens but at the same time not lose adjacency information. The goal is to take
    the phrase, "Disney is my favorite resort", and produce tokens (`disney`, `_234`,
    `_235`, `favorite`, `resort`), because we don''t want short words in our interesting
    phrases—they tend to sneak past simple statistical models and mess up the output.
    Please refer to `src/come/lingpipe/cookbook/chapter4/LengthFilterTokenizerFactoryPreserveToken.java`
    for the source of the third tokenizer. Also, refer to [Chapter 2](ch02.html "Chapter 2. Finding
    and Working with Words"), *Finding and Working with Words* for exposition. Next
    is the background model:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可以理解为什么我们把[第二章](ch02.html "第二章. 查找和使用单词")，*查找和使用单词*，完全用来讨论标记化，但是事实证明，大多数NLP系统对于字符流如何被拆分成单词或标记非常敏感。在前面的代码片段中，我们看到三个标记化工厂对字符序列进行有效的破坏。前两个在[第二章](ch02.html
    "第二章. 查找和使用单词")，*查找和使用单词*中已经得到了充分的介绍，但第三个是一个自定义工厂，需要仔细检查。`LengthFilterTokenizerFactoryPreserveToken`类的目的在于过滤短标记，同时不丢失相邻信息。目标是处理短语"Disney
    is my favorite resort"，并生成标记（`disney`, `_234`, `_235`, `favorite`, `resort`），因为我们不希望在有趣的短语中出现短单词——它们往往能轻易通过简单的统计模型，并破坏输出。有关第三个标记器的源代码，请参见`src/come/lingpipe/cookbook/chapter4/LengthFilterTokenizerFactoryPreserveToken.java`。此外，请参阅[第二章](ch02.html
    "第二章. 查找和使用单词")，*查找和使用单词*以了解更多说明。接下来是背景模型：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'What is being built here is the model that is used to judge the novelty of
    phrases in the foreground model. Then, we will create and train the foreground
    model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里构建的是用于判断前景模型中短语新颖性的模型。然后，我们将创建并训练前景模型：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we will access the `newTermSet()` method from the foreground model. The
    parameters and `phraseSize` determine how long the token sequences are; `minCount`
    specifies a minimum number of instances of the phrase to be considered, and `maxReturned`
    controls how many results to return:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从前景模型中访问`newTermSet()`方法。参数和`phraseSize`决定了标记序列的长度；`minCount`指定要考虑的短语的最小出现次数，`maxReturned`控制返回多少结果：
- en: '[PRE12]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding `for` loop prints out the phrases in order of the most surprising
    phrase to the least surprising one.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的`for`循环按最令人惊讶到最不令人惊讶的短语顺序打印出短语。
- en: The details of what is going on here are beyond the scope of the recipe, but
    the Javadoc again starts us down the road to enlightenment.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生的细节超出了食谱的范围，但Javadoc再次引导我们走向启蒙之路。
- en: The exact scoring used is the z-score, as defined in `BinomialDistribution.z(double,int,int)`,
    with the success probability defined by the n-grams probability estimate in the
    background model, the number of successes being the count of the n-gram in this
    model, and the number of trials being the total count in this model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的确切评分是z-score，如`BinomialDistribution.z(double, int, int)`中定义的那样，其中成功概率由背景模型中的n-gram概率估计定义，成功的次数是该模型中n-gram的计数，试验次数是该模型中的总计数。
- en: There's more...
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'This recipe is the first place where we have faced unknown tokens, which can
    have very bad properties if not handled correctly. It is easy to see why this
    is a problem with a maximum likelihood of a token-based language model, which
    is a fancy name for a language model that provides an estimate of some unseen
    tokens by multiplying the likelihoods of each token. Each likelihood is the number
    of times the token was seen in training divided by the number of tokens seen in
    data. For example, consider training on the following data from *A Connecticut
    Yankee in King Arthur''s Court*:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱是我们第一次遇到未知标记的地方，如果处理不当，它们可能具有非常不好的属性。很容易理解为什么这对于基于标记的语言模型的最大似然估计来说是个问题，这是一种通过将每个标记的似然性相乘来估计一些未见标记的语言模型的花哨名称。每个似然性是标记在训练中出现的次数除以数据中出现的标记总数。例如，考虑使用来自*《康涅狄格州的亚瑟王》*的数据进行训练：
- en: '*"The ungentle laws and customs touched upon in this tale are historical, and
    the episodes which are used to illustrate them are also historical."*'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“这个故事中提到的冷酷的法律和习俗是历史性的，用来说明它们的事件也是历史性的。”*'
- en: This is very little training data, but it is sufficient for the point being
    made. Consider how we will get an estimate for the phrase, "The ungentle inlaws"
    using our language model. There are 24 words with "The" occurring once; we will
    assign a probability of 1/24 to this. We will assign a probability of 1/24 to
    "ungentle" as well. If we stop here, we can say that the likelihood of "The ungentle"
    is 1/24 * 1/24\. However, the next word is "inlaws", which does not exist in the
    training data. If this token is assigned a value of 0/24, it will make the likelihood
    of the entire string 0 (1/24 * 1/24 * 0/20). This means that whenever there is
    an unseen token for which the estimate is likely going to be zero, this is generally
    an unhelpful property.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常少的训练数据，但足以证明所提的观点。考虑一下我们如何通过语言模型来估计短语“The ungentle inlaws”。在训练数据中，“The”出现一次，共有24个单词；我们将给它分配1/24的概率。我们也将给“ungentle”分配1/24的概率。如果我们在这里停止，可以说“The
    ungentle”的概率是1/24 * 1/24。但是，下一个单词是“inlaws”，它在训练数据中不存在。如果这个词元被赋予0/24的值，那么整个字符串的可能性将变为0（1/24
    * 1/24 * 0/20）。这意味着每当有一个未见的词元，且其估计值可能为零时，这通常是一个无用的特性。
- en: 'The standard response to this issue is to substitute and approximate the value
    to stand in for data that has not been seen in training. There are a few approaches
    to solving this problem:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的标准方法是替代并近似未在训练中看到的数据的值。解决此问题有几种方法：
- en: Provide a low but non-zero estimate for unknown tokens. This is a very common
    approach.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为未知词元提供一个低但非零的估计。这是一种非常常见的方法。
- en: Use character language models with the unknown token. There are provisions for
    this in the class—refer to the Javadoc.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用字符语言模型与未知词元。这在类中有相关的规定——请参考Javadoc。
- en: There are lots of other approaches and substantial research literature. Good
    search terms are "back off" and "smoothing".
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有许多其他方法和大量的研究文献。好的搜索词是“back off”和“smoothing”。
- en: Hidden Markov Models (HMM) – part-of-speech
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐马尔可夫模型（HMM）——词性
- en: This recipe brings in the first hard-core linguistic capability of LingPipe;
    it refers to the grammatical category for words or **part-of-speech** (**POS**).
    What are the verbs, nouns, adjectives, and so on in text?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方引入了LingPipe的第一个核心语言学功能；它指的是单词的语法类别或**词性**（**POS**）。文本中的动词、名词、形容词等是什么？
- en: How to do it...
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s jump right in and drag ourselves back to those awkward middle-school
    years in English class or our equivalent:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接进入，回到那些尴尬的中学英语课堂时光，或者是我们相应的经历：
- en: 'As always, head over to your friendly command prompt and type the following:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像往常一样，去你的命令提示符并键入以下内容：
- en: '[PRE13]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The system will respond with a prompt to which we will add a Jorge Luis Borges
    quote:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统将响应一个提示，我们将在其中添加一条豪尔赫·路易斯·博尔赫斯的引用：
- en: '[PRE14]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The system will respond delightfully to this quote with:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 系统将愉快地响应这个引用：
- en: '[PRE15]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Appended to each token is `_` with a part-of-speech tag; `nn` is noun, `rb`
    is adverb, and so on. The complete tag set and description of the corpus of the
    tagger can be found at [http://en.wikipedia.org/wiki/Brown_Corpus](http://en.wikipedia.org/wiki/Brown_Corpus).
    Play around with it a bit. POS tagger was one of the first breakthrough machine-learning
    applications in NLP back in the '90s. You can expect this one to perform at better
    than 90-percent accuracy, although it might suffer a bit on Twitter data given
    that that the underlying corpus was collected in 1961.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每个词元后附加有一个`_`和一个词性标签；`nn`是名词，`rb`是副词，等等。完整的标签集和标注器语料库的描述可以在[http://en.wikipedia.org/wiki/Brown_Corpus](http://en.wikipedia.org/wiki/Brown_Corpus)找到。多玩玩这个。词性标注器是90年代NLP领域最早的突破性机器学习应用之一。你可以期待它的表现精度超过90%，尽管它在Twitter数据上可能会有点问题，因为底层语料库是1961年收集的。
- en: How it works...
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'As appropriate for a recipe book, we are not revealing the fundamentals of
    how part-of-speech taggers are built. There is Javadoc, the Web, and the research
    literature to help you understand the underlying technology—in the recipe for
    training an HMM, there is a brief discussion of the underlying HMM. This is about
    how to use the API as presented:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 适合食谱书的方式是，我们并未透露如何构建词性标注器的基础知识。可以通过Javadoc、Web以及研究文献来帮助你理解底层技术——在训练HMM的配方中，简要讨论了底层HMM。这是关于如何使用呈现的API：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The code starts by setting up `TokenizerFactory`, which makes sense because
    we need to know what the words that are going to get the parts of speech are.
    The next line reads in a previously trained part-of-speech tagger as `HiddenMarkovModel`.
    We will not go into too much detail; you just need to know that an HMM assigns
    a part-of-speech tag for token *n* as a function of the tag assignments that preceded
    it.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先设置 `TokenizerFactory`，这很有意义，因为我们需要知道哪些词将会得到词性标注。接下来的一行读取了一个之前训练过的词性标注器，作为
    `HiddenMarkovModel`。我们不会深入讨论细节；你只需要知道 HMM 将词标记 *n* 的词性标记视为先前标注的函数。
- en: The fact that these tags are not directly observed in data makes the Markov
    model hidden. Usually, one or two tokens back are looked at. There is a lot going
    on with HMMs that is worth understanding.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些标签在数据中并不是直接观察到的，这使得马尔可夫模型成为隐含的。通常，回看一两个标记。隐马尔可夫模型（HMM）中有许多值得理解的内容。
- en: 'The next line with HmmDecoder decoder wraps the HMM in code to tag provided
    tokens. Our standard interactive `while` loop is up next with all the interesting
    bits happening in the firstBest(tokenList,decoder) method at the end. The method
    is as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行的 `HmmDecoder decoder` 将 HMM 包装到代码中，用于标注提供的标记。接下来的标准交互式 `while` 循环将进入 `firstBest(tokenList,
    decoder)` 方法，并且所有有趣的内容都发生在方法的结尾。该方法如下：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note the `decoder.tag(tokenList)` call that produces a `Tagging<String>` tagging.
    Tagging does not have an iterator or useful encapsulation of the tag/token pair,
    so the information is accessed by incrementing an index i.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意 `decoder.tag(tokenList)` 调用，它会产生一个 `Tagging<String>` 标注。Tagging 没有迭代器或有用的标签/标记对封装，因此需要通过递增索引
    i 来访问信息。
- en: N-best word tagging
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: N-best 单词标注
- en: The certainty-driven nature of Computer Science is not reflected in the vagaries
    of linguistics where reasonable PhDs can agree or disagree at least until Chomsky's
    henchmen show up. This recipe uses the same HMM trained in the preceding recipe
    but provides a ranked list of possible tags for each word.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学的确定性驱动特性并未体现在语言学的变数上，合理的博士们至少可以同意或不同意，直到乔姆斯基的亲信出现为止。本配方使用了在前一配方中训练的相同 HMM，但为每个单词提供了可能标签的排名列表。
- en: Where might this be helpful? Imaging a search engine that searched for words
    and a tag—not necessarily part-of-speech. The search engine can index the word
    and the top *n*-best tags that will allow a match into a non-first best tag. This
    can help increase recall.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这在什么情况下可能有帮助？想象一个搜索引擎，它不仅搜索单词，还搜索标签——不一定是词性。这个搜索引擎可以索引单词以及最优的 *n* 个标签，这些标签可以让匹配的标签进入非首选标签。这可以帮助提高召回率。
- en: How to do it...
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: '`N`-best analyses push the sophistication boundaries of NLP developers. What
    used to be a singleton is now a ranked list, but it is where the next level of
    performance occurs. Let''s get started by performing the following steps:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`N`-best 分析推动了 NLP 开发者的技术边界。曾经是单一的，现在是一个排名列表，但它是性能提升的下一阶段。让我们开始执行以下步骤：'
- en: 'Put away your copy of *Syntactic Structures* face down and type out the following:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 把你那本《句法结构》放好，翻过来并键入以下内容：
- en: '[PRE18]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, enter the following:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，输入以下内容：
- en: '[PRE19]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It yields the following output:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将输出以下内容：
- en: '[PRE20]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The output lists from most likely to least likely the estimate of the entire
    sequence of tokens given the estimates from the HMM. Remember that the joint probabilities
    are log 2 based. To compare joint probabilities subtract -93.9 from -91.1 for
    a difference of 2.8\. So, the tagger thinks that option 1 is 2 ^ 2.8 = 7 times
    less likely to occur than option 0\. The source of this difference is in assigning
    green to noun rather than adjective.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 输出列表按从最可能到最不可能的顺序列出整个标记序列的估计，基于 HMM 的估计。记住，联合概率是以对数 2 为基数的。为了比较联合概率，将 -93.9
    从 -91.1 中减去，差值为 2.8。因此，标注器认为选项 1 的出现几率是选项 0 的 2 ^ 2.8 = 7 倍小。这个差异的来源在于将名词标记为绿色，而不是形容词。
- en: How it works...
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'The code to load the model and command I/O is the same as that of the previous
    recipe. The difference is in the method used to get and display the tagging:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 加载模型和命令输入输出的代码与之前的配方相同。不同之处在于获取和显示标注所使用的方法：
- en: '[PRE21]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: There is nothing much to it other than working out the formatting issues as
    the taggings are being iterated over.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在标注迭代过程中解决格式化问题外，没有太多复杂的内容。
- en: Confidence-based tagging
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于置信度的标注
- en: There is another view into the tagging probabilities; this reflects the probability
    assignments at the level of word. The code reflects the underlying `TagLattice`
    and offers insights into whether the tagger is confident or not.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个视图展示了标注概率，这反映了在单词级别的概率分配。代码反映了底层的`TagLattice`，并提供了对标注器是否有信心的洞察。
- en: How to do it...
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'This recipe will focus the probability estimates on the individual token. Perform
    the following steps:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将把概率估计集中在单个标记上。请执行以下步骤：
- en: 'Type in the following on the command line or IDE equivalent:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行或IDE中键入以下内容：
- en: '[PRE22]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, enter the following:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，输入以下内容：
- en: '[PRE23]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It yields the following output:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它会生成以下输出：
- en: '[PRE24]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This view of the data distributes the joint probabilities of the tag and word.
    We can see that there is `.208` chance that `green` should be tagged as `nn` or
    a singular noun, but the correct analysis is still `.788` with adjective `jj`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据视图分配了标签和词的联合概率。我们可以看到`green`有`.208`的概率应该被标记为`nn`（名词单数），但正确的分析仍然是`.788`，标记为形容词`jj`。
- en: How it works…
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'We are still using the same old HMM from the *Hidden Markov Models (HMM) –
    part-of-speech* recipe but using different parts of it. The code to read in the
    model is exactly the same, with a major difference in how we report results. `src/com/lingpipe/cookbook/chapter4/ConfidenceBasedTagger.java`
    the method:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然使用的是*隐藏马尔可夫模型（HMM）——词性*食谱中的旧HMM，但使用了不同的部分。读取模型的代码完全相同，主要的区别在于我们报告结果的方式。`src/com/lingpipe/cookbook/chapter4/ConfidenceBasedTagger.java`中的方法：
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The method demonstrates the underlying lattice of tokens to the probabilities
    explicitly, which is at the heart of the HMM. Change the termination condition
    on the `for` loop to see more or fewer tags.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法明确演示了标记的底层格子到概率的映射，这就是HMM的核心。更改`for`循环的终止条件，以查看更多或更少的标签。
- en: Training word tagging
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练词性标注
- en: Word tagging gets much more interesting when you can create your own models.
    The realm of annotating part-of-speech tagging corpora is a bit too much for a
    mere recipe book—annotation of the part-of-speech data is very difficult because
    it requires considerable linguistic knowledge to do well. This recipe will directly
    address the machine-learning component of the HMM-based sentence detector.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当你可以创建自己的模型时，词性标注变得更加有趣。注释词性标注语料库的领域对于一本简单的食谱书来说有些过于复杂——词性数据的注释非常困难，因为它需要相当多的语言学知识才能做得好。本食谱将直接解决基于HMM的句子检测器的机器学习部分。
- en: As this is a recipe book, we will minimally explain what an HMM is. The token
    language models that we have been working with do their previous context calculations
    on some number of words/tokens that precede the current word being estimated.
    HMMs take into account some length of the previous tags while calculating estimates
    for the current token's tag. This allows for seemingly disparate neighbors such
    as `of` and `in` to be similar, because they are both prepositions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一本食谱书，我们将简单解释一下什么是HMM。我们一直在使用的标记语言模型会根据当前估计词汇前面的一些词/标记来进行前文上下文计算。HMM在计算当前标记的标签估计时，会考虑前面标签的一些长度。这使得看似不同的邻接词，如`of`和`in`，变得相似，因为它们都是介词。
- en: In the *Sentence detection* recipe, from [Chapter 5](ch05.html "Chapter 5. Finding
    Spans in Text – Chunking"), *Finding Spans in Text – Chunking*, a useful but not
    very flexible sentence detector is based on the `HeuristicSentenceModel` in LingPipe.
    Rather than mucking about with modifying/extending the `HeuristicSentenceModel`,
    we will build a machine-learning-based sentence system with the data that we annotate.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在*句子检测*食谱中，来自[第5章](ch05.html "第5章。文本中的跨度 – 分块")，*文本中的跨度 – 分块*，基于`HeuristicSentenceModel`的句子检测器虽然有用，但灵活性不强。与其修改/扩展`HeuristicSentenceModel`，我们将基于我们注释的数据构建一个基于机器学习的句子系统。
- en: How to do it...
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The steps here describe how to run the program in `src/com/lingpipe/cookbook/chapter4/HMMTrainer.java`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的步骤描述了如何运行`src/com/lingpipe/cookbook/chapter4/HMMTrainer.java`中的程序：
- en: 'Either create a new corpus of the sentence-annotated data or use the following
    default data, which is in `data/connecticut_yankee_EOS.txt`. If you are rolling
    your own data, simply edit some text with `['' and '']` to mark the sentence boundaries.
    Our example looks like the following:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以创建一个新的句子注释数据集，或使用以下默认数据，该数据位于`data/connecticut_yankee_EOS.txt`。如果你自己处理数据，只需编辑一些文本，并用`[`和`]`标记句子边界。我们的示例如下：
- en: '[PRE26]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Go to the command prompt and run the program with the following command:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开命令提示符并运行以下命令启动程序：
- en: '[PRE27]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'It will give the following output:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它将输出如下内容：
- en: '[PRE28]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is a tokenized text with one of the three tags: `BOS` for the beginning
    of a sentence, `EOS` for the end of a sentence, and `WORD` for all other tokens.'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出是一个标记化文本，包含三种标签之一：`BOS`表示句子的开始，`EOS`表示句子的结束，`WORD`表示所有其他的标记。
- en: How it works…
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Like many span-based markups, the `span` annotation is translated into a token-level
    annotation, as shown earlier in the recipe''s output. So, the first order of business
    is to collect the annotated text, set up `TokenizerFactory`, and then call a parsing
    subroutine to add to `List<Tagging<String>>`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多基于跨度的标记一样，`span`注解被转换为标记级别的注解，如配方输出中所示。因此，首先的任务是收集注解文本，设置`TokenizerFactory`，然后调用一个解析子程序将其添加到`List<Tagging<String>>`中：
- en: '[PRE29]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The subroutine to parse the preceding format works by first tokenizing the
    text with `IndoEuropeanTokenizer`, which has the desirable property of treating
    the `['' and '']` sentence delimiters as separate tokens. It does not check whether
    the sentence delimiters are well formed—a more robust solution will be needed
    to do this. The tricky bit is that we want to ignore this markup in the resulting
    token stream, but we want to use it to have the token following `['' be a BOS
    and the token preceding '']` be `EOS`. Other tokens are just `WORD`. The subroutine
    builds a parallel `Lists<String>` instance for tags and tokens, which is then
    used to create `Tagging<String>` and is added to `taggingList`. The tokenization
    recipes in [Chapter 2](ch02.html "Chapter 2. Finding and Working with Words"),
    *Finding and Working with Words*, cover what is going on with the tokenizer. Have
    a look at the following code snippet:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 解析前述格式的子程序首先通过`IndoEuropeanTokenizer`对文本进行标记化，这个标记化器的优点是将`[`和`]`作为独立的标记处理。它不检查句子分隔符是否格式正确——一个更健壮的解决方案将需要做这件事。难点在于，我们希望在生成的标记流中忽略这些标记，但又希望使用它来使得`[`后面的标记为BOS，而`]`前面的标记为EOS。其他标记只是`WORD`。该子程序构建了一个并行的`Lists<String>`实例来存储标记和标记词，然后用它创建`Tagging<String>`并将其添加到`taggingList`中。[第2章](ch02.html
    "第2章：查找和处理单词")中的标记化配方，*查找和处理单词*，涵盖了标记化器的工作原理。请看下面的代码片段：
- en: '[PRE30]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: There is a subtlety with the preceding code. The training data is treated as
    a single tagging—this will emulate what the input will look like when we use the
    sentence detector on novel data. If more than one document/chapter/paragraph is
    being used for training, then we will call this subroutine for each block of text.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码有一个微妙之处。训练数据被视为单一的标记——这将模拟当我们在新数据上使用句子检测器时，输入的样子。如果训练中使用了多个文档/章节/段落，那么我们将针对每一块文本调用这个子程序。
- en: 'Returning to the `main()` method, we will set up `ListCorpus` and add the tagging
    to the training side of the corpus, one tagging at a time. There is an `addTest()`
    method as well, but this recipe is not concerned with evaluation; if it was, we
    would most likely use `XValidatingCorpus` anyway:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到`main()`方法，我们将设置`ListCorpus`并逐一将标记添加到语料库的训练部分。也有`addTest()`方法，但本例不涉及评估；如果涉及评估，我们很可能会使用`XValidatingCorpus`：
- en: '[PRE31]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we will create `HmmCharLmEstimator`, which is our HMM. Note that there
    are constructors that allow for customized parameters that affect performance—see
    the Javadoc. Next, the estimator is trained against the corpus, and `HmmDecoder`
    is created, which will actually tag tokens, as shown in the following code snippet:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建`HmmCharLmEstimator`，这就是我们的HMM。请注意，有一些构造函数允许定制参数来影响性能——请参见Javadoc。接下来，估算器将针对语料库进行训练，创建`HmmDecoder`，它将实际标记标记，如下面的代码片段所示：
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the following code snippet, our standard I/O loop gets invoked for some user
    feedback. Once we get some text from the user, it is tokenized with the same tokenizer
    we used for training, and the decoder is presented with the resulting tokens.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们的标准I/O循环会被调用以获取一些用户反馈。一旦我们从用户那获得一些文本，它将通过我们用于训练的相同标记器进行标记化，并且解码器将展示生成的标记。
- en: 'Note that there is no requirement that the training tokenizer be the same as
    the production tokenizer, but one must be careful to not tokenize in a radically
    different way; otherwise, the HMM will not be seeing the tokens it was trained
    with. The back-off model will then be used, which will likely degrade performance.
    Have a look at the following code snippet:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，训练分词器不必与生产分词器相同，但必须小心不要以完全不同的方式进行分词；否则，HMM 将无法看到它训练时使用的标记。接着会使用回退模型，这可能会降低性能。看一下以下的代码片段：
- en: '[PRE33]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: That's it! To truly wrap this as a proper sentence detector, we will need to
    map back to the character offsets in the original text, but this is covered in
    [Chapter 5](ch05.html "Chapter 5. Finding Spans in Text – Chunking"), *Finding
    Spans in Text – Chunking*. This is sufficient to show how to work with HMMs. A
    more full-featured approach will make sure that each BOS has a matching EOS and
    the other way around. The HMM has no such requirement.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！为了真正将其封装成一个合适的句子检测器，我们需要将其映射回原始文本中的字符偏移量，但这部分在[第5章](ch05.html "第5章. 在文本中查找跨度——分块")，*在文本中查找跨度——分块*中有讲解。这足以展示如何使用
    HMM。一个更完备的方法将确保每个BOS都有一个匹配的EOS，反之亦然。而 HMM 并没有这样的要求。
- en: There's more…
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'We have a small and easy-to-use corpus of the part-of-speech tags; this allows
    us to show how training the HMM for a very different problem works out to be the
    same thing. It is like our *How to classify a sentiment – simple version* recipe,
    in [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*;
    the only difference between the language ID and sentiment is the training data.
    We will start with a hard-coded corpus for simplicity—it is in `src/com/lingpipe/cookbook/chapter4/TinyPosCorus.java`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个小型且易于使用的词性标注语料库；这使我们能够展示如何将 HMM 的训练应用于一个完全不同的问题，并得出相同的结果。这就像我们的*如何分类情感——简单版*的食谱，在[第1章](ch01.html
    "第1章. 简单分类器")，*简单分类器*；语言识别和情感分类之间唯一的区别是训练数据。为了简单起见，我们将从一个硬编码的语料库开始——它位于`src/com/lingpipe/cookbook/chapter4/TinyPosCorus.java`：
- en: '[PRE34]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The corpus manually creates tokens as well as the tags for the tokens in the
    static `WORDS_TAGS` and creates `Tagging<String>` for each sentence; `Tagging<String>`
    consists of two aligned `List<String>` instances in this case. The taggings are
    then sent to the `handle()` method for the `Corpus` superclass. Swapping in this
    corpus looks like the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库手动创建了标记和静态`WORDS_TAGS`中标记的每个词的标签，并为每个句子创建了`Tagging<String>`；在这种情况下，`Tagging<String>`由两个对齐的`List<String>`实例组成。然后，这些标注被发送到`Corpus`超类的`handle()`方法。替换这个语料库看起来像这样：
- en: '[PRE35]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We just commented out the code that loads the corpus with sentence detection
    and features in `TinyPosCorpus` in its place. It doesn''t need data to be added
    so we will just train the HMM with it. To avoid confusion we have created a separate
    class `HmmTrainerPos.java`. Running it results in the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅仅注释掉了加载带有句子检测和特征的语料库的代码，并将`TinyPosCorpus`替换进去。它不需要添加数据，所以我们只需使用它来训练 HMM。为了避免混淆，我们创建了一个单独的类`HmmTrainerPos.java`。运行它将得到以下结果：
- en: '[PRE36]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The only mistake is that `in` is a transitive verb `TV`. The training data is
    very small so mistakes are to be expected. Like the difference in language ID
    and sentiment classification in [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"),
    *Simple Classifiers*, the HMM is used to learn a very different phenomenon just
    by changing what the training data is.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的错误是`in`是一个及物动词`TV`。训练数据非常小，因此错误是可以预期的。就像[第1章](ch01.html "第1章. 简单分类器")，*简单分类器*中语言识别和情感分类的区别一样，通过仅仅改变训练数据，HMM
    用来学习一个非常不同的现象。
- en: Word-tagging evaluation
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词标注评估
- en: Word tagging evaluation drives developments in downstream technologies such
    as named entity detection, which, in turn, drives high-end applications such as
    coreference. You will notice that much of the evaluation resembles the evaluation
    from our classifiers except that each tag is evaluated like its own classifier
    category.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 词标注评估推动了下游技术的发展，比如命名实体识别，而这些技术又推动了如共指消解等高端应用。你会注意到，大部分评估与我们分类器的评估相似，唯一的不同是每个标签都像自己的分类器类别一样被评估。
- en: This recipe should serve to get you started on evaluation, but be aware that
    there is a very good tutorial on tagging evaluation on our website at [http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html);
    this recipe goes into greater detail on how to best understand tagger performance.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱应能帮助你开始进行评估，但请注意，我们网站上有一个关于标注评估的非常好的教程，地址是[http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html)；这个食谱更详细地介绍了如何最佳地理解标注器的表现。
- en: This recipe is short and easy to use, so you have no excuses to not evaluate
    your tagger.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱简短且易于使用，因此你没有理由不去评估你的标注器。
- en: Getting ready
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'The following is the class source for our evaluator located at `src/com/lingpipe/cookbook/chapter4/TagEvaluator.java`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们评估器的类源代码，位于`src/com/lingpipe/cookbook/chapter4/TagEvaluator.java`：
- en: '[PRE37]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: How to do it…
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'We will call out the interesting bits of the preceding code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将指出前面代码中的有趣部分：
- en: 'First off, we will set up `TaggerEvaluator` with a null `HmmDecoder` and `boolean`
    that controls whether the tokens are stored or not. The `HmmDecoder` object will
    be set in the cross-validation code later in the code:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将设置`TaggerEvaluator`，其包含一个空的`HmmDecoder`和一个控制是否存储标记的`boolean`。`HmmDecoder`对象将在后续代码的交叉验证代码中设置：
- en: '[PRE38]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we will load `TinyPosCorpus` from the previous recipe and use it to populate
    `XValididatingObjectCorpus`—a pretty neat trick that allows for easy conversion
    between corpus types. Note that we pick 10 folds—the corpus only has 11 training
    examples, so we want to maximize the amount of training data per fold. See the
    *How to train and evaluate with cross validation* recipe in [Chapter 1](ch01.html
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*, if you are new to this
    concept. Have a look at the following code snippet:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载前一个食谱中的`TinyPosCorpus`并使用它填充`XValididatingObjectCorpus`——这是一种非常巧妙的技巧，允许在语料库类型之间轻松转换。注意，我们选择了10折——语料库只有11个训练示例，因此我们希望最大化每个折叠中的训练数据量。如果你是这个概念的新手，请查看[第1章](ch01.html
    "第1章. 简单分类器")，*简单分类器*中的*如何进行训练和交叉验证评估*食谱。请查看以下代码片段：
- en: '[PRE39]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The following code snippet is a `for()` loop that iterates over the number
    of folds. The first half of the loop handles training:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下代码片段是一个`for()`循环，它迭代折叠的数量。循环的前半部分处理训练：
- en: '[PRE40]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The rest of the loop first creates a decoder for the HMM, sets the evaluator
    to use this decoder, and then applies the appropriately configured evaluator to
    the test portion of the corpus:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环的其余部分首先为HMM创建解码器，将评估器设置为使用该解码器，然后将适当配置的评估器应用于语料库的测试部分：
- en: '[PRE41]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The last lines apply after all folds of the corpus have been used for training
    and testing. Notice that the evaluator is `BaseClassifierEvaluator`! It reports
    on each tag as a category:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后的几行代码应用于所有折叠的语料库已用于训练和测试后。注意，评估器是`BaseClassifierEvaluator`！它将每个标签作为一个类别报告：
- en: '[PRE42]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Brace yourself for the torrent of evaluation. The following is a small bit
    of it, namely, the confusion matrix that you should be familiar with from [Chapter
    1](ch01.html "Chapter 1. Simple Classifiers"), *Simple Classifiers*:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为评估的洪流做好准备。以下是其中的一小部分，即你应该从[第1章](ch01.html "第1章. 简单分类器")，*简单分类器*中熟悉的混淆矩阵：
- en: '[PRE43]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: That's it. You have an evaluation setup that is strongly related to the classifier
    evaluation from [Chapter 1](ch01.html "Chapter 1. Simple Classifiers"), *Simple
    Classifiers*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。你有了一个与[第1章](ch01.html "第1章. 简单分类器")，*简单分类器*中的分类器评估密切相关的评估设置。
- en: There's more…
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: There are evaluation classes for the n-best word tagging, that is, `NBestTaggerEvaluator`
    and `MarginalTaggerEvaluator`, for the confidence ranked. Again, look at the more
    detailed tutorial on part-of-speech tagging for a quite thorough presentation
    on evaluation metrics and some example software to help tune the HMM.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 针对n最佳词标注，存在评估类，即`NBestTaggerEvaluator`和`MarginalTaggerEvaluator`，用于信心排名。同样，可以查看更详细的词性标注教程，里面有关于评估指标的详细介绍，以及一些示例软件来帮助调整HMM。
- en: Conditional random fields (CRF) for word/token tagging
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 条件随机场（CRF）用于词/标记标注
- en: '**Conditional random fields** (**CRF**) are an extension of the *Logistic regression*
    recipe in [Chapter 3](ch03.html "Chapter 3. Advanced Classifiers"), *Advanced
    Classifiers*, but are applied to word tagging. At the end of [Chapter 1](ch01.html
    "Chapter 1. Simple Classifiers"), *Simple Classifiers*, we discussed various ways
    to encode a problem into a classification problem. CRFs treat the sequence tagging
    problem as finding the best category where each category (C) is one of the C*T
    tag (T) assignments to tokens.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**条件随机场**（**CRF**）是[第3章](ch03.html "第3章. 高级分类器")的*逻辑回归*配方的扩展，应用于词标注。在[第1章](ch01.html
    "第1章. 简单分类器")的*简单分类器*中，我们讨论了将问题编码为分类问题的各种方式。CRF将序列标注问题视为找到最佳类别，其中每个类别（C）是C*T标签（T）分配到词元的其中之一。'
- en: 'For example, if we have the tokens `The` and `rain` and tag `d` for determiner
    and `n` for noun, then the set of categories for the CRF classifier are:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有词组`The`和`rain`，并且标签`d`表示限定词，`n`表示名词，那么CRF分类器的类别集如下：
- en: '**Category 1**: `d d`'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别 1**：`d d`'
- en: '**Category 2**: `n d`'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别 2**：`n d`'
- en: '**Category 3**: `n n`'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别 3**：`n n`'
- en: '**Category 4**: `d d`'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别 4**：`d d`'
- en: Various optimizations are applied to keep this combinatoric nightmare computable,
    but this is the general idea. Crazy, but it works.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个组合计算的噩梦变得可计算，采用了各种优化方法，但这是大致的思路。疯狂，但它有效。
- en: Additionally, CRFs allow random features to be used in training in the exact
    same way that logistic regression does for classification. Additionally, it has
    data structures optimized for HMM style observations against context. Its use
    for part-of-speech tagging is not very exciting, because our current HMMs are
    pretty close to state of the art. Where CRFs really make a difference is in use
    cases like named entity detection which are covered in [Chapter 5](ch05.html "Chapter 5. Finding
    Spans in Text – Chunking"), *Finding Spans in Text – Chunking*, but we wanted
    to address the pure CRF implementation before complicating the presentation with
    a chunking interface.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，CRF允许像逻辑回归对分类所做的那样，在训练中使用随机特征。此外，它具有针对上下文优化的HMM样式观察的数据结构。它在词性标注中的使用并不令人兴奋，因为我们当前的HMM已经接近最先进的技术。CRF真正有所作为的地方是像命名实体识别这样的使用案例，这些内容在[第5章](ch05.html
    "第5章. 在文本中寻找跨度 - 分块")的*在文本中寻找跨度 - 分块*中有所涵盖，但我们希望在通过分块接口使演示更加复杂之前，先讨论纯CRF实现。
- en: There is an excellent detailed tutorial on CRFs at [http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html);
    this recipe follows this tutorial fairly closely. You will find more information
    and proper references there.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html)上有一篇关于CRF的详细优秀教程；这个配方与该教程非常接近。你将在那里找到更多的信息和适当的参考文献。'
- en: How to do it...
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到……
- en: 'All of the technologies we have been presenting up to now were invented in
    the previous millennium; this is a technology from the new millennium. Perform
    the following steps:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止所展示的所有技术都是在上一个千年发明的；这是一项来自新千年的技术。请按照以下步骤进行操作：
- en: 'In the command line, type:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命令行中输入：
- en: '[PRE44]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The console continues with the convergence results that should be familiar
    from the *Logistic regression* recipe of [Chapter 3](ch03.html "Chapter 3. Advanced
    Classifiers"), *Advanced Classifiers*, and we will get the standard command prompt:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制台继续显示收敛结果，这些结果应该与你在[第3章](ch03.html "第3章. 高级分类器")的*逻辑回归*配方中见过的非常相似，我们将得到标准的命令行提示符：
- en: '[PRE45]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In response to this, we will get some fairly confused output:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对此，我们将得到一些相当混乱的输出：
- en: '[PRE46]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This is an awful output, but the CRF has been trained on 11 sentences. So, let's
    not be too harsh—particularly since this technology mostly reigns supreme for
    word tagging and span tagging when given sufficient training data to do its work.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一个糟糕的输出，但CRF已经在11个句子上进行了训练。所以，我们不要过于苛刻——特别是考虑到这项技术在词标注和跨度标注方面表现得尤为出色，只要提供足够的训练数据来完成它的工作。
- en: How it works…
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Like logistic regression, there are many configuration-related tasks that we
    need to perform to get this class up and running. This recipe will address the
    CRF-specific aspects of the code and refer to *the Logistic regression* recipe
    of [Chapter 3](ch03.html "Chapter 3. Advanced Classifiers"), *Advanced Classifiers*
    for the logistic-regression aspects of the configuration.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与逻辑回归类似，我们需要执行许多与配置相关的任务，以使这个类能够正常运行。本食谱将处理代码中的CRF特定方面，并参考[第3章](ch03.html "第3章。高级分类器")中的*逻辑回归*食谱，了解与配置相关的逻辑回归部分。
- en: 'Starting at the top of the `main()` method, we will get our corpus, which was
    discussed in the earlier three recipes:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 从`main()`方法的顶部开始，我们将获取我们的语料库，这部分在前面三个食谱中有讨论：
- en: '[PRE47]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next up is the feature extractor, which is the actual input to the CRF trainer.
    The only reason it is final is that an anonymous inner class will access it to
    demonstrate how feature extraction works in the next recipe:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是特征提取器，它是CRF训练器的实际输入。它之所以是最终的，仅仅是因为一个匿名内部类将访问它，以展示在下一个食谱中如何进行特征提取：
- en: '[PRE48]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We will address how this class works later in the recipe.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本食谱后面讨论这个类的工作原理。
- en: 'The next block of configuration is for the underlying logistic-regression algorithm.
    Refer to the *logistic regression* recipe in [Chapter 3](ch03.html "Chapter 3. Advanced
    Classifiers"), *Advanced Classifiers*, for more information on this. Have a look
    at the following code snippet:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的配置块是针对底层逻辑回归算法的。有关更多信息，请参考[第3章](ch03.html "第3章。高级分类器")中的*逻辑回归*食谱，看看以下代码片段：
- en: '[PRE49]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next up, the CRF is trained with:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用以下内容来训练CRF：
- en: '[PRE50]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The rest of the code just uses the standard I/O loop. Refer to [Chapter 2](ch02.html
    "Chapter 2. Finding and Working with Words"), *Finding and Working with Words*,
    for how the `tokenizerFactory` works:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码只是使用标准的I/O循环。有关`tokenizerFactory`如何工作的内容，请参考[第2章](ch02.html "第2章。查找和使用单词")，*查找和使用单词*：
- en: '[PRE51]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: SimpleCrfFeatureExtractor
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SimpleCrfFeatureExtractor
- en: 'Now, we will get to the feature extractor. The provided implementation closely
    mimics the features of a standard HMM. The class at `com/lingpipe/cookbook/chapter4/SimpleCrfFeatureExtractor.java`
    starts with:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将进入特征提取器部分。提供的实现 closely mimics 标准HMM的特性。`com/lingpipe/cookbook/chapter4/SimpleCrfFeatureExtractor.java`
    类以如下内容开始：
- en: '[PRE52]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The `ChainCrfFeatureExtractor` interface requires an `extract()` method with
    the tokens and associated tags that get converted into `ChainCrfFeatures<String>`
    in this case. This is handled by an inner class below `SimpleChainCrfFeatures`;
    this inner class extends `ChainCrfFeatures` and provides implementations of the
    abstract methods, `nodeFeatures()` and `edgeFeatures()`:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`ChainCrfFeatureExtractor`接口要求一个`extract()`方法，该方法接收令牌和相关的标签，并将它们转换为`ChainCrfFeatures<String>`，在此案例中是这样的。这个过程由下面的一个内部类`SimpleChainCrfFeatures`处理；该内部类继承自`ChainCrfFeatures`，并提供了抽象方法`nodeFeatures()`和`edgeFeatures()`的实现：'
- en: '[PRE53]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The following constructor access passes the tokens and tags to the super class,
    which will do the bookkeeping to support looking up `tags` and `tokens`:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下构造函数访问将令牌和标签传递给超类，超类将进行账务处理，以支持查找`tags`和`tokens`：
- en: '[PRE54]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The node features are computed as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 节点特征计算如下：
- en: '[PRE55]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The tokens are indexed by their position in the sentence. The node feature for
    the word/token in position `n` is the `String` value returned by the base class
    method `token(n)` from `ChainCrfFeatures` with the prefix `TOK_`. The value here
    is `1.0`. Feature values can be usefully adjusted to values other than 1.0, which
    is handy for more sophisticated approaches to CRFs, such as using the confidence
    estimates of other classifiers. Take a look at the following recipe for an example
    of this.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌根据它们在句子中的位置进行索引。位置为`n`的单词/令牌的节点特征是通过`ChainCrfFeatures`的基类方法`token(n)`返回的`String`值，前缀为`TOK_`。这里的值是`1.0`。特征值可以有用地调整为1.0以外的其他值，这对于更复杂的CRF方法非常有用，比如使用其他分类器的置信度估计。看看下面的食谱，以了解如何实现这一点。
- en: 'Like HMMs, there are features that are dependent on other positions in the
    input—these are called **edge features**. The edge features take two arguments:
    one for the position that features are being generated for `n` and `k`, which
    will apply to all other positions in the sentence:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 与HMM类似，有些特征依赖于输入中的其他位置——这些被称为**边缘特征**。边缘特征接受两个参数：一个是生成特征的位置`n`和`k`，它将适用于句子中的所有其他位置：
- en: '[PRE56]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The next recipe will address how to modify feature extraction.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 下一篇食谱将处理如何修改特征提取。
- en: There's more…
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多内容……
- en: There is an extensive research literature referenced in the Javadoc and a much
    more exhaustive tutorial on the LingPipe website.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Javadoc中引用了大量研究文献，LingPipe网站上也有一个更加详细的教程。
- en: Modifying CRFs
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修改CRF
- en: The power and appeal of CRFs comes from rich feature extraction—proceed with
    an evaluation harness that provides feedback on your explorations. This recipe
    will detail how to create more complex features.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: CRF的强大和吸引力来源于丰富的特征提取——通过提供反馈的评估工具来进行你的探索。本示例将详细介绍如何创建更复杂的特征。
- en: How to do it...
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'We will not train and run a CRF; instead, we will print out the features. Substitute
    this feature extractor for the one in the previous recipe to see them at work.
    Perform the following steps:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会训练和运行CRF；相反，我们将打印出特征。将此特征提取器替换为之前示例中的特征提取器，以查看它们的工作效果。执行以下步骤：
- en: 'Go to a command line and type:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开命令行并输入：
- en: '[PRE57]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The feature extractor class outputs for each token in the training data the
    truth tagging that is being used to learn:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取器类会为训练数据中的每个标记输出真实标签，这些标签用于学习：
- en: '[PRE58]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: This reflects the training tagging for the token `John` as determined by `src/com/lingpipe/cookbook/chapter4/TinyPosCorpus.java`.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这反映了`John`标记的训练标签，它是由`src/com/lingpipe/cookbook/chapter4/TinyPosCorpus.java`文件中确定的。
- en: 'The node features follow the top-three POS tags from our Brown corpus HMM tagger
    and the `TOK_John` feature:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 节点特征遵循我们Brown语料库HMM标注器的前三个POS标签以及`TOK_John`特征：
- en: '[PRE59]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Next, the edge features are displayed for the other tokens in the sentence,
    "John ran":'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，显示句子“John ran”中其他标记的边特征：
- en: '[PRE60]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: The rest of the output are the features for the remaining tokens in the sentence
    and then the remaining sentences in `TinyPosCorpus`.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩余的输出为句子中其余标记的特征，然后是`TinyPosCorpus`中剩余的句子。
- en: How it works…
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理是……
- en: 'Our feature extraction code occurs in `src/com/lingpipe/cookbook/chapter4/ModifiedCrfFeatureExtractor.java`.
    We will start with the `main()` method that loads a corpus, runs the contents
    past the feature extractor, and prints it out:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的特征提取代码位于`src/com/lingpipe/cookbook/chapter4/ModifiedCrfFeatureExtractor.java`。我们将从加载语料库、通过特征提取器处理内容并打印出来的`main()`方法开始：
- en: '[PRE61]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We will tee up `TinyPosCorpus` from the previous recipe as our corpus, and then,
    we will create a feature extractor from the containing class. The use of `final`
    is required by referencing the variable in the anonymous inner class that follows.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前示例中的`TinyPosCorpus`作为我们的语料库，然后从包含类创建特征提取器。引用变量在后面的匿名内部类中需要使用`final`修饰符。
- en: 'Apologies for the anonymous inner class, but it is just the easiest way to
    access what is stored in the corpus for various reasons such as copying and printing.
    In this case, we are just generating and printing the features found for the training
    data:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对于匿名内部类表示歉意，但这是访问语料库中存储内容的最简单方式，原因多种多样，例如复制和打印。在这种情况下，我们只是生成并打印训练数据中找到的特征：
- en: '[PRE62]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The corpus contains `Tagging` objects, and they, in turn, contain a `List<String>`
    of tokens and tags. Then, this information is used to create a `ChainCrfFeatures<String>`
    object by applying the `featureExtractor.extract()` method to the tokens and tags.
    This will involve substantial computation, as will be shown.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库包含`Tagging`对象，而它们又包含一个`List<String>`的标记和标签。然后，使用这些信息通过应用`featureExtractor.extract()`方法到标记和标签，创建一个`ChainCrfFeatures<String>`对象。这将涉及大量计算，如将展示的那样。
- en: 'Next, we will do reporting of the training data with tokens and the expected
    tagging:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对训练数据进行报告，包含标记和预期标签：
- en: '[PRE63]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Then, we will follow with the features that will be used to inform the CRF
    model in attempting to produce the preceding tagging for nodes:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续展示将用于通知CRF模型，以尝试为节点生成前置标签的特征：
- en: '[PRE64]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Then, the edge features are produced by the following iteration of relative
    positions to the source node `i`:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过以下对源节点`i`相对位置的迭代来生成边特征：
- en: '[PRE65]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'This is it to print out the features. Now, we will address how the feature
    extractor is constructed. We assume that you are familiar with the previous recipe.
    First, the constructor that brings in the Brown corpus POS tagger:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们打印出特征。接下来，我们将介绍如何构建特征提取器。假设你已经熟悉之前的示例。首先，构造函数引入了Brown语料库POS标注器：
- en: '[PRE66]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The constructor brings in some external resources for feature generation, namely,
    a POS tagger trained on the Brown corpus. Why involve another POS tagger for a
    POS tagger? We will call the role of the Brown POS tagger a "feature tagger" to
    distinguish it from the tagger we are trying to build. A few reasons to use the
    feature tagger are:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 该构造函数引入了一些外部资源用于特征生成，即一个基于布朗语料库训练的POS标注器。为什么要为POS标注器引入另一个POS标注器呢？我们将布朗POS标注器的角色称为“特征标注器”，以将其与我们正在构建的标注器区分开来。使用特征标注器的原因有几个：
- en: We are using a stupidly small corpus for training, and a more robust generalized
    POS feature tagger will help things out. `TinyPosCorpus` is too small for even
    this benefit, but with a bit more data, the fact that there is a feature `at`
    that unifies `the`, `a`, and `some` will help the CRF recognize that `some dog`
    is `'DET'` `'N'`, even though it has never seen `some` in training.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用的是一个非常小的语料库进行训练，一个更强大的通用POS特征标注器将帮助改善结果。`TinyPosCorpus`语料库甚至太小，无法带来这样的好处，但如果有更多的数据，`at`这个特征统一了`the`、`a`和`some`，这将帮助CRF识别出`some
    dog`应该是`'DET'` `'N'`，即便在训练中它从未见过`some`。
- en: We have had to work with tag sets that are not aligned with POS feature taggers.
    The CRF can use these observations in the foreign tag set to better reason about
    the desired tagging. The simplest case is that `at`, from the Brown corpus tag
    set, maps cleanly onto `DET` in this tag set.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不得不与那些与POS特征标注器不一致的标签集一起工作。CRF可以使用这些外部标签集中的观察结果来更好地推理期望的标注。最简单的情况是，来自布朗语料库标签集中的`at`可以干净地映射到当前标签集中的`DET`。
- en: There can be performance improvements to run multiple taggers that are either
    trained on different data or use different technologies to tag. The CRF can then,
    hopefully, recognize contexts where one tagger outperforms others and use this
    information to guide the analysis. Back in the day, our MUC-6 system featured
    3 POS taggers that voted for the best output. Letting the CRF sort it out will
    be a superior approach.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过运行多个标注器来提高性能，这些标注器可以基于不同的数据进行训练，或使用不同的技术进行标注。然后，CRF可以在希望的情况下识别出一个标注器优于其他标注器的上下文，并利用这些信息来引导分析。在过去，我们的MUC-6系统使用了3个POS标注器，它们投票选出最佳输出。让CRF来解决这个问题会是一种更优的方法。
- en: 'The guts of feature extraction are accessed with the `extract` method:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取的核心通过`extract`方法访问：
- en: '[PRE67]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '`ModChainCrfFeatures` is created as an inner class just to keep the proliferation
    of classes to a minimum, and the enclosing class is very lightweight:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '`ModChainCrfFeatures`作为一个内部类创建，旨在将类的数量保持在最低限度，且外部类非常轻量：'
- en: '[PRE68]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The preceding constructor hands off the tokens and tags to the super class,
    which handles bookkeeping of this data. Then, the "feature tagger" is applied
    to the tokens, and the resulting output is assigned to the member variable, `mBrownTaggingLattice`.
    The code will access the tagging, one token at a time, so it must be computed
    now.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 上述构造函数将令牌和标签交给父类，父类负责处理这些数据的记账工作。然后，“特征标注器”应用于令牌，结果输出被分配给成员变量`mBrownTaggingLattice`。代码将一次访问一个令牌的标注，因此现在必须计算这些标注。
- en: 'The feature creation step happens with two methods: `nodeFeatures` and `edgeFeatures`.
    We will start with a simple enhancement of `edgeFeatures` from the previous recipe:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 特征创建步骤通过两个方法进行：`nodeFeatures`和`edgeFeatures`。我们将从对前一个配方中`edgeFeatures`的简单增强开始：
- en: '[PRE69]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The code adds a token-shaped feature that generalizes `12` and `34` into `2-DIG`
    and many other generalizations. To CRF, the similarity of `12` and `34` as two-digit
    numbers is non-existent unless feature extraction says otherwise. Refer to the
    Javadoc for the complete categorizer output.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 代码添加了一个令牌形态特征，将`12`和`34`泛化为`2-DIG`以及其他许多泛化。对于CRF而言，除非特征提取另有说明，否则`12`和`34`作为两位数之间的相似性是不存在的。请参阅Javadoc获取完整的分类器输出。
- en: Candidate-edge features
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 候选边缘特征
- en: 'CRFs allow random features to be applied, so the question is what features
    make sense to use. Edge features are used in conjunction with node features, so
    another issue is whether a feature should be applied to edges or nodes. Edge features
    will be used to reason about relationships between the current word/token to those
    around it. Some possible edge features are:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: CRF允许应用随机特征，因此问题是哪些特征是有意义的。边缘特征与节点特征一起使用，因此另一个问题是特征应该应用于边缘还是节点。边缘特征将用于推理当前词/令牌与周围词语的关系。一些可能的边缘特征包括：
- en: The token shape (all caps, starts with a number, and so on) of the previous
    token as done earlier.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前一个令牌的形态（全大写、以数字开头等），如前所述。
- en: Recognition of iambic pentameter that requires a correct ordering of stressed
    and unstressed syllables. This will require a syllable-stress tokenizer as well.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要正确排序重音和非重音音节的抑扬格五音步识别。这还需要一个音节重音分词器。
- en: It often happens that text contains one or more languages—this is called code
    switching. It is a common occurrence in tweets. A reasonable edge feature will
    be the language of surrounding tokens; this language will better model that the
    next word is likely to be of the same language as the previous word.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本中经常包含一种或多种语言——这叫做代码切换。这在推文中是常见的现象。一个合理的边缘特征将是周围令牌的语言；这种语言可以更好地建模下一词可能与前一词属于同一语言。
- en: Node features
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点特征
- en: 'The node features tend to be where the action is in CRFs, and they can get
    very rich. The *Named entity recognition using CRFs with better features* recipe
    in [Chapter 5](ch05.html "Chapter 5. Finding Spans in Text – Chunking"), *Finding
    Spans in Text – Chunking*, is an example. We will add part-of-speech tags in this
    recipe to the token feature of the previous recipe:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 节点特征通常是CRF中动作的关键所在，并且它们可以变得非常丰富。在[第5章](ch05.html "Chapter 5. Finding Spans in
    Text – Chunking")中的*使用CRF和更好的特征进行命名实体识别*方法，*Finding Spans in Text – Chunking*，就是一个例子。在这个方法中，我们将为前一个方法的令牌特征添加词性标注：
- en: '[PRE70]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Then as in the previous recipe the token feature is added with:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，像在前一个方法中一样，通过以下方式添加令牌特征：
- en: '[PRE71]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: This results in the token string being prepended with `TOK_` and a count of
    `1`. Note that while `tag(n)` is available in training, it doesn't make sense
    to use this information, as that is what the CRF is trying to predict.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致令牌字符串前面加上`TOK_`和计数`1`。请注意，虽然`tag(n)`在训练中可用，但使用该信息没有意义，因为CRF的目标就是预测这些标签。
- en: Next, the top-three tags are extracted from the POS feature tagger and added
    with the associated conditional probability. CRFs will be able to work with the
    varying weights productively.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从词性特征标注器中提取出前三个标签，并与相关的条件概率一起添加。CRF将能够通过这些变化的权重进行有效的工作。
- en: There's more…
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'When generating new features, some thought about the sparseness of data is
    worth considering. If dates were likely to be an important feature for the CRF,
    it would probably not be a good idea to do the standard Computer Science thing
    and convert the date to milliseconds since Jan 1, 1970 GMT. The reason is that
    the `MILLI_1000000000` feature will be treated as completely different from `MILLI_1000000001`.
    There are a few reasons:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成新特征时，值得考虑数据的稀疏性。如果日期可能是CRF的重要特征，可能不适合做计算机科学中的标准操作——将日期转换为自1970年1月1日格林威治标准时间以来的毫秒数。原因是`MILLI_1000000000`特征将被视为与`MILLI_1000000001`完全不同。原因有几个：
- en: The underlying classifier does not know that the two values are nearly the same
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 底层分类器并不知道这两个值几乎相同。
- en: The classifier does not know that the `MILLI_` prefix is the same—the common
    prefix is only there for human convenience
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器并不知道`MILLI_`前缀是相同的——这个通用前缀仅仅是为了方便人类。
- en: The feature is unlikely to occur in training more than once and will likely
    be pruned by a minimum feature count
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该特征在训练中不太可能出现多次，可能会被最小特征计数修剪掉。
- en: Instead of normalizing dates to milliseconds, consider an abstraction over the
    dates that will likely have many instances in training data, such as the `has_date`
    feature that ignores the actual date but notes the existence of the date. If the
    date is important, then compute all the important information about the date.
    If it is a day of the week, then map to days of the week. If temporal order matters,
    then map to coarser measurement that is likely to have many measurements. Generally
    speaking, CRFs and the underlying logistic regression classifier are robust against
    ineffective features, so feel free to be creative—you are unlikely to make accuracy
    worse by adding features.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是将日期标准化为毫秒，考虑使用一个抽象层来表示日期，这个日期在训练数据中可能有很多实例，例如忽略实际日期但记录日期存在性的`has_date`特征。如果日期很重要，那么计算关于日期的所有重要信息。如果它是星期几，那么映射到星期几。如果时间顺序很重要，那么映射到更粗略的度量，这些度量可能有许多测量值。一般来说，CRF和底层的逻辑回归分类器对于无效特征具有鲁棒性，因此可以大胆尝试创新——添加特征不太可能使准确度更差。
