- en: ChapterÂ 8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬8ç« 
- en: Applying Bayesian Deep Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨è´å¶æ–¯æ·±åº¦å­¦ä¹ 
- en: This chapter will guide you through a variety of applications of Bayesian deep
    learning (BDL). These will include the use of BDL in standard classification tasks,
    as well as demonstrating how it can be used in more sophisticated ways for out-of-distribution
    detection, data selection, and reinforcement learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†å¼•å¯¼ä½ äº†è§£è´å¶æ–¯æ·±åº¦å­¦ä¹ ï¼ˆBDLï¼‰çš„å¤šç§åº”ç”¨ã€‚è¿™äº›åº”ç”¨åŒ…æ‹¬BDLåœ¨æ ‡å‡†åˆ†ç±»ä»»åŠ¡ä¸­çš„ä½¿ç”¨ï¼Œä»¥åŠå±•ç¤ºå¦‚ä½•åœ¨å¼‚å¸¸æ•°æ®æ£€æµ‹ã€æ•°æ®é€‰æ‹©å’Œå¼ºåŒ–å­¦ä¹ ç­‰æ›´å¤æ‚çš„ä»»åŠ¡ä¸­ä½¿ç”¨BDLã€‚
- en: 'We will cover these topics in the following sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­è®¨è®ºè¿™äº›ä¸»é¢˜ï¼š
- en: Detecting out-of-distribution data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ£€æµ‹å¼‚å¸¸æ•°æ®
- en: Being robust against dataset drift
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æé«˜å¯¹æ•°æ®é›†æ¼‚ç§»çš„é²æ£’æ€§
- en: Using data selection via uncertainty to keep models fresh
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨åŸºäºä¸ç¡®å®šæ€§çš„æ•°æ®æ˜¾ç¤ºé€‰æ‹©ï¼Œä¿æŒæ¨¡å‹çš„æ–°é²œåº¦
- en: Using uncertainty estimates for smarter reinforcement learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡è¿›è¡Œæ›´æ™ºèƒ½çš„å¼ºåŒ–å­¦ä¹ 
- en: Susceptibility to adversarial input
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹æŠ—æ€§è¾“å…¥çš„æ˜“æ„Ÿæ€§
- en: 8.1 Technical requirements
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 æŠ€æœ¯è¦æ±‚
- en: 'All of the code for this book can be found on the GitHub repository for the
    book: [https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ä¹¦çš„æ‰€æœ‰ä»£ç éƒ½å¯ä»¥åœ¨æœ¬ä¹¦çš„GitHubä»“åº“ä¸­æ‰¾åˆ°ï¼š[https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference)
- en: 8.2 Detecting out-of-distribution data
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 æ£€æµ‹å¼‚å¸¸æ•°æ®
- en: 'Typical neural networks do not handle out-of-distribution data well. We saw
    in [*ChapterÂ 3*](CH3.xhtml#x1-350003), [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003)
    that a cat-dog classifier classified an image of a parachute as a dog with more
    than 99% confidence. In this section, we will look into what we can do about this
    vulnerability of neural networks. We will do the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å…¸å‹çš„ç¥ç»ç½‘ç»œåœ¨å¤„ç†å¼‚å¸¸æ•°æ®æ—¶è¡¨ç°ä¸ä½³ã€‚æˆ‘ä»¬åœ¨[*ç¬¬3ç« *](CH3.xhtml#x1-350003)ã€[*æ·±åº¦å­¦ä¹ åŸºç¡€*](CH3.xhtml#x1-350003)ä¸­çœ‹åˆ°ï¼ŒçŒ«ç‹—åˆ†ç±»å™¨å°†ä¸€å¼ é™è½ä¼çš„å›¾åƒé”™è¯¯åœ°åˆ†ç±»ä¸ºç‹—ï¼Œå¹¶ä¸”ç½®ä¿¡åº¦è¶…è¿‡99%ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å¦‚ä½•è§£å†³ç¥ç»ç½‘ç»œè¿™ä¸€å¼±ç‚¹ã€‚æˆ‘ä»¬å°†è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: Explore the problem visually by perturbing a digit of the `MNIST` dataset
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é€šè¿‡æ‰°åŠ¨`MNIST`æ•°æ®é›†ä¸­çš„ä¸€ä¸ªæ•°å­—ï¼Œç›´è§‚åœ°æ¢ç´¢è¿™ä¸ªé—®é¢˜
- en: Explain the typical way out-of-distribution detection performance is reported
    in the literature
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è§£é‡Šæ–‡çŒ®ä¸­é€šå¸¸å¦‚ä½•æŠ¥å‘Šå¼‚å¸¸æ•°æ®æ£€æµ‹çš„æ€§èƒ½
- en: Review the out-of-distribution detection performance of some of the standard
    practical BDL methods we look at in this chapter
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å›é¡¾æˆ‘ä»¬åœ¨æœ¬ç« ä¸­è®¨è®ºçš„å‡ ç§æ ‡å‡†å®ç”¨è´å¶æ–¯æ·±åº¦å­¦ä¹ ï¼ˆBDLï¼‰æ–¹æ³•åœ¨å¼‚å¸¸æ•°æ®æ£€æµ‹ä¸­çš„è¡¨ç°
- en: Explore even more practical methods that are specifically tailored to detect
    out-of-distribution detection
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¢ç´¢æ›´å¤šä¸“é—¨ç”¨äºå¼‚å¸¸æ•°æ®æ£€æµ‹çš„å®ç”¨æ–¹æ³•
- en: 8.2.1 Exploring the problem of out-of-distribution detection
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 æ¢ç´¢å¼‚å¸¸æ•°æ®æ£€æµ‹çš„é—®é¢˜
- en: 'To give you a better understanding of what out-of-distribution performance
    is like, we will start with a visual example. Here is what we will do:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ›´å¥½åœ°å¸®åŠ©ä½ ç†è§£å¼‚å¸¸æ•°æ®æ£€æµ‹çš„æ•ˆæœï¼Œæˆ‘ä»¬å°†ä»ä¸€ä¸ªè§†è§‰ç¤ºä¾‹å¼€å§‹ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬å°†è¦åšçš„äº‹æƒ…ï¼š
- en: We will train a standard network on the `MNIST` digit dataset
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨`MNIST`æ•°å­—æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªæ ‡å‡†ç½‘ç»œ
- en: We will then perturb a digit and gradually make it more out-of-distribution
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å°†æ‰°åŠ¨ä¸€ä¸ªæ•°å­—ï¼Œå¹¶é€æ¸ä½¿å…¶å˜å¾—æ›´åŠ å¼‚å¸¸
- en: We will report the confidence score of a standard model and MC dropout
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æŠ¥å‘Šæ ‡å‡†æ¨¡å‹å’ŒMC dropoutçš„ç½®ä¿¡åº¦å¾—åˆ†
- en: With this visual example, we can see how simple Bayesian methods can improve
    the out-of-distribution detection performance over a standard deep learning model.
    We start by training a simple model on the `MNIST` dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ä¸ªè§†è§‰ç¤ºä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç®€å•çš„è´å¶æ–¯æ–¹æ³•å¦‚ä½•åœ¨å¼‚å¸¸æ•°æ®æ£€æµ‹ä¸Šä¼˜äºæ ‡å‡†çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨`MNIST`æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªç®€å•çš„æ¨¡å‹ã€‚
- en: '![PIC](img/file160.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file160.png)'
- en: 'FigureÂ 8.1: The classes of the MNIST dataset: 28x28 pixel images of the digits
    zero to nine'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.1ï¼šMNISTæ•°æ®é›†çš„ç±»åˆ«ï¼šé›¶åˆ°ä¹çš„28x28åƒç´ æ•°å­—å›¾åƒ
- en: We use `TensorFlow` to train our model, `numpy` to make our images more out-of-distribution,
    and `Matplotlib` to visualize our data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨`TensorFlow`æ¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨`numpy`è®©æˆ‘ä»¬çš„å›¾åƒæ›´å…·å¼‚å¸¸æ€§ï¼Œä½¿ç”¨`Matplotlib`æ¥å¯è§†åŒ–æ•°æ®ã€‚
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `MNIST` dataset is available in TensorFlow, so we can just load it:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`MNIST`æ•°æ®é›†å¯ä»¥åœ¨TensorFlowä¸­æ‰¾åˆ°ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æ¥åŠ è½½å®ƒï¼š'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`MNIST` is a simple dataset, so a simple model allows us to achieve a test
    accuracy of more than 99%. We use a standard CNN with three convolutional layers:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`MNIST`æ˜¯ä¸€ä¸ªç®€å•çš„æ•°æ®é›†ï¼Œå› æ­¤ä½¿ç”¨ç®€å•çš„æ¨¡å‹å¯ä»¥è®©æˆ‘ä»¬åœ¨æµ‹è¯•ä¸­è¾¾åˆ°è¶…è¿‡99%çš„å‡†ç¡®ç‡ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ ‡å‡†çš„CNNï¼ŒåŒ…å«ä¸‰å±‚å·ç§¯å±‚ï¼š'
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can then compile and train our model. We obtain a validation accuracy of
    over 99% after just 5 epochs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç¼–è¯‘å¹¶è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚ç»è¿‡5ä¸ªepochsåï¼Œæˆ‘ä»¬çš„éªŒè¯å‡†ç¡®ç‡è¶…è¿‡99%ã€‚
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, letâ€™s see how this model handles out-of-distribution data. Imagine we
    deploy this model to recognize digits, but users sometimes fail to write down
    the entire digit. What happens when users do not write down the entire digit?
    We can get an answer to this question by gradually removing more and more information
    from a digit, and seeing how our model handles the perturbed inputs. We can define
    our function to remove `signal` as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è¿™ä¸ªæ¨¡å‹å¦‚ä½•å¤„ç†åˆ†å¸ƒå¤–æ•°æ®ã€‚å‡è®¾æˆ‘ä»¬éƒ¨ç½²è¿™ä¸ªæ¨¡å‹æ¥è¯†åˆ«æ•°å­—ï¼Œä½†ç”¨æˆ·æœ‰æ—¶æ— æ³•å†™ä¸‹å®Œæ•´çš„æ•°å­—ã€‚å½“ç”¨æˆ·æ²¡æœ‰å†™ä¸‹å®Œæ•´çš„æ•°å­—æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿæˆ‘ä»¬å¯ä»¥é€šè¿‡é€æ¸ç§»é™¤æ•°å­—ä¸­çš„ä¿¡æ¯ï¼Œè§‚å¯Ÿæ¨¡å‹å¦‚ä½•å¤„ç†è¿™äº›æ‰°åŠ¨è¾“å…¥ï¼Œæ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬å¯ä»¥è¿™æ ·å®šä¹‰ç§»é™¤`signal`çš„å‡½æ•°ï¼š
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And then we perturb our images:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯¹å›¾åƒè¿›è¡Œæ‰°åŠ¨ï¼š
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We only add perturbed images to our list of images if setting a row to 0 actually
    changes the original image ( `ifÂ np.array_equal(img,Â img_perturbed))` and stop
    once the image is completely black, meaning it just contains pixels with a value
    of 0\. We run inference on these images:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªæœ‰åœ¨å°†æŸä¸€è¡Œè®¾ä¸º0ç¡®å®æ”¹å˜äº†åŸå§‹å›¾åƒæ—¶ï¼Œæ‰å°†æ‰°åŠ¨åçš„å›¾åƒæ·»åŠ åˆ°æˆ‘ä»¬çš„å›¾åƒåˆ—è¡¨ä¸­ï¼ˆ`if np.array_equal(img, img_perturbed))`ï¼‰ï¼Œå¹¶ä¸”ä¸€æ—¦å›¾åƒå®Œå…¨å˜é»‘ï¼Œå³ä»…åŒ…å«å€¼ä¸º0çš„åƒç´ ï¼Œæˆ‘ä»¬å°±åœæ­¢ã€‚æˆ‘ä»¬å¯¹è¿™äº›å›¾åƒè¿›è¡Œæ¨ç†ï¼š
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can then plot all images with their predicted labels and confidence scores:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶æ‰€æœ‰å›¾åƒåŠå…¶é¢„æµ‹æ ‡ç­¾å’Œç½®ä¿¡åº¦åˆ†æ•°ï¼š
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This produces the following figure:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç”Ÿæˆäº†å¦‚ä¸‹çš„å›¾ï¼š
- en: '![PIC](img/file161.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file161.png)'
- en: 'FigureÂ 8.2: Predicted label and corresponding softmax score of a standard neural
    network for an image that is more and more out-of-distribution'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.2ï¼šæ ‡å‡†ç¥ç»ç½‘ç»œå¯¹äºé€æ¸åç¦»åˆ†å¸ƒçš„å›¾åƒæ‰€é¢„æµ‹çš„æ ‡ç­¾åŠç›¸åº”çš„softmaxåˆ†æ•°
- en: We can see in *Figure* [*8.2*](#x1-135222r2) that, initially, our model confidently
    classifies the image as a **2**. Remarkably, this confidence persists even when
    it seems unreasonable to do so. For example, the model still classifies image
    14 as a **2** with 97.83% confidence. Moreover, the model predicts with 92.32%
    confidence that a completely horizontal line is a **1**, as we can see in image
    17\. It looks like our model is overconfident in its predictions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨*å›¾* [*8.2*](#x1-135222r2)ä¸­çœ‹åˆ°ï¼Œæœ€åˆï¼Œæˆ‘ä»¬çš„æ¨¡å‹éå¸¸è‡ªä¿¡åœ°å°†å›¾åƒåˆ†ç±»ä¸º**2**ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿åœ¨è¿™ç§åˆ†ç±»æ˜¾å¾—ä¸åˆç†æ—¶ï¼Œè¿™ç§è‡ªä¿¡ä¾ç„¶å­˜åœ¨ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹ä»ç„¶ä»¥97.83%çš„ç½®ä¿¡åº¦å°†å›¾åƒ14åˆ†ç±»ä¸º**2**ã€‚æ­¤å¤–ï¼Œæ¨¡å‹è¿˜é¢„æµ‹å®Œå…¨æ°´å¹³çš„çº¿æ¡æ˜¯**1**ï¼Œç½®ä¿¡åº¦ä¸º92.32%ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨å›¾åƒ17ä¸­æ‰€è§ã€‚è¿™çœ‹èµ·æ¥æˆ‘ä»¬çš„æ¨¡å‹åœ¨é¢„æµ‹æ—¶è¿‡äºè‡ªä¿¡ã€‚
- en: 'Letâ€™s see what a slightly different model would predict on these images. Weâ€™ll
    now use MC dropout as our model. By sampling, we should be able to increase the
    modelsâ€™ uncertainty compared to a standard NN. Letâ€™s first define our model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªç¨æœ‰ä¸åŒçš„æ¨¡å‹ä¼šå¦‚ä½•å¯¹è¿™äº›å›¾åƒåšå‡ºé¢„æµ‹ã€‚æˆ‘ä»¬ç°åœ¨å°†ä½¿ç”¨MC Dropoutä½œä¸ºæˆ‘ä»¬çš„æ¨¡å‹ã€‚é€šè¿‡é‡‡æ ·ï¼Œæˆ‘ä»¬åº”è¯¥èƒ½å¤Ÿæé«˜æ¨¡å‹çš„ä¸ç¡®å®šæ€§ï¼Œç›¸è¾ƒäºæ ‡å‡†çš„ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å…ˆå®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ï¼š
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then letâ€™s instantiate it:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæˆ‘ä»¬æ¥å®ä¾‹åŒ–å®ƒï¼š
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Our model with dropout will achieve a similar accuracy as our vanilla model.
    Letâ€™s now run inference with dropout and plot the mean confidence score of MC
    dropout:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨dropoutçš„æ¨¡å‹å°†å®ç°ä¸åŸå§‹æ¨¡å‹ç±»ä¼¼çš„å‡†ç¡®æ€§ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬ä½¿ç”¨dropoutè¿›è¡Œæ¨ç†ï¼Œå¹¶ç»˜åˆ¶MC Dropoutçš„å¹³å‡ç½®ä¿¡åº¦åˆ†æ•°ï¼š
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This again produces a figure showing the predicted labels and their associated
    confidence scores:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å†æ¬¡ç”Ÿæˆäº†ä¸€ä¸ªå›¾ï¼Œæ˜¾ç¤ºäº†é¢„æµ‹æ ‡ç­¾åŠå…¶ç›¸å…³çš„ç½®ä¿¡åº¦åˆ†æ•°ï¼š
- en: '![PIC](img/file162.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file162.png)'
- en: 'FigureÂ 8.3: Predicted label and corresponding softmax score of an MC dropout
    network for an image that is more and more out-of-distribution'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.3ï¼šMC Dropoutç½‘ç»œå¯¹äºé€æ¸åç¦»åˆ†å¸ƒçš„å›¾åƒæ‰€é¢„æµ‹çš„æ ‡ç­¾åŠç›¸åº”çš„softmaxåˆ†æ•°
- en: 'We can see in *Figure* [*8.3*](#x1-135306r3) that the model is less certain
    on average. The modelâ€™s confidence decreases a lot when we remove rows from our
    image. That is desired behaviour: our model does not know the input, so it should
    be uncertain. However, we can also see that the model is not perfect:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨*å›¾* [*8.3*](#x1-135306r3)ä¸­çœ‹åˆ°ï¼Œæ¨¡å‹çš„è‡ªä¿¡åº¦å¹³å‡æ¥è¯´è¾ƒä½ã€‚å½“æˆ‘ä»¬ä»å›¾åƒä¸­ç§»é™¤è¡Œæ—¶ï¼Œæ¨¡å‹çš„ç½®ä¿¡åº¦å¤§å¹…ä¸‹é™ã€‚è¿™æ˜¯æœŸæœ›çš„è¡Œä¸ºï¼šå½“æ¨¡å‹ä¸çŸ¥é“è¾“å…¥æ—¶ï¼Œå®ƒåº”è¯¥è¡¨ç°å‡ºä¸ç¡®å®šæ€§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¹Ÿèƒ½çœ‹åˆ°æ¨¡å‹å¹¶ä¸å®Œç¾ï¼š
- en: It maintains a pretty high confidence for images that do not really look like
    a **2**.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºé‚£äº›çœ‹èµ·æ¥å¹¶ä¸åƒ**2**çš„å›¾åƒï¼Œæ¨¡å‹ä»ç„¶ä¿æŒè¾ƒé«˜çš„ç½®ä¿¡åº¦ã€‚
- en: The modelâ€™s confidence can change a lot when we delete one more row from our
    images. For example, the modelâ€™s confidence jumps from 61.72% to 37.20% between
    image 14 and 15.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬ä»å›¾åƒä¸­åˆ é™¤ä¸€è¡Œæ—¶ï¼Œæ¨¡å‹çš„ç½®ä¿¡åº¦å˜åŒ–å¾ˆå¤§ã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹çš„ç½®ä¿¡åº¦åœ¨å›¾åƒ14å’Œå›¾åƒ15ä¹‹é—´ä»61.72%è·ƒå‡è‡³37.20%ã€‚
- en: The model seems to be more confident that image 20, without any white pixels,
    is a **1**.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ¨¡å‹ä¼¼ä¹æ›´æœ‰ä¿¡å¿ƒå°†æ²¡æœ‰ä»»ä½•ç™½è‰²åƒç´ çš„å›¾åƒ20åˆ†ç±»ä¸º**1**ã€‚
- en: MC dropout is, in this case, a step in the right direction, but is not handling
    the out-of-distribution data perfectly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒMC Dropoutæ˜¯ä¸€ä¸ªæœç€æ­£ç¡®æ–¹å‘è¿ˆå‡ºçš„æ­¥éª¤ï¼Œä½†å®ƒå¹¶æ²¡æœ‰å®Œç¾åœ°å¤„ç†åˆ†å¸ƒå¤–æ•°æ®ã€‚
- en: 8.2.2 Systematically evaluating OOD detection performance
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 ç³»ç»Ÿåœ°è¯„ä¼°OODæ£€æµ‹æ€§èƒ½
- en: 'The preceding example suggests that MC dropout gives out-of-distribution images
    a lower confidence score on average. But we only evaluated 20 images with a limited
    variety â€“ we simply removed a single row. This change moved the image more out-of-distribution,
    but all images shown in the previous section are relatively similar to the training
    distribution of `MNIST` if you compare it to, letâ€™s say, natural images of objects.
    Images of airplanes, cars, or birds will definitely be much more out-of-distribution
    than an image of `MNIST` with a few black rows. So it seems to be reasonable that,
    if we want to evaluate the OOD detection performance of our model, we should also
    test it on images that are even more OOD, that is, from a completely different
    dataset. This is the approach that is typically taken in the literature to evaluate
    out-of-distribution detection performance. The procedure is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šè¿°ç¤ºä¾‹è¡¨æ˜ï¼ŒMC dropouté€šå¸¸ä¼šç»™å‡ºåˆ†å¸ƒå¤–å›¾åƒè¾ƒä½çš„ç½®ä¿¡åº¦åˆ†æ•°ã€‚ä½†æˆ‘ä»¬ä»…è¯„ä¼°äº†20å¼ å›¾åƒï¼Œä¸”å˜åŒ–æœ‰é™â€”â€”æˆ‘ä»¬åªæ˜¯åˆ é™¤äº†ä¸€è¡Œã€‚è¿™ä¸€å˜åŒ–ä½¿å¾—å›¾åƒæ›´åŠ åˆ†å¸ƒå¤–ï¼Œä½†å‰ä¸€éƒ¨åˆ†å±•ç¤ºçš„æ‰€æœ‰å›¾åƒä¸`MNIST`çš„è®­ç»ƒåˆ†å¸ƒç›¸æ¯”ï¼Œè¿˜æ˜¯ç›¸å¯¹ç›¸ä¼¼çš„ï¼Œå¦‚æœæ‹¿å®ƒå’Œè‡ªç„¶ç‰©ä½“å›¾åƒæ¯”è¾ƒã€‚ä¾‹å¦‚ï¼Œé£æœºã€æ±½è½¦æˆ–é¸Ÿç±»çš„å›¾åƒè‚¯å®šæ¯”å¸¦æœ‰å‡ è¡Œé»‘è‰²çš„`MNIST`å›¾åƒæ›´å…·åˆ†å¸ƒå¤–ç‰¹å¾ã€‚å› æ­¤ï¼Œä¼¼ä¹åˆç†çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æƒ³è¯„ä¼°æ¨¡å‹çš„OODæ£€æµ‹æ€§èƒ½ï¼Œæˆ‘ä»¬åº”è¯¥åœ¨æ›´åŠ åˆ†å¸ƒå¤–çš„å›¾åƒä¸Šè¿›è¡Œæµ‹è¯•ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¥è‡ªå®Œå…¨ä¸åŒæ•°æ®é›†çš„å›¾åƒã€‚è¿™æ­£æ˜¯æ–‡çŒ®ä¸­é€šå¸¸ç”¨äºè¯„ä¼°åˆ†å¸ƒå¤–æ£€æµ‹æ€§èƒ½çš„æ–¹æ³•ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š
- en: We train a model on in-distribution (ID) images.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨å†…éƒ¨åˆ†å¸ƒï¼ˆIDï¼‰å›¾åƒä¸Šè®­ç»ƒæ¨¡å‹ã€‚
- en: We take one or more completely different OOD datasets and feed these to our
    model.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€‰å–ä¸€ä¸ªæˆ–å¤šä¸ªå®Œå…¨ä¸åŒçš„OODæ•°æ®é›†ï¼Œå¹¶å°†è¿™äº›æ•°æ®å–‚ç»™æˆ‘ä»¬çš„æ¨¡å‹ã€‚
- en: We now treat the predictions of the model on the ID and OOD test datasets as
    a binary problem and compute a single score for every image.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å°†æ¨¡å‹åœ¨IDå’ŒOODæµ‹è¯•æ•°æ®é›†ä¸Šçš„é¢„æµ‹è§†ä¸ºä¸€ä¸ªäºŒè¿›åˆ¶é—®é¢˜ï¼Œå¹¶ä¸ºæ¯ä¸ªå›¾åƒè®¡ç®—ä¸€ä¸ªå•ä¸€çš„å¾—åˆ†ã€‚
- en: In the case of evaluation of the softmax score, this means that we take the
    modelâ€™s maximum softmax score for every ID and OOD image.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨è¯„ä¼°softmaxåˆ†æ•°çš„æƒ…å†µä¸‹ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬ä¸ºæ¯ä¸ªIDå’ŒOODå›¾åƒå–æ¨¡å‹çš„æœ€å¤§softmaxåˆ†æ•°ã€‚
- en: With these scores, we can compute binary metrics, such as the area under the
    receiver operating characteristic (AUROC).
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¿™äº›å¾—åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—äºŒè¿›åˆ¶æŒ‡æ ‡ï¼Œå¦‚æ¥æ”¶è€…æ“ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUROCï¼‰ã€‚
- en: The better the model performs on these binary metrics, the better the modelâ€™s
    OOD detection performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹åœ¨è¿™äº›äºŒè¿›åˆ¶æŒ‡æ ‡ä¸Šçš„è¡¨ç°è¶Šå¥½ï¼Œæ¨¡å‹çš„OODæ£€æµ‹æ€§èƒ½å°±è¶Šå¥½ã€‚
- en: 8.2.3 Simple out-of-distribution detection without retraining
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 æ— éœ€é‡æ–°è®­ç»ƒçš„ç®€å•åˆ†å¸ƒå¤–æ£€æµ‹
- en: 'Although MC dropout can be an effective method to detect out-of-distribution
    data, it comes with a major disadvantage at inference time: we need to run inference
    five, or maybe even a hundred, times instead of just once. Something similar can
    be said for certain other Bayesian deep learning methods: although they are principled,
    they are not always the most practical way to obtain a good OOD detection performance.
    The main downside is that they often require retraining of your network, which
    can be expensive to do if you have a lot of data. This is why there is an entire
    field of OOD detection methods that are not explicitly grounded on Bayesian theory,
    but can provide a good, simple, or even excellent baseline. These methods often
    do not require any retraining and can be applied out of the box on a standard
    neural network. Two methods that are often used in the OOD detection literature
    are worth mentioning:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡MC dropoutå¯ä»¥æœ‰æ•ˆæ£€æµ‹å‡ºåˆ†å¸ƒå¤–æ•°æ®ï¼Œä½†å®ƒåœ¨æ¨ç†æ—¶å­˜åœ¨ä¸€ä¸ªä¸»è¦ç¼ºç‚¹ï¼šæˆ‘ä»¬éœ€è¦è¿›è¡Œäº”æ¬¡ï¼Œç”šè‡³ä¸€ç™¾æ¬¡æ¨ç†ï¼Œè€Œä¸æ˜¯ä»…ä»…ä¸€æ¬¡ã€‚å¯¹äºæŸäº›å…¶ä»–è´å¶æ–¯æ·±åº¦å­¦ä¹ æ–¹æ³•ä¹Ÿå¯ä»¥è¯´ç±»ä¼¼ï¼šè™½ç„¶å®ƒä»¬æœ‰ç†è®ºä¾æ®ï¼Œä½†å¹¶ä¸æ€»æ˜¯è·å¾—è‰¯å¥½OODæ£€æµ‹æ€§èƒ½çš„æœ€å®é™…æ–¹æ³•ã€‚ä¸»è¦çš„ç¼ºç‚¹æ˜¯ï¼Œå®ƒä»¬é€šå¸¸éœ€è¦é‡æ–°è®­ç»ƒç½‘ç»œï¼Œå¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œè¿™å¯èƒ½ä¼šéå¸¸æ˜‚è´µã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæœ‰ä¸€æ•´å¥—ä¸æ˜¾å¼ä¾èµ–è´å¶æ–¯ç†è®ºçš„OODæ£€æµ‹æ–¹æ³•ï¼Œä½†èƒ½æä¾›è‰¯å¥½ã€ç®€å•ï¼Œç”šè‡³æ˜¯ä¼˜ç§€çš„åŸºçº¿ã€‚è¿™äº›æ–¹æ³•é€šå¸¸ä¸éœ€è¦ä»»ä½•é‡æ–°è®­ç»ƒï¼Œå¯ä»¥ç›´æ¥åœ¨æ ‡å‡†ç¥ç»ç½‘ç»œä¸Šåº”ç”¨ã€‚æ–‡çŒ®ä¸­ç»å¸¸ä½¿ç”¨çš„ä¸¤ç§æ–¹æ³•å€¼å¾—ä¸€æï¼š
- en: '**ODIN**: OOD detection with preprocessing and scaling'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ODIN**ï¼šä½¿ç”¨é¢„å¤„ç†å’Œç¼©æ”¾è¿›è¡ŒOODæ£€æµ‹'
- en: '**Mahalanobis**: OOD detection with intermediate features'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é©¬å“ˆæ‹‰è¯ºæ¯”æ–¯**ï¼šä½¿ç”¨ä¸­é—´ç‰¹å¾è¿›è¡ŒOODæ£€æµ‹'
- en: 'ODIN: OOD detection with preprocessing and scaling'
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ODINï¼šä½¿ç”¨é¢„å¤„ç†å’Œç¼©æ”¾è¿›è¡ŒOODæ£€æµ‹
- en: '*O*ut-of-*DI*stribution detector for *N*eural networks (ODIN) is one of the
    standard methods in practical out-of-distribution detection because of its simplicity
    and effectiveness. Although the method was introduced in 2017, it is still frequently
    used as a comparison method in papers that propose out-of-distribution detection
    methods.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*O*ut-of-*DI*stribution æ£€æµ‹å™¨ï¼ˆODINï¼‰æ˜¯å®é™…åº”ç”¨ä¸­å¸¸ç”¨çš„æ ‡å‡†åˆ†å¸ƒå¤–æ£€æµ‹æ–¹æ³•ä¹‹ä¸€ï¼Œå› ä¸ºå®ƒç®€å•æœ‰æ•ˆã€‚å°½ç®¡è¯¥æ–¹æ³•åœ¨ 2017 å¹´è¢«æå‡ºï¼Œä½†å®ƒä»ç„¶ç»å¸¸ä½œä¸ºæå‡ºåˆ†å¸ƒå¤–æ£€æµ‹æ–¹æ³•çš„è®ºæ–‡ä¸­çš„å¯¹æ¯”æ–¹æ³•ã€‚'
- en: 'ODIN consists of two key ideas:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ODIN åŒ…å«ä¸¤ä¸ªå…³é”®æ€æƒ³ï¼š
- en: '**Temperature scaling** of the logit scores before applying the softmax operation
    to improve the ability of the softmax score to distinguish between in- and out-of-distribution
    images'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹ logit åˆ†æ•°è¿›è¡Œ**æ¸©åº¦ç¼©æ”¾**ï¼Œç„¶åå†åº”ç”¨ softmax æ“ä½œï¼Œä»¥æé«˜ softmax åˆ†æ•°åŒºåˆ†åœ¨åˆ†å¸ƒå†…å’Œåˆ†å¸ƒå¤–å›¾åƒçš„èƒ½åŠ›ã€‚
- en: '**Input preprocessing** to make in-distribution images more in-distribution'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è¾“å…¥é¢„å¤„ç†** ä½¿åˆ†å¸ƒå†…å›¾åƒæ›´ç¬¦åˆåˆ†å¸ƒå†…'
- en: Letâ€™s look at both ideas in a bit more detail.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ›´è¯¦ç»†åœ°çœ‹çœ‹è¿™ä¸¤ä¸ªæ€æƒ³ã€‚
- en: Temperature scaling ODIN works for classification models. Given our softmax
    score computed as
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ¸©åº¦ç¼©æ”¾ ODIN é€‚ç”¨äºåˆ†ç±»æ¨¡å‹ã€‚ç»™å®šæˆ‘ä»¬è®¡ç®—çš„ softmax åˆ†æ•°å¦‚ä¸‹ï¼š
- en: '![pi(x) = âˆ‘--exp(fi(x))--- Nj=1 exp(fj(x)) ](img/file163.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![pi(x) = âˆ‘--exp(fi(x))--- Nj=1 exp(fj(x)) ](img/file163.jpg)'
- en: 'Here, *f*[*i*](*x*) is a single logit output and *f*[*j*](*x*) are the logits
    for all classes for a single example, temperature scaling means that we divide
    these logit outputs by a constant *T*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ*f*[*i*](*x*) æ˜¯å•ä¸ª logit è¾“å‡ºï¼Œ*f*[*j*](*x*) æ˜¯å•ä¸ªç¤ºä¾‹ä¸­æ‰€æœ‰ç±»åˆ«çš„ logitsï¼Œæ¸©åº¦ç¼©æ”¾æ„å‘³ç€æˆ‘ä»¬å°†è¿™äº›
    logit è¾“å‡ºé™¤ä»¥å¸¸æ•° *T*ï¼š
- en: '![ exp(f (x)âˆ•T) pi(x; T) = âˆ‘N------i---------- j=1 exp (fj(x)âˆ•T) ](img/file164.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![ exp(f (x)âˆ•T) pi(x; T) = âˆ‘N------i---------- j=1 exp (fj(x)âˆ•T) ](img/file164.jpg)'
- en: For large values of *T*, temperature scaling causes the softmax scores to be
    closer to a uniform distribution, which helps to reduce overconfident predictions.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¾ƒå¤§çš„ *T* å€¼ï¼Œæ¸©åº¦ç¼©æ”¾ä½¿å¾— softmax åˆ†æ•°æ›´æ¥è¿‘å‡åŒ€åˆ†å¸ƒï¼Œä»è€Œæœ‰åŠ©äºå‡å°‘è¿‡äºè‡ªä¿¡çš„é¢„æµ‹ã€‚
- en: 'We can apply temperature scaling in Python, given a simple model that outputs
    the logits:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨ Python ä¸­åº”ç”¨æ¸©åº¦ç¼©æ”¾ï¼Œå‡è®¾æœ‰ä¸€ä¸ªç®€å•çš„æ¨¡å‹è¾“å‡º logitsï¼š
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Input preprocessing We saw in [*ChapterÂ 3*](CH3.xhtml#x1-350003), [*Fundamentals
    of Deep Learning*](CH3.xhtml#x1-350003) that the **Fast-Gradient** **Sign Method**
    (**FGSM**) allowed us to fool a neural network. By slightly changing an image
    of a cat, we could make the model predict â€dogâ€ with 99.41% confidence. The idea
    here was that we could take the sign of the gradient of the loss with respect
    to the input, multiply it by a small value and add that noise to our image â€“ this
    moved our image away from our in-distribution class. By doing the opposite, that
    is, subtracting the noise from our image, we make the image more in-distribution.
    The authors of the ODIN paper show that this causes in-distribution images to
    have an even higher softmax score compared to out-of-distribution images. This
    means that we increase the difference between OOD and ID softmax scores, leading
    to a better OOD detection performance.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥é¢„å¤„ç† æˆ‘ä»¬åœ¨ [*ç¬¬ 3 ç« *](CH3.xhtml#x1-350003)ï¼Œ[*æ·±åº¦å­¦ä¹ åŸºç¡€*](CH3.xhtml#x1-350003) ä¸­çœ‹åˆ°ï¼Œ**å¿«é€Ÿæ¢¯åº¦**
    **ç¬¦å·æ–¹æ³•**ï¼ˆ**FGSM**ï¼‰ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ¬ºéª—ç¥ç»ç½‘ç»œã€‚é€šè¿‡ç¨å¾®æ”¹å˜ä¸€å¼ çŒ«çš„å›¾åƒï¼Œæˆ‘ä»¬å¯ä»¥è®©æ¨¡å‹ä»¥ 99.41% çš„ç½®ä¿¡åº¦é¢„æµ‹ä¸ºâ€œç‹—â€ã€‚è¿™é‡Œçš„æƒ³æ³•æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥è·å–æŸå¤±ç›¸å¯¹äºè¾“å…¥çš„æ¢¯åº¦ç¬¦å·ï¼Œå°†å…¶ä¹˜ä»¥ä¸€ä¸ªå°å€¼ï¼Œå¹¶å°†è¯¥å™ªå£°æ·»åŠ åˆ°å›¾åƒä¸­â€”â€”è¿™å°†æŠŠæˆ‘ä»¬çš„å›¾åƒä»åˆ†å¸ƒå†…ç±»åˆ«ä¸­ç§»åŠ¨ã€‚é€šè¿‡åšç›¸åçš„äº‹æƒ…ï¼Œå³ä»å›¾åƒä¸­å‡å»å™ªå£°ï¼Œæˆ‘ä»¬ä½¿å¾—å›¾åƒæ›´æ¥è¿‘åˆ†å¸ƒå†…ã€‚ODIN
    è®ºæ–‡çš„ä½œè€…è¡¨æ˜ï¼Œè¿™å¯¼è‡´åˆ†å¸ƒå†…å›¾åƒçš„ softmax åˆ†æ•°æ¯”åˆ†å¸ƒå¤–å›¾åƒæ›´é«˜ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¢åŠ äº† OOD å’Œ ID softmax åˆ†æ•°ä¹‹é—´çš„å·®å¼‚ï¼Œä»è€Œæé«˜äº†
    OOD æ£€æµ‹æ€§èƒ½ã€‚
- en: '![Ëœx = x âˆ’ ğœ€sign(âˆ’ âˆ‡x log SË†y(x;T)) ](img/file165.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![Ëœx = x âˆ’ ğœ€sign(âˆ’ âˆ‡x log SË†y(x;T)) ](img/file165.jpg)'
- en: Where *x* is an input image of which we subtract the perturbation magnitude
    *ğœ–* times the sign of the gradient of the cross-entropy loss with respect to the
    input. See [*ChapterÂ 3*](CH3.xhtml#x1-350003), [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003)
    for the TensorFlow implementation of this technique.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *x* æ˜¯è¾“å…¥å›¾åƒï¼Œæˆ‘ä»¬ä»ä¸­å‡å»æ‰°åŠ¨å¹…åº¦ *ğœ–* ä¹˜ä»¥äº¤å‰ç†µæŸå¤±ç›¸å¯¹äºè¾“å…¥çš„æ¢¯åº¦ç¬¦å·ã€‚æœ‰å…³è¯¥æŠ€æœ¯çš„ TensorFlow å®ç°ï¼Œè¯·å‚è§ [*ç¬¬ 3
    ç« *](CH3.xhtml#x1-350003)ï¼Œ[*æ·±åº¦å­¦ä¹ åŸºç¡€*](CH3.xhtml#x1-350003)ã€‚
- en: 'Although input preprocessing and temperature scaling are simple to implement,
    ODIN now requires two more hyperparameters to be tuned: the temperature for scaling
    the logits and *ğœ–* of the inverse of the fast gradient sign method. ODIN uses
    a separate out-of-distribution dataset to tune these hyperparameters (the validation
    set of the iSUN dataset: 8925 images).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¾“å…¥é¢„å¤„ç†å’Œæ¸©åº¦ç¼©æ”¾æ˜“äºå®ç°ï¼ŒODIN ç°åœ¨è¿˜éœ€è¦è°ƒèŠ‚ä¸¤ä¸ªè¶…å‚æ•°ï¼šç”¨äºç¼©æ”¾ logits çš„æ¸©åº¦å’Œ *ğœ–*ï¼ˆå¿«é€Ÿæ¢¯åº¦ç¬¦å·æ³•çš„é€†ï¼‰ã€‚ODIN ä½¿ç”¨ä¸€ä¸ªå•ç‹¬çš„åˆ†å¸ƒå¤–æ•°æ®é›†æ¥è°ƒèŠ‚è¿™äº›è¶…å‚æ•°ï¼ˆiSUN
    æ•°æ®é›†çš„éªŒè¯é›†ï¼š8925 å¼ å›¾åƒï¼‰ã€‚
- en: 'Mahalanobis: OOD Detection with intermediate features'
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: é©¬æ°è·ç¦»ï¼šä½¿ç”¨ä¸­é—´ç‰¹å¾è¿›è¡Œ OOD æ£€æµ‹
- en: 'In *A Simple Unified Framework for Detecting Out-of-Distribution Samples and*
    *Adversarial Attacks*, Kimin Lee et al. propose a different method to detect OOD
    input. The core of their method is the idea that each class of a classifier follows
    a multivariate Gaussian distribution in the feature space of a network. Given
    this idea, we can define *C* class-conditional Gaussian distributions with a tied
    covariance *Ïƒ*:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ã€Š*ä¸€ç§ç®€å•ç»Ÿä¸€çš„æ¡†æ¶ç”¨äºæ£€æµ‹åˆ†å¸ƒå¤–æ ·æœ¬å’Œ* *å¯¹æŠ—æ”»å‡»*ã€‹ä¸€æ–‡ä¸­ï¼ŒKimin Lee ç­‰äººæå‡ºäº†ä¸€ç§æ£€æµ‹ OOD è¾“å…¥çš„ä¸åŒæ–¹æ³•ã€‚è¯¥æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯æ¯ä¸ªç±»åˆ«çš„åˆ†ç±»å™¨åœ¨ç½‘ç»œçš„ç‰¹å¾ç©ºé—´ä¸­éµå¾ªå¤šå…ƒé«˜æ–¯åˆ†å¸ƒã€‚åŸºäºè¿™ä¸€æ€æƒ³ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰*C*ä¸ªç±»åˆ«æ¡ä»¶é«˜æ–¯åˆ†å¸ƒï¼Œå¹¶ä¸”å…·æœ‰å…±äº«çš„åæ–¹å·®
    *Ïƒ*ï¼š
- en: '![P(f(x) | y = c) = ğ’© (f(x) | Î¼c,Ïƒ ) ](img/file166.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![P(f(x) | y = c) = ğ’© (f(x) | Î¼c,Ïƒ ) ](img/file166.jpg)'
- en: 'Where *Î¼*[*c*] is the mean of the multivariate Gaussian distribution for each
    class *c*. This allows us to compute the empirical mean and covariance of each
    of these distributions for a given output of an intermediate layer, one for each
    class of our network. Based on the mean and covariance, we can compute the Mahalanobis
    distance of a single test image compared to our in-distribution data. We compute
    this for the class that is closest to the input image:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ *Î¼*[*c*] æ˜¯æ¯ä¸ªç±»åˆ« *c* çš„å¤šå…ƒé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ã€‚è¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿè®¡ç®—ç»™å®šä¸­é—´å±‚è¾“å‡ºçš„æ¯ä¸ªç±»åˆ«çš„ç»éªŒå‡å€¼å’Œåæ–¹å·®ã€‚åŸºäºå‡å€¼å’Œåæ–¹å·®ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å•ä¸ªæµ‹è¯•å›¾åƒä¸åˆ†å¸ƒå†…æ•°æ®çš„é©¬æ°è·ç¦»ã€‚æˆ‘ä»¬å¯¹ä¸è¾“å…¥å›¾åƒæœ€æ¥è¿‘çš„ç±»åˆ«è®¡ç®—è¯¥è·ç¦»ï¼š
- en: '![M (x) = max âˆ’ (f(x)âˆ’ ^Î¼c)âŠ¤ ^Ïƒâˆ’ 1(f(x)âˆ’ ^Î¼c) c ](img/file167.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![M (x) = max âˆ’ (f(x)âˆ’ ^Î¼c)âŠ¤ ^Ïƒâˆ’ 1(f(x)âˆ’ ^Î¼c) c ](img/file167.jpg)'
- en: This distance should be small for in-distribution images and large for out-of-distribution
    images.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºåˆ†å¸ƒå†…çš„å›¾åƒï¼Œè¿™ä¸ªè·ç¦»åº”è¯¥è¾ƒå°ï¼Œè€Œå¯¹äºåˆ†å¸ƒå¤–çš„å›¾åƒï¼Œè¿™ä¸ªè·ç¦»åº”è¯¥è¾ƒå¤§ã€‚
- en: '`numpy` has convenient functions to compute the mean and covariance of an array:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`numpy` æä¾›äº†æ–¹ä¾¿çš„å‡½æ•°æ¥è®¡ç®—æ•°ç»„çš„å‡å€¼å’Œåæ–¹å·®ï¼š'
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Given these, we can compute the Mahalanobis distance as such:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºäºè¿™äº›ï¼Œæˆ‘ä»¬å¯ä»¥æŒ‰å¦‚ä¸‹æ–¹å¼è®¡ç®—é©¬æ°è·ç¦»ï¼š
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The Mahalanobis distance computation does not require any retraining and is
    a relatively cheap operation to perform once you have stored the mean and (inverse
    of the) covariance of the classes for the features of a layer of your network.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: é©¬æ°è·ç¦»è®¡ç®—ä¸éœ€è¦ä»»ä½•é‡æ–°è®­ç»ƒï¼Œä¸€æ—¦ä½ å­˜å‚¨äº†ç½‘ç»œæŸä¸€å±‚ç‰¹å¾çš„å‡å€¼å’Œï¼ˆåæ–¹å·®çš„é€†çŸ©é˜µï¼‰ï¼Œè¿™æ˜¯ä¸€é¡¹ç›¸å¯¹å»‰ä»·çš„æ“ä½œã€‚
- en: To improve the performance of the method, the authors show that we can also
    apply the input preprocessing as mentioned in the ODIN paper, or compute and then
    average the Mahalanobis distances extracted from multiple layers of the network.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æé«˜æ–¹æ³•çš„æ€§èƒ½ï¼Œä½œè€…è¡¨æ˜æˆ‘ä»¬è¿˜å¯ä»¥åº”ç”¨ ODIN è®ºæ–‡ä¸­æåˆ°çš„è¾“å…¥é¢„å¤„ç†ï¼Œæˆ–è€…è®¡ç®—å¹¶å¹³å‡ä»ç½‘ç»œå¤šä¸ªå±‚æå–çš„é©¬æ°è·ç¦»ã€‚
- en: 8.3 Being robust against dataset shift
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 æŠµæŠ—æ•°æ®é›†åç§»
- en: We already encountered dataset shift in [*ChapterÂ 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep* *Learning*](CH3.xhtml#x1-350003). As a reminder, dataset
    shift is a common problem in machine learning that happens when the joint distribution
    *P*(*X,Y* ) of inputs *X* and outputs *Y* differs between the model training stage
    and model inference stage (for example, when testing the model or when running
    it in a production environment). Covariate shift is a specific case of dataset
    shift where only the distribution of the inputs changes but the conditional distribution
    *P*(*Y* |*X*) stays constant.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨[*ç¬¬ 3 ç« *](CH3.xhtml#x1-350003)ã€Š*æ·±åº¦å­¦ä¹ åŸºç¡€*ã€‹ä¸­å·²ç»é‡åˆ°è¿‡æ•°æ®é›†åç§»ã€‚æé†’ä¸€ä¸‹ï¼Œæ•°æ®é›†åç§»æ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªå¸¸è§é—®é¢˜ï¼Œå‘ç”Ÿåœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µå’Œæ¨¡å‹æ¨ç†é˜¶æ®µï¼ˆä¾‹å¦‚ï¼Œåœ¨æµ‹è¯•æ¨¡å‹æˆ–åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿è¡Œæ—¶ï¼‰è¾“å…¥
    *X* å’Œè¾“å‡º *Y* çš„è”åˆåˆ†å¸ƒ *P*(*X,Y*) ä¸åŒçš„æƒ…å†µä¸‹ã€‚åå˜é‡åç§»æ˜¯æ•°æ®é›†åç§»çš„ä¸€ä¸ªç‰¹å®šæ¡ˆä¾‹ï¼Œå…¶ä¸­åªæœ‰è¾“å…¥çš„åˆ†å¸ƒå‘ç”Ÿå˜åŒ–ï¼Œè€Œæ¡ä»¶åˆ†å¸ƒ *P*(*Y*
    |*X*) ä¿æŒä¸å˜ã€‚
- en: Dataset shift is present in most production environments because of the difficulty
    of including all possible inference conditions during training and because most
    data is not static but changes over time. The input data can shift along many
    different dimensions in a production environment. Geographic and temporal dataset
    shift are two common forms of shift. Imagine, for example, you have trained your
    model on data taken from one geographical region (for example, Europe) and then
    apply the model in a different geographical region (for example, Latin America).
    Similarly, a model could be trained on data from the years between 2010 and 2020
    and then applied on production data taken from today.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†åç§»åœ¨å¤§å¤šæ•°ç”Ÿäº§ç¯å¢ƒä¸­æ™®éå­˜åœ¨ï¼Œå› ä¸ºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾ˆéš¾åŒ…å«æ‰€æœ‰å¯èƒ½çš„æ¨ç†æ¡ä»¶ï¼Œè€Œä¸”å¤§å¤šæ•°æ•°æ®ä¸æ˜¯é™æ€çš„ï¼Œè€Œæ˜¯éšç€æ—¶é—´å‘ç”Ÿå˜åŒ–ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¾“å…¥æ•°æ®å¯èƒ½æ²¿ç€è®¸å¤šä¸åŒçš„ç»´åº¦å‘ç”Ÿåç§»ã€‚åœ°ç†å’Œæ—¶é—´æ•°æ®é›†åç§»æ˜¯ä¸¤ç§å¸¸è§çš„åç§»å½¢å¼ã€‚ä¾‹å¦‚ï¼Œå‡è®¾ä½ å·²åœ¨ä¸€ä¸ªåœ°ç†åŒºåŸŸï¼ˆä¾‹å¦‚æ¬§æ´²ï¼‰è·å¾—çš„æ•°æ®ä¸Šè®­ç»ƒäº†æ¨¡å‹ï¼Œç„¶åå°†æ¨¡å‹åº”ç”¨äºå¦ä¸€ä¸ªåœ°ç†åŒºåŸŸï¼ˆä¾‹å¦‚æ‹‰ä¸ç¾æ´²ï¼‰ã€‚ç±»ä¼¼åœ°ï¼Œæ¨¡å‹å¯èƒ½æ˜¯åœ¨2010åˆ°2020å¹´é—´çš„æ•°æ®ä¸Šè®­ç»ƒçš„ï¼Œç„¶ååº”ç”¨äºä»Šå¤©çš„ç”Ÿäº§æ•°æ®ã€‚
- en: We will see that in such data shift scenarios, models often perform worse on
    the new shifted data than on their original training distribution. We will also
    see how vanilla neural networks usually do not indicate when the input data deviates
    from the training distribution. Finally, we will explore how various methods introduced
    in this book can be used to indicate dataset shift via uncertainty estimates and
    how these methods can make the models more robust. The following code example
    will be focused on an image classification problem. It should be noted, however,
    that the insights tend to generalize to other domains (such as natural language
    processing) and tasks (such as regression).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†çœ‹åˆ°ï¼Œåœ¨è¿™æ ·çš„æ•°æ®åç§»åœºæ™¯ä¸­ï¼Œæ¨¡å‹åœ¨æ–°çš„åç§»æ•°æ®ä¸Šçš„è¡¨ç°é€šå¸¸æ¯”åœ¨åŸå§‹è®­ç»ƒåˆ†å¸ƒä¸Šçš„è¡¨ç°å·®ã€‚æˆ‘ä»¬è¿˜å°†çœ‹åˆ°ï¼Œæ™®é€šç¥ç»ç½‘ç»œé€šå¸¸æ— æ³•æŒ‡ç¤ºè¾“å…¥æ•°æ®ä½•æ—¶åç¦»è®­ç»ƒåˆ†å¸ƒã€‚æœ€åï¼Œæˆ‘ä»¬å°†æ¢è®¨æœ¬ä¹¦ä¸­ä»‹ç»çš„å„ç§æ–¹æ³•å¦‚ä½•é€šè¿‡ä¸ç¡®å®šæ€§ä¼°è®¡æ¥æŒ‡ç¤ºæ•°æ®é›†åç§»ï¼Œä»¥åŠè¿™äº›æ–¹æ³•å¦‚ä½•å¢å¼ºæ¨¡å‹çš„é²æ£’æ€§ã€‚ä»¥ä¸‹ä»£ç ç¤ºä¾‹å°†é›†ä¸­åœ¨å›¾åƒåˆ†ç±»é—®é¢˜ä¸Šã€‚ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›è§è§£é€šå¸¸å¯ä»¥æ¨å¹¿åˆ°å…¶ä»–é¢†åŸŸï¼ˆå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ï¼‰å’Œä»»åŠ¡ï¼ˆå¦‚å›å½’ï¼‰ã€‚
- en: 8.3.1 Measuring a modelâ€™s response to dataset shift
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 æµ‹é‡æ¨¡å‹å¯¹æ•°æ®é›†åç§»çš„å“åº”
- en: 'Assuming that we have a training dataset and a separate test set, how can we
    measure a modelâ€™s ability to signal to us when the data has shifted? In order
    to do so, it would be necessary to have an additional test set where the data
    has been shifted to check how the model reacts to the dataset shift. One commonly
    applied way to create such a data shift test set for images was originally suggested
    by Dan Hendrycks and Thomas Dietterich in 2019 and others. The idea is straightforward:
    take the images from your initial test set, then apply different image quality
    corruptions at different severity levels to them. Hendrycks and Dietterich proposed
    a set of 15 different types of image quality corruptions, ranging from image noise,
    blur, weather corruptions (such as fog and snow), and digital corruption. Each
    corruption type has five levels of severity, ranging from 1 (mild corruption)
    to 5 (severe corruption). *Figure* [*8.4*](#x1-143002r4) shows what the image
    of a kitty looks like initially (left) and after applying shot noise corruption
    to the image, either at severity level 1 (middle) or severity level 5 (right).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†å’Œä¸€ä¸ªå•ç‹¬çš„æµ‹è¯•é›†ï¼Œæˆ‘ä»¬å¦‚ä½•è¡¡é‡æ¨¡å‹åœ¨æ•°æ®å‘ç”Ÿåç§»æ—¶æ˜¯å¦èƒ½åŠæ—¶ååº”ï¼Ÿä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªé¢å¤–çš„æµ‹è¯•é›†ï¼Œå…¶ä¸­æ•°æ®å·²ç»å‘ç”Ÿåç§»ï¼Œä»¥æ£€æŸ¥æ¨¡å‹å¦‚ä½•å“åº”æ•°æ®é›†åç§»ã€‚ä¸€ä¸ªå¸¸ç”¨çš„åˆ›å»ºæ•°æ®åç§»æµ‹è¯•é›†çš„æ–¹æ³•æœ€åˆç”±Dan
    Hendrycksã€Thomas DietterichåŠå…¶ä»–äººäº2019å¹´æå‡ºã€‚è¿™ä¸ªæ–¹æ³•å¾ˆç®€å•ï¼šä»ä½ çš„åˆå§‹æµ‹è¯•é›†ä¸­å–å‡ºå›¾åƒï¼Œç„¶åå¯¹å…¶åº”ç”¨ä¸åŒç¨‹åº¦çš„å›¾åƒè´¨é‡æŸåã€‚Hendryckså’ŒDietterichæå‡ºäº†ä¸€å¥—åŒ…å«15ç§ä¸åŒç±»å‹å›¾åƒè´¨é‡æŸåçš„æ–¹æ³•ï¼Œæ¶µç›–äº†å›¾åƒå™ªå£°ã€æ¨¡ç³Šã€å¤©æ°”æŸåï¼ˆå¦‚é›¾éœ¾å’Œé›ªï¼‰ä»¥åŠæ•°å­—æŸåç­‰ç±»å‹ã€‚æ¯ç§æŸåç±»å‹éƒ½æœ‰äº”ä¸ªä¸¥é‡ç¨‹åº¦çº§åˆ«ï¼Œä»1ï¼ˆè½»åº¦æŸåï¼‰åˆ°5ï¼ˆä¸¥é‡æŸåï¼‰ã€‚*å›¾*
    [*8.4*](#x1-143002r4)å±•ç¤ºäº†ä¸€åªå°çŒ«çš„å›¾åƒæœ€åˆæ ·å­ï¼ˆå·¦ä¾§ï¼‰ä»¥åŠåœ¨å›¾åƒä¸Šæ–½åŠ å™ªå£°æŸååçš„æ•ˆæœï¼Œåˆ†åˆ«æ˜¯ä¸¥é‡ç¨‹åº¦ä¸º1ï¼ˆä¸­é—´ï¼‰å’Œ5ï¼ˆå³ä¾§ï¼‰çš„æƒ…å†µã€‚
- en: '![PIC](img/file168.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file168.png)'
- en: 'FigureÂ 8.4: Generating artificial dataset shift by applying image quality corruptions
    at different levels of corruption severity'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.4ï¼šé€šè¿‡åœ¨ä¸åŒæŸåä¸¥é‡ç¨‹åº¦ä¸‹åº”ç”¨å›¾åƒè´¨é‡æŸåæ¥ç”Ÿæˆäººå·¥æ•°æ®é›†åç§»
- en: All these image quality corruptions can be generated conveniently using the
    `imgaug` Python package. The following code assumes that we have an image called
    â€kitty.pngâ€ on disk. We load the image using the PIL package. We then specify
    the corruption type (for example, `ShotNoise`) via the name of the corruption
    function, and then apply the corruption function to the image, using either severity
    level 1 or 5 by passing the corresponding integer to the key-worded `severity`
    argument.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›å›¾åƒè´¨é‡æŸåå¯ä»¥æ–¹ä¾¿åœ°ä½¿ç”¨`imgaug` PythonåŒ…ç”Ÿæˆã€‚ä»¥ä¸‹ä»£ç å‡è®¾æˆ‘ä»¬ç£ç›˜ä¸Šæœ‰ä¸€ä¸ªåä¸º"kitty.png"çš„å›¾åƒã€‚æˆ‘ä»¬ä½¿ç”¨PILåŒ…åŠ è½½å›¾åƒã€‚ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡æŸåå‡½æ•°çš„åç§°æŒ‡å®šæŸåç±»å‹ï¼ˆä¾‹å¦‚ï¼Œ`ShotNoise`ï¼‰ï¼Œå¹¶ä½¿ç”¨é€šè¿‡ä¼ é€’ç›¸åº”æ•´æ•°ç»™å…³é”®å­—å‚æ•°`severity`æ¥åº”ç”¨æŸåå‡½æ•°ï¼Œé€‰æ‹©ä¸¥é‡æ€§ç­‰çº§1æˆ–5ã€‚
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The advantage of generating data shift this way is that it can be applied to
    a wide range of computer vision problems and datasets. Some of the few prerequisites
    for applying this method are that the data consists of images and that these image
    quality corruptions have not been used during training, for example, for data
    augmentation. Furthermore, by setting the severity of the image quality corruption,
    we gain control over the degree of the dataset shift. This allows us to measure
    how the model reacts to different degrees of dataset shift. We can measure both
    how performance changes in response to dataset shift and how calibration (introduced
    in [*ChapterÂ 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian Inference*](CH2.xhtml#x1-250002))
    changes. We would expect that models trained with Bayesian methods or extensions
    should be better calibrated, which means that they are able to indicate to us
    that the data has shifted in comparison to training and they are thus less certain
    in their output.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿™ç§æ–¹å¼ç”Ÿæˆæ•°æ®åç§»çš„ä¼˜åŠ¿åœ¨äºï¼Œå®ƒå¯ä»¥åº”ç”¨äºå¹¿æ³›çš„è®¡ç®—æœºè§†è§‰é—®é¢˜å’Œæ•°æ®é›†ã€‚åº”ç”¨è¿™ç§æ–¹æ³•çš„å°‘æ•°å‰ææ¡ä»¶æ˜¯æ•°æ®ç”±å›¾åƒç»„æˆï¼Œå¹¶ä¸”åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰ä½¿ç”¨è¿‡è¿™äº›å›¾åƒè´¨é‡æŸåï¼ˆä¾‹å¦‚ï¼Œç”¨äºæ•°æ®å¢å¼ºï¼‰ã€‚æ­¤å¤–ï¼Œé€šè¿‡è®¾ç½®å›¾åƒè´¨é‡æŸåçš„ä¸¥é‡æ€§ï¼Œæˆ‘ä»¬å¯ä»¥æ§åˆ¶æ•°æ®é›†åç§»çš„ç¨‹åº¦ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¡¡é‡æ¨¡å‹å¯¹ä¸åŒç¨‹åº¦çš„æ•°æ®é›†åç§»çš„ååº”ã€‚æˆ‘ä»¬å¯ä»¥è¡¡é‡æ€§èƒ½å¦‚ä½•éšç€æ•°æ®é›†åç§»è€Œå˜åŒ–ï¼Œä»¥åŠæ ¡å‡†ï¼ˆåœ¨[*ç¬¬äºŒç« *](CH2.xhtml#x1-250002)ï¼Œ[*è´å¶æ–¯æ¨æ–­åŸºç¡€*](CH2.xhtml#x1-250002)ä¸­å¼•å…¥ï¼‰å¦‚ä½•å˜åŒ–ã€‚æˆ‘ä»¬é¢„è®¡ä½¿ç”¨è´å¶æ–¯æ–¹æ³•æˆ–æ‰©å±•æ–¹æ³•è®­ç»ƒçš„æ¨¡å‹ä¼šæœ‰æ›´å¥½çš„æ ¡å‡†ï¼Œè¿™æ„å‘³ç€å®ƒä»¬èƒ½å¤Ÿå‘Šè¯‰æˆ‘ä»¬æ•°æ®ç›¸è¾ƒäºè®­ç»ƒæ—¶å·²ç»å‘ç”Ÿäº†åç§»ï¼Œå› æ­¤å®ƒä»¬å¯¹è¾“å‡ºçš„ä¿¡å¿ƒè¾ƒä½ã€‚
- en: 8.3.2 Revealing dataset shift with Bayesian methods
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 ä½¿ç”¨è´å¶æ–¯æ–¹æ³•æ­ç¤ºæ•°æ®é›†åç§»
- en: In the following code example, we will look at two of the BDL methods (Bayes
    by backprop and deep ensembles) that we have encountered in the book so far and
    see how they perform during the kind of artificial dataset shift described previously.
    We will compare their performance against a vanilla neural network.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹çš„ä»£ç ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†æŸ¥çœ‹ä¹¦ä¸­åˆ°ç›®å‰ä¸ºæ­¢é‡åˆ°çš„ä¸¤ç§BDLæ–¹æ³•ï¼ˆåŸºäºåå‘ä¼ æ’­çš„è´å¶æ–¯æ–¹æ³•å’Œæ·±åº¦é›†æˆï¼‰ï¼Œå¹¶è§‚å¯Ÿå®ƒä»¬åœ¨å‰é¢æè¿°çš„äººå·¥æ•°æ®é›†åç§»ä¸‹çš„è¡¨ç°ã€‚æˆ‘ä»¬å°†å®ƒä»¬çš„è¡¨ç°ä¸æ™®é€šçš„ç¥ç»ç½‘ç»œè¿›è¡Œæ¯”è¾ƒã€‚
- en: 'Step 1: Preparing the environment'
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 1ï¼šå‡†å¤‡ç¯å¢ƒ
- en: We start the example by importing a list of packages. This includes TensorFlow
    and TensorFlow Probability, which we will use for building and training the neural
    networks; `numpy` for manipulating numerical arrays (such as calculating the mean);
    `Seaborn`, `Matplotlib`, and `pandas` for plotting; `cv2` and `imgaug` for loading
    and manipulating images; as well as `scikit-learn` for calculating the accuracy
    of our models.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡å¯¼å…¥ä¸€ç³»åˆ—åŒ…æ¥å¼€å§‹è¿™ä¸ªç¤ºä¾‹ã€‚è¿™äº›åŒ…åŒ…æ‹¬ç”¨äºæ„å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œçš„TensorFlowå’ŒTensorFlow Probabilityï¼›ç”¨äºå¤„ç†æ•°å€¼æ•°ç»„ï¼ˆå¦‚è®¡ç®—å‡å€¼ï¼‰çš„`numpy`ï¼›ç”¨äºç»˜å›¾çš„`Seaborn`ã€`Matplotlib`å’Œ`pandas`ï¼›ç”¨äºåŠ è½½å’Œå¤„ç†å›¾åƒçš„`cv2`å’Œ`imgaug`ï¼›ä»¥åŠç”¨äºè®¡ç®—æ¨¡å‹å‡†ç¡®åº¦çš„`scikit-learn`ã€‚
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In preparation for the training, we will load the `CIFAR10` dataset, which is
    an image classification dataset, and specify the names of the different classes.
    The dataset consists of 10 different classes, the names of which we specify in
    the following code, and provides 50,000 training images as well as 10,000 test
    images. Weâ€™ll also save the number of training images, which will be needed to
    train the model with the reparameterization trick later.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬å°†åŠ è½½`CIFAR10`æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå›¾åƒåˆ†ç±»æ•°æ®é›†ï¼Œå¹¶æŒ‡å®šä¸åŒç±»åˆ«çš„åç§°ã€‚è¯¥æ•°æ®é›†åŒ…å«10ä¸ªä¸åŒçš„ç±»åˆ«ï¼Œæˆ‘ä»¬å°†åœ¨ä»¥ä¸‹ä»£ç ä¸­æŒ‡å®šè¿™äº›ç±»åˆ«çš„åç§°ï¼Œå¹¶æä¾›50,000ä¸ªè®­ç»ƒå›¾åƒå’Œ10,000ä¸ªæµ‹è¯•å›¾åƒã€‚æˆ‘ä»¬è¿˜å°†ä¿å­˜è®­ç»ƒå›¾åƒçš„æ•°é‡ï¼Œè¿™å°†åœ¨ç¨åä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§è®­ç»ƒæ¨¡å‹æ—¶ç”¨åˆ°ã€‚
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Step 2: Defining and training the models'
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 2ï¼šå®šä¹‰å’Œè®­ç»ƒæ¨¡å‹
- en: After this prep work, we can define and train our models. We start by creating
    two functions to define and build the CNN. We will use these functions both for
    the vanilla neural network and the deep ensemble. The first function simply combines
    a convolutional layer with a max-pooling layer â€“ a common approach that we introduced
    in [*ChapterÂ 3*](CH3.xhtml#x1-350003), [*Fundamentals of Deep* *Learning*](CH3.xhtml#x1-350003).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é¡¹å‡†å¤‡å·¥ä½œå®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰å¹¶è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸¤ä¸ªå‡½æ•°æ¥å®šä¹‰å’Œæ„å»ºCNNã€‚æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸¤ä¸ªå‡½æ•°æ¥æ„å»ºæ™®é€šç¥ç»ç½‘ç»œå’Œæ·±åº¦é›†æˆç½‘ç»œã€‚ç¬¬ä¸€ä¸ªå‡½æ•°ç®€å•åœ°å°†å·ç§¯å±‚ä¸æœ€å¤§æ± åŒ–å±‚ç»“åˆèµ·æ¥â€”â€”è¿™æ˜¯ä¸€ç§å¸¸è§çš„åšæ³•ï¼Œæˆ‘ä»¬åœ¨[*ç¬¬3ç« *](CH3.xhtml#x1-350003)ã€Šæ·±åº¦å­¦ä¹ åŸºç¡€ã€‹ä¸­ä»‹ç»è¿‡ã€‚
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The second function then uses several convolutional/max-pooling blocks in sequence
    and follows this sequence with a final dense layer:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªå‡½æ•°åˆ™ä¾æ¬¡ä½¿ç”¨å¤šä¸ªå·ç§¯/æœ€å¤§æ± åŒ–å—ï¼Œå¹¶åœ¨æ­¤åºåˆ—åé¢è·Ÿç€ä¸€ä¸ªæœ€ç»ˆçš„å¯†é›†å±‚ï¼š
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We also create two analogous functions to define and build the network using
    Bayes By Backprop (BBB) based on the reparameterization trick. The strategy is
    the same as for the vanilla neural network, just that weâ€™ll now use the convolutional
    and dense layers from the TensorFlow Probability package instead of the TensorFlow
    package. The convolutional/max-pooling blocks are then defined as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸¤ä¸ªç±»ä¼¼çš„å‡½æ•°ï¼Œç”¨äºåŸºäºé‡æ–°å‚æ•°åŒ–æŠ€å·§å®šä¹‰å’Œæ„å»ºä½¿ç”¨Bayes By Backpropï¼ˆBBBï¼‰çš„ç½‘ç»œã€‚ç­–ç•¥ä¸æ™®é€šç¥ç»ç½‘ç»œç›¸åŒï¼Œåªä¸è¿‡æˆ‘ä»¬ç°åœ¨å°†ä½¿ç”¨æ¥è‡ªTensorFlow
    ProbabilityåŒ…çš„å·ç§¯å±‚å’Œå¯†é›†å±‚ï¼Œè€Œä¸æ˜¯TensorFlowåŒ…ã€‚å·ç§¯/æœ€å¤§æ± åŒ–å—å®šä¹‰å¦‚ä¸‹ï¼š
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And the final network is defined like this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆçš„ç½‘ç»œå®šä¹‰å¦‚ä¸‹ï¼š
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can then train the vanilla neural network:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥è®­ç»ƒæ™®é€šç¥ç»ç½‘ç»œï¼š
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can also train the ensemble, with five ensemble members:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥è®­ç»ƒä¸€ä¸ªäº”æˆå‘˜çš„é›†æˆæ¨¡å‹ï¼š
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: And finally, we train the BBB model. Note that we train the BBB model for 15
    instead of 10 epochs, given that it takes a little longer to converge.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬è®­ç»ƒBBBæ¨¡å‹ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬å°†è®­ç»ƒBBBæ¨¡å‹15ä¸ªepochï¼Œè€Œä¸æ˜¯10ä¸ªepochï¼Œå› ä¸ºå®ƒæ”¶æ•›çš„æ—¶é—´ç¨é•¿ã€‚
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Step 3: Obtaining predictions'
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ­¥éª¤3ï¼šè·å–é¢„æµ‹ç»“æœ
- en: 'Now that we have three trained models, we can use them for predictions on the
    hold-out test set. To keep computations at a manageable degree, in this example,
    we will focus on the first 1,000 images in the test set:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æœ‰äº†ä¸‰ä¸ªè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨å®ƒä»¬å¯¹ä¿ç•™çš„æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ã€‚ä¸ºäº†ä¿æŒè®¡ç®—çš„å¯æ§æ€§ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºæµ‹è¯•é›†ä¸­çš„å‰1000å¼ å›¾åƒï¼š
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If we want to measure the response to dataset shift, we first need to apply
    the artificial image corruptions to the dataset. To do that, we first specify
    a set of functions from the `imgaug` package. From their names, one can infer
    what type of corruption each of these functions implements: for example, the function
    `icl.GaussianNoise` corrupts an image by applying Gaussian noise to it. We also
    infer the number of corruption types from the number of functions and save it
    in the `NUM_TYPES` variable. Finally, we set the number of corruption levels to
    5.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³è¦è¡¡é‡æ•°æ®é›†åç§»çš„å“åº”ï¼Œé¦–å…ˆéœ€è¦å¯¹æ•°æ®é›†åº”ç”¨äººå·¥å›¾åƒæŸåã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆæŒ‡å®šä¸€ç»„æ¥è‡ª`imgaug`åŒ…çš„å‡½æ•°ã€‚ä»è¿™äº›å‡½æ•°çš„åç§°ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ¨æ–­å‡ºæ¯ä¸ªå‡½æ•°å®ç°çš„æŸåç±»å‹ï¼šä¾‹å¦‚ï¼Œå‡½æ•°`icl.GaussianNoise`é€šè¿‡å‘å›¾åƒåº”ç”¨é«˜æ–¯å™ªå£°æ¥æŸåå›¾åƒã€‚æˆ‘ä»¬è¿˜é€šè¿‡å‡½æ•°çš„æ•°é‡æ¨æ–­å‡ºæŸåç±»å‹çš„æ•°é‡ï¼Œå¹¶å°†å…¶ä¿å­˜åœ¨`NUM_TYPES`å˜é‡ä¸­ã€‚æœ€åï¼Œæˆ‘ä»¬å°†æŸåçº§åˆ«è®¾ç½®ä¸º5ã€‚
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Equipped with these functions, let us now corrupt images. In the next code block,
    we loop over the different corruption levels and types. We collect all corrupted
    images in the aptly named `corrupted_images` variable.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: é…å¤‡äº†è¿™äº›å‡½æ•°åï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å¼€å§‹æŸåå›¾åƒäº†ã€‚åœ¨ä¸‹ä¸€ä¸ªä»£ç å—ä¸­ï¼Œæˆ‘ä»¬éå†ä¸åŒçš„æŸåçº§åˆ«å’Œç±»å‹ï¼Œå¹¶å°†æ‰€æœ‰æŸåçš„å›¾åƒæ”¶é›†åˆ°åä¸º`corrupted_images`çš„å˜é‡ä¸­ã€‚
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'With the three models trained and the corrupted images at hand, we can now
    see how our models react to dataset shift of different levels. We will first obtain
    predictions on the corrupted images from the three models. To run inference, we
    need to reshape the corrupted images to an input shape that is accepted by the
    models for inferences. At the moment, the images are still stored on different
    axes for the corruption types and levels. We change this by reshaping the `corrupted_images`
    array:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®­ç»ƒå®Œä¸‰ä¸ªæ¨¡å‹å¹¶è·å¾—æŸåå›¾åƒåï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥çœ‹åˆ°æ¨¡å‹å¯¹ä¸åŒçº§åˆ«æ•°æ®é›†åç§»çš„ååº”ã€‚æˆ‘ä»¬å°†é¦–å…ˆè·å–ä¸‰ä¸ªæ¨¡å‹å¯¹æŸåå›¾åƒçš„é¢„æµ‹ç»“æœã€‚ä¸ºäº†è¿›è¡Œæ¨ç†ï¼Œæˆ‘ä»¬éœ€è¦å°†æŸåçš„å›¾åƒè°ƒæ•´ä¸ºæ¨¡å‹æ¥å—çš„è¾“å…¥å½¢çŠ¶ã€‚ç›®å‰ï¼Œè¿™äº›å›¾åƒä»ç„¶å­˜å‚¨åœ¨é’ˆå¯¹æŸåç±»å‹å’Œçº§åˆ«çš„ä¸åŒè½´ä¸Šã€‚æˆ‘ä»¬é€šè¿‡é‡æ–°è°ƒæ•´`corrupted_images`æ•°ç»„æ¥æ”¹å˜è¿™ä¸€ç‚¹ï¼š
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then we can perform inference with the vanilla CNN model, both on the original
    images and the corrupted images. After we have inferred the model predictions,
    we reshape the predictions in order to separate predictions for the corruption
    types and levels:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ™®é€šCNNæ¨¡å‹å¯¹åŸå§‹å›¾åƒå’Œè…èš€å›¾åƒè¿›è¡Œæ¨ç†ã€‚åœ¨æ¨ç†æ¨¡å‹é¢„æµ‹åï¼Œæˆ‘ä»¬å°†é¢„æµ‹ç»“æœé‡å¡‘ï¼Œä»¥ä¾¿åˆ†ç¦»è…èš€ç±»å‹å’Œçº§åˆ«çš„é¢„æµ‹ï¼š
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To run inference with the ensemble model, we first define a prediction function
    to avoid code duplication. This function handles the looping over the different
    member models of the ensemble and combines the different predictions in the end
    via averaging:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªé¢„æµ‹å‡½æ•°ä»¥é¿å…ä»£ç é‡å¤ã€‚æ­¤å‡½æ•°å¤„ç†å¯¹é›†æˆä¸­ä¸åŒæˆå‘˜æ¨¡å‹çš„å¾ªç¯ï¼Œå¹¶æœ€ç»ˆé€šè¿‡å¹³å‡å°†ä¸åŒçš„é¢„æµ‹ç»“æœç»“åˆèµ·æ¥ï¼š
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Equipped with this function, we can perform inference with the ensemble model
    on both the original and corrupted images:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: é…å¤‡äº†è¿™ä¸ªå‡½æ•°åï¼Œæˆ‘ä»¬å¯ä»¥å¯¹åŸå§‹å›¾åƒå’Œè…èš€å›¾åƒä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œæ¨ç†ï¼š
- en: '[PRE30]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Just as for the ensemble model, we write an inference function for the BBB
    model, which handles the iteration over different sampling loops and collects
    and combines the results:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒå¯¹äºé›†æˆæ¨¡å‹ä¸€æ ·ï¼Œæˆ‘ä»¬ä¸ºBBBæ¨¡å‹ç¼–å†™äº†ä¸€ä¸ªæ¨ç†å‡½æ•°ï¼Œè¯¥å‡½æ•°å¤„ç†ä¸åŒé‡‡æ ·å¾ªç¯çš„è¿­ä»£ï¼Œå¹¶æ”¶é›†å¹¶ç»“åˆç»“æœï¼š
- en: '[PRE31]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We then put this function to use to obtain the BBB model predictions on the
    original and corrupted images. We sample from the BBB model 20 times:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™ä¸ªå‡½æ•°è·å–BBBæ¨¡å‹åœ¨åŸå§‹å›¾åƒå’Œè…èš€å›¾åƒä¸Šçš„é¢„æµ‹ã€‚æˆ‘ä»¬ä»BBBæ¨¡å‹ä¸­é‡‡æ ·20æ¬¡ï¼š
- en: '[PRE32]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can convert the predictions of the three models to predicted classes and
    associated confidence scores by returning the index of the class with the maximum
    softmax score and the maximum softmax score, respectively:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿”å›å…·æœ‰æœ€å¤§softmaxå¾—åˆ†çš„ç±»åˆ«ç´¢å¼•å’Œæœ€å¤§softmaxå¾—åˆ†ï¼Œåˆ†åˆ«å°†ä¸‰ä¸ªæ¨¡å‹çš„é¢„æµ‹è½¬æ¢ä¸ºé¢„æµ‹ç±»åˆ«åŠå…¶ç›¸å…³çš„ç½®ä¿¡åº¦å¾—åˆ†ï¼š
- en: '[PRE33]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This function can then be applied to get the predicted classes and confidence
    scores for our three models:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¯ä»¥åº”ç”¨æ­¤å‡½æ•°æ¥è·å–æˆ‘ä»¬ä¸‰ä¸ªæ¨¡å‹çš„é¢„æµ‹ç±»åˆ«å’Œç½®ä¿¡åº¦å¾—åˆ†ï¼š
- en: '[PRE34]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let us visualize what these predicted classes and confidence scores look like
    for the three models on a selected image showing an automobile. For plotting,
    we first reshape the array that contains the corrupted images to a more convenient
    format:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯è§†åŒ–è¿™ä¸‰ä¸ªæ¨¡å‹åœ¨ä¸€å¼ å±•ç¤ºæ±½è½¦çš„é€‰å®šå›¾åƒä¸Šé¢„æµ‹çš„ç±»åˆ«å’Œç½®ä¿¡åº¦å¾—åˆ†ã€‚ä¸ºäº†ç»˜å›¾ï¼Œæˆ‘ä»¬é¦–å…ˆå°†åŒ…å«è…èš€å›¾åƒçš„æ•°ç»„é‡å¡‘ä¸ºæ›´æ–¹ä¾¿çš„æ ¼å¼ï¼š
- en: '[PRE35]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We then plot the selected automobile image with the first three corruption types
    in the list across all five corruption levels. For each combination, we display
    in the image title the predicted score of each model and in squared parentheses
    the predicted class. The plot is shown in *FigureÂ *[*8.5*](#x1-147361r5).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ç»˜åˆ¶äº†åˆ—è¡¨ä¸­å‰ä¸‰ç§è…èš€ç±»å‹çš„é€‰å®šæ±½è½¦å›¾åƒï¼Œæ¶µç›–æ‰€æœ‰äº”ä¸ªè…èš€çº§åˆ«ã€‚å¯¹äºæ¯ç§ç»„åˆï¼Œæˆ‘ä»¬åœ¨å›¾åƒæ ‡é¢˜ä¸­æ˜¾ç¤ºæ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹å¾—åˆ†ï¼Œå¹¶åœ¨æ–¹æ‹¬å·ä¸­æ˜¾ç¤ºé¢„æµ‹ç±»åˆ«ã€‚è¯¥å›¾å¦‚*å›¾*[*8.5*](#x1-147361r5)æ‰€ç¤ºã€‚
- en: '![PIC](img/file169.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file169.png)'
- en: 'FigureÂ 8.5: An automobile image has been corrupted with different corruption
    types (rows) and levels (columns, severity increases from left to right)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.5ï¼šä¸€å¼ æ±½è½¦å›¾åƒå·²ç»è¢«ä¸åŒçš„è…èš€ç±»å‹ï¼ˆè¡Œï¼‰å’Œçº§åˆ«ï¼ˆåˆ—ï¼Œä¸¥é‡ç¨‹åº¦ä»å·¦åˆ°å³å¢åŠ ï¼‰è…èš€
- en: 'The code continues:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£ç ç»§ç»­ï¼š
- en: '[PRE36]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '*FigureÂ *[*8.5*](#x1-147361r5) only shows results for a single image, so we
    should not read too much into these results. However, we can already observe that
    the prediction scores for the two Bayesian methods (and especially the ensemble
    method) tend to be less extreme than for the vanilla neural network, which has
    predicted scores as high as 0.95\. Furthermore, we see that, for all three models,
    prediction scores usually decrease as the corruption level increases. This is
    expected: given that the car in the image becomes less discernible with more corruption,
    we would want the model to become less confident as well. In particular, the ensemble
    method shows a nice and consistent decrease in predicted scores with increased
    corruption levels.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾*[*8.5*](#x1-147361r5)åªæ˜¾ç¤ºäº†å•å¼ å›¾åƒçš„ç»“æœï¼Œå› æ­¤æˆ‘ä»¬ä¸åº”è¿‡åº¦è§£è¯»è¿™äº›ç»“æœã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å·²ç»å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œä¸¤ä¸ªè´å¶æ–¯æ–¹æ³•ï¼ˆå°¤å…¶æ˜¯é›†æˆæ–¹æ³•ï¼‰çš„é¢„æµ‹å¾—åˆ†é€šå¸¸æ¯”æ™®é€šç¥ç»ç½‘ç»œæ›´ä¸æç«¯ï¼Œåè€…çš„é¢„æµ‹å¾—åˆ†é«˜è¾¾0.95ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çœ‹åˆ°ï¼Œå¯¹äºæ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹ï¼Œé¢„æµ‹å¾—åˆ†é€šå¸¸éšç€è…èš€çº§åˆ«çš„å¢åŠ è€Œé™ä½ã€‚è¿™æ˜¯é¢„æœŸçš„ï¼šç”±äºå›¾åƒä¸­çš„æ±½è½¦åœ¨è…èš€è¶Šä¸¥é‡æ—¶å˜å¾—è¶Šéš¾ä»¥è¾¨è®¤ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹çš„ç½®ä¿¡åº¦ä¹Ÿä¼šéšä¹‹é™ä½ã€‚ç‰¹åˆ«æ˜¯ï¼Œé›†æˆæ–¹æ³•åœ¨å¢åŠ è…èš€çº§åˆ«æ—¶æ˜¾ç¤ºå‡ºäº†é¢„æµ‹å¾—åˆ†çš„æ˜æ˜¾ä¸”ä¸€è‡´çš„ä¸‹é™ã€‚'
- en: 'Step 4: Measuring accuracy'
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬4æ­¥ï¼šè¡¡é‡å‡†ç¡®æ€§
- en: Are some models more robust to dataset shift than other models? We can answer
    this question by looking at the accuracy of the three models at different corruptions
    levels. It is expected that all models will show lower accuracy as the input image
    becomes more and more corrupted. However, more robust models should lose less
    in accuracy as the corruptions become more severe.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº›æ¨¡å‹æ¯”å…¶ä»–æ¨¡å‹æ›´èƒ½é€‚åº”æ•°æ®é›†çš„åç§»å—ï¼Ÿæˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹ä¸‰ç§æ¨¡å‹åœ¨ä¸åŒæŸåæ°´å¹³ä¸‹çš„å‡†ç¡®æ€§æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚é¢„è®¡æ‰€æœ‰æ¨¡å‹åœ¨è¾“å…¥å›¾åƒé€æ¸æŸåæ—¶å‡†ç¡®æ€§ä¼šé™ä½ã€‚ç„¶è€Œï¼Œæ›´é²æ£’çš„æ¨¡å‹åœ¨æŸåå˜å¾—æ›´ä¸¥é‡æ—¶ï¼Œå‡†ç¡®æ€§ä¸‹é™åº”è¯¥è¾ƒå°‘ã€‚
- en: 'First, we can calculate the accuracy of the three models on the original test
    images:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ä¸‰ç§æ¨¡å‹åœ¨åŸå§‹æµ‹è¯•å›¾åƒä¸Šçš„å‡†ç¡®æ€§ï¼š
- en: '[PRE37]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We can store these accuracies in a list of dictionaries, which will make it
    easier to plot them systematically. We pass the respective name of the models.
    For corruption `type` and `level`, we pass `0` because these are the accuracies
    on the original images.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†è¿™äº›å‡†ç¡®æ€§å­˜å‚¨åœ¨å­—å…¸åˆ—è¡¨ä¸­ï¼Œè¿™å°†ä½¿æˆ‘ä»¬æ›´å®¹æ˜“ç³»ç»Ÿåœ°ç»˜åˆ¶å®ƒä»¬ã€‚æˆ‘ä»¬ä¼ é€’ç›¸åº”çš„æ¨¡å‹åç§°ã€‚å¯¹äºæŸåçš„`ç±»å‹`å’Œ`çº§åˆ«`ï¼Œæˆ‘ä»¬ä¼ é€’`0`ï¼Œå› ä¸ºè¿™äº›æ˜¯åŸå§‹å›¾åƒä¸Šçš„å‡†ç¡®æ€§ã€‚
- en: '[PRE38]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we calculate the accuracy of the three models on the different corruption
    type by corruption level combinations. We also append the results to the list
    of accuracies that we started previously:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è®¡ç®—ä¸‰ç§æ¨¡å‹åœ¨ä¸åŒæŸåç±»å‹å’ŒæŸåçº§åˆ«ç»„åˆä¸‹çš„å‡†ç¡®æ€§ã€‚æˆ‘ä»¬è¿˜å°†ç»“æœé™„åŠ åˆ°ä¹‹å‰å¼€å§‹çš„å‡†ç¡®æ€§åˆ—è¡¨ä¸­ï¼š
- en: '[PRE39]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We can then plot the distributions of accuracies for the original images and
    the increasingly corrupted images. We first convert the list of dictionaries to
    a pandas dataframe. This has the advantage that the dataframe can be directly
    passed to the plotting package `seaborn`. This allows us to specify that we want
    to plot the different modelsâ€™ results in different hues.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶åŸå§‹å›¾åƒå’Œé€æ¸æŸåå›¾åƒçš„å‡†ç¡®æ€§åˆ†å¸ƒã€‚æˆ‘ä»¬é¦–å…ˆå°†å­—å…¸åˆ—è¡¨è½¬æ¢ä¸ºpandas dataframeã€‚è¿™æœ‰ä¸€ä¸ªä¼˜åŠ¿ï¼Œå³dataframeå¯ä»¥ç›´æ¥ä¼ é€’ç»™ç»˜å›¾åº“`seaborn`ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥æŒ‡å®šä¸åŒæ¨¡å‹çš„ç»“æœä»¥ä¸åŒè‰²è°ƒè¿›è¡Œç»˜åˆ¶ã€‚
- en: '[PRE40]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This produces the following output:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼šç”Ÿæˆä»¥ä¸‹è¾“å‡ºï¼š
- en: '![PIC](img/file170.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file170.png)'
- en: 'FigureÂ 8.6: Accuracy for the three different models (different hues) for the
    original test images (level 0) as well as for increasing levels of corruption
    (level 1-5)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.6ï¼šä¸‰ç§ä¸åŒæ¨¡å‹ï¼ˆä¸åŒè‰²è°ƒï¼‰åœ¨åŸå§‹æµ‹è¯•å›¾åƒï¼ˆçº§åˆ« 0ï¼‰ä»¥åŠä¸åŒç¨‹åº¦çš„æŸåï¼ˆçº§åˆ« 1-5ï¼‰ä¸Šçš„å‡†ç¡®æ€§
- en: 'The resulting plot is shown in *FigureÂ *[*8.6*](#x1-148194r6). We can see that,
    on the original test images, the vanilla and BBB model have comparable accuracy,
    while the ensemble model has slightly higher accuracy. As corruption is introduced,
    we see that the performance of the vanilla neural network is worse (often significantly)
    than the performance of the ensemble or BBB. This relative improvement in performance
    of the BDL models demonstrates the regularization effect of Bayesian methods:
    these methods are able to capture the distribution of the data more effectively,
    making them more robust to perturbations. BBB exhibits particular resilience to
    increasing amounts of data corruption, demonstrating a key benefit of variational
    learning.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœå›¾å¦‚*å›¾*[*8.6*](#x1-148194r6)æ‰€ç¤ºã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨åŸå§‹æµ‹è¯•å›¾åƒä¸Šï¼Œæ™®é€šæ¨¡å‹å’ŒBBBæ¨¡å‹çš„å‡†ç¡®æ€§ç›¸å½“ï¼Œè€Œé›†æˆæ¨¡å‹çš„å‡†ç¡®æ€§ç¨é«˜ã€‚éšç€æŸåçš„å¼•å…¥ï¼Œæˆ‘ä»¬çœ‹åˆ°æ™®é€šç¥ç»ç½‘ç»œçš„è¡¨ç°æ¯”é›†æˆæ¨¡å‹æˆ–BBBæ¨¡å‹æ›´å·®ï¼ˆé€šå¸¸æ˜¯æ˜¾è‘—å·®ï¼‰ã€‚BDLæ¨¡å‹æ€§èƒ½çš„ç›¸å¯¹æå‡å±•ç¤ºäº†è´å¶æ–¯æ–¹æ³•çš„æ­£åˆ™åŒ–æ•ˆåº”ï¼šè¿™äº›æ–¹æ³•èƒ½æ›´æœ‰æ•ˆåœ°æ•æ‰æ•°æ®çš„åˆ†å¸ƒï¼Œä½¿å…¶å¯¹æ‰°åŠ¨æ›´åŠ é²æ£’ã€‚BBBæ¨¡å‹ç‰¹åˆ«èƒ½æŠµå¾¡æ•°æ®æŸåçš„å¢åŠ ï¼Œå±•ç¤ºäº†å˜åˆ†å­¦ä¹ çš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿ã€‚
- en: 'Step 5: Measuring calibration'
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 5ï¼šè¡¡é‡æ ¡å‡†
- en: Looking at accuracy is a good way to determine how robust a model is against
    dataset shift. But it does not really tell us whether the models are capable of
    signalling to us (via lower confidence scores) when the dataset has shifted and
    the models have become less confident in their output. This question can be answered
    by looking at how well models remain calibrated under dataset shift. We introduced
    calibration and expected calibration errors on a conceptual level back in [*ChapterÂ 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003). We are now going to put
    these concepts into practice to understand whether models adjust their confidence
    appropriately as the images become increasingly corrupted and hard to predict.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥çœ‹å‡†ç¡®åº¦æ˜¯ç¡®å®šæ¨¡å‹åœ¨æ•°æ®é›†å˜åŒ–ä¸‹çš„é²æ£’æ€§çš„ä¸€ç§å¥½æ–¹æ³•ã€‚ä½†å®ƒå¹¶æ²¡æœ‰çœŸæ­£å‘Šè¯‰æˆ‘ä»¬æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿé€šè¿‡è¾ƒä½çš„ç½®ä¿¡åº¦åˆ†æ•°ï¼ˆå½“æ•°æ®é›†å‘ç”Ÿå˜åŒ–æ—¶ï¼‰æé†’æˆ‘ä»¬ï¼Œå¹¶ä¸”æ¨¡å‹åœ¨è¾“å‡ºæ—¶å˜å¾—ä¸é‚£ä¹ˆè‡ªä¿¡ã€‚è¿™ä¸ªé—®é¢˜å¯ä»¥é€šè¿‡è§‚å¯Ÿæ¨¡å‹åœ¨æ•°æ®é›†å˜åŒ–ä¸‹çš„æ ¡å‡†è¡¨ç°æ¥å›ç­”ã€‚æˆ‘ä»¬åœ¨[*ç¬¬3ç« *](CH3.xhtml#x1-350003)çš„ã€Šæ·±åº¦å­¦ä¹ åŸºç¡€ã€‹ä¸­å·²ç»ä»‹ç»äº†æ ¡å‡†å’ŒæœŸæœ›æ ¡å‡†è¯¯å·®çš„æ¦‚å¿µã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†æŠŠè¿™äº›æ¦‚å¿µä»˜è¯¸å®è·µï¼Œä»¥ç†è§£å½“å›¾åƒå˜å¾—è¶Šæ¥è¶Šå—æŸä¸”éš¾ä»¥é¢„æµ‹æ—¶ï¼Œæ¨¡å‹æ˜¯å¦é€‚å½“åœ°è°ƒæ•´äº†å®ƒä»¬çš„ç½®ä¿¡åº¦ã€‚
- en: 'First, we will implement the Expected Calibration Error (ECE) introduced in
    [*ChapterÂ 3*](CH3.xhtml#x1-350003), [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003)
    as a scalar measure of calibration:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å°†å®ç°[*ç¬¬3ç« *](CH3.xhtml#x1-350003)ã€Šæ·±åº¦å­¦ä¹ åŸºç¡€ã€‹ä¸­ä»‹ç»çš„æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰ï¼Œä½œä¸ºæ ¡å‡†çš„æ ‡é‡è¡¡é‡æ ‡å‡†ï¼š
- en: '[PRE41]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can then calculate ECE for the three models on the original test images.
    We set the number of bins to `10`, which is a common choice for calculating ECE:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ä¸‰ä¸ªæ¨¡å‹åœ¨åŸå§‹æµ‹è¯•å›¾åƒä¸Šçš„æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰ã€‚æˆ‘ä»¬å°†ç®±å­çš„æ•°é‡è®¾ç½®ä¸º`10`ï¼Œè¿™æ˜¯è®¡ç®—ECEæ—¶å¸¸ç”¨çš„é€‰æ‹©ï¼š
- en: '[PRE42]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Just as we did for the accuracies earlier, we will store the calibration results
    in a list of dictionaries, which will make it easier to plot them:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæˆ‘ä»¬ä¹‹å‰å¤„ç†å‡†ç¡®åº¦ä¸€æ ·ï¼Œæˆ‘ä»¬å°†æŠŠæ ¡å‡†ç»“æœå­˜å‚¨åœ¨ä¸€ä¸ªå­—å…¸åˆ—è¡¨ä¸­ï¼Œè¿™æ ·å°±æ›´å®¹æ˜“ç»˜åˆ¶å®ƒä»¬ï¼š
- en: '[PRE43]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we calculate the expected calibration error of the three models on the
    different corruption types by corruption level combinations. We also append the
    results to the list of calibration results that we started previously:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ ¹æ®ä¸åŒçš„è…èš€ç±»å‹å’Œè…èš€çº§åˆ«ç»„åˆï¼Œè®¡ç®—ä¸‰ä¸ªæ¨¡å‹çš„æœŸæœ›æ ¡å‡†è¯¯å·®ã€‚æˆ‘ä»¬è¿˜å°†ç»“æœé™„åŠ åˆ°ä¹‹å‰å¼€å§‹çš„æ ¡å‡†ç»“æœåˆ—è¡¨ä¸­ï¼š
- en: '[PRE44]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Finally, we plot the calibration results in a boxplot, again using `pandas`
    and `seaborn`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`pandas`å’Œ`seaborn`å†æ¬¡ç»˜åˆ¶æ ¡å‡†ç»“æœçš„ç®±å½¢å›¾ï¼š
- en: '[PRE45]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The calibration results are shown in *FigureÂ *[*8.7*](#x1-149394r7). We can
    see that, on the original test images, all three models have relatively low calibration
    error, with the ensemble model performing slightly worse than the two other models.
    As we apply increasing levels of dataset shift, we can see that calibration error
    increases by a lot for the vanilla model. For the two Bayesian methods, calibration
    error also increases but by much less than for the vanilla model. This means that
    the Bayesian methods are better at indicating (via lower confidence scores) when
    the dataset has shifted and that the Bayesian models become relatively less confident
    in their output with increased corruption (as they should).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: æ ¡å‡†ç»“æœæ˜¾ç¤ºåœ¨*å›¾*[*8.7*](#x1-149394r7)ä¸­ã€‚æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨åŸå§‹æµ‹è¯•å›¾åƒä¸Šï¼Œæ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹çš„æ ¡å‡†è¯¯å·®éƒ½æ¯”è¾ƒä½ï¼Œé›†æˆæ¨¡å‹çš„è¡¨ç°ç•¥é€Šè‰²äºå¦å¤–ä¸¤ä¸ªæ¨¡å‹ã€‚éšç€æ•°æ®é›†å˜åŒ–ç¨‹åº¦çš„å¢åŠ ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¼ ç»Ÿæ¨¡å‹çš„æ ¡å‡†è¯¯å·®å¤§å¹…å¢åŠ ã€‚å¯¹äºä¸¤ç§è´å¶æ–¯æ–¹æ³•ï¼Œæ ¡å‡†è¯¯å·®ä¹Ÿå¢åŠ äº†ï¼Œä½†æ¯”ä¼ ç»Ÿæ¨¡å‹è¦å°‘å¾—å¤šã€‚è¿™æ„å‘³ç€è´å¶æ–¯æ–¹æ³•åœ¨æ•°æ®é›†å‘ç”Ÿå˜åŒ–æ—¶èƒ½å¤Ÿæ›´å¥½åœ°é€šè¿‡è¾ƒä½çš„ç½®ä¿¡åº¦åˆ†æ•°æ¥æŒ‡ç¤ºï¼ˆå³æ¨¡å‹åœ¨è¾“å‡ºæ—¶å˜å¾—ç›¸å¯¹ä¸é‚£ä¹ˆè‡ªä¿¡ï¼Œéšç€è…èš€ç¨‹åº¦çš„å¢åŠ ï¼Œè¡¨ç°å‡ºè¿™ç§ç‰¹å¾ï¼‰ã€‚
- en: '![PIC](img/file171.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file171.png)'
- en: 'FigureÂ 8.7: Expected calibration error for the three different models for the
    original test images (level 0) as well as for increasing levels of corruption
    (level 1-5)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.7ï¼šä¸‰ç§ä¸åŒæ¨¡å‹åœ¨åŸå§‹æµ‹è¯•å›¾åƒï¼ˆçº§åˆ«0ï¼‰å’Œä¸åŒè…èš€çº§åˆ«ï¼ˆçº§åˆ«1-5ï¼‰ä¸Šçš„æœŸæœ›æ ¡å‡†è¯¯å·®
- en: In the next section, we will look into data selection.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºæ•°æ®é€‰æ‹©ã€‚
- en: 8.4 Using data selection via uncertainty to keep models fresh
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 ä½¿ç”¨é€šè¿‡ä¸ç¡®å®šæ€§è¿›è¡Œçš„æ•°æ®é€‰æ‹©æ¥ä¿æŒæ¨¡å‹çš„æ›´æ–°
- en: 'We saw at the beginning of the chapter that we can use uncertainties to figure
    out whether data is part of the training data or not. We can expand on this idea
    in the context of an area of machine learning called **active learning**. The
    promise of active learning is that a model can learn more effectively on less
    data if we have a way to control the type of data it is trained on. Conceptually,
    this makes sense: if we train a model on data that is not of sufficient quality,
    it will also not perform well. Active learning is a way to guide the learning
    process and data a model is trained on by providing functions that can acquire
    data from a pool of data that is not part of the training data. By iteratively
    selecting the right data from the pool, we can train a model that performs better
    than if we had chosen the data from the pool at random.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æœ¬ç« å¼€å¤´çœ‹åˆ°ï¼Œèƒ½å¤Ÿä½¿ç”¨ä¸ç¡®å®šæ€§æ¥åˆ¤æ–­æ•°æ®æ˜¯å¦æ˜¯è®­ç»ƒæ•°æ®çš„ä¸€éƒ¨åˆ†ã€‚åœ¨ä¸»åŠ¨å­¦ä¹ è¿™ä¸€æœºå™¨å­¦ä¹ é¢†åŸŸçš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥æ‰©å±•è¿™ä¸ªæƒ³æ³•ã€‚ä¸»åŠ¨å­¦ä¹ çš„æ‰¿è¯ºæ˜¯ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿæ§åˆ¶æ¨¡å‹è®­ç»ƒçš„æ•°æ®ç±»å‹ï¼Œæ¨¡å‹å¯ä»¥åœ¨æ›´å°‘çš„æ•°æ®ä¸Šæ›´æœ‰æ•ˆåœ°å­¦ä¹ ã€‚ä»æ¦‚å¿µä¸Šè®²ï¼Œè¿™æ˜¯æœ‰é“ç†çš„ï¼šå¦‚æœæˆ‘ä»¬åœ¨è´¨é‡ä¸è¶³çš„æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œå®ƒçš„è¡¨ç°ä¹Ÿä¸ä¼šå¾ˆå¥½ã€‚ä¸»åŠ¨å­¦ä¹ æ˜¯ä¸€ç§é€šè¿‡æä¾›å¯ä»¥ä»ä¸å±äºè®­ç»ƒæ•°æ®çš„æ•°æ®æ± ä¸­è·å–æ•°æ®çš„å‡½æ•°ï¼Œæ¥å¼•å¯¼æ¨¡å‹å­¦ä¹ è¿‡ç¨‹å’Œè®­ç»ƒæ•°æ®çš„æ–¹æ³•ã€‚é€šè¿‡åå¤ä»æ•°æ®æ± ä¸­é€‰æ‹©æ­£ç¡®çš„æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒå‡ºæ¯”éšæœºé€‰æ‹©æ•°æ®æ—¶è¡¨ç°æ›´å¥½çš„æ¨¡å‹ã€‚
- en: 'Active learning can be used in many modern-day systems where there is a ton
    of unlabeled data available and we need to carefully select the amount of data
    we want to label. An example is an autonomous driving system: the camera on the
    car records a lot of data, but there is typically no budget to label all of it.
    By carefully choosing the most informative data points, we can improve the model
    performance at a lower cost than when we would have randomly selected the data
    to label. In the context of active learning, estimating uncertainties plays an
    important role. A model will typically learn more from areas of the data distribution
    that were predicted with low confidence. Letâ€™s look at a case study to see how
    we can use uncertainty in the context of active learning.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸»åŠ¨å­¦ä¹ å¯ä»¥åº”ç”¨äºè®¸å¤šç°ä»£ç³»ç»Ÿï¼Œåœ¨è¿™äº›ç³»ç»Ÿä¸­æœ‰å¤§é‡æœªæ ‡è®°çš„æ•°æ®å¯ä¾›ä½¿ç”¨ï¼Œæˆ‘ä»¬éœ€è¦ä»”ç»†é€‰æ‹©æƒ³è¦æ ‡è®°çš„æ•°æ®é‡ã€‚ä¸€ä¸ªä¾‹å­æ˜¯è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿï¼šè½¦ä¸Šçš„æ‘„åƒå¤´è®°å½•äº†å¤§é‡æ•°æ®ï¼Œä½†é€šå¸¸æ²¡æœ‰é¢„ç®—æ ‡è®°æ‰€æœ‰æ•°æ®ã€‚é€šè¿‡ä»”ç»†é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ•°æ®ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥æ¯”éšæœºé€‰æ‹©æ•°æ®æ ‡è®°æ—¶æ›´ä½çš„æˆæœ¬æé«˜æ¨¡å‹æ€§èƒ½ã€‚åœ¨ä¸»åŠ¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œä¼°è®¡ä¸ç¡®å®šæ€§å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æ¨¡å‹é€šå¸¸ä¼šä»æ•°æ®åˆ†å¸ƒä¸­é‚£äº›ä½ç½®ä¿¡åº¦é¢„æµ‹çš„åŒºåŸŸå­¦åˆ°æ›´å¤šã€‚è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªæ¡ˆä¾‹ç ”ç©¶æ¥çœ‹çœ‹å¦‚ä½•åœ¨ä¸»åŠ¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹ä½¿ç”¨ä¸ç¡®å®šæ€§ã€‚
- en: 'In this case study, we will reproduce the results from a fundamental active
    learning paper: *Deep Bayesian Active Learning with Image Data* (2017). We will
    use the `MNIST` dataset and train a model on more and more data, where we select
    the data points to add to our training set via an uncertainty method. In this
    case, we will use epistemic uncertainty to select the most informative data points.
    Images with high epistemic uncertainty should be images that the model did not
    see before; the uncertainty can be reduced by adding more of them. As a comparison,
    we will also select data points at random.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç°ä¸€ç¯‡åŸºç¡€æ€§ä¸»åŠ¨å­¦ä¹ è®ºæ–‡çš„ç»“æœï¼š*åŸºäºå›¾åƒæ•°æ®çš„æ·±åº¦è´å¶æ–¯ä¸»åŠ¨å­¦ä¹ *ï¼ˆ2017ï¼‰ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`MNIST`æ•°æ®é›†ï¼Œå¹¶åœ¨è¶Šæ¥è¶Šå¤šçš„æ•°æ®ä¸Šè®­ç»ƒæ¨¡å‹ï¼Œé€šè¿‡ä¸ç¡®å®šæ€§æ–¹æ³•é€‰æ‹©è¦æ·»åŠ åˆ°è®­ç»ƒé›†ä¸­çš„æ•°æ®ç‚¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è®¤çŸ¥ä¸ç¡®å®šæ€§æ¥é€‰æ‹©æœ€å…·ä¿¡æ¯é‡çš„æ•°æ®ç‚¹ã€‚å…·æœ‰é«˜è®¤çŸ¥ä¸ç¡®å®šæ€§çš„å›¾åƒåº”è¯¥æ˜¯æ¨¡å‹ä¹‹å‰æ²¡æœ‰è§è¿‡çš„å›¾åƒï¼›é€šè¿‡å¢åŠ æ›´å¤šè¿™æ ·çš„å›¾åƒï¼Œå¯ä»¥å‡å°‘ä¸ç¡®å®šæ€§ã€‚ä½œä¸ºå¯¹æ¯”ï¼Œæˆ‘ä»¬è¿˜å°†éšæœºé€‰æ‹©æ•°æ®ç‚¹ã€‚
- en: 'Step 1: Preparing our dataset'
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡æ•°æ®é›†
- en: 'We will start by creating our functions to load the dataset. The dataset functions
    need the following library imports:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é¦–å…ˆåˆ›å»ºåŠ è½½æ•°æ®é›†çš„å‡½æ•°ã€‚æ•°æ®é›†å‡½æ•°éœ€è¦ä»¥ä¸‹åº“å¯¼å…¥ï¼š
- en: '[PRE46]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: As our total dataset will have quite a few components, we will create a small
    dataclass to easily access all the different parts of our dataset. We will also
    modify the `__repr__` function of the dataclass. This allows us to print the content
    of the dataset in a more readable format.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬çš„æ€»æ•°æ®é›†å°†åŒ…å«ç›¸å½“å¤šçš„ç»„ä»¶ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå°çš„`dataclass`ï¼Œä»¥ä¾¿è½»æ¾è®¿é—®æ•°æ®é›†çš„ä¸åŒéƒ¨åˆ†ã€‚æˆ‘ä»¬è¿˜å°†ä¿®æ”¹`__repr__`å‡½æ•°ï¼Œä½¿å…¶èƒ½å¤Ÿä»¥æ›´æ˜“è¯»çš„æ ¼å¼æ‰“å°æ•°æ®é›†å†…å®¹ã€‚
- en: '[PRE47]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We can then define our function to load our standard dataset.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥å®šä¹‰å‡½æ•°æ¥åŠ è½½æ ‡å‡†æ•°æ®é›†ã€‚
- en: '[PRE48]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Initially, we will start training on just 20 samples from the *MNIST* dataset.
    We will then acquire 10 data points at a time, and retrain our model again. To
    help our model a little bit in the beginning, we will make sure that the 20 data
    points are balanced across the different classes of the dataset. The following
    function gives us the indices that we can use to create the initial 20 samples,
    2 samples of each class:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åˆï¼Œæˆ‘ä»¬å°†ä»*MNIST*æ•°æ®é›†ä¸­ä»…ä½¿ç”¨20ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚ç„¶åæˆ‘ä»¬æ¯æ¬¡è·å–10ä¸ªæ•°æ®ç‚¹ï¼Œå¹¶é‡æ–°è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚ä¸ºäº†åœ¨å¼€å§‹æ—¶å¸®åŠ©æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†ç¡®ä¿è¿™20ä¸ªæ•°æ®ç‚¹åœ¨æ•°æ®é›†çš„ä¸åŒç±»åˆ«ä¹‹é—´æ˜¯å¹³è¡¡çš„ã€‚ä»¥ä¸‹å‡½æ•°ç»™å‡ºäº†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨çš„ç´¢å¼•ï¼Œç”¨äºåˆ›å»ºåˆå§‹çš„20ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªç±»åˆ«2ä¸ªæ ·æœ¬ï¼š
- en: '[PRE49]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We can then define a small function to actually get our initial dataset:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªå°å‡½æ•°ï¼Œå®é™…è·å–æˆ‘ä»¬çš„åˆå§‹æ•°æ®é›†ï¼š
- en: '[PRE50]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Step 2: Setting up our configuration'
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ­¥éª¤2ï¼šè®¾ç½®é…ç½®
- en: Before we start to build our model and create the active learning loop, we define
    a small configuration `dataclass` to store some main variables we might want to
    play around with when running our active learning script. Creating configuration
    classes such as these allows you to play around with different parameters.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬å¼€å§‹æ„å»ºæ¨¡å‹å¹¶åˆ›å»ºä¸»åŠ¨å­¦ä¹ å¾ªç¯ä¹‹å‰ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå°çš„é…ç½®`dataclass`æ¥å­˜å‚¨ä¸€äº›åœ¨è¿è¡Œä¸»åŠ¨å­¦ä¹ è„šæœ¬æ—¶å¯èƒ½æƒ³è¦è°ƒæ•´çš„ä¸»è¦å˜é‡ã€‚åˆ›å»ºè¿™æ ·çš„é…ç½®ç±»ä½¿ä½ å¯ä»¥çµæ´»è°ƒæ•´ä¸åŒçš„å‚æ•°ã€‚
- en: '[PRE51]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Step 3: Defining the model'
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ­¥éª¤3ï¼šå®šä¹‰æ¨¡å‹
- en: We can now define our model. We will use a small, simple CNN with dropout.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å°å‹CNNå¹¶åŠ å…¥Dropoutã€‚
- en: '[PRE52]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Step 4: Defining the uncertainty functions'
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ­¥éª¤4ï¼šå®šä¹‰ä¸ç¡®å®šæ€§å‡½æ•°
- en: As indicated, we will use epistemic uncertainty (also knowledge uncertainty)
    as our main uncertainty function to acquire new samples. Letâ€™s define the function
    to compute epistemic uncertainty over our predictions. We assume that the input
    predictions (`divds`) are of shape `n_images`, `n_predictions`, `n_classes`. We
    first define a function to compute total uncertainty. Given an ensemble of model
    predictions, this can be defined as the entropy of the averaged predictions of
    the ensemble.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼ˆä¹Ÿç§°ä¸ºçŸ¥è¯†ä¸ç¡®å®šæ€§ï¼‰ä½œä¸ºæˆ‘ä»¬ä¸»è¦çš„ä¸ç¡®å®šæ€§å‡½æ•°æ¥è·å–æ–°æ ·æœ¬ã€‚è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æˆ‘ä»¬é¢„æµ‹çš„è®¤çŸ¥ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬å‡è®¾è¾“å…¥çš„é¢„æµ‹ï¼ˆ`divds`ï¼‰çš„å½¢çŠ¶ä¸º`n_images`ï¼Œ`n_predictions`ï¼Œ`n_classes`ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æ€»ä¸ç¡®å®šæ€§ã€‚ç»™å®šä¸€ä¸ªé›†æˆæ¨¡å‹çš„é¢„æµ‹ï¼Œå®ƒå¯ä»¥å®šä¹‰ä¸ºé›†æˆå¹³å‡é¢„æµ‹çš„ç†µã€‚
- en: '[PRE53]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We then define data uncertainty (or aleatoric uncertainty), which for an ensemble
    is the average of the entropy of each ensemble member.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å®šä¹‰æ•°æ®ä¸ç¡®å®šæ€§ï¼ˆæˆ–ç§°ä¸ºéšæœºä¸ç¡®å®šæ€§ï¼‰ï¼Œå¯¹äºä¸€ä¸ªé›†æˆæ¨¡å‹æ¥è¯´ï¼Œå®ƒæ˜¯æ¯ä¸ªé›†æˆæˆå‘˜çš„ç†µçš„å¹³å‡å€¼ã€‚
- en: '[PRE54]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Finally, we have our knowledge (or epistemic) uncertainty, which is simply subtracting
    data uncertainty from the total uncertainty of the predictions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆï¼Œæˆ‘ä»¬å¾—åˆ°äº†æˆ‘ä»¬çš„çŸ¥è¯†ï¼ˆæˆ–è®¤çŸ¥ï¼‰ä¸ç¡®å®šæ€§ï¼Œè¿™å°±æ˜¯é€šè¿‡ä»é¢„æµ‹çš„æ€»ä¸ç¡®å®šæ€§ä¸­å‡å»æ•°æ®ä¸ç¡®å®šæ€§æ¥å¾—åˆ°çš„ã€‚
- en: '[PRE55]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'With these uncertainty functions defined, we can define the actual acquisition
    functions that take as main input our training data and our model. To acquire
    samples via knowledge uncertainty, we do the following:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰äº†è¿™äº›ä¸ç¡®å®šæ€§å‡½æ•°åï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰å®é™…çš„è·å–å‡½æ•°ï¼Œå®ƒä»¬çš„ä¸»è¦è¾“å…¥æ˜¯æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®å’Œæ¨¡å‹ã€‚ä¸ºäº†é€šè¿‡çŸ¥è¯†ä¸ç¡®å®šæ€§æ¥è·å–æ ·æœ¬ï¼Œæˆ‘ä»¬è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š
- en: Obtain our ensemble of predictions via MC dropout.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡MC Dropoutè·å–æˆ‘ä»¬çš„é›†æˆé¢„æµ‹ã€‚
- en: Compute the knowledge uncertainty values over this ensemble.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—è¿™ä¸ªé›†æˆæ¨¡å‹çš„çŸ¥è¯†ä¸ç¡®å®šæ€§å€¼ã€‚
- en: Sort the uncertainty values, get their index and return the indices of our training
    data with the highest epistemic uncertainty.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¯¹ä¸ç¡®å®šæ€§å€¼è¿›è¡Œæ’åºï¼Œè·å–å®ƒä»¬çš„ç´¢å¼•ï¼Œå¹¶è¿”å›æˆ‘ä»¬è®­ç»ƒæ•°æ®ä¸­å…·æœ‰æœ€é«˜è®¤çŸ¥ä¸ç¡®å®šæ€§çš„ç´¢å¼•ã€‚
- en: We can then, later on, reuse these indices to index into our training data and
    actually acquire the training samples we want to add.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œç¨åæˆ‘ä»¬å¯ä»¥é‡å¤ä½¿ç”¨è¿™äº›ç´¢å¼•æ¥ç´¢å¼•æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ï¼Œå®é™…ä¸Šè·å–æˆ‘ä»¬æƒ³è¦æ·»åŠ çš„è®­ç»ƒæ ·æœ¬ã€‚
- en: '[PRE56]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We obtain our MC dropout predictions as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼è·å¾—MC Dropouté¢„æµ‹ï¼š
- en: '[PRE57]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: To avoid running out of memory, we iterate over our training data in batches
    of six, where for every batch we compute our predictions `n_iter` times. To make
    sure that our predictions are varied, we set the modelâ€™s `training` parameter
    to `True`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é¿å…å†…å­˜æº¢å‡ºï¼Œæˆ‘ä»¬å°†è®­ç»ƒæ•°æ®åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹6ä¸ªæ ·æœ¬ï¼Œå¯¹äºæ¯ä¸€æ‰¹ï¼Œæˆ‘ä»¬å°†è®¡ç®—`n_iter`æ¬¡é¢„æµ‹ã€‚ä¸ºäº†ç¡®ä¿æˆ‘ä»¬çš„é¢„æµ‹å…·æœ‰å¤šæ ·æ€§ï¼Œæˆ‘ä»¬å°†æ¨¡å‹çš„`training`å‚æ•°è®¾ç½®ä¸º`True`ã€‚
- en: 'For our comparison, we define an acquisition function that returns a random
    number of indices as well:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„æ¯”è¾ƒï¼Œæˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ªè·å–å‡½æ•°ï¼Œè¯¥å‡½æ•°è¿”å›ä¸€ä¸ªéšæœºçš„ç´¢å¼•åˆ—è¡¨ï¼š
- en: '[PRE58]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Finally, we define a small function according to the *factory method pattern*
    to make sure that we can use the same function in our loop to use either the random
    acquisition function or knowledge uncertainty. Small factory functions such as
    these help to keep your code modular when you want to run the same code with different
    configurations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬æ ¹æ®*å·¥å‚æ–¹æ³•æ¨¡å¼*å®šä¹‰ä¸€ä¸ªå°å‡½æ•°ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬å¯ä»¥åœ¨å¾ªç¯ä¸­ä½¿ç”¨ç›¸åŒçš„å‡½æ•°ï¼Œä½¿ç”¨éšæœºé‡‡é›†å‡½æ•°æˆ–çŸ¥è¯†ä¸ç¡®å®šæ€§ã€‚åƒè¿™æ ·çš„å·¥å‚å°å‡½æ•°æœ‰åŠ©äºåœ¨ä½ æƒ³ç”¨ä¸åŒé…ç½®è¿è¡Œç›¸åŒä»£ç æ—¶ä¿æŒä»£ç æ¨¡å—åŒ–ã€‚
- en: '[PRE59]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Now that we have defined our acquisition functions, we are ready to actually
    define the loop that runs our active learning iterations.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®šä¹‰äº†é‡‡é›†å‡½æ•°ï¼Œæˆ‘ä»¬å·²ç»å‡†å¤‡å¥½å®é™…å®šä¹‰è¿è¡Œæˆ‘ä»¬ä¸»åŠ¨å­¦ä¹ è¿­ä»£çš„å¾ªç¯ã€‚
- en: 'Step 5: Defining the loop'
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬5æ­¥ï¼šå®šä¹‰å¾ªç¯
- en: Letâ€™s start by defining our configuration. In this case, we are using knowledge
    uncertainty as our uncertainty function. In a different loop, we will use a random
    acquisition function to compare the results of the loop we are about to define.
    We will start our dataset with 20 samples until we reach a total of 1,000 samples.
    Each model will be trained for 50 epochs and per iteration, we acquire 10 samples.
    To obtain our MC dropout predictions, we will run over our full training set (minus
    the already acquired samples) 100 times.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„é…ç½®ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨çŸ¥è¯†ä¸ç¡®å®šæ€§ä½œä¸ºæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§å‡½æ•°ã€‚åœ¨å¦ä¸€ä¸ªå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªéšæœºé‡‡é›†å‡½æ•°æ¥æ¯”è¾ƒæˆ‘ä»¬å³å°†å®šä¹‰çš„å¾ªç¯ç»“æœã€‚æˆ‘ä»¬å°†ä»20ä¸ªæ ·æœ¬å¼€å§‹æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œç›´åˆ°æˆ‘ä»¬è¾¾åˆ°1,000ä¸ªæ ·æœ¬ã€‚æ¯ä¸ªæ¨¡å‹å°†è®­ç»ƒ50ä¸ªepochï¼Œæ¯æ¬¡è¿­ä»£æˆ‘ä»¬è·å–10ä¸ªæ ·æœ¬ã€‚ä¸ºäº†è·å¾—æˆ‘ä»¬çš„MC
    dropouté¢„æµ‹ï¼Œæˆ‘ä»¬å°†åœ¨æ•´ä¸ªè®­ç»ƒé›†ï¼ˆå‡å»å·²è·å–çš„æ ·æœ¬ï¼‰ä¸Šè¿è¡Œ100æ¬¡ã€‚
- en: '[PRE60]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We can then get our data and define an empty dictionary to keep track of the
    test accuracy per iteration. We also create an empty list to keep track of the
    full list of indices we added to our training data.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥è·å–æ•°æ®ï¼Œå¹¶å®šä¹‰ä¸€ä¸ªç©ºå­—å…¸æ¥è·Ÿè¸ªæ¯æ¬¡è¿­ä»£çš„æµ‹è¯•å‡†ç¡®ç‡ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºè·Ÿè¸ªæˆ‘ä»¬æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­çš„æ‰€æœ‰ç´¢å¼•ã€‚
- en: '[PRE61]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We also assign a **universally unique identifier** (**UUID**) to our run to
    make sure we can discover it easily and do not overwrite the outcomes we save
    as part of our loop. We create the directory where we will save our data and save
    our configuration in that directory to ensure that we always know with what kind
    of configuration the data in our `model_dir` was created.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜ä¸ºæˆ‘ä»¬çš„è¿è¡Œåˆ†é…äº†ä¸€ä¸ª**å…¨çƒå”¯ä¸€æ ‡è¯†ç¬¦**ï¼ˆ**UUID**ï¼‰ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬å¯ä»¥è½»æ¾æ‰¾åˆ°å®ƒï¼Œå¹¶ä¸”ä¸ä¼šè¦†ç›–æˆ‘ä»¬ä½œä¸ºå¾ªç¯ä¸€éƒ¨åˆ†ä¿å­˜çš„ç»“æœã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç›®å½•æ¥ä¿å­˜æˆ‘ä»¬çš„æ•°æ®ï¼Œå¹¶å°†é…ç½®ä¿å­˜åˆ°è¯¥ç›®å½•ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬å§‹ç»ˆçŸ¥é“`model_dir`ä¸­çš„æ•°æ®æ˜¯ä½¿ç”¨ä½•ç§é…ç½®åˆ›å»ºçš„ã€‚
- en: '[PRE62]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We can now actually run our active learning loop. We will break this loop into
    three sections:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å®é™…è¿è¡Œæˆ‘ä»¬çš„ä¸»åŠ¨å­¦ä¹ å¾ªç¯ã€‚æˆ‘ä»¬å°†æŠŠè¿™ä¸ªå¾ªç¯åˆ†æˆä¸‰ä¸ªéƒ¨åˆ†ï¼š
- en: 'We define the loop and fit a model on the acquired samples:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰å¾ªç¯ï¼Œå¹¶åœ¨å·²è·å–çš„æ ·æœ¬ä¸Šæ‹Ÿåˆæ¨¡å‹ï¼š
- en: '[PRE63]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We then load the model with the best validation accuracy and update our dataset
    based on the acquisition function:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬åŠ è½½å…·æœ‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡çš„æ¨¡å‹ï¼Œå¹¶æ ¹æ®é‡‡é›†å‡½æ•°æ›´æ–°æˆ‘ä»¬çš„æ•°æ®é›†ï¼š
- en: '[PRE64]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We finally save the added images, compute the test accuracy, and save the results:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬ä¿å­˜å·²æ·»åŠ çš„å›¾ç‰‡ï¼Œè®¡ç®—æµ‹è¯•å‡†ç¡®ç‡ï¼Œå¹¶ä¿å­˜ç»“æœï¼š
- en: '[PRE65]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'In this loop, we defined a few small helper functions. First of all, we defined
    a callback for our model to save the model with the highest validation accuracy
    to our model directory:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå¾ªç¯ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€äº›å°çš„è¾…åŠ©å‡½æ•°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„æ¨¡å‹å®šä¹‰äº†ä¸€ä¸ªå›è°ƒï¼Œä»¥å°†å…·æœ‰æœ€é«˜éªŒè¯å‡†ç¡®ç‡çš„æ¨¡å‹ä¿å­˜åˆ°æˆ‘ä»¬çš„æ¨¡å‹ç›®å½•ï¼š
- en: '[PRE66]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We also defined a function to compute the accuracy of our test set:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æµ‹è¯•é›†çš„å‡†ç¡®ç‡ï¼š
- en: '[PRE67]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'And we defined two small functions to save the results per iteration:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å®šä¹‰äº†ä¸¤ä¸ªå°å‡½æ•°ï¼Œç”¨äºæ¯æ¬¡è¿­ä»£ä¿å­˜ç»“æœï¼š
- en: '[PRE68]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Note that running the active learning loop takes quite a long time: for every
    iteration, we train and evaluate our model for 50 epochs, and then run through
    our pool set (the full training dataset minus the acquired samples) 100 times.
    When using a random acquisition function, we avoid the last step but still run
    our validation data through our model 50 times per iteration, just to make sure
    that we use the model with the best validation accuracy. This takes time, but
    picking the model with just the best *training* accuracy would be risky: our model
    sees the same few images many times during training and is therefore likely to
    overfit to the training data.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œè¿è¡Œä¸»åŠ¨å­¦ä¹ å¾ªç¯éœ€è¦ç›¸å½“é•¿çš„æ—¶é—´ï¼šæ¯æ¬¡è¿­ä»£ï¼Œæˆ‘ä»¬è®­ç»ƒå¹¶è¯„ä¼°æ¨¡å‹50ä¸ªepochï¼Œç„¶ååœ¨æˆ‘ä»¬çš„æ± é›†ï¼ˆå®Œæ•´çš„è®­ç»ƒæ•°æ®é›†å‡å»å·²è·å–çš„æ ·æœ¬ï¼‰ä¸Šè¿è¡Œ100æ¬¡ã€‚ä½¿ç”¨éšæœºé‡‡é›†å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬è·³è¿‡æœ€åä¸€æ­¥ï¼Œä½†ä»ç„¶æ¯æ¬¡è¿­ä»£å°†éªŒè¯æ•°æ®è¿è¡Œ50æ¬¡ï¼Œä»¥ç¡®ä¿ä½¿ç”¨å…·æœ‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡çš„æ¨¡å‹ã€‚è¿™éœ€è¦æ—¶é—´ï¼Œä½†ä»…ä»…é€‰æ‹©å…·æœ‰æœ€ä½³*è®­ç»ƒ*å‡†ç¡®ç‡çš„æ¨¡å‹æ˜¯æœ‰é£é™©çš„ï¼šæˆ‘ä»¬çš„æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¤šæ¬¡çœ‹åˆ°ç›¸åŒçš„å‡ å¼ å›¾ç‰‡ï¼Œå› æ­¤å¾ˆå¯èƒ½ä¼šè¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚
- en: 'Step 6: Inspecting the results'
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬6æ­¥ï¼šæ£€æŸ¥ç»“æœ
- en: 'Now that we have our loop, we can inspect the results of this process. We will
    use `seaborn` and `matplotlib` to visualize our results:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰äº†å¾ªç¯ï¼Œå¯ä»¥æ£€æŸ¥è¿™ä¸ªè¿‡ç¨‹çš„ç»“æœã€‚æˆ‘ä»¬å°†ä½¿ç”¨`seaborn`å’Œ`matplotlib`æ¥å¯è§†åŒ–æˆ‘ä»¬çš„ç»“æœï¼š
- en: '[PRE69]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The main result we are interested in is the test accuracy over time for both
    the models trained with a random acquisition function and the models trained with
    data acquired via knowledge uncertainty. To visualize this, we define a function
    that loads the results and then returns a plot that shows the accuracy per active
    learning iteration cycle:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ€æ„Ÿå…´è¶£çš„ä¸»è¦ç»“æœæ˜¯ä¸¤ç§æ¨¡å‹çš„æµ‹è¯•å‡†ç¡®ç‡éšæ—¶é—´çš„å˜åŒ–ï¼Œè¿™äº›æ¨¡å‹åˆ†åˆ«æ˜¯åŸºäºéšæœºè·å–å‡½æ•°è®­ç»ƒçš„æ¨¡å‹å’Œé€šè¿‡çŸ¥è¯†ä¸ç¡®å®šæ€§è·å–æ•°æ®è®­ç»ƒçš„æ¨¡å‹ã€‚ä¸ºäº†å¯è§†åŒ–è¿™ä¸ªç»“æœï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼ŒåŠ è½½ç»“æœå¹¶è¿”å›ä¸€ä¸ªå›¾è¡¨ï¼Œæ˜¾ç¤ºæ¯ä¸ªä¸»åŠ¨å­¦ä¹ è¿­ä»£å‘¨æœŸçš„å‡†ç¡®ç‡ï¼š
- en: '[PRE70]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We can then use this function to plot the results for both acquisition functions:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªå‡½æ•°ç»˜åˆ¶ä¸¤ä¸ªè·å–å‡½æ•°çš„ç»“æœï¼š
- en: '[PRE71]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'This produces the following output:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†äº§ç”Ÿä»¥ä¸‹è¾“å‡ºï¼š
- en: '![PIC](img/file172.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file172.png)'
- en: 'FigureÂ 8.8: Active learning results'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.8ï¼šä¸»åŠ¨å­¦ä¹ ç»“æœ
- en: '*Figure* [*8.8*](#x1-156074r8) shows that acquiring samples via knowledge uncertainty
    starts to improve the modelâ€™s accuracy significantly after around 300 acquired
    samples. The final accuracy of this model is about two percentage points higher
    than the accuracy of the model trained on random samples. This might not look
    like a lot, but we can also look at the data in another way: how many samples
    were needed to achieve a particular accuracy? If we inspect the plot, we can see
    that the knowledge uncertainty line achieves an accuracy of 96% with 400 training
    samples. The model trained on random samples required at least 750 samples to
    achieve the same accuracy. Thatâ€™s almost double the amount of data for the same
    accuracy. This shows that active learning with the right acquisition function
    can be very useful, specifically in cases where compute resources are available,
    but labeling is expensive: with the right samples, we might be able to decrease
    our labeling cost by a factor of two to achieve the same accuracy.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾* [*8.8*](#x1-156074r8) æ˜¾ç¤ºï¼Œé€šè¿‡çŸ¥è¯†ä¸ç¡®å®šæ€§è·å–æ ·æœ¬å¼€å§‹æ˜¾è‘—æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤§çº¦è·å–äº†300ä¸ªæ ·æœ¬ä¹‹åã€‚è¯¥æ¨¡å‹çš„æœ€ç»ˆå‡†ç¡®ç‡æ¯”éšæœºæ ·æœ¬è®­ç»ƒçš„æ¨¡å‹é«˜å‡ºå¤§çº¦ä¸¤ä¸ªç™¾åˆ†ç‚¹ã€‚è™½ç„¶è¿™çœ‹èµ·æ¥ä¸å¤šï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥ä»å¦ä¸€ä¸ªè§’åº¦æ¥åˆ†ææ•°æ®ï¼šä¸ºäº†å®ç°ç‰¹å®šçš„å‡†ç¡®ç‡ï¼Œéœ€è¦å¤šå°‘æ ·æœ¬ï¼Ÿå¦‚æœæˆ‘ä»¬æ£€æŸ¥å›¾è¡¨ï¼Œå¯ä»¥çœ‹åˆ°ï¼ŒçŸ¥è¯†ä¸ç¡®å®šæ€§çº¿åœ¨400ä¸ªè®­ç»ƒæ ·æœ¬ä¸‹è¾¾åˆ°äº†96%çš„å‡†ç¡®ç‡ã€‚è€Œéšæœºæ ·æœ¬è®­ç»ƒçš„æ¨¡å‹åˆ™è‡³å°‘éœ€è¦750ä¸ªæ ·æœ¬æ‰èƒ½è¾¾åˆ°ç›¸åŒçš„å‡†ç¡®ç‡ã€‚è¿™æ„å‘³ç€ï¼Œåœ¨ç›¸åŒå‡†ç¡®ç‡ä¸‹ï¼ŒçŸ¥è¯†ä¸ç¡®å®šæ€§æ–¹æ³•åªéœ€è¦å‡ ä¹ä¸€åŠçš„æ•°æ®é‡ã€‚è¿™è¡¨æ˜ï¼Œé‡‡ç”¨æ­£ç¡®çš„è·å–å‡½æ•°è¿›è¡Œä¸»åŠ¨å­¦ä¹ éå¸¸æœ‰ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è®¡ç®—èµ„æºå……è¶³ä½†æ ‡æ³¨æˆæœ¬æ˜‚è´µçš„æƒ…å†µä¸‹ï¼šé€šè¿‡æ­£ç¡®é€‰æ‹©æ ·æœ¬ï¼Œæˆ‘ä»¬å¯èƒ½èƒ½å¤Ÿå°†æ ‡æ³¨æˆæœ¬é™ä½ä¸€å€ï¼Œä»è€Œå®ç°ç›¸åŒçš„å‡†ç¡®ç‡ã€‚'
- en: 'Because we saved the acquired samples for every iteration, we can also inspect
    the type of images selected by both models. To make our visualization easier to
    interpret, we will visualize the last five acquired images for every method for
    every label. To do this, we first define a function that returns the images per
    label for a set of model directories:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬ä¿å­˜äº†æ¯æ¬¡è¿­ä»£è·å–çš„æ ·æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¹Ÿå¯ä»¥æ£€æŸ¥ä¸¤ç§æ¨¡å‹é€‰æ‹©çš„å›¾åƒç±»å‹ã€‚ä¸ºäº†ä½¿æˆ‘ä»¬çš„å¯è§†åŒ–æ›´æ˜“äºè§£é‡Šï¼Œæˆ‘ä»¬å°†å¯è§†åŒ–æ¯ç§æ–¹æ³•å¯¹äºæ¯ä¸ªæ ‡ç­¾æ‰€é€‰çš„æœ€åäº”ä¸ªå›¾åƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œè¿”å›æ¯ä¸ªæ ‡ç­¾çš„å›¾åƒé›†ï¼Œå¯¹äºä¸€ç»„æ¨¡å‹ç›®å½•ï¼š
- en: '[PRE72]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We then define a function that creates a `PILÂ Image` where we concatenate the
    images per label for a particular acquisition function:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œåˆ›å»ºä¸€ä¸ª`PIL å›¾åƒ`ï¼Œå…¶ä¸­æŒ‰æ ‡ç­¾å°†å›¾åƒè¿›è¡Œæ‹¼æ¥ï¼Œä»¥ä¾¿ç”¨äºç‰¹å®šçš„è·å–å‡½æ•°ï¼š
- en: '[PRE73]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We can then call these functions, in our case with the following setup and
    *UUID*s:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨è¿™äº›å‡½æ•°ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ä½¿ç”¨ä»¥ä¸‹è®¾ç½®å’Œ*UUID*ï¼š
- en: '[PRE74]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Letâ€™s compare the output.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¯”è¾ƒä¸€ä¸‹è¾“å‡ºã€‚
- en: '![PIC](img/file173.png)![PIC](img/file174.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/file173.png)![å›¾ç‰‡](img/file174.png)'
- en: 'FigureÂ 8.9: Images randomly selected (left) and images selected via knowledge
    uncertainty with MC dropout (right). Every row shows the last five images selected
    for the label'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.9ï¼šéšæœºé€‰æ‹©çš„å›¾åƒï¼ˆå·¦ï¼‰ä¸é€šè¿‡çŸ¥è¯†ä¸ç¡®å®šæ€§å’ŒMCä¸¢å¼ƒæ³•é€‰æ‹©çš„å›¾åƒï¼ˆå³ï¼‰ã€‚æ¯ä¸€è¡Œæ˜¾ç¤ºæ¯ä¸ªæ ‡ç­¾æ‰€é€‰çš„æœ€åäº”ä¸ªå›¾åƒ
- en: We can see in *Figure* [*8.9*](#x1-156175r9) that the images selected by the
    knowledge uncertainty acquisition function are probably more difficult to classify
    compared to the randomly selected images. The uncertainty acquisition function
    selects quite a few unusual representations of the digits in the dataset. Because
    our acquisition function was able to select these images, the model was better
    able to understand the full distribution of the dataset, which resulted in better
    accuracy over time.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨*å›¾* [*8.9*](#x1-156175r9)ä¸­çœ‹åˆ°ï¼Œé€šè¿‡çŸ¥è¯†ä¸ç¡®å®šæ€§è·å–å‡½æ•°é€‰å–çš„å›¾åƒç›¸æ¯”éšæœºé€‰æ‹©çš„å›¾åƒå¯èƒ½æ›´éš¾ä»¥åˆ†ç±»ã€‚è¿™ä¸ªä¸ç¡®å®šæ€§è·å–å‡½æ•°é€‰æ‹©äº†æ•°æ®é›†ä¸­ä¸€äº›ä¸å¯»å¸¸çš„æ•°å­—è¡¨ç¤ºã€‚ç”±äºæˆ‘ä»¬çš„è·å–å‡½æ•°èƒ½å¤Ÿé€‰å–è¿™äº›å›¾åƒï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°ç†è§£æ•°æ®é›†çš„æ•´ä½“åˆ†å¸ƒï¼Œä»è€Œéšç€æ—¶é—´çš„æ¨ç§»æé«˜äº†å‡†ç¡®ç‡ã€‚
- en: 8.5 Using uncertainty estimates for smarter reinforcement learning
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 ä½¿ç”¨ä¸ç¡®å®šæ€§ä¼°è®¡å®ç°æ›´æ™ºèƒ½çš„å¼ºåŒ–å­¦ä¹ 
- en: '**Reinforcement learning** aims to develop machine learning techniques capable
    of learning from their environment. Thereâ€™s a clue to the fundamental principle
    behind reinforcement learning in its name: the aim is to reinforce successful
    behaviour. Generally speaking, in reinforcement learning, we have an agent capable
    of executing a number of actions in an environment. Following these actions, the
    agent receives feedback from the environment, and this feedback is used to allow
    the agent to build a better understanding of which actions are more likely to
    lead to a positive outcome given the current state of the environment.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '**å¼ºåŒ–å­¦ä¹ **æ—¨åœ¨å¼€å‘èƒ½å¤Ÿä»ç¯å¢ƒä¸­å­¦ä¹ çš„æœºå™¨å­¦ä¹ æŠ€æœ¯ã€‚å¼ºåŒ–å­¦ä¹ èƒŒåçš„åŸºæœ¬åŸåˆ™åœ¨å…¶åç§°ä¸­æœ‰ä¸€ä¸çº¿ç´¢ï¼šç›®æ ‡æ˜¯åŠ å¼ºæˆåŠŸçš„è¡Œä¸ºã€‚ä¸€èˆ¬æ¥è¯´ï¼Œåœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨ç¯å¢ƒä¸­æ‰§è¡Œä¸€ç³»åˆ—çš„åŠ¨ä½œã€‚åœ¨è¿™äº›åŠ¨ä½œä¹‹åï¼Œæ™ºèƒ½ä½“ä»ç¯å¢ƒä¸­è·å¾—åé¦ˆï¼Œè€Œè¿™äº›åé¦ˆè¢«ç”¨æ¥å¸®åŠ©æ™ºèƒ½ä½“æ›´å¥½åœ°ç†è§£å“ªäº›åŠ¨ä½œæ›´å¯èƒ½å¯¼è‡´åœ¨å½“å‰ç¯å¢ƒçŠ¶æ€ä¸‹è·å¾—ç§¯æçš„ç»“æœã€‚'
- en: Formally, we can describe this using a set of states, *S*, a set of actions
    *A*, which map from a current state *s* to a new state *s*^â€², and a reward function,
    *R*(*s,s*^â€²), describing the reward for the transition between the current state,
    *s*, and the new state, *s*^â€². The set of states comprises a set of environment
    states, *S*[*e*], and a set of agent states, *S*[*a*], which together describe
    the state of the entire system.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å½¢å¼ä¸Šè®²ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ç»„çŠ¶æ€ *S*ã€ä¸€ç»„åŠ¨ä½œ *A* æ¥æè¿°å®ƒä»¬å¦‚ä½•ä»å½“å‰çŠ¶æ€ *s* è½¬æ¢åˆ°æ–°çš„çŠ¶æ€ *s*^â€²ï¼Œä»¥åŠå¥–åŠ±å‡½æ•° *R*(*s,s*^â€²)ï¼Œæè¿°å½“å‰çŠ¶æ€
    *s* å’Œæ–°çŠ¶æ€ *s*^â€² ä¹‹é—´çš„è¿‡æ¸¡å¥–åŠ±ã€‚çŠ¶æ€é›†ç”±ç¯å¢ƒçŠ¶æ€é›† *S*[*e*] å’Œæ™ºèƒ½ä½“çŠ¶æ€é›† *S*[*a*] ç»„æˆï¼Œä¸¤è€…å…±åŒæè¿°æ•´ä¸ªç³»ç»Ÿçš„çŠ¶æ€ã€‚
- en: We can think of this in terms of a game of Marco Polo, wherein call and response
    is used by one player in order to find another player. When the seeking player
    calls â€Marco,â€ the other player replies â€Polo,â€ giving the seeking player an estimate
    of their location based on the direction and amplitude of the sound. If we simplify
    this to consider it in terms of distance, a closer state would be one for which
    the distance reduces, such as *Î´* = *d* âˆ’ *d*^â€² *>* 0, where *d* is the distance
    at state *s* and *d*^â€² is the distance for state *s*^â€². Conversely, a further
    state would be one for which *Î´* = *d* âˆ’ *d*^â€² *<* 0\. Thus, for this example,
    we can use our *Î´* values as feedback for our model, making our reward function
    *Î´* = *R*(*s,s*^â€²) = *d* âˆ’ *d*^â€².
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†æ­¤ç±»æ¯”ä¸ºä¸€åœºé©¬å¯Â·æ³¢ç½—çš„æ¸¸æˆï¼Œå…¶ä¸­ä¸€ä¸ªç©å®¶é€šè¿‡â€œå–Šå«â€ä¸â€œå›åº”â€çš„æ–¹å¼æ¥æ‰¾åˆ°å¦ä¸€ä¸ªç©å®¶ã€‚å½“å¯»æ‰¾çš„ç©å®¶å–Šâ€œMarcoâ€æ—¶ï¼Œå¦ä¸€ä¸ªç©å®¶å›åº”â€œPoloâ€ï¼Œæ ¹æ®å£°éŸ³çš„æ–¹å‘å’Œå¹…åº¦ç»™å‡ºå¯»æ‰¾è€…å…¶ä½ç½®çš„ä¼°è®¡ã€‚å¦‚æœæˆ‘ä»¬å°†æ­¤ç®€åŒ–ä¸ºè€ƒè™‘è·ç¦»ï¼Œé‚£ä¹ˆè¾ƒè¿‘çš„çŠ¶æ€æ˜¯è·ç¦»å‡å°‘çš„çŠ¶æ€ï¼Œä¾‹å¦‚*Î´*
    = *d* âˆ’ *d*^â€² *>* 0ï¼Œå…¶ä¸­ *d* æ˜¯çŠ¶æ€ *s* çš„è·ç¦»ï¼Œ*d*^â€² æ˜¯çŠ¶æ€ *s*^â€² çš„è·ç¦»ã€‚ç›¸åï¼Œè¾ƒè¿œçš„çŠ¶æ€æ˜¯*Î´* = *d*
    âˆ’ *d*^â€² *<* 0ã€‚å› æ­¤ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„*Î´*å€¼ä½œä¸ºæ¨¡å‹çš„åé¦ˆï¼Œä½¿å¾—æˆ‘ä»¬çš„å¥–åŠ±å‡½æ•°ä¸º*Î´* = *R*(*s,s*^â€²) = *d*
    âˆ’ *d*^â€²ã€‚
- en: '![PIC](img/file175.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file175.jpg)'
- en: 'FigureÂ 8.10: Illustration of a Marco Polo reinforcement learning scenario'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.10ï¼šé©¬å¯Â·æ³¢ç½—å¼ºåŒ–å­¦ä¹ åœºæ™¯çš„æ’å›¾
- en: 'Letâ€™s consider our agent as the seeking player and its target as the hiding
    player. At each step, our agent collects more information about its environment,
    enabling it to better model the relationship between its actions *A*(*s*) and
    the reward function *R*(*s,s*^â€²) (in other words, itâ€™s learning which general
    direction it needs to move in to get closer to the target). At each step, we need
    to predict the reward function given the set of possible actions at the current
    state, *A*[*s*], so that we can choose the action that is most likely to maximize
    this reward function. In this case, the set of actions could be a set of directions
    we can move in, for example: forward, back, left, and right.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æŠŠæ™ºèƒ½ä½“è§†ä¸ºå¯»æ‰¾ç©å®¶ï¼ŒæŠŠç›®æ ‡è§†ä¸ºéšè—ç©å®¶ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œæ™ºèƒ½ä½“ä¼šæ”¶é›†æ›´å¤šå…³äºç¯å¢ƒçš„ä¿¡æ¯ï¼Œä»è€Œæ›´å¥½åœ°å»ºæ¨¡å…¶è¡ŒåŠ¨ *A*(*s*) å’Œå¥–åŠ±å‡½æ•° *R*(*s,s*^â€²)
    ä¹‹é—´çš„å…³ç³»ï¼ˆæ¢å¥è¯è¯´ï¼Œå®ƒåœ¨å­¦ä¹ éœ€è¦æœå“ªä¸ªæ–¹å‘ç§»åŠ¨ï¼Œä»¥ä¾¿æ›´æ¥è¿‘ç›®æ ‡ï¼‰ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦é¢„æµ‹å¥–åŠ±å‡½æ•°ï¼Œç»™å®šå½“å‰çŠ¶æ€ä¸‹çš„å¯èƒ½è¡ŒåŠ¨é›† *A*[*s*]ï¼Œä»¥ä¾¿é€‰æ‹©æœ€æœ‰å¯èƒ½æœ€å¤§åŒ–è¯¥å¥–åŠ±å‡½æ•°çš„è¡ŒåŠ¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¡ŒåŠ¨é›†å¯ä»¥æ˜¯æˆ‘ä»¬å¯ä»¥ç§»åŠ¨çš„æ–¹å‘é›†ï¼Œä¾‹å¦‚ï¼šå‰è¿›ã€åé€€ã€å·¦è½¬å’Œå³è½¬ã€‚
- en: Traditional reinforcement learning uses a method called **Q Learning** to learn
    the relationship between the state, action, and reward. Q Learning doesnâ€™t involve
    neural network models, and instead accumulates state, action, and reward information
    in a table â€“ the Q table â€“ which is then used to determine the action most likely
    to produce the highest reward given the current state. While Q Learning is powerful,
    it becomes computationally prohibitive for large numbers of states and actions.
    To address this, researchers introduced the concept of **Deep Q Learning**, wherein
    the Q table is replaced by a neural network. Over a (usually large) number of
    iterations, the neural network learns which actions are likely to produce a higher
    reward given the current state.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ä½¿ç”¨ä¸€ç§å«åš**Qå­¦ä¹ **çš„æ–¹æ³•æ¥å­¦ä¹ çŠ¶æ€ã€è¡ŒåŠ¨å’Œå¥–åŠ±ä¹‹é—´çš„å…³ç³»ã€‚Qå­¦ä¹ ä¸æ¶‰åŠç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œè€Œæ˜¯å°†çŠ¶æ€ã€è¡ŒåŠ¨å’Œå¥–åŠ±ä¿¡æ¯å­˜å‚¨åœ¨ä¸€ä¸ªè¡¨æ ¼ä¸­â€”â€”Qè¡¨æ ¼â€”â€”ç„¶åç”¨æ¥ç¡®å®šåœ¨å½“å‰çŠ¶æ€ä¸‹æœ€æœ‰å¯èƒ½äº§ç”Ÿæœ€é«˜å¥–åŠ±çš„è¡ŒåŠ¨ã€‚è™½ç„¶Qå­¦ä¹ éå¸¸å¼ºå¤§ï¼Œä½†å¯¹äºå¤§é‡çš„çŠ¶æ€å’Œè¡ŒåŠ¨ï¼Œå®ƒçš„è®¡ç®—æˆæœ¬å˜å¾—ä¸å¯æ‰¿å—ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜å¼•å…¥äº†**æ·±åº¦Qå­¦ä¹ **çš„æ¦‚å¿µï¼Œå…¶ä¸­Qè¡¨æ ¼è¢«ç¥ç»ç½‘ç»œæ‰€æ›¿ä»£ã€‚åœ¨é€šå¸¸ç»è¿‡å¤§é‡è¿­ä»£åï¼Œç¥ç»ç½‘ç»œä¼šå­¦ä¹ åœ¨ç»™å®šå½“å‰çŠ¶æ€çš„æƒ…å†µä¸‹ï¼Œå“ªäº›è¡ŒåŠ¨æ›´æœ‰å¯èƒ½äº§ç”Ÿæ›´é«˜çš„å¥–åŠ±ã€‚
- en: 'To predict which action is likely to yield the highest reward value, we use
    a model trained on all historical actions, *A*[*h*], states *S*[*h*], and rewards,
    *R*[*h*]. Our training input *X* comprises the actions *A*[*h*] and states *S*[*h*],
    while our target output *y* comprises the reward values *R*[*h*]. We can then
    use the model as part of a **Model predictive Controller**, or **MPC**, which
    will select the action depending on which action is associated with the highest
    predicted reward:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†é¢„æµ‹å“ªç§è¡ŒåŠ¨å¯èƒ½äº§ç”Ÿæœ€é«˜çš„å¥–åŠ±å€¼ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç»è¿‡è®­ç»ƒçš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹åŸºäºæ‰€æœ‰å†å²è¡ŒåŠ¨ *A*[*h*]ã€çŠ¶æ€ *S*[*h*] å’Œå¥–åŠ± *R*[*h*]ã€‚æˆ‘ä»¬çš„è®­ç»ƒè¾“å…¥
    *X* åŒ…å«è¡ŒåŠ¨ *A*[*h*] å’ŒçŠ¶æ€ *S*[*h*]ï¼Œè€Œç›®æ ‡è¾“å‡º *y* åŒ…å«å¥–åŠ±å€¼ *R*[*h*]ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†è¯¥æ¨¡å‹ä½œä¸º**æ¨¡å‹é¢„æµ‹æ§åˆ¶å™¨**ï¼ˆ**MPC**ï¼‰çš„ä¸€éƒ¨åˆ†ï¼Œé€‰æ‹©è¡ŒåŠ¨ï¼Œä¾æ®æ˜¯å“ªä¸ªè¡ŒåŠ¨ä¸æœ€é«˜é¢„æµ‹å¥–åŠ±ç›¸å…³ï¼š
- en: '![anext = argmax yiâˆ€ai âˆˆ As ](img/file176.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![anext = argmax yiâˆ€ai âˆˆ As ](img/file176.jpg)'
- en: Here, *y*[*i*] is the reward prediction produced by our model, *f*(*a*[*i*]*,s*),
    which maps the current state *s* and possible actions *a*[*i*] âˆˆ *A*[*s*] to reward
    values. However, before our model is of any use, weâ€™ll need to gather data to
    train on. Weâ€™ll accrue data over a number of episodes, wherein each episode comprises
    a set of actions taken by the agent until some termination criteria are met. The
    ideal termination criterion would be the agent finding the target, but we can
    set other criteria, such as the agent encountering an obstacle, or the agent exhausting
    a maximum number of actions. Because the model has no information to start off
    with, we use a greedy policy commonly used in reinforcement learning, called an
    *ğœ–greedy* policy, to allow the agent to start by randomly sampling from its environment.
    The idea here is that our agent will perform a random action with probability
    *ğœ–*, and will otherwise use model predictions to select the action. After each
    episode, we will decrease *ğœ–*, such that the agent will eventually be selecting
    actions based solely on the model. Letâ€™s put together a simple reinforcement learning
    example to see all of this in action.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œ*y*[*i*] æ˜¯æˆ‘ä»¬çš„æ¨¡å‹äº§ç”Ÿçš„å¥–åŠ±é¢„æµ‹ï¼Œ*f*(*a*[*i*]*,s*)ï¼Œå®ƒå°†å½“å‰çŠ¶æ€ *s* å’Œå¯èƒ½çš„åŠ¨ä½œ *a*[*i*] âˆˆ *A*[*s*]
    æ˜ å°„åˆ°å¥–åŠ±å€¼ã€‚ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬çš„æ¨¡å‹æœ‰ä»»ä½•ç”¨å¤„ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ”¶é›†æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬å°†åœ¨å¤šä¸ªå›åˆä¸­ç§¯ç´¯æ•°æ®ï¼Œæ¯ä¸ªå›åˆåŒ…æ‹¬ä»£ç†é‡‡å–çš„ä¸€ç³»åˆ—åŠ¨ä½œï¼Œç›´åˆ°æ»¡è¶³æŸäº›ç»ˆæ­¢æ ‡å‡†ã€‚ç†æƒ³çš„ç»ˆæ­¢æ ‡å‡†æ˜¯ä»£ç†æ‰¾åˆ°ç›®æ ‡ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥è®¾ç½®å…¶ä»–æ ‡å‡†ï¼Œä¾‹å¦‚ä»£ç†é‡åˆ°éšœç¢ç‰©æˆ–ä»£ç†ç”¨å°½æœ€å¤§åŠ¨ä½œæ•°ã€‚ç”±äºæ¨¡å‹å¼€å§‹æ—¶æ²¡æœ‰ä»»ä½•ä¿¡æ¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ç§åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å¸¸è§çš„è´ªå©ªç­–ç•¥ï¼Œå«åš
    *ğœ–greedy* ç­–ç•¥ï¼Œå…è®¸ä»£ç†é€šè¿‡ä»ç¯å¢ƒä¸­éšæœºé‡‡æ ·å¼€å§‹ã€‚è¿™é‡Œçš„æƒ³æ³•æ˜¯ï¼Œæˆ‘ä»¬çš„ä»£ç†ä»¥ *ğœ–* çš„æ¦‚ç‡æ‰§è¡ŒéšæœºåŠ¨ä½œï¼Œå¦åˆ™ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ¥é€‰æ‹©åŠ¨ä½œã€‚åœ¨æ¯ä¸ªå›åˆä¹‹åï¼Œæˆ‘ä»¬ä¼šå‡å°‘
    *ğœ–*ï¼Œä½¿å¾—ä»£ç†æœ€ç»ˆä»…æ ¹æ®æ¨¡å‹æ¥é€‰æ‹©åŠ¨ä½œã€‚è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªç®€å•çš„å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹ï¼Œçœ‹çœ‹è¿™ä¸€åˆ‡æ˜¯å¦‚ä½•è¿ä½œçš„ã€‚
- en: 'Step 1: Initializing our environment'
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥ï¼šåˆå§‹åŒ–æˆ‘ä»¬çš„ç¯å¢ƒ
- en: 'Our reinforcement learning example will be centred around our environment:
    this defines the space in which everything happens. Weâ€™ll handle this with the
    `Environment` class. First, we set up our environment parameters:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹å°†å›´ç»•æˆ‘ä»¬çš„ç¯å¢ƒå±•å¼€ï¼šè¿™å®šä¹‰äº†æ‰€æœ‰äº‹ä»¶å‘ç”Ÿçš„ç©ºé—´ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`Environment`ç±»æ¥å¤„ç†è¿™ä¸ªé—®é¢˜ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¾ç½®ç¯å¢ƒå‚æ•°ï¼š
- en: '[PRE75]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Here, notice our environment size, denoted by `env_size`, which defines the
    number of rows and columns in our environment â€“ in this case, weâ€™ll have an environment
    of size 8 Ã— 8, resulting in 64 locations (for simplicity, weâ€™ll stick with a square
    environment). Weâ€™ll also set a `max_steps` limit so that episodes donâ€™t go on
    too long while our agent is randomly selecting actions.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæ³¨æ„æˆ‘ä»¬çš„ç¯å¢ƒå¤§å°ï¼Œç”¨`env_size`è¡¨ç¤ºï¼Œå®ƒå®šä¹‰äº†ç¯å¢ƒä¸­çš„è¡Œæ•°å’Œåˆ—æ•°â€”â€”åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ 8 Ã— 8 çš„ç¯å¢ƒï¼Œç»“æœæ˜¯ 64 ä¸ªä½ç½®ï¼ˆä¸ºäº†ç®€ä¾¿ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªæ–¹å½¢ç¯å¢ƒï¼‰ã€‚æˆ‘ä»¬è¿˜å°†è®¾ç½®ä¸€ä¸ª`max_steps`é™åˆ¶ï¼Œä»¥ç¡®ä¿åœ¨ä»£ç†éšæœºé€‰æ‹©åŠ¨ä½œæ—¶ï¼Œå›åˆä¸ä¼šè¿›è¡Œå¾—å¤ªé•¿ã€‚
- en: We also set the `agent_location` and `target_location` variables â€“ the agent
    always starts at point [0, 0], while the target location is randomly allocated.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜è®¾ç½®äº†`agent_location`å’Œ`target_location`å˜é‡â€”â€”ä»£ç†æ€»æ˜¯ä» [0, 0] ç‚¹å¼€å§‹ï¼Œè€Œç›®æ ‡ä½ç½®åˆ™æ˜¯éšæœºåˆ†é…çš„ã€‚
- en: 'Next, we create a dictionary to map an integer value to an action. Going from
    0 to 3, these actions are: forward, backward, right, left. We also set the `delta`
    variable â€“ this is the initial distance between the agent and the target (weâ€™ll
    see how `compute_distance()` is implemented in a moment).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œå°†æ•´æ•°å€¼æ˜ å°„åˆ°ä¸€ä¸ªåŠ¨ä½œã€‚ä» 0 åˆ° 3ï¼Œè¿™äº›åŠ¨ä½œåˆ†åˆ«æ˜¯ï¼šå‘å‰ã€å‘åã€å‘å³ã€å‘å·¦ã€‚æˆ‘ä»¬è¿˜è®¾ç½®äº†`delta`å˜é‡â€”â€”è¿™æ˜¯ä»£ç†ä¸ç›®æ ‡ä¹‹é—´çš„åˆå§‹è·ç¦»ï¼ˆç¨åæˆ‘ä»¬å°†çœ‹åˆ°`compute_distance()`æ˜¯å¦‚ä½•å®ç°çš„ï¼‰ã€‚
- en: 'Finally, we initialize a few variables for tracking whether the termination
    criteria have been met (`is_done`), the total number of steps (`total_steps`),
    and the ideal number of steps (`ideal_steps`). The latter of these variables is
    the minimum number of steps required for the agent to get to the target from its
    starting position. Weâ€™ll use this to calculate the regret, which is a useful indicator
    of performance for reinforcement learning and optimization algorithms. To calculate
    this, weâ€™ll add the following two functions to our class:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬åˆå§‹åŒ–ä¸€äº›å˜é‡ï¼Œç”¨äºè·Ÿè¸ªç»ˆæ­¢æ ‡å‡†æ˜¯å¦å·²æ»¡è¶³ï¼ˆ`is_done`ï¼‰ã€æ€»æ­¥éª¤æ•°ï¼ˆ`total_steps`ï¼‰å’Œç†æƒ³æ­¥éª¤æ•°ï¼ˆ`ideal_steps`ï¼‰ã€‚åè€…æ˜¯ä»£ç†ä»èµ·å§‹ä½ç½®åˆ°è¾¾ç›®æ ‡æ‰€éœ€çš„æœ€å°æ­¥éª¤æ•°ã€‚æˆ‘ä»¬å°†ç”¨å®ƒæ¥è®¡ç®—é—æ†¾ï¼Œè¿™æ˜¯å¼ºåŒ–å­¦ä¹ å’Œä¼˜åŒ–ç®—æ³•ä¸­ä¸€ä¸ªæœ‰ç”¨çš„æ€§èƒ½æŒ‡æ ‡ã€‚ä¸ºäº†è®¡ç®—é—æ†¾ï¼Œæˆ‘ä»¬å°†å‘æˆ‘ä»¬çš„ç±»ä¸­æ·»åŠ ä»¥ä¸‹ä¸¤ä¸ªå‡½æ•°ï¼š
- en: '[PRE76]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Here, `calculate_ideal_steps()` will run until the distance (`delta`) between
    the agent and the target is zero. At each iteration, it uses `calculate_ideal_action()`
    to select the action that will move the agent closest to the target.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ`calculate_ideal_steps()`å°†ä¸€ç›´è¿è¡Œï¼Œç›´åˆ°ä»£ç†ä¸ç›®æ ‡ä¹‹é—´çš„è·ç¦»ï¼ˆ`delta`ï¼‰ä¸ºé›¶ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå®ƒä½¿ç”¨`calculate_ideal_action()`æ¥é€‰æ‹©èƒ½ä½¿ä»£ç†å°½å¯èƒ½æ¥è¿‘ç›®æ ‡çš„åŠ¨ä½œã€‚
- en: 'Step 2: Updating the state of our environment'
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ­¥ï¼šæ›´æ–°æˆ‘ä»¬ç¯å¢ƒçš„çŠ¶æ€
- en: 'Now that weâ€™ve initialized our environment, we need to add one of the most
    crucial pieces of our class: the `update` method. This controls what happens to
    our environment when the agent takes a new action:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»åˆå§‹åŒ–äº†æˆ‘ä»¬çš„ç¯å¢ƒï¼Œæˆ‘ä»¬éœ€è¦æ·»åŠ æˆ‘ä»¬ç±»ä¸­æœ€å…³é”®çš„ä¸€ä¸ªéƒ¨åˆ†ï¼š`update`æ–¹æ³•ã€‚è¿™æ§åˆ¶äº†å½“ä»£ç†é‡‡å–æ–°åŠ¨ä½œæ—¶ç¯å¢ƒçš„å˜åŒ–ï¼š
- en: '[PRE77]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The method receives an action integer, and uses this to access the corresponding
    action in the `action_space` dictionary we defined earlier. It then updates the
    agent location. Because both the agent location and action are vectors, we can
    simply use vector addition to do this. Next, we check whether the agent has moved
    out of bounds of our environment â€“ if it has, we simply adjust its location so
    that it remains within our environment boundary.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ–¹æ³•æ¥æ”¶ä¸€ä¸ªåŠ¨ä½œæ•´æ•°ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥è®¿é—®æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„`action_space`å­—å…¸ä¸­å¯¹åº”çš„åŠ¨ä½œã€‚ç„¶åæ›´æ–°ä»£ç†ä½ç½®ã€‚å› ä¸ºä»£ç†ä½ç½®å’ŒåŠ¨ä½œéƒ½æ˜¯å‘é‡ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç®€å•åœ°ä½¿ç”¨å‘é‡åŠ æ³•æ¥å®Œæˆè¿™ä¸€ç‚¹ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ£€æŸ¥ä»£ç†æ˜¯å¦ç§»å‡ºäº†ç¯å¢ƒçš„è¾¹ç•Œ
    â€“ å¦‚æœæ˜¯ï¼Œåˆ™è°ƒæ•´å…¶ä½ç½®ä½¿å…¶ä»ç„¶ä¿æŒåœ¨æˆ‘ä»¬çš„ç¯å¢ƒè¾¹ç•Œå†…ã€‚
- en: 'The next line is another crucial piece of code: computing the reward with `compute_reward()`
    â€“ weâ€™ll take a look at this in just a moment. Once weâ€™ve computed the reward,
    we increment the `total_steps` counter, check our termination criteria, and return
    the reward value for the action.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€è¡Œæ˜¯å¦ä¸€ä¸ªå…³é”®çš„ä»£ç ç‰‡æ®µï¼šä½¿ç”¨`compute_reward()`è®¡ç®—å¥–åŠ± â€“ æˆ‘ä»¬é©¬ä¸Šå°±ä¼šçœ‹åˆ°è¿™ä¸ªã€‚ä¸€æ—¦è®¡ç®—å‡ºå¥–åŠ±ï¼Œæˆ‘ä»¬å¢åŠ `total_steps`è®¡æ•°å™¨ï¼Œæ£€æŸ¥ç»ˆæ­¢æ¡ä»¶ï¼Œå¹¶è¿”å›åŠ¨ä½œçš„å¥–åŠ±å€¼ã€‚
- en: 'We determine the reward using the following function. This will return a low
    reward (`1`) if the distance between the agent and the target increases, and a
    high reward (`10`) if the distance between the agent and target decreases:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹å‡½æ•°æ¥ç¡®å®šå¥–åŠ±ã€‚å¦‚æœä»£ç†ä¸ç›®æ ‡ä¹‹é—´çš„è·ç¦»å¢åŠ ï¼Œåˆ™è¿”å›ä½å¥–åŠ±ï¼ˆ`1`ï¼‰ï¼Œå¦‚æœå‡å°‘ï¼Œåˆ™è¿”å›é«˜å¥–åŠ±ï¼ˆ`10`ï¼‰ï¼š
- en: '[PRE78]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'This uses the `compute_distance()` function, which calculates the Euclidean
    distance between the agent and the target:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œä½¿ç”¨äº†`compute_distance()`å‡½æ•°ï¼Œè®¡ç®—ä»£ç†ä¸ç›®æ ‡ä¹‹é—´çš„æ¬§æ°è·ç¦»ï¼š
- en: '[PRE79]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Lastly, we need a function to allow us to fetch the state of the environment,
    so that we can associate this with the reward values. We define this as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå‡½æ•°æ¥å…è®¸æˆ‘ä»¬è·å–ç¯å¢ƒçš„çŠ¶æ€ï¼Œä»¥ä¾¿å°†å…¶ä¸å¥–åŠ±å€¼å…³è”èµ·æ¥ã€‚æˆ‘ä»¬å°†å…¶å®šä¹‰å¦‚ä¸‹ï¼š
- en: '[PRE80]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Step 3: Defining our model'
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰æ­¥ï¼šå®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹
- en: 'Now that weâ€™ve set up our environment, weâ€™ll create a model class. This class
    will handle model training and inference, as well as selecting the best action
    according to the modelâ€™s predictions. As always, we start with the `__init__()`
    method:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»è®¾ç½®å¥½äº†ç¯å¢ƒï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ¨¡å‹ç±»ã€‚è¿™ä¸ªç±»å°†å¤„ç†æ¨¡å‹è®­ç»ƒå’Œæ¨æ–­ï¼Œä»¥åŠæ ¹æ®æ¨¡å‹é¢„æµ‹é€‰æ‹©æœ€ä½³åŠ¨ä½œã€‚å’Œå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬ä»`__init__()`æ–¹æ³•å¼€å§‹ï¼š
- en: '[PRE81]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Here, we pass a few variables related to our environment, such as the state
    size and number of actions. The code relating to the model definition should be
    familiar â€“ weâ€™re simply instantiating a neural network using Keras. One point
    to note is that weâ€™re using the Huber loss here, instead of something more common
    such as the mean squared error. This is a common choice in robust regression tasks
    and in reinforcement learning. The reason for this is that the Huber loss dynamically
    switches between mean squared error and mean absolute error. The former is very
    good at penalizing small errors, while the latter is more robust to outliers.
    Through the Huber loss, we arrive at a loss function that is both robust to outliers
    and penalizes small errors.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¼ é€’äº†ä¸€äº›ä¸æˆ‘ä»¬çš„ç¯å¢ƒç›¸å…³çš„å˜é‡ï¼Œå¦‚çŠ¶æ€å¤§å°å’ŒåŠ¨ä½œæ•°é‡ã€‚ä¸æ¨¡å‹å®šä¹‰ç›¸å…³çš„ä»£ç åº”è¯¥å¾ˆç†Ÿæ‚‰ â€“ æˆ‘ä»¬åªæ˜¯ä½¿ç”¨Keraså®ä¾‹åŒ–äº†ä¸€ä¸ªç¥ç»ç½‘ç»œã€‚éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨HuberæŸå¤±ï¼Œè€Œä¸æ˜¯æ›´å¸¸è§çš„å‡æ–¹è¯¯å·®ã€‚è¿™æ˜¯åœ¨å¼ºåŒ–å­¦ä¹ å’Œå¥å£®å›å½’ä»»åŠ¡ä¸­å¸¸è§çš„é€‰æ‹©ã€‚HuberæŸå¤±åŠ¨æ€åœ°åœ¨å‡æ–¹è¯¯å·®å’Œå¹³å‡ç»å¯¹è¯¯å·®ä¹‹é—´åˆ‡æ¢ã€‚å‰è€…éå¸¸æ“…é•¿æƒ©ç½šå°è¯¯å·®ï¼Œè€Œåè€…å¯¹å¼‚å¸¸å€¼æ›´ä¸ºå¥å£®ã€‚é€šè¿‡HuberæŸå¤±ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ªæ—¢å¯¹å¼‚å¸¸å€¼å¥å£®åˆæƒ©ç½šå°è¯¯å·®çš„æŸå¤±å‡½æ•°ã€‚
- en: 'This is particularly important in reinforcement learning because of the exploratory
    nature of the algorithms: we will often encounter some examples that are very
    exploratory, deviating significantly from the rest of the data, and thus causing
    large errors during training.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ç‰¹åˆ«é‡è¦ï¼Œå› ä¸ºç®—æ³•å…·æœ‰æ¢ç´¢æ€§ç‰¹å¾ï¼šæˆ‘ä»¬ç»å¸¸ä¼šé‡åˆ°ä¸€äº›æå…·æ¢ç´¢æ€§çš„æ ·æœ¬ï¼Œå®ƒä»¬ä¸å…¶ä»–æ•°æ®ç›¸æ¯”åç¦»è¾ƒå¤§ï¼Œä»è€Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¼è‡´è¾ƒå¤§çš„è¯¯å·®ã€‚
- en: 'With our class initialization out of the way, we move on to our `fit()` and
    `predict()` functions:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®Œæˆç±»çš„åˆå§‹åŒ–åï¼Œæˆ‘ä»¬ç»§ç»­å¤„ç† `fit()` å’Œ `predict()` å‡½æ•°ï¼š
- en: '[PRE82]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The `fit()` function should look very familiar â€“ weâ€™re just scaling our inputs
    before fitting our Keras model. The `predict()` function has a little more going
    on. Because we need predictions for each of our possible actions (forward, backward,
    right, left), we need to generate inputs for these. We do so by concatenating
    the integer value associated with the action to the state, producing our complete
    state-action vector as we see on line 11\. Doing this for all actions results
    in our input matrix, *X*, for which each row is associated with a specific action.
    We then scale *X* and run inference on this to obtain our predicted reward values.
    To select an action, we simply use `np.argmax()` to obtain the index associated
    with the highest predicted reward.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()` å‡½æ•°åº”è¯¥éå¸¸ç†Ÿæ‚‰â€”â€”æˆ‘ä»¬åªæ˜¯å¯¹è¾“å…¥è¿›è¡Œç¼©æ”¾ï¼Œç„¶åå†æ‹Ÿåˆæˆ‘ä»¬çš„ Keras æ¨¡å‹ã€‚`predict()` å‡½æ•°åˆ™ç¨å¾®å¤æ‚ä¸€ç‚¹ã€‚å› ä¸ºæˆ‘ä»¬éœ€è¦å¯¹æ¯ä¸ªå¯èƒ½çš„åŠ¨ä½œï¼ˆå‰è¿›ã€åé€€ã€å³è½¬ã€å·¦è½¬ï¼‰è¿›è¡Œé¢„æµ‹ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸ºè¿™äº›åŠ¨ä½œç”Ÿæˆè¾“å…¥ã€‚æˆ‘ä»¬é€šè¿‡å°†ä¸åŠ¨ä½œç›¸å…³çš„æ•´æ•°å€¼ä¸çŠ¶æ€è¿›è¡Œæ‹¼æ¥ï¼Œæ¥ç”Ÿæˆå®Œæ•´çš„çŠ¶æ€-åŠ¨ä½œå‘é‡ï¼Œæ­£å¦‚ç¬¬
    11 è¡Œæ‰€ç¤ºã€‚å¯¹æ‰€æœ‰åŠ¨ä½œæ‰§è¡Œæ­¤æ“ä½œåï¼Œæˆ‘ä»¬å¾—åˆ°è¾“å…¥çŸ©é˜µ *X*ï¼Œå…¶ä¸­æ¯ä¸€è¡Œéƒ½å¯¹åº”ä¸€ä¸ªç‰¹å®šçš„åŠ¨ä½œã€‚ç„¶åï¼Œæˆ‘ä»¬å¯¹ *X* è¿›è¡Œç¼©æ”¾ï¼Œå¹¶åœ¨å…¶ä¸Šè¿è¡Œæ¨ç†ï¼Œä»¥è·å¾—é¢„æµ‹çš„å¥–åŠ±å€¼ã€‚ä¸ºäº†é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œï¼Œæˆ‘ä»¬ç®€å•åœ°ä½¿ç”¨
    `np.argmax()` æ¥è·å–ä¸æœ€é«˜é¢„æµ‹å¥–åŠ±ç›¸å…³çš„ç´¢å¼•ã€‚'
- en: 'Step 4: Running our reinforcement learning'
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬ 4 æ­¥ï¼šè¿è¡Œæˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ 
- en: 'Now that weâ€™ve defined our `Environment` and `RLModel` classes, weâ€™re ready
    to do some reinforcement learning! Letâ€™s first set up some important variables
    and instantiate our model:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»å®šä¹‰äº† `Environment` å’Œ `RLModel` ç±»ï¼Œå‡†å¤‡å¼€å§‹å¼ºåŒ–å­¦ä¹ äº†ï¼é¦–å…ˆï¼Œæˆ‘ä»¬è®¾ç½®ä¸€äº›é‡è¦çš„å˜é‡å¹¶å®ä¾‹åŒ–æˆ‘ä»¬çš„æ¨¡å‹ï¼š
- en: '[PRE83]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Most of these should be familiar by now, but weâ€™ll go over a few that weâ€™ve
    not yet covered. The `history` dictionary is where weâ€™ll store our state and reward
    information as we progress through each step in each episode. Weâ€™ll then use this
    information to train our model. Another unfamiliar variable here is `n_samples`
    â€“ weâ€™re setting this because, rather than using all available data each time we
    train our model, weâ€™ll sample 1,000 data points from our data. This helps to avoid
    our training time exploding as we accrue more and more data. The last new variable
    here is `regrets`. This list will store our regret values for each episode. In
    our case, regret is defined simply as the difference between the number of steps
    taken by the model and the minimum number of steps required for the agent to reach
    the target:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å†…å®¹ç°åœ¨åº”è¯¥å·²ç»å¾ˆç†Ÿæ‚‰äº†ï¼Œä½†æˆ‘ä»¬è¿˜æ˜¯ä¼šå†å›é¡¾ä¸€äº›å°šæœªè¦†ç›–çš„éƒ¨åˆ†ã€‚`history` å­—å…¸æ˜¯æˆ‘ä»¬å­˜å‚¨çŠ¶æ€å’Œå¥–åŠ±ä¿¡æ¯çš„åœ°æ–¹ï¼Œåœ¨æ¯ä¸€è½®çš„æ¯ä¸ªæ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬ä¼šæ›´æ–°è¿™äº›ä¿¡æ¯ã€‚ç„¶åï¼Œæˆ‘ä»¬ä¼šåˆ©ç”¨è¿™äº›ä¿¡æ¯æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚å¦ä¸€ä¸ªä¸å¤ªç†Ÿæ‚‰çš„å˜é‡æ˜¯
    `n_samples`â€”â€”æˆ‘ä»¬è®¾ç½®è¿™ä¸ªå˜é‡æ˜¯å› ä¸ºæ¯æ¬¡è®­ç»ƒæ¨¡å‹æ—¶ï¼Œå¹¶ä¸æ˜¯ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„æ•°æ®ï¼Œè€Œæ˜¯ä»æ•°æ®ä¸­éšæœºæŠ½å– 1,000 ä¸ªæ•°æ®ç‚¹ã€‚è¿™æ ·å¯ä»¥é¿å…éšç€æ•°æ®é‡çš„ä¸æ–­å¢åŠ ï¼Œæˆ‘ä»¬çš„è®­ç»ƒæ—¶é—´ä¹Ÿä¸æ–­æš´å¢ã€‚è¿™é‡Œçš„æœ€åä¸€ä¸ªæ–°å˜é‡æ˜¯
    `regrets`ã€‚è¿™ä¸ªåˆ—è¡¨å°†å­˜å‚¨æ¯ä¸€è½®çš„é—æ†¾å€¼ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œé—æ†¾è¢«ç®€å•åœ°å®šä¹‰ä¸ºæ¨¡å‹æ‰€é‡‡å–çš„æ­¥éª¤æ•°ä¸æ™ºèƒ½ä½“åˆ°è¾¾ç›®æ ‡æ‰€éœ€çš„æœ€å°æ­¥éª¤æ•°ä¹‹é—´çš„å·®å€¼ï¼š
- en: '![regret = steps âˆ’ steps model ideal ](img/file177.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![regret = steps âˆ’ steps model ideal ](img/file177.jpg)'
- en: 'As such, regret is zero *â‡”* *steps*[*model*] == *steps*[*ideal*]. Regret is
    useful for measuring performance as our model learns, as weâ€™ll see in a moment.
    All thatâ€™s left is the main loop of our reinforcement learning process:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œé—æ†¾ä¸ºé›¶ *â‡”* *steps*[*model*] == *steps*[*ideal*]ã€‚é—æ†¾å€¼å¯¹äºè¡¡é‡æ¨¡å‹å­¦ä¹ è¿‡ç¨‹ä¸­çš„è¡¨ç°éå¸¸æœ‰ç”¨ï¼Œæ­£å¦‚æˆ‘ä»¬ç¨åå°†çœ‹åˆ°çš„é‚£æ ·ã€‚æ¥ä¸‹æ¥å°±æ˜¯å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹çš„ä¸»å¾ªç¯ï¼š
- en: '[PRE84]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Here, we have our reinforcement learning process run from 100 episodes, reinitializing
    the environment each time. As we can see from the internal `while` loop, we will
    continue iterating â€“ updating our agent and measuring our reward â€“ until one of
    the termination criteria is met (either the agent reaches the target, or we run
    for the maximum allowed number of iterations).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çš„å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹ä¼šè¿è¡Œ 100 è½®ï¼Œæ¯æ¬¡éƒ½é‡æ–°åˆå§‹åŒ–ç¯å¢ƒã€‚é€šè¿‡å†…éƒ¨çš„ `while` å¾ªç¯å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬ä¼šä¸æ–­åœ°è¿­ä»£â€”â€”æ›´æ–°æ™ºèƒ½ä½“å¹¶è¡¡é‡å¥–åŠ±â€”â€”ç›´åˆ°æ»¡è¶³å…¶ä¸­ä¸€ä¸ªç»ˆæ­¢æ¡ä»¶ï¼ˆæ— è®ºæ˜¯æ™ºèƒ½ä½“è¾¾åˆ°ç›®æ ‡ï¼Œè¿˜æ˜¯æˆ‘ä»¬è¾¾åˆ°æœ€å¤§å…è®¸çš„è¿­ä»£æ¬¡æ•°ï¼‰ã€‚
- en: After each episode, a `print` statement lets us know that the episode completed
    without error, and tells us how our agent did compared to the ideal number of
    steps. We then calculate the regret and append this to our `regrets` list, sample
    from our data in `history` and fit our model on the sampled data. Lastly, we finish
    each iteration of the outer loop by reducing epsilon.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸€è½®ç»“æŸåï¼Œ`print`è¯­å¥ä¼šå‘Šè¯‰æˆ‘ä»¬è¯¥è½®æ˜¯å¦æ²¡æœ‰é”™è¯¯å®Œæˆï¼Œå¹¶å‘Šè¯‰æˆ‘ä»¬æˆ‘ä»¬çš„æ™ºèƒ½ä½“ä¸ç†æƒ³æ­¥æ•°çš„å¯¹æ¯”ç»“æœã€‚æ¥ç€ï¼Œæˆ‘ä»¬è®¡ç®—é—æ†¾å€¼ï¼Œå¹¶å°†å…¶é™„åŠ åˆ°`regrets`åˆ—è¡¨ä¸­ï¼Œä»`history`ä¸­çš„æ•°æ®è¿›è¡Œé‡‡æ ·ï¼Œå¹¶åœ¨è¿™äº›æ ·æœ¬æ•°æ®ä¸Šæ‹Ÿåˆæˆ‘ä»¬çš„æ¨¡å‹ã€‚æœ€åï¼Œæ¯æ¬¡å¤–å¾ªç¯è¿­ä»£ç»“æŸæ—¶ï¼Œæˆ‘ä»¬ä¼šå‡å°‘epsilonå€¼ã€‚
- en: 'After running this, we can additionally plot our regret values to see how we
    did:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œå®Œä¹‹åï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç»˜åˆ¶é—æ†¾å€¼å›¾ï¼Œä»¥æŸ¥çœ‹æˆ‘ä»¬çš„è¡¨ç°ï¼š
- en: '[PRE85]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'This produces the following plot, showing how our model did over the 100 episodes:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ç”Ÿæˆä»¥ä¸‹å›¾è¡¨ï¼Œå±•ç¤ºæˆ‘ä»¬æ¨¡å‹åœ¨100è½®è®­ç»ƒä¸­çš„è¡¨ç°ï¼š
- en: '![PIC](img/file178.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file178.png)'
- en: 'FigureÂ 8.11: Plot of regret values following 100 episodes of reinforcement
    learning'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.11ï¼šå¼ºåŒ–å­¦ä¹ 100è½®åçš„é—æ†¾å€¼å›¾
- en: As we can see here, it did poorly to begin with, but the model quickly learned
    to predict reward values, allowing it to predict optimal actions, and reducing
    regret to 0.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„ï¼Œå®ƒä¸€å¼€å§‹è¡¨ç°å¾—å¾ˆå·®ï¼Œä½†æ¨¡å‹å¾ˆå¿«å­¦ä¼šäº†é¢„æµ‹å¥–åŠ±å€¼ï¼Œä»è€Œèƒ½å¤Ÿé¢„æµ‹æœ€ä¼˜åŠ¨ä½œï¼Œå°†é—æ†¾å‡å°‘åˆ°0ã€‚
- en: 'So far, things are pretty simple. In fact, you may be wondering why we need
    a model at all â€“ why not just calculate the distance between the target and the
    proposed positions, and select an action accordingly? Well, firstly, the aim of
    reinforcement learning is for an agent to discover how to interact in a given
    setting without any prior knowledge â€“ so while our agent can execute actions,
    it has no concept of distance. This is something that is learned through interacting
    with the environment. Secondly, it may not be that simple: what if there are obstacles
    in the environment? In this case, our agent needs to be more intelligent than
    simply moving toward the sound.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œäº‹æƒ…è¿˜ç®—ç®€å•ã€‚äº‹å®ä¸Šï¼Œä½ å¯èƒ½ä¼šæƒ³ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æ¨¡å‹å‘¢â€”â€”ä¸ºä»€ä¹ˆä¸ç›´æ¥è®¡ç®—ç›®æ ‡å’Œæ‹Ÿè®®ä½ç½®ä¹‹é—´çš„è·ç¦»ï¼Œç„¶åé€‰æ‹©ç›¸åº”çš„åŠ¨ä½œå‘¢ï¼Ÿé¦–å…ˆï¼Œå¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯è®©æ™ºèƒ½ä½“åœ¨æ²¡æœ‰ä»»ä½•å…ˆéªŒçŸ¥è¯†çš„æƒ…å†µä¸‹å‘ç°å¦‚ä½•åœ¨ç»™å®šç¯å¢ƒä¸­è¿›è¡Œäº¤äº’â€”â€”æ‰€ä»¥ï¼Œå°½ç®¡æˆ‘ä»¬çš„æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡ŒåŠ¨ä½œï¼Œä½†å®ƒæ²¡æœ‰è·ç¦»çš„æ¦‚å¿µã€‚è¿™æ˜¯é€šè¿‡ä¸ç¯å¢ƒçš„äº’åŠ¨æ¥å­¦ä¹ çš„ã€‚å…¶æ¬¡ï¼Œæƒ…å†µå¯èƒ½æ²¡æœ‰é‚£ä¹ˆç®€å•ï¼šå¦‚æœç¯å¢ƒä¸­æœ‰éšœç¢ç‰©å‘¢ï¼Ÿåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“éœ€è¦æ¯”ç®€å•åœ°æœå£°éŸ³æºç§»åŠ¨æ›´èªæ˜ã€‚
- en: While this is just an illustrative example, real-world applications of reinforcement
    learning involve scenarios for which we have very limited knowledge, and thus
    designing an agent that can explore its environment and learn how to interact
    optimally allows us to develop models for applications for which supervised methods
    arenâ€™t an option.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™åªæ˜¯ä¸€ä¸ªç¤ºèŒƒæ€§çš„ä¾‹å­ï¼Œä½†å¼ºåŒ–å­¦ä¹ åœ¨ç°å®ä¸–ç•Œä¸­çš„åº”ç”¨æ¶‰åŠä¸€äº›æˆ‘ä»¬çŸ¥è¯†éå¸¸æœ‰é™çš„æƒ…å¢ƒï¼Œå› æ­¤ï¼Œè®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿæ¢ç´¢ç¯å¢ƒå¹¶å­¦ä¹ å¦‚ä½•æœ€ä¼˜äº’åŠ¨çš„æ™ºèƒ½ä½“ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿä¸ºé‚£äº›æ— æ³•ä½¿ç”¨ç›‘ç£å­¦ä¹ æ–¹æ³•çš„åº”ç”¨å¼€å‘æ¨¡å‹ã€‚
- en: 'Another factor to consider in real-world scenarios is risk: we want our agent
    to make *sensible* decisions, not just decisions that maximize the reward: we
    need it to build some understanding of the risk/reward trade-off. This is where
    uncertainty estimates come in.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªåœ¨ç°å®ä¸–ç•Œæƒ…å¢ƒä¸­éœ€è¦è€ƒè™‘çš„å› ç´ æ˜¯é£é™©ï¼šæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ™ºèƒ½ä½“åšå‡º*æ˜æ™º*çš„å†³ç­–ï¼Œè€Œä¸ä»…ä»…æ˜¯æœ€å¤§åŒ–å¥–åŠ±çš„å†³ç­–ï¼šæˆ‘ä»¬éœ€è¦å®ƒèƒ½å¤Ÿç†è§£é£é™©/å›æŠ¥çš„æƒè¡¡ã€‚è¿™å°±æ˜¯ä¸ç¡®å®šæ€§ä¼°è®¡çš„ä½œç”¨æ‰€åœ¨ã€‚
- en: 8.5.1 Navigating obstacles with uncertainty
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.1 å¸¦æœ‰ä¸ç¡®å®šæ€§çš„éšœç¢ç‰©å¯¼èˆª
- en: With uncertainty estimates, we can balance the reward against the modelâ€™s confidence
    in its prediction. If its confidence is low (meaning that uncertainty is high),
    then we may want to be cautious about how we incorporate our modelâ€™s predictions.
    For example, letâ€™s take the reinforcement learning scenario weâ€™ve just explored.
    For each episode, our model is predicting which action will yield the highest
    reward, and our agent then chooses this action. In the real world, things arenâ€™t
    so predictable â€“ our environment can change, leading to unexpected consequences.
    What if an obstacle appears in our environment, and colliding with that obstacle
    prevents our agent from completing its task? Well, clearly if our agent hasnâ€™t
    yet encountered the obstacle, itâ€™s doomed to fail. Fortunately, in the case of
    Bayesian Deep Learning, this isnâ€™t the case. As long as we have some way of sensing
    the obstacle, our agent can detect the obstacle and take a different route â€“ even
    if the obstacle wasnâ€™t encountered in previous episodes.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å¥–åŠ±å’Œæ¨¡å‹å¯¹å…¶é¢„æµ‹çš„ä¿¡å¿ƒä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚å¦‚æœæ¨¡å‹çš„ä¿¡å¿ƒè¾ƒä½ï¼ˆæ„å‘³ç€ä¸ç¡®å®šæ€§è¾ƒé«˜ï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯èƒ½å¸Œæœ›å¯¹å¦‚ä½•æ•´åˆæ¨¡å‹çš„é¢„æµ‹ä¿æŒè°¨æ…ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬åˆšåˆšæ¢è®¨çš„å¼ºåŒ–å­¦ä¹ åœºæ™¯ã€‚åœ¨æ¯ä¸€è½®ä¸­ï¼Œæˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹å“ªä¸ªåŠ¨ä½œå°†è·å¾—æœ€é«˜çš„å¥–åŠ±ï¼Œç„¶åæˆ‘ä»¬çš„æ™ºèƒ½ä½“é€‰æ‹©è¯¥åŠ¨ä½œã€‚åœ¨ç°å®ä¸–ç•Œä¸­ï¼Œäº‹æƒ…å¹¶ä¸æ˜¯é‚£ä¹ˆå¯é¢„æµ‹â€”â€”æˆ‘ä»¬çš„ç¯å¢ƒå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ï¼Œå¯¼è‡´æ„å¤–çš„åæœã€‚å¦‚æœæˆ‘ä»¬çš„ç¯å¢ƒä¸­å‡ºç°äº†éšœç¢ç‰©ï¼Œå¹¶ä¸”ä¸éšœç¢ç‰©å‘ç”Ÿç¢°æ’ä¼šé˜»æ­¢æˆ‘ä»¬çš„æ™ºèƒ½ä½“å®Œæˆä»»åŠ¡ï¼Œé‚£ä¹ˆæ˜¾ç„¶ï¼Œå¦‚æœæˆ‘ä»¬çš„æ™ºèƒ½ä½“è¿˜æ²¡æœ‰é‡åˆ°è¿‡è¿™ä¸ªéšœç¢ç‰©ï¼Œå®ƒæ³¨å®šä¼šå¤±è´¥ã€‚å¹¸è¿çš„æ˜¯ï¼Œåœ¨è´å¶æ–¯æ·±åº¦å­¦ä¹ çš„æƒ…å†µä¸‹ï¼Œæƒ…å†µå¹¶éå¦‚æ­¤ã€‚åªè¦æˆ‘ä»¬æœ‰æŸç§æ–¹å¼æ¥æ„ŸçŸ¥éšœç¢ç‰©ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“å°±èƒ½å¤Ÿæ£€æµ‹åˆ°éšœç¢ç‰©å¹¶é€‰æ‹©ä¸åŒçš„è·¯å¾„â€”â€”å³ä½¿è¯¥éšœç¢ç‰©åœ¨ä¹‹å‰çš„å›åˆä¸­æ²¡æœ‰å‡ºç°ã€‚
- en: '![PIC](img/file179.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file179.jpg)'
- en: 'FigureÂ 8.12: Illustration of how uncertainty affects the actions of a reinforcement
    learning agent'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.12ï¼šä¸ç¡®å®šæ€§å¦‚ä½•å½±å“å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“è¡ŒåŠ¨çš„ç¤ºæ„å›¾
- en: 'This is possible thanks to our uncertainty estimates. When the model encounters
    something unusual, its uncertainty estimate for that prediction will be high.
    Thus, if we incorporate this into our MPC equation, we can balance reward with
    uncertainty, ensuring that we prioritize lower risk over higher reward. To do
    so, we modify our MPC equation as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€åˆ‡ä¹‹æ‰€ä»¥å¯èƒ½ï¼Œå¾—ç›Šäºæˆ‘ä»¬çš„ä¸ç¡®å®šæ€§ä¼°è®¡ã€‚å½“æ¨¡å‹é‡åˆ°ä¸å¯»å¸¸çš„æƒ…å†µæ—¶ï¼Œå®ƒå¯¹è¯¥é¢„æµ‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡å°†ä¼šè¾ƒé«˜ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬å°†å…¶èå…¥åˆ°æˆ‘ä»¬çš„MPCæ–¹ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°±èƒ½åœ¨å¥–åŠ±å’Œä¸ç¡®å®šæ€§ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ï¼Œç¡®ä¿æˆ‘ä»¬ä¼˜å…ˆè€ƒè™‘è¾ƒä½çš„é£é™©ï¼Œè€Œéè¾ƒé«˜çš„å¥–åŠ±ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä¿®æ”¹äº†æˆ‘ä»¬çš„MPCæ–¹ç¨‹ï¼Œå…·ä½“å¦‚ä¸‹ï¼š
- en: '![anext = argmax (yi âˆ’ Î»Ïƒi)âˆ€ai âˆˆ As ](img/file180.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![anext = argmax (yi âˆ’ Î»Ïƒi)âˆ€ai âˆˆ As ](img/file180.jpg)'
- en: Here, we see that weâ€™re now subtracting a value, *Î»Ïƒ*[*i*], from our reward
    prediction *y*[*i*]. This is because *Ïƒ*[*i*] is our uncertainty associated with
    the *i*th prediction. We use *Î»* to scale the uncertainty so that it appropriately
    penalizes uncertain actions; this is a parameter we can tune depending on the
    application. With a sufficiently well calibrated method, weâ€™ll see larger values
    for *Ïƒ*[*i*] in cases where the model is uncertain about its predictions. Letâ€™s
    build on our earlier code example to see this in action.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬æ­£åœ¨ä»æˆ‘ä»¬çš„å¥–åŠ±é¢„æµ‹ *y*[*i*] ä¸­å‡å»ä¸€ä¸ªå€¼ï¼Œ*Î»Ïƒ*[*i*]ã€‚è¿™æ˜¯å› ä¸º *Ïƒ*[*i*] æ˜¯ä¸ç¬¬ *i* æ¬¡é¢„æµ‹ç›¸å…³çš„ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬ä½¿ç”¨
    *Î»* æ¥ç¼©æ”¾ä¸ç¡®å®šæ€§ï¼Œä»¥ä¾¿é€‚å½“æƒ©ç½šä¸ç¡®å®šçš„åŠ¨ä½œï¼›è¿™æ˜¯ä¸€ä¸ªå¯ä»¥æ ¹æ®åº”ç”¨è¿›è¡Œè°ƒæ•´çš„å‚æ•°ã€‚é€šè¿‡ä¸€ä¸ªç»è¿‡è‰¯å¥½æ ¡å‡†çš„æ–¹æ³•ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°åœ¨æ¨¡å‹å¯¹é¢„æµ‹ä¸ç¡®å®šæ—¶ï¼Œ*Ïƒ*[*i*]
    çš„å€¼ä¼šè¾ƒå¤§ã€‚è®©æˆ‘ä»¬åœ¨ä¹‹å‰çš„ä»£ç ç¤ºä¾‹çš„åŸºç¡€ä¸Šï¼Œçœ‹çœ‹è¿™ä¸€è¿‡ç¨‹å¦‚ä½•å®ç°ã€‚
- en: 'Step 1: Introducing obstacles'
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥ï¼šå¼•å…¥éšœç¢ç‰©
- en: 'To create a challenge for our agent, weâ€™re going to introduce obstacles to
    our environment. To test how our agent responds to unfamiliar input, weâ€™re going
    to change the policy that our obstacle follows - it will either follow a static
    policy or a dynamic policy depending on our environment settings. Weâ€™ll change
    the `__init__()` function for our `Environment` class to incorporate these changes:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç»™æˆ‘ä»¬çš„æ™ºèƒ½ä½“åˆ¶é€ æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å°†å‘ç¯å¢ƒä¸­å¼•å…¥éšœç¢ç‰©ã€‚ä¸ºäº†æµ‹è¯•æ™ºèƒ½ä½“å¦‚ä½•åº”å¯¹ä¸ç†Ÿæ‚‰çš„è¾“å…¥ï¼Œæˆ‘ä»¬å°†æ”¹å˜éšœç¢ç‰©çš„ç­–ç•¥â€”â€”å®ƒå°†æ ¹æ®æˆ‘ä»¬çš„ç¯å¢ƒè®¾ç½®ï¼Œé€‰æ‹©éµå¾ªé™æ€ç­–ç•¥æˆ–åŠ¨æ€ç­–ç•¥ã€‚æˆ‘ä»¬å°†ä¿®æ”¹
    `Environment` ç±»çš„ `__init__()` å‡½æ•°ï¼Œä»¥ä¾¿æ•´åˆè¿™äº›æ›´æ”¹ï¼š
- en: '[PRE86]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: Thereâ€™s quite a lot going on here, so weâ€™ll go through each of the changes.
    First, to determine whether the obstacle is static or dynamic, we set the `dynamic_obstacle`
    variable. If this is `True`, then weâ€™ll randomly set the obstacle location. If
    itâ€™s `False`, then our object will sit in the middle of our environment. Weâ€™re
    also setting our `lambda` (*Î»*) parameter here, which defaults to 2.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ¶‰åŠçš„å†…å®¹æ¯”è¾ƒå¤æ‚ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†é€ä¸€è®²è§£æ¯ä¸ªæ›´æ”¹ã€‚é¦–å…ˆï¼Œä¸ºäº†ç¡®å®šéšœç¢ç‰©æ˜¯é™æ€çš„è¿˜æ˜¯åŠ¨æ€çš„ï¼Œæˆ‘ä»¬è®¾ç½®äº† `dynamic_obstacle` å˜é‡ã€‚å¦‚æœè¯¥å€¼ä¸º
    `True`ï¼Œæˆ‘ä»¬å°†éšæœºè®¾ç½®éšœç¢ç‰©çš„ä½ç½®ã€‚å¦‚æœè¯¥å€¼ä¸º `False`ï¼Œåˆ™æˆ‘ä»¬çš„ç‰©ä½“å°†åœç•™åœ¨ç¯å¢ƒçš„ä¸­å¤®ã€‚æˆ‘ä»¬è¿˜åœ¨æ­¤è®¾ç½®äº†æˆ‘ä»¬çš„ `lambda` (*Î»*)
    å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 2ã€‚
- en: 'Weâ€™ve also introduced a `while` loop here when setting `target_location`: weâ€™ve
    done this to ensure that thereâ€™s some distance between the agent and the target.
    We need to do this to ensure thereâ€™s space between our agent and our target to
    drop in our dynamic obstacle â€“ otherwise our agent may never encounter the obstacle
    (which would somewhat defeat the point of this example).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜åœ¨è®¾ç½® `target_location` æ—¶å¼•å…¥äº†ä¸€ä¸ª `while` å¾ªç¯ï¼šæˆ‘ä»¬è¿™ä¹ˆåšæ˜¯ä¸ºäº†ç¡®ä¿æ™ºèƒ½ä½“å’Œç›®æ ‡ä¹‹é—´æœ‰ä¸€å®šçš„è·ç¦»ã€‚æˆ‘ä»¬éœ€è¦è¿™ä¹ˆåšæ˜¯ä¸ºäº†ç¡®ä¿åœ¨æ™ºèƒ½ä½“å’Œç›®æ ‡ä¹‹é—´ç•™æœ‰è¶³å¤Ÿçš„ç©ºé—´ï¼Œä»¥ä¾¿æ”¾ç½®åŠ¨æ€éšœç¢ç‰©â€”â€”å¦åˆ™ï¼Œæ™ºèƒ½ä½“å¯èƒ½æ°¸è¿œæ— æ³•é‡åˆ°è¿™ä¸ªéšœç¢ç‰©ï¼ˆè¿™å°†ç¨å¾®è¿èƒŒæœ¬ç¤ºä¾‹çš„æ„ä¹‰ï¼‰ã€‚
- en: 'Lastly, we compute the obstacle location on line 17: youâ€™ll note that this
    just sets it to the middle of the environment. This is because we use the `dynamic_obstacle`
    flag later on to place the obstacle between the agent and the target â€“ we do this
    during the `calculate_ideal_steps()` function, as this way we know the obstacle
    will lie along the agentâ€™s ideal path (and is thus more likely to be encountered).'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬åœ¨ç¬¬ 17 è¡Œè®¡ç®—éšœç¢ç‰©çš„ä½ç½®ï¼šä½ ä¼šæ³¨æ„åˆ°è¿™åªæ˜¯å°†å®ƒè®¾ç½®åœ¨ç¯å¢ƒçš„ä¸­å¤®ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬ç¨åä¼šä½¿ç”¨ `dynamic_obstacle` æ ‡å¿—å°†éšœç¢ç‰©æ”¾ç½®åœ¨æ™ºèƒ½ä½“å’Œç›®æ ‡ä¹‹é—´â€”â€”æˆ‘ä»¬åœ¨
    `calculate_ideal_steps()` å‡½æ•°ä¸­è¿™ä¹ˆåšï¼Œå› ä¸ºè¿™æ ·æˆ‘ä»¬å°±çŸ¥é“éšœç¢ç‰©å°†ä½äºæ™ºèƒ½ä½“çš„ç†æƒ³è·¯å¾„ä¸Šï¼ˆå› æ­¤æ›´æœ‰å¯èƒ½è¢«é‡åˆ°ï¼‰ã€‚
- en: 'Step 2: Placing our dynamic obstacle'
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 2ï¼šæ”¾ç½®åŠ¨æ€éšœç¢ç‰©
- en: 'When `dynamic_obstacle` is `True`, we want to place our obstacle somewhere
    different each episode, thus posing more of a challenge for our agent. To do so,
    we add a modification to the `calculate_ideal_steps()` function, as mentioned
    previously:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ `dynamic_obstacle` ä¸º `True` æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨æ¯ä¸ªå›åˆå°†éšœç¢ç‰©æ”¾ç½®åœ¨ä¸åŒçš„ä½ç½®ï¼Œä»è€Œä¸ºæˆ‘ä»¬çš„æ™ºèƒ½ä½“å¸¦æ¥æ›´å¤šæŒ‘æˆ˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åœ¨ä¹‹å‰æåˆ°çš„
    `calculate_ideal_steps()` å‡½æ•°ä¸­è¿›è¡Œäº†ä¸€äº›ä¿®æ”¹ï¼š
- en: '[PRE87]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Here, we see that we call `np.random.randint(0,Â 2)` on each iteration of the
    `while` loop. This is to randomize in which position the obstacle is placed along
    the ideal path.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬åœ¨æ¯æ¬¡æ‰§è¡Œ `while` å¾ªç¯æ—¶éƒ½è°ƒç”¨äº† `np.random.randint(0, 2)`ã€‚è¿™æ˜¯ä¸ºäº†éšæœºåŒ–éšœç¢ç‰©æ²¿ç†æƒ³è·¯å¾„çš„æ”¾ç½®ä½ç½®ã€‚
- en: 'Step 3: Adding sensing'
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 3ï¼šæ·»åŠ æ„ŸçŸ¥åŠŸèƒ½
- en: 'Our agent will have no hope of avoiding the object introduced into our environment
    if it canâ€™t sense the object. As such, weâ€™ll add a function to simulate a sensor:
    `get_obstacle_proximity()`. This sensor will give our agent information on how
    close it would get to an object were it to take a certain action. Weâ€™ll have this
    return progressively higher values depending on how close to the object a given
    action would place our agent. If the action places our agent sufficiently far
    from the object (in this case, at least 4.5 spaces), then our sensor will return
    zero. This sensing function allows our agent to effectively see one step ahead,
    so we can think of the sensor as having a range of one step.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„æ™ºèƒ½ä½“æ— æ³•æ„ŸçŸ¥ç¯å¢ƒä¸­å¼•å…¥çš„ç‰©ä½“ï¼Œé‚£ä¹ˆå®ƒå°†æ²¡æœ‰ä»»ä½•å¸Œæœ›é¿å…è¿™ä¸ªç‰©ä½“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ·»åŠ ä¸€ä¸ªå‡½æ•°æ¥æ¨¡æ‹Ÿä¼ æ„Ÿå™¨ï¼š`get_obstacle_proximity()`ã€‚è¯¥ä¼ æ„Ÿå™¨å°†ä¸ºæˆ‘ä»¬çš„æ™ºèƒ½ä½“æä¾›å…³äºå¦‚æœå®ƒæ‰§è¡ŒæŸä¸ªç‰¹å®šåŠ¨ä½œæ—¶ï¼Œå®ƒä¼šæ¥è¿‘ç‰©ä½“çš„è·ç¦»ä¿¡æ¯ã€‚æ ¹æ®ç»™å®šåŠ¨ä½œå°†æˆ‘ä»¬çš„æ™ºèƒ½ä½“é è¿‘ç‰©ä½“çš„è·ç¦»ï¼Œæˆ‘ä»¬å°†è¿”å›é€æ¸å¢å¤§çš„æ•°å€¼ã€‚å¦‚æœåŠ¨ä½œå°†æ™ºèƒ½ä½“ç½®äºè¶³å¤Ÿè¿œçš„ä½ç½®ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè‡³å°‘
    4.5 ä¸ªç©ºé—´ï¼‰ï¼Œåˆ™æˆ‘ä»¬çš„ä¼ æ„Ÿå™¨å°†è¿”å›é›¶ã€‚è¿™ä¸ªæ„ŸçŸ¥åŠŸèƒ½ä½¿å¾—æˆ‘ä»¬çš„æ™ºèƒ½ä½“èƒ½å¤Ÿæœ‰æ•ˆåœ°çœ‹åˆ°ä¸€æ­¥ä¹‹é¥ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å°†è¯¥ä¼ æ„Ÿå™¨è§†ä¸ºå…·æœ‰ä¸€æ­¥çš„æ„ŸçŸ¥èŒƒå›´ã€‚
- en: '[PRE88]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Here, we first compute the future proximity for the agent given each action,
    after which we compute integer â€proximityâ€ values. These are computed by first
    constructing Boolean arrays for each proximity condition, in this case being *Î´*[*o*]
    *<* 2*.*5, *Î´*[*o*] *<* 3*.*5, and *Î´*[*o*] *<* 4*.*5, where *Î´*[*o*] is the distance
    to the obstacle. We then sum these such that the proximity score has integer values
    of 3, 2, or 1 depending on how many of the criteria are met. This gives us a sensor
    that returns some basic information about the obstacleâ€™s future proximity for
    each of the proposed actions.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é¦–å…ˆè®¡ç®—æ¯ä¸ªåŠ¨ä½œåæ™ºèƒ½ä½“çš„æœªæ¥æ¥è¿‘åº¦ï¼Œç„¶åè®¡ç®—æ•´æ•°â€œæ¥è¿‘åº¦â€å€¼ã€‚è¿™äº›å€¼æ˜¯é€šè¿‡é¦–å…ˆæ„é€ æ¯ä¸ªæ¥è¿‘åº¦æ¡ä»¶çš„å¸ƒå°”æ•°ç»„æ¥è®¡ç®—çš„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹åˆ†åˆ«ä¸º *Î´*[*o*]
    *<* 2*.*5, *Î´*[*o*] *<* 3*.*5 å’Œ *Î´*[*o*] *<* 4*.*5ï¼Œå…¶ä¸­ *Î´*[*o*] æ˜¯ä¸éšœç¢ç‰©çš„è·ç¦»ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†è¿™äº›æ¡ä»¶æ±‚å’Œï¼Œä½¿å¾—æ¥è¿‘åº¦å¾—åˆ†å…·æœ‰
    3ã€2 æˆ– 1 çš„æ•´æ•°å€¼ï¼Œå…·ä½“å–å†³äºæ»¡è¶³å¤šå°‘ä¸ªæ¡ä»¶ã€‚è¿™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä¼ æ„Ÿå™¨ï¼Œå®ƒä¼šæ ¹æ®æ¯ä¸ªæè®®çš„åŠ¨ä½œè¿”å›æœ‰å…³éšœç¢ç‰©æœªæ¥æ¥è¿‘åº¦çš„åŸºæœ¬ä¿¡æ¯ã€‚
- en: 'Step 4: Modifying our reward function'
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: æ­¥éª¤ 4ï¼šä¿®æ”¹å¥–åŠ±å‡½æ•°
- en: 'The last thing we need to do to prepare our environment is to update our reward
    function:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†å¤‡ç¯å¢ƒçš„æœ€åä¸€ä»¶äº‹æ˜¯æ›´æ–°æˆ‘ä»¬çš„å¥–åŠ±å‡½æ•°ï¼š
- en: '[PRE89]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Here, weâ€™ve added a statement to check whether the agent and obstacle have collided
    (checking whether the distance between the two is zero). If so, weâ€™ll return a
    reward of 0, and set both the `collision` and `is_done` variables to `True`. This
    introduces a new termination criteria, **collision**, and will allow our agent
    to learn that collisions are bad, as these receive the lowest reward.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€æ¡è¯­å¥æ¥æ£€æŸ¥ä»£ç†ä¸éšœç¢ç‰©æ˜¯å¦å‘ç”Ÿç¢°æ’ï¼ˆæ£€æŸ¥ä¸¤è€…ä¹‹é—´çš„è·ç¦»æ˜¯å¦ä¸ºé›¶ï¼‰ã€‚å¦‚æœå‘ç”Ÿç¢°æ’ï¼Œæˆ‘ä»¬å°†è¿”å›å¥–åŠ±å€¼0ï¼Œå¹¶å°†`collision`å’Œ`is_done`å˜é‡è®¾ç½®ä¸º`True`ã€‚è¿™å¼•å…¥äº†æ–°çš„ç»ˆæ­¢æ ‡å‡†â€”â€”**ç¢°æ’**ï¼Œå¹¶å°†å…è®¸æˆ‘ä»¬çš„ä»£ç†å­¦ä¹ åˆ°ç¢°æ’æ˜¯æœ‰å®³çš„ï¼Œå› ä¸ºè¿™äº›æƒ…å†µä¼šå¾—åˆ°æœ€ä½çš„å¥–åŠ±ã€‚
- en: 'Step 5: Initializing our uncertainty-aware model'
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬5æ­¥ï¼šåˆå§‹åŒ–æˆ‘ä»¬çš„ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ¨¡å‹
- en: 'Now that our environment is ready, we need a new model â€“ one capable of producing
    uncertainty estimates. For this model, weâ€™ll use an MC dropout network with a
    single hidden layer:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„ç¯å¢ƒå·²ç»å‡†å¤‡å¥½ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ–°çš„æ¨¡å‹â€”â€”ä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆä¸ç¡®å®šæ€§ä¼°è®¡çš„æ¨¡å‹ã€‚å¯¹äºè¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå¸¦æœ‰å•ä¸ªéšè—å±‚çš„MC dropoutç½‘ç»œï¼š
- en: '[PRE90]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: This should look pretty familiar, but youâ€™ll notice a few key differences. First,
    weâ€™re again using the Huber loss. Secondly, weâ€™ve introduced a dictionary, `proximity_dict`,
    which will record the proximity values received from the sensor and the associated
    model uncertainties. This will allow us to evaluate our modelâ€™s sensitivity to
    anomalous proximity values later on.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœ‹èµ·æ¥åº”è¯¥å¾ˆç†Ÿæ‚‰ï¼Œä½†ä½ ä¼šæ³¨æ„åˆ°å‡ ä¸ªå…³é”®çš„ä¸åŒä¹‹å¤„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å†æ¬¡ä½¿ç”¨HuberæŸå¤±å‡½æ•°ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªå­—å…¸`proximity_dict`ï¼Œå®ƒå°†è®°å½•ä»ä¼ æ„Ÿå™¨æ¥æ”¶åˆ°çš„é‚»è¿‘å€¼å’Œç›¸å…³çš„æ¨¡å‹ä¸ç¡®å®šæ€§ã€‚è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿç¨åè¯„ä¼°æ¨¡å‹å¯¹å¼‚å¸¸é‚»è¿‘å€¼çš„æ•æ„Ÿæ€§ã€‚
- en: 'Step 6: Fitting our MC dropout network'
  id: totrans-364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬6æ­¥ï¼šæ‹Ÿåˆæˆ‘ä»¬çš„MC Dropoutç½‘ç»œ
- en: 'Next, we need the following lines:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ä»¥ä¸‹å‡ è¡Œä»£ç ï¼š
- en: '[PRE91]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: This should again look very familiar â€“ weâ€™re simply preparing our data by first
    scaling our inputs before fitting our model.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åº”è¯¥å†æ¬¡çœ‹èµ·æ¥å¾ˆç†Ÿæ‚‰â€”â€”æˆ‘ä»¬åªæ˜¯é€šè¿‡é¦–å…ˆå¯¹è¾“å…¥è¿›è¡Œç¼©æ”¾æ¥å‡†å¤‡æ•°æ®ï¼Œç„¶åæ‹Ÿåˆæˆ‘ä»¬çš„æ¨¡å‹ã€‚
- en: 'Step 7: Making predictions'
  id: totrans-368
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬7æ­¥ï¼šè¿›è¡Œé¢„æµ‹
- en: 'Here, we see that weâ€™ve slightly modified our `predict()` function:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬ç¨å¾®ä¿®æ”¹äº†`predict()`å‡½æ•°ï¼š
- en: '[PRE92]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: More specifically, weâ€™ve added the `obstacle_proximity` and `dynamic_obstacle`
    variables. The former allows us to receive the sensor information and incorporate
    this in the inputs we pass to our model. The latter is a flag telling us whether
    weâ€™re in the dynamic obstacle phase â€“ if so, we want to record information about
    the sensor values and uncertainties in our `proximity_dict` dictionary.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬æ·»åŠ äº†`obstacle_proximity`å’Œ`dynamic_obstacle`å˜é‡ã€‚å‰è€…å…è®¸æˆ‘ä»¬æ¥æ”¶ä¼ æ„Ÿå™¨ä¿¡æ¯ï¼Œå¹¶å°†å…¶çº³å…¥ä¼ é€’ç»™æ¨¡å‹çš„è¾“å…¥ä¸­ã€‚åè€…æ˜¯ä¸€ä¸ªæ ‡å¿—ï¼Œå‘Šè¯‰æˆ‘ä»¬æ˜¯å¦è¿›å…¥äº†åŠ¨æ€éšœç¢ç‰©é˜¶æ®µâ€”â€”å¦‚æœæ˜¯ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨`proximity_dict`å­—å…¸ä¸­è®°å½•ä¼ æ„Ÿå™¨å€¼å’Œä¸ç¡®å®šæ€§çš„ç›¸å…³ä¿¡æ¯ã€‚
- en: 'The next block of prediction code should again look very familiar:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€æ®µé¢„æµ‹ä»£ç åº”è¯¥å†æ¬¡çœ‹èµ·æ¥å¾ˆç†Ÿæ‚‰ï¼š
- en: '[PRE93]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: This function simply implements the MC dropout inference, obtaining predictions
    for `nb_inference` forward passes, and returns the means and standard deviations
    associated with our predictive distributions.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å‡½æ•°ç®€å•åœ°å®ç°äº†MC dropoutæ¨æ–­ï¼Œé€šè¿‡`nb_inference`æ¬¡å‰å‘ä¼ é€’è·å¾—é¢„æµ‹ï¼Œå¹¶è¿”å›ä¸æˆ‘ä»¬çš„é¢„æµ‹åˆ†å¸ƒç›¸å…³çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚
- en: 'Step 8: Adapting our standard model'
  id: totrans-375
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬8æ­¥ï¼šè°ƒæ•´æˆ‘ä»¬çš„æ ‡å‡†æ¨¡å‹
- en: 'To understand the difference that our Bayesian model makes, weâ€™ll need to compare
    it with a non-Bayesian model. As such, weâ€™ll update our `RLModel` class from earlier,
    adding the ability to incorporate proximity information from our proximity sensor:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç†è§£æˆ‘ä»¬çš„è´å¶æ–¯æ¨¡å‹å¸¦æ¥çš„å·®å¼‚ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶ä¸éè´å¶æ–¯æ¨¡å‹è¿›è¡Œæ¯”è¾ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ›´æ–°ä¹‹å‰çš„`RLModel`ç±»ï¼Œæ·»åŠ ä»é‚»è¿‘ä¼ æ„Ÿå™¨è·å–é‚»è¿‘ä¿¡æ¯çš„åŠŸèƒ½ï¼š
- en: '[PRE94]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Crucially, we see here that our decision function has not changed: because
    we donâ€™t have model uncertainties, our modelâ€™s `predict()` function is choosing
    actions based only on the predicted reward.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: è‡³å…³é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°æˆ‘ä»¬çš„å†³ç­–å‡½æ•°å¹¶æ²¡æœ‰å˜åŒ–ï¼šå› ä¸ºæˆ‘ä»¬æ²¡æœ‰æ¨¡å‹ä¸ç¡®å®šæ€§ï¼Œæˆ‘ä»¬çš„æ¨¡å‹çš„`predict()`å‡½æ•°ä»…åŸºäºé¢„æµ‹çš„å¥–åŠ±æ¥é€‰æ‹©åŠ¨ä½œã€‚
- en: 'Step 9: Preparing to run our new reinforcement learning experiment'
  id: totrans-379
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬9æ­¥ï¼šå‡†å¤‡è¿è¡Œæˆ‘ä»¬çš„æ–°å¼ºåŒ–å­¦ä¹ å®éªŒ
- en: 'Now weâ€™re ready to set up our new experiment. Weâ€™ll initialize the variables
    we used previously, and will introduce a few more:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½è®¾ç½®æˆ‘ä»¬çš„æ–°å®éªŒäº†ã€‚æˆ‘ä»¬å°†åˆå§‹åŒ–ä¹‹å‰ä½¿ç”¨çš„å˜é‡ï¼Œå¹¶å¼•å…¥å‡ ä¸ªæ–°çš„å˜é‡ï¼š
- en: '[PRE95]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Here, we see that weâ€™ve introduced a `collisions` variable and a `failed` variable.
    These will keep track of the number of collisions and the number of failed episodes
    so that we can compare the performance of our Bayesian model with that of our
    non-Bayesian model. Weâ€™re now ready to run our experiment!
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ª`collisions`å˜é‡å’Œä¸€ä¸ª`failed`å˜é‡ã€‚è¿™äº›å˜é‡å°†è¿½è¸ªç¢°æ’æ¬¡æ•°å’Œå¤±è´¥çš„å›åˆæ¬¡æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å°†è´å¶æ–¯æ¨¡å‹çš„è¡¨ç°ä¸éè´å¶æ–¯æ¨¡å‹çš„è¡¨ç°è¿›è¡Œæ¯”è¾ƒã€‚ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½è¿è¡Œå®éªŒäº†ï¼
- en: 'Step 10: Running our BDL reinforcement experiment'
  id: totrans-383
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬10æ­¥ï¼šè¿è¡Œæˆ‘ä»¬çš„BDLå¼ºåŒ–å­¦ä¹ å®éªŒ
- en: As before, weâ€™re going to run our experiment for 100 episodes. However, this
    time, weâ€™re only going to run training on our model for the first 50 episodes.
    After that, weâ€™ll stop training, and evaluate how well our model is able to find
    a safe path to the target. During these last 50 episodes, weâ€™ll set `dynamic_obstacle`
    to `True`, meaning our environment will now randomly choose a different position
    for our obstacle for each episode. Importantly, these random positions will be
    *along the* *ideal path* between the agent and its target.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†å¯¹å®éªŒè¿›è¡Œ 100 å›åˆçš„è¿è¡Œã€‚ç„¶è€Œï¼Œè¿™æ¬¡ï¼Œæˆ‘ä»¬åªä¼šåœ¨å‰ 50 å›åˆè¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚ä¹‹åï¼Œæˆ‘ä»¬å°†åœæ­¢è®­ç»ƒï¼Œè¯„ä¼°æ¨¡å‹åœ¨æ‰¾åˆ°å®‰å…¨è·¯å¾„åˆ°è¾¾ç›®æ ‡æ–¹é¢çš„è¡¨ç°ã€‚åœ¨è¿™æœ€å
    50 å›åˆä¸­ï¼Œæˆ‘ä»¬å°†`dynamic_obstacle`è®¾ç½®ä¸º`True`ï¼Œæ„å‘³ç€æˆ‘ä»¬çš„ç¯å¢ƒå°†ä¸ºæ¯ä¸€å›åˆéšæœºé€‰æ‹©ä¸€ä¸ªæ–°çš„éšœç¢ç‰©ä½ç½®ã€‚é‡è¦çš„æ˜¯ï¼Œè¿™äº›éšæœºä½ç½®å°†ä¼šä½äºä»£ç†ä¸ç›®æ ‡ä¹‹é—´çš„*ç†æƒ³è·¯å¾„*ä¸Šã€‚
- en: 'Letâ€™s take a look at the code:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ä»£ç ï¼š
- en: '[PRE96]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: First, we check whether the episode is within the first 50 episodes. If so,
    we instantiate our environment with `dynamic_obstacle=False`, and also set our
    global `dynamic_obstacle` variable to `False`.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬æ£€æŸ¥å›åˆæ˜¯å¦åœ¨å‰ 50 å›åˆä¹‹å†…ã€‚å¦‚æœæ˜¯ï¼Œæˆ‘ä»¬é€šè¿‡è®¾ç½®`dynamic_obstacle=False`å®ä¾‹åŒ–ç¯å¢ƒï¼Œå¹¶å°†å…¨å±€å˜é‡`dynamic_obstacle`è®¾ç½®ä¸º`False`ã€‚
- en: If the episode is one of the last 50 episodes, we create an environment with
    a randomly placed obstacle, and also set `epsilon` to 0, to ensure weâ€™re always
    using our model predictions when selecting actions.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœå›åˆæ˜¯æœ€å 50 å›åˆä¹‹ä¸€ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¸¦æœ‰éšæœºéšœç¢ç‰©çš„ç¯å¢ƒï¼Œå¹¶å°†`epsilon`è®¾ç½®ä¸º 0ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬åœ¨é€‰æ‹©åŠ¨ä½œæ—¶æ€»æ˜¯ä½¿ç”¨æ¨¡å‹é¢„æµ‹ã€‚
- en: 'Next, we enter our `while` loop, setting our agent in motion. This is very
    similar to the loop we saw in the last example, except this time weâ€™re calling
    `env.get_obstacle_proximity()`, using the returned obstacle proximity information
    in our predictions, and also storing this information in our episode history:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬è¿›å…¥`while`å¾ªç¯ï¼Œä½¿æˆ‘ä»¬çš„ä»£ç†å¼€å§‹ç§»åŠ¨ã€‚è¿™ä¸æˆ‘ä»¬åœ¨ä¸Šä¸€ä¸ªç¤ºä¾‹ä¸­çœ‹åˆ°çš„å¾ªç¯éå¸¸ç›¸ä¼¼ï¼Œåªä¸è¿‡è¿™æ¬¡æˆ‘ä»¬è°ƒç”¨äº†`env.get_obstacle_proximity()`ï¼Œå¹¶å°†è¿”å›çš„éšœç¢ç‰©æ¥è¿‘ä¿¡æ¯ç”¨äºæˆ‘ä»¬çš„é¢„æµ‹ï¼ŒåŒæ—¶ä¹Ÿå°†æ­¤ä¿¡æ¯å­˜å‚¨åœ¨å›åˆå†å²ä¸­ï¼š
- en: '[PRE97]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Lastly, weâ€™ll record some information about completed episodes and print the
    outcome of the most recent episode to our terminal. We update our `failed` and
    `collisions` variables and print whether the episode was complete successfully,
    the agent failed to find the target, or the agent collided with the obstacle:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘ä»¬å°†è®°å½•ä¸€äº›å·²å®Œæˆå›åˆçš„ä¿¡æ¯ï¼Œå¹¶å°†æœ€æ–°å›åˆçš„ç»“æœæ‰“å°åˆ°ç»ˆç«¯ã€‚æˆ‘ä»¬æ›´æ–°`failed`å’Œ`collisions`å˜é‡ï¼Œå¹¶æ‰“å°å›åˆæ˜¯å¦æˆåŠŸå®Œæˆï¼Œä»£ç†æ˜¯å¦æœªèƒ½æ‰¾åˆ°ç›®æ ‡ï¼Œæˆ–ä»£ç†æ˜¯å¦ä¸éšœç¢ç‰©å‘ç”Ÿç¢°æ’ï¼š
- en: '[PRE98]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: The last statement here also checks whether weâ€™re in the dynamic obstacle phase,
    and if not, runs a round of training and decrements our epsilon value (as with
    the last example).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æœ€åä¸€æ¡è¯­å¥è¿˜æ£€æŸ¥æˆ‘ä»¬æ˜¯å¦å¤„äºåŠ¨æ€éšœç¢ç‰©é˜¶æ®µï¼Œå¦‚æœä¸æ˜¯ï¼Œåˆ™è¿›è¡Œä¸€æ¬¡è®­ç»ƒï¼Œå¹¶å‡å°‘æˆ‘ä»¬çš„epsilonå€¼ï¼ˆå¦‚åŒä¸Šä¸€ä¸ªç¤ºä¾‹ï¼‰ã€‚
- en: 'So, how did we do? Repeating the above 100 episodes for both the `RLModel`
    and `RLModelDropout` models, we obtain the following results:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œæˆ‘ä»¬çš„è¡¨ç°å¦‚ä½•ï¼Ÿé‡å¤è¿›è¡Œä¸Šè¿° 100 å›åˆçš„å®éªŒï¼Œå¯¹äº`RLModel`å’Œ`RLModelDropout`æ¨¡å‹ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä»¥ä¸‹ç»“æœï¼š
- en: '|'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Model** | **Failed episodes** | **Collisions** | **Successful episodes**
    |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| **æ¨¡å‹** | **å¤±è´¥çš„å›åˆæ•°** | **ç¢°æ’æ¬¡æ•°** | **æˆåŠŸçš„å›åˆæ•°** |'
- en: '|'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **RLModelDropout** | 19 | 3 | 31 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| **RLModelDropout** | 19 | 3 | 31 |'
- en: '|'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **RLModel** | 16 | 10 | 34 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| **RLModel** | 16 | 10 | 34 |'
- en: '|'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  |  |  |  |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |'
- en: 'FigureÂ 8.13: A table showing collision predictions'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 8.13ï¼šä¸€å¼ æ˜¾ç¤ºç¢°æ’é¢„æµ‹çš„è¡¨æ ¼
- en: As we see here, there are advantages and disadvantages to consider when choosing
    whether to use a standard neural network or a Bayesian neural network â€“ the standard
    neural network achieves a greater number of successfully completed episodes. However,
    crucially, the agent using the Bayesian neural network only collided with the
    obstacle three times, compared to the standard methodâ€™s 10 times â€“ thatâ€™s a 70%
    reduction in collisions!
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œåœ¨é€‰æ‹©ä½¿ç”¨æ ‡å‡†ç¥ç»ç½‘ç»œè¿˜æ˜¯è´å¶æ–¯ç¥ç»ç½‘ç»œæ—¶ï¼Œéƒ½æœ‰å…¶ä¼˜ç¼ºç‚¹â€”â€”æ ‡å‡†ç¥ç»ç½‘ç»œå®Œæˆäº†æ›´å¤šçš„æˆåŠŸå›åˆã€‚ç„¶è€Œï¼Œå…³é”®æ˜¯ï¼Œä½¿ç”¨è´å¶æ–¯ç¥ç»ç½‘ç»œçš„ä»£ç†ä»…ä¸éšœç¢ç‰©å‘ç”Ÿäº†
    3 æ¬¡ç¢°æ’ï¼Œè€Œæ ‡å‡†æ–¹æ³•å‘ç”Ÿäº† 10 æ¬¡ç¢°æ’â€”â€”è¿™æ„å‘³ç€ç¢°æ’å‡å°‘äº† 70%ï¼
- en: Note that as the experiment is stochastic, your results may differ, but on the
    GitHub repository we have included the experiment complete with the seed used
    to produce these results.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œç”±äºå®éªŒæ˜¯éšæœºçš„ï¼Œæ‚¨çš„ç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œä½†åœ¨ GitHub ä»“åº“ä¸­ï¼Œæˆ‘ä»¬å·²åŒ…æ‹¬å®Œæ•´çš„å®éªŒä»¥åŠç”¨äºç”Ÿæˆè¿™äº›ç»“æœçš„ç§å­ã€‚
- en: 'We can get a better idea of why this is by looking at the data we recorded
    in `RLModelDropout`â€™s `proximity_dict` dictionary:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡æŸ¥çœ‹åœ¨`RLModelDropout`çš„`proximity_dict`å­—å…¸ä¸­è®°å½•çš„æ•°æ®ï¼Œæ›´å¥½åœ°ç†è§£ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼š
- en: '[PRE99]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'This produces the following plot:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†äº§ç”Ÿä»¥ä¸‹å›¾è¡¨ï¼š
- en: '![PIC](img/file181.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file181.png)'
- en: 'FigureÂ 8.14: Distribution of uncertainty estimates associated with increasing
    proximity sensor values'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8.14ï¼šä¸å¢åŠ çš„æ¥è¿‘ä¼ æ„Ÿå™¨å€¼ç›¸å…³çš„ä¸ç¡®å®šæ€§ä¼°è®¡åˆ†å¸ƒ
- en: As we see here, the model uncertainty estimates increase as the sensor values
    increase. This is because, during the first 50 episodes, our agent learns to avoid
    the centre of the environment (as this is where the obstacle is) â€“ thus it gets
    used to low (or zero) proximity sensor values. This means that higher sensor values
    are anomalous, and are thus able to be picked up by the modelâ€™s uncertainty estimates.
    Our agent then successfully accounts for this uncertainty using the uncertainty-aware
    MPC equation.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ¨¡å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡éšç€ä¼ æ„Ÿå™¨å€¼çš„å¢åŠ è€Œå¢åŠ ã€‚è¿™æ˜¯å› ä¸ºï¼Œåœ¨å‰50ä¸ªå›åˆä¸­ï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“å­¦ä¼šäº†é¿å¼€ç¯å¢ƒçš„ä¸­å¿ƒï¼ˆå› ä¸ºéšœç¢ç‰©å°±åœ¨è¿™é‡Œï¼‰â€”â€”å› æ­¤ï¼Œå®ƒä¹ æƒ¯äº†è¾ƒä½ï¼ˆæˆ–ä¸ºé›¶ï¼‰çš„æ¥è¿‘ä¼ æ„Ÿå™¨å€¼ã€‚è¿™æ„å‘³ç€è¾ƒé«˜çš„ä¼ æ„Ÿå™¨å€¼æ˜¯å¼‚å¸¸çš„ï¼Œå› æ­¤èƒ½å¤Ÿè¢«æ¨¡å‹çš„ä¸ç¡®å®šæ€§ä¼°è®¡æ‰€æ•æ‰åˆ°ã€‚ç„¶åï¼Œæˆ‘ä»¬çš„æ™ºèƒ½ä½“é€šè¿‡ä½¿ç”¨ä¸ç¡®å®šæ€§æ„ŸçŸ¥MPCæ–¹ç¨‹ï¼ŒæˆåŠŸåœ°è§£å†³äº†è¿™ç§ä¸ç¡®å®šæ€§ã€‚
- en: 'In this example, we saw how BDL can be applied to reinforcement learning to
    facilitate more cautious behaviour on the part of our reinforcement learning agents.
    While the example here was fairly basic, the implications are pretty significant:
    imagine this being applied to safety-critical applications. In these settings,
    weâ€™re often happy to accept poorer overall model performance if it meets better
    safety requirements. Thus, BDL has an important place within the field of safe
    reinforcement learning, allowing the development of reinforcement learning methods
    suitable for safety-critical scenarios.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚ä½•å°†BDLåº”ç”¨äºå¼ºåŒ–å­¦ä¹ ï¼Œä»¥ä¿ƒè¿›å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æ›´è°¨æ…çš„è¡Œä¸ºã€‚å°½ç®¡è¿™é‡Œçš„ç¤ºä¾‹ç›¸å¯¹åŸºç¡€ï¼Œä½†å…¶å«ä¹‰å´ç›¸å½“æ·±è¿œï¼šæƒ³è±¡ä¸€ä¸‹å°†å…¶åº”ç”¨äºå®‰å…¨å…³é”®çš„åº”ç”¨åœºæ™¯ã€‚åœ¨è¿™äº›ç¯å¢ƒä¸­ï¼Œå¦‚æœæ»¡è¶³æ›´å¥½çš„å®‰å…¨è¦æ±‚ï¼Œæˆ‘ä»¬å¾€å¾€æ„¿æ„æ¥å—æ¨¡å‹æ€§èƒ½è¾ƒå·®ã€‚å› æ­¤ï¼ŒBDLåœ¨å®‰å…¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸä¸­å æœ‰é‡è¦åœ°ä½ï¼Œèƒ½å¤Ÿå¼€å‘å‡ºé€‚ç”¨äºå®‰å…¨å…³é”®åœºæ™¯çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚
- en: 'In the next section, weâ€™ll see how BDL can be used to create models that are
    robust to another key consideration for real-world applications: adversarial inputs.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨BDLåˆ›å»ºå¯¹æŠ—æ€§è¾“å…¥å…·æœ‰é²æ£’æ€§çš„æ¨¡å‹ï¼Œè¿™æ˜¯ç°å®ä¸–ç•Œåº”ç”¨ä¸­çš„å¦ä¸€ä¸ªå…³é”®è€ƒè™‘å› ç´ ã€‚
- en: 8.6 Susceptibility to adversarial input
  id: totrans-446
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 å¯¹æŠ—æ€§è¾“å…¥çš„æ˜“æ„Ÿæ€§
- en: In [*ChapterÂ 3*](CH3.xhtml#x1-350003), [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003),
    we saw that we could fool a CNN by slightly perturbing the input pixels of an
    image. A picture that clearly looked like a cat was predicted as a dog with high
    confidence. The adversarial attack that we created (*FSGM*) is one of the many
    adversarial attacks that exist, and BDL might offer some protection against these
    attacks. Letâ€™s see how that works in practice.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[*ç¬¬3ç« *](CH3.xhtml#x1-350003)ã€[*æ·±åº¦å­¦ä¹ åŸºç¡€*](CH3.xhtml#x1-350003)ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°é€šè¿‡ç¨å¾®æ‰°åŠ¨å›¾åƒçš„è¾“å…¥åƒç´ ï¼Œå¯ä»¥æ¬ºéª—CNNã€‚åŸæœ¬æ¸…æ™°çœ‹èµ·æ¥åƒçŒ«çš„å›¾ç‰‡ï¼Œè¢«é«˜ç½®ä¿¡åº¦åœ°é¢„æµ‹ä¸ºç‹—ã€‚æˆ‘ä»¬åˆ›å»ºçš„å¯¹æŠ—æ€§æ”»å‡»ï¼ˆ*FSGM*ï¼‰æ˜¯è®¸å¤šå¯¹æŠ—æ€§æ”»å‡»ä¹‹ä¸€ï¼ŒBDLå¯èƒ½æä¾›ä¸€å®šçš„é˜²æŠ¤ä½œç”¨ã€‚è®©æˆ‘ä»¬çœ‹çœ‹è¿™åœ¨å®è·µä¸­æ˜¯å¦‚ä½•è¿ä½œçš„ã€‚
- en: 'Step 1: Model training'
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€æ­¥ï¼šæ¨¡å‹è®­ç»ƒ
- en: 'Instead of using a pre-trained model, as in [*ChapterÂ 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep* *Learning*](CH3.xhtml#x1-350003), we train a model from
    scratch. We use the same train and test data from [*ChapterÂ 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003) â€“ see that chapter for
    instructions on how to load the dataset. As a reminder, the dataset is a relatively
    small dataset of cats and dogs. We first define our model. We use a VGG-like architecture
    but add dropout after every `MaxPooling2D` layer:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸æ˜¯ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚åœ¨[*ç¬¬3ç« *](CH3.xhtml#x1-350003)ã€[*æ·±åº¦å­¦ä¹ åŸºç¡€*](CH3.xhtml#x1-350003)ä¸­æ‰€åšçš„é‚£æ ·ï¼Œè€Œæ˜¯ä»é›¶å¼€å§‹è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸[*ç¬¬3ç« *](CH3.xhtml#x1-350003)ã€[*æ·±åº¦å­¦ä¹ åŸºç¡€*](CH3.xhtml#x1-350003)ä¸­ç›¸åŒçš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®â€”â€”æœ‰å…³å¦‚ä½•åŠ è½½æ•°æ®é›†ï¼Œè¯·å‚è§è¯¥ç« èŠ‚ã€‚æé†’ä¸€ä¸‹ï¼Œæ•°æ®é›†æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„çŒ«ç‹—æ•°æ®é›†ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ç±»ä¼¼VGGçš„æ¶æ„ï¼Œä½†åœ¨æ¯ä¸ª`MaxPooling2D`å±‚ä¹‹ååŠ å…¥äº†dropoutï¼š
- en: '[PRE100]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'We then normalize our data, and compile and train our model:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¹¶ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹ï¼š
- en: '[PRE101]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: This will give us a model accuracy of about 85%.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ä½¿æˆ‘ä»¬çš„æ¨¡å‹å‡†ç¡®ç‡è¾¾åˆ°å¤§çº¦85%ã€‚
- en: 'Step 2: Running inference and evaluating our standard model'
  id: totrans-454
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ­¥ï¼šè¿è¡Œæ¨ç†å¹¶è¯„ä¼°æˆ‘ä»¬çš„æ ‡å‡†æ¨¡å‹
- en: 'Now that we have trained our model, letâ€™s see how much protection it offers
    against an adversarial attack. In [*ChapterÂ 3*](CH3.xhtml#x1-350003), [*Fundamentals
    of Deep Learning*](CH3.xhtml#x1-350003), we created an adversarial attack from
    scratch. In this chapter, weâ€™ll use the `cleverhans` library to create the same
    attack in one line for multiple images at once:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»è®­ç»ƒå¥½äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®ƒå¯¹æŠ—å¯¹æŠ—æ”»å‡»çš„ä¿æŠ¤æ•ˆæœæœ‰å¤šå¥½ã€‚åœ¨[*ç¬¬3ç« *](CH3.xhtml#x1-350003)[*æ·±åº¦å­¦ä¹ åŸºç¡€*](CH3.xhtml#x1-350003)ä¸­ï¼Œæˆ‘ä»¬ä»å¤´å¼€å§‹åˆ›å»ºäº†ä¸€ä¸ªå¯¹æŠ—æ”»å‡»ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`cleverhans`åº“æ¥ä¸ºå¤šä¸ªå›¾åƒä¸€æ¬¡æ€§åˆ›å»ºç›¸åŒçš„æ”»å‡»ï¼š
- en: '[PRE102]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Letâ€™s first measure the accuracy of our deterministic model on the original
    images and the adversarial images:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬è¡¡é‡æˆ‘ä»¬ç¡®å®šæ€§æ¨¡å‹åœ¨åŸå§‹å›¾åƒå’Œå¯¹æŠ—å›¾åƒä¸Šçš„å‡†ç¡®ç‡ï¼š
- en: '[PRE103]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Now that we have our predictions, we can print the accuracy:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†æˆ‘ä»¬çš„é¢„æµ‹ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å°å‡ºå‡†ç¡®ç‡ï¼š
- en: '[PRE104]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'We can see that our standard model offers little protection against this adversarial
    attack. Although it performs pretty well on standard images, it has an accuracy
    of 30.70% on adversarial images! Letâ€™s see if a Bayesian model can do better.
    Because we trained our model with dropout, we can easily make it an MC dropout
    model. We create an inference function where we keep dropout at inference, as
    indicated by the `training=True` parameter:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æ ‡å‡†æ¨¡å‹å¯¹è¿™ç§å¯¹æŠ—æ”»å‡»å‡ ä¹æ²¡æœ‰æä¾›ä»»ä½•ä¿æŠ¤ã€‚å°½ç®¡å®ƒåœ¨æ ‡å‡†å›¾åƒä¸Šçš„è¡¨ç°ç›¸å½“ä¸é”™ï¼Œä½†å®ƒåœ¨å¯¹æŠ—å›¾åƒä¸Šçš„å‡†ç¡®ç‡ä»…ä¸º30.70%ï¼è®©æˆ‘ä»¬çœ‹çœ‹ä¸€ä¸ªè´å¶æ–¯æ¨¡å‹èƒ½å¦åšå¾—æ›´å¥½ã€‚å› ä¸ºæˆ‘ä»¬è®­ç»ƒäº†å¸¦dropoutçš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°å°†å…¶è½¬å˜ä¸ºMC
    dropoutæ¨¡å‹ã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ¨ç†å‡½æ•°ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¿æŒdropoutï¼Œå¦‚`training=True`å‚æ•°æ‰€ç¤ºï¼š
- en: '[PRE105]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'With this function in place, we can replace the standard loop with MC dropout
    inference. We keep track of all our predictions again and run inference on our
    standard images and the adversarial images:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨MC dropoutæ¨ç†æ›¿ä»£æ ‡å‡†çš„å¾ªç¯ã€‚æˆ‘ä»¬å†æ¬¡è·Ÿè¸ªæ‰€æœ‰çš„é¢„æµ‹ï¼Œå¹¶å¯¹æ ‡å‡†å›¾åƒå’Œå¯¹æŠ—å›¾åƒè¿›è¡Œæ¨ç†ï¼š
- en: '[PRE106]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'And we can again print our accuracy:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å†æ¬¡æ‰“å°å‡ºæˆ‘ä»¬çš„å‡†ç¡®ç‡ï¼š
- en: '[PRE107]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: We can see that our simple modification made the model setup much more robust
    to adversarial examples. Instead of an accuracy of around 30%, we now obtain accuracy
    of more than 80%, pretty close to the accuracy of 83% of the deterministic model
    on the non-perturbed images. Moreover, we can see that MC dropout also improves
    the accuracy on our standard images by a few percentage points, from 83% to 86%.
    Almost no method offers perfect robustness to adversarial examples, so the fact
    that we can get so close to our modelâ€™s standard accuracy is a great achievement.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç®€å•çš„ä¿®æ”¹ä½¿å¾—æ¨¡å‹è®¾ç½®åœ¨é¢å¯¹å¯¹æŠ—æ ·æœ¬æ—¶æ›´åŠ ç¨³å¥ã€‚å‡†ç¡®ç‡ä»çº¦30%æé«˜åˆ°äº†80%ä»¥ä¸Šï¼Œæ¥è¿‘äºç¡®å®šæ€§æ¨¡å‹åœ¨æœªæ‰°åŠ¨å›¾åƒä¸Šçš„83%çš„å‡†ç¡®ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°ï¼ŒMC
    dropoutä¹Ÿä½¿å¾—æˆ‘ä»¬çš„æ ‡å‡†å›¾åƒå‡†ç¡®ç‡æé«˜äº†å‡ ä¸ªç™¾åˆ†ç‚¹ï¼Œä»83%æå‡åˆ°äº†86%ã€‚å‡ ä¹æ²¡æœ‰ä»»ä½•æ–¹æ³•èƒ½å¤Ÿå®Œç¾åœ°å¯¹æŠ—å¯¹æŠ—æ ·æœ¬ï¼Œå› æ­¤èƒ½å¤Ÿæ¥è¿‘æˆ‘ä»¬æ¨¡å‹åœ¨æ ‡å‡†å›¾åƒä¸Šçš„å‡†ç¡®ç‡æ˜¯ä¸€ä¸ªä¼Ÿå¤§çš„æˆå°±ã€‚
- en: 'Because our model has not seen the adversarial images before, a model with
    good uncertainty values should also have a lower confidence on average on the
    adversarial images compared to a standard model. Letâ€™s see if this is the case.
    We create a function to compute the average softmax value of the predictions of
    our deterministic model and create a similar function for our MC dropout predictions:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬çš„æ¨¡å‹ä¹‹å‰æ²¡æœ‰è§è¿‡å¯¹æŠ—å›¾åƒï¼Œæ‰€ä»¥ä¸€ä¸ªå…·æœ‰è‰¯å¥½ä¸ç¡®å®šæ€§å€¼çš„æ¨¡å‹åº”è¯¥åœ¨å¯¹æŠ—å›¾åƒä¸Šç›¸å¯¹äºæ ‡å‡†æ¨¡å‹è¡¨ç°å‡ºæ›´ä½çš„å¹³å‡ä¿¡å¿ƒã€‚è®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦æ˜¯è¿™æ ·ã€‚æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—æˆ‘ä»¬ç¡®å®šæ€§æ¨¡å‹é¢„æµ‹çš„å¹³å‡softmaxå€¼ï¼Œå¹¶ä¸ºMC
    dropouté¢„æµ‹åˆ›å»ºä¸€ä¸ªç±»ä¼¼çš„å‡½æ•°ï¼š
- en: '[PRE108]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'We can then print the mean softmax score for both models:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ‰“å°å‡ºä¸¤ä¸ªæ¨¡å‹çš„å¹³å‡softmaxåˆ†æ•°ï¼š
- en: '[PRE109]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: We can see that our standard model is actually slightly more confident on adversarial
    images compared to standard images, although its accuracy dropped significantly.
    However, our MC dropout model shows a lower confidence on the adversarial images
    compared to the standard images. Although the drop in confidence is not very large,
    it is good to see that the model is dropping its mean confidence on adversarial
    images, while keeping a reasonable accuracy.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œä¸æ ‡å‡†å›¾åƒç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ ‡å‡†æ¨¡å‹åœ¨å¯¹æŠ—å›¾åƒä¸Šçš„ä¿¡å¿ƒå®é™…ä¸Šç¨å¾®æ›´é«˜ï¼Œå°½ç®¡å‡†ç¡®ç‡æ˜¾è‘—ä¸‹é™ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„MC dropoutæ¨¡å‹åœ¨å¯¹æŠ—å›¾åƒä¸Šçš„ä¿¡å¿ƒä½äºæ ‡å‡†å›¾åƒã€‚è™½ç„¶ä¿¡å¿ƒçš„ä¸‹é™å¹…åº¦ä¸å¤§ï¼Œä½†æˆ‘ä»¬å¾ˆé«˜å…´çœ‹åˆ°ï¼Œå°½ç®¡å‡†ç¡®ç‡ä¿æŒåˆç†ï¼Œæ¨¡å‹åœ¨å¯¹æŠ—å›¾åƒä¸Šçš„å¹³å‡ä¿¡å¿ƒä¸‹é™äº†ã€‚
- en: 8.7 Summary
  id: totrans-473
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 æ€»ç»“
- en: In this chapter, we have illustrated the various applications of modern BDL
    in five different case studies. Each case study used code examples to highlight
    a particular strength of BDL in response to various, common problems in applied
    machine learning practice. First, we saw how BDL can be used to detect out-of-distribution
    images in a classification task. We then looked at how BDL methods can be used
    to make models more robust to dataset shift, which is a very common problem in
    production environments. Next, we learned how BDL can help us to select the most
    informative data points for training and updating our machine learning models.
    We then turned to reinforcement learning and saw how BDL can be used to facilitate
    more cautious behaviour in reinforcement learning agents. Finally, we saw how
    BDL can help us in the face of adversarial attacks.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡äº”ä¸ªä¸åŒçš„æ¡ˆä¾‹ç ”ç©¶å±•ç¤ºäº†ç°ä»£BDLçš„å„ç§åº”ç”¨ã€‚æ¯ä¸ªæ¡ˆä¾‹ç ”ç©¶éƒ½ä½¿ç”¨äº†ä»£ç ç¤ºä¾‹ï¼Œçªå‡ºäº†BDLåœ¨åº”å¯¹åº”ç”¨æœºå™¨å­¦ä¹ å®è·µä¸­çš„å„ç§å¸¸è§é—®é¢˜æ—¶çš„ç‰¹å®šä¼˜åŠ¿ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çœ‹åˆ°å¦‚ä½•ä½¿ç”¨BDLåœ¨åˆ†ç±»ä»»åŠ¡ä¸­æ£€æµ‹åˆ†å¸ƒå¤–å›¾åƒã€‚æ¥ç€ï¼Œæˆ‘ä»¬æ¢è®¨äº†BDLæ–¹æ³•å¦‚ä½•ç”¨äºä½¿æ¨¡å‹æ›´åŠ é²æ£’ï¼Œä»¥åº”å¯¹æ•°æ®é›†åç§»ï¼Œè¿™æ˜¯ç”Ÿäº§ç¯å¢ƒä¸­ä¸€ä¸ªéå¸¸å¸¸è§çš„é—®é¢˜ã€‚ç„¶åï¼Œæˆ‘ä»¬å­¦ä¹ äº†BDLå¦‚ä½•å¸®åŠ©æˆ‘ä»¬é€‰æ‹©æœ€æœ‰ä¿¡æ¯é‡çš„æ•°æ®ç‚¹ï¼Œä»¥è®­ç»ƒå’Œæ›´æ–°æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚æ¥ç€ï¼Œæˆ‘ä»¬è½¬å‘å¼ºåŒ–å­¦ä¹ ï¼Œçœ‹åˆ°BDLå¦‚ä½•å¸®åŠ©å¼ºåŒ–å­¦ä¹ ä»£ç†å®ç°æ›´åŠ è°¨æ…çš„è¡Œä¸ºã€‚æœ€åï¼Œæˆ‘ä»¬çœ‹åˆ°äº†BDLåœ¨é¢å¯¹å¯¹æŠ—æ€§æ”»å‡»æ—¶çš„åº”ç”¨ã€‚
- en: In the next chapter, we will have a look at the future of BDL by reviewing current
    trends and the latest methods.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡å›é¡¾å½“å‰è¶‹åŠ¿å’Œæœ€æ–°æ–¹æ³•æ¥å±•æœ›BDLçš„æœªæ¥ã€‚
- en: 8.8 Further reading
  id: totrans-476
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.8 è¿›ä¸€æ­¥é˜…è¯»
- en: 'The following reading list will offer a greater understanding of some of the
    topics we touched on in this chapter:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹é˜…è¯»æ¸…å•å°†å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£æˆ‘ä»¬åœ¨æœ¬ç« ä¸­æ¶‰åŠçš„ä¸€äº›ä¸»é¢˜ï¼š
- en: '*Benchmarking neural network robustness to common corruptions and* *perturbations*,
    Dan Hendrycks and Thomas Dietterich, 2019: this is the paper that introduced the
    image quality perturbations to benchmark model robustness, which we saw in the
    robustness case study.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*åŸºå‡†æµ‹è¯•ç¥ç»ç½‘ç»œå¯¹å¸¸è§æŸåå’Œ* *æ‰°åŠ¨*çš„é²æ£’æ€§ï¼ŒDan Hendrycks å’Œ Thomas Dietterichï¼Œ2019å¹´ï¼šè¿™ç¯‡è®ºæ–‡ä»‹ç»äº†å›¾åƒè´¨é‡æ‰°åŠ¨ï¼Œä»¥åŸºå‡†æµ‹è¯•æ¨¡å‹çš„é²æ£’æ€§ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨é²æ£’æ€§æ¡ˆä¾‹ç ”ç©¶ä¸­çœ‹åˆ°çš„é‚£æ ·ã€‚'
- en: '*Can You Trust Your Modelâ€™s Uncertainty? Evaluating predictive* *Uncertainty
    Under Dataset Shift*, Yaniv Ovadia, Emily Fertig *et* *al.*, 2019: this comparison
    paper uses image quality perturbations to introduce artificial dataset shift at
    different severity levels and measures how different deep neural networks respond
    to dataset shift in terms of accuracy and calibration.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ä½ èƒ½ä¿¡ä»»æ¨¡å‹çš„ä¸ç¡®å®šæ€§å—ï¼Ÿè¯„ä¼°æ•°æ®é›†åç§»ä¸‹çš„* *é¢„æµ‹ä¸ç¡®å®šæ€§*ï¼ŒYaniv Ovadiaã€Emily Fertig ç­‰ï¼Œ2019å¹´ï¼šè¿™ç¯‡æ¯”è¾ƒè®ºæ–‡ä½¿ç”¨å›¾åƒè´¨é‡æ‰°åŠ¨ï¼Œåœ¨ä¸åŒçš„ä¸¥é‡ç¨‹åº¦ä¸‹å¼•å…¥äººå·¥æ•°æ®é›†åç§»ï¼Œå¹¶è¡¡é‡ä¸åŒçš„æ·±åº¦ç¥ç»ç½‘ç»œåœ¨å‡†ç¡®æ€§å’Œæ ¡å‡†æ–¹é¢å¦‚ä½•å“åº”æ•°æ®é›†åç§»ã€‚'
- en: '*A Baseline for Detecting Misclassified and Out-of-Distribution* *Examples
    in Neural Networks*, Dan Hendrycks and Kevin Gimpel, 2016: this fundamental out-of-distribution
    detection paper introduces the concept and shows that softmax values are not perfect
    when it comes to OOD detection.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç”¨äºæ£€æµ‹ç¥ç»ç½‘ç»œä¸­è¯¯åˆ†ç±»å’Œåˆ†å¸ƒå¤–* *æ ·æœ¬çš„åŸºå‡†*ï¼ŒDan Hendrycks å’Œ Kevin Gimpelï¼Œ2016å¹´ï¼šè¿™ç¯‡åŸºç¡€æ€§çš„åˆ†å¸ƒå¤–æ£€æµ‹è®ºæ–‡ä»‹ç»äº†è¯¥æ¦‚å¿µï¼Œå¹¶è¡¨æ˜å½“æ¶‰åŠåˆ°åˆ†å¸ƒå¤–ï¼ˆOODï¼‰æ£€æµ‹æ—¶ï¼Œsoftmaxå€¼å¹¶ä¸å®Œç¾ã€‚'
- en: '*Enhancing The Reliability of Out-of-distribution Image Detection in* *Neural
    Networks*, Shiyu Liang, Yixuan Li and R. Srikant, 2017: shows that input perturbation
    and temperature scaling can improve the softmax baseline for out-of-distribution
    detection.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æé«˜ç¥ç»ç½‘ç»œä¸­åˆ†å¸ƒå¤–å›¾åƒæ£€æµ‹çš„å¯é æ€§*ï¼ŒShiyu Liangã€Yixuan Li å’Œ R. Srikantï¼Œ2017å¹´ï¼šè¡¨æ˜è¾“å…¥æ‰°åŠ¨å’Œæ¸©åº¦ç¼©æ”¾å¯ä»¥æ”¹å–„ç”¨äºåˆ†å¸ƒå¤–æ£€æµ‹çš„softmaxåŸºå‡†ã€‚'
- en: '*A Simple Unified Framework for Detecting Out-of-Distribution* *Samples and
    Adversarial Attacks*, Kimin Lee, Kibok Lee, Honglak Lee and Jinwoo Shin, 2018:
    shows that using the Mahalanobis distance can be effective for out-of-distribution
    detection.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç”¨äºæ£€æµ‹åˆ†å¸ƒå¤–* *æ ·æœ¬å’Œå¯¹æŠ—æ€§æ”»å‡»çš„ç®€å•ç»Ÿä¸€æ¡†æ¶*ï¼ŒKimin Leeã€Kibok Leeã€Honglak Lee å’Œ Jinwoo Shinï¼Œ2018å¹´ï¼šè¡¨æ˜ä½¿ç”¨é©¬å“ˆæ‹‰è¯ºæ¯”æ–¯è·ç¦»åœ¨åˆ†å¸ƒå¤–æ£€æµ‹ä¸­å¯èƒ½æ˜¯æœ‰æ•ˆçš„ã€‚'
