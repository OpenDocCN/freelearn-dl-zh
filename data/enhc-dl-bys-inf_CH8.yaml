- en: Chapter 8
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章
- en: Applying Bayesian Deep Learning
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 应用贝叶斯深度学习
- en: This chapter will guide you through a variety of applications of Bayesian deep
    learning (BDL). These will include the use of BDL in standard classification tasks,
    as well as demonstrating how it can be used in more sophisticated ways for out-of-distribution
    detection, data selection, and reinforcement learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将引导你了解贝叶斯深度学习（BDL）的多种应用。这些应用包括BDL在标准分类任务中的使用，以及展示如何在异常数据检测、数据选择和强化学习等更复杂的任务中使用BDL。
- en: 'We will cover these topics in the following sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中讨论这些主题：
- en: Detecting out-of-distribution data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测异常数据
- en: Being robust against dataset drift
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高对数据集漂移的鲁棒性
- en: Using data selection via uncertainty to keep models fresh
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于不确定性的数据显示选择，保持模型的新鲜度
- en: Using uncertainty estimates for smarter reinforcement learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不确定性估计进行更智能的强化学习
- en: Susceptibility to adversarial input
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗性输入的易感性
- en: 8.1 Technical requirements
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 技术要求
- en: 'All of the code for this book can be found on the GitHub repository for the
    book: [https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的所有代码都可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference](https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference)
- en: 8.2 Detecting out-of-distribution data
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 检测异常数据
- en: 'Typical neural networks do not handle out-of-distribution data well. We saw
    in [*Chapter 3*](CH3.xhtml#x1-350003), [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003)
    that a cat-dog classifier classified an image of a parachute as a dog with more
    than 99% confidence. In this section, we will look into what we can do about this
    vulnerability of neural networks. We will do the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的神经网络在处理异常数据时表现不佳。我们在[*第3章*](CH3.xhtml#x1-350003)、[*深度学习基础*](CH3.xhtml#x1-350003)中看到，猫狗分类器将一张降落伞的图像错误地分类为狗，并且置信度超过99%。在本节中，我们将探讨如何解决神经网络这一弱点。我们将进行以下操作：
- en: Explore the problem visually by perturbing a digit of the `MNIST` dataset
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过扰动`MNIST`数据集中的一个数字，直观地探索这个问题
- en: Explain the typical way out-of-distribution detection performance is reported
    in the literature
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释文献中通常如何报告异常数据检测的性能
- en: Review the out-of-distribution detection performance of some of the standard
    practical BDL methods we look at in this chapter
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾我们在本章中讨论的几种标准实用贝叶斯深度学习（BDL）方法在异常数据检测中的表现
- en: Explore even more practical methods that are specifically tailored to detect
    out-of-distribution detection
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索更多专门用于异常数据检测的实用方法
- en: 8.2.1 Exploring the problem of out-of-distribution detection
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 探索异常数据检测的问题
- en: 'To give you a better understanding of what out-of-distribution performance
    is like, we will start with a visual example. Here is what we will do:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地帮助你理解异常数据检测的效果，我们将从一个视觉示例开始。以下是我们将要做的事情：
- en: We will train a standard network on the `MNIST` digit dataset
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在`MNIST`数字数据集上训练一个标准网络
- en: We will then perturb a digit and gradually make it more out-of-distribution
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将扰动一个数字，并逐渐使其变得更加异常
- en: We will report the confidence score of a standard model and MC dropout
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将报告标准模型和MC dropout的置信度得分
- en: With this visual example, we can see how simple Bayesian methods can improve
    the out-of-distribution detection performance over a standard deep learning model.
    We start by training a simple model on the `MNIST` dataset.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个视觉示例，我们可以看到简单的贝叶斯方法如何在异常数据检测上优于标准的深度学习模型。我们首先在`MNIST`数据集上训练一个简单的模型。
- en: '![PIC](img/file160.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file160.png)'
- en: 'Figure 8.1: The classes of the MNIST dataset: 28x28 pixel images of the digits
    zero to nine'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：MNIST数据集的类别：零到九的28x28像素数字图像
- en: We use `TensorFlow` to train our model, `numpy` to make our images more out-of-distribution,
    and `Matplotlib` to visualize our data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`TensorFlow`来训练模型，使用`numpy`让我们的图像更具异常性，使用`Matplotlib`来可视化数据。
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `MNIST` dataset is available in TensorFlow, so we can just load it:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`MNIST`数据集可以在TensorFlow中找到，所以我们可以直接加载它：'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`MNIST` is a simple dataset, so a simple model allows us to achieve a test
    accuracy of more than 99%. We use a standard CNN with three convolutional layers:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`MNIST`是一个简单的数据集，因此使用简单的模型可以让我们在测试中达到超过99%的准确率。我们使用一个标准的CNN，包含三层卷积层：'
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can then compile and train our model. We obtain a validation accuracy of
    over 99% after just 5 epochs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以编译并训练我们的模型。经过5个epochs后，我们的验证准确率超过99%。
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, let’s see how this model handles out-of-distribution data. Imagine we
    deploy this model to recognize digits, but users sometimes fail to write down
    the entire digit. What happens when users do not write down the entire digit?
    We can get an answer to this question by gradually removing more and more information
    from a digit, and seeing how our model handles the perturbed inputs. We can define
    our function to remove `signal` as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这个模型如何处理分布外数据。假设我们部署这个模型来识别数字，但用户有时无法写下完整的数字。当用户没有写下完整的数字时会发生什么？我们可以通过逐渐移除数字中的信息，观察模型如何处理这些扰动输入，来回答这个问题。我们可以这样定义移除`signal`的函数：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And then we perturb our images:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们对图像进行扰动：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We only add perturbed images to our list of images if setting a row to 0 actually
    changes the original image ( `if np.array_equal(img, img_perturbed))` and stop
    once the image is completely black, meaning it just contains pixels with a value
    of 0\. We run inference on these images:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只有在将某一行设为0确实改变了原始图像时，才将扰动后的图像添加到我们的图像列表中（`if np.array_equal(img, img_perturbed))`），并且一旦图像完全变黑，即仅包含值为0的像素，我们就停止。我们对这些图像进行推理：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can then plot all images with their predicted labels and confidence scores:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以绘制所有图像及其预测标签和置信度分数：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This produces the following figure:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了如下的图：
- en: '![PIC](img/file161.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file161.png)'
- en: 'Figure 8.2: Predicted label and corresponding softmax score of a standard neural
    network for an image that is more and more out-of-distribution'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：标准神经网络对于逐渐偏离分布的图像所预测的标签及相应的softmax分数
- en: We can see in *Figure* [*8.2*](#x1-135222r2) that, initially, our model confidently
    classifies the image as a **2**. Remarkably, this confidence persists even when
    it seems unreasonable to do so. For example, the model still classifies image
    14 as a **2** with 97.83% confidence. Moreover, the model predicts with 92.32%
    confidence that a completely horizontal line is a **1**, as we can see in image
    17\. It looks like our model is overconfident in its predictions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图* [*8.2*](#x1-135222r2)中看到，最初，我们的模型非常自信地将图像分类为**2**。值得注意的是，即使在这种分类显得不合理时，这种自信依然存在。例如，模型仍然以97.83%的置信度将图像14分类为**2**。此外，模型还预测完全水平的线条是**1**，置信度为92.32%，正如我们在图像17中所见。这看起来我们的模型在预测时过于自信。
- en: 'Let’s see what a slightly different model would predict on these images. We’ll
    now use MC dropout as our model. By sampling, we should be able to increase the
    models’ uncertainty compared to a standard NN. Let’s first define our model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个稍有不同的模型会如何对这些图像做出预测。我们现在将使用MC Dropout作为我们的模型。通过采样，我们应该能够提高模型的不确定性，相较于标准的神经网络。我们先定义我们的模型：
- en: '[PRE8]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then let’s instantiate it:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们来实例化它：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Our model with dropout will achieve a similar accuracy as our vanilla model.
    Let’s now run inference with dropout and plot the mean confidence score of MC
    dropout:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用dropout的模型将实现与原始模型类似的准确性。现在，我们使用dropout进行推理，并绘制MC Dropout的平均置信度分数：
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This again produces a figure showing the predicted labels and their associated
    confidence scores:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这再次生成了一个图，显示了预测标签及其相关的置信度分数：
- en: '![PIC](img/file162.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file162.png)'
- en: 'Figure 8.3: Predicted label and corresponding softmax score of an MC dropout
    network for an image that is more and more out-of-distribution'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3：MC Dropout网络对于逐渐偏离分布的图像所预测的标签及相应的softmax分数
- en: 'We can see in *Figure* [*8.3*](#x1-135306r3) that the model is less certain
    on average. The model’s confidence decreases a lot when we remove rows from our
    image. That is desired behaviour: our model does not know the input, so it should
    be uncertain. However, we can also see that the model is not perfect:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图* [*8.3*](#x1-135306r3)中看到，模型的自信度平均来说较低。当我们从图像中移除行时，模型的置信度大幅下降。这是期望的行为：当模型不知道输入时，它应该表现出不确定性。然而，我们也能看到模型并不完美：
- en: It maintains a pretty high confidence for images that do not really look like
    a **2**.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于那些看起来并不像**2**的图像，模型仍然保持较高的置信度。
- en: The model’s confidence can change a lot when we delete one more row from our
    images. For example, the model’s confidence jumps from 61.72% to 37.20% between
    image 14 and 15.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们从图像中删除一行时，模型的置信度变化很大。例如，模型的置信度在图像14和图像15之间从61.72%跃升至37.20%。
- en: The model seems to be more confident that image 20, without any white pixels,
    is a **1**.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型似乎更有信心将没有任何白色像素的图像20分类为**1**。
- en: MC dropout is, in this case, a step in the right direction, but is not handling
    the out-of-distribution data perfectly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，MC Dropout是一个朝着正确方向迈出的步骤，但它并没有完美地处理分布外数据。
- en: 8.2.2 Systematically evaluating OOD detection performance
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 系统地评估OOD检测性能
- en: 'The preceding example suggests that MC dropout gives out-of-distribution images
    a lower confidence score on average. But we only evaluated 20 images with a limited
    variety – we simply removed a single row. This change moved the image more out-of-distribution,
    but all images shown in the previous section are relatively similar to the training
    distribution of `MNIST` if you compare it to, let’s say, natural images of objects.
    Images of airplanes, cars, or birds will definitely be much more out-of-distribution
    than an image of `MNIST` with a few black rows. So it seems to be reasonable that,
    if we want to evaluate the OOD detection performance of our model, we should also
    test it on images that are even more OOD, that is, from a completely different
    dataset. This is the approach that is typically taken in the literature to evaluate
    out-of-distribution detection performance. The procedure is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例表明，MC dropout通常会给出分布外图像较低的置信度分数。但我们仅评估了20张图像，且变化有限——我们只是删除了一行。这一变化使得图像更加分布外，但前一部分展示的所有图像与`MNIST`的训练分布相比，还是相对相似的，如果拿它和自然物体图像比较。例如，飞机、汽车或鸟类的图像肯定比带有几行黑色的`MNIST`图像更具分布外特征。因此，似乎合理的是，如果我们想评估模型的OOD检测性能，我们应该在更加分布外的图像上进行测试，也就是说，来自完全不同数据集的图像。这正是文献中通常用于评估分布外检测性能的方法。具体步骤如下：
- en: We train a model on in-distribution (ID) images.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在内部分布（ID）图像上训练模型。
- en: We take one or more completely different OOD datasets and feed these to our
    model.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选取一个或多个完全不同的OOD数据集，并将这些数据喂给我们的模型。
- en: We now treat the predictions of the model on the ID and OOD test datasets as
    a binary problem and compute a single score for every image.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将模型在ID和OOD测试数据集上的预测视为一个二进制问题，并为每个图像计算一个单一的得分。
- en: In the case of evaluation of the softmax score, this means that we take the
    model’s maximum softmax score for every ID and OOD image.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估softmax分数的情况下，这意味着我们为每个ID和OOD图像取模型的最大softmax分数。
- en: With these scores, we can compute binary metrics, such as the area under the
    receiver operating characteristic (AUROC).
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些得分，我们可以计算二进制指标，如接收者操作特征曲线下面积（AUROC）。
- en: The better the model performs on these binary metrics, the better the model’s
    OOD detection performance.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在这些二进制指标上的表现越好，模型的OOD检测性能就越好。
- en: 8.2.3 Simple out-of-distribution detection without retraining
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 无需重新训练的简单分布外检测
- en: 'Although MC dropout can be an effective method to detect out-of-distribution
    data, it comes with a major disadvantage at inference time: we need to run inference
    five, or maybe even a hundred, times instead of just once. Something similar can
    be said for certain other Bayesian deep learning methods: although they are principled,
    they are not always the most practical way to obtain a good OOD detection performance.
    The main downside is that they often require retraining of your network, which
    can be expensive to do if you have a lot of data. This is why there is an entire
    field of OOD detection methods that are not explicitly grounded on Bayesian theory,
    but can provide a good, simple, or even excellent baseline. These methods often
    do not require any retraining and can be applied out of the box on a standard
    neural network. Two methods that are often used in the OOD detection literature
    are worth mentioning:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MC dropout可以有效检测出分布外数据，但它在推理时存在一个主要缺点：我们需要进行五次，甚至一百次推理，而不是仅仅一次。对于某些其他贝叶斯深度学习方法也可以说类似：虽然它们有理论依据，但并不总是获得良好OOD检测性能的最实际方法。主要的缺点是，它们通常需要重新训练网络，如果数据量很大，这可能会非常昂贵。这就是为什么有一整套不显式依赖贝叶斯理论的OOD检测方法，但能提供良好、简单，甚至是优秀的基线。这些方法通常不需要任何重新训练，可以直接在标准神经网络上应用。文献中经常使用的两种方法值得一提：
- en: '**ODIN**: OOD detection with preprocessing and scaling'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ODIN**：使用预处理和缩放进行OOD检测'
- en: '**Mahalanobis**: OOD detection with intermediate features'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**马哈拉诺比斯**：使用中间特征进行OOD检测'
- en: 'ODIN: OOD detection with preprocessing and scaling'
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ODIN：使用预处理和缩放进行OOD检测
- en: '*O*ut-of-*DI*stribution detector for *N*eural networks (ODIN) is one of the
    standard methods in practical out-of-distribution detection because of its simplicity
    and effectiveness. Although the method was introduced in 2017, it is still frequently
    used as a comparison method in papers that propose out-of-distribution detection
    methods.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*O*ut-of-*DI*stribution 检测器（ODIN）是实际应用中常用的标准分布外检测方法之一，因为它简单有效。尽管该方法在 2017 年被提出，但它仍然经常作为提出分布外检测方法的论文中的对比方法。'
- en: 'ODIN consists of two key ideas:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ODIN 包含两个关键思想：
- en: '**Temperature scaling** of the logit scores before applying the softmax operation
    to improve the ability of the softmax score to distinguish between in- and out-of-distribution
    images'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 logit 分数进行**温度缩放**，然后再应用 softmax 操作，以提高 softmax 分数区分在分布内和分布外图像的能力。
- en: '**Input preprocessing** to make in-distribution images more in-distribution'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入预处理** 使分布内图像更符合分布内'
- en: Let’s look at both ideas in a bit more detail.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这两个思想。
- en: Temperature scaling ODIN works for classification models. Given our softmax
    score computed as
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 温度缩放 ODIN 适用于分类模型。给定我们计算的 softmax 分数如下：
- en: '![pi(x) = ∑--exp(fi(x))--- Nj=1 exp(fj(x)) ](img/file163.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![pi(x) = ∑--exp(fi(x))--- Nj=1 exp(fj(x)) ](img/file163.jpg)'
- en: 'Here, *f*[*i*](*x*) is a single logit output and *f*[*j*](*x*) are the logits
    for all classes for a single example, temperature scaling means that we divide
    these logit outputs by a constant *T*:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*f*[*i*](*x*) 是单个 logit 输出，*f*[*j*](*x*) 是单个示例中所有类别的 logits，温度缩放意味着我们将这些
    logit 输出除以常数 *T*：
- en: '![ exp(f (x)∕T) pi(x; T) = ∑N------i---------- j=1 exp (fj(x)∕T) ](img/file164.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![ exp(f (x)∕T) pi(x; T) = ∑N------i---------- j=1 exp (fj(x)∕T) ](img/file164.jpg)'
- en: For large values of *T*, temperature scaling causes the softmax scores to be
    closer to a uniform distribution, which helps to reduce overconfident predictions.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较大的 *T* 值，温度缩放使得 softmax 分数更接近均匀分布，从而有助于减少过于自信的预测。
- en: 'We can apply temperature scaling in Python, given a simple model that outputs
    the logits:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 Python 中应用温度缩放，假设有一个简单的模型输出 logits：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Input preprocessing We saw in [*Chapter 3*](CH3.xhtml#x1-350003), [*Fundamentals
    of Deep Learning*](CH3.xhtml#x1-350003) that the **Fast-Gradient** **Sign Method**
    (**FGSM**) allowed us to fool a neural network. By slightly changing an image
    of a cat, we could make the model predict ”dog” with 99.41% confidence. The idea
    here was that we could take the sign of the gradient of the loss with respect
    to the input, multiply it by a small value and add that noise to our image – this
    moved our image away from our in-distribution class. By doing the opposite, that
    is, subtracting the noise from our image, we make the image more in-distribution.
    The authors of the ODIN paper show that this causes in-distribution images to
    have an even higher softmax score compared to out-of-distribution images. This
    means that we increase the difference between OOD and ID softmax scores, leading
    to a better OOD detection performance.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 输入预处理 我们在 [*第 3 章*](CH3.xhtml#x1-350003)，[*深度学习基础*](CH3.xhtml#x1-350003) 中看到，**快速梯度**
    **符号方法**（**FGSM**）使我们能够欺骗神经网络。通过稍微改变一张猫的图像，我们可以让模型以 99.41% 的置信度预测为“狗”。这里的想法是，我们可以获取损失相对于输入的梯度符号，将其乘以一个小值，并将该噪声添加到图像中——这将把我们的图像从分布内类别中移动。通过做相反的事情，即从图像中减去噪声，我们使得图像更接近分布内。ODIN
    论文的作者表明，这导致分布内图像的 softmax 分数比分布外图像更高。这意味着我们增加了 OOD 和 ID softmax 分数之间的差异，从而提高了
    OOD 检测性能。
- en: '![˜x = x − 𝜀sign(− ∇x log Sˆy(x;T)) ](img/file165.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![˜x = x − 𝜀sign(− ∇x log Sˆy(x;T)) ](img/file165.jpg)'
- en: Where *x* is an input image of which we subtract the perturbation magnitude
    *𝜖* times the sign of the gradient of the cross-entropy loss with respect to the
    input. See [*Chapter 3*](CH3.xhtml#x1-350003), [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003)
    for the TensorFlow implementation of this technique.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x* 是输入图像，我们从中减去扰动幅度 *𝜖* 乘以交叉熵损失相对于输入的梯度符号。有关该技术的 TensorFlow 实现，请参见 [*第 3
    章*](CH3.xhtml#x1-350003)，[*深度学习基础*](CH3.xhtml#x1-350003)。
- en: 'Although input preprocessing and temperature scaling are simple to implement,
    ODIN now requires two more hyperparameters to be tuned: the temperature for scaling
    the logits and *𝜖* of the inverse of the fast gradient sign method. ODIN uses
    a separate out-of-distribution dataset to tune these hyperparameters (the validation
    set of the iSUN dataset: 8925 images).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管输入预处理和温度缩放易于实现，ODIN 现在还需要调节两个超参数：用于缩放 logits 的温度和 *𝜖*（快速梯度符号法的逆）。ODIN 使用一个单独的分布外数据集来调节这些超参数（iSUN
    数据集的验证集：8925 张图像）。
- en: 'Mahalanobis: OOD Detection with intermediate features'
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 马氏距离：使用中间特征进行 OOD 检测
- en: 'In *A Simple Unified Framework for Detecting Out-of-Distribution Samples and*
    *Adversarial Attacks*, Kimin Lee et al. propose a different method to detect OOD
    input. The core of their method is the idea that each class of a classifier follows
    a multivariate Gaussian distribution in the feature space of a network. Given
    this idea, we can define *C* class-conditional Gaussian distributions with a tied
    covariance *σ*:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在《*一种简单统一的框架用于检测分布外样本和* *对抗攻击*》一文中，Kimin Lee 等人提出了一种检测 OOD 输入的不同方法。该方法的核心思想是每个类别的分类器在网络的特征空间中遵循多元高斯分布。基于这一思想，我们可以定义*C*个类别条件高斯分布，并且具有共享的协方差
    *σ*：
- en: '![P(f(x) | y = c) = 𝒩 (f(x) | μc,σ ) ](img/file166.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![P(f(x) | y = c) = 𝒩 (f(x) | μc,σ ) ](img/file166.jpg)'
- en: 'Where *μ*[*c*] is the mean of the multivariate Gaussian distribution for each
    class *c*. This allows us to compute the empirical mean and covariance of each
    of these distributions for a given output of an intermediate layer, one for each
    class of our network. Based on the mean and covariance, we can compute the Mahalanobis
    distance of a single test image compared to our in-distribution data. We compute
    this for the class that is closest to the input image:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *μ*[*c*] 是每个类别 *c* 的多元高斯分布的均值。这使得我们能够计算给定中间层输出的每个类别的经验均值和协方差。基于均值和协方差，我们可以计算单个测试图像与分布内数据的马氏距离。我们对与输入图像最接近的类别计算该距离：
- en: '![M (x) = max − (f(x)− ^μc)⊤ ^σ− 1(f(x)− ^μc) c ](img/file167.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![M (x) = max − (f(x)− ^μc)⊤ ^σ− 1(f(x)− ^μc) c ](img/file167.jpg)'
- en: This distance should be small for in-distribution images and large for out-of-distribution
    images.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分布内的图像，这个距离应该较小，而对于分布外的图像，这个距离应该较大。
- en: '`numpy` has convenient functions to compute the mean and covariance of an array:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`numpy` 提供了方便的函数来计算数组的均值和协方差：'
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Given these, we can compute the Mahalanobis distance as such:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些，我们可以按如下方式计算马氏距离：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The Mahalanobis distance computation does not require any retraining and is
    a relatively cheap operation to perform once you have stored the mean and (inverse
    of the) covariance of the classes for the features of a layer of your network.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 马氏距离计算不需要任何重新训练，一旦你存储了网络某一层特征的均值和（协方差的逆矩阵），这是一项相对廉价的操作。
- en: To improve the performance of the method, the authors show that we can also
    apply the input preprocessing as mentioned in the ODIN paper, or compute and then
    average the Mahalanobis distances extracted from multiple layers of the network.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高方法的性能，作者表明我们还可以应用 ODIN 论文中提到的输入预处理，或者计算并平均从网络多个层提取的马氏距离。
- en: 8.3 Being robust against dataset shift
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 抵抗数据集偏移
- en: We already encountered dataset shift in [*Chapter 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep* *Learning*](CH3.xhtml#x1-350003). As a reminder, dataset
    shift is a common problem in machine learning that happens when the joint distribution
    *P*(*X,Y* ) of inputs *X* and outputs *Y* differs between the model training stage
    and model inference stage (for example, when testing the model or when running
    it in a production environment). Covariate shift is a specific case of dataset
    shift where only the distribution of the inputs changes but the conditional distribution
    *P*(*Y* |*X*) stays constant.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第 3 章*](CH3.xhtml#x1-350003)《*深度学习基础*》中已经遇到过数据集偏移。提醒一下，数据集偏移是机器学习中的一个常见问题，发生在模型训练阶段和模型推理阶段（例如，在测试模型或在生产环境中运行时）输入
    *X* 和输出 *Y* 的联合分布 *P*(*X,Y*) 不同的情况下。协变量偏移是数据集偏移的一个特定案例，其中只有输入的分布发生变化，而条件分布 *P*(*Y*
    |*X*) 保持不变。
- en: Dataset shift is present in most production environments because of the difficulty
    of including all possible inference conditions during training and because most
    data is not static but changes over time. The input data can shift along many
    different dimensions in a production environment. Geographic and temporal dataset
    shift are two common forms of shift. Imagine, for example, you have trained your
    model on data taken from one geographical region (for example, Europe) and then
    apply the model in a different geographical region (for example, Latin America).
    Similarly, a model could be trained on data from the years between 2010 and 2020
    and then applied on production data taken from today.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集偏移在大多数生产环境中普遍存在，因为在训练过程中很难包含所有可能的推理条件，而且大多数数据不是静态的，而是随着时间发生变化。在生产环境中，输入数据可能沿着许多不同的维度发生偏移。地理和时间数据集偏移是两种常见的偏移形式。例如，假设你已在一个地理区域（例如欧洲）获得的数据上训练了模型，然后将模型应用于另一个地理区域（例如拉丁美洲）。类似地，模型可能是在2010到2020年间的数据上训练的，然后应用于今天的生产数据。
- en: We will see that in such data shift scenarios, models often perform worse on
    the new shifted data than on their original training distribution. We will also
    see how vanilla neural networks usually do not indicate when the input data deviates
    from the training distribution. Finally, we will explore how various methods introduced
    in this book can be used to indicate dataset shift via uncertainty estimates and
    how these methods can make the models more robust. The following code example
    will be focused on an image classification problem. It should be noted, however,
    that the insights tend to generalize to other domains (such as natural language
    processing) and tasks (such as regression).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到，在这样的数据偏移场景中，模型在新的偏移数据上的表现通常比在原始训练分布上的表现差。我们还将看到，普通神经网络通常无法指示输入数据何时偏离训练分布。最后，我们将探讨本书中介绍的各种方法如何通过不确定性估计来指示数据集偏移，以及这些方法如何增强模型的鲁棒性。以下代码示例将集中在图像分类问题上。然而，值得注意的是，这些见解通常可以推广到其他领域（如自然语言处理）和任务（如回归）。
- en: 8.3.1 Measuring a model’s response to dataset shift
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 测量模型对数据集偏移的响应
- en: 'Assuming that we have a training dataset and a separate test set, how can we
    measure a model’s ability to signal to us when the data has shifted? In order
    to do so, it would be necessary to have an additional test set where the data
    has been shifted to check how the model reacts to the dataset shift. One commonly
    applied way to create such a data shift test set for images was originally suggested
    by Dan Hendrycks and Thomas Dietterich in 2019 and others. The idea is straightforward:
    take the images from your initial test set, then apply different image quality
    corruptions at different severity levels to them. Hendrycks and Dietterich proposed
    a set of 15 different types of image quality corruptions, ranging from image noise,
    blur, weather corruptions (such as fog and snow), and digital corruption. Each
    corruption type has five levels of severity, ranging from 1 (mild corruption)
    to 5 (severe corruption). *Figure* [*8.4*](#x1-143002r4) shows what the image
    of a kitty looks like initially (left) and after applying shot noise corruption
    to the image, either at severity level 1 (middle) or severity level 5 (right).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个训练数据集和一个单独的测试集，我们如何衡量模型在数据发生偏移时是否能及时反应？为了做到这一点，我们需要一个额外的测试集，其中数据已经发生偏移，以检查模型如何响应数据集偏移。一个常用的创建数据偏移测试集的方法最初由Dan
    Hendrycks、Thomas Dietterich及其他人于2019年提出。这个方法很简单：从你的初始测试集中取出图像，然后对其应用不同程度的图像质量损坏。Hendrycks和Dietterich提出了一套包含15种不同类型图像质量损坏的方法，涵盖了图像噪声、模糊、天气损坏（如雾霾和雪）以及数字损坏等类型。每种损坏类型都有五个严重程度级别，从1（轻度损坏）到5（严重损坏）。*图*
    [*8.4*](#x1-143002r4)展示了一只小猫的图像最初样子（左侧）以及在图像上施加噪声损坏后的效果，分别是严重程度为1（中间）和5（右侧）的情况。
- en: '![PIC](img/file168.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file168.png)'
- en: 'Figure 8.4: Generating artificial dataset shift by applying image quality corruptions
    at different levels of corruption severity'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：通过在不同损坏严重程度下应用图像质量损坏来生成人工数据集偏移
- en: All these image quality corruptions can be generated conveniently using the
    `imgaug` Python package. The following code assumes that we have an image called
    ”kitty.png” on disk. We load the image using the PIL package. We then specify
    the corruption type (for example, `ShotNoise`) via the name of the corruption
    function, and then apply the corruption function to the image, using either severity
    level 1 or 5 by passing the corresponding integer to the key-worded `severity`
    argument.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些图像质量损坏可以方便地使用`imgaug` Python包生成。以下代码假设我们磁盘上有一个名为"kitty.png"的图像。我们使用PIL包加载图像。然后，我们通过损坏函数的名称指定损坏类型（例如，`ShotNoise`），并使用通过传递相应整数给关键字参数`severity`来应用损坏函数，选择严重性等级1或5。
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The advantage of generating data shift this way is that it can be applied to
    a wide range of computer vision problems and datasets. Some of the few prerequisites
    for applying this method are that the data consists of images and that these image
    quality corruptions have not been used during training, for example, for data
    augmentation. Furthermore, by setting the severity of the image quality corruption,
    we gain control over the degree of the dataset shift. This allows us to measure
    how the model reacts to different degrees of dataset shift. We can measure both
    how performance changes in response to dataset shift and how calibration (introduced
    in [*Chapter 2*](CH2.xhtml#x1-250002), [*Fundamentals of Bayesian Inference*](CH2.xhtml#x1-250002))
    changes. We would expect that models trained with Bayesian methods or extensions
    should be better calibrated, which means that they are able to indicate to us
    that the data has shifted in comparison to training and they are thus less certain
    in their output.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式生成数据偏移的优势在于，它可以应用于广泛的计算机视觉问题和数据集。应用这种方法的少数前提条件是数据由图像组成，并且在训练过程中没有使用过这些图像质量损坏（例如，用于数据增强）。此外，通过设置图像质量损坏的严重性，我们可以控制数据集偏移的程度。这使我们能够衡量模型对不同程度的数据集偏移的反应。我们可以衡量性能如何随着数据集偏移而变化，以及校准（在[*第二章*](CH2.xhtml#x1-250002)，[*贝叶斯推断基础*](CH2.xhtml#x1-250002)中引入）如何变化。我们预计使用贝叶斯方法或扩展方法训练的模型会有更好的校准，这意味着它们能够告诉我们数据相较于训练时已经发生了偏移，因此它们对输出的信心较低。
- en: 8.3.2 Revealing dataset shift with Bayesian methods
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 使用贝叶斯方法揭示数据集偏移
- en: In the following code example, we will look at two of the BDL methods (Bayes
    by backprop and deep ensembles) that we have encountered in the book so far and
    see how they perform during the kind of artificial dataset shift described previously.
    We will compare their performance against a vanilla neural network.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下的代码示例中，我们将查看书中到目前为止遇到的两种BDL方法（基于反向传播的贝叶斯方法和深度集成），并观察它们在前面描述的人工数据集偏移下的表现。我们将它们的表现与普通的神经网络进行比较。
- en: 'Step 1: Preparing the environment'
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 1：准备环境
- en: We start the example by importing a list of packages. This includes TensorFlow
    and TensorFlow Probability, which we will use for building and training the neural
    networks; `numpy` for manipulating numerical arrays (such as calculating the mean);
    `Seaborn`, `Matplotlib`, and `pandas` for plotting; `cv2` and `imgaug` for loading
    and manipulating images; as well as `scikit-learn` for calculating the accuracy
    of our models.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过导入一系列包来开始这个示例。这些包包括用于构建和训练神经网络的TensorFlow和TensorFlow Probability；用于处理数值数组（如计算均值）的`numpy`；用于绘图的`Seaborn`、`Matplotlib`和`pandas`；用于加载和处理图像的`cv2`和`imgaug`；以及用于计算模型准确度的`scikit-learn`。
- en: '[PRE15]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In preparation for the training, we will load the `CIFAR10` dataset, which is
    an image classification dataset, and specify the names of the different classes.
    The dataset consists of 10 different classes, the names of which we specify in
    the following code, and provides 50,000 training images as well as 10,000 test
    images. We’ll also save the number of training images, which will be needed to
    train the model with the reparameterization trick later.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，我们将加载`CIFAR10`数据集，这是一个图像分类数据集，并指定不同类别的名称。该数据集包含10个不同的类别，我们将在以下代码中指定这些类别的名称，并提供50,000个训练图像和10,000个测试图像。我们还将保存训练图像的数量，这将在稍后使用重参数化技巧训练模型时用到。
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Step 2: Defining and training the models'
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 2：定义和训练模型
- en: After this prep work, we can define and train our models. We start by creating
    two functions to define and build the CNN. We will use these functions both for
    the vanilla neural network and the deep ensemble. The first function simply combines
    a convolutional layer with a max-pooling layer – a common approach that we introduced
    in [*Chapter 3*](CH3.xhtml#x1-350003), [*Fundamentals of Deep* *Learning*](CH3.xhtml#x1-350003).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项准备工作完成后，我们可以定义并训练我们的模型。我们首先创建两个函数来定义和构建CNN。我们将使用这两个函数来构建普通神经网络和深度集成网络。第一个函数简单地将卷积层与最大池化层结合起来——这是一种常见的做法，我们在[*第3章*](CH3.xhtml#x1-350003)《深度学习基础》中介绍过。
- en: '[PRE17]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The second function then uses several convolutional/max-pooling blocks in sequence
    and follows this sequence with a final dense layer:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数则依次使用多个卷积/最大池化块，并在此序列后面跟着一个最终的密集层：
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We also create two analogous functions to define and build the network using
    Bayes By Backprop (BBB) based on the reparameterization trick. The strategy is
    the same as for the vanilla neural network, just that we’ll now use the convolutional
    and dense layers from the TensorFlow Probability package instead of the TensorFlow
    package. The convolutional/max-pooling blocks are then defined as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了两个类似的函数，用于基于重新参数化技巧定义和构建使用Bayes By Backprop（BBB）的网络。策略与普通神经网络相同，只不过我们现在将使用来自TensorFlow
    Probability包的卷积层和密集层，而不是TensorFlow包。卷积/最大池化块定义如下：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'And the final network is defined like this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的网络定义如下：
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can then train the vanilla neural network:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以训练普通神经网络：
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can also train the ensemble, with five ensemble members:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以训练一个五成员的集成模型：
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: And finally, we train the BBB model. Note that we train the BBB model for 15
    instead of 10 epochs, given that it takes a little longer to converge.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们训练BBB模型。注意，我们将训练BBB模型15个epoch，而不是10个epoch，因为它收敛的时间稍长。
- en: '[PRE23]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Step 3: Obtaining predictions'
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤3：获取预测结果
- en: 'Now that we have three trained models, we can use them for predictions on the
    hold-out test set. To keep computations at a manageable degree, in this example,
    we will focus on the first 1,000 images in the test set:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了三个训练好的模型，可以使用它们对保留的测试集进行预测。为了保持计算的可控性，在这个例子中，我们将专注于测试集中的前1000张图像：
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If we want to measure the response to dataset shift, we first need to apply
    the artificial image corruptions to the dataset. To do that, we first specify
    a set of functions from the `imgaug` package. From their names, one can infer
    what type of corruption each of these functions implements: for example, the function
    `icl.GaussianNoise` corrupts an image by applying Gaussian noise to it. We also
    infer the number of corruption types from the number of functions and save it
    in the `NUM_TYPES` variable. Finally, we set the number of corruption levels to
    5.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要衡量数据集偏移的响应，首先需要对数据集应用人工图像损坏。为此，我们首先指定一组来自`imgaug`包的函数。从这些函数的名称中，我们可以推断出每个函数实现的损坏类型：例如，函数`icl.GaussianNoise`通过向图像应用高斯噪声来损坏图像。我们还通过函数的数量推断出损坏类型的数量，并将其保存在`NUM_TYPES`变量中。最后，我们将损坏级别设置为5。
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Equipped with these functions, let us now corrupt images. In the next code block,
    we loop over the different corruption levels and types. We collect all corrupted
    images in the aptly named `corrupted_images` variable.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了这些函数后，我们现在可以开始损坏图像了。在下一个代码块中，我们遍历不同的损坏级别和类型，并将所有损坏的图像收集到名为`corrupted_images`的变量中。
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'With the three models trained and the corrupted images at hand, we can now
    see how our models react to dataset shift of different levels. We will first obtain
    predictions on the corrupted images from the three models. To run inference, we
    need to reshape the corrupted images to an input shape that is accepted by the
    models for inferences. At the moment, the images are still stored on different
    axes for the corruption types and levels. We change this by reshaping the `corrupted_images`
    array:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练完三个模型并获得损坏图像后，我们现在可以看到模型对不同级别数据集偏移的反应。我们将首先获取三个模型对损坏图像的预测结果。为了进行推理，我们需要将损坏的图像调整为模型接受的输入形状。目前，这些图像仍然存储在针对损坏类型和级别的不同轴上。我们通过重新调整`corrupted_images`数组来改变这一点：
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then we can perform inference with the vanilla CNN model, both on the original
    images and the corrupted images. After we have inferred the model predictions,
    we reshape the predictions in order to separate predictions for the corruption
    types and levels:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用普通CNN模型对原始图像和腐蚀图像进行推理。在推理模型预测后，我们将预测结果重塑，以便分离腐蚀类型和级别的预测：
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To run inference with the ensemble model, we first define a prediction function
    to avoid code duplication. This function handles the looping over the different
    member models of the ensemble and combines the different predictions in the end
    via averaging:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用集成模型进行推理，我们首先定义一个预测函数以避免代码重复。此函数处理对集成中不同成员模型的循环，并最终通过平均将不同的预测结果结合起来：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Equipped with this function, we can perform inference with the ensemble model
    on both the original and corrupted images:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了这个函数后，我们可以对原始图像和腐蚀图像使用集成模型进行推理：
- en: '[PRE30]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Just as for the ensemble model, we write an inference function for the BBB
    model, which handles the iteration over different sampling loops and collects
    and combines the results:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 就像对于集成模型一样，我们为BBB模型编写了一个推理函数，该函数处理不同采样循环的迭代，并收集并结合结果：
- en: '[PRE31]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We then put this function to use to obtain the BBB model predictions on the
    original and corrupted images. We sample from the BBB model 20 times:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们利用这个函数获取BBB模型在原始图像和腐蚀图像上的预测。我们从BBB模型中采样20次：
- en: '[PRE32]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can convert the predictions of the three models to predicted classes and
    associated confidence scores by returning the index of the class with the maximum
    softmax score and the maximum softmax score, respectively:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过返回具有最大softmax得分的类别索引和最大softmax得分，分别将三个模型的预测转换为预测类别及其相关的置信度得分：
- en: '[PRE33]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This function can then be applied to get the predicted classes and confidence
    scores for our three models:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以应用此函数来获取我们三个模型的预测类别和置信度得分：
- en: '[PRE34]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let us visualize what these predicted classes and confidence scores look like
    for the three models on a selected image showing an automobile. For plotting,
    we first reshape the array that contains the corrupted images to a more convenient
    format:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化这三个模型在一张展示汽车的选定图像上预测的类别和置信度得分。为了绘图，我们首先将包含腐蚀图像的数组重塑为更方便的格式：
- en: '[PRE35]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We then plot the selected automobile image with the first three corruption types
    in the list across all five corruption levels. For each combination, we display
    in the image title the predicted score of each model and in squared parentheses
    the predicted class. The plot is shown in *Figure *[*8.5*](#x1-147361r5).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们绘制了列表中前三种腐蚀类型的选定汽车图像，涵盖所有五个腐蚀级别。对于每种组合，我们在图像标题中显示每个模型的预测得分，并在方括号中显示预测类别。该图如*图*[*8.5*](#x1-147361r5)所示。
- en: '![PIC](img/file169.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file169.png)'
- en: 'Figure 8.5: An automobile image has been corrupted with different corruption
    types (rows) and levels (columns, severity increases from left to right)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：一张汽车图像已经被不同的腐蚀类型（行）和级别（列，严重程度从左到右增加）腐蚀
- en: 'The code continues:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 代码继续：
- en: '[PRE36]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '*Figure *[*8.5*](#x1-147361r5) only shows results for a single image, so we
    should not read too much into these results. However, we can already observe that
    the prediction scores for the two Bayesian methods (and especially the ensemble
    method) tend to be less extreme than for the vanilla neural network, which has
    predicted scores as high as 0.95\. Furthermore, we see that, for all three models,
    prediction scores usually decrease as the corruption level increases. This is
    expected: given that the car in the image becomes less discernible with more corruption,
    we would want the model to become less confident as well. In particular, the ensemble
    method shows a nice and consistent decrease in predicted scores with increased
    corruption levels.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*图*[*8.5*](#x1-147361r5)只显示了单张图像的结果，因此我们不应过度解读这些结果。然而，我们已经可以观察到，两个贝叶斯方法（尤其是集成方法）的预测得分通常比普通神经网络更不极端，后者的预测得分高达0.95。此外，我们看到，对于所有三个模型，预测得分通常随着腐蚀级别的增加而降低。这是预期的：由于图像中的汽车在腐蚀越严重时变得越难以辨认，我们希望模型的置信度也会随之降低。特别是，集成方法在增加腐蚀级别时显示出了预测得分的明显且一致的下降。'
- en: 'Step 4: Measuring accuracy'
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第4步：衡量准确性
- en: Are some models more robust to dataset shift than other models? We can answer
    this question by looking at the accuracy of the three models at different corruptions
    levels. It is expected that all models will show lower accuracy as the input image
    becomes more and more corrupted. However, more robust models should lose less
    in accuracy as the corruptions become more severe.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 有些模型比其他模型更能适应数据集的偏移吗？我们可以通过查看三种模型在不同损坏水平下的准确性来回答这个问题。预计所有模型在输入图像逐渐损坏时准确性会降低。然而，更鲁棒的模型在损坏变得更严重时，准确性下降应该较少。
- en: 'First, we can calculate the accuracy of the three models on the original test
    images:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以计算三种模型在原始测试图像上的准确性：
- en: '[PRE37]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We can store these accuracies in a list of dictionaries, which will make it
    easier to plot them systematically. We pass the respective name of the models.
    For corruption `type` and `level`, we pass `0` because these are the accuracies
    on the original images.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些准确性存储在字典列表中，这将使我们更容易系统地绘制它们。我们传递相应的模型名称。对于损坏的`类型`和`级别`，我们传递`0`，因为这些是原始图像上的准确性。
- en: '[PRE38]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Next, we calculate the accuracy of the three models on the different corruption
    type by corruption level combinations. We also append the results to the list
    of accuracies that we started previously:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算三种模型在不同损坏类型和损坏级别组合下的准确性。我们还将结果附加到之前开始的准确性列表中：
- en: '[PRE39]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We can then plot the distributions of accuracies for the original images and
    the increasingly corrupted images. We first convert the list of dictionaries to
    a pandas dataframe. This has the advantage that the dataframe can be directly
    passed to the plotting package `seaborn`. This allows us to specify that we want
    to plot the different models’ results in different hues.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以绘制原始图像和逐渐损坏图像的准确性分布。我们首先将字典列表转换为pandas dataframe。这有一个优势，即dataframe可以直接传递给绘图库`seaborn`，这样我们可以指定不同模型的结果以不同色调进行绘制。
- en: '[PRE40]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This produces the following output:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下输出：
- en: '![PIC](img/file170.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file170.png)'
- en: 'Figure 8.6: Accuracy for the three different models (different hues) for the
    original test images (level 0) as well as for increasing levels of corruption
    (level 1-5)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6：三种不同模型（不同色调）在原始测试图像（级别 0）以及不同程度的损坏（级别 1-5）上的准确性
- en: 'The resulting plot is shown in *Figure *[*8.6*](#x1-148194r6). We can see that,
    on the original test images, the vanilla and BBB model have comparable accuracy,
    while the ensemble model has slightly higher accuracy. As corruption is introduced,
    we see that the performance of the vanilla neural network is worse (often significantly)
    than the performance of the ensemble or BBB. This relative improvement in performance
    of the BDL models demonstrates the regularization effect of Bayesian methods:
    these methods are able to capture the distribution of the data more effectively,
    making them more robust to perturbations. BBB exhibits particular resilience to
    increasing amounts of data corruption, demonstrating a key benefit of variational
    learning.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如*图*[*8.6*](#x1-148194r6)所示。我们可以看到，在原始测试图像上，普通模型和BBB模型的准确性相当，而集成模型的准确性稍高。随着损坏的引入，我们看到普通神经网络的表现比集成模型或BBB模型更差（通常是显著差）。BDL模型性能的相对提升展示了贝叶斯方法的正则化效应：这些方法能更有效地捕捉数据的分布，使其对扰动更加鲁棒。BBB模型特别能抵御数据损坏的增加，展示了变分学习的一个关键优势。
- en: 'Step 5: Measuring calibration'
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 5：衡量校准
- en: Looking at accuracy is a good way to determine how robust a model is against
    dataset shift. But it does not really tell us whether the models are capable of
    signalling to us (via lower confidence scores) when the dataset has shifted and
    the models have become less confident in their output. This question can be answered
    by looking at how well models remain calibrated under dataset shift. We introduced
    calibration and expected calibration errors on a conceptual level back in [*Chapter 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003). We are now going to put
    these concepts into practice to understand whether models adjust their confidence
    appropriately as the images become increasingly corrupted and hard to predict.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 查看准确度是确定模型在数据集变化下的鲁棒性的一种好方法。但它并没有真正告诉我们模型是否能够通过较低的置信度分数（当数据集发生变化时）提醒我们，并且模型在输出时变得不那么自信。这个问题可以通过观察模型在数据集变化下的校准表现来回答。我们在[*第3章*](CH3.xhtml#x1-350003)的《深度学习基础》中已经介绍了校准和期望校准误差的概念。现在，我们将把这些概念付诸实践，以理解当图像变得越来越受损且难以预测时，模型是否适当地调整了它们的置信度。
- en: 'First, we will implement the Expected Calibration Error (ECE) introduced in
    [*Chapter 3*](CH3.xhtml#x1-350003), [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003)
    as a scalar measure of calibration:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将实现[*第3章*](CH3.xhtml#x1-350003)《深度学习基础》中介绍的期望校准误差（ECE），作为校准的标量衡量标准：
- en: '[PRE41]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can then calculate ECE for the three models on the original test images.
    We set the number of bins to `10`, which is a common choice for calculating ECE:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以计算三个模型在原始测试图像上的期望校准误差（ECE）。我们将箱子的数量设置为`10`，这是计算ECE时常用的选择：
- en: '[PRE42]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Just as we did for the accuracies earlier, we will store the calibration results
    in a list of dictionaries, which will make it easier to plot them:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前处理准确度一样，我们将把校准结果存储在一个字典列表中，这样就更容易绘制它们：
- en: '[PRE43]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we calculate the expected calibration error of the three models on the
    different corruption types by corruption level combinations. We also append the
    results to the list of calibration results that we started previously:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们根据不同的腐蚀类型和腐蚀级别组合，计算三个模型的期望校准误差。我们还将结果附加到之前开始的校准结果列表中：
- en: '[PRE44]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Finally, we plot the calibration results in a boxplot, again using `pandas`
    and `seaborn`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用`pandas`和`seaborn`再次绘制校准结果的箱形图：
- en: '[PRE45]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The calibration results are shown in *Figure *[*8.7*](#x1-149394r7). We can
    see that, on the original test images, all three models have relatively low calibration
    error, with the ensemble model performing slightly worse than the two other models.
    As we apply increasing levels of dataset shift, we can see that calibration error
    increases by a lot for the vanilla model. For the two Bayesian methods, calibration
    error also increases but by much less than for the vanilla model. This means that
    the Bayesian methods are better at indicating (via lower confidence scores) when
    the dataset has shifted and that the Bayesian models become relatively less confident
    in their output with increased corruption (as they should).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 校准结果显示在*图*[*8.7*](#x1-149394r7)中。我们可以看到，在原始测试图像上，所有三个模型的校准误差都比较低，集成模型的表现略逊色于另外两个模型。随着数据集变化程度的增加，我们可以看到，传统模型的校准误差大幅增加。对于两种贝叶斯方法，校准误差也增加了，但比传统模型要少得多。这意味着贝叶斯方法在数据集发生变化时能够更好地通过较低的置信度分数来指示（即模型在输出时变得相对不那么自信，随着腐蚀程度的增加，表现出这种特征）。
- en: '![PIC](img/file171.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file171.png)'
- en: 'Figure 8.7: Expected calibration error for the three different models for the
    original test images (level 0) as well as for increasing levels of corruption
    (level 1-5)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7：三种不同模型在原始测试图像（级别0）和不同腐蚀级别（级别1-5）上的期望校准误差
- en: In the next section, we will look into data selection.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论数据选择。
- en: 8.4 Using data selection via uncertainty to keep models fresh
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 使用通过不确定性进行的数据选择来保持模型的更新
- en: 'We saw at the beginning of the chapter that we can use uncertainties to figure
    out whether data is part of the training data or not. We can expand on this idea
    in the context of an area of machine learning called **active learning**. The
    promise of active learning is that a model can learn more effectively on less
    data if we have a way to control the type of data it is trained on. Conceptually,
    this makes sense: if we train a model on data that is not of sufficient quality,
    it will also not perform well. Active learning is a way to guide the learning
    process and data a model is trained on by providing functions that can acquire
    data from a pool of data that is not part of the training data. By iteratively
    selecting the right data from the pool, we can train a model that performs better
    than if we had chosen the data from the pool at random.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章开头看到，能够使用不确定性来判断数据是否是训练数据的一部分。在主动学习这一机器学习领域的背景下，我们可以进一步扩展这个想法。主动学习的承诺是，如果我们能够控制模型训练的数据类型，模型可以在更少的数据上更有效地学习。从概念上讲，这是有道理的：如果我们在质量不足的数据上训练模型，它的表现也不会很好。主动学习是一种通过提供可以从不属于训练数据的数据池中获取数据的函数，来引导模型学习过程和训练数据的方法。通过反复从数据池中选择正确的数据，我们可以训练出比随机选择数据时表现更好的模型。
- en: 'Active learning can be used in many modern-day systems where there is a ton
    of unlabeled data available and we need to carefully select the amount of data
    we want to label. An example is an autonomous driving system: the camera on the
    car records a lot of data, but there is typically no budget to label all of it.
    By carefully choosing the most informative data points, we can improve the model
    performance at a lower cost than when we would have randomly selected the data
    to label. In the context of active learning, estimating uncertainties plays an
    important role. A model will typically learn more from areas of the data distribution
    that were predicted with low confidence. Let’s look at a case study to see how
    we can use uncertainty in the context of active learning.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习可以应用于许多现代系统，在这些系统中有大量未标记的数据可供使用，我们需要仔细选择想要标记的数据量。一个例子是自动驾驶系统：车上的摄像头记录了大量数据，但通常没有预算标记所有数据。通过仔细选择最具信息量的数据点，我们可以以比随机选择数据标记时更低的成本提高模型性能。在主动学习的背景下，估计不确定性发挥着重要作用。模型通常会从数据分布中那些低置信度预测的区域学到更多。让我们通过一个案例研究来看看如何在主动学习的背景下使用不确定性。
- en: 'In this case study, we will reproduce the results from a fundamental active
    learning paper: *Deep Bayesian Active Learning with Image Data* (2017). We will
    use the `MNIST` dataset and train a model on more and more data, where we select
    the data points to add to our training set via an uncertainty method. In this
    case, we will use epistemic uncertainty to select the most informative data points.
    Images with high epistemic uncertainty should be images that the model did not
    see before; the uncertainty can be reduced by adding more of them. As a comparison,
    we will also select data points at random.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将重现一篇基础性主动学习论文的结果：*基于图像数据的深度贝叶斯主动学习*（2017）。我们将使用`MNIST`数据集，并在越来越多的数据上训练模型，通过不确定性方法选择要添加到训练集中的数据点。在这种情况下，我们将使用认知不确定性来选择最具信息量的数据点。具有高认知不确定性的图像应该是模型之前没有见过的图像；通过增加更多这样的图像，可以减少不确定性。作为对比，我们还将随机选择数据点。
- en: 'Step 1: Preparing our dataset'
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一步：准备数据集
- en: 'We will start by creating our functions to load the dataset. The dataset functions
    need the following library imports:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先创建加载数据集的函数。数据集函数需要以下库导入：
- en: '[PRE46]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: As our total dataset will have quite a few components, we will create a small
    dataclass to easily access all the different parts of our dataset. We will also
    modify the `__repr__` function of the dataclass. This allows us to print the content
    of the dataset in a more readable format.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的总数据集将包含相当多的组件，我们将创建一个小的`dataclass`，以便轻松访问数据集的不同部分。我们还将修改`__repr__`函数，使其能够以更易读的格式打印数据集内容。
- en: '[PRE47]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We can then define our function to load our standard dataset.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以定义函数来加载标准数据集。
- en: '[PRE48]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Initially, we will start training on just 20 samples from the *MNIST* dataset.
    We will then acquire 10 data points at a time, and retrain our model again. To
    help our model a little bit in the beginning, we will make sure that the 20 data
    points are balanced across the different classes of the dataset. The following
    function gives us the indices that we can use to create the initial 20 samples,
    2 samples of each class:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们将从*MNIST*数据集中仅使用20个样本进行训练。然后我们每次获取10个数据点，并重新训练我们的模型。为了在开始时帮助我们的模型，我们将确保这20个数据点在数据集的不同类别之间是平衡的。以下函数给出了我们可以使用的索引，用于创建初始的20个样本，每个类别2个样本：
- en: '[PRE49]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We can then define a small function to actually get our initial dataset:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以定义一个小函数，实际获取我们的初始数据集：
- en: '[PRE50]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Step 2: Setting up our configuration'
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤2：设置配置
- en: Before we start to build our model and create the active learning loop, we define
    a small configuration `dataclass` to store some main variables we might want to
    play around with when running our active learning script. Creating configuration
    classes such as these allows you to play around with different parameters.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建模型并创建主动学习循环之前，我们定义一个小的配置`dataclass`来存储一些在运行主动学习脚本时可能想要调整的主要变量。创建这样的配置类使你可以灵活调整不同的参数。
- en: '[PRE51]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Step 3: Defining the model'
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤3：定义模型
- en: We can now define our model. We will use a small, simple CNN with dropout.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义我们的模型。我们将使用一个简单的小型CNN并加入Dropout。
- en: '[PRE52]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Step 4: Defining the uncertainty functions'
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤4：定义不确定性函数
- en: As indicated, we will use epistemic uncertainty (also knowledge uncertainty)
    as our main uncertainty function to acquire new samples. Let’s define the function
    to compute epistemic uncertainty over our predictions. We assume that the input
    predictions (`divds`) are of shape `n_images`, `n_predictions`, `n_classes`. We
    first define a function to compute total uncertainty. Given an ensemble of model
    predictions, this can be defined as the entropy of the averaged predictions of
    the ensemble.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用认知不确定性（也称为知识不确定性）作为我们主要的不确定性函数来获取新样本。让我们定义一个函数来计算我们预测的认知不确定性。我们假设输入的预测（`divds`）的形状为`n_images`，`n_predictions`，`n_classes`。我们首先定义一个函数来计算总不确定性。给定一个集成模型的预测，它可以定义为集成平均预测的熵。
- en: '[PRE53]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We then define data uncertainty (or aleatoric uncertainty), which for an ensemble
    is the average of the entropy of each ensemble member.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义数据不确定性（或称为随机不确定性），对于一个集成模型来说，它是每个集成成员的熵的平均值。
- en: '[PRE54]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Finally, we have our knowledge (or epistemic) uncertainty, which is simply subtracting
    data uncertainty from the total uncertainty of the predictions.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们得到了我们的知识（或认知）不确定性，这就是通过从预测的总不确定性中减去数据不确定性来得到的。
- en: '[PRE55]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'With these uncertainty functions defined, we can define the actual acquisition
    functions that take as main input our training data and our model. To acquire
    samples via knowledge uncertainty, we do the following:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了这些不确定性函数后，我们可以定义实际的获取函数，它们的主要输入是我们的训练数据和模型。为了通过知识不确定性来获取样本，我们进行以下操作：
- en: Obtain our ensemble of predictions via MC dropout.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过MC Dropout获取我们的集成预测。
- en: Compute the knowledge uncertainty values over this ensemble.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算这个集成模型的知识不确定性值。
- en: Sort the uncertainty values, get their index and return the indices of our training
    data with the highest epistemic uncertainty.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对不确定性值进行排序，获取它们的索引，并返回我们训练数据中具有最高认知不确定性的索引。
- en: We can then, later on, reuse these indices to index into our training data and
    actually acquire the training samples we want to add.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，稍后我们可以重复使用这些索引来索引我们的训练数据，实际上获取我们想要添加的训练样本。
- en: '[PRE56]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We obtain our MC dropout predictions as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下方式获得MC Dropout预测：
- en: '[PRE57]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: To avoid running out of memory, we iterate over our training data in batches
    of six, where for every batch we compute our predictions `n_iter` times. To make
    sure that our predictions are varied, we set the model’s `training` parameter
    to `True`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免内存溢出，我们将训练数据分批处理，每批6个样本，对于每一批，我们将计算`n_iter`次预测。为了确保我们的预测具有多样性，我们将模型的`training`参数设置为`True`。
- en: 'For our comparison, we define an acquisition function that returns a random
    number of indices as well:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的比较，我们还定义了一个获取函数，该函数返回一个随机的索引列表：
- en: '[PRE58]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Finally, we define a small function according to the *factory method pattern*
    to make sure that we can use the same function in our loop to use either the random
    acquisition function or knowledge uncertainty. Small factory functions such as
    these help to keep your code modular when you want to run the same code with different
    configurations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们根据*工厂方法模式*定义一个小函数，以确保我们可以在循环中使用相同的函数，使用随机采集函数或知识不确定性。像这样的工厂小函数有助于在你想用不同配置运行相同代码时保持代码模块化。
- en: '[PRE59]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Now that we have defined our acquisition functions, we are ready to actually
    define the loop that runs our active learning iterations.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了采集函数，我们已经准备好实际定义运行我们主动学习迭代的循环。
- en: 'Step 5: Defining the loop'
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第5步：定义循环
- en: Let’s start by defining our configuration. In this case, we are using knowledge
    uncertainty as our uncertainty function. In a different loop, we will use a random
    acquisition function to compare the results of the loop we are about to define.
    We will start our dataset with 20 samples until we reach a total of 1,000 samples.
    Each model will be trained for 50 epochs and per iteration, we acquire 10 samples.
    To obtain our MC dropout predictions, we will run over our full training set (minus
    the already acquired samples) 100 times.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义我们的配置。在这种情况下，我们使用知识不确定性作为我们的不确定性函数。在另一个循环中，我们将使用一个随机采集函数来比较我们即将定义的循环结果。我们将从20个样本开始我们的数据集，直到我们达到1,000个样本。每个模型将训练50个epoch，每次迭代我们获取10个样本。为了获得我们的MC
    dropout预测，我们将在整个训练集（减去已获取的样本）上运行100次。
- en: '[PRE60]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We can then get our data and define an empty dictionary to keep track of the
    test accuracy per iteration. We also create an empty list to keep track of the
    full list of indices we added to our training data.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以获取数据，并定义一个空字典来跟踪每次迭代的测试准确率。我们还创建一个空列表，用于跟踪我们添加到训练数据中的所有索引。
- en: '[PRE61]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We also assign a **universally unique identifier** (**UUID**) to our run to
    make sure we can discover it easily and do not overwrite the outcomes we save
    as part of our loop. We create the directory where we will save our data and save
    our configuration in that directory to ensure that we always know with what kind
    of configuration the data in our `model_dir` was created.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还为我们的运行分配了一个**全球唯一标识符**（**UUID**），以确保我们可以轻松找到它，并且不会覆盖我们作为循环一部分保存的结果。我们创建一个目录来保存我们的数据，并将配置保存到该目录，以确保我们始终知道`model_dir`中的数据是使用何种配置创建的。
- en: '[PRE62]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'We can now actually run our active learning loop. We will break this loop into
    three sections:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以实际运行我们的主动学习循环。我们将把这个循环分成三个部分：
- en: 'We define the loop and fit a model on the acquired samples:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义循环，并在已获取的样本上拟合模型：
- en: '[PRE63]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We then load the model with the best validation accuracy and update our dataset
    based on the acquisition function:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们加载具有最佳验证准确率的模型，并根据采集函数更新我们的数据集：
- en: '[PRE64]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We finally save the added images, compute the test accuracy, and save the results:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们保存已添加的图片，计算测试准确率，并保存结果：
- en: '[PRE65]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'In this loop, we defined a few small helper functions. First of all, we defined
    a callback for our model to save the model with the highest validation accuracy
    to our model directory:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个循环中，我们定义了一些小的辅助函数。首先，我们为我们的模型定义了一个回调，以将具有最高验证准确率的模型保存到我们的模型目录：
- en: '[PRE66]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We also defined a function to compute the accuracy of our test set:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个函数来计算测试集的准确率：
- en: '[PRE67]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'And we defined two small functions to save the results per iteration:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了两个小函数，用于每次迭代保存结果：
- en: '[PRE68]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Note that running the active learning loop takes quite a long time: for every
    iteration, we train and evaluate our model for 50 epochs, and then run through
    our pool set (the full training dataset minus the acquired samples) 100 times.
    When using a random acquisition function, we avoid the last step but still run
    our validation data through our model 50 times per iteration, just to make sure
    that we use the model with the best validation accuracy. This takes time, but
    picking the model with just the best *training* accuracy would be risky: our model
    sees the same few images many times during training and is therefore likely to
    overfit to the training data.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，运行主动学习循环需要相当长的时间：每次迭代，我们训练并评估模型50个epoch，然后在我们的池集（完整的训练数据集减去已获取的样本）上运行100次。使用随机采集函数时，我们跳过最后一步，但仍然每次迭代将验证数据运行50次，以确保使用具有最佳验证准确率的模型。这需要时间，但仅仅选择具有最佳*训练*准确率的模型是有风险的：我们的模型在训练过程中多次看到相同的几张图片，因此很可能会过拟合训练数据。
- en: 'Step 6: Inspecting the results'
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第6步：检查结果
- en: 'Now that we have our loop, we can inspect the results of this process. We will
    use `seaborn` and `matplotlib` to visualize our results:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了循环，可以检查这个过程的结果。我们将使用`seaborn`和`matplotlib`来可视化我们的结果：
- en: '[PRE69]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The main result we are interested in is the test accuracy over time for both
    the models trained with a random acquisition function and the models trained with
    data acquired via knowledge uncertainty. To visualize this, we define a function
    that loads the results and then returns a plot that shows the accuracy per active
    learning iteration cycle:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最感兴趣的主要结果是两种模型的测试准确率随时间的变化，这些模型分别是基于随机获取函数训练的模型和通过知识不确定性获取数据训练的模型。为了可视化这个结果，我们定义一个函数，加载结果并返回一个图表，显示每个主动学习迭代周期的准确率：
- en: '[PRE70]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We can then use this function to plot the results for both acquisition functions:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用这个函数绘制两个获取函数的结果：
- en: '[PRE71]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'This produces the following output:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![PIC](img/file172.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file172.png)'
- en: 'Figure 8.8: Active learning results'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8：主动学习结果
- en: '*Figure* [*8.8*](#x1-156074r8) shows that acquiring samples via knowledge uncertainty
    starts to improve the model’s accuracy significantly after around 300 acquired
    samples. The final accuracy of this model is about two percentage points higher
    than the accuracy of the model trained on random samples. This might not look
    like a lot, but we can also look at the data in another way: how many samples
    were needed to achieve a particular accuracy? If we inspect the plot, we can see
    that the knowledge uncertainty line achieves an accuracy of 96% with 400 training
    samples. The model trained on random samples required at least 750 samples to
    achieve the same accuracy. That’s almost double the amount of data for the same
    accuracy. This shows that active learning with the right acquisition function
    can be very useful, specifically in cases where compute resources are available,
    but labeling is expensive: with the right samples, we might be able to decrease
    our labeling cost by a factor of two to achieve the same accuracy.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '*图* [*8.8*](#x1-156074r8) 显示，通过知识不确定性获取样本开始显著提高模型的准确性，尤其是在大约获取了300个样本之后。该模型的最终准确率比随机样本训练的模型高出大约两个百分点。虽然这看起来不多，但我们也可以从另一个角度来分析数据：为了实现特定的准确率，需要多少样本？如果我们检查图表，可以看到，知识不确定性线在400个训练样本下达到了96%的准确率。而随机样本训练的模型则至少需要750个样本才能达到相同的准确率。这意味着，在相同准确率下，知识不确定性方法只需要几乎一半的数据量。这表明，采用正确的获取函数进行主动学习非常有用，特别是在计算资源充足但标注成本昂贵的情况下：通过正确选择样本，我们可能能够将标注成本降低一倍，从而实现相同的准确率。'
- en: 'Because we saved the acquired samples for every iteration, we can also inspect
    the type of images selected by both models. To make our visualization easier to
    interpret, we will visualize the last five acquired images for every method for
    every label. To do this, we first define a function that returns the images per
    label for a set of model directories:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们保存了每次迭代获取的样本，所以我们也可以检查两种模型选择的图像类型。为了使我们的可视化更易于解释，我们将可视化每种方法对于每个标签所选的最后五个图像。为此，我们首先定义一个函数，返回每个标签的图像集，对于一组模型目录：
- en: '[PRE72]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'We then define a function that creates a `PIL Image` where we concatenate the
    images per label for a particular acquisition function:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义一个函数，创建一个`PIL 图像`，其中按标签将图像进行拼接，以便用于特定的获取函数：
- en: '[PRE73]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'We can then call these functions, in our case with the following setup and
    *UUID*s:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以调用这些函数，在我们的案例中使用以下设置和*UUID*：
- en: '[PRE74]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Let’s compare the output.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较一下输出。
- en: '![PIC](img/file173.png)![PIC](img/file174.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file173.png)![图片](img/file174.png)'
- en: 'Figure 8.9: Images randomly selected (left) and images selected via knowledge
    uncertainty with MC dropout (right). Every row shows the last five images selected
    for the label'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9：随机选择的图像（左）与通过知识不确定性和MC丢弃法选择的图像（右）。每一行显示每个标签所选的最后五个图像
- en: We can see in *Figure* [*8.9*](#x1-156175r9) that the images selected by the
    knowledge uncertainty acquisition function are probably more difficult to classify
    compared to the randomly selected images. The uncertainty acquisition function
    selects quite a few unusual representations of the digits in the dataset. Because
    our acquisition function was able to select these images, the model was better
    able to understand the full distribution of the dataset, which resulted in better
    accuracy over time.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图* [*8.9*](#x1-156175r9)中看到，通过知识不确定性获取函数选取的图像相比随机选择的图像可能更难以分类。这个不确定性获取函数选择了数据集中一些不寻常的数字表示。由于我们的获取函数能够选取这些图像，模型能够更好地理解数据集的整体分布，从而随着时间的推移提高了准确率。
- en: 8.5 Using uncertainty estimates for smarter reinforcement learning
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 使用不确定性估计实现更智能的强化学习
- en: '**Reinforcement learning** aims to develop machine learning techniques capable
    of learning from their environment. There’s a clue to the fundamental principle
    behind reinforcement learning in its name: the aim is to reinforce successful
    behaviour. Generally speaking, in reinforcement learning, we have an agent capable
    of executing a number of actions in an environment. Following these actions, the
    agent receives feedback from the environment, and this feedback is used to allow
    the agent to build a better understanding of which actions are more likely to
    lead to a positive outcome given the current state of the environment.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**旨在开发能够从环境中学习的机器学习技术。强化学习背后的基本原则在其名称中有一丝线索：目标是加强成功的行为。一般来说，在强化学习中，我们有一个智能体能够在环境中执行一系列的动作。在这些动作之后，智能体从环境中获得反馈，而这些反馈被用来帮助智能体更好地理解哪些动作更可能导致在当前环境状态下获得积极的结果。'
- en: Formally, we can describe this using a set of states, *S*, a set of actions
    *A*, which map from a current state *s* to a new state *s*^′, and a reward function,
    *R*(*s,s*^′), describing the reward for the transition between the current state,
    *s*, and the new state, *s*^′. The set of states comprises a set of environment
    states, *S*[*e*], and a set of agent states, *S*[*a*], which together describe
    the state of the entire system.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，我们可以使用一组状态 *S*、一组动作 *A* 来描述它们如何从当前状态 *s* 转换到新的状态 *s*^′，以及奖励函数 *R*(*s,s*^′)，描述当前状态
    *s* 和新状态 *s*^′ 之间的过渡奖励。状态集由环境状态集 *S*[*e*] 和智能体状态集 *S*[*a*] 组成，两者共同描述整个系统的状态。
- en: We can think of this in terms of a game of Marco Polo, wherein call and response
    is used by one player in order to find another player. When the seeking player
    calls ”Marco,” the other player replies ”Polo,” giving the seeking player an estimate
    of their location based on the direction and amplitude of the sound. If we simplify
    this to consider it in terms of distance, a closer state would be one for which
    the distance reduces, such as *δ* = *d* − *d*^′ *>* 0, where *d* is the distance
    at state *s* and *d*^′ is the distance for state *s*^′. Conversely, a further
    state would be one for which *δ* = *d* − *d*^′ *<* 0\. Thus, for this example,
    we can use our *δ* values as feedback for our model, making our reward function
    *δ* = *R*(*s,s*^′) = *d* − *d*^′.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此类比为一场马可·波罗的游戏，其中一个玩家通过“喊叫”与“回应”的方式来找到另一个玩家。当寻找的玩家喊“Marco”时，另一个玩家回应“Polo”，根据声音的方向和幅度给出寻找者其位置的估计。如果我们将此简化为考虑距离，那么较近的状态是距离减少的状态，例如*δ*
    = *d* − *d*^′ *>* 0，其中 *d* 是状态 *s* 的距离，*d*^′ 是状态 *s*^′ 的距离。相反，较远的状态是*δ* = *d*
    − *d*^′ *<* 0。因此，在这个例子中，我们可以使用我们的*δ*值作为模型的反馈，使得我们的奖励函数为*δ* = *R*(*s,s*^′) = *d*
    − *d*^′。
- en: '![PIC](img/file175.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file175.jpg)'
- en: 'Figure 8.10: Illustration of a Marco Polo reinforcement learning scenario'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10：马可·波罗强化学习场景的插图
- en: 'Let’s consider our agent as the seeking player and its target as the hiding
    player. At each step, our agent collects more information about its environment,
    enabling it to better model the relationship between its actions *A*(*s*) and
    the reward function *R*(*s,s*^′) (in other words, it’s learning which general
    direction it needs to move in to get closer to the target). At each step, we need
    to predict the reward function given the set of possible actions at the current
    state, *A*[*s*], so that we can choose the action that is most likely to maximize
    this reward function. In this case, the set of actions could be a set of directions
    we can move in, for example: forward, back, left, and right.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把智能体视为寻找玩家，把目标视为隐藏玩家。在每一步，智能体会收集更多关于环境的信息，从而更好地建模其行动 *A*(*s*) 和奖励函数 *R*(*s,s*^′)
    之间的关系（换句话说，它在学习需要朝哪个方向移动，以便更接近目标）。在每一步，我们需要预测奖励函数，给定当前状态下的可能行动集 *A*[*s*]，以便选择最有可能最大化该奖励函数的行动。在这种情况下，行动集可以是我们可以移动的方向集，例如：前进、后退、左转和右转。
- en: Traditional reinforcement learning uses a method called **Q Learning** to learn
    the relationship between the state, action, and reward. Q Learning doesn’t involve
    neural network models, and instead accumulates state, action, and reward information
    in a table – the Q table – which is then used to determine the action most likely
    to produce the highest reward given the current state. While Q Learning is powerful,
    it becomes computationally prohibitive for large numbers of states and actions.
    To address this, researchers introduced the concept of **Deep Q Learning**, wherein
    the Q table is replaced by a neural network. Over a (usually large) number of
    iterations, the neural network learns which actions are likely to produce a higher
    reward given the current state.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的强化学习使用一种叫做**Q学习**的方法来学习状态、行动和奖励之间的关系。Q学习不涉及神经网络模型，而是将状态、行动和奖励信息存储在一个表格中——Q表格——然后用来确定在当前状态下最有可能产生最高奖励的行动。虽然Q学习非常强大，但对于大量的状态和行动，它的计算成本变得不可承受。为了解决这个问题，研究人员引入了**深度Q学习**的概念，其中Q表格被神经网络所替代。在通常经过大量迭代后，神经网络会学习在给定当前状态的情况下，哪些行动更有可能产生更高的奖励。
- en: 'To predict which action is likely to yield the highest reward value, we use
    a model trained on all historical actions, *A*[*h*], states *S*[*h*], and rewards,
    *R*[*h*]. Our training input *X* comprises the actions *A*[*h*] and states *S*[*h*],
    while our target output *y* comprises the reward values *R*[*h*]. We can then
    use the model as part of a **Model predictive Controller**, or **MPC**, which
    will select the action depending on which action is associated with the highest
    predicted reward:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了预测哪种行动可能产生最高的奖励值，我们使用一个经过训练的模型，该模型基于所有历史行动 *A*[*h*]、状态 *S*[*h*] 和奖励 *R*[*h*]。我们的训练输入
    *X* 包含行动 *A*[*h*] 和状态 *S*[*h*]，而目标输出 *y* 包含奖励值 *R*[*h*]。然后，我们可以将该模型作为**模型预测控制器**（**MPC**）的一部分，选择行动，依据是哪个行动与最高预测奖励相关：
- en: '![anext = argmax yi∀ai ∈ As ](img/file176.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![anext = argmax yi∀ai ∈ As ](img/file176.jpg)'
- en: Here, *y*[*i*] is the reward prediction produced by our model, *f*(*a*[*i*]*,s*),
    which maps the current state *s* and possible actions *a*[*i*] ∈ *A*[*s*] to reward
    values. However, before our model is of any use, we’ll need to gather data to
    train on. We’ll accrue data over a number of episodes, wherein each episode comprises
    a set of actions taken by the agent until some termination criteria are met. The
    ideal termination criterion would be the agent finding the target, but we can
    set other criteria, such as the agent encountering an obstacle, or the agent exhausting
    a maximum number of actions. Because the model has no information to start off
    with, we use a greedy policy commonly used in reinforcement learning, called an
    *𝜖greedy* policy, to allow the agent to start by randomly sampling from its environment.
    The idea here is that our agent will perform a random action with probability
    *𝜖*, and will otherwise use model predictions to select the action. After each
    episode, we will decrease *𝜖*, such that the agent will eventually be selecting
    actions based solely on the model. Let’s put together a simple reinforcement learning
    example to see all of this in action.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*y*[*i*] 是我们的模型产生的奖励预测，*f*(*a*[*i*]*,s*)，它将当前状态 *s* 和可能的动作 *a*[*i*] ∈ *A*[*s*]
    映射到奖励值。然而，在我们的模型有任何用处之前，我们需要收集数据进行训练。我们将在多个回合中积累数据，每个回合包括代理采取的一系列动作，直到满足某些终止标准。理想的终止标准是代理找到目标，但我们也可以设置其他标准，例如代理遇到障碍物或代理用尽最大动作数。由于模型开始时没有任何信息，我们使用一种在强化学习中常见的贪婪策略，叫做
    *𝜖greedy* 策略，允许代理通过从环境中随机采样开始。这里的想法是，我们的代理以 *𝜖* 的概率执行随机动作，否则使用模型预测来选择动作。在每个回合之后，我们会减少
    *𝜖*，使得代理最终仅根据模型来选择动作。让我们构建一个简单的强化学习示例，看看这一切是如何运作的。
- en: 'Step 1: Initializing our environment'
  id: totrans-289
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一步：初始化我们的环境
- en: 'Our reinforcement learning example will be centred around our environment:
    this defines the space in which everything happens. We’ll handle this with the
    `Environment` class. First, we set up our environment parameters:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的强化学习示例将围绕我们的环境展开：这定义了所有事件发生的空间。我们将使用`Environment`类来处理这个问题。首先，我们设置环境参数：
- en: '[PRE75]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Here, notice our environment size, denoted by `env_size`, which defines the
    number of rows and columns in our environment – in this case, we’ll have an environment
    of size 8 × 8, resulting in 64 locations (for simplicity, we’ll stick with a square
    environment). We’ll also set a `max_steps` limit so that episodes don’t go on
    too long while our agent is randomly selecting actions.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，注意我们的环境大小，用`env_size`表示，它定义了环境中的行数和列数——在这个例子中，我们将使用 8 × 8 的环境，结果是 64 个位置（为了简便，我们将使用一个方形环境）。我们还将设置一个`max_steps`限制，以确保在代理随机选择动作时，回合不会进行得太长。
- en: We also set the `agent_location` and `target_location` variables – the agent
    always starts at point [0, 0], while the target location is randomly allocated.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还设置了`agent_location`和`target_location`变量——代理总是从 [0, 0] 点开始，而目标位置则是随机分配的。
- en: 'Next, we create a dictionary to map an integer value to an action. Going from
    0 to 3, these actions are: forward, backward, right, left. We also set the `delta`
    variable – this is the initial distance between the agent and the target (we’ll
    see how `compute_distance()` is implemented in a moment).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个字典，将整数值映射到一个动作。从 0 到 3，这些动作分别是：向前、向后、向右、向左。我们还设置了`delta`变量——这是代理与目标之间的初始距离（稍后我们将看到`compute_distance()`是如何实现的）。
- en: 'Finally, we initialize a few variables for tracking whether the termination
    criteria have been met (`is_done`), the total number of steps (`total_steps`),
    and the ideal number of steps (`ideal_steps`). The latter of these variables is
    the minimum number of steps required for the agent to get to the target from its
    starting position. We’ll use this to calculate the regret, which is a useful indicator
    of performance for reinforcement learning and optimization algorithms. To calculate
    this, we’ll add the following two functions to our class:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们初始化一些变量，用于跟踪终止标准是否已满足（`is_done`）、总步骤数（`total_steps`）和理想步骤数（`ideal_steps`）。后者是代理从起始位置到达目标所需的最小步骤数。我们将用它来计算遗憾，这是强化学习和优化算法中一个有用的性能指标。为了计算遗憾，我们将向我们的类中添加以下两个函数：
- en: '[PRE76]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Here, `calculate_ideal_steps()` will run until the distance (`delta`) between
    the agent and the target is zero. At each iteration, it uses `calculate_ideal_action()`
    to select the action that will move the agent closest to the target.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`calculate_ideal_steps()`将一直运行，直到代理与目标之间的距离（`delta`）为零。在每次迭代中，它使用`calculate_ideal_action()`来选择能使代理尽可能接近目标的动作。
- en: 'Step 2: Updating the state of our environment'
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第二步：更新我们环境的状态
- en: 'Now that we’ve initialized our environment, we need to add one of the most
    crucial pieces of our class: the `update` method. This controls what happens to
    our environment when the agent takes a new action:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经初始化了我们的环境，我们需要添加我们类中最关键的一个部分：`update`方法。这控制了当代理采取新动作时环境的变化：
- en: '[PRE77]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The method receives an action integer, and uses this to access the corresponding
    action in the `action_space` dictionary we defined earlier. It then updates the
    agent location. Because both the agent location and action are vectors, we can
    simply use vector addition to do this. Next, we check whether the agent has moved
    out of bounds of our environment – if it has, we simply adjust its location so
    that it remains within our environment boundary.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法接收一个动作整数，并使用它来访问我们之前定义的`action_space`字典中对应的动作。然后更新代理位置。因为代理位置和动作都是向量，所以我们可以简单地使用向量加法来完成这一点。接下来，我们检查代理是否移出了环境的边界
    – 如果是，则调整其位置使其仍然保持在我们的环境边界内。
- en: 'The next line is another crucial piece of code: computing the reward with `compute_reward()`
    – we’ll take a look at this in just a moment. Once we’ve computed the reward,
    we increment the `total_steps` counter, check our termination criteria, and return
    the reward value for the action.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行是另一个关键的代码片段：使用`compute_reward()`计算奖励 – 我们马上就会看到这个。一旦计算出奖励，我们增加`total_steps`计数器，检查终止条件，并返回动作的奖励值。
- en: 'We determine the reward using the following function. This will return a low
    reward (`1`) if the distance between the agent and the target increases, and a
    high reward (`10`) if the distance between the agent and target decreases:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下函数来确定奖励。如果代理与目标之间的距离增加，则返回低奖励（`1`），如果减少，则返回高奖励（`10`）：
- en: '[PRE78]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'This uses the `compute_distance()` function, which calculates the Euclidean
    distance between the agent and the target:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用了`compute_distance()`函数，计算代理与目标之间的欧氏距离：
- en: '[PRE79]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Lastly, we need a function to allow us to fetch the state of the environment,
    so that we can associate this with the reward values. We define this as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要一个函数来允许我们获取环境的状态，以便将其与奖励值关联起来。我们将其定义如下：
- en: '[PRE80]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Step 3: Defining our model'
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第三步：定义我们的模型
- en: 'Now that we’ve set up our environment, we’ll create a model class. This class
    will handle model training and inference, as well as selecting the best action
    according to the model’s predictions. As always, we start with the `__init__()`
    method:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置好了环境，我们将创建一个模型类。这个类将处理模型训练和推断，以及根据模型预测选择最佳动作。和往常一样，我们从`__init__()`方法开始：
- en: '[PRE81]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Here, we pass a few variables related to our environment, such as the state
    size and number of actions. The code relating to the model definition should be
    familiar – we’re simply instantiating a neural network using Keras. One point
    to note is that we’re using the Huber loss here, instead of something more common
    such as the mean squared error. This is a common choice in robust regression tasks
    and in reinforcement learning. The reason for this is that the Huber loss dynamically
    switches between mean squared error and mean absolute error. The former is very
    good at penalizing small errors, while the latter is more robust to outliers.
    Through the Huber loss, we arrive at a loss function that is both robust to outliers
    and penalizes small errors.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们传递了一些与我们的环境相关的变量，如状态大小和动作数量。与模型定义相关的代码应该很熟悉 – 我们只是使用Keras实例化了一个神经网络。需要注意的一点是，我们在这里使用Huber损失，而不是更常见的均方误差。这是在强化学习和健壮回归任务中常见的选择。Huber损失动态地在均方误差和平均绝对误差之间切换。前者非常擅长惩罚小误差，而后者对异常值更为健壮。通过Huber损失，我们得到了一个既对异常值健壮又惩罚小误差的损失函数。
- en: 'This is particularly important in reinforcement learning because of the exploratory
    nature of the algorithms: we will often encounter some examples that are very
    exploratory, deviating significantly from the rest of the data, and thus causing
    large errors during training.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 这在强化学习中特别重要，因为算法具有探索性特征：我们经常会遇到一些极具探索性的样本，它们与其他数据相比偏离较大，从而在训练过程中导致较大的误差。
- en: 'With our class initialization out of the way, we move on to our `fit()` and
    `predict()` functions:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成类的初始化后，我们继续处理 `fit()` 和 `predict()` 函数：
- en: '[PRE82]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The `fit()` function should look very familiar – we’re just scaling our inputs
    before fitting our Keras model. The `predict()` function has a little more going
    on. Because we need predictions for each of our possible actions (forward, backward,
    right, left), we need to generate inputs for these. We do so by concatenating
    the integer value associated with the action to the state, producing our complete
    state-action vector as we see on line 11\. Doing this for all actions results
    in our input matrix, *X*, for which each row is associated with a specific action.
    We then scale *X* and run inference on this to obtain our predicted reward values.
    To select an action, we simply use `np.argmax()` to obtain the index associated
    with the highest predicted reward.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()` 函数应该非常熟悉——我们只是对输入进行缩放，然后再拟合我们的 Keras 模型。`predict()` 函数则稍微复杂一点。因为我们需要对每个可能的动作（前进、后退、右转、左转）进行预测，所以我们需要为这些动作生成输入。我们通过将与动作相关的整数值与状态进行拼接，来生成完整的状态-动作向量，正如第
    11 行所示。对所有动作执行此操作后，我们得到输入矩阵 *X*，其中每一行都对应一个特定的动作。然后，我们对 *X* 进行缩放，并在其上运行推理，以获得预测的奖励值。为了选择一个动作，我们简单地使用
    `np.argmax()` 来获取与最高预测奖励相关的索引。'
- en: 'Step 4: Running our reinforcement learning'
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第 4 步：运行我们的强化学习
- en: 'Now that we’ve defined our `Environment` and `RLModel` classes, we’re ready
    to do some reinforcement learning! Let’s first set up some important variables
    and instantiate our model:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经定义了 `Environment` 和 `RLModel` 类，准备开始强化学习了！首先，我们设置一些重要的变量并实例化我们的模型：
- en: '[PRE83]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Most of these should be familiar by now, but we’ll go over a few that we’ve
    not yet covered. The `history` dictionary is where we’ll store our state and reward
    information as we progress through each step in each episode. We’ll then use this
    information to train our model. Another unfamiliar variable here is `n_samples`
    – we’re setting this because, rather than using all available data each time we
    train our model, we’ll sample 1,000 data points from our data. This helps to avoid
    our training time exploding as we accrue more and more data. The last new variable
    here is `regrets`. This list will store our regret values for each episode. In
    our case, regret is defined simply as the difference between the number of steps
    taken by the model and the minimum number of steps required for the agent to reach
    the target:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内容现在应该已经很熟悉了，但我们还是会再回顾一些尚未覆盖的部分。`history` 字典是我们存储状态和奖励信息的地方，在每一轮的每个步骤中，我们会更新这些信息。然后，我们会利用这些信息来训练我们的模型。另一个不太熟悉的变量是
    `n_samples`——我们设置这个变量是因为每次训练模型时，并不是使用所有可用的数据，而是从数据中随机抽取 1,000 个数据点。这样可以避免随着数据量的不断增加，我们的训练时间也不断暴增。这里的最后一个新变量是
    `regrets`。这个列表将存储每一轮的遗憾值。在我们的案例中，遗憾被简单地定义为模型所采取的步骤数与智能体到达目标所需的最小步骤数之间的差值：
- en: '![regret = steps − steps model ideal ](img/file177.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![regret = steps − steps model ideal ](img/file177.jpg)'
- en: 'As such, regret is zero *⇔* *steps*[*model*] == *steps*[*ideal*]. Regret is
    useful for measuring performance as our model learns, as we’ll see in a moment.
    All that’s left is the main loop of our reinforcement learning process:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，遗憾为零 *⇔* *steps*[*model*] == *steps*[*ideal*]。遗憾值对于衡量模型学习过程中的表现非常有用，正如我们稍后将看到的那样。接下来就是强化学习过程的主循环：
- en: '[PRE84]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Here, we have our reinforcement learning process run from 100 episodes, reinitializing
    the environment each time. As we can see from the internal `while` loop, we will
    continue iterating – updating our agent and measuring our reward – until one of
    the termination criteria is met (either the agent reaches the target, or we run
    for the maximum allowed number of iterations).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的强化学习过程会运行 100 轮，每次都重新初始化环境。通过内部的 `while` 循环可以看到，我们会不断地迭代——更新智能体并衡量奖励——直到满足其中一个终止条件（无论是智能体达到目标，还是我们达到最大允许的迭代次数）。
- en: After each episode, a `print` statement lets us know that the episode completed
    without error, and tells us how our agent did compared to the ideal number of
    steps. We then calculate the regret and append this to our `regrets` list, sample
    from our data in `history` and fit our model on the sampled data. Lastly, we finish
    each iteration of the outer loop by reducing epsilon.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 每一轮结束后，`print`语句会告诉我们该轮是否没有错误完成，并告诉我们我们的智能体与理想步数的对比结果。接着，我们计算遗憾值，并将其附加到`regrets`列表中，从`history`中的数据进行采样，并在这些样本数据上拟合我们的模型。最后，每次外循环迭代结束时，我们会减少epsilon值。
- en: 'After running this, we can additionally plot our regret values to see how we
    did:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完之后，我们还可以绘制遗憾值图，以查看我们的表现：
- en: '[PRE85]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'This produces the following plot, showing how our model did over the 100 episodes:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图表，展示我们模型在100轮训练中的表现：
- en: '![PIC](img/file178.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file178.png)'
- en: 'Figure 8.11: Plot of regret values following 100 episodes of reinforcement
    learning'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11：强化学习100轮后的遗憾值图
- en: As we can see here, it did poorly to begin with, but the model quickly learned
    to predict reward values, allowing it to predict optimal actions, and reducing
    regret to 0.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这里看到的，它一开始表现得很差，但模型很快学会了预测奖励值，从而能够预测最优动作，将遗憾减少到0。
- en: 'So far, things are pretty simple. In fact, you may be wondering why we need
    a model at all – why not just calculate the distance between the target and the
    proposed positions, and select an action accordingly? Well, firstly, the aim of
    reinforcement learning is for an agent to discover how to interact in a given
    setting without any prior knowledge – so while our agent can execute actions,
    it has no concept of distance. This is something that is learned through interacting
    with the environment. Secondly, it may not be that simple: what if there are obstacles
    in the environment? In this case, our agent needs to be more intelligent than
    simply moving toward the sound.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，事情还算简单。事实上，你可能会想，为什么我们需要模型呢——为什么不直接计算目标和拟议位置之间的距离，然后选择相应的动作呢？首先，强化学习的目标是让智能体在没有任何先验知识的情况下发现如何在给定环境中进行交互——所以，尽管我们的智能体可以执行动作，但它没有距离的概念。这是通过与环境的互动来学习的。其次，情况可能没有那么简单：如果环境中有障碍物呢？在这种情况下，我们的智能体需要比简单地朝声音源移动更聪明。
- en: While this is just an illustrative example, real-world applications of reinforcement
    learning involve scenarios for which we have very limited knowledge, and thus
    designing an agent that can explore its environment and learn how to interact
    optimally allows us to develop models for applications for which supervised methods
    aren’t an option.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这只是一个示范性的例子，但强化学习在现实世界中的应用涉及一些我们知识非常有限的情境，因此，设计一个能够探索环境并学习如何最优互动的智能体，使我们能够为那些无法使用监督学习方法的应用开发模型。
- en: 'Another factor to consider in real-world scenarios is risk: we want our agent
    to make *sensible* decisions, not just decisions that maximize the reward: we
    need it to build some understanding of the risk/reward trade-off. This is where
    uncertainty estimates come in.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在现实世界情境中需要考虑的因素是风险：我们希望我们的智能体做出*明智*的决策，而不仅仅是最大化奖励的决策：我们需要它能够理解风险/回报的权衡。这就是不确定性估计的作用所在。
- en: 8.5.1 Navigating obstacles with uncertainty
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5.1 带有不确定性的障碍物导航
- en: With uncertainty estimates, we can balance the reward against the model’s confidence
    in its prediction. If its confidence is low (meaning that uncertainty is high),
    then we may want to be cautious about how we incorporate our model’s predictions.
    For example, let’s take the reinforcement learning scenario we’ve just explored.
    For each episode, our model is predicting which action will yield the highest
    reward, and our agent then chooses this action. In the real world, things aren’t
    so predictable – our environment can change, leading to unexpected consequences.
    What if an obstacle appears in our environment, and colliding with that obstacle
    prevents our agent from completing its task? Well, clearly if our agent hasn’t
    yet encountered the obstacle, it’s doomed to fail. Fortunately, in the case of
    Bayesian Deep Learning, this isn’t the case. As long as we have some way of sensing
    the obstacle, our agent can detect the obstacle and take a different route – even
    if the obstacle wasn’t encountered in previous episodes.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 通过不确定性估计，我们可以在奖励和模型对其预测的信心之间找到平衡。如果模型的信心较低（意味着不确定性较高），那么我们可能希望对如何整合模型的预测保持谨慎。例如，假设我们刚刚探讨的强化学习场景。在每一轮中，我们的模型预测哪个动作将获得最高的奖励，然后我们的智能体选择该动作。在现实世界中，事情并不是那么可预测——我们的环境可能会发生变化，导致意外的后果。如果我们的环境中出现了障碍物，并且与障碍物发生碰撞会阻止我们的智能体完成任务，那么显然，如果我们的智能体还没有遇到过这个障碍物，它注定会失败。幸运的是，在贝叶斯深度学习的情况下，情况并非如此。只要我们有某种方式来感知障碍物，我们的智能体就能够检测到障碍物并选择不同的路径——即使该障碍物在之前的回合中没有出现。
- en: '![PIC](img/file179.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file179.jpg)'
- en: 'Figure 8.12: Illustration of how uncertainty affects the actions of a reinforcement
    learning agent'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12：不确定性如何影响强化学习智能体行动的示意图
- en: 'This is possible thanks to our uncertainty estimates. When the model encounters
    something unusual, its uncertainty estimate for that prediction will be high.
    Thus, if we incorporate this into our MPC equation, we can balance reward with
    uncertainty, ensuring that we prioritize lower risk over higher reward. To do
    so, we modify our MPC equation as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切之所以可能，得益于我们的不确定性估计。当模型遇到不寻常的情况时，它对该预测的不确定性估计将会较高。因此，如果我们将其融入到我们的MPC方程中，我们就能在奖励和不确定性之间找到平衡，确保我们优先考虑较低的风险，而非较高的奖励。为了做到这一点，我们修改了我们的MPC方程，具体如下：
- en: '![anext = argmax (yi − λσi)∀ai ∈ As ](img/file180.jpg)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![anext = argmax (yi − λσi)∀ai ∈ As ](img/file180.jpg)'
- en: Here, we see that we’re now subtracting a value, *λσ*[*i*], from our reward
    prediction *y*[*i*]. This is because *σ*[*i*] is our uncertainty associated with
    the *i*th prediction. We use *λ* to scale the uncertainty so that it appropriately
    penalizes uncertain actions; this is a parameter we can tune depending on the
    application. With a sufficiently well calibrated method, we’ll see larger values
    for *σ*[*i*] in cases where the model is uncertain about its predictions. Let’s
    build on our earlier code example to see this in action.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到我们正在从我们的奖励预测 *y*[*i*] 中减去一个值，*λσ*[*i*]。这是因为 *σ*[*i*] 是与第 *i* 次预测相关的不确定性。我们使用
    *λ* 来缩放不确定性，以便适当惩罚不确定的动作；这是一个可以根据应用进行调整的参数。通过一个经过良好校准的方法，我们将看到在模型对预测不确定时，*σ*[*i*]
    的值会较大。让我们在之前的代码示例的基础上，看看这一过程如何实现。
- en: 'Step 1: Introducing obstacles'
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一步：引入障碍物
- en: 'To create a challenge for our agent, we’re going to introduce obstacles to
    our environment. To test how our agent responds to unfamiliar input, we’re going
    to change the policy that our obstacle follows - it will either follow a static
    policy or a dynamic policy depending on our environment settings. We’ll change
    the `__init__()` function for our `Environment` class to incorporate these changes:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给我们的智能体制造挑战，我们将向环境中引入障碍物。为了测试智能体如何应对不熟悉的输入，我们将改变障碍物的策略——它将根据我们的环境设置，选择遵循静态策略或动态策略。我们将修改
    `Environment` 类的 `__init__()` 函数，以便整合这些更改：
- en: '[PRE86]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: There’s quite a lot going on here, so we’ll go through each of the changes.
    First, to determine whether the obstacle is static or dynamic, we set the `dynamic_obstacle`
    variable. If this is `True`, then we’ll randomly set the obstacle location. If
    it’s `False`, then our object will sit in the middle of our environment. We’re
    also setting our `lambda` (*λ*) parameter here, which defaults to 2.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这里涉及的内容比较复杂，所以我们将逐一讲解每个更改。首先，为了确定障碍物是静态的还是动态的，我们设置了 `dynamic_obstacle` 变量。如果该值为
    `True`，我们将随机设置障碍物的位置。如果该值为 `False`，则我们的物体将停留在环境的中央。我们还在此设置了我们的 `lambda` (*λ*)
    参数，默认值为 2。
- en: 'We’ve also introduced a `while` loop here when setting `target_location`: we’ve
    done this to ensure that there’s some distance between the agent and the target.
    We need to do this to ensure there’s space between our agent and our target to
    drop in our dynamic obstacle – otherwise our agent may never encounter the obstacle
    (which would somewhat defeat the point of this example).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在设置 `target_location` 时引入了一个 `while` 循环：我们这么做是为了确保智能体和目标之间有一定的距离。我们需要这么做是为了确保在智能体和目标之间留有足够的空间，以便放置动态障碍物——否则，智能体可能永远无法遇到这个障碍物（这将稍微违背本示例的意义）。
- en: 'Lastly, we compute the obstacle location on line 17: you’ll note that this
    just sets it to the middle of the environment. This is because we use the `dynamic_obstacle`
    flag later on to place the obstacle between the agent and the target – we do this
    during the `calculate_ideal_steps()` function, as this way we know the obstacle
    will lie along the agent’s ideal path (and is thus more likely to be encountered).'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在第 17 行计算障碍物的位置：你会注意到这只是将它设置在环境的中央。这是因为我们稍后会使用 `dynamic_obstacle` 标志将障碍物放置在智能体和目标之间——我们在
    `calculate_ideal_steps()` 函数中这么做，因为这样我们就知道障碍物将位于智能体的理想路径上（因此更有可能被遇到）。
- en: 'Step 2: Placing our dynamic obstacle'
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 2：放置动态障碍物
- en: 'When `dynamic_obstacle` is `True`, we want to place our obstacle somewhere
    different each episode, thus posing more of a challenge for our agent. To do so,
    we add a modification to the `calculate_ideal_steps()` function, as mentioned
    previously:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `dynamic_obstacle` 为 `True` 时，我们希望在每个回合将障碍物放置在不同的位置，从而为我们的智能体带来更多挑战。为此，我们在之前提到的
    `calculate_ideal_steps()` 函数中进行了一些修改：
- en: '[PRE87]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: Here, we see that we call `np.random.randint(0, 2)` on each iteration of the
    `while` loop. This is to randomize in which position the obstacle is placed along
    the ideal path.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到我们在每次执行 `while` 循环时都调用了 `np.random.randint(0, 2)`。这是为了随机化障碍物沿理想路径的放置位置。
- en: 'Step 3: Adding sensing'
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 3：添加感知功能
- en: 'Our agent will have no hope of avoiding the object introduced into our environment
    if it can’t sense the object. As such, we’ll add a function to simulate a sensor:
    `get_obstacle_proximity()`. This sensor will give our agent information on how
    close it would get to an object were it to take a certain action. We’ll have this
    return progressively higher values depending on how close to the object a given
    action would place our agent. If the action places our agent sufficiently far
    from the object (in this case, at least 4.5 spaces), then our sensor will return
    zero. This sensing function allows our agent to effectively see one step ahead,
    so we can think of the sensor as having a range of one step.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的智能体无法感知环境中引入的物体，那么它将没有任何希望避免这个物体。因此，我们将添加一个函数来模拟传感器：`get_obstacle_proximity()`。该传感器将为我们的智能体提供关于如果它执行某个特定动作时，它会接近物体的距离信息。根据给定动作将我们的智能体靠近物体的距离，我们将返回逐渐增大的数值。如果动作将智能体置于足够远的位置（在这种情况下，至少
    4.5 个空间），则我们的传感器将返回零。这个感知功能使得我们的智能体能够有效地看到一步之遥，因此我们可以将该传感器视为具有一步的感知范围。
- en: '[PRE88]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Here, we first compute the future proximity for the agent given each action,
    after which we compute integer ”proximity” values. These are computed by first
    constructing Boolean arrays for each proximity condition, in this case being *δ*[*o*]
    *<* 2*.*5, *δ*[*o*] *<* 3*.*5, and *δ*[*o*] *<* 4*.*5, where *δ*[*o*] is the distance
    to the obstacle. We then sum these such that the proximity score has integer values
    of 3, 2, or 1 depending on how many of the criteria are met. This gives us a sensor
    that returns some basic information about the obstacle’s future proximity for
    each of the proposed actions.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先计算每个动作后智能体的未来接近度，然后计算整数“接近度”值。这些值是通过首先构造每个接近度条件的布尔数组来计算的，在这种情况下分别为 *δ*[*o*]
    *<* 2*.*5, *δ*[*o*] *<* 3*.*5 和 *δ*[*o*] *<* 4*.*5，其中 *δ*[*o*] 是与障碍物的距离。然后，我们将这些条件求和，使得接近度得分具有
    3、2 或 1 的整数值，具体取决于满足多少个条件。这为我们提供了一个传感器，它会根据每个提议的动作返回有关障碍物未来接近度的基本信息。
- en: 'Step 4: Modifying our reward function'
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 步骤 4：修改奖励函数
- en: 'The last thing we need to do to prepare our environment is to update our reward
    function:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 准备环境的最后一件事是更新我们的奖励函数：
- en: '[PRE89]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Here, we’ve added a statement to check whether the agent and obstacle have collided
    (checking whether the distance between the two is zero). If so, we’ll return a
    reward of 0, and set both the `collision` and `is_done` variables to `True`. This
    introduces a new termination criteria, **collision**, and will allow our agent
    to learn that collisions are bad, as these receive the lowest reward.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们添加了一条语句来检查代理与障碍物是否发生碰撞（检查两者之间的距离是否为零）。如果发生碰撞，我们将返回奖励值0，并将`collision`和`is_done`变量设置为`True`。这引入了新的终止标准——**碰撞**，并将允许我们的代理学习到碰撞是有害的，因为这些情况会得到最低的奖励。
- en: 'Step 5: Initializing our uncertainty-aware model'
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第5步：初始化我们的不确定性感知模型
- en: 'Now that our environment is ready, we need a new model – one capable of producing
    uncertainty estimates. For this model, we’ll use an MC dropout network with a
    single hidden layer:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的环境已经准备好，我们需要一个新的模型——一个能够生成不确定性估计的模型。对于这个模型，我们将使用一个带有单个隐藏层的MC dropout网络：
- en: '[PRE90]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: This should look pretty familiar, but you’ll notice a few key differences. First,
    we’re again using the Huber loss. Secondly, we’ve introduced a dictionary, `proximity_dict`,
    which will record the proximity values received from the sensor and the associated
    model uncertainties. This will allow us to evaluate our model’s sensitivity to
    anomalous proximity values later on.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来应该很熟悉，但你会注意到几个关键的不同之处。首先，我们再次使用Huber损失函数。其次，我们引入了一个字典`proximity_dict`，它将记录从传感器接收到的邻近值和相关的模型不确定性。这将使我们能够稍后评估模型对异常邻近值的敏感性。
- en: 'Step 6: Fitting our MC dropout network'
  id: totrans-364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第6步：拟合我们的MC Dropout网络
- en: 'Next, we need the following lines:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要以下几行代码：
- en: '[PRE91]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: This should again look very familiar – we’re simply preparing our data by first
    scaling our inputs before fitting our model.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该再次看起来很熟悉——我们只是通过首先对输入进行缩放来准备数据，然后拟合我们的模型。
- en: 'Step 7: Making predictions'
  id: totrans-368
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第7步：进行预测
- en: 'Here, we see that we’ve slightly modified our `predict()` function:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到我们稍微修改了`predict()`函数：
- en: '[PRE92]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: More specifically, we’ve added the `obstacle_proximity` and `dynamic_obstacle`
    variables. The former allows us to receive the sensor information and incorporate
    this in the inputs we pass to our model. The latter is a flag telling us whether
    we’re in the dynamic obstacle phase – if so, we want to record information about
    the sensor values and uncertainties in our `proximity_dict` dictionary.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，我们添加了`obstacle_proximity`和`dynamic_obstacle`变量。前者允许我们接收传感器信息，并将其纳入传递给模型的输入中。后者是一个标志，告诉我们是否进入了动态障碍物阶段——如果是，我们希望在`proximity_dict`字典中记录传感器值和不确定性的相关信息。
- en: 'The next block of prediction code should again look very familiar:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 下一段预测代码应该再次看起来很熟悉：
- en: '[PRE93]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: This function simply implements the MC dropout inference, obtaining predictions
    for `nb_inference` forward passes, and returns the means and standard deviations
    associated with our predictive distributions.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数简单地实现了MC dropout推断，通过`nb_inference`次前向传递获得预测，并返回与我们的预测分布相关的均值和标准差。
- en: 'Step 8: Adapting our standard model'
  id: totrans-375
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第8步：调整我们的标准模型
- en: 'To understand the difference that our Bayesian model makes, we’ll need to compare
    it with a non-Bayesian model. As such, we’ll update our `RLModel` class from earlier,
    adding the ability to incorporate proximity information from our proximity sensor:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解我们的贝叶斯模型带来的差异，我们需要将其与非贝叶斯模型进行比较。因此，我们将更新之前的`RLModel`类，添加从邻近传感器获取邻近信息的功能：
- en: '[PRE94]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Crucially, we see here that our decision function has not changed: because
    we don’t have model uncertainties, our model’s `predict()` function is choosing
    actions based only on the predicted reward.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 至关重要的是，我们在这里看到我们的决策函数并没有变化：因为我们没有模型不确定性，我们的模型的`predict()`函数仅基于预测的奖励来选择动作。
- en: 'Step 9: Preparing to run our new reinforcement learning experiment'
  id: totrans-379
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第9步：准备运行我们的新强化学习实验
- en: 'Now we’re ready to set up our new experiment. We’ll initialize the variables
    we used previously, and will introduce a few more:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好设置我们的新实验了。我们将初始化之前使用的变量，并引入几个新的变量：
- en: '[PRE95]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Here, we see that we’ve introduced a `collisions` variable and a `failed` variable.
    These will keep track of the number of collisions and the number of failed episodes
    so that we can compare the performance of our Bayesian model with that of our
    non-Bayesian model. We’re now ready to run our experiment!
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到我们引入了一个`collisions`变量和一个`failed`变量。这些变量将追踪碰撞次数和失败的回合次数，以便我们可以将贝叶斯模型的表现与非贝叶斯模型的表现进行比较。现在我们准备好运行实验了！
- en: 'Step 10: Running our BDL reinforcement experiment'
  id: totrans-383
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第10步：运行我们的BDL强化学习实验
- en: As before, we’re going to run our experiment for 100 episodes. However, this
    time, we’re only going to run training on our model for the first 50 episodes.
    After that, we’ll stop training, and evaluate how well our model is able to find
    a safe path to the target. During these last 50 episodes, we’ll set `dynamic_obstacle`
    to `True`, meaning our environment will now randomly choose a different position
    for our obstacle for each episode. Importantly, these random positions will be
    *along the* *ideal path* between the agent and its target.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将对实验进行 100 回合的运行。然而，这次，我们只会在前 50 回合进行模型训练。之后，我们将停止训练，评估模型在找到安全路径到达目标方面的表现。在这最后
    50 回合中，我们将`dynamic_obstacle`设置为`True`，意味着我们的环境将为每一回合随机选择一个新的障碍物位置。重要的是，这些随机位置将会位于代理与目标之间的*理想路径*上。
- en: 'Let’s take a look at the code:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下代码：
- en: '[PRE96]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: First, we check whether the episode is within the first 50 episodes. If so,
    we instantiate our environment with `dynamic_obstacle=False`, and also set our
    global `dynamic_obstacle` variable to `False`.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查回合是否在前 50 回合之内。如果是，我们通过设置`dynamic_obstacle=False`实例化环境，并将全局变量`dynamic_obstacle`设置为`False`。
- en: If the episode is one of the last 50 episodes, we create an environment with
    a randomly placed obstacle, and also set `epsilon` to 0, to ensure we’re always
    using our model predictions when selecting actions.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 如果回合是最后 50 回合之一，我们创建一个带有随机障碍物的环境，并将`epsilon`设置为 0，以确保我们在选择动作时总是使用模型预测。
- en: 'Next, we enter our `while` loop, setting our agent in motion. This is very
    similar to the loop we saw in the last example, except this time we’re calling
    `env.get_obstacle_proximity()`, using the returned obstacle proximity information
    in our predictions, and also storing this information in our episode history:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进入`while`循环，使我们的代理开始移动。这与我们在上一个示例中看到的循环非常相似，只不过这次我们调用了`env.get_obstacle_proximity()`，并将返回的障碍物接近信息用于我们的预测，同时也将此信息存储在回合历史中：
- en: '[PRE97]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Lastly, we’ll record some information about completed episodes and print the
    outcome of the most recent episode to our terminal. We update our `failed` and
    `collisions` variables and print whether the episode was complete successfully,
    the agent failed to find the target, or the agent collided with the obstacle:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将记录一些已完成回合的信息，并将最新回合的结果打印到终端。我们更新`failed`和`collisions`变量，并打印回合是否成功完成，代理是否未能找到目标，或代理是否与障碍物发生碰撞：
- en: '[PRE98]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: The last statement here also checks whether we’re in the dynamic obstacle phase,
    and if not, runs a round of training and decrements our epsilon value (as with
    the last example).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的最后一条语句还检查我们是否处于动态障碍物阶段，如果不是，则进行一次训练，并减少我们的epsilon值（如同上一个示例）。
- en: 'So, how did we do? Repeating the above 100 episodes for both the `RLModel`
    and `RLModelDropout` models, we obtain the following results:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们的表现如何？重复进行上述 100 回合的实验，对于`RLModel`和`RLModelDropout`模型，我们得到了以下结果：
- en: '|'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **Model** | **Failed episodes** | **Collisions** | **Successful episodes**
    |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **失败的回合数** | **碰撞次数** | **成功的回合数** |'
- en: '|'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **RLModelDropout** | 19 | 3 | 31 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| **RLModelDropout** | 19 | 3 | 31 |'
- en: '|'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| **RLModel** | 16 | 10 | 34 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| **RLModel** | 16 | 10 | 34 |'
- en: '|'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '* * *'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '|'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|  |  |  |  |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |'
- en: 'Figure 8.13: A table showing collision predictions'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13：一张显示碰撞预测的表格
- en: As we see here, there are advantages and disadvantages to consider when choosing
    whether to use a standard neural network or a Bayesian neural network – the standard
    neural network achieves a greater number of successfully completed episodes. However,
    crucially, the agent using the Bayesian neural network only collided with the
    obstacle three times, compared to the standard method’s 10 times – that’s a 70%
    reduction in collisions!
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在选择使用标准神经网络还是贝叶斯神经网络时，都有其优缺点——标准神经网络完成了更多的成功回合。然而，关键是，使用贝叶斯神经网络的代理仅与障碍物发生了
    3 次碰撞，而标准方法发生了 10 次碰撞——这意味着碰撞减少了 70%！
- en: Note that as the experiment is stochastic, your results may differ, but on the
    GitHub repository we have included the experiment complete with the seed used
    to produce these results.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于实验是随机的，您的结果可能会有所不同，但在 GitHub 仓库中，我们已包括完整的实验以及用于生成这些结果的种子。
- en: 'We can get a better idea of why this is by looking at the data we recorded
    in `RLModelDropout`’s `proximity_dict` dictionary:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看在`RLModelDropout`的`proximity_dict`字典中记录的数据，更好地理解为什么会这样：
- en: '[PRE99]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'This produces the following plot:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下图表：
- en: '![PIC](img/file181.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file181.png)'
- en: 'Figure 8.14: Distribution of uncertainty estimates associated with increasing
    proximity sensor values'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14：与增加的接近传感器值相关的不确定性估计分布
- en: As we see here, the model uncertainty estimates increase as the sensor values
    increase. This is because, during the first 50 episodes, our agent learns to avoid
    the centre of the environment (as this is where the obstacle is) – thus it gets
    used to low (or zero) proximity sensor values. This means that higher sensor values
    are anomalous, and are thus able to be picked up by the model’s uncertainty estimates.
    Our agent then successfully accounts for this uncertainty using the uncertainty-aware
    MPC equation.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，模型的不确定性估计随着传感器值的增加而增加。这是因为，在前50个回合中，我们的智能体学会了避开环境的中心（因为障碍物就在这里）——因此，它习惯了较低（或为零）的接近传感器值。这意味着较高的传感器值是异常的，因此能够被模型的不确定性估计所捕捉到。然后，我们的智能体通过使用不确定性感知MPC方程，成功地解决了这种不确定性。
- en: 'In this example, we saw how BDL can be applied to reinforcement learning to
    facilitate more cautious behaviour on the part of our reinforcement learning agents.
    While the example here was fairly basic, the implications are pretty significant:
    imagine this being applied to safety-critical applications. In these settings,
    we’re often happy to accept poorer overall model performance if it meets better
    safety requirements. Thus, BDL has an important place within the field of safe
    reinforcement learning, allowing the development of reinforcement learning methods
    suitable for safety-critical scenarios.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们看到了如何将BDL应用于强化学习，以促进强化学习智能体更谨慎的行为。尽管这里的示例相对基础，但其含义却相当深远：想象一下将其应用于安全关键的应用场景。在这些环境中，如果满足更好的安全要求，我们往往愿意接受模型性能较差。因此，BDL在安全强化学习领域中占有重要地位，能够开发出适用于安全关键场景的强化学习方法。
- en: 'In the next section, we’ll see how BDL can be used to create models that are
    robust to another key consideration for real-world applications: adversarial inputs.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用BDL创建对抗性输入具有鲁棒性的模型，这是现实世界应用中的另一个关键考虑因素。
- en: 8.6 Susceptibility to adversarial input
  id: totrans-446
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 对抗性输入的易感性
- en: In [*Chapter 3*](CH3.xhtml#x1-350003), [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003),
    we saw that we could fool a CNN by slightly perturbing the input pixels of an
    image. A picture that clearly looked like a cat was predicted as a dog with high
    confidence. The adversarial attack that we created (*FSGM*) is one of the many
    adversarial attacks that exist, and BDL might offer some protection against these
    attacks. Let’s see how that works in practice.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第3章*](CH3.xhtml#x1-350003)、[*深度学习基础*](CH3.xhtml#x1-350003)中，我们看到通过稍微扰动图像的输入像素，可以欺骗CNN。原本清晰看起来像猫的图片，被高置信度地预测为狗。我们创建的对抗性攻击（*FSGM*）是许多对抗性攻击之一，BDL可能提供一定的防护作用。让我们看看这在实践中是如何运作的。
- en: 'Step 1: Model training'
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第一步：模型训练
- en: 'Instead of using a pre-trained model, as in [*Chapter 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep* *Learning*](CH3.xhtml#x1-350003), we train a model from
    scratch. We use the same train and test data from [*Chapter 3*](CH3.xhtml#x1-350003),
    [*Fundamentals of Deep Learning*](CH3.xhtml#x1-350003) – see that chapter for
    instructions on how to load the dataset. As a reminder, the dataset is a relatively
    small dataset of cats and dogs. We first define our model. We use a VGG-like architecture
    but add dropout after every `MaxPooling2D` layer:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是使用预训练模型，如在[*第3章*](CH3.xhtml#x1-350003)、[*深度学习基础*](CH3.xhtml#x1-350003)中所做的那样，而是从零开始训练一个模型。我们使用与[*第3章*](CH3.xhtml#x1-350003)、[*深度学习基础*](CH3.xhtml#x1-350003)中相同的训练和测试数据——有关如何加载数据集，请参见该章节。提醒一下，数据集是一个相对较小的猫狗数据集。我们首先定义我们的模型。我们使用类似VGG的架构，但在每个`MaxPooling2D`层之后加入了dropout：
- en: '[PRE100]'
  id: totrans-450
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'We then normalize our data, and compile and train our model:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对数据进行归一化，并编译和训练模型：
- en: '[PRE101]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: This will give us a model accuracy of about 85%.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们的模型准确率达到大约85%。
- en: 'Step 2: Running inference and evaluating our standard model'
  id: totrans-454
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第二步：运行推理并评估我们的标准模型
- en: 'Now that we have trained our model, let’s see how much protection it offers
    against an adversarial attack. In [*Chapter 3*](CH3.xhtml#x1-350003), [*Fundamentals
    of Deep Learning*](CH3.xhtml#x1-350003), we created an adversarial attack from
    scratch. In this chapter, we’ll use the `cleverhans` library to create the same
    attack in one line for multiple images at once:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练好了我们的模型，让我们看看它对抗对抗攻击的保护效果有多好。在[*第3章*](CH3.xhtml#x1-350003)[*深度学习基础*](CH3.xhtml#x1-350003)中，我们从头开始创建了一个对抗攻击。在本章中，我们将使用`cleverhans`库来为多个图像一次性创建相同的攻击：
- en: '[PRE102]'
  id: totrans-456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'Let’s first measure the accuracy of our deterministic model on the original
    images and the adversarial images:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们衡量我们确定性模型在原始图像和对抗图像上的准确率：
- en: '[PRE103]'
  id: totrans-458
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Now that we have our predictions, we can print the accuracy:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的预测结果，我们可以打印出准确率：
- en: '[PRE104]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'We can see that our standard model offers little protection against this adversarial
    attack. Although it performs pretty well on standard images, it has an accuracy
    of 30.70% on adversarial images! Let’s see if a Bayesian model can do better.
    Because we trained our model with dropout, we can easily make it an MC dropout
    model. We create an inference function where we keep dropout at inference, as
    indicated by the `training=True` parameter:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，我们的标准模型对这种对抗攻击几乎没有提供任何保护。尽管它在标准图像上的表现相当不错，但它在对抗图像上的准确率仅为30.70%！让我们看看一个贝叶斯模型能否做得更好。因为我们训练了带dropout的模型，我们可以很容易地将其转变为MC
    dropout模型。我们创建一个推理函数，在推理过程中保持dropout，如`training=True`参数所示：
- en: '[PRE105]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'With this function in place, we can replace the standard loop with MC dropout
    inference. We keep track of all our predictions again and run inference on our
    standard images and the adversarial images:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个函数，我们可以用MC dropout推理替代标准的循环。我们再次跟踪所有的预测，并对标准图像和对抗图像进行推理：
- en: '[PRE106]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'And we can again print our accuracy:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次打印出我们的准确率：
- en: '[PRE107]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: We can see that our simple modification made the model setup much more robust
    to adversarial examples. Instead of an accuracy of around 30%, we now obtain accuracy
    of more than 80%, pretty close to the accuracy of 83% of the deterministic model
    on the non-perturbed images. Moreover, we can see that MC dropout also improves
    the accuracy on our standard images by a few percentage points, from 83% to 86%.
    Almost no method offers perfect robustness to adversarial examples, so the fact
    that we can get so close to our model’s standard accuracy is a great achievement.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，简单的修改使得模型设置在面对对抗样本时更加稳健。准确率从约30%提高到了80%以上，接近于确定性模型在未扰动图像上的83%的准确率。此外，我们还可以看到，MC
    dropout也使得我们的标准图像准确率提高了几个百分点，从83%提升到了86%。几乎没有任何方法能够完美地对抗对抗样本，因此能够接近我们模型在标准图像上的准确率是一个伟大的成就。
- en: 'Because our model has not seen the adversarial images before, a model with
    good uncertainty values should also have a lower confidence on average on the
    adversarial images compared to a standard model. Let’s see if this is the case.
    We create a function to compute the average softmax value of the predictions of
    our deterministic model and create a similar function for our MC dropout predictions:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的模型之前没有见过对抗图像，所以一个具有良好不确定性值的模型应该在对抗图像上相对于标准模型表现出更低的平均信心。让我们看看是否是这样。我们创建一个函数来计算我们确定性模型预测的平均softmax值，并为MC
    dropout预测创建一个类似的函数：
- en: '[PRE108]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'We can then print the mean softmax score for both models:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以打印出两个模型的平均softmax分数：
- en: '[PRE109]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: We can see that our standard model is actually slightly more confident on adversarial
    images compared to standard images, although its accuracy dropped significantly.
    However, our MC dropout model shows a lower confidence on the adversarial images
    compared to the standard images. Although the drop in confidence is not very large,
    it is good to see that the model is dropping its mean confidence on adversarial
    images, while keeping a reasonable accuracy.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，与标准图像相比，我们的标准模型在对抗图像上的信心实际上稍微更高，尽管准确率显著下降。然而，我们的MC dropout模型在对抗图像上的信心低于标准图像。虽然信心的下降幅度不大，但我们很高兴看到，尽管准确率保持合理，模型在对抗图像上的平均信心下降了。
- en: 8.7 Summary
  id: totrans-473
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 总结
- en: In this chapter, we have illustrated the various applications of modern BDL
    in five different case studies. Each case study used code examples to highlight
    a particular strength of BDL in response to various, common problems in applied
    machine learning practice. First, we saw how BDL can be used to detect out-of-distribution
    images in a classification task. We then looked at how BDL methods can be used
    to make models more robust to dataset shift, which is a very common problem in
    production environments. Next, we learned how BDL can help us to select the most
    informative data points for training and updating our machine learning models.
    We then turned to reinforcement learning and saw how BDL can be used to facilitate
    more cautious behaviour in reinforcement learning agents. Finally, we saw how
    BDL can help us in the face of adversarial attacks.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过五个不同的案例研究展示了现代BDL的各种应用。每个案例研究都使用了代码示例，突出了BDL在应对应用机器学习实践中的各种常见问题时的特定优势。首先，我们看到如何使用BDL在分类任务中检测分布外图像。接着，我们探讨了BDL方法如何用于使模型更加鲁棒，以应对数据集偏移，这是生产环境中一个非常常见的问题。然后，我们学习了BDL如何帮助我们选择最有信息量的数据点，以训练和更新我们的机器学习模型。接着，我们转向强化学习，看到BDL如何帮助强化学习代理实现更加谨慎的行为。最后，我们看到了BDL在面对对抗性攻击时的应用。
- en: In the next chapter, we will have a look at the future of BDL by reviewing current
    trends and the latest methods.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过回顾当前趋势和最新方法来展望BDL的未来。
- en: 8.8 Further reading
  id: totrans-476
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.8 进一步阅读
- en: 'The following reading list will offer a greater understanding of some of the
    topics we touched on in this chapter:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 以下阅读清单将帮助你更好地理解我们在本章中涉及的一些主题：
- en: '*Benchmarking neural network robustness to common corruptions and* *perturbations*,
    Dan Hendrycks and Thomas Dietterich, 2019: this is the paper that introduced the
    image quality perturbations to benchmark model robustness, which we saw in the
    robustness case study.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基准测试神经网络对常见损坏和* *扰动*的鲁棒性，Dan Hendrycks 和 Thomas Dietterich，2019年：这篇论文介绍了图像质量扰动，以基准测试模型的鲁棒性，正如我们在鲁棒性案例研究中看到的那样。'
- en: '*Can You Trust Your Model’s Uncertainty? Evaluating predictive* *Uncertainty
    Under Dataset Shift*, Yaniv Ovadia, Emily Fertig *et* *al.*, 2019: this comparison
    paper uses image quality perturbations to introduce artificial dataset shift at
    different severity levels and measures how different deep neural networks respond
    to dataset shift in terms of accuracy and calibration.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你能信任模型的不确定性吗？评估数据集偏移下的* *预测不确定性*，Yaniv Ovadia、Emily Fertig 等，2019年：这篇比较论文使用图像质量扰动，在不同的严重程度下引入人工数据集偏移，并衡量不同的深度神经网络在准确性和校准方面如何响应数据集偏移。'
- en: '*A Baseline for Detecting Misclassified and Out-of-Distribution* *Examples
    in Neural Networks*, Dan Hendrycks and Kevin Gimpel, 2016: this fundamental out-of-distribution
    detection paper introduces the concept and shows that softmax values are not perfect
    when it comes to OOD detection.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于检测神经网络中误分类和分布外* *样本的基准*，Dan Hendrycks 和 Kevin Gimpel，2016年：这篇基础性的分布外检测论文介绍了该概念，并表明当涉及到分布外（OOD）检测时，softmax值并不完美。'
- en: '*Enhancing The Reliability of Out-of-distribution Image Detection in* *Neural
    Networks*, Shiyu Liang, Yixuan Li and R. Srikant, 2017: shows that input perturbation
    and temperature scaling can improve the softmax baseline for out-of-distribution
    detection.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提高神经网络中分布外图像检测的可靠性*，Shiyu Liang、Yixuan Li 和 R. Srikant，2017年：表明输入扰动和温度缩放可以改善用于分布外检测的softmax基准。'
- en: '*A Simple Unified Framework for Detecting Out-of-Distribution* *Samples and
    Adversarial Attacks*, Kimin Lee, Kibok Lee, Honglak Lee and Jinwoo Shin, 2018:
    shows that using the Mahalanobis distance can be effective for out-of-distribution
    detection.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于检测分布外* *样本和对抗性攻击的简单统一框架*，Kimin Lee、Kibok Lee、Honglak Lee 和 Jinwoo Shin，2018年：表明使用马哈拉诺比斯距离在分布外检测中可能是有效的。'
