- en: The Landscape of Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的概览
- en: Humans and animals learn through a process of trial and error. This process
    is based on our reward mechanisms that provide a response to our behaviors. The
    goal of this process is to, through multiple repetitions, incentivize the repetition
    of actions which trigger positive responses, and disincentivize the repetition
    of actions which trigger negative ones. Through the trial and error mechanism,
    we learn to interact with the people and world around us, and pursue complex,
    meaningful goals, rather than immediate gratification.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 人类和动物通过试验和错误的过程进行学习。这个过程基于我们的奖励机制，对我们的行为作出反应。这个过程的目标是通过多次重复，激励那些引发正向反应的行为的重复，并减少那些引发负向反应的行为的重复。通过试验和错误的机制，我们学会与周围的人和世界互动，追求复杂而有意义的目标，而不是即时的满足感。
- en: Learning through interaction and experience is essential. Imagine having to
    learn to play football by only looking at other people playing it. If you took
    to the field to play a football match based on this learning experience, you would
    probably perform incredibly poorly.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通过互动和经验学习至关重要。试想一下，如果你只能通过观看别人踢足球来学习，结果会怎样？如果你基于这种学习方式去参加一场足球比赛，你可能表现得非常差劲。
- en: This was demonstrated throughout the mid-20th century, notably by Richard Held and
    Alan Hein's 1963 study on two kittens, both of whom were raised on a carousel.
    One kitten was able to move freely (actively), whilst the other was restrained
    and moved following the active kitten (passively). Upon both kittens being introduced
    to light, only the kitten who was able to move actively developed a functioning
    depth perception and motor skills, whilst the passive kitten did not. This was
    notably demonstrated by the absence of the passive kitten's blink-reflex towards
    incoming objects. What this, rather crude experiment demonstrated is that regardless
    of visual deprivation, physical interaction with the environment is necessary
    in order for animals to learn.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点在20世纪中期得到了验证，特别是理查德·赫尔德和艾伦·海因1963年关于两只小猫的研究。两只小猫都在旋转木马上成长。其中一只小猫能够自由活动（主动），而另一只则被限制，只能被动地跟随主动小猫的动作。当两只小猫都被引入光线时，只有能够主动移动的小猫发展出了正常的深度感知和运动技能，而被动的小猫没有。最明显的表现是，被动小猫对接近物体的眨眼反应缺失。这项相当简单的实验表明，无论是否存在视觉剥夺，动物与环境的身体互动对于学习来说是必要的。
- en: Inspired by how animals and humans learn, **reinforcement learning** (**RL**)
    is built around the idea of trial and error from active interactions with the
    environment. In particular, with RL, an agent learns incrementally as it interacts
    with the world. In this way, it's possible to train a computer to learn and behave
    in a rudimentary, yet similar way to how humans do.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 受到动物和人类学习方式的启发，**强化学习**（**RL**）围绕从与环境的主动互动中进行试验和错误的理念构建。具体来说，在强化学习中，智能体在与世界互动的过程中逐步学习。通过这种方式，就可以训练计算机以类似人类的方式，尽管较为初级，但仍能学习和行为。
- en: This book is all about reinforcement learning. The intent of the book is to
    give you the best possible understanding of this field with a hands-on approach.
    In the first chapters, you'll start by learning the most fundamental concepts
    of reinforcement learning. As you grasp these concepts, we'll start developing
    our first reinforcement learning algorithms. Then, as the book progress, you'll
    create more powerful and complex algorithms to solve more interesting and compelling
    problems. You'll see that reinforcement learning is very broad and that there
    exist many algorithms that tackle a variety of problems in different ways. Nevertheless,
    we'll do our best to provide you with a simple but complete description of all
    the ideas, alongside a clear and practical implementation of the algorithms.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本书完全关于强化学习。本书的目的是通过实践方法，给你提供对这一领域最好的理解。在前几章，你将从学习强化学习的最基本概念开始。随着你掌握这些概念，我们将开始开发我们第一个强化学习算法。随着书本内容的深入，你将创建更强大、更复杂的算法来解决更有趣、更具吸引力的问题。你会看到，强化学习非常广泛，存在许多算法以不同的方式解决各种问题。尽管如此，我们仍会尽力为你提供一个简单但完整的描述，伴随清晰且实用的算法实现。
- en: To start with, in this chapter, you'll familiarize yourself with the fundamental
    concepts of RL, the distinctions between different approaches, and the key concepts
    of policy, value function, reward, and model of the environment. You'll also learn
    about the history and applications of RL.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先让你熟悉强化学习的基本概念，了解不同方法之间的区别，以及策略、价值函数、奖励和环境模型等关键概念。你还将了解强化学习的历史和应用。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: An introduction to RL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习简介
- en: Elements of RL
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的要素
- en: Applications of RL
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的应用
- en: An introduction to RL
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习简介
- en: 'RL is an area of machine learning that deals with sequential decision-making,
    aimed at reaching a desired goal. An RL problem is constituted by a decision-maker
    called an **A****gent **and the physical or virtual world in which the agent interacts,
    is known as the **Environment**. The agent interacts with the environment in the
    form of **Action**which results in an effect. As a result, the environment will feedback
    to the agent a new **State** and **Reward**. These two signals are the consequences
    of the action taken by the agent. In particular, the reward is a value indicating
    how good or bad the action was, and the state is the current representation of
    the agent and the environment. This cycle is shown in the following diagram:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是机器学习的一个领域，处理顺序决策问题，旨在实现预定目标。一个强化学习问题由一个决策者组成，称为**代理（Agent）**，以及代理所互动的物理或虚拟世界，称为**环境（Environment）**。代理通过**动作（Action）**与环境互动，从而产生效果。结果，环境会反馈给代理一个新的**状态（State）**和**奖励（Reward）**。这两个信号是代理采取的动作的后果。特别地，奖励是表示动作好坏的一个值，状态则是代理和环境的当前表现。这个循环在下图中展示：
- en: '![](img/4f7a35ed-10cb-4cd7-9994-52fd02ed12a5.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4f7a35ed-10cb-4cd7-9994-52fd02ed12a5.png)'
- en: In this diagram the agent is represented by PacMan that based on the current
    state of the environment, choose which action to take. Its behavior will influence
    the environment, like its position and that of the enemies, that will be returned
    by the environment in the form of a new state and the reward. This cycle is repeated
    until the game ends.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在此图中，代理通过PacMan表示，基于当前环境状态，选择采取哪种行动。它的行为会影响环境，例如它的位置和敌人的位置，这些都将由环境反馈回来，形成新的状态和奖励。这个循环会一直持续，直到游戏结束。
- en: The ultimate goal of the agent is to maximize the total reward accumulated during
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 代理的最终目标是最大化整个过程中的总奖励。
- en: its lifetime. Let's simplify the notation: if ![](img/a33cc177-3cb5-46ba-b5a9-8b22075eff34.png) is the
    action at time ![](img/ab6033a2-bf94-4795-8577-68135a1957b1.png) and ![](img/b3f9f893-8b28-4111-9d2a-8a44cd6cfa43.png) is the reward
    at time ![](img/06acf4d6-c009-4025-8503-3fbd4ba6f3dc.png), then the agent will
    take actions ![](img/443d6aec-83c1-44a1-bffc-4b1a082b8f27.png), to maximize the
    sum of all rewards ![](img/6328a359-9251-4ef9-a5c7-5a45d1b0ae8b.png).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 它的生命周期。我们简化符号表示：如果 ![](img/a33cc177-3cb5-46ba-b5a9-8b22075eff34.png) 是时间 ![](img/ab6033a2-bf94-4795-8577-68135a1957b1.png) 的动作，且 ![](img/b3f9f893-8b28-4111-9d2a-8a44cd6cfa43.png) 是时间 ![](img/06acf4d6-c009-4025-8503-3fbd4ba6f3dc.png) 的奖励，那么代理将采取 ![](img/443d6aec-83c1-44a1-bffc-4b1a082b8f27.png) 动作，以最大化所有奖励 ![](img/6328a359-9251-4ef9-a5c7-5a45d1b0ae8b.png) 的总和。
- en: To maximize the cumulative reward, the agent has to learn the best behavior
    in every situation. To do so, the agent has to optimize for a long-term horizon
    while taking care of every single action. In environments with many discrete or
    continuous states and actions, learning is difficult because the agent should
    be accountable for each situation. To make the problem harder, RL can have very
    sparse and delayed rewards, making the learning process more arduous.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化累积奖励，代理必须在每种情况下学习最佳行为。为此，代理必须在考虑每一个动作的同时，优化长期目标。在具有许多离散或连续状态和动作的环境中，学习非常困难，因为代理需要对每种情况负责。更为复杂的是，强化学习可能包含非常稀疏和延迟的奖励，令学习过程更加艰难。
- en: To give an example of an RL problem while explaining the complexity of a sparse
    reward, consider the well-known story of two siblings, Hansel and Gretel. Their
    parents led them into the forest to abandon them, but Hansel, who knew of their
    intentions, had taken a slice of bread with him when they left the house and managed
    to leave a trail of breadcrumbs that would lead him and his sister home. In the
    RL framework, the agents are Hansel and Gretel, and the environment is the forest.
    A reward of +1 is obtained for every crumb of bread reached and a reward of +10
    is acquired when they reach home. In this case, the denser the trail of bread,
    the easier it will be for the siblings to find their way home. This is because
    to go from one piece of bread to another, they have to explore a smaller area.
    Unfortunately, sparse rewards are far more common than dense rewards in the real
    world.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 举一个强化学习问题的例子，并解释稀疏奖励的复杂性，可以参考著名的故事《汉塞尔与格蕾特》。故事中，父母将两个孩子带入森林准备抛弃他们，但汉塞尔知道父母的意图，于是他在离开家时带上了一片面包，成功地留下了面包屑的踪迹，以便带领他和妹妹找到回家的路。在强化学习框架中，代理人是汉塞尔和格蕾特，环境是森林。每当他们找到一片面包屑时，就能获得+1的奖励，当他们回到家时，获得+10的奖励。在这种情况下，面包屑的轨迹越密集，兄妹俩找到回家的路就越容易。这是因为从一片面包屑走到另一片面包屑，他们需要探索的区域更小。不幸的是，在现实世界中，稀疏奖励比密集奖励要普遍得多。
- en: 'An important characteristic of RL is that it can deal with environments that
    are dynamic, uncertain, and non-deterministic. These qualities are essential for
    the adoption of RL in the real world. The following points are examples of how
    real-world problems can be reframed in RL settings:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）的一个重要特点是它能够应对动态、不确定和非确定性的环境。这些特性对于强化学习在现实世界中的应用至关重要。以下几点是如何将现实世界的问题转化为强化学习环境的示例：
- en: Self-driving cars are a popular, yet difficult, concept to approach with RL.
    This is because of the many aspects to be taken into consideration while driving
    on the road (such as pedestrians, other cars, bikes, and traffic lights) and the
    highly uncertain environment. In this case, the self-driving car is the agent
    that can act on the steering wheel, accelerator, and brakes. The environment is
    the world around it. Obviously, the agent cannot be aware of the whole world around
    it, as it can only capture limited information via its sensors (for example, the
    camera, radar, and GPS). The goal of the self-driving car is to reach the destination
    in the minimum amount of time while following the rules of the road and without
    damaging anything. Consequently, the agent can receive a negative reward if a
    negative event occurs and a positive reward can be received in proportion to the
    driving time when the agent reaches its destination.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动驾驶汽车是一个流行但难以用强化学习来解决的概念。这是因为在驾驶过程中需要考虑许多因素（如行人、其他汽车、自行车和交通信号灯）以及高度不确定的环境。在这种情况下，自动驾驶汽车是代理人，可以操作方向盘、加速器和刹车。环境是它周围的世界。显然，代理人无法意识到周围整个世界的情况，因为它只能通过传感器（例如相机、雷达和GPS）捕获有限的信息。自动驾驶汽车的目标是以最短的时间到达目的地，同时遵守交通规则，避免损坏任何物品。因此，如果发生负面事件，代理人可能会收到负奖励，而在代理人到达目的地时，可以根据行驶时间获得正奖励。
- en: In the game of chess, the goal is to checkmate the opponent's piece. In an RL
    framework, the player is the agent and the environment is the current state of
    the board. The agent is allowed to move the game pieces according to their own
    way of moving. As a result of an action, the environment returns a positive or
    negative reward corresponding to a win or a loss for the agent. In all other situations,
    the reward is 0 and the next state is the state of the board after the opponent
    has moved. Unlike the self-driving car example, here, the environment state equals
    the agent state. In other words, the agent has a perfect view of the environment.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在国际象棋游戏中，目标是将对方的棋子将死。在强化学习框架中，玩家是代理人，环境是棋盘的当前状态。代理人可以根据自己棋子的移动方式来移动棋子。由于某个动作，环境会返回一个与胜负相对应的正面或负面奖励。在其他情况下，奖励为0，下一个状态是对手移动后的棋盘状态。与自动驾驶汽车的例子不同，这里环境状态等同于代理人状态。换句话说，代理人对环境有着完美的视角。
- en: Comparing RL and supervised learning
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较强化学习和监督学习
- en: RL and **supervised learning** are similar, yet different, paradigms to learn
    from data. Many problems can be tackled with both supervised learning and RL;
    however, in most cases, they are suited to solve different tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习和**监督学习**是两种相似但不同的数据学习范式。许多问题可以通过监督学习和强化学习来解决；然而，在大多数情况下，它们适用于解决不同的任务。
- en: Supervised learning learns to generalize from a fixed dataset with a limited
    amount of data consisting of examples. Each example is composed of the input and
    the desired output (or label) that provides immediate learning feedback.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习通过一个固定的数据集进行泛化学习，数据集由包含示例的数据组成。每个示例由输入和期望的输出（或标签）组成，标签提供了即时的学习反馈。
- en: In comparison, RL is more focused on sequential actions that you can take in
    a particular situation. In this case, the only supervision provided is the reward
    signal. There's no correct action to take in a circumstance, as in the supervised
    settings.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，强化学习更加关注在特定情境下可以采取的顺序性动作。在这种情况下，唯一的监督来自奖励信号。在这种情境下，没有像监督学习那样的正确动作可供选择。
- en: 'RL can be viewed as a more general and complete framework for learning. The
    major characteristics that are unique to RL are as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）可以被视为一种更通用、更完整的学习框架。RL独特的主要特征如下：
- en: The reward could be dense, sparse, or very delayed. In many cases, the reward
    is obtained only at the end of the task (for example, in the game of chess).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励可能是密集的、稀疏的或非常延迟的。在许多情况下，奖励仅在任务结束时获得（例如，在国际象棋游戏中）。
- en: The problem is sequential and time-dependent; actions will affect the next actions,
    which, in turn, influence the possible rewards and states.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个问题是顺序性和时间依赖性的；动作会影响下一步动作，而这些动作又会影响可能的奖励和状态。
- en: An agent has to take actions with a higher potential to achieve a goal (exploitation),
    but it should also try different actions to ensure that other parts of the environment
    are explored (exploration). This problem is called the exploration-exploitation
    dilemma (or exploration-exploitation trade-off) and it manages the difficult task
    of balancing between the exploration and exploitation of the environment. This
    is also very important because, unlike supervised learning, RL can influence the
    environment since it is free to collect new data as long as it deems it useful.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个智能体必须采取具有更高潜力实现目标的行动（利用），但它也应该尝试不同的行动，确保探索环境的其他部分（探索）。这个问题被称为探索-利用困境（或探索-利用权衡），它管理着在环境的探索与利用之间的平衡。这一点也非常重要，因为与监督学习不同，强化学习可以影响环境，因为它可以自由地收集新的数据，只要它认为这些数据有用。
- en: The environment is stochastic and nondeterministic, and the agent has to take
    this into consideration when learning and predicting the next action. In fact,
    we'll see that many of the RL components can be designed to either output a single
    deterministic value or a range of values along with their probability.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境是随机的和非确定性的，智能体必须在学习和预测下一步动作时考虑到这一点。事实上，我们将看到，许多强化学习的组件可以设计成输出一个单一的确定性值或一系列值及其概率。
- en: The third type of learning is **unsupervised learning**, and this is used to
    identify patterns in data without giving any supervised information. Data compression,
    clustering, and generative models are examples of unsupervised learning. It can
    also be adopted in RL settings in order to explore and learn about the environment.
    The combination of unsupervised learning and RL is called **unsupervised RL**.
    In this case, no reward is given and the agent could generate an intrinsic motivation
    to favor new situations where they can explore the environment.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种学习类型是**无监督学习**，用于在没有任何监督信息的情况下识别数据中的模式。数据压缩、聚类和生成模型是无监督学习的例子。它也可以在强化学习环境中使用，以便探索并了解环境。无监督学习与强化学习的结合被称为**无监督强化学习**。在这种情况下，没有奖励给出，智能体可能会产生内在动机，偏向于探索新的情境，在这些情境中它们能够探索环境。
- en: It's worth noting that the problems associated with self-driving cars have also been addressed
    as a supervised learning problem, but with poor results. The main problem is derived
    from a different distribution of data that the agent would encounter during its
    lifetime compared to that used during training.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，自动驾驶汽车相关的问题也曾被当作监督学习问题来解决，但结果较差。主要问题源于在训练过程中使用的数据与智能体在其生命周期中遇到的数据分布不同。
- en: History of RL
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的历史
- en: The first mathematical foundation of RL was built during the 1960s and 1970s
    in the field of optimal control. This solved the problem of minimizing a behavior's
    measure of a dynamic system over time. The method involved solving a set of equations
    with the known dynamics of the system. During this time, the key concept of a **Markov
    decision process** (**MDP**) was introduced. This provides a general framework
    for modeling decision-making in stochastic situations. During these years, a solution
    method for optimal control called **dynamic programming** (**DP**) was introduced.
    DP is a method that breaks down a complex problem into a collection of simpler
    subproblems for solving an MDP.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）的第一个数学基础是在1960年代和1970年代的最优控制领域中建立的。这解决了动态系统随时间变化的行为度量最小化问题。该方法涉及求解一组已知动态系统的方程。在这一时期，**马尔可夫决策过程**（**MDP**）这一关键概念被提出。它为在随机情境中建模决策提供了一个通用框架。在这些年里，一种称为**动态规划**（**DP**）的最优控制求解方法被提出。DP是一种将复杂问题分解成一系列简单子问题来求解MDP的方法。
- en: Note that DP only provides an easier way to solve optimal control for systems
    with known dynamics; there is no learning involved. It also suffers from the problem
    of the **curse of dimensionality **because the computational requirements grow
    exponentially with the number of states.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，DP仅为已知动态系统提供了一种更简单的解决最优控制的方法；其中并不涉及学习。它还存在**维度灾难**的问题，因为计算需求会随着状态数量的增加呈指数增长。
- en: Even if these methods don't involve learning, as noted by Richard S. Sutton
    and Andrew G. Barto, we must consider the solution methods of optimal control,
    such as DP, to also be RL methods.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这些方法不涉及学习，正如理查德·S·萨顿和安德鲁·G·巴托所指出的，我们仍然必须将最优控制的求解方法，如DP，也视为强化学习方法。
- en: In the 1980s, the concept of learning by temporally successive predictions—the
    so-called **temporal difference learning** (**TD learning**) method—was finally
    introduced. TD learning introduced a new family of powerful algorithms that will
    be explained in this book.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在1980年代，通过时间上连续的预测进行学习的概念——即所谓的**时间差学习**（**TD学习**）方法——最终被提出。TD学习引入了一类强大的新算法，这些算法将在本书中进行解释。
- en: The first problems solved with TD learning are small enough to be represented
    in tables or arrays. These methods are called **tabular methods**, which are often
    found as an optimal solution but are not scalable. In fact, many RL tasks involve
    huge state spaces, making tabular methods impossible to adopt. In these problems,
    function approximations are used to find a good approximate solution with less
    computational resources.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TD学习解决的第一个问题足够小，可以通过表格或数组表示。这些方法被称为**表格方法**，通常作为最优解出现，但不具备可扩展性。事实上，许多强化学习任务涉及庞大的状态空间，使得表格方法无法采用。在这些问题中，使用函数逼近来找到一个近似解，从而减少计算资源的消耗。
- en: The adoption of function approximations and, in particular, of artificial neural
    networks (and deep neural networks) in RL is not trivial; however, as shown on
    many occasions, they are able to achieve amazing results. The use of deep learning
    in RL is called **deep reinforcement learning** (**deep RL**) and it has achieved
    great popularity ever since a deep RL algorithm named **deep q network** (**DQN**)
    displayed a superhuman ability to play Atari games from raw images in 2015\. Another
    striking achievement of deep RL was with AlphaGo in 2017, which became the first
    program to beat Lee Sedol, a human professional Go player, and 18-time world champion.
    These breakthroughs not only showed that machines can perform better than humans
    in high-dimensional spaces (using the same perception as humans with respect to
    images), but also that they can behave in interesting ways. An example of this
    is the creative shortcut found by a deep RL system while playing Breakout, an
    Atari arcade game in which the player has to destroy all the bricks, as shown
    in the following image. The agent found that just by creating a tunnel on the
    left-hand side of the bricks and by putting the ball in that direction, it could
    destroy much more bricks and thus increase its overall score with just one move.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中采用函数逼近，尤其是人工神经网络（包括深度神经网络），并非易事；然而，正如许多场合所展示的那样，它们能够取得惊人的结果。深度学习在强化学习中的应用被称为**深度强化学习**（**深度
    RL**），自从2015年深度强化学习算法**深度 Q 网络**（**DQN**）展现出超越人类的能力，在原始图像上玩 Atari 游戏以来，它取得了巨大的普及。深度强化学习的另一个显著成就发生在2017年，当时
    AlphaGo 成为了第一个击败李世石——这位人类职业围棋选手及18届世界冠军——的程序。这些突破不仅表明机器可以在高维空间中表现得比人类更好（在人类对于图像的感知方式上），而且它们还能够以有趣的方式进行行为。例如，在玩
    Breakout（一个 Atari 街机游戏，玩家需要摧毁所有砖块）时，深度强化学习系统发现了一个创造性的捷径，系统发现只要在砖块的左侧打通一个隧道，并将球引导到那个方向，就可以摧毁更多砖块，从而通过一次动作提高总体得分。如下图所示：
- en: '![](img/4fda7cc8-8734-4f8d-9fcf-15cf30d8cf3c.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4fda7cc8-8734-4f8d-9fcf-15cf30d8cf3c.jpg)'
- en: There are many other interesting cases where the agents exhibit superb behavior
    or strategies that weren't known to humans, like a move performed by AlphaGo while
    playing Go against Lee Sedol. From a human perspective, that move seemed nonsense
    but ultimately allowed AlphaGo to win the game (the move is called **move 37**).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他有趣的案例，其中智能体展示了卓越的行为或策略，这些行为或策略人类之前并未掌握，例如 AlphaGo 在与李世石对弈时所执行的一步棋。从人类角度来看，那一步棋似乎毫无意义，但最终帮助
    AlphaGo 获得了胜利（这一步棋被称为**37号棋**）。
- en: Nowadays, when dealing with high-dimensional state or action spaces, the use
    of deep neural networks as function approximations becomes almost a default choice.
    Deep RL has been applied to more challenging problems, such as data center energy
    optimization, self-driving cars, multi-period portfolio optimization, and robotics,
    just to name a few.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，在处理高维状态或动作空间时，深度神经网络作为函数逼近的使用几乎已成为默认选择。深度强化学习已被应用于更具挑战性的问题，例如数据中心能源优化、自动驾驶汽车、多期投资组合优化和机器人技术，仅举几例。
- en: Deep RL
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习（Deep RL）
- en: 'Now you could ask yourself—why can deep learning combined with RL perform so
    well? Well, the main answer is that deep learning can tackle problems with a high-dimensional
    state space. Before the advent of deep RL, state spaces had to break down into
    simpler representations, called **features**. These were difficult to design and,
    in some cases, only an expert could do it. Now, using deep neural networks such
    as a **convolutional neural network** (**CNN**) or a **recurrent neural network**
    (**RNN**), RL can learn different levels of abstraction directly from raw pixels
    or sequential data (such as natural language). This configuration is shown in
    the following diagram:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可能会问自己——为什么深度学习与强化学习结合能表现得如此出色？主要原因是深度学习能够处理具有高维状态空间的问题。在深度强化学习出现之前，状态空间必须分解为更简单的表示，称为**特征**。这些特征很难设计，在某些情况下，只有专家才能做到。现在，使用深度神经网络，如**卷积神经网络**（**CNN**）或**递归神经网络**（**RNN**），强化学习可以直接从原始像素或序列数据（如自然语言）中学习不同层次的抽象。这种配置如下图所示：
- en: '![](img/40d4f878-38b9-450a-b79d-0764888303da.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/40d4f878-38b9-450a-b79d-0764888303da.png)'
- en: 'Furthermore, deep RL problems can now be solved completely in an end-to-end
    fashion. Before the deep learning era, an RL algorithm involved two distinct pipelines:
    one to deal with the perception of the system and one to be responsible for the
    decision-making. Now, with deep RL algorithms, these processes are joined and
    are trained end-to-end, from the raw pixels straight to the action. For example,
    as shown in the preceding diagram, it''s possible to train Pacman end-to-end using
    a CNN to process the visual component and a **fully connected neural network**
    (**FNN**) to translate the output of the CNN into an action.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度强化学习问题现在可以完全通过端到端的方式来解决。在深度学习时代之前，强化学习算法涉及两个不同的流程：一个用于处理系统的感知，另一个用于负责决策制定。而现在，借助深度强化学习算法，这些过程已被整合并通过端到端训练，从原始像素直接到行动。例如，如前所示的图表，通过使用卷积神经网络（CNN）处理视觉组件，并通过**全连接神经网络**（**FNN**）将CNN的输出转化为行动，可以端到端地训练吃豆人。
- en: Nowadays, deep RL is a very hot topic. The principal reason for this is that
    deep RL is thought to be the type of technology that will enable us to build highly
    intelligent machines. As proof, two of the more renowned AI companies that are
    working to solve intelligence problems, namely DeepMind and OpenAI, are heavily researching in RL.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，深度强化学习是一个非常热门的话题。其主要原因是，深度强化学习被认为是能够让我们构建高度智能机器的技术类型。作为证明，两个致力于解决智能问题的著名人工智能公司——DeepMind和OpenAI，正在深入研究强化学习。
- en: 'Besides the huge steps achieved with deep RL, there is a long way to go. There
    are many challenges that still need to be addressed, some of which are listed
    as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了深度强化学习取得的巨大进展外，仍然有很长的路要走。仍然存在许多需要解决的挑战，部分挑战列举如下：
- en: Deep RL is far too slow to learn compared to humans.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与人类相比，深度强化学习的学习速度过于缓慢。
- en: Transfer learning in RL is still an open problem.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习中的迁移学习仍然是一个未解的难题。
- en: The reward function is difficult to design and define.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励函数的设计和定义非常困难。
- en: RL agents struggle to learn in highly complex and dynamic environments such
    as the physical world.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习智能体在诸如物理世界等高度复杂和动态的环境中很难学习。
- en: Nonetheless, the research in this field is growing at a fast rate and companies
    are starting to adopt RL in their products.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，这一领域的研究正在迅速增长，越来越多的公司开始在他们的产品中采用强化学习。
- en: Elements of RL
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的元素
- en: As we know, an agent interacts with their environment by the means of actions.
    This will cause the environment to change and to feedback to the agent a reward
    that is proportional to the quality of the actions and the new state of the agent.
    Through trial and error, the agent incrementally learns the best action to take
    in every situation so that, in the long run, it will achieve a bigger cumulative
    reward. In the RL framework, the choice of the action in a particular state is
    done by a **policy**, and the cumulative reward that is achievable from that state
    is called the **value function**. In brief, if an agent wants to behave optimally,
    then in every situation, the policy has to select the action that will bring it
    to the next state with the highest value. Now, let's take a deeper look at these
    fundamental concepts.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，智能体通过行动与环境互动。这将导致环境发生变化，并反馈给智能体一个与行动质量和智能体新状态成比例的奖励。通过不断的试验和错误，智能体逐步学会在每种情况下采取最佳行动，从而在长期内获得更大的累积奖励。在强化学习框架中，特定状态下行动的选择由**策略**决定，而从该状态可以获得的累积奖励被称为**价值函数**。简而言之，如果智能体想要实现最优行为，那么在每种情况下，策略必须选择将其带到具有最高价值的下一个状态的行动。现在，让我们深入了解这些基本概念。
- en: Policy
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略
- en: The policy defines how the agent selects an action given a state. The policy
    chooses the action that maximizes the cumulative reward from that state, not with
    the bigger immediate reward. It takes care of looking for the long-term goal of
    the agent. For example, if a car has another 30 km to go before reaching its destination,
    but only has another 10 km of autonomy left and the next gas stations are 1 km
    and 60 km away, then the policy will choose to get fuel at the first gas station
    (1 km away) in order to not run out of gas. This decision is not optimal in the
    immediate future as it will take some time to refuel, but it will be sure to ultimately
    accomplish the goal.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 策略定义了在给定状态下，代理如何选择一个行动。策略选择最大化从该状态开始的累积奖励的行动，而不是选择即时奖励较大的行动。它关注的是代理的长期目标。例如，如果一辆车距离目的地还有30公里，但仅剩下10公里的续航，而下一个加油站分别在1公里和60公里远的地方，那么策略将选择在第一个加油站（1公里远）加油，以避免油量耗尽。这个决定在短期内并不最优，因为加油需要一些时间，但它最终会确保完成目标。
- en: 'The following diagram shows a simple example where an actor moving in a 4 x
    4 grid has to go toward the star while avoiding the spirals. The actions recommended
    by a policy are indicated by an arrow pointing in the direction of the move. The
    diagram on the left shows a random initial policy, while the diagram on the right
    shows the final optimal policy. In a situation with two equally optimal actions,
    the agent can arbitrarily chooses which action to take:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了一个简单的例子，其中一个在4x4网格中移动的行为者需要朝着星星移动，同时避免螺旋状的障碍物。策略推荐的行动由指向移动方向的箭头表示。左侧的图示展示了一个随机初始策略，而右侧的图示展示了最终的最优策略。在有两个同样最优的行动情况下，代理可以**任意**选择采取哪一个行动：
- en: '![](img/08b03523-bf5e-4da8-9b93-857fe59386ef.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08b03523-bf5e-4da8-9b93-857fe59386ef.png)'
- en: An important distinction is between stochastic policies and deterministic policies.
    In the deterministic case, the policy provides a single deterministic action to
    take. On the other hand, in the stochastic case, the policy provides a probability
    for each action. The concept of the probability of an action is useful because
    it takes into consideration the dynamicity of the environment and helps its exploration.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的区别是随机策略和确定性策略之间的差异。在确定性情况下，策略提供了一个确定性的行动。而在随机情况下，策略则为每个行动提供一个概率。行动概率的概念很有用，因为它考虑到了环境的动态性并有助于其探索。
- en: One way to classify RL algorithms is based on how policies are improved during
    learning. The simpler case is when the policy that acts on the environment is
    similar to the one that improves while learning. Another way to say this is that
    the policy learns from the same data that it generates. These algorithms are called
    **on-policy**. **Off-policy** algorithms, in comparison, involve two policies—one
    that acts on the environment and another that learns but is not actually used.
    The former is called the **behavior policy**, while the latter is called the **target
    policy**. The goal of the behavior policy is to interact with and collect information
    about the environment in order to improve the **passive** target policy. Off-policy
    algorithms, as we will see in the coming chapters, are more unstable and difficult
    to design than on-policy algorithms, but they are more sample efficient, meaning
    that they require less experience to learn.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一种对强化学习算法的分类方法是基于策略在学习过程中如何改进。较简单的情况是当作用于环境的策略与在学习过程中改进的策略相似。换句话说，策略从它自己生成的数据中学习。这些算法被称为**on-policy**。相比之下，**off-policy**算法涉及两种策略——一种作用于环境，另一种进行学习但实际上并未被使用。前者称为**行为策略**，后者称为**目标策略**。行为策略的目标是与环境互动并收集信息，以便改进**被动**的目标策略。正如我们将在接下来的章节中看到的那样，off-policy算法比on-policy算法更不稳定且难以设计，但它们更具样本效率，这意味着它们需要更少的经验来学习。
- en: To better understand these two concepts, we can think of someone who has to
    learn a new skill. If the person behaves as on-policy algorithms do, then every
    time they try a sequence of actions, they'll change their belief and behavior
    in accordance with the reward accumulated. In comparison, if the person behaves
    as an off-policy algorithm, they (the target policy) can also learn by looking
    at an old video of themselves (the behavior policy) doing the same skill—that
    is, they can use old experiences to help them to improve.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这两个概念，我们可以想象某人需要学习一项新技能。如果这个人的行为像在策略算法中那样，那么每当他们尝试一系列动作时，他们会根据积累的奖励改变自己的信念和行为。相比之下，如果这个人的行为像是脱离策略的算法，他们（目标策略）也可以通过观看自己以前做同一项技能的老视频（行为策略）来学习——也就是说，他们可以利用旧经验来帮助自己进步。
- en: The** policy-gradient method** is a family of RL algorithms that learns a parametrized
    policy (as a deep neural network) directly from the gradient of the performance
    with respect to the policy. These algorithms have many advantages, including the
    ability to deal with continuous actions and explore the environment with different
    levels of granularity. They will be presented in greater detail in [Chapter 6](6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml),
    *Learning Stochastic and PG Optimization*, [Chapter 7](4148a47d-a2c7-44da-ba6d-76ae4d8bec2c.xhtml),
    *TRPO and PPO Implementation*, and [Chapter 8](d902d278-a1f5-438a-9ddd-6d9d665f2fa2.xhtml),
    *DDPG and TD3 Applications*.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**策略梯度方法**是一类强化学习算法，它直接从性能相对于策略的梯度中学习一个参数化的策略（如深度神经网络）。这些算法有许多优点，包括能够处理连续动作和以不同粒度探索环境。它们将在[第6章](6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml)
    *学习随机优化与PG优化*、[第7章](4148a47d-a2c7-44da-ba6d-76ae4d8bec2c.xhtml) *TRPO与PPO实现*以及[第8章](d902d278-a1f5-438a-9ddd-6d9d665f2fa2.xhtml)
    *DDPG与TD3应用*中详细介绍。'
- en: The value function
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值函数
- en: The **value function** represents the long-term quality of a state. This is
    the cumulative reward that is expected in the future if the agent starts from
    a given state. If the reward measures the immediate performance, the value function
    measures the performance in the long run. This means that a high reward doesn't
    imply a high-value function and a low reward doesn't imply a low-value function.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**值函数**表示一个状态的长期质量。这是代理从某一给定状态开始时，预计在未来获得的累计奖励。如果奖励衡量的是即时表现，则值函数衡量的是长期表现。这意味着高奖励并不一定意味着高值函数，低奖励也不意味着低值函数。'
- en: 'Moreover, the value function can be a function of the state or of the state-action
    pair. The former case is called a **state-value function**, while the latter is
    called an **action-value function**:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，值函数可以是状态的函数，也可以是状态-动作对的函数。前者称为**状态值函数**，后者称为**动作值函数**：
- en: '![](img/7c4028df-0176-4d99-b893-fbb7866f37df.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c4028df-0176-4d99-b893-fbb7866f37df.png)'
- en: Here, the diagram shows the final state values (on the left side) and the corresponding
    optimal policy (on the right side).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，图表显示了最终状态值（左侧）和相应的最优策略（右侧）。
- en: Using the same gridworld example used to illustrate the concept of policy, we
    can show the state-value function. First of all, we can assume a reward of 0 in
    each situation except for when the agent reaches the star, gaining a reward of
    +1\. Moreover, let's assume that a strong wind moves the agent in another direction
    with a probability of 0.33\. In this case, the state values will be similar to
    those shown in the left-hand side of the preceding diagram. An optimal policy
    will choose the actions that will bring it to the next state with the highest
    state value, as shown in the right-hand side of the preceding diagram.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与策略概念说明相同的网格世界示例，我们可以展示状态值函数。首先，我们可以假设除了代理到达星星时获得+1奖励外，在其他情况下奖励为0。此外，假设强风以0.33的概率将代理吹向另一个方向。在这种情况下，状态值将类似于前图左侧所示的状态。最优策略将选择能够将代理带到下一个状态并具有最高状态值的动作，如前图右侧所示。
- en: '**Action-value methods** (or value-function methods) are the other big family
    of RL algorithms. These methods learn an action-value function and use it to choose
    the actions to take. Starting from [Chapter 3](f2414b11-976a-4410-92d8-89ee54745d99.xhtml),
    *Solving Problems with Dynamic Programming*, you''ll learn more about these algorithms.
    It''s worth noting that some policy-gradient methods, in order to combine the
    advantages of both methods, can also use a value function to learn the appropriate
    policy. These methods are called **actor-critic** **methods**. The following diagram
    shows the three main families of RL algorithms:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**动作价值方法**（或价值函数方法）是强化学习算法的另一大类。这些方法通过学习一个动作价值函数并利用它来选择要执行的动作。从[第3章](f2414b11-976a-4410-92d8-89ee54745d99.xhtml)
    *通过动态规划解决问题* 开始，你将进一步了解这些算法。值得注意的是，一些策略梯度方法为了结合两者的优点，也可以使用一个价值函数来学习合适的策略。这些方法被称为**演员-评论家**
    **方法**。下图展示了强化学习算法的三大类：'
- en: '![](img/faba0017-0735-4055-8eee-bc68e1ec30fe.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/faba0017-0735-4055-8eee-bc68e1ec30fe.png)'
- en: Reward
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励
- en: At each timestep, that is, after each move of the agent, the environment sends
    back a number that indicates how good that action was to the agent. This is called
    a **reward**. As we have already mentioned, the end goal of the agent is to maximize
    the cumulative reward obtained during their interaction with the environment.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，即每次智能体行动之后，环境会向智能体返回一个数字，表示该动作的好坏。这被称为**奖励**。正如我们已经提到的，智能体的最终目标是最大化在与环境互动过程中获得的累计奖励。
- en: In literature, the reward is assumed to be a part of the environment, but that's
    not strictly true in reality. The reward can come from the agent too, but never
    from the decision-making part of it. For this reason and to simplify the formulation,
    the reward is always sent from the environment.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，奖励被认为是环境的一部分，但实际上这并不完全准确。奖励也可以来自智能体，但永远不会来自决策部分。出于这个原因，并且为了简化公式，奖励总是由环境发送。
- en: The reward is the only supervision signal injected into the RL cycle and it
    is essential to design the reward in the correct way in order to obtain an agent
    with good behavior. If the reward has some flaws, the agent may find them and
    follow incorrect behavior. For example, *Coast Runners* is a boat-racing game
    with the goal being to finish ahead of other players. During the route, the boats
    are rewarded for hitting targets. Some folks at OpenAI trained an agent with RL
    to play it. They found that, instead of running to the finish line as fast as
    possible, the trained boat was driving in a circle to capture re-populating targets
    while crashing and catching fire. In this way, the boat found a way to maximize
    the total reward without acting as expected. This behavior was due to an incorrect
    balance between short-term and long-term rewards.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励是注入到强化学习循环中的唯一监督信号，因此设计正确的奖励机制对于获得行为良好的智能体至关重要。如果奖励存在缺陷，智能体可能会发现并跟随不正确的行为。例如，*海岸赛跑者*
    是一款目标是比其他玩家先到达终点的船赛游戏。在比赛过程中，船只会因击中目标而获得奖励。OpenAI的研究人员曾使用强化学习训练智能体来玩这款游戏。结果他们发现，训练后的船并没有尽可能快地驶向终点，而是绕圈行驶，捕捉重新出现的目标，同时撞击并着火。这样，船只找到了最大化总奖励的方法，但并没有按照预期的方式行动。这种行为源于短期奖励和长期奖励之间的不正确平衡。
- en: The reward can appear with different frequencies depending on the environment.
    A frequent reward is called a **dense reward**; however, if it is seen only a
    few times during a game, or only at its end, it is called a **sparse reward**.
    In the latter case, it could be very difficult for an agent to catch the reward
    and find the optimal actions.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励的出现频率可能会因环境而异。频繁出现的奖励被称为**密集奖励**；然而，如果奖励仅在游戏过程中出现几次，或只在游戏结束时出现，则称为**稀疏奖励**。在后一种情况下，智能体可能很难捕捉到奖励并找到最优的行动。
- en: '**Imitation learning** and **inverse RL** are two powerful techniques that
    deal with the absence of a reward in the environment. Imitation learning uses
    an expert demonstration to map states to actions. On the other hand, inverse RL
    deduces the reward function from an expert optimal behavior. Imitation learning
    and inverse RL will be studied in [Chapter 10](e61cf178-d8e8-47bf-ab79-ef546f23e309.xhtml),
    *Imitation Learning with the DAgger Algorithm*.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**模仿学习**和**逆向强化学习**是两种强大的技术，用于处理环境中缺乏奖励的情况。模仿学习利用专家演示将状态映射到动作上。另一方面，逆向强化学习通过专家的最优行为推导出奖励函数。模仿学习和逆向强化学习将在[第10章](e61cf178-d8e8-47bf-ab79-ef546f23e309.xhtml)
    *使用DAgger算法进行模仿学习* 中进行研究。'
- en: Model
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型
- en: The model is an optional component of the agent, meaning that it is not required
    in order to find a policy for the environment. The model details how the environment
    behaves, predicting the next state and the reward, given a state and an action.
    If the model is known, planning algorithms can be used to interact with the model
    and recommend future actions. For example, in environments with discrete actions,
    potential trajectories can be simulated using look ahead searches (for instance,
    using the Monte Carlo tree search).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是代理的一个可选组件，这意味着它不是寻找环境策略所必需的。模型详细描述了环境的行为，给定一个状态和一个动作，预测下一个状态和奖励。如果已知模型，可以使用规划算法与模型进行交互并推荐未来的动作。例如，在具有离散动作的环境中，可以使用前瞻搜索（例如，蒙特卡洛树搜索）来模拟潜在的轨迹。
- en: The model of the environment could either be given in advance or learned through
    interactions with it. If the environment is complex, it's a good idea to approximate
    it using deep neural networks. RL algorithms that use an already known model of
    the environment, or learn one, are called **model-based methods**. These solutions
    are opposed to model-free methods and will be explained in more detail in [Chapter
    9](7e8448cf-7b74-4f07-99bd-da8f98f4505c.xhtml), *Model-Based RL*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 环境的模型可以提前给定，或者通过与环境的交互来学习。如果环境复杂，使用深度神经网络来近似它是个不错的选择。使用已知环境模型或通过学习来获取模型的强化学习算法被称为**基于模型的方法**。这些解决方案与无模型方法相对，且将在[第9章](7e8448cf-7b74-4f07-99bd-da8f98f4505c.xhtml)，*基于模型的强化学习*中做更详细的解释。
- en: Applications of RL
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的应用
- en: RL has been applied to a wide variety of fields, including robotics, finance,
    healthcare, and intelligent transportation systems. In general, they can be grouped
    into three major areas—automatic machines (such as autonomous vehicles, smart
    grids, and robotics), optimization processes (for example, planned maintenance,
    supply chains, and process planning) and control (for example, fault detection
    and quality control).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习已广泛应用于多个领域，包括机器人技术、金融、医疗保健和智能交通系统。通常，这些应用可以分为三个主要领域——自动化机器（如自动驾驶汽车、智能电网和机器人技术）、优化过程（例如，计划性维护、供应链和流程规划）和控制（例如，故障检测和质量控制）。
- en: In the beginning, RL was only ever applied to simple problems, but deep RL opened
    the road to different problems, making it possible to deal with more complex tasks.
    Nowadays, deep RL has been showing some very promising results. Unfortunately,
    many of these breakthroughs are limited to research applications or games, and,
    in many situations, it is not easy to bridge the gap between purely research-oriented applications
    and industry problems. Despite this, more companies are moving toward the adoption
    of RL in their industries and products.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，强化学习仅应用于简单的问题，但深度强化学习为解决不同问题开辟了道路，使得处理更复杂的任务成为可能。如今，深度强化学习已显示出一些非常有前景的结果。不幸的是，许多突破局限于研究应用或游戏，并且在许多情况下，很难弥合纯粹研究导向的应用和工业问题之间的差距。尽管如此，越来越多的公司正朝着在其行业和产品中采用强化学习的方向发展。
- en: We will now take a look at the principal fields that are already adopting or
    will benefit from RL.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看已经在应用或将受益于强化学习（RL）的主要领域。
- en: Games
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 游戏
- en: Games are a perfect testbed for RL because they are created in order to challenge
    human capabilities, and, to complete them, skills common to the human brain are
    required (such as memory, reasoning, and coordination). Consequently, a computer
    that can play on the same level or better than a human must possess the same qualities.
    Moreover, games are easy to reproduce and can be easily simulated in computers.
    Video games proved to be very difficult to solve because of their partial observability
    (that is, only a fraction of the game is visible) and their huge search space
    (that is, it's impossible for a computer to simulate all possible configurations).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏是强化学习的一个完美试验场，因为它们是为了挑战人类能力而创建的，并且要完成这些游戏，需要人类大脑常见的技能（如记忆、推理和协调）。因此，一台能够与人类同等甚至更好的计算机必须具备这些相同的特质。此外，游戏容易复现，可以在计算机中轻松模拟。视频游戏因其部分可观察性（即只有一小部分游戏是可见的）和巨大的搜索空间（即计算机不可能模拟所有可能的配置）而被证明是非常难以解决的。
- en: A breakthrough in games occurred when, in 2015, AlphaGo beat Lee Sedol in the
    ancient game of Go. This win occurred in spite of the prediction that it wouldn't.
    At the time, it was thought that no computer would be able to beat an expert in
    Go for the next 10 years. AlphaGo used both RL and supervised learning to learn
    from professional human games. A few years after that match, the next version,
    named AlphaGo Zero, beat AlphaGo 100 games to 0\. AlphaGo Zero learned to play
    Go in only three days through self-play.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年，AlphaGo在古老的围棋比赛中击败了李世石，突破了游戏领域的一个重要里程碑。这场胜利发生时，许多人曾预测AlphaGo不会获胜。当时人们认为，在未来10年内，任何计算机都无法击败围棋专家。AlphaGo利用强化学习和监督学习从人类职业选手的比赛中学习。几年的时间过去了，AlphaGo的下一个版本——AlphaGo
    Zero，击败了AlphaGo 100局，且一局不败。AlphaGo Zero通过自我对弈，仅用了三天时间就学会了下围棋。
- en: '**Self-play** is a very effective way to train an algorithm because it just
    plays against itself. Through self-play, useful sub-skills or behaviors could
    also emerge that otherwise would not have been discovered.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**自我对弈**是一种非常有效的算法训练方式，因为它只是与自己对弈。通过自我对弈，一些有用的子技能或行为也可能会出现，否则这些技能或行为是无法被发现的。'
- en: To capture the messiness and continuous nature of the real world, a team of
    five neural networks named OpenAI Five was trained to play *DOTA 2*, a real-time
    strategy game with two teams (each with five players) playing against each other.
    The steep learning curve in playing this game is due to the long time horizons
    (a game lasts for 45 minutes on average with thousands of actions), the partial
    observability (each player can only see a small area around themselves), and the
    high-dimensional continuous action and observation space. In 2018, OpenAI Five
    played against the top *DOTA 2* players at The International, losing the match
    but showing innate capabilities in both collaboration and strategy skills. Finally, on
    April 13, 2019, OpenAI Five officially defeated the world champions in the game,
    becoming the first AI to beat professional teams in an esports game.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉真实世界的混乱和连续性，一个由五个神经网络组成的团队——OpenAI Five，接受了训练，参与玩*DOTA 2*这款实时战略游戏。该游戏中有两个队伍（每队五名玩家）相互对抗。玩这款游戏的陡峭学习曲线来源于游戏的长时间跨度（每局游戏平均持续45分钟，且有成千上万的动作）、部分可观察性（每个玩家只能看到自己周围的一小块区域），以及高维度的连续动作和观察空间。2018年，OpenAI
    Five在《国际邀请赛》上与顶级*DOTA 2*玩家对战，虽然最终输了比赛，但展现出了在协作和战略技能方面的天生能力。最终，在2019年4月13日，OpenAI
    Five正式击败了世界冠军，成为第一个在电子竞技游戏中击败职业队伍的人工智能。
- en: Robotics and Industry 4.0
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器人技术与工业4.0
- en: RL in industrial robotics is a very active area of research as it is a natural
    adoption of this paradigm in the real world. The potential and benefit of industrial
    intelligent robots are huge and extensive. RL enables Industry 4.0 (referred to
    as the fourth industrial revolution) with intelligent devices, systems, and robots
    that perform highly complex and rational operations. Systems that predict maintenance,
    real-time diagnoses, and management of manufacturing activities can be integrated
    for better control and productivity.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习在工业机器人领域是一个非常活跃的研究方向，因为这是现实世界中自然而然会采用的范式。工业智能机器人的潜力和益处巨大且广泛。强化学习推动了工业4.0（即第四次工业革命），通过智能设备、系统和机器人来执行高度复杂和理性化的操作。能够预测维护、进行实时诊断和管理制造活动的系统可以集成在一起，从而更好地实现控制和提高生产力。
- en: Machine learning
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: Thanks to the flexibility of RL, it can be employed not only in standalone tasks
    but also as a sort of fine-tune method in supervised learning algorithms. In many
    **natural language processing** (**NLP**) and computer vision tasks, the metric
    to optimize isn't differentiable, so to address the problem in supervised settings with
    neural networks, it needs an auxiliary differentiable loss function. However,
    the discrepancy between the two loss functions will penalize the final performance.
    One way to deal with this is to first train the system using supervised learning
    with the auxiliary loss function, and then use RL to fine-tune the network optimizing
    with respect to the final metric. For instance, this process can be of benefit
    in subfields such as machine translation and question answering, where the evaluation
    metrics are complex and not differentiable.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于RL的灵活性，它不仅可以应用于独立任务，还可以作为一种在监督学习算法中进行微调的方法。在许多**自然语言处理**（**NLP**）和计算机视觉任务中，优化的指标是不可微分的，因此在监督设置中使用神经网络时，需要一个辅助的可微分损失函数。然而，这两个损失函数之间的差异会影响最终的性能。解决这个问题的一种方法是，首先使用监督学习并结合辅助损失函数训练系统，然后利用RL对网络进行微调，优化最终的指标。例如，这个过程在机器翻译和问答等子领域中可以发挥作用，因为这些领域的评估指标复杂且不可微分。
- en: Furthermore, RL can solve NLP problems such as dialogue systems and text generation.
    Computer vision, localization, motion analysis, visual control, and visual tracking
    can all be trained with deep RL.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RL还可以解决NLP问题，例如对话系统和文本生成。计算机视觉、定位、运动分析、视觉控制和视觉跟踪都可以通过深度RL进行训练。
- en: Deep learning proposes to overcome the heavy task of manual feature engineering
    while requiring the manual design of the neural network architecture. This is
    tedious work involving many parts that have to be combined in the best possible
    way. So, why can we not automate it? Well, actually, we can. **Neural architecture
    design** (**NAD**) is an approach that uses RL to design the architecture of deep
    neural networks. This is computationally very expensive, but this technique is
    able to create DNN architectures that can achieve state-of-the-art results in
    image classification.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习旨在克服手动特征工程的繁重任务，同时仍然需要手动设计神经网络架构。这是一项繁琐的工作，涉及多个部分，需要以最佳方式进行组合。那么，为什么我们不能自动化这一过程呢？事实上，我们可以。**神经架构设计**（**NAD**）是一种使用强化学习（RL）设计深度神经网络架构的方法。这在计算上非常昂贵，但该技术能够创建能够在图像分类中达到最先进结果的DNN架构。
- en: Economics and finance
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经济与金融
- en: Business management is another natural application of RL. It has been successfully
    used for internet advertising with the objective to maximize pay-per-click adverts
    for product recommendations, customer management, and marketing. Furthermore,
    finance has benefited from RL for tasks such as option pricing and multi-period
    optimization.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 商业管理是RL的另一种自然应用。它已成功用于互联网广告，目的是最大化按点击付费的广告，用于产品推荐、客户管理和营销。此外，金融领域也从RL中获益，应用包括期权定价和多期优化等任务。
- en: Healthcare
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 医疗健康
- en: RL is used in healthcare both for diagnosis and treatment. It can build the
    baseline for an AI-powered assistant for doctors and nurses. In particular, RL
    can provide individual progressive treatments for patients—a process known as
    the dynamic treatment regime. Other examples of RL in healthcare are personalized
    glycemic control and personalized treatments for sepsis and HIV.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: RL在医疗健康领域被用于诊断和治疗。它可以为医生和护士构建一个基于AI的助手的基础，特别是RL可以为患者提供个性化的渐进治疗——这一过程称为动态治疗方案。在医疗健康中的其他RL应用包括个性化血糖控制、脓毒症和艾滋病的个性化治疗。
- en: Intelligent transportation systems
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能交通系统
- en: Intelligent transportation systems can be empowered with RL to develop and improve
    all types of transportation systems. Its application can range from smart networks
    that control congestion (such as traffic signal controls), traffic surveillance,
    and safety (such as collision predictions), to self-driving cars.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 智能交通系统可以借助RL来开发和改进各种交通系统。其应用范围包括控制拥堵的智能网络（如交通信号控制）、交通监控和安全（如碰撞预测）到自动驾驶汽车。
- en: Energy optimization and smart grid
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 能源优化与智能电网
- en: Energy optimization and smart grids are central for intelligent generation,
    distribution, and consumption of electricity. Decision energy systems and control
    energy systems can adopt RL techniques to provide a dynamic response to the variability
    of the environment. RL can also be used to adjust the demand of electricity in
    response to a dynamic energy pricing or reduce energy usage.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 能源优化和智能电网对于智能发电、分配和消费电力至关重要。决策能源系统和控制能源系统可以采用强化学习技术，提供对环境变化的动态响应。强化学习还可以用于根据动态电价调整电力需求或减少能源使用。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: RL is a goal-oriented approach to decision-making. It differs from other paradigms
    due to its direct interaction with the environment and for its delayed reward
    mechanism. The combination of RL and deep learning is very useful in problems
    with high-dimensional state spaces and in problems with perceptual inputs. The
    concepts of policy and value functions are key as they give an indication about
    the action to take and the quality of the states of the environment. In RL, the
    model of the environment is not required, but it can give additional information
    and, therefore, improve the quality of the policy.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）是一种以目标为导向的决策方法。与其他范式不同，它直接与环境进行交互，并且具有延迟奖励机制。强化学习与深度学习的结合在具有高维状态空间和感知输入问题中非常有用。策略和价值函数的概念至关重要，因为它们指示了采取的行动以及环境状态的质量。在强化学习中，环境的模型并不是必需的，但它可以提供额外的信息，从而提高策略的质量。
- en: Now that all the key concepts have been introduced, in the following chapters,
    the focus will be on actual RL algorithms. But first, in the next chapter, you will
    be given the grounding to develop RL algorithms using OpenAI and TensorFlow.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有关键概念已被介绍，接下来的章节将重点介绍实际的强化学习算法。但首先，在下一章中，你将获得使用OpenAI和TensorFlow开发强化学习算法的基础知识。
- en: Questions
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is RL?
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是强化学习（RL）？
- en: What is the end goal of an agent?
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个智能体的最终目标是什么？
- en: What are the main differences between supervised learning and RL?
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习和强化学习之间的主要区别是什么？
- en: What are the benefits of combining deep learning and RL?
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合深度学习和强化学习有什么好处？
- en: Where does the term "reinforcement" come from?
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “强化”一词来源于哪里？
- en: What is the difference between policy and value functions?
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略和价值函数之间有什么区别？
- en: Can the model of an environment be learned through interacting with it?
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过与环境的互动，是否可以学习到环境的模型？
- en: Further reading
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'For an example of a faulty reward function, refer to the following link: [https://blog.openai.com/faulty-reward-functions/](https://blog.openai.com/faulty-reward-functions/).'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关有缺陷奖励函数的示例，请参阅以下链接：[https://blog.openai.com/faulty-reward-functions/](https://blog.openai.com/faulty-reward-functions/)。
- en: 'For more information about deep RL, refer to the following link: [http://karpathy.github.io/2016/05/31/rl/](http://karpathy.github.io/2016/05/31/rl/).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关深度强化学习的更多信息，请参阅以下链接：[http://karpathy.github.io/2016/05/31/rl/](http://karpathy.github.io/2016/05/31/rl/)。
