- en: Text classification Using Long Short-Term Memory Network
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用长短期记忆网络进行文本分类
- en: In the previous chapter, we used a recurrent neural network to develop a movie
    review sentiment classification model for text data that are characterized by
    a sequence of words. **Long Short-Term Memory** (**LSTM**) neural networks are
    a special type of **Recurrent Neural Networks** (**RNNs**) that are useful with
    data involving sequences and provide advantages that we will discuss in the next
    section. This chapter illustrates the steps for using an LSTM neural network for
    sentiment classification. The steps involved in applying an LSTM network to a
    business problem may include text data preparation, creating the LSTM model, training
    the model, and assessing the model performance.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用递归神经网络开发了一个电影评论情感分类模型，针对的是由单词序列构成的文本数据。**长短期记忆**（**LSTM**）神经网络是**递归神经网络**（**RNNs**）的一种特殊类型，适用于处理包含序列的数据，并提供我们将在下一节讨论的优势。本章将展示如何使用LSTM神经网络进行情感分类的步骤。将LSTM网络应用于商业问题的步骤可能包括文本数据准备、创建LSTM模型、训练模型和评估模型性能。
- en: 'More specifically, in this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在本章中，我们将覆盖以下主题：
- en: Why do we use LSTM networks?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们使用LSTM网络？
- en: Preparing text data for model building
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为模型构建准备文本数据
- en: Creating a long short-term memory network model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建长短期记忆网络模型
- en: Fitting the LSTM model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练LSTM模型
- en: Evaluating model performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型性能
- en: Performance optimization tips and best practices
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能优化技巧和最佳实践
- en: Why do we use LSTM networks?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们使用LSTM网络？
- en: 'We have seen, in the previous chapter, that recurrent neural networks provide
    decent performance when working with data involving sequences. One of the key
    advantages of using LSTM networks lies in the fact that they address the vanishing
    gradient problem that makes network training difficult for a long sequence of
    words or integers. Gradients are used for updating RNN parameters and for a long
    sequence of words or integers; these gradients become smaller and smaller to the
    extent that, effectively, no network training can take place. LSTM networks help
    to overcome this problem and make it possible to capture long-term dependencies
    between keywords or integers in sequences that are separated by a large distance.
    For example, consider the following two sentences, where the first sentence is
    short and the second sentence is relatively longer:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们已经看到，当处理包含序列的数据时，递归神经网络提供了不错的性能。使用LSTM网络的一个关键优势在于它们能够解决梯度消失问题，该问题使得在长序列的单词或整数上进行网络训练变得困难。梯度用于更新RNN参数，在长序列的单词或整数中，这些梯度变得越来越小，直到实际上无法进行网络训练。LSTM网络帮助克服了这个问题，使得能够捕捉序列中被大距离分隔的关键词或整数之间的长期依赖性。例如，考虑以下两句，其中第一句较短，第二句则相对较长：
- en: '**Sentence-1**: I like to eat chocolates.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子-1**：我喜欢吃巧克力。'
- en: '**Sentence-2**: I like, whenever there is a chance and usually there are many
    of them, to eat chocolates.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子-2**：每当有机会时，我喜欢吃巧克力，而通常这样的机会有很多。'
- en: In these sentences, the two important words that capture the main essence of
    the sentence are **like** and **chocolates**. In the first sentence, the words
    **like** and **chocolates** are closer to each other and they are separated by
    just two words in between. On the other hand, in the second sentence, these two
    words are separated by as many as 14 words that lie between them. LSTM networks
    are designed to deal with such long-term dependencies that are observed in longer
    sentences or longer sequences of integers. In this chapter, we focus on applying
    LSTM networks for developing a movie review sentiment classification model.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些句子中，捕捉句子主要意思的两个重要词是**like**和**chocolates**。在第一个句子中，**like**和**chocolates**离得较近，之间只隔了两个词。而在第二个句子中，这两个词之间隔了多达14个词。LSTM网络旨在处理这些长时间依赖性，通常出现在较长的句子或较长的整数序列中。在本章中，我们重点介绍如何应用LSTM网络来开发电影评论情感分类模型。
- en: Preparing text data for model building
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为模型构建准备文本数据
- en: We will continue to use IMDB movie review data that we used in the previous
    chapter on recurrent neural networks. This data is already available in a format
    where we can use it for developing deep network models with minimum need for data
    processing.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用上一章中使用的IMDB电影评论数据，该数据已经以我们可以用于开发深度网络模型的格式提供，数据处理的需求最小化。
- en: 'Let''s take a look at the following code:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下下面的代码：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The sequence of integers capturing train and test data is stored in `train_x` and `test_x` respectively.
    Similarly, `train_y` and `test_y` store labels capturing information about whether
    movie reviews are positive or negative. We have specified the number of most frequent
    words to be 500\. For padding, we are using 200 as the maximum length of a sequence
    of integers for both train and test data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 捕捉训练和测试数据的整数序列分别存储在`train_x`和`test_x`中。同样，`train_y`和`test_y`存储标签，用于表示电影评论是正面还是负面。我们已指定最频繁的单词数量为500。对于填充，我们使用200作为训练和测试数据序列的最大长度。
- en: When the actual length of integers is less than 200, then zeros get added at
    the beginning of the sequence to artificially increase the length of integers
    to 200\. However, when the length of integers is more than 200, integers at the
    beginning are removed so that the total length of integers is maintained at 200.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当整数的实际长度小于200时，序列的开头会填充零，以人为地将整数的长度增加到200。然而，当整数的长度大于200时，序列的开头会移除一些整数，以保持总长度为200。
- en: As mentioned earlier, both train and test datasets are balanced and contain
    data involving 25,000 movie reviews each. For each movie review, positive or negative
    labels are also available.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，训练集和测试集数据都是平衡的，每个数据集包含25,000条电影评论。每条电影评论也都带有正面或负面的标签。
- en: Note that the choice of value for `maxlen` can impact model performance. If
    the value chosen is too small, more words or integers in a sequence will get truncated.
    On the other hand, if the value chosen is too large, then more words or integers
    in a sequence will need padding, with zeroes getting added. One way to avoid too
    much padding or too much truncation is to choose a value closer to the median.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`maxlen`的取值会影响模型的表现。如果选择的值太小，序列中的更多单词或整数会被截断。另一方面，如果选择的值太大，序列中的更多单词或整数需要填充，零会被添加进去。避免过度填充或过度截断的一种方法是选择一个接近中位数的值。
- en: Creating a long short-term memory network model
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个长短期记忆网络模型
- en: In this section, we will start with a simple LSTM network architecture and look
    at calculations to arrive at the number of parameters. Subsequently, we will compile
    the model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将从一个简单的LSTM网络架构开始，看看如何计算出参数数量。随后，我们将编译模型。
- en: LSTM network architecture
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM网络架构
- en: 'We will start with a simple flow chart of the LSTM network architecture, as
    shown in the following screenshot:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的LSTM网络架构流程图开始，如下图所示：
- en: '![](img/ac568b12-cbe5-4312-b47e-1e95783f27ee.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac568b12-cbe5-4312-b47e-1e95783f27ee.png)'
- en: The preceding flow chart for the LSTM network highlights the layers in the architecture
    and activation functions used. In the LSTM layer, the `tanh` activation function
    is used which is the default activation function for the layer. In the dense layer, the
    `sigmoid` activation function is used.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的LSTM网络流程图突出了架构中的层和使用的激活函数。在LSTM层中，使用的是`tanh`激活函数，这是该层的默认激活函数。在密集层中，使用的是`sigmoid`激活函数。
- en: 'Let''s have a look at the following code and summary of the model:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下下面的代码和模型的总结：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Apart from what we used for the RNN model in the last chapter, we are replacing
    `layer_simple_rnn` with `layer_lstm` for the LSTM network in this example. For
    the embedding layer, we have a total of 16,000 (500 x 32) parameters. The calculation
    shown as follows calculates the number of parameters for the LSTM layer:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上一章中用于RNN模型的内容外，我们在此示例中将`layer_simple_rnn`替换为`layer_lstm`，用于LSTM网络。对于嵌入层，我们总共有16,000个参数（500
    x 32）。以下计算展示了LSTM层参数数量的计算方法：
- en: '*=4 x [units in LSTM layer x (units in LSTM layer + output dimension) + units
    in LSTM layer] *'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*=4 x [LSTM层单元数 x (LSTM层单元数 + 输出维度) + LSTM层单元数]*'
- en: '*= 4 x [32(32+32) + 32]*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*= 4 x [32(32+32) + 32]*'
- en: '*= 8320*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*= 8320*'
- en: For a similar architecture involving the RNN layer, we will have 2,080 parameters.
    The four-fold increase in the number of parameters for the LSTM layer also leads
    to more training time and hence requires relatively higher processing costs. The
    number of parameters for the dense layer is *[(32x1) + 1]*, which comes to 33\.
    Hence, overall there are 24,353 parameters in this network.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个类似的包含RNN层的架构，我们将有2,080个参数。LSTM层参数数量的四倍增加也导致了更多的训练时间，因此需要相对更高的处理成本。密集层的参数数量是*[(32x1)
    + 1]*，即33。因此，该网络的总参数数量为24,353。
- en: Compiling the LSTM network model
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译LSTM网络模型
- en: 'For compiling the LSTM network model, we will use the following code:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 用于编译LSTM网络模型的代码如下：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We are using `rmsprop` as optimizer and `binary_crossentropy` for loss, since
    movie reviews have a binary response or, in other words, they are either positive
    or negative. For metrics, we are making use of classification accuracy. After
    compiling the model, we are ready to go to the next step of fitting the LSTM model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`rmsprop`作为优化器，`binary_crossentropy`作为损失函数，因为电影评论的响应是二元的，换句话说，它们要么是积极的，要么是消极的。对于评估指标，我们使用分类准确率。在编译模型后，我们准备好进行LSTM模型的下一步拟合。
- en: Fitting the LSTM model
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拟合LSTM模型
- en: 'For training the LSTM model, we will use the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练LSTM模型，我们将使用以下代码：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will use train data to fit the LSTM model with ten epochs and use a batch
    size of 128\. We will also reserve 20% of train data as validation data for assessing
    loss and accuracy values during model training.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用训练数据来拟合LSTM模型，设定训练周期为10个epoch，批量大小为128。我们还将保留20%的训练数据作为验证数据，用于在模型训练过程中评估损失和准确率值。
- en: Loss and accuracy plot
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失与准确率图
- en: 'The following screenshot shows the loss and accuracy plot for `model_one`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了`model_one`的损失和准确率图：
- en: '![](img/7e6e0763-3476-4566-b4ab-2006bc2b2e06.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e6e0763-3476-4566-b4ab-2006bc2b2e06.png)'
- en: 'The plot for loss and accuracy based on training and validation data shows
    overall closeness between the curves. The observations from the plot are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基于训练和验证数据的损失与准确率图显示了曲线之间的整体接近性。图中的观察结果如下：
- en: There is no major divergence between the two lines, which indicates the lack
    of an over-fitting problem.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两条线之间没有出现显著的分歧，这表明没有出现过拟合问题。
- en: An increase in the number of epochs may not provide any significant improvement
    in model performance.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加epoch的数量可能不会显著改善模型的性能。
- en: However, the loss and accuracy values based on the validation data show some
    amount of unevenness or oscillation where they deviate from the training loss
    and accuracy by a relatively high amount.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，基于验证数据的损失和准确率值显示出一定程度的不均匀性或波动性，它们偏离训练损失和准确率的幅度相对较高。
- en: Epochs 4 and 8 especially stand out in this regard showing significant deviation
    from the loss and accuracy based on training data.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特别是在epoch 4和epoch 8这两个时刻，它们显示了与基于训练数据的损失和准确率的显著偏差。
- en: Next, we will move toward evaluating `model_one` and use it for prediction of
    the movie review sentiment.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续评估`model_one`并将其用于预测电影评论的情感。
- en: Evaluating model performance
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型性能
- en: In this section, we will evaluate the model based on both training and test
    data. We will also create a confusion matrix for both train and test data to gain
    further insights into the movie review sentiment classification performance of
    the model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将基于训练数据和测试数据评估模型。我们还将为训练数据和测试数据创建混淆矩阵，以进一步了解模型在电影评论情感分类方面的表现。
- en: Model evaluation with train data
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用训练数据评估模型
- en: 'We will first evaluate the model performance with train data using the following
    code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用以下代码评估训练数据上的模型性能：
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As seen from the preceding output, for the training data, we obtain a loss
    value of `0.375` and an accuracy of about `0.828`. This is a decent performance
    considering a relatively simple LSTM architecture. We next use this model to make
    predictions for the movie review sentiment and summarize the results by developing
    a confusion matrix using the following code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出可以看出，对于训练数据，我们得到了`0.375`的损失值和大约`0.828`的准确率。考虑到LSTM架构相对简单，这已经是一个不错的表现。接下来，我们将使用该模型预测电影评论的情感，并通过以下代码开发混淆矩阵来总结结果：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can make the following observations from the confusion matrix:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从混淆矩阵中得出以下观察结果：
- en: It is observed that this model seems to be more accurate in predicting positive
    movie reviews (11,430 correct predictions) compared to negative movie reviews
    (9,258 correct predictions). In other words, this model correctly classifies positive
    reviews at the rate of about 91.4% (also called the sensitivity of the model)
    for the training data.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察到该模型在预测积极电影评论（11,430个正确预测）方面似乎比预测消极电影评论（9,258个正确预测）更为准确。换句话说，该模型在训练数据中以约91.4%的正确率（也称为模型的敏感性）正确分类了积极评论。
- en: Similarly, this model correctly classifies negative reviews at the rate of about
    74.1% (also called specificity of the model) for the training data.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似地，该模型在训练数据中以约74.1%的正确率（也称为模型的特异性）正确分类了消极评论。
- en: It is also observed that the negative movie reviews are being misclassified
    as a positive review at the rate of about three times (3,242 reviews) more compared
    to a positive review being misclassified as negative (1,070 reviews).
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还观察到，负面电影评论被错误分类为正面评论的比例约为三倍（3,242 条评论），而正面评论被错误分类为负面评论的比例为 1,070 条评论。
- en: Hence, although overall, this model seems to perform well for the training data,
    looking deeper, we observe some bias toward correctly classifying positive movie
    reviews at the cost of lower accuracy in correctly classifying negative reviews.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，尽管总体而言，该模型似乎在训练数据上表现良好，但深入分析后，我们发现它在正确分类正面电影评论时有一定的偏向，这导致了在正确分类负面评论时准确度较低。
- en: It will be interesting to see whether the model performance observed, based
    on training data, results in similar behavior for the test data or not.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到基于训练数据的模型性能是否会在测试数据上出现类似的行为，将会很有趣。
- en: Model evaluation with test data
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用测试数据进行模型评估
- en: 'We will now use the test data to obtain loss and accuracy values for the model
    using the following code:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用测试数据，通过以下代码获得模型的损失和准确率值：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As seen from the preceding output, for the test data, we obtain a loss value
    of 0.399 and an accuracy of about 0.819\. These values, as expected, are slightly
    inferior to those obtained for the train data. However, they are close enough
    to results based on the train data to consider this model behavior consistent.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出可以看到，对于测试数据，我们得到的损失值为 0.399，准确率约为 0.819。正如预期的那样，这些值略低于训练数据上获得的结果。然而，它们与基于训练数据的结果足够接近，可以认为该模型行为一致。
- en: 'The code to obtain a confusion matrix using test data is as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 获取混淆矩阵的代码如下：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'From the confusion matrix shown above, the following observations can be made:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述混淆矩阵中，可以得出以下观察结果：
- en: The confusion matrix based on predictions using the test data shows a similar
    pattern that we observed earlier for the training data.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于使用测试数据的预测生成的混淆矩阵显示了与我们之前在训练数据中观察到的类似模式。
- en: This model also seems to perform better when accurately classifying positive
    movie reviews (at a rate of about 90.7%), compared to correctly classifying negative
    reviews (at a rate of about 73.3%).
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型在准确分类正面电影评论时似乎表现得更好（约 90.7%），而在正确分类负面评论时则较差（约 73.3%）。
- en: Hence, the model continues to show bias in the performance when correctly classifying
    positive movie reviews.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，当正确分类正面电影评论时，模型在性能上继续显示偏差。
- en: In the next section, we will carry out some experimentation to explore possible
    improvements for the model's movie review sentiment classification performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将进行一些实验，探索模型电影评论情感分类性能的潜在改进。
- en: Performance optimization tips and best practices
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能优化技巧和最佳实践
- en: In this section, we will carry out three different experiments to search for
    an improved LSTM based movie review sentiment classification model. This will
    involve trying a different optimizer at the time of compiling the model, adding
    another LSTM layer when developing the model architecture, and using a bidirectional
    LSTM layer in the network.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进行三项不同的实验，探索改进的基于 LSTM 的电影评论情感分类模型。这将涉及在编译模型时尝试不同的优化器、在模型架构开发时添加另一个
    LSTM 层，以及在网络中使用双向 LSTM 层。
- en: Experimenting with the Adam optimizer
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Adam 优化器进行实验
- en: 'We will use the `adam` (Adaptive Moment Optimization) optimizer instead of
    the `rmsprop` (Root Mean Square Propagation) optimizer that we used earlier when
    compiling the model. To make a comparison of model performance easier, we will
    keep everything else the same as earlier, as shown in the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在编译模型时使用 `adam`（自适应矩估优化）优化器，替代之前使用的 `rmsprop`（均方根传播）优化器。为了使模型性能的比较更加容易，我们将保持其余部分与之前相同，如以下代码所示：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After running the preceding codes and training the model, the accuracy and
    loss values for each epoch are stored in `model_two`. We use the loss and accuracy
    values in `model_two` to develop the following plot:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码并训练模型后，每个 epoch 的准确率和损失值被存储在 `model_two` 中。我们使用 `model_two` 中的损失和准确率值来生成以下图表：
- en: '![](img/2c80e3bf-c6f5-4c71-b9a8-008cd2ff70c2.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c80e3bf-c6f5-4c71-b9a8-008cd2ff70c2.png)'
- en: 'From the preceding loss and accuracy plot, we can make the following observations:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的损失和准确率图中，我们可以得出以下观察结果：
- en: The loss and accuracy plot based on the training and validation data shows a
    slightly improved pattern compared to the plot for the first model that we built
    with `model_one`.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于训练数据和验证数据的损失与准确率图表相比我们为第一个模型所构建的图表（`model_one`）显示出略微改进的模式。
- en: In the plot based on `model_one`, we observed that loss and accuracy values
    for validation data occasionally showed major deviations from the values based
    on the training data. In this plot, we do not see any such major deviation between
    the two lines.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在基于`model_one`的图表中，我们观察到验证数据的损失和准确率值偶尔出现较大偏差，而这些偏差在基于训练数据的值中并没有出现。在这个图表中，我们没有看到两条线之间的任何重大偏差。
- en: Also, the loss and accuracy values based on the last few values of validation
    data seem flat, suggesting that the ten epochs that we have used are sufficient
    to train the model and an increasing number of epochs is not likely to help in
    improving the model performance.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，基于验证数据的最后几个值的损失和准确率值似乎趋于平稳，表明我们使用的十个训练周期已经足够训练模型，增加训练周期数不太可能帮助提高模型性能。
- en: 'Next, let''s obtain the loss, accuracy, and confusion matrix for the training
    data using the following code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下代码获取训练数据的损失、准确率和混淆矩阵：
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'From the preceding code output, we can make the following observations:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码输出中，我们可以得出以下观察结果：
- en: By using the `adam` optimizer, we obtain loss and accuracy for training data
    as 0.360 and 0.843 respectively. Both these numbers show an improvement compared
    to the earlier model where we had used the `rmsprop` optimizer.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用`adam`优化器，我们获得了训练数据的损失和准确率分别为0.360和0.843。这两个数值相比我们之前使用`rmsprop`优化器的模型都有所提升。
- en: Another difference can be observed from the confusion matrix. This model performs
    better when correctly classifying negative movie reviews (at a rate of about 88.9%)
    compared to the correct classification of positive reviews (at a rate of about
    79.7%).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从混淆矩阵中可以观察到另一个区别。该模型在正确分类负面电影评论时的表现较好（正确分类率约为88.9%），而在正确分类正面评论时的正确分类率约为79.7%。
- en: This behavior is the opposite of what was observed in the previous model. This
    model seems to be biased toward correctly classifying negative movie review sentiment
    compared to correctly classifying positive reviews.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种行为与之前模型中的观察结果相反。与正确分类正面评论相比，该模型似乎更偏向于正确分类负面电影评论情感。
- en: 'Having reviewed the performance of the model using the training data, we will
    now repeat the process with the test data, with the following code for obtaining
    the loss, accuracy, and confusion matrix:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在审视完使用训练数据的模型性能后，我们将使用以下代码重复该过程，获取测试数据的损失、准确率和混淆矩阵：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'From the preceding code output, we can make the following observations:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码输出中，我们可以得出以下观察结果：
- en: The loss and accuracy based on test data are 0.385 and 0.829 respectively. These
    results, based on the test data, also show better model performance compared to
    the previous model with the test data.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于测试数据的损失和准确率分别为0.385和0.829。这些基于测试数据的结果相比之前的模型在测试数据上也显示出更好的模型表现。
- en: The confusion matrix shows a similar pattern that we observed for the training
    data. Negative movie review sentiments are correctly classified at a rate of about
    86.9% for the test data.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵显示了我们在训练数据中观察到的类似模式。负面电影评论情感在测试数据中的正确分类率约为86.9%。
- en: Similarly, positive movie review sentiments are correctly classified by the
    model at a rate of about 78.8% for the test data.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，正面电影评论情感在测试数据中的正确分类率约为78.8%。
- en: This behavior is consistent with the model performance that was obtained using
    the training data.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种行为与使用训练数据获得的模型性能一致。
- en: 'Although trying the `adam` optimizer improves overall movie review sentiment
    classification performance, it still retains bias when correctly classifying one
    category compared to the other. A good model should not only improve the overall
    performance, but it should also minimize any bias when correctly classifying a
    category. The following code provides a table showing the number of negative and
    positive reviews in the `train` and `test` data:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管尝试`adam`优化器改善了整体的电影评论情感分类性能，但它在正确分类某一类别时仍然存在偏差。一个好的模型不仅应当提高整体性能，还应最小化在正确分类某一类别时的偏差。以下代码提供了一个表格，展示了`train`和`test`数据中负面和正面评论的数量：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It can be seen from the preceding output of code that this movie review data
    is balanced where both train and test data has 25,000 reviews each. This data
    is also balanced in terms of the number of positive or negative reviews. Both
    train and test datasets have 12,500 positive and 12,500 negative movie reviews
    each. Hence, there is no bias in the amount of negative or positive reviews provided
    to the model for training. However, the bias seen when correctly classifying negative
    and positive movie reviews is certainly something that needs improvement.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码输出可以看出，该电影评论数据是平衡的，训练数据和测试数据各包含 25,000 条评论。这些数据在正面和负面评论的数量上也是平衡的。训练集和测试集各包含
    12,500 条正面评论和 12,500 条负面评论。因此，提供给模型进行训练的负面或正面评论数量没有偏差。然而，在正确分类负面和正面电影评论时所观察到的偏差，显然是需要改进的地方。
- en: In the next experiment, let's explore with more LSTM layers and see whether
    or not we can obtain a better movie review sentiment classification model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个实验中，我们将尝试添加更多的 LSTM 层，看看是否能获得更好的电影评论情感分类模型。
- en: Experimenting with the LSTM network having an additional layer
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对具有额外层的 LSTM 网络进行实验
- en: 'In this second experiment to improve the performance of the classification
    model, we will add an extra LSTM layer. Let''s have a look at the following code:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二次实验中，为了提高分类模型的性能，我们将添加一个额外的 LSTM 层。让我们来看一下以下代码：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: By adding an extra LSTM layer to the network, as shown in the preceding code,
    the total number of parameters with these two LSTM layers will now increase to
    32,673 compared to 24,353 that we had previously with one LSTM layer. This increase
    in the number of parameters will also lead to higher training time when training
    the network. We are also retaining the use of the Adam optimizer when compiling
    the model. We are keeping everything else the same as what we had used in the
    previous model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向网络中添加额外的 LSTM 层，如前面的代码所示，两个 LSTM 层的总参数数量将从之前一个 LSTM 层的 24,353 增加到 32,673。这一参数数量的增加也会导致训练时间的增加。我们仍然在编译模型时使用
    Adam 优化器。除此之外，我们保持与之前模型相同的其他设置。
- en: 'A simple flow chart for the network architecture with two LSTM layers used
    in this experiment, is shown in the following screenshot:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图显示了该实验中使用的具有两个 LSTM 层的网络架构的简单流程图：
- en: '![](img/0418119a-4dae-4bfa-a9b1-0897d0815d18.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0418119a-4dae-4bfa-a9b1-0897d0815d18.png)'
- en: The preceding flow chart shown for the LSTM network highlights the two layers
    in the architecture and activation functions used. In both LSTM layers, `tanh`
    is used as the default activation function. In the dense layer, we continue to
    use the `sigmoid` activation function that we used earlier.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的 LSTM 网络流程图突出了架构中的两层及所使用的激活函数。在这两层 LSTM 中，`tanh` 被用作默认的激活函数。在全连接层中，我们继续使用之前所使用的
    `sigmoid` 激活函数。
- en: 'After training the model, the accuracy and loss values for each epoch is stored
    in `model_three`. We use the loss and accuracy values in `model_three` to develop
    the following plot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型后，每个周期的准确率和损失值被存储在 `model_three` 中。我们使用 `model_three` 中的损失值和准确率来生成以下图表：
- en: '![](img/9cc7906f-0224-47de-8ec8-ef9ed8d7b438.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9cc7906f-0224-47de-8ec8-ef9ed8d7b438.png)'
- en: 'From the loss and accuracy plot shown, we can make the following observations:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 从显示的损失和准确率图中，我们可以做出以下观察：
- en: The plot for loss and accuracy values doesn't indicate the presence of an over-fitting
    problem since the curves for the training and validation data are close to each
    other.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失和准确率的图表没有显示过拟合问题的存在，因为训练数据和验证数据的曲线相互接近。
- en: As in the earlier model, the loss and accuracy for the validation data seem
    to remain flat for the last few epochs, indicating ten epochs are sufficient for
    training the model, and increasing the number of epochs is not likely to improve
    results.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与早期的模型一样，验证数据的损失和准确率似乎在最后几个训练周期保持平稳，这表明十个训练周期足以训练模型，增加训练周期数不太可能改善结果。
- en: 'We can now obtain the loss, accuracy, and confusion matrix for the training
    data using the following code:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下代码来获得训练数据的损失、准确率和混淆矩阵：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'From the preceding code output, we can make the following observations:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码输出中，我们可以做出以下观察：
- en: The loss and accuracy values based on training data are obtained as `0.339`
    and `0.855` respectively. Both loss and accuracy show improvement compared to
    the earlier two models.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于训练数据得到的损失值和准确率分别为 `0.339` 和 `0.855`。与之前的两个模型相比，损失和准确率都有所改善。
- en: We can use this model to make predictions for each review in the training data,
    compare them with actual labels, and then summarize the results in the form of
    a confusion matrix.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用这个模型对训练数据中的每个评论进行预测，将其与实际标签进行比较，然后以混淆矩阵的形式总结结果。
- en: For the training data, the confusion matrix shows that the model correctly classifies
    negative movie reviews about 90% of the time and correctly classifies positive
    reviews about 81% of the time.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于训练数据，混淆矩阵显示模型正确分类负面电影评论的比例约为90%，正确分类正面评论的比例约为81%。
- en: So, although there is an overall improvement in the model performance, we continue
    to observe bias when correctly classifying one category compared to the other.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所以，尽管模型性能整体有所提高，但我们在正确分类某一类别时，相比另一类别仍然观察到偏差。
- en: 'After reviewing the performance of the model using training data, we will now
    repeat the process with the test data. Following is the code for obtaining the
    loss, accuracy, and confusion matrix:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了使用训练数据时模型的表现后，我们现在将使用测试数据重复这个过程。以下是获取损失、准确率和混淆矩阵的代码：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'From the preceding code output, we can make the following observations:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码输出中，我们可以得出以下观察结果：
- en: For the test data, the loss and accuracy values are 0.376 and 0.837 respectively.
    Both results show a better classification performance compared to the previous
    two models for the test data.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于测试数据，损失和准确率分别为0.376和0.837。这两个结果都显示出相较于前两个模型，测试数据的分类表现更好。
- en: The confusion matrix shows that negative movie reviews are correctly classified
    at a rate of about 87.3%, and positive reviews are correctly classified at a rate
    of about 80%.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵显示，负面电影评论的正确分类率约为87.3%，而正面评论的正确分类率约为80%。
- en: Hence, these results are consistent with those obtained using the training data
    and show a similar bias to that we observed for the training data.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，这些结果与使用训练数据得到的结果一致，并且显示出与我们在训练数据中观察到的类似偏差。
- en: To summarize, by adding an extra LSTM layer, we were able to improve the movie
    review sentiment classification performance of the model. However, we continue
    to observe bias when correctly classifying one category compared to the other
    category. Hence, although we obtained moderate success in improving model performance,
    there is scope to further improve the classification performance of the model.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，通过添加额外的LSTM层，我们能够提高模型的电影评论情感分类性能。然而，我们在正确分类某一类别时，仍然观察到偏差，与另一类别相比。虽然我们在提升模型性能方面取得了一定成功，但仍有提升模型分类性能的空间。
- en: Experimenting with a bidirectional LSTM layer
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实验使用双向LSTM层
- en: A bidirectional LSTM, as the name indicates, not only uses the sequence of integers
    provided as input but also makes use of its reverse order as additional input.
    There could be situations where this approach may help to achieve further model
    classification performance improvements by capturing useful patterns in the data
    that may not have been captured by the original LSTM network.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 双向LSTM，顾名思义，不仅使用作为输入提供的整数序列，还利用其反向顺序作为额外输入。在某些情况下，这种方法可能有助于通过捕捉数据中可能未被原始LSTM网络捕获的有用模式，进一步提高模型的分类性能。
- en: 'For this experiment, we will modify the LSTM layer in the first experiment,
    as shown in the following code:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本实验，我们将修改第一次实验中的LSTM层，如下代码所示：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'From the preceding code output, we can make the following observations:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码输出中，我们可以得出以下观察结果：
- en: We converted the LSTM layer into a bidirectional LSTM layer using the bidirectional
    `()` function.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用双向`()`函数将LSTM层转换为双向LSTM层。
- en: This change doubles the number of parameters related to the LSTM layer to 16,640,
    as can be seen from the model summary.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这一改变将与LSTM层相关的参数数量增加到16,640，从模型摘要中可以看到这一点。
- en: The total number of parameters for this architecture now increases to 32,705\.
    This increase in the number of parameters will further reduce the speed at which
    the network will be trained.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该架构的总参数数量现已增加到32,705。参数数量的增加将进一步降低网络训练的速度。
- en: 'Here is a simple flow chart for the bidirectional LSTM network architecture:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这是双向LSTM网络架构的简单流程图：
- en: '![](img/2c0b3651-c617-4563-816c-1469d82c371b.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2c0b3651-c617-4563-816c-1469d82c371b.png)'
- en: 'The flow chart for the bidirectional LSTM network shows embedding, bidirectional,
    and dense layers. In the bidirectional LSTM layer, `tanh` is used as the activation
    function and the dense layer uses the `sigmoid` activation function. The code
    for compiling and training the model is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 双向 LSTM 网络的流程图展示了嵌入层、双向层和密集层。在双向 LSTM 层中，`tanh` 被用作激活函数，而密集层使用 `sigmoid` 激活函数。编译和训练模型的代码如下：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As seen from the preceding code, we will continue to use the `adam` optimizer
    and keep the other settings the same as earlier for compiling and then fitting
    the model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码可以看出，我们将继续使用 `adam` 优化器，并保持其他设置与之前相同，进行模型的编译和拟合。
- en: 'After we train the model, the accuracy and loss values for each epoch are stored
    in `model_four`. We use the loss and accuracy values in `model_four` to develop
    the following plot:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练模型后，每个 epoch 的准确率和损失值会存储在 `model_four` 中。我们使用 `model_four` 中的损失和准确率值来绘制以下图表：
- en: '![](img/ab926820-55f4-4c95-8056-ad5ee3a57beb.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab926820-55f4-4c95-8056-ad5ee3a57beb.png)'
- en: 'From the preceding plot, we can make the following observations:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中，我们可以得出以下观察结果：
- en: The loss and accuracy plot doesn't show any cause for concern regarding over-fitting
    as the lines for training and validation are reasonably close to each other.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失和准确率图表没有显示过拟合的任何问题，因为训练和验证的曲线相对接近。
- en: The plot also shows that we do not need more than ten epochs to train this model.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图表还显示，我们无需超过十个 epoch 就可以训练这个模型。
- en: 'We will obtain the loss, accuracy, and confusion matrix for the training data
    using the following code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码获取训练数据的损失值、准确率和混淆矩阵：
- en: '[PRE17]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'From the preceding code output, we can make the following observations:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码输出中，我们可以得出以下观察结果：
- en: For the training data, we obtain loss and accuracy values of 0.341 and 0.852
    respectively. These results are only marginally inferior to the previous results
    and are not significantly different.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于训练数据，我们得到的损失值和准确率分别为 0.341 和 0.852。这些结果仅比之前的结果稍微差一些，差异不大。
- en: The confusion matrix this time shows a more even performance for correctly classifying
    positive and negative movie reviews.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这次的混淆矩阵显示了正确分类正面和负面电影评论的表现更加均衡。
- en: For negative movie reviews, the correct classification rate is about 84.8% and
    for positive reviews, it is about 85.7%.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于负面电影评论，正确分类率约为 84.8%，对于正面评论，正确分类率约为 85.7%。
- en: This difference of about 1% is much smaller than what we observed for the earlier
    models.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个大约 1% 的差异远小于我们在早期模型中观察到的差异。
- en: 'We will now repeat the preceding process with the test data. Following is the
    code for obtaining the loss, accuracy, and confusion matrix:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将对测试数据重复之前的过程。以下是获取损失、准确率和混淆矩阵的代码：
- en: '[PRE18]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'From the preceding code output, we can make the following observations:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码输出中，我们可以得出以下观察结果：
- en: For the test data, the loss and accuracy values are 0.374 and 0.834 respectively.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于测试数据，损失值和准确率分别为 0.374 和 0.834。
- en: The confusion matrix shows that the negative reviews are correctly classified
    by the model at a rate of about 82.8%.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵显示，负面评论的正确分类率约为 82.8%。
- en: This model correctly classifies positive movie reviews at a rate of about 84.1%.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该模型正确分类正面电影评论的准确率约为 84.1%。
- en: These results are consistent with those obtained for the training data.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些结果与训练数据中获得的结果一致。
- en: The experiment with bidirectional LSTM helped to obtain somewhat comparable
    performance in terms of loss and accuracy than that were obtained with two LSTM
    layers in the previous experiment. However, the main gain that is observed is
    in achieving results where we can correctly classify a negative or positive movie
    review with much better consistency.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 双向 LSTM 的实验帮助我们获得了与之前实验中使用两个 LSTM 层时相似的损失和准确率表现。然而，主要的进展在于我们能够在正确分类负面或正面电影评论时，表现出更好的一致性。
- en: In this chapter, we used the LSTM network to develop a movie review sentiment
    classification model. When data involves sequences, LSTM networks help to capture
    long term dependencies in the sequence of words or integers. We experimented with
    four different LSTM models by making some changes to the model and the results
    for the same are summarized in the following table.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们使用 LSTM 网络开发了一个电影评论情感分类模型。当数据涉及序列时，LSTM 网络有助于捕捉序列中词语或整数的长期依赖关系。我们通过对模型进行一些修改，实验了四种不同的
    LSTM 模型，相关结果总结在以下表格中。
- en: 'This table summarizes the performance of the four LSTM models:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本表总结了四个LSTM模型的性能：
- en: '| **Model** | **LSTM Layers** | **Optimizer** | **Data** | **Loss** | **Accuracy**
    | **Accuracy for Negative Reviews or Specificity** | **Accuracy for Positive Reviews
    or Sensitivity** |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **LSTM层** | **优化器** | **数据** | **损失** | **准确率** | **负面评论准确率或特异性**
    | **正面评论准确率或敏感性** |'
- en: '| One | 1 | `rmsprop` | Train | 0.375 | 82.8% | 74.1% | 91.4% |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 一 | 1 | `rmsprop` | 训练 | 0.375 | 82.8% | 74.1% | 91.4% |'
- en: '|   |   |   | Test | 0.399 | 81.9% | 73.3% | 90.7% |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|   |   |   | 测试 | 0.399 | 81.9% | 73.3% | 90.7% |'
- en: '| Two | 1 | `adam` | Train | 0.360 | 84.3% | 88.9% | 79.7% |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 二 | 1 | `adam` | 训练 | 0.360 | 84.3% | 88.9% | 79.7% |'
- en: '|   |   |   | Test | 0.385 | 82.9% | 86.9% | 78.8% |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|   |   |   | 测试 | 0.385 | 82.9% | 86.9% | 78.8% |'
- en: '| Three | 2 | `adam` | Train | 0.339 | 85.5% | 90.0% | 81.0% |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 三 | 2 | `adam` | 训练 | 0.339 | 85.5% | 90.0% | 81.0% |'
- en: '|   |   |   | Test | 0.376 | 83.7% | 87.3% | 80.0% |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|   |   |   | 测试 | 0.376 | 83.7% | 87.3% | 80.0% |'
- en: '| Four | bidirectional | `adam` | Train | 0.341 | 85.2% | 84.8% | 85.7% |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 四 | 双向 | `adam` | 训练 | 0.341 | 85.2% | 84.8% | 85.7% |'
- en: '|   |   |   | Test | 0.374 | 83.4% | 82.8% | 84.1% |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|   |   |   | 测试 | 0.374 | 83.4% | 82.8% | 84.1% |'
- en: 'We can make the following observations from the preceding table:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从上述表格中做出以下观察：
- en: Out of the four models that were tried, the bidirectional LSTM model provided
    better performance compared to the other three models. It has the lowest loss
    value based on test data.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在尝试的四个模型中，双向LSTM模型相比其他三个模型提供了更好的性能。它在基于测试数据的损失值方面最低。
- en: Although overall accuracy is slightly lower for the fourth model compared to
    the third model, accuracy for correctly classifying negative and positive reviews
    is much more consistent, varying from 82.8% to 84.1%, or a spread of only about
    1.3%.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管第四个模型的整体准确率相比第三个模型略低，但其正确分类负面和正面评论的准确性更加稳定，变化范围从82.8%到84.1%，即约1.3%的波动。
- en: The third model seems biased toward negative reviews that correctly classifies
    such reviews at a rate of 87.3% for the test data. For the third model, the correct
    classification of positive reviews in the test data is only at 80%. Hence, the
    spread between the correct classification of negative and positive reviews for
    the third model is more than 7%.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个模型似乎偏向于负面评论，正确分类该类评论的测试数据准确率为87.3%。对于第三个模型，正面评论在测试数据中的正确分类准确率仅为80%。因此，第三个模型中负面和正面评论的正确分类差距超过7%。
- en: The spread between sensitivity and specificity is even higher for the first
    two models.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前两个模型的敏感性和特异性之间的差距甚至更大。
- en: Although the fourth model provides good results, additional improvements can
    certainly be explored by experimenting further with other variables. Variables
    that can be used for further experiments may include the number of most frequent
    words, use of pre versus post for padding and/or truncation, the maximum length
    used for padding, the number of units in the LSTM layer, and the choice of another
    optimizer at the time of compiling the model.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管第四个模型提供了良好的结果，但通过进一步实验其他变量，仍然可以探索额外的改进。可以用于进一步实验的变量包括最常见词汇的数量、使用前后填充和/或截断、填充时使用的最大长度、LSTM层中的单元数，以及在编译模型时选择其他优化器。
- en: Summary
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we illustrated the use of LSTM networks for developing a movie
    review sentiment classification model. One of the problems faced by recurrent
    neural networks that we used in the previous chapter is that it involves difficulty
    in capturing long-term dependency that may exist between two words/integers in
    a sequence of words or integers. **Long Short-Term Memory** (**LSTM**) networks
    are designed to artificially retain long-term memories that are important when
    dealing with long sentences or a long sequence of integers.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们展示了使用LSTM网络来开发电影评论情感分类模型。我们在上一章中使用的递归神经网络所面临的一个问题是，难以捕捉序列中两个词语/整数之间可能存在的长期依赖关系。**长短期记忆**（**LSTM**）网络的设计目的是人为地保留在处理长句子或长整数序列时重要的长期记忆。
- en: In the next chapter, we will continue to work with text data and explore the
    use of **Convolutional Recurrent Neural Network****s** (**CRNNs**), which combine
    the benefits of **Convolutional Neural Networks** (**CNNs**) and **Recurrent Neural
    Networks** (**RNNs**) into a single network. We will illustrate the use of this
    type of network with the help of an interesting and publicly available text dataset, `reuter_50_50`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续处理文本数据，并探索**卷积递归神经网络**（**CRNNs**）的应用，CRNNs结合了**卷积神经网络**（**CNNs**）和**递归神经网络**（**RNNs**）的优点，形成了一个单一的网络。我们将通过一个有趣且公开可用的文本数据集`reuter_50_50`来说明这种网络的应用。
