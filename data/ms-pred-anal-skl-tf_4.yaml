- en: Introduction to Artificial Neural Networks and TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络和TensorFlow简介
- en: In this chapter, we will give an introduction to **artificial neural networks**
    (**ANNs**), which are basically computational models inspired by living brains,
    and perceptrons, which are the building blocks for ANNs. We will also talk about
    all of the elements to consider when building a deep neural network model. Then,
    we will talk about TensorFlow, which is the library that we will use to create
    these deep neural network models. Finally, we will talk about the core concepts
    that we need to understand about TensorFlow in order to use these library concepts,
    such as variables, placeholders, sessions, graphs, and others that are essential
    for using this library.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍**人工神经网络**（**ANNs**），这些基本上是受到生物大脑启发的计算模型，以及感知器，它们是ANNs的构建块。我们还将讨论在构建深度神经网络模型时需要考虑的所有元素。接着，我们将介绍TensorFlow，这是我们将用于创建这些深度神经网络模型的库。最后，我们将讨论使用该库的核心概念，如变量、占位符、会话、图和其他在使用该库时至关重要的概念。
- en: 'The following are the topics that will be covered as we progress:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将覆盖以下主题：
- en: Introduction to ANNs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工神经网络简介
- en: Elements of a deep neural network
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络的元素
- en: Installation of and introduction to TensorFlow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow的安装和简介
- en: Core concepts in TensorFlow
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow的核心概念
- en: Introduction to ANNs
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络简介
- en: ANNs are biologically inspired computational models that can be used to train
    a computer to perform a task using data. These models are part of the broad category
    of machine learning models. The distinction between these models and others is
    that these models are based on a collection of connected units called **artificial
    neurons**.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANNs）是生物学启发的计算模型，可用于通过数据训练计算机执行任务。这些模型属于机器学习模型的广泛范畴。与其他模型的区别在于，这些模型基于一组名为**人工神经元**的连接单元。
- en: There are many types of ANNs and, in this book, we will use one specific type,
    which is called the **multilayer perceptron **(**MLP**). Please note that there
    are a lot more variations of ANNs. These are machine learning models and we can
    use them for classification and regression tasks, but we can actually extend these
    models and apply them to other very specific tasks such as computer vision, speech
    recognition, and machine translation. These models are the basis of the exciting
    and growing field of deep learning, which has been really successful in the last
    few years in many areas.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络有许多类型，在本书中，我们将使用一种特定类型，称为**多层感知器**（**MLP**）。请注意，人工神经网络有更多的变种。这些是机器学习模型，我们可以用它们进行分类和回归任务，但我们实际上可以扩展这些模型，将其应用于其他非常具体的任务，如计算机视觉、语音识别和机器翻译。这些模型是深度学习这一激动人心且不断发展的领域的基础，近年来在许多领域取得了巨大的成功。
- en: Perceptrons
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器
- en: 'Perceptrons are the simplest type of artificial neuron, invented as a simple
    model for binary classification. Let''s use the context of the dataset that we
    have been using in this book, the credit card dataset. Let''s say that we have
    only two features for classifying defaulters and nondefaulters: age and bill amount.
    So the idea of the perceptron is to create some kind of a score. To do so, you
    take one constant, `w1` ,and multiply it by the value of `age`, and then you add
    another constant, `w2`, which is multiplied by the value of the `bill` amount
    as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是最简单的人工神经元类型，是作为二分类的简单模型发明的。我们使用本书中使用的数据集，即信用卡数据集。假设我们只有两个特征来分类违约者和非违约者：年龄和账单金额。那么感知器的思路是创建某种得分。为此，你将一个常数`w1`与`age`的值相乘，然后再加上另一个常数`w2`，它与`bill`金额的值相乘，如下所示：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As a rule, we classify this person as a defaulter if `score` > `b`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 作为规则，如果`score` > `b`，我们将此人分类为违约者。
- en: So, from this simple operation, we create a score. Then, we follow the rule
    to classify people as defaulters or as nondefaulters. So, if this `score` is greater
    than some number, then we classify this person as a defaulter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从这个简单的操作中，我们创建了一个得分。接着，我们根据规则将人们分类为违约者或非违约者。所以，如果这个`score`大于某个数值，那么我们就将此人归类为违约者。
- en: 'An equivalent way to state this rule is shown in the following screenshot:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了一种等效的规则表述方式：
- en: '![](img/ee8e1fff-8952-4989-b6ff-f3a0dda854dd.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee8e1fff-8952-4989-b6ff-f3a0dda854dd.png)'
- en: So, the prediction of this model will be `1`, or defaulter, if the quantity
    is greater than `0`, and the prediction will be `0`, or nondefaulter, if this
    quantity is less than or equal to `0`. The `b` value is also known as the threshold
    or bias.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这个模型的预测结果将是 `1`，即违约者，如果该数量大于 `0`，预测将是 `0`，即非违约者，如果该数量小于或等于 `0`。`b` 值也称为阈值或偏置。
- en: 'In general, if we have *n* features, then our perceptron would look similar
    to the following screenshot:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，如果我们有 *n* 个特征，那么我们的感知器看起来会类似于下面的截图：
- en: '![](img/7938be7f-8c64-4cdd-9d93-b6ba65e4d5a6.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7938be7f-8c64-4cdd-9d93-b6ba65e4d5a6.png)'
- en: As you can see, we have the same form. We predict **1** if the sum of the weights
    times the values of our features **-b** is actually greater than **0**, otherwise,
    we predict **0**. Assuming that all features are on the same scale, the weights
    would represent the importance of each feature in making the decision. So, we
    know that for this particular problem we have, all features are in very different
    scales. For example, ages are in different scales than bill amount, but let's
    say that you set all of  the features to a similar scale. You can think about
    the **w** variables as the weights, and they are the most important part of each
    feature while making the decision.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们有相同的形式。如果所有特征的权重与特征值的乘积和 **-b** 实际上大于 **0**，那么我们预测 **1**，否则我们预测 **0**。假设所有特征都在相同的尺度上，那么权重将表示每个特征在做出决策时的重要性。因此，我们知道，在这个特定问题中，所有特征的尺度是非常不同的。例如，年龄的尺度与账单金额的尺度不同，但假设你将所有特征设置为相似的尺度。你可以将
    **w** 变量视为权重，它们是做出决策时每个特征最重要的部分。
- en: 'The following screenshot shows another way to visualize this perceptron:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图展示了另一种可视化感知器的方法：
- en: '![](img/bd6b7f5e-ef5e-4f6e-aac0-a65a57a01b6b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bd6b7f5e-ef5e-4f6e-aac0-a65a57a01b6b.png)'
- en: 'So, you have the values of the threshold or the bias, **b**, and you have the
    value of **Age**, **x1** ,and the value of **Bill amount**, **x2**. So the three
    values go into an operation, and then you get an output. Now, there is a little
    modification that we can do to the perceptron, and this is to add what is known
    as an **activation function**. An activation function is any function that takes
    the result of the operation and performs some transformation to the input values
    using the **f** function. So the input for the activation function is the resulting
    quantity from the operation, and then, after applying activation function **f**,
    we will get the following output:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你有阈值或偏置 **b** 的值，以及 **Age**（年龄）、**x1** 和 **Bill amount**（账单金额）、**x2** 的值。然后这三个值会进入一个操作，得出输出。现在，我们可以对感知器做一点修改，那就是添加一个被称为
    **激活函数** 的东西。激活函数是任何一个函数，它接受操作结果，并通过 **f** 函数对输入值进行某些变换。因此，激活函数的输入是操作的结果量，经过激活函数
    **f** 后，我们将得到以下输出：
- en: '![](img/b2e13ce5-8226-4536-a9c6-c425619c50d0.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2e13ce5-8226-4536-a9c6-c425619c50d0.png)'
- en: So, this is the perceptron. We can add an activation function to the perceptron
    and then we get the rule or the classification `1` or `0`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是感知器。我们可以向感知器添加一个激活函数，然后我们得到规则或分类 `1` 或 `0`。
- en: Now, maybe you are wondering how do we decide which are the best weights and
    threshold for our perceptron? What activation function can we use? The answers
    to these questions are provided by the perceptron learning algorithm. So, there
    is a learning algorithm that we can use to actually train perceptrons. The good
    thing about perceptrons is that they are very simple to understand. However, they
    are very weak in performance when compared to more sophisticated methods, such
    as the methods that we used in previous chapters. So, it is not worth actually
    learning about this perceptron learning algorithm. However, these very simple
    models are the building blocks for ANNs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，也许你在想我们如何决定感知器的最佳权重和阈值？我们可以使用什么激活函数？这些问题的答案由感知器学习算法提供。所以，我们有一个学习算法，可以用来实际训练感知器。感知器的好处是它们非常容易理解。然而，与我们在之前章节中使用的更复杂的方法相比，它们的性能非常弱。因此，实际上没有必要学习这个感知器学习算法。然而，这些非常简单的模型是人工神经网络（ANNs）的构建块。
- en: Multilayer perceptron
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'ANNs are models based on perceptrons or other similar basic building blocks,
    and the ones that we will learn about in this book are based on perceptrons. One
    of the most popular ANN models is the MLP, which we will use in this book. The
    motivation for using perceptrons in an ANN is that, instead of using one single
    perceptron for classification, what if we used many of them? Take a look at the
    following screenshot:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络（ANN）是基于感知器或其他类似基本构建块的模型，本书中我们将学习的就是基于感知器的模型。最流行的人工神经网络模型之一是多层感知器（MLP），我们将在本书中使用它。使用感知器作为ANN的一部分的动机是，为什么不使用多个感知器，而不是仅仅使用一个感知器进行分类呢？请看以下截图：
- en: '![](img/6f03b677-a86e-4f32-b98d-7818a77e2a6f.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f03b677-a86e-4f32-b98d-7818a77e2a6f.png)'
- en: 'Here, we have three perceptrons and we notice that we have a different bias
    for each perceptron. But the values for our features will be the same in all cases.
    If we use three perceptrons, we will get three output values, but we know that
    this is a binary classification problem so we need only one output. So, now that
    we have three output values, we can combine them or we can view these output values
    as input values for another perceptron. Take a look at the following screenshot:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有三个感知器，并注意到每个感知器都有不同的偏置项。但我们的特征值在所有情况下都是相同的。如果我们使用三个感知器，我们将得到三个输出值，但我们知道这是一个二分类问题，因此我们只需要一个输出。现在我们有了三个输出值，可以将它们结合起来，或者将这些输出值视为另一个感知器的输入。请看以下截图：
- en: '![](img/8a52b180-965c-47c4-993d-f34100c6e829.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a52b180-965c-47c4-993d-f34100c6e829.png)'
- en: 'As you can see in the following screenshot, we can take the output values from
    the preceding perceptrons and fit them as input values to another perceptron,
    and this perceptron will give us the output. So, this is the intuition on how
    to build neural networks or MLPs, and this is an example of an ANN:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们可以将前面感知器的输出值作为输入值提供给另一个感知器，后者将输出结果。因此，这就是构建神经网络或多层感知器（MLP）的直观理解，下面是一个人工神经网络（ANN）的示例：
- en: '![](img/9d89276a-5c73-444d-a881-a2659c7699bc.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d89276a-5c73-444d-a881-a2659c7699bc.png)'
- en: 'In the preceding screenshot, we have the following three layers of an MLP:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们展示了一个多层感知器（MLP）的三层结构：
- en: '**Input Layer**: In this layer, you have the original data or the training
    data that you will use to train this model'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：在此层中，您将拥有原始数据或用于训练此模型的训练数据'
- en: '**Hidden Layer**: This middle layer is the output from the preceding perceptron,
    which is used as the input for the next perceptron'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：该中间层是前一个感知器的输出，作为下一个感知器的输入'
- en: '**Output Layer**:In this layer, you have the output that you get from the network'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：在此层中，您将得到从网络获得的输出'
- en: 'The following screenshot is another way to visualize the same ANN:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是另一种可视化相同人工神经网络（ANN）的方法：
- en: '![](img/d0c4a68a-1a43-4bc2-96a2-719ca85c0389.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0c4a68a-1a43-4bc2-96a2-719ca85c0389.png)'
- en: This is a more compact way to visualize it, but it's actually the same network.
    So, instead of having three biases, we add one constant feature, **1**, for every
    observation. This value of **1** gets multiplied by the different biases and goes
    as input to the neurons in our hidden layer. The value of **x1** gets multiplied
    by some weight and goes as input for the next neurons, and the same happens with
    the value of **x2**. Then, the result of the neurons in the hidden layer is used
    as input for the last perceptron in our network, which is the overall output.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种更紧凑的可视化方式，但它实际上是相同的网络。所以，除了使用三个偏置项外，我们为每个观察值添加了一个常数特征**1**。该值**1**会与不同的偏置相乘，并作为输入提供给隐藏层的神经元。**x1**的值会与某些权重相乘，并作为下一个神经元的输入，**x2**的值也是如此。然后，隐藏层神经元的结果将作为最后一个感知器的输入，得出整体输出。
- en: Elements of a deep neural network model
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络模型的元素
- en: 'The motivation for **deep neural networks** (**DNNs**) is similar, and the
    question here is, instead of using one single hidden layer, what if we use many
    hidden layers? So in that case, our model will look similar to the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度神经网络（DNN）**的动机是类似的，这里问的是，如果我们使用多个隐藏层，而不是一个隐藏层，会怎样？在这种情况下，我们的模型将看起来类似于以下结构：'
- en: '![](img/0d594d3c-f5ca-4433-af2c-1299e12ae028.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d594d3c-f5ca-4433-af2c-1299e12ae028.png)'
- en: Here, we have the same input layer. However, in this case, we will have many
    hidden layers and the output layer will stay the same. The key thing here is the
    hidden part of the network, the hidden layers; instead of having just one, we
    have many hidden layers and this is called a **DNN**.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有相同的输入层。然而，在这种情况下，我们将有许多隐藏层，而输出层保持不变。这里的关键是网络的隐藏部分，即隐藏层；我们不再只有一个隐藏层，而是有许多隐藏层，这被称为**DNN**。
- en: Deep learning
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: 'Deep learning is a set of machine learning models based on neural networks
    and the associated techniques to train such models using data. There are many
    deep learning models. They are a class of machine learning algorithm with the
    following characteristics:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是一类基于神经网络的机器学习模型及其相关技术，用于使用数据训练此类模型。深度学习模型有很多种。它们是一类具有以下特点的机器学习算法：
- en: These models use a set of many layers of nonlinear processing units, which can
    perform abstract feature extraction and transformation
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些模型使用许多非线性处理单元的多层结构，可以进行抽象特征提取和转换。
- en: These models use some form of gradient descent for training through backpropagation
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些模型使用某种形式的梯度下降，通过反向传播进行训练。
- en: They usually need a lot of data and a lot of computational power for these models
    to perform very well
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些模型通常需要大量数据和大量计算能力才能表现得非常好。
- en: These models are now considered state-of the-art for many applications such
    as computer vision, speech recognition, and game playing
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些模型现在被认为是许多应用（如计算机视觉、语音识别和游戏）中的最先进技术。
- en: Elements of an MLP model
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP模型的元素
- en: 'There are a lot of things to consider when building a deep learning model in
    an multilayer perceptron. You have to consider the architecture, the activation
    function, the optimization algorithm, the `loss` function, the weight initialization
    strategy, the regularization strategy, and the training strategy. We will discuss
    more about them in the following list:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建深度学习模型时，有许多需要考虑的因素，特别是在多层感知机中。你必须考虑架构、激活函数、优化算法、`loss`函数、权重初始化策略、正则化策略和训练策略。我们将在以下列表中进一步讨论它们：
- en: '**Architecture**: The first element that you need to consider when building
    deep learning models is the architecture of your MLP. When we say architecture,
    we are talking about the number of layers and the number of neurons per layer.
    The number of neurons in the input layer is determined by the number of features
    that you have in your dataset. The same thing is true for the number of output
    values. So, they are basically determined by your problem in a classification
    setting. The number of output values is usually the number of classes in your
    classification problem, and in a regression problem you will have only one output
    in your output layer. The choice that you have to make is how many hidden layers
    you are going to use and the number of neurons per hidden layer. There are not
    easy rules to set these numbers; in practice, what we do is we use a few layers
    at first. If a few layers don''t work, maybe we add more layers, and the number
    of neurons for each layer is a number between the number of input values and the
    number of outputs, `[n_inputs, n_outputs]`.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构**：在构建深度学习模型时，第一个需要考虑的元素是你的MLP架构。当我们说架构时，我们指的是层数和每层的神经元数量。输入层的神经元数量由数据集中你所拥有的特征数量决定。输出值的数量也是如此。所以，它们基本上由你的问题在分类设置中来决定。输出值的数量通常是你分类问题中的类别数，在回归问题中，输出层只会有一个输出。你需要做出的选择是使用多少隐藏层以及每个隐藏层的神经元数量。没有简单的规则来设置这些数字；在实践中，我们通常先使用几个层。如果几个层不起作用，也许我们会增加更多的层，而每层的神经元数量通常是在输入值数量和输出数量之间的一个数字，`[n_inputs,
    n_outputs]`。'
- en: This is just a rule of thumb. However, there are more formal methods to choose
    the number of hidden layers and the number of neurons, and researchers are constantly
    trying to come up with better methods for choosing these values.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个经验法则。然而，还有一些更正式的方法可以选择隐藏层的数量和神经元的数量，研究人员也在不断努力提出更好的方法来选择这些值。
- en: '**Activation function**: The activation function is the function that is used
    in every neuron in the hidden layers. There are many choices; **sigmoid** was
    the first function used when these models were developed, but then researchers
    found that there are many problems with using this function, so they came up with
    other activation functions such as the **rectified Linear Unit** (**ReLU**), the **hyperbolic
    tangent**, the **leaky ReLU**, and some other choices that we will use in the
    examples as we progress.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**：激活函数是每个隐藏层神经元中使用的函数。有很多选择；**sigmoid** 是这些模型最早使用的函数，但后来研究人员发现使用这个函数存在很多问题，因此他们提出了其他激活函数，如**修正线性单元**（**ReLU**）、**双曲正切**、**漏泄ReLU**，以及一些其他选择，我们将在接下来的示例中使用这些函数。'
- en: '**Optimization algorithm**: This is the algorithm that will be used to learn
    the weights of the networks. Each algorithm that you choose has different hyperparameters
    that need to be chosen by you, the modeler. The most basic algorithm to train
    these networks is **gradient descent**. However, gradient descent can be slow
    and also has some problems, so researchers have come up with other algorithms
    such as **momentum optimizers**, **AdaGrad**, **RMSProp**, and the **Adam** moment
    algorithm. In TensorFlow, we have a lot of algorithms that we can choose from,
    including the Adam moment algorithm, and this is actually the one that we are
    going to use in the examples.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化算法**：这是用于学习网络权重的算法。每个你选择的算法都有不同的超参数，需要由你，作为建模者来选择。训练这些网络的最基本算法是**梯度下降**。然而，梯度下降可能较慢，并且存在一些问题，因此研究人员提出了其他算法，如**动量优化器**、**AdaGrad**、**RMSProp**
    和 **Adam** 动量算法。在TensorFlow中，我们有很多算法可以选择，包括Adam动量算法，实际上我们将在示例中使用的就是这个算法。'
- en: '**Loss function**: This is the function that will produce the quantity that
    will be minimized by the optimizer. The choice of loss function depends on the
    problem. If we are doing a regression problem, you can choose the mean squared
    error or the mean pairwise squared error. For classification problems, there are
    more choices such as cross entropy, square loss, and hinge loss. This is similar
    to trial and error; sometimes, one loss function will work for your problem and
    sometimes it will not. So, this is why you have to consider a lot of different
    loss functions. However, keep in mind that the loss function will produce the
    quantity that will be used for the optimization algorithm to adjust the different
    weights for the different perceptrons that will be part of your network. Hence,
    this is the function that will produce the quantity, and the goal of the optimizer
    is to make this quantity as small as possible.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**损失函数**：这是一个函数，它会生成一个量，这个量将被优化器最小化。损失函数的选择取决于问题。如果我们处理的是回归问题，可以选择均方误差或均方成对误差。对于分类问题，有更多的选择，比如交叉熵、平方损失和铰链损失。这就像试错一样；有时候，一个损失函数适用于你的问题，而有时候则不适用。因此，你必须考虑很多不同的损失函数。然而，记住，损失函数会生成一个量，这个量将被优化算法用来调整网络中不同感知机的不同权重。因此，这是生成量的函数，而优化器的目标是使这个量尽可能小。'
- en: '**Weight initialization strategy**: The weights for each perceptron in your
    network must be initialized with some values, and these values will be progressively
    changed by the optimization algorithm to minimize the loss. There are many ways
    in which you can initialize these values. You can initialize with all zeros. For
    many years, researchers used to initialize using a random normal distribution
    but, in recent years, researchers have come up with better choices, including
    Xavier initialization and He initialization.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重初始化策略**：网络中每个感知机的权重必须初始化为一些值，这些值将被优化算法逐步调整，以最小化损失。初始化这些值的方法有很多种。你可以选择初始化为零。多年来，研究人员通常使用随机正态分布进行初始化，但近年来，研究人员提出了更好的选择，包括Xavier初始化和He初始化。'
- en: '**Regularization strategy**: This is an optional but highly recommended function
    because deep learning models tend to overfit data due to the quantity of parameters
    that they calculate. You can use many choices, including the L1 regularization,
    L2 regularization, and dropout regularization strategies. In this book, we are
    not going to use regularization in our examples, but keep in mind that, if you
    want to build really effective deep learning models, you will very likely need
    a regularization strategy.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化策略**：这是一种可选但强烈推荐的功能，因为深度学习模型由于计算的参数数量，往往会出现过拟合的情况。你可以使用多种选择，包括L1正则化、L2正则化和Dropout正则化策略。在本书中，我们的示例中不会使用正则化，但请记住，如果你想构建真正有效的深度学习模型，正则化策略很可能是必需的。'
- en: '**Training strategy**: The training strategy refers to the way the data will
    be presented to the training algorithm. This is not part of the model itself,
    but it will have an influence on the results and the performance of the model.
    When talking about training deep learning models, you will hear the word epoch.
    One epoch is one pass of all training examples through the network. In these deep
    learning models, you will have to present the data to the network many times so
    the network can learn the best parameters for the model. There is another concept
    here: batch size. This is the number of elements presented simultaneously to the
    training algorithm. So in the case of deep learning models, we don''t present
    the whole training dataset to the model. What we do is we present batches of the
    dataset and, in each batch, we send just a few examples, maybe 100 or 50, and
    this is the way we train deep learning models. Now, you can use epoch and batch
    size to calculate the number of iterations that you will have in your model, and
    this is the number of training steps, which is the number of adjustments that
    the optimization algorithm makes to the weight in your model. So, for example,
    if you have 1,000 training examples and the batch size that you will use is 100,
    it will take 10 iterations to complete one epoch. You can get the total number
    of iterations with the following formula:'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练策略**：训练策略指的是数据呈现给训练算法的方式。这不是模型本身的一部分，但它会影响模型的结果和性能。在谈到训练深度学习模型时，你会听到“epoch”这个词。一个epoch就是所有训练样本通过网络的一次传递。在这些深度学习模型中，你需要多次将数据输入网络，以便网络可以学习到模型的最佳参数。这里还有一个概念：批次大小（batch
    size）。这是指同时呈现给训练算法的元素数量。因此，在深度学习模型中，我们不会将整个训练数据集一次性提供给模型。我们做的是将数据集分成批次，在每个批次中，我们仅发送一些样本，可能是100个或50个，这就是我们训练深度学习模型的方式。现在，你可以使用epoch和批次大小来计算模型中的迭代次数，而迭代次数就是训练步骤的数量，也就是优化算法对模型中的权重进行调整的次数。例如，如果你有1,000个训练样本，并且你使用的批次大小是100，那么完成一个epoch需要10次迭代。你可以使用以下公式来计算总迭代次数：'
- en: '![](img/c5dca712-14af-4483-8080-112349aedbb5.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5dca712-14af-4483-8080-112349aedbb5.png)'
- en: 'So, there are a lot of decisions that you have to make as a modeler. These
    are very complex models and they can be very tricky to train. So, here is some
    guidance to consider before you start using these models:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作为建模者，你需要做出许多决策。这些模型非常复杂，训练起来可能非常棘手。所以，在你开始使用这些模型之前，以下是一些需要考虑的指导：
- en: Because of the number of choices that we have in these models, they can be very
    tricky to build. So, they shouldn't be your first choice when trying to do predictions.
    Always begin with simpler and more understandable models, and then, if those models
    don't work, move to more complex models.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于这些模型中有很多选择，它们的构建可能非常棘手。因此，当尝试进行预测时，它们不应是你的首选。总是先从简单且易于理解的模型开始，如果这些模型不起作用，再转向更复杂的模型。
- en: There are best practices for all of the choices that we have seen, but you need
    more knowledge about these elements if you want to build effective deep learning
    models.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于我们看到的所有选择，都有最佳实践，但如果你想构建有效的深度学习模型，你需要更多关于这些元素的知识。
- en: For these models to perform really well, you need a lot of data. So, you cannot
    use these models with very small datasets.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了让这些模型表现得很好，你需要大量的数据。因此，你不能在非常小的数据集上使用这些模型。
- en: Learn more about the theory of these models to understand how to use them better.
    So if you really want to use these models for solving real-world problems, learning
    more about the theory behind these models is a must.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解更多关于这些模型的理论，以便更好地使用它们。所以，如果你真的想用这些模型来解决实际问题，深入学习这些模型背后的理论是必须的。
- en: Introduction to TensorFlow
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow简介
- en: TensorFlow is an open source software library for numerical computation using
    data flow graphs. The concept of a computational graph is very important in TensorFlow
    and was specially designed for creating deep learning models. This library allows
    developers to deploy computations to one or more CPUs or GPUs in a desktop, a
    server, or even in mobile devices. This library was originally developed by researchers
    and engineers working at Google. It was open sourced in 2015 and, since then,
    it has become one of the major libraries in the machine learning world.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是一个开源软件库，用于通过数据流图进行数值计算。计算图的概念在 TensorFlow 中非常重要，专为创建深度学习模型而设计。这个库允许开发者将计算任务部署到一个或多个
    CPU 或 GPU 上，可以是台式机、服务器，甚至是移动设备。这个库最初由谷歌的研究人员和工程师开发，并于 2015 年开源。从那时起，它已经成为机器学习领域的主要库之一。
- en: 'TensorFlow provides multiple APIs, and they can be categorized into the following
    two broad types:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 提供了多个 API，通常可以分为以下两大类：
- en: '**Low level**: Also known as TensorFlow Core, this is the lowest-level API.
    This API gives us complete programming control and is aimed at researchers and
    users who need a high degree of flexibility when building their deep learning
    models.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低级**：也称为 TensorFlow Core，这是最低级别的 API。这个 API 为我们提供了完全的编程控制，旨在为研究人员和需要在构建深度学习模型时拥有高度灵活性的用户提供服务。'
- en: '**High level**: High-level APIs such as `tf.contrib.learn`, `keras`, and TF-Slim are
    typically easier to use. They take care of repetitive tasks and low-level details
    that, as a high-level user, you don''t need to worry about. They are designed
    for the fast implementation of commonly used models.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级**：高级 API，例如 `tf.contrib.learn`、`keras` 和 TF-Slim，通常更容易使用。它们会处理重复的任务和低级细节，作为高级用户，你无需担心这些细节。它们旨在快速实现常用的模型。'
- en: TensorFlow installation
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 安装
- en: 'Now, in preparation for our installation, we will create a new virtual environment
    in Anaconda. We can do so by using the following instructions:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了准备我们的安装，我们将在 Anaconda 中创建一个新的虚拟环境。我们可以通过以下指令来完成：
- en: We open the Anaconda prompt.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打开 Anaconda 提示符。
- en: 'We type the following command line for creating a new virtual environment and
    pass the name of the environment with `anaconda`, which will install all of the
    packages that come with Anaconda:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们输入以下命令行来创建一个新的虚拟环境，并使用 `anaconda` 传递环境名称，这样会安装 Anaconda 中包含的所有包：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here `apa` stands for advanced predictive analytics. Installation can take some
    time depending on your internet speed.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 `apa` 代表高级预测分析。根据你的网络速度，安装可能需要一些时间。
- en: 'Once the installation has been completed, type `activate apa` to activate the
    new virtual environment. Here is a screenshot of the Anaconda prompt, showing
    the installation of Anaconda packages:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，输入 `activate apa` 来激活新的虚拟环境。以下是 Anaconda 提示符的屏幕截图，显示了 Anaconda 包的安装过程：
- en: '![](img/165875cf-3aff-4480-9425-a511a15e8cb8.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/165875cf-3aff-4480-9425-a511a15e8cb8.png)'
- en: Now, the new virtual environment has been activated and we are ready to install
    TensorFlow inside this new virtual environment.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，新的虚拟环境已经激活，我们准备在这个新的虚拟环境中安装 TensorFlow。
- en: 'But before installing TensorFlow, you must know that there are basically following
    two installations of TensorFlow:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在安装 TensorFlow 之前，你必须知道，基本上有以下两种 TensorFlow 安装方式：
- en: TensorFlow with CPU support only
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅支持 CPU 的 TensorFlow
- en: TensorFlow with GPU support
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 GPU 的 TensorFlow
- en: The second option is usually faster because it uses the GPUs in your computer
    or your devices, but this installation needs **Nvidia** support. You also need
    additional software in order to run this installation and it is a little bit more
    complicated to install.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择通常更快，因为它利用了你电脑或设备中的 GPU，但这种安装需要 **Nvidia** 的支持。你还需要额外的软件才能运行这种安装，而且安装过程稍微复杂一些。
- en: 'Here, for easiness, we will install and use the CPU version as there is no
    difference in writing a program and running it in the CPU or the GPU versions,
    apart from the speed. We use the following line of code to install TensorFlow
    in our system:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，为了方便起见，我们将安装并使用 CPU 版本，因为在编写程序和运行程序时，CPU 版本与 GPU 版本没有区别，唯一的差别是速度。我们使用以下代码行来安装
    TensorFlow 到我们的系统中：
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'On running the code, the installation of TensorFlow will be initiated and,
    once the installation is completed, you will see the following output on your
    screen:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，TensorFlow 的安装将会启动，安装完成后，你将在屏幕上看到以下输出：
- en: '![](img/095eddc0-a439-417a-b2c0-da57bc00ed32.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/095eddc0-a439-417a-b2c0-da57bc00ed32.png)'
- en: 'Now, we will start a Python shell to test the installation by performing the
    following steps:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将启动一个Python shell，通过执行以下步骤来测试安装：
- en: We type `python` to start the Python shell.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们输入`python`启动Python shell。
- en: We use `import tensorflow as tf` to import TensorFlow into our Python shell.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`import tensorflow as tf`将TensorFlow导入到我们的Python shell中。
- en: We run `hello = tf.constant("Hello")`; this will create a constant named `hello`.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们运行`hello = tf.constant("Hello")`；这将创建一个名为`hello`的常量。
- en: We create a session using `sess = tf.Session()`.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`sess = tf.Session()`创建一个会话。
- en: If you see similar warning messages to the ones in the following screenshot,
    you can ignore them, as they are just telling you that you could install with
    different options so TensorFlow may run faster.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到类似以下截图中的警告消息，可以忽略它们，因为它们只是告诉你可以通过不同的选项进行安装，从而让TensorFlow运行得更快。
- en: 'Let''s print the result of `hello` by running the constant within the session
    using `print(sess.run(hello))`:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过在会话中运行常量`print(sess.run(hello))`来打印`hello`的结果：
- en: '![](img/06b5fd15-2128-45b1-a376-648c9ec656a7.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06b5fd15-2128-45b1-a376-648c9ec656a7.png)'
- en: If you get a result of `Hello`, similar to this screenshot, it means that our
    installation is correct. So, now we are ready to use TensorFlow to build some
    models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你得到一个类似于`Hello`的结果，如截图所示，这意味着我们的安装是正确的。所以，现在我们准备使用TensorFlow来构建一些模型。
- en: Core concepts in TensorFlow
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的核心概念
- en: 'There are some major concepts that we need to understand before actually using
    the `tensorflow` library. The following are the concepts that we will cover in
    this book:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际使用`tensorflow`库之前，有一些重要的概念需要理解。以下是本书将涵盖的概念：
- en: Tensors
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量
- en: Computational graphs
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算图
- en: Sessions
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 会话
- en: Variables
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量
- en: Placeholders
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 占位符
- en: Constants
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常量
- en: Tensors
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量
- en: 'A **tensor** is the central unit of data in TensorFlow. A tensor consists of
    a set of primitive values shaped into an array of any number of dimensions. It
    is basically a multidimensional array similar to a NumPy array. The number of
    dimensions defines the rank of a tensor. Let''s see some of the following examples:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**张量**是TensorFlow中数据的核心单元。张量由一组原始值组成，这些值被组织成一个任意维度的数组。它基本上是一个多维数组，类似于NumPy数组。维度的数量定义了张量的秩。我们来看一些以下的示例：'
- en: '`3`: If we have a single number, the tensor will be considered a rank `0` tensor.
    This can be a scalar with `shape[]`.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3`：如果我们有一个单一的数字，那么张量将被视为秩为`0`的张量。这可以是一个具有`shape[]`的标量。'
- en: '`[2., 2., 1.]`: If we have a vector, it will be considered a rank `1` tensor,
    so this is what we call a vector of shape `3` because it has three elements.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[2., 2., 1.]`：如果我们有一个向量，它将被视为秩为`1`的张量，因此这就是我们所谓的形状为`3`的向量，因为它有三个元素。'
- en: '`[[9., 5., 3.], [4., 5., 7]]`: A matrix with shape `[2, 3]` would be a rank
    `2` tensor.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[[9., 5., 3.], [4., 5., 7]]`：一个形状为`[2, 3]`的矩阵将是一个秩为`2`的张量。'
- en: '`[[[8., 3.]], [[7., 9.,]]]`: A matrix with shape `[2, 1, 2]` would be a rank
    `3` tensor, as you can see in the outermost level we have two elements, then in
    the next level we have only one element, and in the last dimension, we have two
    elements. That''s why we have `2`, `1`, and `2` as the values and these are all
    tensors.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[[[8., 3.]], [[7., 9.,]]]`：一个形状为`[2, 1, 2]`的矩阵将是一个秩为`3`的张量，正如你在最外层看到的，我们有两个元素，然后在下一层我们只有一个元素，最后一维中有两个元素。这就是为什么我们有`2`、`1`和`2`这些值，它们都是张量。'
- en: Computational graph
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算图
- en: 'A computational graph is a series of TensorFlow operations, also known as **ops**,
    arranged into a graph of nodes. The following two principle steps are used by
    TensorFlow Core:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图是由一系列TensorFlow操作（也称为**操作**）组成的，这些操作被排列成一个节点图。TensorFlow核心使用以下两个主要步骤：
- en: Define a computational graph
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义计算图
- en: Run the computational graph
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行计算图
- en: 'Let''s try to understand this concept with a very simple example. Let''s say
    that you have a function with two variables, **x** and **y** as shown in the following
    screenshot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个非常简单的例子来理解这个概念。假设你有一个包含两个变量**x**和**y**的函数，如下截图所示：
- en: '![](img/5212c92e-d91d-4024-ac39-d753f31f3926.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5212c92e-d91d-4024-ac39-d753f31f3926.png)'
- en: 'We will use the preceding formula to calculate or to build a computational
    graph for the actual value of this function when you pass the values **3** and
    2 for **x** and **y** respectively:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用前面的公式来计算或构建一个计算图，以获得当你分别传入**x**和**y**的值**3**和**2**时该函数的实际值：
- en: '![](img/4127c2a0-3c52-4a52-85a3-f1f9e63d87cf.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4127c2a0-3c52-4a52-85a3-f1f9e63d87cf.png)'
- en: 'Now, let''s build a computational graph for actually getting the result from
    this computation model:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为实际从这个计算模型中获取结果构建一个计算图：
- en: '![](img/ef618ce1-282e-41fd-9b3f-55b6317130f4.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ef618ce1-282e-41fd-9b3f-55b6317130f4.png)'
- en: In the preceding screenshot, we see the values that flow through the computational
    graph to different nodes in the graph. So, in the first node, the value **3**
    gets assigned to **x** and, in the other node, the value **2** gets assigned to
    **y**. Now, the value of **x** flows to an operation node where it gets squared,
    and the result of that node flows to another operation where it gets multiplied
    to the value of **y**. We also have another node, where the value of **y** gets
    multiplied by **4**. The result of the **x** and **y** multiplication node and
    the result of the **y** multiplication node flow to the final node, which is the
    addition node, which gives us the final output **26**. So this is essentially
    how TensorFlow works. What flows between nodes are tensors.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，我们看到值流经计算图到达图中的不同节点。因此，在第一个节点中，值**3**被赋给**x**，在另一个节点中，值**2**被赋给**y**。现在，**x**的值流向一个操作节点，在那里它被平方，而该节点的结果流向另一个操作，结果与**y**的值相乘。我们还有另一个节点，其中**y**的值与**4**相乘。**x**和**y**乘法节点的结果以及**y**乘法节点的结果流向最后一个节点，即加法节点，最终输出**26**。所以，这基本上就是
    TensorFlow 的工作原理。在节点之间流动的是张量。
- en: 'There are other following objects that we use in TensorFlow:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在 TensorFlow 中使用的其他对象：
- en: '**Session**: A session is an object that encapsulates the environment in which
    operation objects are executed. So, sessions are objects that place operations
    onto devices such as CPUs or GPUs.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**会话**：会话是一个封装执行操作对象环境的对象。因此，会话是将操作放置到设备（如 CPU 或 GPU）上的对象。'
- en: '**Placeholders**: A placeholder is a promise to provide a value later. These
    objects are usually used to provide training and testing values in machine learning
    models.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**占位符**：占位符是一个承诺，表示稍后会提供一个值。这些对象通常用于在机器学习模型中提供训练和测试值。'
- en: '**Variables**: These are objects that are initialized with a value, and that
    value can change during the execution of the graph. Typically, they are used as
    trainable variables in machine learning models.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变量**：这些是初始化时有值的对象，并且该值可以在图执行过程中改变。通常，它们用作机器学习模型中的可训练变量。'
- en: '**Constants**: Constants are objects whose values never change.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**常量**：常量是那些值永远不变的对象。'
- en: 'To have a better understanding of these object concepts, let''s see an example.
    First, we will import the required libraries by executing the following code snippet:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这些对象概念，让我们看一个例子。首先，我们将通过执行以下代码片段来导入所需的库：
- en: '[PRE3]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then define some TensorFlow objects, placeholders, and a constant by executing
    the following code snippet:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过执行以下代码片段来定义一些 TensorFlow 对象、占位符和常量：
- en: '[PRE4]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we define a placeholder called `x` and another placeholder called `y`.
    You have to explicitly give the type of object that you will use in TensorFlow,
    which we have in our example as `float32`. We then define a constant, `c` ,whose
    value is `5`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个名为`x`的占位符和另一个名为`y`的占位符。你必须明确给出在 TensorFlow 中将要使用的对象类型，在我们的示例中是`float32`。然后我们定义了一个常量`c`，其值为`5`。
- en: 'After you create these objects, if you try to print them, you will not see
    the value of the object, but it will show the type of the object as shown in the
    following screenshot:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这些对象后，如果你尝试打印它们，你将不会看到对象的值，而是会显示对象的类型，如下截图所示：
- en: '![](img/0760ad7d-7c77-46e1-b4e7-fd8c6607998f.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0760ad7d-7c77-46e1-b4e7-fd8c6607998f.png)'
- en: 'Now, let''s implement the following function with our placeholders:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用我们的占位符来实现以下函数：
- en: '![](img/39bb9a13-593d-4fd6-a16f-23b7d66715fb.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39bb9a13-593d-4fd6-a16f-23b7d66715fb.png)'
- en: 'We will use the placeholders that we just created to define the different nodes
    for our graph by executing the following code lines:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用刚刚创建的占位符，通过执行以下代码行来定义我们图中的不同节点：
- en: '[PRE5]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Again, if you try to print the values of these objects, you will get the object
    type and not the values as shown in the following screenshot:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果你尝试打印这些对象的值，你将得到对象类型而不是值，如下截图所示：
- en: '![](img/a9eb5f4e-f40b-4fa1-8adf-bc449ca8d24d.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a9eb5f4e-f40b-4fa1-8adf-bc449ca8d24d.png)'
- en: 'So, to perform the calculations for these objects, you must create a session
    object and then run all of the objects inside a session:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了对这些对象进行计算，你必须创建一个会话对象，然后在会话内运行所有对象：
- en: '![](img/8ebba675-9c71-4e31-b536-c84c7098babd.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ebba675-9c71-4e31-b536-c84c7098babd.png)'
- en: 'If you are doing some computations, you don''t need to define the computational
    graph, as TensorFlow will do this behind the scenes. So, let''s say that you want
    to calculate `f` and we print the value, it will still give the object type. But
    to actually see the value of `f` when you perform the computation, we will run
    the function in a session object again:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在进行一些计算，你不需要定义计算图，因为TensorFlow会在后台自动完成这一过程。所以，假设你想计算`f`并打印出其值，它仍然会给出对象类型。但要实际看到`f`的值，我们将在会话对象中再次运行该函数：
- en: '![](img/e94fb92f-b91a-44bd-9843-10f8d1c6acb7.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e94fb92f-b91a-44bd-9843-10f8d1c6acb7.png)'
- en: 'There are two ways in which you can run objects in TensorFlow. There are other
    ways, but these are the basic and most common ways you can run objects. You can
    use the `run()` method from a session or you can use the `eval()` method from
    the tensor:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过两种方式在TensorFlow中运行对象。还有其他方式，但这些是你可以运行对象的基本且最常见的方式。你可以使用会话中的`run()`方法，或者你可以使用张量中的`eval()`方法：
- en: '![](img/3bd8b8d8-0c08-4461-a214-2376e38ce378.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3bd8b8d8-0c08-4461-a214-2376e38ce378.png)'
- en: As we can see, we created a session using the `with` statement and ran those
    two methods inside this statement.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们使用`with`语句创建了一个会话，并在这个语句中运行了这两个方法。
- en: 'Now, we will build a basic linear model. We will have TensorFlow guess the
    best values for the `b` and `w` parameters shown in the following screenshot:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将构建一个基本的线性模型。我们将让TensorFlow猜测下面截图中显示的`b`和`w`参数的最佳值：
- en: '![](img/270794dc-53bd-4673-8207-ca8fe98c51ec.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/270794dc-53bd-4673-8207-ca8fe98c51ec.png)'
- en: 'In the previous equation, the value of `w` is `5` and `b` is `1`. We will use
    these values for training and plot the values on a scatter plot:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，`w`的值是`5`，`b`的值是`1`。我们将使用这些值进行训练并将它们绘制在散点图上：
- en: '![](img/8ed0ddf5-88be-4207-9482-3e5d43cf1f3b.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8ed0ddf5-88be-4207-9482-3e5d43cf1f3b.png)'
- en: As you can see, we have the linear relationship between the two values.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们在这两个值之间建立了线性关系。
- en: 'We will now initiate the variable objects, `w` and `b` ,with the value `0`,
    and they will be our trainable parameters. The placeholders are usually the objects
    that we use to pass the data, so we will create two placeholders, `x` and `y`,
    and now the linear model will be one of the nodes in our computational graph.
    Then, we will define a `loss` function, which will be used by the optimizer to
    actually change the values of our variable. Every time we run the training operation,
    the optimizer will adjust the values of `w` and `b` in order to minimize the loss.
    We will then initialize the variables and create a session to run the `init` initializer
    node as shown in the following screenshot:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将初始化变量对象`w`和`b`，并将其值设为`0`，它们将成为我们的可训练参数。占位符通常是我们用来传递数据的对象，因此我们将创建两个占位符，`x`和`y`，现在线性模型将是我们计算图中的一个节点。接着，我们将定义一个`loss`函数，优化器将使用它来实际改变我们变量的值。每次运行训练操作时，优化器都会调整`w`和`b`的值，以最小化损失。然后，我们将初始化这些变量并创建一个会话来运行`init`初始化节点，如下图所示：
- en: '![](img/ba6befb8-56cb-4e28-b2eb-f0a824fec4ac.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ba6befb8-56cb-4e28-b2eb-f0a824fec4ac.png)'
- en: 'Now, we can start training our machine learning model. We will run the training
    operation 20 times, which will make corrections to our values of `w` and `b` to
    minimize the loss:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始训练我们的机器学习模型。我们将运行训练操作20次，这将对`w`和`b`的值进行修正，以最小化损失：
- en: '![](img/fb974a24-7293-41d3-a968-b0adf65204e8.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb974a24-7293-41d3-a968-b0adf65204e8.png)'
- en: As we see, after the first iteration, the optimizer corrected the values of
    `w` and `b`, which is also carried out in every iteration.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在第一次迭代后，优化器修正了`w`和`b`的值，这在每次迭代中都会发生。
- en: We can also do this using some linear algebra, but remember that the goal of
    machine learning is to actually learn the parameters from the data, and in this
    case, we have run our first machine learning model using TensorFlow.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用一些线性代数来实现这一点，但请记住，机器学习的目标实际上是从数据中学习参数，在这种情况下，我们已经使用TensorFlow运行了我们的第一个机器学习模型。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we talked about ANNs, deep learning, and the elements of a
    deep learning model. We then installed TensorFlow and learned about the core concepts
    that we use in TensorFlow.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们讨论了人工神经网络（ANNs）、深度学习以及深度学习模型的要素。接着，我们安装了TensorFlow，并学习了TensorFlow中的核心概念。
- en: In the next chapter, we will perform predictive analytics with TensorFlow and
    deep learning.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用TensorFlow和深度学习进行预测分析。
