- en: Chapter 4. Self-Organizing Maps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。自组织映射
- en: 'In this chapter, we present a neural network architecture that is suitable
    for unsupervised learning: self-organizing maps, also known as Kohonen networks.
    This particular type of neural network is able to categorize records of data without
    any target output or find a representation of the data in a smaller dimension.
    Throughout this chapter, we are going to explore how this is achieved, as well
    as examples to attest to its capacity. The subtopics of this chapter are as follows:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一种适合无监督学习的神经网络架构：自组织映射，也称为Kohonen网络。这种特定类型的神经网络能够对数据记录进行分类，而不需要任何目标输出或找到数据在较小维度上的表示。在本章中，我们将探讨如何实现这一点，以及证明其能力的示例。本章的子主题如下：
- en: Neural networks unsupervised learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络无监督学习
- en: Competitive learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 竞争学习
- en: Kohonen self-organizing maps
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kohonen 自组织映射
- en: One-dimensional SOMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一维SOMs
- en: Two-dimensional SOMs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二维SOMs
- en: Problems solved with unsupervised learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习解决的问题
- en: Java implementation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java 实现
- en: Data visualization
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: Practical problems
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际问题
- en: Neural networks unsupervised learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络无监督学习
- en: In [Chapter 2](ch02.xhtml "Chapter 2. Getting Neural Networks to Learn"), *Getting
    Neural Networks to Learn* we've been acquainted with unsupervised learning, and
    now we are going to explore the features of this learning paradigm in more detail.
    The mission of unsupervised learning algorithms is to find patterns in datasets,
    where the parameters (weights in the case of neural networks) are adjusted without
    any error measure (there are no target values).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.xhtml "第2章。让神经网络学会学习")中，我们在《让神经网络学会学习》中已经了解了无监督学习，现在我们将更详细地探讨这种学习范式的特征。无监督学习算法的使命是在数据集中找到模式，其中参数（在神经网络的情况下为权重）在没有误差度量（没有目标值）的情况下进行调整。
- en: While the supervised algorithms provide an output comparable to the dataset
    that was presented, the unsupervised algorithms do not need to know the output
    values. The fundamentals of unsupervised learning are inspired by the fact that,
    in neurology, similar stimuli produce similar responses. So applying this to artificial
    neural networks, we can say that similar data produces similar outputs, so those
    outputs can be grouped or clustered.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然监督算法提供与所提供数据集相当的输出，但无监督算法不需要知道输出值。无监督学习的基础是受神经学事实的启发，即，在神经学中，相似的刺激产生相似的响应。因此，将此应用于人工神经网络，我们可以这样说，相似的数据产生相似的结果，因此这些结果可以被分组或聚类。
- en: Although this learning may be used in other mathematical fields, such as statistics,
    its core functionality is intended and designed for machine learning problems
    such as data mining, pattern recognition, and so on. Neural networks are a subfield
    in the machine learning discipline, and provided that their structure allows iterative
    learning, they serve as a good framework to apply this concept to.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种学习可以应用于其他数学领域，例如统计学，但其核心功能旨在为机器学习问题，如数据挖掘、模式识别等设计。神经网络是机器学习学科的一个子领域，只要它们的结构允许迭代学习，它们就提供了一个很好的框架来应用这一概念。
- en: Most of unsupervised learning applications are aimed at clustering tasks, which
    means that similar data points are to be clustered together, while different data
    points form different clusters. Also, one application that unsupervised learning
    is suitable for is dimensionality reduction or data compression, as long as simpler
    and smaller representations of the data can be found among huge datasets.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数无监督学习应用都旨在聚类任务，这意味着相似的数据点将被聚在一起，而不同的数据点将形成不同的簇。此外，无监督学习适合的一个应用是降维或数据压缩，只要在大量数据集中可以找到更简单、更小的数据表示。 '
- en: Unsupervised learning algorithms
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习算法
- en: Unsupervised algorithms are not unique to neural networks, as K-means, expectation
    maximization, and methods of moments are also examples of unsupervised learning
    algorithms. One common feature of all learning algorithms is the absence of mapping
    among variables in the current dataset; instead, one wishes to find a different
    meaning of this data, and that's the goal of any unsupervised learning algorithm.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督算法不仅限于神经网络，K-means、期望最大化以及矩方法也是无监督学习算法的例子。所有学习算法的一个共同特征是当前数据集中变量之间没有映射；相反，人们希望找到这些数据的不同含义，这就是任何无监督学习算法的目标。
- en: While in supervised learning algorithms, we usually have a smaller number of
    outputs, for unsupervised learning, there is a need to produce an abstract data
    representation that may require a high number of outputs, but, except for classification
    tasks, their meaning is totally different than the one presented in the supervised
    learning. Usually, each output neuron is responsible for representing a feature
    or a class present in the input data. In most architectures, not all output neurons
    need to be activated at a time; only a restricted set of output neurons may fire,
    meaning that that neuron is able to better represent most of the information being
    fed at the neural input.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习算法中，我们通常有较少的输出，而对于无监督学习，需要产生一个抽象的数据表示，这可能需要大量的输出，但除了分类任务外，它们的含义与监督学习中的含义完全不同。通常，每个输出神经元负责表示输入数据中存在的特征或类别。在大多数架构中，不是所有输出神经元都需要同时激活；只有一组受限的输出神经元可能会被激活，这意味着该神经元能够更好地表示被馈送到神经网络输入的大部分信息。
- en: Tip
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: One advantage of unsupervised learning over supervised learning is that less
    computational power required by the first for the learning of huge datasets. Time
    consumption grows linearly while for the supervised learning it grows exponentially.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习相对于监督学习的一个优点是，它对学习大量数据集所需的计算能力较低。时间消耗呈线性增长，而监督学习的时间消耗呈指数增长。
- en: 'In this chapter, we are going to explore two unsupervised learning algorithms:
    competitive learning and Kohonen self-organizing maps.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨两种无监督学习算法：竞争学习和Kohonen自组织映射。
- en: Competitive learning
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 竞争学习
- en: 'As the name implies, competitive learning handles a competition between the
    output neurons to determine which one is the winner. In competitive learning,
    the winning neuron is usually determined by comparing the weights against the
    inputs (they have the same dimensionality). To facilitate understanding, suppose
    we want to train a single layer neural network with two inputs and four outputs:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，竞争学习处理输出神经元之间的竞争，以确定哪个是胜者。在竞争学习中，胜者神经元通常是通过比较权重与输入（它们具有相同的维度）来确定的。为了便于理解，假设我们想要训练一个具有两个输入和四个输出的单层神经网络：
- en: '![Competitive learning](img/B05964_04_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![竞争学习](img/B05964_04_01.jpg)'
- en: Every output neuron is then connected to these two inputs, hence for each neuron
    there are two weights.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输出神经元都与这两个输入相连，因此对于每个神经元都有两个权重。
- en: Tip
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: For this learning, the bias is dropped from the neurons, so the neurons will
    process only the weighted inputs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种学习，从神经元中移除了偏差，因此神经元将只处理加权的输入。
- en: The competition starts after the data has been processed by the neurons. The
    winner neuron will be the one whose weights are *near* to the input values. One
    additional difference compared to the supervised learning algorithm is that only
    the winner neuron may update their weights, while the other ones remain unchanged.
    This is the so-called **winner-takes-all** rule. This intention to bring the neuron
    *nearer* to the input that caused it to win the competition.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争是在数据被神经元处理之后开始的。胜者神经元将是其权重与输入值*接近*的那个。与监督学习算法相比的一个额外区别是，只有胜者神经元可以更新其权重，而其他神经元保持不变。这就是所谓的**胜者全得**规则。这种意图是将神经元*更靠近*导致其赢得竞争的输入值。
- en: 'Considering that every input neuron *i* is connected to all output neurons
    *j* through a weight *wij*, in our case, we would have a set of weights:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到每个输入神经元*i*通过权重*wij*与所有输出神经元*j*相连，在我们的情况下，我们会有一组权重：
- en: '![Competitive learning](img/B05964_04_01_01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![竞争学习](img/B05964_04_01_01.jpg)'
- en: 'Provided that the weights of every neuron have the same dimensionality of the
    input data, let''s consider all the input data points together in a plot with
    the weights of each neuron:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设每个神经元的权重与输入数据的维度相同，让我们在一张图中考虑所有输入数据点以及每个神经元的权重：
- en: '![Competitive learning](img/B05964_04_02.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![竞争学习](img/B05964_04_02.jpg)'
- en: 'In this chart, let''s consider the circles as the data points and the squares
    as the neuron weights. We can see that some data points are closer to certain
    weights, while others are farther but nearer to others. The neural network performs
    computations on the distance on the inputs and the weights:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图表中，让我们将圆圈视为数据点，将正方形视为神经元权重。我们可以看到，某些数据点与某些权重更接近，而其他数据点则更远，但更接近其他权重。神经网络在输入和权重之间的距离上执行计算：
- en: '![Competitive learning](img/B05964_04_02_01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![竞争学习](img/B05964_04_02_01.jpg)'
- en: 'The result of this equation will determine how much *stronger* a neuron is
    against its competitors. The neuron whose weight distance to the input is the
    smaller is considered the winner. After many iterations, the weights are driven
    near enough to the data points that give more cause the corresponding neuron to
    win that the changes are either too small or the weights fall in a zig-zag setting.
    Finally, when the network is already trained, the chart takes another shape:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程式的结果将决定一个神经元相对于其竞争对手有多强。权重距离输入较小的神经元被认为是赢家。经过多次迭代后，权重被驱动到足够接近数据点，使得相应的神经元更有可能获胜，以至于变化要么太小，要么权重处于锯齿形设置中。最后，当网络已经训练好时，图表将呈现出另一种形状：
- en: '![Competitive learning](img/B05964_04_03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![竞争学习](img/B05964_04_03.jpg)'
- en: As can be seen, the neurons form centroids surrounding the points capable of
    making the corresponding neuron stronger than its competitors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，神经元围绕能够使相应神经元比竞争对手更强的点形成中心。
- en: In an unsupervised neural network, the number of outputs is completely arbitrary.
    Sometimes only some neurons are able to change their weights, while in other cases,
    all the neurons may respond differently to the same input, causing the neural
    network to never learn. In these cases, it is recommended either to review the
    number of output neurons, or consider another type of unsupervised learning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督神经网络中，输出的数量完全是任意的。有时只有一些神经元能够改变它们的权重，而在其他情况下，所有神经元可能对相同的输入有不同的反应，导致神经网络无法学习。在这些情况下，建议审查输出神经元的数量，或者考虑另一种无监督学习类型。
- en: 'Two stopping conditions are preferable in competitive learning:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在竞争学习中，有两个停止条件是可取的：
- en: 'Predefined number of epochs: This prevents our algorithm from running for a
    longer time without convergence'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预定义的周期数：这防止我们的算法在没有收敛的情况下运行时间过长
- en: 'Minimum value of weight update: Prevents the algorithm from running longer
    than necessary'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重更新的最小值：防止算法运行时间超过必要
- en: Competitive layer
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 竞争层
- en: 'This type of neural layer is particular, as the outputs won''t be necessarily
    the same as its neuron''s outputs. Only one neuron will be fired at a time, thereby
    requiring a special rule to calculate the outputs. So, let''s create a new class
    called `CompetitiveLayer` that will inherit from `OutputLayer` and starting with
    two new attributes: `winnerNeuron` and `winnerIndex`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的神经网络层是特定的，因为输出不一定与神经元的输出相同。一次只有一个神经元被激活，因此需要特殊的规则来计算输出。因此，让我们创建一个名为 `CompetitiveLayer`
    的新类，它将继承自 `OutputLayer` 并从两个新属性开始：`winnerNeuron` 和 `winnerIndex`：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This new class of neural layer will override the method `calc()` and add some
    new particular methods to get the weights:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新的神经网络层将覆盖 `calc()` 方法并添加一些特定的新方法来获取权重：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the next sections, we will define the class Kohonen for the Kohonen neural
    network. In this class, there will be an `enum` called `distanceCalculation`,
    which will have the different methods to calculate distance. In this chapter (and
    book), we'll stick to the Euclidian distance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将定义 Kohonen 类，用于 Kohonen 神经网络。在这个类中，将有一个名为 `distanceCalculation`
    的 `enum`，它将包含不同的距离计算方法。在本章（和本书）中，我们将坚持使用欧几里得距离。
- en: Tip
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: A new class called `ArrayOperations` was created to provide methods that facilitate
    operations with arrays. Functionalities such as getting the index of the maximum
    or minimum or getting a subset of the array are implemented in this class.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了一个名为 `ArrayOperations` 的新类，以提供便于数组操作的函数。例如，获取最大值或最小值的索引或获取数组的一个子集等功能都实现在这个类中。
- en: 'The distance between the weights of a particular neuron and the input is calculated
    by the method `getWeightDistance( )`, which is called inside the `calc` method:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 特定神经元的权重与输入之间的距离是通过 `getWeightDistance( )` 方法计算的，该方法在 `calc` 方法内部被调用：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The method `getNeuronWeights( )` returns the weights of the neuron corresponding
    to the index passed in the array. Since it is simple and to save space here, we
    invite the reader to see the code to check its implementation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`getNeuronWeights( )` 方法返回与数组中传入的索引对应的神经元的权重。由于它很简单，并且为了节省空间，我们邀请读者查看代码以检查其实现。'
- en: Kohonen self-organizing maps
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kohonen 自组织映射
- en: This network architecture was created by the Finnish professor Teuvo Kohonen
    at the beginning of the 80s. It consists of one single layer neural network capable
    of providing a *visualization* of the data in one or two dimensions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种网络架构是由芬兰教授 Teuvo Kohonen 在 80 年代初创建的。它由一个单层神经网络组成，能够在一维或二维中提供数据的 *可视化*。
- en: In this book, we are going to use Kohonen networks also as a basic competitive
    layer with no links between the neurons. In this case, we are going to consider
    it as zero dimension (0-D).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们还将使用 Kohonen 网络作为没有神经元之间链接的基本竞争层。在这种情况下，我们将将其视为零维（0-D）。
- en: Theoretically, a Kohonen Network would be able to provide a 3-D (or even in
    more dimensions) representation of the data; however, in printed material such
    as this book, it is not practicable to show 3-D charts without overlapping some
    data. Thus in this book, we are going to deal only with 0-D, 1-D, and 2-D Kohonen
    networks.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，Kohonen 网络能够提供数据的 3-D（甚至更多维度）表示；然而，在像这本书这样的印刷材料中，不重叠数据就无法展示 3-D 图表，因此在这本书中，我们将只处理
    0-D、1-D 和 2-D Kohonen 网络。
- en: Kohonen **Self-Organizing Maps** (**SOMs**), in addition to the traditional
    single layer competitive neural networks (in this book, the 0-D Kohonen network),
    add the concept of neighborhood neurons. A dimensional SOM takes into account
    the index of the neurons in the competitive layer, letting the neighborhood of
    neurons play a relevant role during the learning phase.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Kohonen **自组织映射**（**SOM**），除了传统的单层竞争神经网络（在本书中，0-D Kohonen 网络）外，还增加了邻域神经元的概念。一维
    SOM 考虑到竞争层中神经元的索引，让神经元在学习阶段发挥相关的作用。
- en: 'An SOM has two modes of functioning: mapping and learning. In the mapping mode,
    the input data is classified in the most appropriate neuron, while in the learning
    mode, the input data helps the learning algorithm to build the *map*. This map
    can be interpreted as a lower-dimension representation from a certain dataset.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: SOM 有两种工作模式：映射和学习。在映射模式下，输入数据被分类到最合适的神经元中，而在学习模式下，输入数据帮助学习算法构建 *映射*。这个映射可以解释为从某个数据集的降维表示。
- en: Extending the neural network code to Kohonen
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展神经网络代码到 Kohonen
- en: 'In our code, let''s create a new class inherited from `NeuralNet`, since it
    will be a particular type of neural network. This class will be called Kohonen,
    which will use the class `CompetitiveLayer` as the output layer. The following
    class diagram shows how these new classes are arranged:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的代码中，让我们创建一个新的类，它继承自 `NeuralNet`，因为它将是一种特定的神经网络类型。这个类将被命名为 Kohonen，它将使用 `CompetitiveLayer`
    类作为输出层。以下类图显示了这些新类的排列方式：
- en: '![Extending the neural network code to Kohonen](img/B05964_04_04.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![扩展神经网络代码到 Kohonen](img/B05964_04_04.jpg)'
- en: 'Three types of SOMs are covered in this chapter: zero-, one- and two-dimensional.
    These configurations are defined in an `enum MapDimension`:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了三种类型的 SOM：零维、一维和二维。这些配置在 `enum MapDimension` 中定义：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The Kohonen constructor defines the dimension of the Kohonen neural network:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Kohonen 构造函数定义了 Kohonen 神经网络的维度：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Zero-dimensional SOM
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零维 SOM
- en: This is the pure competitive layer, where the order of the neurons is irrelevant.
    Features such as neighborhood functions are not taken into account. Only the winner
    neuron weights are affected during the learning phase. The map will be composed
    only of unconnected dots.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个纯竞争层，其中神经元的顺序无关紧要。如邻域函数等特征不被考虑。在学习阶段，只有获胜神经元的权重受到影响。映射将仅由未连接的点组成。
- en: 'The following code snippet define a zero-dimensional SOM:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段定义了一个零维 SOM：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note the value `0` passed in the argument dim (the last of the constructor).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意传递给参数 dim 的值 `0`（构造函数的最后一个参数）。
- en: One-dimensional SOM
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一维 SOM
- en: 'This architecture is similar to the network presented in the last section,
    **Competitive learning**, with the addition of neighborhood amongst the output
    neurons:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构与上一节中介绍的 **竞争学习** 网络类似，增加了输出神经元之间的邻域关系：
- en: '![One-dimensional SOM](img/B05964_04_05.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![一维 SOM](img/B05964_04_05.jpg)'
- en: Note that every neuron on the output layer has one or two neighbors. Similarly,
    the neuron that fires the greatest value updates its weights, but in a SOM, the
    neighbor neurons also update their weights in a smaller rate.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输出层上的每个神经元都有一个或两个邻居。同样，触发最大值的神经元更新其权重，但在 SOM 中，邻域神经元也会以较小的速率更新其权重。
- en: The effect of the neighborhood extends the activation area to a wider area of
    the map, provided that all the output neurons must observe an organization, or
    a path in the one-dimensional case. The neighborhood function also allows for
    a better exploration of the properties of the input space, since it forces the
    neural network to keep the connections between neurons, therefore resulting in
    more information in addition to the clusters that are formed.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 邻域效应将激活区域扩展到地图的更宽区域，前提是所有输出神经元都必须观察到一种组织，或者在一维情况下，观察到一条路径。邻域函数还允许更好地探索输入空间的特点，因为它迫使神经网络保持神经元之间的连接，因此除了形成的聚类之外，还会产生更多信息。
- en: 'In a plot of the input data points with the neural weights, we can see the
    path formed by the neurons:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入数据点和神经权重的关系图中，我们可以看到由神经元形成的路径：
- en: '![One-dimensional SOM](img/B05964_04_06.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![一维SOM](img/B05964_04_06.jpg)'
- en: In the chart here presented, for simplicity, we plotted only the output weights
    to demonstrate how the map is designed in a (in this case) 2-D space. After training
    over many iterations, the neural network converges to a final shape that represent
    all data points. Provided that structure, a certain set of data may cause the
    Kohonen Network to design another shape in the space. This is a good example of
    dimensionality reduction, since a multidimensional dataset when presented to the
    Self-Organizing Map is able to produce one single line (in the 1-D SOM) that summarizes
    the entire dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里展示的图表中，为了简单起见，我们只绘制了输出权重来展示地图是如何在一个（在这种情况下）2-D空间中设计的。经过多次迭代训练后，神经网络收敛到一个最终形状，代表所有数据点。有了这个结构，一组数据可能会使Kohonen网络在空间中设计出另一种形状。这是一个很好的降维例子，因为当多维数据集被展示给自组织映射时，能够产生一条单线（在1-D
    SOM中）来总结整个数据集。
- en: 'To define a one-dimensional SOM, we need to pass the value `1` as the argument
    `dim`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义一维SOM，我们需要将值`1`作为参数`dim`传递：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Two-dimensional SOM
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二维SOM
- en: 'This is the most used architecture to demonstrate the Kohonen neural network
    power in a visual way. The output layer is one matrix containing M x N neurons,
    interconnected like a grid:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最常用的架构，以直观的方式展示Kohonen神经网络的强大功能。输出层是一个包含M x N神经元的矩阵，像网格一样相互连接：
- en: '![Two-dimensional SOM](img/B05964_04_07.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![二维SOM](img/B05964_04_07.jpg)'
- en: 'In the 2-D SOMs, every neuron now has up to four neighbors (in the square configuration),
    although in some representations, the diagonal neurons may also be considered,
    thus resulting in up to eight neighbors. Hexagonal representations are also useful.
    Let''s see one example of what a 3x3 SOM plot looks like in a 2-D chart (considering
    two input variables):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在2-D SOM中，每个神经元现在最多有四个邻居（在正方形配置中），尽管在某些表示中，对角线神经元也可能被考虑，从而最多有八个邻居。六边形表示也很有用。让我们看看一个3x3
    SOM图在2-D图表中的样子（考虑两个输入变量）：
- en: '![Two-dimensional SOM](img/B05964_04_08.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![二维SOM](img/B05964_04_08.jpg)'
- en: 'At first, the untrained Kohonen Network shows a very strange and screwed-up
    shape. The shaping of the weights will depend solely on the input data that is
    going to be fed to the SOM. Let''s see an example of how the map starts to organize
    itself:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，未经训练的Kohonen网络显示出非常奇怪和扭曲的形状。权重的塑造将完全取决于将要馈送到SOM的输入数据。让我们看看地图开始组织自己的一个例子：
- en: Suppose we have the dense data set shown in the following plot:![Two-dimensional
    SOM](img/B05964_04_09.jpg)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们有一个如以下图表所示的密集数据集：![二维SOM](img/B05964_04_09.jpg)
- en: Applying SOM, the 2-D shape gradually changes, until it achieves the final configuration:![Two-dimensional
    SOM](img/B05964_04_10.jpg)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用SOM后，2-D形状逐渐变化，直到达到最终配置：![二维SOM](img/B05964_04_10.jpg)
- en: The final shape of a 2-D SOM may not always be a perfect square; instead, it
    will resemble a shape that could be drawn from the dataset. The neighborhood function
    is one important component in the learning process because it approximates the
    neighbor neurons in the plot, and the structure moves to a configuration that
    is more *organized*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 2-D SOM的最终形状可能不总是完美的正方形；相反，它将类似于可以从数据集中绘制出的形状。邻域函数是学习过程中的一个重要组成部分，因为它近似了图中的邻近神经元，并将结构移动到一个更*有组织*的配置。
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The grid on a chart is just the more used and didactic. There are other ways
    of showing the SOM diagram, such as the U-matrix and the cluster boundaries.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图表上的网格只是更常用和更具教育意义的。还有其他方式来展示SOM图，比如U矩阵和聚类边界。
- en: 2D competitive layer
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二维竞争层
- en: 'In order to better represent the neurons of a 2D competitive layer in a grid
    form, we''re creating the CompetitiveLayer2D class, which inherits from `CompetitiveLayer`.
    In this class, we can define the number of neurons in the form of a grid of M
    x N neurons:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地在网格形式中表示二维竞争层的神经元，我们正在创建 CompetitiveLayer2D 类，该类继承自 `CompetitiveLayer`。在这个类中，我们可以以
    M x N 神经元网格的形式定义神经元的数量：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The coordinate system in the 2D competitive layer is analogous to the Cartesian.
    Every neuron is assigned a position in the grid, with indexes starting from `0`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 2D 竞争层的坐标系类似于笛卡尔坐标系。每个神经元在网格中都有一个位置，索引从 `0` 开始：
- en: '![2D competitive layer](img/B05964_04_11.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![2D 竞争层](img/B05964_04_11.jpg)'
- en: 'In the illustration above, 12 neurons are arranged in a 3 x 4 grid. Another
    feature added in this class is the indexing of neurons by the position in the
    grid. This allows us to get subsets of neurons (and weights), one entire specific
    row or column of the grid, for example:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的插图上，12 个神经元被排列在一个 3 x 4 的网格中。在这个类中添加的另一个特性是按网格中的位置索引神经元。这允许我们获取神经元的子集（和权重），例如整个特定的行或列：
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: SOM learning algorithm
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SOM 学习算法
- en: A self-organizing map aims at classifying the input data by clustering those
    data points that trigger the same response on the output. Initially, the untrained
    network will produce random outputs, but as more examples are presented, the neural
    network identifies which neurons are activated more often and then their *position*
    in the SOM output space is changed. This algorithm is based on competitive learning,
    which means a winner neuron (also known as best matching unit, or BMU) will update
    its weights and its neighbor weights.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 自组织图旨在通过聚类触发相同响应的输出数据点来对输入数据进行分类。最初，未训练的网络将产生随机输出，但随着更多示例的呈现，神经网络会识别哪些神经元被激活得更频繁，然后改变它们在
    SOM 输出空间中的 *位置*。此算法基于竞争学习，这意味着获胜神经元（也称为最佳匹配单元或 BMU）将更新其权重及其邻居的权重。
- en: 'The following flowchart illustrates the learning process of a SOM Network:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的流程图说明了 SOM 网络的学习过程：
- en: '![SOM learning algorithm](img/B05964_04_12.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![SOM 学习算法](img/B05964_04_12.jpg)'
- en: 'The learning resembles a bit those algorithms addressed in [Chapter 2](ch02.xhtml
    "Chapter 2. Getting Neural Networks to Learn"), *Getting Neural Networks to Learn*
    and [Chapter 3](ch03.xhtml "Chapter 3. Perceptrons and Supervised Learning"),
    *Perceptrons and Supervised Learning*. Three major differences are the determination
    of the BMU with the distance, the weight update rule, and the absence of an error
    measure. The distance implies that nearer points should produce similar outputs,
    thus here the criterion to determine the lowest BMU is the neuron which presents
    a lower distance to some data point. This Euclidean distance is usually used,
    and in this book we will apply it for simplicity:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 学习过程有点类似于第 2 章[“让神经网络学习”](ch02.xhtml "Chapter 2. Getting Neural Networks to
    Learn")和第 3 章[“感知器和监督学习”](ch03.xhtml "Chapter 3. Perceptrons and Supervised Learning")中提到的算法。三个主要区别是
    BMU 的确定基于距离、权重更新规则以及没有错误度量。距离意味着更近的点应该产生相似的输出，因此，确定最低 BMU 的标准是距离某些数据点较近的神经元。通常使用欧几里得距离，本书中我们将为了简单起见应用它：
- en: '![SOM learning algorithm](img/B05964_04_12.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![SOM 学习算法](img/B05964_04_12.jpg)'
- en: The weight-to-input distance is calculated by the method `getWeightDistance(
    )` of the `CompetitiveLayer` class for a specific neuron i (argument neuron).
    This method was described above.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 权重到输入距离是通过 `CompetitiveLayer` 类的 `getWeightDistance( )` 方法计算的，针对特定的神经元 i（参数神经元）。该方法已在上面描述。
- en: Effect of neighboring neurons – the neighborhood function
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 邻近神经元的影响 – 邻域函数
- en: 'The weight update rule uses a neighborhood function *Θ(u,v,s,t)* which states
    how much a neighbor neuron u (BMU unit) is close to a neuron *v*. Remember that
    in a dimensional SOM, the BMU neuron is updated together with its neighbor neurons.
    This update is also dependent on a neighborhood radius, which takes into account
    the number of epoch''s s and a reference epoch *t*:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 权重更新规则使用一个邻域函数 *Θ(u,v,s,t)*，该函数说明了邻居神经元 u（BMU 单元）与神经元 *v* 的接近程度。记住，在多维 SOM 中，BMU
    神经元与其邻居神经元一起更新。这种更新还依赖于一个邻域半径，该半径考虑了 epoch 的数量 s 和参考 epoch *t*：
- en: '![Effect of neighboring neurons – the neighborhood function](img/B05964_04_12_02.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![邻近神经元的影响 – 邻域函数](img/B05964_04_12_02.jpg)'
- en: 'Here, *du,v* is the neuron distance between neurons u and v in the grid. The
    radius is calculated as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*du,v* 是网格中神经元 u 和 v 之间的神经元距离。半径的计算方法如下：
- en: '![Effect of neighboring neurons – the neighborhood function](img/B05964_04_12_03.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![邻近神经元的影响 – 邻域函数](img/B05964_04_12_03.jpg)'
- en: Here, is the initial radius. The effect of the number of epochs (s) and the
    reference epoch (*t*) is the decreasing of the neighborhood radius and thereby
    the effect of neighborhood. This is useful because in the beginning of the training,
    the weights need to be updated more often, because they are usually randomly initialized.
    As the training process continues, the updates need to be weaker, otherwise the
    neural network will continue to change its weights forever and will never converge.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，是初始半径。epoch 数（s）和参考 epoch (*t*) 的影响是减小邻域半径，从而减小邻域的影响。这很有用，因为在训练的初期，权重需要更频繁地更新，因为它们通常随机初始化。随着训练过程的继续，更新需要变得较弱，否则神经网络将永远改变其权重，永远不会收敛。
- en: '![Effect of neighboring neurons – the neighborhood function](img/B05964_04_12_04.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![邻近神经元的影响 – 邻域函数](img/B05964_04_12_04.jpg)'
- en: 'The neighborhood function and the neuron distance are implemented in the `CompetitiveLayer`
    class, with overridden versions for the `CompetitiveLayer2D` class:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 邻域函数和神经元距离在 `CompetitiveLayer` 类中实现，`CompetitiveLayer2D` 类有重载版本：
- en: '| CompetitiveLayer | CompetitiveLayer2D |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 竞争层 | 竞争层2D |'
- en: '| --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'The neighborhood radius function is the same for both classes:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 邻域半径函数对两个类都是相同的：
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The learning rate
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习率
- en: 'The learning rate also becomes weaker as the training goes on:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的进行，学习率也会变得较弱：
- en: '![The learning rate](img/B05964_04_12_05.jpg)![The learning rate](img/B05964_04_12_06.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![学习率](img/B05964_04_12_05.jpg)![学习率](img/B05964_04_12_06.jpg)'
- en: 'The parameter is the initial learning rate. Finally, considering the neighborhood
    function and the learning rate, the weight update rule is as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 参数是初始学习率。最后，考虑到邻域函数和学习率，权重更新规则如下：
- en: '![The learning rate](img/B05964_04_12_07.jpg)![The learning rate](img/B05964_04_12_08.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![学习率](img/B05964_04_12_07.jpg)![学习率](img/B05964_04_12_08.jpg)'
- en: Here, *X* *[k]* is the *kth* input, and *W* *[kj]* is the weight connecting
    the *kth* input to the *jth* output.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*X* *[k]* 是第 *k* 个输入，而 *W* *[kj]* 是连接第 *k* 个输入到第 *j* 个输出的权重。
- en: A new class for competitive learning
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 竞争学习的新类
- en: 'Now that we have a competitive layer, a Kohonen neural network, and defined
    the methods for neighboring functions, let''s create a new class for competitive
    learning. This class will inherit from `LearningAlgorithm` and will receive Kohonen
    objects for learning:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了竞争层、Kohonen 神经网络，并定义了邻域函数的方法，让我们创建一个新的竞争学习类。这个类将继承自 `LearningAlgorithm`，并将接收
    Kohonen 对象进行学习：
- en: '![A new class for competitive learning](img/B05964_04_13.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![竞争学习的新类](img/B05964_04_13.jpg)'
- en: 'As seen in [Chapter 2](ch02.xhtml "Chapter 2. Getting Neural Networks to Learn"),
    *Getting Neural Networks to Learn* a `LearningAlgorithm` object receives a neural
    dataset for training. This property is inherited by the `CompetitiveLearning`
    object, which implements new methods and properties to realize the competitive
    learning procedure:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第2章](ch02.xhtml "第2章。让神经网络学习") 所见，*让神经网络学习* 一个 `LearningAlgorithm` 对象接收一个用于训练的神经网络数据集。此属性由
    `CompetitiveLearning` 对象继承，它实现了新的方法和属性以实现竞争学习过程：
- en: '[PRE14]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The learning rate, as opposed to the previous algorithms, now changes over
    the training process, and it will be returned by the method `getLearningRate(
    )`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的算法不同，学习率现在会在训练过程中变化，并且将通过 `getLearningRate( )` 方法返回：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This method is used in the `calcWeightUpdate( )`:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法用于 `calcWeightUpdate( )`：
- en: '[PRE16]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `train( )` method is adapted as well for competitive learning:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`train( )` 方法也针对竞争学习进行了调整：'
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The implementation for the method `appliedNewWeights( )` is analogous to the
    one presented in the previous chapter, with the exception that there is no bias
    and there is only one output layer.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 方法 `appliedNewWeights( )` 的实现与上一章中介绍的方法类似，只是没有偏差，只有一个输出层。
- en: '**Time to play**: SOM applications in action. Now it is time to get hands-on
    and implement the Kohonen neural network in Java. There are many applications
    of self-organizing maps, most of them being in the field of clustering, data abstraction,
    and dimensionality reduction. But the clustering applications are the most interesting
    because of the many possibilities one may apply them on. The real advantage of
    clustering is that there is no need to worry about input/output relationship,
    rather the problem solver may concentrate on the input data. One example of clustering
    application will be explored in [Chapter 7](ch07.xhtml "Chapter 7. Clustering
    Customer Profiles"), *Clustering Customer Profiles*.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**游戏时间**：SOM应用实战。现在是时候动手实现Java中的Kohonen神经网络了。自组织映射有许多应用，其中大多数应用在聚类、数据抽象和降维领域。但聚类应用最有趣，因为它们有许多可能的应用。聚类的真正优势在于无需担心输入/输出关系，解决问题者可以专注于输入数据。一个聚类应用的例子将在[第7章](ch07.xhtml
    "第7章。聚类客户档案")中探讨，即*聚类客户档案*。'
- en: Visualizing the SOMs
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化SOMs
- en: 'In this section, we are going to introduce the plotting feature. Charts can
    be drawn in Java by using the freely available package `JFreeChart` (which can
    be downloaded from [http://www.jfree.org/jfreechart/](http://www.jfree.org/jfreechart/)).
    This package is attached with this chapter''s source code. So, we designed a class
    called **Chart**:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍绘图功能。在Java中，可以通过使用免费提供的包`JFreeChart`（可以从[http://www.jfree.org/jfreechart/](http://www.jfree.org/jfreechart/)下载）来绘制图表。此包附在本章的源代码中。因此，我们设计了一个名为**Chart**的类：
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The methods implemented in this class are for plotting line and scatter plots.
    The main difference between them lies in the fact that line plots take all data
    series over one x-axis (usually the time axis) where each data series is a line;
    scatter plots, on the other hand, show dots in a 2D plane indicating its position
    in relation to each of the axis. Charts below show graphically the difference
    between them and the codes to generate them:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本类实现的方法用于绘制线图和散点图。它们之间的主要区别在于线图在一个x轴（通常是时间轴）上绘制所有数据系列（每个数据系列是一条线）；而散点图则在二维平面上显示点，表示其相对于每个轴的位置。下面的图表图形地显示了它们之间的区别以及生成它们的代码：
- en: '![Visualizing the SOMs](img/B05964_04_14.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![可视化SOMs](img/B05964_04_14.jpg)'
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Visualizing the SOMs](img/B05964_04_15.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![可视化SOMs](img/B05964_04_15.jpg)'
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We're suppressing the codes for chart generation (methods `linePlot( )` and
    `scatterPlot( ));` however, in the file `Chart.java`, the reader can find their
    implementation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们省略了图表生成代码（方法`linePlot( )`和`scatterPlot( )`）；然而，在`Chart.java`文件中，读者可以找到它们的实现。
- en: Plotting 2D training datasets and neuron weights
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制二维训练数据集和神经元权重
- en: 'Now that we have the methods for plotting charts, let''s plot the training
    dataset and neuron weights. Any 2D dataset can be plotted in the same way shown
    in the diagram of the last section. To plot the weights, we need to get the weights
    of the Kohonen neural network using the following code:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了绘制图表的方法，让我们绘制训练数据集和神经元权重。任何二维数据集都可以以与上一节图示相同的方式绘制。要绘制权重，我们需要使用以下代码获取Kohonen神经网络的权重：
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In competitive learning, we can check visually how the weights *move* around
    the dataset space. So we''re going to add a method (`showPlot2DData( )`) to plot
    the dataset and the weights, a property (`plot2DData`) to hold the reference to
    the `ChartFrame`, and a flag (`show2DData`) to determine whether the plot is going
    to be shown for every epoch:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在竞争学习中，我们可以通过视觉检查权重如何在数据集空间中移动。因此，我们将添加一个方法（`showPlot2DData( )`）来绘制数据集和权重，一个属性（`plot2DData`）来保存对`ChartFrame`的引用，以及一个标志（`show2DData`）来决定是否在每个epoch显示绘图：
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This method will be called from the `train` method at the end of each epoch.
    A property called **sleep** will determine for how many milliseconds the chart
    will be displayed until the next epoch''s chart replaces it:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将在每个epoch结束时从`train`方法中调用。一个名为**sleep**的属性将决定图表显示多少毫秒，直到下一个epoch的图表替换它：
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Testing Kohonen learning
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试Kohonen学习
- en: 'Let''s now define a Kohonen network and see how it works. First, we''re creating
    a Kohonen with zero dimension:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们定义一个Kohonen网络并看看它是如何工作的。首先，我们创建一个零维度的Kohonen：
- en: '[PRE24]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'By running this code, we get the first plot:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我们得到第一个绘图：
- en: '![Testing Kohonen learning](img/B05964_04_16.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![测试Kohonen学习](img/B05964_04_16.jpg)'
- en: 'As the training starts, the weights begin to distribute over the input data
    space, until finally it converges by being distributed uniformly along the input
    data space:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的开始，权重开始分布在输入数据空间中，直到最终通过在输入数据空间中均匀分布而收敛：
- en: '![Testing Kohonen learning](img/B05964_04_17.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![测试Kohonen学习](img/B05964_04_17.jpg)'
- en: 'For one dimension, let''s try something funkier. Let''s create the dataset
    over a cosine function with random noise:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一维，让我们尝试一些更有趣的东西。让我们创建一个基于余弦函数并带有随机噪声的数据集：
- en: '[PRE25]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'By running the same previous code and changing the object to *kn1*, we get
    a line connecting all the weight points:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行相同的先前代码并将对象更改为*kn1*，我们得到一条连接所有权重点的线：
- en: '![Testing Kohonen learning](img/B05964_04_18.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![测试Kohonen学习](img/B05964_04_18.jpg)'
- en: 'As the training continues, the lines tend to be organized along the data wave:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 随着训练的继续，线条倾向于沿着数据波组织：
- en: '![Testing Kohonen learning](img/B05964_04_19.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![测试Kohonen学习](img/B05964_04_19.jpg)'
- en: See the file `Kohonen1DTest.java` if you want to change the initial learning
    rate, maximum number of epochs, and other parameters.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想更改初始学习率、最大时期数和其他参数，请查看`Kohonen1DTest.java`文件。
- en: Finally, let's see the two-dimensional Kohonen chart. The code will be a little
    bit different, since now, instead of giving the number of neurons, we're going
    to inform the Kohonen constructor the dimensions of our neural grid.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看二维的Kohonen图。代码将略有不同，因为现在，我们不是给出神经元的数量，而是要通知Kohonen构建者我们神经网络网格的维度。
- en: 'The dataset used here will be a circle with random noise added:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的数据集将是一个带有随机噪声的圆：
- en: '[PRE26]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now let''s construct the two-dimensional Kohonen:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来构建二维的Kohonen：
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Note that we are using `GaussianInitialization` with mean 500.0 and standard
    deviation 20.0, that is, the weights will be generated at the position (500.0,500.0)
    while the data is centered around (50.0,50.0):'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用的是均值为500.0和标准差为20.0的`GaussianInitialization`，这意味着权重将在位置(500.0,500.0)生成，而数据则围绕(50.0,50.0)中心：
- en: '![Testing Kohonen learning](img/B05964_04_20.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![测试Kohonen学习](img/B05964_04_20.jpg)'
- en: 'Now let''s train the neural network. The neuron weights quickly move to the
    circle in the first epochs:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来训练神经网络。在最初的几个时期，神经元权重迅速移动到圆圈中：
- en: '![Testing Kohonen learning](img/B05964_04_21.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![测试Kohonen学习](img/B05964_04_21.jpg)'
- en: 'By the end of the training, the majority of the weights will be distributed
    over the circle, while in the center there will be an empty space, as the grid
    will be totally stretched out:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 到了训练结束时，大多数权重将分布在圆周上，而在中心将有一个空隙，因为网格将被完全拉伸开：
- en: '![Testing Kohonen learning](img/B05964_04_22.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![测试Kohonen学习](img/B05964_04_22.jpg)'
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we've seen how to apply unsupervised learning algorithms on
    neural networks. We've been presented a new and suitable architecture for that
    end, the self-organizing maps of Kohonen. Unsupervised learning has proved to
    be as powerful as the supervised learning methods, because they concentrate only
    on the input data, without need to make input-output mappings. We've seen graphically
    how the training algorithms are able to drive the weights nearer to the input
    data, thereby playing a role in clustering and dimensionality reduction. In addition
    to these examples, Kohonen SOMs are also able to classify clusters of data, as
    each neuron will provide better responses for a particular set of inputs.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何在神经网络上应用无监督学习算法。我们介绍了一种新的、适合该目的的架构，即Kohonen的自组织图。无监督学习已被证明与监督学习方法一样强大，因为它们只关注输入数据，无需建立输入输出映射。我们通过图形展示了训练算法如何能够将权重驱动到输入数据附近，从而在聚类和降维中发挥作用。除了这些例子之外，Kohonen
    SOMs还能够对数据簇进行分类，因为每个神经元将针对特定的输入集提供更好的响应。
