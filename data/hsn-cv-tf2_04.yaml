- en: Modern Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç°ä»£ç¥ç»ç½‘ç»œ
- en: In [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml), *Computer Vision
    and Neural Networks*, we presented how recent neural networks, which are more
    suitable for image processing, surpassed previous computer vision methods of the
    past decade. However, limited by how much we can reimplement from scratch, we
    only covered basic architectures. Now, with TensorFlow's powerful APIs at our
    fingertips, it is time to discover what **convolutional neural networks**Â (**CNNs**)
    are, and how these modern methods are trained to further improve their robustness.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[ç¬¬1ç« ](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)ã€Šè®¡ç®—æœºè§†è§‰ä¸ç¥ç»ç½‘ç»œã€‹ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†è¿‘å¹´æ¥æ›´é€‚åˆå›¾åƒå¤„ç†çš„ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•è¶…è¶Šè¿‡å»åå¹´è®¡ç®—æœºè§†è§‰æ–¹æ³•çš„ã€‚ç„¶è€Œï¼Œç”±äºå—é™äºæˆ‘ä»¬ä»å¤´å¼€å§‹é‡æ–°å®ç°çš„èƒ½åŠ›ï¼Œæˆ‘ä»¬åªæ¶µç›–äº†åŸºæœ¬æ¶æ„ã€‚ç°åœ¨ï¼Œéšç€TensorFlowå¼ºå¤§çš„APIè§¦æ‰‹å¯å¾—ï¼Œæ˜¯æ—¶å€™æ¢ç´¢ä»€ä¹ˆæ˜¯**å·ç§¯ç¥ç»ç½‘ç»œ**ï¼ˆ**CNNs**ï¼‰ï¼Œä»¥åŠè¿™äº›ç°ä»£æ–¹æ³•æ˜¯å¦‚ä½•è¢«è®­ç»ƒä»¥è¿›ä¸€æ­¥æé«˜å®ƒä»¬çš„é²æ£’æ€§ã€‚
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å°†æ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
- en: CNNs and their relevance to computer vision
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNNåŠå…¶ä¸è®¡ç®—æœºè§†è§‰çš„ç›¸å…³æ€§
- en: Implementing these modern networks with TensorFlow and Keras
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨TensorFlowå’ŒKeraså®ç°è¿™äº›ç°ä»£ç½‘ç»œ
- en: Advanced optimizers and how to train CNNs efficiently
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é«˜çº§ä¼˜åŒ–å™¨å’Œå¦‚ä½•é«˜æ•ˆè®­ç»ƒCNN
- en: Regularization methods and how to avoid overfitting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–æ–¹æ³•å’Œå¦‚ä½•é¿å…è¿‡æ‹Ÿåˆ
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŠ€æœ¯è¦æ±‚
- en: The main resources of this chapter are implemented with TensorFlow. The Matplotlib
    package ([https://matplotlib.org](https://matplotlib.org)) and the scikit-image
    package ([https://scikit-image.org](https://scikit-image.org)) are also used,
    though only to display some results or to load example images.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« çš„ä¸»è¦èµ„æºæ˜¯ç”¨TensorFlowå®ç°çš„ã€‚åŒæ—¶ï¼ŒMatplotlibåŒ…ï¼ˆ[https://matplotlib.org](https://matplotlib.org)ï¼‰å’Œscikit-imageåŒ…ï¼ˆ[https://scikit-image.org](https://scikit-image.org)ï¼‰ä¹Ÿæœ‰ä½¿ç”¨ï¼Œå°½ç®¡å®ƒä»¬ä»…ç”¨äºæ˜¾ç¤ºä¸€äº›ç»“æœæˆ–åŠ è½½ç¤ºä¾‹å›¾åƒã€‚
- en: As in previous chapters, Jupyter notebooks illustrating the concepts covered
    in this chapter can be found in the following GitHub folder:Â [github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter03).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚åŒå‰å‡ ç« ä¸€æ ·ï¼Œå±•ç¤ºæœ¬ç« æ¦‚å¿µçš„Jupyterç¬”è®°æœ¬å¯ä»¥åœ¨ä»¥ä¸‹GitHubæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°ï¼š[github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter03)ã€‚
- en: Discovering convolutional neural networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢ç´¢å·ç§¯ç¥ç»ç½‘ç»œ
- en: In the first part of this chapter, we will present CNNs, also known as **ConvNets**,and
    explain why they have become omnipresent in vision tasks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†ä»‹ç»CNNï¼Œä¹Ÿç§°ä¸º**å·ç§¯ç½‘ç»œ**ï¼ˆ**ConvNets**ï¼‰ï¼Œå¹¶è§£é‡Šä¸ºä»€ä¹ˆå®ƒä»¬åœ¨è§†è§‰ä»»åŠ¡ä¸­å˜å¾—æ— å¤„ä¸åœ¨ã€‚
- en: Neural networks for multidimensional data
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¤šç»´æ•°æ®çš„ç¥ç»ç½‘ç»œ
- en: CNNs were introduced to solve someÂ of the shortcomings of the original neural
    networks. In this section, we will address these issues and present how CNNs deal
    with them.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CNNsçš„å¼•å…¥æ˜¯ä¸ºäº†è§£å†³åŸå§‹ç¥ç»ç½‘ç»œçš„ä¸€äº›ç¼ºé™·ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºè¿™äº›é—®é¢˜ï¼Œå¹¶å±•ç¤ºCNNæ˜¯å¦‚ä½•å¤„ç†å®ƒä»¬çš„ã€‚
- en: Problems with fully connected networks
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®Œå…¨è¿æ¥ç½‘ç»œçš„é—®é¢˜
- en: 'Through our introductory experiment in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*,Â and [Chapter 2](c7c49010-458f-47ef-a538-96118f9cd892.xhtml),
    *TensorFlow Basics and Training a Model*, we have already highlighted the following
    two main drawbacks of basic networks when dealing with images:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æˆ‘ä»¬åœ¨[ç¬¬1ç« ](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)ã€Šè®¡ç®—æœºè§†è§‰ä¸ç¥ç»ç½‘ç»œã€‹å’Œ[ç¬¬2ç« ](c7c49010-458f-47ef-a538-96118f9cd892.xhtml)ã€ŠTensorFlowåŸºç¡€ä¸è®­ç»ƒæ¨¡å‹ã€‹çš„å…¥é—¨å®éªŒï¼Œæˆ‘ä»¬å·²ç»çªå‡ºäº†åŸºæœ¬ç½‘ç»œåœ¨å¤„ç†å›¾åƒæ—¶çš„ä»¥ä¸‹ä¸¤ä¸ªä¸»è¦ç¼ºç‚¹ï¼š
- en: An explosive number of parameters
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çˆ†ç‚¸æ€§çš„å‚æ•°æ•°é‡
- en: A lack of spatial reasoning
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç©ºé—´æ¨ç†çš„ç¼ºä¹
- en: Let's discuss each of these here.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬åœ¨è¿™é‡Œè®¨è®ºä¸€ä¸‹è¿™äº›å†…å®¹ã€‚
- en: An explosive number of parameters
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çˆ†ç‚¸æ€§çš„å‚æ•°æ•°é‡
- en: Images are complex structures with a large number of values (that is,Â *H* Ã—Â *W*
    Ã—Â *D* values withÂ *H*Â indiacting the image's height,Â *W* its width, andÂ *D* its
    depth/number of channels, such asÂ *D* = 3 for RGB images). Even the small, single-channel
    images we used as examples in the first two chapters represent input vectors of
    size *28Â *Ã— *28Â *Ã— *1 = 784* values each. For the first layer of the basic neural
    network we implemented, this meant a weight matrix of shape (784, 64). This equates
    to 50,176Â (784Â Ã—Â 64) parameter values to optimize, just for this variable!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒæ˜¯å…·æœ‰å¤§é‡å€¼çš„å¤æ‚ç»“æ„ï¼ˆå³ï¼Œ*H* Ã— *W* Ã— *D* ä¸ªå€¼ï¼Œå…¶ä¸­*H*è¡¨ç¤ºå›¾åƒçš„é«˜åº¦ï¼Œ*W*è¡¨ç¤ºå®½åº¦ï¼Œ*D*è¡¨ç¤ºæ·±åº¦/é€šé“æ•°ï¼Œä¾‹å¦‚RGBå›¾åƒçš„*D*
    = 3ï¼‰ã€‚å³ä½¿æ˜¯æˆ‘ä»¬åœ¨å‰ä¸¤ç« ä¸­ä½¿ç”¨çš„å•é€šé“å°å›¾åƒï¼Œä¹Ÿä»£è¡¨äº†å¤§å°ä¸º*28 Ã— 28 Ã— 1 = 784*çš„è¾“å…¥å‘é‡ã€‚å¯¹äºæˆ‘ä»¬å®ç°çš„åŸºç¡€ç¥ç»ç½‘ç»œçš„ç¬¬ä¸€å±‚ï¼Œè¿™æ„å‘³ç€ä¸€ä¸ªå½¢çŠ¶ä¸º(784,
    64)çš„æƒé‡çŸ©é˜µã€‚è¿™æ„å‘³ç€ä»…ä»…è¿™ä¸ªå˜é‡å°±éœ€è¦ä¼˜åŒ–50,176ï¼ˆ784 Ã— 64ï¼‰ä¸ªå‚æ•°å€¼ï¼
- en: This number of parameters simply explodes when we consider larger RGB images
    or deeper networks.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è€ƒè™‘æ›´å¤§çš„RGBå›¾åƒæˆ–æ›´æ·±çš„ç½‘ç»œæ—¶ï¼Œå‚æ•°çš„æ•°é‡ä¼šå‘ˆæŒ‡æ•°çº§å¢é•¿ã€‚
- en: A lack of spatial reasoning
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¼ºä¹ç©ºé—´æ¨ç†
- en: Because their neurons receive all the values from the previous layer without
    any distinction (they are *fully connected*), these neural networks do not have
    a notion of *distance*/*spatiality*. Spatial relations in the data are lost. Multidimensional
    data, such as images, could also be anything from column vectors to dense layers
    because their operations do not take into account the data dimensionality nor
    the positions of input values. More precisely, this means that the notion of proximity
    between pixels is lost to **fully connected** (**FC**) layers, as all pixel values
    are combined by the layers with no regard for their original positions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºå®ƒä»¬çš„ç¥ç»å…ƒæ¥æ”¶æ¥è‡ªä¸Šä¸€å±‚çš„æ‰€æœ‰å€¼ä¸”æ²¡æœ‰ä»»ä½•åŒºåˆ†ï¼ˆå®ƒä»¬æ˜¯*å…¨è¿æ¥çš„*ï¼‰ï¼Œè¿™äº›ç¥ç»ç½‘ç»œæ²¡æœ‰*è·ç¦»*/*ç©ºé—´æ€§*çš„æ¦‚å¿µã€‚æ•°æ®ä¸­çš„ç©ºé—´å…³ç³»ä¸§å¤±äº†ã€‚å¤šç»´æ•°æ®ï¼Œå¦‚å›¾åƒï¼Œä¹Ÿå¯ä»¥æ˜¯ä»åˆ—å‘é‡åˆ°å¯†é›†å±‚çš„ä»»ä½•å½¢å¼ï¼Œå› ä¸ºå®ƒä»¬çš„æ“ä½œæ²¡æœ‰è€ƒè™‘æ•°æ®çš„ç»´åº¦æ€§å’Œè¾“å…¥å€¼çš„ä½ç½®ã€‚æ›´å‡†ç¡®åœ°è¯´ï¼Œè¿™æ„å‘³ç€**å…¨è¿æ¥**ï¼ˆ**FC**ï¼‰å±‚ä¸§å¤±äº†åƒç´ é—´çš„æ¥è¿‘æ¦‚å¿µï¼Œå› ä¸ºæ‰€æœ‰åƒç´ å€¼åœ¨å±‚å†…è¢«ç»„åˆæ—¶å¹¶ä¸è€ƒè™‘å®ƒä»¬çš„åŸå§‹ä½ç½®ã€‚
- en: As it does not change the behavior of dense layers, to simplify their computations
    and parameter representations, it is common practice to *flatten* multidimensional
    inputs before passing them to these layers (that is, to reshape them into column
    vectors).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå®ƒä¸ä¼šæ”¹å˜å…¨è¿æ¥å±‚çš„è¡Œä¸ºï¼Œä¸ºäº†ç®€åŒ–è®¡ç®—å’Œå‚æ•°è¡¨ç¤ºï¼Œé€šå¸¸ä¼šåœ¨å°†å¤šç»´è¾“å…¥ä¼ é€’åˆ°è¿™äº›å±‚ä¹‹å‰ï¼Œå…ˆå¯¹å…¶è¿›è¡Œ*å±•å¹³*ï¼ˆå³å°†å…¶é‡å¡‘ä¸ºåˆ—å‘é‡ï¼‰ã€‚
- en: Intuitively, neural layers would be much smarter if they could take into account
    **spatial information**; that is, that some input values belong to the same pixel
    (channel values) or to the same image region (neighbor pixels).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚åœ°çœ‹ï¼Œå¦‚æœç¥ç»ç½‘ç»œèƒ½å¤Ÿè€ƒè™‘**ç©ºé—´ä¿¡æ¯**ï¼Œå³æŸäº›è¾“å…¥å€¼å±äºåŒä¸€ä¸ªåƒç´ ï¼ˆé€šé“å€¼ï¼‰æˆ–å±äºåŒä¸€åŒºåŸŸï¼ˆç›¸é‚»åƒç´ ï¼‰ï¼Œé‚£ä¹ˆç¥ç»å±‚å°†æ›´åŠ æ™ºèƒ½ã€‚
- en: Introducing CNNs
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¼•å…¥å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
- en: CNNs offer simple solutions to these shortcomings. While they work the same
    way as the networks we introduced previouslyÂ (such as feed-forward and backpropagation),
    some clever changes were brought to their architecture.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: CNNä¸ºè¿™äº›ä¸è¶³æä¾›äº†ç®€å•çš„è§£å†³æ–¹æ¡ˆã€‚å°½ç®¡å®ƒä»¬ä¸æˆ‘ä»¬ä¹‹å‰ä»‹ç»çš„ç½‘ç»œï¼ˆå¦‚å‰é¦ˆç½‘ç»œå’Œåå‘ä¼ æ’­ç½‘ç»œï¼‰å·¥ä½œåŸç†ç›¸åŒï¼Œä½†å®ƒä»¬çš„æ¶æ„åšäº†ä¸€äº›å·§å¦™çš„æ”¹è¿›ã€‚
- en: 'First of all, CNNs can handle multidimensional data. For images, a CNN takes
    as input three-dimensional data (heightÂ Ã— width Ã— depth) and has its own neurons
    arranged in a similar volume (refer to *Figure 3.1*). This leads to the second
    novelty of CNNsâ€”unlike fully connected networks, where neurons are connected to
    all elements from the previous layer, each neuron in CNNs only has access to some
    elements in the neighboring region of the previous layer. This region (usually
    square and spanning all channels) is called the **receptive field** of the neurons
    (or theÂ filter size):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼ŒCNNèƒ½å¤Ÿå¤„ç†å¤šç»´æ•°æ®ã€‚å¯¹äºå›¾åƒï¼ŒCNNè¾“å…¥çš„æ˜¯ä¸‰ç»´æ•°æ®ï¼ˆé«˜åº¦ Ã— å®½åº¦ Ã— æ·±åº¦ï¼‰ï¼Œå¹¶ä¸”å®ƒçš„ç¥ç»å…ƒæ’åˆ—æ–¹å¼ä¹Ÿç±»ä¼¼äºä¸€ä¸ªä½“ç§¯ï¼ˆå‚è§*å›¾ 3.1*ï¼‰ã€‚è¿™å¯¼è‡´äº†CNNçš„ç¬¬äºŒä¸ªåˆ›æ–°â€”â€”ä¸å…¨è¿æ¥ç½‘ç»œä¸åŒï¼Œåœ¨CNNä¸­ï¼Œæ¯ä¸ªç¥ç»å…ƒåªè®¿é—®ä¸Šä¸€å±‚ä¸­ç›¸é‚»åŒºåŸŸçš„ä¸€äº›å…ƒç´ ï¼Œè€Œä¸æ˜¯è¿æ¥åˆ°ä¸Šä¸€å±‚çš„æ‰€æœ‰å…ƒç´ ã€‚è¿™ä¸ªåŒºåŸŸï¼ˆé€šå¸¸æ˜¯æ­£æ–¹å½¢å¹¶è·¨è¶Šæ‰€æœ‰é€šé“ï¼‰è¢«ç§°ä¸ºç¥ç»å…ƒçš„**æ„Ÿå—é‡**ï¼ˆæˆ–è¿‡æ»¤å™¨å¤§å°ï¼‰ï¼š
- en: '![](img/adc1e504-6290-43f6-b749-aa6409ffa9be.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/adc1e504-6290-43f6-b749-aa6409ffa9be.png)'
- en: 'Figure 3.1: CNN representation, showing the *receptive fields* of the top-left
    neurons from the first layer to the last (further explanations can be found in
    the following subsections)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3.1ï¼šCNNè¡¨ç¤ºï¼Œå±•ç¤ºäº†ä»ç¬¬ä¸€å±‚åˆ°æœ€åä¸€å±‚çš„å·¦ä¸Šè§’ç¥ç»å…ƒçš„*æ„Ÿå—é‡*ï¼ˆæ›´å¤šè§£é‡Šå¯åœ¨ä»¥ä¸‹å°èŠ‚ä¸­æ‰¾åˆ°ï¼‰
- en: By linking neurons only to their neighboring ones in the previous layer, CNNs
    not only drastically reduce the number of parameters to train, but also preserve
    the localization of image features.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡åªå°†ç¥ç»å…ƒä¸ä¸Šä¸€å±‚ä¸­ç›¸é‚»çš„ç¥ç»å…ƒç›¸è¿ï¼ŒCNNä¸ä»…å¤§å¹…å‡å°‘äº†éœ€è¦è®­ç»ƒçš„å‚æ•°æ•°é‡ï¼Œè¿˜ä¿ç•™äº†å›¾åƒç‰¹å¾çš„å®šä½ã€‚
- en: CNN operations
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·ç§¯ç¥ç»ç½‘ç»œæ“ä½œ
- en: With this architecture paradigm, several new types of layers were also introduced,
    efficiently taking advantage of *multidimensionality* and *local connectivity*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æ¶æ„èŒƒå¼ä¸‹ï¼Œè¿˜å¼•å…¥äº†å‡ ç§æ–°çš„å±‚ç±»å‹ï¼Œå……åˆ†åˆ©ç”¨äº†*å¤šç»´æ€§*å’Œ*å±€éƒ¨è¿æ¥æ€§*ã€‚
- en: Convolutional layers
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å·ç§¯å±‚
- en: CNNs get their name from *convolutional layers*, which are at the core of their
    architecture. In these layers, the number of parameters is further reduced by
    sharing the same weights and bias among all neurons connected to the same output
    channel.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: CNNä¹‹æ‰€ä»¥å¾—åï¼Œæ˜¯å› ä¸ºå®ƒä»¬çš„æ ¸å¿ƒæ¶æ„åŒ…å«äº†*å·ç§¯å±‚*ã€‚åœ¨è¿™äº›å±‚ä¸­ï¼Œé€šè¿‡åœ¨æ‰€æœ‰è¿æ¥åˆ°åŒä¸€è¾“å‡ºé€šé“çš„ç¥ç»å…ƒä¹‹é—´å…±äº«ç›¸åŒçš„æƒé‡å’Œåç½®ï¼Œè¿›ä¸€æ­¥å‡å°‘äº†å‚æ•°çš„æ•°é‡ã€‚
- en: Concept
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¦‚å¿µ
- en: 'These specific neurons with shared weights and bias can also be thought ofÂ as
    a single neuron sliding over the whole input matrix with *spatially limited connectivity*.
    At each step, this neuron is only spatially connected to the local region in the
    input volume (*H* Ã—Â *W* Ã— *D*) it is currently sliding over. Given this limited
    input of dimensions,Â *k[H]* Ã—Â *k[W]* Ã— *D* for a neuron with a filter size (*k[H]*,
    *k[W]*), the neuron still works like the ones modeled in our first chapterâ€”it
    linearly combines the input values (*k[H]* Ã—Â *k[W]* Ã—Â *D* values) before applying
    an activation function to the sum (a linear or non-linear function). Mathematically,
    the response,Â *z[i,j]*, of the neuron when presented with the input patch starting
    at position *(i,* *j*) can be expressed as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å…·æœ‰å…±äº«æƒé‡å’Œåç½®çš„ç‰¹å®šç¥ç»å…ƒä¹Ÿå¯ä»¥è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªåœ¨æ•´ä¸ªè¾“å…¥çŸ©é˜µä¸Šæ»‘åŠ¨çš„å•ä¸€ç¥ç»å…ƒï¼Œå…·æœ‰*ç©ºé—´æœ‰é™çš„è¿æ¥æ€§*ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œè¿™ä¸ªç¥ç»å…ƒåªä¸å½“å‰æ»‘åŠ¨çš„è¾“å…¥ä½“ç§¯ï¼ˆ*H*
    Ã— *W* Ã— *D*ï¼‰ä¸­çš„å±€éƒ¨åŒºåŸŸè¿›è¡Œç©ºé—´è¿æ¥ã€‚è€ƒè™‘åˆ°è¿™ç§æœ‰é™ç»´åº¦çš„è¾“å…¥ï¼Œ*k[H]* Ã— *k[W]* Ã— *D*ï¼Œå¯¹äºå…·æœ‰è¿‡æ»¤å™¨å¤§å°ï¼ˆ*k[H]*ï¼Œ*k[W]*ï¼‰çš„ç¥ç»å…ƒï¼Œè¯¥ç¥ç»å…ƒçš„å·¥ä½œæ–¹å¼ä¸æˆ‘ä»¬ç¬¬ä¸€ç« ä¸­å»ºæ¨¡çš„ç¥ç»å…ƒç±»ä¼¼â€”â€”å®ƒåœ¨çº¿æ€§ç»„åˆè¾“å…¥å€¼ï¼ˆ*k[H]*
    Ã— *k[W]* Ã— *D* ä¸ªå€¼ï¼‰ä¹‹åï¼Œå†åº”ç”¨æ¿€æ´»å‡½æ•°å¯¹å’Œè¿›è¡Œå¤„ç†ï¼ˆçº¿æ€§æˆ–éçº¿æ€§å‡½æ•°ï¼‰ã€‚ä»æ•°å­¦è§’åº¦æ¥çœ‹ï¼Œå½“ç¥ç»å…ƒæ¥å—ä»ä½ç½® *(i, j)* å¼€å§‹çš„è¾“å…¥å—æ—¶ï¼Œå®ƒçš„å“åº”
    *z[i,j]* å¯ä»¥è¡¨ç¤ºä¸ºï¼š
- en: '![](img/2928d949-f803-4a05-af2b-f58df1cee1b8.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2928d949-f803-4a05-af2b-f58df1cee1b8.png)'
- en: '![](img/55a0a8c2-4a60-41e8-a9da-0c3bc0154704.png)Â isÂ the neuron''s weights
    (that is, a two-dimensional matrix of shapeÂ *k[H]* Ã— *k[W]* Ã— *D*),Â ![](img/895581ac-c2d3-4936-8811-a3f896e59e00.png)Â isÂ the
    neuron''s bias, andÂ ![](img/4f85cb73-d794-47a7-a737-750799a3cc72.png)Â isÂ the activation
    function (for instance, *sigmoid*). Repeating this operation for each position
    that the neuron can take over the input data, we obtain its complete response
    matrix, ğ‘§, of dimensions *H*[o] Ã— *W*[o], with *H*[o] and *W*[o]Â being the number
    of times the neuron can slide vertically and horizontally (respectively) over
    the input tensor.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/55a0a8c2-4a60-41e8-a9da-0c3bc0154704.png) æ˜¯ç¥ç»å…ƒçš„æƒé‡ï¼ˆå³å½¢çŠ¶ä¸º *k[H]* Ã— *k[W]*
    Ã— *D* çš„äºŒç»´çŸ©é˜µï¼‰ï¼Œ![](img/895581ac-c2d3-4936-8811-a3f896e59e00.png) æ˜¯ç¥ç»å…ƒçš„åç½®ï¼Œ![](img/4f85cb73-d794-47a7-a737-750799a3cc72.png)
    æ˜¯æ¿€æ´»å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œ*sigmoid*ï¼‰ã€‚å¯¹äºç¥ç»å…ƒå¯ä»¥åœ¨è¾“å…¥æ•°æ®ä¸Šæ»‘åŠ¨çš„æ¯ä¸ªä½ç½®ï¼Œé‡å¤æ­¤æ“ä½œåï¼Œæˆ‘ä»¬å¾—åˆ°å…¶å®Œæ•´çš„å“åº”çŸ©é˜µ ğ‘§ï¼Œå°ºå¯¸ä¸º *H*[o] Ã— *W*[o]ï¼Œå…¶ä¸­
    *H*[o] å’Œ *W*[o] åˆ†åˆ«æ˜¯ç¥ç»å…ƒå¯ä»¥åœ¨è¾“å…¥å¼ é‡ä¸Šå‚ç›´å’Œæ°´å¹³æ»‘åŠ¨çš„æ¬¡æ•°ã€‚'
- en: In practice, most of the time, square filters are used, meaning that they have
    a size (*k,* *k*) with *k =* *k[H]* = *k[W]*. For the rest of this chapter, we
    will only consider square filters to simplify the explanations, though it is good
    to remember that their height and width may vary.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ä½¿ç”¨çš„æ˜¯æ–¹å½¢è¿‡æ»¤å™¨ï¼Œè¿™æ„å‘³ç€å®ƒä»¬çš„å¤§å°æ˜¯ (*k*, *k*)ï¼Œå…¶ä¸­ *k =* *k[H]* = *k[W]*ã€‚åœ¨æœ¬ç« çš„å…¶ä½™éƒ¨åˆ†ï¼Œä¸ºäº†ç®€åŒ–è§£é‡Šï¼Œæˆ‘ä»¬å°†åªè€ƒè™‘æ–¹å½¢è¿‡æ»¤å™¨ï¼Œå°½ç®¡å€¼å¾—è®°ä½çš„æ˜¯ï¼Œå®ƒä»¬çš„é«˜åº¦å’Œå®½åº¦å¯èƒ½ä¼šæœ‰æ‰€å˜åŒ–ã€‚
- en: As a convolutional layer can still have *N* sets of different neurons (that
    is,Â *N* sets of neurons with shared parameters), their response maps are stacked
    together into an output tensor of shape *H*[o] Ã— *W*[o] Ã— *N*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå·ç§¯å±‚ä»ç„¶å¯ä»¥æœ‰ *N* ç»„ä¸åŒçš„ç¥ç»å…ƒï¼ˆå³ï¼Œå…·æœ‰å…±äº«å‚æ•°çš„ *N* ç»„ç¥ç»å…ƒï¼‰ï¼Œå®ƒä»¬çš„å“åº”å›¾è¢«å †å åœ¨ä¸€èµ·ï¼Œå½¢æˆå½¢çŠ¶ä¸º *H*[o] Ã— *W*[o]
    Ã— *N* çš„è¾“å‡ºå¼ é‡ã€‚
- en: 'InÂ the same way that we applied matrix multiplication to fully connected layers,
    the **convolution operation** can be used here to compute all the response maps
    at once (hence the name of these layers). Those familiar with this operation may
    have recognized it as soon as we mentioned *sliding filters over the input matrix*.
    For those who are unfamiliar with the operation, the results of a convolution
    are indeed obtained by sliding a filter,Â *w*, over the input matrix,Â *x*, and
    computing, at each position, the dot product of the filter and the patch ofÂ *x*
    starting at the current position. This operation is illustrated in *Figure 3.2*
    (an input tensor with a single channel is used to keep the diagram easy to understand):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæˆ‘ä»¬åœ¨å…¨è¿æ¥å±‚ä¸­åº”ç”¨çŸ©é˜µä¹˜æ³•ä¸€æ ·ï¼Œ**å·ç§¯æ“ä½œ**ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œç”¨æ¥ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰å“åº”å›¾ï¼ˆå› æ­¤è¿™äº›å±‚çš„åç§°ï¼‰ã€‚ç†Ÿæ‚‰è¿™ç§æ“ä½œçš„äººï¼Œå¯èƒ½åœ¨æˆ‘ä»¬æåˆ°*åœ¨è¾“å…¥çŸ©é˜µä¸Šæ»‘åŠ¨è¿‡æ»¤å™¨*æ—¶å°±è®¤å‡ºäº†å®ƒã€‚å¯¹äºé‚£äº›ä¸ç†Ÿæ‚‰è¿™ç§æ“ä½œçš„äººï¼Œå·ç§¯çš„ç»“æœç¡®å®æ˜¯é€šè¿‡å°†ä¸€ä¸ªè¿‡æ»¤å™¨
    *w* æ»‘åŠ¨åˆ°è¾“å…¥çŸ©é˜µ *x* ä¸Šï¼Œå¹¶åœ¨æ¯ä¸ªä½ç½®è®¡ç®—è¿‡æ»¤å™¨ä¸ä»å½“å‰èµ·å§‹ä½ç½®å¼€å§‹çš„ *x* å—çš„ç‚¹ç§¯æ¥è·å¾—çš„ã€‚æ­¤æ“ä½œåœ¨*å›¾ 3.2*ä¸­å¾—åˆ°äº†è¯´æ˜ï¼ˆä½¿ç”¨å•é€šé“è¾“å…¥å¼ é‡ï¼Œä»¥ä¾¿ä½¿å›¾ç¤ºæ˜“äºç†è§£ï¼‰ï¼š
- en: '![](img/6a7df36e-fb8c-46ea-91bc-ccbb59354477.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a7df36e-fb8c-46ea-91bc-ccbb59354477.png)'
- en: 'Figure 3.2: A convolution illustrated'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3.2ï¼šå·ç§¯ç¤ºæ„å›¾
- en: In *Figure 3.2*, please note that the input,Â *x,* has beenÂ *padded*Â with zeros,
    which is commonly done in convolutional layers; for instance, when we want the
    output to be the same size as the original input (a size of 3 Ã— 3 in this example).
    The notion of padding is further developed later in this chapter.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨*å›¾3.2*ä¸­ï¼Œè¯·æ³¨æ„è¾“å…¥*x*å·²ç»è¢«å¡«å……ä¸ºé›¶ï¼Œè¿™åœ¨å·ç§¯å±‚ä¸­æ˜¯å¸¸è§çš„æ“ä½œï¼Œä¾‹å¦‚å½“æˆ‘ä»¬å¸Œæœ›è¾“å‡ºä¸åŸå§‹è¾“å…¥ï¼ˆä¾‹å¦‚æœ¬ä¾‹ä¸­çš„3 Ã— 3å¤§å°ï¼‰ç›¸åŒå°ºå¯¸æ—¶ã€‚å¡«å……çš„æ¦‚å¿µåœ¨æœ¬ç« åé¢è¿›ä¸€æ­¥å‘å±•ã€‚
- en: 'The proper mathematical term for this operation is actually *cross-correlation*,
    though *convolutionÂ *is commonly used in the machine learning community. The cross-correlation
    of a matrix,Â *x*, with a filter,Â *w*, isÂ ![](img/bc0a22cf-8707-490f-9dcf-d6799fadb04f.png):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ“ä½œçš„æ­£ç¡®æ•°å­¦æœ¯è¯­å®é™…ä¸Šæ˜¯*äº¤å‰ç›¸å…³*ï¼Œå°½ç®¡åœ¨æœºå™¨å­¦ä¹ ç¤¾åŒºä¸­é€šå¸¸ä½¿ç”¨*å·ç§¯*ã€‚çŸ©é˜µ*x*ä¸æ»¤æ³¢å™¨*w*çš„äº¤å‰ç›¸å…³å®šä¹‰ä¸º![](img/bc0a22cf-8707-490f-9dcf-d6799fadb04f.png)ï¼š
- en: '![](img/7679d42b-e3ac-4046-b758-00bd8baab798.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7679d42b-e3ac-4046-b758-00bd8baab798.png)'
- en: 'Notice the correspondence with our equation for *z*. On the other hand, the
    actual mathematical convolution of a matrix,Â *x*, with a filter,Â *w*, is for all
    valid positions (*i*, *j*):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„æˆ‘ä»¬ç”¨äº*z*çš„æ–¹ç¨‹å¼çš„å¯¹åº”å…³ç³»ã€‚å¦ä¸€æ–¹é¢ï¼ŒçŸ©é˜µ*x*ä¸æ»¤æ³¢å™¨*w*çš„å®é™…æ•°å­¦å·ç§¯å¯¹æ‰€æœ‰æœ‰æ•ˆä½ç½®(*i*, *j*)å®šä¹‰ä¸ºï¼š
- en: '![](img/cd2644fe-7b8b-45ee-b290-56ce8f40cbba.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd2644fe-7b8b-45ee-b290-56ce8f40cbba.png)'
- en: As we can see, both operations are quite similar in this setup, and convolution
    results can be obtained from the cross-correlation operation by simply *flippingÂ *the
    filters before it.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œåœ¨è¿™ç§è®¾ç½®ä¸­ï¼Œè¿™ä¸¤ç§æ“ä½œéå¸¸ç›¸ä¼¼ï¼Œé€šè¿‡ç®€å•åœ°åœ¨æ‰§è¡Œä¹‹å‰*ç¿»è½¬*æ»¤æ³¢å™¨ï¼Œå¯ä»¥ä»äº¤å‰ç›¸å…³æ“ä½œä¸­è·å¾—å·ç§¯ç»“æœã€‚
- en: Properties
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å±æ€§
- en: A convolutional layer with *N* sets of different neurons is thus defined byÂ *N*
    weight matrices (also called **filters** or **kernels**) of shapeÂ *D* Ã—Â *k* Ã—
    *k* (when the filters are square), andÂ *N* bias values. Therefore, this layer
    only hasÂ *N* Ã— (*D**k*Â² + 1) values to train. A fully connected layer with similar
    input and output dimensions would need (*H* Ã— *W* Ã— *D*) Ã— (*H*[o] Ã— *W*[o] Ã—
    *N*) parameters instead. As we demonstrated previously, the number of parameters
    for fully connected layers is influenced by the dimensionality of the data, whereas
    this does not affect the parameter numbers for convolutional layers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå…·æœ‰*N*ç»„ä¸åŒç¥ç»å…ƒçš„å·ç§¯å±‚ç”±å½¢çŠ¶ä¸º*D* Ã— *k* Ã— *k*ï¼ˆå½“æ»¤æ³¢å™¨ä¸ºæ­£æ–¹å½¢æ—¶ï¼‰çš„*N*ä¸ªæƒé‡çŸ©é˜µï¼ˆä¹Ÿç§°ä¸º**æ»¤æ³¢å™¨**æˆ–**æ ¸å¿ƒ**ï¼‰å’Œ*N*ä¸ªåç½®å€¼å®šä¹‰ã€‚å› æ­¤ï¼Œè¿™ä¸€å±‚åªéœ€è®­ç»ƒ*N*
    Ã— (*D**k*Â² + 1)ä¸ªå€¼ã€‚ä¸å…·æœ‰ç›¸ä¼¼è¾“å…¥å’Œè¾“å‡ºç»´åº¦çš„å…¨è¿æ¥å±‚ä¸åŒï¼Œåè€…éœ€è¦(*H* Ã— *W* Ã— *D*) Ã— (*H*[o] Ã— *W*[o]
    Ã— *N*)ä¸ªå‚æ•°ã€‚æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€ç¤ºï¼Œå…¨è¿æ¥å±‚çš„å‚æ•°æ•°é‡å—æ•°æ®ç»´åº¦çš„å½±å“ï¼Œè€Œè¿™å¹¶ä¸å½±å“å·ç§¯å±‚çš„å‚æ•°æ•°é‡ã€‚
- en: This property makes convolutional layers really powerful tools in computer vision
    for two reasons. First, as implied in the previous paragraph, it means we can
    train networks for larger input images without impacting the number of parameters
    we would need to tune. Second, this also means that convolutional layers can be
    applied to any images, irrespective of their dimensions! Unlike networks with
    fully connected layers, purely convolutional ones do not need to be adapted and
    retrained for inputs of different sizes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£æ˜¯è¿™ä¸€ç‰¹æ€§ä½¿å¾—å·ç§¯å±‚åœ¨è®¡ç®—æœºè§†è§‰ä¸­æˆä¸ºå¼ºå¤§çš„å·¥å…·ï¼ŒåŸå› æœ‰ä¸¤ç‚¹ã€‚é¦–å…ˆï¼Œæ­£å¦‚å‰æ–‡æ‰€ç¤ºï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥è®­ç»ƒé€‚ç”¨äºæ›´å¤§è¾“å…¥å›¾åƒçš„ç½‘ç»œï¼Œè€Œä¸å½±å“éœ€è¦è°ƒæ•´çš„å‚æ•°æ•°é‡ã€‚å…¶æ¬¡ï¼Œè¿™ä¹Ÿæ„å‘³ç€å·ç§¯å±‚å¯ä»¥åº”ç”¨äºä»»ä½•å°ºå¯¸çš„å›¾åƒï¼ä¸å…·æœ‰å…¨è¿æ¥å±‚çš„ç½‘ç»œä¸åŒï¼Œçº¯å·ç§¯å±‚ä¸éœ€è¦ä¸ºä¸åŒå°ºå¯¸çš„è¾“å…¥è¿›è¡Œé€‚åº”å’Œé‡æ–°è®­ç»ƒã€‚
- en: When applying a CNN to images of various sizes, you still need to be careful
    when sampling the input batches. Indeed, a subset of images can be stacked together
    into a normal batch tensor only if they all have the same dimensions. Therefore,
    in practice, you should either sort the images before batching them (mostly done
    during the training phase) or simply process each image separately (usually during
    the testing phase). However, both to simplify data processing and the network's
    task, people usually preprocess their images so they are all the same size (through
    scaling and/or cropping).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å°†CNNåº”ç”¨äºå„ç§å¤§å°çš„å›¾åƒæ—¶ï¼Œå¯¹è¾“å…¥æ‰¹æ¬¡è¿›è¡Œé‡‡æ ·ä»ç„¶éœ€è¦æ³¨æ„ã€‚äº‹å®ä¸Šï¼Œåªæœ‰æ‰€æœ‰å›¾åƒå…·æœ‰ç›¸åŒå°ºå¯¸æ—¶ï¼Œæ‰èƒ½å°†å›¾åƒçš„å­é›†å †å åˆ°æ™®é€šçš„æ‰¹æ¬¡å¼ é‡ä¸­ã€‚å› æ­¤ï¼Œåœ¨å®è·µä¸­ï¼Œåœ¨æ‰¹å¤„ç†ä¹‹å‰åº”å¯¹å›¾åƒè¿›è¡Œæ’åºï¼ˆä¸»è¦åœ¨è®­ç»ƒé˜¶æ®µï¼‰æˆ–ç®€å•åœ°åˆ†åˆ«å¤„ç†æ¯ä¸ªå›¾åƒï¼ˆé€šå¸¸åœ¨æµ‹è¯•é˜¶æ®µï¼‰ã€‚ç„¶è€Œï¼Œä¸ºäº†ç®€åŒ–æ•°æ®å¤„ç†å’Œç½‘ç»œä»»åŠ¡ï¼Œäººä»¬é€šå¸¸ä¼šé¢„å¤„ç†å›¾åƒï¼Œä½¿å®ƒä»¬çš„å¤§å°éƒ½ç›¸åŒï¼ˆé€šè¿‡ç¼©æ”¾å’Œ/æˆ–è£å‰ªï¼‰ã€‚
- en: Besides those computational optimizations, convolutional layers also have interesting
    properties related to image processing. With training, the layer's filters become
    really good at reacting to specific *local features* (a layer withÂ *N* filters
    means the possibility to react toÂ *N* different features). Each kernel of the
    first convolutional layer in a CNN would, for instance, learn to activate for
    a specific low-level feature, such as a specific line orientation or color gradient.
    Then, deeper layers would use these results to localize more abstract/advanced
    features, such as the shape of a face, and the contours of a particular object.
    Moreover, each filter (that is, each set of shared neurons) would respond to a
    specific image feature, whatever its location(s) in the image. More formally,
    convolutional layers are invariant to translation in the image coordinate space.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: é™¤äº†é‚£äº›è®¡ç®—ä¼˜åŒ–å¤–ï¼Œå·ç§¯å±‚è¿˜æœ‰ä¸€äº›ä¸å›¾åƒå¤„ç†ç›¸å…³çš„æœ‰è¶£ç‰¹æ€§ã€‚ç»è¿‡è®­ç»ƒï¼Œå·ç§¯å±‚çš„æ»¤æ³¢å™¨èƒ½å¤Ÿéå¸¸æ“…é•¿å¯¹ç‰¹å®šçš„*å±€éƒ¨ç‰¹å¾*åšå‡ºååº”ï¼ˆä¸€ä¸ªæ‹¥æœ‰*N*ä¸ªæ»¤æ³¢å™¨çš„å±‚æ„å‘³ç€èƒ½å¤Ÿå¯¹*N*ç§ä¸åŒçš„ç‰¹å¾åšå‡ºååº”ï¼‰ã€‚ä¾‹å¦‚ï¼ŒCNNä¸­ç¬¬ä¸€å±‚å·ç§¯å±‚çš„æ¯ä¸ªå·ç§¯æ ¸å°†å­¦ä¹ å¯¹ç‰¹å®šçš„ä½çº§ç‰¹å¾åšå‡ºå“åº”ï¼Œæ¯”å¦‚ç‰¹å®šçš„çº¿æ¡æ–¹å‘æˆ–é¢œè‰²æ¸å˜ã€‚æ¥ä¸‹æ¥ï¼Œè¾ƒæ·±çš„å±‚å°†ä½¿ç”¨è¿™äº›ç»“æœæ¥å®šä½æ›´åŠ æŠ½è±¡æˆ–é«˜çº§çš„ç‰¹å¾ï¼Œæ¯”å¦‚äººè„¸çš„å½¢çŠ¶æˆ–ç‰¹å®šç‰©ä½“çš„è½®å»“ã€‚æ­¤å¤–ï¼Œæ¯ä¸ªæ»¤æ³¢å™¨ï¼ˆå³æ¯ç»„å…±äº«çš„ç¥ç»å…ƒï¼‰éƒ½ä¼šå¯¹å›¾åƒä¸­çš„ç‰¹å®šç‰¹å¾ä½œå‡ºååº”ï¼Œæ— è®ºè¯¥ç‰¹å¾åœ¨å›¾åƒä¸­çš„ä½ç½®å¦‚ä½•ã€‚æ›´æ­£å¼åœ°è¯´ï¼Œå·ç§¯å±‚å¯¹å›¾åƒåæ ‡ç©ºé—´ä¸­çš„å¹³ç§»æ˜¯ä¸å˜çš„ã€‚
- en: The response map of a filter over the input image can be described as a map
    representing the locations where the filter responded to its target feature. For
    this reason, those intermediary results in CNNs are commonly called **feature
    maps**. A layer withÂ *N* filters will, therefore, returnÂ *N* feature maps, each
    corresponding to the detection of a particular feature in the input tensors. The
    stack of *N* feature maps returned by a layer is commonly called a **feature volume**
    (with a shape ofÂ *H*[o] Ã— *W*[o] Ã— *N*).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ»¤æ³¢å™¨å¯¹è¾“å…¥å›¾åƒçš„å“åº”å›¾å¯ä»¥æè¿°ä¸ºä¸€ä¸ªè¡¨ç¤ºæ»¤æ³¢å™¨å¯¹ç›®æ ‡ç‰¹å¾å“åº”ä½ç½®çš„åœ°å›¾ã€‚å› æ­¤ï¼Œè¿™äº›ä¸­é—´ç»“æœåœ¨CNNä¸­é€šå¸¸è¢«ç§°ä¸º**ç‰¹å¾å›¾**ã€‚å› æ­¤ï¼Œä¸€ä¸ªæ‹¥æœ‰*N*ä¸ªæ»¤æ³¢å™¨çš„å±‚å°†è¿”å›*N*ä¸ªç‰¹å¾å›¾ï¼Œæ¯ä¸ªç‰¹å¾å›¾å¯¹åº”äºè¾“å…¥å¼ é‡ä¸­ç‰¹å®šç‰¹å¾çš„æ£€æµ‹ã€‚ç”±å±‚è¿”å›çš„*N*ä¸ªç‰¹å¾å›¾çš„å †å é€šå¸¸è¢«ç§°ä¸º**ç‰¹å¾ä½“ç§¯**ï¼ˆå½¢çŠ¶ä¸º*H*[o]
    Ã— *W*[o] Ã— *N*ï¼‰ã€‚
- en: Hyperparameters
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¶…å‚æ•°
- en: A convolutional layer is first defined by its number of filters,Â *N*, by its
    input depth,Â *D* (that is, the number of input channels), and by its filter/kernel
    size, (*k[H]*, *k[W]*). As square filters are commonly used, the size is usually
    simply defined by *k* (though, as mentioned earlier, non-square filters are sometimes
    considered).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å·ç§¯å±‚é¦–å…ˆç”±å…¶æ»¤æ³¢å™¨çš„æ•°é‡*N*ã€è¾“å…¥æ·±åº¦*D*ï¼ˆå³è¾“å…¥é€šé“çš„æ•°é‡ï¼‰å’Œæ»¤æ³¢å™¨/å·ç§¯æ ¸çš„å¤§å°ï¼ˆ*k[H]*ï¼Œ*k[W]*ï¼‰æ¥å®šä¹‰ã€‚ç”±äºå¸¸ç”¨çš„æ˜¯æ–¹å½¢æ»¤æ³¢å™¨ï¼Œå¤§å°é€šå¸¸ä»…ç”±*k*æ¥å®šä¹‰ï¼ˆå°½ç®¡å¦‚å‰æ‰€è¿°ï¼Œæœ‰æ—¶ä¹Ÿä¼šè€ƒè™‘éæ–¹å½¢æ»¤æ³¢å™¨ï¼‰ã€‚
- en: However, as mentioned previously, convolutional layers actually differ from
    the homonym mathematical operation. The operation between the input and their
    filters can take several additional hyperparameters, affecting the way the filters
    are *sliding* over the images.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæ­£å¦‚å‰é¢æåˆ°çš„ï¼Œå·ç§¯å±‚å®é™…ä¸Šä¸åŒåçš„æ•°å­¦è¿ç®—æœ‰æ‰€ä¸åŒã€‚è¾“å…¥ä¸æ»¤æ³¢å™¨ä¹‹é—´çš„æ“ä½œå¯ä»¥æœ‰å‡ ä¸ªé¢å¤–çš„è¶…å‚æ•°ï¼Œå½±å“æ»¤æ³¢å™¨åœ¨å›¾åƒä¸Š*æ»‘åŠ¨*çš„æ–¹å¼ã€‚
- en: First, we can apply different *strides* with which the filters are sliding.
    The stride hyperparameter thus defines whether the dot product between the image
    patches and the filters should be computed at every position when sliding (*stride
    = 1*), or everyÂ *s* position (*stride = s*). The larger the stride, the sparser
    the resulting feature maps.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨ä¸åŒçš„*æ­¥å¹…*ï¼Œå³æ»¤æ³¢å™¨æ»‘åŠ¨çš„æ­¥ä¼ã€‚æ­¥å¹…è¶…å‚æ•°å®šä¹‰äº†å½“æ»‘åŠ¨æ—¶ï¼Œå›¾åƒå—ä¸æ»¤æ³¢å™¨ä¹‹é—´çš„ç‚¹ç§¯æ˜¯å¦åº”åœ¨æ¯ä¸ªä½ç½®è®¡ç®—ï¼ˆ*stride = 1*ï¼‰ï¼Œè¿˜æ˜¯åœ¨æ¯*s*ä¸ªä½ç½®è®¡ç®—ï¼ˆ*stride
    = s*ï¼‰ã€‚æ­¥å¹…è¶Šå¤§ï¼Œå¾—åˆ°çš„ç‰¹å¾å›¾å°±è¶Šç¨€ç–ã€‚
- en: Images can also be *zero-padded* before convolution; that is, their sizes can
    be synthetically increased by adding rows and columns of zeros around their original
    content. As shown in *Figure 3.2*, this padding increases the number of positions
    the filters can take over the images. We can thus specify the padding value to
    be applied (that is, the number of empty rows and columns to be added on each
    side of the inputs).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒåœ¨è¿›è¡Œå·ç§¯ä¹‹å‰ä¹Ÿå¯ä»¥è¿›è¡Œ*é›¶å¡«å……*ï¼›å³é€šè¿‡åœ¨åŸå§‹å†…å®¹å‘¨å›´æ·»åŠ é›¶çš„è¡Œå’Œåˆ—ï¼Œåˆæˆåœ°å¢åŠ å›¾åƒçš„å°ºå¯¸ã€‚å¦‚*å›¾3.2*æ‰€ç¤ºï¼Œè¿™ç§å¡«å……å¢åŠ äº†æ»¤æ³¢å™¨å¯ä»¥è¦†ç›–çš„å›¾åƒä½ç½®æ•°é‡ã€‚æˆ‘ä»¬å› æ­¤å¯ä»¥æŒ‡å®šè¦åº”ç”¨çš„å¡«å……å€¼ï¼ˆå³è¦åœ¨è¾“å…¥çš„æ¯ä¸€ä¾§æ·»åŠ çš„ç©ºè¡Œå’Œç©ºåˆ—çš„æ•°é‡ï¼‰ã€‚
- en: The letterÂ *k* is commonly used for the filter/kernel size (*k* for *kernel*).
    Similarly,Â *s* is commonly used for the stride, andÂ *p* for the padding. Note
    that, as with the filter size, the same values are usually used for the horizontal
    and vertical strides *(s = s[H] = s[W]),* as well as for the horizontal and vertical
    padding; though, for some specific use cases, they may have different values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å­—æ¯*k*é€šå¸¸ç”¨äºè¡¨ç¤ºæ»¤æ³¢å™¨/æ ¸å¤§å°ï¼ˆ*k*ä»£è¡¨*kernel*ï¼‰ã€‚ç±»ä¼¼åœ°ï¼Œ*s*é€šå¸¸ç”¨äºè¡¨ç¤ºæ­¥å¹…ï¼Œ*p*ç”¨äºè¡¨ç¤ºå¡«å……ã€‚è¯·æ³¨æ„ï¼Œä¸æ»¤æ³¢å™¨å¤§å°ä¸€æ ·ï¼Œé€šå¸¸ä¼šå¯¹æ°´å¹³å’Œå‚ç›´æ­¥å¹…ä½¿ç”¨ç›¸åŒçš„å€¼ï¼ˆ*s
    = s[H] = s[W]*ï¼‰ï¼Œå¹¶ä¸”æ°´å¹³å’Œå‚ç›´å¡«å……ä¹Ÿé€šå¸¸ä½¿ç”¨ç›¸åŒçš„å€¼ï¼›ä¸è¿‡ï¼Œåœ¨æŸäº›ç‰¹å®šçš„ä½¿ç”¨æ¡ˆä¾‹ä¸­ï¼Œå®ƒä»¬å¯èƒ½ä¼šæœ‰ä¸åŒçš„å€¼ã€‚
- en: 'All these parameters (the number of kernels,Â *N;*Â kernel size,Â *k;*Â stride,Â *s;*Â and
    padding,Â *p*) not only affect the layer''s operations, but also its output shape.
    Until now, we defined this shape as (*H*[o], *W*[o], *N*), with *H*[o] andÂ *W*[o]
    the number of times the neuron can slide vertically and horizontally over the
    inputs. So, what actuallyÂ areÂ *H*[o] and *W*[o]? Formally, they can be computed
    as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›å‚æ•°ï¼ˆæ ¸çš„æ•°é‡*N*ï¼›æ ¸çš„å¤§å°*k*ï¼›æ­¥å¹…*s*ï¼›ä»¥åŠå¡«å……*p*ï¼‰ä¸ä»…å½±å“å±‚çš„æ“ä½œï¼Œè¿˜ä¼šå½±å“å…¶è¾“å‡ºå½¢çŠ¶ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å®šä¹‰çš„è¾“å‡ºå½¢çŠ¶ä¸º(*H*[o]ï¼Œ*W*[o]ï¼Œ*N*)ï¼Œå…¶ä¸­*H*[o]å’Œ*W*[o]*æ˜¯ç¥ç»å…ƒåœ¨è¾“å…¥ä¸Šå‚ç›´å’Œæ°´å¹³æ»‘åŠ¨çš„æ¬¡æ•°ã€‚é‚£ä¹ˆï¼Œ*H*[o]å’Œ*W*[o]*åˆ°åº•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿä»å½¢å¼ä¸Šæ¥çœ‹ï¼Œå®ƒä»¬å¯ä»¥æŒ‰ä»¥ä¸‹æ–¹å¼è®¡ç®—ï¼š
- en: '![](img/ea6641bb-f197-4660-bd66-92dec120b447.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ea6641bb-f197-4660-bd66-92dec120b447.png)'
- en: While we invite you to pick some concrete examples to better grasp these formulas,
    we can intuitively understand the logic behind them. Filters of sizeÂ ğ‘˜ can take
    a maximum ofÂ *H - k + 1* different vertical positions and *W - k + 1* horizontal
    ones in images of size *H* Ã— *W*. Additionally, this number of positions increases
    to *H - k + 2p + 1Â *(with respect toÂ *W - k + 2p + 1*) if these images are padded
    byÂ *p* on every side. Finally, increasing the stride,Â *s,* basically means considering
    only one position out of *s*, explaining the division (note that it is an integer
    division).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æˆ‘ä»¬é‚€è¯·ä½ é€‰æ‹©ä¸€äº›å…·ä½“çš„ä¾‹å­æ¥æ›´å¥½åœ°ç†è§£è¿™äº›å…¬å¼ï¼Œä½†æˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°ç†è§£å®ƒä»¬èƒŒåçš„é€»è¾‘ã€‚å¤§å°ä¸ºğ‘˜çš„æ»¤æ³¢å™¨åœ¨å¤§å°ä¸º*H* Ã— *W*çš„å›¾åƒä¸­ï¼Œå¯ä»¥å æ®æœ€å¤š*H
    - k + 1*ä¸ªä¸åŒçš„å‚ç›´ä½ç½®å’Œ*W - k + 1*ä¸ªæ°´å¹³ä½ç½®ã€‚æ­¤å¤–ï¼Œå¦‚æœè¿™äº›å›¾åƒåœ¨æ¯ä¸€è¾¹éƒ½è¿›è¡Œäº†*p*çš„å¡«å……ï¼Œé‚£ä¹ˆè¿™äº›ä½ç½®çš„æ•°é‡å°†å¢åŠ åˆ°*H - k
    + 2p + 1*ï¼ˆå…³äº*W - k + 2p + 1*ï¼‰ã€‚æœ€åï¼Œå¢åŠ æ­¥å¹…*s*ï¼ŒåŸºæœ¬ä¸Šæ„å‘³ç€åªè€ƒè™‘*s*ä¸ªä½ç½®ä¸­çš„ä¸€ä¸ªï¼Œè¿™å°±è§£é‡Šäº†é™¤æ³•ï¼ˆæ³¨æ„è¿™æ˜¯æ•´æ•°é™¤æ³•ï¼‰ã€‚
- en: With these hyperparameters, we can easily control the layer's output sizes.
    This is particularly convenient for applications such as object segmentation;
    that is, when we want the output segmentation mask to be the same size as the
    input image.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™äº›è¶…å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾æ§åˆ¶å±‚çš„è¾“å‡ºå°ºå¯¸ã€‚è¿™åœ¨ç‰©ä½“åˆ†å‰²ç­‰åº”ç”¨ä¸­å°¤å…¶æ–¹ä¾¿ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“æˆ‘ä»¬å¸Œæœ›è¾“å‡ºçš„åˆ†å‰²æ©ç ä¸è¾“å…¥å›¾åƒçš„å¤§å°ç›¸åŒã€‚
- en: TensorFlow/Keras methods
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow/Kerasæ–¹æ³•
- en: 'Available in the low-level API, `tf.nn.conv2d()` (refer to the documentation
    atÂ [https://www.tensorflow.org/api_docs/python/tf/nn/conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d))
    is the default choice for image convolution. Its main parameters are as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½çº§APIä¸­å¯ç”¨ï¼Œ`tf.nn.conv2d()`ï¼ˆè¯·å‚è€ƒæ–‡æ¡£ï¼š[https://www.tensorflow.org/api_docs/python/tf/nn/conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)ï¼‰æ˜¯è¿›è¡Œå›¾åƒå·ç§¯çš„é»˜è®¤é€‰æ‹©ã€‚å…¶ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š
- en: '`input`: The batch of input images, of shape *(B, H, W, D*), withÂ *BÂ *being
    the batch size.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input`ï¼šè¾“å…¥å›¾åƒçš„æ‰¹æ¬¡ï¼Œå½¢çŠ¶ä¸º*(B, H, W, D*)ï¼Œå…¶ä¸­*B*æ˜¯æ‰¹æ¬¡å¤§å°ã€‚'
- en: '`filter`: TheÂ *N* filters stacked into a tensor of shape (*k[H], k[W], D, N*).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter`ï¼šå †å æˆå½¢çŠ¶ä¸º(*k[H], k[W], D, N*)çš„*N*ä¸ªæ»¤æ³¢å™¨ã€‚'
- en: '`strides`: A list of four integers representing the stride for each dimension
    of the batched input. Typically, you would use *[1, s[H], s[W], 1*] (that is,
    applying a custom stride only for the two spatial dimensions of the image).'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strides`ï¼šè¡¨ç¤ºæ‰¹é‡è¾“å…¥æ¯ä¸ªç»´åº¦æ­¥å¹…çš„å››ä¸ªæ•´æ•°çš„åˆ—è¡¨ã€‚é€šå¸¸ï¼Œä½ ä¼šä½¿ç”¨*[1, s[H], s[W], 1]*ï¼ˆå³ï¼Œä»…å¯¹å›¾åƒçš„ä¸¤ä¸ªç©ºé—´ç»´åº¦åº”ç”¨è‡ªå®šä¹‰æ­¥å¹…ï¼‰ã€‚'
- en: '`padding`: Either a list of *4 Ã— 2* integers representing the padding before
    and after each dimension of the batched input, or a string defining which predefined
    padding case to use; that is, either `VALID`Â or `SAME`Â (explanations follow).'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`ï¼šä¸€ä¸ª*4 Ã— 2*æ•´æ•°åˆ—è¡¨ï¼Œè¡¨ç¤ºæ¯ä¸ªæ‰¹é‡è¾“å…¥ç»´åº¦çš„å‰åå¡«å……ï¼Œæˆ–è€…ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå®šä¹‰è¦ä½¿ç”¨çš„é¢„å®šä¹‰å¡«å……æ¡ˆä¾‹ï¼›å³ï¼Œ`VALID`æˆ–`SAME`ï¼ˆæ¥ä¸‹æ¥çš„è§£é‡Šï¼‰ã€‚'
- en: '`name`: The name to identify this operation (useful for creating clear, readable
    graphs).'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`ï¼šç”¨äºæ ‡è¯†è¯¥æ“ä½œçš„åç§°ï¼ˆæœ‰åŠ©äºåˆ›å»ºæ¸…æ™°ã€æ˜“è¯»çš„å›¾å½¢ï¼‰ã€‚'
- en: 'Note that `tf.nn.conv2d()` accepts some other more advanced parameters, which
    we will not cover yet (refer to the documentation). *Figures 3.3* and*Â 3.4* illustrate
    the effects of two convolutional operations with different arguments:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œ`tf.nn.conv2d()`æ¥å—ä¸€äº›æ›´é«˜çº§çš„å‚æ•°ï¼Œæˆ‘ä»¬æš‚æ—¶ä¸ä¼šä»‹ç»ï¼ˆè¯·å‚è€ƒæ–‡æ¡£ï¼‰ã€‚*å›¾3.3*å’Œ*3.4*å±•ç¤ºäº†ä¸¤ç§ä¸åŒå‚æ•°çš„å·ç§¯æ“ä½œæ•ˆæœï¼š
- en: '![](img/8a9c4782-cf35-48eb-b204-c14d5a29ecb9.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8a9c4782-cf35-48eb-b204-c14d5a29ecb9.png)'
- en: 'Figure 3.3: Example of a convolution performed on an image with TensorFlow.
    The kernel here is a well-known one, commonly used to apply *Gaussian blur* to
    images'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3.3ï¼šä½¿ç”¨ TensorFlow å¯¹å›¾åƒè¿›è¡Œå·ç§¯çš„ç¤ºä¾‹ã€‚è¿™é‡Œçš„å·ç§¯æ ¸æ˜¯ä¸€ä¸ªè‘—åçš„å·ç§¯æ ¸ï¼Œå¸¸ç”¨äºå¯¹å›¾åƒåº”ç”¨*é«˜æ–¯æ¨¡ç³Š*ã€‚
- en: 'In the following screenshot, a kernel that''s well known in computer vision
    is applied:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä»¥ä¸‹æˆªå›¾ä¸­ï¼Œåº”ç”¨äº†è®¡ç®—æœºè§†è§‰é¢†åŸŸä¸­ä¸€ä¸ªçŸ¥åçš„å·ç§¯æ ¸ï¼š
- en: '![](img/0451566f-c965-4ee1-8b51-c38f95f12463.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0451566f-c965-4ee1-8b51-c38f95f12463.png)'
- en: 'Figure 3.4: Example of another TensorFlow convolution, with a larger stride.
    This specific kernel is commonly used to extract edges/contours in images'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3.4ï¼šå¦ä¸€ä¸ª TensorFlow å·ç§¯çš„ç¤ºä¾‹ï¼Œå…·æœ‰æ›´å¤§çš„æ­¥å¹…ã€‚è¿™ä¸ªç‰¹å®šçš„å·ç§¯æ ¸é€šå¸¸ç”¨äºæå–å›¾åƒä¸­çš„è¾¹ç¼˜/è½®å»“ã€‚
- en: Regarding padding, TensorFlow developers made the choice to provide two different
    pre-implemented modes so that users do not have to figure out which value,Â *p,*
    they need for usual cases.Â `VALID`Â means the images won't be padded (*p* = 0),
    and the filters will slide only over the default *valid* positions. When opting
    for `SAME`, TensorFlow will calculate the value,Â *p,* so that the convolution
    outputs have the *same* height and width as the inputs for a stride of `1`Â (that
    is, solvingÂ *H*[o] =Â *H*[o] and *W*[o] =Â *W* given the equations presented in
    the previous section, temporarily settingÂ *s*Â to 1).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºå¡«å……ï¼ŒTensorFlow å¼€å‘è€…é€‰æ‹©æä¾›ä¸¤ç§ä¸åŒçš„é¢„å®ç°æ¨¡å¼ï¼Œä»¥ä¾¿ç”¨æˆ·æ— éœ€è‡ªå·±å»ææ¸…æ¥šåœ¨å¸¸è§„æƒ…å†µä¸‹éœ€è¦ä½¿ç”¨å“ªä¸ªå€¼ï¼Œ*p*ã€‚`VALID` è¡¨ç¤ºå›¾åƒä¸ä¼šè¢«å¡«å……ï¼ˆ*p*
    = 0ï¼‰ï¼Œæ»¤æ³¢å™¨åªä¼šåœ¨é»˜è®¤çš„*æœ‰æ•ˆ*ä½ç½®ä¸Šæ»‘åŠ¨ã€‚è€Œé€‰æ‹© `SAME` æ—¶ï¼ŒTensorFlow ä¼šè®¡ç®— *p* çš„å€¼ï¼Œä»¥ç¡®ä¿å·ç§¯è¾“å‡ºçš„é«˜åº¦å’Œå®½åº¦ä¸è¾“å…¥åœ¨æ­¥å¹…ä¸º
    `1` æ—¶ç›¸åŒï¼ˆä¹Ÿå°±æ˜¯è¯´ï¼Œæ ¹æ®å‰é¢ç« èŠ‚ä¸­ç»™å‡ºçš„æ–¹ç¨‹ï¼Œæš‚æ—¶å°† *s* è®¾ç½®ä¸º 1ï¼Œä»è€Œè§£å¾— *H*[o] = *H*[o] å’Œ *W*[o] = *W*ï¼‰ã€‚
- en: Sometimes, you may want to pad with something more complex than zeros. In those
    cases, it is recommended to use the `tf.pad()`Â method (refer to the documentation
    at [https://www.tensorflow.org/api_docs/python/tf/pad](https://www.tensorflow.org/api_docs/python/tf/pad))
    instead, and then simply instantiate a convolution operation with `VALID`Â padding.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰æ—¶å€™ï¼Œä½ å¯èƒ½éœ€è¦ä½¿ç”¨æ¯”é›¶æ›´å¤æ‚çš„å¡«å……ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå»ºè®®ä½¿ç”¨ `tf.pad()` æ–¹æ³•ï¼ˆè¯·å‚è€ƒæ–‡æ¡£ï¼š[https://www.tensorflow.org/api_docs/python/tf/pad](https://www.tensorflow.org/api_docs/python/tf/pad)ï¼‰ï¼Œç„¶åç®€å•åœ°å®ä¾‹åŒ–ä¸€ä¸ªä½¿ç”¨
    `VALID` å¡«å……çš„å·ç§¯æ“ä½œã€‚
- en: TensorFlow also offers several other low-level convolution methods, such as
    `tf.nn.conv1d()` (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/nn/conv1d](https://www.tensorflow.org/api_docs/python/tf/nn/conv1d))
    and `tf.nn.conv3d()` (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/nn/conv3d](https://www.tensorflow.org/api_docs/python/tf/nn/conv3d)),Â 
    for one-dimensional and three-dimensional data, respectively, or `tf.nn.depthwise_conv2d()`
    (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d))
    to convolve each channel of the images with different filters, and more.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow è¿˜æä¾›äº†å…¶ä»–å‡ ä¸ªä½çº§åˆ«çš„å·ç§¯æ–¹æ³•ï¼Œä¾‹å¦‚ `tf.nn.conv1d()`ï¼ˆè¯·å‚è€ƒæ–‡æ¡£ï¼š[https://www.tensorflow.org/api_docs/python/tf/nn/conv1d](https://www.tensorflow.org/api_docs/python/tf/nn/conv1d)ï¼‰å’Œ
    `tf.nn.conv3d()`ï¼ˆè¯·å‚è€ƒæ–‡æ¡£ï¼š[https://www.tensorflow.org/api_docs/python/tf/nn/conv3d](https://www.tensorflow.org/api_docs/python/tf/nn/conv3d)ï¼‰ï¼Œåˆ†åˆ«ç”¨äºä¸€ç»´å’Œä¸‰ç»´æ•°æ®ï¼Œæˆ–è€…ä½¿ç”¨
    `tf.nn.depthwise_conv2d()`ï¼ˆè¯·å‚è€ƒæ–‡æ¡£ï¼š[https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)ï¼‰å¯¹å›¾åƒçš„æ¯ä¸ªé€šé“è¿›è¡Œä¸åŒæ»¤æ³¢å™¨çš„å·ç§¯ç­‰ã€‚
- en: 'So far, we have only presented convolutions with fixed filters. For CNNs, we
    have to make the filters trainable. Convolutional layers also apply a learned
    bias before passing the result to an activation function. This series of operations
    can, therefore, be implemented as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªå±•ç¤ºäº†ä½¿ç”¨å›ºå®šæ»¤æ³¢å™¨çš„å·ç§¯ã€‚å¯¹äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œæˆ‘ä»¬å¿…é¡»ä½¿æ»¤æ³¢å™¨å¯è®­ç»ƒã€‚å·ç§¯å±‚è¿˜ä¼šåœ¨å°†ç»“æœä¼ é€’ç»™æ¿€æ´»å‡½æ•°ä¹‹å‰åº”ç”¨ä¸€ä¸ªå­¦ä¹ åˆ°çš„åç½®ã€‚å› æ­¤ï¼Œè¿™ä¸€ç³»åˆ—æ“ä½œå¯ä»¥å¦‚ä¸‹å®ç°ï¼š
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This feed-forward function can further be wrapped into a `Layer` object, similar
    to how the fully connected layer we implemented in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*, was built around the matrix operations.
    Through the Keras API, TensorFlow 2 provides its own `tf.keras.layers.Layer` class,
    which we can extend (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)).
    The following code block demonstrates how a simple convolution layer can be built
    on this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå‰é¦ˆå‡½æ•°å¯ä»¥è¿›ä¸€æ­¥å°è£…æˆä¸€ä¸ª`Layer`å¯¹è±¡ï¼Œç±»ä¼¼äºæˆ‘ä»¬åœ¨[ç¬¬1ç« ](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)ä¸­å®ç°çš„å…¨è¿æ¥å±‚ï¼Œ*è®¡ç®—æœºè§†è§‰ä¸ç¥ç»ç½‘ç»œ*ï¼Œæ˜¯å›´ç»•çŸ©é˜µæ“ä½œæ„å»ºçš„ã€‚é€šè¿‡Keras
    APIï¼ŒTensorFlow 2æä¾›äº†è‡ªå·±çš„`tf.keras.layers.Layer`ç±»ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å…¶è¿›è¡Œæ‰©å±•ï¼ˆå‚è§[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer)çš„æ–‡æ¡£ï¼‰ã€‚ä»¥ä¸‹ä»£ç å—æ¼”ç¤ºäº†å¦‚ä½•åŸºäºæ­¤æ„å»ºä¸€ä¸ªç®€å•çš„å·ç§¯å±‚ï¼š
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Most of TensorFlow's mathematical operations (for example, in `tf.math` and
    `tf.nn`) already have their derivatives defined by the framework. Therefore, as
    long as a layer is composed of such operations, we do not have to manually define
    its backpropagation, saving quite some effort!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlowçš„å¤§å¤šæ•°æ•°å­¦æ“ä½œï¼ˆä¾‹å¦‚`tf.math`å’Œ`tf.nn`ä¸­çš„æ“ä½œï¼‰å·²ç»ç”±æ¡†æ¶å®šä¹‰äº†å®ƒä»¬çš„å¯¼æ•°ã€‚å› æ­¤ï¼Œåªè¦ä¸€ä¸ªå±‚ç”±è¿™äº›æ“ä½œç»„æˆï¼Œæˆ‘ä»¬å°±ä¸éœ€è¦æ‰‹åŠ¨å®šä¹‰å®ƒçš„åå‘ä¼ æ’­ï¼ŒèŠ‚çœäº†å¤§é‡çš„ç²¾åŠ›ï¼
- en: 'While this implementation has the advantage of being explicit, the Keras API
    also encapsulates the initialization of common layers (as presented in [Chapter
    2](c7c49010-458f-47ef-a538-96118f9cd892.xhtml), *TensorFlow Basics and Training
    a Model*), thereby speeding up development. With the `tf.keras.layers` module,
    we can instantiate a similar convolutional layer in a single call, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡è¿™ä¸ªå®ç°çš„ä¼˜ç‚¹æ˜¯æ˜¾å¼çš„ï¼Œä½†Keras APIä¹Ÿå°è£…äº†å¸¸è§å±‚çš„åˆå§‹åŒ–ï¼ˆå¦‚[ç¬¬2ç« ](c7c49010-458f-47ef-a538-96118f9cd892.xhtml)ä¸­ä»‹ç»çš„ï¼Œ*TensorFlowåŸºç¡€ä¸æ¨¡å‹è®­ç»ƒ*ï¼‰ï¼Œä»è€ŒåŠ é€Ÿäº†å¼€å‘è¿‡ç¨‹ã€‚é€šè¿‡`tf.keras.layers`æ¨¡å—ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€æ¬¡è°ƒç”¨å®ä¾‹åŒ–ä¸€ä¸ªç±»ä¼¼çš„å·ç§¯å±‚ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`tf.keras.layers.Conv2D()` (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D))
    has a long list of additional parameters, encapsulating several concepts, such
    as weight regularization (presented later in this chapter). Therefore, it is recommended
    to use this method when building advanced CNNs, instead of spending time reimplementing
    such concepts.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.keras.layers.Conv2D()`ï¼ˆå‚è§[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)çš„æ–‡æ¡£ï¼‰æœ‰ä¸€ä¸ªé•¿é•¿çš„é™„åŠ å‚æ•°åˆ—è¡¨ï¼Œå°è£…äº†å¤šä¸ªæ¦‚å¿µï¼Œæ¯”å¦‚æƒé‡æ­£åˆ™åŒ–ï¼ˆå°†åœ¨æœ¬ç« ç¨åä»‹ç»ï¼‰ã€‚å› æ­¤ï¼Œå»ºè®®åœ¨æ„å»ºé«˜çº§CNNæ—¶ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œè€Œä¸æ˜¯èŠ±æ—¶é—´é‡æ–°å®ç°è¿™äº›æ¦‚å¿µã€‚'
- en: Pooling layers
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ± åŒ–å±‚
- en: Another commonly used category of layerÂ introduced with CNNs is the *pooling*
    type.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªä¸CNNä¸€èµ·ä½¿ç”¨çš„å¸¸è§å±‚ç±»åˆ«æ˜¯*æ± åŒ–*ç±»å‹ã€‚
- en: Concept and hyperparameters
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¦‚å¿µå’Œè¶…å‚æ•°
- en: These pooling layers are a bit peculiarÂ becauseÂ they do not have any trainable
    parameters. Each neuron simply takes the values in its *window* (the receptive
    field) and returns a single output, computed from a predefined function. The two
    most common pooling methods are max-pooling and average-pooling. **Max-pooling**
    layers return only the maximum value at each depth of the pooled area (refer to
    *Figure 3.5*), and **average-poolingÂ **layers compute the average at each depth
    of the pooled area (refer to *Figure 3.6*).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ± åŒ–å±‚æœ‰ç‚¹ç‰¹åˆ«ï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰ä»»ä½•å¯è®­ç»ƒçš„å‚æ•°ã€‚æ¯ä¸ªç¥ç»å…ƒä»…ä»…å–å…¶*çª—å£*ï¼ˆæ„Ÿå—é‡ï¼‰ä¸­çš„å€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå•ä¸€çš„è¾“å‡ºï¼Œè¯¥è¾“å‡ºæ˜¯é€šè¿‡é¢„å®šä¹‰å‡½æ•°è®¡ç®—å¾—å‡ºçš„ã€‚æœ€å¸¸è§çš„ä¸¤ç§æ± åŒ–æ–¹æ³•æ˜¯æœ€å¤§æ± åŒ–å’Œå¹³å‡æ± åŒ–ã€‚**æœ€å¤§æ± åŒ–**å±‚ä»…è¿”å›æ± åŒ–åŒºåŸŸæ¯ä¸ªæ·±åº¦çš„æœ€å¤§å€¼ï¼ˆå‚è€ƒ*å›¾
    3.5*ï¼‰ï¼Œè€Œ**å¹³å‡æ± åŒ–**å±‚è®¡ç®—æ± åŒ–åŒºåŸŸæ¯ä¸ªæ·±åº¦çš„å¹³å‡å€¼ï¼ˆå‚è€ƒ*å›¾ 3.6*ï¼‰ã€‚
- en: 'Pooling layers are commonly used with a *stride* value equal to the size of
    their *window/kernel size*, in order to apply the pooling function over non-overlapping
    patches. Their purpose is to *reduce the spatial dimensionality of the data*,
    cutting down the total number of parameters needed in the network, as well as
    its computation time. For instance, a pooling layer with a *2 Ã—Â 2* window size
    and stride ofÂ *2* (that is,Â *k* = 2 and *s* = 2) would take patches of four values
    at each depth and return a single number. It would thus divideÂ the height and
    the width of the featuresÂ by *2;*Â that is, dividingÂ the number of computations
    for the following layersÂ by *2 Ã—Â 2 = 4*. Finally, note that, as with convolutional
    layers, you can pad the tensors before applying the operation (as shown in *Figure
    3.5*):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ± åŒ–å±‚é€šå¸¸ä¸ *æ­¥å¹…* å€¼ç­‰äºå…¶ *çª—å£/æ ¸å¤§å°* ä¸€èµ·ä½¿ç”¨ï¼Œä»¥ä¾¿å¯¹ä¸é‡å çš„åŒºåŸŸåº”ç”¨æ± åŒ–å‡½æ•°ã€‚å®ƒä»¬çš„ç›®çš„æ˜¯ *å‡å°‘æ•°æ®çš„ç©ºé—´ç»´åº¦*ï¼Œä»è€Œå‡å°‘ç½‘ç»œä¸­æ‰€éœ€çš„å‚æ•°æ€»æ•°ä»¥åŠè®¡ç®—æ—¶é—´ã€‚ä¾‹å¦‚ï¼Œå…·æœ‰
    *2 Ã— 2* çª—å£å¤§å°å’Œæ­¥å¹…ä¸º *2* çš„æ± åŒ–å±‚ï¼ˆå³ *k* = 2 å’Œ *s* = 2ï¼‰å°†å–æ¯ä¸ªæ·±åº¦çš„å››ä¸ªå€¼ï¼Œå¹¶è¿”å›ä¸€ä¸ªå•ä¸€çš„æ•°å­—ã€‚è¿™æ ·ï¼Œå®ƒå°†æŠŠç‰¹å¾çš„é«˜åº¦å’Œå®½åº¦é™¤ä»¥
    *2*ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œå‡å°‘æ¥ä¸‹æ¥å±‚çš„è®¡ç®—æ¬¡æ•° *2 Ã— 2 = 4*ã€‚æœ€åï¼Œæ³¨æ„ï¼Œå’Œå·ç§¯å±‚ä¸€æ ·ï¼Œä½ å¯ä»¥åœ¨åº”ç”¨æ“ä½œä¹‹å‰å¯¹å¼ é‡è¿›è¡Œå¡«å……ï¼ˆå¦‚ *å›¾ 3.5* æ‰€ç¤ºï¼‰ï¼š
- en: '![](img/0594d28a-0092-4675-ba9b-4d181ffa9ab2.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0594d28a-0092-4675-ba9b-4d181ffa9ab2.png)'
- en: 'Figure 3.5: Illustration of a max-pooling operation with a window size of 3
    Ã— 3, a padding of 1, and a stride of 2 on a single-channel input'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3.5ï¼šå±•ç¤ºäº†ä¸€ä¸ªçª—å£å¤§å°ä¸º 3 Ã— 3ã€å¡«å……ä¸º 1ã€æ­¥å¹…ä¸º 2 çš„å•é€šé“è¾“å…¥çš„æœ€å¤§æ± åŒ–æ“ä½œ
- en: 'Through the padding and stride parameters, it is thus possible to control the
    dimensions of the resulting tensors. *Figure 3.6* provides another example:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å¡«å……ï¼ˆpaddingï¼‰å’Œæ­¥å¹…ï¼ˆstrideï¼‰å‚æ•°ï¼Œå¯ä»¥æ§åˆ¶ç”Ÿæˆå¼ é‡çš„ç»´åº¦ã€‚*å›¾ 3.6* æä¾›äº†å¦ä¸€ä¸ªç¤ºä¾‹ï¼š
- en: '![](img/aad0f6e0-c963-4531-be17-a1276f82aabc.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aad0f6e0-c963-4531-be17-a1276f82aabc.png)'
- en: 'Figure 3.6: Illustration of an average-pooling operation with a window size
    of 2 Ã—Â 2, a padding of 0, and a stride of 2 on a single-channel input'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3.6ï¼šå±•ç¤ºäº†ä¸€ä¸ªçª—å£å¤§å°ä¸º 2 Ã— 2ã€å¡«å……ä¸º 0ã€æ­¥å¹…ä¸º 2 çš„å•é€šé“è¾“å…¥çš„å¹³å‡æ± åŒ–æ“ä½œ
- en: With hyperparametersÂ being similar to convolutional layers except for the absence
    of trainable kernels, pooling layers are, therefore, easy to use and lightweight
    solutions for controlling data dimensionality.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ± åŒ–å±‚çš„è¶…å‚æ•°ä¸å·ç§¯å±‚ç±»ä¼¼ï¼Œé™¤äº†æ²¡æœ‰å¯è®­ç»ƒçš„å·ç§¯æ ¸ï¼Œå› æ­¤æ± åŒ–å±‚æ˜¯æ˜“äºä½¿ç”¨ä¸”è½»é‡çº§çš„è§£å†³æ–¹æ¡ˆï¼Œé€‚ç”¨äºæ§åˆ¶æ•°æ®çš„ç»´åº¦ã€‚
- en: TensorFlow/Keras methods
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow/Keras æ–¹æ³•
- en: 'Also available from the `tf.nn` package, `tf.nn.max_pool()` (refer to the documentation
    at [https://www.tensorflow.org/api_docs/python/tf/nn/max_pool](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool))
    and `tf.nn.avg_pool()` (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/nn/avg_pool](https://www.tensorflow.org/api_docs/python/tf/nn/avg_pool))
    conveniently have a signature quite similar to `tf.nn.conv2d()`, as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿå¯ä»¥ä» `tf.nn` åŒ…ä¸­è·å¾—ï¼Œ`tf.nn.max_pool()`ï¼ˆå‚è€ƒæ–‡æ¡£ [https://www.tensorflow.org/api_docs/python/tf/nn/max_pool](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool)ï¼‰å’Œ
    `tf.nn.avg_pool()`ï¼ˆå‚è€ƒæ–‡æ¡£ [https://www.tensorflow.org/api_docs/python/tf/nn/avg_pool](https://www.tensorflow.org/api_docs/python/tf/nn/avg_pool)ï¼‰çš„ç­¾åä¸
    `tf.nn.conv2d()` éå¸¸ç›¸ä¼¼ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '`value`: The batch of input images of shape (*B*, *H*, *W*, *D*), withÂ *BÂ *being
    the batch size'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value`ï¼šå½¢çŠ¶ä¸º (*B*, *H*, *W*, *D*) çš„è¾“å…¥å›¾åƒæ‰¹æ¬¡ï¼Œå…¶ä¸­ *B* æ˜¯æ‰¹æ¬¡å¤§å°'
- en: '`ksize`: A list of four integers representing the window size in each dimension;
    commonly, *[1, k, k, 1*] is used'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ksize`ï¼šä¸€ä¸ªåŒ…å«å››ä¸ªæ•´æ•°çš„åˆ—è¡¨ï¼Œè¡¨ç¤ºæ¯ä¸ªç»´åº¦çš„çª—å£å¤§å°ï¼›é€šå¸¸ä½¿ç”¨ *[1, k, k, 1]* '
- en: '`strides`: A list of four integers representing the stride for each dimension
    of the batched input, similar to `tf.nn.conv2d()`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`strides`ï¼šä¸€ä¸ªåŒ…å«å››ä¸ªæ•´æ•°çš„åˆ—è¡¨ï¼Œè¡¨ç¤ºæ‰¹å¤„ç†è¾“å…¥æ¯ä¸ªç»´åº¦çš„æ­¥å¹…ï¼Œç±»ä¼¼äº `tf.nn.conv2d()` '
- en: '`padding`: A string defining which padding algorithm to use (`VALID`Â or `SAME`)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`padding`ï¼šä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå®šä¹‰è¦ä½¿ç”¨çš„å¡«å……ç®—æ³•ï¼ˆ`VALID` æˆ– `SAME`ï¼‰'
- en: '`name`: The name to identify this operation (useful for creating clear, readable
    graphs)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`ï¼šç”¨äºæ ‡è¯†æ­¤æ“ä½œçš„åç§°ï¼ˆå¯¹äºåˆ›å»ºæ¸…æ™°ã€å¯è¯»çš„å›¾å½¢éå¸¸æœ‰ç”¨ï¼‰'
- en: '*Figure 3.7* illustrates an average-pooling operation applied to an image:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*å›¾ 3.7* å±•ç¤ºäº†åº”ç”¨äºå›¾åƒçš„å¹³å‡æ± åŒ–æ“ä½œï¼š'
- en: '![](img/898a506c-2d09-4aa2-bc4d-6f5b4939fd16.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/898a506c-2d09-4aa2-bc4d-6f5b4939fd16.png)'
- en: 'Figure 3.7: Example of average-pooling performed on an image with TensorFlow'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3.7ï¼šä½¿ç”¨ TensorFlow å¯¹å›¾åƒè¿›è¡Œå¹³å‡æ± åŒ–çš„ç¤ºä¾‹
- en: 'In *Figure 3.8*, the max-pooling function is applied to the same image:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ *å›¾ 3.8* ä¸­ï¼Œå¯¹ç›¸åŒçš„å›¾åƒåº”ç”¨äº†æœ€å¤§æ± åŒ–å‡½æ•°ï¼š
- en: '![](img/39987d76-b942-4691-92fb-031dd4c60dc0.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39987d76-b942-4691-92fb-031dd4c60dc0.png)'
- en: 'Figure 3.8: Example of another max-pooling operation, with an excessively large
    window size compared to the stride (purely for demonstration purposes)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.8ï¼šå¦ä¸€ä¸ªæœ€å¤§æ± åŒ–æ“ä½œçš„ç¤ºä¾‹ï¼Œçª—å£å¤§å°ä¸æ­¥å¹…ç›¸æ¯”è¿‡å¤§ï¼ˆä»…ç”¨äºæ¼”ç¤ºç›®çš„ï¼‰
- en: 'Here, again, we can still use the higher-level API to make the instantiation
    slightly more succinct:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥ä½¿ç”¨æ›´é«˜çº§çš„APIï¼Œä½¿å¾—å®ä¾‹åŒ–è¿‡ç¨‹æ›´åŠ ç®€æ´ï¼š
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Since pooling layers do not have trainable weights, there is no real distinction
    between the pooling operation and the corresponding layer in TensorFlow. This
    makes these operations not only lightweight, but easy to instantiate.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ± åŒ–å±‚æ²¡æœ‰å¯è®­ç»ƒçš„æƒé‡ï¼Œå› æ­¤æ± åŒ–æ“ä½œä¸TensorFlowä¸­å¯¹åº”çš„å±‚ä¹‹é—´æ²¡æœ‰å®é™…åŒºåˆ«ã€‚è¿™ä½¿å¾—è¿™äº›æ“ä½œä¸ä»…è½»é‡çº§ï¼Œè€Œä¸”æ˜“äºå®ä¾‹åŒ–ã€‚
- en: Fully connected layers
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…¨è¿æ¥å±‚
- en: It is worth mentioning that FCÂ layers are also used in CNNs, the same way they
    are in regular networks. We will present, in the following paragraphs, when they
    should be considered, and how to include them in CNNs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå…¨è¿æ¥å±‚ä¹Ÿç”¨äºCNNï¼Œå°±åƒåœ¨å¸¸è§„ç½‘ç»œä¸­ä¸€æ ·ã€‚æ¥ä¸‹æ¥çš„æ®µè½ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä½•æ—¶è€ƒè™‘ä½¿ç”¨å®ƒä»¬ï¼Œä»¥åŠå¦‚ä½•å°†å®ƒä»¬åŒ…å«åœ¨CNNä¸­ã€‚
- en: Usage in CNNs
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åœ¨CNNä¸­çš„åº”ç”¨
- en: While FC layers can be added to CNNs processing multidimensional data, this
    implies, however, that the input tensors passed to these layers must first be
    reshaped into a batched column vectorâ€”the way we did with the MNIST images for
    our simple network in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*, and [Chapter 2](c7c49010-458f-47ef-a538-96118f9cd892.xhtml),
    *TensorFlow Basics and Training a Model* (that is,Â *flattening* the height, width,
    and depth dimensions into a single vector).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶å…¨è¿æ¥å±‚å¯ä»¥æ·»åŠ åˆ°å¤„ç†å¤šç»´æ•°æ®çš„CNNä¸­ï¼Œä½†è¿™æ„å‘³ç€ä¼ é€’ç»™è¿™äº›å±‚çš„è¾“å…¥å¼ é‡å¿…é¡»é¦–å…ˆè¢«é‡å¡‘ä¸ºæ‰¹å¤„ç†çš„åˆ—å‘é‡â€”â€”å°±åƒæˆ‘ä»¬åœ¨[ç¬¬1ç« ](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)ã€Šè®¡ç®—æœºè§†è§‰ä¸ç¥ç»ç½‘ç»œã€‹å’Œ[ç¬¬2ç« ](c7c49010-458f-47ef-a538-96118f9cd892.xhtml)ã€ŠTensorFlowåŸºç¡€ä¸æ¨¡å‹è®­ç»ƒã€‹ä¸­æ‰€åšçš„é‚£æ ·ï¼ˆå³å°†é«˜åº¦ã€å®½åº¦å’Œæ·±åº¦ç»´åº¦*å±•å¹³*ä¸ºä¸€ä¸ªå•ä¸€çš„å‘é‡ï¼‰ã€‚
- en: FC layers are also often called **densely connected**, or simply **dense** (as
    opposed to other CNN layers that have more limited connectivity).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å…¨è¿æ¥å±‚ä¹Ÿå¸¸è¢«ç§°ä¸º**å¯†é›†è¿æ¥**å±‚ï¼Œæˆ–ç®€æ´åœ°ç§°ä¸º**å¯†é›†**å±‚ï¼ˆä¸å…¶ä»–è¿æ¥æ€§è¾ƒä¸ºæœ‰é™çš„CNNå±‚ç›¸å¯¹ï¼‰ã€‚
- en: While it can be advantageous in some cases for neurons to have access to the
    complete input map (for instance, to combine spatially distant features), fully
    connected layers have several shortcomings, as mentioned at the beginning of this
    chapter (for example, the loss of spatial information and the large number of
    parameters). Moreover, unlike other CNN layers, dense ones are defined by their
    input and output sizes. A specific dense layer will not work for inputs that have
    a shape different from the one it was configured for. Therefore, using FC layers
    in a neural network usually means losing the possibility to apply them to images
    of heterogeneous sizes.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç¥ç»å…ƒè®¿é—®å®Œæ•´çš„è¾“å…¥å›¾ï¼ˆä¾‹å¦‚ï¼Œç»“åˆç©ºé—´ä¸Šè¿œç¦»çš„ç‰¹å¾ï¼‰å¯èƒ½æ˜¯æœ‰åˆ©çš„ï¼Œä½†å…¨è¿æ¥å±‚æœ‰å‡ ä¸ªç¼ºç‚¹ï¼Œå¦‚æœ¬ç« å¼€å¤´æ‰€è¿°ï¼ˆä¾‹å¦‚ï¼Œç©ºé—´ä¿¡æ¯ä¸¢å¤±å’Œå‚æ•°æ•°é‡åºå¤§ï¼‰ã€‚æ­¤å¤–ï¼Œä¸å…¶ä»–CNNå±‚ä¸åŒï¼Œå…¨è¿æ¥å±‚æ˜¯ç”±å…¶è¾“å…¥å’Œè¾“å‡ºçš„å¤§å°æ¥å®šä¹‰çš„ã€‚ç‰¹å®šçš„å…¨è¿æ¥å±‚ä¸èƒ½å¤„ç†å½¢çŠ¶ä¸å…¶é…ç½®æ—¶ä¸åŒçš„è¾“å…¥ã€‚å› æ­¤ï¼Œåœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨å…¨è¿æ¥å±‚é€šå¸¸æ„å‘³ç€å¤±å»å°†å…¶åº”ç”¨äºä¸åŒå°ºå¯¸å›¾åƒçš„å¯èƒ½æ€§ã€‚
- en: Despite these shortcomings, these layers are still commonly used in CNNs. They
    are usually found among the final layers of a network, for instance, to convert
    the multidimensional features into a 1D classification vector.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å­˜åœ¨è¿™äº›ç¼ºç‚¹ï¼Œè¿™äº›å±‚ä»ç„¶åœ¨CNNä¸­å¹¿æ³›ä½¿ç”¨ã€‚å®ƒä»¬é€šå¸¸ä½äºç½‘ç»œçš„æœ€åå‡ å±‚ï¼Œç”¨äºå°†å¤šç»´ç‰¹å¾è½¬æ¢ä¸º1Dåˆ†ç±»å‘é‡ã€‚
- en: TensorFlow/Keras methods
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow/Keras æ–¹æ³•
- en: 'Although we already used TensorFlow''s dense layers in the previous chapter,
    we did not stop to focus on their parameters and properties. Once again, the signature
    ofÂ `tf.keras.layers.Dense()` (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense))
    is comparable to that of previously introduced layers, with the difference that
    they do not accept any `strides` or `padding` for parameters, but instead useÂ `units`
    representing the number of neurons/output size, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬åœ¨ä¸Šä¸€ç« ä¸­å·²ç»ä½¿ç”¨äº†TensorFlowçš„å…¨è¿æ¥å±‚ï¼Œä½†æˆ‘ä»¬æ²¡æœ‰åœä¸‹æ¥å…³æ³¨å®ƒä»¬çš„å‚æ•°å’Œå±æ€§ã€‚å†æ¬¡æé†’ï¼Œ`tf.keras.layers.Dense()`çš„ç­¾åï¼ˆè¯·å‚è€ƒ[https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)çš„æ–‡æ¡£ï¼‰ä¸ä¹‹å‰ä»‹ç»çš„å±‚ç±»ä¼¼ï¼Œä¸åŒä¹‹å¤„åœ¨äºå®ƒä»¬ä¸æ¥å—ä»»ä½•`strides`æˆ–`padding`å‚æ•°ï¼Œè€Œæ˜¯ä½¿ç”¨`units`æ¥è¡¨ç¤ºç¥ç»å…ƒ/è¾“å‡ºå¤§å°ï¼Œå…·ä½“å¦‚ä¸‹ï¼š
- en: '[PRE4]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Remember that you should, however, take care of *flattening* the multidimensional
    tensors before passing them to dense layers.Â `tf.keras.layers.Flatten()`Â (refer
    to the documentation at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten))
    can be used as an intermediate layer for that purpose.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¯·è®°ä½ï¼Œåœ¨å°†æ•°æ®ä¼ é€’ç»™å¯†é›†å±‚ä¹‹å‰ï¼Œä½ åº”å½“å°å¿ƒ*å±•å¹³*å¤šç»´å¼ é‡ã€‚`tf.keras.layers.Flatten()`ï¼ˆå‚è€ƒæ–‡æ¡£ [https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)ï¼‰å¯ä»¥ä½œä¸ºä¸€ä¸ªä¸­é—´å±‚æ¥å®ç°è¿™ä¸€ç›®çš„ã€‚
- en: Effective receptive field
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æœ‰æ•ˆæ„Ÿå—é‡
- en: As we will detail in this section, the **effective receptive field**Â (**ERF**)
    of a neural network is an important notion in deep learning, as it may affect
    the ability of the network to cross-reference and combine distant elements in
    the input images.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨æœ¬èŠ‚ä¸­å°†è¯¦ç»†è¯´æ˜çš„é‚£æ ·ï¼Œç¥ç»ç½‘ç»œçš„**æœ‰æ•ˆæ„Ÿå—é‡** (**ERF**) æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µï¼Œå› ä¸ºå®ƒå¯èƒ½å½±å“ç½‘ç»œè·¨å¼•ç”¨å¹¶ç»“åˆè¾“å…¥å›¾åƒä¸­è¿œè·ç¦»å…ƒç´ çš„èƒ½åŠ›ã€‚
- en: Definitions
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰
- en: 'While the receptive field represents the local region of the previous layer
    that a neuron is connected to, the ERFÂ defines *the region of the input image*
    (and not just of the previous layer), which affects the activation of a neuron
    for a given layer, as shown in *Figure 3.9*:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶æ„Ÿå—é‡è¡¨ç¤ºç¥ç»å…ƒä¸å‰ä¸€å±‚è¿æ¥çš„å±€éƒ¨åŒºåŸŸï¼Œä½† ERF å®šä¹‰äº†*è¾“å…¥å›¾åƒçš„åŒºåŸŸ*ï¼ˆè€Œä¸ä»…ä»…æ˜¯å‰ä¸€å±‚çš„åŒºåŸŸï¼‰ï¼Œè¯¥åŒºåŸŸå½±å“ç»™å®šå±‚ç¥ç»å…ƒçš„æ¿€æ´»ï¼Œå¦‚*å›¾ 3.9*æ‰€ç¤ºï¼š
- en: '![](img/0b54dcd5-94a2-42de-9d5a-6631a4214ad3.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b54dcd5-94a2-42de-9d5a-6631a4214ad3.png)'
- en: 'Figure 3.9: Illustration of the receptive field of a layer with a simple network
    of two convolutional layers'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3.9ï¼šå…·æœ‰ä¸¤ä¸ªå·ç§¯å±‚ç®€å•ç½‘ç»œçš„å±‚çš„æ„Ÿå—é‡ç¤ºæ„å›¾
- en: Note that it is common to find the term **receptive fieldÂ **(**RF**) used in
    place of ERF, because RF can simply be referred to as the filter size or the window
    size of a layer. Some people also use RF or ERF to specifically define the input
    regions affecting each unit of the output layer (and not just any intermediary
    layer of a network).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œé€šå¸¸ä¼šå°†**æ„Ÿå—é‡** (**RF**) ç”¨ä½œ ERF çš„æ›¿ä»£æœ¯è¯­ï¼Œå› ä¸º RF å¯ä»¥ç®€å•åœ°æŒ‡ä»£å±‚çš„è¿‡æ»¤å™¨å¤§å°æˆ–çª—å£å¤§å°ã€‚ä¸€äº›äººè¿˜ä½¿ç”¨ RF æˆ–
    ERF æ¥ç‰¹æŒ‡å½±å“è¾“å‡ºå±‚æ¯ä¸ªå•å…ƒçš„è¾“å…¥åŒºåŸŸï¼ˆè€Œä¸ä»…ä»…æ˜¯ç½‘ç»œçš„ä»»ä½•ä¸­é—´å±‚ï¼‰ã€‚
- en: Adding to the confusion, some researchers started calling ERFthe subset of the
    input region that is actually affecting a neuron. This was introduced by Wenjie
    Luo et al. in their paper,Â *Understanding the Effective Receptive Field in Deep
    Convolutional Neural Networks,Â *published in *Advances in Neural Information Processing
    Systems (2016)*. Their idea was that not all pixels *seenÂ *by a neuron contribute
    *equally* to its response. We can intuitively accept that, for instance, pixels
    at the center of the RF will have more weight than peripheral ones. The information
    held by these central pixels can be propagated along multiple paths in the intermediary
    layers of the network to reach a given neuron, while pixels in the periphery of
    the receptive field are connected to this neuron through a single path. Therefore,
    the ERF, as defined by Luo et al., follows a pseudo-Gaussian distribution, unlike
    the uniform distribution of a traditional ERF.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´åŠ ä»¤äººå›°æƒ‘çš„æ˜¯ï¼Œä¸€äº›ç ”ç©¶äººå‘˜å¼€å§‹å°† ERF ç§°ä¸ºå®é™…å½±å“ç¥ç»å…ƒçš„è¾“å…¥åŒºåŸŸå­é›†ã€‚è¿™ä¸€è§‚ç‚¹ç”± Wenjie Luo ç­‰äººåœ¨ä»–ä»¬çš„è®ºæ–‡ã€ŠUnderstanding
    the Effective Receptive Field in Deep Convolutional Neural Networksã€‹ä¸­æå‡ºï¼Œè¯¥è®ºæ–‡å‘è¡¨åœ¨ã€ŠAdvances
    in Neural Information Processing Systems (2016)ã€‹ä¸Šã€‚ä»–ä»¬çš„è§‚ç‚¹æ˜¯ï¼Œå¹¶éæ‰€æœ‰è¢«ç¥ç»å…ƒâ€œçœ‹åˆ°â€çš„åƒç´ éƒ½å¯¹å…¶å“åº”æœ‰*ç›¸ç­‰*çš„è´¡çŒ®ã€‚æˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°æ¥å—ï¼Œä¾‹å¦‚ï¼Œæ„Ÿå—é‡ä¸­å¿ƒçš„åƒç´ å¯¹ç¥ç»å…ƒçš„å“åº”æƒé‡ä¼šå¤§äºå¤–å›´åƒç´ ã€‚è¿™äº›ä¸­å¿ƒåƒç´ æºå¸¦çš„ä¿¡æ¯å¯ä»¥é€šè¿‡ç½‘ç»œçš„ä¸­é—´å±‚æ²¿å¤šæ¡è·¯å¾„ä¼ æ’­åˆ°è¾¾æŸä¸ªç¥ç»å…ƒï¼Œè€Œæ„Ÿå—é‡å¤–å›´çš„åƒç´ åˆ™é€šè¿‡å•ä¸€è·¯å¾„è¿æ¥åˆ°è¯¥ç¥ç»å…ƒã€‚å› æ­¤ï¼ŒLuo
    ç­‰äººå®šä¹‰çš„ ERF éµå¾ªä¼ªé«˜æ–¯åˆ†å¸ƒï¼Œè€Œä¼ ç»Ÿ ERF æ˜¯å‡åŒ€åˆ†å¸ƒçš„ã€‚
- en: The authors make an interesting parallel between this representation of the
    receptive field and the human **central fovea**, the region of the eye responsible
    for our sharp central vision. This detailed part of the vision is at the basis
    of many human activities. Half the optical nerves are linked to the fovea (despite
    its relatively small size), in the same way that central pixels in effective receptive
    fields are connected to a higher number of artificial neurons.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œè€…å°†è¿™ç§æ„Ÿå—é‡çš„è¡¨ç¤ºä¸äººç±»çš„**ä¸­å¤®é»„æ–‘**åšäº†æœ‰è¶£çš„ç±»æ¯”ï¼Œä¸­å¤®é»„æ–‘æ˜¯çœ¼ç›ä¸­è´Ÿè´£æ¸…æ™°ä¸­å¤®è§†è§‰çš„åŒºåŸŸã€‚è§†åŠ›çš„è¿™ä¸€ç»†èŠ‚éƒ¨åˆ†æ˜¯è®¸å¤šäººç±»æ´»åŠ¨çš„åŸºç¡€ã€‚å°½ç®¡å…¶ç›¸å¯¹è¾ƒå°ï¼Œä½†ä¸€åŠçš„è§†ç¥ç»ä¸é»„æ–‘ç›¸è¿ï¼Œå°±åƒæœ‰æ•ˆæ„Ÿå—é‡ä¸­çš„ä¸­å¤®åƒç´ è¿æ¥åˆ°æ›´å¤šçš„äººå·¥ç¥ç»å…ƒä¸€æ ·ã€‚
- en: Formula
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å…¬å¼
- en: 'No matter what actual role its pixels are playing, the effective receptive
    field (named *R[i]*Â here) of the *i*^(th) layer of a CNN can be recursively computed
    as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºå…¶åƒç´ å®é™…æ‰®æ¼”ä»€ä¹ˆè§’è‰²ï¼Œå·ç§¯ç¥ç»ç½‘ç»œç¬¬*i*å±‚çš„æœ‰æ•ˆæ„Ÿå—é‡ï¼ˆæ­¤å¤„ç§°ä¸º*R[i]*ï¼‰å¯ä»¥é€šè¿‡é€’å½’æ–¹å¼è®¡ç®—å¦‚ä¸‹ï¼š
- en: '![](img/f651c9be-9751-49c1-875c-8f4e891fab2f.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f651c9be-9751-49c1-875c-8f4e891fab2f.png)'
- en: 'In this equation, *k[i]* is the filter size of the layer, and *s[i]*Â is its
    stride (the last part of the equation thus represents the product of the strides
    for all the previous layers). As an example, we can apply this formula to the
    minimalist two-layer CNN presented in *Figure 3.9*Â to quantitatively evaluate
    the ERF of the second layer as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ–¹ç¨‹ä¸­ï¼Œ*k[i]* æ˜¯è¯¥å±‚çš„æ»¤æ³¢å™¨å¤§å°ï¼Œ*s[i]* æ˜¯å®ƒçš„æ­¥é•¿ï¼ˆå› æ­¤ï¼Œæ–¹ç¨‹çš„æœ€åä¸€éƒ¨åˆ†è¡¨ç¤ºæ‰€æœ‰å‰é¢å±‚çš„æ­¥é•¿çš„ä¹˜ç§¯ï¼‰ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ­¤å…¬å¼åº”ç”¨äº*å›¾
    3.9* ä¸­å±•ç¤ºçš„æç®€äºŒå±‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œä»¥å®šé‡è¯„ä¼°ç¬¬äºŒå±‚çš„æœ‰æ•ˆæ„Ÿå—é‡ï¼ˆERFï¼‰ï¼Œè®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š
- en: '![](img/5395488d-6ad1-4058-9589-62aac19a931c.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5395488d-6ad1-4058-9589-62aac19a931c.png)'
- en: This formula confirms that the ERF of a network is directly affected by the
    number of intermediary layers, their filter sizes, and the strides. Subsampling
    layers, such as pooling layers or layers with larger strides, greatly increase
    the ERF at the cost of lower feature resolution.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥å…¬å¼ç¡®è®¤äº†ç½‘ç»œçš„æœ‰æ•ˆæ„Ÿå—é‡ï¼ˆERFï¼‰ç›´æ¥å—åˆ°ä¸­é—´å±‚æ•°é‡ã€æ»¤æ³¢å™¨å¤§å°å’Œæ­¥é•¿çš„å½±å“ã€‚å­é‡‡æ ·å±‚ï¼ˆä¾‹å¦‚æ± åŒ–å±‚æˆ–å…·æœ‰è¾ƒå¤§æ­¥é•¿çš„å±‚ï¼‰ä¼šå¤§å¤§å¢åŠ æœ‰æ•ˆæ„Ÿå—é‡ï¼Œä½†ä¼šä»¥ç‰ºç‰²ç‰¹å¾åˆ†è¾¨ç‡ä¸ºä»£ä»·ã€‚
- en: Because of the local connectivity of CNNs, you should keep in mind how layers
    and their hyperparameters will affect the flow of visual information across the
    networks when defining their architecture.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å±€éƒ¨è¿æ¥æ€§ï¼Œåœ¨å®šä¹‰ç½‘ç»œæ¶æ„æ—¶ï¼Œä½ åº”å½“ç‰¢è®°å±‚ä¸å…¶è¶…å‚æ•°å¦‚ä½•å½±å“è§†è§‰ä¿¡æ¯åœ¨ç½‘ç»œä¸­çš„æµåŠ¨ã€‚
- en: CNNs with TensorFlow
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨TensorFlowå®ç°å·ç§¯ç¥ç»ç½‘ç»œ
- en: Most state-of-the-art computer vision algorithms are based on CNNs built with
    the three different types of layers we just introduced (that is, convolutional,
    pooling, and FC), with some tweaks and tricks that we will present in this book.
    In this section, we will build our first CNN and apply it to our digit recognition
    task.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°æœ€å…ˆè¿›çš„è®¡ç®—æœºè§†è§‰ç®—æ³•éƒ½åŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Œè¿™äº›ç½‘ç»œä½¿ç”¨æˆ‘ä»¬åˆšåˆšä»‹ç»çš„ä¸‰ç§ä¸åŒç±»å‹çš„å±‚ï¼ˆå³å·ç§¯å±‚ã€æ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚ï¼‰ï¼Œå¹¶è¿›è¡Œä¸€äº›è°ƒæ•´å’ŒæŠ€å·§ï¼Œè¿™äº›å†…å®¹æˆ‘ä»¬å°†åœ¨æœ¬ä¹¦ä¸­ä»‹ç»ã€‚åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å®ç°ç¬¬ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼Œå¹¶å°†å…¶åº”ç”¨äºæ•°å­—è¯†åˆ«ä»»åŠ¡ã€‚
- en: Implementing our first CNN
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®ç°æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
- en: 'For our first convolutional neural network, we will implement *LeNet-5*. First
    introduced by Yann Le Cun in 1995 (in *Learning algorithms for classification:
    A comparison on handwritten digit recognition*,Â *World Scientific Singapore*)
    and applied to the MNIST dataset, LeNet-5 may not be a recent network, but it
    is still commonly used to introduce people to CNNs. Indeed, with its seven layers,
    this network is straightforward to implement, while yielding interesting results.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¯¹äºæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬å°†å®ç°*LeNet-5*ã€‚è¯¥ç½‘ç»œæœ€æ—©ç”±Yann Le Cunäº1995å¹´æå‡ºï¼ˆåœ¨*Learning algorithms
    for classification: A comparison on handwritten digit recognition*, *World Scientific
    Singapore*ä¸€ä¹¦ä¸­ï¼‰ï¼Œå¹¶åº”ç”¨äºMNISTæ•°æ®é›†ã€‚LeNet-5å¯èƒ½ä¸æ˜¯ä¸€ä¸ªæ–°çš„ç½‘ç»œï¼Œä½†å®ƒä»ç„¶æ˜¯ä»‹ç»å·ç§¯ç¥ç»ç½‘ç»œçš„å¸¸ç”¨æ¨¡å‹ã€‚å®é™…ä¸Šï¼Œå‡­å€Ÿå…¶ä¸ƒä¸ªå±‚æ¬¡ï¼Œè¿™ä¸ªç½‘ç»œéå¸¸å®¹æ˜“å®ç°ï¼ŒåŒæ—¶ä¹Ÿèƒ½å¾—åˆ°æœ‰è¶£çš„ç»“æœã€‚'
- en: LeNet-5 architecture
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LeNet-5æ¶æ„
- en: 'As shown in *Figure 3.10*, LeNet-5 is first composed of two blocks, each containing
    a convolutional layer (with the kernel size *k* = 5 and stride *s* = 1) followed
    by a max-pooling layer (withÂ *k* = 2 andÂ *s* = 2). In the first block, the input
    images are zero-padded by 2 on each side before convolution (that is,Â *p* = 2,
    hence an actual input size of *32 Ã— 32*), and the convolution layer has six different
    filters (*N* = 6). There is no padding before the second convolution (*p* = 0),
    and its number of filters is set to 16 (*N* = 16). After the two blocks, three
    fully connected layers merge the features together and lead to the final class
    estimation (the 10 digit classes). Before the first dense layer, the *5 Ã— 5 Ã—
    16* feature volume is flattened into a vector of 400 values. The complete architecture
    is represented in the following diagram:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚*å›¾ 3.10*æ‰€ç¤ºï¼ŒLeNet-5é¦–å…ˆç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼Œæ¯ä¸ªæ¨¡å—åŒ…å«ä¸€ä¸ªå·ç§¯å±‚ï¼ˆå·ç§¯æ ¸å¤§å°*k* = 5ï¼Œæ­¥é•¿*s* = 1ï¼‰ï¼Œåæ¥ä¸€ä¸ªæœ€å¤§æ± åŒ–å±‚ï¼ˆæ± åŒ–æ ¸*k*
    = 2ï¼Œæ­¥é•¿*s* = 2ï¼‰ã€‚åœ¨ç¬¬ä¸€ä¸ªæ¨¡å—ä¸­ï¼Œè¾“å…¥å›¾åƒåœ¨è¿›è¡Œå·ç§¯å‰åœ¨æ¯ä¸€è¾¹å¡«å……2ä¸ªåƒç´ ï¼ˆå³ï¼Œ*p* = 2ï¼Œå› æ­¤å®é™…è¾“å…¥å¤§å°ä¸º*32 Ã— 32*ï¼‰ï¼Œå·ç§¯å±‚æœ‰å…­ä¸ªä¸åŒçš„æ»¤æ³¢å™¨ï¼ˆ*N*
    = 6ï¼‰ã€‚ç¬¬äºŒä¸ªå·ç§¯å±‚å‰æ²¡æœ‰å¡«å……ï¼ˆ*p* = 0ï¼‰ï¼Œå®ƒçš„æ»¤æ³¢å™¨æ•°é‡è®¾ç½®ä¸º16ï¼ˆ*N* = 16ï¼‰ã€‚ç»è¿‡è¿™ä¸¤ä¸ªæ¨¡å—åï¼Œä¸‰ä¸ªå…¨è¿æ¥å±‚å°†ç‰¹å¾åˆå¹¶å¹¶æœ€ç»ˆè¾“å‡ºåˆ†ç±»ç»“æœï¼ˆ10ä¸ªæ•°å­—ç±»åˆ«ï¼‰ã€‚åœ¨ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚ä¹‹å‰ï¼Œ*5
    Ã— 5 Ã— 16* çš„ç‰¹å¾ä½“ç§¯è¢«å±•å¹³æˆä¸€ä¸ª400ä¸ªå€¼çš„å‘é‡ã€‚å®Œæ•´çš„ç½‘ç»œæ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
- en: '![](img/92f819f4-53b5-4b32-90e9-512d872806a1.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/92f819f4-53b5-4b32-90e9-512d872806a1.png)'
- en: 'Figure 3.10: LeNet-5 architecture (rendered with theÂ NN-SVG tool by Alexander
    Lenailâ€”http://alexlenail.me/NN-SVG)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3.10ï¼šLeNet-5 æ¶æ„ï¼ˆä½¿ç”¨ NN-SVG å·¥å…·æ¸²æŸ“ï¼Œç”± Alexander Lenail æä¾›â€”http://alexlenail.me/NN-SVGï¼‰
- en: In the original implementation, each convolution layer and dense layer except
    the last one uses *tanh* as an activation function. However, *ReLU* is nowadays
    preferred to *tanh*, replacing it in most LeNet-5 implementations. For the last
    layer, the *softmax* function is applied. This function takes a vector of *N*
    values and returns a same-size vector,Â *y,* with its values normalized into a
    probability distribution. In other words, *softmax* normalizes a vector so that
    its values are all between 0 and 1, and their sum is exactly equal to 1\. Therefore,
    this function is commonly used at the end of neural networks applied to classification
    tasks in order to convert the network's predictions into per-class probability,
    as mentioned in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml), *Computer
    Vision and Neural Networks* (that is, given an output tensor,Â *y* = [*y[0], ...,
    y[i], ..., y[N]*], *y[i]*Â represents how likely it is that the sample belongs
    to classÂ *i* according to the network).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸå§‹å®ç°ä¸­ï¼Œé™¤äº†æœ€åä¸€å±‚å¤–ï¼Œæ¯ä¸ªå·ç§¯å±‚å’Œå…¨è¿æ¥å±‚éƒ½ä½¿ç”¨ *tanh* ä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚ç„¶è€Œï¼Œ*ReLU* ç°åœ¨æ¯” *tanh* æ›´ä¸ºå¸¸ç”¨ï¼Œå·²åœ¨å¤§å¤šæ•° LeNet-5
    å®ç°ä¸­å–ä»£äº† *tanh*ã€‚å¯¹äºæœ€åä¸€å±‚ï¼Œåº”ç”¨ *softmax* å‡½æ•°ã€‚è¯¥å‡½æ•°æ¥å—ä¸€ä¸ª *N* å€¼çš„å‘é‡ï¼Œå¹¶è¿”å›ä¸€ä¸ªç›¸åŒå¤§å°çš„å‘é‡ï¼Œ*y*ï¼Œå…¶å€¼è¢«å½’ä¸€åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚æ¢å¥è¯è¯´ï¼Œ*softmax*
    ä¼šå½’ä¸€åŒ–ä¸€ä¸ªå‘é‡ï¼Œä½¿å…¶æ‰€æœ‰å€¼éƒ½åœ¨ 0 å’Œ 1 ä¹‹é—´ï¼Œå¹¶ä¸”å®ƒä»¬çš„å’Œæ°å¥½ç­‰äº 1ã€‚å› æ­¤ï¼Œè¿™ä¸ªå‡½æ•°é€šå¸¸åº”ç”¨äºåˆ†ç±»ä»»åŠ¡çš„ç¥ç»ç½‘ç»œæœ«å°¾ï¼Œå°†ç½‘ç»œçš„é¢„æµ‹å€¼è½¬æ¢ä¸ºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡å€¼ï¼Œå¦‚
    [ç¬¬ 1 ç« ](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)ï¼Œ*è®¡ç®—æœºè§†è§‰ä¸ç¥ç»ç½‘ç»œ* ä¸­æ‰€è¿°ï¼ˆå³ï¼Œç»™å®šè¾“å‡ºå¼ é‡ï¼Œ*y*
    = [*y[0], ..., y[i], ..., y[N]*]ï¼Œ*y[i]* è¡¨ç¤ºæ ·æœ¬å±äºç±»åˆ« *i* çš„å¯èƒ½æ€§ï¼‰ã€‚
- en: The network's raw predictions (that is, before normalization) are commonly named
    **logits**. These unbounded values are usually converted into probabilities with
    the *softmax* function. This normalization process makes the prediction more *readable*
    (each value represents the confidence of the network for the corresponding class;
    refer to the belief scores mentioned in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*) and simplifies the computation of the training
    loss (that is, the categorical cross-entropy for classification tasks).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œçš„åŸå§‹é¢„æµ‹ï¼ˆå³ï¼Œæœªç»è¿‡å½’ä¸€åŒ–çš„é¢„æµ‹å€¼ï¼‰é€šå¸¸è¢«ç§°ä¸º **logits**ã€‚è¿™äº›æ— ç•Œçš„å€¼é€šå¸¸é€šè¿‡ *softmax* å‡½æ•°è½¬æ¢ä¸ºæ¦‚ç‡å€¼ã€‚è¿™ä¸ªå½’ä¸€åŒ–è¿‡ç¨‹ä½¿å¾—é¢„æµ‹æ›´åŠ 
    *æ˜“è¯»*ï¼ˆæ¯ä¸ªå€¼ä»£è¡¨ç½‘ç»œå¯¹å¯¹åº”ç±»åˆ«çš„ç½®ä¿¡åº¦ï¼›å‚è§ [ç¬¬ 1 ç« ](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)ï¼Œ*è®¡ç®—æœºè§†è§‰ä¸ç¥ç»ç½‘ç»œ*ï¼‰ï¼Œå¹¶ç®€åŒ–äº†è®­ç»ƒæŸå¤±çš„è®¡ç®—ï¼ˆå³ï¼Œåˆ†ç±»ä»»åŠ¡ä¸­çš„ç±»åˆ«äº¤å‰ç†µï¼‰ã€‚
- en: TensorFlow and Keras implementations
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow å’Œ Keras å®ç°
- en: 'We have all the tools in hand to implement this network. We suggest that you
    try them yourself, before checking the TensorFlow and Keras implementations provided.
    Reusing the notations and variables from [Chapter 2](c7c49010-458f-47ef-a538-96118f9cd892.xhtml),
    *TensorFlow Basics and Training a Model*, a LeNet-5 network using the Keras Sequential
    API would be as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰‹å¤´æœ‰äº†å®ç°æ­¤ç½‘ç»œçš„æ‰€æœ‰å·¥å…·ã€‚åœ¨æŸ¥çœ‹ TensorFlow å’Œ Keras æä¾›çš„å®ç°ä¹‹å‰ï¼Œå»ºè®®ä½ è‡ªå·±å°è¯•å®ç°ã€‚ä½¿ç”¨ [ç¬¬ 2 ç« ](c7c49010-458f-47ef-a538-96118f9cd892.xhtml)ï¼Œ*TensorFlow
    åŸºç¡€ä¸æ¨¡å‹è®­ç»ƒ* ä¸­çš„ç¬¦å·å’Œå˜é‡ï¼Œä½¿ç”¨ Keras Sequential API å®ç°çš„ LeNet-5 ç½‘ç»œå¦‚ä¸‹ï¼š
- en: '[PRE5]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The model is created by instantiating and adding the layers one by one, *sequentially*.
    As mentioned in [Chapter 2](c7c49010-458f-47ef-a538-96118f9cd892.xhtml), *TensorFlow
    Basics and Training a Model*, Keras also provides theÂ **functional API**. This
    API makes it possible to define models in a more object-oriented approach (as
    shown in the following code), though it is also possible to directly instantiateÂ `tf.keras.Model`
    with the layer operations (as illustrated in some of our Jupyter notebooks):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¨¡å‹é€šè¿‡é€ä¸€å®ä¾‹åŒ–å¹¶æ·»åŠ å±‚ï¼Œ*æŒ‰é¡ºåº*åˆ›å»ºã€‚å¦‚åŒåœ¨ [ç¬¬ 2 ç« ](c7c49010-458f-47ef-a538-96118f9cd892.xhtml)ï¼Œ*TensorFlow
    åŸºç¡€ä¸æ¨¡å‹è®­ç»ƒ* ä¸­æåˆ°çš„ï¼ŒKeras è¿˜æä¾›äº† **åŠŸèƒ½æ€§ API**ã€‚è¯¥ API ä½¿å¾—ç”¨æ›´é¢å‘å¯¹è±¡çš„æ–¹å¼å®šä¹‰æ¨¡å‹æˆä¸ºå¯èƒ½ï¼ˆå¦‚ä»¥ä¸‹ä»£ç æ‰€ç¤ºï¼‰ï¼Œå°½ç®¡ä¹Ÿå¯ä»¥ç›´æ¥é€šè¿‡å±‚æ“ä½œå®ä¾‹åŒ–
    `tf.keras.Model`ï¼ˆå¦‚åœ¨æˆ‘ä»¬çš„æŸäº› Jupyter ç¬”è®°æœ¬ä¸­æ‰€ç¤ºï¼‰ï¼š
- en: '[PRE6]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Keras layers can indeed behave like functions that can be applied to input data
    and chained until the desired output is obtained. The functional API allows you
    to build more complex neural networks; for example, when one specific layer is
    reused several times inside the networks, or when layers have multiple inputs
    or outputs.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Keras å±‚ç¡®å®å¯ä»¥åƒå‡½æ•°ä¸€æ ·ï¼Œå¯¹è¾“å…¥æ•°æ®è¿›è¡Œå¤„ç†å¹¶è¿›è¡Œé“¾å¼æ“ä½œï¼Œç›´åˆ°è·å¾—æ‰€éœ€çš„è¾“å‡ºã€‚åŠŸèƒ½æ€§ API å…è®¸ä½ æ„å»ºæ›´å¤æ‚çš„ç¥ç»ç½‘ç»œï¼›ä¾‹å¦‚ï¼Œå½“æŸä¸€ç‰¹å®šå±‚åœ¨ç½‘ç»œä¸­è¢«å¤šæ¬¡å¤ç”¨ï¼Œæˆ–è€…å½“å±‚å…·æœ‰å¤šä¸ªè¾“å…¥æˆ–è¾“å‡ºæ—¶ã€‚
- en: For those who have already experimented with PyTorch ([https://pytorch.org](https://pytorch.org)),
    another machine learning framework, this object-oriented approach to building
    neural networks may seem familiar, as it is favored there.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºå·²ç»å°è¯•è¿‡PyTorchï¼ˆ[https://pytorch.org](https://pytorch.org)ï¼‰çš„ç”¨æˆ·æ¥è¯´ï¼Œè¿™ç§é¢å‘å¯¹è±¡çš„ç¥ç»ç½‘ç»œæ„å»ºæ–¹æ³•å¯èƒ½å¾ˆç†Ÿæ‚‰ï¼Œå› ä¸ºå®ƒåœ¨PyTorchä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚
- en: Application to MNIST
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åº”ç”¨åˆ°MNIST
- en: 'We can now compile and train our model for digit classification. Pursuing this
    with the Keras API (and reusing the MNIST data variables prepared in the last
    chapter), we instantiate the optimizer (a simpleÂ **stochastic gradient descent**
    (**SGD**) optimizer) and define the loss (the categorical cross-entropy) before
    launching the training, as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥ç¼–è¯‘å¹¶è®­ç»ƒæˆ‘ä»¬çš„æ•°å­—åˆ†ç±»æ¨¡å‹ã€‚é€šè¿‡Keras APIï¼ˆå¹¶é‡ç”¨ä¸Šä¸€ç« ä¸­å‡†å¤‡çš„MNISTæ•°æ®å˜é‡ï¼‰ï¼Œæˆ‘ä»¬å®ä¾‹åŒ–ä¼˜åŒ–å™¨ï¼ˆä¸€ä¸ªç®€å•çš„**éšæœºæ¢¯åº¦ä¸‹é™**ï¼ˆ**SGD**ï¼‰ä¼˜åŒ–å™¨ï¼‰ï¼Œå¹¶åœ¨å¯åŠ¨è®­ç»ƒä¹‹å‰å®šä¹‰æŸå¤±å‡½æ•°ï¼ˆç±»åˆ«äº¤å‰ç†µï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE7]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note the use of `sparse_categorical_crossentropy`, instead of `categorical_crossentropy`,
    to avoid one-hot encoding the labels. This loss was described in [Chapter 2](c7c49010-458f-47ef-a538-96118f9cd892.xhtml),
    *TensorFlow Basics and Training a Model*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ä½¿ç”¨ `sparse_categorical_crossentropy`ï¼Œè€Œé `categorical_crossentropy`ï¼Œä»¥é¿å…å¯¹æ ‡ç­¾è¿›è¡Œç‹¬çƒ­ç¼–ç ã€‚è¿™ä¸ªæŸå¤±å‡½æ•°åœ¨[ç¬¬äºŒç« ](c7c49010-458f-47ef-a538-96118f9cd892.xhtml)ä¸­æœ‰æè¿°ï¼Œ*TensorFlowåŸºç¡€ä¸æ¨¡å‹è®­ç»ƒ*ã€‚
- en: After ~60 epochs, we observe that our network's accuracy on the validation data
    reaches above ~98.5%! Compared to our previous attempts with non-convolutional
    networks, the relative error has been divided by *2* (from a ~3.0% to ~1.5% relative
    error), which is a significant improvement (given the high accuracy already).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ç»è¿‡å¤§çº¦60ä¸ªepochåï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç½‘ç»œåœ¨éªŒè¯æ•°æ®ä¸Šçš„å‡†ç¡®ç‡è¾¾åˆ°äº†è¶…è¿‡98.5%ï¼ä¸æˆ‘ä»¬ä¹‹å‰ä½¿ç”¨éå·ç§¯ç½‘ç»œçš„å°è¯•ç›¸æ¯”ï¼Œç›¸å¯¹è¯¯å·®é™ä½äº†*2*å€ï¼ˆä»çº¦3.0%çš„è¯¯å·®é™è‡³çº¦1.5%ï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªæ˜¾è‘—çš„æ”¹è¿›ï¼ˆè€ƒè™‘åˆ°å·²ç»éå¸¸é«˜çš„å‡†ç¡®ç‡ï¼‰ã€‚
- en: In the following chapters, we will fully appreciate the analytical power of
    CNNs, applying them to increasingly complex visual tasks.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å……åˆ†ç†è§£å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„åˆ†æèƒ½åŠ›ï¼Œå¹¶å°†å…¶åº”ç”¨äºè¶Šæ¥è¶Šå¤æ‚çš„è§†è§‰ä»»åŠ¡ã€‚
- en: Refining the training process
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç²¾ç‚¼è®­ç»ƒè¿‡ç¨‹
- en: Network architectures are not the onlyÂ things to have improved over the years.
    The way that networks are trained has also evolved, improving how reliably and
    quickly they can converge. In this section, we will tackle some of the shortcomings
    of the gradient descent algorithm we covered in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*, as well as some ways to avoid overfitting.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ç½‘ç»œæ¶æ„ä¸ä»…åœ¨è¿™äº›å¹´ä¸­å¾—åˆ°äº†æ”¹è¿›ï¼Œç½‘ç»œçš„è®­ç»ƒæ–¹å¼ä¹Ÿåœ¨ä¸æ–­å‘å±•ï¼Œæå‡äº†ç½‘ç»œæ”¶æ•›çš„å¯é æ€§ä¸é€Ÿåº¦ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºåœ¨[ç¬¬ä¸€ç« ](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)ä¸­ä»‹ç»çš„æ¢¯åº¦ä¸‹é™ç®—æ³•çš„ä¸€äº›ä¸è¶³ï¼Œå¹¶æ¢è®¨é¿å…è¿‡æ‹Ÿåˆçš„ä¸€äº›æ–¹æ³•ã€‚
- en: Modern network optimizers
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç°ä»£ç½‘ç»œä¼˜åŒ–å™¨
- en: Optimizing multidimensional functions, such as neural networks, is a complex
    task. The gradient descent solution we presented in the first chapter is an elegant
    solution, though it has some limitations that we will highlight in the following
    section. Thankfully, researchers have been developing new generations of optimization
    algorithms, which we will also discuss.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼˜åŒ–å¤šç»´å‡½æ•°ï¼ˆå¦‚ç¥ç»ç½‘ç»œï¼‰æ˜¯ä¸€é¡¹å¤æ‚çš„ä»»åŠ¡ã€‚æˆ‘ä»¬åœ¨ç¬¬ä¸€ç« ä¸­ä»‹ç»çš„æ¢¯åº¦ä¸‹é™è§£æ³•æ˜¯ä¸€ä¸ªä¼˜é›…çš„è§£å†³æ–¹æ¡ˆï¼Œä½†å®ƒæœ‰ä¸€äº›å±€é™æ€§ï¼Œæ¥ä¸‹æ¥çš„éƒ¨åˆ†å°†é‡ç‚¹è¯´æ˜è¿™äº›å±€é™æ€§ã€‚å¹¸è¿çš„æ˜¯ï¼Œç ”ç©¶äººå‘˜å·²ç»å¼€å‘å‡ºæ–°ä¸€ä»£çš„ä¼˜åŒ–ç®—æ³•ï¼Œæˆ‘ä»¬ä¹Ÿå°†è®¨è®ºè¿™äº›ç®—æ³•ã€‚
- en: Gradient descent challenges
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¢¯åº¦ä¸‹é™çš„æŒ‘æˆ˜
- en: 'We previously presented how the parameters,Â *P*, of a neural network (that
    is, all the weight and bias parameters of its layers) can be iteratively updated
    during training to minimize the loss,Â *L*, backpropagating its gradient. If this
    gradient descent process could be summarized in a single equation, it would be
    the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰å±•ç¤ºäº†ç¥ç»ç½‘ç»œçš„å‚æ•° *P*ï¼ˆå³æ‰€æœ‰å±‚çš„æƒé‡å’Œåç½®å‚æ•°ï¼‰å¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡åå‘ä¼ æ’­æ¢¯åº¦ï¼Œé€æ­¥æ›´æ–°ä»¥æœ€å°åŒ–æŸå¤± *L*ã€‚å¦‚æœæŠŠè¿™ä¸ªæ¢¯åº¦ä¸‹é™è¿‡ç¨‹ç”¨ä¸€ä¸ªå…¬å¼æ¥æ€»ç»“ï¼Œåº”è¯¥æ˜¯ä¸‹é¢è¿™ä¸ªæ ·å­ï¼š
- en: '![](img/d9fc0237-4e1b-4307-ac64-206590a62b1c.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d9fc0237-4e1b-4307-ac64-206590a62b1c.png)'
- en: '![](img/182e2239-4dff-46d3-a9db-a5e364c47e81.png)Â isÂ the learning rate hyperparameter,
    which accentuates or attenuates how the network''s parameters are updated with
    regard to the gradient of the loss at every training iteration. While we mentioned
    that the learning rate value should be set with care, we did not explain how and
    why. The reasons for caution in this setup are threefold.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/182e2239-4dff-46d3-a9db-a5e364c47e81.png) æ˜¯å­¦ä¹ ç‡è¶…å‚æ•°ï¼Œå®ƒå¼ºè°ƒæˆ–å‡å¼±ç½‘ç»œå‚æ•°åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£æ—¶ï¼Œæ ¹æ®æŸå¤±å‡½æ•°çš„æ¢¯åº¦æ›´æ–°çš„æ–¹å¼ã€‚è™½ç„¶æˆ‘ä»¬æåˆ°å­¦ä¹ ç‡å€¼åº”è°¨æ…è®¾ç½®ï¼Œä½†å¹¶æœªè§£é‡Šä¸ºä½•åŠå¦‚ä½•è®¾ç½®ã€‚å¯¹æ­¤éœ€è¦è°¨æ…çš„åŸå› æœ‰ä¸‰ã€‚'
- en: Training velocity and trade-off
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è®­ç»ƒé€Ÿåº¦ä¸æƒè¡¡
- en: 'We partially covered this point earlier. While setting a highÂ learning rate
    may allow the trained network to converge faster (that is, in fewer iterations,
    as the parameters undergo larger updates each iteration), it also may prevent
    the network from finding a proper loss minimum. *Figure 3.11* is a famous illustration
    representing this trade-off between optimization over-cautiousness and haste:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹‹å‰éƒ¨åˆ†åœ°è®¨è®ºè¿‡è¿™ä¸€ç‚¹ã€‚è™½ç„¶è®¾ç½®è¾ƒé«˜çš„å­¦ä¹ ç‡å¯èƒ½ä¼šè®©è®­ç»ƒåçš„ç½‘ç»œæ›´å¿«åœ°æ”¶æ•›ï¼ˆå³ï¼Œåœ¨è¾ƒå°‘çš„è¿­ä»£ä¸­ï¼Œç”±äºæ¯æ¬¡è¿­ä»£æ—¶å‚æ•°æ›´æ–°å¹…åº¦è¾ƒå¤§ï¼‰ï¼Œä½†å®ƒä¹Ÿå¯èƒ½ä¼šé˜»æ­¢ç½‘ç»œæ‰¾åˆ°åˆé€‚çš„æŸå¤±æœ€å°å€¼ã€‚*å›¾
    3.11* æ˜¯ä¸€ä¸ªè‘—åçš„æ’å›¾ï¼Œå±•ç¤ºäº†ä¼˜åŒ–æ—¶è¿‡äºè°¨æ…ä¸æ€¥äºæ±‚æˆä¹‹é—´çš„æƒè¡¡ï¼š
- en: '![](img/6ddc34df-90e3-460b-a9af-8b410dc910d2.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ddc34df-90e3-460b-a9af-8b410dc910d2.png)'
- en: 'Figure 3.11: Illustration of the learning rate trade-off'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3.11ï¼šå­¦ä¹ ç‡æƒè¡¡çš„æ’å›¾
- en: From *Figure 3.11*, we can observe that an excessively low learning rate will
    slow down convergence (diagram A on the left), while an excessively high learning
    rate may cause it to overshoot the local minima (diagram B on the right).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ä»*å›¾ 3.11*ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œè¿‡ä½çš„å­¦ä¹ ç‡ä¼šå‡æ…¢æ”¶æ•›é€Ÿåº¦ï¼ˆå·¦ä¾§çš„å›¾è¡¨Aï¼‰ï¼Œè€Œè¿‡é«˜çš„å­¦ä¹ ç‡å¯èƒ½ä¼šå¯¼è‡´å…¶è¶…è¿‡å±€éƒ¨æœ€å°å€¼ï¼ˆå³ä¾§çš„å›¾è¡¨Bï¼‰ã€‚
- en: Intuitively, there should be a better solution than trial and error to find
    the proper learning rate. For instance, a popular solution is to dynamically adjust
    the learning rate during training, starting with a larger value (for faster exploration
    of the loss domain at first) and decreasing it after every epoch (for more careful
    updating when getting closer to the minimum). This process is named **learning
    rate decay**. Manual decaying can still be found in many implementations, though,
    nowadays, TensorFlow offers more advanced learning rate schedulers and optimizers
    with adaptive learning rates.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‚åœ°è¯´ï¼Œåº”è¯¥æœ‰æ¯”åå¤è¯•éªŒæ›´å¥½çš„æ–¹æ³•æ¥æ‰¾åˆ°åˆé€‚çš„å­¦ä¹ ç‡ã€‚ä¾‹å¦‚ï¼Œä¸€ç§å¸¸è§çš„è§£å†³æ–¹æ¡ˆæ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼Œä»è¾ƒå¤§çš„å€¼å¼€å§‹ï¼ˆä»¥ä¾¿åˆæœŸæ›´å¿«é€Ÿåœ°æ¢ç´¢æŸå¤±åŸŸï¼‰ï¼Œç„¶ååœ¨æ¯ä¸ªå‘¨æœŸåå°†å…¶å‡å°ï¼ˆä»¥ä¾¿åœ¨æ¥è¿‘æœ€å°å€¼æ—¶è¿›è¡Œæ›´è°¨æ…çš„æ›´æ–°ï¼‰ã€‚è¿™ä¸€è¿‡ç¨‹ç§°ä¸º**å­¦ä¹ ç‡è¡°å‡**ã€‚æ‰‹åŠ¨è¡°å‡ä»ç„¶å¯ä»¥åœ¨è®¸å¤šå®ç°ä¸­æ‰¾åˆ°ï¼Œä¸è¿‡ï¼Œç°åœ¨
    TensorFlow æä¾›äº†æ›´å…ˆè¿›çš„å­¦ä¹ ç‡è°ƒåº¦å™¨å’Œå…·æœ‰è‡ªé€‚åº”å­¦ä¹ ç‡çš„ä¼˜åŒ–å™¨ã€‚
- en: Suboptimal local minima
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ¬¡ä¼˜å±€éƒ¨æœ€å°å€¼
- en: 'A common problem when optimizing complex (that is, *non-convex*) methods is
    getting stuck in **suboptimal local minima**. Indeed, gradient descent may lead
    us to a local minimum it cannot escape, even though a *betterÂ *minimum lies close
    by, as shown inÂ *Figure 3.12*:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼˜åŒ–å¤æ‚ï¼ˆå³ï¼Œ*éå‡¸*ï¼‰æ–¹æ³•æ—¶ï¼Œä¸€ä¸ªå¸¸è§çš„é—®é¢˜æ˜¯é™·å…¥**æ¬¡ä¼˜å±€éƒ¨æœ€å°å€¼**ã€‚äº‹å®ä¸Šï¼Œæ¢¯åº¦ä¸‹é™æ³•å¯èƒ½ä¼šå°†æˆ‘ä»¬å¸¦åˆ°ä¸€ä¸ªæ— æ³•é€ƒè„±çš„å±€éƒ¨æœ€å°å€¼ï¼Œå³ä½¿*æ›´å¥½çš„*æœ€å°å€¼å°±åœ¨æ—è¾¹ï¼Œå¦‚*å›¾
    3.12*æ‰€ç¤ºï¼š
- en: '![](img/919aee27-8fb7-4891-bdf7-7494adf55da1.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/919aee27-8fb7-4891-bdf7-7494adf55da1.png)'
- en: 'Figure 3.12: Example of gradient descent ending up in a sub-optimal local minimum'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 3.12ï¼šæ¢¯åº¦ä¸‹é™æœ€ç»ˆé™·å…¥æ¬¡ä¼˜å±€éƒ¨æœ€å°å€¼çš„ç¤ºä¾‹
- en: Because of the random sampling of training samples (causing the gradients to
    often differ from one mini-batch to another), the SGD presented in [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*, is already able to *jump outÂ *of shallow
    local minima.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè®­ç»ƒæ ·æœ¬çš„éšæœºé‡‡æ ·ï¼ˆå¯¼è‡´æ¯ä¸ªå°æ‰¹æ¬¡çš„æ¢¯åº¦å¸¸å¸¸æœ‰æ‰€ä¸åŒï¼‰ï¼Œ[ç¬¬ 1 ç« ](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)ä¸­ä»‹ç»çš„SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰ï¼Œ*è®¡ç®—æœºè§†è§‰ä¸ç¥ç»ç½‘ç»œ*ï¼Œå·²ç»èƒ½å¤Ÿ*è·³å‡º*æµ…å±‚å±€éƒ¨æœ€å°å€¼ã€‚
- en: Note that the gradient descent process cannot ensure the convergence to a **global
    minimum** (that is, the convergence to the best set of parameters among all possible
    combinations). ThisÂ wouldÂ imply scanning the complete loss domain, to make sure
    that a given minimum is indeed the *best* (this would mean, for instance, computing
    the loss for all possible combinations of the parameters). Given the complexity
    of visual tasks and the large number of parameters needed to tackle them, data
    scientists are usually glad to just find a satisfying local minimum.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·æ³¨æ„ï¼Œæ¢¯åº¦ä¸‹é™è¿‡ç¨‹ä¸èƒ½ç¡®ä¿æ”¶æ•›åˆ°**å…¨å±€æœ€å°å€¼**ï¼ˆå³ï¼Œæ”¶æ•›åˆ°æ‰€æœ‰å¯èƒ½ç»„åˆä¸­çš„æœ€ä½³å‚æ•°é›†ï¼‰ã€‚è¿™å°†æ„å‘³ç€æ‰«æå®Œæ•´çš„æŸå¤±åŸŸï¼Œä»¥ç¡®ä¿ç»™å®šçš„æœ€å°å€¼ç¡®å®æ˜¯*æœ€å¥½çš„*ï¼ˆè¿™å°†æ„å‘³ç€ï¼Œä¾‹å¦‚ï¼Œè®¡ç®—æ‰€æœ‰å¯èƒ½å‚æ•°ç»„åˆçš„æŸå¤±ï¼‰ã€‚è€ƒè™‘åˆ°è§†è§‰ä»»åŠ¡çš„å¤æ‚æ€§å’Œè§£å†³è¿™äº›ä»»åŠ¡æ‰€éœ€çš„åºå¤§å‚æ•°é‡ï¼Œæ•°æ®ç§‘å­¦å®¶é€šå¸¸æ›´æ„¿æ„æ‰¾åˆ°ä¸€ä¸ªä»¤äººæ»¡æ„çš„å±€éƒ¨æœ€å°å€¼ã€‚
- en: A single hyperparameter for heterogeneous parameters
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé€‚ç”¨äºå¼‚è´¨å‚æ•°çš„è¶…å‚æ•°
- en: Finally, in traditional gradient descent, the same learning rate is used to
    update all the parameters of the network. However, not all these variables have
    the same sensitivity to changes, nor do they all impact the loss at every iteration.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåœ¨ä¼ ç»Ÿçš„æ¢¯åº¦ä¸‹é™æ³•ä¸­ï¼Œç›¸åŒçš„å­¦ä¹ ç‡ç”¨äºæ›´æ–°ç½‘ç»œä¸­çš„æ‰€æœ‰å‚æ•°ã€‚ç„¶è€Œï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰è¿™äº›å˜é‡å¯¹å˜åŒ–çš„æ•æ„Ÿåº¦ç›¸åŒï¼Œä¹Ÿä¸æ˜¯å®ƒä»¬åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½ä¼šå¯¹æŸå¤±äº§ç”Ÿç›¸åŒçš„å½±å“ã€‚
- en: It may seem beneficial to have different learning rates (for instance, per subset
    of parameters) to update crucial parameters more carefully, and to more boldlyÂ update
    parameters that are not contributingÂ often enough to the network's predictions.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½ä¼šè§‰å¾—ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ï¼ˆä¾‹å¦‚ï¼Œé’ˆå¯¹å‚æ•°çš„å­é›†ï¼‰æ›´æ–°å…³é”®å‚æ•°ï¼Œä»¥ä¾¿æ›´åŠ ç²¾ç»†åœ°æ›´æ–°è¿™äº›å‚æ•°ï¼ŒåŒæ—¶å¤§èƒ†æ›´æ–°é‚£äº›å¯¹ç½‘ç»œé¢„æµ‹è´¡çŒ®ä¸è¶³çš„å‚æ•°ï¼Œè¿™æ ·åšæ˜¯æœ‰åˆ©çš„ã€‚
- en: Advanced optimizers
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é«˜çº§ä¼˜åŒ–å™¨
- en: Some of the intuitions we presented in the previous paragraphs have been properly
    studied and formalized by researchers, leading to new optimization algorithms
    based on SGD. We will now list the most common of these optimizers, detailing
    their contributions and how to use them with TensorFlow.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨å‰è¿°æ®µè½ä¸­æå‡ºçš„ä¸€äº›ç›´è§‰å·²ç»è¢«ç ”ç©¶äººå‘˜è¿›è¡Œäº†é€‚å½“çš„ç ”ç©¶å’Œå½¢å¼åŒ–ï¼Œä»è€Œå¯¼è‡´äº†åŸºäºSGDçš„æ–°ä¼˜åŒ–ç®—æ³•ã€‚ç°åœ¨æˆ‘ä»¬å°†åˆ—å‡ºè¿™äº›ä¼˜åŒ–å™¨ä¸­æœ€å¸¸è§çš„å‡ ç§ï¼Œè¯¦ç»†ä»‹ç»å®ƒä»¬çš„è´¡çŒ®ä»¥åŠå¦‚ä½•åœ¨TensorFlowä¸­ä½¿ç”¨å®ƒä»¬ã€‚
- en: Momentum algorithms
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŠ¨é‡ç®—æ³•
- en: 'First suggested by Boris Polyak (in *Some methods of speeding up the convergence
    of iteration methods*, Elsevier, 1964), the momentum algorithm is basedÂ on SGD
    and inspired by the physics notion of **momentum**â€”as long as an object is moving
    downhill, its speed will increase with each step. Applied to gradient descent,
    the idea is to take previous parameter updates,Â *v[i-1]*,Â into account,Â adding
    them to the new update terms, *v[i]*,as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨é‡ç®—æ³•æœ€æ—©ç”±Boris Polyakæå‡ºï¼ˆè§*æŸäº›åŠ é€Ÿè¿­ä»£æ–¹æ³•æ”¶æ•›çš„æ–¹æ³•*ï¼ŒElsevierï¼Œ1964å¹´ï¼‰ï¼Œè¯¥ç®—æ³•åŸºäºSGDå¹¶å—åˆ°ç‰©ç†å­¦ä¸­**åŠ¨é‡**æ¦‚å¿µçš„å¯å‘â€”â€”åªè¦ä¸€ä¸ªç‰©ä½“åœ¨ä¸‹å¡ï¼Œå®ƒçš„é€Ÿåº¦å°†åœ¨æ¯ä¸€æ­¥ä¸­åŠ é€Ÿã€‚åº”ç”¨äºæ¢¯åº¦ä¸‹é™æ—¶ï¼Œç†å¿µæ˜¯è€ƒè™‘åˆ°ä¹‹å‰çš„å‚æ•°æ›´æ–°ï¼Œ*v[i-1]*ï¼Œå¹¶å°†å…¶åŠ å…¥åˆ°æ–°çš„æ›´æ–°é¡¹ä¸­ï¼Œ*v[i]*ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/c055ee55-d604-4526-93ce-5678c5603373.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c055ee55-d604-4526-93ce-5678c5603373.png)'
- en: Here,Â ![](img/fceb4a4e-5c66-4e5b-93ec-931110036eb0.png) (*mu*) is the momentum
    weighing (the value between 0 and 1), defining the fraction of the previous updates
    to apply. If the current and previous steps have the same direction, their magnitudes
    will add up, accelerating the SGD in this relevant direction. If they have different
    directions, the momentum will dampen these oscillations.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ï¼Œ![](img/fceb4a4e-5c66-4e5b-93ec-931110036eb0.png)ï¼ˆ*mu*ï¼‰æ˜¯åŠ¨é‡æƒé‡ï¼ˆä»‹äº0å’Œ1ä¹‹é—´çš„å€¼ï¼‰ï¼Œå®šä¹‰äº†åº”ç”¨å‰æ¬¡æ›´æ–°çš„æ¯”ä¾‹ã€‚å¦‚æœå½“å‰æ­¥éª¤å’Œå‰ä¸€æ­¥çš„æ–¹å‘ç›¸åŒï¼Œå®ƒä»¬çš„å¹…åº¦å°†ä¼šå åŠ ï¼Œå¯¼è‡´SGDåœ¨è¯¥æ–¹å‘ä¸ŠåŠ é€Ÿã€‚å¦‚æœæ–¹å‘ä¸åŒï¼ŒåŠ¨é‡å°†æŠ‘åˆ¶è¿™äº›æŒ¯è¡ã€‚
- en: 'In `tf.optimizers` (also accessible as `tf.keras.optimizers`), momentum is
    defined as an optional parameter of SGD (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD))
    as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`tf.optimizers`ï¼ˆä¹Ÿå¯ä»¥é€šè¿‡`tf.keras.optimizers`è®¿é—®ï¼‰ä¸­ï¼ŒåŠ¨é‡è¢«å®šä¹‰ä¸ºSGDçš„ä¸€ä¸ªå¯é€‰å‚æ•°ï¼ˆå‚è§[https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD)ï¼‰å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE8]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This optimizer accepts aÂ `decay` parameter, fixing the learning rate decay over
    each update (refer to the previous paragraphs).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥ä¼˜åŒ–å™¨æ¥å—ä¸€ä¸ª`decay`å‚æ•°ï¼Œç”¨äºä¿®æ­£æ¯æ¬¡æ›´æ–°æ—¶çš„å­¦ä¹ ç‡è¡°å‡ï¼ˆå‚è§å‰è¿°æ®µè½ï¼‰ã€‚
- en: 'This optimizer instance can then be directly passed as a parameter to `model.fit()`
    when launching the training through the Keras API. For more complex training scenarios
    (for instance, when training interdependent networks), the optimizer can also
    be called, providing it with the loss gradients and the model''s trainable parameters.
    The following is an example of a simple training step implemented manually:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå¯ä»¥ç›´æ¥å°†æ­¤ä¼˜åŒ–å™¨å®ä¾‹ä½œä¸ºå‚æ•°ä¼ é€’ç»™`model.fit()`ï¼Œé€šè¿‡Keras APIå¯åŠ¨è®­ç»ƒã€‚å¯¹äºæ›´å¤æ‚çš„è®­ç»ƒåœºæ™¯ï¼ˆä¾‹å¦‚ï¼Œå½“è®­ç»ƒäº’ä¾ç½‘ç»œæ—¶ï¼‰ï¼Œä¼˜åŒ–å™¨ä¹Ÿå¯ä»¥è¢«è°ƒç”¨ï¼Œå¹¶æä¾›æŸå¤±æ¢¯åº¦å’Œæ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„è®­ç»ƒæ­¥éª¤ç¤ºä¾‹ï¼Œæ‰‹åŠ¨å®ç°ï¼š
- en: '[PRE9]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`tf.optimizers.SGD` has one interesting Boolean parameterâ€”to switch from the
    common momentum method to Nesterov''s algorithm. Indeed, a major problem of the
    former method is that by the time the network gets really close to its loss minimum,
    the accumulated momentum will usually be quite high, which may cause the method
    to miss or oscillate around the target minimum.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.optimizers.SGD`æœ‰ä¸€ä¸ªæœ‰è¶£çš„å¸ƒå°”å‚æ•°â€”â€”ç”¨äºå°†å¸¸è§çš„åŠ¨é‡æ–¹æ³•åˆ‡æ¢åˆ°Nesterovç®—æ³•ã€‚å®é™…ä¸Šï¼Œå‰ä¸€ç§æ–¹æ³•çš„ä¸€ä¸ªä¸»è¦é—®é¢˜æ˜¯ï¼Œå½“ç½‘ç»œéå¸¸æ¥è¿‘æŸå¤±æœ€å°å€¼æ—¶ï¼Œç§¯ç´¯çš„åŠ¨é‡é€šå¸¸ä¼šéå¸¸é«˜ï¼Œè¿™å¯èƒ½å¯¼è‡´æ–¹æ³•é”™è¿‡æˆ–åœ¨ç›®æ ‡æœ€å°å€¼å‘¨å›´æŒ¯è¡ã€‚'
- en: 'The **Nesterov accelerated gradient** (**NAG**Â or **Nesterov momentum**) offers
    a solution to this problem (a related course isÂ *Introductory Lectures on Convex
    Programming Volume I: Basic course,Â *by Yurii Nesterov, *Springer Science and
    Business Media*). Back in the 1980s, Yurii Nesterov''s idea was to give the optimizer
    the possibility to have a look at the slope ahead so that it *knowsÂ *it should
    slow down if the slope starts going up. More formally, Nesterov suggested directly
    reusing the past term *v[i-1]* to estimate which values,Â *P[i+1]*, the parameters
    would take if we keep following this direction. The gradient is then evaluated
    with respect to those approximate future parameters, and it is used to finally
    compute the actual update as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**Nesterov åŠ é€Ÿæ¢¯åº¦**ï¼ˆ**NAG** æˆ– **Nesterov åŠ¨é‡**ï¼‰ä¸ºè¿™ä¸ªé—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆï¼ˆç›¸å…³è¯¾ç¨‹ä¸º *å‡¸ç¼–ç¨‹åŸºç¡€è®²åº§ç¬¬ä¸€å·ï¼šåŸºç¡€è¯¾ç¨‹*ï¼Œç”±
    Yurii Nesterov ç¼–å†™ï¼Œ*Springer Science and Business Media* å‡ºç‰ˆï¼‰ã€‚æ—©åœ¨ 1980 å¹´ä»£ï¼ŒYurii Nesterov
    çš„æƒ³æ³•æ˜¯è®©ä¼˜åŒ–å™¨æœ‰æœºä¼šæå‰æŸ¥çœ‹å¡åº¦ï¼Œä»¥ä¾¿ *çŸ¥é“* å¦‚æœå¡åº¦å¼€å§‹ä¸Šå‡ï¼Œå®ƒåº”è¯¥æ”¾æ…¢é€Ÿåº¦ã€‚æ›´æ­£å¼åœ°è¯´ï¼ŒNesterov å»ºè®®ç›´æ¥é‡ç”¨è¿‡å»çš„é¡¹ *v[i-1]*
    æ¥ä¼°ç®—å¦‚æœæˆ‘ä»¬ç»§ç»­æ²¿æ­¤æ–¹å‘å‰è¿›ï¼Œå‚æ•° *P[i+1]* ä¼šå–ä»€ä¹ˆå€¼ã€‚ç„¶åï¼Œæ¢¯åº¦å°†åŸºäºè¿™äº›è¿‘ä¼¼çš„æœªæ¥å‚æ•°è¿›è¡Œè¯„ä¼°ï¼Œå¹¶æœ€ç»ˆç”¨äºè®¡ç®—å®é™…çš„æ›´æ–°ï¼Œå…¬å¼å¦‚ä¸‹ï¼š'
- en: '![](img/ae9b495d-7747-4cdb-bbc6-8fcb195b9509.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae9b495d-7747-4cdb-bbc6-8fcb195b9509.png)'
- en: This version of the momentum optimizer (where the loss is derived with respectÂ to
    the parameters' values updated according to the previous steps) is more adaptable
    to gradient changes, and can significantly speed up the gradient descent process.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§åŠ¨é‡ä¼˜åŒ–å™¨ç‰ˆæœ¬ï¼ˆå…¶ä¸­æŸå¤±æ˜¯åŸºäºå‚æ•°çš„å€¼è®¡ç®—çš„ï¼Œæ›´æ–°å€¼æ ¹æ®ä¹‹å‰çš„æ­¥éª¤è¿›è¡Œè°ƒæ•´ï¼‰å¯¹æ¢¯åº¦å˜åŒ–æ›´å…·é€‚åº”æ€§ï¼Œå¯ä»¥æ˜¾è‘—åŠ å¿«æ¢¯åº¦ä¸‹é™çš„è¿‡ç¨‹ã€‚
- en: The Ada family
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ada å®¶æ—
- en: '**Adagrad**, **Adadelta**, andÂ **Adam**Â are several iterations and variations
    around the idea of adapting the learning rate depending on the sensitivity and/or
    activation frequency of each neuron.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**Adagrad**ã€**Adadelta** å’Œ **Adam** æ˜¯å›´ç»•æ ¹æ®æ¯ä¸ªç¥ç»å…ƒçš„æ•æ„Ÿæ€§å’Œ/æˆ–æ¿€æ´»é¢‘ç‡è°ƒæ•´å­¦ä¹ ç‡çš„æ€è·¯çš„å‡ ä¸ªè¿­ä»£å’Œå˜ä½“ã€‚'
- en: Developed first by John Duchi et al. (in *Adaptive Subgradient Methods for Online
    Learning and Stochastic Optimization*, Journal of Machine Learning Research, 2011),
    the *Adagrad* optimizer (for *adaptive gradients*) uses a neat formula (which
    we won't expand on here, though we invite you to search for it) to automatically
    decrease the learning rate more quickly for parameters linked to commonly found
    features, and more slowly for infrequent ones. In other words, as presented in
    the Keras documentation,Â *the more updates a parameter receives, the smaller the
    updatesÂ *(refer to the documentation at [https://keras.io/optimizers/](https://keras.io/optimizers/)).
    This optimization algorithm not only removes the need to manually adapt/decay
    the learning rate, but it also makes the SGD process more stable, especially for
    datasets with sparse representations.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*Adagrad* ä¼˜åŒ–å™¨ï¼ˆç”¨äº *è‡ªé€‚åº”æ¢¯åº¦*ï¼‰æœ€åˆç”± John Duchi ç­‰äººåœ¨ã€Š*åœ¨çº¿å­¦ä¹ å’Œéšæœºä¼˜åŒ–çš„è‡ªé€‚åº”å­æ¢¯åº¦æ–¹æ³•*ã€‹ï¼ˆã€Šæœºå™¨å­¦ä¹ ç ”ç©¶æ‚å¿—ã€‹ï¼Œ2011ï¼‰ä¸­æå‡ºï¼Œä½¿ç”¨ä¸€ä¸ªç²¾å·§çš„å…¬å¼ï¼ˆæˆ‘ä»¬åœ¨è¿™é‡Œä¸å±•å¼€ï¼Œæ¬¢è¿ä½ è‡ªè¡ŒæŸ¥é˜…ï¼‰æ¥è‡ªåŠ¨æ›´å¿«åœ°å‡å°‘ä¸å¸¸è§ç‰¹å¾ç›¸å…³çš„å‚æ•°çš„å­¦ä¹ ç‡ï¼Œå¯¹äºä¸å¸¸è§çš„ç‰¹å¾åˆ™å‡å°å¾—è¾ƒæ…¢ã€‚æ¢å¥è¯è¯´ï¼Œæ­£å¦‚
    Keras æ–‡æ¡£ä¸­æ‰€æè¿°ï¼Œ*ä¸€ä¸ªå‚æ•°æ¥å—çš„æ›´æ–°æ¬¡æ•°è¶Šå¤šï¼Œæ›´æ–°çš„å¹…åº¦è¶Šå°*ï¼ˆå¯ä»¥å‚è€ƒ [https://keras.io/optimizers/](https://keras.io/optimizers/)
    çš„æ–‡æ¡£ï¼‰ã€‚è¿™ç§ä¼˜åŒ–ç®—æ³•ä¸ä»…æ¶ˆé™¤äº†æ‰‹åŠ¨è°ƒæ•´/è¡°å‡å­¦ä¹ ç‡çš„éœ€æ±‚ï¼Œè¿˜ä½¿å¾— SGD è¿‡ç¨‹æ›´åŠ ç¨³å®šï¼Œç‰¹åˆ«æ˜¯åœ¨å…·æœ‰ç¨€ç–è¡¨ç¤ºçš„æ•°æ®é›†ä¸Šã€‚'
- en: 'Introducing *Adadelta* in 2013, Matthew D. Zeiler et al. (in *ADADELTA: An
    Adaptive Learning Rate Method*, *arXiv preprint*) offered a solution to one problem
    inherent to *Adagrad.* As it keeps decaying the learning rate every iteration,
    at some point, the learning rate becomes too small and the network just cannot
    learn anymore (except maybe for infrequent parameters). *Adadelta* avoids this
    problem by keeping in check the factors used to divide the learning rate for each
    parameter.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '2013 å¹´ï¼ŒMatthew D. Zeiler ç­‰äººåœ¨ã€Š*ADADELTA: ä¸€ç§è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•*ã€‹ï¼ˆ*arXiv é¢„å°æœ¬*ï¼‰ä¸­ä»‹ç»äº† *Adadelta*ï¼Œä¸º
    *Adagrad* å›ºæœ‰çš„é—®é¢˜æä¾›äº†è§£å†³æ–¹æ¡ˆã€‚ç”±äº *Adagrad* åœ¨æ¯æ¬¡è¿­ä»£ä¸­éƒ½ä¼šè¡°å‡å­¦ä¹ ç‡ï¼Œå› æ­¤åˆ°æŸä¸ªæ—¶åˆ»ï¼Œå­¦ä¹ ç‡å˜å¾—è¿‡å°ï¼Œç½‘ç»œå°†æ— æ³•ç»§ç»­å­¦ä¹ ï¼ˆé™¤äº†å¯èƒ½ä¸€äº›ä¸å¸¸è§çš„å‚æ•°ï¼‰ã€‚*Adadelta*
    é€šè¿‡æ§åˆ¶æ¯ä¸ªå‚æ•°ç”¨äºé™¤ä»¥å­¦ä¹ ç‡çš„å› å­ï¼Œé¿å…äº†è¿™ä¸ªé—®é¢˜ã€‚'
- en: '**RMSprop** by Geoffrey Hinton is another well-known optimizer (introduced
    in his Coursera course,Â <q>Lecture 6.5-rmsprop: Divide the gradient by a running
    average of its recent magnitude</q>). Associated with, and quite similar to *Adadelta*,
    *RMSprop* was also developed to correct *Adagrad*''s flaw.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**RMSprop** ç”± Geoffrey Hinton æå‡ºï¼Œæ˜¯å¦ä¸€ä¸ªè‘—åçš„ä¼˜åŒ–å™¨ï¼ˆåœ¨ä»–çš„ Coursera è¯¾ç¨‹ä¸­ä»‹ç»ï¼Œ<q>Lecture 6.5-rmsprop:
    å°†æ¢¯åº¦é™¤ä»¥å…¶è¿‘æœŸå¹…åº¦çš„æ»‘åŠ¨å¹³å‡å€¼</q>ï¼‰ã€‚ä¸ *Adadelta* å¯†åˆ‡ç›¸å…³ä¸”ç›¸ä¼¼ï¼Œ**RMSprop** ä¹Ÿè¢«å¼€å‘ç”¨æ¥ä¿®æ­£ *Adagrad* çš„ç¼ºé™·ã€‚'
- en: '**Adam** (for **adaptive moment estimation**) is another iteration by Diederik
    P. Kingma et al. (in *Adam: A method for stochastic optimization*, ICLR, 2015).
    In addition to storing previous update terms,Â *v[i]*, to adapt the learning rate
    for each parameter, *Adam* also keeps track of the past momentum values. It is,
    therefore, often identified as a mix between *Adadelta* and *momentum*. Similarly,
    **Nadam** is an optimizer inheriting from *Adadelta* and *NAG*.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**Adam**ï¼ˆè‡ªé€‚åº”çŸ©ä¼°è®¡æ³•ï¼‰æ˜¯Diederik P. Kingmaç­‰äººæå‡ºçš„å¦ä¸€ç§ä¼˜åŒ–æ–¹æ³•ï¼ˆè§*Adam: A method for stochastic
    optimization*ï¼ŒICLRï¼Œ2015ï¼‰ã€‚é™¤äº†å­˜å‚¨å…ˆå‰çš„æ›´æ–°é¡¹ *v[i]* æ¥è°ƒæ•´æ¯ä¸ªå‚æ•°çš„å­¦ä¹ ç‡å¤–ï¼Œ*Adam* è¿˜ä¼šè·Ÿè¸ªè¿‡å»çš„åŠ¨é‡å€¼ã€‚å› æ­¤ï¼Œå®ƒé€šå¸¸è¢«è®¤ä¸ºæ˜¯*Adadelta*å’Œ*momentum*çš„æ··åˆä½“ã€‚åŒæ ·ï¼Œ**Nadam**æ˜¯ç»§æ‰¿è‡ª*Adadelta*å’Œ*NAG*çš„ä¼˜åŒ–å™¨ã€‚'
- en: All these various optimizers are available in the `tf.optimizers` package (refer
    to the documentation at [https://www.tensorflow.org/api_docs/python/tf/train/](https://www.tensorflow.org/api_docs/python/tf/train/)).
    Note that there is no consensus regarding which of these optimizers may be the
    best. *Adam* is, however, preferred by many computer vision professionals for
    its effectiveness on scarce data. *RMSprop* is also often considered a good choice
    for recurrent neural networks (introduced in [Chapter 8](97884989-bb57-4611-8c66-ebe8ab387965.xhtml),
    *Video and Recurrent Neural Networks*).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›ä¸åŒçš„ä¼˜åŒ–å™¨éƒ½å¯ä»¥åœ¨`tf.optimizers`åŒ…ä¸­æ‰¾åˆ°ï¼ˆè¯·å‚è€ƒ[https://www.tensorflow.org/api_docs/python/tf/train/](https://www.tensorflow.org/api_docs/python/tf/train/)ä¸­çš„æ–‡æ¡£ï¼‰ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç›®å‰æ²¡æœ‰å…±è¯†è®¤ä¸ºå“ªä¸ªä¼˜åŒ–å™¨æ˜¯æœ€å¥½çš„ã€‚ç„¶è€Œï¼Œ*Adam*
    è¢«è®¸å¤šè®¡ç®—æœºè§†è§‰ä¸“ä¸šäººå£«æ‰€é’çï¼Œå› ä¸ºå®ƒåœ¨æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹è¡¨ç°æœ‰æ•ˆã€‚*RMSprop* ä¹Ÿå¸¸è¢«è®¤ä¸ºæ˜¯é€’å½’ç¥ç»ç½‘ç»œçš„ä¸€ä¸ªä¸é”™é€‰æ‹©ï¼ˆå¦‚åœ¨[ç¬¬8ç« ](97884989-bb57-4611-8c66-ebe8ab387965.xhtml)ï¼Œ*è§†é¢‘å’Œé€’å½’ç¥ç»ç½‘ç»œ*ä¸­ä»‹ç»çš„ï¼‰ã€‚
- en: A Jupyter notebook demonstrating how to use these various optimizers is provided
    in the Git repository. Each optimizer is also applied to the training of our *LeNet-5*
    for MNIST classification, in order to compare their convergence.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå±•ç¤ºå¦‚ä½•ä½¿ç”¨è¿™äº›ä¸åŒä¼˜åŒ–å™¨çš„Jupyter notebookå·²æä¾›åœ¨Gitä»“åº“ä¸­ã€‚æ¯ä¸ªä¼˜åŒ–å™¨è¿˜åº”ç”¨äºæˆ‘ä»¬ç”¨äºMNISTåˆ†ç±»çš„*LeNet-5*çš„è®­ç»ƒï¼Œä»¥ä¾¿æ¯”è¾ƒå®ƒä»¬çš„æ”¶æ•›æƒ…å†µã€‚
- en: Regularization methods
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–æ–¹æ³•
- en: Efficiently teaching neural networks so that they minimize the loss over training
    data is, however, not enough. We also want these networks to perform well once
    applied to new images. We do not want them to *overfit* the training set (as mentioned
    inÂ [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),Â *Computer Vision and
    Neural Networks*). For our networks to generalize well, we mentioned that rich
    training sets (with enough variability to cover possible testing scenarios) and
    well-defined architectures (neither too shallow to avoid underfitting, nor too
    complex to prevent overfitting) are key. However, other methods have been developed
    over the years for **regularization**;Â for example, the process of refining the
    optimization phase to avoid overfitting.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä»…ä»…é«˜æ•ˆåœ°è®­ç»ƒç¥ç»ç½‘ç»œï¼Œä½¿å…¶åœ¨è®­ç»ƒæ•°æ®ä¸Šæœ€å°åŒ–æŸå¤±ï¼Œè¿˜ä¸å¤Ÿã€‚æˆ‘ä»¬è¿˜å¸Œæœ›è¿™äº›ç½‘ç»œåœ¨åº”ç”¨äºæ–°å›¾åƒæ—¶è¡¨ç°è‰¯å¥½ã€‚æˆ‘ä»¬ä¸å¸Œæœ›å®ƒä»¬*è¿‡æ‹Ÿåˆ*è®­ç»ƒé›†ï¼ˆå¦‚åœ¨[ç¬¬1ç« ](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml)ï¼Œ*è®¡ç®—æœºè§†è§‰ä¸ç¥ç»ç½‘ç»œ*ä¸­æåˆ°çš„é‚£æ ·ï¼‰ã€‚ä¸ºäº†è®©æˆ‘ä»¬çš„ç½‘ç»œå…·å¤‡è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬æåˆ°è¿‡ï¼Œä¸°å¯Œçš„è®­ç»ƒé›†ï¼ˆè¶³å¤Ÿçš„å˜å¼‚æ€§æ¥è¦†ç›–å¯èƒ½çš„æµ‹è¯•åœºæ™¯ï¼‰å’Œæ˜ç¡®å®šä¹‰çš„æ¶æ„ï¼ˆæ—¢ä¸æµ…ä»¥é¿å…æ¬ æ‹Ÿåˆï¼Œä¹Ÿä¸å¤æ‚ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼‰æ˜¯å…³é”®ã€‚ç„¶è€Œï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œå·²ç»å¼€å‘äº†å…¶ä»–æ­£åˆ™åŒ–æ–¹æ³•ï¼›ä¾‹å¦‚ï¼Œä¼˜åŒ–é˜¶æ®µçš„ç²¾ç»†è°ƒæ•´è¿‡ç¨‹ï¼Œç”¨ä»¥é¿å…è¿‡æ‹Ÿåˆã€‚
- en: Early stopping
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ—©æœŸåœæ­¢
- en: Neural networks start overfitting when they iterate too many times over the
    same small set of training samples. Therefore, a straightforward solution to prevent
    this problem is to figure out the number of training epochs a model needs. The
    number should be low enough to stop before the network starts overfitting, but
    still high enough for the network to learn all it can from this training set.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç¥ç»ç½‘ç»œåœ¨åŒä¸€å°æ‰¹è®­ç»ƒæ ·æœ¬ä¸Šè¿­ä»£è¿‡å¤šæ—¶ï¼Œå®ƒä»¬å°±ä¼šå¼€å§‹è¿‡æ‹Ÿåˆã€‚å› æ­¤ï¼Œé˜²æ­¢è¿™ç§é—®é¢˜çš„ä¸€ä¸ªç®€å•è§£å†³æ–¹æ¡ˆæ˜¯ç¡®å®šæ¨¡å‹éœ€è¦çš„è®­ç»ƒå‘¨æœŸæ•°ã€‚è¿™ä¸ªæ•°å­—åº”è¯¥è¶³å¤Ÿä½ï¼Œä»¥ä¾¿åœ¨ç½‘ç»œå¼€å§‹è¿‡æ‹Ÿåˆä¹‹å‰åœæ­¢ï¼Œä½†åˆè¶³å¤Ÿé«˜ï¼Œè®©ç½‘ç»œä»è¿™ä¸ªè®­ç»ƒé›†å­¦åˆ°å®ƒèƒ½å­¦åˆ°çš„ä¸€åˆ‡ã€‚
- en: '**Cross-validation** is the key here to evaluate when training should be stopped.
    Providing a validation dataset to our optimizer, the latter can measure the performance
    of the model on images the network has not been directly optimized for. By *validating*
    the network, for instance, after each epoch, we can measure whether the training
    should continue (that is, when the validation accuracy appears to be still increasing)
    or be stopped (that is, when the validation accuracy stagnates or drops). The
    latter is called **early stopping**.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**äº¤å‰éªŒè¯** åœ¨è¿™é‡Œæ˜¯è¯„ä¼°ä½•æ—¶åœæ­¢è®­ç»ƒçš„å…³é”®ã€‚é€šè¿‡ä¸ºä¼˜åŒ–å™¨æä¾›éªŒè¯æ•°æ®é›†ï¼Œä¼˜åŒ–å™¨å¯ä»¥æµ‹é‡æ¨¡å‹åœ¨ç½‘ç»œæœªç›´æ¥ä¼˜åŒ–è¿‡çš„å›¾åƒä¸Šçš„è¡¨ç°ã€‚é€šè¿‡å¯¹ç½‘ç»œè¿›è¡Œ*éªŒè¯*ï¼Œä¾‹å¦‚åœ¨æ¯ä¸ªè®­ç»ƒå‘¨æœŸåï¼Œæˆ‘ä»¬å¯ä»¥åˆ¤æ–­è®­ç»ƒæ˜¯å¦åº”ç»§ç»­ï¼ˆå³ï¼Œå½“éªŒè¯å‡†ç¡®ç‡ä»åœ¨ä¸Šå‡æ—¶ï¼‰æˆ–åº”åœæ­¢ï¼ˆå³ï¼Œå½“éªŒè¯å‡†ç¡®ç‡åœæ»æˆ–ä¸‹é™æ—¶ï¼‰ã€‚åä¸€ç§æƒ…å†µç§°ä¸º**æ—©åœ**ã€‚'
- en: In practice, we usually monitor and plot the validation loss and metrics as
    a function of the training iterations, and we restore the saved weights at the
    optima (hence the importance of regularly saving the network during training).
    This monitoring, early stopping, and restoration of optimum weights can be automatically
    covered by one of the optional Keras callbacks (`tf.keras.callbacks.EarlyStopping`),
    as already showcased in our previous training.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘ä»¬é€šå¸¸ä¼šç›‘æ§å¹¶ç»˜åˆ¶éªŒè¯æŸå¤±å’ŒæŒ‡æ ‡ä¸è®­ç»ƒè¿­ä»£æ¬¡æ•°çš„å…³ç³»ï¼Œå¹¶åœ¨æœ€ä¼˜ç‚¹æ¢å¤ä¿å­˜çš„æƒé‡ï¼ˆå› æ­¤ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸä¿å­˜ç½‘ç»œéå¸¸é‡è¦ï¼‰ã€‚è¿™ç§ç›‘æ§ã€æ—©åœå’Œæœ€ä¼˜æƒé‡æ¢å¤å¯ä»¥é€šè¿‡ä¸€ä¸ªå¯é€‰çš„
    Keras å›è°ƒå‡½æ•°ï¼ˆ`tf.keras.callbacks.EarlyStopping`ï¼‰è‡ªåŠ¨å®Œæˆï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰çš„è®­ç»ƒä¸­å±•ç¤ºçš„é‚£æ ·ã€‚
- en: L1 and L2 regularization
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L1 å’Œ L2 æ­£åˆ™åŒ–
- en: Another way to prevent overfitting is to modify the loss in order to include
    regularization as one of the training objectives. The L1 and L2 regularizers are
    prime examples of this.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•æ˜¯ä¿®æ”¹æŸå¤±å‡½æ•°ï¼Œå°†æ­£åˆ™åŒ–ä½œä¸ºè®­ç»ƒç›®æ ‡ä¹‹ä¸€ã€‚L1 å’Œ L2 æ­£åˆ™åŒ–å°±æ˜¯è¿™ç§æ–¹æ³•çš„å…¸å‹ä¾‹å­ã€‚
- en: Principles
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸåˆ™
- en: 'In machine learning, a **regularization term**, *R(P)*, computed over the parameters,
    *P*, of the method,Â *f*, to optimize (for instance, a neural network) can be added
    to the loss function,Â *L*, before training, as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œå¯ä»¥åœ¨è®­ç»ƒå‰å°†ä¸€ä¸ªè®¡ç®—å‡ºæ¥çš„**æ­£åˆ™åŒ–é¡¹** *R(P)*ï¼ˆå®ƒæ˜¯æ–¹æ³• *f* çš„å‚æ•° *P*ï¼‰æ·»åŠ åˆ°æŸå¤±å‡½æ•° *L* ä¸­è¿›è¡Œä¼˜åŒ–ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªç¥ç»ç½‘ç»œï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](img/70fe393a-72e8-46d1-8fba-1b25c99d17ca.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/70fe393a-72e8-46d1-8fba-1b25c99d17ca.png)'
- en: Here,Â ![](img/75a1c0fb-e5e8-42ef-8f01-90efa6fb04ad.png)Â is a factor controlling
    the strength of the regularization (typically, to scale down the amplitude of
    the regularization term compared to the main loss), andÂ *y = f(x, P)Â *is the output
    of the method,Â *f*, parametrized byÂ *P* for the input data,Â *x*. By adding this
    term,Â *R(P)*, to the loss, we force the network not only to optimize its task,
    but to optimize it while *constraining* the values its parameters can take.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ![](img/75a1c0fb-e5e8-42ef-8f01-90efa6fb04ad.png) æ˜¯æ§åˆ¶æ­£åˆ™åŒ–å¼ºåº¦çš„å› å­ï¼ˆé€šå¸¸ï¼Œç”¨æ¥ç¼©å°æ­£åˆ™åŒ–é¡¹ç›¸å¯¹äºä¸»æŸå¤±çš„å¹…åº¦ï¼‰ï¼Œè€Œ
    *y = f(x, P)* æ˜¯è¯¥æ–¹æ³•çš„è¾“å‡ºï¼Œ*f*ï¼Œé€šè¿‡è¾“å…¥æ•°æ® *x* çš„å‚æ•° *P* æ¥å‚æ•°åŒ–ã€‚é€šè¿‡å°†è¿™ä¸ªé¡¹ *R(P)* æ·»åŠ åˆ°æŸå¤±ä¸­ï¼Œæˆ‘ä»¬è¿«ä½¿ç½‘ç»œä¸ä»…ä¼˜åŒ–å…¶ä»»åŠ¡ï¼Œè€Œä¸”åœ¨*çº¦æŸ*å…¶å‚æ•°å¯èƒ½å–å€¼çš„åŒæ—¶è¿›è¡Œä¼˜åŒ–ã€‚
- en: 'For L1 and L2 regularization, the respective terms are as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: L1 å’Œ L2 æ­£åˆ™åŒ–çš„ç›¸åº”é¡¹å¦‚ä¸‹ï¼š
- en: '![](img/d0e89b2e-ff40-46c1-904f-ae6fa01595d6.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d0e89b2e-ff40-46c1-904f-ae6fa01595d6.png)'
- en: '**L2 regularization** (also called **ridge regularization**) thus compels the
    network to minimize the sum of its squared parameter values. While this regularization
    leads to the decay of all parameter values over the optimization process, it more
    strongly punishes large parameters due to the squared term. Therefore, L2 regularization
    encourages the network *to keep its parameter values low and thus more homogeneously
    distributed*. It prevents the network from developing a small set of parameters
    with large values influencing its predictions (as it may prevent the network from
    generalizing).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**L2 æ­£åˆ™åŒ–**ï¼ˆä¹Ÿç§°ä¸º**å²­æ­£åˆ™åŒ–**ï¼‰å› æ­¤è¿«ä½¿ç½‘ç»œæœ€å°åŒ–å…¶å‚æ•°å€¼çš„å¹³æ–¹å’Œã€‚è™½ç„¶è¿™ç§æ­£åˆ™åŒ–å¯¼è‡´æ‰€æœ‰å‚æ•°å€¼åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­é€æ¸è¡°å‡ï¼Œä½†ç”±äºå¹³æ–¹é¡¹çš„å­˜åœ¨ï¼Œå®ƒå¯¹å¤§å‚æ•°çš„æƒ©ç½šæ›´ä¸ºå¼ºçƒˆã€‚å› æ­¤ï¼ŒL2
    æ­£åˆ™åŒ–é¼“åŠ±ç½‘ç»œ*ä¿æŒå…¶å‚æ•°å€¼è¾ƒä½ï¼Œä»è€Œæ›´åŠ å‡åŒ€åœ°åˆ†å¸ƒ*ã€‚å®ƒé˜²æ­¢ç½‘ç»œå‘å±•å‡ºä¸€ç»„å…·æœ‰å¤§å€¼çš„å‚æ•°ï¼Œè¿™äº›å¤§å€¼ä¼šå½±å“å…¶é¢„æµ‹ï¼ˆå› ä¸ºè¿™å¯èƒ½ä¼šé˜»ç¢ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›ï¼‰ã€‚'
- en: On the other hand, the **L1 regularizer** (also called theÂ **LASSO** (**least
    absolute shrinkage and selection operator**)**regularizer**, first introduced
    in *Linear Inversion of Band-Limited Reflection Seismograms,Â *by *Fadil Santosa
    and William Symes, SIAM, 1986*) compels the network to minimize the sum of its
    absolute parameter values. The difference between this and L2 regularization may
    seem symbolic at first glance, but their properties are actually quite different.
    As larger weights are not penalized by squaring, L1 regularization instead makes
    the network shrink the parameters linked to less important features toward zero.
    Therefore, it prevents overfitting by forcing the network to ignore less meaningful
    features (for instance, tied to dataset noise). In other words, L1 regularization
    forces the network to adopt sparse parameters; that is, to rely on a smaller set
    of non-null parameters. This can be advantageous if the footprint of the network
    should be minimized (for mobile applications, for example).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œ**L1 æ­£åˆ™åŒ–å™¨**ï¼ˆä¹Ÿå«**LASSO**ï¼ˆ**æœ€å°ç»å¯¹æ”¶ç¼©å’Œé€‰æ‹©ç®—å­**ï¼‰æ­£åˆ™åŒ–å™¨ï¼Œæœ€æ—©ç”±*Fadil Santosa* å’Œ *William
    Symes*åœ¨ *ã€Šå¸¦é™åå°„åœ°éœ‡å›¾çš„çº¿æ€§åæ¼”ã€‹* ä¸­æå‡ºï¼ŒSIAMï¼Œ1986ï¼‰è¿«ä½¿ç½‘ç»œæœ€å°åŒ–å…¶å‚æ•°å€¼çš„ç»å¯¹å€¼ä¹‹å’Œã€‚ä¹ä¸€çœ‹ï¼ŒL1æ­£åˆ™åŒ–å’ŒL2æ­£åˆ™åŒ–çš„åŒºåˆ«å¯èƒ½æ˜¾å¾—å¾®ä¸è¶³é“ï¼Œä½†å®ƒä»¬çš„ç‰¹æ€§å®é™…ä¸Šæ˜¯éå¸¸ä¸åŒçš„ã€‚ç”±äºè¾ƒå¤§çš„æƒé‡ä¸ä¼šå› å¹³æ–¹è€Œå—åˆ°æƒ©ç½šï¼ŒL1æ­£åˆ™åŒ–ä¼šè¿«ä½¿ç½‘ç»œå°†ä¸ä¸é‡è¦ç‰¹å¾ç›¸å…³è”çš„å‚æ•°ç¼©å°åˆ°é›¶ã€‚å› æ­¤ï¼Œå®ƒé€šè¿‡å¼ºåˆ¶ç½‘ç»œå¿½ç•¥è¾ƒä¸é‡è¦çš„ç‰¹å¾ï¼ˆä¾‹å¦‚ä¸æ•°æ®é›†å™ªå£°ç›¸å…³çš„ç‰¹å¾ï¼‰æ¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚æ¢å¥è¯è¯´ï¼ŒL1æ­£åˆ™åŒ–å¼ºè¿«ç½‘ç»œé‡‡ç”¨ç¨€ç–å‚æ•°ï¼Œå³ä¾èµ–äºä¸€å°éƒ¨åˆ†éé›¶å‚æ•°ã€‚å¦‚æœç½‘ç»œçš„å†…å­˜å ç”¨éœ€è¦æœ€å°åŒ–ï¼ˆä¾‹å¦‚ç§»åŠ¨åº”ç”¨ç¨‹åºï¼‰ï¼Œè¿™ä¸€ç‚¹å°¤å…¶æœ‰ä¼˜åŠ¿ã€‚
- en: TensorFlow and Keras implementations
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlowå’ŒKeraså®ç°
- en: To implement those techniques, we should define the regularization loss and
    attach this function to every target layer. At each training iteration, these
    additional losses should be computed over the layers' parameters, and summed with
    the main task-specific loss (for instance, the cross-entropy over the network's
    predictions) so that they can all be backpropagated together by the optimizer.
    Thankfully, TensorFlow 2 provides several tools to simplify this process.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å®ç°è¿™äº›æŠ€æœ¯ï¼Œæˆ‘ä»¬åº”è¯¥å®šä¹‰æ­£åˆ™åŒ–æŸå¤±å¹¶å°†æ­¤å‡½æ•°é™„åŠ åˆ°æ¯ä¸ªç›®æ ‡å±‚ã€‚åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­ï¼Œè¿™äº›é™„åŠ æŸå¤±åº”è¯¥åŸºäºå±‚çš„å‚æ•°è®¡ç®—ï¼Œå¹¶ä¸ä¸»è¦ä»»åŠ¡ç‰¹å®šçš„æŸå¤±ï¼ˆä¾‹å¦‚ç½‘ç»œé¢„æµ‹çš„äº¤å‰ç†µï¼‰æ±‚å’Œï¼Œä»¥ä¾¿å®ƒä»¬å¯ä»¥é€šè¿‡ä¼˜åŒ–å™¨ä¸€èµ·åå‘ä¼ æ’­ã€‚å¹¸è¿çš„æ˜¯ï¼ŒTensorFlow
    2æä¾›äº†å¤šä¸ªå·¥å…·æ¥ç®€åŒ–è¿™ä¸€è¿‡ç¨‹ã€‚
- en: 'Additional losses can be attached to `tf.keras.layers.Layer` and `tf.keras.Model`
    instances through their `.add_loss(losses, ...)`Â method, with theÂ `losses` tensors
    or zero-argument callables returning the loss values. Once properly added to a
    layer (see the following code), these losses will be computed every time the layer/model
    is called. All the losses attached to a `Layer` or `Model` instance, as well as
    the losses attached to its sublayers, will be computed, and the list of loss values
    will be returned when calling the `.losses` property. To better understand this
    concept, we''ll extend the simple convolution layer implemented previouslyÂ to
    add optional regularization to its parameters:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥é€šè¿‡`tf.keras.layers.Layer`å’Œ`tf.keras.Model`å®ä¾‹çš„`.add_loss(losses, ...)`æ–¹æ³•å°†é™„åŠ çš„æŸå¤±æ·»åŠ åˆ°ç½‘ç»œä¸­ï¼Œå…¶ä¸­`losses`æ˜¯è¿”å›æŸå¤±å€¼çš„å¼ é‡æˆ–æ— å‚å¯è°ƒç”¨å¯¹è±¡ã€‚ä¸€æ—¦æ­£ç¡®åœ°æ·»åŠ åˆ°å±‚ï¼ˆå‚è§ä»¥ä¸‹ä»£ç ï¼‰ï¼Œè¿™äº›æŸå¤±å°†åœ¨æ¯æ¬¡è°ƒç”¨å±‚/æ¨¡å‹æ—¶è®¡ç®—ã€‚é™„åŠ åˆ°`Layer`æˆ–`Model`å®ä¾‹çš„æ‰€æœ‰æŸå¤±ï¼Œä»¥åŠé™„åŠ åˆ°å…¶å­å±‚çš„æŸå¤±ï¼Œå°†ä¼šè®¡ç®—ï¼Œå¹¶ä¸”åœ¨è°ƒç”¨`.losses`å±æ€§æ—¶è¿”å›æŸå¤±å€¼åˆ—è¡¨ã€‚ä¸ºäº†æ›´å¥½åœ°ç†è§£è¿™ä¸€æ¦‚å¿µï¼Œæˆ‘ä»¬å°†æ‰©å±•ä¹‹å‰å®ç°çš„ç®€å•å·ç§¯å±‚ï¼Œå‘å…¶å‚æ•°æ·»åŠ å¯é€‰çš„æ­£åˆ™åŒ–ï¼š
- en: '[PRE10]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Regularization losses should guide the models toward learning more robust features.
    They should not take precedence over the main training loss, which is preparing
    the model for its task. Therefore, we should be careful not to put too much weight
    on the regularization losses. Their values are usually dampened by a coefficient
    between 0 and 1 (refer toÂ `coef` in our `l2_reg()` loss function). This weighing
    is especially important, for instance, when the main loss is averaged (for example,
    MSE andÂ MAE). So that the regularization losses do not outweigh it, we should
    either make sure that they are also averaged over the parameters' dimensions,
    or we should decrease their coefficient further.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£åˆ™åŒ–æŸå¤±åº”å¼•å¯¼æ¨¡å‹å­¦ä¹ æ›´å¼ºå¥çš„ç‰¹å¾ã€‚å®ƒä»¬ä¸åº”ä¼˜å…ˆäºä¸»è¦çš„è®­ç»ƒæŸå¤±ï¼Œåè€…æ˜¯ä¸ºäº†è®©æ¨¡å‹é€‚åº”å…¶ä»»åŠ¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åº”è¯¥å°å¿ƒä¸è¦è¿‡åº¦å¼ºè°ƒæ­£åˆ™åŒ–æŸå¤±ã€‚æ­£åˆ™åŒ–æŸå¤±çš„å€¼é€šå¸¸ä¼šé€šè¿‡ä¸€ä¸ªä»‹äº0å’Œ1ä¹‹é—´çš„ç³»æ•°è¿›è¡Œè¡°å‡ï¼ˆå‚è§æˆ‘ä»¬`l2_reg()`æŸå¤±å‡½æ•°ä¸­çš„`coef`ï¼‰ã€‚è¿™ç§åŠ æƒå°¤å…¶é‡è¦ï¼Œä¾‹å¦‚ï¼Œå½“ä¸»è¦æŸå¤±æ˜¯å¹³å‡å€¼æ—¶ï¼ˆä¾‹å¦‚ï¼ŒMSEå’ŒMAEï¼‰ã€‚ä¸ºäº†ä½¿æ­£åˆ™åŒ–æŸå¤±ä¸è‡³äºè¿‡å¤§ï¼Œæˆ‘ä»¬åº”è¯¥ç¡®ä¿å®ƒä»¬ä¹Ÿåœ¨å‚æ•°ç»´åº¦ä¸Šå¹³å‡ï¼Œæˆ–è€…è¿›ä¸€æ­¥å‡å°å®ƒä»¬çš„ç³»æ•°ã€‚
- en: 'At each training iteration of a network composed of such layers, the regularization
    losses can be computed, listed, and added to the main loss as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­ï¼Œå¯¹äºç”±è¿™äº›å±‚ç»„æˆçš„ç½‘ç»œï¼Œæ­£åˆ™åŒ–æŸå¤±å¯ä»¥è¢«è®¡ç®—ã€åˆ—å‡ºå¹¶åŠ å…¥åˆ°ä¸»æŸå¤±ä¸­ï¼Œå…·ä½“å¦‚ä¸‹ï¼š
- en: '[PRE11]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We introduced `.add_loss()`, as this method can greatly simplify the process
    of adding layer-specific losses to custom networks. However, when it comes to
    adding regularization losses, TensorFlow provides a more straightforward solution.
    We can simply pass the regularization loss function as a parameter of the `.add_weight()`Â method
    (also named `.add_variable()`) used to create and attach variables to a `Layer`
    instance. For example, the kernels'' variable could be directly created with the
    regularization loss as follows: `self.kernels = self.add_weight(..., regularizer=self.kernel_regularizer)`.
    At each training iteration, the resulting regularization loss values can still
    be obtained through the layer or model''s `.losses` property.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¼•å…¥äº†`.add_loss()`æ–¹æ³•ï¼Œå› ä¸ºè¯¥æ–¹æ³•å¯ä»¥æå¤§åœ°ç®€åŒ–å°†ç‰¹å®šå±‚æŸå¤±æ·»åŠ åˆ°è‡ªå®šä¹‰ç½‘ç»œä¸­çš„è¿‡ç¨‹ã€‚ç„¶è€Œï¼Œå½“æ¶‰åŠåˆ°æ·»åŠ æ­£åˆ™åŒ–æŸå¤±æ—¶ï¼ŒTensorFlowæä¾›äº†ä¸€ä¸ªæ›´ç›´æ¥çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬åªéœ€å°†æ­£åˆ™åŒ–æŸå¤±å‡½æ•°ä½œä¸º`.add_weight()`æ–¹æ³•ï¼ˆä¹Ÿç§°ä¸º`.add_variable()`ï¼‰çš„å‚æ•°ï¼Œè¯¥æ–¹æ³•ç”¨äºåˆ›å»ºå¹¶é™„åŠ å˜é‡åˆ°`Layer`å®ä¾‹ã€‚ä¾‹å¦‚ï¼Œå·ç§¯æ ¸çš„å˜é‡å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ç›´æ¥åˆ›å»ºï¼Œå¹¶é™„åŠ æ­£åˆ™åŒ–æŸå¤±ï¼š`self.kernels
    = self.add_weight(..., regularizer=self.kernel_regularizer)`ã€‚åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­ï¼Œå¾—åˆ°çš„æ­£åˆ™åŒ–æŸå¤±å€¼ä»ç„¶å¯ä»¥é€šè¿‡è¯¥å±‚æˆ–æ¨¡å‹çš„`.losses`å±æ€§è·å¾—ã€‚
- en: 'When using predefined Keras layers, we do not need to bother extending the
    classes to add regularization terms. These layers can receive regularizers for
    their variables as parameters. Keras even explicitly defines some regularizer
    callables in its `tf.keras.regularizers` module. Finally, when using Keras training
    operations (such asÂ `model.fit(...)`), Keras automatically takes into account
    additional `model.losses` (that is, the regularization terms and other possible
    layer-specific losses), as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½¿ç”¨é¢„å®šä¹‰çš„Keraså±‚æ—¶ï¼Œæˆ‘ä»¬æ— éœ€æ‰©å±•ç±»æ¥æ·»åŠ æ­£åˆ™åŒ–é¡¹ã€‚è¿™äº›å±‚å¯ä»¥é€šè¿‡å‚æ•°æ¥æ”¶æ­£åˆ™åŒ–å™¨ã€‚Kerasç”šè‡³åœ¨å…¶`tf.keras.regularizers`æ¨¡å—ä¸­æ˜¾å¼å®šä¹‰äº†ä¸€äº›æ­£åˆ™åŒ–å™¨å¯è°ƒç”¨å‡½æ•°ã€‚æœ€åï¼Œåœ¨ä½¿ç”¨Kerasè®­ç»ƒæ“ä½œï¼ˆå¦‚`model.fit(...)`ï¼‰æ—¶ï¼ŒKerasä¼šè‡ªåŠ¨è€ƒè™‘é¢å¤–çš„`model.losses`ï¼ˆå³æ­£åˆ™åŒ–é¡¹å’Œå…¶ä»–å¯èƒ½çš„ç‰¹å®šå±‚æŸå¤±ï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE12]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Dropout
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: So far, the regularization methods we have covered are affecting the way networks
    are trained. Other solutions are affecting their architecture. **Dropout** is
    one such method and one of the most popular regularization tricks.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä»‹ç»çš„æ­£åˆ™åŒ–æ–¹æ³•ä¸»è¦å½±å“ç½‘ç»œçš„è®­ç»ƒæ–¹å¼ã€‚å…¶ä»–ä¸€äº›è§£å†³æ–¹æ¡ˆåˆ™å½±å“ç½‘ç»œçš„æ¶æ„ã€‚**Dropout**ï¼ˆä¸¢å¼ƒæ³•ï¼‰å°±æ˜¯å…¶ä¸­ä¸€ç§æ–¹æ³•ï¼Œä¹Ÿæ˜¯æœ€æµè¡Œçš„æ­£åˆ™åŒ–æŠ€å·§ä¹‹ä¸€ã€‚
- en: Definition
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰
- en: 'Introduced in *Dropout: A Simple Way to Prevent Neural Networks from OverfittingÂ *(*JMLR,
    2014*) by Hinton and his team (who made numerous contributions to deep learning),
    *dropout* consists of randomly disconnecting (*dropping out*) some neurons of
    target layers at every training iteration. This method thus takes a hyperparameter
    ratio,Â ![](img/9860fd1e-4b64-4956-b2f4-e190e386e1d9.png), which represents the
    probability that neurons are being turned off at each training step (usually set
    between 0.1 and 0.5). The concept is illustrated inÂ *Figure 3.13*:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨*Dropout: A Simple Way to Prevent Neural Networks from Overfitting*ï¼ˆ*JMLR,
    2014*ï¼‰ä¸­ï¼ŒHintonåŠå…¶å›¢é˜Ÿï¼ˆä»–ä»¬ä¸ºæ·±åº¦å­¦ä¹ åšå‡ºäº†è®¸å¤šè´¡çŒ®ï¼‰é¦–æ¬¡å¼•å…¥äº†*dropout*æ–¹æ³•ï¼Œ*dropout*é€šè¿‡åœ¨æ¯æ¬¡è®­ç»ƒè¿­ä»£ä¸­éšæœºæ–­å¼€ç›®æ ‡å±‚çš„ä¸€äº›ç¥ç»å…ƒï¼ˆå³â€œä¸¢å¼ƒâ€ï¼‰æ¥å®ç°ã€‚è¿™ç§æ–¹æ³•å› æ­¤éœ€è¦ä¸€ä¸ªè¶…å‚æ•°æ¯”ç‡ï¼Œ
    ![](img/9860fd1e-4b64-4956-b2f4-e190e386e1d9.png)ï¼Œè¯¥æ¯”ç‡è¡¨ç¤ºæ¯æ¬¡è®­ç»ƒæ­¥éª¤ä¸­ç¥ç»å…ƒè¢«å…³é—­çš„æ¦‚ç‡ï¼ˆé€šå¸¸è®¾å®šåœ¨0.1åˆ°0.5ä¹‹é—´ï¼‰ã€‚è¯¥æ¦‚å¿µåœ¨*å›¾3.13*ä¸­æœ‰æ‰€å±•ç¤ºï¼š'
- en: '![](img/3c40f227-304e-4293-a27c-15d56a959f64.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c40f227-304e-4293-a27c-15d56a959f64.png)'
- en: 'Figure 3.13: Dropout represented on a simple neural network (note that dropped-out
    neurons of layers are randomly chosen in each iteration)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3.13ï¼šåœ¨ç®€å•ç¥ç»ç½‘ç»œä¸­è¡¨ç¤ºçš„Dropoutï¼ˆæ³¨æ„ï¼Œæ¯æ¬¡è¿­ä»£æ—¶ï¼Œä¸¢å¼ƒçš„å±‚ç¥ç»å…ƒæ˜¯éšæœºé€‰æ‹©çš„ï¼‰
- en: By artificially and randomly impairing the network, this method forces the learning
    of robust and concurrent features. For instance, as dropout may deactivate the
    neurons responsible for a key feature, the network has to figure out other significant
    features in order to reach the same prediction. This has the effect of developing
    redundant representations of data for prediction.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡äººä¸ºä¸”éšæœºåœ°å‰Šå¼±ç½‘ç»œï¼Œè¿™ç§æ–¹æ³•è¿«ä½¿ç½‘ç»œå­¦ä¹ åˆ°æ›´åŠ é²æ£’ä¸”å¹¶è¡Œçš„ç‰¹å¾ã€‚ä¾‹å¦‚ï¼Œç”±äºdropoutå¯èƒ½ä¼šåœç”¨è´Ÿè´£æŸä¸ªå…³é”®ç‰¹å¾çš„ç¥ç»å…ƒï¼Œç½‘ç»œå¿…é¡»æ‰¾åˆ°å…¶ä»–é‡è¦ç‰¹å¾æ¥è¾¾åˆ°ç›¸åŒçš„é¢„æµ‹ç»“æœã€‚è¿™æ ·å°±èƒ½ä¿ƒä½¿ç½‘ç»œå‘å±•å‡ºå†—ä½™çš„ç‰¹å¾è¡¨ç¤ºï¼Œç”¨äºé¢„æµ‹ã€‚
- en: Dropout is also often explained as a cheap solution to simultaneously train
    a *multitude* of models (the randomly impaired versions of the original network).
    During the testing phase, dropout is not applied to the network, so the network's
    predictions can be seen as the combination of the results that the partial models
    would have provided. Therefore, this information averaging prevents the network
    from overfitting.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¢å¼ƒæ³•ä¹Ÿå¸¸è¢«è§£é‡Šä¸ºä¸€ç§å»‰ä»·çš„è§£å†³æ–¹æ¡ˆï¼Œå¯ä»¥åŒæ—¶è®­ç»ƒ*å¤šä¸ª*æ¨¡å‹ï¼ˆåŸå§‹ç½‘ç»œçš„éšæœºå¤±æ•ˆç‰ˆæœ¬ï¼‰ã€‚åœ¨æµ‹è¯•é˜¶æ®µï¼Œä¸¢å¼ƒæ³•ä¸ä¼šåº”ç”¨äºç½‘ç»œï¼Œå› æ­¤ç½‘ç»œçš„é¢„æµ‹å¯ä»¥çœ‹ä½œæ˜¯å„ä¸ªéƒ¨åˆ†æ¨¡å‹ç»“æœçš„ç»“åˆã€‚å› æ­¤ï¼Œè¿™ç§ä¿¡æ¯å¹³å‡å¯ä»¥é˜²æ­¢ç½‘ç»œè¿‡æ‹Ÿåˆã€‚
- en: TensorFlow and Keras methods
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlowå’ŒKerasæ–¹æ³•
- en: Dropout can be called as a function through `tf.nn.dropout(x, rate, ...)` (refer
    to the documentation at [https://www.tensorflow.org/api_docs/python/tf/nn/dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout))
    to directly obtain a tensor with values randomly dropped, or as a layer through
    `tf.keras.layers.Dropout()` (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/layers/dropout](https://www.tensorflow.org/api_docs/python/tf/layers/dropout)),
    which can be added to neural models. By default, `tf.keras.layers.Dropout()` is
    only applied during training (when the layer/model is called with the `training=True`
    parameter) and is deactivated otherwise (forwarding the values without any alteration).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¢å¼ƒæ³•å¯ä»¥é€šè¿‡å‡½æ•°`tf.nn.dropout(x, rate, ...)`è°ƒç”¨ï¼ˆè¯·å‚é˜…[https://www.tensorflow.org/api_docs/python/tf/nn/dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)ï¼‰ç›´æ¥è·å¾—ä¸€ä¸ªå€¼éšæœºä¸¢å¼ƒçš„å¼ é‡ï¼Œæˆ–è€…é€šè¿‡`tf.keras.layers.Dropout()`ä½œä¸ºå±‚è°ƒç”¨ï¼ˆè¯·å‚é˜…[https://www.tensorflow.org/api_docs/python/tf/layers/dropout](https://www.tensorflow.org/api_docs/python/tf/layers/dropout)ï¼‰ï¼Œå¯ä»¥å°†å…¶æ·»åŠ åˆ°ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œ`tf.keras.layers.Dropout()`ä»…åœ¨è®­ç»ƒæ—¶åº”ç”¨ï¼ˆå½“å±‚/æ¨¡å‹è¢«è°ƒç”¨æ—¶ï¼Œå¸¦æœ‰`training=True`å‚æ•°ï¼‰ï¼Œå¦åˆ™ä¼šè¢«ç¦ç”¨ï¼ˆè½¬å‘æœªç»ä¿®æ”¹çš„å€¼ï¼‰ã€‚
- en: 'Dropout layers should be added directly after layers we want to prevent from
    overfitting (as dropout layers will randomly drop values returned by their preceding
    layers, forcing them to adapt). For instance, you can apply dropout (for example,
    with a ratio,Â ![](img/0d8d3c23-bc8e-4837-b207-6ab366cf4c6d.png)) to a fully connected
    layer in Keras, as shown in the following code block:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¢å¼ƒå±‚åº”è¯¥ç›´æ¥æ·»åŠ åˆ°æˆ‘ä»¬å¸Œæœ›é˜²æ­¢è¿‡æ‹Ÿåˆçš„å±‚åé¢ï¼ˆå› ä¸ºä¸¢å¼ƒå±‚ä¼šéšæœºä¸¢å¼ƒå‰ä¸€å±‚è¿”å›çš„å€¼ï¼Œè¿«ä½¿å…¶è¿›è¡Œé€‚åº”ï¼‰ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥åœ¨Kerasä¸­å¯¹å…¨è¿æ¥å±‚åº”ç”¨ä¸¢å¼ƒæ³•ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ä¸€ä¸ªæ¯”ç‡ï¼Œ![](img/0d8d3c23-bc8e-4837-b207-6ab366cf4c6d.png)ï¼‰ï¼Œå¦‚ä¸‹é¢çš„ä»£ç å—æ‰€ç¤ºï¼š
- en: '[PRE13]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Batch normalization
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‰¹é‡å½’ä¸€åŒ–
- en: Though our list is not exhaustive, we will introduce a final common regularization
    method, which is also directly integrated into the networks' architectures.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡æˆ‘ä»¬çš„åˆ—è¡¨å¹¶ä¸è¯¦å°½ï¼Œä½†æˆ‘ä»¬å°†ä»‹ç»ä¸€ç§å¸¸è§çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•ä¹Ÿè¢«ç›´æ¥é›†æˆåˆ°ç½‘ç»œçš„æ¶æ„ä¸­ã€‚
- en: Definition
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®šä¹‰
- en: 'Like dropout, **batch normalization** (proposed by Sergey Ioffe and Christian
    Szegedy in *Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift*, *JMLR, 2015*) is an operation that can be inserted
    into neural networks and affects their training. This operation takes the batched
    results of the preceding layers and *normalizes* them; that is, it subtracts the
    batch mean and divides it by the batch standard deviation.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºä¸¢å¼ƒæ³•ï¼Œ**æ‰¹é‡å½’ä¸€åŒ–**ï¼ˆç”±Sergey Ioffeå’ŒChristian Szegedyåœ¨ã€Šæ‰¹é‡å½’ä¸€åŒ–ï¼šé€šè¿‡å‡å°‘å†…éƒ¨åæ–¹å·®åç§»åŠ é€Ÿæ·±åº¦ç½‘ç»œè®­ç»ƒã€‹ä¸€æ–‡ä¸­æå‡ºï¼Œ*JMLR,
    2015*ï¼‰æ˜¯ä¸€ç§å¯ä»¥æ’å…¥ç¥ç»ç½‘ç»œå¹¶å½±å“å…¶è®­ç»ƒçš„æ“ä½œã€‚è¯¥æ“ä½œè·å–å‰ä¸€å±‚çš„æ‰¹é‡ç»“æœå¹¶è¿›è¡Œ*å½’ä¸€åŒ–*å¤„ç†ï¼›å³ï¼Œå‡å»æ‰¹é‡å‡å€¼å¹¶é™¤ä»¥æ‰¹é‡æ ‡å‡†å·®ã€‚
- en: Since batches are randomly sampled in SGD (and thus are rarely the same twice),
    this means that the data will almost never be normalized the same way. Therefore,
    the network has to learn how to deal with these data fluctuations, making it more
    robust and generic. Furthermore, this normalization step concomitantly improves
    the way the gradients flow through the network, facilitating the SGD process.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºåœ¨SGDä¸­æ‰¹æ¬¡æ˜¯éšæœºé‡‡æ ·çš„ï¼ˆå› æ­¤å¾ˆå°‘æœ‰ä¸¤ä¸ªæ‰¹æ¬¡å®Œå…¨ç›¸åŒï¼‰ï¼Œè¿™æ„å‘³ç€æ•°æ®å‡ ä¹æ°¸è¿œä¸ä¼šä»¥ç›¸åŒçš„æ–¹å¼è¿›è¡Œå½’ä¸€åŒ–ã€‚å› æ­¤ï¼Œç½‘ç»œå¿…é¡»å­¦ä¹ å¦‚ä½•å¤„ç†è¿™äº›æ•°æ®æ³¢åŠ¨ï¼Œä½¿å…¶å˜å¾—æ›´åŠ å¥å£®å’Œé€šç”¨ã€‚æ­¤å¤–ï¼Œè¿™ä¸€æ­¥å½’ä¸€åŒ–åŒæ—¶æ”¹å–„äº†æ¢¯åº¦åœ¨ç½‘ç»œä¸­çš„æµåŠ¨æ–¹å¼ï¼Œä¿ƒè¿›äº†SGDè¿‡ç¨‹ã€‚
- en: The behavior of batch normalization layers is actually a bit more complex than
    what we have succinctly presented. These layers have a couple of trainable parameters
    that are used in denormalization operations, so that the next layer does not just
    try to learn how to undo the batch normalization.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡å½’ä¸€åŒ–å±‚çš„è¡Œä¸ºå®é™…ä¸Šæ¯”æˆ‘ä»¬ç®€æ´åœ°å‘ˆç°çš„è¦å¤æ‚ã€‚è¿™äº›å±‚æœ‰ä¸€äº›å¯è®­ç»ƒçš„å‚æ•°ï¼Œç”¨äºå»å½’ä¸€åŒ–æ“ä½œï¼Œä»¥ä¾¿ä¸‹ä¸€å±‚ä¸ä¼šä»…ä»…è¯•å›¾å­¦ä¹ å¦‚ä½•æ’¤é”€æ‰¹é‡å½’ä¸€åŒ–ã€‚
- en: TensorFlow and Keras methods
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlowå’ŒKerasæ–¹æ³•
- en: Similar to dropout, batch normalization is available in TensorFlow both as a
    function,Â `tf.nn.batch_normalization()` (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization))
    and as a layer, `tf.keras.layers.BatchNormalization()` (refer to the documentation
    at [https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)),
    making it straightforward to include this regularization tool inside networks.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äº dropoutï¼Œæ‰¹é‡å½’ä¸€åŒ–åœ¨ TensorFlow ä¸­æ—¢å¯ä»¥ä½œä¸ºå‡½æ•° `tf.nn.batch_normalization()` ä½¿ç”¨ï¼ˆè¯·å‚é˜…[https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization)ä¸­çš„æ–‡æ¡£ï¼‰ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºå±‚
    `tf.keras.layers.BatchNormalization()` ä½¿ç”¨ï¼ˆè¯·å‚é˜…[https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization)ä¸­çš„æ–‡æ¡£ï¼‰ï¼Œè¿™ä½¿å¾—å°†è¿™ä¸€æ­£åˆ™åŒ–å·¥å…·è½»æ¾åœ°é›†æˆåˆ°ç½‘ç»œä¸­å˜å¾—æ›´åŠ ç®€å•ã€‚
- en: All these various optimization techniques are precious tools for deep learning,
    especially when training CNNs on imbalanced or scarce datasets, which is often
    the case for custom applications (as elaborated on in [Chapter 7](337ec077-c215-4782-b56c-beae4d94d718.xhtml),
    *Training on Complex and Scarce Datasets*).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰è¿™äº›ä¸åŒçš„ä¼˜åŒ–æŠ€æœ¯éƒ½æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„å®è´µå·¥å…·ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†ä¸å¹³è¡¡æˆ–ç¨€ç¼ºæ•°æ®é›†æ—¶è®­ç»ƒ CNN æ—¶ï¼Œè¿™ç§æƒ…å†µåœ¨å®šåˆ¶åº”ç”¨ä¸­ç»å¸¸å‘ç”Ÿï¼ˆå¦‚[ç¬¬ 7 ç« ](337ec077-c215-4782-b56c-beae4d94d718.xhtml)ã€Šåœ¨å¤æ‚å’Œç¨€ç¼ºæ•°æ®é›†ä¸Šçš„è®­ç»ƒã€‹ä¸­è¯¦ç»†é˜è¿°ï¼‰ã€‚
- en: Similar to the Jupyter notebook for the optimizers study, we provide another
    notebook demonstrating how these regularization methods can be applied, and how
    they affect the performance of our simple CNN.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¼˜åŒ–å™¨å­¦ä¹ çš„ Jupyter ç¬”è®°æœ¬ç±»ä¼¼ï¼Œæˆ‘ä»¬æä¾›äº†å¦ä¸€ä¸ªç¬”è®°æœ¬ï¼Œå±•ç¤ºäº†å¦‚ä½•åº”ç”¨è¿™äº›æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•å½±å“æˆ‘ä»¬ç®€å• CNN çš„æ€§èƒ½ã€‚
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: With the help of TensorFlow and Keras, we caught up with years of research in
    deep learning. As CNNs have become central to modern computer vision (and machine
    learning in general), it is essential to understand how they perform, and what
    kinds of layers they are composed of. As presented in this chapter, TensorFlow
    and Keras provide clear interfaces to efficiently build such networks. They are
    also implementing several advanced optimization and regularization techniques
    (such as various optimizers, L1/L2 regularization, dropout, and batch normalization)
    to improve the performance and robustness of trained models, which is important
    to keep in mind for any application.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ TensorFlow å’Œ Keras çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬èµ¶ä¸Šäº†æ·±åº¦å­¦ä¹ é¢†åŸŸå¤šå¹´çš„ç ”ç©¶è¿›å±•ã€‚ç”±äº CNN å·²æˆä¸ºç°ä»£è®¡ç®—æœºè§†è§‰ï¼ˆä»¥åŠæœºå™¨å­¦ä¹ ä¸€èˆ¬ï¼‰çš„æ ¸å¿ƒï¼Œäº†è§£å®ƒä»¬çš„æ€§èƒ½ä»¥åŠå®ƒä»¬ç”±å“ªäº›å±‚ç»„æˆæ˜¯è‡³å…³é‡è¦çš„ã€‚æ­£å¦‚æœ¬ç« æ‰€å±•ç¤ºçš„ï¼ŒTensorFlow
    å’Œ Keras æä¾›äº†æ¸…æ™°çš„æ¥å£ï¼Œå¯ä»¥é«˜æ•ˆåœ°æ„å»ºè¿™æ ·çš„ç½‘ç»œã€‚å®ƒä»¬è¿˜å®ç°äº†å¤šç§å…ˆè¿›çš„ä¼˜åŒ–å’Œæ­£åˆ™åŒ–æŠ€æœ¯ï¼ˆä¾‹å¦‚å„ç§ä¼˜åŒ–å™¨ã€L1/L2 æ­£åˆ™åŒ–ã€dropout å’Œæ‰¹é‡å½’ä¸€åŒ–ï¼‰ï¼Œä»¥æé«˜è®­ç»ƒæ¨¡å‹çš„æ€§èƒ½å’Œé²æ£’æ€§ï¼Œè¿™å¯¹äºä»»ä½•åº”ç”¨éƒ½æ˜¯éå¸¸é‡è¦çš„ã€‚
- en: We now have the tools to finally tackle more challenging computer vision tasks.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨æ‹¥æœ‰äº†æœ€ç»ˆåº”å¯¹æ›´å…·æŒ‘æˆ˜æ€§çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡çš„å·¥å…·ã€‚
- en: In the next chapter, we will therefore present several CNN architectures applied
    to the task of classifying large picture datasets.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å‡ ç§åº”ç”¨äºå¤§è§„æ¨¡å›¾åƒæ•°æ®é›†åˆ†ç±»ä»»åŠ¡çš„ CNN æ¶æ„ã€‚
- en: Questions
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é—®é¢˜
- en: Why does the output of a convolutional layer have a smaller width and height
    than the input, unless it is padded?
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆå·ç§¯å±‚çš„è¾“å‡ºå®½åº¦å’Œé«˜åº¦æ¯”è¾“å…¥å°ï¼Œé™¤éè¿›è¡Œäº†å¡«å……ï¼Ÿ
- en: What would be the output of a max-pooling layer with a receptive field of (2,
    2) and stride of 2 on the input matrix in *Figure 3.6*?
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªå¤§å°ä¸º (2, 2)ï¼Œæ­¥å¹…ä¸º 2 çš„æœ€å¤§æ± åŒ–å±‚ï¼Œä½œç”¨äº*å›¾ 3.6*ä¸­çš„è¾“å…¥çŸ©é˜µï¼Œè¾“å‡ºä¼šæ˜¯ä»€ä¹ˆï¼Ÿ
- en: How could LeNet-5 be implemented using the Keras functional API in a non-object-oriented
    manner?
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å¦‚ä½•ä½¿ç”¨ Keras å‡½æ•°å¼ API ä»¥éé¢å‘å¯¹è±¡çš„æ–¹å¼å®ç° LeNet-5ï¼Ÿ
- en: How does L1/L2 regularization affect networks?
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: L1/L2 æ­£åˆ™åŒ–å¦‚ä½•å½±å“ç½‘ç»œï¼Ÿ
- en: Further reading
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: è¿›ä¸€æ­¥é˜…è¯»
- en: '*On the importance of initialization and momentum in deep learning* ([http://proceedings.mlr.press/v28/sutskever13.pdf](http://proceedings.mlr.press/v28/sutskever13.pdf)),
    by Ilya Sutskever et al. This often-referenced conference paper, published in
    2013, presents and compares the momentum and NAG algorithms.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å…³äºæ·±åº¦å­¦ä¹ ä¸­åˆå§‹åŒ–å’ŒåŠ¨é‡çš„é‡è¦æ€§*ï¼ˆ[http://proceedings.mlr.press/v28/sutskever13.pdf](http://proceedings.mlr.press/v28/sutskever13.pdf)ï¼‰ï¼ŒIlya
    Sutskever ç­‰äººæ’°å†™ã€‚è¯¥ç¯‡ç»å¸¸è¢«å¼•ç”¨çš„ä¼šè®®è®ºæ–‡äº 2013 å¹´å‘å¸ƒï¼Œæå‡ºå¹¶æ¯”è¾ƒäº†åŠ¨é‡å’Œ NAG ç®—æ³•ã€‚'
- en: '*Dropout: A Simple Way to Prevent Neural Networks from Overfitting* ([http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)),
    by Nitish Srivastava et al. This other conference paper, published in 2014, introduced
    dropout. It is a great read for those who want to know more about this method
    and see it applied to several famous computer vision datasets.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Dropout: é˜²æ­¢ç¥ç»ç½‘ç»œè¿‡æ‹Ÿåˆçš„ç®€å•æ–¹æ³•* ([http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer))ï¼Œä½œè€…ï¼šNitish
    Srivastava ç­‰äººã€‚è¯¥ç¯‡2014å¹´å‘å¸ƒçš„ä¼šè®®è®ºæ–‡ä»‹ç»äº† dropout æ–¹æ³•ã€‚å¯¹äºé‚£äº›æƒ³æ·±å…¥äº†è§£è¿™ä¸€æ–¹æ³•å¹¶çœ‹åˆ°å…¶åœ¨å¤šä¸ªè‘—åè®¡ç®—æœºè§†è§‰æ•°æ®é›†ä¸­çš„åº”ç”¨çš„è¯»è€…æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ç¯‡å€¼å¾—ä¸€è¯»çš„å¥½æ–‡ç« ã€‚'
