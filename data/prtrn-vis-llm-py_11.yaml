- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Detecting, Mitigating, and Monitoring Bias
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别、缓解和监控偏见
- en: In this chapter, we’ll analyze leading bias identification and mitigation strategies
    for large vision, language, and multimodal models. You’ll learn about the concept
    of bias, both in a statistical sense and how it impacts human beings in critical
    ways. You’ll understand key ways to quantify and remedy this in vision and language
    models, eventually landing on monitoring strategies that enable you to reduce
    any and all forms of harm when applying your **machine learning** (**ML**) models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将分析大规模视觉、语言和多模态模型的主要偏见识别与缓解策略。你将了解偏见的概念，包括统计学意义上的偏见，以及它如何在关键方面影响人类。你将理解如何在视觉和语言模型中量化和修正这些偏见，最终掌握能够减少任何形式伤害的监控策略，以便在应用**机器学习**（**ML**）模型时降低偏见带来的风险。
- en: 'We will cover the following topics in the chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Detecting bias in ML models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别 ML 模型中的偏见
- en: Mitigating bias in vision and language models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓解视觉和语言模型中的偏见
- en: Monitoring bias in ML models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控 ML 模型中的偏见
- en: Detecting, mitigating, and monitoring bias with SageMaker Clarify
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SageMaker Clarify 识别、缓解和监控偏见
- en: Detecting bias in ML models
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别 ML 模型中的偏见
- en: At this point in the book, we’ve covered many of the useful, interesting, and
    impressive aspects of large vision and language models. Hopefully, some of my
    passion for this space has started rubbing off on you, and you’re beginning to
    realize why this is as much of an art as it is a science. Creating cutting-edge
    ML models takes courage. Risk is inherently part of the process; you hope a given
    avenue will pay off, but until you’ve followed the track all the way to the end,
    you can’t be positive. Study helps, as does discussion with experts to try to
    validate your designs ahead of time, but personal experience ends up being the
    most successful tool in your toolbelt.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，本书已经涵盖了大规模视觉和语言模型中许多有用、有趣和令人印象深刻的方面。希望我对这一领域的热情已经开始影响到你，你也开始意识到这不仅是一门科学，还是一门艺术。创建最前沿的ML模型需要勇气。风险是过程的一部分；你希望某条路径能够带来回报，但直到你沿着这条路走到最后，你无法确定。研究有所帮助，和专家讨论可以提前验证你的设计，但个人经验最终成为你工具箱中最有效的工具。
- en: 'This entire chapter is dedicated to possibly the most significant Achilles
    heel in ML and **artificial intelligence** (**AI**): **bias**. Notably, here we
    are most interested in bias toward and against specific groups of human beings.
    You’ve probably already heard about **statistical bias**, an undesirable scenario
    where a given model has a statistical preference for part of the dataset, and
    thereby naturally against another part. This is an inevitable stage of every data
    science project: you need to carefully consider which datasets you are using and
    grapple with how that represents the world to your model. If you are over- or
    under-representing any facets of your datasets, this will invariably impact your
    model’s behavior. In the previous chapter, we looked at an example of credit card
    fraud and began to understand how the simple act of extracting and constructing
    your dataset can lead you in completely the wrong direction. Now, we’ll follow
    a similar exercise but focus on people.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的全部内容都集中在可能是 ML 和**人工智能**（**AI**）中最重要的致命弱点——**偏见**。特别地，我们关注的是针对特定群体的人类偏见。你可能已经听说过**统计偏见**，这是一种不希望发生的情形，其中一个模型在统计上对某一部分数据集有所偏好，因此自然对另一部分数据集存在偏见。这是每个数据科学项目中不可避免的阶段：你需要仔细考虑所使用的数据集，并思考这些数据如何向你的模型展示世界。如果你的数据集某些方面的表现过多或过少，这无疑会影响你的模型行为。在上一章中，我们探讨了信用卡欺诈的例子，开始理解提取和构建数据集的简单行为如何将你引入完全错误的方向。现在，我们将进行类似的练习，但这次重点关注人类群体。
- en: When ML and data science started becoming popular in business leadership circles,
    as with any new phenomena, there were naturally a few misunderstandings. A primary
    one of these in terms of ML was the mistaken belief that computers would naturally
    have fewer biased preferences than people. More than a few projects were inspired
    by this falsity. From hiring to performance evaluations, credit applications to
    background checks, and even for sentencing in the criminal justice system, countless
    data science projects were started with an intention of reducing biased outcomes.
    What these projects failed to realize is that *every dataset is limited by historical
    records*. When we train ML models on these records naively, we necessarily introduce
    those same limitations on the output space of the model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习和数据科学开始在商业领导圈中流行时，和任何新现象一样，自然会有一些误解。关于机器学习的一个主要误解就是错误地认为计算机会比人类自然地有更少的偏见。这种虚假信念激发了不少项目。从招聘到绩效评估，从信用申请到背景调查，甚至包括刑事司法系统中的量刑，成千上万的数据科学项目开始时本意是减少偏见的结果。这些项目未能意识到的是，*每一个数据集都受到历史记录的限制*。当我们天真地用这些记录来训练机器学习模型时，我们必然会把这些限制引入到模型的输出空间中。
- en: This means that records from criminal justice to human resources, financial
    services, and imaging systems when naively used to train ML models codified that
    bias and presented it in a digital format. When used at scale—for example, to
    make hundreds to thousands of digital decisions—this actually increases the scale
    of biased decision-making rather than reduces it. Classic examples of this include
    large-scale image classification systems failing to detect African Americans *(1)*
    or resume screening systems that developed a bias against anything female *(2)*.
    While all of these organizations immediately took action to right their wrongs,
    the overall problem was still shockingly public for the entire world to watch.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，从刑事司法到人力资源、金融服务和成像系统的记录，在未经深思熟虑的情况下用于训练机器学习模型时，固化了这些偏见，并以数字格式呈现。当这些数据被大规模使用时——例如，用于做出成百上千个数字决策——这实际上是增加了偏见决策的规模，而不是减少它。经典的例子包括大规模图像分类系统未能检测到非洲裔美国人*(1)*，或者简历筛选系统对任何女性产生了偏见*(2)*。虽然所有这些组织立即采取了行动纠正错误，但整体问题仍然令全世界感到震惊。
- en: Detecting bias in large vision and language models
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测大型视觉和语言模型中的偏见
- en: As you may already suspect, large models trained on massive datasets from internet
    crawls are ripe with bias. This includes everything from types of people more
    likely to produce or not to produce content on the internet, to languages and
    styles, topics, the accuracy of the content, depth of analysis, personalities,
    backgrounds, histories, interests, professions, educational levels, and more.
    It includes visual representations of other people, modes, cultural styles, places,
    events, perspectives, objects, sexuality, preferences, religion—the list goes
    on and on.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经猜到的那样，基于从互联网爬虫获得的大量数据集训练的大型模型充满了偏见。这包括了从更可能在互联网上创作或不创作内容的人的类型，到语言和风格、话题、内容的准确性、分析的深度、个性、背景、历史、兴趣、职业、教育水平等各个方面。它还包括其他人的视觉表现、模式、文化风格、地点、事件、视角、物体、性取向、偏好、宗教——这个列表还可以继续列下去。
- en: 'In most projects, to use a phrase from Amazon, I find it helpful to *work backward*
    from my final application. Here, that refers to some large vision-language model—for
    example, **Stable Diffusion**. Then, I’ll ask myself: *Who* is likely to use this
    model, and how? Try to write down a list of types of people you think might use
    your model; in a bias context, you need to push yourself to think outside of your
    comfort zone. This is another place where having diverse teams is incredibly useful;
    ideally, ask someone with a background different from yours about their perspective.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数项目中，借用亚马逊的一句话，我发现从最终应用程序“倒推”是很有帮助的。在这里，这指的是一些大型视觉-语言模型——例如，**Stable Diffusion**。然后，我会问自己：*谁*可能会使用这个模型，怎么使用？试着写下你认为可能使用你模型的人的类型；在偏见的语境下，你需要迫使自己跳出舒适区。这是一个多元化团队极为有用的地方；理想情况下，向一个与你背景不同的人询问他们的视角。
- en: 'Once you have a target list of types of people who might use your model, think
    to yourself: Are these people represented in my dataset? How are they represented?
    Are they represented in a full spectrum of different emotional states and outcomes,
    or are they only represented in a tiny slice of humanity? If my dataset were the
    sole input to a computational process designed to learn patterns—that is, an ML
    algorithm—would many people from this group consider it a fair and accurate representation?
    Or, would they get angry at me and say: That’s so biased!?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了可能使用你模型的目标人群列表，思考一下：这些人在我的数据集中有体现吗？他们是如何被体现的？他们是否在不同的情绪状态和结果的全谱中得到了体现，还是仅仅在一小部分人群中得到了体现？如果我的数据集是用于学习模式的计算过程的唯一输入——也就是机器学习算法——这些人群中的许多人会认为这是一个公平和准确的体现吗？或者，他们会生气并说：“这太偏见了！”
- en: You can get started with even one or two groups of people, and usually, you
    want to think about certain scenarios where you know there’s a big gap in your
    dataset. For me, I tend to look right at gender and employment. You can also look
    at religion, race, socioeconomic status, sexuality, age, and so on. Try to stretch
    yourself and find an intersection. An intersection would be a place where someone
    from this group is likely to be, or not to be, within another category. This second
    category can be employment, education, family life, ownership of specific objects,
    accomplishments, criminal history, medical status, and so on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从一两组人开始，通常你会考虑某些场景，在这些场景中，你知道数据集中存在一个大的差距。对我来说，我倾向于直接关注性别和就业。你也可以关注宗教、种族、社会经济地位、性取向、年龄等等。尽量扩展自己的思维，找到一个交集。交集指的是某个群体中的人可能会或不会出现在另一个类别中。这个第二类别可以是就业、教育、家庭生活、拥有特定物品、成就、犯罪历史、健康状况等。
- en: A model is biased when it exhibits a clear “preference”, or measurable habit,
    of placing or not placing certain types of people in certain types of groups.
    Bias shows up when your model empirically places or does not place people from
    one of your *A* categories into one of your *B* categories.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型表现出明显的“偏好”或可衡量的习惯，把某些类型的人归入或不归入某些类型的群体时，模型就是有偏见的。当你的模型在经验上把某一类人（*A*类）放入或不放入另一类人（*B*类）时，偏见就会显现。
- en: 'For example, let’s say you’re using a text generation model in the GPT family.
    You might send your GPT-based model a prompt such as, “Akanksha works really hard
    as a …”. A biased model might fill in the blank as “nurse”, “secretary”, “homemaker”,
    “wife”, or “mother”. An unbiased model might fill in the blank as “doctor”, “lawyer”,
    “scientist”, “banker”, “author”, or “entrepreneur”. Imagine using that biased
    model for a resume-screening classifier, an employment chat helpline, or a curriculum-planning
    assistant. It would unwittingly, but very measurably, continue to make subtle
    recommendations against certain careers for women! Let’s look at a few more examples
    in the context of language:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设你在使用 GPT 系列的文本生成模型。你可能会给你的 GPT 模型发送一个提示，比如：“Akanksha 非常努力地工作作为一名……”。一个有偏见的模型可能会把空白处填充为“护士”、“秘书”、“家庭主妇”、“妻子”或“母亲”。一个无偏的模型可能会把空白处填充为“医生”、“律师”、“科学家”、“银行家”、“作家”或“企业家”。想象一下，使用这个有偏见的模型作为简历筛选分类器、就业聊天热线或课程规划助手。它会不知不觉地，但非常可衡量地，继续对某些职业对女性的不利推荐！让我们再看几个语言上的例子：
- en: '![Figure 11.1 – Biased inference results from GPT-J 6B](img/Figure_11.1_B18942.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 来自 GPT-J 6B 的偏见推理结果](img/Figure_11.1_B18942.jpg)'
- en: Figure 11.1 – Biased inference results from GPT-J 6B
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 来自 GPT-J 6B 的偏见推理结果
- en: 'Here, I simply used my own name as a prompt into the GPT-J 6B model, and it
    thought I was a makeup artist. Alternatively, if I used the name “John”, it thought
    I was a software developer:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我仅仅使用了自己的名字作为提示输入到 GPT-J 6B 模型中，模型认为我是一名化妆师。或者，如果我使用了“John”这个名字，它会认为我是一名软件开发者：
- en: '![Figure 11.2 – Biased inference results from GPT-J 6B (continued)](img/Figure_11.2_B18942.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 来自 GPT-J 6B 的偏见推理结果（续）](img/Figure_11.2_B18942.jpg)'
- en: Figure 11.2 – Biased inference results from GPT-J 6B (continued)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 来自 GPT-J 6B 的偏见推理结果（续）
- en: Once you try this again, however, the responses obviously change. This is because
    the random seed isn’t set in the Hugging Face model playground, so the output
    from the **neural network** (**NN**) can change. When I tried it again for John,
    it still gave a response of “software developer”. When I tried it again for myself,
    it responded with “a freelance social media consultant”.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当你再次尝试时，回应显然发生了变化。这是因为在 Hugging Face 模型平台中，随机种子并未设置，因此**神经网络**（**NN**）的输出可能会变化。当我再次为
    John 尝试时，它仍然给出了“软件开发人员”的回答。当我再次为我自己尝试时，它的回答是“自由职业社交媒体顾问”。
- en: 'Some of you might be wondering: Why is this biased? Isn’t this just holding
    to the statistical representation in the dataset? The answer is that the dataset
    itself is biased. There are more examples of male software engineers, fewer examples
    of female entrepreneurs, and so on. When we train AI/ML models on these datasets,
    we bring that bias directly into our applications. This means that if we use a
    biased model to screen resumes, suggest promotions, stylize text, assign credit,
    predict healthy indicators, determine criminal likelihoods, and more, we perpetuate
    that same bias systematically. And that is a big problem—one we need to actively
    fight against.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你们中的一些人可能会想：为什么这有偏见？这不就是依赖数据集中统计表示吗？答案是数据集本身就有偏见。数据集中男性软件工程师的例子更多，女性企业家的例子更少，等等。当我们在这些数据集上训练AI/ML模型时，我们将这种偏见直接带入我们的应用中。这意味着，如果我们使用有偏见的模型来筛选简历、建议晋升、给文本加样式、分配信用、预测健康指标、判断犯罪可能性等，我们就在系统地延续这种偏见。这是一个大问题——我们需要积极应对。
- en: This guess-and-check process we’re doing right now with the pretrained model
    is called **detecting bias** or **identifying bias**. We are taking a pretrained
    model and providing it with specific scenarios at the intersection of our groups
    of interest defined previously, to empirically determine how well it performs.
    Once you’ve found a few empirical examples of bias, it’s helpful to also run summary
    statistics on this to understand how regularly this occurs in your dataset. Amazon
    researchers proposed a variety of metrics to do so here *(3)*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在正在用预训练模型进行的这种猜测和检验过程叫做**检测偏见**或**识别偏见**。我们正在使用一个预训练模型，并在之前定义的感兴趣群体交集的具体场景中进行测试，以实证地确定其表现如何。一旦你找到一些偏见的实证例子，运行汇总统计数据来了解其在数据集中发生的频率也是有帮助的。亚马逊的研究人员在这里提出了多种度量方法来做到这一点*(3)*。
- en: You might do a similar process with a pretrained vision model, such as **Stable
    Diffusion**. Ask your Stable Diffusion model to generate images of people, of
    workers, of different scenarios in life. Try phrasing your prompt to force the
    model to categorize a person around one of the points of intersection, and these
    days you are almost guaranteed to find empirical evidence of bias.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以对预训练的视觉模型做类似的处理，例如**稳定扩散**。让你的稳定扩散模型生成不同场景下的人物图像、工人图像等。尝试调整提示语，强迫模型将一个人分类为交集的某一类别，现如今你几乎可以保证会找到偏见的实证证据。
- en: Fortunately, more models are using “safety filters”, which explicitly bar the
    model from being able to produce violent or explicit content, but as you’ve learned
    in this chapter, that is far from being without bias.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，越来越多的模型正在使用“安全过滤器”，这些过滤器明确禁止模型生成暴力或露骨内容，但正如你在本章中所学到的，这远非没有偏见。
- en: By now, you should have a good idea of what bias means in the context of your
    application. You should know for which groups of people you want to design, and
    in which categories you want to improve your model’s performance. Make sure you
    spend a decent amount of time empirically evaluating bias in your model, as this
    will help you demonstrate that the following techniques actually improve the outcomes
    you care about.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该已经对应用中偏见的含义有了清晰的认识。你应该知道自己想要为哪些群体设计，在哪些类别中希望提升模型的表现。确保你花足够的时间在实证上评估模型中的偏见，因为这将帮助你证明以下技术确实能改善你关心的结果。
- en: Mitigating bias in vision and language models
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少视觉和语言模型中的偏见
- en: Now that you’ve learned about detecting bias in your vision and language models,
    let’s explore methods to mitigate this. Generally, this revolves around updating
    your dataset in various ways, whether through sampling, augmentation, or generative
    methods. We’ll also look at some techniques to use during the training process
    itself, including the concept of fair loss functions and other techniques.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经了解了如何检测视觉和语言模型中的偏差，接下来让我们探讨如何减轻这种偏差。通常，这涉及通过各种方式更新数据集，无论是通过采样、增强还是生成方法。我们还将讨论一些在训练过程中使用的技术，包括公平损失函数的概念以及其他技术。
- en: As you are well aware by now, there are two key training phases to stay on top
    of. The first is the **pretraining process**, and the second is the **fine-tuning**
    or **transfer** **learning** (**TL**). In terms of bias, a critical point is how
    much bias transfer your models exhibit. That is to say, if your pretrained model
    was built on a dataset with bias, does that bias then transfer into your new model
    after you’ve done some fine-tuning?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，目前有两个关键的训练阶段需要关注。第一个是**预训练过程**，第二个是**微调**或**迁移****学习**（**TL**）。在偏差方面，一个关键点是你的模型表现出多少偏差转移。也就是说，如果你的预训练模型是基于带有偏差的数据集构建的，那么在你进行一些微调后，这些偏差是否会转移到新的模型中？
- en: 'A research team out of MIT delivered an interesting study on the effects of
    bias transfer in vision as recently as 2022 *(4)*, where they concluded: “*bias
    in pretrained models remained present even after fine-tuning these models on downstream
    target tasks. Crucially these biases can persist even when the target dataset
    used for fine-tuning did not contain such biases.*” This indicates that in vision,
    it is critical to ensure that your upstream pretrained dataset is bias-free. They
    found that the bias carried through into the downstream task.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 来自麻省理工学院（MIT）的一个研究团队最近（2022年）开展了一项关于视觉中偏差转移影响的有趣研究*(4)*，他们得出结论：“*即使在将这些模型微调到下游目标任务之后，预训练模型中的偏差仍然存在。关键是，即使微调使用的目标数据集本身不包含这些偏差，这些偏差依然会存在。*”这表明，在视觉领域，确保上游的预训练数据集没有偏差是至关重要的。研究发现，偏差会传递到下游任务中。
- en: 'A similar study for language found exactly the opposite of this! (*11*) Using
    a regression analysis in their work, the researchers realized that a better explanation
    for the presence of bias *was in the fine-tuned dataset* rather than the pretrained
    one. They concluded: “*attenuating downstream bias via upstream interventions—including
    embedding-space bias mitigation—is mostly futile.*” In language, the recommendation
    is to mostly mitigate bias in your downstream task, rather than upstream.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语言的类似研究得出了完全相反的结论！(*11*) 研究人员在他们的工作中使用回归分析，意识到偏差的存在更好的解释是*微调数据集*中的偏差，而不是预训练数据集。他们得出结论：“*通过上游干预来减轻下游偏差——包括嵌入空间中的偏差减轻——大多数时候是徒劳的。*”在语言领域，建议主要在下游任务中减轻偏差，而不是在上游。
- en: How interesting is this?! Similar work in two different domains reached opposite
    conclusions about the most effective place to focus for mitigating bias. This
    means if you are working on a vision scenario, you should spend your time optimizing
    your pretrained dataset to remove bias. Alternatively, if you’re on a language
    project, you should focus on reducing bias in the fine-tuning dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这有多有趣？！在两个不同领域的类似工作中，关于减轻偏差的最有效聚焦点得出了相反的结论。这意味着，如果你在做视觉场景的工作，你应该花时间优化你的预训练数据集，消除偏差。相反，如果你在做语言项目，你应该专注于减少微调数据集中的偏差。
- en: Perhaps this means that vision models on average carry more context and background
    knowledge into their downstream performance, such as correlating objects with
    nearby objects and patterns as a result of the convolution, while language only
    applies this context learning at a much smaller sentence-level scope.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 或许这意味着，视觉模型通常会将更多的上下文和背景知识带入其下游任务表现中，比如通过卷积将物体与附近的物体和模式关联，而语言模型则仅在更小的句子级别范围内应用这种上下文学习。
- en: Bias mitigation in language – counterfactual data augmentation and fair loss
    functions
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言中的偏差减轻——反事实数据增强和公平损失函数
- en: In language, many bias mitigation techniques center on *creating counterfactuals*.
    Remember—a counterfactual is a hypothetical scenario that did not happen in the
    real world but could have. For example, this morning you had many options for
    eating breakfast. You might have had coffee with a muffin. You also might have
    had breakfast cereal with orange juice. You might have gone out to a restaurant
    for breakfast with a friend, or maybe you skipped breakfast entirely. One of them
    really happened to you, but the other ones are completely fabricated. Possible,
    but fabricated. Each of these different scenarios could be considered *counterfactual*.
    They represent different scenarios and chains of events that did not actually
    happen but are reasonably likely to occur.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言学中，许多偏见缓解技术侧重于*创造反事实*。记住——反事实是指在现实世界中没有发生但本可以发生的假设情境。例如，今天早上你有很多选择可以吃早餐。你可能选择了咖啡配松饼。你也可能选择了早餐麦片配橙汁。你还可能和朋友一起去餐馆吃早餐，或者完全跳过了早餐。上述其中一个情况确实发生了，而其他情况完全是编造的。它们是可能的，但却是虚构的。每一种不同的情景都可以被视为*反事实*。它们代表了不同的情景和事件链，这些事件并未实际发生，但合理地可能会发生。
- en: 'Now, consider this: what if you wanted to represent each scenario as being
    equally likely to occur? In the dataset of your life, you’ve established certain
    habits of being. If you wanted to train a model to consider all habits as equally
    likely, you’d need to create counterfactuals to equally represent all other possible
    outcomes. This type of dataset-hacking is exactly what we’re doing when we try
    to augment a dataset to debias it, or to mitigate the bias. First, we identify
    the top ways that bias creeps into our models and datasets, and then we mitigate
    that bias by creating more examples of what we don’t have enough examples for,
    creating counterfactuals.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑这个问题：如果你想让每个情景都有相等的发生概率，该怎么办？在你生活的数据集中，你已经建立了某些习惯。如果你想训练一个模型，使得所有习惯都被视为同样可能发生的，你就需要创造反事实，以平衡所有其他可能的结果。这种类型的数据集修改正是我们在尝试通过增强数据集来去偏见，或减少偏见时所做的。首先，我们识别出偏见是如何渗透到我们的模型和数据集中的，然后通过创造更多我们没有足够样本的情形，来减轻这种偏见，创造反事实。
- en: 'One study presenting these methods is available in reference *(5)* in the *References*
    section—it includes researchers from Amazon, UCLA, Harvard, and more. As mentioned
    previously, they focused on the intersection of gender and employment. Let’s take
    a look at an example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍这些方法的研究可以在*参考文献*部分的参考文献*(5)*中找到——这项研究包括了来自亚马逊、UCLA、哈佛等的研究人员。如前所述，他们专注于性别与就业的交集。让我们看一个例子：
- en: '![Figure 11.3 – Comparing responses from normal and debiased models](img/Figure_11.3_B18942.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – 比较普通模型与去偏见模型的响应](img/Figure_11.3_B18942.jpg)'
- en: Figure 11.3 – Comparing responses from normal and debiased models
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 比较普通模型与去偏见模型的响应
- en: To generate counterfactual samples for their fine-tuning dataset, the researchers
    used a common technique of switching pronouns. In particular, they “*use a curated
    dictionary of gender words with male <-> female mapping, for instance, father
    -> mother, she-> he, him-> her, and so on*”. With this pronoun dictionary, they
    generated new sequences and included these in the fine-tuning dataset.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为他们的微调数据集生成反事实样本，研究人员使用了一个常见的技术，即交换代词。具体来说，他们“*使用一个精心编制的性别词典，其中包含男性<->女性的映射，例如父亲
    -> 母亲，她 -> 他，他 -> 她，等等*”。利用这个代词词典，他们生成了新的序列，并将这些序列包含进了微调数据集中。
- en: They also defined a fair knowledge distillation loss function. We’ll learn all
    about knowledge distillation in the upcoming chapter, but at a high level, what
    you need to know is that it’s the process of training a smaller model to mimic
    the performance of a larger model. Commonly this is done to shrink model sizes,
    giving you ideally the same performance as your large model, but on something
    much smaller you can use to deploy in single-GPU environments.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还定义了一个公平的知识蒸馏损失函数。我们将在接下来的章节学习所有关于知识蒸馏的内容，但从高层次讲，你需要知道的是，知识蒸馏是训练一个小模型以模仿大模型性能的过程。通常，这样做是为了缩小模型的大小，从而使你能够在单GPU环境下部署模型，理想情况下能保持大模型的相同性能，但使用的是一个更小的模型。
- en: 'Here, the researchers developed a novel distillation strategy to *equalize
    probabilities*. In generic distillation, you want the student model to learn the
    same probabilities for a given pattern:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，研究人员开发了一种新颖的蒸馏策略来*平衡概率*。在通用的蒸馏中，你希望学生模型学习到相同的概率分布，以应对给定的模式：
- en: '![Figure 11.4 – Equalizing distributions through distillation](img/Figure_11.4_B18942.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.4 – 通过蒸馏均衡分布](img/Figure_11.4_B18942.jpg)'
- en: Figure 11.4 – Equalizing distributions through distillation
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – 通过蒸馏均衡分布
- en: Here, the researchers knew this would lead to the student model learning exactly
    the same biased behavior they wanted to avoid. In response, they developed a novel
    distillation loss function to weight both the original and the counterfactual
    distributions as the same. This equalizing loss function helped their model learn
    to see both outcomes as equally likely and enabled the fair prompt responses you
    just saw! Remember—in order to build AI/ML applications that do not perpetuate
    the bias inherent in the datasets, we need to equalize how people are treated
    in the model itself.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，研究人员知道这将导致学生模型学到他们希望避免的相同偏见行为。为此，他们开发了一种新颖的蒸馏损失函数，将原始分布和反事实分布的权重设为相同。这个均衡损失函数帮助他们的模型学会将这两种结果视为同样可能，并使得你刚刚看到的公平提示响应得以实现！记住——为了构建不延续数据集中固有偏见的
    AI/ML 应用程序，我们需要在模型本身中均衡对待人群的方式。
- en: Now that we’ve learned about a few ways to overcome bias in language, let’s
    do the same for vision.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了克服语言偏差的几种方法，接下来让我们做同样的事情来解决视觉中的偏差。
- en: Bias mitigation in vision – reducing correlation dependencies and solving sampling
    issues
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉中的偏差缓解 – 减少相关性依赖并解决采样问题
- en: 'In vision scenarios, you have at least two big problems to tackle, as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉场景中，你至少有两个大问题需要解决，具体如下：
- en: First, not having enough pictures of under-represented groups of people
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，缺乏足够的关于代表性不足群体的图片
- en: Second, realizing after it’s too late that your pictures are all correlated
    with underlying objects or styles
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，意识到自己的图片与潜在的物体或风格相关，直到为时已晚
- en: 'In the first scenario, your model is likely to not learn that class at all.
    In the second scenario, your model learns a correlated confounding factor. It
    might learn more about objects in the background, overall colors, the overall
    style of the image, and so much more than it does about the objects you think
    it’s detecting. Then, it continues to use those background objects or traces to
    make classification guesses, where it clearly underperforms. Let’s explore a 2021
    study *(6)* from Princeton to learn more about these topics:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个场景中，你的模型可能根本无法学习到该类。 在第二个场景中，你的模型学到了一个相关的混杂因子。它可能学到了更多关于背景中的物体、整体色彩、图像的整体风格等方面的信息，而不是你认为它在检测的物体。然后，它继续利用这些背景物体或痕迹进行分类猜测，从而导致明显的性能不佳。让我们通过普林斯顿大学
    2021 年的一项研究 *(6)* 来深入了解这些话题：
- en: '![Figure 11.5 – Correct and incorrect vision classifications](img/Figure_11.5_B18942.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.5 – 正确与错误的视觉分类](img/Figure_11.5_B18942.jpg)'
- en: Figure 11.5 – Correct and incorrect vision classifications
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – 正确与错误的视觉分类
- en: Fundamentally, what these images show is the correlation problem in computer
    vision. Here the model is simply trying to classify males and females in images.
    However, due to underlying correlations in these datasets, the model makes basic
    mistakes. In terms of sports uniforms, the researchers found that “*males tend
    to be represented as playing outdoor sports like baseball, while females tend
    to be portrayed as playing an indoor sport like basketball or in a swimsuit*”.
    This means the model thought everyone wearing a sports uniform indoors was female,
    and everyone wearing a sports uniform outdoors was male! Alternatively, for flowers,
    the researchers found a “*drastic difference in how males and females are portrayed,
    where males pictured with a flower are in formal, official settings, whereas females
    are in staged settings or paintings*." Hopefully, you can immediately see why
    this is a problem; even the model thinks that everyone in a formal setting is
    male, simply due to a lack of available training data!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上说，这些图像展示了计算机视觉中的相关性问题。这里，模型仅仅是试图对图像中的男性和女性进行分类。然而，由于这些数据集中的潜在相关性，模型会犯一些基本错误。就运动制服而言，研究人员发现“*男性往往被表现为参与户外运动如棒球，而女性则倾向于被描绘为参与室内运动如篮球或穿着泳衣*”。这意味着模型认为所有穿着运动制服在室内的人是女性，所有穿着运动制服在户外的人是男性！或者，在关于花卉的研究中，研究人员发现“*男性与花卉合照时通常是在正式、官方场合，而女性则出现在摆拍场景或画作中*。”希望你能立刻看到这是个问题；即使是模型也认为，所有出现在正式场合的人都是男性，单纯是因为缺乏相关的训练数据！
- en: 'How do we solve this problem? One angle the researchers explored was geography.
    They realized that—consistent with previous analyses—the images’ countries of
    origin were overwhelmingly the United States and European nations. This was true
    across the multiple datasets they analyzed that are common in vision research.
    In the following screenshot, you can see the model learns an association of the
    word “dish” with food items from Eastern Asia while failing to detect plates or
    satellite dishes, which were more common images from other regions:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们该如何解决这个问题呢？研究人员探讨的一个角度是地理因素。他们意识到——与先前的分析一致——图像的原产国主要是美国和欧洲国家。这在他们分析的多个常见视觉研究数据集中都是如此。在以下截图中，您可以看到模型将“dish”一词与东亚的食物物品关联起来，而未能检测到其他地区更常见的盘子或卫星天线：
- en: '![Figure 11.6 – Visual bias in the meaning of “dish” geographically](img/Figure_11.6_B18942.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.6 – “dish”一词在地理意义上的视觉偏见](img/Figure_11.6_B18942.jpg)'
- en: Figure 11.6 – Visual bias in the meaning of “dish” geographically
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 – “dish”一词在地理意义上的视觉偏见
- en: 'The Princeton team developed and open sourced a tool called *REVISE: REvealing
    VIsual biaSEs* *(7)* that any Python developer can use to analyze their own visual
    datasets and identify candidate objects and problems that will cause correlation
    issues. This actually uses Amazon’s Rekognition service in the backend to run
    large-scale classification and object detection on the dataset! You can, however,
    modify it to use open source classifiers if you prefer. The tool automatically
    suggests actions to take to reduce bias, many of which revolve around searching
    for additional datasets to increase the learning for that specific class. The
    suggested actions can also include adding extra tags, reconciling duplicate annotations,
    and more.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '普林斯顿团队开发并开源了一个名为*REVISE: REvealing VIsual biaSEs* *(7)*的工具，任何Python开发人员都可以用来分析自己的视觉数据集，并识别可能导致关联问题的候选对象和问题。该工具实际上在后台使用了亚马逊的Rekognition服务来对数据集进行大规模分类和对象检测！不过，如果您愿意，也可以修改它，使用开源分类器。该工具会自动建议采取减少偏见的措施，其中许多建议围绕着寻找额外的数据集，以增加对特定类别的学习。这些建议的行动还可能包括添加额外标签、整合重复注释等。'
- en: Now that we’ve learned about multiple ways to mitigate bias in vision and language
    models, let’s explore ways to monitor this in your applications.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了多种减轻视觉和语言模型偏见的方法，让我们来探讨如何在您的应用程序中监控这些偏见。
- en: Monitoring bias in ML models
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控机器学习模型中的偏见
- en: At this point in the book, for beginners, you are probably starting to realize
    that in fact, we are just at the tip of the iceberg in terms of identifying and
    solving bias problems. Implications for this range from everything from poor model
    performance to actual harm to humans, especially in domains such as hiring, criminal
    justice, financial services, and more. These are some of the reasons Cathy O’Neil
    raised these important issues in her 2016 book, *Weapons of Math Destruction*
    *(8)*. She argues that while ML models can be useful, they can also be quite harmful
    to humans when designed and implemented carelessly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，对于初学者来说，您可能已经开始意识到，实际上我们仅仅是在识别和解决偏见问题的冰山一角。其影响从模型性能差到对人类的实际伤害不等，尤其是在招聘、刑事司法、金融服务等领域。这也是凯西·奥尼尔在她2016年的书《数学毁灭武器》*(8)*中提出这些重要问题的一部分原因。她认为，尽管机器学习模型可以有用，但如果设计和实施不谨慎，它们也可能对人类造成相当大的伤害。
- en: This raises core issues about ML-driven innovation. How good is good enough
    in a world full of biases? As an ML practitioner myself who is passionate about
    large-scale innovation, and also as a woman who is on the negative end of some
    biases, while certainly on the positive side of others, I grapple with these questions
    a lot.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这引发了关于机器学习驱动创新的核心问题。在充满偏见的世界里，“足够好”到底有多好？作为一名热衷于大规模创新的机器学习从业者，同时也是一个在某些偏见中处于负面端、在其他偏见中处于正面端的女性，我常常会在这些问题上深思。
- en: Personally, there are some data science projects I just refuse to work on because
    of bias. For me, this includes at least hiring and resume screening, performance
    reviews, criminal justice, and some financial applications. Maybe someday we’ll
    have balanced data and truly unbiased models, but based on what I can see, we
    are far away from that. I encourage every ML practitioner to develop a similar
    personal ethic about projects that have the potential for a negative impact on
    humans. You can imagine that even something as seemingly innocuous as online advertising
    can lead to large-scale discrepancies among people. Ads for everything from jobs
    to education, networking to personal growth, products to financial tools, psychology,
    and business advice, can in fact perpetuate large-scale social biases.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 就我个人而言，有些数据科学项目我因为偏见而拒绝参与。对我来说，这至少包括招聘和简历筛选、绩效评估、刑事司法和一些金融应用。也许有一天我们能拥有平衡的数据和真正无偏的模型，但根据我目前的观察，我们离这个目标还很远。我鼓励每个机器学习从业者对那些可能对人类造成负面影响的项目建立类似的个人伦理。你可以想象，即使是看似无害的在线广告，也可能导致人群之间的大规模差异。从招聘、教育、社交网络到个人成长，从产品到金融工具，甚至心理学和商业建议的广告，实际上都可能延续大规模的社会偏见。
- en: At a higher level, I believe as an industry we can continue to evolve. While
    some professions require third-party certification, such as medical, legal, and
    education experts, ours still does not. Some service providers provide ML-specific
    certification, which is certainly a step in the right direction but doesn’t fully
    address the core dichotomy between pressure to deliver results from your employer
    with potential unknown and unidentified harm to your customers. Certainly, I’m
    not claiming to have any answers here; I can see the merits of both sides of this
    argument and can sympathize with the innovators just as well as with the end consumers.
    I’m just submitting that this is in fact a massive challenge for the entire industry,
    and I hope we can develop better mechanisms for this in the future.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 从更高层次看，我相信作为一个行业，我们可以继续发展。虽然一些行业要求第三方认证，例如医学、法律和教育专家，但我们的行业仍然没有。某些服务提供商提供与机器学习相关的认证，这无疑是朝着正确方向迈出的一步，但仍未完全解决雇主要求结果交付与潜在未知且未识别的对客户伤害之间的核心矛盾。当然，我并不是在这里声称自己有答案；我能看到这个论点双方的优点，并能同情创新者，也同样同情最终消费者。我只是在提出，这实际上是整个行业面临的一个巨大挑战，我希望我们能在未来为此开发出更好的机制。
- en: 'On a more immediately actionable note, for those of you who have a project
    to deliver on in the foreseeable future, I would suggest the following steps:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在更直接可操作的层面上，对于那些在可预见的未来有交付项目的朋友，我建议采取以下步骤：
- en: Identify a broad picture of your customer base, ideally with the help of a diverse
    team.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定你客户群体的广泛图景，最好有多元化团队的帮助。
- en: Identify what outcomes your model will impose on your customer; push yourself
    to think beyond the immediate impact on your business and your team. To use a
    phrase from Amazon, think big!
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定你的模型将对客户产生什么结果；推动自己超越对业务和团队的直接影响来思考。用亚马逊的一句口号说，思考更宏大！
- en: Try to find empirical examples of your best- and worst-case scenarios—best case
    being where your model leads to win-wins, and worst case where it leads to lose-lose.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试找到你的最佳和最差情况的实证例子——最佳情况是你的模型带来了双赢的结果，最差情况是导致了两败俱伤。
- en: Use the techniques we learned throughout this book to make the win-wins more
    common, and the lose-lose as infrequent as you can. Remember—this usually comes
    down to analyzing your data, learning about its flaws and inherent perspectives,
    and remedying these either through the data itself or through your model and learning
    process.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用本书中学到的技巧，让双赢的局面更加常见，把两败俱伤的局面尽量减少。记住——这通常归结于分析你的数据，了解其缺陷和固有视角，并通过数据本身或通过你的模型和学习过程加以修正。
- en: Add transparency. As O’Neil points out in her book, part of the industry-wide
    problem is applications that impact humans in major ways not explaining which
    features actually drive their final classification. To solve this, you can add
    simple feature importance testing through LIME *(9)*, or pixel and token mapping,
    as we’ll see next with SageMaker Clarify.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加透明度。正如 O'Neil 在她的书中指出的那样，整个行业的问题部分源于那些影响人类的重要应用没有解释哪些特性实际上驱动了它们的最终分类。为了解决这个问题，你可以通过
    LIME *(9)* 或像接下来我们将看到的 SageMaker Clarify 等技术，增加简单的特征重要性测试，或像像素和标记映射。
- en: Try to develop quantitative measures for especially your worst-case scenario
    outcomes and monitor these in your deployed application.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试为特别是最糟糕的结果场景制定定量衡量标准，并在已部署的应用程序中监控这些标准。
- en: As it turns out, one way to detect, mitigate, and monitor bias in your models
    is SageMaker Clarify!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，检测、缓解和监控模型偏见的一种方式是SageMaker Clarify！
- en: Detecting, mitigating, and monitoring bias with SageMaker Clarify
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker Clarify检测、缓解和监控偏见
- en: 'SageMaker Clarify is a feature within the SageMaker service you can use for
    bias and explainability across your ML workflows. It has a nice integration with
    SageMaker’s Data Wrangler, a fully managed UI for tabular data analysis and exploration.
    This includes nearly 20 bias metrics, statistical terms you can study and use
    to get increasingly more precise about how your model interacts with humanity.
    I’ll spare you the mathematics here, but feel free to read more about them in
    my blog post on the topic here: [https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72](https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72)
    *(10)*!'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Clarify是SageMaker服务中的一项功能，可以在你的机器学习工作流中用于偏见检测和可解释性。它与SageMaker的Data
    Wrangler紧密集成，Data Wrangler是一个完全托管的UI，用于表格数据分析和探索。该功能包括近20个偏见指标，统计术语你可以研究并使用，以便更精准地理解你的模型与人类之间的互动。我会在这里省略数学内容，但你可以随时阅读我关于此主题的博客文章：[https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72](https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72)
    *(10)*！
- en: Arguably more relevant for this book are Clarify’s *vision and language features*!
    This includes explaining image classification and object detection, along with
    language classification and regression. This should help you immediately understand
    what is driving your discriminative models’ output, and help you take steps to
    rectify any biased decisioning.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本书更相关的是Clarify的*视觉和语言特性*！这包括解释图像分类和目标检测，以及语言分类和回归。这应该帮助你立即理解驱动你区分模型输出的因素，并帮助你采取措施纠正任何偏见决策。
- en: Actually, the modularity of large pretrained models in combination with smaller
    outputs, such as using Hugging Face to easily add a classification output to a
    pretrained **large language model** (**LLM**), might be a way we could debias
    pretrained models using Clarify, ultimately using them for generation. A strong
    reason to use Clarify is that you can monitor both bias metrics and model explainability!
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，大型预训练模型的模块化与较小输出（例如，使用Hugging Face轻松添加分类输出到预训练**大语言模型**（**LLM**））的结合，可能是我们利用Clarify去偏见预训练模型的一种方法，最终用于生成任务。使用Clarify的一个重要理由是，你可以监控偏见指标和模型可解释性！
- en: In the next section of the book, *Part 5*, we will dive into key questions around
    deployment. Particularly, in [*Chapter 14*](B18942_14.xhtml#_idTextAnchor217),
    we’ll dive into ongoing operations, monitoring, and maintenance of models deployed
    into production. We’ll cover SageMaker Clarify’s monitoring features extensively
    there, especially discussing how you can connect these both to audit teams and
    automatic retraining workflows.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的下一部分，*第5部分*，我们将深入探讨部署相关的关键问题。特别是在[ *第14章* ](B18942_14.xhtml#_idTextAnchor217)中，我们将深入讨论已部署到生产中的模型的持续操作、监控和维护。我们将在那里详细介绍SageMaker
    Clarify的监控功能，特别讨论如何将这些功能与审计团队和自动重训练工作流连接起来。
- en: Summary
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we dove into the concept of bias in ML, and especially explored
    angles in vision and language. We opened with a general discussion of human bias
    and introduced a few ways these empirically manifest in technology systems, frequently
    without intention. We introduced the concept of “intersectional bias”, and how
    commonly your first job in detecting bias is listing a few common types of intersections
    you want to be wary of, including gender or race and employment, for example.
    We demonstrated how this can easily creep into large vision and language models
    trained on datasets crawled from the internet. We also explored methods to mitigate
    bias in ML models. In language, we presented counterfactual data augmentation
    along with fair loss functions. In vision, we learned about the problem of correlational
    dependencies, and how you can use open source tools to analyze your vision dataset
    and solve sampling problems.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了机器学习中的偏差概念，特别是从视觉和语言的角度进行了探索。我们首先讨论了人类偏差的一般情况，并介绍了几种这些偏差如何在技术系统中以经验性方式表现出来，通常是无意的。我们介绍了“交叉偏差”这一概念，并讲解了检测偏差时，首先需要列出一些常见的交叉类型，包括性别或种族与就业等。例如，我们展示了这一偏差如何轻易渗入从互联网抓取的数据集所训练的大型视觉和语言模型。我们还探讨了缓解机器学习模型中的偏差的方法。在语言方面，我们介绍了反事实数据增强和公平损失函数。在视觉方面，我们了解了相关依赖问题，并展示了如何使用开源工具分析视觉数据集并解决采样问题。
- en: Finally, we learned about monitoring bias in ML models, including a large discussion
    about both personal and professional ethics, and actional steps for your projects.
    We closed out with a presentation of SageMaker Clarify, which you can use to detect,
    mitigate, and monitor bias in your ML models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了如何监控机器学习模型中的偏差，包括关于个人和职业伦理的广泛讨论，以及针对项目的可操作步骤。我们以SageMaker Clarify的展示作为结尾，您可以使用它来检测、缓解并监控您的机器学习模型中的偏差。
- en: 'Now, let’s dive into *Part Five: Deployment!*. In the next chapter, we will
    learn about how to deploy your model on SageMaker.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入研究*第五部分：部署！* 在下一章中，我们将学习如何在SageMaker上部署您的模型。
- en: References
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Please go through the following content for more information on a few topics
    covered in the chapter:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下内容，了解本章涉及的某些主题的更多信息：
- en: '*Google apologises for Photos app’s racist* *blunder*: [https://www.bbc.com/news/technology-33347866](https://www.bbc.com/news/technology-33347866)'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*谷歌为照片应用的种族主义错误道歉*: [https://www.bbc.com/news/technology-33347866](https://www.bbc.com/news/technology-33347866)'
- en: '*Amazon scraps secret AI recruiting tool that showed bias against* *women*:
    [https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*亚马逊废弃了一个显示对女性存在偏见的秘密AI招聘工具*: [https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)'
- en: '*BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation*:
    [https://assets.amazon.science/bd/b6/db8abad54b3d92a2e8857a9a543c/bold-dataset-and-metrics-for-measuring-biases-in-open-ended-language-generation.pdf](https://assets.amazon.science/bd/b6/db8abad54b3d92a2e8857a9a543c/bold-dataset-and-metrics-for-measuring-biases-in-open-ended-language-generation.pdf)'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*粗体：用于衡量开放式语言生成偏差的数据集和指标*: [https://assets.amazon.science/bd/b6/db8abad54b3d92a2e8857a9a543c/bold-dataset-and-metrics-for-measuring-biases-in-open-ended-language-generation.pdf](https://assets.amazon.science/bd/b6/db8abad54b3d92a2e8857a9a543c/bold-dataset-and-metrics-for-measuring-biases-in-open-ended-language-generation.pdf)'
- en: '*When does Bias Transfer in Transfer* *Learning?*: [https://arxiv.org/pdf/2207.02842.pdf](https://arxiv.org/pdf/2207.02842.pdf)'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*偏差在迁移学习中何时发生转移？*: [https://arxiv.org/pdf/2207.02842.pdf](https://arxiv.org/pdf/2207.02842.pdf)'
- en: '*Mitigating Gender Bias in Distilled Language Models via Counterfactual Role*
    *Reversal*: [https://aclanthology.org/2022.findings-acl.55.pdf](https://aclanthology.org/2022.findings-acl.55.pdf)'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*通过反事实角色反转来缓解蒸馏语言模型中的性别偏差*: [https://aclanthology.org/2022.findings-acl.55.pdf](https://aclanthology.org/2022.findings-acl.55.pdf)'
- en: '*REVISE: A Tool for Measuring and Mitigating Bias in Visual* *Datasets*: [https://arxiv.org/pdf/2004.07999.pdf](https://arxiv.org/pdf/2004.07999.pdf)'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*修订：用于衡量和减少视觉数据集偏差的工具*: [https://arxiv.org/pdf/2004.07999.pdf](https://arxiv.org/pdf/2004.07999.pdf)'
- en: '*princetonvisualai/revise-tool*: [https://github.com/princetonvisualai/revise-tool](https://github.com/princetonvisualai/revise-tool)'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*princetonvisualai/revise-tool*：[https://github.com/princetonvisualai/revise-tool](https://github.com/princetonvisualai/revise-tool)'
- en: '*Weapons of Math Destruction: How Big Data Increases Inequality and Threatens
    Democracy Hardcover – September 6,* *2016*: [https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815)'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*数学毁灭武器：大数据如何加剧不平等并威胁民主* 精装版 – 2016年9月6日：[https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815](https://www.amazon.com/Weapons-Math-Destruction-Increases-Inequality/dp/0553418815)'
- en: '*Why Should I Trust You?” Explaining the Predictions of Any* *Classifier*:
    [https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*为什么我应该信任你？解释任何分类器的预测*：[https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)'
- en: '*Dive into Bias Metrics and Model Explainability with Amazon SageMaker* *Clarify*:
    [https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72](https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72)'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*深入了解偏差度量和模型可解释性，使用Amazon SageMaker Clarify*：[https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72](https://towardsdatascience.com/dive-into-bias-metrics-and-model-explainability-with-amazon-sagemaker-clarify-473c2bca1f72)'
- en: '*Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis
    in Pre-Trained Language* *Models:* [https://aclanthology.org/2022.acl-long.247.pdf](https://aclanthology.org/2022.acl-long.247.pdf)'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*上游缓解并非你所需要的一切：测试预训练语言模型中的偏差转移假设*：[https://aclanthology.org/2022.acl-long.247.pdf](https://aclanthology.org/2022.acl-long.247.pdf)'
